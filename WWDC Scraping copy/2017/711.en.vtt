WEBVTT

00:00:26.156 --> 00:00:26.516
>> Thank you.

00:00:26.516 --> 00:00:27.806
And welcome to the Accelerate

00:00:27.806 --> 00:00:28.166
Session.

00:00:28.166 --> 00:00:29.296
My name is Eric Bainville.

00:00:29.296 --> 00:00:30.826
I'm with the CoreOS Vector and

00:00:30.826 --> 00:00:31.566
Numerics Group.

00:00:32.326 --> 00:00:34.916
Our group is maintaining the

00:00:34.916 --> 00:00:35.956
Accelerate framework.

00:00:36.336 --> 00:00:37.566
And this is our agenda for

00:00:37.566 --> 00:00:37.786
today.

00:00:37.786 --> 00:00:38.926
So first I will introduce

00:00:38.926 --> 00:00:41.286
Accelerate, what's inside, how

00:00:41.286 --> 00:00:42.676
to use it, and through a few

00:00:42.676 --> 00:00:43.956
examples I'll show you why you

00:00:43.956 --> 00:00:44.736
want to use that.

00:00:44.736 --> 00:00:47.346
And then we focus on new

00:00:47.346 --> 00:00:48.576
improvements and additions we

00:00:48.576 --> 00:00:50.196
have this year, first with

00:00:50.196 --> 00:00:51.786
Compression, lossless data

00:00:51.786 --> 00:00:54.176
compression library, then BNNS,

00:00:54.236 --> 00:00:55.566
Basic Neural Network

00:00:55.566 --> 00:00:56.126
Subroutines.

00:00:57.006 --> 00:00:58.676
And after that my colleague

00:00:58.676 --> 00:00:59.836
Steve will come on stage and

00:00:59.836 --> 00:01:01.246
tell us what's new in simd.

00:01:01.556 --> 00:01:02.796
And finally, our very own

00:01:02.996 --> 00:01:04.436
Jonathan Hogg will come on stage

00:01:04.436 --> 00:01:05.876
and tell us -- introduce the

00:01:05.876 --> 00:01:07.176
sparse matrices package.

00:01:07.686 --> 00:01:08.666
It will be the first time

00:01:08.716 --> 00:01:10.206
commercialized shipping with

00:01:10.206 --> 00:01:12.386
sparse matrices solver package.

00:01:12.806 --> 00:01:14.196
But let's start with Accelerate.

00:01:15.186 --> 00:01:17.536
Accelerate is a low-level system

00:01:17.536 --> 00:01:19.356
framework dedicated to high

00:01:19.356 --> 00:01:20.846
performance primitive on the

00:01:20.846 --> 00:01:21.356
CPU.

00:01:22.576 --> 00:01:25.186
Actually, it's everywhere when

00:01:25.186 --> 00:01:26.686
you have a significant workload

00:01:26.686 --> 00:01:27.706
to run on the CPU.

00:01:28.466 --> 00:01:30.136
Inside Accelerate there's a lot

00:01:30.136 --> 00:01:32.196
of libraries, like vImage for

00:01:32.196 --> 00:01:33.516
image processing, the Swiss

00:01:34.116 --> 00:01:35.466
knife of image processing.

00:01:35.856 --> 00:01:37.636
We also have vDSP inside for

00:01:37.636 --> 00:01:39.326
DFT's and FFT's signal

00:01:39.326 --> 00:01:41.786
processing, vForce for vector

00:01:41.786 --> 00:01:42.426
functions.

00:01:43.046 --> 00:01:44.686
And then we have a whole lot of

00:01:44.766 --> 00:01:46.586
linear algebra libraries for

00:01:46.586 --> 00:01:48.626
dense and sparse matrices.

00:01:48.626 --> 00:01:50.126
So that's BLAS and LAPACK for

00:01:50.126 --> 00:01:51.436
dense vectors and matrices.

00:01:51.906 --> 00:01:53.056
And Sparse BLAS and Sparse

00:01:53.056 --> 00:01:54.996
Solvers for sparse vectors and

00:01:54.996 --> 00:01:55.576
matrices.

00:01:56.196 --> 00:01:57.656
Last year we also introduced

00:01:57.656 --> 00:01:59.076
BNNS, Basic Neural Network

00:01:59.076 --> 00:02:01.146
Subroutines, which is also

00:02:01.146 --> 00:02:02.146
[inaudible] for neural networks.

00:02:02.356 --> 00:02:04.766
This is used by, for example,

00:02:04.766 --> 00:02:06.726
Core ML and also Vision and NLP

00:02:06.726 --> 00:02:07.406
frameworks.

00:02:08.216 --> 00:02:09.626
Slightly outside Accelerate but

00:02:09.626 --> 00:02:11.476
still maintained by us, we have

00:02:11.476 --> 00:02:13.066
simd, which is a set of headers

00:02:13.066 --> 00:02:14.806
and function allowing you to

00:02:14.806 --> 00:02:16.536
directly talk to the CPU vector

00:02:16.536 --> 00:02:17.026
units.

00:02:17.996 --> 00:02:19.186
And finally, Compression, a

00:02:19.516 --> 00:02:20.616
lossless data compression

00:02:20.616 --> 00:02:21.046
library.

00:02:22.306 --> 00:02:23.696
How do you use Accelerate?

00:02:24.676 --> 00:02:26.216
Well, so you import Accelerate

00:02:26.216 --> 00:02:27.336
or you include the Accelerate

00:02:27.366 --> 00:02:28.946
header and then link with

00:02:28.946 --> 00:02:30.286
Accelerate frameworks on your

00:02:30.286 --> 00:02:30.726
own set.

00:02:30.726 --> 00:02:31.976
Now, I will show you a few

00:02:31.976 --> 00:02:34.086
examples about why you want to

00:02:34.086 --> 00:02:34.996
use Accelerate.

00:02:35.466 --> 00:02:36.596
Let's start with that one.

00:02:37.106 --> 00:02:37.886
So let's say you have this

00:02:37.886 --> 00:02:39.276
vector X of floating point

00:02:39.276 --> 00:02:40.936
values and you want to scale

00:02:40.936 --> 00:02:41.236
them.

00:02:42.126 --> 00:02:43.166
So you have this scale.

00:02:43.166 --> 00:02:44.996
And so you multiply each value

00:02:44.996 --> 00:02:46.216
and store it in the Y vector.

00:02:46.216 --> 00:02:48.206
So it's a very simple loop.

00:02:48.696 --> 00:02:49.676
That's perfectly fine.

00:02:50.176 --> 00:02:51.066
But actually, there is a

00:02:51.066 --> 00:02:52.946
function for you in Accelerate

00:02:52.946 --> 00:02:53.926
doing the same thing, it's

00:02:53.926 --> 00:02:54.696
called vsmul.

00:02:55.786 --> 00:02:57.666
So that's one single line

00:02:57.666 --> 00:02:58.646
replacing your code.

00:02:58.986 --> 00:03:00.996
And the good thing is that we

00:03:00.996 --> 00:03:02.766
optimize it for you on all the

00:03:02.766 --> 00:03:03.716
supported hardware.

00:03:04.226 --> 00:03:05.256
And of course, we maintain it

00:03:05.256 --> 00:03:06.316
for you so you don't have your

00:03:06.316 --> 00:03:07.566
loop to maintain again.

00:03:08.016 --> 00:03:09.856
And, of course, it's faster, as

00:03:09.856 --> 00:03:10.846
Accelerate says.

00:03:11.356 --> 00:03:12.766
So that's a reference speed and

00:03:12.766 --> 00:03:14.176
energy consumption for the loop.

00:03:14.406 --> 00:03:15.536
And this is what you get with

00:03:15.536 --> 00:03:16.136
Accelerate.

00:03:16.856 --> 00:03:18.166
Six times faster.

00:03:19.516 --> 00:03:21.616
[ Applause ]

00:03:22.116 --> 00:03:23.946
And six times less energy

00:03:23.946 --> 00:03:25.086
consumed, which is very

00:03:25.086 --> 00:03:25.566
important.

00:03:25.796 --> 00:03:26.876
Let me give you another one.

00:03:27.256 --> 00:03:28.736
So this time we still have this

00:03:28.876 --> 00:03:30.596
array X, and we want to clip the

00:03:30.596 --> 00:03:31.976
values between the low and high

00:03:31.976 --> 00:03:33.746
bound and store that in Y.

00:03:34.176 --> 00:03:35.156
Again, you could go with this

00:03:35.156 --> 00:03:35.466
code.

00:03:35.576 --> 00:03:36.376
That's perfectly fine.

00:03:36.376 --> 00:03:36.786
That's good.

00:03:37.336 --> 00:03:38.646
But we have a function for you

00:03:38.646 --> 00:03:40.306
in vDSP; it's called vclip.

00:03:40.936 --> 00:03:42.096
Does the same thing.

00:03:42.096 --> 00:03:43.586
And, again, we maintain it for

00:03:43.586 --> 00:03:43.756
you.

00:03:43.756 --> 00:03:44.846
We optimize it for you.

00:03:45.616 --> 00:03:46.796
And, of course, it's faster.

00:03:46.846 --> 00:03:48.276
That's a reference for the loop.

00:03:48.696 --> 00:03:49.736
And this is what you get with

00:03:49.736 --> 00:03:52.126
Accelerate, eight times faster

00:03:52.126 --> 00:03:53.756
and also eight times less energy

00:03:53.756 --> 00:03:54.246
consumed.

00:03:55.146 --> 00:03:58.166
Another one, matrices.

00:03:58.166 --> 00:03:59.016
So let's say you have two

00:03:59.016 --> 00:04:00.766
matrices A and B, and you want

00:04:00.766 --> 00:04:02.126
to compute the product of A and

00:04:02.126 --> 00:04:04.016
B and add the result into the C

00:04:04.016 --> 00:04:04.506
matrix.

00:04:05.026 --> 00:04:06.736
It doesn't sound simple.

00:04:06.776 --> 00:04:08.266
But actually, it's very simple.

00:04:08.266 --> 00:04:09.516
That's just three lines of code.

00:04:09.516 --> 00:04:10.166
You can see that.

00:04:11.356 --> 00:04:12.436
And we have, of course, a

00:04:12.436 --> 00:04:13.716
function for you in Accelerate

00:04:13.716 --> 00:04:15.396
doing that; it's called sgemm

00:04:15.396 --> 00:04:16.745
for the Cblas package.

00:04:17.906 --> 00:04:20.336
And really, really, you never

00:04:20.336 --> 00:04:22.046
want to write code computing

00:04:22.046 --> 00:04:23.206
metrics vector products or

00:04:23.206 --> 00:04:24.786
matrix metrics or anything

00:04:24.786 --> 00:04:25.646
related to metrics.

00:04:25.646 --> 00:04:26.576
You don't want to write that

00:04:26.576 --> 00:04:27.076
code ever.

00:04:28.196 --> 00:04:29.966
You want to call BLAS, LAPACK,

00:04:29.966 --> 00:04:31.196
or anything in Accelerate.

00:04:31.636 --> 00:04:32.946
Why? Well, first because we

00:04:32.946 --> 00:04:34.246
maintain that for you, and it

00:04:34.246 --> 00:04:35.296
will be optimized and

00:04:35.326 --> 00:04:37.336
multithreaded on all the

00:04:37.336 --> 00:04:38.546
architectures we support.

00:04:39.806 --> 00:04:41.676
And this time -- this is a

00:04:41.676 --> 00:04:42.706
reference for the loop -- this

00:04:42.706 --> 00:04:43.986
is what you get with Accelerate.

00:04:43.986 --> 00:04:46.996
I'm not sure you can see it.

00:04:47.686 --> 00:04:52.126
Yeah. That's 100 times faster

00:04:52.126 --> 00:04:54.236
and 26 times more energy

00:04:54.236 --> 00:04:54.716
efficient.

00:04:54.926 --> 00:04:56.296
That's your battery here.

00:04:56.986 --> 00:04:58.296
Okay. One more example, this

00:04:58.296 --> 00:04:59.346
time in vImage.

00:04:59.486 --> 00:05:01.156
So let's say you have a 32-bit

00:05:01.156 --> 00:05:02.656
per pixel image with four

00:05:02.656 --> 00:05:04.006
components per pixel.

00:05:04.006 --> 00:05:05.206
That's alpha, red, green, and

00:05:05.206 --> 00:05:05.536
blue.

00:05:05.916 --> 00:05:07.746
And you want to apply a 4 by 4

00:05:07.746 --> 00:05:09.106
transformation matrix to every

00:05:09.106 --> 00:05:10.116
pixel in the image.

00:05:10.876 --> 00:05:11.746
But you could write the code.

00:05:11.746 --> 00:05:12.736
You could write it here, that

00:05:12.736 --> 00:05:13.276
would be too long.

00:05:13.536 --> 00:05:14.526
But really, you don't want to

00:05:14.526 --> 00:05:15.186
write this code.

00:05:15.346 --> 00:05:16.316
We have a function for you in

00:05:16.316 --> 00:05:18.036
vImage, MatrixMultiply -- that's

00:05:18.036 --> 00:05:19.176
one of the most used functions

00:05:19.176 --> 00:05:21.026
in vImage -- doing exactly that

00:05:21.026 --> 00:05:23.906
and optimized to as fast as it

00:05:23.906 --> 00:05:25.446
can on all the architectures we

00:05:25.446 --> 00:05:25.866
support.

00:05:26.826 --> 00:05:30.066
Last one, this is convolution

00:05:30.066 --> 00:05:30.306
layer.

00:05:30.376 --> 00:05:33.036
That's the working horse of the

00:05:33.036 --> 00:05:34.526
neural networks, the convolution

00:05:34.526 --> 00:05:35.406
neural networks.

00:05:35.956 --> 00:05:38.416
So this layer takes input stacks

00:05:38.486 --> 00:05:40.016
-- that's the right thing on the

00:05:40.016 --> 00:05:42.076
left -- that's a stack of

00:05:42.076 --> 00:05:42.716
images.

00:05:43.066 --> 00:05:45.196
And it will put output image

00:05:45.196 --> 00:05:46.056
stack, the blue thing.

00:05:46.286 --> 00:05:48.046
And each pixel in the output is

00:05:48.046 --> 00:05:49.146
the result of a treaty

00:05:49.146 --> 00:05:51.376
convolution on all the entire

00:05:51.376 --> 00:05:52.226
input stack.

00:05:52.226 --> 00:05:53.886
And we do that for every of the

00:05:53.886 --> 00:05:54.806
three that I mentioned for

00:05:54.806 --> 00:05:55.506
output image.

00:05:56.066 --> 00:05:56.916
So at the end, that's a

00:05:56.916 --> 00:05:59.176
six-dimensional loop.

00:05:59.176 --> 00:06:00.146
And you really don't want to

00:06:00.146 --> 00:06:00.746
write this loop.

00:06:01.336 --> 00:06:02.896
And even when the dimensions are

00:06:02.896 --> 00:06:04.336
small, you're multiplying all of

00:06:04.336 --> 00:06:04.886
them together.

00:06:04.886 --> 00:06:07.066
So that's millions or even

00:06:07.066 --> 00:06:08.376
billions of floating points

00:06:08.376 --> 00:06:09.146
operation here.

00:06:09.786 --> 00:06:11.076
Of course, we have a function

00:06:11.076 --> 00:06:12.626
for you this time in BNNS doing

00:06:12.626 --> 00:06:12.906
that.

00:06:13.396 --> 00:06:14.516
And when you run a Core ML

00:06:14.516 --> 00:06:16.616
model, you will spend, like, 80%

00:06:16.616 --> 00:06:17.866
of the time in this function.

00:06:18.636 --> 00:06:19.386
All right.

00:06:19.386 --> 00:06:21.106
So that was a few examples.

00:06:21.186 --> 00:06:22.956
I could continue almost forever

00:06:22.956 --> 00:06:24.936
because we have more than 2,800

00:06:24.936 --> 00:06:26.516
API's in Accelerate.

00:06:27.146 --> 00:06:28.246
So usually that would be a

00:06:28.246 --> 00:06:29.386
function for you inside.

00:06:29.656 --> 00:06:30.756
And every time you use an

00:06:30.756 --> 00:06:32.456
Accelerate function, the

00:06:32.456 --> 00:06:34.146
benefits are that's less code

00:06:34.146 --> 00:06:36.516
for you to write; we maintain it

00:06:36.516 --> 00:06:38.086
for you; of course, it will be

00:06:38.086 --> 00:06:39.216
faster and more energy

00:06:39.216 --> 00:06:40.756
efficient; and it will be

00:06:40.756 --> 00:06:43.326
optimal -- as close to optimal

00:06:43.326 --> 00:06:44.346
as possible on all the

00:06:44.346 --> 00:06:45.476
architectures we support,

00:06:45.476 --> 00:06:46.606
including the new ones.

00:06:46.606 --> 00:06:47.986
So when we release new hardware,

00:06:48.696 --> 00:06:49.816
you will get your code running

00:06:49.816 --> 00:06:51.506
as fast as it can from day one.

00:06:52.596 --> 00:06:53.966
Okay. So that was it for

00:06:53.966 --> 00:06:54.536
Accelerate.

00:06:54.536 --> 00:06:56.466
Now let's focus on Compression.

00:06:58.826 --> 00:07:00.906
Compression is a lossless data

00:07:00.906 --> 00:07:01.746
compression library.

00:07:01.746 --> 00:07:04.096
It's a very simple API with a

00:07:04.096 --> 00:07:06.076
few select compressors inside.

00:07:06.406 --> 00:07:08.136
So they are represented on this

00:07:08.136 --> 00:07:09.046
little graph here.

00:07:09.466 --> 00:07:11.096
On the x-axis you see the

00:07:11.096 --> 00:07:12.506
relative compression ratio

00:07:12.736 --> 00:07:13.526
compared to ZLIB.

00:07:13.526 --> 00:07:14.616
ZLIB is in the center.

00:07:14.616 --> 00:07:16.596
And on the y-axis is the

00:07:16.596 --> 00:07:18.416
compression speed.

00:07:18.516 --> 00:07:20.556
It's not that exponential.

00:07:21.076 --> 00:07:22.346
So inside the compression

00:07:22.346 --> 00:07:24.486
library we have a selection of

00:07:24.486 --> 00:07:25.476
compressors, as I said.

00:07:26.026 --> 00:07:27.866
We offer LZMA for better

00:07:27.866 --> 00:07:30.166
compression, optimized versions

00:07:30.166 --> 00:07:32.356
of LZ4 for fast compression.

00:07:32.836 --> 00:07:35.146
Of course we have ZLIB with the

00:07:35.146 --> 00:07:36.776
optimized ZLIB decoder, and our

00:07:36.776 --> 00:07:39.196
very own LZFSE, which compresses

00:07:39.196 --> 00:07:41.416
slightly more than ZLIB but much

00:07:41.416 --> 00:07:41.776
faster.

00:07:42.796 --> 00:07:44.366
And last year we open sourced

00:07:44.536 --> 00:07:46.226
LZFSE; it's on GitHub.

00:07:47.136 --> 00:07:49.316
Okay. The API now.

00:07:49.866 --> 00:07:51.626
That, too, API's in the

00:07:51.626 --> 00:07:52.286
compression.

00:07:52.686 --> 00:07:54.116
The first one is a buffer API.

00:07:54.116 --> 00:07:55.166
So that's when you have the

00:07:55.166 --> 00:07:57.986
entire data to compress.

00:07:57.986 --> 00:07:59.106
And so you have a buffer with

00:07:59.106 --> 00:08:00.246
the data to compress, you just

00:08:00.246 --> 00:08:02.406
call one function, provide the

00:08:02.406 --> 00:08:03.456
output buffer, and you will get

00:08:03.456 --> 00:08:04.986
the output in one single code.

00:08:05.146 --> 00:08:06.476
That's good for encode and

00:08:06.476 --> 00:08:06.856
decode.

00:08:07.226 --> 00:08:09.306
And if the data is huge or you

00:08:09.306 --> 00:08:12.086
get it in small pieces, you want

00:08:12.086 --> 00:08:13.086
to use a stream API.

00:08:13.926 --> 00:08:15.606
In that case you will create a

00:08:15.606 --> 00:08:17.806
stream object and send data

00:08:17.806 --> 00:08:19.386
inside and get smaller data

00:08:19.386 --> 00:08:19.916
outside.

00:08:19.916 --> 00:08:20.776
And you will call that

00:08:20.776 --> 00:08:22.076
repeatedly until the entire

00:08:22.076 --> 00:08:23.606
stream is processed.

00:08:25.436 --> 00:08:27.076
And this, what we have, what's

00:08:27.076 --> 00:08:29.316
new, we added a compression tool

00:08:29.316 --> 00:08:29.986
command line.

00:08:30.866 --> 00:08:33.145
So you can compress with

00:08:33.145 --> 00:08:34.296
Compression from the command

00:08:34.296 --> 00:08:34.515
line.

00:08:35.655 --> 00:08:37.025
Okay. That's for Compression.

00:08:37.025 --> 00:08:39.046
Now let's switch to BNNS, Basic

00:08:39.046 --> 00:08:40.366
Neural Network Subroutines.

00:08:40.775 --> 00:08:42.456
As I said, this is the energy

00:08:42.456 --> 00:08:44.386
running of the CPU and

00:08:44.386 --> 00:08:45.916
supporting all the neural

00:08:45.916 --> 00:08:48.746
network and the machine learning

00:08:48.836 --> 00:08:49.446
libraries.

00:08:49.886 --> 00:08:52.686
So you use BNNS almost anytime.

00:08:52.686 --> 00:08:55.156
When you type on the keyboard or

00:08:55.156 --> 00:08:56.766
when you run face recognition,

00:08:57.486 --> 00:09:00.216
all these applications use BNNS.

00:09:00.216 --> 00:09:03.566
And BNNS provides lower-level

00:09:03.806 --> 00:09:04.846
functions for this.

00:09:05.006 --> 00:09:07.276
So that's convolutional layers,

00:09:07.456 --> 00:09:08.286
pooling layers.

00:09:08.856 --> 00:09:10.296
We also have fully connected

00:09:10.296 --> 00:09:10.686
layers.

00:09:10.686 --> 00:09:13.646
And these, we added separate

00:09:13.696 --> 00:09:15.536
activation layers doing just the

00:09:15.536 --> 00:09:16.516
activation function.

00:09:16.816 --> 00:09:18.236
And these layers also can do

00:09:18.236 --> 00:09:20.256
efficient conversion, data type

00:09:20.546 --> 00:09:21.026
conversions.

00:09:21.656 --> 00:09:24.466
Speaking of which, data types.

00:09:24.786 --> 00:09:27.276
So when you train a machine

00:09:27.276 --> 00:09:29.526
learning model, what you get is

00:09:29.596 --> 00:09:31.246
megabytes or hundreds of

00:09:31.246 --> 00:09:33.026
megabytes of data, usually

00:09:33.026 --> 00:09:34.656
32-bit floating point data for

00:09:34.656 --> 00:09:35.076
your model.

00:09:35.076 --> 00:09:37.076
That's the convolution weights,

00:09:37.076 --> 00:09:40.216
etc. It turns out you can

00:09:40.216 --> 00:09:41.936
convert these guys into smaller

00:09:41.936 --> 00:09:43.626
types, like 16-bit floating

00:09:43.626 --> 00:09:45.656
point or even 8-bit integer

00:09:45.956 --> 00:09:47.926
signed or unsigned and still get

00:09:47.926 --> 00:09:49.076
the same precision for your

00:09:49.076 --> 00:09:49.476
model.

00:09:50.056 --> 00:09:51.606
But, of course, when you convert

00:09:51.606 --> 00:09:53.356
32-bit floating point to 8-bit,

00:09:53.356 --> 00:09:54.566
your model is four times

00:09:54.566 --> 00:09:55.116
smaller.

00:09:55.656 --> 00:09:56.656
And that's something you ship

00:09:56.656 --> 00:09:57.316
with your app.

00:09:57.316 --> 00:09:59.906
So you want to consider that.

00:09:59.906 --> 00:10:02.316
This year we optimized BNNS to

00:10:02.316 --> 00:10:03.456
support these types.

00:10:03.846 --> 00:10:06.526
So this is what we support now,

00:10:06.806 --> 00:10:08.456
optimized in the convolutional

00:10:08.456 --> 00:10:08.746
layers.

00:10:08.746 --> 00:10:10.216
The green stuff is new.

00:10:11.186 --> 00:10:13.416
See, we added fp16 storage for

00:10:13.416 --> 00:10:15.126
input and weights and also int8.

00:10:16.206 --> 00:10:18.176
And this is what we support for

00:10:18.176 --> 00:10:19.846
the fully connected layers.

00:10:19.846 --> 00:10:21.656
So we're still accumulating to

00:10:21.656 --> 00:10:24.426
32 bits, but we can take 16-bit

00:10:24.426 --> 00:10:25.746
or even 8-bit inputs and

00:10:25.746 --> 00:10:26.126
weights.

00:10:27.096 --> 00:10:28.006
Now, for the activation

00:10:28.006 --> 00:10:30.546
functions, this is what we had

00:10:30.546 --> 00:10:30.946
last year.

00:10:30.946 --> 00:10:32.166
And this year we added a few

00:10:32.166 --> 00:10:33.696
more, including the most

00:10:33.696 --> 00:10:34.646
requested, Softmax.

00:10:34.646 --> 00:10:36.036
So now we have an optimized

00:10:36.036 --> 00:10:37.066
Softmax in BNNS.

00:10:37.986 --> 00:10:39.616
And if you set the activation

00:10:39.616 --> 00:10:41.716
function to identity, then you

00:10:41.716 --> 00:10:43.526
can change the input and output

00:10:44.116 --> 00:10:45.466
types to different combinations.

00:10:45.716 --> 00:10:47.236
This is what we support now.

00:10:47.236 --> 00:10:48.986
And you will get optimized type

00:10:48.986 --> 00:10:51.146
conversion from BNNS.

00:10:51.666 --> 00:10:53.386
Last but not least, performance.

00:10:54.296 --> 00:10:56.306
So we worked a lot with the Core

00:10:56.306 --> 00:10:58.166
ML team and the Vision and NLP

00:10:58.616 --> 00:11:00.786
teams to optimize what they

00:11:00.786 --> 00:11:01.736
really use a lot.

00:11:02.686 --> 00:11:05.496
So that includes convolutions

00:11:05.496 --> 00:11:08.676
with padding and Stride 1 and 2

00:11:08.676 --> 00:11:10.616
convolutions also and smaller

00:11:10.616 --> 00:11:10.976
kernels.

00:11:10.976 --> 00:11:13.296
Because the new neural networks

00:11:13.376 --> 00:11:14.556
have a lot of layers with

00:11:14.556 --> 00:11:16.716
smaller convolutions -- that's 3

00:11:16.716 --> 00:11:17.966
by 3 and 1 by 1.

00:11:18.066 --> 00:11:19.436
So we optimized these cases.

00:11:19.846 --> 00:11:21.276
And especially for the 3 by 3

00:11:21.276 --> 00:11:22.386
case, we have Winograd

00:11:22.386 --> 00:11:24.166
convolutions, which can be up to

00:11:24.166 --> 00:11:25.776
four times faster than the

00:11:25.776 --> 00:11:27.086
reference implementation.

00:11:27.326 --> 00:11:29.056
And that's it for BNNS.

00:11:29.056 --> 00:11:31.046
So let me invite Steve on stage,

00:11:31.046 --> 00:11:32.186
and he will tell us everything

00:11:32.186 --> 00:11:32.796
about simd.

00:11:32.936 --> 00:11:33.216
Thank you.

00:11:34.516 --> 00:11:37.216
[ Applause ]

00:11:37.716 --> 00:11:38.576
>> Thanks very much, Eric.

00:11:38.766 --> 00:11:39.506
Thank you, everyone.

00:11:40.126 --> 00:11:41.416
As Eric said, my name's Steve.

00:11:41.416 --> 00:11:42.446
And today I'm going to talk to

00:11:42.446 --> 00:11:43.506
you a little bit about the simd

00:11:43.506 --> 00:11:43.836
module.

00:11:43.836 --> 00:11:44.636
I don't think I'll get to cover

00:11:44.636 --> 00:11:45.966
everything, but we'll cover

00:11:45.966 --> 00:11:46.206
some.

00:11:47.316 --> 00:11:50.366
So the simd module lives outside

00:11:50.366 --> 00:11:51.036
of Accelerate.

00:11:51.036 --> 00:11:52.536
It's a small collection of

00:11:52.536 --> 00:11:53.576
headers and user include.

00:11:53.836 --> 00:11:55.016
And it's a module you import in

00:11:55.016 --> 00:11:55.406
Swift.

00:11:55.956 --> 00:11:57.226
And there's sort of three main

00:11:57.226 --> 00:11:58.546
use cases that are going to

00:11:58.546 --> 00:11:59.406
drive you to use simd.

00:12:00.206 --> 00:12:01.906
The first is if you're doing 2

00:12:01.906 --> 00:12:04.986
by 2, 3 by 3, 4 by 4 vector and

00:12:04.986 --> 00:12:06.046
matrix arithmetic, the sort of

00:12:06.046 --> 00:12:07.156
thing that comes up all the time

00:12:07.156 --> 00:12:08.506
when doing graphics or geometry

00:12:08.506 --> 00:12:09.176
operations.

00:12:10.576 --> 00:12:12.436
It also provides a bigger set of

00:12:12.436 --> 00:12:13.596
vector times, both integer

00:12:13.596 --> 00:12:14.616
vectors and floating point

00:12:14.616 --> 00:12:16.726
vectors on lengths up to 64

00:12:16.726 --> 00:12:18.626
bytes for doing sort of general

00:12:18.626 --> 00:12:19.306
vector programming.

00:12:19.306 --> 00:12:21.226
It lets you target all the

00:12:21.226 --> 00:12:22.446
architectures we support on all

00:12:22.446 --> 00:12:24.666
the platforms we support pretty

00:12:24.666 --> 00:12:25.046
easily.

00:12:25.046 --> 00:12:26.216
And you can write one piece of

00:12:26.216 --> 00:12:27.636
code that gets you good vector

00:12:27.836 --> 00:12:28.796
code gen for all of those

00:12:28.796 --> 00:12:29.436
architectures.

00:12:30.616 --> 00:12:31.966
The last reason you'll use simd

00:12:32.656 --> 00:12:34.416
is that it's a great set of

00:12:34.416 --> 00:12:35.686
types and operations for

00:12:35.686 --> 00:12:37.676
interoperating between all of

00:12:37.676 --> 00:12:39.636
the various things that do 3 by

00:12:39.636 --> 00:12:41.096
3, 4 by 4 operations on the

00:12:41.096 --> 00:12:41.416
platform.

00:12:41.416 --> 00:12:42.486
So things like SceneKit,

00:12:42.686 --> 00:12:45.556
SpriteKit, ARKit, Vision -- all

00:12:45.556 --> 00:12:46.556
those different things you have

00:12:46.806 --> 00:12:48.406
lots of matrices and vectors

00:12:48.406 --> 00:12:49.076
flying around.

00:12:49.516 --> 00:12:51.246
And the simd types are a great

00:12:51.246 --> 00:12:52.126
set of things to use with that.

00:12:52.526 --> 00:12:54.136
I should say that SpriteKit in

00:12:54.136 --> 00:12:55.166
particular -- or SceneKit in

00:12:55.166 --> 00:12:56.336
particular added a bunch of new

00:12:56.336 --> 00:12:56.946
stuff this year.

00:12:56.946 --> 00:12:57.906
So check out their session.

00:12:57.906 --> 00:12:59.006
There's some nice stuff for

00:12:59.006 --> 00:12:59.796
working with simd there.

00:12:59.796 --> 00:13:01.346
I'm going to show you a few

00:13:01.346 --> 00:13:02.706
examples of what you can do.

00:13:04.296 --> 00:13:04.966
So let's say you want to

00:13:04.966 --> 00:13:06.756
multiply a three-dimensional

00:13:06.756 --> 00:13:07.726
matrix by a vector.

00:13:07.986 --> 00:13:09.616
You can do that using BLAS,

00:13:09.696 --> 00:13:10.666
which Eric talked about earlier,

00:13:11.176 --> 00:13:11.826
looks like this.

00:13:12.376 --> 00:13:15.776
And this is fine, but BLAS takes

00:13:15.826 --> 00:13:17.106
all the parameters just as raw

00:13:17.106 --> 00:13:17.556
pointers.

00:13:17.586 --> 00:13:19.066
So we have to tell it these are

00:13:19.066 --> 00:13:20.106
the dimensions of the matrix,

00:13:20.106 --> 00:13:20.906
these are the dimensions of the

00:13:20.906 --> 00:13:22.296
vector, this is how the memory's

00:13:22.296 --> 00:13:22.706
laid out.

00:13:23.016 --> 00:13:23.706
There's a lot of other

00:13:23.706 --> 00:13:24.796
information we have to pass,

00:13:25.826 --> 00:13:26.706
which both makes the code a

00:13:26.706 --> 00:13:28.656
little harder to write and it

00:13:28.656 --> 00:13:29.676
makes it harder to read.

00:13:29.976 --> 00:13:31.476
In you're not fluent with BLAS

00:13:31.476 --> 00:13:33.146
already, this last line here,

00:13:33.146 --> 00:13:34.266
it's not really obvious that

00:13:34.266 --> 00:13:35.206
that's doing a matrix vector

00:13:35.206 --> 00:13:35.596
product.

00:13:36.056 --> 00:13:37.066
So we'd like to have something

00:13:37.066 --> 00:13:38.306
simpler than that.

00:13:39.056 --> 00:13:40.206
We could also write this with

00:13:40.206 --> 00:13:40.626
GLKit.

00:13:41.136 --> 00:13:42.796
GLKit makes it considerably

00:13:42.796 --> 00:13:43.156
nicer.

00:13:43.516 --> 00:13:44.966
We have dedicated types for a

00:13:44.966 --> 00:13:46.496
three-dimensional matrix and for

00:13:46.496 --> 00:13:47.956
three-dimensional vector; we

00:13:47.956 --> 00:13:48.366
call this

00:13:48.366 --> 00:13:50.696
GLKMatrix3MultiplyVector3

00:13:50.696 --> 00:13:51.146
function.

00:13:51.676 --> 00:13:52.486
It's pretty clear that's a

00:13:52.486 --> 00:13:53.196
multiplication.

00:13:53.536 --> 00:13:55.416
But we can make this even nicer

00:13:55.786 --> 00:13:56.246
using simd.

00:13:56.246 --> 00:13:58.176
This is what it looks like in

00:13:58.176 --> 00:13:58.496
simd.

00:13:59.016 --> 00:14:00.786
Okay? Totally explicit that it's

00:14:00.786 --> 00:14:01.776
diagonal matrix.

00:14:02.176 --> 00:14:03.686
And multiply a matrix by a

00:14:03.686 --> 00:14:05.006
vector is just using the

00:14:05.006 --> 00:14:05.936
multiplication operator.

00:14:06.356 --> 00:14:07.936
It's really nice, it's really

00:14:07.936 --> 00:14:09.576
simple, and it's also really

00:14:09.576 --> 00:14:09.796
fast.

00:14:10.536 --> 00:14:12.046
All of simd for the most part is

00:14:12.046 --> 00:14:13.296
implemented as header inlines.

00:14:13.616 --> 00:14:14.786
So this is just going to give me

00:14:14.786 --> 00:14:16.626
three vector multiplications on

00:14:16.626 --> 00:14:17.236
my CPU.

00:14:17.646 --> 00:14:18.706
It's really fast, there's no

00:14:18.706 --> 00:14:20.056
call overhead, there's no

00:14:20.056 --> 00:14:21.066
parameter checking, there's no

00:14:21.066 --> 00:14:21.396
nothing.

00:14:21.726 --> 00:14:23.516
I get just nice simple code gen

00:14:23.516 --> 00:14:23.806
from this.

00:14:24.526 --> 00:14:25.496
So that's a Swift example.

00:14:26.076 --> 00:14:27.706
The next example that I'm going

00:14:27.706 --> 00:14:28.896
to show you will be in C.

00:14:30.286 --> 00:14:31.906
Here I'm going to show you an

00:14:31.906 --> 00:14:33.426
example of how you can use simd

00:14:33.426 --> 00:14:34.206
to write vector code.

00:14:34.806 --> 00:14:35.806
So let's say that we want to

00:14:35.806 --> 00:14:37.506
compute a logistic curve with a

00:14:37.506 --> 00:14:39.266
given midpoint and maximum

00:14:39.266 --> 00:14:39.666
slope.

00:14:40.176 --> 00:14:41.236
This is a function that comes up

00:14:41.316 --> 00:14:43.006
all the time in sort of

00:14:43.006 --> 00:14:43.866
computational mathematics,

00:14:43.866 --> 00:14:44.856
especially machine learning.

00:14:44.856 --> 00:14:46.216
So this is a useful function to

00:14:46.216 --> 00:14:46.996
be able to optimize.

00:14:46.996 --> 00:14:47.936
We care a lot about this.

00:14:48.876 --> 00:14:50.126
And what I've put in the comment

00:14:50.286 --> 00:14:51.406
in the body of the function here

00:14:52.176 --> 00:14:54.286
is sort of a typical scalar

00:14:54.286 --> 00:14:55.216
implementation, just

00:14:55.216 --> 00:14:56.496
mathematically what this looks

00:14:56.496 --> 00:14:56.696
like.

00:14:56.906 --> 00:14:59.366
But we want to compute this on

00:14:59.576 --> 00:15:01.236
16 floating point values

00:15:01.236 --> 00:15:02.456
simultaneously because we can

00:15:02.456 --> 00:15:03.656
get better efficiency that way.

00:15:03.896 --> 00:15:05.946
So that's what this simd float16

00:15:05.946 --> 00:15:06.896
type in the function is --

00:15:06.926 --> 00:15:08.226
that's just a vector of 16

00:15:08.226 --> 00:15:08.716
floats.

00:15:09.376 --> 00:15:10.766
And we're going to see if we can

00:15:10.766 --> 00:15:11.676
implement the body of this

00:15:11.676 --> 00:15:11.966
function.

00:15:12.676 --> 00:15:14.746
So vector code is complicated.

00:15:14.746 --> 00:15:16.806
I just break this apart into

00:15:16.806 --> 00:15:18.886
three pieces so we can write

00:15:18.886 --> 00:15:20.126
each one of them individually.

00:15:20.486 --> 00:15:21.496
Let's start with this first

00:15:21.496 --> 00:15:22.876
section, the linear section.

00:15:23.366 --> 00:15:25.976
So we're just subtracting a

00:15:25.976 --> 00:15:26.756
scalar from a vector.

00:15:26.756 --> 00:15:27.556
We're going to subtract it from

00:15:27.556 --> 00:15:28.676
every lane of the vector, and

00:15:28.676 --> 00:15:29.676
then we're going to multiply by

00:15:29.676 --> 00:15:30.076
a scalar.

00:15:30.686 --> 00:15:32.576
What does that look like in

00:15:33.496 --> 00:15:34.136
simd?

00:15:34.206 --> 00:15:35.406
We just subtract and we

00:15:35.406 --> 00:15:35.896
multiply.

00:15:35.896 --> 00:15:36.846
It's really, really simple.

00:15:37.366 --> 00:15:38.756
This works in C, it works in

00:15:38.756 --> 00:15:40.426
Swift, it works in C++, it works

00:15:40.426 --> 00:15:41.146
in Objective-C.

00:15:41.536 --> 00:15:42.286
It's really nice.

00:15:42.286 --> 00:15:43.506
It looks a lot like shader

00:15:43.506 --> 00:15:44.476
programming if you've done any

00:15:44.476 --> 00:15:44.776
of that.

00:15:45.766 --> 00:15:47.176
And this last piece down here

00:15:48.176 --> 00:15:49.446
where we take a reciprocal,

00:15:49.556 --> 00:15:51.326
again, the same thing, we just

00:15:51.406 --> 00:15:52.866
write code that looks like the

00:15:52.866 --> 00:15:53.016
math.

00:15:53.016 --> 00:15:54.196
It looks just like the scaler

00:15:54.196 --> 00:15:55.136
code, it looks just like the

00:15:55.136 --> 00:15:55.736
math we're doing.

00:15:56.876 --> 00:15:57.906
What about this middle section?

00:15:58.336 --> 00:15:58.996
That's a little bit more

00:15:58.996 --> 00:15:59.536
complicated.

00:15:59.536 --> 00:16:01.206
What we want to do here is for

00:16:01.296 --> 00:16:02.456
every element in the vector, we

00:16:02.456 --> 00:16:03.776
want to compute the exponential

00:16:03.776 --> 00:16:05.536
function of that and put that in

00:16:05.536 --> 00:16:06.936
the corresponding element of the

00:16:06.936 --> 00:16:07.586
result factor.

00:16:07.836 --> 00:16:10.056
We can do that with a for loop.

00:16:10.716 --> 00:16:12.806
And this is fine, this works.

00:16:13.686 --> 00:16:14.986
But we have a nice new feature

00:16:14.986 --> 00:16:16.836
for you this year, which is that

00:16:17.406 --> 00:16:18.606
basically all of the math

00:16:18.606 --> 00:16:20.206
functions just work on vectors

00:16:20.206 --> 00:16:21.116
of floats and doubles.

00:16:21.116 --> 00:16:22.876
So any length of vector, float,

00:16:22.876 --> 00:16:23.226
and double.

00:16:23.706 --> 00:16:25.236
You can call XPath, you can call

00:16:25.236 --> 00:16:26.406
sine, you can call cosine,

00:16:26.576 --> 00:16:26.866
whatever.

00:16:27.146 --> 00:16:28.096
The math functions, they just

00:16:28.096 --> 00:16:28.536
work on them.

00:16:28.916 --> 00:16:30.176
It's a really nice convenience

00:16:30.176 --> 00:16:31.506
feature when you're writing this

00:16:31.506 --> 00:16:31.966
kind of code.

00:16:32.736 --> 00:16:35.086
And this is going to target the

00:16:35.086 --> 00:16:38.086
NEON extensions when I write for

00:16:38.086 --> 00:16:38.356
ARM.

00:16:38.456 --> 00:16:39.846
And when I compile for Intel,

00:16:39.846 --> 00:16:41.206
it's going to target AVX and

00:16:41.206 --> 00:16:41.576
SSE.

00:16:41.806 --> 00:16:43.206
So I'm going to get fast code on

00:16:43.206 --> 00:16:43.906
all the platforms.

00:16:43.906 --> 00:16:44.726
This is really nice.

00:16:45.306 --> 00:16:46.856
We have one other big new

00:16:46.856 --> 00:16:47.986
feature this year that a lot of

00:16:47.986 --> 00:16:49.846
people have asked for, which is

00:16:49.846 --> 00:16:50.546
quaternions.

00:16:50.986 --> 00:16:52.076
I'm going to give you a real

00:16:52.076 --> 00:16:53.036
quick introduction to them.

00:16:53.816 --> 00:16:55.816
Just that quaternions extend the

00:16:55.816 --> 00:16:57.666
complex numbers in the same way

00:16:57.666 --> 00:16:58.716
the complex numbers extend the

00:16:58.716 --> 00:16:59.176
reals.

00:17:00.676 --> 00:17:02.196
So complex number, you might

00:17:02.196 --> 00:17:03.796
remember from school, has a real

00:17:03.796 --> 00:17:06.366
part and an imaginary part.

00:17:06.915 --> 00:17:10.066
Quaternions have a real part and

00:17:10.066 --> 00:17:12.016
they have three imaginary parts;

00:17:12.126 --> 00:17:12.836
sometimes you call that the

00:17:12.836 --> 00:17:13.376
vector part.

00:17:13.776 --> 00:17:15.506
My mom always said that if

00:17:15.506 --> 00:17:16.756
having one square root of minus

00:17:16.756 --> 00:17:17.896
one is good, having three square

00:17:17.896 --> 00:17:18.856
roots of minus one must be

00:17:18.856 --> 00:17:19.146
great.

00:17:20.705 --> 00:17:21.486
She was a smart lady.

00:17:21.486 --> 00:17:24.996
You should listen to her.

00:17:25.246 --> 00:17:26.425
There's a lot of fascinating

00:17:26.425 --> 00:17:27.526
mathematical structure about the

00:17:27.526 --> 00:17:28.876
quaternions -- we don't really

00:17:28.876 --> 00:17:29.436
care about that.

00:17:29.956 --> 00:17:30.926
We're interested in one thing.

00:17:32.026 --> 00:17:33.066
Quaternions have a notion of

00:17:33.066 --> 00:17:34.196
length that's just like the

00:17:34.196 --> 00:17:35.016
complex numbers.

00:17:35.016 --> 00:17:36.166
You sum the squares of the

00:17:36.166 --> 00:17:37.006
components and you take the

00:17:37.006 --> 00:17:38.396
square root, that's the length

00:17:38.396 --> 00:17:38.886
of a quaternion.

00:17:39.796 --> 00:17:41.996
Quaternions of length one, we

00:17:41.996 --> 00:17:43.886
call those unit quaternions, and

00:17:43.886 --> 00:17:45.556
they have this really nice

00:17:45.556 --> 00:17:48.286
property, which is that you can

00:17:48.286 --> 00:17:50.126
use them to represent rotations

00:17:50.456 --> 00:17:51.726
in three-dimensional space.

00:17:52.716 --> 00:17:53.796
That's all we care about.

00:17:53.796 --> 00:17:55.156
Forget about all the other math

00:17:55.156 --> 00:17:57.816
that I just mentioned.

00:17:57.896 --> 00:17:59.386
So I'll show you a quick code

00:17:59.386 --> 00:18:00.136
example of that.

00:18:00.566 --> 00:18:01.606
Say we have a vector that's a

00:18:01.606 --> 00:18:02.646
vector in the XY plane.

00:18:03.846 --> 00:18:05.126
And let's build a quaternion.

00:18:05.476 --> 00:18:06.336
This is a quaternion that

00:18:06.336 --> 00:18:08.146
represents a rotation around the

00:18:08.146 --> 00:18:08.956
y-axis.

00:18:10.526 --> 00:18:12.666
Quaternions act on vectors --

00:18:12.666 --> 00:18:14.006
that's the simd act function.

00:18:14.806 --> 00:18:16.356
It's not multiplication.

00:18:16.356 --> 00:18:18.236
When you multiply a vector by a

00:18:18.236 --> 00:18:20.666
matrix -- when you rotate a

00:18:20.666 --> 00:18:21.986
vector by a matrix, you just

00:18:21.986 --> 00:18:22.686
multiply it.

00:18:22.926 --> 00:18:23.886
With quaternions it's called an

00:18:23.886 --> 00:18:24.286
action.

00:18:24.706 --> 00:18:26.356
And we don't care too much about

00:18:26.356 --> 00:18:28.126
the details of that, but that's

00:18:28.126 --> 00:18:29.146
why this is the act function.

00:18:29.506 --> 00:18:31.106
So this is a nice way to

00:18:31.106 --> 00:18:32.246
represent rotations.

00:18:32.686 --> 00:18:33.836
There's a lot of other ways to

00:18:33.836 --> 00:18:34.856
represent rotations.

00:18:35.056 --> 00:18:36.376
Why do you want to use this one

00:18:36.376 --> 00:18:37.346
and when do you want to use this

00:18:37.346 --> 00:18:37.506
one?

00:18:37.596 --> 00:18:40.116
You know, you might use matrices

00:18:40.116 --> 00:18:41.346
instead, you might use Euler

00:18:41.346 --> 00:18:42.836
angles or yaw/pitch/roll, you

00:18:42.836 --> 00:18:43.916
can use axis and angle

00:18:43.916 --> 00:18:44.496
representation.

00:18:44.496 --> 00:18:45.486
There's lots of choices.

00:18:45.946 --> 00:18:47.626
Quaternions are an especially

00:18:47.626 --> 00:18:48.856
good choice for a certain class

00:18:48.856 --> 00:18:50.106
of operations that I'm going to

00:18:50.106 --> 00:18:51.586
tell you about.

00:18:52.336 --> 00:18:53.786
The first nice thing about

00:18:53.786 --> 00:18:54.716
quaternions is they take less

00:18:54.716 --> 00:18:55.746
memory than matrices do.

00:18:56.016 --> 00:18:59.316
This is nice and it's great, but

00:18:59.316 --> 00:19:00.346
that's not really why we want to

00:19:00.346 --> 00:19:01.486
use them.

00:19:02.066 --> 00:19:02.876
The better thing about

00:19:02.876 --> 00:19:04.616
quaternions is that while they

00:19:04.616 --> 00:19:06.936
are not especially fast to do a

00:19:06.936 --> 00:19:08.166
rotation with, you can see here

00:19:08.166 --> 00:19:09.476
they're about a third the speed

00:19:09.476 --> 00:19:10.696
of using matrices to actually

00:19:11.056 --> 00:19:11.956
compute a rotation.

00:19:13.416 --> 00:19:15.086
When you want to do a sequence

00:19:15.086 --> 00:19:16.206
of operations, when you want to

00:19:16.206 --> 00:19:17.596
combine rotations or you want to

00:19:17.596 --> 00:19:18.766
interpolate rotations, you want

00:19:18.766 --> 00:19:19.486
to do anything like that,

00:19:19.846 --> 00:19:21.836
they're the most natural setting

00:19:21.836 --> 00:19:23.016
to do those operations in.

00:19:23.476 --> 00:19:24.806
So when we want to multiply two

00:19:24.806 --> 00:19:25.916
rotations together, you can see

00:19:25.916 --> 00:19:27.846
quaternions are about 30% faster

00:19:28.046 --> 00:19:29.196
than vectors and matrices.

00:19:29.576 --> 00:19:31.316
But they also let us do things

00:19:31.316 --> 00:19:32.366
that are hard to do with

00:19:32.366 --> 00:19:32.856
matrices.

00:19:32.856 --> 00:19:35.336
So let's say we want to

00:19:35.336 --> 00:19:36.946
interpolate between two

00:19:36.946 --> 00:19:38.106
different rotated coordinate

00:19:38.106 --> 00:19:38.556
frames.

00:19:39.256 --> 00:19:40.536
That's a little subtle with

00:19:40.536 --> 00:19:42.036
matrices, but it's really

00:19:42.036 --> 00:19:43.106
natural with quaternions.

00:19:43.536 --> 00:19:45.026
And the reason it's natural has

00:19:45.026 --> 00:19:46.886
to do with this sphere that I've

00:19:46.886 --> 00:19:47.806
drawn over on the side of the

00:19:47.806 --> 00:19:48.306
screen here.

00:19:48.426 --> 00:19:49.336
You might say, "Well, why are

00:19:49.336 --> 00:19:50.836
you drawing rotations on a

00:19:50.836 --> 00:19:51.166
sphere?"

00:19:51.876 --> 00:19:52.766
There's a good reason for that.

00:19:53.606 --> 00:19:55.746
Quaternions you can think of as

00:19:55.846 --> 00:19:57.066
points on the four-dimensional

00:19:57.066 --> 00:19:57.796
projective sphere.

00:19:58.866 --> 00:20:00.586
And that sounds complicated and

00:20:00.586 --> 00:20:02.926
mathematical, and it is, but

00:20:03.166 --> 00:20:04.816
it's the natural space of

00:20:04.816 --> 00:20:05.956
rotations for three-dimensional

00:20:05.956 --> 00:20:06.376
space.

00:20:06.826 --> 00:20:08.406
So when I do operations on the

00:20:08.406 --> 00:20:09.966
surface of this sphere, that

00:20:09.966 --> 00:20:11.446
corresponds exactly to your

00:20:11.446 --> 00:20:12.816
natural intuition for what

00:20:12.816 --> 00:20:13.846
should happen with rotations.

00:20:13.946 --> 00:20:16.576
So for instance, if we want to

00:20:16.576 --> 00:20:17.726
interpolate between them, we

00:20:17.726 --> 00:20:19.666
just interpolate along a great

00:20:19.666 --> 00:20:20.766
circle on the sphere -- that's

00:20:20.766 --> 00:20:22.196
this simd slerp function that

00:20:22.196 --> 00:20:23.376
stands for spherical linear

00:20:23.376 --> 00:20:24.056
interpolation.

00:20:24.676 --> 00:20:26.656
And that's really easy to use

00:20:26.656 --> 00:20:27.926
and it does exactly what you

00:20:27.926 --> 00:20:28.266
want.

00:20:28.636 --> 00:20:30.346
If we have a whole sequence of

00:20:30.386 --> 00:20:31.416
points that we want to

00:20:31.416 --> 00:20:33.596
interpolate between, we could

00:20:33.596 --> 00:20:35.496
call slerp repeatedly to

00:20:35.496 --> 00:20:36.786
interpolate between them, but

00:20:37.056 --> 00:20:38.946
that will have a noticeable jump

00:20:38.946 --> 00:20:40.726
a little bit in the rotation

00:20:40.826 --> 00:20:41.836
when you get to corners.

00:20:42.886 --> 00:20:45.136
Instead we can use the simd

00:20:45.136 --> 00:20:46.616
spline function to get a

00:20:46.616 --> 00:20:48.566
completely smooth interpolation

00:20:48.566 --> 00:20:49.786
using a whole sequence of

00:20:49.786 --> 00:20:50.936
rotated coordinate frames.

00:20:51.626 --> 00:20:52.806
There's a ton of other

00:20:53.016 --> 00:20:54.796
operations like this in the simd

00:20:54.796 --> 00:20:56.746
headers that you can use to do

00:20:56.746 --> 00:20:58.616
these types of operations.

00:20:58.936 --> 00:20:59.846
If you want to work with

00:20:59.846 --> 00:21:01.786
rotations, the API is really

00:21:01.786 --> 00:21:02.356
pretty full.

00:21:02.356 --> 00:21:03.666
I encourage you to check it out.

00:21:04.176 --> 00:21:05.326
And as I said, this was one of

00:21:05.326 --> 00:21:06.306
the most requested features from

00:21:06.306 --> 00:21:06.846
developers.

00:21:06.846 --> 00:21:08.726
So I encourage you to file bugs

00:21:08.726 --> 00:21:10.016
to say, "Hey, can you guys add

00:21:10.016 --> 00:21:11.016
this other thing as well?"

00:21:11.016 --> 00:21:12.016
We're really responsive to that.

00:21:12.016 --> 00:21:13.206
We love to get feature requests

00:21:13.206 --> 00:21:13.846
from our users.

00:21:14.656 --> 00:21:16.206
With that, I'm going to turn you

00:21:16.206 --> 00:21:18.346
over to Jonathan Hogg, who's

00:21:18.346 --> 00:21:19.426
going to tell you all about

00:21:19.586 --> 00:21:21.366
really, really big matrices.

00:21:22.516 --> 00:21:26.946
[ Applause ]

00:21:27.446 --> 00:21:29.846
>> Hi. So you've heard from

00:21:29.846 --> 00:21:32.206
Steve about simd, which is

00:21:32.206 --> 00:21:33.196
really great for very, very

00:21:33.196 --> 00:21:34.146
small matrices.

00:21:34.336 --> 00:21:35.896
And I'm going to tell you in a

00:21:35.896 --> 00:21:37.386
little while about very big

00:21:37.386 --> 00:21:38.226
matrices.

00:21:38.516 --> 00:21:39.856
But first I'm going to tell you

00:21:39.856 --> 00:21:40.886
about BLAS and LAPACK.

00:21:41.396 --> 00:21:43.346
These are libraries for handling

00:21:43.726 --> 00:21:44.406
dense matrices.

00:21:44.406 --> 00:21:46.436
So you can get up to 30,000 or

00:21:46.436 --> 00:21:48.766
40,000 rows of columns here on a

00:21:48.766 --> 00:21:49.266
MacBook.

00:21:50.846 --> 00:21:51.516
What do we have?

00:21:52.816 --> 00:21:55.046
So BLAS stands for Basic Linear

00:21:55.046 --> 00:21:56.886
Algebra Subroutines, and these

00:21:56.886 --> 00:21:59.056
do basic operations on matrices

00:21:59.056 --> 00:21:59.716
and vectors.

00:22:00.646 --> 00:22:02.436
We have BLAS 1, which does

00:22:02.556 --> 00:22:03.996
vector vector operations.

00:22:04.176 --> 00:22:05.846
And then we move through BLAS 2

00:22:05.846 --> 00:22:08.056
for matrix vector, to BLAS 3 for

00:22:08.056 --> 00:22:09.356
matrix matrix operations.

00:22:09.746 --> 00:22:10.546
And you've already seen from

00:22:10.546 --> 00:22:13.046
Eric that we can be 100 times

00:22:13.046 --> 00:22:14.546
faster on the matrix matrix

00:22:14.546 --> 00:22:16.236
multiply than your simple loop.

00:22:17.256 --> 00:22:18.236
If you want to do things more

00:22:18.236 --> 00:22:20.036
complicated than this, then we

00:22:20.036 --> 00:22:21.186
have LAPACK.

00:22:21.906 --> 00:22:22.876
These do your matrix

00:22:22.876 --> 00:22:24.386
factorizations, your linear

00:22:24.386 --> 00:22:26.646
solves, your find Eigenvalues,

00:22:26.736 --> 00:22:28.486
Eigenvectors, SVD's -- pretty

00:22:28.946 --> 00:22:31.066
much everything you want to do.

00:22:32.896 --> 00:22:33.866
That's all I'm going to tell you

00:22:33.866 --> 00:22:35.646
about dense matrices because we

00:22:35.646 --> 00:22:38.286
want to actually talk about

00:22:38.956 --> 00:22:39.376
sparse matrices.

00:22:39.376 --> 00:22:41.406
What is a sparse matrix?

00:22:44.136 --> 00:22:46.516
So James Wilkinson was one of

00:22:46.516 --> 00:22:47.606
the founding fathers of

00:22:47.606 --> 00:22:48.996
computational linear algebra.

00:22:49.226 --> 00:22:51.646
This was his definition, "Sparse

00:22:51.646 --> 00:22:52.896
matrix is any one where

00:22:52.896 --> 00:22:56.076
exploiting zeros is useful to

00:22:57.196 --> 00:22:57.296
us."

00:22:57.516 --> 00:22:59.046
Let's see what sparse matrix

00:22:59.046 --> 00:23:00.446
actually looks like.

00:23:01.416 --> 00:23:03.776
So here are two sparse matrices.

00:23:03.876 --> 00:23:05.096
One's actually the Cholesky

00:23:05.096 --> 00:23:06.786
factorization of the other.

00:23:07.416 --> 00:23:09.266
And each pixel here represents

00:23:09.266 --> 00:23:10.396
multiple non-zeros.

00:23:11.366 --> 00:23:14.826
Where they are white, all of the

00:23:14.826 --> 00:23:16.936
entries behind that pixel are

00:23:16.936 --> 00:23:17.306
zero.

00:23:17.836 --> 00:23:19.746
Where there's blue, that means

00:23:19.746 --> 00:23:21.066
at least one non-zero's present.

00:23:21.066 --> 00:23:22.756
So you can see these matrices

00:23:23.026 --> 00:23:24.156
are mostly empty.

00:23:25.246 --> 00:23:26.936
In fact, if you were to store

00:23:26.936 --> 00:23:28.896
this as a dense matrix, it's

00:23:28.896 --> 00:23:30.226
about 30,000 by 30,000.

00:23:30.226 --> 00:23:32.696
So it takes us 6.5 gigabytes.

00:23:33.396 --> 00:23:34.446
If we store it as a sparse

00:23:34.446 --> 00:23:37.706
matrix, we require 260 times

00:23:38.016 --> 00:23:40.366
less storage -- only 26

00:23:40.366 --> 00:23:41.026
megabytes.

00:23:41.386 --> 00:23:43.106
If we wanted to multiply this

00:23:43.106 --> 00:23:46.246
matrix by a vector, we'd require

00:23:46.436 --> 00:23:48.586
almost 200 times fewer floating

00:23:48.586 --> 00:23:49.746
point operations.

00:23:50.656 --> 00:23:51.716
But if we want to do something

00:23:51.716 --> 00:23:53.906
more complicated like factorize

00:23:53.906 --> 00:23:58.246
this matrix, things get better,

00:23:58.776 --> 00:24:00.596
at least in the floating point.

00:24:00.966 --> 00:24:03.616
We require 2,000 times fewer

00:24:03.616 --> 00:24:05.026
floating point operations to

00:24:05.246 --> 00:24:06.506
factorize this matrix.

00:24:07.496 --> 00:24:09.086
And the dense matrix is still

00:24:09.086 --> 00:24:10.466
the same size, the factor's

00:24:10.466 --> 00:24:11.266
filled in a bit.

00:24:11.806 --> 00:24:13.136
It's slightly less sparse than

00:24:13.136 --> 00:24:15.516
it was, so we're only 30 times

00:24:15.516 --> 00:24:16.596
better on the storage.

00:24:17.446 --> 00:24:19.086
Now, to drive that point home,

00:24:19.256 --> 00:24:20.666
we've set up a little race.

00:24:21.196 --> 00:24:22.746
We decided we'd run the sparse

00:24:22.746 --> 00:24:25.556
solver on a Watch and we put it

00:24:25.686 --> 00:24:27.666
up against our best dense matrix

00:24:27.666 --> 00:24:30.286
solver from LAPACK on a Macbook

00:24:30.356 --> 00:24:30.686
Air.

00:24:31.506 --> 00:24:32.806
And this is what happened.

00:24:33.466 --> 00:24:36.896
Now, this is running at five

00:24:36.896 --> 00:24:38.816
times real time.

00:24:39.136 --> 00:24:40.656
And you can see that the Watch

00:24:40.916 --> 00:24:43.756
is finished in only 16 seconds.

00:24:43.756 --> 00:24:44.946
You got to remember while

00:24:44.946 --> 00:24:46.966
watching this that the floating

00:24:46.966 --> 00:24:48.176
point throughput on the MacBook

00:24:48.176 --> 00:24:50.466
Air is about 50 times that

00:24:50.466 --> 00:24:51.736
available on the Watch.

00:24:52.516 --> 00:24:56.546
[ Laughter ]

00:24:57.046 --> 00:24:59.076
Now, there's actually two phases

00:24:59.076 --> 00:25:00.046
to this factorization in the

00:25:00.046 --> 00:25:00.626
sparse world.

00:25:00.986 --> 00:25:02.596
First we find where the

00:25:02.596 --> 00:25:03.476
positions of the non-zero

00:25:03.476 --> 00:25:05.036
entry's going to be -- that's

00:25:05.036 --> 00:25:05.886
the symbolic phase.

00:25:06.116 --> 00:25:07.966
Then we have a numeric phase

00:25:08.166 --> 00:25:09.056
which calculates the actual

00:25:09.056 --> 00:25:09.566
values.

00:25:10.646 --> 00:25:11.846
These times are just for the

00:25:11.846 --> 00:25:14.436
numeric phase, but the symbolic

00:25:14.436 --> 00:25:15.816
phase only takes about two

00:25:15.816 --> 00:25:16.926
seconds on the Watch.

00:25:17.286 --> 00:25:19.266
So rather than 16 seconds, you

00:25:19.266 --> 00:25:21.046
could say it takes about 18

00:25:21.046 --> 00:25:21.526
seconds.

00:25:21.526 --> 00:25:22.846
But if you're doing more than

00:25:22.846 --> 00:25:25.596
one factorization on the same

00:25:26.236 --> 00:25:27.676
pattern, you can skip that

00:25:27.676 --> 00:25:29.636
symbolic phase on the second and

00:25:29.636 --> 00:25:30.526
third factorization.

00:25:31.336 --> 00:25:32.506
Even if you include that, we're

00:25:32.506 --> 00:25:33.916
still more than 10 times faster

00:25:33.916 --> 00:25:35.266
than the Macbook due to dense

00:25:35.266 --> 00:25:36.006
computation.

00:25:37.336 --> 00:25:39.316
Hopefully I've now convinced you

00:25:39.586 --> 00:25:40.716
that it's worth using sparse

00:25:40.716 --> 00:25:41.346
matrices.

00:25:41.576 --> 00:25:43.296
So let's tell you how to

00:25:43.296 --> 00:25:44.616
actually define one.

00:25:45.436 --> 00:25:47.476
So here is a very, very small

00:25:47.476 --> 00:25:48.426
sparse matrix.

00:25:48.796 --> 00:25:49.596
You'll notice it's missing

00:25:49.596 --> 00:25:51.886
entries -- those are zero.

00:25:51.886 --> 00:25:54.896
We are going to store this using

00:25:54.896 --> 00:25:55.936
a standard format called

00:25:55.936 --> 00:25:57.496
compressed sparse column, which

00:25:57.496 --> 00:25:59.806
uses these three arrays.

00:26:01.146 --> 00:26:02.946
And we'll start off with the

00:26:02.946 --> 00:26:03.926
rowIndices rate.

00:26:04.186 --> 00:26:06.676
So let's put the row numbers

00:26:06.996 --> 00:26:08.156
onto our matrix.

00:26:09.166 --> 00:26:10.526
And we'll just copy those up

00:26:10.656 --> 00:26:11.816
into the rowIndices array.

00:26:12.226 --> 00:26:14.416
And let's do something similar

00:26:14.416 --> 00:26:15.206
for the values.

00:26:15.776 --> 00:26:17.846
You can see we got a one-to-one

00:26:17.846 --> 00:26:18.736
correspondence here.

00:26:19.316 --> 00:26:21.386
That first entry is in row zero

00:26:21.386 --> 00:26:22.386
and has value two.

00:26:23.796 --> 00:26:26.076
So let's just put on the

00:26:26.076 --> 00:26:28.256
positions of those entries in

00:26:28.256 --> 00:26:29.456
that rowIndices and values

00:26:29.456 --> 00:26:29.756
array.

00:26:30.656 --> 00:26:31.746
And the trick to compressed

00:26:31.746 --> 00:26:34.046
sparse column is that all these

00:26:34.046 --> 00:26:36.106
values have to occur in order of

00:26:36.106 --> 00:26:37.106
increasing column.

00:26:37.536 --> 00:26:38.966
All entries in column zero

00:26:38.966 --> 00:26:40.666
actually occur before those from

00:26:40.666 --> 00:26:41.686
column one.

00:26:41.896 --> 00:26:44.116
And then we get to the trick to

00:26:44.116 --> 00:26:45.556
this format, which is we're only

00:26:45.556 --> 00:26:47.876
going to store the position of

00:26:47.876 --> 00:26:50.846
the first entry in each column

00:26:51.826 --> 00:26:52.906
and one additional piece of

00:26:52.906 --> 00:26:55.346
information, the total number of

00:26:55.346 --> 00:26:57.446
entries in the matrix.

00:26:57.446 --> 00:26:58.886
That means that we know how long

00:26:58.886 --> 00:27:00.026
that last column is.

00:27:01.386 --> 00:27:02.986
If you've already using a sparse

00:27:02.986 --> 00:27:04.986
solver, you should probably have

00:27:04.986 --> 00:27:06.546
your data in either this format

00:27:06.856 --> 00:27:08.606
or a coordinate format and we

00:27:08.676 --> 00:27:11.076
provide a converter.

00:27:11.176 --> 00:27:12.846
To use it in Accelerate, we need

00:27:12.846 --> 00:27:14.556
to wrap it in some metadata,

00:27:15.036 --> 00:27:16.526
just telling it how many rows,

00:27:16.526 --> 00:27:17.396
how many columns.

00:27:17.756 --> 00:27:19.326
And we're going to say this one

00:27:19.326 --> 00:27:20.636
is an ordinary sparse matrix.

00:27:20.636 --> 00:27:21.996
I've got an example of an

00:27:21.996 --> 00:27:24.426
unordinary sparse matrix in a

00:27:24.426 --> 00:27:25.386
couple of slides.

00:27:26.136 --> 00:27:27.736
Now I've got my sparse matrix,

00:27:27.866 --> 00:27:30.226
what can I do with it?

00:27:31.426 --> 00:27:33.386
So you can do pretty much

00:27:33.386 --> 00:27:34.296
anything you'd expect.

00:27:34.296 --> 00:27:36.356
You can multiply a dense vector

00:27:36.356 --> 00:27:38.416
or a dense matrix by it; you can

00:27:38.506 --> 00:27:39.886
add two sparse matrices or

00:27:39.886 --> 00:27:41.376
sparse vectors together; you can

00:27:41.376 --> 00:27:43.516
permute the rows or columns; or

00:27:43.516 --> 00:27:45.076
you can find various useful

00:27:45.076 --> 00:27:46.046
matrix norms.

00:27:46.766 --> 00:27:47.996
All that functionality is

00:27:47.996 --> 00:27:49.476
provided by the Sparse BLAS,

00:27:49.776 --> 00:27:51.516
which we introduced a few years

00:27:51.516 --> 00:27:51.616
ago.

00:27:52.476 --> 00:27:53.536
So what's new this time?

00:27:54.756 --> 00:27:56.636
The ability to solve sparse

00:27:56.636 --> 00:28:00.846
systems, that is, given a matrix

00:28:00.846 --> 00:28:03.596
equation A times X equals B

00:28:03.676 --> 00:28:04.866
where we know the matrix A and

00:28:04.866 --> 00:28:07.376
to the right-hand side B, find

00:28:07.526 --> 00:28:09.366
that vector of unknowns X.

00:28:10.666 --> 00:28:12.656
So we got two approaches to this

00:28:12.656 --> 00:28:12.976
for you.

00:28:13.586 --> 00:28:15.636
The first is matrix

00:28:15.636 --> 00:28:16.166
factorization.

00:28:16.246 --> 00:28:17.686
This is exactly what happens in

00:28:17.686 --> 00:28:18.226
LAPACK.

00:28:19.086 --> 00:28:20.396
It's simple, it's accurate, it's

00:28:20.536 --> 00:28:21.616
easy to use.

00:28:22.446 --> 00:28:23.916
But mathematicians being

00:28:23.916 --> 00:28:25.396
mathematicians, they came up

00:28:25.516 --> 00:28:26.646
with a more complicated way of

00:28:26.646 --> 00:28:27.376
doing things.

00:28:28.696 --> 00:28:29.946
That's iterative methods.

00:28:29.946 --> 00:28:30.746
And I'll tell you a bit more

00:28:30.746 --> 00:28:31.546
about those later.

00:28:32.106 --> 00:28:35.536
So now a matrix factorization.

00:28:35.686 --> 00:28:37.196
For those of you who haven't

00:28:37.196 --> 00:28:39.096
come across this before, we want

00:28:39.096 --> 00:28:41.256
to take our green matrix on the

00:28:41.256 --> 00:28:42.926
left here and factorize it into

00:28:42.926 --> 00:28:44.166
the products of two triangular

00:28:44.166 --> 00:28:45.116
matrices on the right.

00:28:46.176 --> 00:28:47.106
That's because we know how to

00:28:47.106 --> 00:28:48.226
solve a system with a triangular

00:28:48.226 --> 00:28:49.616
matrix very well.

00:28:50.626 --> 00:28:52.186
If we're not square, we have to

00:28:52.186 --> 00:28:53.376
do things slightly differently;

00:28:54.136 --> 00:28:55.586
we have to pick a rectangular

00:28:55.586 --> 00:28:57.656
and orthogonal factor here.

00:28:57.656 --> 00:28:59.136
And this is your QR

00:28:59.136 --> 00:29:00.746
factorization if you've heard of

00:29:00.746 --> 00:29:01.466
that before.

00:29:02.526 --> 00:29:05.276
So let's see how to actually do

00:29:05.976 --> 00:29:06.076
this.

00:29:06.256 --> 00:29:08.076
Here is a sparse matrix

00:29:08.206 --> 00:29:08.686
equation.

00:29:09.866 --> 00:29:11.346
And let's define that matrix

00:29:11.346 --> 00:29:11.896
just to begin with.

00:29:12.566 --> 00:29:14.586
So this is going to be very

00:29:14.586 --> 00:29:15.896
similar to what I just showed

00:29:15.896 --> 00:29:17.526
you, except this matrix is

00:29:17.526 --> 00:29:17.936
special.

00:29:18.626 --> 00:29:19.466
It's symmetric.

00:29:19.956 --> 00:29:21.316
That means that the lower

00:29:21.316 --> 00:29:23.426
triangle is just the mirror

00:29:23.426 --> 00:29:24.366
reflection of the other

00:29:24.366 --> 00:29:24.746
triangle.

00:29:24.796 --> 00:29:27.036
So we can take advantage of that

00:29:27.036 --> 00:29:28.376
and let's only store those lower

00:29:28.376 --> 00:29:29.366
triangle entries.

00:29:30.486 --> 00:29:31.966
Wrap it in that metadata and

00:29:31.966 --> 00:29:34.606
this time we're going to specify

00:29:34.606 --> 00:29:35.746
that this is not ordinary, this

00:29:35.746 --> 00:29:37.086
is a symmetric matrix.

00:29:37.476 --> 00:29:38.666
And we're going to tell it that

00:29:38.666 --> 00:29:40.076
we're passing the lower

00:29:40.076 --> 00:29:40.606
triangle.

00:29:40.946 --> 00:29:41.936
We could pass the upper triangle

00:29:41.936 --> 00:29:43.306
if we wanted, we've chosen the

00:29:43.306 --> 00:29:44.246
lower triangle here.

00:29:45.156 --> 00:29:46.806
So we got our matrix.

00:29:47.206 --> 00:29:48.616
Next let's look at that

00:29:48.616 --> 00:29:49.446
right-hand side.

00:29:50.036 --> 00:29:51.526
So this is a dense vector.

00:29:52.606 --> 00:29:54.456
Let's just have a simple array.

00:29:55.146 --> 00:29:56.136
Wrap it in a little bit of

00:29:56.136 --> 00:29:57.296
metadata, telling us how long it

00:29:57.296 --> 00:30:00.906
is, and that's how easy this is.

00:30:01.516 --> 00:30:03.196
Which gets us to the interesting

00:30:03.196 --> 00:30:04.936
part, how do we actually find

00:30:05.036 --> 00:30:08.836
that vector X?

00:30:09.056 --> 00:30:11.016
So let's define some storage to

00:30:11.126 --> 00:30:11.866
put the answer in.

00:30:11.866 --> 00:30:13.736
This is exactly the same as flat

00:30:13.736 --> 00:30:15.206
dense vector B we just saw,

00:30:15.436 --> 00:30:16.646
except we don't have to supply

00:30:16.646 --> 00:30:17.606
any values.

00:30:18.126 --> 00:30:20.476
Then we're going to call

00:30:20.476 --> 00:30:21.376
SparseFactor.

00:30:21.776 --> 00:30:23.136
And I know this matrix is

00:30:23.136 --> 00:30:24.876
positive definite; therefore, I

00:30:24.876 --> 00:30:25.876
can tell it use a Cholesky

00:30:25.876 --> 00:30:26.496
factorization.

00:30:26.646 --> 00:30:28.516
I've got a flowchart for you in

00:30:28.516 --> 00:30:29.606
a couple of slides which tells

00:30:29.606 --> 00:30:30.716
you how to pick a factorization

00:30:30.716 --> 00:30:33.046
to use, but here we're using

00:30:33.046 --> 00:30:33.476
Cholesky.

00:30:33.886 --> 00:30:35.646
That gets us L times L transpose

00:30:35.646 --> 00:30:37.346
factorization, which we then

00:30:37.346 --> 00:30:39.436
feed into SparseSolve, pass that

00:30:39.436 --> 00:30:40.766
right-hand side and the storage

00:30:40.766 --> 00:30:41.416
we specify it.

00:30:41.746 --> 00:30:42.876
And we get our answer.

00:30:43.576 --> 00:30:44.626
We can put that back into the

00:30:44.626 --> 00:30:45.226
equation.

00:30:45.466 --> 00:30:48.876
And we can see this is correct.

00:30:49.006 --> 00:30:51.226
So what if A is not square?

00:30:53.356 --> 00:30:55.136
Well, this is where we have to

00:30:55.136 --> 00:30:56.116
use that QR factorization

00:30:56.116 --> 00:30:57.026
[inaudible] I mentioned before.

00:30:57.026 --> 00:30:59.346
But we got two different cases

00:30:59.346 --> 00:30:59.546
here.

00:30:59.596 --> 00:31:00.906
It's not entirely simple.

00:31:01.266 --> 00:31:03.286
We can be overdetermined since

00:31:03.286 --> 00:31:05.676
we have more rows than columns.

00:31:06.676 --> 00:31:07.656
Unless you've picked a very

00:31:07.656 --> 00:31:08.626
special system here, that

00:31:08.626 --> 00:31:09.786
probably means there's no

00:31:09.786 --> 00:31:11.546
exactly correct answer.

00:31:12.106 --> 00:31:13.026
In fact, you're in this sort of

00:31:13.026 --> 00:31:13.676
situation.

00:31:14.976 --> 00:31:17.186
Put a straight line through

00:31:17.186 --> 00:31:18.206
these four points.

00:31:20.056 --> 00:31:22.246
Clearly that's impossible, but

00:31:22.246 --> 00:31:22.926
if you remember back to your

00:31:22.926 --> 00:31:24.586
school days, you probably did

00:31:24.886 --> 00:31:26.136
some least squares fitting.

00:31:27.396 --> 00:31:29.716
We pick a line which minimizes

00:31:29.866 --> 00:31:31.426
sum of the square of the arrows.

00:31:31.616 --> 00:31:32.746
It's exactly what we do in this

00:31:32.746 --> 00:31:33.266
case.

00:31:34.076 --> 00:31:35.296
Remember we want to solve X

00:31:35.296 --> 00:31:35.896
equals B.

00:31:35.996 --> 00:31:37.966
So the arrow is X minus B.

00:31:38.506 --> 00:31:39.686
Let's take the two normals out,

00:31:39.776 --> 00:31:41.046
which is effectively the sum of

00:31:41.046 --> 00:31:41.936
the square of the arrows in this

00:31:42.436 --> 00:31:45.126
example, and minimize that.

00:31:46.676 --> 00:31:48.336
If we're underdetermined, that

00:31:48.336 --> 00:31:49.876
is, we have more columns than

00:31:49.876 --> 00:31:51.486
rows, we're in a slightly

00:31:51.486 --> 00:31:52.456
different situation.

00:31:53.816 --> 00:31:55.836
It's equivalent to saying put a

00:31:55.836 --> 00:31:57.196
line through this point.

00:31:58.026 --> 00:32:00.086
Obviously, there's an infinite

00:32:00.086 --> 00:32:01.386
family of lines which goes

00:32:01.386 --> 00:32:02.036
through that point.

00:32:02.526 --> 00:32:04.906
So how do we pick one to return

00:32:04.906 --> 00:32:05.496
to you?

00:32:06.476 --> 00:32:08.536
We give you the solution with

00:32:08.536 --> 00:32:09.056
minimum norm.

00:32:09.056 --> 00:32:13.446
Let's look at that on a code

00:32:13.446 --> 00:32:13.856
slide.

00:32:15.276 --> 00:32:17.416
Here it's very, very similar to

00:32:17.416 --> 00:32:18.616
that Cholesky factorization we

00:32:18.616 --> 00:32:19.116
saw before.

00:32:19.656 --> 00:32:22.356
In fact, the only difference is

00:32:22.356 --> 00:32:23.426
that we say to use a QR

00:32:23.426 --> 00:32:25.136
factorization rather than

00:32:25.136 --> 00:32:25.656
Cholesky.

00:32:25.876 --> 00:32:27.756
And this will automatically pick

00:32:27.756 --> 00:32:28.836
whether to do the least squares

00:32:28.836 --> 00:32:30.336
or the minimum norm depending on

00:32:30.336 --> 00:32:32.336
the dimensions of your matrix.

00:32:32.816 --> 00:32:34.906
And I told you that we had a

00:32:34.906 --> 00:32:36.246
flowchart for you on how to

00:32:36.246 --> 00:32:37.636
decide which factorization to

00:32:37.636 --> 00:32:38.016
use.

00:32:38.596 --> 00:32:40.306
So the first question you have

00:32:41.046 --> 00:32:45.346
to ask is: Is your matrix

00:32:45.416 --> 00:32:46.006
symmetric?

00:32:47.536 --> 00:32:49.706
If it isn't, you have to use the

00:32:49.706 --> 00:32:50.956
QR factorization.

00:32:51.556 --> 00:32:53.076
But if it is, we have another

00:32:53.076 --> 00:32:55.496
question for you: Is your matrix

00:32:55.496 --> 00:32:56.266
positive definite?

00:32:56.706 --> 00:32:57.756
Now, if you don't know the

00:32:57.756 --> 00:33:00.416
answer or if you're sure it

00:33:00.416 --> 00:33:02.646
isn't, you can do a symmetric

00:33:02.646 --> 00:33:05.356
indefinite factorization, LDL

00:33:05.446 --> 00:33:06.626
transpose.

00:33:07.246 --> 00:33:08.636
But if you know that extra bit

00:33:08.636 --> 00:33:09.696
of information that you've got

00:33:09.696 --> 00:33:11.496
positive definite matrix, you

00:33:11.496 --> 00:33:12.286
can use the Cholesky

00:33:12.286 --> 00:33:14.206
factorization L times L

00:33:14.206 --> 00:33:15.076
transposes.

00:33:15.926 --> 00:33:17.406
And that's all we have to tell

00:33:17.406 --> 00:33:18.906
you on matrix factorizations.

00:33:18.906 --> 00:33:21.006
Now, I said there was this other

00:33:21.096 --> 00:33:22.936
technique, iterative methods.

00:33:22.936 --> 00:33:26.906
So what is an iterative method?

00:33:27.886 --> 00:33:29.806
Well, we pick a starting point,

00:33:29.806 --> 00:33:31.476
our best guess at the solution

00:33:31.476 --> 00:33:32.566
before we start.

00:33:32.616 --> 00:33:34.326
And this can be zero if you

00:33:34.326 --> 00:33:35.676
don't have any idea what the

00:33:35.676 --> 00:33:37.276
actual answer is going to look

00:33:37.276 --> 00:33:37.536
like.

00:33:38.136 --> 00:33:40.346
And we want to get within some

00:33:40.346 --> 00:33:42.336
small radius of our actual

00:33:42.336 --> 00:33:42.936
solution.

00:33:42.936 --> 00:33:45.286
And the way we do that is we

00:33:45.286 --> 00:33:46.646
iterate through a series of

00:33:46.646 --> 00:33:48.676
points which converge to that

00:33:48.676 --> 00:33:49.186
solution.

00:33:50.156 --> 00:33:51.616
Now, there's a couple of caveats

00:33:51.616 --> 00:33:52.376
with using these.

00:33:53.816 --> 00:33:55.156
Typically they're only going to

00:33:55.156 --> 00:33:56.906
be faster than that matrix

00:33:56.906 --> 00:33:59.176
factorization approach if you've

00:33:59.176 --> 00:34:01.286
got a really, really, really big

00:34:01.286 --> 00:34:02.316
sparse matrix.

00:34:03.026 --> 00:34:04.616
And further, to actually get to

00:34:04.616 --> 00:34:06.256
that performance, you need to

00:34:06.256 --> 00:34:07.576
know a bit mathematically about

00:34:07.576 --> 00:34:08.146
your problem.

00:34:08.306 --> 00:34:09.146
You need something called a

00:34:09.146 --> 00:34:11.126
preconditioner, which is a very

00:34:11.126 --> 00:34:12.126
approximate solution.

00:34:13.246 --> 00:34:14.846
And if you check the literature

00:34:14.846 --> 00:34:16.176
for your field, you'll probably

00:34:16.176 --> 00:34:17.775
find quite a number have been

00:34:18.126 --> 00:34:20.036
derived by mathematicians.

00:34:21.206 --> 00:34:22.266
What does this actually look

00:34:22.266 --> 00:34:23.045
like to use?

00:34:23.676 --> 00:34:25.516
So here's that matrix equation

00:34:25.516 --> 00:34:26.065
we had earlier.

00:34:26.456 --> 00:34:27.456
This time I'm going to use

00:34:27.456 --> 00:34:28.686
iterative method to solve it.

00:34:29.545 --> 00:34:30.946
In fact, I'm going to use

00:34:30.946 --> 00:34:32.106
conjugate gradients.

00:34:32.846 --> 00:34:34.416
So this is positive definite.

00:34:35.315 --> 00:34:38.676
So we just specify to use the

00:34:38.676 --> 00:34:39.656
conjugate gradient methods.

00:34:39.656 --> 00:34:40.525
And you'll notice there's some

00:34:40.525 --> 00:34:41.646
brackets ever this.

00:34:42.735 --> 00:34:43.666
That's actually because this is

00:34:43.666 --> 00:34:45.126
a factory function which

00:34:45.126 --> 00:34:46.846
produces the methods and you can

00:34:46.846 --> 00:34:48.956
specify method-specific

00:34:48.956 --> 00:34:50.485
parameters in those brackets.

00:34:51.216 --> 00:34:52.146
The other thing I'm going to do

00:34:52.146 --> 00:34:53.505
is I'm going to use a diagonal

00:34:53.505 --> 00:34:54.065
precondition.

00:34:54.556 --> 00:34:55.795
This matrix is diagonally

00:34:55.795 --> 00:34:56.166
dominant.

00:34:56.166 --> 00:34:57.486
That means that the entries down

00:34:57.486 --> 00:34:58.716
the diagonal are very large

00:34:58.716 --> 00:34:59.646
compared to those off the

00:34:59.646 --> 00:35:02.006
diagonal; therefore, I know this

00:35:02.006 --> 00:35:03.206
diagonal preconditioner will

00:35:03.206 --> 00:35:04.146
work very well.

00:35:04.736 --> 00:35:06.886
And indeed, if we look at the

00:35:06.886 --> 00:35:08.536
output of the algorithm, you can

00:35:08.536 --> 00:35:10.316
see that this arrow AX minus B

00:35:10.316 --> 00:35:12.556
is decreasing its iteration and

00:35:12.556 --> 00:35:13.696
we get to machine precision in

00:35:13.696 --> 00:35:14.766
four iterations.

00:35:14.876 --> 00:35:16.316
That's because it's 4 by 4

00:35:16.316 --> 00:35:16.906
matrix.

00:35:17.176 --> 00:35:18.046
Mathematically, we should

00:35:18.046 --> 00:35:21.006
converge in at most N iterations

00:35:21.216 --> 00:35:22.646
where N is the size of matrix.

00:35:23.526 --> 00:35:24.826
So this is behaving as expected.

00:35:24.856 --> 00:35:25.916
But if you've got a much larger

00:35:25.916 --> 00:35:27.126
matrix, you probably don't want

00:35:27.126 --> 00:35:28.676
to go that many iterations,

00:35:29.266 --> 00:35:30.196
which is why you get an

00:35:30.196 --> 00:35:31.186
approximate solution.

00:35:32.366 --> 00:35:33.556
And you can see you the get same

00:35:33.556 --> 00:35:34.286
answer as before.

00:35:35.096 --> 00:35:37.196
Now, let's say we want to solve

00:35:37.196 --> 00:35:38.026
the least squares problem

00:35:38.026 --> 00:35:38.516
instead.

00:35:39.466 --> 00:35:41.426
We offer a least square solver,

00:35:41.506 --> 00:35:42.196
which is iterative.

00:35:42.496 --> 00:35:43.126
We don't offer an

00:35:43.126 --> 00:35:45.606
underdetermined system solver,

00:35:45.606 --> 00:35:45.946
however.

00:35:46.196 --> 00:35:47.986
In that case you can just pick a

00:35:47.986 --> 00:35:48.956
[inaudible] of zeros and call

00:35:48.956 --> 00:35:50.086
them in solver square system

00:35:50.086 --> 00:35:50.556
instead.

00:35:51.476 --> 00:35:53.916
And to use this, we use the

00:35:53.956 --> 00:35:56.176
method LSMR and a slightly

00:35:56.176 --> 00:35:57.806
different preconditioner which

00:35:57.806 --> 00:35:59.486
is, again, problem-specific.

00:35:59.996 --> 00:36:01.946
And you can see that we get

00:36:01.946 --> 00:36:03.326
there three iterations this time

00:36:03.326 --> 00:36:05.176
because this is a 4 by 3 matrix.

00:36:05.796 --> 00:36:07.696
But there are some very cool

00:36:07.696 --> 00:36:11.226
things about this particular way

00:36:11.226 --> 00:36:11.976
of doing things.

00:36:13.446 --> 00:36:15.646
The first is that I don't

00:36:15.646 --> 00:36:16.996
actually need my matrix

00:36:16.996 --> 00:36:17.606
explicitly.

00:36:18.946 --> 00:36:20.856
As long as I have a function

00:36:21.306 --> 00:36:22.866
which performs the mathematical

00:36:22.866 --> 00:36:25.006
operation A times X or A

00:36:25.006 --> 00:36:26.606
transpose times X, that is, you

00:36:26.606 --> 00:36:28.886
have an operator, I can

00:36:28.886 --> 00:36:30.676
substitute a block of code in

00:36:30.676 --> 00:36:31.896
place of this actual matrix

00:36:31.896 --> 00:36:32.376
argument.

00:36:33.476 --> 00:36:35.496
The second is you're not

00:36:35.496 --> 00:36:36.996
restricted to using our

00:36:36.996 --> 00:36:37.956
preconditioners.

00:36:38.786 --> 00:36:40.206
You can write your own and just

00:36:40.206 --> 00:36:41.376
provide a function pointer in

00:36:41.376 --> 00:36:41.966
this argument.

00:36:44.646 --> 00:36:46.276
Now, you're probably saying,

00:36:46.366 --> 00:36:47.776
"How do I know which iterative

00:36:47.776 --> 00:36:48.366
method to use?"

00:36:48.446 --> 00:36:49.156
I've got another one of those

00:36:49.156 --> 00:36:49.896
flowcharts for you.

00:36:50.996 --> 00:36:52.386
This time our first question is

00:36:52.386 --> 00:36:54.046
not whether you are symmetric

00:36:54.046 --> 00:36:55.366
but whether you are square.

00:36:55.846 --> 00:36:57.216
If you're not square, you're

00:36:57.216 --> 00:36:58.126
going to have to do a least

00:36:58.126 --> 00:36:58.706
square solve.

00:36:59.926 --> 00:37:01.666
However, if you are, we go

00:37:01.666 --> 00:37:03.066
straight to that question are

00:37:03.066 --> 00:37:04.056
you positive definite?

00:37:05.286 --> 00:37:07.406
If you're not, we have GMRES --

00:37:07.406 --> 00:37:08.926
that will handle pretty much any

00:37:08.926 --> 00:37:09.866
square matrix.

00:37:10.916 --> 00:37:12.516
But if you know that extra bit

00:37:12.516 --> 00:37:13.856
of information, you can, of

00:37:13.856 --> 00:37:15.316
course, use the famous conjugate

00:37:15.316 --> 00:37:15.996
gradient method.

00:37:16.526 --> 00:37:18.946
Now, that's everything I have to

00:37:18.946 --> 00:37:20.476
tell you today about sparse

00:37:20.476 --> 00:37:21.106
matrices.

00:37:22.056 --> 00:37:23.956
So we've got one thing we want

00:37:23.956 --> 00:37:27.396
to point out, and that is that

00:37:27.776 --> 00:37:29.706
you can now use Accelerate on

00:37:29.966 --> 00:37:30.426
the Watch.

00:37:30.426 --> 00:37:31.956
We have provided you that SDK.

00:37:31.956 --> 00:37:34.996
Now, the framework has always

00:37:34.996 --> 00:37:35.626
been there.

00:37:35.866 --> 00:37:36.906
So it's even better.

00:37:36.976 --> 00:37:39.576
Using today's SDK you can back

00:37:39.686 --> 00:37:42.166
deploy to previous Watch OS's.

00:37:42.536 --> 00:37:44.256
So that means that you get

00:37:44.856 --> 00:37:46.316
everything that we've told you

00:37:46.316 --> 00:37:47.986
about today on the Watch.

00:37:48.796 --> 00:37:51.056
So let's just summarize what

00:37:51.056 --> 00:37:51.496
that is.

00:37:51.916 --> 00:37:53.966
By using Accelerate your code

00:37:53.966 --> 00:37:54.936
will run faster.

00:37:55.306 --> 00:37:56.616
It will be more energy

00:37:56.616 --> 00:37:57.076
efficient.

00:37:57.396 --> 00:37:58.596
It will run across all our

00:37:58.596 --> 00:37:59.296
devices.

00:37:59.356 --> 00:38:00.746
And at the end of the day you

00:38:00.746 --> 00:38:02.026
have less code to maintain.

00:38:02.486 --> 00:38:03.676
You get everything here we've

00:38:03.676 --> 00:38:04.626
told you about today -- that

00:38:04.626 --> 00:38:06.246
sparse solver library, the new

00:38:06.246 --> 00:38:08.996
compression tool, changes to the

00:38:09.656 --> 00:38:11.356
BNNS, improvements to simd, and

00:38:11.356 --> 00:38:12.756
many more, and increased

00:38:12.756 --> 00:38:13.926
performance across the

00:38:13.926 --> 00:38:14.386
framework.

00:38:16.166 --> 00:38:17.446
So if you want some more

00:38:17.446 --> 00:38:19.206
information, including some

00:38:19.206 --> 00:38:20.306
extensive sample code we've

00:38:20.306 --> 00:38:21.666
developed for the sparse solver,

00:38:21.986 --> 00:38:22.896
it's all available here.

00:38:23.756 --> 00:38:27.336
And you may be interested in

00:38:27.336 --> 00:38:28.026
reviewing some of these

00:38:28.026 --> 00:38:29.186
sessions, which have already

00:38:29.186 --> 00:38:30.656
been or going to the Metal

00:38:30.656 --> 00:38:32.096
session this afternoon.

00:38:33.416 --> 00:38:34.786
Thank you for your time.

00:38:35.016 --> 00:38:37.000
[ Applause ]