WEBVTT

00:00:18.066 --> 00:00:18.596
>> Good afternoon.

00:00:19.516 --> 00:00:23.776
[ Applause ]

00:00:24.276 --> 00:00:25.326
Welcome to our session

00:00:25.326 --> 00:00:26.716
introducing ARKit.

00:00:27.096 --> 00:00:27.806
My name is Mike.

00:00:27.916 --> 00:00:29.526
I'm an engineer from ARKit team.

00:00:29.676 --> 00:00:31.056
And today I'm thrilled to talk

00:00:31.056 --> 00:00:32.356
to you about the concepts as

00:00:32.646 --> 00:00:34.296
well as the code that go into

00:00:34.296 --> 00:00:35.736
creating your very own augmented

00:00:35.736 --> 00:00:37.636
reality experience on iOS.

00:00:38.341 --> 00:00:40.341
[ Cheering and Applause ]

00:00:40.666 --> 00:00:41.016
Thank you.

00:00:41.336 --> 00:00:42.876
I know many of you are eager to

00:00:42.876 --> 00:00:43.946
get started with augmented

00:00:43.946 --> 00:00:44.336
reality.

00:00:44.336 --> 00:00:46.166
Let's show you just how easy it

00:00:46.166 --> 00:00:47.156
is using ARKit.

00:00:48.206 --> 00:00:50.756
But first, what is augmented

00:00:50.756 --> 00:00:51.236
reality?

00:00:52.056 --> 00:00:53.616
Augmented reality is creating

00:00:53.616 --> 00:00:54.816
the illusion that virtual

00:00:54.816 --> 00:00:56.346
objects are placed in a physical

00:00:56.346 --> 00:00:56.686
world.

00:00:57.096 --> 00:00:58.706
It's using your iPhone or your

00:00:58.706 --> 00:01:00.716
iPad as a lens into a virtual

00:01:00.716 --> 00:01:02.116
world based on what your camera

00:01:02.116 --> 00:01:02.416
sees.

00:01:03.376 --> 00:01:04.446
Let's take a look at some

00:01:04.446 --> 00:01:04.855
examples.

00:01:05.826 --> 00:01:07.436
We gave a group of developers

00:01:07.496 --> 00:01:08.856
early access to ARKit.

00:01:09.276 --> 00:01:10.186
And here's what they made.

00:01:10.436 --> 00:01:12.116
This is a sneak peek at some

00:01:12.116 --> 00:01:13.266
things you might see in the near

00:01:13.266 --> 00:01:14.000
future.

00:01:17.536 --> 00:01:19.506
Within, a company focused on

00:01:19.506 --> 00:01:20.676
immersive storytelling,

00:01:21.046 --> 00:01:22.276
tells the story of Goldilocks

00:01:22.846 --> 00:01:23.660
using AR.

00:01:26.186 --> 00:01:27.646
Transforming a bedroom into a

00:01:27.746 --> 00:01:29.426
virtual storybook, they allow

00:01:29.426 --> 00:01:30.896
you to progress a story by

00:01:30.896 --> 00:01:32.896
reciting the text, but even more

00:01:32.896 --> 00:01:34.246
importantly, they allow you to

00:01:34.366 --> 00:01:35.596
explore the scene from any

00:01:35.596 --> 00:01:36.000
angle.

00:01:39.046 --> 00:01:40.876
This level of interactivity

00:01:40.876 --> 00:01:42.596
really helps bring your virtual

00:01:42.596 --> 00:01:43.500
scene alive.

00:01:48.196 --> 00:01:51.136
Next, Ikea used ARKit in order

00:01:51.136 --> 00:01:52.316
to redesign your living room.

00:01:54.516 --> 00:01:58.046
[ Applause ]

00:01:58.546 --> 00:02:00.066
By being able to place virtual

00:02:00.066 --> 00:02:01.396
content next to physical

00:02:01.396 --> 00:02:03.196
objects, you open up a world of

00:02:03.196 --> 00:02:05.366
possibilities to your users.

00:02:07.806 --> 00:02:09.776
And last, games.

00:02:10.276 --> 00:02:12.456
Pokemon Go, an app that you've

00:02:12.456 --> 00:02:14.806
probably already heard of, used

00:02:14.806 --> 00:02:16.286
ARKit to take catching Pokemon

00:02:16.516 --> 00:02:17.536
to the next level.

00:02:18.906 --> 00:02:20.486
By being able to anchor your

00:02:20.486 --> 00:02:22.116
virtual content in the real

00:02:22.116 --> 00:02:23.516
world, you really allow for a

00:02:23.516 --> 00:02:25.376
more immersive experience than

00:02:25.376 --> 00:02:26.276
previously possible.

00:02:26.806 --> 00:02:28.626
But it doesn't stop there.

00:02:28.626 --> 00:02:30.046
There are a multitude of ways

00:02:30.046 --> 00:02:31.206
that you can use augmented

00:02:31.206 --> 00:02:32.896
reality to enhance your user

00:02:32.896 --> 00:02:33.516
experience.

00:02:34.236 --> 00:02:35.106
So let's see what goes into

00:02:35.106 --> 00:02:35.926
that.

00:02:38.756 --> 00:02:40.446
There's a large amount of domain

00:02:40.446 --> 00:02:41.556
knowledge that goes into

00:02:41.556 --> 00:02:42.976
creating augmented reality.

00:02:43.456 --> 00:02:44.896
Everything from computer vision,

00:02:45.146 --> 00:02:46.936
to sensor fusion, to talking to

00:02:46.936 --> 00:02:48.216
hardware in order to get camera

00:02:48.216 --> 00:02:49.236
calibrations and camera

00:02:49.236 --> 00:02:49.766
intrinsics.

00:02:50.436 --> 00:02:51.516
We wanted to make this all

00:02:51.516 --> 00:02:52.156
easier for you.

00:02:52.646 --> 00:02:54.736
So today we're introducing

00:02:54.876 --> 00:02:55.506
ARKit.

00:02:57.516 --> 00:03:02.226
[ Applause ]

00:03:02.726 --> 00:03:04.686
ARKit is a mobile AR platform

00:03:04.686 --> 00:03:06.136
for developing augmented reality

00:03:06.136 --> 00:03:07.576
apps on iOS.

00:03:07.896 --> 00:03:09.696
It is a high level API providing

00:03:09.696 --> 00:03:11.696
a simple interface to a powerful

00:03:11.696 --> 00:03:12.326
set of features.

00:03:12.886 --> 00:03:14.256
But more importantly, it's

00:03:14.256 --> 00:03:15.626
rolling out supporting hundreds

00:03:15.626 --> 00:03:17.326
of millions of existing iOS

00:03:17.326 --> 00:03:17.786
devices.

00:03:18.486 --> 00:03:19.596
In order to get the full set of

00:03:19.676 --> 00:03:21.236
features for ARKit, you're going

00:03:21.376 --> 00:03:22.796
to want an A9 and up.

00:03:22.966 --> 00:03:24.796
This is most iOS 11 devices,

00:03:24.796 --> 00:03:26.486
including the iPhone 6S.

00:03:28.146 --> 00:03:28.736
Now let's talk about the

00:03:28.736 --> 00:03:29.306
features.

00:03:29.896 --> 00:03:31.336
So what does ARKit provide?

00:03:32.146 --> 00:03:33.536
ARKit can be broken up into

00:03:33.676 --> 00:03:35.516
three distinct layers, the first

00:03:35.516 --> 00:03:37.006
of which is tracking.

00:03:38.476 --> 00:03:39.676
Tracking is the core

00:03:39.676 --> 00:03:40.946
functionality of ARKit.

00:03:40.946 --> 00:03:42.846
It is the ability to track your

00:03:42.846 --> 00:03:44.316
device in real time.

00:03:44.806 --> 00:03:46.436
With world tracking we provide

00:03:46.436 --> 00:03:47.636
you the ability to get your

00:03:47.636 --> 00:03:49.486
device's relative position in

00:03:49.486 --> 00:03:50.496
the physical environment.

00:03:51.136 --> 00:03:53.236
We use visual inertial odometry,

00:03:53.596 --> 00:03:55.216
which is using camera images, as

00:03:55.216 --> 00:03:56.396
well as motion data from your

00:03:56.396 --> 00:03:58.226
device in order to get a precise

00:03:58.226 --> 00:03:59.896
view of where your device is

00:03:59.896 --> 00:04:01.326
located as well as how it is

00:04:01.326 --> 00:04:01.866
oriented.

00:04:02.906 --> 00:04:03.766
But also, more importantly,

00:04:04.126 --> 00:04:05.316
there's no external setup

00:04:05.316 --> 00:04:07.066
required, no pre-existing

00:04:07.066 --> 00:04:07.756
knowledge about your

00:04:07.756 --> 00:04:09.116
environment, as well as no

00:04:09.116 --> 00:04:10.196
additional sensors that you

00:04:10.196 --> 00:04:11.526
don't already have on your

00:04:11.526 --> 00:04:11.986
device.

00:04:13.366 --> 00:04:15.386
Next, building upon tracking we

00:04:15.386 --> 00:04:16.646
provide scene understanding.

00:04:19.386 --> 00:04:20.606
Scene understanding is the

00:04:20.606 --> 00:04:23.276
ability to determine attributes

00:04:23.276 --> 00:04:24.316
or properties about the

00:04:24.316 --> 00:04:25.656
environment around your device.

00:04:26.136 --> 00:04:27.416
It's providing things like plane

00:04:27.416 --> 00:04:27.846
detection.

00:04:28.476 --> 00:04:29.586
Plane detection is the ability

00:04:29.586 --> 00:04:31.556
to determine surfaces or planes

00:04:31.556 --> 00:04:32.676
in the physical environment.

00:04:33.206 --> 00:04:34.366
This is things like the ground

00:04:34.366 --> 00:04:35.706
floor or maybe a table.

00:04:37.036 --> 00:04:38.206
In order to place your virtual

00:04:38.206 --> 00:04:39.576
objects, we provide hit testing

00:04:39.576 --> 00:04:40.126
functionality.

00:04:40.726 --> 00:04:41.646
So this is getting an

00:04:41.646 --> 00:04:43.226
intersection with the real world

00:04:43.226 --> 00:04:44.736
topology so that you can place

00:04:44.736 --> 00:04:46.066
your virtual object in the

00:04:46.066 --> 00:04:46.886
physical world.

00:04:47.616 --> 00:04:49.656
And last, scene understanding

00:04:49.656 --> 00:04:51.026
provides light estimation.

00:04:51.636 --> 00:04:54.146
So light estimation is used to

00:04:54.146 --> 00:04:56.176
render or correctly light your

00:04:56.176 --> 00:04:58.006
virtual geometry to match that

00:04:58.006 --> 00:04:59.016
of the physical world.

00:04:59.916 --> 00:05:01.116
Using all of these together we

00:05:01.116 --> 00:05:03.386
can seamlessly integrate virtual

00:05:03.386 --> 00:05:05.046
content into your physical

00:05:05.206 --> 00:05:05.786
environment.

00:05:06.586 --> 00:05:08.326
And so the last layer of ARKit

00:05:08.586 --> 00:05:09.176
is rendering.

00:05:11.256 --> 00:05:13.016
For rendering we provide easy

00:05:13.016 --> 00:05:14.426
integration into any renderer.

00:05:14.636 --> 00:05:15.926
We provide a constant stream of

00:05:15.926 --> 00:05:17.476
camera images, tracking

00:05:17.476 --> 00:05:18.906
information as well as scene

00:05:18.906 --> 00:05:20.346
understanding that can be

00:05:20.346 --> 00:05:21.536
inputted into any renderer.

00:05:23.206 --> 00:05:24.806
For those of you using SceneKit

00:05:24.806 --> 00:05:26.716
or SpriteKit, we provide custom

00:05:26.716 --> 00:05:28.416
AR views, which implement most

00:05:28.416 --> 00:05:29.446
of the rendering for you.

00:05:29.736 --> 00:05:30.826
So it's really easy to get

00:05:30.826 --> 00:05:31.186
started.

00:05:32.246 --> 00:05:33.246
And for those of you doing

00:05:33.246 --> 00:05:34.436
custom rendering, we provide a

00:05:34.436 --> 00:05:35.966
metal template through Xcode,

00:05:36.636 --> 00:05:37.446
which gets you started

00:05:37.446 --> 00:05:39.166
integrating ARKit into your

00:05:39.166 --> 00:05:39.946
custom renderer.

00:05:39.946 --> 00:05:43.986
And one more thing, Unity and

00:05:43.986 --> 00:05:45.356
UnReal will be supporting the

00:05:45.356 --> 00:05:47.126
full set of features from ARKit.

00:05:48.516 --> 00:05:53.626
[ Applause ]

00:05:54.126 --> 00:05:56.106
So, are you guys ready?

00:05:56.106 --> 00:05:56.736
Let's get started.

00:05:57.426 --> 00:05:59.366
How do I use ARKit in my

00:05:59.366 --> 00:05:59.986
application?

00:06:01.216 --> 00:06:02.346
ARKit is a framework that

00:06:02.346 --> 00:06:03.776
handles all of the processing

00:06:03.996 --> 00:06:05.266
that goes into creating an

00:06:05.266 --> 00:06:06.776
augmented reality experience.

00:06:07.636 --> 00:06:08.746
With the renderer of my choice,

00:06:09.116 --> 00:06:11.626
I can simply use ARKit to do the

00:06:11.626 --> 00:06:12.236
processing.

00:06:12.366 --> 00:06:13.606
And it will provide everything

00:06:13.606 --> 00:06:14.626
that I need to render my

00:06:14.626 --> 00:06:15.836
augmented reality scene.

00:06:16.716 --> 00:06:19.006
In addition to processing, ARKit

00:06:19.006 --> 00:06:20.746
also handles the capturing that

00:06:20.746 --> 00:06:22.776
is done in order to do augmented

00:06:22.776 --> 00:06:23.116
reality.

00:06:23.216 --> 00:06:25.246
So using AVFoundation and Core

00:06:25.246 --> 00:06:27.156
Motion under the hood, we

00:06:27.426 --> 00:06:29.306
capture images as well as get

00:06:29.746 --> 00:06:31.106
motion data from your device in

00:06:31.106 --> 00:06:32.656
order to do tracking and provide

00:06:32.656 --> 00:06:33.726
those camera images to your

00:06:33.726 --> 00:06:34.016
renderer.

00:06:34.566 --> 00:06:36.786
So now how do I use ARKit?

00:06:37.576 --> 00:06:39.396
ARKit is a session-based API.

00:06:39.896 --> 00:06:40.806
The first thing you need to do

00:06:40.806 --> 00:06:42.496
to get started is simply create

00:06:42.496 --> 00:06:43.156
an ARSession.

00:06:44.116 --> 00:06:45.406
ARSession is the object that

00:06:45.406 --> 00:06:46.816
controls all of the processing

00:06:46.816 --> 00:06:48.756
that goes into creating your

00:06:48.756 --> 00:06:50.386
augmented reality app.

00:06:50.676 --> 00:06:51.826
But first I need to determine

00:06:51.826 --> 00:06:53.426
what kind of tracking I want to

00:06:53.516 --> 00:06:55.626
do for my augmented reality app.

00:06:55.626 --> 00:06:56.926
So, to determine this we're

00:06:56.926 --> 00:06:58.466
going to create an AR session

00:06:58.466 --> 00:06:59.136
configuration.

00:06:59.716 --> 00:07:02.586
AR session configuration, and

00:07:02.586 --> 00:07:04.006
its subclasses determine what

00:07:04.006 --> 00:07:05.516
tracking you want to run on your

00:07:05.516 --> 00:07:05.896
session.

00:07:06.856 --> 00:07:07.956
By enabling and disabling

00:07:07.956 --> 00:07:09.276
properties, you can get

00:07:09.276 --> 00:07:10.086
different kinds of scene

00:07:10.086 --> 00:07:11.366
understanding and have your

00:07:11.366 --> 00:07:12.256
ARSession do different

00:07:12.256 --> 00:07:12.756
processing.

00:07:13.826 --> 00:07:15.786
In order to run my session, I

00:07:15.786 --> 00:07:17.776
simply call the Run method on

00:07:17.866 --> 00:07:18.946
ARSession providing the

00:07:18.946 --> 00:07:20.266
configuration I want to run.

00:07:20.266 --> 00:07:23.886
And with that, processing

00:07:23.886 --> 00:07:24.806
immediately starts.

00:07:24.806 --> 00:07:26.706
And we also set up the capturing

00:07:26.706 --> 00:07:27.146
underneath.

00:07:27.286 --> 00:07:28.516
So under the hood you'll see

00:07:28.516 --> 00:07:30.156
there's an AV capture session

00:07:30.156 --> 00:07:32.116
and a CM motion manager that get

00:07:32.116 --> 00:07:32.886
created for you.

00:07:32.886 --> 00:07:35.446
We use these to get image data

00:07:35.536 --> 00:07:36.616
as well as the motion data

00:07:36.616 --> 00:07:37.476
that's going to be used for

00:07:37.606 --> 00:07:38.176
tracking.

00:07:38.226 --> 00:07:39.386
Once processing is done,

00:07:39.716 --> 00:07:41.786
ARSession will output ARFrames.

00:07:42.486 --> 00:07:44.356
So an ARFrame is a snapshot in

00:07:44.356 --> 00:07:46.146
time, including all of the state

00:07:46.146 --> 00:07:47.366
of your session, everything

00:07:47.366 --> 00:07:49.256
needed to render your augmented

00:07:49.256 --> 00:07:49.816
reality scene.

00:07:51.016 --> 00:07:52.946
In order to access ARFrame, you

00:07:52.946 --> 00:07:54.886
can simply call or pull the

00:07:54.886 --> 00:07:56.286
current frame property from you

00:07:56.286 --> 00:07:56.816
ARSession.

00:07:57.516 --> 00:07:58.586
Or, you can set yourself as the

00:07:58.586 --> 00:08:00.256
delegate to receive updates when

00:08:00.256 --> 00:08:01.726
new ARFrames are available.

00:08:01.726 --> 00:08:04.946
So let's take a closer look at

00:08:04.946 --> 00:08:05.936
ARSessionConfiguration.

00:08:09.516 --> 00:08:10.836
ARSession configuration

00:08:10.836 --> 00:08:12.126
determines what kind of tracking

00:08:12.286 --> 00:08:13.256
you want to run on your session.

00:08:13.966 --> 00:08:15.316
So it provides different

00:08:15.316 --> 00:08:16.666
configuration classes.

00:08:17.606 --> 00:08:18.406
The base class,

00:08:18.406 --> 00:08:20.316
ARSessionConfiguration, provides

00:08:20.446 --> 00:08:21.446
three degrees of freedom

00:08:21.446 --> 00:08:22.826
tracking, which is just the

00:08:22.826 --> 00:08:24.246
orientation of your device.

00:08:25.296 --> 00:08:27.506
Its subclass, ARWorldTracking

00:08:27.506 --> 00:08:28.896
Session Configuration provides

00:08:29.036 --> 00:08:30.566
six degrees of freedom tracking.

00:08:30.816 --> 00:08:32.275
So this is using our core

00:08:32.275 --> 00:08:33.566
functionality world tracking in

00:08:33.806 --> 00:08:34.756
order to get not only your

00:08:34.756 --> 00:08:36.726
device's orientation, but also a

00:08:36.726 --> 00:08:38.066
relative position of your

00:08:38.066 --> 00:08:38.535
device.

00:08:39.385 --> 00:08:40.356
With this we also get

00:08:40.356 --> 00:08:41.566
information about the scene.

00:08:41.936 --> 00:08:42.606
So we provide scene

00:08:42.606 --> 00:08:44.316
understanding like feature

00:08:44.316 --> 00:08:46.196
points as well as physical

00:08:46.196 --> 00:08:48.226
positions in your world.

00:08:49.376 --> 00:08:50.636
In order to enable and disable

00:08:50.636 --> 00:08:52.006
features, you simply set

00:08:52.006 --> 00:08:53.216
properties on your session

00:08:53.216 --> 00:08:54.000
configuration classes.

00:08:58.196 --> 00:09:00.366
And session configurations also

00:09:00.366 --> 00:09:01.506
provide availability.

00:09:02.276 --> 00:09:03.516
So if you want to check if world

00:09:03.516 --> 00:09:04.586
tracking is supported on your

00:09:04.586 --> 00:09:06.436
device, you simply need to call

00:09:06.716 --> 00:09:08.316
the class property isSupported

00:09:08.316 --> 00:09:09.726
on ARWorldTracking Session

00:09:09.726 --> 00:09:10.306
Configuration.

00:09:11.186 --> 00:09:12.646
With this you can then use your

00:09:12.646 --> 00:09:13.456
World Tracking Session

00:09:13.456 --> 00:09:15.046
Configuration or fall back to

00:09:15.046 --> 00:09:16.576
the base class, which will only

00:09:16.576 --> 00:09:17.586
provide you with three degrees

00:09:17.586 --> 00:09:18.006
of freedom.

00:09:18.386 --> 00:09:20.006
It's important to note here that

00:09:20.006 --> 00:09:21.546
because the base class doesn't

00:09:21.546 --> 00:09:22.656
have any scene understanding

00:09:22.986 --> 00:09:24.256
functionality like hit tests

00:09:24.256 --> 00:09:25.306
won't be available on this

00:09:25.306 --> 00:09:25.636
device.

00:09:25.636 --> 00:09:27.556
So we're also going to provide a

00:09:27.556 --> 00:09:29.486
UI required device capability

00:09:29.486 --> 00:09:30.886
that you set in your app so that

00:09:30.886 --> 00:09:32.266
your app only appears in the App

00:09:32.266 --> 00:09:33.796
Store on devices that support

00:09:33.796 --> 00:09:34.536
World Tracking.

00:09:36.036 --> 00:09:39.966
Next, let's look at ARSession.

00:09:39.966 --> 00:09:41.586
ARSession, again, is the class

00:09:41.586 --> 00:09:42.446
that manages all of the

00:09:42.446 --> 00:09:44.046
processing for your augmented

00:09:44.046 --> 00:09:45.286
reality app.

00:09:46.556 --> 00:09:48.226
In addition to calling Run with

00:09:48.226 --> 00:09:49.766
a configuration, you can also

00:09:49.766 --> 00:09:50.506
call Pause.

00:09:51.426 --> 00:09:52.366
So Pause allows you to

00:09:52.366 --> 00:09:54.046
temporarily stop all processing

00:09:54.046 --> 00:09:55.236
happening on your session.

00:09:55.366 --> 00:09:56.456
So if your view is no longer

00:09:56.456 --> 00:09:57.906
visible, you may want to stop

00:09:57.906 --> 00:10:01.126
processing to stop using CPU and

00:10:01.126 --> 00:10:02.436
no tracking will occur during

00:10:02.436 --> 00:10:03.026
this pause.

00:10:03.266 --> 00:10:05.286
In order to resume tracking

00:10:05.286 --> 00:10:07.006
after a pause, you can simply

00:10:07.006 --> 00:10:08.656
call Run again with the stored

00:10:08.656 --> 00:10:09.886
configuration on your session.

00:10:11.376 --> 00:10:12.576
And last, you can call Run

00:10:12.576 --> 00:10:14.196
multiple times in order to

00:10:14.196 --> 00:10:15.506
transition between different

00:10:15.506 --> 00:10:16.166
configurations.

00:10:16.616 --> 00:10:18.036
So say I wanted to enable plane

00:10:18.036 --> 00:10:19.946
detection, I can change my

00:10:19.946 --> 00:10:21.286
configuration to enable plane

00:10:21.286 --> 00:10:23.146
detection, call Run again on my

00:10:23.146 --> 00:10:23.536
session.

00:10:23.796 --> 00:10:25.336
My session will automatically

00:10:25.336 --> 00:10:27.176
transition seamlessly between

00:10:27.176 --> 00:10:28.306
one configuration and another

00:10:28.516 --> 00:10:29.816
without dropping any camera

00:10:29.816 --> 00:10:30.136
images.

00:10:32.836 --> 00:10:34.536
So with the Run command we also

00:10:34.536 --> 00:10:36.376
provide resetting of tracking.

00:10:36.546 --> 00:10:37.716
So there's Run options that you

00:10:37.716 --> 00:10:39.446
can provide on the Run command

00:10:40.076 --> 00:10:41.486
in order to reset tracking.

00:10:41.486 --> 00:10:42.756
It'll reinitialize all of the

00:10:42.756 --> 00:10:43.686
tracking that's going on.

00:10:43.896 --> 00:10:44.856
And your camera position will

00:10:44.856 --> 00:10:46.056
start out again at 000.

00:10:46.056 --> 00:10:48.116
So this is useful for your

00:10:48.116 --> 00:10:49.306
application if you want to reset

00:10:49.306 --> 00:10:50.696
it to some starting point.

00:10:51.246 --> 00:10:54.426
So how do I make use of

00:10:54.426 --> 00:10:55.556
ARSessions processing?

00:10:56.256 --> 00:10:57.286
There's session updates

00:10:57.286 --> 00:10:58.626
available by setting yourself as

00:10:58.686 --> 00:10:59.296
the delegate.

00:11:00.456 --> 00:11:02.206
So in order to get the last

00:11:02.206 --> 00:11:03.496
frame that was processed, I

00:11:03.576 --> 00:11:04.796
could implement session

00:11:04.876 --> 00:11:05.696
didUpdate Frame.

00:11:05.896 --> 00:11:07.906
And this will give me the latest

00:11:07.906 --> 00:11:08.136
frame.

00:11:08.356 --> 00:11:09.726
For error handling, you can also

00:11:09.726 --> 00:11:11.666
implement things like session

00:11:11.666 --> 00:11:12.646
DidFailWithError.

00:11:12.646 --> 00:11:13.656
So this is in the case of the

00:11:13.656 --> 00:11:14.246
fatal error.

00:11:14.526 --> 00:11:16.096
Maybe you're running a device

00:11:16.096 --> 00:11:17.076
that doesn't support World

00:11:17.076 --> 00:11:17.506
Tracking.

00:11:17.616 --> 00:11:18.706
You'll get an error like this.

00:11:18.706 --> 00:11:19.926
And your session will be paused.

00:11:21.266 --> 00:11:22.466
The other way to make use of

00:11:22.466 --> 00:11:25.026
ARSessions processing is to pull

00:11:25.026 --> 00:11:26.176
the current frame property.

00:11:26.886 --> 00:11:29.346
So now, what does an ARFrame

00:11:29.346 --> 00:11:29.806
contain?

00:11:30.846 --> 00:11:32.806
Each ARFrame contains everything

00:11:32.806 --> 00:11:34.056
you need to render your

00:11:34.056 --> 00:11:35.146
augmented reality scene.

00:11:35.146 --> 00:11:37.686
The first thing it provides is a

00:11:37.686 --> 00:11:38.246
camera image.

00:11:38.876 --> 00:11:39.796
So this is what you're going to

00:11:39.796 --> 00:11:41.586
use to render the background of

00:11:41.626 --> 00:11:42.176
your scene.

00:11:43.636 --> 00:11:44.826
Next, it provides tracking

00:11:44.826 --> 00:11:47.226
information, or my device's

00:11:47.226 --> 00:11:48.786
orientation as well as location

00:11:49.106 --> 00:11:50.276
and even tracking state.

00:11:51.256 --> 00:11:52.776
And last, it provides scene

00:11:52.776 --> 00:11:53.316
understanding.

00:11:54.206 --> 00:11:55.576
So, information about the scene

00:11:55.856 --> 00:11:57.806
like feature points, physical

00:11:57.806 --> 00:11:59.186
locations in space as well as

00:11:59.186 --> 00:12:00.826
light estimation, or a light

00:12:01.596 --> 00:12:01.826
estimate.

00:12:02.866 --> 00:12:04.476
So, physical locations in space,

00:12:05.086 --> 00:12:07.746
the way that ARKit represents

00:12:07.746 --> 00:12:10.516
these is by using ARFrames -- or

00:12:10.516 --> 00:12:11.546
ARAnchors, sorry.

00:12:12.176 --> 00:12:14.276
An ARAnchor is a relative or a

00:12:14.276 --> 00:12:15.746
real-world position and

00:12:15.746 --> 00:12:17.796
orientation in space.

00:12:17.796 --> 00:12:21.306
ARAnchors can be added and

00:12:21.306 --> 00:12:22.746
removed from your scene.

00:12:23.166 --> 00:12:24.556
And they're used to basically

00:12:24.556 --> 00:12:27.566
represent a virtual content

00:12:27.566 --> 00:12:28.666
anchored to your physical

00:12:28.666 --> 00:12:29.076
environment.

00:12:29.826 --> 00:12:31.146
So, if you want to add a custom

00:12:31.146 --> 00:12:31.996
anchor, you can do that by

00:12:31.996 --> 00:12:32.866
adding it to your session.

00:12:33.066 --> 00:12:34.126
It'll persist through the

00:12:34.126 --> 00:12:35.286
lifetime of your session.

00:12:36.026 --> 00:12:37.736
But an added thing is if you're

00:12:37.736 --> 00:12:38.576
running things like plane

00:12:38.576 --> 00:12:40.236
detection, ARAnchors will be

00:12:40.236 --> 00:12:41.416
added automatically to your

00:12:41.416 --> 00:12:41.786
session.

00:12:42.416 --> 00:12:43.746
So, in order to respond to this,

00:12:44.396 --> 00:12:46.576
you can get them as a full list

00:12:46.576 --> 00:12:47.796
in your current ARFrame.

00:12:48.166 --> 00:12:49.086
So that'll have all of the

00:12:49.086 --> 00:12:50.136
anchors that your session is

00:12:50.136 --> 00:12:50.856
currently tracking.

00:12:51.356 --> 00:12:53.066
Or you can respond to delegate

00:12:53.066 --> 00:12:55.796
methods like add, update, and

00:12:55.796 --> 00:12:57.206
remove, which will notify you if

00:12:57.206 --> 00:12:59.056
anchors were added, updated, or

00:12:59.056 --> 00:13:00.746
removed from your session.

00:13:01.206 --> 00:13:04.276
So that concludes the four main

00:13:04.276 --> 00:13:05.366
classes that you're going to use

00:13:05.366 --> 00:13:06.546
to create augmented reality

00:13:06.546 --> 00:13:07.146
experience.

00:13:07.246 --> 00:13:10.676
Now let's talk about tracking in

00:13:10.676 --> 00:13:11.156
particular.

00:13:13.316 --> 00:13:15.636
So, tracking is the ability to

00:13:15.636 --> 00:13:17.096
determine a physical location in

00:13:17.096 --> 00:13:18.356
space in real time.

00:13:19.836 --> 00:13:20.476
This isn't easy.

00:13:20.856 --> 00:13:22.206
So, but it's essential for

00:13:22.206 --> 00:13:25.016
augmented reality to find your

00:13:25.016 --> 00:13:25.926
device's position.

00:13:25.926 --> 00:13:27.246
So not any position, but the

00:13:27.246 --> 00:13:28.456
position of your device and the

00:13:28.456 --> 00:13:30.036
orientation in order to render

00:13:30.036 --> 00:13:30.646
things correctly.

00:13:31.136 --> 00:13:31.866
So let's take a look at an

00:13:31.866 --> 00:13:32.196
example.

00:13:33.266 --> 00:13:34.836
Here I've placed a virtual chair

00:13:34.836 --> 00:13:36.646
and a virtual table in a

00:13:36.646 --> 00:13:37.546
physical environment.

00:13:38.236 --> 00:13:40.966
You'll notice that if I pan

00:13:40.966 --> 00:13:43.026
around it or reorient to my

00:13:43.026 --> 00:13:44.366
device, that they'll stay fixed

00:13:44.366 --> 00:13:45.006
in space.

00:13:45.316 --> 00:13:46.796
But more importantly, as I walk

00:13:46.796 --> 00:13:48.576
around the scene they also stay

00:13:48.616 --> 00:13:49.176
fixed in space.

00:13:50.056 --> 00:13:51.596
So this is because we're using,

00:13:52.096 --> 00:13:53.446
constantly updating the

00:13:53.446 --> 00:13:55.006
projection transform, or the

00:13:55.006 --> 00:13:55.896
projection matrix that we're

00:13:55.896 --> 00:13:57.606
using to render this virtual

00:13:57.606 --> 00:13:59.216
content so that it appears

00:13:59.216 --> 00:14:00.966
correct from any perspective.

00:14:02.916 --> 00:14:04.276
So now how do we do this?

00:14:05.696 --> 00:14:07.276
ARKit provides world tracking.

00:14:07.276 --> 00:14:09.206
This is our technology that uses

00:14:09.206 --> 00:14:10.546
visual inertial odometry.

00:14:10.896 --> 00:14:11.836
It's your camera images.

00:14:11.836 --> 00:14:12.966
It's the motion of your device.

00:14:12.966 --> 00:14:14.726
And it provides to you a

00:14:14.726 --> 00:14:16.826
rotation as well as a position

00:14:17.026 --> 00:14:18.536
or relative position, of your

00:14:18.536 --> 00:14:18.986
device.

00:14:19.496 --> 00:14:22.456
But more importantly, it

00:14:22.456 --> 00:14:24.016
provides real world scale.

00:14:24.636 --> 00:14:26.006
So all your virtual content is

00:14:26.006 --> 00:14:27.416
actually going to be to scale

00:14:27.656 --> 00:14:29.556
rendered in your physical scene.

00:14:30.196 --> 00:14:32.946
It also means that motion of

00:14:32.946 --> 00:14:34.346
your device correlates to

00:14:34.346 --> 00:14:35.676
physical distance traveled

00:14:36.016 --> 00:14:37.626
measured in meters.

00:14:40.516 --> 00:14:42.036
And all the positions given by

00:14:42.036 --> 00:14:43.586
tracking are relative to the

00:14:43.586 --> 00:14:44.646
starting position of your

00:14:44.646 --> 00:14:45.026
session.

00:14:46.896 --> 00:14:48.476
So one more function of how

00:14:48.476 --> 00:14:49.536
World Tracking works.

00:14:50.156 --> 00:14:51.716
We provide 3-D feature points.

00:14:52.486 --> 00:14:54.046
So, here's a representation of

00:14:54.436 --> 00:14:55.416
how World Tracking works.

00:14:55.566 --> 00:14:56.956
It works by detecting features,

00:14:56.956 --> 00:14:57.996
which are unique pieces of

00:14:57.996 --> 00:14:59.836
information, in a camera image.

00:15:00.616 --> 00:15:01.486
So you'll see the axes

00:15:01.486 --> 00:15:03.486
represents my device's position

00:15:03.486 --> 00:15:04.236
and orientation.

00:15:04.376 --> 00:15:05.796
It's creating a path as I move

00:15:05.796 --> 00:15:06.366
about my world.

00:15:06.406 --> 00:15:07.756
But you also see all these dots

00:15:07.756 --> 00:15:08.036
up here.

00:15:08.406 --> 00:15:09.936
These represent 3-D feature

00:15:09.936 --> 00:15:11.226
points that I've detected in my

00:15:11.226 --> 00:15:11.556
scene.

00:15:11.916 --> 00:15:13.386
I've been able to triangulate

00:15:13.386 --> 00:15:15.166
them by moving about the scene

00:15:15.536 --> 00:15:17.206
and then using these, matching

00:15:17.206 --> 00:15:19.276
these features, you'll see that

00:15:19.276 --> 00:15:20.606
I draw a line when I match an

00:15:20.606 --> 00:15:22.106
existing feature that I've seen

00:15:22.106 --> 00:15:22.496
before.

00:15:22.936 --> 00:15:24.166
And using all of this

00:15:24.166 --> 00:15:25.616
information and our motion data,

00:15:26.076 --> 00:15:27.796
we're able to precisely provide

00:15:28.966 --> 00:15:30.616
a device orientation and

00:15:30.826 --> 00:15:31.296
location.

00:15:31.886 --> 00:15:33.666
So that might look hard.

00:15:33.666 --> 00:15:35.636
Let's look at the code on how we

00:15:35.636 --> 00:15:36.766
run World Tracking.

00:15:37.276 --> 00:15:39.696
First thing you need to do is

00:15:39.696 --> 00:15:40.976
simply create an ARSession.

00:15:40.976 --> 00:15:42.266
Because again, it's going to

00:15:42.266 --> 00:15:43.506
manage all of the processing

00:15:43.616 --> 00:15:44.846
that's going to happen for World

00:15:44.846 --> 00:15:45.196
Tracking.

00:15:46.016 --> 00:15:47.306
Next, you'll set yourself as the

00:15:47.306 --> 00:15:49.396
delegate of the session so that

00:15:49.396 --> 00:15:50.696
you can receive updates on when

00:15:50.696 --> 00:15:51.746
new frames are available.

00:15:53.146 --> 00:15:54.306
By creating a World Tracking

00:15:54.306 --> 00:15:55.166
session configuration you're

00:15:55.166 --> 00:15:56.266
saying, "I want to use World

00:15:56.266 --> 00:15:56.656
Tracking.

00:15:56.656 --> 00:15:58.096
I want my session to run this

00:15:58.096 --> 00:15:58.566
processing."

00:15:59.316 --> 00:16:00.476
Then by simply calling Run,

00:16:00.916 --> 00:16:01.956
immediately processing will

00:16:01.956 --> 00:16:02.366
happen.

00:16:02.366 --> 00:16:03.656
Capturing will begin.

00:16:04.316 --> 00:16:05.636
So, under the hood, our session

00:16:05.916 --> 00:16:08.266
creates an AVCaptureSession --

00:16:08.476 --> 00:16:09.916
sorry, as well as a

00:16:09.996 --> 00:16:12.136
CMMotionManager in order to get

00:16:12.136 --> 00:16:13.436
image and motion data.

00:16:14.476 --> 00:16:15.546
We use the images to detect

00:16:15.546 --> 00:16:16.396
features in the scene.

00:16:16.976 --> 00:16:18.336
And we use the motion data at a

00:16:18.496 --> 00:16:19.506
higher rate in order to

00:16:19.506 --> 00:16:21.046
integrate it over time to get

00:16:21.046 --> 00:16:21.916
your device's motion.

00:16:23.076 --> 00:16:24.886
Using these together we're able

00:16:24.886 --> 00:16:26.566
to use sensor fusion in order to

00:16:26.566 --> 00:16:27.846
provide a precise pose.

00:16:27.846 --> 00:16:29.776
So these are returned in

00:16:29.776 --> 00:16:30.336
ARFrames.

00:16:30.906 --> 00:16:34.986
Each ARFrame is going to include

00:16:34.986 --> 00:16:35.666
an ARCamera.

00:16:36.266 --> 00:16:39.406
So an ARCamera is the object

00:16:39.406 --> 00:16:40.876
that represents a virtual

00:16:40.876 --> 00:16:41.136
camera.

00:16:41.136 --> 00:16:42.106
Or you can use it for a virtual

00:16:42.106 --> 00:16:42.386
camera.

00:16:42.546 --> 00:16:43.826
It represents your device's

00:16:43.826 --> 00:16:45.326
orientation as well as location.

00:16:45.816 --> 00:16:47.196
So it provides a transform.

00:16:47.776 --> 00:16:49.426
Transform is a matrix or a

00:16:49.426 --> 00:16:51.216
[inaudible] float 4 by 4 which

00:16:51.216 --> 00:16:52.956
provides the orientation or the

00:16:52.956 --> 00:16:54.706
rotation as well as translation

00:16:55.136 --> 00:16:56.576
of your physical device from the

00:16:56.576 --> 00:16:57.836
starting point of the session.

00:16:59.026 --> 00:17:00.156
In addition to this we provide a

00:17:00.156 --> 00:17:02.026
tracking state, which informs

00:17:02.026 --> 00:17:03.076
you on how you can use the

00:17:03.076 --> 00:17:03.596
transform.

00:17:04.576 --> 00:17:06.586
And last, we provide camera

00:17:06.586 --> 00:17:07.276
intrinsics.

00:17:07.986 --> 00:17:09.215
So camera intrinsics are really

00:17:09.215 --> 00:17:10.806
important that we get them each

00:17:10.806 --> 00:17:12.476
frame because it matches that of

00:17:12.476 --> 00:17:13.715
the physical camera on your

00:17:13.715 --> 00:17:14.146
device.

00:17:14.726 --> 00:17:15.925
This information like focal

00:17:15.925 --> 00:17:17.096
length and principal point,

00:17:17.356 --> 00:17:18.435
which are used to find a

00:17:18.435 --> 00:17:19.226
projection matrix.

00:17:20.096 --> 00:17:21.876
The projection matrix is also a

00:17:21.876 --> 00:17:23.096
convenience method on ARCamera.

00:17:23.096 --> 00:17:24.886
So you can easily use that to

00:17:24.886 --> 00:17:26.406
render your virtual geometry.

00:17:26.945 --> 00:17:30.516
So with that, that is tracking

00:17:30.516 --> 00:17:31.416
that ARKit provides.

00:17:31.526 --> 00:17:32.326
Let's go ahead and look at a

00:17:32.326 --> 00:17:34.036
demo using World Tracking and

00:17:34.036 --> 00:17:35.136
create your first ARKit

00:17:35.136 --> 00:17:35.776
application.

00:17:36.516 --> 00:17:42.116
[ Applause ]

00:17:42.616 --> 00:17:43.506
So, the first thing that you

00:17:43.506 --> 00:17:45.506
notice when you open new Xcode 9

00:17:45.786 --> 00:17:47.046
is that there's a new template

00:17:47.046 --> 00:17:48.586
available for creating augmented

00:17:48.586 --> 00:17:49.296
reality apps.

00:17:49.296 --> 00:17:50.526
So let's go ahead and select

00:17:50.526 --> 00:17:51.146
that.

00:17:51.326 --> 00:17:52.596
I'm going to create an augmented

00:17:52.596 --> 00:17:53.356
reality app.

00:17:53.356 --> 00:17:55.246
Hit Next. After giving my

00:17:55.246 --> 00:17:57.816
project a name like MyARApp, I

00:17:58.416 --> 00:17:59.666
can choose between the language,

00:18:00.386 --> 00:18:01.186
which here I have the option

00:18:01.186 --> 00:18:02.456
between Swift as well as

00:18:02.456 --> 00:18:04.626
ObjectiveC as well as the

00:18:04.626 --> 00:18:05.576
content technology.

00:18:05.776 --> 00:18:07.536
So the content technology is

00:18:07.536 --> 00:18:08.686
what you're going to use to

00:18:08.686 --> 00:18:10.126
render your augmented reality

00:18:10.126 --> 00:18:10.256
scene.

00:18:10.256 --> 00:18:11.876
You have the option between

00:18:11.876 --> 00:18:14.016
SceneKit, SpriteKit as well as

00:18:14.016 --> 00:18:14.286
Metal.

00:18:14.336 --> 00:18:16.366
I'm going to use SceneKit for

00:18:16.366 --> 00:18:16.936
this example.

00:18:17.386 --> 00:18:20.146
So after hitting Next and

00:18:20.146 --> 00:18:21.546
creating my workspace, it looks

00:18:21.546 --> 00:18:22.286
something like this.

00:18:23.216 --> 00:18:24.286
Here I have a view controller

00:18:24.356 --> 00:18:25.526
that I've created.

00:18:25.716 --> 00:18:26.646
You'll see that it has an

00:18:26.646 --> 00:18:27.496
ARSCNView.

00:18:28.066 --> 00:18:31.196
So this ARSCNView is a custom AR

00:18:31.196 --> 00:18:32.466
subclass that implements all the

00:18:32.466 --> 00:18:33.696
rendering -- or most of the

00:18:33.696 --> 00:18:34.386
rendering for me.

00:18:35.096 --> 00:18:36.426
So it'll handle updating my

00:18:36.426 --> 00:18:38.046
virtual camera based on the

00:18:38.046 --> 00:18:39.506
ARFrames that get returned to

00:18:39.506 --> 00:18:39.576
it.

00:18:40.116 --> 00:18:42.366
As a property of ARSCNView, or

00:18:42.366 --> 00:18:44.536
my sceneView, it has a session.

00:18:45.276 --> 00:18:47.676
So you see that my sceneView, I

00:18:47.676 --> 00:18:49.116
set a scene, which is going to

00:18:49.116 --> 00:18:50.376
be a ship that's translated a

00:18:50.376 --> 00:18:51.756
little bit in front of the world

00:18:51.756 --> 00:18:53.656
origin along the z-axis.

00:18:54.096 --> 00:18:55.596
And then the most important part

00:18:55.596 --> 00:18:57.366
is I'm accessing the session --

00:18:57.956 --> 00:19:00.436
I'm accessing the session and

00:19:00.436 --> 00:19:01.986
calling Run with a World

00:19:01.986 --> 00:19:03.396
Tracking session configuration.

00:19:03.906 --> 00:19:05.156
So this will run World Tracking.

00:19:05.156 --> 00:19:06.456
And automatically the view will

00:19:06.456 --> 00:19:07.676
handle updating my virtual

00:19:07.676 --> 00:19:08.646
camera for me.

00:19:09.766 --> 00:19:10.586
So let's go ahead and give that

00:19:10.586 --> 00:19:10.956
a try.

00:19:11.356 --> 00:19:12.806
Maybe I'm going to change our

00:19:12.806 --> 00:19:15.086
standard ship to use arship.

00:19:16.936 --> 00:19:19.926
So let's run this on the device.

00:19:25.336 --> 00:19:26.526
So after installing, the first

00:19:26.526 --> 00:19:27.856
thing that you'll notice is that

00:19:27.856 --> 00:19:28.846
it's going to ask for camera

00:19:28.846 --> 00:19:29.176
permission.

00:19:29.546 --> 00:19:30.626
This is a required to use

00:19:30.626 --> 00:19:32.006
tracking as well as render the

00:19:32.006 --> 00:19:32.926
backdrop of your scene.

00:19:33.796 --> 00:19:34.786
Next, as you'll see, I get a

00:19:34.786 --> 00:19:35.306
camera feed.

00:19:35.306 --> 00:19:36.706
And right in front of me there's

00:19:36.706 --> 00:19:37.176
a spaceship.

00:19:37.916 --> 00:19:39.236
You'll see as I change the

00:19:39.236 --> 00:19:40.516
orientation of my device, it

00:19:40.516 --> 00:19:41.996
stays fixed in space.

00:19:42.626 --> 00:19:44.236
But more importantly, as I move

00:19:44.236 --> 00:19:46.696
about the spaceship, you'll see

00:19:46.696 --> 00:19:48.196
that it actually is anchored in

00:19:48.196 --> 00:19:49.036
the physical world.

00:19:49.686 --> 00:19:51.476
So this is using both my

00:19:51.476 --> 00:19:52.856
device's orientation as well as

00:19:52.856 --> 00:19:54.556
a relative position to update a

00:19:54.556 --> 00:19:57.586
virtual camera and look at the

00:19:57.586 --> 00:19:58.226
spaceship.

00:19:59.016 --> 00:20:00.326
[ Applause ]

00:20:00.326 --> 00:20:00.856
Thank you.

00:20:02.516 --> 00:20:06.696
[ Applause ]

00:20:07.196 --> 00:20:08.566
So, if that's not interesting

00:20:08.566 --> 00:20:09.756
enough for you, maybe we want to

00:20:09.756 --> 00:20:10.946
add something to the scene every

00:20:10.946 --> 00:20:11.876
time we tap the screen.

00:20:12.546 --> 00:20:13.206
Let's try that out.

00:20:13.206 --> 00:20:14.486
Let's try adding something to

00:20:14.486 --> 00:20:14.996
this example.

00:20:14.996 --> 00:20:18.426
So as I said, I want to add

00:20:18.746 --> 00:20:20.036
geometry to the scene every time

00:20:20.036 --> 00:20:20.956
I tap the screen.

00:20:21.696 --> 00:20:22.626
First thing I need to do to do

00:20:22.626 --> 00:20:25.266
that is add a tap gesture

00:20:25.266 --> 00:20:25.796
recognizer.

00:20:26.016 --> 00:20:28.766
So after adding that to my scene

00:20:28.806 --> 00:20:30.766
view, every time I call the

00:20:30.766 --> 00:20:32.766
handle tap method, or every time

00:20:32.766 --> 00:20:33.946
I tap the screen, the handle tap

00:20:33.946 --> 00:20:34.816
method will get called.

00:20:35.696 --> 00:20:39.416
So let's implement that.

00:20:39.556 --> 00:20:40.546
So, if I want to create some

00:20:40.546 --> 00:20:41.896
geometry, let's say I'm going to

00:20:41.896 --> 00:20:43.706
create a plane or an image

00:20:43.706 --> 00:20:43.946
plane.

00:20:44.546 --> 00:20:47.886
So the first thing I do here is

00:20:47.886 --> 00:20:49.346
create an SCNPlane with a width

00:20:49.346 --> 00:20:49.676
and height.

00:20:49.676 --> 00:20:51.676
But then, the tricky part, I'm

00:20:51.676 --> 00:20:52.736
actually going to set the

00:20:52.796 --> 00:20:54.816
contents -- or the material, to

00:20:54.816 --> 00:20:57.016
be a snapshot of my view.

00:20:57.016 --> 00:20:59.796
So what do you think this is

00:20:59.796 --> 00:21:00.826
going to be?

00:21:00.986 --> 00:21:02.076
Well, this actually going to

00:21:02.076 --> 00:21:03.286
take a snapshot or a rendering

00:21:03.286 --> 00:21:04.746
of my view including the

00:21:04.916 --> 00:21:07.306
backdrop camera image as well as

00:21:07.306 --> 00:21:08.606
the virtual geometry that I've

00:21:08.706 --> 00:21:09.446
placed in front of it.

00:21:10.136 --> 00:21:11.416
I'm setting my lighting model to

00:21:11.416 --> 00:21:12.886
constant so that the light

00:21:12.886 --> 00:21:14.266
estimate provided by ARKit

00:21:14.516 --> 00:21:15.446
doesn't get applied to this

00:21:15.446 --> 00:21:16.746
camera image because it's

00:21:16.746 --> 00:21:17.646
already going to match the

00:21:17.646 --> 00:21:18.330
environment.

00:21:20.146 --> 00:21:21.346
Next, I need to add this to the

00:21:21.346 --> 00:21:21.696
scene.

00:21:22.126 --> 00:21:22.946
So in order to do that, I'm

00:21:22.946 --> 00:21:24.926
going to create a plane node.

00:21:28.116 --> 00:21:29.776
So, after creating an SCNode

00:21:29.776 --> 00:21:31.686
that encapsulates this geometry,

00:21:31.686 --> 00:21:32.576
I add it to the scene.

00:21:33.306 --> 00:21:34.566
So already here, every time I

00:21:34.566 --> 00:21:35.766
tap the screen, it's going to

00:21:35.766 --> 00:21:37.296
add an image plane to my scene.

00:21:37.296 --> 00:21:38.656
But the problem is it's always

00:21:38.656 --> 00:21:39.806
going to be at 000.

00:21:40.546 --> 00:21:41.336
So how do I make this more

00:21:41.336 --> 00:21:41.756
interesting?

00:21:42.546 --> 00:21:44.756
Well, we have provided to us a

00:21:44.756 --> 00:21:46.256
current frame, which contains an

00:21:46.256 --> 00:21:46.676
AR Camera.

00:21:47.806 --> 00:21:49.316
Which I could probably use the

00:21:49.746 --> 00:21:51.386
camera's transform in order to

00:21:51.386 --> 00:21:52.446
update the plane node's

00:21:52.446 --> 00:21:54.306
transform so that the plane node

00:21:54.996 --> 00:21:56.436
is where my camera currently is

00:21:56.436 --> 00:21:57.226
located in space.

00:21:58.446 --> 00:21:59.546
To do that, I'm going to first

00:21:59.546 --> 00:22:01.296
get the current frame from my

00:22:01.296 --> 00:22:02.206
SceneView session.

00:22:04.116 --> 00:22:05.066
Next, I'm going to update the

00:22:05.146 --> 00:22:06.096
plane node's transform

00:22:08.296 --> 00:22:10.156
in order to use the transform of

00:22:10.216 --> 00:22:11.000
my camera.

00:22:15.076 --> 00:22:16.346
So here you'll notice the first

00:22:16.346 --> 00:22:17.606
thing I do I actually create the

00:22:17.606 --> 00:22:18.546
translation matrix.

00:22:19.036 --> 00:22:20.096
Because I don't want to put the

00:22:20.096 --> 00:22:20.966
image plane right where the

00:22:20.966 --> 00:22:22.406
camera's located and obstruct my

00:22:22.406 --> 00:22:23.586
view, I want to place it in

00:22:23.586 --> 00:22:24.296
front of the camera.

00:22:24.796 --> 00:22:25.716
So for this I'm going to use the

00:22:25.716 --> 00:22:27.576
negative z-axis as a

00:22:27.576 --> 00:22:28.286
translation.

00:22:29.276 --> 00:22:30.686
You'll also see that in order to

00:22:30.686 --> 00:22:32.456
get some scale, everything is in

00:22:32.456 --> 00:22:32.826
meters.

00:22:32.826 --> 00:22:34.636
So I'm going to use .1 to

00:22:34.636 --> 00:22:36.296
represent 10 centimeters in

00:22:36.296 --> 00:22:37.426
front of my camera.

00:22:37.956 --> 00:22:39.126
By multiplying this together

00:22:39.276 --> 00:22:41.036
with my camera's transform and

00:22:41.036 --> 00:22:42.746
applying this to my plane node,

00:22:43.286 --> 00:22:44.446
this will be an image plane

00:22:44.816 --> 00:22:46.206
located 10 centimeters in front

00:22:46.206 --> 00:22:46.616
of the camera.

00:22:47.836 --> 00:22:48.936
So let's try this out and see

00:22:48.936 --> 00:22:50.000
what it looks like.

00:22:58.416 --> 00:23:00.046
So, as you see here again, I

00:23:00.046 --> 00:23:01.876
have the camera scene running.

00:23:01.876 --> 00:23:03.666
And I have my spaceship floating

00:23:03.666 --> 00:23:04.156
in space.

00:23:06.456 --> 00:23:07.936
Now, if I tap the screen maybe

00:23:08.006 --> 00:23:10.486
here, here and here, you'll see

00:23:10.486 --> 00:23:12.086
that it leaves a snapshot or an

00:23:12.086 --> 00:23:13.536
image floating in space where I

00:23:13.536 --> 00:23:13.976
took it.

00:23:14.516 --> 00:23:21.846
[ Applause ]

00:23:22.346 --> 00:23:23.276
This shows just one of the

00:23:23.346 --> 00:23:24.716
possibilities that you can use

00:23:24.826 --> 00:23:25.466
ARKit for.

00:23:25.556 --> 00:23:27.496
And it really makes for a cool

00:23:27.656 --> 00:23:28.356
experience.

00:23:28.746 --> 00:23:30.996
Thank you.

00:23:30.996 --> 00:23:32.136
And that's using ARKit.

00:23:33.516 --> 00:23:40.706
[ Applause ]

00:23:41.206 --> 00:23:43.026
So, now that you've seen a demo

00:23:43.026 --> 00:23:44.746
using ARKit's tracking, let's

00:23:44.746 --> 00:23:45.916
talk about getting the best

00:23:45.916 --> 00:23:47.086
quality from your tracking

00:23:47.086 --> 00:23:47.546
results.

00:23:49.016 --> 00:23:50.386
First thing to note is that

00:23:50.386 --> 00:23:51.956
tracking relies on uninterrupted

00:23:51.956 --> 00:23:52.486
sensor data.

00:23:52.836 --> 00:23:54.256
This just means if camera images

00:23:54.256 --> 00:23:55.466
are no longer being provided to

00:23:55.466 --> 00:23:57.046
your session, tracking will

00:23:57.046 --> 00:23:57.396
stop.

00:23:57.776 --> 00:24:00.616
We'll be unable to track.

00:24:00.616 --> 00:24:02.116
Next, tracking works best in

00:24:02.116 --> 00:24:03.386
well-textured environments.

00:24:04.056 --> 00:24:05.486
This means we need enough visual

00:24:05.486 --> 00:24:07.026
complexity in order to find

00:24:07.026 --> 00:24:08.246
features from your camera

00:24:08.246 --> 00:24:08.556
images.

00:24:09.236 --> 00:24:11.036
So if I'm facing a white wall or

00:24:11.036 --> 00:24:11.976
if there's not enough light in

00:24:11.976 --> 00:24:13.366
the room, I will be unable to

00:24:13.406 --> 00:24:14.616
find features.

00:24:14.876 --> 00:24:15.966
And tracking will be limited.

00:24:16.356 --> 00:24:19.056
Next, tracking also works best

00:24:19.056 --> 00:24:20.026
in static scenes.

00:24:20.416 --> 00:24:21.556
So if too much of what my camera

00:24:21.556 --> 00:24:23.676
sees is moving, visual data

00:24:23.676 --> 00:24:25.086
won't correspond to motion data,

00:24:25.426 --> 00:24:27.056
which may result in drift, which

00:24:27.056 --> 00:24:28.506
is also a limited tracking

00:24:28.506 --> 00:24:28.826
state.

00:24:29.796 --> 00:24:31.576
So to help with these, ARCamera

00:24:31.676 --> 00:24:33.716
provides a tracking state

00:24:33.766 --> 00:24:34.156
property.

00:24:35.786 --> 00:24:37.116
Tracking state has three

00:24:37.116 --> 00:24:39.406
possible values: Not Available,

00:24:39.986 --> 00:24:41.226
Normal, and Limited.

00:24:42.036 --> 00:24:42.916
When you first start your

00:24:42.916 --> 00:24:44.886
session, it begins in Not

00:24:44.886 --> 00:24:45.306
Available.

00:24:45.826 --> 00:24:46.546
This just means that your

00:24:46.546 --> 00:24:48.016
camera's transform has not yet

00:24:48.016 --> 00:24:49.616
been populated and is the

00:24:49.616 --> 00:24:50.466
identity matrix.

00:24:51.896 --> 00:24:53.446
Soon after, once we find our

00:24:53.446 --> 00:24:55.076
first tracking pose, the state

00:24:55.076 --> 00:24:56.326
will change from Not Available

00:24:56.866 --> 00:24:57.286
to Normal.

00:24:58.226 --> 00:24:59.376
This signifies that you can now

00:24:59.376 --> 00:25:00.956
use your camera's transform.

00:25:01.336 --> 00:25:04.596
If at any later point after this

00:25:04.726 --> 00:25:05.896
tracing becomes limited,

00:25:06.066 --> 00:25:07.376
tracking state will change from

00:25:07.376 --> 00:25:09.786
Normal to Limited, and also

00:25:09.786 --> 00:25:10.576
provide a reason.

00:25:11.086 --> 00:25:12.106
So, the reason in this case,

00:25:12.106 --> 00:25:13.356
because I'm facing a white wall

00:25:13.356 --> 00:25:14.786
or there's not enough light, is

00:25:14.836 --> 00:25:15.896
Insufficient Features.

00:25:15.896 --> 00:25:18.776
It's helpful to notify your

00:25:18.776 --> 00:25:19.926
users when this happens.

00:25:20.156 --> 00:25:21.816
So, to do that, we're providing

00:25:22.466 --> 00:25:23.736
a session delegate method that

00:25:23.736 --> 00:25:24.276
you can implement:

00:25:24.726 --> 00:25:26.126
cameraDidChangeTrackingState.

00:25:26.956 --> 00:25:27.996
So when this happens, you can

00:25:27.996 --> 00:25:29.686
get the tracking state, if it's

00:25:29.686 --> 00:25:31.296
limited, as well as the reason.

00:25:32.436 --> 00:25:33.716
And from this you'll notify your

00:25:33.716 --> 00:25:34.086
users.

00:25:34.126 --> 00:25:35.216
Because they're the only ones

00:25:35.266 --> 00:25:36.676
that can actually fix the

00:25:36.676 --> 00:25:38.316
tracking situation by either

00:25:38.316 --> 00:25:39.996
turning the lights up or not

00:25:39.996 --> 00:25:40.836
facing a white wall.

00:25:41.376 --> 00:25:46.006
The other part is if sensor data

00:25:46.006 --> 00:25:46.856
becomes unavailable.

00:25:47.896 --> 00:25:49.246
So, for this, we handle this by

00:25:49.246 --> 00:25:50.286
session interruptions.

00:25:51.446 --> 00:25:52.706
So, if your camera input is

00:25:52.706 --> 00:25:54.396
unavailable due to -- the main

00:25:54.396 --> 00:25:55.416
reasons being your app gets

00:25:55.416 --> 00:25:56.906
backgrounded or maybe you're

00:25:56.906 --> 00:25:59.146
doing multitasking on an iPad,

00:25:59.146 --> 00:26:00.576
camera images also won't be

00:26:00.576 --> 00:26:01.506
provided to your session.

00:26:02.226 --> 00:26:03.376
In this case tracking will

00:26:03.376 --> 00:26:05.456
become unavailable or stopped

00:26:05.586 --> 00:26:06.846
and your session will be

00:26:06.846 --> 00:26:07.366
interrupted.

00:26:07.736 --> 00:26:09.006
So, to deal with this, we also

00:26:09.006 --> 00:26:10.996
provide delegate methods to make

00:26:10.996 --> 00:26:11.656
it really easy.

00:26:12.666 --> 00:26:15.086
Here it's a good idea to present

00:26:15.086 --> 00:26:16.226
an overlay or maybe blur your

00:26:16.226 --> 00:26:17.696
screen to signify to the user

00:26:18.016 --> 00:26:18.876
that your experience is

00:26:18.876 --> 00:26:20.626
currently paused and no tracking

00:26:20.626 --> 00:26:21.166
is occurring.

00:26:22.056 --> 00:26:23.636
During an interruption, it's

00:26:23.636 --> 00:26:26.006
also important to note that

00:26:26.156 --> 00:26:26.996
because no tracking is

00:26:26.996 --> 00:26:28.696
happening, the relative position

00:26:28.696 --> 00:26:29.656
of your device won't be

00:26:29.656 --> 00:26:30.076
available.

00:26:30.696 --> 00:26:32.406
So if you had anchors or

00:26:32.406 --> 00:26:33.866
physical locations in the scene,

00:26:34.286 --> 00:26:35.806
they may no longer be aligned if

00:26:35.806 --> 00:26:36.966
there was movement during this

00:26:36.966 --> 00:26:37.486
interruption.

00:26:38.696 --> 00:26:39.916
So for this, you may want to

00:26:40.096 --> 00:26:41.046
optionally restart your

00:26:41.046 --> 00:26:42.406
experience when you come back

00:26:42.406 --> 00:26:43.096
from an interruption.

00:26:43.096 --> 00:26:47.026
And so that's tracking.

00:26:47.026 --> 00:26:49.396
Let's go ahead and hand it over

00:26:49.396 --> 00:26:50.586
to Stefan to talk about scene

00:26:50.586 --> 00:26:51.086
understanding.

00:26:51.086 --> 00:26:51.476
Thank you.

00:26:52.516 --> 00:26:57.396
[ Applause ]

00:26:57.896 --> 00:26:58.346
>> Thank you, Mike.

00:26:59.696 --> 00:27:00.726
Good afternoon everyone.

00:27:01.316 --> 00:27:02.456
My name is Stefan Misslinger.

00:27:02.636 --> 00:27:03.966
I'm an engineer on the ARKit

00:27:03.966 --> 00:27:04.316
team.

00:27:04.726 --> 00:27:05.716
And next we're going to talk

00:27:05.716 --> 00:27:06.856
about scene understanding.

00:27:07.246 --> 00:27:08.476
So the goal of scene

00:27:08.476 --> 00:27:09.776
understanding is to find out

00:27:09.826 --> 00:27:11.586
more about our environment in

00:27:11.586 --> 00:27:13.196
order to place virtual objects

00:27:13.276 --> 00:27:14.346
into this environment.

00:27:15.116 --> 00:27:16.686
This includes information like

00:27:16.746 --> 00:27:18.186
the 3-D topology of our

00:27:18.186 --> 00:27:19.806
environment as well as the

00:27:19.806 --> 00:27:21.706
lighting situation in order to

00:27:22.016 --> 00:27:24.256
realistically place an object

00:27:24.256 --> 00:27:24.466
there.

00:27:24.536 --> 00:27:27.646
Let's look at an example of this

00:27:27.646 --> 00:27:28.236
table here.

00:27:29.176 --> 00:27:30.626
If you want to place an object,

00:27:30.726 --> 00:27:32.086
a virtual object, onto this

00:27:32.086 --> 00:27:33.496
table, the first thing we need

00:27:33.496 --> 00:27:35.006
to know is that there is a

00:27:35.006 --> 00:27:36.366
surface on which we can place

00:27:36.366 --> 00:27:36.766
something.

00:27:37.516 --> 00:27:39.386
And this is done by using plane

00:27:39.386 --> 00:27:39.846
detection.

00:27:41.356 --> 00:27:43.626
Second, we need to figure out a

00:27:43.626 --> 00:27:46.236
3-D coordinate on which we place

00:27:46.236 --> 00:27:47.146
our virtual object.

00:27:47.726 --> 00:27:49.296
In order to find this we are

00:27:49.296 --> 00:27:50.426
using hit-testing.

00:27:51.006 --> 00:27:52.736
This involves sending a ray from

00:27:52.736 --> 00:27:54.556
our device and intersecting it

00:27:54.556 --> 00:27:55.776
with the real world in order to

00:27:55.776 --> 00:27:56.846
find this coordinate.

00:27:57.316 --> 00:28:01.286
And third, in order to place

00:28:01.506 --> 00:28:03.406
this object in a realistic way

00:28:03.756 --> 00:28:06.076
we need a light estimation to

00:28:06.076 --> 00:28:07.416
match the lighting of our

00:28:07.416 --> 00:28:07.986
environment.

00:28:08.896 --> 00:28:10.106
Let's have a look at each one of

00:28:10.106 --> 00:28:11.886
those three things starting with

00:28:11.886 --> 00:28:12.576
plane detection.

00:28:13.066 --> 00:28:15.806
So, plane detection provides you

00:28:15.806 --> 00:28:17.556
with horizontal planes with

00:28:17.556 --> 00:28:18.636
respect to gravity.

00:28:19.526 --> 00:28:20.816
This includes planes like the

00:28:20.816 --> 00:28:22.406
ground plane as well as any

00:28:22.406 --> 00:28:25.146
parallel planes like tables.

00:28:25.586 --> 00:28:28.536
ARKit does this by aggregating

00:28:28.536 --> 00:28:30.356
information over multiple frames

00:28:30.876 --> 00:28:32.116
so it runs in the background.

00:28:32.666 --> 00:28:34.346
And as the user moves their

00:28:34.346 --> 00:28:35.746
device around the scene, it

00:28:35.746 --> 00:28:37.336
learns more about this plane.

00:28:38.816 --> 00:28:42.216
This also allows us to retrieve

00:28:42.216 --> 00:28:44.006
an aligned extent of this plane,

00:28:44.106 --> 00:28:45.766
which means that we're fitting a

00:28:45.766 --> 00:28:47.716
rectangle around all detected

00:28:47.836 --> 00:28:50.166
parts of this plane and align it

00:28:50.236 --> 00:28:51.326
with the major extent.

00:28:51.606 --> 00:28:54.046
So this gives you an idea of the

00:28:54.046 --> 00:28:55.886
major orientation of a physical

00:28:55.916 --> 00:28:56.216
plane.

00:28:58.096 --> 00:28:59.936
Furthermore, if there are

00:28:59.936 --> 00:29:01.626
multiple virtual planes detected

00:29:01.626 --> 00:29:02.896
for the same physical plane,

00:29:02.956 --> 00:29:04.626
ARKit will handle merging those

00:29:04.626 --> 00:29:05.036
together.

00:29:06.276 --> 00:29:08.356
Then the combined plane will

00:29:08.356 --> 00:29:11.456
grow to the extent of both

00:29:11.456 --> 00:29:13.436
planes, hence the newer plane

00:29:13.436 --> 00:29:14.306
will be removed from the

00:29:14.306 --> 00:29:14.686
session.

00:29:15.226 --> 00:29:17.146
Let's have a look at how it's

00:29:17.146 --> 00:29:17.946
used as in code.

00:29:19.936 --> 00:29:21.986
The first thing you want to do

00:29:22.166 --> 00:29:23.626
is create an ARWorldTracking

00:29:23.626 --> 00:29:24.676
session configuration.

00:29:25.666 --> 00:29:26.636
And plane detection is a

00:29:26.636 --> 00:29:28.216
property you can set on an

00:29:28.216 --> 00:29:29.476
ARWorldTracking session

00:29:29.476 --> 00:29:30.206
configuration.

00:29:30.576 --> 00:29:31.996
So, to enable plane detection,

00:29:32.396 --> 00:29:33.716
you simple set the plane

00:29:33.716 --> 00:29:34.876
detection property to

00:29:34.876 --> 00:29:35.496
Horizontal.

00:29:36.746 --> 00:29:38.506
After that, you pass the

00:29:38.506 --> 00:29:40.106
configuration back to the

00:29:40.166 --> 00:29:41.336
ARSession by calling the Run

00:29:41.336 --> 00:29:41.756
method.

00:29:42.096 --> 00:29:43.636
And it will start detecting

00:29:43.636 --> 00:29:44.836
planes in your environment.

00:29:47.176 --> 00:29:48.686
If you want to turn off plane

00:29:48.686 --> 00:29:51.996
detection, we simply set the

00:29:51.996 --> 00:29:53.306
plane detection property to

00:29:53.306 --> 00:29:53.666
None.

00:29:54.176 --> 00:29:56.406
And then call the Run method on

00:29:56.466 --> 00:29:57.266
ARSession again.

00:29:58.076 --> 00:29:59.586
Any previously detected planes

00:29:59.746 --> 00:30:01.116
in the session will remain.

00:30:01.306 --> 00:30:03.666
That means they will be still

00:30:03.666 --> 00:30:06.226
present in our ARFrames anchors.

00:30:07.916 --> 00:30:10.226
So whenever a new plane has been

00:30:10.226 --> 00:30:12.306
detected, they will be surfaced

00:30:12.306 --> 00:30:13.756
to you as ARPlaneAnchors.

00:30:15.046 --> 00:30:17.156
An ARPlaneAnchor is a subclass

00:30:17.156 --> 00:30:18.866
of an ARAnchor, which means it

00:30:18.866 --> 00:30:20.636
represents a real-world position

00:30:20.636 --> 00:30:21.476
and orientation.

00:30:23.196 --> 00:30:24.766
Whenever a new anchor is being

00:30:24.766 --> 00:30:26.646
detected you will receive a

00:30:26.646 --> 00:30:28.596
delegate call session didAdd

00:30:28.596 --> 00:30:29.056
anchor.

00:30:29.716 --> 00:30:31.056
And you can use that, for

00:30:31.056 --> 00:30:32.256
example, to visualize your

00:30:32.256 --> 00:30:32.606
plane.

00:30:34.056 --> 00:30:35.426
The extent of the plane will be

00:30:35.426 --> 00:30:40.056
surfaced to you as the extent,

00:30:40.056 --> 00:30:41.786
which is in respect to a center

00:30:41.786 --> 00:30:42.366
property.

00:30:42.966 --> 00:30:46.256
So as the user moves the device

00:30:46.256 --> 00:30:47.886
around the scene, we'll learn

00:30:47.886 --> 00:30:49.266
more about this plane and can

00:30:49.266 --> 00:30:50.186
update its extent.

00:30:50.186 --> 00:30:53.676
When this happens you will

00:30:53.676 --> 00:30:55.316
receive a delegate session

00:30:55.316 --> 00:30:57.316
didUpdate frame -- or didUpdate

00:30:57.316 --> 00:30:57.676
anchor.

00:30:58.796 --> 00:31:00.726
And you can use that to update

00:31:00.726 --> 00:31:01.556
your visualization.

00:31:02.566 --> 00:31:04.096
Notice how the center property

00:31:04.096 --> 00:31:06.306
actually moved because the plane

00:31:06.306 --> 00:31:07.656
grew more into one direction

00:31:07.656 --> 00:31:08.126
than another.

00:31:11.016 --> 00:31:13.156
Whenever an anchor is being

00:31:13.156 --> 00:31:14.636
removed from the session, you

00:31:14.636 --> 00:31:16.076
will receive a delegate called

00:31:16.076 --> 00:31:17.486
session didRemove anchor.

00:31:18.566 --> 00:31:21.216
This can happen if ARKits merges

00:31:21.356 --> 00:31:22.986
planes together and removes one

00:31:22.986 --> 00:31:23.906
of them as a result.

00:31:24.646 --> 00:31:26.876
In that case, you will receive a

00:31:26.876 --> 00:31:28.616
delegate call session didRemove

00:31:28.616 --> 00:31:30.176
anchor, and you can update your

00:31:30.176 --> 00:31:31.406
visualization accordingly.

00:31:31.986 --> 00:31:35.286
So now that we have an idea of

00:31:35.356 --> 00:31:36.626
where there are planes in our

00:31:36.626 --> 00:31:38.066
environment, let's have a look

00:31:38.066 --> 00:31:39.216
at how to actually place

00:31:39.276 --> 00:31:40.116
something into this.

00:31:40.536 --> 00:31:42.216
And for this we provide

00:31:42.276 --> 00:31:42.876
hit-testing.

00:31:43.426 --> 00:31:47.166
So hit-testing involves sending

00:31:47.166 --> 00:31:48.406
or intersecting a ray

00:31:48.406 --> 00:31:49.906
originating from your device

00:31:49.906 --> 00:31:52.016
with the real world and finding

00:31:52.016 --> 00:31:52.926
the intersection point.

00:31:55.316 --> 00:31:56.926
ARKit uses all the scene

00:31:56.926 --> 00:31:58.676
information available, which

00:31:58.676 --> 00:32:01.006
includes any detected planes as

00:32:01.006 --> 00:32:02.476
well as the 3-D feature points

00:32:02.546 --> 00:32:04.846
that ARWorldTracking is using to

00:32:04.966 --> 00:32:06.056
figure out its position.

00:32:06.516 --> 00:32:10.926
ARKit will then intersect our

00:32:10.926 --> 00:32:15.596
ray with all information that is

00:32:15.596 --> 00:32:17.556
available and return all

00:32:17.556 --> 00:32:19.196
intersection points as an array

00:32:19.196 --> 00:32:21.616
which is sorted by distance.

00:32:22.226 --> 00:32:23.676
So the first entry in this array

00:32:23.676 --> 00:32:25.116
will be the closest intersection

00:32:25.116 --> 00:32:25.646
to the camera.

00:32:25.736 --> 00:32:30.266
And there are different ways on

00:32:30.626 --> 00:32:31.796
how you can perform this

00:32:31.796 --> 00:32:32.456
intersection.

00:32:32.976 --> 00:32:35.336
And you can define this by

00:32:35.536 --> 00:32:37.286
providing a hit-test type.

00:32:38.276 --> 00:32:39.816
So there are four ways on how to

00:32:39.816 --> 00:32:40.676
do this.

00:32:40.676 --> 00:32:43.576
Let's have a look.

00:32:43.576 --> 00:32:44.436
If you are running plane

00:32:44.436 --> 00:32:46.446
detection and ARKit has detected

00:32:46.446 --> 00:32:48.496
a plane in our environment, we

00:32:48.496 --> 00:32:50.746
can make use of that.

00:32:51.536 --> 00:32:53.376
And here you have the choice of

00:32:53.376 --> 00:32:55.296
using the extent of the plane or

00:32:55.296 --> 00:32:55.886
ignoring it.

00:32:56.946 --> 00:32:59.676
So if you want your user to be

00:33:00.246 --> 00:33:03.916
able to move an object just on a

00:33:03.916 --> 00:33:05.616
plane, you can take the extent

00:33:05.616 --> 00:33:07.366
into account, which will mean

00:33:07.366 --> 00:33:09.836
that if a ray intersects within

00:33:09.836 --> 00:33:11.316
its extent, it will provide you

00:33:11.316 --> 00:33:12.216
with an intersection.

00:33:12.936 --> 00:33:14.766
If the ray hits outside of this,

00:33:15.206 --> 00:33:16.006
it will not give you an

00:33:16.006 --> 00:33:16.586
intersection.

00:33:17.156 --> 00:33:21.016
In the case of, for example,

00:33:21.016 --> 00:33:23.296
moving furniture around, or when

00:33:23.296 --> 00:33:24.836
you only have detected a small

00:33:24.836 --> 00:33:26.666
part of the ground plane, we can

00:33:26.666 --> 00:33:28.386
choose to ignore this extent and

00:33:28.386 --> 00:33:29.896
treat an existing plane as

00:33:29.896 --> 00:33:30.736
infinite plane.

00:33:31.916 --> 00:33:33.136
In that case you will always

00:33:33.136 --> 00:33:34.256
receive an intersection.

00:33:34.726 --> 00:33:37.676
And you can just use a patch of

00:33:37.676 --> 00:33:39.636
the real world, but let your

00:33:39.636 --> 00:33:43.886
users move an object along this

00:33:45.296 --> 00:33:45.436
plane.

00:33:45.606 --> 00:33:46.626
If you're not running plane

00:33:46.626 --> 00:33:47.966
detection or we have not

00:33:47.966 --> 00:33:50.906
detected any planes yet, we can

00:33:50.906 --> 00:33:52.846
also estimate a plane based on

00:33:52.846 --> 00:33:54.126
the 3-D feature points that we

00:33:54.126 --> 00:33:54.786
have available.

00:33:56.276 --> 00:33:57.886
In that case, ARKit will look

00:33:57.886 --> 00:33:59.606
for coplanar points in our

00:33:59.606 --> 00:34:01.366
environment and fit a plane into

00:34:01.366 --> 00:34:01.596
that.

00:34:02.746 --> 00:34:04.066
And after that it will return

00:34:04.066 --> 00:34:05.096
you with the intersection of

00:34:05.096 --> 00:34:05.596
this plane.

00:34:06.156 --> 00:34:09.735
In case you want to place

00:34:09.735 --> 00:34:10.906
something on a very small

00:34:10.906 --> 00:34:12.856
surface, which does not form a

00:34:12.856 --> 00:34:14.326
plane, or you have a very

00:34:14.406 --> 00:34:16.295
irregular environment, you can

00:34:16.295 --> 00:34:17.585
also choose to intersect with

00:34:17.585 --> 00:34:18.906
the feature points directly.

00:34:20.795 --> 00:34:22.726
This means that we will find an

00:34:22.726 --> 00:34:24.496
intersection along our ray,

00:34:24.716 --> 00:34:26.076
which is closest to an existing

00:34:26.076 --> 00:34:27.686
feature point, and return this

00:34:27.866 --> 00:34:28.735
as the result.

00:34:29.246 --> 00:34:31.565
Let's have a look at how this is

00:34:31.795 --> 00:34:32.386
done in code.

00:34:32.926 --> 00:34:36.065
So the first thing we need to do

00:34:36.416 --> 00:34:37.936
is define our ray.

00:34:38.726 --> 00:34:41.536
And it intersects on our device.

00:34:42.116 --> 00:34:45.286
You provide this as a CG point,

00:34:45.286 --> 00:34:46.585
which is represented in

00:34:46.585 --> 00:34:47.735
normalized image space

00:34:47.735 --> 00:34:48.346
coordinates.

00:34:48.436 --> 00:34:50.166
This means the top left of our

00:34:50.166 --> 00:34:51.866
image is 0, 0, whereas the

00:34:51.866 --> 00:34:53.326
bottom right is 1, 1.

00:34:53.946 --> 00:34:57.616
So if we want to send a ray or

00:34:58.016 --> 00:34:59.066
find an intersection in the

00:34:59.146 --> 00:35:00.706
center of our screen, we would

00:35:00.706 --> 00:35:04.416
define as CG points with 0.5 for

00:35:04.416 --> 00:35:05.026
x and y.

00:35:05.526 --> 00:35:07.556
If you're using SceneKit or

00:35:07.556 --> 00:35:08.846
SpriteKit, we're providing a

00:35:08.846 --> 00:35:10.976
custom overlay that you can

00:35:10.976 --> 00:35:15.446
simply pass a CG point in a few

00:35:15.446 --> 00:35:16.256
coordinates.

00:35:16.256 --> 00:35:18.796
So you can use the result of a

00:35:18.796 --> 00:35:22.356
UI tap over touch gesture as

00:35:22.356 --> 00:35:23.616
inputs to define this ray.

00:35:24.126 --> 00:35:27.206
So let's pass this point onto

00:35:27.206 --> 00:35:29.486
the hit-test method and define

00:35:29.826 --> 00:35:31.116
the hit-test types that we want

00:35:31.116 --> 00:35:31.566
to use.

00:35:31.786 --> 00:35:33.286
In this case we're using exiting

00:35:33.286 --> 00:35:34.456
planes, which means it will

00:35:34.456 --> 00:35:36.436
intersect with any existing

00:35:36.436 --> 00:35:37.816
planes that ARKit has already

00:35:37.816 --> 00:35:39.916
detected, as well as estimated

00:35:39.916 --> 00:35:40.846
horizontal planes.

00:35:41.126 --> 00:35:42.466
So this can be used as a

00:35:42.466 --> 00:35:44.466
fallback case in case there are

00:35:44.466 --> 00:35:46.216
no planes detected yet.

00:35:46.806 --> 00:35:50.086
After that, ARKit will return an

00:35:50.086 --> 00:35:53.796
array of results.

00:35:53.796 --> 00:35:55.636
And you can access the first

00:35:55.636 --> 00:35:56.676
result, which will be the

00:35:56.676 --> 00:35:58.636
closest intersection to your

00:35:58.636 --> 00:35:58.976
camera.

00:36:01.856 --> 00:36:03.736
The intersection points is

00:36:03.736 --> 00:36:05.186
contained in the worldTransform

00:36:05.186 --> 00:36:07.096
property of our hit-test result.

00:36:07.586 --> 00:36:09.166
And we can create a new ARAnchor

00:36:09.166 --> 00:36:11.276
based on this result and pass it

00:36:11.276 --> 00:36:12.936
back to the session because we

00:36:12.936 --> 00:36:14.566
want to keep track of it.

00:36:16.096 --> 00:36:18.126
So if we take this code and

00:36:18.126 --> 00:36:20.626
would apply it to the scene here

00:36:20.916 --> 00:36:21.976
where we point our phone at a

00:36:21.976 --> 00:36:25.046
table, it would return us the

00:36:25.046 --> 00:36:26.726
intersection points on this

00:36:26.726 --> 00:36:28.046
table in the center of the

00:36:28.046 --> 00:36:28.456
screen.

00:36:28.686 --> 00:36:30.836
And we can place a virtual cup

00:36:30.836 --> 00:36:31.816
at this location.

00:36:33.816 --> 00:36:35.866
By default, your rendering

00:36:35.866 --> 00:36:37.046
engine will assume that your

00:36:37.046 --> 00:36:38.596
background image is perfectly

00:36:38.596 --> 00:36:38.806
lit.

00:36:39.226 --> 00:36:41.526
So your augmentation looks like

00:36:41.526 --> 00:36:42.496
it really belongs there.

00:36:43.226 --> 00:36:44.596
However, if you're in a darker

00:36:44.596 --> 00:36:47.396
environment, then your camera

00:36:47.396 --> 00:36:49.126
image is darker, and it means

00:36:49.286 --> 00:36:50.726
that your augmentation will look

00:36:50.726 --> 00:36:52.106
out of place and it appears to

00:36:52.106 --> 00:36:52.406
glow.

00:36:53.016 --> 00:36:56.506
In order to fix this, we need to

00:36:56.506 --> 00:36:57.956
adjust the relative brightness

00:36:58.426 --> 00:37:00.526
of our virtual object.

00:37:00.696 --> 00:37:03.896
And for this, we are providing

00:37:03.896 --> 00:37:04.646
light estimation.

00:37:05.216 --> 00:37:09.516
So light estimation operates on

00:37:09.516 --> 00:37:10.546
our camera image.

00:37:10.926 --> 00:37:12.156
And it uses its exposure

00:37:12.156 --> 00:37:13.936
information to determine the

00:37:13.936 --> 00:37:15.646
relative brightness of it.

00:37:16.436 --> 00:37:18.046
For a well-lit image, this

00:37:18.046 --> 00:37:19.556
defaults to 1000 lumen.

00:37:20.096 --> 00:37:21.726
For a brighter environment, you

00:37:21.726 --> 00:37:23.146
will get a higher value.

00:37:23.146 --> 00:37:24.536
For a darker environment, a

00:37:24.536 --> 00:37:25.566
lower value.

00:37:26.636 --> 00:37:27.976
You can also assign this value

00:37:27.976 --> 00:37:30.846
directly to an SEN light as its

00:37:30.846 --> 00:37:32.266
ambient intensity property.

00:37:32.866 --> 00:37:34.266
Hence, if you're using

00:37:34.266 --> 00:37:35.656
physically-based lighting, it

00:37:35.656 --> 00:37:36.656
will automatically take

00:37:36.656 --> 00:37:39.276
advantage of this.

00:37:39.486 --> 00:37:40.796
Light estimation is enabled by

00:37:40.796 --> 00:37:41.416
default.

00:37:41.416 --> 00:37:43.746
And you can configure this by

00:37:43.746 --> 00:37:44.306
setting the

00:37:44.306 --> 00:37:47.176
isLightEstimationEnabled

00:37:47.176 --> 00:37:48.736
property on an ARSession

00:37:48.736 --> 00:37:49.456
configuration.

00:37:50.426 --> 00:37:51.946
The results of light estimation

00:37:52.566 --> 00:37:54.256
are provided to you in the Light

00:37:54.256 --> 00:37:56.036
Estimate property on the ARFrame

00:37:56.366 --> 00:37:59.036
as its ambient intensity value.

00:37:59.686 --> 00:38:03.286
So with that, let's dive into a

00:38:03.286 --> 00:38:04.886
demo and look how we're using

00:38:04.886 --> 00:38:06.326
scene understanding with ARKit.

00:38:07.516 --> 00:38:16.636
[ Applause ]

00:38:17.136 --> 00:38:18.516
So the application that I'm

00:38:18.516 --> 00:38:20.836
going to show you is the ARKit

00:38:20.896 --> 00:38:21.826
Sample application.

00:38:22.136 --> 00:38:22.976
Which means you can also

00:38:22.976 --> 00:38:25.186
download it from our developer

00:38:25.186 --> 00:38:25.646
website.

00:38:27.076 --> 00:38:29.156
It's used to place objects into

00:38:29.156 --> 00:38:29.866
our environment.

00:38:30.386 --> 00:38:31.576
And it's using scene

00:38:31.576 --> 00:38:33.366
understanding in order to do

00:38:33.366 --> 00:38:33.576
that.

00:38:33.956 --> 00:38:36.686
So, let's bring it right up

00:38:37.676 --> 00:38:37.776
here.

00:38:37.986 --> 00:38:39.616
And if I move it around here,

00:38:39.996 --> 00:38:41.966
what you see in front of me is

00:38:42.966 --> 00:38:44.376
our focus square.

00:38:44.676 --> 00:38:46.896
And we're placing this by doing

00:38:46.896 --> 00:38:48.616
hit-testing in the center of our

00:38:48.616 --> 00:38:51.366
scene and finding on placing the

00:38:51.366 --> 00:38:52.626
object at its intersection

00:38:52.626 --> 00:38:52.946
point.

00:38:53.776 --> 00:38:55.536
So if I move this along our

00:38:55.536 --> 00:38:57.276
table, you see that it basically

00:38:57.276 --> 00:38:58.556
slides along this table.

00:39:00.046 --> 00:39:02.736
It's also using plane detection

00:39:02.806 --> 00:39:03.616
in parallel.

00:39:03.616 --> 00:39:05.396
And we can visualize this to see

00:39:05.396 --> 00:39:06.086
what's going on.

00:39:06.356 --> 00:39:08.326
So let's bring up our Debug menu

00:39:08.326 --> 00:39:10.606
here and activate the second

00:39:10.606 --> 00:39:11.966
option here, which is Debug

00:39:11.966 --> 00:39:12.846
Visualizations.

00:39:13.736 --> 00:39:14.306
Let's close it.

00:39:15.376 --> 00:39:16.416
And what you see here is the

00:39:16.416 --> 00:39:17.646
plane that it has detected.

00:39:18.356 --> 00:39:21.986
To give you a better idea, let's

00:39:21.986 --> 00:39:27.016
restart this and see how it

00:39:27.016 --> 00:39:27.846
finds new planes.

00:39:27.846 --> 00:39:29.266
So if I'm moving it around here,

00:39:29.546 --> 00:39:30.716
you see it has detected a new

00:39:30.716 --> 00:39:31.056
plane.

00:39:32.106 --> 00:39:33.036
Let's quickly point it at

00:39:33.036 --> 00:39:34.516
another part of this table, and

00:39:34.516 --> 00:39:35.966
it has found another plane.

00:39:36.596 --> 00:39:38.296
And if I'm moving this along

00:39:38.296 --> 00:39:41.706
this table, it eventually merges

00:39:41.816 --> 00:39:42.736
both of them together.

00:39:42.876 --> 00:39:43.946
And it figured out that there's

00:39:43.976 --> 00:39:45.396
just one plane there.

00:39:47.516 --> 00:39:53.856
[ Applause ]

00:39:54.356 --> 00:39:56.006
So next, let's place some actual

00:39:56.006 --> 00:39:56.756
objects here.

00:39:59.256 --> 00:40:01.076
My daughter asked to bring some

00:40:01.076 --> 00:40:02.746
flowers to the presentation.

00:40:02.746 --> 00:40:03.976
And I don't want to disappoint

00:40:03.976 --> 00:40:04.166
her.

00:40:05.036 --> 00:40:07.196
So, let's make this more

00:40:07.196 --> 00:40:09.206
romantic here and place a nice

00:40:09.206 --> 00:40:09.446
vase.

00:40:10.026 --> 00:40:13.286
In that case, we again hit-test

00:40:13.476 --> 00:40:15.126
against the center of our screen

00:40:15.776 --> 00:40:17.116
and find the intersection the

00:40:17.116 --> 00:40:21.436
point to place the object.

00:40:21.596 --> 00:40:22.996
One important aspect here is

00:40:23.566 --> 00:40:25.356
that this vase actually appears

00:40:25.356 --> 00:40:26.486
in real-world scale.

00:40:26.726 --> 00:40:28.156
And this is possible due to two

00:40:28.156 --> 00:40:28.486
things.

00:40:29.446 --> 00:40:31.136
One is that WorldTracking

00:40:31.136 --> 00:40:34.646
provides us with the pose to

00:40:34.846 --> 00:40:35.346
scale.

00:40:35.396 --> 00:40:38.006
And the second thing is that our

00:40:38.006 --> 00:40:39.856
3-D model is actually modeled in

00:40:39.856 --> 00:40:41.566
3-D in real-world coordinates.

00:40:41.746 --> 00:40:43.166
So this is really important if

00:40:43.166 --> 00:40:44.756
you're creating content for

00:40:44.756 --> 00:40:46.596
augmented reality that you take

00:40:46.596 --> 00:40:49.136
this into account that this vase

00:40:49.136 --> 00:40:51.756
should not appear as high as

00:40:51.756 --> 00:40:53.036
building or too small.

00:40:53.456 --> 00:40:57.366
So let's go ahead and place a

00:40:57.366 --> 00:40:59.906
more interactive object, which

00:40:59.906 --> 00:41:01.096
is my chameleon friend here.

00:41:02.196 --> 00:41:04.196
[ Applause ]

00:41:04.376 --> 00:41:07.096
And one nice thing -- thank you

00:41:07.806 --> 00:41:08.856
-- and one nice thing is that

00:41:09.616 --> 00:41:11.106
you always know the position of

00:41:11.106 --> 00:41:14.546
the user when you're running

00:41:14.546 --> 00:41:15.246
WorldTracking.

00:41:15.686 --> 00:41:17.436
So you can have your virtual

00:41:17.436 --> 00:41:19.486
content interact with the user

00:41:19.806 --> 00:41:21.636
in the real world.

00:41:23.516 --> 00:41:29.086
[ Applause ]

00:41:29.586 --> 00:41:32.506
So, if I move over here, it

00:41:32.786 --> 00:41:35.406
might eventually turn to me, if

00:41:35.406 --> 00:41:36.086
he's not scared.

00:41:36.306 --> 00:41:37.966
Yeah, there we go.

00:41:38.516 --> 00:41:43.546
[ Applause ]

00:41:44.046 --> 00:41:45.296
And if I get even closer he

00:41:45.296 --> 00:41:46.486
might react in even different

00:41:46.486 --> 00:41:46.686
ways.

00:41:47.616 --> 00:41:48.066
Let's see.

00:41:48.526 --> 00:41:49.606
It's a bit -- oh!

00:41:49.606 --> 00:41:52.526
There we go.

00:41:53.856 --> 00:41:54.946
Another thing that chameleons

00:41:54.946 --> 00:41:56.976
can do is change their color.

00:41:57.156 --> 00:42:00.966
And if I tap him, he adjusts the

00:42:00.966 --> 00:42:01.326
color.

00:42:03.556 --> 00:42:05.926
So let's give it a green.

00:42:07.976 --> 00:42:09.236
And one nice feature that we put

00:42:09.236 --> 00:42:11.996
in here is I can move him along

00:42:11.996 --> 00:42:15.076
the table, and he will adapt to

00:42:15.076 --> 00:42:16.636
the background color of the

00:42:16.636 --> 00:42:18.076
table in order to blend in

00:42:18.076 --> 00:42:18.526
nicely.

00:42:19.516 --> 00:42:28.546
[ Applause ]

00:42:29.046 --> 00:42:30.606
So this is our sample

00:42:30.606 --> 00:42:31.206
application.

00:42:31.576 --> 00:42:32.906
You can download it from the

00:42:32.906 --> 00:42:35.216
website and put in your own

00:42:35.216 --> 00:42:37.416
contents and play around with

00:42:38.016 --> 00:42:39.686
it, basically.

00:42:39.686 --> 00:42:41.816
So next, we're going to have a

00:42:41.816 --> 00:42:43.916
look at rendering with ARKit.

00:42:47.496 --> 00:42:49.516
Rendering brings tracking and

00:42:49.566 --> 00:42:51.076
scene understanding together

00:42:51.266 --> 00:42:52.046
with your content.

00:42:52.946 --> 00:42:54.156
And in order to render with

00:42:54.156 --> 00:42:56.016
ARKit, you need to process all

00:42:56.016 --> 00:42:57.656
the information that we provide

00:42:57.656 --> 00:42:58.696
you in an ARFrame.

00:42:59.826 --> 00:43:01.616
For those of you using SceneKit

00:43:01.616 --> 00:43:03.686
and SpriteKit, we have already

00:43:03.866 --> 00:43:05.566
created customized views that

00:43:05.566 --> 00:43:07.146
take care of rending ARFrames

00:43:07.186 --> 00:43:07.566
for you.

00:43:08.166 --> 00:43:11.686
If you're using Metal, and want

00:43:11.686 --> 00:43:13.056
to create your own rendering

00:43:13.056 --> 00:43:15.226
engine or integrate ARKit into

00:43:15.226 --> 00:43:16.616
your existing rendering engine,

00:43:16.996 --> 00:43:19.256
we're providing a template that

00:43:19.356 --> 00:43:20.596
gives you an idea of how to do

00:43:20.596 --> 00:43:22.186
this and provides a good

00:43:22.186 --> 00:43:22.866
starting point.

00:43:23.966 --> 00:43:25.266
Let's have a look at each one of

00:43:25.266 --> 00:43:27.626
those, starting with SceneKit.

00:43:28.336 --> 00:43:30.136
For SceneKit we're providing an

00:43:30.136 --> 00:43:31.946
ARSCNView, which is a subclass

00:43:31.946 --> 00:43:33.166
of an SCNView.

00:43:34.376 --> 00:43:36.216
It contains an ARSession that it

00:43:36.216 --> 00:43:38.146
uses to update its rendering.

00:43:39.036 --> 00:43:40.246
So this includes drawing the

00:43:40.246 --> 00:43:41.526
camera image in the background,

00:43:42.646 --> 00:43:44.496
taking into account the rotation

00:43:44.496 --> 00:43:46.466
of the device as well as any

00:43:46.466 --> 00:43:47.036
[inaudible] changes.

00:43:47.486 --> 00:43:51.946
Next, it updates an SCNCamera

00:43:51.946 --> 00:43:53.596
based on the tracking transforms

00:43:53.596 --> 00:43:55.196
that we provide in an ARCamera.

00:43:55.786 --> 00:43:58.706
So your scene stays intact and

00:43:58.846 --> 00:44:00.386
ARKit simply controls an

00:44:00.516 --> 00:44:02.096
SCNCamera by moving it around

00:44:02.096 --> 00:44:03.426
the scene the way you move

00:44:03.426 --> 00:44:05.426
around your device in the real

00:44:05.976 --> 00:44:06.106
world.

00:44:07.076 --> 00:44:08.186
If you're using Light

00:44:08.186 --> 00:44:09.676
Estimation, we automatically

00:44:09.676 --> 00:44:12.786
place an SCN light probe into

00:44:12.786 --> 00:44:15.936
your scene so if you use objects

00:44:15.936 --> 00:44:17.606
with physically-based lighting

00:44:17.766 --> 00:44:20.106
enabled you can already take

00:44:20.106 --> 00:44:21.496
advantage or automatically take

00:44:21.496 --> 00:44:23.006
advantage of Light Estimation.

00:44:23.546 --> 00:44:28.276
And one thing that ARCNView does

00:44:28.616 --> 00:44:32.626
is map SCNNotes to ARAnchors so

00:44:32.626 --> 00:44:33.966
you don't actually need to

00:44:33.966 --> 00:44:35.536
interface with ARAnchors

00:44:35.576 --> 00:44:37.526
directly, but can continue to

00:44:37.526 --> 00:44:38.686
use SCNNotes.

00:44:39.616 --> 00:44:40.646
This means whenever a new

00:44:40.646 --> 00:44:42.026
ARAnchor is being added to the

00:44:42.026 --> 00:44:44.686
session, ARSCNView will create a

00:44:44.686 --> 00:44:45.336
node for you.

00:44:45.986 --> 00:44:47.946
And every time we update the

00:44:47.946 --> 00:44:50.336
ARAnchor, like its transform, we

00:44:50.336 --> 00:44:51.816
update the nodes transform

00:44:51.816 --> 00:44:52.436
automatically.

00:44:52.976 --> 00:44:56.366
And this is handled through the

00:44:56.366 --> 00:44:57.516
ARSCNView delegate.

00:45:00.116 --> 00:45:02.226
So every time we add a new

00:45:02.496 --> 00:45:06.026
anchor to the session, ARSCNView

00:45:06.026 --> 00:45:08.116
will create a new SCNNode for

00:45:08.116 --> 00:45:08.286
you.

00:45:09.216 --> 00:45:10.366
If you want to provide your own

00:45:10.366 --> 00:45:12.276
nodes, you can implement

00:45:12.626 --> 00:45:14.396
renderer nodeFor anchor and

00:45:14.396 --> 00:45:15.686
return to your custom node for

00:45:15.686 --> 00:45:15.986
this.

00:45:16.666 --> 00:45:18.896
After this, the SCNNode will be

00:45:19.206 --> 00:45:21.346
added to the scene graph.

00:45:21.976 --> 00:45:23.316
And you will receive another

00:45:23.316 --> 00:45:25.616
delegate call renderer didAdd

00:45:25.696 --> 00:45:26.506
node for anchor.

00:45:27.056 --> 00:45:29.796
The same holds true for whenever

00:45:29.796 --> 00:45:33.076
a node is being updated.

00:45:34.276 --> 00:45:37.096
So in that case, DSCNNodes

00:45:37.096 --> 00:45:38.496
transform will be automatically

00:45:38.496 --> 00:45:40.066
updated with the ARAnchors

00:45:40.066 --> 00:45:41.906
transform and you will receive

00:45:41.966 --> 00:45:44.246
two callbacks when this happens.

00:45:44.806 --> 00:45:46.326
One before we update its

00:45:46.326 --> 00:45:48.826
transform, and another one after

00:45:48.826 --> 00:45:49.916
we update the transform.

00:45:52.296 --> 00:45:54.186
Whenever an ARAnchor is being

00:45:54.186 --> 00:45:56.846
removed from the session, we

00:45:56.846 --> 00:45:57.936
automatically remove the

00:45:57.936 --> 00:45:59.486
corresponding SCNNode from the

00:45:59.486 --> 00:46:01.186
scene graph and provide you with

00:46:01.186 --> 00:46:03.006
the callback renderer didRemove

00:46:03.056 --> 00:46:03.946
node for anchor.

00:46:04.446 --> 00:46:07.836
So this is SceneKit with ARKit.

00:46:08.726 --> 00:46:10.866
Next, let's have a look at

00:46:12.736 --> 00:46:13.006
SpriteKit.

00:46:13.006 --> 00:46:14.536
For SpriteKit we're providing an

00:46:14.536 --> 00:46:16.726
ARSKview, which is a subclass of

00:46:16.726 --> 00:46:17.266
SKView.

00:46:18.426 --> 00:46:20.316
It contains an ARSession, which

00:46:20.316 --> 00:46:22.896
it uses to update its rendering.

00:46:23.106 --> 00:46:24.596
This includes drawing the camera

00:46:24.596 --> 00:46:27.196
image in the background, and in

00:46:27.196 --> 00:46:29.476
this case, mapping SKNodes to

00:46:29.476 --> 00:46:30.106
ARAnchors.

00:46:30.636 --> 00:46:31.856
So it provides a very similar

00:46:31.856 --> 00:46:33.176
set of delegate methods to

00:46:33.176 --> 00:46:34.906
SceneKit, which it can use.

00:46:36.066 --> 00:46:37.366
One major difference is that

00:46:37.436 --> 00:46:38.996
SpriteKit is a 2-D rendering

00:46:38.996 --> 00:46:39.356
engine.

00:46:39.696 --> 00:46:41.026
So that means we cannot simply

00:46:41.026 --> 00:46:43.036
update a camera that is being

00:46:43.116 --> 00:46:43.506
moved around.

00:46:44.286 --> 00:46:46.706
So what ARKit does here is

00:46:46.996 --> 00:46:49.276
project our ARAnchor's positions

00:46:49.956 --> 00:46:52.446
into the SpriteKit view.

00:46:53.036 --> 00:46:54.616
And then render the Sprites as

00:46:54.676 --> 00:46:56.746
billboards at these locations,

00:46:56.746 --> 00:46:57.956
at the projected locations.

00:46:58.706 --> 00:47:00.186
This means that the Sprites will

00:47:00.186 --> 00:47:04.036
always be facing the camera.

00:47:04.036 --> 00:47:05.506
If you want to learn more about

00:47:05.506 --> 00:47:06.956
this, there a session from the

00:47:06.956 --> 00:47:09.146
SpriteKit team, "Going beyond

00:47:09.146 --> 00:47:11.286
2-D in SpriteKit" which will

00:47:11.286 --> 00:47:13.316
focus on how to integrate ARKit

00:47:13.446 --> 00:47:14.086
with SpriteKit.

00:47:14.686 --> 00:47:19.396
And next, let's have a look at

00:47:19.396 --> 00:47:20.876
custom rendering with ARKit

00:47:21.106 --> 00:47:21.666
using Metal.

00:47:23.136 --> 00:47:24.446
There are four things that you

00:47:24.446 --> 00:47:26.126
need to do in order to render

00:47:26.326 --> 00:47:27.456
with ARKit.

00:47:28.566 --> 00:47:29.896
The first is draw the camera

00:47:29.896 --> 00:47:30.916
image in the background.

00:47:31.806 --> 00:47:34.306
You usually create a texture for

00:47:34.306 --> 00:47:35.426
this and draw it in a

00:47:35.426 --> 00:47:35.906
background.

00:47:37.176 --> 00:47:39.056
The next thing is to update our

00:47:39.056 --> 00:47:40.896
virtual camera based on our

00:47:40.896 --> 00:47:41.396
ARCamera.

00:47:42.306 --> 00:47:44.036
This contains setting the view

00:47:44.036 --> 00:47:45.616
matrix as well as the projection

00:47:45.616 --> 00:47:46.146
matrix.

00:47:48.296 --> 00:47:50.246
Third item is to update the

00:47:50.246 --> 00:47:52.786
lighting situation or the light

00:47:52.786 --> 00:47:54.246
in your scene based on our light

00:47:54.246 --> 00:47:54.706
estimate.

00:47:55.986 --> 00:47:57.406
And finally, if you have placed

00:47:57.536 --> 00:47:59.226
geometry based on scene

00:47:59.226 --> 00:48:01.936
understanding, then you would

00:48:01.936 --> 00:48:04.086
use the ARAnchors in order to

00:48:04.276 --> 00:48:06.596
set the transforms correctly.

00:48:07.816 --> 00:48:09.296
All this information is

00:48:09.296 --> 00:48:10.616
contained in an ARFrame.

00:48:11.146 --> 00:48:12.486
And you have two ways of how to

00:48:12.486 --> 00:48:13.586
access this ARFrame.

00:48:14.136 --> 00:48:17.576
One is by polling the current

00:48:17.576 --> 00:48:19.026
frame property on ARSession.

00:48:19.996 --> 00:48:21.396
So, if you have your own render

00:48:21.396 --> 00:48:24.126
loop you would use -- well, you

00:48:24.126 --> 00:48:25.646
could use this method to access

00:48:25.646 --> 00:48:26.336
the current frame.

00:48:27.036 --> 00:48:28.316
And then you should also take

00:48:28.316 --> 00:48:30.726
advantage of the timestamp

00:48:30.726 --> 00:48:32.796
property on ARFrame in order to

00:48:32.796 --> 00:48:34.326
avoid rendering the same frame

00:48:34.326 --> 00:48:35.056
multiple times.

00:48:35.636 --> 00:48:38.456
An alternative is to use our

00:48:38.456 --> 00:48:40.536
Session Delegate, which provides

00:48:40.536 --> 00:48:42.606
you with session didUpdate frame

00:48:42.676 --> 00:48:43.966
every time a new frame has been

00:48:43.966 --> 00:48:44.566
calculated.

00:48:45.106 --> 00:48:47.806
In that case, you can just

00:48:47.806 --> 00:48:49.406
simply take it and then update

00:48:49.406 --> 00:48:49.956
your rendering.

00:48:51.006 --> 00:48:52.806
By default, this is called on

00:48:52.806 --> 00:48:53.776
the main [inaudible], but you

00:48:53.776 --> 00:48:54.876
can also provide your own

00:48:54.876 --> 00:48:56.366
dispatch queue, which we will

00:48:56.366 --> 00:48:58.826
use to call this method.

00:48:58.826 --> 00:49:02.846
So let's look into what Update

00:49:02.846 --> 00:49:04.856
Rendering contains.

00:49:05.506 --> 00:49:08.616
So the first thing is to draw

00:49:08.616 --> 00:49:09.466
the camera image in the

00:49:09.466 --> 00:49:10.016
background.

00:49:10.556 --> 00:49:12.046
And you can access the captured

00:49:12.106 --> 00:49:13.766
image property on an ARFrame,

00:49:14.166 --> 00:49:15.496
which is the CV Pixel Buffer.

00:49:16.796 --> 00:49:18.546
You can generate Metal texture

00:49:18.796 --> 00:49:20.126
based on this Pixel Buffer and

00:49:20.456 --> 00:49:22.246
then draw in a quad in the

00:49:22.246 --> 00:49:22.756
background.

00:49:23.306 --> 00:49:26.766
Note that this is a Pixel Buffer

00:49:26.766 --> 00:49:28.716
that is vended to us through AV

00:49:28.716 --> 00:49:30.156
Foundation, so you should not

00:49:30.186 --> 00:49:33.136
hold on to too many of those

00:49:33.196 --> 00:49:34.756
frames for too long, otherwise

00:49:34.756 --> 00:49:36.146
you will stop receiving updates.

00:49:36.756 --> 00:49:40.606
The next item is to update our

00:49:40.606 --> 00:49:42.266
virtual camera based on our

00:49:42.266 --> 00:49:42.796
ARCamera.

00:49:43.376 --> 00:49:45.126
For this we have to determine

00:49:45.126 --> 00:49:46.726
the view matrix as well as the

00:49:46.726 --> 00:49:47.726
protection matrix.

00:49:49.066 --> 00:49:50.876
The view matrix is simply the

00:49:50.876 --> 00:49:52.616
inverse of our camera transform.

00:49:53.886 --> 00:49:55.266
And in order to generate the

00:49:55.266 --> 00:49:56.536
projection matrix, we are

00:49:56.536 --> 00:49:57.846
offering you a convenience

00:49:57.846 --> 00:49:59.666
method on the ARCamera, which

00:49:59.666 --> 00:50:00.726
provides you with a projection

00:50:00.726 --> 00:50:01.196
matrix.

00:50:03.656 --> 00:50:05.006
The third step would be to

00:50:05.006 --> 00:50:05.966
update the lighting.

00:50:06.546 --> 00:50:08.976
So for this, simply access the

00:50:08.976 --> 00:50:10.816
Light Estimate property and use

00:50:10.816 --> 00:50:12.446
its ambient intensity in order

00:50:12.446 --> 00:50:15.426
to update your lighting model.

00:50:16.076 --> 00:50:18.896
And finally would be to iterate

00:50:19.156 --> 00:50:20.686
over the anchors and its 3-D

00:50:20.686 --> 00:50:22.226
locations in order to update the

00:50:22.226 --> 00:50:23.706
transform of the geometries.

00:50:24.176 --> 00:50:25.356
So any anchor that you have

00:50:25.526 --> 00:50:27.846
added manually to the session or

00:50:27.926 --> 00:50:28.986
any anchor that has been

00:50:28.986 --> 00:50:30.596
detected or that has been added

00:50:30.766 --> 00:50:33.276
to plane detection will be part

00:50:33.276 --> 00:50:34.396
of these frame anchors.

00:50:37.156 --> 00:50:40.246
Then are a few things to note

00:50:40.246 --> 00:50:41.636
when rendering based on a camera

00:50:41.636 --> 00:50:42.026
image.

00:50:42.976 --> 00:50:44.366
We want to have a look at those.

00:50:45.486 --> 00:50:47.916
So one thing is that the

00:50:47.916 --> 00:50:49.736
captured image that is contained

00:50:49.736 --> 00:50:51.706
in an ARFrame is always provided

00:50:51.706 --> 00:50:52.956
in the same orientation.

00:50:53.776 --> 00:50:55.316
However, if you rotate your

00:50:55.346 --> 00:50:57.566
physical device, it might not

00:50:57.566 --> 00:50:59.756
line up with your user interface

00:50:59.756 --> 00:51:00.376
orientation.

00:51:00.676 --> 00:51:02.056
And a transform needs to be

00:51:02.056 --> 00:51:04.946
applied in order to render this

00:51:06.236 --> 00:51:06.556
correctly.

00:51:06.556 --> 00:51:08.166
Another thing is that the aspect

00:51:08.166 --> 00:51:09.726
ratio of the camera image might

00:51:09.766 --> 00:51:11.496
not necessarily line up with

00:51:11.526 --> 00:51:12.156
your device.

00:51:13.106 --> 00:51:14.256
And this means that we have to

00:51:14.256 --> 00:51:15.706
take this into account in order

00:51:15.706 --> 00:51:18.356
to properly render our camera

00:51:18.406 --> 00:51:19.476
image in the screen.

00:51:20.066 --> 00:51:22.626
To fix this or to make this

00:51:22.626 --> 00:51:24.206
easier for you, we're providing

00:51:24.206 --> 00:51:25.126
you with helper methods.

00:51:25.126 --> 00:51:28.926
So there's one method on

00:51:28.926 --> 00:51:31.126
ARFrame, which is the Display

00:51:31.126 --> 00:51:31.736
Transform.

00:51:32.646 --> 00:51:34.286
The Display Transform transforms

00:51:34.286 --> 00:51:35.636
from frame space into view

00:51:35.636 --> 00:51:36.126
space.

00:51:36.746 --> 00:51:38.426
And you simply provide it with

00:51:38.596 --> 00:51:40.976
your view port size as well as

00:51:40.976 --> 00:51:43.116
your interface orientation, and

00:51:43.116 --> 00:51:44.096
you will get an according

00:51:44.096 --> 00:51:44.656
transform.

00:51:45.456 --> 00:51:46.776
In our Metal example, we are

00:51:46.776 --> 00:51:47.966
using the inverse of this

00:51:47.966 --> 00:51:49.946
transform to adjust the texture

00:51:49.946 --> 00:51:51.016
coordinates of our camera

00:51:51.016 --> 00:51:51.506
background.

00:51:52.076 --> 00:51:55.276
And to go with this is the

00:51:55.276 --> 00:51:58.036
projection matrix variance that

00:51:58.036 --> 00:51:59.276
takes into account the user

00:51:59.276 --> 00:52:00.836
interface orientation as well as

00:52:00.836 --> 00:52:01.716
the view port size.

00:52:02.226 --> 00:52:03.746
So you pass those along with

00:52:03.746 --> 00:52:05.356
clipping planes limits and you

00:52:05.356 --> 00:52:07.596
can use this projection matrix

00:52:07.656 --> 00:52:10.586
in order to correctly draw your

00:52:10.586 --> 00:52:12.266
virtual content on top of the

00:52:12.266 --> 00:52:12.916
camera image.

00:52:13.486 --> 00:52:17.746
So this is ARKit.

00:52:18.676 --> 00:52:21.226
To summarize, ARKit is a high

00:52:21.336 --> 00:52:23.916
level API designed for creating

00:52:23.916 --> 00:52:25.536
augmented reality applications

00:52:25.536 --> 00:52:26.346
on iOS.

00:52:26.896 --> 00:52:29.056
We provide you with World

00:52:29.056 --> 00:52:30.796
Tracking, which gives you the

00:52:30.796 --> 00:52:32.666
relative position of your device

00:52:33.156 --> 00:52:34.036
to a starting point.

00:52:35.766 --> 00:52:37.376
In order to place objects into

00:52:37.376 --> 00:52:38.916
the real world, we provide you

00:52:38.916 --> 00:52:39.956
with Scene Understanding.

00:52:41.306 --> 00:52:42.806
Scene Understanding provides you

00:52:42.806 --> 00:52:44.556
with Plane Detection as well as

00:52:44.556 --> 00:52:46.236
the ability to hit-test the real

00:52:46.236 --> 00:52:47.616
world in order to find 3-D

00:52:47.616 --> 00:52:49.226
coordinates and place objects

00:52:49.226 --> 00:52:49.386
there.

00:52:50.686 --> 00:52:51.886
And in order to improve the

00:52:51.886 --> 00:52:53.886
realism of our augmented

00:52:53.886 --> 00:52:55.026
content, we're providing you

00:52:55.026 --> 00:52:56.786
with a light estimate based on

00:52:56.786 --> 00:52:57.536
the camera image.

00:52:58.096 --> 00:53:00.856
We provide custom integration

00:53:00.856 --> 00:53:03.126
into SceneKit and SpriteKit as

00:53:03.126 --> 00:53:05.136
well as a template for Metal if

00:53:05.136 --> 00:53:06.016
you want to get started

00:53:06.266 --> 00:53:08.406
integrating ARKit into your own

00:53:08.406 --> 00:53:09.056
rendering engine.

00:53:09.616 --> 00:53:13.016
You can find more information on

00:53:13.016 --> 00:53:14.456
the website of our talk here.

00:53:14.926 --> 00:53:17.346
And there are a couple of

00:53:17.346 --> 00:53:18.976
related sessions from the

00:53:18.976 --> 00:53:20.656
SceneKit team who will also have

00:53:20.656 --> 00:53:21.986
a look at how to use dynamic

00:53:21.986 --> 00:53:24.216
shadows with ARKit and Sprite

00:53:24.216 --> 00:53:26.236
and SceneKit as well as a

00:53:26.236 --> 00:53:27.676
session from the SpriteKit team

00:53:27.886 --> 00:53:31.346
who will focus on using ARKit

00:53:31.426 --> 00:53:32.586
with SpriteKit.

00:53:33.186 --> 00:53:34.826
So, we're really excited of

00:53:34.866 --> 00:53:35.946
bringing this out into your

00:53:35.946 --> 00:53:36.306
hands.

00:53:36.596 --> 00:53:38.836
And we are looking forward to

00:53:38.836 --> 00:53:40.186
see the first applications that

00:53:40.186 --> 00:53:41.036
you're going to build with it.

00:53:41.616 --> 00:53:42.826
So please go ahead and download

00:53:42.976 --> 00:53:44.126
the sample code, the sample

00:53:44.126 --> 00:53:45.526
application from our website.

00:53:45.906 --> 00:53:48.006
Put your own content into it and

00:53:48.126 --> 00:53:49.296
show it around.

00:53:49.596 --> 00:53:51.336
And be happy.

00:53:51.836 --> 00:53:53.096
Thank you.

00:53:54.516 --> 00:54:00.300
[ Applause ]