WEBVTT

00:00:18.516 --> 00:00:21.966
[ Applause ]

00:00:22.466 --> 00:00:22.986
>> Good morning.

00:00:23.126 --> 00:00:24.386
And welcome to the modernizing

00:00:24.386 --> 00:00:25.586
Grand Central Dispatch Usage

00:00:25.616 --> 00:00:25.976
Session.

00:00:26.436 --> 00:00:27.806
I'm Daniel Chimene from the Core

00:00:27.806 --> 00:00:29.566
Darwin Team and my colleagues

00:00:29.566 --> 00:00:31.686
and I are here today to show you

00:00:31.766 --> 00:00:33.036
how you can take advantage of

00:00:33.036 --> 00:00:34.746
Grand Central Dispatch to get

00:00:34.746 --> 00:00:35.746
the best performance in your

00:00:35.746 --> 00:00:36.276
application.

00:00:37.536 --> 00:00:39.276
As app developers, you spent

00:00:39.276 --> 00:00:40.956
hundreds or thousands of hours

00:00:41.026 --> 00:00:42.706
building amazing experiences for

00:00:42.786 --> 00:00:43.416
your users.

00:00:43.476 --> 00:00:45.166
Taking advantage of our powerful

00:00:45.166 --> 00:00:45.616
devices.

00:00:46.616 --> 00:00:47.976
You want your users to be able

00:00:47.976 --> 00:00:49.096
to have a great experience.

00:00:49.596 --> 00:00:51.386
Not just on one device, but

00:00:51.386 --> 00:00:52.526
across all the variety of

00:00:52.526 --> 00:00:56.196
devices that Apple makes.

00:00:56.776 --> 00:00:58.476
GCD is designed to help you

00:00:58.476 --> 00:00:59.566
dynamically scale your

00:00:59.566 --> 00:01:00.356
application code.

00:01:00.456 --> 00:01:01.456
From the single core Apple

00:01:01.456 --> 00:01:03.056
Watch, all the way up to a mini

00:01:03.056 --> 00:01:03.536
core Mac.

00:01:04.486 --> 00:01:05.495
You don't want to have to worry

00:01:05.495 --> 00:01:07.086
too much about what kind of

00:01:07.086 --> 00:01:08.246
hardware your users are running.

00:01:09.116 --> 00:01:10.026
But there are problematic

00:01:10.096 --> 00:01:11.386
problems that can affect the

00:01:11.386 --> 00:01:13.216
scalability and the efficiency

00:01:13.676 --> 00:01:14.246
of your code.

00:01:14.296 --> 00:01:15.796
Both on the low end and on the

00:01:15.796 --> 00:01:16.166
high end.

00:01:17.166 --> 00:01:17.986
That's what we're here to talk

00:01:17.986 --> 00:01:18.486
about today.

00:01:19.326 --> 00:01:20.906
We want to help you ensure that

00:01:21.026 --> 00:01:22.216
all the work you're putting into

00:01:22.216 --> 00:01:23.206
your app to make it a great

00:01:23.206 --> 00:01:24.586
experience for your users

00:01:25.206 --> 00:01:26.846
translates across all of these

00:01:26.846 --> 00:01:27.306
devices.

00:01:28.986 --> 00:01:30.536
You may have been using GCD

00:01:30.536 --> 00:01:33.166
API's like dispatch async, and

00:01:33.166 --> 00:01:34.446
others to create cues and

00:01:34.446 --> 00:01:35.426
dispatch work to the system.

00:01:36.336 --> 00:01:37.226
These are only some of the

00:01:37.226 --> 00:01:38.476
interfaces to the concurrency

00:01:38.476 --> 00:01:40.206
technology that we call Grand

00:01:40.206 --> 00:01:40.976
Central Dispatch.

00:01:41.686 --> 00:01:42.896
Today, we're going to take a

00:01:42.896 --> 00:01:44.476
peek under the covers of GCD.

00:01:44.506 --> 00:01:46.576
This is an advanced session

00:01:46.666 --> 00:01:47.846
packed full of information.

00:01:48.366 --> 00:01:49.316
So, let's get started right

00:01:49.316 --> 00:01:50.566
away, by looking at our

00:01:50.566 --> 00:01:50.926
hardware.

00:01:52.136 --> 00:01:53.656
The amazing chips in our devices

00:01:53.656 --> 00:01:54.916
have been getting faster and

00:01:54.916 --> 00:01:55.806
faster over time.

00:01:56.276 --> 00:01:57.826
However, much of the speed is

00:01:57.826 --> 00:01:58.756
not just because the chips

00:01:58.756 --> 00:01:59.836
themselves are getting faster,

00:02:00.316 --> 00:02:01.026
but because they're getting

00:02:01.026 --> 00:02:02.386
smarter, and smarter about

00:02:02.386 --> 00:02:03.066
running your code.

00:02:03.166 --> 00:02:04.496
And they're learning from what

00:02:04.496 --> 00:02:06.086
your code does over time to

00:02:06.086 --> 00:02:07.136
operate more efficiently.

00:02:07.646 --> 00:02:12.246
However, if your code goes off

00:02:12.316 --> 00:02:13.816
core, before it's completed its

00:02:13.816 --> 00:02:15.526
task, then it may no longer be

00:02:15.526 --> 00:02:16.836
able to take advantage of the

00:02:16.906 --> 00:02:18.126
history that that core has built

00:02:18.126 --> 00:02:18.366
up.

00:02:18.476 --> 00:02:20.186
And you might leave performance

00:02:20.186 --> 00:02:21.656
on the table when you come back

00:02:21.846 --> 00:02:22.336
on core.

00:02:23.286 --> 00:02:24.646
We've even seen examples of this

00:02:24.646 --> 00:02:25.996
on our own frameworks, when we

00:02:25.996 --> 00:02:27.326
applied some of the optimization

00:02:27.366 --> 00:02:28.236
techniques that we're going to

00:02:28.236 --> 00:02:30.436
discuss today, we saw large

00:02:30.436 --> 00:02:33.086
speed ups from simple changes to

00:02:33.086 --> 00:02:34.106
avoid these problematic

00:02:34.106 --> 00:02:34.426
patterns.

00:02:36.856 --> 00:02:38.916
So, using these techniques lets

00:02:38.916 --> 00:02:40.406
you bring high performance apps

00:02:40.566 --> 00:02:41.966
to more users with less work.

00:02:42.666 --> 00:02:44.296
Today, we're going to give you

00:02:44.296 --> 00:02:45.316
some insight into what our

00:02:45.316 --> 00:02:46.776
system is doing under the covers

00:02:47.046 --> 00:02:47.756
with your code.

00:02:47.756 --> 00:02:49.316
So, you can tune your code to

00:02:49.316 --> 00:02:50.916
take the best advantage of what

00:02:50.916 --> 00:02:51.886
GCD has to offer.

00:02:52.676 --> 00:02:53.546
We're going to discuss a few

00:02:53.546 --> 00:02:54.086
things today.

00:02:55.126 --> 00:02:56.526
First, we're going to discuss

00:02:56.526 --> 00:02:57.736
how you can best express

00:02:57.946 --> 00:02:59.386
parallelism and concurrency.

00:03:00.026 --> 00:03:01.996
How you can chose the best way

00:03:02.036 --> 00:03:03.906
to express concurrency to grand

00:03:03.906 --> 00:03:04.636
central dispatch.

00:03:05.546 --> 00:03:07.036
We're going to introduce Unified

00:03:07.076 --> 00:03:08.466
Queue Identity, which is a major

00:03:08.466 --> 00:03:09.566
under the hood improvement to

00:03:09.566 --> 00:03:10.646
GCD that we're publishing this

00:03:10.646 --> 00:03:10.846
year.

00:03:11.666 --> 00:03:12.626
And we're finally going to show

00:03:12.626 --> 00:03:13.886
you how you can find problem

00:03:13.886 --> 00:03:15.136
spots in your code, with

00:03:15.136 --> 00:03:15.596
instruments.

00:03:16.626 --> 00:03:17.766
So, let's start by discussing

00:03:17.766 --> 00:03:19.196
parallelism and concurrency.

00:03:20.486 --> 00:03:24.506
So, for the purpose of this

00:03:24.546 --> 00:03:25.846
talk, we're talking about

00:03:25.946 --> 00:03:27.506
parallelism which is about how

00:03:27.506 --> 00:03:30.006
your code executes in parallel,

00:03:30.476 --> 00:03:31.686
simultaneously across many

00:03:31.686 --> 00:03:32.326
different cores.

00:03:32.756 --> 00:03:34.566
Concurrency is about how you

00:03:34.566 --> 00:03:35.806
compose the independent

00:03:35.806 --> 00:03:37.076
components of your application

00:03:37.386 --> 00:03:38.236
to run concurrently.

00:03:38.856 --> 00:03:40.186
The easy way to separate these

00:03:40.186 --> 00:03:41.796
two concepts in your mind, is to

00:03:41.796 --> 00:03:42.846
realize that parallelism is

00:03:42.846 --> 00:03:43.826
something that usually requires

00:03:43.826 --> 00:03:45.146
multiple cores and you want to

00:03:45.146 --> 00:03:46.166
use them all at the same time.

00:03:46.526 --> 00:03:47.696
And concurrency is something

00:03:47.696 --> 00:03:48.866
that you can do even on a single

00:03:48.866 --> 00:03:49.456
core system.

00:03:49.456 --> 00:03:51.016
It's about how you interpose the

00:03:51.016 --> 00:03:52.496
different tasks that are part of

00:03:52.496 --> 00:03:53.126
your application.

00:03:53.576 --> 00:03:55.296
So, let's start by talking about

00:03:55.296 --> 00:03:57.056
parallelism and how you might use

00:03:57.056 --> 00:03:58.576
it when you're writing an app.

00:03:59.676 --> 00:04:02.466
So, let's imagine you make an

00:04:02.466 --> 00:04:04.096
app and it processes huge

00:04:04.096 --> 00:04:04.596
images.

00:04:04.856 --> 00:04:05.916
And you want to be able to take

00:04:05.916 --> 00:04:07.136
advantage of the many cores on a

00:04:07.136 --> 00:04:08.566
Mac Pro to be able to process

00:04:08.566 --> 00:04:09.386
those images faster.

00:04:10.336 --> 00:04:11.936
What you do is break up that

00:04:11.936 --> 00:04:12.746
image into chunks.

00:04:13.576 --> 00:04:15.566
And have each core process those

00:04:15.566 --> 00:04:18.146
chunks in parallel.

00:04:19.466 --> 00:04:20.366
This gives you a speed up,

00:04:20.606 --> 00:04:21.236
because the cores are

00:04:21.236 --> 00:04:22.966
simultaneously working on

00:04:22.966 --> 00:04:23.996
different parts of the image.

00:04:25.346 --> 00:04:26.926
So, how do you implement this?

00:04:27.066 --> 00:04:29.506
Well first, you should stop and

00:04:29.506 --> 00:04:31.126
consider whether or not you could

00:04:31.126 --> 00:04:32.276
take advantage of our system

00:04:32.276 --> 00:04:32.736
frameworks.

00:04:33.396 --> 00:04:35.596
For example, accelerate has

00:04:35.656 --> 00:04:37.246
built-in support for parallel

00:04:37.246 --> 00:04:38.876
execution of advanced image

00:04:38.876 --> 00:04:39.436
algorithms.

00:04:40.066 --> 00:04:42.206
Metal and core image can take

00:04:42.206 --> 00:04:44.236
advantage of the powerful GPU.

00:04:45.846 --> 00:04:47.176
Well, let's say you've decided

00:04:47.176 --> 00:04:49.186
to implement this yourself, GCD

00:04:49.186 --> 00:04:50.596
gives you a tool that lets you

00:04:50.596 --> 00:04:52.146
easily express this pattern.

00:04:52.956 --> 00:04:54.396
The way you express parallelism in

00:04:54.396 --> 00:04:55.696
GCD is with the API called

00:04:55.696 --> 00:04:56.516
concurrentPerform.

00:04:57.246 --> 00:04:58.966
This lets the framework optimize

00:04:58.966 --> 00:05:00.296
the parallel case because it

00:05:00.296 --> 00:05:01.336
knows that you're trying to do a

00:05:01.446 --> 00:05:03.036
parallel computation across all

00:05:03.036 --> 00:05:03.586
the cores.

00:05:04.586 --> 00:05:06.466
concurrentPerform is a parallel

00:05:06.466 --> 00:05:07.986
for loop that automatically load

00:05:07.986 --> 00:05:09.366
balances your computation across

00:05:09.416 --> 00:05:11.896
all the cores in the system.

00:05:12.086 --> 00:05:13.326
When you use this with Swift, it

00:05:13.326 --> 00:05:14.446
automatically chooses the

00:05:14.446 --> 00:05:15.936
correct context to run all your

00:05:15.936 --> 00:05:16.616
computation in.

00:05:17.206 --> 00:05:18.386
This year, we've brought that

00:05:18.386 --> 00:05:19.746
same power to the objective C

00:05:19.746 --> 00:05:21.996
interface dispatch_apply with

00:05:21.996 --> 00:05:23.406
the DISPATCH_APPLY_AUTO keyword.

00:05:24.326 --> 00:05:25.656
This replaces the queue argument

00:05:26.436 --> 00:05:28.106
allowing the system to choose

00:05:28.106 --> 00:05:29.156
the right context to run your

00:05:29.156 --> 00:05:30.096
code in automatically.

00:05:31.416 --> 00:05:32.646
So, now let's take a look at

00:05:32.646 --> 00:05:33.716
this other parameter, which is

00:05:33.716 --> 00:05:34.636
the iteration count.

00:05:35.256 --> 00:05:36.256
This is how many times your

00:05:36.256 --> 00:05:37.416
block is called in parallel

00:05:37.416 --> 00:05:38.086
across the system.

00:05:39.456 --> 00:05:40.706
How do you choose a good value

00:05:40.706 --> 00:05:40.936
here?

00:05:41.776 --> 00:05:43.236
You might imagine that a good

00:05:43.236 --> 00:05:44.346
value would be the number of

00:05:44.406 --> 00:05:44.836
cores.

00:05:45.296 --> 00:05:47.376
Let's imagine that we're

00:05:47.376 --> 00:05:48.436
executing our workload on a

00:05:48.436 --> 00:05:49.246
three-core system.

00:05:50.006 --> 00:05:51.486
Here, you can see the ideal

00:05:51.486 --> 00:05:53.216
case, where three blocks run in

00:05:53.216 --> 00:05:54.396
parallel on all three cores.

00:05:55.016 --> 00:05:56.626
The real world isn't necessarily

00:05:56.626 --> 00:05:57.166
this perfect.

00:05:57.856 --> 00:05:58.906
What might happens if the third

00:05:58.906 --> 00:06:00.346
core is taken up for awhile

00:06:00.346 --> 00:06:02.166
with UI rendering?

00:06:03.356 --> 00:06:05.146
Well, what happens is the load

00:06:05.146 --> 00:06:06.696
balancer has to move that third

00:06:06.696 --> 00:06:09.136
block over to the first core in

00:06:09.136 --> 00:06:10.206
order to execute it, because

00:06:10.206 --> 00:06:11.166
it's the third course taken up.

00:06:11.566 --> 00:06:13.796
And we get a bubble of idle CPU.

00:06:13.946 --> 00:06:15.666
We could have taken advantage of

00:06:15.666 --> 00:06:17.556
this time, to do more parallel

00:06:17.556 --> 00:06:17.766
work.

00:06:18.166 --> 00:06:19.486
And so, instead our workload

00:06:19.486 --> 00:06:19.986
took longer.

00:06:21.296 --> 00:06:22.236
How can we fix that?

00:06:22.586 --> 00:06:24.166
Well, we can increase the

00:06:24.166 --> 00:06:26.866
iteration count and give the

00:06:26.866 --> 00:06:28.256
load balancer more flexibility.

00:06:29.766 --> 00:06:30.216
It looks good.

00:06:30.466 --> 00:06:31.186
That hole is gone.

00:06:31.846 --> 00:06:32.796
There's actually another hole

00:06:32.796 --> 00:06:35.676
over here, on the third core.

00:06:36.186 --> 00:06:37.166
We could take advantage of that

00:06:37.206 --> 00:06:37.736
time as well.

00:06:39.756 --> 00:06:42.226
So, as Tim said on Monday, let's

00:06:42.226 --> 00:06:43.996
turn the iteration count up to 11.

00:06:43.996 --> 00:06:48.986
There. We filled the hole, and

00:06:48.986 --> 00:06:50.106
we have efficient execution.

00:06:50.536 --> 00:06:51.676
We're using all of the available

00:06:51.676 --> 00:06:52.726
resources on the system until we

00:06:52.726 --> 00:06:53.116
finish.

00:06:53.606 --> 00:06:55.576
This is still a very simplistic

00:06:55.576 --> 00:06:56.036
example.

00:06:56.336 --> 00:06:57.546
To deal with the real-world

00:06:57.546 --> 00:06:59.216
complexity, you want to use an

00:06:59.276 --> 00:07:01.246
order of magnitude more, say

00:07:01.426 --> 00:07:02.066
1000.

00:07:03.136 --> 00:07:04.126
You can use a large enough

00:07:04.126 --> 00:07:05.526
iteration count so the load

00:07:05.526 --> 00:07:07.206
balancer has the flexibility to

00:07:07.206 --> 00:07:08.376
fill the gaps in the system and take

00:07:08.376 --> 00:07:10.356
the maximum advantage

00:07:10.356 --> 00:07:11.456
of the available

00:07:11.456 --> 00:07:12.366
resources of the system.

00:07:13.116 --> 00:07:14.456
However, you should be sure to

00:07:14.456 --> 00:07:15.816
balance the overhead of the load

00:07:15.816 --> 00:07:17.826
balancer versus the useful work

00:07:17.956 --> 00:07:19.866
that each block in your parallel

00:07:19.866 --> 00:07:20.596
for loop does.

00:07:21.706 --> 00:07:23.266
Remember that not every CPU is

00:07:23.266 --> 00:07:24.306
available to you all the time.

00:07:24.306 --> 00:07:26.056
There are many tasks running

00:07:26.136 --> 00:07:27.326
concurrently on the system.

00:07:27.636 --> 00:07:29.516
And additionally, not every

00:07:29.516 --> 00:07:30.606
worker thread will make equal

00:07:30.606 --> 00:07:31.006
progress.

00:07:32.496 --> 00:07:34.516
So, to recap, if you have a

00:07:34.516 --> 00:07:36.486
parallel problem, make sure to

00:07:36.486 --> 00:07:37.696
leverage the system frameworks

00:07:37.696 --> 00:07:38.416
that are available to you.

00:07:38.506 --> 00:07:40.476
You can use their power to solve

00:07:40.476 --> 00:07:41.006
your problem.

00:07:42.056 --> 00:07:43.326
Additionally, make sure to take

00:07:43.326 --> 00:07:44.696
advantage of the automatic load

00:07:44.696 --> 00:07:45.576
balancing inside

00:07:45.576 --> 00:07:46.356
concurrentPerform.

00:07:46.356 --> 00:07:47.926
Give it the flexibility to do

00:07:47.926 --> 00:07:48.746
what it does best.

00:07:49.496 --> 00:07:51.106
So, that's the discussion about

00:07:51.106 --> 00:07:51.636
parallelism.

00:07:51.936 --> 00:07:52.906
Now, let's switch to the main

00:07:52.906 --> 00:07:54.146
topic for today, which is

00:07:54.146 --> 00:07:54.806
concurrency.

00:07:56.196 --> 00:07:58.456
So, concurrency.

00:07:59.586 --> 00:08:00.886
Let's image you're writing a

00:08:00.886 --> 00:08:01.776
simple news app.

00:08:02.896 --> 00:08:03.656
How would you structure it?

00:08:03.856 --> 00:08:05.696
Well, you start by breaking it

00:08:05.696 --> 00:08:06.776
up into the independent

00:08:06.776 --> 00:08:08.456
subsystems that make up the app.

00:08:09.236 --> 00:08:10.216
Thinking about how you might

00:08:10.216 --> 00:08:11.766
break up a news app into its

00:08:11.766 --> 00:08:13.236
independent subsystems, you

00:08:13.236 --> 00:08:14.796
might have a UI component that

00:08:14.796 --> 00:08:15.806
renders the UI, that's the main

00:08:15.806 --> 00:08:16.116
thread.

00:08:16.706 --> 00:08:18.266
You might also have a database

00:08:18.626 --> 00:08:20.336
that stores those articles.

00:08:20.736 --> 00:08:21.826
And you might have a networking

00:08:21.826 --> 00:08:22.926
subsystem that fetches those

00:08:22.926 --> 00:08:23.796
articles from the network.

00:08:23.796 --> 00:08:25.606
To give a better picture of

00:08:25.736 --> 00:08:26.926
how this app works and breaking

00:08:26.926 --> 00:08:28.466
it up into subsystems gives you

00:08:28.466 --> 00:08:30.176
an advantage, let's visualize

00:08:30.616 --> 00:08:31.906
how that executes concurrently

00:08:31.956 --> 00:08:32.746
on a modern system.

00:08:33.426 --> 00:08:36.626
So, let's say here's a timeline

00:08:36.785 --> 00:08:38.826
that shows at the top the CPU

00:08:38.826 --> 00:08:39.155
track.

00:08:39.155 --> 00:08:40.176
Let's image that we only have

00:08:40.176 --> 00:08:41.076
one CPU remaining.

00:08:41.076 --> 00:08:42.216
The other CPUs are busy for some

00:08:42.216 --> 00:08:42.446
reason.

00:08:43.145 --> 00:08:44.275
We only have one core available.

00:08:44.736 --> 00:08:46.386
At any time only one of these

00:08:46.386 --> 00:08:47.696
threads can run on that CPU.

00:08:48.526 --> 00:08:50.116
So, what happens when a user

00:08:50.116 --> 00:08:51.696
clicks the button and refreshes

00:08:51.696 --> 00:08:53.356
the article list in the news

00:08:53.356 --> 00:08:53.526
app?

00:08:54.126 --> 00:08:55.286
Well, these interface renders

00:08:55.286 --> 00:08:56.676
the response to that button and

00:08:56.946 --> 00:08:58.026
then sends an asynchronous

00:08:58.026 --> 00:08:58.876
Request to the database.

00:09:00.136 --> 00:09:01.346
And then the database decides it

00:09:01.346 --> 00:09:02.736
needs to refresh the articles,

00:09:02.976 --> 00:09:04.266
which issues another command

00:09:04.846 --> 00:09:05.946
to the networking subsystem.

00:09:06.886 --> 00:09:08.096
However, at this point, the user

00:09:08.096 --> 00:09:08.956
touches the app again.

00:09:09.936 --> 00:09:12.146
And because the database is run

00:09:12.216 --> 00:09:13.086
off the main thread of the

00:09:13.086 --> 00:09:14.896
application, the OS can

00:09:14.896 --> 00:09:16.236
immediately switch the CPU to

00:09:16.236 --> 00:09:18.026
working on the UI thread, and it

00:09:18.026 --> 00:09:19.126
can respond immediately to the

00:09:19.126 --> 00:09:20.696
user without having to wait for

00:09:20.886 --> 00:09:21.986
the database thread to complete.

00:09:23.616 --> 00:09:25.126
This is the advantage of moving

00:09:25.236 --> 00:09:26.386
work off the main thread.

00:09:28.366 --> 00:09:30.016
When the user interface is done

00:09:30.016 --> 00:09:32.296
responding, the CPU can then

00:09:32.296 --> 00:09:33.266
switch back to the database

00:09:33.316 --> 00:09:34.326
thread, and then finish the

00:09:34.326 --> 00:09:35.296
networking task as well.

00:09:36.496 --> 00:09:37.776
So, taking advantage of

00:09:37.776 --> 00:09:39.476
concurrency like this lets you

00:09:39.476 --> 00:09:40.696
build responsive apps.

00:09:40.736 --> 00:09:42.616
The main thread can always

00:09:42.616 --> 00:09:43.806
respond to the user's action

00:09:44.076 --> 00:09:45.256
without having to wait for other

00:09:45.256 --> 00:09:46.146
parts of your application to

00:09:46.146 --> 00:09:46.516
complete.

00:09:46.806 --> 00:09:48.886
So, let's take a look at what

00:09:48.956 --> 00:09:50.376
that looks like to the CPU.

00:09:51.416 --> 00:09:52.716
These white lines above here

00:09:52.896 --> 00:09:54.266
show the context switches

00:09:54.316 --> 00:09:55.206
between the subsystems.

00:09:55.946 --> 00:09:57.616
A context switch is when the CPU

00:09:57.616 --> 00:09:58.686
switches between these different

00:09:58.686 --> 00:10:00.356
subsystems or threads that make

00:10:00.356 --> 00:10:01.146
up your application.

00:10:02.066 --> 00:10:03.146
If you want to visualize what

00:10:03.196 --> 00:10:04.226
this looks like to your

00:10:04.226 --> 00:10:05.636
application, you can use

00:10:05.636 --> 00:10:07.226
Instrument System Trace, which

00:10:07.226 --> 00:10:08.856
shows you what the CPUs and the

00:10:08.856 --> 00:10:10.016
threads are doing when they're

00:10:10.016 --> 00:10:11.026
running in your application.

00:10:11.446 --> 00:10:12.806
If you want to learn more about

00:10:12.806 --> 00:10:14.316
this, you can watch the "System

00:10:14.316 --> 00:10:15.716
Trace In-Depth" Talk from last

00:10:15.716 --> 00:10:16.636
year where the Instrument's team

00:10:16.636 --> 00:10:17.946
described how you use System

00:10:17.946 --> 00:10:18.226
Trace.

00:10:19.276 --> 00:10:21.186
So, this concept of context

00:10:21.186 --> 00:10:22.576
switching is where the power of

00:10:22.576 --> 00:10:23.596
concurrency comes from.

00:10:24.096 --> 00:10:25.546
Let's look at when these context

00:10:25.546 --> 00:10:27.006
switches might happen and what

00:10:27.006 --> 00:10:27.496
causes them.

00:10:28.366 --> 00:10:30.646
Well, they can start when a high

00:10:30.646 --> 00:10:32.226
priority thread needs the CPU as

00:10:32.226 --> 00:10:33.686
we saw earlier, with the UI

00:10:33.686 --> 00:10:34.806
thread pre-empting the database

00:10:34.806 --> 00:10:35.056
thread.

00:10:35.966 --> 00:10:37.366
It can also happen when a thread

00:10:37.366 --> 00:10:39.646
finishes its current work, or

00:10:39.826 --> 00:10:40.796
it's waiting to acquire

00:10:40.796 --> 00:10:41.356
resource.

00:10:41.736 --> 00:10:42.506
Or it's waiting for an

00:10:42.506 --> 00:10:43.406
asynchronous request to

00:10:43.406 --> 00:10:43.736
complete.

00:10:44.856 --> 00:10:46.446
However, with this great power

00:10:46.446 --> 00:10:48.406
of concurrency comes great

00:10:48.406 --> 00:10:49.576
responsibility as well.

00:10:49.766 --> 00:10:51.216
You can have too much of a good

00:10:51.216 --> 00:10:51.436
thing.

00:10:52.896 --> 00:10:54.266
Let's say you're switching

00:10:54.266 --> 00:10:56.116
between the network and database

00:10:56.116 --> 00:10:57.666
threads on your CPU.

00:10:58.386 --> 00:10:59.626
A few context switches is fine,

00:10:59.626 --> 00:11:00.586
that's the power of concurrency

00:11:00.586 --> 00:11:01.236
you're switching between

00:11:01.236 --> 00:11:01.936
different tasks.

00:11:02.616 --> 00:11:04.146
However, if you're doing this

00:11:04.146 --> 00:11:05.686
thousands of times in really

00:11:05.816 --> 00:11:07.706
rapid succession, you run into

00:11:07.706 --> 00:11:08.046
trouble.

00:11:08.126 --> 00:11:08.726
You're starting to lose

00:11:08.786 --> 00:11:10.776
performance because each white

00:11:10.776 --> 00:11:12.656
bar here is a context switch.

00:11:13.066 --> 00:11:14.066
And the overhead of a context

00:11:14.066 --> 00:11:14.726
switch adds up.

00:11:14.726 --> 00:11:16.076
It's not just the time we spend

00:11:16.076 --> 00:11:17.156
executing the context switch,

00:11:17.486 --> 00:11:18.606
it's also the history that the

00:11:18.606 --> 00:11:19.816
core has built up, it has to

00:11:19.846 --> 00:11:21.076
regain that history after every

00:11:21.076 --> 00:11:22.306
Context switch.

00:11:22.996 --> 00:11:24.666
There are also other effects you

00:11:24.666 --> 00:11:25.276
might experience.

00:11:25.556 --> 00:11:26.626
For example, there might be

00:11:26.626 --> 00:11:28.566
others ahead of you in line for

00:11:28.566 --> 00:11:29.506
access to the CPU.

00:11:30.486 --> 00:11:32.356
You have to wait each time you

00:11:32.356 --> 00:11:33.966
context switch for the rest of

00:11:33.966 --> 00:11:35.316
the queue to drain out and so

00:11:35.316 --> 00:11:37.996
you may be delayed by somebody

00:11:37.996 --> 00:11:38.926
else ahead of you in line.

00:11:39.616 --> 00:11:41.026
So, let's look at what might

00:11:41.056 --> 00:11:42.336
cause excessive context

00:11:42.336 --> 00:11:42.576
switching.

00:11:43.696 --> 00:11:45.446
So, there's three main causes

00:11:45.446 --> 00:11:46.376
we're going to talk about today.

00:11:46.966 --> 00:11:48.416
First, repeatedly waiting for

00:11:48.416 --> 00:11:49.786
exclusive access to contended

00:11:49.786 --> 00:11:50.336
resources.

00:11:50.896 --> 00:11:52.086
Repeatedly switching between

00:11:52.086 --> 00:11:53.836
independent operations, and

00:11:53.836 --> 00:11:55.306
repeatedly bouncing an operation

00:11:55.446 --> 00:11:56.136
between threads.

00:11:56.956 --> 00:11:57.936
You note, that I repeated the

00:11:57.936 --> 00:11:59.846
word repeatedly several times.

00:12:00.596 --> 00:12:01.376
That's intentional.

00:12:03.336 --> 00:12:04.856
Context switching a few times is

00:12:04.856 --> 00:12:06.396
okay, that's how concurrency

00:12:06.396 --> 00:12:07.436
works, that's the power that

00:12:07.436 --> 00:12:07.986
we're giving you.

00:12:08.646 --> 00:12:10.206
However, when you repeat it too

00:12:10.206 --> 00:12:11.746
many times, the costs start to

00:12:11.746 --> 00:12:12.126
add up.

00:12:13.376 --> 00:12:14.576
So, let's start by looking at

00:12:14.576 --> 00:12:15.796
the first case, which is

00:12:15.796 --> 00:12:17.176
exclusive access to contended

00:12:17.176 --> 00:12:17.826
resources.

00:12:18.786 --> 00:12:19.546
When can this happen?

00:12:20.306 --> 00:12:22.696
Well, the primary case in which

00:12:22.696 --> 00:12:24.176
this happens is when you have a

00:12:24.176 --> 00:12:26.046
lock and a bunch of threads are

00:12:26.046 --> 00:12:28.176
all trying to acquire that lock.

00:12:29.486 --> 00:12:30.716
So, how can you tell this is

00:12:30.716 --> 00:12:31.686
occurring in your application?

00:12:31.896 --> 00:12:33.426
Well, we can go back to system

00:12:33.426 --> 00:12:33.786
trace.

00:12:34.346 --> 00:12:35.736
We can visualize what it looks

00:12:35.736 --> 00:12:36.466
like in instruments.

00:12:37.246 --> 00:12:38.776
So, let's say this shows us that

00:12:38.776 --> 00:12:40.306
we have many threads running for

00:12:40.306 --> 00:12:41.386
a very short time and they're

00:12:41.386 --> 00:12:42.616
all handing off to each other in

00:12:42.616 --> 00:12:43.376
a little cascade.

00:12:44.276 --> 00:12:45.446
Let's focus on the first thread

00:12:45.446 --> 00:12:46.306
and see what instruments is

00:12:46.306 --> 00:12:46.756
telling us.

00:12:47.896 --> 00:12:49.746
We have this blue track, which

00:12:49.746 --> 00:12:51.846
shows when a thread is on CPU.

00:12:52.526 --> 00:12:53.856
And the red track shows when

00:12:53.856 --> 00:12:54.686
it's making a sys call.

00:12:55.136 --> 00:12:56.776
In this case, it's making

00:12:56.776 --> 00:12:58.586
mutex_wait sys call.

00:12:58.586 --> 00:13:00.336
This shows that most of its time

00:13:00.336 --> 00:13:01.276
is waiting for the mutex

00:13:01.276 --> 00:13:02.216
to become available.

00:13:02.216 --> 00:13:04.536
And the on core time is very

00:13:04.536 --> 00:13:06.096
short at only 10 microseconds.

00:13:06.996 --> 00:13:08.316
And there are a lot of context

00:13:08.316 --> 00:13:09.596
switches going on on the system,

00:13:09.926 --> 00:13:10.596
which it shows you on the

00:13:10.596 --> 00:13:11.566
context switches track at the

00:13:11.566 --> 00:13:11.836
top.

00:13:13.806 --> 00:13:15.636
So, what's causing this?

00:13:15.856 --> 00:13:17.006
Let's go back to look at our

00:13:17.006 --> 00:13:19.456
simple timeline and see how

00:13:19.736 --> 00:13:23.636
excess contention could be

00:13:23.636 --> 00:13:24.146
playing out.

00:13:25.056 --> 00:13:25.956
So, you see this sort of

00:13:25.956 --> 00:13:27.526
staircase pattern in time.

00:13:27.996 --> 00:13:29.056
Where each thread is running for

00:13:29.056 --> 00:13:30.416
a short time, and then giving up

00:13:30.416 --> 00:13:31.446
the CPU to the next thread,

00:13:31.626 --> 00:13:32.976
rinse and repeat, for a long

00:13:32.976 --> 00:13:33.196
time.

00:13:33.906 --> 00:13:35.266
You want your work to look more

00:13:35.266 --> 00:13:35.826
like this.

00:13:36.696 --> 00:13:38.456
Where the CPU can focus

00:13:38.456 --> 00:13:39.926
on one thing at a time, get it

00:13:39.926 --> 00:13:41.096
done, and then work on the next

00:13:41.186 --> 00:13:41.376
task.

00:13:42.336 --> 00:13:43.526
So, what's going on here that

00:13:43.526 --> 00:13:44.536
causes that staircase?

00:13:44.706 --> 00:13:45.876
Let's zoom in to one of these

00:13:45.876 --> 00:13:46.566
stair steps.

00:13:47.616 --> 00:13:49.476
So, here we're focusing on two

00:13:49.476 --> 00:13:50.666
threads, the green thread and

00:13:50.666 --> 00:13:51.226
the blue thread.

00:13:51.776 --> 00:13:53.546
And we have a CPU on top.

00:13:54.246 --> 00:13:55.476
We've added a new lock track

00:13:55.566 --> 00:13:56.806
here that shows the state of the

00:13:56.806 --> 00:13:58.106
lock and what thread owns it.

00:13:58.336 --> 00:14:00.126
In this case, the blue thread

00:14:00.126 --> 00:14:01.676
owns the lock, and the green

00:14:01.676 --> 00:14:02.206
thread is waiting.

00:14:03.456 --> 00:14:04.366
So, when the blue thread

00:14:04.506 --> 00:14:06.166
unlocks, the ownership of that

00:14:06.166 --> 00:14:07.296
lock is transferred to the green

00:14:07.296 --> 00:14:08.276
thread, because it's next in

00:14:08.276 --> 00:14:08.556
line.

00:14:09.446 --> 00:14:11.246
However, when the blue thread

00:14:11.676 --> 00:14:12.926
turns around and grabs the lock

00:14:12.926 --> 00:14:14.856
again, it can't because the lock

00:14:14.856 --> 00:14:15.706
is reserved for the green

00:14:15.706 --> 00:14:15.966
thread.

00:14:16.666 --> 00:14:18.216
It forces a context switch

00:14:18.216 --> 00:14:19.436
because we now have to do

00:14:19.436 --> 00:14:20.106
something else.

00:14:20.486 --> 00:14:21.436
And we switch to the green

00:14:21.436 --> 00:14:23.496
thread, and the CPU can then

00:14:24.106 --> 00:14:25.346
finish the lock and we can

00:14:25.346 --> 00:14:25.696
repeat.

00:14:27.056 --> 00:14:28.056
Sometimes this is useful.

00:14:28.276 --> 00:14:29.376
You want every thread that's

00:14:29.376 --> 00:14:30.256
waiting on the lock to get a

00:14:30.256 --> 00:14:31.886
chance to acquire the resource,

00:14:32.606 --> 00:14:34.756
however, what if you had a lock

00:14:34.806 --> 00:14:35.746
that works a different way.

00:14:36.536 --> 00:14:38.696
Let's start again by looking at

00:14:38.696 --> 00:14:39.856
what an unfair lock does.

00:14:40.866 --> 00:14:43.006
So, this time when blue thread

00:14:43.006 --> 00:14:45.006
unlocks, the lock isn't

00:14:45.056 --> 00:14:45.456
reserved.

00:14:45.856 --> 00:14:46.946
The ownership of the lock is up

00:14:46.946 --> 00:14:47.616
for grabs.

00:14:48.616 --> 00:14:50.156
Blue can take the lock again,

00:14:50.536 --> 00:14:52.026
and it can immediately reacquire

00:14:52.026 --> 00:14:54.066
and stay on CPU without forcing

00:14:54.066 --> 00:14:54.786
a context switch.

00:14:55.716 --> 00:14:56.876
This might make it difficult for

00:14:56.876 --> 00:14:57.896
the green thread to actually get

00:14:57.896 --> 00:14:59.786
a chance at the lock, but it

00:14:59.786 --> 00:15:00.966
reduces the number of context

00:15:00.966 --> 00:15:02.346
switches the blue thread has to

00:15:02.886 --> 00:15:06.466
have in order to reacquire the

00:15:06.466 --> 00:15:06.636
lock.

00:15:08.016 --> 00:15:09.106
So, to recap when we're talking

00:15:09.106 --> 00:15:10.276
about lock contention, you

00:15:10.276 --> 00:15:11.716
actually want to make sure to

00:15:11.716 --> 00:15:12.986
measure your application in

00:15:12.986 --> 00:15:14.616
System Trace and see if you have

00:15:14.616 --> 00:15:15.036
an issue.

00:15:15.496 --> 00:15:17.586
If you do, often the unfair lock

00:15:17.866 --> 00:15:19.296
works best for objects,

00:15:19.646 --> 00:15:21.046
properties, for global state in

00:15:21.046 --> 00:15:22.266
your application that may have

00:15:22.266 --> 00:15:23.926
the lock taken and dropped many, many times.

00:15:25.526 --> 00:15:26.486
There's one other thing I want

00:15:26.486 --> 00:15:27.186
to talk about when we're

00:15:27.186 --> 00:15:28.326
mentioning locks, and that is

00:15:28.326 --> 00:15:29.096
lock ownership.

00:15:29.866 --> 00:15:31.276
So, remember the lock track we

00:15:31.276 --> 00:15:33.386
had earlier, the runtime knows

00:15:33.386 --> 00:15:34.386
which thread will unlock the

00:15:34.386 --> 00:15:34.876
lock next.

00:15:36.016 --> 00:15:37.256
We can take advantage of that

00:15:37.256 --> 00:15:38.796
power to automatically resolve

00:15:38.796 --> 00:15:40.056
priority inversions in your app

00:15:40.496 --> 00:15:41.666
between the waiters and the

00:15:41.666 --> 00:15:42.356
owners of the lock.

00:15:42.776 --> 00:15:44.216
And even enable other

00:15:44.216 --> 00:15:46.056
optimizations, like directed CPU

00:15:46.056 --> 00:15:47.166
handoff to the owning thread.

00:15:47.756 --> 00:15:48.866
Pierre is going to discuss this

00:15:49.156 --> 00:15:50.356
later in our talk, when talking

00:15:50.356 --> 00:15:51.096
about dispatch sync.

00:15:52.196 --> 00:15:54.286
We often get the question, which

00:15:54.346 --> 00:15:55.726
primitives have this power and

00:15:55.726 --> 00:15:56.426
which ones don't.

00:15:57.476 --> 00:15:58.736
Let's take a look at which

00:15:58.736 --> 00:15:59.776
low-level primitives do this

00:15:59.836 --> 00:16:00.106
today.

00:16:01.506 --> 00:16:03.096
So, primitives with a single

00:16:03.146 --> 00:16:04.536
known owner have this power.

00:16:04.926 --> 00:16:06.536
Things like serial queues and OS

00:16:06.536 --> 00:16:07.096
unfair lock.

00:16:08.186 --> 00:16:10.616
However, asymmetric primitives,

00:16:10.616 --> 00:16:11.976
like dispatch semaphore and

00:16:11.976 --> 00:16:13.466
dispatch group don't have this

00:16:13.536 --> 00:16:14.666
power, because the runtime

00:16:14.666 --> 00:16:15.826
doesn't know what thread will

00:16:15.826 --> 00:16:16.886
singal the sync primitive.

00:16:18.636 --> 00:16:20.286
Finally, primitives with

00:16:20.286 --> 00:16:22.106
multiple owners like private,

00:16:22.106 --> 00:16:23.416
concurrent queues and reader or

00:16:23.416 --> 00:16:24.736
writers locks, the systems

00:16:24.736 --> 00:16:25.726
doesn't take advantage of that

00:16:25.776 --> 00:16:26.626
today, because there isn't a

00:16:26.626 --> 00:16:27.236
single owner.

00:16:27.236 --> 00:16:29.516
When you're picking a primitive

00:16:30.036 --> 00:16:31.576
consider whether or not your use

00:16:31.646 --> 00:16:32.936
case involves threads of

00:16:32.936 --> 00:16:33.656
different priorities

00:16:33.656 --> 00:16:34.226
interacting.

00:16:35.096 --> 00:16:37.096
In like a high priority UI thread

00:16:37.096 --> 00:16:38.416
with a lower priority background

00:16:38.416 --> 00:16:38.636
thread.

00:16:39.836 --> 00:16:41.456
If so, you might want to take

00:16:41.456 --> 00:16:42.416
advantage of a primitive with

00:16:42.456 --> 00:16:44.736
ownership that ensures that your

00:16:44.886 --> 00:16:46.416
UI thread doesn't get delayed by

00:16:46.416 --> 00:16:47.336
waiting on a lower priority

00:16:47.336 --> 00:16:48.606
background thread.

00:16:49.256 --> 00:16:51.056
So, in summary, these

00:16:51.126 --> 00:16:52.526
inefficient behaviors are often

00:16:52.526 --> 00:16:53.496
emergent properties of your

00:16:53.496 --> 00:16:54.126
application.

00:16:54.486 --> 00:16:55.806
It's not easy to find these

00:16:55.846 --> 00:16:56.976
problems just by looking at your

00:16:56.976 --> 00:16:57.336
code.

00:16:57.816 --> 00:16:59.196
You should observe it in

00:16:59.196 --> 00:17:00.846
Instruments System Trace to

00:17:00.846 --> 00:17:02.446
visualize your apps true, real

00:17:02.446 --> 00:17:04.425
behavior and so you can use the

00:17:04.486 --> 00:17:05.526
right lock for the job.

00:17:06.526 --> 00:17:07.415
So, I've just discussed the

00:17:07.516 --> 00:17:09.016
first cause on our context

00:17:09.016 --> 00:17:10.165
switching list, which is

00:17:10.165 --> 00:17:11.246
exclusive access.

00:17:12.566 --> 00:17:13.925
To discuss some other ways your

00:17:13.925 --> 00:17:15.256
apps can experience excessive

00:17:15.256 --> 00:17:16.445
context switching, I'm going to

00:17:16.445 --> 00:17:18.726
bring out my Daniel Steffen to

00:17:19.056 --> 00:17:20.476
talk to you about how you can

00:17:20.526 --> 00:17:21.856
organize your concurrency with

00:17:21.856 --> 00:17:23.886
GCD, to avoid these pitfalls.

00:17:26.516 --> 00:17:31.126
[ Applause ]

00:17:31.626 --> 00:17:31.866
>> All right.

00:17:32.076 --> 00:17:32.666
Thank you, Daniel.

00:17:34.336 --> 00:17:35.486
So, we've got a lot to cover

00:17:35.486 --> 00:17:37.036
today, so I won't be able to go

00:17:37.036 --> 00:17:38.556
into too many details on the

00:17:38.556 --> 00:17:39.856
fundamentals of GCD.

00:17:40.406 --> 00:17:41.966
If you're new to the technology,

00:17:42.376 --> 00:17:43.686
or need a bit of a refresher,

00:17:43.796 --> 00:17:44.936
here are some of the sessions at

00:17:44.936 --> 00:17:47.286
previous WWDC conferences that

00:17:47.286 --> 00:17:48.746
covered GCD and the enhancements

00:17:48.746 --> 00:17:49.626
that we've made to it over the

00:17:49.626 --> 00:17:50.106
years.

00:17:50.686 --> 00:17:51.866
So, I encourage you to go and

00:17:51.866 --> 00:17:52.806
see those on video.

00:17:53.496 --> 00:17:55.266
We do need a few of the basic

00:17:55.266 --> 00:17:56.876
concepts of GCD today however,

00:17:56.876 --> 00:17:58.776
starting with the serial

00:17:58.926 --> 00:17:59.566
dispatch queue.

00:18:00.566 --> 00:18:01.936
This is really our fundamental

00:18:02.446 --> 00:18:03.746
synchronization primitive in

00:18:03.746 --> 00:18:04.086
GCD.

00:18:04.086 --> 00:18:06.266
It provides you with mutual

00:18:06.266 --> 00:18:07.706
exclusion as well as FIFO

00:18:07.736 --> 00:18:07.996
ordering.

00:18:07.996 --> 00:18:09.836
This is one of these ordered and

00:18:10.106 --> 00:18:11.426
fair primitives that Daniel just

00:18:11.426 --> 00:18:11.796
mentioned.

00:18:13.296 --> 00:18:15.196
And it has a concurrent atomic

00:18:15.196 --> 00:18:16.406
enqueue operation, so it's fine

00:18:16.406 --> 00:18:17.406
for multiple threads to

00:18:17.406 --> 00:18:18.736
enqueue, operations into the queue

00:18:18.736 --> 00:18:20.396
at the same time, as well as a

00:18:20.396 --> 00:18:21.706
single dequeue thread that the

00:18:21.706 --> 00:18:23.216
system provides to execute

00:18:23.216 --> 00:18:24.376
asynchronous work out of the

00:18:24.376 --> 00:18:24.676
queue.

00:18:25.416 --> 00:18:26.746
So, let's look at an example of

00:18:26.846 --> 00:18:27.496
this in action.

00:18:27.496 --> 00:18:29.436
Here we're creating a serial

00:18:29.466 --> 00:18:30.926
queue by calling the dispatch

00:18:30.926 --> 00:18:33.256
queue constructor and that will

00:18:33.256 --> 00:18:34.976
give you a piece of memory that

00:18:34.976 --> 00:18:36.426
as long as you haven't used it

00:18:36.426 --> 00:18:37.506
yet, it's just inert in your

00:18:37.506 --> 00:18:38.066
application.

00:18:38.826 --> 00:18:40.006
Now, imagine there's two threads

00:18:40.006 --> 00:18:41.046
that come along in call the

00:18:41.046 --> 00:18:43.086
queue.async method to submit

00:18:43.086 --> 00:18:44.416
some asynchronous work into this

00:18:44.416 --> 00:18:44.736
queue.

00:18:45.396 --> 00:18:46.336
As mentioned, it's fine for

00:18:46.336 --> 00:18:47.646
multiple threads to do this, and

00:18:47.646 --> 00:18:49.546
the items will just get enqueued

00:18:49.546 --> 00:18:50.726
in the order that they appeared.

00:18:52.296 --> 00:18:53.186
And because this is the

00:18:53.186 --> 00:18:54.946
asynchronous method, this method

00:18:54.946 --> 00:18:57.846
returns and the threads can go

00:18:57.846 --> 00:18:59.486
on their way, so maybe this

00:18:59.656 --> 00:19:00.776
first thread eventually calls

00:19:00.776 --> 00:19:01.526
queue.sync.

00:19:01.966 --> 00:19:02.886
This is the way you interact

00:19:02.886 --> 00:19:03.936
synchronously with the queue.

00:19:04.316 --> 00:19:05.356
And because this is an ordered

00:19:05.356 --> 00:19:06.526
primitive here, what this does

00:19:06.526 --> 00:19:07.566
is it will enqueue a

00:19:07.566 --> 00:19:09.726
placeholder into the queue so

00:19:09.726 --> 00:19:12.716
that the thread can wait until

00:19:12.716 --> 00:19:13.526
it is its turn.

00:19:14.556 --> 00:19:16.876
And now, there's an asynchronous

00:19:17.136 --> 00:19:18.506
worker thread that will come

00:19:18.506 --> 00:19:19.826
along to execute the

00:19:19.826 --> 00:19:21.556
asynchronous work items, until

00:19:21.556 --> 00:19:22.896
you get to that placeholder at

00:19:22.896 --> 00:19:24.446
which point the ownership of the

00:19:24.446 --> 00:19:25.686
queue will transfer to the

00:19:25.746 --> 00:19:27.136
thread waiting in queue.sync so

00:19:27.136 --> 00:19:29.056
that it can execute its block.

00:19:30.126 --> 00:19:32.746
So, the next concept that we'll

00:19:32.746 --> 00:19:34.876
need is the dispatch source.

00:19:35.106 --> 00:19:36.286
This is our event monitoring

00:19:36.286 --> 00:19:37.456
primitive in GCD.

00:19:37.846 --> 00:19:39.446
Here we are setting one up to

00:19:39.446 --> 00:19:40.536
monitor a file descriptor

00:19:40.536 --> 00:19:42.646
for readability with the

00:19:42.646 --> 00:19:43.436
makeRead source constructor.

00:19:43.806 --> 00:19:45.656
You pass it in a queue which is

00:19:45.686 --> 00:19:46.996
the target queue of the source,

00:19:47.336 --> 00:19:48.906
which is where we execute the

00:19:48.906 --> 00:19:50.126
event handler of the source,

00:19:50.446 --> 00:19:51.496
which here just reads from the

00:19:51.496 --> 00:19:52.246
File descriptor.

00:19:53.046 --> 00:19:54.166
This target queue is also where

00:19:54.166 --> 00:19:55.406
you might put other work that

00:19:55.406 --> 00:19:56.646
should be serialized with this

00:19:56.646 --> 00:19:58.426
operation, such as processing

00:19:58.426 --> 00:19:59.656
the data that was read.

00:20:00.816 --> 00:20:02.206
Then, we set the cancel handler

00:20:02.206 --> 00:20:03.396
for the source, which is how

00:20:03.396 --> 00:20:04.786
sources implement the

00:20:04.816 --> 00:20:06.326
invalidation pattern.

00:20:07.056 --> 00:20:08.066
And finally, when everything is

00:20:08.096 --> 00:20:09.166
set up, we call source. activate

00:20:09.166 --> 00:20:10.566
to start monitoring.

00:20:11.196 --> 00:20:12.736
So, it's worth noting that

00:20:12.826 --> 00:20:14.546
sources are really just an

00:20:14.546 --> 00:20:15.806
instance of a more general

00:20:15.806 --> 00:20:17.606
pattern throughout the OS, where

00:20:17.706 --> 00:20:19.116
you have objects that deliver

00:20:19.116 --> 00:20:20.566
events to you on a target queue

00:20:20.566 --> 00:20:21.486
that you specify.

00:20:22.086 --> 00:20:23.696
So, if you're familiar with XPC,

00:20:23.696 --> 00:20:25.476
that would be another example of

00:20:25.546 --> 00:20:26.666
that, XPC connections.

00:20:27.986 --> 00:20:30.246
And, it's worth noting that all

00:20:30.246 --> 00:20:31.086
of everything we're telling you

00:20:31.086 --> 00:20:32.406
today about sources really

00:20:32.406 --> 00:20:33.926
applies to all such objects in

00:20:33.926 --> 00:20:34.266
general.

00:20:36.286 --> 00:20:37.616
So, putting these two concepts

00:20:37.616 --> 00:20:39.526
together, we get what we call

00:20:39.526 --> 00:20:40.606
the target queue hierarchy.

00:20:41.346 --> 00:20:46.156
So, here we have two sources

00:20:46.156 --> 00:20:48.266
with their associated target

00:20:48.296 --> 00:20:50.466
queues, S1, S2 and the queue is

00:20:50.536 --> 00:20:50.886
Q1 and Q2.

00:20:50.886 --> 00:20:52.416
And we can form a little tree

00:20:52.416 --> 00:20:53.776
out of this situation by adding

00:20:53.776 --> 00:20:54.856
yet another serial queue to the

00:20:54.856 --> 00:20:57.296
mix, by adding a mutual exclusion

00:20:57.296 --> 00:20:58.706
queue, EQ, at the bottom.

00:20:59.336 --> 00:21:00.636
The way we do this is simply by

00:21:00.636 --> 00:21:02.006
passing in the optional target

00:21:02.006 --> 00:21:03.486
argument into the dispatch queue

00:21:03.486 --> 00:21:03.996
constructor.

00:21:03.996 --> 00:21:06.786
So, this gives you a shared

00:21:06.786 --> 00:21:08.216
single mutual exclusion context

00:21:08.216 --> 00:21:09.096
for this whole tree.

00:21:09.506 --> 00:21:11.466
Only one of the sources or one

00:21:11.466 --> 00:21:12.636
item in one of the queues can

00:21:12.636 --> 00:21:13.686
execute at one time.

00:21:14.426 --> 00:21:16.086
But it preserves the independent

00:21:16.086 --> 00:21:17.606
individual queue order for queue

00:21:17.606 --> 00:21:18.756
1 and queue 2.

00:21:19.216 --> 00:21:20.456
So, let's look at what I mean by

00:21:20.456 --> 00:21:21.586
that.

00:21:22.376 --> 00:21:23.876
Here I have the two queues,

00:21:23.876 --> 00:21:25.806
queue 1 and queue 2 with some itmes

00:21:25.806 --> 00:21:27.036
enqueued in a specific order.

00:21:27.796 --> 00:21:28.936
And because we have this extra

00:21:28.936 --> 00:21:30.476
serial queue at the bottom, and

00:21:30.476 --> 00:21:32.186
this executes, they will execute

00:21:32.266 --> 00:21:34.516
in EQ and there will be a single

00:21:34.516 --> 00:21:35.626
worker thread executing these

00:21:35.626 --> 00:21:36.916
items giving you that mutual

00:21:36.916 --> 00:21:38.386
exclusion property, only one

00:21:38.386 --> 00:21:39.686
item executing at one time.

00:21:39.736 --> 00:21:41.476
But as you can see, the items

00:21:41.476 --> 00:21:42.756
from both queues can execute

00:21:42.796 --> 00:21:44.736
interleaved while preserving the

00:21:44.736 --> 00:21:45.996
individual order that they had

00:21:45.996 --> 00:21:47.056
in their original queues.

00:21:47.606 --> 00:21:51.236
So, the last concept that we'll

00:21:51.236 --> 00:21:53.026
need today, is the notion of

00:21:53.026 --> 00:21:54.026
quality of service.

00:21:55.296 --> 00:21:56.906
This is a fairly deep concept

00:21:56.906 --> 00:21:57.876
that was talked about in some

00:21:57.876 --> 00:21:58.886
detail in the past.

00:21:58.886 --> 00:22:00.006
In particular, in the 'Power

00:22:00.006 --> 00:22:01.166
Performance and Diagnostics'

00:22:01.166 --> 00:22:03.726
session in 2014.

00:22:03.726 --> 00:22:05.576
So, if this is new to you, I

00:22:05.576 --> 00:22:07.066
would encourage you to go and

00:22:07.066 --> 00:22:07.606
watch that.

00:22:08.456 --> 00:22:09.826
But what we'll need today from

00:22:09.826 --> 00:22:11.556
this is really mostly it's

00:22:11.916 --> 00:22:13.226
abstract notion of priority.

00:22:14.686 --> 00:22:16.486
And we'll use the terms QOS and

00:22:16.486 --> 00:22:18.446
priority somewhat

00:22:18.446 --> 00:22:19.456
interchangeably in the rest of

00:22:19.496 --> 00:22:19.966
the session.

00:22:21.266 --> 00:22:22.536
We have four quality of service

00:22:22.536 --> 00:22:23.506
classes on the system.

00:22:23.746 --> 00:22:25.056
From the highest user

00:22:25.056 --> 00:22:27.136
interactive UI to user

00:22:27.136 --> 00:22:30.216
initiated, or IN, utility, UT to

00:22:30.366 --> 00:22:31.516
background BG.

00:22:32.056 --> 00:22:32.866
The lowest priority.

00:22:33.986 --> 00:22:35.366
So, let's look at how we would

00:22:35.366 --> 00:22:36.876
combine this concept of quality

00:22:36.876 --> 00:22:38.346
of service with the target queue

00:22:38.346 --> 00:22:39.326
hierarchy that we just looked

00:22:39.326 --> 00:22:39.516
at.

00:22:40.286 --> 00:22:41.856
In this hierarchy, every node in

00:22:41.856 --> 00:22:42.746
the tree can actually have a

00:22:42.746 --> 00:22:43.826
quality of service label

00:22:43.826 --> 00:22:44.646
associated with it.

00:22:45.436 --> 00:22:47.086
So, for instance the source 2

00:22:47.086 --> 00:22:48.146
might be relevant to the user

00:22:48.146 --> 00:22:48.716
interface.

00:22:49.096 --> 00:22:50.156
It might be monitored for an

00:22:50.156 --> 00:22:51.376
event where we should update the

00:22:51.376 --> 00:22:52.556
UI as soon as the event

00:22:52.556 --> 00:22:52.876
triggers.

00:22:52.876 --> 00:22:55.036
So, it could be that we want to

00:22:55.036 --> 00:22:56.546
put the UI label onto the

00:22:56.546 --> 00:22:57.046
source.

00:22:57.916 --> 00:22:59.006
Another common use case would

00:22:59.006 --> 00:23:01.546
be to put a label on the mutual

00:23:01.546 --> 00:23:03.066
exclusion queue to provide a

00:23:03.066 --> 00:23:04.866
flow of execution so that

00:23:04.866 --> 00:23:06.296
nothing in this tree can execute

00:23:06.296 --> 00:23:08.146
below this level, so UT in this

00:23:08.146 --> 00:23:08.606
example.

00:23:09.106 --> 00:23:11.946
And now if anything else in this

00:23:11.946 --> 00:23:13.636
tree fires, for instance source

00:23:13.636 --> 00:23:16.516
1, we will be using this flow

00:23:16.516 --> 00:23:18.016
for the tree if it doesn't have

00:23:18.016 --> 00:23:19.076
its own quality of service

00:23:19.076 --> 00:23:19.626
associated.

00:23:21.426 --> 00:23:23.816
And a source firing is really just

00:23:23.816 --> 00:23:26.116
an async that executes from the

00:23:26.116 --> 00:23:26.516
kernel.

00:23:26.886 --> 00:23:28.576
And the same as before happens,

00:23:28.576 --> 00:23:29.806
we enqueue the source handler

00:23:30.206 --> 00:23:31.366
eventually into the mutual

00:23:31.366 --> 00:23:32.656
exclusion queue for execution.

00:23:34.176 --> 00:23:36.536
For asyncs from user space, your

00:23:36.536 --> 00:23:37.666
quality of service is usually

00:23:37.666 --> 00:23:38.706
determined from the thread that

00:23:38.706 --> 00:23:39.746
called queue.async.

00:23:39.816 --> 00:23:41.646
So we have a user initiated

00:23:41.646 --> 00:23:44.696
thread that submits an item at IN

00:23:44.696 --> 00:23:46.586
into the queue and for execution

00:23:46.586 --> 00:23:47.666
into EQ eventually.

00:23:48.296 --> 00:23:49.626
And now, maybe we have the

00:23:49.626 --> 00:23:50.876
source 2 that fires with this

00:23:50.876 --> 00:23:53.086
very high priority UI relevant

00:23:53.086 --> 00:23:54.836
event that executes its event

00:23:54.836 --> 00:23:56.156
handler, and enqueues its event

00:23:56.156 --> 00:23:57.216
handler into EQ.

00:23:58.216 --> 00:23:59.306
So, now you'll notice that we

00:23:59.306 --> 00:24:01.076
have a priority inversion

00:24:01.076 --> 00:24:01.616
situation.

00:24:01.616 --> 00:24:02.756
We have three items enqueued

00:24:02.806 --> 00:24:04.136
with a very high priority item

00:24:04.136 --> 00:24:06.116
at the end preceded by some low

00:24:06.156 --> 00:24:06.796
priority items.

00:24:07.166 --> 00:24:08.216
And these have to execute in

00:24:08.216 --> 00:24:08.506
order.

00:24:09.716 --> 00:24:10.686
The system resolves this

00:24:10.686 --> 00:24:12.036
inversion for you by bringing up

00:24:12.036 --> 00:24:14.056
a worker thread at the highest

00:24:14.306 --> 00:24:15.696
priority of anything that is

00:24:15.766 --> 00:24:16.466
currently enqueued.

00:24:16.466 --> 00:24:19.116
And it's worth keeping this

00:24:19.116 --> 00:24:20.196
little tree on the right hand

00:24:20.196 --> 00:24:21.306
side here in mind because it

00:24:21.306 --> 00:24:22.436
comes up again later in the

00:24:22.436 --> 00:24:22.776
session.

00:24:24.166 --> 00:24:28.376
And with that let's move on to

00:24:28.376 --> 00:24:29.666
our main topic of the section

00:24:29.666 --> 00:24:31.016
which is how to use what we just

00:24:31.016 --> 00:24:32.216
learned to express good

00:24:32.216 --> 00:24:34.956
granularity of concurrency to

00:24:35.636 --> 00:24:35.726
GCD.

00:24:35.956 --> 00:24:36.946
Let's go back to our news

00:24:36.946 --> 00:24:37.836
application that Daniel

00:24:37.836 --> 00:24:39.526
introduced earlier and focus on

00:24:39.526 --> 00:24:40.806
the networking subsystem for a

00:24:40.806 --> 00:24:43.626
little bit.

00:24:43.626 --> 00:24:45.116
In a networking subsystem,

00:24:45.116 --> 00:24:46.056
you'll have to monitor some

00:24:46.056 --> 00:24:47.056
network connections in the

00:24:47.056 --> 00:24:47.456
kernel.

00:24:47.936 --> 00:24:49.246
And with GCD you'll do that with

00:24:49.246 --> 00:24:50.256
a dispatch source, and the

00:24:50.296 --> 00:24:51.326
dispatch queue like you just

00:24:51.326 --> 00:24:51.596
saw.

00:24:51.596 --> 00:24:53.426
But of course in any networking

00:24:53.426 --> 00:24:54.706
subsystem you usually not just

00:24:54.706 --> 00:24:55.746
have one network connection,

00:24:55.746 --> 00:24:58.036
you'll have many of them and

00:24:58.036 --> 00:24:59.276
they will all replicate the same

00:24:59.276 --> 00:24:59.876
setup.

00:25:00.716 --> 00:25:01.766
So, let's focus on the right

00:25:01.766 --> 00:25:03.036
hand side on the three

00:25:03.036 --> 00:25:04.806
connections here and see how they

00:25:04.806 --> 00:25:05.306
execute.

00:25:06.936 --> 00:25:07.896
If the first connection

00:25:07.896 --> 00:25:09.736
triggers, just like the same

00:25:09.736 --> 00:25:11.146
thing we just saw happens, we

00:25:11.146 --> 00:25:12.496
will enqueue the event handler

00:25:12.496 --> 00:25:14.206
for that source onto its target

00:25:14.206 --> 00:25:14.366
queue.

00:25:14.456 --> 00:25:16.206
Of course if the other two

00:25:16.386 --> 00:25:17.406
connections fire at the same

00:25:17.406 --> 00:25:18.546
time, they'll still replicate

00:25:18.546 --> 00:25:19.386
and you'll end up with three

00:25:19.386 --> 00:25:21.056
queues with an event handler

00:25:21.056 --> 00:25:21.296
enqueued.

00:25:22.026 --> 00:25:23.516
And because you have these three

00:25:23.646 --> 00:25:24.846
independent serial queues at the

00:25:24.846 --> 00:25:26.166
bottom, you've really asked the

00:25:26.166 --> 00:25:27.606
system to provide you with three

00:25:27.606 --> 00:25:28.646
independent concurrency

00:25:28.646 --> 00:25:29.236
contexts.

00:25:29.676 --> 00:25:30.806
If all these become active at

00:25:30.806 --> 00:25:32.846
once, the system will oblige and

00:25:32.846 --> 00:25:34.356
create three threads for you to

00:25:34.356 --> 00:25:35.516
execute these event handlers.

00:25:36.616 --> 00:25:37.616
Now, this may be what you

00:25:37.616 --> 00:25:39.016
wanted, and maybe exactly what

00:25:39.016 --> 00:25:40.856
you were after, but it is quite

00:25:40.856 --> 00:25:42.786
common for these event handlers

00:25:42.786 --> 00:25:45.236
to be small and only read some

00:25:45.276 --> 00:25:46.576
data from the network and

00:25:46.576 --> 00:25:47.486
enqueue it into a common data

00:25:47.486 --> 00:25:47.876
structure.

00:25:48.696 --> 00:25:50.186
Additionally, as we saw before,

00:25:50.186 --> 00:25:51.106
you don't have just three

00:25:51.106 --> 00:25:52.326
connections, you may have many,

00:25:52.326 --> 00:25:53.836
many of them if you have a

00:25:53.836 --> 00:25:55.576
number of network connections in

00:25:55.676 --> 00:25:56.336
your subsystem.

00:25:57.096 --> 00:25:58.246
So, this can lead to a

00:25:58.246 --> 00:25:59.436
situation where you have this

00:25:59.506 --> 00:26:00.726
kind of context switching

00:26:00.726 --> 00:26:02.086
pattern, and excessive context

00:26:02.086 --> 00:26:03.006
switching that Daniel talked

00:26:03.006 --> 00:26:04.636
about where you execute a small

00:26:04.636 --> 00:26:06.196
amount of work, context switch

00:26:06.196 --> 00:26:07.176
to another thread and do that

00:26:07.176 --> 00:26:08.526
again, and again, and again.

00:26:09.146 --> 00:26:10.816
So, how can we improve on this

00:26:10.816 --> 00:26:12.466
situation in this example here?

00:26:13.286 --> 00:26:15.066
We can apply the single mutual

00:26:15.066 --> 00:26:16.326
exclusion context idea that we

00:26:16.326 --> 00:26:18.076
just talked about by simply

00:26:18.076 --> 00:26:19.426
putting in an additional serial

00:26:19.426 --> 00:26:21.056
queue at the bottom and forming

00:26:21.056 --> 00:26:23.436
a hierarchy, you can get a

00:26:23.436 --> 00:26:24.776
single mutual exclusion context

00:26:24.776 --> 00:26:26.006
for all of these network connections.

00:26:26.756 --> 00:26:27.906
And if they fire at the same

00:26:27.906 --> 00:26:29.386
time, the same thing as before

00:26:29.386 --> 00:26:30.436
will happen, the event handlers

00:26:30.436 --> 00:26:31.446
will get enqueued onto the

00:26:31.446 --> 00:26:33.046
target queues, but because

00:26:33.076 --> 00:26:33.986
there's an additional serial

00:26:33.986 --> 00:26:35.526
queue at the bottom here, it's a

00:26:35.526 --> 00:26:36.626
single thread that will come and

00:26:36.626 --> 00:26:38.306
execute them in order instead of

00:26:38.376 --> 00:26:39.516
the multiple threads that we had

00:26:39.546 --> 00:26:39.966
before.

00:26:41.126 --> 00:26:42.276
So, this seems like a really

00:26:42.276 --> 00:26:44.236
simple change but it is exactly

00:26:44.236 --> 00:26:45.816
the type of change that led to

00:26:45.816 --> 00:26:46.986
the 1.3 X performance

00:26:46.986 --> 00:26:48.356
improvement in some of our own

00:26:48.436 --> 00:26:50.706
code that Daniel talked about

00:26:50.706 --> 00:26:52.246
earlier in the session.

00:26:53.256 --> 00:26:57.436
So, this is one example of how

00:26:57.436 --> 00:26:58.956
we can avoid the problematic

00:26:58.956 --> 00:27:00.186
pattern of repeatedly switching

00:27:00.186 --> 00:27:01.506
between independent operations.

00:27:02.426 --> 00:27:03.616
But it really comes under the

00:27:03.616 --> 00:27:04.836
general heading of avoiding

00:27:05.146 --> 00:27:06.696
unwanted and unbounded

00:27:06.696 --> 00:27:08.036
concurrency in your application.

00:27:09.346 --> 00:27:10.746
One way you can get that is by

00:27:10.746 --> 00:27:11.916
having many queues becoming

00:27:11.916 --> 00:27:12.806
active all at once.

00:27:13.366 --> 00:27:15.196
And one example of this is that

00:27:15.196 --> 00:27:16.366
independent per client source

00:27:16.396 --> 00:27:17.906
pattern that we just saw.

00:27:18.236 --> 00:27:19.196
You can also get this if you

00:27:19.196 --> 00:27:20.356
have independent per object

00:27:20.476 --> 00:27:21.036
queues.

00:27:21.556 --> 00:27:22.806
If many objects in your

00:27:22.806 --> 00:27:23.596
application have their own

00:27:23.596 --> 00:27:24.866
serial queues and you submit

00:27:24.866 --> 00:27:26.076
asynchronous work into them at

00:27:26.076 --> 00:27:27.276
the same time you can get

00:27:27.276 --> 00:27:29.196
exactly the same phenomenon.

00:27:31.046 --> 00:27:32.086
You can also see this if you

00:27:32.086 --> 00:27:33.476
have many work items submitted

00:27:33.476 --> 00:27:34.726
to the global concurrent queue

00:27:34.726 --> 00:27:35.386
at the same time.

00:27:36.406 --> 00:27:37.566
In particular if those work

00:27:37.566 --> 00:27:38.176
items block.

00:27:38.726 --> 00:27:40.276
The way the global concurrent

00:27:40.276 --> 00:27:41.496
queue works is that it creates

00:27:41.496 --> 00:27:42.966
more threads when existing

00:27:42.966 --> 00:27:44.316
threads block to give you a

00:27:44.316 --> 00:27:45.236
continuing good level of

00:27:45.236 --> 00:27:46.566
concurrency in your application.

00:27:47.026 --> 00:27:48.276
But if those threads then block

00:27:48.276 --> 00:27:50.096
again, you can get something

00:27:50.346 --> 00:27:51.276
that we call the thread

00:27:51.276 --> 00:27:51.836
explosion.

00:27:52.876 --> 00:27:53.986
This is a topic that we went

00:27:53.986 --> 00:27:55.196
into some detail in the

00:27:55.196 --> 00:27:56.256
"Building Responses and

00:27:56.256 --> 00:27:57.186
Efficient Apps with GCD" in

00:27:57.186 --> 00:27:59.146
2015.

00:27:59.636 --> 00:28:01.516
So, if this sounds new to you,

00:28:01.566 --> 00:28:02.506
I'd encourage you to go and

00:28:02.506 --> 00:28:03.216
watch that session.

00:28:04.826 --> 00:28:06.096
So, how do you choose the right

00:28:06.096 --> 00:28:07.176
amount of concurrency in your

00:28:07.176 --> 00:28:08.316
application to avoid these

00:28:08.316 --> 00:28:09.116
problematic patterns?

00:28:10.286 --> 00:28:11.606
One idea that we've recommended

00:28:11.606 --> 00:28:13.556
to you in the past is to use one

00:28:13.556 --> 00:28:14.536
queue per subsystem.

00:28:15.816 --> 00:28:17.116
So, here back in our news

00:28:17.116 --> 00:28:18.716
application, we already have one

00:28:18.716 --> 00:28:19.816
queue for the user interface,

00:28:19.816 --> 00:28:21.086
the main queue and we could

00:28:21.086 --> 00:28:22.356
choose one serial queue for the

00:28:22.356 --> 00:28:24.076
networking and one serial queue for the

00:28:24.076 --> 00:28:25.776
database subsystem in addition.

00:28:27.016 --> 00:28:28.666
But what we've learned today a

00:28:28.666 --> 00:28:29.676
more general way to think of

00:28:29.736 --> 00:28:31.036
this is really to use one queue

00:28:31.036 --> 00:28:32.336
hierarchy per subsystem.

00:28:33.656 --> 00:28:37.066
This gives you a mutual

00:28:37.066 --> 00:28:37.996
exclusion context for the

00:28:37.996 --> 00:28:39.806
subsystem, where you can leave the

00:28:39.806 --> 00:28:41.516
rest of the queue event

00:28:41.516 --> 00:28:42.936
Substructure of your system alone

00:28:43.306 --> 00:28:45.106
and just target that network

00:28:45.246 --> 00:28:47.406
queue or database queue that

00:28:47.406 --> 00:28:49.486
underlies the bottom of your

00:28:49.486 --> 00:28:50.406
queue hierarchies.

00:28:53.296 --> 00:28:56.756
But, that may well be a bit too

00:28:56.756 --> 00:28:57.756
simplistic a pattern for a

00:28:57.756 --> 00:28:59.226
complex application or a complex

00:28:59.226 --> 00:28:59.766
subsystem.

00:29:00.176 --> 00:29:01.306
The main thing that is important

00:29:01.306 --> 00:29:02.476
here is to have a fixed number

00:29:02.476 --> 00:29:03.526
of serial queue hierarchies in

00:29:03.526 --> 00:29:04.146
your application.

00:29:04.676 --> 00:29:05.706
So, it may make sense to have

00:29:05.706 --> 00:29:07.236
additional queue hierarchies for

00:29:07.236 --> 00:29:08.646
a complicated subsystem, say a

00:29:08.646 --> 00:29:10.286
secondary one for slower work,

00:29:10.286 --> 00:29:11.516
or larger work items, so that

00:29:11.556 --> 00:29:13.286
the first one, the primary one,

00:29:13.286 --> 00:29:14.276
can keep the subsystem

00:29:14.276 --> 00:29:15.796
responsive to requests coming in

00:29:15.796 --> 00:29:16.366
from outside.

00:29:17.396 --> 00:29:19.596
Another thing that's important

00:29:19.596 --> 00:29:20.976
to think about in this context

00:29:21.426 --> 00:29:23.166
is the granularity of the work

00:29:23.306 --> 00:29:24.656
submitted to those subsystems.

00:29:25.856 --> 00:29:26.936
You want to use fairly large

00:29:26.936 --> 00:29:28.276
work items when you move between

00:29:28.276 --> 00:29:30.146
subsystems to get a picture like

00:29:30.146 --> 00:29:31.776
what we saw earlier in the

00:29:31.776 --> 00:29:34.176
session, where the CPU is able to

00:29:34.176 --> 00:29:35.536
execute your subsystem for long

00:29:35.536 --> 00:29:37.726
enough to reach an efficient

00:29:37.726 --> 00:29:38.396
performance state.

00:29:39.986 --> 00:29:40.786
Once you're inside the

00:29:40.786 --> 00:29:41.956
subsystem, say the networking

00:29:41.956 --> 00:29:42.756
subsystem here.

00:29:43.246 --> 00:29:44.736
It may make sense to subdivide

00:29:44.986 --> 00:29:47.676
into smaller work items and

00:29:47.676 --> 00:29:49.166
have a finer granularity to

00:29:49.166 --> 00:29:50.326
improve the responsiveness of

00:29:50.326 --> 00:29:51.156
that subsystem.

00:29:51.606 --> 00:29:52.726
For instance, you can do that by

00:29:52.726 --> 00:29:53.946
splitting up your work and

00:29:53.946 --> 00:29:55.956
re-asyncing to another queue in

00:29:55.956 --> 00:29:56.746
your queue hierarchy.

00:29:56.746 --> 00:29:57.646
And that doesn't introduce a

00:29:57.646 --> 00:29:58.686
context switch because you're

00:29:58.686 --> 00:30:02.926
already in that one subsystem.

00:30:03.426 --> 00:30:04.706
So, in summary, what have we

00:30:04.706 --> 00:30:05.676
looked at in this section?

00:30:06.956 --> 00:30:08.326
We saw how we can organize

00:30:08.326 --> 00:30:09.956
queues and sources into serial

00:30:09.956 --> 00:30:10.726
queue hierarchies.

00:30:11.246 --> 00:30:12.946
How to use a fixed number of the serial

00:30:12.946 --> 00:30:14.526
queue hierarchies to give GCD a

00:30:14.526 --> 00:30:16.096
good granularity of concurrency.

00:30:16.676 --> 00:30:18.796
And how to size your work items

00:30:18.796 --> 00:30:20.406
appropriately earlier in the

00:30:20.406 --> 00:30:22.176
section for parallel work and

00:30:22.176 --> 00:30:24.806
here for concurrent work inside

00:30:24.806 --> 00:30:26.346
your subsystem as well as between

00:30:26.346 --> 00:30:26.916
subsystems.

00:30:27.556 --> 00:30:29.186
And with this, I'll hand it over

00:30:29.186 --> 00:30:30.866
to Pierre to dive into how we

00:30:30.866 --> 00:30:32.746
have improved GCD to always

00:30:32.746 --> 00:30:33.916
execute the queue hierarchy on a

00:30:33.916 --> 00:30:35.216
single thread and how you can

00:30:35.216 --> 00:30:36.926
modernize your code to take

00:30:36.926 --> 00:30:37.686
advantage of this.

00:30:38.516 --> 00:30:43.876
[ Applause ]

00:30:44.376 --> 00:30:45.006
>> Thank you Daniel.

00:30:46.506 --> 00:30:49.036
So, indeed we have completely

00:30:49.036 --> 00:30:51.076
reinvented the internals of GCD

00:30:51.076 --> 00:30:52.826
this year to eliminate some

00:30:52.826 --> 00:30:54.936
unwanted context switches and

00:30:54.936 --> 00:30:56.836
execute single queue hierarchies

00:30:56.836 --> 00:30:58.266
like the ones that Daniel showed

00:30:58.686 --> 00:30:59.456
on the single thread.

00:31:00.476 --> 00:31:02.696
To do so we have created a new

00:31:03.156 --> 00:31:04.476
kind of concept that we call

00:31:04.786 --> 00:31:06.626
Unified Queue Identity that lets

00:31:06.626 --> 00:31:07.296
us do that.

00:31:07.296 --> 00:31:09.126
And we will walk you through how

00:31:09.896 --> 00:31:10.716
it works.

00:31:11.486 --> 00:31:16.046
So, really this part of the talk

00:31:16.046 --> 00:31:17.626
will focus on a single queue

00:31:17.626 --> 00:31:19.386
hierarchy, like the ones Daniel

00:31:19.386 --> 00:31:19.956
showed earlier.

00:31:20.516 --> 00:31:21.696
However, we'll work on

00:31:21.696 --> 00:31:23.956
simplified ones with the sources

00:31:23.956 --> 00:31:25.946
at the top, and your mutual

00:31:25.946 --> 00:31:27.896
exclusion context at the bottom.

00:31:27.896 --> 00:31:28.976
The internal GCD notes are not

00:31:28.976 --> 00:31:31.966
quite relevant for that part of the

00:31:33.236 --> 00:31:33.346
talk.

00:31:33.676 --> 00:31:35.436
So, when you create a

00:31:35.436 --> 00:31:36.196
new execution context you use to

00:31:36.196 --> 00:31:38.876
dispatch queue constructor, that

00:31:38.906 --> 00:31:40.266
creates just a piece of memory

00:31:40.266 --> 00:31:41.426
in your application that is a

00:31:41.426 --> 00:31:41.706
note.

00:31:42.306 --> 00:31:43.476
And one of the first things that

00:31:43.476 --> 00:31:45.786
you may do is to dispatch asynchronous

00:31:45.786 --> 00:31:47.636
Work items to it.

00:31:48.146 --> 00:31:49.496
So, you will have a thread in your

00:31:49.496 --> 00:31:51.946
application that will here enqueue

00:31:51.946 --> 00:31:53.946
a background work item on the

00:31:53.946 --> 00:31:56.976
queue. When that happened before

00:31:56.976 --> 00:31:58.836
we used to request a thread

00:31:58.836 --> 00:32:01.486
anonymously to the system.

00:32:01.666 --> 00:32:03.846
And the resolution of what the thread

00:32:03.846 --> 00:32:05.686
was meant to do happens late

00:32:05.776 --> 00:32:06.686
inside your application.

00:32:08.246 --> 00:32:09.996
In this release, we change that and

00:32:09.996 --> 00:32:11.836
what we do is that we create our

00:32:11.876 --> 00:32:13.676
counter object, the Unified

00:32:13.676 --> 00:32:15.536
Queue Identity that is  strongly tied to

00:32:15.536 --> 00:32:17.716
your queue and is exactly meant

00:32:17.716 --> 00:32:18.806
to represent your queue in the

00:32:18.806 --> 00:32:19.156
kernel.

00:32:20.076 --> 00:32:21.536
We can tie that object with the

00:32:21.536 --> 00:32:23.026
required priority to execute to

00:32:23.026 --> 00:32:24.676
your work, which here is background (BG).

00:32:25.276 --> 00:32:27.276
And that causes the system to

00:32:27.276 --> 00:32:28.036
ask for a thread.

00:32:28.856 --> 00:32:31.066
The thread request, that dotted

00:32:31.066 --> 00:32:33.656
line on the slide, may not be

00:32:33.656 --> 00:32:35.886
fulfilled for some time, because

00:32:35.886 --> 00:32:36.996
here that's a background thread,

00:32:37.236 --> 00:32:38.506
and maybe the system is loaded

00:32:38.506 --> 00:32:40.046
enough that it's not even worth

00:32:40.046 --> 00:32:44.606
giving you a thread for it.

00:32:44.606 --> 00:32:45.926
Later on, some other path of

00:32:46.086 --> 00:32:49.176
your application may actually

00:32:49.176 --> 00:32:50.726
try to enqueue more work.

00:32:50.726 --> 00:32:51.216
Here a UT utility work item that is

00:32:51.216 --> 00:32:53.526
slightly higher priority.

00:32:53.996 --> 00:32:55.956
We can use the queue identity,

00:32:56.086 --> 00:32:57.496
the unified identity in the

00:32:57.496 --> 00:32:58.586
kernel to look and solve the

00:32:58.586 --> 00:32:59.866
priority inversion, and elevate

00:32:59.866 --> 00:33:00.916
the priority of that thread

00:33:00.916 --> 00:33:01.426
request.

00:33:01.906 --> 00:33:03.196
And maybe that is the small

00:33:03.196 --> 00:33:04.846
nudge that the system needed to

00:33:04.846 --> 00:33:06.206
actually give you a thread here

00:33:06.206 --> 00:33:07.736
to execute your work.

00:33:08.016 --> 00:33:09.116
But this thread is in the

00:33:09.116 --> 00:33:10.406
scheduler queues not yet on

00:33:10.406 --> 00:33:10.626
call,

00:33:10.626 --> 00:33:11.356
not executing.

00:33:11.946 --> 00:33:13.326
And the reason why is because

00:33:13.326 --> 00:33:14.456
there is another thread in your

00:33:14.456 --> 00:33:15.676
application that is interacting

00:33:15.676 --> 00:33:16.746
with that queue and working

00:33:16.746 --> 00:33:18.046
synchronously at a higher

00:33:18.046 --> 00:33:19.756
priority, even, user initiated (IN).

00:33:20.286 --> 00:33:23.056
Now that we have that Unified

00:33:23.056 --> 00:33:24.486
Queue Identity, we can actually

00:33:24.736 --> 00:33:26.346
since that thread has to block

00:33:26.566 --> 00:33:27.826
to enqueue the placeholder that

00:33:27.826 --> 00:33:29.716
Daniel told you about a bit

00:33:29.716 --> 00:33:31.956
earlier, we can block the

00:33:31.956 --> 00:33:33.726
synchronous execution of that

00:33:33.836 --> 00:33:35.196
thread on the Unified Queue

00:33:35.196 --> 00:33:35.676
Identity.

00:33:35.676 --> 00:33:36.816
The same on that we use for

00:33:36.816 --> 00:33:38.226
asynchronous work, with all the priority inversion.

00:33:38.226 --> 00:33:41.046
But now that we unified the

00:33:41.046 --> 00:33:43.056
asynchronous and the synchronous

00:33:43.266 --> 00:33:44.786
part of the queue in a single

00:33:44.786 --> 00:33:46.386
identity, we can apply an

00:33:46.866 --> 00:33:48.566
optimization and delicately

00:33:48.566 --> 00:33:49.686
switch the thread that's

00:33:49.686 --> 00:33:51.266
blocking you by passing the

00:33:51.266 --> 00:33:52.536
scheduler queue and registering

00:33:52.536 --> 00:33:54.246
the queue delays that Daniel

00:33:54.406 --> 00:33:55.936
introduced while talking about

00:33:55.936 --> 00:33:56.896
the scheduler very early.

00:33:58.016 --> 00:33:59.826
So, that's how the unified queue

00:33:59.826 --> 00:34:00.986
identity is used for

00:34:00.986 --> 00:34:02.226
asynchronous and synchronous

00:34:02.226 --> 00:34:02.846
work items.

00:34:05.396 --> 00:34:07.256
Now, how would we use that for

00:34:07.256 --> 00:34:07.816
events?

00:34:08.016 --> 00:34:08.906
Why is it useful?

00:34:09.846 --> 00:34:11.606
So, that is the small tree that

00:34:11.606 --> 00:34:13.766
we've been using so far, let's

00:34:13.766 --> 00:34:15.505
look at the creation of these

00:34:15.505 --> 00:34:16.065
sources.

00:34:17.356 --> 00:34:18.516
When you create the source using

00:34:18.516 --> 00:34:20.716
the makeResource factory button,

00:34:20.716 --> 00:34:22.476
you set a bunch of events, of

00:34:22.476 --> 00:34:23.396
favorite handlers and

00:34:23.396 --> 00:34:24.036
properties.

00:34:24.516 --> 00:34:25.536
But what is really interesting

00:34:25.536 --> 00:34:26.576
is what happens when you

00:34:26.576 --> 00:34:28.386
activate the object.

00:34:29.335 --> 00:34:31.576
This is actually at that moment,

00:34:31.966 --> 00:34:33.755
that we will notice that the utility

00:34:33.886 --> 00:34:35.545
is QOS, at which the handler

00:34:35.545 --> 00:34:36.716
for your source will always

00:34:36.716 --> 00:34:37.266
execute.

00:34:37.496 --> 00:34:38.806
Because it's inherited from your

00:34:38.806 --> 00:34:39.406
queue hierarchy.

00:34:39.716 --> 00:34:42.096
We will also know now, with the

00:34:42.096 --> 00:34:44.795
new system, that the handler

00:34:44.795 --> 00:34:46.456
will eventually execute in

00:34:46.576 --> 00:34:49.396
that EQ execution mutual exclusion

00:34:49.396 --> 00:34:49.936
context.

00:34:50.815 --> 00:34:52.996
And will now register the source

00:34:52.996 --> 00:34:55.156
at front with the sync unified

00:34:55.156 --> 00:34:57.136
identity that I just talked

00:34:57.136 --> 00:34:59.296
about a bit earlier.

00:34:59.606 --> 00:35:01.796
If we look at the higher UI QOS

00:35:02.016 --> 00:35:04.506
source that we have on the tree,

00:35:05.146 --> 00:35:06.696
the way we treat it is very

00:35:06.696 --> 00:35:09.276
similar to the first one, except

00:35:09.276 --> 00:35:10.526
that when you're setting the event

00:35:10.586 --> 00:35:12.746
handler here you're specifying

00:35:12.746 --> 00:35:14.406
the QOS that you actually want.

00:35:15.366 --> 00:35:16.576
And again, what's interesting is

00:35:16.576 --> 00:35:18.126
what happens at activate time.

00:35:18.126 --> 00:35:19.886
That is when we take the snapshot and

00:35:19.886 --> 00:35:21.936
unlike before when we got the

00:35:21.936 --> 00:35:24.146
utility QOS from your hierarchy,

00:35:24.436 --> 00:35:25.876
here we get it from your hint.

00:35:26.796 --> 00:35:27.946
We still recall the fact that

00:35:27.986 --> 00:35:29.606
they will execute both the

00:35:29.606 --> 00:35:31.296
sources on the same execution

00:35:31.296 --> 00:35:31.836
context.

00:35:31.836 --> 00:35:33.376
And will register that second

00:35:33.376 --> 00:35:34.486
source up front again, with

00:35:34.706 --> 00:35:36.506
the same unified identity in

00:35:37.096 --> 00:35:39.366
the kernel.

00:35:39.506 --> 00:35:41.746
So, really what we're trying to

00:35:41.746 --> 00:35:43.866
solve with that quite complex

00:35:43.956 --> 00:35:46.446
identity is a problem that we

00:35:46.446 --> 00:35:48.136
had in previous releases of the

00:35:48.206 --> 00:35:51.316
OS, where related operations

00:35:51.316 --> 00:35:52.846
would actually bounce off the

00:35:52.846 --> 00:35:53.466
old threads.

00:35:53.606 --> 00:35:54.766
Let's look at how it used to

00:35:54.766 --> 00:35:55.116
work.

00:35:56.086 --> 00:35:58.436
So, remember that's our queue

00:35:58.436 --> 00:36:01.326
hierarchy, and let's bring up

00:36:01.406 --> 00:36:02.536
the timeline that you've seen a

00:36:02.536 --> 00:36:05.146
bunch of times now in our talk.

00:36:05.276 --> 00:36:07.346
At the top, the CPU, but now

00:36:07.346 --> 00:36:08.896
there is a new track, the

00:36:08.896 --> 00:36:10.756
exclusion queue track that will

00:36:10.756 --> 00:36:12.586
show you what is executing at

00:36:12.586 --> 00:36:15.696
any given moment on that queue.

00:36:15.926 --> 00:36:17.506
So, that's really how the

00:36:17.556 --> 00:36:19.366
runtime used to work before this

00:36:19.366 --> 00:36:22.036
release in macOS Sierra and iOS

00:36:22.036 --> 00:36:22.306
10.

00:36:22.616 --> 00:36:24.616
So, let's look at what happens

00:36:24.616 --> 00:36:25.966
if the first source fails.

00:36:26.626 --> 00:36:28.286
Before, like I said thread

00:36:28.286 --> 00:36:29.396
requests were anonymous.

00:36:29.396 --> 00:36:30.646
We would ask for an anonymous

00:36:30.646 --> 00:36:31.846
thread, deliver the event on the

00:36:31.846 --> 00:36:33.656
thread and then we would look at

00:36:33.656 --> 00:36:33.976
the event.

00:36:34.856 --> 00:36:36.706
And when we look at the event

00:36:36.706 --> 00:36:38.436
inside your application, that is

00:36:38.436 --> 00:36:39.996
only then that we realize that

00:36:39.996 --> 00:36:41.966
this event is meant to run on a

00:36:41.966 --> 00:36:42.206
queue.

00:36:42.206 --> 00:36:43.876
We would then queue the event

00:36:43.876 --> 00:36:44.276
handler.

00:36:44.416 --> 00:36:46.926
But since the queue is

00:36:46.926 --> 00:36:48.156
unclaimed, the thread could

00:36:48.156 --> 00:36:50.856
actually become that queue and

00:36:50.856 --> 00:36:52.356
start executing the event handler

00:36:52.766 --> 00:36:53.596
for your source.

00:36:54.306 --> 00:36:54.886
And we do so.

00:36:55.486 --> 00:36:56.926
Now, the interesting thing is

00:36:56.926 --> 00:36:58.236
what happens when the second

00:36:58.236 --> 00:36:59.346
source that is higher priority

00:36:59.346 --> 00:36:59.896
fires?

00:37:00.716 --> 00:37:01.626
The same actually.

00:37:02.116 --> 00:37:03.726
Since it's a hierarchy QOS here,

00:37:03.906 --> 00:37:05.146
higher priority that's what

00:37:05.146 --> 00:37:06.216
you're executing right now.

00:37:07.426 --> 00:37:08.566
We would bring up a new

00:37:08.626 --> 00:37:10.076
anonymous thread deliver that

00:37:10.136 --> 00:37:11.256
higher priority event on the

00:37:11.306 --> 00:37:11.566
thread.

00:37:13.196 --> 00:37:14.416
And look at what that event

00:37:14.416 --> 00:37:14.886
means.

00:37:15.036 --> 00:37:16.136
And we will notice that it is

00:37:16.136 --> 00:37:17.096
for exactly the same queue

00:37:17.096 --> 00:37:17.966
hierarchy only then.

00:37:18.036 --> 00:37:19.676
And enqueue the handler after

00:37:19.676 --> 00:37:20.756
the one we just pre emptied.

00:37:21.596 --> 00:37:23.186
As you see, we closed our first

00:37:23.606 --> 00:37:24.176
context switch because

00:37:24.716 --> 00:37:26.176
of that higher priority

00:37:26.176 --> 00:37:26.556
event.

00:37:27.136 --> 00:37:28.616
But, we cannot make for what

00:37:28.616 --> 00:37:29.636
progress, because unlike the

00:37:29.636 --> 00:37:30.816
first time, that second thread

00:37:30.816 --> 00:37:32.166
cannot take over the queue it is

00:37:32.166 --> 00:37:33.156
already associated with a

00:37:33.156 --> 00:37:33.676
thread.

00:37:33.946 --> 00:37:34.906
We cannot take it over.

00:37:35.476 --> 00:37:36.286
So, the thread is done.

00:37:36.526 --> 00:37:38.126
Which as Daniel explained is one

00:37:38.126 --> 00:37:39.826
reason why you will context switch

00:37:39.826 --> 00:37:40.096
again.

00:37:40.766 --> 00:37:41.676
And that's what we do, we

00:37:41.676 --> 00:37:42.966
context switch back to the first

00:37:42.966 --> 00:37:43.946
thread that is the one that can

00:37:43.946 --> 00:37:44.886
actually make progress.

00:37:44.886 --> 00:37:46.836
We execute the rest of the first

00:37:46.836 --> 00:37:48.276
handler and finally move to the

00:37:48.276 --> 00:37:48.696
second one.

00:37:50.066 --> 00:37:52.446
So, as you can see, we use two

00:37:52.536 --> 00:37:54.216
threads and two context switches

00:37:54.286 --> 00:37:55.646
that you really didn't want for

00:37:55.646 --> 00:37:56.956
a single execution context.

00:37:57.476 --> 00:38:00.996
We fixed that using Unified

00:38:01.036 --> 00:38:02.996
Identity in macOS High Sierra

00:38:03.066 --> 00:38:04.686
and iOS 11.

00:38:05.826 --> 00:38:09.266
We got rid of that thread.

00:38:10.156 --> 00:38:12.566
And we also, of course got rid

00:38:12.566 --> 00:38:14.846
of the two context switches that

00:38:14.846 --> 00:38:16.416
we had, that were unwanted.

00:38:17.396 --> 00:38:18.716
And of course, its important

00:38:18.716 --> 00:38:20.726
because unlike what happened

00:38:20.776 --> 00:38:22.106
when Daniel showed you the

00:38:22.106 --> 00:38:24.916
pre-emption with that UI touch

00:38:24.916 --> 00:38:26.996
event, where we could take

00:38:26.996 --> 00:38:28.646
advantage of the fact that we

00:38:28.756 --> 00:38:30.596
actually had two threads that

00:38:30.596 --> 00:38:32.186
were independent to be more

00:38:32.186 --> 00:38:33.266
responsive for your application.

00:38:33.626 --> 00:38:35.276
Here, we didn't benefit from any

00:38:35.276 --> 00:38:36.276
of these context switches,

00:38:36.566 --> 00:38:38.206
because these event handlers, S1

00:38:38.206 --> 00:38:40.586
and S2 had to execute in order

00:38:40.586 --> 00:38:41.126
anyways.

00:38:41.126 --> 00:38:42.506
So, knowing about that event

00:38:42.506 --> 00:38:43.546
early was not useful.

00:38:43.546 --> 00:38:46.046
And if you look at how this

00:38:46.046 --> 00:38:48.006
actually, what the flow is

00:38:48.006 --> 00:38:49.916
today, it looks more like this.

00:38:50.816 --> 00:38:53.026
What happened here?

00:38:54.936 --> 00:38:56.926
The most important thing on that

00:38:56.926 --> 00:38:58.346
flow is that now if you look at

00:38:58.346 --> 00:39:00.106
the thread, it's called EQ,

00:39:00.386 --> 00:39:01.516
because that's the point of the

00:39:01.516 --> 00:39:03.206
unified identity, the thread and

00:39:03.206 --> 00:39:04.636
the EQ are basically the same

00:39:04.636 --> 00:39:05.096
object.

00:39:05.386 --> 00:39:06.696
And the kernel knows that it's

00:39:06.696 --> 00:39:08.476
really executing a queue, which

00:39:08.476 --> 00:39:09.976
is reflected on the CPU track.

00:39:09.976 --> 00:39:11.186
You don't see the events anymore,

00:39:11.186 --> 00:39:12.286
it's just running your queue.

00:39:14.476 --> 00:39:16.256
However, you might ask, how did

00:39:16.256 --> 00:39:19.206
we manage to deliver the event,

00:39:19.456 --> 00:39:20.896
that second event without

00:39:20.896 --> 00:39:21.726
requiring a helper.

00:39:21.846 --> 00:39:23.566
That is actually a good

00:39:24.696 --> 00:39:24.916
question.

00:39:25.446 --> 00:39:27.266
When the event fires, now we

00:39:27.266 --> 00:39:28.756
know where it will execute,

00:39:28.756 --> 00:39:29.936
where you will handle it.

00:39:30.326 --> 00:39:31.686
We just mark the thread.

00:39:32.486 --> 00:39:33.416
No helper needed.

00:39:34.226 --> 00:39:38.266
And at the first possible time,

00:39:38.986 --> 00:39:40.786
we will notice that the thread was

00:39:40.786 --> 00:39:42.006
marked with 'you have pending

00:39:42.006 --> 00:39:42.486
events'.

00:39:42.966 --> 00:39:44.356
And we will de-queue the events,

00:39:44.356 --> 00:39:46.596
When it's the right time, right

00:39:46.596 --> 00:39:47.556
after the first handler

00:39:47.556 --> 00:39:48.116
finishes.

00:39:48.696 --> 00:39:50.066
We can grab the events from the

00:39:50.066 --> 00:39:51.566
kernel, look at them, and

00:39:51.606 --> 00:39:53.346
enqueue their handlers on your

00:39:54.006 --> 00:39:54.246
hierarchy.

00:39:54.796 --> 00:39:57.726
So, why did we go through that

00:39:57.726 --> 00:39:59.016
quite complex explanation?

00:40:00.046 --> 00:40:01.856
That's so that you can

00:40:01.856 --> 00:40:03.146
understand how to best take

00:40:03.146 --> 00:40:04.676
advantage of the runtime

00:40:04.676 --> 00:40:05.126
behavior.

00:40:06.086 --> 00:40:07.366
Because clearly, the runtime

00:40:07.366 --> 00:40:08.986
uses every possible hint you're

00:40:08.986 --> 00:40:10.996
giving us to optimize behavior

00:40:10.996 --> 00:40:11.756
in your application.

00:40:12.766 --> 00:40:14.716
And it's important to know

00:40:14.716 --> 00:40:17.356
how to hint and when to hint the

00:40:17.356 --> 00:40:18.866
runtime correctly so that we

00:40:18.866 --> 00:40:19.886
make the right decisions.

00:40:20.386 --> 00:40:24.196
Which brings me to what should

00:40:24.196 --> 00:40:26.226
you do to existing code bases to

00:40:26.226 --> 00:40:28.896
take advantage of all that core

00:40:28.896 --> 00:40:32.966
technology that we rebuilt.

00:40:33.156 --> 00:40:35.966
Now, actually two steps to

00:40:35.966 --> 00:40:37.156
follow to take the full

00:40:37.156 --> 00:40:38.706
advantage of that technology.

00:40:38.936 --> 00:40:41.116
The first one is no mutation

00:40:41.116 --> 00:40:41.966
after activation.

00:40:42.666 --> 00:40:44.036
And the second one is paying

00:40:44.276 --> 00:40:46.796
extra care with extra attention

00:40:46.796 --> 00:40:47.566
to your target queue

00:40:47.566 --> 00:40:48.126
hierarchies.

00:40:49.126 --> 00:40:49.856
So, what does that mean?

00:40:51.236 --> 00:40:52.576
No mutation past activation

00:40:52.656 --> 00:40:53.806
really means that when you have

00:40:53.806 --> 00:40:55.396
any kind of property on a

00:40:55.396 --> 00:40:57.216
dispatch object, you can set

00:40:57.216 --> 00:40:58.446
them, but as soon as you

00:40:58.446 --> 00:40:59.816
activate the object, you should

00:40:59.816 --> 00:41:01.056
stop mutating them.

00:41:01.636 --> 00:41:04.786
The second example, that's our

00:41:04.786 --> 00:41:05.976
source that we've seen quite a

00:41:05.976 --> 00:41:07.466
few times already in the talk.

00:41:07.956 --> 00:41:08.636
That monitors a file descriptor

00:41:08.636 --> 00:41:09.836
for readability.

00:41:09.836 --> 00:41:12.686
And you're setting a bunch of

00:41:12.686 --> 00:41:14.606
properties, handlers; the event

00:41:14.606 --> 00:41:15.806
handler, the cancel handler.

00:41:15.806 --> 00:41:16.826
You may have registration

00:41:16.826 --> 00:41:17.406
handlers.

00:41:18.196 --> 00:41:19.536
You can even change them a few

00:41:19.536 --> 00:41:20.826
times, that's fine, you can

00:41:20.826 --> 00:41:21.496
change your mind.

00:41:21.946 --> 00:41:23.956
And then you activate the

00:41:23.956 --> 00:41:24.346
source.

00:41:25.706 --> 00:41:27.496
The contract here is that you

00:41:27.496 --> 00:41:29.556
should stop mutating your objects.

00:41:30.246 --> 00:41:31.376
It's very tempting to,

00:41:31.706 --> 00:41:33.076
after-the-fact, for example

00:41:33.076 --> 00:41:33.976
change the target queue of your

00:41:33.976 --> 00:41:34.386
source.

00:41:34.386 --> 00:41:35.486
That will cause problems.

00:41:35.786 --> 00:41:37.356
And the reason why is exactly

00:41:37.356 --> 00:41:38.956
what I showed a bit earlier, at

00:41:38.956 --> 00:41:40.876
activate time we take a snapshot

00:41:40.916 --> 00:41:42.006
of the properties of your

00:41:42.006 --> 00:41:43.306
objects, and we will take

00:41:43.306 --> 00:41:44.906
decisions in the future based on

00:41:44.906 --> 00:41:45.556
that snapshot.

00:41:45.686 --> 00:41:48.486
And if you change the target

00:41:48.486 --> 00:41:50.196
queue hierarchy after-the-fact,

00:41:50.476 --> 00:41:51.756
it will render that snapshot

00:41:51.756 --> 00:41:54.256
stale and that will defeat a

00:41:54.256 --> 00:41:55.536
bunch of very important

00:41:55.536 --> 00:41:56.846
optimization such as the

00:41:56.846 --> 00:41:59.726
priority inversion avoidance

00:41:59.726 --> 00:42:01.096
algorithm in GCD, the direct handoff

00:42:01.186 --> 00:42:02.306
that we have for the dispatch

00:42:02.306 --> 00:42:03.686
sync that I presented earlier,

00:42:03.686 --> 00:42:05.756
or all defensive and

00:42:05.756 --> 00:42:07.056
deliberate optimizations that

00:42:07.056 --> 00:42:09.976
we just went through.

00:42:10.896 --> 00:42:12.586
And I insist on the points that

00:42:12.586 --> 00:42:13.966
Daniel made early on, which is

00:42:13.966 --> 00:42:16.476
that many of you probably never

00:42:16.886 --> 00:42:18.476
had to create a dispatch source

00:42:18.476 --> 00:42:19.196
in your application.

00:42:19.196 --> 00:42:20.576
And this is fine, this is really

00:42:20.576 --> 00:42:21.396
how it's supposed to work.

00:42:22.236 --> 00:42:23.846
You probably actually use them a

00:42:23.846 --> 00:42:25.906
lot of them through system

00:42:25.906 --> 00:42:26.476
frameworks.

00:42:26.476 --> 00:42:27.286
It's a shame you have a

00:42:27.286 --> 00:42:28.696
framework that you have to vend

00:42:28.696 --> 00:42:30.666
a dispatch queue to because they

00:42:30.666 --> 00:42:33.166
are asyncing some notifications

00:42:33.166 --> 00:42:34.606
on the queue on your behalf.

00:42:34.706 --> 00:42:36.106
Behind the scenes, they have one

00:42:36.106 --> 00:42:36.886
of these sources.

00:42:37.216 --> 00:42:38.056
So, if you're changing the

00:42:38.056 --> 00:42:39.766
assumptions of the system, you

00:42:39.766 --> 00:42:42.106
will actually break all of these

00:42:42.106 --> 00:42:45.666
optimizations as well.

00:42:46.546 --> 00:42:48.806
So, I hope I made a point really

00:42:48.806 --> 00:42:50.176
clear that to target your

00:42:50.176 --> 00:42:52.396
hierarchy is essential and you

00:42:52.396 --> 00:42:53.266
have to protect it.

00:42:54.656 --> 00:42:55.226
What does that mean?

00:42:56.466 --> 00:42:57.256
And how to do that?

00:42:58.756 --> 00:43:00.156
The first way, which is a very

00:43:00.156 --> 00:43:01.386
simple device, is that when

00:43:01.386 --> 00:43:03.796
you're building one, start from

00:43:03.796 --> 00:43:05.876
the bottom and build it toward

00:43:06.496 --> 00:43:07.306
the top.

00:43:07.496 --> 00:43:09.016
When you show hierarchy on the

00:43:09.016 --> 00:43:11.706
slide build up, as you see,

00:43:12.086 --> 00:43:13.906
these white arrows there, they

00:43:13.906 --> 00:43:14.846
are your target queue

00:43:14.846 --> 00:43:15.516
relationships.

00:43:15.766 --> 00:43:18.146
None of them have to be mutated

00:43:18.146 --> 00:43:19.886
if you build it in that

00:43:20.566 --> 00:43:20.956
order.

00:43:21.036 --> 00:43:22.656
However, when you have a large

00:43:22.656 --> 00:43:24.156
application, or you're hiding

00:43:24.156 --> 00:43:25.486
your frameworks and you're

00:43:25.486 --> 00:43:26.996
vending one of these queues to

00:43:26.996 --> 00:43:30.056
another part of your engineering

00:43:30.166 --> 00:43:32.406
company, you may want to have

00:43:32.406 --> 00:43:33.706
stronger guarantees than that.

00:43:34.256 --> 00:43:35.516
You may want to lockdown these

00:43:35.516 --> 00:43:36.756
relationships, so that really no

00:43:36.756 --> 00:43:37.726
one can mutate them

00:43:37.726 --> 00:43:39.026
after-the-fact.

00:43:39.576 --> 00:43:41.846
This is actually something that

00:43:41.846 --> 00:43:43.306
you can do with the technology

00:43:43.306 --> 00:43:44.746
that we call a

00:43:44.746 --> 00:43:45.256
'static queue hierarchy'.

00:43:45.416 --> 00:43:47.236
We introduced it last year, and

00:43:47.736 --> 00:43:49.456
actually if you are using Swift

00:43:49.456 --> 00:43:50.876
3, then you can stop listening

00:43:50.876 --> 00:43:51.976
to me, because you're already in

00:43:51.976 --> 00:43:53.146
that form and that is the only

00:43:53.146 --> 00:43:54.616
world you're living in.

00:43:55.606 --> 00:43:57.066
However, if you have an existing

00:43:57.066 --> 00:43:58.186
code base, or you use older

00:43:58.186 --> 00:44:01.056
versions than of Swift, you need

00:44:01.056 --> 00:44:03.076
to do some extra steps.

00:44:03.736 --> 00:44:06.756
So, let's focus on the

00:44:06.756 --> 00:44:08.626
relationship between Q1 and EQ

00:44:08.896 --> 00:44:09.046
here.

00:44:09.776 --> 00:44:12.266
When you created that with

00:44:12.266 --> 00:44:13.886
Objective-C you probably wrote

00:44:13.886 --> 00:44:15.356
code that looks like this.

00:44:15.436 --> 00:44:17.346
You create your queue and then

00:44:17.406 --> 00:44:19.216
in the second step, you will set

00:44:19.406 --> 00:44:21.816
your target queue of Q1 to EQ.

00:44:22.156 --> 00:44:23.916
That is not protecting your

00:44:23.916 --> 00:44:24.566
queue hierarchy.

00:44:24.566 --> 00:44:26.416
Anyone can come along and call

00:44:26.416 --> 00:44:27.666
dispatch target queue again and

00:44:27.666 --> 00:44:28.826
break all your assumptions.

00:44:29.056 --> 00:44:30.126
That's not totally great.

00:44:30.906 --> 00:44:32.986
There is a simple step to just

00:44:32.986 --> 00:44:34.316
fix that code into a way that is

00:44:34.316 --> 00:44:37.206
safe, which is to adopt a new

00:44:37.206 --> 00:44:38.536
API we introduced last year,

00:44:38.736 --> 00:44:39.966
which is dispatch_queue_create_with_target,

00:44:39.966 --> 00:44:42.636
which in a single

00:44:42.636 --> 00:44:44.476
atomic step will create the

00:44:44.476 --> 00:44:46.606
queue, set the queue hierarchy

00:44:46.606 --> 00:44:48.116
right, and protect it.

00:44:48.676 --> 00:44:50.066
And that's it.

00:44:50.536 --> 00:44:51.706
These were the two steps to

00:44:51.706 --> 00:44:53.526
follow for you to really work

00:44:53.526 --> 00:44:54.386
with the runtime well. They are really easy.

00:44:54.386 --> 00:44:58.306
Other, a bit like the mute

00:44:58.306 --> 00:45:00.296
case that Daniel walked you

00:45:00.296 --> 00:45:02.566
through early on, finding when

00:45:02.566 --> 00:45:04.316
you're doing one of these things

00:45:04.316 --> 00:45:05.836
wrong is fairly challenging,

00:45:05.836 --> 00:45:07.226
especially on a large cloud

00:45:07.226 --> 00:45:07.646
base.

00:45:07.646 --> 00:45:09.256
Finding that in an existing

00:45:09.256 --> 00:45:10.826
cloud base through code inspection

00:45:10.886 --> 00:45:11.436
is hard.

00:45:12.356 --> 00:45:14.736
This is why we created a new GCD

00:45:14.736 --> 00:45:16.626
performance instruments tool to

00:45:16.626 --> 00:45:18.346
find problem spots in an

00:45:18.346 --> 00:45:19.406
existing application.

00:45:19.976 --> 00:45:21.096
And I will call Daniel back to

00:45:21.096 --> 00:45:23.806
the stage to demo for you.

00:45:24.516 --> 00:45:28.866
[ Applause ]

00:45:29.366 --> 00:45:29.896
>> Thank you, Pierre.

00:45:30.426 --> 00:45:32.766
All right to start out with

00:45:32.766 --> 00:45:34.166
please note that this GCD

00:45:34.166 --> 00:45:35.216
performance instrument that

00:45:35.216 --> 00:45:37.156
we'll see is not yet present in

00:45:37.156 --> 00:45:38.396
the version of XCode 9 that you

00:45:38.396 --> 00:45:39.746
have, but it will be available

00:45:39.746 --> 00:45:41.996
in an upcoming seed of XCode 9.

00:45:41.996 --> 00:45:44.566
So, for this demo, let's analyze

00:45:44.596 --> 00:45:46.176
the execution of our sample news

00:45:46.176 --> 00:45:47.486
application in some detail.

00:45:48.596 --> 00:45:49.566
So, what happens here if you

00:45:49.566 --> 00:45:51.146
click this connect button at the

00:45:51.146 --> 00:45:53.046
bottom, is that this app creates

00:45:53.046 --> 00:45:54.376
a number of network connections

00:45:54.486 --> 00:45:56.336
to a server, to read lists of

00:45:56.336 --> 00:45:57.736
URLs from, which are then

00:45:57.736 --> 00:45:59.196
displayed in the WebViews

00:45:59.196 --> 00:46:00.246
whenever the refresh button is

00:46:00.246 --> 00:46:00.386
hit.

00:46:01.146 --> 00:46:02.776
So, let's jump into XCode to see

00:46:02.776 --> 00:46:03.866
how we are setting up those

00:46:03.866 --> 00:46:04.576
network connections.

00:46:05.446 --> 00:46:07.686
So, here we are in XCode in the

00:46:07.686 --> 00:46:09.806
createConnections method, which

00:46:09.806 --> 00:46:10.956
does just that.

00:46:11.376 --> 00:46:12.006
It's very simple.

00:46:12.006 --> 00:46:13.716
We have a for loop, maybe just

00:46:13.716 --> 00:46:15.436
create some sockets and connect

00:46:15.436 --> 00:46:16.336
them to our server.

00:46:17.266 --> 00:46:18.946
And we monitor that socket for

00:46:18.946 --> 00:46:20.086
readability with one of these

00:46:20.146 --> 00:46:21.696
dispatch read sources that we've

00:46:21.696 --> 00:46:22.896
seen so many times already in

00:46:22.896 --> 00:46:23.396
this session.

00:46:23.396 --> 00:46:24.796
And here this is just the

00:46:24.796 --> 00:46:25.506
'C' API.

00:46:26.546 --> 00:46:27.946
We then set up the event handler

00:46:27.946 --> 00:46:29.506
block for that dispatch source

00:46:29.906 --> 00:46:30.216
here.

00:46:30.366 --> 00:46:32.146
And when the socket becomes

00:46:32.146 --> 00:46:33.346
readable, we just read from it

00:46:33.346 --> 00:46:34.756
with the read system call until

00:46:34.756 --> 00:46:36.016
there is no more data available.

00:46:36.756 --> 00:46:38.416
Once we have the data, we pass

00:46:38.416 --> 00:46:40.586
it to our database, subsystem in

00:46:40.586 --> 00:46:41.576
the application with this

00:46:41.606 --> 00:46:42.556
process 0 method.

00:46:43.706 --> 00:46:45.076
So, let's build and run, and

00:46:45.076 --> 00:46:46.206
take a system trace of this

00:46:46.206 --> 00:46:47.416
application and see how it

00:46:47.416 --> 00:46:48.016
executes.

00:46:48.016 --> 00:46:52.246
So here we are in Instruments,

00:46:52.286 --> 00:46:54.146
in system trace, and in addition

00:46:54.146 --> 00:46:55.256
to the usual tracks in system

00:46:55.256 --> 00:46:56.736
trace, we've added this new GCD

00:46:56.736 --> 00:46:57.686
performance instrument.

00:46:58.096 --> 00:46:59.436
When we click on there, we see a

00:46:59.436 --> 00:47:01.056
number of performance events

00:47:01.056 --> 00:47:02.456
that have been reported for

00:47:02.456 --> 00:47:03.346
performance problems.

00:47:03.746 --> 00:47:05.336
One of these is this 'mutation

00:47:05.336 --> 00:47:06.456
after activation' event, that we

00:47:06.456 --> 00:47:08.256
can also see when we go and mouse

00:47:08.256 --> 00:47:08.886
over the timeline.

00:47:09.316 --> 00:47:10.576
We can also click on one of the

00:47:10.576 --> 00:47:12.176
other events here, such as this,

00:47:12.176 --> 00:47:13.486
're-target after activation'

00:47:13.486 --> 00:47:13.836
event.

00:47:14.506 --> 00:47:15.846
And the list will take us

00:47:15.846 --> 00:47:16.626
directly there.

00:47:16.626 --> 00:47:18.036
If you want more details on

00:47:18.036 --> 00:47:19.646
this, we can disclose the

00:47:19.646 --> 00:47:20.806
backtrace on the right hand side

00:47:20.806 --> 00:47:22.306
of Instruments which will show

00:47:22.306 --> 00:47:23.636
us where exactly this event

00:47:23.636 --> 00:47:24.736
occurred in your application.

00:47:25.166 --> 00:47:26.416
So, here for instance it is in

00:47:26.416 --> 00:47:28.226
our createConnections method.

00:47:29.546 --> 00:47:30.956
If we double click on this

00:47:30.956 --> 00:47:32.306
frame, instruments will show us

00:47:32.306 --> 00:47:33.386
directly the line of code where

00:47:33.766 --> 00:47:34.696
the problem occurred.

00:47:35.396 --> 00:47:36.786
This is actually a target queue

00:47:36.786 --> 00:47:38.176
call here that indeed occurs

00:47:38.176 --> 00:47:38.966
after activate.

00:47:39.416 --> 00:47:40.606
Like, this is the pattern up

00:47:40.606 --> 00:47:41.476
here just told you about.

00:47:41.476 --> 00:47:43.006
To go and fix that, we can jump

00:47:43.136 --> 00:47:44.376
directly into XCode with the

00:47:44.376 --> 00:47:45.676
open file and XCode button in

00:47:45.676 --> 00:47:46.196
Instruments.

00:47:46.676 --> 00:47:48.416
So, here we are at that

00:47:48.416 --> 00:47:50.336
dispatch_set_target_queue_line and indeed

00:47:50.396 --> 00:47:51.716
it, as well as the

00:47:51.716 --> 00:47:53.816
dispatch_source_event_handler set up

00:47:53.816 --> 00:47:54.936
happens after activate.

00:47:55.486 --> 00:47:56.896
So, here in this example, it's

00:47:56.896 --> 00:47:57.636
really easy to fix.

00:47:57.636 --> 00:47:58.936
We just move these two lines

00:47:59.356 --> 00:48:00.206
down below.

00:48:00.206 --> 00:48:01.706
And we have fixed the problem.

00:48:01.706 --> 00:48:03.386
We have activate after we set up

00:48:03.426 --> 00:48:04.856
the source, and not before.

00:48:05.526 --> 00:48:06.616
So, let's jump back into

00:48:06.616 --> 00:48:08.156
instruments and see what we can

00:48:08.156 --> 00:48:09.556
see in the system trace now.

00:48:09.856 --> 00:48:11.176
It looks the same as before,

00:48:11.176 --> 00:48:12.696
except when you click on the GCD

00:48:12.696 --> 00:48:14.196
performance track, you will see

00:48:14.196 --> 00:48:15.336
there is no more significant

00:48:15.336 --> 00:48:16.476
performance problems detected.

00:48:16.476 --> 00:48:17.836
And that's what you ought to see

00:48:17.836 --> 00:48:18.686
if you use this instrument.

00:48:18.836 --> 00:48:20.466
So, of course this was very

00:48:20.466 --> 00:48:21.686
simple in this application.

00:48:21.686 --> 00:48:22.686
You may have to do some work.

00:48:24.566 --> 00:48:25.806
So, let's focus on the points

00:48:25.806 --> 00:48:26.916
track in the application.

00:48:26.916 --> 00:48:28.096
This shows us a number of

00:48:28.146 --> 00:48:29.366
network event handlers.

00:48:29.816 --> 00:48:31.106
And these are the source event

00:48:31.106 --> 00:48:32.166
handlers in our application.

00:48:32.666 --> 00:48:34.066
How did you manage to make these

00:48:34.066 --> 00:48:35.006
show up in instruments?

00:48:35.606 --> 00:48:36.246
That's actually really

00:48:36.246 --> 00:48:37.356
interesting to understand

00:48:37.356 --> 00:48:38.386
because it's something you can

00:48:38.386 --> 00:48:39.916
apply to your own code to

00:48:39.916 --> 00:48:41.646
understand how it executes in

00:48:41.676 --> 00:48:42.176
instruments.

00:48:43.806 --> 00:48:45.556
Well, going back to XCode in our

00:48:45.556 --> 00:48:47.486
createConnections method, when

00:48:47.486 --> 00:48:49.516
we set up our source and its

00:48:49.516 --> 00:48:50.456
source event handler, we are

00:48:50.456 --> 00:48:51.976
interested in the execution of

00:48:52.046 --> 00:48:53.806
that event handler, and try to

00:48:53.806 --> 00:48:54.926
understand its timing.

00:48:55.506 --> 00:48:57.016
To see that in Instruments, we've

00:48:57.016 --> 00:48:59.156
added the kdebug_signpost_start

00:48:59.156 --> 00:49:00.696
function at the beginning of the

00:49:00.696 --> 00:49:02.776
handler, and the kdebug_signpost_end

00:49:02.886 --> 00:49:03.936
function at the end.

00:49:03.936 --> 00:49:05.836
And that is all it takes for the

00:49:05.836 --> 00:49:07.346
section of code to appear

00:49:07.346 --> 00:49:08.486
highlighted in the points track

00:49:08.946 --> 00:49:10.096
in instrument system trace.

00:49:10.396 --> 00:49:11.836
So, if you switch back to

00:49:11.836 --> 00:49:13.766
instruments, that is these red

00:49:13.866 --> 00:49:15.436
dots at the pop in the points

00:49:15.436 --> 00:49:16.826
track and we can see in the back

00:49:16.826 --> 00:49:18.706
trace that it matches our event

00:49:18.706 --> 00:49:20.486
handler for one of these events.

00:49:21.566 --> 00:49:22.726
If you zoom in on one of these

00:49:22.726 --> 00:49:24.416
interesting looking areas in the

00:49:24.416 --> 00:49:26.226
points track, here, you can see

00:49:26.226 --> 00:49:27.956
that there is a number of event

00:49:27.956 --> 00:49:29.876
handlers that are occurring very

00:49:29.876 --> 00:49:30.946
close together.

00:49:31.156 --> 00:49:33.276
And by mousing over we can

00:49:33.276 --> 00:49:34.066
actually see that they

00:49:34.066 --> 00:49:35.426
execute for very short amounts

00:49:35.426 --> 00:49:35.836
of time.

00:49:36.356 --> 00:49:38.286
The pop-up will tell us the

00:49:38.286 --> 00:49:39.746
amount of time it has executed

00:49:39.946 --> 00:49:40.826
and we can even see that

00:49:40.826 --> 00:49:42.186
sometimes we have overlapping

00:49:42.186 --> 00:49:43.226
event handlers that are all

00:49:43.226 --> 00:49:44.886
executing concurrently at the

00:49:44.926 --> 00:49:45.506
same time.

00:49:45.506 --> 00:49:47.766
So, this is one of the symptoms

00:49:47.766 --> 00:49:48.856
of potentially unwanted

00:49:48.856 --> 00:49:50.056
concurrency in our application,

00:49:50.376 --> 00:49:51.546
where something that didn't look

00:49:51.546 --> 00:49:53.346
like it would cause concurrency

00:49:53.556 --> 00:49:55.926
in your code, actually does run

00:49:55.926 --> 00:49:57.146
in a concurrent way or multiple

00:49:57.146 --> 00:49:58.906
threads and cause potentially

00:49:58.906 --> 00:49:59.876
extra context switches.

00:50:01.466 --> 00:50:03.076
So, to understand this better,

00:50:03.076 --> 00:50:04.586
let's bring up the threads in

00:50:04.586 --> 00:50:05.136
instruments.

00:50:05.426 --> 00:50:06.556
And the system trace that are

00:50:06.556 --> 00:50:07.506
executing this code.

00:50:12.446 --> 00:50:13.726
So, here I've highlighted the

00:50:13.726 --> 00:50:14.716
three worker threads that are

00:50:14.716 --> 00:50:16.026
executing these event handlers.

00:50:16.496 --> 00:50:17.856
And we can see as before that

00:50:17.856 --> 00:50:19.366
they are executing on call

00:50:19.556 --> 00:50:20.376
during this time.

00:50:20.926 --> 00:50:22.316
And the time they were running.

00:50:22.316 --> 00:50:23.846
But here we can see they were

00:50:23.846 --> 00:50:25.866
again, running for a very short

00:50:25.866 --> 00:50:27.566
amount of time in this area.

00:50:27.926 --> 00:50:30.386
And we can verify that they are

00:50:30.386 --> 00:50:31.686
making these read system calls

00:50:31.686 --> 00:50:33.146
that we saw earlier in the event

00:50:33.146 --> 00:50:33.416
handler.

00:50:34.136 --> 00:50:35.766
And we can get some more detail

00:50:35.766 --> 00:50:36.856
by looking at the back trace

00:50:36.856 --> 00:50:38.246
again, and seeing, yes it is us

00:50:38.246 --> 00:50:39.276
that is calling that read system

00:50:39.276 --> 00:50:41.246
call and here it reads 97 bytes

00:50:41.576 --> 00:50:42.366
from our socket.

00:50:44.216 --> 00:50:45.246
And looking at the other

00:50:45.246 --> 00:50:46.206
threads, the same pattern

00:50:46.206 --> 00:50:46.766
repeats.

00:50:46.766 --> 00:50:47.786
You can see it's the same read

00:50:47.786 --> 00:50:49.206
system calls occurring there,

00:50:49.206 --> 00:50:49.986
more or less at the same

00:50:49.986 --> 00:50:52.466
timeframe and so on the second

00:50:52.466 --> 00:50:53.496
thread here or on the first

00:50:53.496 --> 00:50:53.736
thread.

00:50:53.996 --> 00:50:54.856
They're really all doing the

00:50:54.856 --> 00:50:56.736
same thing, and overlapping.

00:50:57.886 --> 00:50:59.156
It would be much better for our

00:50:59.156 --> 00:51:01.806
program if these things executed

00:51:01.806 --> 00:51:02.556
on a single thread.

00:51:02.556 --> 00:51:03.736
Here we don't really get any

00:51:03.736 --> 00:51:05.046
benefit from the concurrency

00:51:05.316 --> 00:51:06.456
because we are executing such

00:51:06.456 --> 00:51:07.436
short pieces of code.

00:51:07.896 --> 00:51:09.716
And we are probably getting more

00:51:09.716 --> 00:51:11.236
harm than good from adding these

00:51:11.236 --> 00:51:12.216
extra context switches.

00:51:13.036 --> 00:51:14.496
So, let's apply the patterns

00:51:14.496 --> 00:51:15.776
that we saw earlier to fix this

00:51:15.776 --> 00:51:16.666
problem in this sample

00:51:16.666 --> 00:51:17.216
application.

00:51:17.806 --> 00:51:19.446
Jumping back into XCode, let's

00:51:19.446 --> 00:51:20.576
see how we set up the target

00:51:20.576 --> 00:51:21.616
queue for this source that we

00:51:21.616 --> 00:51:21.836
have.

00:51:23.056 --> 00:51:24.096
So, that's sort of when you

00:51:24.096 --> 00:51:25.766
create this queue at the top of

00:51:25.836 --> 00:51:27.256
this function framework and as

00:51:27.256 --> 00:51:29.186
you can see, we do it simply by

00:51:29.186 --> 00:51:30.046
calling this

00:51:30.046 --> 00:51:30.566
dispatch_queue_create.

00:51:31.446 --> 00:51:32.846
And that creates an independent

00:51:32.846 --> 00:51:34.016
serial queue that isn't

00:51:34.016 --> 00:51:35.346
connected to anything else in

00:51:35.346 --> 00:51:36.046
our application.

00:51:36.046 --> 00:51:37.886
This is exactly like the case we

00:51:37.886 --> 00:51:40.036
had earlier in my example of the

00:51:40.036 --> 00:51:40.936
networking subsystem.

00:51:41.636 --> 00:51:43.356
So, let's fix that by adding a

00:51:43.356 --> 00:51:44.616
mutual exclusion context at the

00:51:44.616 --> 00:51:46.196
bottom of all of these queues

00:51:46.196 --> 00:51:47.106
for all of these connections.

00:51:47.606 --> 00:51:49.326
And we do that by adding the, or

00:51:49.326 --> 00:51:50.666
by switching to the

00:51:50.666 --> 00:51:51.606
dispatch_queue_create_with_target

00:51:51.606 --> 00:51:53.256
function up here introduced to

00:51:53.256 --> 00:51:53.776
you earlier.

00:51:54.416 --> 00:51:58.996
So, here we add

00:51:58.996 --> 00:51:59.746
dispatch_queue_create_wth_target.

00:52:00.206 --> 00:52:01.716
And we use a single mutual

00:52:01.716 --> 00:52:02.906
exclusion queue as the target

00:52:02.906 --> 00:52:03.756
queue for all of these.

00:52:04.196 --> 00:52:05.246
And this is a serial queue that

00:52:05.246 --> 00:52:06.596
we created somewhere else.

00:52:07.416 --> 00:52:09.036
And with that, we build and run

00:52:09.036 --> 00:52:09.976
again and look at the system

00:52:09.976 --> 00:52:10.546
trace again.

00:52:11.576 --> 00:52:12.846
And now it looks very different.

00:52:13.466 --> 00:52:15.436
Here we have still the same

00:52:15.436 --> 00:52:16.616
points track and we still see

00:52:16.616 --> 00:52:18.136
the same network events that

00:52:18.136 --> 00:52:19.426
occur, but as you can see,

00:52:19.656 --> 00:52:20.596
there's no more overlapping

00:52:20.596 --> 00:52:21.686
events in that track, and

00:52:21.906 --> 00:52:23.066
there's a single worker thread

00:52:23.066 --> 00:52:24.226
that executes this code.

00:52:24.306 --> 00:52:26.376
And if we zoom in on one of

00:52:26.426 --> 00:52:27.726
these clusters we can see this

00:52:27.726 --> 00:52:29.066
is actually many instances of

00:52:29.066 --> 00:52:30.996
that event handler executing in

00:52:30.996 --> 00:52:32.306
rapid succession, which is

00:52:32.306 --> 00:52:33.686
exactly what we expected.

00:52:34.216 --> 00:52:36.376
And when you zoom in more on one

00:52:36.376 --> 00:52:37.746
particular event, you can see

00:52:37.746 --> 00:52:38.896
it's still executing for a

00:52:38.896 --> 00:52:40.676
fairly short amount of time, and

00:52:40.676 --> 00:52:41.716
making those same read sys

00:52:41.716 --> 00:52:42.116
calls.

00:52:42.786 --> 00:52:44.216
But now that is much less

00:52:44.216 --> 00:52:45.266
problematic because it's all

00:52:45.266 --> 00:52:46.986
happening on a single thread.

00:52:49.636 --> 00:52:51.886
So, this may seem like a very

00:52:51.886 --> 00:52:53.626
simple and trivial change, but

00:52:53.626 --> 00:52:54.546
it's worth pointing out that

00:52:54.546 --> 00:52:55.926
it's exactly this type of small

00:52:55.926 --> 00:52:57.906
tweak that led to the 1.3X

00:52:57.906 --> 00:52:59.306
performance improvement in some

00:52:59.306 --> 00:53:00.776
of our own framework code that

00:53:00.776 --> 00:53:02.016
Daniel pointed out at the

00:53:02.016 --> 00:53:02.936
beginning of the session.

00:53:03.426 --> 00:53:04.736
So, very small changes like this

00:53:04.736 --> 00:53:05.596
can make a significant

00:53:05.596 --> 00:53:06.036
difference.

00:53:07.476 --> 00:53:12.256
All right so, let's look back at

00:53:12.256 --> 00:53:13.166
what we've covered today.

00:53:13.166 --> 00:53:15.156
Daniel, at the beginning went

00:53:15.156 --> 00:53:16.606
with you over the details of how

00:53:16.606 --> 00:53:18.736
not to go off core unnecessarily

00:53:18.736 --> 00:53:19.976
is ever more important for

00:53:19.976 --> 00:53:21.406
modern CPUs so that it can reach

00:53:21.406 --> 00:53:22.456
the most efficient performance

00:53:22.456 --> 00:53:22.676
state.

00:53:23.506 --> 00:53:25.106
We looked at the importance of

00:53:25.106 --> 00:53:26.686
sizing the workforce of power

00:53:26.686 --> 00:53:28.586
workloads and for work moving

00:53:28.586 --> 00:53:29.726
between subsystems in your

00:53:29.726 --> 00:53:31.456
application as well as inside

00:53:31.536 --> 00:53:32.266
those subsystems.

00:53:33.166 --> 00:53:34.446
We talked about how to choose

00:53:34.446 --> 00:53:36.476
good granularity of concurrency

00:53:36.476 --> 00:53:37.916
with GCD by using a fixed number

00:53:37.916 --> 00:53:38.946
of serial queue hierarchies in

00:53:38.946 --> 00:53:39.596
your application.

00:53:40.096 --> 00:53:41.056
And Pierre walked you through

00:53:41.056 --> 00:53:42.956
how to modernize your GCD usage

00:53:43.256 --> 00:53:44.356
to take full advantage of

00:53:44.356 --> 00:53:46.186
improvements in the OS, and in our

00:53:46.186 --> 00:53:46.626
hardware.

00:53:47.596 --> 00:53:49.096
And finally, we saw how we can

00:53:49.096 --> 00:53:50.486
use instruments to find problems

00:53:50.486 --> 00:53:51.886
spots in our application and how

00:53:52.206 --> 00:53:54.826
to fix them.

00:53:55.186 --> 00:53:56.256
For more information on this

00:53:56.256 --> 00:53:57.926
session, I will direct you to

00:53:57.926 --> 00:54:00.196
this URL where the documentation

00:54:00.196 --> 00:54:02.116
links for GCD are as well as the

00:54:02.116 --> 00:54:04.176
movie for the session, and we

00:54:04.176 --> 00:54:05.386
have some related sessions this

00:54:05.386 --> 00:54:06.566
week that might be worthwhile

00:54:06.876 --> 00:54:07.616
going to.

00:54:07.746 --> 00:54:09.726
Introducing Core ML already

00:54:09.726 --> 00:54:11.226
having happened, the other two

00:54:11.226 --> 00:54:12.386
are going to help you with

00:54:12.386 --> 00:54:13.636
parallel and computing

00:54:13.686 --> 00:54:14.176
intensive task in your

00:54:14.336 --> 00:54:15.746
application like we talked about

00:54:15.746 --> 00:54:16.346
at the beginning.

00:54:16.946 --> 00:54:19.426
And the last two are going to

00:54:19.426 --> 00:54:20.566
help you with more performance

00:54:20.566 --> 00:54:21.656
analysis and improvements of

00:54:21.696 --> 00:54:23.186
different aspects of your app.

00:54:23.186 --> 00:54:25.216
And with that, I'd like to thank

00:54:25.216 --> 00:54:25.836
you for coming.

00:54:26.176 --> 00:54:26.986
If you have any questions,

00:54:26.986 --> 00:54:27.966
please come and see us at the

00:54:27.966 --> 00:54:28.246
labs.

00:54:29.508 --> 00:54:31.508
[ Applause ]