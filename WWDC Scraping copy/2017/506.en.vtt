WEBVTT

00:00:21.126 --> 00:00:21.806
>> Hello everyone.

00:00:22.516 --> 00:00:26.546
[ Applause ]

00:00:27.046 --> 00:00:28.206
I hope you're have a great time

00:00:28.206 --> 00:00:29.006
at WWDC so far.

00:00:29.776 --> 00:00:31.216
Allow me to introduce myself.

00:00:31.906 --> 00:00:33.646
My name is Brett Keating and I'm

00:00:33.646 --> 00:00:34.896
here with my colleague Frank

00:00:34.896 --> 00:00:36.386
Doepke and we're here to tell

00:00:36.386 --> 00:00:38.256
you about Apple's new Vision

00:00:38.256 --> 00:00:39.836
framework, so let's get started.

00:00:39.836 --> 00:00:42.506
We're going to begin by showing

00:00:42.506 --> 00:00:43.716
you what Vision can do for your

00:00:43.716 --> 00:00:44.266
apps.

00:00:44.746 --> 00:00:45.686
We're going to go through a few

00:00:45.686 --> 00:00:47.126
visual examples of the

00:00:47.126 --> 00:00:48.106
algorithms that are going to be

00:00:48.106 --> 00:00:49.016
made available in the Vision

00:00:49.016 --> 00:00:49.766
framework this year.

00:00:50.936 --> 00:00:51.976
At which point I'll hand it off

00:00:51.976 --> 00:00:54.636
to Frank to talk about the

00:00:54.636 --> 00:00:56.116
concepts behind the Vision

00:00:56.116 --> 00:00:57.566
framework, why we designed

00:00:57.566 --> 00:00:58.836
things the way we did, what the

00:00:58.836 --> 00:01:00.696
mental model behind our API is.

00:01:00.796 --> 00:01:02.516
And then we'll go a little

00:01:02.516 --> 00:01:04.906
deeper and go through a code

00:01:04.906 --> 00:01:05.446
example.

00:01:06.006 --> 00:01:08.796
This code example brings

00:01:08.796 --> 00:01:09.716
together a few different

00:01:09.716 --> 00:01:11.386
technologies in our SDK,

00:01:11.496 --> 00:01:14.566
including Core Image, as well as

00:01:14.566 --> 00:01:16.406
the brand-new Core ML framework

00:01:16.656 --> 00:01:17.516
that we're offering this year

00:01:18.386 --> 00:01:19.926
which enables you to put in your

00:01:19.926 --> 00:01:21.226
own custom models and have them

00:01:21.276 --> 00:01:23.746
be accelerated using our

00:01:24.896 --> 00:01:26.106
hardware.

00:01:26.896 --> 00:01:28.716
So, let's begin with what you

00:01:28.716 --> 00:01:29.326
can do with Vision.

00:01:30.596 --> 00:01:32.986
Let's start off with face

00:01:32.986 --> 00:01:33.466
detection.

00:01:33.466 --> 00:01:35.676
Now face detection is something

00:01:35.676 --> 00:01:37.136
we already have in our SDK, but

00:01:37.866 --> 00:01:39.126
we're offering in the Vision

00:01:39.126 --> 00:01:40.536
framework new this year a face

00:01:40.536 --> 00:01:41.646
detection that's based on deep

00:01:41.646 --> 00:01:42.036
learning.

00:01:43.246 --> 00:01:44.776
And you may already know that

00:01:44.986 --> 00:01:46.006
deep learning has made

00:01:46.496 --> 00:01:47.786
groundbreaking changes in the

00:01:47.786 --> 00:01:49.916
accuracy in what we can do with

00:01:49.986 --> 00:01:51.296
Vision technologies and face

00:01:51.296 --> 00:01:53.386
detection is no exception.

00:01:54.136 --> 00:01:54.886
We're going to have higher

00:01:54.886 --> 00:01:56.086
precision which means fewer

00:01:56.086 --> 00:01:57.686
false positives, but we are also

00:01:57.686 --> 00:01:58.906
going to have dramatically

00:01:59.326 --> 00:02:01.806
higher recall which means we'll

00:02:01.806 --> 00:02:02.796
miss less faces.

00:02:03.286 --> 00:02:04.156
So, let's look at some of the

00:02:04.156 --> 00:02:05.986
examples of faces that we will

00:02:05.986 --> 00:02:07.446
now be able to detect with the

00:02:07.446 --> 00:02:08.106
Vision framework.

00:02:08.936 --> 00:02:10.515
For one thing, we'll be able to

00:02:10.515 --> 00:02:11.756
detect smaller faces.

00:02:13.936 --> 00:02:15.536
We'll also be doing a better job

00:02:15.536 --> 00:02:17.066
of detecting strong profiles.

00:02:19.516 --> 00:02:21.386
We'll also do a better job

00:02:21.846 --> 00:02:23.036
detecting more partially

00:02:23.036 --> 00:02:25.616
occluded faces and that includes

00:02:25.826 --> 00:02:27.276
things like hats and glasses.

00:02:27.276 --> 00:02:30.506
Sticking with the faces theme

00:02:30.506 --> 00:02:33.496
for a little longer, we now are

00:02:33.496 --> 00:02:34.786
offering in the Vision framework

00:02:34.786 --> 00:02:37.006
new this year face landmarks,

00:02:37.656 --> 00:02:38.596
what are face landmarks?

00:02:39.496 --> 00:02:41.026
This is a constellation of

00:02:41.026 --> 00:02:42.006
points that we detect on the

00:02:42.006 --> 00:02:43.696
facer, things like the corners

00:02:43.696 --> 00:02:45.036
of the eyes, the outline of the

00:02:45.036 --> 00:02:47.036
mouth, the contour of the chin.

00:02:48.316 --> 00:02:50.876
Here's an example, here's

00:02:50.876 --> 00:02:53.766
another example, and one more

00:02:53.766 --> 00:02:54.266
example.

00:02:55.396 --> 00:02:56.946
We're really excited about this

00:02:56.946 --> 00:02:57.956
I think there's going to be some

00:02:57.956 --> 00:02:59.056
great apps created with this

00:02:59.056 --> 00:02:59.596
technology.

00:03:01.996 --> 00:03:03.696
Next, also new this year in the

00:03:03.696 --> 00:03:04.776
Vision framework is image

00:03:04.776 --> 00:03:05.426
registration.

00:03:06.026 --> 00:03:07.176
If you don't know what image

00:03:07.176 --> 00:03:08.546
registration is it's basically

00:03:08.876 --> 00:03:11.126
aligning two images based on the

00:03:11.126 --> 00:03:12.226
features that are present in

00:03:12.226 --> 00:03:12.876
those images.

00:03:13.496 --> 00:03:15.826
You can use this for stitching

00:03:15.826 --> 00:03:17.166
together used for panorama kind

00:03:17.286 --> 00:03:18.686
of like this example or image

00:03:18.686 --> 00:03:19.726
stacking applications.

00:03:20.536 --> 00:03:22.406
We have two different kinds, one

00:03:22.406 --> 00:03:24.126
that's translation only and one

00:03:24.126 --> 00:03:24.756
that gives you for full

00:03:24.756 --> 00:03:27.176
homography for greater accuracy.

00:03:28.656 --> 00:03:31.136
We're also offering a few

00:03:31.136 --> 00:03:32.276
technologies that are already in

00:03:32.276 --> 00:03:33.566
our SDK through CIDetector

00:03:33.566 --> 00:03:34.406
interface.

00:03:34.796 --> 00:03:35.706
We're making them available in

00:03:35.706 --> 00:03:36.776
the Vision API as well.

00:03:36.896 --> 00:03:38.766
That includes rectangle

00:03:38.766 --> 00:03:40.516
detection as you can see, we

00:03:40.516 --> 00:03:42.296
detect the sign in the picture.

00:03:43.816 --> 00:03:45.566
We're also doing barcode

00:03:45.566 --> 00:03:46.776
detection and recognition in the

00:03:46.776 --> 00:03:50.726
Vision API and text detection as

00:03:52.256 --> 00:03:52.376
well.

00:03:53.766 --> 00:03:54.866
Another new technology,

00:03:55.116 --> 00:03:55.976
brand-new in the Vision

00:03:55.976 --> 00:03:56.986
framework this year is object

00:03:56.986 --> 00:03:57.366
tracking.

00:03:58.186 --> 00:04:00.576
You can use this to track a face

00:04:00.576 --> 00:04:01.726
if you've detected a face.

00:04:01.726 --> 00:04:03.246
You can use that face rectangle

00:04:03.246 --> 00:04:05.186
as an initial condition to the

00:04:05.186 --> 00:04:06.436
tracking and then the Vision

00:04:06.436 --> 00:04:07.756
framework will track that square

00:04:08.196 --> 00:04:09.056
throughout the rest of your

00:04:09.056 --> 00:04:09.446
video.

00:04:10.246 --> 00:04:12.486
Will also track rectangles and

00:04:12.486 --> 00:04:14.036
you can also define the initial

00:04:14.036 --> 00:04:15.036
condition yourself.

00:04:15.846 --> 00:04:17.426
So that's what I mean by general

00:04:17.426 --> 00:04:19.456
templates, if you decide to for

00:04:19.456 --> 00:04:21.305
example, put a square around

00:04:21.305 --> 00:04:24.656
this wakeboarder as I have, you

00:04:24.656 --> 00:04:27.116
can then go ahead and track

00:04:27.116 --> 00:04:27.376
that.

00:04:29.336 --> 00:04:30.556
You can see that we handle

00:04:30.556 --> 00:04:32.666
pretty large changes in scale,

00:04:32.756 --> 00:04:34.396
pretty large deformations fairly

00:04:34.396 --> 00:04:35.746
robustly with this technology.

00:04:39.096 --> 00:04:40.266
Another really exciting

00:04:41.296 --> 00:04:42.956
technology that's new in Apple's

00:04:42.956 --> 00:04:45.086
SDK this year Core ML and you

00:04:45.086 --> 00:04:46.126
can integrate your Core ML

00:04:46.126 --> 00:04:47.576
models directly into Vision.

00:04:48.236 --> 00:04:50.916
As I've mentioned, machine

00:04:50.916 --> 00:04:52.496
learning does great things for

00:04:52.496 --> 00:04:55.336
Computer Vision and you can use

00:04:55.336 --> 00:04:57.066
Core ML if you want to create

00:04:57.066 --> 00:04:58.166
your own models, do your own

00:04:58.166 --> 00:04:58.716
solution.

00:04:59.416 --> 00:05:01.246
Perhaps for example, you want to

00:05:01.246 --> 00:05:02.476
create a wedding application

00:05:02.986 --> 00:05:06.316
where you're able to detect this

00:05:06.316 --> 00:05:08.006
part of the wedding is the

00:05:08.006 --> 00:05:08.956
reception, this part of the

00:05:08.956 --> 00:05:09.816
wedding is where the bride is

00:05:09.816 --> 00:05:10.686
walking down the aisle.

00:05:11.416 --> 00:05:12.456
If you want to train your own

00:05:12.456 --> 00:05:14.706
model and you have the data to

00:05:14.706 --> 00:05:17.196
train your own model you can do

00:05:17.196 --> 00:05:17.476
that.

00:05:18.556 --> 00:05:20.766
Core ML as I mentioned, provides

00:05:20.766 --> 00:05:22.116
native acceleration for custom

00:05:22.116 --> 00:05:23.206
models so they'll run really

00:05:23.206 --> 00:05:25.706
fast and Vision provides the

00:05:25.706 --> 00:05:26.876
imaging pipeline to support

00:05:26.876 --> 00:05:28.776
these models, so you won't have

00:05:28.776 --> 00:05:30.276
to do any rescaling or anything

00:05:30.276 --> 00:05:31.156
like that we'll take care of all

00:05:31.156 --> 00:05:31.586
that for you.

00:05:31.586 --> 00:05:33.106
We know what your model is

00:05:33.106 --> 00:05:34.476
expecting and we'll put the

00:05:34.476 --> 00:05:35.626
image in the right format.

00:05:37.596 --> 00:05:38.826
If you're interested in Core ML

00:05:38.826 --> 00:05:40.396
there's some sessions that you

00:05:40.396 --> 00:05:42.576
can go to, we've listed the labs

00:05:42.576 --> 00:05:43.306
down here for you.

00:05:43.866 --> 00:05:45.346
One of them will be tomorrow

00:05:45.346 --> 00:05:47.356
morning and then another one on

00:05:47.356 --> 00:05:47.986
Friday afternoon.

00:05:49.446 --> 00:05:51.386
So that's basically the features

00:05:51.386 --> 00:05:52.346
that are in the Vision

00:05:52.346 --> 00:05:52.796
framework.

00:05:54.236 --> 00:05:56.526
Overall, what Apple's new Vision

00:05:56.526 --> 00:05:58.306
framework provides are

00:05:58.766 --> 00:06:00.776
high-level on-device solutions

00:06:01.306 --> 00:06:02.586
to Computer Vision problems

00:06:02.706 --> 00:06:03.756
through one simple API.

00:06:03.756 --> 00:06:06.576
Now let me break this statement

00:06:06.576 --> 00:06:08.266
down just a little bit.

00:06:09.276 --> 00:06:10.586
What do I mean by high-level

00:06:10.586 --> 00:06:11.186
solutions?

00:06:11.896 --> 00:06:14.336
Well we don't want you to have

00:06:14.336 --> 00:06:15.556
to be a Computer Vision expert

00:06:15.556 --> 00:06:16.876
to put the magic of Computer

00:06:16.876 --> 00:06:18.066
Vision into your applications.

00:06:18.656 --> 00:06:20.706
You don't want to necessarily

00:06:20.706 --> 00:06:22.216
have to know which feature

00:06:22.216 --> 00:06:23.636
detector you want to use in

00:06:23.636 --> 00:06:25.086
combination with what classifier

00:06:25.086 --> 00:06:26.966
or set of classifiers, we're

00:06:26.966 --> 00:06:28.366
going to handle that for you or

00:06:28.366 --> 00:06:29.196
whether or not you want to use

00:06:29.196 --> 00:06:30.216
machine learning for example.

00:06:30.976 --> 00:06:31.946
If you're a developer you're

00:06:32.326 --> 00:06:33.426
probably thinking I just want to

00:06:33.426 --> 00:06:34.246
know where the faces are.

00:06:35.626 --> 00:06:36.506
And so, we're going to handle

00:06:36.506 --> 00:06:37.666
all that complexity for you.

00:06:38.926 --> 00:06:42.166
Depending on your use case we'll

00:06:42.166 --> 00:06:43.896
be doing either traditional

00:06:43.896 --> 00:06:45.206
approach if that's what's needed

00:06:45.206 --> 00:06:46.846
for maybe real-time applications

00:06:46.846 --> 00:06:49.166
or deep learning algorithms for

00:06:49.416 --> 00:06:50.176
higher accuracy.

00:06:50.846 --> 00:06:54.186
Now I also mentioned that we're

00:06:54.186 --> 00:06:55.666
doing all these algorithms on

00:06:55.666 --> 00:06:58.116
the device, let's talk a little

00:06:58.116 --> 00:07:00.436
bit about why we'd want to do

00:07:00.436 --> 00:07:01.916
things on device versus provided

00:07:01.916 --> 00:07:02.906
a cloud-based solution.

00:07:03.396 --> 00:07:05.766
First of all, it's privacy.

00:07:06.866 --> 00:07:08.836
As you know, Apple cares a lot

00:07:08.836 --> 00:07:11.006
about privacy, I care a lot

00:07:11.006 --> 00:07:12.476
about privacy working at Apple,

00:07:12.796 --> 00:07:14.136
sometimes it makes my job a

00:07:14.136 --> 00:07:16.446
little harder, but nonetheless

00:07:17.026 --> 00:07:18.056
keeping all your data on the

00:07:18.056 --> 00:07:20.156
device is the best way to

00:07:20.156 --> 00:07:21.736
protect your user's data

00:07:21.736 --> 00:07:22.176
privacy.

00:07:24.456 --> 00:07:26.276
Furthermore, with certain

00:07:26.276 --> 00:07:28.036
cloud-based solutions there's a

00:07:28.036 --> 00:07:29.106
cost associated with it.

00:07:29.306 --> 00:07:31.616
If you're a developer maybe

00:07:31.616 --> 00:07:32.796
you're paying usage fees to use

00:07:32.796 --> 00:07:33.976
a cloud-based solution.

00:07:35.316 --> 00:07:37.146
Your users will have to transfer

00:07:37.146 --> 00:07:38.066
the data to that cloud.

00:07:39.516 --> 00:07:41.306
All these costs they can add up

00:07:41.306 --> 00:07:42.296
for both the developers and the

00:07:42.296 --> 00:07:42.726
users.

00:07:42.956 --> 00:07:44.506
So, when everything's on the

00:07:44.506 --> 00:07:45.586
device it's free.

00:07:45.816 --> 00:07:50.566
And you can support real-time

00:07:50.566 --> 00:07:51.966
use cases like the tracking

00:07:51.966 --> 00:07:52.756
example I showed you.

00:07:53.566 --> 00:07:54.616
Imagine trying to track

00:07:54.616 --> 00:07:55.536
something through a video by

00:07:55.536 --> 00:07:56.416
sending every frame to the

00:07:56.416 --> 00:07:57.926
cloud, I don't think that's

00:07:57.926 --> 00:07:58.556
going to work too well.

00:07:59.226 --> 00:08:01.456
So, no latency, fast execution

00:08:01.926 --> 00:08:02.946
that's what we're offering with

00:08:03.226 --> 00:08:03.896
the Vision framework.

00:08:04.786 --> 00:08:07.696
So, I hope you enjoyed that

00:08:08.076 --> 00:08:09.186
introduction, now we're going to

00:08:09.186 --> 00:08:12.226
go a little deeper and talk

00:08:12.226 --> 00:08:13.446
about the Vision concepts.

00:08:13.446 --> 00:08:13.996
For this part of the

00:08:13.996 --> 00:08:15.356
presentation I'm going to hand

00:08:15.356 --> 00:08:15.976
it off to Frank.

00:08:16.516 --> 00:08:19.566
[ Applause ]

00:08:20.066 --> 00:08:20.556
>> Thank you Brett.

00:08:22.956 --> 00:08:24.406
Hi, good afternoon, my name is

00:08:24.406 --> 00:08:25.466
Frank Doepke and I'm going to

00:08:25.466 --> 00:08:26.996
talk about more of the technical

00:08:26.996 --> 00:08:28.876
details what is part of our

00:08:28.876 --> 00:08:29.496
Vision framework.

00:08:29.796 --> 00:08:33.916
So, what do we want to do, when

00:08:33.916 --> 00:08:35.726
we want to analyze an image we

00:08:35.916 --> 00:08:37.836
have three major tasks that we

00:08:37.836 --> 00:08:39.816
actually want to perform.

00:08:40.275 --> 00:08:41.416
So, we [inaudible] finding out

00:08:41.416 --> 00:08:42.576
what is in the image and what do

00:08:42.576 --> 00:08:43.456
I want to know about it.

00:08:44.316 --> 00:08:45.356
There's the machinery,

00:08:46.076 --> 00:08:47.166
somebody's got to do the work

00:08:47.676 --> 00:08:48.956
and we get some results out of

00:08:48.956 --> 00:08:50.316
it, at least we hope that's

00:08:50.316 --> 00:08:51.206
what's going to happen.

00:08:52.106 --> 00:08:54.036
So, in terminology for Vision

00:08:54.036 --> 00:08:56.046
that means the asks these are

00:08:56.046 --> 00:08:56.836
requests.

00:08:57.476 --> 00:08:59.166
And I just did a few examples

00:08:59.166 --> 00:09:01.016
here like the barcode detection

00:09:01.016 --> 00:09:03.936
or face detection and we feed

00:09:03.936 --> 00:09:07.016
them into our request handler.

00:09:07.926 --> 00:09:08.996
That's the one in this case to

00:09:08.996 --> 00:09:10.106
be an image request and

00:09:10.176 --> 00:09:11.136
[inaudible] hold on to the image

00:09:11.446 --> 00:09:12.576
and it's going to do all the

00:09:12.576 --> 00:09:13.416
work for us.

00:09:14.066 --> 00:09:16.486
And as a result, we get back

00:09:16.526 --> 00:09:18.086
what we call observations, what

00:09:18.086 --> 00:09:19.446
did we observe in this image.

00:09:20.046 --> 00:09:21.666
And these observations depend on

00:09:21.666 --> 00:09:22.726
what you asked us to do.

00:09:23.006 --> 00:09:24.516
So, we have classification

00:09:24.516 --> 00:09:26.956
observation or detected objects.

00:09:27.786 --> 00:09:28.986
Now when you want to track

00:09:28.986 --> 00:09:30.076
something in the sequence like

00:09:30.076 --> 00:09:32.656
the wakeboarder it's basically

00:09:32.656 --> 00:09:33.466
the same concept.

00:09:33.566 --> 00:09:34.796
We have some asks, we have the

00:09:34.796 --> 00:09:36.376
machinery, and we get some

00:09:36.376 --> 00:09:39.096
results out of it in the end.

00:09:39.096 --> 00:09:40.826
Again, the asks are requests.

00:09:41.426 --> 00:09:42.936
Now since this changes with

00:09:42.936 --> 00:09:44.686
every frame the image actually

00:09:44.686 --> 00:09:45.926
travels with the request.

00:09:47.736 --> 00:09:48.936
Our machinery is again

00:09:48.936 --> 00:09:49.826
[inaudible] request handler it's

00:09:49.826 --> 00:09:52.506
the sequence request handler and

00:09:52.506 --> 00:09:54.046
we get results which are

00:09:54.046 --> 00:09:55.906
observations that go with our

00:09:55.976 --> 00:09:56.546
requests.

00:09:58.086 --> 00:10:00.866
So, let me talk a little bit

00:10:00.866 --> 00:10:02.066
more about these two image

00:10:02.066 --> 00:10:02.966
request handlers that I

00:10:02.966 --> 00:10:03.806
mentioned so far.

00:10:04.476 --> 00:10:05.606
So, we have the image request

00:10:05.606 --> 00:10:07.666
handler that is mostly if you

00:10:07.666 --> 00:10:09.026
want to do something interactive

00:10:09.026 --> 00:10:09.736
with the image.

00:10:10.166 --> 00:10:11.386
You want to do multiple Vision

00:10:11.386 --> 00:10:13.466
tasks on an image, sometimes you

00:10:13.466 --> 00:10:15.116
actually do one and then based

00:10:15.116 --> 00:10:16.566
on the results you then kick off

00:10:16.566 --> 00:10:17.926
the next one and that's what you

00:10:17.926 --> 00:10:19.016
want to use the image request

00:10:19.016 --> 00:10:19.486
handler for.

00:10:19.966 --> 00:10:21.296
It'll hold on to the image that

00:10:21.296 --> 00:10:22.446
it's set up with for its

00:10:22.446 --> 00:10:25.366
lifecycle and that allows us

00:10:25.396 --> 00:10:26.856
under the cover to do

00:10:26.856 --> 00:10:28.856
performance optimizations by

00:10:28.856 --> 00:10:30.996
holding on to intermediates to

00:10:31.146 --> 00:10:32.866
make these requests perform

00:10:32.866 --> 00:10:33.276
faster.

00:10:34.016 --> 00:10:36.466
On the flipside, if I want to

00:10:36.466 --> 00:10:37.726
track something we use the

00:10:37.726 --> 00:10:38.816
sequence request handler.

00:10:39.276 --> 00:10:40.516
The sequence request handler

00:10:40.516 --> 00:10:42.086
allows us to keep tracking

00:10:42.186 --> 00:10:42.996
[inaudible] in the sequence

00:10:42.996 --> 00:10:43.666
request handler.

00:10:44.316 --> 00:10:46.136
And it will not hold on to all

00:10:46.136 --> 00:10:47.466
the images that gets fed into it

00:10:47.466 --> 00:10:48.626
over its lifecycle so they get

00:10:48.626 --> 00:10:49.326
released earlier.

00:10:50.056 --> 00:10:51.536
But that means on the flipside,

00:10:51.776 --> 00:10:53.486
you cannot do the same

00:10:53.486 --> 00:10:54.866
optimizations if you want to do

00:10:54.866 --> 00:10:56.146
multiple requests on the same

00:10:56.146 --> 00:10:56.486
image.

00:10:57.716 --> 00:10:59.836
So how does this look in the

00:10:59.836 --> 00:11:01.596
code, we are developers that's

00:11:01.596 --> 00:11:02.526
what we want to see.

00:11:04.116 --> 00:11:05.606
So, we start as a blank slate

00:11:05.606 --> 00:11:07.316
that's always good and then we

00:11:07.316 --> 00:11:08.446
create a request.

00:11:08.886 --> 00:11:10.466
In this case, it's a face

00:11:10.466 --> 00:11:12.746
detection request.

00:11:13.346 --> 00:11:14.496
Now we create the request

00:11:14.496 --> 00:11:16.546
handler, what I'm choosing here

00:11:16.546 --> 00:11:17.816
is a request handler based on

00:11:17.816 --> 00:11:19.216
the files so I have a file on

00:11:19.216 --> 00:11:22.556
disk that I want to use.

00:11:22.556 --> 00:11:24.026
Now I ask myRequestHandler to

00:11:24.026 --> 00:11:25.646
perform my request and this in

00:11:25.646 --> 00:11:27.566
this case [inaudible] it's just

00:11:27.566 --> 00:11:29.716
one request I have my array, but

00:11:29.716 --> 00:11:32.746
it could be many and I get my

00:11:32.906 --> 00:11:33.836
observations back.

00:11:34.996 --> 00:11:36.606
And this can be many faces that

00:11:36.606 --> 00:11:37.266
I detected.

00:11:37.836 --> 00:11:39.216
Now the one thing I would like

00:11:39.216 --> 00:11:42.746
to highlight here is the results

00:11:43.006 --> 00:11:45.036
come back as part of the request

00:11:45.156 --> 00:11:46.076
that we actually set up

00:11:46.076 --> 00:11:46.526
initially.

00:11:46.526 --> 00:11:49.916
How does it look when we want to

00:11:51.076 --> 00:11:52.506
track something?

00:11:52.616 --> 00:11:53.946
We create a sequence request

00:11:54.966 --> 00:11:55.776
handler [inaudible] of course

00:11:55.776 --> 00:11:57.026
not set up as an image because

00:11:57.216 --> 00:11:58.236
we have to [inaudible] all the

00:11:58.236 --> 00:11:59.656
frames of the sequence.

00:12:01.476 --> 00:12:02.326
So, I started with an

00:12:02.326 --> 00:12:03.906
observation that I got from the

00:12:03.906 --> 00:12:05.906
previous detection or I mark

00:12:05.956 --> 00:12:07.596
something up and I create my

00:12:07.596 --> 00:12:09.276
tracking request.

00:12:09.826 --> 00:12:12.026
And I simply have to run the

00:12:12.026 --> 00:12:12.596
request.

00:12:13.246 --> 00:12:15.036
And I feed in in this case as a

00:12:15.036 --> 00:12:17.026
pixel buffer the frame that is

00:12:17.026 --> 00:12:17.996
currently being dragged.

00:12:19.406 --> 00:12:21.176
And out of it again I get some

00:12:21.176 --> 00:12:21.616
results.

00:12:22.246 --> 00:12:25.246
So, now that we have talked

00:12:25.246 --> 00:12:26.816
about how this API is kind of

00:12:26.816 --> 00:12:28.426
structured I would like to guide

00:12:28.426 --> 00:12:30.066
you through some best practices

00:12:30.066 --> 00:12:31.616
so that you get, you know, the

00:12:31.616 --> 00:12:33.006
best experience out of Vision.

00:12:33.666 --> 00:12:37.456
So, when we want to put together

00:12:37.456 --> 00:12:38.936
a Computer Vision task you have

00:12:38.936 --> 00:12:39.886
to think about a few things.

00:12:41.476 --> 00:12:43.216
Number one, what is the right

00:12:43.216 --> 00:12:44.586
image type that I want to use.

00:12:45.826 --> 00:12:47.876
Number two, what am I going to

00:12:47.876 --> 00:12:48.746
do with the image.

00:12:50.396 --> 00:12:52.196
And number three, what

00:12:52.226 --> 00:12:53.496
performance do I need or want.

00:12:53.496 --> 00:12:54.346
Of course, you always want

00:12:54.346 --> 00:12:55.346
fastest, but there are some

00:12:55.346 --> 00:12:56.356
tradeoffs that you have to think

00:12:56.356 --> 00:12:56.636
about.

00:12:57.026 --> 00:12:59.136
So, let's talk about the image

00:13:00.526 --> 00:13:00.636
type.

00:13:00.846 --> 00:13:02.346
Vision supports a number of

00:13:02.346 --> 00:13:04.196
image types and they range from

00:13:04.196 --> 00:13:06.836
CVPixelBuffer, CGIImage or even

00:13:06.836 --> 00:13:09.276
as we saw in the previous

00:13:09.276 --> 00:13:11.446
example just from data that I

00:13:11.446 --> 00:13:12.606
use in NSURL.

00:13:13.216 --> 00:13:15.776
And we go over all these types

00:13:15.886 --> 00:13:17.076
in the following slides so that

00:13:17.076 --> 00:13:18.396
you know what to choose when.

00:13:20.556 --> 00:13:22.756
Which to choose depends a lot of

00:13:22.756 --> 00:13:23.916
like what you want to do.

00:13:23.916 --> 00:13:25.836
If you run from a camera stream

00:13:25.836 --> 00:13:27.596
or if you run from files on disk

00:13:28.346 --> 00:13:29.996
you have to look at that

00:13:30.096 --> 00:13:31.396
[inaudible] kind of which type

00:13:31.396 --> 00:13:32.856
of image you want to use.

00:13:33.246 --> 00:13:34.536
Now two important things to

00:13:34.536 --> 00:13:37.576
remember is we already have an

00:13:37.666 --> 00:13:39.206
imaging pipeline in the Vision

00:13:39.206 --> 00:13:41.076
framework you don't need to

00:13:41.076 --> 00:13:42.006
scale the images.

00:13:42.406 --> 00:13:43.616
So, unless you already have a

00:13:43.616 --> 00:13:44.926
very small representation that

00:13:44.926 --> 00:13:45.806
you absolutely want to use,

00:13:45.856 --> 00:13:47.136
please don't pre-scale because

00:13:47.246 --> 00:13:50.476
we'll just do the work twice.

00:13:50.666 --> 00:13:52.356
And mind the orientation.

00:13:52.526 --> 00:13:54.086
Computer Vision algorithms are

00:13:54.086 --> 00:13:57.056
mostly not, you know, sensitive

00:13:57.056 --> 00:13:59.226
to orientation or sorry, they

00:13:59.356 --> 00:14:01.176
are sensitive to orientation so

00:14:01.236 --> 00:14:02.576
you have to pass that in.

00:14:03.166 --> 00:14:04.586
And that is an important part

00:14:04.586 --> 00:14:05.676
because if you pass in a

00:14:05.816 --> 00:14:07.206
portrait image that's actually

00:14:07.206 --> 00:14:08.166
lying on its side we will not

00:14:08.166 --> 00:14:09.616
find the faces and that's one of

00:14:09.616 --> 00:14:10.996
the common mistakes that usually

00:14:10.996 --> 00:14:11.386
happens.

00:14:12.906 --> 00:14:14.396
So, I promised to go over the

00:14:14.396 --> 00:14:14.846
types.

00:14:15.796 --> 00:14:16.696
When you want to do something

00:14:16.696 --> 00:14:17.866
streaming we want to use the

00:14:17.866 --> 00:14:18.746
CVPixelBuffer.

00:14:19.906 --> 00:14:21.956
When you create a VideoDataOut

00:14:21.956 --> 00:14:23.206
[inaudible] capture you will get

00:14:23.286 --> 00:14:24.776
CMSampleBuffers and through

00:14:24.916 --> 00:14:25.806
those we get your

00:14:25.866 --> 00:14:26.686
CVPixelBuffers.

00:14:27.856 --> 00:14:29.456
It's also a pretty good format

00:14:29.456 --> 00:14:30.886
if you already have something

00:14:30.886 --> 00:14:32.806
where you keep your image data

00:14:32.806 --> 00:14:34.376
raw in memory like it's LGB

00:14:34.376 --> 00:14:35.936
pixels and wrap them into a

00:14:35.936 --> 00:14:37.276
CVPixelBuffer this is a great

00:14:37.276 --> 00:14:38.496
format to pass into Vision.

00:14:40.666 --> 00:14:41.976
When you get files from disk

00:14:42.146 --> 00:14:44.046
please use the URL or if it

00:14:44.046 --> 00:14:44.996
comes from the web use the

00:14:44.996 --> 00:14:46.926
NSData path.

00:14:47.126 --> 00:14:48.886
The great thing about that is it

00:14:48.886 --> 00:14:50.796
really allows us to reduce the

00:14:50.796 --> 00:14:51.936
memory for print in your

00:14:51.936 --> 00:14:52.616
application.

00:14:53.086 --> 00:14:54.566
Vision will only read what it

00:14:54.566 --> 00:14:56.236
needs to perform the task.

00:14:57.156 --> 00:14:58.256
If you think about you want to

00:14:58.256 --> 00:14:59.226
do face detection on a

00:14:59.226 --> 00:15:02.376
64-megapixel panorama Vision

00:15:02.376 --> 00:15:03.526
will actually reduce your memory

00:15:03.526 --> 00:15:04.936
for it, but not reading the full

00:15:04.936 --> 00:15:06.436
file actually into the memory

00:15:06.436 --> 00:15:07.466
and that is an important thing

00:15:07.466 --> 00:15:08.136
to keep in mind.

00:15:10.396 --> 00:15:12.006
We will read in this case the

00:15:12.006 --> 00:15:13.336
EXIF Orientation out of the

00:15:13.336 --> 00:15:15.336
file, but you can override it if

00:15:15.336 --> 00:15:17.266
you have to for those formats

00:15:17.266 --> 00:15:18.706
that don't support it.

00:15:20.666 --> 00:15:22.126
If you're already using Core

00:15:22.166 --> 00:15:24.076
Image in your application by all

00:15:24.076 --> 00:15:25.286
means process the CI image.

00:15:25.286 --> 00:15:27.366
This is also important when you

00:15:27.366 --> 00:15:28.096
want to actually do some

00:15:28.096 --> 00:15:28.806
preprocessing.

00:15:28.806 --> 00:15:29.786
If you have some domain

00:15:29.786 --> 00:15:30.946
knowledge of what you want to do

00:15:30.946 --> 00:15:33.286
in your Computer Vision task you

00:15:33.286 --> 00:15:34.506
can do some preprocessing and

00:15:34.506 --> 00:15:35.766
try and enhance the image and,

00:15:35.806 --> 00:15:37.186
therefore, enhance the Vision

00:15:37.186 --> 00:15:37.716
results.

00:15:39.346 --> 00:15:40.316
If you want to learn a bit more

00:15:40.316 --> 00:15:42.706
about Core Image, there's a

00:15:42.706 --> 00:15:45.006
session on Thursday at 1:50 and

00:15:45.396 --> 00:15:46.146
they will also show the

00:15:46.146 --> 00:15:47.836
integration with our Vision

00:15:47.836 --> 00:15:48.286
framework.

00:15:48.756 --> 00:15:52.276
Last but not least, if you have

00:15:52.276 --> 00:15:54.246
all the images in your UI you

00:15:54.426 --> 00:15:56.346
can use the CG image [inaudible]

00:15:56.716 --> 00:15:59.116
out of the NS image or the UI

00:15:59.226 --> 00:16:01.196
images let's say it comes from

00:16:01.196 --> 00:16:02.906
the UI image picker and pass

00:16:02.976 --> 00:16:03.686
those into Vision.

00:16:03.806 --> 00:16:07.006
Now what am I going to do with

00:16:07.096 --> 00:16:08.496
the image and that's where we

00:16:08.496 --> 00:16:10.236
have to decide if I want to do

00:16:10.236 --> 00:16:11.346
something interactive with the

00:16:11.426 --> 00:16:13.206
image in that case I use my

00:16:13.206 --> 00:16:14.336
ImageRequestHandler.

00:16:14.606 --> 00:16:16.226
It will hold on to the image for

00:16:16.226 --> 00:16:17.926
the time and I can do multiple

00:16:17.926 --> 00:16:19.656
passes on that image and get the

00:16:19.656 --> 00:16:20.866
best results out of that.

00:16:22.126 --> 00:16:23.426
Now the CVPixelBuffer

00:16:23.426 --> 00:16:24.926
technically will allow you that

00:16:24.926 --> 00:16:26.826
you could change the pixels

00:16:27.226 --> 00:16:28.216
[inaudible], but we see them as

00:16:28.216 --> 00:16:29.756
immutable so don't do that

00:16:29.756 --> 00:16:30.776
because we'll get some strange

00:16:30.776 --> 00:16:31.246
results.

00:16:31.546 --> 00:16:35.406
Next, if you want to track

00:16:35.606 --> 00:16:36.816
something we use the

00:16:36.816 --> 00:16:38.156
SequenceRequestHandler.

00:16:39.636 --> 00:16:40.506
It allows us to keep the

00:16:40.506 --> 00:16:42.946
tracking state and lifecycle of

00:16:42.946 --> 00:16:44.676
my image is not tied to those

00:16:44.736 --> 00:16:46.256
requests handler anymore, but

00:16:46.296 --> 00:16:47.576
just how long it needs it for

00:16:47.576 --> 00:16:47.976
the tracking.

00:16:52.176 --> 00:16:53.976
Performance, so these Vision

00:16:53.976 --> 00:16:55.796
tasks are computationally

00:16:55.796 --> 00:16:57.726
intensive very often and they do

00:16:57.726 --> 00:16:59.056
take time, so you have to think

00:16:59.056 --> 00:17:01.486
about that you want to actually

00:17:01.486 --> 00:17:03.376
run your task on a different

00:17:03.856 --> 00:17:05.465
queue not your main queue.

00:17:06.616 --> 00:17:08.296
And think about if you want to

00:17:08.296 --> 00:17:09.376
do it in the background, which

00:17:09.376 --> 00:17:11.086
is a bit slower or if you need

00:17:11.086 --> 00:17:12.356
it very quickly use a more

00:17:12.356 --> 00:17:13.915
interactive quality of service

00:17:14.366 --> 00:17:15.516
to get the performance.

00:17:16.596 --> 00:17:19.276
A good practice is to use the

00:17:19.276 --> 00:17:22.026
completion handler to get the

00:17:22.056 --> 00:17:23.606
results back, this is part of

00:17:23.606 --> 00:17:24.026
our API.

00:17:24.026 --> 00:17:26.165
But keep in mind that this

00:17:26.215 --> 00:17:27.866
completion handler gets called

00:17:27.915 --> 00:17:29.236
on that queue in which you

00:17:29.236 --> 00:17:30.686
actually set it off.

00:17:30.686 --> 00:17:32.576
So, if you need to update your

00:17:32.576 --> 00:17:33.936
UI you have to dispatch that

00:17:33.936 --> 00:17:34.746
back to the main queue.

00:17:35.426 --> 00:17:38.796
So as Brett already highlighted,

00:17:38.796 --> 00:17:40.036
we have a new face detection and

00:17:40.036 --> 00:17:41.206
you might say oh God, yet

00:17:41.206 --> 00:17:41.706
another one.

00:17:43.946 --> 00:17:45.156
But we have good reasons for

00:17:45.156 --> 00:17:45.446
this.

00:17:45.706 --> 00:17:47.546
Vision uses deep learning and

00:17:47.546 --> 00:17:48.976
this gives us really a lot

00:17:49.186 --> 00:17:50.516
better precision and recall,

00:17:50.926 --> 00:17:52.366
therefore, much better results.

00:17:52.916 --> 00:17:55.826
The downside of it, on older

00:17:55.826 --> 00:17:56.976
hardware it will run a bit

00:17:56.976 --> 00:17:57.356
slower.

00:17:57.436 --> 00:17:58.946
So, let's look a little bit at

00:17:58.946 --> 00:18:01.006
our overall landscape of face

00:18:01.006 --> 00:18:01.926
detectors that we have

00:18:01.926 --> 00:18:02.476
available.

00:18:03.236 --> 00:18:04.876
So, we have Vision which really

00:18:04.876 --> 00:18:06.346
gives us the best results and

00:18:06.346 --> 00:18:08.736
it's pretty fast and also pretty

00:18:08.736 --> 00:18:09.966
good in its power use as it is

00:18:09.966 --> 00:18:10.926
optimized for that.

00:18:11.246 --> 00:18:12.766
And we have it available on all

00:18:12.766 --> 00:18:14.376
platforms except the watchOS.

00:18:15.446 --> 00:18:16.886
And this is the same in terms of

00:18:16.886 --> 00:18:18.566
availability for Core Image and

00:18:18.566 --> 00:18:19.686
it's a bit faster, but the

00:18:19.686 --> 00:18:21.106
results are not quite as good.

00:18:21.736 --> 00:18:24.266
In the AV capture session which

00:18:24.266 --> 00:18:25.536
is only happening during the

00:18:25.536 --> 00:18:27.016
capture side we can actually use

00:18:27.016 --> 00:18:28.436
hardware so it's really fast in

00:18:28.436 --> 00:18:30.176
performance, but the results

00:18:30.176 --> 00:18:31.666
again are not as good as we get

00:18:31.666 --> 00:18:31.976
out of Vision.

00:18:32.466 --> 00:18:34.006
So, you have to choose depending

00:18:34.006 --> 00:18:35.286
on your application what you

00:18:35.286 --> 00:18:36.996
want to do choose the right

00:18:36.996 --> 00:18:38.146
technology for the face

00:18:38.146 --> 00:18:38.566
detection.

00:18:40.406 --> 00:18:41.446
Now I did mention that our

00:18:41.446 --> 00:18:43.056
quality is better, so let me try

00:18:43.056 --> 00:18:44.096
to prove that a little bit.

00:18:44.576 --> 00:18:46.796
So, I have here an image and I

00:18:46.796 --> 00:18:47.896
ran the face detection through

00:18:47.896 --> 00:18:48.506
Core Image.

00:18:48.976 --> 00:18:50.956
And we find two faces and we see

00:18:50.956 --> 00:18:53.056
roughly where the eyes and where

00:18:53.656 --> 00:18:55.126
the mouth are.

00:18:55.396 --> 00:18:56.986
Now in Vision we find all four

00:18:56.986 --> 00:18:58.496
faces even the occluded ones and

00:18:58.496 --> 00:18:59.986
we get a whole lot more details

00:19:00.036 --> 00:19:03.196
with the visual landmarks.

00:19:04.356 --> 00:19:05.486
Speaking of Core Image, I would

00:19:05.486 --> 00:19:06.426
like to highlight a little bit

00:19:06.426 --> 00:19:07.306
what's happening with the

00:19:07.386 --> 00:19:08.046
CIDetectors.

00:19:08.076 --> 00:19:11.456
So, whoever uses it already can

00:19:11.456 --> 00:19:12.536
keep on using them they are

00:19:12.536 --> 00:19:15.596
still in Core Image, but all new

00:19:15.596 --> 00:19:17.136
parts and all the improvements

00:19:17.136 --> 00:19:18.296
in terms of algorithms for

00:19:18.296 --> 00:19:20.216
computer moving forward will be

00:19:20.216 --> 00:19:22.046
in Vision that's the new home

00:19:22.046 --> 00:19:22.946
for Computer Vision.

00:19:23.546 --> 00:19:28.126
So, an awful lot of sides, how

00:19:28.126 --> 00:19:28.656
about a demo.

00:19:29.606 --> 00:19:30.826
So, what I'm going to show you

00:19:30.826 --> 00:19:33.756
is an application that runs an

00:19:33.756 --> 00:19:35.726
AV capture session on the device

00:19:35.996 --> 00:19:37.256
if the demo Gods are with us.

00:19:37.896 --> 00:19:39.936
And we will do a very simple

00:19:39.936 --> 00:19:41.296
rectangle detection request.

00:19:42.376 --> 00:19:43.916
So, what do I have to do to set

00:19:43.916 --> 00:19:44.296
this up?

00:19:46.156 --> 00:19:48.386
What you see here is I create my

00:19:48.386 --> 00:19:52.136
request, that's my simple

00:19:52.136 --> 00:19:54.516
rectangle detection request in

00:19:55.886 --> 00:19:56.816
this case.

00:19:56.996 --> 00:19:58.376
I'm actually in the wrong sample

00:19:58.376 --> 00:19:59.416
that's why I'm getting confused

00:19:59.416 --> 00:19:59.826
here, my apologies.

00:20:00.356 --> 00:20:02.516
Here we go.

00:20:02.546 --> 00:20:03.826
Okay we have our rectangle

00:20:03.826 --> 00:20:05.956
detection request and I'm

00:20:05.956 --> 00:20:07.406
setting some parameters just as

00:20:07.406 --> 00:20:08.856
an example here, I only want

00:20:08.856 --> 00:20:10.626
them this minimum size in our

00:20:10.626 --> 00:20:11.946
coordinates are normalized so I

00:20:11.946 --> 00:20:13.796
only want a 10% minimize size of

00:20:13.796 --> 00:20:16.666
the image and I just want 20

00:20:16.666 --> 00:20:17.206
rectangles.

00:20:17.436 --> 00:20:19.036
I could get more, but I want 20,

00:20:19.036 --> 00:20:20.156
I just picked a number.

00:20:21.386 --> 00:20:23.086
I set up my area of the request

00:20:23.116 --> 00:20:25.986
that I want to perform and the

00:20:26.236 --> 00:20:28.506
right here this is our

00:20:28.676 --> 00:20:30.616
completion handler and all that

00:20:30.616 --> 00:20:32.326
I'm going to do is I'm going to

00:20:32.326 --> 00:20:34.406
draw my rectangles, but as you

00:20:34.406 --> 00:20:35.816
notice I'm just patching it to

00:20:35.816 --> 00:20:37.826
the main queue to update our UI.

00:20:39.176 --> 00:20:40.216
Where do our images come from?

00:20:40.216 --> 00:20:41.736
So, we look at the capture

00:20:41.736 --> 00:20:45.526
output here and as I promised,

00:20:45.666 --> 00:20:47.006
in the capture output we get our

00:20:47.006 --> 00:20:48.566
pixelBuffer from the

00:20:48.566 --> 00:20:49.606
CMSampleBuffer.

00:20:50.376 --> 00:20:54.276
Right here I'm getting the

00:20:54.276 --> 00:20:55.166
cameraIntrinsics.

00:20:55.166 --> 00:20:56.496
Now this is something that is

00:20:56.596 --> 00:20:57.606
important in some of these

00:20:57.656 --> 00:20:58.816
Computer Vision paths where we

00:20:58.816 --> 00:21:00.126
actually know what the camera is

00:21:00.126 --> 00:21:00.866
kind of looking at.

00:21:02.406 --> 00:21:04.206
As I mentioned, we don't forget

00:21:04.206 --> 00:21:06.446
the acts of orientation and I

00:21:06.446 --> 00:21:07.876
create an image request handler

00:21:08.026 --> 00:21:09.696
and perform our tasks.

00:21:09.696 --> 00:21:10.636
So, how does this look when we

00:21:10.636 --> 00:21:11.886
actually run it?

00:21:12.156 --> 00:21:13.006
All right, so what we're going

00:21:13.006 --> 00:21:14.296
to see here is that now we

00:21:14.586 --> 00:21:16.516
tracked this rectangle and

00:21:16.516 --> 00:21:17.906
that's as simple as it is, we

00:21:17.906 --> 00:21:19.386
can find other rectangles.

00:21:20.896 --> 00:21:22.036
If the cable is long enough we

00:21:22.036 --> 00:21:23.076
can actually look oh, there we

00:21:23.076 --> 00:21:24.406
find a computer with various

00:21:24.406 --> 00:21:24.906
rectangles.

00:21:25.516 --> 00:21:27.976
Now I chose the yellow kind of

00:21:27.976 --> 00:21:29.666
on purpose because it's the same

00:21:29.666 --> 00:21:31.456
color as you saw in the demo

00:21:31.726 --> 00:21:33.246
during the keynote for the new

00:21:33.246 --> 00:21:34.516
document camera on notes.

00:21:34.576 --> 00:21:36.536
And I borrowed their color

00:21:36.536 --> 00:21:37.706
because they borrowed our code

00:21:37.706 --> 00:21:38.686
to do actually the rectangle

00:21:38.686 --> 00:21:38.976
detection.

00:21:39.516 --> 00:21:44.636
[ Applause ]

00:21:45.136 --> 00:21:45.876
Thank you.

00:21:46.016 --> 00:21:48.000
[ Applause ]

00:21:51.046 --> 00:21:52.516
Now that was simple, let's do a

00:21:52.516 --> 00:21:52.916
bit more.

00:21:56.396 --> 00:21:58.746
So, how about we throw some

00:21:58.746 --> 00:22:00.096
machine learning at this as well

00:22:00.096 --> 00:22:01.286
just for the fun of it.

00:22:01.956 --> 00:22:03.776
So, what I have to do is I have

00:22:03.776 --> 00:22:06.136
a little model that I just

00:22:06.196 --> 00:22:07.566
dragged into my project here.

00:22:14.156 --> 00:22:16.456
And that is a classifier that

00:22:16.456 --> 00:22:17.576
will tell us a bit something

00:22:17.576 --> 00:22:18.276
about the image.

00:22:19.456 --> 00:22:21.726
And we see when we look at this

00:22:21.726 --> 00:22:25.536
part here that we need to feed

00:22:25.536 --> 00:22:27.116
it an image of a very strange

00:22:27.116 --> 00:22:30.556
size and get out of it some

00:22:30.556 --> 00:22:31.416
classification.

00:22:32.496 --> 00:22:33.596
Now you don't need to worry

00:22:33.596 --> 00:22:35.886
about that size because Vision

00:22:35.886 --> 00:22:37.006
will do the work for you.

00:22:37.006 --> 00:22:43.026
So, what do I need to do?

00:22:43.626 --> 00:22:46.076
I first need to create a Vision

00:22:46.076 --> 00:22:48.226
model and my request with that.

00:22:48.286 --> 00:22:50.326
And that is the part that we

00:22:50.326 --> 00:22:52.736
have here, so I'm simply loading

00:22:52.736 --> 00:22:56.166
the inception model and I create

00:22:56.166 --> 00:22:57.456
my classification request.

00:22:58.726 --> 00:22:59.476
Now it tells me there's

00:22:59.476 --> 00:23:00.696
something missing and I will get

00:23:00.696 --> 00:23:01.706
to that in just a moment.

00:23:01.996 --> 00:23:03.256
The last thing I want to

00:23:03.256 --> 00:23:05.006
highlight here is it says that

00:23:05.366 --> 00:23:06.956
it was okay square image, but

00:23:06.956 --> 00:23:09.446
our cameras don't see squares so

00:23:09.446 --> 00:23:10.786
I need to tell it actually how

00:23:10.786 --> 00:23:12.586
to handle just, you know, the

00:23:12.586 --> 00:23:14.136
aspect ratio that I want to use.

00:23:14.136 --> 00:23:15.636
And I say okay I want to just

00:23:15.636 --> 00:23:15.976
send a crop.

00:23:20.276 --> 00:23:22.636
So, I need a completion handler

00:23:22.636 --> 00:23:26.246
for my task and I have that

00:23:26.246 --> 00:23:28.336
already pre canned here as well.

00:23:30.496 --> 00:23:31.806
So, in this completion handler I

00:23:31.806 --> 00:23:33.296
simply look at my observation

00:23:33.296 --> 00:23:34.656
and I will get so this

00:23:34.696 --> 00:23:36.216
classifier can see a thousand

00:23:36.216 --> 00:23:37.876
different things and I don't

00:23:37.876 --> 00:23:39.206
want to show all of them I only

00:23:39.376 --> 00:23:40.816
show the ones that I care about.

00:23:40.816 --> 00:23:42.576
So, what I'm doing is a little

00:23:42.576 --> 00:23:43.516
bit of filtering, I only take

00:23:43.516 --> 00:23:46.216
the top four and I only look at

00:23:46.216 --> 00:23:47.706
the ones that have a confidence

00:23:47.706 --> 00:23:49.676
of at least 30%, it just works

00:23:49.676 --> 00:23:50.946
well for my demo here, but you

00:23:50.946 --> 00:23:52.126
know you will figure out what

00:23:52.126 --> 00:23:53.356
kind of works well for your

00:23:53.356 --> 00:23:53.696
model.

00:23:54.036 --> 00:23:57.286
And all I have to do next is add

00:23:58.016 --> 00:24:01.136
my classification request into

00:24:01.136 --> 00:24:02.516
my area of request and now I

00:24:02.516 --> 00:24:05.346
will actually run two requests.

00:24:06.026 --> 00:24:07.416
So, I have this already loaded

00:24:07.416 --> 00:24:09.886
on my device, let's see how this

00:24:09.886 --> 00:24:11.196
actually looks.

00:24:12.156 --> 00:24:13.526
Of course, you will see it when

00:24:13.526 --> 00:24:14.826
I switch to the correct machine,

00:24:15.386 --> 00:24:15.686
there we go.

00:24:22.046 --> 00:24:25.056
Okay, so we have a coffee mug

00:24:25.456 --> 00:24:27.016
which is empty, somebody better

00:24:27.016 --> 00:24:27.966
fill that for me.

00:24:27.966 --> 00:24:33.066
We have a ballpoint pen, we have

00:24:33.066 --> 00:24:37.616
a padlock and look an iPod.

00:24:38.396 --> 00:24:44.076
Who has stolen those empty cards

00:24:44.076 --> 00:24:45.436
away and didn't realize it was

00:24:45.436 --> 00:24:45.976
an iPod?

00:24:46.516 --> 00:24:49.500
[ Applause ]

00:24:53.056 --> 00:24:54.676
All right, let's go back to the

00:24:54.676 --> 00:24:58.096
slides before we get to the next

00:24:58.096 --> 00:24:59.406
show-and-tell.

00:25:00.456 --> 00:25:01.846
For my next demo, I want to do

00:25:01.846 --> 00:25:02.976
something a little bit more

00:25:02.976 --> 00:25:06.516
elaborate and with that I chose

00:25:06.516 --> 00:25:07.246
something that's called

00:25:07.246 --> 00:25:08.006
MNISTVision.

00:25:09.486 --> 00:25:10.876
People in the machine learning

00:25:10.876 --> 00:25:12.296
community have already looked at

00:25:12.296 --> 00:25:13.376
that a little bit more.

00:25:13.566 --> 00:25:15.766
MNIST is a dataset where a bunch

00:25:15.766 --> 00:25:17.076
of government employees and high

00:25:17.076 --> 00:25:18.536
school students wrote numbers

00:25:18.596 --> 00:25:21.086
down and this was marked up and

00:25:21.146 --> 00:25:22.106
people were trained in our

00:25:22.106 --> 00:25:23.166
classifier on that.

00:25:23.796 --> 00:25:25.156
Note this is basically like

00:25:25.156 --> 00:25:26.326
white numbers on black

00:25:26.396 --> 00:25:28.336
background, so I guess they've

00:25:28.336 --> 00:25:30.626
written it with chalk on an old

00:25:30.666 --> 00:25:31.136
blackboard.

00:25:32.516 --> 00:25:34.346
So, in this sample code I'm

00:25:34.346 --> 00:25:35.846
going to show you I want to show

00:25:35.846 --> 00:25:37.356
a few concepts that are kind of

00:25:37.356 --> 00:25:38.646
important like making something

00:25:38.646 --> 00:25:40.606
a bit more elaborate with

00:25:41.276 --> 00:25:41.456
Vision.

00:25:41.606 --> 00:25:43.646
First, we'll spin off model

00:25:43.646 --> 00:25:45.006
requests based on top of each

00:25:45.006 --> 00:25:47.516
other then we use Core Image in

00:25:47.516 --> 00:25:49.566
between to do some image process

00:25:49.566 --> 00:25:51.666
and last but not least, we use

00:25:51.666 --> 00:25:53.246
Core ML again for the machine

00:25:53.246 --> 00:25:53.716
learning part.

00:25:56.666 --> 00:25:57.686
So, how is this going to work?

00:25:58.926 --> 00:26:00.596
We have here an image on which

00:26:00.596 --> 00:26:01.506
we find a sticky note.

00:26:02.106 --> 00:26:04.846
Well we find it by using the

00:26:04.846 --> 00:26:06.796
rectangle detector, there's our

00:26:06.796 --> 00:26:07.326
sticky note.

00:26:07.896 --> 00:26:09.226
Now that is prospectively

00:26:09.226 --> 00:26:10.866
distorted and it's clearly not

00:26:10.926 --> 00:26:12.306
white text on black background.

00:26:13.456 --> 00:26:14.846
So, we use Core Image in the

00:26:14.846 --> 00:26:16.686
next step and we'll actually do

00:26:16.686 --> 00:26:18.046
the perspective correction of it

00:26:18.686 --> 00:26:20.666
and invert the color and enhance

00:26:20.666 --> 00:26:22.086
also the contrast so that we get

00:26:22.086 --> 00:26:23.356
rid this black-and-white image.

00:26:24.896 --> 00:26:26.076
And last but not least, I need

00:26:26.076 --> 00:26:28.826
to run my MNIST classifier on it

00:26:29.016 --> 00:26:30.476
and it should tell me that this

00:26:30.476 --> 00:26:32.256
is the number four and this has

00:26:32.256 --> 00:26:35.096
80% confidence that this is the

00:26:35.096 --> 00:26:35.636
number four.

00:26:35.636 --> 00:26:38.666
Again, let's see how this looks

00:26:38.666 --> 00:26:39.066
in the app.

00:26:41.496 --> 00:26:43.426
So again, I start off as a

00:26:43.426 --> 00:26:45.236
rectangle detector request, it's

00:26:45.236 --> 00:26:46.316
my favorite I know.

00:26:47.886 --> 00:26:48.946
But it's more interesting what

00:26:48.946 --> 00:26:50.246
I'm going to do in the

00:26:50.246 --> 00:26:51.206
completion handler.

00:26:52.426 --> 00:26:53.746
So, I do some validation just to

00:26:53.746 --> 00:26:54.876
make sure that the rectangles

00:26:54.876 --> 00:26:55.746
I'm getting out of it are

00:26:55.746 --> 00:26:56.456
actually okay.

00:26:56.456 --> 00:26:58.656
But the interesting part happens

00:26:58.736 --> 00:26:58.926
here.

00:26:59.596 --> 00:27:01.256
I get the coordinates of the

00:27:01.256 --> 00:27:04.126
corners and feed them into CI to

00:27:04.126 --> 00:27:05.786
use the CIPerspectiveCorrection.

00:27:06.476 --> 00:27:07.636
That allows me to take this

00:27:07.636 --> 00:27:09.226
prospectively distorted image

00:27:09.226 --> 00:27:10.916
and actually bring it upright as

00:27:10.916 --> 00:27:11.966
if I would have the camera

00:27:11.966 --> 00:27:12.506
straight on.

00:27:14.076 --> 00:27:15.936
I use the CIColorControls to

00:27:16.186 --> 00:27:17.456
really bring out the contrast of

00:27:17.506 --> 00:27:19.066
the image to make it kind of

00:27:19.066 --> 00:27:19.546
binarized.

00:27:20.896 --> 00:27:22.246
And as I said, I have to color

00:27:22.246 --> 00:27:23.306
invert it.

00:27:24.976 --> 00:27:26.776
Now the resulting image of that

00:27:26.776 --> 00:27:28.116
I feed into a new request in

00:27:28.116 --> 00:27:28.936
there because we have a new

00:27:28.936 --> 00:27:31.146
image on which I'll run the

00:27:31.146 --> 00:27:32.086
classification.

00:27:32.296 --> 00:27:34.066
So, how does the classification

00:27:34.066 --> 00:27:34.416
look like?

00:27:35.906 --> 00:27:37.446
The classification for that I

00:27:37.446 --> 00:27:39.446
use my endless model which I

00:27:39.446 --> 00:27:41.776
actually have ready and this is

00:27:41.776 --> 00:27:43.046
a small model that I've really

00:27:43.046 --> 00:27:44.866
trained actually on this laptop

00:27:44.866 --> 00:27:45.996
very easily give us a few lines

00:27:45.996 --> 00:27:47.576
of code script and then thanks

00:27:47.576 --> 00:27:49.386
to Core ML I can just drag that

00:27:49.386 --> 00:27:50.706
in and use this very easily.

00:27:51.106 --> 00:27:52.316
So, I have my model here.

00:27:53.766 --> 00:27:55.046
Now again, this one part I would

00:27:55.046 --> 00:27:57.636
like to highlight this, so that

00:27:57.636 --> 00:27:59.436
takes in this case a very small

00:27:59.436 --> 00:28:00.166
grayscale image.

00:28:00.166 --> 00:28:02.636
So, an image 28 by 28 pixels it

00:28:02.636 --> 00:28:03.636
should be able to read these

00:28:03.636 --> 00:28:03.976
numbers.

00:28:12.056 --> 00:28:12.526
So that's where my

00:28:12.526 --> 00:28:14.716
classification is coming from

00:28:15.326 --> 00:28:16.906
and now I need to feed in the

00:28:16.906 --> 00:28:17.216
image.

00:28:17.216 --> 00:28:19.056
So, this sample code has been

00:28:19.056 --> 00:28:20.046
made available also for the

00:28:20.046 --> 00:28:21.506
session to make it easy also to

00:28:21.506 --> 00:28:22.716
run a simulator not running it

00:28:22.716 --> 00:28:24.266
live off of the camera I'm just

00:28:24.266 --> 00:28:25.196
going to use actually the

00:28:25.196 --> 00:28:29.046
UIImagePicker and feed it into

00:28:29.086 --> 00:28:32.306
my VMImageRequestHandler and let

00:28:32.306 --> 00:28:33.526
it just perform the rectangle

00:28:33.526 --> 00:28:33.986
request.

00:28:33.986 --> 00:28:37.136
Now notice I buried the request

00:28:37.136 --> 00:28:38.906
for the classification into the

00:28:38.906 --> 00:28:40.396
completion handler of my

00:28:40.396 --> 00:28:42.146
rectangle detection and that

00:28:42.146 --> 00:28:43.496
allows us to basically cascade

00:28:43.496 --> 00:28:44.966
multiple requests on top of each

00:28:44.966 --> 00:28:45.146
other.

00:28:46.096 --> 00:28:47.546
So, let's try the demo for this.

00:28:50.896 --> 00:28:55.016
Okay, so I have my app here and

00:28:55.196 --> 00:28:56.656
well [inaudible] giveaway.

00:28:59.076 --> 00:29:00.696
Okay, so what I see here is

00:29:00.696 --> 00:29:01.976
again I have my image on the

00:29:01.976 --> 00:29:03.436
top, this was actually the photo

00:29:03.436 --> 00:29:04.346
that I took earlier.

00:29:04.926 --> 00:29:07.476
We see its correctly classifying

00:29:07.476 --> 00:29:10.366
as a number one, it was a really

00:29:10.366 --> 00:29:11.556
high confidence in this case.

00:29:11.556 --> 00:29:12.646
And what you see on the bottom

00:29:12.646 --> 00:29:14.816
is just basically just to

00:29:14.816 --> 00:29:16.126
visualize that I took this

00:29:16.126 --> 00:29:17.296
intermediate image that we

00:29:17.296 --> 00:29:19.676
created in CI and show this as

00:29:19.676 --> 00:29:19.946
well.

00:29:19.946 --> 00:29:21.706
Let's choose another number.

00:29:23.066 --> 00:29:24.316
Yes, this is the number three.

00:29:26.466 --> 00:29:27.756
Can we guess what this number

00:29:27.846 --> 00:29:28.826
is, it's the number four?

00:29:29.626 --> 00:29:30.706
It works correctly.

00:29:32.576 --> 00:29:33.806
All right, thank you.

00:29:34.516 --> 00:29:37.556
[ Applause ]

00:29:38.056 --> 00:29:39.176
Let me go back to our slides.

00:29:41.486 --> 00:29:42.936
So that is our Vision framework.

00:29:43.986 --> 00:29:45.316
Let's capitalize a little bit on

00:29:45.316 --> 00:29:46.466
what we really have seen here.

00:29:47.486 --> 00:29:49.426
So, Vision is a high-level

00:29:49.426 --> 00:29:50.696
framework for Computer Vision

00:29:50.756 --> 00:29:51.816
and it should really make it

00:29:51.816 --> 00:29:53.756
easy for you to use this in your

00:29:53.756 --> 00:29:55.356
applications even if you're not

00:29:55.356 --> 00:29:56.576
a Computer Vision expert.

00:29:57.276 --> 00:29:58.756
We have various detectors and

00:29:58.756 --> 00:30:00.166
there's a whole variety of that

00:30:00.166 --> 00:30:01.946
and they all run through one

00:30:01.946 --> 00:30:03.296
consistent interface which would

00:30:03.336 --> 00:30:04.896
make it very easy to learn that

00:30:04.896 --> 00:30:05.446
set of APIs.

00:30:06.826 --> 00:30:08.596
And last but not least, the

00:30:08.596 --> 00:30:09.886
integration with Core ML.

00:30:10.376 --> 00:30:11.736
By bringing your own custom

00:30:11.736 --> 00:30:13.696
models you can do a lot in your

00:30:13.696 --> 00:30:15.586
application, you can find

00:30:15.586 --> 00:30:17.426
hotdogs and see if they are

00:30:17.426 --> 00:30:18.076
really hotdogs.

00:30:19.416 --> 00:30:20.336
I had to make that joke.

00:30:23.576 --> 00:30:24.606
So, if you want to learn more

00:30:24.606 --> 00:30:26.816
about our session, please go to

00:30:26.816 --> 00:30:28.626
our website and I would

00:30:28.626 --> 00:30:29.746
definitely highlight there are

00:30:29.796 --> 00:30:31.576
some related sessions that you

00:30:31.576 --> 00:30:33.266
should have watched perhaps in

00:30:33.266 --> 00:30:33.706
the past.

00:30:33.706 --> 00:30:34.786
I'll read you the Core ML one,

00:30:34.786 --> 00:30:35.876
but you can find it on our

00:30:35.876 --> 00:30:36.106
website.

00:30:36.106 --> 00:30:39.616
Please come for our get-together

00:30:39.616 --> 00:30:42.146
that we have at 6:30 today, chat

00:30:42.146 --> 00:30:43.416
about what we can do.

00:30:43.896 --> 00:30:45.536
And for the little bit more

00:30:45.536 --> 00:30:46.776
advanced part of Core ML there's

00:30:46.826 --> 00:30:48.886
a session on Thursday, as well

00:30:48.886 --> 00:30:50.256
as we have a session with Core

00:30:50.256 --> 00:30:51.486
Image where they will also do

00:30:51.486 --> 00:30:53.556
some very fancy stuff with Core

00:30:53.556 --> 00:30:54.246
Image and Vision.

00:30:55.826 --> 00:30:57.126
And with that I'd like to thank

00:30:57.126 --> 00:30:58.646
you for coming today and enjoy

00:30:58.646 --> 00:30:59.276
the rest of WWDC.

00:30:59.276 --> 00:30:59.456
Thank you.

00:31:00.516 --> 00:31:06.770
[ Applause ]