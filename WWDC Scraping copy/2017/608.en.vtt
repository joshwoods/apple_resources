WEBVTT

00:00:28.596 --> 00:00:29.576
>> Good afternoon everyone,

00:00:30.076 --> 00:00:32.006
welcome to our talk on using

00:00:32.006 --> 00:00:32.946
Metal 2 for Compute.

00:00:33.886 --> 00:00:35.296
My name is Anna Tikhonova.

00:00:35.296 --> 00:00:36.686
I'm an engineer on the GPU

00:00:36.686 --> 00:00:37.976
Software Team, so let's begin.

00:00:42.156 --> 00:00:44.046
The Metal 2 echo system is so

00:00:44.046 --> 00:00:45.836
much more than the Metal API and

00:00:45.836 --> 00:00:46.356
the language.

00:00:46.796 --> 00:00:48.946
We also have the GPU Tools and

00:00:48.946 --> 00:00:50.336
we have the MetalKit and Metal

00:00:50.336 --> 00:00:51.586
Performance Shaders frameworks.

00:00:53.006 --> 00:00:54.146
You might know Metal as this

00:00:54.496 --> 00:00:56.376
great technology for developing

00:00:56.376 --> 00:00:57.566
high-end games and graphics.

00:00:58.396 --> 00:00:59.616
But it can also be used for

00:00:59.616 --> 00:01:00.506
Compute processing.

00:01:01.626 --> 00:01:02.806
In fact, the Compute side of

00:01:02.806 --> 00:01:04.616
Metal is so powerful and

00:01:04.616 --> 00:01:06.526
flexible that the Metal

00:01:06.526 --> 00:01:08.196
Performance Shaders framework is

00:01:08.196 --> 00:01:09.326
built completely on top of

00:01:09.406 --> 00:01:09.756
Compute.

00:01:11.026 --> 00:01:12.486
And in this session, we'll talk

00:01:12.486 --> 00:01:14.076
about what's new in the Metal

00:01:14.076 --> 00:01:15.176
Performance Shaders framework.

00:01:17.956 --> 00:01:19.596
We introduced the Metal

00:01:19.596 --> 00:01:21.026
Performers Shaders framework, or

00:01:21.026 --> 00:01:22.756
MPS in 2015.

00:01:23.486 --> 00:01:24.546
And the videos of our past

00:01:24.546 --> 00:01:25.916
sessions are available on our

00:01:25.916 --> 00:01:28.906
developer website.

00:01:29.266 --> 00:01:30.686
MPS uses the compute power of

00:01:30.686 --> 00:01:33.446
the GPU to bring GPU accelerated

00:01:33.446 --> 00:01:33.816
primitives.

00:01:34.286 --> 00:01:35.886
For image processing, linear

00:01:35.926 --> 00:01:37.416
algebra and machine learning.

00:01:39.146 --> 00:01:40.416
The framework is optimized for

00:01:40.416 --> 00:01:42.336
iOS and we're happy to announce

00:01:42.336 --> 00:01:44.096
that this year we're also

00:01:44.096 --> 00:01:44.836
bringing MPS to the Mac.

00:01:45.516 --> 00:01:49.786
[ Applause ]

00:01:50.286 --> 00:01:50.716
Thank you.

00:01:51.926 --> 00:01:53.366
The entire feature set is

00:01:53.366 --> 00:01:55.616
available in both iOS and macOS.

00:01:55.616 --> 00:01:58.476
So let's begin with a quick

00:01:58.476 --> 00:02:00.336
update on our image processing

00:02:00.336 --> 00:02:00.686
support.

00:02:02.046 --> 00:02:03.866
So here's a list of all of the

00:02:03.866 --> 00:02:05.576
primitives for image processing

00:02:05.696 --> 00:02:07.486
that we had available in iOS 10.

00:02:08.106 --> 00:02:09.675
So there's Convolution, Gaussian

00:02:09.675 --> 00:02:11.586
Blur, Lanczos Resampling, just

00:02:11.586 --> 00:02:12.196
to name a few.

00:02:13.126 --> 00:02:14.726
They're all now available in

00:02:14.726 --> 00:02:14.996
macOS.

00:02:16.146 --> 00:02:17.656
And this year we're bringing you

00:02:17.656 --> 00:02:18.926
four new image processing

00:02:18.926 --> 00:02:19.336
primitives.

00:02:20.466 --> 00:02:21.816
The Image Keypoints primitive

00:02:22.206 --> 00:02:24.316
can be used -- is often used in

00:02:24.316 --> 00:02:26.256
computer vision algorithms such

00:02:26.256 --> 00:02:28.596
as image stabilization and

00:02:28.596 --> 00:02:29.926
Bilinear Rescale, Image

00:02:29.926 --> 00:02:31.726
Statistics, and Element-wise

00:02:31.726 --> 00:02:33.246
Arithmetic Operators, are

00:02:33.326 --> 00:02:34.826
commonly used to pre-process

00:02:34.826 --> 00:02:35.156
images.

00:02:35.466 --> 00:02:36.386
For example, in machine

00:02:36.386 --> 00:02:36.656
learning.

00:02:37.556 --> 00:02:38.926
And the arithmetic filters also

00:02:38.926 --> 00:02:40.446
support broadcasting operations.

00:02:41.256 --> 00:02:42.716
Which, for example, allow you to

00:02:42.716 --> 00:02:44.766
add a 2D image or the 1D image.

00:02:46.176 --> 00:02:48.406
So that's it for our very quick

00:02:48.406 --> 00:02:49.606
update on image processing.

00:02:49.986 --> 00:02:51.286
And now let's talk about the new

00:02:51.286 --> 00:02:52.386
Linear Algebra operations.

00:02:54.286 --> 00:02:55.686
Without support, Matrix

00:02:55.686 --> 00:02:57.436
Multiplication, Matrix Vector

00:02:57.436 --> 00:02:59.736
Multiplication, and Triangular

00:03:00.076 --> 00:03:01.866
Matrix Factorization and Linear

00:03:01.866 --> 00:03:02.306
Solvers.

00:03:05.356 --> 00:03:06.376
To support Linear Algebra

00:03:06.376 --> 00:03:09.256
operations, we now have multiple

00:03:09.256 --> 00:03:10.356
new data representations.

00:03:11.066 --> 00:03:13.076
First, we have the MPSVector

00:03:13.076 --> 00:03:15.186
object which interprets the data

00:03:15.186 --> 00:03:16.496
in a metal buffer as a

00:03:16.496 --> 00:03:17.416
one-dimensional array.

00:03:19.106 --> 00:03:21.506
And we have an MPSMatrix object

00:03:22.076 --> 00:03:23.276
which interprets the data in a

00:03:23.276 --> 00:03:24.886
metal buffer as a rectangular

00:03:24.886 --> 00:03:25.156
array.

00:03:25.886 --> 00:03:27.656
And MPS matrices are in role

00:03:27.656 --> 00:03:28.176
major order.

00:03:28.956 --> 00:03:30.166
And you can think of both

00:03:30.166 --> 00:03:32.956
MPSVectors and MPSMatrices as

00:03:33.456 --> 00:03:34.736
wrappers around user data

00:03:34.736 --> 00:03:35.096
buffers.

00:03:37.436 --> 00:03:39.296
And we also support a temporary

00:03:39.296 --> 00:03:40.926
variance of MPSMatrix.

00:03:42.296 --> 00:03:45.196
MPS images -- temporary images

00:03:45.196 --> 00:03:47.056
and MPSTemporaryMatrices are

00:03:47.056 --> 00:03:48.666
allocated from a Metal heap

00:03:48.906 --> 00:03:49.956
associated with a command

00:03:49.956 --> 00:03:50.216
buffer.

00:03:50.766 --> 00:03:51.856
And they are called temporary

00:03:52.226 --> 00:03:54.066
because their lifespan is

00:03:54.216 --> 00:03:55.996
limited to the lifetime of the

00:03:55.996 --> 00:03:56.516
command buffer.

00:03:57.546 --> 00:03:58.866
And we recommend you use

00:03:58.896 --> 00:04:00.176
temporary images and matrices

00:04:00.636 --> 00:04:02.526
for most of your intermediate

00:04:03.006 --> 00:04:03.206
storage.

00:04:04.316 --> 00:04:07.416
Both MPSVector and MPSMatrix

00:04:07.576 --> 00:04:09.116
support a number of input types.

00:04:09.646 --> 00:04:11.596
We support single-precision and

00:04:11.596 --> 00:04:14.236
half-precision input types and a

00:04:14.236 --> 00:04:15.266
floating-point input types.

00:04:15.756 --> 00:04:17.696
And 16-bits and 8-bit signed

00:04:17.986 --> 00:04:19.125
integer input types.

00:04:21.016 --> 00:04:22.136
And now let's take a look at how

00:04:22.136 --> 00:04:24.446
we can create an MPSVector of

00:04:24.536 --> 00:04:24.886
size N.

00:04:24.886 --> 00:04:26.856
So if you don't already have a

00:04:26.856 --> 00:04:28.346
Metal buffer, you need to create

00:04:28.346 --> 00:04:28.606
one.

00:04:29.666 --> 00:04:30.666
And then you need to create a

00:04:30.666 --> 00:04:31.686
descriptor for your vector.

00:04:32.526 --> 00:04:34.546
And note here that you specify

00:04:34.726 --> 00:04:36.086
the length of the vector.

00:04:36.666 --> 00:04:38.146
That's because the vector can be

00:04:38.146 --> 00:04:39.946
made from a portion of the

00:04:39.946 --> 00:04:40.926
original Metal buffer.

00:04:41.716 --> 00:04:43.236
And other related offsets can be

00:04:43.236 --> 00:04:44.556
set in a kernel that will use

00:04:44.556 --> 00:04:45.016
this vector.

00:04:45.996 --> 00:04:47.406
And then the last step is

00:04:47.466 --> 00:04:49.586
creating a vector from the

00:04:49.586 --> 00:04:50.906
buffer with a descriptor.

00:04:52.966 --> 00:04:53.976
And now let's take a look at how

00:04:53.976 --> 00:04:56.256
you can create an MPSMatrix with

00:04:56.326 --> 00:04:57.906
M rows and N columns.

00:04:59.516 --> 00:05:00.906
So it's very similar to the way

00:05:00.906 --> 00:05:02.916
you would create MPSVector, but

00:05:02.916 --> 00:05:04.006
there's just a few things we

00:05:04.006 --> 00:05:04.726
want to mention.

00:05:06.176 --> 00:05:08.256
We provide a convenient API that

00:05:08.256 --> 00:05:09.606
you can use to find the

00:05:09.606 --> 00:05:11.966
recommended bytes per row value

00:05:12.556 --> 00:05:13.756
for sizing your Metal buffers.

00:05:14.666 --> 00:05:15.716
And if you choose to use the

00:05:15.796 --> 00:05:17.246
API, this is how you would

00:05:17.246 --> 00:05:18.816
create a metal buffer with this

00:05:18.816 --> 00:05:19.596
recommended value.

00:05:20.596 --> 00:05:22.106
And using this API is completely

00:05:22.106 --> 00:05:24.416
optional, but recommended for

00:05:24.416 --> 00:05:25.136
better performance.

00:05:25.986 --> 00:05:27.156
And then the rest is simple.

00:05:28.256 --> 00:05:29.356
You create a descriptor for your

00:05:29.356 --> 00:05:30.886
matrix, and then you create a

00:05:30.886 --> 00:05:32.346
matrix with a descriptor.

00:05:34.936 --> 00:05:36.506
And now that we talked about the

00:05:36.506 --> 00:05:37.806
data presentations, now let's

00:05:37.806 --> 00:05:39.046
talk about the primitives.

00:05:39.896 --> 00:05:41.176
So for Matrix-Matrix and

00:05:41.176 --> 00:05:43.136
Matrix-Vector multiplication our

00:05:43.136 --> 00:05:44.586
API is modeled after the

00:05:44.586 --> 00:05:46.036
standard BLAS GEMM and GEMV

00:05:46.036 --> 00:05:46.766
interfaces.

00:05:47.646 --> 00:05:48.846
And for triangular matrix

00:05:48.846 --> 00:05:50.196
vectorization and linear

00:05:50.196 --> 00:05:52.216
solvers, our API is modeled

00:05:52.216 --> 00:05:53.246
after standard LAPACK

00:05:53.246 --> 00:05:54.916
decomposition and solve

00:05:54.916 --> 00:05:55.486
interfaces.

00:05:55.846 --> 00:05:57.036
So if you're familiar with those

00:05:57.036 --> 00:05:59.106
interfaces our API will look

00:05:59.106 --> 00:06:00.526
very familiar to you as well.

00:06:02.576 --> 00:06:04.216
And now let's take a look at a

00:06:04.216 --> 00:06:05.546
very simple code example.

00:06:05.836 --> 00:06:07.536
So we'll be doing matrix

00:06:07.536 --> 00:06:08.996
multiplication and computing

00:06:08.996 --> 00:06:10.426
just C = A times B.

00:06:10.426 --> 00:06:12.716
So first we need to create our

00:06:12.716 --> 00:06:14.476
matrices A, B and C.

00:06:14.706 --> 00:06:15.646
But I know you know how to do

00:06:15.646 --> 00:06:16.786
this, I showed you in a previous

00:06:16.836 --> 00:06:18.306
slide, so let's move on.

00:06:19.336 --> 00:06:21.116
Now we want to run matrix

00:06:21.116 --> 00:06:22.596
multiplication on the GPU.

00:06:23.886 --> 00:06:25.386
So first we do our usual Metal

00:06:25.466 --> 00:06:27.446
setup to getting a device, a

00:06:27.446 --> 00:06:29.076
command queue, and a command

00:06:29.076 --> 00:06:29.356
buffer.

00:06:29.356 --> 00:06:31.766
And then we need to create our

00:06:31.846 --> 00:06:33.186
matrix multiplication kernel.

00:06:33.766 --> 00:06:35.196
And note here that you specify

00:06:35.196 --> 00:06:36.296
the size of the result.

00:06:36.826 --> 00:06:38.216
That's because this kernel can

00:06:38.216 --> 00:06:39.896
operate on subregions of

00:06:39.896 --> 00:06:40.436
matrices.

00:06:41.066 --> 00:06:45.576
And then we encode this kernel

00:06:45.656 --> 00:06:47.056
to the GPU and tell it to start

00:06:47.056 --> 00:06:47.476
doing the work.

00:06:47.476 --> 00:06:51.036
And we already have sample code

00:06:51.036 --> 00:06:52.346
from Matrix multiplication

00:06:52.586 --> 00:06:53.906
available on our developer

00:06:53.906 --> 00:06:56.096
website, and the sample code for

00:06:56.096 --> 00:06:57.896
triangular matrix vectorization

00:06:58.206 --> 00:06:59.556
and solving a system of linear

00:06:59.556 --> 00:07:00.986
equations is coming very soon.

00:07:03.026 --> 00:07:05.526
So that's it for our -- for the

00:07:05.756 --> 00:07:07.026
linear algebra operations.

00:07:07.386 --> 00:07:08.996
Let's now move on to the next

00:07:08.996 --> 00:07:10.756
topic, which is Accelerating

00:07:10.756 --> 00:07:12.156
Machine Learning Primitives on

00:07:12.156 --> 00:07:12.636
the GPU.

00:07:14.116 --> 00:07:16.246
There are a number of

00:07:16.246 --> 00:07:17.906
machine-learning related talks

00:07:17.906 --> 00:07:19.156
at WWDC this year.

00:07:19.456 --> 00:07:20.346
And we are a part of the

00:07:20.346 --> 00:07:21.416
machine-learning community.

00:07:22.306 --> 00:07:23.586
And this slide shows the overall

00:07:23.586 --> 00:07:24.206
architecture.

00:07:25.086 --> 00:07:26.606
So as an application developer,

00:07:26.816 --> 00:07:27.876
you can add machine learning

00:07:27.876 --> 00:07:28.816
functionality to your

00:07:28.816 --> 00:07:30.956
applications by using high-level

00:07:30.996 --> 00:07:32.856
domain-specific frameworks such

00:07:32.856 --> 00:07:34.456
as division framework and the

00:07:34.456 --> 00:07:35.566
natural language processing

00:07:35.616 --> 00:07:37.496
framework, which rely on the

00:07:37.496 --> 00:07:38.366
Core ML framework.

00:07:39.216 --> 00:07:40.376
And the Core ML framework is

00:07:40.446 --> 00:07:42.316
powered by the accelerates

00:07:42.316 --> 00:07:44.076
framework BNNS primitives on the

00:07:44.076 --> 00:07:44.606
CPU.

00:07:45.066 --> 00:07:46.176
And by the machine learning --

00:07:47.066 --> 00:07:49.046
and by the Metal Performance

00:07:49.046 --> 00:07:51.946
Shaders framework on the GPU.

00:07:51.946 --> 00:07:52.706
But if you're writing an

00:07:52.706 --> 00:07:54.556
application that uses Metal,

00:07:54.906 --> 00:07:56.046
then you can use the MPS

00:07:56.046 --> 00:07:58.176
framework directly and I will

00:07:58.176 --> 00:07:59.386
show you how in this session.

00:08:01.666 --> 00:08:02.676
So let's start with what are we

00:08:02.736 --> 00:08:03.486
talking about here?

00:08:04.246 --> 00:08:05.116
What is deep learning?

00:08:05.366 --> 00:08:06.236
What is Machine learning?

00:08:07.686 --> 00:08:08.766
So imagine that this is you.

00:08:08.766 --> 00:08:11.436
And when you see an image, you

00:08:11.436 --> 00:08:13.076
know immediately what's depicted

00:08:13.076 --> 00:08:13.316
on it.

00:08:13.416 --> 00:08:13.956
It's a panda.

00:08:14.936 --> 00:08:16.686
But now think about all of the

00:08:16.686 --> 00:08:18.006
images on your iPhone.

00:08:18.596 --> 00:08:20.206
Or all of those pictures in your

00:08:20.206 --> 00:08:20.996
family albums.

00:08:21.606 --> 00:08:22.646
Or all of the images on the

00:08:22.646 --> 00:08:22.966
internet.

00:08:23.816 --> 00:08:27.096
No human can possibly -- can

00:08:27.096 --> 00:08:28.816
classify these many images.

00:08:29.086 --> 00:08:30.506
But deep-learning algorithms is

00:08:30.506 --> 00:08:32.056
designed specifically to do

00:08:32.056 --> 00:08:32.196
that.

00:08:33.186 --> 00:08:34.416
They can be used for sifting

00:08:34.416 --> 00:08:35.576
through large amounts of data

00:08:36.015 --> 00:08:37.866
and answering questions such as

00:08:37.996 --> 00:08:41.676
what is in this image?

00:08:42.236 --> 00:08:43.306
Deep learning algorithms have

00:08:43.405 --> 00:08:43.905
two phases.

00:08:44.206 --> 00:08:45.156
Training and inference.

00:08:45.426 --> 00:08:46.426
So let's talk about training

00:08:46.426 --> 00:08:46.736
first.

00:08:47.946 --> 00:08:49.246
And let's actually use an

00:08:49.246 --> 00:08:49.676
example.

00:08:49.676 --> 00:08:51.326
Let's train a system to classify

00:08:51.326 --> 00:08:51.716
images.

00:08:52.586 --> 00:08:53.536
So the training system to

00:08:53.536 --> 00:08:56.696
classify images, for example, if

00:08:56.696 --> 00:08:58.066
you want to have it recognize

00:08:58.126 --> 00:08:58.536
animals.

00:08:58.966 --> 00:09:00.516
Like, to have it recognize cats,

00:09:00.516 --> 00:09:02.126
you need to feed this system a

00:09:02.556 --> 00:09:04.196
large number of labeled images

00:09:04.196 --> 00:09:06.126
of cats and then rabbits, and

00:09:06.126 --> 00:09:07.336
all the other animals that you

00:09:07.336 --> 00:09:08.406
want your system to be able to

00:09:08.406 --> 00:09:08.866
recognize.

00:09:10.546 --> 00:09:12.316
And this training step is a

00:09:12.316 --> 00:09:13.916
one-time computationally

00:09:13.916 --> 00:09:16.476
expensive and labor-intensive

00:09:16.566 --> 00:09:16.786
step.

00:09:17.896 --> 00:09:19.076
And it's usually done offline.

00:09:19.696 --> 00:09:20.896
But the results of this training

00:09:20.896 --> 00:09:22.336
phase is trained parameters

00:09:23.306 --> 00:09:24.656
which are required for the next

00:09:24.656 --> 00:09:25.856
phase, the inference phase.

00:09:26.906 --> 00:09:28.096
This is when your system is

00:09:28.186 --> 00:09:30.156
presented with a new image that

00:09:30.156 --> 00:09:31.736
it has never seen before and it

00:09:31.736 --> 00:09:33.186
needs to classify, this is a

00:09:33.186 --> 00:09:33.396
cap.

00:09:35.126 --> 00:09:37.016
We provide view acceleration for

00:09:37.016 --> 00:09:38.306
the second phase; the inference

00:09:38.306 --> 00:09:38.526
phase.

00:09:39.096 --> 00:09:40.966
Specifically, last year we

00:09:40.966 --> 00:09:42.936
talked about the building blocks

00:09:43.056 --> 00:09:44.296
for building convolutional

00:09:44.296 --> 00:09:45.716
neural networks on the GPU for

00:09:45.716 --> 00:09:46.146
inference.

00:09:48.466 --> 00:09:50.176
So before we move onto any of

00:09:50.176 --> 00:09:51.966
the new features for machine

00:09:51.966 --> 00:09:52.826
learning that we brought you

00:09:52.856 --> 00:09:53.936
this year, we are going to

00:09:53.936 --> 00:09:55.136
review some of the core

00:09:55.136 --> 00:09:56.886
information that was covered in

00:09:56.886 --> 00:09:58.146
our last year's presentation.

00:09:58.736 --> 00:10:00.416
Such as, what are convolutional

00:10:00.416 --> 00:10:00.966
neural networks?

00:10:02.396 --> 00:10:04.326
And once we do that then we can

00:10:04.326 --> 00:10:06.056
talk about the new primitives

00:10:06.106 --> 00:10:06.836
that we've added for

00:10:06.836 --> 00:10:07.906
convolutional neural networks

00:10:07.966 --> 00:10:09.426
this year, and then we'll

00:10:09.426 --> 00:10:11.146
introduce the new, easy-to-use

00:10:11.516 --> 00:10:12.636
neural network graph API.

00:10:13.166 --> 00:10:14.746
And our last topic will be

00:10:14.746 --> 00:10:15.726
recurrent neural networks.

00:10:18.606 --> 00:10:20.976
So let's go into our recap.

00:10:21.106 --> 00:10:22.066
So what are convolutional neural

00:10:22.066 --> 00:10:22.366
networks?

00:10:24.446 --> 00:10:25.576
Convolutional neural networks

00:10:25.576 --> 00:10:27.426
are biologically inspired and

00:10:27.426 --> 00:10:28.836
designed to resemble the visual

00:10:28.836 --> 00:10:29.246
cortex.

00:10:29.796 --> 00:10:31.406
So let's think about how our

00:10:31.406 --> 00:10:33.076
brain processes visual inputs.

00:10:34.256 --> 00:10:35.636
The first hierarchy of neurons

00:10:35.736 --> 00:10:37.056
that receive information in the

00:10:37.056 --> 00:10:39.396
visual cortex is sensitive to

00:10:39.396 --> 00:10:40.786
specific edges and blobs of

00:10:40.886 --> 00:10:41.196
color.

00:10:42.366 --> 00:10:43.466
While the brain region's further

00:10:43.466 --> 00:10:45.966
down the visual pipeline respond

00:10:45.966 --> 00:10:47.596
to more complex structures such

00:10:47.596 --> 00:10:49.366
as faces of our friends or kinds

00:10:49.366 --> 00:10:50.166
of animals like cats.

00:10:50.996 --> 00:10:53.646
So in a similar way, CNNs are

00:10:53.646 --> 00:10:55.836
organized into a hierarchy of

00:10:55.836 --> 00:10:58.176
layers where high-level features

00:10:58.296 --> 00:10:59.836
are derived from low-level

00:10:59.836 --> 00:11:00.246
features.

00:11:01.156 --> 00:11:02.516
So the first few layers in your

00:11:02.516 --> 00:11:04.566
network respond to low-level

00:11:04.566 --> 00:11:06.766
features like edges and blobs of

00:11:06.826 --> 00:11:07.196
color.

00:11:07.886 --> 00:11:10.646
While subsequent layers respond

00:11:10.766 --> 00:11:12.516
to progressively more complex

00:11:12.616 --> 00:11:14.886
features such as faces.

00:11:16.106 --> 00:11:17.406
And I keep saying features.

00:11:17.846 --> 00:11:19.556
So think of a feature as a

00:11:19.556 --> 00:11:21.176
filter that filters your input

00:11:21.176 --> 00:11:22.706
data; that particular feature.

00:11:25.316 --> 00:11:26.906
And here's a list of all of the

00:11:26.976 --> 00:11:28.076
CNN primitives that we had

00:11:28.076 --> 00:11:29.356
available in iOS 10.

00:11:29.756 --> 00:11:31.806
And in this recap I will be just

00:11:31.806 --> 00:11:33.686
talking about the core

00:11:34.176 --> 00:11:35.146
convolution layer.

00:11:35.216 --> 00:11:36.426
The core building block of a

00:11:36.546 --> 00:11:36.756
CNN.

00:11:36.756 --> 00:11:39.046
And the rest of these primitives

00:11:39.106 --> 00:11:40.906
are covered in great detail in

00:11:40.906 --> 00:11:42.266
our presentation -- in our

00:11:42.266 --> 00:11:43.076
documentation.

00:11:43.196 --> 00:11:44.566
So Pooling, Fully-Connected and

00:11:44.616 --> 00:11:45.156
SoftMax.

00:11:45.746 --> 00:11:46.856
You can find information on

00:11:46.856 --> 00:11:47.056
those.

00:11:48.626 --> 00:11:50.386
So let's talk about the core

00:11:50.386 --> 00:11:51.186
building block.

00:11:52.596 --> 00:11:54.216
So the function of this core

00:11:54.216 --> 00:11:56.196
convolution layer is to

00:11:56.196 --> 00:11:57.766
recognize features in the input

00:11:57.766 --> 00:11:58.906
data and it's called a

00:11:58.906 --> 00:12:01.076
convolution layer because it

00:12:01.126 --> 00:12:02.576
performs a convolution on its

00:12:02.576 --> 00:12:02.846
input.

00:12:03.916 --> 00:12:05.246
So let's recall how regular

00:12:05.246 --> 00:12:06.076
convolution works.

00:12:06.906 --> 00:12:08.146
You have your inputs, your

00:12:08.186 --> 00:12:09.366
outputs and the filter.

00:12:10.366 --> 00:12:12.386
And to convole a filter with the

00:12:12.386 --> 00:12:14.506
input data you need to multiply

00:12:14.756 --> 00:12:16.626
each value in your filter with

00:12:16.626 --> 00:12:18.446
the value in the input data and

00:12:18.446 --> 00:12:19.686
combine that information to

00:12:19.686 --> 00:12:21.106
compute a single output value.

00:12:22.046 --> 00:12:23.536
And you do the same for the rest

00:12:23.636 --> 00:12:25.166
of the output pixels.

00:12:27.596 --> 00:12:29.856
And now the convolution layer is

00:12:29.856 --> 00:12:31.606
a generalization of regular

00:12:31.606 --> 00:12:32.226
convolution.

00:12:32.396 --> 00:12:34.666
It allows you to have multiple

00:12:34.666 --> 00:12:35.136
filters.

00:12:35.526 --> 00:12:37.536
So you have as many filters as

00:12:37.536 --> 00:12:38.916
you have output channels -- or

00:12:38.916 --> 00:12:39.816
16 in this case.

00:12:41.666 --> 00:12:43.046
And these are the filters which

00:12:43.046 --> 00:12:44.656
are going to be filtering input

00:12:44.656 --> 00:12:46.006
data for particular features.

00:12:47.726 --> 00:12:49.016
Now imagine that you're working

00:12:49.016 --> 00:12:50.266
with RGB data.

00:12:50.356 --> 00:12:52.186
So you actually have three

00:12:52.186 --> 00:12:53.266
channels in your input.

00:12:54.156 --> 00:12:56.276
And just because how CNNs work,

00:12:56.476 --> 00:12:58.816
this means you need three sets

00:12:58.886 --> 00:13:00.156
of 16 filters.

00:13:00.866 --> 00:13:02.576
One set for each input channel.

00:13:03.856 --> 00:13:05.916
And then these filters are

00:13:05.916 --> 00:13:07.296
applied to the input data

00:13:08.486 --> 00:13:09.036
separately.

00:13:09.036 --> 00:13:10.816
And then the final step combines

00:13:10.816 --> 00:13:12.386
all of this information to

00:13:12.386 --> 00:13:13.796
compute a single output pixel.

00:13:15.516 --> 00:13:17.156
So that's it for our recap of

00:13:17.156 --> 00:13:18.006
the convolution layer.

00:13:18.536 --> 00:13:19.526
Now let's talk about the new

00:13:19.576 --> 00:13:20.846
primitives we've added for

00:13:20.846 --> 00:13:22.026
convolutional neural networks.

00:13:22.116 --> 00:13:25.176
So as you can see, we've added

00:13:25.176 --> 00:13:25.686
quite a few.

00:13:27.736 --> 00:13:29.096
And I'll be talking about the

00:13:29.096 --> 00:13:31.476
ones highlighted in yellow, but

00:13:31.476 --> 00:13:33.206
the rest of them like L2Norm

00:13:33.256 --> 00:13:34.686
Pooling, Resampling,

00:13:34.686 --> 00:13:36.086
Up-sampling, they will all be

00:13:36.086 --> 00:13:37.396
covered in our documentation.

00:13:39.286 --> 00:13:40.986
So let's talk about updates to

00:13:40.986 --> 00:13:42.786
our core convolution layer.

00:13:43.916 --> 00:13:45.146
We used to support only single

00:13:45.186 --> 00:13:46.716
precision floating-point weight

00:13:46.716 --> 00:13:47.026
types.

00:13:47.466 --> 00:13:49.076
And now to help you reduce the

00:13:49.076 --> 00:13:51.126
memory footprint and to prove

00:13:51.126 --> 00:13:51.966
the performance of your

00:13:51.966 --> 00:13:52.326
networks.

00:13:52.876 --> 00:13:54.546
We also support half-precision

00:13:54.546 --> 00:13:56.466
floating points, 8-bit integer,

00:13:56.806 --> 00:13:58.076
and binary weight types.

00:13:59.496 --> 00:14:01.006
We used to support only standard

00:14:01.006 --> 00:14:02.676
convolution and now we also

00:14:02.736 --> 00:14:03.926
support binary and XNOR

00:14:03.926 --> 00:14:04.646
convolution.

00:14:04.986 --> 00:14:06.096
Dilated convolution.

00:14:06.096 --> 00:14:08.406
Sub-pixel convolution and

00:14:08.406 --> 00:14:09.366
convolution transpose

00:14:09.366 --> 00:14:09.996
operations.

00:14:11.056 --> 00:14:12.156
And many of these are

00:14:12.156 --> 00:14:13.716
orthogonal, so you can even have

00:14:14.006 --> 00:14:15.946
dilated sub-pixel convolution if

00:14:15.946 --> 00:14:16.276
you want.

00:14:17.706 --> 00:14:18.486
So let's go through them

00:14:18.486 --> 00:14:19.046
one-by-one.

00:14:20.806 --> 00:14:22.216
Binary and XNOR convolution

00:14:22.256 --> 00:14:24.276
perform the same exact operation

00:14:24.306 --> 00:14:26.336
as regular convolution but they

00:14:26.336 --> 00:14:28.016
do so with improved performance

00:14:28.476 --> 00:14:29.566
and great space savings.

00:14:30.016 --> 00:14:31.726
So in regular convolution, you

00:14:31.726 --> 00:14:33.546
may have floating point inputs

00:14:33.846 --> 00:14:35.166
and floating point weights.

00:14:36.036 --> 00:14:37.446
What binary convolution allow

00:14:37.516 --> 00:14:39.236
you to do is to use your

00:14:39.236 --> 00:14:41.266
full-sized input with binary

00:14:41.266 --> 00:14:41.486
weights.

00:14:42.416 --> 00:14:44.776
And for XNOR convolution the

00:14:44.776 --> 00:14:46.706
first thing that happens is that

00:14:47.226 --> 00:14:48.626
your input is first converted to

00:14:48.626 --> 00:14:50.826
binary so that both your inputs

00:14:51.176 --> 00:14:52.396
and the weights are binary.

00:14:53.476 --> 00:14:55.286
In regular convolution, the

00:14:55.286 --> 00:14:57.066
input has to be multiplied with

00:14:57.106 --> 00:14:57.446
the weights.

00:14:57.886 --> 00:14:59.876
And for XNOR convolution the

00:14:59.876 --> 00:15:01.806
separation becomes a simple XNOR

00:15:01.806 --> 00:15:02.336
operation.

00:15:04.746 --> 00:15:06.066
And now let's talk about dilated

00:15:06.066 --> 00:15:06.656
convolution.

00:15:07.626 --> 00:15:08.996
So we already know how regular

00:15:08.996 --> 00:15:09.786
convolution works.

00:15:10.486 --> 00:15:12.396
You need to apply a filter to

00:15:12.396 --> 00:15:13.656
the input data to compute a

00:15:13.656 --> 00:15:14.706
single output value.

00:15:17.526 --> 00:15:18.786
But say you're working on an

00:15:18.786 --> 00:15:21.736
algorithm that requires global

00:15:21.736 --> 00:15:24.846
integration of a wider context

00:15:25.216 --> 00:15:26.166
of your input data.

00:15:26.816 --> 00:15:28.466
So instead of a 3 by 3 kernel,

00:15:28.536 --> 00:15:30.496
you may be using a 5 by 5 kernel

00:15:31.736 --> 00:15:32.476
to look out further.

00:15:33.026 --> 00:15:33.666
But that's a lot more

00:15:33.666 --> 00:15:34.756
computationally expensive.

00:15:35.256 --> 00:15:37.266
What you can do instead is use

00:15:37.266 --> 00:15:39.046
dilated convolutions which

00:15:39.046 --> 00:15:43.206
allows you to -- which allows

00:15:43.206 --> 00:15:45.256
you to use dilation factors to

00:15:45.256 --> 00:15:46.836
introduce gaps into your

00:15:46.836 --> 00:15:48.836
convolution kernel so that

00:15:48.836 --> 00:15:50.576
you're still using just a 3 by 3

00:15:50.576 --> 00:15:52.676
kernel, but you can look out

00:15:52.676 --> 00:15:53.016
further.

00:15:54.996 --> 00:15:55.876
And now let's talk about

00:15:55.946 --> 00:15:57.266
subluxal convolution and

00:15:57.266 --> 00:15:58.846
convolution transpose primitive;

00:15:59.766 --> 00:16:01.266
very commonly used for image

00:16:01.266 --> 00:16:01.836
upscaling.

00:16:03.066 --> 00:16:04.126
And let's think about how

00:16:04.176 --> 00:16:05.366
upscaling usually works.

00:16:05.456 --> 00:16:07.456
So you have your input data and

00:16:07.516 --> 00:16:09.196
you want to upscale it by a

00:16:09.196 --> 00:16:09.956
factor of 2.

00:16:12.336 --> 00:16:13.376
So you won't -- you have some

00:16:13.376 --> 00:16:14.566
missing pixels to compute.

00:16:15.216 --> 00:16:16.536
And usually upscaling is the

00:16:16.536 --> 00:16:18.156
fixed operation with a constant

00:16:18.156 --> 00:16:18.606
filter.

00:16:18.766 --> 00:16:20.336
So for example how would a box

00:16:20.336 --> 00:16:22.336
filter help you to upscale this

00:16:22.386 --> 00:16:22.756
image?

00:16:23.416 --> 00:16:25.146
So the box filter, which is take

00:16:25.316 --> 00:16:28.006
the known pixels and copy the

00:16:28.006 --> 00:16:29.326
known data into the missing

00:16:29.326 --> 00:16:30.786
location to get you upscaled

00:16:30.786 --> 00:16:31.216
results.

00:16:33.326 --> 00:16:35.196
For sub-pixel convolution, your

00:16:35.196 --> 00:16:36.646
filters are not constant.

00:16:36.976 --> 00:16:38.226
Your filters are learned from

00:16:38.226 --> 00:16:38.626
the data.

00:16:38.766 --> 00:16:40.286
They are your trained parameters

00:16:40.716 --> 00:16:41.786
that you get from the training

00:16:41.786 --> 00:16:42.906
step where the system was

00:16:42.986 --> 00:16:45.106
trained to do this task; to do

00:16:45.106 --> 00:16:45.946
image upscaling.

00:16:47.086 --> 00:16:49.176
So for 2x upscaling you get 4

00:16:49.236 --> 00:16:49.686
filters.

00:16:49.806 --> 00:16:51.656
For 4x upscaling you get 16

00:16:51.656 --> 00:16:52.656
filters and so on.

00:16:53.566 --> 00:16:55.446
So for our 2x upscaling we get

00:16:55.446 --> 00:16:57.456
our 4 filters and we apply them

00:16:57.456 --> 00:16:58.166
to the input data.

00:16:58.166 --> 00:17:00.216
And then the output of that

00:17:00.216 --> 00:17:02.076
operation is reshuffled to get

00:17:02.076 --> 00:17:03.476
your final full-resolution

00:17:03.476 --> 00:17:03.856
image.

00:17:04.925 --> 00:17:06.445
And now let's talk about how the

00:17:06.445 --> 00:17:08.076
convolution transpose primitive

00:17:08.156 --> 00:17:09.536
can be used to upscale images.

00:17:10.435 --> 00:17:12.026
So we have our inputs and we

00:17:12.026 --> 00:17:13.465
still have to compute our

00:17:13.465 --> 00:17:14.146
missing data.

00:17:14.945 --> 00:17:16.665
So the way that this primitive

00:17:17.156 --> 00:17:18.616
computes the missing data is

00:17:18.616 --> 00:17:19.586
that it applies a kind of

00:17:19.586 --> 00:17:21.296
convolution pass to this

00:17:21.296 --> 00:17:23.165
intermediate result with gaps to

00:17:23.165 --> 00:17:24.596
compute each output pixel.

00:17:25.185 --> 00:17:27.316
So that's how you get your

00:17:27.356 --> 00:17:28.256
upscaled output.

00:17:31.136 --> 00:17:32.216
And now we're going to show you

00:17:32.216 --> 00:17:33.556
how you can use these new

00:17:33.556 --> 00:17:35.046
convolution primitives to

00:17:35.046 --> 00:17:36.626
implement a real-world network.

00:17:37.096 --> 00:17:38.606
So we took this colorization

00:17:38.606 --> 00:17:41.486
network that takes black and

00:17:41.486 --> 00:17:42.886
white images as input and

00:17:42.886 --> 00:17:44.356
produces colorized images.

00:17:44.356 --> 00:17:47.156
And this particular network uses

00:17:47.236 --> 00:17:48.426
the dilated convolution

00:17:48.426 --> 00:17:50.396
primitive to integrate wider

00:17:50.396 --> 00:17:52.296
global context quicker.

00:17:52.926 --> 00:17:54.756
And it uses the convolution

00:17:54.756 --> 00:17:56.726
transpose primitive to upscale

00:17:56.726 --> 00:17:57.776
the results of the network.

00:18:00.166 --> 00:18:01.276
And now let's look at this

00:18:01.666 --> 00:18:03.966
colorization network in action.

00:18:10.226 --> 00:18:11.466
So in this demo we have a

00:18:11.466 --> 00:18:12.886
collection of black and white

00:18:12.886 --> 00:18:14.046
images like this image of a

00:18:14.046 --> 00:18:14.406
lion.

00:18:15.046 --> 00:18:16.416
And as soon as I tap on this

00:18:16.416 --> 00:18:17.796
image, the colorization network

00:18:17.796 --> 00:18:20.046
will run right here live on the

00:18:20.046 --> 00:18:21.126
device, and we'll see a

00:18:21.126 --> 00:18:21.946
colorized image.

00:18:23.906 --> 00:18:25.456
And let's try another example

00:18:25.456 --> 00:18:27.046
for this beautiful snowy

00:18:27.046 --> 00:18:27.616
mountain.

00:18:28.836 --> 00:18:30.036
And now we see it in color.

00:18:31.586 --> 00:18:33.756
And this beautiful lovely image

00:18:33.756 --> 00:18:35.016
of a dad and a daughter playing

00:18:35.016 --> 00:18:35.366
guitar.

00:18:35.366 --> 00:18:37.386
And now you can see them playing

00:18:37.386 --> 00:18:38.026
guitar in color.

00:18:39.516 --> 00:18:40.896
And I really like this one, the

00:18:40.896 --> 00:18:42.306
brown bear walking in the

00:18:42.356 --> 00:18:42.696
forest.

00:18:42.696 --> 00:18:43.756
So I think this network does

00:18:43.786 --> 00:18:45.146
just a really wonderful job.

00:18:46.886 --> 00:18:48.726
Okay. So that's it for the live

00:18:48.726 --> 00:18:48.916
demo.

00:18:49.516 --> 00:18:54.686
[ Applause ]

00:18:55.186 --> 00:18:55.796
Thank you so much.

00:18:57.766 --> 00:18:59.216
So we've added all of these new

00:18:59.216 --> 00:19:01.456
convolution CNN primitives, but

00:19:01.456 --> 00:19:02.056
that's not all.

00:19:02.976 --> 00:19:04.666
We also went back and improved

00:19:04.666 --> 00:19:06.046
the performance of some of the

00:19:06.206 --> 00:19:07.936
core CNN kernels that were

00:19:07.936 --> 00:19:09.646
available to you in iOS 10.

00:19:10.406 --> 00:19:11.776
So this chart will show the

00:19:11.806 --> 00:19:13.846
performance of the Inception-v3

00:19:13.846 --> 00:19:15.386
network, which is a commonly

00:19:15.386 --> 00:19:16.936
used network for image

00:19:17.056 --> 00:19:17.546
recognition.

00:19:18.756 --> 00:19:20.396
So it shows the performance of

00:19:20.396 --> 00:19:22.196
this network in iOS 11.

00:19:22.196 --> 00:19:23.786
And as you can see, we're

00:19:23.786 --> 00:19:25.866
bringing you at least 20 percent

00:19:25.866 --> 00:19:27.546
performance improvement across

00:19:27.756 --> 00:19:28.696
different iOS hardware.

00:19:30.406 --> 00:19:33.786
And now let's talk about the new

00:19:33.786 --> 00:19:37.096
neural network graph API.

00:19:37.716 --> 00:19:40.036
the neural networks are commonly

00:19:40.036 --> 00:19:41.416
described using a graph

00:19:41.416 --> 00:19:42.476
abstraction like this

00:19:42.476 --> 00:19:43.506
visualization of the

00:19:43.506 --> 00:19:44.616
Inception-v3 network.

00:19:44.616 --> 00:19:46.696
And we're now allowing to do

00:19:46.696 --> 00:19:48.706
just this using the new graph

00:19:48.706 --> 00:19:48.966
API.

00:19:50.446 --> 00:19:51.556
So let's zoom in on one of these

00:19:51.556 --> 00:19:53.186
inception modules.

00:19:54.676 --> 00:19:56.496
You have filter nodes which

00:19:56.496 --> 00:19:58.176
describe the operations that you

00:19:58.176 --> 00:19:59.216
can perform on your data.

00:19:59.526 --> 00:20:01.146
Such as convolution, pooling,

00:20:01.146 --> 00:20:01.566
etcetera.

00:20:02.556 --> 00:20:04.316
And you have image nodes which

00:20:04.316 --> 00:20:05.736
describe how the data flows

00:20:05.786 --> 00:20:06.546
between these different

00:20:06.546 --> 00:20:07.096
operations.

00:20:07.826 --> 00:20:11.306
So why did we add this new graph

00:20:11.306 --> 00:20:11.556
API?

00:20:11.926 --> 00:20:13.156
Well because it's easy to use.

00:20:13.686 --> 00:20:14.656
You get this compact

00:20:14.656 --> 00:20:16.116
representation of your entire

00:20:16.116 --> 00:20:18.326
network and you can save it to

00:20:18.326 --> 00:20:20.356
disk and restore it, and that

00:20:20.466 --> 00:20:21.546
works across platforms.

00:20:23.166 --> 00:20:24.426
You only need to initialize the

00:20:24.426 --> 00:20:26.366
graph once and then you can

00:20:26.366 --> 00:20:27.716
reuse it for multiple input

00:20:27.716 --> 00:20:28.106
images.

00:20:29.516 --> 00:20:31.476
And you can execute the entire

00:20:31.476 --> 00:20:34.056
graph on the GPU with a single

00:20:34.056 --> 00:20:34.346
call.

00:20:36.276 --> 00:20:37.536
There are no intermediate images

00:20:37.536 --> 00:20:39.196
for you to manage, you just need

00:20:39.196 --> 00:20:40.756
to take care of your input and

00:20:40.756 --> 00:20:41.036
output.

00:20:42.146 --> 00:20:45.016
Internally we use Metal heaps to

00:20:45.016 --> 00:20:46.796
make sure that the memory

00:20:46.796 --> 00:20:47.916
footprint of all your

00:20:47.916 --> 00:20:49.506
intermediate images is as small

00:20:49.506 --> 00:20:50.126
as possible.

00:20:50.606 --> 00:20:51.296
For example, for the

00:20:51.296 --> 00:20:53.376
Inception-v3 network this means

00:20:53.826 --> 00:20:56.856
5x memory savings and 10x viewer

00:20:56.856 --> 00:20:58.496
allocations, which I think is

00:20:58.556 --> 00:20:59.256
pretty impressive.

00:21:00.836 --> 00:21:02.926
So as I said, the graph does all

00:21:02.926 --> 00:21:03.956
the groundwork for you.

00:21:04.316 --> 00:21:05.646
It takes care of creating

00:21:05.846 --> 00:21:06.846
intermediate images.

00:21:06.996 --> 00:21:08.706
It takes care of sizing them.

00:21:09.296 --> 00:21:11.086
It also -- it even sizes your

00:21:11.086 --> 00:21:11.366
outputs.

00:21:11.786 --> 00:21:12.766
It takes care of the padding

00:21:12.766 --> 00:21:13.406
policies.

00:21:13.796 --> 00:21:15.016
It takes care of censoring.

00:21:15.426 --> 00:21:17.516
So in short, it's a lot less

00:21:17.566 --> 00:21:19.406
code for you to write and a lot

00:21:19.486 --> 00:21:20.746
fewer bugs for you to write as

00:21:20.746 --> 00:21:20.956
well.

00:21:21.956 --> 00:21:24.066
And when I say less code, I mean

00:21:24.596 --> 00:21:25.376
a lot less code.

00:21:26.206 --> 00:21:27.906
So last year we released this

00:21:27.996 --> 00:21:30.726
Metal recognition sample that

00:21:30.726 --> 00:21:32.036
uses the Inception-v3 network

00:21:32.036 --> 00:21:33.286
for image recognition.

00:21:34.326 --> 00:21:36.026
And we took that sample and

00:21:36.026 --> 00:21:37.576
converted it to use the new

00:21:37.806 --> 00:21:40.036
graph API and found that we had

00:21:40.036 --> 00:21:41.956
to write four times less code.

00:21:42.356 --> 00:21:43.956
And that's pretty much the same

00:21:43.956 --> 00:21:45.846
number of lines as Python code

00:21:46.226 --> 00:21:47.916
you would have to write in the

00:21:47.916 --> 00:21:48.886
open-source sensor flow

00:21:48.886 --> 00:21:50.436
framework to implement the same

00:21:50.436 --> 00:21:50.846
network.

00:21:51.476 --> 00:21:52.956
And we just want to mention that

00:21:52.956 --> 00:21:54.046
we will be releasing this

00:21:54.086 --> 00:21:57.196
updated sample code -- updated

00:21:57.196 --> 00:21:58.346
example as sample code.

00:21:59.026 --> 00:22:01.796
And now having all this

00:22:01.796 --> 00:22:03.276
information about your entire

00:22:03.276 --> 00:22:06.116
network allows us to deliver the

00:22:06.116 --> 00:22:07.956
best performance across

00:22:08.106 --> 00:22:08.866
different views.

00:22:09.336 --> 00:22:10.946
We make it easy for your to

00:22:10.946 --> 00:22:12.766
parallelize between the CPU and

00:22:12.766 --> 00:22:13.206
the GPU.

00:22:13.976 --> 00:22:15.876
so as the graph is executing --

00:22:16.326 --> 00:22:18.076
as the GPU is executing the

00:22:18.076 --> 00:22:19.986
graph of one input image, the

00:22:19.986 --> 00:22:21.686
CPU can already prepare to

00:22:21.686 --> 00:22:22.776
execute the graph for a

00:22:22.776 --> 00:22:23.716
different input image.

00:22:25.176 --> 00:22:26.606
We can also fuse graph nodes

00:22:26.676 --> 00:22:28.446
together like the convolution

00:22:28.856 --> 00:22:29.966
and neuron nodes.

00:22:31.856 --> 00:22:33.746
And we can execute graph nodes

00:22:33.746 --> 00:22:34.386
concurrently.

00:22:34.386 --> 00:22:36.256
So if we look at this inception

00:22:36.256 --> 00:22:38.056
module again, you can see that

00:22:38.056 --> 00:22:39.886
there are multiple rows of these

00:22:39.886 --> 00:22:41.386
nodes that can be executed

00:22:41.456 --> 00:22:43.036
completely independently of each

00:22:43.036 --> 00:22:43.236
other.

00:22:44.176 --> 00:22:45.486
And of course the output of

00:22:45.486 --> 00:22:47.326
these independent executions

00:22:47.926 --> 00:22:49.216
need to be concatenated via

00:22:49.216 --> 00:22:50.216
concatenation nodes.

00:22:51.206 --> 00:22:52.656
And the graph is smart enough to

00:22:52.656 --> 00:22:54.276
optimize those away as well.

00:22:54.886 --> 00:22:57.706
And now let's take a look at how

00:22:57.706 --> 00:22:59.586
you can use the new graph API.

00:23:00.296 --> 00:23:02.176
So this is the code for creating

00:23:02.176 --> 00:23:04.126
a convolution node using the

00:23:04.126 --> 00:23:04.646
graph API.

00:23:05.826 --> 00:23:07.256
So it takes an image as source

00:23:08.226 --> 00:23:09.486
and it also has weights.

00:23:09.486 --> 00:23:10.926
So let's talk about weights for

00:23:10.926 --> 00:23:11.176
a minute.

00:23:13.056 --> 00:23:14.256
Neural networks keep growing

00:23:14.256 --> 00:23:15.396
larger and larger in size.

00:23:16.136 --> 00:23:17.736
And if you have many convolution

00:23:17.736 --> 00:23:19.346
nodes in your networks, that

00:23:19.346 --> 00:23:21.436
means that the overall size of

00:23:21.496 --> 00:23:22.466
the weights for your entire

00:23:22.466 --> 00:23:23.616
network could be quite

00:23:23.616 --> 00:23:24.246
considerable.

00:23:25.216 --> 00:23:27.016
And to help with that we've

00:23:27.016 --> 00:23:30.126
added a convolution data source

00:23:30.186 --> 00:23:31.296
protocol that you can implement

00:23:31.656 --> 00:23:33.186
and it provides just in time

00:23:33.606 --> 00:23:35.036
loading and purging of weights

00:23:35.076 --> 00:23:35.276
data.

00:23:36.296 --> 00:23:40.046
So the idea is that the weights

00:23:40.046 --> 00:23:41.546
for your entire network do not

00:23:41.546 --> 00:23:43.196
have to be loaded in memory all

00:23:43.196 --> 00:23:44.086
at the same time.

00:23:44.626 --> 00:23:45.956
They also do not have to be

00:23:45.956 --> 00:23:46.886
loaded in advance.

00:23:48.396 --> 00:23:49.656
To help minimize the memory

00:23:49.656 --> 00:23:51.556
footprint, when we initialize

00:23:51.556 --> 00:23:53.136
the graph and we process a

00:23:53.136 --> 00:23:54.516
particular convolution layer,

00:23:55.096 --> 00:23:56.126
we'll load the weights for that

00:23:56.126 --> 00:23:57.726
convolution layer and then we

00:23:57.856 --> 00:23:59.486
purge them before we move on to

00:23:59.486 --> 00:24:00.726
the next convolution layer.

00:24:02.226 --> 00:24:03.586
What you have to do is to

00:24:03.586 --> 00:24:05.146
implement this initialization

00:24:05.146 --> 00:24:06.916
method which just knows where

00:24:06.916 --> 00:24:08.376
the data is but it doesn't

00:24:08.376 --> 00:24:09.086
actually load it.

00:24:10.096 --> 00:24:11.256
And then when the graph calls

00:24:11.256 --> 00:24:13.266
the load function that alerts

00:24:13.266 --> 00:24:14.846
you that the weights need to be

00:24:14.846 --> 00:24:15.236
loaded.

00:24:15.366 --> 00:24:16.546
And then when the purge function

00:24:16.546 --> 00:24:18.166
is called by the graph then you

00:24:18.166 --> 00:24:19.316
can release the weights.

00:24:21.586 --> 00:24:22.526
And now let's build a graph.

00:24:23.446 --> 00:24:24.926
So here we're implementing this

00:24:24.926 --> 00:24:26.116
makeGraph function.

00:24:26.596 --> 00:24:28.366
And on the left you can see all

00:24:28.366 --> 00:24:29.636
the nodes that make up our

00:24:29.636 --> 00:24:30.826
network that we need to build.

00:24:31.256 --> 00:24:32.926
So then we create the nodes.

00:24:33.226 --> 00:24:34.446
So we create the convolution

00:24:34.446 --> 00:24:34.836
node.

00:24:35.016 --> 00:24:35.616
The pooling node.

00:24:35.616 --> 00:24:37.456
And then the rest of the nodes.

00:24:37.756 --> 00:24:38.606
So we have the nodes.

00:24:38.606 --> 00:24:40.226
How do we connect them into a

00:24:40.226 --> 00:24:40.446
graph?

00:24:41.646 --> 00:24:43.186
So we just take the result image

00:24:43.186 --> 00:24:44.986
of one node and pass it as a

00:24:45.066 --> 00:24:46.516
source image to the next node.

00:24:46.516 --> 00:24:48.106
And then we have our graph.

00:24:49.736 --> 00:24:51.236
And now let's run it on the GPU.

00:24:51.906 --> 00:24:54.066
So first we do our usual Metal

00:24:54.066 --> 00:24:54.456
setup.

00:24:54.886 --> 00:24:56.016
We initialize the graph.

00:24:56.676 --> 00:24:58.166
We take care of our input data

00:24:58.866 --> 00:25:01.066
and then we encode the graph to

00:25:01.066 --> 00:25:01.506
the GPU.

00:25:02.276 --> 00:25:04.026
And the data in the output image

00:25:04.556 --> 00:25:06.546
will be -- the output image will

00:25:06.546 --> 00:25:08.466
be populated with data when the

00:25:08.466 --> 00:25:09.576
command buffer completes.

00:25:10.086 --> 00:25:11.526
And then we have an option to

00:25:11.526 --> 00:25:12.996
wait for the GPU to finish.

00:25:13.406 --> 00:25:14.686
But we don't want you to do

00:25:14.686 --> 00:25:14.956
that.

00:25:15.886 --> 00:25:17.506
When this happens the CPU is

00:25:17.506 --> 00:25:19.246
waiting for the GPU to finish

00:25:19.836 --> 00:25:21.486
before it can start encoding the

00:25:21.486 --> 00:25:22.846
next run of the graph.

00:25:23.486 --> 00:25:25.256
And this introduces bubbles into

00:25:25.256 --> 00:25:26.266
your pipeline, which can

00:25:26.526 --> 00:25:28.036
adversely affect performance.

00:25:29.816 --> 00:25:30.686
So what we want you to do

00:25:30.686 --> 00:25:32.606
instead is to use the new

00:25:32.606 --> 00:25:34.596
asynchronous executeAsync API.

00:25:35.426 --> 00:25:37.896
So with this API your Metal

00:25:37.966 --> 00:25:39.436
setup is even smaller.

00:25:39.626 --> 00:25:41.076
So you just need to get the

00:25:41.076 --> 00:25:41.736
Metal device.

00:25:42.136 --> 00:25:43.096
Then you still need to

00:25:43.096 --> 00:25:44.016
initialize your graph.

00:25:44.056 --> 00:25:46.426
Prepare the input data and then

00:25:46.426 --> 00:25:47.856
you executeAsync call.

00:25:49.586 --> 00:25:52.376
It returns immediately and then

00:25:52.376 --> 00:25:54.216
the output image will be ready

00:25:55.196 --> 00:25:56.236
when this code inside the

00:25:56.236 --> 00:25:57.086
closure executes.

00:25:57.796 --> 00:25:59.056
But in the meantime, you don't

00:25:59.056 --> 00:26:00.136
have to wait for the GPU to

00:26:00.306 --> 00:26:02.056
finish, you can already proceed

00:26:02.056 --> 00:26:03.676
with a coding and new GPU task.

00:26:04.396 --> 00:26:07.236
And this way the CPU and the GPU

00:26:07.236 --> 00:26:08.846
are executing concurrently.

00:26:09.406 --> 00:26:10.316
There are no bubbles in your

00:26:10.316 --> 00:26:12.756
pipeline and they're both

00:26:12.756 --> 00:26:14.376
utilized to full capacity.

00:26:16.756 --> 00:26:18.996
Okay. And now I will do a live

00:26:18.996 --> 00:26:21.106
demo that demonstrates the

00:26:21.146 --> 00:26:22.846
performance difference between

00:26:22.966 --> 00:26:24.526
the synchronous and asynchronous

00:26:24.526 --> 00:26:24.786
APIs.

00:26:24.786 --> 00:26:27.576
And this demo will be using the

00:26:27.576 --> 00:26:29.576
Inception-v3 network for image

00:26:29.576 --> 00:26:30.116
recognition.

00:26:30.516 --> 00:26:30.806
All right.

00:26:31.136 --> 00:26:32.726
So I will be starting with

00:26:32.726 --> 00:26:34.416
synchronous API and here we're

00:26:34.416 --> 00:26:35.706
detecting a water bottle.

00:26:35.706 --> 00:26:38.276
And we're getting about 50

00:26:38.276 --> 00:26:41.756
milliseconds per second per

00:26:41.756 --> 00:26:42.596
image on average.

00:26:42.826 --> 00:26:44.066
And now I will switch to the

00:26:44.066 --> 00:26:44.956
asynchronous API.

00:26:44.956 --> 00:26:47.586
And now we're getting about 36

00:26:47.586 --> 00:26:49.546
milliseconds per image on

00:26:49.546 --> 00:26:49.936
average.

00:26:49.936 --> 00:26:51.656
So that's pretty good

00:26:51.686 --> 00:26:52.666
performance improvement.

00:26:54.776 --> 00:26:55.066
All right.

00:26:55.196 --> 00:26:56.436
So that's it for the live demo.

00:26:58.516 --> 00:27:04.016
[ Applause ]

00:27:04.516 --> 00:27:04.876
Thank you.

00:27:06.566 --> 00:27:07.656
Okay. Now that we've talked

00:27:07.656 --> 00:27:08.626
about the new neural network

00:27:08.626 --> 00:27:10.926
graph API and I showed you how

00:27:10.956 --> 00:27:12.716
easy it is to use and what great

00:27:12.716 --> 00:27:14.016
performance you can achieve with

00:27:14.016 --> 00:27:16.086
it, let's now switch gears and

00:27:16.086 --> 00:27:17.166
talk about recurrent neural

00:27:17.166 --> 00:27:17.566
networks.

00:27:19.416 --> 00:27:20.386
So what are recurrent neural

00:27:20.386 --> 00:27:20.786
networks?

00:27:23.406 --> 00:27:25.456
So one disadvantage of CNNs is

00:27:25.456 --> 00:27:27.276
their inability to remember

00:27:27.306 --> 00:27:28.326
anything that happened in the

00:27:28.326 --> 00:27:28.466
past.

00:27:29.456 --> 00:27:31.226
They can take one image as input

00:27:31.896 --> 00:27:33.926
and generate a single output

00:27:34.446 --> 00:27:36.316
such as the set of probabilities

00:27:36.356 --> 00:27:37.396
of what is depicted in the

00:27:37.396 --> 00:27:37.836
image.

00:27:39.056 --> 00:27:41.016
RNNs on the other hand have

00:27:41.016 --> 00:27:41.446
memory.

00:27:42.346 --> 00:27:43.466
And they're good at operating on

00:27:43.546 --> 00:27:44.066
sequences.

00:27:44.536 --> 00:27:48.256
So they can take one input such

00:27:48.256 --> 00:27:49.576
as a set of probabilities of

00:27:49.636 --> 00:27:51.016
what is depicted in the image

00:27:51.616 --> 00:27:52.966
and generate a sequence of

00:27:53.036 --> 00:27:53.366
outputs.

00:27:53.366 --> 00:27:56.196
So a sequence of words that make

00:27:56.196 --> 00:27:57.586
up a caption for this image.

00:27:59.416 --> 00:28:01.716
They can also take a sequence of

00:28:01.806 --> 00:28:03.506
inputs such as a sentence in

00:28:03.506 --> 00:28:06.626
English and generate a sequence

00:28:06.626 --> 00:28:08.506
of outputs such as the same

00:28:08.666 --> 00:28:09.946
sentence translated to a

00:28:09.946 --> 00:28:12.376
different language like Russian

00:28:12.466 --> 00:28:13.056
or Finnish.

00:28:13.366 --> 00:28:16.356
And we support a number of

00:28:16.356 --> 00:28:17.756
different of variants of RNNs.

00:28:18.586 --> 00:28:20.146
The single gate RNN, the long

00:28:20.146 --> 00:28:22.156
short-term memory RNN or LSTM,

00:28:22.596 --> 00:28:24.586
and multiple variants of LSTMs.

00:28:24.866 --> 00:28:26.336
The GRU and the MGU.

00:28:27.766 --> 00:28:29.336
So let's talk about the simplest

00:28:29.366 --> 00:28:31.326
kind of RNN, the single gate

00:28:31.326 --> 00:28:31.526
RNN.

00:28:33.666 --> 00:28:34.966
the single gate RNN has a

00:28:34.966 --> 00:28:37.006
recurrent unit which enables the

00:28:37.066 --> 00:28:38.676
previous output over RNN to

00:28:38.996 --> 00:28:40.346
affect the output of the

00:28:40.406 --> 00:28:41.846
subsequent iterations of the

00:28:41.846 --> 00:28:42.406
same RNN.

00:28:43.606 --> 00:28:45.496
But the single gate RNNs are not

00:28:45.546 --> 00:28:47.466
powerful enough to carry on

00:28:47.466 --> 00:28:48.746
important information for many

00:28:48.746 --> 00:28:49.286
iterations.

00:28:50.136 --> 00:28:51.676
Because the current output of an

00:28:51.786 --> 00:28:53.976
RNN -- of the single gate RNN is

00:28:53.976 --> 00:28:54.996
also its current state.

00:28:54.996 --> 00:28:55.976
There's nothing else there.

00:28:57.146 --> 00:28:59.426
The solution to this is the long

00:28:59.476 --> 00:29:01.596
short-term memory RNN or LSTM.

00:29:02.376 --> 00:29:03.906
It's built from single gate RNNs

00:29:03.906 --> 00:29:06.246
and it has an internal memory

00:29:06.246 --> 00:29:06.506
cell.

00:29:07.436 --> 00:29:08.856
And a certain combination of

00:29:08.956 --> 00:29:10.596
gates control how the

00:29:10.596 --> 00:29:13.106
information flows inside LSTM.

00:29:13.436 --> 00:29:15.016
And what is stored and not

00:29:15.136 --> 00:29:16.266
stored in the memory cell.

00:29:16.846 --> 00:29:19.656
So let's take a look at the

00:29:19.656 --> 00:29:21.316
architecture of LSTM in more

00:29:21.316 --> 00:29:21.776
detail.

00:29:22.206 --> 00:29:25.246
As I said, the most important

00:29:25.356 --> 00:29:27.846
entity inside LSTM is the memory

00:29:27.886 --> 00:29:30.406
cell which is updated in every

00:29:30.476 --> 00:29:31.536
duration of LSTM.

00:29:31.536 --> 00:29:33.426
So you can think of each

00:29:33.846 --> 00:29:35.246
iteration of LSTM is this

00:29:35.306 --> 00:29:37.456
transition between the old and

00:29:37.456 --> 00:29:38.016
new memory.

00:29:38.676 --> 00:29:40.786
And now let's talk about the

00:29:40.816 --> 00:29:41.036
gates.

00:29:41.566 --> 00:29:43.376
So first there is a forget gate

00:29:44.216 --> 00:29:45.876
which decides what to keep and

00:29:45.876 --> 00:29:47.206
what not to keep from old

00:29:47.206 --> 00:29:47.566
memory.

00:29:48.966 --> 00:29:50.456
And then there are the inputs

00:29:50.456 --> 00:29:51.836
and the cell gates and their

00:29:51.836 --> 00:29:53.996
combined contribution determines

00:29:54.066 --> 00:29:55.786
what from the current input will

00:29:55.786 --> 00:29:56.936
affect the new memory.

00:29:56.936 --> 00:29:59.136
And then the combination of all

00:29:59.136 --> 00:30:00.866
of these three gates is combined

00:30:01.226 --> 00:30:04.596
to update the memory cell.

00:30:05.696 --> 00:30:07.576
And finally, there is the output

00:30:07.626 --> 00:30:09.656
gate which determines what from

00:30:09.656 --> 00:30:11.976
the previous inputs the -- the

00:30:12.476 --> 00:30:14.076
previous output, the current

00:30:14.076 --> 00:30:16.126
inputs and the new memory will

00:30:16.126 --> 00:30:17.936
affect the output of LSTM.

00:30:19.196 --> 00:30:20.626
So now that you know what LSTM

00:30:20.626 --> 00:30:22.206
is made up of, let's take a look

00:30:22.206 --> 00:30:24.006
at how you can create one using

00:30:24.006 --> 00:30:24.576
our framework.

00:30:25.616 --> 00:30:27.536
So first you create a descriptor

00:30:27.756 --> 00:30:28.576
for the LSTM.

00:30:29.146 --> 00:30:31.306
And then you need to initialize

00:30:31.526 --> 00:30:31.876
the gates.

00:30:32.436 --> 00:30:33.676
So what controls the gates?

00:30:33.676 --> 00:30:35.146
So what controls the gates with

00:30:35.306 --> 00:30:36.156
-- what controls how they

00:30:36.156 --> 00:30:37.756
operate is the trained

00:30:37.756 --> 00:30:38.386
parameters.

00:30:39.146 --> 00:30:40.156
The ones that come from the

00:30:40.156 --> 00:30:41.726
training step where you train a

00:30:41.726 --> 00:30:43.526
system to do a particular task.

00:30:45.566 --> 00:30:47.496
And there are multiple gates for

00:30:47.496 --> 00:30:48.586
you to initialize as you can

00:30:48.646 --> 00:30:49.856
see, but we're only showing two

00:30:49.856 --> 00:30:52.356
initializations just to be

00:30:52.356 --> 00:30:52.726
brief.

00:30:53.206 --> 00:30:54.336
And as you can see, we're also

00:30:54.336 --> 00:30:56.046
using a data source provider.

00:30:56.046 --> 00:30:57.446
The same one I showed you before

00:30:57.556 --> 00:30:58.606
to initialize the weights.

00:30:59.656 --> 00:31:01.536
And the next step is to create

00:31:01.536 --> 00:31:03.606
our LSTM layer and now we want

00:31:03.606 --> 00:31:04.876
to run it on the GPU.

00:31:06.586 --> 00:31:08.646
So we need to create our arrays

00:31:08.646 --> 00:31:10.976
that will hold the input and

00:31:10.976 --> 00:31:13.236
output for the sequence of the

00:31:13.236 --> 00:31:14.266
LSTM executions.

00:31:14.886 --> 00:31:16.076
And then we encode the sequence

00:31:16.076 --> 00:31:16.716
to the GPU.

00:31:17.416 --> 00:31:19.446
And here we're showing you the

00:31:19.666 --> 00:31:21.546
-- a matrix-based RNN, but we

00:31:21.546 --> 00:31:22.646
just want to mention that we

00:31:22.686 --> 00:31:25.726
also support RNNs that operate

00:31:25.726 --> 00:31:27.636
on MPS images via convolutions.

00:31:30.176 --> 00:31:31.216
And now let's take a look at an

00:31:31.216 --> 00:31:32.016
actual example.

00:31:32.596 --> 00:31:34.366
So we'll use image captioning as

00:31:34.366 --> 00:31:35.706
an example of using LSTM.

00:31:36.726 --> 00:31:38.566
So as you recall, I told you

00:31:38.896 --> 00:31:40.646
that deep learning algorithms

00:31:40.646 --> 00:31:42.076
have two phases.

00:31:42.346 --> 00:31:43.316
The training phase and the

00:31:43.316 --> 00:31:44.026
inference phase.

00:31:44.816 --> 00:31:46.816
So to train a system to caption

00:31:46.816 --> 00:31:49.196
images you need to feed it a

00:31:49.196 --> 00:31:51.056
large number of images with

00:31:51.056 --> 00:31:52.356
human-generated captions.

00:31:53.836 --> 00:31:56.606
So what does this system have?

00:31:56.656 --> 00:31:57.686
Like what is it made out of?

00:31:58.536 --> 00:32:02.016
So this system has a CNN and a

00:32:02.016 --> 00:32:03.906
RNN working together to generate

00:32:03.906 --> 00:32:04.346
captions.

00:32:04.746 --> 00:32:07.316
The CNN is used to figure out

00:32:07.316 --> 00:32:09.316
what's depicted in the image and

00:32:09.316 --> 00:32:10.886
then the RNN is used to generate

00:32:10.956 --> 00:32:11.806
the actual caption.

00:32:13.256 --> 00:32:15.076
And the output of that process

00:32:15.156 --> 00:32:17.006
is the trained parameters which

00:32:17.006 --> 00:32:19.036
are required for the next step,

00:32:20.186 --> 00:32:20.886
the inference step.

00:32:21.676 --> 00:32:25.606
So in the inference phase, the

00:32:25.656 --> 00:32:27.766
trained parameters control both

00:32:27.766 --> 00:32:29.826
the operation of the CNN layers

00:32:29.826 --> 00:32:31.866
and the operation of the RNN

00:32:31.866 --> 00:32:32.146
gates.

00:32:33.206 --> 00:32:37.276
And then for each image it's

00:32:37.336 --> 00:32:39.166
processed by both the CNN and

00:32:39.446 --> 00:32:41.326
the RNN to generate a caption.

00:32:42.216 --> 00:32:43.246
So we already know a good

00:32:43.246 --> 00:32:44.636
network for figuring out what's

00:32:44.706 --> 00:32:45.816
depicted in the image.

00:32:46.086 --> 00:32:47.506
It's the Inception-v3 network,

00:32:47.876 --> 00:32:48.626
so we'll use that.

00:32:49.186 --> 00:32:50.456
And we just talked about LSTMs,

00:32:50.456 --> 00:32:52.236
so let's use that to generate

00:32:52.446 --> 00:32:53.036
our caption.

00:32:53.996 --> 00:32:56.456
And the caption generation phase

00:32:57.266 --> 00:32:58.176
-- the caption generation

00:32:58.176 --> 00:32:59.726
process also has two phases.

00:33:00.006 --> 00:33:02.206
So first we have the LSTM

00:33:02.486 --> 00:33:03.646
initialization phase.

00:33:04.846 --> 00:33:06.126
So we run our Inception-v3

00:33:06.126 --> 00:33:08.616
network and we actually run all

00:33:08.616 --> 00:33:10.496
of the layers except the very

00:33:10.496 --> 00:33:11.986
last SoftMax layer.

00:33:12.236 --> 00:33:13.386
And the output of that is a

00:33:13.386 --> 00:33:15.016
feature vector which has

00:33:15.016 --> 00:33:16.196
information about what is

00:33:16.196 --> 00:33:17.226
depicted in the image.

00:33:17.906 --> 00:33:19.016
And then we take that feature

00:33:19.016 --> 00:33:20.566
vector and convert it to a

00:33:20.566 --> 00:33:23.246
compact representation that's

00:33:23.246 --> 00:33:24.286
required by LSTM.

00:33:24.286 --> 00:33:26.596
And then run that through LSTM

00:33:26.946 --> 00:33:27.746
to initialize it.

00:33:28.736 --> 00:33:30.466
And then once we have our

00:33:30.466 --> 00:33:32.436
initialized LSTM, then we're

00:33:32.436 --> 00:33:33.586
ready for the next phase.

00:33:34.606 --> 00:33:36.066
Our actual caption generation

00:33:36.066 --> 00:33:36.376
phase.

00:33:38.116 --> 00:33:39.676
And we start this process by

00:33:39.676 --> 00:33:41.366
passing in a special sentence

00:33:41.436 --> 00:33:44.026
start ID token to our LSTM.

00:33:44.236 --> 00:33:45.046
And the output of that

00:33:45.046 --> 00:33:46.756
operations is a sequence of

00:33:46.756 --> 00:33:50.006
words which are, you know, the

00:33:50.006 --> 00:33:51.276
words that are connected to what

00:33:51.276 --> 00:33:52.666
is depicted in the image.

00:33:53.526 --> 00:33:55.806
And then we pass those words to

00:33:55.806 --> 00:33:57.226
a SoftMax layer which computes

00:33:57.226 --> 00:33:58.796
probabilities for these words.

00:33:59.326 --> 00:34:01.176
And we pick the three best ones.

00:34:01.326 --> 00:34:03.276
And these three best words are

00:34:03.276 --> 00:34:05.816
also our one-word partial

00:34:05.816 --> 00:34:07.546
captions for a particular image.

00:34:08.126 --> 00:34:09.726
So we take those words and pass

00:34:09.806 --> 00:34:11.856
them to the next situation of

00:34:11.946 --> 00:34:15.045
LSTM which function is to now

00:34:15.096 --> 00:34:16.886
come up with three best two-word

00:34:16.916 --> 00:34:19.216
captions for our image and so

00:34:19.216 --> 00:34:19.386
on.

00:34:19.466 --> 00:34:21.416
We execute for N iterations

00:34:21.886 --> 00:34:23.076
until we reach a stopping

00:34:23.076 --> 00:34:25.596
condition, which is when we

00:34:25.626 --> 00:34:27.116
either reach the maximum number

00:34:27.116 --> 00:34:28.966
of words that we want to be in

00:34:28.966 --> 00:34:30.856
our caption, or when the

00:34:30.856 --> 00:34:32.136
probabilities evolving the newly

00:34:32.136 --> 00:34:33.906
generating captions drop to 0.

00:34:34.985 --> 00:34:35.996
So I know this is still pretty

00:34:35.996 --> 00:34:36.505
abstract.

00:34:36.846 --> 00:34:39.005
So let's look at the output of

00:34:39.386 --> 00:34:42.266
LSTM -- of an actual output of

00:34:42.326 --> 00:34:44.556
LSTM for several iterations for

00:34:44.556 --> 00:34:45.545
a particular image.

00:34:46.216 --> 00:34:49.786
So in this image we have, you

00:34:49.786 --> 00:34:51.636
know, our surfer riding a wave.

00:34:51.636 --> 00:34:53.146
And we want to compute the top

00:34:53.146 --> 00:34:54.666
three captions for this image.

00:34:55.696 --> 00:34:57.286
And in the first iteration of

00:34:57.366 --> 00:34:59.936
LSTM we generate three best

00:34:59.936 --> 00:35:00.306
words.

00:35:02.336 --> 00:35:04.446
So -- which are our best

00:35:04.446 --> 00:35:05.686
one-word captions for this

00:35:05.686 --> 00:35:05.996
image.

00:35:06.506 --> 00:35:07.596
So "man", "a", and "the".

00:35:08.316 --> 00:35:10.596
And the word "a" has the highest

00:35:10.596 --> 00:35:11.186
probability.

00:35:11.936 --> 00:35:13.546
So we take these three words and

00:35:13.546 --> 00:35:15.196
we pass them to the next

00:35:15.196 --> 00:35:16.436
iteration of LSTM.

00:35:17.166 --> 00:35:19.356
And in this iteration, for every

00:35:19.356 --> 00:35:21.416
one of these three starter

00:35:22.436 --> 00:35:24.416
words, LSTM generates three new

00:35:24.416 --> 00:35:26.026
words that have the highest

00:35:26.026 --> 00:35:28.496
probability of following each

00:35:28.496 --> 00:35:29.606
one of these starter words.

00:35:30.666 --> 00:35:31.966
Right? So we have three new

00:35:31.966 --> 00:35:33.556
words to follow the word "man".

00:35:33.946 --> 00:35:35.256
Three new words to follow the

00:35:35.256 --> 00:35:37.766
word "a", and three new words to

00:35:37.826 --> 00:35:38.976
follow the word "the".

00:35:40.686 --> 00:35:42.296
And now as you can see, each one

00:35:42.296 --> 00:35:44.486
of these two-word captions also

00:35:44.486 --> 00:35:45.486
have a probability.

00:35:46.026 --> 00:35:47.866
And because the word "a" had

00:35:47.936 --> 00:35:49.286
such a high probability in the

00:35:49.286 --> 00:35:53.366
first iteration, then the

00:35:53.366 --> 00:35:54.646
captions that start with the

00:35:54.646 --> 00:35:56.426
word "a" in the second iteration

00:35:56.476 --> 00:35:58.156
also end up having the highest

00:35:58.156 --> 00:35:58.796
probability.

00:35:58.896 --> 00:36:00.916
Why? Because the probability of

00:36:00.916 --> 00:36:02.506
a two-word caption is just a

00:36:02.536 --> 00:36:04.516
product of the probabilities of

00:36:04.516 --> 00:36:05.706
the words that make up that

00:36:05.706 --> 00:36:06.006
caption.

00:36:07.116 --> 00:36:08.886
So that's how we get these three

00:36:08.886 --> 00:36:09.396
best ones.

00:36:09.696 --> 00:36:11.226
And then we take them and we

00:36:11.226 --> 00:36:12.836
move on to the next iteration.

00:36:13.186 --> 00:36:14.406
And in the next iteration we

00:36:14.406 --> 00:36:15.936
just add one more word to our

00:36:15.936 --> 00:36:17.536
captions so that we have

00:36:17.846 --> 00:36:18.806
three-word captions.

00:36:19.206 --> 00:36:19.996
And then we compute the

00:36:19.996 --> 00:36:21.836
probabilities of those captions

00:36:21.836 --> 00:36:22.886
and pick the best three.

00:36:23.936 --> 00:36:25.106
And we move on to the next

00:36:25.106 --> 00:36:27.026
iteration where we just end up

00:36:27.026 --> 00:36:28.746
adding one more word to our

00:36:28.746 --> 00:36:29.186
caption.

00:36:29.186 --> 00:36:30.566
So we have four-word captions.

00:36:30.976 --> 00:36:31.876
And then we compute the

00:36:31.926 --> 00:36:33.186
probabilities of all these

00:36:33.236 --> 00:36:34.546
captions and pick the best

00:36:34.606 --> 00:36:34.816
three.

00:36:36.176 --> 00:36:37.306
And so on -- I think you get the

00:36:37.306 --> 00:36:37.646
idea.

00:36:37.866 --> 00:36:38.936
Let's just skip to the end.

00:36:39.296 --> 00:36:41.416
So in the end, we get our three

00:36:41.416 --> 00:36:43.506
top captions for this particular

00:36:43.506 --> 00:36:43.936
image.

00:36:43.936 --> 00:36:45.906
And the best one is a man riding

00:36:45.906 --> 00:36:47.426
a wave on top of a surfboard,

00:36:47.656 --> 00:36:48.616
which I think it's pretty close.

00:36:50.966 --> 00:36:52.346
So -- [applause] and let's now

00:36:52.346 --> 00:36:52.726
do a demo.

00:36:53.508 --> 00:36:55.508
[ Applause ]

00:36:58.476 --> 00:37:00.116
So now we'll do a demo of this

00:37:00.606 --> 00:37:02.206
-- of the captioning network.

00:37:03.346 --> 00:37:04.856
So we have a collection of

00:37:04.856 --> 00:37:07.156
images here, and as soon as I

00:37:07.156 --> 00:37:09.036
tap on an image then the CNN

00:37:09.136 --> 00:37:10.826
will run to determine what is

00:37:10.826 --> 00:37:12.526
depicted in the image.

00:37:12.526 --> 00:37:14.046
And then the RNN will run to

00:37:14.046 --> 00:37:15.226
generate the actual caption.

00:37:15.356 --> 00:37:16.036
So let's try it out.

00:37:17.826 --> 00:37:19.446
>> A man riding a wave on top of

00:37:19.446 --> 00:37:19.766
a surfboard.

00:37:19.766 --> 00:37:22.356
>> So we already know this.

00:37:23.526 --> 00:37:24.566
Now let's try another image.

00:37:24.996 --> 00:37:26.536
>> An old truck is parked in the

00:37:26.536 --> 00:37:27.106
field.

00:37:27.626 --> 00:37:28.936
>> So the network actually knows

00:37:28.976 --> 00:37:30.356
that it's an old truck and that

00:37:30.356 --> 00:37:31.786
it's parked and not moving,

00:37:31.966 --> 00:37:33.336
which I think is pretty

00:37:33.556 --> 00:37:34.156
impressive.

00:37:34.786 --> 00:37:35.656
Now let's try one more.

00:37:37.026 --> 00:37:38.496
>> A black and white dog laying

00:37:38.496 --> 00:37:39.086
in the grass.

00:37:39.796 --> 00:37:40.896
>> So the network knows that

00:37:40.896 --> 00:37:42.176
it's a black and white dog and

00:37:42.176 --> 00:37:43.636
that it's laying in the grass,

00:37:44.326 --> 00:37:45.106
not running.

00:37:45.236 --> 00:37:46.356
Not walking.

00:37:46.726 --> 00:37:47.876
Not sitting.

00:37:48.336 --> 00:37:50.266
Laying in the grass.

00:37:50.766 --> 00:37:52.796
So pretty cool.

00:37:53.516 --> 00:37:57.786
[ Applause ]

00:37:58.286 --> 00:37:58.726
Thank you.

00:37:59.306 --> 00:38:01.166
And on this note, let's go to

00:38:01.236 --> 00:38:01.586
the summary.

00:38:02.066 --> 00:38:03.536
So in this session we talked

00:38:03.536 --> 00:38:05.246
about all of the new primitives

00:38:05.276 --> 00:38:07.006
that we added to the MPS

00:38:07.336 --> 00:38:08.206
framework this year.

00:38:08.586 --> 00:38:10.236
We've expanded our support for

00:38:10.236 --> 00:38:12.386
image processing primitives and

00:38:12.786 --> 00:38:13.736
for convolutional neural

00:38:13.736 --> 00:38:14.136
networks.

00:38:15.006 --> 00:38:16.596
And we've added support for

00:38:16.596 --> 00:38:18.886
linear algebra and recurrent

00:38:18.886 --> 00:38:22.226
neural networks.

00:38:23.096 --> 00:38:24.666
the framework is optimized for

00:38:24.666 --> 00:38:26.146
iOS, as I told you, and now

00:38:26.596 --> 00:38:29.226
these primitives are also all

00:38:29.226 --> 00:38:30.016
available on the Mac.

00:38:31.116 --> 00:38:32.416
We also talked about the new

00:38:32.416 --> 00:38:34.676
neural network graph API and we

00:38:34.676 --> 00:38:36.406
showed you how easy it is to

00:38:36.406 --> 00:38:38.336
use, to build and execute your

00:38:38.336 --> 00:38:39.666
networks on the GPU.

00:38:40.426 --> 00:38:42.186
And that it makes it possible

00:38:42.186 --> 00:38:43.586
for us to deliver the best

00:38:43.666 --> 00:38:45.346
performance for your networks

00:38:45.346 --> 00:38:46.336
across the different GPUs.

00:38:46.336 --> 00:38:49.776
And we would love to see you use

00:38:49.776 --> 00:38:51.976
all of this new functionality to

00:38:51.976 --> 00:38:53.526
create a really great apps and

00:38:53.526 --> 00:38:55.836
tell us about it.

00:38:56.116 --> 00:38:57.296
So please check out the related

00:38:57.296 --> 00:38:58.856
Metal 2 sessions and the

00:38:58.856 --> 00:39:01.186
sessions on the core ML and

00:39:01.676 --> 00:39:03.006
Accelerate and Vision

00:39:03.006 --> 00:39:03.486
Frameworks.

00:39:04.716 --> 00:39:06.036
And for more information about

00:39:06.096 --> 00:39:07.636
this session and for links of

00:39:07.706 --> 00:39:09.936
sample code, please check out

00:39:09.986 --> 00:39:11.426
this link on our developer

00:39:11.426 --> 00:39:14.276
website and thank you so much

00:39:14.276 --> 00:39:15.636
for coming, and have a great

00:39:15.636 --> 00:39:15.846
WWDC.

00:39:16.516 --> 00:39:20.500
[ Applause ]