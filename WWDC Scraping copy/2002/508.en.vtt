WEBVTT

00:00:11.180 --> 00:00:13.840
Good afternoon and
welcome to session 508,

00:00:13.840 --> 00:00:15.630
Audio Units and Audio Codecs.

00:00:15.680 --> 00:00:16.340
I'm Craig Keithley.

00:00:16.340 --> 00:00:19.240
I'm Apple's USB and FireWire evangelist.

00:00:19.240 --> 00:00:23.450
Today we're going to talk about
Audio Units and Audio Codecs in Mac OS X.

00:00:23.560 --> 00:00:25.020
And Audio Units are very cool.

00:00:25.020 --> 00:00:29.630
They provide a way by which you can
create a graph of processing units

00:00:29.660 --> 00:00:35.800
where you can connect together a bunch
of discrete -- I'll even say digital

00:00:35.800 --> 00:00:41.890
signal processing elements and process
a signal through this graph in a nice,

00:00:41.890 --> 00:00:44.600
timely, time-synchronized manner.

00:00:44.600 --> 00:00:46.830
So to talk about that,
let's bring up Jeff Moore,

00:00:46.960 --> 00:00:48.920
Core Audio Engineering.

00:00:51.150 --> 00:00:56.110
In contrast to what Craig just said,
I'm going to talk a little

00:00:56.110 --> 00:00:59.400
bit about the Audio Codec API.

00:00:59.400 --> 00:01:02.030
Audio Codecs are a new feature to Jaguar.

00:01:02.410 --> 00:01:10.930
They exist to plug into the
Audio Converter to allow for more

00:01:11.000 --> 00:01:15.090
formats converting to and from
all sorts of different stuff.

00:01:16.460 --> 00:01:19.400
Audio Codecs are
basically a component API,

00:01:19.430 --> 00:01:21.300
just like Audio Units are.

00:01:21.300 --> 00:01:24.660
They obviously have different
selectors than Audio Units,

00:01:24.660 --> 00:01:26.770
but they have the same basic layout.

00:01:26.800 --> 00:01:32.200
In Jaguar, like I said,
the Audio Convertor is going

00:01:32.200 --> 00:01:36.840
to use Audio Codecs as its
extendability mechanism.

00:01:38.700 --> 00:01:42.440
There are three basic kinds
of Audio Codec components.

00:01:42.440 --> 00:01:45.140
There are encoders,
which transform linear PCM data

00:01:45.150 --> 00:01:46.660
into some other format.

00:01:46.660 --> 00:01:50.840
There are decoders,
which translate that other

00:01:50.840 --> 00:01:52.840
format back into linear PCM.

00:01:52.840 --> 00:01:56.420
Then there are Unity Codecs,
which transform between

00:01:56.420 --> 00:01:58.550
variants of the same format.

00:01:58.680 --> 00:02:01.660
An example of that would
be an inter-float converter

00:02:01.660 --> 00:02:03.490
or a sample rate converter.

00:02:05.080 --> 00:02:07.920
Audio Codecs are being components.

00:02:07.920 --> 00:02:11.160
They're discovered just like
any other kind of component.

00:02:11.160 --> 00:02:13.140
You use component manager routines.

00:02:13.140 --> 00:02:15.320
In particular,
you will use Find X Component

00:02:15.320 --> 00:02:19.760
to iterate through the list of
different kinds of components.

00:02:19.760 --> 00:02:22.480
Then once you find one that
you're interested in using,

00:02:22.560 --> 00:02:25.700
you can open a component on
it to get an instance of it.

00:02:25.700 --> 00:02:28.550
Then when you're done,
you just call Close Component

00:02:28.550 --> 00:02:29.950
and it will go away.

00:02:31.960 --> 00:02:37.060
The Audio Codecs also have properties
like most of the other core audio APIs,

00:02:37.060 --> 00:02:40.700
and they provide you a way to
configure the transformation

00:02:40.720 --> 00:02:42.160
being performed by the codec.

00:02:42.200 --> 00:02:46.110
Like other properties,
the value of the property is an untyped

00:02:46.110 --> 00:02:50.600
block of memory whose contents are
agreed upon by the property's ID.

00:02:52.260 --> 00:02:55.850
Some of the important properties
for Codecs are the Requires

00:02:55.850 --> 00:03:00.490
Packet Description property,
which says that the format you're

00:03:00.490 --> 00:03:04.870
dealing with requires external
packetization information to describe

00:03:04.870 --> 00:03:07.220
how the data is broken up in the buffer.

00:03:07.220 --> 00:03:13.070
Then you have the Packet Frame Size,
which tells you how many frames are in

00:03:13.070 --> 00:03:15.680
a given packet of data for this format.

00:03:16.820 --> 00:03:21.160
Then you have the Variable
Packet Byte Sizes property,

00:03:21.160 --> 00:03:26.120
which tells you whether or not the number
of bytes in each packet is going to vary.

00:03:26.140 --> 00:03:30.750
This will be the case for
variable bitrate formats like AC3,

00:03:30.750 --> 00:03:33.090
AAC, MP3, what have you.

00:03:33.100 --> 00:03:38.670
Then there's another property that
tells you how big a packet is ever

00:03:38.670 --> 00:03:41.540
going to get for a given format.

00:03:41.540 --> 00:03:44.540
That's the Maximum Packet
Byte Size property.

00:03:47.020 --> 00:03:50.940
Some more important properties that
allow you to configure the rough

00:03:50.940 --> 00:03:53.520
translation between that you're doing.

00:03:53.520 --> 00:03:56.980
You can set the input and output format.

00:03:56.980 --> 00:03:58.250
This is important.

00:03:58.260 --> 00:04:00.020
You may think that sounds a little weird.

00:04:00.160 --> 00:04:03.640
If you have some codecs will
support outputting 16-bit

00:04:03.640 --> 00:04:06.940
integer and floating point,
you might want to choose between them.

00:04:06.940 --> 00:04:10.600
You may want to choose other
aspects about the formats.

00:04:10.660 --> 00:04:13.480
The codecs will also provide
a list of what formats they

00:04:13.620 --> 00:04:15.090
support in each direction.

00:04:15.940 --> 00:04:18.370
Then you also have the
Magic Cookie property.

00:04:18.380 --> 00:04:25.120
The Magic Cookie property
provides an untyped block of data

00:04:25.210 --> 00:04:28.900
that is private to its format.

00:04:28.900 --> 00:04:33.680
It's there to provide out-of-band
configuration information about the

00:04:34.030 --> 00:04:36.960
transformation that you're about to do.

00:04:36.960 --> 00:04:42.560
Most advanced codecs need to have
this information supplied before

00:04:42.560 --> 00:04:45.060
they can do any useful work.

00:04:47.580 --> 00:04:51.840
Audio Codecs operate in two states.

00:04:51.840 --> 00:04:54.640
They can be initialized or uninitialized.

00:04:55.230 --> 00:05:00.350
You can move between these states
using the Audio Codec initialized and

00:05:00.400 --> 00:05:02.980
Audio Codec uninitialized routines.

00:05:04.990 --> 00:05:09.710
The difference between the two states is
that when a codec becomes initialized,

00:05:10.470 --> 00:05:14.920
that's a signal to the codec that it's
going to lock down the transformation

00:05:14.920 --> 00:05:19.460
it's going to do and allocate all its
buffers that it needs and load any

00:05:19.460 --> 00:05:21.450
tables that it needs to do its job.

00:05:21.480 --> 00:05:24.710
This is important because
from that point on,

00:05:24.710 --> 00:05:29.020
until you uninitialize the codec,
you can't change anything

00:05:29.020 --> 00:05:33.210
about the transformation that
the codec is going to do.

00:05:37.210 --> 00:05:42.550
So there are also properties that
will only be able to be known when

00:05:42.550 --> 00:05:44.680
you have initialized the codec.

00:05:44.760 --> 00:05:48.100
A good example is the
maximum packet byte size.

00:05:48.100 --> 00:05:50.950
You can only really ever know
that once you've determined

00:05:50.950 --> 00:05:53.660
what the input format is,
and you've supplied the

00:05:53.660 --> 00:05:57.260
magic cookie for that format,
and then the codec has enough

00:05:57.260 --> 00:06:02.060
information in its possession to say,
okay, the packets is never going to get

00:06:02.060 --> 00:06:04.330
any bigger than X number of bytes.

00:06:05.220 --> 00:06:07.720
And so you won't be able to find
that information out until you

00:06:07.720 --> 00:06:09.040
actually initialize the codec.

00:06:11.160 --> 00:06:16.080
To move data through the codecs,
the codecs use a push

00:06:16.240 --> 00:06:18.800
and then pull model.

00:06:18.870 --> 00:06:23.140
Input data is provided to the codec
using Audio Codec Append Input Data.

00:06:23.140 --> 00:06:27.310
The input data is then copied into
an internal buffer of the codec.

00:06:27.420 --> 00:06:34.660
The codec will also return how much data
it has consumed from your input buffer.

00:06:34.660 --> 00:06:37.720
You have to supply whole
packets of information,

00:06:37.760 --> 00:06:41.200
not partial packets,
if external packet descriptions

00:06:41.300 --> 00:06:42.860
are required for a format.

00:06:43.110 --> 00:06:48.340
A very important format for that
that requires packet descriptions

00:06:48.340 --> 00:06:51.290
is MPEG-4 formats like AAC.

00:06:52.090 --> 00:06:55.360
To get data back out of the codec,
you use Audio Codec

00:06:55.360 --> 00:06:57.220
Produce Output Packets.

00:06:57.220 --> 00:07:02.940
Output is always produced
in full packets of data.

00:07:02.940 --> 00:07:04.520
You can't get partial packets.

00:07:04.580 --> 00:07:11.110
Most codecs just don't have the
capability to cache the data or

00:07:11.110 --> 00:07:15.930
partially process partial input packets.

00:07:17.540 --> 00:07:21.560
Audio Codec Produce Output Packets
will also return a status value to

00:07:21.600 --> 00:07:25.280
tell you a little bit about what's
going on internally in the codec,

00:07:25.460 --> 00:07:29.690
so that you can know whether or
not you need to feed it more data

00:07:29.690 --> 00:07:34.360
or whether it has enough data to
keep pulling out more packets.

00:07:37.810 --> 00:07:44.380
We provide an SDK for building
Audio Codec components.

00:07:44.490 --> 00:07:47.000
It provides a C++ class library.

00:07:47.000 --> 00:07:59.750
I'm going to show you a little
around that SDK here in a second.

00:07:59.750 --> 00:07:59.750
Could I have demo four,

00:08:04.150 --> 00:08:09.890
So the Audio Codec SDK is provided,
you know, it's just a set of C++ classes.

00:08:09.920 --> 00:08:13.560
The important,
and since it's a component,

00:08:13.560 --> 00:08:19.150
since the actual packaging
of the codec is done as a

00:08:22.360 --> 00:08:24.520
It's done as a component.

00:08:24.520 --> 00:08:29.140
The SDK implements all the
nitty-gritty of dealing with

00:08:29.140 --> 00:08:32.030
the component aspect of things,
and you can just override a

00:08:32.030 --> 00:08:33.210
few methods here and there.

00:08:33.210 --> 00:08:37.310
It's very similar to the way
the Audio Unit SDK is laid out.

00:08:37.990 --> 00:08:44.570
You have a base class that provides
the abstraction points of all

00:08:44.620 --> 00:08:48.210
the routines in the Codec API.

00:08:48.210 --> 00:08:51.530
As you can see,
there just really isn't a whole heck

00:08:51.530 --> 00:08:54.140
of a lot of routines in the Codec API.

00:08:54.140 --> 00:08:55.930
You have the property routines.

00:08:55.930 --> 00:08:58.040
You have initialize and uninitialize.

00:08:58.040 --> 00:09:01.150
You have append input data
and produce output packets.

00:09:01.190 --> 00:09:04.680
Then you have the reset routine,
which allows you to reset

00:09:05.140 --> 00:09:07.070
the codec's internal state.

00:09:07.130 --> 00:09:09.770
You can also reset and clear all
its internal buffering to begin

00:09:09.800 --> 00:09:11.370
processing a new stream of data.

00:09:14.040 --> 00:09:18.170
Then the Codecs,
there are further specializations of

00:09:18.200 --> 00:09:24.120
the AC Codec base class that provide
different levels of implementation.

00:09:24.120 --> 00:09:27.940
The first one is the AC Base Codec class.

00:09:27.940 --> 00:09:32.740
It goes about its job by just
providing the raw mechanisms

00:09:32.740 --> 00:09:37.220
necessary to manage things like
the input and output format lists,

00:09:37.220 --> 00:09:41.520
as well as dealing with the
management of the magic cookie.

00:09:41.960 --> 00:09:47.360
It also will further break out the
properties into individual virtual

00:09:47.440 --> 00:09:50.700
methods that you can override to make
it easier to implement this stuff.

00:09:53.210 --> 00:09:57.410
Then the final base class that's
provided is AC Simple Codec.

00:09:57.480 --> 00:10:02.470
And AC Simple Codec goes the
extra mile of providing some

00:10:02.470 --> 00:10:05.200
primitive buffer handling.

00:10:05.210 --> 00:10:11.160
It implements a ring buffer
for input and provides routines

00:10:11.230 --> 00:10:15.080
for managing this ring buffer.

00:10:15.080 --> 00:10:19.440
And then in your subclass of this,
you would override the other

00:10:19.980 --> 00:10:24.390
routines and use these routines
to actually access the input data.

00:10:25.650 --> 00:10:29.180
The SDK will come with
a couple of preliminary

00:10:29.180 --> 00:10:32.820
implementations of some codecs.

00:10:32.840 --> 00:10:36.500
The one I have up here is an IMA codec.

00:10:36.980 --> 00:10:39.910
IMA4 is a pretty common codec.

00:10:39.980 --> 00:10:42.780
It's reasonable in quality.

00:10:42.780 --> 00:10:45.340
It's mildly lossy.

00:10:45.340 --> 00:10:52.610
The way the IMA codecs are brought
up is they have a base codec

00:10:52.610 --> 00:10:57.960
class that wraps up all the common
tables and the management of the

00:10:57.960 --> 00:11:03.930
channel statelist that is needed
to do IMA encoding and decoding.

00:11:03.940 --> 00:11:09.590
Then there are individual subclasses
to implement a decoder and an encoder.

00:11:09.590 --> 00:11:14.450
This is the header file
for the encoder class.

00:11:14.450 --> 00:11:18.020
As you can see,
it doesn't really have a whole lot to do.

00:11:18.020 --> 00:11:20.610
It provides,
it overrides the routine to figure out

00:11:20.720 --> 00:11:25.120
when someone changes the input format and
when someone changes the output format.

00:11:25.120 --> 00:11:30.320
It also overrides produce output packets
so that it can handle the encoding.

00:11:30.340 --> 00:11:35.530
Then IMA, this implementation of IMA is
based... uses a static routine to

00:11:35.530 --> 00:11:37.200
actually do the signal processing.

00:11:44.390 --> 00:11:49.830
I want to go through, just quickly,
the produce output packets routine to

00:11:50.460 --> 00:11:53.730
give you a feel for what you're going
to have to do to implement a codec.

00:11:53.780 --> 00:12:01.030
The first thing to note is that the
codec has to be initialized before

00:12:01.030 --> 00:12:04.050
you can call produce output packets.

00:12:04.100 --> 00:12:06.300
You have to check,
make sure that you're in the right

00:12:06.300 --> 00:12:07.870
state before you can do anything.

00:12:07.900 --> 00:12:11.700
If it's not, we throw an exception here.

00:12:13.180 --> 00:12:16.150
Since this is a C++ framework,
all the exceptions are caught

00:12:16.150 --> 00:12:17.430
up at the component layer.

00:12:17.440 --> 00:12:20.710
Error codes are produced based on that.

00:12:22.490 --> 00:12:25.880
In the codec,
once you know you're initialized,

00:12:25.930 --> 00:12:30.570
you've got to go figure
out how many packets are

00:12:30.570 --> 00:12:31.880
available in the input buffer.

00:12:31.880 --> 00:12:36.980
The IMA encoder is based off
of the simple codec class,

00:12:36.980 --> 00:12:42.240
so it's using the ring buffer
routines in its base class to figure

00:12:42.240 --> 00:12:44.320
out how big its input buffer is.

00:12:44.320 --> 00:12:48.340
Now, this codec is built to take only
a 16-bit integer as its input,

00:12:48.390 --> 00:12:51.790
because that's what the
IMA algorithm is defined to do.

00:12:56.240 --> 00:13:00.150
So then, once it figures out
where its input data is,

00:13:00.150 --> 00:13:03.200
and how much is there,
and whether or not there's

00:13:03.200 --> 00:13:07.040
actually enough input data that
we can produce an output packet,

00:13:07.040 --> 00:13:12.430
we find out how many packets the
client has asked us to produce,

00:13:12.430 --> 00:13:16.900
and then we go through,
we set a few things up,

00:13:16.990 --> 00:13:21.420
and then we end up calling the
encode channel routine that just

00:13:21.420 --> 00:13:25.800
steps through each buffer and
encodes each channel individually.

00:13:26.250 --> 00:13:29.610
Not the most efficient algorithm,
but it does the job.

00:13:31.220 --> 00:13:35.430
and the ENCODE channel is
a pretty involved routine,

00:13:35.430 --> 00:13:39.260
and the IMA algorithm is available
from the IMA website for those

00:13:39.270 --> 00:13:40.300
that are interested in it.

00:13:42.980 --> 00:13:47.430
That's pretty much all
there is to the Codec SDK.

00:13:47.480 --> 00:13:50.120
Next,
I'd like to bring up Doug Wyatt again.

00:13:50.120 --> 00:13:53.160
He's going to talk more about
Audio Units and everything that's

00:13:53.160 --> 00:13:55.160
new and wonderful about writing them.

00:13:55.190 --> 00:13:57.740
Doug Wyatt: Thanks, Jeff.

00:14:02.010 --> 00:14:07.530
Okay, in this section of the session,
I'm going to talk about Audio Units.

00:14:08.490 --> 00:14:12.140
and I will be talking about what they do,
how they're packaged,

00:14:12.140 --> 00:14:17.990
and although we'll touch on some uses
of Audio Units from the client side,

00:14:18.120 --> 00:14:24.400
I'm going to focus in on writing a
simple Audio Unit and an Audio Unit view,

00:14:24.400 --> 00:14:29.640
which is a new feature
we have for Jaguar,

00:14:29.640 --> 00:14:30.860
a user interface component
for an Audio Unit.

00:14:32.940 --> 00:14:37.530
So just to review for those of you
who haven't looked at Audio Units yet,

00:14:37.530 --> 00:14:41.130
the basic functionality,
its purpose is to provide

00:14:41.520 --> 00:14:46.130
some nice little modular piece
of audio signal processing.

00:14:46.200 --> 00:14:52.340
It can have an associated view plug-in,
a user interface for editing it.

00:14:52.340 --> 00:14:58.830
An Audio Unit can be connected to
other Audio Units with any number of

00:14:59.020 --> 00:15:02.590
input and output buses or connections.

00:15:02.810 --> 00:15:05.740
Audio Units use a pull model
when they're connected together,

00:15:05.740 --> 00:15:09.240
meaning that if you've got
a chain of Audio Units,

00:15:09.260 --> 00:15:12.680
you pull on the bottom one,
which will then pull on

00:15:12.680 --> 00:15:15.330
the one above it for input,
and so on up the chain

00:15:15.760 --> 00:15:17.690
until the input is obtained.

00:15:17.810 --> 00:15:21.020
And then from there,
it processes its output.

00:15:21.150 --> 00:15:23.690
And that's how AUGraph works.

00:15:24.290 --> 00:15:31.540
Audio Units operate on 32-bit floating
point numbers as the canonical format.

00:15:31.670 --> 00:15:34.990
It is theoretically possible
to write Audio Units that

00:15:35.180 --> 00:15:40.250
operate on other data formats,
but there's no guarantee that you'll

00:15:40.250 --> 00:15:44.020
be able to connect such Audio Units
with any other Audio Units since

00:15:44.060 --> 00:15:48.000
32-bit float is the canonical format.

00:15:49.770 --> 00:15:54.850
As Jeff was just mentioning,
we do use the Component

00:15:54.850 --> 00:16:01.020
Manager for packaging
Audio Units for various reasons,

00:16:01.020 --> 00:16:04.340
including our
responsibilities to QuickTime.

00:16:04.340 --> 00:16:06.380
And it's funny because I've
talked to developers about how

00:16:06.380 --> 00:16:08.960
we use the Component Manager,
and there's some really major

00:16:08.960 --> 00:16:13.180
developers who've never used
it and don't know how it works.

00:16:13.180 --> 00:16:16.140
And we're very pleasantly just
surprised to find out there's

00:16:16.140 --> 00:16:19.480
really only three functions they
need to know in order to use it:

00:16:19.620 --> 00:16:21.980
find a component,
open -- find next component,

00:16:22.010 --> 00:16:25.340
open a component, and close component.

00:16:26.930 --> 00:16:29.440
So once you've opened
the Audio Unit component,

00:16:29.560 --> 00:16:33.990
then you're free to issue,
from the client point of view,

00:16:34.130 --> 00:16:35.950
you're free to make
calls on the Audio Unit,

00:16:35.950 --> 00:16:38.350
like initialize, setting properties,
and so on.

00:16:41.080 --> 00:16:44.500
So from here on,
I'm going to focus on the

00:16:44.590 --> 00:16:48.230
process of writing an Audio Unit.

00:16:48.240 --> 00:16:52.700
The first thing you'll want to
think about is how many input and

00:16:52.700 --> 00:16:56.000
output buses your Audio Unit needs.

00:16:56.000 --> 00:17:04.230
It's important to bear in mind
that when we talk about a bus,

00:17:04.230 --> 00:17:04.990
it can actually be a multichannel bus.

00:17:05.830 --> 00:17:13.560
In 10.1, if there is a multichannel bus,
we're using interleaved formats.

00:17:13.560 --> 00:17:18.100
But we are working on a version 2
audio unit specification in which

00:17:18.100 --> 00:17:23.980
multichannel buses are represented
as an array of deinterleaved buffers.

00:17:24.090 --> 00:17:27.680
And I'll be explaining
more about that later.

00:17:28.640 --> 00:17:33.520
In any case,
the other first step in the API when you

00:17:33.580 --> 00:17:39.700
want to begin writing an Audio Unit is to
look through our base classes in our SDK,

00:17:39.700 --> 00:17:42.500
because we have a fairly
wide variety of them,

00:17:42.570 --> 00:17:47.890
and see if you can find one that
gives you the most leverage for free.

00:17:49.750 --> 00:17:54.860
So these are the main
base classes in our SDK,

00:17:54.930 --> 00:17:57.270
and I'll be talking about each of them

00:18:01.250 --> 00:18:04.800
At the root of the class
hierarchy is AUBase,

00:18:04.800 --> 00:18:12.240
and its main job is just to translate the
component entry selectors and get and set

00:18:12.490 --> 00:18:17.260
property calls into C++ virtual methods.

00:18:17.260 --> 00:18:21.430
These virtual methods have,
in many cases, default implementations by

00:18:21.430 --> 00:18:23.960
many of the base classes,
but of course,

00:18:23.960 --> 00:18:28.790
since they are virtual methods,
you're free to override them

00:18:28.790 --> 00:18:29.030
to customize the behavior.

00:18:29.960 --> 00:18:36.240
They manage AU Scopes and AU Elements,
which are objects that correspond

00:18:36.240 --> 00:18:40.250
to the scope and element concepts
that you'll see in the API.

00:18:40.380 --> 00:18:43.570
For instance,
whenever you set an Audio Unit parameter,

00:18:43.630 --> 00:18:48.260
you have to say which parameter
in which scope in which element.

00:18:48.270 --> 00:18:55.170
An element is simply a bus
number when you're talking about

00:18:55.170 --> 00:18:55.170
the input or the output scope.

00:18:55.720 --> 00:18:59.510
So you might be talking about
parameter three in the input

00:18:59.510 --> 00:19:03.600
scope element number zero,
for instance.

00:19:03.600 --> 00:19:07.600
And AUBase also handles a
lot of other housekeeping,

00:19:07.600 --> 00:19:10.890
as I said,
providing default implementations for

00:19:10.890 --> 00:19:14.790
the somewhat elaborate Audio Unit API.

00:19:15.850 --> 00:19:20.380
But as we'll see in the example later on,
that housekeeping that happens

00:19:20.380 --> 00:19:24.760
in the base class for you makes
it so that your actual signal

00:19:24.760 --> 00:19:27.760
processing code can be very simple.

00:19:29.120 --> 00:19:32.500
Okay, working our way down
in the class hierarchy,

00:19:32.500 --> 00:19:38.640
the first subclass of AUBase that
we'll talk about is AUEffectBase.

00:19:38.750 --> 00:19:41.180
This may be a little
simplistic for some algorithms,

00:19:41.280 --> 00:19:44.100
but for a lot of algorithms
it's perfectly useful.

00:19:44.100 --> 00:19:52.010
It assumes one input
bus and one output bus,

00:19:52.080 --> 00:19:54.300
and that they each have the
same number of channels.

00:19:54.340 --> 00:20:00.670
The effect I'm going to show
you later is a multi-tap delay,

00:20:00.680 --> 00:20:05.840
and it's implemented as an AUEffectBase.

00:20:05.840 --> 00:20:10.860
And the way it works,
AUEffectBase issues a virtual

00:20:10.860 --> 00:20:14.380
method call asking your derived
class to create what we call

00:20:14.540 --> 00:20:17.240
DSP kernel object for each channel.

00:20:17.240 --> 00:20:20.840
So if it's a stereo,
if your effect is invoked

00:20:20.840 --> 00:20:25.300
in a stereo context,
the base class will ask your subclass

00:20:25.330 --> 00:20:29.930
to create two kernel objects,
one to process each channel.

00:20:29.960 --> 00:20:35.170
And so then when it comes time for
your Audio Unit to render its audio,

00:20:35.290 --> 00:20:40.850
the base class can simply call
your two kernel objects separately,

00:20:40.850 --> 00:20:44.680
passing pointers to
mono buffers to process.

00:20:46.480 --> 00:20:51.140
And so that way you can write your
DSP code for algorithms where there

00:20:51.140 --> 00:20:55.430
isn't any relationships between what's
happening in the different channels.

00:20:55.530 --> 00:20:58.970
You can write your DSP code to
work in mono or stereo or five

00:20:59.140 --> 00:21:01.630
channel or whatever context.

00:21:03.150 --> 00:21:09.560
A slight specialization of
AU Effect Base is AU Inline Effect Base.

00:21:09.560 --> 00:21:12.600
And this just adds a small optimization.

00:21:12.600 --> 00:21:18.520
It pulls its input samples into the
same buffer into which your output

00:21:18.520 --> 00:21:20.010
samples are going to be rendered.

00:21:20.010 --> 00:21:23.390
So it's assuming that you're going
to process your samples in place,

00:21:23.390 --> 00:21:27.370
as you can in many algorithms,
and this is obviously a much

00:21:27.370 --> 00:21:32.340
better thing to do to the cache
when you're doing real-time DSP.

00:21:35.320 --> 00:21:38.300
Okay,
moving on to another subclass of AUBase.

00:21:38.300 --> 00:21:41.820
We have AUConverterBase.

00:21:41.820 --> 00:21:44.290
In our session on Tuesday,
we talked a bit about

00:21:44.290 --> 00:21:45.700
the Audio Converter.

00:21:45.810 --> 00:21:48.100
Jeff was just describing
the Audio Codecs,

00:21:48.190 --> 00:21:51.040
which are plug-ins into
the Audio Converter.

00:21:51.540 --> 00:21:55.220
So the job of the AU Converter
base is simply to wrap up one of

00:21:55.220 --> 00:22:00.850
our Audio Converter objects in
such a way that it can be used in

00:22:00.850 --> 00:22:03.200
the middle of an Audio Unit chain.

00:22:03.200 --> 00:22:08.450
So the emphasis -- the usefulness
of the Audio Converter is a bit

00:22:08.490 --> 00:22:10.220
different in the world of Audio Units.

00:22:10.400 --> 00:22:13.800
There don't tend to be many
needs for format conversions,

00:22:13.800 --> 00:22:17.720
except in some special cases,
which I'll get to in a minute.

00:22:17.880 --> 00:22:20.400
But in the middle of a chain,
you'll probably only be using it for

00:22:20.480 --> 00:22:25.400
things like doing sample rate conversion,
deinterleaving and interleaving,

00:22:25.400 --> 00:22:28.560
and channel mapping,
which means adding or removing

00:22:28.560 --> 00:22:32.330
channels or rearranging the order
of channels like you may have to do

00:22:32.330 --> 00:22:35.460
sometimes with multichannel formats.

00:22:38.820 --> 00:22:43.140
Deriving from AU Converter Base,
we have AU Output Base,

00:22:43.250 --> 00:22:49.830
which we use internally as the base
class for all of our output units.

00:22:50.550 --> 00:22:54.410
Output units have some
extra virtual methods,

00:22:54.410 --> 00:22:55.870
start and stop.

00:22:55.870 --> 00:23:00.360
Those are also component
selectors for output units.

00:23:00.360 --> 00:23:04.650
For hardware output units it's
obvious start and stop mean

00:23:04.650 --> 00:23:07.360
start and stop the hardware.

00:23:07.360 --> 00:23:12.640
You could write a audio output
unit deriving from AU output base

00:23:12.640 --> 00:23:16.900
that wrote audio to an audio file,
and you might use the start

00:23:16.900 --> 00:23:20.900
and stop methods as the place
where you would open the file,

00:23:21.040 --> 00:23:24.280
prepare it, set up the header,
and when you close the

00:23:24.280 --> 00:23:27.640
file in the stop method,
then you could fix up the chunk

00:23:27.640 --> 00:23:31.290
sizes and whatnot that you have
to do when writing AIFF files,

00:23:31.290 --> 00:23:32.220
for example.

00:23:34.110 --> 00:23:39.110
The one other thing that's really useful
about the output unit is that since

00:23:39.110 --> 00:23:44.000
it derives from AU Converter Base,
it gives you very easy

00:23:44.000 --> 00:23:47.060
access to format conversions.

00:23:47.110 --> 00:23:51.440
There are a number of different
ways you could use this.

00:23:51.460 --> 00:23:56.410
For instance, if your application is just
generating audio from -- maybe

00:23:56.420 --> 00:24:01.890
it's playing an integer audio file,
you can set up the output audio

00:24:01.890 --> 00:24:04.330
unit to accept integer input.

00:24:04.390 --> 00:24:11.030
Just say, okay, my input stream format
is 16-bit stereo integer.

00:24:11.100 --> 00:24:16.690
And by doing that,
the AU output base will automatically

00:24:16.700 --> 00:24:21.370
insert a converter into the chain,
converting that integer format into

00:24:21.370 --> 00:24:25.030
the floating point file format,
or data format,

00:24:25.030 --> 00:24:28.340
which is typically required by the HAL.

00:24:28.340 --> 00:24:29.960
And conversely,
if your output unit is running

00:24:29.960 --> 00:24:30.500
from AU Converter Base,
it will automatically convert

00:24:30.500 --> 00:24:31.740
that data format into a file.

00:24:31.740 --> 00:24:31.740
So that's one way to do it.

00:24:31.740 --> 00:24:34.170
The other way is writing to
a file in an integer format.

00:24:36.880 --> 00:24:41.090
You might be receiving your
samples from another Audio Unit,

00:24:41.090 --> 00:24:45.720
which is generating float,
and your file format

00:24:45.860 --> 00:24:47.980
will probably be integer.

00:24:47.980 --> 00:24:52.100
And so the converter that's built
into the output unit can perform

00:24:52.100 --> 00:24:56.600
that conversion for you with your
having to do almost no work at all.

00:24:56.600 --> 00:25:03.350
It's all handled in the base class and
how your clients set the input and output

00:25:03.350 --> 00:25:03.990
stream formats for your Audio Unit.

00:25:08.250 --> 00:25:12.700
Okay, another subclass of AE base
is music device base,

00:25:12.700 --> 00:25:18.070
and this one is fairly substantial
and separate because music device is a

00:25:18.070 --> 00:25:20.890
completely separate type of component.

00:25:21.040 --> 00:25:24.160
It's based on audio unit,
but it has a large number

00:25:24.160 --> 00:25:28.080
of extra selectors,
allowing you to control the audio unit

00:25:28.190 --> 00:25:34.200
by sending it MIDI events and what we
call extended note and control events.

00:25:34.200 --> 00:25:38.220
For example,
our extended note events allow you

00:25:38.260 --> 00:25:45.060
to specify pitch more precisely
than with 127 MIDI note numbers.

00:25:45.060 --> 00:25:49.990
You can specify fractional
pitches and so on.

00:25:50.540 --> 00:25:54.400
The Music Device, though,
is an Audio Unit,

00:25:54.400 --> 00:25:56.890
since that's what it derives from.

00:25:56.900 --> 00:26:00.700
So it renders audio just
like any other Audio Unit.

00:26:00.700 --> 00:26:03.400
You make the same calls to
it to have it render audio.

00:26:03.400 --> 00:26:06.650
And so you can plug it
into chains of Audio Units.

00:26:06.790 --> 00:26:09.660
And we see it as a really good
way for people to implement

00:26:09.810 --> 00:26:11.400
software synthesizers.

00:26:11.400 --> 00:26:14.780
In the system,
we have our own downloadable sample

00:26:15.010 --> 00:26:17.160
software synthesizer built in.

00:26:17.160 --> 00:26:19.380
You'll see it as a music device.

00:26:19.390 --> 00:26:23.400
And I forget its signature offhand,
its component ID,

00:26:23.400 --> 00:26:24.200
but it's in the header files.

00:26:24.200 --> 00:26:26.040
You'll see it there.

00:26:30.510 --> 00:26:34.770
Okay, so the AU base class,
returning back to the top

00:26:34.770 --> 00:26:39.290
of the class hierarchy,
it manages some other objects

00:26:39.290 --> 00:26:42.780
called AU Scope and AU Element.

00:26:43.470 --> 00:26:49.750
As I mentioned before, in our API,
we talk about parameters as

00:26:49.760 --> 00:26:54.610
existing within elements of scopes.

00:26:54.730 --> 00:26:59.840
And so these objects are where
the parameter values are stored.

00:27:00.160 --> 00:27:05.460
You can subclass these objects if you
want to store any additional state per

00:27:05.460 --> 00:27:09.180
input or output bus in your Audio Unit.

00:27:09.530 --> 00:27:13.410
The other thing they do is manage
the stream formats for your

00:27:13.410 --> 00:27:16.920
Audio Unit's inputs and outputs.

00:27:16.920 --> 00:27:19.710
There are variables there that say, okay,
this is the current stream

00:27:19.710 --> 00:27:22.840
format for input number one,
and that lets us do all

00:27:22.840 --> 00:27:24.960
of the format negotiation.

00:27:24.960 --> 00:27:28.300
Well, a lot of the format negotiation,
your subclass has many

00:27:28.400 --> 00:27:31.200
ways it can hook into that,
but when you're connecting

00:27:31.200 --> 00:27:35.660
two Audio Units,
that format negotiation is

00:27:36.030 --> 00:27:39.410
handled largely by the base class.

00:27:40.940 --> 00:27:47.830
AU Element has a subclass,
AU Input Element,

00:27:47.850 --> 00:27:52.560
which obviously operates
on your input buses.

00:27:52.560 --> 00:27:55.940
Its main job is to obtain
your incoming audio data,

00:27:56.000 --> 00:28:03.510
whether it's from an upstream audio unit
or a client-supplied callback function.

00:28:05.030 --> 00:28:10.200
And we also have AU Output Element,
and its main job is to -- or

00:28:10.200 --> 00:28:14.740
specialization of AU Element,
I should say -- is to maintain

00:28:14.740 --> 00:28:21.300
some buffers into which that
output's audio is rendered.

00:28:21.300 --> 00:28:25.400
And there's some reasons -- if you've
looked at some of our audio code,

00:28:25.400 --> 00:28:29.300
you'll notice that we do
support fan-out connections,

00:28:29.300 --> 00:28:34.690
meaning one audio unit can be connected
to two extra audio units in a chain.

00:28:34.830 --> 00:28:40.800
And in order to support that,
we provide a way for

00:28:40.800 --> 00:28:45.950
an AU Output Element,
a cache by which,

00:28:45.950 --> 00:28:50.390
when the first destination
unit pulls and says,

00:28:50.400 --> 00:28:54.000
"Hey, give me your audio," it gets
rendered into this cache buffer

00:28:54.000 --> 00:28:57.110
so that when the second one asks,
"Hey, give me your audio," it's

00:28:57.200 --> 00:29:00.430
already been rendered,
and that cached audio

00:29:00.590 --> 00:29:03.200
can just be passed back.

00:29:06.110 --> 00:29:09.000
So having reviewed the
basic concepts in the API,

00:29:09.000 --> 00:29:12.390
I'd like to go over to machine five here.

00:29:14.880 --> 00:29:16.800
is hopefully awake.

00:29:16.800 --> 00:29:21.800
Okay, it's awake now.

00:29:21.800 --> 00:29:25.800
Clean up after the previous session.

00:29:29.300 --> 00:29:34.870
And I'd like to just walk you
through the process of writing a

00:29:34.870 --> 00:29:39.820
simple multi-tap delay audio unit.

00:29:40.250 --> 00:29:45.200
In Jeff's session,
Jeff's portion of this session,

00:29:45.300 --> 00:29:48.680
he showed a lot of the
implementation of the base classes,

00:29:48.680 --> 00:29:52.410
but unfortunately there's a lot
of code in the base classes.

00:29:52.520 --> 00:29:55.780
So I've just given you an overview
of what they are and what they do,

00:29:55.890 --> 00:29:58.800
and you can explore them in
more detail and ask us questions

00:29:58.800 --> 00:30:00.210
about them on the API list.

00:30:00.430 --> 00:30:06.810
But right now I'd like to just focus on
how little code is actually necessary

00:30:06.810 --> 00:30:08.420
to create a functioning audio unit.

00:30:08.800 --> 00:30:13.100
The Audio Unit I chose to wrote,
since I'm not a great

00:30:13.190 --> 00:30:20.100
DSP programmer by any means,
is just a simple multi-tap delay.

00:30:20.100 --> 00:30:28.340
It derives from AU Effect Base.

00:30:28.340 --> 00:30:28.340
I'm going to close this.

00:30:32.000 --> 00:30:35.090
The first thing I'd actually
like to look at is since we

00:30:35.090 --> 00:30:41.820
are writing a component here,
we do have to provide the component

00:30:41.820 --> 00:30:46.770
manager with a few resources
in order to find our component.

00:30:46.800 --> 00:30:49.250
And when I came to Apple,
this process confused

00:30:49.300 --> 00:30:50.860
me enough that I said,
"Okay,

00:30:50.860 --> 00:30:56.820
I just want to have a really easy way to
do this." And so by defining about eight

00:30:56.980 --> 00:31:01.510
macros here and then including this file,
which contains the magic,

00:31:01.510 --> 00:31:05.220
I get the three or four resources
that are necessary to define my

00:31:05.220 --> 00:31:09.670
Audio Unit component and in such a way
that the component manager will find it.

00:31:14.580 --> 00:31:21.230
So the other little bit of component
manager magic is all encapsulated

00:31:21.410 --> 00:31:26.080
into this one macro here,
component entry.

00:31:26.360 --> 00:31:33.360
What this expands into is the single
function entry into your component,

00:31:33.500 --> 00:31:38.230
which we'll call a dispatch function
for all the component entry selectors,

00:31:38.230 --> 00:31:43.470
and that's where the translation
of all those component entries

00:31:43.540 --> 00:31:46.930
into C++ virtual methods is made.

00:31:53.510 --> 00:31:58.740
The next thing I'd like to show you here.

00:31:58.740 --> 00:32:05.920
So the one virtual method that's really
important and required to be override

00:32:05.980 --> 00:32:08.420
in AU Effect Base is new kernel.

00:32:08.530 --> 00:32:13.070
As you remember,
the kernel object is what gets called by

00:32:13.080 --> 00:32:20.180
the base class to process each channel
in a stereo or multichannel situation.

00:32:20.990 --> 00:32:23.860
So I'm simply,
when asked to create a kernel,

00:32:23.860 --> 00:32:28.130
I'm creating an instance
of a multi-tap kernel.

00:32:28.670 --> 00:32:32.700
Which is another class in this file here.

00:32:32.700 --> 00:32:35.020
These properties we don't
need to look at right now.

00:32:35.020 --> 00:32:36.720
They play into the UI.

00:32:39.720 --> 00:32:45.940
And here's the MultiTap Kernel Class.

00:32:45.940 --> 00:32:48.510
And it's just maintaining a
little bit of state per channel,

00:32:48.560 --> 00:32:51.820
like the maximum number
of frames of delay.

00:32:51.820 --> 00:32:53.900
We've got a five-second
maximum delay time,

00:32:53.900 --> 00:32:57.160
and so I'm translating that into
some number of sample frames.

00:32:57.160 --> 00:33:00.770
I'm allocating a buffer
for the number of delays,

00:33:00.770 --> 00:33:05.350
and I'm initializing some state
as to how I'm using that buffer

00:33:05.350 --> 00:33:07.660
as a ring buffer for the delay.

00:33:09.930 --> 00:33:20.910
While we're looking at the kernel,
its processing function gets called with

00:33:21.100 --> 00:33:26.250
a pointer to a source buffer of samples,
a pointer to a destination

00:33:26.290 --> 00:33:30.000
number of samples,
a number of frames to process,

00:33:30.020 --> 00:33:34.060
and here's the trick by which we can
make this one function work with both

00:33:34.060 --> 00:33:37.480
interleaved and deinterleaved data.

00:33:37.480 --> 00:33:43.260
We're told how many interleaved channels
are in the input and output streams.

00:33:44.610 --> 00:33:47.750
So if it's stereo interleaved,
in-num channels will be two,

00:33:47.850 --> 00:33:51.730
and we'll be expected to go skipping
through the samples two at a time,

00:33:51.740 --> 00:33:54.840
both on source and input.

00:33:54.840 --> 00:33:57.770
And we recognize this isn't the
most efficient way to do things,

00:33:57.790 --> 00:34:00.710
and I'll be discussing more
about interleaved versus

00:34:00.740 --> 00:34:02.640
deinterleaved issues later.

00:34:02.640 --> 00:34:05.230
But this is how it works for interleaved.

00:34:06.140 --> 00:34:08.580
And so at processing time,
we just do a little work to look

00:34:08.660 --> 00:34:12.040
up the parameters of the delay.

00:34:12.040 --> 00:34:15.380
The first parameter -- actually,
I've scrolled down too far.

00:34:15.390 --> 00:34:18.960
The first parameter -- no, I haven't.

00:34:18.960 --> 00:34:21.960
Okay, first we're looking --
this is a five-tap delay,

00:34:21.960 --> 00:34:27.520
and for each delay tap,
we just have a level and a delay time.

00:34:27.520 --> 00:34:32.410
So we're issuing these methods on --
which are implemented in the base class.

00:34:32.410 --> 00:34:34.650
Give me this parameter by its number.

00:34:34.690 --> 00:34:38.100
We're finding out the delay time,
which is converting a

00:34:38.100 --> 00:34:40.000
percentage to a ratio.

00:34:40.000 --> 00:34:44.150
And here we're finding out what
the delay time corresponds --

00:34:44.160 --> 00:34:48.240
which is in seconds -- corresponds
to in terms of number of samples,

00:34:48.240 --> 00:34:49.330
limiting it.

00:34:51.140 --> 00:34:55.540
This is all pretty
straightforward DSP code.

00:34:55.540 --> 00:34:58.820
As we go through the input buffer,
we're writing the incoming

00:34:58.820 --> 00:35:00.340
data to the delay line.

00:35:00.420 --> 00:35:05.800
We're saving a fraction of the
input sample as our dry signal.

00:35:05.870 --> 00:35:15.710
Then we're walking through the five
delay taps and mixing from each

00:35:15.710 --> 00:35:15.990
delay tap into the output signal.

00:35:17.040 --> 00:35:18.930
So that's the DSP code.

00:35:18.940 --> 00:35:20.860
I think there's only about one
more function in here I'd like

00:35:20.860 --> 00:35:27.940
to show you at this point,
which is get parameter info.

00:35:28.400 --> 00:35:32.460
So far we've seen pretty much
everything that we needed to do.

00:35:32.790 --> 00:35:35.120
Actually, I did forget one thing.

00:35:35.120 --> 00:35:40.670
The constructor of the MultiTapAU,
which is the Audio Unit base class,

00:35:40.680 --> 00:35:43.130
here's where we have to
initialize the parameters,

00:35:43.130 --> 00:35:46.280
because if we don't do this here,
the parameters won't exist,

00:35:46.280 --> 00:35:50.230
and when we go later to find
out what their values are,

00:35:50.360 --> 00:35:51.780
we'll miss them.

00:35:51.980 --> 00:35:53.760
Here we see what the parameters are.

00:35:53.760 --> 00:35:56.970
There's the wet/dry mix,
and for each of the five taps,

00:35:57.100 --> 00:36:00.210
there's a delay time and a level.

00:36:00.940 --> 00:36:06.430
This call to create elements simply
makes sure that the scope in which

00:36:06.430 --> 00:36:12.160
I'm creating these parameters exists
before I try to create the parameters.

00:36:12.370 --> 00:36:16.290
Okay, so that's the DSP side
of this Audio Unit.

00:36:16.290 --> 00:36:18.140
That's all there is to it.

00:36:18.140 --> 00:36:23.300
There's a few more things that
are all user interface related.

00:36:23.770 --> 00:36:28.260
One method that we have to
override is Get Parameter Info

00:36:28.260 --> 00:36:32.490
so that a generic user interface,
which is how I'm going to

00:36:32.490 --> 00:36:36.220
show this audio unit first,
so that a generic interface can know

00:36:36.220 --> 00:36:41.690
what the parameters are and what their
ranges are and what their units are.

00:36:41.750 --> 00:36:45.760
And so here in Get Parameter Info,
our caller is specifying

00:36:45.810 --> 00:36:49.580
the parameter and scope,
and we're passing it back this

00:36:49.580 --> 00:36:52.560
Audio Unit Parameter Info structure.

00:36:54.970 --> 00:36:58.340
And since we only actually
have three parameters here,

00:36:58.340 --> 00:37:00.180
this is a short function.

00:37:00.270 --> 00:37:06.020
We have the dry/wet mix,
which goes from 0 to 100%. We have,

00:37:06.020 --> 00:37:09.040
for each of the five taps, a delay time.

00:37:09.150 --> 00:37:11.690
and the level.

00:37:11.690 --> 00:37:14.210
And so this function here,

00:37:14.200 --> 00:37:21.170
This is all that the generic
Audio Unit view will need in order to

00:37:21.410 --> 00:37:23.930
Work with our Audio Unit.

00:37:23.930 --> 00:37:27.140
So I've built this -- I'm
not going to take the time

00:37:27.140 --> 00:37:31.150
to compile it all right now,
but I've built this Audio Unit into

00:37:31.150 --> 00:37:37.860
a component bundle and I've installed
it into system library components.

00:37:37.860 --> 00:37:39.370
And I have a test application.

00:37:39.390 --> 00:37:40.430
How are we doing for time?

00:37:40.480 --> 00:37:44.400
Should I go through the test
step in a bit of detail?

00:37:45.110 --> 00:37:48.760
Yeah, I'll just show you quickly,
because it shows off

00:37:48.760 --> 00:37:50.150
some of our other APIs.

00:37:55.280 --> 00:38:00.370
So it'll help you understand what's
going on when you hear the program,

00:38:00.480 --> 00:38:01.120
too.

00:38:01.120 --> 00:38:06.390
What this program does is
create an Audio Unit graph.

00:38:07.080 --> 00:38:12.970
which connects a DLS software synthesizer
to the multi-tap delay that I just wrote.

00:38:13.070 --> 00:38:18.000
And from that, we go straight to the
default audio output unit.

00:38:18.000 --> 00:38:23.570
This bit of code here creates
nodes in the graph for those three

00:38:23.570 --> 00:38:28.560
components that get specified
by their component descriptions.

00:38:29.300 --> 00:38:33.140
Having created the nodes,
then we just establish connections

00:38:33.230 --> 00:38:37.110
between them from the synth to the delay,
and then from the delay

00:38:37.270 --> 00:38:38.600
to the output node.

00:38:38.700 --> 00:38:41.670
We open the graph, we initialize it.

00:38:41.810 --> 00:38:48.260
And then we call getNodeInfo to get from
the AU graph structure to the actual

00:38:48.260 --> 00:38:50.800
audio unit that we're going to talk to.

00:38:50.860 --> 00:38:52.580
For instance,
we're going to send notes to the synth,

00:38:52.700 --> 00:38:55.420
we're going to send parameter
changes to the delay.

00:38:55.420 --> 00:38:57.700
I don't know if there's a reason
why I'm hanging on to the output.

00:38:57.700 --> 00:38:59.810
Probably none.

00:39:00.340 --> 00:39:03.460
But in any case, we've built the graph.

00:39:03.540 --> 00:39:06.060
It's all connected and it's ready to run.

00:39:06.160 --> 00:39:10.090
And it's doing DSP at this point,
except that nobody's actually

00:39:10.090 --> 00:39:11.620
generating any audio.

00:39:11.700 --> 00:39:16.140
We've got a synthesizer and
nobody's controlling it.

00:39:16.300 --> 00:39:19.490
So another function in here.

00:39:22.000 --> 00:39:26.650
This is just a quick whirlwind tour of
the music sequencing APIs and how you

00:39:26.650 --> 00:39:31.060
can build up songs to play on the fly.

00:39:31.060 --> 00:39:33.160
I create a music sequence object.

00:39:33.260 --> 00:39:34.890
I get its tempo track.

00:39:34.910 --> 00:39:36.760
I set its tempo.

00:39:36.830 --> 00:39:40.940
At time zero,
the tempo is 120 beats per minute.

00:39:40.960 --> 00:39:45.550
Then I add a track into which I'm
going to add all of my other events.

00:39:46.790 --> 00:39:49.590
On MIDI Channel 0,
I'm going to have a piano sound

00:39:49.720 --> 00:39:51.660
with a program change message.

00:39:51.750 --> 00:39:55.270
On MIDI Channel 1,
I've got an electric piano.

00:39:56.280 --> 00:40:00.920
I'm going to pan channel one
hard left and pan channel two

00:40:01.030 --> 00:40:05.180
hard right so that you can really
hear for sure that this effect is

00:40:05.240 --> 00:40:08.420
operating in total stereo manner.

00:40:08.640 --> 00:40:11.210
And then I'm going to
randomly generate some notes.

00:40:11.230 --> 00:40:13.870
And although I kind of
like totally random notes,

00:40:13.870 --> 00:40:15.860
I actually made this less random.

00:40:15.860 --> 00:40:19.740
I'm generating arpeggios,
which are in various keys.

00:40:19.840 --> 00:40:20.840
But that's what this loop is doing.

00:40:20.840 --> 00:40:26.190
It's just generating some nice pretty
patterns of notes and adding the events

00:40:26.320 --> 00:40:29.230
to the music track that we created.

00:40:29.970 --> 00:40:32.660
So now we've got a
sequence full of notes,

00:40:32.710 --> 00:40:35.590
program change at the beginning, pan.

00:40:36.720 --> 00:40:39.420
Then I can connect the
sequence to the graph of audio

00:40:39.420 --> 00:40:41.710
units that I created earlier.

00:40:41.780 --> 00:40:45.050
I can assign this track
full of MIDI events to the

00:40:45.050 --> 00:40:47.640
synthesizer node in that graph.

00:40:47.640 --> 00:40:50.930
I can create a music player object.

00:40:51.160 --> 00:40:56.340
Then I assign that player to play
the sequence I've just created.

00:40:56.410 --> 00:40:59.460
And then, boom,
I call MusicPlayerStart and I hear noise.

00:40:59.510 --> 00:41:01.780
So let's see what that sounds like.

00:41:14.210 --> 00:41:16.920
So we can hear,
this is just the dry sound.

00:41:16.930 --> 00:41:18.220
Do you want it louder?

00:41:18.220 --> 00:41:20.220
OK.

00:41:20.220 --> 00:41:20.720
OK.

00:41:20.720 --> 00:41:22.540
Can you hear it OK?

00:41:22.890 --> 00:41:25.890
So we don't have my effect,
well we do have my effect

00:41:25.980 --> 00:41:27.480
in the signal chain.

00:41:27.480 --> 00:41:29.800
Here's the generic view for the effect.

00:41:29.800 --> 00:41:34.800
You can see that the level
on all five taps is zero.

00:41:34.800 --> 00:41:40.200
So let's get some delays going.

00:41:52.900 --> 00:42:06.250
Okay, so that's how easy it was to create
an Audio Unit to do a multi-tap delay

00:42:06.680 --> 00:42:11.220
with the generic user interface.

00:42:11.220 --> 00:42:15.170
Back to the slides, please.

00:42:16.880 --> 00:42:20.860
So a little later I'll show you how I've
created a custom user interface for that,

00:42:20.930 --> 00:42:27.420
but first I've left some other loose
ends that we should cover first.

00:42:27.720 --> 00:42:33.120
There's been some concern about the way
we've been using interleaved data as

00:42:33.120 --> 00:42:37.680
our canonical format within Audio Units.

00:42:37.720 --> 00:42:39.940
We went to a developer kitchen
a couple of months ago,

00:42:39.940 --> 00:42:44.320
and the majority of the room
told us they'd really rather

00:42:44.320 --> 00:42:48.690
be working in deinterleaved
data instead of interleaved.

00:42:48.690 --> 00:42:48.690
So,

00:42:49.100 --> 00:42:56.100
And we have some other things we want
to do to solidify and clarify the specs.

00:42:56.100 --> 00:43:01.320
We're going to work towards a version
2 specification of the Audio Unit API.

00:43:01.420 --> 00:43:04.250
The main two differences being
that we're going to change the

00:43:04.340 --> 00:43:07.540
way we describe the components,
and we're going to have to pass

00:43:07.540 --> 00:43:12.770
a different data structure to
support the interleaved buffers.

00:43:13.050 --> 00:43:18.390
The component description
differences in the version 1 API,

00:43:18.520 --> 00:43:24.180
as we've shipped in 10.1,
we use the AUNT component type,

00:43:24.320 --> 00:43:30.720
and we also enforce the use
of certain component subtypes.

00:43:30.780 --> 00:43:37.290
We got some feedback that we were
taking up 64 out of the 96 bits

00:43:37.300 --> 00:43:39.700
available for describing a component.

00:43:39.880 --> 00:43:43.400
So in the version 2 API,
we're only going to use

00:43:43.470 --> 00:43:50.700
the component's type,
AUFX, for instance, instead of AUNTEFCT.

00:43:50.700 --> 00:43:56.680
And we'll leave the manufacturer and
subtype available for developers.

00:43:59.450 --> 00:44:02.940
As for the differences in the
rendering part of your code,

00:44:02.940 --> 00:44:08.730
as I mentioned, in the version one API,
we're using interleaved buffers anywhere

00:44:08.730 --> 00:44:11.410
you've got a multichannel stream.

00:44:11.590 --> 00:44:16.420
An interleaved buffer can fit
into our audio buffer structure.

00:44:16.610 --> 00:44:18.910
But in the version 2 API,
in order to support

00:44:19.010 --> 00:44:21.050
deinterleaved buffers,
we need to use a

00:44:21.050 --> 00:44:24.460
different data structure,
which is an audio buffer list.

00:44:24.620 --> 00:44:27.060
Fortunately,
this isn't too traumatic a change,

00:44:27.150 --> 00:44:30.730
because an audio buffer list
has already been defined to be

00:44:30.730 --> 00:44:32.690
an array of an audio buffer.

00:44:32.790 --> 00:44:38.380
These are data structures that
are in the HAL API as well

00:44:38.380 --> 00:44:38.380
as being used by Audio Units.

00:44:38.980 --> 00:44:42.140
I'll go into the details
of this a bit later on,

00:44:42.140 --> 00:44:45.390
but the main changes are
that the render slice,

00:44:45.390 --> 00:44:48.660
Audio Unit render slice
becomes Audio Unit render,

00:44:48.660 --> 00:44:52.950
and there's some callback functions
from the Audio Unit that are

00:44:53.030 --> 00:44:55.020
going to change accordingly.

00:44:55.890 --> 00:45:01.940
And just to reiterate why we're
making this somewhat dramatic change,

00:45:02.110 --> 00:45:05.800
It seemed like developers
really wanted us to do this.

00:45:05.800 --> 00:45:09.950
Better to do it sooner than later,
after there are even more audio

00:45:10.080 --> 00:45:12.270
units written in the world.

00:45:12.280 --> 00:45:17.050
It seems like a lot of people's
DSP code that they've already written,

00:45:17.130 --> 00:45:24.910
this may be partially due to VST,
but there's probably also

00:45:24.910 --> 00:45:24.910
some cache efficiency reasons.

00:45:25.040 --> 00:45:27.660
A lot of existing code
uses deinterleaved buffers,

00:45:27.760 --> 00:45:32.350
and we respect that and want to
make that the canonical format.

00:45:32.830 --> 00:45:38.360
And it does offer us some optimization
opportunities when we're munging channels

00:45:38.360 --> 00:45:41.460
around in multi-channel situations.

00:45:44.450 --> 00:45:48.270
So for you as the potential
author of an Audio Unit,

00:45:48.270 --> 00:45:53.620
here are some implications
of those changes.

00:45:53.620 --> 00:45:57.000
You may have a reason to
support the version 1 API if

00:45:57.080 --> 00:46:02.750
you need to ship on Mac OS 10.1,
if you want to depend on the

00:46:02.750 --> 00:46:05.580
Audio Units that we shipped on 10.1.

00:46:05.580 --> 00:46:11.020
But at some point after Jaguar,
any new Audio Units that

00:46:11.200 --> 00:46:14.380
Apple ships will only be published
with the new component type.

00:46:14.400 --> 00:46:18.380
which means they only
support the new API.

00:46:18.770 --> 00:46:22.490
But in order to make this
transition as painless as possible,

00:46:22.490 --> 00:46:26.460
we're going to do as much as we can
to make it possible to write one

00:46:26.460 --> 00:46:29.100
Audio Unit that conforms to both APIs.

00:46:29.100 --> 00:46:34.050
And the way we can do this is by
having a couple of different ways.

00:46:34.540 --> 00:46:38.290
telling you how to use component
aliases to tell the component

00:46:38.290 --> 00:46:43.090
manager there's two components here,
but actually those two components

00:46:43.090 --> 00:46:46.920
point to the same piece of code,
which is your Audio Unit.

00:46:47.040 --> 00:46:50.130
And that piece of code can
find out which component

00:46:50.130 --> 00:46:54.660
description was used to open it,
and from there it can know, okay,

00:46:54.700 --> 00:46:59.420
I'm being expected to operate in
version one versus version two mode.

00:47:00.130 --> 00:47:03.960
The other thing we can do,
we can do a lot of work in our base

00:47:03.960 --> 00:47:08.160
classes to hide the differences.

00:47:08.570 --> 00:47:13.410
Our current SDK doesn't support this,
but we're very close to being done with

00:47:13.500 --> 00:47:20.610
the new SDK where essentially everything
is being done with version 2 in mind and

00:47:20.610 --> 00:47:26.270
version 1 is being implemented as -- the
old version 1 calls are being implemented

00:47:26.390 --> 00:47:28.720
in terms of the new version 2 ones.

00:47:28.720 --> 00:47:34.890
And since the audio buffer list and
audio buffer structures are so similar,

00:47:34.890 --> 00:47:34.890
that turned out to be not too much work.

00:47:37.320 --> 00:47:41.430
And in fact, if you derive from
AU Effect Base or AU Converter Base,

00:47:41.470 --> 00:47:45.380
the change may be completely
transparent to you because these

00:47:45.380 --> 00:47:49.120
details of interleaving and
deinterleaving are happening on the

00:47:49.230 --> 00:47:52.920
base classes below you or above you,
depending upon how you look

00:47:53.010 --> 00:47:54.540
at the class hierarchy.

00:47:55.810 --> 00:48:00.520
From the client point of view,
you can't mix the version one and

00:48:00.520 --> 00:48:03.380
version two types in an audio unit graph.

00:48:03.380 --> 00:48:09.790
It would just be too difficult
to assume that any given,

00:48:09.910 --> 00:48:12.980
well, if that audio unit had
both personalities,

00:48:12.980 --> 00:48:16.180
you would see it as a version, you know,
you'd be able to see, okay,

00:48:16.270 --> 00:48:18.850
I've got two version two units
here that I can connect up.

00:48:18.860 --> 00:48:21.520
But if you connect up a version
one to a version two unit,

00:48:21.600 --> 00:48:24.070
you would almost certainly
get a stream format mismatch.

00:48:24.140 --> 00:48:26.610
One of them would be saying, hey,
I want the interleaved input,

00:48:26.610 --> 00:48:31.360
and the other one would be saying, hey,
I'm providing interleaved output.

00:48:31.360 --> 00:48:34.030
You may be able to insert a
converter to get around that,

00:48:34.030 --> 00:48:37.640
but in the AU graph API,
we're going to be strict about

00:48:37.640 --> 00:48:39.350
that and just not permit it.

00:48:40.950 --> 00:48:45.210
From the client point of view,
you'll see the changes to use

00:48:45.210 --> 00:48:49.240
the Audio Buffer list instead
of the Audio Buffer in your

00:48:49.240 --> 00:48:51.900
callbacks from the Audio Units.

00:48:52.080 --> 00:48:55.680
And you'll be using the
Audio Unit Render and Audio Unit Render,

00:48:55.680 --> 00:48:58.880
instead of Audio Unit Render Slice.

00:49:02.150 --> 00:49:07.110
Okay, moving back on towards
the world of Audio Unit

00:49:07.350 --> 00:49:09.560
Graphic User Interfaces.

00:49:09.560 --> 00:49:15.000
We have a new component type
called Audio Unit Carbon View,

00:49:15.000 --> 00:49:21.700
which wraps up an entire user interface
for an Audio Unit into a component also.

00:49:21.770 --> 00:49:25.300
We have a generic View component,
which I just showed you.

00:49:25.300 --> 00:49:33.110
It essentially just interrogates all of
the Audio Unit parameters in the global

00:49:33.110 --> 00:49:34.300
scope and shows them one after another.

00:49:34.410 --> 00:49:40.200
But we have defined a property for
the Audio Unit to tell its host,

00:49:40.320 --> 00:49:44.210
here are the component descriptions,
there can be one or more,

00:49:44.410 --> 00:49:47.250
for the view components
that know how to edit me.

00:49:47.390 --> 00:49:51.400
And I'll show you that in
my sample unit in a moment.

00:49:51.400 --> 00:49:56.620
So what the Audio Unit Carbon View does,
its job is simply to

00:49:56.620 --> 00:50:00.520
create a Carbon User Pane,
which can be embedded anywhere

00:50:00.520 --> 00:50:02.400
in the host application's window.

00:50:02.850 --> 00:50:10.430
That User Pane,
I think it's a Carbon term for

00:50:10.430 --> 00:50:10.430
just a view container of some sort.

00:50:10.700 --> 00:50:14.820
That view can contain either
more Carbon controls or

00:50:14.820 --> 00:50:17.000
a completely custom UI.

00:50:17.000 --> 00:50:20.770
It's completely up to you as
long as you use Carbon events to

00:50:20.770 --> 00:50:23.560
receive your events from the OS.

00:50:23.560 --> 00:50:26.730
We didn't want to get into
any of the messiness of OS 9

00:50:26.740 --> 00:50:29.900
and the way that applications,
for instance,

00:50:29.940 --> 00:50:33.880
VST hosts would have to dispatch
mouse and keyboard events and window

00:50:33.880 --> 00:50:35.840
up to -- I don't know what else.

00:50:35.960 --> 00:50:39.840
But a lot of plug-in models, you know,
people had to,

00:50:39.900 --> 00:50:44.950
in their host applications,
manually dispatch events from the

00:50:44.960 --> 00:50:46.390
operating system to the plug-in.

00:50:46.600 --> 00:50:49.260
But if you use Carbon events,
the plug-in can just get those

00:50:49.260 --> 00:50:52.970
messages directly from the OS,
and things are just wonderfully

00:50:52.970 --> 00:50:55.700
simple by comparison to OS 9.

00:50:55.700 --> 00:51:01.630
And similarly to both the Audio Codec
SDK and the Audio Unit SDK,

00:51:01.640 --> 00:51:04.700
for Audio Unit Carbon views,
we're -- we're going to be

00:51:04.700 --> 00:51:05.780
using a lot of different things.

00:51:05.780 --> 00:51:08.350
We're going to supply a small
C++ framework to make it as

00:51:08.350 --> 00:51:13.310
simple as possible for you
to write these components.

00:51:15.210 --> 00:51:21.380
Those of you who use Cocoa may be asking,
"Well, so what about us?

00:51:21.600 --> 00:51:27.020
Why not an Audio Unit Cocoa view?"
There are a lot of tricky issues

00:51:27.020 --> 00:51:29.480
in mixing Carbon and Cocoa.

00:51:29.480 --> 00:51:32.290
There are some directions
in which it already works.

00:51:32.300 --> 00:51:34.760
There are others in which it doesn't.

00:51:34.790 --> 00:51:40.310
And we really want to make this
as transparent as possible.

00:51:40.580 --> 00:51:45.070
So we can't commit to that, to Jaguar,
but we are very aware that there

00:51:45.160 --> 00:51:48.000
are people who want to write
their user interfaces in Cocoa,

00:51:48.110 --> 00:51:51.890
and we're going to find a way
to make it work if we can.

00:51:53.830 --> 00:51:58.530
So moving back to the
Carbon View component,

00:51:58.530 --> 00:52:05.440
the base class, AU Carbon View Base,
all it really has to do is handle

00:52:05.440 --> 00:52:12.440
the one component selector,
which is open yourself, in effect,

00:52:12.560 --> 00:52:13.720
although it has a lot of parameters.

00:52:15.500 --> 00:52:18.920
So the component gets told,
here's your component reference,

00:52:18.920 --> 00:52:20.030
which is in view.

00:52:20.110 --> 00:52:24.030
It's getting told the Audio Unit it's
being expected to control.

00:52:24.050 --> 00:52:30.150
It's being told the window into which
it's going to build its UI and the

00:52:30.250 --> 00:52:32.900
parent control within that window.

00:52:32.900 --> 00:52:34.460
These are all Carbon terms.

00:52:34.470 --> 00:52:38.440
The parent control might typically
be the window's root control,

00:52:38.440 --> 00:52:41.610
and we'll see that in the sample program.

00:52:41.620 --> 00:52:42.600
That's how I'm doing things.

00:52:43.690 --> 00:52:49.030
The host application is providing
a location and requested

00:52:49.030 --> 00:52:53.580
size for your view component,
but the size is actually

00:52:53.580 --> 00:52:54.990
only a suggestion.

00:52:54.990 --> 00:52:59.230
If the host says, well,
you can be 200 by 300 pixels wide,

00:52:59.230 --> 00:53:02.710
and you say, no,
I want to be 400 by 800 pixels in size,

00:53:02.820 --> 00:53:05.920
you're free to make yourself
as large as you want.

00:53:05.920 --> 00:53:09.550
That's just the host's request,
because he's actually free to go back and

00:53:09.550 --> 00:53:11.800
make you smaller afterwards or embed you
inside scroll bars or whatever he wants.

00:53:11.800 --> 00:53:18.100
Or he could be really nice and say, okay,
he's going to be 400 by 800.

00:53:18.100 --> 00:53:22.910
I'm going to give him the whole
window and make the window that size.

00:53:23.460 --> 00:53:30.860
So the last argument there is the
user pane which the plugin creates,

00:53:30.980 --> 00:53:34.910
the view component creates,
and returns to the host.

00:53:35.720 --> 00:53:39.280
The other main function
provided by AU Carbon Base,

00:53:39.450 --> 00:53:43.890
or Carbon View Base, our base class,
is to manage the connecting

00:53:43.890 --> 00:53:46.220
of controls to parameters.

00:53:47.190 --> 00:53:52.100
And the mechanism by which it does
that we call Parameter Listeners.

00:53:52.100 --> 00:53:58.340
And this is our way of making it
possible for multiple pieces of software

00:53:58.340 --> 00:54:04.120
that are all working with the same
Audio Unit to be aware of changes that

00:54:04.120 --> 00:54:07.460
the others make to the parameter values.

00:54:07.570 --> 00:54:12.610
For instance, you might have -- I can
think of a lot of examples.

00:54:12.740 --> 00:54:16.730
You might have two views on the screen,
two different user interfaces

00:54:16.740 --> 00:54:18.460
for the same Audio Unit.

00:54:18.600 --> 00:54:21.310
And they could -- one of them could
be moving the slider -- actually,

00:54:21.310 --> 00:54:22.860
I'll show you this in my program.

00:54:22.970 --> 00:54:26.260
Moving the slider in one will
make the slider move in the other.

00:54:26.380 --> 00:54:30.180
Your program might be
automating the Audio Unit,

00:54:30.180 --> 00:54:35.160
and then the user interface
wants to reflect those changes.

00:54:35.300 --> 00:54:38.440
There's a lot of possibilities here.

00:54:38.440 --> 00:54:41.610
We've created a centralized
dispatch mechanism called

00:54:41.610 --> 00:54:43.760
the AU Parameter Listener.

00:54:43.760 --> 00:54:47.510
All that you have to do from the
point of view of when you change

00:54:47.510 --> 00:54:53.470
parameters is to call AU Parameter Set,
which goes through this

00:54:53.470 --> 00:54:57.590
notification mechanism,
instead of going directly

00:54:57.590 --> 00:54:57.590
to Audio Unit Set Parameter.

00:54:58.180 --> 00:55:03.500
And you can set up one listener to
listen to any number of parameters.

00:55:03.500 --> 00:55:05.940
In our view components, for instance,
there's just one listener,

00:55:05.940 --> 00:55:10.510
and it listens to all of
the parameters in that view.

00:55:10.510 --> 00:55:15.020
And for more details,
you can look at our new, for Jaguar,

00:55:15.020 --> 00:55:18.380
header file, AudioUnitUtilities.h.

00:55:20.180 --> 00:55:24.500
So with that, I'm getting low on time,
so I'm going to put -- actually,

00:55:24.500 --> 00:55:27.090
writing a Carbon view
is also pretty simple.

00:55:27.090 --> 00:55:34.810
I'm going to show you how to write one.

00:55:44.100 --> 00:55:46.500
I'm in the wrong program, no wonder.

00:55:46.610 --> 00:55:47.100
Wrong project.

00:55:47.100 --> 00:55:51.000
Okay.

00:55:51.000 --> 00:55:54.950
Okay,
so this is the custom user interface for

00:55:54.950 --> 00:55:58.370
the multi-tap delay I showed you earlier.

00:55:58.790 --> 00:56:03.110
It derives from AU Carbon Base,
which I was just telling you about,

00:56:03.190 --> 00:56:08.230
and it only has to override one
virtual method called Create UI.

00:56:08.800 --> 00:56:15.220
It has the same magic macro to
create a component dispatcher for me.

00:56:18.770 --> 00:56:24.000
And then this is just a bunch of
kind of tedious Carbon UI code to

00:56:24.000 --> 00:56:29.000
manually create controls on the fly.

00:56:29.180 --> 00:56:34.670
This C++ object specifies the
parameter in terms of its Audio Unit,

00:56:34.740 --> 00:56:41.440
parameter ID, scope, and element number.

00:56:41.450 --> 00:56:45.930
I create a static text control,
which is the name of the parameter.

00:56:46.800 --> 00:56:50.440
and Jeff Moore: Thank you, Jeff.

00:56:50.440 --> 00:56:52.550
Thank you, Jeff.

00:56:52.550 --> 00:56:52.550
Thank you.

00:56:53.260 --> 00:56:58.850
And then the SDK has this handy
little utility function that will,

00:56:58.990 --> 00:57:02.690
for one parameter,
create a slider with labels on both ends

00:57:02.780 --> 00:57:09.030
and an edit text field over to the side
and connect them all up to one parameter.

00:57:10.700 --> 00:57:16.900
And so here that's where I'm creating the
controls for the wet/dry mix parameter.

00:57:18.600 --> 00:57:23.560
And then, as you remember,
this multi-tap delay has five taps.

00:57:23.710 --> 00:57:28.060
So for each of the five taps,
I go through this loop,

00:57:28.060 --> 00:57:35.150
and I do the same process of specifying
the Audio Unit parameter scope element,

00:57:35.180 --> 00:57:39.610
make a rectangle,
and again call this function to create a

00:57:39.620 --> 00:57:42.850
labeled slider with an edit text field.

00:57:44.080 --> 00:57:48.970
Basically, I'm just building a bunch of
Carbon controls on the fly.

00:57:49.110 --> 00:57:55.000
I'm making a lot of use of some
utility functions here to do that.

00:57:55.000 --> 00:57:59.240
You can look at the details
of how that works in the SDK.

00:57:59.240 --> 00:58:01.980
Then the last thing I have to do
when I'm done is just make sure that

00:58:02.120 --> 00:58:05.450
the Carbon pane that I've created
is big enough to enclose all of

00:58:05.450 --> 00:58:07.560
the controls that I've created.

00:58:07.560 --> 00:58:10.180
The base class,
as you create those controls,

00:58:10.190 --> 00:58:13.600
keeps track of which one went
furthest right and which one

00:58:13.710 --> 00:58:15.600
went furthest to the bottom.

00:58:15.600 --> 00:58:21.250
Those are stored in this member variable,
mbottomright.

00:58:21.350 --> 00:58:25.100
Just by adding a little bit of
slop here so that there's space

00:58:25.250 --> 00:58:31.280
at the bottom right of the window,
I can set my custom view to a nice size.

00:58:35.190 --> 00:58:42.860
So this is all the code for the
custom UI for the MultiTap AUView,

00:58:42.860 --> 00:58:45.860
other than the code that
we provide in the SDK.

00:58:45.860 --> 00:58:49.810
Now let's look at that.

00:59:01.500 --> 00:59:05.670
This isn't actually any more
beautiful than the generic view,

00:59:05.670 --> 00:59:08.410
but it is a little more nicely organized.

00:59:19.000 --> 00:59:22.250
You can see how the parameter
listener mechanism is working here.

00:59:22.300 --> 00:59:26.000
These two views,
when I change a parameter in one,

00:59:26.000 --> 00:59:29.300
it's changing in the other.

00:59:39.700 --> 00:59:56.300
[Transcript missing]

01:00:00.490 --> 01:00:05.770
So just to wrap up here,
I realize that the transition to

01:00:05.770 --> 01:00:11.640
the Audio Unit version 2 API may be
causing you some questions and concern.

01:00:11.640 --> 01:00:16.640
I'd like to reemphasize that we're
going to do everything we can in

01:00:16.640 --> 01:00:20.060
the base classes in the SDK to
make sure that those classes

01:00:20.260 --> 01:00:22.560
support both versions of the API.

01:00:22.560 --> 01:00:25.660
So you can start working on
learning your way around the

01:00:25.660 --> 01:00:30.000
SDK without fear of things changing
drastically underneath you later.

01:00:30.400 --> 01:00:34.100
And I'd like to really thank all
the developers who've been giving

01:00:34.100 --> 01:00:37.510
us feedback all along because it
helps us know when we're doing the

01:00:37.510 --> 01:00:40.310
right thing and that feels good,
and it helps us know when we're

01:00:40.310 --> 01:00:43.230
doing the bad thing and it feels good
to be able to do the right thing.

01:00:43.240 --> 01:00:47.590
So thanks again for your feedback,
and please keep giving

01:00:47.720 --> 01:00:51.540
it to us on our API list,
and thank you very much.