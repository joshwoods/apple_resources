WEBVTT

00:00:03.580 --> 00:00:04.400
Good morning, ladies and gentlemen.

00:00:04.400 --> 00:00:07.780
Welcome now to session 407,
Java Performance.

00:00:07.780 --> 00:00:11.210
Please welcome now Jim Laskey
and Victor Hernandez.

00:00:17.600 --> 00:00:20.200
My name is Victor Hernandez,
and I'm a member of the

00:00:20.200 --> 00:00:21.880
Java Runtime Technologies team.

00:00:24.490 --> 00:00:27.260
and I will be giving this talk.

00:00:27.260 --> 00:00:30.770
We're hoping to round off your week
of Java sessions with some hints

00:00:31.010 --> 00:00:36.050
on how to write code for better
performance and also a preview of

00:00:36.190 --> 00:00:42.570
upcoming technologies that will be
available to you in coming JDK 1.4.

00:00:43.340 --> 00:00:45.480
So what is the goal of this session?

00:00:45.540 --> 00:00:48.320
The goal of this session is for
you to have a better understanding

00:00:48.720 --> 00:00:54.200
of exactly how and why your
application performs as it does.

00:00:54.200 --> 00:00:59.440
We'll be discussing improvements that are
being made to the Hotspot JIT compiler,

00:00:59.670 --> 00:01:04.460
especially in the JDK 1.4
timeframe that will improve the

00:01:04.460 --> 00:01:06.430
performance of your application.

00:01:06.730 --> 00:01:11.120
And we'll also be introducing to
you new APIs that are available in

00:01:11.120 --> 00:01:16.100
JDK 1.4 that will be improving the
performance as well if you use them.

00:01:17.870 --> 00:01:20.560
So the structure of the
presentation is Jim will be

00:01:20.560 --> 00:01:22.800
talking about hotspot compilation.

00:01:22.820 --> 00:01:28.550
I'll be introducing new
APIs available to you in JDK 1.4,

00:01:28.590 --> 00:01:32.910
which are also available to you
in the developer preview that

00:01:32.980 --> 00:01:37.440
we're making available to you from
the developer.apple.com website.

00:01:37.450 --> 00:01:39.920
And we'll have a quick summary and
then we'll bring up all the members

00:01:40.080 --> 00:01:46.000
of Java Runtime Technologies team
to answer your questions.

00:01:46.000 --> 00:01:48.580
So I'm going to pass it off to Jim now.

00:01:51.760 --> 00:01:56.520
Good morning, survivors.

00:01:56.920 --> 00:02:00.610
One of the main factors in determining
the performance of a Java VM is how well

00:02:00.610 --> 00:02:07.980
its just-in-time compiler can convert
Java bytecodes into native machine code.

00:02:07.980 --> 00:02:13.070
In past presentations,
we've talked about various elements of

00:02:13.130 --> 00:02:18.210
the Java language and how the compiler
takes those elements and converts them to

00:02:18.210 --> 00:02:24.120
machine code and how we can potentially
rework code so that you can get the best

00:02:24.120 --> 00:02:31.920
possible optimization from the compiler.

00:02:31.920 --> 00:02:35.140
This year,
because we're switching over to 1.4,

00:02:35.140 --> 00:02:39.760
we've been working on 1.4,
I want to talk a bit about the

00:02:40.150 --> 00:02:43.250
A bit about some of the new
technologies that have been introduced

00:02:43.250 --> 00:02:48.930
in the compiler and some of the
new optimizations that are being done.

00:02:49.200 --> 00:02:53.760
Specifically,
I want to talk about six new or

00:02:53.760 --> 00:03:01.730
improved technologies that are
coming along the line with 1.4.

00:03:01.830 --> 00:03:04.640
Class hierarchy analysis,
dynamic compilation.

00:03:04.670 --> 00:03:06.250
Class hierarchy analysis is new.

00:03:06.610 --> 00:03:08.710
Dynamic compilation is partially new.

00:03:08.710 --> 00:03:10.970
Full speed debugging is a new feature.

00:03:10.970 --> 00:03:13.880
Aggressive inlining is a new feature.

00:03:13.880 --> 00:03:19.110
We've had inlining before,
but aggressive inlining is a new feature.

00:03:19.460 --> 00:03:22.580
An expansion on intrinsics
and something totally new,

00:03:22.580 --> 00:03:25.930
which has been sort of the crux
of the new work that we've done,

00:03:25.930 --> 00:03:29.190
something called the low-level
intermediate representation.

00:03:32.330 --> 00:03:36.760
First of all, we'll talk about class
hierarchy analysis.

00:03:36.760 --> 00:03:39.340
One of the things that if you're
working in the Java environment

00:03:39.340 --> 00:03:45.240
you'll eventually figure out is that
in the class hierarchy of all the

00:03:45.240 --> 00:03:48.260
classes that you're working with,
only about 80% of them

00:03:48.880 --> 00:03:50.750
actually are subclass.

00:03:50.790 --> 00:03:56.710
80% of them are never subclassed.

00:03:57.210 --> 00:03:59.780
They're never overshadowed
by other classes.

00:04:00.290 --> 00:04:06.680
There's only about 20% of your working
classes that are actually overshadowed.

00:04:06.680 --> 00:04:10.460
You can think of these classes
as being Gleef classes or in

00:04:10.460 --> 00:04:14.390
Java terminology as final classes,
even though you don't

00:04:14.390 --> 00:04:15.460
have them tagged as final.

00:04:17.360 --> 00:04:21.340
And within those classes that are leafs,
more often than not,

00:04:21.380 --> 00:04:25.960
you only overshadow a few of the
methods that are in the parent class.

00:04:27.150 --> 00:04:31.090
And because you're only
overshadowing a few of them,

00:04:31.090 --> 00:04:34.850
a lot of those methods could
be considered as final as well,

00:04:34.960 --> 00:04:37.700
even though they're not tagged as final.

00:04:37.700 --> 00:04:41.700
And this influences the optimization
that's done by the compiler.

00:04:45.720 --> 00:04:51.700
Another factor that's involved
here is instance of testing.

00:04:51.750 --> 00:04:54.700
Instance of is one of the more
expensive operations in the VM,

00:04:54.810 --> 00:05:01.410
because what we need to do to test to
see if an object is a member of a class

00:05:01.440 --> 00:05:07.780
is to extract the class field that's in
the object and compare it against the

00:05:07.900 --> 00:05:09.900
class that you're trying to test against.

00:05:09.900 --> 00:05:11.960
And if it doesn't match,
then you have to search up the

00:05:11.960 --> 00:05:14.100
class hierarchy in order to
see if you can find a match.

00:05:14.200 --> 00:05:14.200
And another factor that's involved
here is instance of testing.

00:05:14.200 --> 00:05:14.200
Instance of is one of the more
expensive operations in the VM,

00:05:14.200 --> 00:05:14.200
because what we need to do to test to
see if an object is a member of a class

00:05:14.200 --> 00:05:14.200
is to extract the class field that's in
the object and compare it against the

00:05:14.200 --> 00:05:14.200
class that you're trying to test against.

00:05:14.200 --> 00:05:14.200
And if it doesn't match,
then you have to search up the

00:05:14.200 --> 00:05:14.200
class hierarchy in order to
see if you can find a match.

00:05:14.390 --> 00:05:17.800
see if you can find a match in
the class hierarchy to see if

00:05:17.800 --> 00:05:19.090
it's actually an instance of.

00:05:19.420 --> 00:05:20.950
So it's a fairly expensive operation.

00:05:22.470 --> 00:05:25.300
Fundamentally,
what we're trying to do here

00:05:25.300 --> 00:05:27.420
is to improve performance.

00:05:27.420 --> 00:05:35.540
We find that virtual calls cost a lot
more than your final calls or your static

00:05:35.540 --> 00:05:41.350
calls because there's a lot of overhead
in searching for the matching method,

00:05:41.820 --> 00:05:43.620
doing a lookup, and so on.

00:05:43.620 --> 00:05:49.150
What we want to try to do is reduce
as many of our virtual calls into

00:05:49.180 --> 00:05:54.100
the same state as a final call,
a call to a final method.

00:05:54.550 --> 00:05:57.500
Okay, so this is sort of the
background of what we're trying

00:05:57.550 --> 00:06:00.120
to do or trying to influence.

00:06:00.120 --> 00:06:06.330
In 1.3.1 VM,
we have very conservative optimizations

00:06:06.330 --> 00:06:12.640
in relation to class hierarchy and
the influence of how classes relate

00:06:12.640 --> 00:06:15.380
to each other and also with methods.

00:06:15.380 --> 00:06:18.390
When we're doing an instance of,
we check to see if the class

00:06:18.390 --> 00:06:22.790
is a member of the -- or sorry,
the object is a member of the class,

00:06:22.790 --> 00:06:26.010
and then we search up the class
hierarchy to see if we can get a match.

00:06:26.040 --> 00:06:28.300
It can be fairly expensive.

00:06:28.300 --> 00:06:33.260
Then we also have to make the
assumption that any method that's

00:06:33.260 --> 00:06:36.610
being called has to be thought
of as -- in a virtual sense,

00:06:36.610 --> 00:06:38.370
has to be treated as virtual.

00:06:38.460 --> 00:06:43.570
We can't do any kind of optimization
around that because we always have to

00:06:43.780 --> 00:06:48.490
assume that that method can be overridden
and we might introduce a new class

00:06:48.830 --> 00:06:51.440
that might force a virtual dispatch.

00:06:53.520 --> 00:07:01.580
So in 1.4, we have this new technology
called Class Hierarchy Analysis.

00:07:01.630 --> 00:07:05.390
And what it does is it does a
more detailed analysis of how

00:07:05.400 --> 00:07:10.220
classes relate to each other,
both in an inheritance chain and how

00:07:10.220 --> 00:07:17.230
methods that implement interfaces,
how their relation works together.

00:07:17.510 --> 00:07:20.450
And in doing that,
we can do a few things that

00:07:20.810 --> 00:07:25.590
will improve the optimization
in the MUNI compiling method.

00:07:25.660 --> 00:07:31.970
The first thing is that knowing a
-- the first thing is basically a

00:07:31.970 --> 00:07:36.600
faster instance of knowing that a
class is final allows us to do a very

00:07:36.600 --> 00:07:41.360
straightforward test to see if it's a
member when we're doing instance of.

00:07:41.360 --> 00:07:46.220
So you're already aware of
JavaLangString as being a final class.

00:07:46.240 --> 00:07:47.630
So if you're checking to see
if something is a string,

00:07:47.630 --> 00:07:51.870
you're comparing the test internally
is basically pull up the class

00:07:51.870 --> 00:07:56.220
out of the object and compare
it against the JavaLangString

00:07:56.270 --> 00:07:58.620
class and see if there's a match.

00:07:58.620 --> 00:08:00.560
And that's all -- so it's
a very simple compare.

00:08:00.560 --> 00:08:02.620
If it doesn't match,
then it's not a member of -- there's

00:08:02.620 --> 00:08:04.140
no searching up the hierarchy.

00:08:04.140 --> 00:08:05.500
It's a final class.

00:08:05.550 --> 00:08:07.480
That's a very simple test.

00:08:07.590 --> 00:08:12.000
With Class Hierarchy Analysis,
we can actually perform the same simple

00:08:12.300 --> 00:08:17.630
test on any class which is considered
a leaf class from the hierarchy.

00:08:17.740 --> 00:08:17.740
So it's the same thing.

00:08:17.740 --> 00:08:17.740
It's the same thing.

00:08:17.740 --> 00:08:17.740
It's the same thing.

00:08:17.750 --> 00:08:23.740
So we went through and annotated each
of the leaf classes as being final,

00:08:23.920 --> 00:08:26.220
at least for that point in time.

00:08:28.850 --> 00:08:34.100
There's also a secondary type
of mechanism for things that are

00:08:34.100 --> 00:08:35.420
actually in a class hierarchy.

00:08:35.480 --> 00:08:41.490
Instead of searching up a linked list,
we construct an inheritance

00:08:41.490 --> 00:08:43.760
table for all classes.

00:08:43.760 --> 00:08:47.570
In that way,
we can assign an inheritance depth for a

00:08:47.690 --> 00:08:51.010
class and index directly in that table.

00:08:51.050 --> 00:08:54.500
Again, instead of searching up the chain,
we can do a simple check,

00:08:54.690 --> 00:09:04.440
but we pull it out of the inheritance
table and make it a lot faster.

00:09:06.970 --> 00:09:09.620
In the class hierarchy analysis,
we also do a method level analysis.

00:09:09.660 --> 00:09:13.750
So in 1.3, we only check on a per class
basis whether a method is

00:09:13.880 --> 00:09:16.300
overshadowed by a subclass or not.

00:09:16.520 --> 00:09:20.730
In 1.4, we look at every
individual method and say,

00:09:21.020 --> 00:09:23.940
is this actually overshadowed or not?

00:09:25.390 --> 00:09:28.530
And this is true also in interfaces.

00:09:28.540 --> 00:09:31.950
Do we see how many implementations
of a particular method?

00:09:32.000 --> 00:09:37.700
If there's only one implementation,
then we don't have to dispatch

00:09:37.910 --> 00:09:39.140
using an interface-type call.

00:09:39.150 --> 00:09:40.730
We can make a direct
call to that function.

00:09:42.820 --> 00:09:44.540
Okay, so what does this
analysis allow us to do?

00:09:44.540 --> 00:09:48.830
It allows us to make direct calls instead
of virtual or interface-level calls,

00:09:48.920 --> 00:09:50.680
and then it's much faster.

00:09:50.800 --> 00:09:53.110
You just basically do a
direct branch to the routine.

00:09:53.360 --> 00:09:57.930
You don't have to worry about checking to
see if it's the right class or whatever.

00:09:58.190 --> 00:10:00.510
It also allows us to do inlining,
and I'll be talking about this a

00:10:00.510 --> 00:10:04.140
little bit further in other slides.

00:10:04.180 --> 00:10:08.100
It allows us to inline across
virtual and interface calls.

00:10:08.610 --> 00:10:13.100
So where we couldn't inline
a method call before,

00:10:13.100 --> 00:10:16.680
we can now do that because
we basically can say,

00:10:16.680 --> 00:10:19.290
at this point in time,
we can think of this as

00:10:19.360 --> 00:10:20.890
being a final method.

00:10:20.890 --> 00:10:23.310
We're not going to do a virtual dispatch.

00:10:23.360 --> 00:10:28.060
We can inline that code
directly into the call point.

00:10:28.100 --> 00:10:29.240
Thank you.

00:10:32.390 --> 00:10:34.930
The second technology is something
called dynamic compilation,

00:10:34.960 --> 00:10:42.060
and we've always had some kind
of dynamic compilation in Java.

00:10:42.580 --> 00:10:47.330
J2SE kicks in initially when
you're basically converting from an

00:10:47.330 --> 00:10:51.790
interpreted execution for a particular
method to its compiled version.

00:10:51.870 --> 00:10:57.150
We have a transition from
interpreted to compiled.

00:10:57.370 --> 00:11:00.660
But as you'll see in the next slide,
we've gone a little bit further

00:11:00.660 --> 00:11:05.010
and allows us to do a little
bit more substitution of

00:11:05.010 --> 00:11:07.820
compiled and interpreted code.

00:11:09.660 --> 00:11:13.180
So, with dynamic,
and what we're trying to do here is

00:11:13.180 --> 00:11:17.690
basically get better code generation
or better optimization based

00:11:17.750 --> 00:11:19.520
on the current state of things.

00:11:19.520 --> 00:11:23.700
So, as I've said in the
previous slide about CHA,

00:11:23.730 --> 00:11:30.240
we can think of leaf classes or methods
that are in leaf classes as final.

00:11:30.240 --> 00:11:33.730
That means we can do better
optimization knowing exactly

00:11:33.730 --> 00:11:35.920
what method that we're calling.

00:11:36.420 --> 00:11:43.090
So, we can make assumptions
about the current state.

00:11:43.090 --> 00:11:43.090
We'd like to make assumptions
about the current state.

00:11:44.360 --> 00:11:49.230
And then we could do that,
but then we'd be sort of disrupted by the

00:11:49.290 --> 00:11:55.530
fact that dynamic loading of classes may
introduce a subclass and basically force

00:11:55.530 --> 00:11:59.300
us to throw away all of our assumptions.

00:11:59.300 --> 00:12:02.000
So if we have to throw
away our assumptions,

00:12:02.000 --> 00:12:05.290
we have to throw away the code
that we've generated as well.

00:12:05.700 --> 00:12:08.700
And that's where dynamic
compilation kicks in.

00:12:08.800 --> 00:12:11.280
So in 1.3.1, we are very conservative.

00:12:11.280 --> 00:12:14.080
The compiler can make no
assumptions about classes.

00:12:14.120 --> 00:12:18.240
It basically just has to compile them,
assuming that new classes will

00:12:18.240 --> 00:12:22.830
be loaded and that if I have a
virtual dispatch in the code,

00:12:22.860 --> 00:12:25.910
I have to leave the virtual
dispatch there and I can't do

00:12:25.910 --> 00:12:27.500
any optimization around it.

00:12:31.680 --> 00:12:37.080
In 1.4,
we can compile during the current state

00:12:37.150 --> 00:12:39.630
and then do a dynamic replacement.

00:12:39.770 --> 00:12:44.320
So we can compile assuming that
a particular method is final or a

00:12:44.320 --> 00:12:47.170
particular class is final based on CHA.

00:12:47.290 --> 00:12:51.540
But then if a class is loaded
and changes that relationship,

00:12:51.540 --> 00:12:55.130
changes the relationship,
then we can dynamically

00:12:55.240 --> 00:12:57.540
replace that method on the fly.

00:12:57.540 --> 00:13:00.240
And the two technologies
that we use to do that,

00:13:00.490 --> 00:13:04.680
well the first one has always
been there and this is something

00:13:05.100 --> 00:13:10.660
On-Stack Replacement allows
us to replace something that's

00:13:10.660 --> 00:13:10.660
currently being interpreted.

00:13:10.870 --> 00:13:12.470
with its compiled version.

00:13:12.580 --> 00:13:15.070
So that's basically interpreted
through to compiled code.

00:13:16.890 --> 00:13:20.380
OSR specifically deals with
methods that are looping for

00:13:20.380 --> 00:13:23.510
long periods of time and you want

00:13:23.880 --> 00:13:27.210
What's been introduced in 1.4 is
something called deoptimization,

00:13:27.220 --> 00:13:30.890
where we're allowed to replace
a compiled version of the code

00:13:30.910 --> 00:13:32.960
with its interpreted equivalent.

00:13:35.080 --> 00:13:37.360
Now, how is that connected with
replacing it with compile code?

00:13:37.360 --> 00:13:43.890
So presumably what we
can do is that we can

00:13:44.590 --> 00:13:47.470
Initially interpret a method,
then replace it with

00:13:47.470 --> 00:13:50.330
its compiled version,
load a new class which forces

00:13:50.330 --> 00:13:51.900
recompilation of that class.

00:13:52.150 --> 00:13:53.940
So in the meantime,
while it's being recompiled,

00:13:53.940 --> 00:13:57.000
we replace it with its
old interpreted version.

00:13:57.160 --> 00:14:00.320
And then once it's finished
recompiling in its new form,

00:14:00.400 --> 00:14:03.040
then we can switch it back
to this compiled form.

00:14:03.160 --> 00:14:06.590
So this is transition state.

00:14:06.620 --> 00:14:11.220
So the advantages of deoptimization
are that we can correct for

00:14:11.220 --> 00:14:13.610
changes in the class hierarchy,

00:14:13.860 --> 00:14:16.350
We could also introduce
higher levels of optimization.

00:14:16.350 --> 00:14:19.910
So if a method is running for long
periods of time and we think we can

00:14:19.910 --> 00:14:22.310
get better optimization on that,
spend more time making

00:14:22.370 --> 00:14:24.870
a better optimization,
we could replace it with a

00:14:24.870 --> 00:14:26.780
higher level of optimization.

00:14:26.820 --> 00:14:30.650
And then finally,
we could debug compiled methods.

00:14:30.650 --> 00:14:35.130
And then we get into the next slide,
which is something called

00:14:35.130 --> 00:14:36.210
full-speed debugging.

00:14:39.980 --> 00:14:46.160
In 1.3, in order to enable debugging,
what we do is we debug

00:14:46.160 --> 00:14:48.920
only interpreted code.

00:14:51.560 --> 00:14:56.430
And the reason we do that is that
it's very easy to map the interpretive

00:14:56.550 --> 00:14:59.000
bytecode back to the source code.

00:14:59.020 --> 00:15:00.490
So if you want to do
source-level debugging,

00:15:00.490 --> 00:15:03.400
there's very easy mapping there.

00:15:05.580 --> 00:15:10.680
The problem with that is that
when you're running interpreted,

00:15:10.830 --> 00:15:12.170
It can be too slow.

00:15:12.240 --> 00:15:15.000
I'll just give an exaggerated example.

00:15:15.100 --> 00:15:22.560
If you have a program that calculates 20
million data points before it craps out,

00:15:22.610 --> 00:15:26.930
then you could be sitting
there for quite a while getting

00:15:27.210 --> 00:15:28.450
those data points calculated.

00:15:30.530 --> 00:15:36.130
So if you could actually run it in
compiled mode and then hit the bug,

00:15:36.130 --> 00:15:40.740
then you're not sitting
there for as long time.

00:15:40.740 --> 00:15:44.040
Now why can't you compile or why
can't you debug compile code directly?

00:15:44.040 --> 00:15:47.320
Well, because of the optimization that
occurs against the compile code,

00:15:47.420 --> 00:15:53.240
it's not that easy to map from the
compile code back to the source code.

00:15:53.790 --> 00:15:56.690
So we need to transition
from the compiled code back

00:15:57.060 --> 00:15:58.470
into the interpreted code.

00:16:00.220 --> 00:16:03.990
So as I say at the bottom line there,
debugging in 1.3.1 forces

00:16:03.990 --> 00:16:05.760
interpretation across the board.

00:16:05.980 --> 00:16:08.410
So you're forced to interpret.

00:16:12.880 --> 00:16:17.700
In 1.4.1,
when you turn debug using Xdebug,

00:16:17.760 --> 00:16:19.000
we don't disable the compiler.

00:16:19.000 --> 00:16:21.390
The compiler still compiles everything.

00:16:23.010 --> 00:16:26.270
But then if you hit a break point or
you start stepping through the code,

00:16:26.300 --> 00:16:32.850
then those methods get converted
over into interpretive methods.

00:16:32.880 --> 00:16:38.420
So then we can step through
the bytecode and debug it.

00:16:40.600 --> 00:16:43.440
The mechanism for doing
that is quite interesting.

00:16:43.550 --> 00:16:51.430
I won't get into any details here,
but it's basically when we want to

00:16:52.590 --> 00:16:56.780
de-optimize a particular execution,
we just set it up so that when

00:16:56.780 --> 00:17:01.540
we return back to that execution,
we go into some handling code,

00:17:01.580 --> 00:17:06.700
which does the actual substitution
of the current compiled frame

00:17:07.330 --> 00:17:09.740
with some interpreted frames.

00:17:14.290 --> 00:17:19.980
Now, in the first technology,
we talked about class hierarchy analysis

00:17:20.120 --> 00:17:24.420
and the ability to sort of temporarily,
at least,

00:17:24.490 --> 00:17:30.770
think of methods as being final methods.

00:17:31.390 --> 00:17:36.700
Knowing that a method is at least
temporarily thought of as being final,

00:17:36.700 --> 00:17:40.870
this allows us to say, okay,
we can make direct calls to that method,

00:17:40.940 --> 00:17:47.290
or if the method is sufficiently simple,
we can inline it at the call point.

00:17:48.750 --> 00:17:54.780
So more often than not,
if you're writing small, concise methods,

00:17:54.780 --> 00:17:59.190
the cost of actually calling
the method far outweighs what's

00:17:59.270 --> 00:18:00.680
actually going on inside the method.

00:18:00.680 --> 00:18:03.460
So it may be a simple accessor
to return the value of field,

00:18:03.520 --> 00:18:06.710
or do a simple calculation,
or get the value of a static.

00:18:09.200 --> 00:18:20.700
[Transcript missing]

00:18:21.440 --> 00:18:25.980
In 1.3.1, because we didn't have
these earlier technologies,

00:18:25.980 --> 00:18:29.600
we can only really inline simple
accessors and statics and final methods.

00:18:29.600 --> 00:18:35.500
So the level of inlining is very small.

00:18:37.400 --> 00:18:41.560
With Hotspot 1.4,
we can increase the complexity and depth

00:18:41.580 --> 00:18:48.200
of the types of methods that we inline,
primarily because of thinking

00:18:48.200 --> 00:18:49.390
of methods as being...

00:18:49.610 --> 00:18:54.110
as methods as being final so we can
have accessors that call accessors

00:18:54.120 --> 00:18:59.060
that call other accessors that can
be all brought in and in lined.

00:18:59.060 --> 00:19:06.060
We also allow for methods that have
exception handling to also be in line.

00:19:06.200 --> 00:19:10.470
So we've allowed for more
types of methods to be in line.

00:19:10.480 --> 00:19:16.310
It takes advantage of the CHA to
specify candidates as being final,

00:19:16.330 --> 00:19:22.240
knowing that we can always deoptimize
those methods and replace them.

00:19:22.240 --> 00:19:24.570
And this gives us significant
improvement in performance.

00:19:24.660 --> 00:19:28.210
And I think we've already seen one of
the Grand Canyon examples where we've

00:19:28.210 --> 00:19:34.290
actually doubled the frame rate just
purely by using aggressive in line.

00:19:37.120 --> 00:19:42.240
Intrinsics are something
that we've had for a while.

00:19:42.340 --> 00:19:45.400
Basically,
they're core methods that we realize

00:19:45.400 --> 00:19:51.140
that are used a lot and that we
could do better by hand coding some

00:19:51.140 --> 00:19:54.890
assembly code for those routines.

00:19:55.520 --> 00:19:59.380
The types of things you can do are, say,
reduce the JNI call overhead

00:19:59.380 --> 00:20:00.960
if it's a JNI support routine.

00:20:00.960 --> 00:20:05.080
There may be a better
implementation in native code,

00:20:05.080 --> 00:20:08.300
like, for instance, sine or cosine.

00:20:08.300 --> 00:20:15.320
Sometimes there's
performance bottlenecks,

00:20:15.870 --> 00:20:22.830
like maybe Java-laying string charAt,
which are used a lot,

00:20:22.990 --> 00:20:26.800
and we know that we could
hand-tune a little bit better.

00:20:26.800 --> 00:20:32.000
These are types of things that we
would like to convert into intrinsics.

00:20:32.030 --> 00:20:34.280
What happens is that we,
instead of calling out the method,

00:20:34.360 --> 00:20:37.800
we insert specific code to deal
with that particular problem.

00:20:38.300 --> 00:20:39.120
We can use the method
to solve the problem,

00:20:39.120 --> 00:20:40.210
but we don't want to use the
method to solve the problem.

00:20:40.210 --> 00:20:41.300
We want to use the method
to solve the problem.

00:20:41.300 --> 00:20:45.760
Some examples are sine, cosine,
square root, and string charAt.

00:20:47.320 --> 00:20:51.020
With 1.4.1, we've expanded the number of
intrinsics we've implemented.

00:20:51.020 --> 00:20:55.370
The most important one
probably is new I/O.

00:20:55.870 --> 00:21:01.440
In new I/O, when you're calling one of
the native buffer accessors,

00:21:01.440 --> 00:21:07.190
we actually insert the code that
directly accesses the buffer.

00:21:07.320 --> 00:21:12.000
So that's a lot faster than going
through the equivalent Java code.

00:21:12.090 --> 00:21:16.680
And one of the big performance
improvements would be eliminating

00:21:16.680 --> 00:21:18.720
the per-byte accessors.

00:21:18.720 --> 00:21:21.330
So in the old I/O code,
whenever you wanted to read

00:21:21.340 --> 00:21:24.770
an integer from a buffer,
you had to assemble the integer

00:21:25.200 --> 00:21:29.340
value one byte at a time,
and that would take about 21

00:21:29.340 --> 00:21:32.330
native instructions to do that.

00:21:32.380 --> 00:21:33.940
But because we can treat
that as an intrinsic,

00:21:34.030 --> 00:21:37.360
we can actually read one integer
value directly from the buffer.

00:21:37.400 --> 00:21:39.800
So there's a big performance
improvement there.

00:21:40.720 --> 00:21:44.830
Thread allocation is inline,
so when you do a new,

00:21:44.880 --> 00:21:48.710
the code for allocating the new is right
inline with your code because it really,

00:21:48.840 --> 00:21:54.350
really minimally is just incrementing
a pointer to do that allocation.

00:21:54.350 --> 00:21:56.940
So we don't need to call a
subroutine to deal with that.

00:21:56.980 --> 00:21:59.710
Same is true with instance
of and check cast.

00:21:59.710 --> 00:22:03.980
So in the simple cases where we're just
doing a compare to see if the object

00:22:03.980 --> 00:22:08.810
class matches the class we're looking at,
we don't call a subroutine to do that.

00:22:08.810 --> 00:22:09.990
We do it inline.

00:22:10.700 --> 00:22:14.000
And similarly with monitors.

00:22:18.060 --> 00:22:21.360
And final technology I want to talk
about is something called low-level

00:22:21.360 --> 00:22:22.560
intermediate representation.

00:22:22.560 --> 00:22:26.660
It's probably not really much
of an interest unless you're

00:22:26.660 --> 00:22:29.230
into compiler technology and
understand compiler technology.

00:22:29.240 --> 00:22:35.430
But whenever you do compilation,
you have some intermediate representation

00:22:35.430 --> 00:22:37.200
that you do your manipulation on.

00:22:37.330 --> 00:22:40.780
It takes either the bytecode
in Java or let's say if

00:22:40.780 --> 00:22:44.840
you're compiling a C program,
you convert the text of the C program

00:22:44.840 --> 00:22:48.230
into some intermediate representation.

00:22:48.490 --> 00:22:52.040
The problem with the intermediate
representation in 1.3.1 was that

00:22:52.040 --> 00:22:57.010
it was too high level and didn't
allow us to do detail optimizations

00:22:57.500 --> 00:22:59.400
specifically for PowerPC.

00:22:59.400 --> 00:23:02.440
And they were having the same
problem with the other platforms.

00:23:02.440 --> 00:23:07.470
So Sun introduced this

00:23:08.200 --> 00:23:13.920
The LIR is actually much finer grained
and is closer to the native instructions.

00:23:13.920 --> 00:23:18.490
There would be one LIR instruction
for add and there would be one

00:23:18.500 --> 00:23:25.090
LIR instruction for load from an array
or actually load from a memory location.

00:23:27.470 --> 00:23:30.340
There is some higher level
functionality to deal with some

00:23:30.450 --> 00:23:34.000
support things like virtual calls
or type checks or intrinsics.

00:23:34.050 --> 00:23:36.640
But for the most part,
you can deal with everything

00:23:36.640 --> 00:23:37.900
at the lowest level.

00:23:37.900 --> 00:23:39.740
And what does this allow us to do?

00:23:39.740 --> 00:23:41.870
Well, it allows us to do
peephole optimizations.

00:23:41.870 --> 00:23:46.430
And peephole optimizations would
be things like you might have a

00:23:46.430 --> 00:23:51.880
shift operation and a mask operation
at the higher level or in the IR,

00:23:52.000 --> 00:23:53.440
sorry, the LEER.

00:23:53.900 --> 00:23:58.370
And on the PowerPC,
shift and mask is something that's all

00:23:58.370 --> 00:24:01.960
done in one specific PowerPC instruction.

00:24:01.970 --> 00:24:05.400
So we can take those two
instructions and merge them into

00:24:05.400 --> 00:24:07.730
a single PowerPC instruction.

00:24:07.740 --> 00:24:09.990
And we couldn't do that at
the higher level before.

00:24:09.990 --> 00:24:11.500
It was much more complicated.

00:24:13.320 --> 00:24:16.040
Since the IR represents
the native instructions,

00:24:16.040 --> 00:24:20.300
we can do better register allocation
because we can do the register allocation

00:24:20.300 --> 00:24:24.120
rate in the IR and we don't have to
do it in a platform-specific way at

00:24:24.120 --> 00:24:27.610
the higher level where we weren't
doing very good register allocation,

00:24:27.670 --> 00:24:30.120
so that's much improved.

00:24:30.130 --> 00:24:33.010
Because things have
broken down a little bit,

00:24:33.040 --> 00:24:34.800
whenever we access an array,

00:24:34.970 --> 00:24:39.280
We don't have to worry about the range
check and getting the base address

00:24:39.280 --> 00:24:40.710
of the data and so on and so forth.

00:24:40.820 --> 00:24:41.890
That's all broken down in finer grain.

00:24:41.890 --> 00:24:47.160
So now we can do unsafe
array access where we can,

00:24:47.200 --> 00:24:49.900
once we've got the base address
for an array calculated,

00:24:49.900 --> 00:24:52.230
we can get multiple entries
out of that array without going

00:24:52.230 --> 00:24:53.900
through those other calculations.

00:24:53.900 --> 00:24:57.100
So that improves performance as well.

00:24:58.220 --> 00:25:02.660
Okay, so what I want to do now is
talk about how to utilize

00:25:02.840 --> 00:25:05.600
some of these improvements.

00:25:05.680 --> 00:25:08.880
The first one, of course, is write small,
concise methods.

00:25:08.900 --> 00:25:15.760
I've always sort of promoted that,
but before, when you do that,

00:25:15.760 --> 00:25:21.310
you make much cleaner code and
it's a lot easier to compile

00:25:21.310 --> 00:25:25.970
because you're only compiling
specific code at different points.

00:25:26.030 --> 00:25:30.680
But now, you don't really have to pay as
much penalty for writing small,

00:25:30.680 --> 00:25:33.140
concise methods because of inlining.

00:25:33.220 --> 00:25:35.850
If your method is small
enough to fit in its caller,

00:25:35.900 --> 00:25:36.990
it'll get inlined.

00:25:36.990 --> 00:25:40.480
You don't pay the call overhead,
so it's to an advantage

00:25:40.480 --> 00:25:42.280
to continue that process.

00:25:43.900 --> 00:25:50.080
I always think accessors as being in
line to try to keep all external access

00:25:50.080 --> 00:25:54.940
of fields and statics through accessors.

00:25:54.940 --> 00:26:03.600
That way you have the advantage of
changing how those fields are accessed.

00:26:03.600 --> 00:26:06.010
You can put restrictions on them.

00:26:06.250 --> 00:26:08.500
This also makes it easier for
debugging to find out who's

00:26:08.620 --> 00:26:10.030
accessing those fields on you.

00:26:10.970 --> 00:26:15.610
You can write those accessor
functions or methods knowing

00:26:15.610 --> 00:26:20.480
that potentially they will,
for the most part, they will be in line.

00:26:20.480 --> 00:26:23.900
There will be no cost for putting
them in accessor wrappers.

00:26:26.950 --> 00:26:32.040
Final is somewhat superficial at
this point now that we have CHA.

00:26:32.050 --> 00:26:39.540
With CHA, we basically take classes
that we can assume that are,

00:26:39.540 --> 00:26:43.040
at least temporarily, that are final.

00:26:43.110 --> 00:26:47.000
And the optimization levels that you
would have gotten before with specifying

00:26:47.020 --> 00:26:52.060
final basically propagate to as many
cases as the compiler can figure out.

00:26:52.170 --> 00:26:54.970
So final is superficial.

00:26:55.280 --> 00:26:57.870
It's It's really
unnecessary at this point.

00:26:58.840 --> 00:27:03.280
Because new is inlined and we
do this per thread allocation,

00:27:03.280 --> 00:27:08.660
it doesn't make really that
much sense to keep pools around

00:27:08.800 --> 00:27:10.970
for specifically small objects.

00:27:10.990 --> 00:27:13.080
Larger objects, it's okay.

00:27:13.080 --> 00:27:18.120
If you have buffers that are 64K
bytes or something like that,

00:27:18.120 --> 00:27:21.700
it makes sense to have
pools for those maybe.

00:27:21.700 --> 00:27:26.360
But for smaller objects that
are going to be just around

00:27:26.360 --> 00:27:30.560
for a short period of time,
new is pretty fast and you don't

00:27:30.580 --> 00:27:32.760
really need to keep pools for those.

00:27:36.770 --> 00:27:43.600
Java Dev, there was a discussion recently
about programming by exception.

00:27:43.600 --> 00:27:47.590
Try to avoid using exceptions
as your programming model.

00:27:47.590 --> 00:27:54.150
Do your tests in code and use
exceptions for exceptional cases.

00:27:54.170 --> 00:27:56.160
That's really what they're designed for.

00:27:56.160 --> 00:28:00.150
As an example,
if you were to rely on the system

00:28:00.150 --> 00:28:06.360
to do your null checking for you,
when you hit the null pointer exception,

00:28:06.360 --> 00:28:09.060
it actually throws a
Mach kernel exception.

00:28:09.060 --> 00:28:11.200
We go into an exception handler.

00:28:11.200 --> 00:28:13.300
It goes and assembles the exception.

00:28:13.300 --> 00:28:15.250
We have to do a trace
back up through the stack.

00:28:15.330 --> 00:28:19.400
It can be pretty expensive.

00:28:19.400 --> 00:28:23.410
In your code, you could have just said,
if object's not equal to null,

00:28:23.410 --> 00:28:25.390
which is pretty lightweight.

00:28:27.600 --> 00:28:29.590
Exception handling also
inhibits optimization.

00:28:29.640 --> 00:28:38.160
So within your try/catch area,
it can be pretty hard on the compiler

00:28:38.160 --> 00:28:41.680
as far as what it can keep in registers,
as an example,

00:28:41.720 --> 00:28:45.340
because if a value is sitting in a
register and an exception is thrown,

00:28:45.340 --> 00:28:48.080
then that value in the register is lost.

00:28:48.080 --> 00:28:51.230
So the compiler has to make sure that
all values it's currently working

00:28:51.610 --> 00:28:58.040
with are stuffed out in memory,
and that can add to overall cost.

00:28:58.040 --> 00:29:01.680
So try to keep your catch-try around
the minimal amount of code so that

00:29:01.680 --> 00:29:03.880
you can get optimization around that.

00:29:04.820 --> 00:29:07.540
On the other hand,
if you have exception handling in there,

00:29:07.540 --> 00:29:13.500
it doesn't cost anything
until it's actually used.

00:29:13.500 --> 00:29:17.180
So when you enter a try block,
there's no cost as far as additional

00:29:17.180 --> 00:29:18.420
instructions are concerned.

00:29:20.300 --> 00:29:24.270
And finally,
the Hotspot compiler has been

00:29:24.270 --> 00:29:27.590
optimized for fairly clean code.

00:29:27.600 --> 00:29:34.280
Two things that disrupt that a bit
are JIKs and if you use an obfuscator.

00:29:34.280 --> 00:29:39.740
They rearrange the byte codes in such a
way that it's hard to pick up patterns.

00:29:39.740 --> 00:29:43.180
So what we recommend is that
you use JIKs for development,

00:29:43.300 --> 00:29:46.340
which basically gives
you speed of compilation,

00:29:46.340 --> 00:29:48.580
a quick turnaround in the compilation.

00:29:48.730 --> 00:29:51.000
But then if you're actually
going to deploy it and you want

00:29:51.000 --> 00:29:55.070
performance from the resulting code,
then use Java C.

00:29:56.520 --> 00:30:00.470
The obfuscators,
some of them are very mean as

00:30:00.470 --> 00:30:03.640
far as compilers are concerned.

00:30:03.640 --> 00:30:08.960
I've seen some of them that actually
put exception handlers around every

00:30:09.040 --> 00:30:14.030
sequence of byte codes just to
totally confuse the decompilers.

00:30:14.030 --> 00:30:17.840
That can be really hard on
the compiler as far as getting

00:30:17.840 --> 00:30:20.730
optimization is concerned.

00:30:20.820 --> 00:30:23.590
With that,
I'm going to pass it back to Victor.

00:30:34.380 --> 00:30:36.300
Thank you, Jim.

00:30:36.300 --> 00:30:39.290
New API in Java 2 Standard Edition 1.4.

00:30:39.290 --> 00:30:41.920
It's pretty straightforward.

00:30:41.940 --> 00:30:45.140
With every major revision from Sun,
you're going to get a

00:30:45.230 --> 00:30:47.360
lot of new packages,
a lot of new APIs,

00:30:47.360 --> 00:30:50.970
some of which you're interested,
some of which you're not going to use,

00:30:51.040 --> 00:30:54.450
but they're all going to be
there because they've been

00:30:54.450 --> 00:30:57.200
standardized into the Java platform.

00:30:57.200 --> 00:30:59.640
What are the new APIs?

00:31:00.550 --> 00:31:04.850
So I will be introducing the new APIs,
and then I will be concentrating

00:31:04.850 --> 00:31:10.440
on one API in particular that is
very special and exciting because it

00:31:10.580 --> 00:31:15.260
provides a performance opportunity
and not just added functionality.

00:31:16.000 --> 00:31:20.580
I will also be introducing changes
that are going to be made to the

00:31:20.580 --> 00:31:25.330
Java native interface with 1.4,
and I will be showing a demo

00:31:25.600 --> 00:31:30.170
highlighting this new performance
opportunity and also some of the

00:31:30.180 --> 00:31:35.010
work that Jim has been discussing
in the first half of this talk.

00:31:36.620 --> 00:31:40.320
So we know we get tons of new API.

00:31:40.320 --> 00:31:45.000
Basically,
I see the new non-GUI API basically

00:31:45.000 --> 00:31:47.600
into two categories plus a
bunch of miscellaneous packages.

00:31:47.600 --> 00:31:53.420
There's the new I/O packages,
which let you do I/O in a

00:31:53.420 --> 00:31:56.070
much more optimized way,
and I will be going into

00:31:56.070 --> 00:31:57.600
much more detail on those.

00:31:58.050 --> 00:32:00.790
Then there's all the XML packages.

00:32:01.100 --> 00:32:03.550
that have been available
before as standard extensions,

00:32:03.550 --> 00:32:06.710
but have now been brought
into the standard platform.

00:32:06.800 --> 00:32:11.480
If you've attended any of the
WebObjects and Web Services sessions,

00:32:11.480 --> 00:32:13.920
you'll know a lot about these.

00:32:14.140 --> 00:32:16.200
Then there's a few
miscellaneous packages.

00:32:16.240 --> 00:32:20.080
Java util logging gives you the
ability to do logging of your

00:32:20.380 --> 00:32:22.900
application while it's running.

00:32:24.230 --> 00:32:27.660
Java Utilprev lets you have
user and system-wide preferences

00:32:27.660 --> 00:32:29.400
for your Java application.

00:32:29.400 --> 00:32:34.540
And Java UtilregX,
regX meaning regular expressions.

00:32:34.540 --> 00:32:39.020
So basically you now have support
for regular expression processing,

00:32:39.020 --> 00:32:42.420
which is something, for example,
you can do very easily in Perl,

00:32:42.440 --> 00:32:45.180
and now is being brought in
as a standard part of Java.

00:32:45.180 --> 00:32:48.040
And there's also extensions
to the Java security package,

00:32:48.040 --> 00:32:51.720
which can be found in the
JavaX security package.

00:32:52.850 --> 00:32:54.780
So lots and lots of new functionality.

00:32:56.350 --> 00:32:59.340
But specifically,
an opportunity for you to

00:32:59.350 --> 00:33:04.440
improve the performance of your
application by using new I/O.

00:33:04.440 --> 00:33:08.000
So I'm going to be now
concentrating on specifically that.

00:33:10.010 --> 00:33:13.360
So that requires you to have a pretty
good understanding of what exactly is

00:33:13.360 --> 00:33:19.110
wrong with the present state of Java I/O,
how those problems are solved by new I/O,

00:33:19.190 --> 00:33:21.920
and then exactly how you
go about using new I/O.

00:33:23.640 --> 00:33:28.780
The three parts... There's
basically four parts to the new I.O.

00:33:29.260 --> 00:33:29.960
packages.

00:33:29.960 --> 00:33:33.120
So there's native buffers,
there's channels,

00:33:33.120 --> 00:33:37.240
there's Unicode support for the
I.O., and there's also JNI support

00:33:37.240 --> 00:33:38.770
with new added functions.

00:33:45.370 --> 00:33:49.470
So, we all know that
Java distinguishes between the

00:33:49.530 --> 00:33:54.660
Java heap and the native heap,
and the Java heap is only for

00:33:54.980 --> 00:33:57.970
Java objects and Java arrays.

00:33:59.890 --> 00:34:04.440
And it makes this distinction to
be able to do the precise garbage

00:34:04.440 --> 00:34:06.730
collection that we are able to do.

00:34:06.850 --> 00:34:13.040
This unfortunately has the side effect
that if you ever need to gain access to a

00:34:13.220 --> 00:34:15.400
Buffer that's residing in native memory.

00:34:15.420 --> 00:34:19.280
You actually need,
and you want to do processing

00:34:19.280 --> 00:34:21.190
on that buffer from Java code.

00:34:21.230 --> 00:34:25.590
You need to copy it into the Java heap.

00:34:27.520 --> 00:34:30.000
and you do that via JNI.

00:34:30.000 --> 00:34:37.460
That has the added cost
of the JNI overhead,

00:34:37.460 --> 00:34:41.380
which is definitely,
if you can avoid that, would be great.

00:34:41.590 --> 00:34:45.430
There's also another problem with this,
which is the fact that now since

00:34:45.430 --> 00:34:50.270
it's residing in the Java heap,
the garbage collector treats that

00:34:50.270 --> 00:34:55.870
as a Java object and actually moves
it around during garbage collection.

00:34:55.880 --> 00:35:01.880
Even though that native buffer we know
is not containing any objects and is just

00:35:01.880 --> 00:35:05.310
simply an overhead of moving that around.

00:35:12.250 --> 00:35:17.470
So both of these costs add up to--

00:35:18.100 --> 00:35:23.300
Bad performance of your I/O if
you need to use native buffers,

00:35:23.300 --> 00:35:29.100
which you need to do if you're
doing any file I/O or sockets.

00:35:30.460 --> 00:35:33.570
So, why is it that these problems exist?

00:35:33.630 --> 00:35:37.960
The problems exist because you
cannot get access to native

00:35:37.960 --> 00:35:40.030
memory directly from Java code.

00:35:41.240 --> 00:35:46.200
Every buffer has to have a native copy.

00:35:46.200 --> 00:35:49.600
And as I explained before,
there's incredible overhead.

00:35:49.640 --> 00:35:54.600
I don't know about incredible,
but there is overhead associated with the

00:35:54.740 --> 00:35:58.040
JNI and CPU during the GC CPU overhead.

00:35:59.670 --> 00:36:02.490
You might say, well, there is another way
of actually doing this,

00:36:02.560 --> 00:36:07.520
which is basically getting access to
a buffer residing in your Java heap

00:36:07.600 --> 00:36:13.150
from a native method using the
getPrimitiveOrAcritical JNI method.

00:36:14.620 --> 00:36:16.500
That is true,
but you must be aware of the

00:36:16.610 --> 00:36:19.940
fact that whenever you do a
getPrimitiveArrayCritical,

00:36:19.940 --> 00:36:24.600
the garbage collector is actually
paused until you actually

00:36:24.660 --> 00:36:27.220
release that primitive array.

00:36:27.220 --> 00:36:31.250
While you're actually doing the
processing in your native method,

00:36:31.610 --> 00:36:36.380
you actually might be pausing any
of your other threads if any of them

00:36:36.380 --> 00:36:40.810
have gone into a situation where a
garbage collection needs to happen.

00:36:42.970 --> 00:36:46.940
There's also the problem where our
collection moves the buffers around

00:36:46.940 --> 00:36:49.810
so you have the CPU overhead of that.

00:36:50.430 --> 00:36:55.230
You also can't do... There's
also limited functionality due

00:36:55.290 --> 00:36:59.910
to the fact that the Java buffer,
the Java heap copy of the buffer

00:37:00.100 --> 00:37:02.250
is actually moving around.

00:37:02.270 --> 00:37:04.940
So you can't do things
like memory map files,

00:37:05.110 --> 00:37:07.610
and you also can't do non-blocking I.O.

00:37:07.610 --> 00:37:12.670
All of that leads to the fact that it's
pretty... that this does not scale well,

00:37:12.670 --> 00:37:17.380
and anyone who has... who has written
a server trying to use Java I.O.

00:37:17.380 --> 00:37:20.160
knows that you have to be rather careful.

00:37:20.300 --> 00:37:23.190
As to, you know,
how many threads you open for all of

00:37:23.190 --> 00:37:25.280
your connections and that sort of thing.

00:37:25.390 --> 00:37:27.740
So how is Java new I.O.

00:37:27.740 --> 00:37:29.690
going to fix this?

00:37:32.950 --> 00:37:33.560
Pretty straightforward.

00:37:33.560 --> 00:37:39.920
You have the ability now to both
allocate and directly access native

00:37:40.120 --> 00:37:42.190
memory from your Java method.

00:37:44.930 --> 00:37:48.620
You can allocate the native memory.

00:37:48.640 --> 00:37:53.160
The way that this is introduced into
the Java platform is that it's actually

00:37:53.160 --> 00:37:57.760
not an extension to the language,
but actually done via an API.

00:37:59.690 --> 00:38:03.810
So, you actually are creating an
object wrapper that contains a

00:38:03.810 --> 00:38:07.820
pointer to the native memory.

00:38:09.600 --> 00:38:13.540
The really nice thing about this is
that you also have direct access without

00:38:13.540 --> 00:38:16.340
having to go into a native method.

00:38:18.400 --> 00:38:23.260
and that is done via get
and put method calls.

00:38:23.260 --> 00:38:26.440
Those get and put method calls
are actually integrated into the

00:38:26.520 --> 00:38:30.920
Hotspot JIT compiler as an intrinsic,
which Jim spoke about earlier,

00:38:30.920 --> 00:38:35.290
so we can actually get incredibly
good performance out of this.

00:38:36.930 --> 00:38:40.490
We also avoid the garbage collection in
JNI overrides simply because you don't

00:38:40.490 --> 00:38:41.940
actually need to have any extra copies.

00:38:41.940 --> 00:38:44.780
You have one copy which you can
directly access from both your

00:38:45.050 --> 00:38:46.820
native methods and your Java code.

00:38:46.830 --> 00:38:50.420
And you actually can do non-blocking I.O.

00:38:50.740 --> 00:38:55.550
now since you have the ability
to do asynchronous operations

00:38:55.640 --> 00:38:57.640
on that one location.

00:38:59.260 --> 00:39:03.170
and the goal has been achieved,
which is basically to get

00:39:03.800 --> 00:39:05.040
native IO performance.

00:39:07.180 --> 00:39:09.840
One of the things that's interesting
about this is that since it's

00:39:09.880 --> 00:39:13.760
been introduced as an API,
it actually is supplementing

00:39:13.910 --> 00:39:15.940
the pre-existing Java I/O.

00:39:16.250 --> 00:39:18.100
It doesn't replace it.

00:39:18.100 --> 00:39:23.410
So you can continue to use your code,
which uses Java I/O.

00:39:24.060 --> 00:39:27.740
But it might be interesting,
it might be worthwhile to change

00:39:27.930 --> 00:39:29.380
it depending on the situation.

00:39:29.380 --> 00:39:32.520
I'll go into those situations later.

00:39:32.520 --> 00:39:36.840
But also it's interesting to note
that it is not fully used throughout

00:39:36.840 --> 00:39:41.080
the Java runtime environment.

00:39:41.080 --> 00:39:41.080
Particularly a lot of...

00:39:41.290 --> 00:39:46.870
Graphics operations still are requiring
Java arrays as their underlying

00:39:46.870 --> 00:39:54.760
representation for the graphics
and are not yet using native buffers.

00:39:54.760 --> 00:39:58.640
That will be coming in
future releases of the JDK.

00:39:58.640 --> 00:40:00.170
Support for that.

00:40:04.150 --> 00:40:09.100
So there are three parts
to the new I/O APIs.

00:40:09.110 --> 00:40:15.550
The first one is the
Java and I/O buffers.

00:40:16.470 --> 00:40:18.500
These are containers for
data of primitive types.

00:40:18.500 --> 00:40:23.480
There's a Java and I/O buffer
for every Java type.

00:40:23.520 --> 00:40:26.290
There's an int buffer,
there's a char buffer,

00:40:26.320 --> 00:40:27.400
there's a long buffer.

00:40:27.400 --> 00:40:34.060
But underneath all of
that is a byte buffer.

00:40:34.740 --> 00:40:39.660
and all the I/O operations are
actually done on the byte buffer.

00:40:39.680 --> 00:40:43.560
As I said earlier,
you access these via get and put

00:40:43.640 --> 00:40:46.970
methods and these are integrated
into the hotspot compiler.

00:40:48.620 --> 00:40:53.110
Since the I/O operations are
actually done on byte buffers,

00:40:53.110 --> 00:40:56.210
you actually need to
create a byte buffer,

00:40:56.210 --> 00:40:59.600
use that byte buffer for
all your I/O operations,

00:40:59.600 --> 00:41:03.320
and then actually when you
want to use it from Java,

00:41:03.320 --> 00:41:07.760
you say byte buffer as int buffer
or byte buffer as long buffer

00:41:08.730 --> 00:41:13.900
to get access to the Java types.

00:41:15.690 --> 00:41:19.160
The byte buffer has
support for being direct.

00:41:19.160 --> 00:41:21.920
By direct,
I mean that any I/O operation is

00:41:21.920 --> 00:41:25.980
done trying to use the native buffer.

00:41:25.980 --> 00:41:29.920
You have the ability of actually
adding a backing array that actually

00:41:29.920 --> 00:41:33.960
is resident in the Java heap,
but if it's direct,

00:41:33.960 --> 00:41:38.940
you can actually set it so
it uses the byte buffer.

00:41:39.020 --> 00:41:41.870
Since the byte buffer
represents a native buffer,

00:41:41.870 --> 00:41:44.070
you actually can do the memory mapped

00:41:45.090 --> 00:41:47.010
One of the things that's
interesting about this is that the

00:41:47.090 --> 00:41:56.360
allocation-to-allocation costs of
creating a Java NIO buffer is actually

00:41:56.360 --> 00:41:58.580
greater than of creating an array.

00:41:58.580 --> 00:42:02.040
Hotspot does a great job of
optimizing the allocation

00:42:02.510 --> 00:42:06.810
directly into the Java heap,
both of objects and of arrays.

00:42:06.820 --> 00:42:12.260
So it's not possible to get
as good performance as that.

00:42:12.540 --> 00:42:15.440
Actually,
I wouldn't make that assertion anyways.

00:42:15.440 --> 00:42:20.030
But the reality is that you
should be aware of this.

00:42:20.040 --> 00:42:23.140
And it's not just simply, oh,
anywhere where I was using an array,

00:42:23.140 --> 00:42:24.650
I now need to use an NIO buffer.

00:42:24.740 --> 00:42:28.120
You basically need to check to see
if it applies to your application.

00:42:28.120 --> 00:42:32.620
The best rule of thumb is that if it's
going to be long-living and it's large,

00:42:32.620 --> 00:42:34.440
you want to avoid all those copies.

00:42:34.440 --> 00:42:36.820
You want to avoid all the CPU overhead.

00:42:36.820 --> 00:42:40.540
And therefore, it's probably worthwhile
to use a Java NIO buffer.

00:42:44.900 --> 00:42:49.480
So where are the actual
I/O operations defined?

00:42:49.480 --> 00:42:52.230
They're defined inside of the
Java and I/O channels package.

00:42:54.640 --> 00:42:58.260
The Java and I/O channels
allow non-blocking and

00:42:58.260 --> 00:43:01.470
interruptible operations.

00:43:01.850 --> 00:43:07.340
There is support for file system access,
and that's in the file channel.

00:43:07.340 --> 00:43:10.080
And that actually has,
as I've said before,

00:43:10.080 --> 00:43:13.640
memory mapping support and
also file locking support,

00:43:13.640 --> 00:43:18.110
so you can actually lock certain
sections of a file from other

00:43:18.110 --> 00:43:20.960
threads within your application.

00:43:22.710 --> 00:43:25.410
There's also support for
inter-process communication.

00:43:26.000 --> 00:43:30.600
Remember, these are interruptible.

00:43:30.680 --> 00:43:32.900
There's socket channel and pipe channel.

00:43:32.900 --> 00:43:40.870
These are equivalent to the old
input and output stream classes

00:43:40.870 --> 00:43:42.760
that you were accustomed to before.

00:43:47.560 --> 00:43:54.660
The third part of the new
I/O APIs is the Unicode support.

00:43:54.660 --> 00:43:57.080
This one's rather straightforward.

00:43:57.180 --> 00:44:02.570
Since you might want to be doing text
processing on that input and output,

00:44:02.580 --> 00:44:10.030
you want to have the support for mapping
directly from the bytes to Unicode,

00:44:10.630 --> 00:44:13.090
and Java supports that.

00:44:13.580 --> 00:44:17.720
It supports all Unicode encodings,
and that actually is done

00:44:17.890 --> 00:44:22.380
specifically on the byte buffer
and character buffer classes.

00:44:27.500 --> 00:44:34.500
The final part of the NIO functionality
is actually additions to the JNI methods.

00:44:34.500 --> 00:44:42.300
They've basically added three
JNI functions so you can use new I/O.

00:44:42.390 --> 00:44:43.760
First, there's the new direct

00:44:45.360 --> 00:44:46.180
Method.

00:44:46.570 --> 00:44:51.690
That lets you create a byte buffer
from your native code so that you

00:44:51.690 --> 00:44:57.170
can actually wrap any pre-existing
native buffer in a direct byte

00:44:57.170 --> 00:45:00.410
buffer and then pass it back to Java.

00:45:00.420 --> 00:45:06.630
Then there's the opposite operation,
which is getting access to

00:45:06.650 --> 00:45:11.970
the native buffer from the
encapsulating Java object,

00:45:11.980 --> 00:45:14.730
which is the byte buffer.

00:45:15.800 --> 00:45:20.230
You might be concerned that since
there's new functions available in the

00:45:20.230 --> 00:45:25.740
1.4 Java Native interface that you will
have to update your old JNI libraries.

00:45:25.740 --> 00:45:27.770
That is actually not the case.

00:45:28.160 --> 00:45:30.840
Your old JNI libraries
will be compatible,

00:45:30.840 --> 00:45:35.340
but there are other reasons
to maybe update yours.

00:45:35.550 --> 00:45:41.680
The first one is actually to be able
to use the new I/O functionality.

00:45:41.680 --> 00:45:44.300
The other reason is,
as was mentioned in the

00:45:44.300 --> 00:45:49.120
Java VM internals talk,
we've actually made changes to the

00:45:49.120 --> 00:45:56.140
Java implementation in Jaguar so that
we actually support JNI libraries

00:45:56.230 --> 00:45:59.370
that are dilibs and not just bundles.

00:45:59.370 --> 00:46:03.560
It might be a good opportunity
to switch over to doing that

00:46:03.790 --> 00:46:05.880
with the 1.4 transition.

00:46:08.710 --> 00:46:11.980
So, APIs are great,
but there's nothing better than

00:46:11.980 --> 00:46:13.540
actually looking at some code.

00:46:16.820 --> 00:46:23.970
I have here some example code that
I was using to check to see how

00:46:24.000 --> 00:46:26.930
good the performance actually is.

00:46:27.930 --> 00:46:32.600
There are three main things
I want to highlight as far

00:46:32.730 --> 00:46:33.940
as the code is concerned.

00:46:33.940 --> 00:46:37.940
The first one is how

00:46:38.320 --> 00:46:44.210
to do memory map file input and output.

00:46:44.260 --> 00:46:46.200
And that's pretty straightforward.

00:46:46.200 --> 00:46:50.420
You still use the old file input stream.

00:46:50.420 --> 00:46:56.990
And now file input stream
actually has a get channel method.

00:46:57.140 --> 00:47:00.400
which gives you access to the channel.

00:47:00.490 --> 00:47:02.780
At that point,
then you can call the mapping

00:47:02.890 --> 00:47:10.660
function and you can say read
only or read/write and whatnot.

00:47:11.210 --> 00:47:12.710
This is where it gets interesting.

00:47:12.790 --> 00:47:16.060
This actual piece of code,
what I've actually done is that

00:47:16.060 --> 00:47:20.820
what I'm doing is I'm reading
in a file of pixel information.

00:47:20.900 --> 00:47:24.340
I'm making myself a copy,
which then I'm creating a server

00:47:24.340 --> 00:47:26.900
for that I'm going to send
over to another application.

00:47:26.920 --> 00:47:31.810
So the reason I'm making a copy
is because I'm then expecting the

00:47:31.820 --> 00:47:35.490
other guy to send it back to me,
and I need to have some place to

00:47:35.730 --> 00:47:37.870
store the pixels he sends back to me.

00:47:38.040 --> 00:47:42.530
So I'm going to actually throw away the
reference to the memory mapped file.

00:47:44.360 --> 00:47:48.040
So you can see that I actually allocate
the byte buffer as a byte buffer,

00:47:48.040 --> 00:47:53.750
not as an int buffer,
even though we know that that's

00:47:53.810 --> 00:47:58.160
the correct representation
for a buffered image.

00:47:58.160 --> 00:48:01.230
And then here's a few operations.

00:48:04.780 --> 00:48:08.030
You need to mark the buffer because
basically a buffer needs to know

00:48:08.440 --> 00:48:11.030
what its beginning position is,
what its limit is,

00:48:11.030 --> 00:48:14.390
and where the present location is,
which is actually known as the position.

00:48:15.980 --> 00:48:22.880
By default, when you create a buffer,
it does not have a beginning location.

00:48:23.410 --> 00:48:27.940
What you need to actually do is call
mark on it to make the beginning of

00:48:27.940 --> 00:48:30.800
it the actual beginning location.

00:48:30.800 --> 00:48:35.330
Otherwise you'll get an exception
later when you try to revert

00:48:35.330 --> 00:48:37.580
to the beginning location.

00:48:37.580 --> 00:48:41.880
Then reading the file contents
is as simple as calling put.

00:48:42.430 --> 00:48:45.750
And that advances the buffer to the end.

00:48:45.780 --> 00:48:49.960
So I need to do a reset to bring it
back all the way to the beginning.

00:48:52.520 --> 00:48:56.390
The last operation you can
see right now is actually

00:48:56.940 --> 00:49:02.320
copying into a Java int array.

00:49:02.320 --> 00:49:05.260
As I said before,
this isn't integrated throughout

00:49:05.260 --> 00:49:06.990
the Java runtime environment.

00:49:07.560 --> 00:49:13.300
I actually still need to do the copy,
which would be great if I could avoid,

00:49:13.300 --> 00:49:16.300
but unfortunately the
APIs don't support that yet.

00:49:16.300 --> 00:49:21.570
That's basically what's going on there.

00:49:22.700 --> 00:49:33.000
[Transcript missing]

00:49:34.580 --> 00:49:38.460
There's selectors inside of
the Java and I/O package.

00:49:38.490 --> 00:49:41.700
You create a server socket channel.

00:49:41.720 --> 00:49:44.060
And then here's a key one right here.

00:49:44.130 --> 00:49:46.480
By default,
it actually is set to blocking,

00:49:46.490 --> 00:49:51.520
and you can get non-blocking
I/O simply by making it false.

00:49:52.300 --> 00:49:55.480
Then you do your standard binding.

00:49:55.660 --> 00:50:01.000
You need to bind that
channel to an address.

00:50:04.500 --> 00:50:24.700
[Transcript missing]

00:50:29.750 --> 00:50:33.370
So this is actually the iterator,
and this is where I actually

00:50:33.370 --> 00:50:35.580
accept the connection.

00:50:39.730 --> 00:50:47.400
And now what I'm actually going to do
is write my first transcending over

00:50:47.400 --> 00:50:50.000
the pixels over to the other guy.

00:50:51.660 --> 00:50:58.190
And you can see right here, where is it?

00:51:00.760 --> 00:51:01.130
There it is.

00:51:01.130 --> 00:51:03.520
So this is how you write to the channel.

00:51:03.520 --> 00:51:06.780
And you'll see that I'm doing
that directly for my byte

00:51:06.780 --> 00:51:11.240
buffer and not having to do
that directly from my int array,

00:51:11.240 --> 00:51:14.560
even though I still needed that to
actually draw the pixels to the screen.

00:51:16.840 --> 00:51:19.250
I use this position stuff to
figure out how far I've gone,

00:51:19.520 --> 00:51:23.510
but you see that every time you write,
you actually update the

00:51:23.510 --> 00:51:26.330
present position in the buffer.

00:51:27.970 --> 00:51:29.090
So that gives you a general idea.

00:51:29.160 --> 00:51:32.850
There's really good example code
available at the Sun website.

00:51:32.990 --> 00:51:36.440
Just look for a new I/O and you'll
see plenty of great examples.

00:51:39.510 --> 00:51:43.070
Now I want to give you a
demonstration of all this

00:51:43.110 --> 00:51:46.500
technology we've been talking about.

00:51:46.710 --> 00:51:52.810
You might have seen this before,
which is the Grand Canyon demo.

00:51:55.530 --> 00:51:59.820
But what I want to emphasize this
time is actually what part of the

00:52:00.050 --> 00:52:04.270
technologies that Jim and I have
been talking about today is actually

00:52:04.270 --> 00:52:06.390
in use during this demonstration.

00:52:06.500 --> 00:52:11.820
The Grand Canyon demo is a great flight
simulator written by Ken Russell,

00:52:11.960 --> 00:52:14.820
who is a hotspot engineer over at Sun.

00:52:17.100 --> 00:52:23.170
And what we have here
is a real-time renderer.

00:52:23.520 --> 00:52:27.470
And right now, we have the notion of,
all right, we're flying in 1.3.1 mode,

00:52:27.470 --> 00:52:29.100
and we're flying in 1.4 mode.

00:52:29.120 --> 00:52:33.530
The reality is that we're running
this on the 1.4 VM that is available

00:52:33.530 --> 00:52:35.660
to you in the developer preview.

00:52:39.550 --> 00:52:42.960
Let me actually get it so
it's looking at something.

00:52:42.960 --> 00:52:44.200
There we go.

00:52:44.550 --> 00:52:47.360
And then I'll fly down.

00:52:51.000 --> 00:52:54.660
So, where am I?

00:52:55.270 --> 00:52:56.840
There we go.

00:52:56.840 --> 00:52:58.760
Or somewhere.

00:52:58.760 --> 00:52:59.860
I've actually never flown a plane.

00:53:01.430 --> 00:53:03.500
All right,
so this is actually in 1.3.1 mode,

00:53:03.840 --> 00:53:08.600
and the exciting thing is to
show you that... All right,

00:53:08.600 --> 00:53:11.360
if I can fly a little
farther down... There we go.

00:53:11.500 --> 00:53:15.920
You see that there's a choppiness,
but if I switch over to 1.4 mode,

00:53:15.950 --> 00:53:18.660
it suddenly gets smooth,
and then I go back...

00:53:23.400 --> 00:53:28.600
[Transcript missing]

00:53:33.470 --> 00:53:37.840
So,
but both modes are running on the 1.4 VM.

00:53:37.840 --> 00:53:43.640
So there are actually benefits to the
1.3.1 mode that wouldn't even be gotten

00:53:43.640 --> 00:53:45.760
if you were running under a 1.3.1 VM.

00:53:47.320 --> 00:53:50.510
Primarily,
all the deep inlining and compiler

00:53:50.510 --> 00:53:53.390
optimizations that Jim talked about.

00:53:55.100 --> 00:53:57.330
and are available to it.

00:53:57.380 --> 00:54:01.500
So it actually,
if we were running this on a 1.3.1 VM,

00:54:01.510 --> 00:54:06.560
even though I don't know how
well we could since it needs

00:54:06.560 --> 00:54:12.400
to be able to run the 1.4 mode,
it wouldn't be running even this fast.

00:54:13.040 --> 00:54:18.180
What the 1.3.1 mode is saying is that
basically the inner rendering loop

00:54:18.540 --> 00:54:22.070
is using NIO and,
in contrast to the 1.3.1

00:54:22.100 --> 00:54:26.260
inner rendering loop,
is actually using the typical

00:54:26.260 --> 00:54:32.570
copy out to native buffer,
and then from there copy, copy out,

00:54:32.570 --> 00:54:32.570
and then from there

00:54:33.540 --> 00:54:34.280
I'm going to fly.

00:54:34.280 --> 00:54:35.080
All right.

00:54:35.220 --> 00:54:39.280
I'm going to leave this alone
if I can and just talk about it.

00:54:40.900 --> 00:54:43.550
Let's see, is it in 1.3.1 mode?

00:54:43.560 --> 00:54:46.890
Let me put it into... Oh, no,
that is 1.3.1 mode.

00:54:47.160 --> 00:54:51.770
That is 1.4 mode.

00:54:51.770 --> 00:54:51.770
There we go.

00:54:51.770 --> 00:54:51.770
So...

00:54:52.160 --> 00:54:57.300
The inner rendering loop needs to
get the pixels over to the graphics

00:54:58.480 --> 00:55:02.460
card because this is actually
built on top of GL for Java.

00:55:03.260 --> 00:55:07.700
By reducing the amount of copying
overhead by using Java New I.O.

00:55:07.720 --> 00:55:12.920
That's the main part of where we
get the improvement in the drawing.

00:55:12.940 --> 00:55:16.730
The other part where we get the
improvement is the fact that

00:55:16.730 --> 00:55:20.940
by using all the deep inlining,
we actually are able to...

00:55:22.370 --> 00:55:26.540
have no virtual function calls
inside of that tight rendering loop,

00:55:26.540 --> 00:55:29.300
which actually makes a huge difference.

00:55:29.300 --> 00:55:34.700
If you turn off the deep inlining,
the performance of this is

00:55:34.700 --> 00:55:37.250
an order of magnitude worse.

00:55:37.460 --> 00:55:41.300
That is basically
highlighting the fact that

00:55:41.730 --> 00:55:46.120
All of the technologies we've
described today are going to

00:55:46.120 --> 00:55:48.900
be benefiting your application.

00:55:48.900 --> 00:55:51.130
That is that demonstration.

00:56:00.680 --> 00:56:03.050
So in summary,
what we've done is we've discussed the

00:56:03.120 --> 00:56:05.050
performance of your Java application.

00:56:06.490 --> 00:56:10.980
We've told you what improvements
we're making for you in our changes

00:56:11.050 --> 00:56:15.510
to the Hotspot JIT compiler,
and also what changes you might

00:56:15.600 --> 00:56:19.970
want to make to your application
when writing in Java 1.4 that

00:56:20.420 --> 00:56:23.120
will improve your performance.

00:56:23.120 --> 00:56:25.940
What else?

00:56:30.280 --> 00:56:30.810
Who to contact?

00:56:30.960 --> 00:56:34.030
There's the contact information
for our technologies evangelist,

00:56:34.030 --> 00:56:35.220
our product manager.

00:56:35.220 --> 00:56:38.840
That's Alan Samuel and Alan Dennison,
Jim and myself.

00:56:44.410 --> 00:56:46.300
Here's some more information.

00:56:46.300 --> 00:56:47.400
Where to find us.

00:56:47.400 --> 00:56:50.460
Where to find information
about 1.4 directly from Sun.

00:56:50.460 --> 00:56:54.210
Also this great website that actually
gives you a pretty good synopsis

00:56:54.220 --> 00:56:57.310
of all the other APIs that I didn't
go into too much depth about,

00:56:57.310 --> 00:56:59.360
why you would be
interested in using them.

00:57:02.200 --> 00:57:11.360
We also have performance
tips from last year's talk.

00:57:11.360 --> 00:57:16.870
Last year's talk we actually talked
about how to profile your application,

00:57:16.870 --> 00:57:17.640
that sort of thing.

00:57:17.640 --> 00:57:20.570
We have handouts that
might be useful to you,

00:57:20.570 --> 00:57:22.690
so please come and get those.

00:57:25.720 --> 00:57:31.900
Here's a few more information
on how to find documentation,

00:57:31.900 --> 00:57:34.260
how to contact us.

00:57:34.260 --> 00:57:35.960
A lot of us read the
Java dev mailing list.

00:57:36.010 --> 00:57:39.300
And how to get technical support,
DTS at Apple.com.