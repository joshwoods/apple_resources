WEBVTT

00:00:03.860 --> 00:00:06.700
Welcome to the
Core Audio Technologies session.

00:00:06.740 --> 00:00:07.500
I'm Craig Keithley.

00:00:07.500 --> 00:00:10.260
I'm Apple's USB and FireWire evangelist.

00:00:10.260 --> 00:00:13.700
My role in evangelism is to help,
primarily,

00:00:13.700 --> 00:00:17.640
developers bring products to the Mac,
and audio is a special favorite of mine,

00:00:17.640 --> 00:00:22.180
so I'm pleased to help introduce
the audio folks here today.

00:00:22.300 --> 00:00:24.660
One of the things -- of all the things
we're going to talk about today,

00:00:24.660 --> 00:00:28.120
I think you'll find the 3D
audio stuff that we've been

00:00:28.120 --> 00:00:30.050
working on pretty exciting.

00:00:30.070 --> 00:00:36.070
We'll also go into the MIDI aspects of
Core Audio and the multichannel reverb.

00:00:36.270 --> 00:00:37.720
So with that, let me bring up Jeff.

00:00:37.720 --> 00:00:40.620
Thank you very much.

00:00:47.300 --> 00:00:48.200
Good afternoon.

00:00:48.210 --> 00:00:50.100
My name is Jeff Moore.

00:00:50.110 --> 00:00:52.240
I'm an engineer with
the Core Audio Group.

00:00:52.370 --> 00:00:55.290
And today I'm going to tell you
a little bit about how to use

00:00:55.360 --> 00:00:59.670
the AudioConverter API in the
Core Audio -- in the Audio Toolbox.

00:01:00.240 --> 00:01:06.310
So the AudioConvertor API is
for transforming audio data in

00:01:06.310 --> 00:01:09.960
one format into another format.

00:01:09.960 --> 00:01:12.130
We provide converters for
doing rate conversion,

00:01:12.130 --> 00:01:14.800
integer to floating point,
floating point to integer,

00:01:14.800 --> 00:01:18.540
interleaving and deinterleaving
of multichannel data,

00:01:18.540 --> 00:01:23.300
and of course,
encoding and decoding nonlinear PCM data.

00:01:25.630 --> 00:01:28.400
So the,
first I want to get into a little bit

00:01:28.400 --> 00:01:31.420
of how audio formats are described.

00:01:32.130 --> 00:01:38.600
The way we describe formats is common
throughout all of the Core Audio APIs,

00:01:38.600 --> 00:01:41.380
so having a good grounding
here will help you when you get

00:01:41.380 --> 00:01:43.100
into the more advanced APIs.

00:01:43.100 --> 00:01:46.740
The basic structure for
encapsulating format information is

00:01:46.740 --> 00:01:49.100
the AudioStream Basic Description.

00:01:49.110 --> 00:01:53.100
It has fields for
describing the sample rate,

00:01:53.100 --> 00:01:57.100
the 4-care code, format ID,
some flags to identify

00:01:57.100 --> 00:02:00.040
to specialize the format,
and a bunch of other information

00:02:00.090 --> 00:02:02.560
that's important to know:
how big the data is,

00:02:02.560 --> 00:02:06.740
what its channelization is like,
and other information about

00:02:06.740 --> 00:02:09.100
how to process the data.

00:02:09.100 --> 00:02:13.260
In Core Audio,
we use 32-bit floating-point numbers

00:02:13.260 --> 00:02:16.100
as the canonical linear PCM format.

00:02:16.100 --> 00:02:19.200
In general,
what you'll find is that everything

00:02:19.200 --> 00:02:24.100
goes to 32-bit float and then from
there back into some other format.

00:02:24.140 --> 00:02:27.590
And like I said,
both interleaved and non-interleaved

00:02:27.590 --> 00:02:30.100
multichannel data is fully supported.

00:02:30.100 --> 00:02:34.230
So you can take a buffer full of
interleaved stereo and split it into two

00:02:34.230 --> 00:02:39.100
monobuffers or put them back together,
whatever your needs are.

00:02:40.380 --> 00:02:45.640
So new in the AudioConverter and Jaguar
is support for encoded audio formats.

00:02:45.640 --> 00:02:51.800
And what I mean by encoded formats
are formats that are not linear PCM.

00:02:51.800 --> 00:02:58.510
Examples of this are MP3 data, AAC data,
AU, IMA, all sorts of stuff that is

00:02:58.510 --> 00:03:06.060
grouped together in -- that
can't be separated apart.

00:03:06.060 --> 00:03:09.720
Encoded formats are typically dealt
with in terms of packets of data

00:03:09.720 --> 00:03:13.750
instead of in terms of frames,
like linear PCM data is.

00:03:13.880 --> 00:03:18.320
This means that you will be getting
chunks of data that contain a certain

00:03:18.320 --> 00:03:21.770
number of frames of audio data in them.

00:03:22.100 --> 00:03:42.400
[Transcript missing]

00:03:42.640 --> 00:03:46.550
and David And then when you start
talking about the MPEG-4 formats,

00:03:46.550 --> 00:03:51.180
you're going to start seeing that even
more external description in terms of

00:03:51.190 --> 00:03:56.920
how the packets are broken up is also
required to fully describe the data.

00:03:56.920 --> 00:03:59.660
And all the APIs that we've added
to the AudioConverter and Jaguar

00:03:59.770 --> 00:04:02.270
fully support these mechanisms.

00:04:03.180 --> 00:04:06.090
So, first thing you want to need
to know is how to create and

00:04:06.090 --> 00:04:10.040
configure an AudioConverter to
do what it is you want to do.

00:04:10.040 --> 00:04:14.590
So, here you see the way you do it.

00:04:14.710 --> 00:04:17.080
You basically just fill
out the input format,

00:04:17.080 --> 00:04:21.090
you fill out the output format,
and then you call AudioConverter new.

00:04:21.120 --> 00:04:26.330
And if there is a translation
that can be built from your input

00:04:26.570 --> 00:04:31.220
format to your output format,
the converter will create it

00:04:31.220 --> 00:04:35.990
for you and configure it and be
ready to go to a certain extent.

00:04:37.270 --> 00:04:40.960
Next,
once you have the converter created,

00:04:41.130 --> 00:04:45.300
some formats of data,
particularly encoded formats of data,

00:04:45.300 --> 00:04:48.540
require some external
configuration information.

00:04:48.540 --> 00:04:52.050
In the API,
we've turned this the magic cookie

00:04:52.050 --> 00:04:57.730
because the data is wholly private to the
format and should under no circumstances

00:04:57.730 --> 00:04:59.260
be looked at by the application.

00:04:59.260 --> 00:05:00.700
You won't understand it anyway.

00:05:02.220 --> 00:05:05.180
So,
and the way you tell the converter about

00:05:05.180 --> 00:05:10.980
the magic cookie for the format you're
encoding or decoding is quite simple.

00:05:10.980 --> 00:05:15.100
You just call AudioConverterSetProperty
with the CodecParameters property.

00:05:15.100 --> 00:05:20.060
And it'll just get handed off to
the appropriate encoder or decoder

00:05:20.130 --> 00:05:23.270
depending on the task you have at hand.

00:05:24.050 --> 00:05:29.420
So there are other configuration
parameters that you can change about your

00:05:29.420 --> 00:05:31.380
AudioConverter while you're doing it.

00:05:31.380 --> 00:05:33.900
One of the most common ones that
you want to play with is changing

00:05:33.900 --> 00:05:35.730
the algorithm for rate conversion.

00:05:35.880 --> 00:05:37.900
Now, you want to do this for
a variety of reasons.

00:05:37.900 --> 00:05:41.020
You want to trade quality for
performance in a lot of cases,

00:05:41.020 --> 00:05:42.680
like in a game or something.

00:05:42.680 --> 00:05:46.800
And then you -- or in some cases,
you may want the best quality possible.

00:05:46.800 --> 00:05:50.680
So here we see an example of
setting the Converter Rate Converter

00:05:50.790 --> 00:05:54.530
algorithm to the polyphase,
which is the highest quality rate

00:05:54.540 --> 00:05:56.400
converter algorithm we support.

00:05:56.400 --> 00:05:59.710
Again,
it shows up in the API as a property.

00:05:59.710 --> 00:06:02.390
You just call -- and it's
suitably named AudioConverter

00:06:02.390 --> 00:06:05.120
Sample Rate Converter Algorithm.

00:06:05.120 --> 00:06:07.720
In the header file,
you'll see an enumeration of the

00:06:07.770 --> 00:06:09.950
currently supported algorithm types.

00:06:10.250 --> 00:06:15.210
They include linear interpolation and
a few others in addition to polyphase

00:06:15.220 --> 00:06:20.500
that have different tradeoffs in
terms of performance for quality.

00:06:22.220 --> 00:06:25.400
Then, once you have created and
configured your converter,

00:06:25.400 --> 00:06:28.160
you're going to want to feed some
data into it and get some data

00:06:28.160 --> 00:06:32.040
out of it to do the transformation
that you're looking for.

00:06:32.060 --> 00:06:38.030
So, the AudioConverter actually provides
three separate mechanisms for doing this.

00:06:38.460 --> 00:06:42.720
And the reason why is that
not every mechanism is

00:06:42.720 --> 00:06:45.300
suited for your circumstance.

00:06:45.300 --> 00:06:48.590
So, you will want to pick and choose
the most appropriate mechanism

00:06:48.590 --> 00:06:50.220
for what you're trying to do.

00:06:51.160 --> 00:06:56.880
The first mechanism that we have is
for simple data formats that have

00:06:56.880 --> 00:06:59.130
very simple data flow requirements.

00:06:59.160 --> 00:07:03.200
This is only really available for
transformation where the ratio between

00:07:03.250 --> 00:07:08.160
the input buffer and the output buffer
is both constant and an integer.

00:07:08.250 --> 00:07:12.160
What this comes down to, meaning,
is that when you provide an input buffer,

00:07:12.160 --> 00:07:17.200
it has to be completely consumed to
produce exactly the output buffer.

00:07:17.220 --> 00:07:20.220
Any other combination,
you'll get an error from AudioConverter.

00:07:20.220 --> 00:07:23.150
from audio converter to convert buffer.

00:07:23.400 --> 00:07:27.390
Here we see a simple loop that kind
of illustrates the calling sequence.

00:07:27.400 --> 00:07:33.940
It's a very simplistic example,
but it kind of gets across the flow.

00:07:33.940 --> 00:07:36.980
So basically the flow is
while you have input data,

00:07:36.980 --> 00:07:40.640
you get a new output buffer,
and then you call AudioConvert or

00:07:40.640 --> 00:07:43.320
ConvertBuffer with those buffers.

00:07:43.320 --> 00:07:46.610
And previously,
by setting up the conversion,

00:07:46.620 --> 00:07:51.630
you've already specified everything
the converter needs to know to evaluate

00:07:51.630 --> 00:07:54.960
whether or not this call will succeed.

00:07:54.960 --> 00:07:58.310
You'll find these calls the most
useful in the cases of doing linear

00:07:58.320 --> 00:08:00.520
PCM to linear PCM translations.

00:08:00.620 --> 00:08:06.490
A good example would be going from, say,
16-bit integer to 32-bit float.

00:08:07.360 --> 00:08:11.780
So the next most common application
is when you have simple data,

00:08:11.780 --> 00:08:14.930
but you have a much more
complex data flow to deal with.

00:08:15.140 --> 00:08:18.510
In your application,
you might have a thread,

00:08:18.540 --> 00:08:22.830
a producer thread that is reading data
off a disk and putting it in a queue,

00:08:22.830 --> 00:08:25.420
or reading it from the
network or whatever,

00:08:25.420 --> 00:08:28.640
and it's not necessarily going
to be in a position where

00:08:28.640 --> 00:08:31.740
you're going to be able to say,
here's the input buffer,

00:08:31.740 --> 00:08:33.000
here's the output buffer.

00:08:33.080 --> 00:08:34.820
You're going to need more
logic involved with that.

00:08:34.840 --> 00:08:41.850
So the AudioConvertor fill
buffer routine allows for this by

00:08:41.850 --> 00:08:46.100
providing an input data callback.

00:08:46.200 --> 00:08:49.740
So that will get called
whenever the converter wants

00:08:49.740 --> 00:08:51.770
more input data to process.

00:08:52.110 --> 00:08:56.600
Here's an example of calling
AudioConvertor fill buffer.

00:08:56.600 --> 00:08:58.650
In this case,
the input data is completely

00:08:58.650 --> 00:09:01.100
encapsulated inside the input data proc.

00:09:01.380 --> 00:09:05.630
There's no actual buffer
handling about the input that

00:09:05.630 --> 00:09:07.300
goes into making this call.

00:09:07.320 --> 00:09:11.780
Once you define your input data proc,
you just pass it in and then

00:09:11.780 --> 00:09:15.870
it will be called when the
converter wants data from you.

00:09:16.150 --> 00:09:19.650
Then you'll give it to it and
the conversion will happen,

00:09:19.660 --> 00:09:24.050
up to so that it fills up the entire
buffer that you're asking for in

00:09:24.110 --> 00:09:26.460
your call to convert fill buffer.

00:09:27.330 --> 00:09:30.420
Then in Jaguar,
we have a new API that we've

00:09:30.420 --> 00:09:35.450
added to support more complex
data formats like MPEG-4 AAC,

00:09:35.770 --> 00:09:37.790
among others.

00:09:38.100 --> 00:09:45.000
This was the most general of
the calls in the AudioConverter.

00:09:45.000 --> 00:09:51.500
It's there for data formats
that are much more complex than,

00:09:51.500 --> 00:09:55.880
say, linear PCM,
or that require external packetization.

00:09:55.980 --> 00:10:01.040
This is also the API you need to use for
interleaving and deinterleaving data.

00:10:01.280 --> 00:10:06.090
The API call itself is called
AudioConverter Fill Complex Buffer.

00:10:06.830 --> 00:10:12.200
Here's a simple example
of how to call it.

00:10:12.200 --> 00:10:16.900
It looks very similar to the regular
AudioConverter fill buffer call.

00:10:17.000 --> 00:10:21.570
The key thing to notice about this is
that AudioConverter fill complex buffer

00:10:21.650 --> 00:10:23.590
operates in terms of packets of data.

00:10:23.600 --> 00:10:27.570
As you recall, I mentioned that encoded
formats are almost always

00:10:27.670 --> 00:10:29.700
dealt with in terms of packets.

00:10:29.700 --> 00:10:32.600
This is why the API is
structured like this.

00:10:32.600 --> 00:10:38.320
What this implies is that you
can only do I/O in terms of this

00:10:38.320 --> 00:10:41.970
API in increments of full packets.

00:10:42.390 --> 00:10:47.430
That's very important to consider,
especially for encoded VBR formats

00:10:47.430 --> 00:10:51.880
where that's really the only
way to do timing on the data.

00:10:53.720 --> 00:10:58.860
So we've also wrapped up
the AudioConverter into

00:10:58.930 --> 00:11:01.700
an AudioUnit in Jaguar.

00:11:01.700 --> 00:11:05.320
The AudioConverter,
the AUConverter AudioUnit is basically

00:11:05.320 --> 00:11:09.260
something you can stick in your graph
of AudioUnits and you can feed it

00:11:09.260 --> 00:11:13.300
data so that it can be converted
and doled out to the rest of the

00:11:13.320 --> 00:11:15.830
graph in kind of a very unified way.

00:11:15.830 --> 00:11:21.000
It makes integrating data into the
graph and getting it back out very easy.

00:11:23.020 --> 00:11:26.370
I would be surprised if there aren't
most of your graphs that you end

00:11:26.410 --> 00:11:29.980
up constructing include a converter
unit of one kind or another.

00:11:29.980 --> 00:11:35.900
The most common example of a converter
unit is actually the output AudioUnits,

00:11:35.900 --> 00:11:40.440
which are actually a subclass of
AUConverter AudioUnit and provide

00:11:40.630 --> 00:11:44.510
that mechanism right at the
output stage for the easiest way

00:11:44.510 --> 00:11:46.850
to get audio out of the system.

00:11:48.570 --> 00:11:52.760
Also new in Jaguar,
the AudioConvertor now will

00:11:52.760 --> 00:11:55.690
support a plug-in mechanism.

00:11:55.750 --> 00:12:00.880
I will be speaking more tomorrow
in session 508 about AudioCodec

00:12:00.880 --> 00:12:07.740
components and how they're written,
how they're used, and what they do.

00:12:07.740 --> 00:12:11.200
But in the meantime,
the SDK will be available.

00:12:11.200 --> 00:12:16.400
It's available now where now
means sometime real soon.

00:12:16.880 --> 00:12:23.330
and it will include the Codec
APIs and a set of C++ base classes for

00:12:23.870 --> 00:12:27.960
implementing audio codec components.

00:12:28.040 --> 00:12:30.990
In the seed of Jaguar
that you have right now,

00:12:31.110 --> 00:12:37.290
this stuff is not currently supported,
so you won't find it on the seed

00:12:37.320 --> 00:12:39.580
of Jaguar that you have now.

00:12:41.530 --> 00:12:44.240
So that's pretty much it
for the AudioConvertor.

00:12:44.240 --> 00:12:46.890
It's kind of an introductory
to how it works.

00:12:46.950 --> 00:12:53.170
You'll encounter it mostly in terms
of the AudioUnit Converter AudioUnit,

00:12:53.420 --> 00:12:56.380
but it's there for separate use as well.

00:12:56.400 --> 00:12:59.130
So next up,
I'd like to bring up Chris Rogers,

00:12:59.130 --> 00:13:02.900
and he's going to talk a little
bit about audio spatialization.

00:13:09.290 --> 00:13:10.200
Hi.

00:13:10.200 --> 00:13:12.270
As Jeff Moore said,
my name is Chris Rogers,

00:13:12.270 --> 00:13:15.660
and I'm a member of the
Core Audio team also.

00:13:21.400 --> 00:13:24.940
I'm going to talk a bit
about 3D audio technology,

00:13:24.940 --> 00:13:27.520
some of the stuff we've been
working on at Apple here.

00:13:27.520 --> 00:13:31.650
I'm going to start off giving
kind of a conceptual overview

00:13:31.650 --> 00:13:36.320
of 3D principles in general,
psychoacoustic principles

00:13:36.320 --> 00:13:38.120
and that kind of stuff.

00:13:38.120 --> 00:13:41.680
And then in the second part,
I'm going to go over our 3D

00:13:41.680 --> 00:13:46.790
mixer and our reverb audio unit,
some more practical applications,

00:13:46.860 --> 00:13:49.040
and show some demonstrations of that.

00:13:50.790 --> 00:13:56.910
So here I just wanted to show kind
of where this top fits into the

00:13:57.190 --> 00:13:59.100
larger scheme of Core Audio here.

00:13:59.100 --> 00:14:04.540
And you can see in the AudioUnits section
there the 3D mixer and the reverb.

00:14:04.540 --> 00:14:08.540
And we'll also be seeing a little
bit of the AudioUnit views that

00:14:08.540 --> 00:14:10.920
are off to the right there.

00:14:14.330 --> 00:14:20.560
So this diagram is... I kind of want to
make an anchor point for the whole talk,

00:14:20.560 --> 00:14:22.610
so I'm going to spend a
little bit of time trying to

00:14:22.610 --> 00:14:23.980
go over what I mean by this.

00:14:24.050 --> 00:14:28.020
This isn't the only way to break down...

00:14:29.000 --> 00:14:49.100
[Transcript missing]

00:14:49.360 --> 00:14:53.540
So we have two types of sources.

00:14:53.610 --> 00:14:57.060
I'm making a distinction between
more environmental type of

00:14:57.060 --> 00:15:00.880
sounds where you want to actually
capture a spatial quality through

00:15:01.020 --> 00:15:05.530
a microphone recording technique,
and then the guitar down there.

00:15:05.620 --> 00:15:08.570
It represents a type of a sound
source which doesn't have any spatial

00:15:08.590 --> 00:15:13.390
information inherent in the sound signal,
and where maybe further processing is

00:15:13.390 --> 00:15:15.420
applied to create a spatial aspect.

00:15:15.460 --> 00:15:20.080
I think the reproduction...

00:15:20.800 --> 00:15:38.300
[Transcript missing]

00:15:41.210 --> 00:15:45.090
I'm going to go over a
couple of principles,

00:15:45.160 --> 00:15:47.570
really basic principles
of psychoacoustics,

00:15:47.620 --> 00:15:53.600
how human beings interpret
spatial position of sounds.

00:15:53.600 --> 00:15:56.970
I don't have time to really
go into enough detail to

00:15:56.970 --> 00:16:02.600
really cover this thoroughly,
but I'm going to just

00:16:02.600 --> 00:16:02.600
cover some basic stuff.

00:16:02.870 --> 00:16:07.750
Two of the really fundamental ways
that human beings can tell the

00:16:08.290 --> 00:16:12.860
direction of a sound source is
through interaural time difference

00:16:12.940 --> 00:16:16.130
and interaural intensity difference.

00:16:16.130 --> 00:16:21.280
A very simple spatialization model
can be created called spherical

00:16:21.640 --> 00:16:23.430
head model based off that.

00:16:23.720 --> 00:16:28.060
More advanced models can be
created that are called HRTF,

00:16:28.070 --> 00:16:31.280
head-related transfer function.

00:16:32.800 --> 00:16:34.030
Thank you.

00:16:34.200 --> 00:16:39.980
After that, I'm going to talk about how
human beings interpret distance.

00:16:40.260 --> 00:16:41.640
So, interaural time difference.

00:16:41.640 --> 00:16:43.560
What is that?

00:16:43.650 --> 00:16:46.660
Imagine a sound that's off to your right.

00:16:46.660 --> 00:16:49.640
Your right ear is going to hear the
sound before your left ear does,

00:16:49.640 --> 00:16:53.940
simply because the sound wave
arrives at the right ear sooner.

00:16:53.940 --> 00:17:00.650
The brain is able to use that slight
delay to make a judgment on what

00:17:01.410 --> 00:17:03.310
direction the sound is coming from.

00:17:03.390 --> 00:17:06.600
This works only for sounds
that are below about 1,000 Hz.

00:17:06.710 --> 00:17:10.200
So sounds coming straight ahead
hit both ears at the same time.

00:17:10.210 --> 00:17:15.700
Also, sounds coming from straight above
do and from straight behind.

00:17:15.700 --> 00:17:20.120
So the brain knows that the
sound is in the median plane.

00:17:20.120 --> 00:17:23.330
But for sounds that are extremely
on one side or the other,

00:17:23.450 --> 00:17:26.820
the delay is quite pronounced,
and we can tell the direction.

00:17:28.360 --> 00:17:33.060
Another method that we use is called
interaural intensity difference,

00:17:33.300 --> 00:17:37.270
and that's a frequency-dependent
intensity difference

00:17:37.270 --> 00:17:38.620
between the two ears.

00:17:38.620 --> 00:17:40.640
So, for example,
if once again we have a sound

00:17:40.840 --> 00:17:44.440
coming from the right-hand side,
it will appear louder to the right

00:17:44.650 --> 00:17:46.450
ear than it does to the left ear.

00:17:46.720 --> 00:17:50.800
And not only is it louder,
but the left ear might hear somewhat of

00:17:50.880 --> 00:17:55.080
a more muffled sound because the head
is blocking the high frequencies more

00:17:55.250 --> 00:17:57.290
than it blocks the low frequencies.

00:17:57.950 --> 00:18:01.130
and for sounds that are
above around 1,000 Hz,

00:18:01.330 --> 00:18:06.140
the brain is able to use that information
to determine the direction of a sound.

00:18:09.940 --> 00:18:13.740
Don't worry about the details of this,
but this is just showing that

00:18:13.740 --> 00:18:19.680
through simple rules of geometry,
it's possible to construct a basic model

00:18:19.850 --> 00:18:23.310
using a sphere as a model for the head.

00:18:23.760 --> 00:18:30.760
The delay time between the two ears,
depending on the direction

00:18:30.880 --> 00:18:33.830
that the sound hits the ears,
and also using some basic

00:18:33.900 --> 00:18:49.500
[Transcript missing]

00:18:50.610 --> 00:18:54.360
HRTF, or Head-Related Transfer Functions,
it's kind of related to what

00:18:54.360 --> 00:18:57.160
I was talking about with
interaural intensity difference.

00:18:57.160 --> 00:19:01.870
You remember when I was talking about
having one sound off to the right,

00:19:01.960 --> 00:19:04.990
how the close ear is going
to hear a louder sound,

00:19:05.190 --> 00:19:07.800
the left ear is going to
hear maybe a more muffled,

00:19:07.810 --> 00:19:08.900
quieter sound.

00:19:08.900 --> 00:19:11.050
It's quite a bit more
complicated than that,

00:19:11.050 --> 00:19:11.740
actually.

00:19:11.740 --> 00:19:15.510
And the outer ear,
the fine structure of the outer ear,

00:19:15.510 --> 00:19:20.400
the complex folds of skin and so on,
act as a highly directional filter.

00:19:20.500 --> 00:19:23.470
So depending on the direction
that the sound is coming from,

00:19:23.580 --> 00:19:26.920
how it hits the ear,
the ear will filter that sound in a

00:19:26.920 --> 00:19:32.120
different way through various reflections
in the skin and other things that

00:19:32.120 --> 00:19:38.620
are going on there to create a very
unique signature of resonances and

00:19:38.620 --> 00:19:42.680
notches in the frequency response.

00:19:42.700 --> 00:19:47.160
And here in this diagram,
this is just an example.

00:19:47.160 --> 00:19:49.300
I don't know what direction
the sound was coming from,

00:19:49.380 --> 00:19:52.960
but you see a very deep notch there.

00:19:53.300 --> 00:19:56.280
And it's possible under very
highly controlled laboratory

00:19:56.280 --> 00:20:00.660
conditions to take probe microphones,
place them in the ear canals,

00:20:00.730 --> 00:20:04.610
and make measurements playing
sound through a speaker at

00:20:04.610 --> 00:20:09.420
various positions in the room,
various angles relative to the ears.

00:20:09.420 --> 00:20:13.830
The microphones pick up the sound
and the frequency response can be

00:20:13.830 --> 00:20:19.500
measured at these various positions,
stored in a computer and a database.

00:20:19.500 --> 00:20:23.950
Later,
these filters can be used to filter

00:20:24.130 --> 00:20:28.480
synthetic sound sources to create
the illusion that sounds are coming

00:20:28.480 --> 00:20:32.040
from a particular position in space.

00:20:32.040 --> 00:20:35.610
In fact,
it's a combination of a filter for

00:20:35.610 --> 00:20:38.130
the right ear and the left ear.

00:20:38.160 --> 00:20:42.140
It's the combination of that
that creates the effect.

00:20:42.140 --> 00:20:45.690
The playback generally has to be done
on headphones for the best effect,

00:20:45.690 --> 00:20:47.640
but it can also be done on stereo.

00:20:47.640 --> 00:20:51.180
area speakers with certain compensation.

00:20:52.840 --> 00:20:56.950
So how do human beings judge
how far away a sound is?

00:20:57.200 --> 00:20:59.700
There are a couple different ways.

00:20:59.700 --> 00:21:03.680
If you're inside,
maybe inside a large church or a hall,

00:21:04.090 --> 00:21:07.520
one of those methods is the ratio
of direct to reverberant sound.

00:21:07.520 --> 00:21:09.320
So if a dog is in the
very back of the hall,

00:21:09.320 --> 00:21:12.270
very far away from you and is barking,
you're going to hear this really

00:21:12.380 --> 00:21:13.970
kind of reverberant barking sound.

00:21:14.040 --> 00:21:16.700
If the dog is in the same
hall but very close to you,

00:21:16.700 --> 00:21:19.020
you're going to hear more
of a direct barking sound.

00:21:19.020 --> 00:21:24.000
You'll still hear the reverb,
but that'll be less in the mix.

00:21:24.000 --> 00:21:29.160
Doppler shift is in effect when
we're hearing moving sources.

00:21:29.160 --> 00:21:33.430
So if you've ever been walking
down a sidewalk noticing the

00:21:33.460 --> 00:21:37.000
sound of cars rushing by you,
as the car is approaching,

00:21:37.000 --> 00:21:40.440
the pitch of the whooshing
sound of the car increases.

00:21:40.440 --> 00:21:44.120
And then as it passes by
you and goes away from you,

00:21:44.270 --> 00:21:47.780
the pitch goes down, it drops.

00:21:47.780 --> 00:21:54.000
And so that's a very important
cue for judging moving sources.

00:21:54.000 --> 00:21:55.290
And in our demonstration,

00:21:55.370 --> 00:21:59.830
A little bit later we'll
hopefully hear that effect.

00:21:59.940 --> 00:22:04.140
Another way that we use to
judge distance is high-frequency

00:22:04.260 --> 00:22:06.830
drop-off due to air absorption.

00:22:06.960 --> 00:22:08.290
It's also just intensity drop-off.

00:22:08.360 --> 00:22:13.920
Generally,
the farther away a sound source is,

00:22:13.920 --> 00:22:13.920
the quieter it's going to sound.

00:22:14.150 --> 00:22:18.820
and David Koehn.

00:22:18.820 --> 00:22:33.900
I'm going to talk about the
different types of audio services

00:22:33.900 --> 00:22:35.150
that are available in Mac OS X.

00:22:35.150 --> 00:22:35.150
I'm going to talk about the
different types of audio services

00:22:35.150 --> 00:22:35.150
that are available in Mac OS X.

00:22:35.150 --> 00:22:35.150
I'm going to talk about the
different types of audio services

00:22:35.150 --> 00:22:35.150
that are available in Mac OS X.

00:22:35.150 --> 00:22:35.150
I'm going to talk about the
different types of audio services

00:22:35.150 --> 00:22:35.150
that are available in Mac OS X.

00:22:35.710 --> 00:22:43.120
The source will span a large
angle as it's moving and

00:22:43.200 --> 00:23:14.100
[Transcript missing]

00:23:14.550 --> 00:23:18.250
and Diffusion is another effect, very,
very, very far away sounds,

00:23:18.250 --> 00:23:22.170
maybe kind of diffused,
kind of reverberant in a way.

00:23:22.840 --> 00:23:25.850
So if we think back to that
original diagram that I had up

00:23:26.010 --> 00:23:30.510
where we had the sound sources,
the encoding, the reproduction,

00:23:30.650 --> 00:23:34.530
and the perception,
on the left we had the sources.

00:23:34.530 --> 00:23:35.700
And there were two types of sources.

00:23:35.700 --> 00:23:38.370
I talked about maybe ambient
environmental sources,

00:23:38.370 --> 00:23:39.940
the waterfall.

00:23:39.940 --> 00:23:45.020
And I'm going to call these intrinsic
sources and use microphones to

00:23:45.080 --> 00:23:47.010
record these types of sounds.

00:23:47.010 --> 00:23:49.960
And there are various
microphone recording techniques.

00:23:49.960 --> 00:23:53.010
And in order to hear the spatial effect,
there's little or no further

00:23:53.010 --> 00:23:54.270
processing necessary.

00:23:54.270 --> 00:23:56.400
You can just play it
back on the speakers,

00:23:56.400 --> 00:23:57.340
essentially.

00:23:57.340 --> 00:24:01.510
And then the second type of source I'm
going to talk about is I'm going to call

00:24:01.510 --> 00:24:06.340
synthetic or mono or dry recorded source.

00:24:06.620 --> 00:24:10.590
In order to create a spatial quality,
we need to do some kind

00:24:10.830 --> 00:24:16.050
of synthetic processing,
some kind of digital signal processing,

00:24:16.090 --> 00:24:21.730
in order to locate it in space
or make it sound like it's in

00:24:21.730 --> 00:24:21.730
some kind of acoustic space.

00:24:25.850 --> 00:24:30.040
So for these intrinsic sources,
these environmental ambient sources,

00:24:30.040 --> 00:24:32.040
there are a couple different
recording techniques.

00:24:32.120 --> 00:24:35.360
One of these is called
binaural recording.

00:24:35.360 --> 00:24:38.460
Another is using a
sound field microphone.

00:24:38.460 --> 00:24:43.750
And then there are many other ways
that are commonly used with distantly

00:24:43.830 --> 00:24:49.260
spaced omnidirectional microphones
or crossed directional microphones.

00:24:49.980 --> 00:24:53.460
I'm not going to talk much
about those common methods,

00:24:53.620 --> 00:24:56.210
but I'm going to talk a little
bit now about the first two.

00:24:56.220 --> 00:24:57.470
Thank you.

00:24:58.430 --> 00:25:05.640
In binaural recording,
we make use of the principle of the HRTF,

00:25:05.640 --> 00:25:08.820
the head-related transfer function
that we talked about before.

00:25:08.820 --> 00:25:14.210
The way that this recording
works is take a dummy head,

00:25:14.320 --> 00:25:20.010
a mannequin's head,
with actual ears that are

00:25:20.010 --> 00:25:22.980
molded after human ears,
and tiny little microphones

00:25:22.980 --> 00:25:23.970
inside of the ears.

00:25:24.430 --> 00:25:27.160
Somebody takes this head,
believe it or not,

00:25:27.160 --> 00:25:30.430
into an environment where
there are some interesting

00:25:30.510 --> 00:25:34.550
sounds with spatial qualities,
and makes a recording,

00:25:34.550 --> 00:25:38.760
just a stereo recording,
and then plays back this

00:25:38.760 --> 00:25:44.750
recording on headphones in order
to recreate the original space,

00:25:44.750 --> 00:25:46.460
spatial effect.

00:25:48.680 --> 00:25:55.850
Some people go so far as to have
molds taken of their own ears,

00:25:56.230 --> 00:26:00.800
because the effect is actually better
if you make a recording with your own

00:26:00.800 --> 00:26:04.360
ears compared with somebody else's,
because there are actually

00:26:04.480 --> 00:26:08.260
large differences in the size
and the shape of people's ears.

00:26:08.260 --> 00:26:10.990
This effect is highly sensitive on that.

00:26:12.920 --> 00:26:16.400
Another interesting way of recording
spatial information is using what's

00:26:16.400 --> 00:26:19.220
known as a sound field microphone.

00:26:19.330 --> 00:26:28.030
That records into Ambisonics B format.

00:26:28.030 --> 00:26:28.030
I'll talk a bit more about
Ambisonics in the next slide.

00:26:28.450 --> 00:26:32.870
It attempts to capture the
complete 3D pressure sound field,

00:26:32.910 --> 00:26:40.210
and it uses four microphone elements that
are very tightly spaced into one capsule.

00:26:40.420 --> 00:26:42.420
It's a very finely jeweled
piece of equipment.

00:26:42.420 --> 00:26:45.200
It's quite expensive,
but it can make some

00:26:45.590 --> 00:26:47.080
really nice recordings.

00:26:50.420 --> 00:26:53.600
So the Sound Field Microphone
records into ambisonics format.

00:26:53.600 --> 00:26:59.240
Ambisonics is both a recording technique
using the Sound Field Microphone and

00:26:59.240 --> 00:27:04.430
other techniques and a panning technique
to synthetically spatialize sounds.

00:27:04.430 --> 00:27:08.310
And it aims to recreate the
original sound field using all of

00:27:08.320 --> 00:27:12.700
the available playback speakers,
whether that be two speakers, four,

00:27:12.700 --> 00:27:16.100
five in a surround system,
or more speakers.

00:27:16.100 --> 00:27:19.980
It has a nice quality in that it
doesn't really require a sweet

00:27:20.160 --> 00:27:26.480
spot for listening as other
spatialization reproduction systems do.

00:27:26.480 --> 00:27:29.560
And as sound is being panned around,
the sound does not collapse

00:27:29.600 --> 00:27:31.330
into a single speaker.

00:27:31.330 --> 00:27:33.900
That's another nice principle.

00:27:33.900 --> 00:27:37.610
Other techniques like vector-based
panning suffer from this problem,

00:27:37.820 --> 00:27:39.890
from moving sources.

00:27:41.700 --> 00:27:44.840
The ambisonics encoding is
a four-channel encoding,

00:27:44.840 --> 00:27:46.750
four mono-channels.

00:27:47.380 --> 00:27:51.540
Those channels are W,
which is an omnidirectional component.

00:27:51.540 --> 00:27:55.080
It comes from the
omnidirectional microphone.

00:27:55.080 --> 00:27:57.420
X is a front-back component.

00:27:57.470 --> 00:27:58.810
Y is left-right.

00:27:58.950 --> 00:28:00.340
Z is an up-down.

00:28:00.340 --> 00:28:04.260
So you capture the three
dimensions plus an omni-component.

00:28:04.260 --> 00:28:15.440
It's possible to generate these W, X, Y,
Z coordinates given a coordinate in

00:28:15.750 --> 00:28:20.290
spherical coordinates to synthesize
the location of a synthetic source.

00:28:20.290 --> 00:28:24.220
We're going to show that in
a demo in a little while.

00:28:26.990 --> 00:28:30.110
Now we get on to the practical matters.

00:28:30.180 --> 00:28:33.780
I'm going to talk about
the 3D Mixer Audio Unit.

00:28:33.840 --> 00:28:36.750
It's an audio unit that
will ship in Jaguar.

00:28:36.820 --> 00:28:41.310
You should come to this session
tomorrow where we talk in quite

00:28:41.430 --> 00:28:44.620
a lot of detail about AudioUnits.

00:28:44.620 --> 00:28:47.160
The Mixer is just another
AudioUnit and it adheres to

00:28:47.160 --> 00:28:48.900
the same APIs and everything.

00:28:48.900 --> 00:28:52.060
I'm not going to really
talk about the APIs today.

00:28:52.060 --> 00:28:54.700
The Mixer takes in mono inputs.

00:28:54.700 --> 00:29:00.340
It can take stereo inputs,
but in my demo these are all mono inputs.

00:29:00.420 --> 00:29:05.860
It localizes each of these inputs
in three-dimensional space.

00:29:06.220 --> 00:29:07.570
There's one output bus.

00:29:07.580 --> 00:29:13.800
The stream format of the output
bus depends on what type of speaker

00:29:13.800 --> 00:29:20.810
configuration you're applying to,
whether that's stereo, headphones, 5.1,

00:29:20.810 --> 00:29:21.980
or whatever.

00:29:22.140 --> 00:29:28.640
There's an optional reverb that's in the
3D Mixer so that we can actually have

00:29:28.980 --> 00:29:34.990
more of a wet mix for distant sources,
as we talked about in distance effects.

00:29:35.980 --> 00:29:40.650
Each of the inputs can be
parametrically controlled.

00:29:40.680 --> 00:29:43.280
The first three parameters
are azimuth angle,

00:29:43.350 --> 00:29:47.900
elevation, and distance are part of a
spherical coordinate system.

00:29:47.900 --> 00:29:51.210
Azimuth is, say,
if I'm pointing straight ahead,

00:29:51.210 --> 00:29:56.730
as I trace my arm out to the right,
I'm spanning positive angle

00:29:56.730 --> 00:30:01.910
in azimuth from 0 degrees all
the way to minus 180 degrees.

00:30:02.240 --> 00:30:04.480
And as I go to the left, it's negative.

00:30:04.480 --> 00:30:08.770
So, say, negative 10 degrees,
negative 90, minus 180.

00:30:10.500 --> 00:30:12.800
Elevation is just going up and down.

00:30:12.800 --> 00:30:17.400
So straight ahead is zero degrees,
all the way down to minus 90 degrees.

00:30:17.490 --> 00:30:21.400
And straight up is 90 degrees.

00:30:22.350 --> 00:30:23.540
Distance is measured in meters.

00:30:23.540 --> 00:30:24.370
It's pretty simple.

00:30:24.380 --> 00:30:27.000
There's also an additional
parameter that's gain.

00:30:27.000 --> 00:30:30.610
It's measured in decibels.

00:30:33.200 --> 00:30:37.220
As most AudioUnits,
the 3D Mixer AudioUnit is

00:30:37.220 --> 00:30:38.740
configurable through properties.

00:30:38.800 --> 00:30:41.800
I talked about the parameters.

00:30:42.330 --> 00:30:46.200
The playback configuration needs to
be configured through a property.

00:30:46.460 --> 00:30:49.480
The default is stereo,
so if you know you're

00:30:49.590 --> 00:30:52.090
playing back through stereo,
you don't have to worry about it.

00:30:52.480 --> 00:30:56.200
There's also a setting for
the rendering algorithm.

00:30:56.200 --> 00:30:59.960
I talked about a couple of those,
but in the next slide or two

00:30:59.960 --> 00:31:02.120
we'll see the list of them.

00:31:02.410 --> 00:31:06.770
Optionally, you can configure the source
to have a Doppler shift.

00:31:08.750 --> 00:31:10.700
The playback configurations
are listed here,

00:31:10.700 --> 00:31:17.040
and I think I've talked
about them before.

00:31:21.760 --> 00:31:24.000
and the rendering
algorithms are listed here.

00:31:24.240 --> 00:31:28.700
For stereo playback,
there are three different methods.

00:31:28.780 --> 00:31:30.010
The first is equal power panning.

00:31:30.130 --> 00:31:33.330
And this is the traditional
type of panning that you see

00:31:33.380 --> 00:31:37.070
on a lot of mixing boards,
where you just have a control

00:31:37.170 --> 00:31:40.040
that moves from left to right and
it moves the sound from the left

00:31:40.040 --> 00:31:41.220
speaker to the right speaker.

00:31:41.220 --> 00:31:45.240
When it's in the middle,
the sound comes from the center,

00:31:45.240 --> 00:31:47.290
between the two speakers.

00:31:47.820 --> 00:31:51.390
The algorithm using the spherical
head model is a little bit

00:31:51.390 --> 00:31:53.520
more sophisticated than that.

00:31:53.520 --> 00:31:58.710
It incorporates the interaural
time difference and the interaural

00:31:58.710 --> 00:32:03.050
intensity difference using a
very simple model of the sphere,

00:32:03.050 --> 00:32:07.640
as we saw in the picture,
to attempt to position sound.

00:32:07.640 --> 00:32:09.940
It can work pretty well for
certain types of sources.

00:32:09.940 --> 00:32:15.080
The third way is the most
expensive in terms of the CPU.

00:32:15.080 --> 00:32:19.450
That's HRTF,
to actually apply these measured filters,

00:32:19.560 --> 00:32:24.170
measured in the laboratory,
to sources to position sound over

00:32:24.170 --> 00:32:29.320
stereo speakers or headphones,
where it works the best, actually.

00:32:29.320 --> 00:32:33.520
For more complicated speaker
configurations like 5.1 Surround,

00:32:33.520 --> 00:32:37.630
we have two different methods:
ambisonics method, which is what we're

00:32:37.640 --> 00:32:41.100
going to see in the demo,
and also vector-based panning,

00:32:41.100 --> 00:32:44.140
which is an alternate method.

00:32:45.690 --> 00:32:48.570
So I'm going to move over to
the other machine now and see

00:32:48.570 --> 00:32:52.500
if we can get some sounds going.

00:33:00.020 --> 00:33:02.560
The interface is very, very simple here.

00:33:02.560 --> 00:33:09.560
You developers can imagine your own
much more interesting looking interface,

00:33:09.560 --> 00:33:13.140
but I think it's fine for
illustrating the principle.

00:33:13.140 --> 00:33:15.800
All of these sources, once again,
are mono sources,

00:33:15.900 --> 00:33:20.200
so any kind of spatial quality
is created by the mixer itself.

00:33:20.280 --> 00:33:21.900
You should be able to hear,
in certain cases,

00:33:21.900 --> 00:33:26.900
the Doppler shift and also distant
sources getting more muffled.

00:33:27.060 --> 00:33:32.180
Just general spatialization type
of stuff I've been talking about.

00:35:28.000 --> 00:35:41.800
[Transcript missing]

00:35:42.190 --> 00:35:44.760
It can be configured as just
an ordinary stereo reverb.

00:35:44.760 --> 00:35:46.700
That's the default configuration,
in fact.

00:35:46.760 --> 00:35:48.800
Or a 5.1 surround reverb.

00:35:48.950 --> 00:35:52.800
And it has various parameters: room size,
density, brightness, etc.

00:35:52.800 --> 00:35:57.580
We'll see in a minute
some of those parameters.

00:36:00.230 --> 00:36:04.890
So there are a couple different ways
you can use this in addition to just

00:36:05.080 --> 00:36:11.590
an ordinary stereo reverb as people
have used in the studio forever.

00:36:12.060 --> 00:36:15.960
There are a couple of different ways
that producers today use surround reverb.

00:36:15.960 --> 00:36:19.150
One way is to actually have the
reverb in all of the speakers

00:36:19.150 --> 00:36:22.620
with a separate reverberant stream
in each of the five speakers.

00:36:22.730 --> 00:36:24.120
That's what we'll be seeing.

00:36:24.120 --> 00:36:28.210
Another way that producers sometimes
like to use surround is to take an

00:36:28.210 --> 00:36:32.380
ordinary stereo reverb and put the
stereo in the rear two speakers.

00:36:32.520 --> 00:36:35.810
This could be used for this application,
too, but we're not going

00:36:36.030 --> 00:36:39.080
to see that in a demo.

00:36:39.080 --> 00:36:45.210
Another thing that you can do is
take a mono put it through a reverb,

00:36:45.500 --> 00:36:50.900
[Transcript missing]

00:36:51.500 --> 00:37:01.800
[Transcript missing]

00:37:03.480 --> 00:37:11.720
The surround reverb is an AudioUnit,
and its output format can be configured

00:37:11.930 --> 00:37:15.810
either to five channels or two channels.

00:37:16.470 --> 00:37:20.080
and, as I said, if it's in surround mode,
each of the five speakers will

00:37:20.080 --> 00:37:23.140
have a separate reverberant stream.

00:37:23.160 --> 00:37:27.840
So five independent reverb streams.

00:37:27.940 --> 00:37:30.540
So let's have a listen.

00:37:53.220 --> 00:37:56.400
So this is the dry sound right here.

00:37:56.400 --> 00:37:58.890
This is the wet sound.

00:38:33.000 --> 00:38:35.440
and David I'm going to go over
that too much because I think

00:38:35.440 --> 00:38:39.390
that Bill's going to have some
fun with that a little bit later.

00:38:40.000 --> 00:38:40.940
I think that might be it.

00:38:40.990 --> 00:38:44.250
I think that's the end of my talk.

00:38:46.160 --> 00:38:49.990
So I'd like to introduce Bill Stewart.

00:38:50.000 --> 00:38:52.780
He'll play around a little bit
more with this 3D stuff and also

00:38:52.780 --> 00:38:56.810
talk about some other interesting
stuff that we've got going.

00:38:57.500 --> 00:41:31.100
[Transcript missing]

00:41:32.550 --> 00:41:32.810
Excuse me.

00:41:32.810 --> 00:41:39.400
I'm going to basically do a couple
more slides then we'll get to a demo.

00:41:39.400 --> 00:41:43.590
What we decided to do with this
demo is to show you how to assemble

00:41:43.590 --> 00:41:45.890
a processing graph of audio units.

00:41:45.960 --> 00:41:49.370
The source of the audio that
you're going to be hearing

00:41:49.390 --> 00:41:54.200
is coming from the DLS synth,
which is an audio unit in itself.

00:41:54.200 --> 00:41:58.910
It's a special type of audio
unit called a music device.

00:41:58.910 --> 00:42:02.800
It has some additional
component selectors to get a

00:42:02.860 --> 00:42:05.130
MIDI message or MIDI raw data.

00:42:05.200 --> 00:42:08.370
We also have an extended
protocol for turning notes on

00:42:08.410 --> 00:42:10.350
and off and controlling them.

00:42:10.380 --> 00:42:14.630
It gives you a much greater
flexibility than MIDI does for

00:42:15.020 --> 00:42:18.160
scheduling note events in the synth.

00:42:18.160 --> 00:42:25.410
In this graph we're going to take the
reverb send from the DLS device rather

00:42:25.530 --> 00:42:27.420
than doing a reverb interpretation.

00:42:27.420 --> 00:42:27.420
Speaker 1: I'm going to show you a
little bit of the DLS synth.

00:42:27.420 --> 00:42:27.420
I'm going to show you the DLS synth.

00:42:27.420 --> 00:42:27.420
I'm going to show you a
little bit of the DLS synth.

00:42:27.420 --> 00:42:27.420
I'm going to show you a
little bit of the DLS synth.

00:42:27.420 --> 00:42:27.420
I'm going to show you a
little bit of the DLS synth.

00:42:27.420 --> 00:42:27.420
I'm going to show you a
little bit of the DLS synth.

00:42:27.420 --> 00:42:27.420
I'm going to show you a
little bit of the DLS synth.

00:42:27.420 --> 00:42:28.680
I'm internal to the DLS.

00:42:28.740 --> 00:42:31.950
We're going to actually put it
through a multi-channel reverb as

00:42:31.950 --> 00:42:35.230
Chris showed in his previous demo.

00:42:35.350 --> 00:42:38.390
We'll take a dry,
then we'll mix those two,

00:42:38.390 --> 00:42:41.610
and then we'll pass it
through an output unit.

00:42:41.610 --> 00:42:44.690
We'll have five channels of out.

00:42:44.690 --> 00:42:48.950
Typically with a 5.1 system,
as many of you know,

00:42:48.950 --> 00:42:54.080
a lot of 5.1 content won't have much
content at all in the .1 channel.

00:42:54.080 --> 00:42:59.740
A lot of systems will have what they call
a base management that will do a filter

00:43:00.050 --> 00:43:06.120
of the signal and push frequencies,
typically at 80 Hz or below to the sub,

00:43:06.500 --> 00:43:10.260
and then take the rest of
the feeds to the speakers.

00:43:10.260 --> 00:43:13.280
You could also have a low
frequency effect in the system.

00:43:13.280 --> 00:43:14.350
That's a choice you make.

00:43:16.400 --> 00:45:25.700
[Transcript missing]

00:45:26.340 --> 00:45:30.620
This is the flow of
the graph that we have,

00:45:30.620 --> 00:45:32.180
just to give you some
idea of what we're doing.

00:45:32.180 --> 00:45:36.890
The DLS synth is outputting two buses,
and on each bus it's a stereo bus.

00:45:36.980 --> 00:45:39.620
It's the bus that you would
normally take through the reverb

00:45:39.620 --> 00:45:41.110
and the bus that you wouldn't.

00:45:41.140 --> 00:45:44.890
So we put that through the reverb,
and we've configured the reverb to

00:45:44.890 --> 00:45:46.680
give us five channels of output.

00:45:46.680 --> 00:45:50.830
And then what we want to do is to
create a mix between the dry send of

00:45:50.830 --> 00:45:54.580
the reverb and the wet send that's
been passed through the reverb.

00:45:54.580 --> 00:45:59.100
So we de-interleave the reverb stream.

00:45:59.100 --> 00:46:05.640
And the three channels on that side,
my right, your left, I guess,

00:46:05.720 --> 00:46:09.420
they're actually three mono
channels in the demo we've got now.

00:46:09.420 --> 00:46:12.910
And then the one on this
side is a two-channel.

00:46:12.920 --> 00:46:15.890
So the de-interleave is actually
doing a stereo de-interleave

00:46:15.970 --> 00:46:17.640
and then three mono channels.

00:46:17.660 --> 00:46:20.920
Both of those go into the stereo mixer.

00:46:20.920 --> 00:46:24.060
We can adjust the volumes of those two.

00:46:24.500 --> 00:46:29.330
That creates a stereo mix stream,
which then gets interleaved back into the

00:46:29.330 --> 00:46:32.180
five channels that goes to the output.

00:46:32.180 --> 00:46:34.610
So with that, I'll get Doug to come up.

00:46:34.710 --> 00:46:38.390
And if we could have the demo
machine just on this one.

00:46:38.400 --> 00:46:39.570
Thank you.

00:46:44.520 --> 00:46:47.070
So this is Doug White,
in case you don't know.

00:46:47.070 --> 00:46:49.200
Round of applause for Doug.

00:46:49.200 --> 00:46:53.030
He's going to play some keyboard for us.

00:46:53.410 --> 00:46:55.900
So I'm going to probably make
some horrible sounds at the

00:46:55.900 --> 00:46:59.100
moment just to give you... Oh,
this is dry.

00:47:07.740 --> 00:47:12.080
Have you... It's very wet.

00:47:12.210 --> 00:47:15.630
So if I turn this to zero,
all you're hearing is a wet

00:47:15.720 --> 00:47:18.430
mix of the reverb itself.

00:47:18.670 --> 00:47:27.860
Can we just sort of jack up the
back two speakers in the center?

00:47:27.880 --> 00:47:32.140
Um...

00:47:39.710 --> 00:47:45.480
So I can change the mix of the
reverb to have both a mix between

00:47:45.480 --> 00:47:46.600
small and large room sizes.

00:47:46.600 --> 00:47:50.150
So if I look at the small mix first,
and I can change the mix

00:47:50.150 --> 00:47:50.150
between the small and large

00:47:52.110 --> 00:47:54.160
You'll get some tweaking on
some of these parameters.

00:47:54.160 --> 00:47:56.000
Some of them you shouldn't
change in real time,

00:47:56.000 --> 00:47:56.750
some you can.

00:47:56.750 --> 00:48:01.720
So if you hear any of those artifacts,
that's for... you're

00:48:01.720 --> 00:48:07.710
sort of expecting that.

00:48:10.300 --> 00:48:14.760
Now I'm going to a fairly wet mix here.

00:48:14.780 --> 00:48:19.320
This is the size of the wet room.

00:48:19.320 --> 00:48:19.320
Let's do that.

00:48:23.900 --> 00:48:26.800
You can hear that that's quite spacious,
and you've got basically the

00:48:26.950 --> 00:48:29.970
dry signal from the reverb is
only coming from the synth,

00:48:29.970 --> 00:48:33.260
it's coming from the left and right,
and the reverb is coming from all five.

00:48:36.900 --> 00:48:40.230
One of the things I can do here
is actually filter the sound

00:48:40.240 --> 00:48:46.490
that goes from the large mix,
so I can push that down,

00:48:46.490 --> 00:48:46.490
which will make it...

00:48:58.580 --> 00:49:01.360
The audio interface here,
that gets very muffled and it's

00:49:01.360 --> 00:49:04.140
like a room that's got a lot of
curtains in it against a room

00:49:04.140 --> 00:49:07.650
that would be a bit brighter.

00:49:13.130 --> 00:49:16.390
The two views that you're seeing here,
the one on the left,

00:49:16.490 --> 00:49:21.500
multi-channel reverb view,
this is made by a view component that is

00:49:21.570 --> 00:49:25.600
just what we call a generic Carbon view.

00:49:25.600 --> 00:49:28.880
What it does is it interrogates
the AudioUnit and it asks the

00:49:28.880 --> 00:49:32.740
AudioUnit what's your parameters,
what is their range,

00:49:32.740 --> 00:49:35.840
their default values, and so forth.

00:49:35.940 --> 00:49:39.730
It will just interrogate any AudioUnit
and put up this view for you.

00:49:42.000 --> 00:49:45.140
We'll be talking more about the
view stuff in tomorrow's session.

00:49:45.140 --> 00:49:49.700
The one on the right is actually a
custom view that I've made using the

00:49:49.700 --> 00:49:52.350
C++ base classes that we put in our SDK.

00:49:52.450 --> 00:49:57.260
I've just created a custom
view for my mixer because I've

00:49:57.260 --> 00:49:59.800
got two ins and one output.

00:49:59.800 --> 00:50:02.680
Let's have a bit of fun here.

00:50:02.680 --> 00:50:06.410
I'll stop yabbing and get
Doug to play some sounds.

00:50:30.400 --> 00:50:49.800
[Transcript missing]

00:50:53.200 --> 00:51:06.570
and Chris Rogers, the sound engineer,
will be joining us for a

00:51:06.570 --> 00:51:06.570
discussion on the new Mac OS X.

00:52:00.100 --> 00:52:07.260
So, every sound that you heard
was coming from the Mac.

00:52:07.350 --> 00:52:08.670
All that we have in the Mac.

00:52:08.760 --> 00:52:13.760
We decided this year,
last year we showed a device from eMagic.

00:52:13.850 --> 00:52:16.700
It was a little USB device.

00:52:16.700 --> 00:52:17.760
It's about this big.

00:52:17.880 --> 00:52:19.760
It has six channels of out, two in.

00:52:19.760 --> 00:52:24.480
This year we thought we'd not want to
show too much favouritism to any of

00:52:24.480 --> 00:52:26.620
our vendors because we love them all.

00:52:26.620 --> 00:52:30.830
We've got a Miniman 1010 card.

00:52:30.830 --> 00:52:35.150
There's drivers for this on Mac OS X,
Delta 1010.

00:52:35.160 --> 00:52:36.570
It's a PCI card.

00:52:36.570 --> 00:52:38.900
We're just running five
channels out of the box.

00:52:38.980 --> 00:52:40.990
That goes just straight
into the speakers.

00:52:40.990 --> 00:52:45.340
There really is no tricks.

00:52:45.400 --> 00:52:47.160
Nothing up my sleeve.

00:52:50.080 --> 00:52:54.630
This is just a list of some of
the other forums that we've got.

00:52:54.840 --> 00:52:59.330
The Feedback Forum on Friday is a
great place to come and yell at us

00:52:59.330 --> 00:53:02.440
for all the things we've done wrong
and all the things we've done right,

00:53:02.440 --> 00:53:02.900
hopefully.

00:53:02.900 --> 00:53:07.460
There's a couple of sessions tomorrow
using the basic levels of the system,

00:53:07.460 --> 00:53:10.880
both the audio and the MIDI stuff,
at 3.30.

00:53:10.880 --> 00:53:14.090
And then at 5,
there'll be some in-depth discussions

00:53:14.220 --> 00:53:18.250
about AudioUnits and the Codex,
how to use the AudioUnit views.

00:53:18.960 --> 00:53:22.870
And there's a short overview of AAC,
which is also in your Jaguar seeds,

00:53:22.880 --> 00:53:26.880
in the QuickTime session at the
same time as the Feedback Forum,

00:53:26.880 --> 00:53:28.310
which was good scheduling, I thought.

00:53:28.320 --> 00:53:31.300
So I think we do Q&A now.

00:53:31.340 --> 00:53:37.290
So Chris and Jeff and
Doug will come up and...

00:53:37.610 --> 00:53:38.940
Go to the mics.

00:53:38.940 --> 00:53:41.500
Here's some URLs.

00:53:41.500 --> 00:53:45.740
We have extensive developer
support in Core Audio.

00:53:45.740 --> 00:53:47.800
We do lots of things.

00:53:47.920 --> 00:53:52.370
We have a mailing list
that we're very active on,

00:53:52.370 --> 00:53:56.100
and you can get details
of that at list.apple.com.

00:53:56.240 --> 00:53:59.060
We also try to keep our
SDKs fairly up-to-date,

00:53:59.060 --> 00:54:03.920
and we have a website,
developer.apple.com/audio,

00:54:03.940 --> 00:54:06.860
and we're constantly tweaking that.

00:54:06.860 --> 00:54:08.600
There you go.