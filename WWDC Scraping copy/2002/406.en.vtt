WEBVTT

00:00:00.810 --> 00:00:01.510
Good afternoon.

00:00:01.760 --> 00:00:03.860
We're going to hear about
the Java Virtual Machine.

00:00:03.860 --> 00:00:12.800
I am trying to gear this talk a
little bit towards those who may

00:00:12.850 --> 00:00:17.280
never have seen Java on Apple and
those of you sort of in the middle

00:00:17.380 --> 00:00:20.320
who have seen a little bit of Java,
particularly on Apple.

00:00:20.320 --> 00:00:23.620
And then some of the talk should be
for those of you who have seen Java on

00:00:23.660 --> 00:00:27.180
Apple for quite a while now and just
want to know what's really new and

00:00:27.180 --> 00:00:29.940
what's interesting and what's different.

00:00:29.940 --> 00:00:32.420
So a little bit of a gauge,
I want to know whether anybody

00:00:32.600 --> 00:00:35.760
in this audience other than
the people who work for me was

00:00:35.820 --> 00:00:38.200
actually saw my talk last year.

00:00:39.270 --> 00:00:41.200
Okay, so, all right.

00:00:41.370 --> 00:00:42.540
A few.

00:00:42.540 --> 00:00:44.050
Well, you're not going to
hear last year's talk,

00:00:44.140 --> 00:00:46.190
although I did steal one slide from it.

00:00:46.510 --> 00:00:49.640
But today, this year,
what we're going to talk about is an

00:00:49.650 --> 00:00:54.200
overview of what the VM is all about,
what it does, what it does well.

00:00:54.620 --> 00:01:00.890
We're going to talk a little
bit about where we've gone

00:01:00.890 --> 00:01:00.890
from last year at this time.

00:01:01.100 --> 00:01:32.300
[Transcript missing]

00:01:36.020 --> 00:01:36.830
First assertion.

00:01:36.900 --> 00:01:40.780
So what does a Java VM do?

00:01:40.820 --> 00:01:44.920
A Java VM basically does four things.

00:01:44.920 --> 00:01:44.920
It

00:01:46.820 --> 00:01:51.660
First of all, the first thing is that
it executes bytecodes.

00:01:51.660 --> 00:01:55.510
You write your Java code once, right,
and it gets turned into .class files

00:01:55.510 --> 00:01:57.580
which get sucked into jar files.

00:01:57.620 --> 00:01:59.010
And in theory,
you're supposed to be able to

00:01:59.010 --> 00:02:01.980
ship those jar files anywhere
on the planet on any machine.

00:02:01.980 --> 00:02:04.360
And they're supposed to run and
they're supposed to do the same thing.

00:02:04.370 --> 00:02:08.480
So in order for those jar
files to do the same thing,

00:02:08.590 --> 00:02:14.630
you need an engine to actually interpret
those bytecodes and make it and do it

00:02:14.630 --> 00:02:16.660
in exactly the same way on that machine.

00:02:16.720 --> 00:02:22.150
So that's mostly what the virtual
machine is aimed at doing,

00:02:22.150 --> 00:02:25.570
is implementing those bytecodes.

00:02:25.620 --> 00:02:27.810
That's what everybody thinks
about the first time when they

00:02:27.890 --> 00:02:29.700
think about what a Java VM does.

00:02:29.700 --> 00:02:33.470
The other things it has to do, though,
is it has to manage its own threads,

00:02:33.470 --> 00:02:37.920
you know, the threads on the system,
because part of Java is lots of threads,

00:02:37.920 --> 00:02:41.290
right, synchronization and all
those kinds of things.

00:02:42.000 --> 00:02:44.630
VM has to deal with memory management.

00:02:44.950 --> 00:02:49.040
Those of you coming from a C or
C++ environment know all about

00:02:49.040 --> 00:02:50.560
what I'm talking about here.

00:02:50.560 --> 00:02:52.130
We take care of your garbage for you.

00:02:52.320 --> 00:03:00.050
We spend our mental cycles figuring
out how to do that so that you don't

00:03:00.050 --> 00:03:02.470
have to spend cycles on that and you
can spend your cycles thinking about

00:03:02.470 --> 00:03:02.470
the problem you're trying to solve.

00:03:02.840 --> 00:03:06.700
The last thing I consider a VM,
in a broad sense,

00:03:06.700 --> 00:03:12.260
of having responsibility for is
going outside the machine into,

00:03:12.390 --> 00:03:16.460
back into C code for
one reason or another.

00:03:16.460 --> 00:03:20.740
And both going out into C,
but also being able to be

00:03:21.060 --> 00:03:23.600
called into from C at times.

00:03:23.600 --> 00:03:26.460
So a VM manages these four things.

00:03:26.580 --> 00:03:31.790
It has to do it well,
or we want it to do it well,

00:03:31.790 --> 00:03:36.660
so that your code runs and runs
very efficiently on the system

00:03:36.670 --> 00:03:38.410
that we give you with Mac OS X.

00:03:39.860 --> 00:03:42.810
So the real question,
or the thing that I want to try to

00:03:42.810 --> 00:03:46.540
get across in this part of the talk
is I want to give you a measure a

00:03:46.540 --> 00:03:54.260
little bit about how well we do Java,
the Java virtual machine on Mac OS X.

00:03:57.090 --> 00:04:02.470
A few weeks ago, we had Java 1,
which is where you learn all about Java.

00:04:02.840 --> 00:04:08.280
And I was kind of surprised to see
that Java Pro had picked us as one of

00:04:08.410 --> 00:04:10.900
their finalists for the best Java VM.

00:04:10.930 --> 00:04:14.340
I thought that was pretty neat.

00:04:14.340 --> 00:04:14.340
And so --

00:04:16.010 --> 00:04:17.100
We were finalists.

00:04:17.220 --> 00:04:20.010
We didn't actually walk
off with the total reward,

00:04:20.010 --> 00:04:24.000
but that got me to thinking,
and that led to this slide, in fact.

00:04:24.090 --> 00:04:29.740
We are, by perhaps some people's opinion,
you know, the best of breed for Java VMs.

00:04:29.740 --> 00:04:34.200
I put a question mark up there,
not because I doubt it, because I know,

00:04:34.240 --> 00:04:38.100
but it's really for you guys to decide,
right?

00:04:38.100 --> 00:04:41.800
So I'd like to make this assertion,
see whether or not I can, you know,

00:04:41.800 --> 00:04:44.490
this hypothesis or whatever,
and I'll let you guys figure out,

00:04:44.490 --> 00:04:47.780
and you can come to your own
conclusions as to whether this is true.

00:04:47.780 --> 00:04:51.920
So I'm going to be talking
about these four things,

00:04:52.190 --> 00:04:57.170
or these four areas, and in fact,
these are going to support those

00:04:57.170 --> 00:05:02.380
four theses that I think that
a Java VM has to do altogether.

00:05:02.380 --> 00:05:05.300
So first of all,
advanced language integration.

00:05:06.920 --> 00:05:09.450
We have,
in addition to the JNI that you all

00:05:09.510 --> 00:05:14.840
know and love from any other platform,
we have two other systems that allow

00:05:14.840 --> 00:05:18.820
you to not even think about JNI because
we do the JNI thinking for you.

00:05:18.920 --> 00:05:20.900
The first of those is JDIRECT.

00:05:21.010 --> 00:05:26.610
JDIRECT allows you to, from Java,
access C function calls.

00:05:26.620 --> 00:05:29.740
Well, C++ if you could really
get the naming right,

00:05:29.740 --> 00:05:32.820
but for the most part,
C libraries and whatnot.

00:05:33.000 --> 00:05:37.400
So you can write a little bit of
Java code that basically says,

00:05:37.400 --> 00:05:38.930
open a file.

00:05:38.940 --> 00:05:41.910
And you'll get back an integer
that's your file descriptor.

00:05:41.940 --> 00:05:44.180
And you can write data to it.

00:05:44.300 --> 00:05:47.290
And you can get, well,
you can't get to Erno,

00:05:47.470 --> 00:05:50.770
but you can get the read
result back from that.

00:05:50.810 --> 00:05:53.180
And the fun thing about that
is you don't have to write

00:05:53.180 --> 00:05:54.530
any JNI code to get to that.

00:05:54.560 --> 00:05:59.300
And that's because we do that for
you with a facility we call JDIRECT.

00:05:59.400 --> 00:06:02.980
It actually writes the stubs on the web.

00:06:02.980 --> 00:06:06.020
It writes the stubs on the fly
using sort of the type information

00:06:06.020 --> 00:06:08.730
that you provide in Java native,
in your declaration of

00:06:08.730 --> 00:06:10.100
a Java native method.

00:06:10.120 --> 00:06:13.900
And we build the JNI stubs
for you on the fly.

00:06:13.920 --> 00:06:17.390
A different category of language
integration we provide is

00:06:17.560 --> 00:06:19.260
that from the Java bridge.

00:06:19.260 --> 00:06:21.960
It turns out that...

00:06:22.320 --> 00:06:26.180
If you went to the job overview,
you heard James Gosling say that,

00:06:26.180 --> 00:06:29.190
you know, there were a few guys from
Next who went over to this thing

00:06:29.190 --> 00:06:32.160
called First Person and they
took some of their ideas along.

00:06:32.160 --> 00:06:36.340
Well, it turns out that a lot of those
ideas got reflected in Java.

00:06:36.490 --> 00:06:41.270
So the Objective-C runtime model,
the object model and runtime model

00:06:41.270 --> 00:06:44.060
are almost identical to Java.

00:06:44.060 --> 00:06:46.600
So we've been working with this
technology for a long time.

00:06:46.600 --> 00:06:49.970
And what that allows us to
do is essentially map one

00:06:49.970 --> 00:06:55.700
for one the Cocoa classes and
methods into Java counterparts.

00:06:55.730 --> 00:07:00.460
And we use the Java Bridge technology
to write what for us are those

00:07:00.460 --> 00:07:05.440
JNI wrappers in a very neat way
such that you can actually subclass.

00:07:05.440 --> 00:07:07.610
So all of Cocoa comes
up with a Java face.

00:07:07.860 --> 00:07:09.970
So there's Java,
there's an NS application,

00:07:09.970 --> 00:07:13.540
there's an NS view,
and you can subclass it in Java.

00:07:13.580 --> 00:07:16.610
And, you know, when you do super,
the method goes across

00:07:16.610 --> 00:07:20.470
the bridge into the native
Objective-C stuff and things happen.

00:07:20.480 --> 00:07:25.160
So you can subclass NS view from
Java and we make use of Cocoa underneath.

00:07:25.250 --> 00:07:26.470
Pretty neat trick.

00:07:26.490 --> 00:07:30.060
That's the kind of language integration
that we think is a big value add.

00:07:31.390 --> 00:07:33.680
This is nothing new for us.

00:07:33.770 --> 00:07:37.350
We've had both of these technologies
for several years now and they both

00:07:37.390 --> 00:07:39.290
shipped last year with Mac OS XO.

00:07:39.450 --> 00:07:41.800
I won't go into them anymore.

00:07:41.830 --> 00:07:46.230
There's release notes and our email
addresses and all discussion lists

00:07:46.230 --> 00:07:48.300
and stuff that can talk about this.

00:07:48.300 --> 00:07:52.300
Let me talk about the
Java virtual machine itself.

00:07:52.300 --> 00:07:54.490
We did superb language integration.

00:07:54.560 --> 00:07:57.270
The next thing is the
virtual machine itself.

00:07:57.730 --> 00:08:01.020
We use the Hotspot
virtual machine from Sun.

00:08:01.070 --> 00:08:04.460
Sun provides us with
this huge pile of code,

00:08:04.510 --> 00:08:10.260
700 files of C++, tightly designed,
well abstracted.

00:08:10.260 --> 00:08:13.640
It's our job to make it run on
our PowerPC architecture on our

00:08:14.080 --> 00:08:18.040
Darwin Mach BSD system underneath.

00:08:18.100 --> 00:08:22.900
The neat thing about the Hotspot virtual
machine technology is that it is years,

00:08:22.900 --> 00:08:24.820
has been years in refinement.

00:08:24.820 --> 00:08:28.650
We get most of the benefits of that
refinement without us having to add

00:08:28.680 --> 00:08:34.770
all that much innovation in terms of
how it decides when to do a bytecode,

00:08:34.770 --> 00:08:38.380
when to do this, when to do that.

00:08:38.380 --> 00:08:42.940
We get it up and running on
our Mac OS X system and do the

00:08:42.940 --> 00:08:48.860
small job of the PowerPC part,
which turns out to be 70% of the work.

00:08:48.860 --> 00:08:52.120
In any case,
it's tuned such that only 10%

00:08:52.130 --> 00:08:53.820
of your time is really running
on the Hotspot virtual machine.

00:08:53.820 --> 00:08:54.820
The next thing we want to
talk about is the interpreter.

00:08:54.820 --> 00:08:59.040
The interpreter, as I say,
is on the fly built,

00:08:59.070 --> 00:09:02.000
which means that we can take advantage.

00:09:02.100 --> 00:09:07.160
Every time you start up Java,
we create a new interpreter for you.

00:09:09.230 --> 00:09:13.580
We make that interpreter different
based on whether you're on a G3 or

00:09:13.690 --> 00:09:17.920
a G4 or whether you're on a single
processor or on a multi-processor.

00:09:17.920 --> 00:09:21.200
So that interpreter can be really tuned.

00:09:21.200 --> 00:09:23.950
The other one that we see a lot is
whether you're running debug mode or not.

00:09:23.950 --> 00:09:27.050
If you're running debug mode,
the interpreter for every little,

00:09:27.180 --> 00:09:30.860
you know, bytecode has an extra branch in
there to find out whether or not

00:09:30.860 --> 00:09:34.270
you're being--whether you have a
break point or other things going on.

00:09:34.630 --> 00:09:39.060
These--this is very advanced
technology even for the interpreter.

00:09:39.060 --> 00:09:42.240
So it has to run fast but
the real bulk of your code,

00:09:42.240 --> 00:09:47.000
the bulk of time spent in your
code is done in the compiled stuff,

00:09:47.150 --> 00:09:51.200
where we take those bytecodes
and we on the fly compile them,

00:09:51.770 --> 00:09:55.540
hopefully without any pause times
while you're running your app,

00:09:55.540 --> 00:09:59.150
visible pause times that is,
into a fairly small amount of code

00:09:59.150 --> 00:10:01.320
relative to the size of your application.

00:10:01.320 --> 00:10:03.880
We've really only measured about two
megabytes worth of compilation time.

00:10:03.880 --> 00:10:06.340
So you can compile code at any time.

00:10:06.460 --> 00:10:09.200
That is just really awesome stuff.

00:10:09.210 --> 00:10:12.640
I'll talk a little bit more about
compiler stuff later in this talk and

00:10:12.640 --> 00:10:16.520
then on Friday there'll be another
talk where you'll learn--you can learn

00:10:16.520 --> 00:10:19.640
even more about--and about why you
need to know a little bit about it.

00:10:19.640 --> 00:10:21.480
The other thing about Hotspot--

00:10:24.090 --> 00:10:28.080
It has a patented low cost
synchronization mechanism such that

00:10:28.180 --> 00:10:33.350
if you have synchronized methods,
there's virtually no cost to using them.

00:10:33.470 --> 00:10:36.350
So you use them liberally.

00:10:36.450 --> 00:10:37.260
Use them freely.

00:10:37.260 --> 00:10:40.110
If there's no two threads
contending for it,

00:10:40.120 --> 00:10:41.520
it's virtually costless.

00:10:41.520 --> 00:10:44.630
We make it as cheap as we
can for the CPU you're on.

00:10:44.690 --> 00:10:46.730
For the PowerPC, that is darn cheap.

00:10:47.000 --> 00:10:50.530
There's only a traditional
expense of a big,

00:10:50.690 --> 00:10:54.540
heavy,
go to the kernel and wait kind of stop

00:10:54.540 --> 00:10:56.680
when there's actual contention on things.

00:10:56.680 --> 00:10:59.310
So the finer grained your locks are,
the less likely you're

00:10:59.310 --> 00:11:00.560
going to get contended.

00:11:00.560 --> 00:11:04.720
Part of the patent has to do with the
fact that there's no memory overhead

00:11:04.800 --> 00:11:06.920
for the synchronization either.

00:11:06.920 --> 00:11:09.750
You can synchronize on
any object within Java,

00:11:09.750 --> 00:11:12.480
but we don't have an
extra lock word for that.

00:11:12.480 --> 00:11:15.350
We build the lock word on the fly,
on the stack.

00:11:15.350 --> 00:11:16.700
It's very neat.

00:11:17.000 --> 00:11:19.620
And of course,
Hotspot brings with it a state

00:11:19.910 --> 00:11:22.000
of the art generational GC.

00:11:22.000 --> 00:11:25.640
Last year I talked about that
in a fair amount of detail.

00:11:25.700 --> 00:11:28.570
This year I'll talk a little
somewhat less about it,

00:11:28.670 --> 00:11:30.620
but give you some of the high points.

00:11:31.210 --> 00:11:33.140
Tight OS integration.

00:11:33.140 --> 00:11:36.460
We know our OS.

00:11:36.460 --> 00:11:40.300
Hotspot comes, I mean,
it's like a hand in a glove.

00:11:40.380 --> 00:11:43.170
Hotspot on top of
Mac OS X is just so tight,

00:11:43.170 --> 00:11:44.610
it's beautiful.

00:11:44.770 --> 00:11:48.870
The threads are one to one,
files are one to one, I/O is one to one.

00:11:48.930 --> 00:11:52.710
It is like the Java virtual
machine is just right a thin

00:11:52.710 --> 00:11:54.380
layer above the operating system.

00:11:54.380 --> 00:11:57.200
It's really neat because that
way the operating system guys

00:11:57.200 --> 00:11:58.810
get to do the scheduling stuff.

00:11:58.980 --> 00:12:01.600
The operating system guys
get to do the I/O buffering.

00:12:01.700 --> 00:12:04.960
The operating system guys get to
do all the stuff that they do well.

00:12:04.980 --> 00:12:08.850
And, you know, from the VM viewpoint,
we just hand that work off to them.

00:12:09.060 --> 00:12:11.580
We like that part a lot.

00:12:11.580 --> 00:12:14.680
We use, for example, we have some tricks.

00:12:14.680 --> 00:12:15.950
We know the kernel guys.

00:12:16.070 --> 00:12:17.280
We ask them for favors.

00:12:17.440 --> 00:12:21.200
So in certain places where
on other VMs you would see,

00:12:21.200 --> 00:12:26.060
you know, code expansion to do, you know,
like null pointer exceptions,

00:12:26.060 --> 00:12:27.660
that's what MPE is there.

00:12:27.660 --> 00:12:29.560
We actually put a little
trap word in place--well,

00:12:29.580 --> 00:12:30.860
we actually take a fault.

00:12:30.860 --> 00:12:33.750
We take a memory access
fault from the kernel.

00:12:33.910 --> 00:12:35.540
It's turned into a Mach exception.

00:12:35.540 --> 00:12:36.310
We intercept it.

00:12:36.340 --> 00:12:38.800
We say, "Hmm,
this is an area that we know about.

00:12:39.220 --> 00:12:43.340
That was a null pointer exception."
In other places we insert actual

00:12:43.340 --> 00:12:47.700
trap instructions and take those,
fault those, handle those,

00:12:47.710 --> 00:12:49.170
and deal with that.

00:12:49.250 --> 00:12:54.040
What that gives us, again,
is gives you very fast code paths and

00:12:54.330 --> 00:12:56.100
you only pay for them when you use them.

00:12:56.150 --> 00:13:01.090
So the basic advice here as
you've read in any Java program

00:13:01.090 --> 00:13:02.740
is don't program using exceptions.

00:13:02.870 --> 00:13:08.090
And it's because they're very
expensive and that's the design point.

00:13:09.650 --> 00:13:16.370
The last thing we do is a little thing,
but Mach was a design,

00:13:16.370 --> 00:13:19.630
one of the designers of
Mach was Avi Tavanian,

00:13:19.630 --> 00:13:22.060
who you may have heard of or seen before.

00:13:22.060 --> 00:13:26.380
And one of the things he did with Mach is
give it a great virtual memory subsystem.

00:13:26.380 --> 00:13:30.220
So part of the virtual memory subsystem
is the idea that when our garbage

00:13:30.220 --> 00:13:34.320
collector is done with a whole chunk
of memory and we no longer want it,

00:13:34.320 --> 00:13:37.840
it's in our address space and
typically it would get swapped out

00:13:37.840 --> 00:13:40.060
to the disk once we stopped using it.

00:13:40.080 --> 00:13:43.790
But what we do is we tell the OS, "Hey,
we don't need those pages anymore.

00:13:43.790 --> 00:13:46.310
We want to keep the addresses, you know,
reserved.

00:13:46.310 --> 00:13:50.010
But we don't need the dirty bits
behind it." And so the OS just kind of

00:13:50.140 --> 00:13:51.830
starts using those pages right away.

00:13:52.290 --> 00:13:55.630
The next time we start faulting it,
it gives us zero-filled pages.

00:13:55.860 --> 00:13:57.770
So it's just little tricks like that.

00:13:57.830 --> 00:14:01.870
We try to, you know, use them wherever we
can so that you guys,

00:14:01.870 --> 00:14:05.180
who program in Java,
just don't have to worry about any

00:14:05.180 --> 00:14:09.850
of these little nasty little details
that make a lot of difference.

00:14:10.450 --> 00:14:12.200
CPU utilization.

00:14:12.210 --> 00:14:15.950
Not only do we know our OS,
we know our CPUs.

00:14:16.250 --> 00:14:19.400
As I said before,
the code gen that our compiler does as

00:14:19.500 --> 00:14:24.230
well as our interpreter varies based
on whether you're on a multiprocessor

00:14:24.340 --> 00:14:28.690
system or on a single processor system,
whether you're on a G3 or

00:14:28.690 --> 00:14:30.790
whether you're on a G4.

00:14:30.890 --> 00:14:31.540
What could be better?

00:14:31.540 --> 00:14:33.730
You don't have to pick
your processor type.

00:14:33.870 --> 00:14:36.490
You know, as a compiler group,
we don't have to pick something

00:14:36.570 --> 00:14:39.340
that works everywhere even
though--and best on one but,

00:14:39.340 --> 00:14:41.800
you know,
not as great on another kind of thing.

00:14:41.800 --> 00:14:45.320
We can do the best job
for you at runtime,

00:14:45.350 --> 00:14:46.850
and we try to.

00:14:47.580 --> 00:14:51.740
For example, last year we introduced the
use of the Velocity Engine,

00:14:51.750 --> 00:14:54.560
which is on G4 chips, for fast copying.

00:14:54.620 --> 00:14:59.080
The Velocity Engine can soak the
memory bandwidth coming out of

00:14:59.080 --> 00:15:01.120
the chip like nothing else can.

00:15:01.120 --> 00:15:05.100
A tight loop cannot drive memory
faster than the Velocity Engine.

00:15:05.100 --> 00:15:06.960
We use that to copy bits.

00:15:06.960 --> 00:15:10.680
It was so good, in fact,
that we've handed that code off and

00:15:10.680 --> 00:15:15.780
it's now at the base of Mac OS X and
is used for -- is it MemCopy or B-Copy?

00:15:15.870 --> 00:15:17.430
It's MemCopy.

00:15:17.500 --> 00:15:20.060
It's based for MemCopy and Jaguar.

00:15:20.060 --> 00:15:23.190
Some of our ideas are done so well
that they're even getting picked

00:15:23.190 --> 00:15:25.280
up elsewhere within our system.

00:15:25.280 --> 00:15:29.860
On the G4, the G4 has a lot of registers.

00:15:30.280 --> 00:15:33.380
So when we compile things,
we don't have to follow

00:15:33.380 --> 00:15:36.570
the ABI that was laid down,
you know, how many years ago when they

00:15:36.570 --> 00:15:37.780
designed the PowerPC chip.

00:15:37.780 --> 00:15:40.920
We have our own ABI, you know,
where registers get, you know,

00:15:40.920 --> 00:15:43.340
where the parameters go, where locals go.

00:15:43.480 --> 00:15:47.890
And so we actually dedicate the use
of a few registers for things that

00:15:47.930 --> 00:15:53.680
make interpreting and compiling
Java and running Java even faster.

00:15:53.990 --> 00:15:56.370
One of the other uses for
registers is cache locals,

00:15:56.460 --> 00:15:57.080
of course.

00:15:57.080 --> 00:16:00.270
This is a standard compiler technique.

00:16:00.380 --> 00:16:04.000
If you have a lot of registers,
you just dedicate or you get to use some

00:16:04.000 --> 00:16:06.400
of them for local variables and whatnot.

00:16:06.400 --> 00:16:13.990
We've introduced that in our hotspot
technology in the 10.1 timeframe.

00:16:14.860 --> 00:16:20.130
We, another little nifty-ism,
the CPU has, the CPU chip has sort of a

00:16:20.550 --> 00:16:25.130
countdown timer on it or actually
a continually clocking timer.

00:16:25.130 --> 00:16:30.030
And so we gauge and make use of that
for use when we do get time millis.

00:16:30.190 --> 00:16:33.280
So when you need to do
timing of your stuff,

00:16:33.280 --> 00:16:35.490
we don't have to go to the
system to make use of that.

00:16:35.490 --> 00:16:40.100
We can interpolate from the system and
use of the clock register directly such

00:16:40.100 --> 00:16:43.040
that our get time millis is bloody fast.

00:16:43.040 --> 00:16:46.450
We were actually talking in the hallway
earlier this afternoon about not even

00:16:46.460 --> 00:16:50.550
having to go to a subroutine for that,
but actually inlining it every time

00:16:50.550 --> 00:16:52.940
you use it inside your compiled code.

00:16:52.940 --> 00:16:55.690
No promises on that one,
but it's an idea for us to explore.

00:16:58.450 --> 00:17:01.690
Let me talk about this.

00:17:02.080 --> 00:17:06.280
So the thesis is we do a really
good job with the compiler,

00:17:06.280 --> 00:17:08.400
with the OS, with the CPU.

00:17:08.400 --> 00:17:11.710
What better way to show it than to

00:17:12.160 --> 00:17:15.160
Cook up a micro benchmark.

00:17:15.230 --> 00:17:18.650
A micro benchmark,
most people call them benchmarks.

00:17:18.700 --> 00:17:53.600
[Transcript missing]

00:17:53.830 --> 00:18:00.700
Now, the reason I like this benchmark in
particular is that it's multi threads,

00:18:00.700 --> 00:18:03.810
there's contention,
there's that garbage collection going on,

00:18:03.810 --> 00:18:06.770
and then you got the effect of the
compiler coming in when you run it enough

00:18:06.800 --> 00:18:08.660
so you're actually compiling this code.

00:18:08.740 --> 00:18:12.700
So it shows all those four things
I think a VM has to do well.

00:18:13.040 --> 00:18:16.130
We have to run it on a
dual processor machine,

00:18:16.140 --> 00:18:17.910
of course.

00:18:18.400 --> 00:18:21.730
And the famous note
about micro benchmarks,

00:18:21.730 --> 00:18:24.090
it's a contrived usage pattern.

00:18:25.180 --> 00:18:27.560
Your mileage will be way less.

00:18:27.810 --> 00:18:31.890
Don't look at this as being anything
representative of speed advantages

00:18:31.920 --> 00:18:33.980
you're going to see in your program.

00:18:33.990 --> 00:18:37.250
But let's see what happens with this.

00:18:38.340 --> 00:18:44.760
So between C, C++, and Objective-C,
let me explain the slide a little bit.

00:18:44.760 --> 00:18:47.440
The vertical column
measures peak allocations,

00:18:47.490 --> 00:18:51.830
cumulative,
I mean total allocations per second.

00:18:51.800 --> 00:19:45.800
[Transcript missing]

00:19:46.280 --> 00:19:51.890
When we compare Java to this, however,
we have to change the scale on the graph.

00:19:53.940 --> 00:19:59.340
The, what we provided to you in 10.1
for a single threaded was 8,000

00:19:59.340 --> 00:20:02.530
allocations a second as opposed to 700.

00:20:02.630 --> 00:20:05.320
So that's over 10 times faster.

00:20:05.380 --> 00:20:08.970
Note that as soon as we go to
a multi-threaded case though,

00:20:09.040 --> 00:20:12.080
Java's allocation rate
drops significantly,

00:20:12.150 --> 00:20:13.260
more than half.

00:20:13.260 --> 00:20:16.850
But even so,
we get 3,000 allocations per second

00:20:16.980 --> 00:20:19.600
in the multi-threaded contended case.

00:20:19.650 --> 00:20:21.720
And you see that it's basically linear.

00:20:21.720 --> 00:20:26.520
No matter how many threads you get, the,
you know, the total number of allocations

00:20:26.520 --> 00:20:28.500
really doesn't drop off.

00:20:28.500 --> 00:20:31.210
That's really what you want to see.

00:20:31.210 --> 00:20:31.210
For

00:20:31.750 --> 00:20:33.760
We've done a little bit better.

00:20:33.870 --> 00:20:37.740
In fact, we've doubled the
contended allocation rate.

00:20:37.740 --> 00:20:43.340
And I'll tell you a little bit
about how we did that as we go on.

00:20:43.340 --> 00:20:45.100
Memory utilization.

00:20:46.590 --> 00:20:52.510
Pre-Mac OS X, 10.0,
and all other Java systems today.

00:20:53.660 --> 00:20:54.920
have the following issue.

00:20:54.960 --> 00:20:58.240
JAR files are not shared libraries.

00:20:58.240 --> 00:21:03.970
The classes.jar that contains
swing is not a shared library.

00:21:04.080 --> 00:21:09.560
Every app has to read those system JARs,
has to process those system JARs.

00:21:09.650 --> 00:21:11.350
They're compressed, remember.

00:21:11.350 --> 00:21:15.220
Has to build up data structures
to make use of those -- make use

00:21:15.220 --> 00:21:17.980
of the data that's within them.

00:21:18.350 --> 00:21:20.790
And, you know,
literally it keeps a copy of all

00:21:20.810 --> 00:21:23.250
those byte codes in every application.

00:21:23.250 --> 00:21:27.100
In Mac OS 10.0 last year,
we introduced a technology known

00:21:27.100 --> 00:21:31.890
as the shared generation to help
resolve some of these issues.

00:21:34.280 --> 00:21:38.080
A hotspot garbage collector,
GC garbage collector,

00:21:38.080 --> 00:21:41.400
that we get is a very
sophisticated garbage collector.

00:21:41.400 --> 00:21:47.520
What it tries to do is spend as few
cycles allocating an object as it can,

00:21:47.520 --> 00:21:49.130
because most objects die young.

00:21:49.130 --> 00:21:53.250
And if they're not dead,
it spends as few cycles as possible

00:21:53.250 --> 00:21:55.700
remembering that they're alive.

00:21:55.800 --> 00:22:00.010
And so what they do is they
provide essentially four stages,

00:22:00.100 --> 00:22:04.180
four different types of generations
where in the first stage,

00:22:04.230 --> 00:22:07.040
the first generation,
the fastest generation,

00:22:07.040 --> 00:22:09.140
it's essentially a pointer bump.

00:22:09.140 --> 00:22:14.290
Now, in Mac OS 10.0 and 10.1,
there was a little bit of a

00:22:14.290 --> 00:22:15.890
lock around that pointer bump.

00:22:16.710 --> 00:22:19.840
And in Jaguar,
actually in the Java update,

00:22:19.880 --> 00:22:21.040
there is no lock for now.

00:22:21.190 --> 00:22:23.870
And that's where we got
our doubled performance.

00:22:23.880 --> 00:22:27.840
The other generations,
two-space copy is where things

00:22:27.840 --> 00:22:29.380
live for a little while.

00:22:29.380 --> 00:22:33.040
And tenured is where your
objects live when they're adults.

00:22:33.110 --> 00:22:37.440
Now, the permanent generation is kind
of a hotspot implementation detail,

00:22:37.440 --> 00:22:41.500
in that all those byte codes and
stuff that you get out of class files,

00:22:41.580 --> 00:22:46.190
that stuff lives, that metadata lives in
the permanent generation.

00:22:47.040 --> 00:22:49.670
The shared generation idea
we introduced last year was

00:22:49.670 --> 00:22:53.000
that those metadata objects,
those bytecodes,

00:22:53.050 --> 00:22:56.900
many of them don't even change
and they essentially never die.

00:22:57.060 --> 00:23:01.040
So we added sort of an
immortal generation.

00:23:01.530 --> 00:23:04.490
What we do is we pre-process
the system jars once,

00:23:04.490 --> 00:23:07.630
reading in all those byte
codes and strings and class

00:23:07.630 --> 00:23:09.340
tables and all that stuff.

00:23:09.340 --> 00:23:11.900
And if you ever watch
your Mac OS X machine,

00:23:11.900 --> 00:23:16.180
you might see this fleeting message,
building Java shared archive.

00:23:16.180 --> 00:23:18.660
Well, that's actually when we cook it.

00:23:18.760 --> 00:23:21.200
We cook it when you boot your system.

00:23:21.300 --> 00:23:25.850
So we cook up all that data,
we write it out to disk,

00:23:25.910 --> 00:23:30.880
and every other time you launch Java,
we just M-map that region in,

00:23:30.880 --> 00:23:32.680
do a little bit of patch up, and run.

00:23:32.790 --> 00:23:34.800
And so, guess what?

00:23:35.180 --> 00:23:37.710
None of that time is
spent reading those IOs.

00:23:37.830 --> 00:23:39.300
We get to share some of that memory.

00:23:39.300 --> 00:23:42.100
It's just a total win.

00:23:42.180 --> 00:23:43.560
I'll talk a little bit more about it.

00:23:43.600 --> 00:23:49.160
So Apple's Hotspot Garbage
Collector has that shared

00:23:49.170 --> 00:23:50.920
generation as a new technology.

00:23:51.130 --> 00:23:53.390
technology at the bottom.

00:23:54.480 --> 00:23:56.490
So this is a very busy graph.

00:23:56.630 --> 00:23:58.400
I won't go into it in detail.

00:23:58.450 --> 00:24:03.000
If you look at the middle graph,
that purple area is the region

00:24:03.000 --> 00:24:06.050
we use for all those byte
codes and things like that.

00:24:06.260 --> 00:24:08.960
And as I said before,
we split it into a couple regions.

00:24:08.960 --> 00:24:10.960
One's totally read-only, totally shared.

00:24:10.960 --> 00:24:14.640
The other one is shared
until it's written to.

00:24:14.640 --> 00:24:17.790
And then for dynamically
discovered class files and stuff,

00:24:17.890 --> 00:24:19.600
you still have a bit of purple.

00:24:19.600 --> 00:24:24.230
So your code goes in the area,
the purple area at the very beginning.

00:24:24.400 --> 00:24:29.020
Now, the really neat thing about this

00:24:29.100 --> 00:24:42.000
[Transcript missing]

00:24:42.180 --> 00:24:44.230
and We spend zero cycles on them.

00:24:44.390 --> 00:24:47.330
I mean, how fewer cycles can you get,
right?

00:24:47.480 --> 00:24:48.280
Zero is zero.

00:24:48.460 --> 00:24:49.910
It doesn't get better than that.

00:24:49.950 --> 00:24:53.840
And so we see that in terms of
running time of your application.

00:24:53.840 --> 00:24:58.800
Our full GC times are lower for the--when
we're running the shared GC stuff.

00:24:59.140 --> 00:25:03.300
We get a, there's some benefit,
not as much as we'd like in terms of

00:25:03.400 --> 00:25:05.880
packing because of the way we arrange,
you know,

00:25:05.880 --> 00:25:09.370
we can load methods that are actually
used instead of the entire class and

00:25:09.370 --> 00:25:11.630
including the methods that you don't use.

00:25:11.820 --> 00:25:14.340
So there's some memory
packing benefits as well.

00:25:16.930 --> 00:25:22.650
The coolest thing on here is
that Apple's shared generation

00:25:22.650 --> 00:25:26.840
technology may well become Suns.

00:25:26.900 --> 00:25:52.400
[Transcript missing]

00:25:53.130 --> 00:25:54.910
Timeline.

00:25:54.910 --> 00:25:57.160
Let me shift.

00:25:57.160 --> 00:26:05.990
Let me talk a little bit about, okay,
so this is the end of the part where,

00:26:05.990 --> 00:26:05.990
you know, best of breed.

00:26:05.990 --> 00:26:05.990
You've seen this stuff.

00:26:05.990 --> 00:26:05.990
You can, you know, sit back,
think about it.

00:26:08.290 --> 00:26:16.190
Make your own decisions.

00:26:16.190 --> 00:26:16.190
Let me talk about a few of
the things that we've done

00:26:16.190 --> 00:26:16.190
since we spoke to you last.

00:26:16.430 --> 00:26:24.620
In September, we shipped Java 1.3.1.

00:26:24.950 --> 00:26:28.930
1.3.1 was a huge amount of work for
us because it was a whole different

00:26:28.970 --> 00:26:30.540
generation of the Hotspot compiler.

00:26:30.540 --> 00:26:34.890
So we actually had to work long,
hard hours to get 1.3.1 out to you,

00:26:34.890 --> 00:26:37.090
even though it was a .1 release.

00:26:37.590 --> 00:26:40.470
It provided practical debugging.

00:26:40.530 --> 00:26:42.400
130 did not.

00:26:42.550 --> 00:26:46.180
130 did not provide much
of a working JVM PI,

00:26:46.290 --> 00:26:49.330
the profiling interface.

00:26:49.330 --> 00:26:49.330
131 did.

00:26:49.770 --> 00:26:52.780
We introduced,
not only got all that stuff going,

00:26:52.780 --> 00:26:55.180
but we introduced the
velocity engine use,

00:26:55.350 --> 00:26:57.700
we introduced cache locals,
and we improved our shared

00:26:57.760 --> 00:27:03.260
generation technology above and
beyond what we did in Cheetah 10.0.

00:27:06.500 --> 00:27:10.320
We've been working on 1.4, as you know,
but that didn't stop us from doing

00:27:10.320 --> 00:27:13.740
some things and shipping them
in our 1.3.1 update that went

00:27:13.740 --> 00:27:15.670
out just a couple months ago.

00:27:15.700 --> 00:27:20.140
We extended the shared generation
to include constant pools.

00:27:20.140 --> 00:27:22.800
If you know what class file formats are,
you know what this is.

00:27:22.820 --> 00:27:25.860
But basically,
we figured out a way to avoid

00:27:25.950 --> 00:27:30.700
even more garbage collection
cycles by sharing even more data.

00:27:30.700 --> 00:27:36.420
And so those benefits are just there
without you guys having to lift a finger.

00:27:36.500 --> 00:27:39.900
We added something
called thread local Eden.

00:27:39.900 --> 00:27:42.330
It's not on by default.

00:27:42.330 --> 00:27:46.320
You have to trigger it
yourself in this update.

00:27:46.320 --> 00:27:48.800
And the way you trigger
it is that little,

00:27:48.800 --> 00:27:51.580
that funny little command line argument.

00:27:51.580 --> 00:27:57.330
We believe we're going to be
providing a handout at the,

00:27:57.330 --> 00:28:03.970
at our talk on Friday and we'll try to
put some of those tricks on that handout.

00:28:04.210 --> 00:28:10.530
If not, well,
you'll know where to find me because my

00:28:10.530 --> 00:28:10.530
email is going to be up on the slide.

00:28:10.810 --> 00:28:14.750
The other thing we did was
we had some complaints.

00:28:14.970 --> 00:28:16.490
People would bring scripts over,
you know,

00:28:16.560 --> 00:28:19.890
jar files that just worked except
the shell script would like die

00:28:19.900 --> 00:28:23.500
because they had dash server on their,
you know, Java invocation line.

00:28:23.640 --> 00:28:28.700
So we decided we ought to at least fix
dash server such that it didn't complain.

00:28:28.700 --> 00:28:33.120
But we did a little bit more than that
and we actually started tuning it for

00:28:33.140 --> 00:28:35.590
what we consider to be server usage.

00:28:36.780 --> 00:28:40.740
Java Dash Server is not,
as it is on other systems,

00:28:40.740 --> 00:28:42.780
a different compiler.

00:28:42.810 --> 00:28:45.700
It is, at the moment,
just a bit of tuning.

00:28:45.700 --> 00:28:50.220
We do realize, though,
that servers run longer and we

00:28:50.530 --> 00:28:55.150
probably should do more in terms of
compilation than we do right now.

00:28:55.150 --> 00:28:59.400
But Java Dash Server is just the start of
where we're going to be going with that.

00:28:59.420 --> 00:29:02.480
If you took a look at,
if you saw Steve's keynote,

00:29:02.580 --> 00:29:07.340
it appears that we're actually
maybe going to start selling some,

00:29:07.340 --> 00:29:11.800
you know, server hardware as well beyond
server software that we have.

00:29:11.800 --> 00:29:16.360
So we're going to see more and
more interest in that as we go on.

00:29:16.410 --> 00:29:19.300
The thread local lead,
and let me back up a bit, that was,

00:29:19.390 --> 00:29:25.560
as I said before, the main we,
the main method or mechanism by which we

00:29:25.800 --> 00:29:30.710
got that doubled allocation performance.

00:29:31.680 --> 00:29:36.200
We have provided multiple JDK capability.

00:29:36.230 --> 00:29:38.190
This means

00:29:38.510 --> 00:30:08.380
Thank you for watching.

00:30:08.380 --> 00:30:08.380
I'll see you next time.

00:30:08.400 --> 00:30:13.800
How we do this is pretty much
the way we've done it before.

00:30:13.800 --> 00:30:20.950
We have one symlink in the system in the
magic place that points to either a 131

00:30:20.950 --> 00:30:24.420
tree of things or a 14 tree of things.

00:30:24.420 --> 00:30:28.140
The only difference that you should
probably see is that the header files

00:30:28.140 --> 00:30:30.160
are actually now part of that tree.

00:30:30.160 --> 00:30:34.600
So there's a 131 JNI.h and a 14
JNI.h and they're in different

00:30:34.660 --> 00:30:36.540
places just as they should be.

00:30:37.820 --> 00:30:45.590
We have some support for actually
allowing you to bind an application

00:30:45.760 --> 00:30:48.720
to one or the other of these VMs.

00:30:48.750 --> 00:30:54.740
You should look for that
in the release notes.

00:30:54.820 --> 00:30:57.830
We're not sure we're ever
going to ship this this way,

00:30:57.830 --> 00:31:00.530
but we're at least
thinking maybe we might.

00:31:00.540 --> 00:31:07.020
We'd like to get your input as to whether
or not you see that as a requirement.

00:31:07.240 --> 00:31:12.300
Or whether you think that's nice to
have or what you think about that.

00:31:13.760 --> 00:31:17.080
The other thing we're doing for
Jaguar is providing or making use

00:31:17.080 --> 00:31:21.230
of the two-level namespace feature
that got introduced in 10.1.

00:31:21.260 --> 00:31:26.700
So the main impact to you folks
is that if you build JNI libs,

00:31:26.880 --> 00:31:28.180
they no longer have to be bundles.

00:31:28.190 --> 00:31:30.250
They can be dilibs.

00:31:30.250 --> 00:31:35.570
So they can export symbols to other
libraries and other frameworks as well.

00:31:35.980 --> 00:31:39.910
So a consequence of two-level
namespace is that you really

00:31:39.910 --> 00:31:45.300
should never link directly against
a particular version of Hotspot.

00:31:45.300 --> 00:31:49.700
You should only link against the
JNI stub library that we have there.

00:31:49.700 --> 00:31:54.100
And then our launcher will pick
the right VM for you to use.

00:31:58.340 --> 00:32:02.700
A little thing, a nice thing.

00:32:02.700 --> 00:32:06.310
We are taking advantage of some
rearrangements of memory that are

00:32:06.310 --> 00:32:08.280
going on within the rest of Jaguar.

00:32:08.280 --> 00:32:11.110
We help motivate some of that,
such that in terms of the

00:32:11.230 --> 00:32:14.680
four gigabyte address space,
Apple is no longer scattering

00:32:14.790 --> 00:32:16.350
things all through it.

00:32:16.470 --> 00:32:20.230
So we're trying to clean up some
big chunk of it so that the net

00:32:20.230 --> 00:32:24.430
effect for you folks who buy
1.5 gigabytes of memory in your

00:32:24.430 --> 00:32:26.700
servers is you can use most of it.

00:32:27.240 --> 00:32:30.410
So I think,
I don't think that's quite in place

00:32:30.410 --> 00:32:34.050
in the Jaguar that you see right now,
but you should just know

00:32:34.050 --> 00:32:35.310
that we're working on that.

00:32:35.420 --> 00:32:38.090
So that will be there by
the time Jaguar ships.

00:32:40.170 --> 00:32:47.030
We, you know,
if only I had good sound effects.

00:32:48.500 --> 00:32:54.620
EIEIO is an instruction on the CPU.

00:32:54.620 --> 00:32:56.500
And we figured out how to use it.

00:32:56.550 --> 00:33:01.620
It turns out that a combination of that
instruction and one other is actually

00:33:01.620 --> 00:33:06.010
faster for doing synchronization
than what we've been doing before.

00:33:06.090 --> 00:33:08.820
So, like I said,
when we figure these things out,

00:33:08.820 --> 00:33:12.310
we can roll them in and your code
gets better without you touching it.

00:33:12.470 --> 00:33:16.910
So, if only I could, you know,
bring in a McDonald's song in here.

00:33:16.910 --> 00:33:20.650
But anyway,
we could all break out in chorus,

00:33:21.190 --> 00:33:25.760
The last thing we did,
we updated to 13103.

00:33:25.760 --> 00:33:28.840
Every now and then we get these little
security updates and stuff like that.

00:33:28.840 --> 00:33:33.100
And so we roll them in, fold them in,
and get them out as quick as we can.

00:33:33.100 --> 00:33:36.260
So those are the things
you're going to see in Jaguar.

00:33:36.830 --> 00:33:38.340
Oh yeah.

00:33:38.370 --> 00:33:42.810
Last year I mentioned that
there was a third collector,

00:33:42.990 --> 00:33:47.280
the train collector, the incremental GC,
and that we hadn't done

00:33:47.280 --> 00:33:48.450
really much testing at it.

00:33:48.470 --> 00:33:50.740
And I said, test it.

00:33:50.810 --> 00:33:53.740
If this works for you, ship with it.

00:33:53.920 --> 00:33:55.560
Well,
somebody went off and actually did that,

00:33:55.560 --> 00:33:57.070
and they found out that it didn't work.

00:33:57.130 --> 00:34:00.330
So again, if you have code coming
in from other systems,

00:34:00.470 --> 00:34:03.340
what we've done is we've,
unfortunately we haven't

00:34:03.440 --> 00:34:06.360
actually fixed it,
but we've at least disabled

00:34:06.360 --> 00:34:06.780
it so that it can work.

00:34:06.800 --> 00:34:07.530
So the Inc.

00:34:07.530 --> 00:34:10.700
GC doesn't eventually cause
your program to crash.

00:34:10.700 --> 00:34:12.830
So that's an outstanding bug on our part.

00:34:12.850 --> 00:34:14.930
But you can use the option.

00:34:14.930 --> 00:34:16.560
It's just ignored.

00:34:19.700 --> 00:34:27.700
[Transcript missing]

00:34:28.050 --> 00:34:29.890
First of all, there's a whole talk.

00:34:30.000 --> 00:34:33.750
1.4 is coming along with some
technology that we're going to use

00:34:33.750 --> 00:34:36.590
to make running your programs better.

00:34:38.200 --> 00:34:40.930
and there's going to be stuff
that you have to use to make

00:34:41.060 --> 00:34:43.840
your programs run better.

00:34:43.840 --> 00:34:45.800
You'll have to go to that talk
to find out more of the detail.

00:34:45.800 --> 00:34:49.710
I'll give you two slides worth of an
overview of what's going on in that talk.

00:34:50.600 --> 00:34:53.810
If you want to make
your program run faster,

00:34:53.810 --> 00:34:57.810
my best advice is to go buy
a professional tool for that.

00:34:57.810 --> 00:35:00.760
We've worked with the folks that
optimize it and they have recently

00:35:00.760 --> 00:35:05.930
been acquired by JBuilder and
that does really nice things.

00:35:07.490 --> 00:35:10.760
If you don't--you do all
of these if you want.

00:35:10.760 --> 00:35:13.560
More the better, right?

00:35:13.590 --> 00:35:18.240
I believe HP has a public domain
HP Prof program that interprets

00:35:18.320 --> 00:35:20.400
the standard output of Java stuff.

00:35:20.600 --> 00:35:24.410
I believe that's a free download,
and it uses just the

00:35:24.410 --> 00:35:26.240
standard APIs as well.

00:35:26.410 --> 00:35:28.730
Of course, JVM Profiling Interface.

00:35:28.760 --> 00:35:31.380
If you really know what you're doing,
you can write your own, you know,

00:35:31.380 --> 00:35:35.100
event monitoring things and
keep track of particular heavy,

00:35:35.100 --> 00:35:36.940
you know, items yourself.

00:35:36.940 --> 00:35:41.030
The built-in profilers are, you know,
basically no different except

00:35:41.030 --> 00:35:45.560
we've actually tested them
and made sure they work.

00:35:45.560 --> 00:35:47.220
And they come in three flavors.

00:35:47.230 --> 00:35:53.790
There's basic CPU and
monitor profiling with

00:35:54.100 --> 00:36:05.200
[Transcript missing]

00:36:05.900 --> 00:36:10.580
yet another more single thread
profiling facility called XPROF.

00:36:10.810 --> 00:36:13.600
The interesting thing about that
one is it actually shows you when

00:36:13.600 --> 00:36:15.960
methods get compiled and when
they're interpreted and stuff,

00:36:16.090 --> 00:36:18.820
and so you can use that
for various diagnostics.

00:36:18.820 --> 00:36:25.630
So the details from all of these
are really tedious to learn

00:36:25.850 --> 00:36:31.080
and understand and make use of,
and that's why at the top of the

00:36:31.080 --> 00:36:34.210
slide before I said go buy a tool,
because that integrates it,

00:36:34.240 --> 00:36:36.520
turns it into graphs,
gives you a visual sense

00:36:36.550 --> 00:36:37.440
of what's going on.

00:36:37.460 --> 00:36:39.160
That's really my
recommendation on this one.

00:36:41.710 --> 00:36:43.600
Beyond Jaguar.

00:36:43.750 --> 00:36:45.600
1.4.

00:36:45.940 --> 00:36:50.800
60% more stuff in 1.4
than what was in 1.3.

00:36:50.800 --> 00:36:54.710
There's just piles of stuff.

00:36:56.600 --> 00:37:00.570
The guy who wrote,
writes the Java in a Nutshell series,

00:37:00.570 --> 00:37:06.720
Flanagan, David Flanagan, I believe,
has a list out on the web.

00:37:06.770 --> 00:37:08.580
That's the URL to it.

00:37:08.820 --> 00:37:12.230
It's his top ten features in Java 1.4.

00:37:12.240 --> 00:37:15.840
There's parsing and transforming of XML.

00:37:15.840 --> 00:37:18.120
There's a new preferences API.

00:37:18.120 --> 00:37:19.920
There's logging API.

00:37:19.920 --> 00:37:22.590
There's sockets, secure sockets.

00:37:22.740 --> 00:37:26.440
He had a real thing for linked hash map.

00:37:26.600 --> 00:37:27.440
I don't know why.

00:37:27.440 --> 00:37:30.070
I think it was because
he needed a tenth thing.

00:37:33.440 --> 00:37:34.270
Read the article.

00:37:34.390 --> 00:37:37.540
Memory map files, non-blocking I/O,
regular expressions,

00:37:37.540 --> 00:37:38.700
and language assertions.

00:37:38.880 --> 00:37:41.930
These are all really neat things in 1.4.

00:37:42.000 --> 00:37:45.600
and they're all available in our preview.

00:37:45.600 --> 00:37:51.590
Top 10 things are there
in our preview for

00:38:00.830 --> 00:38:04.200
The preview contains the
Hotspot Client Compiler.

00:38:04.200 --> 00:38:07.420
The Client Compiler is not done yet.

00:38:07.420 --> 00:38:10.740
It doesn't make any
mistakes that we know of,

00:38:10.740 --> 00:38:13.110
but its performance is not...

00:38:13.200 --> 00:38:31.300
[Transcript missing]

00:38:31.450 --> 00:38:34.340
Almost all of those classes,
all the classes you saw on the

00:38:34.340 --> 00:38:37.800
previous slide plus bunches of
others we didn't have room for,

00:38:37.800 --> 00:38:39.540
those are in the classes.

00:38:39.610 --> 00:38:46.390
The UI, the GUI stuff,
the new classes for GUI is not there.

00:38:46.390 --> 00:38:52.720
What we've done instead is taken the
131 GUI classes and repackaged them.

00:38:52.800 --> 00:38:54.110
Now there's a separate copy of them.

00:38:54.240 --> 00:38:56.210
It's not like we're just
shifting the VM underneath.

00:38:56.210 --> 00:38:58.010
So there's a separate
copy of the UI classes.

00:38:58.020 --> 00:39:01.980
The reason this is important, I think,
is that for you to take advantage

00:39:01.980 --> 00:39:05.400
of all those NIO facilities,
to start writing your XML stuff

00:39:05.430 --> 00:39:08.160
and doing all that kinds of stuff,
it would be good if you could

00:39:08.170 --> 00:39:09.820
have an app that basically ran.

00:39:09.880 --> 00:39:12.240
So you could take your
existing 1.3 based app,

00:39:12.240 --> 00:39:19.520
start making use of all those other
APIs that are now available to you,

00:39:19.520 --> 00:39:19.520
and when we

00:39:19.910 --> 00:39:24.760
When we're ready to show
you the 1.4 UI classes,

00:39:24.760 --> 00:39:26.840
then you'll be three steps ahead.

00:39:26.840 --> 00:39:31.080
So that's the idea in terms of
putting together this packaging.

00:39:33.030 --> 00:39:36.030
You get it by going to
developer.apple.com/java.

00:39:36.190 --> 00:39:37.500
There's a download section.

00:39:37.500 --> 00:39:39.620
Look for WWDC downloads.

00:39:39.650 --> 00:39:40.880
And actually I had a question.

00:39:40.960 --> 00:39:42.750
Has anybody actually gone and done this?

00:39:42.750 --> 00:39:46.560
Has anybody installed Jaguar
and gone and put one four up?

00:39:46.560 --> 00:39:48.600
We have one brave soul in the audience.

00:39:48.600 --> 00:39:49.060
Okay.

00:39:49.210 --> 00:39:50.470
You can ask him questions later.

00:39:54.710 --> 00:39:55.880
Use it.

00:39:55.950 --> 00:39:58.840
We have a little,
a trivial little shell script

00:39:59.040 --> 00:40:03.700
that flips that one magic
symlink between 130 and 140.

00:40:08.840 --> 00:40:09.820
Compiler.

00:40:09.880 --> 00:40:11.910
Now this is the slide,
one of the two slides that kind

00:40:11.910 --> 00:40:15.700
of is a preview of what you'll
hear about in more detail later.

00:40:15.710 --> 00:40:20.210
There's a big feature in the 1.4
compiler called deoptimization.

00:40:20.600 --> 00:40:27.870
What this allows is for your compiled and
running code to have really deep inlining

00:40:27.880 --> 00:40:30.860
so that this method calls that method,
calls that method.

00:40:30.860 --> 00:40:35.080
We can just take all of those,
roll them up into one big special

00:40:35.080 --> 00:40:37.520
purpose function and execute it.

00:40:37.520 --> 00:40:41.200
So we don't have to do all those
server-tank call overheads.

00:40:41.220 --> 00:40:46.390
Well, with 1.4 what we do,
in 1.3 we can only do that for final

00:40:46.390 --> 00:40:48.160
methods and for static methods.

00:40:48.160 --> 00:40:50.580
In 1.4 we do it for almost any method.

00:40:50.580 --> 00:40:54.440
And the only problem we would
get into is if somebody loaded,

00:40:54.440 --> 00:40:57.130
dynamically loaded a
jar that had a class,

00:40:57.200 --> 00:41:00.040
subclass of something that we
aggressively inlined and in fact

00:41:00.140 --> 00:41:03.860
that re-implemented one of those
methods such that our compiled

00:41:03.960 --> 00:41:08.540
code isn't going to call out to
that dynamically loaded class.

00:41:08.540 --> 00:41:09.460
So what do we do?

00:41:09.460 --> 00:41:12.360
We've got this code and it's running,
it's compiled,

00:41:12.370 --> 00:41:15.930
and so what we do is we throw it away.

00:41:16.100 --> 00:41:21.380
And we replace the call frames on
the stack with--we call it in to our

00:41:21.430 --> 00:41:25.820
interpreter such that when you fall down
the stack you start implementing the

00:41:25.870 --> 00:41:27.900
interpreter for all of those methods.

00:41:27.900 --> 00:41:31.960
So if you've--you're in
the middle of this inline,

00:41:31.960 --> 00:41:36.140
you know, 60 method,
you'll end up with five frames of

00:41:36.190 --> 00:41:38.800
interpreter on your stack frame.

00:41:38.800 --> 00:41:44.740
So this gives us the ability to inline,
aggressively inline non-final methods.

00:41:44.740 --> 00:41:49.490
If--how many of you saw the
Grand Canyon fly-through demo?

00:41:49.920 --> 00:41:54.050
So there's a part of that where--so
that was based on GL for Java.

00:41:54.240 --> 00:41:59.440
There's a part of that where they'd
flip a switch and it would go to 1.3.

00:41:59.620 --> 00:42:02.400
And the truth of it is,
it was really only using

00:42:02.640 --> 00:42:04.760
the I/O facilities from 1.3.

00:42:05.070 --> 00:42:08.550
Because our compiler could not have
compiled that innermost loop and you

00:42:08.550 --> 00:42:11.800
would not have gotten that speed,
even half the speed that you

00:42:11.800 --> 00:42:13.760
saw there for the 1.3 part.

00:42:13.810 --> 00:42:16.520
So that compilation
part's really important.

00:42:16.520 --> 00:42:19.700
The other thing that it allows
is full speed debugging,

00:42:19.700 --> 00:42:24.730
which means that your code runs and gets
compiled and when you hit a break point,

00:42:24.860 --> 00:42:28.380
only the routines that need the
break point are interpreted.

00:42:28.450 --> 00:42:30.900
And that is a lot faster
than what you have now,

00:42:30.900 --> 00:42:33.550
which is when you do debugging,
you have to do it wholly

00:42:33.550 --> 00:42:34.660
in interpreter mode.

00:42:34.660 --> 00:42:36.300
So we're really excited
about this feature.

00:42:36.300 --> 00:42:42.790
Oh, I wish I could go back.

00:42:42.790 --> 00:42:42.790
Let me try down.

00:42:45.550 --> 00:42:48.620
Second big thing is the low level
intermediate representation.

00:42:48.650 --> 00:42:53.830
Basically inside the compiler there's
another layer that maps better

00:42:53.930 --> 00:42:59.110
to our instruction streams and
so we can map or we can map and do

00:42:59.380 --> 00:43:01.290
better optimizations at the bottom.

00:43:01.480 --> 00:43:05.860
So you'll hear more about
that and why from that at the

00:43:05.860 --> 00:43:09.110
Java performance talk on Friday.

00:43:09.420 --> 00:43:13.020
There's three of those many APIs I want
to talk about a little bit here.

00:43:13.050 --> 00:43:15.270
First one is native buffers.

00:43:15.400 --> 00:43:20.300
What is,
what is native buffers all about?

00:43:22.470 --> 00:43:24.030
You've got memory.

00:43:24.030 --> 00:43:27.910
You fill up memory in the
regular heap with data of some

00:43:28.290 --> 00:43:33.350
kind and to use it in Java,
you have to copy it into the Java heap.

00:43:33.410 --> 00:43:36.760
Once it's in the Java heap,
it might migrate from one generation

00:43:36.800 --> 00:43:38.400
to another so it gets copied yet again.

00:43:38.660 --> 00:43:43.330
This is a lot of copying.

00:43:43.380 --> 00:43:44.470
So native buffers allow
you to represent bits.

00:43:44.780 --> 00:43:47.920
that are allocated out of the
C heap with a Java object.

00:43:48.020 --> 00:43:51.180
The Java object,
it's not a language array.

00:43:51.180 --> 00:43:53.540
It's like an NSArray.

00:43:53.540 --> 00:43:57.740
You have to use methods to get to the
bytes and words and stuff within it.

00:43:57.860 --> 00:44:02.700
But we use a compiler trick such
that whenever you compile codes

00:44:02.780 --> 00:44:07.680
that use those accessor methods,
we compile in the access to it because

00:44:07.680 --> 00:44:11.320
we recognize that magic class such
that you get array-like performance.

00:44:11.980 --> 00:44:15.000
So this is another almost kind of
a language integration feature.

00:44:15.000 --> 00:44:23.840
And that gives us really high-speed
ability to map and do I.O.

00:44:23.840 --> 00:44:25.630
and not waste memory for that.

00:44:25.690 --> 00:44:26.580
The new I.O.

00:44:26.580 --> 00:44:29.000
package makes use of
those native buffers.

00:44:31.550 --> 00:44:33.000
for its IO operations.

00:44:33.000 --> 00:44:38.700
There's also support for IPv6,
support for non-blocking IO.

00:44:38.700 --> 00:44:41.580
We put together an
implementation of preferences.

00:44:41.610 --> 00:44:44.860
And in fact, to show you and talk a
little bit about that stuff,

00:44:44.860 --> 00:44:46.740
I thought we'd have our own little demo.

00:44:46.740 --> 00:44:49.330
So at this time,
I'd like to bring up Greg Parker,

00:44:49.330 --> 00:44:52.420
who actually implemented
most of that stuff.

00:44:52.430 --> 00:44:57.390
And where you saw in the
Grand Canyon demo issues

00:44:57.490 --> 00:45:00.740
about how to do memory mapping

00:45:02.910 --> 00:45:09.080
and the ability to represent those
objects out on the video cards memory.

00:45:09.080 --> 00:45:14.140
What Greg's going to talk about
is network I/O and how we use this

00:45:14.140 --> 00:45:17.480
package to do faster networking.

00:45:17.560 --> 00:45:20.430
Take over.

00:45:27.120 --> 00:45:30.180
All right,
so the first thing I'm going to show you

00:45:30.180 --> 00:45:34.030
is a little bit of that JUs command that
we told you about that allows you to

00:45:34.030 --> 00:45:35.880
switch between the two virtual machines.

00:45:35.880 --> 00:45:37.660
So this is Jaguar.

00:45:37.660 --> 00:45:39.900
This has the developer
preview installed on it.

00:45:40.090 --> 00:45:43.620
And we can just say JUs 131.

00:45:43.620 --> 00:45:49.720
We have to type our password because
this is mucking with stuff in the system,

00:45:49.720 --> 00:45:51.710
but now we have 131 enabled.

00:45:51.750 --> 00:45:54.610
Java-version.java.

00:45:56.540 --> 00:45:58.570
There's 1.3.1.

00:45:58.570 --> 00:46:00.300
I can do the same thing with 1.4.

00:46:00.580 --> 00:46:02.590
J is 1.4.

00:46:02.750 --> 00:46:03.560
Java Dash version.

00:46:03.560 --> 00:46:05.060
There it is.

00:46:07.380 --> 00:46:13.190
So, what I'm going to show you now is a
taste of what you can do with Java 1.4.

00:46:13.190 --> 00:46:16.950
I've written a simple
streaming video server in Java.

00:46:16.950 --> 00:46:19.080
It doesn't really do much.

00:46:19.430 --> 00:46:22.610
It reads GIFs from a file,
reads them to a network cable,

00:46:22.680 --> 00:46:26.980
and then we have a client on the
other side that renders those GIFs.

00:46:26.980 --> 00:46:29.720
So what I'm going to do is run
-- I have two versions of it,

00:46:29.920 --> 00:46:33.980
one of them written in 1.3.1,
one of them written in 1.4.

00:46:34.350 --> 00:46:39.810
So this one is going
to run the 1.4 version.

00:46:39.810 --> 00:46:41.200
There it is up and running.

00:46:41.270 --> 00:46:44.100
And over in this window,
you probably won't be able to see it,

00:46:44.110 --> 00:46:46.470
I'm going to use JUs to
switch over to 1.3.1,

00:46:46.840 --> 00:46:50.580
and I will run the 1.3.1
version of the server.

00:46:51.570 --> 00:46:57.220
with the 131 classes without 131 NIO,
all that kind of stuff.

00:46:57.550 --> 00:47:00.520
So, let's switch over to the client.

00:47:02.860 --> 00:47:04.760
This is the client program here.

00:47:04.760 --> 00:47:06.370
It's an Objective-C program.

00:47:06.490 --> 00:47:12.040
It simply has a thread that reads
the GIFs off the network from the

00:47:12.120 --> 00:47:16.570
Java program on the other machine and
just draws one of those threads to

00:47:16.680 --> 00:47:18.270
the screen to display the animation.

00:47:18.270 --> 00:47:21.470
And the other threads just read
the data and throw it away.

00:47:21.690 --> 00:47:24.910
So we can run a lot of threads,
a lot of server connections,

00:47:24.940 --> 00:47:27.080
and try and stress test the server.

00:47:27.380 --> 00:47:31.100
So let me show you the 131
server here on the left.

00:47:31.120 --> 00:47:36.940
This is simply using the one
thread blocking IO per connection.

00:47:37.040 --> 00:47:41.130
It's using file input streams
to read the GIFs off the disk.

00:47:41.130 --> 00:47:45.080
And it's using simple sockets and
output streams to write the data.

00:47:45.120 --> 00:47:47.440
So the standard Java mechanism.

00:47:47.590 --> 00:47:48.240
So we can run this.

00:47:48.570 --> 00:47:52.780
It runs pretty fast, actually.

00:47:52.780 --> 00:47:53.280
Java 131 is not that slow.

00:47:53.480 --> 00:47:55.620
We can run several connections.

00:47:55.620 --> 00:47:58.220
These background connections
are reading data very fast and

00:47:58.220 --> 00:48:00.630
throwing it away very fast.

00:48:00.930 --> 00:48:04.600
And we can see that on this connection,
this is using a gigabit Ethernet

00:48:04.870 --> 00:48:06.410
network in Apple's hardware.

00:48:06.670 --> 00:48:07.700
Whoops.

00:48:07.720 --> 00:48:09.370
Oh, damn.

00:48:11.360 --> 00:48:12.370
SegFault, cool.

00:48:12.540 --> 00:48:14.180
I haven't seen that before.

00:48:14.200 --> 00:48:15.700
So that's probably an Objective-C fault.

00:48:15.730 --> 00:48:16.900
This is not a Java program.

00:48:21.460 --> 00:48:25.600
So we see it getting about
170 megabits per second.

00:48:25.640 --> 00:48:29.080
This is a 1000 megabit per second wire,
theoretically.

00:48:29.100 --> 00:48:31.230
So that's reasonable performance.

00:48:33.490 --> 00:48:38.510
The 1.4 version is using the
new APIs available in 1.4.

00:48:38.600 --> 00:48:43.900
It's using a single thread to serve all
the connections using non-blocking I/O.

00:48:43.900 --> 00:48:46.240
So that's going to be a lot
less threading overhead.

00:48:46.240 --> 00:48:49.960
We don't have to do
thread switches as often.

00:48:49.990 --> 00:48:54.730
It's using the byte buffers,
in particular memory mapped byte buffers,

00:48:54.730 --> 00:48:56.660
to read in the files.

00:48:56.810 --> 00:49:00.330
So we read in the files once and the
data stays in the C heap so you don't

00:49:00.470 --> 00:49:02.890
have to copy it into Java space.

00:49:02.970 --> 00:49:08.290
Also, all the connections can be
served using that same memory.

00:49:08.300 --> 00:49:10.900
We don't have to copy the
memory for each connection.

00:49:11.140 --> 00:49:15.540
And finally, we're using the NIO socket
channel to actually send the

00:49:15.550 --> 00:49:17.210
data out across the wire.

00:49:17.700 --> 00:49:20.980
That also knows about
the new byte buffer APIs,

00:49:21.010 --> 00:49:26.060
which means that we don't have to
copy the data in Java from when

00:49:26.060 --> 00:49:29.900
we read it from the disk between
the Java various arrays or objects

00:49:29.900 --> 00:49:32.390
and then finally to the network.

00:49:32.400 --> 00:49:33.270
So we can run it.

00:49:33.270 --> 00:49:36.040
It's going to increase straight
from the file into memory

00:49:36.040 --> 00:49:37.560
into the network subsystem.

00:49:37.560 --> 00:49:40.670
So less memory copying,
less memory overhead, no GC overhead for

00:49:40.840 --> 00:49:42.400
these blocks of memory.

00:49:42.400 --> 00:49:43.180
So we can run it.

00:49:43.180 --> 00:49:44.940
One connection is still pretty fast.

00:49:44.940 --> 00:49:47.300
The animation is actually
slowing us down here so it's

00:49:47.300 --> 00:49:49.160
just as fast as the 1.3.1 was.

00:49:49.400 --> 00:49:52.680
When we increase to two
connections or more connections,

00:49:52.680 --> 00:49:54.230
it's running a lot faster.

00:49:54.260 --> 00:49:58.540
If you remember the previous one,
it was about at 160, 170.

00:49:58.610 --> 00:50:01.140
In fact, it stopped there at 170.

00:50:02.030 --> 00:50:07.020
The new version with a single thread
and less memory copying is getting a

00:50:07.020 --> 00:50:12.960
significant speed increase in data rate
that the server is managing to send out.

00:50:12.960 --> 00:50:18.180
So this is maybe 40, 50% faster,
something like that.

00:50:18.180 --> 00:50:20.450
The other thing I want
to show you on the right,

00:50:20.460 --> 00:50:24.700
this is a process listing
on the server machine.

00:50:24.700 --> 00:50:28.640
So this is the 1.4 Java server process.

00:50:28.670 --> 00:50:32.100
It's only one thread,
so it's using 100% CPU on that thread.

00:50:32.100 --> 00:50:36.240
This is a dual processor machine,
so the other thread is busy actually

00:50:36.370 --> 00:50:38.840
in the kernel network setting.

00:50:38.840 --> 00:50:41.420
But it is not using all the CPU.

00:50:41.420 --> 00:50:42.540
You can't see it.

00:50:42.540 --> 00:50:43.940
It's off to the right.

00:50:43.940 --> 00:50:47.490
But that second CPU is
significantly unused right now,

00:50:48.000 --> 00:50:49.320
which is good.

00:50:49.320 --> 00:50:54.090
So we're getting good data performance.

00:50:55.480 --> 00:50:56.510
Objective C is not that bad.

00:50:56.600 --> 00:50:57.330
Objective C is nice.

00:50:57.510 --> 00:50:59.340
Oh, man.

00:50:59.370 --> 00:51:01.380
Objective C is not perfect,

00:51:03.100 --> 00:51:06.360
So it's getting better data
rate with 100% of one CPU,

00:51:06.360 --> 00:51:08.150
the other CPU not fully used.

00:51:08.180 --> 00:51:13.560
If we look at the 1.3.1 version,
it's getting worse data rate like we see.

00:51:13.560 --> 00:51:18.670
Note its CPU usage,
166%. So it's using one

00:51:18.670 --> 00:51:20.760
CPU 100% of the time.

00:51:20.760 --> 00:51:23.470
It's using the other
CPU 70% of that time.

00:51:23.480 --> 00:51:27.830
And that last 30% is actually the
kernel doing the actual network IO.

00:51:28.300 --> 00:51:34.360
So the 1.3.1 version is totally loading
the system and still getting less data

00:51:34.360 --> 00:51:37.180
throughput than the 1.4 version does.

00:51:37.180 --> 00:51:44.210
So these 1.4 APIs allow us to get
better IO rates with less CPU usage.

00:51:44.210 --> 00:51:45.990
That's pretty cool.

00:51:46.130 --> 00:51:47.230
We like that.

00:51:48.960 --> 00:51:50.760
This is available in 1.4.

00:51:50.770 --> 00:51:55.240
This is using the 1.4 developer
preview you have right now.

00:51:55.900 --> 00:51:59.760
We'd love to hear back from
you whether this stuff works.

00:51:59.820 --> 00:52:02.520
It's still under development.

00:52:02.520 --> 00:52:06.200
It works pretty well except
for the Objective-C side,

00:52:06.200 --> 00:52:08.080
but that's not our fault.

00:52:08.080 --> 00:52:11.220
We'd like to get your feedback
on the NIO classes and also

00:52:11.360 --> 00:52:15.440
the other classes in 1.4:
the Preferences API that shipped,

00:52:15.440 --> 00:52:17.580
the IPv6 that's available.

00:52:17.580 --> 00:52:23.500
Oh, by the way, the server can serve IPv6
connections with zero extra code.

00:52:23.530 --> 00:52:27.000
I don't have a demo for that,
but that's pretty nice.

00:52:27.000 --> 00:52:29.220
If you're using any of these things,
we'd like to hear back from you,

00:52:29.220 --> 00:52:33.480
hear your feedback, your bug reports,
how well it works for you so we

00:52:33.480 --> 00:52:41.560
can make sure that our final 1.4
release is as good as we can make it.

00:52:50.100 --> 00:52:54.280
I don't have much more to say.

00:52:54.290 --> 00:53:02.470
Best of breed, that's for you to believe
the assertion or not.

00:53:02.750 --> 00:53:05.610
As you can see,
the system is getting better

00:53:05.610 --> 00:53:09.490
all the time and it's going
to be even better in Jaguar.

00:53:09.950 --> 00:53:15.880
We want you to try out the 1.4 VM.

00:53:15.880 --> 00:53:18.890
There is an issue though about
talking about it on Java dev.

00:53:19.050 --> 00:53:20.920
You're under NDA.

00:53:20.920 --> 00:53:24.870
1.4 is only available to

00:53:25.610 --> 00:53:30.150
to folks who have Jaguar and we
don't really want you talking

00:53:30.240 --> 00:53:32.630
about this stuff on Java Dove.

00:53:34.300 --> 00:53:36.260
but we are interested in
your feedback for that.

00:53:36.300 --> 00:53:39.860
Still to come, obviously,
the GUI for 1.4.

00:53:39.860 --> 00:53:42.630
Even better, shared generation stuff.

00:53:42.700 --> 00:53:45.450
The folks at Sun have got some
interesting ideas on how to

00:53:45.450 --> 00:53:46.600
move the stuff even further.

00:53:46.600 --> 00:53:53.260
We hope to not only help them do it,
but fold it right back in and ship it.

00:53:53.340 --> 00:53:56.100
And compiler tuning, of course.

00:53:56.100 --> 00:54:02.860
As you have seen and will probably see
again and hear about in more detail,

00:54:03.100 --> 00:54:08.600
the Grand Canyon fly-through makes use
of both NIO as well as that greater

00:54:08.600 --> 00:54:13.580
compiler technology that is just
beginning to come through for us.

00:54:13.600 --> 00:54:18.320
Who do... Oh, yeah,
where do we go from here?

00:54:18.320 --> 00:54:21.350
Well, we actually have our feedback
forum right after this.

00:54:21.360 --> 00:54:23.200
Most of our Java talks are done.

00:54:23.200 --> 00:54:25.900
We only have our
Java performance talk left,

00:54:26.130 --> 00:54:30.240
so come give us some feedback
right after this over in room J,

00:54:30.340 --> 00:54:31.120
isn't it?

00:54:31.200 --> 00:54:31.530
J1.

00:54:31.900 --> 00:54:35.140
And then the Java performance
talk on Friday at 9 a.m.

00:54:35.230 --> 00:54:37.820
I know it's a little early,
but make it your one,

00:54:37.820 --> 00:54:40.420
your first and best talk on Friday.

00:54:42.270 --> 00:54:43.670
who to contact.

00:54:43.990 --> 00:54:45.200
That's where you can reach me.

00:54:45.200 --> 00:54:47.080
That's where you can reach Greg.

00:54:47.100 --> 00:54:50.060
Alan Samuel is our
Java Technologies evangelist.

00:54:50.060 --> 00:54:54.500
I'm going to bring him up
and let him lead some Q&A,

00:54:54.500 --> 00:54:55.020
I think.