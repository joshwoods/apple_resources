WEBVTT

00:00:02.070 --> 00:00:03.120
Good morning.

00:00:03.260 --> 00:00:05.240
My name is Mark Tozer-Vilches.

00:00:05.330 --> 00:00:09.900
I'm the hardware technology
manager for developer relations.

00:00:09.990 --> 00:00:13.200
This morning is session 102,
Performance Optimization

00:00:13.200 --> 00:00:15.260
with Velocity Engine.

00:00:15.520 --> 00:00:19.390
This session, for those of you,
if you ever find yourself doodling,

00:00:19.510 --> 00:00:22.700
doing square roots, factoring polynomials
while you're on the phone,

00:00:22.700 --> 00:00:25.000
this is the session for you.

00:00:25.000 --> 00:00:28.360
So quite a way to start the
morning with some cool math.

00:00:28.360 --> 00:00:31.130
We've got some great content,
some cool demos,

00:00:31.210 --> 00:00:32.920
and I'd like to bring up Dr.

00:00:32.920 --> 00:00:37.290
Ali Sazegari, the head of the Vector
and Numerics group.

00:00:42.850 --> 00:00:45.520
Hi, I'm Ali Sazegari,
and welcome to our session.

00:00:45.520 --> 00:00:51.400
Today, we're going to talk about a
couple of different things.

00:00:51.400 --> 00:00:55.430
The first topic that I'm going
to talk about is what we've done

00:00:55.430 --> 00:00:57.770
for Jaguar on the numerics side.

00:00:57.780 --> 00:01:03.390
Then, we're going to dive into the
benefits of using the vector units,

00:01:03.500 --> 00:01:08.660
either by itself or using our
libraries that we have in Jaguar.

00:01:09.220 --> 00:01:13.460
And in the last section of our talk,
we are going to show you how to use

00:01:13.460 --> 00:01:18.560
the new performance tools that are
absolutely crucial to your success.

00:01:18.640 --> 00:01:23.430
So, let's get started with this,
if I can figure out what to do.

00:01:27.930 --> 00:01:32.240
All right,
so numerics libraries in Jaguar.

00:01:32.240 --> 00:01:40.460
You know that our numerics library,
the LibM, that you've been using up to

00:01:40.460 --> 00:01:45.200
10.1.3 was based on a freely
distributable library from Sun,

00:01:45.200 --> 00:01:45.940
FD LibM.

00:01:45.940 --> 00:01:48.400
We changed that completely,
and we're going to go through that.

00:01:48.440 --> 00:01:53.160
Then we're going to go talk about
the benefits of vectorization.

00:01:53.160 --> 00:01:55.460
And if you don't want
to vectorize your code,

00:01:55.460 --> 00:01:58.320
vector libraries in Jaguar
are going to be very helpful.

00:01:58.320 --> 00:02:04.900
Then some optimization techniques,
details of what to do about your code,

00:02:04.900 --> 00:02:07.480
how to make it a little bit faster.

00:02:07.480 --> 00:02:11.670
Hopefully you're going to
use those in your code.

00:02:11.800 --> 00:02:12.700
And then profiling tools.

00:02:18.100 --> 00:02:19.390
Okay, what you will learn.

00:02:19.450 --> 00:02:20.860
Available libraries.

00:02:20.860 --> 00:02:26.850
We have expanded the available libraries
on the Jaguar as we go along in Mac OS,

00:02:26.960 --> 00:02:30.020
then we'll have a richer
and richer set of libraries.

00:02:30.020 --> 00:02:32.210
Some optimization techniques.

00:02:32.210 --> 00:02:36.000
The data injection method that
we're going to talk about.

00:02:36.100 --> 00:02:38.020
Vector versus scalar algorithms.

00:02:38.360 --> 00:02:43.010
We are going to draw your attention to
be aware of the LSU bottleneck on our

00:02:43.010 --> 00:02:48.330
machine and using the instruction set
architecture details of the processor

00:02:48.430 --> 00:02:53.520
if you really want to squeeze a lot
of performance out of this machine.

00:02:53.540 --> 00:02:59.410
The whole idea here is that the
increased parallelism is really key

00:02:59.410 --> 00:03:02.680
for better throughput on PowerPC.

00:03:02.810 --> 00:03:06.170
And again, at the end,
we're going to talk

00:03:06.170 --> 00:03:08.280
about profiling tools.

00:03:08.360 --> 00:03:09.360
Thanks.

00:03:11.910 --> 00:03:12.820
What's new in Jaguar?

00:03:12.820 --> 00:03:17.330
On the numerics front,
we have revamped LibM.

00:03:17.330 --> 00:03:19.780
It is now based on MathLib V5.

00:03:19.780 --> 00:03:23.740
For those of you who come
from the OS 9 platform,

00:03:23.740 --> 00:03:26.840
you are familiar with MathLib V3.

00:03:26.840 --> 00:03:30.100
We went to MathLib V4,
and I'm going to talk about

00:03:30.190 --> 00:03:31.820
that a little bit later.

00:03:31.930 --> 00:03:33.740
And now, MathLib V5.

00:03:33.740 --> 00:03:39.180
We also have brought FixMath
up to snuff again on Jaguar.

00:03:39.670 --> 00:03:43.100
On the vector domain, we have an expanded
signal processing library.

00:03:43.100 --> 00:03:49.360
We also have a full standard
basic linear algebra subroutines,

00:03:49.360 --> 00:03:53.400
which you can build your
algorithms on top of.

00:03:53.480 --> 00:03:56.860
And then we're going to show
you some vectorized technologies

00:03:56.930 --> 00:03:58.900
that are going to come to Jaguar.

00:04:02.750 --> 00:04:03.840
New Libem in Jaguar.

00:04:04.200 --> 00:04:06.600
Mathlib V4 shipped in 10.1.3.

00:04:06.600 --> 00:04:10.970
We had a pretty bad performance
before that because of the

00:04:10.970 --> 00:04:12.130
FD Libem that was used.

00:04:12.180 --> 00:04:17.930
We switched that completely and we
went to what was the core of the OS9.

00:04:17.930 --> 00:04:19.440
We ported that.

00:04:19.440 --> 00:04:25.970
So 10.1.3 and beyond basically
has parity with OS9's performance.

00:04:26.060 --> 00:04:28.420
Coming in Jaguar is Mathlib V5.

00:04:28.420 --> 00:04:34.480
Mathlib V5 is faster than V3,
which is in Mac OS 9.

00:04:34.480 --> 00:04:39.770
And the important part to remember about
V5 is that it is completely IEEE 754

00:04:40.150 --> 00:04:42.380
and C99 compliant on double precision.

00:04:42.380 --> 00:04:48.750
The implementations which are available
are on PowerPC and IA32 for Darwin users.

00:04:48.760 --> 00:04:55.760
This is a no-holds-bar, high-performance,
extremely accurate, and very robust.

00:04:56.210 --> 00:05:00.100
mathematical library that
we're pretty proud of.

00:05:02.560 --> 00:05:03.840
New fixed math in Jaguar.

00:05:03.840 --> 00:05:06.200
For those of you who
continue to use fixed math,

00:05:06.340 --> 00:05:11.160
we have ported what we had
initially done for OS 9.

00:05:11.160 --> 00:05:15.460
Now in Jaguar,
fixed math is going to be floating point

00:05:15.470 --> 00:05:18.380
based as it used to be in Mac OS 9.

00:05:18.380 --> 00:05:23.220
I would like to emphasize that even
though fixed math is extremely fast now,

00:05:23.220 --> 00:05:26.710
even a little bit faster
than what OS 9 was,

00:05:27.000 --> 00:05:30.000
I highly recommend that
you don't use fixed math,

00:05:30.000 --> 00:05:34.000
that you migrate your code to using
floating point because floating

00:05:34.000 --> 00:05:37.250
point on PowerPC is significantly
faster than what we do in here.

00:05:40.380 --> 00:05:43.420
Okay,
now it's a good time to talk about some

00:05:43.420 --> 00:05:46.540
general guidelines for vectorization.

00:05:46.540 --> 00:05:51.600
What we do is, basically,
this page here describes what

00:05:51.760 --> 00:05:55.340
we do every day at Apple for
everything we just come across.

00:05:55.420 --> 00:06:01.680
And my first and most important
suggestion here is to profile your code,

00:06:02.300 --> 00:06:05.070
profile and profile,
and then when you're done,

00:06:05.230 --> 00:06:05.960
profile some more.

00:06:06.530 --> 00:06:09.390
Because you really want to
know where your bottlenecks are

00:06:09.490 --> 00:06:11.070
and then move on from there.

00:06:11.080 --> 00:06:12.940
Use Chut2Profile.

00:06:12.940 --> 00:06:16.220
Chut is the computer hardware
understanding development tools

00:06:16.220 --> 00:06:19.460
that we're going to talk in
details later on in this session.

00:06:19.460 --> 00:06:24.360
It will show you, it has a lot of tools,
it will show you where the

00:06:24.770 --> 00:06:29.960
bottlenecks are and where you
should go and make your code fly.

00:06:29.960 --> 00:06:32.520
A lot of times,
developers may think that a

00:06:32.580 --> 00:06:35.800
particular routine is really
the cause of the problem.

00:06:35.940 --> 00:06:38.660
And after working on it for several
days and making it a lot faster,

00:06:38.660 --> 00:06:42.830
they find out that they really
haven't budged the performance at all.

00:06:42.970 --> 00:06:44.520
And this has happened to us before.

00:06:44.520 --> 00:06:48.090
Code has been brought to us when
they thought that this was the

00:06:48.090 --> 00:06:50.580
bottleneck and after working
on it for about a week or two,

00:06:50.580 --> 00:06:51.420
we've given them back.

00:06:51.500 --> 00:06:54.610
And what we've done is that
we've basically taken 2 or 3% of

00:06:54.650 --> 00:06:58.060
the entire processing time down
to 1% or maybe half a percent.

00:06:58.060 --> 00:07:01.130
But the overall application
is not that much faster.

00:07:01.260 --> 00:07:03.580
So Chut is extremely
important in that regard.

00:07:03.600 --> 00:07:05.920
The next thing is to rewrite.

00:07:05.920 --> 00:07:08.190
So you rewrite the scalar
using data parallelism,

00:07:08.190 --> 00:07:09.760
which I will go over in detail of that.

00:07:10.030 --> 00:07:13.480
Then after you've done that
and after you've found out

00:07:13.610 --> 00:07:16.520
where your bottlenecks are,
you should rewrite

00:07:16.720 --> 00:07:18.340
select parts in vector.

00:07:18.500 --> 00:07:22.460
And then use the vector libraries
that we have as much as possible.

00:07:22.700 --> 00:07:27.310
In our game,
basically you need to take charge of what

00:07:27.310 --> 00:07:30.870
the compiler is doing as much as you can.

00:07:31.250 --> 00:07:35.580
And one of the things to worry about
is that because we have a fairly...

00:07:35.580 --> 00:07:39.340
We have a large set of registers,
a very rich set of registers,

00:07:39.340 --> 00:07:41.910
and we want to make sure that
we use that all the time,

00:07:41.940 --> 00:07:45.900
you need to be aware of the register
spillage inside your compiler and

00:07:46.280 --> 00:07:51.100
see if this is not working out
and do something else instead.

00:07:52.840 --> 00:07:54.860
At this time, what I'd like to do is
to go over an example.

00:07:54.860 --> 00:07:58.900
This example is the IMDCT for MP3.

00:07:58.900 --> 00:08:01.740
For some of you who are not
familiar with this particular thing,

00:08:01.740 --> 00:08:06.600
this is a transformation not unlike FFTs,
the Inverse Modified

00:08:06.700 --> 00:08:08.160
Discrete Cosine Transform.

00:08:08.160 --> 00:08:13.240
And this chart here shows what
happens to this particular piece of

00:08:13.240 --> 00:08:16.180
code as we go along and optimize it.

00:08:17.300 --> 00:08:21.620
I'd like to mention that the results
of all these are exactly the same.

00:08:21.620 --> 00:08:23.760
We have not changed the results
of any of this stuff here.

00:08:23.760 --> 00:08:25.550
There's no shortcut in the results.

00:08:25.590 --> 00:08:29.370
The results are still
correct towards the bottom.

00:08:29.400 --> 00:08:34.080
The first IMDCT code has, of course,
this guy calls cosines.

00:08:34.350 --> 00:08:39.840
And if you put the cosines in a loop,
it has a loop, and you call our libm,

00:08:40.100 --> 00:08:47.280
which is highly accurate,
53 bits of significant, you'll get 583.

00:08:47.280 --> 00:08:51.390
And this is units of time on any machine,
let's say.

00:08:51.430 --> 00:08:53.610
The smaller numbers in
this chart are better.

00:08:53.640 --> 00:08:56.800
Just taking this cosine out
and making it a table lookup,

00:08:56.800 --> 00:09:00.420
which becomes the original ISO code,
so if you paid your money and

00:09:00.420 --> 00:09:03.540
you got your original ISO manual
and you just typed it in,

00:09:03.790 --> 00:09:05.090
you get 20.1.

00:09:05.140 --> 00:09:10.180
So the moral of the story in here is
that don't call libm 5 million times

00:09:10.180 --> 00:09:14.380
inside of a loop if you can help it,
especially if you're not

00:09:14.380 --> 00:09:17.140
interested in very high accuracy.

00:09:17.300 --> 00:09:17.300
So the moral of the story in here is
that don't call libm 5 million times

00:09:17.300 --> 00:09:17.310
inside of a loop if you can help it,
especially if you're not

00:09:17.310 --> 00:09:17.440
interested in very high accuracy.

00:09:17.550 --> 00:09:21.570
The next thing is an algorithmic
optimization in here.

00:09:21.580 --> 00:09:25.200
And we have not moved into
the vector domain right now.

00:09:25.200 --> 00:09:27.750
We're still sitting on
top of the scalar domain.

00:09:27.760 --> 00:09:32.330
The Lie optimization is a technique
which will bring the size of the

00:09:32.330 --> 00:09:36.280
2048 cosine table lookup down to 384.

00:09:36.280 --> 00:09:39.890
It turns out that table lookup is fast,
but it's not as fast as doing

00:09:39.890 --> 00:09:42.050
actual computation on the machine.

00:09:42.060 --> 00:09:47.450
And bringing the size of the
table lookup will have... an

00:09:47.450 --> 00:09:49.840
improvement over the performance.

00:09:49.940 --> 00:09:54.690
After the Lie optimization is done,
we found a way, this is actually a

00:09:54.690 --> 00:10:00.640
standard way of doing this,
of making our IMDCT call our FFT.

00:10:00.640 --> 00:10:03.860
We make IMDCT based on the FFT.

00:10:03.860 --> 00:10:07.120
This will bring the cosine
table even further down.

00:10:07.120 --> 00:10:10.420
And now the scalar code
is running at 1.23.

00:10:10.420 --> 00:10:13.420
1.23 units of time,
when we started with 583,

00:10:13.420 --> 00:10:17.280
or if you just look at
the original ISO code,

00:10:17.300 --> 00:10:19.520
it's at 20.1.

00:10:19.700 --> 00:10:22.310
So it's a fair amount
of improvement already.

00:10:22.320 --> 00:10:26.180
And going from the scalar,
which we're still sitting

00:10:26.280 --> 00:10:29.260
on the scalar domain here,
to the vector domain,

00:10:29.260 --> 00:10:32.800
then we are going to almost
three times performance just

00:10:33.170 --> 00:10:35.000
moving from scalar to vector.

00:10:35.000 --> 00:10:39.160
The moral of the story in here is that
you should not be calling libraries

00:10:39.200 --> 00:10:40.880
and site loops if you don't have to.

00:10:40.880 --> 00:10:45.040
You should be minimizing your
table lookups if that's possible.

00:10:45.350 --> 00:10:48.680
And the second thing is that work on your
scalar code before you move on to vector

00:10:48.800 --> 00:10:54.160
to make sure where your bottlenecks are
and how to squeeze the performance out of

00:10:54.160 --> 00:10:56.620
it and then move on to the vector code.

00:11:00.890 --> 00:11:02.060
What are the vectorization benefits?

00:11:02.140 --> 00:11:05.770
And this is true for all platforms,
including ours.

00:11:05.990 --> 00:11:10.240
On ours, the PowerPC vector unit
has an orthogonal design.

00:11:10.240 --> 00:11:12.660
It really is not a second-class
citizen to anybody.

00:11:12.660 --> 00:11:19.120
As opposed to the design of the MMX,
which sits on top of the floating-point

00:11:19.120 --> 00:11:24.410
register file and has a granularity
which is larger than one instruction.

00:11:24.520 --> 00:11:27.800
So in order to amortize the
use of the vector engine,

00:11:27.800 --> 00:11:31.540
you have to use a few of the
instructions before you move on

00:11:31.550 --> 00:11:33.070
so that you'll see the benefits.

00:11:33.120 --> 00:11:36.350
You can actually use this in
conjunction with anything else.

00:11:36.460 --> 00:11:37.050
There is no penalty.

00:11:37.150 --> 00:11:39.490
There is no overhead in
using the vector engine.

00:11:39.500 --> 00:11:43.630
The engine is completely pipeline,
single-cycle instruction execution,

00:11:43.710 --> 00:11:47.460
unlike some other parts of the PowerPC,
which I will go through.

00:11:47.460 --> 00:11:51.310
So this is extremely important because
if you want to have a lot of performance,

00:11:51.310 --> 00:11:53.770
you do not want to have
stalls in the pipeline.

00:11:54.660 --> 00:11:58.800
The other benefits of the
vectorization is some of the

00:11:58.910 --> 00:12:01.960
libraries that we offer in Jaguar.

00:12:02.080 --> 00:12:05.440
The first one is the digital
signal processing library,

00:12:05.440 --> 00:12:07.450
which we have enhanced quite a bit.

00:12:07.520 --> 00:12:09.910
The BLOS, which is a standard BLOS.

00:12:10.140 --> 00:12:11.920
I will go into detail of that.

00:12:12.080 --> 00:12:15.350
The basic operations,
and that is the add, subtract, multiply,

00:12:15.420 --> 00:12:18.200
divide for anything which does
not exist on the processor.

00:12:18.200 --> 00:12:24.200
So that you can use, let's say,
32 bits multiplies.

00:12:24.320 --> 00:12:26.530
We don't have 32 bit
multipliers on the AlteVec.

00:12:26.610 --> 00:12:30.800
And Mathlib,
which is a counterpart of the regular

00:12:30.800 --> 00:12:36.350
library that we have on the scalar
domain that you access with LibM.

00:12:36.360 --> 00:12:38.510
You can access that with VMathlib.

00:12:38.660 --> 00:12:43.320
We promise you better results and better
performance on VMathlib as we go on,

00:12:43.320 --> 00:12:46.020
and Jaguar will see some of that benefit.

00:12:46.090 --> 00:12:48.990
At this time,
what I would like to do is show you

00:12:48.990 --> 00:12:53.520
what would happen if you actually
took your code and vectorized it.

00:12:54.120 --> 00:12:57.060
And spent some time going over it,
profiled it,

00:12:57.060 --> 00:12:59.730
and found out where the bottlenecks are.

00:12:59.920 --> 00:13:03.760
And what I would like to do is introduce
my friend Ralph Bruner of Core Graphics.

00:13:03.900 --> 00:13:08.130
And he is going to show us
vectorization work that we have

00:13:08.130 --> 00:13:10.460
done for portions of cores.

00:13:17.100 --> 00:13:18.610
I want to use that microphone there.

00:13:18.640 --> 00:13:23.430
So what do we have here?

00:13:28.300 --> 00:13:42.270
The higher the speedometer,
the better frame rate,

00:13:42.340 --> 00:13:44.320
and this is what we want,
just like the speed, okay?

00:13:44.320 --> 00:13:45.570
Okay.

00:13:51.720 --> 00:14:01.590
I get a frame rate of
about 20 frames per second.

00:14:01.600 --> 00:14:07.310
Go and turn on all the vector code
we have in Quartz and do the same.

00:14:07.310 --> 00:14:07.310
You get about 30 frames per second.

00:14:08.810 --> 00:14:09.320
You get the idea.

00:14:09.320 --> 00:14:13.860
Okay, so this is the Quartz compositor
that has been vectorized.

00:14:13.860 --> 00:14:16.950
So the Quartz compositor is the
piece of code that takes all the

00:14:16.950 --> 00:14:22.120
Windows content and mixes together
to present the final on-screen result

00:14:22.120 --> 00:14:25.600
and does all the translucency and
genie effects and stuff like that.

00:14:25.600 --> 00:14:27.250
So this is part of Jaguar, right?

00:14:27.260 --> 00:14:28.300
This is part of Jaguar.

00:14:28.420 --> 00:14:31.510
So if you bought a G4 and
you put a Jaguar on it,

00:14:31.510 --> 00:14:35.690
you'll see the benefits right
away on at least resizing.

00:14:37.510 --> 00:14:41.430
So, the other little demo
I have is life resizing.

00:14:41.480 --> 00:14:44.430
We identified that as
one of the performance

00:14:44.430 --> 00:14:48.500
bottlenecks in using Mac OS X,
so we worked on that a bit.

00:14:48.520 --> 00:14:53.620
And first thing I'm going to show
again is life resizing a window.

00:14:54.100 --> 00:15:01.500
[Transcript missing]

00:15:02.600 --> 00:15:07.490
Okay, and here what has been vectorized
is the compositing part,

00:15:07.490 --> 00:15:10.990
like in the translucent
terminal demo before,

00:15:11.000 --> 00:15:15.790
but also pattern fields like the
pinstripes and the scrollbars,

00:15:15.790 --> 00:15:19.900
text drawing, shadow generation,
a set of about eight

00:15:19.900 --> 00:15:22.660
functions that we vectorized.

00:15:22.660 --> 00:15:27.010
And each of these functions measured
by themselves gave us something like

00:15:27.150 --> 00:15:33.380
three to five X performance improvement,
and the overall performance

00:15:33.380 --> 00:15:35.140
improvement is then quite noticeable.

00:15:35.140 --> 00:15:37.620
Okay.

00:15:41.490 --> 00:15:46.200
The third demo I have
is a bit of an oddity.

00:15:46.290 --> 00:15:51.170
So I have an image here which
is stored in an unusual format.

00:15:51.240 --> 00:15:54.400
It doesn't have red, green,
and blue at every pixel.

00:15:54.440 --> 00:15:57.160
It has a polynomial for red, green,
and blue for every pixel.

00:15:57.160 --> 00:16:04.280
And so you can actually evaluate the
entire image at different points.

00:16:04.280 --> 00:16:07.700
And what that means here,
we can set the focus

00:16:07.700 --> 00:16:07.700
distance in that image.

00:16:08.190 --> 00:16:12.460
So this is an example for an algorithm
that you probably wouldn't even

00:16:12.460 --> 00:16:14.880
try if you hadn't have a SIMD unit.

00:16:14.960 --> 00:16:17.420
So how many polynomials
are you evaluating here?

00:16:17.510 --> 00:16:20.300
So that's 1.3 million
polynomials for a single frame.

00:16:20.300 --> 00:16:25.730
1.3 million polynomials
for a single frame.

00:16:25.730 --> 00:16:28.250
Don't try this at home.

00:16:28.250 --> 00:16:31.010
And the data that's backing is
just the coefficients for these

00:16:31.010 --> 00:16:31.010
polynomials is 8 megabytes.

00:16:31.340 --> 00:16:37.440
So, let me just wiggle this around a bit
to get the performance numbers here.

00:16:38.100 --> 00:16:45.390
- And we see we run about
15 to 18 frames per second.

00:16:45.390 --> 00:16:45.390
And just for kicks,
I implemented a scalar version of that.

00:16:45.800 --> 00:16:46.830
Which is rather sad.

00:16:46.840 --> 00:16:49.490
It's like two frames per second.

00:16:49.560 --> 00:16:50.120
Okay.

00:16:53.210 --> 00:16:54.590
So that's it for the demos.

00:16:54.600 --> 00:16:56.080
All right.

00:16:56.300 --> 00:16:56.770
Thank you, Ralph.

00:16:56.990 --> 00:16:58.390
So as you see...

00:17:04.300 --> 00:17:04.840
Thank you very much.

00:17:04.860 --> 00:17:07.530
So as you see, just spending some time
vectorizing your code,

00:17:07.530 --> 00:17:10.310
and this is not calling our libraries,
just finding out where the

00:17:10.310 --> 00:17:14.320
bottlenecks are and spending
time and vectorizing will have a

00:17:14.390 --> 00:17:16.850
huge effect on the performance.

00:17:16.860 --> 00:17:21.500
Of course, Quartz is an extremely
important component of Mac OS X,

00:17:21.520 --> 00:17:26.830
and everything you saw in here besides
the 1.3 million polynomials per frame

00:17:27.240 --> 00:17:31.430
is going to be included in Jaguar,
and Jaguar will see the benefits of that.

00:17:35.320 --> 00:17:37.730
The next thing I would
like to show you again is,

00:17:37.730 --> 00:17:40.110
again,
some more benefits of vectorization,

00:17:40.210 --> 00:17:42.960
what happens if we vectorize
some other portions,

00:17:42.960 --> 00:17:46.010
and I would like to ask
Robert Murley to come up.

00:17:46.320 --> 00:17:52.840
We have vectorized the RGB to
RGB color transformation for ColorSync,

00:17:52.900 --> 00:17:59.720
and Bob is going to show us what happens
with the vectorization of that component.

00:18:05.900 --> 00:18:18.500
[Transcript missing]

00:18:19.410 --> 00:18:26.240
One on your right is running
without any vectorization.

00:18:26.310 --> 00:18:29.920
The vectorization you will
see is the one on your left.

00:18:29.920 --> 00:18:34.230
The part that has been
vectorized is ColorSync,

00:18:34.250 --> 00:18:39.580
the color correction software
that is built into the map.

00:18:39.580 --> 00:18:44.860
So I wanted to show you the
speed difference between scalar

00:18:44.860 --> 00:18:47.580
and vector for ColorSync.

00:18:55.120 --> 00:18:58.940
Looks like as we can see the... Oops,
that wasn't fair.

00:18:58.940 --> 00:19:00.240
All right.

00:19:00.270 --> 00:19:02.760
He really handicapped
the scalar one this time.

00:19:02.760 --> 00:19:03.490
Let me try that again.

00:19:03.500 --> 00:19:03.880
Okay.

00:19:07.270 --> 00:19:10.880
The scaler is slow, but not that slow.

00:19:10.960 --> 00:19:11.270
All right.

00:19:11.350 --> 00:19:17.640
Is that clicked here?

00:19:17.640 --> 00:19:24.440
All right.

00:19:24.440 --> 00:19:26.100
OK, here we go.

00:19:26.100 --> 00:19:26.870
I got it this time.

00:19:26.960 --> 00:19:27.290
All right.

00:19:27.460 --> 00:19:28.820
The gods of demos are with us again.

00:19:42.700 --> 00:19:49.610
We can see that the vector one is faster,
and the vector was

00:19:49.610 --> 00:19:52.620
done before the scaler,
so we can take it back up also.

00:19:52.620 --> 00:19:56.820
I just wanted to mention also that
about roughly 60% of the processing

00:19:56.820 --> 00:20:00.790
going on here is color sync,
and the rest is other things.

00:20:00.800 --> 00:20:04.180
The color sync vector is,
if you saw it by itself,

00:20:04.180 --> 00:20:07.920
it's about 2.2 times
faster than the scaler.

00:20:10.260 --> 00:20:15.740
And that is an overall speed improvement
of the ColorSync scalar code,

00:20:15.890 --> 00:20:16.530
right?

00:20:16.540 --> 00:20:17.780
Correct.

00:20:17.780 --> 00:20:18.480
Okay.

00:20:18.480 --> 00:20:21.550
All right.

00:20:21.560 --> 00:20:22.860
Thank you, Bob.

00:20:23.050 --> 00:20:24.020
Again, this is...

00:20:29.000 --> 00:20:31.600
So again,
this is a major component of Mac OS X,

00:20:31.600 --> 00:20:33.410
and Jaguar will have this.

00:20:33.410 --> 00:20:37.880
And if you have a G4,
you will get a better

00:20:37.920 --> 00:20:40.940
RGB to RGB transformation.

00:20:45.750 --> 00:20:51.490
So let's talk in detail about
some of the libraries that

00:20:51.490 --> 00:20:54.590
we have for the new Jaguar.

00:20:54.600 --> 00:20:57.480
The new one,
which is quite dear to my heart,

00:20:57.630 --> 00:20:59.600
is the signal processing one.

00:20:59.600 --> 00:21:01.840
The DSP, VDSP,
we've had this for a while.

00:21:01.840 --> 00:21:08.180
It was a single-precision-based,
very fast signal processing library.

00:21:08.180 --> 00:21:12.410
And what we've done for Jaguar,
we've added double-precision

00:21:12.490 --> 00:21:13.660
DSP functions.

00:21:14.460 --> 00:21:19.770
And we've also added base 3 and
base 5 FFTs because there was some

00:21:19.770 --> 00:21:21.880
demand for base 3 and base 5s.

00:21:22.560 --> 00:21:26.940
These are very high performance
and excellent single-complex

00:21:27.400 --> 00:21:29.140
on 1D and 2D FFTs.

00:21:29.310 --> 00:21:32.160
For those of you who
are using FFTs in here,

00:21:32.700 --> 00:21:35.600
these numbers should be fairly familiar.

00:21:35.600 --> 00:21:41.830
The 1024 complex FFT takes only
9.8 microseconds on a 1GHz PowerPC.

00:21:41.950 --> 00:21:43.200
That's only on one processor.

00:21:43.220 --> 00:21:46.630
And the real one takes 5.2 microseconds.

00:21:46.640 --> 00:21:52.290
I'd like to tell you that on PowerPC,
we were the first ones

00:21:52.290 --> 00:21:56.330
to make available FFTs,
which were getting processed

00:21:56.420 --> 00:21:59.580
at 1024 complex FFTs,
which were processed under

00:21:59.580 --> 00:22:02.230
20 microseconds and now
under 10 microseconds.

00:22:02.240 --> 00:22:09.500
So going over some of
the details of the DSP,

00:22:09.500 --> 00:22:12.720
I'd like to show you the
performance and compare that to,

00:22:13.220 --> 00:22:18.260
what used to be the highest
clocked Pentium machine.

00:22:18.620 --> 00:22:24.650
What we have here is various
nodes of the FFT from 256 to 4096.

00:22:25.150 --> 00:22:29.970
The blue one is our VDSP library
and the yellow one is the IPP.

00:22:29.980 --> 00:22:33.900
IPP is the Intel's Integrated
Performance Primitive Library,

00:22:34.100 --> 00:22:35.740
that many of you may be familiar with.

00:22:35.740 --> 00:22:39.450
And we are calling it, these are in-cache
transformations that we are doing.

00:22:39.660 --> 00:22:47.120
And as you can see, the 1GHz G4,
is compared fairly favorably

00:22:47.460 --> 00:22:51.050
with the 2.2GHz P4.

00:22:51.070 --> 00:22:55.440
If you go to a 2.4GHz P4,
then you'll get a little

00:22:55.440 --> 00:22:59.740
bit better performance,
about 4 or 5%, but the numbers

00:22:59.920 --> 00:23:02.060
look pretty much the same.

00:23:02.060 --> 00:23:04.400
We're still faster than a 1GHz machine.

00:23:04.500 --> 00:23:06.580
Here is the real one, the FFT.

00:23:06.770 --> 00:23:11.940
We have a slightly better algorithm
and we do better than the IPP,

00:23:11.940 --> 00:23:16.030
which is, which is interesting.

00:23:16.180 --> 00:23:19.140
I'd like to follow this up for
the image processing folks.

00:23:19.140 --> 00:23:22.900
We have a 2D FFT, also the same thing.

00:23:22.900 --> 00:23:27.140
The 2D FFT is a fairly tough
guy to implement and to

00:23:27.140 --> 00:23:28.900
get right and make it fast.

00:23:28.950 --> 00:23:35.900
But again, the VDSP is doing a better job
than the Intel's primitives.

00:23:35.900 --> 00:23:39.140
I'd like to follow this
up with the complex one.

00:23:39.140 --> 00:23:43.100
And again, the complex also is faster.

00:23:43.100 --> 00:23:45.120
We only have four of
these graphs in here,

00:23:45.120 --> 00:23:46.700
so I don't want to bore you,
but I want to tell you that the signal

00:23:46.700 --> 00:23:50.100
processing library that we have in
Jaguar is really second to none.

00:23:50.140 --> 00:23:53.060
It is extremely high performance
on the vector engine.

00:23:53.130 --> 00:23:56.100
It's also very high performance
on the floating point unit now,

00:23:56.170 --> 00:23:58.790
if you wanted to choose the
double precision instead

00:23:58.790 --> 00:24:01.100
of the single precision.

00:24:01.100 --> 00:24:03.450
Typically,
the difference between the vector

00:24:03.450 --> 00:24:08.010
performance and the scalar performance
right now is anywhere from 4 to 5 times.

00:24:08.210 --> 00:24:12.100
So you will go,
if you use the VDSP on the vector engine,

00:24:12.100 --> 00:24:13.060
you'll get 4 to 5 times.

00:24:13.080 --> 00:24:17.060
So you'll get 4 times better performance
than you'd use it on the scalar engine,

00:24:17.060 --> 00:24:19.580
and the scalar engine
is already hand tuned.

00:24:20.970 --> 00:24:23.780
What I would like to do here
is now we've talked about the

00:24:23.780 --> 00:24:27.580
benefits of vectorization if
you just went and vectorized.

00:24:27.580 --> 00:24:32.460
And what I would like to now show is
what happens if you use our libraries

00:24:32.620 --> 00:24:35.040
like the FFTs and other things.

00:24:35.120 --> 00:24:41.760
I'd like to introduce my friend here,
Bill Kincaid, who is going to show what

00:24:41.760 --> 00:24:44.700
iTunes does with our library.

00:24:47.700 --> 00:24:49.490
All right,
so all those other demos are cool,

00:24:49.490 --> 00:24:51.340
but we all know what
really matters is MP3.

00:24:51.340 --> 00:24:51.970
Okay.

00:24:51.970 --> 00:24:57.160
Here's an unbiased view of some
guy who actually works on iTunes.

00:25:01.500 --> 00:25:07.210
A couple of different
builds of iTunes here,

00:25:07.210 --> 00:25:13.850
and I was going to show them to you on
identical machines running side-by-side,

00:25:13.850 --> 00:25:13.850
but we don't have identical machines,
so I'm not going to do that.

00:25:13.850 --> 00:25:13.850
Instead, I'm going to ask you to...

00:25:14.190 --> 00:25:17.380
Watch the two versions,
one running after the other,

00:25:17.380 --> 00:25:19.800
and remember what the performance
of the first one was as you're

00:25:19.810 --> 00:25:20.920
watching the second one.

00:25:20.920 --> 00:25:22.520
It should be pretty obvious.

00:25:22.520 --> 00:25:25.590
Just before I actually
run the demo for you,

00:25:25.590 --> 00:25:30.370
let me give you some figures on the
sort of demands that we're making on

00:25:30.370 --> 00:25:33.180
the processor and vector unit here.

00:25:33.500 --> 00:25:36.790
What I'm going to show
you is MP3 encoding,

00:25:36.790 --> 00:25:42.820
during the course of which
at sort of typical settings,

00:25:42.820 --> 00:25:48.500
we'll run through on the
order of half a million FFTs,

00:25:48.500 --> 00:25:51.230
complex FFTs, to encode an album.

00:25:51.240 --> 00:25:54.880
And we'll run through a
similar number of MDCTs.

00:25:54.880 --> 00:25:59.940
The MDCT is essentially identical to
the IMDCT that Ali was talking about.

00:25:59.940 --> 00:26:02.840
It's the forward transform
instead of the inverse transform.

00:26:02.840 --> 00:26:05.920
So there's an awful lot of
computation going on here.

00:26:05.920 --> 00:26:09.900
In fact, a couple of years ago,
it was considered...

00:26:10.400 --> 00:26:39.000
[Transcript missing]

00:26:40.080 --> 00:26:42.550
The first thing we want to talk
about is the special build of

00:26:42.550 --> 00:26:44.760
iTunes that is not vectorized.

00:26:44.760 --> 00:26:50.370
So, whereas the normal shipping
iTunes uses Ali's vector library

00:26:50.520 --> 00:26:54.160
for the FFTs and the MDCTs,
this one does not.

00:26:54.210 --> 00:27:01.430
It uses, in fact, a very fast scalar FFT,
but it is scalar.

00:27:06.230 --> 00:27:08.680
So as not to add another
variable to the mix here,

00:27:08.680 --> 00:27:12.280
we're not going to encode off a
CD as you would typically do at home.

00:27:12.280 --> 00:27:15.160
Instead, we've got an AIFF file.

00:27:15.160 --> 00:27:21.890
It turns out I actually
tried in preparing this demo,

00:27:21.890 --> 00:27:27.950
I tried encoding off a CD,
and the problem is we're I.O.

00:27:27.950 --> 00:27:27.950
bound instead of CPU bound,
so it didn't make a very good demo.

00:27:30.600 --> 00:27:43.010
So here's our AIFF file that
we're going to convert to an MP3.

00:27:43.010 --> 00:27:43.010
And here it goes.

00:27:43.010 --> 00:27:43.010
So the number that I'd like you to
notice will appear here in a second.

00:27:43.920 --> 00:27:54.420
It's the X factor here, 9.6X,
so that means we're roughly 10 times

00:27:54.420 --> 00:27:54.420
real-time encoding from AIFF to MP3.

00:27:55.130 --> 00:27:59.130
So this is using a scalar FFT and
a scalar MDCT at this time,

00:27:59.250 --> 00:27:59.980
okay?

00:28:06.300 --> 00:28:13.910
www.aliksa.com/velocity-engineering/

00:28:18.500 --> 00:28:22.310
As impressive as 10X is,
you can do just one heck of a lot

00:28:22.320 --> 00:28:29.760
better if you use the vector engine.

00:28:29.760 --> 00:28:29.760
And I'll show you that in a second.

00:28:29.760 --> 00:28:29.760
Or 29 seconds.

00:28:32.360 --> 00:28:35.240
We could probably hear the
tune that it's trying to do.

00:28:35.240 --> 00:28:40.000
Yeah, I didn't handicap it by asking
it to play while it was in touch.

00:28:40.000 --> 00:28:40.720
Oh, okay, I see.

00:28:40.720 --> 00:28:41.030
All right.

00:28:41.040 --> 00:28:44.030
So he's putting all
his energies into that.

00:28:44.060 --> 00:28:45.450
He handicapped the other one.

00:28:45.540 --> 00:28:45.760
Okay.

00:28:45.760 --> 00:28:47.250
That's always fair.

00:28:52.110 --> 00:28:55.030
At roughly about 9x.

00:28:55.290 --> 00:28:58.590
So, 9 times real-time.

00:28:58.610 --> 00:28:59.000
Okay?

00:28:59.000 --> 00:29:00.000
All right.

00:29:00.000 --> 00:29:01.600
That's that.

00:29:01.660 --> 00:29:06.930
And this is the vector version.

00:29:43.700 --> 00:29:57.900
[Transcript missing]

00:29:59.270 --> 00:30:01.660
So if any of you guys are
doing MP3 applications,

00:30:01.660 --> 00:30:04.950
you should really look into the
stuff that's available in the vector

00:30:04.950 --> 00:30:07.160
libraries down in the BDSP libraries.

00:30:07.160 --> 00:30:08.680
There's good stuff there.

00:30:08.680 --> 00:30:09.680
All right.

00:30:09.720 --> 00:30:12.820
Okay, thank you, Bill.

00:30:12.820 --> 00:30:14.910
So this is...

00:30:17.910 --> 00:30:23.980
This is a very good
reason to use our VDSP.

00:30:23.980 --> 00:30:27.590
It's extremely fast,
and it does the job right,

00:30:27.820 --> 00:30:30.340
and I think you will see the
benefits of it right away.

00:30:30.340 --> 00:30:35.480
The important thing also to remember is
that VDSP has also a scalar component

00:30:35.480 --> 00:30:39.980
into it on the single precision engine,
and you don't have to worry about

00:30:39.980 --> 00:30:41.930
which machine you're running on.

00:30:41.940 --> 00:30:42.550
Just call it.

00:30:42.680 --> 00:30:44.190
It will make the right decision for you.

00:30:44.640 --> 00:30:47.140
It will run the correct
thing on the scalar machines.

00:30:47.170 --> 00:30:48.590
It will write the correct
thing on the vector one.

00:30:50.730 --> 00:30:54.230
The next topic I'd like to talk about
is the basic linear algebra subroutines.

00:30:54.240 --> 00:30:57.730
We've been asked about this many times.

00:30:57.740 --> 00:31:01.970
I'm happy to announce here that
we have a full suite of the basic

00:31:02.060 --> 00:31:03.900
linear algebra subroutines in JAGUAR.

00:31:03.900 --> 00:31:10.960
This is an important addition to JAGUAR,
and VDSP, VMathLib,

00:31:10.960 --> 00:31:15.970
and BLAS is a cornerstone of
a lot of the engineering and

00:31:16.070 --> 00:31:18.360
scientific work that is required.

00:31:18.960 --> 00:31:23.290
I think people using JAGUAR are
going to be happy about that.

00:31:23.350 --> 00:31:26.540
The BLAS that we have is
an industry-standard BLAS.

00:31:26.740 --> 00:31:31.460
It comes in single and double precision,
real and complex, vector and scalar.

00:31:31.460 --> 00:31:34.050
It transparently selects
between scalar and vector,

00:31:34.210 --> 00:31:37.420
so you don't have to worry about
which machine you're running on.

00:31:37.420 --> 00:31:41.810
We maintain multi-gigaflop performance,
even coming from DRAM,

00:31:41.810 --> 00:31:44.360
which I'll show you in a little bit.

00:31:47.220 --> 00:31:50.260
We have the fact that we are
going through a lot of machinery

00:31:50.260 --> 00:31:52.220
to make sure that we're running
on a scalar or a vector machine.

00:31:52.220 --> 00:31:58.250
We also have included some
specific calls to very small,

00:31:58.250 --> 00:32:02.980
tiny matrix multiplies,
or complex matrix multiplies,

00:32:02.980 --> 00:32:06.650
or vector matrix multiplies that
you can use if you're not worried

00:32:06.740 --> 00:32:09.840
about the BLAS compatibility
from one platform to another.

00:32:09.840 --> 00:32:12.060
There's a bunch of those
calls in there as well,

00:32:12.060 --> 00:32:13.470
and they're extremely fast.

00:32:13.680 --> 00:32:16.390
You have to know you're running on a
vector machine or a scalar machine.

00:32:16.480 --> 00:32:19.590
You have to know what you're doing,
but you get the raw speed.

00:32:20.570 --> 00:32:23.970
What I would like to show you
now is the performance of S-GEM.

00:32:23.980 --> 00:32:29.940
S-GEM, for those of you who know,
is the complex multiplication of a matrix

00:32:30.580 --> 00:32:35.150
times another one plus another matrix,
and there are two scalars

00:32:35.150 --> 00:32:37.350
involved in here that have to
be multiplying in the middle.

00:32:37.360 --> 00:32:41.770
And what we have here
is the comparison of the

00:32:41.770 --> 00:32:49.960
Mac OS X BLAST S-GEM versus the MKL,
the Mathematical Kernel

00:32:49.960 --> 00:32:51.840
Library from Intel.

00:32:51.840 --> 00:32:55.120
As you can see,
we do quite well in the crucial

00:32:55.120 --> 00:32:59.680
and very important portion,
up to 64 by 64.

00:32:59.680 --> 00:33:02.620
It turns out that most of the
applications that we deal with

00:33:02.810 --> 00:33:05.310
are on the tiny side of things,
and we like to give them

00:33:05.380 --> 00:33:06.720
as much boost as possible.

00:33:07.020 --> 00:33:08.910
And we maintain performance after that.

00:33:09.050 --> 00:33:11.550
The Intel crowd does better
because of the way they handle

00:33:11.550 --> 00:33:13.080
I.O., but that's beyond our reach.

00:33:13.230 --> 00:33:15.040
There is nothing we can do
about this at this time.

00:33:15.270 --> 00:33:19.590
But up to 64 by 64,
we can give you an enormous

00:33:19.590 --> 00:33:21.920
amount of performance.

00:33:21.920 --> 00:33:25.630
And it's pretty important to see,
it's a 1 GHz machine.

00:33:25.640 --> 00:33:29.930
This is per processor again,
so if you multi-thread your application

00:33:30.380 --> 00:33:34.940
and you run it on two processors,
you're going to get two of these guys.

00:33:34.940 --> 00:33:35.870
You get 5 GFLOPS.

00:33:36.910 --> 00:33:41.000
Per matrix multiply per processor.

00:33:42.770 --> 00:33:47.200
So S-GAM again is one of those things,
it's like a 1024 complex FFT.

00:33:47.200 --> 00:33:49.260
Everybody has it in their mind,
at least I do.

00:33:49.260 --> 00:33:53.130
And I want to make sure what my
gigaflop rate is on that one.

00:33:55.870 --> 00:34:00.370
We also have had traditionally
another library called VectorOps in

00:34:00.370 --> 00:34:04.150
our OS for those of you who
have used previous versions of

00:34:04.220 --> 00:34:06.640
OS X and some of the OS 9 before.

00:34:06.640 --> 00:34:10.780
The differences between the
BLAS and the VectorOps is that

00:34:10.780 --> 00:34:13.440
the BLAS is an industry standard.

00:34:13.440 --> 00:34:16.380
If you have calls to BLAS,
it will just compile on our machine now.

00:34:16.380 --> 00:34:20.080
The BLAS that we have is
based on a patented algorithm,

00:34:20.080 --> 00:34:22.950
and it's very high performance.

00:34:23.800 --> 00:34:27.260
VectorOps now is relegated
more to an educational tool.

00:34:27.260 --> 00:34:28.480
The source is available.

00:34:28.480 --> 00:34:30.100
You can download it from our website.

00:34:30.100 --> 00:34:32.520
You can look at it and see
how we have vectorized it.

00:34:32.560 --> 00:34:35.880
It has still pretty good performance,
but it's not as good as the BLAS.

00:34:36.020 --> 00:34:39.360
VectorOps is good for small arrays.

00:34:39.360 --> 00:34:42.900
If you really need source and you don't
want to worry about the BLAS and you

00:34:42.900 --> 00:34:45.760
have small matrices or small vectors,
this is for you.

00:34:45.760 --> 00:34:49.080
It also is only on the vector domain,
and it uses the vector registers as

00:34:49.080 --> 00:34:51.080
opposed to regular pointers to arrays.

00:34:51.800 --> 00:34:55.030
So if you're sitting on the
vector registers and you want

00:34:55.030 --> 00:34:59.360
to use vector multiplies,
matrix multiplies,

00:34:59.360 --> 00:35:00.730
and this is what you want to do.

00:35:01.060 --> 00:35:04.880
What I would recommend is that the
BLAS is the preferred way of doing

00:35:05.000 --> 00:35:07.440
work now as opposed to the VectorOps.

00:35:10.160 --> 00:35:14.580
At this point, what I want to go into is
the performance issues,

00:35:14.580 --> 00:35:19.370
and I have a few of these performance
issues that I will talk about,

00:35:19.720 --> 00:35:24.880
and how you can do better
with your applications.

00:35:24.900 --> 00:35:27.140
The first one is the
data injection technique,

00:35:27.220 --> 00:35:30.140
and this is a word I came up with
as I was writing this slide here

00:35:30.140 --> 00:35:33.420
because starving the processor
didn't just sound too good.

00:35:34.350 --> 00:35:36.630
So this is a software-only approach.

00:35:36.670 --> 00:35:39.350
It minimizes the pipeline stalls.

00:35:39.350 --> 00:35:44.570
It really takes the execution
pipeline into account.

00:35:44.670 --> 00:35:46.490
I will have a detailed example of that.

00:35:46.600 --> 00:35:50.250
Remember, as deeper your pipeline is,
as we are going further and

00:35:50.400 --> 00:35:53.840
further into processor land here,
and more and more progress

00:35:53.840 --> 00:35:57.270
in gigahertz is happening,
well, these processors are getting

00:35:57.590 --> 00:36:00.560
longer and longer pipelines,
and you have to worry about that.

00:36:01.050 --> 00:36:05.550
So the deeper pipelines need
more independent set of data,

00:36:05.550 --> 00:36:10.470
and this is exactly what I mean
by data injection techniques.

00:36:10.500 --> 00:36:16.400
I also want to tell you that the dynamic
dependency will slow a machine down.

00:36:16.400 --> 00:36:22.480
If you have, let's say, an expression,
and a result of that is required

00:36:22.540 --> 00:36:25.790
for the next expression,
and the result of that is

00:36:25.850 --> 00:36:29.210
required for the next expression,
that's what I call dynamic dependency,

00:36:29.320 --> 00:36:31.000
it's hard to avoid stalls there.

00:36:31.020 --> 00:36:33.970
Because the machine just has to wait
until the first one is done before

00:36:33.970 --> 00:36:35.310
we can dive into the other one.

00:36:35.400 --> 00:36:39.200
So this is something that
we would like to avoid.

00:36:39.220 --> 00:36:43.830
The next performance issue that
comes to mind is vector algorithms.

00:36:43.840 --> 00:36:48.880
It is very important for people who
are coming from the scalar domain to

00:36:49.310 --> 00:36:53.480
remember that some very fast scalar
algorithms are just not vectorizable.

00:36:53.480 --> 00:36:58.040
They don't have a parallel
grain in their lines.

00:36:58.140 --> 00:37:00.300
I mean, there's just nothing
you can do about that.

00:37:01.070 --> 00:37:02.990
You have to rethink your algorithms.

00:37:03.130 --> 00:37:05.020
Different algorithms may be required.

00:37:05.020 --> 00:37:08.480
Perhaps the different algorithm that you
chucked because it wasn't good enough

00:37:08.480 --> 00:37:09.920
for scalar is not good enough for vector.

00:37:10.050 --> 00:37:11.720
So this is important.

00:37:12.100 --> 00:37:15.360
Again, the dynamic dependency of the
instruction causes slowdowns.

00:37:15.700 --> 00:37:17.520
Keep that in mind.

00:37:17.520 --> 00:37:20.530
And the data injection techniques
will not work in the presence

00:37:20.980 --> 00:37:22.440
of dynamic dependencies.

00:37:22.520 --> 00:37:26.120
I will follow this up with an example.

00:37:26.610 --> 00:37:30.990
The next issue is the LSU bottleneck,
the load store unit on the PowerPC.

00:37:31.100 --> 00:37:38.670
We typically see two or three loads and
stores for FPU and vector instructions.

00:37:38.840 --> 00:37:40.790
Per vector and FPU and
vector instructions,

00:37:40.880 --> 00:37:43.320
this is the way the
compiler generates things.

00:37:43.380 --> 00:37:50.150
Some of the remedy for that would be
to inline or remove trivial functions.

00:37:50.180 --> 00:37:53.280
We're in the habit of compartmentalizing
things and putting things away.

00:37:53.280 --> 00:37:57.140
And for further use,
we've got one or two lines of code,

00:37:57.140 --> 00:37:59.480
we wrap it up in a
function and put it away,

00:37:59.480 --> 00:38:00.610
and then we call it.

00:38:00.760 --> 00:38:03.650
What I'm advocating
here is not to do that.

00:38:03.740 --> 00:38:05.520
Try to inline as much as possible.

00:38:05.520 --> 00:38:11.020
Try to alleviate the problem of
going from one function to another.

00:38:11.020 --> 00:38:13.510
Minimize memory movement.

00:38:13.520 --> 00:38:15.260
And this is part of that.

00:38:15.330 --> 00:38:19.560
By inlining,
you would minimize memory movement.

00:38:19.560 --> 00:38:22.280
Keep values and registers
as much as possible.

00:38:22.280 --> 00:38:27.280
This is somewhat in your
discretion and somewhat it's not.

00:38:27.710 --> 00:38:31.690
Using the compiler,
it's somewhat not in your discretion.

00:38:31.790 --> 00:38:35.720
And then if you wrote it in assembly,
of course, you can do whatever you want.

00:38:35.820 --> 00:38:41.680
The LSU over-reliance frequently
causes two to three times slowdowns

00:38:41.680 --> 00:38:44.600
in the codes that we have seen.

00:38:44.600 --> 00:38:49.540
And this is one major problem that
we generally have to deal with.

00:38:49.560 --> 00:38:53.430
It's to remove some of
these LSU instructions from

00:38:53.540 --> 00:38:55.760
the instruction stream.

00:38:57.390 --> 00:39:00.240
The next performance issue
is that you really have to

00:39:00.420 --> 00:39:03.010
be aware of the ISA detail,
the instruction set

00:39:03.010 --> 00:39:04.360
architecture of the PowerPC.

00:39:04.380 --> 00:39:05.920
Be aware of that.

00:39:05.940 --> 00:39:08.720
Know what the processor
is doing under your feet.

00:39:08.800 --> 00:39:14.510
Remember, the vector engine is pipeline,
single instruction, and for all of 162

00:39:14.510 --> 00:39:16.470
instructions that we have.

00:39:16.800 --> 00:39:18.640
This is not true for the FPU.

00:39:18.640 --> 00:39:20.880
The FPU produces stalls.

00:39:20.900 --> 00:39:23.340
As an example, I have the FP divide.

00:39:23.390 --> 00:39:26.390
The FP divide is not
pipelined but sequenced.

00:39:27.190 --> 00:39:32.100
The FP divide on the early G4s
would take about 33 cycles,

00:39:32.100 --> 00:39:34.460
would not allow anything
else to be dispatched,

00:39:34.460 --> 00:39:37.040
and it will give you
the result after that.

00:39:37.320 --> 00:39:40.750
And the new one,
the new G4s in the 1 GHz machines,

00:39:40.750 --> 00:39:42.360
takes 37 cycles.

00:39:42.380 --> 00:39:43.670
So be aware of that.

00:39:43.790 --> 00:39:46.830
Also remember that the reciprocal
estimate is not pipelined.

00:39:46.840 --> 00:39:50.480
The reciprocal estimate takes
anywhere from 14 to 21 cycles,

00:39:50.480 --> 00:39:53.780
depending on which flavor
of PowerPC you use.

00:39:53.900 --> 00:39:57.060
One other detail of the ISA.

00:39:57.060 --> 00:40:04.060
that you should keep in mind is that we
only have dual issue on the older G4s.

00:40:04.120 --> 00:40:07.740
These are, of course, superscalar,
out-of-order cores,

00:40:07.900 --> 00:40:11.560
but the more issues you have,
the more work you can do.

00:40:11.740 --> 00:40:15.940
But the old ones only have two,
and the new ones have triple issues,

00:40:15.940 --> 00:40:19.300
so we can issue more of these
instructions and do more work.

00:40:19.570 --> 00:40:22.820
Just have to remember that even
though we have a triple issue,

00:40:23.080 --> 00:40:26.300
the vector engine can take
only two at the same time.

00:40:26.940 --> 00:40:30.800
On the older designs,
there was a restriction of which

00:40:30.800 --> 00:40:35.400
subunit would be able to process that,
but on the newer designs,

00:40:35.520 --> 00:40:39.050
all of the VALU and the VPERM unit
inside the vector engine is up

00:40:39.250 --> 00:40:41.260
for grabs for the dual issue.

00:40:45.030 --> 00:40:48.680
Now I'd like to go over an example
of what happens if we use the data

00:40:48.770 --> 00:40:50.900
injection technique that I alluded to.

00:40:50.900 --> 00:40:56.260
Here is just one run-of-the-mill
example of a floating point program.

00:40:56.260 --> 00:41:00.220
And this floating point
program is doing something,

00:41:00.220 --> 00:41:01.260
let's say.

00:41:01.260 --> 00:41:04.360
I don't know what it's doing,
but it's doing something.

00:41:04.360 --> 00:41:09.140
And what we like to do is to find
out what happens to the processor.

00:41:11.970 --> 00:41:13.170
I mentioned CHUD.

00:41:13.180 --> 00:41:16.440
This is an output from a
CHUD tool that you will see later.

00:41:16.440 --> 00:41:19.480
It's showing us what
the machine is doing,

00:41:19.480 --> 00:41:23.740
the various sections of the
processor pipeline for the older

00:41:23.740 --> 00:41:26.180
style G4s at this time here.

00:41:26.180 --> 00:41:28.850
There's an instruction
stream on the left side,

00:41:28.850 --> 00:41:31.920
and on the right side is
where the pipeline looks like.

00:41:32.030 --> 00:41:35.370
The letters in D are
the stalls that you see,

00:41:35.520 --> 00:41:41.100
and of course the compiler for me has
generated an add immediate and a load

00:41:41.100 --> 00:41:47.100
floating point single that may or may
not be useful or needed at this time.

00:41:47.140 --> 00:41:52.200
It takes 25 cycles to do this
particular little program here.

00:41:52.200 --> 00:41:56.140
What we want to do is
find out what we can do.

00:41:56.220 --> 00:41:59.510
can do to enhance this.

00:41:59.740 --> 00:42:02.690
What I've done here is that I've
taken exactly that algorithm,

00:42:02.690 --> 00:42:07.270
and now instead of using one set of data,
I'm using three sets of data.

00:42:07.270 --> 00:42:10.660
Because what I saw was that
my pipeline had bubbles in it.

00:42:10.780 --> 00:42:13.890
I had a dynamic dependency,
and that dynamic dependency

00:42:13.890 --> 00:42:16.900
would not allow me to go as
fast as I would like to do.

00:42:16.900 --> 00:42:19.410
Here what I've done is,
taken the same thing,

00:42:19.410 --> 00:42:21.950
and now I'm doing three
things at the same time,

00:42:22.020 --> 00:42:24.400
basically parallelizing my algorithm.

00:42:25.430 --> 00:42:30.220
And looking one more
time at the CHUD tool,

00:42:30.440 --> 00:42:32.740
again the instruction
stream is at the left side,

00:42:33.040 --> 00:42:35.000
the pipeline is at the right side.

00:42:35.030 --> 00:42:38.700
This is as perfectly as you can
get on this machine pipeline.

00:42:38.700 --> 00:42:39.680
There is no stalls.

00:42:39.990 --> 00:42:42.370
There are a little nitty-gritty
thing happening here,

00:42:42.420 --> 00:42:44.460
but we're not going to talk about that.

00:42:44.660 --> 00:42:46.390
But what happens is
that this is the best.

00:42:46.390 --> 00:42:48.980
If all of your code would be like this,
your machine would be

00:42:49.090 --> 00:42:50.570
flying at 10 gigahertz.

00:42:52.190 --> 00:42:55.780
And you won't need to worry
about the frequency anymore.

00:42:55.780 --> 00:42:57.570
This is as good as it gets.

00:42:57.570 --> 00:43:01.200
And basically now we're doing
three sets of this for the

00:43:01.440 --> 00:43:03.780
same program in 32 cycles.

00:43:03.780 --> 00:43:06.910
So basically,
instead of the 25 that we have,

00:43:06.940 --> 00:43:09.790
now we've brought it down to 10 cycles.

00:43:09.820 --> 00:43:12.640
Extremely powerful technique,
this data injection,

00:43:12.640 --> 00:43:15.520
which basically parallelizes
the set of data that you're

00:43:15.520 --> 00:43:17.130
sending inside the program.

00:43:17.140 --> 00:43:20.310
And we use this all the time.

00:43:20.310 --> 00:43:27.620
And one of the basic problems of getting
slower performance on the PowerPC.

00:43:30.770 --> 00:43:34.190
I'd like to wrap up this
section of the talk by telling

00:43:34.190 --> 00:43:36.540
you where to go to learn more.

00:43:36.540 --> 00:43:39.730
I'm not expecting you to write all
this stuff down and go and apply it

00:43:39.730 --> 00:43:41.980
right away after this session here.

00:43:41.980 --> 00:43:43.620
So where can you browse it?

00:43:43.730 --> 00:43:47.780
We have a new Spiffy website
which has a lot of information

00:43:47.840 --> 00:43:52.960
for you and you can find it in
developer.apple.com hardware VE.

00:43:53.580 --> 00:43:58.040
You can code it by going
to the Veclip framework.

00:43:58.040 --> 00:44:00.180
This is where all of our frameworks are.

00:44:00.180 --> 00:44:04.610
If you wanted to talk to the VDSP,
to the BLAS, to the VectorOps or the

00:44:04.610 --> 00:44:08.850
VBasicOps or the VMathlib,
this is where you would find it.

00:44:09.130 --> 00:44:11.650
How you would actually
use it in your program,

00:44:11.650 --> 00:44:15.080
you would include Veclip as
a hierarchical include here,

00:44:15.080 --> 00:44:16.740
Veclip slash Veclip dot h.

00:44:17.470 --> 00:44:22.130
And using it is as easy as putting
the minus framework in front of the,

00:44:22.230 --> 00:44:27.430
as a compile option and our library which
is Veclip and then you're set to go.

00:44:27.440 --> 00:44:31.820
And you'll get the benefits of the
vectorization whether you like it or not.

00:44:31.880 --> 00:44:33.680
Well,
you have to call the vector library.

00:44:33.680 --> 00:44:35.390
All right.

00:44:36.820 --> 00:44:41.620
So I'd like to turn the
podium to my friends from the

00:44:41.620 --> 00:44:45.960
hardware performance group,
and they're going to tell us a little

00:44:45.960 --> 00:44:51.140
bit more about what they've done to
show us where the bottlenecks are.

00:44:51.140 --> 00:44:54.370
I'd like to introduce Sanjay
Patel and Nathan Slingerland.

00:45:09.930 --> 00:45:09.930
Hi, my name is Sanjay Patel.

00:45:09.930 --> 00:45:09.930
I'm in the Architecture and
Performance group.

00:45:10.140 --> 00:45:11.360
This is Nathan Slingerland.

00:45:11.360 --> 00:45:14.880
So we live over in hardware,
so we like to optimize down to the metal.

00:45:14.880 --> 00:45:17.500
And that's why we've
created the CHUD tools,

00:45:17.580 --> 00:45:20.260
Computer Hardware
Understanding Development Tools.

00:45:22.100 --> 00:45:27.600
So I'd like to introduce the first
couple of tools are used to actually

00:45:28.000 --> 00:45:32.320
Get down to PowerPC machine-level
instruction profiling.

00:45:32.320 --> 00:45:34.340
So the first tool is AMBER.

00:45:34.340 --> 00:45:35.900
It's a command line tracer.

00:45:35.900 --> 00:45:40.030
And what that allows you to do
is get the machine code for your

00:45:40.030 --> 00:45:44.880
program to actually see it in a
pipeline view through a cycle-accurate

00:45:45.000 --> 00:45:50.220
simulator that's called SIMG4,
which models the PPC7400.

00:45:51.120 --> 00:45:57.720
So as Ali was showing earlier,
we have our example of division where we

00:45:57.720 --> 00:46:00.620
use a reciprocal estimate in this case.

00:46:00.620 --> 00:46:04.540
And so all you have to do to
instrument your code is throw in

00:46:04.540 --> 00:46:08.540
these little start/stop functions,
which we'll take a look at in a second.

00:46:08.540 --> 00:46:13.300
There are actually many different
ways we can start and stop tracing.

00:46:13.300 --> 00:46:18.260
So at this point, I guess we'll go to...

00:46:21.600 --> 00:46:24.780
Well, we'll take a trace in a second.

00:46:24.800 --> 00:46:29.190
But as we saw earlier,
this is the output from SimG4,

00:46:29.190 --> 00:46:31.570
which is the cycle-accurate simulator.

00:46:31.570 --> 00:46:34.490
And what we see here is
that the first instruction,

00:46:34.500 --> 00:46:37.830
the reciprocal estimate,
is a long latency instruction,

00:46:37.830 --> 00:46:39.350
and it's not pipelined.

00:46:39.450 --> 00:46:43.210
So what you see there in the E's,
the processor is actually

00:46:43.210 --> 00:46:46.070
executing over that,
but none of the following

00:46:46.140 --> 00:46:48.540
instructions can proceed ahead of it.

00:46:48.650 --> 00:46:51.640
And of course you see the
other data dependency stalls

00:46:51.670 --> 00:46:53.980
with the highlighted red D's.

00:46:56.730 --> 00:47:01.810
So what we want to do is,
we'll take a trace that

00:47:01.810 --> 00:47:07.400
shows this in a second,
and show how we can break

00:47:07.400 --> 00:47:13.650
some of these pipeline stalls.

00:47:13.650 --> 00:47:17.050
So if we can switch over
to the demo computer.

00:47:28.200 --> 00:47:29.190
There we go.

00:47:29.220 --> 00:47:43.960
So let's first take a look at the
start/stop function that we introduced.

00:47:43.980 --> 00:47:47.880
In this case,
it's just a single machine instruction,

00:47:47.900 --> 00:47:51.960
which is actually an illegal
instruction from user-level code.

00:47:51.990 --> 00:47:57.150
And what we've done is we've overridden
the illegal instruction handler in

00:47:57.150 --> 00:48:01.740
amber to use this as a trigger to say,
"Start or stop tracing my program."

00:48:01.740 --> 00:48:03.860
So it's as simple as that.

00:48:03.860 --> 00:48:05.720
There are a number of different
ways you could do this as well.

00:48:05.720 --> 00:48:10.610
You could either hot-key if your program
is long-running to take a massive trace.

00:48:10.620 --> 00:48:15.840
Then you're just depending on how fast
you can hit the keys to start and stop.

00:48:15.840 --> 00:48:19.490
You can actually start and
stop based on a symbol or an

00:48:19.550 --> 00:48:21.700
instruction address as well.

00:48:21.740 --> 00:48:25.300
So these are the parameters
for amber you see on screen.

00:48:25.340 --> 00:48:28.380
So since we have an
instrumented function,

00:48:28.440 --> 00:48:32.250
we're going to use the dash
I option and take a trace.

00:48:35.980 --> 00:48:37.960
So you can see it's
really a quick process to,

00:48:37.980 --> 00:48:40.900
since our program is so short,
we're just going to take a

00:48:40.900 --> 00:48:42.880
trace of 16 instructions.

00:48:42.890 --> 00:48:46.110
And if we look here,
we'll see what Amber has created a

00:48:46.110 --> 00:48:48.880
directory called trace with some offset.

00:48:48.900 --> 00:48:51.740
And if you go in there,
you'll see there's a thread file.

00:48:51.910 --> 00:48:56.500
So if your program is multi-threaded,
we'll create multiple

00:48:56.500 --> 00:48:56.500
files for each thread.

00:48:59.770 --> 00:49:02.540
And so now what we want to do is,
this file is actually binary,

00:49:02.540 --> 00:49:05.880
so to make sense of it you
need to input it to SIMG4,

00:49:05.880 --> 00:49:10.320
and you can get a pipeline
view as we saw earlier.

00:49:13.430 --> 00:49:16.710
Okay, so there's our pipeline
view as we saw in the slide.

00:49:16.740 --> 00:49:20.770
So I guess let's go back to
the presentation computer so we

00:49:20.770 --> 00:49:23.270
can take a look at that output.

00:49:34.850 --> 00:49:36.700
Can we go back to the
presentation computer?

00:49:36.700 --> 00:49:40.580
Okay, so here we are again.

00:49:40.580 --> 00:49:44.200
So this is the same output
we just saw from SIMG4.

00:49:44.280 --> 00:49:46.980
And as we said,
it takes a long time to execute

00:49:47.040 --> 00:49:49.450
because of that reciprocal estimate.

00:49:49.500 --> 00:49:53.310
So as Ali showed us,
we can break that bottleneck

00:49:53.310 --> 00:49:58.020
by using a reciprocal square
root and then multiplying it,

00:49:58.040 --> 00:50:01.380
because reciprocal square
root is a pipeline instruction

00:50:01.380 --> 00:50:01.380
and it's low latency.

00:50:02.520 --> 00:50:05.900
So again, we'd have our start and stop
calls to Amber to take a trace,

00:50:06.010 --> 00:50:08.560
and if we took that trace
and input it to SIMG4,

00:50:08.560 --> 00:50:12.030
we'd see that the pipeline output,
now we've eliminated the

00:50:12.040 --> 00:50:15.200
long delay at the beginning,
we still have all of our

00:50:15.200 --> 00:50:17.010
data dependency stalls.

00:50:17.020 --> 00:50:24.380
But we've already gotten this
program down to 24 cycles,

00:50:24.380 --> 00:50:24.380
so we're 29% faster than the base case.

00:50:25.300 --> 00:50:28.130
So again,
if we were to use the data injection

00:50:28.130 --> 00:50:31.880
technique where we do multiple sets
of data rather than a single to

00:50:31.880 --> 00:50:36.240
match the length of the pipeline of
the processor we're interested in,

00:50:36.240 --> 00:50:40.010
we'll see that we have the SIMG4 output
shows that there are no more stalls.

00:50:40.030 --> 00:50:44.190
And what we've actually done now is
throughput is 2.9 times as fast as

00:50:44.190 --> 00:50:46.140
what we started in our base case.

00:50:46.200 --> 00:50:52.300
This program is only one cycle
longer than what it would take

00:50:52.300 --> 00:50:52.750
to do the single division.

00:50:53.820 --> 00:50:58.050
Okay, so in summary,
once you're ready to get down

00:50:58.070 --> 00:51:02.490
to optimizing at machine level,
you want to take traces and use SIMG4

00:51:02.490 --> 00:51:06.130
to analyze your output so you can
identify all the bottlenecks that you

00:51:06.140 --> 00:51:07.490
might be encountering on the processor.

00:51:07.500 --> 00:51:10.250
So in this case,
we saw we got a 29% improvement

00:51:10.300 --> 00:51:14.020
just by switching one instruction,
one machine instruction.

00:51:14.020 --> 00:51:17.340
And then by using data injection,
we can actually be almost three

00:51:17.490 --> 00:51:19.280
times as fast as our base case.

00:51:21.330 --> 00:51:26.480
So I'll now turn it over to Nathan to
talk about a couple of our other tools.

00:51:30.710 --> 00:51:32.240
Thank you, Sanjay.

00:51:32.340 --> 00:51:34.800
Okay, so I'm going to talk
about a few other tools,

00:51:34.870 --> 00:51:37.300
and they rely on the
performance counters,

00:51:37.300 --> 00:51:41.390
which are built into the hardware on
the PowerPC chip and also in the memory

00:51:41.390 --> 00:51:43.640
controller on some of our machines.

00:51:43.640 --> 00:51:47.050
And with these performance counters,
you can count interesting

00:51:47.410 --> 00:51:50.990
performance events like cache
misses or instruction counts,

00:51:50.990 --> 00:51:52.690
execution stalls and the like.

00:51:52.740 --> 00:51:57.370
And the Chud toolkit gives you the
ability to actually ask those counters

00:51:57.440 --> 00:52:00.070
what's going on inside your program.

00:52:01.890 --> 00:52:06.400
So the first tool we're going to talk
about and I'll demo for you is Shikari.

00:52:06.400 --> 00:52:10.140
And Shikari is a
system-wide profiling tool.

00:52:10.140 --> 00:52:15.490
Okay, so it's going to show you over time
what's happening in the whole system.

00:52:15.600 --> 00:52:18.400
So you can use,
you can sample based on time,

00:52:18.400 --> 00:52:21.360
like I said, so it would be similar to
a program like Sampler.

00:52:21.420 --> 00:52:24.000
Or you can sample on events,
so any of those performance events.

00:52:24.000 --> 00:52:26.760
So you can find out where
cache misses are coming from.

00:52:26.820 --> 00:52:29.620
And then what it does is it
will correlate those events

00:52:29.720 --> 00:52:32.260
with your source code.

00:52:32.330 --> 00:52:37.060
And it'll show you performance
hotspots based on those events.

00:52:37.300 --> 00:52:42.890
and you can also look at an
annotated disassembly of any

00:52:42.890 --> 00:52:42.890
function that is a hotspot.

00:52:43.640 --> 00:52:47.260
Now let's go to the demo machine.

00:52:47.270 --> 00:52:50.590
So in order to demonstrate
how you can use Shikari to

00:52:50.660 --> 00:52:54.210
track down real performance
bottlenecks and then fix them,

00:52:54.210 --> 00:52:56.590
we have this converted screensaver.

00:52:56.600 --> 00:52:59.000
It's called Flurry by Calum Robinson.

00:52:59.130 --> 00:53:02.100
And right now this is the
original Flurry code running.

00:53:02.140 --> 00:53:05.900
And what we're doing is we're timing
how long it takes to render 500 frames.

00:53:05.900 --> 00:53:08.010
It's an OpenGL screensaver.

00:53:08.100 --> 00:53:10.250
And in this machine,
it's almost 12 seconds,

00:53:10.250 --> 00:53:12.650
11.6 seconds to do that.

00:53:13.380 --> 00:53:16.720
Okay, so the first step in understanding
any kind of performance issues

00:53:16.750 --> 00:53:21.260
would be to profile this,
especially because this is a screensaver.

00:53:21.260 --> 00:53:24.450
We don't know if we're CPU-bound
or graphics card-bound or what.

00:53:24.500 --> 00:53:27.200
So this is the main Shikari window.

00:53:27.200 --> 00:53:32.200
What we have here first is the profile
that's going to list from top to bottom

00:53:32.200 --> 00:53:34.210
the most frequently sampled functions.

00:53:34.240 --> 00:53:38.700
And below that we have a list
of sampling configurations.

00:53:38.700 --> 00:53:42.040
This pop-up here is basically
the defaults that come with it,

00:53:42.060 --> 00:53:44.460
but you're free to add configurations.

00:53:44.460 --> 00:53:47.140
You can take advantage of all
the performance counters that

00:53:47.140 --> 00:53:49.290
are available in your system.

00:53:50.610 --> 00:53:58.880
So,
let's actually use the time profile and

00:53:58.880 --> 00:53:58.880
see where Flurry is spending its time.

00:54:02.780 --> 00:54:04.790
So it's taken a second's
worth of samples.

00:54:04.790 --> 00:54:09.220
And we can see the first thing that
pops out to us here is that the

00:54:09.280 --> 00:54:12.900
lib system square root is taking
a significant portion of time.

00:54:12.900 --> 00:54:17.270
Now, as Ali told you,
the Mac OS X square root is actually

00:54:17.270 --> 00:54:19.780
quite fast and very precise.

00:54:19.800 --> 00:54:23.920
But because this is a screensaver,
maybe we don't want to be getting

00:54:23.920 --> 00:54:28.750
back the IEEE 754 compliant,
53 bits of precision, all that goodness,

00:54:28.750 --> 00:54:29.910
for the screensaver.

00:54:30.320 --> 00:54:32.260
So if we go and look at
where we're calling this,

00:54:32.260 --> 00:54:37.370
we can see that this is actually a pretty
good candidate for replacement by what

00:54:37.380 --> 00:54:41.240
we have as the floating point reciprocal
square root estimate instruction.

00:54:41.240 --> 00:54:44.690
And now, this isn't appropriate
for every algorithm,

00:54:44.690 --> 00:54:45.580
of course.

00:54:45.580 --> 00:54:47.910
A lot of times you will need
to use the lib system math

00:54:47.910 --> 00:54:49.540
library square root function.

00:54:49.540 --> 00:54:52.350
But in our case,
we know that we're okay with that

00:54:52.450 --> 00:54:56.560
and that the 5 bits of precision
that this provides is sufficient.

00:54:56.560 --> 00:55:00.080
So let's see how much that helps us.

00:55:11.010 --> 00:55:12.430
So almost a 2x speedup.

00:55:12.490 --> 00:55:14.630
That's pretty good for, you know,
it's still scalar.

00:55:14.630 --> 00:55:20.040
So the question now is, again,
like as Ali said, profile, profile,

00:55:20.040 --> 00:55:21.000
profile.

00:55:21.000 --> 00:55:24.560
Let's profile again and see where
we're spending our time now that we've

00:55:24.880 --> 00:55:26.600
eliminated the call-to-lib system.

00:55:35.320 --> 00:55:39.340
So we can see that now the system square
root function is gone from the profile.

00:55:39.340 --> 00:55:40.340
That's what we expect.

00:55:40.340 --> 00:55:43.670
And now we're spending a lot of
time in update smoke and draw smoke.

00:55:43.760 --> 00:55:44.380
That's our code.

00:55:44.400 --> 00:55:46.520
That's inside of Flurry.

00:55:46.610 --> 00:55:49.900
Okay, so we know that,
because this is our code,

00:55:49.900 --> 00:55:53.070
we know that this is dominated
by floating point computation,

00:55:53.070 --> 00:55:55.700
this particular, these two functions.

00:55:55.750 --> 00:56:01.440
And so what we're going
to do next is look at,

00:56:01.440 --> 00:56:03.680
and what Shaqari can tell you,
is floating point issue stalls.

00:56:03.750 --> 00:56:09.510
These are the dependency stalls
that we talked about earlier.

00:56:09.510 --> 00:56:09.510
And there's a preset for that.

00:56:09.690 --> 00:56:14.710
So this will tell you where in
your code the most floating-point

00:56:14.710 --> 00:56:14.710
dependency stalls are coming from.

00:56:28.760 --> 00:56:32.040
Okay, so we can see now that most of
our floating point issue stalls

00:56:32.040 --> 00:56:33.940
are coming from update smoke.

00:56:34.050 --> 00:56:36.790
So with Shaqari,
you can drill down and see the

00:56:36.960 --> 00:56:40.120
actual disassembled code behind that.

00:56:40.240 --> 00:56:43.460
And it's going to visually point
out to you where the hot spot is.

00:56:43.460 --> 00:56:46.650
So blue meaning not hot or cold,
it wasn't referenced or

00:56:46.650 --> 00:56:48.620
wasn't recorded in the sample.

00:56:48.700 --> 00:56:52.160
And then we'll see yellow or
orange when we get to a hot spot.

00:56:52.170 --> 00:56:56.010
So here it is, around,
in the file smoke.c,

00:56:56.010 --> 00:56:57.810
around line 279 or so.

00:56:59.800 --> 00:57:01.660
If we go back to our source code here.

00:57:01.710 --> 00:57:03.400
So that's this loop.

00:57:03.480 --> 00:57:06.660
So, okay,
we already know what to do about this,

00:57:06.660 --> 00:57:07.000
right?

00:57:07.050 --> 00:57:08.630
We have dependency stalls.

00:57:08.720 --> 00:57:12.240
We could use data injection,
unroll the loop, have it try to do more

00:57:12.240 --> 00:57:13.200
things in parallel.

00:57:13.200 --> 00:57:18.930
Or, if we're going to spend the time,
maybe we should vectorize

00:57:18.930 --> 00:57:20.630
it and unroll it.

00:57:20.630 --> 00:57:23.190
So do both at the same time.

00:57:23.190 --> 00:57:26.400
And that's what this code is here.

00:57:26.400 --> 00:57:29.360
So, let's see how that performs.

00:57:36.090 --> 00:57:40.090
So almost another 2x
speedup for using AltaVec.

00:57:40.200 --> 00:57:46.110
Okay, so where would we go next, right?

00:57:46.470 --> 00:57:50.570
Well, let's actually, you know,
we might think, well, let's try and find

00:57:50.650 --> 00:57:52.840
AlteVec issue stalls or,
you know,

00:57:52.840 --> 00:57:55.960
maybe we should MP it or something,
you know, multi-thread this app

00:57:55.960 --> 00:57:56.950
or something like that.

00:57:56.960 --> 00:58:02.200
So let's use,
let's profile again and let's see where

00:58:02.200 --> 00:58:02.200
Sha'Carri says we're spending time.

00:58:07.890 --> 00:58:09.820
So what's interesting here
is now that we're not,

00:58:10.020 --> 00:58:12.080
most of the time we're actually
not in our code at all.

00:58:12.080 --> 00:58:14.400
We're actually in the
driver for the GeForce3,

00:58:14.400 --> 00:58:16.880
this is the GeForce3
card on this machine.

00:58:16.880 --> 00:58:20.940
And actually, what we found is on other
machines with lesser graphics

00:58:20.940 --> 00:58:22.890
cards you hit this much quicker.

00:58:23.010 --> 00:58:26.970
So you actually, you know,
you may think that you need to, you know,

00:58:27.000 --> 00:58:30.970
in order to optimize this application you
need to keep beating on the CPU aspect,

00:58:31.090 --> 00:58:33.740
the, you know, that we're CPU bound.

00:58:33.740 --> 00:58:37.080
But in actuality,
our OpenGL colleagues have told

00:58:37.080 --> 00:58:39.830
us there are probably better
ways to be rendering this and,

00:58:39.840 --> 00:58:42.800
you know, that would be the next step.

00:58:43.700 --> 00:58:49.330
Okay, let's go back to the slides again,
the presentation.

00:59:01.030 --> 00:59:04.460
So in addition to the tools
Shikari and Amber and SimG4,

00:59:04.460 --> 00:59:08.780
that's the instruction simulator,
the cycle-accurate simulator that

00:59:08.780 --> 00:59:12.670
we've been showing you the output from,
these are the other tools,

00:59:12.670 --> 00:59:15.430
some of the other tools that
you get in the CHUD toolkit.

00:59:15.430 --> 00:59:17.440
Monster,
which is probably the most direct

00:59:17.690 --> 00:59:20.900
way you'd think about accessing
the performance counters.

00:59:20.900 --> 00:59:25.530
It's going to give you a tabular view,
like an Excel spreadsheet more or less,

00:59:25.530 --> 00:59:28.980
of the performance counter data,
and it can also give you a

00:59:28.980 --> 00:59:30.990
chart of that data over time.

00:59:31.500 --> 00:59:34.960
ACID is another way to filter
traces for performance statistics.

00:59:35.100 --> 00:59:39.040
And then Reggie is a way to look at
special purpose registers that are

00:59:39.040 --> 00:59:42.210
normally-- they're supervisor-only,
so you can't normally manipulate

00:59:42.210 --> 00:59:43.560
those from user space.

00:59:43.710 --> 00:59:47.640
And finally, all of our tools,
Monster and Shikari,

00:59:47.640 --> 00:59:50.640
and others are built on
top of the chud framework.

00:59:50.640 --> 00:59:53.280
And what you can do with this
framework is you can make

00:59:53.560 --> 00:59:57.130
your own performance tools,
or you can actually instrument your

00:59:57.240 --> 01:00:01.260
code to start and stop our tools,
or actually instrument your code

01:00:01.260 --> 01:00:03.490
and use the counters directly.

01:00:04.400 --> 01:00:11.460
So all these tools are currently
available on the web at this address.

01:00:11.460 --> 01:00:14.800
They will at some point also be
available on the developer CDs.

01:00:14.840 --> 01:00:19.140
And any issues you can report to
cheddtoolsfeedback@group.apple.com.

01:00:20.100 --> 01:00:23.000
And at this time,
I'm going to turn the presentation over

01:00:23.000 --> 01:00:26.750
to Mark Tozer for a few more slides.

01:00:26.750 --> 01:00:26.750
Thank you.

01:00:32.010 --> 01:00:32.220
Thank you.

01:00:32.280 --> 01:00:37.040
So I just want to clarify my earlier
comments at the opening that if you

01:00:37.040 --> 01:00:41.120
also understand or remember how to do
the square root function or symbol,

01:00:41.120 --> 01:00:44.130
you also qualify to be in this session,
not just being able to

01:00:44.130 --> 01:00:45.340
actually perform one.

01:00:45.380 --> 01:00:48.080
So roadmap, let's see,
other places where you can go since

01:00:48.130 --> 01:00:49.820
this is the beginning of the week.

01:00:49.880 --> 01:00:52.580
Some more performance tool
sessions that you should visit and

01:00:52.590 --> 01:00:54.150
learn more about the CHUD tools.

01:00:54.650 --> 01:00:57.900
Also more sessions on how to
optimize your application for

01:00:57.900 --> 01:01:02.840
multi-threaded applications,
specifically for MP systems themselves.

01:01:02.840 --> 01:01:08.060
One thing to know, Apple is shipping,
except for one product line, the iBook,

01:01:08.090 --> 01:01:10.900
every system is shipping
with a G4 processor with the

01:01:10.900 --> 01:01:11.910
introduction of the iMac.

01:01:11.920 --> 01:01:17.030
So the G4 and the Velocity Engine is
available for you on all of our systems

01:01:17.190 --> 01:01:22.040
that are available to our customers,
both in the consumer, the portable,

01:01:22.180 --> 01:01:23.670
and the PowerMac line.

01:01:26.590 --> 01:01:28.260
So who to contact for
further information?

01:01:28.260 --> 01:01:32.430
Here's my email,
and feel free to contact me.

01:01:33.020 --> 01:01:39.980
Documentation, again, developer.apple.com
slash hardware slash VE.

01:01:39.980 --> 01:01:43.720
Some more information with regards
to our scientific computing group.

01:01:43.720 --> 01:01:47.650
Also have some more information on
the BLAST information that worked,

01:01:47.650 --> 01:01:50.260
actually, that we've done with Genentech.

01:01:50.530 --> 01:01:53.440
That's also available off
that same page as a link.