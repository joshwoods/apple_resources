WEBVTT

00:00:00.610 --> 00:00:03.930
Ladies and gentlemen,
please welcome Technology Manager of

00:00:03.990 --> 00:00:08.210
Development Tools,
Godfrey DiGiorgi.

00:00:11.130 --> 00:00:11.580
Good afternoon.

00:00:11.580 --> 00:00:13.000
It's very good to see you all here again.

00:00:13.000 --> 00:00:14.680
Have a nice full house.

00:00:14.680 --> 00:00:16.680
I hope you've been having a great show.

00:00:19.170 --> 00:00:19.720
Fantastic.

00:00:19.850 --> 00:00:22.770
Applause.

00:00:22.870 --> 00:00:25.990
In this session,
we're going to show you probably what's

00:00:26.000 --> 00:00:29.600
considered the third major portion
of our Development Tools package.

00:00:29.680 --> 00:00:31.510
Number one,
the integrated development environment,

00:00:31.580 --> 00:00:33.250
which we showed you yesterday.

00:00:33.350 --> 00:00:36.600
Number two, the interface design program,
Interface Builder,

00:00:36.600 --> 00:00:38.720
which was in the session
just preceding this.

00:00:38.860 --> 00:00:41.070
And in this session,
the package of performance

00:00:41.070 --> 00:00:42.250
tools that we supply.

00:00:42.350 --> 00:00:45.690
And to do that,
I'm going to introduce Robert Bowdidge,

00:00:45.800 --> 00:00:48.920
the engineer for Performance Tools.

00:00:51.300 --> 00:00:51.960
Thank you, Godfrey.

00:00:51.960 --> 00:00:55.700
Thank you, Godfrey.

00:00:55.700 --> 00:00:57.500
Okay.

00:01:00.930 --> 00:01:04.190
So, before we start off,
let's come down to the point

00:01:04.200 --> 00:01:05.200
of exactly why you're here.

00:01:05.200 --> 00:01:07.470
And we all know you're here because
you want to learn about the tools.

00:01:07.570 --> 00:01:09.260
But why are you doing that?

00:01:09.260 --> 00:01:11.260
Well, the first reason that you want to
learn about the tools or why you

00:01:11.260 --> 00:01:14.010
should care about them is that
performance should actually be a

00:01:14.100 --> 00:01:15.980
selling point for your applications.

00:01:16.040 --> 00:01:18.500
If you have an application
that's slower than a competitor's

00:01:18.590 --> 00:01:21.140
but has the same features,
that forces users to work

00:01:21.140 --> 00:01:24.420
slower or doesn't allow them
to work with as big a data set,

00:01:24.590 --> 00:01:26.310
people are going to go
over to other applications.

00:01:26.450 --> 00:01:28.120
And so you want to make
sure your application is

00:01:28.120 --> 00:01:31.720
performing as good as it is,
or as it possibly can.

00:01:31.720 --> 00:01:34.650
Secondly, most of the performance
problems you're going to run

00:01:34.690 --> 00:01:36.280
into are going to be invisible.

00:01:36.320 --> 00:01:39.150
Apart from a few really egregious bugs,
you're probably not going to notice

00:01:39.150 --> 00:01:41.670
that your application is using
much more memory than it should.

00:01:41.700 --> 00:01:44.440
And until you actually start diving
in with the performance tools,

00:01:44.490 --> 00:01:46.970
you might not even notice that
you have performance problems.

00:01:47.060 --> 00:01:50.520
And so you need to care about this,
even if you don't think there's an issue.

00:01:51.130 --> 00:01:54.000
The third one, which may not be a reason
that you actually came here,

00:01:54.060 --> 00:01:59.000
is that your application
is not the entire world.

00:01:59.070 --> 00:02:01.550
If you're anything like me,

00:02:01.710 --> 00:02:05.890
When your application is running,
when customers are using it,

00:02:06.070 --> 00:02:08.440
they're not just going to
be using your application.

00:02:08.570 --> 00:02:10.750
What else are they going to be running?

00:02:13.870 --> 00:02:16.320
Okay, I think I know who's in
the front audience here.

00:02:16.480 --> 00:02:17.740
OmniWeb.

00:02:18.250 --> 00:02:19.860
They're going to be running, well,
maybe they'll be running

00:02:19.860 --> 00:02:21.120
Internet Explorer,
though I hope not.

00:02:21.540 --> 00:02:25.510
Maybe they're going to be running Mail.

00:02:25.510 --> 00:02:25.510
Maybe they're going to be...

00:02:26.010 --> 00:02:27.060
Maybe they're going to be running Mail.

00:02:27.060 --> 00:02:29.110
Maybe they're going to be playing iTunes.

00:02:29.170 --> 00:02:30.980
They're certainly going
to be playing iTunes.

00:02:30.980 --> 00:02:34.530
And the problem is that your application
is going to have to fight with all

00:02:34.610 --> 00:02:36.900
of those applications for memory use.

00:02:37.580 --> 00:02:41.220
And so you can't just judge how good your
app is by how it's performing on its own,

00:02:41.220 --> 00:02:43.420
but you need to make sure
it's as lean as possible,

00:02:43.490 --> 00:02:45.200
that it's using as little
memory as possible,

00:02:45.200 --> 00:02:47.450
that it's as efficient
with the CPU as possible,

00:02:47.520 --> 00:02:51.560
so that when I'm listening to iTunes,
my music doesn't skip.

00:02:51.560 --> 00:02:54.710
This is very important, at least to me.

00:02:57.610 --> 00:02:59.750
So how do you find these
performance problems?

00:02:59.960 --> 00:03:01.740
How do you actually find
out that there's a problem?

00:03:01.880 --> 00:03:03.840
And that's what I'm here
to tell you about today.

00:03:03.960 --> 00:03:07.520
What you're going to learn is that
Apple does have tools that will help you.

00:03:07.520 --> 00:03:11.200
It has tools--we at Apple actually
have tools that will help you

00:03:11.270 --> 00:03:13.580
identify that you may have a
performance problem and will help

00:03:13.660 --> 00:03:16.290
you track down exactly what that is.

00:03:16.360 --> 00:03:18.830
I'm going to go through some
very quick examples of how you

00:03:18.830 --> 00:03:20.850
might use some of these tools.

00:03:20.910 --> 00:03:23.020
The tools that you're going to see,
first from me,

00:03:23.020 --> 00:03:26.360
are going to be the tools that
are on the Developer Tools CD.

00:03:26.410 --> 00:03:28.170
After that,
we're going to have some folks from the

00:03:28.170 --> 00:03:31.660
hardware group tell you about some tools
for doing even lower-level analysis,

00:03:31.770 --> 00:03:35.080
for getting information from the
PowerPC's performance register

00:03:35.190 --> 00:03:38.040
so that you can actually optimize
every little bit of performance

00:03:38.120 --> 00:03:39.160
out of your application.

00:03:39.420 --> 00:03:40.760
These are tools that are
available for download.

00:03:40.860 --> 00:03:43.960
So these are all tools
that you can go off

00:03:44.640 --> 00:03:47.000
There's a couple things
you're not going to hear here.

00:03:47.080 --> 00:03:49.140
First of all, you're not going to get
a lot of details on,

00:03:49.140 --> 00:03:51.410
well, if you use this API,
you'll be in much better shape

00:03:51.410 --> 00:03:52.380
than if you use that API.

00:03:52.380 --> 00:03:55.160
If you want that information,
you need to go to the framework talks.

00:03:55.160 --> 00:03:57.230
You should have been
going to them all week.

00:03:57.230 --> 00:03:59.350
And they can give you
those sorts of details.

00:03:59.440 --> 00:04:01.290
Secondly,
this is just going to be a teaser.

00:04:01.360 --> 00:04:02.780
I'm not going to give you tutorials.

00:04:02.890 --> 00:04:05.610
You're probably not going to get enough
information here to really become

00:04:05.610 --> 00:04:06.980
experts at the performance tools.

00:04:07.250 --> 00:04:10.160
But hopefully you'll know that they
exist and you'll have a chance to

00:04:10.230 --> 00:04:13.400
understand how they might work so that
you can try using them on your own.

00:04:13.470 --> 00:04:16.940
Hopefully some of you will actually
be doing that as we go along today.

00:04:18.100 --> 00:04:21.300
So what are some of the
causes of poor performance?

00:04:21.380 --> 00:04:23.500
Well, one case is you have to
worry about memory use.

00:04:23.630 --> 00:04:26.750
Your application may be using
more memory than it should.

00:04:26.850 --> 00:04:29.920
You could be executing too much code,
either the wrong code or code

00:04:29.930 --> 00:04:33.230
that turns out to be more
expensive than it needs to be.

00:04:33.330 --> 00:04:35.700
You might be waiting for other devices.

00:04:35.730 --> 00:04:39.300
You may be waiting for the
disk to come back with events.

00:04:39.300 --> 00:04:43.160
You may be trying to draw and
talking with the Windows server,

00:04:43.160 --> 00:04:43.660
which is actually a separate process,
which actually does the drawing.

00:04:44.930 --> 00:04:49.410
And one of the key issues on any
system that has virtual memory is

00:04:49.420 --> 00:04:53.750
that pretty much all of these problems
eventually become memory problems.

00:04:54.060 --> 00:04:58.420
Because the code that you execute is
going to try to sit in physical memory.

00:04:58.420 --> 00:05:00.670
When you do drawing,
you're going to be using physical memory.

00:05:00.920 --> 00:05:04.340
When you allocate memory,
you're certainly using physical memory.

00:05:04.340 --> 00:05:06.280
And as a result, the

00:05:06.900 --> 00:05:09.150
When you use too much,
when you get to the point where you don't

00:05:09.150 --> 00:05:11.870
actually have enough RAM in your system,
you're going to get to the point

00:05:12.220 --> 00:05:14.500
where the operating system is
trying to be very nice to you.

00:05:14.500 --> 00:05:16.520
And what it'll do is
what's called paging.

00:05:16.650 --> 00:05:18.710
It'll take some of those
blocks out of memory and it'll

00:05:18.890 --> 00:05:20.030
throw them out onto the disk.

00:05:20.190 --> 00:05:22.540
It'll write them out onto disk
and bring something else in

00:05:22.540 --> 00:05:25.260
that needs to be done right now.

00:05:25.370 --> 00:05:27.430
And when your application
needs that memory again,

00:05:27.530 --> 00:05:29.940
it'll go back off to disk,
copy your stuff in,

00:05:29.940 --> 00:05:31.640
copy whatever else is out there.

00:05:31.760 --> 00:05:33.980
And as a result,
as soon as your application

00:05:34.070 --> 00:05:37.360
starts swapping,
as soon as it needs to page,

00:05:37.520 --> 00:05:39.500
your application will start
running at the speed of the disk,

00:05:39.500 --> 00:05:40.700
not at the speed of memory.

00:05:40.820 --> 00:05:44.160
Because to get that memory that
you carefully placed in RAM,

00:05:44.240 --> 00:05:47.300
you're actually going to have to go
off and do several disk accesses.

00:05:47.430 --> 00:05:51.910
So you want to minimize memory
use in all possible ways.

00:05:52.900 --> 00:05:54.650
So there's a number of tools available.

00:05:54.780 --> 00:05:56.750
Here I've divided them
into two categories.

00:05:56.990 --> 00:05:59.760
First,
the tools that are used for monitoring,

00:05:59.760 --> 00:06:01.680
for understanding that
you may have a problem,

00:06:01.680 --> 00:06:03.960
and the tools for doing deep analysis.

00:06:04.270 --> 00:06:06.720
And what I'm going to do is,
for this talk,

00:06:06.760 --> 00:06:09.950
go through a few of the tools in
each of the categories in terms

00:06:10.010 --> 00:06:12.930
of understanding memory use,
in terms of understanding

00:06:12.930 --> 00:06:16.240
execution behavior,
in terms of understanding resource use.

00:06:16.240 --> 00:06:20.190
Now, to do a good performance talk,
we need a victim.

00:06:20.710 --> 00:06:24.360
We need some program to analyze to
actually look for performance problems.

00:06:24.510 --> 00:06:28.230
And this year,
we are using a program called SimPhysics.

00:06:28.370 --> 00:06:30.780
And this is actually a project
that was written by one of the

00:06:30.850 --> 00:06:33.060
people in our Developer Tools group,
Scott Tooker.

00:06:33.060 --> 00:06:38.390
And I'd like to bring Scott up on stage
right now to actually demonstrate this.

00:06:41.830 --> 00:06:43.710
The two things to remember
as you see this demo is that,

00:06:43.720 --> 00:06:47.300
first of all,
this is a relatively realistic example.

00:06:47.360 --> 00:06:51.150
This is a program that shows
you electric field interactions.

00:06:51.200 --> 00:06:53.890
It was intended to teach
students on physics.

00:06:53.930 --> 00:06:56.240
Can we actually switch
to the demo machine?

00:06:56.240 --> 00:06:58.240
Yes, please demo.

00:06:58.240 --> 00:06:59.450
Thank you.

00:06:59.570 --> 00:07:01.160
First of all,
this is a realistic example.

00:07:01.200 --> 00:07:05.090
This is something that Scott actually
developed for classroom use.

00:07:05.250 --> 00:07:08.400
And second, it was actually an app that
was ported from Java to Cocoa.

00:07:08.400 --> 00:07:12.070
And so there's a number of
inefficiencies caused there.

00:07:12.850 --> 00:07:14.800
Okay, so here's the view.

00:07:14.800 --> 00:07:18.100
What you see here are three particles,
positively charged and

00:07:18.100 --> 00:07:22.510
negatively charged in a normal,
or a neutral particle.

00:07:22.560 --> 00:07:24.600
Scott can

00:07:25.010 --> 00:07:28.390
Display the information about
these particles in several ways.

00:07:28.580 --> 00:07:31.570
Here we're showing intensity
according to color.

00:07:31.790 --> 00:07:36.640
The first, it also can show in terms of a
vector graph where you can see

00:07:36.640 --> 00:07:42.110
at each point where the charge is
or which way the charge is going.

00:07:42.930 --> 00:07:45.600
And one thing that we can see here is
that there may be a performance problem,

00:07:45.750 --> 00:07:48.740
that as Scott drags that particle,
the redraw rate tends

00:07:48.740 --> 00:07:50.160
to be relatively slow.

00:07:50.160 --> 00:07:53.990
And so this may imply that we actually
have something that we need to deal with.

00:07:54.000 --> 00:07:55.280
Okay.

00:07:55.280 --> 00:08:00.460
Can we switch back to the slides, please?

00:08:03.990 --> 00:08:06.200
Oh, and one other thing that
you should know about this.

00:08:06.350 --> 00:08:08.740
This,
although this is a realistic example,

00:08:08.780 --> 00:08:11.080
with some of the problems in
here are things that you might

00:08:11.190 --> 00:08:12.630
actually find in your own code.

00:08:12.750 --> 00:08:13.830
Not all the problems
are actually Scott's.

00:08:13.980 --> 00:08:15.940
There were actually a number
that we actually salted into the

00:08:15.940 --> 00:08:17.060
program to make a better demo.

00:08:17.060 --> 00:08:21.940
So if you see anything
that looks really nasty,

00:08:21.940 --> 00:08:21.940
it's probably my fault
because I put it in.

00:08:22.560 --> 00:08:23.640
Okay.

00:08:23.690 --> 00:08:26.420
So how do we actually
find performance problems?

00:08:26.480 --> 00:08:28.820
Well, we found one, we had one way there.

00:08:28.820 --> 00:08:31.590
We looked at SimPhysics,
we dragged and we said, gee,

00:08:31.590 --> 00:08:34.480
that drawing rate seems a little slow.

00:08:34.480 --> 00:08:36.670
So one way that you can track
down performance problems is just

00:08:36.670 --> 00:08:40.180
notice things that are egregious,
that are just too ugly to live,

00:08:40.180 --> 00:08:42.010
that you need to deal with.

00:08:42.530 --> 00:08:45.220
But that's not the only way.

00:08:45.220 --> 00:08:47.860
In fact,
what you probably need to do in order

00:08:47.860 --> 00:08:51.460
to really do a good job of performance
analysis is you need to measure.

00:08:51.490 --> 00:08:55.390
You need to actually
find out how much memory,

00:08:55.700 --> 00:08:59.320
how your application needs,
how quickly it needs to redraw,

00:08:59.320 --> 00:09:02.270
and so on,
so that you can at least compare.

00:09:02.570 --> 00:09:04.340
So in the case of SimPhysics,
one of the things we did which was

00:09:04.420 --> 00:09:07.570
not displayed there was we actually
kept track of the frame rate.

00:09:07.610 --> 00:09:10.240
So we could see,
as Scott dragged a particle,

00:09:10.280 --> 00:09:12.200
how many frames per second he could draw.

00:09:12.200 --> 00:09:15.890
We could then write down that number and
actually compare before against after

00:09:15.940 --> 00:09:17.740
and decide whether we did a good job.

00:09:20.620 --> 00:09:23.850
Now, the normal way that I actually
prefer to do analysis,

00:09:23.850 --> 00:09:26.170
though,
to at least find the nastier bugs,

00:09:26.250 --> 00:09:28.470
is to use a command-line tool called Top.

00:09:28.610 --> 00:09:29.830
Could we switch to the
demo machine again?

00:09:35.960 --> 00:09:40.880
Okay, so here we have
Scott actually running TOP.

00:09:41.140 --> 00:09:44.630
And this is a command line tool
that's available on the system.

00:09:44.690 --> 00:09:47.800
The output from TOP can be
divided into two categories.

00:09:47.850 --> 00:09:50.050
At the top is the information
that's about the system.

00:09:50.060 --> 00:09:52.560
It tells us how many processors
are currently running,

00:09:52.640 --> 00:09:54.930
how much of the CPU is
actually being used.

00:09:55.070 --> 00:09:58.400
The most interesting line
I find here is the bottom line,

00:09:58.640 --> 00:10:01.000
where we see the number
of page ins and page outs.

00:10:01.060 --> 00:10:04.190
That represents the number of pages that
are having to be taken out of memory

00:10:04.200 --> 00:10:07.500
and written to disk because other stuff
is needed to be put into that memory,

00:10:07.720 --> 00:10:10.420
and the number of pages that are being
brought off of disk and into memory

00:10:10.760 --> 00:10:13.200
because we need that memory again.

00:10:13.220 --> 00:10:16.540
So this number gives you an idea
about how much paging you're doing.

00:10:16.690 --> 00:10:20.150
The main number, such as the 4785,
represents the number of

00:10:20.280 --> 00:10:23.440
pages that have been paged out
since the system was booted.

00:10:23.580 --> 00:10:27.440
The number in parentheses represents the
number of pages per second that changed.

00:10:27.560 --> 00:10:30.360
And so if you ever see that
number in parentheses ever go,

00:10:30.360 --> 00:10:34.000
let's say, above 100 or 200 for an
extended period of time,

00:10:34.120 --> 00:10:37.760
that usually implies that your
system is doing nothing but paging.

00:10:37.760 --> 00:10:40.720
All it's doing is writing out pages and
then immediately doing some code which

00:10:40.830 --> 00:10:43.760
causes another page to be brought in,
and all it's doing is writing pages

00:10:43.760 --> 00:10:47.240
out to memory-- or out to disk and
bringing them in off of memory.

00:10:47.240 --> 00:10:49.300
And that usually implies that
your application just plain

00:10:49.430 --> 00:10:52.760
doesn't have enough memory for all
the things you're trying to do.

00:10:53.090 --> 00:10:56.810
The second part about Top that's
interesting is the bottom part,

00:10:56.930 --> 00:11:00.000
which is focused on
each individual process.

00:11:00.120 --> 00:11:02.980
And we see here that there's a
column that shows the percentage

00:11:03.010 --> 00:11:06.440
of CPU used by each process,
so we can see which ones

00:11:06.560 --> 00:11:08.040
are particularly expensive.

00:11:08.250 --> 00:11:11.550
The second column that's
interesting is the Rprivate,

00:11:11.620 --> 00:11:13.540
two-thirds of the way over.

00:11:13.610 --> 00:11:17.280
Rprivate stands for the amount
of resident private memory

00:11:17.280 --> 00:11:19.090
used by that application.

00:11:19.230 --> 00:11:21.660
Resident meaning it's
actually physically in memory,

00:11:21.720 --> 00:11:25.040
so it gives you an idea about the
footprint of the application in memory.

00:11:25.040 --> 00:11:29.100
And private because that memory is
only for use by that application.

00:11:29.100 --> 00:11:31.930
It's not used by any other applications.

00:11:31.940 --> 00:11:34.930
So we can see here that SymPhysics
currently is not doing anything,

00:11:34.990 --> 00:11:37.040
so it's using 0.0 percent of the CPU.

00:11:37.100 --> 00:11:38.080
That's very good.

00:11:38.160 --> 00:11:41.190
We would prefer that SymPhysics is not
doing stuff while my iTunes is playing.

00:11:42.770 --> 00:11:46.820
and we see that its private
memory is about 1.3 megabytes.

00:11:46.860 --> 00:11:49.970
And Scott,
can you actually move something around

00:11:50.390 --> 00:11:53.600
while you're--just so we see it active?

00:11:53.650 --> 00:11:59.970
Actually,
do the vector graph if you could.

00:11:59.970 --> 00:11:59.970
Okay.

00:12:04.700 --> 00:12:06.160
Interesting.

00:12:06.220 --> 00:12:10.420
So we can see here that the private
memory size is about two megabytes,

00:12:10.460 --> 00:12:11.550
which seems a little high.

00:12:11.560 --> 00:12:14.460
I mean, calculators shouldn't take more
than like about a megabyte,

00:12:14.460 --> 00:12:15.770
so that seems a little odd.

00:12:15.960 --> 00:12:17.980
The second thing we can
see is as we're redrawing,

00:12:18.000 --> 00:12:19.070
we're actually pegging the CPU.

00:12:19.070 --> 00:12:21.880
We're using 100 percent of the
CPU to actually do that drawing,

00:12:21.880 --> 00:12:24.940
which implies that drawing
is pretty expensive.

00:12:24.940 --> 00:12:26.300
Thank you.

00:12:26.300 --> 00:12:28.370
Can we go back to slides?

00:12:30.900 --> 00:12:34.390
Okay, so most important things,
look at the CPU usage for each process,

00:12:34.390 --> 00:12:37.350
look at the swapping and paging rate,
look at our private and top.

00:12:37.400 --> 00:12:38.890
Try to keep it open.

00:12:38.890 --> 00:12:40.000
It's very interesting.

00:12:40.010 --> 00:12:41.760
And as you heard,
our vice president actually

00:12:41.760 --> 00:12:43.970
uses it on our applications,
so he must be seeing

00:12:43.970 --> 00:12:45.580
something interesting.

00:12:47.710 --> 00:12:50.270
Okay,
so we looked at SimPhysics there and

00:12:50.280 --> 00:12:56.010
we saw that it was using about 2.2
megabytes of memory when it was running.

00:12:56.150 --> 00:12:58.540
This seems a little high.

00:12:59.300 --> 00:13:01.240
However, we didn't see if it was
growing or anything.

00:13:01.250 --> 00:13:03.660
You may ask, well, you know,
was--did we have a leak?

00:13:03.660 --> 00:13:07.050
Were we actually allocating an extra
100,000 bytes every minute or so?

00:13:07.100 --> 00:13:09.810
And you couldn't really see that on top.

00:13:09.820 --> 00:13:13.090
However, even though we don't quite
have a tool for doing that,

00:13:13.090 --> 00:13:15.220
there's ways that you can do that.

00:13:15.250 --> 00:13:18.560
Because a number of the performance tools
are actually Unix command-line tools,

00:13:18.560 --> 00:13:21.900
and that is very advantageous because
that means that you can pretty much

00:13:21.910 --> 00:13:25.360
roll your own little performance tools
as you need to for specific tasks.

00:13:25.570 --> 00:13:28.180
For example,
if we wanted to see if SimPhysics

00:13:28.250 --> 00:13:31.110
was actually leaking memory,
we could write a little script

00:13:31.110 --> 00:13:34.300
that would get the output from top,
it would search for the

00:13:34.300 --> 00:13:36.460
line for SimPhysics,
save that to a file,

00:13:36.460 --> 00:13:38.170
and then do that every second.

00:13:38.240 --> 00:13:40.160
And then we could look at that
file over time and see whether we

00:13:40.160 --> 00:13:42.040
actually saw a change in memory use.

00:13:42.040 --> 00:13:47.000
Now, these command-line tools
don't seem Mac-like.

00:13:48.130 --> 00:13:49.000
They're very Unix-like.

00:13:49.000 --> 00:13:49.460
They're very ugly.

00:13:49.460 --> 00:13:50.500
You're dealing with text.

00:13:50.590 --> 00:13:52.100
However,
they do have the advantage that you're

00:13:52.240 --> 00:13:54.910
able to do customized data collection.

00:13:55.520 --> 00:13:58.530
And these tools still work even
if you're running them remotely

00:13:58.530 --> 00:14:00.800
from a terminal because you
don't want to upset the screen,

00:14:00.800 --> 00:14:04.640
perhaps because you're developing a game,
or if your system is hung and the

00:14:04.640 --> 00:14:08.340
Windows server won't respond and you're
trying to find out what's going on.

00:14:08.340 --> 00:14:13.090
So command-line tools do have a
purpose in performance analysis.

00:14:16.110 --> 00:14:18.720
So we think that sim physics
may have too much memory.

00:14:18.780 --> 00:14:20.260
Why could that be?

00:14:20.310 --> 00:14:22.110
Well,
one reason is that sim physics could

00:14:22.110 --> 00:14:23.820
be allocating huge amounts of memory.

00:14:23.820 --> 00:14:26.350
It could be allocating large structures.

00:14:26.430 --> 00:14:31.030
A second possible concern
for applications is caching.

00:14:31.440 --> 00:14:35.400
So let's think of a
hypothetical case here.

00:14:35.430 --> 00:14:36.580
I have an application.

00:14:36.620 --> 00:14:39.820
It wants to read a bunch
of data in off of disk.

00:14:39.820 --> 00:14:42.540
So it reads the data in off of disk,
puts it in memory,

00:14:42.560 --> 00:14:44.760
and then says this memory
is so important to me,

00:14:44.760 --> 00:14:47.880
this information is so important,
that I don't want to have to read

00:14:47.880 --> 00:14:49.080
it off of disk when I need it.

00:14:49.310 --> 00:14:51.590
Instead, I'll read it at the start,
save it into memory,

00:14:51.590 --> 00:14:54.560
and then when I actually need it,
I'll go to memory and get it,

00:14:54.640 --> 00:14:56.370
because that'll be faster.

00:14:56.590 --> 00:14:58.400
Sounds good, right?

00:14:59.650 --> 00:15:03.690
So the problem is,
let's imagine that iTunes starts up,

00:15:03.690 --> 00:15:06.040
or Internet Explorer, or OmniWeb,
or Mail,

00:15:06.040 --> 00:15:09.730
and a bunch of those pages get chased
off to disk because that memory's needed

00:15:09.730 --> 00:15:11.640
by one of the current applications.

00:15:11.640 --> 00:15:14.960
So that memory you've carefully--that
data you've carefully put in memory

00:15:14.960 --> 00:15:19.160
so that you didn't have to do a disk
read suddenly gets written out to disk.

00:15:19.200 --> 00:15:23.790
And when you actually go to access it,
you need to go to the disk again.

00:15:24.100 --> 00:15:27.060
And so to save a single disk
access close to the use,

00:15:27.060 --> 00:15:30.420
you end up doing three disk
accesses over the--throughout

00:15:30.570 --> 00:15:32.240
the lifetime of the application.

00:15:32.280 --> 00:15:33.220
That's not very efficient.

00:15:33.290 --> 00:15:36.330
So you want to make sure that
you're not out creating data,

00:15:36.330 --> 00:15:39.270
loading it in,
and keeping it on hand just in case.

00:15:39.300 --> 00:15:43.960
Try to cut your memory use by
getting rid of those caching cases.

00:15:44.400 --> 00:15:46.090
The third case is that you
could be leaking memory.

00:15:46.090 --> 00:15:47.890
You could be allocating it
and forgetting about it,

00:15:47.890 --> 00:15:50.120
increasing your footprint.

00:15:50.880 --> 00:15:54.200
As I said at the intro,
any of this excessive memory use

00:15:54.380 --> 00:15:57.010
will eventually lead to the fact that
you're going to be doing disk accesses,

00:15:57.080 --> 00:15:59.630
and it'll slow your application
and you don't want to do it.

00:15:59.640 --> 00:16:01.840
So keep your memory as small as possible.

00:16:01.990 --> 00:16:03.850
But how do you do that?

00:16:04.780 --> 00:16:08.620
So the tool for doing that is
a tool called malloc-debug.

00:16:08.710 --> 00:16:12.390
It's interest--actually,
there's two tools, I should say.

00:16:12.650 --> 00:16:15.560
The first tool, malloc-debug,
allows you to look at allocations

00:16:15.590 --> 00:16:19.240
according to where they were allocated,
where in your program.

00:16:19.320 --> 00:16:24.200
The second program, called object-alloc,
treats--analyses memory by looking at how

00:16:24.370 --> 00:16:26.540
many objects of each type were created.

00:16:26.690 --> 00:16:28.260
And what I'll do is I'll look
at each of these in turn.

00:16:28.260 --> 00:16:29.480
Okay.

00:16:29.480 --> 00:16:34.970
First,
let's switch over to the demo screen.

00:16:39.110 --> 00:16:43.130
Okay,
so Scott started up malloc-debug here,

00:16:43.230 --> 00:16:46.930
and he will create a new window and
select the SimPhysics program as

00:16:47.020 --> 00:16:49.010
the app that he wants to launch.

00:16:59.390 --> 00:17:02.600
Okay, and he launches the program.

00:17:02.600 --> 00:17:06.720
Now, what you can see in malloc-debug is,
first of all,

00:17:06.730 --> 00:17:10.880
up in the upper right-hand corner,
we can see that we're using

00:17:10.930 --> 00:17:12.800
about 1.6 megabytes of

00:17:16.340 --> 00:17:17.300
Actually, go back a little.

00:17:17.300 --> 00:17:19.860
Or actually, no, that's perfect.

00:17:19.970 --> 00:17:20.360
Never mind.

00:17:20.360 --> 00:17:24.080
So according to this,
SimPhysics is using about 1.6 megabytes

00:17:24.180 --> 00:17:26.190
of memory that it allocated via malloc.

00:17:26.370 --> 00:17:30.060
Malloc debug is only showing us the
memory that we're allocating on the heap.

00:17:30.060 --> 00:17:33.980
The middle section of the window
here shows us a call tree.

00:17:34.080 --> 00:17:37.660
It describes where in the
program we called malloc.

00:17:37.660 --> 00:17:40.240
So in this case,
what we found is that 1.6

00:17:40.290 --> 00:17:43.590
megabytes of that memory was
allocated in main and below,

00:17:43.590 --> 00:17:45.980
in the things that are called from main.

00:17:46.000 --> 00:17:48.250
Now,
because this is an Objective-C program,

00:17:48.250 --> 00:17:51.220
main immediately calls the
function NSApplicationMain.

00:17:51.220 --> 00:17:53.930
And NSApplicationMain
calls several functions,

00:17:53.930 --> 00:17:57.000
each responsible for,
or each allocating some

00:17:57.010 --> 00:18:00.380
memory below that point,
with most of the memory actually

00:18:00.380 --> 00:18:02.540
allocated with NSApplicationRun.

00:18:02.540 --> 00:18:06.330
So malloc debug is trying to help us look
at our program in terms of its structure,

00:18:06.330 --> 00:18:07.630
in terms of what it calls.

00:18:07.700 --> 00:18:09.320
Now,
this is really nice if you've written a

00:18:09.320 --> 00:18:11.100
program with a functional decomposition.

00:18:11.140 --> 00:18:14.110
That is, first you read in the data,
then you process the data,

00:18:14.110 --> 00:18:17.620
then you print out the data,
then you close the data.

00:18:17.620 --> 00:18:20.830
In that way, malloc debug will tell you
exactly how much memory is used

00:18:20.830 --> 00:18:22.560
in each phase of your program.

00:18:22.570 --> 00:18:24.980
Now,
SymPhysics is actually a Cocoa program.

00:18:24.990 --> 00:18:29.360
And so the idea of an object-oriented
program with lots of objects calling

00:18:29.360 --> 00:18:33.120
around means that looking from the top of
the call tree down may not be so helpful.

00:18:33.120 --> 00:18:37.980
Instead, Scott can switch from showing
the standard way of things,

00:18:37.980 --> 00:18:41.300
from a top-down view of the tree,
to an inverted view of the tree.

00:18:41.300 --> 00:18:45.340
So rather than looking at how much
memory was allocated in main and below,

00:18:45.340 --> 00:18:48.160
we can see how much memory
was allocated via malloc,

00:18:48.220 --> 00:18:51.090
and who called malloc,
and how much memory was allocated there.

00:18:51.320 --> 00:18:54.970
And we can see here that about 1.3
megabytes of the memory on the heap was

00:18:54.970 --> 00:18:59.160
allocated in calls to NS--NXZoneMalloc,
which is one of the wrapper

00:18:59.210 --> 00:19:01.110
functions around malloc.

00:19:01.140 --> 00:19:06.040
And 904K of that, interestingly,
turns out to have been

00:19:06.040 --> 00:19:08.490
allocated in an NSBitmap image.

00:19:10.120 --> 00:19:12.560
and of that 904K,
which is about half of the

00:19:12.630 --> 00:19:16.360
memory in this program,
all of that was allocated in two classes,

00:19:16.380 --> 00:19:18.180
the general graph and the color graph.

00:19:18.240 --> 00:19:21.000
Okay, so when Scott and I looked at this,
we said, "Hmm,

00:19:21.010 --> 00:19:26.670
this is interesting." It turns out that
those bitmaps are used to draw the image

00:19:26.670 --> 00:19:29.830
on the screen of the particles and the

00:19:30.510 --> 00:19:34.220
So this is the bitmap that you
actually see drawn on the screen.

00:19:34.390 --> 00:19:36.020
And actually,
it turns out that what ends up--the

00:19:36.030 --> 00:19:39.840
reason why there's more than one here
is that it turns out that SimPhysics,

00:19:39.970 --> 00:19:43.940
in order to be clever about caching,
tried to allocate both

00:19:43.940 --> 00:19:46.640
of those data structures,
or both of those bitmaps,

00:19:46.640 --> 00:19:49.340
at the launch of the program,
even before we'd chosen

00:19:49.340 --> 00:19:52.400
which view we wanted,
whether we wanted the general graph,

00:19:52.400 --> 00:19:55.140
which showed the most simple view,
or the color graph,

00:19:55.310 --> 00:19:57.020
showing us that nice shaded view.

00:19:57.020 --> 00:20:00.400
So we were allocating structures
just in case we needed them.

00:20:01.350 --> 00:20:03.600
And as a result,
we're using half of our memory in

00:20:03.600 --> 00:20:06.800
memory that we haven't even touched yet,
probably.

00:20:06.800 --> 00:20:07.490
So this is bad.

00:20:07.490 --> 00:20:09.560
This indicates something
that perhaps we should fix.

00:20:09.610 --> 00:20:12.750
Maybe we should only allocate
those when we actually need them.

00:20:14.610 --> 00:20:17.320
Okay, Malik Debug has a number
of other features.

00:20:17.420 --> 00:20:19.330
One thing that you can do is
you can actually set a mark

00:20:19.360 --> 00:20:22.700
by pressing the Mark button,
marking a point in time.

00:20:22.830 --> 00:20:25.260
And then you can actually change
from showing all nodes to only

00:20:25.280 --> 00:20:28.540
showing the things that have been
created since the mark was pressed.

00:20:28.590 --> 00:20:35.420
And this gives you a way to actually
identify all the allocations that

00:20:35.420 --> 00:20:35.420
occur during a specific point in time.

00:20:35.420 --> 00:20:35.420
And you don't have to do this.

00:20:36.340 --> 00:20:40.820
Another mode that mallocdebug has
is the ability to track down leaks.

00:20:40.820 --> 00:20:43.200
And so Scott can actually
choose the definite leaks mode.

00:20:43.300 --> 00:20:45.600
Show me only the things that
are leaked data structures.

00:20:45.660 --> 00:20:48.550
What mallocdebug does here is
it goes scanning through the

00:20:48.670 --> 00:20:51.400
memory of the application,
looking for things that

00:20:51.510 --> 00:20:52.520
look like pointers.

00:20:52.520 --> 00:20:54.680
And for everything that
looks like a pointer,

00:20:54.680 --> 00:20:57.780
it tries to decide if that pointer
actually points to a malloc block.

00:20:57.850 --> 00:21:00.280
And if so,
it marks that malloc block as reachable.

00:21:00.340 --> 00:21:04.070
And then what it does is it shows
us only the things that aren't

00:21:04.300 --> 00:21:06.550
And now that it's done it,
let's see what it found.

00:21:06.640 --> 00:21:11.680
Well, it finds that we've got about
900K of memory that's leaked.

00:21:11.980 --> 00:21:15.260
And it turns out it was both of those
data structures that we noticed.

00:21:15.360 --> 00:21:18.530
So not only were we creating them
and then forget--and then--or

00:21:18.750 --> 00:21:21.960
creating them just in case,
but we were then losing them.

00:21:21.960 --> 00:21:24.360
Because leak detection
is not just saying,

00:21:24.490 --> 00:21:25.960
maybe you don't use this.

00:21:25.960 --> 00:21:28.610
We don't even have pointers to
these data structures anywhere.

00:21:28.660 --> 00:21:30.300
And so there's no way
we could even call free.

00:21:30.300 --> 00:21:35.060
So this memory is wasted and it will stay
around until the application finishes.

00:21:36.450 --> 00:21:38.720
So by tracking down leaks,
we can actually cut our memory use,

00:21:38.850 --> 00:21:40.620
and in this case,
we can cut our memory use by half.

00:21:40.620 --> 00:21:41.930
And that's very nice.

00:21:43.820 --> 00:21:46.360
Do you want me to show the memory view?

00:21:46.360 --> 00:21:48.570
Actually, why don't you do that as well?

00:21:48.690 --> 00:21:49.680
Thank you.

00:21:49.750 --> 00:21:51.760
You can also take one of
the buffers that you found,

00:21:51.780 --> 00:21:53.400
and if you actually
care about the contents,

00:21:53.400 --> 00:21:54.940
for example,
you want to understand exactly

00:21:55.020 --> 00:21:56.900
what string's being leaked,
you can actually double-click

00:21:56.980 --> 00:22:00.120
on something in the lower half,
which represents the actual allocations

00:22:00.120 --> 00:22:03.110
at that point in the call tree,
and you can actually get a hex dump

00:22:03.370 --> 00:22:04.830
showing you details about that.

00:22:04.900 --> 00:22:06.260
Thank you, Scott.

00:22:06.260 --> 00:22:08.210
Could we go back to slides?

00:22:10.470 --> 00:22:12.100
Okay,
and one last thing I should mention,

00:22:12.140 --> 00:22:14.510
malloc-debug is only showing
you the current allocations,

00:22:14.520 --> 00:22:16.680
so that as you free things,
those will actually disappear.

00:22:21.400 --> 00:22:24.570
A third thing that malloc debug
does that's actually very helpful

00:22:24.770 --> 00:22:28.220
is it can help you with debugging,
not just analysis.

00:22:28.220 --> 00:22:31.180
For example,
pointer bugs can be particularly nasty.

00:22:31.240 --> 00:22:34.780
They tend to be insidious,
they tend to be subtle, they tend to only

00:22:34.780 --> 00:22:36.160
happen in certain cases.

00:22:36.170 --> 00:22:40.060
And so you end up with cases where
your program just crashes because

00:22:40.110 --> 00:22:44.260
suddenly you're trying to access
some variable that's been trashed.

00:22:44.260 --> 00:22:46.360
For example,
you may have a case where your

00:22:46.360 --> 00:22:50.720
application frees memory but continues
to write to that memory or read from that

00:22:50.830 --> 00:22:54.640
memory until whoever now owns the memory
decides to read the value you wrote,

00:22:54.710 --> 00:22:57.430
which has corrupted their data,
or writes to those values that

00:22:57.430 --> 00:22:58.910
you thought were so interesting.

00:23:00.690 --> 00:23:03.050
Another case where you can
have problems with pointer

00:23:03.050 --> 00:23:05.840
bugs is if you overrun buffers,
if you write past the end of

00:23:05.840 --> 00:23:08.360
an array and keep writing on
to the next data structure,

00:23:08.500 --> 00:23:09.680
trashing memory.

00:23:09.780 --> 00:23:13.590
Malictabug gives you some ways to
actually track down such problems.

00:23:13.870 --> 00:23:15.940
The way that it does it is
that mallocdebug actually

00:23:15.960 --> 00:23:18.190
uses its own malloc library,
and it actually turns on

00:23:18.190 --> 00:23:19.450
a couple special features.

00:23:19.570 --> 00:23:22.450
One of those is that every time
that you free a block of memory,

00:23:22.570 --> 00:23:27.300
it overwrites it with a garbage value,
55 hex, so that if a program continues

00:23:27.300 --> 00:23:30.850
to access that memory,
hopefully the program will get

00:23:30.890 --> 00:23:32.970
garbage values and will crash.

00:23:33.010 --> 00:23:35.010
mallocdebug also provides guard words.

00:23:35.080 --> 00:23:38.120
It places a special word on each
end of a buffer so that mallocdebug

00:23:38.120 --> 00:23:41.410
can actually detect whether that
buffer has been overrun or underrun,

00:23:41.440 --> 00:23:43.780
whether you've written before the
beginning of it or after the end.

00:23:43.780 --> 00:23:47.100
In mallocdebug itself,
you can track down the overruns and

00:23:47.170 --> 00:23:50.790
underruns by looking for trash nodes.

00:23:51.500 --> 00:23:54.560
However, the other fix,
or the other trick,

00:23:54.620 --> 00:23:59.090
of overwriting free memory will
actually cause applications to

00:23:59.090 --> 00:24:01.520
crash when you do stupid things.

00:24:01.520 --> 00:24:04.620
And so if your program crashes in
malloc-debug but doesn't crash normally,

00:24:04.620 --> 00:24:07.740
you should attach with a debugger and try
to check out what's going on with that,

00:24:07.820 --> 00:24:11.170
because it generally means that your
program has a pointer problem that you

00:24:11.170 --> 00:24:14.440
should try to track down so that you
can make your program more reliable.

00:24:14.480 --> 00:24:16.990
malloc-debug also prints out a
number of warning messages to the

00:24:16.990 --> 00:24:20.650
console in an extremely helpful way,
or to the standard output.

00:24:20.660 --> 00:24:22.900
So check the standard output
when you're using malloc-debug

00:24:23.080 --> 00:24:26.720
for tracking down memory bugs.

00:24:30.500 --> 00:24:32.110
Okay,
the second of the tools for analyzing

00:24:32.110 --> 00:24:33.810
memory use is called ObjectAlloc.

00:24:33.830 --> 00:24:38.400
And ObjectAlloc is interesting because
rather than trying to describe objects

00:24:38.400 --> 00:24:41.630
according to where they were allocated,
it tries to allocate or it

00:24:41.690 --> 00:24:45.780
tries to determine how many
objects have been allocated.

00:24:45.780 --> 00:24:48.130
Because sometimes it's easier
to reason about certain types of

00:24:48.130 --> 00:24:50.720
pointer problems by noticing that.

00:24:50.720 --> 00:24:53.430
So can we switch back to the demo screen,
please?

00:24:55.190 --> 00:25:00.810
So once again, Scott can launch
SimPhysics in ObjectAlloc,

00:25:00.810 --> 00:25:02.840
and then he can start it running.

00:25:04.090 --> 00:25:09.750
And what you should see here is that, um,

00:25:10.370 --> 00:25:12.040
Immediately this graph starts appearing.

00:25:12.110 --> 00:25:14.270
What ObjectAlloc is showing us here is,
first of all,

00:25:14.270 --> 00:25:18.370
how many objects of each type,
each Objective-C type, currently exist.

00:25:18.460 --> 00:25:19.680
That's what the current column is.

00:25:19.690 --> 00:25:22.050
It'll also show the peak number
of objects of each type that have

00:25:22.090 --> 00:25:24.470
existed during the lifetime of
the program and the total number

00:25:24.470 --> 00:25:25.940
of objects that have existed.

00:25:25.940 --> 00:25:29.660
And it also shows a histogram to give us
a graphical way to actually detect that,

00:25:29.720 --> 00:25:31.860
so that we might be
able to detect trends.

00:25:31.920 --> 00:25:36.920
For example, is the number of arrows
increasing over time?

00:25:38.080 --> 00:25:42.870
So one question we could easily ask
ourselves in terms of performance is,

00:25:42.940 --> 00:25:48.130
are we correctly actually freeing all the
data structures when we close Windows?

00:25:48.210 --> 00:25:52.590
So Scott, for example,
could create several SymPhysics Windows.

00:25:54.900 --> 00:25:57.560
and then destroy those windows.

00:25:57.560 --> 00:26:02.040
Now, for each of those windows,
we had to create several data structures.

00:26:02.210 --> 00:26:04.710
And Scott can sort the list by category,
and then we can go and

00:26:04.710 --> 00:26:05.560
take a look at a couple.

00:26:05.560 --> 00:26:07.730
For example, the camera view,
or the camera.

00:26:07.810 --> 00:26:11.520
There should be exactly one camera
and one camera view per window.

00:26:11.520 --> 00:26:14.920
So at a peak,
we should have seen six objects,

00:26:14.920 --> 00:26:15.980
and currently we should see one.

00:26:15.980 --> 00:26:20.160
However, currently, right now,
what we see is six objects of each type.

00:26:20.290 --> 00:26:22.370
This implies that as we
were closing those windows,

00:26:22.370 --> 00:26:25.410
we were forgetting to actually
free up those data structures.

00:26:25.460 --> 00:26:29.280
And so this implies that we actually
have a bug in our cleanup routines.

00:26:31.290 --> 00:26:36.860
Object ALEC also has
another neat feature.

00:26:36.990 --> 00:26:38.760
If you're a beginning
Objective-C programmer,

00:26:38.760 --> 00:26:41.400
you've often probably been
caught by cases where you

00:26:41.400 --> 00:26:43.060
release an object too many times.

00:26:43.110 --> 00:26:45.700
And on the last release,
when you're not expecting it,

00:26:45.760 --> 00:26:48.180
suddenly your program crashes
when you go to look for an

00:26:48.250 --> 00:26:49.990
object that doesn't really exist.

00:26:50.070 --> 00:26:53.830
Object ALEC also gives you a way
to log each retain and release.

00:26:53.830 --> 00:26:56.620
And then you can go
deeper into Object ALEC,

00:26:56.650 --> 00:27:00.760
ask about specific objects,
and find every retain and release

00:27:00.760 --> 00:27:04.190
that's been done on that object so
that you can try to understand what

00:27:04.210 --> 00:27:08.220
the retain count should be and why
an object is being freed before it,

00:27:08.220 --> 00:27:09.560
before the end.

00:27:09.610 --> 00:27:11.390
Back to the slides, please.

00:27:18.800 --> 00:27:23.540
Back to the slides, please.

00:27:23.660 --> 00:27:25.930
They really like my program.

00:27:30.240 --> 00:27:31.700
Could we please go back to these slides?

00:27:31.860 --> 00:27:33.690
Thank you.

00:27:34.250 --> 00:27:36.320
Okay, so those are memory problems.

00:27:36.460 --> 00:27:39.280
The second category of
problems is CPU usage.

00:27:39.280 --> 00:27:43.250
So what are the possible problems you
can have where you're executing code?

00:27:43.250 --> 00:27:45.000
Well,
one thing you could be doing is you could

00:27:45.030 --> 00:27:46.510
be executing code you don't need to.

00:27:46.510 --> 00:27:48.210
Well, how do you find that out?

00:27:48.330 --> 00:27:50.330
Well, you don't find it out because
performance is invisible.

00:27:50.330 --> 00:27:53.420
How do you know that you're actually
executing code you shouldn't?

00:27:53.420 --> 00:27:55.330
So this is one of the reasons
you actually need to analyze

00:27:55.330 --> 00:27:56.460
what your program's doing.

00:27:56.460 --> 00:27:59.300
You may have an algorithm that you
thought was cheap but turns out to be

00:27:59.380 --> 00:28:01.190
much more expensive than you thought.

00:28:01.190 --> 00:28:04.460
For example, when you actually have the
total of a thousand objects,

00:28:04.530 --> 00:28:08.080
it may actually be very inefficient
compared to a hundred objects.

00:28:08.180 --> 00:28:11.100
You may have code that you--
or an API that you were calling

00:28:11.100 --> 00:28:14.600
that you thought should be cheap,
but turns out to be extremely expensive.

00:28:14.690 --> 00:28:17.370
You can't actually detect
that unless you measure.

00:28:17.820 --> 00:28:19.550
And finally,
you might be doing things like polling,

00:28:19.570 --> 00:28:21.650
constantly asking the mouse, "Hey,
what's your position?

00:28:21.650 --> 00:28:22.240
What's your position?

00:28:22.240 --> 00:28:25.580
What's your position?" Instead of waiting
for the operating system to actually

00:28:25.580 --> 00:28:29.700
tell you that the mouse has moved,
cutting the amount of effort you do.

00:28:29.700 --> 00:28:33.820
In general, with CPU usage,
you want to do the law

00:28:33.820 --> 00:28:34.800
of diminishing returns.

00:28:34.800 --> 00:28:37.600
You want to go after the functions
that are the most expensive,

00:28:37.690 --> 00:28:40.520
because those are the ones that if
you can even--that improving their

00:28:40.990 --> 00:28:44.570
performance will make the biggest
difference on your application.

00:28:45.240 --> 00:28:47.280
So to find those hotspots,
find those bits of code that

00:28:47.280 --> 00:28:51.440
are extremely expensive,
you need to use a tool called Sampler.

00:28:51.440 --> 00:28:54.860
Sampler is a tool that allows you to
stop the program while it's executing,

00:28:54.860 --> 00:28:57.200
find out where in the
code it's executing,

00:28:57.200 --> 00:28:58.500
and then continues.

00:28:58.500 --> 00:29:00.680
Can we switch over to the demo slides?

00:29:02.600 --> 00:29:04.500
Scott can launch Sampler here.

00:29:04.500 --> 00:29:07.860
And we'll again go with SymPhysics.

00:29:07.860 --> 00:29:10.840
What Sampler does is it's sampling.

00:29:10.960 --> 00:29:14.340
It stops the program every, say,
20 milliseconds or 50 milliseconds,

00:29:14.370 --> 00:29:16.740
finds the code that's executing,
and then lets the program continue.

00:29:16.740 --> 00:29:19.220
And it actually sums up that data.

00:29:19.300 --> 00:29:23.050
In this case, Scott can take a look at a
specific part of the program,

00:29:23.050 --> 00:29:25.840
specifically the drawing
in that vector graph,

00:29:25.910 --> 00:29:27.950
which we found was extremely slow.

00:29:28.010 --> 00:29:32.060
So what he can do is press
Start Sampling for the app,

00:29:32.180 --> 00:29:35.770
do some drag so we can find
out what's going on in that

00:29:37.740 --> 00:29:39.770
And then he can press Stop Sampling.

00:29:39.860 --> 00:29:43.070
And Sampler again gives us a
call tree representation of

00:29:43.070 --> 00:29:45.310
the code that was executing.

00:29:45.710 --> 00:29:48.260
In this case,
the numbers to the left of the functions,

00:29:48.260 --> 00:29:51.360
rather than being the amount of memory,
represents how often that

00:29:51.410 --> 00:29:52.600
code was found executing.

00:29:52.740 --> 00:29:54.760
The 500 represents 500 samples.

00:29:54.860 --> 00:29:58.200
500 times the program was stopped,
the program was in main.

00:29:58.380 --> 00:30:01.670
Scott can also switch it to seconds,
which should be a little more

00:30:01.670 --> 00:30:05.310
helpful because that's a unit that
we actually care about as humans.

00:30:05.410 --> 00:30:08.760
And we can see here that we
sampled for about 10 seconds,

00:30:08.790 --> 00:30:10.800
and there were two threads
that we found executing.

00:30:10.870 --> 00:30:13.290
Let's look at that second one, Scott.

00:30:14.400 --> 00:30:17.100
When we find out that that first
thread pretty much was spending

00:30:17.100 --> 00:30:20.570
all of its time--actually,
click down a couple layers.

00:30:24.670 --> 00:30:27.620
During all ten seconds that we
were looking at this application,

00:30:27.700 --> 00:30:30.390
we found that the program was
running in this Mach message trap.

00:30:30.490 --> 00:30:33.500
Now, it could be that it was actually
calling that several times,

00:30:33.570 --> 00:30:36.810
but odds are what was happening was
it was sitting there waiting for a

00:30:36.810 --> 00:30:38.270
message and just never came back.

00:30:38.470 --> 00:30:41.440
So the second thread probably
was doing no execution.

00:30:41.540 --> 00:30:43.600
So as a result,
we just want to pull that out of

00:30:43.680 --> 00:30:44.860
the graph because we don't care.

00:30:44.910 --> 00:30:47.060
Scott can press the

00:30:47.480 --> 00:30:49.160
Now we get to the main thread.

00:30:49.210 --> 00:30:52.480
And we can see here that we
spent about ten seconds of time.

00:30:52.650 --> 00:30:56.010
Most of the time was,
and the information on the left,

00:30:56.130 --> 00:30:58.960
excuse me, on the right actually
shows a sample call stack,

00:30:58.960 --> 00:31:03.030
so we get some idea of
what code was executing.

00:31:03.100 --> 00:31:06.540
And we can see here that all
ten seconds we were in main.

00:31:06.540 --> 00:31:08.070
That's not surprising.

00:31:08.260 --> 00:31:08.320
But the

00:31:08.340 --> 00:31:11.010
Pretty much all the time
we were in CF Run/Loop/Run,

00:31:11.010 --> 00:31:17.380
which turns out to be one of
the APIs for doing run loops.

00:31:18.330 --> 00:31:20.420
and of that time,
about three seconds of the

00:31:20.420 --> 00:31:23.800
time was in Mach message,
which is one of the kernel routines

00:31:23.940 --> 00:31:26.050
for doing inter-process communication.

00:31:26.210 --> 00:31:28.460
This is probably when the
application was asking,

00:31:28.550 --> 00:31:30.740
"Hey, has anything happened?"

00:31:31.080 --> 00:31:32.290
has the mouse move.

00:31:32.340 --> 00:31:34.520
And so pretty much this was causing,
this was when the

00:31:34.520 --> 00:31:35.600
application was just sitting.

00:31:35.600 --> 00:31:37.710
So we can prune that out as well.

00:31:38.490 --> 00:31:42.390
The remaining seven seconds,
we were down in CFRunLoop in

00:31:42.390 --> 00:31:45.820
some of the code for actually
handling window displays.

00:31:45.900 --> 00:31:46.880
And Scott can go up that.

00:31:46.920 --> 00:31:49.620
And we can see that we
were in NSView draw rec,

00:31:49.640 --> 00:31:54.040
so we were actually drawing the
rectangle that represents the view.

00:31:54.120 --> 00:31:57.010
And all that code was in the camera view,
which is actually in the

00:31:57.020 --> 00:31:58.960
code that is SimPhysics.

00:31:59.040 --> 00:32:03.430
And we find, of the six seconds that we
spent drawing that view,

00:32:04.560 --> 00:32:08.400
5.88 seconds of that was drawing vectors,
that is, drawing those arrows.

00:32:08.400 --> 00:32:10.110
Hmm, that's interesting.

00:32:10.110 --> 00:32:13.180
Ninety-nine percent of our
time we were drawing arrows.

00:32:13.230 --> 00:32:15.840
And of that 99 percent of the
time that we were drawing arrows,

00:32:15.930 --> 00:32:17.520
three seconds of the
time we were in arrow,

00:32:17.630 --> 00:32:20.520
draw arrow for vector,
and then we were in a

00:32:20.520 --> 00:32:22.680
number of other functions.

00:32:22.680 --> 00:32:25.840
So this tells us that the
reason why we were having so

00:32:25.970 --> 00:32:29.800
much trouble dragging that,
dragging that particle around

00:32:29.940 --> 00:32:32.700
was that drawing the arrows
was extremely expensive.

00:32:32.800 --> 00:32:33.950
And there's a few reasons for this.

00:32:34.030 --> 00:32:36.260
If we actually click down,
you can see here that some of

00:32:36.260 --> 00:32:38.460
the time was actually spent
in the NSBezierPath class.

00:32:38.470 --> 00:32:41.000
That's the AppKit class that
you use for just general

00:32:41.000 --> 00:32:43.560
string--or general line drawing.

00:32:43.650 --> 00:32:47.260
The reason why is because the arrows
are actually a little more complex.

00:32:47.340 --> 00:32:49.820
Not only are they just straight lines,
but there's also a curved

00:32:49.820 --> 00:32:51.720
part for the arrowhead.

00:32:51.810 --> 00:32:55.160
We also fill in the arrow
to make the nice arrowheads.

00:32:55.250 --> 00:32:57.430
And as a result,
drawing those arrows tends

00:32:57.430 --> 00:32:59.110
to be relatively expensive.

00:32:59.480 --> 00:33:01.690
So one thing we could do to improve
this is we could actually try to

00:33:01.720 --> 00:33:03.500
simplify the drawing of the arrow.

00:33:03.500 --> 00:33:06.800
We could try to draw fewer arrows
by spacing them out further.

00:33:06.940 --> 00:33:11.250
Or we could actually try to find
the cases where we're drawing,

00:33:11.250 --> 00:33:13.280
let's say, very small arrows.

00:33:13.760 --> 00:33:16.430
Where we don't really care about having
all that detail and instead just draw

00:33:16.430 --> 00:33:18.760
a little line showing which direction.

00:33:18.890 --> 00:33:22.730
So with this information from Sampler,
we can get several ideas about how

00:33:22.810 --> 00:33:28.260
we should improve the algorithm to
try to make the screen redraw faster.

00:33:28.410 --> 00:33:30.750
Thank you.

00:33:30.750 --> 00:33:30.750
Can we go back to slides?

00:33:34.340 --> 00:33:37.140
Okay.

00:33:37.140 --> 00:33:41.710
The third category that we can
look at is use of resources.

00:33:41.760 --> 00:33:43.460
And I can think of at
least two cases of that.

00:33:43.610 --> 00:33:46.390
One is that we can go for
resources such as devices.

00:33:46.500 --> 00:33:50.600
For example, are we accessing the disk
incorrectly or accessing files?

00:33:50.800 --> 00:33:54.020
This can be particularly nasty
because when you go out to get a file,

00:33:54.230 --> 00:33:57.560
such as a preference file, let's say,
you can't actually guarantee

00:33:57.560 --> 00:33:58.650
it's on a local machine anymore.

00:33:58.650 --> 00:34:00.260
It might be on somebody's iPod.

00:34:00.280 --> 00:34:03.630
It could be out on the
network on somebody's iDisk.

00:34:03.690 --> 00:34:05.780
And so getting files could
be extremely expensive.

00:34:05.780 --> 00:34:08.050
And so--and unless you actually
know which accesses you're doing,

00:34:08.080 --> 00:34:10.960
you might not have a
way to track that down.

00:34:10.960 --> 00:34:14.480
The second case of resource
use is that your application

00:34:14.520 --> 00:34:16.070
may not be just your code.

00:34:16.070 --> 00:34:19.400
There are other parts of the
system that are involved with

00:34:19.520 --> 00:34:21.480
getting your application to run.

00:34:21.480 --> 00:34:22.820
Two examples of that.

00:34:22.820 --> 00:34:25.260
One of those is the ATS server.

00:34:25.330 --> 00:34:28.120
So in order to actually render fonts,
to create the fonts,

00:34:28.120 --> 00:34:31.750
your application actually talks
with a separate process called the

00:34:31.750 --> 00:34:34.070
ATS server to actually do that work.

00:34:34.120 --> 00:34:36.100
When you're doing drawing,
the Windows server's actually

00:34:36.100 --> 00:34:37.610
responsible for doing your drawing.

00:34:37.700 --> 00:34:40.480
You actually send messages to it
and it actually does the work.

00:34:40.560 --> 00:34:43.780
And so when you actually
look at your program on top,

00:34:43.910 --> 00:34:45.630
you need to look at
those processes as well,

00:34:45.630 --> 00:34:48.260
because your application may
only take 50 percent of the CPU,

00:34:48.310 --> 00:34:51.150
but if the Windows server's taking
the other 50 percent of the CPU,

00:34:51.150 --> 00:34:53.970
you may have a problem in your code.

00:34:56.120 --> 00:34:58.560
So to track down--I'll tell you
about one of those in particular.

00:34:58.650 --> 00:35:01.690
How do we track down information
about how we're using the disk?

00:35:01.800 --> 00:35:04.200
For example,
let's imagine that we did something in

00:35:04.200 --> 00:35:06.700
SimPhysics to load values on startup.

00:35:06.790 --> 00:35:11.280
And we noticed that loading all the
information off of disk tends to be slow.

00:35:11.400 --> 00:35:14.540
One way that we can track that down
to decide if that's a disk problem is

00:35:14.540 --> 00:35:16.990
we can use a utility called fsusage.

00:35:17.080 --> 00:35:18.560
This is a command-line tool.

00:35:18.680 --> 00:35:23.840
What you do with fsusage is you need
to run it as root and name a process,

00:35:23.840 --> 00:35:26.590
and it will actually give you a
running total of all the file system

00:35:26.590 --> 00:35:28.540
accesses done by that process.

00:35:28.550 --> 00:35:30.360
So it'll actually say you did a read,
you did a write,

00:35:30.460 --> 00:35:32.620
here are what the parameters are,
here's how long it took,

00:35:32.730 --> 00:35:34.180
here's whether it was blocking.

00:35:34.310 --> 00:35:36.200
And with that information,
you should be able to tell which

00:35:36.350 --> 00:35:40.340
files you're opening and how long
it was taking to access those files.

00:35:40.470 --> 00:35:42.740
Once you've done that,
there's actually another mode of

00:35:42.740 --> 00:35:45.140
sampler that will actually help
you track down exactly where in

00:35:45.150 --> 00:35:46.640
your code you were calling that.

00:35:46.670 --> 00:35:48.480
Because you don't actually
get the full call-- you don't

00:35:48.480 --> 00:35:50.140
actually know where you called it.

00:35:50.310 --> 00:35:52.220
What you can do with samplers,
there's a mode that says

00:35:52.270 --> 00:35:53.840
look at file system accesses.

00:35:53.840 --> 00:35:57.840
And every time that you call open, read,
write, et cetera,

00:35:57.840 --> 00:36:02.100
the sampler will automatically get
the backtrace to that call from

00:36:02.100 --> 00:36:04.550
your code so that you can find out
where in your code you might be

00:36:04.550 --> 00:36:07.230
calling open and how many times.

00:36:18.540 --> 00:36:22.560
The second case of resources is,
could we be drawing too much?

00:36:22.620 --> 00:36:24.160
And how do we actually track that down?

00:36:24.220 --> 00:36:25.960
Now,
drawing is particularly nasty because

00:36:26.080 --> 00:36:27.530
it affects performance in so many ways.

00:36:27.530 --> 00:36:29.960
When you're deciding to draw,
you end up using CPU time,

00:36:29.960 --> 00:36:33.060
you end up using memory time,
you're dealing with devices,

00:36:33.060 --> 00:36:35.650
and as a result,
it can be extremely costly.

00:36:36.170 --> 00:36:39.290
Even just having Windows that are
off-screen requires the Windows server

00:36:39.300 --> 00:36:40.390
to allocate memory for those.

00:36:40.400 --> 00:36:42.680
And so you're increasing
your application's memory

00:36:42.680 --> 00:36:44.800
use by having the Windows.

00:36:44.890 --> 00:36:48.900
So how do you actually track
down how you're doing drawing?

00:36:48.900 --> 00:36:51.100
Drawing is kind of hard,
because although you can see

00:36:51.100 --> 00:36:53.650
some of what's actually changing,
you don't actually know

00:36:53.750 --> 00:36:54.790
what's being redrawn.

00:36:54.860 --> 00:36:56.290
And luckily there's a tool that can help.

00:36:56.410 --> 00:36:58.560
Can we go to the demo machine again?

00:37:02.820 --> 00:37:07.600
The tool provided by the Core Graphics
team is called Quartz Debug.

00:37:07.640 --> 00:37:15.430
And Scott here can turn on an option in
Quartz Debug called Flash Screen Updates

00:37:15.430 --> 00:37:15.430
Yellow and then can do some actions.

00:37:17.110 --> 00:37:21.000
Here we see, as Scott moves the window,
we can see that the window server needs

00:37:21.000 --> 00:37:25.490
to actually redraw the area around the
window and parts of the window title,

00:37:25.500 --> 00:37:26.700
I believe.

00:37:26.820 --> 00:37:30.440
So redrawing a window, moving it around,
is relatively cheap.

00:37:30.530 --> 00:37:33.570
Now if Scott tries to resize that window,

00:37:39.000 --> 00:37:41.650
We can see that when we
actually resize the window,

00:37:41.670 --> 00:37:45.780
SimPhysics is constantly redrawing
the contents of that window to

00:37:45.780 --> 00:37:47.790
make up for the size change.

00:37:48.720 --> 00:37:50.940
So this shows one of the
issues that you need to worry

00:37:50.940 --> 00:37:51.950
about with your application.

00:37:51.960 --> 00:37:55.360
You may not have realized how much work
was being done when resizing a window,

00:37:55.360 --> 00:37:57.400
but Quartz Debug makes it obvious.

00:37:57.400 --> 00:37:59.540
It makes the invisible visible.

00:37:59.880 --> 00:38:02.100
And in this case,
perhaps some physics should be a little

00:38:02.100 --> 00:38:04.040
more clever about how it does redraws.

00:38:04.060 --> 00:38:06.220
For example,
maybe it shouldn't automatically try

00:38:06.340 --> 00:38:09.460
to relay out the entire window every
time that it gets a change event saying

00:38:09.460 --> 00:38:11.120
that the window size has changed.

00:38:11.580 --> 00:38:13.740
We can also find out how much
memory we're using for Windows by

00:38:13.740 --> 00:38:16.580
pressing the Show Window List button.

00:38:16.580 --> 00:38:19.190
And here we get a running total
according to each application

00:38:19.230 --> 00:38:22.260
of which Windows exist and
how much memory they're using.

00:38:22.320 --> 00:38:25.240
In this case, we'll actually see several
Windows for the SimPhysics,

00:38:25.240 --> 00:38:27.880
even though we only have
one main window open,

00:38:27.880 --> 00:38:30.790
because there are other
things that count as Windows,

00:38:30.790 --> 00:38:34.260
such as the icon in the dock,
such as the menu bar, and so on.

00:38:34.260 --> 00:38:35.150
Thank you.

00:38:35.150 --> 00:38:37.290
Can we switch back to slides?

00:38:40.930 --> 00:38:41.900
Okay.

00:38:42.010 --> 00:38:45.450
So those are the three major
categories of performance problems.

00:38:45.490 --> 00:38:48.260
And so we can ask ourselves,
after looking at that and looking

00:38:48.360 --> 00:38:50.760
at some particular problems,
what would happen if we

00:38:50.830 --> 00:38:51.980
actually fixed those?

00:38:52.140 --> 00:38:53.390
And luckily Scott actually did that.

00:38:53.390 --> 00:38:54.580
Thank you, Scott.

00:38:54.690 --> 00:38:55.190
Yeah.

00:38:55.400 --> 00:38:56.610
One clarification.

00:38:56.610 --> 00:38:59.920
It looks like I made things
worse for the plane case.

00:39:00.020 --> 00:39:03.220
I'll explain it.

00:39:03.810 --> 00:39:05.470
Thank you.

00:39:05.470 --> 00:39:05.470
So...

00:39:07.050 --> 00:39:09.510
So what we've done here is we
actually measured before and after.

00:39:09.630 --> 00:39:12.760
And the before case is actually kind
of interesting on the plain view.

00:39:12.790 --> 00:39:15.540
One of the things that we noticed
as we were actually redraw--as we

00:39:15.540 --> 00:39:19.050
were optimizing this is that we
were getting these insanely high

00:39:19.050 --> 00:39:22.800
frame rates when we were actually
moving around with the plain view.

00:39:23.190 --> 00:39:25.250
In this case, we saw 35 frames a second.

00:39:25.260 --> 00:39:29.890
On a faster machine,
we saw much more than that.

00:39:29.900 --> 00:39:31.790
And because we're not playing Quake,
we really don't care

00:39:31.790 --> 00:39:32.740
about high frame rates.

00:39:32.740 --> 00:39:35.160
And so, as a result,
there's probably no reason to

00:39:35.160 --> 00:39:38.620
actually redraw at 35 frames
a second for this program.

00:39:38.620 --> 00:39:40.260
So instead,
we throttled it down and we made

00:39:40.260 --> 00:39:42.820
it so that the plain view was
only going at 20 frames a second.

00:39:42.920 --> 00:39:45.550
So we weren't saturating the CPU,
but we were still giving enough of a

00:39:45.550 --> 00:39:49.680
frame rate change so that you actually
got the feeling of direct manipulation.

00:39:49.680 --> 00:39:53.260
For the case of the vector graph,
the intensity arrows, though,

00:39:53.260 --> 00:39:56.890
we went from a case where we were
doing about two frames a second.

00:39:57.030 --> 00:39:58.700
according to the
measurements we were doing,

00:39:58.700 --> 00:40:00.500
up to about 15 frames a second.

00:40:00.500 --> 00:40:03.130
And that was because we changed
how we were drawing arrows,

00:40:03.130 --> 00:40:06.340
changed to drawing only little lines
when we were doing small arrows,

00:40:06.400 --> 00:40:06.950
and so on.

00:40:06.950 --> 00:40:09.050
For doing that intensive--or
the color graph,

00:40:09.190 --> 00:40:12.220
we ended up going from about
2 frames a second to 7.

00:40:12.340 --> 00:40:14.250
And by getting rid of those
memory leaks that we found,

00:40:14.350 --> 00:40:17.130
we ended up--and by getting rid of
several other data structures that

00:40:17.140 --> 00:40:21.200
were a result of porting from Java to
Cocoa in a relatively simple way,

00:40:21.200 --> 00:40:24.620
we managed to cut the memory use
of the application from about 2.5

00:40:24.620 --> 00:40:27.600
megabytes before to 1.1 megabytes after.

00:40:27.600 --> 00:40:31.130
So if we could do this with only a
few hours of looking at Scott's app,

00:40:31.210 --> 00:40:33.860
imagine what you could
do with your application.

00:40:35.210 --> 00:40:37.400
Now, I didn't go through
all the possible tools.

00:40:37.540 --> 00:40:40.480
Here are some of the others
that you may want to look at.

00:40:40.480 --> 00:40:42.850
For looking at the heap,
there's three command-line

00:40:42.950 --> 00:40:44.660
tools that may be interesting.

00:40:44.660 --> 00:40:46.850
Heap, leaks, and malloc history.

00:40:47.080 --> 00:40:48.010
All of these have man pages.

00:40:48.010 --> 00:40:49.640
Go check them out.

00:40:49.640 --> 00:40:52.580
For looking at process state,
there's a program called Sample,

00:40:52.580 --> 00:40:54.610
which is like Sampler,
except it works on a command

00:40:54.610 --> 00:40:57.430
line and puts out a text file
describing what it found running.

00:40:57.430 --> 00:40:59.710
This is really good when
your application hangs.

00:40:59.710 --> 00:41:03.560
You can go to a terminal, type sample,
process ID or process name,

00:41:03.560 --> 00:41:07.380
ten seconds of sampling,
sampling every ten milliseconds,

00:41:07.380 --> 00:41:09.140
and get a report like that.

00:41:09.140 --> 00:41:10.940
You can also,
if you're doing command-line tools,

00:41:10.940 --> 00:41:14.530
use all the Unix tools that you may be
used to for doing performance analysis,

00:41:14.690 --> 00:41:18.510
such as Time, where you can say Time and
give it a command line,

00:41:18.730 --> 00:41:23.670
and it will tell you how long it
took to actually get that task to

00:41:24.540 --> 00:41:27.240
For understanding resources,
we have tools such as IOSTAT.

00:41:27.310 --> 00:41:31.390
This actually was not working on 10.1,
but works on JAGUAR.

00:41:31.520 --> 00:41:34.260
And this will actually tell you how
many file system accesses you're

00:41:34.260 --> 00:41:36.430
doing and that sort of thing.

00:41:38.810 --> 00:41:43.080
Okay, the tools that I have shown you
so far will work on any binary.

00:41:43.160 --> 00:41:45.050
You may need to launch them
inside the tool for them to work,

00:41:45.140 --> 00:41:46.650
but in general they
will work on anything.

00:41:46.750 --> 00:41:48.460
You don't need to recompile them.

00:41:48.500 --> 00:41:50.350
You don't need to include some
special debugging library.

00:41:50.570 --> 00:41:52.150
It just works.

00:41:52.220 --> 00:41:54.680
And this makes it very easy to
get their performance information.

00:41:54.710 --> 00:41:56.890
You have no excuses.

00:41:56.980 --> 00:42:02.180
If you're doing development
with CFM or with Code Warrior,

00:42:02.180 --> 00:42:02.180
how many people are doing that?

00:42:02.990 --> 00:42:06.140
Okay, so for all of you,
make sure that when you compile

00:42:06.140 --> 00:42:09.060
your applications in Code Warrior,
that you turn on the option that

00:42:09.160 --> 00:42:12.790
says that you want to compile your
app with in-line traceback tables.

00:42:12.790 --> 00:42:15.040
If you do this,
then all the performance tools can

00:42:15.150 --> 00:42:19.270
actually get your symbolic data out
and can tell which of your functions

00:42:19.270 --> 00:42:24.720
was found executing or was found at the
bottom of a malloc stack or whatever.

00:42:24.720 --> 00:42:29.620
If you are using Code Warrior's
version of the standard library,

00:42:29.730 --> 00:42:33.520
make sure to compile it with the
option set so that you use the

00:42:33.520 --> 00:42:36.900
systems version of malloc rather
than MetroWorks' version of malloc.

00:42:36.900 --> 00:42:39.480
In that way, you can actually use malloc
debug to analyze stack accesses.

00:42:39.710 --> 00:42:41.690
Otherwise,
we never actually see all the accesses.

00:42:43.870 --> 00:42:46.860
There are a number of sources for
documentation about these tools.

00:42:46.900 --> 00:42:48.590
First of all,
they were all in the Developer Tools CD,

00:42:48.600 --> 00:42:50.460
so you can go off and use them.

00:42:50.460 --> 00:42:54.130
The documentation for the
command-line tools is in man pages.

00:42:54.200 --> 00:42:57.320
The documentation for the graphical
tools is within the applications

00:42:57.420 --> 00:42:58.700
themselves in a help menu.

00:42:58.700 --> 00:43:03.160
There's also a release notes document in
each Developer Tools release so that you

00:43:03.160 --> 00:43:05.700
can find out about the latest changes.

00:43:05.880 --> 00:43:08.700
There's also two books you should look
at if you're interested in performance.

00:43:08.700 --> 00:43:11.980
The first is the
Inside Mac OS X Performance Book,

00:43:12.000 --> 00:43:15.420
which will tell you about the tools and
give you some details about understanding

00:43:15.860 --> 00:43:18.690
how the Mac OS X operating system works.

00:43:18.700 --> 00:43:22.240
The second book, the System Overview,
will give you lots of just

00:43:22.310 --> 00:43:25.700
sort of general details about
Mac OS X and its architecture.

00:43:25.700 --> 00:43:30.050
So in conclusion,
you want to tune your app

00:43:30.170 --> 00:43:33.040
to make the best impression,
so go and do it.

00:43:33.870 --> 00:43:35.660
Most of the problems that you're
going to encounter are going

00:43:35.660 --> 00:43:38.060
to be a side effect of memory,
just because of the way that

00:43:38.200 --> 00:43:40.090
systems using virtual memory work.

00:43:40.180 --> 00:43:42.160
So make sure to cut your
memory use in all its ways,

00:43:42.330 --> 00:43:44.470
whether that's the memory
that you allocate via malloc,

00:43:44.570 --> 00:43:47.900
whether it's the amount of code that
you're actually touching and executing,

00:43:47.940 --> 00:43:50.310
whether it's the amount
of drawing you're doing.

00:43:50.620 --> 00:43:53.530
Make sure that you actually
can compare performance,

00:43:53.560 --> 00:43:55.760
so you can actually figure out
if you're making a difference.

00:43:55.900 --> 00:43:59.660
Find some metric to measure,
measure that on every build,

00:43:59.670 --> 00:44:02.900
and compare your results
across every build.

00:44:03.010 --> 00:44:05.640
And remember that your application
is not just your application,

00:44:05.760 --> 00:44:08.760
but you also need to watch the other
applications that might help it,

00:44:08.890 --> 00:44:11.510
and you also need to find out if your
application behaves well on a system

00:44:11.590 --> 00:44:13.240
that has lots of other things running.

00:44:13.360 --> 00:44:16.160
And go out there and create
some great Mac OS X apps.

00:44:16.270 --> 00:44:20.570
Now, if you have used all these tools,
and you've gotten all the

00:44:20.580 --> 00:44:24.240
information you can out of Sampler,
the question is where you go from there.

00:44:24.340 --> 00:44:27.580
And what I'd like to do at this point
is bring on Eric Miller from the

00:44:27.580 --> 00:44:30.970
Performance and Architecture Group in
Hardware to tell us about the

00:44:31.070 --> 00:44:34.600
Computer Hardware Understanding Tools to
find out how you can get information

00:44:34.600 --> 00:44:36.980
from the performance registers
to further tune your code.

00:44:37.020 --> 00:44:37.740
Thank you.

00:44:37.740 --> 00:44:38.880
Is this on?

00:44:39.020 --> 00:44:39.840
Thanks, Robert.

00:44:39.840 --> 00:44:40.880
Good luck.

00:44:40.930 --> 00:44:43.600
That's a lot of stuff to memorize.

00:44:44.270 --> 00:44:46.610
So, Computer Hardware
Understanding Development Tools,

00:44:46.610 --> 00:44:49.590
we like to call them CHUD tools.

00:44:49.650 --> 00:44:52.390
There's just a couple things
we want to go through.

00:44:52.900 --> 00:44:54.600
So, well,
first I want to kind of tell you

00:44:54.600 --> 00:44:56.070
what you can do with our tools.

00:44:56.070 --> 00:45:02.270
You can find out what's going on with
the hardware through the CHUD libraries.

00:45:02.270 --> 00:45:06.800
You can find critical code
segments with the tools we supply,

00:45:06.800 --> 00:45:09.600
and once you've found
those critical code areas,

00:45:09.600 --> 00:45:12.980
you can then use other tools that
we supply to help you find the

00:45:13.020 --> 00:45:15.120
right way to tune those segments.

00:45:15.920 --> 00:45:18.700
And then also,
the CHUD tools are system-wide

00:45:18.700 --> 00:45:22.720
tools where you saw a sampler
sampling a particular process.

00:45:22.720 --> 00:45:26.650
The CHUD tools can do both a
single process and they can

00:45:26.730 --> 00:45:29.280
also sample the entire system.

00:45:29.280 --> 00:45:33.180
So you can see if you have issues in
your code or perhaps you're limited

00:45:33.180 --> 00:45:37.440
by some system facility or some
other application using resources.

00:45:39.200 --> 00:45:42.020
So, in order to find out what
the hardware is doing,

00:45:42.020 --> 00:45:46.900
the engineers at Apple and our processor
suppliers like Motorola kindly provide

00:45:46.900 --> 00:45:50.720
us with performance monitor counters
built right into the processors and

00:45:50.900 --> 00:45:52.700
built into the memory controller.

00:45:52.740 --> 00:45:56.540
So what we do with the Chud tools
is provide a mechanism to access

00:45:56.610 --> 00:46:02.080
all that information and bring it
to you in a human-readable format.

00:46:02.080 --> 00:46:07.840
So you can basically count what's
called a performance monitor events.

00:46:07.840 --> 00:46:09.980
Events like instructions completed.

00:46:10.020 --> 00:46:13.500
So every time an instruction completes,
one of the performance counters

00:46:13.880 --> 00:46:17.790
will count one or three
if you set it up to do so.

00:46:17.880 --> 00:46:20.360
If there's cache misses,
you can count the number

00:46:20.370 --> 00:46:21.250
of misses that occur.

00:46:21.410 --> 00:46:25.930
And these can be combined in different
ways to tell you statistics about the

00:46:25.930 --> 00:46:28.140
system at the very hardware level.

00:46:28.140 --> 00:46:31.770
So in effect,
you can read your system directly.

00:46:32.010 --> 00:46:37.420
You don't have to worry about guessing
or having a model or a simulation.

00:46:37.420 --> 00:46:37.420
You can use your own system.

00:46:37.420 --> 00:46:46.180
So the Chud tools provide an overall
framework and some tools that bring this

00:46:46.180 --> 00:46:48.350
all together and make it a bit simpler.

00:46:48.570 --> 00:46:50.590
Otherwise, you'd have to write your
own tools from scratch,

00:46:50.950 --> 00:46:53.430
a lot of assembly language,
and calling out to

00:46:53.430 --> 00:46:54.920
other kinds of systems.

00:46:54.940 --> 00:46:55.960
That might be difficult.

00:46:55.990 --> 00:46:59.650
One of the tools we don't really
have time to demonstrate is Monster.

00:46:59.680 --> 00:47:05.690
Monster is a very straightforward way
to access the performance counters.

00:47:05.790 --> 00:47:10.520
So you're going to be able to collect
data from the counters in several ways.

00:47:10.780 --> 00:47:13.760
One of the things that Chud
Tools provides throughout all of

00:47:13.760 --> 00:47:16.980
our tools is a hot key facility,
which is part of Cocoa in

00:47:16.980 --> 00:47:18.590
the core foundation.

00:47:18.650 --> 00:47:23.190
We wrap it up in Chud so that you
can access it through your tools when

00:47:23.190 --> 00:47:26.210
they're not -- they don't have to
have a focus in order to start and

00:47:26.210 --> 00:47:28.990
stop the collection of information.

00:47:29.280 --> 00:47:33.440
You can also use a timed interval
mechanism that we have in the CHUD tools.

00:47:33.440 --> 00:47:37.320
We have it down to approximately
100 microseconds level.

00:47:37.330 --> 00:47:41.930
You can take a sample over time
from the performance monitor system.

00:47:42.140 --> 00:47:46.500
You can also measure event counts
in relation to other event counts

00:47:46.830 --> 00:47:50.310
and create histograms with Monster.

00:47:50.550 --> 00:47:54.640
If you're computing performance
metrics with Monster,

00:47:54.640 --> 00:48:00.090
like Memoryband would say,
you can collect a number of bytes

00:48:00.140 --> 00:48:02.240
being transferred on the memory bus.

00:48:02.420 --> 00:48:07.200
Then you can also collect
these samples over time.

00:48:07.320 --> 00:48:11.040
And then you can compute bytes over time,
bytes per second per say.

00:48:11.040 --> 00:48:16.820
If you collect cycles per instruction
by collecting cycles on one counter,

00:48:16.880 --> 00:48:20.990
instructions on another,
and do the math between them.

00:48:21.540 --> 00:48:24.820
Monster has a very strong
capability to do this.

00:48:24.960 --> 00:48:28.480
It has a built-in calculator
that lets you create your own,

00:48:28.540 --> 00:48:30.200
what we call shortcuts.

00:48:30.340 --> 00:48:33.440
These are not too hard to do.

00:48:33.440 --> 00:48:35.960
I'll just go through a
couple of slides on it.

00:48:36.040 --> 00:48:39.240
So what's going on here on this
slide is that Monster has been

00:48:39.240 --> 00:48:42.230
set up to count a number of things
on the performance counters.

00:48:42.240 --> 00:48:47.200
In orange, you can see in the left column
that we have the third and fourth

00:48:47.200 --> 00:48:49.360
performance monitor counter.

00:48:49.460 --> 00:48:54.500
Our measuring floating point unit
instructions and instructions completed.

00:48:54.580 --> 00:48:58.730
And then below the listing of PMCs,
you see a section that says shortcuts.

00:48:58.850 --> 00:49:02.740
These are calculations that will be
performed on the performance monitor

00:49:02.740 --> 00:49:05.680
counts after they're collected over time.

00:49:05.770 --> 00:49:09.330
So the second part of the screen
on the right shows a single

00:49:09.330 --> 00:49:14.400
run with lots of intervals,
perhaps 10 millisecond samples.

00:49:14.490 --> 00:49:17.280
In these intervals,
you can see that the third and fourth

00:49:17.280 --> 00:49:19.000
column are highlighted in orange.

00:49:19.110 --> 00:49:23.760
So we've collected the various floating
point unit instruction counts and the

00:49:23.760 --> 00:49:26.170
overall instructions completed in counts.

00:49:26.260 --> 00:49:28.740
You can see about the sixth
sample that it starts to count.

00:49:28.750 --> 00:49:30.880
It starts to kind of regularize.

00:49:30.880 --> 00:49:34.200
We have 2 million, 7 million,
8 million FPU counts.

00:49:34.200 --> 00:49:37.990
Again, 23 million, 35 million,
36 million overall

00:49:37.990 --> 00:49:39.700
instructions completed.

00:49:40.150 --> 00:49:45.370
So then if you were to just slide
over there to the rest of the...

00:49:47.200 --> 00:50:29.000
[Transcript missing]

00:50:31.260 --> 00:50:33.400
So, I don't have too much
to say about Monster,

00:50:33.400 --> 00:50:37.840
except, oh yeah,
you can graph the things that

00:50:37.910 --> 00:50:41.790
you plot by selecting columns.

00:50:41.800 --> 00:50:46.010
So what I did was the percent
floating point instructions are

00:50:46.010 --> 00:50:49.550
selected so you can see the little
graph over time of the percentage,

00:50:49.550 --> 00:50:50.340
how it varied.

00:50:50.340 --> 00:50:53.000
So you can see there was some
work done for a few samples,

00:50:53.060 --> 00:50:55.360
and then a drop in the
amount of work and back up,

00:50:55.360 --> 00:50:58.280
draw a little drop,
back up every four or five intervals.

00:50:59.980 --> 00:51:01.810
At this time,
I want to bring up my colleague,

00:51:01.810 --> 00:51:04.450
Nathan Slingerland,
so we can do a live demo of one

00:51:04.450 --> 00:51:07.450
of our other tools called Shikari.

00:51:10.800 --> 00:51:14.000
Thank you, Eric.

00:51:14.210 --> 00:51:19.300
So, actually,
can we go back to the slide for a second?

00:51:19.400 --> 00:51:21.200
Talk a little bit about it first.

00:51:21.320 --> 00:51:24.300
Okay, so Shikari is a
system-wide profiling tool.

00:51:24.300 --> 00:51:30.100
It's similar in purpose to Sampler,
except that besides time,

00:51:30.100 --> 00:51:34.000
you can sample on any of the performance
events for those performance monitor

00:51:34.000 --> 00:51:36.590
counters that Eric was talking about.

00:51:36.800 --> 00:51:39.500
So this lets you correlate these
events to your source code.

00:51:39.500 --> 00:51:41.600
So if you want to know where
cache misses are coming from

00:51:41.600 --> 00:51:46.000
or alignment exceptions or,
you know, instruction counts,

00:51:46.000 --> 00:51:47.500
any of those things,
you can find that out.

00:51:47.540 --> 00:51:52.130
And then using Shikari,
you can get an annotated

00:51:52.130 --> 00:51:56.140
list of any function,
and it'll tell you exactly how

00:51:56.150 --> 00:51:59.140
many cache misses come from
that piece of source code.

00:51:59.140 --> 00:52:02.180
So, yeah,
let's switch over to the demo machine.

00:52:02.190 --> 00:52:05.470
It's probably the best way to
see exactly how this is used.

00:52:07.310 --> 00:52:11.290
So in order to demonstrate Shikari
and also to show you how you can solve

00:52:11.300 --> 00:52:15.080
a real performance problem with it,
we're going to look at Flurry.

00:52:15.220 --> 00:52:21.340
That's a colorful, open source,
OpenGL screensaver by Calum Robinson.

00:52:21.340 --> 00:52:23.320
That's what it does.

00:52:24.400 --> 00:52:29.290
So unless you look at
this with a sampling tool,

00:52:29.310 --> 00:52:31.680
you don't really know what's
going on in the system.

00:52:31.760 --> 00:52:34.470
So because it is a screensaver
and because it's OpenGL,

00:52:34.470 --> 00:52:38.810
we might be processor-bound,
we might be graphics card-bound,

00:52:38.810 --> 00:52:39.890
we don't know.

00:52:39.970 --> 00:52:44.110
So in order to see what's going on,
let's bring up Shikari.

00:52:47.050 --> 00:52:50.340
So what we see here,
this is the main Shikari window.

00:52:50.420 --> 00:52:52.800
And this list, this table,
when you sample,

00:52:52.800 --> 00:52:56.770
is going to give you a profile
listing from most frequently seen to

00:52:56.770 --> 00:52:58.350
least frequently seen function names.

00:52:58.360 --> 00:53:05.660
And we also have built in a bunch of
very useful sampling configurations.

00:53:05.690 --> 00:53:06.760
You're free to create your own.

00:53:06.760 --> 00:53:08.580
There's a facility in here to do that.

00:53:08.640 --> 00:53:11.680
But in the most basic way,
we just want to see a

00:53:11.680 --> 00:53:13.440
time profile at first.

00:53:13.440 --> 00:53:17.060
And, you know,
that's really the mantra of

00:53:17.060 --> 00:53:20.740
performance optimization is profile,
profile, and profile again.

00:53:20.740 --> 00:53:23.010
You want to see,
you want to keep seeing how,

00:53:23.100 --> 00:53:25.910
when you change your code,
what really is happening to,

00:53:25.960 --> 00:53:27.430
in the system each time.

00:53:27.440 --> 00:53:31.700
So because the Shikari also
uses the hotkey facility

00:53:31.700 --> 00:53:35.930
that Eric mentioned before,
it doesn't have to be in the foreground.

00:53:35.940 --> 00:53:37.130
It can sit in the background.

00:53:37.130 --> 00:53:38.640
And that works well for a full system.

00:53:38.640 --> 00:53:39.640
screen app.

00:53:39.640 --> 00:53:42.990
So he's going to use the hot key.

00:53:44.980 --> 00:53:48.240
And actually, the sampling itself is
pretty low overhead.

00:53:48.240 --> 00:53:51.430
It's only when it starts looking
at symbols and everything that it

00:53:51.480 --> 00:53:53.500
actually starts to get in the way.

00:53:53.500 --> 00:53:56.270
So what's interesting here,
we see that square root,

00:53:56.270 --> 00:53:59.350
system square root,
is taking up 40% of the time.

00:53:59.350 --> 00:54:01.640
And that's in the whole system, right?

00:54:01.720 --> 00:54:06.300
If we actually -- we see that
flurry is 99% of the system,

00:54:06.300 --> 00:54:10.680
and we saw the other half was .8% there,
our other piece.

00:54:10.680 --> 00:54:13.960
So 50% -- or 40% of the
time in square root.

00:54:13.960 --> 00:54:19.690
Now, Mac OS X now comes with a very fast
and very precise square root routine.

00:54:19.800 --> 00:54:25.030
But -- so it's IEEE 754 compliant
and 53 bits of precision.

00:54:25.140 --> 00:54:27.950
And that's great,
except this is a screen saver.

00:54:28.010 --> 00:54:32.760
So we might be able to get away
with a little less precision there.

00:54:32.760 --> 00:54:36.170
So if -- let's take a look at the source.

00:54:36.170 --> 00:54:39.480
So this is the original source code.

00:54:39.480 --> 00:54:40.680
We can see that we have
a little bit of a delay.

00:54:40.680 --> 00:54:44.870
We're -- we got a division there
and a square root based on that

00:54:44.870 --> 00:54:49.660
result -- or a square root,
and then we're dividing with that.

00:54:49.660 --> 00:54:49.660
But --

00:54:49.750 --> 00:54:53.880
What this actually is is a very
good opportunity to use the

00:54:53.880 --> 00:54:57.190
floating-point reciprocal square-root
estimate instruction that's

00:54:57.190 --> 00:55:00.200
available on any G3 or G4 processor.

00:55:00.200 --> 00:55:03.710
So there we are, in-line assembly.

00:55:03.710 --> 00:55:09.050
Now, I should emphasize that this is not
always appropriate for your code,

00:55:09.060 --> 00:55:12.350
and usually you probably want
the full result for square root.

00:55:12.400 --> 00:55:16.580
But in our case, this is approximately
five bits of precision,

00:55:16.580 --> 00:55:17.700
and it turns out to be plenty.

00:55:17.700 --> 00:55:19.900
It's, you know, visually,
that's all we care about.

00:55:19.900 --> 00:55:20.660
It looks great.

00:55:20.700 --> 00:55:22.840
So let's see how long it takes.

00:55:22.940 --> 00:55:25.290
Oh, I should tell you exactly
what our metric is here.

00:55:25.300 --> 00:55:28.200
So it took 12 seconds,
and we're measuring how long

00:55:28.230 --> 00:55:29.660
it takes to render 500 frames.

00:55:29.700 --> 00:55:33.700
And we've modified the original flurry
a little bit so that it's deterministic,

00:55:33.700 --> 00:55:38.700
and every time you get the same type
of pattern on the screen and like that.

00:55:38.700 --> 00:55:39.700
So let's hit Go here.

00:55:39.700 --> 00:55:42.160
We'll see how long that takes.

00:55:46.570 --> 00:55:47.410
Six seconds.

00:55:47.530 --> 00:55:49.490
So that's a 2x speedup, right?

00:55:49.570 --> 00:55:52.900
So not bad.

00:55:52.900 --> 00:55:56.900
Now, okay, so we want to profile again.

00:55:56.900 --> 00:56:00.600
We want to see where we're
actually spending time now.

00:56:00.600 --> 00:56:03.800
And this is an iterative process.

00:56:03.800 --> 00:56:09.140
You keep just banging on it with your
profiling tool to see what's happening.

00:56:09.150 --> 00:56:12.730
And we expect not to see
square root in there.

00:56:13.370 --> 00:56:14.540
Sure enough, it's not there.

00:56:14.540 --> 00:56:21.610
So now our functions inside of Flurry,
updateSmoke and drawSmoke, are up.

00:56:21.750 --> 00:56:27.860
So because we know that Update Smoke and
Draw Smoke are floating-point intensive,

00:56:27.910 --> 00:56:29.620
we can use another one
of Shikari's features,

00:56:29.620 --> 00:56:31.550
and that is to tell us
where floating-point issue

00:56:31.560 --> 00:56:33.140
stalls are coming from.

00:56:33.310 --> 00:56:36.090
And that's just another
configuration here.

00:56:37.800 --> 00:56:39.640
Run the demo again.

00:56:39.640 --> 00:56:41.400
And it's the same thing.

00:56:41.400 --> 00:56:43.680
So you start with a hotkey.

00:56:53.290 --> 00:56:57.640
So, like we might expect,
Update Smoke has the majority of

00:56:57.710 --> 00:56:59.600
the floating-point issue stalls.

00:56:59.660 --> 00:57:02.400
And what Chikari will let you do is
drill down inside of that function

00:57:02.400 --> 00:57:04.960
and see the disassembly of it.

00:57:05.010 --> 00:57:07.280
And besides that,
it's going to visually show

00:57:07.310 --> 00:57:08.420
you where your hotspots are.

00:57:08.550 --> 00:57:12.460
In this case, a hotspot will be orange,
and the colder spots,

00:57:12.460 --> 00:57:14.290
which weren't sampled, will be blue.

00:57:14.390 --> 00:57:16.990
So this is statistical sampling,
so you're not going to see an

00:57:16.990 --> 00:57:19.300
exact execution sequence here.

00:57:19.300 --> 00:57:22.700
So if we go down here near the bottom,
we see, wow,

00:57:22.730 --> 00:57:24.000
Floating point move register.

00:57:24.000 --> 00:57:27.240
Yeah, it's dependent on that floating
point reciprocal estimate.

00:57:27.240 --> 00:57:29.200
Yeah, okay.

00:57:29.200 --> 00:57:32.330
So most of the floating point
issues still stem from there.

00:57:33.890 --> 00:57:35.800
So let's go see what is that.

00:57:35.800 --> 00:57:37.800
If we can bring it back.

00:57:37.800 --> 00:57:39.800
Shikari, you gonna... Shikari back.

00:57:39.800 --> 00:57:43.800
It's line 279 in the source code.

00:57:43.840 --> 00:57:46.020
So now we can go back to the source.

00:57:47.630 --> 00:57:50.260
So that's this loop right here.

00:57:50.260 --> 00:57:51.520
And so what can we do with this?

00:57:51.540 --> 00:57:55.550
This is a scalar floating-point loop.

00:57:56.360 --> 00:58:00.000
One thing we might do is
use software pipelining,

00:58:00.000 --> 00:58:03.720
or if you attended the AltaVec session,
it's called data injection.

00:58:03.720 --> 00:58:06.190
And that is unroll the loop.

00:58:06.190 --> 00:58:10.940
Give the processor more to do
that's not dependent on results,

00:58:10.940 --> 00:58:14.060
so you're not serially dependent there.

00:58:14.720 --> 00:58:16.690
Or maybe we'd like to vectorize it.

00:58:16.730 --> 00:58:21.710
It turns out that this
algorithm is quite vectorizable.

00:58:21.710 --> 00:58:23.170
So what we did is we did both.

00:58:23.240 --> 00:58:25.940
We unrolled and we vectorized.

00:58:25.940 --> 00:58:26.860
Here we are.

00:58:26.860 --> 00:58:29.890
That's the new source code.

00:58:32.200 --> 00:58:35.240
So let's bring up Shikari
and ask it where--well,

00:58:35.240 --> 00:58:36.420
let's actually see Flurry first.

00:58:36.460 --> 00:58:38.810
Let's see how fast it is now.

00:58:43.600 --> 00:58:44.550
Okay, so three and a half seconds.

00:58:44.560 --> 00:58:45.420
So, not bad.

00:58:45.460 --> 00:58:46.990
Almost another 2x speedup.

00:58:46.990 --> 00:58:52.130
So, again,
maybe we could make Flurry even faster,

00:58:52.130 --> 00:58:52.600
right?

00:58:52.600 --> 00:58:59.590
Maybe we would want to look at,
there's an Altevec issue stalls preset.

00:58:59.800 --> 00:59:02.590
Or maybe we want to try
multi-threading this.

00:59:02.610 --> 00:59:04.660
I mean,
if we have a dual processor machine,

00:59:04.660 --> 00:59:06.580
we might be able to take advantage
of that second processor.

00:59:06.600 --> 00:59:10.600
But before we go forth and go do all the,
invest in all that work,

00:59:10.600 --> 00:59:18.150
let's take another time profile
during the Altevec rendering.

00:59:20.700 --> 00:59:22.460
Okay, whoa.

00:59:22.510 --> 00:59:24.100
Driver.

00:59:24.100 --> 00:59:25.460
Can't do a lot about that.

00:59:25.460 --> 00:59:27.940
So the driver is actually dominating.

00:59:27.940 --> 00:59:29.840
I mean,
it's taking about a third of the time.

00:59:29.840 --> 00:59:33.060
Our code is still important,
but it's less so now, right?

00:59:33.060 --> 00:59:35.340
So the better the job
you do at optimization,

00:59:35.340 --> 00:59:38.230
the less you're going to see
that show up in the profile.

00:59:39.180 --> 00:59:41.800
But it turns out,
maybe now it's time to review how

00:59:41.810 --> 00:59:43.260
we actually implemented Flurry.

00:59:43.260 --> 00:59:44.250
Actually, we didn't.

00:59:44.250 --> 00:59:45.280
Kalem Robinson did.

00:59:45.280 --> 00:59:47.760
But, you know,
it'd be time to go back and look at that.

00:59:47.760 --> 00:59:50.910
Because we've talked with
our colleagues in OpenGL,

00:59:50.910 --> 00:59:54.130
and they've told us, yeah,
probably there are better

00:59:54.130 --> 00:59:57.980
ways to render this,
to be drawing this as is done right here.

00:59:57.980 --> 01:00:01.500
So, you know,
you'd go and try and fix how you're

01:00:01.710 --> 01:00:04.240
interacting with OpenGL instead.

01:00:04.240 --> 01:00:07.680
So at this time,
could we go back to the slides?

01:00:09.100 --> 01:00:11.060
Just to give you a taste
of what Shikari can do.

01:00:11.060 --> 01:00:13.000
And of course,
you can use any of the performance

01:00:13.000 --> 01:00:19.140
events that are available on the
memory controller or the CPU.

01:00:20.170 --> 01:00:23.890
So underlying Monster and underlying
Shikari are the Chet framework.

01:00:23.960 --> 01:00:27.290
And this allows you to write
your own performance tools.

01:00:27.340 --> 01:00:28.890
All the functionality is there.

01:00:28.890 --> 01:00:31.800
We're not hiding anything
from you from what we use.

01:00:31.900 --> 01:00:34.500
You can access the
performance monitor counters,

01:00:34.600 --> 01:00:38.070
special purpose registers on the chip,
collect information about the hardware.

01:00:38.160 --> 01:00:39.380
It's not that you couldn't
do this on your own.

01:00:39.400 --> 01:00:42.990
It's just the framework tries to
consolidate this into an easy-to-use,

01:00:43.000 --> 01:00:45.370
simple C API.

01:00:45.660 --> 01:00:48.250
And--or the other thing you can
do is to instrument your code.

01:00:48.260 --> 01:00:50.360
Like, if you know you have
an important function,

01:00:50.360 --> 01:00:52.900
or a sampler or a shikari tells
you you have an important function,

01:00:52.900 --> 01:00:55.950
and you really just want to know how
many cache misses are inside of this

01:00:55.950 --> 01:00:59.940
function or something like that,
you can put chud calls inside of there.

01:01:00.000 --> 01:01:02.220
There's also a remote facility
that'll allow you to start

01:01:02.220 --> 01:01:06.040
and stop an application like
Monster from within your application.

01:01:06.040 --> 01:01:10.110
So you can just capture a small
bit of what you're interested in.

01:01:10.920 --> 01:01:13.200
So here's an example of that,
just to see.

01:01:13.410 --> 01:01:15.800
So call chat initialize.

01:01:15.800 --> 01:01:18.890
Everybody has to do that
when they use the framework.

01:01:18.900 --> 01:01:23.730
Get some information about the processor
and the bus frequency and like that.

01:01:23.750 --> 01:01:26.040
And then next comes the
more important part.

01:01:26.040 --> 01:01:27.300
We're gonna set up the counters.

01:01:27.300 --> 01:01:29.660
We tell it what event
we want and which PMC.

01:01:29.670 --> 01:01:31.600
So these are on the CPU.

01:01:31.760 --> 01:01:34.980
Clear the counters, start the counters,
and then do what you're interested in,

01:01:34.980 --> 01:01:35.490
right?

01:01:36.020 --> 01:01:38.140
So that's where--whatever
you're interested in is there.

01:01:38.270 --> 01:01:39.650
Stop the counters right after that.

01:01:39.660 --> 01:01:42.830
And then have some kind of reporting
function that's going to list out

01:01:42.830 --> 01:01:45.910
what was in the PMCs over time.

01:01:47.810 --> 01:01:53.050
Okay, in the Chud Tools package,
there are also a few other useful tools,

01:01:53.100 --> 01:01:54.690
especially if you're
doing things like Altevec.

01:01:54.700 --> 01:01:57.610
There's Amber,
which is an instruction tracer.

01:01:57.640 --> 01:02:00.590
It's going to record the exact
sequence of instructions that

01:02:00.590 --> 01:02:01.960
your program goes through.

01:02:01.960 --> 01:02:05.720
ACID, or SIMG4,
takes the output of Amber,

01:02:05.720 --> 01:02:10.520
and that we have a,
it's a G4 or 7400 simulator.

01:02:10.520 --> 01:02:13.120
Or ACID can give you
information about the,

01:02:13.120 --> 01:02:16.030
like a newer processor like the 7450.

01:02:16.290 --> 01:02:18.900
It's not a simulator, though,
but it can tell you about

01:02:18.930 --> 01:02:21.820
dependency stalls and how,
you know, different kinds of latencies.

01:02:21.820 --> 01:02:27.110
And then Reggie is a tool to look at
and modify special purpose registers.

01:02:30.060 --> 01:02:30.590
So where to get it?

01:02:30.620 --> 01:02:33.640
This is the website where you can
download the Chud Tools package.

01:02:33.640 --> 01:02:38.420
I should tell you right now
that the package that's up

01:02:38.560 --> 01:02:44.180
there on the FTP site there,
the package that's on the FTP site

01:02:44.420 --> 01:02:46.100
right now will not run in Jaguar.

01:02:46.100 --> 01:02:48.980
It won't hurt Jaguar,
but the tools won't work.

01:02:48.980 --> 01:02:51.980
But you can expect within the
next few days to see a package.

01:02:51.980 --> 01:02:56.110
You want the Chud 200 beta package.

01:02:56.110 --> 01:02:57.840
There's a beta up there right now.

01:02:57.880 --> 01:03:01.360
And that has a lot of features
that have been added very recently.

01:03:01.360 --> 01:03:04.250
And like I said,
you can expect to see a version you can

01:03:04.250 --> 01:03:05.970
use with your new Jaguar CDs very soon.

01:03:05.980 --> 01:03:08.900
And any issues you can report
to Chud Tools feedback.

01:03:12.300 --> 01:03:14.610
So at this time,
I'm going to-- let's bring

01:03:14.620 --> 01:03:20.260
Godfrey DiGiorgi back up to
run-- to have a few more slides.

01:03:24.970 --> 01:03:26.500
Thank you, Nathan.

01:03:26.800 --> 01:03:28.560
And a big hand for our presenters.

01:03:28.560 --> 01:03:31.420
I think they've done
a fantastic job today.

01:03:32.960 --> 01:03:34.060
So more information.

01:03:34.060 --> 01:03:36.570
I listed out the two
manuals that are available.

01:03:36.570 --> 01:03:41.160
They're both available on your disk in
PDF format in the Developer Documentation

01:03:41.160 --> 01:03:42.360
Essentials Directory.

01:03:42.360 --> 01:03:45.040
You can also order them from our website.

01:03:45.040 --> 01:03:48.030
If you go to the TechPubs
website listed above,

01:03:48.060 --> 01:03:51.300
you can go to our supplier
for print-on-demand.

01:03:51.300 --> 01:03:54.200
We, of course,
have the Connection Tools page

01:03:54.200 --> 01:03:56.260
and a link to the bug reporter.

01:03:56.260 --> 01:03:58.540
So if you find problems
with any of the tools,

01:03:58.540 --> 01:04:00.200
we want to hear that feedback.

01:04:00.200 --> 01:04:02.290
We want to hear the bugs
and see them in our system.

01:04:03.700 --> 01:05:17.900
[Transcript missing]