WEBVTT

00:00:02.950 --> 00:00:10.890
Ladies and gentlemen,
please welcome Technology Manager for

00:00:10.890 --> 00:00:10.890
Development Tools,
Godfrey DiGiorgi.

00:00:12.010 --> 00:00:12.410
Good morning.

00:00:12.410 --> 00:00:16.880
It's good to see that on a
Friday morning we get as many of you

00:00:16.880 --> 00:00:19.360
here for an early morning session.

00:00:19.360 --> 00:00:20.990
How's the show been going for you?

00:00:22.340 --> 00:00:23.300
Good.

00:00:23.300 --> 00:00:24.240
Great.

00:00:24.240 --> 00:00:28.290
We've talked to you over the course
of the week on various technologies

00:00:28.680 --> 00:00:30.460
in our frameworks and applications.

00:00:30.570 --> 00:00:33.990
We've seen Jaguar and all the
new things we're doing there.

00:00:34.210 --> 00:00:39.140
Today we're going to be focusing
on a lot more of the tools in terms

00:00:39.140 --> 00:00:43.820
of their detail aspects and how
to use them and stuff like that.

00:00:43.820 --> 00:00:46.370
So for the first session today
we're going to talk about

00:00:46.370 --> 00:00:47.880
developing for performance.

00:00:47.880 --> 00:00:51.760
And to do that I want
to introduce Joe Sokle,

00:00:51.760 --> 00:00:55.700
the manager of the
Core OS Performance Group.

00:01:06.120 --> 00:01:08.100
Hi, good morning.

00:01:08.120 --> 00:01:11.020
Hope everyone had a fun time
at the beer bash last night.

00:01:11.030 --> 00:01:14.820
Maybe be a little bit awake
this morning for this.

00:01:15.450 --> 00:01:19.020
Anyway, I am Joe Sokol, and I'm going to

00:01:28.200 --> 00:01:30.800
Man, that's very sensitive.

00:01:30.800 --> 00:01:31.570
Okay.

00:01:32.020 --> 00:01:35.700
So today we're going to actually
talk a little bit about performance

00:01:35.700 --> 00:01:41.380
philosophy as opposed to the actual
APIs and things that you might

00:01:41.380 --> 00:01:44.800
use to make your applications go.

00:01:44.800 --> 00:01:50.310
So the philosophy that we've kind of
chosen to follow here is that improving

00:01:50.590 --> 00:01:52.860
system performance is like a diet.

00:01:52.860 --> 00:01:54.540
It's a never-ending battle.

00:01:55.520 --> 00:01:58.990
Once you start down this road,
you've just got to kind

00:01:59.000 --> 00:02:01.000
of stay with the fight.

00:02:01.120 --> 00:02:04.680
And just like a diet,
it's a lot easier to do if

00:02:04.760 --> 00:02:07.550
you're participating in a group.

00:02:07.600 --> 00:02:20.300
[Transcript missing]

00:02:20.790 --> 00:02:24.990
To win this battle,
we need to look at using our

00:02:24.990 --> 00:02:29.970
system resources efficiently,
both when the app is quiescent

00:02:29.970 --> 00:02:33.710
and when it's actively
running and doing something.

00:02:33.820 --> 00:02:37.690
Now, quiescent does not necessarily
mean that it's in the background.

00:02:37.810 --> 00:02:39.650
That app could be in the foreground.

00:02:39.750 --> 00:02:43.730
If the user is not interacting with it,
it has no work to do,

00:02:43.730 --> 00:02:45.200
it should do just that.

00:02:45.350 --> 00:02:50.070
It should take no system resources.

00:02:50.700 --> 00:02:55.540
When the app is active,
we'd like you to use the system resources

00:02:55.540 --> 00:03:02.090
as efficiently as possible to implement
whatever work that you have to do.

00:03:02.900 --> 00:03:05.670
You also need to pay attention,
basically what we're asking you to do

00:03:05.680 --> 00:03:07.760
is pay attention to the fundamentals.

00:03:07.890 --> 00:03:09.820
Okay,
so these are things that everyone has

00:03:09.820 --> 00:03:13.860
learned as an engineer through the years,
but sometimes we forget,

00:03:13.860 --> 00:03:16.600
and what we want to do is
really focus on why this is

00:03:16.600 --> 00:03:19.260
important to system performance.

00:03:19.380 --> 00:03:21.400
There are no magic APIs.

00:03:21.450 --> 00:03:23.970
You know,
I cannot give you an API that basically

00:03:23.970 --> 00:03:28.200
puts you on a slim-fast diet and
makes your application more efficient,

00:03:28.410 --> 00:03:28.560
right?

00:03:28.560 --> 00:03:32.150
It's up to you guys to actually
go and take a look at what's

00:03:32.150 --> 00:03:37.670
going on and make changes in how
you're utilizing these resources.

00:03:39.680 --> 00:03:44.980
Okay, so basically we're going to go
through this in three stages today.

00:03:45.430 --> 00:03:49.090
I'm going to do a brief description
of the difference between performance

00:03:49.090 --> 00:03:54.750
and efficiency and why you care
about both of these concepts.

00:03:54.840 --> 00:03:57.650
We're going to talk about the fundamental
elements of system performance.

00:03:57.710 --> 00:04:01.260
So these are the three things when
I'm referring to system resources.

00:04:01.380 --> 00:04:05.260
These are basically the three
things that we're talking about:

00:04:05.320 --> 00:04:08.910
use of the CPU, use of memory,
and use of file system.

00:04:09.100 --> 00:04:11.300
And at the end,
we're going to go through some

00:04:11.300 --> 00:04:15.170
processes and some lists of
tools on how to evaluate your

00:04:15.170 --> 00:04:18.080
use of these system resources.

00:04:20.390 --> 00:04:22.340
Okay, so performance and efficiency.

00:04:22.340 --> 00:04:24.400
They're not synonymous.

00:04:24.610 --> 00:04:26.600
People get these confused all the time.

00:04:27.240 --> 00:04:29.200
Performance is a result of efficiency.

00:04:29.220 --> 00:04:31.800
So the more efficient you
make your application,

00:04:31.860 --> 00:04:35.310
the better it's going to perform
in a variety of environments,

00:04:35.310 --> 00:04:40.960
not just in a kind of stand-alone
test bed that you might be looking at.

00:04:41.030 --> 00:04:44.370
And performance does not
guarantee efficiency.

00:04:44.430 --> 00:04:48.200
So you may go and really work
hard to make your app perform

00:04:48.200 --> 00:04:52.020
and meet certain criteria that
your customers are asking for,

00:04:52.020 --> 00:04:56.620
but that does not necessarily guarantee
that it's doing it in an efficient way.

00:04:56.760 --> 00:04:58.320
And in fact, often that's what happens.

00:04:58.320 --> 00:05:03.090
You set a performance goal,
you reach that goal, and then you stop.

00:05:03.090 --> 00:05:07.390
You may be using -- consuming
100% or 90% of the system

00:05:07.390 --> 00:05:12.900
available resources at that point,
but you've met your performance goal.

00:05:13.090 --> 00:05:27.710
The processes for improving
both are very similar.

00:05:27.710 --> 00:05:27.710
We will address that in the
third portion of this talk.

00:05:27.710 --> 00:05:27.710
We will also talk about why you should
care about each of these points as we go.

00:05:28.590 --> 00:05:30.420
So what is performance?

00:05:30.710 --> 00:05:32.570
It's the speed at which
operations complete,

00:05:32.600 --> 00:05:32.940
right?

00:05:32.940 --> 00:05:34.260
It's something that is measurable.

00:05:34.260 --> 00:05:36.450
It's something that is
perceived by the user,

00:05:36.460 --> 00:05:39.110
which is why you care,
because it's also something that the user

00:05:39.110 --> 00:05:41.300
is going to call in and complain about,
right,

00:05:41.400 --> 00:05:44.460
if it doesn't meet his requirements.

00:05:45.580 --> 00:05:51.680
Some common performance metrics
that are used to measure this,

00:05:51.690 --> 00:05:54.950
to measure how well you're doing,
would be things like throughput,

00:05:55.140 --> 00:05:57.690
you know, so bytes per second,
so maybe how fast you can move

00:05:57.690 --> 00:05:59.330
that file across the network.

00:05:59.950 --> 00:06:02.630
Operations per second,
so that would be maybe like the number

00:06:02.660 --> 00:06:06.630
of frames per second that you could draw
on the screen for maybe like a game,

00:06:06.630 --> 00:06:08.120
something like that.

00:06:08.120 --> 00:06:11.350
And of course, latency,
which is pretty much what

00:06:11.350 --> 00:06:13.370
the user always sees,
right?

00:06:13.370 --> 00:06:15.150
It's the response time.

00:06:15.260 --> 00:06:19.580
So he sees all of these things
as a request that then takes some

00:06:20.140 --> 00:06:23.820
amount of time to actually complete,
and, you know,

00:06:23.820 --> 00:06:26.640
that's really what interests him.

00:06:26.640 --> 00:06:29.870
So how long does he have to sit in
front of the screen there and wait

00:06:29.870 --> 00:06:31.700
for that operation to complete?

00:06:33.640 --> 00:06:35.080
So what is efficiency?

00:06:35.140 --> 00:06:41.160
Efficiency in this context is really
the cost to complete an operation.

00:06:41.160 --> 00:06:44.080
For any one of those given
operations that your app is

00:06:44.150 --> 00:06:48.670
making many thousands of,
what was the cost in system resources?

00:06:48.670 --> 00:06:51.660
It minimizes the use of system resources.

00:06:51.660 --> 00:06:56.540
If you have an efficient algorithm,
it tends to minimize the

00:06:56.600 --> 00:06:58.440
use of system resources.

00:06:58.440 --> 00:07:02.720
Efficiency in this context is really
the cost to complete an operation.

00:07:02.730 --> 00:07:05.980
For any one of those given
operations that your app is

00:07:05.980 --> 00:07:08.440
making many thousands of,
what was the cost in system resources?

00:07:08.440 --> 00:07:08.890
It minimizes the use of system resources.

00:07:08.890 --> 00:07:09.600
If you have an efficient algorithm,
it tends to minimize the

00:07:09.600 --> 00:07:09.900
use of system resources.

00:07:09.900 --> 00:07:10.600
Efficiency in this context is really
the cost to complete an operation.

00:07:10.600 --> 00:07:11.080
For any one of those given
operations that your app is

00:07:11.080 --> 00:07:11.880
making many thousands of,
what was the cost in system resources?

00:07:11.880 --> 00:07:12.410
It minimizes the use of system resources.

00:07:12.440 --> 00:07:13.060
If you have an efficient algorithm,
it tends to minimize the

00:07:13.060 --> 00:07:13.440
use of system resources.

00:07:13.440 --> 00:07:14.220
Efficiency in this context is really
the cost to complete an operation.

00:07:14.220 --> 00:07:14.790
For any one of those given
operations that your app is

00:07:14.860 --> 00:07:15.680
making many thousands of,
what was the cost in system resources?

00:07:15.680 --> 00:07:16.440
It minimizes the use of system resources.

00:07:16.440 --> 00:07:19.600
If you have an efficient algorithm,
generally we look at it as an average.

00:07:19.600 --> 00:07:21.620
The memory footprint,
which is probably the most

00:07:21.760 --> 00:07:25.480
critical of the three to look at,
because that kind of dictates

00:07:25.480 --> 00:07:29.610
whether or not things are really
going to operate efficiently,

00:07:29.610 --> 00:07:30.940
and IO frequency.

00:07:30.940 --> 00:07:37.020
So how frequently are you going to
the disk and for what size operations?

00:07:39.570 --> 00:07:46.120
Okay, so this all kind of rolls
up into system performance.

00:07:46.120 --> 00:07:47.650
System performance,
when we talk about that,

00:07:47.690 --> 00:07:50.600
what we're really talking about,
it's a result of all the individual

00:07:50.600 --> 00:07:52.810
apps and OS efficiencies,
right?

00:07:52.820 --> 00:07:56.610
So when you roll that all together and
you take a look at a system and it's

00:07:56.680 --> 00:08:00.380
got 20 applications running on it,
it has a bunch of stuff that

00:08:00.380 --> 00:08:04.010
the system is also doing,
that really dictates how well

00:08:04.010 --> 00:08:08.490
the system or the efficiencies
of what you're doing there really

00:08:08.490 --> 00:08:09.480
dictate what the system is doing.

00:08:09.500 --> 00:08:18.910
So if you're working set size exceeds
the amount of physical memory you have,

00:08:18.910 --> 00:08:21.990
I mean,
the system performance just tanks.

00:08:21.990 --> 00:08:24.180
I mean, it's not a slow drop-off.

00:08:24.180 --> 00:08:27.900
It just becomes, you know,
pretty much unusable.

00:08:27.960 --> 00:08:30.160
Why is this important to you?

00:08:30.430 --> 00:08:33.590
Because the system performance
affects the user's perception of any

00:08:33.700 --> 00:08:35.550
individual application performance.

00:08:35.630 --> 00:08:39.310
So you guys can work really hard to
make your app perform really well.

00:08:39.920 --> 00:08:43.060
But if you've tested that in isolation
and you haven't mixed it in with other

00:08:43.060 --> 00:08:46.710
applications to see how they affect it
and what the system is going to do to it,

00:08:46.880 --> 00:08:50.580
when the user actually uses it,
they may see a very different thing,

00:08:50.630 --> 00:08:51.120
right?

00:08:51.120 --> 00:08:54.210
Your testers are telling you, "Hey,
we meet all of the

00:08:54.320 --> 00:08:55.640
performance goals that we set.

00:08:55.790 --> 00:08:59.100
Everything looks cool." All of
your customers are telling you,

00:08:59.100 --> 00:09:02.340
"Hey, you know,
this thing just doesn't rock," right?

00:09:02.340 --> 00:09:05.800
It's not performing the way you
claim it's going to perform.

00:09:05.800 --> 00:09:07.690
So how do we go about improving this?

00:09:07.690 --> 00:09:09.480
Because that's really the -- Yeah.

00:09:10.500 --> 00:09:11.400
-- the nut here.

00:09:11.580 --> 00:09:12.080
All right.

00:09:12.080 --> 00:09:13.340
We focus on the fundamentals.

00:09:13.380 --> 00:09:14.260
Okay.

00:09:14.260 --> 00:09:17.910
There are three fundamentals that really
control how well the system is going

00:09:17.910 --> 00:09:20.040
to perform and how efficient your app.

00:09:20.160 --> 00:09:21.400
It's your use of the CPU.

00:09:21.450 --> 00:09:25.160
It's how much memory you're
actually tying up in a working set.

00:09:25.160 --> 00:09:28.840
And it's how much file system
I/O that you're initiating.

00:09:28.840 --> 00:09:31.110
When you look at it
from that perspective,

00:09:31.260 --> 00:09:35.330
I mean, that -- every system performance
problem that I look at,

00:09:35.330 --> 00:09:38.500
right, it always boils down to
one of these three things.

00:09:39.500 --> 00:09:43.330
All right, in looking at the CPU...

00:09:45.100 --> 00:09:47.790
There are two models
that are commonly used.

00:09:47.980 --> 00:09:50.120
And I know that we've been
driving this point home,

00:09:50.120 --> 00:09:51.380
I think, throughout the conference.

00:09:51.380 --> 00:09:54.840
I was at the Carbon event session.

00:09:54.930 --> 00:09:59.870
And we're asking you to be event-driven.

00:09:59.990 --> 00:10:05.280
This is really a good way to
control your appetite for the CPU.

00:10:05.800 --> 00:10:08.400
The important thing here is
that only real work is done.

00:10:08.400 --> 00:10:11.500
So when you get an event,
you respond to the event, right?

00:10:11.620 --> 00:10:12.900
You do the work that's necessary.

00:10:12.900 --> 00:10:17.030
You do it in an efficient way,
and then you go back to sleep.

00:10:17.140 --> 00:10:19.940
The app can become totally idle,
which is a really good thing.

00:10:19.940 --> 00:10:23.970
We're going to talk about a couple of
implications of that here in a second.

00:10:24.180 --> 00:10:27.430
The other way to go is
more of a polling model.

00:10:27.580 --> 00:10:31.420
And the thing that's really
bad about this is that often,

00:10:31.460 --> 00:10:33.500
in fact,
the majority of the time when you poll,

00:10:33.500 --> 00:10:35.550
there's actually no
real work for you to do.

00:10:35.560 --> 00:10:39.310
You run around your polling
loop looking for things to do,

00:10:39.420 --> 00:10:43.170
and the majority of the time,
there's no real work to do.

00:10:43.670 --> 00:10:48.280
So, it's a pretty inefficient
use of the resource.

00:10:48.380 --> 00:10:51.170
Now, the other thing that happens with
the CPU is every time you use it,

00:10:51.170 --> 00:10:54.840
right, you're causing memory and
possibly the disk to be accessed.

00:10:54.840 --> 00:10:58.480
So, it's a, you know, it's a VM system.

00:10:58.480 --> 00:11:01.000
You don't know whether or not that
piece of memory you're touching is

00:11:01.000 --> 00:11:03.680
really physically present or not,
or it's going to have

00:11:03.680 --> 00:11:05.670
to come in off the disk.

00:11:07.510 --> 00:11:09.200
So why event-driven?

00:11:09.200 --> 00:11:13.500
So really, fundamentally,
what's really good about this?

00:11:13.500 --> 00:11:18.120
In terms of the fundamentals, anyway,
it minimizes the working set.

00:11:18.510 --> 00:11:22.520
It just, if you're only responding to
events and only doing the work

00:11:22.520 --> 00:11:26.390
that's absolutely necessary,

00:11:27.250 --> 00:11:29.430
you're going to touch a lot less pages.

00:11:29.560 --> 00:11:33.160
Touching less pages means that there's
less paging going on in the system.

00:11:33.250 --> 00:11:37.370
It just means the system is going
to feel that much more responsive.

00:11:37.650 --> 00:11:43.400
Another point that you might not think
about is the ability of the system

00:11:43.400 --> 00:11:46.800
to aggressively manage its power.

00:11:46.960 --> 00:11:50.540
In Mac OS X,
we do all of our power management

00:11:50.540 --> 00:11:52.300
basically in the idle thread.

00:11:52.300 --> 00:11:57.250
So the system has to go idle before
we can start to make use of any

00:11:57.250 --> 00:12:01.780
of the little hardware tricks for
napping or dozing the processor or

00:12:01.780 --> 00:12:07.050
possibly doing frequency shifting
if that processor is capable of

00:12:07.050 --> 00:12:09.620
doing that or it needs to be done.

00:12:09.770 --> 00:12:11.800
And the more aggressively
we can manage power,

00:12:11.800 --> 00:12:15.370
the longer a battery lasts
in a portable-- and the fan

00:12:15.370 --> 00:12:18.210
just comes on less often,
too, and like your TI.

00:12:18.220 --> 00:12:19.810
So, you know, that's a good thing, too.

00:12:19.820 --> 00:12:20.900
People complain about that.

00:12:20.900 --> 00:12:23.880
They don't realize there's
a fan in there until,

00:12:23.880 --> 00:12:24.900
you know, the system is able to do that.

00:12:24.900 --> 00:12:27.040
The system has been
banging around for a while.

00:12:27.200 --> 00:12:29.900
And there are more CPU cycles
available to do real work.

00:12:29.910 --> 00:12:35.400
So if you're not using the CPU to
just go pull for things to do,

00:12:35.400 --> 00:12:39.990
that leaves CPU cycles available
for other applications.

00:12:40.620 --> 00:12:41.700
And why not to pull?

00:12:41.700 --> 00:12:44.400
So it's just the converse.

00:12:44.400 --> 00:12:47.100
When you're pulling,
you're basically keeping a

00:12:47.100 --> 00:12:48.800
larger working set heated.

00:12:48.800 --> 00:12:51.540
And that means there's going to
be more contention for memory.

00:12:51.560 --> 00:12:54.190
When it's a single app doing this,
it's easy to say, you know,

00:12:54.190 --> 00:12:56.760
it doesn't really matter that
I'm going around this loop

00:12:56.760 --> 00:13:00.180
and making a few API calls and
touching a little bit of memory.

00:13:00.280 --> 00:13:02.360
You know, what can that hurt?

00:13:02.380 --> 00:13:06.240
But when you think about it in the
context of there are 20 applications

00:13:06.240 --> 00:13:09.790
running or 30 systems running on the
system that people have started up,

00:13:09.790 --> 00:13:12.520
forgotten about,
and they're sitting in the background,

00:13:12.590 --> 00:13:16.620
it starts to add up and it starts to make
for quite a bit of contention for memory.

00:13:17.900 --> 00:13:20.900
More contention for memory, more paging.

00:13:20.900 --> 00:13:23.140
If the system is paging,
it's a sluggish system.

00:13:23.140 --> 00:13:26.740
It's just not going to perform very well.

00:13:26.740 --> 00:13:28.800
And then back to our
power management point.

00:13:28.860 --> 00:13:31.110
The system,
if we kind of take this to an extreme,

00:13:31.110 --> 00:13:34.190
the system may never become idle,
or it may be idle for only

00:13:34.190 --> 00:13:37.690
short periods of time,
not enough for us to really do

00:13:37.970 --> 00:13:40.550
anything with the power management,
so the power management

00:13:40.580 --> 00:13:41.550
may be compromised.

00:13:41.560 --> 00:13:44.060
And, you know, the battery life shrinks.

00:13:44.060 --> 00:13:45.640
The fan is on all the time.

00:13:45.640 --> 00:13:47.540
And people are unhappy about that.

00:13:47.540 --> 00:13:50.040
But that's a part of
system performance that,

00:13:50.040 --> 00:13:52.490
you know, people pay attention to, right?

00:13:52.490 --> 00:13:54.200
How long that battery lasts.

00:13:54.200 --> 00:13:57.070
And it's an inefficient
use of system resources.

00:13:57.080 --> 00:13:59.160
You're going to be using the CPU,
touching memory,

00:13:59.290 --> 00:14:02.190
possibly causing pages to come
in off the disk to find out that

00:14:02.190 --> 00:14:03.970
you don't have anything to do.

00:14:03.970 --> 00:14:07.450
There isn't really anything
more inefficient than that.

00:14:07.490 --> 00:14:10.570
I mean,
that's just about as bad as it gets.

00:14:12.670 --> 00:14:15.390
All right, so touching memory.

00:14:15.470 --> 00:14:16.590
So we've got this CPU.

00:14:16.720 --> 00:14:18.360
He's executing some instructions.

00:14:18.360 --> 00:14:20.400
What happens?

00:14:20.820 --> 00:14:21.660
All right.

00:14:21.780 --> 00:14:25.790
So if everything is like really
operating nice and tight in your cache,

00:14:25.820 --> 00:14:30.500
you're just sitting in this L1 cache
and you're just banging away at it.

00:14:30.500 --> 00:14:34.770
The TLB there is the translation
look-aside buffer for doing

00:14:34.800 --> 00:14:41.170
the virtual to physical address
translations at the CPU level.

00:14:41.940 --> 00:14:43.420
That's a good place to be.

00:14:43.420 --> 00:14:49.980
As memory use expands,
then you get into maybe the L2 cache,

00:14:49.980 --> 00:14:52.330
the L3 cache.

00:14:52.330 --> 00:14:54.490
Now you're down to memory.

00:14:54.490 --> 00:14:57.790
We've color-coded this so
you can see that as we move

00:14:57.790 --> 00:15:02.040
further down the chain here,
things get slower and slower.

00:15:02.040 --> 00:15:05.470
So at this point, if we're actually
executing out of memory,

00:15:05.480 --> 00:15:09.440
so if our working set,
if the set of memory that we're touching

00:15:09.860 --> 00:15:14.740
has exceeded those first level caches,
if we're executing out of memory,

00:15:14.740 --> 00:15:16.640
we're running ten times
slower than the L1.

00:15:16.640 --> 00:15:22.730
And it's actually probably a little
worse than that on a faster processor.

00:15:23.320 --> 00:15:25.500
If we're actually hitting the disk,
right,

00:15:25.500 --> 00:15:30.540
to go get data or to go get code that
has to be executed in that polling loop,

00:15:30.660 --> 00:15:34.200
you're now running 10,000
times slower than the L1.

00:15:34.650 --> 00:15:37.260
So you can see why you want
to stay out of that case.

00:15:37.260 --> 00:15:42.420
The other thing to note is that if
you just touch one byte of memory,

00:15:42.420 --> 00:15:44.340
it has a lot bigger
impact than you think.

00:15:44.360 --> 00:15:47.330
Caches are organized
around 32-byte cache lines,

00:15:47.330 --> 00:15:47.900
right?

00:15:47.960 --> 00:15:52.120
So to get that one byte in,
you're evicting another cache

00:15:52.120 --> 00:15:54.680
line all through the L1,
L2, and L3.

00:15:54.680 --> 00:15:59.030
The TLB table is built around pages,
right?

00:15:59.080 --> 00:16:03.860
So if you have to address a
byte in a virtual page that it

00:16:03.890 --> 00:16:07.390
currently doesn't have an entry for,
it's going to evict an entry in

00:16:07.560 --> 00:16:09.580
order to make that translation.

00:16:09.580 --> 00:16:14.400
And that basically represents a
4K set of addresses that are being

00:16:14.410 --> 00:16:16.340
tossed in terms of translation.

00:16:16.340 --> 00:16:18.820
Memory.

00:16:18.820 --> 00:16:21.140
Memory,
we allocate everything in 4K pages.

00:16:21.140 --> 00:16:25.190
So the minimal size physical page of
memory that you're going to get when

00:16:25.190 --> 00:16:27.280
you touch a byte is going to be 4K.

00:16:27.280 --> 00:16:31.520
So that means, again,
your byte basically is causing

00:16:31.520 --> 00:16:33.500
4K worth of data to be evicted.

00:16:33.500 --> 00:16:39.540
And the disk is managed the same way
in terms of 4K pages to back that.

00:16:39.560 --> 00:16:46.180
But, again, up around the processor,
if you can keep your memory

00:16:46.180 --> 00:16:50.040
appetite small and you can
keep everything in the caches,

00:16:50.040 --> 00:16:52.720
or worst case, in the memory,
the system performs a lot better.

00:16:52.720 --> 00:16:55.620
Once it goes to the disk, things get bad.

00:16:55.640 --> 00:17:00.480
All right, so now let's move on to,
memory.

00:17:01.380 --> 00:17:04.100
All right, so again,
in terms of the fundamental

00:17:04.100 --> 00:17:06.960
thing that we're looking at here,
it's really the working set.

00:17:06.960 --> 00:17:11.400
This is really the thing that
controls system performance.

00:17:11.400 --> 00:17:13.740
And basically,
a working set are the pages

00:17:13.740 --> 00:17:17.000
that are being actively
touched by the application.

00:17:17.170 --> 00:17:21.360
So these could be in RAM--
hopefully they're in RAM-- but

00:17:21.360 --> 00:17:23.010
could be out on the disk also.

00:17:23.160 --> 00:17:25.220
And let me tell you,
when you're working sets--

00:17:25.330 --> 00:17:27.790
if you have a set of pages,
a set of memory that you have

00:17:27.900 --> 00:17:31.230
to go pounding through in
order to complete an operation,

00:17:31.330 --> 00:17:35.630
and you can't get it all to
fit into your physical RAM,

00:17:35.680 --> 00:17:39.500
that's a bad place to be.

00:17:39.840 --> 00:17:43.900
You have to be aware of that fact
that if you're banging on memory,

00:17:43.900 --> 00:17:46.880
it may actually be
coming in off of a disk.

00:17:47.040 --> 00:17:50.530
What does the working set
consist of in this case?

00:17:50.530 --> 00:17:53.770
Well,
it would be all of your heap or heaps

00:17:53.870 --> 00:17:56.050
and any code that you're executing.

00:17:56.220 --> 00:17:59.840
Every API that you use increases
the size of your working set,

00:17:59.840 --> 00:18:00.400
right.

00:18:00.400 --> 00:18:02.640
Sometimes it has long-term effects.

00:18:02.730 --> 00:18:06.530
So sometimes it's short-term like
it's -- you're heating up stack pages,

00:18:06.700 --> 00:18:11.650
running some code, other times, though,
it's actually leaving

00:18:11.650 --> 00:18:15.100
memory allocations behind,
right, so like window buffers,

00:18:15.320 --> 00:18:16.720
which can be quite large.

00:18:16.780 --> 00:18:23.280
So, again, be aware of the fact that
each API you use has a cost.

00:18:23.280 --> 00:18:24.180
Leaks.

00:18:24.180 --> 00:18:27.860
Leaks increase size
via heap fragmentation.

00:18:27.860 --> 00:18:27.860
So --

00:18:28.290 --> 00:18:33.180
Interestingly, as we've moved along in
the OS development and the

00:18:33.380 --> 00:18:37.160
OS has become more stable,
applications have become more stable,

00:18:37.160 --> 00:18:42.680
we've implemented things like
sleep capabilities for the system.

00:18:42.680 --> 00:18:47.020
People are not rebooting their systems
nearly as often as they used to.

00:18:47.020 --> 00:18:50.000
People are not quitting
their apps either.

00:18:50.000 --> 00:18:55.820
So what ends up happening is you now have
applications that are very long-lived.

00:18:56.570 --> 00:19:00.790
They may be actually running for
weeks at a time before they either

00:19:00.790 --> 00:19:04.130
quit or the system is rebooted.

00:19:04.470 --> 00:19:06.520
Time is your enemy when you have a leak.

00:19:06.520 --> 00:19:11.450
Even if the leak is slow,
eventually it will start to build up.

00:19:11.540 --> 00:19:15.380
What will happen is,
depending upon your usage patterns

00:19:15.400 --> 00:19:18.330
of how you're allocating memory
and how those leaks are showing up,

00:19:18.490 --> 00:19:21.420
it may actually be increasing
the size of your working set.

00:19:21.490 --> 00:19:24.880
So if the leak doesn't all happen
at once and just get paged out,

00:19:24.880 --> 00:19:24.880
then you're going to
have a lot of problems.

00:19:54.220 --> 00:19:57.240
You can look at your application,
and even if it's a slow leak,

00:19:57.250 --> 00:19:58.880
go after it and get rid of it.

00:19:59.310 --> 00:20:04.160
Another thing that you can assume is that
long-term caches are always paged out.

00:20:04.290 --> 00:20:07.540
So if you're creating a cache
and it's building up over time,

00:20:07.650 --> 00:20:12.500
when you go back to access it,
be very careful in how you do this.

00:20:12.500 --> 00:20:15.800
Try to avoid walking
through the whole cache.

00:20:15.800 --> 00:20:20.080
If you can just do cache lookups into it,
that's fine.

00:20:20.200 --> 00:20:25.500
But when you're doing maintenance,
be aware that when you go

00:20:25.500 --> 00:20:25.500
running through the whole cache,

00:20:25.670 --> 00:20:28.590
Most likely,
a really high degree of probability,

00:20:28.730 --> 00:20:30.580
it's been paged out,
which means you're going to

00:20:30.680 --> 00:20:33.840
page it all back in to do some
little bit of maintenance.

00:20:33.840 --> 00:20:37.800
You need to really be aware of that and
make sure that if you do have to do it,

00:20:37.800 --> 00:20:44.000
do it in a way that's efficient and
that you're not doing it too frequently.

00:20:44.100 --> 00:20:46.350
Best performance when
all the pages are in RAM.

00:20:46.410 --> 00:20:49.020
Again, can't stress that enough.

00:20:49.170 --> 00:20:52.790
The system performs a lot
better if it's not paging.

00:20:54.460 --> 00:20:57.980
So how does your memory
appetite and the VM,

00:20:57.980 --> 00:21:00.000
how do they interact?

00:21:00.010 --> 00:21:01.390
Well, here's the problem.

00:21:01.400 --> 00:21:05.400
Typically,
40% of the pages of the system are dirty.

00:21:05.490 --> 00:21:10.290
So these are valid pages that some
application wants to keep a hold on,

00:21:10.320 --> 00:21:11.390
but they're dirty.

00:21:11.400 --> 00:21:14.520
They've been written to,
which means that we're going to have

00:21:14.520 --> 00:21:20.100
to write those pages to disk whenever
unused pages are not available.

00:21:20.100 --> 00:21:22.870
New memory is accessed,
so if you go and allocate a new big chunk

00:21:22.870 --> 00:21:26.180
of memory and then go and access it,
the very first time you access it,

00:21:26.180 --> 00:21:29.370
we're going to have to provide
you some physical pages.

00:21:29.500 --> 00:21:32.060
Well, those physical pages have
to come from someplace,

00:21:32.230 --> 00:21:36.310
and the system is going to go through,
and there's an LRU to keep track

00:21:36.370 --> 00:21:38.400
of working sets and all that.

00:21:38.610 --> 00:21:43.410
But basically, when the system is
running kind of saturated,

00:21:43.420 --> 00:21:46.770
we're going to have to page pages out.

00:21:46.880 --> 00:21:51.090
So when you initiate file I/O,
that's also going to

00:21:51.350 --> 00:21:53.000
possibly trigger this.

00:21:53.000 --> 00:21:55.740
And of course, when you're paging in,
you're paging -- this is the worst case.

00:21:55.820 --> 00:21:58.550
If you're paging -- you're paging
pages back in that you need,

00:21:58.550 --> 00:22:01.240
and we're having to page pages out
in order to make room for them,

00:22:01.250 --> 00:22:01.630
right?

00:22:01.760 --> 00:22:06.080
So you're getting a dual kind
of I/O going on there that

00:22:06.080 --> 00:22:08.830
you don't want to have happen.

00:22:08.940 --> 00:22:11.380
Clean pages in the
system are just stolen,

00:22:11.580 --> 00:22:15.030
but again, 40% of the time,
when we go to steal a page,

00:22:15.030 --> 00:22:19.180
we're going to have to write it out when
the system is kind of in a steady state,

00:22:19.180 --> 00:22:24.380
you know, fully used or utilized
in terms of memory.

00:22:25.130 --> 00:22:31.750
All right, so let's talk a little bit
about working set impact.

00:22:31.840 --> 00:22:39.970
So here's a way to basically organize
a cache that is probably fairly common,

00:22:39.970 --> 00:22:39.970
where you have a header.

00:22:40.200 --> 00:22:43.040
attached to some data,
and you just kind of

00:22:43.370 --> 00:22:47.750
allocate the whole thing,
chain them together, and hopefully,

00:22:47.890 --> 00:22:51.120
you know, get some use out of the cache.

00:22:51.140 --> 00:22:55.430
But the thing to remember here is
that each time you touch any portion

00:22:55.600 --> 00:23:01.190
of that item--so this element--you're
bringing the whole page in.

00:23:01.190 --> 00:23:05.590
So if you just need to run the
list to update timestamps or,

00:23:05.740 --> 00:23:07.640
you know,
do some other maintenance on it,

00:23:07.640 --> 00:23:10.180
and you're not actually
even--you're not actively looking

00:23:10.180 --> 00:23:12.510
at the data at that point,
you're still causing the

00:23:12.510 --> 00:23:14.680
whole page to be brought in.

00:23:14.680 --> 00:23:18.030
So if you had a cache with, say,
you know, 10,000 elements and

00:23:18.040 --> 00:23:19.830
it organized this way,
and you chose to run through

00:23:19.910 --> 00:23:22.590
and do some maintenance on it,
you're touching, you know,

00:23:22.760 --> 00:23:25.380
basically 10,000 pages of memory.

00:23:25.400 --> 00:23:30.880
Unlikely that that will be sitting in the
cache or sitting in memory at this point.

00:23:31.220 --> 00:23:38.300
So here's another way to organize it that
reduces that in the case where you're

00:23:38.300 --> 00:23:39.880
having to do these maintenance functions.

00:23:39.880 --> 00:23:43.960
Break out the headers into
their own separate allocation,

00:23:43.960 --> 00:23:44.680
right?

00:23:44.680 --> 00:23:45.950
Point them off to the data.

00:23:45.960 --> 00:23:49.690
So at least in the case where
you're having to go through the

00:23:49.870 --> 00:23:52.400
headers to do something with
the timestamps or whatever,

00:23:52.400 --> 00:23:55.920
it minimizes the impact
on the working set.

00:23:55.960 --> 00:23:59.560
Now, with that said,
there is still a downside

00:23:59.560 --> 00:24:01.040
to going this route.

00:24:01.120 --> 00:24:01.750
There's a downside to it,
and that's one of the things

00:24:01.760 --> 00:24:04.320
that you have to be aware of,
which is if you're allocating these

00:24:04.320 --> 00:24:07.940
things into your cache over time,
so if there's a lot of temporal

00:24:07.940 --> 00:24:12.040
spacing between the allocations and
between the insertions into the cache,

00:24:12.040 --> 00:24:15.370
you're not really going to
help if you do each of these

00:24:15.430 --> 00:24:19.290
headers independently because,
again, they'll spread out

00:24:19.290 --> 00:24:22.640
through your working set,
and we'll end up with each one of those

00:24:22.670 --> 00:24:26.730
little headers being in its own page,
right, but mixed in with other things,

00:24:26.730 --> 00:24:29.860
not with your data,
and it won't have accomplished anything.

00:24:30.300 --> 00:24:33.180
So in this case,
if you absolutely have to have a cache,

00:24:33.180 --> 00:24:36.500
when you break it into
this kind of organization,

00:24:36.500 --> 00:24:40.350
make sure you cluster
allocate those headers.

00:24:40.360 --> 00:24:42.840
So, you know,
ask the system for enough memory

00:24:42.970 --> 00:24:45.790
so that you can have 100 headers,
right?

00:24:45.870 --> 00:24:47.850
And then you keep track
of that and dole them out,

00:24:47.850 --> 00:24:49.210
right, as the cache grows.

00:24:49.220 --> 00:24:50.930
And when you run out,
you ask the system for enough

00:24:50.930 --> 00:24:53.360
memory for another 100 headers,
so at least you keep

00:24:53.500 --> 00:24:56.810
the headers compressed,
right, in terms of the amount of physical

00:24:56.810 --> 00:24:58.190
memory that they're sitting on.

00:24:58.200 --> 00:25:00.250
So that's a lot better way to do it.

00:25:00.300 --> 00:25:01.780
that.

00:25:02.370 --> 00:25:05.390
Okay, so now we're done with memory.

00:25:05.490 --> 00:25:08.120
Moving on to file system I/O.

00:25:09.770 --> 00:25:14.640
All right,
so sequential versus random file I/O.

00:25:14.680 --> 00:25:15.800
This is an interesting thing.

00:25:15.800 --> 00:25:22.780
Again, we've kind of come to
expect that disks are fast.

00:25:22.840 --> 00:25:26.450
The kind of modern disks that we
ship now are easily able to do,

00:25:26.450 --> 00:25:28.700
you know,
40 megabytes per second sequential,

00:25:28.700 --> 00:25:30.250
a single disk.

00:25:30.330 --> 00:25:34.990
But that's going purely sequential,
which we often don't do in the system.

00:25:35.110 --> 00:25:38.160
And in fact, when we're paging,
we never do.

00:25:38.230 --> 00:25:43.040
when we're paging, we're doing much more
random accesses on the disk.

00:25:43.260 --> 00:25:46.910
And disks really have not gotten
any faster in the last six,

00:25:47.190 --> 00:25:49.340
seven years, eight years.

00:25:49.420 --> 00:25:53.160
They really don't
randomly seek much faster.

00:25:53.320 --> 00:25:55.730
And basically,
the throughput that you can get out

00:25:55.740 --> 00:26:00.940
of a disk that's purely random is
only around 0.4 megabytes per second.

00:26:00.940 --> 00:26:03.280
So it's about 1% of the
sequential throughput.

00:26:03.280 --> 00:26:05.970
It's about 100 IOs per second.

00:26:05.970 --> 00:26:08.370
That's really kind of pushing it.

00:26:08.460 --> 00:26:10.810
That's like a 7200 RPM drive, right?

00:26:10.820 --> 00:26:13.080
Some of the slower drives in a
Mac OS X system are really pushing it.

00:26:13.100 --> 00:26:17.700
But if you're using a portable
that don't spin that fast

00:26:17.700 --> 00:26:23.090
because of power considerations,
they're not going to even yield this.

00:26:23.900 --> 00:26:29.550
So that's just something to keep in
mind when you're accessing files,

00:26:29.550 --> 00:26:32.600
too, that, you know,
sequential access patterns are a lot

00:26:32.600 --> 00:26:35.710
better than random access patterns.

00:26:36.030 --> 00:26:37.890
Now, larger I/Os are more
efficient up to a point.

00:26:37.970 --> 00:26:42.870
We really like you to
not do one-byte reads.

00:26:42.930 --> 00:26:46.900
That's not a very efficient way to get
your data out of the file system cache.

00:26:46.900 --> 00:26:50.890
So larger I/O requests
are more efficient,

00:26:50.900 --> 00:26:54.900
but the thing to remember about this is
that your I/O buffer that you've created

00:26:54.900 --> 00:26:59.140
and that you're sucking data into,
that is eventually-- I mean,

00:26:59.140 --> 00:27:02.900
that basically consists of dirty
pages after the I/O is completed,

00:27:02.960 --> 00:27:04.900
or while the I/O is completing.

00:27:04.900 --> 00:27:07.900
So a reasonable size is 128K.

00:27:07.900 --> 00:27:09.900
I mean, that's--you know,
it could be smaller.

00:27:09.900 --> 00:27:11.900
It could be a little bit larger.

00:27:11.900 --> 00:27:16.470
That's a pretty reasonable size that
will pay back good dividends in terms

00:27:16.470 --> 00:27:21.290
of the efficiencies making the requests,
but doesn't get too much

00:27:21.320 --> 00:27:24.900
in the way of the system in
terms of a lot of dirty pages.

00:27:24.900 --> 00:27:27.700
Again, taking--you know,
taking this kind of

00:27:27.840 --> 00:27:30.090
thought to an extreme,
if you were to create

00:27:30.090 --> 00:27:34.900
an I/O buffer that was,
say, I don't know, 32 megabytes in size,

00:27:34.900 --> 00:27:38.900
and you issue a single read
of 32 megabytes to fill it,

00:27:38.900 --> 00:27:40.900
because you've got files
that are gigabytes in size,

00:27:40.900 --> 00:27:44.900
and you think, "Well, Joe said, you know,
"bigger is better when it comes to I/O,

00:27:44.900 --> 00:27:50.900
"so let me go to 32 megabytes." Well,
what would happen there potentially

00:27:50.900 --> 00:27:54.810
is that as the I/O is completing--
because it doesn't all complete at once,

00:27:54.910 --> 00:27:55.900
right?

00:27:55.900 --> 00:27:58.900
Early pages are being filled with data.

00:27:58.900 --> 00:28:02.240
As the I/Os are completing,
we're releasing those

00:28:02.240 --> 00:28:03.900
pages back to the system.

00:28:03.900 --> 00:28:08.900
By the time that you actually get
control back that your read is completed,

00:28:08.900 --> 00:28:11.950
if the system is busy doing other things,
we may have paged those

00:28:11.950 --> 00:28:13.900
pages out for you,
right?

00:28:13.900 --> 00:28:16.900
So your I/O buffer is now
sitting out in the swap file.

00:28:16.900 --> 00:28:18.900
And when you come back to touch it,
right,

00:28:18.900 --> 00:28:22.870
you're gonna pay the cost of sucking
it back in off the paging device again.

00:28:22.900 --> 00:28:26.900
So you'll actually end up with, you know,
doing at least two I/Os there,

00:28:26.900 --> 00:28:29.350
probably more,
because we'll probably also be paging

00:28:29.410 --> 00:28:31.900
stuff out again to make room for it.

00:28:31.900 --> 00:28:34.900
So that's why I say, you know,
it's up to a point.

00:28:34.900 --> 00:28:37.900
You don't want to take
this to an extreme.

00:28:37.900 --> 00:28:40.710
Another way to go would be to
investigate doing some file

00:28:40.820 --> 00:28:42.890
mapping for reading large files.

00:28:42.900 --> 00:28:47.900
So in the Cocoa world, I know this is,
you know, this is kind of the norm.

00:28:47.900 --> 00:28:50.900
In the Carbon world, you know,
this may not be something

00:28:50.900 --> 00:28:53.110
that you're used to doing,
but it's something that you

00:28:53.190 --> 00:28:56.900
should investigate using the,
you know, the basic system primitives

00:28:56.900 --> 00:29:00.880
for actually mapping files
and touching them in that way.

00:29:00.980 --> 00:29:02.900
The interesting thing about this,
of course,

00:29:02.900 --> 00:29:04.800
is that you don't create any dirty pages.

00:29:04.950 --> 00:29:07.890
If you're just reading, the pages,
you know, are brought into the system,

00:29:07.890 --> 00:29:09.900
but they're not dirty.

00:29:09.900 --> 00:29:13.900
They can just be tossed if they need
to be tossed in order to make room.

00:29:13.900 --> 00:29:15.900
We don't have to page them out.

00:29:15.900 --> 00:29:17.900
And if you go back and
retouch the memory,

00:29:17.900 --> 00:29:20.890
we just go back to the backing file and,
you know, retrieve it from there.

00:29:20.900 --> 00:29:24.900
There's no swapping involved at all.

00:29:26.300 --> 00:30:21.700
[Transcript missing]

00:30:22.910 --> 00:30:27.660
in 10, we're actually keeping track of
the access times in those cases.

00:30:27.660 --> 00:30:30.400
And those access times
are part of the metadata.

00:30:30.550 --> 00:30:34.420
Now, we don't write those out right away,
but every 30 seconds,

00:30:34.600 --> 00:30:37.470
the update process is gonna run,
and it's gonna issue a sync on

00:30:37.520 --> 00:30:41.040
the system to try to sync out
anything interesting to the disk.

00:30:41.190 --> 00:30:44.320
And it's gonna go ahead and sweep all
those up and sync them out to the disk.

00:30:44.410 --> 00:30:48.960
So if you're sitting there polling,
looking for some work to do

00:30:48.960 --> 00:30:51.890
by enumerating the directory,
looking for a file-- you know, say,

00:30:52.080 --> 00:30:55.710
waiting for a file to appear,
basically what you're causing

00:30:55.710 --> 00:30:59.100
to happen is every 30 seconds,
we're gonna write the disk.

00:30:59.100 --> 00:31:02.440
Okay, that's gonna keep the
disks from going to sleep.

00:31:02.440 --> 00:31:04.180
So that's not such a good thing.

00:31:04.180 --> 00:31:09.940
A better way to do that would be to
stat the file or do a getcat info

00:31:09.940 --> 00:31:13.190
and look at its modification date,
look at the directory's modification

00:31:13.190 --> 00:31:15.340
date to see whether it's changed or not.

00:31:15.420 --> 00:31:18.040
Because the stats,
just retrieving the information

00:31:18.040 --> 00:31:21.300
about that directory doesn't
cause the access date to change.

00:31:21.400 --> 00:31:24.510
So-- in that case,
you could sit there and keep, you know,

00:31:24.510 --> 00:31:26.650
accessing,
looking to see whether anything

00:31:26.650 --> 00:31:29.560
new has come into the directory,
but you're not having any

00:31:29.560 --> 00:31:34.280
impact on the file system in
terms of IOs going to the disk.

00:31:35.900 --> 00:31:55.900
[Transcript missing]

00:31:56.090 --> 00:31:59.860
you have to be aware as developers
that your application may actually

00:31:59.860 --> 00:32:02.330
be living out on a server.

00:32:02.430 --> 00:32:05.670
Or if it's not living on the server,
maybe the preference files or your

00:32:05.840 --> 00:32:09.860
history files or your cache files that
you're creating actually are ending

00:32:09.860 --> 00:32:14.500
up on a networked server someplace,
not on a local disk.

00:32:14.650 --> 00:32:17.100
So you need to take
that into consideration.

00:32:17.210 --> 00:32:19.060
You need to probably make-- well, you do.

00:32:19.060 --> 00:32:20.060
It's not probably.

00:32:20.060 --> 00:32:21.540
You need to test with that in mind.

00:32:21.540 --> 00:32:25.830
So when you're doing your final
testing to see how things perform,

00:32:25.930 --> 00:32:30.550
make sure you do some testing with things
targeted to networked file systems,

00:32:30.550 --> 00:32:33.150
not just off the local disk,
'cause that's the way your

00:32:33.150 --> 00:32:35.250
customer is gonna use it.

00:32:35.430 --> 00:32:37.540
And basically,
the thing to remember-- network

00:32:37.580 --> 00:32:39.180
performance is unpredictable,
right?

00:32:39.180 --> 00:32:44.300
So again,
if you're designing something that is

00:32:44.300 --> 00:32:50.300
predicated on your ability to predict how
long the I/O is gonna take to complete,

00:32:50.450 --> 00:32:53.960
then you need to really
make your customer aware,

00:32:53.960 --> 00:32:56.840
then, that he needs those files
to live on a local disk,

00:32:56.840 --> 00:33:00.980
that he cannot have them sitting
out on a network store someplace.

00:33:01.110 --> 00:33:04.800
And then the third point here is polling
models tend to overwhelm networks.

00:33:04.900 --> 00:33:08.980
Again, when it's one or two or three
machines that are doing it,

00:33:08.980 --> 00:33:09.820
it's not too bad.

00:33:09.820 --> 00:33:14.000
When it's 2,000, 3,000,
4,000 machines in a K-12, you know,

00:33:14.000 --> 00:33:19.430
institute or higher ed institute or,
you know, a larger corporation,

00:33:19.710 --> 00:33:22.740
the network administrators
really dislike that,

00:33:22.740 --> 00:33:23.420
right?

00:33:23.420 --> 00:33:26.620
If you're sitting there constantly,
banging on the net,

00:33:26.620 --> 00:33:31.070
looking to see whether a file
has come into existence or not,

00:33:31.230 --> 00:33:34.680
you know,
that's just not a very good thing to do.

00:33:34.760 --> 00:33:36.920
All right, so...

00:33:38.430 --> 00:33:41.500
The big message here,
the fundamentals are important.

00:33:41.500 --> 00:33:43.810
It's the most important thing
that you can be looking at,

00:33:43.840 --> 00:33:48.400
right, is that your app does
not run in isolation.

00:33:48.400 --> 00:33:52.240
Your app affects and is
affected by other apps.

00:33:52.240 --> 00:33:55.680
That's basically what we're saying
here is that when you're testing,

00:33:55.680 --> 00:34:00.000
don't assume that just because you've
reached a certain level of performance

00:34:00.130 --> 00:34:04.380
or efficiency that that is adequate,
right, testing in isolation.

00:34:04.380 --> 00:34:08.780
You need to really look at it, you know,
with some other typical applications

00:34:08.780 --> 00:34:12.880
that your customers might be running
when they're running your application.

00:34:12.910 --> 00:34:18.200
And app efficiency,
this directly drives system performance.

00:34:19.010 --> 00:34:24.800
We can provide you very efficient APIs,
which we really don't do yet,

00:34:24.850 --> 00:34:28.020
but we're working on it.

00:34:28.590 --> 00:34:31.360
But even if we did,
if the app is intent on

00:34:31.370 --> 00:34:34.480
using a lot of memory,
there's not a lot we can do about it.

00:34:34.560 --> 00:34:37.720
There's only so fast that we can
move pages in and off of a disk,

00:34:37.720 --> 00:34:38.320
right?

00:34:38.460 --> 00:34:41.110
Most of our systems ship with one disk.

00:34:41.360 --> 00:34:44.850
You know, there's contention for just
normal file system mixed in with

00:34:44.920 --> 00:34:46.670
all the paging that's going on.

00:34:46.690 --> 00:34:49.990
Really, the only way to keep
the system performance,

00:34:49.990 --> 00:34:54.500
you know, adequate is to everyone,
you know, stay on this diet, right,

00:34:54.500 --> 00:34:57.590
and really control your
appetite for memory.

00:34:57.600 --> 00:35:00.600
Minimizing the working size.

00:35:00.600 --> 00:35:02.800
I mean, that's, this is really the thing
that we're driving,

00:35:02.800 --> 00:35:04.600
or I'm driving at today.

00:35:04.600 --> 00:35:07.600
I mean,
this is the most critical element.

00:35:09.440 --> 00:35:13.460
And to reinforce this,
so let's take a look at what's

00:35:13.460 --> 00:35:17.200
going on in terms of working sets.

00:35:17.200 --> 00:35:20.070
Say you've got, you know,
so you've got a system working set which

00:35:20.360 --> 00:35:22.860
basically consists of like the finder,
the dock,

00:35:23.030 --> 00:35:26.830
all of the little demons and things
that run behind or back to make

00:35:26.830 --> 00:35:29.210
the system work the way it does.

00:35:29.210 --> 00:35:29.970
Wired memory in the kernel.

00:35:29.980 --> 00:35:32.780
There's a whole bunch of stuff
that just kind of sits there.

00:35:32.780 --> 00:35:36.370
Some other set of applications,
and I threw in, you know,

00:35:36.370 --> 00:35:38.690
IE and mail as some typical
things that might be running

00:35:38.690 --> 00:35:40.020
on the system all the time.

00:35:40.050 --> 00:35:43.280
Stuff that you probably would never,
you know, you never really quit.

00:35:43.310 --> 00:35:46.730
It doesn't have to be our mail program,
but any mail program.

00:35:46.940 --> 00:35:49.950
So say at this point,
the sum of all the working sets,

00:35:49.950 --> 00:35:52.960
so the things that everyone,
the working sets that

00:35:52.960 --> 00:35:56.390
the apps are keeping hot,
either through their idle loops

00:35:56.390 --> 00:36:00.020
or through work that's being
requested or event processing,

00:36:00.140 --> 00:36:00.940
whatever.

00:36:00.940 --> 00:36:06.130
Say that that represents 80% of the
physical memory that's available.

00:36:06.260 --> 00:36:08.690
At this point, this is really good.

00:36:08.700 --> 00:36:11.180
You've got 20% headroom here.

00:36:11.180 --> 00:36:13.980
The system is going to feel very snappy.

00:36:13.980 --> 00:36:17.440
You're going to be able to do
things with the applications

00:36:17.440 --> 00:36:20.690
that you haven't been doing,
and they're going to basically be able to

00:36:20.700 --> 00:36:23.140
happen without any page outs occurring.

00:36:23.260 --> 00:36:26.180
The user is going to be
pretty happy in this case.

00:36:28.920 --> 00:36:32.190
But now let's say the user decides
he wants to listen to some music,

00:36:32.200 --> 00:36:33.960
so he brings iTunes up.

00:36:34.030 --> 00:36:37.120
So now iTunes is put in the background
and he kind of forgets about it and

00:36:37.120 --> 00:36:39.750
it's playing elevator music or whatever.

00:36:39.840 --> 00:36:42.800
It's just providing some
entertainment for him.

00:36:42.920 --> 00:36:45.260
And say that that, again,
this is not to scale,

00:36:45.260 --> 00:36:50.500
not to say that iTunes utilizes 15%
of any physical memory footprint,

00:36:50.590 --> 00:36:55.180
but let's say that and maybe some
other apps that were launched

00:36:55.180 --> 00:36:58.360
pushed this up to 95%. Well,
at this point,

00:36:58.360 --> 00:37:01.660
as long as you don't sneeze,
the system is still going to be fine.

00:37:01.750 --> 00:37:05.400
So basically, don't touch it,
don't ask it to do anything

00:37:05.400 --> 00:37:08.800
that it isn't already doing,
and it's probably going to be okay.

00:37:08.910 --> 00:37:10.160
But there's very little headroom here.

00:37:10.160 --> 00:37:11.850
At this point,
almost anything that you do

00:37:11.850 --> 00:37:15.390
that you haven't been doing,
so if you launch another application,

00:37:15.510 --> 00:37:18.700
if you print and you haven't
printed in a long time,

00:37:18.700 --> 00:37:21.610
almost anything major that
you do is probably going to

00:37:21.620 --> 00:37:24.030
cause some paging to happen.

00:37:24.110 --> 00:37:40.860
But it's still not, you know,
it's not fatal.

00:37:40.860 --> 00:37:40.860
Basically what will end up happening
is there will be a little blip and

00:37:40.860 --> 00:37:40.860
then things will settle back down and,
you know, okay, fine.

00:37:40.860 --> 00:37:40.860
You know, it didn't feel quite as
good as it could have,

00:37:40.860 --> 00:37:40.860
but it wasn't horrible.

00:37:42.500 --> 00:37:43.310
All right.

00:37:43.590 --> 00:37:48.640
Now let's throw in, let's say, your app,
for instance.

00:37:48.800 --> 00:37:52.980
And again, not to say that any one app is
going to take this much space,

00:37:52.980 --> 00:37:55.680
but, you know,
just for purposes of keeping this short.

00:37:55.680 --> 00:38:00.700
And we push the working set up to 125%.
So the sum of all the working sets.

00:38:00.770 --> 00:38:04.730
So this is, you know, every second,
basically, you're trying to touch

00:38:04.820 --> 00:38:08.540
memory that represents 125%
of your physical footprint.

00:38:08.540 --> 00:38:11.330
You are now in, like, no man's land.

00:38:11.340 --> 00:38:13.310
This is, your system is thrashing.

00:38:13.310 --> 00:38:15.300
You don't want to be here.

00:38:15.300 --> 00:38:17.500
Anything that you do,
anything that you try to do is

00:38:17.500 --> 00:38:20.020
just going to be painfully slow.

00:38:20.020 --> 00:38:23.240
And there's not a whole
lot we can do about it.

00:38:23.240 --> 00:38:26.620
I mean, if the memory is being touched,
we have to shuffle it in and out.

00:38:26.690 --> 00:38:31.030
And we do our best,
but we can only go so fast.

00:38:33.730 --> 00:38:36.650
All right,
so how do we avoid that situation?

00:38:36.900 --> 00:38:39.240
We go on the system resource diet.

00:38:39.260 --> 00:38:41.550
Okay, so again,
we've been talking about this.

00:38:41.550 --> 00:38:44.500
You need to minimize your working set,
right?

00:38:44.500 --> 00:38:45.960
And a couple of good ways to do this.

00:38:45.960 --> 00:38:48.240
Lazy initialization, right?

00:38:48.310 --> 00:38:51.850
So when your app launches,
don't initialize everything that it could

00:38:51.850 --> 00:38:54.110
possibly do and touch a bunch of memory.

00:38:54.110 --> 00:38:57.380
Wait until the user actually
requests a particular service.

00:38:57.400 --> 00:39:00.570
I know a lot of apps have a
lot of really cool features,

00:39:00.570 --> 00:39:03.580
but they're not always used,
and certainly they're not always used.

00:39:03.600 --> 00:39:07.210
Event-driven, again,
we talked about that and we've

00:39:07.730 --> 00:39:10.860
talked about that in other sessions,
I'm sure.

00:39:10.930 --> 00:39:15.870
That will tend to really
help control the working set.

00:39:17.300 --> 00:39:20.100
don't optimize at the
expense of efficiency.

00:39:20.180 --> 00:39:23.160
So again,
if you look at it in isolation and you're

00:39:23.160 --> 00:39:26.530
trying to improve a particular aspect
of performance in your application,

00:39:26.640 --> 00:39:30.110
and you choose to do this by, say,
creating a very large

00:39:30.200 --> 00:39:33.430
cache because you say,
hey, the disks are too slow,

00:39:33.430 --> 00:39:34.480
I don't want to deal with it.

00:39:34.480 --> 00:39:36.020
I'm going to pull all
my data that I need,

00:39:36.020 --> 00:39:39.540
I'm going to slam it into a cache,
and that's what I'm going to use.

00:39:39.590 --> 00:39:42.750
Well, your app may actually
end up running slower,

00:39:42.750 --> 00:39:46.200
depending upon what the other
needs of the system are,

00:39:46.200 --> 00:39:48.080
other applications that are running.

00:39:48.080 --> 00:39:50.800
You can't assume that you can
have all the physical memory,

00:39:50.820 --> 00:39:51.250
right?

00:39:51.250 --> 00:39:53.860
There are other apps running,
they need to share that.

00:39:53.970 --> 00:39:56.290
And what will happen is your
cache will just get ending

00:39:56.500 --> 00:39:58.290
up being paged in and out.

00:39:58.290 --> 00:40:01.570
And it will be paged in and
out at our discretion and

00:40:01.700 --> 00:40:03.850
the patterns that we choose.

00:40:03.950 --> 00:40:06.300
And that may end up making your app,
you know,

00:40:06.300 --> 00:40:08.860
just not run as well as it could.

00:40:08.860 --> 00:40:12.030
Caches are not necessarily evil,
but they should be used, you know,

00:40:12.160 --> 00:40:12.710
as a cache.

00:40:12.710 --> 00:40:16.490
If you're going to be accessing
a particular cluster of data,

00:40:16.490 --> 00:40:20.440
you know,
over a very short period of time, sure,

00:40:20.690 --> 00:40:24.410
pull it in, keep it in the cache,
but basically understand that it

00:40:24.410 --> 00:40:27.830
might be better for you to release
it when you're done and then recreate

00:40:27.830 --> 00:40:30.760
it later when you need it again
as opposed to letting it go out,

00:40:30.920 --> 00:40:34.600
depending upon how long it, you know,
takes you to create it.

00:40:34.780 --> 00:40:37.370
You have to monitor
efficiency during development,

00:40:37.460 --> 00:40:37.940
right?

00:40:38.030 --> 00:40:40.620
This is not something that
you can do after the fact,

00:40:40.620 --> 00:40:42.550
or it's not done easily after the fact.

00:40:42.620 --> 00:40:48.230
You want to watch your CPU usage,
memory footprint, file system IOs.

00:40:48.240 --> 00:40:51.940
I'm going to talk about the tools
here in a couple minutes and,

00:40:51.940 --> 00:40:54.640
you know,
how you would apply the tools to do this,

00:40:54.640 --> 00:40:56.400
but this is really critical.

00:40:56.400 --> 00:40:59.870
If you're not monitoring,
you don't know where you're at.

00:40:59.960 --> 00:41:01.900
Don't test in isolation.

00:41:02.200 --> 00:41:04.900
So once you've decided that
your app is performing okay,

00:41:04.900 --> 00:41:09.120
come up with some kind of a test
scenario that includes half a dozen or,

00:41:09.120 --> 00:41:11.680
you know, seven,
eight other applications that you

00:41:11.680 --> 00:41:15.750
think that your customer might have
actively running on the system.

00:41:15.760 --> 00:41:19.090
And remember that, again,
we're encouraging them to

00:41:19.100 --> 00:41:20.950
leave the apps running,
right?

00:41:20.960 --> 00:41:22.320
I mean, that's what the dock is for.

00:41:22.320 --> 00:41:24.070
People launch applications.

00:41:24.080 --> 00:41:26.830
A week later,
they're still sitting in the dock, right?

00:41:26.910 --> 00:41:28.400
They've forgotten that
they even launched it.

00:41:28.400 --> 00:41:31.200
If that app is continuously polling,
you know,

00:41:31.200 --> 00:41:32.180
it's putting a load on the system.

00:41:32.180 --> 00:41:35.190
It's not doing anything for
the user at that point other

00:41:35.190 --> 00:41:36.560
than chewing up resources.

00:41:40.910 --> 00:41:42.560
All right, so here's a recipe.

00:41:42.560 --> 00:41:45.680
This is a basic recipe
for improving efficiency.

00:41:45.680 --> 00:41:49.100
And they're really, you know,
they're probably different ways

00:41:49.230 --> 00:41:52.470
that you can cut this a little bit,
but basically it all boils down to

00:41:52.490 --> 00:41:54.380
having some process that you can repeat.

00:41:54.410 --> 00:41:57.280
And something that is measurable.

00:41:57.280 --> 00:41:58.730
So you want to set a goal, right?

00:41:58.830 --> 00:42:00.660
You don't want to just
go into it thinking,

00:42:00.660 --> 00:42:03.910
well, I'll just start, you know,
making my code more efficient

00:42:04.030 --> 00:42:06.450
and we'll see where we,
you know, kind of get to.

00:42:06.450 --> 00:42:07.530
You want to set a goal.

00:42:07.530 --> 00:42:11.040
You want to look at how much memory
you're using currently and say,

00:42:11.210 --> 00:42:13.400
you know,
let's see if I can cut that down by

00:42:13.400 --> 00:42:15.930
a third or cut that down by half,
right?

00:42:15.930 --> 00:42:17.690
Set a very aggressive goal.

00:42:17.930 --> 00:42:20.310
Identify a metric that you can measure,
right?

00:42:20.310 --> 00:42:21.370
And then measure it.

00:42:21.510 --> 00:42:22.720
Get yourself a baseline.

00:42:22.730 --> 00:42:25.560
Really understand what your
app is doing at this point.

00:42:27.110 --> 00:42:29.460
Once you've done that,
use the tools that we're going

00:42:29.560 --> 00:42:33.820
to talk about to analyze where
that memory usage is coming from.

00:42:33.820 --> 00:42:38.060
So where in your application are you
actually allocating all that heap space?

00:42:38.060 --> 00:42:43.150
And then go through, analyze,
apply the changes that

00:42:43.150 --> 00:42:47.460
will bring that usage down,
and then iterate until the goal is met.

00:42:47.470 --> 00:42:49.040
It's not a one-step process.

00:42:49.040 --> 00:42:51.560
Often you're going to have
to go through this six,

00:42:51.560 --> 00:42:53.780
seven,
eight times in order to actually hit

00:42:53.780 --> 00:42:55.820
the goal that you've set for yourself.

00:42:56.240 --> 00:42:59.340
it's unlikely that you're going
to catch it the very first time.

00:43:02.700 --> 00:43:04.520
Okay, improving performance.

00:43:04.630 --> 00:43:08.900
So, it's the same recipe.

00:43:08.900 --> 00:43:08.900
So,

00:43:09.060 --> 00:43:11.240
I'm asking you to improve efficiency.

00:43:11.240 --> 00:43:13.880
Your customers are asking
you to improve performance,

00:43:13.880 --> 00:43:20.420
right, or specific performance
aspects of your product.

00:43:20.420 --> 00:43:22.460
You can use the same
recipe for doing this,

00:43:22.510 --> 00:43:22.990
right?

00:43:23.010 --> 00:43:25.440
You set a goal,
select a metric which will be

00:43:25.440 --> 00:43:27.620
different than one for efficiency,
right?

00:43:27.800 --> 00:43:29.480
Measure it, apply, and iterate.

00:43:29.640 --> 00:43:31.480
I mean, this will work, right?

00:43:31.560 --> 00:43:33.160
This works every time.

00:43:33.180 --> 00:43:36.700
Now, the thing to remember is you should
be monitoring all those efficiency

00:43:36.700 --> 00:43:39.900
metrics while you're doing this
so that while you're improving

00:43:39.900 --> 00:43:45.340
your performance in one area,
you don't want to become less efficient

00:43:45.340 --> 00:43:48.990
in terms of your fundamental usage.

00:43:51.310 --> 00:43:53.530
All right,
so here are some tools that we can

00:43:53.620 --> 00:43:57.020
use to actually go after this stuff.

00:43:57.020 --> 00:43:59.400
So to look at what you're
doing in terms of CPU usage,

00:43:59.430 --> 00:44:01.710
I've broken these things
up into two categories,

00:44:01.710 --> 00:44:02.850
basically, the tools.

00:44:02.920 --> 00:44:06.080
There are tools for monitoring,
which basically just give you

00:44:06.080 --> 00:44:08.050
an idea of where you stand,
right,

00:44:08.080 --> 00:44:12.580
what--how your--or how much of these
system resources that you're consuming.

00:44:12.700 --> 00:44:16.940
So a good one for CPU usage,
an easy one to use is top with either

00:44:17.000 --> 00:44:20.740
the minus D or the minus A option.

00:44:20.740 --> 00:44:23.930
The D option just gives you a snapshot,
basically, you know, a delta,

00:44:24.000 --> 00:44:28.030
so what happened in the last period,
one second period, five second period,

00:44:28.210 --> 00:44:30.260
whatever period that you select.

00:44:30.300 --> 00:44:33.300
The minus A option is kind of
interesting because it basically is

00:44:33.350 --> 00:44:37.380
an--it accumulates all of the data
from the point that you launched top,

00:44:37.380 --> 00:44:42.060
and it gives you a snapshot then of
what happened from that point through to

00:44:42.190 --> 00:44:44.000
the point where you decide to stop it.

00:44:44.000 --> 00:44:47.380
So it's really useful for kind
of getting an idea of what

00:44:47.380 --> 00:44:51.660
impact you have on the system,
like when you launch an app or

00:44:51.730 --> 00:44:57.200
you do a particular operation
like maybe print or save a file.

00:44:57.200 --> 00:45:01.120
You could start top with the minus
A option up right before you start

00:45:01.120 --> 00:45:04.680
the operation and then shut it down
right as the operation finishes,

00:45:04.680 --> 00:45:08.260
and it'll kind of tell you, okay,
for that period of time, you know,

00:45:08.410 --> 00:45:10.100
this was my impact on the system.

00:45:10.100 --> 00:45:15.030
I used this percentage of
the data to get this data.

00:45:15.030 --> 00:45:19.410
So it's a really useful tool,
and it's really useful to be able

00:45:19.410 --> 00:45:22.230
to make these kind of things happen.

00:45:22.360 --> 00:45:24.640
So I think that's a good point.

00:45:24.640 --> 00:45:31.350
And then the last thing, I think,
is that you can use the D or the minus

00:45:31.350 --> 00:45:37.810
A option to get the data from the
point that you launched the operation.

00:45:37.810 --> 00:45:41.560
So you could start with
the minus A option,

00:45:41.560 --> 00:45:47.450
and then you path was to
that particular primitive.

00:45:47.620 --> 00:45:49.570
There's Thread Viewer, which will,
if you've got a

00:45:49.570 --> 00:45:53.180
multi-threaded application,
is very useful in terms of

00:45:53.180 --> 00:45:56.500
determining which threads are,
you know, actually consuming the CPU.

00:45:56.500 --> 00:46:00.020
And then Shikari and
Monster are two new apps,

00:46:00.020 --> 00:46:05.320
or two new utilities,
that have a lot of power for really

00:46:05.320 --> 00:46:08.800
drilling down and finding out
what you're doing to the hardware,

00:46:08.800 --> 00:46:09.250
right?

00:46:09.300 --> 00:46:13.550
So this basically gives you full access
to all the performance registers,

00:46:13.760 --> 00:46:16.320
you know,
either on an individual application

00:46:16.320 --> 00:46:18.120
basis or on a system-wide basis.

00:46:18.150 --> 00:46:22.870
But if you want to find out,
or if you care what your cache hit

00:46:22.870 --> 00:46:26.810
rate is or how many branches per
second that you're executing or,

00:46:27.020 --> 00:46:31.230
you know, on and on and on,
those tools can, you know,

00:46:31.380 --> 00:46:34.060
really give you that view.

00:46:35.350 --> 00:46:40.380
Okay, for memory footprint, again, top,
just the basic default top will

00:46:40.380 --> 00:46:44.030
give you a pretty good view of
what's going on in terms of how

00:46:44.030 --> 00:46:46.110
much memory you're utilizing.

00:46:46.120 --> 00:46:53.400
Top-minus-W gives you a lot more
detail and also will provide you with

00:46:53.400 --> 00:46:58.250
a kind of running delta of what's
happened since you launched top.

00:46:58.340 --> 00:47:01.050
So if you run it that way
and you watch it over time,

00:47:01.050 --> 00:47:04.920
you can see whether or not your
memory footprint is actually growing.

00:47:05.360 --> 00:47:08.080
And it will detail that for you.

00:47:08.190 --> 00:47:12.100
So that's a pretty good
one to run longer term.

00:47:12.100 --> 00:47:16.450
Like if you had an automated test suite,
for instance, and you fired that up,

00:47:16.530 --> 00:47:18.870
I would fire up
top-minus-W along with it,

00:47:18.960 --> 00:47:22.480
let the automated test suite
run for however many hours

00:47:22.480 --> 00:47:25.880
or days that you run it,
and then take a look at kind of,

00:47:25.880 --> 00:47:28.760
you know, how much, you know,
did you grow in terms of

00:47:28.760 --> 00:47:30.420
your memory consumption.

00:47:30.420 --> 00:47:35.180
Leaks, okay, so again,
after you've been running for a while,

00:47:35.180 --> 00:47:37.170
run leaks, right?

00:47:37.410 --> 00:47:40.480
Sometimes the leaks are small enough
that you may not really notice them,

00:47:40.480 --> 00:47:42.480
but if they're, like,
as we talked about earlier,

00:47:42.480 --> 00:47:45.270
if they're there,
they do add up over time.

00:47:45.370 --> 00:47:50.880
So you want to keep, you know,
kind of on top of that.

00:47:50.950 --> 00:47:53.270
Don't let it go to the last minute.

00:47:53.400 --> 00:48:00.150
And then for analysis, we have heap,
which will tell you where things are,

00:48:00.190 --> 00:48:02.960
I guess, organized,
where your mallocs are sitting.

00:48:03.400 --> 00:48:09.040
Malloc debug and object alloc which
is more for our Objective-C friends,

00:48:09.060 --> 00:48:10.620
I think.

00:48:12.950 --> 00:48:15.190
File system I/O monitoring.

00:48:15.350 --> 00:48:16.640
Top-minus-D is good again.

00:48:16.640 --> 00:48:20.780
That just gives you a good
indication whether or not there's

00:48:20.780 --> 00:48:23.030
any file I/O going on at all,
right?

00:48:23.140 --> 00:48:26.560
If you see file I/O going on to
the disks or through the network,

00:48:26.560 --> 00:48:27.960
top will show you either.

00:48:28.070 --> 00:48:33.260
Then FS usage is a good tool for
monitoring and getting a better

00:48:33.330 --> 00:48:35.800
view as to what's going on.

00:48:35.910 --> 00:48:39.170
If you've used FS usage before,
we've added a couple of new

00:48:39.400 --> 00:48:40.790
things to it for Jaguar.

00:48:40.900 --> 00:48:45.990
It now actually reports any physical
I/Os that are actually occurring and

00:48:45.990 --> 00:48:50.800
also the physical block address on the
disk and how much I/O actually occurred.

00:48:50.800 --> 00:48:55.330
So you can really get a feel for
how random the performance of

00:48:55.330 --> 00:48:59.800
the disk really is in terms of
the I/Os that you're issuing,

00:48:59.800 --> 00:49:02.800
where they're actually
being pulled from the disk.

00:49:02.800 --> 00:49:08.800
It also breaks that up into plain data
versus metadata versus VM operations.

00:49:08.800 --> 00:49:13.800
So it gives you a pretty clear picture
of what you're doing to the disk.

00:49:13.960 --> 00:49:17.530
Sampler has a nice feature in
it that you can basically tell

00:49:17.620 --> 00:49:19.790
it to sit on any I/O operation.

00:49:19.800 --> 00:49:23.270
So you just kind of click a box, say,
"I want to know any time

00:49:23.270 --> 00:49:26.660
an I/O operation is called,
please record the callback stack for

00:49:26.660 --> 00:49:32.580
me so I can find out how I got to
that point." That's a very powerful

00:49:32.580 --> 00:49:36.800
way to be able to track where all
the I/O is coming from because

00:49:36.800 --> 00:49:36.800
sometimes you don't issue it directly,
right?

00:49:36.800 --> 00:49:43.090
It's some API that you're calling
that ends up being a fairly expensive

00:49:43.090 --> 00:49:46.800
operation in terms of file system I/O.

00:49:46.800 --> 00:49:49.790
So this would allow you
to see that that API,

00:49:49.800 --> 00:49:52.800
maybe you don't want to do
that API in your polling loop.

00:49:55.200 --> 00:50:03.070
So, in closing, applications that perform
efficiently should be your goal.

00:50:03.720 --> 00:50:07.090
We want you, you know,
we want all those great features.

00:50:07.180 --> 00:50:11.000
We want your apps to perform well,
but we want them to perform efficiently

00:50:11.040 --> 00:50:13.500
because that makes our customers happy,
right?

00:50:13.500 --> 00:50:15.040
That makes the whole system feel better.

00:50:15.040 --> 00:50:18.700
And it makes, quite frankly, you know,

00:50:18.700 --> 00:52:36.500
[Transcript missing]

00:52:38.840 --> 00:52:42.480
First, I want to thank Joe for
an excellent presentation.

00:52:42.480 --> 00:52:44.550
I hope you got a lot out of that.

00:52:45.260 --> 00:52:46.190
Some references.

00:52:46.210 --> 00:52:49.570
The Mac OS X System Overview book
is always a useful reference for

00:52:49.570 --> 00:52:53.760
understanding the system architecture
and how and where to get things done.

00:52:53.800 --> 00:52:56.860
Mac OS X Performance talks a
lot about the same principles

00:52:56.950 --> 00:53:01.000
that Joe's been talking about,
and they're available both on your

00:53:01.130 --> 00:53:06.630
disk in PDF form or as print-on-demand
through the developer website.

00:53:06.650 --> 00:53:09.360
We have our Tools page on
Apple Developer Connection where we

00:53:09.360 --> 00:53:12.800
talk about all of our performance
tools and performance tools

00:53:12.800 --> 00:53:14.950
available from the third-party world.

00:53:15.460 --> 00:53:16.940
A roadmap for today.

00:53:16.980 --> 00:53:20.160
Okay, in this session we have drilled
rather deeply into principles

00:53:20.160 --> 00:53:22.070
of developing for performance.

00:53:22.070 --> 00:53:26.080
In the next session in the tools track
we are going to talk about the compilers.

00:53:26.090 --> 00:53:28.290
And a good way of getting
good performance out of your

00:53:28.400 --> 00:53:31.220
applications is to understand
your compilation environment,

00:53:31.350 --> 00:53:33.860
understand exactly what the
compiler is doing and where

00:53:33.860 --> 00:53:35.340
we are going with compilers.

00:53:35.340 --> 00:53:38.500
That is in room J immediately
following this session.

00:53:38.770 --> 00:53:43.830
This afternoon we have both the
development tools feedback forum

00:53:43.830 --> 00:53:48.700
in room J1 and session 908,
which is not listed on this slide

00:53:48.700 --> 00:53:50.990
for reasons beyond my control.

00:53:50.990 --> 00:53:54.500
Session 908 talks about getting
the most out of project builder to

00:53:54.500 --> 00:53:56.840
utilize the best of the build system.

00:53:56.880 --> 00:54:01.020
And again the last session of the day,
debugging in Mac OS X right in this room,

00:54:01.020 --> 00:54:03.020
5:00 this evening.

00:54:03.060 --> 00:54:08.260
And with that I would like to invite
the Q&A panel up to the stage.

00:54:09.050 --> 00:54:12.290
For contact, discussion of performance,
performance tools,

00:54:12.290 --> 00:54:14.730
if you have any need to get in
touch with the engineering team,

00:54:14.730 --> 00:54:17.360
our development tools
engineering feedback address,

00:54:17.380 --> 00:54:22.300
macos10-tools-feedback
at group.apple.com.

00:54:22.320 --> 00:54:25.360
That's beginning to roll
off my tongue lightly now.

00:54:27.650 --> 00:54:32.540
is a useful way of contacting the entire
development tools engineering group,

00:54:32.580 --> 00:54:34.830
and I am always available
as a primary contact.