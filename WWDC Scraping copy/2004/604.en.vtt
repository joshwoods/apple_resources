WEBVTT

00:00:15.920 --> 00:00:16.800
Thank you.

00:00:16.800 --> 00:00:22.080
Good morning and welcome to session 604,
HPC System Design on Mac OS X.

00:00:22.140 --> 00:00:25.200
I know we want to give as much
time as possible to our guest

00:00:25.380 --> 00:00:27.440
speaker so let’s just dive right in.

00:00:29.580 --> 00:00:32.790
It seems hard to believe,
but we only last year released the

00:00:32.790 --> 00:00:35.150
Power Mac G5 at this very conference.

00:00:35.340 --> 00:00:38.800
The Power Mac G5 marked the
introduction of several new

00:00:38.800 --> 00:00:47.180
technologies to the Apple platform:
an incredible system architecture,

00:00:47.420 --> 00:00:51.230
gigantic memory bandwidth to
feed the processors driving

00:00:51.300 --> 00:00:54.660
over a gigahertz frontside bus,
an expansion bay running

00:00:54.660 --> 00:00:58.740
at 133 megahertz,
PCI-X cards.

00:00:58.740 --> 00:01:00.980
So very, very nice machine.

00:01:00.980 --> 00:01:05.210
And at the heart of it all, of course,
was the Power Mac

00:01:06.770 --> 00:01:10.830
The heart of it, of course,
was the PowerPC G5,

00:01:10.880 --> 00:01:14.090
which was the first chip to
emerge from an extraordinary

00:01:14.090 --> 00:01:17.290
partnership between Apple and IBM.

00:01:17.400 --> 00:01:23.170
But the PowerMac G5 and the G5 chip
were about to go places the designers

00:01:23.170 --> 00:01:26.090
of the PowerMac G5 never intended.

00:01:26.810 --> 00:01:29.880
We began getting inquiries from
the field sales team that support

00:01:29.880 --> 00:01:34.490
Virginia Tech that said that they wanted
to buy 1,100 of these machines and they

00:01:34.560 --> 00:01:37.040
wanted to turn them into a supercomputer.

00:01:37.260 --> 00:01:41.910
And they had ambitions to actually
make the top 10 on the supercomputer

00:01:41.910 --> 00:01:44.660
list on the top500.org list.

00:01:44.660 --> 00:01:49.030
The kicker was they had 90 days
to complete the first benchmark.

00:01:50.160 --> 00:01:53.290
The challenges were absolutely
enormous and it’s been

00:01:53.290 --> 00:01:54.940
compared to mounting an assault

00:01:55.610 --> 00:02:04.320
The Power Mac G5 only existed
in prototype stage at that time.

00:02:04.320 --> 00:02:07.580
The only prototypes were in Cupertino.

00:02:07.580 --> 00:02:11.700
The Infiniban architecture that was
needed to drive enough interconnect

00:02:11.700 --> 00:02:16.840
bandwidth between the machines
had not been ported to Mac OS X.

00:02:16.840 --> 00:02:20.580
This is a technology so complex
that the specification alone

00:02:20.710 --> 00:02:23.220
is over a thousand pages.

00:02:23.260 --> 00:02:26.150
I should point out that the
errata to that specification

00:02:26.150 --> 00:02:28.540
are several hundred pages.

00:02:28.540 --> 00:02:32.650
The Infiniban vendor, Mellanox,
had never touched Mac OS X and

00:02:32.650 --> 00:02:37.420
in fact the chief architect and
designer at Virginia Tech had also

00:02:37.620 --> 00:02:43.400
not touched Mac OS X so we had to
learn everything from the ground up.

00:02:45.150 --> 00:02:48.420
The timeline of course
was absolutely crushing.

00:02:48.420 --> 00:02:51.490
Everything had to work perfectly
and had to come together perfectly,

00:02:51.540 --> 00:02:55.580
both physical plant, the optimization of
the libraries and code,

00:02:55.680 --> 00:02:59.280
the drivers for the host channel
adapters for the Infiniban,

00:02:59.480 --> 00:03:02.420
switches had to be debugged,
everything had to come together,

00:03:02.450 --> 00:03:04.920
had to hit that first
benchmark in 90 days.

00:03:04.920 --> 00:03:08.340
At one point we actually were
doing follow the sun development.

00:03:08.440 --> 00:03:11.410
There was a team in Israel who
would hand off to a team in

00:03:11.410 --> 00:03:14.940
Virginia and in Boston who would
then hand off to Cupertino.

00:03:14.940 --> 00:03:18.100
We had a DTS engineer
working out of Ireland.

00:03:18.120 --> 00:03:22.080
So an incredible amount
of effort expended.

00:03:22.080 --> 00:03:23.360
At one point Dr.

00:03:23.610 --> 00:03:28.520
Varadjuran actually did several
short bursts of 20 hour coding

00:03:28.520 --> 00:03:31.070
with only cat naps here and there.

00:03:31.130 --> 00:03:35.780
So it was a truly extraordinary
amount of effort expended.

00:03:36.260 --> 00:03:39.640
So I don't think I need
to tell you the result.

00:03:39.640 --> 00:03:44.120
The result was Academia's
fastest supercomputer ever,

00:03:44.140 --> 00:03:51.040
the second fastest in the United States,
and the third in the world.

00:03:51.040 --> 00:03:55.410
There were large teams to thank
at Virginia Tech and at Apple,

00:03:55.410 --> 00:03:59.740
but we want to point out that this
venture was essentially doing everything

00:03:59.840 --> 00:04:01.700
that we expect of our developers.

00:04:01.770 --> 00:04:04.940
A developer has a vision,
and they want to execute

00:04:04.940 --> 00:04:06.600
that vision on our platform.

00:04:06.600 --> 00:04:10.420
And they engage the same resources that
we make available to every developer,

00:04:10.470 --> 00:04:13.250
our DTS services,
who in turn invoke engineering.

00:04:13.250 --> 00:04:18.320
Worldwide product marketing got involved,
technology evangelism got involved.

00:04:18.320 --> 00:04:20.700
So the same services that are
available to every developer are

00:04:20.960 --> 00:04:23.490
exactly what Virginia Tech did.

00:04:26.230 --> 00:04:29.200
So in the wake of this success,
we commissioned a video,

00:04:29.200 --> 00:04:30.840
what we call a success video.

00:04:31.070 --> 00:04:36.430
And as we interviewed Srinihdi,
he makes a statement in this video

00:04:36.490 --> 00:04:41.230
that says that with 5 million,
anyone can build a supercomputer.

00:04:41.290 --> 00:04:44.030
And everywhere we've shown this,
as we've taken our enterprise

00:04:44.030 --> 00:04:48.780
and HPC story on the road,
it's always gotten a good laugh because

00:04:48.780 --> 00:04:53.460
it's got to be more complex than that.

00:04:55.320 --> 00:05:00.640
To explain some of the methodology
and how you can incorporate

00:05:00.710 --> 00:05:04.830
the same type of system design
principles in your HPC systems,

00:05:04.830 --> 00:05:06.700
I would like to introduce Dr.

00:05:06.700 --> 00:05:10.110
Srivni Varadajran,
the Director of the TerraScale

00:05:10.180 --> 00:05:14.200
Computing Facility at Virginia Tech.

00:05:14.200 --> 00:05:14.200
Srivni, thank you.

00:05:19.800 --> 00:05:20.790
Thank you very much Skip.

00:05:20.790 --> 00:05:23.240
Thank you all for this opportunity.

00:05:23.350 --> 00:05:26.310
So today I just wanted to step
back from all the work that we

00:05:26.310 --> 00:05:30.040
have done on the machine and talk
about just building machines.

00:05:30.220 --> 00:05:32.720
What kind of applications,
how do you analyze your applications,

00:05:32.720 --> 00:05:34.800
how do you come up with an
architecture for a machine,

00:05:34.800 --> 00:05:38.870
how do you modify the place where
it's intended to go in order

00:05:38.960 --> 00:05:41.300
to take a resource that large.

00:05:41.680 --> 00:05:42.960
Simply put, a primer.

00:05:42.960 --> 00:05:46.160
It's an outline of the talk.

00:05:46.160 --> 00:05:48.220
I'll sort of introduce the
need for supercomputing and

00:05:48.410 --> 00:05:50.480
then go into architectures.

00:05:50.480 --> 00:05:52.040
How do you build machines like this?

00:05:52.040 --> 00:05:53.680
What are the various flavors out there?

00:05:53.680 --> 00:05:56.020
Supercomputing has been
around for a good 30,

00:05:56.020 --> 00:06:00.080
35 years, so there are a variety of
architectures that have been tried out.

00:06:00.080 --> 00:06:01.240
What's coming up in the future?

00:06:01.240 --> 00:06:05.510
What kind of software do you need
to be able to get performance

00:06:05.540 --> 00:06:06.900
out of systems like this?

00:06:06.900 --> 00:06:10.030
How do you tie all of these machines
together and make them behave like one?

00:06:11.730 --> 00:06:12.310
Then the deployment.

00:06:12.320 --> 00:06:16.440
So if you decide to go ahead and
follow a build-it-yourself approach,

00:06:16.440 --> 00:06:19.240
how do you go and deploy
a resource like this?

00:06:19.240 --> 00:06:19.960
How do you do it quickly?

00:06:19.960 --> 00:06:23.020
What kind of resources do you
internally need to gather in order

00:06:23.020 --> 00:06:24.500
to put up a system like this?

00:06:24.500 --> 00:06:27.160
And finally, a little bit of
perspectives on the future.

00:06:27.160 --> 00:06:29.500
What's going to be happening
a year or two down the line

00:06:29.500 --> 00:06:31.000
and even looking further out?

00:06:32.920 --> 00:06:35.540
The first question is why
supercomputing in the first place?

00:06:35.540 --> 00:06:37.180
There are a lot of places that
have built supercomputers,

00:06:37.180 --> 00:06:38.200
so why do you do it?

00:06:38.200 --> 00:06:41.840
The typical application for machines
like this is something called,

00:06:41.840 --> 00:06:44.320
comes from an area called
computational science and engineering.

00:06:44.320 --> 00:06:48.320
So if you look traditionally back
into the workings of science,

00:06:48.320 --> 00:06:53.370
there were two major ways in which
you could go about empirical work.

00:06:53.490 --> 00:06:56.580
One side you had theory,
and the other side you had experiment.

00:06:56.580 --> 00:06:58.280
And these two had always
gone hand in hand,

00:06:58.280 --> 00:07:00.790
500 years worth of history in there.

00:07:00.940 --> 00:07:03.860
About the 50s or so,
a third area opened up

00:07:03.880 --> 00:07:06.360
called computational science,
where essentially you start off

00:07:06.360 --> 00:07:10.420
from first principles and try to
derive higher level theories directly

00:07:10.420 --> 00:07:14.640
from first principles by computing
starting from very basic elements.

00:07:14.670 --> 00:07:17.480
Very basic elements whose laws are known.

00:07:17.480 --> 00:07:20.280
Now this requires a phenomenal
amount of computing capability,

00:07:20.310 --> 00:07:23.500
and over the years it's become
another mode of science,

00:07:23.500 --> 00:07:25.220
just like experiment and theory.

00:07:25.240 --> 00:07:28.700
The big advantage with computational
science is it lets you do things

00:07:28.700 --> 00:07:30.940
that you cannot really do in
experiment most of the time.

00:07:31.080 --> 00:07:34.320
It lets you see things that
are too large or too small,

00:07:34.320 --> 00:07:37.060
things that cannot be directly
observed by experimentation,

00:07:37.130 --> 00:07:39.060
but can be derived from
first principle laws.

00:07:39.230 --> 00:07:42.730
Most of the high-end work that's
going on today is occurring

00:07:42.740 --> 00:07:44.290
in computational science.

00:07:44.350 --> 00:07:48.370
And the goal is now you need
computers fast enough to start

00:07:48.370 --> 00:07:51.320
analyzing elements at very,
very small scales,

00:07:51.320 --> 00:07:55.070
and very high levels of compute
capability are needed there.

00:07:55.780 --> 00:07:58.690
So, in order to support
computational science,

00:07:58.800 --> 00:08:00.660
you need a fairly large
scale supercomputer.

00:08:00.780 --> 00:08:01.690
Now why?

00:08:01.690 --> 00:08:05.500
In the ideal world really,
one processor that can solve the

00:08:05.500 --> 00:08:10.340
entire problem would be the best
solution that you can come up with.

00:08:10.340 --> 00:08:12.010
You wouldn't need parallel processing,
you wouldn't,

00:08:12.030 --> 00:08:14.780
and that one processor itself
would be a supercomputer.

00:08:14.780 --> 00:08:16.100
Why is that interesting?

00:08:16.100 --> 00:08:20.110
Because it's much easier to program,
if not anything else.

00:08:20.130 --> 00:08:23.350
It's easier to program because
typical sequential logic from von

00:08:23.350 --> 00:08:25.360
Neumann is very well understood.

00:08:25.360 --> 00:08:26.910
We tend to think sequentially.

00:08:26.910 --> 00:08:29.280
There are very few branch
points in our thinking,

00:08:29.320 --> 00:08:31.600
and it's very easy to program,
very natural.

00:08:31.600 --> 00:08:33.760
Parallel systems are not
very easy to program.

00:08:33.760 --> 00:08:36.730
You've got to think about many
things happening simultaneously,

00:08:36.730 --> 00:08:39.750
making sure that all the things that
are happening simultaneously are

00:08:39.750 --> 00:08:41.310
happening at roughly the same speed.

00:08:41.340 --> 00:08:45.580
Otherwise,
there are some processors doing nothing.

00:08:45.580 --> 00:08:49.840
Now since you can't get one processor
that can handle the power essentially

00:08:49.840 --> 00:08:52.880
of these high-end applications,
you need to harness the

00:08:52.880 --> 00:08:55.690
power of a lot of processors,
all of them working in concert.

00:08:55.730 --> 00:08:57.600
Question is, how do you do this?

00:09:00.200 --> 00:09:04.540
So if you look at basic architectures,
the oldest of the architecture families

00:09:04.540 --> 00:09:06.450
is something called shared memory.

00:09:06.500 --> 00:09:11.240
So what you do in here is you take
a bunch of processors and enable

00:09:11.240 --> 00:09:14.210
them to access a common memory bank.

00:09:14.840 --> 00:09:16.600
So all of them access the same memory.

00:09:16.600 --> 00:09:19.060
The first architectures
used to be called UMA,

00:09:19.060 --> 00:09:21.800
Unified Memory Access,
Uniform Memory Access.

00:09:21.800 --> 00:09:24.680
So essentially,
if you try to access data from memory,

00:09:24.680 --> 00:09:28.800
all your processors could get
to memory at the same speed.

00:09:28.800 --> 00:09:31.820
Now what’s the obvious problem with this?

00:09:32.340 --> 00:09:34.420
If you have a lot of processors
trying to access the same memory,

00:09:34.420 --> 00:09:36.300
you don't have enough
bandwidth to get to memory.

00:09:36.300 --> 00:09:39.140
Then it's a moot point how
fast your processors are.

00:09:39.140 --> 00:09:41.240
If you can't feed them data,
if you can't even feed

00:09:41.240 --> 00:09:43.860
them their instructions,
they're sitting there idling.

00:09:43.860 --> 00:09:47.650
This would be equivalent of putting
a Ferrari engine and putting

00:09:47.650 --> 00:09:49.610
bicycle tires on the vehicle.

00:09:49.620 --> 00:09:51.110
It's not going to do very well.

00:09:51.120 --> 00:09:53.640
So that's basically a problem.

00:09:53.640 --> 00:09:54.850
You cannot feed data to it.

00:09:54.860 --> 00:09:58.510
The next generation of that came
up with something called NUMA,

00:09:58.510 --> 00:10:00.440
non-uniform memory access.

00:10:01.380 --> 00:10:04.300
So the memory really was distributed
in many banks across the system.

00:10:04.300 --> 00:10:06.610
Each processor could really
access all the memory.

00:10:06.620 --> 00:10:09.230
But the time to get data from
memory varied depending on

00:10:09.230 --> 00:10:10.840
where the memory was located.

00:10:10.840 --> 00:10:13.280
So if you have a supercomputer
the size of this room,

00:10:13.280 --> 00:10:16.800
depending upon where exactly the
memory was that held your data,

00:10:16.800 --> 00:10:18.320
your latency would change.

00:10:18.320 --> 00:10:21.780
Most modern shared memory machines
follow the NUMA architecture.

00:10:23.530 --> 00:10:26.190
Shared memory machines are custom.

00:10:26.200 --> 00:10:27.650
You can get small SMPs.

00:10:27.850 --> 00:10:30.660
Basically,
these are blown-up portions of SMPs.

00:10:30.660 --> 00:10:35.920
So in your Apple, in a PalMac, G5 or XR,
you have a two-processor SMP.

00:10:35.930 --> 00:10:38.500
That’s about right for the kind of memory
bandwidth that you have in the machine.

00:10:38.500 --> 00:10:42.420
But if you look at larger SMPs,
which are shared memory machines,

00:10:42.420 --> 00:10:46.800
you can go to 16, 32 or even larger,
these are completely custom machines.

00:10:46.800 --> 00:10:49.720
And their price doesn’t scale linearly.

00:10:49.720 --> 00:10:53.120
The more processors you add,
if you have four processors on a board

00:10:53.310 --> 00:10:56.750
versus eight processors on a board,
eight processors on a board

00:10:56.790 --> 00:10:57.880
is not twice as expensive.

00:10:57.880 --> 00:11:00.870
It��s significantly more than
twice as expensive to do that.

00:11:00.870 --> 00:11:02.920
So it does not scale very easily.

00:11:02.920 --> 00:11:04.960
On the other hand,
it has a very good advantage,

00:11:05.210 --> 00:11:06.120
very easy to program.

00:11:07.540 --> 00:11:10.840
Threaded programming is one of
the standard things at Statham,

00:11:10.870 --> 00:11:13.900
say junior operating
systems in computer science.

00:11:13.900 --> 00:11:15.260
It’s been done for many years.

00:11:15.260 --> 00:11:17.870
There are a lot of other applications
that use threaded programming.

00:11:17.890 --> 00:11:19.240
It’s a very simple model.

00:11:19.370 --> 00:11:25.380
There are even things called
parallelizing compilers that if you

00:11:25.400 --> 00:11:29.890
give them a piece of sequential code,
we’ll attempt to extract

00:11:29.890 --> 00:11:29.890
parallelism out of it that can
run on a shared memory machine.

00:11:30.100 --> 00:11:31.490
They don't do very well necessarily.

00:11:31.620 --> 00:11:34.090
So if you have 32 processors and
you let a paralyzing compiler

00:11:34.090 --> 00:11:37.290
take your code and run it,
you're not going to get 32 times

00:11:37.290 --> 00:11:38.710
the performance of one processor.

00:11:38.840 --> 00:11:41.000
You'd be lucky if you
get about five or six.

00:11:41.040 --> 00:11:43.760
So you pay,
the trade-off that you make over there

00:11:43.760 --> 00:11:48.000
is you don't get the same kind of
efficiency that you could possibly get.

00:11:48.000 --> 00:11:53.790
However, as a computational scientist,
a lot of us like to spend as little

00:11:53.790 --> 00:11:56.000
time as possible paralyzing the code.

00:11:56.000 --> 00:11:58.540
We want some automated system
to take it over from us.

00:11:58.640 --> 00:11:59.530
Here, I've written this code.

00:11:59.560 --> 00:12:00.530
It works on my desktop.

00:12:00.630 --> 00:12:02.000
Make it run on your supercomputer.

00:12:02.000 --> 00:12:03.980
That's the basic idea there.

00:12:04.150 --> 00:12:07.750
And in that model,
shared memory works quite well.

00:12:07.940 --> 00:12:09.930
So big advantages are
relatively easy to program.

00:12:10.130 --> 00:12:12.260
Disadvantages, they don't scale.

00:12:12.320 --> 00:12:14.450
You cannot build very large
shared memory machines.

00:12:14.590 --> 00:12:17.080
They not only get
prohibitively expensive,

00:12:17.080 --> 00:12:19.610
their performance falls pretty rapidly.

00:12:21.690 --> 00:12:24.900
The other kind of architecture out
there is called distributed memory.

00:12:24.900 --> 00:12:27.160
So you have no common pool of memory.

00:12:27.160 --> 00:12:30.150
Every processor has its own memory
and you hook a whole bunch of

00:12:30.150 --> 00:12:33.560
these processors together and
that's your parallel architecture.

00:12:33.800 --> 00:12:35.700
So how do you program these machines?

00:12:35.830 --> 00:12:38.300
If you don't have a common memory,
you can't have different

00:12:38.300 --> 00:12:41.130
parts of your processor,
different threads exchange

00:12:41.130 --> 00:12:42.350
data through memory.

00:12:42.450 --> 00:12:44.550
Instead you send messages.

00:12:45.000 --> 00:12:46.520
So effectively,
you have 1,000 processors,

00:12:46.520 --> 00:12:49.150
all of which are communicating with
each other just by sending messages.

00:12:49.160 --> 00:12:53.520
So one processor would say,
processor 1 would say, processor 3,

00:12:53.660 --> 00:12:54.960
go ahead and do this.

00:12:54.960 --> 00:12:56.840
Processor 5, go ahead and do this.

00:12:56.840 --> 00:12:59.990
In return, it might be expecting data
from processor 8 in response to

00:12:59.990 --> 00:13:01.640
which it will do something else.

00:13:01.640 --> 00:13:06.700
So essentially, each processor runs a
copy of the application.

00:13:06.700 --> 00:13:08.760
And depending on the data it gets,
it does different things.

00:13:08.760 --> 00:13:13.310
And all the state is maintained just by
exchanging messages across processors.

00:13:14.380 --> 00:13:17.980
The big advantage with distributed
memory machines is they're inexpensive.

00:13:17.980 --> 00:13:21.240
They're also very good at scaling.

00:13:21.240 --> 00:13:23.590
You can build very,
very large distributed memory

00:13:23.590 --> 00:13:25.320
machines that scale very nicely.

00:13:26.870 --> 00:13:27.600
But they're hard to program.

00:13:27.620 --> 00:13:31.060
The trade-offs that you make when
you go to message passing systems,

00:13:31.060 --> 00:13:32.600
it's very hard to program.

00:13:32.600 --> 00:13:37.670
There's a fairly steep learning curve,
after which things start settling down.

00:13:37.680 --> 00:13:39.400
And that learning curve
might last a year or two.

00:13:39.400 --> 00:13:41.760
In fact, very simple applications
like Hello World,

00:13:41.760 --> 00:13:44.800
which is probably the first
C program everybody writes,

00:13:44.800 --> 00:13:48.280
writing a parallel version of
it invariably generates errors.

00:13:48.280 --> 00:13:51.240
So I've seen a lot of
programmers start off saying,

00:13:51.240 --> 00:13:52.090
hey, this should be easy.

00:13:52.100 --> 00:13:53.740
We're just sending one message.

00:13:53.740 --> 00:13:55.890
And there's always
something wrong in there.

00:13:56.800 --> 00:14:01.180
So MPI, which is the most common message
passing interface that's used over here,

00:14:01.180 --> 00:14:02.460
is somewhat hard to program.

00:14:04.030 --> 00:14:07.820
Another kind of architecture which was
started off in the 70s is something

00:14:07.820 --> 00:14:11.560
called SIMD or vector processors,
single instruction multiple data.

00:14:11.640 --> 00:14:15.210
So the idea over here is if you
look at scientific applications,

00:14:15.210 --> 00:14:17.690
they do a lot of very regular operations.

00:14:17.840 --> 00:14:21.520
So if you know this fact,
then you can try to exploit it.

00:14:21.650 --> 00:14:24.240
Simple example is,
let's say I've got two matrices,

00:14:24.240 --> 00:14:26.080
and I want to multiply them.

00:14:26.110 --> 00:14:28.950
So all of us have looked at high
school matrix multiplication.

00:14:29.020 --> 00:14:31.750
So you take a row on one matrix,
and the column on the other matrix,

00:14:31.770 --> 00:14:33.760
the first column, we say first row,
first matrix,

00:14:33.850 --> 00:14:36.100
first column on the second matrix,
multiply each element,

00:14:36.100 --> 00:14:37.620
add the whole thing together.

00:14:37.650 --> 00:14:39.300
Now, what are you doing here?

00:14:39.450 --> 00:14:41.800
You're multiplying one of these elements.

00:14:41.800 --> 00:14:42.700
The instruction is the same.

00:14:42.810 --> 00:14:43.980
You're just multiplying.

00:14:44.000 --> 00:14:45.800
The data is different.

00:14:45.810 --> 00:14:48.230
That's what single instruction
multiple data does.

00:14:48.330 --> 00:14:52.640
It executes the same instruction on
multiple units of data simultaneously.

00:14:52.650 --> 00:14:55.550
So if you know that your
application domain has a lot

00:14:55.550 --> 00:14:58.230
of this kind of parallelism,
where the same instruction is

00:14:58.300 --> 00:15:01.300
being repeated over and over over
a lot of different pieces of data,

00:15:01.470 --> 00:15:04.750
then SIMD architectures
of vectors work very well.

00:15:04.760 --> 00:15:07.680
And vector processors,
this was pioneered first by Cray,

00:15:07.680 --> 00:15:10.520
the first Cray machines that came
out in the '70s and then the '80s.

00:15:10.630 --> 00:15:13.310
They do very well in
scientific computing.

00:15:13.430 --> 00:15:14.380
They're highly efficient.

00:15:14.380 --> 00:15:19.010
They deliver numbers that are not that
far from peak for optimized applications.

00:15:19.060 --> 00:15:20.980
On the other hand,
they're completely custom designs.

00:15:21.030 --> 00:15:23.120
The processor has to
be designed ground up.

00:15:23.120 --> 00:15:27.190
And they don't do very
well on scalar codes.

00:15:27.370 --> 00:15:29.080
So what do you mean by scalar codes?

00:15:29.240 --> 00:15:31.350
So in vector codes,
you have the same instruction

00:15:31.360 --> 00:15:33.500
executing on different pieces of data.

00:15:33.530 --> 00:15:37.110
But if you look at a typical program,
you don't necessarily

00:15:37.110 --> 00:15:39.060
have this regularity.

00:15:39.070 --> 00:15:41.640
You're executing different
things one after another.

00:15:41.660 --> 00:15:43.160
Then vectors don't do very well.

00:15:43.220 --> 00:15:45.720
The second thing vectors don't
like at all are conditionals.

00:15:45.720 --> 00:15:48.660
If A is greater than B, do this.

00:15:48.660 --> 00:15:50.980
Else, do something else.

00:15:51.510 --> 00:15:54.800
That kind of code breaks your
pipeline for the vector and then

00:15:54.930 --> 00:15:56.830
your performance can be very poor.

00:15:56.900 --> 00:16:00.900
In fact, it will be much worse than a
desktop machine in many cases.

00:16:02.700 --> 00:16:06.880
- Scalar processors are what
typically get deployed in desktops.

00:16:07.060 --> 00:16:08.860
So they're also called
multiple instruction,

00:16:08.860 --> 00:16:09.600
multiple data.

00:16:09.630 --> 00:16:12.240
What they basically do is execute
independent instructions on

00:16:12.240 --> 00:16:14.210
independent pieces of data.

00:16:14.340 --> 00:16:16.740
The first generation of
scalar architectures executed

00:16:16.780 --> 00:16:18.300
instructions one at a time.

00:16:18.320 --> 00:16:20.680
Modern ones are called superscalar,
that is they can actually

00:16:20.680 --> 00:16:23.500
execute multiple instructions
simultaneously inside the processor.

00:16:23.540 --> 00:16:25.680
And there are some limitations
on what kind of instructions

00:16:25.680 --> 00:16:27.070
you can put together.

00:16:27.100 --> 00:16:31.040
For instance,
if you look at the PowerPC 970,

00:16:31.060 --> 00:16:33.360
you can execute about five
instructions simultaneously.

00:16:33.380 --> 00:16:35.380
One of which is a branch,
two of which are floating

00:16:35.380 --> 00:16:36.940
point operations,
and two of which are

00:16:36.940 --> 00:16:38.060
load store operations.

00:16:38.220 --> 00:16:39.860
So these are superscalar designs.

00:16:39.870 --> 00:16:42.360
They can execute multiple
things simultaneously.

00:16:42.470 --> 00:16:45.820
The advantage is their application
domain is fairly broad.

00:16:45.820 --> 00:16:48.680
They run all the way from
desktops onwards to servers

00:16:48.720 --> 00:16:49.920
to many other platforms.

00:16:49.920 --> 00:16:55.080
And hence, just sheer volume makes
them relatively inexpensive.

00:16:55.080 --> 00:16:57.960
It's volume at the end of the
day that gets your prices down.

00:16:57.960 --> 00:17:01.030
And they can handle a fairly
broad variety of applications.

00:17:01.080 --> 00:17:04.460
On the negative side,
they're not necessarily as efficient

00:17:04.510 --> 00:17:07.440
for very regular operations,
because they don't necessarily

00:17:07.530 --> 00:17:11.490
exploit the patterns that show
up in scientific applications.

00:17:12.450 --> 00:17:15.950
So if you look at some examples of
modern supercomputing architectures,

00:17:15.950 --> 00:17:19.150
in vector systems you have
the Earth Simulator in Japan,

00:17:19.150 --> 00:17:20.630
which is an NECSX6.

00:17:20.670 --> 00:17:22.400
It's a custom vector processor.

00:17:22.560 --> 00:17:28.290
There are a total of 5,400,
5,500 odd processors in the system.

00:17:28.400 --> 00:17:32.380
And they're an interesting mix of
shared memory and distributed memory.

00:17:32.730 --> 00:17:36.780
64 processors see the
same shared memory image.

00:17:37.000 --> 00:19:05.900
[Transcript missing]

00:19:06.880 --> 00:19:09.300
So why clusters?

00:19:09.380 --> 00:19:11.420
So if you look at the way
processor performance grows,

00:19:11.420 --> 00:19:12.800
grows by Moore's Law.

00:19:12.840 --> 00:19:19.840
This was a sort of axiomatic
presentation up in the 70s,

00:19:19.890 --> 00:19:22.340
where they noticed that
processor performance,

00:19:22.340 --> 00:19:26.810
actually VLSI complexity really,
translates to processor performance,

00:19:26.810 --> 00:19:28.660
doubles every 18 months.

00:19:28.660 --> 00:19:31.330
So every 18 months,
you're going to see a processor twice

00:19:31.330 --> 00:19:33.210
as fast as the one you're running today.

00:19:33.940 --> 00:19:37.460
This is an exponential curve,
and it's a fairly deadly curve,

00:19:37.460 --> 00:19:40.560
in that if you take too long
in designing your machine,

00:19:40.560 --> 00:19:44.010
by the time your machine comes out,
it's going to be a processor

00:19:44.010 --> 00:19:45.980
a lot faster than yours.

00:19:45.980 --> 00:19:49.160
The problem is if you're a custom design,
you need to spend a lot

00:19:49.240 --> 00:19:52.040
of time on your machine,
because you've got everything

00:19:52.040 --> 00:19:55.200
from board design to processor
design that has to occur.

00:19:55.200 --> 00:19:57.490
All of this has to go to a foundry,
get manufactured,

00:19:57.540 --> 00:20:00.050
and by the time it comes out
and your first machine's out,

00:20:00.050 --> 00:20:02.740
there are processors available
twice as fast as yours.

00:20:02.740 --> 00:20:03.450
By the time you end up with a processor,
it's going to be a processor

00:20:03.450 --> 00:20:03.860
twice as fast as yours.

00:20:03.860 --> 00:20:04.190
If you're a custom design,
you're going to have to spend

00:20:04.190 --> 00:20:04.560
a lot of time on your machine,
because you've got everything

00:20:04.560 --> 00:20:05.160
from board design to processor
design that has to occur.

00:20:05.160 --> 00:20:06.060
All of this has to go to a foundry,
get manufactured,

00:20:06.060 --> 00:20:07.020
and by the time it comes out
and your first machine's out,

00:20:07.020 --> 00:20:07.990
there are processors available
twice as fast as yours.

00:20:08.060 --> 00:20:09.600
And you can't turn around quickly.

00:20:09.660 --> 00:20:12.060
Custom designs take a
long time to generate,

00:20:12.060 --> 00:20:14.360
and you can't turn around very quickly.

00:20:14.460 --> 00:20:19.190
So essentially, clusters,
on the other hand, you can build quickly.

00:20:19.200 --> 00:20:22.110
So you want to be able to
harness the power of Moore's Law,

00:20:22.110 --> 00:20:24.400
use it to your advantage, essentially.

00:20:24.440 --> 00:20:26.690
So the goal, basically,
is design quickly.

00:20:26.720 --> 00:20:29.890
Don't waste too much time
in the design phase of it,

00:20:30.000 --> 00:20:31.490
but do it thoroughly.

00:20:31.520 --> 00:20:33.880
You'll have to spend a lot of
effort in the design phase.

00:20:33.880 --> 00:20:36.060
But cut the time down,
because the longer you

00:20:36.060 --> 00:20:38.970
wait in your design phase,
you cannot do a one-year design or

00:20:38.970 --> 00:20:41.100
a one-and-a-half-year design cycle.

00:20:41.100 --> 00:20:44.580
Because during that period,
your entire design essentially is

00:20:44.850 --> 00:20:46.670
worthless by the time it comes out.

00:20:46.740 --> 00:20:49.750
And you also have to
move very aggressively.

00:20:49.780 --> 00:20:52.920
Because you're going to base your
designs on the latest processor,

00:20:52.920 --> 00:20:56.240
do your design and do your construction
in a shorter period of time.

00:20:56.240 --> 00:20:57.690
Otherwise, you're picking depreciation.

00:20:57.700 --> 00:20:59.880
You're going to be paying
for your machine on day one.

00:20:59.900 --> 00:21:04.340
And after that,
the last... The longer you

00:21:04.340 --> 00:21:06.820
take to deploy your solution,
the more depreciation you're picking up,

00:21:06.820 --> 00:21:07.460
essentially.

00:21:07.460 --> 00:21:11.340
So custom architectures,
this is where they face a problem.

00:21:11.340 --> 00:21:14.030
Their lead times for design
and deployment are much longer,

00:21:14.030 --> 00:21:15.210
which is what hurts you.

00:21:17.480 --> 00:21:20.290
So let's look at some architectures.

00:21:20.410 --> 00:21:22.540
So if you want to design
a cluster system now,

00:21:22.580 --> 00:21:23.990
how do you go about doing it?

00:21:24.070 --> 00:21:27.380
What variables do you take
into account in this design?

00:21:27.680 --> 00:21:30.030
First thing to do is a
very simple question.

00:21:30.080 --> 00:21:33.390
Look at your applications and
figure out if they are tightly

00:21:33.390 --> 00:21:35.200
coupled or loosely coupled.

00:21:35.330 --> 00:21:36.780
So what do you mean by this?

00:21:37.050 --> 00:21:40.240
In loosely coupled applications,
you take a parallel application,

00:21:40.290 --> 00:21:42.170
it runs across a bunch of processors.

00:21:42.170 --> 00:21:45.390
They don't really exchange that
much data among themselves.

00:21:45.850 --> 00:21:47.760
they can almost proceed independently.

00:21:47.800 --> 00:21:50.080
There are some great
examples in bioinformatics.

00:21:50.080 --> 00:21:54.800
Blast, the genome search engine,
is very good at doing things in parallel,

00:21:54.860 --> 00:21:57.410
simply because it really
doesn't need much input.

00:21:57.570 --> 00:22:00.000
There are other domains of parallel
applications where you can use

00:22:00.000 --> 00:22:01.620
something called data parallelism.

00:22:01.720 --> 00:22:04.540
So let's say you want to crunch
through X amount of data,

00:22:04.590 --> 00:22:06.620
and you have N processors.

00:22:06.670 --> 00:22:10.140
You give X by N amount of
data to each processor,

00:22:10.190 --> 00:22:13.960
and the processor can go and work
completely independently on that data.

00:22:14.040 --> 00:22:15.920
Let me give you a trivial example.

00:22:15.920 --> 00:22:17.740
Let's say you want to search.

00:22:17.740 --> 00:22:19.980
You have a very long list of names,
and you want to search

00:22:19.980 --> 00:22:23.140
through that list of names,
and you have, say, 10 processors.

00:22:23.140 --> 00:22:25.240
Take your list,
break it up into 10 independent chunks,

00:22:25.240 --> 00:22:27.320
and let them all go independently.

00:22:27.320 --> 00:22:30.160
There's very little
communication between them.

00:22:30.160 --> 00:22:31.860
This is a loosely coupled application.

00:22:31.860 --> 00:22:35.280
In a tightly coupled application,
you've essentially taken something

00:22:35.280 --> 00:22:37.990
that requires the work of everybody,
and they have to keep exchanging

00:22:37.990 --> 00:22:39.060
data among themselves.

00:22:39.060 --> 00:22:41.760
In such systems,
your network design is quite different.

00:22:41.760 --> 00:22:44.540
The other thing that you need
to understand to identify

00:22:44.540 --> 00:22:47.740
in your application pool is
where are your bottlenecks?

00:22:47.740 --> 00:22:51.380
Always profile the application,
because there's a lot of dependencies.

00:22:51.390 --> 00:22:53.620
Do you need a lot of
processor performance?

00:22:53.640 --> 00:22:55.380
Do you need a lot of memory?

00:22:55.390 --> 00:22:56.960
Do you need very high-speed
access to memory,

00:22:56.960 --> 00:23:01.780
because you move large chunks of
memory in and out of the processor?

00:23:01.980 --> 00:23:04.520
Do you need large caches?

00:23:04.520 --> 00:23:06.980
Are your applications
really cache-friendly?

00:23:06.980 --> 00:23:10.500
There are examples in scientific
computing that are very cache-unfriendly.

00:23:10.500 --> 00:23:12.280
You have things called sparse matrices.

00:23:12.300 --> 00:23:13.990
You have a matrix where there
are very few data elements

00:23:14.030 --> 00:23:16.920
scattered everywhere around,
so there is no locality.

00:23:16.930 --> 00:23:19.460
Having a large cache really doesn't
help you very much over there.

00:23:19.460 --> 00:23:22.820
Do you need a lot of I/O bandwidth?

00:23:22.820 --> 00:23:24.240
If you have a tightly
coupled application,

00:23:24.240 --> 00:23:27.340
you might want to send
and receive a lot of data.

00:23:27.360 --> 00:23:28.980
So this is a lot of
I/O that you're doing,

00:23:28.980 --> 00:23:30.140
really, over your network.

00:23:30.330 --> 00:23:33.530
Then your bus design should be
capable of handling this level

00:23:33.540 --> 00:23:35.730
of I/O that you're looking at.

00:23:35.790 --> 00:23:38.950
And finally, do your applications
work with a lot of data?

00:23:39.400 --> 00:23:41.260
So what is the disk
capacity that you need,

00:23:41.260 --> 00:23:43.600
and what is the disk
bandwidth that you need?

00:23:43.600 --> 00:23:46.530
And by the way,
real life applications don't

00:23:46.530 --> 00:23:48.480
have just one bottleneck.

00:23:48.480 --> 00:23:50.380
They have more than one bottleneck.

00:23:50.380 --> 00:23:52.270
So if you think you've
found the solution,

00:23:52.270 --> 00:23:55.200
you've profiled an application
and you've found that processor

00:23:55.280 --> 00:23:58.720
performance is your biggest problem,
you just want a faster processor.

00:23:58.720 --> 00:24:01.300
If you go and get a faster
processor and run your application,

00:24:01.390 --> 00:24:04.600
you might suddenly find
memory bandwidth is an issue.

00:24:04.600 --> 00:24:06.120
These things always couple.

00:24:06.340 --> 00:24:08.080
You eliminate one, another one pops up.

00:24:08.080 --> 00:24:11.360
It keeps happening round and round
until finally you get something

00:24:11.430 --> 00:24:13.440
you call satisfactory performance.

00:24:14.150 --> 00:24:17.650
The thing to emphasize over
here is it's always nice doing

00:24:17.850 --> 00:24:19.950
designs against a fixed budget.

00:24:20.050 --> 00:24:21.760
This seems a little counterintuitive.

00:24:21.840 --> 00:24:25.600
In the ideal world,
we'd like people to give us infinite

00:24:25.650 --> 00:24:30.800
amount of money and we do a design
against a completely unlimited budget.

00:24:31.560 --> 00:24:34.960
Problem there is you don't
necessarily optimize anything.

00:24:35.480 --> 00:24:38.370
You go find the vendor that's
got the most powerful machine.

00:24:38.570 --> 00:24:43.490
Essentially you're going to be buying
a Ferrari for driving to the grocery

00:24:43.810 --> 00:24:46.280
store because you can afford to do it.

00:24:46.280 --> 00:24:49.740
The question is when you start doing
working against a fixed budget it

00:24:49.740 --> 00:24:53.790
forces you to prioritize your needs
and then you might find that with the

00:24:53.790 --> 00:24:58.240
set of constraints that you're working
in there might be a radically different

00:24:58.570 --> 00:25:00.400
design that works very well for you.

00:25:00.400 --> 00:25:03.040
Your set of applications it
does very well on and that's

00:25:03.040 --> 00:25:03.040
what you're looking for.

00:25:06.120 --> 00:25:08.110
Analyze your application mix.

00:25:08.220 --> 00:25:10.650
The first thing you want to do is
figure out how much total computing

00:25:10.650 --> 00:25:16.620
capacity you need in order to solve
the problems that you are faced with.

00:25:16.620 --> 00:25:20.640
And never fall into the trap of
working against peak performance.

00:25:20.710 --> 00:25:22.580
Machines typically are
quoted by peak performance.

00:25:22.600 --> 00:25:25.060
You might say this machine has
20 teraflops peak performance or

00:25:25.090 --> 00:25:26.950
25 teraflops peak performance.

00:25:27.070 --> 00:25:30.410
Real applications rarely
get peak performance.

00:25:30.670 --> 00:25:34.610
There are some benchmarks out there,
LINPACK being one example of it.

00:25:34.970 --> 00:25:36.420
These are synthetic benchmarks.

00:25:36.480 --> 00:25:40.000
They try to exercise the system for
an application domain of interest.

00:25:40.000 --> 00:25:43.020
So when you look at LINPACK,
the application domain is

00:25:43.020 --> 00:25:44.380
scientific applications.

00:25:44.430 --> 00:25:45.920
If you're running
enterprise applications,

00:25:45.980 --> 00:25:48.760
LINPACK's not a good measure
of how the supercomputer is

00:25:48.760 --> 00:25:50.040
going to behave in your domain.

00:25:50.130 --> 00:25:52.250
There is no substitute, really,
for running your

00:25:52.260 --> 00:25:53.700
application on the system.

00:25:53.700 --> 00:25:56.810
So if you look at large DOD contracts,
they give you a set of

00:25:56.870 --> 00:25:58.740
applications and say,
run this application.

00:25:58.740 --> 00:26:01.100
Tell me how your system does.

00:26:01.250 --> 00:26:01.790
And that's it.

00:26:01.800 --> 00:26:03.100
That's the best way of doing it.

00:26:03.100 --> 00:26:04.590
Run actual applications on the system.

00:26:04.600 --> 00:26:09.330
. The next thing is,
first select your processor.

00:26:09.680 --> 00:26:12.460
Select a processor that can
achieve your desired performance

00:26:12.780 --> 00:26:15.850
in the smallest number of units.

00:26:15.850 --> 00:26:19.900
The more units you have,
the harder it becomes to program.

00:26:20.120 --> 00:26:22.880
The ideal world you want to have one
processor that solves everything.

00:26:22.880 --> 00:26:26.510
In the not so ideal world you want
to minimize the number of processors

00:26:26.510 --> 00:26:28.700
that are used to solve the problem.

00:26:28.700 --> 00:26:31.960
It leads to better scalability
and also better reliability.

00:26:33.600 --> 00:26:34.600
Then select your node platform.

00:26:34.600 --> 00:26:38.100
And in selecting your node platform,
there are a couple of choices.

00:26:38.100 --> 00:26:40.460
You can select what
are called thin nodes,

00:26:40.460 --> 00:26:44.710
where a node has a motherboard,
one processor, and memory.

00:26:44.720 --> 00:26:47.770
Or you can select increasing
levels of fatter nodes,

00:26:47.860 --> 00:26:51.020
two or more processors
accessing the same memory.

00:26:51.020 --> 00:26:53.610
The problem with this is
there's a trade-off there.

00:26:53.620 --> 00:26:56.970
The more processors you put on the bus,
the more chances you're going to run

00:26:56.970 --> 00:26:59.000
into contention within the node itself.

00:26:59.000 --> 00:27:01.690
All the processors put together may
not be able to get to memory anymore.

00:27:02.370 --> 00:27:04.270
So this kind of contention is
something you'll have to think about.

00:27:04.330 --> 00:27:09.330
On the other hand, in some cases,
if you have a poor network, for instance,

00:27:09.340 --> 00:27:12.460
it might actually be better to
put more processors in one node,

00:27:12.460 --> 00:27:15.550
simply because they can communicate using
memory instead of going over the network.

00:27:15.580 --> 00:27:18.840
So this is a trade-off that you have to
look at from an application perspective.

00:27:18.840 --> 00:27:21.830
More processors you put in,
potentially they'll all run slower simply

00:27:21.830 --> 00:27:23.720
because they've got memory contention.

00:27:23.720 --> 00:27:25.910
On the other hand,
they don't have to go over the network,

00:27:25.910 --> 00:27:26.940
so they may run faster.

00:27:26.940 --> 00:27:31.850
The other thing that you have to
be careful about when you pick

00:27:31.850 --> 00:27:35.290
your node is it's got to meet
all your other application needs.

00:27:35.300 --> 00:27:38.940
It's got to have enough IO bandwidth,
enough memory capacity and bandwidth,

00:27:38.940 --> 00:27:42.800
potential support maybe even for
local disks and things like that.

00:27:42.800 --> 00:27:46.510
And in the ideal world,
you don't want to pick the

00:27:46.990 --> 00:27:48.240
cheapest price for your node.

00:27:48.240 --> 00:27:50.140
Reliability is a pretty big concern.

00:27:50.140 --> 00:27:54.150
There are fairly large gaps in
reliability between what you can get

00:27:54.230 --> 00:27:59.890
at the very low end to what you can
get for even moderately higher prices.

00:28:01.340 --> 00:28:03.660
The next component is
picking your interconnect.

00:28:03.700 --> 00:28:07.270
This is your network that interconnects
all of your processors together.

00:28:07.330 --> 00:28:11.270
And this is probably the most
important design decision in building

00:28:11.280 --> 00:28:13.960
a cluster-design-based supercomputer.

00:28:13.990 --> 00:28:17.910
And your application domain should
dictate what kind of budget you

00:28:17.910 --> 00:28:20.500
allocate to your interconnect itself.

00:28:20.630 --> 00:28:22.460
So if you have a loosely
coupled application,

00:28:22.550 --> 00:28:24.260
you can work with gigabit Ethernet.

00:28:24.300 --> 00:28:25.380
Some cases,
you might even be able to work

00:28:25.380 --> 00:28:27.620
with 100 megabit Ethernet.

00:28:27.970 --> 00:28:30.690
There are applications out there
that typically the CS community

00:28:30.690 --> 00:28:33.330
calls embarrassingly parallel,
where you can even probably

00:28:33.350 --> 00:28:35.420
get away with smoke signals.

00:28:35.530 --> 00:28:39.750
You can get away with a lot if you have
a very loosely coupled application.

00:28:39.940 --> 00:28:41.250
But here is the kicker.

00:28:41.510 --> 00:28:43.260
If you have a very loosely
coupled application,

00:28:43.270 --> 00:28:45.260
why a supercomputer?

00:28:45.430 --> 00:28:49.130
If you have a large IT organization
or even within your enterprise,

00:28:49.170 --> 00:28:52.490
if you have a lot of desktop machines,
just cycle steel from them.

00:28:52.840 --> 00:28:55.460
Instead of running screen
savers on your application,

00:28:55.460 --> 00:28:57.500
SETI@Home is a great example of that.

00:28:57.500 --> 00:29:00.300
You don't need a supercomputer
to run SETI@Home.

00:29:00.300 --> 00:29:02.850
Although it is a high-performance
computing application,

00:29:02.850 --> 00:29:05.980
it can run in loosely coupled fashion
across desktops around the world.

00:29:05.980 --> 00:29:10.660
You can get a lot more bang for your buck
by not even putting up an interconnect.

00:29:10.660 --> 00:29:13.010
On the other hand,
for tightly coupled applications,

00:29:13.010 --> 00:29:16.660
you need a pretty fast interconnect,
very tightly coupled system.

00:29:16.660 --> 00:29:19.060
So if you have a tightly coupled
application and you take a fast

00:29:19.150 --> 00:29:23.260
processor and put a slow interconnect,
that train wreck is waiting

00:29:23.260 --> 00:29:25.060
to happen essentially.

00:29:25.060 --> 00:29:27.880
You cannot get the system to really
deliver any reasonable performance

00:29:27.880 --> 00:29:31.310
for you because everyone is waiting
for data to come in and they are

00:29:31.310 --> 00:29:33.500
spinning CPU cycles doing nothing.

00:29:33.690 --> 00:29:36.490
Well, not doing nothing,
they are generating heat.

00:29:36.830 --> 00:29:42.920
So how do you measure the
performance of an interconnect?

00:29:42.940 --> 00:29:46.440
So typical metrics there: Low latency.

00:29:46.440 --> 00:29:48.660
So low latency is if you
send a very small message,

00:29:48.660 --> 00:29:52.500
how much time does it take between the
point you sent the first bit to when

00:29:52.500 --> 00:29:54.920
the first bit arrived at the receiver.

00:29:54.920 --> 00:29:57.000
So for small messages this
is important and there are

00:29:57.000 --> 00:29:58.660
applications that are latency bound.

00:29:58.660 --> 00:30:00.670
They only send small messages
but they want them to get to the

00:30:00.670 --> 00:30:02.960
destination as quickly as possible.

00:30:02.960 --> 00:30:06.290
So low latency,
the order of magnitude you are

00:30:06.350 --> 00:30:09.320
looking at is very low microseconds.

00:30:09.320 --> 00:30:10.790
The other thing you want
is very high bandwidth.

00:30:10.800 --> 00:30:14.260
There are a lot of applications that
are bandwidth sensitive and you want

00:30:14.440 --> 00:30:20.220
several gigabits per second of bandwidth,
5, 10, even higher if you can get it.

00:30:20.220 --> 00:30:23.060
So in a good supercomputer
design you have something,

00:30:23.060 --> 00:30:27.310
a metric called bandwidth to flops ratio
and this bandwidth can be either memory

00:30:27.310 --> 00:30:29.620
bandwidth or interconnect bandwidth.

00:30:29.600 --> 00:30:32.650
So your bandwidth measured
in say bytes or bits,

00:30:32.650 --> 00:30:34.480
it doesn't really matter.

00:30:34.480 --> 00:30:36.960
So let me take bytes as a simple example.

00:30:36.960 --> 00:30:39.780
So bytes divided by flops.

00:30:39.780 --> 00:30:43.510
How many bytes per second can you
interconnect send divided by how

00:30:43.660 --> 00:30:45.910
many flops does your processor have?

00:30:48.070 --> 00:30:54.000
So, can anyone take a guess what a good
bandwidth to flop ratio should be?

00:30:54.020 --> 00:30:56.500
Four.

00:30:56.510 --> 00:30:58.680
We have a takeoff of four.

00:30:58.690 --> 00:31:00.220
Any other?

00:31:01.200 --> 00:31:09.400
[Transcript missing]

00:31:12.700 --> 00:31:15.640
If you have all the money in the world
and you want to do a good design.

00:31:15.640 --> 00:31:17.640
One.

00:31:18.320 --> 00:31:19.640
Ten.

00:31:19.640 --> 00:31:21.550
I'm going to claim eight.

00:31:22.060 --> 00:31:24.760
Just picked a number out of my hat.

00:31:24.760 --> 00:31:26.240
Why eight?

00:31:26.240 --> 00:31:28.540
So if you look at
scientific applications,

00:31:28.550 --> 00:31:31.000
they use double precision arithmetic.

00:31:31.120 --> 00:31:31.580
Okay?

00:31:31.700 --> 00:31:33.910
So the end of your
mathematical computation,

00:31:33.920 --> 00:31:36.740
you're going to spit out
one double precision number.

00:31:36.740 --> 00:31:38.570
That's eight bytes.

00:31:38.990 --> 00:31:43.100
Okay, each flop in there is spitting
out 8 bytes worth of data.

00:31:43.180 --> 00:31:45.810
You need to get that 8
bytes out of the system.

00:31:46.450 --> 00:31:48.950
Your memory bandwidth also
follows the same curve.

00:31:49.000 --> 00:31:50.080
It depends on your architecture.

00:31:50.080 --> 00:31:53.310
If your architecture can
load two doubles in one shot,

00:31:53.310 --> 00:31:55.370
then you want actually 16.

00:31:55.540 --> 00:31:58.300
If you can consume two
doubles in one shot,

00:31:58.300 --> 00:32:00.090
you again want 16.

00:32:02.750 --> 00:32:04.500
"One variable, correct, exactly.

00:32:04.500 --> 00:32:08.310
So essentially you need to be able
to get the output of your flop out of

00:32:08.360 --> 00:32:12.060
the system and that's your B2F ratio.

00:32:12.060 --> 00:32:15.530
So given that,
any guesses what really good

00:32:15.530 --> 00:32:17.860
interconnects do today?"

00:32:22.350 --> 00:32:27.300
memory bandwidth of 0.5
is considered very good.

00:32:27.300 --> 00:32:34.330
As an example the value
for a PowerMac G5,

00:32:34.330 --> 00:32:37.430
you've got 6.4 gigabytes per
second of memory bandwidth and

00:32:37.430 --> 00:32:39.300
you've got 16 gigaflops of peak.

00:32:39.300 --> 00:32:41.600
So it's little less than 0.5.

00:32:41.600 --> 00:32:44.980
In the case of a Xeon,
a dual Xeon you would get

00:32:45.120 --> 00:32:51.310
something like 12 gigaflops for 3
gigabytes per second of bandwidth.

00:32:52.390 --> 00:32:54.300
So that's a quarter.

00:32:54.300 --> 00:32:56.120
A really good design.

00:32:56.410 --> 00:32:59.590
You would have much higher
memory bandwidth essentially

00:32:59.590 --> 00:33:00.290
to feed your system.

00:33:02.190 --> 00:33:04.320
The next thing that you
have to worry about is fine,

00:33:04.390 --> 00:33:09.010
I've gone and selected my
interconnection networks.

00:33:09.410 --> 00:33:13.160
a specification,
and I want to hook up all

00:33:13.160 --> 00:33:17.300
my switches that form the
backplane of my communication.

00:33:17.300 --> 00:33:18.250
Now how do I hook this up?

00:33:18.290 --> 00:33:21.260
What is the topology that
I have to hook these things up?

00:33:21.260 --> 00:33:24.290
So typically,
you need to set up a backplane that can

00:33:24.300 --> 00:33:26.480
handle your peak bandwidth requirement.

00:33:26.650 --> 00:33:29.560
And in a good design,
you measure the capacity of

00:33:29.720 --> 00:33:33.210
your backplane by a quantity
called bi-section bandwidth.

00:33:33.210 --> 00:33:36.300
So bi-section bandwidth,
if you want to look at it intuitively,

00:33:36.350 --> 00:33:37.470
take your entire cluster.

00:33:37.470 --> 00:33:40.590
If half the nodes in the cluster
want to talk to the other half of the

00:33:40.590 --> 00:33:43.340
nodes in the cluster simultaneously,
you can do that.

00:33:43.870 --> 00:33:47.000
How much bandwidth do you
have in the backplane?

00:33:47.000 --> 00:33:49.100
Bisection bandwidth can
be expressed as a ratio,

00:33:49.100 --> 00:33:51.640
it can be expressed as a full number.

00:33:51.640 --> 00:33:53.970
In an ideal system,
if half your nodes talk to the other

00:33:54.080 --> 00:33:56.280
half of the nodes simultaneously,
you should have no

00:33:56.280 --> 00:33:57.320
blocking in the network.

00:33:57.320 --> 00:33:58.300
They should be able to do so.

00:33:58.480 --> 00:33:59.900
That’s called full bisection bandwidth.

00:33:59.900 --> 00:34:02.680
There are designs that you can
do with half bisection bandwidth,

00:34:02.890 --> 00:34:05.090
hoping for some statistical
multiplexing that you say,

00:34:05.090 --> 00:34:07.080
"Okay, well,
my scientific application is not

00:34:07.080 --> 00:34:10.740
going to communicate perfectly
synchronously across the entire system.

00:34:10.740 --> 00:34:13.680
Maybe I’ll get lucky." Um...

00:34:14.520 --> 00:34:17.790
So if you look at modern interconnects,
things like Infiniban,

00:34:17.900 --> 00:34:19.860
they're connected over something
called a factory topology.

00:34:19.900 --> 00:34:22.630
In factory topology,
you actually have a mesh and they

00:34:22.630 --> 00:34:23.900
provide excellent bisection bandwidth.

00:34:23.900 --> 00:34:27.620
You can easily build designs with
full bisection where if half the

00:34:27.620 --> 00:34:29.900
nodes talk to the other half,
you have no blocking.

00:34:29.900 --> 00:34:32.070
In contrast,
if you start using things like fast

00:34:32.070 --> 00:34:35.210
Ethernet or Gigabit Ethernet networks,
all you can do,

00:34:35.440 --> 00:34:37.260
connect between two switches,
is eight links.

00:34:37.400 --> 00:34:39.940
There's something called channel bonding
that you use over there and there's

00:34:39.950 --> 00:34:41.400
a specification called Ether Channel.

00:34:41.400 --> 00:34:46.400
Ether Channel lets you bond eight links
together and make it look like one link.

00:34:46.700 --> 00:34:48.400
Ethernet uses a tree topology.

00:34:48.400 --> 00:34:51.400
If you do layer two switching,
you've got a complete tree topology.

00:34:51.400 --> 00:34:55.400
And all you can connect between one
switch and its parent is one thick link.

00:34:55.400 --> 00:34:57.070
If you've got Gigabit and
you put eight 1G links,

00:34:57.140 --> 00:34:59.770
you've got 8 Gigabit per second
going from one switch to the other.

00:34:59.920 --> 00:35:01.820
That's your bisection.

00:35:02.000 --> 00:35:07.280
If you've got 10Gbps links,
you've got 80Gbps.

00:35:07.280 --> 00:35:08.210
That's not very much.

00:35:08.300 --> 00:35:12.500
As an example, System 10,
our bisection bandwidth is

00:35:12.500 --> 00:35:15.270
something like 11.6Tbps.

00:35:15.420 --> 00:35:18.600
So these numbers reach many many
terabits per second very quickly.

00:35:18.620 --> 00:35:22.500
The overall switching capacity
is like 46 terabits per second.

00:35:23.000 --> 00:35:25.980
So you can do Gigabit Ethernet as
long as you have loosely coupled and

00:35:26.010 --> 00:35:29.120
you're not very bandwidth hungry.

00:35:29.150 --> 00:35:33.580
Next thing that you need to look at
is your storage subsystem design.

00:35:34.010 --> 00:35:35.540
Typically,
if you look at cluster designs,

00:35:35.540 --> 00:35:39.820
they use a large shared store
simply because it's easy

00:35:39.890 --> 00:35:43.350
to drop software into it,
common software that's used

00:35:43.350 --> 00:35:44.480
across all the systems.

00:35:44.490 --> 00:35:46.880
And also,
it's easy to start up applications.

00:35:46.880 --> 00:35:48.340
There are some exceptions to it.

00:35:48.340 --> 00:35:51.020
The Earth simulator, for instance,
doesn't use a shared

00:35:51.020 --> 00:35:52.680
store subsystem at all.

00:35:52.680 --> 00:35:54.490
In fact, it uses a local store.

00:35:54.500 --> 00:35:56.300
It's fairly interesting.

00:35:56.410 --> 00:36:00.380
The design criteria over here is,
when you have to come up

00:36:00.450 --> 00:36:03.260
with a storage subsystem,
you probably do it over a set of nodes.

00:36:03.270 --> 00:36:06.510
And these set of nodes comprise
some kind of storage area network.

00:36:06.610 --> 00:36:09.700
And the storage nodes that you
have in the storage area network,

00:36:09.700 --> 00:36:11.260
they run a parallel
file system across them,

00:36:11.260 --> 00:36:13.040
and this is the typical design.

00:36:13.170 --> 00:36:15.840
So what kind of things do
you have to look out for?

00:36:15.870 --> 00:36:18.180
First, you need to worry about the
capacity of your storage system.

00:36:18.180 --> 00:36:21.280
Then you need to worry about how fast
you can access your storage system.

00:36:21.450 --> 00:36:23.990
So for capacity of the storage system,
the first thing to do is

00:36:24.020 --> 00:36:25.540
look at your application.

00:36:25.560 --> 00:36:27.790
What are the needs of
your application domain?

00:36:27.880 --> 00:36:30.340
If you don't have any numbers
from your application domain,

00:36:30.340 --> 00:36:33.960
that is, you're having to blindly
design this portion.

00:36:33.960 --> 00:36:35.320
This is a simple rule of thumb.

00:36:35.320 --> 00:36:37.220
Whatever is the total memory
footprint of your cluster,

00:36:37.220 --> 00:36:38.480
multiply that by 20.

00:36:38.480 --> 00:36:41.760
That will be a good starter
system for a shared store.

00:36:41.760 --> 00:36:45.160
This is a fairly normal
calculation there.

00:36:45.200 --> 00:36:47.600
If you have a lot of
disk access in there,

00:36:47.600 --> 00:36:49.640
then you need to worry about
a bunch of other parameters.

00:36:49.640 --> 00:36:52.130
You need to worry about how
does your storage area network

00:36:52.140 --> 00:36:54.120
hook into your cluster?

00:36:54.120 --> 00:36:56.880
So let's say you built your
cluster with InfiniBand,

00:36:56.880 --> 00:36:59.110
and you've got a lot of
bandwidth inside the cluster,

00:36:59.120 --> 00:37:01.780
and then you put your storage area
network over gigabit Ethernet.

00:37:01.970 --> 00:37:03.740
That would not be a
good design necessarily,

00:37:03.770 --> 00:37:06.400
particularly if you're trying
to get a lot of bandwidth.

00:37:06.470 --> 00:37:08.080
In your ideal world,
you want to hook your storage area

00:37:08.080 --> 00:37:12.340
network into your cluster at as high
a bandwidth as you can possibly get.

00:37:12.500 --> 00:37:15.720
Now, network bandwidth alone
is not the criterion here.

00:37:15.730 --> 00:37:20.580
At the end of the day, these are nowadays
electromechanical devices,

00:37:20.580 --> 00:37:21.160
right?

00:37:21.180 --> 00:37:25.100
You've got a motor spinning that's
rotating a bunch of platters.

00:37:25.310 --> 00:37:29.200
You can only access data in
and out of the disk so fast.

00:37:29.200 --> 00:37:31.700
So the more spindles you have in there,
the better access rate

00:37:31.780 --> 00:37:32.760
you get to disk itself.

00:37:32.760 --> 00:37:34.460
Your network might not be the bottleneck.

00:37:34.460 --> 00:37:37.210
You might have a great network,
but have very few spindles in there.

00:37:37.310 --> 00:37:39.120
You've got only 10 hard disks,
for instance.

00:37:39.120 --> 00:37:41.380
There's only so much you can get out
of the disk at the end of the day.

00:37:41.380 --> 00:37:44.500
You want to increase your capacity,
you'll have to go up for more

00:37:44.500 --> 00:37:46.540
and more spindles in there.

00:37:46.540 --> 00:37:49.240
And sometimes you have
to design for bandwidth,

00:37:49.240 --> 00:37:51.980
and capacity just comes as a
result of that calculation.

00:37:51.980 --> 00:37:55.480
You don't have control over capacity.

00:37:55.480 --> 00:37:58.560
And this is typically how you
end up with interconnecting

00:37:58.560 --> 00:38:00.540
your SAN into your cluster.

00:38:00.540 --> 00:38:03.340
The thing to always remember
is reliability is a key issue.

00:38:03.370 --> 00:38:05.460
If you've got a set of
storage nodes operating,

00:38:05.460 --> 00:38:07.450
each of the storage nodes should
at least be running RAID to

00:38:07.500 --> 00:38:09.160
recover from disk failures.

00:38:09.160 --> 00:38:12.620
In environments where you have a shared
store and there is a lot of disk access,

00:38:12.640 --> 00:38:15.060
failure is a fairly common thing.

00:38:15.060 --> 00:38:16.860
So you need to have RAID in there.

00:38:16.860 --> 00:38:20.020
And whatever parallel file system you're
running across should also be capable

00:38:20.020 --> 00:38:21.550
of recovering from entire node failures.

00:38:22.110 --> 00:38:25.010
RAID will recover as long
as you have disk failures.

00:38:25.040 --> 00:38:27.970
If the whole storage node crashes,
you need a parallel file

00:38:27.980 --> 00:38:30.640
system that can somehow recover
gracefully in that condition,

00:38:30.660 --> 00:38:33.480
instead of shutting off
the entire namespace.

00:38:35.670 --> 00:38:38.600
Next thing to worry about is software.

00:38:38.600 --> 00:38:40.270
So first thing there is compilers.

00:38:40.370 --> 00:38:42.340
C, C++, Fortran.

00:38:42.570 --> 00:38:45.080
I come from the scientific
computing community side of things

00:38:45.080 --> 00:38:46.640
so these are the typical languages.

00:38:46.750 --> 00:38:50.620
Java is not very common in this domain.

00:38:50.720 --> 00:38:53.560
And wherever possible in
terms of compiler choices,

00:38:53.650 --> 00:38:55.360
pick the vendor compiler.

00:38:55.360 --> 00:39:00.150
The vendor would be somebody who actually
knows the processor family very well,

00:39:00.150 --> 00:39:02.860
maybe even the manufacturer
of the processor.

00:39:03.500 --> 00:39:05.680
Vendor designs typically for the
compiler make use of a lot of

00:39:05.680 --> 00:39:09.270
the hardware features and they
do much better code optimization.

00:39:09.270 --> 00:39:12.040
And this helps a lot in
terms of pulling performance.

00:39:12.040 --> 00:39:14.170
The money is usually well spent.

00:39:14.230 --> 00:39:15.260
Think about it this way.

00:39:15.330 --> 00:39:22.180
If you have two compilers and one
can generate code that's 25% faster,

00:39:22.180 --> 00:39:22.180
on a large scale cluster design,
you're saving a couple

00:39:22.180 --> 00:39:22.180
of million right there.

00:39:22.700 --> 00:39:25.940
So this kind of calculation you'll
have to look at and it might appear

00:39:25.940 --> 00:39:35.170
that the vendor compiler is expensive,
few tens of thousands of dollars,

00:39:35.170 --> 00:39:35.170
but generally it's worth the
money as long as it can show

00:39:35.170 --> 00:39:35.170
tangible benefits with respect
to any other compiler you find.

00:39:36.380 --> 00:39:40.360
The most important communication library
that you are going to look at is MPI,

00:39:40.650 --> 00:39:42.240
Message Passing Interface.

00:39:42.240 --> 00:39:45.450
This was standardized in the
early 90s and this is the most

00:39:45.500 --> 00:39:49.300
common messaging layer that is
used across supercomputer designs.

00:39:49.310 --> 00:39:51.370
In fact,
the design of MPI was probably one of

00:39:51.370 --> 00:39:55.300
the biggest reasons why we are talking
about supercomputing here today.

00:39:55.300 --> 00:39:59.300
Prior to the arrival of MPI,
every machine was programmed differently.

00:39:59.310 --> 00:40:02.240
Each machine had its own message passing
layer and codes were not portable.

00:40:02.240 --> 00:40:05.360
You couldn't write code on one
supercomputer and expect it to

00:40:05.360 --> 00:40:07.910
run anywhere else unless the
anywhere else happened to be

00:40:07.910 --> 00:40:09.300
the same manufacturer's machine.

00:40:09.300 --> 00:40:11.300
Codes were not portable.

00:40:11.300 --> 00:40:13.550
You had to write to a machine and there
were very few people who could do that

00:40:13.620 --> 00:40:16.300
simply because there were very few
people who had access to these machines.

00:40:16.300 --> 00:40:20.250
So MPI was sort of the first step along
the democratization of supercomputing.

00:40:20.330 --> 00:40:21.300
A lot more people got access to it.

00:40:21.300 --> 00:40:25.880
And there are several versions available:
MPitch, LAM, MPI Pro,

00:40:26.000 --> 00:40:29.300
and there is a new one
coming out called OpenMPI.

00:40:29.300 --> 00:40:32.860
Sometimes you might have your
interconnect vendor give you a

00:40:32.860 --> 00:40:38.300
specific version of MPI and this might
have hardware acceleration features.

00:40:38.300 --> 00:40:41.250
They would have drivers inside
MPI that very efficiently use

00:40:41.250 --> 00:40:44.450
the hardware capabilities of your
interconnect and might be able

00:40:44.450 --> 00:40:46.300
to do a lot better performance.

00:40:48.390 --> 00:40:49.780
The next thing is something
called a queuing system.

00:40:49.780 --> 00:40:54.160
So unlike running applications on typical
desktops where you just type the name

00:40:54.160 --> 00:40:57.250
of the application and it fires off,
supercomputers are governed

00:40:57.250 --> 00:40:58.700
by batch queuing systems.

00:40:58.730 --> 00:41:02.060
So you have your application,
you submit it to a batch queuing system.

00:41:02.060 --> 00:41:05.260
The batch queuing system then figures
out how many resources you need,

00:41:05.260 --> 00:41:07.800
what kind of resources you need,
are the resources available,

00:41:07.800 --> 00:41:11.260
and then it generates a priority
on top of it and then decides at

00:41:11.260 --> 00:41:12.520
some point to run your application.

00:41:12.520 --> 00:41:16.590
So applications go into the batch queue,
they execute for some time, come out.

00:41:17.180 --> 00:41:20.070
Typically, supercomputing applications
are sort of autistic.

00:41:20.120 --> 00:41:21.920
They don't talk to the outside world.

00:41:21.920 --> 00:41:23.080
They're not interactive applications.

00:41:23.100 --> 00:41:25.790
They're fired off in batch
mode and results come back.

00:41:25.900 --> 00:41:27.360
That's basically how it operates.

00:41:27.360 --> 00:41:31.380
Essentially,
the scheduling system over here

00:41:31.380 --> 00:41:34.300
is governed by the administrative
policies that govern the use of

00:41:34.300 --> 00:41:35.900
the supercomputer that you use.

00:41:35.900 --> 00:41:39.560
And those set of policies are
applied on top of the jobs and

00:41:39.630 --> 00:41:42.050
that tells you exactly when your
application is going to run.

00:41:44.600 --> 00:41:48.530
Apart from the queuing system you
might also have management software,

00:41:48.540 --> 00:41:52.440
where the job of the management software
is to analyze the health of the system,

00:41:52.440 --> 00:41:55.300
generate logs,
in some cases do notifications,

00:41:55.350 --> 00:41:58.140
call your pager or call your cell phone.

00:41:58.140 --> 00:42:01.140
There's a lot of notification capability
that's built into several pieces

00:42:01.140 --> 00:42:03.580
of management software available.

00:42:03.630 --> 00:42:05.600
The other thing that you might want
to consider is if you're running

00:42:05.620 --> 00:42:09.210
your supercomputer in a data center
environment at an IT infrastructure,

00:42:09.210 --> 00:42:12.330
your IT infrastructure might
already be running some form of

00:42:12.440 --> 00:42:15.680
data center wide management system,
which might have its own notification.

00:42:15.680 --> 00:42:17.800
You might want to hook these
two things together so you

00:42:17.810 --> 00:42:19.240
have a common access platform.

00:42:19.240 --> 00:42:21.030
You don't want to run multiple
management systems across

00:42:21.030 --> 00:42:23.310
domains that you don't need to.

00:42:25.970 --> 00:42:28.710
The last choice that you have
here in terms of software and a

00:42:28.710 --> 00:42:32.240
major one at that is what kind of
operating system do you plan to run?

00:42:32.240 --> 00:42:34.020
And this choice can make
quite a bit of a difference.

00:42:34.190 --> 00:42:37.670
OS performance is critical
in running HPC applications.

00:42:37.900 --> 00:42:40.930
So in the ideal world,
an HPC operating system should

00:42:40.930 --> 00:42:42.900
be so thin that it disappears.

00:42:42.900 --> 00:42:44.850
Your application directly
gets to the hardware.

00:42:44.850 --> 00:42:47.900
There is no cost to running
an operating system.

00:42:47.900 --> 00:42:53.900
All it provides is a very simple set of
primitives that the application can use.

00:42:53.900 --> 00:42:54.900
Very small footprint.

00:42:55.000 --> 00:42:55.870
You really want OS to disappear.

00:42:55.900 --> 00:42:57.870
And there are some
radical examples of this.

00:42:57.920 --> 00:43:00.060
The operating system
that runs on Blue Gene,

00:43:00.060 --> 00:43:05.960
which is a machine from IBM that's going
to be delivered to Lawrence Livermore,

00:43:05.960 --> 00:43:06.880
I think, later this year.

00:43:06.900 --> 00:43:08.900
Very large-scale system.

00:43:08.900 --> 00:43:10.810
It runs a very, very thin microkernel.

00:43:10.810 --> 00:43:11.880
And that's all it runs.

00:43:11.900 --> 00:43:12.890
It's a single-tasking machine.

00:43:12.890 --> 00:43:13.890
You can't do fork.

00:43:13.990 --> 00:43:15.890
You can't run two
applications on the node.

00:43:15.900 --> 00:43:18.900
It's a radical example there.

00:43:18.900 --> 00:43:21.900
And then there is Cougar and Catamount
that run on the Cray-Redstorm series.

00:43:21.900 --> 00:43:23.900
Again, very, very thin microkernels.

00:43:23.930 --> 00:43:24.690
Single-tasking.

00:43:24.690 --> 00:43:26.900
In many cases,
they have no virtual memory either.

00:43:26.930 --> 00:43:29.310
The physical memory is all you get.

00:43:31.080 --> 00:43:32.000
So why?

00:43:32.000 --> 00:43:35.000
Why do we want to create a
very thin operating system?

00:43:35.000 --> 00:43:38.280
If we look at a cluster design,
each node of the cluster runs an

00:43:38.320 --> 00:43:40.750
independent copy of the operating system.

00:43:40.810 --> 00:43:43.110
So in our case, for instance,
we have a thousand nodes.

00:43:43.110 --> 00:43:49.460
A thousand node system,
you've got a thousand copies

00:43:49.460 --> 00:43:49.490
of the operating system.

00:43:49.490 --> 00:43:49.490
More importantly, you've got a thousand
copies of the scheduler.

00:43:50.000 --> 00:43:51.660
And this is a problem.

00:43:51.660 --> 00:43:54.660
The reason this is a problem is
let's take a look at something

00:43:54.660 --> 00:43:56.300
called a block synchronous code.

00:43:56.300 --> 00:43:58.660
So in block synchronous
scientific applications,

00:43:58.660 --> 00:44:01.400
they execute some piece of a model.

00:44:01.450 --> 00:44:01.640
Okay?

00:44:01.640 --> 00:44:03.200
And they execute one time step of it.

00:44:03.200 --> 00:44:05.340
At the end of the time step,
everybody stops.

00:44:05.350 --> 00:44:07.740
They all make sure they've gotten
to the end of the time step,

00:44:07.740 --> 00:44:09.350
then they begin the next time step.

00:44:09.820 --> 00:44:12.980
This particular operation that
enables all of the nodes to stop

00:44:13.060 --> 00:44:15.160
and synchronize is called a barrier.

00:44:15.160 --> 00:44:18.350
And typically a barrier,
if everything is working well,

00:44:18.410 --> 00:44:20.350
executes in a few microseconds.

00:44:21.060 --> 00:44:26.530
However, if one of the schedulers,
one of the thousand different schedulers

00:44:26.600 --> 00:44:30.020
running on the system glitches,
glitches meaning it runs some

00:44:30.020 --> 00:44:32.900
other process instead of yours,
what happens?

00:44:33.800 --> 00:44:36.290
The barrier doesn't run in microseconds.

00:44:36.320 --> 00:44:38.360
It runs for the entire
scheduler interval,

00:44:38.360 --> 00:44:40.640
which can be as large as 10 milliseconds.

00:44:41.340 --> 00:44:44.780
So now if your time step is very small,
your time step itself executes

00:44:44.780 --> 00:44:47.290
only in like 500 microseconds,
and then the barrier

00:44:47.290 --> 00:44:49.460
takes 10 milliseconds,
it doesn't matter how

00:44:49.460 --> 00:44:51.970
big a machine you built,
the rate of processing is going to

00:44:51.970 --> 00:44:54.630
be the inverse of your scheduler,
and this happens.

00:44:54.810 --> 00:44:57.140
The interesting thing is,
this is not just a theoretical curiosity.

00:44:57.210 --> 00:44:58.020
This happens a lot.

00:44:58.020 --> 00:45:00.350
This takes out 20% out of your system.

00:45:00.550 --> 00:45:02.960
So what do you do to solve this problem?

00:45:02.990 --> 00:45:06.600
You synchronize all the schedulers,
such that they all do context

00:45:06.600 --> 00:45:09.000
switching at exactly the same time.

00:45:09.030 --> 00:45:12.050
This is a solution
called gang scheduling.

00:45:12.110 --> 00:45:15.520
The problem is you need to have
a clock that runs very accurately

00:45:15.520 --> 00:45:17.960
across your entire system,
and the cost of that

00:45:17.960 --> 00:45:19.330
clock can be very high.

00:45:19.470 --> 00:45:21.760
You might have to get a GPS clock,
put it on each node,

00:45:21.940 --> 00:45:24.260
and query it to be able to
make a scheduling decision.

00:45:24.260 --> 00:45:27.360
In a cluster design,
doing gang scheduling is not so easy.

00:45:27.380 --> 00:45:29.460
The other trivial solution to this,
which is what was picked up

00:45:29.540 --> 00:45:34.290
by the two extreme examples,
is don't do multitasking.

00:45:35.270 --> 00:45:38.540
Okay, so you never run into
gang scheduling problems.

00:45:38.580 --> 00:45:41.810
That's an interesting solution except
for the fact that once in a while

00:45:41.810 --> 00:45:43.390
you do want to do multitasking.

00:45:43.530 --> 00:45:48.950
When your monitoring system management
demon wants to collect some resource

00:45:48.950 --> 00:45:51.300
information and report it back.

00:45:52.140 --> 00:45:55.050
So in this case we've actually worked
on something which I won't talk about.

00:45:55.080 --> 00:45:57.350
It turns out microkernel designs,
you can take a general purpose

00:45:57.350 --> 00:46:00.590
microkernel design and be able to
whittle it down such that it only runs

00:46:00.590 --> 00:46:04.080
a small set of processes and you can
control exactly when context switching

00:46:04.240 --> 00:46:06.050
occurs across the entire system.

00:46:06.140 --> 00:46:11.100
There's a fairly elegant way of
solving this gang scheduling problem.

00:46:12.810 --> 00:46:14.920
The other question,
problem that you have with cluster

00:46:14.920 --> 00:46:17.360
designs is basically reliability.

00:46:17.390 --> 00:46:18.880
Take a large scale cluster.

00:46:18.880 --> 00:46:22.500
1,000 nodes, 1,100 nodes, 2,000 nodes.

00:46:22.500 --> 00:46:24.460
You've got a fundamental problem there.

00:46:24.460 --> 00:46:27.060
The number of components is very large.

00:46:27.060 --> 00:46:30.280
So let's say each of your nodes,
you go and pick the best node

00:46:30.280 --> 00:46:31.820
design that you can find.

00:46:31.820 --> 00:46:32.830
Something known for reliability.

00:46:32.840 --> 00:46:33.820
Okay?

00:46:33.840 --> 00:46:37.300
So you're going to pay
a premium for this.

00:46:37.300 --> 00:46:39.020
But the premium might be worth it.

00:46:39.020 --> 00:46:40.600
So let's take the extreme case of it.

00:46:40.600 --> 00:46:41.880
Six sigma reliability.

00:46:41.880 --> 00:46:44.780
Three sigma after that comes at a very,
very high price point because the

00:46:44.920 --> 00:46:46.600
quality control is very expensive.

00:46:46.600 --> 00:46:50.280
You'll have to watch every single
component that goes into the design.

00:46:50.280 --> 00:46:51.230
So six sigma reliability.

00:46:51.300 --> 00:46:54.980
So you buy a node that
crashes once a year.

00:46:54.980 --> 00:46:56.380
That you would call a pretty good node.

00:46:56.430 --> 00:46:59.640
It crashes once a year and
comes back in like a minute.

00:47:00.510 --> 00:47:05.560
Once a year crash, 1100 node system,
we have 1100 crashes a year.

00:47:05.780 --> 00:47:07.630
That's about three a day.

00:47:09.400 --> 00:47:12.640
These are the kinds of statistics
that you get on large scale clusters.

00:47:12.640 --> 00:47:14.900
Two to three times a
day you lose something.

00:47:14.920 --> 00:47:19.390
Now, in a typical desktop environment,
take IT infrastructure inside

00:47:19.390 --> 00:47:23.520
a university or an industry,
you might have a few thousand machines.

00:47:23.600 --> 00:47:25.680
If one of them crashes,
it doesn't really matter

00:47:25.680 --> 00:47:28.460
because all you lose is whatever
was running on that machine.

00:47:28.470 --> 00:47:29.960
Very graceful degradation.

00:47:29.960 --> 00:47:33.910
In tightly coupled parallel applications,
you lose one node,

00:47:33.910 --> 00:47:33.910
you lose the whole thing.

00:47:34.320 --> 00:47:35.170
And this is a big problem.

00:47:35.210 --> 00:47:37.790
So we've been working on a
solution called DejaVu that

00:47:37.790 --> 00:47:40.190
builds transparent fault tolerance
directly into application,

00:47:40.200 --> 00:47:42.740
analyzes the application,
instruments it directly,

00:47:42.840 --> 00:47:45.080
and any failure from the system,
the application doesn't see it.

00:47:45.280 --> 00:47:46.960
It recovers transparently from it.

00:47:46.960 --> 00:47:49.150
So this is funded by the
National Science Foundation and

00:47:49.210 --> 00:47:50.300
we got a patent on this.

00:47:50.400 --> 00:47:51.520
And it's being commercialized this year.

00:47:51.610 --> 00:47:54.150
Hopefully later in the year
it will be available through a

00:47:54.150 --> 00:47:55.830
company called California Digital.

00:47:58.500 --> 00:48:00.200
This question is deployment.

00:48:00.200 --> 00:48:02.490
So now you've done your
analysis on your cluster,

00:48:02.580 --> 00:48:05.320
you've figured out exactly what
your design should look like,

00:48:05.320 --> 00:48:06.620
now you want to build it.

00:48:07.010 --> 00:48:09.080
The first thing you've got
to worry about is your team.

00:48:09.080 --> 00:48:11.580
That is your most
important asset over there.

00:48:11.870 --> 00:48:17.580
You need a fairly large,
fairly diverse group

00:48:17.580 --> 00:48:19.960
of skills in this team.

00:48:20.110 --> 00:48:22.140
The team generally is kept fairly small.

00:48:22.140 --> 00:48:24.700
But if you're trying to do
a large scale deployment,

00:48:24.700 --> 00:48:26.830
you've got a whole slew of
issues that you've got to

00:48:26.830 --> 00:48:28.790
worry about in the deployment,
and I'll talk about them.

00:48:28.840 --> 00:48:31.910
And you need people with very specific
skill sets to be able to address

00:48:31.910 --> 00:48:33.660
each of those deployment issues.

00:48:35.060 --> 00:48:37.180
One or two people really
won't be able to do it,

00:48:37.180 --> 00:48:40.100
unless they're a true know-it-all.

00:48:40.100 --> 00:48:42.100
It doesn't happen.

00:48:42.100 --> 00:48:44.940
The kind of knowledge that you need,
very specific knowledge,

00:48:44.940 --> 00:48:48.820
you're not going to find people that
can expand the entire width over there.

00:48:48.840 --> 00:48:50.840
Your project plan can run
to hundreds of elements.

00:48:50.860 --> 00:48:53.540
In our case,
it was something like nine double-sided

00:48:53.540 --> 00:48:55.460
pages at like eight point font.

00:48:55.460 --> 00:48:56.650
And this was the project plan.

00:48:56.660 --> 00:48:58.180
These things are all gated.

00:48:58.200 --> 00:49:00.700
One of them slips and you have trouble.

00:49:00.700 --> 00:49:03.780
And then you've probably got 10 or
20 elements running simultaneously

00:49:03.780 --> 00:49:04.880
at any given point in time.

00:49:04.910 --> 00:49:06.600
And you've got to be able to
manage this whole thing to make

00:49:06.600 --> 00:49:08.200
sure it delivers correctly.

00:49:08.380 --> 00:49:11.860
So you start off with a core team,
kept fairly small,

00:49:11.900 --> 00:49:16.010
and it's responsible for the overall
logistical planning of how this

00:49:16.010 --> 00:49:18.610
deployment is going to go about.

00:49:19.140 --> 00:49:21.120
Your next team is
physical infrastructure.

00:49:21.120 --> 00:49:22.680
It consists of two components.

00:49:22.850 --> 00:49:23.490
Your IT team.

00:49:23.830 --> 00:49:27.800
Your IT team is very critical because
they generally have the experience not

00:49:27.800 --> 00:49:30.600
only for deploying but also managing
large-scale production systems.

00:49:30.600 --> 00:49:33.000
So having a good IT team
is absolutely critical.

00:49:33.000 --> 00:49:35.470
And the second thing you're
going to need is physical plant

00:49:35.470 --> 00:49:38.980
personnel for site reconstruction.

00:49:39.000 --> 00:49:40.980
You're going to need
renovations into your center,

00:49:41.030 --> 00:49:43.990
particularly if you're planning to
do a very large-scale deployment.

00:49:44.000 --> 00:49:46.000
You need finance people.

00:49:46.000 --> 00:49:48.000
There are lots of components you're
going to be ordering in here.

00:49:48.000 --> 00:49:50.000
You need to keep track of everything.

00:49:50.150 --> 00:49:53.990
You'll need PR people if you're doing
something that's interesting and

00:49:54.140 --> 00:49:56.000
worthy of everyone else to take note.

00:49:56.000 --> 00:49:58.000
And vendor liaisons.

00:49:58.000 --> 00:50:00.000
You'll have to have dedicated people
that can liaison with each vendor.

00:50:00.000 --> 00:50:03.960
You're typically going to have
anywhere between 8 to 20 vendors,

00:50:04.010 --> 00:50:07.000
independent vendors,
all of whom have to match your schedules.

00:50:07.000 --> 00:50:10.000
Delivery times have to be coordinated,
so it gets fairly complex.

00:50:11.970 --> 00:50:14.230
So in terms of facilities,
first thing is a data center

00:50:14.240 --> 00:50:15.020
that has a raised floor.

00:50:15.020 --> 00:50:16.040
It's very useful.

00:50:16.040 --> 00:50:19.620
Typically, you need to run power cabling
below the raised floor.

00:50:19.620 --> 00:50:21.560
You don't want to run a
lot of power cables on top.

00:50:21.640 --> 00:50:25.880
In our system, we had something like
432 power receptacles.

00:50:25.880 --> 00:50:29.210
Running the cabling for all of
those power receptacles would be a

00:50:29.220 --> 00:50:30.260
nightmare without a raised floor.

00:50:30.260 --> 00:50:32.960
And if the raised floor
is sufficiently high,

00:50:32.960 --> 00:50:35.640
several feet, you can also run your
network cabling below it.

00:50:35.640 --> 00:50:38.000
There are some good examples
of that kind of design.

00:50:38.000 --> 00:50:39.900
Earth Simulator's raised
floor is six feet.

00:50:39.900 --> 00:50:41.240
They've got a whole floor below.

00:50:41.240 --> 00:50:44.540
It's very easy to do network wiring then,
but then it causes other problems,

00:50:44.540 --> 00:50:45.730
which I'll talk about.

00:50:45.740 --> 00:50:47.870
Typically,
network cabling is easier if you

00:50:47.920 --> 00:50:49.590
try to run it on trace overhead.

00:50:49.600 --> 00:50:54.440
The other thing you have to look at
is how do you locate your machines?

00:50:54.440 --> 00:50:55.420
What is your aisle design?

00:50:55.420 --> 00:50:57.730
Typically,
what you do is you have aisles,

00:50:57.730 --> 00:51:00.220
what are called hot
aisles and cold aisles.

00:51:00.220 --> 00:51:02.790
So in hot aisles,
you have machines blowing hot air,

00:51:02.790 --> 00:51:03.970
which is sucked out.

00:51:04.040 --> 00:51:06.490
And in cold aisles is where
you force in cold air.

00:51:06.670 --> 00:51:08.010
Typically, machines cool front to back.

00:51:08.100 --> 00:51:10.760
And in the first row,
your machine cools front to back.

00:51:10.840 --> 00:51:11.860
The next row is reversed.

00:51:11.890 --> 00:51:14.850
So it's blowing hot air
into the same first aisle.

00:51:14.860 --> 00:51:16.510
The aisle after that,
both the fronts of the

00:51:16.590 --> 00:51:17.560
machines are facing.

00:51:17.560 --> 00:51:18.560
You pump in cold air.

00:51:18.600 --> 00:51:20.060
You do hot aisle, cold aisle design.

00:51:20.060 --> 00:51:21.800
There are other designs
that you can do also.

00:51:21.800 --> 00:51:25.790
You can do star kind of topologies
where you have cold air coming out

00:51:25.790 --> 00:51:28.000
and hot air exiting out this way.

00:51:28.000 --> 00:51:29.130
Even vice versa can be done.

00:51:29.140 --> 00:51:32.080
But typically, hot aisle,
cold aisle is the most common design

00:51:32.080 --> 00:51:33.900
that you'll see in supercomputers.

00:51:36.590 --> 00:51:40.970
I cannot stress that
first part high enough.

00:51:40.970 --> 00:51:42.700
You need a loading dock.

00:51:42.850 --> 00:51:45.920
If you have a facility
without a loading dock,

00:51:45.920 --> 00:51:47.340
forget putting a large scale cluster.

00:51:47.340 --> 00:51:49.730
Putting a small scale cluster
itself will be a problem.

00:51:49.860 --> 00:51:52.710
There are nightmares,
everything from your rack itself

00:51:52.710 --> 00:51:56.100
not fitting in an elevator to not
being able to wheel things in.

00:51:56.140 --> 00:51:58.800
This can be a very big
problem very quickly.

00:51:59.190 --> 00:52:01.730
Fire suppression.

00:52:01.840 --> 00:52:02.800
This is necessary.

00:52:02.800 --> 00:52:05.620
And if you can find a solution
for fire suppression that

00:52:05.760 --> 00:52:08.800
doesn't destroy your machines,
that will be useful.

00:52:08.800 --> 00:52:11.350
Because it's generally a devil
in deep sea decision that

00:52:11.410 --> 00:52:12.800
you have to make over there.

00:52:12.800 --> 00:52:16.800
Water sprinklers save the building,
not the machine.

00:52:16.800 --> 00:52:18.800
Physical security.

00:52:18.980 --> 00:52:21.400
Security inside these machines
is generally kept very low

00:52:21.430 --> 00:52:22.800
for management purposes.

00:52:22.800 --> 00:52:26.590
You want very good physical security
and you also want very good network

00:52:26.590 --> 00:52:28.490
security coming into this system.

00:52:29.240 --> 00:52:30.700
Power is a major issue.

00:52:30.700 --> 00:52:32.700
Budget for sufficient power.

00:52:32.700 --> 00:52:36.880
The way you do it is take your machine,
do load tests on it.

00:52:36.880 --> 00:52:40.320
No load is when the machine
is on but not computing.

00:52:40.320 --> 00:52:43.760
Full load is when the machine
is on and computing at peak.

00:52:43.760 --> 00:52:45.820
So find your most compute
intensive application.

00:52:45.820 --> 00:52:47.200
There are some good ones out there.

00:52:47.200 --> 00:52:51.190
Run it and let it chew up as
many CPU cycles as it can.

00:52:51.190 --> 00:52:54.720
If you've got I/O devices,
run the I/O devices also.

00:52:54.720 --> 00:52:59.130
That gives you expected
load on the machine.

00:52:59.220 --> 00:53:01.320
Power load from each node.

00:53:01.320 --> 00:53:03.940
Not only do you have to worry about
power load from just the nodes alone,

00:53:04.000 --> 00:53:06.970
you have to worry about power
load from your communications and

00:53:06.970 --> 00:53:10.350
more importantly you have to worry
about your power load from cooling.

00:53:10.650 --> 00:53:13.600
So remember, at the end of the day,
whatever power you're going to feed into

00:53:13.600 --> 00:53:16.400
the machine is going to come out as heat.

00:53:16.460 --> 00:53:19.400
So you need a cooling system to take
the heat out and that takes power again.

00:53:19.400 --> 00:53:22.040
So all of this has to be budgeted
into your power calculations.

00:53:22.040 --> 00:53:25.540
And power redundancy
is very important here.

00:53:25.540 --> 00:53:28.310
If you power cycle a node,
a perfectly healthy node that's

00:53:28.460 --> 00:53:31.390
had an uptime of like one year,
there's a very high chance

00:53:31.480 --> 00:53:32.870
it won't come back up again.

00:53:33.440 --> 00:53:35.880
So don't power cycle a
node if you don't have to.

00:53:35.880 --> 00:53:39.380
And typical cases of power
cycling are the unintended ones,

00:53:39.380 --> 00:53:42.960
brownouts,
lightning strike next to the building.

00:53:42.960 --> 00:53:45.760
If you lose power,
you could end up with a lot of

00:53:45.760 --> 00:53:47.030
your nodes not coming back up.

00:53:47.160 --> 00:53:51.380
So typically you can do two-way
redundancy with dual main feeds.

00:53:51.410 --> 00:53:53.340
You want them coming from
different substations so you're

00:53:53.340 --> 00:53:56.800
not dependent on upstream,
and be able to switch between them.

00:53:56.840 --> 00:53:59.200
Three-way redundancy would
have a UPS backup behind it.

00:53:59.450 --> 00:54:01.920
And for a large-scale system,
that UPS can be very large.

00:54:01.920 --> 00:54:03.820
In our case, it's like a megawatt or so.

00:54:03.830 --> 00:54:07.400
And after the UPS, you might even want to
go four-way redundancy,

00:54:07.440 --> 00:54:09.980
with the diesel generator
backing up the UPS also.

00:54:09.990 --> 00:54:12.980
And in this case, remember,
budget for enough redundant power

00:54:13.050 --> 00:54:16.000
to power not just the machine,
but also the cooling system.

00:54:16.000 --> 00:54:17.710
'Cause if you have a large-scale
cluster and you budgeted for

00:54:17.710 --> 00:54:20.720
enough redundant power to bring
the machine down gracefully,

00:54:20.830 --> 00:54:23.000
it won't come down gracefully.

00:54:23.010 --> 00:54:26.440
Because the system's so large that by
the time you can complete your shutdown,

00:54:26.440 --> 00:54:28.140
with the cooling system off,
the temperature might be high

00:54:28.140 --> 00:54:29.780
enough that it'll just crash nodes.

00:54:30.250 --> 00:54:31.760
Temperatures rise.

00:54:31.890 --> 00:54:34.240
In a large-scale design, very quickly.

00:54:34.240 --> 00:54:36.040
We're talking seconds.

00:54:36.060 --> 00:54:38.350
Turn the cooling system off, seconds,
and your room would be

00:54:38.350 --> 00:54:43.470
reaching well over 100,
130, 150 kind of temperatures.

00:54:44.870 --> 00:54:46.460
cooling capacity is very important.

00:54:46.520 --> 00:54:48.720
You need to analyze this very carefully.

00:54:48.770 --> 00:54:51.250
Large clusters produce
phenomenal amounts of heat.

00:54:51.450 --> 00:54:54.050
You're talking millions of BTUs.

00:54:54.190 --> 00:55:00.130
And if they're very tightly placed,
that is they take a very small

00:55:00.150 --> 00:55:03.870
amount of square footage,
then the heat density is very high.

00:55:03.950 --> 00:55:09.350
So you can use typical air cooling
to handle low heat densities.

00:55:09.450 --> 00:55:11.590
But with high heat densities,
this becomes a very big problem,

00:55:11.600 --> 00:55:13.080
and I'll talk about that.

00:55:14.040 --> 00:55:16.520
Don't plan for cooling and
you'll have a really good heater.

00:55:16.520 --> 00:55:20.170
And that's all the machine would
be capable really of doing.

00:55:20.190 --> 00:55:22.640
The side effect is basically
thermal degradation,

00:55:22.650 --> 00:55:26.870
and this is insidious because it
doesn't show up really as a clear-cut

00:55:26.870 --> 00:55:32.400
phenomenon in the sense that you will
have random instability in the machine.

00:55:32.430 --> 00:55:34.220
Nodes will just crash.

00:55:34.220 --> 00:55:37.260
And you're gonna go back and
blame your node manufacturer,

00:55:37.280 --> 00:55:39.120
saying you've just given
us really poor nodes.

00:55:39.210 --> 00:55:41.120
It's actually not the node
manufacturer's problem.

00:55:41.160 --> 00:55:44.670
It is the fact that your thermal
load inside that facility is not

00:55:44.720 --> 00:55:47.070
enough to keep the system stable.

00:55:47.100 --> 00:55:49.460
And eventually what happens
is your processor lifetime

00:55:49.460 --> 00:55:51.040
is shortened in this process.

00:55:51.040 --> 00:55:55.240
Stability, interestingly enough,
with respect to heat, is not linear,

00:55:55.240 --> 00:55:56.460
it's exponential.

00:55:56.490 --> 00:55:58.380
So if you cross certain
temperature domains,

00:55:58.410 --> 00:56:01.260
your machine is eight times
more stable than before.

00:56:01.280 --> 00:56:04.380
So it's very important that you try
to keep your cold dial well below

00:56:04.380 --> 00:56:06.860
specification of the manufacturer.

00:56:06.860 --> 00:56:08.730
In our case,
we keep our cold dial something

00:56:08.730 --> 00:56:12.400
around 70 Fahrenheit coming in,
and our exit air is only barely above 80.

00:56:12.400 --> 00:56:15.880
So it's a fairly well-balanced system,
and you want to keep

00:56:15.880 --> 00:56:17.220
these kinds of numbers.

00:56:17.240 --> 00:56:19.800
And depending upon your
arrangement and your heat density,

00:56:19.800 --> 00:56:21.500
air cooling might not work for you.

00:56:21.500 --> 00:56:25.180
See, one of the problems with
air cooling is it uses air.

00:56:25.180 --> 00:56:28.420
Air has very poor specific heat.

00:56:28.430 --> 00:56:32.320
If you're trying to blow a lot of air,
it loses its cooling capacity

00:56:32.320 --> 00:56:33.500
very quickly in a room.

00:56:33.530 --> 00:56:36.400
And you want to be very careful about
how you cool it and how quickly air

00:56:36.400 --> 00:56:38.460
comes out of your intake aisles.

00:56:38.480 --> 00:56:40.430
If you have very high heat densities,
if you're looking at eight

00:56:40.430 --> 00:56:44.340
kilowatts or 16 kilowatts of heat,
per rack, then you should be looking

00:56:44.340 --> 00:56:46.400
at liquid cooling solutions.

00:56:46.400 --> 00:56:49.340
They're a better way of
getting heat out of the system.

00:56:50.040 --> 00:56:53.340
And finally,
you want to look at system assembly.

00:56:53.340 --> 00:56:55.190
The scale is just overwhelming in here.

00:56:55.200 --> 00:56:57.020
You're going to have thousands of nodes.

00:56:57.040 --> 00:56:59.010
You're going to have
communications interconnects.

00:56:59.100 --> 00:57:00.530
You're going to have
communications cables.

00:57:00.560 --> 00:57:02.130
If you have more than
one communication fabric,

00:57:02.140 --> 00:57:04.620
you're going to have even more cables.

00:57:04.620 --> 00:57:06.560
All of this has to be managed.

00:57:06.600 --> 00:57:08.460
Pipeline everything.

00:57:08.640 --> 00:57:10.100
Assembly lines work very well.

00:57:10.100 --> 00:57:12.480
And there are certain elements
of assembly line that can

00:57:12.480 --> 00:57:14.000
actually be parallelized.

00:57:14.000 --> 00:57:16.410
There are a lot of steps in the hardware,
everything from unboxing,

00:57:16.410 --> 00:57:18.360
which can actually
become a pretty big deal.

00:57:18.390 --> 00:57:20.460
If you have 1,000 boxes
sitting in your warehouse,

00:57:20.460 --> 00:57:23.310
unboxing itself takes a long time.

00:57:23.320 --> 00:57:24.860
So you've got everything from unboxing.

00:57:24.860 --> 00:57:26.860
After you unbox,
you test and burn in right there

00:57:26.860 --> 00:57:30.060
to make sure that you didn't
have any shipping problems.

00:57:30.060 --> 00:57:32.400
Rack your nodes,
and then do your cabling,

00:57:32.410 --> 00:57:34.190
network cabling, power cabling.

00:57:34.200 --> 00:57:36.580
You could have thousands of
cables running over here.

00:57:36.580 --> 00:57:40.050
It has to be done carefully such that
you don't have interference potentially,

00:57:40.140 --> 00:57:42.570
and you don't have cables
being stretched too taut,

00:57:42.570 --> 00:57:45.350
or in some cases, even being too long.

00:57:46.620 --> 00:57:49.940
In terms of software,
you need to create a node image.

00:57:49.940 --> 00:57:52.510
All your nodes in the cluster run
exactly the same copy of the operating

00:57:52.510 --> 00:57:54.090
system with pretty much the same image.

00:57:54.200 --> 00:57:56.620
You need to create an image
and be able to blast this image

00:57:56.620 --> 00:57:58.100
to all the nodes very quickly.

00:57:58.120 --> 00:58:00.000
You want an automated way of doing this.

00:58:00.000 --> 00:58:03.260
The last thing you want to do is
put a CD into every node and start

00:58:03.260 --> 00:58:05.320
installing the operating system.

00:58:05.320 --> 00:58:08.590
Even at scales of four,
it becomes a problem.

00:58:08.590 --> 00:58:12.120
At anything beyond four,
this is a huge time waste,

00:58:12.200 --> 00:58:14.310
particularly if you have to
upgrade your operating system.

00:58:15.030 --> 00:58:16.850
So,
and then power up the system gradually.

00:58:16.860 --> 00:58:18.520
You might want to do sectional bring up.

00:58:18.530 --> 00:58:21.000
Diagnose a section before
you go to the next section.

00:58:21.090 --> 00:58:23.560
And then finally,
one of the things you can also do here

00:58:23.560 --> 00:58:25.900
is use benchmarks to burn in the system.

00:58:25.900 --> 00:58:28.580
They tell you very quickly,
because the system starts off and

00:58:28.680 --> 00:58:31.550
starts computing at peak load,
it very quickly tells

00:58:31.590 --> 00:58:33.080
you if something's flaky.

00:58:33.140 --> 00:58:35.000
And finally,
open up to friendly users first,

00:58:35.000 --> 00:58:38.110
because believe me,
not all the bugs have been worked out.

00:58:38.230 --> 00:58:41.480
Open up to users who understand
that these systems are very large,

00:58:41.480 --> 00:58:44.120
very complex, and potentially unstable.

00:58:44.130 --> 00:58:47.420
This is like a NASCAR in some sense.

00:58:47.420 --> 00:58:49.700
This is a highly tweaked car.

00:58:49.700 --> 00:58:51.880
It's not meant for everyday driving,
really.

00:58:51.950 --> 00:58:55.360
It has its characteristics
that you have to work with.

00:58:55.360 --> 00:58:58.070
And work out all your kinks
before you go into production

00:58:58.150 --> 00:58:59.780
mode with full-scale users.

00:59:01.760 --> 00:59:03.200
Some perspectives on the future.

00:59:03.200 --> 00:59:07.580
So our system was built at
about $5.2 million and we have

00:59:08.280 --> 00:59:11.170
This kind of budget,
now supercomputing is really within reach

00:59:11.170 --> 00:59:12.960
of not just academia but also industry.

00:59:13.110 --> 00:59:15.320
5.2 million is not really
that large a budget.

00:59:15.320 --> 00:59:19.440
Universities run annual budgets
of well over 500 million.

00:59:19.610 --> 00:59:22.190
So this is not a
particularly large budget.

00:59:22.200 --> 00:59:26.020
This might be the beginning
of a paradigm shift.

00:59:26.020 --> 00:59:32.190
If you look back into the 70s,
you had these glass house mainframes.

00:59:32.260 --> 00:59:35.200
A whole university could
afford one if it was lucky.

00:59:35.330 --> 00:59:38.170
There was a famous statement by Watson.

00:59:38.200 --> 00:59:42.640
It said the worldwide demand
for computers estimated at

00:59:42.640 --> 00:59:44.480
that time by IBM was six.

00:59:44.860 --> 00:59:47.840
And that was true because at
that time the machines were very,

00:59:47.840 --> 00:59:50.250
very expensive and almost
entire countries or states

00:59:50.250 --> 00:59:51.250
could afford one machine.

00:59:51.260 --> 00:59:53.160
But then the minis came about.

00:59:53.220 --> 00:59:54.920
They were a lot cheaper.

00:59:54.920 --> 00:59:58.850
A lot more people had access to them
because universities could buy them.

00:59:58.860 --> 01:00:00.470
In some cases,
even departments could afford them.

01:00:00.520 --> 01:00:03.250
And this started a large
body of computing research,

01:00:03.250 --> 01:00:07.420
results from which made the mini better
and better and better and led to the PC.

01:00:08.900 --> 01:00:11.400
Essentially, as the cycle starts,
where you have the machines

01:00:11.860 --> 01:00:14.370
getting more stable,
results going back to industry,

01:00:14.420 --> 01:00:15.670
more units being built.

01:00:15.720 --> 01:00:17.320
And the cycle keeps continuing.

01:00:17.320 --> 01:00:19.520
It gets cheaper and
cheaper at each iteration.

01:00:19.520 --> 01:00:21.040
And this is what you want to start.

01:00:21.120 --> 01:00:24.540
Today,
supercomputing is considered exotic.

01:00:24.540 --> 01:00:30.260
And programming methodologies are
still at their infancy simply because

01:00:30.320 --> 01:00:35.370
it's not had enough people being able
to address the problem because there

01:00:35.370 --> 01:00:36.280
are not that many of them around.

01:00:36.300 --> 01:00:38.330
And each one of them
is in very high demand.

01:00:38.900 --> 01:00:40.940
So how do you make this a commodity?

01:00:40.980 --> 01:00:42.440
How do you make this something?

01:00:42.460 --> 01:00:47.010
Today, if you in the enterprise want
to deploy a supercomputer,

01:00:47.010 --> 01:00:50.130
you want to do it in-house
because you believe there is a

01:00:50.130 --> 01:00:52.860
lot of technical knowledge that
goes into building the system.

01:00:52.880 --> 01:00:56.120
You would not really trust
a data center to do it.

01:00:56.380 --> 01:00:58.220
But that's going to happen.

01:00:58.280 --> 01:00:59.960
When it becomes a commodity,
this is something that

01:01:00.020 --> 01:01:02.090
you would look at and say,
"Why should we be spending

01:01:02.090 --> 01:01:05.300
money and wasting IT resources
on putting up supercomputers?

01:01:05.320 --> 01:01:08.470
Let's have data centers come up and let
them handle all the hassles of building,

01:01:08.470 --> 01:01:10.360
maintaining, upgrading these things.

01:01:10.420 --> 01:01:13.830
We'll just use the cycles."
How do we get there?

01:01:15.190 --> 01:01:18.470
In terms of architectures,
we are looking at something interesting.

01:01:18.480 --> 01:01:21.250
There are some emerging processor
designs that make it possible.

01:01:21.260 --> 01:01:27.210
So if you look at processors today,
many of them support vector designs.

01:01:27.210 --> 01:01:29.420
In fact, one of the best examples
there is Velocity Engine,

01:01:29.420 --> 01:01:32.260
the Altivec, on the G5 processor.

01:01:32.370 --> 01:01:34.150
It’s a full vector implementation.

01:01:34.320 --> 01:01:35.730
It also has a scalar component.

01:01:35.730 --> 01:01:39.260
So most of you are used to vectors from
the image processing side of things.

01:01:39.310 --> 01:01:41.660
Photoshop onwards,
all of these are optimized pretty

01:01:41.720 --> 01:01:43.250
heavily to run on the Altivec.

01:01:43.260 --> 01:01:45.260
And that’s where you get your
really big performance difference.

01:01:45.390 --> 01:01:47.690
Because the vector is highly
efficient for these operations.

01:01:47.800 --> 01:01:50.200
If you were to run that on the scalar,
your performance would be a lot worse.

01:01:50.270 --> 01:01:52.760
And there is a true
vector implementation.

01:01:52.760 --> 01:01:55.070
Now for scientific applications,
you need something called

01:01:55.070 --> 01:01:56.250
double precision floating point.

01:01:56.260 --> 01:01:58.260
The Altivec today doesn’t support that.

01:01:58.260 --> 01:02:01.880
But a processor design that
supports a double precision vector,

01:02:02.000 --> 01:02:04.070
and the next session will be on
the new and emerging hardware

01:02:04.260 --> 01:02:06.470
architecture technologies
including hardware infrastructure,

01:02:06.500 --> 01:02:08.970
storage solutions, file systems,
and software frameworks.

01:02:32.000 --> 01:02:33.000
card.

01:02:33.000 --> 01:02:34.350
That's pretty balanced right now.

01:02:34.560 --> 01:02:36.320
Both of them are bottlenecks,
but at least the

01:02:36.320 --> 01:02:39.610
bottlenecks are balanced,
which is pretty important because if

01:02:39.700 --> 01:02:43.460
you're looking at gigabit Ethernet,
that's 125 megabytes a second.

01:02:43.460 --> 01:02:46.230
So get out on the network
at 125 megabytes a second,

01:02:46.230 --> 01:02:48.700
you get data from memory
at 4 gigabytes a second.

01:02:48.700 --> 01:02:49.700
You got a problem.

01:02:49.720 --> 01:02:52.070
But it's here,
and this opens a door because

01:02:52.070 --> 01:02:54.930
there are some new versions coming
out which have low latency also,

01:02:54.930 --> 01:03:00.200
which effectively you can use a cluster
architecture and in software be able

01:03:00.210 --> 01:03:04.440
to create a shared memory machine
that rivals a shared memory machine.

01:03:04.440 --> 01:03:07.540
So basically our goal over here is
we're working towards an architecture

01:03:07.540 --> 01:03:10.970
that using a basic cluster design
does both vector and scalar and both

01:03:11.080 --> 01:03:13.240
distributed memory and shared memory.

01:03:13.240 --> 01:03:15.970
One unified architecture,
you should be able to run codes

01:03:15.990 --> 01:03:17.660
of any flavor on the same system.

01:03:17.740 --> 01:03:20.840
Biggest thing being you get rid of
the price performance of clusters,

01:03:20.920 --> 01:03:22.730
which are very low today.

01:03:22.740 --> 01:03:25.500
The other thing that we need to
work on is programming models

01:03:25.500 --> 01:03:29.500
on how to make these things a
lot accessible to a much larger

01:03:29.500 --> 01:03:30.830
community than the current community.

01:03:30.940 --> 01:03:33.070
that uses it.

01:03:34.060 --> 01:03:36.220
And at Virginia Tech,
a little bit of the future.

01:03:36.370 --> 01:03:39.990
So System 10 is our current system
and it's just the beginning.

01:03:39.990 --> 01:03:43.480
We have a long-term interest in the field
of computational science and engineering.

01:03:43.600 --> 01:03:45.350
We are recruiting pretty heavily there.

01:03:45.350 --> 01:03:47.470
And this system,
our current one will be followed

01:03:47.480 --> 01:03:49.890
by another one that will
probably start off in '06 and

01:03:49.900 --> 01:03:51.790
deploy through '06 essentially.

01:03:51.990 --> 01:03:54.440
And I know I'm going to
get asked this question.

01:03:54.500 --> 01:03:55.900
What happened to it?

01:03:56.020 --> 01:03:57.900
Are XServe upgrades going on?

01:03:58.110 --> 01:04:00.920
Those are a bunch of the
machines that are right.

01:04:02.480 --> 01:04:04.760
This is a handiwork of the IT group.

01:04:04.790 --> 01:04:05.900
Beautiful cabling.

01:04:05.940 --> 01:04:07.400
Absolutely beautiful cabling.

01:04:07.540 --> 01:04:10.680
And every row looks identical.

01:04:10.680 --> 01:04:12.000
You can't see cables anywhere.

01:04:12.000 --> 01:04:12.940
Very neatly tied up.

01:04:13.040 --> 01:04:15.040
High speed communication
coming on one side,

01:04:15.050 --> 01:04:17.690
power and the low speed
communication on the other side.

01:04:21.800 --> 01:04:23.800
There is how many racks of this?

01:04:23.800 --> 01:04:25.800
36 racks.

01:04:25.800 --> 01:04:29.800
Plus I think what,
two more in the third aisle, right?

01:04:29.800 --> 01:04:33.190
Thank you.