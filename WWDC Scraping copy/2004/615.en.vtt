WEBVTT

00:00:12.770 --> 00:00:14.590
Hello, everybody.

00:00:14.630 --> 00:00:17.910
This is the second of
our two Xsan sessions,

00:00:17.950 --> 00:00:19.020
Xsan In Depth.

00:00:20.050 --> 00:00:24.890
Basically, I'm Greg Vaughan,
and I'm going to be talking about

00:00:24.890 --> 00:00:27.780
a lot of the same material that
was in the overview session.

00:00:27.780 --> 00:00:30.240
How many of you were here
for the overview session?

00:00:30.240 --> 00:00:31.600
Okay, good.

00:00:33.840 --> 00:00:35.430
I'm going to cover some
of the same material,

00:00:35.430 --> 00:00:37.240
but from a slightly
different perspective.

00:00:37.330 --> 00:00:39.840
I won't have any product prices up here.

00:00:39.840 --> 00:00:43.300
Instead,
a little code samples and hopefully

00:00:43.400 --> 00:00:46.130
some more technical information.

00:00:46.400 --> 00:00:49.260
I'm going to start actually going
over some of the same stuff with

00:00:49.260 --> 00:00:52.850
the different types of file systems,
just to separate them

00:00:52.850 --> 00:00:57.000
out and make it clear,
sort of, in terms of file systems,

00:00:57.000 --> 00:01:01.350
what makes a SAN file system
different from other types.

00:01:01.370 --> 00:01:05.710
I'm going to talk specifically about
Xsan and sort of the communication

00:01:05.710 --> 00:01:08.340
protocols and how it's really working.

00:01:09.300 --> 00:01:13.000
I'll go into a bit of
depth on the Xsan admin.

00:01:13.170 --> 00:01:19.050
We'll show a demo of setting up the SAN,
some of the other features of the admin.

00:01:19.290 --> 00:01:25.140
I'll talk about how the volumes work
in terms of when you're writing apps,

00:01:25.230 --> 00:01:29.310
there's some different characteristics
both from local volumes and network

00:01:29.320 --> 00:01:31.480
volumes you need to be aware of.

00:01:32.320 --> 00:01:34.650
And then finally,
I'll talk specifically about

00:01:34.650 --> 00:01:38.210
some developer APIs that can
be used in the applications to

00:01:38.210 --> 00:01:42.790
make them work better with Xsan,
be more Xsan aware.

00:01:45.520 --> 00:01:48.860
So starting out with the
different file system types.

00:01:48.990 --> 00:01:53.240
Tom mentioned direct-attached storage,
which is basically your

00:01:53.470 --> 00:01:57.360
traditional local hard drive,
just some new fancy names for it.

00:01:57.460 --> 00:02:02.340
The way in which a local hard drive
works is you've got your file system.

00:02:02.340 --> 00:02:04.950
The drive's presenting
just an array of blocks.

00:02:04.950 --> 00:02:09.200
The job of the file system is to
organize that data using the catalog,

00:02:09.200 --> 00:02:12.420
present a higher-level
API up to applications.

00:02:12.420 --> 00:02:16.350
So in a typical diagram,
you've got basically

00:02:16.360 --> 00:02:21.560
at the high-level API,
you're dealing in terms of files.

00:02:21.560 --> 00:02:27.300
The application's going to open a file,
write to a file, use file offsets.

00:02:27.400 --> 00:02:31.330
The file system's going to translate
that to the blocks on the disk.

00:02:31.400 --> 00:02:35.630
This is the technology that's
been around for decades.

00:02:36.260 --> 00:02:39.600
But of course,
a block-level device can't be shared

00:02:39.600 --> 00:02:43.510
by multiple computers because there's
no way to keep the catalog in sync

00:02:43.600 --> 00:02:45.200
between the multiple computers.

00:02:45.200 --> 00:02:50.480
So this early limitation got solved
by the network-attached storage,

00:02:50.480 --> 00:02:52.170
or the file server.

00:02:52.220 --> 00:02:56.690
Network-attached storage basically
is just taking the exact same file

00:02:56.690 --> 00:02:58.200
server and putting it into a box.

00:02:58.200 --> 00:03:01.200
So the underlying technology
is exactly the same.

00:03:01.200 --> 00:03:04.150
The idea here is you're going
to take the high-level call,

00:03:04.280 --> 00:03:10.200
ship it across the network to the server,
and the server will perform the action.

00:03:10.200 --> 00:03:14.200
So you've got the same diagram.

00:03:14.200 --> 00:03:17.150
You're making the call
at the file system layer.

00:03:17.210 --> 00:03:21.200
The network file system
directly mirrors that call.

00:03:21.200 --> 00:03:24.290
And on the server,
when it makes the call,

00:03:24.290 --> 00:03:27.200
you're doing the offsets to the disk.

00:03:27.220 --> 00:03:31.200
This will allow you
basically to have the call.

00:03:31.200 --> 00:03:35.280
You have the volume integrity,
but at the price of funneling

00:03:35.280 --> 00:03:39.550
all the data over the network
to the server and all funneling

00:03:39.550 --> 00:03:42.200
it through the single machine.

00:03:42.200 --> 00:03:46.200
The other slight thing about
that in dealing with file servers

00:03:46.200 --> 00:03:50.450
is when you scale them up,
you tend to have problems with

00:03:50.450 --> 00:03:55.690
the different types of requests,
especially metadata-intensive requests,

00:03:55.750 --> 00:03:59.200
doing large directory
listings and so forth,

00:03:59.200 --> 00:04:01.200
causes the server to load catalog blocks.

00:04:01.200 --> 00:04:04.200
And so, you have to have a lot
of data that blocks off,

00:04:04.200 --> 00:04:05.200
which can interfere with the
streaming of data off the hard drive.

00:04:05.200 --> 00:04:08.320
And conversely,
those metadata requests can be

00:04:08.320 --> 00:04:10.200
blocked behind the large IOs.

00:04:10.200 --> 00:04:12.200
And when you're dealing
with heavy file servers,

00:04:12.200 --> 00:04:16.200
you see the big latencies and, like,
opening up a little directory listing.

00:04:16.200 --> 00:04:20.850
And those are really hard to overcome,
those sort of scalability

00:04:20.850 --> 00:04:23.200
limitations in file servers.

00:04:25.600 --> 00:04:28.040
RAID solves the other
part of the problem.

00:04:28.310 --> 00:04:32.220
You're trying to overcome the
limitations of disk speed by

00:04:32.280 --> 00:04:34.000
combining the multiple drives.

00:04:34.050 --> 00:04:38.070
In addition to the performance,
you're giving the reliability of

00:04:38.070 --> 00:04:40.210
redundancy between the drives.

00:04:40.280 --> 00:04:44.900
The important thing about the RAID is
that it's happening sort of behind

00:04:44.900 --> 00:04:46.660
the scenes of the file system.

00:04:46.670 --> 00:04:48.280
The file system isn't aware of that.

00:04:48.280 --> 00:04:52.440
The RAID system is presenting the
single drive to the file system,

00:04:52.450 --> 00:04:58.060
and the RAID is mapping the block offsets
internally to the multiple drives.

00:04:58.140 --> 00:05:01.050
And you've got the
different RAID schemes.

00:05:01.050 --> 00:05:05.270
I always get them confused,
especially RAID 0 and RAID 1.

00:05:05.280 --> 00:05:07.280
But RAID 0, you've got the striping.

00:05:07.280 --> 00:05:10.260
Mirroring provides the redundancy.

00:05:10.260 --> 00:05:13.760
And then with RAID 5,
you've got both the performance

00:05:13.760 --> 00:05:17.290
of writing to multiple drives
plus the redundant data so

00:05:17.290 --> 00:05:21.880
that if any one drive fails,
it can be rebuilt easily without

00:05:21.880 --> 00:05:24.370
losing access to your data.

00:05:25.850 --> 00:05:30.190
In addition to the underlying RAID,
you've got software RAID,

00:05:30.200 --> 00:05:33.910
which happens at the driver
level to distribute your data

00:05:33.910 --> 00:05:35.910
out to the multiple drives.

00:05:37.310 --> 00:05:41.970
Once you've done that,
then the RAID box will further distribute

00:05:41.970 --> 00:05:44.200
it out to the individual disks.

00:05:44.200 --> 00:05:47.580
But the point of all this is
that the RAID system is all

00:05:47.580 --> 00:05:50.190
happening down at the block device.

00:05:50.250 --> 00:05:53.480
The file system is still
your traditional file system,

00:05:53.480 --> 00:05:57.900
be it HFS or UFS,
that is just dealing with what it

00:05:57.900 --> 00:06:03.020
sees as a single array of blocks,
and it's maintaining the same

00:06:03.020 --> 00:06:05.190
catalog data as it always would.

00:06:05.200 --> 00:06:11.260
So it's just doing the same
translation down to the disk offset,

00:06:11.340 --> 00:06:14.170
and then down at the RAID level,
that's being distributed

00:06:14.170 --> 00:06:16.200
out to the multiple disks.

00:06:18.040 --> 00:06:21.990
So the problem with that is you're
still faced with the file server if

00:06:22.090 --> 00:06:24.280
you want to distribute your data.

00:06:24.380 --> 00:06:26.900
Even though you've
sped up the hard drive,

00:06:26.900 --> 00:06:34.210
all the data is flowing
through the one file server,

00:06:34.210 --> 00:06:34.210
and that is your big limitation.

00:06:37.060 --> 00:06:43.830
So how the SAN file system really works
to overcome this is by separating out

00:06:43.830 --> 00:06:46.990
the notion of the catalog from the data.

00:06:47.070 --> 00:06:52.550
There's no real reason why the
catalog needs to live with the data.

00:06:53.480 --> 00:06:58.450
Once you've decided a one particular
place to store your catalog,

00:06:58.530 --> 00:07:04.540
you can have a special purpose
server that is going to just deal

00:07:04.540 --> 00:07:10.090
with the catalog and do part of the
job of a traditional file system.

00:07:10.330 --> 00:07:15.720
Basically, update the catalog and figure
out where on the drives the

00:07:15.750 --> 00:07:18.480
data is going to actually live.

00:07:19.010 --> 00:07:23.660
That way the client file systems
can talk to the metadata controller

00:07:23.870 --> 00:07:29.000
to get the catalog information,
but then do the I/O directly

00:07:29.000 --> 00:07:30.930
to the RAID devices.

00:07:32.620 --> 00:07:35.500
So here's your typical Xsan setup.

00:07:35.540 --> 00:07:37.980
Diagram's a little different
than Tom's diagrams,

00:07:38.090 --> 00:07:40.100
but it's got all the same components.

00:07:40.150 --> 00:07:41.290
Got your client system.

00:07:41.380 --> 00:07:43.300
I'm only showing one here.

00:07:43.360 --> 00:07:46.830
Couple RAID boxes and your controller.

00:07:46.930 --> 00:07:51.330
Got everything hooked up with the fiber
channel and you've got the IP network

00:07:51.330 --> 00:07:53.870
between the client and the controller.

00:07:54.310 --> 00:07:57.130
In this particular case,
you've got the normal

00:07:57.130 --> 00:07:59.530
RAID configuration for the example.

00:07:59.650 --> 00:08:04.440
Because you've got the two controllers,
we have the two virtual disks per RAID to

00:08:04.460 --> 00:08:11.470
a total of four LUNs that we're going
to group together to be our Xsan volume.

00:08:13.200 --> 00:08:16.670
So the first thing you do is you
select one of the LUNs that's

00:08:16.740 --> 00:08:18.460
going to store your catalog data.

00:08:18.460 --> 00:08:24.160
You can decide to either dedicate that
to storing the catalog data or you can

00:08:24.160 --> 00:08:27.460
choose to store other data alongside it.

00:08:27.540 --> 00:08:30.230
The point is, though,
you don't want the other data

00:08:30.230 --> 00:08:33.330
being stored with it to be
high-performance data because,

00:08:33.330 --> 00:08:36.960
again, you'll start to run into the same
limitations you have with the

00:08:36.960 --> 00:08:40.740
file server of the data competing
with the catalog information.

00:08:40.740 --> 00:08:44.310
So even though you might want
to store other files there,

00:08:44.310 --> 00:08:49.500
they should be less accessed files than
your more performance-critical ones.

00:08:49.500 --> 00:08:55.080
So in this example,
we've chosen LUN1 to store our metadata.

00:08:57.210 --> 00:09:00.580
Basically, in setting up your volume,
all you're really doing is

00:09:00.660 --> 00:09:02.400
configuring the controller.

00:09:02.400 --> 00:09:06.840
The controller is the only
machine that has a real notion

00:09:06.840 --> 00:09:09.500
of what comprises the volume.

00:09:09.620 --> 00:09:14.480
So, you tell the controller, basically,
what all the LUNs are

00:09:14.500 --> 00:09:18.260
that compose the volume,
and I'll go into a bit of additional

00:09:18.260 --> 00:09:21.660
information you can give it
in terms of how to construct

00:09:21.660 --> 00:09:23.970
the volume out of these LUNs.

00:09:24.300 --> 00:09:28.780
But first,
to go over our same old example,

00:09:28.840 --> 00:09:33.920
you've got the client application
making the file-level call.

00:09:34.620 --> 00:09:38.010
At this stage,
rather than shipping the whole

00:09:38.010 --> 00:09:41.120
call off to a file server,
as it would in the normal

00:09:41.120 --> 00:09:46.370
network-attached storage case,
it's just going to make a request to

00:09:46.450 --> 00:09:50.870
find where the data for this file lives,
but keeping the actual

00:09:50.870 --> 00:09:53.540
data on the client system.

00:09:54.200 --> 00:09:57.810
The controller's going to read
the catalog information out of its

00:09:58.010 --> 00:10:01.490
private metadata storage on LUN1.

00:10:02.220 --> 00:10:05.800
And then reply with the disk
offsets back to the client.

00:10:05.810 --> 00:10:09.560
At this point,
once the client has dedicated

00:10:09.560 --> 00:10:14.560
storage on a particular LUN,
it can talk directly to that LUN and

00:10:14.560 --> 00:10:20.140
stream the data across without worrying
about corrupting block offsets.

00:10:20.300 --> 00:10:24.200
The other thing is,
because you've got a collection of LUNs,

00:10:24.200 --> 00:10:26.400
you're not only telling
the client the offset,

00:10:26.400 --> 00:10:30.200
but you're telling it which
LUN the actual data lives on.

00:10:30.200 --> 00:10:34.220
But other than that,
the client has no notion of the actual

00:10:34.220 --> 00:10:39.200
sort of file system layout on these LUNs,
how they're divided into files,

00:10:39.200 --> 00:10:42.130
how the catalog data is structured,
or any of that.

00:10:42.200 --> 00:10:47.160
All it knows is, this is the data,
this is the area where it wants to write

00:10:47.160 --> 00:10:50.080
the data the application has given it.

00:10:51.880 --> 00:10:56.610
The other thing you can do, as Tom said,
is group the LUNs into storage pools,

00:10:56.610 --> 00:11:00.070
which has an effect very similar
to striping in software RAID.

00:11:00.100 --> 00:11:05.180
The big difference here is even though
you're getting the same performance

00:11:05.180 --> 00:11:09.370
effect of being able to talk to two LUNs,
rather than being handled by

00:11:09.370 --> 00:11:13.570
a driver on the client system,
it's the metadata controller that's going

00:11:13.600 --> 00:11:16.510
to control the access to these two LUNs.

00:11:17.920 --> 00:11:20.810
So in this particular case,
the client's going to ask

00:11:20.810 --> 00:11:24.460
for that same file offset,
and the controller's going to tell

00:11:24.460 --> 00:11:28.840
it that part of this file lives on
LUN 3 and part of it lives on LUN 4.

00:11:28.840 --> 00:11:31.450
And the client is just
smart enough to know,

00:11:31.450 --> 00:11:34.580
"Oh, well,
if I'm writing to two different LUNs,

00:11:34.580 --> 00:11:37.510
I can stream the data out
simultaneously." So you get the

00:11:37.510 --> 00:11:41.200
same performance effect that
you would in software striping.

00:11:43.470 --> 00:11:48.400
The other main job of the controller
is to handle the file locking.

00:11:48.400 --> 00:11:51.650
With the Xsan,
because you've got the controller

00:11:51.740 --> 00:11:55.090
handling the catalog data,
you don't need to worry about

00:11:55.260 --> 00:11:56.690
file system-level corruption.

00:11:56.720 --> 00:12:01.280
But still, within individual files,
you need to worry about data corruption.

00:12:01.280 --> 00:12:03.440
That's the same as any
network file server.

00:12:03.440 --> 00:12:06.890
And the way that is handled is,
at the application level,

00:12:06.980 --> 00:12:09.080
you need to make locking calls.

00:12:09.080 --> 00:12:10.400
This works.

00:12:10.580 --> 00:12:14.580
It's the exact same locking calls
you'd use for NFS or anything else,

00:12:14.580 --> 00:12:18.660
but it's the controller that needs to
keep track of those locks and actually

00:12:18.660 --> 00:12:20.400
arbitrate between the different clients.

00:12:20.510 --> 00:12:22.380
So it has that role as well.

00:12:22.380 --> 00:12:27.680
And then other than that, as I said,
the clients are just writing data out

00:12:27.680 --> 00:12:30.560
wherever the controller asks it to.

00:12:30.560 --> 00:12:34.360
And to the client applications,
they just see it as one big volume,

00:12:34.360 --> 00:12:37.460
and they don't really know
about the LUNs behind it.

00:12:42.080 --> 00:12:44.700
The other thing, as mentioned before,
is the failover.

00:12:44.700 --> 00:12:47.910
Basically,
how this is going to work is when the

00:12:47.910 --> 00:12:52.580
clients notice that a controller is down,
they actually get together and

00:12:52.580 --> 00:12:57.200
vote for a backup controller,
and you can configure it such that

00:12:57.200 --> 00:13:00.320
it fails over in a predictable way.

00:13:01.540 --> 00:13:04.860
The backup controller comes up,
it knows where to read

00:13:04.860 --> 00:13:07.990
its catalog data from,
reads out the catalog data,

00:13:08.050 --> 00:13:12.090
it's a journaled file system,
so it's able to look at the journal and

00:13:12.250 --> 00:13:17.570
reconstruct the last few transactions,
very quickly come up and start running.

00:13:17.580 --> 00:13:21.220
We're still doing some of
the performance benchmarks,

00:13:21.220 --> 00:13:24.590
but as Tom said,
it should just be a few seconds.

00:13:24.590 --> 00:13:28.340
I think 15 seconds would
be sort of the outer limit.

00:13:28.360 --> 00:13:31.350
The important thing to
note is during this time,

00:13:31.540 --> 00:13:36.130
15 seconds can be a long time
in terms of video streaming,

00:13:36.130 --> 00:13:39.530
but the clients don't always need
to talk to the metadata controller.

00:13:39.740 --> 00:13:42.840
Once they've asked the metadata
controller and gotten the

00:13:42.840 --> 00:13:45.860
offsets for their files,
they're dealing directly

00:13:45.860 --> 00:13:47.040
with the RAID box.

00:13:47.040 --> 00:13:50.110
So if they're streaming a
file off the RAID box and the

00:13:50.200 --> 00:13:54.370
metadata controller fails,
it's possible the new one will be up

00:13:54.370 --> 00:13:58.480
again before they even notice it's gone,
before they have a need to actually

00:13:58.480 --> 00:14:01.370
talk to the metadata controller again,
in which case you'll have to

00:14:01.370 --> 00:14:01.540
wait a little longer for the
new one to come up again.

00:14:01.540 --> 00:14:03.990
have uninterrupted access
to the file system,

00:14:03.990 --> 00:14:06.280
even through a failover.

00:14:07.810 --> 00:14:11.630
The other thing the clients need to do
is once the controller comes back up,

00:14:11.720 --> 00:14:14.790
the new controller, of course,
doesn't know about the locks

00:14:14.920 --> 00:14:16.190
the clients have taken out.

00:14:16.320 --> 00:14:19.170
So when the clients see that
the new controller's up,

00:14:19.240 --> 00:14:22.360
they need to go and tell it
about all the locks they have,

00:14:22.360 --> 00:14:24.440
and the controller will
rebuild the lock table.

00:14:28.580 --> 00:14:33.390
So in terms of volume configuration,
we talked about the various ways

00:14:33.470 --> 00:14:37.730
that you can group your LUNs together
to build up your whole volume.

00:14:37.790 --> 00:14:40.720
The first thing you're going to
need to do is to pick which LUN your

00:14:40.730 --> 00:14:42.960
metadata is going to be stored on.

00:14:43.110 --> 00:14:46.580
It's going to be both the catalog
information and the journal.

00:14:46.640 --> 00:14:49.780
Technically, you can configure those
to be on different LUNs,

00:14:49.890 --> 00:14:52.620
but usually there's no reason to do so.

00:14:52.620 --> 00:14:56.510
So in our admin software, generally,
they'll both always be

00:14:56.530 --> 00:14:58.190
stored on the same LUN.

00:14:59.720 --> 00:15:04.780
The other thing you're going to do is
decide whether you want to store other

00:15:04.780 --> 00:15:08.070
data files along with the catalog data.

00:15:08.490 --> 00:15:11.240
Basically,
the catalog data doesn't take much room,

00:15:11.240 --> 00:15:14.900
so you can either partition your
RAID in such a way that you have

00:15:14.900 --> 00:15:18.540
a very small LUN and make it
exclusive for the catalog data,

00:15:18.560 --> 00:15:24.300
or if you have a larger LUN,
you may choose to store

00:15:24.300 --> 00:15:24.300
other files alongside it.

00:15:25.100 --> 00:16:27.800
[Transcript missing]

00:16:28.560 --> 00:16:30.550
And like I said, it's not very big.

00:16:30.870 --> 00:16:35.810
Ten million files will probably only use
up about 10 gig of space for a catalog.

00:16:35.860 --> 00:16:38.890
So you don't need a lot of storage there.

00:16:39.050 --> 00:16:42.770
But high-performance
storage is very important.

00:16:45.960 --> 00:16:48.900
So once you've got your
metadata controller,

00:16:48.920 --> 00:16:51.720
the question for the
rest of the storage is,

00:16:51.840 --> 00:16:55.370
how are you going to group
it into storage pools?

00:16:55.570 --> 00:17:00.000
Certainly, if you take all your LUNs and
combine them into storage pools,

00:17:00.000 --> 00:17:02.960
the idea is the client could
talk to all the LUNs at once and

00:17:03.080 --> 00:17:06.440
theoretically get very high performance.

00:17:06.440 --> 00:17:08.610
There's a few limitations to that.

00:17:08.660 --> 00:17:13.310
One is you want to make sure all the
LUNs have the same characteristics.

00:17:13.530 --> 00:17:15.870
Because it's effectively
like software striping,

00:17:15.870 --> 00:17:18.270
when you've combined
all these LUNs together,

00:17:18.320 --> 00:17:23.080
you're really going to get sort
of the smallest LUN and the

00:17:23.130 --> 00:17:26.740
slowest LUN will be the gating
factor for the whole storage pool.

00:17:26.740 --> 00:17:30.240
So you really want to take
all your identical LUNs and

00:17:30.350 --> 00:17:32.740
group them into storage pools.

00:17:33.500 --> 00:18:37.400
[Transcript missing]

00:18:38.420 --> 00:18:42.400
And the other side effect of that is
that you are going to end up then with

00:18:42.790 --> 00:18:48.280
some storage pools that are faster and
other storage pools that are slower.

00:18:48.900 --> 00:20:15.400
[Transcript missing]

00:20:17.260 --> 00:20:21.620
So now I'll talk a bit about our
administration software and how

00:20:21.630 --> 00:20:26.400
that works and how you set up a
SAN using the administration software.

00:20:26.400 --> 00:20:30.630
We've tried,
certainly as we do in all our products,

00:20:30.720 --> 00:20:37.000
to consolidate this and sort of make
it easy and understandable as possible.

00:20:37.000 --> 00:20:40.360
Although,
certainly a SAN is a complicated thing

00:20:40.360 --> 00:20:43.110
and there's lots of different aspects.

00:20:43.180 --> 00:20:46.900
There's always trade-offs then
in terms of giving access to

00:20:46.990 --> 00:20:53.740
functionality versus making it sort
of easy and straightforward to use.

00:20:55.860 --> 00:21:02.100
So here's just a few slide shots
of what a setup looks like.

00:21:02.170 --> 00:21:07.580
The first step in the setup is to,
you've got to define all the machines

00:21:07.580 --> 00:21:11.560
that are going to be part of your SAN,
both the clients and the controllers.

00:21:11.820 --> 00:21:14.520
It'll find these
machines over rendezvous.

00:21:14.520 --> 00:21:17.970
It actually detects,
asks each machine what LUNs it's

00:21:17.970 --> 00:21:21.660
hooked up to so it can decide
which machines are actually on

00:21:21.660 --> 00:21:23.980
the same fiber channel network.

00:21:24.000 --> 00:21:25.400
And then it'll come up.

00:21:25.400 --> 00:21:28.230
It'll allow you to select
these machines and say,

00:21:28.230 --> 00:21:32.380
yes, these are the machines that I want
to be part of my Xsan system.

00:21:32.380 --> 00:21:36.060
You then enter the serial
numbers for those machines.

00:21:36.060 --> 00:21:40.480
Because you've bought a
separate Xsan box for each one,

00:21:40.480 --> 00:21:41.800
you'll have a separate serial number.

00:21:41.800 --> 00:21:44.780
You'll have a separate serial number
to enter each of those machines.

00:21:44.780 --> 00:21:47.850
And then you choose whether you want
them to be clients or controllers.

00:21:47.860 --> 00:21:51.160
And for the controllers,
you decide their failover priority.

00:21:51.160 --> 00:21:53.600
You can actually make
them all controllers.

00:21:53.600 --> 00:21:57.190
If a machine is a backup
controller just in standby,

00:21:57.230 --> 00:22:01.010
even if it's normally used
as a client editing system,

00:22:01.010 --> 00:22:03.880
there's really no problem with that.

00:22:04.020 --> 00:22:06.810
Unless it actually becomes a controller,
just because it's set

00:22:06.810 --> 00:22:08.710
as a backup controller,
there won't be any

00:22:08.710 --> 00:22:10.200
performance degradation.

00:22:10.200 --> 00:22:11.800
And the license.

00:22:11.800 --> 00:22:14.940
The license allows,
once you've installed Xsan on a system,

00:22:14.940 --> 00:22:17.410
you can either make it a
client or a controller.

00:22:17.410 --> 00:22:18.580
It's your choice.

00:22:21.420 --> 00:22:24.640
Once you've done that,
you need to configure the storage.

00:22:24.750 --> 00:22:28.490
Basically, this is you decide how many,
which volumes you want,

00:22:28.670 --> 00:22:31.430
what the storage pools
in each volume are,

00:22:31.560 --> 00:22:35.440
and then what LUNs are part of
each of those storage pools.

00:22:36.860 --> 00:22:40.220
And then once you've done that,
you basically select the

00:22:40.220 --> 00:22:44.000
volume and tell the controller
to start up on that volume.

00:22:44.000 --> 00:22:47.650
And as soon as the controllers start up,
the volume is available

00:22:47.650 --> 00:22:49.340
to be mounted on clients.

00:22:50.240 --> 00:22:56.110
So at this point, oh actually no,
a few other things it does.

00:22:56.420 --> 00:22:59.580
In addition to setting up the volumes,
you can set up certain

00:22:59.580 --> 00:23:03.340
administrator notifications.

00:23:03.410 --> 00:23:09.820
You can set up email or pager
notifications if storage pools fill

00:23:09.820 --> 00:23:16.330
up or you have certain failures
and users exceed their quotas.

00:23:17.010 --> 00:23:20.990
You also can mount and unmount
volumes on each of the clients.

00:23:21.060 --> 00:23:25.400
You can see whether clients
currently have the volume mounted.

00:23:25.400 --> 00:23:28.690
You control when they mount
and unmount the volumes.

00:23:28.720 --> 00:23:32.700
You can do this all from
the one centralized place.

00:23:33.280 --> 00:23:37.500
You can set the user
quotas or group quotas.

00:23:37.580 --> 00:23:42.200
You can view logs on the various systems.

00:23:42.220 --> 00:23:45.510
And you can create the
folders with affinities,

00:23:45.510 --> 00:23:46.890
as I said before.

00:23:47.870 --> 00:23:53.360
So now we'll have a demo of the
various admin functionality.

00:23:54.780 --> 00:23:56.650
Good morning, everyone.

00:23:56.710 --> 00:23:59.420
You all got your Xsan
developer preview CDs,

00:23:59.440 --> 00:24:01.290
and hopefully you installed
it and tried to play with it.

00:24:01.410 --> 00:24:04.560
But unfortunately, without a SAN,
it's not terribly interesting.

00:24:04.560 --> 00:24:06.700
Now, I have a SAN here set
up for your enjoyment,

00:24:06.700 --> 00:24:10.380
and I'm going to make the lights blink,
and I'm going to make everything happy.

00:24:12.240 --> 00:24:16.560
The first thing to do when
setting up your Xsan system is to

00:24:16.560 --> 00:24:19.780
determine which computer is going
to be your metadata controller.

00:24:19.780 --> 00:24:22.280
So let's go ahead and set this guy.

00:24:22.280 --> 00:24:24.730
So we entered the serial number before,
because I don't think you want

00:24:24.730 --> 00:24:27.080
to watch me type in the serial
number on all these computers.

00:24:27.080 --> 00:24:30.960
We set the role to be controller,
and if we had multiple controllers,

00:24:31.100 --> 00:24:33.420
we could choose the failover priority.

00:24:33.420 --> 00:24:36.950
Also, since you want to be on a
private metadata network,

00:24:36.950 --> 00:24:40.760
if you have a dual-nick machine
or multiple Ethernet cards,

00:24:40.760 --> 00:24:42.220
you could choose the
role to be controller.

00:24:42.220 --> 00:24:45.070
So you can choose which interface to
access the SAN easily right there.

00:24:45.070 --> 00:24:47.670
And here's some information about
the computer to help choose which

00:24:47.830 --> 00:24:49.520
machine you want to be the controller.

00:24:49.520 --> 00:24:52.200
Next, you move on to your LUNs.

00:24:52.200 --> 00:24:54.880
All you need to do with your
LUNs is give them a name.

00:24:54.880 --> 00:25:01.260
All the information before is defined
by your RAID admin configurations.

00:25:01.260 --> 00:25:03.440
So you can just rename that there.

00:25:03.440 --> 00:25:05.860
And here's really where the fun part is.

00:25:05.920 --> 00:25:08.970
You take your storage,
and to create your storage,

00:25:08.970 --> 00:25:11.330
you need to define your first volume.

00:25:12.200 --> 00:25:14.960
So we'll go ahead and create volume,
and we'll name this WWDC volume.

00:25:14.960 --> 00:25:18.620
And you can change things like the log
size and the max number of connections

00:25:18.620 --> 00:25:21.000
you want to access to the SAN.

00:25:21.000 --> 00:25:25.170
Now, the block allocation size is an
important field to pay attention to,

00:25:25.340 --> 00:25:28.220
and it goes in power
of 2 from 4K to 512K.

00:25:28.220 --> 00:25:30.570
And that is a performance
tuning parameter that,

00:25:30.700 --> 00:25:33.690
depending on your typical IOS size,
you may need to tweak.

00:25:33.710 --> 00:25:35.950
And if you need to know more
about any of these values,

00:25:35.950 --> 00:25:38.250
and you'll see some more coming up,
we have help buttons

00:25:38.370 --> 00:25:40.460
in all of the sheets,
and it'll bring up contextual

00:25:40.460 --> 00:25:42.180
help for every single one of them.

00:25:42.180 --> 00:25:42.680
single field.

00:25:44.160 --> 00:25:48.300
We create our first storage pool,
simply the same way.

00:25:48.300 --> 00:25:50.040
Let's make that pool one.

00:25:50.040 --> 00:25:55.000
Let's say we want it to be an exclusive
metadata and journaling storage pool,

00:25:55.000 --> 00:25:57.410
so other data won't
interfere with that traffic.

00:25:57.440 --> 00:26:01.680
Stripe breadth is another
important performance tuning value.

00:26:01.680 --> 00:26:04.430
If you have multiple LUNs
in your storage pool,

00:26:04.430 --> 00:26:08.960
this is how many bytes it will write to
each LUN before moving on to the next.

00:26:10.560 --> 00:26:15.330
We don't need to change this here because
the metadata pool will only have one LUN.

00:26:15.690 --> 00:26:18.360
So now that we've done that,
we bring up our little drawer with LUNs,

00:26:18.360 --> 00:26:20.280
and I have a pre-configured LUN.

00:26:20.330 --> 00:26:21.210
This is just by naming it.

00:26:21.250 --> 00:26:22.380
That's all you have
to do to configure it.

00:26:22.720 --> 00:26:25.480
Drag, drop, there's your metadata.

00:26:25.480 --> 00:26:28.120
And actually,
we can come back here and call

00:26:28.120 --> 00:26:30.440
this MDE so you know it's metadata.

00:26:31.180 --> 00:26:33.360
Now we create another storage
pool for all of our data.

00:26:33.440 --> 00:26:36.220
Come in here, we'll call this Video Data.

00:26:36.220 --> 00:26:41.020
And we want to ensure that no journaling
and metadata spills over to interfere

00:26:41.020 --> 00:26:45.280
with our high-definition video or SD or
whatever we happen to have on here.

00:26:45.280 --> 00:26:48.770
And we can change our strike
breadth to 128 blocks.

00:26:48.780 --> 00:26:52.680
And this size here, 512K,
is how many bytes it writes.

00:26:52.680 --> 00:26:56.820
And the size here also depends
on the block allocation size

00:26:56.820 --> 00:26:58.900
you define in your volume.

00:26:58.900 --> 00:27:01.160
And you can change multi-path methods.

00:27:01.160 --> 00:27:04.960
You can change the size of your data
set and permissions and other stuff.

00:27:05.090 --> 00:27:07.270
And the help will tell
you all about that.

00:27:07.280 --> 00:27:10.070
So let's skip this disk here
because it's not the same.

00:27:10.180 --> 00:27:11.160
Drag all that in.

00:27:11.270 --> 00:27:11.990
There you go.

00:27:11.990 --> 00:27:16.080
We've configured a 6.84 terabyte,
actually 7.29 terabyte

00:27:16.080 --> 00:27:17.310
SAN in about a minute.

00:27:17.310 --> 00:27:19.260
And that's all you need to do.

00:27:26.420 --> 00:27:29.560
Okay, so we have a SAN setup
before with some files,

00:27:29.580 --> 00:27:31.770
so we're just going to revert.

00:27:31.960 --> 00:27:34.510
And you can see here we
have the metadata pool.

00:27:34.740 --> 00:27:38.000
We have a small audio pool because we
don't need as much bandwidth for audio.

00:27:38.040 --> 00:27:41.290
And we have our SD video, our high def,
and our post-production.

00:27:41.340 --> 00:27:43.930
So let's move over here.

00:27:44.070 --> 00:27:47.980
Here you can see all of our storage
pools and you can see a snapshot

00:27:47.980 --> 00:27:50.390
of the currently running volume.

00:27:50.480 --> 00:27:54.180
Each of these will fill up to show you
how full each storage pool is to know

00:27:54.180 --> 00:27:56.350
when you need to grow your storage.

00:27:56.420 --> 00:27:59.570
In the Logs tab,
you can get all the relevant logs on

00:27:59.680 --> 00:28:05.310
all of the machines on the SAN and you
can even filter for certain things.

00:28:05.390 --> 00:28:07.340
In the Clients tab,
you can mount and unmount.

00:28:07.340 --> 00:28:10.300
You can mount them all at the same
time if you really feel so inclined,

00:28:10.310 --> 00:28:11.460
or unmount at the same time.

00:28:11.460 --> 00:28:15.420
And over in Affinities,
we can set up all the affinities.

00:28:15.420 --> 00:28:19.040
And in Quotas, you can create quotas,
delete quotas, and it's really simple.

00:28:19.040 --> 00:28:22.440
You just go ahead and you drag in users,
and this is all LDAP integrated,

00:28:22.440 --> 00:28:25.840
so if you have a directory server,
you'll see all the records there.

00:28:25.840 --> 00:28:29.640
You can drag in stuff here,
set the quota, 10 megabyte soft quota,

00:28:29.640 --> 00:28:32.960
probably 10 gigabyte soft quota,
20 gig hard quota,

00:28:32.960 --> 00:28:34.700
and give them 24 hours.

00:28:35.350 --> 00:28:37.610
And then go ahead and hit save,
and it would send it out.

00:28:37.660 --> 00:28:39.750
And if you actually
had some data in here,

00:28:39.750 --> 00:28:43.260
the quota status would show how full,
how close they are to

00:28:43.290 --> 00:28:46.010
their soft or hard quota,
or if they're even

00:28:46.010 --> 00:28:47.140
above their soft quota.

00:28:47.140 --> 00:28:49.360
And that's Xsan Admin.

00:29:00.900 --> 00:29:04.570
So basically we've shown
you what the admin does.

00:29:04.570 --> 00:29:08.100
If you're familiar with Mac OS X Server,
you'll notice that it

00:29:08.140 --> 00:29:09.360
looks awfully familiar.

00:29:09.360 --> 00:29:12.900
And that's because basically
we leverage the same technology

00:29:12.900 --> 00:29:14.860
as we did for the server admin.

00:29:14.860 --> 00:29:18.200
The main difference is
in the server admin,

00:29:18.200 --> 00:29:23.940
its main goal was to connect to a
server and administer that one machine.

00:29:24.140 --> 00:29:27.390
Even though you could administer
multiple machines from the admin,

00:29:27.390 --> 00:29:31.140
each was considered to be a
sort of separate unit in the UI.

00:29:31.140 --> 00:29:37.790
The Xsan admin sort of treats the
whole SAN as a particular entity.

00:29:37.800 --> 00:29:40.630
So you saw that when
you're administering it,

00:29:40.630 --> 00:29:44.160
you're administering the
entire SAN at the same time.

00:29:44.160 --> 00:29:47.410
Basically,
the server admin agent is going to

00:29:47.410 --> 00:29:52.110
run on each of the Xsan machines,
both controllers and clients.

00:29:52.200 --> 00:29:54.090
The Xsan admin is going to
take control of the entire SAN.

00:29:54.230 --> 00:29:56.380
It's going to take care of
replicating your configuration

00:29:56.380 --> 00:29:57.960
files around between the machines.

00:29:57.960 --> 00:30:01.250
So it's particularly important
between the primary controller

00:30:01.250 --> 00:30:04.660
and backup controllers that they
have the same configuration.

00:30:04.660 --> 00:30:08.150
If the backup controller thought the
volumes were arranged differently,

00:30:08.150 --> 00:30:09.680
that wouldn't be a good idea.

00:30:09.680 --> 00:30:13.100
So it'll make sure that the
configurations are all the same.

00:30:13.100 --> 00:30:16.650
It'll be able to monitor the
status of the machine so you can

00:30:16.650 --> 00:30:20.600
quickly look up and see which
machines are currently active,

00:30:20.600 --> 00:30:22.640
which ones have volumes mounted.

00:30:23.420 --> 00:30:27.740
And it'll contact machines as
necessary to perform its functions.

00:30:27.740 --> 00:30:29.350
It's sort of behind the scenes.

00:30:29.360 --> 00:30:31.020
It establishes connections.

00:30:33.140 --> 00:30:35.820
In addition to the admin app,
we do provide a set

00:30:35.940 --> 00:30:37.580
of command line tools.

00:30:37.630 --> 00:30:41.820
As I said,
we try to keep the admin app streamlined,

00:30:41.850 --> 00:30:46.090
so in certain cases there's additional
functionality available in the

00:30:46.090 --> 00:30:50.310
command line tools that we don't
actually surface through the admin.

00:30:50.410 --> 00:30:53.900
All the tools live in one place
inside library file systems.

00:30:53.900 --> 00:30:58.360
There's an Xsan folder, and inside there,
there's a binaries folder.

00:30:58.360 --> 00:31:02.470
That's also where config
files live and other things.

00:31:02.750 --> 00:31:05.090
The tools will all be documented.

00:31:05.100 --> 00:31:08.600
However, if you look at the documentation
on the concurrent CD,

00:31:08.600 --> 00:31:09.560
they aren't there.

00:31:09.560 --> 00:31:13.910
There are actually man files, though,
on the install that you can look at.

00:31:13.940 --> 00:31:18.070
But we'll try and come up with some
better documentation for these.

00:31:21.080 --> 00:31:22.080
Here's an example of a few of these.

00:31:22.080 --> 00:31:24.460
CV Admin is sort of the main tool.

00:31:24.460 --> 00:31:27.650
It's the one we used a lot when
we were still developing the

00:31:27.650 --> 00:31:31.570
user interface because it does a
lot of the same functionalities.

00:31:31.570 --> 00:31:35.200
It's one of those sort of
interactive command line admin tools.

00:31:35.200 --> 00:31:38.980
You can start and stop the controller
and do a lot of the various functions.

00:31:40.380 --> 00:31:43.350
CV Affinity is the one that
you can use if you want finer

00:31:43.350 --> 00:31:45.180
control over the affinities.

00:31:45.180 --> 00:31:49.980
The admin only allows you to set folders
and everything in that folder is set.

00:31:51.610 --> 00:31:54.190
The folder will have
a particular affinity.

00:31:54.190 --> 00:31:57.640
If you want to set affinities on
particular files or see what the

00:31:57.690 --> 00:32:01.690
affinity on a file is currently,
you can use the CV Affinity tool.

00:32:02.850 --> 00:32:09.490
CVFS Check is the normal FS Check style
utility for the Xsan volume.

00:32:09.580 --> 00:32:12.940
Actually, if you bring up Disk Utility,
you'll be able to click on the volume

00:32:12.940 --> 00:32:15.650
and do a normal verifier repair,
and it'll call this

00:32:15.650 --> 00:32:16.920
tool behind the scenes.

00:32:16.920 --> 00:32:20.460
But if you want to have
scripts or whatever to run it,

00:32:20.460 --> 00:32:22.280
the tool is available.

00:32:23.010 --> 00:32:26.790
The final one is the Defrag
tool that was mentioned.

00:32:26.850 --> 00:32:32.180
So the Defrag tool can be used
to defragment your file data.

00:32:32.180 --> 00:32:38.610
It does have one particular
extra utility that can be useful.

00:32:39.660 --> 00:32:44.430
Sometimes during data flow stuff,
you might, for instance,

00:32:44.430 --> 00:32:49.270
ingest a file into one storage
pool because it has particular

00:32:49.380 --> 00:32:53.720
performance criteria,
but then later on you might want to

00:32:53.720 --> 00:32:58.460
access it using different RAID LUNs
so that you can ingest new files.

00:32:58.460 --> 00:33:02.990
S&FS Defrag can be used to migrate
the storage for a file from one

00:33:02.990 --> 00:33:08.460
storage pool to another without
affecting where it appears in the file.

00:33:09.290 --> 00:33:14.440
So, again, the Defrag tool can be used to
migrate the storage for a file from

00:33:14.540 --> 00:33:19.950
one storage pool to another without
affecting where it appears in the file.

00:33:22.220 --> 00:33:26.240
We mentioned the cross-platform
setup with the StoreNext file system.

00:33:26.240 --> 00:33:30.070
Just wanted to quickly go through
and sort of show how easy that is.

00:33:30.070 --> 00:33:31.840
There's two scenarios.

00:33:31.840 --> 00:33:36.360
Adding the StoreNext clients
to the Xsan system.

00:33:37.360 --> 00:33:40.360
First of all, you're going to set up
your Xsan system normally.

00:33:40.360 --> 00:33:44.360
You're going to get a license
for your StoreNext clients.

00:33:44.360 --> 00:33:46.360
That'll actually get installed
on the Xsan controller.

00:33:46.360 --> 00:33:50.230
And then you're going to just set up
your StoreNext clients the way you would

00:33:50.230 --> 00:33:52.360
normally do for a StoreNext system.

00:33:52.360 --> 00:33:55.110
There's basically some information
you just need to enter into

00:33:55.110 --> 00:33:56.350
a couple of config files.

00:33:56.360 --> 00:34:00.080
The trickier one is when you
want to add an Xsan client

00:34:00.200 --> 00:34:02.360
to a StoreNext file system.

00:34:02.360 --> 00:34:06.360
Our admin software basically is
written to administer the Xsan client.

00:34:07.360 --> 00:34:11.890
It's the entire Xsan environment,
and so it doesn't really understand

00:34:11.890 --> 00:34:17.360
a single Xsan client connecting to
some other type of SAN file system.

00:34:17.360 --> 00:34:21.360
So in this case, you're going to need to
administer the SAN manually.

00:34:21.360 --> 00:34:23.360
Luckily, it's fairly easy to do.

00:34:23.360 --> 00:34:25.770
The main thing is you have
to add your serial number

00:34:25.770 --> 00:34:27.350
manually to the config file.

00:34:27.460 --> 00:34:29.420
And then there's just
a couple other files,

00:34:29.500 --> 00:34:33.360
mainly the controller addresses to
tell it how to contact the controller.

00:34:33.360 --> 00:34:36.240
So that's all quite straightforward,
and it'll be fully documented.

00:34:37.360 --> 00:34:39.590
in the documentation.

00:34:42.030 --> 00:34:48.400
So now I want to talk a bit about sort of
how these volumes appear to applications.

00:34:48.500 --> 00:34:52.900
The first thing that is important
is that it is a shared volume.

00:34:52.900 --> 00:34:54.800
Pretty much, though,
in terms of writing and

00:34:54.890 --> 00:34:57.300
testing applications,
it's going to be just like a

00:34:57.310 --> 00:34:59.220
network file system in that way.

00:34:59.380 --> 00:35:02.850
The only issue you may run
into is you do find sometimes

00:35:02.900 --> 00:35:07.610
there are certain apps that,
because of performance considerations,

00:35:07.610 --> 00:35:10.780
aren't used to running
on network volumes.

00:35:10.780 --> 00:35:13.580
I mean,
if you've got something that basically

00:35:13.580 --> 00:35:17.700
ingests high-definition video,
there aren't that many file servers

00:35:17.790 --> 00:35:20.200
that are able to handle that bandwidth.

00:35:20.200 --> 00:35:23.710
And so it may not be used to
running on a shared file system.

00:35:23.790 --> 00:35:28.640
So it is important to make sure that
the applications are doing file locking.

00:35:29.380 --> 00:35:32.650
You can also be managed
sort of at the user level,

00:35:32.740 --> 00:35:36.980
but it's better if the application
itself does the coordination

00:35:36.980 --> 00:35:41.090
to make sure another copy isn't
going to stomp on your data.

00:35:42.610 --> 00:35:48.190
The file system supports the normal calls
that would be done through Mac OS X.

00:35:48.400 --> 00:35:52.770
You have both the file open flags,
the shared lock and exclusive lock,

00:35:52.770 --> 00:35:56.950
as well as the F set lock,
F control for doing byte range locking.

00:35:56.950 --> 00:36:01.340
This is commonly referred to
as POSIX locks and BSD locks.

00:36:01.340 --> 00:36:08.760
Also, the open deny modes in Carbon get
translated into these the same

00:36:08.760 --> 00:36:11.680
way as they would for NFS volume.

00:36:13.440 --> 00:36:15.060
The other thing to be aware of,
of course,

00:36:15.140 --> 00:36:17.260
is that these volumes are very large.

00:36:17.310 --> 00:36:20.780
I mean,
X-rayed volumes are already quite large,

00:36:20.780 --> 00:36:23.920
but the SAN volumes are going
to be built up even larger.

00:36:23.920 --> 00:36:26.480
I mean, multi-terabyte volumes,
as you saw,

00:36:26.480 --> 00:36:30.540
it's really easy to build up these big
volumes because there's a tendency to

00:36:30.540 --> 00:36:35.280
try and consolidate all your storage,
even if you've got a bunch of different

00:36:35.290 --> 00:36:37.500
RAID boxes into one big volume.

00:36:37.740 --> 00:36:41.240
So, in writing software,
that's an important consideration,

00:36:41.240 --> 00:36:45.540
that as well as having very big files,
if you have these huge volumes,

00:36:45.540 --> 00:36:48.460
you're going to have, you know,
possibly many millions

00:36:48.460 --> 00:36:49.940
of files on this volume.

00:36:49.940 --> 00:36:53.500
And so, if you're writing backup
software and so forth,

00:36:53.530 --> 00:36:57.370
you need to be aware that things
tend to get grouped into larger

00:36:57.390 --> 00:37:00.010
volumes than they may have otherwise.

00:37:00.700 --> 00:39:37.600
[Transcript missing]

00:39:38.190 --> 00:39:41.490
But the file I/O is going to
be going directly to the RAID,

00:39:41.530 --> 00:39:44.680
so in that case it shouldn't be
any different than if you had

00:39:44.680 --> 00:39:46.990
a locally mounted RAID volume.

00:39:49.790 --> 00:39:55.650
So now I'm going to talk
about a few APIs you can use.

00:39:55.820 --> 00:40:01.080
Certainly, it's expected that you'll
start to have server clusters

00:40:01.080 --> 00:40:06.360
that'll be using the Xsan,
and so server apps may want to take

00:40:06.400 --> 00:40:09.420
advantage of some of these features.

00:40:09.460 --> 00:40:15.200
Distributing computing apps
and then certainly multimedia

00:40:15.590 --> 00:40:18.410
apps is a very strong focus.

00:40:19.930 --> 00:40:22.770
So, three APIs I'm going to highlight.

00:40:22.860 --> 00:40:26.700
I'll mention a couple other minor ones,
but the extent preloading,

00:40:26.720 --> 00:40:29.580
the infinities,
and then the bandwidth reservation

00:40:29.580 --> 00:40:31.160
that Tom mentioned earlier.

00:40:31.160 --> 00:40:36.320
The APIs all use a similar mechanism.

00:40:36.340 --> 00:40:40.110
They're specific to Xsan volumes.

00:40:40.110 --> 00:40:43.660
They're going to be
accessed through syscontrol,

00:40:43.980 --> 00:40:48.360
but basically we have some sample
code that sort of helps you call it,

00:40:48.530 --> 00:40:51.990
because the actual glue
code is a bit gross.

00:40:52.000 --> 00:40:59.270
And the other thing to note is
that the API is still in flux,

00:40:59.380 --> 00:41:05.730
so we provide some sample code on the CD,
but if you compiled

00:41:05.850 --> 00:41:09.880
using that sample code,
you would need to recompile before

00:41:09.880 --> 00:41:12.610
the final shipping version comes out.

00:41:12.630 --> 00:41:13.920
So, it's there.

00:41:13.980 --> 00:41:16.500
It's there just to try out the
APIs and see how they work,

00:41:16.530 --> 00:41:20.640
but we'll be seeding final
APIs closer to the ship date.

00:41:20.640 --> 00:41:25.940
And then the last thing is,
because these are Xsan-specific APIs,

00:41:26.040 --> 00:41:29.680
you should use StataFS to
determine whether this is an

00:41:29.680 --> 00:41:31.420
Xsan volume you're talking to.

00:41:31.420 --> 00:41:34.680
Here's some easy code, basically.

00:41:34.680 --> 00:41:39.190
Just going to call StataFS on the file,
and the FS type name

00:41:39.190 --> 00:41:41.230
will be unique to Xsan.

00:41:41.240 --> 00:41:43.980
We actually have a constant in
the header that you can compare.

00:41:43.980 --> 00:41:45.810
against.

00:41:49.250 --> 00:41:54.340
So here's an example of the typical
sort of lovely SIFS control call.

00:41:54.340 --> 00:41:58.680
You've got your structure that you're
going to pass down into the kernel,

00:41:58.680 --> 00:42:01.990
and it's going to get filled
out and then passed back up.

00:42:02.080 --> 00:42:06.780
This is an easy call in that it's
just getting some version information.

00:42:07.230 --> 00:42:10.170
The other thing about this API is
you always need an open file

00:42:10.240 --> 00:42:11.750
descriptor to make the call.

00:42:12.150 --> 00:42:15.860
Obviously, in version information,
you don't really care what's not

00:42:15.960 --> 00:42:18.060
particular to an individual file.

00:42:18.060 --> 00:42:21.890
So a common thing to do is to just
open up the root directory of the

00:42:21.910 --> 00:42:24.310
file system and make a call on that.

00:42:24.390 --> 00:42:27.880
But this particular call will return
the same information no matter what

00:42:27.880 --> 00:42:32.790
file descriptor it's called against,
as long as that's for a

00:42:32.800 --> 00:42:34.980
file on an Xsan volume.

00:42:38.570 --> 00:42:44.410
So the load extents call,
the key here is when you open a file

00:42:44.480 --> 00:42:49.080
and start reading and writing it,
the file system is going to react

00:42:49.080 --> 00:42:51.380
to your calls as you make them.

00:42:51.400 --> 00:42:54.880
In my example, you saw there's the
write file system call.

00:42:54.880 --> 00:42:58.530
The file system needs to go out,
ask the metadata controller

00:42:58.530 --> 00:43:02.490
where this file lives before it
can start writing the data out.

00:43:03.560 --> 00:43:07.980
But because you have that latency in
talking to the metadata controller,

00:43:07.980 --> 00:43:11.510
you may have a hiccup in terms
of the reading and writing.

00:43:11.860 --> 00:43:16.410
The load extents call can tell this
system up front that you're going to be

00:43:16.410 --> 00:43:21.610
reading or writing these offsets for this
particular file and tell it to go ahead

00:43:21.610 --> 00:43:23.700
and get all that information up front.

00:43:23.700 --> 00:43:27.740
So when you're actually doing the I/O,
you don't have any of the latencies

00:43:27.770 --> 00:43:29.690
of talking to the controller.

00:43:32.290 --> 00:43:39.010
So the affinities,
the thing here is often you don't

00:43:39.010 --> 00:43:43.720
want the layout on the file system
to necessarily reflect where

00:43:43.720 --> 00:43:45.530
things are stored in storage pools.

00:43:45.540 --> 00:43:49.960
A common example of this is you
might have a project folder.

00:43:49.960 --> 00:43:54.250
That project folder could contain
audio files and video files,

00:43:54.250 --> 00:43:59.740
but as far as the user is concerned,
they want all these files grouped

00:43:59.740 --> 00:44:01.290
together in a single folder.

00:44:01.300 --> 00:44:05.340
But as far as the system is concerned,
you may want to store the audio

00:44:05.340 --> 00:44:08.900
files on a different storage
pool than the video files.

00:44:08.900 --> 00:44:13.030
The most efficient way,
because configuring that all sort of by

00:44:13.060 --> 00:44:17.850
hand could be a very complicated thing,
applications can take advantage

00:44:17.850 --> 00:44:22.590
of this because they know what
types of files they're saving

00:44:22.600 --> 00:44:25.600
out and what the characteristics
of those files are going to be.

00:44:30.400 --> 00:44:30.400
So we're going to have a demo of this.

00:44:30.400 --> 00:44:32.670
I have a demo of this.

00:44:34.620 --> 00:44:38.540
Alright,
so I'm going to demo Affinity Steering.

00:44:38.590 --> 00:44:40.360
So I'll open up my demo app.

00:44:40.360 --> 00:44:42.600
Sorry, I don't have an icon.

00:44:42.760 --> 00:44:49.300
So we'll create a file called my file,
and we're going to put it in the

00:44:49.300 --> 00:44:51.100
post-production storage pool.

00:44:51.100 --> 00:44:52.600
So I'm going to go ahead and start that.

00:44:53.360 --> 00:44:55.710
And you can see it's
writing at reasonable speed,

00:44:55.710 --> 00:44:58.700
so we'll get an initial burst of
speed as it fills the RAID cache,

00:44:58.770 --> 00:45:00.170
but then it will level out.

00:45:00.360 --> 00:45:05.490
And if you can see the lights over here,
you should see one LUN being pegged.

00:45:05.540 --> 00:45:10.340
Now, if we start up a second file,
say this is our video file,

00:45:10.340 --> 00:45:16.190
video file.mpeg, and we store that, say,
in high def, we start that up,

00:45:16.190 --> 00:45:22.610
we should be pegging two different LUNs,
or two different storage pools.

00:45:23.040 --> 00:45:26.900
And it should be going much faster,
which it is.

00:45:26.900 --> 00:45:28.990
Now,
if you go ahead and look in our volume,

00:45:29.060 --> 00:45:31.730
it was stored in project files,
they're both sitting

00:45:31.730 --> 00:45:33.190
right next to each other.

00:45:33.190 --> 00:45:35.030
And that is Affinity Steering.

00:45:35.040 --> 00:45:36.000
Greg?

00:45:42.550 --> 00:45:48.720
So one of the points on that demo was,
you know, basically we wanted to show the

00:45:48.720 --> 00:45:52.530
difference between the storage pools,
but we only have one fiber

00:45:52.530 --> 00:45:56.110
channel connected up to the
system and didn't really tune it.

00:45:56.180 --> 00:45:59.980
So don't take those performance
numbers as typical performance numbers,

00:45:59.980 --> 00:46:03.290
but we just wanted to show the
ways in which an application can

00:46:03.290 --> 00:46:05.210
talk to the different storage.

00:46:06.770 --> 00:46:11.290
Basically, how it's going to do that is
first it needs to find out what

00:46:11.450 --> 00:46:13.350
storage pools are available.

00:46:13.570 --> 00:46:17.580
Early on,
we called storage pools stripe groups,

00:46:17.590 --> 00:46:21.810
so that's still reflected in the API,
get SG info.

00:46:21.820 --> 00:46:24.110
It's going to give you information
about the stripe groups,

00:46:24.150 --> 00:46:28.860
so theoretically an application might be
able to do some intelligent figuring out

00:46:28.860 --> 00:46:30.980
of which stripe group it wants to use.

00:46:31.010 --> 00:46:33.230
The other thing that's
probably more common is to do

00:46:33.230 --> 00:46:36.080
what we did in the demo app,
which is basically just present a pop-up.

00:46:36.690 --> 00:46:36.700
So one of the points on that demo was,
you know, basically we wanted to show the

00:46:36.700 --> 00:46:36.700
difference between the storage pools,
but we only have one fiber

00:46:36.700 --> 00:46:36.710
channel connected up to the
system and didn't really tune it.

00:46:36.720 --> 00:46:40.720
to the user to choose which
storage pool they want.

00:46:40.720 --> 00:46:43.910
In the API,
it's also going to give you an

00:46:43.910 --> 00:46:45.720
8-byte key for that storage pool.

00:46:45.720 --> 00:46:49.720
That's what you actually
use in the setAffinity call.

00:46:49.720 --> 00:46:54.720
So, normally, you would open a file,
create the file using open,

00:46:54.720 --> 00:46:57.720
set the affinity on the file,
and then start writing it.

00:46:57.910 --> 00:47:01.720
The other thing you can do,
what we did actually in this app,

00:47:01.720 --> 00:47:04.670
was call allocExtentSpace,
which will pre-allocate

00:47:04.740 --> 00:47:08.720
the space for the file,
load the extents into the client,

00:47:08.720 --> 00:47:11.870
and then allow you to start writing out,
and that gives you the

00:47:11.870 --> 00:47:13.720
highest performance writing.

00:47:18.430 --> 00:47:22.060
So the next thing I wanted to talk
about is bandwidth reservation.

00:47:22.450 --> 00:47:27.060
Basically, I mean,
Tom described this pretty well.

00:47:27.060 --> 00:47:31.040
For the people that weren't here,
I'll try my little vague thing.

00:47:31.040 --> 00:47:34.100
Basically,
the idea here is that when you're

00:47:34.240 --> 00:47:38.980
doing a critical operation,
you can't necessarily control what other

00:47:38.980 --> 00:47:42.080
people are going to be accessing the SAN.

00:47:42.080 --> 00:47:47.140
So if you've got your ingest station,
and it's really critical that you

00:47:47.140 --> 00:47:50.460
get your high-definition video,
you know,

00:47:50.460 --> 00:47:54.880
streamed onto there without any hiccups,
you don't want somebody else coming

00:47:55.000 --> 00:47:59.160
up and just starting to stream some
other file on or off that same storage

00:47:59.160 --> 00:48:01.300
pool and mess up the bandwidth.

00:48:01.300 --> 00:48:04.320
So basically,
this is a way for applications to

00:48:04.320 --> 00:48:09.040
guarantee that they're going to get
a particular amount of bandwidth.

00:48:09.040 --> 00:48:13.690
If somebody else launches something...
If somebody else launches something where

00:48:13.690 --> 00:48:16.760
they don't care about the performance,
it'll just get scaled back.

00:48:16.960 --> 00:48:19.910
If somebody launches an
application that's also demanding

00:48:19.910 --> 00:48:23.460
the critical performance,
they'll get an error saying, "Look,

00:48:23.460 --> 00:48:25.020
this is already in use.

00:48:25.110 --> 00:48:27.730
This amount of bandwidth
has already been reserved,

00:48:27.730 --> 00:48:31.340
so there isn't enough left for you
to do your application." This is

00:48:31.370 --> 00:48:36.440
basically used for streaming,
especially real-time streaming,

00:48:36.440 --> 00:48:38.420
and it's per storage pool because,
as I said earlier, if somebody's writing

00:48:38.420 --> 00:48:39.350
to one storage pool,
it's per storage pool because,

00:48:39.350 --> 00:48:40.020
as I said earlier, if somebody's writing
to one storage pool,

00:48:40.020 --> 00:48:41.490
it's per storage pool because,
as I said earlier, if somebody's writing

00:48:41.490 --> 00:48:43.770
to one storage pool,
it doesn't affect the I/O to

00:48:43.770 --> 00:48:45.970
another storage pool anyway.

00:48:46.030 --> 00:48:50.100
So you're reserving bandwidth
on a particular storage pool,

00:48:50.100 --> 00:48:52.600
and people reading or writing
other storage pools won't be

00:48:52.600 --> 00:48:55.320
affected by the reservation.

00:48:55.860 --> 00:48:58.200
So we'll have a demo of this.

00:48:58.210 --> 00:49:00.340
All right, so we're going to use
the same application.

00:49:00.340 --> 00:49:03.240
And say we have a video
file and an audio file.

00:49:03.260 --> 00:49:06.490
And we're going off and writing
those to the same storage pool,

00:49:06.490 --> 00:49:07.260
high def.

00:49:07.260 --> 00:49:09.980
Now,
say we want 120 megabytes per second.

00:49:09.980 --> 00:49:12.580
And unfortunately,
we're sharing it at about 80 or 90

00:49:12.580 --> 00:49:15.040
megasecond over the single fiber channel.

00:49:15.040 --> 00:49:17.490
And go ahead and attempt
to reserve bandwidth here.

00:49:17.730 --> 00:49:20.560
And that will jump up while
the other one goes down.

00:49:20.570 --> 00:49:23.880
And this is being written to
the exact same storage pool.

00:49:23.890 --> 00:49:26.190
And they're still sitting
right next to each other.

00:49:26.250 --> 00:49:28.360
But one is getting more
bandwidth than the other,

00:49:28.360 --> 00:49:30.740
as much as we had configured it to need.

00:49:30.750 --> 00:49:31.830
And there you go.

00:49:31.840 --> 00:49:34.710
That is bandwidth reservation.

00:49:40.290 --> 00:49:44.940
So bandwidth reservation is the one
feature that only works if applications

00:49:44.940 --> 00:49:51.010
support it because the application has
to tell the system what file it is that

00:49:51.010 --> 00:49:56.380
they want to reserve bandwidth for and
how much bandwidth needs to be reserved.

00:49:56.630 --> 00:50:01.570
The other thing about it is that it
requires additional configuration that

00:50:01.570 --> 00:50:04.470
we don't support in the admin API.

00:50:04.630 --> 00:50:05.970
It's pretty simple.

00:50:06.060 --> 00:50:11.410
Basically, the system,
when you configure the volumes,

00:50:11.500 --> 00:50:15.180
it isn't able to determine
what the throughput to your

00:50:15.180 --> 00:50:17.720
varied storage pools is.

00:50:17.720 --> 00:50:21.250
It's a very hard thing to
determine programmatically.

00:50:21.530 --> 00:50:23.080
There's a lot of variables involved.

00:50:23.160 --> 00:50:26.880
So you just need to run a simple test,
run an app like the one we just had,

00:50:26.880 --> 00:50:29.730
find out what the throughput
to your storage pool is,

00:50:29.790 --> 00:50:33.070
and just enter that field
into the configuration file.

00:50:33.200 --> 00:50:36.210
And then it'll know, basically,
how much is able to be

00:50:36.210 --> 00:50:38.380
reserved off of that.

00:50:38.380 --> 00:50:42.860
Another important thing you can add
is to tell it how much you don't

00:50:42.860 --> 00:50:44.850
want to be able to be reserved.

00:50:44.930 --> 00:50:47.750
As you saw,
once somebody makes a reservation,

00:50:47.920 --> 00:50:51.620
the rest of the performance is
going to drop way down to allow

00:50:51.620 --> 00:50:54.230
the person to have that bandwidth.

00:50:54.240 --> 00:50:57.980
It's critical that you don't have
everybody else drop to zero and have

00:50:57.980 --> 00:51:01.860
one person reserve the entire bandwidth,
because that can lead to dead

00:51:01.860 --> 00:51:03.620
locks and other problems.

00:51:03.790 --> 00:51:07.830
So at a minimum,
the system leaves one megabyte per second

00:51:08.020 --> 00:51:14.160
free that other people can at least
do very slow I/O to that storage pool.

00:51:14.290 --> 00:51:17.350
But under certain circumstances,
you may actually want to increase that.

00:51:17.440 --> 00:51:22.770
So there's another field to determine
the non-reservable part of the bandwidth.

00:51:27.210 --> 00:51:30.280
So the call is basically
set real-time I/O.

00:51:30.280 --> 00:51:36.190
The idea is that you're going to put
the storage pool into real-time mode.

00:51:36.200 --> 00:51:43.660
That means that basically once
a client has loaded the extents,

00:51:43.790 --> 00:51:46.450
normally it's just doing the file I/O.

00:51:46.460 --> 00:51:48.700
As I said,
it usually doesn't even care whether

00:51:48.700 --> 00:51:50.750
the metadata controller is still around.

00:51:50.990 --> 00:51:52.900
It's doing its file I/O.

00:51:52.900 --> 00:51:54.430
It's happy.

00:51:54.470 --> 00:51:58.650
But once you've put the storage
pool into real-time mode,

00:51:58.700 --> 00:52:03.320
it goes out and tells all the clients
that are using the storage pool that

00:52:03.320 --> 00:52:06.090
they now need to make requests for I/O.

00:52:06.200 --> 00:52:09.910
So each client will
then ask the controller,

00:52:09.910 --> 00:52:14.790
say basically,
"I want to do I/O to this storage pool."

00:52:14.820 --> 00:52:19.810
The controller will give it a token,
allowing it to do a particular amount

00:52:19.810 --> 00:52:23.250
of I/O for a certain time slice,
depending on how many

00:52:23.290 --> 00:52:24.440
clients are asking for I/O.

00:52:24.440 --> 00:52:29.090
It'll parcel out different amounts
of I/O to the different clients,

00:52:29.100 --> 00:52:36.100
and it'll balance that sort of
dynamically as time goes on.

00:52:36.170 --> 00:52:41.340
The important thing is that the person
who's reserving the bandwidth is,

00:52:41.340 --> 00:52:44.010
when they make the call,
they're going to specify a file

00:52:44.010 --> 00:52:48.300
descriptor that's going to be used for
the performance-critical operation.

00:52:48.370 --> 00:52:52.170
And that file descriptor,
it will not be limited.

00:52:52.180 --> 00:52:54.300
You'll be able to make
reads and writes freely.

00:52:54.300 --> 00:52:55.420
It's going to be a file descriptor
that's going to be used for the

00:52:55.420 --> 00:52:56.260
performance-critical operation.

00:52:56.260 --> 00:52:59.300
There's actually another
call you can make.

00:52:59.360 --> 00:53:02.350
If you want to have
multiple file descriptors,

00:53:02.350 --> 00:53:07.320
you can make another call to enable
multiple file descriptors to be ungated.

00:53:11.170 --> 00:53:15.100
That's basically my session.

00:53:15.120 --> 00:53:20.580
You saw how Xsan can allow you to
configure your LUNs together in a much

00:53:20.720 --> 00:53:26.560
more flexible way and get the performance
to all of them out to various clients.

00:53:27.080 --> 00:53:32.480
The important points I want to make
are that it is a shared file system.

00:53:32.480 --> 00:53:36.290
That's a very important thing that
applications need to be aware of.

00:53:36.350 --> 00:53:40.980
And that there are these APIs available
to add additional value to applications

00:53:40.980 --> 00:53:43.160
if you're running on an Xsan system.

00:53:43.160 --> 00:53:48.000
And then I think we will have Q&A.

00:53:48.000 --> 00:53:51.350
Oh, well, more information.

00:53:51.580 --> 00:53:58.310
Basically, these are the documents that
are available on the CD.

00:53:58.410 --> 00:54:02.890
And then I think Tom is going
to come back up for Q&A,

00:54:03.080 --> 00:54:04.160
or Eric.