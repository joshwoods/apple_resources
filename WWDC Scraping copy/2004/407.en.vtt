WEBVTT

00:00:13.010 --> 00:00:14.000
Good afternoon.

00:00:14.000 --> 00:00:16.790
This is session 407,
the Accelerate Framework.

00:00:17.070 --> 00:00:20.730
The Accelerate Framework was introduced
at last year's developers conference.

00:00:20.760 --> 00:00:26.250
This year we'll rehash a little bit
about what was introduced last year,

00:00:26.250 --> 00:00:28.380
but also give you part two.

00:00:28.560 --> 00:00:32.340
If you know anything about Ali's team,
you know these are the guys who,

00:00:32.410 --> 00:00:34.950
in your calculus class,
were the ones finishing the

00:00:34.950 --> 00:00:37.710
word problems or the homework
before the class was even over.

00:00:37.770 --> 00:00:38.670
They love math.

00:00:38.890 --> 00:00:40.940
So I hope you guys enjoy this session.

00:00:41.130 --> 00:00:43.580
And with that, I'd like to introduce Dr.

00:00:43.580 --> 00:00:44.480
Ali Sazegari.

00:00:52.000 --> 00:00:55.200
First, thank you very much for
coming to our session.

00:00:55.200 --> 00:01:01.180
And what I'd like to talk to you about
is the Accelerate framework and what we

00:01:01.240 --> 00:01:05.380
have done and we plan on doing for Tiger.

00:01:05.380 --> 00:01:08.120
The talk is in three parts.

00:01:08.120 --> 00:01:11.500
I'm going to give you a general
overview and some snippets

00:01:11.500 --> 00:01:13.140
of results that we have.

00:01:13.140 --> 00:01:16.020
And after that,
I'm going to pass it on to my colleague,

00:01:16.020 --> 00:01:20.870
Ian Oldman, and he's going to talk about
the image processing library,

00:01:20.870 --> 00:01:24.480
which was introduced last
year in the Panther OS.

00:01:24.480 --> 00:01:27.050
And after that, I'm going to pass it
to my other colleague,

00:01:27.050 --> 00:01:33.170
Steve, and he's going to talk more
about the numerics and the linear

00:01:33.180 --> 00:01:36.940
algebra results that we have.

00:01:36.940 --> 00:01:38.970
So let's get started.

00:01:40.330 --> 00:01:44.470
So as you know, we have had this
particular configuration,

00:01:44.470 --> 00:01:47.660
the AXLRATE framework,
which is a collection of all

00:01:47.660 --> 00:01:52.520
the computational underpinnings
of the Mac OS X in Panther.

00:01:52.520 --> 00:01:55.830
We've had the Veclip
section of it for a while.

00:01:55.930 --> 00:01:57.960
Last year we introduced Vimage.

00:01:58.050 --> 00:02:01.800
Veclip section had the signal processing,
the linear algebra,

00:02:01.890 --> 00:02:06.320
the matrix computations, the BLAS,
the large number computations

00:02:06.320 --> 00:02:10.140
and the math libraries
which took hardware vectors,

00:02:10.140 --> 00:02:11.480
28 bit vectors.

00:02:11.480 --> 00:02:12.450
We added image processing.

00:02:12.480 --> 00:02:15.660
I'm happy to tell you a lot of people
are using our image processing inside

00:02:15.660 --> 00:02:21.020
Apple and outside and we're going to
talk a little bit more about that.

00:02:21.020 --> 00:02:26.540
One of the additions to the new
operating system is V-force.

00:02:26.540 --> 00:02:30.160
V-force,
we've had a lot of calls for people who

00:02:30.220 --> 00:02:35.420
wanted to use an array of elements and
pass it on to the elementary functions

00:02:35.420 --> 00:02:39.970
and get the elementary function results,
not just pass it on in hardware vectors.

00:02:39.980 --> 00:02:42.800
Hardware vectors are just
one scalar at a time.

00:02:42.800 --> 00:02:46.650
V-force is our new library which
we will talk in depth later on.

00:02:46.660 --> 00:02:48.900
Steve will talk about that.

00:02:50.220 --> 00:02:54.840
What is delivered in Mac OS X Tiger?

00:02:55.240 --> 00:02:58.260
Basically,
the Accelerate Framework is one-stop

00:02:58.270 --> 00:03:03.500
shopping for computational performance,
digital signal processing.

00:03:03.500 --> 00:03:05.360
We have expanded that in Tiger.

00:03:05.450 --> 00:03:10.860
We will have about 340 new
functions in the VDSP subframework.

00:03:10.860 --> 00:03:13.510
We have digital image processing.

00:03:13.510 --> 00:03:17.990
We have expanded that also and with
added performance in some of the

00:03:18.050 --> 00:03:19.790
core routines such as convolution.

00:03:19.870 --> 00:03:23.030
The BLAS Level 1, 2, and 3,
if you're familiar with that,

00:03:23.030 --> 00:03:25.640
these are the basic linear
algebra subroutines.

00:03:25.720 --> 00:03:30.860
Again, these are the structures of
computations that people do for LAPACK.

00:03:30.860 --> 00:03:34.900
The entire LAPACK that's
single and double,

00:03:34.900 --> 00:03:38.860
real and complex for all of the routines.

00:03:38.860 --> 00:03:40.860
Basically, this is the exact API that we
use for all of the routines.

00:03:40.860 --> 00:03:42.860
This is the API that people who
are using LAPACK are used to.

00:03:42.860 --> 00:03:46.850
VForce, the array LMs that we'll talk a
little bit more about that today,

00:03:46.890 --> 00:03:48.860
and that's new in Tiger.

00:03:48.860 --> 00:03:52.780
VMathLib,
the counterpart of regular LibM,

00:03:52.780 --> 00:03:54.860
which runs on Scalar.

00:03:54.860 --> 00:03:58.190
This one here runs on the Vector Engine.

00:03:59.040 --> 00:04:03.300
I'm going to just touch on some of
the performance improvements and

00:04:03.300 --> 00:04:09.070
performances that we have right
now in TIGER in some of the CDs,

00:04:09.080 --> 00:04:10.240
the CDs that you've had.

00:04:10.310 --> 00:04:14.100
First, I'd like to talk about
vFORCE performance.

00:04:14.100 --> 00:04:15.400
These are DLMs.

00:04:15.510 --> 00:04:17.000
We have single and double precisions.

00:04:17.000 --> 00:04:18.700
They're highly accurate.

00:04:18.700 --> 00:04:22.040
They operate on arrays instead
of elements or hardware vectors,

00:04:22.050 --> 00:04:23.990
128-bit hardware vectors.

00:04:24.680 --> 00:04:27.770
Monotonicity is observed over
the entire range of definitions.

00:04:27.780 --> 00:04:32.200
That's pretty important because
there are competitors which do

00:04:32.200 --> 00:04:36.680
have functionality such as this,
but they have cut lots of corners

00:04:36.680 --> 00:04:40.880
and developers have to worry about
pitfalls of where to call or what

00:04:40.880 --> 00:04:43.140
not to call or what elements to call.

00:04:43.140 --> 00:04:44.930
Here, you're free to call anything.

00:04:44.930 --> 00:04:47.250
Basically,
if it's in the floating point domain,

00:04:47.280 --> 00:04:49.900
it will work, and it will not trip you,
and it will not give

00:04:49.900 --> 00:04:51.180
you the wrong results.

00:04:51.180 --> 00:04:56.000
I have a small little table here showing
you what the... benefits of vFORCE are.

00:04:56.000 --> 00:05:00.020
I'm quite proud of this particular
piece of work that our group has done,

00:05:00.020 --> 00:05:02.400
and Steve will talk about
this a little bit further.

00:05:02.400 --> 00:05:09.570
Square root is 3.5, 3.1 times,
over 3 times faster than the current one.

00:05:09.590 --> 00:05:14.020
Exponential is over 6 times,
and sine is 11 times faster.

00:05:14.020 --> 00:05:18.980
Square root was already
pretty fast on G5s,

00:05:18.980 --> 00:05:22.920
but we have it even faster on this.

00:05:22.920 --> 00:05:27.290
The reason that these things... are
faster is that we are able to plug

00:05:27.380 --> 00:05:32.300
in the bubbles in the computational
structure of the algorithms because

00:05:32.300 --> 00:05:35.830
regular elementary functions just
don't have enough data to go through,

00:05:35.850 --> 00:05:39.830
and you end up having a lot
of empty cycles going by.

00:05:39.840 --> 00:05:44.680
This allows it to fill it up completely
and have stellar performance.

00:05:44.680 --> 00:05:48.380
The next thing I'd like to talk to
you about is the LAPAC performance.

00:05:48.450 --> 00:05:51.600
LINPAC, this is a lot of people
know about these results.

00:05:51.680 --> 00:05:53.700
I just have a little bit of it here.

00:05:53.700 --> 00:05:54.660
The DLP1000.

00:05:54.680 --> 00:06:00.200
This is the double precision
LINPAC 1000 by 1000 matrix,

00:06:00.220 --> 00:06:03.320
and we're above 5 gigaflops
for double precision,

00:06:03.320 --> 00:06:09.040
and single precision
is over 7.5 gigaflops.

00:06:09.040 --> 00:06:12.500
This is on a 2.5 gigahertz power PC.

00:06:15.100 --> 00:06:17.540
Blas performance.

00:06:17.540 --> 00:06:21.800
The quintessential Blas performance
benchmark is really DGIM,

00:06:21.800 --> 00:06:26.440
and that's the double generalized
matrix-matrix multiply,

00:06:26.440 --> 00:06:30.580
which is an enhanced matrix multiply
of a scalar times a matrix times a

00:06:30.580 --> 00:06:32.940
matrix plus a scalar times a matrix.

00:06:32.940 --> 00:06:36.680
And a lot of people like to look at that
to see the prowess of the implementation.

00:06:36.680 --> 00:06:44.310
And what we have here is comparing that
with Optron because I get asked how

00:06:44.310 --> 00:06:46.980
we compare with the competition here.

00:06:46.980 --> 00:06:51.390
Higher numbers are better
on this particular one here.

00:06:51.910 --> 00:06:58.600
Double precision size 5,500,
so if you multiplied 5,500 size matrices,

00:06:58.600 --> 00:07:02.900
you will have a 12.8 on a matrix.

00:07:03.120 --> 00:07:06.470
PowerPC and over 7
gigaflops for the Optron.

00:07:06.470 --> 00:07:11.810
Now the Optron that we had on our
hand was a 2.0 gigahertz machine.

00:07:11.810 --> 00:07:14.260
We just added 20% more to it.

00:07:14.320 --> 00:07:20.640
We were unable to get a hold of a live
2.4 gigahertz machine to run that stuff.

00:07:20.710 --> 00:07:22.310
We gave them exactly 20% up.

00:07:22.420 --> 00:07:26.490
Generally, frequency goes up that much,
performance doesn't go up that much,

00:07:26.630 --> 00:07:27.150
but we did.

00:07:27.150 --> 00:07:29.680
So it's 12.8 versus 8.55.

00:07:29.680 --> 00:07:32.690
What I have for graders
here is just for fun.

00:07:32.820 --> 00:07:35.430
What would the S-GEM performance be?

00:07:35.430 --> 00:07:39.320
And the S-GEM performance on
our machine is 23 gigaflops.

00:07:39.320 --> 00:07:41.040
I don't know,
some of you know I've been in

00:07:41.100 --> 00:07:45.050
competition for a while now,
and 20 gigaflops before required

00:07:45.050 --> 00:07:48.320
many millions of dollars to achieve.

00:07:48.320 --> 00:07:51.320
But 23 gigaflops is just a pittance now.

00:07:51.320 --> 00:07:57.240
You can buy yourself a PowerPC at
2.5 gigahertz and get that.

00:07:57.750 --> 00:08:02.570
VDSP Performance, our FFT,
we continue having a stellar collection

00:08:02.670 --> 00:08:05.540
of FFTs for our users to use.

00:08:05.540 --> 00:08:09.960
Single and double precision,
real and complex, 1D and 2D,

00:08:09.960 --> 00:08:14.760
in place and out of place basically,
and Radix 2, 3, and 5.

00:08:14.760 --> 00:08:17.320
We have them hand-tuned
for the vector engine,

00:08:17.340 --> 00:08:20.340
and we also have it hand-tuned
for the dual scaler.

00:08:20.340 --> 00:08:26.910
I have them for,
I'm comparing them with 3.2 GHz Xeons.

00:08:27.330 --> 00:08:29.220
This time around,
we're not looking at the gigaflops,

00:08:29.340 --> 00:08:31.620
we're looking at timing,
real microseconds,

00:08:31.620 --> 00:08:35.190
because signal processors really
don't care very much about what the

00:08:35.260 --> 00:08:36.320
throughput of floating point is.

00:08:36.340 --> 00:08:39.050
They like to,
because they're real-time folks,

00:08:39.050 --> 00:08:42.010
they like to find out exactly
what timing is on that.

00:08:42.320 --> 00:08:46.800
Single precision 1024 complex,
which is always etched into my mind,

00:08:46.800 --> 00:08:50.040
is 4.56 microseconds versus 6.3.

00:08:50.390 --> 00:08:54.930
The FFT is only 1.13 on a 3.2 GHz Xeon,
and these are one processor only,

00:08:54.990 --> 00:08:58.000
because the floating point,
the FFT and floating point,

00:08:58.040 --> 00:09:01.340
it doesn't do that much work
to dole it out to processors.

00:09:01.480 --> 00:09:06.950
Single precision 1024 real is 2.3
microseconds versus 4.27 microseconds.

00:09:06.950 --> 00:09:09.270
So, you would think this is fast enough.

00:09:09.270 --> 00:09:11.840
Why would you like to
make this any faster?

00:09:11.970 --> 00:09:15.100
Just one example,
the quintessential example that

00:09:15.100 --> 00:09:17.170
I like to give always is iTunes.

00:09:17.170 --> 00:09:20.240
iTunes uses our FFT at a tune of 1.2 MHz.

00:09:20.240 --> 00:09:25.160
It's that real FFT that gets used.

00:09:25.270 --> 00:09:28.870
The more we shave off of that,
the faster your decoding

00:09:28.870 --> 00:09:32.560
and encoding will go,
and the more we shave off

00:09:32.640 --> 00:09:36.240
of computational things
like FFTs and IMDCTs,

00:09:36.240 --> 00:09:38.240
the better your battery life will be.

00:09:38.240 --> 00:09:43.140
So, this is pretty darn important to make
sure it always runs extremely fast.

00:09:44.700 --> 00:09:46.640
Image Processing Library.

00:09:46.640 --> 00:09:49.170
I'm very, very proud of this
particular set of work.

00:09:49.220 --> 00:09:53.150
We set out and worked on this for
a year on Panther and delivered it.

00:09:53.310 --> 00:10:00.980
And it's used in a lot of applications
that we have in-house and outside.

00:10:01.500 --> 00:10:04.600
I just have a couple of
little things in here.

00:10:04.600 --> 00:10:09.190
We have planar and chunky – kind of a
funny word – ARGB interleaved formats,

00:10:09.220 --> 00:10:12.830
native support for 8-bit
and floating-point samples,

00:10:12.830 --> 00:10:14.620
can be used in real-time.

00:10:14.620 --> 00:10:18.880
It's multi-threaded so that
large images you can do better.

00:10:18.880 --> 00:10:21.680
I have a small table
here for performance,

00:10:21.790 --> 00:10:25.170
and what we have is
8-gigabyte image blurring.

00:10:25.170 --> 00:10:28.220
I'm comparing that to the IPP,
which is the Intel Integrated

00:10:28.260 --> 00:10:31.480
Performance Primitives that
some of you might have heard of.

00:10:31.480 --> 00:10:36.870
The 8-gigabyte image
blur is 5.5 times faster.

00:10:36.870 --> 00:10:42.510
The 8-gigabyte image
emboss is 2.2 times faster.

00:10:44.000 --> 00:10:46.370
Delivered in Mac OS X,
also let's not forget

00:10:46.490 --> 00:10:50.120
the underpinnings of,
again, regular computation.

00:10:50.120 --> 00:10:56.900
LibM – we are standards-conforming
APIs for IEEE 754 and C99.

00:10:56.900 --> 00:10:57.720
Single and double.

00:10:57.720 --> 00:11:02.050
Nguyen Tiger is our long-lost
128-bit long-double friend,

00:11:02.050 --> 00:11:06.680
which is going to make
an appearance again.

00:11:06.680 --> 00:11:10.680
We have really stellar
implementation for that – very,

00:11:10.680 --> 00:11:13.220
very accurate computationally.

00:11:13.220 --> 00:11:16.220
All of these guys are numerically robust,
highly accurate,

00:11:16.220 --> 00:11:24.120
worry about the environmental
controls and never mess up anything.

00:11:24.120 --> 00:11:29.900
We take a lot of care to make sure that
we conform to any existing standards.

00:11:29.970 --> 00:11:32.720
Best-of-breed algorithms, basically.

00:11:32.720 --> 00:11:36.240
Coding to LibM in C – straightforward.

00:11:36.240 --> 00:11:38.800
You just call the compiler.

00:11:38.800 --> 00:11:42.440
You don't have to say "-lm" on that.

00:11:42.440 --> 00:11:47.680
Using the Accelerate Framework
in C – it's also straightforward.

00:11:47.680 --> 00:11:51.600
All you need to do is just put
in "-framework" "accelerate".

00:11:51.600 --> 00:11:56.770
So basically what I've done here
in the last few minutes is to just

00:11:56.770 --> 00:12:01.780
give you a small sampling of what
we have in the image processing,

00:12:01.910 --> 00:12:05.980
signal processing, BLAS, vforce,
and LAPAC.

00:12:05.980 --> 00:12:11.620
And we're going to go into some of the
details of this work as we go along.

00:12:11.730 --> 00:12:16.010
Now I'd like to pass this on
to my colleague Ian Ullman,

00:12:16.150 --> 00:12:19.480
who is going to talk more
about the image processing.

00:12:24.610 --> 00:12:25.500
Thank you.

00:12:25.500 --> 00:12:30.730
vImage was introduced last year
at WWDC and shipped with Panther,

00:12:30.730 --> 00:12:33.620
and since then we've gotten
a lot of feedback on it.

00:12:33.630 --> 00:12:36.930
And we've taken your
suggestions to heart,

00:12:36.930 --> 00:12:41.060
and so we've got more
improvements for it now.

00:12:41.060 --> 00:12:46.180
vImage functionality remains
much as introduced previously

00:12:46.180 --> 00:12:47.860
with some new features added on.

00:12:47.860 --> 00:12:50.890
We still have native support for
8-bit and floating-point samples.

00:12:50.890 --> 00:12:54.380
These can be arranged either in a planar,
which is to say all

00:12:54.450 --> 00:12:57.140
one channel per array,
or a chunky format,

00:12:57.140 --> 00:12:58.980
which would interleave several channels.

00:12:58.980 --> 00:13:05.380
If you're doing 8-bit work and images,
then we throw in saturated clipping,

00:13:05.380 --> 00:13:08.670
usually the ends of functions that can
overflow so you don't get the white goes

00:13:08.670 --> 00:13:10.660
to black or black goes to white problem.

00:13:10.660 --> 00:13:14.650
We've put in a lot of effort,
thinking the design over to make sure

00:13:14.650 --> 00:13:20.260
that you can use these things real-time,
that we don't arbitrarily call malloc.

00:13:20.260 --> 00:13:22.690
We give you the opportunity to
provide us with the temporary

00:13:22.690 --> 00:13:24.580
buffers so you won't block on that,
that kind of thing.

00:13:24.580 --> 00:13:28.020
We're also re-entrant so you can call
us in a multi-threaded environment,

00:13:28.020 --> 00:13:33.300
and of course it's high performance,
accelerated for alt-evac.

00:13:34.000 --> 00:16:12.300
[Transcript missing]

00:16:12.820 --> 00:16:16.690
We also do morphology operations
where you can do kind of different

00:16:16.690 --> 00:16:18.440
shape changing operations,
that kind of thing.

00:16:18.440 --> 00:16:21.470
So here would be an example where
we've got a nice picture except

00:16:21.530 --> 00:16:24.460
for what looks like a power
line up in the top left corner.

00:16:24.460 --> 00:16:26.280
Wouldn't it be nice if
we could remove that?

00:16:26.330 --> 00:16:27.950
Well, there's lots of ways,
but we'll just use

00:16:27.950 --> 00:16:29.160
morphology for this example.

00:16:29.160 --> 00:16:32.240
And so we can apply a max filter.

00:16:32.240 --> 00:16:35.080
And max will go around and look at
all the pixels around this pixel in

00:16:35.080 --> 00:16:36.880
question and take the maximum value.

00:16:36.880 --> 00:16:38.460
So the power line is
kind of a dark image.

00:16:38.460 --> 00:16:41.180
So as we apply the max filter,
it just goes away.

00:16:41.920 --> 00:16:44.100
But you notice that some of the
white highlights got bigger.

00:16:44.100 --> 00:16:49.440
So we can apply a min filter and
kind of subtract them back out again.

00:16:49.440 --> 00:16:52.490
And so you have something that looks
like your original image back except now

00:16:52.490 --> 00:16:54.270
that the power line is completely gone.

00:16:54.280 --> 00:16:58.680
So you can do these for interesting
effects in addition to just shape

00:16:58.680 --> 00:17:01.020
changing and that kind of thing.

00:17:01.020 --> 00:17:03.190
So here's performance on that.

00:17:03.200 --> 00:17:07.460
We've got a new algorithm for max,
which works substantially better.

00:17:07.460 --> 00:17:11.020
Here you can see the 3.2 gigahertz
dual processor Xeon results.

00:17:11.140 --> 00:17:13.140
Again, normalized to one is the
red line across the bottom.

00:17:13.140 --> 00:17:16.380
And as the kernel size gets larger,
you can see our performance relative

00:17:16.380 --> 00:17:18.000
to Xeon gets better and better.

00:17:18.000 --> 00:17:20.620
And we're up to four times
faster for really large filters.

00:17:22.740 --> 00:17:25.340
We do alpha compositing.

00:17:25.340 --> 00:17:28.880
We can support either premultiplied
images or non-premultiplied images.

00:17:28.880 --> 00:17:31.260
We have functions to
premultiply/unpremultiply data.

00:17:31.260 --> 00:17:33.920
We've now added a few
new functions for Tiger.

00:17:34.050 --> 00:17:37.370
You can mix non-premultiplied
into a premultiplied layer,

00:17:37.390 --> 00:17:41.700
which allows you to do multiple
stacks as you go along.

00:17:41.700 --> 00:17:44.920
And we added in compositing
with a scalar fade value,

00:17:44.920 --> 00:17:49.230
which allows you to sort of fade in
the whole image without going through

00:17:49.230 --> 00:17:51.510
and writing over the alpha channel.

00:17:51.700 --> 00:17:53.700
So those will be available.

00:17:53.700 --> 00:17:56.700
We also have new type
conversion features.

00:17:56.700 --> 00:17:58.700
This was actually surprising,
at least to us.

00:17:58.700 --> 00:17:59.700
The number one requested feature.

00:17:59.700 --> 00:18:03.690
It seems that everybody has their own
data format that they like to use.

00:18:03.700 --> 00:18:06.940
So we've got a lot of conversions
to get that in and out of

00:18:06.940 --> 00:18:08.700
what vImage likes to use.

00:18:08.700 --> 00:18:13.700
So we now can handle 24-bit,
8-bit per channel color.

00:18:13.700 --> 00:18:20.700
Also the older ARGB 1555 and
RGB 565 16-bit per pixel formats.

00:18:20.700 --> 00:18:26.700
We do also the 16-bit per channel integer
support and signed and unsigned flavors.

00:18:26.700 --> 00:18:29.150
And we've also introduced
OpenEXR compliance 16-bit floating

00:18:29.220 --> 00:18:31.100
point conversion functions
in case you need to work with

00:18:31.210 --> 00:18:32.700
video cards that use those things.

00:18:32.700 --> 00:18:38.730
Also added a few other things that allow
you to insert channels into interleaved

00:18:38.730 --> 00:18:41.700
images or permute channels around.

00:18:41.700 --> 00:18:44.100
Like let's say you need to
swap around an ARGB image to

00:18:44.100 --> 00:18:45.690
an RGBA or something like that.

00:18:45.700 --> 00:18:46.970
So those things will be there.

00:18:46.990 --> 00:18:49.700
They'll be fully vectorized and
they're pretty much operated.

00:18:49.700 --> 00:18:54.510
Bandwidth limited rates.

00:18:54.700 --> 00:18:57.580
Also added color space transforms.

00:18:57.700 --> 00:19:00.700
We originally didn't put these
in because we thought we would

00:19:00.700 --> 00:19:01.690
leave these up to ColorSync.

00:19:01.700 --> 00:19:04.700
But now ColorSync wants to use our codes.

00:19:04.700 --> 00:19:05.700
So we have them in there.

00:19:05.700 --> 00:19:07.680
We have matrix multiplication.

00:19:07.700 --> 00:19:12.640
So saturated clipping for 8-bit,
of course, to prevent overflow.

00:19:12.710 --> 00:19:15.700
We allow you to put in an
optional pre and post bias.

00:19:15.700 --> 00:19:17.360
Mathematically,
the pre and post bias are the same,

00:19:17.360 --> 00:19:18.700
but it's a little easier to use that way.

00:19:18.700 --> 00:19:20.700
So we put that feature in.

00:19:20.700 --> 00:19:24.600
And again, like the convolution,
this one only does work

00:19:24.600 --> 00:19:25.700
for non-zero elements.

00:19:25.700 --> 00:19:29.890
So you can safely pass this rather
sparse matrix and we'll just do the work

00:19:29.890 --> 00:19:31.700
that we need to in order to do that.

00:19:31.700 --> 00:19:34.680
We're also introducing a whole
set of gamma correction functions.

00:19:34.700 --> 00:19:37.670
These come in a variety of flavors.

00:19:37.710 --> 00:19:39.700
You can get a generic power curve.

00:19:39.700 --> 00:19:42.690
We also provide a few
specialty gammas like SRGB,

00:19:42.700 --> 00:19:44.700
which aren't exactly generic power.

00:19:44.700 --> 00:19:47.700
These are available in
two different formats.

00:19:47.700 --> 00:19:53.630
They're generally floating point geared,
but you can get them in either a full

00:19:53.630 --> 00:19:56.700
24-bit or 12-bit precision variance.

00:19:56.710 --> 00:20:00.550
And the 12-bit precision obviously
is appropriate for data that was

00:20:00.580 --> 00:20:02.700
8-bit integer data to begin with.

00:20:02.700 --> 00:20:05.890
We also have a few functions to
do simultaneous 8-bit conversion

00:20:05.900 --> 00:20:09.700
with clipping while they're
doing the gamma correction.

00:20:09.700 --> 00:20:14.090
And we also provide an interpolated
lookup table stuff for cases where

00:20:14.090 --> 00:20:16.700
your gamma curve is not nicely
described by a power function.

00:20:16.710 --> 00:20:18.950
by a power function.

00:20:19.300 --> 00:20:22.280
So I'd like to invite
Steve Peters up to talk about the

00:20:22.300 --> 00:20:25.590
numerics improvements for Tiger.

00:20:35.400 --> 00:20:38.220
I'm going to take some time
this afternoon to present the

00:20:38.220 --> 00:20:40.160
credentials of our math libraries.

00:20:40.160 --> 00:20:43.560
Perhaps some of you have not used
them before and would like to know

00:20:43.560 --> 00:20:47.390
a bit about the motivation and also
spend some time with performance.

00:20:47.400 --> 00:20:49.050
Hey, it works.

00:20:49.050 --> 00:20:50.400
Excellent.

00:20:50.400 --> 00:20:54.960
So, job number one for us is conformance
to make porting your applications,

00:20:54.960 --> 00:20:59.020
building your applications,
correspond to experience you've

00:20:59.020 --> 00:21:03.400
learned on other platforms,
learned in the classroom,

00:21:03.400 --> 00:21:04.400
learned from reading the standards.

00:21:04.400 --> 00:21:05.400
Who does that anymore?

00:21:05.400 --> 00:21:10.150
And at the base, we have a…

00:21:10.340 --> 00:21:16.010
We're delivering platforms based on G3,
G4, G5 chips, all of which have IEEE 754

00:21:16.030 --> 00:21:20.410
compliant floating-point arithmetic,
both single and double.

00:21:22.390 --> 00:21:25.340
When we move up one level
to the elementary functions,

00:21:25.340 --> 00:21:28.610
the basic math libraries,
these are also compliant

00:21:28.610 --> 00:21:30.590
with the C99 standard.

00:21:30.730 --> 00:21:36.860
All the required C99 APIs are present
for complex and long double as well

00:21:36.860 --> 00:21:39.700
as we come into the tiger world.

00:21:42.100 --> 00:21:46.220
I'm going to have to use these.

00:21:46.220 --> 00:21:50.480
We build our linear algebra, the BLAS,
the basic linear algebra subroutines,

00:21:50.590 --> 00:21:53.670
from Atlas,
the widely respected open source

00:21:53.670 --> 00:21:58.100
package that is automatically
tuned linear algebra software.

00:21:58.100 --> 00:22:05.340
We offer the full
panoply of APIs in float,

00:22:05.340 --> 00:22:05.450
double, complex, complex double.

00:22:05.880 --> 00:22:11.820
And similarly for the gold
standard of numerical computing,

00:22:11.930 --> 00:22:16.200
LAPACK, all routines float double,
complex, complex double,

00:22:16.220 --> 00:22:19.430
entry points for both C and Fortran.

00:22:24.770 --> 00:22:29.830
After conformance,
we're really concerned with performance.

00:22:29.830 --> 00:22:35.690
And the flagship of performance now
at Apple is the marvelous G5 CPU,

00:22:35.690 --> 00:22:41.590
the PowerPPC 970,
which offers dual floating-point cores.

00:22:41.670 --> 00:22:44.810
My recollection,
the first in Apple's line,

00:22:44.810 --> 00:22:49.030
and has given us really
stellar performance.

00:22:49.240 --> 00:22:54.650
So on each 970 CPU, we find two

00:22:54.820 --> 00:22:59.220
Floating Point Cores capable
of doing double precision IEEE,

00:22:59.220 --> 00:23:00.900
single precision IEEE.

00:23:02.120 --> 00:23:05.360
On any machine cycle,
both of those units can

00:23:05.370 --> 00:23:06.400
be pressed into action.

00:23:06.400 --> 00:23:10.630
We can start a floating point
instruction down each pipe on

00:23:10.630 --> 00:23:13.130
both pipes in a single cycle.

00:23:15.250 --> 00:23:17.680
All the basic arithmetic
operations – add,

00:23:17.680 --> 00:23:20.570
multiply, subtract,
and divide – are present.

00:23:20.650 --> 00:23:23.810
We also get hardware square
root in the PowerPC 970.

00:23:23.810 --> 00:23:25.240
That's a real boon to us.

00:23:29.200 --> 00:23:34.630
And another class of instructions that
have been present in G4 and now as well

00:23:34.630 --> 00:23:36.930
in G5 called the Fused Multiply Add.

00:23:37.040 --> 00:23:43.880
Fused Multiply Add takes three operands,
multiplies the first two together,

00:23:43.960 --> 00:23:49.790
adds it to the third,
all in the course of one instruction.

00:23:49.790 --> 00:23:49.800
So this ends up

00:23:50.140 --> 00:23:54.690
Being a key, important operation
fundamental to linear algebra,

00:23:54.790 --> 00:23:58.680
the dot product, which is essentially
multiply and accumulate,

00:23:58.760 --> 00:24:01.000
multiply, accumulate, multiply,
accumulate.

00:24:01.000 --> 00:24:04.700
It's fundamental to the
FFT in much the same way.

00:24:05.060 --> 00:24:08.700
If you're doing a function evaluation by,
say, polynomial approximation,

00:24:08.700 --> 00:24:10.500
you'll probably want
to use Horner's Rule.

00:24:10.500 --> 00:24:14.420
And if you think a little bit about
the way Horner's Rule works out,

00:24:14.430 --> 00:24:17.580
it's essentially a fuse, multiply, add,
win.

00:24:18.130 --> 00:24:21.090
And at the bottom line,
we get to count two floating point

00:24:21.090 --> 00:24:23.220
operations per fuse multiply add.

00:24:23.220 --> 00:24:26.580
So on a machine with two
floating point cores,

00:24:26.580 --> 00:24:28.550
we get four flops per cycle.

00:24:29.850 --> 00:24:32.190
So let's see, four flops per cycle.

00:24:32.360 --> 00:24:33.470
I always have to do this in my head.

00:24:33.590 --> 00:24:37.560
Four flops per cycle,
two CPUs in the dual G5,

00:24:37.560 --> 00:24:42.100
so that's eight flops across two CPUs,
and we clock them at two gigahertz.

00:24:42.180 --> 00:24:47.170
So we top out at 16 double-precision
floating-point operations,

00:24:47.200 --> 00:24:53.080
16 gigaflops worth of floating-point
operations on a two gigahertz G5.

00:24:53.200 --> 00:24:56.200
And now that we're using 2.5s,
I have to update my thinking.

00:24:56.200 --> 00:24:57.030
It's 20.

00:24:57.600 --> 00:24:58.810
20 gigaflops.

00:24:58.860 --> 00:25:01.990
Theoretical peak.

00:25:01.990 --> 00:25:01.990
Theoretical peak.

00:25:06.900 --> 00:25:09.760
So how do you get to this performance?

00:25:09.760 --> 00:25:12.230
How do you get to this great
double-precision performance?

00:25:12.380 --> 00:25:17.250
If you've got an existing
Apple Mac OS X binary,

00:25:17.400 --> 00:25:21.290
perhaps built for G4,
just bring it across.

00:25:21.430 --> 00:25:23.560
The scheduling in the
CPU is really smart.

00:25:23.600 --> 00:25:26.130
As the instruction stream
comes along and we start seeing

00:25:26.240 --> 00:25:29.450
floating-point instructions,
they get dispatched off to dual CPUs,

00:25:29.450 --> 00:25:32.620
and they will finish faster than
if they were sent to a single pipe.

00:25:33.640 --> 00:25:35.730
So part of the answer is you
don't have to do anything,

00:25:35.730 --> 00:25:38.900
and you should see some performance
in existing binary apps.

00:25:40.700 --> 00:25:42.690
Second,
if you're able to recompile your app

00:25:42.690 --> 00:25:46.760
– say it's an open-source application,
a code you've developed

00:25:46.760 --> 00:25:51.000
– recompile with GCC,
set the proper options that I'll

00:25:51.000 --> 00:25:56.060
point to in a tech note later,
and let it schedule instructions in

00:25:56.060 --> 00:26:02.010
an even more optimal way for the G5,
and you can see yet more gains.

00:26:03.100 --> 00:26:06.470
It's also possible by paying
special attention to algorithmic

00:26:06.480 --> 00:26:10.440
details to get even further gains.

00:26:10.440 --> 00:26:12.340
For example,
if you're computing a rational

00:26:12.390 --> 00:26:15.370
function approximation,
you may be able to arrange the

00:26:15.370 --> 00:26:19.390
calculation so that the numerator
is computed simultaneously with

00:26:19.460 --> 00:26:23.040
the denominator on the two pipes,
and at the end you just weld

00:26:23.230 --> 00:26:25.020
them together with the divide.

00:26:25.020 --> 00:26:28.140
This level of attention
we've paid already to LibM,

00:26:28.140 --> 00:26:32.790
the basic math library, our BLAS,
our LAPACK, and the vForce library.

00:26:40.260 --> 00:26:45.850
Both our G4 and G5 platforms offer
the altivec single instruction

00:26:45.870 --> 00:26:48.260
multiple data processor.

00:26:48.260 --> 00:26:51.200
This is a four-way parallel
single precision engine.

00:26:51.200 --> 00:26:52.200
Doesn't do double precision.

00:26:52.200 --> 00:26:53.200
Not at all.

00:26:53.200 --> 00:26:54.200
Ian keeps telling me this.

00:26:54.200 --> 00:26:55.200
It'll never do double precision.

00:26:55.210 --> 00:27:00.320
It's a single precision engine with
a huge appetite for floating point.

00:27:00.330 --> 00:27:03.080
It really just rips through
floating point calculations.

00:27:03.080 --> 00:27:08.580
All the basic operations are present,
as well as a vector fused multiply add.

00:27:08.580 --> 00:27:13.410
So now we get two flops counted for
the fused multiply add on four operands

00:27:13.490 --> 00:27:17.450
strung across the 128-bit vector.

00:27:17.520 --> 00:27:20.760
That gives us eight flops per cycle.

00:27:20.760 --> 00:27:21.760
Let's see.

00:27:21.760 --> 00:27:25.920
Can I do the math in my
head for a 2.5 gigahertz G5?

00:27:25.920 --> 00:27:28.760
I think that tops out at 40 gigaflops.

00:27:28.760 --> 00:27:29.760
Thank you.

00:27:29.760 --> 00:27:30.760
Yes.

00:27:30.760 --> 00:27:31.750
40 gigaflops tops.

00:27:31.760 --> 00:27:32.670
All right.

00:27:37.300 --> 00:27:39.040
So how do you get to this performance?

00:27:39.040 --> 00:27:41.960
Well, sorry,
you've got to do a little bit of work.

00:27:42.130 --> 00:27:51.080
You're going to have to learn a
little bit about vector programming.

00:27:51.080 --> 00:27:51.080
There's an out that we've
announced this week.

00:27:51.530 --> 00:27:57.570
But it helps to get in
there with your code,

00:27:57.570 --> 00:27:57.570
understand where there's
inherent parallelism

00:27:58.740 --> 00:28:03.170
In your algorithms,
work those over with the SIMD instruction

00:28:03.170 --> 00:28:06.700
set and pass them through the compiler.

00:28:06.700 --> 00:28:09.400
Our advice is always profile
first before you dig in.

00:28:09.530 --> 00:28:14.480
Find out where the 10% of the code is,
where you're spending 90% of your time,

00:28:14.480 --> 00:28:15.450
and go look at those.

00:28:15.460 --> 00:28:29.090
Shark is a wonderful tool
for figuring out these cases.

00:28:29.090 --> 00:28:29.090
I hope you've seen Shark or plan to
see a Shark Talk sometime this week.

00:28:29.090 --> 00:28:29.090
They're playing in a theater near you,
I'm sure.

00:28:29.840 --> 00:28:33.710
Auto vectorization is an option,
and this slide was actually written

00:28:34.300 --> 00:28:37.380
before the announcement was made
that GCC 3.5 will be offering

00:28:37.460 --> 00:28:39.330
some auto vectorization features.

00:28:39.490 --> 00:28:40.270
Check those out.

00:28:40.300 --> 00:28:48.420
It may be a real boon to getting better
use of the SIMD unit on the G4s and G5s.

00:28:48.650 --> 00:28:53.160
There's also a third-party application
called Vaas that can analyze,

00:28:53.160 --> 00:28:59.300
I think, Fortran codes to discover
inherent parallelism and

00:28:59.300 --> 00:29:02.670
emit the proper Altevec code.

00:29:03.470 --> 00:29:09.470
We've gone through at Apple and
paid this kind of attention,

00:29:09.470 --> 00:29:09.470
algorithmic attention,

00:29:11.110 --> 00:29:14.680
Recasting algorithms
for our vForce library,

00:29:14.950 --> 00:29:18.140
our single-precision BLAS,
our single-precision FFTs and

00:29:18.140 --> 00:29:25.070
digital signal processing algorithms,
and heavily in vImage.

00:29:30.520 --> 00:29:34.500
Well, when you come to our platform
as a developer and kind of

00:29:34.510 --> 00:29:38.850
come to that final step,
you know, how do I access these

00:29:38.880 --> 00:29:42.640
wonderful libraries,
link, load, and go,

00:29:42.640 --> 00:29:45.620
we try to make that as
straightforward as possible.

00:29:45.620 --> 00:29:49.420
The library APIs generally
will internally dispatch

00:29:49.430 --> 00:29:53.220
for the correct platform,
so we won't go off and try to

00:29:53.220 --> 00:29:58.180
execute code that's appropriate
for a G5 on a machine that's a G3,

00:29:58.180 --> 00:29:59.440
for example.

00:30:00.400 --> 00:30:01.560
Thank you.

00:30:02.500 --> 00:30:08.670
Generally, the rule is if the API uses a

00:30:09.370 --> 00:30:15.440
The Accelerate Framework is a powerful
suite of platform-optimized libraries

00:30:15.440 --> 00:30:21.450
designed to provide high performance
and extensions to these libraries.

00:30:21.450 --> 00:30:21.450
The Accelerate Framework
takes full advantage of G3,

00:30:21.450 --> 00:30:21.450
G4, and G5 – processors to unlock
best-of-class performance

00:30:21.450 --> 00:30:21.450
in your application.

00:30:21.450 --> 00:30:21.450
View this session to learn
how the Accelerate Framework

00:30:21.450 --> 00:30:21.450
can take your application's
performance to the next level.

00:30:21.450 --> 00:30:21.450
Ali Sazegari, Ian Ollmann, Robert Murley,
Steve Peters

00:30:22.070 --> 00:30:24.670
Libm links by default.

00:30:24.720 --> 00:30:25.760
It's part of Libsystem.

00:30:25.760 --> 00:30:27.700
You don't need to say
anything about that.

00:30:27.700 --> 00:30:35.000
For our long, double, and complex APIs,
please add -lmx to your link line.

00:30:35.180 --> 00:30:42.910
And for vForce, the BLAS, LAPACK, VDSP,
and vImage, the one-stop shopping place

00:30:43.030 --> 00:30:45.670
is framework-accelerate.

00:30:45.670 --> 00:30:47.340
Just add -framework-accelerate
to your compile and link lines.

00:30:50.500 --> 00:30:58.000
I know that's a popular flag,
so I'll let you copy that down.

00:30:58.000 --> 00:30:59.400
Well what's new for Math and Tiger?

00:30:59.400 --> 00:31:00.990
What have we been working on?

00:31:01.000 --> 00:31:05.700
Ali hit the highlights
of the vForce library.

00:31:05.700 --> 00:31:06.940
Basically,
we've been told people don't want

00:31:06.940 --> 00:31:09.660
to do one square root at a time.

00:31:09.660 --> 00:31:13.820
They'd really like to do 768 at a time.

00:31:13.820 --> 00:31:16.540
And sure enough,
there are advantages to be had when you

00:31:16.610 --> 00:31:18.900
can do many of these things at once.

00:31:18.900 --> 00:31:23.190
We also took a BLAS update,
an update to Atlas 3.6.

00:31:23.190 --> 00:31:26.080
This helped us in a couple of places.

00:31:26.080 --> 00:31:29.450
We of course do additional
Mac OS X specific tune-ups

00:31:29.450 --> 00:31:33.280
to that open source drop,
and our compiler technology

00:31:33.280 --> 00:31:36.580
improved – thank you,
compiler team – to give

00:31:36.650 --> 00:31:40.120
us some nice gains,
and somewhat unexpected gains.

00:31:43.750 --> 00:31:47.490
And because of the
faster underlying BLAS,

00:31:47.540 --> 00:31:51.130
some improved compilation,
our LAPACK is going faster too.

00:31:52.900 --> 00:31:57.020
Now, Ali always likes me to lead
with the strongest graph,

00:31:57.060 --> 00:32:01.180
so I can give you a couple
of performance numbers here.

00:32:01.750 --> 00:32:07.070
These are some numbers I collected
for the 2.5 GHz G5 dual processor.

00:32:07.160 --> 00:32:11.350
It's a set of numbers that in the
sort of computational linear algebra

00:32:11.350 --> 00:32:13.700
community you'll see quite a bit.

00:32:13.810 --> 00:32:21.360
It measures matrix-multiplied DMM and
then the three decompositions,

00:32:21.370 --> 00:32:26.910
LU and the symmetric decompositions,
LL transpose Koleski and the kraut,

00:32:26.910 --> 00:32:26.910
U transpose U.

00:32:27.630 --> 00:32:33.050
For matrix multiply,
we use various matrix sizes ranging

00:32:33.050 --> 00:32:37.680
from 500 up to roughly 9,000.

00:32:37.680 --> 00:32:43.720
We get our first plateau a bit over 11
gigaflops and then an interesting jump

00:32:43.720 --> 00:32:50.900
around size 5,000 as we push up beyond
the 12 into the 13 gigaflop range.

00:32:50.900 --> 00:32:53.660
The decompositions are
a little bit less jumpy,

00:32:53.660 --> 00:32:58.210
a little less of a step function,
but look like they're hitting an

00:32:58.240 --> 00:33:01.390
asymptote at around 10 gigaflops.

00:33:02.670 --> 00:33:05.400
Well,
what's the competition up to these days?

00:33:05.510 --> 00:33:08.990
Let's just look at matrix multiply.

00:33:09.580 --> 00:33:20.120
Again in yellow is the dual 2.5
GHz G5 topping out at or above 12 GHz.

00:33:20.940 --> 00:33:26.140
On the bottom in blue is Opteron,
a 2.0 GHz Opteron.

00:33:26.180 --> 00:33:32.480
And they sort of get to about
7 GigaFLOPS in the 2.0 model.

00:33:33.800 --> 00:33:36.670
For the purposes of comparison,
we know that they've got

00:33:36.740 --> 00:33:39.220
a 2.4 GHz part out there,
and if they were allowed

00:33:39.340 --> 00:33:42.780
to perfectly scale,
they'd hit that dashed white line and

00:33:42.780 --> 00:33:45.440
come in just a bit over 8 GigaFLOPS.

00:33:45.440 --> 00:33:48.510
We expect to see that when
we measure those machines.

00:33:49.080 --> 00:33:54.680
Dual 3.2 GHz Xeon is the green.

00:33:54.680 --> 00:33:57.330
Gets up a little bit above 10,
probably touches 11 in a

00:33:57.400 --> 00:33:58.000
couple of those places.

00:33:58.000 --> 00:34:05.950
So 2.5 GHz G5 seems to dominate in the
matrix-multiply game quite handily.

00:34:13.450 --> 00:34:18.410
The slide is a bit more busy, but again,
the color should be the guide here.

00:34:18.610 --> 00:34:20.960
Yellow, again, G5.

00:34:20.960 --> 00:34:24.180
Green is a Xeon and Opteron in blue.

00:34:24.450 --> 00:34:29.400
And again, we've scaled Opteron by 20%
for the white dashed line.

00:34:29.520 --> 00:34:33.680
G5 seems to dominate again.

00:34:38.620 --> 00:34:41.600
This looks a little bit out of place.

00:34:41.640 --> 00:34:42.740
I mentioned we did long double.

00:34:42.740 --> 00:34:44.220
I think Ali mentioned we did long double.

00:34:44.220 --> 00:34:47.000
And we'll also have the
type generic math function,

00:34:47.000 --> 00:34:48.440
so that's good to know.

00:34:48.750 --> 00:34:52.480
So I want to come back
to this vforce business.

00:34:53.070 --> 00:34:57.170
And as Ali alluded to,
the elementary functions

00:34:57.170 --> 00:35:00.400
in LibM – square root,
cos, sine,

00:35:00.400 --> 00:35:05.940
arcsine – pass a single operand,
do a fairly heavy amount of computation,

00:35:06.000 --> 00:35:08.190
and burp out a single result.

00:35:08.290 --> 00:35:13.560
It turns out that leaves bubbles
in the modern RISC pipelines.

00:35:13.560 --> 00:35:17.950
So we say these C99
APIs are data-starved.

00:35:18.580 --> 00:35:24.010
We're also required by IEEE 754
to have very careful control over

00:35:24.010 --> 00:35:27.970
the rounding modes and exceptions
that might be generated in the

00:35:28.000 --> 00:35:29.760
course of such a computation.

00:35:29.760 --> 00:35:31.940
And that adds a fair amount of overhead.

00:35:32.120 --> 00:35:39.390
There are instructions that
we'll have to synchronize the

00:35:39.390 --> 00:35:39.420
pipe to get that stuff right,
and we pay a pretty good price for that.

00:35:40.520 --> 00:35:45.220
So the ideas in V-force are let's pass
many operands through a single call.

00:35:45.320 --> 00:35:47.400
Maybe we can get some advantage there.

00:35:47.500 --> 00:35:53.580
So if we had 768 values in a vector x and
we wanted to compute the single precision

00:35:53.580 --> 00:35:57.230
floating point sine of those things,
we could call VV sine f,

00:35:57.230 --> 00:36:01.050
pass x 768 in a place
to stuff the answers y,

00:36:01.080 --> 00:36:05.160
or we might have 117 numbers
we want the arc tan of,

00:36:05.160 --> 00:36:08.110
and there's a call for that.

00:36:08.650 --> 00:36:12.280
We're going to insist on the
IEEE default rounding modes,

00:36:12.540 --> 00:36:15.200
and we're not going to
set any exception flags.

00:36:15.200 --> 00:36:19.240
So this is for, you know,
close to the metal, high performance,

00:36:19.240 --> 00:36:25.340
go as fast as you can,
we don't expect any big problems,

00:36:25.340 --> 00:36:29.590
and if there are any, well,
we'll deal with them in some other

00:36:29.590 --> 00:36:29.590
manner than the IEEE approach.

00:36:33.530 --> 00:36:38.420
So we also get some mileage here
because given multiple operands,

00:36:38.620 --> 00:36:42.640
we can pack them together into hardware
vectors on the single precision side and

00:36:42.640 --> 00:36:44.340
send them through the AltaVec engine.

00:36:44.380 --> 00:36:46.770
This is a very good thing.

00:36:46.920 --> 00:36:50.710
Similarly, on the G5,
we can make sure to utilize the two

00:36:50.710 --> 00:36:53.270
pipes as effectively as possible.

00:36:55.120 --> 00:36:58.860
We do a lot of software pipelining
that is sort of arranging,

00:36:59.050 --> 00:37:03.340
let's just say arranging to
fill all the available cycles

00:37:03.340 --> 00:37:06.520
on all the floating point pipes.

00:37:06.520 --> 00:37:10.450
We unroll loops like crazy,
and we also have taken some

00:37:10.800 --> 00:37:15.990
algorithmic approaches that favor
calculation over table lookup and try

00:37:15.990 --> 00:37:18.800
to avoid branches like the plague.

00:37:19.460 --> 00:37:21.630
It makes these things go very, very,
very fast.

00:37:21.630 --> 00:37:26.250
And as Ali pointed out,
we have gains in square root to 3x,

00:37:26.390 --> 00:37:32.710
x to nearly 7, and sine was almost 12,
3612.

00:37:34.870 --> 00:37:36.340
So, some caveats, right?

00:37:36.350 --> 00:37:38.460
This is close to the metal programming.

00:37:38.520 --> 00:37:41.070
Generally,
the results are as accurate as LibM,

00:37:41.070 --> 00:37:43.070
but they're not bitwise identical.

00:37:43.170 --> 00:37:49.960
Don't expect to call and compare
for equality on a list of arguments.

00:37:50.490 --> 00:37:53.970
We handle almost all the edge
cases according to C99 and

00:37:53.970 --> 00:37:56.000
XG for the special functions.

00:37:56.000 --> 00:38:00.840
The exceptions are a few places
around signed zeros – what

00:38:00.890 --> 00:38:04.600
happens when plus or minus zero is
passed to one of these routines.

00:38:04.600 --> 00:38:08.070
We make no alignment requirements,
although you will get best performance

00:38:08.070 --> 00:38:13.330
if you can 16-byte align your data.

00:38:13.370 --> 00:38:18.040
Storage returned by malloc is on
Mac OS X by default 16-byte aligned.

00:38:19.690 --> 00:38:21.060
This stuff is tuned for the G5.

00:38:21.110 --> 00:38:22.500
I mean,
that's the performance flagship here.

00:38:22.500 --> 00:38:26.350
But the good news is it runs
quite nicely on G4 and G3.

00:38:26.440 --> 00:38:28.940
And of course, we dispatch internally to
the appropriate routine.

00:38:28.940 --> 00:38:33.000
You don't need to worry about where
you're running vForce routines.

00:38:33.000 --> 00:38:35.080
They just do the right thing.

00:38:38.240 --> 00:38:43.040
So one final change of gears
here is to come back to the

00:38:43.040 --> 00:38:50.380
elementary functions themselves,
where we've done a bit of tune-up work.

00:38:50.380 --> 00:38:56.400
Here are a selected sample of the
probably most used and most loved

00:38:56.400 --> 00:39:00.130
elementary functions in our library.

00:39:00.140 --> 00:39:06.660
We report the number of G5 cycles
on a random selection over a wide

00:39:06.660 --> 00:39:12.130
range of arguments and averaged
over the number of iterations,

00:39:12.190 --> 00:39:15.500
square root taking about
35 cycles per element,

00:39:15.500 --> 00:39:18.900
sine 52, and so forth.

00:39:19.960 --> 00:39:26.390
If you look at what the competition
publishes for the performance of x87,

00:39:26.570 --> 00:39:30.660
These are essentially hardware
implementations of these

00:39:30.660 --> 00:39:32.620
transcendental functions.

00:39:32.810 --> 00:39:34.940
Their square root runs at about 38.

00:39:35.090 --> 00:39:38.270
Their exponential,
depending on how you want to count,

00:39:38.350 --> 00:39:44.140
runs no less than 150 cycles
to do the 2 to the x part,

00:39:44.140 --> 00:39:47.750
and there's a bit of
massaging to get e to the x.

00:39:47.850 --> 00:39:53.520
Their logarithm is a winner,
and otherwise we get

00:39:53.520 --> 00:39:55.490
all the wins in yellow.

00:39:58.470 --> 00:40:00.780
Now those are just sort
of raw x87 numbers.

00:40:00.800 --> 00:40:05.120
When you actually package these
things into a library that take

00:40:05.500 --> 00:40:10.520
account of rounding requirements and
error flags such as in GNU/Linux,

00:40:10.520 --> 00:40:13.500
the performance falls off a bit more.

00:40:13.940 --> 00:40:17.310
These G5 numbers are already
in the prescribed IEEE –

00:40:17.310 --> 00:40:20.500
in compliance with IEEE,
so there's nothing further to say.

00:40:20.500 --> 00:40:22.000
That is LibM.

00:40:22.000 --> 00:40:24.200
That's GNU Linux on Intel.

00:40:26.390 --> 00:40:30.380
The Accelerate Framework is a powerful
suite of platform-optimized libraries

00:40:30.400 --> 00:40:33.530
designed to provide high performance,
high performance,

00:40:33.530 --> 00:40:33.590
and high performance in your application.

00:40:33.590 --> 00:40:33.590
View this session to learn
how the Accelerate Framework

00:40:33.590 --> 00:40:33.590
can take your application's
performance to the next level.

00:40:33.590 --> 00:40:33.590
The Accelerate Framework
takes full advantage of G3,

00:40:33.590 --> 00:40:33.590
G5, and G5 – processors to unlock
best-of-class performance

00:40:33.590 --> 00:40:33.590
in your application.

00:40:33.590 --> 00:40:33.590
View this session to learn
how the Accelerate Framework

00:40:33.590 --> 00:40:33.590
can take your application's
performance to the next level.

00:40:33.820 --> 00:40:35.450
Raw elementary function performance.

00:40:35.460 --> 00:40:38.190
I think G5 wins,
but I work on that stuff, so.

00:40:40.230 --> 00:40:43.300
There are some notes in
our technical library:

00:40:43.300 --> 00:40:57.370
Techno 2086, tuning for the G5,
Techno 2087,

00:40:57.370 --> 00:40:57.370
a quick look at the G4 and G5 if you're
familiar with programming for G4,

00:40:57.370 --> 00:40:57.370
that will get you bumped
up to G5 in a hurry.

00:41:02.400 --> 00:41:08.500
I see some note-takers
finishing up on that.

00:41:08.500 --> 00:41:12.860
And some really nice documentation
in the developer reference library

00:41:13.390 --> 00:41:17.590
for the Accelerate Framework and
some of its individual components,

00:41:17.590 --> 00:41:24.560
vImage, vDSP, and a piece that Ian mainly
maintains on the Velocity

00:41:24.570 --> 00:41:29.020
Engine that's sort of a wonderful,
general, gentle introduction to

00:41:29.180 --> 00:41:30.180
Simdy Programming.

00:41:30.230 --> 00:41:31.100
Is there such a thing, Bob?

00:41:31.100 --> 00:41:31.800
I don't know.

00:41:31.840 --> 00:41:34.110
That's a good point.