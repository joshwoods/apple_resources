WEBVTT

00:00:12.130 --> 00:00:13.100
Good morning.

00:00:13.100 --> 00:00:16.100
Welcome to session 200,
Core Audio in Depth.

00:00:16.100 --> 00:00:18.100
Please welcome your
digital audio plumber,

00:00:18.100 --> 00:00:20.090
James McCartney.

00:00:27.600 --> 00:00:28.000
Test?

00:00:28.000 --> 00:00:29.040
OK.

00:00:29.520 --> 00:00:35.440
OK, I'm going to talk about new features
for Tiger in the audio toolbox.

00:00:35.810 --> 00:00:39.080
First, the audio converter.

00:00:39.610 --> 00:00:45.000
First of all, I guess I'll introduce
what's going to happen.

00:00:45.040 --> 00:00:47.410
I'm going to speak first,
then Doug Wyatt will speak about

00:00:47.410 --> 00:00:55.940
some other new features for Tiger,
and then Bill Stewart will speak.

00:00:56.970 --> 00:01:01.600
So the audio converter is a-- what is it?

00:01:01.600 --> 00:01:08.160
It's an API for Core Audio that
allows you to convert different

00:01:08.160 --> 00:01:10.940
audio formats from one to another.

00:01:11.090 --> 00:01:15.600
It does floating point
integer conversion,

00:01:15.620 --> 00:01:19.400
interleaving and deinterleaving channels,
sample rate conversion,

00:01:19.420 --> 00:01:23.300
channel reordering,
and encoding and decoding

00:01:23.300 --> 00:01:25.660
compressed formats.

00:01:25.660 --> 00:01:30.340
And any codec that's installed
on the system can be used by the

00:01:30.340 --> 00:01:33.730
audio converter to convert data.

00:01:34.110 --> 00:01:35.800
The API looks like this.

00:01:35.800 --> 00:01:37.480
These are the calls.

00:01:37.480 --> 00:01:40.960
You create a new audio converter
using Audio Converter New.

00:01:40.960 --> 00:01:47.230
And you set properties like
setting the bit rate for encoding.

00:01:47.240 --> 00:01:51.870
And then you use Audio Converter
Fill Complex Buffer to--

00:01:52.120 --> 00:01:57.140
to fill your buffers
with the converted data,

00:01:57.140 --> 00:02:04.310
and you pass an input data proc for
getting the input data to the converter.

00:02:04.320 --> 00:02:07.620
It's a full model.

00:02:07.640 --> 00:02:12.560
And then if you want to stop a
stream and restart the converter,

00:02:12.740 --> 00:02:18.400
you'd use Audio Converter Reset to flush
out all the buffers and start over again.

00:02:18.420 --> 00:02:24.290
And then Audio Converter Dispose
will dispose the converter.

00:02:24.680 --> 00:02:32.280
So new in Tiger is a new call
called Audio Converter New Specific,

00:02:32.340 --> 00:02:37.150
and that allows you to choose a
specific manufacturer's codec.

00:02:37.240 --> 00:02:39.630
By default,
when you create a new audio converter,

00:02:39.700 --> 00:02:41.970
you get the default codec
for a certain format.

00:02:41.980 --> 00:02:48.440
So this allows you to specifically
choose a certain manufacturer's codec.

00:02:49.960 --> 00:02:55.220
You can see the difference between
Audio Converter New Specific

00:02:55.220 --> 00:02:58.820
and Audio Converter New is that
there's an array of audio class

00:02:59.340 --> 00:03:01.500
descriptions that are passed in.

00:03:01.500 --> 00:03:06.610
And you pass in a list of
audio class descriptions,

00:03:06.610 --> 00:03:12.740
and it will choose the earliest
matching codec in the list.

00:03:15.470 --> 00:03:20.780
So you would put the codec
manufacturers in the order that

00:03:20.900 --> 00:03:22.440
you preferred that they matched.

00:03:22.460 --> 00:03:26.950
The audio class description
looks like this.

00:03:27.040 --> 00:03:30.200
There's three fields that
are four character codes.

00:03:30.260 --> 00:03:36.910
The first is the type,
which is either ADEC or

00:03:36.980 --> 00:03:41.740
AINC for decoder and encoder.

00:03:41.940 --> 00:03:47.000
And then the second one, Subtype,
should match the format

00:03:47.000 --> 00:03:48.940
ID code for the format.

00:03:48.940 --> 00:03:59.090
So like AAC space for AAC or .mp3 for
mp3 or whatever the format type is,

00:03:59.100 --> 00:03:59.760
IMA4.

00:03:59.760 --> 00:04:03.720
And then the last four-chart
code is the manufacturer code.

00:04:03.720 --> 00:04:08.720
So for Apple's AAC decoder,
it would look as shown here.

00:04:08.720 --> 00:04:11.020
The type is ADEC for decoder.

00:04:11.900 --> 00:04:19.460
Subtype is AAC space for AAC and
then APPL for Apple manufacturer.

00:04:19.850 --> 00:04:27.130
Okay, another new feature for
the Audio Converter is the

00:04:27.150 --> 00:04:29.040
Audio Converter property settings.

00:04:29.480 --> 00:04:37.540
This is a CFArray that you get back from
the audio converter that contains the

00:04:37.920 --> 00:04:40.560
parameter settings for the converter.

00:04:40.570 --> 00:04:44.490
And you can use that for building
a user interface for the audio

00:04:44.570 --> 00:04:52.500
converter or configuring another audio
converter similarly to the current

00:04:52.500 --> 00:04:57.620
one without having to run through and
do all the parameter settings again.

00:04:57.620 --> 00:04:59.700
The property settings.

00:04:59.820 --> 00:05:09.450
So the format of the CF object is
the CFArray of CFDictionary's for

00:05:09.550 --> 00:05:12.100
each converter that's in
the audio converter chain.

00:05:12.100 --> 00:05:15.290
An audio converter,
when it converts audio,

00:05:15.290 --> 00:05:20.780
may have several components inside of it,
like a float-to-integer conversion

00:05:20.800 --> 00:05:25.090
or sample rate converters or
interleavers and deinterleavers

00:05:25.120 --> 00:05:27.210
or encoders or decoders.

00:05:27.830 --> 00:05:34.260
And so each subconverter that has
property settings in the chain will have

00:05:34.260 --> 00:05:41.490
a CFDictionary that contains all the
property settings for that converter.

00:05:42.190 --> 00:05:48.810
So each converter dictionary is a
CFDictionary and it has several keys.

00:05:48.810 --> 00:05:52.150
The first is converter,
which is a non-localized,

00:05:52.150 --> 00:05:56.110
the first key is converter and
then the value for that key is the

00:05:56.260 --> 00:05:58.980
non-localized name of the converter.

00:05:58.980 --> 00:06:01.930
This is like a key that
you would use for matching.

00:06:02.000 --> 00:06:06.530
Like if you want to find this sample rate
converter in the audio converter chain,

00:06:06.530 --> 00:06:09.750
you would go through and look
for sample rate converter and

00:06:09.790 --> 00:06:11.700
that will always be the same.

00:06:12.250 --> 00:06:15.960
And then name, the key name,
the value for that is the

00:06:15.960 --> 00:06:18.580
localized name of the converter.

00:06:18.580 --> 00:06:24.100
So that would change depending
on the local language.

00:06:24.100 --> 00:06:29.050
And then parameters is a
key that is a dictionary.

00:06:29.050 --> 00:06:34.100
It was an array of dictionaries
for each parameter.

00:06:34.100 --> 00:06:41.060
So each parameter of the audio converter
will have a dictionary that describes it.

00:06:42.100 --> 00:06:44.090
And so the parameter
dictionaries look like this.

00:06:44.090 --> 00:06:52.610
There's a key,
the key key is the non-localized

00:06:52.700 --> 00:06:56.880
name of the parameter,
which you use for matching again.

00:06:57.190 --> 00:07:01.600
And then name is the localized name of
the parameter for displaying to the user.

00:07:01.700 --> 00:07:07.990
And then available values is all the
possible values for that parameter.

00:07:08.020 --> 00:07:10.250
So if it's sample rates,
it would be a list of the

00:07:10.250 --> 00:07:11.900
sample rates that are supported.

00:07:11.900 --> 00:07:15.140
Or if it's bit rates,
it would be the list of the

00:07:15.140 --> 00:07:16.090
bit rates that are supported.

00:07:16.100 --> 00:07:18.400
That would be all the bit
rates supported for a codec.

00:07:18.610 --> 00:07:22.660
And then limited values would be
the ones that would be enabled at

00:07:22.660 --> 00:07:28.500
the current time for the current
settings of the converter or codec.

00:07:28.500 --> 00:07:35.680
So if you have the codec set up for,
like in AAC's case,

00:07:35.820 --> 00:07:41.700
for a large number of channels,
then you can't choose lower bit rates.

00:07:41.720 --> 00:07:46.700
So those values would not appear
in the limited values list.

00:07:46.700 --> 00:07:50.330
And then current value
contains the current value.

00:07:50.330 --> 00:07:55.030
It's an index into the
list of available values.

00:07:55.200 --> 00:08:02.690
And then hint is a Boolean which tells
you if it's a normal or expert parameter.

00:08:02.700 --> 00:08:08.700
And then summary is a little
help string for the parameter.

00:08:08.700 --> 00:08:11.500
So if someone moused over
it on the user interface,

00:08:11.500 --> 00:08:16.270
interface, you could show that to the
user to describe the parameter.

00:08:16.860 --> 00:08:22.710
Okay,
so this is what a dictionary looks like.

00:08:23.050 --> 00:08:26.370
This would be for an audio converter that
just has a sample rate converter in it.

00:08:26.460 --> 00:08:31.050
So there's,
at the top level it's a CFArray,

00:08:31.050 --> 00:08:34.940
there's one entry in it,
it's a CFDictionary that contains the

00:08:34.940 --> 00:08:39.440
parameters for the sample rate converter,
or contains the keys for

00:08:39.540 --> 00:08:41.800
the sample rate converter.

00:08:41.800 --> 00:08:44.580
So the first key is converter,
which is sample rate converter,

00:08:44.580 --> 00:08:46.520
that's what you could use for matching.

00:08:46.880 --> 00:08:47.800
You can find it.

00:08:47.800 --> 00:08:50.620
The name is sample rate converter,
that will change depending

00:08:50.620 --> 00:08:52.060
on the local language.

00:08:52.350 --> 00:08:54.590
And then you have a parameter array.

00:08:54.590 --> 00:08:58.700
So there's two parameters for
the sample rate converter.

00:08:58.700 --> 00:09:03.630
The first parameter is called quality,
and this next parameter

00:09:03.720 --> 00:09:06.000
is called priming method.

00:09:06.050 --> 00:09:09.340
So then you have the localized
name for those parameters.

00:09:09.340 --> 00:09:13.150
And available values for
quality would be minimum,

00:09:13.150 --> 00:09:15.740
low, medium, high, maximum.

00:09:16.800 --> 00:09:20.170
And then for priming
method it would be pre,

00:09:20.170 --> 00:09:21.800
normal, or none.

00:09:21.800 --> 00:09:27.590
And then you have limited values,
which is basically the same in this case,

00:09:27.590 --> 00:09:29.520
that doesn't change.

00:09:29.520 --> 00:09:34.500
And current value would be the index
into the available values array.

00:09:34.500 --> 00:09:37.600
And summary,
the help string you showed to the user.

00:09:37.600 --> 00:09:45.080
And then the hint field tells you
whether it's normal or expert.

00:09:46.800 --> 00:09:47.800
Thank you for listening.

00:09:49.520 --> 00:09:53.460
The priming method is
an expert parameter.

00:09:53.550 --> 00:09:57.970
So, okay, that's the audio converter.

00:09:58.330 --> 00:10:04.170
That's just the new features basically
in a short review of what it is.

00:10:04.230 --> 00:10:09.560
So Audio File is Core Audio's
interface to audio files parsing.

00:10:09.560 --> 00:10:16.150
And the newest feature for
Tiger is Audio File Component.

00:10:16.160 --> 00:10:19.700
So if you have an audio
file format and you say,

00:10:19.700 --> 00:10:23.740
gee, I wish Core Audio could
read this file format,

00:10:23.740 --> 00:10:27.580
well,
you can write your own format parsing

00:10:27.580 --> 00:10:30.800
component and install it in the system.

00:10:30.800 --> 00:10:34.450
And then everybody who uses the
Audio File API will be able to read the

00:10:34.480 --> 00:10:36.920
format that you wrote your component for.

00:10:37.160 --> 00:10:43.880
So there are component plugins
for the Audio File API.

00:10:45.990 --> 00:10:51.620
And we're going to provide SDK code so
that you can write your own component.

00:10:51.620 --> 00:10:56.060
And there will be a sample
component implemented there.

00:10:56.070 --> 00:11:01.220
And now if your file container
contains a compressed format,

00:11:01.300 --> 00:11:08.670
then you'll want to write an audio
codec so that people will be able

00:11:08.670 --> 00:11:12.880
to decode and encode to that data
format and then put it in your file

00:11:12.880 --> 00:11:15.240
format or some other file format.

00:11:15.320 --> 00:11:16.400
Okay.

00:11:17.670 --> 00:11:22.420
So this is the audio file component API.

00:11:22.670 --> 00:11:26.180
It's basically a mirror
of the audio file API.

00:11:26.180 --> 00:11:28.660
The audio file API will
just call down to these.

00:11:28.660 --> 00:11:32.720
And the most important thing about
the audio file component API is

00:11:32.720 --> 00:11:36.990
that you don't call this API.

00:11:37.350 --> 00:11:40.950
It's called from the
audio toolbox framework.

00:11:41.050 --> 00:11:47.120
Your audio file component
implements the component selectors

00:11:47.190 --> 00:11:49.440
that this API calls into.

00:11:49.440 --> 00:11:57.070
So this API is basically glue code.

00:11:57.080 --> 00:12:00.140
You can access it through the
audio toolbox if you wanted to

00:12:00.140 --> 00:12:01.660
test your component directly.

00:12:01.950 --> 00:12:06.780
For normal use, it's meant to be called
from the audio file API,

00:12:06.860 --> 00:12:11.500
calling your component for the client.

00:12:11.500 --> 00:12:18.280
And the SDK has classes that--
that implement those calls.

00:12:18.280 --> 00:12:23.510
So you would only have to subclass one
of the classes we provide in the SDK to

00:12:23.510 --> 00:12:27.320
create your audio file parsing component.

00:12:27.490 --> 00:12:33.890
So, as an aside,
the Audio Codec API is another similar,

00:12:33.890 --> 00:12:39.140
it's a plug-in API for
the audio converter.

00:12:39.140 --> 00:12:43.400
And you don't call the
Audio Codec API either.

00:12:43.400 --> 00:12:47.700
It's meant to be called from the
toolbox by the audio converter.

00:12:47.700 --> 00:12:54.460
So if you write an audio codec,
or if you want to convert data,

00:12:54.890 --> 00:12:58.900
you shouldn't use the
audio codec directly.

00:12:58.900 --> 00:13:02.850
You should use the audio converter.

00:13:03.400 --> 00:13:08.090
The audio converter instantiates
the codecs when someone requests

00:13:08.090 --> 00:13:10.680
to do a certain conversion.

00:13:10.680 --> 00:13:16.040
And there's a sample codec implementation
in developer examples as well.

00:13:16.060 --> 00:13:21.590
That brings us to what's the
philosophical split between a

00:13:21.670 --> 00:13:26.460
container for data and the data itself.

00:13:26.660 --> 00:13:31.160
Audio File API is meant to deal
with the container for the data.

00:13:31.160 --> 00:13:37.060
When you open an audio file and you say,
I want PCM samples out of this,

00:13:37.060 --> 00:13:43.040
the audio file provides you access
to the raw data as it is in the file.

00:13:43.160 --> 00:13:47.910
So if you want to convert that
from whatever format it's in

00:13:47.910 --> 00:13:53.920
to what you're interested in,
maybe PCM, then you would need to use the

00:13:54.370 --> 00:13:55.840
audio converter to do that.

00:13:56.240 --> 00:14:01.780
So if,
now for extending the audio file API,

00:14:01.780 --> 00:14:07.110
you have audio file components to
support new file types and then to extend

00:14:07.670 --> 00:14:10.500
what new data formats you can handle,
you have the audio codec

00:14:10.510 --> 00:14:17.340
API for writing components for
converting compressed formats.

00:14:17.880 --> 00:14:22.060
And then to find out what capabilities
are available in the system,

00:14:22.130 --> 00:14:24.700
for audio file,
you use audio file get global info

00:14:24.700 --> 00:14:29.500
that will allow you to find out what
file types are supported on the system.

00:14:29.500 --> 00:14:33.660
And then for audio converter,
you use the audio format API,

00:14:33.660 --> 00:14:40.910
which will tell you what audio
formats that have codecs installed

00:14:40.910 --> 00:14:46.330
on the systems and you can get
information about those formats.

00:14:47.400 --> 00:14:56.190
So, okay, well,
I just blew through those.

00:14:56.190 --> 00:14:56.190
Okay.

00:14:56.810 --> 00:15:02.860
So here's an example of using the
audio format API to find out what

00:15:02.860 --> 00:15:06.680
encoders are installed in the system.

00:15:06.680 --> 00:15:09.760
Basically,
you have a get property info to find

00:15:09.760 --> 00:15:14.940
out what's the size of the list you need
to allocate to get the format IDs back.

00:15:15.120 --> 00:15:17.580
And then you call audio
format get property,

00:15:17.660 --> 00:15:21.580
and that will give you a list of all
the four-character codes that would

00:15:21.590 --> 00:15:25.770
go in the subtype field of the audio
class description or the format ID field

00:15:25.770 --> 00:15:28.260
of an audio stream basic description.

00:15:28.260 --> 00:15:32.130
And so you have an array
there of the format IDs,

00:15:32.130 --> 00:15:38.170
and then I have a loop here that goes
through and asks the audio format API to

00:15:38.170 --> 00:15:43.810
get the format name as a CFString,
and then I print it out there.

00:15:43.880 --> 00:15:45.100
So that's how you do it.

00:15:45.120 --> 00:15:49.940
That's how you could print out a
list of all the encoder-- available

00:15:49.940 --> 00:15:52.440
formats for encoding on the system.

00:15:52.650 --> 00:15:59.430
And similarly, here's a code example for
printing out all the file types

00:15:59.930 --> 00:16:01.960
that are writable on the system.

00:16:01.960 --> 00:16:03.410
It's the same setup.

00:16:03.420 --> 00:16:06.930
Basically, you call audio file,
get global info to find out how

00:16:06.930 --> 00:16:10.240
big of a list you need to allocate,
and audio file,

00:16:10.260 --> 00:16:15.320
get global info to-- audio file,
get global info size for

00:16:15.320 --> 00:16:16.860
the size of the list,
and audio file,

00:16:16.860 --> 00:16:21.020
get global info for getting
the list of file types.

00:16:21.020 --> 00:16:22.510
Those are also four-character codes.

00:16:22.520 --> 00:16:27.600
And then you can get the name,
the CFString name for each file type,

00:16:27.760 --> 00:16:29.940
and that's printed out in
the loop there as well.

00:16:32.230 --> 00:16:36.280
So, okay,
another new feature in Tiger is to be

00:16:36.280 --> 00:16:41.660
able to get and set chunks in chunky
file formats like AIFF and WAV files.

00:16:41.760 --> 00:16:46.580
So, and the data that you get back
or set should be in the raw

00:16:46.980 --> 00:16:49.360
native format of the file.

00:16:49.360 --> 00:16:52.080
There's no parsing done on what
the contents of the chunk are.

00:16:53.660 --> 00:16:57.820
So, there's an audio file property,
chunk IDs,

00:16:57.820 --> 00:17:03.140
which allow you to get the list
of four-character codes for the

00:17:03.140 --> 00:17:04.310
chunks that are in the file.

00:17:04.320 --> 00:17:09.890
And then audio file count user
data will be if you have a certain

00:17:10.020 --> 00:17:14.240
chunk ID that you're interested in
and you want to find out how many

00:17:14.240 --> 00:17:15.300
of that chunk are in the file.

00:17:15.300 --> 00:17:18.120
You can call this and get
the number of that chunk,

00:17:18.120 --> 00:17:21.180
a number of occurrences
of that chunk in the file.

00:17:22.700 --> 00:17:26.550
And then get user data size will tell
you the size of a specific chunk.

00:17:26.560 --> 00:17:33.380
You pass up the file and the chunk ID and
the index of the chunk ID in the file.

00:17:33.380 --> 00:17:37.180
So, if there's like four and
you want the second one,

00:17:37.180 --> 00:17:39.560
you can pass one for the index.

00:17:39.560 --> 00:17:41.270
It's zero-based.

00:17:41.280 --> 00:17:45.640
So, and then that will tell
you the size of that chunk.

00:17:45.750 --> 00:17:50.100
So, then you could allocate a
buffer and you use audio file

00:17:50.100 --> 00:17:52.390
get user data to get the data.

00:17:52.700 --> 00:17:54.010
So, you can get the data for
that chunk out of the file.

00:17:54.020 --> 00:18:01.330
And so, if you're writing an audio file
component to support your own file

00:18:01.970 --> 00:18:05.600
format and it was a chunky file format,
then there's these call down

00:18:05.830 --> 00:18:09.400
into selectors in the audio
file component API as well.

00:18:09.400 --> 00:18:15.550
So, you could pass back your own
chunks from your file format.

00:18:15.560 --> 00:18:18.150
So, all right.

00:18:18.210 --> 00:18:20.740
So, then get user data actually
gets the data out of the file.

00:18:20.740 --> 00:18:22.660
There's a flags feel.

00:18:22.700 --> 00:18:29.820
There's a flags feel for this for it's
basically reserved for future use.

00:18:29.820 --> 00:18:36.920
So, then you pass up the size and
the buffer for the user data.

00:18:36.920 --> 00:18:44.290
And then audio file set user data is
you pass up the data and the size and

00:18:44.450 --> 00:18:46.600
the data and it sets that on the file.

00:18:46.600 --> 00:18:47.750
All right.

00:18:48.200 --> 00:19:06.400
[Transcript missing]

00:19:06.600 --> 00:19:10.450
And so the file marker is a
structure and it contains the name,

00:19:10.450 --> 00:19:14.340
position,
and other information about the marker.

00:19:14.340 --> 00:19:16.340
Okay.

00:19:16.360 --> 00:19:20.000
And with that, it's time for Doug Wyatt.

00:19:21.520 --> 00:19:23.970
Thanks, James.

00:19:29.400 --> 00:19:35.320
So I'd like to introduce a few more new
features in the audio toolbox for Tiger.

00:19:35.410 --> 00:19:40.100
We have new features for
high-level access to audio files.

00:19:40.100 --> 00:19:42.300
This builds on top of
the audio file APIs.

00:19:42.300 --> 00:19:50.100
We have some new audio units to
support scheduled playback of

00:19:50.110 --> 00:19:56.050
audio files and scheduled playback
of arbitrary audio buffers.

00:19:56.300 --> 00:19:59.660
And we have a new UI component,
which is in Cocoa,

00:19:59.660 --> 00:20:04.730
which you can use in your applications
to present the user with lists of

00:20:04.730 --> 00:20:10.300
audio formats and file formats,
as James was just describing.

00:20:10.300 --> 00:20:16.210
And we also have an audio unit that
supports rendering on a separate thread.

00:20:18.580 --> 00:20:22.260
So first, the Extended Audio File API.

00:20:22.260 --> 00:20:26.700
It provides an even higher
level of access to an audio file

00:20:26.700 --> 00:20:29.690
compared to the Audio File API.

00:20:30.150 --> 00:20:33.280
As James was mentioning,
we have good reasons for separating

00:20:33.950 --> 00:20:37.810
the audio file and converter
APIs at the lower levels,

00:20:37.810 --> 00:20:40.620
but it becomes really useful at
a higher level to just be able

00:20:40.620 --> 00:20:44.660
to sequentially access an audio
file in an arbitrary PCM format.

00:20:44.660 --> 00:20:49.690
Those of you who have our SDK,
you've seen the

00:20:49.690 --> 00:20:57.290
CA Audio File C++ class in there,
and this basically replaces it.

00:21:02.120 --> 00:21:05.500
So diagrammatically,
how this fits into the picture,

00:21:05.570 --> 00:21:10.510
extended audio file contains an
audio converter and an audio file.

00:21:11.020 --> 00:21:14.630
It uses the audio file API to
communicate with the file on

00:21:14.650 --> 00:21:17.490
disk in its native file format.

00:21:17.490 --> 00:21:21.490
But through the embedded audio converter,
it then presents your application

00:21:22.140 --> 00:21:26.890
with the ability to just do I/O in
the PCM format that you prefer.

00:21:27.030 --> 00:21:31.860
This is a very simple API by
comparison to the lower level ones.

00:21:32.080 --> 00:21:35.100
Pretty much all you have to
do is open or create a file,

00:21:35.110 --> 00:21:38.000
configure the converter
if you're doing encoding,

00:21:38.240 --> 00:21:41.890
and then you can sequentially read
from the file or write to it if

00:21:41.900 --> 00:21:44.310
you're encoding or just writing PCM.

00:21:44.580 --> 00:21:48.040
You can seek around within the
file down to sample accuracy

00:21:48.040 --> 00:21:50.000
even within an encoded file.

00:21:50.000 --> 00:21:54.560
And you can also tell,
which is the logical thing

00:21:54.560 --> 00:21:55.860
to have if you can seek.

00:21:57.670 --> 00:22:03.870
And so this API is described
in this header file here.

00:22:06.730 --> 00:22:10.400
So the second new Tiger feature
I'd like to mention is the

00:22:10.580 --> 00:22:15.200
audio format selection view,
which is a Cocoa UI.

00:22:15.200 --> 00:22:19.670
It's a subclass of NSViews,
so you can do things like

00:22:19.840 --> 00:22:23.330
just embed it in a save panel.

00:22:23.540 --> 00:22:24.470
and others.

00:22:25.500 --> 00:22:25.500
So, what is the Mac OS X?

00:22:25.500 --> 00:22:26.500
Well, it's a very simple, easy-to-use,
and very easy-to-use, as the accessory.

00:22:26.500 --> 00:22:28.820
It's subclassable,
meaning that it's got hooks

00:22:28.820 --> 00:22:32.260
so you can decide things like,
well, I know I always want to write

00:22:32.260 --> 00:22:36.120
this particular file format,
so don't show that menu.

00:22:36.120 --> 00:22:40.310
And it will, as it says there,
let you select an audio file type,

00:22:40.350 --> 00:22:44.130
data format that's
appropriate to that file type,

00:22:44.130 --> 00:22:49.160
any encoding parameters, sample rate,
and the channel layout

00:22:49.160 --> 00:22:50.740
if it's more than stereo.

00:22:50.820 --> 00:22:54.660
I'll just give you a quick peek at that.

00:22:54.660 --> 00:22:58.690
In the demo, please.

00:23:03.660 --> 00:23:08.710
And so here I've got it embedded as
the safe panel on an application.

00:23:08.950 --> 00:23:13.420
And right now I've got AIFF selected,
so my available audio data

00:23:13.420 --> 00:23:15.890
formats are just integer formats.

00:23:16.030 --> 00:23:21.590
But if I select AIFC,
then suddenly I've got access

00:23:21.590 --> 00:23:27.610
to the encoded formats which
can be embedded in an AIFC file.

00:23:27.950 --> 00:23:30.040
And if I were to select
an encoded format,

00:23:30.040 --> 00:23:33.070
I'd have a button here where
I could configure the encoder,

00:23:33.190 --> 00:23:35.240
but that's not hooked up yet.

00:23:35.250 --> 00:23:41.230
So this isn't in your seed,
but it will be in future Tiger seeds.

00:23:41.300 --> 00:23:43.410
Back to the slides, please.

00:23:46.590 --> 00:23:52.060
Okay, so the way it works,
you just give the view object the

00:23:52.060 --> 00:23:55.290
source data format that you're
encoding from and its channel layout

00:23:55.290 --> 00:23:57.060
if it's got more than two channels.

00:23:57.060 --> 00:24:02.100
And what you get back are an audio
file type ID like AIFF or WAV.

00:24:02.140 --> 00:24:08.110
You get back the output data format
like float or 24-bitint or 16-bitint,

00:24:08.240 --> 00:24:11.730
bigger little endian,
whatever you're encoding

00:24:11.730 --> 00:24:13.600
to or just writing to.

00:24:14.240 --> 00:24:18.990
And you get audio converter properties
back again if you're encoding.

00:24:22.750 --> 00:24:25.200
Okay, I just did that.

00:24:25.330 --> 00:24:28.200
Okay, so next.

00:24:28.620 --> 00:24:34.000
We have several new audio units in Tiger,
and there's a couple of them which

00:24:34.070 --> 00:24:37.950
are of a new class called generators,
meaning that they don't

00:24:38.030 --> 00:24:39.700
take audio or MIDI in.

00:24:39.700 --> 00:24:43.240
You either interact with
them programmatically by

00:24:43.240 --> 00:24:48.210
setting properties on them,
or by bringing up their user interface

00:24:48.210 --> 00:24:54.000
and letting the user do something that
tells them to make noise of some sort,

00:24:54.000 --> 00:24:55.460
for example, a test tone generator.

00:24:56.240 --> 00:25:00.880
This one, the AU Audio File Player,
falls into the programmatic

00:25:00.880 --> 00:25:02.470
category for the most part.

00:25:02.500 --> 00:25:08.530
It's designed to be a nice building
block for playing back little slices

00:25:08.530 --> 00:25:14.250
of audio files or large slices
with sample-accurate precision.

00:25:14.280 --> 00:25:17.780
It's built on top of the
Extended Audio File API,

00:25:17.840 --> 00:25:22.030
which gives it the ability
to play back encoded formats.

00:25:25.800 --> 00:25:30.830
So it's particularly appropriate
for situations where you want to

00:25:30.890 --> 00:25:33.640
stream a long file off of disk.

00:25:33.640 --> 00:25:37.420
It's sample accurate,
so you could use it as a building

00:25:37.420 --> 00:25:40.420
block in even a digital audio
workstation kind of application.

00:25:40.420 --> 00:25:45.850
And it's also usable in simpler
situations where you just want to

00:25:45.870 --> 00:25:49.000
play a long file asynchronously.

00:25:49.000 --> 00:25:54.870
The one thing it's not good
for is playing short sounds.

00:25:55.000 --> 00:25:58.700
If all your application is going to do is
play a few short sounds here and there,

00:25:58.700 --> 00:26:05.260
it's not worth the overhead because
this audio unit will create a separate

00:26:05.260 --> 00:26:08.750
thread for reading the audio off of disk.

00:26:08.760 --> 00:26:13.920
And if you've got a 200K audio file
that's an alert sound or something,

00:26:13.980 --> 00:26:15.990
you don't need that thread overhead.

00:26:16.000 --> 00:26:18.250
You might as well just load the
file into memory and play it.

00:26:18.340 --> 00:26:21.980
You can use NSSound to do that,
and you can also use

00:26:21.980 --> 00:26:26.170
the system sound APIs,
which are really the best way to be

00:26:26.170 --> 00:26:29.410
playing alert sounds on the system now.

00:26:32.900 --> 00:26:38.460
So conceptually, the way that the
AU Audio File Player works,

00:26:38.520 --> 00:26:40.150
there's this hardware timeline.

00:26:40.320 --> 00:26:44.300
If you've ever looked at the
timestamps going by on an audio unit,

00:26:44.300 --> 00:26:46.710
you'll see that they're
typically these giant numbers,

00:26:46.710 --> 00:26:50.880
and it's the number of samples
since the hardware was started.

00:26:50.910 --> 00:26:54.330
So there's that timeline
that's going along.

00:26:55.120 --> 00:27:00.220
And then to play audio files,
in this example, I've got two events,

00:27:00.220 --> 00:27:04.660
two chunks of audio files that I want to
play at some point along this timeline.

00:27:04.660 --> 00:27:09.050
And instead of having to know
where on the timeline I am,

00:27:09.070 --> 00:27:12.880
I just construct my sequence of
events to be relative to time zero,

00:27:12.910 --> 00:27:16.940
which is when I'm going to start
playing this sequence of events.

00:27:17.160 --> 00:27:21.310
So the one on the left,
I'm playing at time zero.

00:27:21.310 --> 00:27:25.210
It's file A.AIF,
and I'm playing the first

00:27:25.240 --> 00:27:26.870
100,000 samples out of it.

00:27:26.920 --> 00:27:29.860
And I've got another chunk out
of audio file B that I want

00:27:29.860 --> 00:27:31.500
to play a little bit later.

00:27:31.500 --> 00:27:36.200
So that's how the events look that
I schedule when I want to have

00:27:36.420 --> 00:27:39.140
them played relative to time zero.

00:27:39.140 --> 00:27:43.270
And after that,
I just need to set a start time.

00:27:43.270 --> 00:27:45.570
And I can do this in one of two ways.

00:27:45.620 --> 00:27:49.000
If I'm in a situation where I need
to synchronize very accurately,

00:27:49.010 --> 00:27:52.150
and I know exactly where on that
audio device's timeline I want

00:27:52.150 --> 00:27:55.510
to start playing those events,
I can specify the start

00:27:55.510 --> 00:27:57.350
time in sample numbers.

00:27:57.400 --> 00:28:01.300
The other alternative,
in a simpler situation,

00:28:01.300 --> 00:28:06.350
is I can just say start now by
passing a start time of minus one.

00:28:09.390 --> 00:28:14.750
So just to give you a little
more detail about how this

00:28:14.870 --> 00:28:19.580
API or this audio unit is used,
you open the audio files in your

00:28:19.580 --> 00:28:22.820
application externally to the audio unit.

00:28:23.110 --> 00:28:27.480
Your application owns those
references to the audio file,

00:28:27.480 --> 00:28:28.620
not the unit.

00:28:28.670 --> 00:28:30.910
So your application keeps it open.

00:28:31.060 --> 00:28:34.960
You pass an array of audio
file IDs to the unit.

00:28:35.810 --> 00:28:38.750
So when it comes time
for it to read off disk,

00:28:38.780 --> 00:28:40.630
it will have the files open.

00:28:40.780 --> 00:28:44.140
You schedule the events that
you initially want to have play.

00:28:44.190 --> 00:28:47.530
Then you prime the unit,
which is just a set property call on it.

00:28:47.530 --> 00:28:51.150
And what that does is goes and
fills up the disk buffers enough

00:28:51.150 --> 00:28:54.090
so that when you do say start,
it will have the data

00:28:54.090 --> 00:28:55.520
off the disk already.

00:28:55.520 --> 00:28:57.880
Then you set the start time.

00:28:57.880 --> 00:29:00.550
And as the events play,
you get callbacks saying

00:29:00.700 --> 00:29:03.290
this one's been done,
this one's been done.

00:29:03.290 --> 00:29:07.990
And in response to those callbacks,
or other events in your application,

00:29:07.990 --> 00:29:13.500
you can always schedule additional
events onto the file player unit.

00:29:13.500 --> 00:29:15.840
And at any time,
you can just call AudioUnitReset

00:29:15.840 --> 00:29:17.660
and it will stop playing.

00:29:17.660 --> 00:29:23.190
And there's about eight paragraphs
of documentation on the tiger seed in

00:29:23.210 --> 00:29:27.730
AudioUnitProperties.h describing how
to use this unit programmatically,

00:29:27.730 --> 00:29:29.130
and that is in your seed.

00:29:32.490 --> 00:29:36.400
Just to zoom in one more level,
here's the structure that you use for

00:29:36.900 --> 00:29:39.400
scheduling regions to the audio file.

00:29:39.400 --> 00:29:41.430
It's got an audio timestamp.

00:29:41.500 --> 00:29:45.870
You can actually schedule using
sample numbers or host times,

00:29:45.930 --> 00:29:49.080
which can be useful sometimes.

00:29:49.080 --> 00:29:56.520
And the audio timestamp structure,
as you may know, contains both a sample

00:29:56.520 --> 00:29:56.520
number and a host time.

00:29:56.710 --> 00:29:59.960
You have an optional callback
completion proc that gets called

00:29:59.960 --> 00:30:05.180
when the region has been either
successfully loaded from disk.

00:30:05.220 --> 00:30:06.380
Well, actually,
it's always called when it's

00:30:06.380 --> 00:30:09.440
successfully loaded from disk,
but that's also used to tell

00:30:09.440 --> 00:30:13.300
you if it didn't get it off
the disk and played in time.

00:30:13.460 --> 00:30:18.400
The structure also contains a
reference to an audio file object.

00:30:18.400 --> 00:30:22.450
You just say where in the file in
terms of sample frames and number

00:30:22.450 --> 00:30:24.400
frames that you want to play.

00:30:24.590 --> 00:30:26.740
This can even be in an encoded format.

00:30:26.910 --> 00:30:29.400
For example, it's an MP3 file.

00:30:29.400 --> 00:30:31.900
You say I want sample 512.

00:30:31.900 --> 00:30:34.370
Well, that's a bit of the ways
into the first packet.

00:30:34.570 --> 00:30:39.570
It will actually seek into the middle
of the packet and begin playback there.

00:30:41.990 --> 00:30:49.380
And I've got a little demo application
which illustrates the AU Audio file.

00:30:50.600 --> 00:30:55.650
So what this does is lets me generate
a random playlist out of a bunch

00:30:55.650 --> 00:30:58.440
of audio files on my hard disk.

00:30:58.460 --> 00:31:05.830
I can say how long I want the playlist
to be and what the minimum slice size is.

00:31:06.290 --> 00:31:08.190
The longest one is.

00:31:08.300 --> 00:31:09.090
Click that.

00:31:09.120 --> 00:31:13.020
And this is a little slow because
it's actually parsing through

00:31:13.020 --> 00:31:16.500
all of the frames of the MP3s.

00:31:17.020 --> 00:31:18.720
And so it's generated a playlist here.

00:31:18.720 --> 00:31:19.760
Oh, they're all the same length.

00:31:19.760 --> 00:31:21.870
That's no fun.

00:31:21.880 --> 00:31:23.780
Oh, it's OK.

00:31:26.780 --> 00:31:33.380
So it's my little music concrete player.

00:31:33.380 --> 00:31:37.420
So that's dynamically reading
and decoding the MP3s on the I/O,

00:31:37.420 --> 00:31:42.480
or rather on the disk read thread,
and preparing them to be played back.

00:31:42.510 --> 00:31:48.300
So it wasn't cheating and
generating the playlist ahead

00:31:48.350 --> 00:31:51.010
of time into an audio buffer.

00:31:51.010 --> 00:31:51.010
It was actually playing
those back as we went.

00:31:53.690 --> 00:31:57.590
Okay, so one related audio unit, in fact,
this one is a subclass,

00:31:57.590 --> 00:32:01.720
or actually the base class, I'm sorry,
of AU Audio File Player,

00:32:01.720 --> 00:32:03.700
is the AU Scheduled Sound Player.

00:32:03.700 --> 00:32:06.220
And it shares a lot
of the same semantics,

00:32:06.220 --> 00:32:09.420
like how to set the start
time on it in particular.

00:32:09.420 --> 00:32:15.060
The difference is instead of
scheduling chunks of audio files to it,

00:32:15.060 --> 00:32:18.980
you give it audio buffers
and memory to play.

00:32:18.980 --> 00:32:23.690
And one internal client for
this is the speech synthesizer.

00:32:23.700 --> 00:32:28.210
And if your application dynamically
generates audio into buffers,

00:32:28.220 --> 00:32:33.040
you may find this a useful way to pump
it out into a chain of audio units.

00:32:33.100 --> 00:32:39.070
And this unit's also documented in
AudioUnitProperties.h and is in the seed.

00:32:42.940 --> 00:32:46.980
The last of the three new audio
units I'd like to talk about

00:32:46.980 --> 00:32:49.300
is the AU Deferred Renderer.

00:32:49.300 --> 00:32:52.920
So this one is a converter audio unit.

00:32:52.920 --> 00:32:59.240
And the idea here is to have it pull its
input on a separate thread from the one

00:32:59.240 --> 00:33:02.260
on which it's being pulled for output.

00:33:02.260 --> 00:33:05.590
And this lets you put a
processor-intensive task on a

00:33:05.800 --> 00:33:10.280
separate thread instead of on
the real-time audio thread.

00:33:10.280 --> 00:33:16.460
So you can process your audio in larger
chunks at a slightly lower priority.

00:33:16.460 --> 00:33:21.160
There's opportunities for taking
advantage of multiprocessing.

00:33:21.160 --> 00:33:24.990
The API is documented in
detail in AudioUnitProperties.h

00:33:27.100 --> 00:33:31.370
And I'll just give you an illustration
of when you might want to use it.

00:33:31.460 --> 00:33:34.200
So here's an example.

00:33:35.280 --> 00:33:37.390
It could be anything that
I'm using as my source here,

00:33:37.400 --> 00:33:41.610
but it's an AU Audio file player,
and I've got a CPU-intensive audio

00:33:42.100 --> 00:33:44.470
unit going to an audio output unit.

00:33:45.560 --> 00:33:48.480
And I'm running the
hardware at 128 frames,

00:33:48.480 --> 00:33:53.490
which is around 3 milliseconds
at 44.1 kilohertz.

00:33:53.750 --> 00:33:59.100
And the fullness of those green
IO cycle rectangles is how much of

00:33:59.100 --> 00:34:04.740
each render cycle is being occupied
by this CPU-intensive audio unit.

00:34:04.780 --> 00:34:06.300
And so we're kind of
running on the edge here.

00:34:06.300 --> 00:34:11.810
We're probably consuming around 85%
of the CPU just rendering that audio.

00:34:13.480 --> 00:34:18.790
So what you can do if latency
isn't so important and you want to

00:34:19.120 --> 00:34:24.590
actually get some more CPU power
back is use the deferred renderer.

00:34:24.680 --> 00:34:30.900
So now on the upper line
here we have the source,

00:34:30.920 --> 00:34:32.520
the CPU intensive audio unit.

00:34:32.710 --> 00:34:36.970
Those are running on a separate
thread owned by the deferred renderer,

00:34:37.030 --> 00:34:39.690
which is on the lower line here,
and it's still being driven

00:34:39.690 --> 00:34:42.190
by the audio output unit.

00:34:42.400 --> 00:34:48.640
So what this does is it periodically just
wakes up the lower priority thread to do

00:34:48.640 --> 00:34:53.250
work and pulls that at the larger grain,
which you can specify.

00:34:53.350 --> 00:34:56.440
So in this example,

00:34:56.670 --> 00:35:02.020
We still have our 128 frame I/O cycles,
but we're only doing our

00:35:02.020 --> 00:35:05.680
rendering in 512 frame cycles.

00:35:05.680 --> 00:35:09.350
And depending on the algorithm,
of course, your mileage may vary,

00:35:09.360 --> 00:35:13.430
but in many algorithms you can
get a good performance gain by

00:35:13.430 --> 00:35:15.860
processing more samples at a time.

00:35:18.500 --> 00:35:23.480
So just to review,
I mentioned the extended audio file API,

00:35:23.600 --> 00:35:26.160
which combines an audio file
with an audio converter.

00:35:26.160 --> 00:35:29.900
And all of these are in the seed,
by the way, of Panther.

00:35:29.900 --> 00:35:34.380
There's the audio format,
except the audio format selection view,

00:35:34.410 --> 00:35:35.330
excuse me.

00:35:35.540 --> 00:35:38.980
That will be in the new
Core Audio Kit framework.

00:35:38.980 --> 00:35:41.650
We have three new
programmable audio units,

00:35:41.650 --> 00:35:45.010
the AU Audio File Player,
the AU Scheduled Sound Player,

00:35:45.060 --> 00:35:47.080
and the AU Deferred Renderer.

00:35:47.080 --> 00:35:51.100
And with that,
I'll bring up Bill Stewart,

00:35:51.150 --> 00:35:55.170
who's going to talk about
Mac OS X and OpenAL.

00:35:56.600 --> 00:36:02.000
[Transcript missing]

00:36:03.910 --> 00:36:05.960
Okay, so OpenAL and Mac OS X.

00:36:05.960 --> 00:36:14.020
We had some conversations about this
last year in the games developer session,

00:36:14.020 --> 00:36:18.770
and I wanted to give you
an update of where we are.

00:36:19.150 --> 00:36:22.220
So OpenAL is,
just to give you some background

00:36:22.220 --> 00:36:26.620
if people don't know what it is,
it's an engine that's designed for games.

00:36:26.620 --> 00:36:30.620
It's designed to be a
complement to OpenGL,

00:36:30.620 --> 00:36:35.230
so it follows similar API conventions,
coordinate systems, and so forth.

00:36:36.240 --> 00:36:39.440
It's been originally
developed at Creative Labs.

00:36:39.440 --> 00:36:43.220
There's now a specific
website devoted to it.

00:36:43.300 --> 00:36:47.150
It supports multiple platforms
and has for a number of years,

00:36:47.280 --> 00:36:50.750
I think it's been available
for about four years now.

00:36:50.780 --> 00:36:55.010
So on the PCs, there's implementations
for the original Mac OS,

00:36:55.010 --> 00:36:59.030
Mac OS X, Windows and Linux,
and you'll also see some support

00:36:59.030 --> 00:37:03.380
that was recently announced at
the Games Developer Conference for

00:37:03.380 --> 00:37:05.640
game consoles,
PlayStations.

00:37:06.240 --> 00:37:07.320
GameCube and Xbox.

00:37:07.420 --> 00:37:12.650
So it's a pretty broad API and it was
one of the things that attracted us to

00:37:12.650 --> 00:37:15.990
support this because of its broad reach.

00:37:16.000 --> 00:37:22.130
To give you some example of the games
that are supporting it in brackets,

00:37:22.130 --> 00:37:26.580
there's a list of the
platforms that are doing it.

00:37:26.580 --> 00:37:29.040
So there's quite a good
collection of games.

00:37:29.040 --> 00:37:33.530
The OpenAL website actually
has a full list of all of

00:37:33.530 --> 00:37:36.040
the different games and apps.

00:37:36.240 --> 00:37:38.240
And companies that are supporting it.

00:37:38.240 --> 00:37:41.520
So if you're interested to
see the sort of coverage,

00:37:41.620 --> 00:37:43.770
that's a good website to look at.

00:37:44.210 --> 00:37:48.650
And in the
Games Developer Conference of 2004,

00:37:48.650 --> 00:37:52.220
we announced that we were going
to support this in the API,

00:37:52.220 --> 00:37:56.490
provide an implementation for it.

00:37:56.500 --> 00:38:01.860
The implementation that we did was based
on pre-existing Core Audio services,

00:38:01.860 --> 00:38:05.950
and I'll give you a bit of an
overview of how we've done this.

00:38:06.020 --> 00:38:11.840
We also made the source available
for the implementation of the

00:38:12.000 --> 00:38:15.380
Core Audio API for it on the website.

00:38:15.750 --> 00:38:19.660
OpenAL has both hardware and
software implementations.

00:38:19.700 --> 00:38:24.350
You've got implementations that
are implemented in sound cards like

00:38:24.350 --> 00:38:28.190
Creative Sound Blaster and so forth,
and there's also implementations

00:38:28.190 --> 00:38:29.760
that run on CPUs.

00:38:30.140 --> 00:38:33.000
Ours, of course, is running on the CPU.

00:38:33.000 --> 00:38:36.580
And one of the things we're announcing
in this conference is that we're going to

00:38:36.580 --> 00:38:38.500
be pre-installing the framework in Tiger.

00:38:38.500 --> 00:38:42.800
It's actually on your
TigerSeed disks at the moment.

00:38:42.820 --> 00:38:45.410
OpenAL framework,
that's essentially what's

00:38:45.430 --> 00:38:46.900
available on the OpenAL website.

00:38:46.900 --> 00:38:52.800
As we continue to develop and enhance
the OpenAL to Core Audio bridge,

00:38:52.830 --> 00:38:56.800
we'll be putting the source back
into the OpenAL organization.

00:38:56.800 --> 00:39:01.040
So it's also a good reference
if you want to see how to use

00:39:01.110 --> 00:39:05.400
Core Audio in different ways as well,
and we fully support the open source.

00:39:05.400 --> 00:39:12.100
So that's good news, I think.

00:39:12.100 --> 00:39:18.790
One of the things that's fairly unique
about OpenAL on Mac OS X is that it uses

00:39:18.790 --> 00:39:25.570
system sound services and automatically
will configure to the system that

00:39:25.570 --> 00:39:28.500
the user has currently installed.

00:39:28.500 --> 00:39:32.380
So if you have a surround system
and the user has gone and set up

00:39:32.380 --> 00:39:36.140
the surround in Audio/Midi setup,
it will be automatically

00:39:36.260 --> 00:39:38.800
configured and will just play
through the surround system.

00:39:38.800 --> 00:39:42.950
If you've just got stereo,
then the OpenAL logic that's

00:39:43.000 --> 00:39:45.910
in the implementation will
find that it's a stereo device,

00:39:45.910 --> 00:39:49.330
and then the mixes that it
will do will be for stereo.

00:39:49.400 --> 00:39:52.300
And you can have a look at
how to configure systems,

00:39:52.320 --> 00:39:55.300
which you would need to do if you're
doing any kind of multi-channel

00:39:55.310 --> 00:40:00.300
surround playback in applications,
utilities.

00:40:00.430 --> 00:40:04.560
There's some abilities there to go in,
say, where your speakers

00:40:04.620 --> 00:40:06.100
are and what channels,
and so forth.

00:40:06.100 --> 00:40:08.190
And having done that,
then you're just ready to go,

00:40:08.190 --> 00:40:09.600
and that's a system-wide service.

00:40:09.600 --> 00:40:13.930
So other applications that are
doing surround will just work

00:40:14.000 --> 00:40:16.670
in the same way that this does.

00:40:17.050 --> 00:40:22.460
This is a diagram of the
implementation that we do for it.

00:40:22.510 --> 00:40:26.630
It uses two audio units.

00:40:26.670 --> 00:40:28.350
It uses the output unit.

00:40:28.350 --> 00:40:33.700
Output unit AUHAL is an audio unit
that interfaces to the device.

00:40:33.700 --> 00:40:37.930
We recommend very strongly
that unless you're doing very,

00:40:38.070 --> 00:40:42.600
very particular things with devices
that you use the output unit

00:40:42.600 --> 00:40:44.400
rather than the audio device API.

00:40:44.400 --> 00:40:49.800
It provides a lot of management of just
device state that's very handy for you.

00:40:49.980 --> 00:40:54.710
The output unit is just going to get
data and output it from the device,

00:40:54.710 --> 00:40:58.660
and it's getting data from
the 3D mixer audio unit.

00:40:58.660 --> 00:41:03.040
This audio unit can take
any number of inputs.

00:41:03.380 --> 00:41:10.130
It has different parameters that you can
set on the inputs like a pitch parameter,

00:41:10.540 --> 00:41:13.610
distant parameters that can
be particular for each input.

00:41:13.640 --> 00:41:16.620
You can have mono or stereo inputs.

00:41:16.620 --> 00:41:20.000
And the 3D mixer will just take these.

00:41:20.000 --> 00:41:23.750
It will take panning
coordinates for each input,

00:41:24.160 --> 00:41:25.610
place them in a sound field.

00:41:25.620 --> 00:41:30.820
You can describe with the 3D mixer
different panning algorithms.

00:41:30.820 --> 00:41:33.010
One is called vector panning.

00:41:33.090 --> 00:41:35.450
Vector panning will just look
at the location of a sound.

00:41:35.460 --> 00:41:37.600
Let's say it's there,
and it will look to the

00:41:38.040 --> 00:41:40.470
closest two speakers,
and it will just place the

00:41:40.470 --> 00:41:41.950
sound in those two speakers.

00:41:42.020 --> 00:41:46.360
The one we use in OpenAO,
and I think it's the more pleasing

00:41:46.480 --> 00:41:52.020
algorithm for this sort of usage,
is a sound field approach.

00:41:52.020 --> 00:41:55.680
What that will do is in each speaker
it creates a representation of

00:41:55.700 --> 00:41:57.770
the sound field at that location.

00:41:57.780 --> 00:42:01.380
So if you have a sound over there,
that sound will be in

00:42:01.380 --> 00:42:02.740
all of the speakers,
but it will be very quiet.

00:42:02.760 --> 00:42:07.090
In this one, as you get closer to the
location of the sound,

00:42:07.130 --> 00:42:08.420
it's louder.

00:42:08.650 --> 00:42:11.610
And so they're the kind of
things that the 3D Mixer does,

00:42:11.670 --> 00:42:16.400
and this is also the sort of things
that they do on the creative cards

00:42:16.420 --> 00:42:19.020
and on other hardware implementations.

00:42:19.020 --> 00:42:23.240
Then the inputs to the
3D Mixer are OAL sources,

00:42:23.240 --> 00:42:29.240
and a source object is something in
the OpenAL API that you can manipulate,

00:42:29.240 --> 00:42:32.320
that you can set properties on,
and so forth.

00:42:32.440 --> 00:42:36.140
And one of the main things the
OpenAL source has is a list

00:42:36.140 --> 00:42:38.360
of buffers that you can play.

00:42:38.770 --> 00:42:41.600
And you can be sharing buffers
between different sources.

00:42:41.600 --> 00:42:45.600
The buffer can have different
format characteristics,

00:42:45.600 --> 00:42:48.560
mono, stereo,
etc., different sample rates.

00:42:48.680 --> 00:42:53.820
And the game engine just basically
queues the buffers up to be played.

00:42:54.340 --> 00:42:58.880
Now in the line between the two buffers,
we're using the audio converter

00:42:59.020 --> 00:43:01.020
API that James went through.

00:43:01.120 --> 00:43:05.600
And that'll take the
buffers that are on this,

00:43:05.600 --> 00:43:07.100
it'll convert them.

00:43:07.100 --> 00:43:08.000
We've spent a couple of years on this,
and it's a really good idea.

00:43:08.110 --> 00:43:12.570
We've spent some time doing
optimizations to the blitters

00:43:12.630 --> 00:43:14.100
that we've most of that in path.

00:43:14.140 --> 00:43:17.990
To the end, some of the right
conversions are all there.

00:43:21.700 --> 00:43:27.250
And then we also provide a utility
call in the OpenAL library that

00:43:27.250 --> 00:43:30.600
uses the audio file API to read...

00:43:31.100 --> 00:43:36.500
[Transcript missing]

00:43:36.800 --> 00:43:47.000
[Transcript missing]

00:43:49.400 --> 00:43:58.000
[Transcript missing]

00:44:02.300 --> 00:44:18.000
[Transcript missing]

00:44:18.880 --> 00:44:23.890
So we spent a lot of time over the last
couple of months just looking at our

00:44:24.030 --> 00:44:29.030
components of this rendering process,
seeing whether...

00:44:31.050 --> 00:44:34.670
The problems we were seeing where
we could optimize the performance,

00:44:34.820 --> 00:44:43.990
making sure that the
that we were looking at,

00:44:44.010 --> 00:44:44.010
you know, distance practice.

00:44:44.010 --> 00:44:44.010
One of the things that
distance will do is

00:44:45.040 --> 00:44:51.080
frequencies so ... you'll hear less
high frequencies coming through,

00:44:51.250 --> 00:44:53.400
and there's also a volume attenuation.

00:44:53.400 --> 00:44:58.320
In OpenAL you can specify a
very nice sort of volume curve,

00:44:58.320 --> 00:45:04.320
and we've supported a similar thing
in the 3D mixer for quite some time,

00:45:04.320 --> 00:45:08.610
where you can say, "well,
in this distance the sound did not get

00:45:08.720 --> 00:45:11.130
louder or quieter." So you can imagine,
like,

00:45:11.130 --> 00:45:15.170
you're going up to a B or something,
and if we take a foot of

00:45:15.370 --> 00:45:17.190
that B it will be the same.

00:45:17.800 --> 00:45:22.830
The sound should get quieter and
then you can't hear the sound at all.

00:45:22.830 --> 00:45:26.350
You can specify those conditions
when you're doing a game so that

00:45:26.480 --> 00:45:30.150
your sounds have characteristics
of how they're going to be mixed,

00:45:30.150 --> 00:45:32.160
how they're going to be played.

00:45:33.100 --> 00:45:36.950
and you can imagine if you've got an
engine sitting in the corner over here,

00:45:36.950 --> 00:45:39.630
then in this room we're never
going to get far enough.

00:45:41.110 --> 00:45:52.410
We're going to talk about some of
the things we went through when we

00:45:52.410 --> 00:46:05.140
were looking at doing this because
this enhances the game experience.

00:46:05.960 --> 00:46:11.200
As we went through all of
this optimization work,

00:46:11.370 --> 00:46:18.350
we managed to gain about a 50%
improvement in the 3D mixer audio

00:46:18.350 --> 00:46:20.350
unit as a part of this work.

00:46:20.440 --> 00:46:22.720
We also profiled a couple of games.

00:46:22.720 --> 00:46:27.430
Blizzard uses the applications directly,
so we had a look at some of

00:46:27.430 --> 00:46:29.450
their uses of our system.

00:46:30.310 --> 00:46:35.290
We overall, I would say,
about a 50% improvement in rendering

00:46:35.290 --> 00:46:37.400
from what we had in Panther.

00:46:37.400 --> 00:46:41.980
And then to give you some idea of the
difference between the implementation

00:46:41.980 --> 00:46:47.560
we provide now and the implementation
that we have previously for Mac OS X,

00:46:47.850 --> 00:46:54.440
on a PowerBook 1 GHz G4,
we take about a 5% CPU load

00:46:54.560 --> 00:46:58.740
to mix 64 sources into stereo.

00:46:58.900 --> 00:47:03.900
So that's 5%. So that's not
doing sort of some fancy stuff.

00:47:03.900 --> 00:47:08.220
You'll see a bit more of a load if
you're doing other kinds of things.

00:47:08.270 --> 00:47:12.820
But if you think of a 1Gb PowerBook G4,
that's not a high-end

00:47:12.820 --> 00:47:14.560
machine in today's standards.

00:47:14.630 --> 00:47:18.950
We're not saying 5% on a 2.5 Joule G5.

00:47:18.980 --> 00:47:21.420
This is a fairly representative system.

00:47:21.490 --> 00:47:27.720
And the kinds of improvements we saw,
the next best implementation

00:47:27.720 --> 00:47:31.850
that was previously available,
was about 20% for that same test.

00:47:31.890 --> 00:47:34.780
So overall,
with the improvements to the 3D

00:47:34.780 --> 00:47:38.680
mixer plus more optimal blitters in
the audio converter and so forth,

00:47:38.680 --> 00:47:42.560
we're down at about 25% of
what was previously available.

00:47:42.590 --> 00:47:48.760
So that should be really good news for
you when you're looking at your game.

00:47:48.760 --> 00:47:51.860
And I think you've heard
enough of all of us talking,

00:47:51.860 --> 00:47:58.220
so we're going to kill some aliens.

00:47:58.260 --> 00:47:59.380
Thank you.

00:48:00.900 --> 00:48:10.490
Unreal Tournament,
and maybe if walks around and we've got

00:48:10.490 --> 00:48:10.490
sound coming from all the speakers here.

00:48:16.460 --> 00:48:18.400
Have we got the front speakers on?

00:48:18.400 --> 00:48:21.900
Useless.

00:48:21.900 --> 00:48:24.900
Left and right.

00:48:24.900 --> 00:48:30.400
Are you guys hearing sound from all?

00:48:30.400 --> 00:48:33.550
It's hard to tell from up here.

00:48:36.690 --> 00:48:39.950
As Bob's sort of moving around,
you can see probably the people

00:48:40.010 --> 00:48:43.040
kind of around here are getting
the best listening experience.

00:48:43.040 --> 00:48:43.860
I'm not.

00:48:43.870 --> 00:48:47.300
You can see how the
sound is moving around.

00:48:47.300 --> 00:48:49.220
It should be coming to the front of you.

00:48:49.220 --> 00:48:50.900
I don't know why that is.

00:48:50.900 --> 00:49:00.020
Find the quiet sound.

00:49:29.750 --> 00:49:33.480
You can hear just sort of how the
sounds are kind of phasing in and out.

00:49:33.480 --> 00:49:35.600
You can hear some of the distance work.

00:49:36.840 --> 00:49:43.330
And that's the things we
were talking about earlier.

00:50:13.200 --> 00:50:44.800
[Transcript missing]

00:50:52.920 --> 00:50:59.800
We have an excellent survival video
on this on a pretty regular basis.

00:50:59.800 --> 00:51:04.800
We have a very good team of developers,
and we have a very good

00:51:04.800 --> 00:51:04.800
team of developers.

00:51:04.800 --> 00:51:10.500
We have a very good team of developers,
and we have a very good

00:51:10.500 --> 00:51:12.350
team of developers.