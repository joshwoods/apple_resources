WEBVTT

00:00:12.100 --> 00:00:12.500
Hello.

00:00:12.500 --> 00:00:17.340
Welcome to Maximizing
Java Virtual Machine Performance.

00:00:17.460 --> 00:00:19.300
This session will be
given by three of us,

00:00:19.340 --> 00:00:23.030
myself, Victor Hernandez,
and Roger Hoover from the

00:00:23.040 --> 00:00:25.580
Java Virtual Machine team,
and also Christy Warren,

00:00:25.600 --> 00:00:29.910
who is the responsible person
for responsiveness in Tiger.

00:00:30.370 --> 00:00:33.580
So what we'll be talking about
is the Hotspot Virtual Machine in

00:00:33.580 --> 00:00:38.110
Mac OS X that is used to actually
execute your Java applications.

00:00:38.200 --> 00:00:41.060
This Hotspot
Java Virtual Machine comes from Sun,

00:00:41.110 --> 00:00:44.800
and we take that source,
and we tailor it for Mac OS X and

00:00:44.800 --> 00:00:47.960
optimize it specifically for PowerPC.

00:00:48.410 --> 00:00:54.400
Currently, we are supporting J2SC 142,
and as announced yesterday,

00:00:54.430 --> 00:00:57.880
we will be supporting J2SC 5.0.

00:00:58.200 --> 00:01:01.940
Everywhere throughout this talk,
we'll be referring to it as Java 150,

00:01:01.950 --> 00:01:07.980
which is what we all know it as,
but the official name is J2SC 5.0.

00:01:08.540 --> 00:01:13.000
So what do you get with the Hotspot
Virtual Machine on Mac OS X?

00:01:13.000 --> 00:01:15.500
You get a variety of features.

00:01:15.500 --> 00:01:20.020
You get a client just-in-time compiler,
a variety of garbage

00:01:20.020 --> 00:01:23.600
collection algorithms,
an implementation of class data sharing,

00:01:23.600 --> 00:01:27.890
which was innovated by
Apple starting in Java 1.3.

00:01:28.120 --> 00:01:33.200
You get native G5 support,
also a JDK whose classes are

00:01:33.200 --> 00:01:36.000
optimized specifically for Mac OS X.

00:01:36.000 --> 00:01:38.610
And finally, you get the debugger
and profiler interfaces,

00:01:38.670 --> 00:01:43.440
JVMDI and PI,
which development tools can use

00:01:43.500 --> 00:01:45.980
to analyze your application.

00:01:46.010 --> 00:01:46.960
Uh-oh.

00:01:47.060 --> 00:01:48.790
What's going on?

00:01:49.920 --> 00:01:52.360
This thing is a little confused.

00:01:52.360 --> 00:01:53.260
All right.

00:01:53.320 --> 00:01:56.080
Sorry, I wasn't looking at the slides.

00:01:56.080 --> 00:01:59.980
And now with 1.5,
there are a whole bunch of new features.

00:02:00.000 --> 00:02:02.320
Specifically,
you will be getting new language

00:02:02.610 --> 00:02:05.820
features in the Java language,
which will make your development

00:02:05.880 --> 00:02:10.830
time a lot more-- basically,
it'll simplify a lot of development.

00:02:10.860 --> 00:02:15.550
There's also a new client compiler
feature called SafePoint Polling.

00:02:15.630 --> 00:02:18.640
Startup time should also be a bit faster.

00:02:18.650 --> 00:02:23.440
And there will be explicit
concurrency exposed in the Java APIs.

00:02:23.450 --> 00:02:26.380
Also,
we're really excited about this one.

00:02:26.410 --> 00:02:30.190
The class data sharing implementation
from Apple has been adopted by Sun,

00:02:30.200 --> 00:02:34.640
and it will be available on all
of their platforms in Java 1.5.

00:02:34.820 --> 00:02:38.300
They've not only taken our source,
but they've also optimized it themselves.

00:02:38.300 --> 00:02:41.380
And so what you will be seeing
will be improvements to our

00:02:41.380 --> 00:02:43.560
initial sharing implementation.

00:02:43.780 --> 00:02:46.480
Finally,
there will be a new tools interface,

00:02:46.480 --> 00:02:49.820
which will be replacing the
debugger and profiler interface

00:02:49.820 --> 00:02:51.780
that have been deprecated.

00:02:52.970 --> 00:02:57.260
So Java 1.5 is available for you today.

00:02:57.310 --> 00:03:03.330
It is equivalent to the beta 2 version
that is presently being previewed by Sun.

00:03:03.460 --> 00:03:06.870
It installs on your
Tiger Preview DVD that you got

00:03:06.870 --> 00:03:11.290
at the conference this week,
and you can go ahead and download

00:03:11.290 --> 00:03:14.020
it from connect.apple.com.

00:03:15.980 --> 00:03:19.900
So today's talk will be
divided into three parts.

00:03:20.040 --> 00:03:24.110
First, Roger Hoover will be discussing
what is new in Java 1.5.

00:03:24.200 --> 00:03:26.100
The second part,
I will be discussing how the

00:03:26.130 --> 00:03:29.870
Hotspot Virtual Machine optimizes
your application.

00:03:29.940 --> 00:03:32.540
And then finally,
Christy Warren will be introducing a

00:03:32.590 --> 00:03:40.110
very exciting new Mac OS X application
for profiling your Java applications.

00:03:40.190 --> 00:03:41.940
So here's Roger.

00:03:46.690 --> 00:03:48.020
Hello.

00:03:48.680 --> 00:03:51.160
I'm going to give you a brief
overview of the new and pretty

00:03:51.160 --> 00:03:54.050
exciting things that are in 1.5.

00:03:54.500 --> 00:03:56.690
I was over at Java 1
a little bit earlier,

00:03:56.690 --> 00:04:00.000
and they've got whole talks
for each of my slides.

00:04:00.000 --> 00:04:04.780
So this is going to be very high level,
very quick, but I'm doing this to get you

00:04:04.780 --> 00:04:08.700
interested in new things in 1.5
if you haven't seen them before,

00:04:08.700 --> 00:04:12.220
and also to point you to where to
find more information about those

00:04:12.220 --> 00:04:14.600
pieces that you're interested in.

00:04:14.720 --> 00:04:19.070
So, biggest changes pretty much
in the history of Java.

00:04:19.440 --> 00:04:22.050
There are lots of language changes,
and I'll go into those

00:04:22.050 --> 00:04:23.400
in the coming slides.

00:04:23.460 --> 00:04:26.370
There are also a bunch of
library and runtime changes,

00:04:26.440 --> 00:04:28.600
which are also pretty interesting.

00:04:28.790 --> 00:04:31.600
Note the blue bubble with
the JSR numbers in it.

00:04:31.600 --> 00:04:34.600
This is the Java community process.

00:04:34.600 --> 00:04:38.350
It has these Java specification
request numbers that correspond with

00:04:38.350 --> 00:04:41.490
the specifications for this new stuff,
and if there's something you

00:04:41.660 --> 00:04:42.600
want more information in,
remember the number in the blue bubble.

00:04:42.600 --> 00:04:48.600
And you'll be able to look it up with
the URL that I'll have at the end.

00:04:48.600 --> 00:04:50.440
Okay.

00:04:50.680 --> 00:04:51.600
Why change the language?

00:04:51.600 --> 00:04:55.860
Well, there are some great things in here
that I think are going to give a lot of

00:04:55.860 --> 00:04:58.360
improvements in programmer productivity.

00:04:58.890 --> 00:05:01.500
Most of these changes
are handled by Java C,

00:05:01.500 --> 00:05:04.450
but there are a few
things that touch the VM.

00:05:04.450 --> 00:05:10.580
There's a flag-source 1.5 for
Java C that turns these things on.

00:05:10.600 --> 00:05:14.720
It was not the default in earlier betas,
but with beta 2,

00:05:14.720 --> 00:05:18.420
the stuff that we're giving you,
it is the default.

00:05:18.850 --> 00:05:20.590
There's a new keyword called enum.

00:05:20.600 --> 00:05:24.550
If you used enum as an identifier
in the past in your old code,

00:05:24.610 --> 00:05:28.340
you're going to have to say -source
1.4 to turn off these things in

00:05:28.340 --> 00:05:30.600
order to compile your old code.

00:05:30.640 --> 00:05:31.040
Okay.

00:05:31.070 --> 00:05:33.600
Let's look at some of
these great features.

00:05:33.600 --> 00:05:36.600
The first one is auto-boxing
and auto-unboxing.

00:05:36.600 --> 00:05:38.550
Well, what's boxing?

00:05:38.760 --> 00:05:44.100
Consider a primitive type like
int or boolean to be an unboxed

00:05:44.100 --> 00:05:48.600
type and capital integer and
capital boolean to be a box type.

00:05:48.600 --> 00:05:52.600
Namely, the primitive object is
inside an object box.

00:05:52.600 --> 00:05:56.210
So with 1.4,
if you used the box types and

00:05:56.210 --> 00:06:00.600
the unboxed types together,
you had to do lots of conversions.

00:06:00.600 --> 00:06:04.310
In this example here,
I show you are all the time creating

00:06:04.310 --> 00:06:06.520
the new containers for things.

00:06:06.600 --> 00:06:10.590
With 1.5, the compiler does this for you.

00:06:10.600 --> 00:06:14.600
It type checks it,
codes much more readable, simple.

00:06:14.690 --> 00:06:16.600
This is excellent for anybody
dealing with these things.

00:06:19.030 --> 00:06:19.840
Generics.

00:06:19.900 --> 00:06:22.900
People have talked about doing
generic types for a while in Java.

00:06:23.130 --> 00:06:24.940
It's finally here.

00:06:25.150 --> 00:06:30.180
Before you had the dilemma in Java of
either writing a very general type using

00:06:30.180 --> 00:06:34.740
object and then having to do all these
casts in and out and worrying about

00:06:35.660 --> 00:06:40.100
doing the type checking or having runtime
cast exceptions when you did it wrong,

00:06:40.100 --> 00:06:45.620
or you could make something very
specific and not be able to reuse them.

00:06:45.620 --> 00:06:50.390
Now you can write the code with
parameters for the types that are

00:06:50.390 --> 00:06:55.350
embedded in the data type and get
the reusability and get the safe

00:06:55.360 --> 00:06:59.340
type checking from the compiler,
which then generates the cast.

00:06:59.340 --> 00:07:01.740
This is for object types only.

00:07:01.740 --> 00:07:06.700
You can't use primitive
types as the parameters.

00:07:07.190 --> 00:07:12.140
Here's an example, a pair that takes an
arbitrary left and right type,

00:07:12.140 --> 00:07:15.460
capital L, capital R,
a constructor for that.

00:07:15.460 --> 00:07:20.060
And public accessor
functions for looking inside.

00:07:20.060 --> 00:07:24.500
And then at the bottom two lines,
the next to last line has a

00:07:24.500 --> 00:07:29.180
place where we've created,
we're creating a new pair

00:07:29.180 --> 00:07:31.380
of capital integer string.

00:07:31.390 --> 00:07:37.420
And note that the 17 in that line there
is going to be auto-boxed for you,

00:07:37.420 --> 00:07:40.020
so these features interact.

00:07:40.020 --> 00:07:45.290
And also a teaser at the last line there,
we can now do C-style printing,

00:07:45.300 --> 00:07:45.300
and I'll get to that in a minute.

00:07:45.300 --> 00:07:49.300
But there's where we're
calling the accessor functions

00:07:49.300 --> 00:07:51.180
to pull the values out.

00:07:51.770 --> 00:07:55.440
Those of you who know C++,
here's equivalent code that does

00:07:55.440 --> 00:07:57.600
exactly the same thing in C++.

00:07:57.600 --> 00:08:01.170
The major difference is that C++
compilers typically instantiate

00:08:01.480 --> 00:08:07.400
the template for every instance,
and thus you can use primitive types,

00:08:07.400 --> 00:08:08.590
and you can't do that in Java.

00:08:08.620 --> 00:08:12.390
So here we're using Int and Care Star.

00:08:13.030 --> 00:08:15.460
Okay, another feature, static import.

00:08:15.720 --> 00:08:16.580
Why static import?

00:08:16.780 --> 00:08:20.220
Well,
there's two reasons why you'd want this.

00:08:20.390 --> 00:08:24.030
One reason is that it eliminates
a binary compatibility issue with

00:08:24.300 --> 00:08:25.600
importing an entire interface.

00:08:25.600 --> 00:08:29.400
You're just pulling out the static
methods and fields of another class,

00:08:29.400 --> 00:08:30.900
and so it's simpler for the compiler.

00:08:30.900 --> 00:08:33.530
But probably the main reason
that you want to use this

00:08:33.570 --> 00:08:36.960
is it simplifies the naming,
because you can actually import those

00:08:36.960 --> 00:08:38.900
names into the current namespace.

00:08:38.900 --> 00:08:44.100
So in 1.4, if you were going to use this
MyMath class that has this

00:08:44.100 --> 00:08:48.520
constant pi and this method times,
you have to be saying MyMath.this

00:08:48.610 --> 00:08:50.360
and MyMath.that all the time.

00:08:50.360 --> 00:08:55.520
But in 1.5, if you do an import static,
you can use pi and times

00:08:55.780 --> 00:08:57.860
without qualification.

00:08:57.860 --> 00:08:59.480
So simpler code.

00:08:59.490 --> 00:09:00.880
Compiler does all the work.

00:09:00.880 --> 00:09:02.880
to make it right.

00:09:04.270 --> 00:09:05.870
For loop has been enhanced.

00:09:05.990 --> 00:09:11.420
Instead of having to specify the
induction variable in the loop,

00:09:11.440 --> 00:09:15.100
if you're using arrays or a
new Java lang iterable type,

00:09:15.110 --> 00:09:22.690
you can simply say for
type variable colon

00:09:23.150 --> 00:09:27.220
Well, here's an example of a string
concatenation in the old method.

00:09:27.650 --> 00:09:29.400
Here's what you can write now.

00:09:29.550 --> 00:09:34.160
You simply name the variable that
represents each iteration of the array,

00:09:34.160 --> 00:09:38.360
the piece of data you're interested in,
and you can use it in the loop.

00:09:38.520 --> 00:09:42.320
Again, simpler code,
compiler does the work for you.

00:09:43.790 --> 00:09:47.200
I showed you the printf example before.

00:09:47.200 --> 00:09:50.320
This is enabled by
variable-arity methods.

00:09:50.340 --> 00:09:52.820
You can do this.

00:09:52.820 --> 00:09:54.370
Here's an example.

00:09:54.620 --> 00:10:02.940
You can say type name dot dot
dot in a method definition,

00:10:02.970 --> 00:10:07.140
and the compiler automatically
converts that into an array.

00:10:07.290 --> 00:10:10.510
So you simply use it as an array.

00:10:11.000 --> 00:10:14.620
This function here just
concatenates-- or no,

00:10:14.640 --> 00:10:17.840
it chooses a maximum of a bunch
of strings that you give it.

00:10:18.000 --> 00:10:20.750
But you could pass any
number of strings to it.

00:10:21.720 --> 00:10:24.260
Enumerations.

00:10:24.520 --> 00:10:26.500
This is similar to the enum type in C.

00:10:26.800 --> 00:10:32.670
You specify a bunch of constants that
get instantiated by the compiler.

00:10:33.590 --> 00:10:40.320
And you can use it as such by saying,
in this case, myColor.yellow picks

00:10:40.320 --> 00:10:41.360
out an individual one.

00:10:41.360 --> 00:10:44.240
But this also interacts
with static import.

00:10:44.380 --> 00:10:47.440
And if you import this stuff,
you can actually just

00:10:47.440 --> 00:10:49.760
talk about yellow and red.

00:10:49.760 --> 00:10:55.360
But there's a lot more to enumerations
than simply a list of constants.

00:10:55.480 --> 00:10:58.870
You can actually have
methods inside enumerations.

00:10:58.900 --> 00:11:03.380
Here's an example of where I've taken the
color and defined another enumeration,

00:11:03.380 --> 00:11:07.110
fruit,
which has a method that does a switch on

00:11:07.120 --> 00:11:11.360
the type of fruit and returns the color.

00:11:11.520 --> 00:11:13.290
Now,
note that we were able to say red and

00:11:13.290 --> 00:11:15.260
yellow because we did a static import.

00:11:15.380 --> 00:11:19.200
The third one, orange,
is actually both a fruit and a color.

00:11:19.280 --> 00:11:19.320
And if I add a color,
it's a color that's actually

00:11:19.330 --> 00:11:19.520
a color that's actually a
color that's actually a color.

00:11:19.520 --> 00:11:23.660
And if I had just said orange
there instead of myColor.orange,

00:11:23.770 --> 00:11:27.830
the compiler would have complained
that that was ambiguous.

00:11:27.850 --> 00:11:35.520
So we can go on and in another
file import both color and fruit

00:11:35.520 --> 00:11:36.800
and do computations on them.

00:11:36.800 --> 00:11:40.000
Here I'm doing apple.myColor.

00:11:40.000 --> 00:11:42.000
That'll return the color of an apple.

00:11:42.000 --> 00:11:46.840
And because the compiler keeps
these things as unique instances,

00:11:46.840 --> 00:11:49.280
you can do equal equal
on enumeration types.

00:11:49.280 --> 00:11:52.090
and it gives you equality.

00:11:52.590 --> 00:11:55.700
Okay, another big thing is metadata.

00:11:55.750 --> 00:12:00.130
This is currently not hooked in with the
metadata stuff you heard about yesterday,

00:12:00.210 --> 00:12:03.500
Spotlight, although we'd love to do that
at some point in the future.

00:12:03.500 --> 00:12:07.420
But this is metadata inside the
Java program that allows you to add

00:12:07.520 --> 00:12:10.400
additional information into your program.

00:12:10.570 --> 00:12:14.500
And there are three parts
in order to make this work.

00:12:14.630 --> 00:12:18.820
There are declarations,
there are annotations, which, well,

00:12:19.110 --> 00:12:22.480
declarations say what you're
going to keep track of.

00:12:22.700 --> 00:12:29.500
Annotations say where you use that and
you instantiate that inside the program.

00:12:29.500 --> 00:12:33.100
And then runtime access,
you can write programs that actually

00:12:33.100 --> 00:12:35.500
look at this metadata via reflection.

00:12:35.610 --> 00:12:41.360
And the good thing about this is that
it eliminates the encoding of data into

00:12:41.360 --> 00:12:43.500
flag classes like Jack's RPC stuff did.

00:12:43.500 --> 00:12:47.460
Just to indicate that these
are special functions.

00:12:47.500 --> 00:12:49.500
You can do this more
cleanly with metadata.

00:12:49.500 --> 00:12:54.440
It doesn't have the compiler implications
of being dependent upon other classes.

00:12:54.530 --> 00:12:58.500
And this will be used for programming,
documentation, tools.

00:12:58.500 --> 00:13:02.500
I suspect we'll see a lot of
neat stuff that uses metadata.

00:13:02.500 --> 00:13:04.490
So how does it work?

00:13:04.490 --> 00:13:07.720
A metadata declaration is similar
to an interface declaration,

00:13:07.720 --> 00:13:12.470
except you say @interface
instead of interface.

00:13:12.510 --> 00:13:15.480
So you can give a bunch of members
value is going to be special,

00:13:15.500 --> 00:13:16.500
and I'll talk about that in a minute.

00:13:16.750 --> 00:13:19.500
And you can give default values.

00:13:19.540 --> 00:13:20.480
So here's an example.

00:13:20.500 --> 00:13:23.500
We've got a bunch of bugs in our code.

00:13:23.500 --> 00:13:30.610
And so we define metadata called fixme.

00:13:30.900 --> 00:13:53.600
[Transcript missing]

00:13:53.850 --> 00:13:57.630
Okay, so once we have the declaration,
then we need an annotation

00:13:57.640 --> 00:14:01.660
in order to place this,
and we can place the annotations before

00:14:01.660 --> 00:14:03.700
any declaration in our Java program.

00:14:03.700 --> 00:14:10.110
There's a special way with a file that
we can put a package annotation because

00:14:10.110 --> 00:14:14.540
Java really doesn't have a declaration
of a package that's explicit in code.

00:14:14.700 --> 00:14:17.660
And we can also put these
in front of enum constants.

00:14:17.730 --> 00:14:18.700
So what do they look like?

00:14:18.700 --> 00:14:21.570
Well, here's several in a piece of code.

00:14:21.750 --> 00:14:23.720
I've got this class
called perpetual motion,

00:14:23.800 --> 00:14:28.700
and you'll note that the thing is
preceded by a fix-me annotation

00:14:28.700 --> 00:14:34.700
that says there's no such thing,
and you get a holiday if you fix it.

00:14:34.970 --> 00:14:44.690
Well, inside we have a method sum that
also has a fix-me annotation,

00:14:44.810 --> 00:14:46.700
and this time I just have a string.

00:14:46.700 --> 00:14:47.700
Why does sum subtract?

00:14:47.700 --> 00:14:50.080
Well,

00:14:50.450 --> 00:14:53.670
If you don't specify a member name,
it assumes value.

00:14:53.680 --> 00:14:55.300
That's what's special about value.

00:14:55.300 --> 00:14:59.140
And I didn't have to say what the reward
was because there's a default on that.

00:14:59.350 --> 00:15:01.540
It'll use "cookie" as the default reward.

00:15:01.680 --> 00:15:07.530
And finally, notice the debug annotation
in the last line.

00:15:07.960 --> 00:15:12.050
And since debug has no members,
I don't have to do the open/close parens,

00:15:12.060 --> 00:15:16.550
so I can just simply and cleanly
put those where I need them.

00:15:16.750 --> 00:15:20.630
Okay, so how do I use these things?

00:15:20.730 --> 00:15:23.580
Well, I can write tools that actually
look at the source to use these

00:15:23.580 --> 00:15:25.750
because it just gets compiled out.

00:15:25.880 --> 00:15:28.990
But I can also look at them
at runtime via reflection.

00:15:29.110 --> 00:15:34.980
And there's another special annotation
called @retention that is used to tell

00:15:34.980 --> 00:15:39.360
the compiler to retain that and put it in
the class file so reflection can find it.

00:15:39.460 --> 00:15:43.270
So in my previous definition
or declaration of fixme,

00:15:43.370 --> 00:15:48.850
if I had said @retention blah, blah,
blah, it would remember this

00:15:48.850 --> 00:15:51.660
stuff for runtime access.

00:15:51.700 --> 00:15:55.430
And in this example,
I'm using reflection to get at the

00:15:55.770 --> 00:15:59.550
method that corresponds with sum,
and I do a .get annotation.

00:15:59.560 --> 00:16:02.050
So that gives me back an
array of these annotations,

00:16:02.150 --> 00:16:05.560
and then I use the enhanced for loop
just to print them out on the screen.

00:16:05.560 --> 00:16:10.550
So this is how you'd write tools
to use the metadata information.

00:16:10.980 --> 00:16:14.210
Great things for people who
write concurrent programs

00:16:14.270 --> 00:16:16.940
with multiple threads.

00:16:17.150 --> 00:16:18.660
There are some new classes.

00:16:18.960 --> 00:16:22.250
I'm going to kind of
look at these inside out.

00:16:22.430 --> 00:16:29.800
The Java Udall Concurrent Atomic does
single access atomic on individual

00:16:29.800 --> 00:16:32.930
variables that's exposed in this API.

00:16:33.070 --> 00:16:37.060
On top of that,
there's locks that are built on those.

00:16:37.350 --> 00:16:43.420
Those are completely independent
of Java internal synchronization.

00:16:43.470 --> 00:16:47.100
Then there's Java Udall Current that
has a bunch of classes that are pretty

00:16:47.220 --> 00:16:48.620
useful that's built on all of this.

00:16:48.620 --> 00:16:53.710
This was done through
JSR by Doug Lee and company,

00:16:53.710 --> 00:16:55.640
some of you know of.

00:16:56.080 --> 00:17:00.140
Here's a brief overview of
some of the things you can do.

00:17:00.170 --> 00:17:02.220
In threads,
there's an executor interface.

00:17:02.220 --> 00:17:05.710
It gives you fairly convenient thread
pools without doing lots of work.

00:17:05.830 --> 00:17:08.140
There are lots of
different kinds of queues.

00:17:08.140 --> 00:17:12.540
There's nano time for nanosecond
clock time within a given JVM,

00:17:12.540 --> 00:17:15.520
which performance people
are going to love.

00:17:15.730 --> 00:17:18.270
Lots of synchronization
primitives that more match the

00:17:18.360 --> 00:17:21.450
literature than what's in Java,
so you can implement

00:17:21.890 --> 00:17:24.900
published algorithms easier,
and concurrent access to

00:17:24.900 --> 00:17:26.370
various kinds of collections.

00:17:26.460 --> 00:17:28.400
This is great stuff.

00:17:29.370 --> 00:17:32.760
Also,
in terms of multi-threaded programming,

00:17:32.760 --> 00:17:34.690
a new Java memory model.

00:17:34.730 --> 00:17:39.350
It says what to expect with
multiple threaded program

00:17:39.350 --> 00:17:42.070
accessing shared storage.

00:17:42.200 --> 00:17:46.800
There's a specification in the
original thread specification for

00:17:46.800 --> 00:17:51.120
Java that was widely ignored because
it says you can't do things that were

00:17:51.120 --> 00:17:55.480
widely done by optimizing compilers,
as well as processors like the

00:17:55.480 --> 00:17:58.330
PowerPC that reorder instructions.

00:17:58.350 --> 00:18:01.470
What this does is it presents
a realistic model of what can

00:18:01.470 --> 00:18:03.260
happen before other things.

00:18:03.260 --> 00:18:09.260
It also guarantees that when you build
a new object that the final fields

00:18:09.260 --> 00:18:13.180
are set when you exit the constructor,
so you don't see an

00:18:13.180 --> 00:18:14.490
intermediate state there.

00:18:14.680 --> 00:18:16.420
Practically speaking,
what does this mean?

00:18:16.640 --> 00:18:22.510
Well, we're going to make the Apple 1.5
JVM obey the Java memory model,

00:18:22.560 --> 00:18:24.090
so you'll be able to count on it.

00:18:24.220 --> 00:18:29.400
And in particular, you really have to use
synchronization any time you're

00:18:29.400 --> 00:18:31.300
mucking with shared storage.

00:18:31.350 --> 00:18:33.260
Either the Java synchronizer
or the Java virtual machine.

00:18:33.260 --> 00:18:35.310
You can use the same thing,
but you really have to

00:18:35.310 --> 00:18:36.440
use synchronization.

00:18:36.530 --> 00:18:40.890
Also, if you're doing a multi-threaded
thing where one thread sets up a

00:18:40.890 --> 00:18:44.780
bunch of data and then a bunch of
other threads take off and start

00:18:44.860 --> 00:18:49.090
working on it when everything's done,
that variable that says that

00:18:49.090 --> 00:18:53.340
things are done has to be volatile,
where the write of that volatile

00:18:53.340 --> 00:18:57.440
by one thread releases all of the
things that were done before it.

00:18:57.890 --> 00:19:02.820
And the read of that volatile in the
other threads acquires all of that stuff.

00:19:02.850 --> 00:19:03.260
This is the way it works.

00:19:03.260 --> 00:19:06.900
This is the standard way you
should use thread communication

00:19:06.900 --> 00:19:08.470
via shared variables.

00:19:08.480 --> 00:19:10.320
They've got to be volatile.

00:19:10.340 --> 00:19:13.540
Otherwise, don't be surprised if
things happen out of order.

00:19:13.570 --> 00:19:15.440
In particular,
things that you've tested and

00:19:15.450 --> 00:19:20.440
debugged on a multiprocessor G4 are
most likely going to fail at some

00:19:20.440 --> 00:19:25.390
point when you run them on a G5
if you haven't followed the rules.

00:19:25.760 --> 00:19:31.080
There's a new tool interface
that replaces JVMPI and DI,

00:19:31.080 --> 00:19:40.090
which are now deprecated and going away,
presumably in the next version of Java.

00:19:40.090 --> 00:19:40.090
This has a slew of--

00:19:40.730 --> 00:19:43.430
functionality that lets
you implement these tools,

00:19:43.600 --> 00:19:45.960
plus hopefully a whole lot more.

00:19:45.960 --> 00:19:48.970
Basically,
agents plug into the JVM -- these are

00:19:48.970 --> 00:19:54.370
C++ programs or at least a piece of
your program -- that say which callbacks

00:19:54.370 --> 00:19:59.090
they want to get and then are notified,
and there's also a whole bunch of

00:19:59.090 --> 00:20:00.890
functions that they can query the VM.

00:20:00.910 --> 00:20:07.400
So it will be exciting to see what comes
of that in the coming months and years.

00:20:07.400 --> 00:20:11.090
There's a new monitoring and management
interface that's basically designed

00:20:11.090 --> 00:20:15.210
so you can do things like load
balancing in server environments.

00:20:15.250 --> 00:20:18.590
You can look at the memory
usage in classes and thread

00:20:18.590 --> 00:20:20.660
information inside the JVM.

00:20:20.660 --> 00:20:25.900
You can look at number of processors,
CPU utilization in the OS,

00:20:25.970 --> 00:20:27.690
things like that.

00:20:27.860 --> 00:20:29.640
And finally,
there's lots more things that

00:20:29.690 --> 00:20:31.100
I don't have time to talk about.

00:20:31.310 --> 00:20:35.510
Here's a list of some more things
that you may be interested in,

00:20:35.510 --> 00:20:40.710
and note the last two lines, the URLs,
the next to last line,

00:20:40.710 --> 00:20:44.030
fill in the JSR number and
you'll find out where to get the

00:20:44.030 --> 00:20:48.610
specs for that particular JSR,
and the Sun site, the last line,

00:20:48.610 --> 00:20:50.990
also has some great information.

00:20:58.010 --> 00:21:00.700
Thank you, Roger.

00:21:00.830 --> 00:21:04.390
So I will be going over some
implementation details of

00:21:04.390 --> 00:21:06.200
the Hotspot Virtual Machine.

00:21:06.310 --> 00:21:11.230
That should give you a better idea of how
your application can be better optimized

00:21:11.500 --> 00:21:14.510
when running specifically on Mac OS X.

00:21:17.420 --> 00:21:20.480
So what do you get with the
Hotspot Java Virtual Machine?

00:21:20.480 --> 00:21:23.840
You get a client-just-in-time compiler,
a variety of garbage

00:21:23.840 --> 00:21:27.300
collection algorithms,
automatic G5 optimization,

00:21:27.300 --> 00:21:30.700
and the sharing of class data
between JVM instances when you

00:21:30.700 --> 00:21:34.500
have multiple Java applications
running on the same machine.

00:21:34.500 --> 00:21:39.340
You also get the tuned JRE implementation
that we have done for Mac OS X.

00:21:39.340 --> 00:21:42.290
So let's talk a little
about the client compiler.

00:21:43.040 --> 00:21:47.890
The Client Compiler dynamically
compiles your application's hot methods.

00:21:47.950 --> 00:21:51.650
After you've called a particular
method a certain amount of times,

00:21:51.760 --> 00:21:57.480
we go ahead and stop executing in the
interpreter and continue execution in

00:21:57.480 --> 00:22:01.330
a JIT-compiled version of that method.

00:22:01.360 --> 00:22:06.260
We have personally optimized the
client compiler from Sun for PowerPC.

00:22:06.470 --> 00:22:09.400
That means that we've come up
with optimal code sequences

00:22:09.760 --> 00:22:17.360
for each Java bytecode,
and we've also figured out

00:22:17.360 --> 00:22:17.360
how to make best use of the
full PowerPC register set.

00:22:17.900 --> 00:23:27.000
[Transcript missing]

00:23:28.280 --> 00:23:29.960
So what exactly is inlining?

00:23:30.040 --> 00:23:31.180
It's pretty straightforward.

00:23:31.180 --> 00:23:36.140
In the example I have up on the slides,
you got average, it calls sum.

00:23:36.160 --> 00:23:40.000
There's extra overhead of
actually calling the function sum.

00:23:40.300 --> 00:23:42.350
So ideally,
you would want to have a situation

00:23:42.690 --> 00:23:48.620
where the body of sum is just inlined
right into the body of average.

00:23:48.650 --> 00:23:50.760
You don't want to have to
do that in your own code,

00:23:50.890 --> 00:23:55.240
because the code is not as extensible.

00:23:55.250 --> 00:23:57.890
So we do it for you dynamically.

00:23:58.280 --> 00:24:00.200
In what situations can we do it?

00:24:00.370 --> 00:24:03.000
Well,
there's a few opportunities that are very

00:24:03.000 --> 00:24:04.980
straightforward to be able to do this.

00:24:05.090 --> 00:24:07.160
One, our field accessor methods.

00:24:07.310 --> 00:24:14.630
There's no reason for you to access
directly fields via their names.

00:24:14.630 --> 00:24:16.960
You can use methods to do that.

00:24:17.040 --> 00:24:20.910
And also,
constructors in your Java classes.

00:24:21.090 --> 00:24:24.540
We're also able to inline
a bunch of intrinsics.

00:24:24.630 --> 00:24:28.240
Intrinsic methods are methods that
we don't even need to look at the

00:24:28.240 --> 00:24:29.980
implementation of the bytecodes.

00:24:30.110 --> 00:24:32.600
We know what the behavior
of that method is,

00:24:32.600 --> 00:24:39.380
and we go ahead and execute it in the
optimal PowerPC code sequence for those.

00:24:39.670 --> 00:24:41.520
We know how to do array copy.

00:24:41.730 --> 00:24:44.400
So this basically applies to JDK classes.

00:24:44.400 --> 00:24:49.940
And we keep on adding methods as
they become either possible to come

00:24:50.000 --> 00:24:56.390
up with an optimal code sequence or
they're used more heavily in the JDK.

00:24:57.200 --> 00:25:03.210
and there's examples of a
bunch of intrinsic methods.

00:25:03.340 --> 00:25:08.310
Well, this doesn't include a huge
set of methods that are

00:25:08.310 --> 00:25:11.360
used in your applications,
and that's--

00:25:11.790 --> 00:25:13.180
virtual methods.

00:25:13.310 --> 00:25:16.850
As I said before,
it is possible that the target of an

00:25:17.000 --> 00:25:22.280
invoke virtual bytecode is actually
not always the exact same method.

00:25:22.280 --> 00:25:26.870
It depends on what variety of
implementations of those virtual

00:25:26.870 --> 00:25:29.870
methods have actually been loaded.

00:25:30.070 --> 00:25:34.870
But it turns out that we actually are
able to inline those if we know that

00:25:34.960 --> 00:25:39.640
there has only been one implementation of
that virtual method that has been loaded,

00:25:39.670 --> 00:25:43.370
in which case that method can
be considered monomorphic.

00:25:43.800 --> 00:25:48.770
It turns out that in most usage patterns,
this is actually the case.

00:25:48.870 --> 00:25:52.720
So we're actually hitting a
large percentage of invoke

00:25:52.850 --> 00:25:55.630
virtuals with this optimization.

00:25:56.470 --> 00:25:58.590
There are, however, limitations to this.

00:25:58.670 --> 00:26:01.640
Since we're able to
inline so many methods,

00:26:01.640 --> 00:26:05.400
the limiting factor no longer becomes
finding opportunities to inline,

00:26:05.480 --> 00:26:08.300
but actually the size
of the compiled method.

00:26:08.400 --> 00:26:12.400
So if your hot method
happens to be really large,

00:26:12.430 --> 00:26:15.230
it might not be able to
inline methods that it calls,

00:26:15.330 --> 00:26:19.300
or it might not be able to
be inlined by its callers.

00:26:19.380 --> 00:26:23.280
You also need to be aware of the fact
that we are unable to inline methods

00:26:23.280 --> 00:26:27.320
that are synchronized and also methods
that are with exception handlers.

00:26:27.320 --> 00:26:30.830
There's a limitation
in the client compiler.

00:26:31.640 --> 00:26:35.940
And I want to leave you with the tip
that in previous versions of Java,

00:26:36.090 --> 00:26:41.840
it was necessary or it was useful to
use the final keyword to basically

00:26:41.840 --> 00:26:44.520
to make a virtual method in-lineable.

00:26:44.610 --> 00:26:46.200
And now that's not needed at all.

00:26:46.530 --> 00:26:51.430
And you should basically be using final
only for its object-oriented purpose.

00:26:53.250 --> 00:26:54.740
Okay, safe point polling.

00:26:54.780 --> 00:26:59.620
There's a new feature in the
client compiler in Hotspot 1.5.

00:26:59.700 --> 00:27:00.940
What is a safe point?

00:27:00.990 --> 00:27:04.270
A safe point is the state that
a Java thread needs to reach

00:27:04.600 --> 00:27:06.310
for exact garbage collection.

00:27:06.420 --> 00:27:09.350
Specifically,
the location of all Java objects

00:27:09.350 --> 00:27:11.690
needs to be known at that location.

00:27:12.530 --> 00:27:19.510
Currently, compiled methods reach a safe
point in Java 1.4.2 as follows.

00:27:19.670 --> 00:27:25.260
Basically, you have a Java thread
executing through compiled code,

00:27:25.260 --> 00:27:29.830
and the virtual machine has
to hard suspend that thread,

00:27:29.900 --> 00:27:33.560
make a copy of the code,
and then insert traps at

00:27:33.660 --> 00:27:38.550
pre-designated locations,
and then continue executing at

00:27:38.570 --> 00:27:41.630
the previous location in the copy.

00:27:42.500 --> 00:27:45.210
You actually hit one of the traps,
at which point the virtual

00:27:45.210 --> 00:27:48.130
machine can take over,
and it's known that it's at a safe point.

00:27:48.240 --> 00:27:51.610
The fact that we're hard
suspending the threads makes

00:27:51.670 --> 00:27:57.290
this possibly a dangerous thing,
and it also requires a lot of

00:27:57.290 --> 00:28:01.150
extra overhead in the client
compiler to keep track of the

00:28:01.160 --> 00:28:03.440
locations to insert traps at.

00:28:04.410 --> 00:28:08.140
This is now greatly simplified in 1.5.

00:28:08.190 --> 00:28:11.280
What we're doing in 1.5 is
basically you have your compiled

00:28:11.280 --> 00:28:14.670
method that is currently executing
through all the instructions,

00:28:14.670 --> 00:28:19.090
and every once in a while,
there is an access to a safe point page

00:28:19.090 --> 00:28:21.830
in memory that basically is a no-op.

00:28:21.910 --> 00:28:24.690
It's not reading or writing
anything that's actually useful.

00:28:24.700 --> 00:28:27.500
But at some point,
the virtual machine decides

00:28:27.500 --> 00:28:30.480
to memory protect that page,
and so the next time you

00:28:30.590 --> 00:28:33.730
come around to that access,
it actually hits a trap,

00:28:33.730 --> 00:28:36.460
and the virtual machine
is able to take over.

00:28:36.500 --> 00:28:41.570
This will result in much
more optimal -- basically,

00:28:41.660 --> 00:28:48.560
the overhead taken for your compiled
methods to be able to get to in a state

00:28:48.560 --> 00:28:54.100
for garbage collection will be greatly
improved in Java 1.5 because of this.

00:28:54.100 --> 00:28:55.360
Thank you.

00:28:56.220 --> 00:28:59.820
Okay, let's talk a little about
garbage collection in Hotspot.

00:28:59.880 --> 00:29:01.640
Right now,
we're currently supporting three

00:29:01.640 --> 00:29:03.700
different garbage collection algorithms.

00:29:03.950 --> 00:29:10.020
They're each designed to meet needs
of different kinds of applications.

00:29:10.170 --> 00:29:15.100
There is no longer this notion of one
garbage collector to meet all needs.

00:29:15.100 --> 00:29:18.600
So, the original garbage collector,
the Sierra garbage collector

00:29:18.710 --> 00:29:21.920
that you're familiar with
from Hotspot since Java 1.2,

00:29:21.920 --> 00:29:24.380
or even before then,
is still there and it still

00:29:24.380 --> 00:29:26.100
is the default collector.

00:29:26.100 --> 00:29:28.400
But since Java 1.4,
there have been two garbage

00:29:28.400 --> 00:29:29.980
collection algorithms introduced.

00:29:30.380 --> 00:29:33.100
First, there's the concurrent
mark-and-sweep algorithm,

00:29:33.100 --> 00:29:39.100
which has been designed to have a
higher throughput for larger Java heaps.

00:29:39.100 --> 00:29:42.120
And there's also the
parallel scavenge algorithm,

00:29:42.190 --> 00:29:45.100
which is designed to have
a shorter pause time.

00:29:45.610 --> 00:29:50.440
What I recommend to you is to run your
application with all three and also

00:29:50.510 --> 00:29:55.860
change the Java heap parameters to see
where you can fine-tune your application.

00:29:56.040 --> 00:30:01.170
There is not one garbage collection
algorithm that will apply to everybody.

00:30:02.190 --> 00:30:07.000
There are a few small changes to garbage
collection happening in Java 1.5.

00:30:07.180 --> 00:30:09.730
The main thing is that the
parallel scavenge collector

00:30:09.730 --> 00:30:13.860
will now be the default in all
Mac OS X server installations.

00:30:13.970 --> 00:30:17.840
This is similar to Sun's
approach at garbage collection

00:30:17.930 --> 00:30:20.520
configurations for Java 1.5.

00:30:20.760 --> 00:30:24.090
They're also making it so that
all server installations will

00:30:24.090 --> 00:30:26.900
automatically get the parallel GC.

00:30:26.970 --> 00:30:28.440
And we're applying that as well.

00:30:28.440 --> 00:30:32.510
The way they detect that is any
dual CPU machine with greater

00:30:32.510 --> 00:30:37.090
than 2 gigabytes memory gets
classified as a server machine.

00:30:37.090 --> 00:30:39.700
Therefore, it should get the parallel
garbage collector.

00:30:39.870 --> 00:30:43.080
This will definitely be very
good for compatibility between

00:30:43.080 --> 00:30:45.480
various installations of Java 1.5.

00:30:45.820 --> 00:30:50.470
We don't necessarily want all of a sudden
the performance characteristics to be

00:30:50.470 --> 00:30:55.600
different just because you didn't happen
to get parallel scavenge on our platform.

00:30:55.790 --> 00:31:00.350
The other set of differences in
Java 1.5 is the fact that we're

00:31:00.370 --> 00:31:04.080
making it more convenient to you.

00:31:04.140 --> 00:31:09.240
to configure the heap parameters
for performance purposes in each

00:31:09.350 --> 00:31:11.010
garbage collection algorithm.

00:31:11.090 --> 00:31:15.220
Specifically, in parallel scavenge,
you can now designate a percentage

00:31:15.220 --> 00:31:19.310
of time that you hope that your
application is spent during GC.

00:31:19.420 --> 00:31:22.370
Under the covers,
that gets translated to particular

00:31:22.480 --> 00:31:26.930
heap sizes of the permanent generation,
the new size.

00:31:27.270 --> 00:31:30.660
Basically, in the past,
you had to kind of know how these garbage

00:31:30.660 --> 00:31:32.360
collection algorithms were implemented.

00:31:32.380 --> 00:31:36.940
And now, the goal is to abstract
that away for you.

00:31:37.200 --> 00:31:42.410
If you've been using use
adaptive size policy in Java 1.4,

00:31:42.410 --> 00:31:44.990
that also has a few convenience features.

00:31:45.230 --> 00:31:47.060
That also has a few convenience flags.

00:31:47.150 --> 00:31:50.330
Specifically,
you can specify how long you want a

00:31:50.350 --> 00:31:57.140
pause to take and also what ratio of
the time of your full Java application

00:31:57.140 --> 00:32:00.200
is spent doing garbage collection.

00:32:02.510 --> 00:32:07.800
Finally, I want to talk about how we've
optimized Java for the G5.

00:32:07.800 --> 00:32:11.720
These optimizations that we've done on
the G5 require absolutely no code changes

00:32:11.810 --> 00:32:14.100
on your part and also no recompilation.

00:32:14.100 --> 00:32:16.100
Basically,
what we've done is we've taken full

00:32:16.100 --> 00:32:20.460
advantage of the double-word registers
available on the G5 and also all

00:32:20.460 --> 00:32:23.000
of the double-word instructions.

00:32:23.000 --> 00:32:25.910
This has been done both to the
hotspot interpreter and the compiler,

00:32:26.050 --> 00:32:30.290
and there should be big gains overall,
but especially those people who

00:32:30.290 --> 00:32:34.110
are doing arithmetic of longs,
doubles, and floats will see a

00:32:34.110 --> 00:32:36.000
substantial improvement.

00:32:36.120 --> 00:32:37.300
There are specific reasons why.

00:32:37.300 --> 00:32:42.800
I mean, we can do CAS inline
from float to integer.

00:32:42.800 --> 00:32:46.200
We're also -- bit extractions
from those values are much faster,

00:32:46.200 --> 00:32:49.800
and also the square root instruction
is actually available on the G5,

00:32:49.830 --> 00:32:54.600
and we actually call directly into that
instead of writing source for that.

00:32:54.600 --> 00:32:57.150
As you can imagine, that's much faster.

00:32:57.540 --> 00:33:03.640
Also, synchronization has been improved
by taking advantage of the extra

00:33:03.640 --> 00:33:06.640
lightweight synchronization
instruction available on the G5.

00:33:06.640 --> 00:33:13.710
And so we are taking full advantage
of all of the PowerPC instructions

00:33:13.710 --> 00:33:13.710
for synchronization.

00:33:15.270 --> 00:33:18.400
How have we actually been able
to measure that performance gain?

00:33:18.410 --> 00:33:21.300
Well, we've been tracking SCIMARC 2.0.

00:33:21.360 --> 00:33:27.400
SCIMARC 2.0 is a very good example
of a few scientific algorithms.

00:33:27.400 --> 00:33:29.440
It does fast Fourier transform.

00:33:29.440 --> 00:33:31.940
It does the Monte Carlo approximation.

00:33:31.940 --> 00:33:35.260
And you might be familiar with the
composite score numbers from the

00:33:35.260 --> 00:33:37.150
Java State of the Union yesterday.

00:33:37.170 --> 00:33:41.160
But there are a few points that I want to
point out that are different from that.

00:33:41.180 --> 00:33:47.320
But as you can see, these are our scores
on a 1.25 gigahertz G4.

00:33:47.480 --> 00:33:50.680
The 98 number is definitely pretty low.

00:33:50.680 --> 00:33:54.040
And our goal with the G5 is
definitely to get the number up.

00:33:54.110 --> 00:33:56.460
We expect it at least twice as fast.

00:33:56.700 --> 00:34:02.660
The second bar is basically
a pretend 2.5 gigahertz G5,

00:34:02.760 --> 00:34:03.620
just twice as fast.

00:34:03.650 --> 00:34:07.210
The scores are twice as
high as the first numbers.

00:34:07.260 --> 00:34:12.260
What we actually get on the G5 are
something that's substantially larger.

00:34:12.260 --> 00:34:13.930
And we're pretty excited about that.

00:34:14.250 --> 00:34:20.530
This makes the composite score is now
very competitive with scores reported

00:34:20.530 --> 00:34:22.960
on Pentium and other platforms.

00:34:22.960 --> 00:34:24.400
And we're very excited about that.

00:34:24.420 --> 00:34:27.500
The one thing I do want to point out is,
for example,

00:34:27.500 --> 00:34:29.390
why Monte Carlo is still low.

00:34:29.550 --> 00:34:33.970
It turns out that Monte Carlo is doing
unnecessary synchronization by attaching

00:34:34.220 --> 00:34:36.280
synchronize to the front of a method.

00:34:36.320 --> 00:34:39.970
You remove that,
and the score increases dramatically.

00:34:39.970 --> 00:34:43.250
It does increase dramatically
on the G4 as well.

00:34:43.300 --> 00:34:46.890
The reason I want to point that out
is that the client compiler is unable

00:34:47.350 --> 00:34:50.260
to detect unnecessary synchronization.

00:34:50.260 --> 00:34:55.550
And it therefore falls into the hands
of the Java developer to do analysis

00:34:55.620 --> 00:35:01.250
on their application to see if this
could be the case in your application.

00:35:03.880 --> 00:35:04.370
And that's it.

00:35:04.510 --> 00:35:09.300
And I'd like to invite Christy
to introduce Shark for Java.

00:35:15.330 --> 00:35:16.700
Hi, everyone.

00:35:16.700 --> 00:35:18.700
I hope you're having a
good afternoon today.

00:35:18.700 --> 00:35:23.000
I'm here to talk about Shark for
Java and high-level performance analysis.

00:35:23.240 --> 00:35:26.910
How many of you have heard
of Shark or used Shark?

00:35:27.270 --> 00:35:31.100
Wow, I didn't expect quite that many,
so you know about this program.

00:35:31.230 --> 00:35:34.100
But now we're going to show it for Java.

00:35:34.150 --> 00:35:37.100
So I'm going to go through
this part pretty quickly then.

00:35:37.100 --> 00:35:41.100
So Shark is basically the ultimate
profiler you can get on Mac OS X.

00:35:41.110 --> 00:35:43.100
It's a really neat program.

00:35:43.100 --> 00:35:45.820
In the past,
it's been great for analyzing our

00:35:45.820 --> 00:35:48.060
C and C++ and Objective-C programs.

00:35:48.190 --> 00:35:51.100
It does both what's called
cost and use analysis.

00:35:51.130 --> 00:35:53.100
I'll go into that a little more later.

00:35:53.100 --> 00:35:58.100
It can profile a running process,
a thread, or even the entire system.

00:35:58.370 --> 00:36:03.100
And for Java, you know,
it's limited to just a single process.

00:36:03.100 --> 00:36:06.030
But it can do time samples, you know,
like other profilers do.

00:36:06.110 --> 00:36:08.680
But it does some things that
we used to be able to do in

00:36:08.680 --> 00:36:11.990
the old sampler programs,
such as allocation tracing

00:36:12.100 --> 00:36:14.090
and even exact method tracing.

00:36:14.100 --> 00:36:18.100
So it can act like Gprof and record
every invocation of a method.

00:36:18.120 --> 00:36:20.220
So these two other methods are
really nice additions to the

00:36:20.240 --> 00:36:22.100
usual time profiling you see.

00:36:22.100 --> 00:36:25.400
on other profilers.

00:36:25.790 --> 00:36:29.310
It also does non-Java profiling,
as I mentioned, for time, memory,

00:36:29.310 --> 00:36:31.850
function, even low-level hardware events.

00:36:31.860 --> 00:36:34.700
If your application is
a real-time type thing,

00:36:34.840 --> 00:36:35.990
you can use that.

00:36:36.070 --> 00:36:39.200
And it's useful to study JNI calls.

00:36:39.980 --> 00:36:44.990
And you can download this
beta from developer.apple.com.

00:36:44.990 --> 00:36:47.240
And please get that,
because the version of Shark on your

00:36:47.310 --> 00:36:49.380
Tiger CD does not have the Java support.

00:36:49.430 --> 00:36:52.300
We worked really hard in the
last few weeks to deliver

00:36:52.300 --> 00:36:54.860
this for you guys for WWDC,
so I hope you enjoy it.

00:36:54.920 --> 00:36:56.400
But you have to get it off the website.

00:36:56.400 --> 00:37:01.270
The good news is it runs
on both Panther and Tiger.

00:37:04.850 --> 00:37:06.320
You know,
so you can just take this back to

00:37:06.320 --> 00:37:10.040
your development system as it is,
use it, and just rock with it.

00:37:10.040 --> 00:37:11.960
It's awesome.

00:37:12.160 --> 00:37:16.460
So some key features of Shark is it
provides a profile view that gives you a

00:37:16.510 --> 00:37:19.110
simultaneous heavy and tree perspective.

00:37:19.310 --> 00:37:22.150
And that will become more
clear when I show you the demo.

00:37:22.440 --> 00:37:28.010
And we're also introducing with Shark 4
sophisticated data mining and filtering.

00:37:28.140 --> 00:37:30.800
We also provide a chart view
that lets you visualize the

00:37:30.800 --> 00:37:32.600
execution of your program.

00:37:32.670 --> 00:37:35.110
And especially for
enterprise applications,

00:37:35.140 --> 00:37:36.390
this is a really neat feature.

00:37:36.580 --> 00:37:38.800
You can do remote
profiling over a network.

00:37:38.840 --> 00:37:42.690
You run a command line tool on your
X serve sitting in a cage somewhere.

00:37:42.850 --> 00:37:47.400
You can talk to it to Shark for
your rendezvous and control it.

00:37:47.610 --> 00:37:49.040
So you have minimal
impact on your server.

00:37:49.040 --> 00:37:53.370
You can analyze Tomcat, your JSPs,
whatever.

00:37:53.470 --> 00:37:54.380
So that's a really neat thing.

00:37:54.440 --> 00:37:58.100
And you learn more about the
detailed features of Shark at

00:37:58.100 --> 00:38:01.310
Got Shark this Friday at 3:30 PM.

00:38:01.590 --> 00:38:03.840
So I'm going to talk about just
a few general principles here

00:38:03.940 --> 00:38:06.180
to motivate the data mining.

00:38:06.440 --> 00:38:08.340
What makes software slow?

00:38:08.470 --> 00:38:10.530
Well,
probably best known as bad algorithms.

00:38:10.700 --> 00:38:13.500
You're using a bubble sort
instead of a quick sort.

00:38:13.680 --> 00:38:18.300
You know, if you use a large set of data,
then your software is

00:38:18.360 --> 00:38:20.470
going to go really slow.

00:38:20.520 --> 00:38:23.480
Excessive memory allocations and locking,
these are primitives that are expensive.

00:38:23.500 --> 00:38:27.240
Victor just talked about this
example with Monte Carlo,

00:38:27.240 --> 00:38:32.490
where the overuse of a synchronization
primitive just hosed performance.

00:38:32.670 --> 00:38:36.150
Disk IO, network call, IPC,
these are all really expensive

00:38:36.150 --> 00:38:38.500
operations compared to doing an ad.

00:38:38.790 --> 00:38:41.500
These things you want to
do as little as possible.

00:38:41.500 --> 00:38:45.200
Now, a more insidious thing that
happens in software is doing the

00:38:45.670 --> 00:38:47.500
same operation more than once.

00:38:47.500 --> 00:38:51.590
Let's suppose I write a module
that quick sorts properties

00:38:51.590 --> 00:38:53.470
that I read out of a file.

00:38:53.500 --> 00:38:57.480
And let's say Victor had written another
function that does the same quick sort.

00:38:57.500 --> 00:39:01.470
These two will just show up as
calls to quick sort in the profile,

00:39:01.550 --> 00:39:04.620
but it won't show the fact that
two different pieces of the code in

00:39:04.620 --> 00:39:09.340
two different parts of the program,
you know, did this call to quick sort.

00:39:09.500 --> 00:39:13.490
And this is a simple example of
what we call complexity in software.

00:39:13.580 --> 00:39:17.450
You know, I have a little graph of an
execution trace here of a program.

00:39:17.510 --> 00:39:21.500
The horizontal axis is time
slices or sample slices.

00:39:21.500 --> 00:39:23.480
In this case, it's memory allocations.

00:39:23.500 --> 00:39:26.370
The vertical axis is
the call stack depth.

00:39:26.500 --> 00:39:28.390
So what we're really
doing is we're taking,

00:39:28.390 --> 00:39:31.500
like, slices of your program
as it's running and,

00:39:31.660 --> 00:39:33.500
you know, doing this plot.

00:39:33.500 --> 00:39:35.500
And you can see these
interesting patterns there.

00:39:35.500 --> 00:39:38.200
And I'll go into that a
little more in a minute.

00:39:38.400 --> 00:39:40.500
So what do we mean by complexity?

00:39:40.570 --> 00:39:44.570
Large-scale software has
multiple layers and many modules.

00:39:44.900 --> 00:39:47.540
And the bigger the system,
the more of these things you get.

00:39:47.590 --> 00:39:51.660
And because we're good programmers,
we hide the implementation

00:39:52.310 --> 00:39:54.780
details from our clients.

00:39:54.920 --> 00:40:02.020
So a method called foo could do something
like just set a bit in a class somewhere.

00:40:02.080 --> 00:40:05.050
Or it could cause a
transaction to a database,

00:40:05.060 --> 00:40:08.300
update some rows, and even result in the
launch of a rocket.

00:40:08.300 --> 00:40:10.600
Through some I/O devices.

00:40:10.750 --> 00:40:12.700
One takes microseconds or milliseconds.

00:40:12.700 --> 00:40:14.980
The other one can take minutes or hours.

00:40:14.980 --> 00:40:17.960
So innocuous-looking
calls can result in crazy,

00:40:17.960 --> 00:40:21.460
complex, unexpected execution paths.

00:40:21.460 --> 00:40:23.190
So going back to this example,
which is actually the

00:40:23.190 --> 00:40:25.740
finder get-info dialog,
we zoom in.

00:40:25.940 --> 00:40:29.920
And the patterns of repetition
show up in deeper levels.

00:40:30.090 --> 00:40:32.890
On this find level,
you see repeated structure.

00:40:33.090 --> 00:40:35.820
And as you're zooming out,
it shows up again.

00:40:35.820 --> 00:40:38.300
This is like two layers that
are both doing iterative--

00:40:38.300 --> 00:40:40.140
iteration and repetition.

00:40:40.170 --> 00:40:41.040
And they're layered.

00:40:41.060 --> 00:40:43.160
Now imagine this multiplied
with five layers.

00:40:43.160 --> 00:40:46.800
Imagine you're adding AWT,
all the Sun libraries,

00:40:46.830 --> 00:40:48.160
all of your libraries.

00:40:48.170 --> 00:40:49.880
You're a huge application.

00:40:49.910 --> 00:40:52.040
This thing can be insane.

00:40:52.260 --> 00:40:54.020
So how do we deal with this?

00:40:54.030 --> 00:40:57.150
Well, in analyzing performance,
you can break the impact of

00:40:57.160 --> 00:40:59.260
an operation into two pieces.

00:40:59.430 --> 00:41:03.980
The cost of the operation times
the number of places it's used.

00:41:04.000 --> 00:41:06.600
And traditional profilers,
for a long time,

00:41:06.600 --> 00:41:08.020
have made it easy to analyze costs.

00:41:08.300 --> 00:41:11.250
And I can tell what leaf function I'm in.

00:41:11.430 --> 00:41:14.400
The hard part is understanding
the patterns of usage.

00:41:14.400 --> 00:41:19.350
Did Victor and I unintentionally
both quicksort the same array when

00:41:19.350 --> 00:41:24.140
we could have just done it once and
one of us accessed a cached copy?

00:41:24.150 --> 00:41:26.960
And when you introduce
over-modularization,

00:41:27.010 --> 00:41:30.040
people tend to over-abstract,
design too much,

00:41:30.040 --> 00:41:32.080
go kind of crazy with design.

00:41:32.100 --> 00:41:35.130
You get these really crazy
multi-level call stacks,

00:41:35.180 --> 00:41:38.050
multi-level things that just
really kill performance.

00:41:38.140 --> 00:41:39.700
performance.

00:41:39.880 --> 00:41:41.720
So, analyzing use.

00:41:42.140 --> 00:41:46.110
Shark provides two classes of
features to help you analyze usage.

00:41:46.280 --> 00:41:48.980
The first one is called
call stack data mining.

00:41:49.050 --> 00:41:51.320
And in this case,
what you do is you want to

00:41:51.540 --> 00:41:54.540
filter unwanted information.

00:41:54.630 --> 00:41:58.120
How many of you have profiled
something and not seen a line

00:41:58.120 --> 00:42:00.220
of your code in the top profile?

00:42:00.220 --> 00:42:03.080
Rather,
you've seen all these Java libraries,

00:42:03.080 --> 00:42:04.040
system libraries.

00:42:04.110 --> 00:42:06.200
Have any of you had that problem?

00:42:06.260 --> 00:42:07.010
Yeah, I bet you have.

00:42:07.020 --> 00:42:10.000
That was the first thing that
happened to me when I did a profile.

00:42:10.040 --> 00:42:14.360
Now, the other side of this
is graphical analysis.

00:42:14.410 --> 00:42:16.550
In this case,
you can visualize the dynamic

00:42:16.550 --> 00:42:19.060
behavior of your program,
like those plots that I showed you.

00:42:19.060 --> 00:42:21.260
Those are not just cute
graphs to make a point.

00:42:21.320 --> 00:42:24.390
Those are data from real
program that I was able to use

00:42:24.430 --> 00:42:26.210
to find performance problems.

00:42:26.410 --> 00:42:29.940
And you do this through a technique
called software fingerprinting.

00:42:30.000 --> 00:42:33.330
And in software fingerprinting,
you recognize that if the

00:42:33.330 --> 00:42:36.740
pattern on the picture looks
the same over and over again,

00:42:36.740 --> 00:42:39.280
it means you're going
through the same code path.

00:42:39.330 --> 00:42:41.160
And if you're going
through the same code path,

00:42:41.280 --> 00:42:44.420
you're either just doing the
same thing over and over again,

00:42:44.510 --> 00:42:48.970
or you're iterating over some
array or other structure data.

00:42:49.170 --> 00:42:51.140
And in that case,
you can still look at the

00:42:51.140 --> 00:42:53.330
opportunity to hoist information.

00:42:53.510 --> 00:42:57.260
In other words, in quicksort,
you have to do a compare function.

00:42:57.360 --> 00:43:00.110
But suppose your compare operator
then has to go through a whole bunch

00:43:00.110 --> 00:43:03.980
of different classes and call stacks,
and so to actually get down to where

00:43:03.980 --> 00:43:06.460
it's doing the real A is less than B.

00:43:06.460 --> 00:43:07.900
Well, that's not a good.

00:43:07.900 --> 00:43:11.220
You should decapsulate that stuff,
and you remove the amount of

00:43:11.220 --> 00:43:13.300
overhead to do that compare.

00:43:13.330 --> 00:43:17.270
So the software fingerprinting can
also identify those kind of cases.

00:43:18.200 --> 00:44:26.400
[Transcript missing]

00:44:26.590 --> 00:44:31.220
So-- and focus package is the
same thing for all the functions

00:44:31.220 --> 00:44:33.090
within a Pegler package.

00:44:33.360 --> 00:44:34.970
So I'm just going to show
you this graphically,

00:44:34.970 --> 00:44:37.420
because there's a lot to cover.

00:44:37.490 --> 00:44:40.380
So in excluding library,
here we have an example of a main

00:44:40.380 --> 00:44:45.420
program that calls an init function,
a do example, and a cleanup.

00:44:45.680 --> 00:44:49.140
And do example, in this case,
calls the function bar four times.

00:44:49.630 --> 00:44:51.890
And let's say it uses Java util.

00:44:52.190 --> 00:44:54.810
And it uses a hash table.

00:44:55.140 --> 00:44:58.420
So in this case, when you profile it,
you're just going to

00:44:58.420 --> 00:45:01.200
see all these samples,
as indicated in yellow,

00:45:01.200 --> 00:45:04.420
in Java util and not in bar.

00:45:04.510 --> 00:45:07.540
So we don't know that we've
been using bar to do this.

00:45:07.540 --> 00:45:10.790
But by excluding,
it turns bar effectively

00:45:10.800 --> 00:45:12.460
into a leaf function.

00:45:12.460 --> 00:45:15.290
And now you can see, well,
I'm making four calls to bar.

00:45:15.300 --> 00:45:17.000
Well,
if they're computing the same thing,

00:45:17.000 --> 00:45:18.340
I don't need to do that.

00:45:18.340 --> 00:45:20.740
If they are, I can hoist.

00:45:20.740 --> 00:45:23.130
Now there's another operation
that's very similar to exclude

00:45:23.140 --> 00:45:25.480
library called flattening a library.

00:45:25.480 --> 00:45:28.180
And that is, makes the library go away.

00:45:28.180 --> 00:45:31.660
But instead of making it go away
all the way to the end of the tree,

00:45:31.660 --> 00:45:33.460
it makes the library go away.

00:45:33.460 --> 00:45:36.030
And that's because it's not
going to be able to do it.

00:45:36.040 --> 00:45:38.800
So you can see that the library
is now going to be able to do it.

00:45:38.800 --> 00:45:42.370
And if you want to do it completely,
it replaces the library with

00:45:42.370 --> 00:45:44.380
all of the entry points into it.

00:45:44.380 --> 00:45:48.620
So you can observe your usage of
the library in that situation.

00:45:48.620 --> 00:45:50.110
And finally, focusing.

00:45:50.240 --> 00:45:52.580
We want to focus on do example.

00:45:52.580 --> 00:45:55.410
It makes main, init, and cleanup go away.

00:45:55.430 --> 00:45:57.450
And you're just left with this subtree.

00:46:01.470 --> 00:46:02.880
Okay, thank you.

00:46:03.020 --> 00:46:08.260
So we have a sort of modified version
of the Java 2D application here.

00:46:08.340 --> 00:46:10.930
And to use--

00:46:11.580 --> 00:46:14.360
You know, Java for Shark,
you add an XRUN parameter.

00:46:14.580 --> 00:46:16.420
You use the JVMPI interface.

00:46:16.600 --> 00:46:18.860
It'll migrate to JVMTI in the future.

00:46:18.860 --> 00:46:21.880
And you add a dash XRUN shark argument.

00:46:21.900 --> 00:46:25.100
So with that in mind, let's run this.

00:46:28.000 --> 00:46:32.620
So you get a message in the console,
"Java for Shark is enabled."

00:46:32.640 --> 00:46:35.740
And here is our familiar example.

00:46:35.770 --> 00:46:38.660
We added a new pane here
called Bouncing Strings.

00:46:38.680 --> 00:46:42.710
And this is kind of a cooked example in
that it has some performance problems

00:46:42.710 --> 00:46:45.150
introduced that we want to go find.

00:46:45.540 --> 00:46:47.990
So let's go over and launch Shark.

00:46:49.280 --> 00:46:53.390
And Shark has all sorts of traces,
but we're going to choose, for now,

00:46:53.390 --> 00:46:55.140
Java Time Trace.

00:46:55.190 --> 00:46:57.940
And when it does so,
you can pick the Java application.

00:46:57.970 --> 00:47:01.080
In this case,
we ran it from a command line type shell,

00:47:01.080 --> 00:47:02.240
so you just see Java.

00:47:02.490 --> 00:47:05.420
It would see your application name
if you made it double clickable.

00:47:05.560 --> 00:47:06.590
So let's just start sampling.

00:47:06.660 --> 00:47:09.050
Oh, by the way,
you notice that it just paused?

00:47:09.220 --> 00:47:11.100
It does that every so often.

00:47:11.150 --> 00:47:12.970
And it's because we're
garbage collecting.

00:47:13.040 --> 00:47:14.860
You see it just did it again.

00:47:14.960 --> 00:47:16.750
So that's kind of odd.

00:47:16.900 --> 00:47:19.080
So let's just start sampling.

00:47:21.730 --> 00:47:23.890
So just let it go for a few seconds.

00:47:24.100 --> 00:47:26.610
You know, given the sampling rate,
it's probably good to

00:47:26.610 --> 00:47:28.440
sample for about 10 seconds.

00:47:28.500 --> 00:47:30.620
And let's stop sampling.

00:47:31.210 --> 00:47:34.800
and we now have a typical profile view.

00:47:34.800 --> 00:47:38.120
We have a list of the various
symbols and the percentage of

00:47:38.120 --> 00:47:39.930
samples that occurred in them.

00:47:40.180 --> 00:47:44.900
On the right here,
you'll see a backtrace of the calls.

00:47:45.020 --> 00:47:48.360
So you see doString, drawString,
and it goes down to

00:47:48.360 --> 00:47:50.100
bouncing strings paint.

00:47:50.100 --> 00:47:53.100
If you click on another symbol,
you'll see its backtrace.

00:47:53.100 --> 00:47:56.100
And one thing to kind of help keep
track of things a little better,

00:47:56.100 --> 00:47:59.100
there's a neat feature
called color by library.

00:47:59.100 --> 00:48:01.100
So if you click on that,
look what happens.

00:48:01.100 --> 00:48:03.010
It colors all the strings.

00:48:03.100 --> 00:48:06.100
And this will help us
identify everything.

00:48:06.100 --> 00:48:13.100
AWT is colored in this red color,
brown for this, and so on.

00:48:13.100 --> 00:48:15.100
And one little problem here.

00:48:15.100 --> 00:48:20.180
The Java runtime in JVMPI isn't perfect
about reporting all the symbols,

00:48:20.240 --> 00:48:23.100
so we have a method
with an unknown library.

00:48:23.100 --> 00:48:25.770
So we're going to use the exclude library
to get rid of that and attribute those

00:48:25.780 --> 00:48:27.060
to things that are more meaningful.

00:48:27.100 --> 00:48:30.020
And when this happens,
you'll see that these

00:48:30.020 --> 00:48:31.040
percentages went up.

00:48:31.150 --> 00:48:35.100
Paint strings went up,
native font wrapper went up, and so on.

00:48:35.100 --> 00:48:41.100
If we look at native font wrapper,
we can exclude the library again.

00:48:41.800 --> 00:48:44.080
and now it pushes initialized font up.

00:48:44.160 --> 00:48:45.240
And this is kind of interesting.

00:48:45.300 --> 00:48:49.660
Initialized font, instead of drawing,
we're drawing, we're painting strings.

00:48:49.730 --> 00:48:51.190
Why are we initializing a font?

00:48:51.300 --> 00:48:55.200
And this is taking up almost as much
time as it's taking to draw the string.

00:48:55.530 --> 00:48:58.030
So let's take a look at
the heavy and tree view.

00:48:58.100 --> 00:49:00.090
And we've been looking
kind of from the bottom up.

00:49:00.100 --> 00:49:02.900
We've been looking at the
leaves of the execution tree.

00:49:02.920 --> 00:49:05.400
Now we can look from top down.

00:49:05.430 --> 00:49:09.340
Here's our event dispatch thread run,
and it works its way down.

00:49:09.340 --> 00:49:11.380
And here's bouncing strings paint.

00:49:11.460 --> 00:49:15.220
So we see that this bouncing strings
paint is an important place to look at.

00:49:15.230 --> 00:49:17.760
And here is one of the really
cool features that we worked

00:49:17.880 --> 00:49:19.600
hard to get in for you guys.

00:49:19.600 --> 00:49:23.590
You can double-click on bouncing
strings paint and get source.

00:49:24.500 --> 00:49:27.580
And in source here,
it's annotated by the relative

00:49:28.060 --> 00:49:30.160
densities of the calls.

00:49:30.160 --> 00:49:34.240
So about 9% of the time
is spent in fill-rect,

00:49:34.320 --> 00:49:37.440
and 89% is spent in this
paint-strings function.

00:49:37.600 --> 00:49:39.860
And you notice that these
things are underlined.

00:49:40.050 --> 00:49:42.120
Well,
that means you can double-click on it

00:49:42.220 --> 00:49:45.470
and navigate to the associated function.

00:49:45.640 --> 00:49:46.850
And it's like a little web browser.

00:49:47.050 --> 00:49:49.400
You have a backwards and a forward arrow.

00:49:49.490 --> 00:49:52.960
And you see here,
there's three areas of interest.

00:49:53.060 --> 00:49:55.520
And we found, oh,
that looks like a problem.

00:49:55.590 --> 00:49:58.710
We're calling set font new font Lucida.

00:49:58.830 --> 00:50:01.520
So we're constructing a font
every time we're painting,

00:50:01.580 --> 00:50:02.680
actually inside of a for loop.

00:50:02.790 --> 00:50:04.710
That's pretty bad.

00:50:06.400 --> 00:50:08.580
So let's go fix that.

00:50:08.590 --> 00:50:10.750
And yeah,
we kind of rigged this a little bit,

00:50:11.130 --> 00:50:13.380
but I've made errors
like this in programs,

00:50:13.380 --> 00:50:16.060
and I'm sure other people might have too.

00:50:16.300 --> 00:50:19.480
So this is worth doing.

00:50:20.790 --> 00:50:23.540
So I happen to have the corrected
code here just to save us time.

00:50:23.740 --> 00:50:25.880
So I'm going to change those.

00:50:32.100 --> 00:50:36.180
And also, let's quit the app.

00:50:36.450 --> 00:50:40.430
And we're going to run again.

00:50:40.640 --> 00:50:42.940
Give it a second to load.

00:50:45.190 --> 00:50:46.960
And we go back to bouncing strings.

00:50:47.010 --> 00:50:48.540
And look at that.

00:50:48.550 --> 00:50:51.110
We just about doubled its speed.

00:50:51.880 --> 00:50:56.310
So now, just by doing some analysis,
we are able to speed up our

00:50:56.310 --> 00:50:58.880
program pretty significantly.

00:50:58.930 --> 00:51:02.870
Now, let's do one more trace.

00:51:02.880 --> 00:51:04.800
Since we're feeling lucky,
we made some progress,

00:51:04.800 --> 00:51:06.140
let's see if we can make some more.

00:51:06.230 --> 00:51:08.990
So I'm going to do a memory trace,
because this is a different

00:51:08.990 --> 00:51:11.550
kind of technique than you
might be familiar with.

00:51:11.810 --> 00:51:13.870
So we're going to do start.

00:51:14.090 --> 00:51:16.610
and the memory trace slows it
down a little bit because we're

00:51:16.610 --> 00:51:19.040
sampling every memory allocation.

00:51:19.040 --> 00:51:21.000
And let's stop that.

00:51:21.050 --> 00:51:25.490
And now we see, wow,
69% of our allocations

00:51:25.490 --> 00:51:29.100
occur in component bounds.

00:51:29.580 --> 00:51:33.860
and we change this to value,
we can see that just in those few seconds

00:51:33.860 --> 00:51:37.180
we allocated half a megabyte of memory.

00:51:37.610 --> 00:51:39.200
No wonder we were garbage
collecting so much.

00:51:39.280 --> 00:51:41.730
We were allocating all
these bounds objects.

00:51:41.940 --> 00:51:43.720
So let's look at what's up with that.

00:51:43.880 --> 00:51:48.290
So if you look in the back trace here,
you'll see that bouncing strings

00:51:48.290 --> 00:51:50.660
ball tick is doing most of it.

00:51:50.740 --> 00:51:55.240
And we go in here and you see that
we have this bounds equals get bounds

00:51:55.260 --> 00:51:57.000
that's being called in every tick.

00:51:57.070 --> 00:52:00.880
Now I've already got code in here
to turn that off and cache it,

00:52:00.890 --> 00:52:01.900
which would be the obvious thing.

00:52:01.900 --> 00:52:06.000
Just compute it once since the
window size doesn't change.

00:52:06.100 --> 00:52:08.660
So let's go ahead and make that change.

00:52:10.710 --> 00:52:15.600
We made a convenient little
Boolean here called cache bounds.

00:52:15.630 --> 00:52:17.360
And by the way,
this is not a cooked up example.

00:52:17.360 --> 00:52:19.590
This was something that
we found in the program,

00:52:19.860 --> 00:52:22.240
just as it was given to me.

00:52:22.240 --> 00:52:23.220
So we're going to do that.

00:52:23.250 --> 00:52:25.310
We're going to run it again.

00:52:33.320 --> 00:52:37.140
Go to Bouncing Strings.

00:52:37.150 --> 00:52:37.630
And look at that.

00:52:37.640 --> 00:52:39.920
We're now at about 183, 190.

00:52:39.920 --> 00:52:41.210
I've seen this thing go over 200.

00:52:41.320 --> 00:52:44.740
So just doing simple
memory optimizations,

00:52:44.770 --> 00:52:48.840
because memory allocation
is so expensive in Java,

00:52:48.890 --> 00:52:52.040
can get you huge wins.

00:52:52.040 --> 00:52:57.550
I was working on an application server
a few years ago where we reduced

00:52:57.550 --> 00:53:00.970
the number of allocations by 10x,
and we got a 3x throughput

00:53:01.050 --> 00:53:02.840
improvement in that server.

00:53:02.900 --> 00:53:08.880
So doing just memory analysis and memory
reduction is a really amazing technique.

00:53:08.880 --> 00:53:11.080
So thank you very much.

00:53:11.080 --> 00:53:12.920
I'm going to give this back to Victor.

00:53:16.440 --> 00:53:20.910
Thank you, Christy.

00:53:28.540 --> 00:53:33.330
So I just wanted to conclude
with one recommendation.

00:53:33.500 --> 00:53:36.720
For optimizing for Hotspot,
the main thing you need to know is

00:53:36.760 --> 00:53:38.500
exactly what your HOT methods are.

00:53:38.750 --> 00:53:44.780
You can find out bottlenecks
in your own code,

00:53:44.780 --> 00:53:44.780
and you can identify them
using Shark for Java.

00:53:45.950 --> 00:53:47.770
The other thing that you
need to be aware of is,

00:53:47.920 --> 00:53:51.880
even if there's nothing more to be done,
you also need to make sure that the

00:53:51.880 --> 00:53:57.170
HOT method is as amenable to Hotspot's
optimization opportunities as possible.

00:53:57.250 --> 00:54:00.620
So I want to highlight the fact, again,
that inlining is probably one

00:54:00.620 --> 00:54:03.460
of the biggest optimizations
that we're able to do.

00:54:03.550 --> 00:54:07.330
And therefore,
it should be in your best interest

00:54:07.370 --> 00:54:10.250
to make your HOT methods inlinable.

00:54:10.490 --> 00:54:15.400
So here's the reminder of the key things
that keep a method from being inlined:

00:54:15.450 --> 00:54:18.600
it being too large,
it being synchronized,

00:54:18.600 --> 00:54:21.240
or it having exception handlers.

00:54:21.370 --> 00:54:24.660
And the last tip I want to
tell you is that we do have a

00:54:24.660 --> 00:54:26.890
Java lab here at WWC all week.

00:54:27.000 --> 00:54:29.940
And if you want to see your
application running on a Mac and

00:54:29.980 --> 00:54:32.580
you haven't done so before,
do go down there.

00:54:32.650 --> 00:54:35.550
Or if there's any performance
bottlenecks that you want to

00:54:35.550 --> 00:54:38.760
identify to our engineering,
definitely we're able to

00:54:38.770 --> 00:54:40.300
do that there as well.

00:54:41.350 --> 00:54:43.140
So that concludes everything.

00:54:43.210 --> 00:54:45.120
I want to point out a few URLs.

00:54:45.270 --> 00:54:50.560
You can get Java reference documentation
from Apple at the ADC website.

00:54:50.760 --> 00:54:54.240
You can also get Java 1.5
documentation at the Sun website.

00:54:54.300 --> 00:55:00.200
And finally,
that is the URL again for where you can

00:55:00.200 --> 00:55:07.470
download Shark that has the Java support
that runs both on Panther and Tiger.

00:55:08.990 --> 00:55:13.050
Finally, if you have any more questions,
the people to contact are Alan Samuel,

00:55:13.140 --> 00:55:17.900
he's our Java Technologies Evangelist,
Bob Fraser, he's our Product Manager,

00:55:17.900 --> 00:55:21.140
and finally Francois Jouel,
who is the Manager of

00:55:21.140 --> 00:55:22.900
all things Java at Apple.