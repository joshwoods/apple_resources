WEBVTT

00:00:12.420 --> 00:00:15.000
Okay, good morning.

00:00:15.000 --> 00:00:15.690
My name's Doug Brooks.

00:00:15.800 --> 00:00:18.180
I'm product manager of
server hardware at Apple.

00:00:18.350 --> 00:00:20.550
And I'd like to --

00:00:24.860 --> 00:00:28.970
This session is entitled
HPC Technology Update.

00:00:28.970 --> 00:00:32.200
In this session we'd like to
take a look at Apple and HPC,

00:00:32.300 --> 00:00:36.780
and specifically Apple products
and technologies that

00:00:36.980 --> 00:00:39.640
contribute to HPC deployments.

00:00:39.770 --> 00:00:43.450
We'll also look at industry-leading
third-party products that

00:00:43.450 --> 00:00:45.440
complement those solutions.

00:00:45.630 --> 00:00:49.160
And also we'll hear from two
customers to talk about their

00:00:49.160 --> 00:00:52.190
HPC deployments using Apple technology.

00:00:52.580 --> 00:00:55.500
First,
let's take a look at Apple and HPC.

00:00:55.530 --> 00:00:59.340
Now, in the last year,
when you think of Apple and HPC,

00:00:59.340 --> 00:01:03.500
usually the first thing that
comes to your mind is this.

00:01:11.600 --> 00:01:25.020
Virginia Tech saw the vision and
power of the G5 and Mac OS X server,

00:01:25.020 --> 00:01:29.010
and combining over a thousand
Power Mac G5s and Mac OS X server

00:01:29.010 --> 00:01:33.290
software using technology for
interconnect and FinnaBand

00:01:33.860 --> 00:01:38.960
achieved phenomenal performance,
achieving over 10 teraflops of computing

00:01:38.960 --> 00:01:44.090
power on its debut at rank number three
on the top 500 list and is the number

00:01:44.090 --> 00:01:47.160
one academic supercomputer in the world.

00:01:47.160 --> 00:01:49.260
An amazing achievement.

00:01:49.280 --> 00:01:54.100
And they also really proved the
value and the price point that

00:01:54.100 --> 00:01:58.310
you could build and deliver a very
high performance supercomputer

00:01:58.310 --> 00:02:01.280
system with Apple technology.

00:02:01.280 --> 00:02:04.570
Now what's interesting is while they
may have been the first and definitely

00:02:04.570 --> 00:02:09.030
the largest G5 cluster deployment,
they were definitely not the

00:02:09.140 --> 00:02:10.730
first Apple cluster deployment.

00:02:10.730 --> 00:02:13.320
Matter of fact,
a lot of the early cluster work done

00:02:13.320 --> 00:02:18.170
on the Macintosh platform was actually
done a number of years earlier,

00:02:18.180 --> 00:02:22.240
most notably at UCLA with
the Appleseeds work.

00:02:22.240 --> 00:02:25.320
You may remember this from the late '90s.

00:02:25.320 --> 00:02:28.480
Actually, I believe this is circa 1998.

00:02:28.480 --> 00:02:32.600
UCLA with the Appleseeds project took,
at the time, you know,

00:02:32.640 --> 00:02:34.150
pretty fast technology.

00:02:34.150 --> 00:02:38.750
Bayes G3, 233 megahertz systems,
10-way wireless, 10-way wireless,

00:02:38.750 --> 00:02:41.470
10-100 Ethernet as an interconnect.

00:02:41.480 --> 00:02:46.360
It was running early on Mac OS 8 and was
using Apple events as the middleware.

00:02:46.360 --> 00:02:50.070
Nevertheless, doing high-energy physics
work that they were doing,

00:02:50.200 --> 00:02:54.440
it achieved phenomenal
performance at very low cost.

00:02:54.440 --> 00:02:58.170
And actually, the system you see here on
the screen outperformed Cray

00:02:58.170 --> 00:03:00.300
YMP running similar codes.

00:03:00.300 --> 00:03:03.140
So again, showing the value,
some of the same things

00:03:03.140 --> 00:03:05.260
Virginia Tech has proved with the G5.

00:03:05.260 --> 00:03:07.440
They actually shown some
of those same capabilities.

00:03:07.440 --> 00:03:12.420
Of course,
we've come a long way since then,

00:03:12.420 --> 00:03:16.770
from the G3, Bayes G3 days, of course,
to the G4,

00:03:16.770 --> 00:03:20.310
introducing desktop supercomputing
with the Velocity Engine,

00:03:20.310 --> 00:03:23.720
bringing phenomenal vector
processing capabilities that many

00:03:23.720 --> 00:03:27.630
applications have been able to take a
tremendous benefit from by leveraging

00:03:27.630 --> 00:03:29.420
that power and that technology.

00:03:29.420 --> 00:03:32.780
And of course, most recently, the G5,
bringing phenomenal

00:03:32.780 --> 00:03:35.970
floating-point performance,
vector processing with a system that

00:03:36.020 --> 00:03:40.710
delivers very high memory bandwidth and
system throughput through this processor,

00:03:40.820 --> 00:03:44.720
providing a phenomenal
foundation for computing.

00:03:44.720 --> 00:03:47.250
And of course, customers have responded.

00:03:47.250 --> 00:03:51.690
And of course, higher education customers
tend to be on the leading edge.

00:03:51.690 --> 00:03:55.160
They've been some of our most
strongest early adopters in

00:03:55.160 --> 00:03:56.520
the higher education market.

00:03:56.520 --> 00:03:58.840
And of course,
if you heard the last session,

00:03:58.840 --> 00:04:02.060
you've heard about some of the
deployments in the scientific field.

00:04:02.060 --> 00:04:04.410
Matter of fact,
life sciences in particular has

00:04:04.410 --> 00:04:04.910
been adopters of our technology.

00:04:04.980 --> 00:04:05.830
And so, we've been able to get a lot
of our customers to come in and

00:04:05.860 --> 00:04:06.050
take a look at the technology.

00:04:06.100 --> 00:04:06.750
And of course,
we've been able to get a lot

00:04:06.760 --> 00:04:07.760
of our customers to come in and
take a look at the technology.

00:04:07.760 --> 00:04:07.940
And of course,
we've been able to get a lot

00:04:07.940 --> 00:04:08.750
of our customers to come in and
take a look at the technology.

00:04:08.760 --> 00:04:09.150
And of course,
we've been able to get a lot

00:04:09.150 --> 00:04:09.960
of our customers to come in and
take a look at the technology,

00:04:09.960 --> 00:04:10.760
primarily because many of the key
applications that are run day in and

00:04:10.760 --> 00:04:14.070
day out in the life sciences arena
have been Velocity Engine optimized

00:04:14.070 --> 00:04:18.560
and run very high performance
on the G4 and the G5 processors.

00:04:18.560 --> 00:04:22.840
And so, we're seeing lots of deployments
taking advantage of our

00:04:23.010 --> 00:04:25.540
tools and our technology,
combined with the ease of

00:04:25.540 --> 00:04:30.310
use that we provide in our
systems in the sciences field.

00:04:31.700 --> 00:06:28.300
[Transcript missing]

00:06:28.890 --> 00:06:31.800
And finally,
we've had a phenomenal customer response.

00:06:31.800 --> 00:06:35.700
This is just a small sampling of
customers who have recently deployed

00:06:35.810 --> 00:06:40.540
clusters based on Apple technology,
Xserve, and Mac OS X Server.

00:06:40.630 --> 00:06:43.700
Really,
we see this market continuing to grow and

00:06:43.700 --> 00:06:46.630
more very exciting customer deployments.

00:06:46.670 --> 00:06:50.940
And again, you're going to hear about two
of them later on in this session.

00:06:51.200 --> 00:06:54.470
As a matter of fact,
we're seeing right now roughly 40% of

00:06:54.470 --> 00:06:58.600
our Xserve units going into clusters
and high performance computing.

00:06:58.600 --> 00:07:00.940
And as we see as Xserve
continues to grow,

00:07:00.990 --> 00:07:07.810
we see this continuing to grow as well
as an important slice of the Xserve pie,

00:07:07.840 --> 00:07:09.400
you might say.

00:07:10.340 --> 00:07:13.780
What does it take to actually put a
cluster together with Apple technology?

00:07:13.900 --> 00:07:16.420
Obviously,
it's a lot more than just racking

00:07:16.420 --> 00:07:17.800
a bunch of Xserves in a rack.

00:07:17.900 --> 00:07:22.670
So what we want to do is take a
look at the HPC technology stack.

00:07:22.780 --> 00:07:29.800
What are the components on Mac OS X that
it takes to build an HPC deployment,

00:07:29.800 --> 00:07:35.600
and what technologies and products
are available from Apple and third

00:07:35.600 --> 00:07:35.600
parties to complement this stack?

00:07:35.930 --> 00:07:40.040
So this is a view of, again,
what we'll say HPC building blocks.

00:07:40.040 --> 00:07:47.300
The components that are required to build
a HPC deployment in technology space.

00:07:47.300 --> 00:07:50.390
So if we look from the bottom up,
from the hardware,

00:07:50.390 --> 00:07:54.800
the actual hardware platform itself,
the operating system, the interconnects,

00:07:54.800 --> 00:07:59.270
the compiler and optimization tools,
the communications middleware,

00:07:59.300 --> 00:08:02.280
and finally the management tools,
to really do a complete

00:08:02.280 --> 00:08:05.370
cluster deployment,
we need components from all of these.

00:08:05.990 --> 00:08:09.420
Now if we take a look at what
Apple products and technologies provide,

00:08:09.550 --> 00:08:13.190
you'll see Apple provides
products that fit really in

00:08:13.190 --> 00:08:15.590
about four of the six areas.

00:08:15.600 --> 00:08:17.700
And if we look at third
party technologies,

00:08:17.750 --> 00:08:20.400
really industry leading
third party technologies,

00:08:20.410 --> 00:08:24.350
we have, again,
about four or six of those components

00:08:24.750 --> 00:08:26.800
that we have to choose from.

00:08:26.890 --> 00:08:29.600
And as we walk through,
you'll be able to see,

00:08:29.600 --> 00:08:33.110
you have a wide selection of
products and technologies for

00:08:33.190 --> 00:08:35.300
deploying clusters on Mac OS X.

00:08:35.300 --> 00:08:38.770
So let's take a quick review,
walking the stack from the bottom up,

00:08:38.770 --> 00:08:41.580
and let's take a quick review
from an Apple perspective

00:08:41.580 --> 00:08:43.190
at the hardware components.

00:08:43.300 --> 00:08:46.290
So this is pretty straightforward.

00:08:46.290 --> 00:08:49.240
First and foremost,
we have the G5 processor.

00:08:49.350 --> 00:08:54.260
The G5 processor really stands out as a
processor for high performance computing.

00:08:54.320 --> 00:08:57.480
With the 64-bit capabilities,
with the massive floating

00:08:57.480 --> 00:09:00.300
point and velocity engine
support in this processor,

00:09:00.300 --> 00:09:04.800
coupled with the power
advantages that it has,

00:09:04.800 --> 00:09:10.420
with the smaller 90-manometer technology,
really delivers a phenomenal

00:09:10.700 --> 00:09:14.800
bang for your buck and for your
power and heat output as well.

00:09:14.800 --> 00:09:21.650
So obviously that becomes one of the
core foundation pieces of the product.

00:09:21.810 --> 00:09:25.800
As we look up, of course,
then we wrap that up in the XSERV G5.

00:09:25.800 --> 00:09:29.800
Been able to provide dual processor
performance and a 1U form factor,

00:09:29.800 --> 00:09:34.300
a system that delivers peak
performance of 16 gigaflops of

00:09:34.300 --> 00:09:36.480
double precision floating point,
or 32 gigaflops of single precision

00:09:36.480 --> 00:09:38.790
floating point with a velocity engine.

00:09:38.800 --> 00:09:43.780
Again, a very, very powerful system to
deliver phenomenal performance.

00:09:44.220 --> 00:09:47.590
Couple that with the
latest of I/O technologies,

00:09:47.590 --> 00:09:51.510
PCI-X, ECC memory,
integrated hardware monitoring for

00:09:51.520 --> 00:09:53.800
systems management and monitoring.

00:09:53.800 --> 00:09:57.800
And again, very low power when we look
at it compared to competitors.

00:09:57.800 --> 00:10:01.770
You know,
roughly a compute node configuration of

00:10:01.870 --> 00:10:09.440
XSERV at 100% CPU is under 250 watts of
total power usage and well under 1,000

00:10:09.440 --> 00:10:14.800
BTUs an hour in heat output for the data
center that needs to cool these systems.

00:10:14.840 --> 00:10:19.000
That significantly lower the competitive
systems and a great advantage that

00:10:19.000 --> 00:10:21.300
we have with the G5 processor.

00:10:21.300 --> 00:10:25.190
We also have XSERV RAID,
and kind of the corollary to a lot

00:10:25.190 --> 00:10:29.300
of computing power is the data that
needs to be fed into those systems,

00:10:29.300 --> 00:10:31.300
and that data needs to
be stored somewhere.

00:10:31.540 --> 00:10:36.800
We see XSERV RAID as the ideal,
ideal storage device for storing data

00:10:36.800 --> 00:10:39.800
for high performance computing clusters.

00:10:39.800 --> 00:10:43.800
With phenomenal performance
and phenomenal capacity,

00:10:43.810 --> 00:10:46.800
you have the ability to,
through fiber channel,

00:10:46.800 --> 00:10:51.770
quite a large amount of storage online
and serve that throughout your cluster.

00:10:51.840 --> 00:10:55.800
And again, a breakthrough price
performance with XSERV RAID.

00:10:55.800 --> 00:11:00.790
Just again, ideal storage device for high
performance computing clusters.

00:11:01.050 --> 00:11:03.520
We then take a look up the next step.

00:11:03.590 --> 00:11:05.940
Obviously,
from an operating system perspective,

00:11:05.940 --> 00:11:07.590
we have Mac OS X.

00:11:08.030 --> 00:11:14.990
Mac OS X has really provided a key
foundation for cluster applications.

00:11:15.000 --> 00:11:19.370
What's really unique about Mac OS X is
that we have the ability with Mac OS X to

00:11:19.480 --> 00:11:23.000
combine the power of Unix under the hood,
which allows us to bring

00:11:23.000 --> 00:11:28.260
applications and technologies over
that allow us to compile and run

00:11:28.260 --> 00:11:30.000
those applications on Mac OS X.

00:11:30.020 --> 00:11:32.340
But combine that with
the great ease of use,

00:11:32.340 --> 00:11:34.940
ease of deployment,
ease of management that the

00:11:34.940 --> 00:11:37.000
Mac OS X server services provide.

00:11:37.000 --> 00:11:40.840
And of course, with the G5 optimizations,
we've been able to deliver the

00:11:40.890 --> 00:11:42.900
performance out of the G5 processor.

00:11:43.040 --> 00:11:47.170
And of course, if you've been to any of
the sessions yesterday,

00:11:47.170 --> 00:11:49.980
you know that we've introduced Tiger.

00:11:50.000 --> 00:11:53.100
And with Tiger Server,
one of the most important things

00:11:53.100 --> 00:11:56.490
in the HPC space that we're
able to bring with Tiger is true

00:11:56.490 --> 00:11:59.000
64-bit user space environment.

00:11:59.000 --> 00:12:03.000
We've been able to break that
4-gigabyte barrier in user space.

00:12:03.000 --> 00:12:05.710
I mean, we've always been able to
access more than 4 gigabytes

00:12:05.710 --> 00:12:08.000
of memory with the G5 system,
even on Panther.

00:12:08.000 --> 00:12:11.990
But now we have the ability to have
applications access large data sets.

00:12:12.000 --> 00:12:14.420
So especially code coming
over from other platforms,

00:12:14.550 --> 00:12:17.320
from other operating systems,
we're able to take advantage

00:12:17.320 --> 00:12:18.970
of that large memory footprint.

00:12:19.080 --> 00:12:23.030
Of course, we expect to see a number of
across-the-board improvements

00:12:23.030 --> 00:12:26.050
in other areas of Tiger,
improved SMP performance,

00:12:26.070 --> 00:12:29.840
improved network performance,
improved NFS performance,

00:12:30.060 --> 00:12:32.990
things that we think are going to
really deliver a phenomenal platform

00:12:33.130 --> 00:12:37.000
for the future of high-performance
computing with Mac OS X.

00:12:38.300 --> 00:12:41.600
It's also interesting – I encourage you
to attend if you're interested in some

00:12:41.600 --> 00:12:44.160
of the XSAN sessions later this week.

00:12:44.280 --> 00:12:49.200
XSAN is Apple's SAN file
system for Mac OS X.

00:12:49.420 --> 00:12:53.200
And XSAN has a role to
play in clusters as well.

00:12:53.300 --> 00:12:58.200
With – especially on larger clusters
where file I/O bandwidth is a concern,

00:12:58.200 --> 00:13:01.350
XSAN gives you a file system
that has the ability to scale

00:13:01.350 --> 00:13:04.200
out in I/O – file I/O services.

00:13:04.200 --> 00:13:08.200
So XSAN again plays a role in
high performance clusters as well,

00:13:08.280 --> 00:13:14.200
being able to share a large data pool
across a large clustered environment.

00:13:16.100 --> 00:16:46.400
[Transcript missing]

00:16:53.230 --> 00:16:53.690
Thank you, Doug.

00:16:53.690 --> 00:16:57.020
Alright.

00:16:57.020 --> 00:17:01.540
So to talk about InfiniBand is
certainly to talk about clusters,

00:17:01.570 --> 00:17:04.370
and Doug just gave a
good introduction there.

00:17:04.500 --> 00:17:09.660
But what we're seeing in the HPC market
is a lot of transition of supercomputers,

00:17:09.700 --> 00:17:14.420
mainframes, SMP machines to a bunch
of interconnected servers.

00:17:14.420 --> 00:17:14.900
Why?

00:17:14.950 --> 00:17:16.340
The reason is cost.

00:17:16.340 --> 00:17:18.580
There's an overriding reason.

00:17:18.580 --> 00:17:22.960
We'll see if we can put together one of
these clusters with a bunch of servers

00:17:22.960 --> 00:17:29.080
for realistically one-tenth the cost
of some of the existing SMP machines.

00:17:29.250 --> 00:17:33.080
Historically, though,
that's been fraught with complexities,

00:17:33.080 --> 00:17:37.970
underutilization of processors,
storage bottlenecks,

00:17:37.970 --> 00:17:41.710
just the complexity of hooking these up.

00:17:42.240 --> 00:17:44.610
So what does it take
to build an effective,

00:17:44.610 --> 00:17:45.950
efficient cluster?

00:17:46.010 --> 00:17:49.870
You have physical distribution
and logical consolidation.

00:17:50.010 --> 00:17:52.550
On the physical side,
you need a high bandwidth.

00:17:52.580 --> 00:17:57.910
InfiniBand offers 10 gigabits
and 30 gigabits per second today.

00:17:58.040 --> 00:18:01.560
Low latency for
inter-processor communications.

00:18:01.730 --> 00:18:04.240
Low CPU overhead,
you don't want the CPU spending all

00:18:04.250 --> 00:18:08.080
their processing time communicating
with the other processors.

00:18:08.080 --> 00:18:10.380
The ability to scale.

00:18:10.380 --> 00:18:15.610
We also need logical consolidation,
the ability to logically group nodes

00:18:15.610 --> 00:18:21.620
and systems into logical sets or groups,
domains.

00:18:21.640 --> 00:18:23.770
So you need a high
performance interconnect and

00:18:23.770 --> 00:18:27.060
an intelligent interconnect.

00:18:27.060 --> 00:18:31.760
What InfiniBand brings to the
table is it's an open standard.

00:18:31.760 --> 00:18:35.270
So it is the first open standard
interconnect designed from the

00:18:35.270 --> 00:18:39.540
ground up for high performance
interconnect and RDMA support.

00:18:39.620 --> 00:18:44.540
So the extraneous features and
functions that you might get in

00:18:44.540 --> 00:18:46.440
other technologies are not there.

00:18:46.440 --> 00:18:50.360
This was designed for the ground up
with high end clustering in mind.

00:18:50.360 --> 00:18:54.020
Because of that,
it has the significantly, we think,

00:18:54.020 --> 00:19:00.400
lower cost performance ratio than other
options do available for clustering.

00:19:00.400 --> 00:19:05.900
The latency is about 140
nanoseconds per hop and 5.8

00:19:05.900 --> 00:19:08.880
microseconds end-to-end latency.

00:19:09.720 --> 00:19:12.450
Key feature of InfiniBand is
it supports multiple types of

00:19:12.450 --> 00:19:14.180
traffic over the same fabric.

00:19:14.180 --> 00:19:19.250
So whether it's file, block, network,
IPC, it's all using a single

00:19:19.280 --> 00:19:22.550
technology efficiently.

00:19:23.360 --> 00:19:27.000
Right from the beginning,
we built in extensive management

00:19:27.000 --> 00:19:30.630
monitoring capabilities,
so high availability, quality of service,

00:19:30.720 --> 00:19:35.070
partitioning type capabilities
are built in from day one.

00:19:35.200 --> 00:19:39.200
Again, it already supports 30
gigabits per second today.

00:19:39.310 --> 00:19:44.040
There's something called DDR double
data rate and QDR quad data rate

00:19:44.040 --> 00:19:49.120
for supports up to 120 gigabits
per second being worked on today.

00:19:49.200 --> 00:19:55.100
So three real key points in FinnaBand:
high bandwidth, low latency,

00:19:55.100 --> 00:19:59.030
low CPU utilization,
and the ability to scale.

00:20:00.100 --> 00:20:06.430
This is a high level representation
of an SMP machine on the left showing

00:20:06.430 --> 00:20:12.280
an 8 processor with a proprietary
interconnect versus four two-way

00:20:12.450 --> 00:20:15.100
servers interconnected with InfiniBand.

00:20:15.100 --> 00:20:21.060
I want to make it clear the SMP,
the symmetric multiprocessor systems,

00:20:21.290 --> 00:20:25.100
will still be a perfect solution
for a lot of applications.

00:20:25.100 --> 00:20:31.100
To use the cluster you have to be able
to parallelize the application and

00:20:31.100 --> 00:20:34.100
we're looking at near memory speeds.

00:20:34.140 --> 00:20:39.100
But the cost performance
compared to the SMP is drastic.

00:20:41.620 --> 00:20:44.800
On the InfiniBand link protocol,
I'll just really try to make one

00:20:44.800 --> 00:20:46.370
or two points on this overhead.

00:20:46.370 --> 00:20:50.710
One is with a single event,
you can move a large amount of

00:20:50.770 --> 00:20:53.000
data – two gigabytes of data.

00:20:53.120 --> 00:20:56.000
And the entire link protocol
is handled in hardware.

00:20:56.150 --> 00:21:03.340
So all reassembly and segmentation and
all that is all handled in hardware.

00:21:03.510 --> 00:21:08.760
And we are in the third generation
of ASIC technology for InfiniBand.

00:21:11.060 --> 00:21:14.240
Some more InfiniBand link attributes.

00:21:14.400 --> 00:21:16.650
Each packet is sent with a service level.

00:21:16.760 --> 00:21:20.480
So there's up to 16 service
level supported SLs.

00:21:20.540 --> 00:21:23.610
There's also something they call a VL,
virtual lane.

00:21:23.770 --> 00:21:29.500
So there's 15 virtual lanes possible
over a single physical link.

00:21:29.540 --> 00:21:33.410
So the SL is mapped to a VL,
which is then arbitrated

00:21:33.420 --> 00:21:35.650
across the physical link.

00:21:36.390 --> 00:21:39.220
Basically,
that is the basis for your quality

00:21:39.220 --> 00:21:43.680
of service type implementation,
which lets you mix and match efficiently

00:21:43.680 --> 00:21:48.310
different types of traffic across
the same InfiniBand connection.

00:21:49.930 --> 00:21:53.520
InfiniBand also defines
what we call partitions.

00:21:53.520 --> 00:22:00.440
So this would be similar to zoning and
fiber channel or VLANs in the IP world.

00:22:00.610 --> 00:22:04.610
So it's a mechanism for
defining isolated domains.

00:22:04.610 --> 00:22:09.390
So each port or node can be defined to
a certain partition and communicate with

00:22:09.390 --> 00:22:13.680
only nodes in that partition or given
full or limited rights within that group.

00:22:13.700 --> 00:22:17.690
And that's all defined
by the subnet managers,

00:22:17.700 --> 00:22:21.470
SM managers by assigning partition keys.

00:22:23.060 --> 00:22:27.580
InfiniBand is based on a
2.5 GHz signaling rate.

00:22:27.580 --> 00:22:30.080
So when you hear the
rates for InfiniBand,

00:22:30.080 --> 00:22:31.850
1x is 2.5.

00:22:31.910 --> 00:22:34.400
There were really no implementations
done at that data rate.

00:22:34.440 --> 00:22:37.870
4x is really where all
the implementations are,

00:22:37.870 --> 00:22:39.900
most of the implementations today.

00:22:39.900 --> 00:22:42.140
12x is the 30 gigabits per second.

00:22:42.140 --> 00:22:45.330
So today, Voltaire,
we can support the 10 and

00:22:45.330 --> 00:22:47.660
30 gigabit per second rates.

00:22:49.420 --> 00:22:53.320
Over copper cabling,
it's a 17 meter distance limitation,

00:22:53.320 --> 00:22:55.910
1 kilometer with multi-mode fiber.

00:22:55.920 --> 00:23:02.020
And then, you see there's efforts to,
in the works, the 5 and 10 gigahertz

00:23:02.060 --> 00:23:05.710
signaling rate are in process.

00:23:07.450 --> 00:23:12.120
InfiniBand has a very rich
protocol stack to find.

00:23:12.120 --> 00:23:15.390
On the upper layer you'll see a
bunch of stuff that looks familiar.

00:23:15.400 --> 00:23:18.400
You know, NFS, RDMA in version 4.

00:23:18.510 --> 00:23:21.400
NFS will have RDMA, InfiniBand support.

00:23:21.470 --> 00:23:25.650
MPI is, I think Doug mentioned,
a message passing interface.

00:23:25.650 --> 00:23:29.570
Far and away the most popular
– MPI is the most popular

00:23:29.570 --> 00:23:33.790
IPC API in the HPC world.

00:23:33.800 --> 00:23:37.360
I think that was four three-letter
acronyms in a short sentence.

00:23:37.400 --> 00:23:40.820
So let me see if I can say that again.

00:23:40.820 --> 00:23:45.940
So message passing interface is the
most popular application program

00:23:45.940 --> 00:23:49.710
and interface for inter-processor
communications in the HPC market.

00:23:49.710 --> 00:23:52.250
And that's supported in
the Apple world today.

00:23:52.260 --> 00:23:57.420
iSCSI is for storage
support across the fabric.

00:23:57.470 --> 00:24:01.150
SDP is Sockets Direct Protocol.

00:24:01.150 --> 00:24:07.380
And any application with a
Sockets-level API can utilize that.

00:24:07.400 --> 00:24:10.400
Of course, TCP and IP over IB.

00:24:10.510 --> 00:24:15.070
And DAPL is
Direct Access Programmer's Library.

00:24:15.070 --> 00:24:17.540
A lot of acronyms here.

00:24:17.540 --> 00:24:21.580
And DAPL defines the API to RDMA.

00:24:21.580 --> 00:24:25.150
And then there's a full suite of
InfiniBand services below that

00:24:25.150 --> 00:24:27.280
for management and monitoring.

00:24:27.280 --> 00:24:29.740
And in my 10-minute time slot,
we won't be going

00:24:29.780 --> 00:24:31.250
through those right now.

00:24:31.260 --> 00:24:34.720
And HCA is terminology used
for the host channel adapter.

00:24:34.720 --> 00:24:37.250
I think that's sort of an
InfiniBand term just for a net.

00:24:37.260 --> 00:24:40.280
Network Interface or Host Bus Adapter.

00:24:40.280 --> 00:24:43.270
We call them HCAs.

00:24:44.130 --> 00:24:48.090
So what does InfiniBand,
the value it brings to the HPC market?

00:24:48.160 --> 00:24:52.100
It's the first industry standard
to enable server clustering.

00:24:52.310 --> 00:24:55.520
Doug mentioned Marinette
and there's Quadrix,

00:24:55.540 --> 00:24:57.700
two other ones out there that
are proprietary interconnects

00:24:57.700 --> 00:24:58.560
that are available.

00:24:58.560 --> 00:25:05.010
The clustering is the fastest
growing segment in the HPC market,

00:25:05.010 --> 00:25:10.090
so that's why as a company,
Voltaire and Apple working together

00:25:10.100 --> 00:25:12.500
are very interested in that space.

00:25:14.170 --> 00:25:18.060
Excellent performance
advantages over other options.

00:25:18.170 --> 00:25:21.900
We're currently seeing lots of
interest at the universities and labs.

00:25:22.050 --> 00:25:26.300
The DOE labs specifically are very
aggressive in pushing forward the

00:25:26.300 --> 00:25:30.550
standard and purchasing product
and implementing large clusters.

00:25:30.630 --> 00:25:32.900
And pricing has dropped quite a bit.

00:25:32.900 --> 00:25:37.240
We're certainly not at what I would
call economies of scale yet,

00:25:37.390 --> 00:25:42.260
but we've still seen about a 50% price
reduction in the last 12 months or so.

00:25:42.260 --> 00:25:44.100
And we think we'll see more
reductions as we go forward.

00:25:44.100 --> 00:25:45.280
Thanks.

00:25:45.500 --> 00:25:47.300
as volume increases.

00:25:47.300 --> 00:25:54.400
And the Virginia Tech system,
Doug mentioned, 1,105 five-note clusters,

00:25:54.400 --> 00:25:57.880
number three in last
November's top 500 list.

00:25:57.940 --> 00:26:04.640
Really key, the 5.2 million,
which is a lot of money anywhere,

00:26:04.640 --> 00:26:07.870
but for this type of system,
it is literally one-tenth the

00:26:07.920 --> 00:26:10.840
cost or greater than that of
the other systems in the top 10.

00:26:10.840 --> 00:26:15.820
Want to thank Doug and Apple for
having us here and working with them,

00:26:15.820 --> 00:26:17.910
and that's all I've got.

00:26:26.100 --> 00:26:27.000
Thanks, John.

00:26:27.000 --> 00:26:30.400
We're really excited about Voltaire's
InfiniBand offerings because for

00:26:30.400 --> 00:26:35.690
customers who are looking for a very
versatile interconnect with great

00:26:35.690 --> 00:26:40.380
latency and bandwidth properties,
InfiniBand is very attractive and gaining

00:26:40.760 --> 00:26:43.150
tremendous momentum in the HPC space.

00:26:43.570 --> 00:26:46.610
I'd like to move on,
go back to walking up our

00:26:46.610 --> 00:26:50.150
HPC building block stack here,
and take a look at compiler

00:26:50.160 --> 00:26:51.390
and optimization tools.

00:26:51.500 --> 00:26:56.380
You know, the interesting thing about
HPC is that it's really a segment

00:26:56.390 --> 00:26:58.490
of end users and developers.

00:26:58.500 --> 00:27:03.030
I've never met an HPC to deployment
that's not taking advantage of their own

00:27:03.420 --> 00:27:05.500
tools or compiling their own programs.

00:27:05.500 --> 00:27:09.920
And so the compiler and the optimization
tools that are needed to really eke out

00:27:09.920 --> 00:27:14.390
the most performance out of their code,
becomes a very important

00:27:14.390 --> 00:27:16.470
piece of the technology.

00:27:16.550 --> 00:27:19.500
From Apple's perspective, of course,
we have Xcode.

00:27:19.500 --> 00:27:23.030
And Xcode is just a phenomenal
development environment to be able to

00:27:23.240 --> 00:27:28.400
leverage the productivity features,
be able to deliver great user interface,

00:27:28.400 --> 00:27:32.410
great tools to build, develop,
debug applications.

00:27:32.500 --> 00:27:34.620
I mean,
the fact that I can write a program

00:27:34.620 --> 00:27:38.500
on my PowerBook and then send it
up to my cluster for execution,

00:27:38.500 --> 00:27:41.490
optimized for the G5,
is incredibly powerful.

00:27:41.500 --> 00:27:45.500
And so as we improve Xcode, for example,
the betas that you've received,

00:27:45.500 --> 00:27:48.500
the pre-release versions
you've received this week,

00:27:48.500 --> 00:27:51.960
actually begin to introduce
some of the 64-bit capabilities

00:27:52.050 --> 00:27:53.490
for large memory space.

00:27:53.500 --> 00:27:57.410
So you can already begin
working with those tools.

00:27:57.500 --> 00:28:01.500
A very important part of Xcode
are actually the CHUD tools.

00:28:01.500 --> 00:28:04.030
If you're not familiar with them,
CHUD stands for Computer Hardware

00:28:04.110 --> 00:28:05.500
Understanding Development.

00:28:05.500 --> 00:28:10.500
And these are tools originally written
internally within Apple to help us

00:28:10.500 --> 00:28:13.500
optimize and understand the implications
of code executing on our systems.

00:28:13.730 --> 00:28:18.650
These tools turned out to be
extremely powerful and actually

00:28:18.650 --> 00:28:23.070
have been made available as part of
our developer tool set and now are a

00:28:23.180 --> 00:28:25.500
standard part of Xcode installation.

00:28:25.500 --> 00:28:28.010
If you have Xcode installed,
you'll find them right

00:28:28.010 --> 00:28:29.350
in your developer folder.

00:28:29.520 --> 00:28:32.250
These tools are incredibly important
in this space to be able to

00:28:32.250 --> 00:28:36.800
really understand the performance
bottlenecks and implications

00:28:36.800 --> 00:28:39.500
of your code running on our systems.

00:28:39.540 --> 00:28:41.850
I've seen numerous of examples,
for example,

00:28:41.910 --> 00:28:45.760
people who are convinced their code
is processor bound on a G5 and with

00:28:46.070 --> 00:28:48.490
some simple profiling with Shark,
for example,

00:28:48.500 --> 00:28:52.190
one of the key tools in the CHUD set,
really find out maybe it's more memory

00:28:52.190 --> 00:28:55.500
bound and there's some tuning that
can be made to improve throughput.

00:28:55.500 --> 00:29:00.500
So these are very,
very important tools to our tool set.

00:29:00.500 --> 00:29:03.890
And if you have an opportunity,
I really encourage you to go to

00:29:03.890 --> 00:29:06.900
some of the sessions this week
on the CHUD tools for better

00:29:06.900 --> 00:29:08.490
understanding of their capabilities.

00:29:08.500 --> 00:29:09.990
strategies.

00:29:10.210 --> 00:29:14.340
Another important piece
of this space is Fortran.

00:29:14.340 --> 00:29:18.800
Fortran continues to be one of the
top scientific programming languages.

00:29:18.920 --> 00:29:22.210
And it's an area where Apple works
with third parties to really

00:29:22.260 --> 00:29:26.300
develop and provide great
solutions on top of Mac OS X.

00:29:26.430 --> 00:29:30.100
So it gives me a lot of pleasure to
introduce Dave Paulmark from IBM,

00:29:30.100 --> 00:29:33.230
who's going to talk about XL Fortran.

00:29:33.450 --> 00:29:33.450
Dave.

00:29:41.900 --> 00:29:43.040
Thanks, Doug.

00:29:43.170 --> 00:29:44.710
I'm really happy to be here today.

00:29:44.760 --> 00:29:49.110
It's great to be able to stand in front
of a bunch of Apple developers as an

00:29:49.110 --> 00:29:51.800
IBMer and talk about our technology.

00:29:51.800 --> 00:29:53.570
It's not just the processor this time.

00:29:53.570 --> 00:29:55.790
We're going to talk about
some software today.

00:29:55.790 --> 00:29:58.780
And some hardware, too.

00:29:58.800 --> 00:29:59.750
Let's see.

00:29:59.830 --> 00:30:00.800
There we go.

00:30:00.950 --> 00:30:05.770
So what have we brought to the
Apple processor and the Mac OS X?

00:30:05.770 --> 00:30:09.800
We've got a compiler that's
got a long history behind it.

00:30:09.800 --> 00:30:15.380
This is the IBM Fortran compiler that's
been behind our systems since the very

00:30:15.420 --> 00:30:18.800
early '90s and even going beyond that.

00:30:18.800 --> 00:30:23.800
We use this technology inside
of our C compilers as well.

00:30:23.800 --> 00:30:29.790
And we have Excel compilers for both
C++ and Fortran on this platform.

00:30:29.800 --> 00:30:33.660
And it's been used by some
very important IBM customers,

00:30:33.660 --> 00:30:36.220
mainly on AIX,
but we're starting to see some

00:30:36.220 --> 00:30:37.800
movement to Linux and Mac now.

00:30:37.800 --> 00:30:43.800
People like LLNL, NERSC, NCAR,
and a European weather forecasting group.

00:30:43.800 --> 00:30:46.800
And we deal with these people every day.

00:30:46.800 --> 00:30:47.800
We understand their problems.

00:30:47.800 --> 00:30:50.700
We understand the kinds of
applications they have to develop.

00:30:50.800 --> 00:30:54.690
And we built a compiler for them.

00:30:55.680 --> 00:31:00.600
Now, when you pick up XL Fortran,
you're not just getting performance,

00:31:00.600 --> 00:31:03.360
you're getting language
standards and conformance.

00:31:03.450 --> 00:31:05.600
So, this helps a lot with porting.

00:31:05.600 --> 00:31:09.110
If you have something that runs
somewhere else that's conformant,

00:31:09.110 --> 00:31:10.590
we're going to be able to handle that.

00:31:10.600 --> 00:31:14.880
So, we're fully Fortran 77,
fully Fortran 90, fully Fortran 95,

00:31:14.980 --> 00:31:18.640
and we started on Fortran 2003,
which we expect to be ratified

00:31:18.640 --> 00:31:20.600
hopefully the end of this year.

00:31:20.600 --> 00:31:23.820
But people have asked us for
some things early in that,

00:31:23.840 --> 00:31:27.550
and as the standard congealed
and got a little bit more stable,

00:31:27.610 --> 00:31:30.600
we went ahead and did
things like IEEE module,

00:31:30.600 --> 00:31:33.180
allocatable components,
stream IO – things that

00:31:33.180 --> 00:31:34.580
people were asking us for.

00:31:34.600 --> 00:31:39.630
And we have people on those standards
committees so that we know what's coming,

00:31:39.630 --> 00:31:41.590
and we have a voice in there.

00:31:41.590 --> 00:31:46.620
We also handle things like OpenMP,
which we also have folks on

00:31:46.620 --> 00:31:49.290
those standards committees.

00:31:50.530 --> 00:31:54.400
We're not just standards conformant,
we also have extensions.

00:31:54.400 --> 00:31:55.400
Surprised.

00:31:55.400 --> 00:31:56.400
It's Fortran.

00:31:56.400 --> 00:32:00.400
So we do open MP2.0,
fully compliant to that.

00:32:00.410 --> 00:32:04.380
On Mac OS X,
it's a technology preview as yet.

00:32:04.450 --> 00:32:08.250
It's a preview of some of the
technology that we've deployed

00:32:08.250 --> 00:32:11.400
on AIX and Linux PowerPC.

00:32:11.420 --> 00:32:13.210
That's been out there
for quite a while now.

00:32:13.420 --> 00:32:16.400
But we do other things
from other companies.

00:32:16.400 --> 00:32:20.400
We've got Cray pointers,
128-bit floating point, 64-bit ints,

00:32:20.400 --> 00:32:24.260
structure record union map,
and so on and so on.

00:32:24.430 --> 00:32:27.920
There's way too many options
to talk about to try and

00:32:27.950 --> 00:32:30.400
describe them all in this group.

00:32:30.400 --> 00:32:34.370
But suffice to say,
things like structure record union map,

00:32:34.390 --> 00:32:37.400
we've had customers come
to us with things like,

00:32:37.400 --> 00:32:43.390
"We would like to buy IBM hardware,
but you don't have this." Well,

00:32:43.680 --> 00:32:45.400
they got it.

00:32:45.400 --> 00:32:48.400
And now they have IBM hardware.

00:32:48.400 --> 00:32:50.400
We do that sort of thing all the time.

00:32:50.400 --> 00:32:51.400
These kinds of requests come in.

00:32:51.530 --> 00:32:57.350
We want to hear what you need,
and we'll talk to you about that.

00:32:57.400 --> 00:33:02.250
We've also got some very important
extensions for the PowerPC in particular.

00:33:02.430 --> 00:33:09.660
The PowerPC hardware intrinsics functions
and directives get you access at a source

00:33:09.670 --> 00:33:14.380
level to the hardware instructions.

00:33:14.460 --> 00:33:16.450
You can use something as a
directive or as a function call,

00:33:16.460 --> 00:33:19.140
and what you're going to get
there is a particular instruction

00:33:19.140 --> 00:33:20.400
that you need at that point.

00:33:20.400 --> 00:33:24.390
Something like data prefetch,
for example, is very powerful.

00:33:24.510 --> 00:33:27.530
We also give you an XLF utility
module that you can use to get access

00:33:27.530 --> 00:33:29.400
to some common system services.

00:33:29.400 --> 00:33:31.400
You don't have to go off
and code that yourself.

00:33:33.080 --> 00:33:38.160
Now, we're in Xcode,
and that's real exciting.

00:33:38.250 --> 00:33:39.880
But for folks that still
like their Unix command line,

00:33:39.890 --> 00:33:40.460
we're there too.

00:33:40.460 --> 00:33:41.730
We make files still work.

00:33:41.800 --> 00:33:43.950
GDB works with us.

00:33:43.990 --> 00:33:45.920
We work well with GDB.

00:33:45.920 --> 00:33:49.380
Now, as you go up the op levels,
obviously things start to go down a bit,

00:33:49.390 --> 00:33:51.670
but we've given – the
support goes down a bit,

00:33:51.670 --> 00:33:54.920
but we do have some directives and
so on that you can put in your source

00:33:54.920 --> 00:33:58.380
at certain points to get you the
information you need to debug that stack

00:33:58.380 --> 00:34:02.710
– that trace back or whatever it is
you're having trouble with at the time.

00:34:03.590 --> 00:34:06.790
Something that isn't here
that shouldn't be – Shark.

00:34:06.790 --> 00:34:09.360
I love that tool.

00:34:09.360 --> 00:34:11.230
I wish we had that on AIX.

00:34:11.230 --> 00:34:14.830
We work well with Shark,
and that's the message.

00:34:14.900 --> 00:34:15.230
Use it.

00:34:15.320 --> 00:34:17.320
You can find some amazing things.

00:34:17.320 --> 00:34:19.450
It's one of the most popular
things in Toronto for digging

00:34:19.450 --> 00:34:21.910
down into these problems that we
get from our customers when we're

00:34:21.910 --> 00:34:23.430
analyzing performance problems.

00:34:23.440 --> 00:34:27.500
But it's not always just
debuggers and so on.

00:34:27.500 --> 00:34:31.280
We also give you some options to use
for finding problems in your code.

00:34:31.280 --> 00:34:34.480
So you can automatically
– Insert checks to find,

00:34:34.480 --> 00:34:37.280
you know, oh,
I went off the bounds of that array.

00:34:37.520 --> 00:34:39.740
It'll trap and tell you that,
stop you from going off

00:34:39.740 --> 00:34:40.680
and corrupting memory.

00:34:40.680 --> 00:34:43.090
You know,
automatic initialization of variables

00:34:43.100 --> 00:34:46.290
where you need that to happen,
and a rich set of listing information

00:34:46.290 --> 00:34:50.000
that you can dig through to understand
what's going on with your program.

00:34:52.720 --> 00:34:53.860
The runtime environment.

00:34:53.860 --> 00:34:58.320
We have our own Fortran runtime
that we ship with the compiler.

00:34:58.320 --> 00:35:01.710
And the message there is that is
something that if you build an

00:35:01.760 --> 00:35:07.250
application with Excel Fortran,
you can take that runtime and give it to

00:35:07.330 --> 00:35:15.310
your customers as well so that they can
run that Fortran code on their systems.

00:35:16.260 --> 00:35:19.940
We give you a lot of tuning
levers and buttons and dials

00:35:20.030 --> 00:35:22.080
through environment variables.

00:35:22.080 --> 00:35:25.910
And you can control things such as
the characteristics of the I/O that's

00:35:25.910 --> 00:35:29.200
going on when you're doing that,
error reporting,

00:35:29.200 --> 00:35:31.740
what kinds of messages you want,
do you want to know when

00:35:31.750 --> 00:35:34.680
you're doing something that
isn't Fortran 90 conformant,

00:35:34.680 --> 00:35:35.390
for example.

00:35:35.450 --> 00:35:37.190
You can do that sort of thing.

00:35:37.200 --> 00:35:40.060
But in this space, certainly,
thread scheduling models,

00:35:40.070 --> 00:35:43.200
number of threads,
thread profiling environment variables,

00:35:43.200 --> 00:35:44.200
these are all important things.

00:35:44.200 --> 00:35:46.670
and of course all the
things that OpenMP defines,

00:35:46.720 --> 00:35:48.770
we've got those.

00:35:49.250 --> 00:35:53.860
Now binary compatibility is a
very important thing in this.

00:35:53.860 --> 00:35:59.030
You can take our objects,
work with other objects from GCC, G++,

00:35:59.290 --> 00:36:01.900
and of course IBM Excel C and C++.

00:36:02.060 --> 00:36:04.700
Take that whole bundle, put it together,
and there you go.

00:36:04.700 --> 00:36:08.100
You've got your applications mixed
as many languages as you like.

00:36:08.260 --> 00:36:13.350
And we've added some things like QFloat,
Complex, GCC, -QX name,

00:36:13.350 --> 00:36:15.030
just option names.

00:36:15.150 --> 00:36:17.880
The message is, where we need them,
we've added some things to help

00:36:17.880 --> 00:36:20.710
out with that binary compatibility.

00:36:24.830 --> 00:36:29.340
Now, we exist because of optimization.

00:36:29.420 --> 00:36:34.030
If we weren't a good optimizing compiler,
we wouldn't be there in Toronto doing

00:36:34.030 --> 00:36:36.600
this every day for the last 14,
15 years.

00:36:36.620 --> 00:36:44.540
So, the optimization components that are
in XL4TRAN are in all of IBM's core

00:36:44.540 --> 00:36:46.720
compilers and all our important systems.

00:36:46.720 --> 00:36:56.570
C++, COBOL, PL1, on AIX, Linux,
the mainframes, Mac, and now P-Series,

00:36:56.570 --> 00:36:59.740
I-Series, and of course G4, G5.

00:36:59.740 --> 00:37:03.560
The message is, we've taken all that,
that we've built up over

00:37:03.670 --> 00:37:06.740
those number of years,
and all those different platforms,

00:37:06.740 --> 00:37:09.080
and brought it down
to the Apple platform,

00:37:09.160 --> 00:37:13.140
and we're seeing some really
important success with that.

00:37:13.350 --> 00:37:21.320
The XL compilers are used by IBM on
AIX to announce spec performance numbers.

00:37:21.320 --> 00:37:25.200
So again, the message is we know how
to tune for those chips.

00:37:25.420 --> 00:37:28.440
IBM does the chips,
we work with the chip designers,

00:37:28.540 --> 00:37:31.200
we know what's coming,
we know how to tune for those things.

00:37:31.200 --> 00:37:34.650
And we build our own software with it.

00:37:34.650 --> 00:37:40.780
AIX, DB2, Lotus, Domino – they're all
built with IBM compilers,

00:37:40.780 --> 00:37:40.780
as you might expect.

00:37:42.390 --> 00:37:45.240
Optimization options.

00:37:45.240 --> 00:37:50.810
We go to five at the base level,
so zero all the way to 05.

00:37:50.980 --> 00:37:54.860
And you can go from basically
almost no optimization up to,

00:37:54.940 --> 00:37:57.540
wow, what did this thing do to my code?

00:37:57.540 --> 00:38:01.490
I can't recognize it anymore.

00:38:01.900 --> 00:38:05.400
And we've got a whole set of, again,
switches, dials, knobs,

00:38:05.400 --> 00:38:08.130
and levers that you can play
with in order to tune the

00:38:08.130 --> 00:38:11.800
optimization to what you need to
have happen on your application.

00:38:12.230 --> 00:38:17.130
Things like -qhot enables the high
order transformation loop optimizer.

00:38:17.130 --> 00:38:21.800
It was built to understand Fortran
90 array language and syntax.

00:38:21.840 --> 00:38:25.800
It can take those loops and do
some amazing things with them.

00:38:25.910 --> 00:38:27.970
It'll also work with C code,
C loops as well,

00:38:27.970 --> 00:38:29.760
when you use it in our C compiler.

00:38:29.800 --> 00:38:34.020
The qarch option tells
you on the Mac OS machine,

00:38:34.020 --> 00:38:39.970
do you want to target a generic PowerPC,
in other words, G4,

00:38:39.970 --> 00:38:42.880
or do you want to go to G5,
which I'm sure most of this

00:38:42.880 --> 00:38:43.800
group is interested in.

00:38:43.800 --> 00:38:46.710
And that enables,
inside of our optimizer,

00:38:46.710 --> 00:38:51.800
all the modeling and tuning
capabilities that we bring to bear from,

00:38:51.800 --> 00:38:54.800
that we brought up,
and specifically done, well,

00:38:54.800 --> 00:38:57.800
person years of effort tuning to the G5.

00:38:57.800 --> 00:39:01.160
Gives you access to all - well,
it doesn't give you,

00:39:01.160 --> 00:39:06.690
it gives the optimizer - using qarch
G5 allows the optimizer to precisely

00:39:06.730 --> 00:39:09.800
model your code as it's going to see it.

00:39:09.840 --> 00:39:13.020
Because it understands the chip,
understands how many units are going,

00:39:13.020 --> 00:39:14.770
and how to keep that processor busy.

00:39:14.790 --> 00:39:17.790
That's what it's trying to
do with the scheduling model.

00:39:17.800 --> 00:39:22.390
And using qarch G5 also gives you access
to those rich set of PowerPC intrinsics,

00:39:22.490 --> 00:39:25.800
again, that you can use for
things like cache control,

00:39:25.800 --> 00:39:25.800
certain arithmetic options.

00:39:25.800 --> 00:39:29.800
That you might need.

00:39:29.800 --> 00:39:32.900
And floating point control,
you want to toggle things in

00:39:32.920 --> 00:39:35.680
the status and control register,
for example.

00:39:35.800 --> 00:39:39.820
And the nice thing about that,
if you're interested in moving your

00:39:39.840 --> 00:39:43.710
code from one IBM kind of system,
one IBM chip to another,

00:39:43.710 --> 00:39:49.800
those same intrinsics work on compatible
chips if you're going to AIX or Linux.

00:39:49.800 --> 00:39:51.730
Same story in the other direction.

00:39:51.780 --> 00:39:53.800
You have some code up there on AIX,
you want to bring down to G5,

00:39:53.800 --> 00:39:53.800
those intrinsics are going to work too.

00:39:53.970 --> 00:39:56.350
Those intrinsics are going to work too.

00:39:58.450 --> 00:40:03.030
IPA is sort of the keystone of
our optimization technology and

00:40:03.030 --> 00:40:08.200
really differentiates us in what
we can do with your application.

00:40:08.200 --> 00:40:14.220
When you've got IPA involved in your
compiler and in your compile and

00:40:14.220 --> 00:40:21.740
it runs automatically at 04 and 05,
what it does for you is when you

00:40:21.780 --> 00:40:24.630
compile your code is it inserts
information into your object.

00:40:25.130 --> 00:40:27.020
Which is essentially
invisible to the linker.

00:40:27.020 --> 00:40:30.080
So if you just take those
objects and feed them into LD,

00:40:30.120 --> 00:40:33.480
out comes your ADOT out and you're happy.

00:40:33.510 --> 00:40:38.530
But if you then use IPA when
you link your application,

00:40:38.600 --> 00:40:42.220
it then extracts that information
that's hidden away in the .Os

00:40:42.300 --> 00:40:48.010
and re-optimizes your code again,
this time not on a file-by-file basis,

00:40:48.010 --> 00:40:50.140
but it's got the entire
application there.

00:40:50.150 --> 00:40:53.080
It's got all the .Os that
make up the whole thing.

00:40:53.110 --> 00:40:57.150
And it understands that called that,
and it was called with this,

00:40:57.150 --> 00:40:59.080
and it was called with that,
and so we don't have to

00:40:59.080 --> 00:41:01.940
worry about this parameter,
we'll just stick a 7 in there.

00:41:02.180 --> 00:41:03.200
That kind of thing.

00:41:03.270 --> 00:41:07.570
So what it can do is it re-partitions
your application into more logical

00:41:07.570 --> 00:41:12.960
units that keep memory together
and does massive amounts of

00:41:12.980 --> 00:41:15.370
inlining where it makes sense.

00:41:15.410 --> 00:41:18.640
And it can even go across languages.

00:41:18.710 --> 00:41:22.730
So if you build your
application mixed mode with C,

00:41:22.730 --> 00:41:26.740
C++, and Fortran, if you build all that
with IBM Excel compilers,

00:41:26.770 --> 00:41:31.140
run the IPA link step,
it will do things like take

00:41:31.250 --> 00:41:34.700
your C code and inline it
into your Fortran application.

00:41:34.710 --> 00:41:41.570
And that's an amazing technology that
we've been able to bring down to the G5.

00:41:43.190 --> 00:41:46.480
And of course after it does all that,
then we go back down into the

00:41:46.570 --> 00:41:51.070
low level optimizer again,
which is the one that really

00:41:51.510 --> 00:41:55.000
understands the chip and tunes for that.

00:41:57.030 --> 00:42:01.170
PDF – Profile Directed Feedback
is another important technology,

00:42:01.290 --> 00:42:06.270
especially useful for codes where you
may have it instrumented with debug or

00:42:06.270 --> 00:42:10.840
perhaps some tuning information that
you want to use to gather statistics.

00:42:10.970 --> 00:42:18.760
What PDF will do for you is you build
your application once with -qpdf1,

00:42:18.770 --> 00:42:22.470
run your application
with typical sample data,

00:42:22.690 --> 00:42:29.140
That will write out a statistics file,
compile your application again with PDF2,

00:42:29.150 --> 00:42:33.490
and it will read that statistics file,
and that will tell the optimizer, "Oh,

00:42:33.490 --> 00:42:37.660
look, 99% of the time you take
the branch this way,

00:42:37.660 --> 00:42:42.540
not this way." And so we can take
your most frequently executed

00:42:42.540 --> 00:42:45.840
code and put that in line,
and the stuff that almost never

00:42:45.840 --> 00:42:48.630
executes goes off to the side,
and you get a much better

00:42:48.640 --> 00:42:50.800
performance out of that.

00:42:50.800 --> 00:42:53.740
And of course, again,
the message is the Excel compilers

00:42:53.740 --> 00:42:55.620
share the technology,
so you can,

00:42:55.680 --> 00:43:00.100
if you build stuff with our C compilers,
use PDF, you can mix that in with

00:43:00.110 --> 00:43:01.570
the Fortran compiler.

00:43:03.660 --> 00:43:08.990
OpenMP and SMP are very
important to this space,

00:43:08.990 --> 00:43:11.890
and we've got a lot of
experience with these.

00:43:12.000 --> 00:43:16.040
Again,
technology preview on Mac OS X right now,

00:43:16.130 --> 00:43:20.170
but again, we're bringing that down from
some platforms where we've had

00:43:20.170 --> 00:43:22.030
a lot of time to work on that.

00:43:22.210 --> 00:43:27.330
We fully implement the 2.0 standard and
the important thing about OpenMP for

00:43:27.330 --> 00:43:32.820
us is our optimizer fully understands
what OpenMP is and what SMP is.

00:43:32.820 --> 00:43:38.420
And so we can take things like a -qsmp
auto option and put it in our compiler

00:43:38.880 --> 00:43:42.930
where it can take a look at your
application and automatically parallelize

00:43:42.930 --> 00:43:45.300
things where it makes sense to do so.

00:43:45.300 --> 00:43:48.570
So you've got a couple of choices
in the way you want to do things.

00:43:48.580 --> 00:43:50.260
If you want to code to
the OpenMP standard,

00:43:50.260 --> 00:43:51.060
that's great.

00:43:51.180 --> 00:43:52.090
We'll handle that.

00:43:52.190 --> 00:43:56.140
But we'll also automatically
parallelize for you where we can.

00:43:56.410 --> 00:43:58.700
And again,
it's another one with dozens of switches

00:43:58.700 --> 00:44:01.180
that I can't talk about right now.

00:44:02.740 --> 00:44:10.080
We give you lots of directives
and options on the optimizer,

00:44:10.190 --> 00:44:11.770
as I said before.

00:44:11.780 --> 00:44:15.740
There's a couple of variations on this,
where you can go into your source

00:44:15.740 --> 00:44:20.020
and say things about your code,
say this loop has this characteristic,

00:44:20.060 --> 00:44:23.520
and that'll give the optimizer even more
opportunities to go and do things that it

00:44:23.520 --> 00:44:24.960
might not be able to recognize otherwise.

00:44:25.700 --> 00:44:28.880
But in some cases you want
to constrain the optimizer.

00:44:28.960 --> 00:44:33.920
A lot of older code especially may
not be 100% standards compliant,

00:44:34.020 --> 00:44:38.570
so things like -q alias, non-standard,
will let you crank up the

00:44:38.570 --> 00:44:42.240
optimization level and still
have your code run correctly,

00:44:42.250 --> 00:44:48.970
even though it might not be as opportune
as if your code was standards conformant.

00:44:49.190 --> 00:44:54.030
And of course things like -qprefetch and
- will automatically insert prefetching

00:44:54.040 --> 00:44:55.930
directives where that's useful.

00:44:55.940 --> 00:45:01.240
Had a great example of that yesterday
where we had a gentleman in the lab

00:45:01.250 --> 00:45:02.930
across the hall working with us.

00:45:02.940 --> 00:45:06.860
We brought his code in and
just with some analysis with

00:45:06.860 --> 00:45:11.940
Shark and looking at things,
we stuck in one directive and speeded

00:45:11.940 --> 00:45:17.960
up the core loop in his application by a
factor of two just by doing a prefetch.

00:45:21.990 --> 00:45:28.160
So, the summary is: IBM XL, Fortran,
and XL-C bring to the Apple G5

00:45:28.160 --> 00:45:32.950
systems technology that's been
in the works at IBM since,

00:45:33.040 --> 00:45:35.660
honestly, the mid-80s.

00:45:35.700 --> 00:45:37.340
And it's been improved every year.

00:45:37.340 --> 00:45:42.690
We have a large team in Toronto,
and we work closely with the chip folks.

00:45:43.390 --> 00:45:46.780
We're fully backed by IBM's
premier customer service.

00:45:46.870 --> 00:45:52.300
It doesn't matter if you buy the compiler
from Appsoft or you buy it from IBM.

00:45:52.340 --> 00:45:55.770
It's still the team in Toronto that's
going to be looking after you.

00:45:56.560 --> 00:46:02.380
Our standards compliance and the
large range of extensions that

00:46:02.380 --> 00:46:06.400
we have let you bring your code
down from pretty much anywhere.

00:46:06.400 --> 00:46:09.400
It will help you out with
things that you need.

00:46:09.480 --> 00:46:11.310
Thank you.

00:46:16.800 --> 00:46:18.180
Thanks Dave.

00:46:18.180 --> 00:46:20.680
Great, thank you Dave.

00:46:23.250 --> 00:46:26.410
Okay, continuing along our stack,
I wanted to talk a little bit

00:46:26.410 --> 00:46:29.340
about communications middleware.

00:46:29.340 --> 00:46:33.070
This is typically what we see
as the MPI layers of a cluster.

00:46:33.140 --> 00:46:37.100
The great thing about Mac OS X, again,
leveraging off that Unix foundation,

00:46:37.100 --> 00:46:41.170
is that just about all the major
MPI stacks have been brought

00:46:41.180 --> 00:46:43.930
over to Mac OS X and run really,
really well.

00:46:44.100 --> 00:46:47.350
As a matter of fact,
some of them have been really optimized

00:46:47.350 --> 00:46:49.670
for Mac OS X and are available,
for example,

00:46:49.780 --> 00:46:51.970
LAMMPI as a package installer
for really ease of installation

00:46:52.360 --> 00:46:54.060
right on top of Mac OS X.

00:46:54.180 --> 00:46:56.100
So, great selection of tools.

00:46:56.100 --> 00:46:58.690
As a matter of fact,
if you have experience with

00:46:58.690 --> 00:47:02.080
a particular MPI stack,
hopefully you'll see that the

00:47:02.080 --> 00:47:07.080
exact same stack is available on
Mac OS X and can leverage that

00:47:07.080 --> 00:47:10.100
familiarity on the platform.

00:47:10.100 --> 00:47:13.100
So, both open source and commercial
stacks available for Mac OS X.

00:47:14.100 --> 00:47:16.100
There are a number of
other pieces of middleware.

00:47:16.100 --> 00:47:24.250
Obviously, we talked about OpenMP,
Globus, PVM, Paradise Linda from SCA,

00:47:24.570 --> 00:47:29.210
and a recent product, Accelerate,
from Gridiron Software,

00:47:29.210 --> 00:47:32.100
all also fall into this
communications middleware stack.

00:47:32.100 --> 00:47:35.880
And, of course,
all are available on Mac OS X.

00:47:37.080 --> 00:47:39.800
Finally,
I wanted to touch on management tools.

00:47:39.800 --> 00:47:43.140
This is an area where we
think Mac OS X really shines.

00:47:43.370 --> 00:47:45.390
Because again,
you have the best of breed tools

00:47:45.390 --> 00:47:48.320
available from Apple to really
make managing these systems,

00:47:48.520 --> 00:47:51.460
particularly head nodes and
things where you're providing file

00:47:51.460 --> 00:47:54.100
services and network services,
are able to provide very,

00:47:54.100 --> 00:47:56.450
very ease of use for system
administrators managing,

00:47:56.450 --> 00:47:59.180
you know, whether it be a small
cluster or a large cluster.

00:47:59.660 --> 00:48:03.120
We also have the benefit of great
open source tools to really provide

00:48:03.120 --> 00:48:05.060
added value and functionality.

00:48:05.060 --> 00:48:07.220
So if we drill into this, of course,
first of all,

00:48:07.220 --> 00:48:09.150
we start with Apple's management tools.

00:48:09.160 --> 00:48:11.700
Server admin,
work group manager for providing

00:48:11.700 --> 00:48:15.310
kind of the bread and butter,
you know, file services, DNS, DHCP,

00:48:15.310 --> 00:48:18.920
directory services, things that kind of,
you know, you forget about, but,

00:48:18.920 --> 00:48:20.320
you know, it's a network infrastructure.

00:48:20.320 --> 00:48:23.010
You need these to support
cluster operations.

00:48:23.020 --> 00:48:27.570
I wanted to highlight Server Monitor,
the tool that's unique to Xserve.

00:48:27.640 --> 00:48:29.530
Xserve G5 has over 30,000 servers.

00:48:29.620 --> 00:48:31.180
It has over 30 sensors
on the logic board.

00:48:31.180 --> 00:48:33.250
I like to joke it's one of
the most instrumented one-use

00:48:33.310 --> 00:48:34.430
servers in the industry.

00:48:34.840 --> 00:48:38.950
Server Monitor is the tool that allows
you to wrap up that data and provide that

00:48:39.010 --> 00:48:41.540
status information about the hardware.

00:48:41.930 --> 00:48:45.770
Temperatures, predictive drive failures,
power consumption,

00:48:45.770 --> 00:48:48.820
all that data is available
in Server Monitor.

00:48:48.820 --> 00:48:51.780
It's a great complement when you're
managing a large number of machines.

00:48:51.800 --> 00:48:55.600
Beyond that,
we also have a new piece of technology

00:48:55.680 --> 00:48:58.920
from Apple introduced not too long ago.

00:48:58.920 --> 00:48:59.560
It's a technology that's really,
really cool.

00:48:59.560 --> 00:49:00.060
It's called Xgrid.

00:49:00.060 --> 00:49:01.050
It's a technology preview,
which is Xgrid.

00:49:01.060 --> 00:49:04.270
Again,
taking that ease-of-use approach of how

00:49:04.270 --> 00:49:07.690
do we make deploying clusters easier,
how do we make distributed

00:49:07.770 --> 00:49:12.100
computing easier,
Xgrid's really a great solution

00:49:12.100 --> 00:49:16.130
for these class of problems where
you want to distribute workloads

00:49:16.130 --> 00:49:17.860
across a number of machines.

00:49:17.860 --> 00:49:21.530
What's interesting about it is
that not only can it take advantage

00:49:21.530 --> 00:49:24.760
of dedicated cluster resources,
such as a rack of Xserves,

00:49:24.790 --> 00:49:29.490
you can also bring ad hoc
resources through rendezvous,

00:49:29.500 --> 00:49:31.710
and you can bring technologies
out across to desktops and

00:49:31.710 --> 00:49:33.600
other machines on your network.

00:49:33.740 --> 00:49:37.550
The recent technology preview, too,
added MPI support,

00:49:37.840 --> 00:49:42.240
which makes running and dispatching
MPI jobs across your cluster much easier.

00:49:42.240 --> 00:49:44.180
And of course,
it provides great user interface

00:49:44.180 --> 00:49:47.640
all the way down to the tachometer
to let you see how much performance

00:49:47.640 --> 00:49:49.080
you're getting on your jobs.

00:49:49.160 --> 00:49:51.660
So we're really excited about Xgrid.

00:49:51.660 --> 00:49:55.620
And of course,
now with it being brought into Tiger,

00:49:55.640 --> 00:49:59.440
it's going to be very broadly
available to Mac and Mac users.

00:49:59.440 --> 00:50:01.700
ECOS 10 systems.

00:50:03.390 --> 00:50:06.880
Finally, I wanted to touch on some of
the leading open source and

00:50:06.910 --> 00:50:11.290
commercial tools in this space,
most notably schedulers.

00:50:11.300 --> 00:50:14.660
Again,
the top schedulers available in the

00:50:14.660 --> 00:50:17.300
industry are available on Mac OS X.

00:50:17.300 --> 00:50:25.450
Platform LSF in the commercial space,
PBS and OpenPBS, Sun Grid Engine,

00:50:25.450 --> 00:50:28.770
now called N1 Grid Engine,
even the Maui Scheduler

00:50:28.770 --> 00:50:30.300
are available for Mac OS X.

00:50:30.300 --> 00:50:33.540
And also some of the leading
cluster management monitoring tools,

00:50:33.540 --> 00:50:38.300
tools like Gangly and Big Brother,
also available for Mac OS X.

00:50:38.300 --> 00:50:41.300
And very valuable resources there.

00:50:42.130 --> 00:50:44.690
So in summary,
if we look from all the way from the

00:50:44.690 --> 00:50:47.880
hardware up to the management tools,
we have a really compelling set

00:50:47.880 --> 00:50:49.730
of products and technologies,
both from Apple and

00:50:49.730 --> 00:50:53.100
industry-leading third parties,
that allow you to build really

00:50:53.100 --> 00:50:58.680
phenomenal cluster solutions
with Mac OS X and PowerPC G5 at

00:50:58.680 --> 00:51:01.640
the foundation of this stack.

00:51:01.700 --> 00:51:06.720
What I'd like to do is now introduce
some customers who are going to talk

00:51:07.050 --> 00:51:11.160
about how they've deployed Xserve and
Mac OS X Server to solve some of their

00:51:11.160 --> 00:51:13.170
high performance computing needs.

00:51:13.600 --> 00:51:15.700
First customer I'd like
to introduce is actually

00:51:15.720 --> 00:51:20.250
Ben Singer from Princeton University,
who's going to talk about his

00:51:20.250 --> 00:51:23.790
deployment of Xserve in their center.

00:51:23.790 --> 00:51:23.790
Ben?

00:51:30.290 --> 00:51:31.200
Thanks, Doug.

00:51:31.200 --> 00:51:34.640
It's a delight to be here.

00:51:34.680 --> 00:51:37.610
I'm here to talk about – a little bit
about the Princeton XServe cluster

00:51:37.610 --> 00:51:39.700
at the Center for the Study of Brain,
Mind,

00:51:39.700 --> 00:51:41.800
and Behavior that we're still setting up.

00:51:41.980 --> 00:51:45.110
We got it about a month ago and

00:51:45.300 --> 00:52:44.900
[Transcript missing]

00:52:45.450 --> 00:52:50.170
So really what we are is a place that
provides resources for all these faculty.

00:52:50.180 --> 00:52:53.850
And we have staff and we have
resources in the computing

00:52:54.070 --> 00:52:56.230
and data acquisition area.

00:52:56.630 --> 00:53:01.520
And on the staff side,
there's software engineers,

00:53:01.520 --> 00:53:05.660
MRI physicists, system administrator,
and administrators for

00:53:05.660 --> 00:53:06.700
running the center.

00:53:06.700 --> 00:53:12.270
The big data acquisition instrument
that I was alluding to is the

00:53:12.410 --> 00:53:18.220
MRI brain scanner from Siemens
that we picked up a few years ago.

00:53:18.280 --> 00:53:20.700
And it was the first – at
the time it was installed,

00:53:20.700 --> 00:53:22.690
it was the first
research-only installation.

00:53:22.700 --> 00:53:27.600
So most of the time when you use an MRI,
it's in a hospital setting.

00:53:27.600 --> 00:53:32.030
So it has first priority for clinical
applications and you end up doing

00:53:32.030 --> 00:53:34.700
work at 3 in the morning or something.

00:53:34.700 --> 00:53:39.510
And one nice thing about our
facility is that it's there just

00:53:39.590 --> 00:53:45.700
a few doors away in the psychology
building from the CSBMB staff center.

00:53:45.700 --> 00:53:50.190
And that provides all the data that
I'm going to be talking about and

00:53:50.190 --> 00:53:52.680
why we ended up getting an Xserve.

00:53:52.700 --> 00:53:56.980
We already had a file server when
I went shopping for a cluster,

00:53:56.980 --> 00:54:00.510
which was actually the first
thing they had me do when

00:54:00.510 --> 00:54:02.700
I came about six months ago.

00:54:02.710 --> 00:54:05.700
And that was in place already.

00:54:05.700 --> 00:54:09.860
It's a BlueArc 9TB file server
to store all this data that

00:54:10.220 --> 00:54:11.700
comes from the MRI brain scanner.

00:54:11.710 --> 00:54:14.700
And we need to back it up
and we need to process it.

00:54:14.700 --> 00:54:18.690
So that's how we ended up
with 64 Xserve G5 nodes.

00:54:18.920 --> 00:54:23.390
And I'm going to explain a little
bit about how we chose the Xserve.

00:54:23.890 --> 00:54:27.080
But before I do that,
I want to just say what it is from a

00:54:27.240 --> 00:54:30.700
computing perspective that what we do.

00:54:30.700 --> 00:54:37.640
What we do is motivating us to
pursue an Xserve in the first place.

00:54:37.870 --> 00:54:41.700
We have a whole lot of brain
data coming out of the MRI.

00:54:41.700 --> 00:54:45.650
A single study will produce
hundreds of gigabytes of data.

00:54:45.740 --> 00:54:49.800
You take a single scan from somebody
and if you're doing functional MRI,

00:54:49.860 --> 00:54:52.780
even though a single slice of
the brain is at lower resolution,

00:54:52.790 --> 00:54:56.660
64 squared image,
you're taking 25 slices and then you're

00:54:56.750 --> 00:54:58.700
taking maybe 30 of these a second.

00:54:58.860 --> 00:55:03.140
And in one experiment recently
we had subjects watch Raiders of

00:55:03.230 --> 00:55:07.610
the Lost Ark for two hours and
recorded their brain for two hours.

00:55:07.720 --> 00:55:09.630
That produces a lot of data.

00:55:09.780 --> 00:55:13.700
And so we did that with multiple
subjects too because we want to see

00:55:13.700 --> 00:55:18.580
are their brains doing the same thing
when they're watching this movie.

00:55:18.700 --> 00:55:20.700
That's sort of a fun example.

00:55:20.930 --> 00:55:25.690
And to crunch through that is
going to take some computing power.

00:55:25.700 --> 00:55:27.670
The other thing is people are
moving their head in the scanner.

00:55:27.710 --> 00:55:30.570
They have a little head rest
so we tell them not to move but

00:55:30.660 --> 00:55:32.690
they still do and that's natural.

00:55:32.870 --> 00:55:38.240
And so we need to align every image with
the first one or some reference and that

00:55:38.240 --> 00:55:40.480
takes a lot of time in the workflow.

00:55:40.700 --> 00:55:43.690
So does filtering in space and time.

00:55:43.700 --> 00:55:45.680
There's a lot of noise coming.

00:55:45.740 --> 00:55:49.330
This data when you first get it is
not like suddenly something pops out

00:55:49.330 --> 00:55:53.700
at you and you know exactly what's
happening except in very simple cases.

00:55:53.860 --> 00:55:54.690
There's a lot of noise in the data.

00:55:54.700 --> 00:55:56.650
It needs to be filtered out.

00:55:56.710 --> 00:56:01.700
There's other machines in the room
that will put a signature in the data,

00:56:01.700 --> 00:56:04.700
maybe some low frequency noise,
maybe high frequency noise.

00:56:04.700 --> 00:56:06.700
So you have to do filtering.

00:56:06.810 --> 00:56:10.560
And then finally you need to
do a statistical analysis.

00:56:10.700 --> 00:56:14.330
And you're comparing brains where
they were just sitting there doing

00:56:14.340 --> 00:56:17.700
nothing with when they were doing
the task that you have them do.

00:56:17.700 --> 00:56:21.700
And so comparing those two things
is a simple statistical test.

00:56:21.700 --> 00:56:23.580
But you need to do it for
every voxel in the brain.

00:56:23.700 --> 00:56:25.670
So that's thousands,
hundreds of thousands of voxels.

00:56:25.780 --> 00:56:33.700
And that can take traditionally days
of CPU time to do a single study.

00:56:34.540 --> 00:56:38.470
And one problem with that is that when
people are – they've got all this data

00:56:38.500 --> 00:56:42.350
and it takes all this time to analyze it,
they don't tend to play with it much.

00:56:42.360 --> 00:56:45.590
They don't tend to try new things or
look at it from a new angle because

00:56:45.640 --> 00:56:47.510
there's a big cost to doing that.

00:56:47.620 --> 00:56:50.000
They're going to tie up the
lab resources for a day.

00:56:50.000 --> 00:56:54.380
They can't just put this data on
their portable and run away with it.

00:56:54.440 --> 00:56:57.050
They have to stay and use
up the center resources.

00:56:57.050 --> 00:57:00.390
And sometimes people won't do it.

00:57:00.390 --> 00:57:03.150
And so it just stifles creativity.

00:57:03.150 --> 00:57:04.480
That's one thing.

00:57:04.500 --> 00:57:06.290
So why did we choose Xserve?

00:57:06.780 --> 00:57:11.060
Well, when I first started looking
– and we all were a group,

00:57:11.160 --> 00:57:14.830
but I was sort of the one that
was doing it at the time – I got

00:57:14.830 --> 00:57:16.480
really my head deep into benchmarks.

00:57:16.500 --> 00:57:20.890
And although the Xserve does
really well with benchmarks,

00:57:20.890 --> 00:57:23.780
I think the reason we chose it
wasn't just because of benchmarks.

00:57:23.780 --> 00:57:27.020
But anyway,
let me point out the benchmark

00:57:27.110 --> 00:57:29.840
that I have on the slide.

00:57:29.840 --> 00:57:33.430
The AFNI Speedo score is
from the AFNI package.

00:57:33.430 --> 00:57:34.480
It's from the
National Institute of Technology.

00:57:34.500 --> 00:57:36.460
It's from the
National Institute of Health.

00:57:36.640 --> 00:57:40.400
It's a free software package
for analyzing MRI brain scans.

00:57:40.400 --> 00:57:46.760
And off the website where they published
their single processor 32-bit benchmarks

00:57:47.510 --> 00:57:50.180
come the bottom three bars here.

00:57:50.180 --> 00:57:53.210
And then I ran it last
week on our Xserve,

00:57:53.240 --> 00:57:55.820
and it came out a little better.

00:57:55.820 --> 00:57:58.340
This benchmark tests the whole system.

00:57:58.390 --> 00:58:01.720
So maybe it was IO or something
that caused the Xserve to

00:58:01.780 --> 00:58:04.460
do better than the desktop,
even though it's a little

00:58:04.460 --> 00:58:04.460
bit more expensive.

00:58:04.460 --> 00:58:22.930
But anyway,
let me point out the benchmark

00:58:23.630 --> 00:58:34.330
that I have on the slide.

00:58:34.410 --> 00:58:34.440
The Xserve does really
well with benchmarks.

00:58:34.440 --> 00:58:34.440
And although it has the same speed chip.

00:58:34.440 --> 00:58:38.120
We also knew that in the future,
Apple's operating system would be

00:58:38.120 --> 00:58:43.050
fully 64-bit to do a fair comparison
with that compile on 64-bit Linux.

00:58:43.110 --> 00:58:44.460
So we knew it would get better.

00:58:47.750 --> 00:58:50.760
The power and the cooling,
as has been alluded to earlier,

00:58:50.960 --> 00:58:57.030
were a great story for us because we are
in a small area and we said we wanted

00:58:57.030 --> 00:58:58.800
to get 64 nodes in the facilities.

00:58:58.800 --> 00:58:59.860
People laughed at us.

00:58:59.860 --> 00:59:02.670
So we said, well,
how if we put it in that room there?

00:59:02.710 --> 00:59:04.450
And they said, well, good luck.

00:59:04.530 --> 00:59:05.800
So we thought about it.

00:59:05.800 --> 00:59:08.870
We did get them to put in some
additional air conditioning.

00:59:08.870 --> 00:59:14.390
And then we looked at the stats
and the specs and the G5 Xserve,

00:59:14.390 --> 00:59:20.140
which had just come out at the time,
the specs showed that it used about

00:59:20.140 --> 00:59:24.070
half the power and the cooling,
I think, roughly at that time.

00:59:24.070 --> 00:59:26.860
And we were really happy with that.

00:59:26.920 --> 00:59:29.410
So we could actually buy it.

00:59:29.500 --> 00:59:31.140
It was actually a great, great feeling.

00:59:31.140 --> 00:59:34.120
And we knew that we'd be able to cool it.

00:59:34.230 --> 00:59:35.880
And it's also very quiet.

00:59:35.880 --> 00:59:39.550
I think in the last session,
Bud Tribble mentioned that we

00:59:39.550 --> 00:59:43.260
went into the room with them
all on for the first time.

00:59:43.260 --> 00:59:45.800
And there was this strange
high-pitched noise.

00:59:45.800 --> 00:59:48.020
And we thought, oh, great,
this is going to be kind of noisy.

00:59:48.270 --> 00:59:51.940
It turned out it was the two
Dell PowerEdges in the far.

00:59:51.940 --> 00:59:54.920
So we were happy with that.

00:59:58.220 --> 01:00:03.490
The great thing – we have a whole lot
of people that are – we're coming off an

01:00:03.610 --> 01:00:08.910
SGI system and a bunch of Linux systems,
so there were a lot of people that said,

01:00:08.980 --> 01:00:11.820
well, we're not going to be able to use
these open source packages and we're

01:00:11.820 --> 01:00:13.250
going to have to recompile them.

01:00:13.280 --> 01:00:16.560
Well, I've been – in the beginning
of the process of porting,

01:00:16.690 --> 01:00:18.640
it's been really pretty easy to do.

01:00:18.640 --> 01:00:23.250
The G5 is becoming a more and more
popular target in GCC make files

01:00:23.250 --> 01:00:27.190
for the packages that we depend on,
including AFNI.

01:00:28.100 --> 01:00:32.290
And so, in fact, they even have a binary
distribution for the G5 already.

01:00:32.530 --> 01:00:34.000
So that was great.

01:00:34.120 --> 01:00:38.820
And the administration of this thing
has been really straightforward so far.

01:00:38.820 --> 01:00:43.680
We're really still bringing it up,
but the server admin and the server

01:00:43.810 --> 01:00:49.070
monitor tools that Doug showed in one
of his slides have been really helpful.

01:00:49.100 --> 01:00:53.450
I can just bring up my G4 desktop
and look on the screen and see

01:00:53.450 --> 01:00:56.070
what's going on with the cluster.

01:00:56.080 --> 01:00:58.100
And I don't have to be
a full-time sysadmin.

01:00:58.100 --> 01:00:58.320
Amen.

01:01:01.070 --> 01:01:02.000
This is our system.

01:01:02.000 --> 01:01:05.420
What's a little different about us
is what I emphasized in this slide.

01:01:05.420 --> 01:01:08.000
We have all this data and we
have this file server already,

01:01:08.100 --> 01:01:10.580
so we sort of had to work with it.

01:01:10.880 --> 01:01:12.400
And so we have this second network.

01:01:12.400 --> 01:01:17.920
Each G5 node has two built-in
Ethernet ports on the back,

01:01:17.970 --> 01:01:21.240
and so we decided to use both of them,
which when we were setting it up,

01:01:21.240 --> 01:01:24.550
we realized we had to do 256
crimps of network cables,

01:01:24.700 --> 01:01:26.850
so we were wondering why we did that.

01:01:26.880 --> 01:01:29.550
But then we realized
once we set it all up,

01:01:29.550 --> 01:01:32.290
and we had a lot of help
from Apple doing that,

01:01:32.350 --> 01:01:38.880
and someone came and did most of the
crimps and redid the ones that I did.

01:01:39.690 --> 01:01:42.260
and David We got all this
stuff up and running.

01:01:42.260 --> 01:01:45.120
And what we have here,
we have a foundry switch that

01:01:45.160 --> 01:01:48.210
we got along with this thing,
this XServe cluster.

01:01:48.210 --> 01:01:50.500
I shouldn't call it a thing.

01:01:50.550 --> 01:01:54.510
It's got a few gigabit ports on it,
which goes to our existing

01:01:54.580 --> 01:01:58.320
Blue Arc file server,
which is an appliance, sort of,

01:01:58.320 --> 01:02:01.220
as most of its software is in firmware.

01:02:01.220 --> 01:02:04.170
And we connect out to the world with
that into the head node of our cluster.

01:02:04.300 --> 01:02:09.390
Our cluster in red uses what
Apple ships with the cluster,

01:02:09.390 --> 01:02:10.940
the Asante gigabit switches.

01:02:10.940 --> 01:02:13.390
And then down below is the
foundry connections going.

01:02:13.400 --> 01:02:16.600
And that's where all the
NFS traffic is between – well,

01:02:16.610 --> 01:02:19.830
it stays on its own network,
so it doesn't interfere with what's

01:02:19.830 --> 01:02:21.640
going on on the other network.

01:02:21.640 --> 01:02:24.750
And most of our applications
are single processor,

01:02:24.770 --> 01:02:28.770
embarrassingly parallel,
so we don't have a need for MPI yet or

01:02:28.810 --> 01:02:31.550
any of the high speed interconnects.

01:02:31.740 --> 01:02:34.320
So we were happy with this.

01:02:34.320 --> 01:02:35.460
And we just need to
move a lot of data on,

01:02:35.460 --> 01:02:37.370
churn it, churn and run it,
and then we can churn it,

01:02:37.380 --> 01:02:40.710
churn and work on it for a couple
hours and then write it back out.

01:02:40.710 --> 01:02:45.620
So we didn't have a need to do MPI yet.

01:02:45.620 --> 01:02:49.300
And there will be a lot
of opportunity for that.

01:02:49.370 --> 01:02:51.400
Just to

01:02:51.500 --> 01:04:33.900
[Transcript missing]

01:04:37.800 --> 01:04:38.300
Thank you, Ben.

01:04:38.300 --> 01:04:41.280
Okay.

01:04:41.280 --> 01:04:45.390
The second customer I'd
like to introduce is Dr.

01:04:45.390 --> 01:04:46.800
John Medeiros.

01:04:46.800 --> 01:04:52.270
Now, if you read any news last week,
you may have heard about a small little

01:04:52.270 --> 01:04:59.440
cluster going in down in Huntsville,
Alabama, a 1,566-node cluster being

01:04:59.440 --> 01:05:01.920
put together by Colsa.

01:05:01.920 --> 01:05:03.540
And I'd like to introduce Dr.

01:05:03.680 --> 01:05:05.780
John Medeiros,
the senior scientist from Colsa,

01:05:05.900 --> 01:05:07.270
who's going to tell you a
little bit more about it.

01:05:07.800 --> 01:05:08.930
Thank you, Doug.

01:05:15.110 --> 01:05:15.990
Thank you.

01:05:16.000 --> 01:05:17.700
As Doug mentioned,
I want to talk to you about our

01:05:17.700 --> 01:05:21.360
cluster and tell you a bit about
it and about why we got it.

01:05:21.510 --> 01:05:24.200
But to put things in
context a little bit,

01:05:24.200 --> 01:05:26.040
I'd like to tell you a
bit about who we are,

01:05:26.040 --> 01:05:30.400
what kind of computing we do,
and why we need so much of it,

01:05:30.540 --> 01:05:39.560
and what process we go through to
pick the cluster system that we did,

01:05:39.970 --> 01:05:43.090
and why we picked the Xserve cluster.

01:05:44.390 --> 01:05:46.600
In any case, who is Colsa?

01:05:46.600 --> 01:05:50.540
Well, we're a small engineering
services contractor,

01:05:50.540 --> 01:05:54.300
about 800 people, based in Huntsville,
Alabama, as Doug mentioned.

01:05:54.300 --> 01:05:57.920
And we have a few offices
throughout the U.S.

01:05:57.920 --> 01:06:02.470
I have to mention our company president,
Al Sullivan, and Dr.

01:06:02.530 --> 01:06:04.540
Tony DiRienzo,
our executive vice president,

01:06:04.650 --> 01:06:06.720
who actually have championed
this project for us,

01:06:07.310 --> 01:06:11.540
providing a lot of corporate support
for the vision that we have in terms

01:06:11.550 --> 01:06:14.280
of bringing that system to business.

01:06:14.330 --> 01:06:17.780
My particular project I'm
involved in is the hypersonic

01:06:17.780 --> 01:06:21.800
missile technology program,
and we have a dedicated corporate

01:06:21.800 --> 01:06:25.460
facility that we recently
renovated for the system called

01:06:25.460 --> 01:06:29.700
the Research and Operations Center,
so that makes us the HMT Rock,

01:06:29.870 --> 01:06:33.600
which sounds a bit like a radio station,
but really the only music

01:06:33.820 --> 01:06:35.410
there is on the iPods.

01:06:37.120 --> 01:06:38.520
Anyway, our program manager there is Mr.

01:06:38.520 --> 01:06:41.530
Mike Whitlock, and I'm the technical
lead on the project.

01:06:41.680 --> 01:06:44.130
Our primary customer is the U.S.

01:06:44.130 --> 01:06:47.060
Army's
Research Development Engineering Command

01:06:47.580 --> 01:06:50.280
out of Redstone Arsenal,
RDECOM.

01:06:50.380 --> 01:06:54.210
And the principal scientists there
on that project are the doctors

01:06:54.210 --> 01:06:56.260
Billy Walker and Kevin Kennedy.

01:06:57.120 --> 01:07:02.720
So what kind of computing requirements
are there for the project?

01:07:02.720 --> 01:07:06.790
Well,
supporting their hypersonic aerodynamic

01:07:07.130 --> 01:07:14.850
This session focuses on the
computational fluid dynamic analysis of

01:07:14.850 --> 01:07:19.810
the hypersonic endo-atmospheric regime,
that is,

01:07:19.810 --> 01:07:22.000
the very fast and near Earth atmosphere.

01:07:22.270 --> 01:07:27.400
The cartoons on the right show some
of the schematic data that comes out

01:07:27.580 --> 01:07:33.300
– visualized data that comes out of it
– where you display parameters in the

01:07:33.740 --> 01:07:36.580
space around an object flying very fast.

01:07:37.000 --> 01:07:43.150
And it's in fact a very complex
and difficult problem that we are

01:07:43.590 --> 01:07:48.900
simply attacking by brute force
using a code that's proprietary,

01:07:49.000 --> 01:07:52.770
double precision,
Fortran code to solve the Navier-Stokes

01:07:52.780 --> 01:07:54.980
partial differential equations.

01:07:54.980 --> 01:08:00.980
And it explores the full combustion
chemistry that goes on in that regime.

01:08:00.980 --> 01:08:06.010
And we explore problem sizes with the
space around an object divided into

01:08:06.120 --> 01:08:10.980
20 million or more individual points
at which the computations are done.

01:08:11.010 --> 01:08:15.430
That's a lot of points,
but the good news is that the blocks

01:08:15.490 --> 01:08:19.480
of those points can be assigned to a
given processor and computations are

01:08:19.540 --> 01:08:21.980
carried out within that processor.

01:08:22.180 --> 01:08:26.950
And then the results are compared,
stepped through, and iterations continue.

01:08:27.380 --> 01:08:30.980
And as a result,
the way that this whole process works,

01:08:31.190 --> 01:08:35.750
the problem is very CPU intensive and
very little time relatively is spent

01:08:36.020 --> 01:08:37.980
in inter-processor communications.

01:08:37.980 --> 01:08:41.170
It's in the category which you might
call almost embarrassingly parallel,

01:08:41.170 --> 01:08:43.980
which is good from our point of view.

01:08:44.020 --> 01:08:47.980
And in fact drove the design of the
kind of cluster that we went after.

01:08:47.980 --> 01:08:53.100
Now we've been doing some computing
in this project for a while,

01:08:53.160 --> 01:08:53.980
and we've done it.

01:08:53.980 --> 01:08:57.950
Well, systems we have include a
traditional supercomputer system.

01:08:58.030 --> 01:09:04.010
We have an IBM SP Power 3 system
and 284 CPUs that when we got it,

01:09:04.010 --> 01:09:09.430
as pictured there, back in June of 2000,
it came in as number

01:09:09.430 --> 01:09:10.980
47 on the top 500 list.

01:09:10.980 --> 01:09:13.980
And four years later today,
it's completely off the list.

01:09:14.170 --> 01:09:18.320
It gives you an idea of
how things are progressing.

01:09:18.680 --> 01:09:21.890
So our goal was, I mean,
for this project,

01:09:22.120 --> 01:09:25.450
like it can't be too rich or too thin
or have too much computational power.

01:09:25.450 --> 01:09:28.700
We need a lot more than this.

01:09:28.730 --> 01:09:31.600
And mainframes,
as expensive as they were,

01:09:31.750 --> 01:09:36.360
while they work very well,
were too expensive to get to the kind

01:09:36.450 --> 01:09:37.900
of computational levels that we wanted.

01:09:37.900 --> 01:09:40.130
So we began exploring clusters.

01:09:40.130 --> 01:09:43.600
And in the interim,
since we got that mainframe system,

01:09:43.690 --> 01:09:48.110
we've acquired and put together and
played with a number of clusters

01:09:48.190 --> 01:09:52.660
and explored a whole range of
architectures from major vendors,

01:09:52.660 --> 01:09:55.130
including AMD, Intel, and Apple.

01:09:55.140 --> 01:10:02.300
Our first system back in June of 2000
was a 34 processor AMD Athlon system.

01:10:02.300 --> 01:10:06.510
And about the same timeframe,
a little later,

01:10:07.240 --> 01:10:07.880
we acquired a 34 processor
AMD Athlon system.

01:10:07.880 --> 01:10:07.880
And about the same timeframe,
a little later,

01:10:07.880 --> 01:10:07.880
we acquired a 34 processor
AMD Athlon system.

01:10:07.880 --> 01:10:11.180
And we had a G4 system
about the same size,

01:10:11.570 --> 01:10:15.140
which at that time, I believe,
was probably one of the

01:10:15.410 --> 01:10:17.870
biggest Apple clusters around.

01:10:17.870 --> 01:10:22.780
That system performed fairly well,
but it was only 800 MHz,

01:10:22.940 --> 01:10:26.690
and we wanted to scale
up very substantially.

01:10:27.460 --> 01:10:33.670
So we want to look at other architectures
including rack mounted obviously.

01:10:34.390 --> 01:10:36.990
Only a few of the
systems that we have now,

01:10:36.990 --> 01:10:39.580
we've worked with, are shown here.

01:10:39.690 --> 01:10:42.300
The first, up or two,
for historical reasons,

01:10:42.620 --> 01:10:45.490
were the early days of
looking at clustering.

01:10:45.600 --> 01:10:48.570
So we did tower systems
for PCs and Apples,

01:10:48.730 --> 01:10:54.650
and that little Apple cluster system
we affectionately called Apple Orchard.

01:10:54.900 --> 01:10:59.160
We've looked at 64-bit
systems very extensively,

01:10:59.160 --> 01:11:01.570
including the Optron system.

01:11:02.430 --> 01:11:06.040
And a lot of our computations
now are done on an

01:11:06.370 --> 01:11:08.640
Intel architecture 32-bit system.

01:11:08.640 --> 01:11:14.750
You can see there our largest cluster
now is a 522 processor system.

01:11:14.790 --> 01:11:19.800
But we, in fact, needed larger, more.

01:11:19.800 --> 01:11:22.040
So we explored additional possibilities.

01:11:22.040 --> 01:11:27.210
Now the whole thing is set up in this
un-presupposing building in Huntsville

01:11:27.210 --> 01:11:29.680
that we acquired back late last year.

01:11:29.790 --> 01:11:35.070
And the building was gutted internally,
virtually internally.

01:11:35.070 --> 01:11:41.840
And this shows the computer room
being put together back last fall.

01:11:41.980 --> 01:11:45.080
And we renovated it
literally from top to bottom,

01:11:45.090 --> 01:11:46.990
the ceiling and the computer room floor.

01:11:47.000 --> 01:11:51.520
We've got about 3,000 square
feet of computer room floor.

01:11:52.280 --> 01:11:59.240
And this shows our configuration with
that 522 processor Intel architecture

01:11:59.240 --> 01:12:01.540
system on the left as you see it.

01:12:02.100 --> 01:12:05.160
Our SP mainframe system on the right.

01:12:05.740 --> 01:12:08.860
And center is what I'm going
to be talking about here,

01:12:09.070 --> 01:12:14.990
the cluster system that we
are acquiring from Apple.

01:12:15.940 --> 01:12:20.680
Okay, how do you pick such a system?

01:12:20.680 --> 01:12:21.440
You have to benchmark it.

01:12:21.440 --> 01:12:24.220
And from our point of view,
the only benchmark that

01:12:24.220 --> 01:12:26.040
counted was our application.

01:12:26.210 --> 01:12:30.460
So we ran our code using sort
of a simplified geometry,

01:12:30.560 --> 01:12:38.200
but the full complexity of the problem
in terms of a reasonable problem size

01:12:38.200 --> 01:12:42.610
and full combustion chemistry with
a whole range of chemical species.

01:12:43.670 --> 01:12:47.450
What we did find, among other things,
in testing across the

01:12:47.500 --> 01:12:50.990
whole range of processors,
that the inter-processor communication,

01:12:51.000 --> 01:12:54.590
as I mentioned, is a small fraction of
the total compute time.

01:12:54.600 --> 01:12:57.800
That is, in a given iteration,
we found that that might

01:12:58.190 --> 01:13:00.790
take typically a few seconds.

01:13:00.790 --> 01:13:05.560
The amount of time that was done in
communicating between processors,

01:13:05.560 --> 01:13:09.390
between those iterations,
was in the range of milliseconds.

01:13:09.470 --> 01:13:14.670
So there was very little penalty
in worrying about the interconnect,

01:13:14.670 --> 01:13:19.630
which is why, in fact,
we've chosen gigabit

01:13:19.630 --> 01:13:23.590
Ethernet for the system.

01:13:23.590 --> 01:13:23.590
Let me go back a minute.

01:13:26.320 --> 01:13:29.340
The last point about it,
part of the reason for doing that,

01:13:29.340 --> 01:13:31.580
of course,
is that these other interconnects,

01:13:31.580 --> 01:13:34.520
as you've already heard about
previously in the session,

01:13:34.630 --> 01:13:37.960
you can get better performance for
a broader range of applications,

01:13:38.110 --> 01:13:48.400
but the cost difference is not trivial
compared to the Gigabit Ethernet switch.

01:13:48.400 --> 01:13:48.400
Okay, this shows

01:13:49.380 --> 01:13:50.620
Yes.

01:13:50.760 --> 01:13:55.460
Some of the data that we used and the
kinds of things that drove our decision.

01:13:55.460 --> 01:14:00.560
What you see in there,
and this is a log log plot of the time

01:14:00.590 --> 01:14:04.890
to do a given step of the computation
as a function of the number of

01:14:04.890 --> 01:14:06.580
processors you throw at the problem.

01:14:06.580 --> 01:14:10.030
For all the different processors,
five are shown there on this chart.

01:14:10.030 --> 01:14:13.860
For all the different processors,
you see they're actually scaled

01:14:13.860 --> 01:14:16.270
very well for our kind of problem.

01:14:16.270 --> 01:14:18.940
That is as you double
the number of processors,

01:14:18.940 --> 01:14:22.640
the processing time cuts in half.

01:14:22.790 --> 01:14:25.290
The grouping there,
if you can make it out,

01:14:25.290 --> 01:14:27.220
this is a log-log plot,
but it does break up

01:14:27.220 --> 01:14:28.530
naturally into two groups.

01:14:28.540 --> 01:14:35.700
The upper two for the Athlon and
Xeon systems are 32-bit systems.

01:14:35.700 --> 01:14:38.810
And the bottom three, Optron,
Titanium II,

01:14:38.860 --> 01:14:41.700
and G5s are the 64-bit systems.

01:14:41.700 --> 01:14:44.680
The lower is better on this chart.

01:14:44.800 --> 01:14:46.020
The less time it takes
to do an iteration,

01:14:46.020 --> 01:14:48.140
the more of them you can do
in a given amount of time,

01:14:48.140 --> 01:14:49.700
the more processing you can do.

01:14:49.880 --> 01:14:54.390
And on that basis,
you can see that the G5, in fact,

01:14:54.400 --> 01:14:56.460
performed the best.

01:14:56.600 --> 01:14:59.770
Now, this particular comparison is
maybe not a little bit fair

01:14:59.770 --> 01:15:02.940
because these were different
processors with different speeds.

01:15:03.160 --> 01:15:06.020
So the next chart is the same data.

01:15:06.460 --> 01:15:09.910
This time all the results were
normalized as if each of the processors

01:15:09.910 --> 01:15:12.380
had the same 2 GHz clock speed.

01:15:12.580 --> 01:15:14.720
They didn't,
but you can normalize the data that

01:15:14.720 --> 01:15:17.900
way just for demonstration purposes.

01:15:17.940 --> 01:15:20.380
What you see here is the results
are essentially the same,

01:15:20.380 --> 01:15:23.220
they're not changed very much,
and the only difference here now

01:15:23.350 --> 01:15:26.700
is that the Itanium 2 looks a
little bit better than the G5.

01:15:26.700 --> 01:15:29.400
It's a little bit faster on that chart.

01:15:29.400 --> 01:15:32.800
But you've got to keep in
mind that Itanium 2 is not

01:15:32.800 --> 01:15:38.770
available at any cost for 2 GHz,
and at its fastest implementation,

01:15:39.400 --> 01:15:42.910
about 1.5 GHz,
a system built with Itanium

01:15:42.940 --> 01:15:47.440
2s comparable to the G5 would
cost about 5 times as much.

01:15:47.440 --> 01:15:50.680
So, G5s it

01:15:57.840 --> 01:16:00.390
Okay, well, process is one thing,
but there's a lot of issues in

01:16:00.390 --> 01:16:02.300
putting together a big cluster.

01:16:02.300 --> 01:16:04.110
Now, we've put together,
as you kind of saw,

01:16:04.150 --> 01:16:08.250
clusters that are pretty reasonable size,
but even for us, this is a big cluster.

01:16:08.250 --> 01:16:11.520
And there are a lot of issues
that come up in terms of scale,

01:16:11.680 --> 01:16:14.080
but there's a whole laundry list of
things that I'm going to show here.

01:16:14.080 --> 01:16:15.910
I'm not going to go
through them in detail,

01:16:16.170 --> 01:16:18.850
except to highlight a couple things
that you've heard before today,

01:16:18.930 --> 01:16:20.730
and I want to emphasize them yet again.

01:16:21.060 --> 01:16:23.030
Power and cooling on the bottom.

01:16:24.410 --> 01:16:27.620
Especially at this kind of scale,

01:16:29.940 --> 01:16:32.100
This is very much non-trivial.

01:16:32.230 --> 01:16:40.420
For example, for the current system that
Apple is delivering to us right now,

01:16:40.420 --> 01:16:40.420
as we're getting it in,

01:16:41.100 --> 01:16:45.390
We've had to upgrade our
power into the building.

01:16:45.400 --> 01:16:46.880
I'll tell you about that in a minute.

01:16:47.000 --> 01:16:51.160
But just to give you an idea,
something we haven't shared with

01:16:51.160 --> 01:16:55.180
our corporate executives yet,
but just to run the system,

01:16:55.180 --> 01:17:02.430
our utility bill for the year is going to
run about $250,000 just to keep it going.

01:17:04.610 --> 01:17:07.240
Cooling is also, of course,
a very important issue.

01:17:07.350 --> 01:17:09.260
And just like power,
you can calculate how

01:17:09.260 --> 01:17:12.400
much cooling you need,
and you can get that

01:17:12.410 --> 01:17:15.640
cooling into your facility.

01:17:15.720 --> 01:17:17.870
But in addition,
we have the added complication

01:17:17.870 --> 01:17:20.370
that you've got to get the
cooling to the right place,

01:17:20.530 --> 01:17:23.470
and you've got to look at how you
distribute that and remove the

01:17:23.510 --> 01:17:25.460
heat and bring in the cooler air.

01:17:25.560 --> 01:17:27.630
So that's something
that we're playing with,

01:17:27.630 --> 01:17:29.980
and we expect to actually
have to fuss with a fair bit

01:17:29.980 --> 01:17:31.490
over the next little while.

01:17:33.200 --> 01:17:34.380
So how do we do this process?

01:17:34.490 --> 01:17:38.840
We decided it was going to be G5s,
but you can't say that when you're

01:17:39.010 --> 01:17:40.150
buying on a government contract.

01:17:40.200 --> 01:17:41.990
You've got to be generic, and we did.

01:17:41.990 --> 01:17:46.180
We put a general request for quotes
out to the community at large,

01:17:46.180 --> 01:17:50.270
and one of the quotes we got back,
in fact, was the G5 system,

01:17:50.290 --> 01:17:51.720
coincidentally.

01:17:52.440 --> 01:17:56.480
But the requirements that we had
included a theoretical performance for

01:17:56.480 --> 01:17:58.920
the system of at least 25 teraflops.

01:17:58.920 --> 01:18:02.590
We wanted a processor
count in excess of 3,000.

01:18:02.590 --> 01:18:07.100
We wanted all the fit into a
1,000-square-foot footprint,

01:18:07.100 --> 01:18:08.930
excuse me.

01:18:09.570 --> 01:18:12.920
Wanted minimal power and
cooling requirements,

01:18:12.920 --> 01:18:16.400
and we wanted it all delivered
by 12 July this year.

01:18:16.400 --> 01:18:20.400
And you don't want to pay
a lot for this cluster.

01:18:21.950 --> 01:18:25.730
We didn't share with the vendors what
the cost figure we had in mind was.

01:18:25.730 --> 01:18:27.900
We had to go for the lowest bid.

01:18:27.900 --> 01:18:31.470
But we wanted the whole thing,
including the switch and all the

01:18:31.470 --> 01:18:36.670
ancillary equipment we need with it,
to come in at under $6 million and

01:18:36.670 --> 01:18:40.200
we're going to make that target.

01:18:45.220 --> 01:18:48.630
So the system award exclusive
of the network component was

01:18:48.630 --> 01:18:50.680
done on 17th of May this year.

01:18:50.680 --> 01:18:55.290
So that's really a three week turnaround,
which is in this business is a very

01:18:55.290 --> 01:19:00.360
short timeframe for getting that done,
but we wanted it.

01:19:00.550 --> 01:19:07.900
We're calling it Mach5.

01:19:07.900 --> 01:19:15.600
It stands for Multiple Advanced Computers
for Hypersonics Using G5s.

01:19:15.900 --> 01:19:22.890
We've got 1562 dual Xserve G5
compute nodes and 4 head nodes.

01:19:22.890 --> 01:19:25.560
And these nodes are being
delivered as we speak.

01:19:25.620 --> 01:19:27.030
In fact,
there was a lot of complaining back home

01:19:27.040 --> 01:19:31.960
that I get to play and come here and
attend WWDC while they're working on

01:19:31.960 --> 01:19:34.410
putting the system together back there,
and I've got to fly back there

01:19:34.410 --> 01:19:36.560
tomorrow because I couldn't get
much more time off than that.

01:19:36.840 --> 01:19:41.470
The systems we've taken delivery
about 350 nodes as of yesterday,

01:19:41.470 --> 01:19:45.260
and it's coming in at a tractor
trailer load worth a day.

01:19:45.390 --> 01:19:49.770
Consists of 25 pallets
of a dozen Xserves,

01:19:49.770 --> 01:19:54.410
and everybody's got to pull them out,
assemble them, rack them,

01:19:54.410 --> 01:19:55.770
get them hooked up.

01:19:56.000 --> 01:19:59.000
At that kind of scale,
it gets kind of interesting.

01:19:59.000 --> 01:20:02.940
So the physical configuration is
going to be set up in 40 racks

01:20:02.940 --> 01:20:05.940
with 39 XServe nodes in each.

01:20:06.000 --> 01:20:10.570
Each rack, these are 42 U racks,
and a 48 port gigabit

01:20:10.570 --> 01:20:12.680
Ethernet switch in that rack.

01:20:12.680 --> 01:20:16.670
The switch we're getting from
Foundry Networks is actually a very

01:20:16.670 --> 01:20:20.430
high performance gigabit switch,
and will work great for our purposes,

01:20:20.440 --> 01:20:20.940
we believe.

01:20:21.000 --> 01:20:28.850
And one rack includes 4 head nodes,
a couple more cluster nodes,

01:20:28.850 --> 01:20:35.400
and a large 320 port gigi main
switch that are trunked from the,

01:20:35.400 --> 01:20:35.400
through the main switch.

01:20:35.400 --> 01:20:40.160
to which the individual 48 port switches
at each of the nodes are trunked,

01:20:40.280 --> 01:20:44.560
and access the nexus
for the cluster network.

01:20:45.240 --> 01:20:49.310
The whole thing occupies
less than 600 square feet,

01:20:49.340 --> 01:20:53.800
so it beats the thousand square
foot limitation that we imposed.

01:20:54.780 --> 01:21:03.400
And we're expecting to draw about 400
kilowatts of peak power for the system.

01:21:03.400 --> 01:21:05.860
And we're having – we didn't have
that much – enough power being brought

01:21:05.860 --> 01:21:07.580
into the building at that point.

01:21:07.690 --> 01:21:12.490
And we had Huntsville Utilities
bring us in a new transformer

01:21:12.540 --> 01:21:15.090
rated at over 2 megawatts.

01:21:15.090 --> 01:21:21.260
We're planning to build
actually a bigger system,

01:21:21.260 --> 01:21:21.830
but that's another story.

01:21:22.760 --> 01:21:29.740
For cooling, we know we require about
110 tons of cooling.

01:21:29.750 --> 01:21:31.200
For those of you who might
not be familiar with that,

01:21:31.200 --> 01:21:35.900
the ton unit that is used to rate these
big chillers is an archaic unit in the

01:21:35.900 --> 01:21:42.690
heating industry that relates to being
able to remove the latent heat of fusion

01:21:43.150 --> 01:21:45.680
of water in one ton of water in one day.

01:21:46.060 --> 01:21:48.700
That is, make a ton of ice in a day.

01:21:48.700 --> 01:21:52.500
So, we've got 150 tons of that installed,
and if we ever get out of

01:21:52.620 --> 01:21:56.360
the computing business,
I guess we could make a lot of ice.

01:21:58.100 --> 01:22:59.200
[Transcript missing]

01:23:02.470 --> 01:23:04.200
As I mentioned,
the system's being delivered.

01:23:04.450 --> 01:23:07.590
And these are pictures we took
last week before we came out here.

01:23:07.700 --> 01:23:13.800
And those were the first 40 units
packed in the high bay being delivered.

01:23:13.920 --> 01:23:17.400
Some work's still going on in the
computer room in terms of getting the

01:23:17.400 --> 01:23:19.300
rest of the infrastructure set up.

01:23:19.530 --> 01:23:22.490
And you see some of the guys
working on putting in some

01:23:22.490 --> 01:23:23.690
of the hardware in the racks.

01:23:23.740 --> 01:23:28.100
Now we've got 40 racks,
and a man all these Xserves in the racks.

01:23:28.180 --> 01:23:30.860
You have to put these little
clips to which you can screw

01:23:30.860 --> 01:23:32.500
into to get the rack in there.

01:23:32.560 --> 01:23:34.000
You gotta put them in
the front and the back.

01:23:34.100 --> 01:23:38.100
And for this many, we calculated,
the guys had to put in

01:23:38.230 --> 01:23:40.980
over 14,000 such clips.

01:23:41.290 --> 01:23:43.070
They did it in an afternoon.

01:23:43.170 --> 01:23:45.910
We had a bunch of folks working on it.

01:23:47.310 --> 01:23:50.650
Okay, so to kind of wind down
the story a little bit,

01:23:50.850 --> 01:23:53.500
to give you some perspective on the
progression of computer technology,

01:23:53.500 --> 01:23:57.670
I'm comparing here that mainframe
system we acquired back in

01:23:57.670 --> 01:24:02.700
2000 to Mach 5 coming in now.

01:24:02.750 --> 01:24:08.890
And cost-wise,
we're paying a little bit more – $6

01:24:08.910 --> 01:24:11.000
million compared to $4 million back then,
so it's about 5% more.

01:24:11.670 --> 01:24:14.560
Floor space is about twice as much.

01:24:14.840 --> 01:24:19.360
However, for that, we get more than 10
times more processors,

01:24:19.360 --> 01:24:22.280
and we get more than 60
times more performance.

01:24:25.150 --> 01:24:29.350
So in summary,
we chose the Apple's XServe G5

01:24:29.350 --> 01:24:35.090
architecture for a major production
cluster for computational fluid

01:24:35.090 --> 01:24:38.850
dynamic analysis and hypersonic flight.

01:24:39.330 --> 01:24:43.520
The proposal we got from
Apple on the Xserve G5 delivered

01:24:43.820 --> 01:24:46.250
the best bang for the buck.

01:24:46.310 --> 01:24:49.100
In essence, best price performance.

01:24:49.360 --> 01:24:53.470
Now, as I've kind of mentioned,
Mach 5 has been designed for a

01:24:53.470 --> 01:24:57.300
compute-intensive problem with
relatively little demand on network.

01:24:57.300 --> 01:25:00.030
Now that means,
in terms of standard measures

01:25:00.130 --> 01:25:05.090
that put systems on the top 500,
it will not do as well, relatively,

01:25:05.160 --> 01:25:08.300
as a system sort of purpose design
with a higher speed network.

01:25:08.480 --> 01:25:11.060
That being said,
we fully expect to achieve something

01:25:11.060 --> 01:25:15.230
over 12 teraflops of real performance,
and we believe we might be able to get

01:25:15.280 --> 01:25:18.200
up to 15 teraflops of real performance.

01:25:18.300 --> 01:25:21.730
If we can do that,
we'll still be easily in the top five

01:25:22.060 --> 01:25:24.260
when the November list comes out.

01:25:24.430 --> 01:25:26.460
Hopefully we can get there.

01:25:31.490 --> 01:25:34.060
So as I mentioned, just to finish up,
the system is being installed and

01:25:34.170 --> 01:25:38.190
we hope to get it into production,
actually get it working by the fall.

01:25:38.190 --> 01:25:41.780
And from the solicitation of the
system to actually production work,

01:25:41.860 --> 01:25:45.860
we're looking for a six-month timeframe,
which is pretty phenomenal for

01:25:45.900 --> 01:25:48.050
a system of this kind of scale.

01:25:48.050 --> 01:25:49.890
And we hope it works out.

01:25:49.890 --> 01:25:49.890
Thanks.

01:25:52.700 --> 01:25:53.700
Thank you.

01:25:53.700 --> 01:25:58.910
Okay, well, we're running a little late,
so just to summarize and

01:25:58.910 --> 01:26:01.330
finish up the session.

01:26:03.890 --> 01:26:06.980
What I really wanted to say is, you know,
Apple is investing in the high

01:26:06.980 --> 01:26:08.480
performance computing market.

01:26:08.480 --> 01:26:11.720
We're doing it through our products,
our technologies,

01:26:11.720 --> 01:26:13.800
and the solutions that we're providing.

01:26:13.860 --> 01:26:16.510
Working very closely to make sure
the right solutions are available,

01:26:16.510 --> 01:26:18.840
both from third parties and
the open source community.

01:26:18.840 --> 01:26:21.670
And the adoption has
really been phenomenal,

01:26:21.670 --> 01:26:23.380
and momentum continues.

01:26:23.380 --> 01:26:26.680
So, in summary, you know,
Apple has products from

01:26:26.680 --> 01:26:30.630
the workgroup cluster,
turnkey, easy to use, and bioinformatics,

01:26:30.690 --> 01:26:34.120
all the way up to, you know,
the top supercomputers.

01:26:34.140 --> 01:26:36.820
So with that, thank you very much.

01:26:36.940 --> 01:26:40.000
Unfortunately,
we're running out of time for formal Q&A,

01:26:40.000 --> 01:26:43.120
but if there are any questions,
it will be available up front

01:26:43.130 --> 01:26:45.130
for any questions you might have.