WEBVTT

00:00:12.940 --> 00:00:20.240
Hello and welcome to
session 201 on Core Image.

00:00:20.290 --> 00:00:24.120
That's me, Ralph Brunner, and

00:00:27.850 --> 00:00:30.440
So here's the agenda of this session.

00:00:30.500 --> 00:00:32.340
First,
I'm going to explain to you what the

00:00:32.460 --> 00:00:37.320
Core Image framework provides and well,
essentially what's there.

00:00:37.340 --> 00:00:42.840
Then how to use the API and I will
not go into too much detail there.

00:00:42.840 --> 00:00:44.840
There's a nice set of
documentation on this.

00:00:44.870 --> 00:00:47.810
But essentially what the spirit
of the API is and what are the

00:00:47.860 --> 00:00:52.240
key methods do you have to know.

00:00:52.280 --> 00:00:55.390
Then Mark Zimmer will come up
and explain to you how to write

00:00:55.460 --> 00:00:57.700
your own image processing kernel.

00:00:57.720 --> 00:01:01.430
After that,
Frank Doepke will show you how to

00:01:01.580 --> 00:01:05.260
take that image processing kernel
and create an image unit out of it.

00:01:05.280 --> 00:01:09.040
And by then you'll hopefully
fully know what an image unit is.

00:01:09.100 --> 00:01:14.020
And then I will bore you a little
more at the end with giving you

00:01:14.020 --> 00:01:17.800
essentially ideas how this could
be used in various applications.

00:01:17.890 --> 00:01:20.130
So if you're not interested
in any of these things and

00:01:20.130 --> 00:01:22.760
rather hear about databases,
now would be a good time to leave.

00:01:26.550 --> 00:01:28.390
Okay, so what's Core Image?

00:01:28.600 --> 00:01:33.860
Core Image is an image processing
framework now in Mac OS X Tiger,

00:01:33.860 --> 00:01:37.500
and the goal is really to put
image processing onto the GPU.

00:01:37.500 --> 00:01:40.700
It has a full floating point pipeline,
so there is no clamping to

00:01:40.830 --> 00:01:45.490
0 to 1 through the pipeline,
and you get really nice deep pixels.

00:01:45.500 --> 00:01:49.990
There's a base set of about 60 filters,
which contains various things

00:01:49.990 --> 00:01:54.150
from fairly pedestrian stuff like
contrast brightness to some more

00:01:54.150 --> 00:01:58.500
funky stuff like bump distortion,
and things like that.

00:01:58.820 --> 00:02:01.210
There is the concept of Image Units,
which is essentially a

00:02:01.210 --> 00:02:04.640
plug-in architecture,
how you can make a filter and

00:02:04.650 --> 00:02:07.490
put it on the system and have
it show up in other applications

00:02:07.610 --> 00:02:10.400
that are hosts for Image Units.

00:02:11.370 --> 00:02:17.500
And there is, while Core Image is really
focused on working on the GPU,

00:02:17.620 --> 00:02:19.260
there is a CPU fallback available.

00:02:19.360 --> 00:02:24.290
So if your GPU is not adequately
equipped with fragment programs and

00:02:24.330 --> 00:02:28.960
the nice things that we utilize,
then it will run on the CPU.

00:02:28.960 --> 00:02:35.960
And actually the CPU, it's a fallback,
but it is working pretty well,

00:02:35.960 --> 00:02:37.470
actually fairly good,
very well optimized code.

00:02:39.230 --> 00:02:42.040
So, why the GPU?

00:02:42.070 --> 00:02:45.430
Well, there are essentially two reasons.

00:02:45.700 --> 00:02:49.090
If you can put something on the GPU,
well, the CPU is free.

00:02:49.170 --> 00:02:52.600
And CPU cycles are kind of a scarce
resource in whatever you're doing.

00:02:52.600 --> 00:02:55.150
So that helps.

00:02:55.310 --> 00:03:00.370
And the second reason is GPUs
actually are outpacing CPUs in a

00:03:00.370 --> 00:03:04.490
number of interesting benchmarks
like memory bandwidth and the

00:03:04.590 --> 00:03:07.040
floating point processing power.

00:03:07.170 --> 00:03:10.340
So this graph down here
tries to illustrate that.

00:03:10.460 --> 00:03:13.200
We essentially took a bunch
of image processing filters,

00:03:13.200 --> 00:03:17.350
about five, stack them behind each other,
and then run it on different hardware.

00:03:17.480 --> 00:03:20.810
So the first four are different
hardware configurations from

00:03:20.810 --> 00:03:25.760
the iMac G4 to the dual 2.5 G5.

00:03:25.860 --> 00:03:29.230
And that's interesting here because,
by the way,

00:03:29.250 --> 00:03:32.920
the number is in megapixels per
second for that particular operation.

00:03:33.090 --> 00:03:37.260
So the interesting thing here
is the dual G5 is actually about

00:03:37.390 --> 00:03:40.270
four times faster than the iMac,
which makes sense.

00:03:40.320 --> 00:03:43.080
It's twice the gigahertz and it's
twice the number of processors.

00:03:43.180 --> 00:03:46.070
What about right?

00:03:46.420 --> 00:03:50.910
So, if you look at the graphics cards,
there is,

00:03:51.040 --> 00:03:54.370
if you take a low-end graphics card,
the GeForce FX 5200,

00:03:54.370 --> 00:03:57.810
and compare that to the
high-end graphics cards,

00:03:57.990 --> 00:04:02.300
then you actually get a factor of,
you know, almost 9.

00:04:02.300 --> 00:04:06.220
And that's kind of a challenge
because a factor 4 is something

00:04:06.320 --> 00:04:08.300
that your software can handle.

00:04:08.300 --> 00:04:10.760
It means, well,
if you had 20 frames per second before,

00:04:10.760 --> 00:04:12.300
now we have 5 frames per second.

00:04:12.300 --> 00:04:16.300
So, it's not exactly good, but yeah,
you can live with that.

00:04:16.300 --> 00:04:21.110
If you have a factor 10, well,
maybe you want to consider

00:04:21.430 --> 00:04:24.900
Adding a special code into
your application that if it

00:04:24.950 --> 00:04:29.460
detects if the gap is that big,
well, maybe we should do something else.

00:04:29.460 --> 00:04:31.920
And for example,
if you have something like a

00:04:31.920 --> 00:04:36.690
keynote presentation application,
if you can't do the transition smoothly,

00:04:36.760 --> 00:04:40.660
well, maybe just fall back to a
dissolve or something like that.

00:04:40.740 --> 00:04:41.390
Okay.

00:04:41.620 --> 00:04:46.230
Well, on Mac OS X,
the way to access the GPU is OpenGL.

00:04:46.390 --> 00:04:49.520
So let me tell a bit more about that.

00:04:49.800 --> 00:04:53.500
There are essentially two different ways,
well, I see things,

00:04:53.500 --> 00:04:57.200
I see two different ways
how you can use OpenGL.

00:04:57.370 --> 00:05:00.700
Well,
one is use it as a low-level 3D renderer.

00:05:00.850 --> 00:05:04.380
And the characteristics there,
you have high polygon count,

00:05:04.560 --> 00:05:08.180
moderate amount of textures,
and you're doing a lot of work in

00:05:08.180 --> 00:05:12.200
optimizing your scene and figuring
out which pieces you need to draw.

00:05:12.200 --> 00:05:16.200
And the model new matrix is set up that
is a camera looking into a 3D scene.

00:05:16.200 --> 00:05:21.700
And pretty much all 3D games you play,
ever played, are in that category.

00:05:21.980 --> 00:05:27.680
Second category is using OpenGL more
as a pixel calculation engine.

00:05:27.740 --> 00:05:31.650
In that case, you usually have a lot of
large textures and the geometry

00:05:31.650 --> 00:05:33.200
is pretty much negligible.

00:05:33.410 --> 00:05:36.180
And the model new matrix is
really set up to allow you

00:05:36.210 --> 00:05:38.200
to address individual pixels.

00:05:39.630 --> 00:05:42.040
So, I call that Quartz versus Quake.

00:05:42.590 --> 00:05:45.000
And...

00:05:45.830 --> 00:05:49.080
Since Jaguar,
the Windows Server compositor

00:05:49.210 --> 00:05:51.480
was layered on top of OpenGL.

00:05:51.480 --> 00:05:54.360
We call that Quartz Extreme.

00:05:54.360 --> 00:05:59.100
It took essentially what Peter put it,
I think, two years ago as it removes

00:05:59.100 --> 00:06:01.560
the transparency tax.

00:06:01.680 --> 00:06:04.460
The cost of doing all that
compositing is now on the GPU and

00:06:04.520 --> 00:06:08.560
from the CPU's perspective,
it's free.

00:06:08.560 --> 00:06:12.740
This year,
Quartz Extreme also encompasses

00:06:12.740 --> 00:06:15.660
the Quartz 2D Drawing API.

00:06:15.660 --> 00:06:18.700
And Core Image uses
OpenGL in that way as well.

00:06:21.030 --> 00:06:22.760
So why are we doing this?

00:06:22.760 --> 00:06:28.000
Well, for an application developer,
using OpenGL efficiently is hard.

00:06:28.000 --> 00:06:32.630
You have a lot of stuff you need
to learn about P buffers and the

00:06:32.770 --> 00:06:36.650
right way to switch context so
that everything is still streamed

00:06:36.740 --> 00:06:39.000
to the graphics card correctly.

00:06:39.000 --> 00:06:42.000
And the goal of Core Image is
essentially to abstract you,

00:06:42.000 --> 00:06:43.920
the developer, from that burden.

00:06:44.000 --> 00:06:48.690
You should be able to concentrate
on drawing big quads with

00:06:48.690 --> 00:06:51.000
video on it and that's it.

00:06:51.000 --> 00:06:57.990
And not know about the 200 lines of code
underneath to manage all the buffers.

00:06:59.100 --> 00:07:01.240
So after talking about
that much about hardware,

00:07:01.240 --> 00:07:04.430
well here are actually
the hardware requirements.

00:07:04.630 --> 00:07:07.620
So,
the key thing that Core Image needs is an

00:07:07.700 --> 00:07:10.450
ARP fragment programmable graphics card.

00:07:10.500 --> 00:07:16.680
And it works pretty much on any
ARP fragment programmable graphics card,

00:07:16.980 --> 00:07:19.920
but more memory tends to be better,
especially with all these

00:07:19.920 --> 00:07:24.590
other system services going
to the graphics card as well.

00:07:24.910 --> 00:07:29.970
Well, more is better,
but it does work well on my laptop with

00:07:29.970 --> 00:07:34.800
the ATI card which has 64 MB of memory.

00:07:34.800 --> 00:07:38.800
So,
if the GPU isn't fragment programmable,

00:07:38.800 --> 00:07:42.800
then we have a fallback
which uses the CPU.

00:07:42.820 --> 00:07:45.800
And here the G4 and the G5
are strongly recommended.

00:07:45.800 --> 00:07:49.790
It does work on a G3, however,
because all the computation

00:07:49.800 --> 00:07:53.050
is done in floating point,
having that regular Velocity

00:07:53.050 --> 00:07:55.800
Engine unit there is a great game.

00:07:55.800 --> 00:07:58.630
And it supports multiple processors,
so if you have one of

00:07:58.630 --> 00:08:03.480
these nice dual desktops,
then you will utilize your two

00:08:03.480 --> 00:08:07.030
processors pretty much optimally.

00:08:08.850 --> 00:08:13.000
So with that,
I would like to go to Demo Machine 2

00:08:13.040 --> 00:08:19.830
and give you a bit of an impression
what kind of filters are available.

00:08:20.700 --> 00:08:26.800
[Transcript missing]

00:08:29.300 --> 00:08:35.040
Here I have a sepia tone filter and
more of the more pedestrian ones.

00:08:35.040 --> 00:08:36.520
There is saturation.

00:08:36.520 --> 00:08:37.550
That looks ugly.

00:08:37.790 --> 00:08:41.350
Brightness and contrast.

00:08:48.540 --> 00:08:52.400
You can make a monochrome image.

00:08:52.400 --> 00:08:55.950
This is not terribly exciting,
but it turns out that these are actually

00:08:55.960 --> 00:08:57.600
really useful in your workflows.

00:08:57.600 --> 00:08:59.700
They have to be there.

00:08:59.780 --> 00:09:03.440
So let me try to get some
more interesting stuff going.

00:09:03.510 --> 00:09:08.140
Here is an edge detection filter.

00:09:09.760 --> 00:09:14.490
Picture here, can do a bump distortion,
and if that's too scary,

00:09:14.490 --> 00:09:17.570
let's make that a bit smaller.

00:09:19.710 --> 00:09:21.700
Okay.

00:09:21.700 --> 00:09:26.700
Now I have a glass distortion filter and
I can move the glass around like this.

00:09:26.700 --> 00:09:32.750
Or modulate how thick
the glass actually is.

00:09:35.380 --> 00:09:42.080
Yeah, more funky stuff like perspective,
transform, oops, that was a bit too much.

00:09:44.380 --> 00:09:46.740
or put a spotlight on an image.

00:09:46.740 --> 00:09:47.360
Things like that.

00:09:47.360 --> 00:09:50.740
So there are about 60 of
those and some are fun,

00:09:50.740 --> 00:09:57.220
some are useful, some are both.

00:09:59.520 --> 00:10:02.010
You can also do effects on text,
which turns out to be

00:10:02.010 --> 00:10:04.240
fairly interesting if you,
for example,

00:10:04.450 --> 00:10:08.500
get your drawing out of Core Graphics
and then put effects on top of it.

00:10:08.500 --> 00:10:11.500
So you can do things like that.

00:10:11.510 --> 00:10:15.490
Or my favorite, the zoom blur.

00:10:17.470 --> 00:10:20.860
Actually, Zoom Blur works even
better if I go and say,

00:10:20.890 --> 00:10:24.900
"Fur to the edge detection
filter." Like this.

00:10:24.960 --> 00:10:27.720
And then put the Zoom Blur on top of it.

00:10:27.760 --> 00:10:31.030
I like that better.

00:10:33.790 --> 00:10:40.180
So let me try to do something a bit more
useful and we had a bunch of artists

00:10:40.180 --> 00:10:44.150
come up with scenarios that I could demo,
so I will just repeat one of them.

00:10:44.220 --> 00:10:48.670
So here we have a guitar and
first thing I'm going to do...

00:10:49.800 --> 00:10:57.180
I'm using the monochrome filter to
produce more of a blue-like look here.

00:10:57.180 --> 00:10:59.140
And then...

00:10:59.690 --> 00:11:03.360
Exposure to make it a bit darker.

00:11:03.360 --> 00:11:05.470
Actually,
I want to focus on the hand here,

00:11:05.470 --> 00:11:07.790
so I will add a spotlight.

00:11:09.600 --> 00:11:12.240
Yeah, that's good.

00:11:12.250 --> 00:11:14.500
And then let me add a
layer on top of this,

00:11:14.500 --> 00:11:17.940
like the loose album text line art.

00:11:17.940 --> 00:11:21.500
And at the very end,

00:11:21.750 --> 00:11:23.570
Crop to frame.

00:11:23.570 --> 00:11:28.280
And now I have a little album
cover for my blues band.

00:11:28.280 --> 00:11:32.180
So the key thing here is that I stacked
a bunch of filters on top of each other

00:11:32.560 --> 00:11:35.480
and all of these filters are still live.

00:11:35.480 --> 00:11:39.620
None of these get the result
written back into main memory.

00:11:39.620 --> 00:11:43.290
So I can still go and change the
blue tint in the middle or if

00:11:43.290 --> 00:11:50.460
I don't like the spotlight I can
move it around underneath and so on.

00:11:53.360 --> 00:11:55.960
So the next thing I would like to show,
we can actually handle

00:11:56.410 --> 00:11:58.780
fairly decent sized images.

00:11:58.780 --> 00:12:02.680
So what I showed before,
these were images pretty much

00:12:02.740 --> 00:12:06.610
screen sized or projector sized.

00:12:07.060 --> 00:12:08.310
This one here is different.

00:12:08.450 --> 00:12:11.670
This one is an 11-megapixel image
and it's 16-bit per component,

00:12:11.760 --> 00:12:14.000
so it's a 90-megabyte image total.

00:12:14.000 --> 00:12:18.800
And, well, it's a 90-megabyte image,
but it goes to a 1-megapixel projector,

00:12:18.800 --> 00:12:22.670
so you can't really see that much of it.

00:12:23.600 --> 00:12:35.600
[Transcript missing]

00:12:36.900 --> 00:12:41.100
There's a statue of a bull, and so on.

00:12:41.100 --> 00:12:45.630
So let's do something on this
image and go back and use the

00:12:45.680 --> 00:12:48.310
glass distortion I used before.

00:12:49.150 --> 00:12:50.710
So let me show you,
you can actually see all the

00:12:50.730 --> 00:12:55.750
detail in the glass distortion
applied to this 90 megabyte image.

00:12:56.020 --> 00:12:59.490
So naturally, 90 megabytes,
this takes a bit longer.

00:12:59.490 --> 00:13:02.620
It's not totally real time,
so if I drag this around you see it's

00:13:02.620 --> 00:13:05.180
something like 3 to 4 frames per second.

00:13:05.180 --> 00:13:09.940
But it still works and the key message
here is this is actually well beyond

00:13:09.940 --> 00:13:11.540
the capabilities of the hardware.

00:13:11.620 --> 00:13:16.710
The hardware has a texture limit,
a size limitation, and this image is big,

00:13:16.840 --> 00:13:17.650
too big.

00:13:17.860 --> 00:13:20.710
And Core Image goes and
cuts it into little tiles,

00:13:20.780 --> 00:13:24.620
sends them up to the card,
does all the magic that for

00:13:24.620 --> 00:13:27.320
every filter there's enough
information there to do its job,

00:13:27.360 --> 00:13:30.140
and then stitches everything
together in the end so that you

00:13:30.140 --> 00:13:31.710
can enjoy this on your screen.

00:13:31.740 --> 00:13:37.880
So the last thing I would like to show

00:13:40.120 --> 00:13:44.360
is, well, if it's fast,
you can also apply it to movies.

00:13:44.400 --> 00:13:47.100
So here I have my little bump
distortion on this movie trailer.

00:13:47.100 --> 00:13:54.560
I just have hours of fun with this.

00:13:57.640 --> 00:13:58.600
Okay.

00:13:58.600 --> 00:14:03.600
Or yeah, do the sepia tone.

00:14:03.650 --> 00:14:07.070
And even then,
you can go and stack filters like

00:14:09.300 --> 00:14:11.300
So let's do an edge detection
filter on this movie.

00:14:11.300 --> 00:14:15.300
So, okay, I think you get the idea.

00:14:15.450 --> 00:14:18.060
With that, let's go back to the slides.

00:14:28.480 --> 00:14:31.530
So now, how does this all work?

00:14:31.620 --> 00:14:35.040
So you essentially have to
know three core classes in

00:14:35.150 --> 00:14:37.860
Core Image to use filters.

00:14:37.920 --> 00:14:41.120
So the first one is the CI Image,
and to everybody's surprise,

00:14:41.300 --> 00:14:43.260
that is an image.

00:14:43.350 --> 00:14:44.780
So,

00:14:45.180 --> 00:14:49.020
An image is typically something you
bring in from the outside world like a

00:14:49.020 --> 00:14:52.580
JPEG file or it's the output of a filter.

00:14:52.580 --> 00:14:54.210
And more on that a bit later.

00:14:54.410 --> 00:14:57.580
There is a context which is the
other end of the workflow which

00:14:57.580 --> 00:14:59.480
is the destination abstraction.

00:14:59.730 --> 00:15:04.450
You build a context say on top of a
ZG context or an OpenGL context and

00:15:04.450 --> 00:15:07.300
the context is where you draw into.

00:15:08.600 --> 00:15:13.070
And the third thing,
third piece you need is the CI filter.

00:15:13.070 --> 00:15:15.040
And the filter is what
actually does the work.

00:15:15.050 --> 00:15:19.500
You put one side, the image comes in,
the filter modifies it somehow,

00:15:19.500 --> 00:15:21.480
and then the output is an image.

00:15:21.530 --> 00:15:24.770
You can either take that image
and pass it to the next filter

00:15:24.770 --> 00:15:26.500
or draw it into the context.

00:15:27.600 --> 00:15:31.040
So a bit more detail
about what the image does.

00:15:31.140 --> 00:15:34.160
So an image can get created from,
for example, a CG image ref.

00:15:34.250 --> 00:15:37.640
There's also a possibility you can
pass in NSData with row bytes and an

00:15:37.640 --> 00:15:40.430
encoding or for an OpenGL texture.

00:15:40.540 --> 00:15:44.080
And this is the API call you do,
in this case, for a CG image.

00:15:44.300 --> 00:15:46.340
They all look very similar.

00:15:46.510 --> 00:15:50.960
And typically the output of a
filter is a CI image and this is

00:15:50.960 --> 00:15:53.340
the API call you do to get that out.

00:15:53.340 --> 00:15:56.880
So an important thing here is that

00:15:57.150 --> 00:15:59.860
A CI image can have infinite extent.

00:16:00.030 --> 00:16:05.740
So if you imagine an infinite plane
with a checkerboard pattern on it,

00:16:05.740 --> 00:16:07.980
that's a valid CI image.

00:16:08.030 --> 00:16:11.290
I hope you will never try to draw
something with an infinite extent,

00:16:11.290 --> 00:16:15.210
so better specify a sub-rectangle
you're really interested in.

00:16:15.890 --> 00:16:17.960
So filters.

00:16:18.320 --> 00:16:19.730
Filters are created by name.

00:16:19.940 --> 00:16:26.230
So this case here we create
a filter with the sepia tone.

00:16:26.530 --> 00:16:29.360
by name CICP at home.

00:16:29.360 --> 00:16:34.120
And all the parameters that go into the
filter are set using key value coding.

00:16:34.120 --> 00:16:37.680
So in this case down here,
I will create an NSNumber and set

00:16:37.680 --> 00:16:40.960
it to the key input intensity.

00:16:40.960 --> 00:16:43.480
And key value coding is also
how you get your output.

00:16:43.480 --> 00:16:46.600
So you can ask the filter,
give me the value for key output image.

00:16:46.600 --> 00:16:49.000
That's the output image.

00:16:49.000 --> 00:16:53.030
So the reason why that is done is, well,
first reason is we're lazy.

00:16:53.040 --> 00:16:57.140
We have 60 filters and we don't want
getters and setters for each one of them.

00:16:57.170 --> 00:17:00.520
And the second one is this allows you
to introspect the capabilities of a

00:17:00.520 --> 00:17:05.920
filter so you can build automatic UI and
support plug-ins and things like that.

00:17:05.920 --> 00:17:08.420
So here's the part about introspection.

00:17:08.480 --> 00:17:11.250
If you want to know what
filters are available,

00:17:11.270 --> 00:17:14.960
well, solution one,
you go and read in the documentation.

00:17:14.960 --> 00:17:18.030
Or solution two,
you ask the CF filter class,

00:17:18.040 --> 00:17:21.180
give me all filters in
a specific category.

00:17:21.180 --> 00:17:23.020
You can ask for give me all filters,
period.

00:17:23.020 --> 00:17:27.980
Or give me all filters that are
transitions and suitable for video.

00:17:28.020 --> 00:17:30.600
And then you get a reduced set.

00:17:30.600 --> 00:17:33.370
There's a bunch of categories
for color adjustment,

00:17:33.370 --> 00:17:35.730
geometric adjustment, things like that.

00:17:35.780 --> 00:17:38.990
And there are attributes
like suitable for video,

00:17:39.090 --> 00:17:42.960
suitable for interlaced data,
suitable for still images

00:17:42.960 --> 00:17:44.570
and things like that.

00:17:44.950 --> 00:17:48.120
So once you have a filter,
you can also ask the filter for, well,

00:17:48.130 --> 00:17:49.240
what are your parameters?

00:17:49.240 --> 00:17:51.810
What are the input keys and
what are the output keys?

00:17:51.850 --> 00:17:56.860
And there's an attribute
dictionary you can get which

00:17:57.140 --> 00:17:58.260
Sorry.

00:17:58.260 --> 00:18:00.690
Which contains all the
metadata about an input.

00:18:00.690 --> 00:18:02.560
So it has things like type.

00:18:02.620 --> 00:18:06.100
This is a number, this is a vector,
this is an image.

00:18:06.300 --> 00:18:08.050
And it has semantic information.

00:18:08.140 --> 00:18:11.100
So for a number you could, for example,
have this is an angle,

00:18:11.100 --> 00:18:14.400
this is a distance,
and if you want to put up a slider,

00:18:14.430 --> 00:18:16.870
it's from this value to this value.

00:18:17.140 --> 00:18:19.690
So the attributes dictionary
is fairly rich and allows

00:18:19.700 --> 00:18:21.100
you to build automatic UI.

00:18:21.100 --> 00:18:25.360
In fact, the demo I showed you,
I didn't write any UI code specific

00:18:25.360 --> 00:18:26.100
to these two filters there.

00:18:26.100 --> 00:18:30.160
They just introspect the
filter attributes and build,

00:18:30.250 --> 00:18:33.100
okay, for each of these values,
add a slider for each of this

00:18:33.100 --> 00:18:37.030
at a point that you can move
around and things like that.

00:18:38.150 --> 00:18:41.490
So there is an interesting
detail behind that API.

00:18:41.490 --> 00:18:45.320
It's fairly straightforward in the
sense it looks like immediate mode.

00:18:45.320 --> 00:18:47.620
You take an image,
you pass it to the filter,

00:18:47.620 --> 00:18:49.000
you get a new image out.

00:18:49.260 --> 00:18:53.710
But what's really happening
behind the scenes is that

00:18:53.870 --> 00:18:57.000
image hasn't been rendered.

00:18:57.130 --> 00:19:01.940
The image you got back is
kind of an image recipe.

00:19:02.000 --> 00:19:06.320
It's an image that contains
references to original data plus

00:19:06.400 --> 00:19:11.020
a program that says what to do if
somebody really wants to draw this.

00:19:11.230 --> 00:19:16.250
And it also contains a snapshot of the
parameters that were in the filter at the

00:19:16.250 --> 00:19:19.200
time when you asked for the output image.

00:19:19.330 --> 00:19:23.720
And the image is really only rendered
if you go to the context and say,

00:19:23.800 --> 00:19:29.510
"Okay, draw this image." And that has a
bunch of interesting implications,

00:19:29.510 --> 00:19:33.110
and pretty much all of them
related to performance.

00:19:33.160 --> 00:19:35.100
So first of all,
if you take an image from one filter

00:19:35.100 --> 00:19:38.060
and pass it to the next filter,
the next filter just says, "Okay,

00:19:38.060 --> 00:19:39.920
I have this image here.

00:19:39.930 --> 00:19:44.290
I'll append this program that
I need to execute," and just

00:19:44.440 --> 00:19:46.820
passes the data further on.

00:19:46.880 --> 00:19:49.380
So if you never draw, it's really,
really cheap.

00:19:49.380 --> 00:19:52.820
But probably you want
to draw it at one time.

00:19:52.820 --> 00:19:54.940
And at the time you draw,
there is a compiler

00:19:55.340 --> 00:19:57.660
concatenating these programs,
and actually it's like

00:19:57.710 --> 00:20:01.260
a just-in-time inliner,
that tries to,

00:20:01.510 --> 00:20:05.900
for the target that you're drawing,
to produce an optimal program

00:20:06.310 --> 00:20:08.810
that runs in these pixels.

00:20:09.400 --> 00:20:11.750
So, concatenation of filter operations.

00:20:11.750 --> 00:20:14.680
And the second thing it allows you
to do if you actually draw only a

00:20:14.730 --> 00:20:19.930
sub-rectangle out of your image,
then it at that point knows which

00:20:19.930 --> 00:20:22.290
pieces it actually needs to process.

00:20:22.300 --> 00:20:28.370
So,
let me go on to that point a bit more.

00:20:30.120 --> 00:20:33.030
So what this does is you can
have several components in your

00:20:33.030 --> 00:20:37.000
app or the operating system that
conspire on an image computation.

00:20:37.000 --> 00:20:41.690
So as an example down here,
I have an image that comes from the

00:20:41.730 --> 00:20:47.000
Image I/O subsystem and it does a
color adjust and returns a new image.

00:20:47.040 --> 00:20:49.260
That gets passed into
a different subsystem,

00:20:49.260 --> 00:20:51.820
in this case a thumbnail renderer,
which scales the image

00:20:51.820 --> 00:20:54.730
down and then draws it on,
I don't know,

00:20:54.730 --> 00:20:57.000
a sheet with a bunch of thumbnails,
I guess.

00:20:57.490 --> 00:21:01.940
Because all the operations are deferred,
at the very end, when the context sees,

00:21:02.060 --> 00:21:05.780
well, I have to draw a small image,
and it propagates it all

00:21:05.970 --> 00:21:10.110
back and figures out,
well, I could move that color adjust to

00:21:10.150 --> 00:21:14.140
the right level and process far
less pixels than doing the color

00:21:14.210 --> 00:21:16.390
adjust on the original size image.

00:21:18.240 --> 00:21:21.980
So this is essentially an optimization
technique and it happens completely

00:21:21.980 --> 00:21:26.860
transparently to the user of the API.

00:21:28.870 --> 00:21:33.060
So let me give you a really
simple example how code with using

00:21:33.060 --> 00:21:35.200
Core Image actually looks like.

00:21:35.200 --> 00:21:37.110
In fact,
it's so embarrassingly simple you

00:21:37.110 --> 00:21:41.770
might easily ask why would you
use Core Image at all for this,

00:21:41.960 --> 00:21:45.400
but I'm going to build on it,
so stay with me.

00:21:45.400 --> 00:21:47.700
So the first thing I'm doing
is creating a CI image.

00:21:47.700 --> 00:21:51.000
In this case, I'm using a CG image ref.

00:21:52.280 --> 00:21:55.490
The next step, I'm creating a context.

00:21:55.500 --> 00:21:59.190
And again, I like CG,
so I'm using a CG context

00:21:59.190 --> 00:22:01.280
ref on top of that.

00:22:01.690 --> 00:22:05.200
And then I go and draw the image here.

00:22:05.230 --> 00:22:06.510
And I specify two things.

00:22:06.640 --> 00:22:09.460
One, I specify the point where
I'm going to draw it,

00:22:09.670 --> 00:22:12.780
and I specify the sub-rect out
of the image I'm going to draw.

00:22:12.950 --> 00:22:14.970
Remember that there are
infinite sized images,

00:22:15.120 --> 00:22:19.510
so it's a really good idea to
specify a sub-rectangle in that case.

00:22:19.910 --> 00:22:22.280
So let's add something
interesting in between,

00:22:22.280 --> 00:22:25.550
which so far we just draw an image which,
uh,

00:22:25.790 --> 00:22:28.980
It's hardly worth the
time I should spend on it.

00:22:29.140 --> 00:22:31.020
So let's create a filter in the middle.

00:22:31.020 --> 00:22:32.920
I create a CI color invert filter.

00:22:32.940 --> 00:22:34.170
It's the simplest filter we have.

00:22:34.180 --> 00:22:39.320
It inverts the colors and has no
parameters at all except one image.

00:22:39.360 --> 00:22:45.310
And keys and values is a convenience
method to set all the keys and

00:22:45.310 --> 00:22:48.870
values instead of having to call
individually for each one of them.

00:22:49.130 --> 00:22:52.900
So it sets the input image
to the image we created.

00:22:52.920 --> 00:22:56.180
And the draw image call,
instead of taking the original image,

00:22:56.180 --> 00:22:58.560
it just asks the filter
for the output image.

00:22:58.580 --> 00:23:02.450
And so this is our four lines of
code example how to use a filter.

00:23:02.520 --> 00:23:07.810
So the bigger picture of things--

00:23:09.260 --> 00:23:12.350
Everything that happens within
the space of Core Image happens

00:23:12.350 --> 00:23:14.000
in a defined working space.

00:23:14.000 --> 00:23:16.200
And that thing has two aspects to it.

00:23:16.840 --> 00:23:21.700
There is a color working space
and a working coordinate space.

00:23:21.800 --> 00:23:23.190
So imagine the diagram down here.

00:23:23.190 --> 00:23:25.190
You have a bunch of images coming in.

00:23:25.290 --> 00:23:30.000
It flows through a graph of filters
and then it exits to the context,

00:23:30.000 --> 00:23:31.200
essentially.

00:23:31.500 --> 00:23:33.770
So let me talk about the...

00:23:35.680 --> 00:23:37.540
"Coordinate Space" first.

00:23:37.620 --> 00:23:41.740
Coordinate Space is essentially an
infinite sized plane with a defined

00:23:41.740 --> 00:23:45.200
origin where you can place your images.

00:23:45.220 --> 00:23:48.840
And there are a bunch of filters
that do things like affine transforms

00:23:48.950 --> 00:23:52.810
so you can move them around
and scale and things like that.

00:23:53.540 --> 00:23:55.040
So that's pretty simple.

00:23:55.180 --> 00:23:58.090
Again, this is the reason why you have
to specify a sub-rectangle at

00:23:58.090 --> 00:24:02.990
the very end when you actually
draw out of that working space.

00:24:03.370 --> 00:24:05.700
The color space is a
bit more interesting.

00:24:05.860 --> 00:24:07.810
So color matching happens
on input and output.

00:24:07.840 --> 00:24:11.450
If an image comes in,
is tagged with a color space,

00:24:11.450 --> 00:24:14.200
it gets converted into
a working color space.

00:24:14.200 --> 00:24:17.240
All the processing is done
in that working color space,

00:24:17.240 --> 00:24:21.200
and on output there is a conversion
to the context color space.

00:24:21.200 --> 00:24:23.980
Which, for example,
if you created off a CG context,

00:24:24.000 --> 00:24:27.200
you don't have to do anything,
it figures it out directly.

00:24:27.200 --> 00:24:31.340
The default working space is CChemo2.

00:24:31.430 --> 00:24:35.200
CChemo2 has a bunch of interesting
properties for image processing.

00:24:35.200 --> 00:24:38.200
First one is it's light linear.

00:24:38.200 --> 00:24:40.460
So if you imagine you have
two flashlights and they

00:24:40.460 --> 00:24:43.200
point at the same spot on,
I don't know, this stage,

00:24:43.200 --> 00:24:48.150
then the amount of light that is
reflected is somehow proportional to

00:24:48.270 --> 00:24:52.200
the light from one flashlight plus
the light of the other flashlight.

00:24:52.200 --> 00:24:55.700
Light linear essentially means
that the math inside your filter

00:24:55.700 --> 00:24:57.200
program has that proportion.

00:24:57.200 --> 00:25:01.200
And we'll have an example
for that a bit later.

00:25:01.200 --> 00:25:03.200
And the second one is
it has infinite gamut.

00:25:03.200 --> 00:25:07.200
So CChemo2 doesn't clamp
colors to 0/1 range.

00:25:07.250 --> 00:25:11.420
So if you, for example,
have a YCbCr image coming in from

00:25:11.800 --> 00:25:15.470
video and you want to process it,
if you convert it to RGB,

00:25:15.470 --> 00:25:18.020
if you clamp to 0/1,
you will lose some color.

00:25:18.470 --> 00:25:23.190
But because it has an infinite gamut,
in this case it doesn't.

00:25:23.260 --> 00:25:27.190
So conversion from YCbCr to
RGB and back is lossless.

00:25:30.720 --> 00:25:33.260
There's a bit of secret sauce
on the color space issue

00:25:33.260 --> 00:25:35.020
when it comes to CI images.

00:25:35.250 --> 00:25:38.340
And the color space of
a CI image can be nil.

00:25:38.360 --> 00:25:43.540
But that means you're telling the system,
this image, I'll give you an image,

00:25:43.540 --> 00:25:46.260
but it actually doesn't
contain any color information.

00:25:46.400 --> 00:25:48.740
So for example,
if you have elevation maps,

00:25:48.740 --> 00:25:52.880
normal vectors,
or actually function tables you want

00:25:53.110 --> 00:25:58.180
to pass into your filter kernel,
then clearly you don't want to have color

00:25:58.180 --> 00:26:01.160
matching on your sampled asign function.

00:26:01.270 --> 00:26:05.560
So nil is the color
space used in that case.

00:26:05.670 --> 00:26:08.840
So note that there is a
subtle difference between nil,

00:26:08.840 --> 00:26:10.960
which means this isn't
colors or don't match,

00:26:10.990 --> 00:26:13.510
and this is already
in device color space.

00:26:13.540 --> 00:26:18.020
If you want to send an image in that
is already in the device color space,

00:26:18.060 --> 00:26:19.700
just tag it with the working space.

00:26:19.700 --> 00:26:23.840
Then Core Image will say, OK,
image and working space are the same,

00:26:23.950 --> 00:26:26.370
so there's no work to be done here.

00:26:27.600 --> 00:26:31.080
Let me try to give you an
example why this matters.

00:26:31.080 --> 00:26:34.430
So what I did, I took a tripod,
put my camera on it,

00:26:34.550 --> 00:26:36.700
and took a picture with bracketing on.

00:26:36.760 --> 00:26:41.340
So I have three different exposures
of the same scene and it's two

00:26:41.340 --> 00:26:44.360
f-spots apart from left to right.

00:26:44.430 --> 00:26:45.160
So this is physics.

00:26:45.250 --> 00:26:50.760
This is letting more light through
the shutter by keeping it open longer.

00:26:51.100 --> 00:26:55.710
So what I'm doing now is
take that leftmost image,

00:26:55.760 --> 00:26:59.470
darkest one, and try to simulate the same
thing inside Core Image.

00:26:59.530 --> 00:27:01.810
So I take that image.

00:27:04.160 --> 00:27:08.340
And multiply all the numbers by two,
gives me the middle one.

00:27:08.340 --> 00:27:10.660
Multiply the numbers by two again,
gives me the end one.

00:27:10.660 --> 00:27:14.860
This is why that CCAM two
color space matters.

00:27:14.860 --> 00:27:19.010
It actually matches what physics does.

00:27:19.260 --> 00:27:20.920
So for comparison,
if you actually do that

00:27:21.030 --> 00:27:25.020
in device color space,
so skip all the color matching,

00:27:25.730 --> 00:27:28.760
The results are rather different.

00:27:28.760 --> 00:27:31.020
So for your application that means,
for example,

00:27:31.020 --> 00:27:36.490
if you're dealing with photographs,
you can have an exposure adjust UI and

00:27:36.490 --> 00:27:42.460
it actually produces the same results
that the user is used to from his camera.

00:27:44.910 --> 00:27:51.070
Okay, let me switch gears here a
little and talk about the overall

00:27:51.070 --> 00:27:53.070
model behind the architecture.

00:27:53.070 --> 00:27:56.640
And it's based on this SIGGRAPH paper,
"Modeler for Efficient and Flexible

00:27:56.740 --> 00:27:59.800
Image Computing" by Michael Schanzes.

00:27:59.830 --> 00:28:04.170
And it's published in SIGGRAPH '94.

00:28:04.170 --> 00:28:08.410
And it essentially explains how to build

00:28:08.900 --> 00:28:13.260
The pipeline of image processing
operations and how to propagate through

00:28:13.260 --> 00:28:18.690
that pipeline all the information that
is needed to do the operation in the end.

00:28:18.720 --> 00:28:22.650
And actually Shake is
based on the same model.

00:28:23.810 --> 00:28:26.100
So I will use the terms
out of that paper because,

00:28:26.110 --> 00:28:29.940
well, why invent something new
if it's already been done?

00:28:29.940 --> 00:28:32.460
So there are two key concepts here.

00:28:32.460 --> 00:28:35.040
The first one is the
domain of definition.

00:28:35.040 --> 00:28:39.090
Domain of definition is essentially
the area of an image that is,

00:28:39.240 --> 00:28:44.870
in the broadest sense, interesting,
meaning that everything outside that

00:28:44.870 --> 00:28:47.500
domain of definition is transparent.

00:28:47.500 --> 00:28:49.360
As I mentioned before,
the domain of definition

00:28:49.450 --> 00:28:51.320
could be infinite,
but for pretty much every be

00:28:51.320 --> 00:28:52.840
an image you load from disk?

00:28:52.940 --> 00:28:55.050
It's probably not.

00:28:56.690 --> 00:29:00.360
Second term is the region
of interest or ROI.

00:29:00.600 --> 00:29:06.600
So when you actually draw at the
very end of your filter chain,

00:29:06.600 --> 00:29:10.600
you specify the area that you want
to pick out of the working space.

00:29:10.600 --> 00:29:13.170
This is called the region of interest.

00:29:14.970 --> 00:29:18.400
So here's an example how this works.

00:29:18.520 --> 00:29:21.140
So at leftmost image is the
original image and then do

00:29:21.140 --> 00:29:24.440
the zoom blur filter on it,
which you saw before.

00:29:24.560 --> 00:29:27.140
So the domain of definition
of the original image is,

00:29:27.140 --> 00:29:28.840
well, the extent of the image.

00:29:28.960 --> 00:29:33.360
The domain of definition of the filtered
image is larger because the filter

00:29:33.360 --> 00:29:36.810
bleeds out into neighboring pixels.

00:29:37.520 --> 00:29:39.770
So when I actually go,
and now I'm at the bottom

00:29:39.860 --> 00:29:45.310
right corner of this diagram,
and draw a sub-rectangle,

00:29:45.310 --> 00:29:47.990
in this case the duck's eye,
out of that scene,

00:29:47.990 --> 00:29:51.260
that region of interest is propagated
back into the original image.

00:29:51.290 --> 00:29:54.990
So the filter has to have all of
the yellow area in the bottom left

00:29:54.990 --> 00:29:58.280
corner available to do its operation.

00:29:58.350 --> 00:30:01.290
And what it does,
it intersects the region of interest and

00:30:01.290 --> 00:30:04.840
the domain of definition of the source,
and that's the real data

00:30:04.840 --> 00:30:06.400
that needs to be fetched.

00:30:06.480 --> 00:30:07.400
So why is that important?

00:30:07.400 --> 00:30:10.400
If you write your own filters,
you actually have to provide

00:30:10.530 --> 00:30:12.790
functions that do this mapping.

00:30:15.590 --> 00:30:19.700
With that we come to the topic
of how to write your own filters.

00:30:19.700 --> 00:30:23.330
So there are essentially two more
classes that you need to know on

00:30:23.350 --> 00:30:27.500
top of the three I mentioned before,
how to write your own filters.

00:30:27.750 --> 00:30:31.480
There's the CI kernel which
represents the per pixel operation.

00:30:31.550 --> 00:30:35.340
It's essentially a little program
that produces a pixel at the output

00:30:35.490 --> 00:30:39.490
and it can have things in it like
looking up pixels in the source

00:30:39.490 --> 00:30:45.500
image at various places or just math
or scaling and things like that.

00:30:46.560 --> 00:30:50.580
And there is the CI Sampler which
is the kind of the mediator between

00:30:50.630 --> 00:30:53.500
the kernel and the original image.

00:30:53.500 --> 00:30:55.700
Sampler is an accessor function.

00:30:55.750 --> 00:30:58.910
So when you go and say "give
me the pixel at coordinate x,

00:30:59.060 --> 00:31:03.500
y" then the sampler will kick in
and do some magic things to that.

00:31:03.500 --> 00:31:06.500
So a sampler has two key elements.

00:31:06.500 --> 00:31:09.500
It has an interpolation style,
so you have to know which,

00:31:09.500 --> 00:31:13.500
whether you want to do linear
interpolation or no interpolation.

00:31:13.570 --> 00:31:17.320
It has wrap mode attributes,
so if you ask outside the

00:31:17.320 --> 00:31:19.010
DoD what should happen.

00:31:21.230 --> 00:31:25.700
So the kernel here, um... Yeah,
sorry about that.

00:31:25.740 --> 00:31:28.900
The kernel represents
the per pixel operation.

00:31:28.980 --> 00:31:30.900
And I have a little sample here.

00:31:31.120 --> 00:31:33.500
This is the magnifier code
I showed in the previous demo,

00:31:33.500 --> 00:31:36.650
the little circle that
showed you the Eiffel Tower.

00:31:36.920 --> 00:31:38.040
That's pretty much all there is to it.

00:31:38.120 --> 00:31:41.760
There is a distance computation to
make the little gradient on the side

00:31:41.800 --> 00:31:44.800
and pick out the right scaled pixels.

00:31:44.810 --> 00:31:50.600
With that,
I'll go back to demo machine number two.

00:31:59.020 --> 00:32:01.290
Some point?

00:32:01.320 --> 00:32:01.870
Okay.

00:32:01.870 --> 00:32:08.940
So what I'm going to show you now
is essentially more of the same,

00:32:08.940 --> 00:32:10.960
but now that you have
the overall concept,

00:32:10.960 --> 00:32:13.630
I can explain a bit more what's going on.

00:32:13.750 --> 00:32:18.490
So the first thing I'm doing,
this is a little test app, by the way,

00:32:18.550 --> 00:32:21.880
that we wrote to figure
out how the framework,

00:32:21.880 --> 00:32:25.960
how to tune the framework,
and it has a nice aspect that it

00:32:26.050 --> 00:32:27.860
visualizes nicely what's going on.

00:32:28.120 --> 00:32:33.370
So I'll take an image, hibiscus here,
then I pipe that through,

00:32:33.370 --> 00:32:36.230
let's say the bump distortion
filter you saw before,

00:32:36.230 --> 00:32:37.520
and then display it.

00:32:37.640 --> 00:32:40.960
So this is the output image.

00:32:42.500 --> 00:32:45.260
And in the top left corner
you see that little flow,

00:32:45.260 --> 00:32:46.660
how the data flows through.

00:32:46.810 --> 00:32:48.490
So this is the bump
distortion you saw before,

00:32:48.490 --> 00:32:49.430
I can move that around.

00:32:49.480 --> 00:32:54.850
And it has a bunch of parameters
like the radius and things like that.

00:32:55.310 --> 00:32:58.960
So the first thing I mentioned
is that there is a CPU fallback.

00:32:59.010 --> 00:33:02.810
So what this case here is doing,

00:33:03.240 --> 00:33:05.770
Is the bump distortion
running on the GPU?

00:33:05.770 --> 00:33:08.880
And it's probably really
hard to read in the audience,

00:33:08.880 --> 00:33:11.810
but there is a little frame rate
indicator in the bottom left corner,

00:33:11.960 --> 00:33:15.100
and it says this runs
at 93 frames per second.

00:33:15.100 --> 00:33:19.800
So let me go and switch...

00:33:22.700 --> 00:33:25.920
and switch this to use
the CPU renderer instead.

00:33:26.500 --> 00:33:29.840
And you see it's a bit more chunky,
but the CPU renderer runs on, yeah,

00:33:29.840 --> 00:33:33.960
about 20 to 25 frames per
second for this operation.

00:33:37.140 --> 00:33:43.490
and he produced the same result,
which is kind of surprising at times.

00:33:45.080 --> 00:33:49.980
So let me try to build a bit more
complicated example that shows

00:33:49.990 --> 00:33:53.870
the power behind the framework,
especially when the just-in-time

00:33:53.870 --> 00:33:58.000
compiler kicks in and does some
more or less smart things at times.

00:33:58.370 --> 00:34:04.000
So let me start with the edge
detection filter you saw before.

00:34:04.040 --> 00:34:06.690
So it looks like this.

00:34:09.010 --> 00:34:11.270
Let's make these edges really visible.

00:34:11.270 --> 00:34:14.640
And I'm going to change
the image as well.

00:34:14.640 --> 00:34:18.990
So let's open an image like Core Image.

00:34:19.000 --> 00:34:24.990
Make a little wire here,
I'm going to need that a bit later.

00:34:27.610 --> 00:34:31.660
So this is the edge detection
filter on Core Image line art.

00:34:31.830 --> 00:34:33.800
So far, not that interesting.

00:34:33.800 --> 00:34:37.800
I'm going to add a new filter for...

00:34:37.950 --> 00:34:39.900
Exposure Adjustment.

00:34:39.930 --> 00:34:41.890
And then I'm adding the zoom
blur I showed you before.

00:34:41.900 --> 00:34:45.900
So this is how the zoom
blur image looks like.

00:34:45.900 --> 00:34:47.880
Let me make that a bit...

00:34:49.780 --> 00:34:53.700
So this is the zoom
blur effect on line art.

00:34:53.700 --> 00:34:58.350
At the very end I'm going
to add a compositing mode,

00:34:58.430 --> 00:35:00.320
in this case "Addition".

00:35:00.870 --> 00:35:04.740
and add the original image
back to the Zoom Blur image.

00:35:04.840 --> 00:35:08.150
So this looks like this.

00:35:09.490 --> 00:35:10.300
So it looks like that.

00:35:10.300 --> 00:35:14.040
And with the mouse I'm just moving
the origin of the zoom blur around.

00:35:14.130 --> 00:35:16.610
So you can easily imagine
that if you want to do some

00:35:16.610 --> 00:35:20.690
visual effects on line art,
building some interesting

00:35:20.790 --> 00:35:23.300
visuals with this is pretty easy.

00:35:23.320 --> 00:35:28.210
And that's the key where the
compiler actually comes in.

00:35:28.450 --> 00:35:31.400
Most of these operations actually
are collapsed into single pass.

00:35:31.450 --> 00:35:35.600
It's only the zoom blur which already
does multiple passes toward this,

00:35:35.640 --> 00:35:40.570
to create this effect that still
has multiple passes in the very end,

00:35:40.640 --> 00:35:42.990
but everything else is collapsed.

00:35:45.640 --> 00:35:53.150
www.microsoft.com/mechanics/coreimage-im
ages

00:36:00.500 --> 00:36:05.460
I would like to ask Mark Zimmer up
on the stage to explain to you how to

00:36:05.460 --> 00:36:07.980
actually write one of those things.

00:36:13.230 --> 00:36:13.650
Thanks, Ralph.

00:36:13.680 --> 00:36:18.660
Okay, the fun part about this is writing
your own image processing kernel.

00:36:18.660 --> 00:36:23.540
The thing about core image is that
it does come with 60-plus filters,

00:36:23.540 --> 00:36:25.680
but if they're not really
the filters that you want,

00:36:25.680 --> 00:36:28.240
or you can't put them together
like you just saw into a

00:36:28.240 --> 00:36:31.140
graph to do what you want,
or that graph that you did put

00:36:31.140 --> 00:36:33.520
together is not running fast enough,
then you may want to

00:36:33.520 --> 00:36:34.490
write your own kernel.

00:36:34.500 --> 00:36:36.980
In fact,
that turns out to be one of the things in

00:36:36.980 --> 00:36:41.830
Tiger that really runs much faster than
anything we've ever produced in the past.

00:36:42.540 --> 00:36:44.300
Think about it.

00:36:45.240 --> 00:36:49.290
So you can basically make your
Tiger Roar by building your own filter.

00:36:49.430 --> 00:36:54.030
So creating your own plug-in filter
in Core Image is something that you

00:36:54.030 --> 00:36:59.100
can do and basically when you do that
you end up creating a pixel shader to

00:36:59.100 --> 00:37:05.320
program a GPU and it's something that
sounds actually pretty complicated but

00:37:05.320 --> 00:37:05.320
as it turns out it's really very simple.

00:37:05.530 --> 00:37:08.040
The neat thing is that these filters,
once you've created them,

00:37:08.040 --> 00:37:09.390
are first-class citizens.

00:37:09.390 --> 00:37:12.390
So that means your filter can
run as fast as our filters.

00:37:12.400 --> 00:37:17.390
You basically are going to concentrate
on the kernel implementation.

00:37:17.630 --> 00:37:20.400
Most of the stuff around there represents
a small fraction of what you do.

00:37:20.400 --> 00:37:24.720
A lot of your time will be spent
working on the actual shader.

00:37:25.580 --> 00:37:30.000
So, filters are based on
Objective-C at the top level.

00:37:30.000 --> 00:37:33.350
They have a few methods which you'll see
and it's pretty simple to create one.

00:37:33.400 --> 00:37:36.790
It has an image processing kernel
which is the fun part and you'll see

00:37:36.840 --> 00:37:38.820
what I'm talking about in a minute.

00:37:38.930 --> 00:37:43.320
All of the filters that you've seen
so far have kernels and are built

00:37:43.330 --> 00:37:47.570
on top of the GL shading language
and we have a special variant of

00:37:47.630 --> 00:37:50.400
that called the CI kernel language.

00:37:50.400 --> 00:37:53.890
The filter is embedded in the
Image Unit Bundle structure

00:37:53.890 --> 00:37:57.270
which Frank Doepke will talk
about in the next speaker.

00:37:58.560 --> 00:38:00.890
Cool thing about this,
I've been writing image processing

00:38:00.890 --> 00:38:02.610
effects essentially for 18 years.

00:38:02.760 --> 00:38:05.840
I found that this was the easiest
way to write them and it produced

00:38:06.030 --> 00:38:09.360
the fastest results for any
filter that I've worked on.

00:38:09.360 --> 00:38:12.080
So for instance,
the Zoom Blur filter that I produced

00:38:12.170 --> 00:38:15.450
ended up being about a hundred times
faster than the same filter in Photoshop.

00:38:18.270 --> 00:38:23.160
Alright,
that's running on a G4 1.4 dual anyway.

00:38:23.200 --> 00:38:26.860
So,
let's talk about the Objective-C portion.

00:38:26.920 --> 00:38:29.840
What you do is you're going
to build a class definition

00:38:29.840 --> 00:38:31.690
that's subclassing CI filter.

00:38:31.690 --> 00:38:35.180
In the class definition you
will define your input keys.

00:38:35.190 --> 00:38:40.980
The init method of that basically helps
you to locate your kernel which is going

00:38:40.980 --> 00:38:43.800
to be in a text file from the bundle.

00:38:43.800 --> 00:38:47.200
It's actually pretty simple
although it's several steps.

00:38:47.400 --> 00:38:50.730
The custom attributes method
helps you to specify key defaults

00:38:50.730 --> 00:38:52.540
and ranges for your input keys.

00:38:52.890 --> 00:38:55.870
That's particularly useful if you
load your filter into a program

00:38:56.240 --> 00:39:00.780
that automatically produces UI such
as Quartz Composer or any of

00:39:00.790 --> 00:39:03.130
the demo apps that you saw here.

00:39:03.200 --> 00:39:07.990
The output image method finally is
something that helps you to take your,

00:39:07.990 --> 00:39:11.060
okay let's talk about this,
take a step back.

00:39:11.200 --> 00:39:14.200
The kernel is executed once per pixel.

00:39:14.260 --> 00:39:16.200
So, what you want to do is take
everything inside of there.

00:39:16.200 --> 00:39:21.100
It doesn't need to be there and hoist it
into the Objective-C output image method.

00:39:21.160 --> 00:39:25.630
So, that's what I mean by performing
pixel loop invariant calculations.

00:39:26.030 --> 00:39:28.910
Anything you can hoist or
constant fold is pulled out.

00:39:29.180 --> 00:39:32.150
It will organize your
input keys as you'll see.

00:39:32.170 --> 00:39:36.300
It calls the kernel using an apply
method which is a filter method.

00:39:36.300 --> 00:39:41.240
And then it also helps you to specify the
domain of definition of the operation.

00:39:41.240 --> 00:39:43.890
That's part of the apply as you'll see.

00:39:44.630 --> 00:39:47.690
Okay, so the Funhaus filter,
which is demonstrated

00:39:47.690 --> 00:39:49.670
up here in the corner,
shows how,

00:39:49.720 --> 00:39:53.860
is something I think has been shown here,
is something that we're now

00:39:54.000 --> 00:39:55.890
showing a class definition for.

00:39:56.320 --> 00:39:58.780
You'll see input image,
which is basically your one

00:39:58.780 --> 00:40:00.110
image that comes into it.

00:40:00.360 --> 00:40:03.700
You'll see three parameters: the center,
the width, and the amount.

00:40:03.700 --> 00:40:05.630
So, we're creating an interface for that.

00:40:05.800 --> 00:40:11.690
Notice the header files at the top:
Quartz Core, Core Image.h,

00:40:11.690 --> 00:40:11.690
I hope I got that right.

00:40:12.370 --> 00:40:15.330
All right, okay.

00:40:15.480 --> 00:40:18.100
Now, the init method is kind of,
this is where things get a

00:40:18.200 --> 00:40:19.290
little bit more complicated.

00:40:19.300 --> 00:40:23.010
And what we're trying to do here is
we're trying to locate our kernel

00:40:23.260 --> 00:40:25.230
from the bundle and process it.

00:40:25.480 --> 00:40:28.300
So, the first thing you do
is to find your bundle,

00:40:28.380 --> 00:40:30.090
so it's bundled for class.

00:40:31.500 --> 00:40:35.890
The second thing you
do is to load up code,

00:40:35.890 --> 00:40:38.120
basically string with contents of file.

00:40:38.270 --> 00:40:40.470
So you've located your bundle,
you brought it in as a,

00:40:40.520 --> 00:40:45.700
it's like opening a text file
and putting it into a string.

00:40:45.700 --> 00:40:45.700
The third thing you do

00:40:47.070 --> 00:40:49.030
is to, the clicker's got to work.

00:40:49.120 --> 00:40:51.860
I probably have my hand
in front of the thing.

00:40:51.860 --> 00:40:55.660
Okay,
is to basically use the CI kernel kernels

00:40:55.700 --> 00:41:00.600
with string method to extract all of the
kernels from the contents of that file.

00:41:00.600 --> 00:41:04.050
And what happens is there's multiple
kernels per file if you want,

00:41:04.210 --> 00:41:06.030
but usually a filter will only have one.

00:41:06.040 --> 00:41:09.870
The reason you might want multiples
is so you can do multiple pass

00:41:09.870 --> 00:41:12.110
operations internally to a filter.

00:41:12.120 --> 00:41:14.130
And that's useful for various things.

00:41:14.230 --> 00:41:17.250
If you wanted to build a blur,
for instance, you really would have

00:41:17.300 --> 00:41:18.870
to use multiple passes.

00:41:21.800 --> 00:41:23.440
Okay.

00:41:23.440 --> 00:41:23.670
There we go.

00:41:23.680 --> 00:41:27.060
The custom attributes method,
this is something I'll

00:41:27.060 --> 00:41:28.180
pretty much skim over.

00:41:28.180 --> 00:41:29.720
It's not massively important.

00:41:29.720 --> 00:41:35.580
For this, you see input width is defined
with a range and a given default.

00:41:35.580 --> 00:41:37.790
Also, its type is defined, type distance.

00:41:37.810 --> 00:41:41.220
There's type scalar, there's angles,
there's other possible types.

00:41:41.300 --> 00:41:43.110
Check the header out
for the various types.

00:41:43.180 --> 00:41:46.140
You'll also have to set the
ranges and defaults for input

00:41:46.140 --> 00:41:49.420
amount and input center,
which are the two other keys,

00:41:49.420 --> 00:41:50.820
non-image keys.

00:41:51.390 --> 00:41:54.500
They'll be provided here as well,
I just didn't show them.

00:41:55.150 --> 00:41:58.260
The output image method is
probably the most important

00:41:58.350 --> 00:42:00.100
part of the Objective-C level.

00:42:00.150 --> 00:42:03.380
And you'll see the apply
method being called there.

00:42:03.430 --> 00:42:07.280
And it gets passed the kernel and also
the other parameters of the filter.

00:42:07.300 --> 00:42:10.470
It's important to note that the
parameters in the filter have to mirror

00:42:10.470 --> 00:42:17.100
exactly what's being passed into the
kernel in the kernel program itself.

00:42:17.220 --> 00:42:19.720
So what we're doing here,
we have 1 over radius,

00:42:19.830 --> 00:42:23.270
1 over 10 to the float
value of input amount,

00:42:23.270 --> 00:42:23.760
etc.

00:42:23.840 --> 00:42:28.160
These are things that we sort of constant
folded and put into here so we won't

00:42:28.160 --> 00:42:30.700
have to do them in the kernel program.

00:42:30.720 --> 00:42:33.300
And as always,
parameters are passed as objects.

00:42:33.460 --> 00:42:35.280
So we're using NSNumber.

00:42:35.280 --> 00:42:38.540
If you were passing a coordinate,
you'd pass a CI vector.

00:42:38.570 --> 00:42:41.490
If you were passing a color,
you'd pass CI color.

00:42:41.810 --> 00:42:44.200
And of course,
the images are passed as samplers.

00:42:44.220 --> 00:42:47.650
So you'll see how we load the
sampler and then pass it in there.

00:42:47.700 --> 00:42:50.700
So that's really all you have
to do inside of your filter.

00:42:50.700 --> 00:42:53.700
Except now we're getting to
the most interesting part.

00:42:53.700 --> 00:42:57.700
You know,
if filters were like a piece of candy,

00:42:57.700 --> 00:43:01.740
then I suppose the shader part,
which processes the pixels,

00:43:01.840 --> 00:43:05.680
you'd probably consider that the soft,
chewy center, the best part.

00:43:05.800 --> 00:43:08.690
This is the part you want to spend
your time thinking about what to do.

00:43:08.700 --> 00:43:13.480
Okay, we're using a subset of the GL,
the OpenGL shading language here.

00:43:13.700 --> 00:43:16.700
You can specify multiple kernels.

00:43:16.700 --> 00:43:19.700
Other functions can be
included for modularity's sake.

00:43:19.700 --> 00:43:20.790
If you have something that's
used multiple times and

00:43:20.870 --> 00:43:23.270
you want to extract it,
you can basically treat it

00:43:23.270 --> 00:43:24.700
like you would normal C.

00:43:24.700 --> 00:43:27.640
This is the place where you're
going to want to go to find out

00:43:27.640 --> 00:43:29.510
about the OpenGL shading language.

00:43:29.710 --> 00:43:35.010
So, opengl.org/documentations/oglsl.html.

00:43:35.450 --> 00:43:37.930
Okay, each kernel procedure gets
called once per pixel.

00:43:38.010 --> 00:43:40.770
I mentioned this before,
and it becomes important when you

00:43:40.770 --> 00:43:42.260
decide how to build your shader.

00:43:42.400 --> 00:43:45.660
When you are building your shader,
another thing you should know is

00:43:45.660 --> 00:43:50.400
that there is no knowledge passed
or accumulated from pixel to pixel.

00:43:50.400 --> 00:43:52.360
So it's kind of like a ray tracer.

00:43:52.400 --> 00:43:55.400
You don't know anything else but
what you're doing for this pixel.

00:43:55.450 --> 00:43:56.400
Think of it that way.

00:43:56.400 --> 00:43:59.430
And like I said before,
hoist as much invariant

00:43:59.430 --> 00:44:01.400
calculation as you can out.

00:44:01.400 --> 00:44:05.290
Anything you can do is going to save time
because it gets executed once per pixel.

00:44:05.400 --> 00:44:07.980
When you pass in color,
remember the colors are

00:44:07.980 --> 00:44:12.400
pre-multiplied alpha,
and in general, so are the images.

00:44:14.760 --> 00:44:17.160
Okay,
so here's some effects you can look at.

00:44:17.200 --> 00:44:20.340
There's the original image,
there's the Funhaus effect which

00:44:20.340 --> 00:44:24.280
provides sort of a distortion in X,
the Edges effect which

00:44:24.280 --> 00:44:27.200
everybody seems to like to show,
got shown in the keynote,

00:44:27.360 --> 00:44:28.840
and the Spotlight effect.

00:44:28.890 --> 00:44:32.810
These effects are all
going to be shown here.

00:44:33.290 --> 00:44:35.200
So let's start with a little
fun in the shader here.

00:44:35.200 --> 00:44:38.000
We've got the displacement effect.

00:44:38.070 --> 00:44:40.760
One thing you should notice for
displacement effects is if you

00:44:40.920 --> 00:44:42.990
want to do a displacement effect,
you want to sort of operate

00:44:42.990 --> 00:44:44.190
using the inverse transform.

00:44:44.200 --> 00:44:46.540
In other words,
for the destination point,

00:44:46.580 --> 00:44:48.210
what the source point is.

00:44:48.450 --> 00:44:51.140
Anybody who's done, you know,
in the texture that you're loading,

00:44:51.140 --> 00:44:54.540
anybody who's done an image processing
transform or a displacement transform

00:44:54.540 --> 00:44:55.880
will know this is how it works.

00:44:55.980 --> 00:44:59.020
It's kind of the opposite of, you know,
where does my pixel go?

00:44:59.020 --> 00:45:00.410
It's, you know, the opposite thing.

00:45:01.060 --> 00:45:05.160
So in this case here,
we start by loading the dest cord.

00:45:05.160 --> 00:45:10.130
This is a built-in function in our
kernel language that allows you

00:45:10.130 --> 00:45:14.140
to load the location in working
coordinates of the current pixel.

00:45:16.060 --> 00:45:17.890
Then you want to apply the transform.

00:45:18.060 --> 00:45:21.340
So I'm subtracting a center X,
multiply radius, whatever,

00:45:21.340 --> 00:45:22.220
blah blah blah.

00:45:22.430 --> 00:45:27.430
Basically what it does is it's
going to distort T1.X which

00:45:27.540 --> 00:45:30.610
is the destination coordinate.

00:45:30.940 --> 00:45:35.020
Finally, we fetch the displaced sample.

00:45:35.020 --> 00:45:37.840
And what we're doing here is
we're using the sample function,

00:45:37.840 --> 00:45:40.520
again, from the sampler,
and the coordinates

00:45:40.670 --> 00:45:42.410
being passed in is T1.

00:45:42.420 --> 00:45:45.480
But what we have to do is we need
to run it through sampler transform.

00:45:45.480 --> 00:45:49.770
This is if that sampler actually
is referenced under transform.

00:45:49.780 --> 00:45:52.160
So it will take care of that for you.

00:45:52.220 --> 00:45:55.100
If you don't want the sampler to
be referenced under transform,

00:45:55.100 --> 00:45:58.570
then I guess you can just use sampler
coord instead of sampler transform.

00:45:58.580 --> 00:45:59.940
I'll show you that method.

00:46:00.800 --> 00:46:03.880
At the end, I'll talk about it.

00:46:04.380 --> 00:46:07.530
And... is this battery running out?

00:46:07.590 --> 00:46:09.800
Is that what's going on?

00:46:09.800 --> 00:46:09.800
Okay.

00:46:10.300 --> 00:46:13.090
All right, the edges effect,
which seems to be shown quite a bit,

00:46:13.190 --> 00:46:14.330
is really quite simple.

00:46:14.530 --> 00:46:17.200
What it is is a cross-gradient function.

00:46:17.370 --> 00:46:20.540
And one thing you want to remember
is when you do calculations in here,

00:46:21.010 --> 00:46:25.630
particularly on things like vec4s,
which is R1, R3, R2, R0,

00:46:25.640 --> 00:46:29.940
what's happening is you're doing
all the operations component-wise.

00:46:30.070 --> 00:46:33.750
So you're using the power of
the vector instructions to get

00:46:33.750 --> 00:46:37.880
it done several times faster,
even inside the shader itself.

00:46:38.170 --> 00:46:41.960
Remember that the graphics cards
are actually multi-pipelines.

00:46:42.090 --> 00:46:48.100
So what that means is it's doing,
you know, when I say R1 minus R3 there,

00:46:48.100 --> 00:46:51.530
it actually does four
subtractions simultaneously.

00:46:51.670 --> 00:46:55.040
But there's also multiple pipes,
so it's times the number of pipes,

00:46:55.070 --> 00:46:57.740
which is the number of calculations
happening at the same time.

00:46:57.800 --> 00:47:02.750
So you can actually get, you know,
20 gigaflops on these cards quite easily.

00:47:02.800 --> 00:47:05.900
Okay, so the first thing we do is
load a neighborhood of samplers,

00:47:05.960 --> 00:47:07.700
of samples, sorry.

00:47:07.760 --> 00:47:10.960
And that's just a square neighborhood,
very simple.

00:47:11.010 --> 00:47:13.590
Okay, then the second thing we do is
to compute the cross-gradient.

00:47:13.740 --> 00:47:18.360
So we're subtracting a
cross-gradient like so.

00:47:18.400 --> 00:47:25.940
And then we're doing the least squares
computation and multiplying by the scale,

00:47:26.040 --> 00:47:28.640
so we can scale up or down the edges.

00:47:28.690 --> 00:47:32.660
And then I just throw in an alpha of 1
at the end just so it will be visible.

00:47:32.700 --> 00:47:36.790
It doesn't really have a reasonable
value that you might want to put there.

00:47:39.700 --> 00:47:43.840
Okay, the final example is the spotlight.

00:47:43.970 --> 00:47:46.790
And often when you're
doing a 2D rendering,

00:47:46.790 --> 00:47:49.700
you'll want to use some kind of a 3D
effect on it to make it look cool.

00:47:49.700 --> 00:47:52.200
In this case here,
we're doing a spotlight.

00:47:52.200 --> 00:47:54.460
The spotlight is done in the shader.

00:47:54.460 --> 00:47:56.520
This is probably the
most complicated shader.

00:47:56.620 --> 00:48:01.390
The first thing I do is
to get the pixel color.

00:48:01.390 --> 00:48:01.390
So that's the color that we're shading.

00:48:02.660 --> 00:48:06.240
Okay, second thing I do is to
calculate the vector to the light

00:48:06.240 --> 00:48:08.700
source and then normalize it.

00:48:08.700 --> 00:48:12.500
Because I'm assuming the
picture is in the XY plane,

00:48:12.800 --> 00:48:14.930
the normal to the picture is 001.

00:48:15.190 --> 00:48:21.770
So if I, so n.l, where n is 001,
means that the n.l is

00:48:21.770 --> 00:48:24.480
going to be r0.z here.

00:48:27.110 --> 00:48:32.400
Okay, and then I calculate the
light solid angle cosine.

00:48:32.400 --> 00:48:36.000
This tells me how much light
is being put into that spot.

00:48:36.100 --> 00:48:38.500
So it's just really a cosine,
a dot product is for

00:48:38.500 --> 00:48:39.880
calculating the cosine.

00:48:40.110 --> 00:48:47.220
Then I raise it to a power to
concentrate it and make it a little,

00:48:47.220 --> 00:48:48.690
you know,
so if you wanted to have a concentrated

00:48:48.690 --> 00:48:48.690
or a wide spread or a thin spread.

00:48:50.970 --> 00:48:53.940
Then finally,
I calculate the pixel color by

00:48:54.050 --> 00:48:59.850
multiplying n.l by the light color by R1,
which is the color of the pixel,

00:49:00.070 --> 00:49:01.240
times the beam concentration.

00:49:01.240 --> 00:49:07.280
That gives me the final
result for the spotlight.

00:49:08.380 --> 00:49:09.820
It's not really a lot to it, really.

00:49:09.820 --> 00:49:12.650
When you talk about the
OpenGL Shading Language,

00:49:12.720 --> 00:49:14.920
let me just sort of
go over it in general.

00:49:15.020 --> 00:49:17.440
The first thing is it uses
standard C precedents,

00:49:17.450 --> 00:49:18.660
and it looks pretty much like C.

00:49:18.660 --> 00:49:22.120
The second thing is that there
is no coercion in this language,

00:49:22.120 --> 00:49:25.320
which means if you want
to add an int to a float,

00:49:25.320 --> 00:49:27.550
it's going to give you an
error when it compiles it.

00:49:27.600 --> 00:49:32.650
So you should have a constants should be
specified for floating point as like 1.0,

00:49:32.650 --> 00:49:33.350
0.0.

00:49:33.360 --> 00:49:34.780
Okay.

00:49:34.780 --> 00:49:39.340
The third thing is you noticed in
those examples we have float variables,

00:49:39.340 --> 00:49:42.860
scalar, or we have vector variables,
vec2, vec3, vec4,

00:49:42.860 --> 00:49:44.340
which are vectors of floats.

00:49:44.420 --> 00:49:47.010
There's also Boolean vectors
and such things as that.

00:49:47.070 --> 00:49:49.270
But basically,
the vector variables are how you

00:49:49.330 --> 00:49:50.700
get a lot of the computation done.

00:49:50.700 --> 00:49:54.280
The built-in functions,
I should talk a little bit about it.

00:49:54.340 --> 00:49:58.180
The first row of built-in functions,
sine, cos, pow, et cetera,

00:49:58.180 --> 00:50:00.960
those are kind of the
more expensive functions,

00:50:00.960 --> 00:50:03.580
with the ones at the beginning
kind of being more expensive

00:50:03.580 --> 00:50:04.460
than the ones at the end.

00:50:04.860 --> 00:50:07.300
Square root or inverse square
root It's actually pretty cheap.

00:50:07.380 --> 00:50:10.010
But, you know, if you have like...

00:50:11.340 --> 00:50:13.550
Sign calls in a fragment program,
you know,

00:50:13.620 --> 00:50:16.630
it may be a very slow fragment program.

00:50:16.860 --> 00:50:20.340
The second line, these are practically
zero cost functions,

00:50:20.340 --> 00:50:22.200
or they cost one instruction.

00:50:22.200 --> 00:50:24.800
Things like absolute value, sign, floor,
etc.

00:50:24.950 --> 00:50:25.900
They're all available for you.

00:50:25.900 --> 00:50:27.360
And then there's the
things like distance,

00:50:27.450 --> 00:50:28.710
normalize, etc.

00:50:28.970 --> 00:50:33.800
which are extremely useful in doing
radial or centric kind of calculations.

00:50:34.030 --> 00:50:36.740
Also, it uses a swizzle syntax
for load and mass store.

00:50:36.890 --> 00:50:41.450
So you can reference something
like R0.R for the red component

00:50:41.450 --> 00:50:45.800
or R0.RGB for the red,
green, and blue components of it.

00:50:45.800 --> 00:50:49.100
In particular, when you're doing a store,
it really just says which of the

00:50:49.100 --> 00:50:51.670
components you're storing into.

00:50:53.040 --> 00:50:56.320
The language for CI Kernels
is the OpenGL shading.

00:50:56.570 --> 00:50:59.410
We've added two things to it.

00:50:59.450 --> 00:51:02.030
In particular,
we've added the kernel specifier,

00:51:02.040 --> 00:51:04.140
which you've seen in each example.

00:51:04.140 --> 00:51:08.480
The sampler type is used to
declare image parameters.

00:51:08.640 --> 00:51:11.080
DeskCord is added to give
you the working coordinates

00:51:11.080 --> 00:51:14.380
location of the current pixel.

00:51:14.380 --> 00:51:17.660
There's a sample function which
allows you to do a texture map lookup.

00:51:17.660 --> 00:51:21.800
So it's very simple,
as you saw in all the examples.

00:51:21.800 --> 00:51:23.060
There's a sample.

00:51:23.180 --> 00:51:27.040
SamplerCord is for giving you
the location on that sample,

00:51:27.040 --> 00:51:29.900
and that's actually how
you access a texture unit,

00:51:29.900 --> 00:51:31.900
if you're familiar with OpenGL.

00:51:32.190 --> 00:51:34.650
So each sampler may get
its own texture unit.

00:51:34.790 --> 00:51:37.630
The sampler transform is one
where a sampler may have an

00:51:37.640 --> 00:51:40.870
affine transform applied to it,
and that's how you can make

00:51:40.870 --> 00:51:42.140
sure that is preserved also.

00:51:42.390 --> 00:51:44.640
It literally generates instructions.

00:51:44.910 --> 00:51:53.490
We use the __ColorType to
define Vec4s that are color

00:51:53.550 --> 00:51:57.950
matched inside of your shader.

00:51:58.640 --> 00:52:02.040
And finally,
because colors are pre-multiplied,

00:52:02.120 --> 00:52:05.350
if you're doing something like a
color transform operation like,

00:52:05.420 --> 00:52:07.770
well, invert is a good example,
color invert,

00:52:07.840 --> 00:52:11.660
or any kind of color control, brightness,
contrast, the first thing you want to do

00:52:11.760 --> 00:52:13.290
is un-premultiply that color.

00:52:13.300 --> 00:52:16.190
Then do your light linear transform.

00:52:16.650 --> 00:52:20.580
"And make sure you preserve alpha in your
transform if that's what you want to do.

00:52:20.580 --> 00:52:24.740
So there are some things like for
instance if you just want to scale,

00:52:24.890 --> 00:52:27.800
you know,
do an opacity calculation you can

00:52:27.800 --> 00:52:31.760
do on the entire color without
pre-multiply or un-pre-multiply.

00:52:32.360 --> 00:52:36.990
You could multiply your VEC4 color
by the opacity fraction to get a

00:52:37.160 --> 00:52:40.610
pre-multiplied color that was correct.

00:52:40.830 --> 00:52:44.300
Okay, and basically that's the entire,
that's how to write the

00:52:44.300 --> 00:52:45.660
internals of a filter.

00:52:45.660 --> 00:52:47.730
Now I'd like to introduce...

00:52:47.800 --> 00:52:54.500
[Transcript missing]

00:52:57.600 --> 00:52:58.560
Thank you, Mark.

00:52:58.850 --> 00:52:59.900
Good afternoon.

00:52:59.980 --> 00:53:04.240
As you can see,
I'm the packaging expert on the... how

00:53:04.310 --> 00:53:07.000
we put the eye candy now into the box.

00:53:07.900 --> 00:53:08.990
So what are Image Units?

00:53:09.060 --> 00:53:12.860
Image Units is our architecture that
you can provide a plug-in that we

00:53:12.900 --> 00:53:16.800
can use in any application that will
use the Core Image architecture.

00:53:16.930 --> 00:53:21.050
And for that we chose NSBundle as the
delivery mechanism for the whole part.

00:53:21.090 --> 00:53:25.910
And that makes it very easy to write,
actually, in Image Units.

00:53:26.370 --> 00:53:31.080
The key point here is that this is
actually your business opportunity.

00:53:31.080 --> 00:53:33.360
We know that this is something
that we only introduced in Tiger,

00:53:33.360 --> 00:53:33.930
but we have

00:53:35.400 --> 00:53:36.960
"We're going to talk about how
applications will pick up this

00:53:37.020 --> 00:53:41.600
technology and we already talked
our applications division and

00:53:42.150 --> 00:53:45.100
"In the future on our stuff,
so there's a good opportunity that

00:53:45.100 --> 00:53:48.130
you can buy just a filter and have
a good audience of like people who

00:53:48.200 --> 00:53:49.700
want to use these filters later."

00:53:51.930 --> 00:53:56.680
One concept that is interesting to know
here is we have a non-executable filter.

00:53:56.680 --> 00:54:00.130
That means we just have a CI kernel,
that's all what UHDCD would

00:54:00.240 --> 00:54:03.530
provide in your plug-in,
and you don't have any

00:54:03.530 --> 00:54:05.450
CPU executable code.

00:54:05.890 --> 00:54:09.480
It's important when you talk about
security sensitive applications like

00:54:09.480 --> 00:54:14.220
some system servers or if you have like
something like the screen saver you want

00:54:14.220 --> 00:54:19.360
to pass around and those you definitely
don't want to have any kind of Trojan

00:54:19.360 --> 00:54:23.390
horses or viruses in it since they will
be executed without you even know it.

00:54:24.200 --> 00:54:26.360
So now let me talk a little
bit like where do we actually

00:54:26.360 --> 00:54:27.600
store these image units.

00:54:27.650 --> 00:54:30.700
Location is the key point here
and we have two spots where we

00:54:30.870 --> 00:54:32.600
would normally introduce them.

00:54:32.780 --> 00:54:35.700
And those are the graphics
plugins architecture,

00:54:35.730 --> 00:54:42.600
sorry, folders inside the system library
folder or the user library folder.

00:54:42.600 --> 00:54:45.640
That's where the custom
load API basically will

00:54:45.640 --> 00:54:47.600
look and find your filters.

00:54:47.730 --> 00:54:51.160
If your application has additional
filters that they just want

00:54:51.160 --> 00:54:53.970
to have in your own bundle,
we have an API that will

00:54:53.970 --> 00:54:57.100
load those units one by one.

00:54:57.100 --> 00:54:59.100
So you have to call those by yourself.

00:54:59.100 --> 00:55:01.490
And on that part,
that brings me over a little bit to

00:55:01.490 --> 00:55:03.330
the structure of our image units.

00:55:04.230 --> 00:55:08.100
And you can see I put up a little
screenshot like how it looks in Xcode.

00:55:08.100 --> 00:55:11.930
And as you can see in the very top part,
I have a little loader part.

00:55:12.390 --> 00:55:15.100
That's all like my Objective-C code.

00:55:15.100 --> 00:55:17.100
And then I stole Mark's filter.

00:55:17.100 --> 00:55:20.100
The Funhouse filter,
which has some Objective-C stuff in it.

00:55:20.100 --> 00:55:23.100
And then we see there's
in the resources part,

00:55:23.100 --> 00:55:27.100
that's where the in terms
of image units is right now.

00:55:27.210 --> 00:55:29.500
We have the CI kernels,
which as Mark explained are

00:55:29.500 --> 00:55:31.100
really the core of the filter.

00:55:31.100 --> 00:55:33.060
And we have a description P list.

00:55:33.130 --> 00:55:37.350
And that is the part which gives us
the part what is really inside this

00:55:37.350 --> 00:55:39.100
image unit and what do I get out of it.

00:55:39.100 --> 00:55:41.720
Especially important for the
non-executable ones because we

00:55:41.720 --> 00:55:45.280
have to communicate somehow,
okay, which are the parameters

00:55:45.280 --> 00:55:46.460
that I can pass in and out.

00:55:46.600 --> 00:55:50.600
And we provide also a way that you
can put in your localization into it.

00:55:50.600 --> 00:55:54.600
So you can provide your
filter for multiple languages.

00:55:55.730 --> 00:55:59.690
From there let me go to the API,
which you can see is really extensive.

00:55:59.690 --> 00:56:02.600
We have three calls that are
important for the loading.

00:56:02.620 --> 00:56:05.200
The first one will just load all filters.

00:56:05.200 --> 00:56:07.590
The second one will only
load the non-executable ones.

00:56:07.680 --> 00:56:10.820
So if you write an application where
you want to make sure that I cannot

00:56:10.830 --> 00:56:15.600
load any security sensitive plug-ins,
that will be the call to use.

00:56:15.600 --> 00:56:19.420
And the third one is the one
that you would use if you have

00:56:19.910 --> 00:56:23.680
your image units in a specific,
your own application bundle

00:56:23.680 --> 00:56:25.010
and you want to load those.

00:56:25.290 --> 00:56:27.390
So you would just go through
your folder and each of the

00:56:27.390 --> 00:56:32.070
bundles that you find there,
you would load them with this API call.

00:56:32.220 --> 00:56:35.790
On the other hand,
when we look into the plug-in side,

00:56:35.860 --> 00:56:45.670
we have a very simple call that we recall
on the primary class of the NSBundle,

00:56:45.670 --> 00:56:47.000
and that is the load call.

00:56:47.040 --> 00:56:49.350
And you can see this
actually returns a Boolean.

00:56:49.420 --> 00:56:52.580
So this is a place where you can,
for instance, do your registration,

00:56:52.580 --> 00:56:54.600
or you can check your
hardware requirements.

00:56:54.600 --> 00:56:57.790
You can say, "Oh, well,
that's a serial number that ends on a 3.

00:56:57.800 --> 00:57:01.000
I'm not running that machine."
So I can send you return false,

00:57:01.000 --> 00:57:03.370
and the filter will not be loaded.

00:57:03.570 --> 00:57:06.560
So then from there,
I would like to go to demo machine 2.

00:57:14.730 --> 00:57:18.060
Okay, what we see here is now again
the Xcode project and I just want

00:57:18.060 --> 00:57:19.610
to get quickly over the stuff.

00:57:20.220 --> 00:57:24.210
Since we've seen how the
Funhouse filter works,

00:57:24.280 --> 00:57:26.700
I won't go too much into details there.

00:57:26.880 --> 00:57:31.970
But I show you now an example of a
non-executable filter and this is

00:57:31.970 --> 00:57:34.690
my filter function that I have here.

00:57:34.690 --> 00:57:38.700
This is something like a pixelate filter,
just does a slight twist to it.

00:57:38.700 --> 00:57:42.700
And then at the end,
I have the description "plist".

00:57:42.810 --> 00:57:46.170
So the only part that's kind of
important to know about my little

00:57:46.230 --> 00:57:49.680
kernel is that I have like two
parameters that I need to pass in.

00:57:49.700 --> 00:57:53.150
So it takes an sampler,
which is my image,

00:57:53.150 --> 00:57:55.850
and I have a scale vector on it.

00:57:59.930 --> 00:58:03.760
So I open now the description
plist in our plist editor.

00:58:03.770 --> 00:58:05.910
It might be a little bit hard
hopefully for you to read,

00:58:06.000 --> 00:58:08.800
but I try to go with it so that
you understand what I'm doing here.

00:58:08.800 --> 00:58:11.770
In the description plist you
see that I have two filters.

00:58:11.790 --> 00:58:14.900
We have in the first part the
Funhouse Mirror filter and

00:58:14.970 --> 00:58:16.800
just my simple kernel filter.

00:58:16.800 --> 00:58:19.950
The important part are
then the attributes.

00:58:19.950 --> 00:58:22.800
And as Ralph mentioned in the beginning,
we have different categories.

00:58:22.840 --> 00:58:27.800
So I can see this is a distortion effect,
that's how I can detect it.

00:58:27.930 --> 00:58:32.240
It's suitable for video
and also for still images.

00:58:33.310 --> 00:58:35.170
This is my display name.

00:58:35.170 --> 00:58:37.440
Kind of pay attention that here
it's still my kernel filter.

00:58:37.440 --> 00:58:41.110
I'm actually using our localization
technique so that it will actually

00:58:41.110 --> 00:58:45.030
later on in the UI show up as a correct
and a little bit more nicer name.

00:58:45.170 --> 00:58:47.190
And then the important part
is actually the inputs.

00:58:47.220 --> 00:58:50.480
And I have two of them,
as I already mentioned in the beginning.

00:58:50.520 --> 00:58:53.770
One part is an image,
so I just give it a class and say, "Well,

00:58:53.770 --> 00:58:55.590
this is my input image."

00:58:56.550 --> 00:58:59.680
And the second part to it,
I have an NSNumber,

00:58:59.710 --> 00:59:02.280
which is the scale of my floating,
so I can just set some

00:59:02.280 --> 00:59:03.840
scale point up here.

00:59:03.860 --> 00:59:04.880
And that's pretty much all I need.

00:59:04.970 --> 00:59:07.950
So all what I do now is basically
I would have to build this bundle

00:59:07.960 --> 00:59:13.130
once and put it in the right location,
and now I can use it in my applications.

00:59:14.440 --> 00:59:18.630
And for this,
I go into the Quartz Composer.

00:59:18.740 --> 00:59:21.700
So right now what I set up
is just simply an image that

00:59:22.230 --> 00:59:23.300
will be shown on the screen.

00:59:23.300 --> 00:59:25.920
So this looks like this.

00:59:26.010 --> 00:59:27.990
And we have a surfer.

00:59:31.450 --> 00:59:37.200
And now, I can simply go in here and
look into my distortion effects.

00:59:37.230 --> 00:59:40.580
Look there, it's my kernel filter,
and you see it has a

00:59:40.580 --> 00:59:42.860
little bit more nicer name.

00:59:42.900 --> 00:59:47.460
And all that I will do now is simply
reroute the image through that filter.

00:59:50.330 --> 00:59:52.200
And run this thing again.

00:59:52.200 --> 00:59:56.530
And you see,
it looks like a little bit pixelated.

00:59:59.950 --> 01:00:02.200
And I can do the same thing
with the Funhouse filter,

01:00:02.200 --> 01:00:03.500
which we also have in here.

01:00:03.500 --> 01:00:09.810
And let me reroute that through here.

01:00:14.400 --> 01:00:17.400
And as you can clearly see
on the side of the image,

01:00:17.480 --> 01:00:18.840
there's a distortion.

01:00:18.860 --> 01:00:23.000
That is basically how easy it is to
create one and how to use it as well.

01:00:24.510 --> 01:00:26.370
And as no pixels were heard
in this demonstration,

01:00:26.400 --> 01:00:29.290
I would like to pass back to
Ralph to finish up our demonstration.

01:00:29.300 --> 01:00:31.370
Thank you.

01:00:40.930 --> 01:00:44.400
I said that no pixels
were hurt in this demo.

01:00:44.400 --> 01:00:45.390
That's not totally true.

01:00:45.400 --> 01:00:47.590
There were two.

01:00:47.640 --> 01:00:49.470
But they had it coming.

01:00:51.100 --> 01:00:57.780
Okay, so the key message—let me
get back to the first one.

01:00:57.780 --> 01:01:01.020
So what I'm going to do now
is give you some ideas what

01:01:01.020 --> 01:01:03.160
we could do with these things.

01:01:03.160 --> 01:01:06.790
I assume if you're in the business
of writing an application that

01:01:06.840 --> 01:01:10.610
deals with photographs or video,
you should have a good idea

01:01:10.610 --> 01:01:12.290
by now what to do with it.

01:01:12.510 --> 01:01:15.390
But there are actually
applications where the use of

01:01:15.390 --> 01:01:18.760
core images isn't totally obvious,
but could actually

01:01:18.760 --> 01:01:20.360
make a nice difference.

01:01:20.360 --> 01:01:24.540
So first thing I would like
to say here is the key message

01:01:24.750 --> 01:01:28.120
that in this millennium,
image processing is something that

01:01:28.120 --> 01:01:29.620
happens in the display pipeline.

01:01:30.070 --> 01:01:34.060
There is no separate render stage.

01:01:34.060 --> 01:01:37.940
Core image essentially does most
of the things in real time today.

01:01:37.940 --> 01:01:41.320
On next year's hardware,
most likely it does a very significant

01:01:41.320 --> 01:01:46.280
portion of what we would ever want
to do on an image in real time.

01:01:46.280 --> 01:01:49.620
So if you're building an application
that has—these are a bunch of things.

01:01:49.830 --> 01:01:52.390
These are a bunch of settings,
and then you press apply and

01:01:52.410 --> 01:01:56.100
it gets rendered into a bitmap,
and that's probably

01:01:56.100 --> 01:01:57.900
the wrong UI to pursue.

01:01:58.220 --> 01:02:04.200
Instead, try to make the application
respond in real time.

01:02:04.200 --> 01:02:08.420
So you have a slider,
and as the user moves the slider around,

01:02:08.510 --> 01:02:12.120
the image data is processed right away.

01:02:12.490 --> 01:02:15.390
And this has a bunch of
interesting implications.

01:02:15.400 --> 01:02:18.130
For example, if you have an undo buffer.

01:02:18.240 --> 01:02:22.470
In the last millennium,
undo buffers on images were

01:02:22.470 --> 01:02:24.390
really quite a science.

01:02:24.580 --> 01:02:29.400
You have to keep megabytes of data
around for each stage that you're doing.

01:02:29.400 --> 01:02:31.830
In this case, you no longer do that.

01:02:31.860 --> 01:02:35.520
You just have your filter
and a handful of parameters.

01:02:35.520 --> 01:02:38.400
And the only thing that the undo really
affects is these handful of parameters.

01:02:38.400 --> 01:02:41.300
So things like infinite undo
on an image is almost trivial.

01:02:41.400 --> 01:02:42.920
Trivial.

01:02:45.020 --> 01:02:48.020
So here's a bunch of ideas
what to do with it while you're

01:02:48.280 --> 01:02:52.760
processing photographs and video,
using it for transition effects,

01:02:52.760 --> 01:02:57.180
or work on creating a
richer user interface.

01:02:57.880 --> 01:03:01.660
We're running kind of late on time here,
so I'm going to switch

01:03:01.660 --> 01:03:02.860
to the demo right away.

01:03:02.860 --> 01:03:05.860
Demo machine two, please.

01:03:13.300 --> 01:03:18.480
Okay, so the first thing
I would like to demo...

01:03:19.300 --> 01:03:21.160
Which key is it?

01:03:21.560 --> 01:03:21.560
Here.

01:03:21.600 --> 01:03:25.080
Well, you saw Dashboard in the keynote.

01:03:25.240 --> 01:03:27.960
And when I take something
and drag it here,

01:03:27.960 --> 01:03:29.750
you see this little ripple effect.

01:03:29.810 --> 01:03:31.420
And that's Core Image at work.

01:03:31.540 --> 01:03:35.790
So this is an example how you
can put Core Image somewhere in a

01:03:35.790 --> 01:03:39.180
little piece of your application
and do something nice with it.

01:03:39.290 --> 01:03:45.160
So let's do that again,
just because it was so much work

01:03:45.160 --> 01:03:45.160
to make that ripple working.

01:03:46.700 --> 01:03:52.800
[Transcript missing]

01:03:53.760 --> 01:03:56.890
First one is a little toy.

01:03:57.770 --> 01:04:01.310
When I'm really bored in airplanes,
and that happens a lot,

01:04:02.000 --> 01:04:06.740
I take pictures out of the window
and unfortunately it looks like this.

01:04:06.750 --> 01:04:09.760
The key problem is there's just
so much air between you and your

01:04:09.760 --> 01:04:15.760
subject and it produces haze effect
and it looks completely washed out.

01:04:15.760 --> 01:04:19.980
So I tried to build a filter that
tries to correct some of this.

01:04:20.350 --> 01:04:24.370
So, this slider here tries to simulate
the distance to the ground

01:04:24.490 --> 01:04:26.480
at the bottom of the image.

01:04:26.480 --> 01:04:28.650
So I'll kind of move that
around and let's see,

01:04:28.650 --> 01:04:32.440
this is about the brown that you
would expect from the ground.

01:04:32.530 --> 01:04:35.400
And then this slider here,
the second one,

01:04:35.400 --> 01:04:36.910
is the distance at the top.

01:04:36.960 --> 01:04:40.080
So it assumes there is a
slanted plane you're looking at.

01:04:40.200 --> 01:04:42.680
So you can figure that around like this.

01:04:42.690 --> 01:04:46.180
And by the way,
this is Crater Lake in Oregon.

01:04:46.810 --> 01:04:52.900
So just to, where did I put it, here.

01:04:54.290 --> 01:04:57.810
So this is the before/after
shot essentially.

01:04:57.810 --> 01:05:02.810
And it's not exactly a particularly
problem that you encounter all the time.

01:05:02.880 --> 01:05:04.180
It's a very specific solution.

01:05:04.200 --> 01:05:07.070
I would probably never have
attempted it to do if it would

01:05:07.070 --> 01:05:08.920
have taken me a half a day of work.

01:05:08.940 --> 01:05:16.720
But with Core Image it was literally
40 minutes of work so I could,

01:05:16.720 --> 01:05:16.720
well,
not experiment a bit and try to do this.

01:05:17.640 --> 01:05:23.610
"Clearly, the white point isn't right,
so that mountain needs some adjustment."

01:05:25.310 --> 01:05:27.590
So, and by the way,
the source code for that and as well

01:05:27.650 --> 01:05:33.000
the source code of how to build an image
unit is available on the Apple website.

01:05:33.760 --> 01:05:36.940
Sursko is also available
for this example here.

01:05:36.940 --> 01:05:41.570
This is essentially doing nine different
transitions all at the same time.

01:05:42.390 --> 01:05:45.200
And why do I show that?

01:05:45.410 --> 01:05:49.260
This is an example of what new
types of UI this could enable.

01:05:49.630 --> 01:05:53.840
Imagine an application like Keynote,
which has a widget to select the

01:05:54.010 --> 01:05:56.280
transition between one slide to the next.

01:05:56.300 --> 01:06:01.040
Well, today that widget is a pop-up menu
and a little preview at the bottom.

01:06:01.390 --> 01:06:03.300
But with the power of
Core Image at your hands,

01:06:03.300 --> 01:06:06.210
you could actually go and say, "Well,
skip the pop-up,

01:06:06.300 --> 01:06:11.090
just show all transitions at the same
time." The user just clicks on one.

01:06:12.950 --> 01:06:16.880
This makes a more compelling UI because,
well, it's clearly more discoverable what

01:06:16.980 --> 01:06:19.620
kind of functionality is available.

01:06:19.660 --> 01:06:23.930
And I have to admit I was
cheating for this demo.

01:06:24.330 --> 01:06:28.420
This morning,
I found a bug in our GPU implementation

01:06:28.420 --> 01:06:29.910
with this particular case.

01:06:29.920 --> 01:06:42.500
So this is actually running
on the software . OK,

01:06:42.500 --> 01:06:45.410
with that, we go back to the slides.

01:06:55.500 --> 01:06:58.300
So, where to go from now?

01:06:58.320 --> 01:07:01.600
Well, tomorrow morning there's
the Graphics and Media Lab.

01:07:01.930 --> 01:07:04.010
My colleagues and I will be
there if you have questions,

01:07:04.180 --> 01:07:07.010
that's the right place to go.

01:07:07.100 --> 01:07:09.830
If you're interested in how to use
Core Image together with Video,

01:07:09.900 --> 01:07:12.830
then the new directions for
QuickTime Performance is

01:07:12.830 --> 01:07:14.140
the session to go to.

01:07:14.140 --> 01:07:20.200
You will learn about Core Video and
how to create pipe video

01:07:20.200 --> 01:07:20.200
frames through Core Image.

01:07:20.650 --> 01:07:23.980
And on Friday, there is the Discovering
Quartz Composer session.

01:07:23.980 --> 01:07:27.090
You saw it in Frank's section.

01:07:27.740 --> 01:07:32.310
Quartz Composer is a really great
tool to just wrap in a prototype,

01:07:32.450 --> 01:07:36.630
string a bunch of filters together,
figure out how things look,

01:07:36.630 --> 01:07:39.800
and if you build your own filters,
it will load them,

01:07:39.900 --> 01:07:43.210
so that's just a session to check out.

01:07:45.490 --> 01:08:09.140
For more information, well,
there's documentation available.

01:08:09.140 --> 01:08:09.140
Actually on the Tiger DVD,
there is the reference on

01:08:09.140 --> 01:08:09.140
Core Image and on the Apple website,
connect.apple.com,

01:08:09.140 --> 01:08:09.140
is the actual architecture
and fairly rich set of,

01:08:09.140 --> 01:08:09.140
all filters are in there with an image,
you know, before,

01:08:09.140 --> 01:08:09.140
after that explains what they're doing.

01:08:09.140 --> 01:08:09.140
That's a great way to start.