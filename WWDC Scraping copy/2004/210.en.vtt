WEBVTT

00:00:15.230 --> 00:00:16.660
Okay, so my name's John Stauffer.

00:00:16.740 --> 00:00:19.100
I manage the OpenGL Engineering group.

00:00:19.100 --> 00:00:22.650
So let's get right into it.

00:00:24.210 --> 00:00:29.090
So what we're going to talk
about today is a brief overview

00:00:29.100 --> 00:00:31.330
of what's new in OpenGL.

00:00:31.360 --> 00:00:34.210
This will be what we have
done differently or optimized

00:00:34.590 --> 00:00:37.560
since WWC last year.

00:00:38.040 --> 00:00:42.420
Got to give you a yearly update
as we go through this session.

00:00:42.470 --> 00:00:44.460
We're going to go into some basic tips.

00:00:44.500 --> 00:00:45.400
We try to keep this short.

00:00:45.480 --> 00:00:49.660
We want to get into the more advanced
optimization techniques for OpenGL.

00:00:49.720 --> 00:00:55.450
Then we're going to get into some detail
on optimizing OpenGL texture uploads.

00:00:55.720 --> 00:01:00.160
And then a new optimization
technique for asynchronously reading

00:01:00.240 --> 00:01:03.280
pixels back off the graphics card,
which is very important for people who

00:01:03.280 --> 00:01:06.420
are trying to get pixel data back off.

00:01:06.420 --> 00:01:09.020
Vertex data throughput,
how to optimize your vertex data uploads,

00:01:09.020 --> 00:01:12.890
making sure that you're
getting optimal performance,

00:01:12.910 --> 00:01:17.060
uploading your vertex data up to the GPU.

00:01:17.060 --> 00:01:18.340
One-shot images.

00:01:18.340 --> 00:01:19.640
Sometimes you have images that are small.

00:01:19.640 --> 00:01:21.020
Or aren't going to be reused.

00:01:21.020 --> 00:01:23.960
And you just want to draw
them once up to the screen.

00:01:23.960 --> 00:01:26.700
So we'll talk briefly
about how to optimize that.

00:01:26.700 --> 00:01:30.220
Pixel copy operations,
how to optimally copy pixels around.

00:01:30.220 --> 00:01:32.510
There's certain ways that
you can optimize that,

00:01:32.510 --> 00:01:35.190
making sure that they
are VRAM to VRAM copies.

00:01:35.280 --> 00:01:36.300
And using threads.

00:01:36.300 --> 00:01:38.260
There's a lot of people out
there trying to use threads.

00:01:38.260 --> 00:01:40.200
We see a lot of problems with that.

00:01:40.200 --> 00:01:43.480
Want to briefly cover that and make sure
that people understand the limitations

00:01:43.590 --> 00:01:46.210
and how to make that work optimally.

00:01:46.420 --> 00:01:48.560
So briefly, optimization strategies.

00:01:48.560 --> 00:01:50.980
There's two basic ways,
two basic things that

00:01:51.360 --> 00:01:52.280
people are trying to do.

00:01:52.460 --> 00:01:56.150
They're either trying to maximize
performance of their application,

00:01:56.270 --> 00:01:59.020
or they may be trying to
minimize the CPU burden.

00:01:59.020 --> 00:02:02.260
And depending on what your
application demands are,

00:02:02.260 --> 00:02:05.960
you'll want to focus on one of
those two types of strategies.

00:02:05.970 --> 00:02:10.610
And as you'll see in my presentation,
that effectively using the CPU can lead

00:02:10.610 --> 00:02:16.280
to greater optimizations than just simply
just offloading all of your applications.

00:02:16.300 --> 00:02:38.440
So there's two different
types of techniques.

00:02:38.440 --> 00:02:43.750
Don't be sure that you understand
that the CPU is still a very

00:02:43.750 --> 00:02:46.280
effective processing device
for getting performance.

00:02:46.300 --> 00:02:47.730
performance.

00:02:47.800 --> 00:04:07.900
[Transcript missing]

00:04:10.520 --> 00:04:12.400
Okay, so what's new?

00:04:12.440 --> 00:04:16.130
Well,
we've spent a year optimizing OpenGL.

00:04:16.220 --> 00:04:18.440
Some of the highlights are that
we've been focusing quite a bit

00:04:18.460 --> 00:04:19.480
on immediate mode performance.

00:04:19.510 --> 00:04:22.500
There's a number of applications
out there that have been ported

00:04:22.740 --> 00:04:26.340
that use immediate mode performance,
immediate mode drawing modes

00:04:26.480 --> 00:04:29.990
that make it a key optimization,
and we've been spending a lot of time

00:04:30.000 --> 00:04:33.380
on that to try to help those types of
applications coming over to the platform.

00:04:34.240 --> 00:04:37.210
Pixel transfer paths,
and what I mean by that is any

00:04:37.680 --> 00:04:41.540
kind of copying of pixel data,
RGBA, RGB data, what have you.

00:04:41.540 --> 00:04:43.790
We've been spending a lot of
time optimizing those paths

00:04:43.790 --> 00:04:45.270
and continuing to improve that.

00:04:45.340 --> 00:04:48.670
So if your application is
sending a lot of pixel data,

00:04:48.670 --> 00:04:50.740
we're working on those paths.

00:04:50.740 --> 00:04:56.290
Vertex program emulation for any
kind of application that needs to

00:04:56.380 --> 00:05:03.480
run across all of our CPUs and is
relying on the vertex program feature.

00:05:03.910 --> 00:05:08.400
We are continuing to improve the
emulation of vertex program on the CPU.

00:05:08.450 --> 00:05:11.960
So you can run a vertex program
on all of our platforms and get

00:05:11.960 --> 00:05:14.200
the best performance possible.

00:05:16.100 --> 00:05:18.700
Asynchronous texture downloads,
as I mentioned before.

00:05:18.700 --> 00:05:21.720
That is also a new feature
since the last time we talked,

00:05:21.820 --> 00:05:23.800
something that's very important.

00:05:23.930 --> 00:05:26.250
So as you can see,
we have a list of extensions

00:05:26.250 --> 00:05:29.000
that we've added since last year,
quite a few.

00:05:29.000 --> 00:05:30.850
We're continuing to
add features regularly,

00:05:30.920 --> 00:05:36.100
and as fast as they get approved and
made a part of the OpenGL standard,

00:05:36.160 --> 00:05:38.520
we will fold those into OpenGL.

00:05:39.900 --> 00:05:43.200
Okay, so some basic tips.

00:05:43.360 --> 00:05:46.670
Things that you don't want to do or
avoid is you want to avoid geoflushes.

00:05:46.670 --> 00:05:49.250
Geoflushes,
what those do is they truncate the

00:05:49.250 --> 00:05:53.800
command stream going to the processor
and flush that command up to the GPU.

00:05:53.800 --> 00:05:55.650
Now,
the reason you want to avoid flushes,

00:05:55.650 --> 00:05:57.750
one, is that it's a command,
it's a kernel trap,

00:05:57.750 --> 00:05:59.490
so you want to avoid that kernel trap.

00:05:59.510 --> 00:06:03.790
Secondly, you only have a limited
number of command buffers.

00:06:03.820 --> 00:06:06.030
So if you keep issuing
geoflushes back to back,

00:06:06.060 --> 00:06:09.170
you will run out of that resource,
and that will be a synchronization

00:06:09.620 --> 00:06:12.800
point where we will have to wait
for the graphics processor to finish

00:06:12.800 --> 00:06:16.640
processing those command buffers in
flight before we can get a free command

00:06:16.720 --> 00:06:17.790
buffer to start working with again.

00:06:17.800 --> 00:06:20.800
So we tell people just
to avoid geoflushes.

00:06:20.800 --> 00:06:22.910
Now, there are points,
there are times when

00:06:23.120 --> 00:06:25.570
you'll want to use those,
and we'll go into that

00:06:25.570 --> 00:06:26.800
a little bit later.

00:06:26.800 --> 00:06:28.650
I tell people never to use geofinish.

00:06:28.650 --> 00:06:30.780
Geofinish is a truly asynchronous call.

00:06:30.920 --> 00:06:32.800
What geofinish does is it submits
the command stream to the GPU.

00:06:32.800 --> 00:06:32.800
So if you keep issuing
geoflushes back to back,

00:06:32.800 --> 00:06:32.800
you will run out of that resource,
and that will be a synchronization

00:06:32.800 --> 00:06:32.800
point where we will have to wait
for the graphics processor to finish

00:06:32.800 --> 00:06:32.800
processing those command buffers in
flight before we can get a free command

00:06:32.800 --> 00:06:32.800
buffer to start working with again.

00:06:32.800 --> 00:06:33.800
So we tell people never
to use geoflushes.

00:06:33.800 --> 00:06:33.800
Now, there are points,
there are times when we'll

00:06:33.800 --> 00:06:33.800
want to use geofinish.

00:06:33.800 --> 00:06:36.670
It submits the current commands, stops,
waits for the graphics

00:06:36.740 --> 00:06:39.630
processor to be done with those
commands before it will return.

00:06:39.800 --> 00:06:44.120
So it is truly a serialized
synchronization point that will cause the

00:06:44.120 --> 00:06:46.800
CPU and GPU to stall against each other.

00:06:46.800 --> 00:06:49.800
So I tell people just
don't call that at all.

00:06:49.800 --> 00:06:52.800
Avoid GLREPIXELS if you can.

00:06:52.800 --> 00:06:56.780
You would want to use some of
our more modern ways of doing it.

00:06:56.800 --> 00:07:01.790
One of the techniques for replacing
GLREPIXELS is to use COPYPIXELS.

00:07:01.840 --> 00:07:06.800
And COPYPIXELS is useful for
getting VRAM to VRAM copies.

00:07:06.800 --> 00:07:09.150
So for instance, if you wanted to save
off some pixel data,

00:07:09.150 --> 00:07:12.610
some depth data, some stencil data,
instead of reading it across the bus,

00:07:12.610 --> 00:07:15.800
saving it with the CPU,
and then uploading it back,

00:07:15.820 --> 00:07:18.970
what you want to do is you want
to use COPYPIXELS to store it

00:07:18.970 --> 00:07:20.800
on the VRAM somewhere else.

00:07:20.800 --> 00:07:21.750
Don't read it across the bus.

00:07:21.890 --> 00:07:24.850
Save it somewhere in
another buffer up in VRAM,

00:07:24.860 --> 00:07:28.610
and that way you can get the
high bandwidth of copying it and

00:07:28.610 --> 00:07:30.800
restoring it when you need it.

00:07:30.800 --> 00:07:34.760
So texture downloads is also a
good way to replace GLREPIXELS,

00:07:34.910 --> 00:07:37.790
get an asynchronous
readback of your data,

00:07:37.790 --> 00:07:42.790
and not have a stall of waiting
for the read pixels to finish.

00:07:42.900 --> 00:07:46.540
So again, immediate mode performance,
we've been optimizing it,

00:07:46.550 --> 00:07:48.740
but it is still one of the slower paths.

00:07:48.840 --> 00:07:51.800
So we tell people when possible,
avoid immediate mode drawing.

00:07:52.010 --> 00:07:55.800
Instead,
use some of our more advanced extensions.

00:07:55.800 --> 00:07:58.800
Now, there's one exception to this,
and that is in display lists.

00:07:58.800 --> 00:07:59.800
If you use immediate mode
drawing in display lists,

00:07:59.800 --> 00:08:04.320
we will take that immediate mode data,
we will convert it into

00:08:04.320 --> 00:08:07.680
a more optimal form,
and prepare the data and

00:08:07.680 --> 00:08:10.790
then upload it into VRAM,
cache it in VRAM for you.

00:08:10.800 --> 00:08:14.800
So display list is an acceptable
place to use immediate mode,

00:08:14.800 --> 00:08:17.990
and it turns out that's fairly
convenient for a lot of people who

00:08:17.990 --> 00:08:20.800
already are using immediate mode,
but they realize their data is static.

00:08:20.800 --> 00:08:23.800
If your data is static,
you wrap a new list, end list around it.

00:08:23.920 --> 00:08:26.800
We will post-process the data
and stick it in video memory,

00:08:26.800 --> 00:08:28.760
and then you'll get the
benefit of that optimization.

00:08:28.890 --> 00:08:30.800
So minimize state changes.

00:08:30.800 --> 00:08:33.800
Most people that have been working
with Omnigil know this one.

00:08:33.800 --> 00:08:35.800
State changes are expensive.

00:08:35.800 --> 00:08:39.800
They do cause a revalidation
of the hardware state,

00:08:39.800 --> 00:08:41.800
which can be slow if you do it a lot.

00:08:41.800 --> 00:08:46.800
So avoid redundant state changes,
and do your drawing in groups of state.

00:08:46.800 --> 00:08:48.750
So what you want to do is you
want to coalesce your drawing

00:08:48.800 --> 00:08:50.870
under a given state setting,
which allows you to

00:08:50.870 --> 00:08:52.800
minimize your state changes.

00:08:55.100 --> 00:08:57.330
Okay, so let's get into more detail.

00:08:57.410 --> 00:08:58.580
Texture uploads.

00:08:58.580 --> 00:09:01.740
So what we're going to talk about
is we're going to talk about

00:09:01.740 --> 00:09:04.830
the texture pipeline overview,
give people a brief description

00:09:04.830 --> 00:09:06.510
of what the pipeline looks like.

00:09:06.610 --> 00:09:09.640
We're going to talk about some
of the optimization basics,

00:09:09.640 --> 00:09:11.640
and then we're going to get
into some of the extensions.

00:09:11.640 --> 00:09:15.700
The extensions can be different
depending on whether you're talking

00:09:15.700 --> 00:09:17.910
about a Power 2 or non-Power 2 texture.

00:09:18.000 --> 00:09:21.340
So we'll differentiate a little bit
between those two types of textures.

00:09:21.340 --> 00:09:25.690
For people who aren't familiar with that,
there are Power 2 textures,

00:09:25.730 --> 00:09:29.060
which is more standard OpenGL,
and recently, over the last few years,

00:09:29.120 --> 00:09:31.300
there's been the non-Power 2,
which allows you to have

00:09:31.300 --> 00:09:33.100
a texture of any size,
which is very useful

00:09:33.100 --> 00:09:39.020
for general image data,
video, pictures, what have you,

00:09:39.020 --> 00:09:41.390
will use non-Power 2.

00:09:42.630 --> 00:09:45.920
So here's a basic diagram
of the OpenGL pipeline.

00:09:45.980 --> 00:09:48.840
The part that we're going to focus
in on for this section of the talk,

00:09:48.910 --> 00:09:51.520
we're going to focus in
on the Pixel pipeline.

00:09:52.600 --> 00:09:56.240
And looking at just a block diagram
of what the pipeline looks like,

00:09:57.870 --> 00:10:02.010
standard OpenGL on Mac OS X,
you can end up with, at any time,

00:10:02.020 --> 00:10:05.300
if you're passing a
texture through the system,

00:10:05.300 --> 00:10:06.960
you can end up with
four copies of the data.

00:10:06.960 --> 00:10:10.360
So, obviously, that's a lot, right?

00:10:10.360 --> 00:10:11.860
You want to avoid those.

00:10:11.900 --> 00:10:14.810
So, what we're going to talk about is
we're going to talk about how to

00:10:14.820 --> 00:10:17.900
eliminate each one of those copies
and get you performance increases,

00:10:17.940 --> 00:10:19.300
obviously, as you do that.

00:10:19.560 --> 00:10:22.390
But in the default setting,
you can end up with four copies of your

00:10:22.390 --> 00:10:24.400
texture as it passes through the system.

00:10:24.400 --> 00:10:26.390
One copy is going to be
the copy that you have.

00:10:26.600 --> 00:10:28.360
One is going to be
what the framework has.

00:10:28.360 --> 00:10:31.660
One is going to be a copy
that the driver keeps.

00:10:31.660 --> 00:10:33.140
And then one is going
to be in video memory.

00:10:33.140 --> 00:10:38.830
So, let's get into some of
the ways to optimize that.

00:10:39.020 --> 00:10:42.560
So, again,
minimizing CPU copies is the key here.

00:10:42.560 --> 00:10:48.530
We don't want to give
the CPU redundant things.

00:10:49.560 --> 00:10:51.560
We want to optimize its time.

00:10:51.560 --> 00:10:54.060
So, correct setup will
minimize the CPU copies.

00:10:54.060 --> 00:10:58.920
And what we mean by that is that you're
going to use the right texture formats,

00:10:58.970 --> 00:11:02.920
the right pixel formats,
which will ensure optimal paths.

00:11:03.020 --> 00:11:07.600
It will also ensure that the graphics
processor accepts that data type.

00:11:07.700 --> 00:11:12.180
So, you know, OpenGL supports a very
large number of pixel types.

00:11:12.260 --> 00:11:17.390
And the graphics processors also
accept quite a few pixel types.

00:11:17.670 --> 00:11:19.460
But if you stay, if possible,
you want to stay in the middle,

00:11:19.600 --> 00:11:24.600
you want to stay on the confined set
such that you are guaranteed that the

00:11:24.600 --> 00:11:27.560
particular graphics processor you're
on has native support for that type.

00:11:27.570 --> 00:11:30.710
And it won't have to go through some
kind of conversion to a type that is

00:11:30.710 --> 00:11:32.560
compatible for that graphics processor.

00:11:32.560 --> 00:11:35.060
So, here I've got listed three types.

00:11:35.060 --> 00:11:40.560
BGRA and the 8888 reversed
and the 1555 reversed.

00:11:40.560 --> 00:11:43.880
Now,
those are the native Macintosh formats.

00:11:44.060 --> 00:11:47.720
So, when you set your monitor to
32-bit pixel mode or millions of

00:11:47.720 --> 00:11:50.600
pixels or thousands of pixels,
those are the pixel types

00:11:50.680 --> 00:11:52.060
that the screen is running in.

00:11:52.060 --> 00:11:54.060
And that will give you a compatible type.

00:11:54.060 --> 00:11:58.410
It also turns out that the graphics
processors understand that type natively.

00:11:58.840 --> 00:12:02.510
Also, you'll see a YUV type there for
people who are doing video or want,

00:12:02.550 --> 00:12:04.060
have a YUV source.

00:12:04.060 --> 00:12:07.560
They can use a YUV texture and
that will be accepted as well.

00:12:07.560 --> 00:12:10.790
So, when I usually put these up,
some people ask, "Well,

00:12:10.860 --> 00:12:14.560
what about RGBA?" Which is
the standard OpenGL type.

00:12:14.560 --> 00:12:18.050
RGBA isn't natively accepted
by all graphics processors.

00:12:18.560 --> 00:12:20.620
Sometimes it will have to go
through a copy and get swizzled

00:12:20.620 --> 00:12:21.560
into a different format.

00:12:21.560 --> 00:12:25.560
So, usually it's fairly optimal copy and
sometimes it might be natively supported.

00:12:25.560 --> 00:12:29.560
But, in general, you have to be a little
careful of that type.

00:12:32.230 --> 00:12:34.110
So let's talk about the extensions.

00:12:34.220 --> 00:12:37.140
So client storage is an Apple extension.

00:12:37.460 --> 00:12:43.080
That extension is a way to eliminate
the framework's copy of a texture.

00:12:43.200 --> 00:12:46.960
What it does is that instead of having
the framework make a copy of the texture,

00:12:47.000 --> 00:12:50.000
the framework instead keeps a pointer
of that texture into your memory.

00:12:50.000 --> 00:12:52.640
So if the application has
retained a copy of the texture,

00:12:52.650 --> 00:12:55.930
you can just tell us, use my copy,
don't make a copy for yourself.

00:12:56.190 --> 00:12:58.930
And that will eliminate one CPU copy,
that will eliminate the memory

00:12:58.940 --> 00:13:00.520
associated with keeping that copy.

00:13:00.520 --> 00:13:04.000
Apple Texture Range is
another Apple extension.

00:13:04.000 --> 00:13:06.880
This extension eliminates
the driver's copy.

00:13:06.880 --> 00:13:08.700
And there's two different
ways to drive this.

00:13:08.700 --> 00:13:10.000
It's cached and shared.

00:13:10.000 --> 00:13:13.520
What those mean is cached means
keep a copy of the driver.

00:13:13.520 --> 00:13:16.740
It's telling the driver to keep a
copy of the texture in video memory.

00:13:16.740 --> 00:13:20.170
Shared means simply point to
the copy in system memory when

00:13:20.170 --> 00:13:21.960
you're doing your drawing.

00:13:21.960 --> 00:13:24.320
And I'll get a little more
detail on this in a little bit.

00:13:24.470 --> 00:13:25.610
But keep those concepts in mind.

00:13:25.720 --> 00:13:26.520
They are important.

00:13:26.520 --> 00:13:30.460
So now,
EXT Texture Rectangle is an extension

00:13:30.460 --> 00:13:36.480
required by some hardware to allow
texture range to work properly.

00:13:36.480 --> 00:13:39.000
And the reason for that is
that some hardware requires

00:13:39.000 --> 00:13:42.080
the power of two textures to be
formatted in a certain format.

00:13:42.080 --> 00:13:46.670
So if you don't use that extension,
you're not necessarily going to

00:13:46.680 --> 00:13:51.310
be guaranteed on all graphics
processors to eliminate the graphics

00:13:51.310 --> 00:13:53.650
driver's copy of the texture.

00:13:53.660 --> 00:13:56.410
So keep in mind that Texture Rectangle
is a very important extension.

00:13:56.520 --> 00:14:00.850
And Texture Rectangles tend to be more
widely supported for eliminating a

00:14:00.850 --> 00:14:03.460
driver copy when using texture range.

00:14:03.680 --> 00:14:04.180
OK.

00:14:04.180 --> 00:14:06.580
So let's go back to the block diagram.

00:14:06.610 --> 00:14:08.580
So what we see is that Client Storage,
using that,

00:14:08.620 --> 00:14:11.580
we eliminate the framework copy,
as I said.

00:14:11.650 --> 00:14:13.820
And looking at a little bit of source,
it's fairly simple.

00:14:13.820 --> 00:14:20.220
All you do is you enable the
Client Storage option when

00:14:20.220 --> 00:14:22.120
you are building your texture.

00:14:22.120 --> 00:14:25.260
So before you load it,
just call PixelStoreI,

00:14:25.270 --> 00:14:26.520
enabling the Client Storage option.

00:14:26.520 --> 00:14:28.520
And that will eliminate
the framework copy.

00:14:28.520 --> 00:14:30.480
Do remember that when you do that,
that you are now

00:14:30.680 --> 00:14:32.010
responsible for the memory.

00:14:32.020 --> 00:14:33.680
So if you go and delete
your copy of the texture,

00:14:33.680 --> 00:14:34.900
the framework is pointing to that.

00:14:35.020 --> 00:14:39.090
And if you try to do something that
requires us to access that memory,

00:14:39.160 --> 00:14:40.260
you'll crash.

00:14:41.160 --> 00:14:43.620
Now,
looking at the Apple Texture Range and

00:14:43.630 --> 00:14:47.800
Apple Texture Rectangle extensions,
as I said before,

00:14:47.800 --> 00:14:49.700
this eliminates the driver's copy.

00:14:49.760 --> 00:14:52.560
And I'm showing a block diagram
here of it running in cached mode.

00:14:52.560 --> 00:14:56.190
So what happens is,
using these two extensions,

00:14:56.280 --> 00:15:00.240
the driver will be pointing directly
to the framework's copy and DMAing

00:15:00.240 --> 00:15:02.420
it directly from the framework's
copy up into video memory and

00:15:02.540 --> 00:15:04.760
keeping the copy in video memory.

00:15:04.880 --> 00:15:06.830
That's running in cached mode.

00:15:06.930 --> 00:15:09.740
Okay, and again,
here's a little code snippet

00:15:09.740 --> 00:15:12.000
showing how to use Texture Range.

00:15:12.070 --> 00:15:13.440
Very simple, one call.

00:15:13.580 --> 00:15:17.800
Text param i,
texture rectangle extension, target type,

00:15:17.890 --> 00:15:22.950
storage hint apple,
and then cached apple for the hint type.

00:15:23.420 --> 00:15:27.100
Now, using all these together,
what we get is we get that the

00:15:27.130 --> 00:15:31.210
graphics processor now is going to
be pointing the GPU directly to the

00:15:31.500 --> 00:15:35.890
application's copy of the memory,
and it's going to be DMing it

00:15:35.890 --> 00:15:36.940
directly into video memory.

00:15:36.940 --> 00:15:40.520
So what you get is you've eliminated
the CPU actually making a copy.

00:15:40.520 --> 00:15:42.950
We point directly to
your copy of the texture,

00:15:42.990 --> 00:15:44.000
DM it directly.

00:15:44.000 --> 00:15:46.080
So the CPU never actually
makes a pixel-to-pixel copy.

00:15:46.640 --> 00:15:49.600
The graphics processor is DMing
it directly into video memory.

00:15:49.600 --> 00:15:52.530
And looking at those
code snippets together,

00:15:52.530 --> 00:15:54.400
it simply looks like this.

00:15:54.400 --> 00:15:57.330
It adds basically two calls to this.

00:15:57.340 --> 00:15:59.520
You'll see that I'm using
a texture rectangle type.

00:15:59.650 --> 00:16:04.180
I've inserted the two previous code
snippets between the bind and the text

00:16:04.180 --> 00:16:09.160
image 2D call so that I'm getting the
direct DMA transfer as I just showed.

00:16:09.160 --> 00:16:13.000
Now, switching gears a little bit and
looking at the shared option.

00:16:13.000 --> 00:16:15.920
Now, the shared option, as I said before,
makes it such that we are not

00:16:15.920 --> 00:16:16.620
going to cache a copy of the GPU.

00:16:16.630 --> 00:16:20.270
Instead, what we're going to do is we are
going to set it up such that the

00:16:20.440 --> 00:16:23.860
graphics processor is going to look
up the textiles at rasterization

00:16:23.890 --> 00:16:26.040
time directly from system memory.

00:16:26.040 --> 00:16:28.820
So while it's drawing the polygon,
each time it goes to

00:16:28.820 --> 00:16:31.540
have to fetch a textile,
it'll go across the AGP bus,

00:16:31.540 --> 00:16:33.400
look it up out of system memory.

00:16:33.400 --> 00:16:36.840
So that eliminates the copy
that's in video memory.

00:16:36.840 --> 00:16:40.960
And there are some uses for that,
and I will show a demo of that shortly.

00:16:40.960 --> 00:16:43.370
So here's what the code looks like.

00:16:43.480 --> 00:16:45.720
It's the same thing as the cached.

00:16:46.670 --> 00:16:50.240
Text param i, except for you would pass,
instead of the storage cached apple,

00:16:50.300 --> 00:16:54.270
you would pass the storage shared apple,
hint, type, and that will mean don't

00:16:54.270 --> 00:16:55.970
cache a copy in video memory.

00:16:55.980 --> 00:16:57.150
Okay.

00:16:57.620 --> 00:17:00.810
So, looking at the block diagram again.

00:17:01.030 --> 00:17:03.160
So, this is what it looks like
when you run in shared mode.

00:17:03.160 --> 00:17:04.540
There's no copy in video memory.

00:17:04.540 --> 00:17:07.770
As you're rasterizing,
it is looking the textiles up directly

00:17:07.860 --> 00:17:10.550
from the application's memory,
and you end up with one copy,

00:17:10.620 --> 00:17:11.960
your copy in the application.

00:17:11.960 --> 00:17:16.290
There's times when this is useful,
times when it's better to use cached.

00:17:17.900 --> 00:17:19.760
Okay, so same code snippet.

00:17:19.780 --> 00:17:22.140
All I had to do to get the
shared option in there,

00:17:22.190 --> 00:17:25.340
again, is change the cache to shared.

00:17:25.390 --> 00:17:26.250
I'm going kind of fast.

00:17:26.530 --> 00:17:30.470
So what I should point out is
that this example is actually

00:17:30.510 --> 00:17:31.780
available on the website.

00:17:31.840 --> 00:17:34.300
You'll see at the bottom that
there is the sample name.

00:17:34.390 --> 00:17:37.270
You can look this up,
and it has this code in there,

00:17:37.270 --> 00:17:39.590
so you don't need to
be copying this down.

00:17:42.470 --> 00:17:44.510
Okay, let's talk a little bit
about cache and shared,

00:17:44.740 --> 00:17:46.320
when one is appropriate versus the other.

00:17:46.320 --> 00:17:49.640
So cache mode is better for textures when
you're going to use it multiple times.

00:17:49.680 --> 00:17:53.440
You don't want to be reading a texture
across the AGP bus multiple times.

00:17:53.460 --> 00:17:56.470
So if you're going to use it a lot,
you're going to want to cache it

00:17:56.470 --> 00:17:59.430
in video memory and then use it
from there multiple times and not

00:17:59.430 --> 00:18:01.200
require it to go across the bus.

00:18:01.200 --> 00:18:04.390
So it's also best when you're
using linear filtering.

00:18:04.400 --> 00:18:07.040
Linear filtering is a little bit
higher bandwidth usage because

00:18:07.050 --> 00:18:10.030
it's having to look up neighboring
textiles to do the linear filtering.

00:18:10.200 --> 00:18:14.190
And now shared is -- talking
about shared for a second.

00:18:14.210 --> 00:18:17.200
So shared is better for one-shot
images that are very large.

00:18:17.200 --> 00:18:21.240
And the reason that I say very
large is that if you have low

00:18:21.740 --> 00:18:25.200
video memory cases and if you
wanted to upload a large texture,

00:18:25.200 --> 00:18:27.860
what you don't want to have
happen is that texture to,

00:18:27.860 --> 00:18:29.200
say, evict everything else
out of video memory.

00:18:29.200 --> 00:18:29.470
So if you're running in
a low video memory case,

00:18:29.470 --> 00:18:29.810
you're going to want to have a
texture that's going to be able

00:18:29.810 --> 00:18:30.200
to run in a low video memory case.

00:18:30.200 --> 00:18:30.200
So if you're running in
a low video memory case,

00:18:30.200 --> 00:18:30.200
you're going to want to have a
texture that's going to be able

00:18:30.200 --> 00:18:30.200
to run in a low video memory case.

00:18:30.200 --> 00:18:31.670
you're running in a
low video memory case,

00:18:31.670 --> 00:18:35.730
it is possibly a benefit to run
in shared mode where you're not

00:18:35.880 --> 00:18:37.320
going to consume any video memory.

00:18:37.320 --> 00:18:40.130
You can leave what's resonant
in video memory there and just

00:18:40.190 --> 00:18:44.080
look up your image straight,
DM8, as you're rasterizing as opposed

00:18:44.080 --> 00:18:45.450
to DM8 into video memory.

00:18:45.660 --> 00:18:48.590
There are some caveats to that,
and that is that the shared mode runs

00:18:48.590 --> 00:18:50.420
best when running in nearest filtering.

00:18:50.420 --> 00:18:53.730
If you're running in linear filtering,
again, as I said, as it's rasterizing,

00:18:53.740 --> 00:18:55.550
it's going to be looking
up those textiles.

00:18:55.650 --> 00:18:58.960
Well, if you're running in linear filter,
linear filtering requires neighboring

00:18:58.960 --> 00:19:05.140
textiles and it'll fetch more textiles
from the neighboring part of the texture,

00:19:05.140 --> 00:19:08.340
which will cause you
some performance degrade.

00:19:08.380 --> 00:19:12.230
As well, shared works really well when
you're scaling the image down.

00:19:12.330 --> 00:19:15.180
For the same reason I just said,
if you're scaling it down,

00:19:15.180 --> 00:19:17.630
it's not going to have to pull
all the pixels across a bus.

00:19:20.610 --> 00:19:24.110
So for Power2, briefly talk about that.

00:19:24.390 --> 00:19:27.600
All the same extensions I just talked
about are actually applicable to Power2.

00:19:27.600 --> 00:19:31.570
This is the same code snippet I had,
and all I did was replace the

00:19:31.570 --> 00:19:35.770
rectangle texture option with
the texture 2D enumerant type.

00:19:35.800 --> 00:19:39.720
And the difference here is that,
as I mentioned before,

00:19:39.940 --> 00:19:43.160
Power2 sometimes won't
get you a direct DMA.

00:19:43.200 --> 00:19:46.000
So not all graphics
processors support direct DMA,

00:19:46.000 --> 00:19:49.200
and instead what will happen
is the driver will make a copy.

00:19:49.200 --> 00:19:52.000
You can still use the options,
all the same extensions

00:19:52.000 --> 00:19:54.540
I've been talking about,
but sometimes you won't

00:19:54.540 --> 00:19:55.790
get the direct DMA.

00:19:56.700 --> 00:20:01.460
Okay, let's talk about how to
manage texture range.

00:20:01.500 --> 00:20:05.150
Now, as we saw in the diagram,
the graphics processor is going to be

00:20:05.150 --> 00:20:07.600
starting to look directly at your memory.

00:20:07.600 --> 00:20:09.340
Okay,
so the graphics processor and CPU now

00:20:09.340 --> 00:20:12.200
are going to be sharing the same
piece of memory as it's rasterizing.

00:20:12.200 --> 00:20:15.200
Now, there's a problem with that,
and that is that you are now

00:20:15.630 --> 00:20:18.600
going to have to synchronize the
CPU and the graphics processor

00:20:18.610 --> 00:20:20.480
such that they don't collide.

00:20:20.480 --> 00:20:22.430
You can't have the CPU and
the GPU reading the same piece

00:20:22.460 --> 00:20:23.820
of data at the same time,
right?

00:20:23.820 --> 00:20:26.580
It's a standard problem when
you have multiple devices.

00:20:26.600 --> 00:20:28.310
It's looking at the same piece of data.

00:20:28.310 --> 00:20:30.940
So what you want to do is you're
going to have to double buffer it,

00:20:31.150 --> 00:20:34.160
and between double buffering,
you're going to have to issue a flush.

00:20:34.160 --> 00:20:35.690
So I've got a diagram here.

00:20:35.690 --> 00:20:36.710
I'll show this.

00:20:36.710 --> 00:20:40.170
So if you're running single buffer mode,
and you have the CPU just

00:20:40.170 --> 00:20:43.410
generated texture,
let's say read it in or decompressed it,

00:20:43.410 --> 00:20:47.270
and now the CPU is going to want to
flush that up to the graphics processor,

00:20:47.310 --> 00:20:51.100
so it issues a geo-flush to get that
command in flight and get the transfer

00:20:51.100 --> 00:20:54.730
of that data up into video memory,
and then the graphics processor is

00:20:54.730 --> 00:20:58.160
going to do its work of processing
it and swapping it to the screen.

00:20:58.160 --> 00:20:59.680
So there you just did one frame, right?

00:20:59.680 --> 00:21:02.880
And when single buffered,
I have to synchronize my CPU and

00:21:02.950 --> 00:21:06.610
GPU serializing the processing
because I only have one data set.

00:21:06.610 --> 00:21:08.640
Only one can work on it at a time.

00:21:08.640 --> 00:21:12.310
So as we build through this,
this is how the frames go.

00:21:12.420 --> 00:21:15.940
I can only have the CPU and
GPU working one at a time.

00:21:15.940 --> 00:21:19.060
Now, if I go with double buffering,
let's see what happens.

00:21:19.080 --> 00:21:21.140
So let's say we start this sequence.

00:21:21.140 --> 00:21:22.440
CPU generated a frame.

00:21:22.580 --> 00:21:23.430
It flushes it.

00:21:23.490 --> 00:21:26.580
Now, in the next frame,
I can see that if I had double buffering,

00:21:26.600 --> 00:21:29.190
the CPU can start working on
the second texture while the

00:21:29.190 --> 00:21:31.100
GPU is processing the first,
right?

00:21:31.100 --> 00:21:34.280
So now we can flush the second
one and swap the first one.

00:21:34.280 --> 00:21:37.700
So I just showed one frame while
I'm submitting the next one to the

00:21:37.830 --> 00:21:42.620
graphics processor for processing,
and likewise, it continues on, right?

00:21:42.620 --> 00:21:45.190
Now, the CPU can start working
back on texture one,

00:21:45.440 --> 00:21:48.990
and the GPU is working on texture two,
and so on and so on.

00:21:48.990 --> 00:21:53.300
And basically, if you had, you know,
this is an exaggeration where

00:21:53.300 --> 00:21:56.580
you have perfect parallelization,
but it does make a difference.

00:21:56.600 --> 00:22:00.680
It does make a difference where
you are getting asynchronous

00:22:00.680 --> 00:22:02.180
behavior between the CPU and GPU.

00:22:10.850 --> 00:22:11.040
Okay.

00:22:11.040 --> 00:22:13.700
Fence.

00:22:13.700 --> 00:22:15.280
So let's talk about how to synchronize.

00:22:15.300 --> 00:22:20.020
How do we synchronize the two commands?

00:22:20.020 --> 00:22:22.010
How do I know when the GPU is
done processing my data,

00:22:22.010 --> 00:22:22.700
for instance?

00:22:22.700 --> 00:22:24.540
So it's pointing at my data set.

00:22:24.540 --> 00:22:27.380
How do I know when it's
done accessing that data?

00:22:27.380 --> 00:22:30.160
Well, if you're using texture range,
what you need to use is

00:22:30.160 --> 00:22:31.620
the Apple Fence extension.

00:22:31.820 --> 00:22:34.890
And the Apple Fence extension,
what you can do is you insert a

00:22:34.890 --> 00:22:37.720
token into the command stream,
and you can query for the

00:22:37.720 --> 00:22:40.680
token to determine when it
is done reading your texture,

00:22:40.680 --> 00:22:42.950
so that you'll know that it
is now safe for the CPU to

00:22:42.970 --> 00:22:44.560
start touching the data again.

00:22:44.560 --> 00:22:46.490
So there's a couple ways to do that.

00:22:46.490 --> 00:22:49.680
You can use it by inserting a token,
or you can actually use it

00:22:49.800 --> 00:22:52.970
straight by accessing or
referencing a texture object.

00:22:53.090 --> 00:22:56.300
And a texture object is just
your standard texture ID,

00:22:56.310 --> 00:23:00.220
and what you'll do is in the
fenced object test object command,

00:23:00.220 --> 00:23:01.490
you just send it the texture.

00:23:01.850 --> 00:23:05.390
the geotexture object type.

00:23:05.570 --> 00:23:07.440
So looking at a little
bit of code for that.

00:23:07.660 --> 00:23:10.290
So the first two commands here
talk about how to set up a fence.

00:23:10.380 --> 00:23:12.580
So you just do a GL set fence apple.

00:23:12.580 --> 00:23:14.540
You can pass it any name you want,
just a token that you pass

00:23:14.540 --> 00:23:15.520
into the command stream.

00:23:15.520 --> 00:23:19.740
And then when you are ready to start
touching the texture again with the CPU,

00:23:19.740 --> 00:23:22.800
you would then test to make
sure that the GPU is done.

00:23:22.800 --> 00:23:25.710
And that's a synchronization point
where the CPU will wait for the

00:23:25.780 --> 00:23:28.680
GPU to be done reading that data,
and at which point then you can

00:23:28.680 --> 00:23:30.520
start touching it again with CPU.

00:23:30.520 --> 00:23:35.350
Now the last command up on this screen
is the way that you could use it,

00:23:35.630 --> 00:23:36.880
the fence extension.

00:23:37.300 --> 00:23:41.440
Without having to set a fence explicitly,
you can just test for a texture object.

00:23:41.720 --> 00:23:46.260
And all you would do is just call the
finished object apple with the GL texture

00:23:46.260 --> 00:23:49.000
target type and the name of the texture.

00:23:49.000 --> 00:23:51.880
So if you bound to a texture,
you would just test against

00:23:51.880 --> 00:23:53.330
that same texture ID number.

00:23:53.500 --> 00:23:56.000
Okay.

00:23:56.000 --> 00:23:59.960
So we're going to switch
to demo machine two.

00:24:01.910 --> 00:24:05.360
So what I wanted to show here
is an example of texture range.

00:24:05.570 --> 00:24:09.360
So you'll notice that the
CPU is doing very little here.

00:24:09.510 --> 00:24:12.670
First thing I wanted to point out
very quickly is that the CPU is

00:24:12.730 --> 00:24:14.640
continuing to do very little.

00:24:14.680 --> 00:24:18.340
And that's a sign that the CPU is
not making copies of the pixels,

00:24:18.420 --> 00:24:18.760
right?

00:24:18.760 --> 00:24:21.100
Because if the CPU was
copying the pixels,

00:24:21.140 --> 00:24:23.790
I would be seeing a big
spike in CPU processing time.

00:24:24.140 --> 00:24:27.360
Instead, what's happening is the graphics
processor is talking directly

00:24:27.360 --> 00:24:30.070
to the memory controller,
getting a direct DMA.

00:24:30.080 --> 00:24:32.610
So the data is not going through the CPU.

00:24:32.720 --> 00:24:35.230
So now I'm going to turn
on my Infinite button.

00:24:35.400 --> 00:24:38.620
It's a button I recommend
everyone put on their app.

00:24:38.620 --> 00:24:41.180
Make it go infinitely fast.

00:24:41.380 --> 00:24:43.710
So even though now I'm
doing 240 frames a second,

00:24:43.740 --> 00:24:47.010
I'm getting a gigabyte a
second across the AGP bus,

00:24:47.070 --> 00:24:48.340
I still have no CPU time.

00:24:48.410 --> 00:24:50.520
Because again,
the CPU is not doing anything here.

00:24:50.520 --> 00:24:53.800
The CPU is orchestrating this
and not copying the data.

00:24:53.930 --> 00:24:57.680
It's simply directing the traffic,
as you might think of it that way.

00:24:57.730 --> 00:24:59.440
Now,
what I really wanted to show with this

00:24:59.440 --> 00:25:02.090
example is using the shared option.

00:25:02.220 --> 00:25:05.510
So as I said before,
the cached option is good

00:25:05.600 --> 00:25:06.770
for drawing multiple times.

00:25:06.780 --> 00:25:08.980
But the shared option is good
when you shrink an image down.

00:25:08.980 --> 00:25:11.480
Now, it turns out this image
actually is 1024 by 1024.

00:25:11.480 --> 00:25:13.100
It's a lot larger than my window.

00:25:13.330 --> 00:25:16.810
So if I switch to shared mode,
now you'll see I'm getting

00:25:16.810 --> 00:25:18.090
three gigabytes a second.

00:25:18.170 --> 00:25:21.300
Well, that's not even possible.

00:25:21.330 --> 00:25:22.760
The application thinks it is, though.

00:25:22.760 --> 00:25:25.550
And the reason is that
I've shrunk the image.

00:25:25.580 --> 00:25:26.780
It's nearest to filtering.

00:25:26.820 --> 00:25:29.630
Some of the textiles of that image
aren't actually going across the AGP bus.

00:25:29.640 --> 00:25:33.700
Because the graphics processor is
skipping scan lines and skipping

00:25:33.740 --> 00:25:36.760
pixels and only selecting the
ones it needs to draw the image.

00:25:36.980 --> 00:25:42.000
So now I'm getting 700 frames a second,
as opposed to one gigabyte a

00:25:42.150 --> 00:25:44.900
second and 240 frames a second.

00:25:44.930 --> 00:25:46.700
So quite a boost.

00:25:46.700 --> 00:25:49.260
It doesn't take any video memory,
because I'm not caching

00:25:49.260 --> 00:25:50.770
and copying video memory.

00:25:50.780 --> 00:25:53.130
I'm reading it directly across the bus.

00:25:53.200 --> 00:25:59.420
So there's times when this type
of technique can be a large win.

00:25:59.420 --> 00:26:01.470
I'm going to switch back slides.

00:26:05.600 --> 00:26:07.850
Okay, asynchronous texture downloads.

00:26:07.850 --> 00:26:10.610
Let's talk about how to
set up texture range.

00:26:10.610 --> 00:26:11.300
I'm sorry.

00:26:11.330 --> 00:26:14.840
Asynchronous texture downloads
are basically the same thing

00:26:14.840 --> 00:26:17.860
as uploading a texture,
where we just talked about,

00:26:17.870 --> 00:26:21.320
where you set up a texture as
an AGP texture for direct DMing.

00:26:21.320 --> 00:26:24.270
Asynchronous texture
downloads is the same setup,

00:26:24.270 --> 00:26:25.270
but in reverse.

00:26:25.270 --> 00:26:29.300
So you set up the texture the same way,
and then you use copy text subimage

00:26:29.360 --> 00:26:32.040
to copy the data into that texture.

00:26:32.040 --> 00:26:37.000
So the way it works is that the copy
text subimage is the call that initiates

00:26:37.000 --> 00:26:40.270
the transfer from video memory back
into your texture and system memory.

00:26:40.280 --> 00:26:41.210
The reverse.

00:26:41.210 --> 00:26:46.140
And that is an autonomous call
that will happen asynchronously.

00:26:46.210 --> 00:26:49.120
So the next time you issue a flush,
there will be a copy text

00:26:49.120 --> 00:26:52.000
subimage call in there,
and the flush will issue a copy,

00:26:52.000 --> 00:26:54.980
a DMA transfer from video
memory into system memory.

00:26:54.980 --> 00:26:55.450
Okay?

00:26:55.630 --> 00:26:56.610
And that's autonomous.

00:26:56.690 --> 00:26:58.500
The CPU doesn't need to wait
for that event to happen.

00:26:58.580 --> 00:26:58.580
Now, there needs to be a
synchronization point,

00:26:58.580 --> 00:26:58.580
because the CPU needs a new system.

00:26:58.610 --> 00:27:01.180
Now, there needs to be a
synchronization point,

00:27:01.180 --> 00:27:02.680
because the CPU needs to
know when that's done.

00:27:02.680 --> 00:27:06.130
So what you use is you use,
at some later point,

00:27:06.140 --> 00:27:11.280
you use the get text image call,
and that's a synchronization point that

00:27:11.280 --> 00:27:13.180
will wait until the transfer is done.

00:27:13.180 --> 00:27:16.090
Now, hopefully you've done enough
processing between your copy text

00:27:16.150 --> 00:27:18.800
subimage and your get text image,
where the transfer is done,

00:27:18.800 --> 00:27:20.530
and you don't have to stall and wait.

00:27:20.670 --> 00:27:23.400
So the idea here is that you
separate those as far as you can,

00:27:23.400 --> 00:27:25.720
maybe double buffer them,
triple buffer them,

00:27:25.800 --> 00:27:27.710
do some processing between those.

00:27:28.580 --> 00:27:32.200
So the basic setup of those is that,
again, it's the same.

00:27:32.200 --> 00:27:35.520
The get text image will take the
same pointer as you originally

00:27:35.520 --> 00:27:38.280
passed it for the texture,
and the parameters must match

00:27:38.280 --> 00:27:40.100
the setup of the texture.

00:27:40.100 --> 00:27:42.570
So however you set up the texture,
those same parameters will be

00:27:42.570 --> 00:27:43.800
used in these calls as well.

00:27:43.800 --> 00:27:46.020
And, again,
you do this as late as possible to

00:27:46.020 --> 00:27:48.280
get the maximum asynchronous behavior.

00:27:48.280 --> 00:27:50.270
So let's look at a little bit of code.

00:27:50.430 --> 00:27:52.940
The setup, you'll notice,
the asynchronous setup is the same

00:27:52.940 --> 00:27:54.340
as it is for a texture upload.

00:27:54.340 --> 00:27:56.420
It's exactly the same.

00:27:58.870 --> 00:28:01.440
Now,
the download is the key part of this.

00:28:01.490 --> 00:28:03.800
And all you have to do,
the two key calls,

00:28:03.800 --> 00:28:06.670
are the copy text subimage
and the get text image.

00:28:06.800 --> 00:28:10.800
If you issue those calls on
a properly set up texture,

00:28:10.800 --> 00:28:12.800
you'll get an asynchronous download.

00:28:12.800 --> 00:28:16.050
And on my systems at work,
I can get about 500 megabytes

00:28:16.050 --> 00:28:19.100
a second download performance,
which is usually pretty

00:28:19.260 --> 00:28:22.710
acceptable for most people,
particularly considering that

00:28:22.710 --> 00:28:24.910
can be an asynchronous operation.

00:28:27.610 --> 00:28:29.030
Okay, let's talk about vertex data.

00:28:29.160 --> 00:28:35.720
So you'll notice in this part of the talk
that vertex data setup and optimizations

00:28:35.740 --> 00:28:38.500
is about the same as texture,
it's just a different data type.

00:28:38.500 --> 00:28:42.210
So a lot of the parts of
the discussion are the same.

00:28:42.380 --> 00:28:45.110
And all we're going to do is walk
you through and point out some of the

00:28:45.110 --> 00:28:47.030
differences and peculiarities of this.

00:28:47.040 --> 00:28:49.130
So we're going to go
through a pipeline overview,

00:28:49.130 --> 00:28:50.900
we're going to talk
about the basics again,

00:28:50.900 --> 00:28:54.020
and then we're going to
talk about the extensions.

00:28:54.020 --> 00:28:57.880
Now, we're going to separate
dynamic and static data,

00:28:57.880 --> 00:28:59.920
talk about some differences,
and get a little bit more

00:28:59.920 --> 00:29:01.030
detail on display lists.

00:29:01.040 --> 00:29:04.520
So in this part of the talk,
we're going to point out the

00:29:04.900 --> 00:29:09.040
geometry part of the pipeline,
not the pixel part of the pipeline.

00:29:09.040 --> 00:29:12.040
And let's talk about some basics.

00:29:12.040 --> 00:29:14.070
So the first thing is that
you want to pick data types,

00:29:14.070 --> 00:29:16.000
like in the pixel talk,
you want to pick data types

00:29:16.080 --> 00:29:18.120
that are most optimal.

00:29:18.250 --> 00:29:20.700
And the most optimal for
the vertex paths are floats,

00:29:20.700 --> 00:29:22.690
shorts, and unsigned bytes.

00:29:22.690 --> 00:29:24.630
If possible, stick with those types.

00:29:24.700 --> 00:29:27.570
Most graphics processors will
handle those types natively,

00:29:27.720 --> 00:29:30.700
so you will be able to get
optimal upload performance.

00:29:30.880 --> 00:29:33.690
And in some of the cases where
the CPU might be making a copy,

00:29:33.800 --> 00:29:36.130
we've spent time optimizing those paths.

00:29:36.190 --> 00:29:38.700
So these are the ones that will
give you optimal performance.

00:29:38.700 --> 00:29:43.750
The other basic point of optimizing
vertex upload performance is that you

00:29:43.750 --> 00:29:46.360
want to avoid function call overhead.

00:29:46.360 --> 00:29:49.330
Now, obviously, immediate mode,
where you're sending one

00:29:49.450 --> 00:29:51.520
vector of data per call,
is pretty inefficient as

00:29:51.520 --> 00:29:52.860
far as a copy routine.

00:29:52.860 --> 00:29:56.360
What you want to be able to do
is use a vertex array calls,

00:29:56.360 --> 00:29:58.470
draw arrays,
draw elements to get the data

00:29:58.610 --> 00:30:02.360
through the system with minimal
function calls as possible.

00:30:02.770 --> 00:30:05.350
Another good technique
is to use CGL macros.

00:30:05.350 --> 00:30:08.360
CGL macros is a way to directly
reduce function call overhead.

00:30:08.360 --> 00:30:10.360
And I'll show an example of that.

00:30:10.360 --> 00:30:14.870
It can be pretty dramatic how efficient
you can make the function calls when

00:30:15.100 --> 00:30:16.020
you start using the function parts.

00:30:16.040 --> 00:30:20.020
So you can see that the function points
are in the top-level library interpoints.

00:30:20.020 --> 00:30:27.680
So that's a concept that people may want
to keep in mind if they are seeing or if

00:30:28.030 --> 00:30:31.900
they're making a lot of function calls.

00:30:32.090 --> 00:30:37.680
So another key concept is if you are
-- when you're drawing vertex data,

00:30:37.730 --> 00:30:42.930
passing vertex data into OpenGL,
you want to maximize the number

00:30:42.930 --> 00:30:45.670
of vertices per draw command.

00:30:45.680 --> 00:30:45.680
So if you're using arrays,
you want to maximize the number

00:30:45.680 --> 00:30:45.680
of vertices per draw command.

00:30:45.680 --> 00:30:48.000
So if you're using arrays,
you want to maximize the number

00:30:48.000 --> 00:30:49.240
of vertices per draw command.

00:30:49.430 --> 00:30:51.660
You want to get as many vertices
per begin end as possible.

00:30:51.660 --> 00:30:53.070
And that can make a
significant performance

00:30:53.450 --> 00:30:55.180
improvement if you can do that.

00:30:55.180 --> 00:30:58.630
Another optimization technique is
to offload CPU processing using

00:30:58.630 --> 00:31:00.250
vertex programs onto the GPU.

00:31:00.250 --> 00:31:03.750
So if you have computational processing
you're doing on the vertices,

00:31:04.120 --> 00:31:06.770
think about trying to offload
that work to a vertex program

00:31:06.810 --> 00:31:08.520
on the graphics processor.

00:31:09.470 --> 00:31:10.400
Okay.

00:31:10.400 --> 00:31:12.930
So how do we eliminate CPU copies?

00:31:12.930 --> 00:31:17.060
Another key concept is almost
the exact same as the textures.

00:31:17.060 --> 00:31:19.880
What we can use is we can use
the Apple Vertex Array Range,

00:31:19.890 --> 00:31:23.140
which is a parallel API to
the texture range extension.

00:31:23.140 --> 00:31:27.260
And you could also think about
using the new ARB standard,

00:31:27.260 --> 00:31:29.740
which is Vertex Buffer Object.

00:31:29.740 --> 00:31:32.680
Those are nearly the same type of API.

00:31:32.680 --> 00:31:34.000
One is cross-platform.

00:31:34.000 --> 00:31:37.420
The ARB Vertex Buffer Object is a
cross-platform API that will allow

00:31:37.420 --> 00:31:39.380
you to optimize your vertex data.

00:31:39.400 --> 00:31:42.460
It's a cross-platform API that allows you
to optimize your vertex data throughput.

00:31:42.460 --> 00:31:44.280
Caching static data in
VRAM is a key concept here.

00:31:44.390 --> 00:31:45.750
Just like textures,
you want to be able to

00:31:45.750 --> 00:31:46.890
cache that data in VRAM.

00:31:46.890 --> 00:31:48.910
Use display list for
VRAM for static data.

00:31:48.910 --> 00:31:51.820
Again, we'll process that data and
cache that in VRAM for you if

00:31:51.830 --> 00:31:53.390
you want to use display list.

00:31:53.390 --> 00:31:58.620
It's a very effective way to get your
data processed properly and into VRAM.

00:31:58.620 --> 00:32:00.900
So looking at the pipeline,
like textures,

00:32:01.190 --> 00:32:04.660
there can be multiple copies of the
data as it goes through the pipeline.

00:32:04.660 --> 00:32:06.860
This is showing a media mode drawing.

00:32:06.860 --> 00:32:09.380
A media mode drawing,
we are required to keep the current

00:32:09.380 --> 00:32:12.610
vertex state before we pass it
on to the graphics processor.

00:32:12.610 --> 00:32:16.290
So using immediate mode,
you get one extra copy immediately.

00:32:16.300 --> 00:32:20.500
So if I switch to using vertex arrays,
I can eliminate that one copy

00:32:20.500 --> 00:32:23.530
just by using vertex arrays.

00:32:23.530 --> 00:32:27.280
So I'm saving myself some processing
time by using vertex arrays immediately.

00:32:29.230 --> 00:32:31.400
Now, let's go into some talking
about these extensions.

00:32:31.400 --> 00:32:36.220
So, Apple Vertex Array
Range for dynamic data,

00:32:36.220 --> 00:32:38.640
what you want to do is you want to
pass it the storage shared apple,

00:32:38.640 --> 00:32:41.580
just like on the textures,
we were using the shared hint for

00:32:41.780 --> 00:32:43.680
how we want the data to be treated.

00:32:43.680 --> 00:32:48.280
And for the Arb vertex buffer object,
we want to use the

00:32:48.760 --> 00:32:53.020
dynamic draw Arb constant,
which is the equivalent to

00:32:53.020 --> 00:32:55.340
our storage shared constant.

00:32:55.390 --> 00:33:01.030
So, it will give you the optimal
treatment for dynamic data.

00:33:01.520 --> 00:33:05.560
And what happens when we use these
extensions combined with vertex arrays?

00:33:05.900 --> 00:33:09.400
We end up with the same
thing we had for textures,

00:33:09.400 --> 00:33:13.830
and that is we get a direct DMA from the
application's copy of the data directly

00:33:13.830 --> 00:33:16.290
into the graphics processor's pipeline.

00:33:16.410 --> 00:33:19.060
So it will read it directly into
its pipeline and process it.

00:33:19.150 --> 00:33:21.360
No copies in video memory.

00:33:23.380 --> 00:33:25.790
So, looking at some sample code,
how to set up for dynamic

00:33:25.790 --> 00:33:27.650
data using vertex array range.

00:33:27.720 --> 00:33:28.270
It's very simple.

00:33:28.380 --> 00:33:29.670
There's two calls that are key to this.

00:33:29.670 --> 00:33:33.920
Vertex array range, Apple,
you just set it a pointer and a size,

00:33:33.940 --> 00:33:35.940
and that tells us how big of a
piece of memory you want to map.

00:33:36.120 --> 00:33:38.660
You malloc the data, you give it to us,
tell us where the pointer

00:33:38.860 --> 00:33:39.900
is and what size it is.

00:33:40.130 --> 00:33:44.920
We map and AGP prepare it for a
suitable storage area for direct DMA.

00:33:45.780 --> 00:33:47.260
Now, you need to make sure
you flush that data.

00:33:47.260 --> 00:33:49.350
So, you need to tell us
when the data's changed.

00:33:49.360 --> 00:33:51.540
So,
you have to call the flush vertex array

00:33:51.540 --> 00:33:54.270
range Apple call anytime you change data.

00:33:54.270 --> 00:33:56.560
That includes initially when
you first set it up or when

00:33:56.580 --> 00:33:57.940
you modify some sub-region.

00:33:57.940 --> 00:34:00.880
You tell us a pointer and a
size that you want us to flush.

00:34:00.880 --> 00:34:02.300
It can flush sub-regions.

00:34:02.360 --> 00:34:04.100
It can flush the whole thing.

00:34:04.100 --> 00:34:09.010
You tell us anytime you change the data,
and we'll make sure that all caches,

00:34:09.030 --> 00:34:13.210
all copies, all are synchronized with
your copy of the data.

00:34:13.940 --> 00:34:16.900
Okay, Vertex Buffer Object,
a little more setup.

00:34:16.970 --> 00:34:19.040
Not much more, though.

00:34:19.090 --> 00:34:20.560
You'll see that we bind.

00:34:20.560 --> 00:34:28.220
Buffer Object has an object-type
binding where you will bind to a name,

00:34:28.290 --> 00:34:31.120
and that will give you the ability
to switch between buffer objects.

00:34:31.170 --> 00:34:33.790
So what you want to do is
you can create many of these,

00:34:33.890 --> 00:34:35.740
and you can bind to them as you need to.

00:34:35.740 --> 00:34:41.180
And what you do is you pass
in your pointer and the size,

00:34:41.720 --> 00:34:46.080
just like you set up your data size,
basically, using the buffer data.

00:34:46.080 --> 00:34:48.600
And in that call,
you'll see that we're passing

00:34:48.710 --> 00:34:51.550
here the dynamic draw,
and that tells us that this

00:34:51.630 --> 00:34:55.570
particular buffer object is going
to be set up for dynamic drawing,

00:34:55.570 --> 00:34:57.400
and we're going to be
changing the data frequently.

00:34:57.500 --> 00:34:58.900
And then you call Map Buffer.

00:34:58.900 --> 00:35:01.690
Map Buffer then actually is
where you get the pointer back.

00:35:01.740 --> 00:35:05.020
So instead of you allocating the memory,
OpenGL is going to allocate the memory

00:35:05.020 --> 00:35:05.580
for you and hand back the pointer.

00:35:05.580 --> 00:35:09.580
Then you fill out the data,
and then you unmap it.

00:35:09.580 --> 00:35:12.580
And on Unmap is the
equivalent of our flush.

00:35:12.580 --> 00:35:16.580
So Unmap, we flush the data out,
and now the GPU is,

00:35:16.580 --> 00:35:20.580
all the caches have been synchronized
and ready for using that data.

00:35:23.240 --> 00:35:26.740
So again, you'll see that the sample
code that shows this is listed

00:35:26.740 --> 00:35:27.490
at the bottom of the slide.

00:35:27.490 --> 00:35:33.240
It's going to be available tonight
up on the server so that everyone

00:35:33.240 --> 00:35:34.050
will be able to download this.

00:35:34.050 --> 00:35:36.760
We've updated it with vertex buffer
objects so that you'll be able to

00:35:36.760 --> 00:35:38.320
see how to use the new extension.

00:35:42.360 --> 00:35:43.270
Okay, static data.

00:35:43.340 --> 00:35:45.220
So static data is almost
like dynamic data,

00:35:45.220 --> 00:35:46.870
it just uses different constants.

00:35:46.970 --> 00:35:48.840
So we can use,
for Apple vertex array range,

00:35:48.900 --> 00:35:52.300
we use the storage cached Apple hint.

00:35:52.300 --> 00:35:56.700
Or for arb vertex buffer object,
we use the static draw arb constant.

00:35:56.700 --> 00:35:59.680
And the rest of the setup's the same,
basically.

00:35:59.750 --> 00:36:00.870
You just pass the different constants.

00:36:00.880 --> 00:36:03.870
Display lists, again,
between a begin end,

00:36:03.870 --> 00:36:07.510
you can use immediate mode
between a begin end call.

00:36:07.520 --> 00:36:12.050
We'll post-process the data to put it
in an optimal format for uploading.

00:36:12.180 --> 00:36:15.020
A couple key things to remember
for display lists is that you

00:36:15.360 --> 00:36:18.000
want to keep the data in a,
pass it consistent vertices.

00:36:18.190 --> 00:36:22.300
So, and what I mean by that is
that if you pass a GL begin,

00:36:22.340 --> 00:36:24.660
and then you pass a
GL color and a normal,

00:36:25.070 --> 00:36:28.760
and then you pass it a GL normal,

00:36:29.050 --> 00:36:31.850
I'm sorry, pass it a color, a vertex,
and then you pass it

00:36:31.880 --> 00:36:34.750
a normal and a vertex,
you're passing different

00:36:34.750 --> 00:36:35.930
types of data per vertex.

00:36:35.980 --> 00:36:38.180
And what that does is it'll
actually confuse our optimizer,

00:36:38.180 --> 00:36:41.080
make it such that it
won't optimize the data,

00:36:41.110 --> 00:36:42.870
and you won't actually
get any benefit from it.

00:36:43.000 --> 00:36:46.160
So what you want to do is
the first vertex you call,

00:36:46.220 --> 00:36:48.930
you want to make sure that you're
passing all the data that you're

00:36:48.930 --> 00:36:50.510
going to be required per vertex.

00:36:50.720 --> 00:36:53.320
So if I need a normal and a color,
pass a normal and color

00:36:53.670 --> 00:36:55.990
for the first vertex,
then you can call anything you

00:36:55.990 --> 00:37:00.140
want as long as you don't call,
say,

00:37:00.260 --> 00:37:02.590
something besides a color and a normal.

00:37:02.610 --> 00:37:08.280
I'm not sure that's 100% clear,
but we can talk about it more.

00:37:08.370 --> 00:37:12.110
Okay,
so vertex array range and display list.

00:37:12.190 --> 00:37:15.190
So what that gets us for static data is,
like textures,

00:37:15.200 --> 00:37:17.080
we can cache the data in VRAM.

00:37:17.340 --> 00:37:18.820
And there's three
different ways to do it,

00:37:18.920 --> 00:37:19.530
as you'll see.

00:37:19.610 --> 00:37:20.600
There's the vertex array range.

00:37:20.600 --> 00:37:22.540
There's the vertex array range
display list or vertex buffer object.

00:37:22.630 --> 00:37:26.040
They get you basically the same behavior,
and that means that you're

00:37:26.040 --> 00:37:28.600
caching static data in VRAM,
and now when you reuse that data,

00:37:28.600 --> 00:37:31.600
you're getting the full bandwidth
out of the graphics processor bus,

00:37:31.600 --> 00:37:34.600
and it gets you a significant
performance boost.

00:37:35.460 --> 00:37:39.750
Okay, looking at static data setup,
similar to the dynamic data setup,

00:37:39.750 --> 00:37:43.950
where you are passing the cached
hint instead of the shared hint

00:37:43.950 --> 00:37:46.210
for the vertex array range.

00:37:46.740 --> 00:37:50.310
and again, for the vertex buffer object,
all you would do is change it from

00:37:50.310 --> 00:37:55.150
being the dynamic to the static
draw arb constant for the static

00:37:55.200 --> 00:37:58.340
data setup for vertex buffer object.

00:37:58.610 --> 00:38:01.910
and parallel talk about how
to synchronize your data.

00:38:01.930 --> 00:38:05.960
So if you're using vertex array range,
the application and the graphics

00:38:06.080 --> 00:38:11.620
processor could be sharing the same data,
so you're going to need to be able

00:38:11.620 --> 00:38:15.770
to synchronize and flush between,
manage the synchronization and

00:38:15.770 --> 00:38:15.770
flush between your draws if you're
going to double buffer the data.

00:38:16.410 --> 00:38:18.330
So it's the same type of operation,
right?

00:38:18.590 --> 00:38:21.340
CPU's generating data,
flushes it to the GPU.

00:38:21.400 --> 00:38:22.730
GPU now is going to process the data.

00:38:22.850 --> 00:38:25.920
It's going to flush it up to
the screen for single buffering.

00:38:25.940 --> 00:38:29.300
And the CPU and GPU are going to
be running in series-- serialized.

00:38:29.360 --> 00:38:32.420
They're not going to be in
parallel-- operating parallel.

00:38:32.550 --> 00:38:33.810
So let's see.

00:38:34.030 --> 00:38:36.520
Same all the way through the frames.

00:38:37.210 --> 00:38:40.320
And for double buffering the data,
now again, what we can do is we can have

00:38:40.380 --> 00:38:43.190
the CPU process the data,
flush it to the GPU,

00:38:43.190 --> 00:38:46.700
and as the GPU's processing,
CPU can start processing the data again,

00:38:46.850 --> 00:38:48.810
and so on.

00:38:49.250 --> 00:38:51.430
"Through the frames,
we can theoretically get up to

00:38:51.460 --> 00:38:55.340
double the performance for something
that's perfectly parallelized."

00:39:00.500 --> 00:39:02.400
Okay.

00:39:02.820 --> 00:39:06.860
Similar to the texture range,
you use the fence for synchronizing

00:39:06.860 --> 00:39:10.490
the vertex array range data.

00:39:10.640 --> 00:39:13.940
You'll need to know when the
GPU is done processing the data.

00:39:14.340 --> 00:39:18.830
So what you do is you would
set a fence or you would use

00:39:18.840 --> 00:39:24.570
the test object mechanism for
referencing a vertex array object.

00:39:24.720 --> 00:39:28.010
And that will let you know when
that processing has been completed

00:39:28.010 --> 00:39:30.980
so that you can start touching
the data again with the CPU.

00:39:31.050 --> 00:39:33.050
So the fence extension is
what you'll be looking for to

00:39:33.050 --> 00:39:35.240
use for vertex array range.

00:39:35.320 --> 00:39:37.690
And vertex buffer objects
don't require this.

00:39:37.690 --> 00:39:40.270
They have their own
synchronization mechanism.

00:39:40.450 --> 00:39:43.330
And it's the -- you map,
you change your data, and you unmap.

00:39:43.440 --> 00:39:46.970
So you don't need to use the fence
extension for the buffer object,

00:39:47.080 --> 00:39:50.890
only for the vertex
array range extension.

00:39:52.580 --> 00:39:54.650
Just like the textures,
looking at some sample code.

00:39:54.650 --> 00:39:57.470
First two are the same,
where I'm setting a fence and finishing

00:39:57.500 --> 00:39:59.700
against that token I've inserted,
the finish fence.

00:39:59.700 --> 00:40:02.200
And then the third line
of code down there is,

00:40:02.730 --> 00:40:08.300
instead of using a GL texture type,
I'm using a vertex array type for

00:40:08.430 --> 00:40:10.800
testing against a vertex object type.

00:40:11.060 --> 00:40:14.150
And that will allow me to set
a synchronization point where

00:40:14.150 --> 00:40:17.610
I can be guaranteed the graphics
processor is done touching the

00:40:17.620 --> 00:40:23.130
data in the vertex array range,
and allow me to synchronize

00:40:23.190 --> 00:40:24.420
the graphics processor and CPU.

00:40:26.440 --> 00:40:28.780
So here's a little bit of history here.

00:40:28.780 --> 00:40:29.960
So last year I showed this slide.

00:40:29.960 --> 00:40:30.890
Not quite.

00:40:30.900 --> 00:40:32.160
I showed it going out
a little bit further.

00:40:32.160 --> 00:40:34.540
But I wanted to show
what we've been doing.

00:40:34.540 --> 00:40:35.800
So I talked about some optimizations.

00:40:35.800 --> 00:40:40.180
What this slide shows is what
exactly the data I showed last year.

00:40:40.180 --> 00:40:43.040
And with all the hardware
changes and software changes

00:40:43.040 --> 00:40:45.780
we've made over the year,
here's where we are this year.

00:40:45.780 --> 00:40:49.020
So it's a huge increase in performance.

00:40:49.080 --> 00:40:51.260
And looking at some charts here.

00:40:51.260 --> 00:40:53.160
So if we look at a
medium-load performance,

00:40:53.540 --> 00:40:59.000
for eight vertices per begin-end,
we've gone up 800%. Vertex arrays,

00:40:59.000 --> 00:41:04.960
1,100%, 1,000% for vertex array range,
and 1,700% for display lists.

00:41:04.980 --> 00:41:11.360
Now this is using very small,
only eight vertices per draw command.

00:41:11.360 --> 00:41:13.120
So it's a very small data
set per draw command.

00:41:13.120 --> 00:41:15.820
So this shows some of the
functional overhead associated

00:41:15.820 --> 00:41:17.410
with small drawing batches.

00:41:17.440 --> 00:41:20.300
Looking at another chart,
another point in that chart,

00:41:20.560 --> 00:41:22.860
this is using 42 vertices
per draw command.

00:41:23.340 --> 00:41:24.860
So this is a little more optimal setup.

00:41:24.860 --> 00:41:28.300
But you'll see that we're still making
quite a bit of performance gains,

00:41:28.300 --> 00:41:32.340
up to 477% for immediate mode.

00:41:32.340 --> 00:41:34.680
So as I said, we're working quite a
bit on immediate mode.

00:41:34.680 --> 00:41:37.300
So since last year,
we've almost increased performance

00:41:37.300 --> 00:41:41.760
of our systems by 500%. And this
is not only a software change,

00:41:41.760 --> 00:41:42.580
but a hardware change, right?

00:41:42.580 --> 00:41:44.580
So this is comparing
state-of-the-art hardware,

00:41:44.580 --> 00:41:46.860
software last year,
state-of-the-art hardware,

00:41:46.860 --> 00:41:47.600
and software this year.

00:41:48.210 --> 00:41:49.310
500% faster almost.

00:41:49.380 --> 00:41:53.240
Okay, switching demo two.

00:41:53.300 --> 00:41:54.740
please.

00:41:58.170 --> 00:42:01.760
Okay, what I wanted to show here is just
some of the effects I talked about.

00:42:01.840 --> 00:42:04.050
So here I've got just a simple mesh.

00:42:04.320 --> 00:42:08.000
This only has eight vertices per strip.

00:42:08.000 --> 00:42:12.490
You can see that I'm only
getting 1.5 million triangles

00:42:12.490 --> 00:42:14.890
a second with immediate mode.

00:42:15.130 --> 00:42:21.130
Now, if I increase the detail of this
mesh by selecting this option,

00:42:21.270 --> 00:42:25.930
so now I'm up to a mesh that
has 198 vertices per strip.

00:42:26.140 --> 00:42:28.250
You'll see that my performance
jumped dramatically.

00:42:28.330 --> 00:42:30.560
Now I'm up to 12 million
triangles a second.

00:42:30.560 --> 00:42:33.230
Now, as I said before,
you can reduce function call

00:42:33.230 --> 00:42:34.760
overhead using CGL macro.

00:42:34.760 --> 00:42:37.790
So I've got an option here
to turn on CGL macros.

00:42:37.790 --> 00:42:41.560
So I just select CGL macros,
and you'll see that I went

00:42:41.610 --> 00:42:43.820
from 12,000 to 17,000.

00:42:43.840 --> 00:42:47.920
So I got 5 million triangles
a second in immediate mode

00:42:47.920 --> 00:42:50.900
performance by enabling CGL macros.

00:42:52.050 --> 00:42:54.400
It's a really large difference.

00:42:54.440 --> 00:42:59.370
But now let's see what we get when we
use a more optimal form of drawing.

00:42:59.370 --> 00:43:04.030
So I'm going to switch now from
immediate mode to just draw arrays.

00:43:04.030 --> 00:43:05.990
Now I go to 24 million.

00:43:05.990 --> 00:43:11.080
So draw arrays is more optimal than
the best you can make immediate mode.

00:43:11.220 --> 00:43:15.030
Now if I try using some of the
extensions we talked about,

00:43:15.030 --> 00:43:15.700
let's switch to vertex array range.

00:43:15.700 --> 00:43:15.700
So draw arrays with vertex array range.

00:43:15.870 --> 00:43:18.210
I go to 50 million.

00:43:18.620 --> 00:43:21.060
Okay, so we're making some
pretty good strides here.

00:43:21.110 --> 00:43:24.060
Now let's say my data's static,
which this happens to be.

00:43:24.100 --> 00:43:25.600
Now let's go switch to display list.

00:43:25.670 --> 00:43:28.060
Now again,
display lists are set up to cache

00:43:28.110 --> 00:43:30.130
the data static in video memory.

00:43:30.360 --> 00:43:34.640
I'll be using the bus bandwidth
available on the graphics processor.

00:43:34.760 --> 00:43:38.070
So I go from 50 million
to over 100 million.

00:43:39.160 --> 00:43:43.100
So we started off at just a few million,
and now we're up at 100 million.

00:43:43.230 --> 00:43:47.640
So using the proper extensions,
understanding how to optimally pass your

00:43:47.640 --> 00:43:52.490
data through the system can make a very,
very large difference.

00:43:52.680 --> 00:43:54.970
OK, back to slides, please.

00:43:59.600 --> 00:44:01.500
Okay, one-shot images.

00:44:01.500 --> 00:44:06.210
The best way to pass up one-shot images
that are small is using draw pixels.

00:44:06.370 --> 00:44:09.650
The reason being is that the
overhead of small images is not

00:44:09.650 --> 00:44:12.070
going to be the copy of the data,
which draw pixels does.

00:44:12.190 --> 00:44:13.760
Draw pixels will always
copy your pixel data.

00:44:13.760 --> 00:44:16.300
It's the functional overhead of
getting in and out of the system.

00:44:16.300 --> 00:44:21.670
So you have to weigh off the functional
cost of the driving OpenGL versus

00:44:21.870 --> 00:44:24.760
the expense of copying the pixels.

00:44:24.760 --> 00:44:27.720
So draw pixels works really
well for small images.

00:44:27.720 --> 00:44:31.470
And I recommend that you
experiment with this.

00:44:31.580 --> 00:44:36.550
If your images are smaller
than 128 by 128 pixels in size,

00:44:37.990 --> 00:44:39.850
Now,
one of the keys for making DropPixels

00:44:39.850 --> 00:44:43.180
go fast is you want to disable
any complex rasterization state.

00:44:43.310 --> 00:44:46.100
And the reason for that is
that DropPixels goes fastest

00:44:46.100 --> 00:44:48.100
when we're not going through,
really, the 3D pipeline.

00:44:48.100 --> 00:44:51.800
As much as we're allowing the graphics
processor to use its 2D pipeline,

00:44:51.890 --> 00:44:54.140
we can just get a straight
blit into the frame buffer.

00:44:54.210 --> 00:44:57.360
So we're not doing blending,
we're not doing dithering, no stenciling,

00:44:57.390 --> 00:45:00.660
alpha testing,
nothing that the 3D pipe needs to do

00:45:00.710 --> 00:45:02.590
so we can stay on the 2D pipeline.

00:45:02.650 --> 00:45:06.950
So disable complex state will
get you the best performance.

00:45:07.180 --> 00:45:10.890
and again, this demo is available
on the website today,

00:45:10.890 --> 00:45:14.780
so people can look at the example
I'm going to be running here.

00:45:14.840 --> 00:45:17.220
Okay, so simple little code snippet.

00:45:17.540 --> 00:45:20.430
Disable complex state,
and then you issue draw pixels.

00:45:20.570 --> 00:45:21.960
Again, same with textures.

00:45:22.000 --> 00:45:26.460
You want to use a pixel format that is
supported by the graphics processor,

00:45:26.460 --> 00:45:31.280
so we don't have to do
expensive conversions.

00:45:31.280 --> 00:45:33.720
Because if you pass in
some type like a float,

00:45:33.780 --> 00:45:34.910
we're going to have to
convert it to something the

00:45:34.910 --> 00:45:38.680
graphics processor can handle,
and that may be slower

00:45:38.680 --> 00:45:41.190
than you would like to see.

00:45:41.530 --> 00:45:44.410
Okay, so back to demo two, please.

00:45:45.120 --> 00:45:49.220
So this is a demo showing draw pixels.

00:45:49.340 --> 00:45:53.970
Again, I put the fast button on there,
something I do highly recommend.

00:45:54.150 --> 00:45:58.670
So if I zoom this down to something very
small-- and you can't almost see it,

00:45:58.680 --> 00:45:59.390
and I apologize.

00:45:59.460 --> 00:46:02.020
But it's two pixels by two pixels.

00:46:02.330 --> 00:46:06.880
So you can see I'm getting a million
draw pixel commands per second.

00:46:07.200 --> 00:46:10.940
Now, you can see I'm only getting 15
megabytes a second of bandwidth of

00:46:10.940 --> 00:46:12.960
actual pixel copying performance.

00:46:13.000 --> 00:46:14.560
I'm getting a million draw commands.

00:46:14.600 --> 00:46:17.360
Now, as I move this up in size,

00:46:18.100 --> 00:46:46.600
[Transcript missing]

00:46:47.030 --> 00:46:50.320
So, and this is kind of the boundary
of which I was describing,

00:46:50.320 --> 00:46:53.010
is that small images are
going to go really fast,

00:46:53.080 --> 00:46:56.450
larger images are going to start
hitting a memory bandwidth limit,

00:46:56.450 --> 00:46:58.110
and you might want to start
considering some of the other

00:46:58.230 --> 00:47:01.900
techniques for uploading textures
for doing this operation.

00:47:02.440 --> 00:47:04.620
Okay.

00:47:05.840 --> 00:47:08.590
Please back to slides.

00:47:10.990 --> 00:47:14.800
Okay, so let's talk about pixel
copy operations real quickly.

00:47:14.900 --> 00:47:20.330
So the key to pixel copy operations
is to get VRAM to VRAM performance.

00:47:20.570 --> 00:47:23.900
You can get extremely high bandwidth if
you don't have to come across the bus.

00:47:24.130 --> 00:47:29.180
So anytime you're storing data that you
want to have temporary stashed off and

00:47:29.190 --> 00:47:32.700
you want to be able to restore it back,
copy pixels is a great way to do that.

00:47:32.940 --> 00:47:36.890
Now, where you want to store the data,
there's a couple options.

00:47:36.890 --> 00:47:38.990
One is to use an auxiliary buffer.

00:47:38.990 --> 00:47:41.560
Auxiliary buffers,
you can create an auxiliary buffer.

00:47:41.560 --> 00:47:45.800
Apple has extensions where you can
have auxiliary buffers that have depth

00:47:45.800 --> 00:47:50.540
and stencil associated with them,
such that you can copy a depth buffer,

00:47:50.540 --> 00:47:52.880
a stencil buffer,
or a color buffer off into a temporary

00:47:52.880 --> 00:47:56.670
location and then use copy pixels
to copy back to restore your data.

00:47:56.680 --> 00:48:00.500
So if you wanted to refresh any
one of those types of buffers,

00:48:00.500 --> 00:48:02.720
an auxiliary buffer
will work well for you.

00:48:04.520 --> 00:48:09.340
Okay, so let's talk about
pixel copy operations.

00:48:09.340 --> 00:48:14.330
So the first option is to
use an auxiliary buffer.

00:48:14.330 --> 00:48:19.160
Auxiliary buffers are a
great way to store data.

00:48:19.160 --> 00:48:24.240
So you can use an auxiliary
buffer to store data.

00:48:28.410 --> 00:48:31.650
Just like with DropPixels, CopyPixels,
you'll want to have the state

00:48:31.650 --> 00:48:34.290
in a very simple form because
you want to use a 2D pipeline.

00:48:34.300 --> 00:48:36.700
When you're copying from
one memory location,

00:48:36.700 --> 00:48:39.300
the graphics processor,
video memory to another,

00:48:39.300 --> 00:48:42.300
you want to have the 2D engine
do that operation if possible.

00:48:42.350 --> 00:48:45.150
And to do that,
you want to minimize your state,

00:48:45.310 --> 00:48:47.300
have your state in a
very simple settings.

00:48:47.300 --> 00:48:49.230
And it turns out it's basically
the same thing as DropPixels.

00:48:49.280 --> 00:48:51.210
It has the same basic restrictions.

00:48:51.300 --> 00:48:53.930
So you'll want to disable as much
of the state as you can to try

00:48:53.930 --> 00:48:57.800
to get that VRAM to VRAM 2D blit.

00:48:59.420 --> 00:49:03.600
Okay, so looking at the little
piece of sample code for that,

00:49:03.600 --> 00:49:06.440
you just disable your state,
you set up your read

00:49:06.440 --> 00:49:08.640
buffer and draw buffer,
and then I copy pixels.

00:49:08.640 --> 00:49:13.060
So here you can see that I'm going to
copy data from the auxiliary buffer,

00:49:13.060 --> 00:49:14.520
maybe where I stored
the data temporarily,

00:49:14.520 --> 00:49:19.750
back into the back buffer for restoring,
say, a depth buffer, or in this case,

00:49:19.750 --> 00:49:24.160
a color buffer, and getting a very fast
restore of that image.

00:49:25.960 --> 00:49:26.140
Okay.

00:49:26.250 --> 00:49:26.490
Threads.

00:49:26.600 --> 00:49:27.630
Let's talk about threads a little bit.

00:49:27.670 --> 00:49:33.780
So rules for threading are, well,
first off, let's talk about what

00:49:33.780 --> 00:49:34.830
I'm going to talk about.

00:49:34.830 --> 00:49:36.690
Rules for threading is
what I'm going to go over.

00:49:36.690 --> 00:49:38.020
And then I'm going to talk
about divisions of work.

00:49:38.170 --> 00:49:40.640
How you can,
what kind of strategies you can

00:49:40.640 --> 00:49:45.120
use for dividing up your open gel
processing onto multiple threads.

00:49:45.330 --> 00:49:47.120
What are some of the
effective techniques?

00:49:47.120 --> 00:49:48.510
Sharing data between contexts.

00:49:48.630 --> 00:49:50.060
How do you effectively share data?

00:49:50.140 --> 00:49:53.310
You can set up multiple contexts to
have them share some common data set.

00:49:53.460 --> 00:49:55.190
Synchronizing your threads.

00:49:55.190 --> 00:49:58.410
We'll go over a little bit about
what is the proper mechanisms for

00:49:58.490 --> 00:49:59.720
synchronizing multiple threads.

00:50:01.800 --> 00:51:25.200
[Transcript missing]

00:51:25.510 --> 00:51:26.960
Let's talk about division
to work a little bit.

00:51:27.000 --> 00:51:32.260
So possibilities are moving
OpenGL onto a separate thread.

00:51:32.260 --> 00:51:36.580
So you can have your application on one
thread and OpenGL on a separate thread,

00:51:36.580 --> 00:51:38.540
a very obvious way of doing it.

00:51:38.650 --> 00:51:40.880
That's not always the optimal way.

00:51:40.880 --> 00:51:44.220
So another thing you can think
about is splitting OpenGL vertex

00:51:44.300 --> 00:51:46.000
and texture processing.

00:51:46.000 --> 00:51:50.670
That's very useful for when you
want to have video data or you're

00:51:50.670 --> 00:51:56.340
generating some pixel data coming
from a disk or coming from some source

00:51:56.340 --> 00:51:58.700
that you want to load into OpenGL,
and then you want a second

00:51:58.700 --> 00:51:59.700
thread to be drawing it.

00:51:59.700 --> 00:52:03.000
So you can have OpenGL have
multiple threads,

00:52:03.000 --> 00:52:04.420
one for loading and one for drawing.

00:52:07.230 --> 00:52:09.590
So, what gets shared between contexts?

00:52:09.670 --> 00:52:13.010
So, a lot of times,
people don't clearly understand when

00:52:13.010 --> 00:52:17.420
you have multiple contexts set up
to share each other's object state.

00:52:17.630 --> 00:52:19.650
The things that get
shared are display lists,

00:52:19.680 --> 00:52:24.260
textures, vertex and fragment programs,
and vertex array objects.

00:52:24.360 --> 00:52:28.070
That data gets shared when
you share two contexts.

00:52:28.070 --> 00:52:31.950
So, that data set will become common
between multiple contexts,

00:52:31.950 --> 00:52:35.290
if you set them up properly,
and we will manage the mutex

00:52:35.290 --> 00:52:38.200
locking of accessing that data.

00:52:39.570 --> 00:52:41.740
Okay, and like I said,
you can share an OpenGL surface.

00:52:41.740 --> 00:52:44.660
So you can also set it up
such that a multiple context

00:52:44.750 --> 00:52:46.560
can talk to one VRAM buffer.

00:52:46.560 --> 00:52:49.270
So let's look at some
diagrams of how that looks.

00:52:49.280 --> 00:52:53.060
So here in the red circles,
I've got threads.

00:52:53.220 --> 00:52:59.280
And on the left, I've got the application
doing some CPU processing.

00:52:59.340 --> 00:53:02.510
It passes that data
off to the thread two.

00:53:02.620 --> 00:53:06.210
Thread two then takes that data
and uses it to draw some OpenGL.

00:53:06.210 --> 00:53:07.220
Very simple.

00:53:07.220 --> 00:53:13.190
Simply using one OpenGL context.

00:53:13.210 --> 00:53:13.210
OpenGL is on its own thread.

00:53:16.270 --> 00:53:19.520
Here's an example of splitting
OpenGL across multiple threads.

00:53:19.550 --> 00:53:22.080
Now, what I've got here is
I've got two threads,

00:53:22.190 --> 00:53:23.870
one OpenGL context per thread.

00:53:23.910 --> 00:53:26.350
I've got them set up such that
they're sharing OpenGL state,

00:53:26.570 --> 00:53:31.100
and I've got it such that they are
talking to the same video memory surface.

00:53:31.330 --> 00:53:35.390
So they share state,
they share the VRAM buffer,

00:53:35.570 --> 00:53:39.200
and we manage the object's shared state.

00:53:39.290 --> 00:53:43.050
And what this shows is just
using texture data on one thread

00:53:43.180 --> 00:53:45.180
and vertex data on another.

00:53:45.300 --> 00:53:47.200
Obviously, those are arbitrary.

00:53:47.200 --> 00:53:50.630
You can obviously mix those up,
have any kind of inputs

00:53:50.630 --> 00:53:54.400
from either talking to the
shared object state machine.

00:53:55.650 --> 00:53:58.830
Slight different variation on that,
if people want to use P buffers,

00:53:58.850 --> 00:54:02.190
obviously you can have one talking,
one thread talking to a P buffer,

00:54:02.360 --> 00:54:06.520
and then link that P buffer into
the shared state for as a texture,

00:54:06.600 --> 00:54:08.710
and then referencing a
P buffer for drawing,

00:54:08.710 --> 00:54:15.940
and using thread one to
draw using that P buffer,

00:54:15.940 --> 00:54:16.970
draw some scene that is using
generated textures as a P buffer.

00:54:19.180 --> 00:54:22.340
So, looking at a little
bit of setup code here.

00:54:22.400 --> 00:54:24.030
So this is using Cocoa.

00:54:24.220 --> 00:54:27.580
So this is how to set up a
shared context using Cocoa.

00:54:27.950 --> 00:54:32.900
So you'll see that I create a context,
and then I knit with a pixel format,

00:54:32.970 --> 00:54:34.260
and I'm passing in a shared context.

00:54:34.260 --> 00:54:37.340
So the third line down
is a shared context,

00:54:37.340 --> 00:54:40.940
and that's the way you can link two
contexts together to have a common

00:54:41.160 --> 00:54:42.810
object shared data structures.

00:54:42.820 --> 00:54:48.400
And that allows you to share textures,
display lists, programs,

00:54:48.450 --> 00:54:50.390
vertex object data.

00:54:53.430 --> 00:54:55.640
Okay, so synchronization between threads.

00:54:55.650 --> 00:54:58.320
The way you want to do that is you
want to use standard OS thread locking.

00:54:58.320 --> 00:55:01.140
Use NSThread, NSLock, for instance.

00:55:01.140 --> 00:55:04.280
One example,
obviously you can use any other type of

00:55:04.280 --> 00:55:07.340
OS-level facility for managing threads.

00:55:07.350 --> 00:55:10.580
And I guess the main point of this
slide is that there's nothing in

00:55:10.630 --> 00:55:12.360
OpenGL to manage your threads.

00:55:12.360 --> 00:55:13.730
It's synchronization.

00:55:13.770 --> 00:55:17.480
It's standard OS tools,
facilities that do that.

00:55:17.540 --> 00:55:22.220
Don't use the Apple Fence extension
for managing your threads.

00:55:22.240 --> 00:55:25.720
Apple Fence extension is for managing
synchronization between CPU and GPU,

00:55:25.720 --> 00:55:27.340
not between two CPU threads.

00:55:27.370 --> 00:55:32.500
So that's an important point to
remember if you're going to start

00:55:32.500 --> 00:55:33.910
dabbling in multiple threads.

00:55:33.940 --> 00:55:37.470
And by the way, just as a point,
if you mess up threads and you

00:55:37.540 --> 00:55:40.940
have multiple OpenGL contexts,
multiple OpenGL threads

00:55:40.940 --> 00:55:45.930
talking into the same context,
you will cause all kinds of bad things.

00:55:45.960 --> 00:55:48.040
And bad things can go as
far as hanging your system.

00:55:48.040 --> 00:55:51.080
You'll introduce a bad command
into the graphics processor.

00:55:51.080 --> 00:55:55.330
The graphics processor may hang
and your screen will wedge and

00:55:55.330 --> 00:56:00.140
the CPU will block up against that
and everybody will come to a halt.

00:56:00.230 --> 00:56:03.290
Okay,
so let's switch to demo machine two,

00:56:03.290 --> 00:56:04.110
please.

00:56:06.380 --> 00:56:09.930
So in the beginning of this talk,
I talked about effectively

00:56:10.520 --> 00:56:12.260
using CPU and the GPU.

00:56:12.730 --> 00:56:15.600
And one of the things
I wanted to show is,

00:56:15.870 --> 00:56:20.350
first off,
we wrote a Altavec routine of this

00:56:20.760 --> 00:56:22.860
little sinusoidal wave simulator here.

00:56:23.180 --> 00:56:25.380
And you'll see it's going pretty fast.

00:56:25.450 --> 00:56:28.370
It's generating 18 million
triangles a second.

00:56:28.570 --> 00:56:32.330
So what we've got here on this chart
is we've got the red time at the bottom

00:56:32.330 --> 00:56:36.080
is time spent in the system outside
of just the application or OpenGL.

00:56:36.110 --> 00:56:39.100
The green is time spent
calculating the wave,

00:56:39.100 --> 00:56:41.500
and blue is time spent in OpenGL.

00:56:41.610 --> 00:56:43.470
So I'm going to just multi-thread that.

00:56:43.580 --> 00:56:48.500
I'm going to split it across both of
the CPUs that I have on the system.

00:56:48.500 --> 00:56:49.500
So that bumped my
performance up a little bit.

00:56:49.500 --> 00:56:50.500
I was at 19.

00:56:50.500 --> 00:56:53.500
Now I'm up to 21-something.

00:56:53.500 --> 00:56:58.500
And you can see that I've
improved performance a little bit.

00:56:58.500 --> 00:57:01.120
Now the thing that is surprising,
this actually has a high-end

00:57:01.120 --> 00:57:02.230
graphics card in it.

00:57:02.500 --> 00:57:06.240
And now what I'm going to do is I'm
going to move the wave calculation

00:57:06.240 --> 00:57:08.500
into a vertex program onto the GPU.

00:57:08.500 --> 00:57:12.500
So before I do that,
you'll see that the CPUs are very busy.

00:57:12.500 --> 00:57:16.500
There's a lot of time going into
calculating this wave with the CPU.

00:57:16.500 --> 00:57:21.230
And now I'm going to move the
wave calculation off onto the GPU.

00:57:21.660 --> 00:57:23.480
And the thing to watch
is the performance.

00:57:23.500 --> 00:57:27.950
or 21 million triangles a second,
338 frames a second.

00:57:28.210 --> 00:57:28.610
And look at that.

00:57:28.620 --> 00:57:29.760
It dropped down to 15 million.

00:57:29.760 --> 00:57:32.280
And I know there's people out
here from the hardware vendors,

00:57:32.340 --> 00:57:33.720
and they're saying, no,
that can't be possible.

00:57:33.720 --> 00:57:37.160
Our hardware always can outrun the CPU,
but it's not true.

00:57:37.160 --> 00:57:39.300
The CPU is really good at some things.

00:57:39.400 --> 00:57:43.020
And you can actually write really
efficient code sometimes that'll

00:57:43.020 --> 00:57:44.980
outrun the graphics processors.

00:57:44.980 --> 00:57:48.160
And you'll see that, by the way,
notice that the CPU is

00:57:48.330 --> 00:57:52.320
now barely doing anything,
but my performance went down.

00:57:52.320 --> 00:57:54.460
So that kind of is what I'm
trying to point out here,

00:57:54.460 --> 00:57:56.740
is that if your goal
is maximum performance,

00:57:57.670 --> 00:57:59.330
sometimes you want the
CPU to be doing work.

00:57:59.400 --> 00:58:01.400
But if your goal is to
have the CPU do nothing,

00:58:01.400 --> 00:58:04.560
for sure offload all the
processing onto the GPU,

00:58:04.560 --> 00:58:06.980
and the CPU can be free
to do something else.

00:58:06.980 --> 00:58:09.480
But that won't guarantee
maximum performance.

00:58:09.480 --> 00:58:11.480
Maximum performance will be
found by experimenting what

00:58:11.480 --> 00:58:13.180
the optimal combination is.

00:58:13.220 --> 00:58:17.290
Okay, back to slides, please.

00:58:20.670 --> 00:58:21.610
Okay, let's wrap up.

00:58:21.730 --> 00:58:26.410
Okay, so after this session,
there's a couple more

00:58:26.410 --> 00:58:29.400
OpenGL sessions that are really
good and I recommend people go to.

00:58:29.400 --> 00:58:33.260
There's the optimization live session,
which is going to have a live

00:58:33.260 --> 00:58:36.930
session talking about our tools,
using our tools, the OpenGL profiler,

00:58:36.950 --> 00:58:39.740
OpenGL driver monitor,
live on stage showing

00:58:39.850 --> 00:58:41.090
people how to use it.

00:58:41.200 --> 00:58:42.780
Really good session.

00:58:42.780 --> 00:58:45.800
I find that I'm always
using the OpenGL profiler

00:58:45.800 --> 00:58:48.710
for analyzing applications,
figuring out where the bottlenecks are,

00:58:49.650 --> 00:58:50.930
what I need to be optimizing.

00:58:50.940 --> 00:58:54.020
It's a similar tool for
OpenGL as Shark is for the CPU.

00:58:55.480 --> 00:58:57.530
And then on Friday,
we've got the introduction to

00:58:57.530 --> 00:58:58.920
the OpenGL Shader Language.

00:58:58.920 --> 00:59:01.820
For those that don't know what
the OpenGL Shader Language is,

00:59:01.840 --> 00:59:04.600
it's a good introduction to
what that language looks like,

00:59:04.810 --> 00:59:11.910
some of the capabilities it has,
and highly recommended for

00:59:11.910 --> 00:59:11.910
people that are interested in
programming the graphics processor.

00:59:12.780 --> 00:59:14.410
And who to contact?

00:59:14.790 --> 00:59:17.830
Contact myself or Travis Brown.

00:59:17.960 --> 00:59:20.350
If people want to talk to me,
they can come up afterwards and

00:59:20.350 --> 00:59:22.820
I can give them a business card,
so you don't need to write

00:59:22.830 --> 00:59:24.340
that down too quickly.

00:59:25.250 --> 00:59:29.890
So for more information,
you can go to the Apple website.

00:59:29.980 --> 00:59:32.600
So it's developer.apple.com/opengl.

00:59:32.680 --> 00:59:36.240
That's a good resource for
OpenGL information from Apple.

00:59:36.500 --> 00:59:41.570
Or you can go to the OpenGL.org website,
the www.opengl.org.

00:59:42.020 --> 00:59:45.200
That's OpenGL's official website.

00:59:45.310 --> 00:59:49.340
It contains specifications,
pointers to a variety of resources

00:59:49.400 --> 00:59:51.910
that people will find useful.

00:59:53.370 --> 00:59:54.790
and reference libraries.

00:59:54.820 --> 00:59:56.920
So we do have some references out there.

00:59:57.170 --> 01:00:00.290
You might want to take note of these.

01:00:00.290 --> 01:00:05.010
A couple of documentations
that are out on the system.