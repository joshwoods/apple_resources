WEBVTT

00:00:13.230 --> 00:00:17.960
Good morning, and welcome to session 613,
Creating Application Clusters

00:00:18.410 --> 00:00:20.040
for Apache and MySQL.

00:00:20.170 --> 00:00:24.930
It is my pleasure to introduce to
you Thomas Loran from Emic Networks,

00:00:24.940 --> 00:00:27.910
a partner of ours that we've been
working with for the last six months

00:00:27.910 --> 00:00:30.130
in providing this cluster technology.

00:00:30.130 --> 00:00:33.190
So without further ado, Thomas,
come on up.

00:00:38.180 --> 00:00:40.580
Good morning, everybody.

00:00:40.680 --> 00:00:44.020
Let's make sure that this
thing works right here.

00:00:44.250 --> 00:00:46.440
Like Chris said, my name's Thomas Loran.

00:00:46.440 --> 00:00:48.820
I'm the principal systems
engineer for Emic Networks.

00:00:48.870 --> 00:00:52.330
We're a San Jose-based
company specializing in

00:00:52.330 --> 00:00:55.440
Apache and MySQL clustering.

00:00:55.440 --> 00:01:00.380
We do have engineering offices
overseas in Ulu and Helsinki,

00:01:00.380 --> 00:01:04.730
Finland, so we're kind of an
international company,

00:01:04.790 --> 00:01:06.510
but we are located here in the Bay Area.

00:01:07.220 --> 00:01:10.830
What I'm going to talk to you
about is application clustering,

00:01:10.830 --> 00:01:15.450
the concepts, the technologies for,
like I said, Apache and MySQL.

00:01:15.650 --> 00:01:19.380
I'm going to talk about why we use load
balancing and why we use clustering.

00:01:19.380 --> 00:01:21.510
I'm going to talk about the
various types of clustering.

00:01:21.510 --> 00:01:24.700
I'm also going to talk about the
various types of load balancing

00:01:24.700 --> 00:01:26.350
and how load balancing works.

00:01:26.410 --> 00:01:29.180
Now, when you're talking to
an audience like this,

00:01:29.180 --> 00:01:31.390
there's a wide range of experiences.

00:01:31.400 --> 00:01:34.790
So some of the early slides might be
a little bit slow for people that are

00:01:34.790 --> 00:01:38.690
experienced in load balancing or some
of the other cluster technologies.

00:01:38.870 --> 00:01:41.460
But I think it's important to
get a baseline technology so

00:01:41.460 --> 00:01:44.980
people understand what I'm talking
about when I say load balancing,

00:01:45.050 --> 00:01:47.900
people understand what I'm
talking about for clustering,

00:01:47.900 --> 00:01:51.700
because clustering can mean
different things to different people.

00:01:52.850 --> 00:01:55.640
So, why do we use clustering
and load balancing?

00:01:55.980 --> 00:01:58.260
Well, basically, there's two reasons.

00:01:58.260 --> 00:02:00.360
The first one is redundancy.

00:02:00.360 --> 00:02:02.590
People have got SLAs that
they need to meet.

00:02:02.640 --> 00:02:06.090
You've got to have uptime for your
internal and external customers.

00:02:06.090 --> 00:02:08.670
Redundancy is one of the
primary reasons that people use

00:02:08.670 --> 00:02:10.400
clustering and load balancing.

00:02:10.400 --> 00:02:12.040
The second one is scalability.

00:02:12.040 --> 00:02:14.960
A lot of times,
people have hit performance

00:02:15.050 --> 00:02:18.480
walls on a single box,
and they need to put more users

00:02:18.480 --> 00:02:20.220
on a particular application.

00:02:20.630 --> 00:02:21.850
Well, how do you do that?

00:02:21.960 --> 00:02:25.870
How do you make sure that your incoming
client requests go to multiple servers?

00:02:25.870 --> 00:02:28.260
That's the load balancing part.

00:02:28.530 --> 00:02:33.450
And then, how do you make sure that the
various servers have the same data

00:02:33.520 --> 00:02:35.830
consistently on the back side?

00:02:35.830 --> 00:02:39.830
And databases have a
particular challenge for that.

00:02:40.400 --> 00:02:42.880
So, what are we talking about here?

00:02:42.880 --> 00:02:45.580
What we're talking about
is a multi-tiered approach.

00:02:45.630 --> 00:02:47.800
Now, this is something that
most everybody understands.

00:02:47.800 --> 00:02:50.690
You've got your web clients,
you've got your application

00:02:50.690 --> 00:02:53.390
servers in the middle,
and you've got your MySQL database

00:02:53.390 --> 00:02:54.790
servers on the back end.

00:02:54.800 --> 00:02:57.440
Now, sometimes,
these layers can be blurred,

00:02:57.440 --> 00:02:59.880
and you'll see that
throughout the slides.

00:02:59.880 --> 00:03:02.970
Like, a lot of times,
I'll be combining your middle tier

00:03:02.970 --> 00:03:05.930
and your back end tier together,
and bear with me as I go

00:03:05.930 --> 00:03:07.460
through these slides.

00:03:07.490 --> 00:03:08.940
It's several times I'm
going to have to say,

00:03:09.050 --> 00:03:10.200
okay, please understand that.

00:03:10.500 --> 00:03:13.120
When you're talking about Apache,
you're talking about the client

00:03:13.120 --> 00:03:15.640
being your web browser going
to your application servers,

00:03:15.640 --> 00:03:18.080
but then sometimes when
I'm talking about MySQL,

00:03:18.080 --> 00:03:21.480
you need to understand that the
client in that particular case is

00:03:21.480 --> 00:03:23.420
your web app going to the back end.

00:03:23.490 --> 00:03:25.730
But for space and the way
that the slides are laid out,

00:03:25.730 --> 00:03:28.080
that this is the only time
really that I'm going to put the

00:03:28.080 --> 00:03:29.820
three-tier architecture up there.

00:03:29.910 --> 00:03:32.610
But that's kind of what
we're talking about.

00:03:32.860 --> 00:03:35.030
Like I said,
Apache clustering between your

00:03:35.030 --> 00:03:37.400
browsers and your application servers.

00:03:37.400 --> 00:03:40.380
MySQL clustering between
your application servers.

00:03:40.400 --> 00:03:41.840
And your database back end.

00:03:41.840 --> 00:03:45.900
And sometimes you can combine, you know,
in Emix case in particular,

00:03:46.110 --> 00:03:49.680
for very low-end SMB people,
you have the option of combining

00:03:49.680 --> 00:03:53.060
your application servers
with your database servers.

00:03:53.080 --> 00:03:54.100
So.

00:03:55.590 --> 00:03:59.420
One of the things that's pretty important
is understanding what clustering is.

00:03:59.420 --> 00:04:02.300
Clustering means various
things to different people.

00:04:02.350 --> 00:04:06.300
You've got computational clustering,
application clustering,

00:04:06.300 --> 00:04:07.300
and data store clustering.

00:04:07.300 --> 00:04:10.260
I'm going to kind of mention what
each one of those are and then

00:04:10.260 --> 00:04:14.300
focus primarily on application
clustering and data store clustering.

00:04:14.300 --> 00:04:17.480
Computational Clustering
Computational clustering.

00:04:17.490 --> 00:04:20.060
There's a lot of sessions here today,
this week,

00:04:20.110 --> 00:04:22.300
about computational clustering.

00:04:22.300 --> 00:04:26.780
Basically, it's designed to seamlessly
integrate resources like CPUs

00:04:26.800 --> 00:04:29.170
into a computational grid,
sometimes called

00:04:29.170 --> 00:04:31.030
high-performance clustering.

00:04:31.140 --> 00:04:34.410
A typical application
would be video rendering,

00:04:34.420 --> 00:04:36.610
scientific and technical environments.

00:04:36.770 --> 00:04:39.700
A good example, as you can read up there,
is Virginia Tech.

00:04:39.980 --> 00:04:42.300
That's not the focus
of this presentation.

00:04:42.300 --> 00:04:47.080
There are other presentations that talk
about computational clustering and HPC.

00:04:47.080 --> 00:04:50.190
This is a traditional
application clustering.

00:04:50.270 --> 00:04:53.440
And traditionally,
what happens is—and hopefully you

00:04:53.440 --> 00:04:56.960
guys can see the little tiny dot
here—is you've got two applications

00:04:56.980 --> 00:04:59.080
running on two different servers here.

00:04:59.080 --> 00:05:01.850
And traditionally,
there's an application heartbeat

00:05:01.850 --> 00:05:04.610
between those two servers.

00:05:06.240 --> 00:05:07.600
I'm going to go back here to this one.

00:05:07.630 --> 00:05:12.410
The reason I hesitated for a
second is normally on my other

00:05:12.410 --> 00:05:14.260
slides I had IP addresses.

00:05:14.330 --> 00:05:18.480
What would happen is your top server
up here would have one IP address,

00:05:18.480 --> 00:05:21.520
like a 192.168.1.2.

00:05:21.520 --> 00:05:26.620
The one down here would
have a 192.168.1.3.

00:05:26.620 --> 00:05:29.470
And in the center,
you would have like a virtual IP address

00:05:29.470 --> 00:05:34.960
that people would connect to those two,
and it would be like a 192.168.1.1.

00:05:34.960 --> 00:05:37.930
The application heartbeat
determines which one of these two

00:05:37.930 --> 00:05:40.780
applications is going generally,
but not always.

00:05:41.040 --> 00:05:44.400
Only one of those applications
is going at a particular time.

00:05:44.470 --> 00:05:47.390
What makes application clustering
different is the fact that

00:05:47.390 --> 00:05:50.690
traditionally—and once again,
there's always exceptions.

00:05:50.720 --> 00:05:52.960
I'm talking about generally here.

00:05:52.960 --> 00:05:55.580
Generally, there's a single data store.

00:05:55.890 --> 00:05:59.840
In this particular case,
I've got an XServe RAID on the back end.

00:05:59.870 --> 00:06:03.780
That means—and there are exceptions
to this—but generally speaking,

00:06:03.780 --> 00:06:06.810
only one of those applications can
be active at a time because you

00:06:06.810 --> 00:06:11.520
can't have two applications writing
to your same disk or mounting

00:06:11.520 --> 00:06:13.070
the same volume at the same time.

00:06:13.120 --> 00:06:16.680
So that's what happens with
application clustering.

00:06:17.730 --> 00:06:21.600
Now, data store clustering is
kind of a different concept.

00:06:21.650 --> 00:06:25.340
In data store, what we're talking about
is the physical data,

00:06:25.340 --> 00:06:26.630
the disks themselves.

00:06:26.790 --> 00:06:29.770
Sometimes it happens at the file level,
and this really has no

00:06:29.820 --> 00:06:31.600
awareness of the application.

00:06:31.600 --> 00:06:36.240
So there are various volume replicators,
there's some file system replicators,

00:06:36.240 --> 00:06:38.710
things like that,
that will take your data that's

00:06:38.710 --> 00:06:41.840
physically on the disk and
replicate it back and forth.

00:06:41.920 --> 00:06:45.170
No awareness whatsoever
to the application.

00:06:47.180 --> 00:06:48.760
This requires data replication.

00:06:48.760 --> 00:06:52.950
The challenge with this is the
applications hold data in memory.

00:06:53.350 --> 00:06:57.120
Now, what that means,
why that's a problem is,

00:06:57.230 --> 00:06:59.560
frequently when you're
using data store clustering,

00:06:59.670 --> 00:07:03.280
your data underneath an application
can change physically on the disk,

00:07:03.780 --> 00:07:08.880
but your application is physically
pulling from what's cached in memory.

00:07:08.880 --> 00:07:11.980
So you can have a case where
things are out of sync.

00:07:12.040 --> 00:07:14.760
So when you're running data store
computation or data store clustering,

00:07:14.760 --> 00:07:14.760
you're not going to be able to replicate
the data that's being collected.

00:07:14.760 --> 00:07:14.760
So you're going to have
to do a lot of work.

00:07:15.770 --> 00:07:16.420
So when you're running data store
computation or data store clustering,

00:07:16.420 --> 00:07:18.770
it's very,
very important that you have hooks

00:07:18.860 --> 00:07:21.160
that are built into your application.

00:07:21.160 --> 00:07:23.940
That's where a company
like Emmet comes in,

00:07:24.290 --> 00:07:26.940
because we have hooks where we
can do the application clustering,

00:07:26.940 --> 00:07:29.520
the data store clustering,
and make sure that the

00:07:29.610 --> 00:07:33.580
application is aware that the data
physically on a disk has changed.

00:07:33.580 --> 00:07:35.230
And once again,
I'll talk more about this.

00:07:35.230 --> 00:07:37.540
It's particularly important
with databases where you

00:07:37.540 --> 00:07:38.780
have the concept of order.

00:07:41.230 --> 00:07:46.920
So, data store replication,
traditional file or disk replication.

00:07:46.920 --> 00:07:49.960
One of the ways that you do this
is a master-slave architecture.

00:07:49.990 --> 00:07:53.300
What happens is you've got read-writes
that come down to a primary server.

00:07:53.300 --> 00:07:58.290
Then what that server does is it forms
out the request or replicates those

00:07:58.290 --> 00:08:01.150
over to other servers that are slaves.

00:08:01.190 --> 00:08:04.260
Those other servers sometimes,
but not always,

00:08:04.260 --> 00:08:07.760
can be used as read-only replicas,
but you can't write to them,

00:08:07.760 --> 00:08:10.040
and the reason being is data consistency.

00:08:10.620 --> 00:08:13.410
That's why you've only got a single
place where you do your writes,

00:08:13.410 --> 00:08:15.520
and then you've got
read-only on top of that.

00:08:15.840 --> 00:08:20.620
One of the things, one of the,
when you're talking about databases,

00:08:20.680 --> 00:08:24.300
the concept that follows that
is lazy database replication.

00:08:24.300 --> 00:08:28.780
Lazy database replication does use
the same master-slave architecture,

00:08:28.780 --> 00:08:31.850
and it uses log shipping via
a peer-to-peer connection

00:08:31.850 --> 00:08:33.800
to maintain slave databases.

00:08:33.820 --> 00:08:37.360
One of the big advantages of that,
of course, it's easy to set up.

00:08:40.080 --> 00:08:43.520
It's able to survive relatively
high latency for replication,

00:08:43.780 --> 00:08:45.390
and the slaves are read-only.

00:08:45.400 --> 00:08:48.020
Now, the cons, of course,
are the failover's not

00:08:48.150 --> 00:08:50.060
transparent for applications.

00:08:50.060 --> 00:08:52.260
It's got limited scalability.

00:08:52.260 --> 00:08:56.780
Data can be lost if the master becomes
unavailable for extended periods of time.

00:08:56.780 --> 00:08:58.250
Those are some of the cons of that.

00:08:58.320 --> 00:09:02.870
Data store clustering,
what we're talking about here,

00:09:02.990 --> 00:09:05.800
and this is actually masterless.

00:09:05.800 --> 00:09:07.180
Peer-to-peer is not correct.

00:09:07.710 --> 00:09:10.020
Masterless is the correct term for this.

00:09:10.020 --> 00:09:14.760
So what happens is each one of your nodes
is able to handle both a read-write,

00:09:15.010 --> 00:09:18.280
and each one of your nodes
takes your data and transports

00:09:18.280 --> 00:09:20.040
it to all the other ones.

00:09:20.040 --> 00:09:24.010
Now, when you're doing this, it's very,
very important that you have some kind of

00:09:24.010 --> 00:09:27.920
a consistency protocol so that each one
of your nodes has exactly the same data,

00:09:27.920 --> 00:09:30.800
because if you don't,
what'll happen is one of your nodes

00:09:30.800 --> 00:09:34.700
will read one set of data that hasn't
been transported over to the other side,

00:09:34.700 --> 00:09:36.640
and you've got inconsistent views.

00:09:36.640 --> 00:09:39.590
When you're dealing with a database,
that could be very bad.

00:09:41.060 --> 00:09:44.340
So, active database replication.

00:09:44.340 --> 00:09:47.360
Once again, this is not a peer-to-peer,
this is a master list.

00:09:47.360 --> 00:09:50.230
And what happens is,
all database requests are sent

00:09:50.230 --> 00:09:51.820
to all nodes in a cluster.

00:09:51.820 --> 00:09:56.300
It ensures that the database requests
are executed in the correct order.

00:09:56.300 --> 00:09:58.450
And I'll talk more about
the ordering problem.

00:09:58.460 --> 00:10:04.660
The advantage here is the application
is—the fail-over is absolutely seamless.

00:10:04.660 --> 00:10:07.860
You get a single IP address
where the applications go.

00:10:08.200 --> 00:10:11.360
The nodes are consistent at all times.

00:10:11.360 --> 00:10:14.130
The cons are it's a little
bit more complex to set up,

00:10:14.390 --> 00:10:16.220
and you need additional network.

00:10:16.290 --> 00:10:18.660
At least in Emic's case,
what you need is an additional network

00:10:18.740 --> 00:10:22.410
on the backside to handle replication,
but I'll show that in another slide.

00:10:22.420 --> 00:10:26.590
Now, what I'm going to talk
about is load balancing.

00:10:27.170 --> 00:10:29.600
Now, load balancing,
the problem that we're

00:10:29.600 --> 00:10:32.210
going to try to solve is,
how do you spread the load

00:10:32.210 --> 00:10:34.050
between the various servers?

00:10:34.790 --> 00:10:37.800
Well, so you've got,
in this particular case,

00:10:37.820 --> 00:10:40.760
what I've got here is some web
clients that are trying to go

00:10:40.760 --> 00:10:42.450
to a generic cluster over here.

00:10:42.600 --> 00:10:45.650
So the first request comes on in,
it goes to one server,

00:10:45.950 --> 00:10:48.630
and you want to make sure that the
next requests are replicated and

00:10:48.630 --> 00:10:50.150
goes over to the other servers.

00:10:50.150 --> 00:10:57.800
Well, how do you do that?

00:10:57.800 --> 00:10:57.800
Well, make sure that...

00:10:59.300 --> 00:12:11.300
[Transcript missing]

00:12:11.480 --> 00:12:15.680
Okay, so there's traditionally
three types of load balancing.

00:12:15.680 --> 00:12:18.920
DNS round robin,
which I sort of talked about before,

00:12:19.210 --> 00:12:21.960
hardware load balancing,
software load balancing.

00:12:21.960 --> 00:12:26.770
So in a DNS round robin scenario,
which is very, very common,

00:12:26.770 --> 00:12:30.800
what happens is—and this could be
very elementary for some people here,

00:12:30.800 --> 00:12:33.060
but just bear with me so
we have a level set here.

00:12:33.560 --> 00:12:37.420
What happens is a web
client does a request for,

00:12:37.420 --> 00:12:40.580
let's say, www.mysite.com.

00:12:40.580 --> 00:12:42.550
The request goes to a DNS server.

00:12:42.580 --> 00:12:45.150
The DNS server responds
back with an address,

00:12:45.230 --> 00:12:48.850
in this case, 17.100.100.5.

00:12:48.860 --> 00:12:54.010
Then it takes that physical IP address,
that 17.100.100.5,

00:12:54.310 --> 00:12:57.350
and ships it over to one of the servers.

00:12:58.130 --> 00:13:02.000
The advantage for
DNS Round Robin is it's very free.

00:13:02.000 --> 00:13:03.000
It's easy.

00:13:03.000 --> 00:13:06.810
Let me back up here and talk a
little bit more about this slide.

00:13:06.910 --> 00:13:11.000
So what would happen is the next
request would do the very same thing.

00:13:11.000 --> 00:13:14.000
It would go up to www.mywebsite.com.

00:13:14.000 --> 00:13:16.000
It would get a different IP address.

00:13:16.000 --> 00:13:23.600
Instead of 17.100.100.5,
it would get 17.100.100.4,

00:13:23.600 --> 00:13:25.000
which would take it over to my website.

00:13:25.930 --> 00:13:29.300
So, what happens is,
in your DNS zone records, you would have,

00:13:29.300 --> 00:13:32.640
for www.mywebsite.com,
you would have four, five, six,

00:13:32.640 --> 00:13:34.860
seven different physical IP addresses.

00:13:34.890 --> 00:13:37.730
The advantage is, it's free,
it's very easy to set up,

00:13:37.740 --> 00:13:39.400
it's very easy to administer.

00:13:39.400 --> 00:13:43.010
However, like I said before, the con is,
there's no awareness of the

00:13:43.100 --> 00:13:44.270
servers being up or down.

00:13:44.430 --> 00:13:47.320
There's no inherent ability
to separate out services,

00:13:47.330 --> 00:13:49.590
and that's one of the
advanced features is,

00:13:49.590 --> 00:13:52.410
there's no awareness of the
servers being up or down.

00:13:52.410 --> 00:13:55.010
So, you don't have to worry about that.

00:13:56.200 --> 00:13:58.270
While you can use different
names for different services,

00:13:58.270 --> 00:14:01.270
it has no way to say that
an HTTP request goes here,

00:14:01.400 --> 00:14:05.560
an HTTPS request goes there,
a database request goes here.

00:14:05.710 --> 00:14:07.560
So, it can't handle any
of that kind of stuff.

00:14:07.640 --> 00:14:09.570
It's not very intelligent whatsoever.

00:14:10.780 --> 00:14:14.390
So, hardware load balancing
requires a network device,

00:14:14.430 --> 00:14:16.350
not necessarily a network switch.

00:14:16.350 --> 00:14:18.390
They're very specialized
devices to do this.

00:14:18.390 --> 00:14:21.720
And what happens is,
you get an address that comes in,

00:14:21.720 --> 00:14:25.180
in this case, 17—say, 216.17.1.

00:14:26.310 --> 00:14:30.900
And then, the load balancer applies
some sort of a policy,

00:14:30.900 --> 00:14:35.920
and I'll talk about what
the policies are on this.

00:14:36.010 --> 00:14:40.150
There are various parameters that the
hardware load balancer will use to

00:14:40.150 --> 00:14:41.700
determine which one of these goes to.

00:14:41.700 --> 00:14:46.730
It translates that address to a
private address of one of these.

00:14:46.740 --> 00:14:50.180
It could go to 1001, 23, etc., etc.

00:14:50.480 --> 00:14:55.160
Like I said, the first one comes in,
it gets translated, goes to 1001.

00:14:55.290 --> 00:14:56.090
And then, the second one comes in,
it gets translated, goes to 1002.

00:14:56.180 --> 00:14:59.000
For your next request, 1003.

00:14:59.740 --> 00:15:03.270
Let me back up here and talk about
what some of the types of policies are.

00:15:03.570 --> 00:15:06.430
And when I say policies,
typical ways that they do it

00:15:06.430 --> 00:15:08.140
are things like round robin.

00:15:09.020 --> 00:15:12.630
Now, it's different than DNS round robin
because the hardware load balancer

00:15:12.780 --> 00:15:14.740
physically knows what server is up.

00:15:14.740 --> 00:15:18.000
And if a server is down,
it simply drops it out of the rotation.

00:15:18.770 --> 00:15:21.280
One of the other policies
is weighted round robin.

00:15:21.390 --> 00:15:22.650
Here's a good example.

00:15:22.860 --> 00:15:28.920
Let's say you've got a— A new dual
G5 running an application over here,

00:15:29.130 --> 00:15:33.280
and you've got a low-end Linux box
running Pentium 166 over here.

00:15:34.070 --> 00:15:38.020
What you can do is say—is you can
weight the G5 to be more preferable

00:15:38.020 --> 00:15:39.760
than the low-end Pentium box.

00:15:40.800 --> 00:15:43.920
So, let's say for every one request
that the Pentium box gets,

00:15:44.000 --> 00:15:45.160
your G5 gets five.

00:15:45.190 --> 00:15:47.160
That's one of the weighted ones.

00:15:48.000 --> 00:15:51.080
You can do things like
simple—let's say first in,

00:15:51.080 --> 00:15:52.540
first out, latency.

00:15:52.570 --> 00:15:55.120
But what you can't do,
and where Emmet comes in,

00:15:55.120 --> 00:15:59.430
is you can't query the server for
things like server load balanc—or for

00:15:59.430 --> 00:16:03.960
server CPU utilization and parameters
that are specific to an application

00:16:03.960 --> 00:16:05.080
that's running on that server.

00:16:05.940 --> 00:16:08.860
That's one of the policies that
Emmet has that's an advantage

00:16:08.860 --> 00:16:10.700
over a hardware load balancer.

00:16:10.990 --> 00:16:12.900
Like I said, they're very, very powerful.

00:16:12.900 --> 00:16:13.980
They're flexible.

00:16:13.980 --> 00:16:17.610
You can separate out the services,
and they can handle special cases.

00:16:17.620 --> 00:16:19.640
Well, what do I mean by special cases?

00:16:19.640 --> 00:16:23.400
And I'm not going to go into a lot of
detail about what special cases are,

00:16:23.400 --> 00:16:29.260
but they're things like SSL encryption,
things like persistence.

00:16:29.260 --> 00:16:32.020
And what I mean by persistence
is sometimes when you're

00:16:32.020 --> 00:16:35.040
running a web application,
a financial application,

00:16:35.040 --> 00:16:39.060
you want to make sure that the exact
same tuple or exact same client on the

00:16:39.060 --> 00:16:43.180
same port goes to exactly that same
server before that gets committed.

00:16:43.180 --> 00:16:44.400
That's called persistence.

00:16:44.400 --> 00:16:48.750
One of the other things that hardware
load balancers can do is they can do

00:16:48.830 --> 00:16:52.200
things called global load balancing,
where you may have a cluster in New York,

00:16:52.300 --> 00:16:54.140
and you may have a
cluster in Los Angeles.

00:16:54.140 --> 00:16:57.640
How do you make sure that somebody
in Chicago goes to the right one?

00:16:57.660 --> 00:16:59.800
Those are global load balancing things.

00:16:59.850 --> 00:17:03.400
These are things that hardware load
balancers kind of specialize on.

00:17:03.440 --> 00:17:06.920
They're usually deployed in pairs,
and they're very, very expensive.

00:17:07.180 --> 00:17:11.870
They're $15,000 to $50,000 each,
so $30,000 to $100,000

00:17:11.870 --> 00:17:12.710
to get them set up.

00:17:12.980 --> 00:17:15.100
They're pretty complex
to set up and administer.

00:17:15.160 --> 00:17:17.280
There's a lot of policies, rules,
and things like that.

00:17:17.560 --> 00:17:21.180
And like I said, they don't have,
as part of their policies,

00:17:21.180 --> 00:17:25.140
they don't have any direct awareness
of what your application load is.

00:17:29.570 --> 00:17:31.340
So, software load balancing.

00:17:31.340 --> 00:17:34.500
An example is the Emic
Application Cluster.

00:17:34.500 --> 00:17:38.290
What we do is we make sure
that each one of our nodes

00:17:38.420 --> 00:17:42.640
gets—each one of our sessions,
and then using our policies inside there,

00:17:42.640 --> 00:17:46.510
we will make sure that one of those gets
accepted and the other two get dropped.

00:17:46.610 --> 00:17:50.070
And in a second, I will go into that,
and I'll explain how we drop those

00:17:50.140 --> 00:17:51.940
and how we actually run our flow.

00:17:51.940 --> 00:17:53.870
This is on the front,
or the public network,

00:17:53.870 --> 00:17:55.940
and we're talking about
the load balancing.

00:17:56.260 --> 00:17:58.850
Later on in the presentation,
I'm going to talk specifically

00:17:58.920 --> 00:18:01.510
about Emic and how we do—after
we've load balanced that,

00:18:01.640 --> 00:18:06.680
how do we do total ordered replication
of the database on the back end?

00:18:07.530 --> 00:18:10.340
Like I said,
the next one would go to the next server.

00:18:10.340 --> 00:18:11.840
It gets pretty redundant.

00:18:11.840 --> 00:18:14.340
You guys kind of get the idea here.

00:18:14.860 --> 00:18:21.390
So, the advantages for MMIC application
clusters or software load balancer,

00:18:21.410 --> 00:18:25.760
in our particular case,
were designed for HTTP and MySQL.

00:18:25.760 --> 00:18:28.980
Policy based,
based on server application load,

00:18:28.980 --> 00:18:32.240
it's very easy to set up
and easier to administer.

00:18:32.850 --> 00:18:36.130
I said on the con,
it does require a separate

00:18:36.340 --> 00:18:38.320
network on the backside.

00:18:38.320 --> 00:18:41.180
It can be a virtual network,
it can be a physical one.

00:18:41.180 --> 00:18:45.260
And also, it's designed to support
only HTTP and MySQL,

00:18:45.350 --> 00:18:48.640
and it doesn't handle the special cases.

00:18:48.640 --> 00:18:51.600
Currently, in this version of code
right now that we've got out,

00:18:51.600 --> 00:18:53.640
it doesn't handle the special cases.

00:18:53.640 --> 00:18:55.310
So, let me switch gears.

00:18:55.310 --> 00:18:59.380
That was kind of a high overview of
what load balancing is in general

00:18:59.380 --> 00:19:01.920
and what replication is in general.

00:19:01.920 --> 00:19:02.580
Very broad broads.

00:19:02.800 --> 00:19:03.210
So, let me just brush on that.

00:19:03.320 --> 00:19:05.520
Let's talk about
MMIC application clustering.

00:19:05.570 --> 00:19:08.720
It's the same technology framework.

00:19:08.720 --> 00:19:10.300
And this is where I condense it.

00:19:10.450 --> 00:19:12.810
I talked before about a
three-tier architecture.

00:19:13.130 --> 00:19:15.310
Well,
this will always be your clients here.

00:19:15.560 --> 00:19:17.660
And this is a cluster,
and I deliberately left it out

00:19:17.790 --> 00:19:20.740
whether that's going to be like an
Apache cluster or whether that's

00:19:20.740 --> 00:19:21.890
going to be a MySQL cluster.

00:19:22.000 --> 00:19:25.370
So, several times in this presentation,
I'm going to talk about

00:19:25.480 --> 00:19:26.830
these web clients here.

00:19:26.920 --> 00:19:30.240
And remember that I'll try
to make a specific reference,

00:19:30.240 --> 00:19:32.800
whether that's a web browser or
whether that's an application.

00:19:32.860 --> 00:19:37.130
as your client that's talking
to the database on the back end.

00:19:37.900 --> 00:19:43.900
So, what MySQL does is we apply
Apache load balancing on the front end,

00:19:43.900 --> 00:19:48.660
then we can do MySQL load balancing,
and then we can do MySQL replication.

00:19:48.660 --> 00:19:52.000
And you notice one of the
things I didn't put up here was

00:19:52.000 --> 00:19:54.180
actually Apache replication.

00:19:54.180 --> 00:19:58.660
That's currently not in this
particular product because when you're

00:19:58.660 --> 00:20:01.760
talking about these kind of websites,
generally speaking,

00:20:01.830 --> 00:20:05.060
you're talking about static
content for a lot of your web data.

00:20:05.060 --> 00:20:07.870
Your dynamic content is
generally in your database.

00:20:07.900 --> 00:20:08.850
It gets replicated.

00:20:08.850 --> 00:20:11.390
So, there are other
mechanisms that you could,

00:20:11.390 --> 00:20:15.170
since it doesn't change that often,
that you could actually replicate your

00:20:15.170 --> 00:20:17.330
web content between your various servers.

00:20:17.340 --> 00:20:20.050
But remember, to have a cluster,
you've got to have load balancing,

00:20:20.050 --> 00:20:22.600
and you've got to make sure
that your content is consistent.

00:20:22.600 --> 00:20:25.630
It's databases that change constantly,
and databases have got

00:20:25.630 --> 00:20:27.860
the total order problem,
and that's what we're

00:20:27.860 --> 00:20:29.030
going to talk about.

00:20:30.530 --> 00:20:34.330
So, Emic Application Cluster,
you get dynamic load balancing,

00:20:34.560 --> 00:20:38.200
you get an active replication
as opposed to lazy replication,

00:20:38.200 --> 00:20:40.540
and you get fault
detection and isolation.

00:20:40.540 --> 00:20:45.490
So, let's take a walkthrough here.

00:20:45.500 --> 00:20:49.630
What we do is we configure our switches,
and we're changing the way that we do

00:20:49.630 --> 00:20:54.350
this right now between various versions,
but we take our interfaces and switches

00:20:54.350 --> 00:20:58.960
are configured to allow all packets
from all clients to arrive on all nodes.

00:20:59.440 --> 00:21:03.550
So basically, everything gets flooded to
each one of these servers.

00:21:03.930 --> 00:21:07.980
Generally speaking,
there will be a SYN packet at the front.

00:21:07.980 --> 00:21:10.490
And the SYN packet has
got to match our policy.

00:21:10.500 --> 00:21:14.410
So the first server is set up so
that it will accept SYN packet one,

00:21:14.410 --> 00:21:17.170
the second one is set up
so that it accepts two,

00:21:17.170 --> 00:21:19.220
the second one sets up three.

00:21:19.800 --> 00:21:22.730
What happens is the other
SIN packets are dropped so that

00:21:22.840 --> 00:21:24.860
we only have one session going.

00:21:24.860 --> 00:21:28.030
The initial SIN setup
goes to all the servers,

00:21:28.250 --> 00:21:29.680
and then the other sessions are dropped.

00:21:30.140 --> 00:21:33.500
That's how it gets flooded,
and then based on that policy,

00:21:33.500 --> 00:21:35.970
we decide whether we're going
to accept that particular

00:21:35.970 --> 00:21:37.480
flow or not accept that flow.

00:21:37.480 --> 00:21:40.870
Let me give you an example
here of the kind of scalability

00:21:40.870 --> 00:21:42.720
that we're talking about.

00:21:42.720 --> 00:21:49.210
The first slide right here is an
example of a standalone MySQL server.

00:21:49.730 --> 00:21:51.910
Down here,
these are transactions per second.

00:21:51.910 --> 00:21:54.680
These are fairly typical of
what we're seeing over some

00:21:54.680 --> 00:21:57.820
low-end three-node clusters here,
but they're relative depending

00:21:57.820 --> 00:22:00.500
on things like your network,
your hardware, your application.

00:22:00.500 --> 00:22:04.450
On the back side here, this is,
let's say, server units.

00:22:04.510 --> 00:22:07.460
So this is like half a server,
one server, two server units.

00:22:07.480 --> 00:22:11.280
You see when we put MySQL or we
put M-ApplicationCluster on a node,

00:22:11.320 --> 00:22:15.000
on a single node, and we turn it on,
we really don't take much of

00:22:15.000 --> 00:22:19.110
a performance hit whatsoever,
and obviously we don't get any scaling

00:22:19.110 --> 00:22:21.360
because there's only one server.

00:22:21.360 --> 00:22:24.820
When we go to two servers here,
you can see that we don't

00:22:25.100 --> 00:22:28.490
quite double the load,
but we definitely get up there.

00:22:28.490 --> 00:22:32.720
And then when we go to three servers,
you see that the transactions

00:22:32.760 --> 00:22:34.110
per second go up.

00:22:34.170 --> 00:22:37.460
It doesn't go up exactly one for one,
but there is a load.

00:22:37.460 --> 00:22:40.910
a linear relationship as we go up.

00:22:43.130 --> 00:22:47.110
So,
it's managed off of a Java application,

00:22:47.180 --> 00:22:48.880
it's Aquafied.

00:22:48.880 --> 00:22:51.720
This is an example of our client,
and later on in the demo,

00:22:51.790 --> 00:22:53.550
I'm going to talk about that.

00:22:53.650 --> 00:22:54.800
I'm going to show you this.

00:22:54.800 --> 00:22:56.760
I'm going to connect up to a cluster.

00:22:56.790 --> 00:22:59.790
I'm going to bring a couple nodes online.

00:23:00.510 --> 00:23:05.590
Each one of the nodes is represented
by a particular glyph or an icon that

00:23:05.590 --> 00:23:07.720
has various colors for various meaning.

00:23:07.720 --> 00:23:11.220
It can be active,
where it handles all our requests.

00:23:11.410 --> 00:23:13.930
It can be offline,
where it does not handle any

00:23:13.930 --> 00:23:16.800
requests whatsoever—in other words,
totally dead.

00:23:16.800 --> 00:23:18.160
It can be standby.

00:23:18.160 --> 00:23:20.540
Standby is kind of interesting,
because what a standby

00:23:20.540 --> 00:23:22.830
server is doing is,
it's replicating all your

00:23:22.910 --> 00:23:26.940
database changes on the back end,
but it's not servicing any read requests.

00:23:26.940 --> 00:23:30.380
So it can come online very, very quickly,
because it doesn't have

00:23:30.380 --> 00:23:30.380
to be a standby server.

00:23:30.400 --> 00:23:32.910
It doesn't have to synchronize
any kind of a database,

00:23:33.020 --> 00:23:34.020
so it's ready to go.

00:23:34.020 --> 00:23:37.670
Transitional,
it's sort of moving between the states.

00:23:37.690 --> 00:23:40.750
You'll see this glyph come up several
times when you're actually moving

00:23:40.750 --> 00:23:44.750
between a standby and an active,
or an offline and an active status.

00:23:44.870 --> 00:23:49.040
And a maintenance mode is where you
take the database down for maintenance.

00:23:49.040 --> 00:23:52.110
So I'm going to switch
over to a cluster here.

00:23:54.150 --> 00:24:01.660
So what I've got here is I've already
hooked up to an application cluster.

00:24:01.660 --> 00:24:05.990
This is a prototype software we're
running here for an OS X port.

00:24:06.020 --> 00:24:08.640
And I should say that
we're in version 2.0.

00:24:08.680 --> 00:24:09.680
We're built for Linux.

00:24:09.680 --> 00:24:13.000
We've got production customers
running in version—on Linux.

00:24:13.070 --> 00:24:16.540
But several months ago,
we started in an OS X port.

00:24:16.540 --> 00:24:19.880
So what I'm showing now is
the OS X port of our product,

00:24:19.880 --> 00:24:21.840
which is prototype software.

00:24:21.840 --> 00:24:22.980
And I'll talk about when
we're going to be using it.

00:24:23.000 --> 00:24:24.680
And I'll talk about when
we're looking at releasing it.

00:24:24.780 --> 00:24:27.140
But it's not a build,
because that's already out there.

00:24:27.140 --> 00:24:33.380
It's just a conversion
over to a different OS.

00:24:33.680 --> 00:24:36.220
What we're seeing here is
Node 3 up here at the very top

00:24:36.310 --> 00:24:39.420
is running in an active status.

00:24:39.420 --> 00:24:41.780
Node 2 and 3 are in a cold status.

00:24:41.780 --> 00:24:44.390
And we've set up a Linux server
that's running a script

00:24:44.510 --> 00:24:46.480
that's banging this for load.

00:24:46.480 --> 00:24:48.550
So it's got—if I remember
the script right,

00:24:48.550 --> 00:24:52.570
it's set up—it's putting about
350 clients are connecting on it.

00:24:52.870 --> 00:24:55.190
We've got a lot of artificial
things in the script to

00:24:55.190 --> 00:24:58.110
generate 100 percent load here.

00:24:58.300 --> 00:25:03.660
Now what I'm going to do is I'm going to
control-click on our first server here,

00:25:03.660 --> 00:25:07.790
and I'm going to move it
from an offline status.

00:25:08.800 --> 00:25:15.800
[Transcript missing]

00:25:16.880 --> 00:25:20.490
And as it starts to move,
what's happening now is the

00:25:20.660 --> 00:25:24.120
first server is sending its
load over to the second one.

00:25:24.120 --> 00:25:26.450
It's actually copying
the database physically,

00:25:26.460 --> 00:25:27.840
so you don't have to
have that one set up.

00:25:27.920 --> 00:25:29.850
It's physically copying
that database over,

00:25:29.900 --> 00:25:31.670
and now the second one is online.

00:25:33.300 --> 00:25:34.800
Now,
if you take a look at the chart here,

00:25:34.810 --> 00:25:38.330
it took a little bit of a dip,
and then our CPU utilization

00:25:38.440 --> 00:25:42.030
should go right back up,
but our transactions per

00:25:42.030 --> 00:25:46.410
second—let me move over here
so we're not checking on load,

00:25:46.430 --> 00:25:51.200
we're checking on our request rate,
our transactions per second.

00:25:51.700 --> 00:25:53.520
Our transactions per second went up here.

00:25:53.520 --> 00:25:56.670
Now, you notice that this doesn't
have the same scalability that

00:25:56.730 --> 00:25:59.900
I showed in the other slide,
because we're running prototype software,

00:25:59.900 --> 00:26:02.170
Thomas Loran and our engineers
are still connecting.

00:26:02.540 --> 00:26:03.250
So, we're kind of tweaking the code.

00:26:03.290 --> 00:26:05.440
I just wanted to show
that it does increase.

00:26:05.490 --> 00:26:08.190
It's not as large an increase
as we have on our Linux,

00:26:08.660 --> 00:26:12.370
but while we're still running
100% load on two servers now,

00:26:12.380 --> 00:26:13.900
because that's what
our script is pushing,

00:26:13.900 --> 00:26:15.600
we're actually getting more throughput.

00:26:17.510 --> 00:26:22.450
These two servers—one's at 96%, one's at
98%—that's about as high as they'll get.

00:26:22.530 --> 00:26:25.890
They usually don't go to 100%,
but sometimes it takes a little

00:26:25.890 --> 00:26:29.700
while to level out the load because
of the number of transactions.

00:26:29.900 --> 00:26:32.090
Some have got a timeout,
and the load balancer shifts.

00:26:32.100 --> 00:26:32.100
So, we're kind of tweaking the code.

00:26:32.100 --> 00:26:32.100
I just wanted to show
that it does increase.

00:26:32.100 --> 00:26:32.100
It's not as large an increase
as we have on our Linux,

00:26:32.100 --> 00:26:32.100
but while we're still running
100% load on two servers now,

00:26:32.100 --> 00:26:32.100
because that's what
our script is pushing.

00:26:32.100 --> 00:26:32.100
We're actually getting more throughput.

00:26:32.940 --> 00:26:36.470
Sometimes it'll take three to
five minutes before they level

00:26:36.880 --> 00:26:38.880
out at the same utilization.

00:26:38.880 --> 00:26:43.500
Now what I'm going to do is I'm going
to control-click on our third node.

00:26:43.670 --> 00:26:45.990
I'm going to bring that online.

00:26:46.760 --> 00:26:50.420
Now, what's interesting here,
and while we always recommend

00:26:50.420 --> 00:26:53.820
deploying these in three,
is what's going to happen is one

00:26:53.820 --> 00:26:57.720
of the servers is going to go into
a standby mode—a special kind of

00:26:57.720 --> 00:27:01.900
standby mode—where it's not going
to be servicing requests out.

00:27:01.900 --> 00:27:01.900
It's going to freeze the server.

00:27:02.900 --> 00:27:04.740
So, we're going to freeze the
database as a snapshot,

00:27:04.830 --> 00:27:06.480
copy it over to the new
node that's coming online,

00:27:06.480 --> 00:27:08.220
and cache all the requests
that are coming in.

00:27:08.300 --> 00:27:11.900
Then, when both of them have the
exact same copy of the database,

00:27:11.900 --> 00:27:15.800
what it's going to do is take the cached
information that it was transmitting,

00:27:15.800 --> 00:27:18.720
verify that everything's consistent,
and bring all three of

00:27:18.720 --> 00:27:19.870
them up on the same time.

00:27:20.760 --> 00:27:25.240
So, it's better always to have three than
it is two so that you can always have

00:27:25.240 --> 00:27:28.940
one running as you're doing this,
because one does move into a

00:27:28.940 --> 00:27:31.690
maintenance mode or into standby mode.

00:27:32.820 --> 00:27:36.060
So, as you can see,
what's happening is both of these

00:27:36.060 --> 00:27:38.400
have gone into transition state.

00:27:38.400 --> 00:27:39.660
One of them is still active.

00:27:39.700 --> 00:27:43.660
Now, this guy's gone offline,
and while the top guy is still

00:27:43.800 --> 00:27:48.100
servicing all the requests,
the guy on the right has now transferred

00:27:48.100 --> 00:27:50.700
his database over to the one on the left.

00:27:51.470 --> 00:27:54.890
He's got caught up, and then he takes the
information that's cached,

00:27:54.940 --> 00:27:56.700
and he brings those together.

00:27:56.700 --> 00:27:59.750
And, well, down here,
this was running—the

00:27:59.760 --> 00:28:01.500
first line was a single.

00:28:02.500 --> 00:28:06.500
The second one was two boxes,
and now that's increased up to three.

00:28:06.650 --> 00:28:10.500
So, while we're still running,
our load—let me go back here and check.

00:28:10.640 --> 00:28:11.300
Yeah.

00:28:11.300 --> 00:28:15.760
Our load overall for the cluster
is still running 97%. We've got the

00:28:15.840 --> 00:28:18.990
Linux box set up to just blast the
box with as much as it can take.

00:28:19.380 --> 00:28:21.500
You'll see that our requests
per second keep going up.

00:28:21.500 --> 00:28:25.480
Don't pay that much attention to
what the delta is between the steps,

00:28:25.540 --> 00:28:29.450
because that's code optimization
that's specific to the OS X platform

00:28:29.450 --> 00:28:31.300
that we're still working on.

00:28:32.300 --> 00:28:33.750
So, that basically is the demo
that I wanted to show you,

00:28:33.830 --> 00:28:36.540
and I'll go back to the presentation.

00:28:38.350 --> 00:28:41.710
It's kind of interesting how one of
them will go offline and then copy

00:28:41.710 --> 00:28:47.570
the database over while the first one
is still servicing all your requests.

00:28:48.770 --> 00:28:51.460
Okay, can we cut back over to the slides?

00:28:51.460 --> 00:28:53.640
Done with the demo?

00:28:53.730 --> 00:28:55.730
All right, thanks.

00:28:55.860 --> 00:29:00.100
So, let's talk about how EAC does
the active replication.

00:29:00.100 --> 00:29:05.490
And I told you that one of the problems
that we had was a two-phase commit.

00:29:05.500 --> 00:29:08.880
I don't think that was
actually in the slide.

00:29:08.880 --> 00:29:13.380
Lazy—when people traditionally
did active replication,

00:29:13.380 --> 00:29:15.120
they had a concept of a two-phase commit.

00:29:15.120 --> 00:29:18.980
It means your database write has
got to be committed on one server,

00:29:18.980 --> 00:29:21.350
and then it's got to be
committed on the other server,

00:29:21.350 --> 00:29:24.960
acknowledged, before it actually becomes,
let's say, a final commit.

00:29:24.960 --> 00:29:26.500
Well, that's very, very slow.

00:29:26.620 --> 00:29:28.560
So,
what we wanted to do—the whole essence

00:29:28.640 --> 00:29:31.960
of why Emic was created and the problem
that we were trying to solve was,

00:29:31.960 --> 00:29:36.110
how do we avoid this two-phase
commit and still have a lot of speed?

00:29:36.440 --> 00:29:39.910
Well, we came up for our concept
of active replication,

00:29:39.910 --> 00:29:42.190
and I'll explain how we do that.

00:29:42.560 --> 00:29:47.100
So, what we do is we've got a
group communication protocol,

00:29:47.150 --> 00:29:50.080
and we use a total reliable
multicast protocol.

00:29:50.080 --> 00:29:53.220
It's a token-based passing protocol.

00:29:53.260 --> 00:29:55.200
A lot of people joke with me and say,
token ring.

00:29:55.200 --> 00:29:55.650
No, no, no.

00:29:55.810 --> 00:29:57.440
It's not token ring, okay?

00:29:57.440 --> 00:30:04.440
It's a token passing scheme that we use,
and we use the reliable multicast.

00:30:04.520 --> 00:30:08.060
And, of course, if you're a network guy,
you say, well, wait a second.

00:30:08.060 --> 00:30:09.400
Multicast is an oxymoron.

00:30:09.400 --> 00:30:10.640
It's not reliable.

00:30:11.230 --> 00:30:15.100
Well, because we use the token scheme,
and what we do is each one of our

00:30:15.100 --> 00:30:19.300
requests gets a sequence number that's
assigned to that particular request,

00:30:19.300 --> 00:30:21.910
and then we're able to
keep everything in track.

00:30:21.980 --> 00:30:26.180
The token gives a particular
node permission to transmit,

00:30:26.250 --> 00:30:30.120
and it doesn't wait for the
event the request actually

00:30:30.120 --> 00:30:32.730
execute on each one of the nodes.

00:30:32.740 --> 00:30:36.150
What we're trying to do is make sure
that it gets queued in a proper order.

00:30:36.150 --> 00:30:39.590
So, that's what we're doing with a
sequence number for each request.

00:30:39.820 --> 00:30:46.780
We also have a mechanism for
handling partition clusters.

00:30:46.780 --> 00:30:49.630
And what I mean by partition clusters,
let's say I've got five

00:30:49.790 --> 00:30:50.700
clusters in a node.

00:30:50.700 --> 00:30:54.020
There's some kind of a
partition on the back end,

00:30:54.020 --> 00:30:57.440
and what happens is they're still
connected up on the front end,

00:30:57.540 --> 00:31:00.070
so it's being load
balanced across the front,

00:31:00.070 --> 00:31:02.130
but it's partitioned on the back.

00:31:02.140 --> 00:31:04.570
So, you've got two nodes over here
in this group that think that

00:31:04.690 --> 00:31:07.400
they're servicing all the requests,
and you've got three nodes over

00:31:07.430 --> 00:31:08.450
here determining that they're
still servicing all the requests.

00:31:08.720 --> 00:31:13.090
Well, one of those two can't be right,
because then you've got

00:31:13.090 --> 00:31:14.760
a partition cluster.

00:31:15.110 --> 00:31:18.470
We have a mechanism for handling
that and shutting down what we call

00:31:18.890 --> 00:31:20.610
the node that isn't the quorum.

00:31:20.730 --> 00:31:24.100
The quorum, or the majority,
continues to operate,

00:31:24.100 --> 00:31:26.720
and the other part actually drops off.

00:31:26.840 --> 00:31:30.720
The other thing is we're actually
integrated with MySQL lock manager,

00:31:30.720 --> 00:31:33.570
so the problem that we're
trying to solve is how do we

00:31:33.570 --> 00:31:35.710
have total ordered consistency?

00:31:35.920 --> 00:31:38.250
Here's our front network.

00:31:38.340 --> 00:31:42.680
We're doing our standard load balancing
that we had talked about before.

00:31:42.680 --> 00:31:45.800
We receive the tokens
on a private network.

00:31:45.800 --> 00:31:48.990
We assign an order request,
and then we multicast

00:31:48.990 --> 00:31:50.800
them out on the back side.

00:31:50.800 --> 00:31:54.800
You'll notice here
that we've got a token.

00:31:54.990 --> 00:31:57.320
Right here on the top,
this just gives him

00:31:57.430 --> 00:32:00.110
permission to transmit,
and then each one of your requests gets

00:32:00.140 --> 00:32:01.910
assigned a particular sequence number.

00:32:02.050 --> 00:32:05.330
So the sequence number that each
one of these nodes gets is one,

00:32:05.330 --> 00:32:06.860
for example.

00:32:07.140 --> 00:32:08.870
Then we do our next operation.

00:32:09.020 --> 00:32:11.840
The load balancer goes over, let's say,
to the middle server.

00:32:11.840 --> 00:32:12.700
He does a request.

00:32:12.770 --> 00:32:14.840
That request is number two, for example.

00:32:16.240 --> 00:32:19.660
He gets the token,
so he has permission to transmit that,

00:32:19.660 --> 00:32:21.340
and he multicasts that out.

00:32:21.340 --> 00:32:23.560
And then everybody says, well,
the last one I saw was one.

00:32:23.560 --> 00:32:26.460
I'm getting a two,
so I know that everything's in order.

00:32:26.560 --> 00:32:27.470
It's got to be sequential.

00:32:27.470 --> 00:32:32.080
Once again, the load balancer changes
over to the third node.

00:32:32.080 --> 00:32:37.440
It's got a request that comes in,
a database replication request.

00:32:37.510 --> 00:32:39.430
It assigns it number three.

00:32:39.780 --> 00:32:40.780
It gets the token.

00:32:41.050 --> 00:32:44.940
It multicasts that out
to each one of the nodes.

00:32:44.940 --> 00:32:47.780
So, as you can see,
there's no two-phase commit here,

00:32:47.780 --> 00:32:51.090
and that's really what we're trying to do
is solve that two-phase commit problem.

00:32:51.090 --> 00:32:54.950
Now, for your programmers out there,
this makes a lot of sense.

00:32:55.020 --> 00:32:56.940
Here's the problem that
we're trying to solve.

00:32:56.940 --> 00:33:00.040
With databases,
it makes a big difference the order that

00:33:00.040 --> 00:33:04.610
you receive your particular request in,
because as you're well aware, you know,

00:33:04.710 --> 00:33:07.490
three plus four divided by five
is a lot different than three

00:33:07.540 --> 00:33:08.950
divided by four plus five.

00:33:09.000 --> 00:33:11.610
So, with databases,
you've got to make sure that

00:33:11.700 --> 00:33:13.620
the order is exactly consistent.

00:33:13.650 --> 00:33:16.820
Now, there are a couple of things that
go into making up that order.

00:33:17.000 --> 00:33:19.120
You've got network latency
and things like that,

00:33:19.120 --> 00:33:21.310
but on each server,
each one of your threads could

00:33:21.310 --> 00:33:22.770
be operating at a different rate.

00:33:22.850 --> 00:33:25.360
There could be different CPU load
on each one of the servers.

00:33:25.410 --> 00:33:27.770
So, you need to make sure,
even though the request gets

00:33:27.820 --> 00:33:31.400
to that particular server,
how do you ensure that it gets

00:33:31.400 --> 00:33:34.970
ordered or executed in exactly the
right order on different servers?

00:33:34.970 --> 00:33:36.590
This is a very challenging problem.

00:33:36.610 --> 00:33:39.680
And like I said, what we do is we assign
sequence numbers to it,

00:33:39.820 --> 00:33:40.770
and we queue them.

00:33:40.860 --> 00:33:40.860
Okay.

00:33:41.620 --> 00:33:44.380
And we do that for execution
in exactly the same order.

00:33:44.380 --> 00:33:47.040
And as long as they're queued,
then we can free up and

00:33:47.100 --> 00:33:48.510
go do the next operation.

00:33:50.050 --> 00:33:55.980
So, I talked about fault
detection and isolation.

00:33:55.980 --> 00:34:00.410
One of the challenges that we've got,
on our back end we've got a

00:34:00.410 --> 00:34:05.000
secondary network that can
be either real or virtual.

00:34:05.000 --> 00:34:11.340
What happens is we use a token
loss timeout and a probe,

00:34:11.340 --> 00:34:12.710
which is ping-like.

00:34:12.710 --> 00:34:12.710
Ping is the best way to describe it.

00:34:12.770 --> 00:34:19.040
We have a particular fail—we have a
failure detected on the network here.

00:34:19.040 --> 00:34:22.270
We detect that the token has timed out.

00:34:22.580 --> 00:34:24.740
And by the way,
the target that we're looking at

00:34:24.740 --> 00:34:26.480
to do this is 10 milliseconds.

00:34:26.620 --> 00:34:30.670
So we're not talking, you know, 10, 15,
20 seconds or a minute or two.

00:34:30.770 --> 00:34:32.360
10 milliseconds is what we're looking at.

00:34:32.460 --> 00:34:35.500
We get a token loss,
and then what we do is

00:34:35.500 --> 00:34:37.590
we'll send a probe down.

00:34:38.200 --> 00:37:17.600
[Transcript missing]

00:37:18.000 --> 00:37:32.200
[Transcript missing]

00:37:32.670 --> 00:37:39.790
So, kind of in summary there,
we do MySQL and Apache clusters,

00:37:39.810 --> 00:37:42.040
not computational clusters.

00:37:42.040 --> 00:37:45.720
We're the hybrid approach where we do
data store and application clustering,

00:37:45.720 --> 00:37:47.900
and we bring those two together.

00:37:47.900 --> 00:37:51.670
Our focus is accurate replication,
where you need very,

00:37:51.670 --> 00:37:55.400
very tight replication
between your various servers,

00:37:55.400 --> 00:37:59.100
where you need to scale
your databases very solidly,

00:37:59.100 --> 00:38:01.680
very low total cost of operation.

00:38:01.680 --> 00:38:04.010
The other thing that I'm kind
of looking to you guys for is

00:38:04.010 --> 00:38:06.190
our release candidate program.

00:38:06.390 --> 00:38:10.630
Like I said,
we're on version 2.0 on other platforms,

00:38:10.630 --> 00:38:14.350
but we're looking for beta testers
and release candidate testers

00:38:14.350 --> 00:38:16.460
for our code to run on OS X.

00:38:16.460 --> 00:38:21.100
We're looking at releasing
that sometime late Q3,

00:38:21.100 --> 00:38:26.280
early Q4, with production as scheduled,
sometime around the end of the

00:38:26.280 --> 00:38:28.010
year or early part of the year.

00:38:28.140 --> 00:38:30.390
The product managers get
very upset when I actually,

00:38:30.440 --> 00:38:32.260
you know, nail them down with dates.

00:38:32.260 --> 00:38:33.560
You guys are familiar with that.

00:38:33.560 --> 00:38:36.550
I mean, there could be a bug that,
you know, somebody determines during

00:38:36.550 --> 00:38:39.390
the actual release candidate
program to slip those dates.

00:38:39.520 --> 00:38:42.340
But anyway,
we're looking at Q4 and early Q1

00:38:42.340 --> 00:38:46.640
2005 for production code on OS X.

00:38:46.640 --> 00:38:48.950
So, if you're interested in that,

00:38:49.800 --> 00:38:53.750
somewhere up here it had my email
address on there or you can go to

00:38:53.750 --> 00:39:01.060
our website at www.emicnetworks.com
or my email address is thomas.loran

00:39:01.060 --> 00:39:03.680
at applicationcluster.com.

00:39:03.740 --> 00:39:07.210
One of the things about having an early
stage company is sometimes we really

00:39:07.270 --> 00:39:08.990
can't decide on what our name is.

00:39:09.000 --> 00:39:11.900
So my email address is
actually thomas.loran,

00:39:11.900 --> 00:39:14.790
L-O-R-A-N, at applicationcluster.com.

00:39:14.820 --> 00:39:19.180
So if you guys want to be part of the
release candidate program and test this

00:39:19.180 --> 00:39:23.400
as we're running our production release,
I'd be happy to talk to you and getting

00:39:23.400 --> 00:39:25.280
this set up in your environment.

00:39:25.280 --> 00:39:28.500
So with that,
I am going to work with Chris who's going

00:39:28.500 --> 00:39:30.550
to handle any kind of Apple questions.

00:39:30.550 --> 00:39:33.510
I wanted to see if there are any
questions from the audience here

00:39:33.580 --> 00:39:35.240
on MySQL and Apache clustering.