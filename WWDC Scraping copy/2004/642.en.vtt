WEBVTT

00:00:12.270 --> 00:00:15.860
I'm Warner Yuen and welcome
to this afternoon's session

00:00:15.860 --> 00:00:18.840
on HPC software optimization.

00:00:18.840 --> 00:00:22.520
So once again a lot of people are
always wondering Apple and high

00:00:22.520 --> 00:00:27.240
performance computing and in reality
what they think about is this.

00:00:27.350 --> 00:00:33.440
But the reality is there are a lot of
great tools for building high performance

00:00:33.570 --> 00:00:36.260
computers with Apple hardware.

00:00:36.370 --> 00:00:40.780
And today in this session we'll be
talking about some of these things,

00:00:40.780 --> 00:00:44.760
specifically what's new at Apple for
high performance computing.

00:00:44.840 --> 00:00:50.050
And also some of the things for
preparation of your Apple hardware

00:00:50.120 --> 00:00:52.800
for high performance computing.

00:00:52.900 --> 00:00:57.830
And a great introduction on
how to write parallel code.

00:00:58.050 --> 00:01:03.080
So we don't have time to show you
all of the cool new Apple tools,

00:01:03.090 --> 00:01:07.430
but I wanted to give a plug for the new
Chud performance optimization tools.

00:01:07.500 --> 00:01:11.710
Specifically tomorrow there's a
great session called Got Shark that

00:01:11.840 --> 00:01:15.030
talks about performance profiling.

00:01:15.160 --> 00:01:16.430
High performance compilers.

00:01:16.480 --> 00:01:19.030
So this is what has really
allowed us to play in the high

00:01:19.030 --> 00:01:20.650
performance computing world.

00:01:20.880 --> 00:01:26.580
Specifically new this week was
the 64 bit compiling ability

00:01:26.600 --> 00:01:29.550
with GCC in your Tiger previews.

00:01:29.780 --> 00:01:34.360
In today's session what you'll really,
what we'll get into is performance

00:01:34.360 --> 00:01:39.740
benefits using Mac OS X's Accelerate
framework for high performance computing.

00:01:39.740 --> 00:01:44.390
And we'll have a presenter
talking about streamlining the

00:01:44.390 --> 00:01:46.480
OS services for performance.

00:01:46.520 --> 00:01:50.820
And another person will be coming
up to talk about what options are

00:01:50.820 --> 00:01:54.460
available for building high performance
computers with Apple hardware.

00:01:54.540 --> 00:01:58.490
And then we'll have our introductions
on creating parallel code with

00:01:58.490 --> 00:02:05.030
both MPI and also a next generation
parallel computing developer framework.

00:02:05.280 --> 00:02:06.300
So who's talking today?

00:02:06.300 --> 00:02:08.000
We have some really great speakers.

00:02:08.000 --> 00:02:12.150
We have Steve Peters from Apple's
Vector Numerics Group coming

00:02:12.150 --> 00:02:14.820
up to talk about Mac OS X.

00:02:14.820 --> 00:02:18.110
We'll have Josh Durham from
Virginia Tech that will come up

00:02:18.210 --> 00:02:22.540
and talk about use of Mac OS X for
high performance computing.

00:02:22.720 --> 00:02:26.540
And Dean Dauger from Dauger
Research will come up to talk

00:02:26.580 --> 00:02:29.150
about writing parallel MPI code.

00:02:29.180 --> 00:02:32.390
And finally,
we have Steve Forde from GridIron

00:02:32.390 --> 00:02:37.640
Software to talk about next generation
parallel development framework.

00:02:37.640 --> 00:02:41.490
So with that, I'm just going to go
to our first speaker,

00:02:41.540 --> 00:02:43.660
Steve Peters.

00:02:46.300 --> 00:02:54.050
I'm going to tell you about the
mathematical facilities in Mac OS X,

00:02:54.050 --> 00:02:58.840
sort of base of much of scientific
high performance computing

00:02:58.850 --> 00:03:01.320
that's available for Mac OS X.

00:03:01.320 --> 00:03:05.740
The agenda today is survey the
APIs that we ship with Mac OS X,

00:03:05.740 --> 00:03:09.620
tell you about new Tiger APIs,
a little bit of comparative

00:03:09.740 --> 00:03:14.840
performance to let you know that we're,
I think, leading the field in sort

00:03:14.840 --> 00:03:19.620
of this price space anyway,
and then reinforce the mantra that

00:03:20.000 --> 00:03:23.350
first check out our frameworks
when you're interested in getting

00:03:23.350 --> 00:03:25.550
performance out of the machines on math.

00:03:27.530 --> 00:03:34.470
So we start out with chips
that are IEEE 754R compliant.

00:03:34.850 --> 00:03:38.510
Like much of the industry,
this is what gives us

00:03:38.600 --> 00:03:40.970
our substrate in math.

00:03:41.180 --> 00:03:46.650
Then layer on top of that LibM,
a C99 compliant library full of the

00:03:46.690 --> 00:03:50.590
elementary transcendental functions
that everyone knows and loves,

00:03:50.590 --> 00:03:53.700
sine, cosine, square roots, and so forth.

00:03:53.700 --> 00:03:59.400
We present these in single, double,
and long double precisions,

00:03:59.510 --> 00:04:02.530
complex and in the real domain.

00:04:05.190 --> 00:04:09.040
For linear algebra,
a place where many vendors add value,

00:04:09.050 --> 00:04:11.940
and so do we,
we take the basic linear algebra

00:04:11.940 --> 00:04:17.400
subroutines as shipped in the
Atlas open source package,

00:04:17.400 --> 00:04:20.540
do some additional tuning,
and ship that as part of

00:04:20.540 --> 00:04:21.740
our Accelerate framework.

00:04:21.740 --> 00:04:25.480
So if you're looking for any of the BLAs,
look first to the Accelerate framework

00:04:25.490 --> 00:04:29.310
where they've been closely matched to
the hardware on each of our platforms.

00:04:29.320 --> 00:04:32.460
And those come in all
the familiar flavors.

00:04:33.610 --> 00:04:37.470
And layered on top of that
is the gold standard of dense

00:04:37.490 --> 00:04:41.760
numerical linear algebra solvers,
LAPAC.

00:04:41.890 --> 00:04:45.610
And again, in all the familiar flavors.

00:04:49.180 --> 00:04:56.070
When we begin to talk about performance,
we're looking to the G5 as our flagship.

00:04:56.580 --> 00:05:00.950
It's really gotten us into a really
interesting place in the HPC space.

00:05:00.960 --> 00:05:05.130
And in my opinion, for scientific work,
it's the dual floating point

00:05:05.170 --> 00:05:06.780
cores that have taken us there.

00:05:06.870 --> 00:05:11.720
So for every 970 CPU,
you get two floating point cores

00:05:11.720 --> 00:05:16.590
capable of doing double and
single precision IEEE arithmetic.

00:05:18.570 --> 00:05:20.790
The CPU can dispatch to
both of those floating point

00:05:20.860 --> 00:05:22.830
instruction units on every cycle.

00:05:22.830 --> 00:05:25.980
It can start a new
operation on every cycle.

00:05:26.040 --> 00:05:30.740
All the basic arithmetic operations are
present as well as hardware square roots.

00:05:30.780 --> 00:05:37.910
That's new to PowerPC as we've
seen it anyway and good for us.

00:05:38.210 --> 00:05:42.220
The G5 also offers a class of
instructions called fused multiply add.

00:05:42.460 --> 00:05:45.380
These are three operand
instructions basically.

00:05:45.470 --> 00:05:48.530
It multiplies the first two together,
adds the third,

00:05:48.530 --> 00:05:51.320
and does that as one machine instruction

00:05:53.880 --> 00:05:59.950
Saving a rounding and adding,
finishing the job in smaller

00:05:59.950 --> 00:06:03.030
number of cycles than back to
back multiply followed by add.

00:06:03.080 --> 00:06:05.040
So fuse multiply add
fuses those together.

00:06:05.040 --> 00:06:07.340
Why do we make a fuss about this?

00:06:07.340 --> 00:06:09.350
There are some people I've heard who say,
"What's the big deal about

00:06:09.460 --> 00:06:12.460
fuse multiply add?" Well,
it's fundamental to linear algebra.

00:06:12.460 --> 00:06:15.040
It's the dot product,
the essential piece of the dot product.

00:06:15.040 --> 00:06:18.940
And that is fundamental
to matrix multiplication.

00:06:19.010 --> 00:06:21.800
It's a big part of the
fast Fourier transform.

00:06:21.800 --> 00:06:25.040
The butterflies are
essentially fuse multiply adds,

00:06:25.040 --> 00:06:26.880
multiply adds which we can fuse.

00:06:26.880 --> 00:06:31.120
And if you're doing function evaluation,
say by Horner's rule,

00:06:31.120 --> 00:06:36.870
you'll arrange polynomials in a way that
can take advantage of fuse multiply adds.

00:06:38.300 --> 00:06:40.910
So a FUSE multiply ad
counts for two flops,

00:06:40.910 --> 00:06:41.760
a multiply and an add.

00:06:41.810 --> 00:06:48.110
So we're credited with four
floating point ops per cycle.

00:06:48.270 --> 00:06:53.730
On a modern G5, dual G5,
that gives us eight flops across

00:06:53.850 --> 00:06:58.760
both processors per cycle and we
get 2.5 gigacycles per second.

00:06:59.060 --> 00:07:04.950
So we peak out at 20 theoretical double
precision floating point gigaflops,

00:07:05.010 --> 00:07:11.750
20 gigaflops on the new dual
G5 2.5 gigahertz Power Mac.

00:07:13.550 --> 00:07:15.990
So how do you take advantage
of this coming to the platform?

00:07:16.000 --> 00:07:21.260
Well, if you've already got compiled
binaries for Mac OS X,

00:07:21.300 --> 00:07:23.660
bring them to the G5, our flagship.

00:07:23.700 --> 00:07:27.160
They'll immediately see some advantage
from the ability of the CPU to schedule

00:07:27.210 --> 00:07:29.810
to both of those floating point cores.

00:07:31.120 --> 00:07:34.280
Recompile and you get even better
performance because now the compiler

00:07:34.280 --> 00:07:38.160
knows there are two floating point
units out there and can rearrange the

00:07:38.160 --> 00:07:44.730
order of operations in your code to take
advantage of both and make some efficient

00:07:44.730 --> 00:07:46.930
use of the dual floating point cores.

00:07:48.070 --> 00:07:50.760
And if you have an opportunity
to think about your algorithms,

00:07:50.760 --> 00:07:56.150
you may be able to cast them in ways that
can squeeze out a bit more performance.

00:07:56.800 --> 00:08:00.550
This kind of detail we've
paid to our libraries,

00:08:00.600 --> 00:08:03.030
LibM, VForce,
which I'll talk about in a moment,

00:08:03.140 --> 00:08:08.520
the BLAS, LAPAC, and our digital signal
processing libraries,

00:08:08.520 --> 00:08:08.920
VDSP.

00:08:13.940 --> 00:08:17.120
In the area of single
precision floating point,

00:08:17.120 --> 00:08:21.420
we have a very formidable capability
on both the G4 and the G5,

00:08:21.520 --> 00:08:23.180
the Altevec SIMD processor.

00:08:23.260 --> 00:08:26.530
It's a four way parallel
single precision engine.

00:08:26.640 --> 00:08:28.900
Again,
all the basic arithmetic operations

00:08:28.960 --> 00:08:30.940
and a vector fused multiply add.

00:08:31.120 --> 00:08:38.580
We top out here at 40 gigaflops
single precision on the new PowerMax.

00:08:38.630 --> 00:08:41.800
And there are some codes
that can get fairly close to

00:08:41.900 --> 00:08:43.450
using most or all of those.

00:08:43.580 --> 00:08:47.560
Convolutions are very,
very effective on that box.

00:08:51.140 --> 00:08:54.700
How do you get to high
performance on the AlteVec unit?

00:08:54.950 --> 00:08:56.110
You've got to work a little bit harder.

00:08:56.260 --> 00:08:59.780
You're really going to have to think
your algorithms through and cast

00:08:59.790 --> 00:09:04.740
them in terms of parallel operations.

00:09:04.760 --> 00:09:07.800
We have some advice on the
web about how to do that.

00:09:09.410 --> 00:09:11.800
But first of all,
it's probably wise to profile.

00:09:11.890 --> 00:09:14.680
And here's another plug
for the Chud folks.

00:09:14.680 --> 00:09:20.140
They have wonderful profiling tools that
will focus you on that 10% of the code

00:09:20.140 --> 00:09:21.890
where you're spending 90% of your time.

00:09:22.010 --> 00:09:24.010
Look there first.

00:09:24.470 --> 00:09:28.090
Auto vectorization is an
option and even a better option

00:09:28.170 --> 00:09:33.020
announced this week that GCC 3.5,
available later in the year,

00:09:33.100 --> 00:09:35.680
will have auto vectorization features.

00:09:35.710 --> 00:09:39.490
That's a good way to get
into the AlteVec game.

00:09:40.400 --> 00:09:43.740
and finally the level of detail
that gets really good Altevec

00:09:43.790 --> 00:09:47.690
performance and single precision
we've already paid in V-Force,

00:09:47.690 --> 00:09:49.960
the BLAS, VDSP and V-Image.

00:09:58.440 --> 00:10:02.180
How do you use these things?

00:10:02.210 --> 00:10:04.160
We try to make it straightforward.

00:10:04.160 --> 00:10:09.140
Try to hide at least a bit the nature
of the platform from your code.

00:10:09.140 --> 00:10:11.990
You call the API,
we'll dispatch to the proper code

00:10:12.320 --> 00:10:15.800
suited to the underlying chip.

00:10:17.160 --> 00:10:19.290
LibM, the math library,
links in by default.

00:10:19.340 --> 00:10:21.540
There's nothing special you need to do.

00:10:21.540 --> 00:10:25.550
If you want the long double
facilities and the complex APIs,

00:10:25.610 --> 00:10:28.480
we have LibMX, LibM Extended.

00:10:28.520 --> 00:10:33.030
That's a flag on the link line for GCC.

00:10:33.350 --> 00:10:37.160
And for our value added library,
the Accelerate framework,

00:10:37.280 --> 00:10:40.460
you simply specify dash
framework accelerate.

00:10:40.460 --> 00:10:42.950
It gets you on the air.

00:10:43.410 --> 00:10:47.000
and of course we ship these
performance libraries on every copy

00:10:47.000 --> 00:10:49.040
of Mac OS X that goes out the door.

00:10:49.170 --> 00:10:51.810
You can always expect to find it there.

00:10:53.530 --> 00:10:54.970
Well, what did we do that's new in Tiger?

00:10:55.000 --> 00:11:01.040
We've added a library called VForce.

00:11:01.040 --> 00:11:04.600
It had been called to our
attention that the C99 APIs for

00:11:04.600 --> 00:11:10.990
the familiar elementary functions
were data starved on our machines.

00:11:11.480 --> 00:11:16.430
We were seeing bubbles in the floating
point pipes that were going unused

00:11:16.430 --> 00:11:18.400
cycles and we hate to see those go by.

00:11:18.400 --> 00:11:27.110
And also the C99 and IEEE demand very
careful attention to the rounding

00:11:27.110 --> 00:11:31.400
modes and the way exceptions are
handled and that adds quite a bit

00:11:31.400 --> 00:11:36.400
of overhead for APIs that are only
processing just one operand at a time.

00:11:36.400 --> 00:11:41.400
So the ideas in V-force were to pass
many operands through a single call.

00:11:41.560 --> 00:11:45.400
For example,
if you need 768 values of the sine of X,

00:11:45.400 --> 00:11:51.400
well there's a call called VV sine F that
lets you pass all of them in at once.

00:11:51.400 --> 00:11:54.480
We amortize the overhead
and get back in a big,

00:11:54.480 --> 00:11:55.400
big hurry.

00:11:55.430 --> 00:12:00.190
And in fact that code runs about
12 times faster than a naive loop

00:12:00.270 --> 00:12:03.400
calling the traditional sine function.

00:12:04.760 --> 00:12:08.440
There's some caveats here.

00:12:08.490 --> 00:12:13.700
You have to expect IEEE default rounding
mode and you won't see any exceptions.

00:12:13.700 --> 00:12:20.450
We're expecting that you'll give
arguments that are within the

00:12:20.450 --> 00:12:24.660
domain of the function and so forth.

00:12:26.040 --> 00:12:30.900
Using these ideas opened up a
number of performance opportunities.

00:12:31.000 --> 00:12:33.070
On single precision, hit Altevec.

00:12:33.260 --> 00:12:34.000
Hit Altevec hard.

00:12:34.000 --> 00:12:37.930
That gives us four way
parallelism to begin with.

00:12:38.350 --> 00:12:41.080
Double precision, we've got two FPUs.

00:12:41.080 --> 00:12:43.990
Let's make sure we
schedule those effectively.

00:12:44.400 --> 00:12:47.840
do some software pipelining,
fill up those bubbles with

00:12:48.590 --> 00:12:51.850
sort of independent parallel
streams of computation.

00:12:52.270 --> 00:12:56.330
And then we take great care in choice
of algorithms to avoid branching,

00:12:56.340 --> 00:13:01.090
which is very tricky
on pipeline machines.

00:13:04.010 --> 00:13:07.250
Some caveats,
we're generally as accurate as the

00:13:07.250 --> 00:13:12.660
traditional libm elementary functions,
but we're not always bitwise identical.

00:13:13.710 --> 00:13:18.000
We handled nearly all the edge
cases according to the C99 specs.

00:13:18.000 --> 00:13:20.760
Plus and minus zero are
the occasional exceptions.

00:13:20.760 --> 00:13:23.640
There's documentation to tell you where.

00:13:24.100 --> 00:13:27.420
We make no alignment requirements,
but if you really want top performance

00:13:27.570 --> 00:13:31.720
aligned to 16 byte boundaries,
that lets our SIMD engines

00:13:32.120 --> 00:13:34.680
collect the data most efficiently.

00:13:36.080 --> 00:13:41.120
We're tuned for G5,
but we also run very well on G4 and G32.

00:13:41.240 --> 00:13:47.220
Here's what's in there, the inventory,
some simple division like functions,

00:13:47.380 --> 00:13:53.800
roots, X-mentals, logs, and powers,
trigonometrics, arc trigonometrics,

00:13:53.840 --> 00:13:57.240
hyperbolics,
and some integer manipulation.

00:13:59.330 --> 00:14:00.390
How do you code to these things?

00:14:00.480 --> 00:14:02.190
Couldn't be simpler.

00:14:02.290 --> 00:14:08.800
The blue on the top is C,
below is Fortran,

00:14:08.930 --> 00:14:15.190
and in orange the obvious
command line compilations.

00:14:18.270 --> 00:14:19.130
What else is new in Tiger?

00:14:19.140 --> 00:14:23.820
We updated to Atlas 3.6,
did some additional

00:14:23.820 --> 00:14:25.940
Mac OS X specific tune ups.

00:14:26.220 --> 00:14:31.220
We get some LA Pack performance
gain since it relies on those

00:14:31.240 --> 00:14:34.770
BLAs and from some compiler advances.

00:14:36.000 --> 00:14:40.490
Little performance chart showing in
blue our matrix multiply performance

00:14:41.090 --> 00:14:45.820
and in the cluster of orange,
green, and burnt orange,

00:14:45.820 --> 00:14:48.070
the matrix decompositions.

00:14:48.160 --> 00:14:52.240
LU, the symmetric decomposition,
LL transpose and the

00:14:52.240 --> 00:14:54.300
symmetric U transpose U.

00:14:54.300 --> 00:14:59.260
Notice that matrix multiply
tops out around 13 gigaflops

00:14:59.260 --> 00:15:04.930
on a new dual 2.5 gigahertz G5
Power Mac and the decompositions,

00:15:04.930 --> 00:15:05.590
let's say 11.

00:15:09.280 --> 00:15:14.810
Here's what the Xeon gets to,
a 3.0 Xeon running MKL6,

00:15:14.940 --> 00:15:17.630
matrix multiply topping
out well under 10,

00:15:17.630 --> 00:15:22.890
and the decomposition's
just about getting to 8.

00:15:24.080 --> 00:15:26.210
and finally Opteron.

00:15:26.210 --> 00:15:32.350
This is quite old numbers,
it looks about from last summer.

00:15:32.500 --> 00:15:34.400
They're topping out about
five and a half last summer.

00:15:34.400 --> 00:15:37.700
Let's give them 50% since
they've upclocked by that much.

00:15:37.700 --> 00:15:40.150
That would be perfect
scaling and that takes them,

00:15:40.150 --> 00:15:42.830
well, not quite to eight I think
on matrix multiply.

00:15:42.840 --> 00:15:47.840
So G5 cruising along near
13 and the Opteron eight,

00:15:47.840 --> 00:15:58.810
maybe nine and Xeon probably closer
to 12 in the 3.6 incarnation.

00:15:59.980 --> 00:16:02.800
All right,
finally we bring Long Double back

00:16:02.800 --> 00:16:03.660
to the Mac platform.

00:16:03.660 --> 00:16:08.860
It was present in Mac OS 9 and earlier,
and the complex Long Double,

00:16:08.860 --> 00:16:11.720
and the C99 TGMath type generic math.

00:16:11.720 --> 00:16:15.260
So you can say sign of a
complex number and the compiler

00:16:15.270 --> 00:16:18.150
figures out what you mean in C.

00:16:18.150 --> 00:16:19.610
Isn't that nice?

00:16:19.880 --> 00:16:24.820
I'm going to pass by this since I am
running very close to my time and maybe

00:16:24.820 --> 00:16:30.480
just jump to the end here where I show
on the left our elementary functions,

00:16:30.630 --> 00:16:34.440
number of cycles to get in and
out of our library functions,

00:16:34.500 --> 00:16:41.110
in the middle column what the
competition publishes for their x87,

00:16:41.200 --> 00:17:02.500
[Transcript missing]

00:17:04.250 --> 00:17:10.870
There's several documents out on the
web on our developer site that can get

00:17:11.030 --> 00:17:13.450
you started with this kind of stuff.

00:17:13.670 --> 00:17:16.350
We've already had the
Accelerate Framework talk.

00:17:16.490 --> 00:17:17.630
We're in this talk.

00:17:17.630 --> 00:17:21.540
If you haven't seen the Chud stuff,
by all means, please go see it.

00:17:23.400 --> 00:17:29.500
[Transcript missing]

00:17:36.330 --> 00:17:38.320
Thanks, Steve.

00:17:38.420 --> 00:17:41.900
So today I'm going to do kind of a brief
overview of Virginia Tech System 10,

00:17:41.900 --> 00:17:43.800
which I'm pretty sure
most of you have heard of.

00:17:43.800 --> 00:17:47.460
It's the 1100 node
cluster at Virginia Tech.

00:17:47.510 --> 00:17:50.510
And then we go into some detail
about some of the services in

00:17:50.510 --> 00:17:54.550
OS X that you can turn off,
briefly go over what kind of things

00:17:54.640 --> 00:17:58.030
we did at Virginia Tech to kind
of improve our benchmark scores,

00:17:58.200 --> 00:18:01.580
and very briefly I'm going to just
kind of go into some of the management

00:18:01.700 --> 00:18:04.130
tools that we use at Virginia Tech.

00:18:05.200 --> 00:18:09.600
So Virginia Tech System 10 is
1100 dual processor XSERVs.

00:18:09.680 --> 00:18:13.500
So that, of course, is 2200 PowerPC 970s.

00:18:13.510 --> 00:18:17.480
Each XSERV cluster node
has four gigs of memory,

00:18:17.510 --> 00:18:21.390
which gives us basically an
overall 4.4 terabytes of system

00:18:21.390 --> 00:18:24.730
memory for the whole cluster.

00:18:25.140 --> 00:18:27.950
One of the things that early on
when we were deploying this is,

00:18:27.950 --> 00:18:29.230
well, they've got to be running Linux.

00:18:29.360 --> 00:18:30.500
Why would they run OS X?

00:18:30.500 --> 00:18:32.200
Or maybe they're running Darwin.

00:18:32.250 --> 00:18:34.320
But we really are running OS X.

00:18:34.360 --> 00:18:36.600
And it's the OS X that
was shipped with it.

00:18:36.600 --> 00:18:39.900
We did a little bit of modifications,
which I'm going to kind of go into.

00:18:39.930 --> 00:18:42.790
But it is OS X.

00:18:44.740 --> 00:18:51.400
Briefly, our interconnect that we use
at Virginia Tech was InfiniBand

00:18:51.490 --> 00:18:55.420
and we went with 24 Mellanox
96 port InfiniBand switches.

00:18:55.600 --> 00:19:00.000
So these switches give basically
20 gigabits per second,

00:19:00.000 --> 00:19:03.000
a full duplex per port,
and that's basically 1.92

00:19:03.000 --> 00:19:06.430
terabits per second for the
overall bandwidth for the switch.

00:19:06.600 --> 00:19:10.950
We got about 12 microsecond latency
and that's across the entire network,

00:19:10.950 --> 00:19:14.570
about 8.5 microsecond latency
across just one switch.

00:19:14.600 --> 00:19:18.280
We use a fat tree topology,
which I kind of did a really rudimentary

00:19:18.280 --> 00:19:22.600
diagram at the bottom of the thing there
of what a kind of fat tree topology is.

00:19:22.600 --> 00:19:24.590
And in our case it's a half bisection.

00:19:24.680 --> 00:19:29.540
And half bisection means that at any
time if you have half the cluster

00:19:29.630 --> 00:19:32.890
trying to talk to the other half,
you're guaranteed basically

00:19:32.890 --> 00:19:36.600
half the bandwidth,
which in our case is basically 4.5.

00:19:36.630 --> 00:19:38.600
So that's about 5 gigabits per second.

00:19:38.600 --> 00:19:41.770
In addition to that we also have
a secondary gigabit network,

00:19:41.920 --> 00:19:46.600
which comprises six Cisco switches
with 240 ports each.

00:19:46.600 --> 00:19:50.600
And we basically use that to do
management and some file sharing

00:19:50.600 --> 00:19:54.170
to basically kind of get the system
up and running and kind of do some

00:19:54.170 --> 00:19:57.110
of the administrative stuff on it.

00:19:58.150 --> 00:20:03.480
Power and cooling can never be emphasized
enough when you're looking at clusters,

00:20:03.630 --> 00:20:05.540
especially this size.

00:20:05.620 --> 00:20:08.160
So at Virginia Tech we're lucky to
have this really wonderful computer

00:20:08.160 --> 00:20:12.140
facility that has basically three
megawatts of electrical power,

00:20:12.170 --> 00:20:14.880
half of which is basically
dedicated for System 10,

00:20:14.920 --> 00:20:15.730
the cluster.

00:20:15.730 --> 00:20:18.140
So we have dual
redundancy with our power,

00:20:18.140 --> 00:20:20.900
we have a UPS system,
and we actually have a diesel

00:20:20.940 --> 00:20:24.760
generator which is pretty much
the size of a diesel locomotive.

00:20:24.760 --> 00:20:27.400
It just sits back on a
pad and it's gigantic.

00:20:28.220 --> 00:20:33.680
So as I said, about half that,
1.5 megawatts, is reserved for System 10.

00:20:33.970 --> 00:20:38.650
Cooling, we have basically 2 million
BTUs of cooling capacity using

00:20:38.650 --> 00:20:41.020
Lieber's extreme density cooling.

00:20:41.040 --> 00:20:43.150
I'll have some pictures later
and I can kind of point it out,

00:20:43.200 --> 00:20:47.120
but basically it's a rack mounted
system where it kind of blows

00:20:47.120 --> 00:20:50.160
cold air from above and we also
have floor cooling as well.

00:20:50.160 --> 00:20:53.740
So it uses standard refrigerant,
overhead chillers.

00:20:53.800 --> 00:20:58.370
So we're looking at different kinds of
cooling and we had a regular data center.

00:20:58.790 --> 00:21:01.300
And the typical way to do that is you
have air conditioners throughout the room

00:21:01.300 --> 00:21:05.180
that basically bring in air from the top,
chill it, and kind of push it out

00:21:05.180 --> 00:21:05.800
throughout the floor.

00:21:05.870 --> 00:21:09.800
And then you basically put tiles in
the right places to get that air.

00:21:09.800 --> 00:21:13.470
So we looked at trying to do that
and if we did that we'd have had

00:21:13.470 --> 00:21:17.800
wind velocities of about 60 miles
per hour underneath the floor.

00:21:17.970 --> 00:21:19.590
So imagine just pulling
off one of those tiles,

00:21:19.600 --> 00:21:23.270
you get shot with that 60
miles per hour of wind.

00:21:24.960 --> 00:21:28.710
So that's basically what I'm
going to say about System 10.

00:21:28.780 --> 00:21:33.800
One of the things I'm going to overview
is some of the services in System 10.

00:21:33.850 --> 00:21:39.260
And we're going to turn off some
of them to optimize it slightly for

00:21:39.260 --> 00:21:42.140
more of the HPC type application.

00:21:42.190 --> 00:21:45.510
So OSN Server by default
comes with about 40 processes,

00:21:45.630 --> 00:21:48.340
just the regular default install.

00:21:48.370 --> 00:21:51.170
And so why do we want
to reduce the services?

00:21:51.240 --> 00:21:56.220
Well, one of course is we want to free up
resources like memory and CPU time.

00:21:56.240 --> 00:21:59.140
And increases some security.

00:21:59.140 --> 00:22:00.630
Obviously,
if you're not running things like a

00:22:00.630 --> 00:22:03.140
web server or something like that,
you're not going to have to

00:22:03.240 --> 00:22:04.440
worry about securing that.

00:22:04.450 --> 00:22:06.940
And it reduces the amount of
time for the system to start up,

00:22:06.950 --> 00:22:11.750
which kind of lowers your
mean time between failures.

00:22:12.130 --> 00:22:14.920
So one of the things I want to
emphasize is I always use this

00:22:14.930 --> 00:22:18.030
analogy for when you're turning
off services that it's kind of like

00:22:18.080 --> 00:22:20.440
these guys that buy the Honda Civic.

00:22:20.460 --> 00:22:22.860
And they basically rip out the engine.

00:22:22.860 --> 00:22:26.200
They put a turbocharger on it,
big spoilers on it.

00:22:26.220 --> 00:22:28.120
And they kind of soup it up.

00:22:28.210 --> 00:22:30.520
So they kind of design
it for their own purpose.

00:22:30.530 --> 00:22:34.630
And the problem with that is
Honda's not going to do any sort

00:22:34.630 --> 00:22:36.680
of hardware support for you.

00:22:36.840 --> 00:22:40.370
And so the thing to keep in
mind is if you're starting

00:22:40.470 --> 00:22:43.040
to turn off these services,
this isn't something Apple's

00:22:43.150 --> 00:22:44.260
going to recommend you to do.

00:22:44.260 --> 00:22:47.950
This is kind of what we
did here at Virginia Tech.

00:22:48.890 --> 00:22:50.870
So the first,
basically I'm going to kind of step

00:22:50.940 --> 00:22:53.910
through the different places in
OS X where you turn off services.

00:22:54.090 --> 00:22:56.790
And the first one is
the Etsy host config.

00:22:57.310 --> 00:22:59.640
and there in orange are
basically some of the services

00:22:59.640 --> 00:23:01.100
that run from ETSI Host Config.

00:23:01.100 --> 00:23:04.400
We have things like CupsD,
which is a printing service.

00:23:04.540 --> 00:23:06.260
We have the AutoMount,
which basically handles

00:23:06.730 --> 00:23:09.790
mounting removable file systems
and network file systems.

00:23:09.800 --> 00:23:12.730
And we have the Crash Reporter D,
which sort of sounds important,

00:23:12.730 --> 00:23:16.580
but basically it's just for creating
crash logs for the GUI applications.

00:23:16.580 --> 00:23:20.140
And we have Server Manager D,
which is basically if you

00:23:20.140 --> 00:23:23.980
ever used with the XServe,
Server Admin or Server Monitor or

00:23:23.980 --> 00:23:26.330
any of those tools,
this is what uses that.

00:23:26.420 --> 00:23:27.820
So that's one thing
I'd like to point out,

00:23:27.820 --> 00:23:30.570
is if you want to use these tools,
and they're great tools,

00:23:31.100 --> 00:23:34.310
you want to keep this service on.

00:23:34.610 --> 00:23:37.640
So basically this file has a list
of services and just changing the

00:23:37.640 --> 00:23:41.910
service equal yes to service equal no,
that will disable the service

00:23:41.910 --> 00:23:43.710
on the next time you reboot.

00:23:44.430 --> 00:23:48.460
The next thing we do, and this is kind of
blasphemy in the OS X thing,

00:23:48.460 --> 00:23:50.580
is we're going to turn off the GUI.

00:23:50.850 --> 00:23:53.180
Unfortunately,
we have 1,100 GUIs running.

00:23:53.180 --> 00:23:54.490
There's no real need to do that.

00:23:54.500 --> 00:23:56.800
No one's ever going to
actually see these GUIs.

00:23:56.950 --> 00:24:00.610
So the place to do that is an Etsy TTYS.

00:24:00.710 --> 00:24:03.830
And so I have,
basically there's this very

00:24:03.830 --> 00:24:07.690
complicated line there and
it needs to be commented out.

00:24:07.690 --> 00:24:10.240
And commenting out that one line
pretty much is going to prevent the

00:24:10.240 --> 00:24:13.600
Windows Server from running and the
login window thing from running.

00:24:13.780 --> 00:24:16.470
Actually this just does the login window.

00:24:16.740 --> 00:24:21.840
The Windows Server is in another place.

00:24:22.470 --> 00:24:24.690
Basically in OS X there's all
sorts of different places where

00:24:24.690 --> 00:24:25.600
these services get started.

00:24:25.600 --> 00:24:30.710
This one is in a directory
called etsymockinit.d.

00:24:30.760 --> 00:24:33.430
The way I disable it,
and this is just a personal preference,

00:24:33.520 --> 00:24:35.470
as opposed to just
I could delete the file,

00:24:35.470 --> 00:24:36.420
I could remove it.

00:24:36.420 --> 00:24:39.380
Instead I'm just going to create
another directory and move the

00:24:39.380 --> 00:24:40.840
script into that directory.

00:24:40.840 --> 00:24:43.550
That way if I change my mind later
I don't have to go find it or

00:24:43.550 --> 00:24:45.090
make sure it's the right thing.

00:24:45.090 --> 00:24:46.160
I just can move it back.

00:24:48.080 --> 00:24:50.410
So next thing that we turn
off is the ATS server,

00:24:50.410 --> 00:24:52.730
which basically provides font services.

00:24:52.840 --> 00:24:55.540
Since we disabled the GUI,
we're obviously not going to

00:24:55.540 --> 00:24:57.340
need font service on the system.

00:24:57.410 --> 00:24:59.440
That's another thing that gets
run out of Etsy Mocking at D.

00:24:59.440 --> 00:25:03.350
So basically just moving
that P list into the disabled

00:25:03.350 --> 00:25:05.590
directory is going to do that.

00:25:06.830 --> 00:25:11.390
The next thing that we're going to hit,
the next stop on turning off the

00:25:11.430 --> 00:25:14.370
services is basically modifying Watchdog.

00:25:14.500 --> 00:25:19.770
Watchdog is this process that
basically monitors your system to

00:25:19.780 --> 00:25:21.190
make sure that processes are running.

00:25:21.200 --> 00:25:24.360
If they aren't running,
it restarts those processes.

00:25:24.510 --> 00:25:28.160
Another thing that Watchdog does
that's really nice is that it enables

00:25:28.360 --> 00:25:30.350
the system to reboot if it crashes.

00:25:30.350 --> 00:25:34.630
So that's actually pretty nice in the
HPC thing because the system will come

00:25:34.630 --> 00:25:36.980
back up and hopefully rejoin the network.

00:25:36.980 --> 00:25:40.500
It kind of reduces your time between
failures because it's kind of almost

00:25:40.500 --> 00:25:42.480
like a self-healing kind of thing.

00:25:42.480 --> 00:25:45.100
So in etcwatchdog.conf,
we're going to disable two

00:25:45.100 --> 00:25:46.620
services that we don't need.

00:25:46.830 --> 00:25:49.230
The print service monitor,
obviously we're not going to be

00:25:49.230 --> 00:25:50.780
printing from the cluster nodes.

00:25:50.830 --> 00:25:52.180
And master.

00:25:52.360 --> 00:25:54.350
Master, I didn't know what it
was when I first started.

00:25:54.360 --> 00:25:57.300
But it sounded really important.

00:25:57.300 --> 00:26:02.640
It's actually just the main
server for the mail server.

00:26:02.840 --> 00:26:04.680
So we turn that off.

00:26:05.030 --> 00:26:08.970
One more thing that's on there is HWmonD,
which is the hardware monitor.

00:26:09.230 --> 00:26:11.800
And so basically this
thing is pulling every,

00:26:11.800 --> 00:26:14.080
I think, five seconds is the default.

00:26:14.120 --> 00:26:17.740
And basically it's just
keeping track of your hardware.

00:26:17.740 --> 00:26:21.880
So it keeps track of all your fans,
your temperatures throughout the system,

00:26:21.930 --> 00:26:23.960
and just kind of records that.

00:26:23.960 --> 00:26:26.860
And it can also send notifications,
stuff like that.

00:26:26.980 --> 00:26:30.480
So I thought every five
seconds is a little too much.

00:26:30.620 --> 00:26:33.620
So we kind of bumped
that up by adding -s 60.

00:26:33.670 --> 00:26:36.500
And that's going to make
it only run once a minute.

00:26:36.600 --> 00:26:40.290
So that kind of reduces the
CPU overhead of this one service.

00:26:42.190 --> 00:26:47.110
Next thing I turn off is MDNS Responder
and I have a feeling that this is not

00:26:47.390 --> 00:26:49.920
going to be something we're going to
be able to turn off as more and more

00:26:49.920 --> 00:26:53.080
things start to rely on Rendezvous.

00:26:53.100 --> 00:26:56.300
So we have things like XGrid and if
you plan on using XGrid you don't

00:26:56.400 --> 00:26:59.740
want to turn this thing off because
XGrid is going to use Rendezvous

00:26:59.740 --> 00:27:04.100
to find other compute nodes.

00:27:04.100 --> 00:27:08.470
And there are things like if you ever
want to use the distributed compile

00:27:08.470 --> 00:27:11.100
option in Xcode it also uses Rendezvous.

00:27:11.140 --> 00:27:14.170
So if you don't plan on doing
anything with Rendezvous this is

00:27:14.170 --> 00:27:16.100
definitely something you can turn off.

00:27:16.140 --> 00:27:20.570
It will reduce the amount of
network stuff it sends out and

00:27:20.570 --> 00:27:23.100
a little bit of CPU overhead.

00:27:23.100 --> 00:27:27.170
So this one basically has a script
in the system library startup

00:27:27.170 --> 00:27:32.780
items folder and basically I just
comment out the line that starts it.

00:27:35.120 --> 00:27:41.360
So in OS X there's lots of different
places to kind of look for services and

00:27:41.470 --> 00:27:46.650
you kind of see there the grayed out
services that we kind of went through.

00:27:46.690 --> 00:27:50.430
And there are some things on there
that people disagree with that either

00:27:50.430 --> 00:27:52.240
need to stay or some things need to go.

00:27:52.390 --> 00:27:54.770
Like I leave the time server
on there because I think that's

00:27:54.850 --> 00:27:57.240
important for the cluster that I run.

00:27:57.240 --> 00:27:59.720
I leave cron turned on because we
actually use cron to kind of do

00:27:59.720 --> 00:28:01.780
things every so often on the cluster.

00:28:01.810 --> 00:28:06.100
But some people can turn that off
and not have any issues with that.

00:28:07.410 --> 00:28:11.230
So I'm going to talk about the
LINPACK optimizations and the kind

00:28:11.230 --> 00:28:14.380
of things we did at Virginia Tech.

00:28:14.380 --> 00:28:17.890
LINPACK is basically the benchmark
that's used in the top 500 list.

00:28:18.100 --> 00:28:22.140
So we were number third
in the world in November.

00:28:22.630 --> 00:28:28.160
And so the way this is established is we
have to run this benchmark called HPL.

00:28:28.490 --> 00:28:33.000
So we had some,
this was about a year ago,

00:28:33.010 --> 00:28:35.390
and this was before a lot
of the optimizations went

00:28:35.390 --> 00:28:36.780
into the XLR8 framework.

00:28:36.930 --> 00:28:41.830
So we had a person in
Japan named Kazushiki Goto,

00:28:41.830 --> 00:28:45.860
and he did basically some assembly level
optimizations on major subroutines.

00:28:45.910 --> 00:28:50.180
Basically the DGEM subroutines.

00:28:50.180 --> 00:28:52.470
I have a website there for more
information if you want to kind

00:28:52.630 --> 00:28:54.770
of look at his optimizations.

00:28:54.780 --> 00:28:57.360
One of the things that we had
to do though is we had to kind

00:28:57.370 --> 00:29:00.160
of write our own memory manager.

00:29:00.160 --> 00:29:05.540
Because the blast routines that he
was writing did a much better job

00:29:05.540 --> 00:29:08.580
if it was guaranteed a contiguous
physical amount of memory.

00:29:08.580 --> 00:29:14.030
As opposed to having it kind of
get segmented or partitioned.

00:29:14.270 --> 00:29:17.820
So with those optimizations,
we actually had about a 10% increase

00:29:17.890 --> 00:29:20.190
over the Apple Veclib at the time.

00:29:20.200 --> 00:29:22.480
Remember, this was using Jaguar,
so we didn't have XLR8.

00:29:22.480 --> 00:29:25.030
We were still using Veclib.

00:29:25.170 --> 00:29:29.720
So with the optimizations and some of the
tweaking that we did at Virginia Tech,

00:29:29.810 --> 00:29:32.160
we actually got 10.28
teraflops per second,

00:29:32.160 --> 00:29:35.000
which was the third fastest in the world.

00:29:35.000 --> 00:29:37.600
And without those go-to optimizations,
we probably would have

00:29:37.720 --> 00:29:41.060
gotten around 8.4 teraflops,
which on that list probably

00:29:41.060 --> 00:29:42.990
would have put us around fourth.

00:29:45.210 --> 00:29:47.300
Very quickly,
I just want to kind of go over some

00:29:47.300 --> 00:29:48.980
of the system management stuff.

00:29:49.050 --> 00:29:53.310
And I can talk for maybe two hours
or five hours or 12 hours on this.

00:29:53.410 --> 00:29:56.170
It's something that
I do a lot of work with.

00:29:56.260 --> 00:29:59.050
The tool that I love for system
management is called Ganglia.

00:29:59.140 --> 00:30:02.330
And I know like BioTeam uses
it inside their package.

00:30:02.440 --> 00:30:05.160
And what's really great about
Ganglia is it runs on each system.

00:30:05.160 --> 00:30:08.530
It kind of just gets system
status and kind of broadcasts

00:30:08.530 --> 00:30:10.180
that out on the network.

00:30:10.400 --> 00:30:12.550
So by default it has a couple
of displays and I have a few

00:30:12.550 --> 00:30:14.700
of their displays at the top.

00:30:14.700 --> 00:30:16.660
Like at the top there's a
cluster load percentage.

00:30:16.670 --> 00:30:17.810
So it's kind of really great.

00:30:17.900 --> 00:30:20.920
You can see what's going
on with your 1100 systems.

00:30:20.920 --> 00:30:24.110
You know, you kind of get a,
take a step back and be able to see

00:30:24.500 --> 00:30:26.320
what's really going on in the cluster.

00:30:26.350 --> 00:30:29.540
And what I love about that
is you can drill down.

00:30:29.600 --> 00:30:32.550
So we have that big cluster overview,
but you can drill down and

00:30:32.550 --> 00:30:34.080
look at like a specific node.

00:30:34.080 --> 00:30:37.640
And what I love about it
is that it's XML data.

00:30:37.690 --> 00:30:38.800
So you can parse that XML data.

00:30:38.890 --> 00:30:42.420
So we at Virginia Tech made a,
basically a kind of a custom

00:30:42.420 --> 00:30:45.340
display there that kind of shows
us a physical representation

00:30:45.340 --> 00:30:46.720
of what our cluster's doing.

00:30:46.790 --> 00:30:49.810
So we can see if a CPU's kind of
doing something weird or if we can

00:30:49.810 --> 00:30:51.930
look at temperatures and loads.

00:30:52.000 --> 00:30:53.650
Kind of get a physical view of it.

00:30:53.670 --> 00:30:57.170
And it really helps with just
quickly discovering what's

00:30:57.170 --> 00:30:58.930
going on with our system.

00:30:59.880 --> 00:31:06.280
So, things I talked about of course were,
you know, overviewing our System 10,

00:31:06.320 --> 00:31:10.260
reducing the number of services,
and what we did on the LIMPAC scores and

00:31:10.270 --> 00:31:12.740
some of the management features we did.

00:31:12.800 --> 00:31:15.930
So people of course are,
if you went to Dr.

00:31:15.930 --> 00:31:19.800
Vrajanan's presentation yesterday,
you probably saw some of this,

00:31:19.800 --> 00:31:22.770
but people keep asking us,
so what's going on with System 10?

00:31:22.800 --> 00:31:27.360
So, you know, we dropped off the list and
it's because we swapped out our

00:31:27.360 --> 00:31:29.800
PowerMax and we're upgrading XSERVs.

00:31:29.900 --> 00:31:33.470
So I can say that people are
very hard at work installing

00:31:34.030 --> 00:31:36.800
systems and we have about 850 in.

00:31:36.800 --> 00:31:39.800
And so there's some of
the racks that we have.

00:31:39.800 --> 00:31:42.880
And, you know,
one of the things that is really

00:31:42.880 --> 00:31:46.450
interesting is that we have,
we're using basically a third of the

00:31:46.610 --> 00:31:47.800
space that we do with the PowerMax.

00:31:47.800 --> 00:31:50.740
So we only have one aisle where
we can do all the cabling.

00:31:50.800 --> 00:31:52.800
And so it gets kind of crowded.

00:31:52.800 --> 00:31:55.790
And there's, I don't know how many
people are in that picture,

00:31:55.800 --> 00:31:59.780
but that's a small space
for a whole bunch of people.

00:31:59.800 --> 00:32:02.800
And that's basically us doing
the wiring in the background.

00:32:02.800 --> 00:32:05.740
We have to wire your Ethernet,
do the power, and run an InfiniBand.

00:32:05.800 --> 00:32:09.300
So quite a bit of cabling going on.

00:32:10.280 --> 00:32:16.210
So with that, I'm going to introduce
Dean Dauger from Dauger Research.

00:32:22.830 --> 00:32:24.480
Thank you, Josh.

00:32:24.520 --> 00:32:27.800
Yes, so let's see.

00:32:27.800 --> 00:32:30.440
It's definitely a pleasure to be
here today and to be speaking to you.

00:32:30.440 --> 00:32:36.270
I very much appreciate the kind people at
Apple to invite me to come out and talk

00:32:36.270 --> 00:32:40.120
about plug and play clustering and how
you can build your cluster in minutes.

00:32:40.480 --> 00:32:44.060
And so what I'd like to go
over first is an outline of

00:32:44.060 --> 00:32:46.290
what I'd like to talk about.

00:32:46.300 --> 00:32:49.220
And first of all, why parallel computing,
why parallel computing

00:32:49.220 --> 00:32:52.460
was interesting to do,
and what we did to go about inventing

00:32:52.460 --> 00:32:56.720
or essentially reinventing the cluster,
inventing the Mac cluster,

00:32:56.720 --> 00:33:00.040
and an introduction to
basic message passing code.

00:33:00.080 --> 00:33:04.190
And then a description of how you
can build your own Mac cluster,

00:33:04.190 --> 00:33:07.010
and hopefully if the
demo gods are kind to us,

00:33:07.090 --> 00:33:10.250
I can show you what we
can do with a Mac cluster.

00:33:10.480 --> 00:33:11.480
Thank you.

00:33:11.770 --> 00:33:14.080
So why parallel computing?

00:33:14.110 --> 00:33:16.470
Really,
parallel computing is good for problems

00:33:16.520 --> 00:33:20.780
that are too large to solve in one
sense or another on one computer.

00:33:20.780 --> 00:33:23.190
The simple reason of simply
taking up too much time,

00:33:23.190 --> 00:33:25.490
too much CPU time,
but also in some cases,

00:33:25.500 --> 00:33:29.260
or in many cases I know,
it requires too much memory.

00:33:29.260 --> 00:33:32.510
Some problems can easily
outgrow the RAM capacity that's

00:33:32.510 --> 00:33:34.220
available on a single box.

00:33:34.220 --> 00:33:37.800
And I know codes that run 15 billion
particles and it has to keep all

00:33:37.800 --> 00:33:40.790
that data all in RAM and so multiply
that by however many dozens of

00:33:41.100 --> 00:33:45.320
bytes per particle and you can see
that's quite a bit of memory space.

00:33:45.340 --> 00:33:51.140
So the other thing that's happened in the
last decade or so is that the programming

00:33:51.140 --> 00:33:55.020
API has become standardized on what's
known as message passing interface,

00:33:55.020 --> 00:33:57.030
also known as MPI.

00:33:57.050 --> 00:34:00.970
It's a specification that was
established in 1994 and by the end

00:34:01.110 --> 00:34:04.930
of the 90s it became the dominant
software interface that's available

00:34:04.930 --> 00:34:08.460
at supercomputing centers such as
the San Diego Supercomputing Center.

00:34:08.460 --> 00:34:14.000
As well as NERSC and also
on many cluster systems.

00:34:14.000 --> 00:34:18.110
And so this development
enabled the possibility of

00:34:18.110 --> 00:34:22.740
having portable parallel code,
code that's portable between the

00:34:22.740 --> 00:34:27.340
supercomputing centers and the
clusters in both 4G and C by using MPI.

00:34:27.340 --> 00:34:30.450
And that's been a real
benefit to scientists and many

00:34:30.450 --> 00:34:32.350
other users of such systems.

00:34:33.890 --> 00:34:36.900
So to give you an idea of
some of our experience,

00:34:37.020 --> 00:34:40.870
this is a current picture of
the UCLA physics Apple C cluster

00:34:40.870 --> 00:34:42.810
established in 1998.

00:34:42.860 --> 00:34:47.080
And as you can see,
we use a mixture of G5s and G4s

00:34:47.080 --> 00:34:49.010
connected with a fast switch.

00:34:49.100 --> 00:34:53.640
And we are running on a mix of OS X,
various versions of OS X,

00:34:53.640 --> 00:34:54.700
as well as OS 9.

00:34:54.700 --> 00:34:57.980
So we're able to mix and match
nodes older and newer hardware.

00:34:58.080 --> 00:35:01.880
And then we can combine this cluster
with machines that are on people's desks,

00:35:01.880 --> 00:35:06.640
such as my colleagues' professors
or postdocs or graduate students.

00:35:06.690 --> 00:35:09.450
Combine them as we need to
when they're away to go home

00:35:09.550 --> 00:35:10.940
from work or on their vacation.

00:35:10.960 --> 00:35:14.420
Or if a colleague needs time
just before a conference,

00:35:14.470 --> 00:35:15.820
they can go ahead and
just use the machines,

00:35:15.820 --> 00:35:18.020
ask permission,
and involve them together.

00:35:18.020 --> 00:35:21.220
And that's really saved
a lot of people's work.

00:35:21.250 --> 00:35:24.350
And just a little quick note,
this is a picture just from last week,

00:35:24.430 --> 00:35:26.860
the Dawson cluster.

00:35:26.880 --> 00:35:29.620
It's going to be 256x dual processors.

00:35:29.700 --> 00:35:30.820
Currently, 128 online.

00:35:30.820 --> 00:35:31.860
It was literally physical.

00:35:31.930 --> 00:35:33.820
It was technically just
assembled last week.

00:35:33.900 --> 00:35:36.880
And we were able to get this picture
connected with Gigabit running 10.3.

00:35:36.880 --> 00:35:40.670
So we'll be definitely having some
results of that later in the month.

00:35:41.180 --> 00:35:45.170
So cluster computing with Mac OS X,
essentially we went about

00:35:45.170 --> 00:35:48.340
reinventing the cluster computer.

00:35:48.420 --> 00:35:54.520
And it really is a very nice
approach to cluster computing,

00:35:54.690 --> 00:35:58.430
much more reliable than many other
systems that I'm familiar with.

00:35:58.480 --> 00:36:02.680
It's independent of shared storage or any
kind of command line login or static data

00:36:02.680 --> 00:36:05.350
like machine lists or static IP files.

00:36:05.600 --> 00:36:08.790
And so that leads a lot to a great
deal of reliability because you

00:36:09.060 --> 00:36:12.460
don't have to make sure that every
little switch is just right in

00:36:12.470 --> 00:36:14.400
order for the cluster to work.

00:36:14.400 --> 00:36:17.920
And so this all results in the
lowest barrier to entry for people

00:36:17.920 --> 00:36:21.740
who are using clusters and really
saves a lot of time and money.

00:36:21.740 --> 00:36:25.750
And really the purpose of this whole
approach is to be able to enable users

00:36:25.920 --> 00:36:29.270
to focus on getting useful work done
so they don't have to be bogged down

00:36:29.270 --> 00:36:30.800
with the mechanics of the cluster.

00:36:30.800 --> 00:36:33.140
They can actually get real
research and real work done.

00:36:33.140 --> 00:36:38.700
And that was our motivation to be able
to assemble and design the Mac cluster.

00:36:38.700 --> 00:36:40.810
So, we're going to do a little bit of

00:36:40.900 --> 00:37:59.700
[Transcript missing]

00:38:00.220 --> 00:38:04.130
So let me give you an introduction
to parallel code using MPI.

00:38:04.140 --> 00:38:08.390
Basically, it's code that coordinates
its work using messages.

00:38:08.510 --> 00:38:11.390
The model is that there are
n tasks or virtual processors

00:38:11.460 --> 00:38:14.380
that are running simultaneously,
and you label them from 0 to n minus 1.

00:38:14.560 --> 00:38:18.090
These executables often use this
identification data to determine what

00:38:18.110 --> 00:38:21.880
part of the work they're going to do
and how to coordinate work between them.

00:38:22.020 --> 00:38:26.510
And so they pass messages between all
these virtual processors or tasks to

00:38:26.510 --> 00:38:29.150
organize the data and organize the work.

00:38:29.230 --> 00:38:31.930
It's really analogous to a number of
employees at a company who make phone

00:38:31.930 --> 00:38:35.940
calls with each other or have meetings
to be able to coordinate work and

00:38:35.960 --> 00:38:39.010
to accomplish a much larger project.

00:38:39.180 --> 00:38:41.570
Any group of tasks can communicate,
which implies there are order

00:38:41.570 --> 00:38:44.700
N square connections that
are supported by the MPI.

00:38:44.810 --> 00:38:46.640
And that can support
simple sends and receives,

00:38:46.690 --> 00:38:49.090
as well as collective calls,
such as broadcasts,

00:38:49.150 --> 00:38:51.460
where you're sending from
one task to all the others,

00:38:51.520 --> 00:38:55.310
or gather where you're collecting data,
say for data output,

00:38:55.330 --> 00:38:59.190
or reduction operations,
such as computing the maximum of an

00:38:59.190 --> 00:39:02.470
array that's spread across the cluster,
or the sum,

00:39:02.470 --> 00:39:04.730
or other parameters like that.

00:39:04.800 --> 00:39:10.240
And also matrix operations such as
transpose and vector operations.

00:39:10.340 --> 00:39:13.800
And synchronization is not
required in between the tasks.

00:39:13.800 --> 00:39:16.540
No precise synchronization is necessary,
but it's only implied by the fact

00:39:16.540 --> 00:39:18.880
that messages need to be able to
get from one task to the other.

00:39:18.880 --> 00:39:21.820
So to give you an idea
of what it looks like,

00:39:21.820 --> 00:39:25.770
I'll introduce a simple example,
a simple example I know

00:39:25.770 --> 00:39:29.660
of message passing called,
we call parallel knock.

00:39:29.720 --> 00:39:34.200
And in this case, in this diagram,
the time axis is down.

00:39:34.200 --> 00:39:34.780
And we have a time axis that's down.

00:39:34.780 --> 00:39:36.640
And we have two tasks that are
communicating with each other.

00:39:36.640 --> 00:39:40.350
At first,
task zero sends a message to task one.

00:39:40.360 --> 00:39:42.280
And then they both print that message.

00:39:42.280 --> 00:39:44.610
And so task zero prints
the message it just sent,

00:39:44.610 --> 00:39:47.180
and task one prints the
message it just received.

00:39:47.180 --> 00:39:51.430
And then a reply is sent back
from task one to task zero,

00:39:51.460 --> 00:39:54.560
which is then printed by both tasks.

00:39:54.560 --> 00:39:57.570
And so task zero prints the
message it just received,

00:39:57.570 --> 00:40:00.800
the reply, and task one prints
the reply it just sent.

00:40:00.840 --> 00:40:04.180
So to give you an idea of
what the code looks like,

00:40:05.700 --> 00:41:22.100
[Transcript missing]

00:41:24.480 --> 00:41:28.270
Now the next example I'd like
to go over is Pascal's triangle.

00:41:28.410 --> 00:41:30.860
This is an example that
illustrates local propagation.

00:41:31.100 --> 00:41:33.430
Propagation,
what I mean is that every element

00:41:33.430 --> 00:41:37.440
eventually interacts with every
other element in the problem.

00:41:37.530 --> 00:41:44.360
But the interactions are all local
because any one element is simply

00:41:44.360 --> 00:41:47.240
the sum of the two neighboring
elements in the preceding line.

00:41:47.330 --> 00:41:50.060
And so eventually they all
interact with each other,

00:41:50.200 --> 00:41:52.150
but every interaction itself is local.

00:41:52.430 --> 00:41:57.400
And this is similar to a variety of
physical problems such as fluid modeling

00:41:57.400 --> 00:42:02.890
where you're looking at fluid flow
through like inside a plasma or inside

00:42:03.000 --> 00:42:05.360
a blood vessel or things like that.

00:42:05.420 --> 00:42:08.370
And you can use partial, one tool,
you can use partial differential

00:42:08.450 --> 00:42:12.250
equations for that where you're
having neighboring elements

00:42:12.250 --> 00:42:14.000
interact with each other.

00:42:14.090 --> 00:42:18.460
As well as elastic deformation which
occurs when you're trying to simulate say

00:42:18.460 --> 00:42:24.150
using finite element modeling to be able
to understand how the earth's crust is

00:42:24.230 --> 00:42:27.960
going to deform when say a fault slips.

00:42:28.040 --> 00:42:30.580
Or a Gaussian blur where you're
talking about one point spreading this

00:42:30.580 --> 00:42:34.260
information to all its neighboring ones
and so forth using localized convolution.

00:42:34.320 --> 00:42:37.090
Or molecular dynamics where you have
molecules interacting with each other

00:42:37.090 --> 00:42:40.580
in a local manner or certain parts
of particle based plasma models.

00:42:40.680 --> 00:42:43.760
Are all,
those kinds of codes are all good

00:42:43.910 --> 00:42:46.510
examples of local propagation.

00:42:47.270 --> 00:42:50.460
So in parallel Pascal's triangle,
the way that you recognize

00:42:50.460 --> 00:42:53.620
where the message passing is,
is the first layout that you

00:42:53.620 --> 00:42:57.380
can think of this as the time
axis being down from one to one,

00:42:57.380 --> 00:42:58.140
one, and so forth.

00:42:58.200 --> 00:43:01.910
And the thing to recognize is to
understand where the communication

00:43:01.910 --> 00:43:05.880
is happening in the problem in
order to perform the computation.

00:43:05.880 --> 00:43:09.660
And so what I've drawn here is all
the arrows indicating all the places

00:43:09.760 --> 00:43:13.660
where there's a certain amount of
information being propagated or data

00:43:13.660 --> 00:43:16.320
being propagated from element to element.

00:43:17.070 --> 00:43:21.230
And so the thing to recognize is that
when you partition the problem up,

00:43:21.360 --> 00:43:23.900
let's say up into three
different sections,

00:43:24.090 --> 00:43:27.090
and you can recognize that a
certain amount of information or

00:43:27.090 --> 00:43:30.540
a certain amount of data being
propagated through the partitions in

00:43:30.540 --> 00:43:32.170
between each section of the problem.

00:43:32.200 --> 00:43:35.850
And so you can handle all the
internal communication as normal

00:43:35.850 --> 00:43:39.630
for any single processor code,
but then the MPI calls correspond

00:43:39.630 --> 00:43:43.710
to the arrows that cross the
red boundaries that are here.

00:43:43.770 --> 00:43:47.220
But by choosing this method,
this arrangement for the partitioning,

00:43:47.220 --> 00:43:49.920
the computation becomes proportional
to the volume of the problem

00:43:49.920 --> 00:43:52.870
and the communication becomes
proportional to the surface area.

00:43:52.890 --> 00:43:57.080
So you can think of it sort of physically
that you'll probably end up with a

00:43:57.080 --> 00:44:03.090
good communication to computation
ratio with this kind of organization.

00:44:03.280 --> 00:44:05.190
So by splitting it up into
the three different sections,

00:44:05.210 --> 00:44:07.430
imagine you have three
different tasks running,

00:44:07.530 --> 00:44:09.590
these are the messages
being sent and received.

00:44:09.700 --> 00:44:12.470
So that for every odd and even line,
you're sending messages either

00:44:12.470 --> 00:44:14.360
to the left or to the right,
to the left or to the right

00:44:14.490 --> 00:44:16.750
for every alternate line.

00:44:16.850 --> 00:44:19.820
And so for the computation,
all the computation needs to

00:44:19.870 --> 00:44:24.430
know is simply that there's
an array to form and take,

00:44:24.480 --> 00:44:27.720
to compute the value of
an element in one line,

00:44:27.720 --> 00:44:29.590
you simply sum the previous two.

00:44:29.700 --> 00:44:32.830
But what the message passing does,
it fills in the gaps as it needs to,

00:44:32.830 --> 00:44:36.010
to be able to propagate the
information in between each section.

00:44:36.100 --> 00:44:39.470
And so you can see, say,
the left edge of the middle

00:44:39.470 --> 00:44:43.160
task is a duplicate of the
right edge of the left task.

00:44:43.290 --> 00:44:46.600
And so the fact that there's a duplicate,
this is also known as guard cells,

00:44:46.680 --> 00:44:50.960
where you're able to set up these
kinds of guard cells to allow the

00:44:50.960 --> 00:44:54.040
computation to proceed as if it was
the only process you're running,

00:44:54.110 --> 00:44:56.550
but then the MPI simply fills
in the guard cells at the

00:44:56.550 --> 00:44:58.090
moment where it's needed.

00:44:58.170 --> 00:45:02.150
And so this is actually a fairly
prototypical example of a lot of

00:45:02.150 --> 00:45:04.630
local propagation type problems.

00:45:05.320 --> 00:45:08.900
So to show you a code example,
again this is available

00:45:08.900 --> 00:45:10.820
in Fortran as we'll see.

00:45:10.820 --> 00:45:13.660
In this case,
this if statement is alternating between

00:45:13.670 --> 00:45:15.780
odd and even lines of the problem.

00:45:15.780 --> 00:45:19.520
And for example, we start at the top part
of the if statement.

00:45:19.520 --> 00:45:23.290
We have an MPI receive that's
performed on the right edge of the

00:45:23.300 --> 00:45:25.560
array from the right processor.

00:45:25.560 --> 00:45:27.960
And what I mean is that
it's an immediate receive,

00:45:27.960 --> 00:45:31.430
it immediately returns,
also known as an asynchronous receive.

00:45:32.040 --> 00:45:34.980
So you're allowed to continue to
execute while the receive is happening.

00:45:34.980 --> 00:45:38.880
And then an MPI send is performed
on the left part of the array,

00:45:38.910 --> 00:45:41.440
array zero, to the left processor.

00:45:41.440 --> 00:45:44.720
And then an MPI wedge is performed to
be able to balance out the I receive and

00:45:44.720 --> 00:45:46.800
complete the I receive that came before.

00:45:46.800 --> 00:45:48.860
So in this case,
since everybody is sending

00:45:48.870 --> 00:45:51.560
something to the left,
that means that you're receiving

00:45:51.560 --> 00:45:53.060
something from the right.

00:45:53.060 --> 00:45:54.470
And so that's what that corresponds to.

00:45:54.480 --> 00:45:57.150
Likewise,
in the lower half of the statement,

00:45:57.150 --> 00:46:00.980
we're doing an I receive from the
left and then a send to the right.

00:46:01.840 --> 00:46:03.290
And then a wait to complete the receive.

00:46:03.320 --> 00:46:05.950
And so we're all sending
to the right instead.

00:46:05.960 --> 00:46:08.760
And so the result of
this code is like this.

00:46:08.800 --> 00:46:11.690
If we divide it up into
three different tasks,

00:46:11.950 --> 00:46:14.630
and the way that this is drawn,

00:46:15.680 --> 00:46:19.040
is that all the odd values is
drawn with an asterisk and all

00:46:19.040 --> 00:46:21.140
the even values has a space.

00:46:21.140 --> 00:46:23.260
And so we can see that
they're essentially task

00:46:23.260 --> 00:46:30.010
one has a seed at the top,
which then propagates through and

00:46:30.010 --> 00:46:33.130
propagates out to the boundaries
across the partitions into the

00:46:33.130 --> 00:46:33.130
sections on task zero and task two.

00:46:33.490 --> 00:46:38.200
And so by forming, by arranging this way,
we can see that we actually

00:46:38.340 --> 00:46:40.430
have maintained our guard cells.

00:46:40.440 --> 00:46:42.900
If you look carefully,
the left edge of task one is identical

00:46:42.900 --> 00:46:44.610
to the right edge of task zero.

00:46:44.780 --> 00:46:48.500
And so those guard cells will
be maintained by the MPI.

00:46:48.500 --> 00:46:50.460
So we see that's successful there.

00:46:50.550 --> 00:46:53.080
And the other thing that
we see out of this way,

00:46:53.080 --> 00:46:56.300
this forms a shape also known
as the Sierpinski gasket

00:46:56.300 --> 00:46:57.960
in the Pascal's triangle.

00:46:57.980 --> 00:47:03.220
So we're able to perform this
problem using MPI in this way.

00:47:04.230 --> 00:47:07.720
So that's just one of many possible
message passing patterns that are

00:47:08.570 --> 00:47:10.670
available that are supported using MPI.

00:47:10.850 --> 00:47:12.620
And for example,
that's the example of the

00:47:12.620 --> 00:47:14.240
nearest neighbor on the left.

00:47:14.390 --> 00:47:17.660
And of course, the arrows are reversible,
so you can do left, right, left, right.

00:47:17.670 --> 00:47:20.580
And then the upper left is another
common message passing pattern,

00:47:20.580 --> 00:47:21.800
also known as master/slave.

00:47:21.800 --> 00:47:24.220
This is something that
is relatively simple.

00:47:24.220 --> 00:47:28.200
And so in this case,
it shows a broadcast from

00:47:28.200 --> 00:47:29.880
one task to all the others.

00:47:29.950 --> 00:47:32.160
Or you can reverse this and, of course,
do a gather.

00:47:32.230 --> 00:47:34.800
But also,
the all-to-all communications pattern,

00:47:34.800 --> 00:47:37.180
where every node is
communicating to every other one,

00:47:37.230 --> 00:47:40.340
that's very important for
data transposes of matrices.

00:47:40.350 --> 00:47:43.380
And that's important, say,
for performing a very

00:47:43.470 --> 00:47:45.670
large 1D FFT in parallel.

00:47:45.840 --> 00:47:47.940
You have to go through
data transposes and,

00:47:47.940 --> 00:47:49.720
consequently,
the message passing patterns.

00:47:49.720 --> 00:47:52.400
You have a lot of
all-to-all communications.

00:47:52.430 --> 00:47:53.780
Or you could have other
message passing patterns,

00:47:53.780 --> 00:47:56.160
like a tree,
where one is sending to two others,

00:47:56.160 --> 00:47:57.760
and they, in turn, send to two others.

00:47:57.820 --> 00:48:00.120
Or in a regular pattern
for a more regular problem,

00:48:00.130 --> 00:48:02.800
or any combination of the above,
or any multidimensional

00:48:02.800 --> 00:48:04.200
versions of any of these.

00:48:04.230 --> 00:48:08.360
So these are all things that are
possible with MPI and are important

00:48:08.360 --> 00:48:11.700
for a variety of interesting problems.

00:48:11.700 --> 00:48:14.460
So to give an idea of some of
those interesting problems,

00:48:14.460 --> 00:48:18.060
these are the applications that
have been run on MAC clusters

00:48:18.120 --> 00:48:19.960
that I'm familiar with.

00:48:19.960 --> 00:48:22.480
And for example,
on the upper left is a picture

00:48:22.480 --> 00:48:24.470
of the electric Tokamak device.

00:48:24.510 --> 00:48:29.450
So Tokamak is a plasma device
that attempts to hold a plasma in

00:48:29.450 --> 00:48:34.160
confinement in a ring shape pattern,
or torus shape pattern.

00:48:34.160 --> 00:48:40.220
And one of the things about many kinds
of Tokamaks is that the plasma in it

00:48:40.220 --> 00:48:42.440
is very hot and very hard to handle.

00:48:42.440 --> 00:48:46.080
And so it typically leaks
out to the walls around it.

00:48:46.080 --> 00:48:48.400
And so they wanted to
try to confine it better.

00:48:48.400 --> 00:48:51.650
But in order to be able to see inside it,
if you stick a probe in it,

00:48:51.770 --> 00:48:53.710
it's so hot it could just vaporize.

00:48:53.740 --> 00:48:56.900
So it would be very hard to be able
to try to really probe in there.

00:48:56.920 --> 00:48:59.080
So that's why computational
simulation is very interesting to do.

00:48:59.080 --> 00:49:00.080
And so this is an example.

00:49:00.080 --> 00:49:00.980
This is a very interesting example.

00:49:01.080 --> 00:49:06.250
A QuickTime movie made from a gyrokinetic
simulation of a Tokamak plasma in the

00:49:06.250 --> 00:49:09.580
cross-section showing the electric
potential and seeing how it evolves

00:49:09.580 --> 00:49:12.190
from a linear state to a saturated state.

00:49:12.230 --> 00:49:16.860
and then on the right is the
planetarium rendering that was

00:49:16.860 --> 00:49:20.560
performed by a customer over at
Northern Kentucky University.

00:49:20.610 --> 00:49:24.980
This was submitted to the first ever full
dome festival and actually won an award.

00:49:25.150 --> 00:49:28.260
This was performed on a 50
node Mac cluster rendering

00:49:28.290 --> 00:49:34.040
out a three dimensional
simulation inside a planetarium.

00:49:34.070 --> 00:49:36.520
On the lower left is an
example that comes from Dr.

00:49:36.520 --> 00:49:40.940
Hulzenbeck over at UCSD in biology
where he and his colleagues

00:49:41.270 --> 00:49:42.950
wrote a program called P-Mr.

00:49:42.950 --> 00:49:47.480
Bayes that computes the posterior
probabilities of phylogenetic trees.

00:49:47.480 --> 00:49:50.560
So I'll say that five times fast.

00:49:50.560 --> 00:49:53.240
And what it studies is
it looks at the DNA,

00:49:53.240 --> 00:49:57.670
the similarities in DNA between various
species and tries to determine the

00:49:57.670 --> 00:49:59.400
evolutionary path in between them.

00:49:59.400 --> 00:50:01.640
And this was a code that he
consulted with me on to be able to do

00:50:01.640 --> 00:50:03.900
parallelization as well as vectorization.

00:50:03.970 --> 00:50:06.130
With vectorization we were able
to get a three time speed up and

00:50:06.140 --> 00:50:09.100
of course with parallelization we
were able to boost that even more.

00:50:09.100 --> 00:50:11.740
And the number of
processors that involved.

00:50:11.740 --> 00:50:13.810
On the lower right is a
quantum PIC simulation,

00:50:13.820 --> 00:50:16.480
some diagrams from a
quantum PIC simulation.

00:50:16.480 --> 00:50:19.600
In this case this is showing a two
dimensional quantum wave function

00:50:19.660 --> 00:50:23.280
in a simple harmonic oscillator
and showing the circulation of the

00:50:23.280 --> 00:50:25.540
electron around the wave function.

00:50:25.540 --> 00:50:28.540
This was actually work that was
based on my doctoral dissertation

00:50:28.610 --> 00:50:31.460
which I did entirely on Mac clusters.

00:50:31.460 --> 00:50:34.480
And what it involves is an
approximation of path integrals that

00:50:34.480 --> 00:50:39.040
to be able to choose sample just
all the possible classical paths.

00:50:39.040 --> 00:50:43.270
And use plasma code to be able to push
those paths forward and determine the

00:50:43.420 --> 00:50:46.150
evolution of a quantum wave function.

00:50:47.730 --> 00:50:50.320
So the Mac cluster recipe.

00:50:50.640 --> 00:50:52.800
Basically this is all the
description that you need to be

00:50:52.800 --> 00:50:55.020
able to assemble a Mac cluster.

00:50:55.020 --> 00:50:58.770
The ingredients simply take a bunch
of Power Macs or extra G4s or G5s,

00:50:58.800 --> 00:51:02.080
upgrade the memory as you need to
and get a fast Ethernet switch or

00:51:02.120 --> 00:51:05.740
faster if you have more money and
get a bunch of Ethernet cables.

00:51:06.620 --> 00:51:09.810
Then the directions are connect the
cables from the Macs to the switch

00:51:09.810 --> 00:51:13.400
and download Pooch which you can
get from our website and install

00:51:13.400 --> 00:51:16.160
Pooch and it only takes seconds per
node to be able to install Pooch.

00:51:16.240 --> 00:51:20.290
And then use the Altifact Fractal
demo to be able to test the cluster.

00:51:20.300 --> 00:51:25.130
And so what I'd like to do is to be able
to see if I can give you a demonstration.

00:51:25.140 --> 00:51:28.560
So if we could switch to demo two.

00:51:28.560 --> 00:51:29.560
Yes, thanks.

00:51:29.560 --> 00:51:34.790
And let's see.

00:51:41.260 --> 00:51:43.200
Okay, so let me uninstall that.

00:51:43.200 --> 00:51:48.240
Okay, good.

00:51:48.240 --> 00:51:51.280
So let me give you sort of a
prototypical idea of a numerically

00:51:51.280 --> 00:51:54.960
intensive code that we have here.

00:51:54.960 --> 00:51:56.840
This is known as the
AltaVec Fractal Demo.

00:51:56.840 --> 00:52:00.300
Right now it's not using the vector
processor that's in this G5 here.

00:52:00.300 --> 00:52:03.320
And it adds up,
it uses a Z to the Z fourth computation,

00:52:03.320 --> 00:52:07.380
something I thought was a little
bit more numerically challenging.

00:52:09.380 --> 00:52:12.720
And also it counts up how many
floating point operations it does

00:52:12.720 --> 00:52:16.620
in times itself to really determine
how many megaflops it achieves.

00:52:16.620 --> 00:52:19.780
And it gets about 1,100
megaflops in this case.

00:52:19.780 --> 00:52:22.590
But if I use the vector processor,
I can go ahead and use that.

00:52:22.610 --> 00:52:24.430
And it goes quite a bit faster.

00:52:24.440 --> 00:52:28.340
It gets about five, six gigaflops or so,
which is pretty nice.

00:52:29.710 --> 00:52:34.180
and this also can make use
of a dual processor machine.

00:52:34.180 --> 00:52:36.900
That gives me another factor of two,
but what if I want to get

00:52:36.900 --> 00:52:38.100
beyond a factor of two?

00:52:38.220 --> 00:52:39.720
Well,
that's where parallel computing comes in.

00:52:39.720 --> 00:52:41.200
That's where Pooch comes in.

00:52:41.390 --> 00:52:46.810
This is how long it
takes to install Pooch.

00:52:46.810 --> 00:52:46.810
Just double click on the installer.

00:52:47.950 --> 00:52:49.740
and there we are.

00:52:49.780 --> 00:52:52.030
And POOCH is an acronym,
Parallel Operation and

00:52:52.110 --> 00:52:54.280
Control Heuristic Application.

00:52:54.300 --> 00:52:58.970
And let's see, I just need to be able
to log in to the cluster.

00:53:00.430 --> 00:53:03.820
And to be able to start up a new job,
I go ahead and click

00:53:03.820 --> 00:53:05.760
New Job from the File menu.

00:53:05.760 --> 00:53:07.340
It opens up a new job window.

00:53:07.500 --> 00:53:11.170
And this has two panes in the job window.

00:53:11.170 --> 00:53:15.180
It holds a list of files on the left
and executable that will be copied

00:53:15.180 --> 00:53:17.380
to the machines listed on the right.

00:53:17.630 --> 00:53:21.430
and execute as a parallel
computing job there.

00:53:21.520 --> 00:53:24.000
So if I click on select app,
I can go ahead and use the file

00:53:24.000 --> 00:53:27.170
dialogue to be able to navigate
through the file manager.

00:53:27.220 --> 00:53:28.960
But I really don't
prefer doing it that way.

00:53:28.960 --> 00:53:30.870
I prefer using drag and drop.

00:53:30.940 --> 00:53:36.820
So how many of the parallel
computers can you think of you

00:53:36.820 --> 00:53:36.820
can launch using drag and drop?

00:53:36.820 --> 00:53:36.820
There really aren't too many.

00:53:37.200 --> 00:53:41.600
By default, it selects the node I'm on,
which is NobHill Demo 2.

00:53:41.650 --> 00:53:45.600
And if I click on Select Nodes,
this opens up a new network scan window.

00:53:45.600 --> 00:53:49.980
And this uses both Rendezvous and
SLP simultaneously to determine the

00:53:49.980 --> 00:53:54.890
names and IP addresses of other machines
on the local area network that's here.

00:53:56.410 --> 00:54:00.350
and so I can see that it drew,
it used this information,

00:54:00.350 --> 00:54:02.590
it uses the IP addresses
to be able to contact the

00:54:02.590 --> 00:54:05.670
pooches on the other machines,
in this case the XSRVs that are here.

00:54:05.780 --> 00:54:09.770
And involve those and then determine
whether or not they're busy or okay.

00:54:09.780 --> 00:54:12.290
Busy means that they're running a
parallel job with show up in red

00:54:12.290 --> 00:54:14.700
letters or how much RAM they have.

00:54:14.800 --> 00:54:17.540
And it also queries other
information such as,

00:54:17.580 --> 00:54:18.970
gee, you know,
what's the clock speed of them

00:54:18.980 --> 00:54:23.110
or what operating system or
how much load does it have,

00:54:23.110 --> 00:54:24.700
how much disk space.

00:54:24.810 --> 00:54:27.480
You know, when was the last time
someone touched the mouse or,

00:54:27.560 --> 00:54:31.200
and it uses this information to be
able to form a rating of the cluster.

00:54:31.280 --> 00:54:34.320
And so it helps you choose the
nodes that are more suitable

00:54:34.320 --> 00:54:36.020
for running in the cluster.

00:54:36.150 --> 00:54:38.140
And it actually can give
you a recommendation.

00:54:38.140 --> 00:54:41.860
You can go ahead and choose the
add best function if you want.

00:54:42.250 --> 00:54:49.130
Or you can go ahead and drag and drop or
double click on the nodes that are there.

00:54:49.670 --> 00:54:52.980
So if I click on the
options of the job window,

00:54:52.980 --> 00:54:54.690
this opens up the options drawer.

00:54:54.930 --> 00:54:58.570
And you can, say,
place the executable in a particular

00:54:58.590 --> 00:55:02.030
subdirector on each one of the machines,
maybe perhaps delay the launch

00:55:02.030 --> 00:55:04.550
until some later time of day,
so like after a colleague

00:55:04.590 --> 00:55:07.200
leaves home to go from work.

00:55:07.200 --> 00:55:09.430
And you can also pretend
that you're on a very,

00:55:09.430 --> 00:55:12.320
very large system by
launching as many tasks.

00:55:12.360 --> 00:55:15.740
By default, it launches as many tasks
as there are processors.

00:55:15.740 --> 00:55:20.400
So you can also really benchmark
or stress test your code.

00:55:20.400 --> 00:55:23.260
We support three different MPIs,
as I described earlier.

00:55:23.260 --> 00:55:24.320
And if you want to get
through a firewall,

00:55:24.320 --> 00:55:27.990
you can use a particular port number
or queue the job for later execution.

00:55:28.210 --> 00:55:31.460
So to be able to launch the job,
I go ahead and click on Launch Job.

00:55:31.550 --> 00:55:36.440
And this copy is executable to the
other machines and then passes control

00:55:36.780 --> 00:55:40.630
to the parallel computing code,
which then divides up into the various

00:55:40.630 --> 00:55:44.650
different sections and then collects
results back here for display.

00:55:44.660 --> 00:55:50.170
And we get something like
44 gigaflops in this case.

00:55:50.170 --> 00:55:50.170
Thank you.

00:55:54.100 --> 00:55:57.380
Let's see,
I just want to check something.

00:55:57.440 --> 00:56:00.320
OK, so from there,
and just to show you that

00:56:00.320 --> 00:56:04.140
this isn't just for fractals,
is that this is an example of

00:56:04.140 --> 00:56:05.890
a physics code that we have.

00:56:06.020 --> 00:56:08.340
And let me go ahead and actually, oh,
that's fine.

00:56:08.340 --> 00:56:11.450
I'll just involve the
same nodes that are here.

00:56:11.640 --> 00:56:15.820
And we can go ahead and-- so what's
happening here is that this is

00:56:15.820 --> 00:56:18.120
actually a plasma physics code.

00:56:18.130 --> 00:56:22.040
It's running at least a few
million particle simulation.

00:56:22.080 --> 00:56:25.630
And it's being performed on the nine
processors that are available here.

00:56:25.820 --> 00:56:29.330
And if I go ahead and run this job,
we can see that the electrostatic

00:56:29.420 --> 00:56:34.640
potential is going to show that there's a
plasma stability that grows out of that.

00:56:34.660 --> 00:56:38.520
And we can see that in the lower right,
there is the Mac MPI monitor window,

00:56:38.520 --> 00:56:44.000
which is very useful for diagnosing
and debugging parallel codes in MPI.

00:56:44.120 --> 00:56:50.300
And so in the top part of the window,
it shows the messages.

00:56:50.420 --> 00:56:51.880
White means it's not sending any data.

00:56:51.880 --> 00:56:52.970
Red means it's receiving data.

00:56:53.030 --> 00:56:53.900
Green means it's sending.

00:56:53.900 --> 00:56:55.360
Yellow means it's swapping.

00:56:55.370 --> 00:57:00.800
And so a typical thing that happens when
you're learning how to write a parallel

00:57:00.800 --> 00:57:03.510
computing code is that a lockup happens.

00:57:03.650 --> 00:57:09.290
And so it freezes in the
light pattern of the hang.

00:57:09.480 --> 00:57:11.870
But also, down below,
there's a histogram of the

00:57:11.870 --> 00:57:14.660
messages being sent and received
as a function of message size.

00:57:14.660 --> 00:57:18.400
So this encourages you to send
fewer large messages as opposed

00:57:18.400 --> 00:57:20.040
to many smaller messages.

00:57:20.040 --> 00:57:24.080
It also shows you dials of how much
time it's been communicating and how

00:57:24.080 --> 00:57:28.220
many megabytes per second are being sent
or received in between these machines.

00:57:28.250 --> 00:57:32.920
So this is a utility that myself and
many colleagues and many venerable

00:57:32.920 --> 00:57:38.410
institutions have used to be able
to diagnose and debug their codes.

00:57:38.990 --> 00:57:41.900
And to give you another
example of a code,

00:57:41.900 --> 00:57:44.280
also a physics code,
this is an example of a code that

00:57:44.680 --> 00:57:49.100
performs a Fresnel diffraction problem
where you have a point source of light

00:57:49.100 --> 00:57:53.290
producing spherical waves and projecting
a diffraction image on the screen.

00:57:53.300 --> 00:57:57.790
And so from there, we can actually,
this actually has a feature where

00:57:57.840 --> 00:58:02.560
you're able to automatically launch
itself in parallel on a cluster.

00:58:03.000 --> 00:58:07.490
And so this is the way that I would hope
that applications become so easy to use

00:58:07.570 --> 00:58:10.940
that you can simply drag and you can just
simply use a menu click to be able to

00:58:10.940 --> 00:58:15.480
have it launch itself onto a cluster and
make use of the resources that are there.

00:58:15.480 --> 00:58:17.530
And you can see again the
Mac monitor window showing

00:58:17.530 --> 00:58:20.160
the messages being received,
mostly very large messages.

00:58:20.160 --> 00:58:21.700
Let me make the problem,
I was going so fast,

00:58:21.790 --> 00:58:23.280
I got to make the problem size bigger.

00:58:23.280 --> 00:58:26.650
And then it's also showing the colors of
different parts of the problem that are

00:58:26.670 --> 00:58:28.730
being assigned to the various processors.

00:58:30.350 --> 00:58:33.000
And so just one more feature,
one other thing I want to show you,

00:58:33.050 --> 00:58:37.120
something that was just announced
this week was what we call Pooch Pro.

00:58:37.260 --> 00:58:40.310
It has a new user menu where you
can actually assign a certain

00:58:40.310 --> 00:58:41.560
amount of quota for each user.

00:58:41.560 --> 00:58:43.900
And so it computes how much
compute time is being used.

00:58:44.020 --> 00:58:46.900
And then this is a cluster,
the only cluster or server computer

00:58:46.900 --> 00:58:48.560
I know of that has rollover minutes.

00:58:48.650 --> 00:58:51.390
So you can rollover your
compute time from week to week,

00:58:51.390 --> 00:58:52.140
let's say.

00:58:52.190 --> 00:58:55.440
And also, now this is something you would
only see as an administrator.

00:58:55.440 --> 00:58:59.240
You can actually administrate
the users that are there.

00:58:59.280 --> 00:59:04.700
And A means that it's administrate,
you have administrative capabilities.

00:59:04.700 --> 00:59:08.040
Q means it has a quota, rollover minutes,
being able to migrate

00:59:08.070 --> 00:59:09.370
and password changing.

00:59:09.610 --> 00:59:11.200
And you can have different
passwords and so on.

00:59:11.200 --> 00:59:13.790
So I can double click on
a particular one and edit,

00:59:13.840 --> 00:59:17.580
say, how much time our good
friend Warner Yuen has CPUs.

00:59:17.580 --> 00:59:20.270
Like, let's say,
I'll give him just a really little

00:59:20.270 --> 00:59:22.530
time or something like that,
who knows.

00:59:22.600 --> 00:59:24.580
And that's not a limb changes password,
okay.

00:59:25.500 --> 00:59:28.770
Anyway, so these are the kinds of things
that are available in Pooch Pro.

00:59:28.850 --> 00:59:30.080
So that will be it for the demo for now.

00:59:30.080 --> 00:59:32.300
I'd like to switch back to slides.

00:59:35.840 --> 00:59:38.160
Thank you.

00:59:38.380 --> 00:59:41.700
So just very quickly
for more information,

00:59:41.710 --> 00:59:44.560
the reference library
that we can refer to,

00:59:44.560 --> 00:59:46.520
basically we go to the
data search website.

00:59:46.580 --> 00:59:48.000
You can find a whole
bunch of information.

00:59:48.000 --> 00:59:50.610
The Pooch website,
you can find out the cluster recipe,

00:59:50.630 --> 00:59:53.630
you can download a trial version,
and we have a tutorial on writing

00:59:53.690 --> 00:59:57.290
parallel codes as well as a zoology of
parallel computing that is a description

00:59:57.290 --> 00:59:59.560
of the various parallel computing types.

00:59:59.560 --> 01:00:04.340
And this will all be
linked from the WWC URLs.

01:00:04.340 --> 01:00:07.780
As well as the parallel NOC tutorial
with both code examples,

01:00:07.910 --> 01:00:10.940
Fortran and C, a parallel adder tutorial,
both languages,

01:00:11.250 --> 01:00:14.750
parallel Pascal's triangle,
and as well as related publications and

01:00:14.750 --> 01:00:18.510
actually another video that's a little
bit longer than what we displayed here

01:00:18.510 --> 01:00:20.420
of some of the work that we've done.

01:00:20.420 --> 01:00:24.860
So I'd like to introduce Steve Forde and
thank you very much for your attention.

01:00:32.100 --> 01:00:35.140
Steve Forde,
I'm CEO of GridIron Software.

01:00:35.140 --> 01:00:40.030
I'm going to go over a real brief
overview of what we would call a next

01:00:40.180 --> 01:00:43.000
generation parallel computing framework.

01:00:43.000 --> 01:00:47.300
And we're going to do that really
from a very commercial perspective.

01:00:47.300 --> 01:00:51.240
So probably a lot of the same
points that you've heard before,

01:00:51.240 --> 01:00:54.810
I'll go through a little bit,
but we'll go from here.

01:00:55.650 --> 01:00:58.740
So one of the key things that
ISVs that we work with are

01:00:58.740 --> 01:01:03.750
looking for are obviously speed,
but a lot of times the resources

01:01:03.770 --> 01:01:09.980
that are available to end users for
products that they ship are not vast.

01:01:10.080 --> 01:01:14.620
So you get into a scenario where I need
to provide 100% performance or provide

01:01:14.620 --> 01:01:16.610
linearity for every CPU that I add.

01:01:16.740 --> 01:01:21.600
Because you might have a company like
a Pixar that has thousands of machines,

01:01:21.600 --> 01:01:24.150
but you also might have a
small post-production facility

01:01:24.150 --> 01:01:25.390
or something like that.

01:01:25.500 --> 01:01:28.140
And you're sitting in the basement with
just one or two machines kicking around.

01:01:28.140 --> 01:01:29.770
Is that actually going to
provide some value for them?

01:01:29.780 --> 01:01:34.600
So the challenge for developers is to how
do you build in a parallel application

01:01:34.600 --> 01:01:39.410
that provides this performance in a
very easy to use and seamless fashion?

01:01:39.460 --> 01:01:44.180
Power, ISVs are really interested
in the money quotient.

01:01:44.180 --> 01:01:48.000
This is what I would like to
call our million dollar slide.

01:01:48.000 --> 01:01:51.270
But from this perspective,
this was a customer we

01:01:51.270 --> 01:01:55.400
have in the print space,
and we actually did a comparison.

01:01:55.400 --> 01:02:01.270
Between five G5Xers and a
Sun SparkFire 6800 12 CPU.

01:02:01.400 --> 01:02:04.920
The interesting thing is
that this was the result,

01:02:04.920 --> 01:02:10.980
and this is the cost that's generally
associated with machines like that.

01:02:10.980 --> 01:02:14.490
And you can kind of get the idea
of why commercial software vendors

01:02:14.650 --> 01:02:18.670
are very interested in seeing how
can they provide this functionality

01:02:18.750 --> 01:02:23.710
from a commercial perspective
to everyone in their user base.

01:02:23.900 --> 01:02:26.970
So methods of grid computing,
we've heard a lot about different things,

01:02:26.970 --> 01:02:29.970
but from the grid's perspective,
there's three basic kinds.

01:02:30.110 --> 01:02:33.130
There's the middleware perspective,
there's the opportunity

01:02:33.200 --> 01:02:34.990
for message passing,
which Dean talked a lot about,

01:02:35.200 --> 01:02:37.970
and some development tools that
try to make this whole black art of

01:02:37.970 --> 01:02:40.720
parallelism a little bit easier on you,
the developer.

01:02:42.310 --> 01:02:45.240
Script distribution,
there's obviously some pros and cons.

01:02:45.240 --> 01:02:47.220
It's very good,
as we see with distributed

01:02:47.300 --> 01:02:49.390
resource managers,
and if you're familiar

01:02:49.390 --> 01:02:51.910
with things like Xgrid,
and that kind of stuff,

01:02:51.910 --> 01:02:54.470
to go out and say, okay,
I'm going to use existing

01:02:54.470 --> 01:02:57.850
resources with existing
applications and do things across.

01:02:57.920 --> 01:03:01.370
But there generally needs to be some
sort of skill set for the end user

01:03:01.590 --> 01:03:03.640
to understand how to do those things.

01:03:03.640 --> 01:03:08.470
So it's very useful in areas of
scientific computing and in research.

01:03:08.480 --> 01:03:10.580
But when you go into a
shrink-wrapped application and

01:03:10.600 --> 01:03:14.010
you're trying to put that onto a CD,
it's a little tough for a lot

01:03:14.040 --> 01:03:15.890
of the user base to grasp.

01:03:16.120 --> 01:03:19.780
Message passing,
as Dean talked a lot about,

01:03:19.880 --> 01:03:24.160
it's used quite extensively in
the scientific and research areas.

01:03:24.160 --> 01:03:26.900
But the interesting thing that
we found as we went through our

01:03:27.310 --> 01:03:30.100
engagement with several ISVs
from the commercial perspective,

01:03:30.470 --> 01:03:32.430
obviously pros and cons
from that perspective,

01:03:32.430 --> 01:03:35.650
but the biggest thing was that there
wasn't a lot of confidence with their

01:03:36.000 --> 01:03:37.590
ability to ship that with a product.

01:03:37.760 --> 01:03:42.670
So it was the learning curve associated
with actually putting that into

01:03:42.700 --> 01:03:47.290
their products and their users to
understand how this thing works.

01:03:47.600 --> 01:03:51.380
Development tools is where you're
probably going to see a lot more

01:03:51.380 --> 01:03:54.980
emphasis on this down the road,
especially as chip design

01:03:54.980 --> 01:03:57.340
and so on is going to move in
a few different directions.

01:03:57.440 --> 01:04:00.650
But from our perspective,
we wanted to create an application

01:04:00.770 --> 01:04:04.130
development environment that had
a very high level of abstraction.

01:04:04.230 --> 01:04:07.480
So message passing is a
message passing interface.

01:04:07.560 --> 01:04:10.210
But to turn around and say, OK,
you still have to write

01:04:10.320 --> 01:04:12.100
a parallel application.

01:04:12.200 --> 01:04:14.060
That provides you the messages,
but everything else

01:04:14.070 --> 01:04:14.820
that you're associated.

01:04:14.850 --> 01:04:17.700
So you not only have to worry about
how do you partition your algorithm,

01:04:17.780 --> 01:04:19.320
but then how do you message?

01:04:19.320 --> 01:04:22.260
And then how do you build all
the things such as discovery

01:04:22.260 --> 01:04:26.520
and resource fault tolerance,
all those kind of things?

01:04:26.560 --> 01:04:30.160
You've got some good tools, again,
like Dean's tool that can come along

01:04:30.550 --> 01:04:32.780
and work with MPI and so on on the top.

01:04:33.090 --> 01:04:35.770
But from the perspective of
what happens from within,

01:04:35.770 --> 01:04:38.280
and that's something
that's very important.

01:04:38.280 --> 01:04:41.820
GridIron XLR8 just as
a real brief overview,

01:04:41.820 --> 01:04:44.710
it's a peer to peer based
distributed computing architecture.

01:04:44.820 --> 01:04:47.700
And the APIs are built into source.

01:04:47.700 --> 01:04:49.150
It's more wrapping the source.

01:04:49.240 --> 01:04:50.990
I'm going to give a
quick example of that,

01:04:50.990 --> 01:04:53.220
what we did for some MPEG encoding.

01:04:53.280 --> 01:04:55.980
And then the work is dynamically
addressed across the network.

01:04:55.980 --> 01:05:02.040
And that can be to a dedicated cluster
or to specific resources of desktops.

01:05:02.080 --> 01:05:03.420
It doesn't really matter.

01:05:03.460 --> 01:05:06.880
The key thing is that you can go
into that scenario very quickly.

01:05:06.930 --> 01:05:11.210
And from within your application,
once it's been programmatically added,

01:05:11.270 --> 01:05:14.360
provide a user with a
very engaging experience.

01:05:14.440 --> 01:05:18.260
So the development tools have a
lot of the same pros and cons.

01:05:18.430 --> 01:05:21.470
There's obviously requires code
modification and we as developers

01:05:21.470 --> 01:05:22.810
don't like to modify code.

01:05:23.230 --> 01:05:27.100
It's a very non-trivial thing,
especially when you get

01:05:27.100 --> 01:05:28.320
into busting up algorithms.

01:05:28.320 --> 01:05:32.220
Anybody who works in multi-threading
can go into that attestment.

01:05:32.280 --> 01:05:34.810
But from our perspective,
we're kind of like a hybrid

01:05:34.860 --> 01:05:37.610
between OpenMP and MPI.

01:05:37.620 --> 01:05:40.540
We wanted the ability to provide
in the demo I'm gonna show you with

01:05:40.540 --> 01:05:43.840
Adobe After Effects was to be able to
take advantage of just another machine,

01:05:43.840 --> 01:05:48.890
another CPU within the same box for a
very serial threaded based application.

01:05:49.480 --> 01:05:51.980
To grid or not to grid,
obviously that's the big question.

01:05:51.980 --> 01:05:55.050
What kind of development work
or effort do you have to put

01:05:55.050 --> 01:05:57.900
into the parallelization of your
code to return some results?

01:05:57.940 --> 01:05:59.300
And is it worth it?

01:05:59.360 --> 01:06:00.310
That's the big question.

01:06:00.460 --> 01:06:04.020
And that's always why parallelism
has been the black art.

01:06:04.140 --> 01:06:06.850
So from our perspective,
we wanted to really start to get

01:06:06.850 --> 01:06:10.600
rid of that black art connotation,
provide an interesting

01:06:10.680 --> 01:06:12.520
framework to do this.

01:06:12.570 --> 01:06:15.230
A lot of times, I mean,
there's been a lot of talk also about

01:06:15.230 --> 01:06:17.060
processor intensive applications.

01:06:17.060 --> 01:06:20.260
But ironically, most of the applications
that we've worked with,

01:06:20.320 --> 01:06:21.960
they have more data problems.

01:06:22.010 --> 01:06:25.470
Data movement, reads and writes,
and those kind of things seem to be

01:06:25.570 --> 01:06:29.410
a major bottleneck with a lot of the
applications that we've worked with.

01:06:29.590 --> 01:06:31.980
So we wanted to come up with
various different means.

01:06:32.010 --> 01:06:35.520
Again, saying to you, the developer,
really you focus on your algorithm.

01:06:35.560 --> 01:06:37.680
You focus on the thing
that you know very well.

01:06:37.740 --> 01:06:41.170
And we'll provide you a parallel
application that you can call into.

01:06:41.640 --> 01:06:43.500
So which grid method to use?

01:06:43.660 --> 01:06:47.340
Obviously, if you're doing embarrassingly
parallel or scriptable type things,

01:06:47.410 --> 01:06:50.460
distributed resource managers
and scripted batch queue

01:06:50.460 --> 01:06:52.520
systems are very good.

01:06:52.580 --> 01:06:56.110
If your source code is not available,
again, that's probably the only

01:06:56.110 --> 01:06:57.160
route that you have.

01:06:57.160 --> 01:06:59.890
But then you also have the opportunity,
depending on the resources

01:06:59.890 --> 01:07:02.310
at your disposal,
to go into a message pass or into

01:07:02.310 --> 01:07:05.690
another type of framework such
as a development environment.

01:07:05.800 --> 01:07:10.800
Quickly on grid enabling an application,
obviously 90/10, 80/20,

01:07:10.800 --> 01:07:12.260
basically the same thing.

01:07:12.350 --> 01:07:14.940
But when you're looking
at the 80/20 rule,

01:07:15.100 --> 01:07:18.340
focus on that 20% of the code
that does 80% of the work.

01:07:18.620 --> 01:07:22.090
The abstraction level again
is very important here because

01:07:22.090 --> 01:07:24.830
what really we're trying to do
from a development perspective,

01:07:24.830 --> 01:07:27.340
there's no such thing as
real automatic parallelism.

01:07:27.340 --> 01:07:29.640
But from that perspective,
maybe there is ways to wrap

01:07:29.640 --> 01:07:32.990
and provide hints instead
of breaking your algorithms.

01:07:33.030 --> 01:07:35.560
In other words,
they can still run the same way that they

01:07:35.560 --> 01:07:38.620
did before and you don't have to worry
about totally wrecking your application.

01:07:38.620 --> 01:07:41.560
Application modification,
we've broken out our

01:07:41.560 --> 01:07:46.020
architecture into three plug-ins,
defining tasks, task compilation,

01:07:46.040 --> 01:07:47.780
and then result reassembly.

01:07:47.820 --> 01:07:50.550
But again,
the goal is to provide the end user

01:07:50.550 --> 01:07:54.090
with a really engaging experience
where they can basically think

01:07:54.470 --> 01:07:57.120
that it was just done on their PC,
just on their Mac,

01:07:57.120 --> 01:08:00.090
and they can use whatever
machines that are on the network.

01:08:00.340 --> 01:08:03.060
MPEG encoding was a challenge that
we were given by a certain company

01:08:03.060 --> 01:08:04.160
that does a lot of encoding.

01:08:04.160 --> 01:08:08.140
And we wanted to see what kind of
results because this was high data.

01:08:08.160 --> 01:08:11.160
So this is HD video.

01:08:11.160 --> 01:08:16.150
We actually went in and did a
modification and we ran the test

01:08:16.150 --> 01:08:19.130
on several XSERVs and we got
some very interesting results.

01:08:19.160 --> 01:08:22.990
Basically,
we tried it out originally on 12.

01:08:23.240 --> 01:08:27.220
We did actually go up to 40 but we
started seeing some degradation in

01:08:27.220 --> 01:08:28.160
the curve of diminishing returns.

01:08:28.160 --> 01:08:32.520
But we took an HD encode and brought that
down from two and a half hours to six

01:08:32.520 --> 01:08:35.150
minutes and provided the seamless result.

01:08:35.160 --> 01:08:38.160
The nice thing is I also didn't
have a lot of disk space.

01:08:38.160 --> 01:08:40.160
I didn't need to have a lot of things.

01:08:40.160 --> 01:08:43.650
It just went and dynamically moved
the data when it needed to and brought

01:08:43.650 --> 01:08:45.120
the results back for the end user.

01:08:45.260 --> 01:08:47.100
More importantly,
from a development perspective,

01:08:47.180 --> 01:08:49.090
we only modified 100 lines.

01:08:49.160 --> 01:08:52.720
Out of this 1100 source files,
we modified three,

01:08:52.810 --> 01:08:55.150
basically about a thousand lines.

01:08:55.170 --> 01:08:57.160
Of course, we published a white paper on
this so there's a lot of comments.

01:08:57.160 --> 01:08:58.020
But we did it.

01:08:58.020 --> 01:09:02.020
And that's available on our website
if you do want to check that out.

01:09:03.510 --> 01:09:06.440
So another thing that we did is,
and what we're shipping with right

01:09:06.440 --> 01:09:09.100
now is with Adobe After Effects.

01:09:09.130 --> 01:09:12.440
And I wanted to kind of show that to you,
if I can switch over to demo two.

01:09:12.470 --> 01:09:16.660
I wanted to show that to you very
quickly because this is what we

01:09:16.660 --> 01:09:19.340
feel the end user has to experience.

01:09:19.370 --> 01:09:23.450
And this is the challenge for us as
developers to bring into an engaging

01:09:23.450 --> 01:09:25.640
experience for end customers.

01:09:25.690 --> 01:09:28.040
The funny thing as well,
and I'm just gonna make a reference,

01:09:28.040 --> 01:09:30.700
I think in the keynote on Monday,
there was a reference to, you know,

01:09:30.700 --> 01:09:33.840
the challenge of bringing
chips into a smaller design.

01:09:33.890 --> 01:09:35.540
Well,
a lot of folks are announcing now the

01:09:35.590 --> 01:09:40.000
ability to go into a core multi-threading
or a multiple cell type chip.

01:09:40.090 --> 01:09:44.040
So parallelism is gonna be absolutely
key if our software environments are

01:09:44.080 --> 01:09:45.720
actually gonna take advantage of it.

01:09:45.810 --> 01:09:47.980
So one of the things that
we did in this scenario is,

01:09:47.980 --> 01:09:52.400
is that we're actually gonna
use these four XSERVs here.

01:09:52.400 --> 01:09:55.400
But from an end user perspective,
they don't know anything.

01:09:55.420 --> 01:09:59.800
I mean, this is a product that ships,
you can buy it at Fry's for 900 bucks.

01:09:59.820 --> 01:10:01.540
And from that perspective,
if you go in there,

01:10:01.540 --> 01:10:02.940
they don't know anything about DHCP.

01:10:02.940 --> 01:10:04.480
They don't know anything about DNS.

01:10:04.630 --> 01:10:09.080
They don't know anything other than how
you have to plug it in and I hit go.

01:10:09.220 --> 01:10:12.720
So from that perspective,
that's literally all they have to do.

01:10:12.870 --> 01:10:14.920
They go out,
it'll automatically dynamically

01:10:14.920 --> 01:10:16.280
find all the other machines.

01:10:16.280 --> 01:10:18.880
It'll pass the data that's
relevant for it to work on.

01:10:18.880 --> 01:10:22.100
But the most important thing is,
is to provide the results right

01:10:22.100 --> 01:10:25.340
in an application in a method
that they're very familiar with.

01:10:25.470 --> 01:10:27.580
So if you look down here,
we're starting to bring in,

01:10:27.580 --> 01:10:31.850
this is an HD 1080i clip by the way,
for those who are interested.

01:10:32.170 --> 01:10:35.520
But the interesting thing is,
is that we're bringing the results right

01:10:35.520 --> 01:10:38.480
into the RAM cache of After Effects.

01:10:38.610 --> 01:10:42.960
So from the user's perspective,
this looks just like that always worked.

01:10:43.110 --> 01:10:44.960
And that's very engaging.

01:10:44.960 --> 01:10:46.920
The other side too though,
is that we have an interesting

01:10:46.930 --> 01:10:49.400
side effect of using a
grid to do all the work.

01:10:49.400 --> 01:10:52.080
And that is,
you can render and work at the same time.

01:10:52.320 --> 01:10:55.960
That's never been doable before in a
single threaded application like this.

01:10:56.130 --> 01:10:58.160
And I can do things and render and so on.

01:10:58.260 --> 01:11:01.010
So I'll go back to, to the slides.

01:11:01.140 --> 01:11:03.440
That's just a very quick demo.

01:11:03.440 --> 01:11:09.500
And the power that provides
an end user is very engaging.

01:11:09.500 --> 01:11:11.180
And we've been able to see that.

01:11:11.180 --> 01:11:12.700
This has been shipping for a month.

01:11:12.700 --> 01:11:17.700
And I think the stat was
17,000 users are using this,

01:11:17.700 --> 01:11:20.500
cobbling together machines
in their basements,

01:11:20.510 --> 01:11:23.150
using it in very large
infrastructure as well.

01:11:23.310 --> 01:11:25.280
The NBA finals were
brought to you by this.

01:11:25.300 --> 01:11:30.100
And from that perspective, if we can,
as developers bring,

01:11:30.100 --> 01:11:33.190
engaging experiences for our customers,
our products will come at the

01:11:33.190 --> 01:11:34.540
market in a very engaging way.

01:11:35.840 --> 01:11:38.060
So summary, obviously speed.

01:11:38.240 --> 01:11:39.490
Speed's great.

01:11:39.750 --> 01:11:41.760
Is it worth the work?

01:11:41.940 --> 01:11:43.800
That's really up to you.

01:11:43.850 --> 01:11:46.940
You need to look at environments
that are going to help you get

01:11:46.940 --> 01:11:51.200
to a more optimized and parallel
infrastructure without the headaches

01:11:51.200 --> 01:11:52.740
or the worry of breaking your code.

01:11:53.240 --> 01:11:55.900
There's going to be new hardware
technologies coming down the road,

01:11:55.900 --> 01:11:58.300
specifically multi-cell
chips that are going to mean

01:11:58.300 --> 01:11:59.970
parallelism is absolutely key.

01:11:59.970 --> 01:12:03.020
So we've got to start
thinking about it now.

01:12:03.020 --> 01:12:06.410
And significant linear
performance is really the thing

01:12:06.420 --> 01:12:08.290
that customers want to buy.