---
Title:  The Evolution of Standards
Year:   2004
Web:    https://nonstrict.eu/wwdcindex/wwdc2004/730/

[!] This transcript was generated using Whisper, it has known transcription errors.
---

Hi, everyone. Hope you've been having a good day. Thanks for coming to our session, and it's the evolution of standards. I'm very pleased to welcome you to our panel discussion today, and that panel discussion is going to be led by Dave Singer, who is one of our standards experts at Apple. So, Dave? Thank you very much.

So yes, welcome to the Evolution of Standards. This is a moderated panel session. We're going to go through a few slides at the beginning and give you time for questions at the end. We're going to give you a rough idea of what's going on, what's happened recently, what may be coming up out of the standards organization. And I'd like to introduce to you the distinguished panelists I have here. So on the left here, a leading luminary in the video coding business, a published author, and a respected figure worldwide in video coding, Barry Haskell of Apple Computer.

It gives me great pleasure and a small frisson of excitement to introduce the chair of the International MPEG Audio Subgroup, a sage in the audio coding business, again a leading figure in the international audio business, Schuyler Quackenbush. I'm bringing out the back. I'll talk a little bit about system stuff. We're going to introduce the session. You ask questions. We have a lot of fun.

So I'm going to go through the standards organizations, at least some of them that you may have heard of and who they are roughly and what they do. Give you a couple of things about some things that I think are exciting about what's happened in systems related standards recently. A couple of slides on licensing. Then I'm going to turn it over to Schuyler who's going to talk to you about audio, and then we'll do a video update at the end.

We will be taking lots of questions at the end. And we'll take straightforward clarification questions during the sessions. So if you have a clarification question, don't hesitate to interrupt. But if it's a general question, hold it to the end. We're going to leave plenty of time for it.

Unlike what you might expect, we're going to be frank, honest, even forthright about questions like, when will Apple introduce a particular technology? Or what legal advice do you have for this and so on? In order to save time, however, I'm going to give you that question now. The answer is, we don't know. You might want to save those questions for some other time.

So the standards organizations, this is the most boring part, so you can sleep for a minute, OK? ISO and ITU, two very big organizations. ISO has two important working groups for the QuickTime related people. Working group one under SC29, the Joint Photographic Experts Group, the people who brought you Classic JPEG, JPEG 2000, and so on. They're doing lots of exciting work. I'm not going to talk about it today.

Working group 11, the Motion Pictures Expert Group, where we all go to, they're the people who brought you MPEG-1, MPEG-2, MPEG-4, and MPEG with lots of other numbers behind it too. And then the ITU has the Video Coding Expert Group, which you've heard about, which is where the H.264 work started, for example, and lots of other H-series codecs.

Cellular telephony â€“ I know that's been talked about a lot at this session. There's two really big organizations there. They're both partnership projects. They're partnership projects between standards organizations, believe it or not, mostly national standards organizations. 3GPP sets the standards for roughly the people who are in the GSM family of cellular standards, the ones that started in Europe but are now basically worldwide. 3GPP2 follows the other track, the CDMA/CDMA2000 related set of families. You'll find them both have open websites in which you can find a lot of information.

Two more interesting ones: Society of Motion Picture and Television Engineers. They do a lot of the standards work for production and broadcast and so on. That's SMPTE. And you'll find lots of information on their website. And finally, those of you who are interested in streaming, the standards for Internet-based streaming, the protocol specifications and so on, they come from the Internet Engineering Task Force, IETF.

Now, there are also out in that field industry consortia. These aren't really standards organized as such. These people do collaboration agreements and so on, implementation agreements where they all agree to implement the same thing. They do a lot of the testing. Sometimes some of the more business-related discussions happen in consortia like this. So Internet Streaming Media Association, ISMA, they do interoperability agreements for the Internet and for IP-based networks.

Open Mobile Alliance, they do that for cellular and for telephony-related high-level functions. DVB, they do it for digital video broadcasting, as their title implies, mostly based in Europe. DVD Forum, they're the key people who talk about shiny disks, and we had a lot about that yesterday in the H.264 session.

Finally, the MPEG Industry Forum, put this one last, but it's actually a good place to go to. This is one where they promote and explain MPEG technologies. They have a lot of information on their website. They tend to keep a lot of news there. It may be a good place for you to start. and you'll find lots of links to other useful places from the MPEG-IF.

So with that, I'm going to just give you a little quick update of two things that I think are really exciting in systems. Unfortunately, I know systems is not that exciting. So I'm going to talk about file format, and I'm going to talk about file format simply because it's my baby, okay? So you're just going to have to humor me for one slide. What's been going on with the ISO file format?

The ISO file format, as we told you last year and the year before, was adopted into MPEG as the basis for the MP4 file format. Then it became the Motion JPEG file format, then the 3GPP file format, and the 3GPP2 file format, and the SD Video Card Association's file format.

So you can see that this file format's going places and really going there. With the AVC, the H.264 work, the committee realized that because the codec itself offered had such a variety of features and functions, we could do with better file format support. So there's lots of fun stuff in there.

You can group samples together. So you can do layers. You can do layering and sub-sequence selection of an AVC session in an ISO-based file. You can get much more dependency information because the dependency structure of AVC is so much more complicated. We give you clues in the file format, or at least we enable you to put clues in the file format so you can disentangle the dependency information.

There's an amendment underway which includes those extensions and actually a lot of other stuff too. A lot of you have been saying, guys, the ISO file format approach to metadata is very weak, and we knew that. So we put in a really good foundation for metadata support and explicit support for MPEG-7, but it's also open to storing other kinds of metadata.

So if you're a metadata-oriented person, this amendment may be for you. And then there's support there for content protection. So you can do protected, rights protected DRM content in the file format in a standard way. So finally we've got a standard container format for exchanging rights protected content, which is, I think, a breakthrough in the industry.

An interesting development going on in 3GPP, release six is almost done. There's a terrific amount of stuff in there for those of you who follow the cellular world. This is probably the release that I think will become the one that becomes prevalent in the 3G world very rapidly.

Not only does it include the H.264 codec, which will obviously is a video feature, but in the systems area, they finally put content protection in there, so we can talk about rights-protected content in the cellular world. There's buffer management and adaptive streaming across the network, so we can expect better quality streaming, better adaptation between the handset and the server back at the operator's place.

And indeed, there's RTP retransmission support, so if a packet does get lost, despite all the lower layers' attempts to get it through to you, there's retransmission support at that level. There's also quality of experience monitoring, which is going to be interesting. It's going to enable the operator to work out, hey, that sucked for you, and not charge you so much before you actually had to call them and complain that it sucked.

Of course, they're hoping it won't suck at all. And then there's multicast and broadcast support, so there's lots of opportunities in business now to send out real-time live content in a multicast broadcast fashion, which puts less load on the network and obviously should cost less to the end user.

So with that, I'm just going to give you a really quick update on licensing. So we normally do licensing. Licensing is perfectly ordinary. There are companies whose job it is to do basic research and development and then license it. And we all appreciate that because we couldn't get that number of experts together in the same place and get the same result, right?

So licensing is a normal part of the high-tech business. It became difficult over the last few years, we know, but we think it's getting easier again. Almost any successful market needs an interoperable standard. You've got to have multiple players in the market who can interoperate, providing the different products that fill out all the niches, both in terms of function and in terms of placement, that are needed to make a market mature.

For multimedia standards, licensing models have included encoder, decoder, manufacturing, and bundling fees, that's perfectly normal. You haven't seen them in the past because they've typically been embedded in the chips that were put into the products. So that chip costs a little bit more because it paid a license.

There have also been content replication fees, that's perfectly normal again. So when you buy a DVD, there's an amount of money going back to the rights owner here who developed the standards for how you make that DVD. That's how they make their money. We typically cover a good chunk of what we can for you.

So you can use the technology without having to worry too much. So, the question is, of course, okay, if Apple's covering a good chunk of it, do I need a license?

[Transcript missing]

So with that, I'd like to hand it over to Schuyler, who's going to give you an exciting audio update. Thanks, Dave.

Okay, so I want to begin by reviewing the MPEG audio standards, talk about where MPEG has been and where MPEG is going. I'd like to begin the timeline with the compact disc, 1982, which I think kicked off the whole digital audio revolution. More than ten years after that, MPEG standardized MPEG-1, Layer 3, or MP3. So that is looking already like a pretty old guy, but still holding an important position in the marketplace.

Several years after that, MPEG-2 issued Advanced Audio Coading, and this also exists in the MPEG-4 standard. This is arguably the state of the art in perceptual audio coding. Several years after that, 2003, last year, MPEG issued the high efficiency AAC codec. This builds on AAC. We'll talk about how that works. But in fact, it uses a different dimension, exploits a different dimension to get more compression.

In 2005, next year, I anticipate MPEG will issue a standard on lossless audio coding. So what we see is â€“ I'm giving you a timeline view here, but what we see is that we've got compression, and then we kind of take a step backwards and say, oh, let's do something else. Let's do compression, but I want all the bits back.

And then finally, we're about to begin a new work item in spatial audio coding. We're back in the compression business, but again, we're not just doing a perceptual audio codec. We're exploiting a different aspect, and that actually sounds stage compression to give you total bit rate compression. So here's the timeline of what MPEG's been doing. I'm going to stand over here. And so let me give you another view of that. of this.

and look at performance. So, the top line is the compact disc and if you look at the right hand side, I want to look at bits per channel per second, which kind of normalizes my whole rate because we'll see that we're going to go from two channel to multi-channel in the course of looking at the various specifications. And I reversed the timeline and we're looking at, on the right hand column, bits per channel per second.

So the ALS, the audio lossless coding, it's at 325 kilobits per second. That's a lot. But that's about two to one compression and you get all the bits back. So it's bit exact. I think that number is at 48 kilohertz sampling rate. It's not a perceptual coder. It's just compress it mathematically and give me the bits back, which is a departure for MPEG.

Now the next line is MP3. It's the oldest of the MPEG coders. It's a perceptual audio coder and it gives you good compression, not the best. And typically people, I mean, I think it should be run at like 80 kilobits per second per channel. So 160 kilobits for stereo. A lot of people run at 128. A lot of people run at 64. But at 80, it really sounds good.

Now I said the next after that was AAC, and in effect really sounds good at 64 kilobits per second per channel. As a matter of fact, MPEG benchmarked it as an EBU broadcast quality codec at that rate. So, as I mentioned before, MP3 and AAC are perceptual audio coders. You're getting compression, and it primarily exploits aspects of human hearing to shape the coding noise so that you don't hear it.

Now the next standard is High Efficiency AAC. Again, another slide will show you how it works. Here we're not just doing perceptual coding. In fact, it builds on top of AAC. And it uses AAC, but then adds a feature of coding the bandwidth, so-called bandwidth extension. It gives you compression by coding the bandwidth in a clever way. So, there's shape noise, bandwidth coding, and in the bottom line, SC stands for spatial audio coding. A whole other dimension in compression by exploiting sound stage compression in a perceptual way. So, it is a perceptual coder, but it's not shape noise.

It's how we perceive a sound stage, and can we do clever things to code that in a very compact way? And the answer is yes. And so we see that, to recap, MP3, 80 kilobits, AAC, 64 kilobits, HEAC, a huge leap forward, 24 kilobits per second, and finally spatial audio coding, 21. So, we are really marching forward, but we've had to use different tricks to get there. So, let's talk about what are those tricks. How does it work?

Okay, now first, I get ahead of myself. So, those are the MPEG standards. Where are they in the marketplace? Are they being used? So, let's talk about that. First, MP3, it's everywhere. I mean, it's old. It has taken 10 plus years to be really successful in the marketplace. So successful, it's never going to go away. So, MP3 is hugely proud of that piece of technology.

AAC, which I say is definitely one better, the state of the art. It's being used by Apple iTunes and the iPod. Cool, great. Real Networks, Rhapsody uses AAC at a very high bit rate. They want to really bring you great digital audio, so they run at 192 kilobits per second for stereo. And, in fact, you will see AAC in a number of 3G cell phones. HEAAC, it's the stuff that makes XM satellite radio. Satellite radio is what I call expensive bits.

And you need compression when bits are expensive. And so, they can only dedicate like,

[Transcript missing]

Finally, spatial audio coding, it's a new work item. It's going to begin at the next MPEG meeting in July. I'm very excited about it and what I think is so great is that I told you about HEAAC, 48 kilobits per second to my car. What if you want to do that and say now it's 5.1 channel? Well, I think the spatial audio coding could make that happen. So, I think it's a very powerful technology that I hope will open a lot of markets, but we're just beginning that.

Now, I'm going to talk about these technologies and very quickly give you the 50,000 foot view of what makes them work. This is MP3. A lot of boxes, just look at the blue boxes. What you see is the 32-band filter bank that's common for layers 1, 2, and 3. Layer 3 follows it by an 18-point MDCT. The result is a high-resolution filter bank, and this is the first component to get compression. It's what I call statistical compression. You need to de-correlate the signal, and just in a mathematical sense, you get compression.

The second component is the perceptual model, which is up on the left. This goes one better and says, "Okay, I see what the mathematical model says, and statistically speaking, I can get rid of some bits." But now the perceptual model says, "Well, what can you really hear?" And so based on that model, it says, "Okay, it's important to code this. You don't have to code that at all." And so it is really driving the allocation of bits in the whole coder.

And then finally, the blue box, noisless coding, is essentially an entropy matching block, and it's needed to deal with some mathematical aspects of compression. And so in a very first-order approximation, the filter bank is like a factor of 2 compression, the perceptual model about 2.5, and the lossless coding another factor of 2. You get to about a factor of 10 in this classic perceptual coder framework.

Now, AAC, it's better. What really happened? A lot more boxes. I'm not going to go into that. But now you've got a 1024-point filter bank, even higher resolution. It made a difference. But You get the same perceptual model, the same noiseless coding. So, in the first approximation, it's just a cleaner looking layer three in which you don't have the two filter banks, but just one. Same principles apply.

The classic perceptual audio coder. Now what is this other stuff I was talking about? High efficiency AAC? Well, really it's AAC core with something else. And what you begin is with an encoder that down samples a signal and just codes the first half of the bandwidth with AAC.

But it has some side information. The encoder looks at the whole thing and it says, "Hey, can I approximate that upper band by what I know about the lower band?" And I send some hints. And that's the side information. The answer is yes. And it works incredibly well. So in the decoder, you've got a classic AAC decoder. You get back the hatch mark here.

And then you essentially map it up to the high band, fix it up, equalize it with the side information, and you get the wide band. So the point is, you got as a foundation the classic perceptual coder, but the new dimension is this bandwidth coding. We had to use a new idea to get more compression. This is one of the new ideas.

Now spatial audio coding is another case of a new idea. You have a five-channel signal. You present it to the encoder, and the first thing it does is it mixes it down to, let's say, two channels, and encodes it with a classic AAC coder.

[Transcript missing]

why what's new well the answer is nothing but it's mpeg so if you look at the mpeg standards there's compression but nothing lossless and so as i said before uh you know i'm the i'm the bbc i save all my programs i got disc farms that stuff's worth some money i want to make sure i can read that 20 years from now and standards provide a kind of a comfort level where they can say well the standard won't change i've got multi-vendor support i'm going with the standard as opposed to a proprietary solution where that company maybe they're going to go out of business so it doesn't have to be new and better it can just be standardized and that's the role that we want to fulfill with this and i think it's a it's a legitimate one Okay, I want to give you one demo. It's AAC versus high efficiency AAC.

And in fact, I'm going to do it at 32 kilobits per second. I just want to show you that this is a new thing, and I think you'll see that relative to AAC, there's a remarkable difference delivered by HEAAC. So we can go to the demo machine. Okay, we're not going to go there. Oh, it's back. Okay, so 32 kilobits. I'm going to play AAC. Can I just say one word? It's band limited. I can't do better than that with this classic architecture. So let me just bounce between that and HEAAC.

[Transcript missing]

So I wish I could demo the spatial coding, but I don't have five channel here. But I just wanted to show you that MPEG has got some new stuff going. Again, this is at a bit rate which is 32 kilobits per second for stereo. That's â€“ I would have said that was ridiculous several years ago. But it has a place in the marketplace. It sounds awfully good.

And as I said, XM radio runs that at 48. But in this room, 32 says â€“ everybody says, I hear the difference. So you kind of push the compression. So you realize what's going on. But can we have the slides back again? But I think the answer is cool â€“ cool technology.

Okay, so I have one more slide, find out more. Two URLs, the one is the MPEG homepage, the second one is the audio homepage. So I certainly invite you to go to the audio homepage, and then there's some books on MPEG. These tend to be all of MPEG, but you'll find a chapter on audio, and so there's a couple for your reference.

And that's all I have. Thanks Schuyler. And now to something completely different â€“ a video. We're going to try and cover these three questions: What are they? What can they do for us? And what are they going to do for us in the future? Well, everybody loves pictures, and the test of the video projection is whether we can see the moon in the upper right-hand corner. And we can.

That's great. But they take an awful lot of bits if you don't have any compression. Home video telephone, for example, six megabits just for home video telephone. High-definition TV gets you up to one and a half gigabits â€“ an incredible number of bits if you can't do compression.

For example, now, here's what compression can do for us. For the home video telephone, we can get compression ratios of 50 with very, very high quality, or push it up to as high as 300 if we're comfortable with usable quality. I won't go through all of these, but down at entertainment television or high-definition TV, we can get compression ratios in the range of 40 or 50 with very high quality, or even with a little more for quality that's usable, something like you're used to on your cable television. The current video coding standards look like this.

And here are the dates in which they were introduced. H261, I guess, was one of the first ones to catch on in a big way, and it's used even today in video conferencing. Internet video conferencing still uses this a little bit. MPEG-1 came on in 1992. That was aimed mostly at compact disks. Karochi machines use MPEG-1 almost exclusively.

MPEG-2 was jointly done with the ITU. ITU gave it this number, H262. MPEG gave it this â€“ well, it's got a bigger number, actually, but everybody calls it MPEG-2. And that's what's used in today's digital television, HDTV, DVD, and personal video recorders. A whole bunch of things use MPEG-2. This is probably the most successful video standard ever.

H263 is an ITU standard, follows on H261. It's a better video conferencing standard. MPEG-4, part two is the video, came on a few years ago, aimed at PC and Internet applications. It's more than a compression standard. It just has a plethora of things that it's trying to do with MPEG-4. And last but not least, just coming on a few months ago, is H26 â€“ this, again, was jointly developed by ITU. ITU and MPEG. ITU gave it this number, H264.

MPEG calls it AVC. And that's a bit of a confusion. You'll find people bouncing back and forth. I tend to call it H264. David tends to call it AVC. And different people just bounce back and forth. So if you can just plant in your brain, AVC and H264 are the same. They're the same animal. And it's evolving.

A little technology. I don't know how much technology to introduce here, actually. How many were at the H.264 session yesterday? Same time, same place. Ah, a lot of people. Well, and some weren't. Okay, well, I'll mention a little technology. Of course, we all like color pictures. And we know we send color â€“ they come out of the camera as three components, red, green, and blue, and those can be digitized, of course. But if you just do that, you end up with a lot of bits.

We don't send the red, green, blue. We convert that with a linear transform into luminance and two chrominance signals. It's a reversible transform, but with this transform, it enables us to subsample the two chrominance signals. The chrominance is usually less samples per picture than the luminance. A little more technology. Jargon. I-frames, P-frames, and B-frames. Well, what do these mean? Well, what it means is typically we don't send the pixels themselves. We divide the picture up into small blocks. I believe this is eight by eight blocks. It's eight pixels by eight pixels.

Sometimes they're four by four. And instead of sending the pixels, we transform those into transform coefficients. It's a discrete cosine transform is what's used by everybody today. Historically there were other transforms, but this is what we use. So we don't send the pixels, we send transform coefficients. And iframes means, well, we just send the transform coefficients in one frame, very similar to the way JPEG does. The coding does not depend on any other frame.

Now we also use motion compensation. If we want to send this block in the current frame, we try to predict it from the previous frame. And since video often has motion, the previous frame, the block is in a different position. So we use that shifted block as a prediction of the block we're trying to send in the current frame, and we send the difference between the current frame and the previous frame. If this is an 8x8 block, we get 64 differences, we transform the difference, and send that. And that's what we call a P-frame. The motion compensated predicting frame, it depends on one other frame.

We also use bidirectional motion compensated prediction. In this case, if we wanted to send this block, we calculate the same motion compensated prediction in the previous frame, but we also calculate a motion compensated prediction in the future frame. So we can send this block in the current frame, and we send the difference between the current frame and the previous frame. And that's what we call a P-frame.

The motion compensated predicting frame depends on one other frame. We then calculate the two differences from the current frame to the previous frame, current frame to the future frame, add up the differences, divide by two, and send the average. And that's what's referred to as a B-frame, bidirectional motion compensated predicted frame. This coding depends on two other frames.

We put it all together, you get a block diagram that looks like this. I won't go through all the components. Here's a predictor. Predictor goes into a subtractor. You calculate the difference, transform the difference, quantize, code it, send it off to the transmitter. A decoder does virtually the same thing.

It calculates a prediction, but the received signal is a difference signal, so that has to be added to the prediction to get the decoded image. So that's how all these coders work. Now an important issue on these standards is that they specify the bitstream. They tell you what the bits are, and they tell you how to decode mostly.

They tell you how to decode those bits. They do not specify the encoder. They do not tell you how to produce the bits. That's the difference between the audio and the video standards. The encoders, in fact, continue to improve for years and years and years after the standard is finalized. MPEG-2 encoders are still improving every single year.

MPEG-4 video, part two is video, there are all kinds of other parts, has a wide range of bit rates and a wide range of options. Now here's the picture we used to describe MPEG-4. MPEG-4 was aimed at coding separate 3D objects, separate audio objects, separate video objects, and to allow those objects to be coded and transmitted separately and composed at the receiver, at the decoder, to produce the final picture. The final picture might be choosable at the transmitter, might be choosable at the receiver. I think it's fair to say this vision is not implemented very much anywhere. The academics are having a great time with it, hasn't really hit the marketplace.

. There are other features like text to speech, face and body animation, wireframe models, sprite coatings of backgrounds, just a wide variety of features. The newest video coding standard, the one you heard about yesterday, the one we're introducing at Apple this week actually, is the H.264AVC standard. Much better compression than MPEG-2, approximately twice as much as MPEG-2 on the compression side.

So for example, for PC or CD-ROM, you can use the H.264AVC standard, and you can also use the H.264AVC standard for video coding. . For PC or CD-ROM applications, which typically MPEG-1 handled, compression ratio of 55, well, these compression ratios, as you know, depend on the quality of the coding, so this is a range.

55, you can double that over 100. For normal TV, compression ratios of 60, figure, double that, you can get over 100. Double the number of channels that you can send on the digital transmission. For HD, again, doubling that would mean you could get double the number of HD pictures that you can on today's cable.

Now, what's in this H.264 technology? You heard much of this yesterday, and I won't go through these in great detail. They use a 4x4 transform instead of an 8x8 transform. The motion compensation blocks, instead of all being the same size, they're all the same size. . . The motion compensating blocks, instead of all being the same size, can have different sizes. If you're on the edge of an object, you want to have small motion compensated blocks. There's a deblocking filter. Many of these compression algorithms, if you really squeeze them down, you begin to see the 8x8 blocks. So there's a deblocking filter that compensates for that.

What else? There's an intra-prediction. Instead of coding the little blocks separately, they can use predictive coding to do that. There's some error resilience techniques. If you have enough bits, you can actually send the pictures several times. So if one of them doesn't make it during the transmission, you can use one of the others. You don't have to send the whole picture.

You can send pieces of the picture. Some pieces are more important than others. Multiple reference frames. This coder allows you to use in your motion compensation a whole bunch of previous frames, not just one. So you can choose which of the reference frames you want to use. What's coming up?

Well, in MPEG, in H.264AVC, an amendment just being worked on now is called Fidelity Range Extensions. That's a mouthful. We typically call it FREXT. That's a one-syllable acronym. Perfect. It allows up to 12 bits per pixel. Prior to this, most of these standards used 8. This 12 may be extended, actually, to 16. It allows higher color resolution.

Typically, the color resolution has been subsampled by two in both horizontal and vertical directions. The subsampling can be completely eliminated for professional applications. It has an optional 8x8 DCT. On some videos, the 8x8 DCT works a little better than the 4x4 DCT, so they threw in an optional one. And an alpha channel is being considered currently. currently.

Another one that's just getting underway. This is MPEG21. Now, you may ask, "How does MPEG come up with these numbers?" 1, MPEG2, MPEG4, MPEG7, MPEG21. The answer says, "We don't know." Right. We have no idea. It's a marketing thing, I think. Scalable video coding. Scalable video coding has a checkered history in this business. MPEG2 had a scalable video coding profile.

It was never really implemented as far as we know. It's a great match for networking that has multicast. You know, the multicast IETF mechanisms for the internet. Unfortunately, multicast is, it really isn't everywhere. That's the problem. So if you want to do scalable multicast broadcast, you may not be able to get everywhere.

So it's not used very much. MPEG21 is going to give it another try and we'll see what happens. H265, that's more or less a placeholder, actually. It'll be the follow-on to H264. At this stage, it's probably a research effort. And that's how most of these video standards come about, actually. A research effort starts and then technology is either adopted or tossed.

And then after quite a number of years, in the case of MPEG, it's a little bit more complicated. But it's a good example of how, in the case of H264, the development of the standard took place with contributions from lots and lots of people. The MPEG video standards typically have more than a couple hundred people contributing to these standards.

So H265, we have high hopes for. I'll give it six or seven years, maybe. Another issue on these standards, of course, is the processing power that we have to deal with. A lot of the ideas are not particularly new, but with the incredible processing power that our chip friends have provided us, we can now implement these ideas. So given a few more years, hopefully the curve will continue to give us more processing power in the future and we can implement a lot of these ideas. Okay, that in a nutshell is what video standards are all about.
