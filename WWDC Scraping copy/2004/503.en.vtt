WEBVTT

00:00:13.040 --> 00:00:13.890
Good morning.

00:00:14.230 --> 00:00:17.540
Welcome to session 503:
Optimizing for Power Mac G5.

00:00:17.720 --> 00:00:20.130
It's been about a year since
we introduced – actually,

00:00:20.150 --> 00:00:23.900
exactly a year since we
introduced the Power Mac G5 chip.

00:00:23.900 --> 00:00:27.740
And last year, if you're here at WWDC,
most of the content that we

00:00:27.750 --> 00:00:31.840
presented dealt a lot with the
architecture of the G5 chip itself.

00:00:31.900 --> 00:00:36.930
We had optimization labs running
several days into the night,

00:00:36.930 --> 00:00:39.900
so late night at times,
trying to get as much information

00:00:39.900 --> 00:00:43.740
to you as a developer to understand
the differences between the G4

00:00:43.750 --> 00:00:45.900
chip architecture and the G5.

00:00:45.900 --> 00:00:47.890
There are very stark differences there.

00:00:47.900 --> 00:00:51.100
This year, we want to, again,
reemphasize the architectural

00:00:51.150 --> 00:00:54.900
differences and why it matters
to writing optimized code.

00:00:54.900 --> 00:00:59.480
But we also want to now make sure that
you guys understand how to utilize the

00:00:59.530 --> 00:01:03.790
tools that we supply in our toolset,
as well as the compiler options

00:01:03.790 --> 00:01:07.900
that you have in terms of helping
you optimize your code itself.

00:01:07.900 --> 00:01:08.900
So we have several speakers this morning.

00:01:08.900 --> 00:01:11.900
From the compiler team
to our performance group,

00:01:11.900 --> 00:01:15.900
as well as a guest speaker
from IBM this morning.

00:01:15.900 --> 00:01:19.340
So with that,
I'd like to start off this morning,

00:01:19.350 --> 00:01:23.290
introduce Sanjay Patel
from our performance group.

00:01:30.190 --> 00:01:31.100
Good morning.

00:01:31.100 --> 00:01:34.200
It's a tough time slot.

00:01:34.210 --> 00:01:38.500
For those of you who were here last year,
we're going to do a bit of review.

00:01:38.690 --> 00:01:40.830
How many of you were here
last year for this talk?

00:01:40.860 --> 00:01:45.460
Okay, so we're going to go through some
G5 architecture and then we want

00:01:45.460 --> 00:01:49.760
to talk about things you can do to
help improve your code on G5 and,

00:01:49.820 --> 00:01:52.200
in fact, all platforms, really.

00:01:52.290 --> 00:01:55.650
And then we'll turn it over
to some of our compiler guys

00:01:55.650 --> 00:01:58.380
to help guide you through your
process of optimizing your code.

00:01:59.390 --> 00:02:01.590
So to start off with,
when you're talking about the G5,

00:02:01.660 --> 00:02:05.330
you have to of course start
with the PowerPC 970 chip.

00:02:05.380 --> 00:02:11.150
And this is of course a super pipeline,
super scalar processor we got from

00:02:11.190 --> 00:02:13.850
– we teamed up with IBM to make.

00:02:13.910 --> 00:02:17.660
It's based on the Power 4
server architecture.

00:02:18.040 --> 00:02:22.450
The big addition that we had to
make it an Apple chip was to add

00:02:22.450 --> 00:02:25.890
what we call the AlteVec engine,
also known as the Velocity Engine.

00:02:25.910 --> 00:02:30.540
This is a 128-bit vector unit,
which does floating-point

00:02:30.570 --> 00:02:32.100
and integer math.

00:02:32.230 --> 00:02:35.420
And the other big difference
is we have this high-bandwidth,

00:02:35.530 --> 00:02:39.940
point-to-point interface that connects
the chips to the memory controller.

00:02:40.930 --> 00:02:44.580
And to help take advantage of all the
bandwidth that we have available in the

00:02:44.580 --> 00:02:48.620
system in theory are these automatic
hardware prefetch engines that help put

00:02:48.620 --> 00:02:51.420
all that theoretical bandwidth to use.

00:02:53.770 --> 00:02:58.230
So this is a die shot of the 970
chip to – for hardware engineers,

00:02:58.230 --> 00:02:59.690
this is kind of like pornography.

00:02:59.700 --> 00:03:02.200
You know, you look at this and you'll
hear them say things like,

00:03:02.200 --> 00:03:07.180
"Look at the FPUs on that
one." For software engineers,

00:03:07.180 --> 00:03:09.860
what you want to take away from
this is that there are lots of

00:03:10.010 --> 00:03:13.690
execution units available for your
program that all operate in parallel.

00:03:13.840 --> 00:03:17.930
So two independent load store units,
two independent IEEE-compliant

00:03:18.050 --> 00:03:19.380
floating point units.

00:03:19.510 --> 00:03:21.140
You have the full
implementation of AltaVec,

00:03:21.220 --> 00:03:23.960
two fixed point units as well.

00:03:24.020 --> 00:03:28.060
So there's just a lot of space here
to get things done in parallel.

00:03:28.300 --> 00:03:30.760
Another way to look at this
from the software perspective,

00:03:30.870 --> 00:03:33.400
again, is if you start at the top,
you have the L1 cache,

00:03:33.400 --> 00:03:35.360
which holds your instructions.

00:03:35.460 --> 00:03:39.080
From the L1 cache,
instructions flow into a fetch queue,

00:03:39.170 --> 00:03:42.620
and from there they get dispatched,
up to five instructions on each clock.

00:03:42.710 --> 00:03:48.010
So at 2.5 GHz,
you're dispatching over 10 – in theory,

00:03:48.010 --> 00:03:51.550
10 billion instructions per second.

00:03:51.810 --> 00:03:54.690
Now that's actually the narrow
point of the 970 architecture.

00:03:54.720 --> 00:03:57.300
So as you can see,
this stuff in the kind of greenish-gray

00:03:57.300 --> 00:03:59.140
is out-of-order execution.

00:03:59.270 --> 00:04:03.280
So you can feed 12 independent
execution units with 10 issue

00:04:03.390 --> 00:04:05.420
queues once you've dispatched.

00:04:05.520 --> 00:04:08.040
And then again,
your instructions complete at the

00:04:08.040 --> 00:04:10.960
bottom of this picture in order,
up to 5 per clock.

00:04:12.330 --> 00:04:13.930
Now,
a good way to put this in perspective is

00:04:14.000 --> 00:04:17.640
to look at how the G5 compares to the G4.

00:04:17.720 --> 00:04:22.200
And so, we keep talking about the
parallelism of this chip.

00:04:22.350 --> 00:04:24.400
One way to measure that is
how many instructions you

00:04:24.400 --> 00:04:26.800
keep in flight simultaneously.

00:04:26.930 --> 00:04:30.220
And for the G5,
it's over 200 instructions in

00:04:30.230 --> 00:04:34.600
flight compared to a little
over 30 for a G4 architecture.

00:04:34.720 --> 00:04:37.000
And we've also increased
the pipeline stages,

00:04:37.000 --> 00:04:41.700
so you can see that it's more than
doubled for a simple integer instruction.

00:04:41.790 --> 00:04:43.900
And usually you'd say, "Well,
that's not so good.

00:04:43.900 --> 00:04:47.340
Why'd you do that?" The reason
we increase the pipeline stages

00:04:47.340 --> 00:04:52.100
is to help increase frequency,
so we just announced that we hit 2.5 GHz.

00:04:52.230 --> 00:04:55.030
In order to hit higher and
higher frequency numbers,

00:04:55.030 --> 00:04:57.120
you increase the pipeline depths.

00:04:57.840 --> 00:05:00.310
We talk about some of the
execution units again.

00:05:00.360 --> 00:05:04.800
We've doubled up on the load store units,
doubled up on floating point units.

00:05:04.910 --> 00:05:08.800
You now have two fixed point – two
general purpose fixed point units,

00:05:08.800 --> 00:05:12.000
whereas on a G4 architecture you
have up to three simple units that

00:05:12.000 --> 00:05:15.490
do things like adds and subtracts,
and one complex unit to handle

00:05:15.490 --> 00:05:17.800
things like multiplies and divides.

00:05:17.800 --> 00:05:19.800
The vector units are pretty similar.

00:05:19.800 --> 00:05:22.800
You have an ALU that handles
floating point and integer,

00:05:22.800 --> 00:05:23.800
and you have a permute unit.

00:05:23.800 --> 00:05:26.800
For any of you who have done any
vector programming with AltaVec,

00:05:26.800 --> 00:05:30.790
you know what the power of the permute
unit is in terms of swizzling data

00:05:30.800 --> 00:05:32.800
out of memory and into the registers.

00:05:34.620 --> 00:05:37.850
As we work our way out from the core,
the biggest program or visible

00:05:37.850 --> 00:05:40.700
difference that you'll find is that
the cache line size is different.

00:05:40.700 --> 00:05:44.870
It's under 28 bytes,
whereas it was 32 for a G4.

00:05:44.970 --> 00:05:46.980
Now, that can either be a really
good thing or a bad thing,

00:05:46.980 --> 00:05:52.020
and I'll show an example
of how that happens.

00:05:52.130 --> 00:05:56.440
As we work our way again to memory,
you have the L1 cache is the same size,

00:05:56.530 --> 00:05:58.920
different associativity and write policy.

00:05:59.030 --> 00:06:02.080
The L2 cache – sorry,
the L1 instruction cache

00:06:02.080 --> 00:06:03.810
is also doubled up in size.

00:06:04.060 --> 00:06:05.500
There's the L2 cache.

00:06:05.560 --> 00:06:10.080
The L2 cache is also doubled
in size compared to 7450.

00:06:10.350 --> 00:06:13.300
You'll notice there's no
L3 cache on a G5 system,

00:06:13.450 --> 00:06:17.830
whereas you had up to 2
megabytes of L3 on a G4 system.

00:06:17.900 --> 00:06:20.340
Now,
we've made up for that by increasing the

00:06:20.340 --> 00:06:24.850
processor bandwidth substantially – one,
by doubling the width

00:06:24.990 --> 00:06:28.140
of the DDR interface and
increasing its frequency,

00:06:28.140 --> 00:06:30.160
but also the front-side bus.

00:06:30.220 --> 00:06:32.950
And this slide is actually
a little out of date.

00:06:33.030 --> 00:06:35.840
At 2.5 GHz,
we've increased the front-side

00:06:35.840 --> 00:06:39.120
bus frequency to 1.25 GHz,
so you can actually, in theory,

00:06:39.120 --> 00:06:42.710
get 5 gigabytes per
second in each direction.

00:06:45.020 --> 00:06:48.690
So I want to talk about some programmer
problems I've seen over the last

00:06:48.740 --> 00:06:51.260
year since we've introed the machine.

00:06:51.260 --> 00:06:54.910
And the one that comes up most frequently
turns out to be a rather simple thing.

00:06:55.140 --> 00:06:57.900
It's conversions from
floating point to integers.

00:06:57.900 --> 00:07:01.150
And the reason this shows up a lot
is because when you write this in C,

00:07:01.150 --> 00:07:02.820
it looks really cheap, right?

00:07:02.890 --> 00:07:06.900
You just cast your variable
into int or cast it to float.

00:07:06.900 --> 00:07:08.900
But it turns out this
is not cheap at all.

00:07:08.900 --> 00:07:12.210
Particularly on a G5,
because it has so much

00:07:12.210 --> 00:07:15.000
going on in parallel,
when it hits this condition,

00:07:15.000 --> 00:07:19.900
because the PowerPC architecture
doesn't have integer register transfers,

00:07:20.010 --> 00:07:21.900
you actually have to go to
the L1 cache and come back.

00:07:21.900 --> 00:07:25.090
So you have a load and
store operation going on.

00:07:25.510 --> 00:07:27.460
There are a lot of things you
can do to avoid the problem,

00:07:27.460 --> 00:07:30.930
and the biggest one that I've found
that actually turns out to be one of the

00:07:30.940 --> 00:07:35.890
easier solutions is simply don't do that.

00:07:35.970 --> 00:07:38.710
It turns out, again,
because it looks so cheap and easy,

00:07:38.800 --> 00:07:43.150
that a lot of people just cast from one
to the other without thinking about it.

00:07:43.240 --> 00:07:46.440
And when you examine that code,
you realize you could have stayed in one

00:07:46.440 --> 00:07:52.270
domain or the other without hampering
or affecting the algorithm in any way.

00:07:52.550 --> 00:07:54.640
The other cool way you can
get around the problem,

00:07:54.710 --> 00:07:59.360
and of course if you use this, Altevec,
you're going to get a much

00:07:59.450 --> 00:08:03.190
greater speedup because of all the
parallelism in the Altevec unit.

00:08:03.310 --> 00:08:05.790
But the Altevec unit handles
floating point and integer

00:08:05.790 --> 00:08:09.800
identically in the same register set,
so there's no memory operations

00:08:09.800 --> 00:08:11.980
when you transfer between types.

00:08:12.350 --> 00:08:17.500
Another potential optimization is
to use the gcc compiler flag – fast.

00:08:17.500 --> 00:08:20.580
This tries to schedule your loads
and stores and inserts no-ops

00:08:20.580 --> 00:08:24.340
even to separate them out to keep
things flowing through the G5.

00:08:24.440 --> 00:08:28.310
IBM's Excel compilers also do
this kind of optimization if

00:08:28.310 --> 00:08:30.980
you specify the G5 architecture.

00:08:31.910 --> 00:08:37.140
So I want to show you a quick and
really simple example of bad code.

00:08:37.200 --> 00:08:39.670
So this is a real lame loop.

00:08:39.940 --> 00:08:44.160
All it does is int to float conversion
because the loop counter in this case,

00:08:44.160 --> 00:08:47.760
i, has been declared an int,
but we're adding it into

00:08:47.760 --> 00:08:49.320
a floating point sum.

00:08:49.390 --> 00:08:52.080
So that looks really cheap, but it's not.

00:08:52.390 --> 00:08:55.330
Every time you do that add,
you're going to have to convert the

00:08:55.330 --> 00:08:58.920
i from integer to floating point
domain to store it into the sum.

00:08:58.990 --> 00:09:01.300
So how would you get around this problem?

00:09:04.160 --> 00:09:07.620
What you can do is create what
I call a shadow of the i variable

00:09:07.950 --> 00:09:09.820
in the floating point unit.

00:09:09.820 --> 00:09:14.120
So I've just named it i_fp to denote
that it's floating point value.

00:09:14.170 --> 00:09:17.000
And when I initialize i,
I also initialize its shadow.

00:09:17.000 --> 00:09:19.500
And when I increment i,
I increment its shadow.

00:09:19.670 --> 00:09:22.320
Now inside the loop,
we're going to use the floating

00:09:22.330 --> 00:09:25.490
point value for the sum
rather than the integer value.

00:09:25.650 --> 00:09:27.990
So on a G5, I measured this.

00:09:28.000 --> 00:09:31.500
It turns out this code is three
times faster than the previous code

00:09:31.500 --> 00:09:34.820
where you're doing the conversions
because this code won't have to do

00:09:34.830 --> 00:09:36.950
all the load and store operations.

00:09:38.660 --> 00:09:42.160
And the next biggest thing I've seen
over the last year is just improvements

00:09:42.250 --> 00:09:44.940
to try to schedule your code.

00:09:45.010 --> 00:09:48.600
And like I say,
the G5 has a lot of execution units

00:09:48.600 --> 00:09:52.250
that you can operate in parallel,
but if you write dependent code – so

00:09:52.250 --> 00:09:54.950
one operation depends on the next,
depends on the next,

00:09:54.950 --> 00:09:58.020
it's all serial – you're not going
to take advantage of all those units,

00:09:58.040 --> 00:10:00.620
and furthermore, you're not going to take
advantage of these long pipelines.

00:10:00.640 --> 00:10:04.180
You have to schedule your code
so that you're filling in all

00:10:04.180 --> 00:10:08.290
these pipeline slots instead of
causing bubbles in execution.

00:10:09.400 --> 00:10:10.770
So, compiler help is here.

00:10:10.810 --> 00:10:15.650
You have gcc 3.3,
which has G5 architecture tuning,

00:10:15.740 --> 00:10:19.100
so that tries to schedule for
the available units and slots.

00:10:19.170 --> 00:10:23.510
XLC also has the same kind of
flag where you specify that

00:10:23.510 --> 00:10:26.170
you have a G5 architecture.

00:10:26.260 --> 00:10:29.020
And the other thing you
can do is use Shark,

00:10:29.030 --> 00:10:31.690
which you've probably heard about,
and we'll talk about it in

00:10:31.690 --> 00:10:33.300
more detail tomorrow at 3:30.

00:10:33.450 --> 00:10:37.200
We have a full session on how to use
Shark and what it can do for you.

00:10:39.770 --> 00:10:43.030
Now again, I mentioned that we've
increased the pipeline stages

00:10:43.040 --> 00:10:45.450
compared to a G4 on the G5.

00:10:45.550 --> 00:10:47.090
So what does that mean?

00:10:47.240 --> 00:10:50.770
Well, it means it takes longer in
terms of clocks for a simple

00:10:50.770 --> 00:10:52.540
instruction to complete.

00:10:52.630 --> 00:10:57.270
So, for example, an addition instruction
may take one cycle on a G4,

00:10:57.270 --> 00:11:00.560
but it may take two
cycles of latency on a G5.

00:11:01.410 --> 00:11:04.780
So what you want to do is account
for that in your program by

00:11:04.780 --> 00:11:08.650
kind of grouping a bunch of
the similar operations together.

00:11:08.730 --> 00:11:11.980
So that means you can unroll
your important loops or you

00:11:12.020 --> 00:11:13.300
can use a compiler flag.

00:11:13.300 --> 00:11:16.510
And as well, you want to schedule
your code for the G5,

00:11:16.510 --> 00:11:19.610
so you're going to fill in
all those pipeline slots.

00:11:19.770 --> 00:11:22.930
Now, people often ask, "Well,
shouldn't the compiler do that for

00:11:22.930 --> 00:11:25.940
me?" And in all these examples,
you can always ask that question,

00:11:25.940 --> 00:11:28.550
"Shouldn't the compiler do
that for me?" In some cases,

00:11:28.590 --> 00:11:32.220
the compiler can do it for you
if you specify the right flags.

00:11:32.250 --> 00:11:36.400
But there's always a downside if
you just lean on the automatic

00:11:36.400 --> 00:11:39.930
way to work over this problem,
because the compiler hardly

00:11:40.200 --> 00:11:43.130
usually doesn't know when
a loop is important or not.

00:11:43.430 --> 00:11:46.230
That's something you have to tell it.

00:11:46.350 --> 00:11:49.420
So if you choose to unroll all
loops or unroll most loops,

00:11:49.420 --> 00:11:52.150
you're going to have a
big increase in code size,

00:11:52.230 --> 00:11:54.710
which could be detrimental
to your performance.

00:11:54.770 --> 00:11:57.700
That's why as a first pass,
you should profile your program.

00:11:57.830 --> 00:12:01.500
And try to do manually some
of these optimizations just

00:12:01.500 --> 00:12:03.500
in the important spots.

00:12:05.090 --> 00:12:08.370
So here's another example
of some code that's,

00:12:08.390 --> 00:12:10.700
again, just a silly example.

00:12:10.850 --> 00:12:16.640
We're just going to sum a
bunch of ones in this case.

00:12:16.880 --> 00:12:20.520
The 970 architecture of the G5
has two floating-point units,

00:12:20.530 --> 00:12:22.560
and they're each six stages long.

00:12:22.580 --> 00:12:26.120
So this code is only going to
get approximately one-twelfth

00:12:26.430 --> 00:12:31.190
efficiency because every instruction
is dependent on the previous sum.

00:12:31.560 --> 00:12:33.490
So this is a simple example.

00:12:33.560 --> 00:12:36.230
Now, the code has exploded, right?

00:12:36.340 --> 00:12:39.140
Because we're trying to fill
all the pipeline stages,

00:12:39.170 --> 00:12:43.460
and here we actually only unrolled
to eight ways of partial sums,

00:12:43.490 --> 00:12:46.130
so we wouldn't fill all
of 12 pipeline slots.

00:12:46.170 --> 00:12:50.550
You would actually want to do 12 in
order to maximize your gains on the G5.

00:12:50.590 --> 00:12:55.720
So you can think of the floating-point
units as either one 12-stage

00:12:55.720 --> 00:12:58.370
pipeline or 12 single units.

00:12:58.650 --> 00:13:00.350
But they're all going
to operate in parallel.

00:13:00.400 --> 00:13:04.810
So this code turns out to be 10
times faster just using partial

00:13:04.810 --> 00:13:07.110
sums instead of one variable.

00:13:10.150 --> 00:13:13.100
The other big thing you have to worry
about when you're optimizing code –

00:13:13.100 --> 00:13:17.300
and this goes for all architectures,
but it's particularly bad on a G5

00:13:17.300 --> 00:13:19.600
because the G5 core is so good.

00:13:19.630 --> 00:13:21.810
It makes memory look really, really slow.

00:13:22.000 --> 00:13:26.360
What you have to do is try
to reduce operations where

00:13:26.730 --> 00:13:29.000
you're waiting on memory.

00:13:29.000 --> 00:13:31.000
So effectively reduce your latency.

00:13:31.000 --> 00:13:32.950
There are a couple of ways to do that.

00:13:33.000 --> 00:13:37.000
One is to rely on
hardware prefetch engines.

00:13:37.000 --> 00:13:38.830
And I'll show you
another example of that.

00:13:39.000 --> 00:13:42.420
The other thing you can do is use
software prefetch instructions to

00:13:42.420 --> 00:13:46.000
get the data before you actually
need to use it for computations.

00:13:46.000 --> 00:13:48.590
For example, if you're in a loop,
you can batch all your loads

00:13:48.600 --> 00:13:51.000
together at the top of the loop,
do a bunch of math,

00:13:51.000 --> 00:13:52.840
and then do stores at the bottom.

00:13:53.040 --> 00:13:57.120
That's going to perform better than
doing serial operations of load,

00:13:57.120 --> 00:13:58.190
math, store.

00:14:00.270 --> 00:14:04.960
I mentioned that the data cache is
different on the G5 than the G4,

00:14:04.960 --> 00:14:07.580
and the biggest difference
is the cache line size.

00:14:07.650 --> 00:14:09.670
So it's four times as big.

00:14:09.740 --> 00:14:10.430
What does that mean?

00:14:10.510 --> 00:14:13.260
Well,
it means you may get one-fourth the cache

00:14:13.260 --> 00:14:15.950
misses if your data is organized nicely.

00:14:16.040 --> 00:14:19.430
It may also mean that you're
getting really terrible performance

00:14:19.430 --> 00:14:23.340
if you're accessing one byte,
skipping 127 and accessing

00:14:23.340 --> 00:14:24.630
another one byte.

00:14:24.690 --> 00:14:27.140
At that point,
you're getting less than 1%

00:14:27.140 --> 00:14:28.740
efficiency from your cache.

00:14:30.320 --> 00:14:33.130
So what you want to do – and
this is sort of basic CS,

00:14:33.130 --> 00:14:33.760
right?

00:14:33.870 --> 00:14:37.280
Pack your data together
to maximize its locality.

00:14:37.370 --> 00:14:42.730
So as you walk through your array,
you want to be stepping sequentially

00:14:42.730 --> 00:14:44.530
rather than jumping around.

00:14:44.750 --> 00:14:48.140
This has an additional benefit
of triggering the hardware

00:14:48.140 --> 00:14:50.600
memory – the hardware prefetcher.

00:14:50.600 --> 00:14:54.680
So the CPU is automatically going to
detect that you're walking a straight

00:14:54.680 --> 00:14:58.320
line either up or down through
memory and start prefetching cache

00:14:58.320 --> 00:15:00.400
lines from memory into the cache.

00:15:01.620 --> 00:15:04.640
So again, here's another simple example.

00:15:04.640 --> 00:15:08.510
This is a classic two-dimensional
array where we're walking

00:15:08.510 --> 00:15:10.130
the wrong way through it.

00:15:10.210 --> 00:15:13.150
We're iterating on the
columns – or rather,

00:15:13.150 --> 00:15:15.830
the rows – rather than the columns.

00:15:16.190 --> 00:15:19.210
So we're skipping large chunks of memory.

00:15:19.250 --> 00:15:22.620
In this case,
what you'd want to do is just switch the

00:15:22.620 --> 00:15:28.060
four loops so that you could sequentially
access every element in this array.

00:15:28.240 --> 00:15:31.040
So any guesses on how much
faster this is going to be?

00:15:31.040 --> 00:15:33.440
It's a big difference.

00:15:33.910 --> 00:15:38.990
This is simple stuff,
but 30 times faster if you do the right

00:15:38.990 --> 00:15:41.500
thing rather than the wrong thing.

00:15:41.700 --> 00:15:47.460
It highlights the problem of how
important accessing memory is.

00:15:48.580 --> 00:15:51.460
So I just want to summarize some of the
things you should be doing and looking

00:15:51.460 --> 00:15:53.820
at while you're trying to optimize code.

00:15:53.820 --> 00:15:58.500
The first thing to do is try to
unroll and schedule important loops.

00:15:58.560 --> 00:16:00.150
Because you have all these
execution units – the

00:16:00.150 --> 00:16:03.970
independent floating-point units,
the independent load-store units.

00:16:04.010 --> 00:16:07.050
You have hundreds of
instructions in flight.

00:16:08.450 --> 00:16:10.400
This number is actually
out of date as well.

00:16:10.400 --> 00:16:14.480
You can now use Altevec to
calculate more than 36 gigaflops

00:16:14.480 --> 00:16:16.920
per second at 2.5 gigahertz.

00:16:17.030 --> 00:16:19.750
This is of course the best
solution if you have code

00:16:19.770 --> 00:16:21.840
that's just massively parallel.

00:16:21.840 --> 00:16:24.910
You can operate on all the
elements simultaneously.

00:16:26.290 --> 00:16:29.560
For those of you writing
floating-point code,

00:16:29.560 --> 00:16:31.960
the G5 has a hardware
square-root instruction,

00:16:31.960 --> 00:16:36.600
which can be enabled with gcc
with this PowerPC GP opt flag.

00:16:36.670 --> 00:16:40.530
XLC will recognize that this
instruction is available if you

00:16:40.530 --> 00:16:42.840
specify the G5 architecture.

00:16:42.940 --> 00:16:47.180
This has made a very large difference
in some ray-tracers and renderers and

00:16:47.180 --> 00:16:51.790
other programs that I've looked at that
have heavy dependence on square-root.

00:16:53.260 --> 00:16:57.100
If you're using 64-bit integers,
long-longs in C,

00:16:57.470 --> 00:17:02.400
you can turn on flags to specify
that you have a 64-bit machine,

00:17:02.400 --> 00:17:06.500
because G5 truly does have
64-bit integer registers.

00:17:06.550 --> 00:17:11.810
This can be a huge difference
for code compared to actually

00:17:11.810 --> 00:17:15.490
breaking up into 32-bit chunks.

00:17:18.830 --> 00:17:23.100
So again, the system and the chip were
designed for high bandwidth.

00:17:23.220 --> 00:17:25.700
They were designed to do
lots of things in parallel.

00:17:25.700 --> 00:17:29.700
It's part of the server heritage
coming from the Power 4.

00:17:29.840 --> 00:17:32.630
You have 40 gigabytes per
second to the L1 cache,

00:17:32.670 --> 00:17:35.560
up to 80 gigabytes per
second between the caches,

00:17:35.640 --> 00:17:38.920
and up to 5 gigabytes per
second to and from main memory.

00:17:40.170 --> 00:17:43.140
And the way you want to take advantage
of all this theoretical throughput

00:17:43.160 --> 00:17:47.500
and put it into practice is to take
advantage of hardware prefetch engines.

00:17:47.590 --> 00:17:50.950
So these will start scooping data
out of main memory and bringing to

00:17:50.950 --> 00:17:52.770
cache before you actually need it.

00:17:53.830 --> 00:17:55.250
And that's all I have.

00:17:55.320 --> 00:17:57.990
So with that,
I'd like to introduce Steve Hikiida

00:17:58.240 --> 00:18:00.770
from the IBM compiler team.

00:18:05.620 --> 00:18:06.600
Good morning, all.

00:18:06.600 --> 00:18:08.700
Good morning.

00:18:08.720 --> 00:18:14.850
I'm actually very excited to be here as
we have now introduced our Excel CC++ and

00:18:14.990 --> 00:18:21.080
Fortran compilers for the Mac OS X – or,
Mac OS X, my apologies.

00:18:22.580 --> 00:18:26.030
So IBM compiler technology.

00:18:26.160 --> 00:18:30.280
We've been in the business of compilation
technologies for over 15 years and

00:18:30.280 --> 00:18:36.520
exploiting primarily PowerPC technology,
but we've also been on

00:18:36.600 --> 00:18:40.990
about nine other platforms,
mainly IBM platforms.

00:18:41.220 --> 00:18:44.080
In among all this technology,
we've got numerous types of

00:18:44.090 --> 00:18:49.860
optimization patents that truly
exploit the PowerPC technology.

00:18:50.910 --> 00:18:55.030
Our goal in the IBM compiler
team is actually threefold.

00:18:55.150 --> 00:18:58.240
The first one is to exploit the hardware.

00:18:58.400 --> 00:19:03.680
Our actual key here is to drive
out the maximum performance we can

00:19:03.680 --> 00:19:07.310
possibly get out of the G5 processor.

00:19:07.620 --> 00:19:12.200
Among these things, we have an extensive
portfolio of optimizations.

00:19:12.200 --> 00:19:15.300
These include things like
intra-procedural analysis,

00:19:15.340 --> 00:19:19.950
which does whole-loop program analysis.

00:19:20.180 --> 00:19:25.290
It has the profile-directed feedback.

00:19:25.330 --> 00:19:28.470
Loop optimizations for parallelism,
instructions,

00:19:28.550 --> 00:19:33.290
and all for locality scheduling.

00:19:35.600 --> 00:19:40.100
One of the things that we do regularly
is we actually work very closely

00:19:40.100 --> 00:19:45.270
with the chip architecture team,
where we've been working with the core

00:19:45.270 --> 00:19:50.850
team that actually develops the chips
and provide them with information as

00:19:50.850 --> 00:19:57.840
to what ways that they may want to
change the actual chip versus also

00:19:57.840 --> 00:19:59.300
the type of information that we can
exploit within our own compilers.

00:20:02.470 --> 00:20:05.620
The second thing that our compiler
group really is focused on is

00:20:05.620 --> 00:20:07.300
specifications and standards.

00:20:07.380 --> 00:20:17.200
For our C and C++,
we are C/1999 and C++/1998 compliant.

00:20:17.210 --> 00:20:22.960
For 4-Trend, we're 4-Trend 70, 90,
95 and partial 2003.

00:20:23.050 --> 00:20:25.270
We also have OpenMP support.

00:20:25.740 --> 00:20:31.310
Primarily that was introduced on
the AX platforms and we are bringing

00:20:31.310 --> 00:20:33.370
it over to the Mac OS X also.

00:20:33.750 --> 00:20:40.060
Our developers within our CEC++ and
FORTRAN teams are also representative

00:20:40.170 --> 00:20:41.600
on the standards committees.

00:20:41.600 --> 00:20:44.840
They're not only on the
ISO standards committee,

00:20:44.920 --> 00:20:49.340
but they're also on
the OpenMP consortium.

00:20:50.470 --> 00:20:56.730
So being really focused on compatibility
and also on standard specification,

00:20:56.820 --> 00:20:59.870
our compilers is also source
codes that can be pumped through

00:20:59.940 --> 00:21:03.520
our compilers are easily portable
between numerous platforms.

00:21:03.600 --> 00:21:09.590
For example, Mac OS X, Linux, AX,
and our mainframes at OS.

00:21:09.890 --> 00:21:16.320
The third thing that we're really
focused on is the customer care.

00:21:16.560 --> 00:21:21.980
We work very closely with various ISVs
and also customers on tuning their code.

00:21:22.030 --> 00:21:25.100
As a matter of fact,
we're down in the optimization lab all

00:21:25.100 --> 00:21:29.890
this week and some of our engineers
have been working very closely with

00:21:29.890 --> 00:21:34.850
people that have brought in their code
and we've actually seen markups from

00:21:34.850 --> 00:21:40.560
anywhere between 20% and 200% even
in a short period of time by using

00:21:40.560 --> 00:21:40.560
our compilers to exploit their code.

00:21:45.740 --> 00:21:50.820
So, for the CC++ and the Fortran
compilers on Mac OS X,

00:21:50.880 --> 00:21:55.100
they are based on our
AIX and Linux compilers.

00:21:55.100 --> 00:21:58.390
On AIX and Linux,
we call it right now VisualHC++.

00:21:58.490 --> 00:22:01.160
So,
the VisualHC++ for those platforms are

00:22:01.160 --> 00:22:05.700
essentially the same compilers that
we have for the Mac OS X platform.

00:22:05.700 --> 00:22:10.530
So, this actually leverages all the
proven optimizations and language

00:22:10.530 --> 00:22:14.700
specifications that we've already
introduced on those platforms.

00:22:14.700 --> 00:22:19.940
Some of the common things between our
XLC and C++ compilers and Fortran is,

00:22:19.940 --> 00:22:23.700
as I mentioned already,
the exploitation of the G5 architecture.

00:22:23.830 --> 00:22:27.100
We are integrated with Xcode.

00:22:27.230 --> 00:22:44.500
Symbolic to Bangalore GDB.

00:22:44.500 --> 00:22:44.500
And also we support a number
of the Apple's profiling tools.

00:22:44.500 --> 00:22:44.500
The Shark one in particular
is just an outstanding tool

00:22:44.500 --> 00:22:44.500
in helping tune your code.

00:22:44.500 --> 00:22:44.500
For sure, even ourselves,
we wish that it was available

00:22:44.500 --> 00:22:44.500
on some of IBM's platforms.

00:22:44.970 --> 00:22:47.540
Among the two other things
that we have is part of what

00:22:47.540 --> 00:22:49.200
we call a technology preview.

00:22:49.220 --> 00:22:53.930
So these are features that we
are actually looking at trying

00:22:53.930 --> 00:22:56.080
to bring into our product.

00:22:56.080 --> 00:22:58.860
Although they're there right now,
they're not formally supported.

00:22:58.860 --> 00:23:00.860
And in particular is the OpenMP.

00:23:00.860 --> 00:23:05.580
So our direction is to have
full support for OpenMP 2.0.

00:23:05.580 --> 00:23:08.570
And the other one is
automatic parallelization.

00:23:11.230 --> 00:23:14.230
Specifically then for C/C++,
as I mentioned, there's the standards

00:23:14.280 --> 00:23:20.800
compliance for C99 and C++98,
exploitation of AlteVec.

00:23:20.800 --> 00:23:25.600
Although this compiler right
now can actually generate codes

00:23:25.600 --> 00:23:31.600
to the AlteVec instructions,
one of the things that we are ongoing

00:23:31.600 --> 00:23:36.660
looking at in our research and
development is the automatic syndication,

00:23:36.660 --> 00:23:40.200
or otherwise known as automatic
generation of AlteVec instructions.

00:23:40.200 --> 00:23:42.900
So these are things that we
are definitely focused on and

00:23:42.910 --> 00:23:44.610
looking at in future releases.

00:23:46.130 --> 00:23:48.700
Compatibility with GCC 3.3.

00:23:48.700 --> 00:23:49.500
This is two-fold.

00:23:49.560 --> 00:23:52.840
One is we are fully binary compatible,
so you can intermix

00:23:52.840 --> 00:23:55.000
GCC objects with our compiler.

00:23:55.000 --> 00:23:58.440
And the other one is that
we have a number of language

00:23:58.440 --> 00:24:02.380
extensions that are GCC-specific,
so you can have source

00:24:02.380 --> 00:24:04.000
code compatibility.

00:24:04.000 --> 00:24:10.890
For C++, we also have an
Objective-C technology preview.

00:24:11.400 --> 00:24:16.520
And then for XL4TRN, as I mentioned,
we already have the 4TRN 70, 90,

00:24:16.520 --> 00:24:18.970
95 and 2003 partial.

00:24:19.130 --> 00:24:24.680
We also introduced many IBM and common
industry standards language extensions.

00:24:24.800 --> 00:24:31.020
And these include some from VM,
from ZOS and other well-known

00:24:31.020 --> 00:24:33.310
platforms for 4TRN.

00:24:33.590 --> 00:24:37.770
So that concludes actually just
a quick overview of the Excel,

00:24:37.770 --> 00:24:40.200
C/C++, and Fortran compilers.

00:24:40.200 --> 00:24:44.300
If you have any questions
on how to exploit your code,

00:24:44.350 --> 00:24:47.800
how to gain even more
optimization capabilities and

00:24:47.800 --> 00:24:52.740
performance out of your G5,
come on down to the optimization lab

00:24:52.750 --> 00:24:59.110
where there's a number of us IBMers there
to help answer any of your questions.

00:24:59.320 --> 00:25:00.230
Thanks a lot.

00:25:00.330 --> 00:25:02.360
And the next person up is Ron Price.

00:25:06.840 --> 00:25:08.300
Thank you, Steve.

00:25:08.450 --> 00:25:11.300
Well,
this has been a terrific year for the G5,

00:25:11.300 --> 00:25:15.800
and I think we all now understand
what kind of power lurks in that box.

00:25:15.800 --> 00:25:20.030
But those of us who have really
worked a lot with it understand

00:25:20.030 --> 00:25:22.780
what it takes to extract that power.

00:25:22.800 --> 00:25:25.950
And Sanjay covered some of that,
and of course,

00:25:25.950 --> 00:25:31.800
Apple offers a set of tools that really
facilitates understanding your program

00:25:31.800 --> 00:25:35.800
and being able to algorithmically
allow it to extract that power.

00:25:35.800 --> 00:25:40.250
I want to talk today about
what the compiler can do to

00:25:40.250 --> 00:25:44.380
get you started on that path,
because not everyone is ready to step

00:25:44.380 --> 00:25:48.740
up and start tuning their program and
changing the algorithms and so forth.

00:25:48.820 --> 00:25:52.720
And so Sanjay mentioned a number
of compiler options that can

00:25:52.720 --> 00:25:54.800
help you in certain situations.

00:25:54.820 --> 00:25:59.090
And what we have done within the
compiler group is actually put

00:25:59.090 --> 00:26:04.800
together a mode we call Dash Fast,
and I want to talk about that today.

00:26:04.800 --> 00:26:08.870
I also want to talk about
feedback-directed optimizations,

00:26:08.870 --> 00:26:12.080
which is another component
of the Dash Fast mode that

00:26:12.080 --> 00:26:14.800
can help you significantly.

00:26:14.800 --> 00:26:17.310
And then, of course,
you have all heard the announcement

00:26:17.410 --> 00:26:21.260
this week that we're on the
path to deliver with Tagger our

00:26:21.260 --> 00:26:26.370
initial cut at auto vectorization,
and I want to talk about

00:26:26.460 --> 00:26:28.200
exactly what that is.

00:26:28.460 --> 00:26:33.580
So GCC and the Dash Fast Mode.

00:26:33.580 --> 00:26:37.810
Could I just ask in here if
anyone's using this mode today?

00:26:37.920 --> 00:26:40.860
Ah, geez, amazing.

00:26:41.090 --> 00:26:45.200
We've had so little feedback on how
it has been working for people that

00:26:45.200 --> 00:26:48.660
we've wondered if anyone's using it,
and that's why we wanted

00:26:48.710 --> 00:26:50.110
to talk about it today.

00:26:50.430 --> 00:26:55.590
The Dash Fast Mode is really a collection
of a lot of the compiler options,

00:26:55.650 --> 00:26:59.400
but in many ways it's more than
just a collection of options.

00:26:59.400 --> 00:27:04.340
We put them together in a
homogeneous type of fashion to

00:27:04.340 --> 00:27:10.400
our best ability to target what
I would call typical applications.

00:27:10.590 --> 00:27:12.720
And of course,
we all know there's nothing

00:27:12.720 --> 00:27:17.400
like a typical application,
but in this case I mean applications

00:27:17.400 --> 00:27:20.400
that are computationally intended.

00:27:20.490 --> 00:27:24.820
So if you do a lot of
mathematical computation,

00:27:24.820 --> 00:27:29.340
we've tried to target a mode that
will give you a first step into

00:27:29.340 --> 00:27:31.400
getting some of the performance.

00:27:31.400 --> 00:27:35.630
However, the details of when you
use that mode is important,

00:27:35.630 --> 00:27:39.620
so you can't totally say I don't
understand my program and

00:27:39.620 --> 00:27:41.400
what's going on in my program.

00:27:41.440 --> 00:27:44.940
And so I'll talk a little bit
about the details that are

00:27:44.940 --> 00:27:50.400
important and give you a good feel,
at least for Dash Fast and what it is.

00:27:50.400 --> 00:27:56.400
And then finally, there is a variant of
Dash Fast called Dash Fast F,

00:27:56.400 --> 00:28:01.390
and that's really what you should
be using if you're working with C++.

00:28:01.450 --> 00:28:04.920
There are some things that we
do slightly different to try and

00:28:05.030 --> 00:28:07.400
address performance in the C++ world.

00:28:07.400 --> 00:28:11.400
So what are some of the
specifics about Dash Fast Mode?

00:28:11.400 --> 00:28:14.200
What are we trying to actually attack?

00:28:14.400 --> 00:28:18.190
Well, Sanjay talked about and others
have talked about the deeply

00:28:18.350 --> 00:28:19.400
pipeline nature of the Dash Fast.

00:28:19.400 --> 00:28:20.400
So what are some of the
specifics about Dash Fast Mode?

00:28:20.400 --> 00:28:20.400
What are we trying to actually attack?

00:28:20.400 --> 00:28:25.400
architecture and the
wide functional units.

00:28:25.400 --> 00:28:28.470
And so one of the things that
you have to really be concerned

00:28:28.470 --> 00:28:32.290
about to get performance is
keeping the pipeline filled,

00:28:32.290 --> 00:28:33.400
as we call it.

00:28:33.400 --> 00:28:35.830
And so there are a
number of optimizations,

00:28:35.830 --> 00:28:39.230
some that Sanjay mentioned,
that we have brought together to

00:28:39.230 --> 00:28:41.400
try and keep this pipeline filled.

00:28:41.400 --> 00:28:45.720
So we're feeding this monster at
the speed it would like to be fed.

00:28:45.800 --> 00:28:49.810
I want to talk a little bit about
standard conformance and some of

00:28:49.910 --> 00:28:55.040
the things that we do to relax
the rules so that the compiler

00:28:55.040 --> 00:28:58.740
can actually do a better job for
you in terms of optimization.

00:28:58.740 --> 00:29:02.440
And then finally, of course,
the G5 instruction set.

00:29:02.760 --> 00:29:07.310
This is a presentation on the G5.

00:29:07.610 --> 00:29:11.010
So, to start off with,
I don't know how many of

00:29:11.010 --> 00:29:16.490
you have ventured into the
-03 level optimization mode,

00:29:16.500 --> 00:29:19.900
but I want you to know that's just
the starting point for DashFast.

00:29:19.900 --> 00:29:22.500
And so, you'll get that with DashFast.

00:29:22.500 --> 00:29:26.500
And along with that come a
couple important options.

00:29:26.500 --> 00:29:27.500
One is inlining functions.

00:29:27.500 --> 00:29:30.500
And basically,
and you may understand this,

00:29:30.500 --> 00:29:34.500
but basically that says
within a computational unit,

00:29:34.560 --> 00:29:40.370
the compiler can do some heuristics
to determine how to inline functions

00:29:40.480 --> 00:29:43.090
within that computational unit.

00:29:43.520 --> 00:29:47.960
And the real purpose behind the compiler
doing that is that the more code that the

00:29:47.960 --> 00:29:52.910
compiler can also have an inline view of,
the better all of the

00:29:53.040 --> 00:29:55.470
optimizations can be performed.

00:29:55.600 --> 00:29:58.470
And so,
the bigger view that the optimizer has,

00:29:58.470 --> 00:30:00.480
the better the optimization.

00:30:00.500 --> 00:30:04.500
The second is rename registers option.

00:30:04.500 --> 00:30:08.250
And what this simply does is it
gives the compiler more freedom in

00:30:08.360 --> 00:30:10.500
terms of its register allocation.

00:30:10.500 --> 00:30:14.490
And it does that at the expense of
you being able to debug your code,

00:30:14.490 --> 00:30:20.390
but if you're on this ragged edge of
trying to get optimum performance,

00:30:20.870 --> 00:30:25.500
that is one of the pitfalls
you have to deal with.

00:30:25.500 --> 00:30:32.600
The second capability that I want to talk
about is intermodule inlining or function

00:30:32.600 --> 00:30:34.470
inlining across the entire system.

00:30:34.500 --> 00:30:36.470
And that's the whole program.

00:30:36.500 --> 00:30:40.760
And what that does, where the inlining,
the previous inlining option

00:30:40.850 --> 00:30:46.500
looked at one computational unit,
the intermodule function inlining

00:30:46.500 --> 00:30:48.500
looks at the whole program.

00:30:48.500 --> 00:30:52.100
And so it gives you that many
more opportunities to consider

00:30:52.100 --> 00:30:54.500
inlining throughout your program.

00:30:54.500 --> 00:30:57.850
And once again,
there are heuristics that we have

00:30:57.920 --> 00:31:02.500
determined are the best when you're
making guesses about inlining.

00:31:02.500 --> 00:31:04.500
And you really don't know.

00:31:04.770 --> 00:31:06.500
Whether a function is
called a lot or not.

00:31:06.500 --> 00:31:09.540
I'll be mentioning another
feature a little later on,

00:31:09.540 --> 00:31:11.500
though, that deals with that.

00:31:11.780 --> 00:31:17.730
This is a command line then that
would represent you implementing

00:31:17.730 --> 00:31:20.500
intermodule function inlining.

00:31:20.500 --> 00:31:24.580
And basically that's triggered by
putting all of your compilation

00:31:24.580 --> 00:31:29.890
units on the same compile line so the
compiler can look at them all at once.

00:31:31.630 --> 00:31:36.440
The next – and Sanjay talked about
this – has to do with loop unrolling.

00:31:36.510 --> 00:31:40.190
And the compiler can actually
do that loop unrolling for you.

00:31:40.400 --> 00:31:44.980
Once again, a very simple-minded loop,
but it will serve as

00:31:44.980 --> 00:31:47.090
a representation here.

00:31:47.290 --> 00:31:51.560
The compiler can actually
deal with more complex loops.

00:31:51.710 --> 00:31:56.520
But inlining simply means that it
reduces the number of iterations

00:31:56.660 --> 00:32:01.610
of the loop and it puts various
iterations actually in line.

00:32:01.760 --> 00:32:04.810
And so once again,
what you're doing is cutting down

00:32:05.150 --> 00:32:10.320
on the branching operations and
trying to give the scheduler more

00:32:10.320 --> 00:32:15.330
opportunities for scheduling the other
operations in the functional unit.

00:32:16.690 --> 00:32:20.660
There is another form of loop
unrolling that the compiler

00:32:20.660 --> 00:32:22.730
does called loop peeling.

00:32:22.850 --> 00:32:25.260
And in that situation,
you can see here we have

00:32:25.260 --> 00:32:28.930
an even smaller loop,
and these do occur in code.

00:32:29.030 --> 00:32:33.540
And the compiler will simply
unroll the entire loop and

00:32:33.540 --> 00:32:36.500
eliminate the loop altogether.

00:32:39.750 --> 00:32:47.370
The next option is loop transposition,
and we have several loop transpositions.

00:32:47.440 --> 00:32:50.460
This is similar to what
Sanjay was talking about,

00:32:50.460 --> 00:32:54.940
and I think he indicated you
could turn on this option.

00:32:54.940 --> 00:32:57.880
But basically,
we have a double-nested loop here,

00:32:57.880 --> 00:33:02.450
and it is stepping through memory in
fairly large increments – in this case,

00:33:02.450 --> 00:33:03.860
1335.

00:33:03.960 --> 00:33:08.620
And that has a terrible effect
on the paging within the machine,

00:33:08.620 --> 00:33:15.100
and so for data locality reasons,
we include this loop transpose function.

00:33:15.100 --> 00:33:18.560
And what it will do is the
compiler is able to recognize

00:33:18.650 --> 00:33:22.500
that situation and actually do
the transposition of the loop,

00:33:22.660 --> 00:33:29.680
so that now we're incrementing in
increments of one throughout the memory.

00:33:32.560 --> 00:33:35.860
We have a specialized optimization
called loop-to-memset,

00:33:35.860 --> 00:33:39.940
and what that is,
is if you have initialization

00:33:40.040 --> 00:33:43.600
types of arrays where you're
initializing things to zero,

00:33:43.600 --> 00:33:47.850
the compiler actually will transform
that into what's called a memset.

00:33:47.860 --> 00:33:53.290
And memset on each of our architectures,
including the GFly,

00:33:53.290 --> 00:34:00.020
has been highly tuned in such a way that
you can't beat it with your own code.

00:34:01.160 --> 00:34:06.270
And last, we talked about tuning for G5,
and tuning for G5 is really

00:34:06.270 --> 00:34:11.070
important because it tells the
compiler this is a G5 architecture.

00:34:11.100 --> 00:34:15.550
The compiler understands then how
to schedule instructions for the

00:34:15.660 --> 00:34:20.070
maximum grouping so that we can keep
all of the functional units going

00:34:20.070 --> 00:34:22.520
as much as possible in parallel.

00:34:22.520 --> 00:34:29.100
So we're really extracting the power
of having a wide functional unit set.

00:34:31.580 --> 00:34:35.780
Okay,
I mentioned standard conformance and so

00:34:35.780 --> 00:34:39.260
I have a couple of relaxation rules here.

00:34:39.260 --> 00:34:44.060
One of them is having to do with
an option called strict aliasing.

00:34:44.200 --> 00:34:49.940
And aliasing is a situation where you
have two pointers and those pointers are

00:34:49.940 --> 00:34:52.890
actually pointing to the same object.

00:34:52.900 --> 00:34:57.040
So those objects are then aliased.

00:34:57.140 --> 00:35:01.940
And the compiler often can't tell, well,
even if they're different data types,

00:35:02.000 --> 00:35:05.690
if in fact the objects are aliased.

00:35:05.750 --> 00:35:08.830
And so it has to make the
assumption that they are.

00:35:08.960 --> 00:35:14.280
Well, you can help the compiler out by,
if you know in your program – and this

00:35:14.280 --> 00:35:19.140
is one of your program knowledge items
– if you know that your pointers are

00:35:19.140 --> 00:35:22.880
never aliased within your program,
you can tell the compiler,

00:35:22.930 --> 00:35:25.700
use strict aliasing assumptions here.

00:35:25.830 --> 00:35:31.570
And so in the example,
basically what this means is – and

00:35:31.570 --> 00:35:35.700
this is a very simple example –
strict aliasing tells the compiler,

00:35:35.700 --> 00:35:41.460
by the way, that if pointers are
a different data type,

00:35:41.490 --> 00:35:42.780
they will not be aliased.

00:35:42.800 --> 00:35:44.340
You can assume they're not aliased.

00:35:44.340 --> 00:35:50.300
So in this particular case here,
without strict aliasing,

00:35:50.300 --> 00:35:55.140
we would actually have to reload
the PI with a 1 before we return PI.

00:35:55.400 --> 00:35:59.530
By saying strict aliasing,
we're able to understand that we

00:35:59.530 --> 00:36:03.850
don't have to worry about reloading
that and in fact can be put into

00:36:03.850 --> 00:36:06.860
a register and returned that way.

00:36:06.900 --> 00:36:11.920
So this can have,
interestingly enough in many programs,

00:36:11.920 --> 00:36:13.890
a pretty big impact.

00:36:16.370 --> 00:36:23.460
The second that falls within the area
of conformance is our fast math option.

00:36:23.460 --> 00:36:27.630
And fast math, you should understand,
is not IEEE conformant.

00:36:27.740 --> 00:36:34.050
But, by the way, almost all code doesn't
require IEEE conformance.

00:36:34.150 --> 00:36:36.540
And you know that if it does.

00:36:36.540 --> 00:36:43.580
And so, by relaxing that rule,
we relax that back to where the compiler

00:36:43.580 --> 00:36:48.010
can assume that the associative,
distributive,

00:36:48.080 --> 00:36:50.180
and community principles hold.

00:36:50.180 --> 00:36:57.080
And so it can actually rearrange
code in the fashion that I show

00:36:57.080 --> 00:37:03.800
here on the screen to best utilize
the scheduling of math and the

00:37:03.800 --> 00:37:06.480
computation of these operations.

00:37:06.520 --> 00:37:10.890
This is another one that
can really win for you.

00:37:10.950 --> 00:37:15.990
And if you really don't need to
understand whether on the boundary

00:37:16.000 --> 00:37:20.560
conditions that it's not a number,
it is a number, or it's infinity,

00:37:20.560 --> 00:37:26.310
then you should try using fast
math and try that in your program.

00:37:27.210 --> 00:37:35.000
Hardware-specific then, of course,
we say mcpu = g5 and in the gcc compiler

00:37:35.000 --> 00:37:42.100
that says you're perfectly free to use
any G5 instructions that are available.

00:37:42.210 --> 00:37:46.390
And then inline floor happens to
make use of a couple of specialized

00:37:46.390 --> 00:37:54.100
instructions in the G5 to actually
inline the floor intrinsic right in line.

00:37:54.100 --> 00:37:57.260
The next three have to do with alignment.

00:37:57.260 --> 00:38:01.600
And one of the things that we've
learned about the G5 is that

00:38:01.640 --> 00:38:04.100
it's very sensitive to alignment.

00:38:04.100 --> 00:38:08.730
And you can make dramatic improvements
in your code performance if you

00:38:08.730 --> 00:38:15.940
try to deliver well-aligned types
of data and well-aligned code.

00:38:16.140 --> 00:38:19.460
In this case, we're aligning loops,
jumps,

00:38:19.460 --> 00:38:23.100
and functions all on 16-byte boundaries.

00:38:23.100 --> 00:38:26.090
And yes, this does cause some
bloat in your program.

00:38:26.100 --> 00:38:29.680
But our experience is that
the performance far outweighs

00:38:29.680 --> 00:38:33.990
the bloat that you get and the
size increase in the program.

00:38:34.100 --> 00:38:40.700
The last item there, malign-natural,
says, alright, align all data types on

00:38:40.700 --> 00:38:43.100
their natural boundary.

00:38:43.600 --> 00:38:46.970
So you have to be cognizant of that
if you're concerned about data being

00:38:46.970 --> 00:38:52.450
packed together or things of that
nature because data types will then

00:38:52.450 --> 00:38:57.820
be aligned with perhaps gaps in them.

00:38:58.140 --> 00:39:03.960
I would just encourage you in moving
out of the Dash Fast part of this

00:39:03.960 --> 00:39:06.670
that you should give that a try.

00:39:06.810 --> 00:39:10.960
Feel free to give us feedback
in terms of problems you have.

00:39:10.960 --> 00:39:15.390
One of the things that it can
help you trigger is the need for

00:39:15.390 --> 00:39:17.740
going the next step of analysis.

00:39:18.160 --> 00:39:23.360
If in fact you use Dash Fast and you
don't see speedup within your code,

00:39:23.360 --> 00:39:27.960
then you should be thinking that, well,
maybe I've got some algorithmic problems

00:39:28.010 --> 00:39:32.960
or some memory accessing problems
that are the real performance killers.

00:39:32.970 --> 00:39:36.470
And so there's no way the compiler
is going to optimize you out of this.

00:39:36.650 --> 00:39:38.290
You will have to go to Shark and
try and understand the problem.

00:39:39.320 --> 00:39:40.320
So you have to be cognizant of that.

00:39:40.320 --> 00:39:45.480
And try and understand the program
and what's causing that to happen.

00:39:46.580 --> 00:39:52.260
The next thing I want to talk about
is feedback-directed optimization.

00:39:52.260 --> 00:39:57.720
And feedback-directed optimization is
really an optimization that allows you to

00:39:58.380 --> 00:40:04.500
tell the compiler in more detail exactly
how you expect your code to execute.

00:40:04.500 --> 00:40:08.440
And the compiler will take that
knowledge into account and will

00:40:08.440 --> 00:40:10.480
do a better job of optimizing.

00:40:10.500 --> 00:40:14.470
It's used, number one, for inlining.

00:40:14.480 --> 00:40:19.800
The concern about inlining – and it
was mentioned by Sanjay – is that,

00:40:19.800 --> 00:40:23.490
boy, if you over-inline,
you can kill performance as well.

00:40:23.660 --> 00:40:27.000
Well,
using feedback-directed optimization,

00:40:27.130 --> 00:40:31.500
we actually tell the compiler
from results from a training run

00:40:31.500 --> 00:40:35.410
exactly how many times that a
call site was a function called,

00:40:35.500 --> 00:40:40.500
how many loops are in an iteration
that has a function in it,

00:40:40.500 --> 00:40:42.500
and how many loops are in an iteration
that has a function inside of that loop.

00:40:42.500 --> 00:40:46.780
And so you can make very good
decisions in terms of performance

00:40:46.780 --> 00:40:54.270
versus size trade-offs,
as opposed to using guesses,

00:40:54.270 --> 00:40:54.270
which are the normant –

00:40:55.120 --> 00:41:01.660
The second thing it's used for is what
we call hot and cold partitioning.

00:41:01.660 --> 00:41:05.160
And hot and cold partitioning
– the best example I would have

00:41:05.160 --> 00:41:07.240
for that is an if statement.

00:41:07.240 --> 00:41:10.090
You have two branches,
and one of those gets

00:41:10.090 --> 00:41:14.800
executed predominantly,
and the other one is only occasionally,

00:41:14.800 --> 00:41:17.520
maybe only in an error condition.

00:41:17.520 --> 00:41:22.000
So we tag the hot one and we start
grouping the hot code together,

00:41:22.000 --> 00:41:25.310
and we take the cold code
and we move that off together

00:41:25.310 --> 00:41:26.940
at the end of the program.

00:41:26.940 --> 00:41:30.640
And so we help to compact
the program down and keep the

00:41:30.680 --> 00:41:35.950
footprint for that program so
that we reduce paging once again.

00:41:37.140 --> 00:41:44.000
In operation, there are a couple of flags
that you use to do this.

00:41:44.000 --> 00:41:49.710
So first you would use the create profile
flag and you would actually create an

00:41:49.710 --> 00:41:56.390
executable that is instrumented such that
it can gather the profiling information.

00:41:56.530 --> 00:41:59.370
You run that with a training set of data.

00:41:59.760 --> 00:42:04.920
Then you rebuild your program,
optimizing it using the

00:42:04.920 --> 00:42:07.900
profile that you just created.

00:42:08.010 --> 00:42:11.900
Not all applications, I realize,
lend themselves to

00:42:11.900 --> 00:42:13.900
this type of profiling.

00:42:13.900 --> 00:42:17.900
Maybe it's an interactive
type of application.

00:42:17.900 --> 00:42:22.130
But certainly if you have
computationally intensive applications

00:42:22.130 --> 00:42:25.890
that work on large data sets,
taking advantage of trying

00:42:25.890 --> 00:42:30.400
to train the application,
the compiler to optimize the application

00:42:30.400 --> 00:42:33.750
for that is really a great thing to do.

00:42:33.910 --> 00:42:35.400
Well worth the effort.

00:42:35.520 --> 00:42:38.900
Then finally I want to talk
about AutoVectorization.

00:42:39.090 --> 00:42:42.210
Just out of curiosity,
how many are using the

00:42:42.210 --> 00:42:44.390
AltiVec processor today?

00:42:44.470 --> 00:42:47.190
Okay, we have quite a few hearty souls.

00:42:47.420 --> 00:42:49.900
But you also understand that
it doesn't come for free.

00:42:49.900 --> 00:42:51.400
It takes work to program it today.

00:42:51.400 --> 00:42:58.970
What we are doing is we're trying
to open up the vista of using the

00:42:58.970 --> 00:43:01.400
AltiVec to a broader scope of folks.

00:43:01.480 --> 00:43:05.800
And so areas where you may
not in fact want to spend the

00:43:05.800 --> 00:43:08.400
effort tuning it yourself.

00:43:08.400 --> 00:43:11.400
So what is AutoVectorization?

00:43:11.400 --> 00:43:15.060
It's simply the compiler
being able to transform serial

00:43:15.060 --> 00:43:17.340
loops into vectorizable loops.

00:43:17.400 --> 00:43:21.310
So what is AutoVectorization?

00:43:21.720 --> 00:43:24.240
And what are vectors?

00:43:24.240 --> 00:43:28.640
For those who don't know that, well,
a vector is 128 bits.

00:43:28.790 --> 00:43:34.800
It can be operated on in a number of
different sizes for integers and floating

00:43:34.800 --> 00:43:38.900
point and bit operations and so forth.

00:43:39.020 --> 00:43:45.870
All of those operations within that
128 bits actually occur in parallel,

00:43:45.880 --> 00:43:48.920
and so therein lies the speedup.

00:43:49.950 --> 00:43:54.300
Just a quick overview:
the types of operations are arithmetic,

00:43:54.300 --> 00:43:56.900
logical, compare, rotate and shift.

00:43:56.900 --> 00:43:59.440
They're all done within the vector unit.

00:43:59.530 --> 00:44:02.900
And of course the data
types we just talked about.

00:44:02.960 --> 00:44:08.610
So, in your DVD that you've received,
there is a preview compiler,

00:44:08.630 --> 00:44:12.230
a preview of the 3.5 compiler.

00:44:12.360 --> 00:44:18.900
And that is a first introduction
to you of the autovectorization.

00:44:18.900 --> 00:44:22.900
It has limitations and our
goal is to really work on those

00:44:23.050 --> 00:44:27.900
limitations between now and the
time it's released with Tiger.

00:44:27.900 --> 00:44:29.900
But today, what can it handle?

00:44:29.900 --> 00:44:33.900
It can handle loops with both
known and unknown bounds.

00:44:33.900 --> 00:44:38.430
And there's different code that
we have to generate to discover

00:44:38.430 --> 00:44:42.900
the loop iteration counts at
runtime if they're not known.

00:44:42.900 --> 00:44:45.900
Loops with even and odd vector lengths.

00:44:45.900 --> 00:44:47.900
Loops with conditional loops.

00:44:48.050 --> 00:44:52.900
With conditionals in those,
particularly simple conditionals.

00:44:53.110 --> 00:44:55.680
And misaligned vectors on loads.

00:44:56.070 --> 00:44:58.900
So we're able to take unaligned vectors.

00:44:58.900 --> 00:45:03.740
And what I mean by that is, once again,
AltaVec operates on a 16

00:45:03.740 --> 00:45:05.900
byte boundary type of vector.

00:45:05.900 --> 00:45:10.100
And so if your vectors aren't
aligned on 16 byte boundaries,

00:45:10.130 --> 00:45:15.730
and you can get that from
malloc arrays and of course your

00:45:15.830 --> 00:45:16.900
own arrays that you allocate.

00:45:16.900 --> 00:45:21.900
But we go through vector
operations to align them.

00:45:21.900 --> 00:45:25.670
And I'll show you a little bit
about the performance penalty

00:45:25.720 --> 00:45:27.890
that can occur when you do that.

00:45:30.250 --> 00:45:33.850
Auto-vectorization has difficulties
with pointers and aliasing.

00:45:33.860 --> 00:45:36.270
Well,
I talked a little bit about that before

00:45:36.270 --> 00:45:39.600
in the scalar part of the presentation.

00:45:39.810 --> 00:45:42.300
That's true here as well.

00:45:42.600 --> 00:45:47.770
In this particular example,
there is no way that the compiler,

00:45:48.170 --> 00:45:51.100
unless A and B are globals, are local.

00:45:51.100 --> 00:45:53.820
They're certainly not
local within this function.

00:45:53.860 --> 00:45:57.930
And so unless they're globals,
there's no way the compiler can

00:45:57.930 --> 00:46:00.410
discover that these are not aliased.

00:46:00.470 --> 00:46:04.840
And so it'll have to make the
assumption they are in today's

00:46:04.840 --> 00:46:07.720
world and not vectorize this loop.

00:46:07.840 --> 00:46:11.750
However, you can help the compiler
out in a simple way.

00:46:11.840 --> 00:46:14.460
You can actually use
the restrict keyword.

00:46:14.460 --> 00:46:19.550
And the restrict tells the compiler,
"Okay, this pointer does not alias with

00:46:19.550 --> 00:46:24.630
any other pointed-to object."
And so that's a simple help.

00:46:24.910 --> 00:46:31.020
It turns the loop into one that
can be vectorized that can't today.

00:46:31.410 --> 00:46:35.860
The next thing that it has difficulty
with that you need to watch out for

00:46:35.860 --> 00:46:41.440
is that scalar loops may have data
dependencies that work perfectly fine

00:46:41.440 --> 00:46:43.400
when you're dealing in the scalar mode.

00:46:43.400 --> 00:46:47.960
But to try and transform that into
a vector type of operation where

00:46:47.960 --> 00:46:52.310
you have a number of elements
being computed at the same time,

00:46:52.400 --> 00:46:54.400
you can't have those dependencies.

00:46:54.400 --> 00:46:59.330
And so the first illustration of a
loop here is one in which we simply

00:46:59.750 --> 00:47:04.370
couldn't vectorize it because you
will have this data dependency.

00:47:04.400 --> 00:47:08.540
And the second one – the
second one looks similar,

00:47:08.540 --> 00:47:14.400
but in fact there's no data dependency
here because this is offset by n.

00:47:15.380 --> 00:47:19.020
And then misaligned vector stores.

00:47:19.020 --> 00:47:21.180
We simply can't handle
that in the preview.

00:47:21.410 --> 00:47:27.450
We'll have that available
in the GM release,

00:47:27.540 --> 00:47:30.700
but if you're going to
play with the 3.5 compiler,

00:47:30.700 --> 00:47:35.400
be aware that the vector that
you're storing into needs to

00:47:35.460 --> 00:47:38.300
be correctly aligned in memory.

00:47:38.390 --> 00:47:42.280
So what is using the auto
vectorization all about?

00:47:42.350 --> 00:47:43.950
Well, it's about performance.

00:47:44.050 --> 00:47:49.080
And so I have some initial numbers here,
and these are already out of date

00:47:49.090 --> 00:47:52.300
as we continue to tune the code.

00:47:52.300 --> 00:47:55.620
But for simple types
of operations in loops,

00:47:55.620 --> 00:48:00.300
you can see speedups here that
go all the way to 14 times.

00:48:00.300 --> 00:48:06.240
And we're now seeing even around
20 times in some of our work.

00:48:06.300 --> 00:48:10.990
If you have misaligned data,
the types of impacts that you

00:48:11.080 --> 00:48:16.300
can see is you really reduce
the performance significantly.

00:48:16.420 --> 00:48:19.660
Now this, as I said, I expect to improve.

00:48:19.800 --> 00:48:24.300
We're in a very early stage
with the auto vectorization.

00:48:24.300 --> 00:48:30.140
We have a limited set of loops that
we're able to recognize and vectorize,

00:48:30.140 --> 00:48:34.190
and so I would encourage
you to take a look at this.

00:48:34.300 --> 00:48:40.060
We are really open to you sending us
kernels of code of things that we don't

00:48:40.060 --> 00:48:46.300
seem to be able to vectorize because we
want to build up and mature that ability.

00:48:46.300 --> 00:48:48.300
And this will be something
that we're working on.

00:48:48.300 --> 00:48:52.480
As you can see, though,
the reason we're excited is because

00:48:52.480 --> 00:48:55.300
this can really offer some speedups.

00:48:55.300 --> 00:48:59.790
And particularly if you haven't
already been using the auto

00:48:59.790 --> 00:49:05.300
vector processor on your system,
it's sitting there just wasting away,

00:49:05.350 --> 00:49:08.300
and you can get some real
performance out of it.

00:49:09.690 --> 00:49:16.260
So the operation here includes the
enabling of a couple of options.

00:49:16.440 --> 00:49:21.420
I believe that in Xcode today
there's actually an option for auto

00:49:21.420 --> 00:49:28.220
vectorization that will do that
for you and enable the process.

00:49:29.760 --> 00:49:32.380
So if you're looking
for more information,

00:49:32.830 --> 00:49:36.740
you can contact Mark Tozer
or Matthew Formica.

00:49:36.740 --> 00:49:40.770
And Mark, do you want to come up?

00:49:41.810 --> 00:49:46.120
So to add to that, the reference library,
some documentation that's on

00:49:46.120 --> 00:49:50.680
Apple's developer website,
some tech notes that are written that

00:49:50.680 --> 00:49:54.700
were posted since last developers
conference with a lot of information.

00:49:54.700 --> 00:49:58.350
We'll have George Warner up here
in a few seconds with the Q&A who

00:49:58.470 --> 00:50:02.640
participated in writing some of
that technical documentation.

00:50:02.640 --> 00:50:07.760
A couple of takeaways I want to make
sure you go away with this morning.

00:50:07.760 --> 00:50:10.910
You know, it's been a year since we
introduced the PowerPC chip,

00:50:10.910 --> 00:50:12.050
as I said earlier.

00:50:12.070 --> 00:50:16.960
So you should be looking at
transitioning your code to the G5.

00:50:16.960 --> 00:50:19.740
You should be looking at
optimizing the code and making

00:50:19.750 --> 00:50:22.100
sure that it performs at its best.

00:50:22.110 --> 00:50:24.760
Optimization is a skill.

00:50:24.760 --> 00:50:26.760
It is not something that comes for free.

00:50:26.760 --> 00:50:30.420
Although the tools that you heard
about today in the compilers in both

00:50:30.420 --> 00:50:34.960
Apple's compilers as well as IBM's
will provide a lot of assistance.

00:50:34.960 --> 00:50:37.520
But there is sometimes when you
need to get in there and roll up

00:50:37.520 --> 00:50:39.450
your sleeves and do the hard work.

00:50:39.560 --> 00:50:43.750
For that,
we have optimization workshops at Apple.

00:50:43.850 --> 00:50:47.960
For the past year,
we've had over eight workshops,

00:50:47.960 --> 00:50:51.300
one a month essentially,
helping developers like

00:50:51.390 --> 00:50:55.060
yourselves to work through the
problems of optimizing your code.

00:50:55.170 --> 00:50:58.260
So I encourage you to
participate in those workshops.

00:50:58.290 --> 00:51:01.960
They're posted through the
Apple developer connection emails.

00:51:01.960 --> 00:51:05.260
And they'll be continuing on
throughout the rest of the year.

00:51:05.260 --> 00:51:06.560
I'll have the next one
starting in August.

00:51:06.560 --> 00:51:10.830
I believe the first or
second week of August,

00:51:10.830 --> 00:51:11.750
if I can remember correctly.

00:51:11.760 --> 00:51:13.830
And then we have the next one.

00:51:14.090 --> 00:51:20.320
The other thing is that it does take
a lot of work to do optimization work,

00:51:20.320 --> 00:51:23.380
but there are a lot of rewards to it,
as Sanjay pointed out in

00:51:23.400 --> 00:51:25.000
some of the sample code.

00:51:25.000 --> 00:51:28.000
We're here to help you
through those problems.

00:51:28.000 --> 00:51:32.680
So, as Steve mentioned, from IBM,
there is the optimization

00:51:32.680 --> 00:51:33.950
lab here all week.

00:51:34.000 --> 00:51:36.000
Please take advantage of those resources.

00:51:36.000 --> 00:51:38.470
We're here and committed to
helping you guys write the

00:51:38.470 --> 00:51:40.000
best code for this platform.

00:51:40.000 --> 00:51:43.000
We feel that the G5 has a lot to offer.

00:51:43.000 --> 00:51:44.990
It has a lot of headroom to grow.

00:51:45.000 --> 00:51:49.380
The best applications on the platform
are those that take advantage of all the

00:51:49.380 --> 00:51:51.980
abilities that the hardware has to offer.

00:51:52.030 --> 00:51:54.280
So please, again,
make sure that that's something

00:51:54.310 --> 00:51:57.000
in mind when you're looking
at revving your application,

00:51:57.000 --> 00:52:01.080
writing a new application,
or just taking the time to take

00:52:01.100 --> 00:52:05.800
a look at what you've done in the
past and maybe improve upon that.