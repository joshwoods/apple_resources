WEBVTT

00:00:13.770 --> 00:00:17.530
Hi, everyone.

00:00:17.590 --> 00:00:19.340
My name is Pat Dirks.

00:00:19.340 --> 00:00:23.020
I've been working on file systems
and file servers on the Mac ever

00:00:23.020 --> 00:00:26.060
since the days when the most
excellent Mac 512KE supplanted

00:00:26.060 --> 00:00:27.930
the 512 as the top of the heap.

00:00:28.010 --> 00:00:32.400
So it's been a while,
and we've come a very, very long way.

00:00:32.400 --> 00:00:35.600
We put together a talk to give
you some highlights of things

00:00:35.600 --> 00:00:38.110
that you should be aware of when
you're using the file system,

00:00:38.230 --> 00:00:40.600
mostly from an application perspective.

00:00:40.650 --> 00:00:43.650
We are gonna go in a
little bit of background,

00:00:43.740 --> 00:00:46.870
just enough to sort of put the stage
for all these things and run an anchor

00:00:46.950 --> 00:00:48.900
for all these things to make sense.

00:00:48.910 --> 00:00:50.900
We'll talk a little bit
about the BSD-level APIs,

00:00:50.960 --> 00:00:53.540
but also about the
higher-level frameworks.

00:00:53.690 --> 00:00:58.660
So what we're gonna focus on is how
to make your applications go faster,

00:00:58.820 --> 00:01:01.730
basically how to make your
I/O go faster and how to work

00:01:01.840 --> 00:01:05.000
better overall with the system,
particularly in light of some new

00:01:05.040 --> 00:01:08.520
technologies that we're introducing,
extended attributes, ACLs,

00:01:08.520 --> 00:01:09.650
that sort of thing.

00:01:10.880 --> 00:01:16.090
We'll say a few words about monitoring
other ongoing things in the system,

00:01:16.090 --> 00:01:19.400
kind of common areas that applications
often get bogged down doing things

00:01:19.400 --> 00:01:21.800
that are unnecessarily wasteful.

00:01:21.800 --> 00:01:23.540
So we'll try to hit all the
highlights and then we'll try to

00:01:23.550 --> 00:01:26.490
leave lots of time for Q&A afterwards,
where we'll have the whole team up

00:01:26.490 --> 00:01:28.270
and you can ask anything you like.

00:01:28.730 --> 00:01:32.600
So, a bit of background on the kernel
and the BSD parts of the system.

00:01:32.600 --> 00:01:37.340
There are actually a number
of different frameworks that

00:01:37.410 --> 00:01:38.600
people access these APIs through.

00:01:38.600 --> 00:01:42.600
There's the object-oriented Cocoa set
of frameworks that people use.

00:01:42.600 --> 00:01:45.600
There's the familiar Carbon APIs.

00:01:45.600 --> 00:01:49.600
And the low-level BSD APIs that
are the foundation for all of them.

00:01:49.670 --> 00:01:53.710
And the thing to bear in mind is that
everything that is done in the system is

00:01:53.710 --> 00:01:56.510
done at some point through some BSD API.

00:01:56.600 --> 00:01:59.600
So, if you don't mind getting down
to the really nitty-gritty,

00:01:59.600 --> 00:02:03.790
the BSD API will let you
do anything you want to do.

00:02:04.500 --> 00:02:09.000
Inside the system, at the highest level,
is the virtual file system layer.

00:02:09.080 --> 00:02:13.260
And that provides a common set
of system call interfaces to

00:02:13.260 --> 00:02:16.950
all other parts of the system,
as well as providing some common

00:02:16.950 --> 00:02:21.400
services like path name translation,
a certain amount of access checking,

00:02:21.400 --> 00:02:25.830
NFS export service that are just
common to all layers of the system.

00:02:25.950 --> 00:02:29.790
But beyond that,
it uses an extensible interface

00:02:29.800 --> 00:02:33.320
to lower level specific
file system implementations.

00:02:33.710 --> 00:02:39.010
and works with Buffer Cache and VM that
I'll say a few words more in a moment.

00:02:39.190 --> 00:02:45.920
Because the buffer cache in,
I think it was Jaguar or maybe Panther,

00:02:45.920 --> 00:02:50.740
was changed from a separate buffer
pool that set aside the VM system and

00:02:50.740 --> 00:02:55.540
competed with VM for who had the correct
copy of something to a unified buffer

00:02:55.620 --> 00:03:00.340
cache that coordinates with VM to make
sure that there's only one copy of your

00:03:00.410 --> 00:03:02.520
data in the system at any one place.

00:03:02.520 --> 00:03:06.140
And there's a set of code called UBC,
the Unified Buffer Cache,

00:03:06.140 --> 00:03:10.300
that basically coordinates who owns
a given page at any given time and

00:03:10.300 --> 00:03:12.400
swaps ownership between the two.

00:03:12.400 --> 00:03:15.900
So the important thing is,
whatever there is of your data,

00:03:15.900 --> 00:03:19.410
there's only one copy of it
floating around in the system.

00:03:19.760 --> 00:03:22.720
Now, at the lowest level,
there's a few very

00:03:22.720 --> 00:03:24.390
simple set of I/O calls.

00:03:24.460 --> 00:03:29.580
There's open, close, read/write,
truncate, seek,

00:03:29.580 --> 00:03:31.190
just the bare necessities.

00:03:31.200 --> 00:03:34.030
And there's mmap,
which we'll talk about it in

00:03:34.030 --> 00:03:35.950
some more length later on.

00:03:36.190 --> 00:03:40.030
We have some basic metadata operations.

00:03:40.030 --> 00:03:42.020
You can open a directory for reading.

00:03:42.090 --> 00:03:45.670
You can read through it and get
some information on the various

00:03:45.670 --> 00:03:47.940
items that are in a directory.

00:03:47.940 --> 00:03:49.770
We'll talk about that
some more later too,

00:03:49.840 --> 00:03:52.660
because that's another way that
you might want to be careful.

00:03:52.810 --> 00:03:56.900
Stat is a basic call to get
some metadata on an item.

00:03:56.900 --> 00:03:59.010
And then there's a few other calls.

00:04:02.880 --> 00:04:09.760
We extended the basic BSD mechanism
a bit to support some of the things

00:04:09.760 --> 00:04:14.800
that HFS had brought to Mac OS 9
when we first introduced Mac OS X.

00:04:14.800 --> 00:04:18.760
You couldn't quite port all of the
existing Mac OS 9 functionality to

00:04:18.790 --> 00:04:20.800
the BSD APIs that I just mentioned.

00:04:20.800 --> 00:04:24.870
So we introduced a pair of calls,
getAddrList and setAddrList,

00:04:24.950 --> 00:04:27.800
that let you do stat and chmod
things in a slightly different,

00:04:27.800 --> 00:04:30.800
more expendable way.

00:04:30.910 --> 00:04:33.800
And a new call, getDirEntriesAddr,
that probably few people call directly,

00:04:33.800 --> 00:04:37.330
but is used by the frameworks
in the system to do efficient

00:04:37.330 --> 00:04:42.410
enumeration of items in the same
way that getCatalogInfo did.

00:04:43.010 --> 00:04:48.570
In addition, we introduce SearchFS,
which is an interesting API because

00:04:48.570 --> 00:04:53.230
it lets you search the entire catalog
of a disk in more or less block order.

00:04:53.340 --> 00:04:57.260
So the performance of SearchFS is
dramatically faster than doing a

00:04:57.260 --> 00:05:01.770
complete treat reversal of every
single directory in the file system.

00:05:02.510 --> 00:05:07.080
We added a few F-control selectors to let
you do things like pre-allocating space

00:05:07.080 --> 00:05:10.400
on a disk and other things like that.

00:05:10.400 --> 00:05:14.400
And new in the TIGER release,
and we'll talk a bit more about that,

00:05:14.400 --> 00:05:18.400
are extended attributes and ACLs.

00:05:18.400 --> 00:05:20.690
In general,
although I said you can do everything

00:05:20.690 --> 00:05:24.400
at the lowest level in the BSD system,
you're probably best off using

00:05:24.400 --> 00:05:27.400
the higher level frameworks,
Cocoa or Carbon, in the system.

00:05:27.910 --> 00:05:33.950
Because the BSD level of APIs makes no
attempt to hide all the differences.

00:05:34.640 --> 00:05:36.360
between the different file systems.

00:05:36.410 --> 00:05:39.130
So if some file system doesn't
support a certain feature,

00:05:39.130 --> 00:05:41.090
it's just not going to
work in the BSD call.

00:05:41.110 --> 00:05:46.640
And frameworks like Carbon will
provide uniform access to things

00:05:46.640 --> 00:05:50.220
like resource forks and even emulate
them if necessary on file systems

00:05:50.290 --> 00:05:51.860
that don't directly support them.

00:05:51.930 --> 00:05:54.720
So in general,
you're best off using the highest

00:05:54.760 --> 00:05:56.550
level API that you can use.

00:06:00.630 --> 00:06:03.860
So I'll say a bit more about
the extended attributes.

00:06:03.930 --> 00:06:07.200
Extended attributes are new in Tiger.

00:06:07.200 --> 00:06:11.020
The release that you have has
only a very limited implementation

00:06:11.020 --> 00:06:12.190
of the extended attributes.

00:06:12.270 --> 00:06:15.490
They're basically only supported
on the HFS file system.

00:06:15.580 --> 00:06:24.940
But in Tiger, extended attributes will be
supported across all file systems.

00:06:25.350 --> 00:06:28.000
They are, however,
going to be fragile in much the same way

00:06:28.000 --> 00:06:30.200
that the resource fork has always been.

00:06:30.200 --> 00:06:33.710
And one of the things I hope you'll take
away from this talk is that if you're

00:06:33.710 --> 00:06:36.580
manipulating files in the file system,
if you're making copies of things,

00:06:36.600 --> 00:06:40.510
if you're transferring things around,
you should be mindful that,

00:06:40.510 --> 00:06:42.570
like a resource fork,
they may have extended

00:06:42.580 --> 00:06:45.780
attributes associated with them,
and you should be

00:06:45.780 --> 00:06:47.590
careful to preserve them.

00:06:49.500 --> 00:06:50.990
While I'm on the subject
of the resource fork,

00:06:51.080 --> 00:06:56.320
by the way, if your code still has any
references to file name/resource,

00:06:56.320 --> 00:06:58.080
now is definitely the
release to get rid of that.

00:06:58.220 --> 00:07:02.280
We have supported that for a while,
but it's not the way to refer things.

00:07:02.330 --> 00:07:06.230
If you want to,
you can use . .name/resource.

00:07:06.480 --> 00:07:11.580
The biggest trick is
always doing a safe-save.

00:07:12.310 --> 00:07:16.420
Unfortunately, Exchange Files,
which was invented for

00:07:16.530 --> 00:07:20.130
that purpose in Mac OS 9,
is not supported on all file

00:07:20.140 --> 00:07:22.370
systems or all protocols.

00:07:22.940 --> 00:07:26.300
Besides which,
you're going to have to be, in general,

00:07:26.440 --> 00:07:29.300
careful to preserve extended
attributes and ACLs as well.

00:07:29.300 --> 00:07:32.820
If you were here at 2 o'clock today,
you would have heard a talk about

00:07:32.860 --> 00:07:35.100
the ACLs and what to do about that.

00:07:35.290 --> 00:07:38.600
We have a few recommendations.

00:07:40.600 --> 00:09:22.500
[Transcript missing]

00:09:24.150 --> 00:09:28.280
Wherever possible,
if Exchange files are supported

00:09:28.450 --> 00:09:32.240
and your document format is
a single straight data file,

00:09:32.240 --> 00:09:36.100
the best way to do a save-save
is to do an Exchange files.

00:09:36.100 --> 00:09:38.100
But you're always going to
have to have a fallback option,

00:09:38.100 --> 00:09:43.090
and the best thing you can do,
if you've created the file correctly,

00:09:43.210 --> 00:09:46.100
is to just do a rename in place.

00:09:46.210 --> 00:09:49.890
If your document format is a bundle,
if it's really a directory with

00:09:49.890 --> 00:09:54.100
files contained underneath there,
you can do one of two things.

00:09:54.140 --> 00:09:57.760
Either create a complete
parallel directory in place,

00:09:57.770 --> 00:10:01.100
and then rename one out
of the way and rename it.

00:10:01.100 --> 00:10:02.100
It's not atomic, but it'll work.

00:10:02.100 --> 00:10:05.380
And the other option is to
create the updated file somewhere

00:10:05.380 --> 00:10:09.100
inside the top level directory so
they're invisible from the user,

00:10:09.200 --> 00:10:11.280
and then using either
Exchange files or rename,

00:10:11.280 --> 00:10:14.100
switch them in place with the
pieces that you're updating.

00:10:14.100 --> 00:10:16.060
And only you can decide based on that.

00:10:16.100 --> 00:10:18.690
And that's the only way you can
decide based on your application

00:10:18.690 --> 00:10:20.100
which model makes most sense.

00:10:20.100 --> 00:10:22.140
Now,

00:10:23.500 --> 00:10:26.020
Some things to be aware of.

00:10:26.020 --> 00:10:30.500
Locking support is unfortunately
not uniform across all file systems.

00:10:30.500 --> 00:10:35.400
It's good practice to lock files that
you think there may be contention for,

00:10:35.400 --> 00:10:39.040
but there will be file systems,
either local or network file systems,

00:10:39.040 --> 00:10:40.500
that just don't support that.

00:10:40.500 --> 00:10:44.430
Rename does not support file IDs.

00:10:44.520 --> 00:10:47.500
That was one of the big advantages
that Exchange Files brought,

00:10:47.500 --> 00:10:52.500
is that if you had an alias to a file,
Exchange Files would replace the data

00:10:52.500 --> 00:10:55.500
while leaving the file ID intact,
and an alias would find it again.

00:10:55.500 --> 00:10:58.480
That's one of the appeals
of using Exchange Files.

00:10:58.490 --> 00:11:01.580
Fortunately,
alias data structures have some

00:11:01.580 --> 00:11:04.850
additional resolution mechanisms
built in that will make it

00:11:04.850 --> 00:11:07.940
possible to find the file,
even if the file ID is lost because you

00:11:07.940 --> 00:11:09.500
used rename instead of Exchange Files.

00:11:09.500 --> 00:11:11.500
But Exchange Files is still preferable.

00:11:11.500 --> 00:11:15.580
And as I mentioned,
rename cannot atomically

00:11:15.580 --> 00:11:17.500
replace directories.

00:11:18.430 --> 00:11:20.960
In addition,
one thing to be careful about is that

00:11:21.300 --> 00:11:25.800
not all network file systems allow
you to rename files that are open.

00:11:25.800 --> 00:11:29.690
So if you have a choice about it,
it's best to close the file first,

00:11:29.750 --> 00:11:35.200
or at least flush it,
before you try replacing it.

00:11:35.200 --> 00:11:39.190
And on the subject of that, be careful.

00:11:41.050 --> 00:11:45.190
Because on some file systems,
close may actually do a very

00:11:45.190 --> 00:11:47.190
significant amount of I/O.

00:11:47.230 --> 00:11:50.200
In a WebDAO file system, for instance,
it's not until close time

00:11:50.200 --> 00:11:54.830
that the actual data is all
transferred over to the server.

00:11:54.860 --> 00:11:59.330
There may also be systems that
don't actually guarantee that your

00:11:59.500 --> 00:12:02.670
quota has been checked properly,
for instance, until the file is actually

00:12:02.670 --> 00:12:03.750
closed and on disk.

00:12:03.860 --> 00:12:09.710
So you should try closing the file
before you use it to replace data.

00:12:09.760 --> 00:12:13.320
And if you can't close it,
at least call fsync on it to force

00:12:13.320 --> 00:12:16.870
a flush of the data and check for
errors on both of those before

00:12:16.870 --> 00:12:19.390
you allow the data to be replaced.

00:12:20.330 --> 00:12:24.060
So a word about file copying,
and this is actually one of the

00:12:24.060 --> 00:12:25.490
shortest sections of the talk.

00:12:25.510 --> 00:12:28.740
We've had lots of requests over
the years to allow people access

00:12:28.820 --> 00:12:32.400
to the Finder copy engine,
and that's what we're

00:12:32.400 --> 00:12:33.560
introducing in Tiger.

00:12:33.630 --> 00:12:36.740
So you'll actually get access--

00:12:39.730 --> 00:12:42.300
Not directly to the finder's engine,
but to the same engine

00:12:42.300 --> 00:12:43.720
that the finder uses.

00:12:44.060 --> 00:12:46.760
But you'll be able to do copies
exactly the same way the finder does.

00:12:46.760 --> 00:12:53.570
And if you can use this new FS file
operations API to transfer files around,

00:12:53.860 --> 00:12:59.410
moving or copying, that will be by far
the best way to do it,

00:12:59.500 --> 00:13:03.720
because we'll guarantee that that
does the best job possible on the

00:13:03.720 --> 00:13:03.720
various file systems involved.

00:13:03.820 --> 00:13:09.530
The API will let you do either
synchronous or asynchronous copies,

00:13:09.530 --> 00:13:13.000
and you can set a callback routine to
be called at a rate that you specify

00:13:13.000 --> 00:13:16.650
so that you can do whatever animation
you want to do to show the copy

00:13:16.730 --> 00:13:18.800
in progress or anything like that.

00:13:18.800 --> 00:13:21.480
It's very nice,
and that's what the Tiger Finder

00:13:21.480 --> 00:13:22.720
is going to be using.

00:13:22.720 --> 00:13:29.710
So that's an example where the highest
possible level of API is just ideal.

00:13:30.160 --> 00:13:34.430
So, a few words about performance.

00:13:34.470 --> 00:13:39.690
You may have more choices about
how to do I/O in Mac OS X and in

00:13:39.690 --> 00:13:42.280
Tiger than you may have otherwise.

00:13:42.310 --> 00:13:45.660
There are a few caveats
that apply generally.

00:13:45.660 --> 00:13:48.740
Obviously, the fewer I/Os you do,
the better.

00:13:48.770 --> 00:13:51.800
And the more you can aggregate
I/Os into a few large I/Os,

00:13:51.800 --> 00:13:52.810
the better.

00:13:52.820 --> 00:13:59.560
The kernel will, to some extent,
coalesce operations that you do

00:13:59.790 --> 00:14:03.160
to make fewer larger operations
out of a few single ones.

00:14:03.190 --> 00:14:06.040
It will do read-ahead caching
and write-behind caching,

00:14:06.040 --> 00:14:10.460
so you may be picking up benefits
that you weren't even aware of.

00:14:10.460 --> 00:14:15.420
There is something very specific,
mostly on HFS, about zero fill.

00:14:15.420 --> 00:14:19.090
The Unix file semantics specify
that data that has not been

00:14:19.130 --> 00:14:21.300
written should read back as zeros.

00:14:21.300 --> 00:14:26.020
And the system goes out of its way
to make sure that that is true,

00:14:26.040 --> 00:14:29.380
even on file systems like HFS,
which don't actually support

00:14:29.380 --> 00:14:31.260
sparse file semantics.

00:14:31.340 --> 00:14:35.280
So what that means is,
if you take a file and create

00:14:35.490 --> 00:14:40.100
it and skip ahead 100K into the
file and start writing there,

00:14:40.120 --> 00:14:42.270
and you try to read
something out of the middle,

00:14:42.290 --> 00:14:45.700
the system will actually supply
zeros for all those areas and

00:14:45.700 --> 00:14:47.410
write those on disk if necessary.

00:14:47.440 --> 00:14:50.520
If you close the file,
only having written that last

00:14:50.520 --> 00:14:54.510
little bit at 100K out there,
it will fill the intermediate

00:14:54.510 --> 00:14:57.360
space on disk with zeros.

00:14:57.360 --> 00:14:59.020
So that's another example of this.

00:14:59.020 --> 00:15:01.930
So there's a significant
amount of I/O that you might

00:15:01.930 --> 00:15:08.010
incur when you do close.

00:15:08.520 --> 00:15:11.220
I have a few words about
zero fill later too.

00:15:11.250 --> 00:15:13.100
Now in general,
there is a couple of different

00:15:13.100 --> 00:15:15.500
ways that you can do I/O,
and we'll talk about them all in turn.

00:15:15.500 --> 00:15:18.500
You can do standard
buffered reads and writes.

00:15:18.500 --> 00:15:22.570
You can directly memory
map a file on disk,

00:15:22.570 --> 00:15:25.500
anywhere on disk, into your address space
and access it that way.

00:15:25.500 --> 00:15:30.400
You can do uncached, unbuffered I/O,
which is great if you're ripping

00:15:30.400 --> 00:15:33.490
through very large data sets that
you have no reason to believe

00:15:33.490 --> 00:15:34.500
will be touched again later.

00:15:34.500 --> 00:15:37.880
And we're now introducing
support for true asynchronous

00:15:37.880 --> 00:15:40.450
I/O in the kernel as well,
and that's another option

00:15:40.450 --> 00:15:41.500
you may want to consider.

00:15:43.500 --> 00:15:47.060
Now in the buffered read/write case,
it's the most general

00:15:47.060 --> 00:15:48.600
of all the mechanisms.

00:15:48.650 --> 00:15:53.500
You can transfer any amount of data to
any place you want in your address space.

00:15:53.500 --> 00:15:55.500
There are some costs
associated with that.

00:15:55.500 --> 00:15:58.580
The data is first transferred
into the buffer cache,

00:15:58.580 --> 00:16:02.620
and then explicitly copied out of the
buffer cache into your application space,

00:16:02.780 --> 00:16:04.700
dirtying a page in the process.

00:16:04.700 --> 00:16:08.200
So you're incurring an extra copy
and you're dirtying the page.

00:16:08.350 --> 00:16:12.370
But if you're reading
or writing small files,

00:16:12.370 --> 00:16:15.770
and if you have no reason to think that
they might not be touched shortly after,

00:16:16.120 --> 00:16:19.350
or if you really need the flexibility of
writing a small amount of data in a very

00:16:19.360 --> 00:16:23.300
particular place in your address space,
this is the perfect mechanism to use.

00:16:23.300 --> 00:16:28.390
And if you're just doing reads
and writes and doing nothing else,

00:16:28.390 --> 00:16:28.390
this is the mechanism you're using.

00:16:29.460 --> 00:16:33.630
By contrast, you can,
if you want to give up control

00:16:33.640 --> 00:16:37.040
over the location of the data,
use memory mapped IO.

00:16:37.040 --> 00:16:38.630
You can call MMP.

00:16:38.730 --> 00:16:40.900
You have to call the low-level
BSD file system call.

00:16:40.900 --> 00:16:42.500
There's no framework-level call for this.

00:16:42.500 --> 00:16:45.440
But you can ask a system to map
a file in your address space,

00:16:45.440 --> 00:16:46.840
and it will just appear.

00:16:46.840 --> 00:16:49.300
You'll be given the address,
and you can access the data right there,

00:16:49.300 --> 00:16:51.990
and it will be paged in
directly off the file on disk.

00:16:52.020 --> 00:16:55.800
If you're reading in a file,
this is a great way to read it.

00:16:55.900 --> 00:16:59.770
If you don't care where the data ends up,
you can just use your favorite beat

00:16:59.770 --> 00:17:01.630
copy routine to read your file.

00:17:04.020 --> 00:17:07.850
There's only a single copy made of
the data because the system doesn't

00:17:07.850 --> 00:17:11.900
have to relocate it to the place
where you requested it be put.

00:17:12.390 --> 00:17:15.390
Everything is left clean,
so if the pages need to be reused later,

00:17:15.510 --> 00:17:16.900
they can simply be tossed.

00:17:16.900 --> 00:17:20.210
They don't need to be
written out to swap space.

00:17:23.950 --> 00:17:25.930
And although you can
overwrite data that way,

00:17:25.930 --> 00:17:28.820
and it'll be paged out to the file,
you can't actually

00:17:28.820 --> 00:17:29.750
extend the file that way.

00:17:29.760 --> 00:17:31.830
That's the only limitation.

00:17:32.110 --> 00:17:37.100
The big caveat on using memory
mapped I/O is that if you

00:17:37.170 --> 00:17:42.260
encounter any I/O errors because,
say, it was on some removable disk

00:17:42.320 --> 00:17:44.980
and the disk was removed,
or it was on a network file

00:17:45.060 --> 00:17:47.360
and the network connection
was broken or something,

00:17:47.360 --> 00:17:50.270
it's as if you hit a bad RAM chip.

00:17:50.380 --> 00:17:52.880
I mean, you get a memory access error.

00:17:52.880 --> 00:17:55.160
So it's a little bit more severe
than just getting an error

00:17:55.160 --> 00:17:56.460
back on some I/O transfer.

00:17:56.460 --> 00:18:03.070
You may want to keep that in mind and
limit disk type of operation to local,

00:18:03.070 --> 00:18:05.390
non-removable media.

00:18:05.400 --> 00:18:09.870
But with that caveat in mind,
it may be an attractive way to do I/O,

00:18:09.880 --> 00:18:11.420
and it's very clean.

00:18:12.790 --> 00:18:16.340
Some of the benefits of doing
that kind of I/O can also be had

00:18:16.340 --> 00:18:19.630
by doing direct uncached I/O.

00:18:19.700 --> 00:18:25.200
You can call fcontrol in the system
to request that I/O to a certain

00:18:25.200 --> 00:18:30.320
file descriptor is done uncached,
and the system will take

00:18:30.570 --> 00:18:32.790
advantage as much as possible

00:18:32.800 --> 00:18:54.200
[Transcript missing]

00:18:54.850 --> 00:18:59.140
The system cache is filled with
pages from your application,

00:18:59.150 --> 00:19:02.570
or pages from other files
that you've touched,

00:19:02.570 --> 00:19:06.670
or pages from the disk
structures that you're hitting.

00:19:06.670 --> 00:19:11.380
You don't end up forcing them all
out of memory just because you're

00:19:11.380 --> 00:19:14.700
reading some multi-megabyte data file.

00:19:14.740 --> 00:19:16.950
So it's something to think about.

00:19:17.100 --> 00:19:21.560
And there are ways that you can do
uncached I/O from the Carbon framework,

00:19:21.600 --> 00:19:22.630
for instance.

00:19:22.630 --> 00:19:24.700
We'll show that in a minute.

00:19:27.240 --> 00:19:34.420
Async I/O is a standard API that we're
introducing support for in Tiger.

00:19:34.420 --> 00:19:37.200
It's a complete implementation
of the standard POSIX async I/O.

00:19:37.200 --> 00:19:41.540
You can initiate async I/O requests,
you can get notified by

00:19:41.550 --> 00:19:43.200
a variety of mechanisms.

00:19:43.200 --> 00:19:46.790
The I/Os are handed down
to the kernel in parallel,

00:19:46.940 --> 00:19:51.190
so in theory they could proceed
in parallel to some extent.

00:19:51.200 --> 00:19:56.190
And if you're calling the
Carbon PB Read Fork Async

00:19:56.200 --> 00:20:01.170
or PB Write Fork Async,
you can specify explicitly that you want

00:20:01.200 --> 00:20:04.200
the system to do concurrent async I/O.

00:20:04.200 --> 00:20:07.580
By default,
the Carbon framework actually emulates

00:20:07.580 --> 00:20:13.200
async I/O by having a separate thread
that is introduced by the system.

00:20:13.200 --> 00:20:14.960
The I/O is then sent to the kernel to
take care of the I/Os while the rest

00:20:15.020 --> 00:20:16.160
of your process is allowed to proceed.

00:20:16.200 --> 00:20:19.190
And so things are essentially
single-threaded on the async side.

00:20:19.220 --> 00:20:22.600
But if you want, you can now specify that
the system should do true

00:20:22.700 --> 00:20:26.190
concurrent asynchronous I/O,
and it will get done that way.

00:20:29.700 --> 00:21:58.600
[Transcript missing]

00:22:03.430 --> 00:22:08.240
The system goes out of its way to
fill pages with zeros if necessary.

00:22:08.320 --> 00:22:10.990
It also tries to defer
that as much as possible.

00:22:11.000 --> 00:22:14.030
It tracks in memory what
is actually being written,

00:22:14.030 --> 00:22:17.750
and it's not as simple as
skipping around and doing

00:22:17.750 --> 00:22:19.600
I/O ahead in a non-sequential way.

00:22:19.600 --> 00:22:23.540
It will guarantee that you
end up doing writes of zeros,

00:22:23.540 --> 00:22:26.840
but it does introduce that possibility,
and if you can possibly write

00:22:26.850 --> 00:22:29.880
your files out sequentially,
you'll be that much better off.

00:22:29.960 --> 00:22:36.090
You can use allocate instead
of setEOF to allocate space

00:22:36.090 --> 00:22:41.260
without forcing zero fills,
because setEOF actually would move the

00:22:41.260 --> 00:22:45.520
EOF of the file out on disk and would
force zero fills of areas that you don't

00:22:45.580 --> 00:22:47.220
actually write as part of that write.

00:22:47.300 --> 00:22:49.760
Allocate simply reserves
the space in advance.

00:22:49.760 --> 00:22:52.410
Allocate is not supported
on all file systems,

00:22:52.420 --> 00:22:54.090
so you should just ignore
any errors you get,

00:22:54.210 --> 00:22:57.010
because the write will allocate
the space as it goes anyway.

00:22:57.020 --> 00:22:59.920
But calling allocate is a good idea,
and it's a better way to do it.

00:22:59.920 --> 00:23:02.590
idea than calling setEOF.

00:23:03.090 --> 00:23:07.190
Finally, if the file you're writing
is just a scratch file,

00:23:07.250 --> 00:23:09.400
and you're writing it
randomly in different places,

00:23:09.400 --> 00:23:11.040
so you're going to be
leaving lots of holes,

00:23:11.040 --> 00:23:15.150
truncate the file to
zero before you close it.

00:23:15.230 --> 00:23:18.890
Because that will preclude the
system from writing any zeros

00:23:18.890 --> 00:23:23.000
out to disk on data that you're
going to just throw away anyway.

00:23:23.080 --> 00:23:25.650
So if you have a scratch file,
and you're going to delete it

00:23:25.680 --> 00:23:28.420
as soon as you close it anyway,
truncate it to zero first and

00:23:28.420 --> 00:23:31.090
make sure you may have skipped
any zero fill that the system

00:23:31.110 --> 00:23:33.000
might have to introduce for you.

00:23:35.040 --> 00:23:38.770
A few words about directory enumeration,
which is another area that lots

00:23:38.770 --> 00:23:39.990
of apps can spend lots of time.

00:23:39.990 --> 00:23:47.990
You can call the raw BSD, read dir,
and stat calls on full paths.

00:23:48.000 --> 00:23:52.140
You can use get catalog info in
the Carbon framework to get the

00:23:52.490 --> 00:23:55.000
equivalent information out of stat.

00:23:55.000 --> 00:23:58.000
There are new calls that were introduced.

00:23:58.000 --> 00:24:00.480
The get their entries at,
or I referred to earlier,

00:24:00.480 --> 00:24:01.880
are the BSD level call of it.

00:24:02.000 --> 00:24:07.130
The get catalog info bulk is the
Carbon equivalent of that call,

00:24:07.130 --> 00:24:10.990
and it will enumerate a directory
and collect the metadata

00:24:11.000 --> 00:24:13.970
that you request as it goes.

00:24:14.330 --> 00:24:19.580
The big way that you can save there is
the call actually has a field which info,

00:24:19.590 --> 00:24:23.810
which specifies which fields in the
metadata you're actually interested in.

00:24:23.820 --> 00:24:26.880
And for some file systems that
would otherwise end up emulating

00:24:26.880 --> 00:24:30.530
some of the data or go through
contortions to generate some version

00:24:30.530 --> 00:24:33.330
or derive some version of the data,
if you don't actually care

00:24:33.750 --> 00:24:36.760
about some of the information,
you can indicate that by setting

00:24:36.870 --> 00:24:38.370
the which info carefully.

00:24:38.420 --> 00:24:41.040
And if you set that to as
small a set as possible,

00:24:41.070 --> 00:24:44.900
the system will only collect together
as much metadata as is absolutely

00:24:44.900 --> 00:24:47.700
necessary to satisfy the request.

00:24:47.890 --> 00:24:55.700
Finally, the file system SearchFS has a
Carbon equivalent of catalog search.

00:24:55.780 --> 00:25:01.400
It's an interesting mechanism that if
you're going to do a very large search

00:25:01.400 --> 00:25:05.410
of essentially the whole disk anyway,
may be a far better way

00:25:05.410 --> 00:25:07.150
of searching the data.

00:25:07.160 --> 00:25:08.720
It's surprisingly fast.

00:25:08.720 --> 00:25:12.350
In about 3-4 seconds you can
search an enormously large hard

00:25:12.450 --> 00:25:16.710
disk catalog in its entirety,
and you can search for matches based on

00:25:16.710 --> 00:25:24.110
a whole slew of metadata parameters or
file name substrings or things like that.

00:25:24.120 --> 00:25:27.550
So if you're scanning a volume,
use SearchFS.

00:25:27.740 --> 00:25:32.530
Don't do a treat reversal of the
whole thing starting from the root.

00:25:33.240 --> 00:25:42.200
Think carefully about how you access
data and make sure that you choose

00:25:42.200 --> 00:25:45.450
an appropriate mechanism for the
type of access that you're doing,

00:25:45.650 --> 00:25:49.320
or perhaps change the way you
access data or write data to

00:25:49.450 --> 00:25:53.200
better suit the mechanism and avoid
zero-fill on file systems like HFS.

00:25:53.200 --> 00:25:58.060
There's a great quote that says,
"More sins are committed in the

00:25:58.080 --> 00:26:02.690
name of optimization." However,
illusory than in actual code,

00:26:02.690 --> 00:26:04.200
and that's very true.

00:26:04.200 --> 00:26:07.220
If you're going to set about
optimizing your application,

00:26:07.220 --> 00:26:11.180
analyze it first and make sure you find
the true hotspots in your application

00:26:11.240 --> 00:26:15.710
before you go wild introducing some
new caching scheme or something because

00:26:15.710 --> 00:26:18.200
you're convinced that this part of the
code is what's actually really slow,

00:26:18.200 --> 00:26:21.490
and instead end up creating a
much larger memory footprint that

00:26:21.490 --> 00:26:23.190
costs you in the first place.

00:26:23.200 --> 00:26:25.550
performance when it
should have gained you.

00:26:27.350 --> 00:26:30.300
If you're porting an
app from another system,

00:26:30.400 --> 00:26:34.080
this is a good time to consider.

00:26:34.350 --> 00:26:37.630
How you're actually hitting
the disk in the file system.

00:26:37.630 --> 00:26:41.420
And some things that you may think
are cheap are in fact expensive,

00:26:41.420 --> 00:26:43.670
and some things that would
have been expensive on other

00:26:43.730 --> 00:26:44.930
systems are in fact quite cheap.

00:26:44.940 --> 00:26:48.230
So this is a good time to revisit
some of the fundamental assumptions

00:26:48.230 --> 00:26:49.940
that you may have in your data.

00:26:49.940 --> 00:26:54.480
And speaking of assumptions,
don't make assumptions about the

00:26:54.700 --> 00:26:59.520
speed of accessing some of these
things because innocent things like

00:26:59.570 --> 00:27:03.780
preferences could easily be located
across the net on an NFS server

00:27:03.780 --> 00:27:06.240
that may have crashed a minute ago,
for instance.

00:27:06.240 --> 00:27:09.110
And what you think is a very quick
access to refresh your preferences

00:27:09.200 --> 00:27:11.830
or something could potentially
take a long amount of time.

00:27:11.830 --> 00:27:17.050
So be careful about what you assume.

00:27:17.600 --> 00:27:22.000
Now,
say a few words about security and ACLs,

00:27:22.020 --> 00:27:25.400
which are both new in the Tiger release.

00:27:26.750 --> 00:27:29.850
You're by now probably
familiar with the STANDEX,

00:27:29.850 --> 00:27:33.490
UNIX permissions model that
Mac OS X has supported all this time.

00:27:33.540 --> 00:27:37.590
There's a single owner, a single group,
and three categories

00:27:37.590 --> 00:27:41.220
of access for each one:
read, write, and execute.

00:27:41.620 --> 00:27:45.140
There is a slight wrinkle in that we
allow the ignoring of who actually

00:27:45.140 --> 00:27:48.490
owns a file on removable media,
and we do that by default.

00:27:48.500 --> 00:27:51.350
But this is the basic model.

00:27:52.230 --> 00:27:55.460
Access Control List, change all that.

00:27:55.490 --> 00:28:00.370
Instead of having one owner and
one group and one others category,

00:28:00.370 --> 00:28:03.340
you can invent as many
categories as you want.

00:28:03.340 --> 00:28:06.260
And you can build up
a large list of ACES,

00:28:06.310 --> 00:28:11.280
Access Control Entries,
that specify for individual categories

00:28:11.280 --> 00:28:14.840
what they're allowed to do or
what they're not allowed to do.

00:28:14.840 --> 00:28:19.120
And the system takes the whole list
of things into account before falling

00:28:19.120 --> 00:28:21.750
back to the standard POSIX permissions.

00:28:23.340 --> 00:28:28.160
So, it's very powerful and it lets you do
some of the things that AFP servers,

00:28:28.180 --> 00:28:31.210
for instance, let you do,
where owners and groups

00:28:31.210 --> 00:28:37.440
can each be associated with
their own group of access.

00:28:38.730 --> 00:28:42.800
As owner or as group.

00:28:42.800 --> 00:28:48.040
If you were at the 2 o'clock ACLS talk,
you saw that the group mechanism

00:28:48.040 --> 00:28:49.690
has been greatly expanded.

00:28:49.700 --> 00:28:53.690
The system is no longer limited
to fixed 16 groups per user.

00:28:53.690 --> 00:28:56.630
It has given up on the idea
of trying to pre-enumerate the

00:28:56.630 --> 00:28:58.700
groups that every user belongs to.

00:28:58.830 --> 00:29:01.560
Instead,
there is a very flexible user level

00:29:01.560 --> 00:29:05.850
group membership demon that the
system consults to determine which

00:29:05.850 --> 00:29:10.640
users are members of which groups
dynamically and cache those results.

00:29:10.640 --> 00:29:14.130
As a result, there are things possible
that were not easily before,

00:29:14.260 --> 00:29:16.710
which is to have groups
that contain other groups,

00:29:16.710 --> 00:29:21.190
for instance,
and groups that are completely determined

00:29:21.190 --> 00:29:26.630
over the network in a very flexible way.

00:29:26.630 --> 00:29:26.630
That's great stuff.

00:29:26.680 --> 00:29:32.600
The ACL API is based on the
POSIX standard proposal that was not

00:29:32.600 --> 00:29:36.600
adopted and has since been withdrawn,
but it's very much based on that.

00:29:36.600 --> 00:29:40.410
And the actual permissions,
rather than being strictly the

00:29:40.410 --> 00:29:44.580
POSIX read/write/execute model,
are a much more flexible,

00:29:44.590 --> 00:29:46.940
much more fine-grained
set of permissions that,

00:29:46.940 --> 00:29:50.600
for instance,
separate control over metadata

00:29:50.600 --> 00:29:54.600
extended attributes from
control over the data of a file.

00:29:54.600 --> 00:29:59.820
So it would be conceivable, for instance,
to have a set of images, photos,

00:30:00.020 --> 00:30:04.730
that are read-only documents whose
metadata you can control and set your

00:30:04.740 --> 00:30:07.600
own keywords on and things like that.

00:30:07.600 --> 00:30:11.600
So what you can and cannot do
has become much more flexible.

00:30:14.150 --> 00:30:18.110
Besides more flexible control over
the appearance of a single file,

00:30:18.560 --> 00:30:22.600
there's also more flexible
control over the metadata and the

00:30:22.600 --> 00:30:24.000
directory structures themselves.

00:30:24.000 --> 00:30:27.810
There's ways that you can inherit
access control information from a

00:30:27.830 --> 00:30:29.960
parent to a file that gets created.

00:30:30.000 --> 00:30:34.210
There are separate controls over
the ability to control items within

00:30:34.210 --> 00:30:39.000
a directory without regard for the
access controls on the item itself.

00:30:39.000 --> 00:30:44.000
There's a lot of new stuff there.

00:30:44.000 --> 00:30:47.790
There's a session tomorrow at 5:00.

00:30:47.990 --> 00:30:52.300
Not sure where,
about the extended metadata APIs.

00:30:52.300 --> 00:30:54.160
There was an session today at 2 o'clock.

00:30:54.160 --> 00:30:56.600
I'll have references to all those later.

00:30:57.650 --> 00:31:03.490
So finally,
notification is another area that an

00:31:03.490 --> 00:31:07.050
app can easily spend lots of cycles,
perhaps unnecessarily.

00:31:07.060 --> 00:31:11.330
The obvious way you might go about
seeing whether something changed,

00:31:11.340 --> 00:31:14.760
if you have a folder that people can
drop plugins into or something like that,

00:31:14.760 --> 00:31:18.750
is to periodically poll and see if the
mod date of that directory has changed.

00:31:18.760 --> 00:31:20.800
That will work,
but you're spending time even when

00:31:20.800 --> 00:31:22.460
nothing's happening in the system.

00:31:22.460 --> 00:31:25.770
One improvement on that
is the whole FN subscribe,

00:31:25.770 --> 00:31:28.620
FN notify mechanism that's been around.

00:31:28.620 --> 00:31:34.960
But a much nicer solution that
we introduced support for is KQs.

00:31:34.960 --> 00:31:38.580
And I'll say a few words about KQs.

00:31:38.640 --> 00:31:44.420
The basic model of KQs was
derived from the observation that

00:31:44.920 --> 00:31:49.740
applications spending lots of time
in basically a select loop or pull,

00:31:49.740 --> 00:31:52.200
the problems are not fundamentally
different between the two,

00:31:52.200 --> 00:31:57.420
are very wasteful just in the system
call overhead to do those things.

00:31:57.420 --> 00:32:02.940
And so KQs were introduced to give
a model where instead of specifying,

00:32:02.940 --> 00:32:06.390
like on a select,
the set of file descriptors that

00:32:06.390 --> 00:32:10.440
you're interested in data being
ready for on the select call itself,

00:32:10.440 --> 00:32:14.610
you would create a KQ and you're
giving back a file descriptor

00:32:14.660 --> 00:32:15.940
like an open file would be.

00:32:15.940 --> 00:32:20.820
And then you call K event to specify
the set of file descriptors that

00:32:20.830 --> 00:32:24.040
you're interested in I/O happening on.

00:32:24.040 --> 00:32:27.240
And then your process is put to sleep.

00:32:27.240 --> 00:32:30.220
Waiting for anything to
happen on any of those file

00:32:30.220 --> 00:32:34.550
descriptors and when it wakes up,
is told something has happened

00:32:34.650 --> 00:32:36.770
on this file descriptor.

00:32:38.310 --> 00:32:46.860
So just the basic select loop mechanism
is much more efficient that way.

00:32:46.870 --> 00:32:53.280
We've implemented a source compatible
with the 3SB version of KQs,

00:32:53.280 --> 00:32:56.680
but we've also extended the
mechanism a bit by introducing

00:32:56.680 --> 00:32:59.340
additional event filter classes.

00:32:59.340 --> 00:33:05.180
And one of the classes is a Mach port
filter class that will tell you whether

00:33:05.290 --> 00:33:08.480
some message has arrived on a Mach port.

00:33:08.480 --> 00:33:12.160
And another one tells you about
things happening in the system.

00:33:12.160 --> 00:33:15.950
For instance,
whether file systems get mounted or

00:33:16.060 --> 00:33:21.050
unmounted anywhere in the system,
if one of those events happens,

00:33:21.050 --> 00:33:24.280
your KQ would get an
event delivered to it.

00:33:26.340 --> 00:33:32.260
The one thing to keep in mind about KQs,
and part of the reason why they're

00:33:32.270 --> 00:33:36.330
so efficient in the kernel,
is that it's not an actual

00:33:36.470 --> 00:33:41.300
queue of individual discrete
events being notified to you,

00:33:41.360 --> 00:33:46.420
but rather a single notification
that some condition has come

00:33:46.540 --> 00:33:48.170
to be true in the system.

00:33:48.180 --> 00:33:52.070
So, you wouldn't get ten little
events saying three bytes are

00:33:52.070 --> 00:33:55.470
available at your file descriptor,
five more are available

00:33:55.470 --> 00:33:57.090
on this file descriptor,
ten more.

00:33:57.100 --> 00:34:01.180
You would get a single event
that says there's data available

00:34:01.180 --> 00:34:03.090
on this file descriptor.

00:34:03.630 --> 00:34:06.260
That's something to be
mindful of in the model.

00:34:06.310 --> 00:34:10.690
It's not supported on all volume formats
because the implementation does require

00:34:10.700 --> 00:34:13.500
support inside the file system itself.

00:34:13.500 --> 00:34:17.260
And if you are a client
to a network file server,

00:34:17.260 --> 00:34:21.670
you should be mindful that
you may get notifications,

00:34:21.670 --> 00:34:25.590
but you will only get them about
the local events happening,

00:34:25.590 --> 00:34:26.500
going out to that file server.

00:34:26.500 --> 00:34:30.560
Not necessarily all the other things
all the other clients of that server

00:34:30.560 --> 00:34:32.490
are doing on the server itself.

00:34:32.510 --> 00:34:34.500
Those are two things to keep in mind.

00:34:34.500 --> 00:34:37.960
And when you open a file
descriptor on a directory,

00:34:37.960 --> 00:34:42.490
for instance, to request notifications
about that directory,

00:34:42.990 --> 00:34:49.330
there is a flag you should set,
"O Event Only," which says that

00:34:49.420 --> 00:34:53.500
this file descriptor is going to be
used strictly for event delivery.

00:34:53.670 --> 00:34:56.500
Because that will let
the system correctly,

00:34:56.500 --> 00:35:02.500
decide at unmount time whether a
file system is truly busy or not.

00:35:03.030 --> 00:35:06.380
So to avoid file descriptors that
are open solely for the purposes

00:35:06.380 --> 00:35:09.990
of notification from preventing
you from unmounting a file system,

00:35:10.010 --> 00:35:13.650
if you set OEvent only on the open,
the file system will let

00:35:13.650 --> 00:35:15.560
the unmount go ahead anyway.

00:35:15.600 --> 00:35:18.670
So OEvent only is one
to be very mindful of.

00:35:22.730 --> 00:35:26.070
Overall, unfortunately,
despite the best efforts of

00:35:26.070 --> 00:35:30.310
the higher level frameworks,
there are lots of capabilities that are

00:35:30.310 --> 00:35:33.700
very specific to individual file systems.

00:35:33.750 --> 00:35:39.110
And if you rely on some
particular behavior in the system,

00:35:39.110 --> 00:35:44.400
you should probably check beforehand
whether the particular file system

00:35:44.400 --> 00:35:46.760
you're operating on supports that.

00:35:46.800 --> 00:35:49.890
And there's a variety of
ways that that can be done.

00:35:50.010 --> 00:35:52.570
There are get-out-of-list variants
that will tell you something about the

00:35:52.610 --> 00:35:55.260
capabilities of a certain volume format.

00:35:55.280 --> 00:36:00.640
At the Carbon level,
there's the get-vol parms calls.

00:36:01.120 --> 00:36:06.180
But be mindful of it and either
test ahead of time or even better,

00:36:06.180 --> 00:36:10.640
be prepared with a fallback option
in case the behavior you need is not

00:36:10.640 --> 00:36:12.340
supported by the particular file system.

00:36:12.340 --> 00:36:16.370
But with that caveat,
make use of all the file system

00:36:16.420 --> 00:36:19.270
special capabilities that you want.

00:36:22.200 --> 00:36:28.170
If you're worried about some of the
tools that you all have available,

00:36:29.190 --> 00:36:34.240
One that I find myself using on an
at least daily basis is FS Usage.

00:36:34.290 --> 00:36:37.370
It's a great tool that lets
you monitor all the file

00:36:37.370 --> 00:36:39.100
system activity in the system.

00:36:39.190 --> 00:36:42.090
And you can constrict it to
a particular application,

00:36:42.100 --> 00:36:47.100
or you can monitor everything that's
happening in the whole system.

00:36:47.100 --> 00:36:50.280
If you want to find, for instance,
who it is that's writing a certain

00:36:50.280 --> 00:36:53.500
file or creating a certain something,
and you find out the file

00:36:53.500 --> 00:36:55.480
system call that's made,
the process that made it,

00:36:55.490 --> 00:36:59.100
what the results of the call were,
so you can see errors happening.

00:36:59.170 --> 00:37:02.960
If some application is throwing
up its hands without any clear

00:37:02.960 --> 00:37:07.280
indication of what's going wrong,
you can run FS Usage and see what kind

00:37:07.280 --> 00:37:08.100
of file system errors it's getting.

00:37:08.100 --> 00:37:10.040
It's a great tool.

00:37:10.090 --> 00:37:14.100
SC Usage does the same thing
for other system calls.

00:37:15.420 --> 00:37:18.790
The features have been somewhat
expanded in Tiger since the last

00:37:18.790 --> 00:37:23.140
release you may be familiar with,
so it's worth checking out the -f option.

00:37:24.700 --> 00:37:30.520
And if you run with the debug
versions of the system frameworks,

00:37:30.520 --> 00:37:31.790
you'll get tracing at that level.

00:37:31.790 --> 00:37:35.600
So you'll find out not just
that a stat call is being made,

00:37:35.610 --> 00:37:38.490
but that the application is
calling get catalog info,

00:37:38.490 --> 00:37:40.860
which is causing the
stat call to be made.

00:37:40.860 --> 00:37:42.220
So it's good stuff.

00:37:43.200 --> 00:37:46.910
Top is another great tool to get a
quick look at sort of at a very high

00:37:47.330 --> 00:37:48.950
level what's going on in the system.

00:37:48.950 --> 00:37:50.650
You see all the running processes.

00:37:50.700 --> 00:37:54.830
You'll see who's using CPU time,
who's making system calls,

00:37:54.910 --> 00:37:58.760
because you can see the
context switches happening.

00:37:58.760 --> 00:38:03.820
There's some summary information that'll
tell you the paging rates in the system,

00:38:03.820 --> 00:38:07.830
the number of I/Os that are happening,
how busy the system is.

00:38:07.830 --> 00:38:10.090
It's great for just sort
of getting a feel for,

00:38:10.090 --> 00:38:13.090
hey, the system's a little sluggish,
what's going on.

00:38:13.100 --> 00:38:17.030
You find some app that's
pegged at 99% of the CPU time,

00:38:17.030 --> 00:38:20.020
you've got your suspect to go look at it.

00:38:20.590 --> 00:38:24.510
Time is a great command line tool
that will just count I/Os and

00:38:24.510 --> 00:38:28.240
tell you how much I/O is done,
how much user time and how much

00:38:28.240 --> 00:38:34.380
system time is taken as part
of executing a certain process.

00:38:35.200 --> 00:38:39.620
If you're setting about
optimizing your application,

00:38:39.630 --> 00:38:42.540
monitoring both of those
is interesting to do.

00:38:42.620 --> 00:38:48.740
Sample is great for looking where
a process is spending its time.

00:38:48.830 --> 00:38:52.860
There are more powerful
profiling tools available,

00:38:52.860 --> 00:38:57.240
like Shark, which is part of the Chud
tools that you can install.

00:38:57.300 --> 00:39:00.240
But Sample, especially when you
think a process is stuck,

00:39:00.260 --> 00:39:02.560
is a great,
quick command line tool to get

00:39:02.560 --> 00:39:06.300
a look at what the current stack
frame is in another process.

00:39:07.680 --> 00:39:11.600
And finally, LSOF will tell you about all
the open files in the system.

00:39:11.730 --> 00:39:15.440
So if you can't eject a volume
because some file is busy,

00:39:15.440 --> 00:39:17.590
LSOF can tell you who has it open.

00:39:17.600 --> 00:39:22.580
Or if you want to find out what
files an application is keeping open,

00:39:22.580 --> 00:39:25.100
you can find out through LSOF.

00:39:25.200 --> 00:39:26.660
Those are just some of the tools.

00:39:26.660 --> 00:39:32.100
There are specific sessions that were
devoted to doing performance analysis

00:39:32.110 --> 00:39:36.500
and profiling of your tools that
are very much worth looking into.

00:39:36.590 --> 00:39:39.780
But FS Usage is one of my favorites.

00:39:39.870 --> 00:39:41.500
Shark is definitely worth checking out.

00:39:41.500 --> 00:39:44.040
I think there's a session on
Friday devoted just to Shark,

00:39:44.040 --> 00:39:44.840
for instance.

00:39:44.900 --> 00:39:47.440
So check it out.

00:39:47.440 --> 00:39:53.100
For more information in general,
you can contact Jason Yeo.

00:39:53.160 --> 00:39:56.360
And there are also some resources.

00:39:56.580 --> 00:40:01.990
on the DVD and online that will
tell you more about the particular

00:40:01.990 --> 00:40:04.900
file system or in general,
software design guidelines.