---
Title:  Preprocessing Principles
Year:   2004
Web:    https://nonstrict.eu/wwdcindex/wwdc2004/704/

[!] This transcript was generated using Whisper, it has known transcription errors.
---

And we're here today to talk about preprocessing principles and we're very, very lucky to have, I think, one of the premier folks in this area, Ben Waggoner. He's from Ben Waggoner Digital. Everybody knows Ben. Ben's been around for a long, long time. We're going to go for about an hour.

I'll hold all questions to the very end of the session. And then when we do the questions, please come up to the microphone so we can catch your voice on the recording. Thank you. OK. Let's dive in. So how many people saw this session at the 2003 show?

Not too many of you. OK. That's good. Because I only thought I was doing this about five days ago. So it's the exact same slides, but the content will be new. So I'll bring my own spin to things. So it works now? It's you? And there's me. And it's all good? That's who I am.

Wonderful. OK. So today we're talking about preprocessing. And you can think about preprocessing as everything you do between the source frame of video and the frame that actually goes into the codec. So pretty simple definition here. And let me walk over here so everyone can see this stuff.

So big, this stage up here. The focus is going to be on web video. I'll mention MPEG-2 stuff here and there. Who cares about preprocessing for DVD? I'll be talking a fair amount about preprocessing for DVD now. I have nothing but interactive. Who cares about preprocessing for web? For CD-ROM? Anyone doing preprocessing for high definition?

Oh, got one here. Okay. That's cool stuff. We'll talk about that. I spent far too much time working with damaged D5 tapes in HD last fall. The first half of the session is going to be me talking about slides and showing some screenshots and that kind of stuff.

The second half is I'm going to be doing some demonstrations. I've got a lot of source clips here. I've got a lot of tools here. I'm going to let you guys pick the demonstrations you guys want to see. The more extroverted among you, start planning what your questions are going to be. The focus here is just hands-on stuff that's going to let you get better quality video up on the web or your DVD or however you're doing it. We're trying to increase the bang for the bit out of our digital media.

So, we're going to define preprocessing in a little more detail, explain why it matters, talk about some of the core techniques, I'm going to do some demos of some cool stuff on the Mac, my power work over there, and mainly web video, but some about DVD as well. And I'll do more of that as planning on, because you guys care about it a lot. There are actually some pretty cool tricks for DVD you can do these days. Like 704 wide encoding, we'll talk about that.

So, preprocessing, I mentioned before. You think of your video stream as a series of frames. You've got a DV, you've got a whole bunch of 720 by 480 frames. Who here mainly works in PAL? Any of you guys? Okay. So, 720 wide by either 480 or 576 tall frames. You know, your web video is a 320 by 240. You've got a DVD, it's probably the same size as your video source. You know, whatever you want to do. So, you're doing everything to transform the source frame into the optimum output frame for the codec. back.

This is both the most artistic part of compression, because stylistically you have to make some stylistic decisions off in the process, trade-offs, that kind of stuff. It's often the hardest part. I mean, knowing what the data, you want to get the right data right, you just type in the data rate value you want to have. Knowing what's the right luma level for having it to look good, that requires some thought.

I probably spend, on a typical project, where it's going to be kind of a high volume stuff, maybe 90% of my time in the compression process on preprocessing for challenging content. Just because the other stuff's pretty easy. You know what codec you want, you know what data rate you want, you know what the audio should be, but you've got to kind of tweak and tweak and tweak preprocessing if you're incredibly quality obsessive, which I recommend you all become, because there's too much bad web video out there. So, why it matters. It's all about maximizing the bang for the get bit. You want to get the maximum communication value.

You want to get the maximum value per unit data possible to your end users. And it really does matter a lot. I mean, correctly processed video at 200 kilobits can be way better than badly processed video at 1,000 kilobits. So, you can always think of it as like, you're almost like you're buying your customers more bandwidth by treating your bits better.

So you want to make sure every pixel has data that matters, every bit is something you care about, and you're not wasting bits and pixels on things that aren't actually communicating to the end user. So, let me just cut a couple frames here. Let's see if this is scaling correctly. Okay, so this is just from a movie trailer. This is from the Biker Boys movie trailer. Anyone see Biker Boys? Neither, but I have the trailer. Oh, there we go.

Any good? No, I don't think so. That's the problem. But I have the movie trailer. It's often good to work with not very interesting content when you're experimenting with things. You don't get distracted by plot and that kind of stuff. So, this is from Biker Boys. Let's go with what the source frame looks like.

Pretty typical interlaced frame. Projectors kind of squish in a little bit. They get the idea here. And because it's interlaced, you wind up having the two different fields. There's motion, all that kind of stuff. Now, if we preprocess it correctly, a couple of things will go from that to this.

and the shape of the frame should be changing a little bit. All right, hold on. If we take the source frame, don't modify it at all, encode it, we get that. This is 800 kilobit source in 3 Pro. And that looks terrible. That's the word, terrible. Because codecs, modern codecs, based around DCT or something else like that, do really good with gradients. But sharp edges take a lot of bits to encode.

So when you're using, say, the thin horizontal lines of fields, you wind up having that be very hard to encode. And almost all your bits wind up trying to code the lines you don't care about, as opposed to coding the content you do. So same data rate, preprocess the frame, we get that instead. So is that perfect? No. But obviously, from an end user perspective, that is a lot worse than that. Same bit rate. So for the end user, they just get a better experience, no sacrifice.

And that's not really an exaggerated case. I mean, I see a lot of people trying to do this on the web, and I wish they would not. One of the reasons why QuickTime has such a reputation for having high quality delivery technology is really because the people at Apple who do the QuickTime movie trailers are so much more confident than the people who do the movie trailers. You can find it at windowsmedia.com and realmedia.com. It's much more about preprocessing than the codecs. They give you quality.

The most critical feature at Mission Tier is deinterlacing. You've seen video before. Your even lines and your odd lines contain information that's temporally separated by half a frame duration. If you're at 30 frames a second, the two fields will have an image a 60th of a second apart if it's interlaced video. Obviously, it's progressive video. You don't have that going on at all. Anyone here not grasp interlaced video? Who's mainly a video person here? Mainly a computer person? All right.

Does anyone not grasp interlaced at this point? All right. Okay. You'd be too embarrassed to tell me if you were, but at least I can now claim I asked. So, and what happens is that one frame you saw where when you have a lot of motion, areas where there's a lot of motion, you wind up with this kind of crosshatch effect. In areas where there isn't a lot of motion, it looks normal.

Computer display back is always progressive. So if you're going to a computer device, it's always progressive. Projectors are pretty much always progressive. There really aren't very many truly native interlaced display devices that are being designed at this point. We have like legacy televisions and that kind of stuff, but clearly everything is turning into a progressive display device and have interlaced content that's just getting converted to progressive on playback. So the future is progressive, is my feeling.

And if you're delivering for any kind of web codec, you need to deinterlace it because none of the web codecs we care about support interlaced mode. If you leave it interlaced, one, it just looks stupid. Even before you compress it, it looks stupid because someone's throwing a baseball. Instead of seeing one baseball, you see two baseballs that have half their lines to it, and it's very confusing. And also just because the codecs find those sharp lines on the edges of the moving objects so hard to encode, almost all your bits wind up getting cut off.

So you're going to have a lot of interlacing content being spent on the stupid part of your image and very few are left for actually making your image look good. So big degradation in quality. Of course, progressive content doesn't need to be deinterlaced because it's already progressive. And if you're delivering on DVD with MPEG-2, because that's also a fielded medium, you're just going to keep the same field mode. So if you have interlaced source, you're making DVD, you're going to keep it interlaced throughout. If you have progressive source, you're making a progressive MPEG-2 file for the optimum results. And these days, on most modern Macs, DVD player will always be in the middle of the screen.

So you automatically deinterlace on playback of interlaced content. A lot of older systems, putting your graphics card doesn't always do it. I'm not sure what the actual rule is on that. But it used to be that the Mac DVD player couldn't play interlaced content very well at all. But it seems to be a lot better in more recent versions.

The basic method of de-interlacing, if you will, is just, okay, I've got my even lines and my odd lines have a different image in them. Well, I'll just throw out all my even lines, I'll throw out all my odd lines, and then process the image from there. So if you have a 720x480 db frame, essentially you're just throwing out half the lines and you're left with a 720x240 db frame, and then it gets stretched or squished or whatever, processing like any other kind of normal Photoshop style image processing.

And that works. The problem is you're throwing away half your image data. And if you're doing like little small web video, that's not a problem. Like QuickTime Broadcaster does that internally. You know, if you're doing 320 by 240 or less for broadcast, not a huge quality drop. But if you're trying to go to bigger frame sizes, you can actually wind up with a lot of compression artifacts. So if you're going from, because your skidding has to get stretched, so if you're going to do a 480 line out from what's internally a 240, you wind up doubling the height with the typical scaling blocky artifacts.

So if you're really doing deinterlacing, what you want to be doing is what's called adaptive deinterlacing. And all the tools we care about these days support some flavor of adaptive deinterlacing, lots of different names. Basic idea of adaptive deinterlacing is to detect the parts of the frame that are moving.

And

[Transcript missing]

It was half the resolution of the baseball. But if someone's throwing a baseball and they've got a big static background-- the fence or whatever the background-- the fence won't get the interlace and it'll remain sharp. And that works well for our visual system. We can either detect motion or detail, but we can't really see fast moving detail.

So it's a nice setup. Some occasionally, it would guess wrong, but most of the modern implementations, 99% of the time, is going to give you the right result. Sometimes, like scrolling credits, you might see some kind of weird results that have to be interlacing. Probably the worst case. So try not to have scrolling credits. I've actually, sometimes, just gone through and re-implemented the credits. Just typed them in again and re-rendered them in progressive just to get around that problem.

Now the most important thing for you NTSC folks, for film content, and you PAL people can happily and pridefully ignore this part because you don't have this problem, is inverse telecine. So, film runs at exactly 24 frames a second. Video runs at not exactly 30 frames a second. It really runs at 29.97, and each of those frames has two fields in it.

So, when film gets converted to video in a telecine machine, to NTSC video, what happens is what's called 3-2 pull-down. So, the first frame of film becomes three fields of video. The next frame of film becomes two fields of video, 3-2-3-2, and it's 3-2 pull-down. And that basically works to get your 24 images, become 60 fields per second, and you're good to go. It works about as well as you'd expect. Of course, the motion is never quite smooth.

Some source frames will last three fields of video, and other ones will last two fields of video. So, motion that would have been very smooth in the beginning would have been a little bit jerky because the duration of each frame playback would be a little bit off. And that's why if you watch movies on PAL, a movie on PAL, horizontal motion and pans and that kind of stuff, always looks a little bit smoother than the same movie would look in NTSC.

The way that PAL conversion is done is that just the 24 frames a second are sped up 4%, 25%. And it just remains 25 frames a second progressive. And that's so easy, and I should move to London because half of my life is dealing with NTSC weirdnesses like this. But we've got to do it right, and it's hard to do, and that's why we can make money charging for doing video work.

So, what you wind up with when you've created a file with this, and you've seen this a lot, is a file where you'll see three progressive frames and two interlaced frames repeating. And I'll show you a sample of that in a few minutes. And that's an easy way to test. So, you can go to the Word and QuickTime Player, go through frame by frame in a section with motion. You'll see three progressive frames, two interlaced frames, three progressive, two interlaced.

And that's what you'll see. And the nice thing with that is if you have a tool which hasn't inversed TELS in the algorithm, and we have several on the Mac, it'll be able to reverse that process. Instead of having to deinterlace it and throw image data out, it's just able to restitch the original source frame. So, it's able to reassemble both fields into the original 24 frames a second video. And that's great for two reasons. One, it just, you know, it's, we don't, we keep the full image data. And second, we're able to restore the original time base.

So, when we output, we can actually encode at 24 frames a second instead of doing the 3-2 pull-down thing. So, actually, we get smoother motion, same source on computer playback that we would have had on the video playback. Because every frame will have the same duration of exactly 24th of a second.

Now, one complexity is when you do the transfer, the film is really slowed down to 23.976 frames a second to match the way that 60 compares to 59.94. Details don't really matter that much. But for most tools, if you're encoding with using inverse TELSINI, it doesn't support changing time. You have to actually encode at 23.976 frames a second. It's a magic number. Other tools will let you change the time base. You can easily switch that around.

And... If you have film kind of content, that's pretty much any music video, movie trailer, feature film, primetime drama, those are all going to be content that was created either in a film or the 24p high definition camera. And if you have content that's like that, that winds up with 3D2 pulldown, you absolutely want to have this available. This pays off hugely in terms of output quality. One complexity is when someone has source that's done on video, but then it's edited as source that came out as 24p.

You think it's edited in a fashion where it's not keeping track of where the original frames were. Now Final Cut is trivial. You just have a 24p project, take a 24p source, it'll do it frame accurate, and when you export it with 3D2 pulldown, it'll keep what's called the cadence of the 3 and the 2 consistent. However, if someone just takes so much to tell us they need video files, puts them in a Final Cut, add a video frame rate project to an interlaced 29.97 file, when they do edits, they're not going to wind up putting edits.

So it's not going to be exactly where the edits would have been in the film originally. And then when you take that into a tool, you can wind up with issues where it just can't figure out where the frames were because of what's called the cadence. So instead of getting the 3232, you make it 323232132323232223, kind of weird pattern as that, where video edits get dropped in. And some tools just completely fail, like After Effects when given that kind of content. Other tools like Cleaner deal with it pretty robustly. Ideally, you just have content where it's done correctly.

Cropping. Cropping is a place where I see people messing up a lot. Especially, you'll see a lot of web video out there while there's a few pixels of black on the left and the right or on the top or the bottom, something like that. The reason about that is video monitors, televisions, don't show the edge of the video signal by definition. I mean, a consumer TV does not have an underscan mode.

You just wind up with a safe area around the edge that you know is going to get left out. Obviously, when you play back on a computer on a screen, it's going to give you every single pixel. The upper left-hand corner pixel is going to be shown on the screen, or else it's horribly miscalibrated.

Because of that, when you're converting from content composited for video, there may be junk around the edge of the screen that would never be seen on a television, but that would show up when you look at the same frame on a computer. And the simple thing there is to remove that, crop it out.

and very many of you want to crop out edge blanking. That's going to be with the thin black lines at the top of the bottom or the left or the right. Often DV source has no blanking at all, but typically analog source is going to. And sources being captured at 486 lines tall is almost certainly going to have a few black lines at the top.

The lower the resolution you're going to go to, the more you can crop. Because when we shot with the safe area, which is the region that's known to work in all televisions, no cinematographer is going to put critical content within 10% of the edge of the screen. Because they know that plenty of systems aren't capable to... consumers are going to have TVs that are going to either give you a distorted image or no image at all at the very edge of the screen.

So they're not going to put anything critical there. Your lower third text is not going to be intruded into the 10% boundary around the screen. The actor's heads aren't going to be in there, all that kind of stuff. So the lower the resolution you're going to, you can actually crop pretty aggressively into the safe area, making the foreground objects a little bit larger.

The next thing you want to do is crop out letterboxing. There's no point in sending black bars to a customer, for most stuff. Certainly not for web video, because you can make your web video any size. If you leave any kind of black bar in there from letterboxing, you're just spending bits and CPU cycles on playback on nothing. Much better to just crop those out and call it done. And also because many codecs, especially at low bit rates, give you artifacts with sharp lines, that very sharp black line of letterboxing winds up messing things up a little bit. You often get distortion around that top.

When you go into DVD, of course, DVD only supports up to 16x9 anamorphic. So if you have any kind of film source that's more than 16x9 in aspect ratio, like most films are like 1.85 to 1 or 2.35 to 1, you're going to need to leave in some letterboxing, and that's inevitable with DVD. But for web video, you're going to need to leave in some letterboxing, and that's inevitable with DVD. But for web video, you're going to need to leave in some letterboxing, and that's inevitable with DVD. For web video, you never need to have letterboxing in your video.

Just a reference there. So this is the 5% boundary here called action safe. That's the 10% called title safe. So the general rule of thumb is for the action safe area, motions in this area should be able to be seen as motion. Anything out here is fair game. It may or may not be presented.

Anything in here should be able to see it kind of moving, but if it's text, you might not be able to see it because it'll be distorted around the edge. So the title safe area, the image should be pretty clear. I mean, you're not going to get any distortion at all theoretically. So text should be visible, all that kind of stuff.

So there's never going to be anything critical outside the action safe. You should assume everything inside title safe is going to be critical. And depending on the content, somewhere in here is kind of the range between where things get critical or not. And you can just kind of look at that. If you can imagine, let me turn the laser thing. There we go. I got laser pointer.

If you look at the bounding box there, if you're doing like a little like 160 by 120 web movie for modem use or GVP kind of stuff, the difference between having a frame that shows, there's all this other stuff. And the drum is kind of neat and, you know, his head. But if you just crop down to that, you can see his hands is better, his facial expression is better and it can help out a lot.

Then scaling. Scaling is taking what's our-- or after we've cropped our source, basically crop is telling don't take these pixels outside this box into consideration when you're doing your scaling. And then the bitmap, this change in size to the output bitmap actually gets handed off to the codec. And two things. One is, especially for web video, you need to make the video smaller to play back on the web.

And two, you're also going to be correcting for aspect ratio in here. One thing that just typically freaks out web and computer people is I start talking about non-square pixels. Because from the computer world, the idea of a non-square pixel is like talking about a square wheel. But the video world, all the professional video formats use pixels that are rectangular in shape.

So 720 by 480 could be either a 4 by 3 aspect ratio or a 60 by 9 aspect ratio. But if you just looked at 720 by 480, a square pixel, that would actually be a 3 by 2 aspect ratio. And that never actually occurs. So any kind of DV file is always distorted, either squished or stretched horizontally, depending on what the format is.

When you're going to a web video format that's going to be square pixel, and almost all of them are going to be square pixel, you need to be able to correct for the fact that the source is non-square pixel. You have to make it two square pixel. You have to either stretch it or squish it in order to make sure you correct for that. The basic goal is if on the video monitor, there was a circle, on the computer monitor, on playback, there's after compression, you want there to be circles as well.

and uh... that's pretty straight, that's the goal. You see a lot of stuff where things are about, stretched about ten percent too wide on the web. Quite common. People do, people figure, okay, 720 by 480, I'll cut it in half size, it'll be 360 by 240, and there we go, and I'll put it up on the web.

You can assume, anytime you ever see a 360 by 240 web video file up on, someone did it totally wrong, because they did not get the aspect ratio correct. If you have four by three source, you want, you want to have your output resolution in square pixel also be four by three.

So, four by three source, a 320 by 240 would be good, because 320 divided by 240 is, goes down to four by three. Six by 480, five by 384, anything where you have four units wide by three high is going to work. If you do 360 by 480, everything's going to be ten percent too wide.

If you deal with actors very much, when you make them about ten percent extra, ten percent more fat, they complain a lot. So, bad win there. And your circles are ovals, all that kind of stuff. So, you just want to make sure that you're, we're going to square pixel output format. You want to make sure that your output resolution matches your, your output and your source frame aspect ratio.

So, two, two great examples for web video are, you're coming from 720 by 480, typical DV content. You want to have a 320 by 240, or four by three. And if it's a 16 by nine source, 432 by 240 works just fine. So, the, you're picking the output, and the only difference here is the aspect ratio of the source file. The resolution is 720 by 480 in both cases. And if we're doing PAL, these are both good numbers as well.

Because, again, even, PAL is 576 by 720, but it's also either 4 by 3 or 16 by 9. So, you need to have a 4 by 3 or 16 by 9 output frame size. Some codecs like, don't like, don't like odd numbers and that kind of stuff. As a rule of thumb, as long as height and width are both divisible by 16, you're in pretty good shape. With Sorenson and MPEG-4 and that kind of stuff. MPEG-2 has a few very specified frame sizes it supports for DVD stuff.

So you just have to pick one of those. And typically when you're doing a DVD encode, you don't have to worry about this at all. You're just gonna stay at, you're not doing any scaling at all. If it's a 16 by 9, 720 by 480, you're gonna go to a 16 by 9, 720 by 480.

There are a few cases like if you have a 720 by 486, for example, you'll need to crop from, you know, the 486 down to 480. It's really important when you're converting from a 486 source to a 480 line source, you don't scale it, but you crop it.

Because the 486 is actually grabbing three more lines out of the, I mean six more lines out of the video signal than the 480 does. So if you scale it, you'll get a little bit of a distortion. And a little bit loss of image quality as well. The, the, if you have a 46 source, like a, you know, like a SDI capture, something like that, wanna make a DVD out of it, you wanna crop four lines off the top and two lines off the bottom.

You may be tempted to crop three and three, cuz that sounds the right numbers. But if you, if you crop an odd number of lines, depending on the tool, you may or may not wind up reversing your field order. So as the odd lines become even lines, and then when you play back, things go higgledy-piggledy.

So much better to just pick four at the top, two at the bottom. You don't have to worry about those changes happening. And also the quality of your scaling algorithm matters as well. Professional encoding tools we use, you know, assign or make you the kind of scaling, you just do like a quick time export, you know, for a quick time player.

Go for a really big file and a really small file. You'll often get a little bit lower quality scaling just cuz, you know, it's quick time. Quick time's not meant as a professional encoding tool in the player itself. Compressor will give you a much better result than the quick time player will, the exact same settings, because these are higher quality scaling algorithm.

Okay. So when we're authoring for web video, or anything really, our goal is, if we're going to scale, we only ever want to scale down. Because any time you scale up, you're interpolating. That's like going into Photoshop or After Effects and trying to make something bigger than it was. And it always gets soft, and it will often get blocky. It's not a good experience. When you're shrieking down, you maybe lose some detail, but the resulting image will at least be sharp.

So let me just kind of walk through a scenario here. If you have a 720x480, you're doing a non-adaptive deinterlace, or if you have content where the entire frame is moving, and if you have a case where the whole video frame is moving at once, adaptive deinterlacing does not pay off at all.

Because all adaptive deinterlacing does is it doesn't deinterlate the parts of the frame that aren't moving. If the entire frame moved, you're just going to have to eliminate one of the two fields. If we did a safe area crop of 10%, that would take us down to a 7.5% drop.

If we did a safe area crop of 10%, that would take us down to a 6.48x216 size. And then we would convert it from there to a 320x240, which seems like going from a 720x480 by a 320x240 should be scaling down by a lot. But after we've deinterlaced, after we've cropped, we're actually scaling up vertically, even though we're going that way horizontally.

Pre-processing is one of the most important aspects of video processing. When working with interlaced, vertical is vastly more important and tricky than horizontal is. You have all the horizontal you could possibly want, but the vertical is really what you're trying to preserve. It's down to figuring out what you're trying to preserve. Typically you're going to try to preserve as much vertical detail as you can. You typically don't want to crop even one extra line of vertical you don't need to, and then just the horizontal is necessary.

So, when you're going to 320x240 or higher from NTSC, or 384x288 or higher with PAL, you want to crop as little as possible. You're definitely always going to crop out any head of blanking or letterboxing because there's just no data there, but you don't want to crop any extra stuff in the safe area.

And you want to use adaptive deinterlacing if it's a true interlaced source, and if it was a film source with 3-2 pulldown, you definitely want to use inverse telecine and restore the true progressive mode. And with inverse telecine, even if you have a frame that's fully in motion because it's just restitching the original source frames, that's going to work for you just fine.

Next is luma adjustment. And luma is basically but not quite the same thing as brightness. And I recommend you read Charles Poynton's book if you want to care about what the difference is. You can sort of think about video filters in two classes. There are filters that affect luma, brightness, and filters that affect chroma, or color.

And best to think about them separately. And typically you're going to do a lot of work on luma for a lot of cases, but typically chroma, you don't really mess with it very much because it tends to survive the process. And also, we see mainly luma, so that's where it pays off. And the classic luma filters are contrast, brightness, and gamma, or the levels filter in After Effects and tools like that are also just luma.

So this is a complex issue now. If anyone's been doing QuickTime for a while, you probably have it in your head that you have to raise the gamma a bunch when you're encoding on a Mac for PC playback. Anyone doing that? Do you know that rule of thumb? You guys all knew? All right. You know what I'm talking about. All right.

So if you were doing all that, and if you didn't know about that, you can not feel bad because you largely don't have to do that anymore. So there are two classes of codecs in QuickTime, the ones that will correct for gamma on the fly and ones that won't. So for example, if you use the Sorensen Video 3 codec, if you code that file on a Mac and play it back on a Windows box, it'll appear darker on a Windows box than it will on a Mac.

If you use the MPEG-4 codec, it'll appear the same brightness on a Windows box as it will on a Mac because that particular codec will automatically correct for the gamma. This is confusing, unfortunately. So you have to know which codec you're going to do. That's one of the reasons I recommend that if you're doing QuickTime for Mac users, use Sorensen because it's a better codec and all that kind of good stuff.

But if you're really trying to make a QuickTime file for a cross-platform audience, the MPEG-4 codec has the advantage that you're able to, it'll correct for its gamma for it, it won't appear too dark on PC. And it's also possible to use QuickTime Movie Alternates to make different versions of the file with different gamma for Mac and Windows, and Movie Alternates to automatically switch between them.

So you can switch between them on the web, but that's for a different class. So the two, let me kind of walk through the filters there. First, we have brightness and contrast, and these are often grouped together. Brightness basically exaggerates how different the thing is from gray. So you have an exact middle gray, sorry, that's contrast. Brightness just shifts the entire luma range. It just like adds X amounts. You can do a brightness of plus 20, every pixel will become 20 units brighter. So pixel value zero will become 20. pixel value 200 will become 220.

People get in trouble with brightness, because people say, "I want my video to look brighter, so I'll turn the brightness up." And it's actually almost always the wrong thing to do. You virtually never want to actually raise brightness as a filter itself. If you want to have the video seem perceptually brighter, you're going to use the Gamma Filter we're going to talk about next. Because brightness just adds to the entire range, what was black, well, it can't be black.

If you had just one unit of brightness, what was black of zero becomes one, and all of a sudden, your black becomes sort of a dirty gray. So if you're using brightness filters, you're almost only ever going to use it with negative values. And the goal of using the brightness is to make elements that should be black, like white text in a black background for title card, or fade to back, that kind of stuff. You might turn the brightness down a little bit, just so that it becomes all the way down to zero.

Now, typically, we have rendered graphics, like you're rendering out from Final Cut or whatever with a black background. It's going to stay black throughout. You don't have to worry about that. But if you have any kind of analog source, those little luma values for each pixel will be a little bit randomized.

So even if when it was rendered, they're all zeros, it goes out to beta SP and comes back again, you get some zeros, some ones, some threes, some fives, that kind of stuff. So you can just use brightness a little bit, say brightness minus five. The fives go down to zero. The twos still go down to zero.

The zeros stay at zero. And all of a sudden, a noisy background becomes all the way black. So that's a negative brightness, a slight amount, can really help make it crisper and add some more vibrancy to it. And also, if you have a case where you have a black background that's really noisy, by making it really black, I mean, a big rectangle with a number zero over and over again is very easy to compress. If you have totally random analog noise, that's actually kind of hard for a codec. So it actually will encode better by doing that.

Contrast exaggerates how different a pixel is from gray. So at a total mid-gray, contrast has no effect on. Absolute black, absolute white has the most effect on. The closer you get to either black or white, the more effect contrast is going to have. Now, a few years ago, when you're doing encoding with QuickTime for the web, you almost always had to add a plus 27 contrast value in order to get your blacks to come out as black, coming from a video source to an output source.

The good news is now QuickTime handles all that in the background. So even if so, I still see people who are still doing this, and they wind up getting like a double contrast effect, and they get really crushed blacks and whites. So again, with really clean digital content, you normally don't need to add contrast anymore. But that is useful for analog source, because again, that helps push the blacks a little bit more to black.

And if you have a little bit of analog noise, it can make the whites into a flatter white, and that'll just seem a little more vibrant and can encode better. So when I'm using these filters, it's almost always because I'm trying to get my blacks blacker. My general rule of thumb is I'm going to use one unit of brightness for every unit of contrast I'm going to have.

So if I have a choice between using a minus 10 brightness to get my blacks black, or a minus 5 brightness plus 5 contrast, the combination of plus 5 minus 5 will give me the same black effect, but it'll leave whites about the same. So it won't make the images as dark if I use the brightness filter overall.

So I'll take a look at showing some of that stuff later on. But it's a rule of thumb. You don't want to use only brightness or only contrast to crush your blacks. You want to use a combination of them, and that'll leave the rest of the luma range a little more intact.

Other LumaFilter we care about is Gamma. I give them in this order because all truly virtuous processing tools put the GammaFilter after Brightness and Contrast. Because when you laboriously use Brightness and Contrast and your blacked out is zero, you want it to stay at zero. If you have Gamma after that, Gamma's not going to leave it at zero.

You have the GammaFilter before Brightness and Contrast, you wind up having the GammaFilter changes where the zero lands, and it gets much more complex. Anyone out there making compression tools? GammaFilter is the inverse of contrast. It has the most effect at the middle grade and has no effect in the extremes.

Video is the place to start. The complexity here is that Macs by default use a gamma value of 1.8. Video by default uses a value of 2.2. Windows machines use something between 2.2 and 2.5, and it's not really defined. It used to be a big problem to make different files for Mac and Windows for all this kind of stuff.

The codecs inside QuickTime that use what's called the 2VUI color space will automatically correct the local gamma. So if you play back the file, it'll store it in the local gamma. If you play back the file, it'll store it in the local gamma. If you play back internally 2.2, you play it back on a Mac, it'll just assume it's 1.8. Play it back on a Windows machine, it'll assume it's 2.5, and play it back correspondingly.

The complexity is it's not actually reading whatever the gamma value it is. If you've gone into your monitor's control panel and specified a different gamma value, it's going to ignore that value. It just assumes every Mac in the world's a gamma of 1.8. If you told the system differently, it doesn't care.

Same thing with Windows. It has no way to actually get the real value. It just assumes it's a 2.5 value. But still, it's a good thing to have. And so the good news is if you want to make a single file that looks the same on Mac and Windows, use a 2VUI codec, which MPEG-4 is the best distribution option right now.

If you were using Sorenson, you want to make a Sorenson file on a Mac that will play back on Windows right, you do need to add about a plus 30 gamma for it to look identical Next kind of filter we look at is noise reduction. And noise reduction is kind of like a smart blur filter. The idea behind that is to try to take out parts of the image that aren't image, but are noise.

I mean, grain or random analog stuff. Blur those out, but try to keep the actual content like sharp lines, text, all that stuff intact. And these are hard things to do. I mean, even the best algorithms will blur more than you want them to, but it's better than just throwing a Gaussian blur over the entire image.

The way different tools implement this vary a lot. You got, you know, some have like things called grain killer or grain suppress, which can work for some kinds of noise. They have also temporal noise reduction filters. It varies a lot. The thing is, if you have source that's got really bad analog noise, you're pretty much hosed.

These filters can take you from mediocre, from like bad to like nearly mediocre in quality, but you're never going to actually be able to get good quality output by using noise reduction if it's noisy. It's better than nothing, but it's much better to have noise reduction. It's much better to have clean source to begin with, and you can't ever fake clean source. Kind of like you can never make high definition for real out of standard def.

Next up is audio normalization. For the most part, audio does not require a lot of preprocessing, especially if it's something that's mixed for TV or DVD, something like that. That's going to encode pretty well for the web at reasonable bit rates. The one thing I often want to do is audio normalization. If you have a clip where the overall level is just too low, what normalization will do is it will find the loudest single sample, and then it will either raise or lower the overall volume, keeping the dynamic range intact, but just changing the overall amplitude.

to a specified value. And typically, minus 3 dB is good for most modern codecs. Some QuickTime tools use the default about minus 6 dB, because the old QDesign 1 codec had trouble with things that were near peak, but you don't care about that anymore, so minus 3 is just fine these days.

You don't want to go all the way to 0 dB peak, because there are some codecs that will just... When you're codecs at approximation, you can wind up having to try to give a digital value that is not possible to express with a codec, which can give you a little audible distortion. So it's a good rule of thumb to go to minus 3 dB.

You might use some other filtering. And typically, these are kind of going beyond the realm of audio processing and preprocessing off to like, you know, you're doing some kind of cool audio work to clean up bad source audio restoration stuff. Doing compression, basically making the quieter parts of the audio louder, or even the loud parts loud. You're targeting a 3GPP kind of playback device. A playback on a cell phone, you know, subtle dialogue is not going to be audible for most people. So often, limited dynamic range there can help a lot.

Notched filters, we've got like hum and that kind of stuff in the background can work quite well. Same with noise removal. Your compression tools aren't going to have this kind of filtering in them for the most part. So if you have clips you need to apply this kind of stuff with, you're diving into some kind of pro audio tool.

[Transcript missing]

This is the interactive portion of it. We've got about 36 minutes left here. I've got FilmSource, I've got NTSC, I might even have a couple of Palo Clips here. I've got every compression tool known to humanity. Anyone have a preprocessing project they're working on with a particular tool they want to see how to do? Or that kind of stuff? Someone got a question for me? Have you demonstrated one of these techniques? Some of this will be on microphones. You mean, if we go to the microphone back there, so we can get it recorded in audio?

Hi, I'm Scott Thompson from NewTek, and I'd just like to see some DV source, maybe, recompressed down to maybe 320 by 240. Okay, do you have a picker tool you use for doing that kind of processing? Nothing in particular. I use MediaCleaner once in a while if I do that, or something like that. Sure, we can dive into Cleaner. This is Cleaner 6. I don't know if anyone from Discrete's been around to the show at all.

We haven't had a new... It's like a beta of Cleaner 6.02. It's about a year old now, but that's the last thing we saw of it. I've actually had a real release of Cleaner, even a point release, for over a year now. I fear Cleaner may be done, but it's still useful for a lot of stuff. They actually have a pretty good design for tram preprocessing off.

This clip here is called NASA.move. It's a pretty interesting one here.

[Transcript missing]

It has blanking on the edges here. See the little black line there a little bit? And it's also letterboxed. So, first thing we want to do is we want to be able to crop out that stuff.

So, this is a 4x3 source. If it was mostly just a 720x480, we'd get an image like that. When I say about cleaners, we could tell it to, okay, show it to me as a 4x3 source clip, and it'll show it to me correctly. If it's 60x9, you could flag it like that as well. I'll need to assign a preset to it.

First thing I'm going to do is crop it. This little crop filter here. All I need to do here is grab it and draw a box.

[Transcript missing]

"It's like some archival footage. Often you'll have things where it's the frame will vary a little bit. There's a spatial. See right here in this frame, even though all the other frames are good, there's actually a little bit more letterbox in this frame here or there. So I actually have to go a little bit tighter here.

One thing you have to watch out for, and I'm going to see if this monitor is off at the very edge, you see a little bit of distortion. You can see that the first line of video is a little bit wider than the line before it. So you don't want to crop at least one pixel from the top and bottom just to get outside the distortion range. It'll look a little bit funky. Your top and bottom lines are often a little bit off. So here we've cropped in about one or two lines from the start of it. And that's grabbed us our whole thing there. All right, so.

I'm cropping unconstrained because I didn't-- yeah, you can also just do a 16 by 9. If I was going that to DVD, I would definitely do a 16 by 9. But I don't know if this is quite-- yeah, it's about 16 by 9 there. Yeah, you can do it either way.

So in this case, if you know you want output to exactly 16 by 9, you can set it in that value. Or also, you can even do a custom aspect ratio if you happen to know it's a particular thing. In this case, we know it's 16 by 9. And we can just do a box like that.

and this is one of the cropping out a lot of other frames that we don't need to. Most compression tools only let you do global settings, you have to do setting overall. If you really need to do a lot of tweaking per thing, I tend to dive into After Effects and do all those filters there, if I need to do different processing on a per frame basis. Which ends up being pretty labor intensive and expensive, but you know. So that's how we set the crop up, and the processing side is pretty straightforward.

Let's just say I'm making a I would say, first, deinterlace. I want to make sure I have the adaptive mode on. That gives me the adaptive deinterlace. Image size, 320 by 240, like that. Cleaner, by default, has a sharpen filter turned on. You do not want to use a sharpen filter for most preprocessing. Sharpen makes it a little crisper before you encode it, but sharpen adds noise as well.

It will typically give you more artifacts on playback because it will tend to exaggerate any noise as well as make edges sharper. So always leave sharpen off. The adaptive noise reduction on Cleaner is pretty good, so you can leave that on. Also, you have these filters here. Look at gamma, brightness, and contrast. The default settings are somewhat random. They were designed for Cleaner 5, and Cleaner 6 is a different processing mode for how it does video. So actually, you wind up having different values. It's a great little utility you get. Let's see what we can get with Mac OS X.

I use all the time for this kind of stuff. It's called Digital Color Meter. If you've got Mac OS, I think it's from 10.1 on. It just comes free with the OS. and what it does is you just point at a place in the screen that will tell you the RGB value of that point.

And Cleaner really ought to incorporate this built into it. It would be very useful, but it doesn't. So what we can do here is we can just go over here and look at our values and see how they look and all that kind of stuff. And we can say, like, let's pick a frame like this.

[Transcript missing]

This is a preview window here.

Hello, did I not? Where are we? What's the problem here? Quick time. Oh, I didn't apply it, yes. I've not been using Cleaner as much lately. It's kind of... It's like sort of the dominant tool of my career for a long time, but it's getting so buggy these days, it's kind of hard. I wind up not using it as I used to here. Well, I had applied, didn't I? Maybe it was one of the bugs I was talking about. Yeah. Setting default.

[Transcript missing]

1 to 0 is around. OK, let's go to 10. Dash goes pretty much there.

[Transcript missing]

You're going back to and making sure it's going to look at further frames. Because Cleaner and other tools like Compressor only give you a global setting. Did you forget my crop box now? You're just killing me here.

Oh, right. We're not going to do 13 by 240 at all, of course, because we're doing 60 by 9. It's actually 320 by 180. there. Confuse myself by giving you a-- OK. Right, that's for sure there. And before and after here, the A/B slider shows the effects of image processing.

So, right there we're seeing the effects of having thrown in that brightness and contrasting. You can see it's a little bit darker overall. Ideally, if you're doing image processing, it shouldn't feel like you're modifying the video. It should feel like you're kind of peeling a layer of grime off it. It's the effect you're trying to go for. It shouldn't seem overly dark and that kind of stuff.

So, before and after, too bad. It's a little bit darker. It gives a little more richness to it. The original video had kind of weird black levels. All right. Looks pretty good like that. and then so I'm pretty happy as image settings there and then the audio side of it just do a.

"Normalized 90% and 90% in cleaners about minus 3 dB are good to go. And that's pretty much all we need to do to do a preprocessing in cleaner. So, did that look like what you're talking about? Any questions about that or specifics there? Alright, cool. We got a question over there? Yeah."

We are aware of the normalized function in Cleaner. We're also aware of it in Logic, for example. But we have a lot of video coming in to Final Cut Pro, and our users do not really know how to handle normalization. I'm getting some echo right there. Can we get a little closer to your microphone? Yeah. Are you aware of a plug-in available, an audio plug-in for normalization that we could plug into Final Cut Pro? Hmm. Because we have a lot of dirty audio coming in and video, and our users don't really know how to handle normalization manually.

Hmm. I can't imagine that someone hasn't done one of those, but I can't, off the top of my head, name one. Does anyone know of a normalized plug-in for Final Cut? Say that again? Waves. Oh, Waves. Yeah, Waves. But is it really a normalized plugin? You know, like as simple as this? Because Waves is usually too good. Too many variables. Yeah. Yeah. Yeah.

[Transcript missing]

Daniel Benner, University of Texas What are some best practices for importing the video in Final Cut without having to render all the time? You've got SVHS tape. It's pretty straightforward. I'm in love with the AJA.io systems. For a grand or so, they have a full one that is SDI and analog. You can buy a $1,000 one that's small and analog.

You can get an AJA.io that will take your S-video input and your XLR audio. If you have a professional or industrial SVHS deck, like I've got an AJ555 Panasonic, and that's a device controllable XLR audio out SVHS deck. Plug that straight into the AJA. It'll put the firewire into your Mac to run Final Cut, and it will be able to give you device control, 10-bit uncompressed capture, balanced audio off your SVHS tape into Final Cut. And then, you know, just in a G5, you'll have multiple real-time effects on the G5 just in software from that capture using the uncompressed codec.

Would it be bad to take the S-video from a S-video deck and then go into the back of a DV deck and then capture it via firewire that way? Yes. Yeah. Because the DV codec is only 25 megabits. It uses only a very limited 4:1:1 color space. DV is a fine acquisition format.

I mean, if you're just like shooting things in the world with DV, it's fine. But when you try to convert anything to it that's already got analog noise in it or if it's got motion graphics, that kind of stuff, the DV codec's really not robust enough to be the second generation of anything.

So, I mean, yeah, if you do it, it would work. You'd get video out of it. But the red I'm talking about gives you a substantially higher quality. And also, if you have a good VHS tape and you have something with a good time-based corrector, and all those analog things which most of us have unfortunately been able to forget about come back into play, right? I mean, you spend a bundle on cables and proc amps and all this kind of stuff.

But it's not too bad. I mean, in the AJA system, actually, I'm very pleased that it's like, you know, a straightforward, cheap, you can plug it into a laptop, all that kind of stuff, set up for doing that kind of work. Does that convert it to DV? No, it can if you want. It can convert it to DV. You can leave it as uncompressed, 8-bit or 10-bit. It can convert to DV50, convert it to DV25. Whatever you want. And motion JPEG, I assume. Thanks. Great.

Hey, François Nenoune-Machère, Capgemini. I'm using a cleaner. I've been using a cleaner for a lot of time, but I'm a bit concerned about its future. Yeah. What kind of product, equivalent product, would you advise? The two actively, the products that have active engineering on them for Mac encoding tools.

Obviously, Apple's got Compressor, which I'm not sure if it's... Compressor's pretty good for some stuff. It's got some limitations. It can't do like two-pass VBR with Sorensen encoding and that kind of stuff. As the 264 codec becomes dominant in QuickTime over next year, I would expect that Compressor will be relatively more useful because it'll have access to a codec that's a lot more competitive than it does right now. Sorensen Squeeze is a major, major new version that's been announced. It's been in beta. It should be in a few months.

Squeeze 4, pretty much a whole new tool really aimed squarely at the cleaner space. And Sorensen's working hard to make that work. And they've got 264, and they've got all kinds of stuff in there. And that should be, that looks probably promising as well. And a few months. And there's Popwire's Compression Master. That's out now. It's really good. It's mainly an MPEG-4 tool. It can make great MPEG-4 inside .mov files and MPEG-4 files and also 3GPP files.

For doing 2Paths and all that kind of stuff. So if you want to use the MPEG-4 codec and almost any flavor of it, Compression Master is my favorite tool right now. But if you want to make really good .mov files, Cleaner is still the best thing out there. It's got some unique features only it has.

As far as being able to peak data rate for 2Paths VBR. It can automatically do an audio sync fix for B-frame content, that kind of stuff. So I mean, I still use, I expect as long as I need to create legacy Cleaner content, I mean legacy QuickTime content, I'm going to keep on using Cleaner for keeping around on the hard drive for years to come. Even though it seems unlikely at this point they'll ever see any more releases for it or even bug fixes. I don't know.

I mean, Discrete says they got some engineers working on something related to Cleaner, but they won't say what or how or even which version of it and that kind of stuff. And the beta of 6.0.2 came out almost a year ago. And they haven't even like released it for real yet. It's still in beta for all this time. So that's required for Panther compatibility.

So we're talking about Tiger now, so it's not a very good sign. So, um... Yeah, so we're getting there. I just had a comment about the VHS to DV. There's also the early Sony decks and things that have head-to-head transfers from cableless and it has TBC and some other things on it too. So it actually does some sweetening of the signal as well.

Yeah, so there are ways you can make it work with DV. Just the DV bitstream itself, I don't... I mean, if you're on a budget and you just need the video in any form, it can work. But if what you care about is really high quality, that's a limitation. So I actually got a project here called VHS Ugly File, which I decided like, what is the worst nastiest analog garbage you can imagine? Which is hard. I had some guys, like a fourth generation EP mode VHS dub.

[Transcript missing]

I can talk a little bit about what we can do to make this thing better, which is not a whole lot.

[Transcript missing]

One thing about video that's important to realize is that no matter how bad the frames are, the motion is always really good. Even if it's a really bad, horrible generation of SVHS thing. So the thing you really need to try to do it is, when you have bad quality video like this, is make the frame size small and keep the frame rate high.

Because you've got 60 fields a second of motion here, so make a 320x240 60 frame a second video out of it. Because that'll give you, you know, you shrink it down a lot, it'll help average out some of that stuff. So let me drop into After Effects and I'll show you what you could possibly do to make this interesting clip.

After Effects is overkill for a lot of compression stuff, but you need to do weird kind of video processing stuff. It's still kind of like a Swiss Army Knife tool. The new version is pretty good here. It actually comes with the Synthetic Aperture Color Finesse plugin, which is kind of beyond preprocessing. If you need to take stuff that was shot badly and really clean it up, it's a really wonderful plugin. It's also available for Final Cut, I believe. They're not bundled. Let's open up this horrible piece of tripe here.

[Transcript missing]

Preserve edges, best quality only means adaptability interlace. So it's not a clear defined thing, but if you are using A for preprocessing, you definitely might have that turned on. You also do 3-2 pull-down removal in a very nasty UI here where you have to guess it.

And 6-5 is better in the past, but After Effects totally cannot deal with any kind of thing with a cadence break. So you have a two-hour movie, and there's like one field that's off in order in the middle of it, it can't do it versus tele-sending. So, each, the entire clip you work with has to have a perfectly straight cadence throughout the entire thing. You guessed it? No, it doesn't have any. Let's see if we can see something like that. So let's say I was going to make a really small version of this. I could just make a 640x480 timeline, it's not a bad place to go.

[Transcript missing]

So when I've done that, I'm actually able to produce an output that's really all the way there. So there are two ways you can handle doing things in After Effects. You can do it a full res in the comp, and then you can do a nested comp at the final output resolution or set it on the output resolution. It's often most informative to actually make your comp the size you want to have it at.

So if you're making a little web video thing like that, work it at the lower size. You just need to go through and obviously deal with the scale right. And scaling in After Effects is kind of weird because you can't really crop per se. You just kind of scale and position it right to give you the crop you want. So like, well, 50's not good enough.

I'll do 51. Another thing about this clip is it's got what's called tearing in VHS mode. All this stuff at the bottom is kind of not all to an angle here. When you play it back it just looks stupid. When you're in VHS you want to crop all that stuff out of the bottom.

So I'm going to zoom in a little bit here. So we'll just scroll down a little bit. Pretty straightforward. And we'll get something like that. The feature that people don't steal enough from After Effects is the all-important levels filter. This is probably the best visual processing filter in the history of humanity here, because it gives you an integrated histogram of black and white points. So you have a video clip that's not all the way to black or all the way to white.

You're actually able to just see the histogram. Like, okay, this clip here is full range. It's not a problem. But you can, okay, my mid-tone, you know, like that, all that.

[Transcript missing]

But generally for kind of video like this, often the luxury gamma can do you really good. So just a little 1.1 in gamma.

is a little more presence. It depends on the clip here. So we've got what we can there. Now, yeah, there's really little you can do on this to make it look any good. But actually, 100% size there is scaled down. It isn't that horrible. It really isn't as horrible as you could imagine.

I just got to do a little preview there. And the one part you have is you do have a lot of difference per frame in terms of temporal noise there, just because it varies from frame to frame. You can sometimes get some better results by using, you've got the pro version, your remove grain filter, some kinds of video noise you can actually do a semi-decent job with.

I won't belabor you the incredibly complex set of things you can do on it, but... You can see sort of inside the preview box there, you get a little bit, little bit less effect there. And you get tuned in more. And where that really comes in is because grain is random for every frame, but using the grain suppression filters, it's going to try to find errors that are totally different frame to frame and suppress those, leaving what the actual underlying motion is. And again, it's not going to be perfect, but it'll give you a little bit better quality and take out some of those errors there.

This session will give me a full 60 frames per second output. So if you do have that old content you're trying to put on the web, emphasize the frame rate, because you've got so much frame rate, and just shrink it down to the point where the artifacts kind of disappear where you can. But also, I mean, it's always going to be garbage in and garbage out.

For a really bad source, you could make it, again, make it less bad, but even mediocrity is typically out of reach. Unless you're going to go in and rotoscope the whole thing. Treat it as a source to paint over, effectively, is the only way to do it in a lot of cases. Okay, next question. Is it up there? Yeah, come to the microphone.

Hi, Ben. It's Cliff Wooden here. Just a quick question. You mentioned MPEG-4 a couple of times, but were you meaning MPEG-4 Part 2 there, and specifically using H.264 to refer to Part 10? Yeah, so... My parlance right now is I say MPEG-4, I mean MPEG-4 Part 2, and if I mean MPEG-4 Part 10, I'll say 264.

That'll probably change over time as kind of MPEG-4 becomes, five years from now when we say MPEG-4, we're going to mean 264. Because Part 2 never really got all that much traction, so clearly the entire MPEG-4 industry is waiting for 264, and that's going to be the mainstream implementation of it, because it's just so much better. But today, QuickTime is only Part 2, Part 2 simple. So that's what we're using for stuff right now.

I didn't get the fields right there. Let me turn this off. Because it's distracting when I play back video when I'm trying to talk. Yeah, the real... Now, one thing to bear in mind is that the... Personally, Apple people, QuickTime's built-in MPEG-4 encoder is, to put it charitably, more speed-optimized than quality-optimized.

So, even if you want to make a .mp4 file or .mov file with MPEG-4 codec, there are other tools like Squeeze and Compression Master that will give you a lot better quality at lower bit rates than Apple's encoder. So, even if people have had kind of a... questions about MPEG-4's compression quality, there are other tools making compatible bitstream that can give you better results. Both Squeeze and Compression Master have a two-pass encoding mode.

You can tell it to go really slow and really high quality, while the QuickTime encoder is really tuned to just, like, you know, massive action in real-time, real-time broadcast kind of applications, and it works great for that. But it doesn't have a slow and sweet mode, which some third-party products do. So, you can make a QuickTime-compatible MPEG-4 file with a lot better quality than a lot of the stuff we see out there right now. You just use the Apple exporter.

Okay, who's next? Someone's got someone. I'll just have to do some demos otherwise. Anyone want to see some stuff in Compressor or other stuff? Oh, here we go. Can you show an example of using a mask to do some specific compression where you're masking out? A mask? Give me an example. Yeah, maybe I'm not saying that correctly. But to soften a portion of the image field and compress it differently. That almost winds up, it's almost never worth it in the end. Because you're trying to do compression, you have multiple frames, you have an acycle image.

I mean, you can use motion tracking to like, yeah, I honestly haven't had a case of a video where that was worth doing for years. Because typically, any kind of error in it is going to be overall. I mean, let me think. Sometimes I'll do per frame processing. But I'm actually doing a masking for it.

A good example might be if you had a video of a talking head and they shot it against a moving background like leaves blowing in the wind or something. Oh, it's a constant shot. You might use After Effects to mask the person out and blur the background so it looked like it had a shallower depth of field or something. Yeah, I mean, you do that like you would do an After Effects play.

Well, actually, with After Effects, I mean, I did have a case last year where I was doing this high def project where I needed to, where it actually was a damaged D5 tape. So there were some, they actually would have macro block errors sometimes. I don't like the, so like, you'd have like a 16 by 16 block of the frame where the video wasn't, you know, We're basically the only one field to be intact kind of stuff.

And actually, the 6.5 actually goes into rotoscope. So, I mean, you can go in and hand paint and use the Clone Stamp tool on a per frame basis. And it's surprisingly powerful. I mean, like, 6.5 on a dual G5. I mean, you can do really good real-time rotoscoping kind of stuff with it. So, that's definitely in there, I'm sure.

I use 5% of After Effects a lot, so I don't really do much mask stuff. I have a question. I was wondering if you could pontificate on this. As content creators transition to using HD and much higher quality cameras and acquisition, like the Vericam working in 24 progressive square pixels, giving us much higher quality content to start with, will we be out of a job?

Preprocessing in HD is much easier than the standard definition. Because HD is always analog, I mean sorry, always digital. Analog is half the problem with preprocessing and non-square pixels is a big issue as well. And DV is almost always square pixel digital, which makes it a lot easier. But you still get some complexities. You know, there's converting from 720 to 1080.

Computer playback. I mean, everyone who's showing off high-def computer-based playback today, it's all 24p. So if you have 60i source, typically you have to get converted or something like that. Today's codecs don't do a great job. Computer codecs don't do a great job with interlaced content. MPEG-2 will work, but it takes a pretty beefy machine to do real-time MPEG-2 deinterlacing on computer screens to 60p playback, and it's a nightmare. So it's still a fair amount you need to do with it.

Often, sometimes you get a little bit of letterboxing, you know, if it's more than 60 by 9 aspect ratio. It's just occurred to me that the difference between what a less experienced user might get from compressing a really high-quality source from HD down to web-based. With, you know, default settings in some of these tools versus what we might be able to get, that difference is much less with much higher quality source. Yeah, there are subtleties. There's also HD and SD use different color spaces. Bear in mind.

So HD uses the 709 color space, and SD and almost all computer playback uses 601. So you actually have to do a transform in there to get the colors to come out matching perfectly accurately. And it's handled transparently in a lot of tools, but not always. You have to get that right as an issue.

Another thing that happens is typically you are going to, a lot of HD for computer play gets compressed horizontally. So you encode at 1440 by 1080, just to make it a little bit easier to do, and some stuff going on there. But yeah, I mean, HD is massively easier to compress than standard def in the real world.

You just need a lot more computer for it, but, you know, that's coming along as well. But yeah, I mean, it's amazing to me. The part is, I only work on hard stuff pretty much, but I mean, so I've had some HD projects require a lot of pre-processing, but kind of in general, yeah, you just kind of like drag it in and say go when you're done. It's, it's, it is almost always progressive, and it is almost always full frame and square pixel and all that kind of stuff.
