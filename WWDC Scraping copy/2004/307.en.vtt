WEBVTT

00:00:12.350 --> 00:00:16.190
Ladies and gentlemen,
please welcome performance

00:00:16.270 --> 00:00:18.570
engineer Nathan Slingerland.

00:00:23.520 --> 00:00:26.870
Ladies and gentlemen,
please welcome performance

00:00:27.700 --> 00:00:29.780
engineer Nathan Slingerland.

00:00:53.500 --> 00:00:55.580
we've got in Shark 4.

00:00:55.670 --> 00:00:59.810
So, and then of course, if you've seen,
some of you may have

00:00:59.810 --> 00:01:04.260
seen the demo on Monday,
in the DevTools keynote.

00:01:04.260 --> 00:01:08.330
We want to go a lot more in
depth with what we did there to

00:01:08.330 --> 00:01:11.760
optimize Celestia using Shark.

00:01:11.870 --> 00:01:13.860
So let's go.

00:01:14.320 --> 00:01:15.050
So what is Shark?

00:01:15.290 --> 00:01:19.240
Well, it's been called Apple's
best-kept secret,

00:01:19.420 --> 00:01:22.020
but not anymore.

00:01:22.020 --> 00:01:25.960
Starting this year, in this year's WWDC,
we've had a big push for this,

00:01:26.030 --> 00:01:29.490
and we want you all to know that
this is a great way to find and

00:01:29.490 --> 00:01:31.670
fix your performance problems.

00:01:32.190 --> 00:01:34.180
So, what is Shark?

00:01:34.350 --> 00:01:37.560
Well,
it's a really easy-to-use profiling tool.

00:01:37.560 --> 00:01:40.980
There's a GUI version,
as well as a command line version.

00:01:40.980 --> 00:01:43.700
The command line version is
very useful for scripting

00:01:43.720 --> 00:01:47.230
performance regression testing,
so you can archive your

00:01:47.230 --> 00:01:49.880
performance analysis,
later review it,

00:01:50.040 --> 00:01:54.900
see how you've made progress as you
continually optimize your application.

00:01:54.970 --> 00:01:57.170
So what is Shark?

00:01:57.180 --> 00:02:00.370
Well,
it's a really easy to use profiling tool.

00:02:00.470 --> 00:02:03.760
There's a GUI version as well
as a command line version.

00:02:04.700 --> 00:02:09.200
And it runs on Panther and Tiger,
so that means, yes,

00:02:09.340 --> 00:02:13.490
you can use it on your
development machines right now.

00:02:14.550 --> 00:02:17.760
And of course we support
basically any kind of executable:

00:02:17.760 --> 00:02:21.880
Mac OS, CFM,
compatible with all the compilers

00:02:21.920 --> 00:02:27.350
that you can use in Mac OS X,
XSIM, debug format, stabs format,

00:02:27.470 --> 00:02:31.230
so whatever you can throw at it,
Shark can show you what's going on.

00:02:32.490 --> 00:02:34.400
So let's see how easy it is.

00:02:34.400 --> 00:02:37.680
What do you do to get started with Shark?

00:02:37.840 --> 00:02:39.200
It's just one click.

00:02:39.210 --> 00:02:41.330
There's just this main
window with one button:

00:02:41.480 --> 00:02:42.040
Start and Stop.

00:02:42.200 --> 00:02:45.100
I mean,
this is the typical way that you start

00:02:45.100 --> 00:02:49.560
profiling and stop it with Shark,
and you're on your way.

00:02:49.600 --> 00:02:54.120
By default, we select a time profile,
but we also supply a rich set

00:02:54.310 --> 00:02:56.820
of other preset configurations.

00:02:57.050 --> 00:03:00.200
You can also create your
own configurations for doing

00:03:00.200 --> 00:03:02.350
customized performance analysis.

00:03:03.040 --> 00:03:06.900
By default,
we're going to profile the entire system,

00:03:06.960 --> 00:03:12.260
but some profiling configs perhaps make
more sense to target a specific process.

00:03:12.500 --> 00:03:16.890
So, for example, malloc tracing,
which we'll talk more about later,

00:03:17.120 --> 00:03:19.500
you want to pick a target process.

00:03:19.500 --> 00:03:22.760
Or static analysis,
you want to pick a particular

00:03:22.760 --> 00:03:24.780
executable file to look at.

00:03:24.800 --> 00:03:26.800
But for the most part,
you don't even need to go here.

00:03:26.800 --> 00:03:29.710
You can just click the
start button and go.

00:03:29.880 --> 00:03:32.920
So once you've selected
your configuration or just

00:03:32.920 --> 00:03:35.310
left it at the default,
probably you're going

00:03:35.310 --> 00:03:36.560
to do a time profile,
right?

00:03:36.560 --> 00:03:38.240
So what is that?

00:03:38.940 --> 00:03:40.560
This is the most common workflow.

00:03:40.650 --> 00:03:43.480
Basically,
your time is just like optimizing

00:03:43.480 --> 00:03:44.800
application performance.

00:03:44.800 --> 00:03:48.800
You want to spend your time where you can
affect application performance the most.

00:03:48.800 --> 00:03:51.880
So that's whatever functions
take the most wall clock time,

00:03:51.880 --> 00:03:55.500
or whatever pieces of your code are
taking the most wall clock time.

00:03:56.400 --> 00:03:59.710
So we're using—for time profile,
this is a statistical form of sampling,

00:03:59.760 --> 00:04:03.900
and we're taking, by default,
a thousand samples per second,

00:04:03.960 --> 00:04:07.870
and each sample contains backtrace,
so how you got to a particular

00:04:07.960 --> 00:04:12.540
location in your code,
which process, which thread, et cetera.

00:04:12.620 --> 00:04:14.740
So all this information,
and we collect that,

00:04:14.930 --> 00:04:17.070
and when we combine it,
we can create a profile.

00:04:17.300 --> 00:04:21.220
So this is going to show you,
over the sampling period,

00:04:21.280 --> 00:04:25.340
where you were spending time
in a given process or thread,

00:04:25.340 --> 00:04:26.270
or overall in the system.

00:04:26.300 --> 00:04:27.380
Thank you.

00:04:27.800 --> 00:04:28.660
And we're capturing everything.

00:04:28.830 --> 00:04:33.900
So this includes the driver, any drivers,
any kernel code that's executing,

00:04:33.900 --> 00:04:35.100
applications.

00:04:35.160 --> 00:04:36.880
So no matter what kind
of developer you are,

00:04:36.900 --> 00:04:39.660
this can help you profile your code.

00:04:39.660 --> 00:04:42.440
And we're able to do this because
we also have a kernel extension,

00:04:42.440 --> 00:04:42.820
right?

00:04:42.960 --> 00:04:46.140
So we're sitting in the kernel
capturing these samples,

00:04:46.140 --> 00:04:47.740
and this gives us very low overhead.

00:04:47.740 --> 00:04:51.180
We really don't perturb the code
in the system hardly at all.

00:04:52.810 --> 00:04:55.900
So once you've got a profile,
you've hit start, you've hit stop,

00:04:55.950 --> 00:04:59.500
and you run whatever it
is you're interested in,

00:04:59.510 --> 00:05:01.940
you're presented with this by default:
the heavy view.

00:05:02.030 --> 00:05:03.160
And this is really the bottom line.

00:05:03.300 --> 00:05:05.690
This is showing you,
starting from the top,

00:05:05.790 --> 00:05:10.300
the most sampled to the least sampled
function in the selected process.

00:05:10.400 --> 00:05:13.700
By default, we select the heaviest
process on the system.

00:05:13.800 --> 00:05:15.480
So most of the time, this is your code.

00:05:15.590 --> 00:05:18.490
This is your application that
you're trying to look at.

00:05:20.380 --> 00:05:23.180
So that showed you where—the heavy
view showed you where the hotspots are.

00:05:23.240 --> 00:05:26.420
The tree view is going to show
you where the hot code paths are.

00:05:26.530 --> 00:05:30.170
So how did you get to
those important functions?

00:05:30.280 --> 00:05:33.220
Right, so that's just this tree
button here at the bottom.

00:05:33.760 --> 00:05:36.400
And new in Shark 4,
heavy entry simultaneously,

00:05:36.510 --> 00:05:39.970
so you can use one to help
you navigate the other.

00:05:43.470 --> 00:05:47.040
So those are two ways we
use very commonly to look at

00:05:47.040 --> 00:05:49.720
high-level problems in our code.

00:05:49.720 --> 00:05:51.900
The third way is the enhanced chart view.

00:05:51.900 --> 00:05:53.520
So this chart view has
been there a while,

00:05:53.520 --> 00:05:56.790
but it's been greatly
enhanced in Shark 4.

00:05:56.880 --> 00:06:00.190
Basically, what we're looking at is call
stack depth—that's on the

00:06:00.190 --> 00:06:03.320
y-axis—versus time on the x-axis.

00:06:03.320 --> 00:06:06.730
And this can give you a really
good idea of a couple things.

00:06:06.800 --> 00:06:08.920
First of all,
it can show you patterns in your code,

00:06:08.920 --> 00:06:09.230
right?

00:06:09.320 --> 00:06:11.800
So patterns in the
execution of your code.

00:06:11.800 --> 00:06:14.690
And this can be good to look
for repetitive behavior,

00:06:14.930 --> 00:06:17.180
and we'll talk more about that
later in relation to malloc

00:06:17.180 --> 00:06:20.800
tracing and function tracing,
which we'll get to.

00:06:20.800 --> 00:06:23.990
It also is a good way to
look for—if you see thin,

00:06:24.000 --> 00:06:27.150
narrow spikes in this chart view,
that usually indicates that

00:06:27.150 --> 00:06:30.130
you're spending a lot of time
in function call overhead,

00:06:30.130 --> 00:06:30.550
right?

00:06:30.550 --> 00:06:32.810
Because you had to call
many function levels deep,

00:06:32.970 --> 00:06:35.880
but you're not actually doing all
that much work once you get there.

00:06:35.880 --> 00:06:38.860
So this can be—we see this
a lot in object-oriented,

00:06:38.860 --> 00:06:41.760
modern— you know, layered code,
where we just—we have

00:06:41.760 --> 00:06:43.880
everything hidden in layers.

00:06:43.920 --> 00:06:47.040
And so sometimes that
can hurt performance.

00:06:47.040 --> 00:06:49.710
So for the colors here,
blue is the user space.

00:06:49.790 --> 00:06:53.100
You may see there are a
few maroon-colored stacks.

00:06:53.100 --> 00:06:54.300
Those are kernel.

00:06:54.300 --> 00:06:56.890
And the yellow call
stack is what's selected.

00:06:56.990 --> 00:07:00.350
So that's what's being shown
at the right-hand side there

00:07:00.350 --> 00:07:02.820
is the current call stack.

00:07:04.590 --> 00:07:06.160
Lightness doesn't mean
anything except for layers,

00:07:06.300 --> 00:07:09.850
so it's just indicating,
as you see these gradients in color,

00:07:10.020 --> 00:07:13.900
each color change is indicating
a new function vertically.

00:07:16.560 --> 00:07:20.210
So those are the three main
tools that Shark is providing

00:07:20.210 --> 00:07:22.650
for looking at your application.

00:07:22.740 --> 00:07:26.930
But a lot of you probably are working
on some pretty sophisticated code bases,

00:07:26.940 --> 00:07:29.540
and you have some complex
function call behavior,

00:07:29.550 --> 00:07:32.300
and you really would like
to—you want to understand the

00:07:32.300 --> 00:07:33.510
performance of that application.

00:07:35.030 --> 00:07:39.350
So, Shark 4 has introduced a lot of new
features to try and help with high-level

00:07:39.450 --> 00:07:44.300
analysis and understanding these more
complex application architectures.

00:07:44.300 --> 00:07:49.320
The first is this color by library,
and what this is going to do

00:07:49.430 --> 00:07:52.040
is just going to color each
different module that your program

00:07:52.040 --> 00:07:55.270
is using—so different frameworks
or libraries—a different color.

00:07:55.440 --> 00:07:58.920
And it remembers the color and it's
consistent between the different views.

00:07:58.920 --> 00:08:01.190
And this is an easy,
really quick visual way to see the

00:08:01.190 --> 00:08:04.450
difference between the modules,
visually to see where you're

00:08:04.570 --> 00:08:05.860
spending time in what modules.

00:08:08.850 --> 00:08:14.150
Okay, so another common problem you
probably will see in your code is,

00:08:14.160 --> 00:08:17.730
this is a tree view,
and you may see that you have this

00:08:17.730 --> 00:08:22.510
important--this intermediate function,
this interior function, for example,

00:08:22.510 --> 00:08:26.350
B in this example,
and it's--you'd like--you see it

00:08:26.350 --> 00:08:29.370
in two different spots in the tree,
right?

00:08:29.420 --> 00:08:32.210
And you'd really like to know
how much time total is being

00:08:32.300 --> 00:08:36.760
spent in B and its children,
right, because we tend to subdivide work.

00:08:36.760 --> 00:08:42.770
So in this example, we have B calling C1,
C2, and C3, and not a whole lot of time

00:08:42.770 --> 00:08:44.940
is being spent in B itself.

00:08:44.940 --> 00:08:49.460
So what we've added is a canonical view,
and if you select that,

00:08:49.460 --> 00:08:52.840
we're going to see not just--you know,
in the tree view,

00:08:52.840 --> 00:08:54.960
normally we see main calls--Fu calls bar.

00:08:55.040 --> 00:08:57.110
Well, in this case,
we're going to see all the symbols,

00:08:57.180 --> 00:08:59.600
all the interior symbols,
every symbol that shows up,

00:08:59.830 --> 00:09:01.940
and its total contribution, right?

00:09:02.120 --> 00:09:04.890
So in this case,
B and what it calls total up

00:09:04.980 --> 00:09:06.960
to 100 percent of the time
where you're being spent.

00:09:07.010 --> 00:09:09.780
So this is a good way to to understand
how much time you're spending

00:09:09.780 --> 00:09:12.250
in a function and its children.

00:09:14.730 --> 00:09:20.260
So that kind of raises this question,
right?

00:09:20.390 --> 00:09:23.940
Traditional profiling tools tend
to look at performance just like

00:09:23.940 --> 00:09:28.020
the same way that the program
is broken down into functions,

00:09:28.020 --> 00:09:28.680
right?

00:09:28.700 --> 00:09:31.460
And that works pretty well
when you have a few functions

00:09:31.460 --> 00:09:32.770
where you spend a lot of time,
right?

00:09:32.840 --> 00:09:34.160
A few heavy functions.

00:09:34.230 --> 00:09:36.920
But if you have just a lot
of functions in your code,

00:09:36.920 --> 00:09:40.620
you've subdivided the work a lot
and you're reusing code everywhere,

00:09:40.690 --> 00:09:43.380
or you're spending a lot of
time in system libraries,

00:09:43.410 --> 00:09:45.250
that really isn't the best presentation.

00:09:45.260 --> 00:09:47.300
And that's what data
mining allows you to do.

00:09:47.300 --> 00:09:51.380
It's going to allow you to customize
how you're looking at a profile.

00:09:51.380 --> 00:09:54.050
And by customizing it,
you can help ease your

00:09:54.050 --> 00:09:55.870
understanding of it.

00:09:56.730 --> 00:10:00.650
So Shark provides a couple of
basic data mining operations.

00:10:00.820 --> 00:10:03.810
The first is exclude symbol or library.

00:10:03.890 --> 00:10:06.430
And what this is gonna do,
it's gonna hide a symbol

00:10:06.430 --> 00:10:08.280
or library that you select.

00:10:08.290 --> 00:10:10.250
And so we're not just
taking the samples out.

00:10:10.330 --> 00:10:11.200
It's important to remember this.

00:10:11.200 --> 00:10:13.030
We're not just removing these samples,
right?

00:10:13.130 --> 00:10:18.330
So if you hide the function foo,
any samples that landed in foo are just

00:10:18.330 --> 00:10:20.170
being assigned to whatever called it,
right?

00:10:20.270 --> 00:10:22.750
So if you have a piece of
code that calls square root,

00:10:22.750 --> 00:10:26.760
for example, and you exclude square root,
it's gonna just assign

00:10:26.760 --> 00:10:28.740
the time to that function,
your function, right?

00:10:28.740 --> 00:10:31.460
And really that's a lot easier
for a lot of us to understand

00:10:31.460 --> 00:10:33.100
is time spent in our code,
right?

00:10:33.140 --> 00:10:36.340
'Cause we can't do a lot about
system libraries typically.

00:10:38.190 --> 00:10:44.980
So, along those same lines
is Flatten Library.

00:10:44.980 --> 00:10:48.840
We are familiar with the pieces of
AppKit or Core Foundation or the

00:10:48.840 --> 00:10:52.090
other system libraries that we call,
those function entry points.

00:10:52.090 --> 00:10:57.680
But a lot of times you'll see interior
functions inside of those libraries

00:10:57.910 --> 00:11:01.500
that you really don't know much about,
and you really don't care much about

00:11:01.500 --> 00:11:03.600
the implementation interior to that.

00:11:03.730 --> 00:11:05.910
So, Flatten Library is going to do that.

00:11:06.110 --> 00:11:10.580
It's going to hide everything inside of
a library except for the entry point.

00:11:10.600 --> 00:11:13.100
So, you're going to just see all the
time it show up in the entry point,

00:11:13.100 --> 00:11:15.800
which is what you understand, right,
because that's what you

00:11:15.800 --> 00:11:17.200
called from your code.

00:11:18.560 --> 00:11:21.500
And finally, Focus Symbol.

00:11:21.560 --> 00:11:24.160
If you typically navigate
very deep call trees,

00:11:24.180 --> 00:11:27.930
or if you know you have a certain
symbol that you only want to

00:11:28.010 --> 00:11:30.380
look at and what it calls,
that's what Focus does.

00:11:30.520 --> 00:11:36.490
It's going to just start the display of
the various call paths from that symbol.

00:11:36.600 --> 00:11:39.150
So these are different ways
to really customize how

00:11:39.150 --> 00:11:40.950
you're looking at the profile.

00:11:40.980 --> 00:11:45.370
And internally at Apple,
we've found this very helpful in looking

00:11:45.450 --> 00:11:48.030
at performance analysis for Tiger.

00:11:49.890 --> 00:11:53.220
So those are the detailed
data mining operations.

00:11:53.460 --> 00:11:57.230
There are more macro-level
operations that are also very useful.

00:11:57.240 --> 00:12:00.560
Flattened Recursion:
So you've probably seen this if

00:12:00.560 --> 00:12:03.450
you've ever looked at a piece of code.

00:12:03.450 --> 00:12:06.600
You've seen a recursive call,
maybe in NSView or some

00:12:06.620 --> 00:12:07.960
other—even your own code.

00:12:08.070 --> 00:12:11.190
And really,
that's not the most informative way

00:12:11.190 --> 00:12:13.960
to look at a piece of a function,
right?

00:12:13.960 --> 00:12:15.510
But if you're looking at
this recursive function,

00:12:15.510 --> 00:12:18.530
you'd actually like to see all those
recursive calls collapse into one,

00:12:18.550 --> 00:12:22.230
and see all the time together,
and what all the recursive calls

00:12:22.230 --> 00:12:25.080
actually do—collapse together.

00:12:25.080 --> 00:12:29.190
So this Flattened Recursion checkbox
is in this Advanced Settings drawer

00:12:29.410 --> 00:12:31.080
that we've been looking at.

00:12:31.080 --> 00:12:35.420
And you check this checkbox,
and all the recursive calls anywhere in

00:12:35.420 --> 00:12:38.110
the profile just get collapsed to one.

00:12:39.130 --> 00:12:42.980
So we talked about
flattening a single library.

00:12:43.420 --> 00:12:46.440
This flattened system libraries
will flatten all of the system

00:12:46.530 --> 00:12:48.000
libraries that occur in the profile.

00:12:48.140 --> 00:12:51.730
So this is just a really quick
way to hide all that interior code

00:12:52.030 --> 00:12:54.100
that you don't really want to see.

00:12:54.640 --> 00:12:58.630
Another very useful one is Exclude
Entries Without Source Info.

00:12:58.780 --> 00:13:01.000
Usually when you're working on
an application in your system,

00:13:01.000 --> 00:13:03.240
you've turned on debugging
information because you want to

00:13:03.240 --> 00:13:06.000
see symbols in Shark and you want
to get all the source information.

00:13:06.170 --> 00:13:08.720
Well, the good news is that that's
almost the only thing in the

00:13:08.720 --> 00:13:10.340
system with that information in it.

00:13:10.490 --> 00:13:12.970
So if you exclude everything
without that information,

00:13:12.970 --> 00:13:14.660
you're going to only see your code.

00:13:14.660 --> 00:13:18.260
It's going to perform that exclude
operation on any symbol that doesn't have

00:13:18.390 --> 00:13:20.450
source information associated with it.

00:13:20.450 --> 00:13:22.720
So this, again,
this will just focus in on what

00:13:22.720 --> 00:13:24.500
you know on your source code.

00:13:25.570 --> 00:13:29.500
I encourage you to play with
all these different operations.

00:13:29.500 --> 00:13:33.840
You'll see very quickly what they
do and how they can help you.

00:13:34.060 --> 00:13:37.450
Finally, Exclude Supervisor.

00:13:37.940 --> 00:13:41.340
A lot of us aren't very familiar with
the kernel and what it's doing inside,

00:13:41.340 --> 00:13:45.840
and how did I get these
red samples in my profile?

00:13:46.270 --> 00:13:48.710
This is just a quick way to get
rid of those samples and keep

00:13:48.790 --> 00:13:52.120
them from being a distraction
while you're looking at your code.

00:13:54.030 --> 00:13:58.020
Okay, so those are the high-level
optimization tools that Shark provides.

00:13:58.120 --> 00:13:59.900
Once you've kind of gotten
those out of the way,

00:13:59.900 --> 00:14:04.240
you should be able to at least understand
your code at a level where you want

00:14:04.240 --> 00:14:06.900
to optimize a specific function,
right?

00:14:06.900 --> 00:14:10.900
So that's where Shark shines
for function-level optimization.

00:14:10.900 --> 00:14:14.800
The most basic thing that Shark does for
you is it shows you your source code.

00:14:14.910 --> 00:14:18.450
If you double-click on any symbol
in those profile views that we

00:14:18.450 --> 00:14:20.890
talked about—heavier tree—it's
going to show the source code.

00:14:20.900 --> 00:14:23.840
And what it's doing here,
it's coloring the source according

00:14:23.960 --> 00:14:26.900
to how many samples landed
on a particular source line.

00:14:26.900 --> 00:14:29.900
So the brighter the yellow,
the more samples landed there.

00:14:29.900 --> 00:14:32.890
It's a really quick visual
way to see where you're

00:14:32.890 --> 00:14:34.980
spending time in your source.

00:14:35.730 --> 00:14:38.560
You also note that along
the—in the scroll bar,

00:14:38.740 --> 00:14:41.790
the coloring in the background of
the scroll bar matches the coloring

00:14:41.790 --> 00:14:43.360
in the background of the table.

00:14:43.490 --> 00:14:46.630
So this is a very easy way to navigate,
you know,

00:14:46.940 --> 00:14:49.670
quickly between hotspots in your code.

00:14:51.910 --> 00:14:56.680
And finally, to make the profile and
optimization cycle complete,

00:14:56.930 --> 00:14:58.040
there's an edit button.

00:14:58.130 --> 00:15:00.290
And this is going to show your
source code inside of Xcode.

00:15:00.300 --> 00:15:02.740
It's going to bring you back to Xcode.

00:15:02.770 --> 00:15:05.240
So the typical optimization cycle, right?

00:15:05.270 --> 00:15:08.180
You profile with Shark,
you go to the code browser,

00:15:08.180 --> 00:15:11.410
you see a problem,
you click "Edit," go back to Xcode,

00:15:11.470 --> 00:15:12.730
recompile, and so on.

00:15:12.880 --> 00:15:16.170
So it's this iterative process
where you continually go back

00:15:16.170 --> 00:15:19.890
and optimize it again and again
until you're happy with the code.

00:15:20.390 --> 00:15:24.130
Another great thing that Shark will
do for you is it will provide

00:15:24.130 --> 00:15:26.140
you with optimization tips.

00:15:26.140 --> 00:15:29.480
So if you see one of these exclamation
points in the code browser,

00:15:29.480 --> 00:15:32.180
in your code,
that means Shark has a tip for you.

00:15:32.370 --> 00:15:36.830
So it has an idea about what might be
wrong or why you might be spending extra

00:15:36.890 --> 00:15:39.620
time on that particular line of source.

00:15:39.690 --> 00:15:45.380
And so if you click on
the exclamation point,

00:15:45.380 --> 00:15:45.380
it will bring up this help balloon.

00:15:47.360 --> 00:15:51.040
And finally, in Shark 4,
we've added this ability to look at

00:15:51.040 --> 00:15:53.320
both source and assembly side by side,
right?

00:15:53.320 --> 00:15:57.100
So now, rather than having to jump back
and forth or trying to figure out

00:15:57.210 --> 00:16:00.510
what the compiler generated for
a particular line of source code,

00:16:00.510 --> 00:16:02.820
when you navigate around
in the source viewer,

00:16:03.130 --> 00:16:07.000
you'll see very quickly, OK,
the highlighted line, if you select here,

00:16:07.000 --> 00:16:09.440
we have a line of source
selected on the left.

00:16:09.650 --> 00:16:11.700
And there are maybe three or
four instructions there that

00:16:11.700 --> 00:16:12.820
are selected on the right.

00:16:12.950 --> 00:16:14.880
And those are the instructions
that go with that source line.

00:16:15.380 --> 00:16:19.810
So this is a real quick way to see
what instructions were being generated.

00:16:21.010 --> 00:16:23.900
Okay, well that's great,
but I don't know any PowerPC assembly.

00:16:23.900 --> 00:16:26.680
Well, here you go, right?

00:16:26.720 --> 00:16:27.700
There's this online help.

00:16:27.740 --> 00:16:31.850
It will follow along as you
navigate through the assembly,

00:16:31.940 --> 00:16:34.860
and it'll tell you in English, you know,
here's what this instruction does.

00:16:35.120 --> 00:16:36.750
And, you know,
assembly language is just like

00:16:36.750 --> 00:16:38.100
any other programming language.

00:16:38.100 --> 00:16:43.610
It has these simple operations,
and you can read about it here

00:16:43.610 --> 00:16:43.610
in this online help guide.

00:16:45.800 --> 00:16:52.400
[Transcript missing]

00:16:52.850 --> 00:16:53.800
That's time profiling.

00:16:53.800 --> 00:16:57.760
Those are the tools that
we use with time profiling.

00:16:57.810 --> 00:17:00.320
But you'll see that we also use
these same tools over and over,

00:17:00.440 --> 00:17:02.570
but with more sophisticated
types of profiling,

00:17:02.620 --> 00:17:03.980
which Shark also provides.

00:17:04.000 --> 00:17:07.340
So in Shark 4, we added malloc tracing.

00:17:07.340 --> 00:17:13.460
And what this is doing is every time an
allocation happens in a target program,

00:17:13.580 --> 00:17:14.560
it's going to take a sample.

00:17:14.560 --> 00:17:18.080
It's going to remember the call stack
and also how much memory was allocated.

00:17:18.820 --> 00:17:20.800
So it's not shown here,
but you can look at this in

00:17:20.800 --> 00:17:23.800
the profile view and say,
"Okay, this function over the sampling

00:17:23.860 --> 00:17:27.340
runtime allocated this much memory."
And that's useful for tracking down,

00:17:27.340 --> 00:17:30.500
you know, overall how much memory was
allocated in various functions.

00:17:30.540 --> 00:17:34.880
But another interesting thing to
do is to use the chart view to see

00:17:34.880 --> 00:17:37.820
patterns in the program behavior,
because this is not a

00:17:37.820 --> 00:17:38.840
statistical profile.

00:17:38.840 --> 00:17:41.410
This is an exact trace
of the malloc functions.

00:17:41.440 --> 00:17:45.150
And most interesting program behavior
does some sort of allocation.

00:17:45.200 --> 00:17:48.740
You can see patterns in your code,
in the execution of your code.

00:17:48.740 --> 00:17:51.680
So here what we're looking at,
this is the dock as we mouse over and

00:17:51.790 --> 00:17:53.740
it kind of magnifies different icons.

00:17:53.740 --> 00:17:56.200
We're looking at three icons here, right?

00:17:56.240 --> 00:17:58.240
And so you can see
this repeated behavior.

00:17:58.240 --> 00:18:00.500
Now sometimes,
like in the case of the dock, well,

00:18:00.500 --> 00:18:01.480
maybe we expect that.

00:18:01.590 --> 00:18:02.720
We expect repetition.

00:18:02.740 --> 00:18:06.050
But this can also be a good
clue when you're doing extra

00:18:06.220 --> 00:18:08.700
operations that aren't necessary.

00:18:08.740 --> 00:18:12.430
So malloc tracing is more
general than just looking at

00:18:12.640 --> 00:18:14.740
where memory was allocated.

00:18:18.770 --> 00:18:22.020
and a more general
form of malloc tracing,

00:18:22.020 --> 00:18:22.980
is function tracing.

00:18:22.980 --> 00:18:26.600
So rather than just tracing the
allocation functions in your code,

00:18:26.600 --> 00:18:28.620
you can trace any functions in your code.

00:18:28.620 --> 00:18:30.990
And again, this is an exact trace.

00:18:31.120 --> 00:18:34.600
So there are some predefined
presets for file AO APIs,

00:18:34.600 --> 00:18:36.940
typical network APIs, locking APIs.

00:18:36.940 --> 00:18:39.240
But you can also add your own functions,
right?

00:18:39.240 --> 00:18:43.240
So if you have a set of functions
that you want to watch in your

00:18:43.350 --> 00:18:46.250
code and you want to measure exact
function count and the amount

00:18:46.250 --> 00:18:49.260
of time that the function took,
this is a great way to do it.

00:18:49.370 --> 00:18:53.310
You just attach to your
application and select the

00:18:53.310 --> 00:18:54.880
functions that you want to watch.

00:18:54.880 --> 00:19:00.630
And this is measuring each function
call's time very accurately

00:19:00.630 --> 00:19:04.600
with the PowerPC decrementer,
but without having to write any

00:19:04.600 --> 00:19:07.320
instrumentation code in your application.

00:19:09.090 --> 00:19:10.210
And finally, event profiling.

00:19:10.400 --> 00:19:14.360
For you long-time Shark users,
we've had this for a while,

00:19:14.360 --> 00:19:16.960
but this is also a very good
way to correlate hardware

00:19:17.130 --> 00:19:20.280
and operating system events,
so things like cache misses in

00:19:20.280 --> 00:19:23.800
the hardware or virtual memory
faults in the operating system,

00:19:24.110 --> 00:19:25.000
with your code.

00:19:26.000 --> 00:19:29.480
Here, in this example,
we have L1 and L2 data cache misses

00:19:29.480 --> 00:19:34.950
that we're event profiling along
with our normal time profile.

00:19:35.020 --> 00:19:41.990
We see two columns have been added to
the profile for each of those counts,

00:19:42.000 --> 00:19:42.960
L1 and L2 cache misses.

00:19:43.050 --> 00:19:46.610
Also, in the chart view,
we can see there's a chart above,

00:19:46.630 --> 00:19:49.540
so you can track the L2
cache miss over time.

00:19:49.550 --> 00:19:53.000
How is that changing over
time in your application?

00:19:54.170 --> 00:19:56.980
And there are literally
hundreds and hundreds of these

00:19:57.100 --> 00:19:59.000
performance events available.

00:20:00.870 --> 00:20:03.280
Finally in Shark 4, Java profiling.

00:20:03.460 --> 00:20:08.070
Everything that you could do with
CFM and Mac OS X applications

00:20:08.110 --> 00:20:11.810
is also available for Java now:
time and allocation tracing,

00:20:11.930 --> 00:20:12.800
function tracing.

00:20:12.800 --> 00:20:15.210
It's going to show if you
double-click on a Java function

00:20:15.210 --> 00:20:18.080
and the source code is available,
it's going to show you the

00:20:18.080 --> 00:20:19.800
Java source in a code browser.

00:20:22.880 --> 00:20:28.510
So those are the built-in configurations
that we've been talking about.

00:20:28.520 --> 00:20:31.600
You can create your own using
this configuration editor.

00:20:31.600 --> 00:20:38.140
And one of the hard things that there
was in Shark 3 was finding where

00:20:38.140 --> 00:20:40.160
performance counter events lived.

00:20:40.220 --> 00:20:43.280
So which PMC,
which performance monitor counter,

00:20:43.280 --> 00:20:45.840
does an event that counts
cache misses live on?

00:20:45.920 --> 00:20:49.080
Well, you'd have to either go to the
processor user guide and look it up,

00:20:49.460 --> 00:20:52.910
or you'd just kind of
blindly search around.

00:20:52.930 --> 00:20:55.310
Well, now there's a text search
for the performance event.

00:20:55.320 --> 00:20:57.820
So you can just type
in cache miss or cache,

00:20:57.820 --> 00:21:00.460
and it'll come up with a list
of which PMCs have an event

00:21:00.530 --> 00:21:03.010
with that substring in it.

00:21:05.120 --> 00:21:07.980
Okay, in Shark 4,
we've also added network profiling.

00:21:07.990 --> 00:21:10.100
This is using Rendezvous.

00:21:10.100 --> 00:21:13.800
You can set up your
XServe in your closet,

00:21:13.800 --> 00:21:18.380
or you have an HPC cluster,
and you want to monitor what's

00:21:18.380 --> 00:21:20.750
going on in that machine that
doesn't have a monitor and a mouse

00:21:20.750 --> 00:21:22.090
in front of it and a keyboard.

00:21:22.100 --> 00:21:24.870
Well, this will let you monitor
one or more of those machines

00:21:24.940 --> 00:21:27.060
that you've set that up on,
so you don't have to sit

00:21:27.060 --> 00:21:28.100
in front of this machine.

00:21:28.100 --> 00:21:32.090
You just go out—it works very similar
to Xcode's distributed builds.

00:21:32.160 --> 00:21:37.090
You pick the target machines and hit
start and stop on your client machine.

00:21:37.170 --> 00:21:41.100
And so, let's have a demo of that.

00:21:41.200 --> 00:21:44.390
Can we switch to the demo machine,
please?

00:21:46.300 --> 00:21:48.190
Great.

00:21:48.280 --> 00:21:51.180
All right,
so over here I have a machine—this is

00:21:51.180 --> 00:21:57.140
Sanjay's machine that's running some
code on it—but I'll launch Shark here,

00:21:57.190 --> 00:21:57.830
right?

00:21:57.830 --> 00:22:01.000
And I'll go to the
network profiling window,

00:22:01.190 --> 00:22:03.560
and I'll say,
"Control network profiling of shared

00:22:03.560 --> 00:22:10.810
computers." And there I can say,
"Oh, somebody has shared this WWDC6.

00:22:10.810 --> 00:22:10.810
Okay, let's use that."

00:22:12.460 --> 00:22:14.160
And it's running something over here.

00:22:14.160 --> 00:22:15.960
Let's hit start.

00:22:16.080 --> 00:22:17.300
So again, Shark is very simple.

00:22:17.300 --> 00:22:19.240
Just hit the start button.

00:22:19.310 --> 00:22:21.080
Stop.

00:22:21.130 --> 00:22:24.490
And now we're processing
on the other machine here.

00:22:25.430 --> 00:22:29.000
This is the profile
taken from this machine.

00:22:29.000 --> 00:22:31.090
We can see, "Oh, that's Celestia.

00:22:31.310 --> 00:22:34.250
I wonder what that is."
Maybe Sanjay can tell us.

00:22:34.470 --> 00:22:38.990
Sanjay Patel, can we have you come
up here and explain it?

00:22:45.610 --> 00:22:48.980
So, for those of you here on Monday,
you probably saw our Celestia demo.

00:22:49.140 --> 00:22:53.780
We'd like to show you a few more
details about how we optimize that code.

00:22:53.890 --> 00:22:56.690
So, if we go back to slides
for just a second.

00:23:13.880 --> 00:23:18.020
Okay, so Celestia is this open source
program we found on the web,

00:23:18.020 --> 00:23:18.640
and it's really cool.

00:23:18.640 --> 00:23:23.780
It's a 3D space simulation of all
the large bodies in the universe.

00:23:23.960 --> 00:23:26.920
For more information,
you can check them out on this web link.

00:23:26.990 --> 00:23:31.010
They actually use real data
from Pasadena down at JPL to

00:23:31.090 --> 00:23:35.320
calculate where the planets and
the stars are—how they're orbiting.

00:23:35.420 --> 00:23:37.860
So let's go back to the demo machine.

00:23:43.390 --> 00:23:44.440
And so this is Celestia's main window.

00:23:44.440 --> 00:23:48.340
You can see it's hovering
around the earth right now.

00:23:48.370 --> 00:23:52.300
And we wanted to find out how
this thing performs on Mac OS X.

00:23:52.370 --> 00:23:54.340
So the first thing you have to do
if you're going to try to optimize

00:23:54.340 --> 00:23:56.360
performance is put down a good metric.

00:23:56.560 --> 00:24:00.460
In our case, that would be how long
it takes using a timer,

00:24:00.480 --> 00:24:04.210
as you can see in the corner,
to run our script.

00:24:04.770 --> 00:24:07.900
And so right now,
we're running and timing,

00:24:08.030 --> 00:24:10.200
so we can bring up Shark.

00:24:12.210 --> 00:24:15.130
And you can see, as Nathan said,
Shark is pretty simple.

00:24:15.240 --> 00:24:17.150
There's a start button
in the middle here.

00:24:17.150 --> 00:24:20.950
We can choose our configuration,
but by default, most of us are going

00:24:20.950 --> 00:24:21.770
to use a time profile.

00:24:21.970 --> 00:24:24.230
Let's just hit the start button.

00:24:24.790 --> 00:24:26.830
So right now we're actually
taking a thousand samples per

00:24:26.830 --> 00:24:30.990
second of whatever's running on
the system beyond just Celestia.

00:24:31.170 --> 00:24:32.500
If you hit stop,

00:24:35.790 --> 00:24:37.500
Shark is now processing
the samples it took.

00:24:37.500 --> 00:24:40.600
And it's going to show us by
default what we call the heavy view.

00:24:40.730 --> 00:24:45.060
So this is a list in order of
importance of the functions that

00:24:45.060 --> 00:24:47.540
actually ran on this system.

00:24:47.560 --> 00:24:50.000
Now at the bottom here you can
see Celestia's taking up less

00:24:50.130 --> 00:24:52.600
than half of the processor cycles.

00:24:52.630 --> 00:24:55.760
And if you pop that open you notice
the first thing is the Mach kernel.

00:24:56.010 --> 00:24:59.220
So new for Shark 4,
we actually filter out the idle thread,

00:24:59.340 --> 00:25:02.850
so you won't pop there by default.

00:25:03.010 --> 00:25:06.010
Thanks for old Shark fans.

00:25:06.360 --> 00:25:08.510
And you might wonder why is
Celestia getting less than half

00:25:08.600 --> 00:25:10.060
the CPU cycles on this system?

00:25:10.060 --> 00:25:12.920
If you pop open the thread pop up,
you'll see that it's

00:25:12.920 --> 00:25:14.140
a single threaded app.

00:25:14.270 --> 00:25:16.440
So we're running on a dual G5.

00:25:16.460 --> 00:25:20.940
It can't possibly take all of
the CPU cycles on the machine.

00:25:21.140 --> 00:25:24.600
Now another way to look at this code is,
as Nathan mentioned, we have a tree view.

00:25:24.900 --> 00:25:27.850
So this is the traditional
top-down view of code.

00:25:27.950 --> 00:25:31.030
So you start at start or main,
and you work your way down to

00:25:31.030 --> 00:25:32.460
the leaves of the function.

00:25:32.490 --> 00:25:35.540
This is great for understanding
the structure of your code:

00:25:35.600 --> 00:25:36.860
How did you get there?

00:25:36.900 --> 00:25:41.780
And new for Shark 4, we can view both:
heavy and tree simultaneously,

00:25:41.780 --> 00:25:46.300
so you can get a really good feel
for the behavior of your program.

00:25:46.300 --> 00:25:49.770
And of course, as Nathan mentioned,
we have the new chart view.

00:25:51.080 --> 00:25:51.940
So this is interesting.

00:25:51.940 --> 00:25:55.000
We have a dual processor system,
so we get two charts.

00:25:55.000 --> 00:25:59.160
And on the x-axis,
we have time for each CPU.

00:25:59.160 --> 00:26:01.190
Now, we know that Celestia's a
single-threaded program,

00:26:01.290 --> 00:26:05.010
so let's just look at its
one thread in one chart.

00:26:05.030 --> 00:26:07.420
And new for Shark 4,
we have this zoom slider.

00:26:07.420 --> 00:26:11.840
This was a big upgrade
compared to previous zooming.

00:26:11.840 --> 00:26:13.400
And you can go back and forth.

00:26:13.400 --> 00:26:16.760
The nice part is if you click
on one of these call stacks,

00:26:16.760 --> 00:26:18.540
you'll get the backtrace
for that point in time.

00:26:18.790 --> 00:26:22.870
So you can see chronologically how
your program behaved over time.

00:26:22.880 --> 00:26:27.280
And you'll notice that there are
some of these maroon or red samples.

00:26:28.640 --> 00:26:31.300
Those are calls into supervisor space.

00:26:31.380 --> 00:26:35.120
And one thing I noticed was,
for the Celestia profile,

00:26:35.140 --> 00:26:37.970
we had a lot of these calls
into the supervisor space.

00:26:38.010 --> 00:26:42.120
It turns out we're trying to draw
about a thousand times per second.

00:26:42.270 --> 00:26:45.260
And for those of us who are
familiar with graphics applications,

00:26:45.260 --> 00:26:48.500
you don't really need to try to
draw that often because you can't.

00:26:48.620 --> 00:26:52.110
So we limited it to 30 frames per second.

00:26:52.340 --> 00:26:55.620
And that's what I call Warp 2.

00:26:55.710 --> 00:26:57.570
We can go to that.

00:26:58.090 --> 00:27:00.890
So you can see things have sped
up immensely by not doing all that

00:27:00.890 --> 00:27:03.240
work of trying to draw too often.

00:27:03.280 --> 00:27:07.000
We can also go to the threading model,
which would be Warp 3.

00:27:07.120 --> 00:27:09.710
So it turns out you can do
drawing in parallel with some

00:27:09.710 --> 00:27:11.540
of the computations for orbits.

00:27:11.630 --> 00:27:14.190
And we can take another sample here.

00:27:14.930 --> 00:27:22.350
So, I'll put on my hardware hat for a
second and say that threading is

00:27:22.350 --> 00:27:25.820
probably the most important thing you
guys can do if you have any serious

00:27:25.940 --> 00:27:29.820
computation going on in your programs,
because no matter what

00:27:29.820 --> 00:27:33.150
architecture we're talking about,
we're going to end up being multi-core,

00:27:33.150 --> 00:27:35.490
multi-processor, multi-threaded,
multi-something.

00:27:35.850 --> 00:27:42.190
So, if you're not taking advantage
of that parallelism in hardware,

00:27:42.190 --> 00:27:42.190
you're going to lose out.

00:27:43.120 --> 00:27:46.810
But now we can see,
if we compare the first

00:27:46.980 --> 00:27:49.070
session to our second session,

00:27:49.340 --> 00:27:50.910
You'll notice that it's very different.

00:27:51.020 --> 00:27:53.800
On the first session,
we had this called a log at the top,

00:27:53.940 --> 00:27:56.000
but now that we've limited
drawing and done threading,

00:27:56.190 --> 00:27:58.200
we have a totally different profile.

00:27:58.310 --> 00:28:01.120
So this brings up an important point:
You shouldn't try to

00:28:01.120 --> 00:28:03.200
micro-optimize right off the bat.

00:28:03.200 --> 00:28:06.510
You should look for some high-level
problems to figure out what your code

00:28:06.530 --> 00:28:09.990
is actually doing before you start
diving into function-level tuning.

00:28:10.520 --> 00:28:13.860
But now we have a new profile,
and you'll see that Cosine from LibSystem

00:28:13.860 --> 00:28:15.900
is actually at the top of the chart.

00:28:16.040 --> 00:28:17.640
And there's not much
we can do about Cosine,

00:28:17.640 --> 00:28:19.910
but we should figure out
who's calling Cosine.

00:28:20.090 --> 00:28:24.120
So if you open up a disclosure triangle,
you can see our main caller

00:28:24.120 --> 00:28:25.880
is this orbit function.

00:28:26.480 --> 00:28:29.710
Now, new in Shark 4, we have what we call
data mining operations,

00:28:29.880 --> 00:28:33.020
and as Nathan mentioned,
if we want to hide, say, a whole library,

00:28:33.020 --> 00:28:36.770
for example, lib system,
we can just exclude it,

00:28:36.990 --> 00:28:39.260
and now all of the samples
that used to be in cosine are

00:28:39.260 --> 00:28:40.690
attributed to its callers.

00:28:40.820 --> 00:28:43.740
So you see our orbit function
is now at the top of the chart.

00:28:43.860 --> 00:28:46.810
And of course, if you double-click,

00:28:48.690 --> 00:28:50.960
You get your source code,
and it's highlighted

00:28:50.960 --> 00:28:52.420
according to Hotspot.

00:28:52.480 --> 00:28:55.460
You can see in this function,
almost all of the time

00:28:55.470 --> 00:28:58.260
it's going into one line,
calling cosine.

00:28:58.520 --> 00:29:00.420
And of course, it's a for loop.

00:29:00.580 --> 00:29:04.640
What we should do is open up Xcode
and see what we can do about that.

00:29:04.820 --> 00:29:06.970
So you just hit the edit button.

00:29:14.780 --> 00:29:17.380
And you can see we're
calling cosine in a loop,

00:29:17.380 --> 00:29:19.900
and we're actually just iterating
over a whole array of data and

00:29:19.900 --> 00:29:21.790
getting the cosine of each value.

00:29:21.850 --> 00:29:23.880
Now,
new in Tiger in the Accelerate framework,

00:29:23.900 --> 00:29:26.690
there just happens to be a
cosine function that does that.

00:29:26.820 --> 00:29:29.300
It just iterates over an entire array.

00:29:29.520 --> 00:29:31.840
And if we jump over to that line,

00:29:32.190 --> 00:29:33.530
You can see I just made a call here.

00:29:33.670 --> 00:29:34.800
It's called vvcosinf.

00:29:34.940 --> 00:29:37.140
It's available on Tiger.

00:29:37.260 --> 00:29:40.400
And what it does is it
just takes an entire array,

00:29:40.460 --> 00:29:44.380
operates cosine across the whole path,
and gives you a return array.

00:29:44.500 --> 00:29:46.180
So we made that optimization.

00:29:46.180 --> 00:29:48.350
We call that Warp 4.

00:29:50.800 --> 00:29:54.080
You'll see we took about 20 seconds to
run through the demo before while we

00:29:54.080 --> 00:29:56.550
were threading and limiting drawing.

00:29:58.470 --> 00:30:02.560
Using the Accelerate framework,
we doubled the performance.

00:30:02.650 --> 00:30:05.100
But there is a downside:
I kind of cheated.

00:30:05.180 --> 00:30:09.000
That cosine function is single precision,
and the inputs and outputs in

00:30:09.010 --> 00:30:11.400
Celestia were double precision.

00:30:11.480 --> 00:30:14.600
So I didn't want to cause any
kind of astronomical catastrophe

00:30:14.600 --> 00:30:16.400
or global warming or anything.

00:30:17.400 --> 00:30:20.350
So we decided we'll do our
own cosine approximation

00:30:20.440 --> 00:30:22.400
function in double precision.

00:30:22.400 --> 00:30:26.840
And I called that Warp 5,
and it ended up being a little slower

00:30:26.900 --> 00:30:29.050
than just calling it a V-force.

00:30:29.800 --> 00:30:32.690
So, we decided we'll do our
own cosine approximation

00:30:33.160 --> 00:30:35.140
function in double precision.

00:30:35.140 --> 00:30:39.420
And I called that Warp 5,
and it ended up being a little slower

00:30:39.420 --> 00:30:41.460
than just calling it a V-force.

00:30:50.520 --> 00:30:53.420
So, we decided we'll do our
own cosine approximation

00:30:53.450 --> 00:30:55.740
function in double precision.

00:30:55.740 --> 00:30:58.010
And I called that Warp 5,
and it ended up being a little slower

00:30:58.010 --> 00:30:58.010
than just calling it a V-force.

00:30:58.340 --> 00:31:01.730
and figure out what our code is doing
after we've made these optimizations.

00:31:01.870 --> 00:31:04.060
So this is performance tuning, of course.

00:31:04.060 --> 00:31:06.160
It's an iterative process.

00:31:06.320 --> 00:31:08.570
So again,
you can see the profile has changed.

00:31:08.570 --> 00:31:09.960
Cosine's no longer there.

00:31:10.080 --> 00:31:12.500
We have our approximation
function on top.

00:31:12.780 --> 00:31:15.670
And what I want to do is take a
look at this function called BigFix.

00:31:15.770 --> 00:31:19.060
It's a C++ constructor.

00:31:19.060 --> 00:31:23.320
And you look here and you say, "Whoa,
Shark is angry about something."

00:31:24.570 --> 00:31:27.780
It's got a lot of what we
call the callouts there,

00:31:27.780 --> 00:31:31.760
and if you click on one of those,
you get some friendly advice—or

00:31:31.760 --> 00:31:35.140
not-so-friendly advice,
depending on how bad your code was.

00:31:35.140 --> 00:31:37.360
In this case,
we're doing type conversions

00:31:37.540 --> 00:31:41.720
from floating point to int,
so this is one of the big bottlenecks

00:31:41.750 --> 00:31:45.280
for a G5 or even a G4 processor.

00:31:45.310 --> 00:31:47.540
And what we want to do
is try to optimize that.

00:31:47.900 --> 00:31:50.950
But let's take a look at what
the compiler generates underneath

00:31:51.060 --> 00:31:56.110
this So we can now look at
source and assembly side by side.

00:31:56.790 --> 00:31:59.490
Of course,
we're trying to sell those big monitors.

00:31:59.740 --> 00:32:01.400
This looks really great on 30-inch.

00:32:07.520 --> 00:32:10.680
Now the really nice part here is
you can see Nathan scrolling around,

00:32:10.680 --> 00:32:12.950
and for each line of source,
we're actually in sync

00:32:12.950 --> 00:32:14.100
with the ASIM view.

00:32:14.270 --> 00:32:19.000
So that's going to pop as you
scroll around in the source.

00:32:19.050 --> 00:32:23.370
So you can see exactly which lines of
ASIM correspond to which lines of source.

00:32:25.930 --> 00:32:30.580
Now if we just want to look at
ASIM and don't really understand ASIM,

00:32:30.590 --> 00:32:33.890
then we have this new
handy PPC help button.

00:32:34.870 --> 00:32:36.560
As Nathan showed.

00:32:36.640 --> 00:32:39.840
And the cool part here is,
as you click on your ASIM,

00:32:39.870 --> 00:32:43.180
the right page of the
manual pops open with you.

00:32:46.790 --> 00:32:48.940
So, lots of information
right at your fingertips.

00:32:48.940 --> 00:32:51.090
You don't have to go
fumbling around for manuals.

00:32:51.100 --> 00:32:54.690
That's a pretty big improvement
for someone like me.

00:32:55.810 --> 00:32:59.760
And of course, if you're really tuning
for G5 or PowerPC 970,

00:32:59.760 --> 00:33:01.660
we have this extra help here.

00:33:01.770 --> 00:33:06.130
If we click this "Show Dispatch Groups,"
your source will now be highlighted

00:33:06.130 --> 00:33:08.530
according to dispatch groups for the G5.

00:33:08.640 --> 00:33:11.080
So for those of you familiar
with the G5 architecture,

00:33:11.080 --> 00:33:13.200
you can actually tune
down to the hardware.

00:33:13.300 --> 00:33:17.900
The other nice feature is
this 970 details drawer.

00:33:18.220 --> 00:33:20.260
And if you highlight code,
you can see dynamically we're

00:33:20.260 --> 00:33:23.440
updating the charts at the bottom,
so you can see where every

00:33:23.530 --> 00:33:25.450
ASIM instruction is going.

00:33:30.420 --> 00:33:32.520
So on the left we have
the functional units,

00:33:32.520 --> 00:33:34.800
and on the right we
have the dispatch slots.

00:33:34.800 --> 00:33:37.340
So for those of you who
really want to tune,

00:33:37.490 --> 00:33:38.930
this is great stuff.

00:33:41.020 --> 00:33:44.880
Okay, so let's go back to Celestia.

00:33:44.880 --> 00:33:48.000
So over a period of
about two weeks or so,

00:33:48.020 --> 00:33:50.490
with a lot of interruptions
doing real work,

00:33:50.490 --> 00:33:53.500
I spent—I used Shark to
try to optimize this code,

00:33:53.500 --> 00:33:57.410
so I ended up writing some AlteVec code,
I did some scheduling for the G5,

00:33:57.490 --> 00:34:01.100
I did some G5-specific optimizations,
and the sum result of that

00:34:01.100 --> 00:34:02.490
is what I call Warp 9.

00:34:02.490 --> 00:34:06.660
And if we run through that,

00:34:09.630 --> 00:34:13.370
Okay, so let's go back to Celestia.

00:34:13.490 --> 00:34:16.500
So over a period of
about two weeks or so,

00:34:16.500 --> 00:34:18.960
with a lot of interruptions
doing real work,

00:34:19.050 --> 00:34:22.000
I spent—I used Shark to
try to optimize this code,

00:34:22.000 --> 00:34:24.600
so I ended up writing some AlteVec code,
I did some scheduling for the G5,

00:34:24.600 --> 00:34:24.600
I did some G5-specific optimizations,
and the sum result of that

00:34:24.600 --> 00:34:24.600
is what I call Warp 9.

00:34:24.600 --> 00:34:24.600
And if we run through that,

00:34:30.560 --> 00:34:34.070
Here's the result of a couple
of weeks of tuning effort.

00:34:34.110 --> 00:34:36.660
We started out—if you
run this demo as is,

00:34:36.660 --> 00:34:40.400
it'll take over 20, 25 minutes on a G5.

00:34:40.520 --> 00:34:42.820
And when we were done, of course,
we got to five seconds,

00:34:42.960 --> 00:34:45.250
so around 300 times the performance.

00:34:45.250 --> 00:34:49.530
So that was pretty nice.

00:34:55.480 --> 00:34:57.390
So let's just summarize what
we've talked about today.

00:34:57.400 --> 00:35:03.100
Things we really want to say are,
as you can see, hopefully,

00:35:03.150 --> 00:35:06.820
that it's a really quick
process to start using Shark.

00:35:06.950 --> 00:35:09.340
You don't have to do any
kind of hardcore profiling.

00:35:09.340 --> 00:35:13.660
You can take your optimized
binary and simply profile that.

00:35:13.690 --> 00:35:19.330
In order to see your source,
all you have to do is add the

00:35:19.330 --> 00:35:19.330
-g flag so you don't strip
out your debugging symbols.

00:35:21.950 --> 00:35:24.140
And of course we provide
a lot of high-level views,

00:35:24.140 --> 00:35:26.320
so if you're used to trees,
you can see it in tree view,

00:35:26.320 --> 00:35:29.280
you can use the heavy view,
you can look at your code

00:35:29.280 --> 00:35:30.920
over time as a chart.

00:35:32.320 --> 00:35:34.870
And of course, once you've identified
functions that are hotspots,

00:35:35.020 --> 00:35:38.420
you want to tune the source
code or tune the assembly,

00:35:38.470 --> 00:35:41.600
and Shark gives you optimization tips.

00:35:42.410 --> 00:35:45.040
And of course, once you've identified
functions that are hotspots,

00:35:45.100 --> 00:35:48.520
you want to tune the source
code or tune the assembly,

00:35:48.520 --> 00:35:51.670
and Shark gives you optimization tips.

00:35:57.990 --> 00:36:01.620
And of course, hopefully you can tell,
Shark we've tried really

00:36:01.620 --> 00:36:02.890
hard to make easy.

00:36:03.400 --> 00:36:05.080
There's no magic here.

00:36:05.110 --> 00:36:07.900
You just push the start button
and that should get you going.

00:36:07.900 --> 00:36:10.880
But there is a lot of machinery
underneath here that makes it

00:36:11.020 --> 00:36:14.100
powerful enough for even the most
expert users who want to tune

00:36:14.150 --> 00:36:15.880
all the way down to the metal.

00:36:17.770 --> 00:36:21.320
And of course, hopefully you can tell,
Shark we've tried really

00:36:21.320 --> 00:36:22.560
hard to make easy.

00:36:22.700 --> 00:36:24.510
There's no magic here.

00:36:24.510 --> 00:36:24.510
You just push the start button
and that should get you going.

00:36:24.510 --> 00:36:24.510
But there is a lot of machinery
underneath here that makes it

00:36:24.510 --> 00:36:24.510
powerful enough for even the most
expert users who want to tune

00:36:24.510 --> 00:36:24.510
all the way down to the metal.

00:36:25.840 --> 00:36:29.510
And of course, hopefully you can tell,
Shark we've tried really

00:36:29.520 --> 00:36:30.750
hard to make easy.

00:36:30.790 --> 00:36:32.980
There's no magic here.

00:36:32.980 --> 00:36:35.650
You just push the start button
and that should get you going.

00:36:35.980 --> 00:36:38.850
But there is a lot of machinery
underneath here that makes it

00:36:38.880 --> 00:36:41.980
powerful enough for even the most
expert users who want to tune

00:36:41.990 --> 00:36:41.990
all the way down to the metal.