# Wwdc2020 10611

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Explore ARKit 4ARKit 4 enables you to build the next generation of augmented reality apps to transform how people connect with the world around them. We'll walk you through the latest improvements to Apple's augmented reality platform, including how to use Location Anchors to connect virtual objects with a real-world longitude, latitude, and altitude. Discover how to harness the LiDAR Scanner on iPad Pro and obtain a depth map of your environment. And learn how to track faces in AR on more devices, including the iPad Air (3rd generation), iPad mini (5th generation), and all devices with the A12 Bionic chip or later that have a front-facing camera. 

To get the most out of this session, you should be familiar with how your apps can take advantage of LiDAR Scanner on iPad Pro. Watch “Advanced Scene Understanding in AR” for more information. 

Once you've learned how to leverage ARKit 4 in your iOS and iPadOS apps, explore realistic rendering improvements in “What's New in RealityKit” and other ARKit features like People Occlusion and Motion Capture with “Introducing ARKit 3”.ResourcesARKitCreating a fog effect using scene depthDisplaying a point cloud using scene depthTracking geographic locations in ARHD VideoSD VideoRelated VideosWWDC21Explore ARKit 5WWDC20Support performance-intensive apps and gamesThe artist’s AR toolkitWhat's new in RealityKitWWDC19Introducing ARKit 3Introducing RealityKit and Reality Composer

ARKit 4 enables you to build the next generation of augmented reality apps to transform how people connect with the world around them. We'll walk you through the latest improvements to Apple's augmented reality platform, including how to use Location Anchors to connect virtual objects with a real-world longitude, latitude, and altitude. Discover how to harness the LiDAR Scanner on iPad Pro and obtain a depth map of your environment. And learn how to track faces in AR on more devices, including the iPad Air (3rd generation), iPad mini (5th generation), and all devices with the A12 Bionic chip or later that have a front-facing camera. 

To get the most out of this session, you should be familiar with how your apps can take advantage of LiDAR Scanner on iPad Pro. Watch “Advanced Scene Understanding in AR” for more information. 

Once you've learned how to leverage ARKit 4 in your iOS and iPadOS apps, explore realistic rendering improvements in “What's New in RealityKit” and other ARKit features like People Occlusion and Motion Capture with “Introducing ARKit 3”.

ARKit

Creating a fog effect using scene depth

Displaying a point cloud using scene depth

Tracking geographic locations in AR

HD VideoSD Video

HD Video

SD Video

Explore ARKit 5

Support performance-intensive apps and games

The artist’s AR toolkit

What's new in RealityKit

Introducing ARKit 3

Introducing RealityKit and Reality Composer

Search this video…♪Voiceover: Hello, and welcome to WWDC.Quinton: Hi, my name's Quinton,and I'm an engineer on the ARKit team.Today both Praveen and I get to show yousome of the new features in ARKit with iOS 14.So let's jump right in and explore ARKit 4.This release adds many advancements to ARKitwhich already powers the world's largest AR platform, iOS.ARKit gives you the tools to create AR experiencesthat change the way your users see the world.Some of these tools include device motion tracking,camera scene capture,and advanced scene processing which all help to simplifythe task of building a realistic and immersive AR experience.Let's see what's next with ARKit.So first, we're going to take a look at the location anchor API.Location anchors bring your AR experienceonto the global scaleby allowing you to position virtual contentin relation to the globe.Then we'll see whatt he new LiDAR sensorbrings to ARKit with scene geometry.Scene geometry provides appswith a mesh of the surrounding environmentthat can be used for everything from occlusion to lighting.Next we'll look at the technologythat enables scene geometry, the depth API.We're opening up this APIto give apps access to a dense depth mapto enable new possibilities using the LiDAR sensor.And additionally,the LiDAR sensor improves object placement.We'll go over some best practicesto make sure your apps take full advantageof the newest object-placement techniques.And we'll wrap up with some improvements to FaceTracking.Let's start with location anchors.Before we get too far,let's look at how we got to this point.ARKit started on iOS with the best tracking.No QR codes. No external equipment needed.Just start an AR experience by placing content around you.Then we added multi-user experiences.Your AR content could then be shared with a friendusing a separate device to make experiences social.And last year we brought people into ARKit.AR experiences are now aware of the people on the scene.Motion capture is possible with just a single iOS device,and people occlusion makes AR contenteven more immersiveas people can walk right in front of a virtual object.All these features combineto make some amazing experiences.But what's next?So now we're bringing AR into the outdoorswith location anchors.Location anchors enable youto place AR content in relation to the globe.This means you can now place virtual objectsand create AR experiences by specifying a latitude,longitude, and altitude.ARKit will take your geographic coordinatesas well as high-resolution map data from Apple Mapsto place your AR experiences at the specific world location.This whole process is called visual localization,and it will precisely locate your devicein relation to the surrounding environmentmore accurately than could be done beforewith just GPS.All this is possibledue to advance machine learning techniquesrunning right on your device.There's no processing in the cloudand no images sent back to Apple.ARKit also takes care of merging the local coordinate systemto the geographic coordinate system,so you can work in one unified system,regardless of how you want to create your AR experiences.To access these features we've added a new configuration,ARGeoTrackingConfiguration and ARGeoAnchorsare what you'll use to place in contentthe same way as other ARKit anchors.Let's see some location anchors in action.We've got a video herein front of the Ferry Building in San Francisco.You can see a large virtual sculpture.That's actually the companion sculpture created by KAWS,and viewed in the acute art app.Since it was placed with location anchors,everyone who uses the app at the Ferry Buildingcan enjoy the virtual art in the same place and the same way.Let's see what's under the hood in ARKit to make this all work.So when using geo tracking,we download all the detailed map data from Apple Mapsaround your current location.Part of this data is a localization mapthat contains feature points of the surrounding areathat can be seen from the street.Then with the localization map, your current location,and images from your device,we can use advanced machine learningto visually localize and determineyour device's position.All this is happening under the hood in ARKitto give you a precise, globally-aware posewithout worrying about any of this complexity.The location anchor APIcan be broken down into three main parts.ARGeoTrackingConfiguration is the configurationthat you'll use to take advantageof all of the new location anchor features.This configuration contains a subsetof the world-tracking featuresthat are compatible with geo tracking.Then once you've started an AR sessionwith the geo-tracking configuration,you'll be able to create ARGeoAnchorsjust like any other ARKit anchor.And also while using geo tracking,there's a new tracking status that's important to monitor.This is contained in ARGeoTrackingStatusand provides valuable feedbackto improve the geo-tracking experience.So building an app with location anchorscan be broken down into a few steps.The first is checking availability of geo tracking.ARGeoTrackingConfiguration has a few methodsthat let us check the preconditionsusing the rest of the location anchor API.Then location anchors can be addedonce you know there's full geo-tracking support.And after anchors are added,we can use a rendering engine to place virtual content.We'll then need take care of geo-tracking transitions.Once started, geo tracking will move through a few statesthat may need some user interventionto ensure the best geo-tracking experience.So let's build a simple point of interest appto see what these steps look like in practice.In our app, we're going to start with helping our usersfind the iconic Ferry Building in San Francisco, California.As you can see,we've placed a sign to make the building easy to spot.To begin the app,let's first start with checking availability.As with many ARKit features,we need to make sure the current device is supportedbefore attempting to start an experience.Location anchors are availableon devices with an A12 bionic chipand newer, as well as GPS.ARGeoTrackingConfigurations isSupported class methodshould be used to check for the support.For geo tracking,we also need to check if the current location is supported.We need to be in a locationthat has all the required maps data to localize.The geo tracking configurationhas a method to check your current location supportas well as an arbitrary latitude and longitude.Additionally, once a geo tracking session is started,ARKit will ask a user for permissionfor both camera and location.ARKit has always asked for camera permission,but location permission is needed to do geo tracking.Let's see what this looks like in code.ARGeoTrackingConfiguration has all the class methodsthat we need to check before starting our AR session.We'll first check if the current device is supportedwith "isSupported,"and then we'll checkif our current location is available for geo trackingwith "checkAvailability."If this check fails,we'll get an error with more info to display to the user.For example, if the user hasn't given the app location permissions.Then once we know our current deviceand location are supported,we can go ahead and start the session.Since we're using RealityKit, we'll need our ARViewand then update the configuration.By default, ARView uses a world-tracking configuration,and so we need to pass in a GeoTrackingConfigurationwhen running the session.The next step is adding a location anchor.To do this, we'll use the new ARAnchor subclass ARGeoAnchor.geoAnchors are similar to existing ARKit anchorsin many ways.However, because geoAnchors operate on global coordinates,we can't create them with just transforms.We need to specify their geographic coordinateswith latitude, longitude, and altitude.The most common way to create geoAnchorswill be through specifying just latitude and longitude,which this allows ARKit to fill in the altitudebased on maps data of the ground-level.Let's now add a location anchor to our point of interest app.So for our app,we need to start by finding the Ferry Building's location.One way we can get the latitude and longitudeis through the maps app.When we place the marker in the maps app,we now get up to six digits of precision after the decimal.It's important to use six or more digitsso that we get a precise location to place our contents.Once we have a latitude and longitude,we can make a geoAnchor.We don't need to specify an altitudebecause we'll let ARKit use maps datato determine the elevation of the ground level.Then we'll add the geoAnchor to our session.And since we're using RealityKit to render our virtual content,and we've already created our geoAnchor,we can go ahead and attach the anchor to an entityto mark the Ferry Building.Let's run our app and see what it looks like.We'll start near the Ferry Building in San Franciscolooking towards Market Street.And as we pan around,we can see some of the palm trees that line the city.And soon the Ferry Building will come into view.Our sign looks to be on the ground, which is expected,but the text is rotated.Since we'd like to find the Ferry Building easilyfrom a distance,we'd really like to have the sign floatinga few meters in the air and facing towards the city.So how do we do this?Well, to position this content,we need to first look at the coordinate's systemof the geoAnchor.geoAnchors are fixed to cardinal directions.Their axes are set when you create the anchor,and this orientation will remain unchangedfor the rest of the session.A geoAnchors X-axis is always pointed east,and the Z-axis is always pointed southfor any geographic coordinate.Since we're using a right-handed coordinate system,this leaves positive y pointing up away from the ground.geoAnchors, like all other ARKit anchorsare immutable.This means we'll need to use our rendering engineto rotate or translate our virtual objectsfrom the geoAnchors origin.Let's clean up our signthat we placed in front of the Ferry Building.So here's some RealityKit code to start updating our sign.After getting the signEntityand adding it to the geoAnchor entity,we want to rotate the sign towards the city.To do this,we'll rotate it by a little less than 90 degrees clockwiseand we'll elevate the sign's position by 35 meters.Both of these operations are in relation to the geoAnchor entitythat we had previously created.Let's see what this looks like in the app.Now when we pan around and we get to our Ferry Building,our sign is high in the air,and we can see it from a distance.The text is much easier to read in this orientation.This looks great,but we're missing some crucial information hereabout the geo-tracking state that we can useto guide the user to the best geo-tracking experience.When using a GeoTrackingConfiguration,there's a new GeoTrackingStatus objectthat's available on ARFrame and ARsession observer.ARGeoTrackingStatus encapsulatesall the current state information of geo tracking,similar to the world-tracking informationthat's available on ARCamera.Within geo tracking status, there is a state.This state indicateshow far along geo tracking is during localization.There's also a property that provides more informationabout the current localization statecalled geo Tracking State Reason,and there's an accuracy provided once geo tracking localizes.Let's take a closer look at the geo tracking state.When an AR session begins,geoTrackingState starts at initializing.At this point, geo tracking is waiting for the world tracking to initialize.From initializing,the tracking state can immediately go to not availableif geo tracking isn't supported in the current location.If you're using the checkAvailability class methodon geoTrackingConfiguration,you should rarely get into the state.Once geo tracking moves to localizing,ARKit is receiving images as well as maps dataand is trying to compute both.However, during boththe initializing and localizing states,there could be issues detected that prevent localization.These issues are communicated through geoTrackingStateReason.This reason should be used to inform the userhow to help geo tracking localize.Some possible reasons include the device is pointed too low,which would then inform the user to raise the device,or geoDataNotLoaded,and we'd inform the user that a network connection is required.For all possible reasons,have a look at ARGeoTrackingTypes.h.In general we want to encourage usersto point their devices at buildingsand other stationary structuresthat are visible from the street.Parking lots, open fields,and other environments that dynamically changehave a lower chance of localizing.After addressing any geoTrackingStateReasons,geo tracking should become localized.It's at this point that you should start your AR experience.If you place objects before localization,the objects could jump to unintended locations.Additionally once localized ARGeoTrackingAccuracyis provided to help you gaugewhat experiences should be enabled.It's also important to always monitor geo tracking stateas it's possible for geo trackingto move back to localizing or even initializingsuch as when tracking is lost or map data isn't available.Let's take a look at how we can add this tracking stateto improve our sample app.Now we can see this whole time we were actually localizingwhen looking at Market Street and the surrounding buildings.As we pan around,we can see from the tracking state that we localizeand then the accuracy increases to high.I think we've got our app just about ready,at least for the Ferry Building.So we've added a more expansive location anchor sample projecton developer.apple.comthat I encourage you to check out after this talk.For more information on the RealityKit features used,check out last year's talkintroducing RealityKit and Reality Composer.In our sample app,we saw how to create location anchorsby directly specifying coordinates.We already knew the geographic coordinatesfor the Ferry Building.However, these coordinates could have come from any source,such as our app bundle, our web backend,or really any database.Another way to create a location anchoris via user interaction.We could expand on our app in the futureby allowing users to tap the screento save their own point of interest.getGeoLocation(for point) on ARSessionallows us to get geographic coordinatesfrom any world point in ARKit coordinate space.For example, this could have come from a raycast,or location on a plane.Location anchors are available for you today with iOS14,and we're starting with support in the San Francisco Bay Area,New York, Los Angeles, Chicago, and Miami,with more cities coming through the summer.All iPhones and iPads with an A12 bionic chip or newer,as well as GPS, are supported.Also, for any apps that require location anchors exclusively,you can use device capability keysto limit your app in the App Storeto only compatible hardware.In addition to the GPS key, you'll need to use the new keyfor devices with an A12 bionic chip or newerthat is available in iOS14.So with location anchors,you can now bring your AR experiencesonto the global scale.We went over how ARGeoTrackingConfigurationis the entry point to adding location anchors to your app.We saw how to add ARGeoAnchors to your ARSceneand how to position content in relation to those anchors.We also saw how ARGeoTrackingStatus can be usedto help guide the user to the best geo-tracking experience.And now here's Praveento tell you more about scene geometry.Praveen Gowda: Hi everyone.I'm Praveen Gowda. I'm an engineer on the ARKit team.Today I'm going to take you throughsome of the APIs available in iOS14 that helpbring the power of the LiDAR scanner to your applications.In ARKit 3.5, we introduced the scene geometry APIpowered by the LiDAR scanner on the new iPad Pro.Before we go into scene geometry,let's take a look at how the LiDAR scanner works.The LiDAR shoots light onto the surroundingsand then collects the light reflected off the surfacesin the scene.The depth is estimated by measuring the time it tookfor the light to go from the LiDAR to the environmentand reflect back to the scanner.And this entire process runs millions of times every second.The LiDAR scanner is used by the same geometry APIto provide a topological map of the environment.This can be optionally fused with semantic classificationwhich enables apps to recognize and classify physical objects.This provides an opportunity for creating richer AR experienceswhere apps can now upload virtual objectswith the real world or use physics to enablerealistic interactions between virtual and physical objects.Or to use virtual lighting on real world surfacesand in many other use cases that we were to imagine.Let's take a quick look at scene geometry in action.Here is a living roomand once the scene geometry API is turned onthe entire visible room is meshed.Triangles vary in size to show the optimum detailfor each surface.The color mesh appearsonce semantic classification is enabled.Each color represents a different classificationsuch as blue for the seats and green for the floor.As we saw, the scene geometry feature is builtby leveraging the depth data gathered from the LiDAR scanner.In iOS14, we have a new ARKit depth APIthat provides access to the same depth data.The API provides a dense depth imagewhere a pixel in the image corresponds to depth in metersfrom the camera.What we see here is a debug visualization of this depthwhere there's a gradient from blue to red,where blue represents regions closer to the cameraand red represents those far away.The depth data would be available at 60 Hz,associated with each AR frame.The scene geometry feature is built on top of this APIwhere depth data across multiple frames are aggregatedand processed to construct a 3D mesh.This API is powered by the LiDAR scannerand is available on devices which have LiDAR.Here is an illustration of how the depth map is generated.The colored RGB image from the wide-angle cameraand the depth ratings from the LiDAR scannerare fused together using advanced machine learningalgorithms to create a dense depth mapthat is exposed through the API.This operation runs at 60 times per secondwith the depth map available on every AR frame.To access the depth data,each AR frame will have a new property called sceneDepth.This provides an object of type, ARDepthData.ARDepthData is a container for two buffers.One is a depthMap and the other is a confidenceMap.The depthMap is a CV pixel bufferbut each pixel represents depth and is in meters,and this depth correspondsto the distance from plane of the camerato a point in the world.One thing to note is that the depth mapis smaller in resolution compared to the captured imageon the AR frame which still presents the same aspect ratio.The other buffer on the ARDepthData objectis the confidenceMap.Since the measurement of depth using LiDARis based on the light which reflects from objects,the accuracy of the depth map can be impactedby the nature of the surrounding environment.Challenging surfaces, such as thosewhich are highly reflective or those with high absorption,can lower the accuracy of the depth.This accuracy is expressed through a valuewe call confidence.For each depth pixel,there is a corresponding confidence value of typeARConfidenceLevel,and this value can either be low, medium, or highand will help to filter depthbased on the requirements of your application.Let's see how we can use the depth API.I begin with creating an ARSessionand an ARTrackingConfiguration.There is a new frame semantic called sceneDepth,which allows you to turn on the depth API.As always, I check if the frameSemanticis supported on the device using the supportsFrameSemanticsmethod on the configuration class.Then we can set the frameSemantic to sceneDepthand run the configuration.After this, I can access the depth datafrom the sceneDepth property on ARFrameusing the didUpdate frame delegate method.Additionally if you have an AR appthat uses people occlusion feature,and then search the personSegmentationWithDepthframeSemantic, then you will automatically get sceneDepthon devices that support the sceneDepth frameSemanticwith no additional power cost to your application.Here is a demo of an app that we built using the depth API.The depth from the depthMap is unprojected to 3Dto form a point cloud.The point cloud is coloredusing the captured image on the ARFrame.By accumulating depth data across multiple AR frames,you get a dense 3D point cloud like the one we see here.I can also filter the point cloudsbased on the confidence level.This is the point cloud formed by all the depth pixelsincluding those with low confidence.And here is the point cloud while filtering depthwith confidence is medium or high.And this is the point cloud we get by using only that depthwhich has high confidence.This gives us a clear pictureof how the physical properties of surfaces can impactthe confidence level of its depth.Your application and its tolerance to inaccuraciesin depth will determine how you will filter the depthbased on its confidence level.Let's take a closer look at how we built this app.For each ARFrame we access,the sceneDepth property with the ARDepth data objects,providing us with the depth and the confidenceMap.The key part of the appis a metal vertex shader called unproject.As the name suggests, it unprojects the depth datafrom the depth map to the 3D spaceusing parameters on the ARCamerasuch as the cameras transform, it's intrinsics,and the projection matrix.The shader also uses captured image to sample colorfor each depth pixel.What we get as an output of thisis the 3D point cloud which is then rendered using Metal.To summarize, we have a new depth API in ARKit 4which gives a highly accurate representation of the world.There is a frame semantic called sceneDepthwhich allows you to enable the feature.Once enabled, the depth data will be available at 60 Hzon each AR frame.The depth data will have a depthMap and a confidenceMap,and the API is supported on devices with the LiDAR scanner.One of the fundamental tasks in many AR apps is placing objects,and in ARKit 3, we introduced the raycasting APIto make object placement easier.In ARKit 4, The LiDAR scannerbrings some great implements to raycasting.Raycasting is highly optimized for object placementand makes it easy to precisely place virtual objectsin your AR app.Placing objects in ARKit 4 is more precise and quicker,thanks to the LiDAR scanner.Your apps that already use raycastingwill automatically benefit on a LiDAR-enabled device.Raycasting also leverages scene depth or scene geometrywhen available to instantly place objects in AR.This works great even on featureless officessuch as white walls.In iOS14, the raycast API is recommended over hit-testingfor object placement.Before you start raycasting,you will need to create a raycast query.A raycast query describes the direction and the behaviorof the ray used for raycasting.It is composed of a raycast target which describesthe type of surface that a ray can intersect with.Existing planes correspond to planes detected by ARKit,while considering the shape and size of the plane.Infinite planes are the same planesbut with the shape and size ignored.And estimated planes are planes of arbitrary orientationformed from the feature points around the surface.The raycasttarget alignment specifies the alignmentof surfaces that a ray can intersect with.This can be horizontal, vkertical, or any.There are two types of raycasts.There are single-shot raycasts which return a one-time result.And then that tracked raycastswhich continuously update the resultsas ARKit's understanding of the world evolves.In order to get the latest features object placementwe are recommending migrating to the raycasting APIas we deprecate hit-testing.The code we see on the top is extracted from a sample appwhich uses hit-testing to place objects.It performs a testwith three different kinds of hit-test options.And it is usually followed by some custom heuristicsto filter those results and figure outwhere to place the object.All of that can be replaced with the few lines of raycasting codelike the one we see belowand ARKit will do the heavy lifting under the hoodto make sure that your virtual objectsalways stay at the right place.Raycasting makes it easier than ever before to precisely placevirtual objects in your ARKit applications.Let's move over to FaceTracking.FaceTracking allows you to detect facesin your front camera AR experience,overlay virtual content on them,and animate facial expressions in real time.This is supported on all devices with the TrueDepth camera.Now with ARKit 4, FaceTracking support is extendedto devices without a TrueDepth camera,as long as they have an Apple A12 bionic processor or later.This includes the devices without the TrueDepth camerasuch as the new iPhone SE.Elements of FaceTracking, such as face anchors,face geometry, and blendshapeswill be available on all supported devicesbut capture depth data will be limited to deviceswith the TrueDepth camera.And that is ARKit 4.With location anchors, you can now bring your AR experiencesonto the global scale.And we looked at how we can use the LiDARto build rich AR apps using the same geometry and the depth API.There are exciting improvements in raycastingto make object placement in AR easier than ever before.And finally, FaceTracking is now supportedon a wider range of devices.Thank you.And we can't wait to check outall the great apps that you will
build using ARKit 4.

♪Voiceover: Hello, and welcome to WWDC.

Quinton: Hi, my name's Quinton,and I'm an engineer on the ARKit team.

Today both Praveen and I get to show yousome of the new features in ARKit with iOS 14.

So let's jump right in and explore ARKit 4.

This release adds many advancements to ARKitwhich already powers the world's largest AR platform, iOS.

ARKit gives you the tools to create AR experiencesthat change the way your users see the world.

Some of these tools include device motion tracking,camera scene capture,and advanced scene processing which all help to simplifythe task of building a realistic and immersive AR experience.

Let's see what's next with ARKit.

So first, we're going to take a look at the location anchor API.

Location anchors bring your AR experienceonto the global scaleby allowing you to position virtual contentin relation to the globe.

Then we'll see whatt he new LiDAR sensorbrings to ARKit with scene geometry.

Scene geometry provides appswith a mesh of the surrounding environmentthat can be used for everything from occlusion to lighting.

Next we'll look at the technologythat enables scene geometry, the depth API.

We're opening up this APIto give apps access to a dense depth mapto enable new possibilities using the LiDAR sensor.

And additionally,the LiDAR sensor improves object placement.

We'll go over some best practicesto make sure your apps take full advantageof the newest object-placement techniques.

And we'll wrap up with some improvements to FaceTracking.

Let's start with location anchors.

Before we get too far,let's look at how we got to this point.

ARKit started on iOS with the best tracking.

No QR codes. No external equipment needed.

Just start an AR experience by placing content around you.

Then we added multi-user experiences.

Your AR content could then be shared with a friendusing a separate device to make experiences social.

And last year we brought people into ARKit.

AR experiences are now aware of the people on the scene.

Motion capture is possible with just a single iOS device,and people occlusion makes AR contenteven more immersiveas people can walk right in front of a virtual object.

All these features combineto make some amazing experiences.

But what's next?So now we're bringing AR into the outdoorswith location anchors.

Location anchors enable youto place AR content in relation to the globe.

This means you can now place virtual objectsand create AR experiences by specifying a latitude,longitude, and altitude.

ARKit will take your geographic coordinatesas well as high-resolution map data from Apple Mapsto place your AR experiences at the specific world location.

This whole process is called visual localization,and it will precisely locate your devicein relation to the surrounding environmentmore accurately than could be done beforewith just GPS.

All this is possibledue to advance machine learning techniquesrunning right on your device.

There's no processing in the cloudand no images sent back to Apple.

ARKit also takes care of merging the local coordinate systemto the geographic coordinate system,so you can work in one unified system,regardless of how you want to create your AR experiences.

To access these features we've added a new configuration,ARGeoTrackingConfiguration and ARGeoAnchorsare what you'll use to place in contentthe same way as other ARKit anchors.

Let's see some location anchors in action.

We've got a video herein front of the Ferry Building in San Francisco.

You can see a large virtual sculpture.

That's actually the companion sculpture created by KAWS,and viewed in the acute art app.

Since it was placed with location anchors,everyone who uses the app at the Ferry Buildingcan enjoy the virtual art in the same place and the same way.

Let's see what's under the hood in ARKit to make this all work.

So when using geo tracking,we download all the detailed map data from Apple Mapsaround your current location.

Part of this data is a localization mapthat contains feature points of the surrounding areathat can be seen from the street.

Then with the localization map, your current location,and images from your device,we can use advanced machine learningto visually localize and determineyour device's position.

All this is happening under the hood in ARKitto give you a precise, globally-aware posewithout worrying about any of this complexity.

The location anchor APIcan be broken down into three main parts.

ARGeoTrackingConfiguration is the configurationthat you'll use to take advantageof all of the new location anchor features.

This configuration contains a subsetof the world-tracking featuresthat are compatible with geo tracking.

Then once you've started an AR sessionwith the geo-tracking configuration,you'll be able to create ARGeoAnchorsjust like any other ARKit anchor.

And also while using geo tracking,there's a new tracking status that's important to monitor.

This is contained in ARGeoTrackingStatusand provides valuable feedbackto improve the geo-tracking experience.

So building an app with location anchorscan be broken down into a few steps.

The first is checking availability of geo tracking.

ARGeoTrackingConfiguration has a few methodsthat let us check the preconditionsusing the rest of the location anchor API.

Then location anchors can be addedonce you know there's full geo-tracking support.

And after anchors are added,we can use a rendering engine to place virtual content.

We'll then need take care of geo-tracking transitions.

Once started, geo tracking will move through a few statesthat may need some user interventionto ensure the best geo-tracking experience.

So let's build a simple point of interest appto see what these steps look like in practice.

In our app, we're going to start with helping our usersfind the iconic Ferry Building in San Francisco, California.

As you can see,we've placed a sign to make the building easy to spot.

To begin the app,let's first start with checking availability.

As with many ARKit features,we need to make sure the current device is supportedbefore attempting to start an experience.

Location anchors are availableon devices with an A12 bionic chipand newer, as well as GPS.

ARGeoTrackingConfigurations isSupported class methodshould be used to check for the support.

For geo tracking,we also need to check if the current location is supported.

We need to be in a locationthat has all the required maps data to localize.

The geo tracking configurationhas a method to check your current location supportas well as an arbitrary latitude and longitude.

Additionally, once a geo tracking session is started,ARKit will ask a user for permissionfor both camera and location.

ARKit has always asked for camera permission,but location permission is needed to do geo tracking.

Let's see what this looks like in code.

ARGeoTrackingConfiguration has all the class methodsthat we need to check before starting our AR session.

We'll first check if the current device is supportedwith "isSupported,"and then we'll checkif our current location is available for geo trackingwith "checkAvailability."If this check fails,we'll get an error with more info to display to the user.

For example, if the user hasn't given the app location permissions.

Then once we know our current deviceand location are supported,we can go ahead and start the session.

Since we're using RealityKit, we'll need our ARViewand then update the configuration.

By default, ARView uses a world-tracking configuration,and so we need to pass in a GeoTrackingConfigurationwhen running the session.

The next step is adding a location anchor.

To do this, we'll use the new ARAnchor subclass ARGeoAnchor.

geoAnchors are similar to existing ARKit anchorsin many ways.

However, because geoAnchors operate on global coordinates,we can't create them with just transforms.

We need to specify their geographic coordinateswith latitude, longitude, and altitude.

The most common way to create geoAnchorswill be through specifying just latitude and longitude,which this allows ARKit to fill in the altitudebased on maps data of the ground-level.

Let's now add a location anchor to our point of interest app.

So for our app,we need to start by finding the Ferry Building's location.

One way we can get the latitude and longitudeis through the maps app.

When we place the marker in the maps app,we now get up to six digits of precision after the decimal.

It's important to use six or more digitsso that we get a precise location to place our contents.

Once we have a latitude and longitude,we can make a geoAnchor.

We don't need to specify an altitudebecause we'll let ARKit use maps datato determine the elevation of the ground level.

Then we'll add the geoAnchor to our session.

And since we're using RealityKit to render our virtual content,and we've already created our geoAnchor,we can go ahead and attach the anchor to an entityto mark the Ferry Building.

Let's run our app and see what it looks like.

We'll start near the Ferry Building in San Franciscolooking towards Market Street.

And as we pan around,we can see some of the palm trees that line the city.

And soon the Ferry Building will come into view.

Our sign looks to be on the ground, which is expected,but the text is rotated.

Since we'd like to find the Ferry Building easilyfrom a distance,we'd really like to have the sign floatinga few meters in the air and facing towards the city.

So how do we do this?Well, to position this content,we need to first look at the coordinate's systemof the geoAnchor.

geoAnchors are fixed to cardinal directions.

Their axes are set when you create the anchor,and this orientation will remain unchangedfor the rest of the session.

A geoAnchors X-axis is always pointed east,and the Z-axis is always pointed southfor any geographic coordinate.

Since we're using a right-handed coordinate system,this leaves positive y pointing up away from the ground.

geoAnchors, like all other ARKit anchorsare immutable.

This means we'll need to use our rendering engineto rotate or translate our virtual objectsfrom the geoAnchors origin.

Let's clean up our signthat we placed in front of the Ferry Building.

So here's some RealityKit code to start updating our sign.

After getting the signEntityand adding it to the geoAnchor entity,we want to rotate the sign towards the city.

To do this,we'll rotate it by a little less than 90 degrees clockwiseand we'll elevate the sign's position by 35 meters.

Both of these operations are in relation to the geoAnchor entitythat we had previously created.

Let's see what this looks like in the app.

Now when we pan around and we get to our Ferry Building,our sign is high in the air,and we can see it from a distance.

The text is much easier to read in this orientation.

This looks great,but we're missing some crucial information hereabout the geo-tracking state that we can useto guide the user to the best geo-tracking experience.

When using a GeoTrackingConfiguration,there's a new GeoTrackingStatus objectthat's available on ARFrame and ARsession observer.

ARGeoTrackingStatus encapsulatesall the current state information of geo tracking,similar to the world-tracking informationthat's available on ARCamera.

Within geo tracking status, there is a state.

This state indicateshow far along geo tracking is during localization.

There's also a property that provides more informationabout the current localization statecalled geo Tracking State Reason,and there's an accuracy provided once geo tracking localizes.

Let's take a closer look at the geo tracking state.

When an AR session begins,geoTrackingState starts at initializing.

At this point, geo tracking is waiting for the world tracking to initialize.

From initializing,the tracking state can immediately go to not availableif geo tracking isn't supported in the current location.

If you're using the checkAvailability class methodon geoTrackingConfiguration,you should rarely get into the state.

Once geo tracking moves to localizing,ARKit is receiving images as well as maps dataand is trying to compute both.

However, during boththe initializing and localizing states,there could be issues detected that prevent localization.

These issues are communicated through geoTrackingStateReason.

This reason should be used to inform the userhow to help geo tracking localize.

Some possible reasons include the device is pointed too low,which would then inform the user to raise the device,or geoDataNotLoaded,and we'd inform the user that a network connection is required.

For all possible reasons,have a look at ARGeoTrackingTypes.h.

In general we want to encourage usersto point their devices at buildingsand other stationary structuresthat are visible from the street.

Parking lots, open fields,and other environments that dynamically changehave a lower chance of localizing.

After addressing any geoTrackingStateReasons,geo tracking should become localized.

It's at this point that you should start your AR experience.

If you place objects before localization,the objects could jump to unintended locations.

Additionally once localized ARGeoTrackingAccuracyis provided to help you gaugewhat experiences should be enabled.

It's also important to always monitor geo tracking stateas it's possible for geo trackingto move back to localizing or even initializingsuch as when tracking is lost or map data isn't available.

Let's take a look at how we can add this tracking stateto improve our sample app.

Now we can see this whole time we were actually localizingwhen looking at Market Street and the surrounding buildings.

As we pan around,we can see from the tracking state that we localizeand then the accuracy increases to high.

I think we've got our app just about ready,at least for the Ferry Building.

So we've added a more expansive location anchor sample projecton developer.apple.comthat I encourage you to check out after this talk.

For more information on the RealityKit features used,check out last year's talkintroducing RealityKit and Reality Composer.

In our sample app,we saw how to create location anchorsby directly specifying coordinates.

We already knew the geographic coordinatesfor the Ferry Building.

However, these coordinates could have come from any source,such as our app bundle, our web backend,or really any database.

Another way to create a location anchoris via user interaction.

We could expand on our app in the futureby allowing users to tap the screento save their own point of interest.

getGeoLocation(for point) on ARSessionallows us to get geographic coordinatesfrom any world point in ARKit coordinate space.

For example, this could have come from a raycast,or location on a plane.

Location anchors are available for you today with iOS14,and we're starting with support in the San Francisco Bay Area,New York, Los Angeles, Chicago, and Miami,with more cities coming through the summer.

All iPhones and iPads with an A12 bionic chip or newer,as well as GPS, are supported.

Also, for any apps that require location anchors exclusively,you can use device capability keysto limit your app in the App Storeto only compatible hardware.

In addition to the GPS key, you'll need to use the new keyfor devices with an A12 bionic chip or newerthat is available in iOS14.

So with location anchors,you can now bring your AR experiencesonto the global scale.

We went over how ARGeoTrackingConfigurationis the entry point to adding location anchors to your app.

We saw how to add ARGeoAnchors to your ARSceneand how to position content in relation to those anchors.

We also saw how ARGeoTrackingStatus can be usedto help guide the user to the best geo-tracking experience.

And now here's Praveento tell you more about scene geometry.

Praveen Gowda: Hi everyone.

I'm Praveen Gowda. I'm an engineer on the ARKit team.

Today I'm going to take you throughsome of the APIs available in iOS14 that helpbring the power of the LiDAR scanner to your applications.

In ARKit 3.5, we introduced the scene geometry APIpowered by the LiDAR scanner on the new iPad Pro.

Before we go into scene geometry,let's take a look at how the LiDAR scanner works.

The LiDAR shoots light onto the surroundingsand then collects the light reflected off the surfacesin the scene.

The depth is estimated by measuring the time it tookfor the light to go from the LiDAR to the environmentand reflect back to the scanner.

And this entire process runs millions of times every second.

The LiDAR scanner is used by the same geometry APIto provide a topological map of the environment.

This can be optionally fused with semantic classificationwhich enables apps to recognize and classify physical objects.

This provides an opportunity for creating richer AR experienceswhere apps can now upload virtual objectswith the real world or use physics to enablerealistic interactions between virtual and physical objects.

Or to use virtual lighting on real world surfacesand in many other use cases that we were to imagine.

Let's take a quick look at scene geometry in action.

Here is a living roomand once the scene geometry API is turned onthe entire visible room is meshed.

Triangles vary in size to show the optimum detailfor each surface.

The color mesh appearsonce semantic classification is enabled.

Each color represents a different classificationsuch as blue for the seats and green for the floor.

As we saw, the scene geometry feature is builtby leveraging the depth data gathered from the LiDAR scanner.

In iOS14, we have a new ARKit depth APIthat provides access to the same depth data.

The API provides a dense depth imagewhere a pixel in the image corresponds to depth in metersfrom the camera.

What we see here is a debug visualization of this depthwhere there's a gradient from blue to red,where blue represents regions closer to the cameraand red represents those far away.

The depth data would be available at 60 Hz,associated with each AR frame.

The scene geometry feature is built on top of this APIwhere depth data across multiple frames are aggregatedand processed to construct a 3D mesh.

This API is powered by the LiDAR scannerand is available on devices which have LiDAR.

Here is an illustration of how the depth map is generated.

The colored RGB image from the wide-angle cameraand the depth ratings from the LiDAR scannerare fused together using advanced machine learningalgorithms to create a dense depth mapthat is exposed through the API.

This operation runs at 60 times per secondwith the depth map available on every AR frame.

To access the depth data,each AR frame will have a new property called sceneDepth.

This provides an object of type, ARDepthData.

ARDepthData is a container for two buffers.

One is a depthMap and the other is a confidenceMap.

The depthMap is a CV pixel bufferbut each pixel represents depth and is in meters,and this depth correspondsto the distance from plane of the camerato a point in the world.

One thing to note is that the depth mapis smaller in resolution compared to the captured imageon the AR frame which still presents the same aspect ratio.

The other buffer on the ARDepthData objectis the confidenceMap.

Since the measurement of depth using LiDARis based on the light which reflects from objects,the accuracy of the depth map can be impactedby the nature of the surrounding environment.

Challenging surfaces, such as thosewhich are highly reflective or those with high absorption,can lower the accuracy of the depth.

This accuracy is expressed through a valuewe call confidence.

For each depth pixel,there is a corresponding confidence value of typeARConfidenceLevel,and this value can either be low, medium, or highand will help to filter depthbased on the requirements of your application.

Let's see how we can use the depth API.

I begin with creating an ARSessionand an ARTrackingConfiguration.

There is a new frame semantic called sceneDepth,which allows you to turn on the depth API.

As always, I check if the frameSemanticis supported on the device using the supportsFrameSemanticsmethod on the configuration class.

Then we can set the frameSemantic to sceneDepthand run the configuration.

After this, I can access the depth datafrom the sceneDepth property on ARFrameusing the didUpdate frame delegate method.

Additionally if you have an AR appthat uses people occlusion feature,and then search the personSegmentationWithDepthframeSemantic, then you will automatically get sceneDepthon devices that support the sceneDepth frameSemanticwith no additional power cost to your application.

Here is a demo of an app that we built using the depth API.

The depth from the depthMap is unprojected to 3Dto form a point cloud.

The point cloud is coloredusing the captured image on the ARFrame.

By accumulating depth data across multiple AR frames,you get a dense 3D point cloud like the one we see here.

I can also filter the point cloudsbased on the confidence level.

This is the point cloud formed by all the depth pixelsincluding those with low confidence.

And here is the point cloud while filtering depthwith confidence is medium or high.

And this is the point cloud we get by using only that depthwhich has high confidence.

This gives us a clear pictureof how the physical properties of surfaces can impactthe confidence level of its depth.

Your application and its tolerance to inaccuraciesin depth will determine how you will filter the depthbased on its confidence level.

Let's take a closer look at how we built this app.

For each ARFrame we access,the sceneDepth property with the ARDepth data objects,providing us with the depth and the confidenceMap.

The key part of the appis a metal vertex shader called unproject.

As the name suggests, it unprojects the depth datafrom the depth map to the 3D spaceusing parameters on the ARCamerasuch as the cameras transform, it's intrinsics,and the projection matrix.

The shader also uses captured image to sample colorfor each depth pixel.

What we get as an output of thisis the 3D point cloud which is then rendered using Metal.

To summarize, we have a new depth API in ARKit 4which gives a highly accurate representation of the world.

There is a frame semantic called sceneDepthwhich allows you to enable the feature.

Once enabled, the depth data will be available at 60 Hzon each AR frame.

The depth data will have a depthMap and a confidenceMap,and the API is supported on devices with the LiDAR scanner.

One of the fundamental tasks in many AR apps is placing objects,and in ARKit 3, we introduced the raycasting APIto make object placement easier.

In ARKit 4, The LiDAR scannerbrings some great implements to raycasting.

Raycasting is highly optimized for object placementand makes it easy to precisely place virtual objectsin your AR app.

Placing objects in ARKit 4 is more precise and quicker,thanks to the LiDAR scanner.

Your apps that already use raycastingwill automatically benefit on a LiDAR-enabled device.

Raycasting also leverages scene depth or scene geometrywhen available to instantly place objects in AR.

This works great even on featureless officessuch as white walls.

In iOS14, the raycast API is recommended over hit-testingfor object placement.

Before you start raycasting,you will need to create a raycast query.

A raycast query describes the direction and the behaviorof the ray used for raycasting.

It is composed of a raycast target which describesthe type of surface that a ray can intersect with.

Existing planes correspond to planes detected by ARKit,while considering the shape and size of the plane.

Infinite planes are the same planesbut with the shape and size ignored.

And estimated planes are planes of arbitrary orientationformed from the feature points around the surface.

The raycasttarget alignment specifies the alignmentof surfaces that a ray can intersect with.

This can be horizontal, vkertical, or any.

There are two types of raycasts.

There are single-shot raycasts which return a one-time result.

And then that tracked raycastswhich continuously update the resultsas ARKit's understanding of the world evolves.

In order to get the latest features object placementwe are recommending migrating to the raycasting APIas we deprecate hit-testing.

The code we see on the top is extracted from a sample appwhich uses hit-testing to place objects.

It performs a testwith three different kinds of hit-test options.

And it is usually followed by some custom heuristicsto filter those results and figure outwhere to place the object.

All of that can be replaced with the few lines of raycasting codelike the one we see belowand ARKit will do the heavy lifting under the hoodto make sure that your virtual objectsalways stay at the right place.

Raycasting makes it easier than ever before to precisely placevirtual objects in your ARKit applications.

Let's move over to FaceTracking.

FaceTracking allows you to detect facesin your front camera AR experience,overlay virtual content on them,and animate facial expressions in real time.

This is supported on all devices with the TrueDepth camera.

Now with ARKit 4, FaceTracking support is extendedto devices without a TrueDepth camera,as long as they have an Apple A12 bionic processor or later.

This includes the devices without the TrueDepth camerasuch as the new iPhone SE.

Elements of FaceTracking, such as face anchors,face geometry, and blendshapeswill be available on all supported devicesbut capture depth data will be limited to deviceswith the TrueDepth camera.

And that is ARKit 4.

With location anchors, you can now bring your AR experiencesonto the global scale.

And we looked at how we can use the LiDARto build rich AR apps using the same geometry and the depth API.

There are exciting improvements in raycastingto make object placement in AR easier than ever before.

And finally, FaceTracking is now supportedon a wider range of devices.

Thank you.

And we can't wait to check outall the great apps that you will
build using ARKit 4.

6:58 -Availability

8:38 -Adding Location Anchors

10:32 -Positioning Content

14:08 -User Interactive Location Anchors

20:32 -Enabling the Depth API

21:12 -Depth API alongside person occlusion

25:41 -Raycasting

## Code Samples

```swift
// Check device support for geo-tracking


guard
 
ARGeoTrackingConfiguration
.isSupported 
else
 {
    
// Geo-tracking not supported on this device

    
return

}


// Check current location is supported for geo-tracking


ARGeoTrackingConfiguration
.checkAvailability { (available, error) 
in

    
guard
 available 
else
 {
        
// Geo-tracking not supported at current location

        
return

    }
    
// Run ARSession

    
let
 arView 
=
 
ARView
()
    arView.session.run(
ARGeoTrackingConfiguration
())
}
```

```swift
// Create coordinates


let
 coordinate 
=
 
CLLocationCoordinate2D
(latitude: 
37.795313
, longitude: 
-
122.393792
)


// Create Location Anchor


let
 geoAnchor 
=
 
ARGeoAnchor
(name: 
"Ferry Building"
, coordinate: coordinate)


// Add Location Anchor to session

arView.session.add(anchor: geoAnchor)


// Create a RealityKit anchor entity 


let
 geoAnchorEntity 
=
 
AnchorEntity
(anchor: geoAnchor)


// Anchor content under the RealityKit anchor

geoAnchorEntity.addChild(generateSignEntity())


// Add the RealityKit anchor to the scene

arView.scene.addAnchor(geoAnchorEntity)
```

```swift
// Create a new entity for our virtual content


let
 signEntity 
=
 generateSignEntity();


// Add the virtual content entity to the Geo Anchor entity

geoAnchorEntity.addChild(signEntity)


// Rotate text to face the city


let
 orientation 
=
 simd_quatf.
init
(angle: 
-
Float
.pi 
/
 
3.5
, axis: 
SIMD3
<
Float
>(
0
, 
1
, 
0
))
signEntity.setOrientation(orientation, relativeTo: geoAnchorEntity)


// Elevate text to 35 meters above ground level


let
 position 
=
 
SIMD3
<
Float
>(
0
, 
35
, 
0
)
signEntity.setPosition(position, relativeTo: geoAnchorEntity)
```

```swift
let
 session 
=
 
ARSession
()

let
 worldPosition 
=
 raycastLocationFromUserTap()
session.getGeoLocation(forPoint: worldPosition) { (location, altitude, error) 
in

    
if
 
let
 error 
=
 error {
        
...

    }
    
let
 geoAnchor 
=
 
ARGeoAnchor
(coordinate: location, altitude: altitude)
}
```

```swift
// Enabling the depth API



let
 session 
=
 
ARSession
()

let
 configuration 
=
 
ARWorldTrackingConfiguration
()


// Check if configuration and device supports .sceneDepth


if
 
type
(of: configuration).supportsFrameSemantics(.sceneDepth) {
    
// Activate sceneDepth

    configuration.frameSemantics 
=
 .sceneDepth
}
session.run(configuration)


...



// Accessing depth data


func
 
session
(
_
 
session
: 
ARSession
, 
didUpdate
 
frame
: 
ARFrame
)
 {
    
guard
 
let
 depthData 
=
 frame.sceneDepth 
else
 { 
return
 }
    
// Use depth data

}
```

```swift
// Using the depth API alongside person occlusion



let
 session 
=
 
ARSession
()

let
 configuration 
=
 
ARWorldTrackingConfiguration
()


// Set required frame semantics


let
 semantics: 
ARConfiguration
.
FrameSemantics
 
=
 .personSegmentationWithDepth
        

// Check if configuration and device supports the required semantics


if
 
type
(of: configuration).supportsFrameSemantics(semantics) {
    
// Activate .personSegmentationWithDepth

    configuration.frameSemantics 
=
 semantics
}
session.run(configuration)
```

```swift
let
 session 
=
 
ARSession
()
hitTest(point, types: [.existingPlaneUsingGeometry,
                       .estimatedVerticalPlane,
                       .estimatedHorizontalPlane])


let
 query 
=
 arView.makeRaycastQuery(from: point,
                                    allowing: .estimatedPlane,
                                    alignment: .any)


let
 raycast 
=
 session.trackedRaycast(query) { results 
in

   
// result updates

}
```

