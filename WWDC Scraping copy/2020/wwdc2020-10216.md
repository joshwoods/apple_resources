# Wwdc2020 10216

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

What's new in ResearchKitResearchKit continues to simplify how developers build research and care apps. Explore how the latest ResearchKit updates expand the boundaries of data researchers can collect. Learn about features like enhanced onboarding, extended options for surveys, and new active tasks. Discover how Apple has partnered with the research community to leverage this framework, helping developers build game-changing apps that empower care teams and the research community.ResourcesResearch and Care WebsiteHD VideoSD VideoRelated VideosWWDC20Beyond counting stepsWhat's new in CareKit

ResearchKit continues to simplify how developers build research and care apps. Explore how the latest ResearchKit updates expand the boundaries of data researchers can collect. Learn about features like enhanced onboarding, extended options for surveys, and new active tasks. Discover how Apple has partnered with the research community to leverage this framework, helping developers build game-changing apps that empower care teams and the research community.

Research and Care Website

HD VideoSD Video

HD Video

SD Video

Beyond counting steps

What's new in CareKit

Search this videoâ€¦Hello and welcome to WWDC. Hello everybody and welcome to What's Newin ResearchKit. My name is Pariece McKinney and I'm a software engineeron the health team. Later in the talk, we'll also be joined by my fellowcolleague and software engineer Joey LaBarck. Thank you for taking the timeto join. And we're extremely excited to show you all the new updates ResearchKithas to offer. There's quite a bit to cover so let's jump right in. In orderto make things easier to follow this year we created a ResearchKit stickerpack where each sticker corresponds to a particular topic of this talk.At the end of each topic we'll collect a sticker for that particular subjectand slap it on the back of our laptop which is pretty empty at the moment.Now that everyone knows how to follow along. Let's get started with ourfirst topic, community updates. Each year we're excited about the new appsthat take advantage of our frameworks to advance health and learnings andvarious health areas. To name a few, the Spirit Trial app built by Thread,was created in support of a clinical trial on advanced pancreatic cancer.Also Care Evolution and the NIH launched the All of Us app to speed up healthresearch and breakthroughs by building a community of a million or more peopleacross the U.S. with the aim to advance personalized medicine. We've alsoseen apps utilize our frameworks to build high quality apps very quicklyin response to COVID-19. The Stanford First Responder app aims to help firstresponders navigate the challenges of COVID-19 and the University of NebraskaMedical Center's One Check app aims to provide real-time situational awarenessof COVID-19 to investigators. Last year, Apple also announced and releasedthe Research app which heavily utilizes ResearchKit while paving the wayfor conducting large scale studies all through your iPhone. Last year, wealso announced that we would release a newly redesigned website and wewere proud to share that website with you which launched in late 2019at researchandcare.org. On our overview page you can read about theframeworks and their capabilities and features before diving in to createyour own app. If you navigate to the ResearchKit page you can find evenmore information about the models it provides as well as case studies thatshowcase amazing examples of studies and programs built in the communitylike the one you see here. We also announced our new investigator supportprogram to which researchers can submit proposals for watchers who supporttheir studies. You can now read about that on our website and learn howto reach out to us if you're interested in the program. And lastly wewelcome all of you to reach out to us through our website so that we canhear about all the amazing work you all are doing. Now that we've collectedour community update sticker let's move on to the next topic which is onboarding updates.For the vast majority of study based apps, the onboarding views are usuallythe first thing your participants will see and interact with. Knowing thisis extremely important to convey exactly what the study is and what theparticipants should expect if they decide to join. As you can see herewe move towards leaning on the instruction steps capability to supportcustom text and images so that you can have complete control over the contentyou wish to display. Let's take a look at the code to create this step.After importing ResearchKit, the first thing we'll do is initialize theinstructions step and pass in a unique identifier. After setting the titleand detail text, the last thing we have to do is set the image propertyto the "health_blocks" image seen in the previous slide. In the second stepof our onboarding flow using a instruction step again but this time wealso incorporate body items which is an extremely useful feature to furthereducate your participants. In this example we use SF Symbols for our iconsbut it's important to note that everyone watching this video has accessto these icons and more. So if you're interested feel free to the SF Symbolsapp to find the icons that match your use case. Let's take a look at thecode to create this step. Much like the previous code slide we initializeour ORKInstructionStep and pass in our unique identifier. After setting our title property,we also set our image property again. But this time we pass in an "informed_consent"image seen in the slide before. Next we initialize our first bodyitem making sure that we pass an image and that we set our bodyItemStyleto .image. The last thing we have to do is upend our newly created bodyItemto the bodyItems array that sits on the InstructionStep.Now let's take a look at an enhancement we've made to our web view step.Previously presenting user with an overview the consent document and collectingthe signature for it were handled by two different steps. Now we've addedthe signature capture functionality to the web view step so that you canpresent the overview of the consent document and ask for the participantsignature within the same view. Let's see how this step is created.The first thing we have to do is initialize the ORKWebViewStep passingin an identifier and the html content you wish to display. And the lastthing we have to do is set the showSignatureAfterContent attributethe true. And this will ensure that the signature view is shown below thehtml content when the step is presented. This year we're also introducingthe request permission step. Previously if you wanted to request access tohealth data you would have to do so outside of the ResearchKit flow whichmeans you had to create the necessary views to ask for access and maintainthose views yourself. Now all you have to do is initialize the request permissionstep and pass in the health care types you want access to and we'll dothe heavy lifting requesting the data for you. Now you can do more withless code while making the experience and flow of your app much better.Let's take a look at the code to create the ORKRequestPermissionStep.We start off by creating a set of HKSampleTypes and these representthe types you want right access for. Then we create a set ofHKObjectTypes and these represent the types you want read access for. Next we'll initializethe ORKHealthKitPermissionType making sure that we pass in thehkTypesToWrite and hkTypesToRead sets created above. And the last thingwe have to do is initialize the ORKHealthKitPermissionStep and passingan array of permission types that currently only has our ORKHealthKitPermissionType within it.That brings us to a close for the onboarding update section so let's collectour sticker. Now that we have our sticker let's keep moving forward andtalk about survey enhancements. Before we show any questions we alwayswant to give the user some insight on what the point of the survey is. Todo this using an instruction step again to provide some brief context.But this time we also provide an icon image that is left aligned to the screenwhich is all handled by ResearchKit. And the second step with the onboardingsurvey, we use an ORKFormStep to collect basic information about the participant.But as you can see here we made some UI improvements by using labels todisplay errors as opposed to previously using alerts which didn't alwaysmake for the best user experience. Let's fix those errors and move on tothe next step in our onboarding survey.In the third step we preview the new SESAnswerFormat, which canhelp present scale based questions much like the example here, where weask the user to select the option that they feel best depicts the currentstate of their health. Let's look at the code needed to present this step.The first thing we do is initialize the SESAnswerFormat and pass inthe top rung text and bottom rung text as seen here. The last thing wehave to do is simply initialize the ORKFormItem and pass in the SESAnswerFormatcreated above. In this step we use a continuous scale answer formatand a scale answer format to get information on the participants currentstress level and pain level. In the past, if the user wasn't comfortableenough to answer the question or simply didn't know the answer, they wouldeither leave the question blank or provide an answer that wasn't accuratebecause the question might be required. Now we've added the ability to usethe ORKDontKnow button with select answer formats. This will allowthe participants to select the "I Don't Know" button when they don't wantto answer the questions presented to them. You can also pass in customtext as seen here to replace the default I don't know text. Let's takea look at the code for the second skill question to see how we added thedon't know button and added custom text. First we initialize the ORKScaleAnswerFormatand pass in all the required values. Next we set theshouldShowDontKnowButton attribute to true. Then we set the customDontKnowButtonTextto "Prefer not to answer". And this will override the default text"I Don't Know". The last thing we have to do is initialize ORKFormItemand pass in the scale answer format created above. And the last questionin the survey we use an ORKTextAnswerFormat to collect any additionalinformation the participant thinks we should know. Previously we supportedsetting a maximum character count but there was no visual to let youknow what the limit is or how close they were to approaching it. Now we'veadded a maximum character count label so that the user can have a much betteridea of how much information they can provide and base their responseoff that. We've also added a "Clear" button so that the user can remove anytext that they've typed in. Let's check out the code to make this happen.First we initialize a ORKTextAnswerFormat.Then we begin to set some properties on the answer format such as settingmultipleLines to true. Setting maximumLength to 280. And setting hideCharacterCountLabeland hideClearButton to false to make sure that both of theseUI elements are shown when this step is presented. And the last thing wehave to do is initialize ORKQuestionStep in passing the textAnswerFormatcreated above. At the finish of the Onboarding Survey we presentthe participant with the new ORKReviewViewController. One of the biggestchallenges for any study is making sure that the data entered by the participantis accurate. As humans, making mistakes in our everyday lives it's very common.So when participants fill out surveys it might be safe to assume that asmall mistake might have happened. To help alleviate this problem,ResearchKit now provides a ReviewViewController that will allow the participantto view a breakdown of all the questions they were asked and the responsethey gave. If they want to update any of those questions they can simplyclick edit and update their answer. Let's look at the code to present theReviewViewController. First we initialize the ReviewViewControllerwhich requires us to pass in a task, and a result object which in thiscase we get from the taskViewController object, passed back to us bythe didFinishWithReason delegate method. But keep in mind that you canalso initialize your task separately and also pass in a result that mayhave been saved at an earlier time. Next we set ourself as the delegate.And this requires us to implement The didUpdateResult and didSelectIncompleteCellmethods. Then after setting a reviewTitle and text we'redone creating our first ReviewViewController. Now that we finished reviewingour survey enhancements and collected our well-deserved sticker,let's move on to the next topic which is Active Tasks. Let's first take a lookat the improvements we've made to our hearing task. For the environmentSPL meters that we added a new animation that clearly indicates if you'rewithin the set threshold for background noise as seen here. We also madeupdates to our dB HL tone audiometry step by tweaking the button UI, addingbetter haptics, changing the progress indicator to a label to make it moreclear to the participant how far they've made it. And we also added calibrationdata to support AirPod Pros. Let's collect our hearing sticker before moving on.Now that we have our hearing sticker let's chat about our next topic whichis 3D models. We're running a study giving your participants clear andinformative visuals to explain a specific concept can be invaluable, especiallyif they can also interact with it. Using 3D models to do this is by farone of the best solutions to educate your participants while also increasing engagement.However writing the code necessary to present 3D models and maintainingit can be cumbersome to say the least. So whether you're trying to presentsomething as simple as a human hand or something more complex such as thehuman muscular system, we've made the process of presenting 3D models mucheasier for you by creating two new classes. Those two classes are the ORK3DModelStepand the ORKUSDZModelManager. Using these two classes,you can now quickly present 3D models within your ResearchKit app by first,adding a USDZ file to your Xcode project. Second, creating aUSDZModelManager instance and passing in the name of the desired USDZfile that was imported in your project. And third initialize an ORK3DModelStepwith the USDZModelManager instance and present it.And just like that you can now present high quality 3D models that your userscan interact with and touch without having to maintain any of the code yourself.Before moving on, we wanted to point out that we're well aware that creatingyour own model can take a good amount of time. However, there are modelsaccessible to you online that you can download for free to practice withSo an upcoming example uses a toy robot and a toy drummer 3D model that areboth publicly available at the URL seen here. Let's get started.And the first example, we'll present the toy robot model. Selection has beenenabled and the user is required to touch any part of the model before continuing.In the second example we'll present the drummer model. Selection is disabledwith certain objects on the model have been pre highlighted to draw theuser's attention. The user also has full control to inspect the highlighted areas.Let's take a look at some code to see how simple it is to present a 3D model.First, we initialize our USDZModelManager passing in the name of the USDZfile that we wish to present. Next, we setup a few properties on the ModelManager,such as allowSelection, highlightColor and setting enableContinueAfterSelectionto false to ensure that the user isn't blocked from moving forward.Next we pass an optional array of identifiers where each identifier matches aspecific object on the model we want to highlight before we present it.Then the last thing we do is initialize the ORK3DModelStep and passin the USDZModelManager created above. So some of you might be wondering,why create a ModelManager class instead of just adding that functionalityto the 3DModelStep itself. And the reason is to make the process of creatinga custom 3D model experience much easier for any developer interested indoing so. To understand it further, we need to learn about the parent classof the USDZModelManager which is the ORK3DModelManager. Let's takea look. The ORK3DModelManager class is an abstract class that we've createdwhich shouldn't be used directly. The point of the 3DModelManager is tobe subclassed while requiring the subclass to implement specific featuresthat we believe every 3D model experience should have. So after creatingyour subclass and making sure that these features are handled, you canthen move forward to add all the extra functionality you want. As seenhere with the USDZModelManager. At the talk word in here we definitelybelieve they using the USDZModelManager could create endless possibilitiesfor your ResearchKit app. However, we are open source and we always encouragemembers of the community to contribute to help push ResearchKit forward.With that being said we're excited to announce that someone from the communityhas also taken the opportunity to create their own ModelManager class.BioDigital, an interactive 3D software platform for anatomy visualizationhas provided the ORKBioDigitalModelManager class so that they'realready powerful iOS SDK can now be integrated easily into any ResearchKit project.Some of their features includes: presenting custom models created via theadmin portal, programmatically adding labels, colors, and annotations to anymodel loaded within your app. And since all of BioDigital models are loadedvia the web, you can dynamically add new models to your project withouthaving to update any code. Let's see a couple examples in action. In thefirst example, we use an instruction step to inform the user that we'll presentan interactive human model. This can be used in many situations and mostof us have experienced, such as visiting a physical therapy clinic or theOrthopedic physician's office where you're usually given a piece of paper todescribe your pain or circle the area on some kind of picture. Now wecan get rid of paper and make the experience much better. As you see herewe've loaded our model while also being presented with a card that containsuseful information that can be updated via BioDigital's admin portal or their SDK.Users can also interact with the model so that they can reach and viewthe exact areas of interest. After clicking on the muscle where pain hasbeen experienced we're also presented with a label that can give us evenmore information on that specific organ. This can be updated viaBioDigital's admin portal or locally through their SDK. In the next example weimagine a scenario where a patient has visited a hospital for chest pain.After receiving a C.T. scan, the physician would like to give a visualto show the patient the exact arteries that are experiencing blockages.To do this we'll present an interactive 3D human heart model with dynamicallyadded annotations to specific coronary arteries all done directly throughBioDigitalModelManager class. As you can see here, we presented our heartmodel along with another card view for additional information. The usercan then interact with the heart model and select the programmatically added annotationsto find more information on the severity of each individual blockage.Let's take a look at the code to present the animated heart model.After importing ResearchKit and HumanKit which is provided by BioDigitalwe first initialize ORKBioDigitalModelManager instance...Then we set a couple of properties that were inherited from the ORK3DModelManagerclass such as highlight color and identifiers of objects to highlight.Then we focus on some properties and instance methods added by BioDigital,such as identifiers of objects to hide, the load method where we pass in the I.D.of the model we want to present, in this case the heart model. And the annotatemethod where we pass an identifier the object we want to annotate. In thiscase the right coronary artery. After setting the title in text the lastthing we have to do is initialize the ORK3DModelStep and pass in thebioDigitalModelManager created above. To find out more information aboutBioDigital and their SDK visit their GitHub page, seen here. Now thatwe collected our 3D model sticker I'll hand things over to my teammate Joeyto talk about building a custom active task. Take it away Joey.Thanks Pariece for those awesome updates coming to ResearchKit. Today,I'm going to be showing you how to create your very own custom active task.So we've collected a bunch of stickers already. So to collect our frontfacing camera sticker, we will walk through the process of creating an activetask in ResearchKit. Then we will open Xcode and implement a customapplication to show off our new task. Our task is going to show the usera preview of what they're recording in real time. We'll let the user controlwhen to start and stop recording and show a timer for how long they'vebeen recording for. Additionally, the user will have the opportunity toreview and retry the recording in case they want another take. Before weget into it, I want to give you a quick refresher on the relevant classesand protocols included in ResearchKit to help you accomplish this.First your application needs to create an ORKTask object. ORKTask is a protocolwhich your app can use to reference a collection of various step objects.Most applications can use the concrete ORKOrdered or ORKNavigableOrdertask included in ResearchKit. The task object you create is than injectedinto an ORKTaskViewController object. This object is responsible forshowing each step in your task as they are dequeued. Your application has noneed to subclass ORKTaskViewController so you can use it as is.Additionally ORKTaskViewController is a subclass of UIViewControllerinternally so you can present it in your app as you normally would anyother view controller in UIKit. Finally, the ORKTaskResult isan object which contains the aggregate results for each step in your task.The results that are collected from the task are then delegated back toyour application upon completion of the task using the ORKTaskViewControllerdelegate. This is the essential roundtrip from your applicationinto ResearchKit and then back. Since I'm going to be showing you how tocreate your very own active task we need to dive one level deeper withsome coding examples that will set up our active task. So first our applicationneeds to create a collection of ORKStep and ORKActiveStep objects tomake up the data model of our task. Since we will be creating a front facingcamera active task we will really create a task which includes an activestep subclass. First import the ORKActiveStep header from ResearchKitand create a new subclass of ORKActiveStep. We'll name this classORKFrontFacingCameraStep. I'm also going to add three additional propertieshere I would like to configure. An NSTimeInterval to limit the maximumduration we want to record for and two booleans for allowing the user toreview the recording as well as allowing them to retry the recording.Next, we will declare the view controller type to display when the step is dequeued.In the case of an ORKActiveStep there should be a subclass ofORKActiveStepViewController which you can implement similar to any UIViewControllerin UIKit. The ORKTaskViewController presenting your task is responsiblefor instantiating the associated view controller of each step in your task.Here's a quick look at the interface of ORKFrontFacingCamera stepview controller which subclasses ORKActiveStepViewController. In ourORKFrontFacingCameraStep we will declare the type of view controllerto associate with the step so we can override the stepViewControllerClassmethod of the superclass. In this case we will return theORKFrontFacingCameraStepViewController class object. You can use a customUIView to represent the content of your step. So here our ORKFrontFacingCameraStepContentViewis a simple subclass of UIView. I've declaredsome view events here which we can use to pass relevant events back toour view controller as well as a block typedef. Inside of our interfacewe have a method to set the block parameter to invoke when events are passedfrom the contentView. Since we want to give the user a preview of therecording in real time we will pass the AVCaptureSession to the contentview and internally the contentView will setup an AV capturevideo preview wire. And finally we've added a method to start our timer with amaximum duration as well as a method to show certain recording optionsbefore submitting. We can now add this contentView into our view controller.We already have a reference to an AVCaptureSession which is initializedin another method. We also have a property to reference ourORKFrontFacingCameraStepContentView. By the time we reach viewDidLoad we areready to initialize our contentView. Next we will handle events comingfrom our contentView. We'll use weakSelf here to avoid a reference cycle.We will add our contentView as a subview. And finally we will set thepreview layer session using our AVCaptureSession from before.After our step finishes ORKActiveStepViewController asks for theORKStepResult. This is your view controller's call to add the appropriateresults and any data you collect delegated back to the application whenyour step finishes. In our case ORKFrontFacingCameraStepResultis going to be a subclass of ORKFileResult. We've also added an integerproperty so we can keep track of how many times the user deleted and retriedtheir recording. If we revisit the view controller we override the superclass'method result method to append our custom ORKFrontFacingCameraStepResultFirst we create an instance of our ORKFrontFacingCameraStepResult.Then we set the relevant parameters such as the identifier,contentType, retryCount, as well as the file URL. Finally we appendour new result into the current results collection and return. This effectivelycompletes the implementation of our custom active task. Let's jump intoXcode and try it out.So here I have a demo application that I've been working on which includesResearchKit as a sub module. So I'm good to go ahead and I'm going tocreate a method which allows us to construct and present our task. Insideof this method, we're going to instantiate our steps that are part of our task.So I'm going to go ahead and create an instruction step which welcomesthe user to the task. Then I'm going to use the ORKFrontFacingCameraStepthat we just created. We'll set the maximum recording limit to aboutfifteen seconds and we'll allow the user to retry and review their recording.Then we'll go ahead and add a completion step thanking the user for their time.So now that we have all of our steps we'll go ahead and create an ORKTaskObject.So here we have an ORKOrderedTask and we'll include all the steps from before.Then create a taskViewController object injecting our task as well.Then present this task view controller. In viewDidAppear, we can go aheadand present FrontFacingCameraActiveTask and conform to the delegatethe ORKTaskViewController delegate. Then we can make ourselves the delegatefor the task view controller. Then respond to the didFinishWith reasonORKTaskViewControllerDelegate method. Inside of this method we'regoing to check to see if the currently presented view controller is thetask controller and then dismiss it. And we'll go ahead and try to extractthe ORKFrontFacingCameraStepResult. And once we have that resultobject we can go ahead and print the recording file URL as well asthe retry count. So let's go ahead and run this on the device. Okay, so herewe have our instruction step that we created and we're welcome to WWDC.So here we have a preview of your session which we can see are recordingin real time. We'll go ahead and click get started. So here we have ourfront facing camera step and I'll go ahead and create a recording.Hello and welcome to WWDC. So let's go ahead and review this video.Hello and welcome to WWDC.Ok, let's go ahead and just retry that because I didn't like that.Hello and welcome to WWDC. I think that one was good. So go ahead and submit.Here's our completion step thanking the user and we'll go ahead and exitthe task gracefully.If we go back to Xcode we should be able to see in the console that wehave printed the recording file URL as well as the retry count.This concludes our demonstration for today and we have implemented our own customactive step in ResearchKit and constructed the task in our application.We then extracted the results object to verify our results. We hope you enjoyed.Thank you. Pariece, back to you.Thank you for that demo Joey. We hope everyone viewing enjoyed it and we'revery excited to see what you can do with the new front facing camera stepor any task that you decide to create yourself. Now that we collected ourfront facing camera step sticker that brings us to a close to all of ourResearchKit updates this year. But before moving on let's go over allthe stickers we collected throughout our talk. First we talked about community updateswhere we mentioned a few apps that have leveraged our frameworks over the past year,our new website at researchandcare.org, and the new investigatorsupport program. Then we moved on to onboarding updates. We spoke aboutthe new additions such as body items, in line sensor functionalityand the request permission step. Then we talked about survey enhancementswhere we previewed the new era labels, the "I don't know" button and the ReviewViewControllerto name a few. Then we talked about hearing test UI updateswhere we previewed UI enhancements to the environment SPL meter and tone audiometrystep. Then we moved on to 3D models where we went over and previewedthe 3D model step the USDZ model manager and about digital model managerclasses to add 3D models to your app. And last but not least, Joey walked youthrough the process of building your own active task while also previewingthe functionality of the new front facing camera step. We have a prettysolid collection of stickers here but it wouldn't be complete without thefinal ResearchKit sticker. For more information on the topics discussed today,feel free to visit the resources shared here. As always we want to remindeveryone watching that we are open source and we welcome anyone using orinterested in ResearchKit to visit our GitHub repo shown here and contributeto help the framework grow. Thank you again for taking the time to watchour talk. And we're looking forward to see the powerful apps and experiencesyou will create with
ResearchKit. Thank you.

Hello and welcome to WWDC. Hello everybody and welcome to What's Newin ResearchKit. My name is Pariece McKinney and I'm a software engineeron the health team. Later in the talk, we'll also be joined by my fellowcolleague and software engineer Joey LaBarck. Thank you for taking the timeto join. And we're extremely excited to show you all the new updates ResearchKithas to offer. There's quite a bit to cover so let's jump right in. In orderto make things easier to follow this year we created a ResearchKit stickerpack where each sticker corresponds to a particular topic of this talk.At the end of each topic we'll collect a sticker for that particular subjectand slap it on the back of our laptop which is pretty empty at the moment.

Now that everyone knows how to follow along. Let's get started with ourfirst topic, community updates. Each year we're excited about the new appsthat take advantage of our frameworks to advance health and learnings andvarious health areas. To name a few, the Spirit Trial app built by Thread,was created in support of a clinical trial on advanced pancreatic cancer.Also Care Evolution and the NIH launched the All of Us app to speed up healthresearch and breakthroughs by building a community of a million or more peopleacross the U.S. with the aim to advance personalized medicine. We've alsoseen apps utilize our frameworks to build high quality apps very quicklyin response to COVID-19. The Stanford First Responder app aims to help firstresponders navigate the challenges of COVID-19 and the University of NebraskaMedical Center's One Check app aims to provide real-time situational awarenessof COVID-19 to investigators. Last year, Apple also announced and releasedthe Research app which heavily utilizes ResearchKit while paving the wayfor conducting large scale studies all through your iPhone. Last year, wealso announced that we would release a newly redesigned website and wewere proud to share that website with you which launched in late 2019at researchandcare.org. On our overview page you can read about theframeworks and their capabilities and features before diving in to createyour own app. If you navigate to the ResearchKit page you can find evenmore information about the models it provides as well as case studies thatshowcase amazing examples of studies and programs built in the communitylike the one you see here. We also announced our new investigator supportprogram to which researchers can submit proposals for watchers who supporttheir studies. You can now read about that on our website and learn howto reach out to us if you're interested in the program. And lastly wewelcome all of you to reach out to us through our website so that we canhear about all the amazing work you all are doing. Now that we've collectedour community update sticker let's move on to the next topic which is onboarding updates.

For the vast majority of study based apps, the onboarding views are usuallythe first thing your participants will see and interact with. Knowing thisis extremely important to convey exactly what the study is and what theparticipants should expect if they decide to join. As you can see herewe move towards leaning on the instruction steps capability to supportcustom text and images so that you can have complete control over the contentyou wish to display. Let's take a look at the code to create this step.

After importing ResearchKit, the first thing we'll do is initialize theinstructions step and pass in a unique identifier. After setting the titleand detail text, the last thing we have to do is set the image propertyto the "health_blocks" image seen in the previous slide. In the second stepof our onboarding flow using a instruction step again but this time wealso incorporate body items which is an extremely useful feature to furthereducate your participants. In this example we use SF Symbols for our iconsbut it's important to note that everyone watching this video has accessto these icons and more. So if you're interested feel free to the SF Symbolsapp to find the icons that match your use case. Let's take a look at thecode to create this step. Much like the previous code slide we initializeour ORKInstructionStep and pass in our unique identifier. After setting our title property,we also set our image property again. But this time we pass in an "informed_consent"image seen in the slide before. Next we initialize our first bodyitem making sure that we pass an image and that we set our bodyItemStyleto .image. The last thing we have to do is upend our newly created bodyItemto the bodyItems array that sits on the InstructionStep.

Now let's take a look at an enhancement we've made to our web view step.Previously presenting user with an overview the consent document and collectingthe signature for it were handled by two different steps. Now we've addedthe signature capture functionality to the web view step so that you canpresent the overview of the consent document and ask for the participantsignature within the same view. Let's see how this step is created.

The first thing we have to do is initialize the ORKWebViewStep passingin an identifier and the html content you wish to display. And the lastthing we have to do is set the showSignatureAfterContent attributethe true. And this will ensure that the signature view is shown below thehtml content when the step is presented. This year we're also introducingthe request permission step. Previously if you wanted to request access tohealth data you would have to do so outside of the ResearchKit flow whichmeans you had to create the necessary views to ask for access and maintainthose views yourself. Now all you have to do is initialize the request permissionstep and pass in the health care types you want access to and we'll dothe heavy lifting requesting the data for you. Now you can do more withless code while making the experience and flow of your app much better.

Let's take a look at the code to create the ORKRequestPermissionStep.

We start off by creating a set of HKSampleTypes and these representthe types you want right access for. Then we create a set ofHKObjectTypes and these represent the types you want read access for. Next we'll initializethe ORKHealthKitPermissionType making sure that we pass in thehkTypesToWrite and hkTypesToRead sets created above. And the last thingwe have to do is initialize the ORKHealthKitPermissionStep and passingan array of permission types that currently only has our ORKHealthKitPermissionType within it.

That brings us to a close for the onboarding update section so let's collectour sticker. Now that we have our sticker let's keep moving forward andtalk about survey enhancements. Before we show any questions we alwayswant to give the user some insight on what the point of the survey is. Todo this using an instruction step again to provide some brief context.But this time we also provide an icon image that is left aligned to the screenwhich is all handled by ResearchKit. And the second step with the onboardingsurvey, we use an ORKFormStep to collect basic information about the participant.But as you can see here we made some UI improvements by using labels todisplay errors as opposed to previously using alerts which didn't alwaysmake for the best user experience. Let's fix those errors and move on tothe next step in our onboarding survey.

In the third step we preview the new SESAnswerFormat, which canhelp present scale based questions much like the example here, where weask the user to select the option that they feel best depicts the currentstate of their health. Let's look at the code needed to present this step.

The first thing we do is initialize the SESAnswerFormat and pass inthe top rung text and bottom rung text as seen here. The last thing wehave to do is simply initialize the ORKFormItem and pass in the SESAnswerFormatcreated above. In this step we use a continuous scale answer formatand a scale answer format to get information on the participants currentstress level and pain level. In the past, if the user wasn't comfortableenough to answer the question or simply didn't know the answer, they wouldeither leave the question blank or provide an answer that wasn't accuratebecause the question might be required. Now we've added the ability to usethe ORKDontKnow button with select answer formats. This will allowthe participants to select the "I Don't Know" button when they don't wantto answer the questions presented to them. You can also pass in customtext as seen here to replace the default I don't know text. Let's takea look at the code for the second skill question to see how we added thedon't know button and added custom text. First we initialize the ORKScaleAnswerFormatand pass in all the required values. Next we set theshouldShowDontKnowButton attribute to true. Then we set the customDontKnowButtonTextto "Prefer not to answer". And this will override the default text"I Don't Know". The last thing we have to do is initialize ORKFormItemand pass in the scale answer format created above. And the last questionin the survey we use an ORKTextAnswerFormat to collect any additionalinformation the participant thinks we should know. Previously we supportedsetting a maximum character count but there was no visual to let youknow what the limit is or how close they were to approaching it. Now we'veadded a maximum character count label so that the user can have a much betteridea of how much information they can provide and base their responseoff that. We've also added a "Clear" button so that the user can remove anytext that they've typed in. Let's check out the code to make this happen.

First we initialize a ORKTextAnswerFormat.

Then we begin to set some properties on the answer format such as settingmultipleLines to true. Setting maximumLength to 280. And setting hideCharacterCountLabeland hideClearButton to false to make sure that both of theseUI elements are shown when this step is presented. And the last thing wehave to do is initialize ORKQuestionStep in passing the textAnswerFormatcreated above. At the finish of the Onboarding Survey we presentthe participant with the new ORKReviewViewController. One of the biggestchallenges for any study is making sure that the data entered by the participantis accurate. As humans, making mistakes in our everyday lives it's very common.So when participants fill out surveys it might be safe to assume that asmall mistake might have happened. To help alleviate this problem,ResearchKit now provides a ReviewViewController that will allow the participantto view a breakdown of all the questions they were asked and the responsethey gave. If they want to update any of those questions they can simplyclick edit and update their answer. Let's look at the code to present theReviewViewController. First we initialize the ReviewViewControllerwhich requires us to pass in a task, and a result object which in thiscase we get from the taskViewController object, passed back to us bythe didFinishWithReason delegate method. But keep in mind that you canalso initialize your task separately and also pass in a result that mayhave been saved at an earlier time. Next we set ourself as the delegate.And this requires us to implement The didUpdateResult and didSelectIncompleteCellmethods. Then after setting a reviewTitle and text we'redone creating our first ReviewViewController. Now that we finished reviewingour survey enhancements and collected our well-deserved sticker,let's move on to the next topic which is Active Tasks. Let's first take a lookat the improvements we've made to our hearing task. For the environmentSPL meters that we added a new animation that clearly indicates if you'rewithin the set threshold for background noise as seen here. We also madeupdates to our dB HL tone audiometry step by tweaking the button UI, addingbetter haptics, changing the progress indicator to a label to make it moreclear to the participant how far they've made it. And we also added calibrationdata to support AirPod Pros. Let's collect our hearing sticker before moving on.

Now that we have our hearing sticker let's chat about our next topic whichis 3D models. We're running a study giving your participants clear andinformative visuals to explain a specific concept can be invaluable, especiallyif they can also interact with it. Using 3D models to do this is by farone of the best solutions to educate your participants while also increasing engagement.However writing the code necessary to present 3D models and maintainingit can be cumbersome to say the least. So whether you're trying to presentsomething as simple as a human hand or something more complex such as thehuman muscular system, we've made the process of presenting 3D models mucheasier for you by creating two new classes. Those two classes are the ORK3DModelStepand the ORKUSDZModelManager. Using these two classes,you can now quickly present 3D models within your ResearchKit app by first,adding a USDZ file to your Xcode project. Second, creating aUSDZModelManager instance and passing in the name of the desired USDZfile that was imported in your project. And third initialize an ORK3DModelStepwith the USDZModelManager instance and present it.And just like that you can now present high quality 3D models that your userscan interact with and touch without having to maintain any of the code yourself.

Before moving on, we wanted to point out that we're well aware that creatingyour own model can take a good amount of time. However, there are modelsaccessible to you online that you can download for free to practice withSo an upcoming example uses a toy robot and a toy drummer 3D model that areboth publicly available at the URL seen here. Let's get started.And the first example, we'll present the toy robot model. Selection has beenenabled and the user is required to touch any part of the model before continuing.

In the second example we'll present the drummer model. Selection is disabledwith certain objects on the model have been pre highlighted to draw theuser's attention. The user also has full control to inspect the highlighted areas.Let's take a look at some code to see how simple it is to present a 3D model.

First, we initialize our USDZModelManager passing in the name of the USDZfile that we wish to present. Next, we setup a few properties on the ModelManager,such as allowSelection, highlightColor and setting enableContinueAfterSelectionto false to ensure that the user isn't blocked from moving forward.Next we pass an optional array of identifiers where each identifier matches aspecific object on the model we want to highlight before we present it.Then the last thing we do is initialize the ORK3DModelStep and passin the USDZModelManager created above. So some of you might be wondering,why create a ModelManager class instead of just adding that functionalityto the 3DModelStep itself. And the reason is to make the process of creatinga custom 3D model experience much easier for any developer interested indoing so. To understand it further, we need to learn about the parent classof the USDZModelManager which is the ORK3DModelManager. Let's takea look. The ORK3DModelManager class is an abstract class that we've createdwhich shouldn't be used directly. The point of the 3DModelManager is tobe subclassed while requiring the subclass to implement specific featuresthat we believe every 3D model experience should have. So after creatingyour subclass and making sure that these features are handled, you canthen move forward to add all the extra functionality you want. As seenhere with the USDZModelManager. At the talk word in here we definitelybelieve they using the USDZModelManager could create endless possibilitiesfor your ResearchKit app. However, we are open source and we always encouragemembers of the community to contribute to help push ResearchKit forward.With that being said we're excited to announce that someone from the communityhas also taken the opportunity to create their own ModelManager class.

BioDigital, an interactive 3D software platform for anatomy visualizationhas provided the ORKBioDigitalModelManager class so that they'realready powerful iOS SDK can now be integrated easily into any ResearchKit project.Some of their features includes: presenting custom models created via theadmin portal, programmatically adding labels, colors, and annotations to anymodel loaded within your app. And since all of BioDigital models are loadedvia the web, you can dynamically add new models to your project withouthaving to update any code. Let's see a couple examples in action. In thefirst example, we use an instruction step to inform the user that we'll presentan interactive human model. This can be used in many situations and mostof us have experienced, such as visiting a physical therapy clinic or theOrthopedic physician's office where you're usually given a piece of paper todescribe your pain or circle the area on some kind of picture. Now wecan get rid of paper and make the experience much better. As you see herewe've loaded our model while also being presented with a card that containsuseful information that can be updated via BioDigital's admin portal or their SDK.

Users can also interact with the model so that they can reach and viewthe exact areas of interest. After clicking on the muscle where pain hasbeen experienced we're also presented with a label that can give us evenmore information on that specific organ. This can be updated viaBioDigital's admin portal or locally through their SDK. In the next example weimagine a scenario where a patient has visited a hospital for chest pain.After receiving a C.T. scan, the physician would like to give a visualto show the patient the exact arteries that are experiencing blockages.To do this we'll present an interactive 3D human heart model with dynamicallyadded annotations to specific coronary arteries all done directly throughBioDigitalModelManager class. As you can see here, we presented our heartmodel along with another card view for additional information. The usercan then interact with the heart model and select the programmatically added annotationsto find more information on the severity of each individual blockage.Let's take a look at the code to present the animated heart model.

After importing ResearchKit and HumanKit which is provided by BioDigitalwe first initialize ORKBioDigitalModelManager instance...Then we set a couple of properties that were inherited from the ORK3DModelManagerclass such as highlight color and identifiers of objects to highlight.

Then we focus on some properties and instance methods added by BioDigital,such as identifiers of objects to hide, the load method where we pass in the I.D.of the model we want to present, in this case the heart model. And the annotatemethod where we pass an identifier the object we want to annotate. In thiscase the right coronary artery. After setting the title in text the lastthing we have to do is initialize the ORK3DModelStep and pass in thebioDigitalModelManager created above. To find out more information aboutBioDigital and their SDK visit their GitHub page, seen here. Now thatwe collected our 3D model sticker I'll hand things over to my teammate Joeyto talk about building a custom active task. Take it away Joey.

Thanks Pariece for those awesome updates coming to ResearchKit. Today,I'm going to be showing you how to create your very own custom active task.

So we've collected a bunch of stickers already. So to collect our frontfacing camera sticker, we will walk through the process of creating an activetask in ResearchKit. Then we will open Xcode and implement a customapplication to show off our new task. Our task is going to show the usera preview of what they're recording in real time. We'll let the user controlwhen to start and stop recording and show a timer for how long they'vebeen recording for. Additionally, the user will have the opportunity toreview and retry the recording in case they want another take. Before weget into it, I want to give you a quick refresher on the relevant classesand protocols included in ResearchKit to help you accomplish this.First your application needs to create an ORKTask object. ORKTask is a protocolwhich your app can use to reference a collection of various step objects.Most applications can use the concrete ORKOrdered or ORKNavigableOrdertask included in ResearchKit. The task object you create is than injectedinto an ORKTaskViewController object. This object is responsible forshowing each step in your task as they are dequeued. Your application has noneed to subclass ORKTaskViewController so you can use it as is.

Additionally ORKTaskViewController is a subclass of UIViewControllerinternally so you can present it in your app as you normally would anyother view controller in UIKit. Finally, the ORKTaskResult isan object which contains the aggregate results for each step in your task.

The results that are collected from the task are then delegated back toyour application upon completion of the task using the ORKTaskViewControllerdelegate. This is the essential roundtrip from your applicationinto ResearchKit and then back. Since I'm going to be showing you how tocreate your very own active task we need to dive one level deeper withsome coding examples that will set up our active task. So first our applicationneeds to create a collection of ORKStep and ORKActiveStep objects tomake up the data model of our task. Since we will be creating a front facingcamera active task we will really create a task which includes an activestep subclass. First import the ORKActiveStep header from ResearchKitand create a new subclass of ORKActiveStep. We'll name this classORKFrontFacingCameraStep. I'm also going to add three additional propertieshere I would like to configure. An NSTimeInterval to limit the maximumduration we want to record for and two booleans for allowing the user toreview the recording as well as allowing them to retry the recording.Next, we will declare the view controller type to display when the step is dequeued.In the case of an ORKActiveStep there should be a subclass ofORKActiveStepViewController which you can implement similar to any UIViewControllerin UIKit. The ORKTaskViewController presenting your task is responsiblefor instantiating the associated view controller of each step in your task.

Here's a quick look at the interface of ORKFrontFacingCamera stepview controller which subclasses ORKActiveStepViewController. In ourORKFrontFacingCameraStep we will declare the type of view controllerto associate with the step so we can override the stepViewControllerClassmethod of the superclass. In this case we will return theORKFrontFacingCameraStepViewController class object. You can use a customUIView to represent the content of your step. So here our ORKFrontFacingCameraStepContentViewis a simple subclass of UIView. I've declaredsome view events here which we can use to pass relevant events back toour view controller as well as a block typedef. Inside of our interfacewe have a method to set the block parameter to invoke when events are passedfrom the contentView. Since we want to give the user a preview of therecording in real time we will pass the AVCaptureSession to the contentview and internally the contentView will setup an AV capturevideo preview wire. And finally we've added a method to start our timer with amaximum duration as well as a method to show certain recording optionsbefore submitting. We can now add this contentView into our view controller.We already have a reference to an AVCaptureSession which is initializedin another method. We also have a property to reference ourORKFrontFacingCameraStepContentView. By the time we reach viewDidLoad we areready to initialize our contentView. Next we will handle events comingfrom our contentView. We'll use weakSelf here to avoid a reference cycle.

We will add our contentView as a subview. And finally we will set thepreview layer session using our AVCaptureSession from before.After our step finishes ORKActiveStepViewController asks for theORKStepResult. This is your view controller's call to add the appropriateresults and any data you collect delegated back to the application whenyour step finishes. In our case ORKFrontFacingCameraStepResultis going to be a subclass of ORKFileResult. We've also added an integerproperty so we can keep track of how many times the user deleted and retriedtheir recording. If we revisit the view controller we override the superclass'method result method to append our custom ORKFrontFacingCameraStepResultFirst we create an instance of our ORKFrontFacingCameraStepResult.Then we set the relevant parameters such as the identifier,contentType, retryCount, as well as the file URL. Finally we appendour new result into the current results collection and return. This effectivelycompletes the implementation of our custom active task. Let's jump intoXcode and try it out.

So here I have a demo application that I've been working on which includesResearchKit as a sub module. So I'm good to go ahead and I'm going tocreate a method which allows us to construct and present our task. Insideof this method, we're going to instantiate our steps that are part of our task.So I'm going to go ahead and create an instruction step which welcomesthe user to the task. Then I'm going to use the ORKFrontFacingCameraStepthat we just created. We'll set the maximum recording limit to aboutfifteen seconds and we'll allow the user to retry and review their recording.Then we'll go ahead and add a completion step thanking the user for their time.So now that we have all of our steps we'll go ahead and create an ORKTaskObject.So here we have an ORKOrderedTask and we'll include all the steps from before.

Then create a taskViewController object injecting our task as well.

Then present this task view controller. In viewDidAppear, we can go aheadand present FrontFacingCameraActiveTask and conform to the delegatethe ORKTaskViewController delegate. Then we can make ourselves the delegatefor the task view controller. Then respond to the didFinishWith reasonORKTaskViewControllerDelegate method. Inside of this method we'regoing to check to see if the currently presented view controller is thetask controller and then dismiss it. And we'll go ahead and try to extractthe ORKFrontFacingCameraStepResult. And once we have that resultobject we can go ahead and print the recording file URL as well asthe retry count. So let's go ahead and run this on the device. Okay, so herewe have our instruction step that we created and we're welcome to WWDC.So here we have a preview of your session which we can see are recordingin real time. We'll go ahead and click get started. So here we have ourfront facing camera step and I'll go ahead and create a recording.Hello and welcome to WWDC. So let's go ahead and review this video.

Hello and welcome to WWDC.

Ok, let's go ahead and just retry that because I didn't like that.Hello and welcome to WWDC. I think that one was good. So go ahead and submit.Here's our completion step thanking the user and we'll go ahead and exitthe task gracefully.

If we go back to Xcode we should be able to see in the console that wehave printed the recording file URL as well as the retry count.This concludes our demonstration for today and we have implemented our own customactive step in ResearchKit and constructed the task in our application.We then extracted the results object to verify our results. We hope you enjoyed.Thank you. Pariece, back to you.

Thank you for that demo Joey. We hope everyone viewing enjoyed it and we'revery excited to see what you can do with the new front facing camera stepor any task that you decide to create yourself. Now that we collected ourfront facing camera step sticker that brings us to a close to all of ourResearchKit updates this year. But before moving on let's go over allthe stickers we collected throughout our talk. First we talked about community updateswhere we mentioned a few apps that have leveraged our frameworks over the past year,our new website at researchandcare.org, and the new investigatorsupport program. Then we moved on to onboarding updates. We spoke aboutthe new additions such as body items, in line sensor functionalityand the request permission step. Then we talked about survey enhancementswhere we previewed the new era labels, the "I don't know" button and the ReviewViewControllerto name a few. Then we talked about hearing test UI updateswhere we previewed UI enhancements to the environment SPL meter and tone audiometrystep. Then we moved on to 3D models where we went over and previewedthe 3D model step the USDZ model manager and about digital model managerclasses to add 3D models to your app. And last but not least, Joey walked youthrough the process of building your own active task while also previewingthe functionality of the new front facing camera step. We have a prettysolid collection of stickers here but it wouldn't be complete without thefinal ResearchKit sticker. For more information on the topics discussed today,feel free to visit the resources shared here. As always we want to remindeveryone watching that we are open source and we welcome anyone using orinterested in ResearchKit to visit our GitHub repo shown here and contributeto help the framework grow. Thank you again for taking the time to watchour talk. And we're looking forward to see the powerful apps and experiencesyou will create with
ResearchKit. Thank you.

3:24 -instructionStep

4:08 -informedConsentInstructionStep

5:04 -webViewStep

7:43 -sesAnswerFormat

8:47 -scaleAnswerFormItem

9:47 -textAnswerQuestionStep

11:00 -ORKReviewViewController

14:30 -ORK3DModelStep

## Code Samples

```swift
let
 instructionStep 
=
 
ORKInstructionStep
(identifier: 
"InstructionStepIdentifier"
)
instructionStep.title 
=
 
"Welcome!"

instructionStep.detailText 
=
 
"Thank you for joining our study. Tap Next to learn more before signing up."

instructionStep.image 
=
  
UIImage
(named: 
"health_blocks"
)
!
```

```swift
let
 informedConsentInstructionStep 
=
 
ORKInstructionStep
(identifier: 
"ConsentStepIdentifier"
)
informedConsentInstructionStep.title 
=
 
"Before You Join"

informedConsentInstructionStep.image 
=
 
UIImage
(named: 
"informed_consent"
)
!



let
 heartBodyItem 
=
 
ORKBodyItem
(text: exampleText, 
                                detailText: 
nil
, 
                                image: 
UIImage
(systemName: 
"heart.fill"
), 
                                learnMoreItem: 
nil
, 
                                bodyItemStyle: .image)

informedConsentInstructionStep.bodyItems 
=
 [heartBodyItem]
```

```swift
let
 webViewStep 
=
 
ORKWebViewStep
(identifier: 
String
(describing: 
Identifier
.webViewStep), html: exampleHtml)
webViewStep.showSignatureAfterContent 
=
 
true
```

```swift
let
 sesAnswerFormat 
=
 
ORKSESAnswerFormat
(topRungText: 
"Optimal Health"
, 
                                         bottomRungText: 
"Poor Health"
)


let
 sesFormItem 
=
 
ORKFormItem
(identifier: 
"sesIdentifier"
, 
                                      text: exampleText, 
                                      answerFormat: sesAnswerFormat)
```

```swift
let
 scaleAnswerFormat 
=
 
ORKScaleAnswerFormat
(maximumValue: 
10
, minimumValue: 
1
, defaultValue: 
11
, step: 
1
)
scaleAnswerFormat.shouldShowDontKnowButton 
=
 
true

scaleAnswerFormat.customDontKnowButtonText 
=
 
"Prefer not to answer"



let
 scaleAnswerFormItem 
=
 
ORKFormItem
(identifier: 
"ScaleAnswerFormItemIdentifier"
, 
                                      text: 
"What is your current pain level?"
, 
                                      answerFormat: scaleAnswerFormat)
```

```swift
let
 textAnswerFormat 
=
 
ORKAnswerFormat
.textAnswerFormat()
textAnswerFormat.multipleLines 
=
 
true

textAnswerFormat.maximumLength 
=
 
280
;
textAnswerFormat.hideWordCountLabel 
=
 
false

textAnswerFormat.hideClearButton 
=
 
false

        

let
 textAnswerQuestionStep 
=
 
ORKQuestionStep
(identifier: textAnswerIdentifier),
                                             title: exampleTitle,
                                             question: exampleQuestionText,
                                             answer: textAnswerFormat)
```

```swift
let
 reviewVC 
=
 
ORKReviewViewController
(task: taskViewController.task,
                                       result: taskViewController.result,
                                       delegate: 
self
)
reviewVC.reviewTitle 
=
 
"Review your response"

reviewVC.text 
=
 
"Please take a moment to review your responses below. If you need to change any answers just tap the edit button to update your response."
```

```swift
let
 usdzModelManager 
=
 
ORKUSDZModelManager
(usdzFileName: 
"toy_drummer"
)
usdzModelManager.allowsSelection 
=
 
false

usdzModelManager.highlightColor 
=
 .yellow
usdzModelManager.enableContinueAfterSelection 
=
 
false

usdzModelManager.identifiersOfObjectsToHighlight 
=
 arrayOfIdentifiers


let
 threeDimensionalModelStep 
=
 
ORK3DModelStep
(identifier: drummerModelIdentifier,
                                               modelManager: usdzModelManager)
```

