# Wwdc2020 10642

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Build Image and Video Style Transfer models in Create MLBring stylized effects to your photos and videos with Style Transfer in Create ML. Discover how you can train models in minutes that make it easy to bring creative visual features to your app. Learn about the training process and the options you have for controlling the results. And we'll explore the real-time performance of these models by demonstrating three of them simultaneously in ARKit.

To get the most out of this session, you should be familiar with Create ML. For an overview, watch “Introducing the Create ML App.”ResourcesCreate MLHD VideoSD VideoRelated VideosWWDC20Control training in Create ML with SwiftWWDC19Introducing the Create ML App

Bring stylized effects to your photos and videos with Style Transfer in Create ML. Discover how you can train models in minutes that make it easy to bring creative visual features to your app. Learn about the training process and the options you have for controlling the results. And we'll explore the real-time performance of these models by demonstrating three of them simultaneously in ARKit.

To get the most out of this session, you should be familiar with Create ML. For an overview, watch “Introducing the Create ML App.”

Create ML

HD VideoSD Video

HD Video

SD Video

Control training in Create ML with Swift

Introducing the Create ML App

Search this video…Hello and welcome to WWDC. Hey everyone. I'm David a machine learningengineer from the Create ML Team. And in this session we're goingto talk about building Style Transfer models and Create ML. Style Transferis a new machine learning taskavailable this year in the Create ML app. It can be used to blenda style and content image together.Let's see what happens when we apply Style Transfer to this pair of images.Wow that's pretty cool! The stylized result looks great. The model hastransferred the colors shapes and textures from the style image to thecontent image. The color of the style image is very important. The whitebackground and black paint strokes from this style are transferred by themodel to create a black and white result. Paintings aren't the only sourceof style. The model can learn and transfer the tiles from this mosaic as well.Express your creativity by using different patterns from nature. For examplethe ice cracks in this style can be transferred by the model to producea pretty compelling result. These are just a few examples of what you cando with Style Transfer and Create ML. But wouldn't it be great if wecould do even better. What if you could apply Style Transfer to more thanjust a static image. Amazing. Each frame is stylized fast enough to maintaina smooth stylization experience. You can use a Style Transfer model inreal time for your video apps. Using the A13 bionic chip we can achievea processing speed up to 120 frames per second. To train a Style Transfermodel we need to provide some training data, a style image, and a directoryof content images. The model learns the balance between content and stylefrom your images. For optimal results the content images used for trainingshould be similar to what you expect to stylize at inference time. In thisexample I have a directory of natural content images. You can use modelparameters to optimize the behavior of a Style Transfer model. You canconfigure your model behavior with style strength and style density model parameters.Let's explore style strength first. With a style strength parameter.You can tune the balance between style and content. With low style strength,only parts of the background adopt the style image qualities and the peopledancing have adopted only a small amount of blue color. Now with high stylestrength the background is even more stylized and the people dancing completelyadopt the style image colors and ice cracks. Notice the difference whenI put them next to each other. Next let's explore style density. With thestyle density parameter, course details can be learned by the model. Here,I've drawn a grid on top of the style image. Let's zoom in on a region.The model focuses on high level details in this region such as the birdand learns these coarse qualities. Here's an example of coarse stylization.Fine details can also be learned by the model by setting the style densityparameter to a higher setting. With a fine grid the zoomed in region containsmostly color and brush strokes. This produces a different stylized result.Let's compare the coarse and fine results side by side to visualize thedifference. You can use the style density parameter to explore a wide rangeof such stylizations. Let's train a style transfer model together in the CreateML app. I'll open the Create ML app and create a new document.I'll select the new Style Transfer template. I'll name the projectDanceStylizer and fill in a short description. The app has already selectedthe settings tab and is ready for my training data and model parameters.I have this style image that you saw earlier in the session. I'll dragit into the training style image data well. I also have an image of people dancing.I'll use that for validation in order to visualize the model quality throughoutthe training process. Lastly I need a directory of content images to learnthe balance between content and style. I can either download a set of afew hundred natural content images directly from the app, which works ina wide variety of use cases. Or I can use my own folder of content images.I'll drag in a folder of six hundred natural content images. I have theoption to optimize for image or video use cases. I'll choose video sinceI'm interested in real-time Style Transfer apps and I'll train for fourhundred iterations. I can use the style strength slider to control the balancebetween content and style and I can also use the style density sliderto explore coarse or fine stylization. The default parameters work prettywell in most cases. Now that I'm finished configuring my training settingsI'll click the train button in the toolbar. The app processes the style andcontent images and immediately starts training the model. Every five iterationsa new model checkpoint stylizes my validation image. I can use this tovisualize the model stylization interactively through the training process.At any point I can take a model snapshot by clicking the snapshot buttonin the toolbar. A snapshot is an ML model that I can use later in an app.My model snapshots are saved under model sources. Style loss and contentloss graphs helped me understand the balance between content and style.The style loss decreases as my model learns to adopt the artistic qualitiesof my style image. The model has completed training 400 iterations andit looks like the style loss has converged. I can train my model for moreiterations by clicking the train more button in the toolbar. I'm happywith my model stylization on the validation image, so let's go to the preview tab.I can navigate to the preview tab to test out my model with some new data.I'll drag in a test image.I can toggle back and forth between the stylized image and the originalcontent so that I can clearly visualize the stylized effect.I can compare the stylized result from different models snapshots.Since this model was optimized for video, I'll try dragging in a test video.Now this is really getting interesting. At this point I can choose to download,share with a colleague, or open the stylized result in the QuickTime Player app.In the output tab,I can find out more information about my model. The size of the model isquite small, under one megabyte, which makes it really convenient to bundlewith my apps. I can see the OS availability of my model to ensure compatibilityand by selecting predictions I can find out even more about my model suchas input and output layer names. At this point I can get the model to usein my apps by clicking the get, open in Xcode, or share button in the toolbar.And that's how easy it is to train a Style Transfer model. As you justsaw in the demo, training a model takes only a few minutes. Let's recapsome important concepts from the demo we learned about training checkpointsand how to interact with the model training process in a new and exciting way.We compared model snapshots at different training iterations and discovereda new ability to extend training to improve the quality of a model. Formore fun eye control, be sure to check out the session Control Trainingin Create ML with Swift. To show you some amazing experiencesStyle Transfer can bring to your apps I'd like to introduce Geppy.Hello I'm Geppy Parziale. I'm going to show you how to combineStyle Transfer with ARKit. I got several style transform models from Davidand I decided to create a new virtual world with them. Let me show you how.ARKit captures this around the real world environment and each StyleTransform model allows me to stylize the scene. Cool. Let me show how todo it. Here I'm using ARKit to capture the surrounding real world environmentand stylizing it with one of our models. ARKit generates AR frames.Each AR frame contains a CVPixelBuffer that I rescale to the input sizeexpected by my Style Transfer model. Then the rescale and CVPixelBufferis stylized using the CoreML model and rendered onscreen using Metal.I can use ARKit to add a virtual object to the scene.And here she is, Michelle. Oh and she's a very good dancer. To add a 3Dobject to the scene I can define an ARAnchor that specifies the locationin the real world. The virtual object is then rendering seamlessly in thescene using Metal. But I can do more.Here it is. And it seems she really likes her new look.Here I stylize offline the virtual object texture with a different stylebut I want to do even more.Using the power of the Apple Neural Engine and this optimizedStyle Transfer model I can run multiple style concurrently in real time.Combine it with ARKit person segmentation. I'm executing two styletransfer model one for the background environment and one for the segmented person.The stylize the result are blended together using Metal. Pretty cool, huh?These highly optimized Style Transfer models, generated with Create ML combinedwith ARKit and the hardware acceleration of the Apple NeuralEngine and Metal allow you to unleash all the power of iOS14 foryour apps. I can't wait to see all the cool experiences you will buildwith Style Transfer models trained with Create ML. And now I'll hand itback to David for a recap.Thanks Geppy. Your app looks really amazing. Let's summarize. With Style Transfer and Create ML,you can train models for both image and video use cases. The video styletransfer model is extremely performant and can be easily combined withother apple technologies such as ARKit. The model size is small whichmakes it convenient to bundle with your apps. Reduced training timemakes the training experience interactive and fun. You can train a model ina few minutes and quickly iterate with different styles and model parameters.We can't wait to see what you come up withusing Style Transfer in
Create ML. Thank you.

Hello and welcome to WWDC. Hey everyone. I'm David a machine learningengineer from the Create ML Team. And in this session we're goingto talk about building Style Transfer models and Create ML. Style Transferis a new machine learning taskavailable this year in the Create ML app. It can be used to blenda style and content image together.Let's see what happens when we apply Style Transfer to this pair of images.Wow that's pretty cool! The stylized result looks great. The model hastransferred the colors shapes and textures from the style image to thecontent image. The color of the style image is very important. The whitebackground and black paint strokes from this style are transferred by themodel to create a black and white result. Paintings aren't the only sourceof style. The model can learn and transfer the tiles from this mosaic as well.Express your creativity by using different patterns from nature. For examplethe ice cracks in this style can be transferred by the model to producea pretty compelling result. These are just a few examples of what you cando with Style Transfer and Create ML. But wouldn't it be great if wecould do even better. What if you could apply Style Transfer to more thanjust a static image. Amazing. Each frame is stylized fast enough to maintaina smooth stylization experience. You can use a Style Transfer model inreal time for your video apps. Using the A13 bionic chip we can achievea processing speed up to 120 frames per second. To train a Style Transfermodel we need to provide some training data, a style image, and a directoryof content images. The model learns the balance between content and stylefrom your images. For optimal results the content images used for trainingshould be similar to what you expect to stylize at inference time. In thisexample I have a directory of natural content images. You can use modelparameters to optimize the behavior of a Style Transfer model. You canconfigure your model behavior with style strength and style density model parameters.Let's explore style strength first. With a style strength parameter.You can tune the balance between style and content. With low style strength,only parts of the background adopt the style image qualities and the peopledancing have adopted only a small amount of blue color. Now with high stylestrength the background is even more stylized and the people dancing completelyadopt the style image colors and ice cracks. Notice the difference whenI put them next to each other. Next let's explore style density. With thestyle density parameter, course details can be learned by the model. Here,I've drawn a grid on top of the style image. Let's zoom in on a region.The model focuses on high level details in this region such as the birdand learns these coarse qualities. Here's an example of coarse stylization.

Fine details can also be learned by the model by setting the style densityparameter to a higher setting. With a fine grid the zoomed in region containsmostly color and brush strokes. This produces a different stylized result.Let's compare the coarse and fine results side by side to visualize thedifference. You can use the style density parameter to explore a wide rangeof such stylizations. Let's train a style transfer model together in the CreateML app. I'll open the Create ML app and create a new document.I'll select the new Style Transfer template. I'll name the projectDanceStylizer and fill in a short description. The app has already selectedthe settings tab and is ready for my training data and model parameters.I have this style image that you saw earlier in the session. I'll dragit into the training style image data well. I also have an image of people dancing.I'll use that for validation in order to visualize the model quality throughoutthe training process. Lastly I need a directory of content images to learnthe balance between content and style. I can either download a set of afew hundred natural content images directly from the app, which works ina wide variety of use cases. Or I can use my own folder of content images.I'll drag in a folder of six hundred natural content images. I have theoption to optimize for image or video use cases. I'll choose video sinceI'm interested in real-time Style Transfer apps and I'll train for fourhundred iterations. I can use the style strength slider to control the balancebetween content and style and I can also use the style density sliderto explore coarse or fine stylization. The default parameters work prettywell in most cases. Now that I'm finished configuring my training settingsI'll click the train button in the toolbar. The app processes the style andcontent images and immediately starts training the model. Every five iterationsa new model checkpoint stylizes my validation image. I can use this tovisualize the model stylization interactively through the training process.At any point I can take a model snapshot by clicking the snapshot buttonin the toolbar. A snapshot is an ML model that I can use later in an app.My model snapshots are saved under model sources. Style loss and contentloss graphs helped me understand the balance between content and style.

The style loss decreases as my model learns to adopt the artistic qualitiesof my style image. The model has completed training 400 iterations andit looks like the style loss has converged. I can train my model for moreiterations by clicking the train more button in the toolbar. I'm happywith my model stylization on the validation image, so let's go to the preview tab.I can navigate to the preview tab to test out my model with some new data.I'll drag in a test image.

I can toggle back and forth between the stylized image and the originalcontent so that I can clearly visualize the stylized effect.I can compare the stylized result from different models snapshots.Since this model was optimized for video, I'll try dragging in a test video.Now this is really getting interesting. At this point I can choose to download,share with a colleague, or open the stylized result in the QuickTime Player app.In the output tab,I can find out more information about my model. The size of the model isquite small, under one megabyte, which makes it really convenient to bundlewith my apps. I can see the OS availability of my model to ensure compatibilityand by selecting predictions I can find out even more about my model suchas input and output layer names. At this point I can get the model to usein my apps by clicking the get, open in Xcode, or share button in the toolbar.

And that's how easy it is to train a Style Transfer model. As you justsaw in the demo, training a model takes only a few minutes. Let's recapsome important concepts from the demo we learned about training checkpointsand how to interact with the model training process in a new and exciting way.We compared model snapshots at different training iterations and discovereda new ability to extend training to improve the quality of a model. Formore fun eye control, be sure to check out the session Control Trainingin Create ML with Swift. To show you some amazing experiencesStyle Transfer can bring to your apps I'd like to introduce Geppy.Hello I'm Geppy Parziale. I'm going to show you how to combineStyle Transfer with ARKit. I got several style transform models from Davidand I decided to create a new virtual world with them. Let me show you how.

ARKit captures this around the real world environment and each StyleTransform model allows me to stylize the scene. Cool. Let me show how todo it. Here I'm using ARKit to capture the surrounding real world environmentand stylizing it with one of our models. ARKit generates AR frames.Each AR frame contains a CVPixelBuffer that I rescale to the input sizeexpected by my Style Transfer model. Then the rescale and CVPixelBufferis stylized using the CoreML model and rendered onscreen using Metal.

I can use ARKit to add a virtual object to the scene.

And here she is, Michelle. Oh and she's a very good dancer. To add a 3Dobject to the scene I can define an ARAnchor that specifies the locationin the real world. The virtual object is then rendering seamlessly in thescene using Metal. But I can do more.

Here it is. And it seems she really likes her new look.

Here I stylize offline the virtual object texture with a different stylebut I want to do even more.

Using the power of the Apple Neural Engine and this optimizedStyle Transfer model I can run multiple style concurrently in real time.

Combine it with ARKit person segmentation. I'm executing two styletransfer model one for the background environment and one for the segmented person.The stylize the result are blended together using Metal. Pretty cool, huh?These highly optimized Style Transfer models, generated with Create ML combinedwith ARKit and the hardware acceleration of the Apple NeuralEngine and Metal allow you to unleash all the power of iOS14 foryour apps. I can't wait to see all the cool experiences you will buildwith Style Transfer models trained with Create ML. And now I'll hand itback to David for a recap.Thanks Geppy. Your app looks really amazing. Let's summarize. With Style Transfer and Create ML,you can train models for both image and video use cases. The video styletransfer model is extremely performant and can be easily combined withother apple technologies such as ARKit. The model size is small whichmakes it convenient to bundle with your apps. Reduced training timemakes the training experience interactive and fun. You can train a model ina few minutes and quickly iterate with different styles and model parameters.We can't wait to see what you come up withusing Style Transfer in
Create ML. Thank you.

## Code Samples

