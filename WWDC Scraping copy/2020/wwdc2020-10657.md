# Wwdc2020 10657

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Make apps smarter with Natural LanguageExplore how you can leverage the Natural Language framework to better analyze and understand text. Learn how to draw meaning from text using the framework's built-in word and sentence embeddings, and how to create your own custom embeddings for specific needs.

We'll show you how to use samples to train a custom text classifier or word tagger to extract important pieces of information out of text— all powered by the transfer learning algorithms in Natural Language. Find out how you can create apps that can answer user questions, recognize similarities in text, and find relevant documents, images, and more.

To get the most out of this session, you should have a basic understanding of the Natural Language framework. For an overview, watch “Introducing Natural Language Framework” and “Advances in Natural Language Framework.” You can also brush up on model training using Create ML through “Introducing the Create ML App.”ResourcesHD VideoSD VideoRelated VideosWWDC23Explore Natural Language multilingual modelsWWDC19Advances in Natural Language Framework

Explore how you can leverage the Natural Language framework to better analyze and understand text. Learn how to draw meaning from text using the framework's built-in word and sentence embeddings, and how to create your own custom embeddings for specific needs.

We'll show you how to use samples to train a custom text classifier or word tagger to extract important pieces of information out of text— all powered by the transfer learning algorithms in Natural Language. Find out how you can create apps that can answer user questions, recognize similarities in text, and find relevant documents, images, and more.

To get the most out of this session, you should have a basic understanding of the Natural Language framework. For an overview, watch “Introducing Natural Language Framework” and “Advances in Natural Language Framework.” You can also brush up on model training using Create ML through “Introducing the Create ML App.”

HD VideoSD Video

HD Video

SD Video

Explore Natural Language multilingual models

Advances in Natural Language Framework

Search this video…Hello and welcome to WWDC. Hello everyone. Welcome to our sessionon natural language processing. The goal of this session is to help youmake your apps smarter by using the power of NLP in the Natural Languageframework. I'm Vivek and I'll be jointly presenting this session withmy colleague Doug Davidson.So let's get started. Let's begin with the central notion of language.Language is a code system that helps us humans solve difficult problemsthrough communication and it also provides us with a very unique typeof social interaction.You could think of how we communicate using language. Language is an intermediaterepresentation that helps us translate concepts into symbols which can thenbe expressed in the form of words, phrases or sentences with some grammaticalstructure. The medium of expression can be through speech, perhaps throughwriting on a keyboard or Apple Pencil. It can even be an image or a videothat you capture using your camera. Now language also has this remarkableproperty that not only helps us translate concepts into symbols but italso helps us assimilate content into concepts. And the last few yearsas we have moved from human intelligence into machine intelligence,the central notion of language has been replaced by NLP. NLP has now becomethe intermediate representation that helps machines translate conceptsinto symbols and also assimilate content into concepts. But what does ondevice NLP at Apple look like? Until 2017,the primary modality where NLP was exposed at Apple was through the NSLinguisticTagger Class and Foundation. Now this provides fundamental textprocessing such as language identification, tokenization and so on.In 2018, we introduced the Natural Language framework. The NaturalLanguage framework provides everything that NSLinguisticTagger canand on top of it, we started focusing on state of the art machine learningand modern linguistic techniques such as text embedding and custom models.Not only that, we also started tightly integrating Natural Language frameworkwith the rest of the machine learning ecosystem at Apple through tightintegration with Create ML and Core ML. Now, before we jump into therest of the session we'd like to tell you that ENSLinguisticTagger hasbeen marked for deprecation. We strongly encourage you to move towardsNatural Language for all your language processing needs. Now if you lookat the kinds of functionalities provided in the Natural Language frameworkthey can be broadly broken down into three different categories. The firstis in the area of fundamental text processing.The second is in the realm of text embeddings. And the third is in thearea of custom models. So let's begin with fundamental text processing.Natural language framework provides several basic fundamental buildingblocks such as language identification, tokenization, part of speech tagging,lemmatization and named entity recognition. And we provide these APIsacross a wide variety of languages. For more information about these APIsyou can refer to our 2018 and 2019 WWDC sessions.But at a high level all of these APIs operate on a piece of text and whatthey give is an output is a hypothesis or a prediction. However, it did nottell us a notion of confidence associated with this prediction. And thisyear we have a brand new API call for confidence course. So this buildson top of the existing functionality. And in addition to the hypothesisor the predicted labels you can also get the confidence course using the APIs.Let's see how we can use this. We start off by creating an instance ofNLTagger and specify the tag scheme to be named type. This is somethingthat you've been used to so far. Now we have a brand new API called tagHypotheses.So when you use tagHypotheses in addition to getting the predictionseither at the sentence level or the token level you also get a confidencecode associated with that prediction.Let's look at how to use these confidence scores through the lens of ahypothetical app called Buzz. Buzz is a News reader app. As part of this application,you can browse articles, you can bookmark them and you can organize themso that you can read them later.And what we would like to do is add a new feature to this application.Then we extract recent entities from the articles that you've written.So we want to populate these entities on the right side bin. And whenyou click on an entity, you can be taken back to the article that you'vealready read. So how do we do this. So we want to use a Named Entity RecognitionAPI to automatically analyze this text and extract these named entitiesso that we get these named entities such as Cartagena and so on and so forth.Now if you take a close look at the entities on the right side you'll seethat is a spurious entry. We have something called Do Not Disturb while driving.So the Named Entity Recognition API gives us person names, organizationnames as well as location names. This seems like a false positive fromthis machine learning API. So how do we fix this. Suppose we had an inputsentence such as he was driving with Do Not Disturb while driving turned on.When we passed the sentence through the Named Entity Recognition API whatit does is it analyzes the sequence of tokens and produces a span of tokensas an organization name. Now this hypothesis is incorrect in this machine learningmodel. Now on the power of confidence course you can also get the confidencecourse for each of these labels. As you can see the confidence score ispretty low. By setting the threshold of, for instance, point eight for organization names,you can easily filter out this false positive. That this if you now goback to the app and incorporate this in your application you can easilyfilter out the false positive and you have a much better and enhanced user experience.We do have a few recommendations in terms of best practices. First we'dlike to recommend that you avoid heuristic hard coding of these thresholdvalues and calibrate it on representative data that is pertinent to yourapp and domain of operation. We'd also recommend that you consider creatingthresholds on a per class basis rather than setting a global thresholdfor all the classes in a particular application you considered on a perclass basis so that you get final control of false positives because there'sfalse negatives in your app. Now let's move on and shift our attentionto text embeddings. Text embeddings are really important. In fact theyhave been the cornerstone of recent advances in modern and to really understandText embeddings, let's begin with the notion of a text corpus. What isa text corpus? A text corpus is a collection of documents which are comprisedof paragraphs sentences phrases and words and in conventional NLP.Then we start with the text corpus. The first thing that we do is to tokenizedthis corpus. Then we tokenized a text corpus. What we get is an inventoryof words in this text corpus. And this inventory of words can be thought ofas a bag of words representation that each word is independent. Now ifyou were to look at this from a machine representation standpoint it isalso called this one-hot encoding. So in this example if you have a bunchof words food, burger, pizza, automobile, bus and car and we've gone over the textcorpus and extracted these words. Now each word here is represented by abit vector which has one bit on and the rest of the bits off. And the lengthof this vector is the same as the number of unique words in your text corpus.Now as humans we can see that food, burger and pizza are related concepts.And similarly, automobile, car and bus are also related concepts.However if you just look at this bit vector representation, it doesn't provide anyinformation about the similarity or dissimilarity of goods. So wouldn'tit be great if we had a representation that also incorporated the informationabout similarities of words? And this is really where word and embeddings come in.When you use word embeddings again, you start with the text corpus andwhat you get as an output is a vector representation of words. Words thatare similar are clustered together and words that are dissimilar are clustered away.So in this example you can see that burger, pizza and food are clusteredtogether and away from the concepts of automobile, car and bus. To obtainthese word embeddings you can use different sorts of machine learningalgorithms which would be linear models or non-linear models. But at a highlevel they capture this vector representation by analyzing global corecoincidence of words in the text corpus. If you consider this and look at itfrom a machine representation standpoint, now the representation is differentfrom one hot-encoding. Each word gets a real value vector of these dimensionsor you can think of it as D columns. And now if you look at the vectorfor food, burger and pizza they are close to each other in the vector space.Similarly the vectors for automobile, car and bus are also close to eachother but far away from the food concepts. Now that you understood wordembeddings let's look at the different types of word embeddings.The first is called Static Word Embeddings. Let's understand this concept.Suppose we had an input sentence. "I want a burger from a fast food joint."And we want to extract the word embedding for the word food. For the caseof static embeddings what we do is for all the words in the vocabularywe pre-compute the embeddings and stored it as a lookup table. Now thislookup table is computed and stored on -evice in an efficient manner.So when we need to look up the word embedding for a particular word such asfood, we simply go into this lookup table pick the corresponding vectorand give it as the output. Now static word embeddings are really useful.They are very useful to give the nearest neighbors of words in the vectorspace and they're also very useful as inputs to neural network algorithms.But they do have some shortcomings. Suppose we had an input sentence suchas "It is food for thought" where the word food is represented in adifferent kind of connotation based on the context. What happens in staticword embeddings is you will still pass this to the lookup table and extractthe same vector for the word food. Even though we know that the connotationis different and the context is different. So even though the semanticconnotation of the word food is different because of the context in whichit's used, we still get the same vector representation. So can we do better?And this is where dynamic word embeddings come into the picture.So in dynamic word embeddings what we do is we pass every sentence througha neural network. And what we get is a dynamic embedding for every wordin that sequence which is completely contextual. So we passed these twosentences through dynamic word embeddings which can be a neural networksuch as a transformer network or an ELMo style model. What we get is anoutput is one vector for each word that is different based on the context.So the word food now gets completely different vector representations becausethe context of food in these two sentences is different. Now on the OSwe support static embeddings in a variety of languages, also across differentApple platforms. For more information about static word embeddings youcan refer to our 2019 WWDC sessions. In addition to static wordembeddings, we also support what we call is custom word embeddings whereinyou can train your own embeddings using a third party toolkit such asfasttext, word2vec, GloVe, or perhaps even a custom neural network inTensorFlow or PyTorch. Once you do this you can bring these embeddings toApple platforms, compress them and store them and use them in an efficient way.Once you convert them to a representation on device you can use them justthe same way as static word embeddings. Now in order to use word embeddingslet's look at how you use it. So you create an instance of NLM embedding,Word embedding and you specify the language. And once you have this you canperform three different operations. The first is given a word, you canget the vector representation of the word. The second is given two words,you can get the distance between these two words in the vector space.And the third is given a word, you can find the nearest neighbors of the wordin the vector space. Now let's look at the use of word embeddings througha hypothetical app called Nosh. Nosh is a food delivery app and as partof this application, we have an FAQ section. Now the user experience in thisapp especially in the FAQ section is not great. So if I were to find some information,I have to scroll to all these questions and look for the question thatI'm interested in, and then for the corresponding answer. So we wantto improve this user experience in the Nosh app by adding an automaticsearch feature so that you can type or you can speak the query and youcan pick the corresponding question and show you the relevant answer.How do we build this using word embeddings? So one way to build this is using staticword embeddings. Let's say you have an input query. Do you deliver it toCupertino? When you pass it through the Word Embeddings API, you can enumerateevery word and get one vector representation for each word in the sequence.Once you do that a heuristic way of getting a sentence representationis to simply take the average of the vectors of every word and what you'dget is an output is one vector of the same dimension. Now you can alsope-compute the word embeddings for every single FAQ question in your database.So you would take every question, run it through word embeddings, get the vectors,average them and pre-compute the embeddings. So at runtime,given a query, you find the question that is closest to the input queryvector and you pick the question and show the corresponding answer in the UI.Now this seems like a reasonable way of solving this problem but it doeshave several shortcomings. The first is the issue with word coverage sincestatic word embeddings work with the finite vocabulary, if you have aninput query that does not have a word in the lookup table, it will lose information.The second is using this averaging process is very noisy. It's akin toa bag of words representation that loses compositional knowledge. For instanceif we had a query such as "Do you deliver from Cupertino to San Jose?"By simply taking the average we are jumbling up the words and we lose the compositionalinformation contained in words such as from and to. So the big questionis can we do better? And yes we certainly can. And we are delighted totell you that we have a brand new technology called a sentence embeddingthat solves this problem. Now by using Sentence Embedding API then youpass an input query on a sentence such as "Do you deliver to Cupertino,"it analyzes this entire sentence and encodes this information into a finitedimensional vector in the current API. The dimension of this vector is512 dimensions. So how does this work? Intuitively you can think of itas starting from a text corpus and in the text corpus, if you were to tokenizethe text at the sentence level when you pass it through the Sentence Embeddinginstead of working with words, now you have sentence representations.Each of these sentences are represented in this vector space in such a way thatsentences that are conceptually similar are clustered together and sentencesthat are dissimilar are clustered away from each other. Now the technologyunder this is fairly complex and utilizes several machine learning techniques,one of which is pre-trained models in conjunction with custom layers suchas bi-directional listing  as well as fully connected layers.And we train these networks in a multi-task training set up on different tasks such asnatural language inference, binary text similarity as well next sentence prediction.But to use it, you don't have to worry about these details. You simply haveto ask for it. So you start by importing NaturalLanguage. And you createan instance of NLEmbedding and sentence Embedding and specify the languageas English. Once you have this you can ask for the vector of an input sentence.When you do this the sentence is run through the neural network. And whatyou get is an output is a finite 512 dimensional vectorthat encodes the meaning of this sentence. Given two sentences you canalso find the distance between these two sentences in the vector space.You simply run these two sentences underneath this API through the neural network,get the vector representation and then compute the distance. Now thereare a wide variety of other potential applications for this technology.But since we don't have a finite list of sentences right now and you cannotpre-compute the embedding for all the sentences a priori that is nonearest neighbors API available for this technology. But later in the sessionDoug will tell you how you can use sentence embeddings and do nearestneighbors by leveraging custom embedding technology. Now if you had togo back to the Nosh application then when you have a query such as "Do you deliverto Cupertino," you simply pass this through the Sentence Embedding API andyou get one vector that encodes all of the meaning. Similarly for all ofthe FAQ questions in your index you can pre-compute the sentenceembeddings and at runtime given an input you simply find the closest question.And once you do this you show the relevant answer in the application atthe UI level. To see the sentence embeddings in action I'm going to handit over to Doug who's going to show us a demo of this working in the Noshapplication. Over to youDoug. Thanks Vivek. So let's see some of this in action. In our Nash application,what we're going to do is to let the user type in a query string and thenwe're going to return an appropriate answer from our frequently asked questionsusing Sentence Embeddings.So let's look at some code. So the first thing we're going to do in thismethod is to just get a Sentence Embedding, in this case for English. Very simple.And then we'll ask that embedding for the vector for the user's query string.When we first constructed this application we took each of our answersand constructed for it, two or three example queries and pre-calculatedthe Sentence Embedding vectors for each one and put them in a table.So what we're going to do is just iterate through that table. We'll go throughfor each key, we have two or three vectors representing these example queries.We'll find the distance between each of those and our query factor.And the smallest distance is our nearest neighbor which represents the answerthat we're going to show and will return that answer. So let's try it out.So if the user types for example "how do I use it," then we can search andgo through and find the nearest neighbor and point them to the "How doesit work" section of our frequently asked questions.Or maybe they ask "Where do you deliver?"And then we'll search and find the nearest neighbor and point them to thedelivery area section of our frequently asked questions.Or maybe the user wants to know "where is my order?"In which case we search, and we can point them directly to the order statussection of our frequently asked questions.Now there are many other possible uses for this. Let's consider anotherhypothetical sample application called Verse and Verse is an applicationfor showing poetry.So Verse has many many different poems in it. And one obvious UI for thisis that we could just have a long list of the poems where the user picksone and the new user sees that poem. And that's fine but wouldn't it benice to have some additional ways of looking for these poems. For example,suppose that I type in "You're beautiful."Well then we can find out that Shakespeare said it better and we can dothis using Sentence Embeddings. So what we can do is take each line ofeach poem and calculate the Sentence Embedding vector for that line andthen put them in a table and then iterate through them just as we did inthe Nosh application. But there's one twist here and that is that we havehundreds of poems, and thousands of lines. So it may be that this simple linearsearch and table that we use in the Nosh app isn't efficient enough andwe have a solution to that.And the solution is to make use of custom embeddings. And what do weneed in order to create a custom embedding? We need a dictionary. The keysin the dictionary are arbitrary. So I've chosen them here to be for examplepoem 1 line 1, poem 1 line 2 to be strings from which we can readily determinewhich poem we were looking at and which line. And then the values are justthese vectors that we got for each line. And from that we can produce acustom embedding. And the custom embedding has two important properties.First it gives a very efficient space efficient representation of that dictionary.And second it has geometric information that we can use to do efficientnearest neighbor search without having to go through the entire thing.And now to create one of these customized beddings. That's very simple.You can do this and create it now and then. All you do is to take thatdictionary and pass it into Create ML. And what comes out is a Core MLmodel that represents that custom embedding. So let's take a look at this in action.Let's take a look at some code in our verse application. And here's thecorresponding method in verse that takes the user's query string and returnsthe answer key. So just as before, we get the sentence embedding for Englishand we get the query vector for that embedding. But now the rest is even simpler.We just take our custom embedding and pass it in that query vector andit will directly return to us. The nearest neighbor and it will returnthe key that we put into that dictionary from which we created the customembedding. And as I mentioned we can easily determine which is the rightpoem to return from that key. So let's try it out.The user types in something like let's say "I love you" we can get a poeticexpression for that and find a poem that represents that sentiment.Or maybe they type in something like "Don't forget me" and we can find apoem that expresses that sentiment - just about anything we want. We canfind a suitable expression. Maybe it's "love isn't everything."And here's a poem for that too as well. Now I don't want to give the impressionthat the only thing you can do with Sentence Embeddings is this sort of textretrieval because Sentence Embeddings are useful for all sorts of different applications.For example consider a hypothetical app called FindMyShot which storesimages and happens to have captions for each of those images. Now sincethe images associated with captions I can use Sentence Embeddings to findan image based on similarity between the user's query text and the caption.And there are many other possible usages for these. You can use them fordetecting paraphrases. You can use them for input, for training more complicatedmodels and you can use them for clustering. So let me spend a moment totalk about clustering. If you don't have any pre-arranged text if all thetext comes in from the user then you can still make use of Sentence Embeddings.For example if you had messages or maybe reviews or maybe problem reportsor users you can take a Sentence Embeddings and calculate a vector foreach one of these and then you can use standard clustering algorithms togroup these into as many groups as you want.And what Sentence Embedding means is that these groups are going to besentences close together in meaning. The availability of the SentencebEmeddings is for a number of different languages English, Spanish, French,German, Italian, Portuguese and simplified Chinese. Add on macOS, iOSand iPadOS. Now these Sentence Embeddings are intended for use on naturallanguage text especially text that comes in from the user. You don't haveto do a lot of pre-processing on this text. You don't have to move stopwords for example because the Sentence Embeddings has seen all this intheir training and they're intended for being applied to text that is similarin length to a single sentence, maybe a couple of sentences or a short paragraph.If you have text that's longer than that, then you can divide it up into sentencesand apply the Sentence Embeddings to each one. Just as we did with our poems.And also you can make use of the custom embeddings in case you have largenumbers of these that you want to store and look through.So next I'd like to turn to the topic of custom models.The idea in custom models is that you bring in your custom training dataand we train a model for you for some particular NLP task. Now thereare two broad kinds of NLP tasks that we support that cover a wide rangeof functionality. The first is a text classifier, which the object is totake a piece of text and supply a label to it. And the other is word word tagger,in which the object is to take a sequence of words in a sentence and supplya label for each one. The custom model training is exposed through Create ML.You passing your training data, Create ML passes it to Natural Language.Natural Language produces a model and what you get out is a Core ML model,either a tagger or a text classifier. And our focus for the last coupleof years has been on applying the power of transfer learning to these models.With transfer learning, the idea is that you can incorporate a pre-existingknowledge of the language so that you don't have to supply quite so muchtraining data in order to produce a good model. And this pre-existing knowledgecomes in by means of word embeddings because the word embeddings havebeen trained on large amounts of natural language text. Now we introducedthis last year for text classifiers and that provides a very powerful solutionfor many apps. For example we can consider a hypothetical app called Merchwhich is intended for transactions between buyers and sellers and theycommunicate with each other about these transactions. But one complaintthe users have perhaps is that they get sometimes spam messages and theydon't want to have to look at all these. Well one possible solution isthat you can train a text classifier by bringing in large amounts of examplesentences labeled as spam or not spam and then train a text classifierand transfer learning model is actually very effective for this sort of task.And then the model in your app will tell you whether a particular messageis likely to be spam and then you can show it appropriately or not to the user.But what I really want to talk about today is the application of transferlearning to word tagging which is new this year. Now let's go back andtalk about the task of word tagging. As I said the object is to take sequenceof words in a sentence and supply a label for each one. And probably theprototypical task for this is part of speech tagging but it can be usedfor many other things. For example, you can potentially use word taggingto divide a sentence up into phrases or, and this is what we're going tobe talking about here,you can take a sentence and extract important pieces of information fromit even though it's unstructured text. For example in a travel applicationI might want to know where the user is coming from and where they're going to.Can we make use of this in our Nosh application? Well let's take a look.So we saw that with the Sentence Embedding vectors we could return generalanswers to the user's queries. But there are other things that I mightwant to look at. And a user sentence for example, I might want to know whatkind of food they're looking for, or where they want to get it from.And I can label, potentially label these parts of the sentence as food or acity where the food is coming from. Now the most obvious and simple wayto handle this sort of problem would be to just list all the potentialfoods and potential cities and then just search through the text for eachof those. And of course we support that sort of thing. We have an NLGazetteerclass which provides an efficient representation for any number oftables of items that you might want to look for in text.But the problem with this approach is that in general you're not goingto be able to list all the potential values that you might want to look for.So as soon as you encounter some piece of text that you hadn't thoughtof before then this simple search is not going to help you.And the other problem with this approach is that it doesn't take into accountanything about the meaning of words and context. And a word tagger can solveboth of these problems.In addition it's possible to combine a word tagger and an NLGazetteerfor even greater accuracy. So suppose I've decided that I actuallywant to use a word tagger for my Nosh application. Where do I start? The firstthing to do is to decide what pieces of information I want to get out andassign labels to those. Then I collect sufficiently many as ample sentencesthat the user might enter and I decide how I'm going to label them.And then I actually label those sentences and continue to repeat this processuntil I have enough data to train a good model and I might have to continuerepeating it. If my model ever runs across a situation that it doesn'thandle adequately, usually the solution is to add some more training dataand retrain the model. So what does our training data look like?So in our Nosh application we're going to add some labels to sentenceslike this so we'll use a neutral label. Oh here in this case for otherfor the pieces of text that we're not particularly interested in and we'lluse labels like food from city restaurant for the pieces of text that weare specifically interested in.Now why did I say from_city rather than just city. Because I noticed thatin these example sentences there are two kinds of ways where a city cancome in. The first is where it's the city where the restaurant is locatedwhere the food is supposed to be coming from.And the second is whether it's the city the user is located where the foodis being delivered to. And so I'm going to label those differently as fromcity to city. And because the word tagger can take advantage of the meaningof words in context. It can distinguish between these two provided I giveit sufficient training data. And here is what the training data looks likein Json format which is very convenient for use with Create ML.So what I want to go and train a model and Create ML. It's very simple.If I'm doing it in code I just import Create ML and then I ask Create MLto provide me a model. Now we've supported this for a couple of yearsusing an algorithm known as CRF Conditional Random Fields and it works well.But what's new this year is that we are applying the power of transferlearning to word tagging. And as I said before what transfer learning doesis to allow us to apply pre-existing knowledge of the language so thatyou don't have to supply quite so much training data in order to traina good model. And the way in which this knowledge comes in is via dynamicword embeddings. As we said before the dynamic word embeddings understandsomething about the meaning words and context. Was it just what the wordtagger wants. So we use the dynamic word embedding as an input layer andon top of it we take the data that you provide and we train a multi-layerneural network and that is the thing that actually produces the output labels.Now this sounds complex but if you want it all you have to do is ask for it.So instead of asking for the CRF algorithm you just ask for a transferlearning algorithm with dynamic word embeddings and then it will traina model for you. So let's take a look at that in action.So here's the Nosh application and here is some of the training data thatI have trained, added for it. And I produced, oh somewhat over a thousandsentences of this format. And you'll notice these are in Json format.So each example is a parallel sequence of tokens and labels one label foreach token and you'll notice that cities are labeled from_city or to_cityand you'll notice that foods are labeled and restaurants are labeled.And this is the data that I'm going to use to train my model. And so it's possibleto train it in code but I'm going to train this model using the CreateML application which makes it very simple. So here's the Create ML application.All I have to do is point it to my training data, tell it which of the labelsand tokens, and tell it which algorithm I want to use. In this case we'regoing to use the transfer learning algorithm. And this is going to be for English.And that's really about all there is to it. I just started off and sentit to train. And the first thing that does is to load all the data extractfeatures from it. And then it's going to start training using transfer learning.And it will train a neural network. So this takes a number of iterationswith each iteration it gets more and more accurate. Now this particulartraining process takes two or three minutes. So I'm not going to make yousit through all of it.I actually have a pre-trained model. So let's go back to the applicationand take a look at some code.So here is an example method in the Nosh application that's going to makeuse of our trained model. So we're going to be passed in the user's stringthat they've typed. First thing we'll do is load our model our word taggermodel as an NL model. And then what we're going to do here is useit with a NL tagger. That's convenient because the NL tagger will takecare of all of the tokenization and application and just give us the results.So we've created a custom tag scheme that's just a string constant thatrefers to this set of tags and we'll tell our tagger that's what we wantto use. And then we tell our tagger to use our custom model for this customtag scheme. We attach the user's string to the tagger and that's reallyall there is to it. We can then use the tagger to go through the wordsand it will tell us for each one what it thinks the label should be whetherit's restaurant or food or from_city or to_city or nothing. And then we takenote of those particular pieces of what the user has entered and then wecan use that according to the needs of the application to generate anysort of custom response that we might want to provide. So I've taken theliberty of adding a few custom responses to the Nosh application.Let's try it out. So the user might type something like "Do you deliver to Cupertino?"So what is our model going to tell us? It's going to look at all thesewords and it will notice that Cupertino is a city and it's a citythey want delivery to. So we can generate a customer response that isspecific to Cupertino. Or they might ask you "Do you deliver pizza" and then ourmodel will notice that pizza is a food name so we can generate a customresponse based on pizza. Or maybe they ask if we deliver from a specificrestaurant Pizza City and the one in Cupertino.And in that case the model will tell us that Pizza City is a restaurantname and Cupertino is a city where the food is coming from.And we can use either or both of those to generate a custom response that mentions those.So that shows the power of word tagging to extract pieces of informationfrom unstructured text. So let's go back to the slides and let me turnit back over to Vivek.Thank you Doug for showing us a demo of transfer learning for word tagging.Now transfer learning for word tagging is supported for the same languagesas static embeddings and sentence embeddings across Apple platforms.To get the best use out of transfer learning for word tagging technology.we have a few recommendations. We recommend that you first start offwith a conditional random field, especially for languages such as English.The conditional random field, or CRF, pays attention to syntacticfeatures which are quite useful in many applications. However if you donot know the kind of distribution that you'll be encountering at runtimeit is better to use transfer learning because it provides better generalization.We also recommend you to use more data for transfer learning for wordtagging since the prediction is at a per token level in contrast with text classification,it requires an order of more magnitude data. As I mentioned NL LinguisticTagger Class has now been marked for deprecation. We strongly encourage you tomove towards Natural Language framework. We also told you how to use confidencecodes along with existing APIs. And this can be used to prune out falsepositives in your application. Then we provided an overview of SentenceEmbedding technology and demonstrated how you can use this in several hypothetical apps.We concluded with a new technology for transfer learning for word tagging.With that, we'd like to conclude by saying make your arm smarter by usingthe Natural Language framework.
Thank you for your attention.

Hello and welcome to WWDC. Hello everyone. Welcome to our sessionon natural language processing. The goal of this session is to help youmake your apps smarter by using the power of NLP in the Natural Languageframework. I'm Vivek and I'll be jointly presenting this session withmy colleague Doug Davidson.So let's get started. Let's begin with the central notion of language.Language is a code system that helps us humans solve difficult problemsthrough communication and it also provides us with a very unique typeof social interaction.

You could think of how we communicate using language. Language is an intermediaterepresentation that helps us translate concepts into symbols which can thenbe expressed in the form of words, phrases or sentences with some grammaticalstructure. The medium of expression can be through speech, perhaps throughwriting on a keyboard or Apple Pencil. It can even be an image or a videothat you capture using your camera. Now language also has this remarkableproperty that not only helps us translate concepts into symbols but italso helps us assimilate content into concepts. And the last few yearsas we have moved from human intelligence into machine intelligence,the central notion of language has been replaced by NLP. NLP has now becomethe intermediate representation that helps machines translate conceptsinto symbols and also assimilate content into concepts. But what does ondevice NLP at Apple look like? Until 2017,the primary modality where NLP was exposed at Apple was through the NSLinguisticTagger Class and Foundation. Now this provides fundamental textprocessing such as language identification, tokenization and so on.

In 2018, we introduced the Natural Language framework. The NaturalLanguage framework provides everything that NSLinguisticTagger canand on top of it, we started focusing on state of the art machine learningand modern linguistic techniques such as text embedding and custom models.

Not only that, we also started tightly integrating Natural Language frameworkwith the rest of the machine learning ecosystem at Apple through tightintegration with Create ML and Core ML. Now, before we jump into therest of the session we'd like to tell you that ENSLinguisticTagger hasbeen marked for deprecation. We strongly encourage you to move towardsNatural Language for all your language processing needs. Now if you lookat the kinds of functionalities provided in the Natural Language frameworkthey can be broadly broken down into three different categories. The firstis in the area of fundamental text processing.The second is in the realm of text embeddings. And the third is in thearea of custom models. So let's begin with fundamental text processing.

Natural language framework provides several basic fundamental buildingblocks such as language identification, tokenization, part of speech tagging,lemmatization and named entity recognition. And we provide these APIsacross a wide variety of languages. For more information about these APIsyou can refer to our 2018 and 2019 WWDC sessions.But at a high level all of these APIs operate on a piece of text and whatthey give is an output is a hypothesis or a prediction. However, it did nottell us a notion of confidence associated with this prediction. And thisyear we have a brand new API call for confidence course. So this buildson top of the existing functionality. And in addition to the hypothesisor the predicted labels you can also get the confidence course using the APIs.

Let's see how we can use this. We start off by creating an instance ofNLTagger and specify the tag scheme to be named type. This is somethingthat you've been used to so far. Now we have a brand new API called tagHypotheses.So when you use tagHypotheses in addition to getting the predictionseither at the sentence level or the token level you also get a confidencecode associated with that prediction.Let's look at how to use these confidence scores through the lens of ahypothetical app called Buzz. Buzz is a News reader app. As part of this application,you can browse articles, you can bookmark them and you can organize themso that you can read them later.

And what we would like to do is add a new feature to this application.Then we extract recent entities from the articles that you've written.So we want to populate these entities on the right side bin. And whenyou click on an entity, you can be taken back to the article that you'vealready read. So how do we do this. So we want to use a Named Entity RecognitionAPI to automatically analyze this text and extract these named entitiesso that we get these named entities such as Cartagena and so on and so forth.Now if you take a close look at the entities on the right side you'll seethat is a spurious entry. We have something called Do Not Disturb while driving.So the Named Entity Recognition API gives us person names, organizationnames as well as location names. This seems like a false positive fromthis machine learning API. So how do we fix this. Suppose we had an inputsentence such as he was driving with Do Not Disturb while driving turned on.When we passed the sentence through the Named Entity Recognition API whatit does is it analyzes the sequence of tokens and produces a span of tokensas an organization name. Now this hypothesis is incorrect in this machine learningmodel. Now on the power of confidence course you can also get the confidencecourse for each of these labels. As you can see the confidence score ispretty low. By setting the threshold of, for instance, point eight for organization names,you can easily filter out this false positive. That this if you now goback to the app and incorporate this in your application you can easilyfilter out the false positive and you have a much better and enhanced user experience.We do have a few recommendations in terms of best practices. First we'dlike to recommend that you avoid heuristic hard coding of these thresholdvalues and calibrate it on representative data that is pertinent to yourapp and domain of operation. We'd also recommend that you consider creatingthresholds on a per class basis rather than setting a global thresholdfor all the classes in a particular application you considered on a perclass basis so that you get final control of false positives because there'sfalse negatives in your app. Now let's move on and shift our attentionto text embeddings. Text embeddings are really important. In fact theyhave been the cornerstone of recent advances in modern and to really understandText embeddings, let's begin with the notion of a text corpus. What isa text corpus? A text corpus is a collection of documents which are comprisedof paragraphs sentences phrases and words and in conventional NLP.Then we start with the text corpus. The first thing that we do is to tokenizedthis corpus. Then we tokenized a text corpus. What we get is an inventoryof words in this text corpus. And this inventory of words can be thought ofas a bag of words representation that each word is independent. Now ifyou were to look at this from a machine representation standpoint it isalso called this one-hot encoding. So in this example if you have a bunchof words food, burger, pizza, automobile, bus and car and we've gone over the textcorpus and extracted these words. Now each word here is represented by abit vector which has one bit on and the rest of the bits off. And the lengthof this vector is the same as the number of unique words in your text corpus.

Now as humans we can see that food, burger and pizza are related concepts.And similarly, automobile, car and bus are also related concepts.However if you just look at this bit vector representation, it doesn't provide anyinformation about the similarity or dissimilarity of goods. So wouldn'tit be great if we had a representation that also incorporated the informationabout similarities of words? And this is really where word and embeddings come in.

When you use word embeddings again, you start with the text corpus andwhat you get as an output is a vector representation of words. Words thatare similar are clustered together and words that are dissimilar are clustered away.

So in this example you can see that burger, pizza and food are clusteredtogether and away from the concepts of automobile, car and bus. To obtainthese word embeddings you can use different sorts of machine learningalgorithms which would be linear models or non-linear models. But at a highlevel they capture this vector representation by analyzing global corecoincidence of words in the text corpus. If you consider this and look at itfrom a machine representation standpoint, now the representation is differentfrom one hot-encoding. Each word gets a real value vector of these dimensionsor you can think of it as D columns. And now if you look at the vectorfor food, burger and pizza they are close to each other in the vector space.Similarly the vectors for automobile, car and bus are also close to eachother but far away from the food concepts. Now that you understood wordembeddings let's look at the different types of word embeddings.The first is called Static Word Embeddings. Let's understand this concept.

Suppose we had an input sentence. "I want a burger from a fast food joint."And we want to extract the word embedding for the word food. For the caseof static embeddings what we do is for all the words in the vocabularywe pre-compute the embeddings and stored it as a lookup table. Now thislookup table is computed and stored on -evice in an efficient manner.So when we need to look up the word embedding for a particular word such asfood, we simply go into this lookup table pick the corresponding vectorand give it as the output. Now static word embeddings are really useful.They are very useful to give the nearest neighbors of words in the vectorspace and they're also very useful as inputs to neural network algorithms.

But they do have some shortcomings. Suppose we had an input sentence suchas "It is food for thought" where the word food is represented in adifferent kind of connotation based on the context. What happens in staticword embeddings is you will still pass this to the lookup table and extractthe same vector for the word food. Even though we know that the connotationis different and the context is different. So even though the semanticconnotation of the word food is different because of the context in whichit's used, we still get the same vector representation. So can we do better?And this is where dynamic word embeddings come into the picture.So in dynamic word embeddings what we do is we pass every sentence througha neural network. And what we get is a dynamic embedding for every wordin that sequence which is completely contextual. So we passed these twosentences through dynamic word embeddings which can be a neural networksuch as a transformer network or an ELMo style model. What we get is anoutput is one vector for each word that is different based on the context.So the word food now gets completely different vector representations becausethe context of food in these two sentences is different. Now on the OSwe support static embeddings in a variety of languages, also across differentApple platforms. For more information about static word embeddings youcan refer to our 2019 WWDC sessions. In addition to static wordembeddings, we also support what we call is custom word embeddings whereinyou can train your own embeddings using a third party toolkit such asfasttext, word2vec, GloVe, or perhaps even a custom neural network inTensorFlow or PyTorch. Once you do this you can bring these embeddings toApple platforms, compress them and store them and use them in an efficient way.

Once you convert them to a representation on device you can use them justthe same way as static word embeddings. Now in order to use word embeddingslet's look at how you use it. So you create an instance of NLM embedding,Word embedding and you specify the language. And once you have this you canperform three different operations. The first is given a word, you canget the vector representation of the word. The second is given two words,you can get the distance between these two words in the vector space.And the third is given a word, you can find the nearest neighbors of the wordin the vector space. Now let's look at the use of word embeddings througha hypothetical app called Nosh. Nosh is a food delivery app and as partof this application, we have an FAQ section. Now the user experience in thisapp especially in the FAQ section is not great. So if I were to find some information,I have to scroll to all these questions and look for the question thatI'm interested in, and then for the corresponding answer. So we wantto improve this user experience in the Nosh app by adding an automaticsearch feature so that you can type or you can speak the query and youcan pick the corresponding question and show you the relevant answer.How do we build this using word embeddings? So one way to build this is using staticword embeddings. Let's say you have an input query. Do you deliver it toCupertino? When you pass it through the Word Embeddings API, you can enumerateevery word and get one vector representation for each word in the sequence.

Once you do that a heuristic way of getting a sentence representationis to simply take the average of the vectors of every word and what you'dget is an output is one vector of the same dimension. Now you can alsope-compute the word embeddings for every single FAQ question in your database.So you would take every question, run it through word embeddings, get the vectors,average them and pre-compute the embeddings. So at runtime,given a query, you find the question that is closest to the input queryvector and you pick the question and show the corresponding answer in the UI.

Now this seems like a reasonable way of solving this problem but it doeshave several shortcomings. The first is the issue with word coverage sincestatic word embeddings work with the finite vocabulary, if you have aninput query that does not have a word in the lookup table, it will lose information.

The second is using this averaging process is very noisy. It's akin toa bag of words representation that loses compositional knowledge. For instanceif we had a query such as "Do you deliver from Cupertino to San Jose?"By simply taking the average we are jumbling up the words and we lose the compositionalinformation contained in words such as from and to. So the big questionis can we do better? And yes we certainly can. And we are delighted totell you that we have a brand new technology called a sentence embeddingthat solves this problem. Now by using Sentence Embedding API then youpass an input query on a sentence such as "Do you deliver to Cupertino,"it analyzes this entire sentence and encodes this information into a finitedimensional vector in the current API. The dimension of this vector is512 dimensions. So how does this work? Intuitively you can think of itas starting from a text corpus and in the text corpus, if you were to tokenizethe text at the sentence level when you pass it through the Sentence Embeddinginstead of working with words, now you have sentence representations.Each of these sentences are represented in this vector space in such a way thatsentences that are conceptually similar are clustered together and sentencesthat are dissimilar are clustered away from each other. Now the technologyunder this is fairly complex and utilizes several machine learning techniques,one of which is pre-trained models in conjunction with custom layers suchas bi-directional listing  as well as fully connected layers.And we train these networks in a multi-task training set up on different tasks such asnatural language inference, binary text similarity as well next sentence prediction.

But to use it, you don't have to worry about these details. You simply haveto ask for it. So you start by importing NaturalLanguage. And you createan instance of NLEmbedding and sentence Embedding and specify the languageas English. Once you have this you can ask for the vector of an input sentence.When you do this the sentence is run through the neural network. And whatyou get is an output is a finite 512 dimensional vectorthat encodes the meaning of this sentence. Given two sentences you canalso find the distance between these two sentences in the vector space.

You simply run these two sentences underneath this API through the neural network,get the vector representation and then compute the distance. Now thereare a wide variety of other potential applications for this technology.

But since we don't have a finite list of sentences right now and you cannotpre-compute the embedding for all the sentences a priori that is nonearest neighbors API available for this technology. But later in the sessionDoug will tell you how you can use sentence embeddings and do nearestneighbors by leveraging custom embedding technology. Now if you had togo back to the Nosh application then when you have a query such as "Do you deliverto Cupertino," you simply pass this through the Sentence Embedding API andyou get one vector that encodes all of the meaning. Similarly for all ofthe FAQ questions in your index you can pre-compute the sentenceembeddings and at runtime given an input you simply find the closest question.

And once you do this you show the relevant answer in the application atthe UI level. To see the sentence embeddings in action I'm going to handit over to Doug who's going to show us a demo of this working in the Noshapplication. Over to youDoug. Thanks Vivek. So let's see some of this in action. In our Nash application,what we're going to do is to let the user type in a query string and thenwe're going to return an appropriate answer from our frequently asked questionsusing Sentence Embeddings.So let's look at some code. So the first thing we're going to do in thismethod is to just get a Sentence Embedding, in this case for English. Very simple.And then we'll ask that embedding for the vector for the user's query string.When we first constructed this application we took each of our answersand constructed for it, two or three example queries and pre-calculatedthe Sentence Embedding vectors for each one and put them in a table.So what we're going to do is just iterate through that table. We'll go throughfor each key, we have two or three vectors representing these example queries.We'll find the distance between each of those and our query factor.And the smallest distance is our nearest neighbor which represents the answerthat we're going to show and will return that answer. So let's try it out.

So if the user types for example "how do I use it," then we can search andgo through and find the nearest neighbor and point them to the "How doesit work" section of our frequently asked questions.Or maybe they ask "Where do you deliver?"And then we'll search and find the nearest neighbor and point them to thedelivery area section of our frequently asked questions.Or maybe the user wants to know "where is my order?"In which case we search, and we can point them directly to the order statussection of our frequently asked questions.

Now there are many other possible uses for this. Let's consider anotherhypothetical sample application called Verse and Verse is an applicationfor showing poetry.So Verse has many many different poems in it. And one obvious UI for thisis that we could just have a long list of the poems where the user picksone and the new user sees that poem. And that's fine but wouldn't it benice to have some additional ways of looking for these poems. For example,suppose that I type in "You're beautiful."Well then we can find out that Shakespeare said it better and we can dothis using Sentence Embeddings. So what we can do is take each line ofeach poem and calculate the Sentence Embedding vector for that line andthen put them in a table and then iterate through them just as we did inthe Nosh application. But there's one twist here and that is that we havehundreds of poems, and thousands of lines. So it may be that this simple linearsearch and table that we use in the Nosh app isn't efficient enough andwe have a solution to that.And the solution is to make use of custom embeddings. And what do weneed in order to create a custom embedding? We need a dictionary. The keysin the dictionary are arbitrary. So I've chosen them here to be for examplepoem 1 line 1, poem 1 line 2 to be strings from which we can readily determinewhich poem we were looking at and which line. And then the values are justthese vectors that we got for each line. And from that we can produce acustom embedding. And the custom embedding has two important properties.First it gives a very efficient space efficient representation of that dictionary.And second it has geometric information that we can use to do efficientnearest neighbor search without having to go through the entire thing.

And now to create one of these customized beddings. That's very simple.You can do this and create it now and then. All you do is to take thatdictionary and pass it into Create ML. And what comes out is a Core MLmodel that represents that custom embedding. So let's take a look at this in action.Let's take a look at some code in our verse application. And here's thecorresponding method in verse that takes the user's query string and returnsthe answer key. So just as before, we get the sentence embedding for Englishand we get the query vector for that embedding. But now the rest is even simpler.We just take our custom embedding and pass it in that query vector andit will directly return to us. The nearest neighbor and it will returnthe key that we put into that dictionary from which we created the customembedding. And as I mentioned we can easily determine which is the rightpoem to return from that key. So let's try it out.

The user types in something like let's say "I love you" we can get a poeticexpression for that and find a poem that represents that sentiment.Or maybe they type in something like "Don't forget me" and we can find apoem that expresses that sentiment - just about anything we want. We canfind a suitable expression. Maybe it's "love isn't everything."And here's a poem for that too as well. Now I don't want to give the impressionthat the only thing you can do with Sentence Embeddings is this sort of textretrieval because Sentence Embeddings are useful for all sorts of different applications.For example consider a hypothetical app called FindMyShot which storesimages and happens to have captions for each of those images. Now sincethe images associated with captions I can use Sentence Embeddings to findan image based on similarity between the user's query text and the caption.

And there are many other possible usages for these. You can use them fordetecting paraphrases. You can use them for input, for training more complicatedmodels and you can use them for clustering. So let me spend a moment totalk about clustering. If you don't have any pre-arranged text if all thetext comes in from the user then you can still make use of Sentence Embeddings.For example if you had messages or maybe reviews or maybe problem reportsor users you can take a Sentence Embeddings and calculate a vector foreach one of these and then you can use standard clustering algorithms togroup these into as many groups as you want.And what Sentence Embedding means is that these groups are going to besentences close together in meaning. The availability of the SentencebEmeddings is for a number of different languages English, Spanish, French,German, Italian, Portuguese and simplified Chinese. Add on macOS, iOSand iPadOS. Now these Sentence Embeddings are intended for use on naturallanguage text especially text that comes in from the user. You don't haveto do a lot of pre-processing on this text. You don't have to move stopwords for example because the Sentence Embeddings has seen all this intheir training and they're intended for being applied to text that is similarin length to a single sentence, maybe a couple of sentences or a short paragraph.

If you have text that's longer than that, then you can divide it up into sentencesand apply the Sentence Embeddings to each one. Just as we did with our poems.

And also you can make use of the custom embeddings in case you have largenumbers of these that you want to store and look through.

So next I'd like to turn to the topic of custom models.

The idea in custom models is that you bring in your custom training dataand we train a model for you for some particular NLP task. Now thereare two broad kinds of NLP tasks that we support that cover a wide rangeof functionality. The first is a text classifier, which the object is totake a piece of text and supply a label to it. And the other is word word tagger,in which the object is to take a sequence of words in a sentence and supplya label for each one. The custom model training is exposed through Create ML.You passing your training data, Create ML passes it to Natural Language.Natural Language produces a model and what you get out is a Core ML model,either a tagger or a text classifier. And our focus for the last coupleof years has been on applying the power of transfer learning to these models.With transfer learning, the idea is that you can incorporate a pre-existingknowledge of the language so that you don't have to supply quite so muchtraining data in order to produce a good model. And this pre-existing knowledgecomes in by means of word embeddings because the word embeddings havebeen trained on large amounts of natural language text. Now we introducedthis last year for text classifiers and that provides a very powerful solutionfor many apps. For example we can consider a hypothetical app called Merchwhich is intended for transactions between buyers and sellers and theycommunicate with each other about these transactions. But one complaintthe users have perhaps is that they get sometimes spam messages and theydon't want to have to look at all these. Well one possible solution isthat you can train a text classifier by bringing in large amounts of examplesentences labeled as spam or not spam and then train a text classifierand transfer learning model is actually very effective for this sort of task.And then the model in your app will tell you whether a particular messageis likely to be spam and then you can show it appropriately or not to the user.

But what I really want to talk about today is the application of transferlearning to word tagging which is new this year. Now let's go back andtalk about the task of word tagging. As I said the object is to take sequenceof words in a sentence and supply a label for each one. And probably theprototypical task for this is part of speech tagging but it can be usedfor many other things. For example, you can potentially use word taggingto divide a sentence up into phrases or, and this is what we're going tobe talking about here,you can take a sentence and extract important pieces of information fromit even though it's unstructured text. For example in a travel applicationI might want to know where the user is coming from and where they're going to.

Can we make use of this in our Nosh application? Well let's take a look.So we saw that with the Sentence Embedding vectors we could return generalanswers to the user's queries. But there are other things that I mightwant to look at. And a user sentence for example, I might want to know whatkind of food they're looking for, or where they want to get it from.And I can label, potentially label these parts of the sentence as food or acity where the food is coming from. Now the most obvious and simple wayto handle this sort of problem would be to just list all the potentialfoods and potential cities and then just search through the text for eachof those. And of course we support that sort of thing. We have an NLGazetteerclass which provides an efficient representation for any number oftables of items that you might want to look for in text.

But the problem with this approach is that in general you're not goingto be able to list all the potential values that you might want to look for.So as soon as you encounter some piece of text that you hadn't thoughtof before then this simple search is not going to help you.

And the other problem with this approach is that it doesn't take into accountanything about the meaning of words and context. And a word tagger can solveboth of these problems.

In addition it's possible to combine a word tagger and an NLGazetteerfor even greater accuracy. So suppose I've decided that I actuallywant to use a word tagger for my Nosh application. Where do I start? The firstthing to do is to decide what pieces of information I want to get out andassign labels to those. Then I collect sufficiently many as ample sentencesthat the user might enter and I decide how I'm going to label them.And then I actually label those sentences and continue to repeat this processuntil I have enough data to train a good model and I might have to continuerepeating it. If my model ever runs across a situation that it doesn'thandle adequately, usually the solution is to add some more training dataand retrain the model. So what does our training data look like?So in our Nosh application we're going to add some labels to sentenceslike this so we'll use a neutral label. Oh here in this case for otherfor the pieces of text that we're not particularly interested in and we'lluse labels like food from city restaurant for the pieces of text that weare specifically interested in.Now why did I say from_city rather than just city. Because I noticed thatin these example sentences there are two kinds of ways where a city cancome in. The first is where it's the city where the restaurant is locatedwhere the food is supposed to be coming from.And the second is whether it's the city the user is located where the foodis being delivered to. And so I'm going to label those differently as fromcity to city. And because the word tagger can take advantage of the meaningof words in context. It can distinguish between these two provided I giveit sufficient training data. And here is what the training data looks likein Json format which is very convenient for use with Create ML.So what I want to go and train a model and Create ML. It's very simple.If I'm doing it in code I just import Create ML and then I ask Create MLto provide me a model. Now we've supported this for a couple of yearsusing an algorithm known as CRF Conditional Random Fields and it works well.But what's new this year is that we are applying the power of transferlearning to word tagging. And as I said before what transfer learning doesis to allow us to apply pre-existing knowledge of the language so thatyou don't have to supply quite so much training data in order to traina good model. And the way in which this knowledge comes in is via dynamicword embeddings. As we said before the dynamic word embeddings understandsomething about the meaning words and context. Was it just what the wordtagger wants. So we use the dynamic word embedding as an input layer andon top of it we take the data that you provide and we train a multi-layerneural network and that is the thing that actually produces the output labels.

Now this sounds complex but if you want it all you have to do is ask for it.

So instead of asking for the CRF algorithm you just ask for a transferlearning algorithm with dynamic word embeddings and then it will traina model for you. So let's take a look at that in action.

So here's the Nosh application and here is some of the training data thatI have trained, added for it. And I produced, oh somewhat over a thousandsentences of this format. And you'll notice these are in Json format.So each example is a parallel sequence of tokens and labels one label foreach token and you'll notice that cities are labeled from_city or to_cityand you'll notice that foods are labeled and restaurants are labeled.And this is the data that I'm going to use to train my model. And so it's possibleto train it in code but I'm going to train this model using the CreateML application which makes it very simple. So here's the Create ML application.All I have to do is point it to my training data, tell it which of the labelsand tokens, and tell it which algorithm I want to use. In this case we'regoing to use the transfer learning algorithm. And this is going to be for English.

And that's really about all there is to it. I just started off and sentit to train. And the first thing that does is to load all the data extractfeatures from it. And then it's going to start training using transfer learning.And it will train a neural network. So this takes a number of iterationswith each iteration it gets more and more accurate. Now this particulartraining process takes two or three minutes. So I'm not going to make yousit through all of it.

I actually have a pre-trained model. So let's go back to the applicationand take a look at some code.

So here is an example method in the Nosh application that's going to makeuse of our trained model. So we're going to be passed in the user's stringthat they've typed. First thing we'll do is load our model our word taggermodel as an NL model. And then what we're going to do here is useit with a NL tagger. That's convenient because the NL tagger will takecare of all of the tokenization and application and just give us the results.So we've created a custom tag scheme that's just a string constant thatrefers to this set of tags and we'll tell our tagger that's what we wantto use. And then we tell our tagger to use our custom model for this customtag scheme. We attach the user's string to the tagger and that's reallyall there is to it. We can then use the tagger to go through the wordsand it will tell us for each one what it thinks the label should be whetherit's restaurant or food or from_city or to_city or nothing. And then we takenote of those particular pieces of what the user has entered and then wecan use that according to the needs of the application to generate anysort of custom response that we might want to provide. So I've taken theliberty of adding a few custom responses to the Nosh application.Let's try it out. So the user might type something like "Do you deliver to Cupertino?"So what is our model going to tell us? It's going to look at all thesewords and it will notice that Cupertino is a city and it's a citythey want delivery to. So we can generate a customer response that isspecific to Cupertino. Or they might ask you "Do you deliver pizza" and then ourmodel will notice that pizza is a food name so we can generate a customresponse based on pizza. Or maybe they ask if we deliver from a specificrestaurant Pizza City and the one in Cupertino.

And in that case the model will tell us that Pizza City is a restaurantname and Cupertino is a city where the food is coming from.And we can use either or both of those to generate a custom response that mentions those.So that shows the power of word tagging to extract pieces of informationfrom unstructured text. So let's go back to the slides and let me turnit back over to Vivek.

Thank you Doug for showing us a demo of transfer learning for word tagging.Now transfer learning for word tagging is supported for the same languagesas static embeddings and sentence embeddings across Apple platforms.

To get the best use out of transfer learning for word tagging technology.we have a few recommendations. We recommend that you first start offwith a conditional random field, especially for languages such as English.The conditional random field, or CRF, pays attention to syntacticfeatures which are quite useful in many applications. However if you donot know the kind of distribution that you'll be encountering at runtimeit is better to use transfer learning because it provides better generalization.

We also recommend you to use more data for transfer learning for wordtagging since the prediction is at a per token level in contrast with text classification,it requires an order of more magnitude data. As I mentioned NL LinguisticTagger Class has now been marked for deprecation. We strongly encourage you tomove towards Natural Language framework. We also told you how to use confidencecodes along with existing APIs. And this can be used to prune out falsepositives in your application. Then we provided an overview of SentenceEmbedding technology and demonstrated how you can use this in several hypothetical apps.We concluded with a new technology for transfer learning for word tagging.

With that, we'd like to conclude by saying make your arm smarter by usingthe Natural Language framework.
Thank you for your attention.

3:53 -Confidence Scores: tagHypotheses

12:19 -Word Embedding

16:32 -Sentence Embedding

18:36 -Finding nearest neighbor with sentence embedding

22:19 -Sentence embedding compressed as custom embedding

22:47 -Finding nearest neighbor with sentence embedding and custom embedding

33:29 -WordTagger

36:46 -Using custom word tagger

## Code Samples

```swift
import
 NaturalLanguage 


let
 tagger 
=
 
NLTagger
(tagSchemes: [.nameType])

let
 str 
=
 
"Tim Cook is very popular in Spain."


let
 strRange 
=
 
Range
(uncheckedBounds: (str.startIndex, str.endIndex))
tagger.string 
=
 str
tagger.setLanguage(.english, range: strRange)
 
tagger.enumerateTags(in: str.startIndex
..<
str.endIndex, unit: .word, scheme: .nameType, options: .omitWhitespace) { (tag, tokenRange) -> 
Bool
 
in

    
let
 (hypotheses, 
_
) 
=
 tagger.tagHypotheses(at: tokenRange.lowerBound, unit: .word, 
                                               scheme: .nameType, maximumCount: 
1
)
    
print
(hypotheses)
    
return
 
true

}
```

```swift
import
 NaturalLanguage


if
 
let
 embedding 
=
 
NLEmbedding
.wordEmbedding(for: .english) {
    
let
 word 
=
 
"bicycle"

    
    
if
 
let
 vector 
=
 embedding.vector(for: word) {
        
print
(vector)
    }
    
    
let
 dist 
=
 embedding.distance(between:word, and: 
"motorcycle"
)
    
print
(dist)
    
    embedding.enumerateNeighbors(for: word, maximumCount: 
5
) { neighbor, distance 
in

        
print
(
"
\(neighbor)
: 
\(distance.description)
"
)
        
return
 
true

    }
}
```

```swift
import
 NaturalLanguage


if
 
let
 embedding 
=
 
NLEmbedding
.sentenceEmbedding(for: .english) {
    
let
 sentence 
=
 
"This is a sentence."

    
    
if
 
let
 vector 
=
 sentenceEmbedding.vector(for: sentence) {
        
print
(vector)
    }
    
    
let
 dist 
=
 sentenceEmbedding.distance(between: sentence, and: 
"That is a sentence."
)
    
print
(dist)
}
```

```swift
func
 
answerKey
(
for
 
string
: 
String
)
 -> 
String
? {
        
guard
 
let
 embedding 
=
 
NLEmbedding
.sentenceEmbedding(for: .english) 
else
 { 
return
 
nil
 }
        
guard
 
let
 queryVector 
=
 embedding.vector(for: string) 
else
 { 
return
 
nil
 }

        
var
 answerKey: 
String
? 
=
 
nil

        
var
 answerDistance 
=
 
2.0


        
for
 (key, vectors) 
in
 
self
.faqEmbeddings {
            
for
 (vector) 
in
 vectors {
                
let
 distance 
=
 
self
.cosineDistance(vector, queryVector)
                
if
 (distance 
<
 answerDistance) {
                    answerDistance 
=
 distance
                    answerKey 
=
 key
                }
            }
        }

        
return
 answerKey
    }
```

```swift
import
 NaturalLanguage

import
 CreateML



let
 embedding 
=
 
try
 
MLWordEmbedding
(dictionary: sentenceVectors)


try
 embedding.write(to: 
URL
(fileURLWithPath: 
"/tmp/Verse.mlmodel"
))
```

```swift
func
 
answerKeyCustom
(
for
 
string
: 
String
)
 -> 
String
? {
        
guard
 
let
 embedding 
=
 
NLEmbedding
.sentenceEmbedding(for: .english) 
else
 { 
return
 
nil
 }
        
guard
 
let
 queryVector 
=
 embedding.vector(for: string) 
else
 { 
return
 
nil
 }

        
guard
 
let
 (nearestLineKey, 
_
) 
=
 
self
.customEmbedding.neighbors(for: queryVector, maximumCount: 
1
).first 
else
 { 
return
 
nil
 }

        
return
 
self
.poemKeyFromLineKey(nearestLineKey)
    }
```

```swift
import
 CreateML


let
 modelParameters 
=
 
MLWordTagger
.
ModelParameters
(algorithm: .crf(revision: 
1
))
```

```swift
func
 
findTags
(
for
 
string
: 
String
)
 {
        
let
 model 
=
 
try!
 
NLModel
(contentsOf: 
Bundle
.main.url(forResource: 
"Nosh"
, withExtension: 
"mlmodelc"
)
!
)
        
let
 tagger 
=
 
NLTagger
(tagSchemes: [
NoshTags
])

        tagger.setModels([model], forTagScheme: 
NoshTags
)
        tagger.string 
=
 string

        tagger.enumerateTags(in: string.startIndex
..<
string.endIndex, unit: .word, scheme: 
NoshTags
, options: .omitWhitespace) { (tag, tokenRange) -> 
Bool
 
in


            
let
 name 
=
 
String
(string[tokenRange])

            
switch
 tag {
                
case
 
NoshTagRestaurant
:
                    
self
.noteRestaurant(name)
                
case
 
NoshTagFood
:
                    
self
.noteFood(name)
                
case
 
NoshTagFromCity
:
                    
self
.noteFromCity(name)
                
case
 
NoshTagToCity
:
                    
self
.noteToCity(name)
                
default
:
                    
break

            }
            
return
 
true

        }
    }
```

