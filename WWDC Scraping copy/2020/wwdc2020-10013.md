# Wwdc2020 10013

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Get to know Metal function pointersMetal is a low-level, low-overhead hardware-accelerated graphics framework and shader application programming interface for producing stunning visual effects in applications. Discover how to make your shaders written in Metal Shading Language more programmable and extensible by using function pointers. Learn how to take advantage of this new feature for dynamic flow control in Metal shaders. Discover how to use function pointers to specify custom intersection functions in your ray tracing application. We'll explain how function pointers allow several compilations models so you can balance GPU pipeline size against runtime performance.ResourcesAccelerating ray tracing and motion blur using MetalDebugging the shaders within a draw command or compute dispatchMetalMetal Feature Set TablesMetal for Accelerating Ray TracingMetal Performance ShadersMetal Shading Language SpecificationModern Rendering with MetalHD VideoSD VideoRelated VideosWWDC21Discover compilation workflows in MetalDiscover Metal debugging, profiling, and asset creation tools

Metal is a low-level, low-overhead hardware-accelerated graphics framework and shader application programming interface for producing stunning visual effects in applications. Discover how to make your shaders written in Metal Shading Language more programmable and extensible by using function pointers. Learn how to take advantage of this new feature for dynamic flow control in Metal shaders. Discover how to use function pointers to specify custom intersection functions in your ray tracing application. We'll explain how function pointers allow several compilations models so you can balance GPU pipeline size against runtime performance.

Accelerating ray tracing and motion blur using Metal

Debugging the shaders within a draw command or compute dispatch

Metal

Metal Feature Set Tables

Metal for Accelerating Ray Tracing

Metal Performance Shaders

Metal Shading Language Specification

Modern Rendering with Metal

HD VideoSD Video

HD Video

SD Video

Discover compilation workflows in Metal

Discover Metal debugging, profiling, and asset creation tools

Search this videoâ€¦Hello and welcome to WWDC.Hello everyone.My name is Rich Forster, and I'm a GPU software engineer at Apple.In this session, I'm going to talk to you about the new function pointers APIthat we've added to Metal this year.Let's start with the humble function pointer.Function pointers give us the power to refer to code that we can call.They make our code extensible by allowing us to write our codeso that it can later call code that we have never seen before.The classic example is the callback,where execution jumps to code identified with a function pointer.This lets us provide functions for plug-ins,specializations or notifications.But we can do so much more.Today I am going to talk about how we expose function pointer support in Metaland how we use visible functions to achieve our goals.I will cover the different compilation models we support,what they mean, and when to use them.I will then discuss tables of visible functionsand finish with a discussion of performance.So let's start with function pointers in Metal.When we added function pointers to Metal,we knew there would be a range of opportunities to use them.The obvious case is ray tracing, where we use function pointersto specify our custom intersection functions.And I would encourage you to watch the ray tracing talkwhich accompanies this session.Ray tracing is also a great placeto talk about other uses of function pointers,and that's where I'd like to start today.When we apply ray tracing to the classic Cornell Box,we fire our rays into the scene,and when they hit surfaces, we need to shade the intersections.Typically, we have the material for the surface that we intersect,and then we will continue tracing until we hit a light,at which point we will evaluate the contribution of the light.Let's revisit the process flow for ray tracing.First, we generate rays which start from the cameraand are emitted into the scene.We then test those rays for intersection with the geometry in the scene.Next, we compute a color at each intersection pointand update the image.This process is called shading.The shading process can also generate additional rays.So we test those rays for intersection with the sceneand repeat this process as many times as we'd liketo simulate light bouncing around the scene.Today, we're going to simplify things for this presentationand start by considering a path tracer that performs a single intersectionand then shades the result of that intersection.This will be in a single computer kernel that creates a compute pipelinethat includes the code for each of these three stages.However, the stage I want to focus on is shading.This is the last step before we output our imageand provides a range of opportunities to use function pointers.Within our ray tracing kernel, shading happens near the end.Once we have an intersection, we find the matching materialand perform our shading for that material.In this example,all of our material and lighting code lives in the shade functionwhich exists elsewhere in our file.Let's dig deeper into the shade function.Our shade function has several steps.This is a simple path tracer,so it immediately calculates the lighting from the lightat the intersection point on the surface.And then we use the material to apply that lighting at the intersection point.In terms of flow, we can consider lighting and material as separate stages.But there's more than one type of light and more than one type of material.We can break lighting into separate types of light,which will require different code for each light type.And the code for materials can be even more varied than the code for lights.We are going to use the new visible function type in Metalto help us work with our lighting and material functions.Visible is a new function qualification attribute like vertex, fragment or kernel.We can use the visible attributes on function definitions,and when we use the visible attribute, we are declaringthat we want to manipulate the function from the Metal API.We check if we can use the API to perform this manipulation with the device query.With visible functions, we can consider our codeas a flexible object that we can refer to.In this case, let's consider the code for our area lights.That code is an object that can exist outside of the kernelthat represents our pipeline.The code can exist in another Metal file or another Metal library.To declare our lighting function as visible,we add the visible attribute before the definition of the function.This will allow us to create a Metal function objectto represent the code in this function.Our next step is to connect our Metal shader code to our pipelineso that we can call it.First, we wrap our area light code in a Metal function object.Then the new Metal function object can be added to the pipeline.With the visible attribute on our area light function,we can wrap it in a Metal function object on the CPU.We create a Metal function object by name for our visible functionusing the standard methods.We then add the function object to the compute pipeline descriptoras part of the linkedFunctions to allow the pipeline creation processto add it to the pipeline so that we can refer to it later with a function pointer.Now that we're talking about adding visible functions to our pipelines,we need to discuss the compilation model choices that we have available.Once we have Metal function objectsfor each of our lighting and material functions,we can add them to our pipeline.When we build our pipeline, we can take a copy of each of those visible functions.We call this "single compilation,"since we then have a single object that represents our pipelineand all of the visible functions that it uses.We use the same linkedFunctions object that we saw earlierto list the functions that we may call from our pipeline.Then we create our ComputePipelineState using the standard pipeline creation API.The creation of our pipeline results in an objectthat contains both a specialized version of our kernel codeand specialized copies of all of our functions.This specialization is similar to link-time optimization,where the kernel code and the visible functionscan be optimized based on their usage.However, this optimization can increase the time taken to create our pipelineand results in a larger pipeline object due to the copies of the function objectsthat we add into the pipeline.But what we may want to do is have our function objects outside of our pipelineas separate objects that our pipeline can call but does not have to copy.This is the foundation of a separately compiled pipeline.When we create each Metal function object,we can compile each function to a stand-alone GPU binary formso that we can keep the function code separateand reuse it across multiple pipelines.To compile our functions to binary, we will use a function descriptor.This allows us to add options to the creation of a Metal function object.In the case of creating our function,this snippet of code creates the same resulting Metal functionas the same method with a name parameter.However, the functionDescriptor lets us specify more configuration options.We request binary precompilationby using the "options" property of the descriptor.The function creation will now precompile our function to binary.We then provide our new functionin the binary function's array of the linkedFunctions object.This indicates that we are referring to the binary version of the functionrather than requesting copying and specialization of the functionfor the compute pipeline.As you can see, we can mix and match precompiled functionswith those we want specialized.And then we use the standard call to create the pipeline, as before.Now that we have covered both single and separate compilation,this is a good point to compare the advantages of each.For single compilation, we create visible functions using our standard methods.For separate compilation, we will precompile to binaryto allow us to share the compile binaryand avoid the binary compilation of our function during pipeline creation.When we configure our pipeline descriptor,we use the functions array to indicate that we want them specializedand the binaryFunctions array to indicate that we want to use the binary versions.The result of specializationis a larger pipeline in the single-compilation case.Separate compilation will result in a pipelinethat adds calls to binary functions,and the binary functions will be shared.Specialization of functions also requires a longer pipeline creation timethan adding calls to binary functions.As I mentioned earlier, this is similar to link-time optimization,where the compiler can take advantage of knowing the whole pipelinebut requires extra build time to apply the specializations.All of that function specializationmeans that single compilation has the best runtime performance.The flexibility of the separately compiled pipelinemeans that there is some runtime overhead to calling our binary functions.As we saw earlier,you can mix sets of precompiled functions and functions for specialization,but a fully specialized, single-compilation pipelinewill offer the best performance.It should always be possibleto create a new single-compilation pipeline in the backgroundto replace any separately compiled functions in an existing pipeline.Going back to our separate compilation pipeline,we have our array of functions available to call from the pipeline.However, whenever you think you have a fixed set of functions,another function always appears.In this case, we have a new wood material that we want to add to our pipeline.Incremental compilation is intended for adding new functionsto an existing pipeline.The appearance of new functions is typically quite commonin dynamic environments,especially games where streaming may load new assets with new shaders.Putting this into code,first we need to make the choice to support incremental compilationwhen we create our initial compute pipeline.The default is to not support incremental compilationbecause enabling the later addition of binary functionsmeans that pipeline creation has to expect possible calls to binary functionseven if none are specified at pipeline creation time.We then use a function descriptor to create the Metal function objectfor our wood shader as a precompiled binary function.Finally, we use a newly added method on ComputePipelineStateto create a compute pipeline with any additional binary functions.Next, I want to talk about visible function tables.Visible function tables are how we pass function pointersto our visible functions to the GPU.Going back to the set of the lighting and material functionsthat we considered for our shading,we need to provide these to the kernel running on the GPU.To allow us to group our related functions togetherand pass them to the GPU,we create visible function tables using the Metal API.Visible function tables are then an object that we can pass to our Metal shader code.Before we start, let's add some using declarationsto define our function pointer types to avoid making our code examples too wordy.Visible function tables can be specified as kernel parameters,and they use buffer binding points.We can also pass them in argument buffers.Then, in our shaders, we access our functions by index.We can take a pointer to the function or call it directly from the table.On the CPU, we allocate the visible function tablesfrom the ComputePipelineStatebased on the number of function entries we want in the table.We then populate the table with handles to the functionsthat we specified when creating the pipeline state.Finally, when we come to use the table,we set it on our computeCommandEncoder or argumentEncoder as required.Don't forget to call useResource if using an argumentEncoder.To ensure that using incrementally compiled pipelinesis as low-cost as possible,we ensure that you can reuse the visible function tablesfrom the ancestor pipelines.The handles that are already in the table are still valid,and you just need to add the new function handles to the table.You do not need to create and build an entirely new table.And finally, you just need to make surethat you do not access the newly added function handles from older pipelines.Last but not least today,I want to discuss the performance considerationsof using function pointers in your application.There's three main areas we will cover in performance.Let's start with function groups for optimization.Going back to our earlier diagram,we could already see that we had groups of functions for specific purposes.However, pipeline creation is unaware of these relationships.We can group the functions based upon their use in the shader.We know which functions we will be using for lightingand which we will use for materials.To express the grouping of functions in our shading function,we've updated it to include function groups for the calls we make.In Metal, function groups let us tell the compilerwhere the functions will be used.In our shader code, we apply function groups attributesto the lines where we invoke our functionsto give names to the group of possible functionsthat are callable from that location.Then, when configuring our compute pipeline descriptor,we specify the groups in a dictionarywith the setter functions that may be called for each group.This gives the compiler the maximum amount of information possibleand can help with the optimization when generating the pipeline.On top of the specialization that the compiler performsfor functions in our functions array,the compiler is able to take advantage of any commonalitiesbetween the setter functions in each groupand then apply that to the call site marked with the function groups attribute.All this new flexibility also means that you can now implement recursive functions.If we go back to where we started with the ray tracing process flow,this model of ray tracing, where we consider multiple bounces,can be implemented in either an iterative or recursive manner.If we focus on the relationship between intersection and shading,we can follow this call graph.With our recursive path tracer,after an intersection, we shade the intersection point.In this case, we hit our new wood material function.This wood material includes a reflection component,and so it intersects another ray with the scene and shades the intersection.Again, we intersect a surface covered in the wood material.This then reflects and intersects the scene again.We hit the wood material one more time.We could continue, but typically you would limit the depth in a recursive ray tracerfor both performance and to avoid overflowing the stack,so we will stop here.To support the varying stack usagefor compute kernels that call chains of visible functions,we expose the maxCallStackDepth propertyon the compute pipeline descriptorso that we can specify how deep we expect to call functions from our kernel.The default is 1so that the typical use cases of visible functions and intersection functionswork out of the box.This value is expected to be used for any chains of visible function calls,but in the case of ray tracing, this code could have been writtenin an iterative manner to reduce stack usage.The last performance consideration I want to discuss todaywas the impact of divergence when we use function pointers.Before diving into function pointers, though,I should highlight that last year's "Ray Tracing with Metal" presentationcovered high-level methods to handle the divergence inherent in ray tracingwhen subsequent bounces become less coherent.This included ray coherency with block linear layout,ray compaction for active raysand interleaved tiling for load balancing across GPUs.I recommend reviewing that presentation for a great review of ideasof ray tracing optimizations.For function pointers, however,we need to consider divergence at the thread level.When we dispatch our threadgroups,we know that they execute in smaller groups of threads,defined as a SIMD group.In this example, we have eight SIMD groups.The threads in the SIMD group perform bestwhen they execute the same instruction at the same time.Divergence occurs when the next instruction to executeis different for the threads of the SIMD group,typically, when they reach a branch in our shader code.This is normally handled by letting a subset of the threadsexecute their instruction while the other threads are left unused.We have the best performancewhen all threads are actively executing useful instructions.Function pointers are another case that can lead to divergence.When we invoke a function via a function pointer,we need to consider how divergent the setter function pointers will be.In the worst case, within a SIMD group,each thread could be calling a different function,and the execution time will be the time to executeeach of the required functions one after the other.But if we consider the entire threadgroup,we may have enough work to create full SIMD groups.To handle our divergence,we are going to reorder the function calls to introduce coherence to our SIMD groups.This means that the function call for one threadwill likely be executed by another thread in the threadgroup.In terms of code, we will start by writing all the function parametersand the index of the threadand the function to call to threadgroup memory.We will then use our favorite sort to sort a function indices.Next, we invoke the functions in their sorted order.The results of the functions go back to threadgroup memory,and then each thread can read its result from there.We can also implement a similar version using device memoryfor our parameters and results.In the case of calling complex functions,this should reduce the overhead of divergence.There's a lot of great things you can do with function pointers in Metal.Today we've covered the creation and use of visible functionsand how we add them to visible function tablesto provide functions that we can call from our compute kernels.We've discussed the different compilation modelsthat allow you to choose how you balance pipeline sizeand creation time against runtime performanceand needing to be able to dynamically add functions.And we finished on some performance considerationsfor using function pointers in your application.I've really enjoyed working on function pointers this year,and I'm really glad to be able to share it with you.I hope you've had a great WWDC.

Hello and welcome to WWDC.

Hello everyone.My name is Rich Forster, and I'm a GPU software engineer at Apple.In this session, I'm going to talk to you about the new function pointers APIthat we've added to Metal this year.Let's start with the humble function pointer.

Function pointers give us the power to refer to code that we can call.They make our code extensible by allowing us to write our codeso that it can later call code that we have never seen before.

The classic example is the callback,where execution jumps to code identified with a function pointer.This lets us provide functions for plug-ins,specializations or notifications.But we can do so much more.Today I am going to talk about how we expose function pointer support in Metaland how we use visible functions to achieve our goals.I will cover the different compilation models we support,what they mean, and when to use them.I will then discuss tables of visible functionsand finish with a discussion of performance.So let's start with function pointers in Metal.

When we added function pointers to Metal,we knew there would be a range of opportunities to use them.The obvious case is ray tracing, where we use function pointersto specify our custom intersection functions.And I would encourage you to watch the ray tracing talkwhich accompanies this session.

Ray tracing is also a great placeto talk about other uses of function pointers,and that's where I'd like to start today.

When we apply ray tracing to the classic Cornell Box,we fire our rays into the scene,and when they hit surfaces, we need to shade the intersections.Typically, we have the material for the surface that we intersect,and then we will continue tracing until we hit a light,at which point we will evaluate the contribution of the light.Let's revisit the process flow for ray tracing.First, we generate rays which start from the cameraand are emitted into the scene.

We then test those rays for intersection with the geometry in the scene.

Next, we compute a color at each intersection pointand update the image.This process is called shading.

The shading process can also generate additional rays.So we test those rays for intersection with the sceneand repeat this process as many times as we'd liketo simulate light bouncing around the scene.Today, we're going to simplify things for this presentationand start by considering a path tracer that performs a single intersectionand then shades the result of that intersection.This will be in a single computer kernel that creates a compute pipelinethat includes the code for each of these three stages.However, the stage I want to focus on is shading.This is the last step before we output our imageand provides a range of opportunities to use function pointers.

Within our ray tracing kernel, shading happens near the end.

Once we have an intersection, we find the matching materialand perform our shading for that material.In this example,all of our material and lighting code lives in the shade functionwhich exists elsewhere in our file.Let's dig deeper into the shade function.

Our shade function has several steps.

This is a simple path tracer,so it immediately calculates the lighting from the lightat the intersection point on the surface.

And then we use the material to apply that lighting at the intersection point.

In terms of flow, we can consider lighting and material as separate stages.But there's more than one type of light and more than one type of material.

We can break lighting into separate types of light,which will require different code for each light type.

And the code for materials can be even more varied than the code for lights.We are going to use the new visible function type in Metalto help us work with our lighting and material functions.

Visible is a new function qualification attribute like vertex, fragment or kernel.We can use the visible attributes on function definitions,and when we use the visible attribute, we are declaringthat we want to manipulate the function from the Metal API.We check if we can use the API to perform this manipulation with the device query.

With visible functions, we can consider our codeas a flexible object that we can refer to.In this case, let's consider the code for our area lights.That code is an object that can exist outside of the kernelthat represents our pipeline.The code can exist in another Metal file or another Metal library.

To declare our lighting function as visible,we add the visible attribute before the definition of the function.

This will allow us to create a Metal function objectto represent the code in this function.

Our next step is to connect our Metal shader code to our pipelineso that we can call it.First, we wrap our area light code in a Metal function object.Then the new Metal function object can be added to the pipeline.

With the visible attribute on our area light function,we can wrap it in a Metal function object on the CPU.

We create a Metal function object by name for our visible functionusing the standard methods.

We then add the function object to the compute pipeline descriptoras part of the linkedFunctions to allow the pipeline creation processto add it to the pipeline so that we can refer to it later with a function pointer.Now that we're talking about adding visible functions to our pipelines,we need to discuss the compilation model choices that we have available.

Once we have Metal function objectsfor each of our lighting and material functions,we can add them to our pipeline.

When we build our pipeline, we can take a copy of each of those visible functions.We call this "single compilation,"since we then have a single object that represents our pipelineand all of the visible functions that it uses.

We use the same linkedFunctions object that we saw earlierto list the functions that we may call from our pipeline.Then we create our ComputePipelineState using the standard pipeline creation API.

The creation of our pipeline results in an objectthat contains both a specialized version of our kernel codeand specialized copies of all of our functions.This specialization is similar to link-time optimization,where the kernel code and the visible functionscan be optimized based on their usage.

However, this optimization can increase the time taken to create our pipelineand results in a larger pipeline object due to the copies of the function objectsthat we add into the pipeline.But what we may want to do is have our function objects outside of our pipelineas separate objects that our pipeline can call but does not have to copy.This is the foundation of a separately compiled pipeline.

When we create each Metal function object,we can compile each function to a stand-alone GPU binary formso that we can keep the function code separateand reuse it across multiple pipelines.

To compile our functions to binary, we will use a function descriptor.This allows us to add options to the creation of a Metal function object.In the case of creating our function,this snippet of code creates the same resulting Metal functionas the same method with a name parameter.However, the functionDescriptor lets us specify more configuration options.

We request binary precompilationby using the "options" property of the descriptor.The function creation will now precompile our function to binary.

We then provide our new functionin the binary function's array of the linkedFunctions object.This indicates that we are referring to the binary version of the functionrather than requesting copying and specialization of the functionfor the compute pipeline.As you can see, we can mix and match precompiled functionswith those we want specialized.And then we use the standard call to create the pipeline, as before.

Now that we have covered both single and separate compilation,this is a good point to compare the advantages of each.

For single compilation, we create visible functions using our standard methods.For separate compilation, we will precompile to binaryto allow us to share the compile binaryand avoid the binary compilation of our function during pipeline creation.

When we configure our pipeline descriptor,we use the functions array to indicate that we want them specializedand the binaryFunctions array to indicate that we want to use the binary versions.

The result of specializationis a larger pipeline in the single-compilation case.Separate compilation will result in a pipelinethat adds calls to binary functions,and the binary functions will be shared.

Specialization of functions also requires a longer pipeline creation timethan adding calls to binary functions.As I mentioned earlier, this is similar to link-time optimization,where the compiler can take advantage of knowing the whole pipelinebut requires extra build time to apply the specializations.

All of that function specializationmeans that single compilation has the best runtime performance.The flexibility of the separately compiled pipelinemeans that there is some runtime overhead to calling our binary functions.As we saw earlier,you can mix sets of precompiled functions and functions for specialization,but a fully specialized, single-compilation pipelinewill offer the best performance.It should always be possibleto create a new single-compilation pipeline in the backgroundto replace any separately compiled functions in an existing pipeline.

Going back to our separate compilation pipeline,we have our array of functions available to call from the pipeline.However, whenever you think you have a fixed set of functions,another function always appears.

In this case, we have a new wood material that we want to add to our pipeline.Incremental compilation is intended for adding new functionsto an existing pipeline.The appearance of new functions is typically quite commonin dynamic environments,especially games where streaming may load new assets with new shaders.

Putting this into code,first we need to make the choice to support incremental compilationwhen we create our initial compute pipeline.The default is to not support incremental compilationbecause enabling the later addition of binary functionsmeans that pipeline creation has to expect possible calls to binary functionseven if none are specified at pipeline creation time.

We then use a function descriptor to create the Metal function objectfor our wood shader as a precompiled binary function.

Finally, we use a newly added method on ComputePipelineStateto create a compute pipeline with any additional binary functions.

Next, I want to talk about visible function tables.Visible function tables are how we pass function pointersto our visible functions to the GPU.

Going back to the set of the lighting and material functionsthat we considered for our shading,we need to provide these to the kernel running on the GPU.

To allow us to group our related functions togetherand pass them to the GPU,we create visible function tables using the Metal API.

Visible function tables are then an object that we can pass to our Metal shader code.

Before we start, let's add some using declarationsto define our function pointer types to avoid making our code examples too wordy.

Visible function tables can be specified as kernel parameters,and they use buffer binding points.We can also pass them in argument buffers.

Then, in our shaders, we access our functions by index.We can take a pointer to the function or call it directly from the table.

On the CPU, we allocate the visible function tablesfrom the ComputePipelineStatebased on the number of function entries we want in the table.

We then populate the table with handles to the functionsthat we specified when creating the pipeline state.

Finally, when we come to use the table,we set it on our computeCommandEncoder or argumentEncoder as required.Don't forget to call useResource if using an argumentEncoder.To ensure that using incrementally compiled pipelinesis as low-cost as possible,we ensure that you can reuse the visible function tablesfrom the ancestor pipelines.The handles that are already in the table are still valid,and you just need to add the new function handles to the table.You do not need to create and build an entirely new table.And finally, you just need to make surethat you do not access the newly added function handles from older pipelines.

Last but not least today,I want to discuss the performance considerationsof using function pointers in your application.There's three main areas we will cover in performance.Let's start with function groups for optimization.

Going back to our earlier diagram,we could already see that we had groups of functions for specific purposes.However, pipeline creation is unaware of these relationships.

We can group the functions based upon their use in the shader.We know which functions we will be using for lightingand which we will use for materials.

To express the grouping of functions in our shading function,we've updated it to include function groups for the calls we make.In Metal, function groups let us tell the compilerwhere the functions will be used.

In our shader code, we apply function groups attributesto the lines where we invoke our functionsto give names to the group of possible functionsthat are callable from that location.

Then, when configuring our compute pipeline descriptor,we specify the groups in a dictionarywith the setter functions that may be called for each group.This gives the compiler the maximum amount of information possibleand can help with the optimization when generating the pipeline.

On top of the specialization that the compiler performsfor functions in our functions array,the compiler is able to take advantage of any commonalitiesbetween the setter functions in each groupand then apply that to the call site marked with the function groups attribute.

All this new flexibility also means that you can now implement recursive functions.

If we go back to where we started with the ray tracing process flow,this model of ray tracing, where we consider multiple bounces,can be implemented in either an iterative or recursive manner.

If we focus on the relationship between intersection and shading,we can follow this call graph.

With our recursive path tracer,after an intersection, we shade the intersection point.

In this case, we hit our new wood material function.This wood material includes a reflection component,and so it intersects another ray with the scene and shades the intersection.

Again, we intersect a surface covered in the wood material.This then reflects and intersects the scene again.We hit the wood material one more time.We could continue, but typically you would limit the depth in a recursive ray tracerfor both performance and to avoid overflowing the stack,so we will stop here.To support the varying stack usagefor compute kernels that call chains of visible functions,we expose the maxCallStackDepth propertyon the compute pipeline descriptorso that we can specify how deep we expect to call functions from our kernel.The default is 1so that the typical use cases of visible functions and intersection functionswork out of the box.This value is expected to be used for any chains of visible function calls,but in the case of ray tracing, this code could have been writtenin an iterative manner to reduce stack usage.The last performance consideration I want to discuss todaywas the impact of divergence when we use function pointers.Before diving into function pointers, though,I should highlight that last year's "Ray Tracing with Metal" presentationcovered high-level methods to handle the divergence inherent in ray tracingwhen subsequent bounces become less coherent.This included ray coherency with block linear layout,ray compaction for active raysand interleaved tiling for load balancing across GPUs.I recommend reviewing that presentation for a great review of ideasof ray tracing optimizations.

For function pointers, however,we need to consider divergence at the thread level.When we dispatch our threadgroups,we know that they execute in smaller groups of threads,defined as a SIMD group.In this example, we have eight SIMD groups.

The threads in the SIMD group perform bestwhen they execute the same instruction at the same time.

Divergence occurs when the next instruction to executeis different for the threads of the SIMD group,typically, when they reach a branch in our shader code.This is normally handled by letting a subset of the threadsexecute their instruction while the other threads are left unused.We have the best performancewhen all threads are actively executing useful instructions.

Function pointers are another case that can lead to divergence.When we invoke a function via a function pointer,we need to consider how divergent the setter function pointers will be.In the worst case, within a SIMD group,each thread could be calling a different function,and the execution time will be the time to executeeach of the required functions one after the other.

But if we consider the entire threadgroup,we may have enough work to create full SIMD groups.

To handle our divergence,we are going to reorder the function calls to introduce coherence to our SIMD groups.This means that the function call for one threadwill likely be executed by another thread in the threadgroup.

In terms of code, we will start by writing all the function parametersand the index of the threadand the function to call to threadgroup memory.We will then use our favorite sort to sort a function indices.

Next, we invoke the functions in their sorted order.

The results of the functions go back to threadgroup memory,and then each thread can read its result from there.We can also implement a similar version using device memoryfor our parameters and results.In the case of calling complex functions,this should reduce the overhead of divergence.There's a lot of great things you can do with function pointers in Metal.Today we've covered the creation and use of visible functionsand how we add them to visible function tablesto provide functions that we can call from our compute kernels.

We've discussed the different compilation modelsthat allow you to choose how you balance pipeline sizeand creation time against runtime performanceand needing to be able to dynamically add functions.And we finished on some performance considerationsfor using function pointers in your application.I've really enjoyed working on function pointers this year,and I'm really glad to be able to share it with you.I hope you've had a great WWDC.

3:09 -Our simple Path Tracer in Metal Shading Language:

3:30 -Our shading  function

4:59 -Declare a function as visible

5:30 -Single compilation pipeline on CPU

7:43 -Introducing MTLFunctionDescriptor

8:08 -Binary preâ€“compilation

8:20 -Binary functions

11:04 -Incremental compilation pipeline

12:22 -Visible function tables on the GPU

12:49 -Visible function tables on the CPU

14:23 -Function groups on GPU

14:49 -Function groups on CPU

## Code Samples

```swift
float3 shade(...);

[[kernel]] 
void
 rtKernel(...
                         device Material *materials,
                         constant Light &light)
{
    
// ...


    device Material &material = materials[intersection.geometry_id];
    float3 result = shade(ray, triangleIntersectionData, material, light);

    
// ...

}
```

```swift
float3 shade(...)
{
    Lighting lighting = LightingFromLight(light, triangleIntersectionData);

    
return
 CalculateLightingForMaterial(material, lighting, triangleIntersectionData);
}
```

```swift
[[visible]]
Lighting Area(Light light, TriangleIntersectionData triangleIntersectionData)
{
    Lighting result;
    
    
// Clever math code ...

    
    
return
 result;
}
```

```swift
// Single compilation configuration


let
 linkedFunctions 
=
 
MTLLinkedFunctions
()
linkedFunctions.functions 
=
 [area, spot, sphere, hair, glass, skin]
computeDescriptor.linkedFunctions 
=
 linkedFunctions


// Pipeline creation


let
 pipeline 
=
 
try
 device.makeComputePipelineState(descriptor: computeDescriptor,
                                                   options: [],
                                                   reflection: 
nil
)
```

```swift
// Create by function descriptor:


let
 functionDescriptor 
=
 
MTLFunctionDescriptor
()
functionDescriptor.name 
=
 
"Area"


// More configuration goes here


let
 areaBinaryFunction 
=
 
try
 library.makeFunction(descriptor: functionDescriptor)
```

```swift
// Create and compile by function descriptor:


let
 functionDescriptor 
=
 
MTLFunctionDescriptor
()
functionDescriptor.name 
=
 
"Area"

functionDescriptor.options 
=
 
MTLFunctionOptions
.compileToBinary

let
 areaBinaryFunction 
=
 
try
 library.makeFunction(descriptor: functionDescriptor)
```

```swift
// Specify binary functions on compute pipeline descriptor


let
 linkedFunctions 
=
 
MTLLinkedFunctions
()
linkedFunctions.functions 
=
 [spot, sphere, hair, glass, skin]
linkedFunctions.binaryFunctions 
=
 [areaBinaryFunction]
computeDescriptor.linkedFunctions 
=
 linkedFunctions


// Pipeline creation


let
 pipeline 
=
 
try
 device.makeComputePipelineState(descriptor: computeDescriptor,
                                                   options: [],
                                                   reflection: 
nil
)
```

```swift
// Create initial pipeline with option

computeDescriptor.supportAddingBinaryFunctions 
=
 
true



// Create and compile by function descriptor


let
 functionDescriptor 
=
 
MTLFunctionDescriptor
()
functionDescriptor.name 
=
 
"Wood"

functionDescriptor.options 
=
 
MTLFunctionOptions
.compileToBinary

let
 wood 
=
 
try
 library.makeFunction(descriptor: functionDescriptor)


// Create new pipeline from existing pipeline


let
 newPipeline 
=

   
try
 pipeline.makeComputePipelineStateWithAdditionalBinaryFunctions(functions: [wood])
```

```swift
// Helper using declaration in Metal

using LightingFunction = Lighting(Light, TriangleIntersectionData);
using MaterialFunction = float3(Material, Lighting, TriangleIntersectionData);


// Specify tables as kernel parameters

visible_function_table<LightingFunction> lightingFunctions [[buffer(
1
)]],
visible_function_table<MaterialFunction> materialFunctions [[buffer(
2
)]],


// Access via index

LightingFunction *lightingFunction = lightingFunctions[light.index];
Lighting lighting = lightingFunction(light, triangleIntersection);

return
 materialFunctions[material.index](material, lighting, triangleIntersection);
```

```swift
// Initialize descriptor


let
 vftDescriptor 
=
 
MTLVisibleFunctionTableDescriptor
()
vftDescriptor.functionCount 
=
 
3


let
 lightingFunctionTable 
=
 pipeline.makeVisibleFunctionTable(descriptor: vftDescriptor)
!



// Find and set functions by handle


let
 functionHandle 
=
 pipeline.functionHandle(function: spot)
!

lightingFunctionTable.setFunction(functionHandle, index:
0
)


// Find and set functions by handle

computeCommandEncoder.setVisibleFunctionTable(lightingFunctionTable, bufferIndex:
1
)
argumentEncoder.setVisibleFunctionTable(lightingFunctionTable, index:
1
)
```

```swift
// Add function groups to our shading function

float3 shade(...)
{
    LightingFunction *lightingFunction = lightingFunctions[light.index];
    [[function_groups(
"lighting"
)]] Lighting lighting = lightingFunction(light,
triangleIntersection);
    
    MaterialFunction *materialFunction = materialFunctions[material.index];
    [[function_groups(
"material"
)]] float3 result = materialFunction(material, lighting, triangleIntersection);
    
return
 result;
}
```

```swift
// Function Group configuration


let
 linkedFunctions 
=
 
MTLLinkedFunctions
()
linkedFunctions.functions 
=
 [area, spot, sphere, hair, glass, skin]
linkedFunctions.groups 
=
 [
"lighting"
 : [area, spot, sphere ],
                          
"material"
 : [hair, glass, skin ] ]
computeDescriptor.linkedFunctions 
=
 linkedFunctions
```

