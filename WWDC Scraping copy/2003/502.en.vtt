WEBVTT

00:00:35.400 --> 00:00:37.080
Good morning.

00:00:37.130 --> 00:00:40.200
Welcome to session 502,
Power Macintosh G5

00:00:40.200 --> 00:00:41.350
System Architecture Overview.

00:00:41.360 --> 00:00:43.400
My name is Mark Tozer-Vilches.

00:00:43.400 --> 00:00:45.780
I am the desktop hardware
evangelist for Apple Computer.

00:00:45.780 --> 00:00:48.920
So what do you guys think
of the Power Macintosh G5?

00:00:48.930 --> 00:00:50.020
Awesome!

00:00:53.920 --> 00:00:54.560
Well, great.

00:00:54.620 --> 00:00:57.880
Steve gave a great,
exciting presentation yesterday.

00:00:57.980 --> 00:00:59.740
Today we're going to follow
that up with a little bit more

00:00:59.740 --> 00:01:02.580
in-depth technical information,
both about the CPU as well

00:01:02.710 --> 00:01:04.060
as the system architecture.

00:01:04.060 --> 00:01:07.640
So without further ado,
I'd like to introduce Mr.

00:01:07.700 --> 00:01:10.980
Peter Sandon,
the IBM Senior Power PC Processor

00:01:10.980 --> 00:01:12.120
Architect.

00:01:12.120 --> 00:01:13.350
Thank you.

00:01:20.890 --> 00:01:21.450
Thanks, Mark.

00:01:21.470 --> 00:01:22.120
Good morning.

00:01:22.120 --> 00:01:25.790
As Mark said,
I want to describe to you this morning

00:01:25.790 --> 00:01:29.200
the IBM PowerPC 970 microprocessor.

00:01:29.250 --> 00:01:35.100
Steve covered it pretty well yesterday,
but he left me a few details to fit in.

00:01:35.100 --> 00:01:36.850
So I'm going to do that.

00:01:36.920 --> 00:01:41.820
What I'd like to do is provide some
details that perhaps you'll find

00:01:41.820 --> 00:01:44.620
useful in your work with the G5.

00:01:45.220 --> 00:01:49.670
Other details I'm going to also put in
that perhaps you may not use directly,

00:01:49.810 --> 00:01:55.000
but will take advantage of indirectly as
you use and work with the G5 processor.

00:01:57.900 --> 00:02:04.320
So last fall I gave a high-level overview
of the 970 at the microprocessor forum,

00:02:04.330 --> 00:02:07.430
and I'm going to start with several
slides from that presentation

00:02:07.810 --> 00:02:08.800
to give the high-level overview.

00:02:08.800 --> 00:02:10.990
That's the first two bullets.

00:02:11.000 --> 00:02:17.110
Secondly, I'll go into details on the
several aspects mentioned here.

00:02:19.030 --> 00:02:22.060
So let me start with some
key aspects of the 970.

00:02:22.060 --> 00:02:28.820
First, this design was derived from the
high-performance Power 4 microprocessor,

00:02:28.820 --> 00:02:31.750
which is used in IBM's
high-end server systems.

00:02:31.820 --> 00:02:35.130
So the 970 is also a
high-performance design.

00:02:35.290 --> 00:02:38.200
It runs at 2 gigahertz.

00:02:38.200 --> 00:02:42.510
It executes instructions
multiple dispatch at a time,

00:02:42.510 --> 00:02:43.820
multiple issue.

00:02:43.820 --> 00:02:49.660
It also executes instructions out of
order to a degree that you haven't

00:02:49.750 --> 00:02:53.200
seen in previous Power PC processors.

00:02:53.300 --> 00:02:59.300
The 970 is a full implementation
of the 64-bit PowerPC architecture,

00:02:59.360 --> 00:03:03.750
but is compatible with, in fact,
runs natively 32-bit code.

00:03:05.310 --> 00:03:09.960
The G5 includes the vector enhancements
called the velocity engine,

00:03:10.100 --> 00:03:14.280
also includes a prefetch engine
to reduce memory latency.

00:03:14.540 --> 00:03:17.580
And finally,
the high-speed bus that Steve mentioned

00:03:17.580 --> 00:03:23.000
yesterday to off-chip memory and
I/O runs up to 1 GHz corresponding

00:03:23.000 --> 00:03:26.460
to 8 GB/s of peak bandwidth.

00:03:28.250 --> 00:03:32.260
So this is a block diagram of the
970 showing its major components.

00:03:32.260 --> 00:03:35.720
I'm going to kind of use this block
diagram as a map as we go along and

00:03:35.720 --> 00:03:37.680
discuss the different components.

00:03:37.680 --> 00:03:40.270
All the text surrounding
I won't go through here,

00:03:40.390 --> 00:03:41.990
but I'll cover as we go along.

00:03:44.620 --> 00:03:47.780
So let me start with the
instruction pipeline shown on the

00:03:47.780 --> 00:03:50.000
left side of the block diagram.

00:03:50.020 --> 00:03:57.300
The L1 cache is a 64 K byte L1 from
which eight instructions per cycle can

00:03:57.560 --> 00:04:03.260
be fetched by the instruction fetch
unit and up to five instructions fed

00:04:03.270 --> 00:04:09.310
into the instruction dispatch unit,
instruction decode unit

00:04:09.310 --> 00:04:11.820
and then on to dispatch.

00:04:12.120 --> 00:04:15.720
So as a group, up to five instructions
can be dispatched,

00:04:15.750 --> 00:04:20.430
up to ten instructions per cycle can
be issued to the execution units.

00:04:20.430 --> 00:04:24.600
And in all, over 200 instructions can be
in flight at any one time.

00:04:25.150 --> 00:04:28.950
The data pipe shown on the
right side of the diagram starts

00:04:28.950 --> 00:04:32.000
with the 32k byte L1D cache.

00:04:32.000 --> 00:04:37.030
The two load store units below that
L1D cache move data between the cache

00:04:37.030 --> 00:04:43.080
and the three register files shown there,
the FPR, GPR, and vector register file.

00:04:43.740 --> 00:04:50.720
The two L1 caches are backed,
shown at the top, by a half meg L2 cache,

00:04:50.760 --> 00:04:55.670
which in turn is backed by
main memory via the BIU.

00:04:58.050 --> 00:05:00.760
and continuing same diagram.

00:05:00.760 --> 00:05:04.230
In the middle of the diagram are
shown the memory management arrays

00:05:04.500 --> 00:05:05.950
that support virtual memory.

00:05:05.950 --> 00:05:10.850
This is a 64-bit implementation,
so effective addresses are 64 bits wide.

00:05:10.850 --> 00:05:17.770
Real addresses are 42 bits wide
for a 4 terabyte memory range.

00:05:18.380 --> 00:05:22.450
Finally, down at the bottom are the
computational execution units,

00:05:22.670 --> 00:05:25.410
the dual fixed point,
the dual floating point,

00:05:25.450 --> 00:05:27.300
and the dual vector units.

00:05:27.300 --> 00:05:30.410
Along with the two load store
units that I just mentioned and the

00:05:30.500 --> 00:05:34.200
branch and condition register unit,
which aren't shown in the diagram,

00:05:34.200 --> 00:05:37.530
those comprise the 10 execution
units of the processor.

00:05:40.850 --> 00:05:44.270
So what I want to do now
is repeat what I just said,

00:05:44.270 --> 00:05:47.220
but in a little more
detail in certain areas,

00:05:47.220 --> 00:05:49.220
and starting with instruction processing.

00:05:51.250 --> 00:05:57.140
So this is a pipeline diagram showing how
instructions move through the processor.

00:05:57.140 --> 00:06:02.620
Each block here represents a stage
where an instruction spends a cycle.

00:06:02.620 --> 00:06:07.250
Instructions move through the
pipeline starting at the top where

00:06:07.540 --> 00:06:11.540
they're fetched from the cache,
move down through decode, dispatch,

00:06:11.540 --> 00:06:14.760
then to issue and execution,
and finally at the bottom

00:06:14.760 --> 00:06:16.400
come out and complete.

00:06:18.140 --> 00:06:21.080
The lower part of the diagram
shows the individual pipelines of

00:06:21.140 --> 00:06:22.850
the individual execution units.

00:06:22.850 --> 00:06:25.690
The upper part just represents
the movement of the instructions

00:06:25.690 --> 00:06:27.010
through fetch and decode.

00:06:27.010 --> 00:06:29.720
So I'm going to start at the
top with instruction fetch.

00:06:31.100 --> 00:06:52.900
[Transcript missing]

00:06:53.260 --> 00:06:59.320
All of the caches in the 970 are
organized as 128-byte cache lines,

00:06:59.380 --> 00:07:02.120
but the instruction cache
lines are further subdivided

00:07:02.260 --> 00:07:03.980
into four 32-byte sectors.

00:07:03.980 --> 00:07:09.400
So it's a sector each cycle that gets
fetched from the instruction cache,

00:07:09.400 --> 00:07:14.210
and therefore for maximizing performance,
it's important to align your branch

00:07:14.250 --> 00:07:18.860
targets on these 32-byte boundaries
to maximize the fetch bandwidth.

00:07:20.830 --> 00:07:24.370
Once the instructions are fetched,
they're put into the 32-instruction

00:07:24.370 --> 00:07:28.640
fetch buffer shown at the bottom,
and then up to five instructions per

00:07:28.670 --> 00:07:33.630
cycle are removed from the fetch buffer
to send off to decode and dispatch.

00:07:33.640 --> 00:07:37.500
So the goal of this part of the
hardware is to keep that pipeline

00:07:37.630 --> 00:07:39.570
busy below the fetch buffer.

00:07:39.580 --> 00:07:43.560
And what could prevent that, for example,
is a miss in the ICache.

00:07:44.080 --> 00:07:47.680
When the IFAR address is
not found in the L1 ICache,

00:07:47.680 --> 00:07:51.200
a request goes to the L2 cache,
the data is brought back if

00:07:51.200 --> 00:07:54.300
it's found in the L2 cache,
and the fetch stream continues.

00:07:54.320 --> 00:07:58.720
But that stream is stopped for
12 cycles while that happens.

00:07:58.820 --> 00:08:03.350
So when that L1 cache misoccurs,
not only does the fetch hardware

00:08:03.440 --> 00:08:07.230
go after the missed cache line,
but it goes after the next

00:08:07.230 --> 00:08:09.670
sequential cache line as well.

00:08:09.680 --> 00:08:12.200
It brings it back into one of
the four prefetch buffers shown

00:08:12.200 --> 00:08:13.470
at the top of the diagram.

00:08:14.140 --> 00:08:16.460
So that the next time
an L1 cache misoccurs,

00:08:16.520 --> 00:08:19.650
if that address is found
in the prefetch buffer,

00:08:20.000 --> 00:08:24.440
there's only three cycles
missed of fetching.

00:08:24.730 --> 00:08:28.360
Similarly, on that table at the bottom
are shown these latencies.

00:08:28.360 --> 00:08:32.940
When a branch is taken,
predicted is taken,

00:08:32.940 --> 00:08:37.340
the branch prediction logic
updates that IFAR register with

00:08:37.420 --> 00:08:42.480
the new address and there's a two
cycle bubble in the fetch stream.

00:08:43.060 --> 00:08:47.020
Of course, the point of the fetch buffer
is that as you're feeding it,

00:08:47.020 --> 00:08:50.400
it's starting to fill up so that
when you get these two or three

00:08:50.710 --> 00:08:54.340
cycle bubbles in the fetch stream,
you're still able to maintain the

00:08:54.340 --> 00:08:58.270
stream of data down into decode,
the stream of instructions.

00:09:01.260 --> 00:09:05.340
Branch processing occurs
in two places in the 970.

00:09:05.340 --> 00:09:09.390
First, branches are predicted as they
are fetched from the cache,

00:09:09.390 --> 00:09:12.430
and second,
they are resolved when they get

00:09:12.470 --> 00:09:15.100
down to the branch execution unit.

00:09:16.470 --> 00:09:22.280
"Why do branch processing?"
Steve asked yesterday.

00:09:23.660 --> 00:09:28.110
Because particularly in a deeply
pipelined design like this,

00:09:28.110 --> 00:09:31.790
we're always fetching
well ahead of executing.

00:09:31.790 --> 00:09:36.030
So if you had to wait until you execute,
that is, until you know the conditions of

00:09:36.310 --> 00:09:39.510
whether a branch will be taken or not,
you will miss opportunities

00:09:39.520 --> 00:09:41.190
to keep the pipeline full.

00:09:41.200 --> 00:09:45.720
So what you want to do is predict
branches early and predict them

00:09:45.940 --> 00:09:48.580
accurately to avoid those delays.

00:09:48.580 --> 00:09:51.460
So as instructions are
fetched from the cache,

00:09:51.460 --> 00:09:54.720
they are scanned for branches,
and up to two branches

00:09:54.780 --> 00:09:56.500
per cycle are predicted.

00:09:56.500 --> 00:10:00.660
There are two branch mechanisms,
one to predict the direction a

00:10:00.800 --> 00:10:04.290
conditional branch will take,
and that mechanism uses

00:10:04.290 --> 00:10:09.420
three branch history tables,
which implement two different algorithms,

00:10:09.420 --> 00:10:11.180
a local and a global, for predicting the
direction of the branch.

00:10:11.200 --> 00:10:18.230
The second mechanism is for
predicting branches to registers.

00:10:18.230 --> 00:10:24.460
So there's a count cache that's used to
predict branch-to-count branch targets,

00:10:24.460 --> 00:10:28.300
and a link stack to predict
branch-to-link targets.

00:10:28.300 --> 00:10:32.310
Each of those data structures
holds previously seen branch target

00:10:32.360 --> 00:10:34.670
addresses for later predictions.

00:10:36.700 --> 00:10:40.550
So, predictions are made up as
instructions are fetched.

00:10:40.720 --> 00:10:44.070
The branch now works its way
through decode and dispatch.

00:10:44.120 --> 00:10:47.130
It finally gets to the execution
unit and now it resolves.

00:10:47.130 --> 00:10:50.030
That is, now it knows whether the
condition was true or false,

00:10:50.140 --> 00:10:52.400
whether the branch should
have been taken or not.

00:10:52.700 --> 00:10:56.310
If it predicted correctly, life goes on,
life is good.

00:10:56.350 --> 00:11:00.830
If it predicted incorrectly,
what the branch execution unit does is

00:11:00.830 --> 00:11:05.700
it updates the IFAR with the correct
branch target address and it flushes,

00:11:05.700 --> 00:11:08.970
of course,
all the instructions that were behind

00:11:08.970 --> 00:11:13.820
that branch because they now no
longer belong to the correct stream.

00:11:15.640 --> 00:11:19.830
The delay in that case to fill the pipe
and get it going again is 12 cycles.

00:11:19.870 --> 00:11:24.630
So it's that 12 cycle branch
penalty that one wants to avoid.

00:11:24.630 --> 00:11:28.540
This mechanism,
this prediction mechanism over a

00:11:28.540 --> 00:11:34.400
wide range of applications tends to
be accurate in the mid 90% range.

00:11:34.400 --> 00:11:39.010
So perhaps one out of 20 times a
branch will be mispredicted and so very

00:11:39.010 --> 00:11:41.460
few times will you pay the penalty.

00:11:42.060 --> 00:11:48.730
The last bullet here simply points out
that this dynamic branch prediction

00:11:48.730 --> 00:11:54.410
facility can be overridden by software
using an extended branch conditional

00:11:54.420 --> 00:12:01.310
instruction in the 970 which allows the
compiler or the programmer to statically

00:12:01.310 --> 00:12:07.590
predict that a branch should always
be predicted taken or always not taken.

00:12:08.950 --> 00:12:13.390
Instruction decode is a
multi-stage process here.

00:12:13.440 --> 00:12:17.050
I'm just going to mention one
aspect of instruction decode as it's

00:12:17.120 --> 00:12:20.200
different from most previous Power PCs.

00:12:20.200 --> 00:12:22.050
And it is as follows.

00:12:22.060 --> 00:12:24.830
The Power PC architecture
is a RISC-type architecture,

00:12:24.890 --> 00:12:29.000
and therefore each instruction in general
corresponds to one simple operation.

00:12:30.580 --> 00:12:32.810
However, there are exceptions to that.

00:12:32.950 --> 00:12:34.880
For instance,
a load with update instruction

00:12:34.880 --> 00:12:39.120
corresponds to two simple operations,
a load of one register and update

00:12:39.120 --> 00:12:40.980
of a separate index register.

00:12:40.980 --> 00:12:43.430
What the 970 does is it cracks,
as we say,

00:12:43.430 --> 00:12:46.930
that instruction into two internal ops,
and those internal ops then

00:12:46.930 --> 00:12:48.580
flow through the pipeline.

00:12:49.140 --> 00:12:52.180
And furthermore,
there are more complex instructions,

00:12:52.250 --> 00:12:54.960
like load multiple,
that correspond to a sequence

00:12:54.960 --> 00:12:56.520
of several operations.

00:12:56.520 --> 00:12:59.040
Those are translated into
a microcoded sequence,

00:12:59.130 --> 00:13:01.260
which then flows through the pipeline.

00:13:04.120 --> 00:13:06.460
And finally,
finally in terms of fetch and decode,

00:13:06.590 --> 00:13:07.640
we get to dispatch.

00:13:07.640 --> 00:13:14.200
This corresponds to the transition from
the fetch decode to the execution stages.

00:13:14.200 --> 00:13:17.650
It also corresponds to the transition
between in-order processing and

00:13:17.650 --> 00:13:20.030
out-of-order processing of instructions.

00:13:20.500 --> 00:13:23.790
So when instructions
reach the dispatch stage,

00:13:23.930 --> 00:13:27.960
they can be dispatched as a group
of up to five instructions if all of

00:13:27.960 --> 00:13:30.450
their hardware resources are available.

00:13:30.450 --> 00:13:34.420
Most instruction types can
dispatch out of any of the

00:13:34.420 --> 00:13:37.140
first four dispatch slots there.

00:13:37.140 --> 00:13:44.340
The fifth dispatch slot is
reserved for branch instructions.

00:13:46.080 --> 00:13:49.910
So once dispatched,
an instruction will take a place

00:13:49.960 --> 00:13:51.190
in one of these issue queues.

00:13:51.330 --> 00:13:53.670
All of the boxes there in the
issue queues show how many

00:13:53.670 --> 00:13:55.340
entries can be in the issue queue.

00:13:55.340 --> 00:13:59.120
Once in the issue queue,
an instruction can issue to be executed

00:13:59.120 --> 00:14:01.410
if all of its operands are available.

00:14:01.480 --> 00:14:06.580
And so if one instruction is waiting
on operands from a cache miss,

00:14:06.640 --> 00:14:11.520
for example, other instructions behind it
can continue to be processed.

00:14:11.520 --> 00:14:16.500
And it's this massive opportunity
for out-of-order execution of

00:14:16.500 --> 00:14:21.140
instructions that allows the G5 to
keep processing even in the presence

00:14:21.230 --> 00:14:26.180
of pipeline and memory delays,
which you normally run into in

00:14:26.180 --> 00:14:28.190
the normal course of processing.

00:14:28.200 --> 00:14:33.010
Finally, once instructions execute,
they wait until all of the instructions

00:14:33.010 --> 00:14:37.720
in their dispatch group are finished
and they complete together in order.

00:14:40.160 --> 00:14:46.150
Just briefly on virtual memory,
the memory management unit,

00:14:46.400 --> 00:14:50.780
one of its main features
is the support of address

00:14:50.780 --> 00:14:52.290
translation for virtual memory.

00:14:52.300 --> 00:14:56.020
Now virtual memory is something that
makes the programmer's job easier,

00:14:56.020 --> 00:15:00.200
its programming model is easier,
it makes OS implementations easier,

00:15:00.200 --> 00:15:02.710
but it actually involves
some complexities in the

00:15:02.910 --> 00:15:04.050
hardware to support it.

00:15:05.830 --> 00:15:11.420
So briefly, a segmented,
paged virtual memory system like

00:15:11.520 --> 00:15:14.780
this one requires a two-step
address translation process.

00:15:14.780 --> 00:15:18.110
First, an effective address,
what you program in,

00:15:18.260 --> 00:15:21.750
is mapped to a virtual
address using a segment table.

00:15:21.750 --> 00:15:25.510
And second, a virtual address is
mapped to a real address,

00:15:25.510 --> 00:15:29.080
what the hardware understands,
using a page table.

00:15:29.080 --> 00:15:33.530
And what's needed to support this
two-step process and then look up in

00:15:33.530 --> 00:15:38.460
the cache is some sort of hardware
optimization to make this efficient.

00:15:38.460 --> 00:15:41.690
So what's implemented
here is the usual TLB,

00:15:41.690 --> 00:15:46.030
table look-aside buffer,
which caches page table entries,

00:15:46.120 --> 00:15:51.440
but also a segment look-aside
buffer new to the 64-bit processor.

00:15:51.460 --> 00:15:55.210
This replaces the segment
registers of the 32-bit processors,

00:15:55.270 --> 00:15:57.740
which caches the segment table entries.

00:15:58.160 --> 00:16:01.960
And still, that two-stage translation
could be costly,

00:16:01.960 --> 00:16:07.590
except that we've implemented another
level of caching of address translation.

00:16:07.590 --> 00:16:08.630
It's called an ERAT.

00:16:08.690 --> 00:16:11.610
That's the effective-to-real
address translation table.

00:16:11.620 --> 00:16:14.880
It caches the most
recent effective-to-real,

00:16:14.880 --> 00:16:19.940
the two-stage process, effective-to-real
addresses in a small cache,

00:16:19.940 --> 00:16:21.490
small fast cache.

00:16:21.490 --> 00:16:26.310
So what the diagram shows then
is that the effective address in

00:16:26.360 --> 00:16:30.340
the IFAR accesses the L1 cache,
the L1 directory,

00:16:30.340 --> 00:16:32.240
and the ERAT all at the same time.

00:16:32.240 --> 00:16:35.250
And if all goes well,
like it usually does, and those all hit,

00:16:35.250 --> 00:16:37.690
you get the instructions
out on the next cycle.

00:16:38.460 --> 00:16:44.570
Similarly, there's a DERAT to go
with data cache accesses.

00:16:46.190 --> 00:16:49.690
For data processing,
just a couple points to make.

00:16:50.240 --> 00:16:52.690
One is on the registers.

00:16:53.530 --> 00:16:58.680
What the programmer sees is a set
of 32 general purpose registers,

00:16:58.680 --> 00:17:03.760
a set of 32 floating point registers,
and a set of 32 vector registers.

00:17:03.760 --> 00:17:06.830
Those are the architected registers.

00:17:06.840 --> 00:17:10.530
What's implemented in the
hardware to support those are

00:17:10.680 --> 00:17:12.890
more registers for two reasons.

00:17:12.910 --> 00:17:16.050
There's out-of-order execution and
there's multiple execution units.

00:17:16.520 --> 00:17:20.530
So to handle out-of-order execution,
we need a place to put the results

00:17:20.530 --> 00:17:24.630
that we've executed out-of-order
until they become the official result

00:17:24.730 --> 00:17:27.150
and go into the architected register.

00:17:27.210 --> 00:17:29.880
We call those rename registers,
and since there is so much

00:17:29.880 --> 00:17:32.820
capability for out-of-order,
there are more rename registers

00:17:32.820 --> 00:17:34.460
than architected registers.

00:17:34.460 --> 00:17:41.100
So the 970 has 32 GPRs architected plus
48 renames for a total of 80 registers,

00:17:41.100 --> 00:17:42.700
all 64 bits wide.

00:17:42.720 --> 00:17:46.460
The FPR similarly, 32 architected,
48 renegades.

00:17:46.520 --> 00:17:52.790
The vector registers similarly,
32 architected, 48 renames.

00:17:52.990 --> 00:17:57.470
In addition, we've got multiple execution
units and to keep up the supply

00:17:57.600 --> 00:18:02.600
of data operands to those units,
we've duplicated those register files.

00:18:02.610 --> 00:18:06.000
So there's two exact
copies of the 80 GPRs,

00:18:06.230 --> 00:18:08.350
two exact copies of
the FPRs and so forth.

00:18:08.480 --> 00:18:14.280
So the 32 architected registers
we've implemented as 160 registers

00:18:14.470 --> 00:18:16.890
for each of the register files.

00:18:17.080 --> 00:18:23.380
The latencies at the bottom
just show load to use delays.

00:18:23.640 --> 00:18:26.360
When you do a load of an operand
and then you want to use it,

00:18:26.440 --> 00:18:30.440
you can issue the load and then you
have to wait some number of cycles

00:18:30.440 --> 00:18:32.750
to issue the dependent operation.

00:18:32.750 --> 00:18:35.220
In the case of the fixed point unit,
for example, it's three.

00:18:35.320 --> 00:18:38.310
Floating point, it's five,
and the other values are shown there.

00:18:40.350 --> 00:18:44.010
The second thing I want to say about
the data side is that there is a data

00:18:44.010 --> 00:18:52.170
prefetch facility that in hardware
initiates data stream prefetching.

00:18:52.220 --> 00:18:56.030
So the idea is that this
prefetch hardware monitors the

00:18:56.030 --> 00:18:58.520
activity of the L1 data cache.

00:18:59.160 --> 00:19:03.450
When it sees two misses to
two adjacent cache lines,

00:19:03.450 --> 00:19:06.580
it says, oh, there's a pattern.

00:19:06.580 --> 00:19:11.320
I'll go after, I'll prefetch the third
cache line in the sequence.

00:19:11.380 --> 00:19:14.520
If it then sees a hit to
that third cache line,

00:19:14.520 --> 00:19:18.260
it'll go after the fourth line and
prefetch it into the L1 and so forth.

00:19:18.300 --> 00:19:21.870
So it's demand-paced,
which means it'll keep fetching ahead for

00:19:21.930 --> 00:19:24.340
as long as the data stream is accessed.

00:19:24.340 --> 00:19:28.180
Cache lines are brought into the L1.

00:19:28.380 --> 00:19:33.090
and further ahead they're brought
into the L2 using this mechanism.

00:19:33.320 --> 00:19:36.290
So in addition to this
hardware-initiated prefetch,

00:19:36.290 --> 00:19:40.320
software can also initiate a data
stream prefetch using an extended

00:19:40.400 --> 00:19:42.770
version of the DCB touch instruction.

00:19:42.780 --> 00:19:46.680
The 970 supports this
extension of the DCB touch,

00:19:46.680 --> 00:19:50.560
which allows it to touch not just
one cache line and bring it in,

00:19:50.790 --> 00:19:54.350
but to start this prefetch
mechanism to keep fetching ahead.

00:19:55.060 --> 00:20:00.730
And a third mechanism for prefetch
is the implementation of the data

00:20:00.730 --> 00:20:06.290
stream touch instruction associated
with the vector extensions.

00:20:09.660 --> 00:20:12.700
The computation units at the
bottom of the block diagram,

00:20:12.700 --> 00:20:15.340
I just want to cover
what gets executed where.

00:20:15.340 --> 00:20:19.060
There are two fixed point units
that are nearly symmetrical.

00:20:19.140 --> 00:20:22.580
They both execute the usual
arithmetic and logical and shift

00:20:22.580 --> 00:20:24.540
and rotate type instructions.

00:20:24.540 --> 00:20:29.020
They both also execute multiplies,
so you can have two multiplies

00:20:29.020 --> 00:20:30.740
going at the same time.

00:20:30.740 --> 00:20:34.460
The difference is that the one
unit executes the fixed point

00:20:34.460 --> 00:20:38.820
divides while the other unit
executes the SPR move instructions.

00:20:39.600 --> 00:20:42.440
The two floating point
units are symmetric.

00:20:42.440 --> 00:20:46.380
They both execute IEEE single
and double precision operations.

00:20:46.600 --> 00:20:49.410
They both support the
IEEE formats for denorms,

00:20:49.570 --> 00:20:52.360
not a numbers, infinities, and so forth.

00:20:52.430 --> 00:20:55.280
They both support precise exceptions.

00:20:55.280 --> 00:20:58.370
They also both support the
optional floating point

00:20:58.660 --> 00:21:02.890
instructions for square root,
select, reciprocal estimate,

00:21:02.920 --> 00:21:05.800
and reciprocal square root estimate.

00:21:05.800 --> 00:21:09.050
They do not support a non-IEEE mode.

00:21:10.600 --> 00:21:34.900
[Transcript missing]

00:21:36.370 --> 00:21:40.920
And finally at the top,
the L2 and bus interface,

00:21:41.060 --> 00:21:44.600
which will segue us
into the next segment.

00:21:44.600 --> 00:21:50.490
The memory subsystem has a
few subcomponents itself.

00:21:51.990 --> 00:21:54.920
The cache interface unit shown
at the top takes four types

00:21:55.300 --> 00:21:57.500
of requests from the core.

00:21:57.500 --> 00:22:00.660
One from the fetch
unit for iCache misses,

00:22:00.660 --> 00:22:04.120
one from each of the load store
units for de-cache misses,

00:22:04.120 --> 00:22:09.380
and a fourth one for the TLB hardware
table walker and the prefetch hardware.

00:22:09.380 --> 00:22:14.360
What the CIU does is simply direct
those requests to the right place.

00:22:14.360 --> 00:22:17.800
For instance,
an L1 iCache miss will be directed to

00:22:17.810 --> 00:22:20.560
the L2 cache where it will be looked up.

00:22:20.640 --> 00:22:22.300
If the data is found,
it will be returned.

00:22:22.300 --> 00:22:26.000
If the data is not found,
the L2 cache controller will forward

00:22:26.280 --> 00:22:28.390
it on to the BIU and on to memory.

00:22:28.400 --> 00:22:35.840
The non-cacheable unit on the left side
simply handles all of the other activity

00:22:36.070 --> 00:22:39.120
not associated with the L2 cache.

00:22:39.120 --> 00:22:39.360
The G5 processor is a very powerful
and powerful tool for the iCache.

00:22:39.410 --> 00:22:41.690
goes off to the bus.

00:22:44.620 --> 00:22:48.360
So this high bandwidth processor bus
is what we call the elastic interface.

00:22:48.360 --> 00:22:52.360
It consists of two buses,
two unidirectional buses,

00:22:52.360 --> 00:22:55.400
each four bytes wide, point to point.

00:22:55.400 --> 00:22:56.560
It's not a shared bus.

00:22:56.560 --> 00:22:57.760
Source synchronous.

00:22:57.800 --> 00:22:59.650
The clocks are sent with the data.

00:22:59.660 --> 00:23:03.750
And I put in this point about
initialization alignment.

00:23:03.750 --> 00:23:08.550
At power on reset,
there's a procedure that the processor

00:23:08.550 --> 00:23:13.590
and system controller go through
to de-skew all of the bits on a

00:23:13.590 --> 00:23:19.350
bus and then to center the clock
within the eye of those data bits.

00:23:19.360 --> 00:23:22.910
And my reason for pointing this
out is to say that there's a lot of

00:23:22.970 --> 00:23:26.830
work involved on both the processor
and the system controller side to

00:23:26.830 --> 00:23:28.880
get a bus to run at one gigahertz.

00:23:30.810 --> 00:23:34.620
The logical interface here supports a
pipelined out-of-order transactions.

00:23:34.650 --> 00:23:38.840
The address and control information
shares the same bus as the data.

00:23:38.840 --> 00:23:42.900
There are three types of command packets,
read, write, and control.

00:23:42.900 --> 00:23:48.030
Each of those consist of two
4-byte beats on the bus that

00:23:48.030 --> 00:23:54.050
contain the 42-bit real address,
transaction type, size,

00:23:54.050 --> 00:23:57.970
other control information, and a tag.

00:23:58.940 --> 00:24:04.900
Data packets come in sizes from
2 4-byte beats to 32-beats.

00:24:04.900 --> 00:24:11.440
To send one byte on the bus requires
a 2-beat packet from 1 to 8 bytes.

00:24:11.630 --> 00:24:15.770
The 32-beat packet is
the cache line size,

00:24:15.770 --> 00:24:17.840
128 bytes.

00:24:19.450 --> 00:24:23.370
On the right, the diagram shows a little
bit more detail about what

00:24:23.370 --> 00:24:26.040
I called a four-byte wide bus.

00:24:26.040 --> 00:24:28.640
The bus actually consists
of three segments.

00:24:28.660 --> 00:24:32.780
One, the address data segment,
which is actually 35 bits,

00:24:32.780 --> 00:24:35.600
the 32 of data plus some control bits.

00:24:35.600 --> 00:24:39.750
Second, there's a transfer handshake,
single signal,

00:24:39.930 --> 00:24:42.610
and two signals for snoop responses.

00:24:42.610 --> 00:24:46.440
And so the outgoing,
with respect to the processor,

00:24:46.440 --> 00:24:49.080
and ingoing buses are shown here.

00:24:50.730 --> 00:24:54.230
Here are those three
segments per direction.

00:24:54.320 --> 00:24:57.600
Again, just to show an example
of a read transaction.

00:24:57.600 --> 00:25:01.860
The transaction is initiated by the
processor by putting a read command

00:25:01.860 --> 00:25:07.600
packet up in the upper left corner
out on the address data out bus.

00:25:07.600 --> 00:25:10.600
And I'll give the end of the story first.

00:25:10.600 --> 00:25:17.340
Out on the other side to the
right is the data coming back

00:25:17.340 --> 00:25:19.780
from the memory controller.

00:25:19.950 --> 00:25:22.560
What's happening in between,
without giving a lot of detail,

00:25:22.560 --> 00:25:27.620
is that there's handshaking going on
to acknowledge transfer of information

00:25:27.620 --> 00:25:30.500
and also to support memory coherency.

00:25:30.560 --> 00:25:34.790
So again, this is a point-to-point bus,
so one processor can't see directly

00:25:34.860 --> 00:25:37.070
what the other processor is doing.

00:25:37.080 --> 00:25:41.090
In order to maintain memory coherency,
the system controller has to get

00:25:41.090 --> 00:25:44.350
involved and reflect commands
back to all the processors so

00:25:44.400 --> 00:25:46.480
they can snoop and stay coherent.

00:25:46.480 --> 00:25:49.250
And that's what you see,
some of that handshaking.

00:25:49.860 --> 00:25:54.550
This looks like not very
good utilization of the bus.

00:25:54.550 --> 00:25:57.290
That's because I just
isolated the read transaction.

00:25:57.320 --> 00:25:59.850
Normally,
all of this activity would be interleaved

00:25:59.850 --> 00:26:01.810
with all the other activity on the bus.

00:26:04.760 --> 00:26:08.440
And the other point,
the numbering shows that there are,

00:26:08.770 --> 00:26:13.610
the way the bus is managed is that
there are fixed delays between

00:26:13.890 --> 00:26:15.740
activity and responses to activity.

00:26:15.740 --> 00:26:19.550
And this is the way we correspond
the handshaking with the original

00:26:19.820 --> 00:26:24.050
transaction because things are
happening out of order and the snoop

00:26:24.080 --> 00:26:27.720
responses and the handshakes are
not tagged or validated in any way.

00:26:30.530 --> 00:26:35.170
Okay, so let me just go over one
more time what I've said.

00:26:35.170 --> 00:26:39.300
This G5 processor is a
high performance processor.

00:26:39.300 --> 00:26:43.650
It achieves its high performance
by running at 2 gigahertz,

00:26:43.650 --> 00:26:48.420
also by its superscalar completion
of five instructions per cycle,

00:26:48.420 --> 00:26:52.140
by its out-of-order
execution of instructions.

00:26:53.520 --> 00:26:58.360
It's an implementation that
supports both 64-bit and 32-bit

00:26:58.440 --> 00:27:01.280
applications and operating systems.

00:27:01.280 --> 00:27:06.300
I've mentioned kind of the width of
the pipeline that we can fetch eight,

00:27:06.300 --> 00:27:10.300
dispatch five,
issue ten instructions every cycle.

00:27:10.300 --> 00:27:15.190
Also that the branch prediction scheme
is highly accurate across a range

00:27:15.190 --> 00:27:20.200
of applications so that we avoid
that branch penalty that I mentioned.

00:27:20.200 --> 00:27:23.000
We get high computational throughput.

00:27:23.630 --> 00:27:27.640
By using two fixed point,
two floating point, and two vector units,

00:27:27.640 --> 00:27:31.510
as well as two load store units
to keep everything busy with data.

00:27:31.520 --> 00:27:35.790
And also this data prefetch engine,
which keeps the latency to memory,

00:27:35.790 --> 00:27:39.530
the effective latency to memory,
low by keeping things as close

00:27:39.580 --> 00:27:41.780
in to the processor as possible.

00:27:41.780 --> 00:27:46.410
And finally, the high-speed bus,
which I just mentioned,

00:27:46.410 --> 00:27:53.500
on the 2 gigahertz processor will run at
1 gigahertz for an 8 gigabyte percentage.

00:27:53.520 --> 00:27:58.520
bandwidth to off-chip memory and I/O.

00:27:59.890 --> 00:28:01.860
So that's all I have to say.

00:28:01.860 --> 00:28:05.860
I'd like to thank Mark and
Jesse Stein from IBM for helping

00:28:05.860 --> 00:28:08.950
me prepare this presentation,
and I'd like to thank you for your

00:28:08.950 --> 00:28:10.960
attention and your interest in the G5.

00:28:19.920 --> 00:28:21.900
Thank you, Peter.

00:28:21.900 --> 00:28:23.900
And you thought he was going
to only answer the branch

00:28:23.930 --> 00:28:25.360
processing questions Steve had,
huh?

00:28:25.400 --> 00:28:29.940
So to point you to some more information,
if you wanted to get some more documents

00:28:29.990 --> 00:28:34.530
specifically from the IBM PowerPC page,
a couple of URLs here available for you.

00:28:34.530 --> 00:28:36.240
There are several documents posted there.

00:28:36.240 --> 00:28:39.550
Later on in the presentation,
I'll give you some more pointers to

00:28:39.550 --> 00:28:41.510
other references on the Apple site.

00:28:49.630 --> 00:28:55.100
So to continue our journey from
where IBM handed off the PowerPC 970,

00:28:55.100 --> 00:28:59.640
the G5 processor,
to Apple and what we did then

00:28:59.640 --> 00:29:01.970
with the system architecture,

00:29:02.180 --> 00:29:08.690
I'd like to introduce to you Keith Cox,
principal engineer, systems architecture.

00:29:17.620 --> 00:29:19.040
Thank you, Mark.

00:29:19.140 --> 00:29:22.520
So Peter told you a little bit
about the G5 processor itself.

00:29:22.520 --> 00:29:26.830
I'm here to tell you more about the
system we wrapped around it and how we,

00:29:27.020 --> 00:29:30.710
our vision of bringing that
performance out and turning it

00:29:30.710 --> 00:29:35.070
into real world performance for
your users and your applications.

00:29:35.960 --> 00:29:42.830
So this is the general block
diagram of the Power Mac G5.

00:29:43.070 --> 00:29:46.800
The thing I want you to get from this is
that we started over with this system.

00:29:46.800 --> 00:29:50.970
We did not take the Power Mac G4
architecture and say,

00:29:50.980 --> 00:29:52.410
okay, how do we tweak it?

00:29:52.420 --> 00:29:53.620
We got to get a little faster.

00:29:53.620 --> 00:29:57.410
What we said was, we're getting a really
cool processor from IBM.

00:29:57.420 --> 00:29:59.960
It's going to really
chew up instructions.

00:29:59.970 --> 00:30:01.830
It's going to really need data.

00:30:01.830 --> 00:30:04.320
We really need to keep this sucker fed.

00:30:04.320 --> 00:30:06.640
So we started from the ground up.

00:30:06.640 --> 00:30:08.570
We opened up all the pipes.

00:30:08.570 --> 00:30:12.370
So what I want you to get from my
presentation is that not only is this the

00:30:12.370 --> 00:30:15.290
next generation Power PC architecture,
but in addition to that,

00:30:15.290 --> 00:30:17.740
we've added high bandwidth
buses everywhere.

00:30:17.740 --> 00:30:20.840
We've improved the memory system greatly.

00:30:20.970 --> 00:30:23.680
We've increased the
PCI buses in the IO system.

00:30:23.680 --> 00:30:26.510
And on top of that,
we've added an advanced thermal

00:30:26.510 --> 00:30:30.790
management system because we know the
users like their systems to be quiet.

00:30:30.790 --> 00:30:35.180
They don't like them to be loud and
roaring like jet airplanes or anything.

00:30:36.990 --> 00:30:40.790
So, this is the general block
diagram of the Power Mac G5.

00:30:40.790 --> 00:30:45.620
It's actually very similar just
in blocks to a G4 block diagram,

00:30:45.620 --> 00:30:47.820
but there are some important
differences to note.

00:30:47.870 --> 00:30:52.330
The first is that the processor bus is
not shared in a multiprocessor system.

00:30:52.360 --> 00:30:55.930
That's a key difference when you get to
MP and the kind of performance that we

00:30:55.930 --> 00:30:59.720
have and the kind of bandwidth that we
need to be able to deliver to the user.

00:31:00.580 --> 00:31:05.160
Another important difference is that the
system controller connection to the I.O.

00:31:05.190 --> 00:31:06.860
system is no longer a PCI bus.

00:31:06.900 --> 00:31:10.170
It's actually a hyper transport
bus that has up to 3.2

00:31:10.190 --> 00:31:14.070
gigabytes a second of bandwidth,
connects to high bandwidth devices

00:31:14.160 --> 00:31:16.080
down below the system controller.

00:31:16.150 --> 00:31:17.350
That's all new.

00:31:18.790 --> 00:31:21.480
So if we compare the G4
and the G5 processors,

00:31:21.480 --> 00:31:26.200
you've just heard from Peter about how
the G5 can keep a million things in

00:31:26.200 --> 00:31:28.240
flight or at least 200 and some odd.

00:31:28.390 --> 00:31:33.880
It runs at 2 gigahertz and can
complete five instructions at a time.

00:31:33.880 --> 00:31:37.920
It just has a huge appetite and
it's a big leap over the G4.

00:31:37.920 --> 00:31:42.080
The system, similarly,
we believe is a big leap over the G4.

00:31:42.080 --> 00:31:46.700
The front side bus has six times
the bandwidth of a G4 system.

00:31:46.700 --> 00:31:49.750
If you've got a multiprocessor system,
it actually has 12 times the

00:31:49.860 --> 00:31:51.080
bandwidth of a G4 system.

00:31:51.080 --> 00:31:55.790
The memory system is more than two
times faster and the PCI system

00:31:55.860 --> 00:31:57.980
is seven times the bandwidth.

00:31:57.980 --> 00:32:01.220
So we've really tried to open
up the inside of the system.

00:32:01.220 --> 00:32:03.650
Let's dig down in a little
more detail on all of that.

00:32:03.720 --> 00:32:07.890
The front side bus is
8 gigabytes per second.

00:32:07.900 --> 00:32:11.410
We quote it as double data rate 64-bit.

00:32:11.420 --> 00:32:15.820
As Peter was just showing you,
that's not quite correct.

00:32:15.910 --> 00:32:18.400
It's actually a pretty
complicated bus to describe.

00:32:18.400 --> 00:32:23.680
That's what we put in the
marketing fluff to describe it.

00:32:26.760 --> 00:32:30.520
I mean, we really want our users to
understand the basic gist of it,

00:32:30.540 --> 00:32:33.780
which is it's effectively
64 bits wide of data,

00:32:33.780 --> 00:32:36.020
and it's 8 gigabytes
a second of bandwidth.

00:32:36.020 --> 00:32:39.140
In reality, that's two 4 gigabyte
per second channels,

00:32:39.160 --> 00:32:42.440
4 gigabytes a second going up,
4 gigabytes a second coming

00:32:42.500 --> 00:32:44.060
down on each processor.

00:32:46.520 --> 00:32:49.510
There's a little bit of overhead for the
packet headers and that sort of stuff.

00:32:49.610 --> 00:32:52.590
So the real achievable bandwidth
number is a little smaller than that.

00:32:52.760 --> 00:32:58.520
But it is close to the 8 gigabytes
per second total on that interface.

00:32:58.520 --> 00:33:02.840
Then if you had two processors,
we've got two interfaces.

00:33:02.840 --> 00:33:06.180
So that's a total of
16 gigabytes a second,

00:33:06.180 --> 00:33:09.480
four up, four down,
times two processors to

00:33:09.480 --> 00:33:11.560
get the full bandwidth.

00:33:12.300 --> 00:33:14.960
In order to deal with that,
you really need a really high

00:33:14.960 --> 00:33:16.610
bandwidth system controller.

00:33:16.620 --> 00:33:20.000
This was a ground-up redesign
at Apple that really intended

00:33:20.130 --> 00:33:23.250
to achieve these real levels
of performance and be able to

00:33:23.250 --> 00:33:25.620
deliver these kinds of bandwidths.

00:33:25.620 --> 00:33:30.060
In addition to just moving 16
gigabytes a second of data,

00:33:30.120 --> 00:33:35.460
there's all the coherency protocol
that Peter was just describing where

00:33:35.460 --> 00:33:39.580
one processor requests something,
you've got to check the other processors.

00:33:39.580 --> 00:33:41.240
It may have it modified in the cache.

00:33:41.820 --> 00:33:45.000
So Apple's always delivered
cache-coherent systems.

00:33:45.000 --> 00:33:45.820
We do that here.

00:33:45.820 --> 00:33:49.860
The G5 implements something
called cache intervention as well,

00:33:49.970 --> 00:33:53.550
which it says that if processor
one wants a line in the cache,

00:33:53.550 --> 00:33:57.180
processor two has it modified,
the system controller actually

00:33:57.180 --> 00:34:00.460
delivers the data coming out of
processor two straight across and back

00:34:00.590 --> 00:34:04.060
up to processor one without having
to go through the memory system.

00:34:04.060 --> 00:34:06.480
What this does is it does two things.

00:34:06.480 --> 00:34:09.840
One, it doesn't chew up your valuable
memory bandwidth if you don't need to.

00:34:09.860 --> 00:34:11.390
The other thing is it.

00:34:11.670 --> 00:34:15.330
It takes full advantage of the
high bandwidth of the processor

00:34:15.450 --> 00:34:19.680
interfaces to deliver things fast to
the other processor while not really

00:34:19.680 --> 00:34:21.460
interfering with the other processor.

00:34:21.510 --> 00:34:23.330
Yes,
it takes a few beats of the bandwidth

00:34:23.330 --> 00:34:26.500
for processor two to deliver the data,
but it had to do that anyway.

00:34:26.500 --> 00:34:27.180
It had it modified.

00:34:27.180 --> 00:34:28.460
It owned that data.

00:34:28.460 --> 00:34:32.620
And so it cost it nothing else,
and yet we got the lower latency

00:34:32.620 --> 00:34:35.230
and higher throughput by doing that.

00:34:37.170 --> 00:34:38.480
In addition,
one of the points you're going to

00:34:38.570 --> 00:34:43.100
hear throughout my talk is that
all these links are point-to-point.

00:34:43.100 --> 00:34:47.600
We're connecting endpoints directly to
get the highest efficiency possible,

00:34:47.600 --> 00:34:50.680
the lowest latency possible,
and really just make the data

00:34:51.110 --> 00:34:54.280
scream through the system without
bottlenecking at any single point.

00:34:54.680 --> 00:34:57.890
So you just heard how the G5
processors can talk directly to

00:34:58.020 --> 00:35:01.690
each other without interacting
with any of the rest of the system.

00:35:01.690 --> 00:35:06.020
In reality, the AGP bus has its own
direct port into memory.

00:35:06.020 --> 00:35:09.460
The IO system through hypertransport
has a port into memory.

00:35:09.460 --> 00:35:13.280
Each processor has their own individual
read and write cues into memory.

00:35:13.280 --> 00:35:16.340
If you look inside the system controller,
if you could open it up,

00:35:16.340 --> 00:35:20.480
there are actually direct point-to-point
links between all the interfaces as well.

00:35:20.480 --> 00:35:24.650
So we've really tried to avoid any of the
bottlenecks of some system controllers.

00:35:24.720 --> 00:35:29.320
controller designs where
things really get choked up.

00:35:30.390 --> 00:35:32.640
If we move on to the memory system,
the first thing we did

00:35:32.650 --> 00:35:33.850
was we doubled the width.

00:35:33.940 --> 00:35:35.220
I mean, that's the obvious thing.

00:35:35.220 --> 00:35:38.460
You need more bandwidth, you go wider,
you get more bandwidth.

00:35:38.460 --> 00:35:42.560
In addition,
we pushed it up to 400 megatransfers

00:35:42.660 --> 00:35:48.040
per second or PC3200 DRAM or
whatever label you want to apply.

00:35:49.800 --> 00:35:53.000
This gives us a total bandwidth
of 6.4 gigabytes a second.

00:35:53.000 --> 00:35:54.980
That's pretty much state-of-the-art.

00:35:54.980 --> 00:35:58.130
That's the best you can do with
current memory technology without

00:35:58.130 --> 00:36:00.960
going really extremely wide,
which starts to impact your

00:36:00.960 --> 00:36:02.810
cost in a very negative manner.

00:36:02.820 --> 00:36:06.280
Going 128 bits wide,
you do have to put two DIMMs wide

00:36:06.710 --> 00:36:10.940
because each DIMM is 64 bits,
so too wide to get 128 bits.

00:36:10.940 --> 00:36:13.200
So you have to install them in pairs.

00:36:13.200 --> 00:36:17.080
But one thing you'll see in the
Power Mac G5 system that you don't

00:36:17.080 --> 00:36:19.010
see anywhere else is the depth.

00:36:19.800 --> 00:36:22.940
Our memory system is two
DIMMs wide by four DIMMs deep

00:36:22.970 --> 00:36:25.780
at 400 megatransfers per second.

00:36:25.780 --> 00:36:28.880
That, as far as I am aware,
is not done anywhere

00:36:28.910 --> 00:36:30.620
else in the industry.

00:36:30.620 --> 00:36:34.800
It's actually a great challenge to
get 400 megatransfers per second on

00:36:34.800 --> 00:36:38.960
four DIMMs that are all connected
together to the same memory interface.

00:36:38.960 --> 00:36:42.900
And that's one of the values that are one
of the places where Apple put a lot of

00:36:42.900 --> 00:36:46.150
engineering to get both the memory speed,
the memory width,

00:36:46.150 --> 00:36:49.760
and the memory depth so that we
can have the large memory system.

00:36:49.800 --> 00:36:52.940
And the customers can get the
eight gigabytes of memory and

00:36:52.980 --> 00:36:54.670
eight DIMMs that we support.

00:36:57.000 --> 00:37:02.130
If we move on to the AGP system,
it's pretty much a standard AGP 8X,

00:37:02.140 --> 00:37:07.520
AGP 3.0, all buzzword compliant or
spec compliant interface.

00:37:07.520 --> 00:37:11.550
AGP Pro is new for us
and it's a great idea.

00:37:12.500 --> 00:37:15.290
Our case,
we support up to 70-watt AGP cards.

00:37:15.290 --> 00:37:21.220
The AGP Pro spec has different levels,
and at those different levels,

00:37:21.220 --> 00:37:24.660
you can start growing your heat sink
into the slot space of the PCI cards.

00:37:24.740 --> 00:37:28.640
So technically, at a 70-watt card,
the card vendor is allowed to

00:37:28.650 --> 00:37:32.180
take up two of your PCI slots
with just heat sink to cool that.

00:37:32.260 --> 00:37:35.820
So that's something to be aware of.

00:37:38.700 --> 00:37:40.700
I don't know that there's
much more to say about that.

00:37:40.700 --> 00:37:44.380
If we move on to the I/O system,
coming out of the system

00:37:44.380 --> 00:37:48.910
controller is the last major bus,
which is the hyper transport bus

00:37:48.950 --> 00:37:51.180
coming down to the PCIX bridge.

00:37:51.560 --> 00:37:55.140
That bus, hyper transport--

00:37:55.400 --> 00:37:56.980
describes it as a 16-bit bus.

00:37:56.980 --> 00:37:59.480
It's really two 16-bit
point-to-point interfaces,

00:37:59.480 --> 00:38:01.780
one each direction,
similar to the processor bus.

00:38:01.780 --> 00:38:06.970
So you've got 16 bits up, 16 bits down,
running at 800 megatransfers per

00:38:07.030 --> 00:38:09.690
second in our implementation.

00:38:10.210 --> 00:38:13.060
Connected to that,
you've got a PCIX bridge with two

00:38:13.280 --> 00:38:15.660
completely independent PCIX buses.

00:38:15.670 --> 00:38:18.410
So the PCIX spec says
that if you have one slot,

00:38:18.410 --> 00:38:20.790
you can run it at 133 megahertz.

00:38:20.790 --> 00:38:24.740
If you have two slots,
you can only run it at 100 megahertz.

00:38:24.820 --> 00:38:26.180
So that's what we did.

00:38:26.210 --> 00:38:27.610
We needed three slots.

00:38:27.670 --> 00:38:28.770
We had two buses.

00:38:28.770 --> 00:38:30.420
This is the bandwidth we get.

00:38:30.730 --> 00:38:34.070
It's seven times the 64-bit
PCI bandwidth of what we've

00:38:34.110 --> 00:38:36.100
had in our previous systems.

00:38:37.260 --> 00:38:40.110
So one thing you might be aware
of is on the two-slot bus,

00:38:40.110 --> 00:38:43.540
if you plug in two cards and one of
them's slow and one of them's fast,

00:38:43.540 --> 00:38:46.370
the bus has to run at the speed
of the slowest card so it can

00:38:46.390 --> 00:38:49.750
handle the transactions and
understand what's going on.

00:38:49.780 --> 00:38:54.170
So as a configuration issue,
maybe if you're designing cards and

00:38:54.250 --> 00:38:58.330
documenting how to install them,
you should be aware that if

00:38:58.330 --> 00:39:02.010
you've got two cards that
are fast and one that's slow,

00:39:02.010 --> 00:39:06.420
you might actually want to put
the slow card in the single slot.

00:39:06.530 --> 00:39:06.660
Okay.

00:39:07.260 --> 00:39:10.700
as opposed to slowing down the other two.

00:39:11.750 --> 00:39:16.820
Another thing to do with PCIX,
the PCIX spec drops support

00:39:16.820 --> 00:39:20.200
for 5-volt PCI cards.

00:39:20.200 --> 00:39:24.800
That's really just a requirement
to get the interface to run

00:39:24.890 --> 00:39:26.570
at the speeds that it runs at.

00:39:26.620 --> 00:39:30.480
So what happens is
there are 5-volt cards.

00:39:30.550 --> 00:39:33.430
They're mostly very old cards.

00:39:33.480 --> 00:39:37.200
There's not new 5-volt cards being
designed that I'm aware of or haven't

00:39:37.220 --> 00:39:39.130
been aware of for a couple of years.

00:39:39.770 --> 00:39:42.100
Most cards nowadays are
3.3-volt universal cards,

00:39:42.160 --> 00:39:43.130
as they're called.

00:39:44.850 --> 00:39:48.800
Those cards can exist on a 5-volt bus
but only signal at 3.3-volt levels.

00:39:48.800 --> 00:39:52.200
And then, of course,
standard 3.3-volt PCI cards

00:39:52.200 --> 00:39:54.400
also signal 3.3-volt levels.

00:39:54.420 --> 00:39:58.100
Those two flavors,
the 3.3-volt and the universal cards,

00:39:58.100 --> 00:40:00.160
are fully compatible with PCI-X.

00:40:00.160 --> 00:40:04.550
The bus controller figures out
that I've got a PCI card instead

00:40:04.550 --> 00:40:09.360
of a PCI-X card and it's capable
or it only runs at 33 megahertz,

00:40:09.370 --> 00:40:14.340
say, and it slows down the clocks on
the bus to support that card.

00:40:15.160 --> 00:40:18.030
Likewise, there are PCI-X cards that
only run at 100 megahertz,

00:40:18.030 --> 00:40:20.120
so even if you plug
them into the 133 slot,

00:40:20.170 --> 00:40:22.890
they won't run 133 because
they've reported the speed

00:40:22.890 --> 00:40:24.360
that they're capable of.

00:40:26.910 --> 00:40:29.000
If we move on to the I.O.

00:40:29.000 --> 00:40:33.070
system,
it also hangs off hyper transport coming

00:40:33.110 --> 00:40:37.420
out the far side of the PCIX bridge
is another hyper transport interface.

00:40:37.420 --> 00:40:39.290
This one's only 8 bits wide.

00:40:39.290 --> 00:40:43.330
That's really,
it's not 16 bits because it doesn't

00:40:43.350 --> 00:40:45.760
need to be is the basic answer.

00:40:45.760 --> 00:40:51.360
The 8-bit hyper transport has 1.6
gigabytes a second of bandwidth for I.O.

00:40:51.360 --> 00:40:52.630
Historically, the I.O.

00:40:52.630 --> 00:40:54.990
controllers had about
100 megabytes a second,

00:40:54.990 --> 00:40:56.320
so it's only 16 times.

00:40:56.990 --> 00:40:59.200
So it was sufficient.

00:40:59.200 --> 00:41:02.710
We did move the gigabit Ethernet
interface and the FireWire

00:41:02.710 --> 00:41:04.680
interface down into the I.O.

00:41:04.680 --> 00:41:07.520
controller,
which works just fine because it now

00:41:07.700 --> 00:41:10.030
has plenty of bandwidth to do that.

00:41:10.060 --> 00:41:12.760
If any of you remember
the G4 block diagram,

00:41:12.850 --> 00:41:16.850
those two functions were in the
north bridge or the system controller

00:41:16.850 --> 00:41:20.840
in the G4 system simply because
they couldn't get enough bandwidth

00:41:20.900 --> 00:41:22.870
off the PCI bus to exist there.

00:41:22.880 --> 00:41:26.780
In addition, we've gone to serial ATA,
which is a higher.

00:41:26.800 --> 00:41:30.790
It's actually roughly
equivalent to Ultra ATA 100,

00:41:30.790 --> 00:41:35.400
but the thing is,
now you've got two of them,

00:41:35.500 --> 00:41:43.700
and the disks are completely independent
as opposed to an Ultra ATA master-slave

00:41:43.780 --> 00:41:50.990
where the drives really interact
horribly as far as if you're accessing

00:41:51.220 --> 00:41:54.790
stuff off one versus the other.

00:41:54.790 --> 00:41:54.790
You have to wait for one
before you get to the other.

00:41:54.790 --> 00:41:54.790
Here, the drive interfaces are
completely independent,

00:41:54.790 --> 00:41:54.790
so the drives can be run
simultaneously at full bandwidth

00:41:54.790 --> 00:41:54.790
without beating on each other.

00:41:55.450 --> 00:41:57.940
A note about the USB 2 controller.

00:41:57.940 --> 00:42:03.910
I've seen lots of comments and
confusion out in the technical

00:42:03.910 --> 00:42:08.340
community as well as the user community
about when somebody says USB 2.0,

00:42:08.340 --> 00:42:13.240
is it really 480 megabit per
second or is it just USB 2.0?

00:42:13.240 --> 00:42:14.960
Which label did they have?

00:42:14.960 --> 00:42:17.410
High speed or full speed, one of those.

00:42:17.410 --> 00:42:21.210
They're playing games with names and
saying they're USB 2.0 when they really

00:42:21.220 --> 00:42:23.440
still only run at 12 megabits a second.

00:42:24.210 --> 00:42:27.300
And just to be clear,
this implementation is the full

00:42:27.300 --> 00:42:30.100
480 megabit per second USB 2.0.

00:42:32.230 --> 00:42:34.850
Also,
we added the optical digital audio I/O.

00:42:35.020 --> 00:42:37.830
We have customers that really like that.

00:42:37.950 --> 00:42:42.420
Analog audio I/O in and out as usual.

00:42:42.420 --> 00:42:48.500
This machine supports Bluetooth and
it also supports Airport Extreme.

00:42:48.560 --> 00:42:52.740
Since as you can see,
this enclosure is basically a metal box,

00:42:52.750 --> 00:42:55.180
it's kind of hard to get
an antenna out of that.

00:42:55.270 --> 00:43:00.160
So there's actually ports on the rear
with small antennas that stick out that

00:43:00.160 --> 00:43:03.870
are installable that either come with
the machine or with the Bluetooth or

00:43:03.870 --> 00:43:06.370
Airport option when you buy it.

00:43:06.720 --> 00:43:10.200
In addition, we put some new ports on
the front of the machine.

00:43:10.210 --> 00:43:13.490
In addition to the headphone port,
we've added a USB port

00:43:13.570 --> 00:43:15.820
and a FireWire 400 port.

00:43:15.830 --> 00:43:18.180
That's really for connecting.

00:43:22.800 --> 00:43:24.980
That's really for connecting
those digital hub type devices,

00:43:24.980 --> 00:43:27.000
you know, when you bring your iPod
or your digital camera,

00:43:27.000 --> 00:43:29.640
something that you plug
in and out all the time.

00:43:29.640 --> 00:43:30.680
It's really just for convenience.

00:43:30.680 --> 00:43:33.100
And I'm glad to hear that you
guys like it because there was

00:43:33.160 --> 00:43:34.650
quite a bit of debate about that.

00:43:34.650 --> 00:43:36.120
It's hard to do, believe me.

00:43:36.140 --> 00:43:39.300
It sounds simple,
but SCC gets involved and they

00:43:39.300 --> 00:43:43.600
like things not to interfere
with radio stations and such.

00:43:46.040 --> 00:43:48.320
Anyhow,
now I'm going to talk a little bit

00:43:48.430 --> 00:43:50.880
about thermal management in the system.

00:43:50.880 --> 00:43:55.820
This is one of the places where we really
put a lot of thought and a lot of effort

00:43:55.820 --> 00:43:58.390
and really wanted to do a good job.

00:43:59.980 --> 00:44:02.710
Thermal management, in some sense,
is about cooling,

00:44:02.710 --> 00:44:04.300
but it's really about noise.

00:44:04.300 --> 00:44:06.970
It's really about you
walk into an office,

00:44:07.010 --> 00:44:11.640
or you walk, much more important,
you walk into that bedroom or office in

00:44:11.640 --> 00:44:15.990
your home where you've got your computer,
and if it's roaring away,

00:44:15.990 --> 00:44:19.200
it's just a horribly noisy,
annoying thing.

00:44:21.470 --> 00:44:24.520
We implemented sleep a few
years ago as one way to help

00:44:24.520 --> 00:44:27.180
solve that problem because when
you put your machine to sleep,

00:44:27.410 --> 00:44:29.290
it goes virtually silent.

00:44:29.340 --> 00:44:32.900
For this machine,
we wanted it to be virtually

00:44:32.980 --> 00:44:34.860
silent while running.

00:44:34.920 --> 00:44:37.760
Now,
that's a challenge because you've got

00:44:37.760 --> 00:44:42.880
two of these G5 processors which have
just huge amounts of processing power

00:44:42.880 --> 00:44:46.230
and it takes electrical power to do that,
which generates heat.

00:44:47.480 --> 00:44:50.740
You've also got PCI cards that in
some people's systems can take huge

00:44:50.740 --> 00:44:54.630
amounts of power if they're doing video
processing and that sort of stuff.

00:44:54.700 --> 00:44:58.520
So managing all this to a
least common denominator type

00:44:58.520 --> 00:45:00.730
solution just would not work.

00:45:00.840 --> 00:45:02.320
The thing would roar like an airplane.

00:45:02.320 --> 00:45:04.580
And we knew that wasn't acceptable.

00:45:04.580 --> 00:45:07.100
So what we've done is we've
broken the machine into

00:45:07.100 --> 00:45:09.020
separate discrete thermal zones.

00:45:09.040 --> 00:45:12.020
You can kind of see them coming
in the picture on the left there.

00:45:12.020 --> 00:45:15.230
Let's start at the bottom.

00:45:15.240 --> 00:45:17.420
That's the power supply
actually hiding under there.

00:45:17.420 --> 00:45:19.740
It's pretty much hidden from the user.

00:45:19.740 --> 00:45:23.780
You can't see it below this edge,
but there's actually a wall right

00:45:23.780 --> 00:45:25.780
here in the bottom of the box.

00:45:25.780 --> 00:45:28.010
The power supply takes in
cool air from the front and

00:45:28.080 --> 00:45:29.480
exhausts hot air out the back.

00:45:29.540 --> 00:45:32.100
That means it's not
preheated by the CPUs,

00:45:32.100 --> 00:45:34.440
nor does it preheat anything else.

00:45:37.480 --> 00:45:39.890
The power supply manages itself
and the fact that it's getting cool

00:45:39.890 --> 00:45:43.830
air means that it does not have
to run its fans very fast to keep

00:45:43.830 --> 00:45:46.540
all its parts within specification,
which has been a challenge

00:45:46.560 --> 00:45:47.280
to us in the past.

00:45:47.300 --> 00:45:51.230
If we go up to the top of the box,
that's where the optical drive is,

00:45:51.290 --> 00:45:53.510
that's where the hard drives are.

00:45:54.760 --> 00:45:58.620
That zone has its separate
thermal chamber as well.

00:45:58.620 --> 00:46:01.910
Air comes in the front,
goes through the box,

00:46:02.010 --> 00:46:02.840
and comes out the back.

00:46:02.910 --> 00:46:06.560
In this particular case,
we have a temperature sensor mounted

00:46:06.640 --> 00:46:11.100
up in the corner of the box that
monitors exhaust temperature constantly.

00:46:11.860 --> 00:46:14.980
If the machine moves into a hot room,
we need to move a little more

00:46:14.980 --> 00:46:16.680
air to keep those drives cool.

00:46:16.680 --> 00:46:19.010
If all of a sudden you're
hitting your hard drive hard,

00:46:19.090 --> 00:46:21.170
it's going to be putting
off an awful lot of power,

00:46:21.340 --> 00:46:24.030
heating up the air, we see it get hotter,
we turn up the cooling

00:46:24.030 --> 00:46:25.400
to keep that drive cool.

00:46:25.400 --> 00:46:28.970
We maintain that zone within spec,
but only to the amount you're

00:46:29.060 --> 00:46:32.640
using it and only to the amount
required by your environment.

00:46:32.660 --> 00:46:35.200
So if you're in a cold room,
your machine's quieter.

00:46:35.200 --> 00:46:38.830
If you're in a hot room,
it has to move the air a little

00:46:39.000 --> 00:46:41.720
faster to keep the machine cool.

00:46:41.740 --> 00:46:45.990
But as I say, it's absolutely the minimum
required to maintain the

00:46:45.990 --> 00:46:48.280
machine in its operating state.

00:46:48.280 --> 00:46:50.240
If you go down into the
next zone right here,

00:46:50.410 --> 00:46:52.580
you can actually see the kind
of dip in the plastic chamber.

00:46:52.580 --> 00:46:55.780
This guides the airflow
over the PCI cards.

00:46:55.780 --> 00:46:58.220
So rather than all the air
running up over the top and

00:46:58.220 --> 00:47:01.000
out the back as fast as it can,
it actually runs through the

00:47:01.000 --> 00:47:05.350
cards between the cards and
keeps them cool individually.

00:47:05.560 --> 00:47:09.710
Given the huge variety of placement
options and power configuration

00:47:09.770 --> 00:47:12.800
options there are in PCI cards,
there's no way we can predict, you know,

00:47:12.800 --> 00:47:15.560
that the slot card in slot two
is going to be hot while the

00:47:15.560 --> 00:47:17.120
card in slot three is cool.

00:47:17.120 --> 00:47:19.920
And we can't put a temperature
sensor anywhere to determine

00:47:20.510 --> 00:47:21.820
how to cool that zone.

00:47:21.820 --> 00:47:24.410
So instead,
we went to actually monitoring the

00:47:24.410 --> 00:47:26.340
power consumed by all your cards.

00:47:26.740 --> 00:47:29.690
So if you have a graphics
card in there and that's it,

00:47:29.690 --> 00:47:33.480
and it's consuming very little power,
the fan's going to run

00:47:33.480 --> 00:47:36.160
at its minimum speed,
which is quiet.

00:47:36.160 --> 00:47:37.260
It's really quiet.

00:47:37.260 --> 00:47:38.200
You can't hear it.

00:47:40.530 --> 00:47:44.470
If you have a high performance
NVIDIA card or ATI card that

00:47:44.470 --> 00:47:48.280
actually is pretty high power,
but you're not gaming right now,

00:47:48.280 --> 00:47:50.770
you're not using that power
and it's not being consumed and

00:47:50.770 --> 00:47:52.190
the fan still runs low speed.

00:47:52.200 --> 00:47:55.210
If you're gaming, yeah,
the card starts to get hot,

00:47:55.210 --> 00:47:59.390
but we just start turning up the fan
and keeping it cool just to the absolute

00:47:59.390 --> 00:48:01.840
level required to cool the machine.

00:48:01.840 --> 00:48:03.740
We've got lots of airflow to work with.

00:48:03.850 --> 00:48:08.720
We don't have to work incredibly
hard to cool most of these cards.

00:48:08.820 --> 00:48:10.590
Until you get to a
full PCI configuration,

00:48:10.590 --> 00:48:12.120
that fan runs relatively slow.

00:48:13.520 --> 00:48:17.780
The most complex zone in the system,
of course,

00:48:17.780 --> 00:48:19.320
is the one that handles the G5s.

00:48:19.320 --> 00:48:23.230
If you notice,
there's actually two fans in

00:48:23.420 --> 00:48:26.250
the front of the box and two
fans in the rear of the box.

00:48:26.250 --> 00:48:29.620
There's actually these two right
here and then two right back

00:48:29.770 --> 00:48:31.370
here at the back of the box.

00:48:31.980 --> 00:48:34.530
Now, I've been watching the
web and people are saying,

00:48:34.530 --> 00:48:37.430
you know, with nine fans, man,
the thing's going to roar.

00:48:37.450 --> 00:48:39.660
Well, it's actually the exact opposite.

00:48:39.660 --> 00:48:42.000
As I've been explaining
about the other fans,

00:48:42.000 --> 00:48:44.340
you know,
we only cool to the minimum possible

00:48:44.340 --> 00:48:47.760
and since we don't preheat any air
going from one device to another,

00:48:47.760 --> 00:48:51.520
everything's getting cold air so it
just takes much less air to cool it.

00:48:52.970 --> 00:48:56.800
The CPUs have this same philosophy,
and the push-pull nature of those

00:48:56.830 --> 00:49:00.170
fans actually let us run them
slower as well because the heat

00:49:00.170 --> 00:49:02.380
sink has a resistance to airflow.

00:49:02.380 --> 00:49:05.040
So as we push air through it,
if we didn't have something

00:49:05.040 --> 00:49:08.260
pulling on the other side,
then we'd have to push harder, i.e.

00:49:08.260 --> 00:49:10.060
we'd have to run the fan faster.

00:49:10.060 --> 00:49:12.950
The fan pushing against that pressure
actually is what makes it make noise,

00:49:12.950 --> 00:49:15.970
or a good portion of that noise is
actually the back pressure the fan feels.

00:49:16.500 --> 00:49:19.190
So by putting the two fans in
the push-pull configuration

00:49:19.190 --> 00:49:22.220
for a given amount of airflow,
we're actually much quieter than

00:49:22.220 --> 00:49:23.820
we would be with a single fan.

00:49:25.430 --> 00:49:27.720
In addition,
they're paired top and bottom

00:49:27.720 --> 00:49:29.270
to match up with the CPUs.

00:49:29.300 --> 00:49:31.350
I mean,
you can see the lines in the animation.

00:49:31.350 --> 00:49:35.000
This fan and this fan cool
this CPU for the most part.

00:49:35.090 --> 00:49:39.300
I mean, there is some cross coupling
and we call it one zone,

00:49:39.300 --> 00:49:43.360
but the two pairs of fans
are controlled separately.

00:49:43.740 --> 00:49:47.410
So say you have a multiprocessor
machine and you have one thread

00:49:47.450 --> 00:49:51.090
that just eats the CPU and then
the other CPU is sitting idle.

00:49:51.110 --> 00:49:54.120
We don't have to turn up
the fans on both CPUs even.

00:49:54.120 --> 00:49:57.530
We just turn up the fans on
the CPU that's getting hot.

00:49:57.620 --> 00:50:00.520
In addition,

00:50:03.970 --> 00:50:09.890
The fans are controlled by the
temperature actually of the CPU.

00:50:09.960 --> 00:50:12.830
So we're actually sensing the
temperature that's important

00:50:12.900 --> 00:50:15.220
to keep within specification.

00:50:15.320 --> 00:50:22.330
So we're once again cooling only
the amount required by the CPU.

00:50:22.600 --> 00:50:24.860
This brings up another trick that
we've got in our back pocket,

00:50:24.860 --> 00:50:28.120
which is on PowerBooks for
a few years now,

00:50:28.120 --> 00:50:32.420
you've seen the options to run them
faster or slower or automatic switching.

00:50:34.420 --> 00:50:39.190
Today, in the Power Mac G5,
we've added that technology to the G5.

00:50:39.200 --> 00:50:43.800
What the 2 gigahertz machine
actually does is when you need it,

00:50:43.880 --> 00:50:44.980
it runs 2 gigahertz.

00:50:45.060 --> 00:50:49.000
When you don't need it,
it runs at 1.3 gigahertz or

00:50:49.000 --> 00:50:51.820
two-thirds of its full horsepower.

00:50:51.820 --> 00:50:56.400
Now, in reality, most of the time,
for what you're doing, that's plenty.

00:50:56.400 --> 00:51:00.050
I mean, a 1.3 gigahertz G5 is a screamer.

00:51:00.420 --> 00:51:04.380
But, you know,
there are people running Photoshop and

00:51:04.800 --> 00:51:08.040
Final Cut Pro renders and all
sorts of high-end applications

00:51:08.040 --> 00:51:09.500
that really chew on the processor.

00:51:09.540 --> 00:51:11.310
And that performance is
fully there for them.

00:51:11.340 --> 00:51:14.870
And it's fully there for you whenever you
need it to run your compiles or whatever.

00:51:14.880 --> 00:51:17.830
But the thing is,
is when we can drop that

00:51:17.830 --> 00:51:23.140
performance by that 1.3,
we can save roughly 60% of the power

00:51:23.140 --> 00:51:26.020
consumed by the processor itself.

00:51:26.020 --> 00:51:28.600
And when we put on all
our dynamic scaling,

00:51:28.600 --> 00:51:30.260
we actually get down to about 1.6.

00:51:30.400 --> 00:51:31.880
That's 1.6 of the
maximum processor power.

00:51:31.890 --> 00:51:34.380
So when your machine's sitting
there idle in the finder,

00:51:34.380 --> 00:51:37.080
it's consuming 1.6 of the
power that you have available

00:51:37.140 --> 00:51:38.500
to you any time you need it.

00:51:38.630 --> 00:51:41.520
It switches in milliseconds
and speeds back up.

00:51:41.640 --> 00:51:45.100
And there's actually not
even a processing latency hit

00:51:45.100 --> 00:51:46.920
to speed up and slow down.

00:51:46.920 --> 00:51:50.280
We continue execution as we
go from 1.3 to 2 gigahertz and

00:51:50.280 --> 00:51:52.480
then back down from 2 to 1.3.

00:51:52.480 --> 00:51:56.630
You just slowly get faster and slowly
get slower if you don't need it.

00:51:56.700 --> 00:51:59.780
So that allows us to save
a whole lot of power.

00:52:00.340 --> 00:52:04.160
And then we can run the fans
incredibly slowly on the processor.

00:52:04.430 --> 00:52:08.450
In fact, when the machine is idling,
we may end up with the fans

00:52:08.450 --> 00:52:13.200
spinning a little bit just because,
but we don't need to turn them at all.

00:52:13.240 --> 00:52:17.440
That's the efficiency of the cooling
system that we put into the G5 machine.

00:52:17.490 --> 00:52:21.620
We do not actually have to spin the
fans to cool these CPUs when they're

00:52:21.620 --> 00:52:23.800
sitting there idle in the finder.

00:52:23.990 --> 00:52:28.190
So if you leave your machine and
you get up to go to the bathroom,

00:52:28.370 --> 00:52:29.300
It's not doing anything.

00:52:29.300 --> 00:52:31.340
Or you're just sitting
there staring at your mail.

00:52:31.340 --> 00:52:33.120
It's not doing anything.

00:52:33.120 --> 00:52:36.290
Well,
some mails take time to read and process.

00:52:36.290 --> 00:52:39.680
Actually clicking next on your
little mail program doesn't

00:52:39.800 --> 00:52:41.940
take any real horsepower either.

00:52:41.940 --> 00:52:47.130
So a lot of the stuff that you do,
you know, editing source code,

00:52:47.130 --> 00:52:50.860
for example,
doesn't take a lot of the CPU.

00:52:50.860 --> 00:52:55.260
So when you're in that mode,
we're down at a sixth the power and

00:52:55.260 --> 00:52:57.960
the fan's hardly spinning at all.

00:52:58.330 --> 00:52:58.750
If at all.

00:52:58.780 --> 00:53:03.400
So we think that's really important
and it's of real value to our users.

00:53:03.400 --> 00:53:07.090
And one of the messages I just
want to get across is although

00:53:07.190 --> 00:53:11.210
there's nine fans in there,
that's so we can spin them slow.

00:53:11.210 --> 00:53:13.940
If you only have one fan,
something's probably going to be hot just

00:53:14.060 --> 00:53:17.380
because it's been heated by everything
else as the air winds its way through the

00:53:17.380 --> 00:53:19.360
box and all the different heat sources.

00:53:19.360 --> 00:53:21.520
And you've got to run
it fast all the time.

00:53:21.520 --> 00:53:24.200
And by putting all the different
fans in that control the

00:53:24.210 --> 00:53:27.370
different zones independently
and only to as much as they need,

00:53:27.380 --> 00:53:32.040
we can manage to keep all the fans
running slowly as much of the time as

00:53:32.040 --> 00:53:34.660
possible and keep the whole system quiet.

00:53:37.010 --> 00:53:41.580
So I guess in summary,
I'd just like to point out that the real

00:53:42.110 --> 00:53:47.660
goal of the G5 architecture was to take
the G5 processor and wrap a system around

00:53:47.660 --> 00:53:52.480
it that could allow you guys to deliver
the applications and the performance to

00:53:52.480 --> 00:53:56.220
your users that really screams and really
makes them want to buy more computers,

00:53:56.260 --> 00:53:56.840
really.

00:53:56.840 --> 00:54:00.820
That's my only personal take.

00:54:00.900 --> 00:54:03.150
But anyhow,
so what we did was we just opened

00:54:03.240 --> 00:54:04.910
up all the pipes in the system.

00:54:05.260 --> 00:54:07.230
We've got the high bandwidth
interfaces from the processor,

00:54:07.260 --> 00:54:09.340
the system controller,
the system controller that

00:54:09.690 --> 00:54:10.980
connects everything together.

00:54:10.980 --> 00:54:15.110
And then high bandwidth memory system,
AGP interface,

00:54:15.350 --> 00:54:20.550
and IO system to boot to deliver
everything to everybody that they need.

00:54:20.560 --> 00:54:23.860
Thank you very much.

00:54:39.200 --> 00:54:43.000
In terms of reference tech notes that
we've posted that went live yesterday,

00:54:43.000 --> 00:54:47.890
there are two important ones here,
Tuning for G5, a practical guide,

00:54:47.890 --> 00:54:50.220
and the PowerPC G5 Performance Primer.

00:54:50.220 --> 00:54:56.850
Now, the presentation that you saw today
regarding the G5 processor from Peter and

00:54:56.850 --> 00:55:02.060
the system architecture from Keith,
I don't want you to leave here thinking,

00:55:02.060 --> 00:55:04.870
great,
Apple delivered this super fast system,

00:55:04.870 --> 00:55:06.580
my application is going to run fast.

00:55:06.680 --> 00:55:07.770
And yes, it will.

00:55:08.440 --> 00:55:10.940
But there's a whole lot more
performance that you can achieve

00:55:10.940 --> 00:55:11.980
out of this architecture.

00:55:12.000 --> 00:55:13.270
And that was the goal.

00:55:13.280 --> 00:55:18.380
The PowerPC G5 has a lot more
to offer than what you see here.

00:55:18.380 --> 00:55:22.540
We've provided a lot of resources
at the developers conference,

00:55:22.540 --> 00:55:26.560
online, and following the developers
conference in terms of developer

00:55:26.560 --> 00:55:29.820
kitchens that we will have to help
you understand how to unlock that

00:55:29.820 --> 00:55:31.920
performance in your applications.

00:55:31.920 --> 00:55:35.370
There are several sessions that
will cover how you do that.

00:55:35.440 --> 00:55:37.780
So I want to go and show
you just a few of those.

00:55:37.780 --> 00:55:40.210
of those here on the roadmap.

00:55:41.230 --> 00:55:43.930
Wednesday,
there's a session entitled "Chud

00:55:43.940 --> 00:55:48.180
Performance Optimization Tools in
Depth." That's session 506.

00:55:48.310 --> 00:55:49.480
Highly recommended.

00:55:49.480 --> 00:55:52.340
If you are not profiling,
analyzing the performance

00:55:52.380 --> 00:55:55.380
of your application,
looking at where your function

00:55:55.380 --> 00:55:58.720
calls are spending the most time,
you are leaving a lot of

00:55:58.720 --> 00:56:00.220
performance on the table.

00:56:00.320 --> 00:56:03.250
You need to be at this session to
understand how to optimize your

00:56:03.350 --> 00:56:06.090
applications for the G5 processor.

00:56:06.180 --> 00:56:10.530
Session 507,
MacOSX High Performance Math Libraries.

00:56:10.710 --> 00:56:15.520
Our math high performance group
has worked extensively to tune

00:56:15.570 --> 00:56:19.030
these libraries specifically
for the G5 processor.

00:56:19.140 --> 00:56:22.160
These are libraries that come in
MacOSX that will be available as

00:56:22.160 --> 00:56:27.850
well in Panther as well as in Jaguar
that will give you high performance

00:56:27.850 --> 00:56:30.320
access to arithmetic functions.

00:56:30.370 --> 00:56:33.470
Session 304, GCC in Depth,
will talk about how using the

00:56:33.600 --> 00:56:37.060
compiler you can set flags
appropriate for the G5 processor

00:56:37.060 --> 00:56:39.400
to again unlock that performance.

00:56:39.510 --> 00:56:41.880
And then finally,
throughout this whole week

00:56:41.980 --> 00:56:45.960
and today until midnight,
we are holding a G5 optimization lab on

00:56:46.000 --> 00:56:48.910
the first floor in the California room.

00:56:49.020 --> 00:56:53.230
There are 40 systems set up to enable
you developers to bring and work with

00:56:53.230 --> 00:56:57.110
our engineers on your source code
to understand exactly how to use the

00:56:57.170 --> 00:57:02.350
tools to profile your application for
performance and what changes you need

00:57:02.430 --> 00:57:05.310
to make to unlock that performance.

00:57:05.500 --> 00:57:08.800
Again, one of this, the main goal of this
lab is not to sit down,

00:57:08.800 --> 00:57:10.630
take a test drive,
see how fast these dual

00:57:10.750 --> 00:57:12.020
processor systems work.

00:57:12.100 --> 00:57:15.330
It's really to sit down with an
engineer and work on your code.

00:57:15.340 --> 00:57:18.750
Later on in the week,
there is an ADC compatibility lab at

00:57:18.790 --> 00:57:23.100
the very end of the labs on the first
floor where I'll have a system there.

00:57:23.100 --> 00:57:25.970
If you want to take a look at
the insides and just kind of get

00:57:25.980 --> 00:57:29.310
a feel for the system itself,
I'll have a system there for you.

00:57:29.400 --> 00:57:33.090
But again, the lab itself is really gold
for you to work on source code,

00:57:33.090 --> 00:57:34.640
work with our engineers.

00:57:34.780 --> 00:57:35.920
We have engineers from IBM.

00:57:35.920 --> 00:57:40.080
We have engineers from Apple,
several of Apple's engineering groups.

00:57:40.080 --> 00:57:41.910
So please take advantage of that.

00:57:42.060 --> 00:57:46.680
Again, the hours will be today,
all day through midnight, Wednesday,

00:57:47.240 --> 00:57:49.060
Thursday, Friday, 9 a.m.

00:57:49.060 --> 00:57:50.220
to 6 p.m.

00:57:54.490 --> 00:57:55.060
Who to contact?

00:57:55.060 --> 00:57:58.050
If you have questions, information,
follow-up on any of the

00:57:58.460 --> 00:58:01.900
information that you saw today,
please contact me via email,

00:58:01.900 --> 00:58:05.390
tozer at apple.com,
and hopefully you'll be hearing

00:58:05.400 --> 00:58:08.290
from me shortly after the
developers conference on kitchens

00:58:08.310 --> 00:58:10.470
specifically designed to help you,
again,

00:58:10.470 --> 00:58:12.740
optimize your applications for the G5.

00:58:12.740 --> 00:58:14.250
Thank you very much.

00:58:14.370 --> 00:58:15.340
We'll start our Q&A.