WEBVTT

00:00:26.060 --> 00:00:30.010
This is session 111, Writing Threaded
Applications for Mac OS X.

00:00:30.340 --> 00:00:32.380
As you know,
writing a threaded application is one

00:00:32.380 --> 00:00:36.100
way that you can maximize resources
and performance on any system,

00:00:36.100 --> 00:00:38.540
whether it's a single or dual processor.

00:00:38.540 --> 00:00:41.040
This session will go over a
lot of the tips and tricks,

00:00:41.040 --> 00:00:44.320
the do's and don'ts of writing a
threaded application and why you

00:00:44.320 --> 00:00:48.460
would want to do it and times when
you wouldn't want to write a thread.

00:00:48.850 --> 00:00:52.320
So hopefully you've been able
to take a look at the resources

00:00:52.320 --> 00:00:54.790
online at the developer site.

00:00:55.190 --> 00:01:00.020
This session will supplant a lot of
that information and I'd like to bring

00:01:00.020 --> 00:01:02.950
up George Warner who will deliver
the presentation this afternoon.

00:01:03.040 --> 00:01:04.410
Thank you.

00:01:16.210 --> 00:01:19.240
Before I get off this slide,
I just want to touch on my new job title.

00:01:19.240 --> 00:01:21.520
Some people have noticed, asked me about.

00:01:21.540 --> 00:01:25.760
For any of you that don't know,
I'm the engineer formerly known as the

00:01:25.760 --> 00:01:28.320
Mixed Mode Magic Fragment Scientist.

00:01:28.360 --> 00:01:31.890
For obvious reasons,
those technologies have

00:01:31.890 --> 00:01:34.180
kind of faded out somewhat.

00:01:34.180 --> 00:01:38.220
So people have asked,
suggested that I update my job title.

00:01:39.340 --> 00:01:39.970
Excuse me.

00:01:39.970 --> 00:01:44.340
So the Schizophrenic
Optimization Scientist now.

00:01:44.340 --> 00:01:47.860
A little debate over whether
it should be plural or not,

00:01:47.860 --> 00:01:48.130
but...

00:01:54.060 --> 00:01:57.840
So the agenda will be
covering some terminologies.

00:01:57.890 --> 00:02:01.880
There's some main conflicts in
some of the threading models.

00:02:01.880 --> 00:02:04.300
The why, the why not.

00:02:04.300 --> 00:02:06.520
We'll be covering
different architectures,

00:02:06.580 --> 00:02:07.770
talk about the do's and the don'ts.

00:02:07.770 --> 00:02:12.740
We'll be doing some demos and
then we'll break for some Q&As.

00:02:14.770 --> 00:02:19.520
So the original thread manager was
a cooperative thread manager and

00:02:19.520 --> 00:02:23.910
everything was scheduled cooperatively
and we referred to it threads.

00:02:23.910 --> 00:02:28.110
When we came out with the
MP implementation in 8.6,

00:02:28.110 --> 00:02:31.970
we didn't want to call them
threads so we called them task.

00:02:31.970 --> 00:02:35.040
Now we're running on a Unix
system where everybody knows

00:02:35.040 --> 00:02:38.540
what a task is and there's a bit
of a clash in the naming space.

00:02:38.540 --> 00:02:41.690
So for the purpose of
today's presentation,

00:02:41.730 --> 00:02:44.680
when I'm talking about a thread,
we're talking about an MP.

00:02:44.710 --> 00:02:46.080
It's an independent path of execution.

00:02:46.080 --> 00:02:48.700
Everybody is pretty much
up on what a thread is.

00:02:48.700 --> 00:02:51.360
When I talk about a process,
it's a collection of

00:02:51.360 --> 00:02:54.450
threads plus the resources,
basically an application

00:02:54.450 --> 00:02:56.080
or a task that's running.

00:02:56.080 --> 00:02:59.140
In the Unix world, you'd call it a task,
but I have a tendency

00:02:59.150 --> 00:03:00.420
to call it a process.

00:03:00.420 --> 00:03:06.420
And I want to point out multiprocessing
is a general case of multitasking.

00:03:06.420 --> 00:03:11.460
Back in 8.6 when we introduced all this,
a lot of people I'd suggest, well,

00:03:11.490 --> 00:03:13.600
why don't you use an MP thread?

00:03:13.600 --> 00:03:14.680
And they go, well,
I don't have a MP thread.

00:03:14.680 --> 00:03:15.460
I have an MP box.

00:03:15.460 --> 00:03:18.220
And it took me a while to
educate the market and make them

00:03:18.220 --> 00:03:21.520
understand that multithreading
really doesn't have anything to

00:03:21.520 --> 00:03:23.300
do with multiprocessing per se.

00:03:23.300 --> 00:03:26.390
There's just advantages that if
you do have another processor,

00:03:26.390 --> 00:03:29.420
the multithreading API can help
you take advantage of that.

00:03:32.250 --> 00:03:35.230
So why use threads?

00:03:35.520 --> 00:03:38.280
There's a customer expectation
that your application is

00:03:38.280 --> 00:03:40.310
going to be very responsive,
that when I click a button

00:03:40.310 --> 00:03:41.650
something's going to happen.

00:03:41.790 --> 00:03:46.790
And there's nothing worse than when
you get the little spinning beach ball.

00:03:47.220 --> 00:03:48.010
Scalability.

00:03:48.280 --> 00:03:51.200
Like I said earlier, if you actually have
a dual processor box,

00:03:51.370 --> 00:03:54.190
customers really want to see
both processors getting utilized.

00:03:54.190 --> 00:03:58.640
So if you write your code threaded,
there's a better chance that

00:03:58.640 --> 00:04:03.160
those second processor resources
will be getting utilized.

00:04:03.350 --> 00:04:08.950
So, way back when I first started
working on the MP stuff,

00:04:09.080 --> 00:04:11.290
these benchmark numbers came
out and they keep getting

00:04:11.300 --> 00:04:12.940
regurgitated and reused every year.

00:04:12.940 --> 00:04:15.180
Anybody that's come to a previous
session has seen these numbers.

00:04:15.180 --> 00:04:17.930
The really interesting
number there is the top one.

00:04:17.930 --> 00:04:21.930
On a dual processor box,
we went to 2.3 and everybody goes,

00:04:22.060 --> 00:04:24.380
wait a minute,
how can you get better than

00:04:24.440 --> 00:04:26.350
two times the performance?

00:04:26.400 --> 00:04:27.720
You only got twice the horsepower.

00:04:28.120 --> 00:04:30.560
And that's exactly what I thought
when the first time I ran the numbers,

00:04:30.560 --> 00:04:31.670
so I ran the numbers again.

00:04:31.670 --> 00:04:34.760
And when I went and analyzed
the numbers and tried to

00:04:34.760 --> 00:04:38.220
figure out what was going on,
what we finally determined was we

00:04:38.220 --> 00:04:41.620
don't just have twice the processors,
we also have twice the cache.

00:04:41.620 --> 00:04:46.030
So, our data job just happened to not
quite fit well in a single processor,

00:04:46.030 --> 00:04:50.040
one megabyte cache, but it fit extremely
well in a two megacache.

00:04:50.040 --> 00:04:52.570
And so we got actually better
than twice the performance

00:04:52.580 --> 00:04:54.210
and that's called superscalar.

00:04:54.220 --> 00:04:58.100
So, why?

00:04:58.120 --> 00:05:00.130
I use threads.

00:05:01.750 --> 00:05:06.940
This is what I call
the shelf space theory.

00:05:06.990 --> 00:05:09.300
The more things you have
scheduled and running out there,

00:05:09.300 --> 00:05:10.450
the more CPU time you'll get.

00:05:10.450 --> 00:05:12.800
So that's one way.

00:05:12.800 --> 00:05:15.350
If there's two apps running
with the same priority,

00:05:15.350 --> 00:05:19.360
then they'll both get
approximately 50-50% of the time.

00:05:19.420 --> 00:05:23.340
If you've got three threads running
and there's only one other thread

00:05:23.340 --> 00:05:26.580
running in the same priority,
then you're getting three

00:05:26.590 --> 00:05:27.940
quarters of the time.

00:05:27.940 --> 00:05:30.910
So it's kind of a cheesy
way to borrow more time.

00:05:31.700 --> 00:05:35.490
No way I would ever do that.

00:05:36.130 --> 00:05:39.690
So synchronous request,
talking about the spinning beach ball,

00:05:39.690 --> 00:05:45.280
if you call an API in your event
loop and stall the processor

00:05:45.280 --> 00:05:48.320
for whatever the time is,
all of a sudden you get the spinning

00:05:48.320 --> 00:05:51.190
beach ball and your customer goes,
well, this isn't a very responsive app.

00:05:51.230 --> 00:05:52.040
I don't like this one.

00:05:53.940 --> 00:05:56.480
So you can put blocking calls
in your own thread and let the

00:05:56.520 --> 00:05:59.940
main thread continue running,
dispatching events, updating the screen,

00:05:59.940 --> 00:06:02.240
and let the user do what he wants to do.

00:06:02.240 --> 00:06:05.860
And when the synchronous call finishes,
he can signal back to

00:06:05.870 --> 00:06:08.540
the main thread that,
hey, I've finished doing this.

00:06:08.540 --> 00:06:10.400
You can update the progress bar.

00:06:10.400 --> 00:06:12.100
You can continue running on.

00:06:13.910 --> 00:06:16.130
So polling is bad.

00:06:16.200 --> 00:06:17.860
Another one of my favorite examples.

00:06:17.960 --> 00:06:20.560
Anyone that's ever taken a
vacation with a nine-year-old,

00:06:20.610 --> 00:06:22.680
this is the equivalent
of are we there yet?

00:06:22.780 --> 00:06:23.560
Are we there yet?

00:06:23.700 --> 00:06:24.970
Are we there yet?

00:06:25.140 --> 00:06:26.330
And for me, bad is annoying.

00:06:26.340 --> 00:06:29.400
So blocking is much better.

00:06:29.400 --> 00:06:32.240
You get away from all the asynchronous
calls and playing with callbacks

00:06:32.280 --> 00:06:36.460
and dealing with does it happen when
I thought it would and chaining events,

00:06:36.460 --> 00:06:37.100
et cetera.

00:06:37.230 --> 00:06:40.340
You can just put everything
in its own thread.

00:06:40.380 --> 00:06:44.720
A good example of this was in,
I think it was 8.1 or so when we

00:06:44.720 --> 00:06:47.520
actually went to the finder copy.

00:06:47.520 --> 00:06:52.380
And once upon a time you do a
finder copy and you go get coffee

00:06:52.590 --> 00:06:55.580
because it was only to do the copy.

00:06:55.650 --> 00:07:00.130
When a very clever engineer,
I won't mention his name, it wasn't me.

00:07:00.240 --> 00:07:03.500
I did say clever engineer, didn't I?

00:07:03.670 --> 00:07:08.480
Wrote the asynchronous copy and
basically he chained all these

00:07:08.480 --> 00:07:10.060
IO completion with this state machine.

00:07:10.060 --> 00:07:13.020
It's about four pages of really
complicated code that gave you a headache

00:07:13.020 --> 00:07:15.560
to even be in the same room with.

00:07:15.610 --> 00:07:19.270
And it was really sweet and
everybody liked the fact that

00:07:19.270 --> 00:07:22.420
you have multiple copies going on
and all this kind of fun stuff.

00:07:22.440 --> 00:07:25.440
And that was great up until 8.6 came out.

00:07:25.520 --> 00:07:28.200
And we took those four pages
of state machine and replaced

00:07:28.430 --> 00:07:30.570
it with like six lines of code.

00:07:30.700 --> 00:07:32.130
We spawn it off in another thread.

00:07:32.140 --> 00:07:37.080
We do source file open,
destination file open, read from source,

00:07:37.190 --> 00:07:40.020
write to the destination,
loop until the end of file,

00:07:40.160 --> 00:07:43.140
close the source, close the destination,
done.

00:07:43.190 --> 00:07:43.650
Pretty simple.

00:07:43.660 --> 00:07:47.220
The only other thing going on with
all that was updating the progress

00:07:47.220 --> 00:07:50.400
bar over in the finder and that
was all happening behind its back.

00:07:50.400 --> 00:07:57.060
Very simple code, easy to maintain.

00:07:57.060 --> 00:07:57.060
We let the interns do that.

00:07:59.360 --> 00:08:01.780
So when to avoid threads.

00:08:01.800 --> 00:08:05.310
So you want to avoid threads
when it adds complexity.

00:08:05.600 --> 00:08:08.560
If you have global data in your
application that you're going to

00:08:08.570 --> 00:08:10.340
share between multiple threads,
then you're going to

00:08:10.340 --> 00:08:11.140
have to have locks on it.

00:08:11.180 --> 00:08:14.020
And that can get,
you can introduce deadlock conditions

00:08:14.020 --> 00:08:16.960
and so you've got to program
to prevent that and et cetera.

00:08:17.120 --> 00:08:19.140
So global data requires locks.

00:08:19.140 --> 00:08:21.340
If it's just shared,
it may not require locks.

00:08:21.340 --> 00:08:24.620
If you've got one thread
that's only reading the data,

00:08:24.640 --> 00:08:27.450
it may not require locking the data.

00:08:29.550 --> 00:08:33.160
Other reason to avoid threads if
you have non-thread safe APIs.

00:08:33.260 --> 00:08:40.500
Then there's alternatives.

00:08:40.500 --> 00:08:40.500
We'll get to that.

00:08:42.440 --> 00:08:47.590
added overhead,
it takes about 200 microseconds on

00:08:47.630 --> 00:08:56.020
a dual gigahertz machine to create
a thread from the mako thread up.

00:08:56.020 --> 00:08:59.430
And the preemption time is about
40 microseconds on a dual gig.

00:08:59.440 --> 00:09:02.610
Now, interestingly enough,
I had the numbers for the new hardware,

00:09:02.610 --> 00:09:05.460
but I couldn't put it in my slide
for what should be obvious reasons.

00:09:05.460 --> 00:09:09.620
I couldn't rehearse to an audience that
hadn't been briefed on the new numbers.

00:09:09.620 --> 00:09:13.460
So on the new box,
it's around 170 microseconds on

00:09:13.460 --> 00:09:17.720
the creation and it's still exactly
40 microseconds on the new box

00:09:17.920 --> 00:09:22.890
because the preemption has to save
twice the number of registers,

00:09:22.940 --> 00:09:25.500
the width of the registers.

00:09:25.500 --> 00:09:28.000
So I was pretty happy to break
even on the preemption time,

00:09:28.010 --> 00:09:32.780
but we'll continue working,
hopefully bring that down some more.

00:09:32.780 --> 00:09:35.040
The memory footprint.

00:09:35.040 --> 00:09:37.260
Each stack that you create gets a 512.

00:09:37.340 --> 00:09:38.750
12K virtual stack.

00:09:39.070 --> 00:09:41.150
Obviously,
that's not using your physical,

00:09:41.200 --> 00:09:47.080
but it will eat up your
virtual memory space.

00:09:47.080 --> 00:09:52.090
And so you don't want to create a whole
bunch of threads and run out of memory.

00:09:54.290 --> 00:09:55.700
Kernel Resources.

00:09:55.760 --> 00:09:58.350
Every thread you create
is a Mac OS thread.

00:09:58.370 --> 00:10:02.400
It has kernel resources
that we allocate to it.

00:10:02.490 --> 00:10:06.220
The hardware context storage,
which is all those registers,

00:10:06.220 --> 00:10:10.200
and I said 32 here,
but now it probably changes to 64.

00:10:10.370 --> 00:10:14.160
About 2K per thread.

00:10:14.550 --> 00:10:17.580
And that memory is physical and
it's locked down and so we want

00:10:17.580 --> 00:10:19.950
to avoid using too many of that.

00:10:20.090 --> 00:10:21.230
Hundreds of threads.

00:10:21.400 --> 00:10:24.160
When I talked about the preemption
and the 40 microseconds,

00:10:24.160 --> 00:10:24.740
that's a cumulative.

00:10:24.740 --> 00:10:26.730
So if you've got thousands
of threads running,

00:10:26.730 --> 00:10:29.640
you're going to probably spend as
much time preempting and switching

00:10:29.640 --> 00:10:32.300
between all those threads as
running actually useful code.

00:10:32.300 --> 00:10:38.330
So you don't want to get
crazy with inventing,

00:10:38.330 --> 00:10:38.330
with spawning threads.

00:10:40.820 --> 00:10:45.680
So other options, cooperative threads,
if you've got unthread safe APIs,

00:10:45.800 --> 00:10:51.100
you can use the cooperative thread
manager and schedule those cooperatively.

00:10:51.650 --> 00:10:55.510
You can use timers, Carbon timers,
Cocoa timers, et cetera,

00:10:55.510 --> 00:10:59.980
to have tasks run at
predetermined intervals.

00:11:01.660 --> 00:11:05.540
Threading architectures.

00:11:05.540 --> 00:11:07.400
Parallel task with parallel I/O buffers.

00:11:07.400 --> 00:11:13.040
An example of this would be if
you've got multiple independent

00:11:13.040 --> 00:11:15.980
tasks that don't have really
that much to do with each other.

00:11:16.130 --> 00:11:18.860
I think one of the very
first multiprocessing

00:11:18.860 --> 00:11:23.320
projects I ever worked on,
I want to date myself, 20 years ago,

00:11:23.330 --> 00:11:25.280
we were actually running
a driving simulator,

00:11:25.320 --> 00:11:29.500
had an AI running all the cars,
and we had a physics engine

00:11:29.500 --> 00:11:33.080
doing collision detection,
literally,

00:11:33.140 --> 00:11:40.940
and then the Newtonian physics and
then the graphics rendering thread,

00:11:40.940 --> 00:11:41.860
et cetera.

00:11:42.160 --> 00:11:44.250
And when I tell people this, they said,
"Wow,

00:11:44.250 --> 00:11:45.560
you guys were doing that 20 years ago?

00:11:45.560 --> 00:11:49.080
That's pretty impressive."
And then I kind of qualify that.

00:11:49.080 --> 00:11:53.280
We were doing about five minutes
worth of video in a month.

00:11:53.450 --> 00:11:56.610
So things have progressed considerably.

00:11:56.700 --> 00:11:59.710
No real time back then, or not much.

00:12:00.040 --> 00:12:02.410
So it's real slow time.

00:12:02.800 --> 00:12:04.640
Parallel task with shared I/O buffers.

00:12:04.770 --> 00:12:08.940
Now this would be an example of where
you've got the same thing to do on a

00:12:08.940 --> 00:12:12.660
lot of different pieces of data and
you can split it up into little pieces.

00:12:12.710 --> 00:12:15.960
Like if you've got a graphics image and
you can like take little postage stamps

00:12:15.960 --> 00:12:19.600
and feed each little postage stamp off
to a different processor or a different

00:12:19.720 --> 00:12:22.600
task and they all do their crunching
and do everything to do and then put

00:12:22.600 --> 00:12:26.110
them in the output buffer and put it
all back together on the other side.

00:12:26.120 --> 00:12:30.000
And this is the best model for
dealing with exactly that case

00:12:30.000 --> 00:12:33.570
and you typically want as many
tasks as there are processors.

00:12:35.510 --> 00:12:38.070
Sequential task with
multiple I/O buffers.

00:12:38.070 --> 00:12:39.840
I call this the pizza oven.

00:12:39.900 --> 00:12:43.080
Some people call it the assembly line.

00:12:43.150 --> 00:12:49.440
One of my other engineers in DTS wrote an
application that the first input buffer

00:12:49.440 --> 00:12:52.900
was nothing more than some FS specs
that you got from a drag and drop.

00:12:52.960 --> 00:12:58.960
The first task took the list of
FS specs and for each one found

00:12:58.960 --> 00:13:01.950
if it was a file or a folder.

00:13:02.090 --> 00:13:02.490
File or a folder.

00:13:02.490 --> 00:13:04.080
If it was a folder,
it would feed it back in.

00:13:04.080 --> 00:13:06.890
Or if it was a file,
it would send it to the

00:13:07.000 --> 00:13:09.060
output of the first task.

00:13:09.100 --> 00:13:11.300
If it was a folder,
it would iterate over everything and

00:13:11.300 --> 00:13:13.630
then send each file to the output.

00:13:13.680 --> 00:13:18.260
The second task would take this
iterated or unflattened list

00:13:18.270 --> 00:13:22.960
of file specs and start reading
them in and streaming them out.

00:13:22.960 --> 00:13:26.160
The next task would take the
data that's streaming through,

00:13:26.160 --> 00:13:30.220
it would compress it and then the next
would send it over a network port.

00:13:30.240 --> 00:13:31.560
And then et cetera, et cetera.

00:13:31.560 --> 00:13:34.410
On the opposite side of the network,
it would uncompress it and then

00:13:34.410 --> 00:13:36.640
it would unflatten it and then
it would create the directories.

00:13:36.640 --> 00:13:39.670
And so it would basically do a
backup across the network and with

00:13:39.670 --> 00:13:43.630
the compression and everything else,
we actually got better throughput

00:13:43.690 --> 00:13:47.760
than most of the finder did doing
a flat uncompressed copy across.

00:13:47.760 --> 00:13:51.290
But this is an example of a
sequential task where you're basically

00:13:51.290 --> 00:13:52.960
doing one thing after another.

00:13:52.960 --> 00:13:56.100
after another after
another to the same data.

00:13:56.710 --> 00:13:58.940
Threading architectures.

00:13:58.940 --> 00:14:04.700
Many applications have both parallel and
serial or sequential execution paths.

00:14:04.700 --> 00:14:08.720
So it's basically everyone has to know
their app best and know which model

00:14:08.720 --> 00:14:10.940
works best with what they're doing.

00:14:10.940 --> 00:14:15.470
So word processor,
you could have a grammar or spell

00:14:15.470 --> 00:14:20.980
check running independently of font,
texture, current rendering, et cetera.

00:14:20.980 --> 00:14:23.460
So driving simulator,
like I mentioned earlier,

00:14:23.470 --> 00:14:25.680
you could have AIs driving
the other cars.

00:14:25.740 --> 00:14:27.560
Same time you got a
physics engine running,

00:14:27.580 --> 00:14:30.120
same time you got a rendering
engine running that's

00:14:30.120 --> 00:14:33.110
putting bits on the screen.

00:14:35.050 --> 00:14:38.100
So implementations,
I'm not going to go into an

00:14:38.100 --> 00:14:40.740
API account or anything like that.

00:14:40.750 --> 00:14:44.160
Depending on which architecture
you prefer programming in,

00:14:44.160 --> 00:14:46.030
there's thread implementations
in all of them.

00:14:46.270 --> 00:14:50.560
Java has Java threads,
Carbon has NS threads,

00:14:50.580 --> 00:14:53.840
and Cocoa has NS threads.

00:14:53.840 --> 00:14:56.310
Oh, this is all mixed up.

00:14:57.120 --> 00:14:59.700
They just wanted to see
if I was paying attention.

00:14:59.730 --> 00:15:02.340
Okay.

00:15:02.340 --> 00:15:05.240
And so you use the one that's
appropriate for your environment.

00:15:05.310 --> 00:15:07.360
I have to say, having looked at the
implementation of these,

00:15:07.360 --> 00:15:09.460
the top layers are extremely thin.

00:15:09.460 --> 00:15:11.260
They're sitting right
on top of P threads.

00:15:11.260 --> 00:15:15.520
So I wouldn't worry too much about the
overhead in that model and I wouldn't

00:15:15.520 --> 00:15:19.790
go to P threads unless you really,
really, really, really want to.

00:15:19.790 --> 00:15:21.800
So...

00:15:24.020 --> 00:15:28.660
So the common concepts between
the different threading models

00:15:28.800 --> 00:15:33.680
is basically thread management,
creating threads, destroying threads,

00:15:33.680 --> 00:15:36.500
setting the thread priority or weight,
et cetera.

00:15:36.720 --> 00:15:40.220
The synchronization primitives,
you got mutexes and semaphores and

00:15:40.330 --> 00:15:44.960
queues and all the event groups,
et cetera, ways of communicating

00:15:44.960 --> 00:15:46.960
between different threads.

00:15:47.350 --> 00:15:49.180
And you've got thread safe services.

00:15:49.410 --> 00:15:53.520
So this is like currently malloc
and the memory allocators,

00:15:53.520 --> 00:15:56.940
the file I/O, the synchronous file
I/O are all thread safe,

00:15:56.940 --> 00:15:57.750
et cetera.

00:15:57.910 --> 00:16:00.690
So this is probably the most
important part of my talk,

00:16:00.690 --> 00:16:02.140
the do's and the don'ts.

00:16:02.140 --> 00:16:05.770
This is kind of a collection of
all the things that I ran into from

00:16:06.070 --> 00:16:10.050
developers coming to me with issues and
sitting down and going over code and

00:16:10.070 --> 00:16:11.870
figuring out why we're having problems.

00:16:12.820 --> 00:16:18.390
So I mentioned earlier that 200
microsecond creation time can add

00:16:18.390 --> 00:16:22.620
up if you're dynamically creating
and destroying a lot of threads.

00:16:22.780 --> 00:16:26.300
So a good way to avoid that is
to preallocate and use pools.

00:16:26.420 --> 00:16:29.380
Same thing with the memory and et cetera.

00:16:33.010 --> 00:16:35.230
Try to be as data-driven as possible.

00:16:35.240 --> 00:16:38.640
You want to write your code CPU-driven,
so you want to feed as much

00:16:38.650 --> 00:16:43.190
data to it and have it ready
to go when you get ready to do.

00:16:43.440 --> 00:16:45.120
Typically, this isn't a problem.

00:16:45.120 --> 00:16:48.860
Most people write their code
in what I call this format.

00:16:49.050 --> 00:16:50.930
They have a prologue
where they open files,

00:16:50.980 --> 00:16:53.190
allocate memory, et cetera.

00:16:53.350 --> 00:16:56.120
They crunch, crunch, crunch, crunch,
and when they're done,

00:16:56.210 --> 00:16:59.280
they close their files,
dispose of memory, and release,

00:16:59.280 --> 00:17:00.130
et cetera.

00:17:04.150 --> 00:17:07.460
Some of the other implementations
on other operating systems for some

00:17:07.460 --> 00:17:11.970
reason love doing the kill and the
cancel kind of things where they have

00:17:11.970 --> 00:17:15.170
one thread stopping another thread.

00:17:16.300 --> 00:17:20.720
Our system doesn't play well with
what I call asynchronous behavior.

00:17:20.720 --> 00:17:24.980
We didn't design it that way
for good reasons and we really,

00:17:24.980 --> 00:17:29.390
if you want to take advantage
of the way we do things,

00:17:29.510 --> 00:17:33.890
you really want to avoid
using those kind of APIs.

00:17:34.360 --> 00:17:38.620
When at all possible,
use the synchronization methods instead.

00:17:38.620 --> 00:17:44.210
So use a mutex or semaphore, et cetera,
to control.

00:17:47.370 --> 00:17:51.320
So when you use Semaphore
or UTX on a data structure,

00:17:51.420 --> 00:17:54.660
you have to kind of achieve a balance
between having too many and too little.

00:17:54.660 --> 00:17:58.300
If you have one big huge data tree
and you've only got one lock on it,

00:17:58.360 --> 00:18:00.870
then the chances of someone
else needing that if you've got

00:18:00.870 --> 00:18:04.270
multiple threads running is a lot
higher than if you do your locks

00:18:04.290 --> 00:18:06.720
down in the branches of the tree.

00:18:06.720 --> 00:18:10.690
So you don't want to lock everything
because you'll probably spend more

00:18:10.690 --> 00:18:14.880
time locking and unlocking things
than actually accessing your data.

00:18:14.880 --> 00:18:17.160
So there's a trick to
kind of find the balance.

00:18:17.300 --> 00:18:18.400
Amen.

00:18:21.260 --> 00:18:24.100
This is back again to are we there yet?

00:18:24.280 --> 00:18:28.170
The void that's been waiting.

00:18:28.500 --> 00:18:31.500
One of the common mistakes I see is
occasionally we'll have a developer that

00:18:31.500 --> 00:18:34.040
wants to wait on two things at once.

00:18:34.040 --> 00:18:37.930
And what they'll typically do is they'll
check one with a timeout and after the

00:18:38.000 --> 00:18:41.530
timeout they'll check the other one
with a timeout and then they'll look

00:18:41.540 --> 00:18:43.560
back and check the first one again.

00:18:43.560 --> 00:18:45.900
And now they're basically still polling.

00:18:45.900 --> 00:18:46.550
Are we there yet?

00:18:46.550 --> 00:18:47.250
Are we there yet?

00:18:47.260 --> 00:18:49.760
What you can do instead
is have two threads,

00:18:49.760 --> 00:18:53.300
either thread waiting
on a different event,

00:18:53.300 --> 00:18:55.810
different mutex,
and when either one of those

00:18:55.810 --> 00:18:58.940
threads gets its signal,
it signals the third thread

00:18:58.940 --> 00:19:00.520
that I've got my signal.

00:19:00.540 --> 00:19:04.040
And so when he gets that signal,
that's the OR, he can continue.

00:19:04.040 --> 00:19:07.550
If you're waiting on two things,
if you want both events to

00:19:07.550 --> 00:19:10.620
happen before you continue,
then you can just block on

00:19:10.620 --> 00:19:13.020
one and wait for it to finish.

00:19:13.020 --> 00:19:15.590
And when it finishes,
go and block on the other one

00:19:15.590 --> 00:19:17.090
and wait for it to finish.

00:19:17.100 --> 00:19:20.770
And if the second one's already happened,
you'll just continue running.

00:19:20.870 --> 00:19:24.560
So there's no reason to,
there's never a reason to poll.

00:19:27.270 --> 00:19:27.590
GUI.

00:19:27.590 --> 00:19:30.660
This is another one of those
areas I get a lot of questions on.

00:19:30.660 --> 00:19:32.200
It seems like everybody
that ever jumps in,

00:19:32.200 --> 00:19:34.940
the very first thing they want to
do is try to do some GUI things.

00:19:34.940 --> 00:19:39.680
The good news is, I mean,
things are getting much better.

00:19:39.800 --> 00:19:43.250
In the Mac OS 9 days,
it was just the definite don't.

00:19:43.250 --> 00:19:45.250
Nowadays, we have Quartz.

00:19:45.260 --> 00:19:47.660
You can draw output to the screen.

00:19:47.660 --> 00:19:48.920
We have OpenGL.

00:19:48.970 --> 00:19:51.070
You can output to the screen.

00:19:51.070 --> 00:19:54.640
And if you're Carbon,
I'll pick on Carbon here.

00:19:55.380 --> 00:19:58.200
If, like, for example,
you're doing that file copy and

00:19:58.310 --> 00:20:01.240
you want to update a progress bar,
a real easy way to do that,

00:20:01.240 --> 00:20:02.390
use post event to queue.

00:20:02.390 --> 00:20:04.780
And that way you can tell
your main event thread,

00:20:04.800 --> 00:20:06.190
update the progress bar.

00:20:06.190 --> 00:20:09.530
So one nice thing about the
Carbon event model in particular is

00:20:09.530 --> 00:20:13.580
I can send it 14 update events and it
doesn't put 14 events in the queue.

00:20:13.580 --> 00:20:16.300
It's smart enough to know I've already
got an update event in the queue

00:20:16.300 --> 00:20:18.500
and just leaves that one in there.

00:20:18.500 --> 00:20:22.680
So you don't have to worry
about overflowing the queue.

00:20:22.680 --> 00:20:24.140
So I've got a demo.

00:20:29.260 --> 00:20:31.060
This will be the not-threaded version.

00:20:31.100 --> 00:20:35.260
And you see the menus are dead.

00:20:35.660 --> 00:20:37.480
I can't drag the menu down.

00:20:37.490 --> 00:20:39.200
Oops, I could drag the menu down.

00:20:39.200 --> 00:20:40.680
Finished.

00:20:44.650 --> 00:20:48.600
I've got a threader running down in the
bottom so you could see what's going on.

00:20:48.600 --> 00:20:49.950
We've got three threads.

00:20:50.090 --> 00:20:51.600
I'll move my arrow down here.

00:20:51.600 --> 00:20:52.970
Everybody can see that?

00:20:53.040 --> 00:20:53.860
Okay.

00:20:54.010 --> 00:20:57.670
So if I click here,
you'll see taking up all the CPU time

00:20:57.670 --> 00:20:59.890
down in the main thread here.

00:21:01.300 --> 00:21:15.800
[Transcript missing]

00:21:18.210 --> 00:21:20.200
You can drag the window around.

00:21:20.200 --> 00:21:22.200
Menus work.

00:21:22.200 --> 00:21:28.310
And that's the behavior that
your users are going to expect.

00:21:35.340 --> 00:21:43.260
So as you can tell,
there hasn't been a whole

00:21:43.260 --> 00:21:44.420
lot of new information.

00:21:44.420 --> 00:21:48.460
The best thing we can say is I think
you know what we've been working on.

00:21:48.460 --> 00:21:51.290
We haven't broken anything
that we know about.

00:21:51.450 --> 00:21:54.610
Everything that there works,
works the way it has pretty well and

00:21:54.610 --> 00:21:56.730
hopefully we'll continue working on that.

00:21:56.850 --> 00:21:59.230
I'll keep working on the
preemption time to get that down,

00:21:59.330 --> 00:22:01.040
but other than that,
that's about the extent of it.

00:22:01.180 --> 00:22:04.710
So hopefully we'll cover anything
else you want to know in the Q&As.

00:22:04.880 --> 00:22:07.010
So thank you.

00:22:13.750 --> 00:22:16.240
So just to give you a wrap
up of some of the sessions,

00:22:16.240 --> 00:22:18.430
the Kernel Extension Programming
Techniques,

00:22:18.430 --> 00:22:20.470
that was on Monday,
so on the DVD you'll be able

00:22:20.470 --> 00:22:24.380
to get the opportunity to
take a look at that session.

00:22:26.390 --> 00:22:28.700
For contacts, George Warner and DTS.

00:22:28.700 --> 00:22:32.560
You may already be familiar with
George through your contacts

00:22:32.560 --> 00:22:35.620
and email and myself as a
desktop hardware evangelist.

00:22:35.640 --> 00:22:39.530
Let's go ahead and start
Q&A and invite a few folks up.