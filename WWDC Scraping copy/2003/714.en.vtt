WEBVTT

00:00:25.970 --> 00:00:29.350
This is session number 714,
Large-Scale Webcasting:

00:00:29.490 --> 00:00:31.240
Macworld Keynote Case Study.

00:00:31.240 --> 00:00:34.700
Our goal today is to try and
give you guys an idea of what it

00:00:34.700 --> 00:00:40.700
takes to be able to execute very
large-scale Internet webcasting,

00:00:40.820 --> 00:00:42.900
live and video on demand.

00:00:42.900 --> 00:00:47.060
The case we use is the Macworld Keynote,
which is arguably one of,

00:00:47.060 --> 00:00:49.890
if not the largest,
live Internet streaming

00:00:49.890 --> 00:00:51.750
events every time we do one.

00:00:51.900 --> 00:00:53.900
As a matter of fact,
every time we do one,

00:00:53.900 --> 00:00:54.900
it gets bigger and bigger and bigger.

00:00:56.070 --> 00:01:01.030
The reason we do a webcast of the event
is because when we have Steve in a hall,

00:01:01.030 --> 00:01:07.900
he has 5,000 to 6,000 people who are
sitting there getting two hours of Steve,

00:01:07.910 --> 00:01:11.890
a direct marketing message from probably
one of the best marketers around.

00:01:11.900 --> 00:01:15.980
But while there are 5,000 to
6,000 people there in the room,

00:01:15.980 --> 00:01:20.360
we have over 100,000 unique viewers
who are watching via the Internet,

00:01:20.360 --> 00:01:20.900
live.

00:01:20.900 --> 00:01:24.050
And then over the next seven days,
we have probably more than 500,000

00:01:24.210 --> 00:01:24.780
people watching the event live.

00:01:24.930 --> 00:01:31.800
So that's over 600,000 people who get to
see two hours of Steve Jobs delivering

00:01:31.840 --> 00:01:37.890
a precise and very precise marketing
message about Apple's new products.

00:01:37.900 --> 00:01:40.900
That's a very powerful thing,
as you can imagine.

00:01:40.900 --> 00:01:45.150
So today to speak about this,
we have Clark Smith and Ryan Lynch from

00:01:45.200 --> 00:01:47.890
the Apple QuickTime Operations Group.

00:01:47.900 --> 00:01:51.470
These guys are the two that are
majorly responsible for doing the

00:01:51.550 --> 00:01:53.890
keynote preparation and execution.

00:01:54.080 --> 00:01:57.630
And in addition, a little bit later on,
we'll have Bill Weil,

00:01:57.630 --> 00:02:00.900
the CTO from Akamai Technologies,
here to talk about how

00:02:00.910 --> 00:02:04.110
Akamai works with us before,
during, and then after the keynote to

00:02:04.110 --> 00:02:06.900
pull one of these things off.

00:02:06.930 --> 00:02:09.760
So let me turn it over to Ryan,
and we'll get started.

00:02:09.900 --> 00:02:13.140
One thing, when we have questions,
it's really important that

00:02:13.270 --> 00:02:14.880
you step up to the microphone.

00:02:14.950 --> 00:02:17.890
We have simultaneous translation
happening in the back of the room,

00:02:17.900 --> 00:02:21.560
and if we don't have a clear version of
your voice going over the microphone,

00:02:21.560 --> 00:02:22.880
they can't translate.

00:02:22.900 --> 00:02:22.900
Okay?

00:02:22.940 --> 00:02:23.860
can't translate.

00:02:23.990 --> 00:02:24.260
Okay, thanks.

00:02:27.660 --> 00:02:28.600
Thanks, Dennis.

00:02:28.600 --> 00:02:32.060
So we're going to talk today about the
process behind the keynote and what we

00:02:32.110 --> 00:02:38.800
do at Apple to prepare for the impending
doom of going live on the Internet.

00:02:38.870 --> 00:02:43.200
So we're going to start with
what we do behind the scenes with

00:02:43.200 --> 00:02:47.790
network requirements and planning
for the whole event all the way

00:02:47.790 --> 00:02:50.100
up to the event and afterwards.

00:02:50.100 --> 00:02:54.330
And three months ahead of time,
before we actually start,

00:02:54.330 --> 00:02:56.100
we deal with network.

00:02:56.100 --> 00:02:58.100
And Clark is going to talk
a little bit to that point.

00:02:59.180 --> 00:03:03.490
In order to provision a network
for the kind of load that

00:03:03.490 --> 00:03:08.310
the keynote would require,
you have to plan ahead and decide, well,

00:03:08.310 --> 00:03:10.560
what other events might be
happening that day for Akamai.

00:03:10.600 --> 00:03:15.050
Akamai is a content distribution
network that serves many,

00:03:15.130 --> 00:03:16.310
many clients.

00:03:16.410 --> 00:03:18.600
Apple is an important client, obviously.

00:03:18.600 --> 00:03:19.600
But if there's a large news event,
it's going to be a big one.

00:03:19.600 --> 00:03:22.200
And if there's a large news event
or something that takes place

00:03:22.210 --> 00:03:24.750
on the same day as our keynote,
we need to make sure that there's

00:03:24.750 --> 00:03:28.900
going to be enough room for us to
still get our data out on the Web.

00:03:29.510 --> 00:03:34.140
So we try to estimate two
months in advance how much data

00:03:34.140 --> 00:03:37.180
we think we're going to need,
and we try to negotiate with them

00:03:37.180 --> 00:03:39.330
to see how much they can give us.

00:03:39.420 --> 00:03:45.650
So we go into traffic expectations
based on the location that

00:03:45.710 --> 00:03:46.800
the event is taking place.

00:03:46.800 --> 00:03:48.900
If it's in San Francisco,
Japan is going to be more

00:03:48.900 --> 00:03:51.280
involved because it's still
within their day range.

00:03:51.280 --> 00:03:54.170
If it's a European event,
Japan's not going to be a player,

00:03:54.180 --> 00:03:55.940
and probably neither is California.

00:03:56.160 --> 00:03:59.040
But if it's in New York,
there's a good chance

00:03:59.040 --> 00:04:01.970
that California will come
on later in the stream.

00:04:02.120 --> 00:04:05.380
So based on the amount
of data available to us,

00:04:05.510 --> 00:04:08.430
the bandwidth available to us,
and the expectation we have,

00:04:08.430 --> 00:04:11.640
the amount of promotion we expect is
going to happen for that event and so on,

00:04:11.750 --> 00:04:14.390
we start determining--
if you want to hit that,

00:04:14.460 --> 00:04:14.850
hit it.

00:04:14.990 --> 00:04:15.380
Sorry about that.

00:04:15.420 --> 00:04:15.930
Bit rates.

00:04:16.000 --> 00:04:16.240
I hit it.

00:04:16.340 --> 00:04:16.570
You hit it.

00:04:16.680 --> 00:04:18.000
We both hit it.

00:04:18.000 --> 00:04:19.660
We start determining our bit rates.

00:04:19.810 --> 00:04:21.480
We normally do four bit rates.

00:04:21.490 --> 00:04:26.920
We do a 28k for audio-only users,
which has to run well below 28k because

00:04:26.980 --> 00:04:29.210
we have to be able to build a buffer up.

00:04:29.270 --> 00:04:34.490
And then 56k users are a very
strong subscriber to our events,

00:04:34.520 --> 00:04:38.940
so we have to make sure that we actually
hit somewhere around-- 37k is our target,

00:04:38.940 --> 00:04:40.610
somewhere around 37.

00:04:40.650 --> 00:04:42.660
We even like to get below that sometimes.

00:04:42.900 --> 00:04:46.870
And that gives 56k users enough
of-- most 56k users aren't

00:04:46.870 --> 00:04:49.290
getting much over 42 anyway,
so we have to make sure

00:04:49.290 --> 00:04:50.410
that they have some buffer.

00:04:50.500 --> 00:04:55.860
And then 100k users are generally
from Europe and on dual ISDN.

00:04:55.860 --> 00:05:01.230
And the benefit of 100k also is it
gives you a pretty darn good stream,

00:05:01.340 --> 00:05:04.580
and if we do have to roll down-- we'll
talk about how we roll down later--

00:05:04.580 --> 00:05:09.230
but if we have to roll down to 100k,
you still get a pretty good stream.

00:05:10.300 --> 00:05:14.500
And let's see,
so the next step is we provision

00:05:14.710 --> 00:05:20.620
the Akamai entry points and ports,
and that's just a process of basically

00:05:20.790 --> 00:05:24.940
requesting from the edge servers that
we're going to begin streaming to

00:05:24.940 --> 00:05:28.860
specific IP addresses and specific ports.

00:05:28.860 --> 00:05:31.440
And you're hitting the button, so cool.

00:05:31.440 --> 00:05:34.390
And then we have a backup plan,
and the backup plan is always to

00:05:34.500 --> 00:05:39.980
have an additional connection that
is also as close to flawless as

00:05:39.990 --> 00:05:44.170
possible so that if somehow something
happens to that initial connection or

00:05:44.320 --> 00:05:48.310
something happens to that entry point,
Akamai entry point,

00:05:48.470 --> 00:05:53.560
we have an immediate solution to
rolling over to another live stream

00:05:53.560 --> 00:06:00.300
that will look as close as possible
to identical to the Akamai entry

00:06:00.350 --> 00:06:03.440
point that we serve as a primary.

00:06:04.550 --> 00:06:09.300
So the first thing we do is we start
to gather our hardware and software.

00:06:09.330 --> 00:06:11.290
Because we're working in
a co-location facility,

00:06:11.290 --> 00:06:14.330
which is basically all rack mounted,
the XSERVs are actually

00:06:14.360 --> 00:06:15.760
wonderful for the solution.

00:06:16.010 --> 00:06:18.940
We have a group of five or
six XSERVs right next to,

00:06:18.940 --> 00:06:22.140
within feet, of the Akamai entry point.

00:06:22.240 --> 00:06:26.010
We have an MPEG-2 recorder,
which is a digital disc recorder.

00:06:26.030 --> 00:06:28.130
It gives you a random access playback.

00:06:28.260 --> 00:06:32.060
So immediately when the event is
over and we're given approval,

00:06:32.210 --> 00:06:36.600
we begin a replay of the event,
which is pretty much

00:06:36.600 --> 00:06:38.690
undetectable from the original.

00:06:38.770 --> 00:06:45.190
And that continues to loop for a
significant period of time until we can

00:06:45.240 --> 00:06:48.490
get the data posted for video on demand.

00:06:48.900 --> 00:08:27.600
[Transcript missing]

00:08:30.100 --> 00:08:32.430
So we're going to talk,
after we get that all set up,

00:08:32.500 --> 00:08:35.280
we want to test and make sure that
the hardware actually functions.

00:08:35.280 --> 00:08:36.960
Because on occasion
you can have a little,

00:08:36.960 --> 00:08:38.890
you know,
somebody can plug a FireWire plug-in

00:08:38.900 --> 00:08:40.380
backwards and fry the whole thing.

00:08:40.380 --> 00:08:40.980
It's kind of fun.

00:08:40.980 --> 00:08:45.190
And so we test all of those,
make sure we have enough disk space

00:08:45.190 --> 00:08:47.940
available for the actual event if
you're going to actually record a disk.

00:08:47.940 --> 00:08:51.320
We did a two and a quarter
hour program on Monday,

00:08:51.320 --> 00:08:56.340
and that took roughly 250 megabytes
to record that to disk at 250K.

00:08:56.340 --> 00:09:00.000
So we did a, it was pretty sizable,
and if you have a full

00:09:00.120 --> 00:09:02.810
event and you don't actually
delete anything on your disk,

00:09:02.900 --> 00:09:03.660
you're going to run into trouble.

00:09:03.660 --> 00:09:07.550
And then we take those
settings that we figured out,

00:09:07.580 --> 00:09:10.680
okay, we need to target about
37K for 56 and all that,

00:09:10.750 --> 00:09:15.000
and set those up and make sure, and test,
and make sure that they're optimal

00:09:15.000 --> 00:09:18.600
for the bit rates we want to target,
and that they don't swing too much.

00:09:18.600 --> 00:09:21.660
Because if you swing,
then you might max out your bandwidth,

00:09:21.660 --> 00:09:24.420
and somebody on the network will
get a really crappy experience,

00:09:24.420 --> 00:09:25.790
and you don't want that to happen.

00:09:25.800 --> 00:09:29.040
And we take these settings,
and we copy them to every

00:09:29.070 --> 00:09:32.180
single machine so they have an
identical setup on every box,

00:09:32.180 --> 00:09:36.360
just in case we need to pull
one out or something like that.

00:09:36.390 --> 00:09:39.880
And we,
like we had on the previous graphic,

00:09:39.880 --> 00:09:45.680
there is a spare machine sitting,
standing by, just in case we have to

00:09:45.810 --> 00:09:48.490
do something drastic.

00:09:49.320 --> 00:09:50.100
Then we take the team.

00:09:50.190 --> 00:09:52.590
So we have everything set up,
we've got our settings,

00:09:52.600 --> 00:09:53.500
we're ready to go.

00:09:53.820 --> 00:09:56.480
We need to get people together about
three months ahead of time and start

00:09:56.550 --> 00:10:00.760
talking about what's happening so
that we're ready to go for the day.

00:10:00.820 --> 00:10:03.030
We have Akamai Engineering who's
provisioning the network and

00:10:03.030 --> 00:10:05.710
taking care of everything to
make sure that we're ready,

00:10:05.710 --> 00:10:10.600
and we have our 16 gigabits or whatever
we're going to use for the event.

00:10:10.650 --> 00:10:13.170
And then we have
QuickTime Engineering team.

00:10:13.290 --> 00:10:16.010
So sometimes we'll run into a
little funny bug or something and

00:10:16.010 --> 00:10:17.460
we wonder what's going on with this.

00:10:17.460 --> 00:10:20.940
So we have them kind of do some packet
tracing and make sure that everything's

00:10:20.940 --> 00:10:22.520
functioning correctly for us.

00:10:22.570 --> 00:10:25.290
And then we have our,
the PR team is actually really important.

00:10:25.300 --> 00:10:27.700
They're the ones who give us
the final go to say we can

00:10:27.700 --> 00:10:30.040
actually use VOD and be ready.

00:10:30.170 --> 00:10:32.560
Because if we show something we
don't have legal clearance for,

00:10:32.560 --> 00:10:34.600
we don't want anybody coming
back at us and saying,

00:10:34.600 --> 00:10:39.060
"Hey, no, no." It wouldn't be too fun
to get into legal over that.

00:10:39.110 --> 00:10:41.650
And then we go down to the
television contractors.

00:10:41.650 --> 00:10:45.030
We have them actually on site recording
the videos so that when we have a

00:10:45.030 --> 00:10:47.420
tape we can go back and do an encode.

00:10:47.420 --> 00:10:49.340
after the fact.

00:10:52.580 --> 00:10:56.500
So then we have the preparation step.

00:10:56.500 --> 00:10:58.320
We have everything ready to go.

00:10:58.320 --> 00:11:03.900
We need to export SDP files
from the QuickTime broadcaster.

00:11:03.900 --> 00:11:06.340
And these SDP files,
Session Description Protocol,

00:11:06.510 --> 00:11:09.460
basically tell the QuickTime player
where to go to get the stream,

00:11:09.590 --> 00:11:13.290
how do I get the stream,
and everything about it.

00:11:14.920 --> 00:11:17.850
Since we're using Akamai,
we have something called ARLs,

00:11:17.870 --> 00:11:19.620
Akamaized Resource Locators.

00:11:19.740 --> 00:11:23.210
They're essentially URLs that
point to that content,

00:11:23.210 --> 00:11:27.520
the STP files,
and have a little metadata along with it.

00:11:27.580 --> 00:11:33.100
It helps Akamai figure out how to get the
stream and how to deliver it best to you.

00:11:34.320 --> 00:11:36.200
And we have, of course, reference movies.

00:11:36.390 --> 00:11:39.000
Now,
reference movies are kind of interesting.

00:11:39.000 --> 00:11:45.160
They allow you to have one link and point
to as many other movies as you want.

00:11:45.160 --> 00:11:47.570
So you can basically,
what we do is we filter

00:11:47.570 --> 00:11:49.040
based upon bit rate.

00:11:49.040 --> 00:11:52.300
So if you claim that you have a 300K
connection and you can get that,

00:11:52.300 --> 00:11:55.430
we say,
"Good," and we deliver the 300K to you.

00:11:55.440 --> 00:11:58.740
So that's all based upon your
configurations of your machine.

00:11:58.740 --> 00:12:00.530
So what we do is we have a web page.

00:12:00.530 --> 00:12:02.770
So we have a web page
that has all the links,

00:12:02.850 --> 00:12:07.540
or It has one link for all,
one ref movie for all of the links.

00:12:07.720 --> 00:12:13.630
And that ref movie points to,
lo and behold, four different bit rates.

00:12:13.630 --> 00:12:16.310
And based upon your settings,
we deliver that to you.

00:12:16.470 --> 00:12:19.600
So we have the ARL that says,
"Here you go." And it

00:12:19.600 --> 00:12:21.620
hands you the stream,
and you've got that.

00:12:21.800 --> 00:12:24.830
You notice at the bottom that
we have a spliced ref movie.

00:12:24.940 --> 00:12:28.660
And what that means is we have a
graphic that we actually paste on

00:12:28.660 --> 00:12:32.980
top of the stream so that there's a
visual experience for the 28K people.

00:12:33.110 --> 00:12:34.600
Because, you know,
you've got to provide something.

00:12:34.600 --> 00:12:37.290
You've got to make it somewhat
aesthetically pleasing.

00:12:37.740 --> 00:12:42.840
And then this comes into the fact
that we have the velvet rope system,

00:12:42.870 --> 00:12:45.480
which is first point
on the Akama network.

00:12:45.610 --> 00:12:48.880
And what that does is, you know,
we have hard stop at

00:12:48.880 --> 00:12:50.860
some bandwidth limit.

00:12:50.940 --> 00:12:54.600
So for a typical keynote,
we have maybe 16 gigabits.

00:12:54.600 --> 00:12:56.890
And that's all we have.

00:12:57.090 --> 00:12:58.980
And if we go beyond that,
like I said before,

00:12:58.980 --> 00:13:01.380
we're going to have some
really terrible experiences for

00:13:01.380 --> 00:13:02.840
everybody across the network.

00:13:02.840 --> 00:13:07.090
So when we, in a particular region,
if we're ramping too quickly,

00:13:07.090 --> 00:13:10.080
we have too much bandwidth
being consumed too quickly,

00:13:10.080 --> 00:13:13.680
we want to make sure that we can
cut that off and we slip down,

00:13:13.680 --> 00:13:17.600
back down to a 100k stream as
the top bit rate available,

00:13:17.600 --> 00:13:22.070
or a 56k, depending upon how many
people we have on the network.

00:13:22.290 --> 00:13:27.070
And of course, those webpages point to
ref movies and again,

00:13:27.070 --> 00:13:30.200
point to individual
STP files to grab the stream.

00:13:30.200 --> 00:13:32.200
So do you change the webpages?

00:13:32.200 --> 00:13:34.290
Could you speak into the mic, please?

00:13:46.300 --> 00:13:48.780
Yes,
essentially that's what Akamai- what the

00:13:48.780 --> 00:13:54.370
First Point system does is it swaps out
which webpages deliver to which region.

00:13:54.400 --> 00:13:58.700
Okay, I was just curious whether it's the
reference movie you swap or the webpage.

00:13:58.800 --> 00:13:59.700
Both.

00:13:59.700 --> 00:14:03.670
Oh, you change both the
webpage and the- well,

00:14:03.670 --> 00:14:06.600
the webpage points to a
different reference movie.

00:14:06.680 --> 00:14:07.600
Yes.

00:14:07.600 --> 00:14:08.600
Those first four webpages
were already made.

00:14:08.600 --> 00:14:09.600
Yes.

00:14:09.600 --> 00:14:11.600
So you just change the- or
you change it in the backend.

00:14:11.600 --> 00:14:12.590
Yes.

00:14:12.600 --> 00:14:14.030
That's all we're doing is swapping
out the webpage essentially,

00:14:14.030 --> 00:14:17.720
and each one has has a different link
on it to a different rough movie.

00:14:17.960 --> 00:14:20.900
Now if you are like Apple and you have
a whole bunch of zealots on campus,

00:14:20.900 --> 00:14:24.220
and they all want to watch
this stream at the same time,

00:14:24.370 --> 00:14:28.740
you don't want to consume a ton of
bandwidth on your local network.

00:14:28.740 --> 00:14:34.530
So if you have a 600K stream,
and you have 100 people wanting to watch,

00:14:34.530 --> 00:14:38.320
that's 60 megabits that they're going
to be pulling down across your pipe.

00:14:38.320 --> 00:14:42.390
So you're not going to be able to provide
anybody outside of your company anything.

00:14:42.470 --> 00:14:45.640
And that's going to be a
really terrible experience.

00:14:45.700 --> 00:14:48.960
So what we do at Apple is we
have an internal web page.

00:14:48.960 --> 00:14:51.740
Again,
using the Akamai First Point system,

00:14:51.780 --> 00:14:56.380
we say anybody that's coming from
our network gets routed to a specific

00:14:56.440 --> 00:14:58.760
web page for internal traffic.

00:14:58.760 --> 00:15:01.940
And that has a link for a multicast.

00:15:02.000 --> 00:15:08.010
And in short, what a multicast does is it
takes one stream and sends

00:15:08.010 --> 00:15:09.420
it out across the network.

00:15:09.420 --> 00:15:13.420
So if you have 100 people watching it,
you have 600K bandwidth.

00:15:13.510 --> 00:15:17.690
If you have one person watching it,
you have 600K.

00:15:17.900 --> 00:15:21.890
So it's a great way to conserve
bandwidth on a local network.

00:15:23.400 --> 00:15:26.300
And now I'm gonna turn it back
to Clark to talk about testing.

00:15:26.440 --> 00:15:31.300
So we test in a lot of different
levels throughout the process,

00:15:31.300 --> 00:15:33.300
but there are specific
things that can go wrong.

00:15:33.300 --> 00:15:36.030
If you're creating all these
little files and you're moving

00:15:36.030 --> 00:15:38.300
things around so many times,
something can go wrong.

00:15:38.300 --> 00:15:40.300
An example would be annotations.

00:15:40.300 --> 00:15:43.900
Sometimes the annotations change somehow,
and those are actually

00:15:44.680 --> 00:15:47.620
generated in the SDP file,
and they're--so at the very

00:15:47.710 --> 00:15:50.920
beginning of the chain,
if that SDP file has the wrong annotation

00:15:50.920 --> 00:15:55.460
in it or a wrong copyright or something,
that can translate throughout the

00:15:55.540 --> 00:16:00.300
entire-- throughout the entire group of
reference movies that we just outlined.

00:16:00.300 --> 00:16:02.260
We're very attentive to
data rates and swings.

00:16:02.310 --> 00:16:07.230
I spend hours staring at Command-J,
you know,

00:16:07.230 --> 00:16:11.300
the get info or the get properties
function in QuickTime Pro,

00:16:11.300 --> 00:16:13.270
because I can watch
how full the buffer is,

00:16:13.390 --> 00:16:16.650
whether the buffer is seeing any
kind of sawtoothing from some

00:16:16.740 --> 00:16:18.230
kind of network interference.

00:16:18.300 --> 00:16:20.300
I can see whether or not
there's any packet loss,

00:16:20.300 --> 00:16:23.300
but I can actually see where
the packet loss is taking place.

00:16:23.360 --> 00:16:26.150
Sometimes you can't tell why there's
something wrong with a stream,

00:16:26.290 --> 00:16:29.570
but you can actually visually
see why that--you know,

00:16:29.570 --> 00:16:33.290
why you're dropping audio at certain
times or why there--sometimes the video

00:16:33.300 --> 00:16:35.960
will come across absolutely perfectly,
but because there's a

00:16:35.990 --> 00:16:38.220
sawtooth in the buffer,
it'll drop actually

00:16:38.330 --> 00:16:40.260
just a moment of audio,
and you're kind of wondering

00:16:40.660 --> 00:16:41.300
why--where that's coming about.

00:16:41.300 --> 00:16:43.300
A lot of it's very visual.

00:16:43.300 --> 00:16:45.590
The importance of swing is
if you're provisioned for a

00:16:45.590 --> 00:16:49.300
certain number of gigabits,
and you have, you know,

00:16:49.300 --> 00:16:53.300
10 or 15 or 80,000
different viewers watching,

00:16:53.300 --> 00:16:54.240
and you have a certain
number of people watching,

00:16:54.290 --> 00:16:58.300
and you have a slight swing from a camera
zooming in to Steve or Pan on the stage

00:16:58.300 --> 00:17:01.870
where almost every pixel is changing,
it can be a very dramatic

00:17:01.870 --> 00:17:04.260
swing in Akamai's data rate.

00:17:04.310 --> 00:17:06.300
So the higher the data rate
that you're playing with,

00:17:06.300 --> 00:17:09.090
the higher the percentage,
so it's gonna be obviously much more

00:17:09.160 --> 00:17:15.300
reflected in a 300K versus a 56K,
but it's extremely important that you

00:17:15.300 --> 00:17:19.300
pay attention to where that median
point is and how far you're allowing it.

00:17:19.300 --> 00:17:23.300
The reference movies all have
to point to the right things.

00:17:23.300 --> 00:17:27.210
So I saw just a moment ago in Ryan's
diagram how each reference movie

00:17:27.330 --> 00:17:28.300
points to a different data rate.

00:17:28.300 --> 00:17:30.860
One of the best ways to test
that is actually go into your

00:17:30.860 --> 00:17:35.290
QuickTime preferences and just
keep changing your data rates down

00:17:35.300 --> 00:17:42.300
in the user-defined connection
speed portion of your preferences,

00:17:42.300 --> 00:17:45.300
and just make sure that the right
data rate comes up for that one.

00:17:45.300 --> 00:17:47.300
So we go through this
over and over again,

00:17:47.300 --> 00:17:50.300
'cause sometimes we make
reference movies more than once,

00:17:50.300 --> 00:17:53.360
and so I--especially as you're
getting closer and you're hurrying to

00:17:53.360 --> 00:17:55.300
the reference movies more and more,
you really have to be very attentive

00:17:55.300 --> 00:17:56.300
to what mistakes you might make.

00:17:56.300 --> 00:18:00.390
The splice graphic is another
potential problem in that if

00:18:00.550 --> 00:18:04.290
you--when you first create a splice
graphic using one of our tools,

00:18:04.290 --> 00:18:06.300
sometimes it's not
layered exactly properly,

00:18:06.300 --> 00:18:11.300
so you have to go into the properties and
actually set the layering for that image.

00:18:11.300 --> 00:18:15.300
If the image is layered incorrectly,
you'll actually get a little cue,

00:18:15.300 --> 00:18:19.870
and the splice will actually be behind,
and being that that's all tied into

00:18:19.870 --> 00:18:23.300
all the other reference movies,
it's just critical that you test it.

00:18:23.300 --> 00:18:25.290
And last of all is packet loss.

00:18:25.300 --> 00:18:27.300
Different places will
get different results.

00:18:27.300 --> 00:18:30.300
Even though we have such a wonderful
content distribution provider,

00:18:30.300 --> 00:18:33.300
there are certain places that
will get different results,

00:18:33.300 --> 00:18:35.300
and so what we try to do
is we try to call people.

00:18:35.300 --> 00:18:37.860
We try to have some contact with
people in New York and other

00:18:37.860 --> 00:18:39.230
places and see how they're doing.

00:18:39.300 --> 00:18:41.760
All they have to do is open
the properties that I was just

00:18:41.860 --> 00:18:44.120
talking about and tell me,
you know, what's the percentage of

00:18:44.120 --> 00:18:45.280
packet loss that you're seeing?

00:18:45.440 --> 00:18:47.270
Are you having any kind
of dropping issues?

00:18:47.300 --> 00:18:49.300
Are you able to get the
stream whenever you want it?

00:18:49.300 --> 00:18:53.300
Those are all the kinds of tests
that we make certain that we--

00:18:54.110 --> 00:18:56.110
were very attentive to.

00:18:56.200 --> 00:18:58.950
The day of the event,
we established a conference

00:18:59.130 --> 00:19:02.320
call with many of the members
of the team that Ryan outlined,

00:19:02.400 --> 00:19:04.020
but predominantly Akamai.

00:19:04.320 --> 00:19:07.560
And they'll be standing by and paying
attention to their network before

00:19:07.690 --> 00:19:09.430
we actually push anything live.

00:19:09.540 --> 00:19:16.420
I'm also on the phone with our
encoding partner at the satellite site.

00:19:16.480 --> 00:19:18.920
And I sit there and
I watch all of his streams.

00:19:19.060 --> 00:19:21.950
And then at about half
an hour before the event,

00:19:22.240 --> 00:19:25.890
we restart all the broadcasters
just to make sure there aren't

00:19:25.890 --> 00:19:27.550
any memory leaks or anything.

00:19:27.650 --> 00:19:29.940
I mean, when you're dealing with
such a high profile event,

00:19:30.000 --> 00:19:31.440
even though it probably
doesn't make any difference,

00:19:31.500 --> 00:19:36.120
you do a lot of little things that
you just don't want to take any risks.

00:19:36.270 --> 00:19:38.240
So we restart the broadcasters.

00:19:38.450 --> 00:19:42.190
And then shortly before the event,
we roll the MPEG-2 and a

00:19:42.200 --> 00:19:44.180
backup Betacam SP machine.

00:19:44.260 --> 00:19:47.780
We're glad we did that this last event,
because there was a disruption in

00:19:47.780 --> 00:19:52.880
the NTSC signal into the MPEG-2
machine and had that disruption.

00:19:52.950 --> 00:19:56.160
That disruption was long enough to
actually stop the disk recorder.

00:19:56.200 --> 00:20:00.410
And that was catastrophic,
because we had no backup to go to.

00:20:00.490 --> 00:20:04.480
So what we ended up doing is using the
Betacam SP through the exact same proc

00:20:04.480 --> 00:20:06.880
amp that we were talking about before.

00:20:06.910 --> 00:20:10.320
And at the same time,
we recorded to the MPEG-2 machine.

00:20:10.320 --> 00:20:11.660
And no one was the wiser.

00:20:11.700 --> 00:20:13.430
It was just absolutely seamless.

00:20:13.570 --> 00:20:16.200
But had we not had just yet
another layer of backup,

00:20:16.200 --> 00:20:22.100
we wouldn't have been in a
position to make ourselves look OK.

00:20:22.220 --> 00:20:22.710
Yes?

00:20:23.900 --> 00:20:24.900
"What broadcaster are you using?

00:20:24.900 --> 00:20:29.260
Are you using the old Sorenson or is
it the new MPEG-4 from Apple?" "Well,

00:20:29.260 --> 00:20:31.900
the broadcaster product
is Broadcaster 1.01.

00:20:31.900 --> 00:20:32.900
It's QuickTime Broadcaster.

00:20:32.900 --> 00:20:36.770
It's free on our site." "Right,
but you're using yours now.

00:20:36.770 --> 00:20:39.150
In the past you were
using Sorenson's." "No,

00:20:39.150 --> 00:20:39.800
absolutely.

00:20:39.810 --> 00:20:42.000
Yes, we're using ours and
we're using MPEG-4's,

00:20:42.000 --> 00:20:45.400
as you mentioned, and AAC for audio."

00:20:45.570 --> 00:20:49.340
AAC is just a wonderful audio codec,
and for streaming MPEG-4,

00:20:49.340 --> 00:20:54.330
it's just a delight to work with, really.

00:20:54.800 --> 00:20:57.240
And we've come from a
chain of codecs beforehand,

00:20:57.240 --> 00:21:01.800
so believe me, we have the scar tissue.

00:21:02.270 --> 00:21:05.250
So then the actual term,
pushing the event live, takes place.

00:21:05.260 --> 00:21:10.880
We actually, when we start to see
pans of the audience,

00:21:10.910 --> 00:21:16.490
that means that our television content
provider from the location is likely to

00:21:16.490 --> 00:21:19.190
stay on programming from that point on.

00:21:19.660 --> 00:21:24.150
So when we start seeing
that with the lower third,

00:21:24.150 --> 00:21:26.910
I call our web team,
and I tell our web team to go

00:21:27.030 --> 00:21:29.280
ahead and push the page live.

00:21:29.320 --> 00:21:33.100
And then the excitement really begins,
because there are hundreds of people,

00:21:33.350 --> 00:21:35.560
thousands of people out
there just waiting to get on.

00:21:35.560 --> 00:21:37.240
They want to be the first ones on,
because they think they're

00:21:37.240 --> 00:21:40.260
going to get the highest data
rate if they're the first ones.

00:21:40.500 --> 00:21:43.330
And to a degree,
they're correct for most events.

00:21:43.520 --> 00:21:47.250
So they jump on,
and you just see the ramping.

00:21:47.350 --> 00:21:52.440
We have our own monitoring tool,
but so does Akamai back at their NOC.

00:21:52.540 --> 00:21:54.870
And you can just see the
ramping of people getting on.

00:21:54.880 --> 00:21:57.380
And it's very exciting, really,
because you have to

00:21:57.380 --> 00:21:58.380
know when to pull off.

00:21:58.580 --> 00:22:01.480
You know what the highest point is,
and you have to make sure that

00:22:01.490 --> 00:22:02.870
everybody's going to be able to get on.

00:22:03.090 --> 00:22:04.680
It's stressing their network to death.

00:22:04.700 --> 00:22:08.690
It's a wonderful test,
but at the same time, it's frightening.

00:22:08.920 --> 00:22:12.610
So there's a point when we say, OK,
we're going to roll off some region

00:22:12.620 --> 00:22:16.030
that might be particularly stressed,
like maybe Australia,

00:22:16.030 --> 00:22:17.190
which seems to be first.

00:22:17.410 --> 00:22:20.210
But there are certain areas that
we sometimes have to roll off,

00:22:20.410 --> 00:22:21.570
and then we'll come back to them.

00:22:21.600 --> 00:22:23.720
We'll come back to them after
things moderate a little bit.

00:22:23.810 --> 00:22:25.580
We'll come back to the 300K.

00:22:25.620 --> 00:22:28.930
So if you ever have an experience
where you come on and you get 100K,

00:22:28.930 --> 00:22:32.730
and for some reason leave and come back,
you 300K if you have the

00:22:32.740 --> 00:22:36.530
bandwidth provided to you.

00:22:36.970 --> 00:22:39.380
So we watch the consumption,
and then the last thing is actually

00:22:39.480 --> 00:22:42.680
probably the most fun for me,
is we report the numbers.

00:22:42.720 --> 00:22:45.420
And there's usually someone,
usually Dennis, underneath the stage,

00:22:45.420 --> 00:22:48.840
or in the back of the stage,
and just before Steve goes on,

00:22:48.840 --> 00:22:51.080
someone will ask, you know,
"What are our numbers?

00:22:51.150 --> 00:22:54.000
How many people?" You know,
and last time, I think,

00:22:54.000 --> 00:22:56.180
if any of you watched,
the Vatican was on.

00:22:56.220 --> 00:22:58.660
And that was just so thrilling
to have Steve go out and say,

00:22:58.660 --> 00:23:00.050
"Yeah, and even the Vatican's watching.

00:23:00.060 --> 00:23:01.850
You know,
you guys are our friends." So those

00:23:02.080 --> 00:23:06.700
kinds of little reports are just,
they just add a little something.

00:23:06.700 --> 00:23:10.880
They add a little realism to the keynote,
and we all benefit from that.

00:23:11.080 --> 00:23:14.780
So I think now we're going to bring on
the great and good Bill Weil of Akamai.

00:23:14.780 --> 00:23:16.080
Bill Weil: Post event.

00:23:16.090 --> 00:23:16.700
Oh, post event.

00:23:16.700 --> 00:23:17.040
I'm sorry.

00:23:17.040 --> 00:23:17.670
I got a post event.

00:23:17.700 --> 00:23:17.950
Sorry.

00:23:18.060 --> 00:23:18.360
Hold on.

00:23:18.410 --> 00:23:20.220
Bill Weil: That's all right.

00:23:20.680 --> 00:23:21.600
Post- I'm sorry.

00:23:21.600 --> 00:23:22.530
So then we do the rebroadcast.

00:23:22.560 --> 00:23:23.640
I spoke about that earlier.

00:23:23.640 --> 00:23:27.070
We hit that MPEG-2 machine,
and we start playing back as

00:23:27.100 --> 00:23:30.900
soon as we're told that we're
going to do so from Apple PR.

00:23:31.540 --> 00:23:33.420
And at the same time,
a tape is being rushed

00:23:33.750 --> 00:23:39.000
to our encoding partner,
who will capture that entire

00:23:39.760 --> 00:23:44.030
tape and do a pre-process on it,
and probably within 12 hours have

00:23:44.030 --> 00:23:48.430
content for us to start posting,
which I'll do later when

00:23:48.430 --> 00:23:49.460
that data starts to arrive.

00:23:49.500 --> 00:23:52.900
And then we slowly start replacing
the links that were originally

00:23:52.900 --> 00:23:56.300
being posted to on that web page,
the same reference movies.

00:23:56.400 --> 00:23:58.160
So the VOD is posted.

00:23:58.470 --> 00:24:01.250
The pages are changed so we're
no longer doing the velvet rope.

00:24:01.540 --> 00:24:04.400
And then we--the last thing
is usually days later,

00:24:04.400 --> 00:24:09.570
we do an analysis and a reporting
to determine exactly how well we

00:24:09.580 --> 00:24:13.990
did and how many people came and
whether people had a good experience.

00:24:14.010 --> 00:24:17.300
We're very keen and interested in that.

00:24:17.350 --> 00:24:18.360
So, Bill, now you're up.

00:24:18.540 --> 00:24:19.700
Sorry about that.

00:24:19.700 --> 00:24:20.700
No problem.

00:24:20.740 --> 00:24:22.690
Thanks.

00:24:27.750 --> 00:24:30.950
So I want to tell you a little
bit about what happens once

00:24:30.990 --> 00:24:34.280
the bits get handed off to us.

00:24:34.280 --> 00:24:35.440
And I'm going to start by telling you

00:24:35.670 --> 00:24:39.700
A little bit about who we are and
what we do in terms of webcasting,

00:24:39.700 --> 00:24:41.140
both large and small scale.

00:24:41.420 --> 00:24:46.400
Apple, I think, as these guys said,
put on some of the largest

00:24:46.430 --> 00:24:48.980
live events on the Internet in
terms of the amount of traffic,

00:24:48.980 --> 00:24:50.080
number of viewers, and so on.

00:24:50.160 --> 00:24:52.150
But we do events from small to large.

00:24:52.270 --> 00:24:54.170
We do on demand as well as live.

00:24:54.350 --> 00:24:55.340
So I'll tell you a bit about that.

00:24:55.450 --> 00:24:58.870
I'll talk about the partnership
between Apple and Akamai in terms

00:24:58.880 --> 00:25:01.280
of webcasting and other things.

00:25:01.390 --> 00:25:05.490
And then a little bit more
detail on the keynote itself.

00:25:06.730 --> 00:25:09.140
So today, what is Akamai?

00:25:09.140 --> 00:25:10.600
What do we let you do?

00:25:10.600 --> 00:25:11.820
What do we do for our customers?

00:25:11.890 --> 00:25:15.250
Fundamentally,
what we do is allow our customers to

00:25:15.360 --> 00:25:18.940
extend their e-business infrastructure,
web and streaming,

00:25:18.970 --> 00:25:22.080
out to the edge of the Internet,
close to the users.

00:25:22.330 --> 00:25:25.470
This gives better performance,
better reliability, better scalability,

00:25:25.540 --> 00:25:29.630
in many cases better security,
and to gain greater control over

00:25:29.630 --> 00:25:32.990
that infrastructure and over the
delivery of the applications,

00:25:32.990 --> 00:25:35.200
the content,
and so on across the Internet.

00:25:35.500 --> 00:25:38.700
Today, if you're delivering content
over the web or via streaming,

00:25:38.700 --> 00:25:41.200
you can control what you
do in your data center,

00:25:41.200 --> 00:25:43.200
you can control your first mile.

00:25:43.350 --> 00:25:46.200
But at that point, your control ends,
and from there to the end user,

00:25:46.200 --> 00:25:49.290
there's a collection of
networks that are going to take

00:25:49.290 --> 00:25:51.090
your bits and transport them.

00:25:51.260 --> 00:25:53.070
You have no control over that.

00:25:53.240 --> 00:25:57.200
If someone out there screws up,
if UUNED's backbone goes down,

00:25:57.200 --> 00:26:00.080
if the slammer worm hits and
there's chaos across the Internet,

00:26:00.210 --> 00:26:02.170
there's nothing you can do about that.

00:26:02.230 --> 00:26:05.000
We give a lot of control
all the way out to the edge,

00:26:05.000 --> 00:26:07.200
very, very close to the end user.

00:26:07.200 --> 00:26:11.550
We are the leading delivery
service for streaming web

00:26:11.550 --> 00:26:16.090
content and web applications,
and the real value is improving in

00:26:16.090 --> 00:26:20.200
the end the user experience of the
people who are watching the streams

00:26:20.200 --> 00:26:24.910
or accessing your applications,
your content over the web,

00:26:25.020 --> 00:26:28.180
and often at lower cost to the provider.

00:26:28.330 --> 00:26:31.550
We've moved from what we
did in the early days,

00:26:31.630 --> 00:26:34.160
which you could think of as
simply static content delivery,

00:26:34.160 --> 00:26:37.260
we've moved to really now
doing distributed computing.

00:26:37.300 --> 00:26:41.450
So when I talk about applications,
I'm really talking about the

00:26:41.450 --> 00:26:46.300
ability for one of our customers,
people we used to call content providers,

00:26:46.300 --> 00:26:47.300
but it's not just content.

00:26:47.300 --> 00:26:50.590
People are doing business on the net,
and they're reaching out to

00:26:50.710 --> 00:26:54.520
their users with an application,
be it a configurator for a car or an

00:26:54.520 --> 00:26:59.960
e-commerce site or any other kind of
interactive application on a site.

00:26:59.960 --> 00:27:03.300
We're providing the ability for
pieces of those applications to run

00:27:03.300 --> 00:27:07.200
on the edge close to the end user,
which allows you to give sub-second

00:27:07.220 --> 00:27:11.690
response time to users around the world,
wherever they are, regardless of what's

00:27:11.690 --> 00:27:13.110
going on in the Internet.

00:27:13.300 --> 00:27:16.360
We have almost 1,000
recurring customers today.

00:27:16.370 --> 00:27:19.300
We've got about $145
million in annual revenue.

00:27:19.300 --> 00:27:24.300
We've survived the dot-com boom and bust.

00:27:24.410 --> 00:27:29.340
And spent the last two years building
a very solid customer base in

00:27:29.340 --> 00:27:34.300
terms of large enterprise customers
as well as smaller companies.

00:27:34.360 --> 00:27:38.300
And I think we're well poised for
growth over the next few years.

00:27:38.430 --> 00:27:42.820
And we also have a lot of intellectual
properties surrounding the way we deliver

00:27:42.820 --> 00:27:45.290
content and applications over the net.

00:27:46.200 --> 00:28:51.100
[Transcript missing]

00:28:52.500 --> 00:30:28.000
[Transcript missing]

00:30:28.570 --> 00:30:32.700
The other major thing that we give to our
customers is a great deal of information

00:30:32.780 --> 00:30:38.020
about what is happening both on the
Internet in general and to their content,

00:30:38.100 --> 00:30:41.360
their streams, their web applications
across the Internet.

00:30:41.400 --> 00:30:45.450
So information, for example,
about the number of streams

00:30:45.460 --> 00:30:48.290
that are being delivered,
the number of streams

00:30:48.310 --> 00:30:51.480
at different bit rates,
the total amount of bandwidth,

00:30:51.630 --> 00:30:53.400
furthermore broken down
by geographical area.

00:30:53.400 --> 00:30:56.760
So you can see how many people
are watching in Australia,

00:30:56.760 --> 00:30:59.630
how much bandwidth are
we pushing in Australia,

00:30:59.630 --> 00:31:00.390
and so on.

00:31:02.660 --> 00:31:05.100
Just to say a little bit
more about our platform,

00:31:05.100 --> 00:31:07.390
as I said, we've got over 15,000 servers.

00:31:07.390 --> 00:31:10.160
We're in over 1,100 different networks.

00:31:10.240 --> 00:31:13.930
And this is really a range
of networks across the board,

00:31:13.990 --> 00:31:20.940
from hosting and access providers to
companies that provide data center space,

00:31:21.130 --> 00:31:24.430
colo space,
and connectivity to companies that are

00:31:24.490 --> 00:31:28.410
providing sites and streams and so on,
access providers that are

00:31:28.500 --> 00:31:31.540
providing dial-up or broadband
or other access to end users,

00:31:31.580 --> 00:31:36.060
as well as tier one backbones and,
of course, more and more broadband for

00:31:36.060 --> 00:31:38.350
access of one form or another.

00:31:38.510 --> 00:31:44.670
So we are in the same network
as--have machines in the same

00:31:44.730 --> 00:31:49.440
network as on the order of 70 to 80%
of the end users on the Internet.

00:31:49.740 --> 00:31:52.740
Which means that if a user
wants to get a stream,

00:31:52.760 --> 00:31:56.360
there's one of our machines very close by
that can serve that stream to that user.

00:31:56.360 --> 00:31:59.590
Okay, same thing for web content
or for web applications.

00:31:59.610 --> 00:32:03.170
And that's been one of the basic premises
of our company from the beginning is

00:32:03.240 --> 00:32:06.580
that it's vital to be near the end user.

00:32:06.580 --> 00:32:10.950
In terms of performance and reliability,
and in terms of the

00:32:11.020 --> 00:32:13.800
scalability of the system,
that is as there are more users,

00:32:13.800 --> 00:32:16.670
as there are more eyeballs,
we'll have more machines

00:32:16.800 --> 00:32:18.080
near those users.

00:32:18.080 --> 00:32:21.950
And the system as a whole
will scale with the user base.

00:32:23.980 --> 00:32:27.710
Okay, so let me talk a little bit
about how live streaming works.

00:32:27.960 --> 00:32:32.280
The stuff on the left,
Ryan and Clark have talked about,

00:32:33.170 --> 00:32:36.680
The actual video signal
needs to be captured,

00:32:36.680 --> 00:32:40.150
and it needs to be sent to an encoder,
which is going to produce then a

00:32:40.150 --> 00:32:43.140
digital stream in a certain bit rate.

00:32:44.140 --> 00:32:45.460
That stream is then sent.

00:32:45.490 --> 00:32:50.070
So first you get it from the
camera through satellite and other

00:32:50.160 --> 00:32:51.660
mechanisms through the encoder.

00:32:51.660 --> 00:32:53.410
From there,
it goes to what we call an entry point.

00:32:53.420 --> 00:32:56.690
Now we've got a stream of
packets entering our network,

00:32:56.790 --> 00:33:00.550
the Akamai cloud,
represented here with the four circles.

00:33:00.560 --> 00:33:03.270
The entry points themselves
actually are fault-tolerant,

00:33:03.270 --> 00:33:05.370
so there are mechanisms,
as they discussed,

00:33:05.560 --> 00:33:09.900
to allow that path to fail over to a
different one should that entry point,

00:33:09.900 --> 00:33:11.540
for example, go down.

00:33:11.960 --> 00:33:15.930
Or, in fact, it might still be up,
but the path between it and

00:33:15.960 --> 00:33:18.080
the encoder might be congested.

00:33:18.180 --> 00:33:21.000
Might have been fine when you started,
but at a certain point,

00:33:21.000 --> 00:33:22.430
the quality there degrades.

00:33:22.520 --> 00:33:26.480
So you want to make sure you're
talking to an entry point where you can

00:33:26.560 --> 00:33:28.990
send packets with very minimal loss.

00:33:29.760 --> 00:33:35.100
From there, we send that stream to a set
of what we call reflectors.

00:33:35.230 --> 00:33:37.910
We have many of these
scattered around the Internet,

00:33:38.380 --> 00:33:43.500
typically in major backbones
with very good connectivity.

00:33:43.500 --> 00:33:46.580
So the stream is basically
being replicated.

00:33:46.600 --> 00:33:48.430
Now I'm talking for live streaming.

00:33:48.490 --> 00:33:50.500
So this is for a keynote.

00:33:50.500 --> 00:33:54.320
You've got one packet stream going
from the encoder to the entry point,

00:33:54.460 --> 00:33:57.500
and then from there,
that same stream is being replicated.

00:33:57.500 --> 00:34:02.490
Essentially, a separate unicast to
each of those reflectors.

00:34:02.490 --> 00:34:06.500
We can't use network-level multicasts
because these aren't on the same network.

00:34:06.500 --> 00:34:09.500
And you can't do multicasts really
across the Internet in that way.

00:34:09.500 --> 00:34:12.660
You can think of this
as an overlay multicast,

00:34:12.730 --> 00:34:13.820
if you will.

00:34:14.850 --> 00:34:17.080
The idea with the reflectors
is then from there,

00:34:17.080 --> 00:34:20.630
if a user wants a stream,
he'll contact an edge server.

00:34:20.630 --> 00:34:23.100
And let's say, in this case,
it's the middle one.

00:34:23.100 --> 00:34:25.220
He'll say,
I want to get the Steve Jobs keynote.

00:34:25.220 --> 00:34:29.210
And the ARL that was mentioned tells us,
when the user hands us that,

00:34:29.490 --> 00:34:33.550
tells us how to actually get that,
what the appropriate port is and so on,

00:34:33.720 --> 00:34:37.560
to make sure we get the right
stream from the right entry point.

00:34:38.500 --> 00:34:40.260
He'll say, I want the Steve Jobs keynote.

00:34:40.260 --> 00:34:45.460
That edge server then subscribes to
that stream from one or more reflectors,

00:34:45.460 --> 00:34:47.800
and packets will start to flow.

00:34:47.800 --> 00:34:50.800
Now, you might have a situation
where packets start to flow,

00:34:50.920 --> 00:34:52.560
but then some of them get lost.

00:34:52.600 --> 00:34:56.490
Because the Internet,
while it's amazingly reliable for such

00:34:56.590 --> 00:35:01.380
a large and essentially decentralized
system in terms of how it's managed,

00:35:01.380 --> 00:35:04.280
packet loss happens all the time.

00:35:04.280 --> 00:35:06.640
Congestion happens, it appears,
and then disappears.

00:35:06.640 --> 00:35:08.480
How long it lasts depends on the stream.

00:35:08.480 --> 00:35:09.900
And then, of course,
the edge server starts to pull

00:35:10.000 --> 00:35:11.610
from the length of the flows
that go through a congested link.

00:35:11.720 --> 00:35:14.370
So, in this case,
we're showing those packets

00:35:14.470 --> 00:35:18.080
all grayed out because,
in fact, those four packets got lost.

00:35:18.280 --> 00:35:20.970
The edge server,
if there's congestion when it's

00:35:21.070 --> 00:35:25.010
pulling from a single reflector,
will then start to pull from more.

00:35:25.120 --> 00:35:28.160
And it will pull from enough
to guarantee that it gets a

00:35:28.160 --> 00:35:30.040
complete copy of the stream.

00:35:30.060 --> 00:35:32.740
So, in this case,
it was seeing congestion on the initial

00:35:32.740 --> 00:35:34.580
stream that it got from one reflector.

00:35:34.640 --> 00:35:38.450
It subscribes to the same
stream from another reflector.

00:35:38.540 --> 00:35:41.040
It manages to get some of the packets,
but still not all.

00:35:41.040 --> 00:35:43.220
So, it will subscribe to yet another.

00:35:43.280 --> 00:35:48.250
So, we will pull multiple copies to an
edge server as needed to guarantee

00:35:48.280 --> 00:35:52.370
that that edge server gets a
complete copy of the stream.

00:35:52.380 --> 00:35:55.070
In the early days of the system,
we just sent multiple

00:35:55.080 --> 00:35:58.010
copies to every edge server,
doing sort of blind-forward

00:35:58.020 --> 00:35:59.120
error correction.

00:35:59.120 --> 00:36:02.330
But, as you can imagine,
this is expensive.

00:36:02.430 --> 00:36:07.150
And we've built a system now that
is much more adaptive and responds

00:36:07.150 --> 00:36:11.410
to... the conditions of the network
between the reflectors and the edge

00:36:11.410 --> 00:36:16.490
servers to do that adaptively and only
pull as many copies as are needed.

00:36:16.880 --> 00:36:21.350
The other thing I should say about
this process is that one of the key

00:36:21.360 --> 00:36:26.000
things that's not really shown here
is-- and then the end user gets the

00:36:26.000 --> 00:36:30.550
stream from that edge server and
gets a very high quality stream.

00:36:30.720 --> 00:36:37.940
One of the key things that isn't
shown here is the mapping process

00:36:38.060 --> 00:36:39.900
that decides which edge server an end
user will talk to to pull the stream.

00:36:40.420 --> 00:36:43.700
And that is one of the key pieces of
technology that our system is built on,

00:36:43.700 --> 00:36:47.050
both on the web side
and the streaming side,

00:36:47.050 --> 00:36:51.300
to monitor the entire Internet on
a real-time basis and then

00:36:51.300 --> 00:36:55.400
make mapping decisions every
10 seconds that determine,

00:36:55.400 --> 00:37:02.780
among other things, for a given end user,
when he wants some piece of content,

00:37:02.790 --> 00:37:05.000
be it a stream or web content,
which edge server he should talk to.

00:37:05.570 --> 00:37:08.880
Okay, and we choose an edge server
that is lightly loaded,

00:37:08.920 --> 00:37:14.060
that's likely to have the content,
that's up, that's always a good thing,

00:37:14.240 --> 00:37:21.030
and where the path between the end
user and that server is uncongested,

00:37:21.090 --> 00:37:23.730
can deliver high quality.

00:37:23.840 --> 00:37:28.860
Because the goal is to
deliver that content,

00:37:28.860 --> 00:37:31.610
whatever sort it is,
to that user quickly and reliably.

00:37:32.070 --> 00:37:37.450
A couple other things I want to
mention here about what we do to ensure

00:37:37.450 --> 00:37:44.100
quality and good performance and so on,
we do something that

00:37:44.100 --> 00:37:46.140
we call pre-bursting.

00:37:46.140 --> 00:37:50.650
So you could imagine that when a user
connects to an Edge server and says,

00:37:50.740 --> 00:37:54.470
"I want the Steve Jobs keynote," well,
a subscription goes from there

00:37:54.470 --> 00:37:57.640
to one of the reflector nodes,
or perhaps more than one,

00:37:57.640 --> 00:38:01.200
and if those reflector nodes
are not currently getting the

00:38:01.200 --> 00:38:06.600
stream from the entry point,
they'll subscribe from there.

00:38:06.600 --> 00:38:08.980
And then the stream will start to flow.

00:38:10.700 --> 00:38:13.350
In a normal situation,
you might simply start to

00:38:13.350 --> 00:38:17.360
send the stream at the speed,
at the data rate of that stream.

00:38:17.360 --> 00:38:20.110
So if it's 300K,
you start sending packets

00:38:20.180 --> 00:38:23.460
paced at approximately 300K,
or whatever the actual

00:38:23.510 --> 00:38:25.120
bit rate of the stream is.

00:38:25.860 --> 00:38:29.630
Which means there's going to be
significant latency until the user

00:38:29.630 --> 00:38:34.400
has built up a buffer in the player
before it actually starts playing.

00:38:34.570 --> 00:38:38.060
What we do across the network,
which fits well with the instant on

00:38:38.060 --> 00:38:42.690
feature of the current QuickTime system,
is we do pre-bursting from the entry

00:38:42.790 --> 00:38:47.100
point through the set reflectors
all the way to the edge machine.

00:38:47.210 --> 00:38:50.540
So when an edge machine or a set
reflector subscribes to a stream,

00:38:50.540 --> 00:38:55.200
we will send the data at eight times
the actual bit rate to build up a

00:38:55.200 --> 00:38:58.300
buffer very quickly close to the user.

00:38:58.360 --> 00:39:02.790
And then the player itself does
that from the edge server to

00:39:02.950 --> 00:39:07.900
what will pull as fast as it can
to fill up the buffer initially,

00:39:07.950 --> 00:39:12.580
and then from there will pull at
more or less the normal bit rate.

00:39:13.120 --> 00:39:18.590
The other thing that we do to try to
ensure high quality is you might think

00:39:18.660 --> 00:39:24.000
that the best thing to do in terms of
mapping a user is simply to map a user

00:39:24.000 --> 00:39:29.700
to the nearest edge server that has good
connectivity between it and the user.

00:39:29.800 --> 00:39:32.730
In fact, because of the characteristics
of the stream servers that

00:39:32.730 --> 00:39:37.050
run on the edge servers,
and the dynamics of building

00:39:37.160 --> 00:39:40.140
a system like this and running
many different streams,

00:39:40.140 --> 00:39:43.540
many different kinds of content over it,
you can get much better quality

00:39:43.690 --> 00:39:47.920
by being much more judicious about
what streams you serve from where.

00:39:47.940 --> 00:39:53.320
We use something we call block maps,
which rather than essentially

00:39:53.320 --> 00:39:58.780
spreading the load for a given stream,
in some sense, over the whole network,

00:39:58.820 --> 00:40:02.650
we will restrict the regions
that it is mapped to.

00:40:02.680 --> 00:40:04.930
When I say region, I mean data center.

00:40:04.930 --> 00:40:08.100
We will restrict it somewhat so that,
for example,

00:40:08.100 --> 00:40:12.360
we would rather serve the same stream
out of a small number of servers

00:40:12.360 --> 00:40:17.920
and some other stream out of other
servers than just mix them all at once.

00:40:17.920 --> 00:40:19.520
over the place.

00:40:19.520 --> 00:40:20.720
We can get better quality that way.

00:40:22.140 --> 00:40:25.750
We then monitor the whole
system on a regular basis,

00:40:25.750 --> 00:40:29.890
I mean continuously,
certainly for an event like this,

00:40:30.020 --> 00:40:35.020
but in general we monitor the
whole system to watch a number of

00:40:35.020 --> 00:40:38.850
different metrics on the performance
and quality of the stream.

00:40:39.000 --> 00:40:42.230
For example,
what's the actual bit rate in terms

00:40:42.230 --> 00:40:44.990
of packets that are delivered on time?

00:40:45.000 --> 00:40:47.920
Because you can deliver a packet,
but if it's too late and

00:40:47.920 --> 00:40:51.000
the player throws it away
because it's arrived too late,

00:40:51.000 --> 00:40:51.990
then it's not useful.

00:40:52.010 --> 00:40:54.390
The number of packets,
the percentage of packets

00:40:54.390 --> 00:40:56.880
that are delivered on time
to the player in the end.

00:40:56.970 --> 00:40:57.970
That's an important metric.

00:40:58.010 --> 00:41:02.190
How much thinning takes place
between the server and the end

00:41:02.590 --> 00:41:05.000
user is another important metric.

00:41:05.060 --> 00:41:07.980
Because the user might
have a 300k stream,

00:41:07.980 --> 00:41:12.960
but there might be enough congestion
on that path that it starts

00:41:13.030 --> 00:41:18.000
thinning and actually delivering
a much lower bit rate stream.

00:41:18.000 --> 00:41:22.000
How long does it take to connect
and get started is another metric.

00:41:22.010 --> 00:41:25.430
You want people when they connect
to not sit there for 20 or 30

00:41:25.530 --> 00:41:30.000
seconds looking at something and
waiting for the stream to start.

00:41:30.000 --> 00:41:35.000
You want it to start in a few seconds,
3 or 4 or 5 seconds at most if possible.

00:41:35.000 --> 00:41:40.310
And then how much rebuffering and
other things like that take place

00:41:40.310 --> 00:41:43.000
during the playing of the stream.

00:41:43.100 --> 00:41:45.300
And all of the different
technologies that we put in place

00:41:45.380 --> 00:41:50.180
in the network are derived from,
really in part,

00:41:50.180 --> 00:41:56.290
measuring all of those metrics and trying
to understand when there are issues,

00:41:56.420 --> 00:42:00.120
when there are problems,
what's causing those, and then developing

00:42:00.180 --> 00:42:03.510
mechanisms like pre-bursting,
like block maps that will

00:42:03.510 --> 00:42:06.770
allow us to get much,
much better quality and continue

00:42:06.770 --> 00:42:10.000
to deliver a high quality
experience to the end user.

00:42:12.370 --> 00:42:16.610
Okay,
so let me talk about Apple and Akamai.

00:42:16.850 --> 00:42:22.290
Apple has been one of our major customers
since the early days of the company.

00:42:22.300 --> 00:42:24.900
The company actually started in 1998.

00:42:24.950 --> 00:42:30.660
Our first paying customer was early 1999,
and Apple has been a major customer.

00:42:30.660 --> 00:42:33.020
In fact, it was an investor in
the early days as well.

00:42:33.020 --> 00:42:38.530
Akamai is Apple's platform for
the QuickTime streaming network.

00:42:38.630 --> 00:42:44.070
We've done, since 1999,
over 1,000 live and on-demand events,

00:42:44.070 --> 00:42:47.980
and we've done nine live
Steve Jobs keynote addresses.

00:42:48.120 --> 00:42:51.350
Those have been, every time,
the biggest event to

00:42:51.350 --> 00:42:52.750
date on the Internet.

00:42:53.180 --> 00:42:55.020
Steve is the rock star of the Internet.

00:42:55.020 --> 00:42:57.750
I think there's not much
question about that.

00:42:58.350 --> 00:43:03.020
There are movie trailers,
Lord of the Rings, and many others.

00:43:03.020 --> 00:43:06.010
There's Tomb Raider and lots of others.

00:43:06.010 --> 00:43:10.010
You can go look and see what's on
the QuickTime streaming network.

00:43:10.010 --> 00:43:13.010
Those streams are coming over us.

00:43:13.020 --> 00:43:16.010
We also do a number of
other things for Apple.

00:43:16.010 --> 00:43:24.090
We are delivering the iTunes music store,
both the actual music downloads,

00:43:24.100 --> 00:43:27.020
so when you download a song,
that's coming from our network,

00:43:27.020 --> 00:43:29.900
but also a lot of the data and
control information that is

00:43:29.900 --> 00:43:32.020
used to make decisions about
playlists and other things.

00:43:32.020 --> 00:43:42.010
Movie trailer downloads,
software updates also come through us.

00:43:42.030 --> 00:43:46.240
We're providing web analytics services,
so reporting on what's happening

00:43:46.310 --> 00:43:49.700
on the site and what's happening in
different parts of the world to allow

00:43:49.820 --> 00:43:54.020
the marketing folks and other people
to make decisions about what to do.

00:43:54.180 --> 00:43:56.020
And then geolocation services.

00:43:56.020 --> 00:44:01.020
For example, for iTunes, there are,
I think, contractual obligations in

00:44:01.020 --> 00:44:01.020
terms of where you're going
to be able to get your music.

00:44:01.280 --> 00:44:07.020
Where in the world you have to be if you
want to actually download those songs.

00:44:07.020 --> 00:44:10.820
And we help ensure that those
contractual obligations are met.

00:44:13.730 --> 00:44:16.640
So let me talk a little bit
about the keynote itself,

00:44:16.640 --> 00:44:18.830
what happens both before
the event and day of,

00:44:18.880 --> 00:44:21.890
and I'll say a little bit
about afterwards as well.

00:44:22.280 --> 00:44:28.290
We ourselves, for an event of this scale,
this is not just a normal event.

00:44:28.380 --> 00:44:32.270
As I said, most people,
most of our customers don't do events

00:44:32.270 --> 00:44:35.880
on anything approaching this scale,
and they run events all the

00:44:35.880 --> 00:44:40.370
time and don't even let us know
in advance that it's happening.

00:44:40.420 --> 00:44:42.770
Large events, a gigabit,
several gigabits,

00:44:42.770 --> 00:44:46.720
or in the case of Steve, 16 gigabits,
we need to know about because

00:44:46.720 --> 00:44:49.040
we don't have infinite capacity.

00:44:49.040 --> 00:44:53.580
So, well in advance of the event,
we internally develop a project plan.

00:44:53.700 --> 00:44:57.580
Who are all the people that are going
to be involved from engineering to

00:44:57.580 --> 00:45:02.650
various support groups to the network
groups to make sure that we have

00:45:02.800 --> 00:45:08.980
adequate capacity in all the different
regions of the world that it's needed.

00:45:08.980 --> 00:45:12.690
Do capacity planning,
understand what capacity we

00:45:12.730 --> 00:45:17.900
have on the network for serving
QuickTime and where is it.

00:45:17.900 --> 00:45:20.480
Furthermore, what else is going to be
happening on the network?

00:45:20.480 --> 00:45:21.360
Some of that is hard to predict.

00:45:21.360 --> 00:45:24.880
Two months in advance,
you can't always say when we

00:45:24.880 --> 00:45:29.480
might be at war or when some
major event might happen.

00:45:29.480 --> 00:45:32.950
So, we obviously have a certain amount
of headroom in terms of capacity

00:45:33.370 --> 00:45:38.140
in our network and are prepared to
deal with fairly significant bursts.

00:45:38.330 --> 00:45:42.860
But 16 gigabits for Steve and another
who knows what for a war and so

00:45:42.970 --> 00:45:45.540
on can certainly add some stress.

00:45:45.540 --> 00:45:48.910
And so, we want to do as much
planning for that as we can.

00:45:49.730 --> 00:45:53.420
We then talk about and talk with
the Apple folks about what is

00:45:53.420 --> 00:45:56.570
needed in terms of velvet rope.

00:45:56.670 --> 00:46:00.700
And the idea of a velvet rope
is basically to limit the total

00:46:00.700 --> 00:46:03.090
amount of bandwidth that is used.

00:46:03.100 --> 00:46:05.600
Think of it as a velvet rope
around some fancy event,

00:46:05.600 --> 00:46:09.460
and you've got to be inside
the rope to take part.

00:46:09.600 --> 00:46:13.580
Now, there are many ways to
think about velvet rope.

00:46:13.670 --> 00:46:16.600
You could imagine simply saying,
"I'll let people come in,

00:46:16.600 --> 00:46:19.760
grab whatever bitrate
stream they can get,

00:46:19.970 --> 00:46:23.000
and then when I hit my limit,
I turn them off."

00:46:24.360 --> 00:46:26.800
Or you could say, well, I'll just,
you know,

00:46:26.820 --> 00:46:29.770
I won't provide a very high bitrate
stream because that way I can let

00:46:29.820 --> 00:46:31.100
as many people in as possible.

00:46:31.100 --> 00:46:36.480
But if you don't know how many people
are going to come and you've provisioned

00:46:36.480 --> 00:46:39.010
for a certain amount of capacity,
what you'd like to do is let

00:46:39.040 --> 00:46:41.300
as many people in as you can,
or this is certainly

00:46:41.300 --> 00:46:43.880
what Apple wants to do,
let as many people in as you can

00:46:43.970 --> 00:46:48.000
and give as many of them as possible
the highest bitrate they can get.

00:46:48.060 --> 00:46:52.710
So the idea is that we start out with...

00:46:52.900 --> 00:46:55.800
All of the bit rates being
available to everyone,

00:46:55.870 --> 00:46:59.120
and we watch the ramp,
and then there's a decision point

00:46:59.120 --> 00:47:04.370
when we hit certain thresholds to
decide to clamp down and not provide

00:47:04.760 --> 00:47:08.940
access to the higher bit rates,
depending on how much of the

00:47:08.940 --> 00:47:10.910
available capacity is left.

00:47:11.040 --> 00:47:17.040
The interesting thing is that that
decision is made not just globally.

00:47:17.040 --> 00:47:22.800
You don't want to say, "Oh, you know,
we've got 16 gig globally, and gee,

00:47:22.800 --> 00:47:25.500
we're using 12 gig,
so nobody should have access

00:47:25.580 --> 00:47:29.260
to the 300k stream," but rather
it's done on a regional basis.

00:47:29.490 --> 00:47:32.360
And the reason for that is
that we want to serve people

00:47:32.360 --> 00:47:33.960
to give them good quality.

00:47:34.150 --> 00:47:36.210
We want to serve them
from reasonably close by.

00:47:36.280 --> 00:47:40.280
So if we just made a decision globally,
then it might be that, in fact,

00:47:40.280 --> 00:47:40.280
we're not going to be able to do that.

00:47:40.280 --> 00:47:45.080
We want to make sure that, in fact,
the servers and the network links in

00:47:45.080 --> 00:47:48.150
Australia from our servers are maxed out.

00:47:48.300 --> 00:49:23.000
[Transcript missing]

00:49:23.930 --> 00:49:26.930
So we need to determine the
need for Velvet Rope and what

00:49:26.930 --> 00:49:30.040
the thresholds are going to be.

00:49:30.040 --> 00:49:33.450
We need to provision
First Point to do the load

00:49:33.450 --> 00:49:35.300
balancing and the load management.

00:49:35.300 --> 00:49:39.530
And then for an event like this
that is so high profile and

00:49:39.530 --> 00:49:43.740
so critical and where failure,
I mean, as I said, if Steve's unhappy,

00:49:43.820 --> 00:49:45.200
you can imagine what happens.

00:49:45.200 --> 00:49:46.930
You don't want to fail.

00:49:47.180 --> 00:49:48.100
Okay?

00:49:48.100 --> 00:49:49.100
It's just not acceptable.

00:49:49.100 --> 00:49:51.500
You don't want to fail even for,
you know, 30 seconds.

00:49:51.700 --> 00:49:54.590
Having things drop out for 30
seconds would be a disaster.

00:49:54.690 --> 00:49:55.500
Okay?

00:49:55.500 --> 00:49:57.980
And not to say that we want
to fail for other customers.

00:49:57.980 --> 00:49:59.120
Obviously, we don't.

00:49:59.200 --> 00:50:05.500
But as I said, for an event like this,
Apple cares enough about

00:50:05.500 --> 00:50:09.020
what happens with the event,
the profile is sufficiently high

00:50:09.020 --> 00:50:13.270
that they are willing to invest
in a level of testing and a level

00:50:13.270 --> 00:50:15.100
of attention that's paid to it.

00:50:15.100 --> 00:50:20.380
To just make sure that any contingency
that comes up will be covered.

00:50:20.380 --> 00:50:24.620
So there's a lot of testing that goes
on before the event end to end to

00:50:24.920 --> 00:50:27.700
make sure we can capture a signal,
send it to the entry points,

00:50:27.700 --> 00:50:31.170
fail over the entry points,
send it through the network

00:50:31.220 --> 00:50:34.950
to the edge servers,
and then get it with high quality

00:50:34.950 --> 00:50:37.070
to users around the world.

00:50:37.920 --> 00:50:41.580
The day of the event, we provide,
first of all,

00:50:41.690 --> 00:50:44.170
automated network management.

00:50:44.280 --> 00:50:49.300
Our entire network, those 15,000 servers,
the entry points, the reflectors,

00:50:49.340 --> 00:50:52.940
the edge servers that actually
run the QuickTime server,

00:50:52.970 --> 00:50:55.520
is remarkably self-managing.

00:50:55.580 --> 00:51:01.770
Our NOC, on a normal basis,
has on the order of four people sitting

00:51:01.770 --> 00:51:01.770
in it watching the whole network.

00:51:02.070 --> 00:51:07.160
And that's because we have a very
extensive automated system for monitoring

00:51:07.630 --> 00:51:11.160
lots of different aspects of what's
happening on every machine and what's

00:51:11.160 --> 00:51:16.080
happening on the network paths between
machines and between them and end users.

00:51:16.140 --> 00:51:19.960
And we have automated failover
at a number of different levels

00:51:19.960 --> 00:51:24.480
of the system so that when a
machine fails or a region fails,

00:51:24.500 --> 00:51:28.840
a data center goes offline
or connectivity is disrupted,

00:51:28.880 --> 00:51:33.870
there's automatic failover and remapping
so that very few users will see any

00:51:33.920 --> 00:51:36.900
impact at all from that kind of event.

00:51:36.950 --> 00:51:41.150
But there are times when something
goes wrong that needs attention,

00:51:41.160 --> 00:51:45.710
and so we do have people who
are actually actively watching.

00:51:45.820 --> 00:51:51.390
So our NOC monitors on a daily
basis constantly the whole network,

00:51:51.500 --> 00:51:54.260
and for an event like this of this
magnitude and this importance,

00:51:54.290 --> 00:51:59.760
then they are also specifically watching
what's going on with this event.

00:51:59.760 --> 00:52:02.760
And in addition,
we set up a situation room

00:52:02.790 --> 00:52:06.680
for an event like this,
where the team that has been assembled

00:52:07.080 --> 00:52:11.240
to put together the event and run it
is in that room watching the network,

00:52:11.240 --> 00:52:13.600
watching what's going on,
and making sure that if there

00:52:13.700 --> 00:52:17.520
are any anomalies that crop up,
that they get fixed very, very quickly,

00:52:17.550 --> 00:52:21.410
often before any end
user notices an impact.

00:52:23.420 --> 00:52:26.110
So what do you get from all this?

00:52:26.270 --> 00:52:30.140
Well, in January of this year,

00:52:30.490 --> 00:52:33.700
There's a lot of expertise that we have
for delivering these kinds of events.

00:52:33.700 --> 00:52:36.940
We provided 100% availability.

00:52:36.940 --> 00:52:40.580
We provided over 12 gigabits
per second peak delivery to

00:52:40.690 --> 00:52:43.760
almost 80,000 concurrent users.

00:52:43.760 --> 00:52:49.830
I think the total number of users during
the event was on the order of 100,000.

00:52:49.830 --> 00:52:54.790
At the same time,
maintaining a high-level service,

00:52:54.790 --> 00:52:57.440
high quality to all our
other customers as well.

00:52:57.440 --> 00:53:02.060
And then in addition,
we provide real-time and, after the fact,

00:53:02.060 --> 00:53:06.290
historical reporting on what's
going on with the event and on

00:53:06.290 --> 00:53:08.730
the traffic that's being served.