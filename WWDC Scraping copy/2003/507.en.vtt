WEBVTT

00:00:19.990 --> 00:00:21.140
Good morning.

00:00:21.190 --> 00:00:22.340
My name is Mark Tozer-Vilches.

00:00:22.380 --> 00:00:27.180
I'm the desktop hardware evangelist at
Apple and Worldwide Developer Relations.

00:00:27.240 --> 00:00:31.800
This is session 507,
Mac OS X High Performance Libraries.

00:00:32.160 --> 00:00:34.660
The Vector Numerics Group,
if you're not familiar with them

00:00:34.660 --> 00:00:38.010
already through the different sessions
this year as well as previous years,

00:00:38.020 --> 00:00:41.450
has been doing a lot of work in
optimizing performance in the libraries

00:00:41.460 --> 00:00:44.600
that we ship in our operating system,
as well as working with

00:00:44.790 --> 00:00:48.600
several of our developers and
our own application groups.

00:00:48.600 --> 00:00:51.870
If you've used iTunes,
a lot of the performance that

00:00:51.870 --> 00:00:56.860
you see that we achieve in that
application is a result of this group.

00:00:56.940 --> 00:00:59.660
Today we offer some more
and new information.

00:00:59.720 --> 00:01:03.870
Yesterday the group also introduced
a new library for V-image,

00:01:03.870 --> 00:01:06.350
vector image library processing.

00:01:06.440 --> 00:01:09.160
Today I'd like to introduce
Steve Peters who will go

00:01:09.160 --> 00:01:13.560
over these new libraries,
particularly understand how the 970,

00:01:13.640 --> 00:01:16.990
the G5 processor takes
advantage of these libraries.

00:01:16.990 --> 00:01:18.370
Thank you.

00:01:22.720 --> 00:01:23.270
Thank you, Mark.

00:01:23.410 --> 00:01:23.990
Welcome.

00:01:24.060 --> 00:01:25.360
Good morning.

00:01:25.370 --> 00:01:28.240
It's a wonderful time to be doing
mathematics on the Macintosh,

00:01:28.250 --> 00:01:32.100
and I'd like to tell you a
little bit about why I think so.

00:01:32.100 --> 00:01:35.560
One more time.

00:01:38.720 --> 00:01:41.650
So by way of introduction,
I wanted to lay out what I think

00:01:41.710 --> 00:01:45.900
our charter is in the numerics
and vectorization group.

00:01:46.010 --> 00:01:49.180
First of all,
we want your apps to achieve

00:01:49.460 --> 00:01:52.740
optimum floating point
performance on our platform.

00:01:52.780 --> 00:01:56.740
We're in a great position
to help you with that today.

00:01:56.990 --> 00:01:59.460
We also want to make sure
that your apps deliver robust,

00:01:59.730 --> 00:02:01.900
high-quality numerics.

00:02:01.970 --> 00:02:06.700
It's been a tradition at Apple since
the sane days back on the 68OXO to

00:02:06.700 --> 00:02:13.040
deliver really the best in numerical
algorithms and the numerical science.

00:02:13.090 --> 00:02:19.870
We continue to keep the bar high
and aim to set it even higher.

00:02:20.110 --> 00:02:23.240
We want your apps to readily
port to industry standard

00:02:23.540 --> 00:02:27.990
APIs in our Mac OS X frameworks.

00:02:28.150 --> 00:02:29.470
This is a leverage point.

00:02:29.630 --> 00:02:35.160
We take advantage of
open source software in

00:02:36.130 --> 00:02:40.780
building these frameworks,
and we pay a great deal of attention

00:02:40.780 --> 00:02:42.540
to optimization in these frameworks.

00:02:42.540 --> 00:02:46.000
And finally,
we want your apps to enjoy easy access

00:02:46.000 --> 00:02:49.860
to the Apple value-added features
such as Altevec in our platform,

00:02:49.860 --> 00:02:54.160
and where we can transparently
give you access to those features,

00:02:54.160 --> 00:02:56.870
we do, and you'll see a little
later how that works out.

00:02:59.210 --> 00:03:00.400
So what you'll learn today.

00:03:00.560 --> 00:03:03.900
We'd like to inform you about
floating point performance,

00:03:03.900 --> 00:03:10.100
the floating point performance work
we're delivering for G5 in Panther.

00:03:10.200 --> 00:03:12.970
We'll show you how to leverage
that work in your development.

00:03:12.970 --> 00:03:17.440
We'll walk you through a regimen
that I've been using to squeeze

00:03:17.440 --> 00:03:21.080
out the last bit of floating point
performance in the libraries.

00:03:21.170 --> 00:03:23.080
You might find that helpful
in your applications.

00:03:23.090 --> 00:03:26.170
And we'll recommend
some learning resources.

00:03:29.840 --> 00:03:32.790
So this is where in many
presentations you'll see the

00:03:32.790 --> 00:03:38.810
obligatory layer cake diagram and
sort of one piece cut out and put in.

00:03:38.810 --> 00:03:44.440
It's the technology framework assessment
and being a bit of a contrarian I decided

00:03:44.440 --> 00:03:50.170
well you know we actually touch in
more than one place on that layer cake.

00:03:50.890 --> 00:03:56.160
Math performance depends on the silicon,
the floating point cores,

00:03:56.220 --> 00:03:59.480
it depends on the frameworks
that we deliver to you,

00:03:59.560 --> 00:04:04.500
and it relies ever more on
the performance tools that

00:04:04.500 --> 00:04:05.850
help you optimize your apps.

00:04:05.930 --> 00:04:08.650
And that's in the Chud toolkit.

00:04:13.100 --> 00:04:23.560
So let's review what's been done over
the last year since we last met at WWDC.

00:04:24.100 --> 00:04:27.020
What do we deliver in Jaguar?

00:04:27.020 --> 00:04:31.010
Libm, our standard C math library,

00:04:31.640 --> 00:04:34.470
Libm is centered on the
following design principles.

00:04:34.580 --> 00:04:37.630
First,
that we should be standards conforming.

00:04:37.880 --> 00:04:43.330
You should see the C99 floating point
function entry points on our platform

00:04:43.330 --> 00:04:44.610
just the way you do on everybody else's.

00:04:44.870 --> 00:04:48.580
And that's certainly true
for doubles in Jaguar.

00:04:50.270 --> 00:04:53.660
Our algorithms are
numerically robust in Libm.

00:04:53.670 --> 00:04:57.840
It's really the central point that
our core approximations should

00:04:57.980 --> 00:05:00.180
deliver correctly rounded results.

00:05:00.330 --> 00:05:08.170
and the other engineers provide
insight into the features

00:05:08.170 --> 00:05:08.170
of the Mac OS X library.

00:05:08.600 --> 00:05:10.650
We bring you best of breed algorithms.

00:05:10.840 --> 00:05:15.140
These are modern algorithms,
nothing from numerical

00:05:15.140 --> 00:05:22.640
recipes here folks,
and matched to the hardware.

00:05:22.750 --> 00:05:25.100
How do you code to this
stuff if you're writing in C?

00:05:25.400 --> 00:05:27.000
Couldn't be simpler.

00:05:27.000 --> 00:05:29.170
Include math.h.

00:05:29.490 --> 00:05:32.100
If you have special needs and
you know who you are to touch

00:05:32.100 --> 00:05:35.520
the floating point environment,
you can include FN.h and

00:05:35.520 --> 00:05:37.400
otherwise compile and go.

00:05:37.400 --> 00:05:41.720
Libm is part of the Lib system
umbrella that's automatically

00:05:41.720 --> 00:05:46.100
linked on the CC line,
so you need to say no more.

00:05:50.210 --> 00:05:53.120
So from time to time,
we get questions and

00:05:53.120 --> 00:05:55.140
phone calls that say,
gee,

00:05:55.210 --> 00:06:00.270
is it possible for me to go even faster
than what you've delivered in Libm?

00:06:01.490 --> 00:06:06.760
The answer is yes,
but you have to make some compromises.

00:06:06.760 --> 00:06:09.970
And it's been our position
that we're not making those

00:06:09.970 --> 00:06:13.380
compromises in our deliverable Libm,
but we can clue you in

00:06:13.380 --> 00:06:15.410
about what you need to do.

00:06:16.650 --> 00:06:19.040
Libm emphasizes numerical robustness.

00:06:19.140 --> 00:06:23.980
If you're willing to compromise on that,
you might be able to go a little faster.

00:06:24.230 --> 00:06:29.700
Libm uses the standard C99 APIs,
which turn out to data starve our FPUs.

00:06:29.970 --> 00:06:34.330
Typically,
a call like "sign" presents the

00:06:34.340 --> 00:06:38.960
math library with a single argument,
and you're asked to -- the contract

00:06:38.970 --> 00:06:40.970
is to return a single return value.

00:06:41.270 --> 00:06:45.100
That ends up just leaving bubbles all
the way through the floating-point pipes.

00:06:45.130 --> 00:06:48.450
If you can provide more
data through an API,

00:06:48.450 --> 00:06:56.100
it's quite possible to produce results
faster and furiously out the other end.

00:06:56.100 --> 00:06:59.480
Libm is obliged to handle all
the special cases -- nans,

00:06:59.480 --> 00:07:02.100
infinities,
plus and minus zeros as needed.

00:07:02.310 --> 00:07:06.170
And Libm is obliged to preserve, protect,
and defend the state of

00:07:06.170 --> 00:07:08.100
the floating-point flags.

00:07:08.240 --> 00:07:12.100
So there's a fair amount of detail
work that goes on in the library.

00:07:12.100 --> 00:07:14.140
If you're willing to
compromise on that stuff,

00:07:14.150 --> 00:07:15.090
you can go faster.

00:07:15.100 --> 00:07:19.100
So here's a homely little example.

00:07:19.100 --> 00:07:22.340
A very respected developer
came to us and said,

00:07:22.340 --> 00:07:26.040
"Hey,
your hypotenuse function is way slow.

00:07:26.040 --> 00:07:31.680
I can write a one-liner that goes
much faster than what you guys do."

00:07:31.870 --> 00:07:34.920
So here you see the
Pythagorean algorithm,

00:07:34.920 --> 00:07:38.880
a little pithy Pythagorean algorithm,
return the square root of

00:07:38.880 --> 00:07:40.570
X squared plus Y squared.

00:07:40.570 --> 00:07:45.820
And sure enough,
you run this function on a triangle

00:07:46.010 --> 00:07:49.610
whose one leg is length three,
the other leg is length four,

00:07:49.610 --> 00:07:50.980
and the hypotenuse is five.

00:07:51.350 --> 00:07:54.540
Well, let's sort of scale this thing up.

00:07:54.540 --> 00:07:58.020
I don't know,
is this the size of the universe

00:07:58.220 --> 00:08:00.590
as we understand it these days?

00:08:00.590 --> 00:08:00.590
In any case,

00:08:00.920 --> 00:08:04.900
The result of applying this
function to these arguments is,

00:08:04.900 --> 00:08:07.560
hmm, not five times e to the 160th,
but infinity.

00:08:07.560 --> 00:08:08.000
Ooh.

00:08:08.000 --> 00:08:12.180
So there's some
intermediate overflow here,

00:08:12.180 --> 00:08:13.080
right?

00:08:13.080 --> 00:08:19.710
And this developer made some compromises,
probably over the range of arguments that

00:08:19.840 --> 00:08:22.460
he was interested in using this function.

00:08:22.460 --> 00:08:23.930
Worked just fine.

00:08:23.940 --> 00:08:28.210
But Libm takes on all comers
and we're obliged by standards

00:08:28.210 --> 00:08:30.560
to do something different.

00:08:30.800 --> 00:08:35.480
And here's just for...

00:08:37.600 --> 00:08:42.970
This is like my clicker at home
after the kids have gotten to it.

00:08:42.970 --> 00:08:42.970
There we go.

00:08:43.160 --> 00:08:45.880
The purpose of this is for gestalt,
right?

00:08:46.010 --> 00:08:50.200
There's a lot more going on
here than that simple one-liner.

00:08:50.320 --> 00:08:55.140
Those with sharp eyes, unlike mine,
may notice, "Hmm, let's see.

00:08:55.230 --> 00:08:58.010
"There's some comparisons up front."

00:08:59.600 --> 00:09:03.160
a division halfway down,
finally that square root,

00:09:03.170 --> 00:09:05.750
and a rescaling,
and some work on the environment.

00:09:05.880 --> 00:09:09.620
So those are the kind of details
that LibM and the lengths that

00:09:09.660 --> 00:09:13.630
LibM goes to to meet standards
and robustness requirements.

00:09:13.640 --> 00:09:16.230
If you can compromise on those,
you might be able to go faster.

00:09:16.380 --> 00:09:19.500
So that's the end of a long aside.

00:09:21.600 --> 00:09:24.500
Delivered in Jaguar are vecLib.

00:09:24.500 --> 00:09:27.690
We view vecLib as a one-stop
shopping place for math performance.

00:09:27.700 --> 00:09:32.820
We deliver digital signal processing,
1 and 2D, real and complex FFTs,

00:09:33.000 --> 00:09:35.280
the BLAS, level 1, 2, and 3.

00:09:35.280 --> 00:09:39.400
These are based on the
open-source software Atlas,

00:09:39.400 --> 00:09:42.600
automatically tuned
linear algebra system.

00:09:42.600 --> 00:09:45.020
It actually is a code generation system.

00:09:45.020 --> 00:09:50.260
Back at Cupertino, before every release,
I grind through Atlas.

00:09:50.260 --> 00:09:53.900
It generates code for linear
algebra matched to the processor.

00:09:53.960 --> 00:09:56.320
We get extraordinary
performance from this.

00:09:56.320 --> 00:10:01.050
And for certain of the entry points,
it's actually SMP aware,

00:10:01.050 --> 00:10:04.660
so you can go multiprocessor
transparently.

00:10:04.660 --> 00:10:07.230
If the processor's there,
it goes ahead and uses it.

00:10:07.400 --> 00:10:09.300
It's a wonderful, wonderful thing.

00:10:09.300 --> 00:10:15.000
We also delivered LAPAC for
solving linear systems.

00:10:15.020 --> 00:10:19.610
and eigenvalue problems, again,
open source software.

00:10:21.400 --> 00:10:29.070
We delivered tuned 4x4, 8x8, 16x16,
and 32x32 matrix multiply, matrix matrix,

00:10:29.130 --> 00:10:30.600
matrix vector, vector matrix.

00:10:30.610 --> 00:10:36.770
These are for folks who know that
they're dealing with very specific

00:10:36.810 --> 00:10:40.700
sizes and want to go really fast.

00:10:40.700 --> 00:10:45.040
These are basically completely
unrolled loops and on the single

00:10:45.340 --> 00:10:48.840
precision side hit AlteVec very,
very hard.

00:10:49.460 --> 00:10:52.380
And so that's the next point,
is wherever AlteVec is

00:10:52.390 --> 00:10:57.580
appropriate and available to use,
vecLib tries to go to AlteVec.

00:10:59.880 --> 00:11:05.170
How do you code to vecLib using C?

00:11:05.590 --> 00:11:09.130
Include vecLib vecLib.h,
that's the framework header.

00:11:09.190 --> 00:11:12.150
You might want to take a peek
in there and see all the other

00:11:12.960 --> 00:11:17.920
header files that get included,
cblas.h, clapac.h, vbignum, et cetera.

00:11:18.420 --> 00:11:24.210
From there, you can fan out and find the
interfaces that you'd like to code to.

00:11:24.220 --> 00:11:30.520
And then just add framework
vecLib to the CC line.

00:11:31.150 --> 00:11:34.240
Well, we announced this week that there's
an umbrella framework called

00:11:34.240 --> 00:11:38.260
Accelerate that will collect
all of our high-performance math.

00:11:38.260 --> 00:11:41.670
And so in Panther,
you could just as well,

00:11:41.670 --> 00:11:45.600
and it's probably the
right migration path,

00:11:45.650 --> 00:11:47.640
link against framework Accelerate.

00:11:47.690 --> 00:11:49.910
You'll get the same stuff.

00:11:58.300 --> 00:12:00.830
So again, that same question comes
up every once in a while.

00:12:00.940 --> 00:12:05.900
Can I go faster than the subroutine
that I'm calling in vecLib?

00:12:05.940 --> 00:12:09.190
Well, if you've chosen the
appropriate entry point,

00:12:09.280 --> 00:12:11.440
we think the answer is no, probably not.

00:12:11.460 --> 00:12:17.910
We've squeezed these things pretty
hard and we'll continue to do so.

00:12:20.140 --> 00:12:21.820
So let's move to Panther.

00:12:21.940 --> 00:12:23.840
What's new in Panther for Libm?

00:12:23.940 --> 00:12:26.860
Well, clearly, we've tuned for 970.

00:12:26.970 --> 00:12:33.790
We've opened up all the core algorithms,
recast them to exploit the two FPUs,

00:12:33.920 --> 00:12:39.390
the two LSUs,
paid very careful attention to

00:12:39.390 --> 00:12:43.450
the way the instructions are
issued to the machine so that

00:12:43.450 --> 00:12:43.450
we achieve maximum parallelism.

00:12:44.490 --> 00:12:46.640
and we watch out for load store issues.

00:12:46.640 --> 00:12:49.570
You'll see one of those
come up a little later.

00:12:49.570 --> 00:12:53.080
And at long last we have
hardware square root.

00:12:53.150 --> 00:12:57.320
This should be the end of

00:12:57.600 --> 00:13:16.100
[Transcript missing]

00:13:16.920 --> 00:13:19.640
There's also an opportunity
to inline Square Root.

00:13:19.640 --> 00:13:21.880
There are compiler flags now.

00:13:21.970 --> 00:13:24.540
If you're compiling and
you know you're on G5,

00:13:24.630 --> 00:13:27.220
you can inline the Square Root to great,
great advantage.

00:13:27.220 --> 00:13:31.190
So it turns out having
done all this work,

00:13:32.680 --> 00:13:35.390
We went back and said, well,
now how's this going to

00:13:35.390 --> 00:13:37.630
go on the older models,
the G4s?

00:13:37.670 --> 00:13:39.840
Are we going to have to maintain
two copies of the library?

00:13:39.840 --> 00:13:40.740
It turns out not.

00:13:40.740 --> 00:13:45.100
At least for this code,
doing these kinds of things to tune

00:13:45.230 --> 00:13:47.970
for 970 also helped us a little bit.

00:13:48.020 --> 00:13:50.800
A few percent, not a whole lot,
but we're faster these

00:13:50.800 --> 00:13:52.060
days on G4 as well.

00:13:52.460 --> 00:13:53.920
Your mileage may vary.

00:13:53.940 --> 00:13:59.870
It's worth experimenting before you
contemplate having multiple modules,

00:13:59.870 --> 00:14:00.630
multiple plug-ins.

00:14:00.630 --> 00:14:05.500
Go look and see if the things you
do for G5 don't also help you on G4.

00:14:11.580 --> 00:14:14.740
So I talked a little bit in
the last slide about careful

00:14:14.820 --> 00:14:19.840
construction of issue groups,
dispatch groups on the 970.

00:14:19.870 --> 00:14:25.640
I wanted to put up one slide and
simply remark that the issue is this.

00:14:25.670 --> 00:14:30.630
We've got a four-issue machine,
five if we've got a branch.

00:14:30.760 --> 00:14:35.460
Need to be very careful that
if we're gonna try to get two,

00:14:35.460 --> 00:14:40.490
let's say,
floating point multiply ads to issue on

00:14:41.330 --> 00:14:44.900
Simultaneously on the two CPUs,
we need to make sure that each

00:14:44.920 --> 00:14:51.300
instruction is fed into the issue
queue appropriate to the unit.

00:14:51.410 --> 00:14:56.260
And that means placing the
instruction appropriately as it forms,

00:14:56.280 --> 00:14:59.850
as the machine forms dispatch groups.

00:15:00.850 --> 00:15:06.750
Chud is a champ in showing us
where we need to line up our

00:15:06.750 --> 00:15:09.380
instructions a little bit differently.

00:15:09.530 --> 00:15:14.990
The timers, the SIMG4, SIMG5,
soon to come tools are

00:15:14.990 --> 00:15:17.690
also essential to do that.

00:15:17.700 --> 00:15:19.290
That's probably enough said about that.

00:15:19.360 --> 00:15:23.410
There's a trace later
on where we can return.

00:15:24.970 --> 00:15:29.560
So how did we do when we
opened up our core algorithms,

00:15:29.670 --> 00:15:34.920
did this work, and moved on to the G5?

00:15:35.040 --> 00:15:38.430
Here are your favorite Libm functions.

00:15:40.380 --> 00:15:43.930
Here are the cycles,
the number of machine cycles,

00:15:43.930 --> 00:15:49.560
invocations of these functions for
typical arguments took on the 7455,

00:15:49.660 --> 00:15:53.290
the G4, the high-end G4 model.

00:15:53.640 --> 00:15:57.140
Here's what we've done on the 970.

00:15:57.140 --> 00:16:01.880
Here's what our competition publishes.

00:16:03.030 --> 00:16:03.700
for the P4.

00:16:03.900 --> 00:16:10.220
And in general,
I think you'll see we're dominating the

00:16:10.220 --> 00:16:14.110
competition even through clock scaling.

00:16:14.200 --> 00:16:18.280
We're gonna go faster than
the fastest P4 you can buy.

00:16:18.310 --> 00:16:23.280
The only question is around square root,
where it's neck and neck,

00:16:23.440 --> 00:16:26.660
if you're able to inline,
you can get square root,

00:16:26.820 --> 00:16:29.690
the first square root
out in about 40 cycles,

00:16:29.940 --> 00:16:35.950
subsequent ones in 35 cycles,
and you can do that on two FPUs.

00:16:36.070 --> 00:16:39.740
If you need to call
the subroutine library,

00:16:39.740 --> 00:16:42.400
we pick up a little dynamic
linking overhead and can

00:16:42.400 --> 00:16:46.020
only go essentially on one,
it's one of those data starve situations,

00:16:46.020 --> 00:16:50.990
and we only go through one CPU,
one FPU in 52 cycles.

00:16:51.020 --> 00:16:54.260
So let's call that a draw maybe.

00:16:54.260 --> 00:16:58.550
Unless you can inline two at a time,
and then it's not so

00:16:58.570 --> 00:17:00.110
much of a draw anymore.

00:17:06.400 --> 00:17:11.030
So turning to the other big
component of our mathematics efforts,

00:17:11.060 --> 00:17:12.870
vecLib, what's new in Panther for vecLib?

00:17:13.110 --> 00:17:15.930
Double precision math performance, right?

00:17:15.980 --> 00:17:19.170
We've all been waiting for real
good double precision engines.

00:17:19.220 --> 00:17:20.920
We've now got two.

00:17:20.970 --> 00:17:25.750
And vecLib takes great
advantage of those now.

00:17:26.540 --> 00:17:30.280
Our DSP routines have been tuned
in double precision for the 970.

00:17:30.330 --> 00:17:33.620
Blas has been tuned and has really,
I think,

00:17:33.660 --> 00:17:38.430
just phenomenal performance on the 970.

00:17:38.950 --> 00:17:44.160
The scheme we use, for example,
for matrix multiply derives from Atlas.

00:17:44.310 --> 00:17:47.800
Given a very large matrix,
say a thousand by a thousand elements,

00:17:47.800 --> 00:17:50.720
it gets broken down into
much smaller pieces,

00:17:50.720 --> 00:17:52.620
64 by 64 in our case.

00:17:53.280 --> 00:17:56.750
That data gets moved into
cache for each of the operands,

00:17:56.890 --> 00:17:59.780
A times B, resulting in C.

00:17:59.880 --> 00:18:04.620
And then we cut loose with what's
called the matmul kernel on that guy,

00:18:04.620 --> 00:18:07.910
which takes the data,
which is lying in cache,

00:18:08.320 --> 00:18:14.200
runs it through the floating point units,
develops the output argument, and repeat.

00:18:14.200 --> 00:18:20.590
In that matmul kernel, we're able to,
oops.

00:18:20.720 --> 00:18:25.790
we're able to sustain about 3.4
floating ops per clock cycle.

00:18:25.840 --> 00:18:30.260
That's 84% of the peak
available on the machine,

00:18:30.260 --> 00:18:36.070
and it measures out just about
6.7 gigaflops at 2 gigahertz.

00:18:36.140 --> 00:18:40.570
When we look at the larger problem,
reassembling these 64

00:18:40.590 --> 00:18:43.590
by 64 block results,
the overhead of pulling all

00:18:43.730 --> 00:18:46.810
this stuff in and out of cache,
we get the DGEM performance

00:18:46.880 --> 00:18:49.580
on a 1,000 by 1,000 matrix.

00:18:50.360 --> 00:18:54.390
That runs from memory, from RAM,
1,000 by 1,000 matrix is

00:18:54.390 --> 00:18:57.650
going to sit in RAM after all,
at 2.4 flops per clock.

00:18:57.720 --> 00:19:02.120
That's about 60% of peak,
and measures out at about 4.8 gigaflops

00:19:02.210 --> 00:19:05.250
on a single 2.0 gigahertz processor.

00:19:07.640 --> 00:19:12.440
We've also added double precision 4x4,
8x8 special size matrices,

00:19:12.460 --> 00:19:14.960
all tuned for the 970.

00:19:14.960 --> 00:19:18.620
They achieve similar,
if not in some cases slightly better

00:19:18.620 --> 00:19:21.240
performance than the DGIMM numbers.

00:19:21.290 --> 00:19:23.600
We've completely
unrolled all those loops.

00:19:23.600 --> 00:19:28.070
The matrices are small enough they
sit in the cache all the time.

00:19:28.070 --> 00:19:28.070
It just goes like crazy.

00:19:28.200 --> 00:19:32.530
and finally, we've goosed up LAPAC a bit.

00:19:32.540 --> 00:19:35.210
If you're a fan of the
singular value decomposition,

00:19:35.210 --> 00:19:39.440
you'll be happy to know we
go really fast on SVDs now.

00:19:39.590 --> 00:19:45.500
And expect in Panther that
LAPAC will be thread safe.

00:19:47.100 --> 00:19:56.120
and David Hale are the founders
of the Mac OS X library.

00:19:57.260 --> 00:19:59.400
The results are in.

00:19:59.480 --> 00:20:01.400
FFTs are really fast on this box.

00:20:01.490 --> 00:20:05.970
These are some slides
giving performance of,

00:20:06.440 --> 00:20:09.540
you'll see 1 and 2D,
real and complex FFTs

00:20:09.540 --> 00:20:12.130
going to the vector unit.

00:20:12.740 --> 00:20:15.440
Smaller numbers here in
microseconds are faster,

00:20:15.660 --> 00:20:22.600
so we want to be underneath
the competition here and 1.

00:20:25.300 --> 00:20:39.900
[Transcript missing]

00:20:43.180 --> 00:20:44.790
How about our linear algebra performance?

00:20:44.830 --> 00:20:52.350
The industry standard is the LAPAC 1000,
which is a solution of a linear system,

00:20:52.350 --> 00:20:54.100
thousand by a thousand.

00:20:54.220 --> 00:20:56.800
We use the Atlas-based techniques.

00:20:57.000 --> 00:20:59.500
Those are blocked in the
way I told you before,

00:20:59.500 --> 00:21:05.040
moving 64 by 64 chunks in and out of the
cache and going like mad with the kernel.

00:21:05.150 --> 00:21:07.090
And let's just focus on this.

00:21:07.100 --> 00:21:11.320
The first number is the one
that probably most folks...

00:21:11.600 --> 00:21:14.950
Here's the pointer, are familiar with.

00:21:14.950 --> 00:21:19.770
So our DLP,
that's Double Precision LINPACK 1000,

00:21:19.770 --> 00:21:25.060
that's the Supercomputer Benchmark,
2.64 gigaflops.

00:21:25.060 --> 00:21:28.430
We think that's PC industry leading.

00:21:38.890 --> 00:21:44.780
So here's a pitch following on
from yesterday's Vimage session.

00:21:44.800 --> 00:21:49.800
There's a new umbrella framework
in Panther called Accelerate.

00:21:49.850 --> 00:21:54.800
One-stop shopping for all your
math and image processing needs.

00:21:54.800 --> 00:22:00.880
Minus framework Accelerate gets you all
our digital signal processing stuff,

00:22:00.890 --> 00:22:05.820
the linear algebra I've described,
a vector version of selected entry

00:22:05.890 --> 00:22:07.800
points in Libm at single precision.

00:22:08.950 --> 00:22:13.530
Some large number of arithmetic
support and the new Vimage

00:22:13.670 --> 00:22:15.720
image processing library.

00:22:16.110 --> 00:22:19.210
Moving forward,
code minus framework accelerates.

00:22:19.320 --> 00:22:20.170
It's the right thing to do.

00:22:20.270 --> 00:22:23.980
And additional math stuff
is going to end up in there.

00:22:23.980 --> 00:22:28.410
So it's a way of the future.

00:22:29.680 --> 00:22:32.810
Okay, so,
actually let me go back from it.

00:22:32.810 --> 00:22:36.380
And it's how we leverage, right?

00:22:36.770 --> 00:22:51.630
Use Accelerate and leverage Apple's work.

00:22:51.630 --> 00:22:51.630
Here's the point of
leverage for you folks.

00:22:51.630 --> 00:22:51.630
By selecting the right API,
you get the advantage of our

00:22:51.630 --> 00:22:51.630
efforts getting these things fast.

00:22:52.940 --> 00:22:57.800
So that's the segue into, well,
what if you've got code that

00:22:57.910 --> 00:23:01.660
you'd like to go faster,
really doesn't fit into

00:23:01.660 --> 00:23:06.270
any of the work we've done,
how do you go about doing that?

00:23:11.600 --> 00:23:20.940
and David Pryor are the founders
of the Mac OS X library.

00:23:22.720 --> 00:23:26.800
Profiling on the 970 is very interesting.

00:23:26.800 --> 00:23:30.340
This is a machine that keeps
hundreds of instructions in flight,

00:23:30.390 --> 00:23:33.250
has long and deep pipelines,

00:23:34.010 --> 00:23:37.820
Very often I've looked at a trace
and had the "aha" experience.

00:23:37.860 --> 00:23:40.800
This wasn't where I expected the
performance to be being spent.

00:23:40.890 --> 00:23:45.860
Let's arrange to tune for that.

00:23:46.710 --> 00:23:47.740
Mark R.

00:23:47.800 --> 00:23:55.810
For a rough cut, you can use the command
line tool called sample.

00:23:56.190 --> 00:23:58.510
But for the real action,
you want to use Shark.

00:23:58.630 --> 00:24:00.270
That's part of the Chud toolkit.

00:24:00.370 --> 00:24:04.930
It will zero in on your hot spots and
let you make really rapid progress.

00:24:04.930 --> 00:24:07.640
It's been key in our development.

00:24:07.730 --> 00:24:15.880
And so I'd like to introduce Eric Miller,
who'll come up and talk a

00:24:15.880 --> 00:24:17.790
little bit about the Chud tools.

00:24:30.900 --> 00:24:36.580
So anyways, I'm Eric Miller with the
Architecture and Performance Group.

00:24:36.580 --> 00:24:38.400
Just a quick thing about the Chud Tools.

00:24:38.400 --> 00:24:41.400
There was a big session yesterday,
I believe it was number 506.

00:24:41.400 --> 00:24:42.890
There we go.

00:24:42.890 --> 00:24:43.550
Microphone.

00:24:43.550 --> 00:24:45.160
Can everybody hear me now?

00:24:45.160 --> 00:24:46.230
Quick thing about Chud Tools.

00:24:46.230 --> 00:24:49.500
There was a session yesterday,
I believe the number was 506,

00:24:49.500 --> 00:24:50.500
is that correct?

00:24:50.500 --> 00:24:52.080
And so that'll be on the DVD.

00:24:52.140 --> 00:24:56.060
A very lengthy demonstration with
Shark that I think you should

00:24:56.060 --> 00:24:59.940
all take advantage of that and
use Shark as much as you can.

00:25:00.000 --> 00:25:01.940
But there are a couple
of other Chud Tools.

00:25:01.940 --> 00:25:05.870
There's a suite, so there are probably
nine tools to go through.

00:25:07.300 --> 00:25:11.320
We leverage the low-level performance
monitor counters inside the hardware

00:25:11.680 --> 00:25:13.070
and in the operating system.

00:25:13.070 --> 00:25:16.600
We put the ones in the operating
system in there just for this purpose.

00:25:16.770 --> 00:25:21.800
You can find the problems and improve
your code with Shark and Monster.

00:25:21.800 --> 00:25:23.620
And all the best things about
Chud tools is they're free.

00:25:23.620 --> 00:25:25.320
We have an FTP site.

00:25:25.460 --> 00:25:29.390
I urge you when you get your developer
tools CD and there's a Chud package,

00:25:29.440 --> 00:25:32.690
install that package,
immediately update because we

00:25:32.770 --> 00:25:36.800
drop an update every several days
while we're in our beta period.

00:25:36.810 --> 00:25:43.210
And once we have a gold master,
it'll be probably on a semi-weekly basis.

00:25:43.210 --> 00:25:43.890
There'll be fixes and improvements.

00:25:45.330 --> 00:25:48.180
So we're introducing the Chud Tools 3.0.

00:25:48.260 --> 00:25:53.710
Shark was formerly called
Shikari in version 2.0 and 2.5.

00:25:53.890 --> 00:25:58.770
and the rest of the team are
working on the software for the 970.

00:25:58.800 --> 00:26:00.800
The 970 is a great tool
for performance testing.

00:26:00.800 --> 00:26:06.800
It can help you tune for the
dispatch grouping that the 970 uses.

00:26:06.800 --> 00:26:09.800
Monster is a spreadsheet
for performance events,

00:26:09.800 --> 00:26:13.870
things like cache misses,
instructions completed,

00:26:13.870 --> 00:26:17.990
instructions dispatched, whether or not,
what kind of utilization you're

00:26:18.000 --> 00:26:19.800
getting out of the various
floating point and factory units,

00:26:19.800 --> 00:26:21.800
integer units in the processor.

00:26:21.810 --> 00:26:28.000
Saturn can actually take advantage of the
PMCs and do call grab visualization with

00:26:28.000 --> 00:26:31.800
additional information about what types
of performance events are being utilized

00:26:31.800 --> 00:26:34.750
in a particular function in your code.

00:26:35.290 --> 00:26:37.360
We have several tracing
tools and Steve will probably

00:26:37.360 --> 00:26:39.040
talk to some of this stuff.

00:26:39.140 --> 00:26:44.200
Amber is the most important tool because
it actually collects a trace from your

00:26:44.200 --> 00:26:47.800
application of every single instruction
that's executed on the processor.

00:26:47.910 --> 00:26:52.390
You take this trace and you
use that as input to ACID,

00:26:52.390 --> 00:26:56.440
SIMG4, and once it comes out SIMG5.

00:26:58.810 --> 00:27:00.850
You can also take advantage
of the CHUD framework,

00:27:00.920 --> 00:27:05.700
which allows you to actually instrument
your applications to directly monitor

00:27:05.700 --> 00:27:11.060
performance events or control the
profiling tools from inside your code so

00:27:11.090 --> 00:27:13.700
you can bracket important pieces of code.

00:27:13.800 --> 00:27:16.360
So I've mentioned performance
counters many times already

00:27:16.360 --> 00:27:17.700
in just a couple of slides.

00:27:17.700 --> 00:27:21.450
What they are is a set of special purpose
registers that exist in the processor

00:27:21.450 --> 00:27:25.700
and memory controller and off in the
OS we have virtual performance counters.

00:27:25.920 --> 00:27:28.820
These can be accessed by software,
obviously,

00:27:28.820 --> 00:27:32.700
so we created the CHUD tools to
do that for you automatically.

00:27:32.700 --> 00:27:35.870
Although there are actually
user versions of some of the

00:27:35.890 --> 00:27:39.800
performance monitor counters that
you can access from user code,

00:27:39.800 --> 00:27:42.700
but in general to set them up and
drive them requires a supervisor

00:27:42.700 --> 00:27:47.590
application like the kernel,
so we put stuff in there.

00:27:47.840 --> 00:27:52.370
As I mentioned earlier,
it's performance events and page

00:27:52.560 --> 00:27:55.800
faults is one of the operating
systems you can measure.

00:27:55.800 --> 00:27:59.330
There's quite a number of virtual
memory counters available in the

00:27:59.330 --> 00:28:02.260
operating system performance counters
that you might want to take advantage

00:28:02.290 --> 00:28:09.520
of for a very high-level look at
what's going on with your disk access

00:28:09.520 --> 00:28:10.990
and virtual memory and physical
memory access in the operating system.

00:28:13.420 --> 00:28:16.280
So, Steve has alluded to Shark.

00:28:16.340 --> 00:28:20.660
That's the icon in the upper right there.

00:28:20.660 --> 00:28:24.030
The nice thing about Shark is you
can profile over time and with the

00:28:24.040 --> 00:28:27.900
Chud tools the profiling can be
as small as 50 microseconds per

00:28:27.900 --> 00:28:31.300
sample or as large as a second.

00:28:31.300 --> 00:28:32.380
Or you can use the events.

00:28:32.380 --> 00:28:35.290
You can actually profile every so
many cycles you can take a sample.

00:28:35.310 --> 00:28:38.300
Every so many instructions
completed you can take a sample.

00:28:38.300 --> 00:28:41.620
You capture everything with
Shark from the kernel up through

00:28:41.620 --> 00:28:45.420
your application so you can know
exactly where time is being spent,

00:28:45.420 --> 00:28:47.300
in what framework time is being spent.

00:28:47.300 --> 00:28:49.300
And as I mentioned the
overhead is very low.

00:28:49.300 --> 00:28:55.060
It's about 40 to 50 microseconds per
sample you can expect to expend taking

00:28:55.070 --> 00:28:58.610
data from the performance counters.

00:29:00.270 --> 00:29:02.740
The other nice thing about Shark is
there's automated analysis.

00:29:02.790 --> 00:29:05.640
Not only will it show you
where you're spending time,

00:29:05.640 --> 00:29:08.820
it'll try and explain to you why
time is being spent there and

00:29:09.080 --> 00:29:13.120
give you opportunities to try
to alleviate that bottleneck.

00:29:13.350 --> 00:29:18.000
Steve also mentioned that you need,
again, that we use static analysis

00:29:18.050 --> 00:29:23.610
to construct the theoretical
dispatch groups on the 970.

00:29:23.670 --> 00:29:26.960
And this is all demonstrated
quite nicely in the 506 session,

00:29:26.960 --> 00:29:29.270
which unfortunately was yesterday.

00:29:29.320 --> 00:29:32.520
You can save and review all the sessions,
which of course should always be there,

00:29:32.520 --> 00:29:33.850
but it's new in CHUD 3.

00:29:33.910 --> 00:29:36.910
We didn't have save and
review for our tools before.

00:29:37.700 --> 00:30:17.800
[Transcript missing]

00:30:18.150 --> 00:30:21.860
The heavy tree trace,
in this particular case,

00:30:21.860 --> 00:30:22.870
square roots on top.

00:30:22.870 --> 00:30:25.890
And you can see some of the
types of outputs you can get.

00:30:25.940 --> 00:30:28.100
There's some source code
in the right picture.

00:30:28.100 --> 00:30:33.920
And some of the shark commentary that
comes in the form of these little

00:30:33.920 --> 00:30:39.410
exclamation points in the column
toward the right of the window.

00:30:41.200 --> 00:30:43.820
I'll just briefly touch on Monster.

00:30:43.880 --> 00:30:46.880
Again, same thing, timed intervals,
event counts.

00:30:46.980 --> 00:30:49.670
All the Chud tools have a hotkey,
even the command line tools.

00:30:49.690 --> 00:30:53.540
If you're on the console,
you can hit option escape and

00:30:53.540 --> 00:30:57.780
launch Shark's profiler and option
escape to toggle it off again.

00:30:57.780 --> 00:30:59.780
Monster uses control escape.

00:30:59.780 --> 00:31:02.260
So they can both be on the
system at the same time.

00:31:03.760 --> 00:31:04.880
So the hotkey's kind of neat.

00:31:04.920 --> 00:31:07.060
Even in a command line tool,
it'll just sit there waiting

00:31:07.060 --> 00:31:08.080
for you to start profiling.

00:31:08.080 --> 00:31:11.320
And you don't have to go into
the shell and type something.

00:31:11.320 --> 00:31:16.110
You can just do it from anywhere on your,
wherever your app is,

00:31:16.110 --> 00:31:18.770
you can go ahead and start that.

00:31:18.840 --> 00:31:21.140
So the big thing about
Monster is shortcuts.

00:31:21.140 --> 00:31:24.380
You can take these performance
monitor event counts and combine

00:31:24.430 --> 00:31:27.980
them together with a simple four
function calculator notation.

00:31:27.980 --> 00:31:33.740
So you can take cache misses and cycles.

00:31:33.760 --> 00:31:37.320
And compute cache misses per
cycle or cycles per cache miss,

00:31:37.320 --> 00:31:38.350
whichever way you want to do that.

00:31:38.460 --> 00:31:38.980
Ratio.

00:31:38.990 --> 00:31:43.350
You can also use the controller counter,
the memory controller counters and

00:31:43.410 --> 00:31:46.400
compute bandwidth by collecting all
the transactions and multiplying by a

00:31:46.400 --> 00:31:50.870
certain value that will represent bytes
per transaction for reads and writes,

00:31:50.870 --> 00:31:51.570
this sort of thing.

00:31:51.580 --> 00:31:55.860
And you can also create,
there are several shortcuts that

00:31:55.910 --> 00:31:58.260
are predefined for each CPU.

00:31:58.270 --> 00:32:03.050
But you can also make your own ratios and
proportions and percentages to print out.

00:32:03.080 --> 00:32:03.760
And they come out in a
couple of different ways.

00:32:03.760 --> 00:32:11.940
You can put it in the tabular columns of
Monster and then you can chart that data.

00:32:11.940 --> 00:32:15.400
Same thing, save review sessions,
command line version.

00:32:15.500 --> 00:32:16.340
There's a picture.

00:32:16.340 --> 00:32:17.920
So there's some shortcuts
that have been highlighted,

00:32:17.940 --> 00:32:20.340
those purple columns on the left,
and then you graph them by

00:32:20.340 --> 00:32:21.820
pressing the draw chart button.

00:32:21.910 --> 00:32:24.660
These are percentages of load
store instructions with regard

00:32:24.660 --> 00:32:28.290
to all the instructions that
were collected in the trace.

00:32:28.930 --> 00:32:29.790
and David S.

00:32:29.800 --> 00:32:31.730
The first step is to create a code
that will record your function call

00:32:31.730 --> 00:32:37.700
history and instruments all your code
by using the GCC instrumentation flags.

00:32:37.700 --> 00:32:40.390
Very similar to Code Warrior has
some instrumentation flags

00:32:40.390 --> 00:32:43.700
that will put a prologue and
an epilogue in every function.

00:32:43.700 --> 00:32:45.610
Then once you have those
prologues and epilogues,

00:32:45.610 --> 00:32:46.800
the data can be collected.

00:32:46.800 --> 00:32:51.780
You can see a typical call tree in the
top half of the screen and then you

00:32:51.780 --> 00:32:54.340
can get the picture of the call tree.

00:32:54.340 --> 00:32:58.880
The call depth is vertical and the time
the call was executing is horizontal.

00:32:58.880 --> 00:33:02.320
So if you have long spiky calls,
you want to try to

00:33:02.320 --> 00:33:04.310
alleviate those issues.

00:33:07.180 --> 00:33:09.380
You can use call,
you can collect call counts,

00:33:09.390 --> 00:33:11.690
that's great,
but you can also collect PMC event

00:33:11.700 --> 00:33:15.990
counts and see those things and
what kind of duration they had.

00:33:17.150 --> 00:33:21.100
So, and we have these instruction
tracing tools that I mentioned.

00:33:21.140 --> 00:33:22.760
ACID's kind of nice as a quick pass.

00:33:22.780 --> 00:33:26.220
It's sort of SIMG4, SIMG5 lite.

00:33:26.250 --> 00:33:29.420
You just can collect trace statistics.

00:33:29.420 --> 00:33:32.420
So you use AMBER and collect an
instruction trace that's accurate.

00:33:32.440 --> 00:33:33.940
Then you run ACID on it.

00:33:33.950 --> 00:33:37.240
You can get these pieces of data
out of ACID very readily in maybe

00:33:37.240 --> 00:33:39.780
one or two screens in a terminal.

00:33:39.820 --> 00:33:44.060
Whereas SIMG4 and SIMG5 are very
cycle accurate simulators for their

00:33:44.060 --> 00:33:47.900
respective processors and it takes
some learning which Steve will get

00:33:47.900 --> 00:33:50.630
into to understand their output.

00:33:52.970 --> 00:33:57.120
Using the Chud framework,
you can instrument your source code,

00:33:57.280 --> 00:34:00.610
start and stop the other
graphic user interface tools,

00:34:00.680 --> 00:34:05.220
and also directly read and write the
performance counters in your code.

00:34:08.630 --> 00:34:12.200
There's also the HTML reference
guide is generated every time

00:34:12.200 --> 00:34:14.000
we do a build of the framework.

00:34:14.000 --> 00:34:18.030
The HTML is updated to any new
things we put in the prologues.

00:34:19.210 --> 00:34:22.850
So a quick example, you can,
you always chud initialize

00:34:22.850 --> 00:34:24.380
whenever you use this stuff.

00:34:24.420 --> 00:34:30.150
You require remote access and you tell
Shark that you want to use remote access.

00:34:30.150 --> 00:34:33.510
Then you start Shark and give it a label.

00:34:33.690 --> 00:34:36.820
Then your important function executes.

00:34:36.820 --> 00:34:39.960
Then you stop it and release
remote access so another thread

00:34:39.970 --> 00:34:41.730
or another client can use it.

00:34:41.730 --> 00:34:45.060
But then Shark will automatically
profile your important function

00:34:45.060 --> 00:34:46.620
and your important function only.

00:34:46.620 --> 00:34:48.540
And you'll get the results in the GUI.

00:34:50.610 --> 00:34:54.320
Slightly longer example where you
actually set the counters explicitly,

00:34:54.320 --> 00:34:57.780
clear them, start them,
then your important work happens.

00:34:57.810 --> 00:35:00.860
Then you stop the counters
and then you take the,

00:35:00.860 --> 00:35:04.780
it returns arrays of double
precision floating point values.

00:35:04.780 --> 00:35:07.720
If there's six counters,
there'll be six entries, one for each,

00:35:07.800 --> 00:35:10.080
zero through five, in the output arrays.

00:35:10.080 --> 00:35:12.580
You take those output arrays
and then do whatever you want to

00:35:12.580 --> 00:35:14.400
do to present them to yourself.

00:35:14.440 --> 00:35:18.370
Maybe log them or chart them.

00:35:20.110 --> 00:35:21.100
This thing is a little finicky.

00:35:21.190 --> 00:35:22.810
So, how do you get CHUD?

00:35:22.940 --> 00:35:24.930
The easiest way is it's
on the developer tool CD,

00:35:25.040 --> 00:35:27.330
but updates will come
directly from the web.

00:35:27.370 --> 00:35:29.230
There's an updater that
will run automatically the

00:35:29.230 --> 00:35:32.420
first time you install CHUD,
and thereafter there's preferences.

00:35:32.420 --> 00:35:40.380
You can check the status of the
CHUD package for new updates hourly,

00:35:40.380 --> 00:35:40.380
daily, weekly, or monthly.

00:35:41.720 --> 00:35:44.700
We do have internal guys that do check
hourly and they get upset when a couple

00:35:44.700 --> 00:35:47.290
hours go by and there's no new updates.

00:35:47.330 --> 00:35:50.470
So the best way to get in contact
with our team is to use the

00:35:50.470 --> 00:35:55.480
chud-tools-feedback@group.apple.com.

00:35:55.490 --> 00:35:58.970
And with that I will turn
it back over to Steve.

00:36:03.900 --> 00:36:04.900
Excellent.

00:36:04.900 --> 00:36:05.900
Thank you.

00:36:05.900 --> 00:36:12.530
Thank you, Eric.

00:36:12.530 --> 00:36:12.530
I think I'm going to go with a keyboard.

00:36:13.260 --> 00:36:22.300
Eric has covered the astonishing
capabilities of Chud,

00:36:23.040 --> 00:36:26.690
but don't think you have to
do anything really special.

00:36:26.690 --> 00:36:31.740
The most you need to do, really,
is to learn to use the hot key.

00:36:32.660 --> 00:36:35.170
If you haven't been down
to the performance lab,

00:36:35.240 --> 00:36:38.090
the G5 performance lab, with your app,
please do come down.

00:36:38.090 --> 00:36:40.430
If you find me there,
I'm likely to be a pest and

00:36:40.440 --> 00:36:42.900
hover over your shoulder,
wait until your app is

00:36:42.900 --> 00:36:46.340
running and grinding the CPU,
and say, hey, can I start Chud?

00:36:46.340 --> 00:36:50.970
And it's basically start the app,
hit the hot key, wait a few seconds,

00:36:51.190 --> 00:36:52.980
and then look at the samples.

00:36:52.980 --> 00:36:57.680
People have been dropping jaw at
what they see and how simple it is.

00:36:57.730 --> 00:36:59.200
Chud comes up.

00:36:59.200 --> 00:37:02.220
I'll take a question at the end.

00:37:19.550 --> 00:37:22.360
- Quick question,
you don't even need that, so,

00:37:22.360 --> 00:37:24.550
and we can show you why.

00:37:24.990 --> 00:37:29.700
So people are quite astonished to see
how quickly it happens and sort of

00:37:29.700 --> 00:37:32.780
the almost immediate insight they get,
gee, there's my high runner right

00:37:32.780 --> 00:37:33.710
at the top of the window.

00:37:33.710 --> 00:37:38.430
Let's click into that and see which
instructions are slowing me down here.

00:37:38.440 --> 00:37:39.890
It's really a great thing.

00:37:39.900 --> 00:37:43.900
So with some background
on Chud behind us,

00:37:43.900 --> 00:37:50.120
I now want to talk a little bit about
this regimen I go through to really

00:37:51.000 --> 00:37:56.700
torque down and get some performance
out of floating point intensive code.

00:37:56.820 --> 00:37:58.860
First concern often is memory.

00:37:58.940 --> 00:38:03.610
The machine has an enormous amount of
bandwidth if you can use it effectively.

00:38:04.090 --> 00:38:10.620
You need to load data early so
that it's available early to the

00:38:10.720 --> 00:38:14.810
out-of-order execution cores.

00:38:15.170 --> 00:38:18.910
If the data ain't there,
the cores can't do the instructions,

00:38:18.960 --> 00:38:19.930
so get the data there early.

00:38:20.010 --> 00:38:28.180
So in examples, you'll see that I load
polynomial coefficients,

00:38:28.440 --> 00:38:31.150
literal constants very
early on in subroutines,

00:38:31.260 --> 00:38:32.340
even speculatively.

00:38:32.340 --> 00:38:36.060
Even if they may not be used in
a particular branch of the code,

00:38:36.060 --> 00:38:38.720
I'll often load them early just
to have them available in the

00:38:38.720 --> 00:38:39.950
case we drop through and go.

00:38:41.590 --> 00:38:46.150
So load early, load often,
harness the two LSUs to

00:38:46.150 --> 00:38:48.390
drive those two FPUs.

00:38:48.430 --> 00:38:52.860
If you can load data sequentially,
there are hardware-initiated prefetch

00:38:52.860 --> 00:38:58.040
streams that are really effective at
getting the data into the machine.

00:39:00.200 --> 00:39:09.370
DST and vecDST, the moto entry point,
are bad eggs.

00:39:10.150 --> 00:39:15.100
and David Koehn are the founders of
the DPC-ZL class instruction framework.

00:39:15.100 --> 00:39:17.100
They're a big help on G4.

00:39:17.100 --> 00:39:20.090
They're execution synchronizing on G5.

00:39:20.100 --> 00:39:22.100
They probably ought to be avoided.

00:39:22.100 --> 00:39:28.100
You're better off if you want to
prefetch data using the DCB-TL,

00:39:28.100 --> 00:39:30.100
DCB-ZL class instructions.

00:39:30.350 --> 00:39:39.600
You need to be aware that the cache
line size on the 970 is 128 bytes.

00:39:45.600 --> 00:39:48.200
So did I say that the
970 has two FPU cores?

00:39:48.250 --> 00:39:48.980
I think so.

00:39:48.980 --> 00:39:51.890
Use them.

00:39:52.100 --> 00:40:48.600
[Transcript missing]

00:40:49.400 --> 00:40:53.260
Since there are two FPUs,
a simple strategy for making

00:40:53.260 --> 00:40:58.240
sure that you're getting both
FPUs fully utilized is to think

00:40:58.350 --> 00:40:59.980
that you've got a 12 cycle pipe.

00:41:00.150 --> 00:41:03.790
That is, start a result,
don't plan to use it

00:41:03.800 --> 00:41:07.420
until 12 cycles later,
12 intervening operations.

00:41:07.490 --> 00:41:12.910
That often means you need to think a
little bit more about parallelizing

00:41:13.620 --> 00:41:22.200
software pipelining your algorithms to
get that kind of distance between uses.

00:41:27.560 --> 00:41:33.030
So here's a little piece
on choice of algorithm.

00:41:33.220 --> 00:41:36.540
Sometimes you need to sort of just
pop up a couple of levels and think,

00:41:36.580 --> 00:41:40.260
is there some way to recast
the algorithm I have to be more

00:41:40.260 --> 00:41:41.250
effective on floating point?

00:41:41.440 --> 00:41:46.200
So the example is it takes
two hands to matrix multiply.

00:41:46.200 --> 00:41:50.280
You're trying to form the matrix
C as the product of A and B.

00:41:50.920 --> 00:41:54.910
In high school,
or I guess junior high school nowadays,

00:41:54.920 --> 00:42:00.800
you learn how to take the product
of two N by N matrices by using two

00:42:00.800 --> 00:42:04.740
hands to form the output elements C,
I, J.

00:42:04.740 --> 00:42:08.090
You take the Ith row in one hand
and the Jth column in the other

00:42:08.580 --> 00:42:10.500
hand and you form a dot product.

00:42:12.700 --> 00:42:21.400
2n fetches and n multiplied later,
you have a single element

00:42:21.400 --> 00:42:21.400
of the output Cij.

00:42:22.200 --> 00:42:37.400
[Transcript missing]

00:42:37.950 --> 00:42:45.960
To form the 4x4 output block in matrix C,
you grab four elements.

00:42:46.620 --> 00:42:55.750
and David Koehn will be
joined by the two of them.

00:42:55.750 --> 00:42:55.750
Thank you.

00:42:57.360 --> 00:43:01.670
Possible products, pairwise products.

00:43:01.670 --> 00:43:05.680
Continue down in rows and in columns.

00:43:06.440 --> 00:43:08.670
You've done, and let me,
I'm just going to look at this

00:43:08.690 --> 00:43:10.800
so I remember it exactly right.

00:43:10.890 --> 00:43:14.430
You're accumulating 16
simultaneous inner products.

00:43:14.770 --> 00:43:19.800
It requires 8 end fetches,

00:43:20.700 --> 00:43:29.330
16N operations,
that's actually a factor of four

00:43:29.330 --> 00:43:29.330
reduction in memory bandwidth.

00:43:29.690 --> 00:43:34.330
You've used nearly all the registers
and it's possible to keep the

00:43:34.380 --> 00:43:37.010
floating point units bubble free.

00:43:37.130 --> 00:43:41.790
And basically that's the trick that
Atlas uses and our MatML kernel

00:43:41.840 --> 00:43:46.300
to drive the machine at,
in the kernel, 84% of peak.

00:43:47.220 --> 00:43:51.170
So the take-home message is,
think parallel if you can,

00:43:51.250 --> 00:43:54.770
but small parallel, you know,
four by four, it's manageable.

00:43:58.330 --> 00:44:03.840
So here's a little case study I thought
we'd go through that gets a little

00:44:03.840 --> 00:44:08.210
bit more into the tracing tools.

00:44:08.380 --> 00:44:15.100
The code is the arm of the Libm sine
function for arguments that are smallish,

00:44:15.250 --> 00:44:18.740
between pi over 4 and minus pi over 4.

00:44:19.180 --> 00:44:23.470
and just to set some
landmarks for orientation,

00:44:23.490 --> 00:44:30.270
you'll see that there's an absolute
value function taken at the top,

00:44:30.530 --> 00:44:35.660
comparison here to decide
if we're in the right arm,

00:44:35.770 --> 00:44:39.990
some manipulation of the
floating point environment,

00:44:39.990 --> 00:44:39.990
some arithmetic and then what looks
like a polynomial approximation.

00:44:42.940 --> 00:44:46.020
Formation of the final result.

00:44:46.020 --> 00:44:50.910
Some more adjustment of the floating
point environment and out we go.

00:44:52.990 --> 00:44:57.900
So on the G4 series,
we had a tool called SIMG4.

00:44:57.900 --> 00:45:01.950
And if we look at this
segment of code run on,

00:45:01.950 --> 00:45:04.800
again, a smallish argument to sign,

00:45:04.920 --> 00:45:06.200
We see this picture.

00:45:06.240 --> 00:45:11.020
This is actually a very good picture,
SIMG4 wise.

00:45:11.020 --> 00:45:16.350
At the top, let's see if I can get
this thing going again.

00:45:18.450 --> 00:45:20.300
All right, here we go.

00:45:20.380 --> 00:45:22.530
For our landmark,
there's that FABs instruction,

00:45:22.530 --> 00:45:28.510
and we read across to see this FABs
instruction issued roughly at cycle 1200,

00:45:28.520 --> 00:45:29.760
1, 2, 3, 4, 5, 6, 7, 8.

00:45:29.760 --> 00:45:34.060
Spent two cycles getting
instruction fetch,

00:45:34.090 --> 00:45:37.450
one in dispatch, a couple in execution,
and then retired.

00:45:38.220 --> 00:45:41.840
Next line, store word with update,
additional time.

00:45:41.840 --> 00:45:44.820
The object here is to fall off the cliff.

00:45:45.060 --> 00:45:48.110
You're retiring instructions
as fast as you can.

00:45:48.120 --> 00:45:49.950
So this looks pretty good.

00:45:49.950 --> 00:45:52.120
This code's really quite good on G4.

00:45:52.120 --> 00:45:58.360
Taking just that code unchanged,
bringing it to the G5, the 970,

00:45:58.360 --> 00:46:03.380
and using the SIM G5 trace, well,
first of all,

00:46:03.380 --> 00:46:06.100
it's a more complicated machine.

00:46:09.610 --> 00:46:13.000
and the letters have changed.

00:46:13.180 --> 00:46:15.890
So we spend some time in fetch,

00:46:16.190 --> 00:46:19.200
Five cycles in dispatch,
some time in the mapper,

00:46:19.200 --> 00:46:26.010
we finally hit execution
unit five for that Fabs,

00:46:26.010 --> 00:46:26.010
which is now on the
other side of the screen.

00:46:26.510 --> 00:46:31.250
We finish that operation, what,
six cycles later,

00:46:31.250 --> 00:46:34.290
and we hang around and wait
for the rest of that completion

00:46:34.550 --> 00:46:38.680
group to all finish up,
and then we complete.

00:46:39.800 --> 00:46:41.370
This doesn't look too bad either.

00:46:41.560 --> 00:46:45.260
Again,
the object is to fall off the cliff.

00:46:45.470 --> 00:46:47.240
Well,
until we get down to the bottom here,

00:46:47.240 --> 00:46:48.850
this is trouble.

00:46:48.890 --> 00:46:55.140
And there's a key for these letters,
and if we were to look up the key,

00:46:55.180 --> 00:46:59.300
we'd see that we had
essentially a load store reject.

00:46:59.340 --> 00:47:03.060
So somebody is trying to
load a load store reject.

00:47:03.690 --> 00:47:06.880
from an address that was recently stored.

00:47:06.910 --> 00:47:08.980
Well, how recently?

00:47:09.090 --> 00:47:12.680
Turns out that store occurred way up top.

00:47:12.860 --> 00:47:17.150
This is a machine that's putting
hundreds of instructions in flight.

00:47:17.230 --> 00:47:23.670
These dependencies can stretch out
over really large lengths of code.

00:47:24.140 --> 00:47:28.280
Beware, be forewarned,
and watch out for these kind of things.

00:47:28.320 --> 00:47:33.260
By adjusting for this,
it turns out this is the manipulation

00:47:33.260 --> 00:47:36.250
of the floating point environment.

00:47:37.600 --> 00:47:39.940
We can end up doing quite a bit better.

00:47:39.980 --> 00:47:44.090
You'll notice, first of all,
this is shorter.

00:47:45.050 --> 00:47:50.400
and the fall off the clip
is much more precipitous.

00:47:51.200 --> 00:47:51.590
And we've eliminated that

00:47:52.280 --> 00:47:54.880
nastiness down in here.

00:47:54.880 --> 00:47:59.110
So, that's the kind of information
you can gain from the SIMG4,

00:47:59.220 --> 00:48:00.850
SIMG5 class of tools.

00:48:00.970 --> 00:48:05.180
I think it's an adjunct to Chud.

00:48:05.300 --> 00:48:08.670
I think Chud with Shark is
the first place to look.

00:48:08.790 --> 00:48:11.000
But when you, you know,
wanna eek out the last

00:48:11.000 --> 00:48:15.300
little bit of performance,
this is the tool that I turn to.

00:48:18.450 --> 00:48:20.600
So here's the final version of that code.

00:48:20.640 --> 00:48:27.200
Just for landmarks,
there's that FABs again.

00:48:27.340 --> 00:48:34.760
Still have the compares, but we've,
turns out we've manipulated the

00:48:34.760 --> 00:48:38.810
environment by bringing it into
register rather than storing.

00:48:41.500 --> 00:48:48.250
I've adopted this style where I split
across a single line in the C code places

00:48:48.260 --> 00:48:50.070
where I think I can gain parallelism.

00:48:50.150 --> 00:48:55.960
So here's loading early and often
of the polynomial coefficients.

00:48:56.670 --> 00:49:01.450
Here are operations that I believe
go in parallel on the dual FPUs.

00:49:01.540 --> 00:49:04.620
And then out.

00:49:08.200 --> 00:49:12.270
So, quick summary of the
regimen I like to use.

00:49:12.440 --> 00:49:16.780
Start with Chud, look at Shark,
go back to your code,

00:49:16.950 --> 00:49:19.780
pay attention to load store issues.

00:49:20.420 --> 00:49:36.200
Think in terms of dual FPUs.

00:49:36.200 --> 00:49:36.200
Even just organizing the layout of your
code can help you think when you can take

00:49:36.200 --> 00:49:36.200
advantage of things going in parallel.

00:49:36.200 --> 00:49:36.200
Use as many registers as possible.

00:49:37.360 --> 00:49:41.320
Let the hardware-initiated prefetch
streams help you get data into

00:49:41.320 --> 00:49:43.760
the machine early and often.

00:49:46.370 --> 00:49:51.550
and when directed by SIMG4,
SIMG5 kind of tools,

00:49:51.550 --> 00:49:55.250
look at dispatch group formation
just to make sure that you're not

00:49:55.250 --> 00:49:59.590
crowding instructions that you think
ought to be going to separate FPUs,

00:49:59.730 --> 00:50:01.780
one on top of the other
in a single issue queue.

00:50:01.780 --> 00:50:05.420
That was the slide that came
much earlier in the talk.

00:50:05.420 --> 00:50:10.730
So, time to wrap up.

00:50:15.320 --> 00:50:19.000
You can review these sessions on the DVD.

00:50:24.880 --> 00:50:28.200
can contact myself or Ali for questions.

00:50:28.200 --> 00:50:32.600
Ali does a little skunk works
operation every once in a while,

00:50:32.600 --> 00:50:34.910
but he can tell you about that.

00:50:36.300 --> 00:50:40.660
and for more information,
there are two really fine tech notes

00:50:40.660 --> 00:50:45.570
now on the web at the developer site.

00:50:46.100 --> 00:51:03.100
[Transcript missing]

00:51:06.020 --> 00:51:09.600
There's some other interesting
documentation at the,

00:51:09.600 --> 00:51:11.340
somebody.html here.

00:51:11.340 --> 00:51:14.890
We'll have to resolve this.

00:51:16.320 --> 00:51:20.400
And finally,
I'm a big fan of the fellow who writes

00:51:21.500 --> 00:51:23.660
for Ars Technica describing the PowerPC.

00:51:23.660 --> 00:51:28.370
This is a really lovely introduction
to the machine and a good place to

00:51:28.370 --> 00:51:33.970
start a quiet evening with your laptop.