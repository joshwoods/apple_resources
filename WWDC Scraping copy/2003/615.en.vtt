WEBVTT

00:00:28.110 --> 00:00:30.180
Thank you and good afternoon.

00:00:30.290 --> 00:00:35.220
So we're going to talk this
afternoon a little bit on building

00:00:35.220 --> 00:00:38.880
computational clusters with
Mac OS X Server and with XServe.

00:00:38.880 --> 00:00:42.290
And I think it's really important
before we get too far into the

00:00:42.290 --> 00:00:46.530
presentation to make sure we are all
here understanding the right thing.

00:00:46.540 --> 00:00:48.820
Clustering is one of
those really powerful,

00:00:48.820 --> 00:00:52.040
overloaded words that means a
lot of things to a lot of people.

00:00:52.040 --> 00:00:56.700
And I get asked all the time, ooh,
I want to cluster my Xserves.

00:00:56.750 --> 00:00:58.080
Well, what does that mean?

00:00:58.100 --> 00:01:02.750
And so I think it's important to
recognize that clusters are used,

00:01:02.750 --> 00:01:06.420
the term clustering is actually
used in two very distinct areas.

00:01:06.420 --> 00:01:09.280
The first is clustering
for high availability.

00:01:09.280 --> 00:01:12.960
I want to take a server service
and cluster two or more servers

00:01:12.980 --> 00:01:16.840
together for high availability
such that if one would go down,

00:01:16.840 --> 00:01:21.680
the other one takes over its place
with ideally no network interruption.

00:01:21.680 --> 00:01:25.550
That's something very different than
what we're here to talk about today,

00:01:25.630 --> 00:01:27.680
which is clustering for
computational ability.

00:01:28.100 --> 00:01:32.750
Aggregating the computational performance
of several servers together to

00:01:32.750 --> 00:01:35.300
generate a larger compute farm per se.

00:01:35.300 --> 00:01:37.600
So just to be clear,
that's what we're going to be

00:01:37.690 --> 00:01:39.420
talking about this afternoon.

00:01:39.420 --> 00:01:42.880
Anyone interested in learning a
little bit more about applications

00:01:42.980 --> 00:01:46.490
for high availability with XServe,
invite you to my session tomorrow

00:01:46.590 --> 00:01:50.220
afternoon on deploying XServe,
where we'll touch on some approaches

00:01:50.270 --> 00:01:52.270
to the other kind of clustering.

00:01:56.970 --> 00:01:59.360
So let's talk a little bit about
what we want to cover in this

00:01:59.360 --> 00:02:02.770
session and dive right into it.

00:02:02.820 --> 00:02:06.110
So obviously the first thing might be
why I might want to build a cluster.

00:02:06.160 --> 00:02:08.640
What goals would it deliver for me?

00:02:08.640 --> 00:02:12.910
How can we use Xserve and
Mac OS X Server in a cluster?

00:02:12.940 --> 00:02:14.340
What are the benefits and advantages?

00:02:14.340 --> 00:02:16.640
Typical cluster architecture.

00:02:16.670 --> 00:02:18.200
What does it mean to build a cluster?

00:02:18.270 --> 00:02:22.220
What is the topology, the network,
the basic system requirements,

00:02:22.230 --> 00:02:25.050
and physical requirements
needed to do that?

00:02:25.090 --> 00:02:28.360
We'll talk a little bit about
deploying applications on a cluster,

00:02:28.360 --> 00:02:31.860
what kind of things can be distributed,
and some approaches to that.

00:02:31.900 --> 00:02:34.410
Physical network concerns.

00:02:34.420 --> 00:02:38.220
When you bring a large number
of CPUs into a certain area,

00:02:38.220 --> 00:02:41.920
there are obviously requirements that go
above and beyond the number of machines.

00:02:41.920 --> 00:02:47.180
It requires planning on power, cooling,
and network requirements.

00:02:47.180 --> 00:02:50.020
We'll touch on the requirements
in those areas as well.

00:02:50.020 --> 00:02:52.560
Tools and techniques
for deploying a cluster.

00:02:52.560 --> 00:02:53.800
What resources are available?

00:02:53.800 --> 00:02:55.060
What tools?

00:02:55.080 --> 00:02:59.100
are available and highlight
some of those capabilities.

00:02:59.600 --> 00:05:06.900
[Transcript missing]

00:05:07.130 --> 00:05:10.060
The other advantage of clusters is that,
and we'll talk more about this,

00:05:10.060 --> 00:05:14.310
is that it's very easy to scale a
cluster to both budget and problem.

00:05:14.320 --> 00:05:17.550
So, you know, based on a given budget,
you can, you know,

00:05:17.550 --> 00:05:20.720
very granularly add more
computing power by adding more

00:05:20.720 --> 00:05:22.620
compute elements to a problem.

00:05:22.620 --> 00:05:26.230
And, of course,
it's also easy to scale out that cluster

00:05:26.240 --> 00:05:28.590
based on the size of your problem.

00:05:28.600 --> 00:05:31.590
And so,
if you need more computational power,

00:05:31.590 --> 00:05:35.480
add some more modular servers
to your cluster deployment.

00:05:35.990 --> 00:05:39.190
So it makes it very
flexible in these areas.

00:05:39.350 --> 00:05:41.810
Some example applications,
so take some of the

00:05:41.810 --> 00:05:43.240
more obvious ones first.

00:05:43.280 --> 00:05:46.020
Image manipulations, rendering,
compositing,

00:05:46.020 --> 00:05:50.150
the only way these digital effects
that are being done in theaters

00:05:50.160 --> 00:05:55.230
today are being generated is by
massive computation behind the scenes.

00:05:55.240 --> 00:05:58.790
And again, clustering is the way to
achieve some of these goals.

00:05:59.720 --> 00:06:04.310
Simulation, biology,
simulating car crashes, airplanes,

00:06:04.320 --> 00:06:11.300
financial models of the markets
is something very well mapping out

00:06:11.300 --> 00:06:13.820
to compute cluster environments.

00:06:13.860 --> 00:06:16.740
And of course,
one of the mainstays in the cluster

00:06:16.740 --> 00:06:20.640
market and some of the areas where
we're seeing some of the biggest,

00:06:20.650 --> 00:06:23.690
strongest,
earliest successes with Xserve is in

00:06:23.710 --> 00:06:28.540
the life sciences with genomics and
other kind of life science analysis.

00:06:28.540 --> 00:06:29.540
Let me give you some examples.

00:06:29.720 --> 00:06:31.640
So, let me give you some
more dramatic examples.

00:06:31.780 --> 00:06:35.580
So, Pixar's Toy Story,
they have an amazing little website

00:06:35.580 --> 00:06:37.720
up on their pixar.com website.

00:06:37.720 --> 00:06:40.620
And there's a whole category
called "How We Do It," you know,

00:06:40.620 --> 00:06:41.720
behind the scenes.

00:06:41.720 --> 00:06:45.720
And on that site, they talk about what it
took to build Toy Story 2.

00:06:45.720 --> 00:06:51.180
And the example they give is
the average frame in Toy Story 2

00:06:51.180 --> 00:06:53.720
took six hours to render.

00:06:53.720 --> 00:06:55.410
Now,
I'll stress average because they also

00:06:55.500 --> 00:06:58.720
highlight some of the more complex
work took closer to 80 hours a frame.

00:06:58.720 --> 00:06:58.720
So, that's a lot of work.

00:06:58.720 --> 00:06:59.710
So, let's talk about the average frame.

00:06:59.720 --> 00:07:02.590
But just as an example,
six hours a frame,

00:07:02.590 --> 00:07:07.230
if you do a little bit of quick math
and you multiply that by 24 frames a

00:07:07.230 --> 00:07:13.720
second times 60 seconds in a minute
times 92 minutes in the feature film,

00:07:13.720 --> 00:07:18.610
that turns out to be around, you know,
shy of 800,000 hours or, you know,

00:07:18.990 --> 00:07:21.720
roughly 92 years of
computation behind that.

00:07:21.720 --> 00:07:25.620
And that's, you know,
just for the rendering piece of the film.

00:07:25.760 --> 00:07:26.940
So, you can imagine that the only
way to deliver this kind of

00:07:26.940 --> 00:07:27.720
work is to use the feature film.

00:07:27.720 --> 00:07:28.280
So, you can imagine that the only
way to deliver this kind of

00:07:28.280 --> 00:07:28.720
work is to use the feature film.

00:07:28.720 --> 00:07:33.180
deliver this kind of result
is with massive computational

00:07:33.180 --> 00:07:36.480
clustering power behind the scenes.

00:07:36.480 --> 00:07:38.440
It's also interesting
that this is Toy Story 2,

00:07:38.440 --> 00:07:39.760
which is a number of years ago.

00:07:39.760 --> 00:07:43.880
I can only imagine what
Finding Nemo took to render.

00:07:43.920 --> 00:07:49.050
Look at another example is in the
genomic life sciences environment.

00:07:49.260 --> 00:07:53.280
The fact is that the data that
needs to be analyzed is growing

00:07:53.280 --> 00:07:56.130
significantly faster than Moore's law.

00:07:56.460 --> 00:08:00.310
Processors aren't getting as
fast as the data is growing.

00:08:00.820 --> 00:08:06.800
Moore's law processor speed
doubles every 18 months.

00:08:06.800 --> 00:08:10.620
This is data from GenBank, NCBI.

00:08:10.620 --> 00:08:13.430
Roughly in the same timeframe,
the amount of data is

00:08:13.430 --> 00:08:15.450
growing by the order of 8X.

00:08:15.450 --> 00:08:19.270
The only way to do this is
aggregating computing power.

00:08:19.280 --> 00:08:22.250
Let's talk a little bit about XServe.

00:08:22.410 --> 00:08:26.800
Why do we think XServe has a
story in the computational space?

00:08:26.940 --> 00:08:28.980
Well, a couple of different reasons.

00:08:28.980 --> 00:08:33.040
First and foremost,
in the 1U form factor,

00:08:33.040 --> 00:08:37.200
we're able to deliver quite a bit of
processing power in a small form factor.

00:08:37.200 --> 00:08:40.680
When we couple that with
velocity engine in the G4,

00:08:40.680 --> 00:08:46.100
XServe becomes a very potent
machine for computational analysis.

00:08:46.210 --> 00:08:51.790
I'll save the question now,
which is the G5 question with XServe.

00:08:51.880 --> 00:08:54.210
A lot of people ask me that.

00:08:54.210 --> 00:08:56.660
I'm sure that will come up in Q&A.

00:08:56.660 --> 00:08:56.660
I'll mention it now.

00:08:56.660 --> 00:09:00.860
I'm not able to comment and answer
that question for you today.

00:09:00.970 --> 00:09:04.750
But anyone who's seen the G5 machines
when they were here earlier in the week,

00:09:04.750 --> 00:09:06.500
the heat sink is rather significant.

00:09:06.500 --> 00:09:10.200
And so I have a little bit of a
challenge to get this in an XServe,

00:09:10.200 --> 00:09:13.520
but it will be an interesting challenge.

00:09:13.520 --> 00:09:17.450
Regarding Mac OS X,
Mac OS X is a tremendous advantage

00:09:17.460 --> 00:09:23.550
for XServe in the cluster space in
that we have a very powerful open

00:09:23.550 --> 00:09:26.440
source BST core Unix operating system.

00:09:26.760 --> 00:09:32.930
That can leverage the major open source
projects and compile major applications

00:09:32.930 --> 00:09:37.250
out in industry and yet take advantage
of an operating system that's very

00:09:37.250 --> 00:09:39.180
easy to deploy and easy to maintain.

00:09:39.180 --> 00:09:41.940
This is a real highlight of
XServe and Mac OS X server.

00:09:41.940 --> 00:09:45.760
And we'll talk actually some of
the ways you can very easily and

00:09:45.760 --> 00:09:50.360
rapidly deploy Mac OS X servers
and XServe in this space.

00:09:50.430 --> 00:09:52.740
And finally, remote management.

00:09:52.740 --> 00:09:56.500
Rack mounted servers are meant
to live in a rack and not with

00:09:56.500 --> 00:09:59.730
the system administrator in front
of the rack at all the times.

00:09:59.870 --> 00:10:03.200
And clusters are even more
so in that environment.

00:10:03.200 --> 00:10:07.650
And so having powerful remote management
tools is essential to be able to maintain

00:10:07.650 --> 00:10:11.100
and monitor the status of a cluster.

00:10:12.300 --> 00:11:51.400
[Transcript missing]

00:11:52.810 --> 00:11:54.740
Let's talk a little bit about
a typical cluster deployment.

00:11:54.740 --> 00:11:59.630
What does it typically look like when we
deploy XServe in a cluster environment?

00:11:59.640 --> 00:12:04.080
And so there are three main pieces
in XServe cluster deployments.

00:12:04.080 --> 00:12:07.730
We'll start with what we'll call the top,
which is the head node.

00:12:07.740 --> 00:12:12.080
The head node,
and actually in the cluster we have here,

00:12:12.080 --> 00:12:15.660
is built upon this model
as the top machine.

00:12:16.150 --> 00:12:19.110
Typically this is a full XServe
with multiple drive bays.

00:12:19.110 --> 00:12:23.000
This is really the machine
responsible for managing the cluster.

00:12:23.000 --> 00:12:27.330
Obviously it typically runs the
software to distribute the load,

00:12:27.330 --> 00:12:30.230
manage user access,
provide storage to the compute

00:12:30.230 --> 00:12:31.580
elements in the cluster.

00:12:31.580 --> 00:12:35.800
That could be through internal
storage or through external storage.

00:12:35.800 --> 00:12:38.150
And again,
in this example here we have on

00:12:38.150 --> 00:12:41.980
stage an XServe RAID providing
the storage for that head node.

00:12:44.310 --> 00:12:46.840
One of the most important
pieces in a cluster is,

00:12:46.840 --> 00:12:48.430
of course, the interconnect network.

00:12:48.440 --> 00:12:51.340
So typically,
the head node is the only machine

00:12:51.340 --> 00:12:56.760
that you have connected to your
campus or corporate network,

00:12:56.860 --> 00:12:58.260
production network per se.

00:12:58.260 --> 00:13:00.860
It's typically the only thing
that's directly accessible

00:13:00.870 --> 00:13:02.300
to end users on the network.

00:13:02.300 --> 00:13:04.690
The interconnect network
actually connects the head

00:13:04.700 --> 00:13:06.200
node to the compute elements.

00:13:06.200 --> 00:13:09.260
This can be done in a number of ways.

00:13:09.640 --> 00:13:14.180
In most traditional fashion is using
Ethernet networking technologies,

00:13:14.180 --> 00:13:19.100
100 megabit networks or gigabit
networks for added performance.

00:13:19.100 --> 00:13:24.920
We have mirror net capabilities
on Mac OS X and Mac OS X Server.

00:13:24.920 --> 00:13:29.530
So mirror net is a high-performance
PCI interconnect card with extremely

00:13:29.530 --> 00:13:31.580
low latencies between nodes.

00:13:31.600 --> 00:13:35.460
And for certain kinds of applications,
this is very, very important.

00:13:35.460 --> 00:13:39.080
And so mirror net is an option available.

00:13:39.620 --> 00:13:41.880
And so there's also an
accompanying mirror net switch

00:13:41.880 --> 00:13:43.500
that allows that interconnect.

00:13:43.500 --> 00:13:46.350
And actually,
FireWire becomes very interesting for

00:13:46.350 --> 00:13:48.360
smaller clusters as an interconnect.

00:13:48.360 --> 00:13:52.280
Since for the cost of a cable,
you can chain several XSERVs together,

00:13:52.280 --> 00:13:55.730
especially with FireWire
800 built in on XSERV.

00:13:55.740 --> 00:13:58.640
And that's actually why on the
back of the XSERV that when

00:13:58.710 --> 00:14:02.520
we introduced FireWire 800,
we made sure that there were two ports

00:14:02.520 --> 00:14:06.570
of FireWire 800 on the back so that we
could chain down the back of a small

00:14:06.570 --> 00:14:08.750
cluster and have FireWire connectivity.

00:14:09.480 --> 00:14:13.030
Now, FireWire has a lot of interesting
properties as an interconnect network.

00:14:13.110 --> 00:14:15.200
It has extremely low latencies.

00:14:15.200 --> 00:14:20.710
It has DMA capability between nodes,
which means one node can DMA memory out

00:14:20.710 --> 00:14:25.180
of another node without any processor
intervention by the secondary host.

00:14:25.180 --> 00:14:28.660
We also have in the latest
revisions of Mac OS X Server have

00:14:28.660 --> 00:14:30.380
added IP over FireWire.

00:14:30.380 --> 00:14:34.100
So we have the ability to
run standard IP networking.

00:14:34.100 --> 00:14:37.300
And any application that
takes advantage of standard

00:14:37.300 --> 00:14:39.400
IP stacks can take advantage of.

00:14:39.440 --> 00:14:43.460
The FireWire network built
right into Mac OS X Server.

00:14:43.460 --> 00:14:46.750
So this becomes very interesting, again,
for the cost of a FireWire

00:14:46.750 --> 00:14:48.220
9-pin to 9-pin cable.

00:14:48.220 --> 00:14:50.990
Chain down a small cluster, you know,
somewhere between four

00:14:50.990 --> 00:14:52.060
and eight machines.

00:14:52.060 --> 00:14:53.160
This is really ideal.

00:14:53.160 --> 00:14:55.160
No additional switching costs.

00:14:58.030 --> 00:15:01.490
And of course,
the workhorse of the cluster is

00:15:01.490 --> 00:15:03.620
of course the compute elements.

00:15:03.740 --> 00:15:10.410
So any number of compute elements can be
added to the environment and scale that

00:15:10.420 --> 00:15:13.830
up to your specific tasks and problems.

00:15:15.580 --> 00:15:17.690
So the way we'd see this
deployed with XServe hardware is,

00:15:17.690 --> 00:15:22.960
of course, an XServe standard server
configuration as the head node,

00:15:23.190 --> 00:15:26.020
providing storage and network access.

00:15:26.020 --> 00:15:29.210
Optionally,
an XServe RAID for up to two and a half

00:15:29.210 --> 00:15:34.100
terabytes of RAID-protected storage,
again, through fiber channel,

00:15:34.100 --> 00:15:36.700
which are represented by
the heavy white lines.

00:15:38.040 --> 00:15:43.580
An XServe compute node configurations,
and, of course,

00:15:43.590 --> 00:15:48.480
in this particular example,
a 10100 Ethernet switch.

00:15:51.920 --> 00:15:54.800
So if we look beyond the hardware now,
let's talk about some of the other

00:15:54.800 --> 00:15:58.100
issues that we have to deal with
when we look at deploying clusters,

00:15:58.100 --> 00:16:03.320
which are some of the most interesting
things from a XSERVE perspective,

00:16:03.320 --> 00:16:05.130
which is deployment and management.

00:16:05.330 --> 00:16:08.760
So obviously the first things we have
to look at are physical concerns.

00:16:08.760 --> 00:16:10.900
You know,
where is this equipment going to live?

00:16:11.050 --> 00:16:14.730
What are the power, environmental,
and networking requirements?

00:16:14.920 --> 00:16:19.760
And also logical concerns, you know,
software installation, configuration,

00:16:19.760 --> 00:16:21.300
management.

00:16:21.490 --> 00:16:23.960
So let's take each of these individually.

00:16:23.960 --> 00:16:26.500
So let's first talk about
power environmentals.

00:16:26.500 --> 00:16:30.160
So Xserve actually has quite
an interesting advantage in

00:16:30.160 --> 00:16:36.640
that with the G4 processor,
it has quite a low power consumption

00:16:36.850 --> 00:16:42.300
and heat output compared to other
competing processors in its class.

00:16:42.300 --> 00:16:46.300
So when we look at the Xserve,
off the back of the data sheet,

00:16:46.300 --> 00:16:49.730
we rate it at 3.6 amps at 345 watts.

00:16:49.800 --> 00:16:53.400
Now this, I should add, is for a fully,
fully loaded system,

00:16:53.400 --> 00:16:56.760
if you stuff every possible
thing you could in the box,

00:16:56.760 --> 00:16:59.900
and that actually has
margin on top of that.

00:16:59.950 --> 00:17:03.340
What we've actually found is that when
you actually measure real-world usage,

00:17:03.340 --> 00:17:07.820
when you max the processor at 100%,
running velocity engine optimized code,

00:17:07.820 --> 00:17:10.460
really leveraging the processing
power out of the Xserve,

00:17:10.500 --> 00:17:16.440
you're going to really have to work
hard to draw around 2 amps at 134 watts.

00:17:16.440 --> 00:17:19.500
So the actual real-world
power consumption,

00:17:19.700 --> 00:17:23.040
is actually much lower than
the actual system rating.

00:17:23.140 --> 00:17:26.650
And we actually publish these
numbers in our knowledge base,

00:17:26.670 --> 00:17:29.190
kbase.info.apple.com.

00:17:29.210 --> 00:17:35.940
And we also publish BTUs per hour,
which is just shy of 460 BTUs an

00:17:35.940 --> 00:17:39.470
hour for a dual processor system.

00:17:41.620 --> 00:17:44.330
So for one system,
that's not too much of a challenge.

00:17:44.340 --> 00:17:46.600
You can pretty much plug an
XServe in just about anywhere

00:17:46.600 --> 00:17:47.490
and not have a problem.

00:17:47.500 --> 00:17:50.360
But what happens is when you
rack a whole bunch of these in a

00:17:50.360 --> 00:17:55.090
space and multiplying these out,
you've got to really take into account

00:17:55.130 --> 00:17:58.300
both power and heat requirements.

00:17:58.430 --> 00:18:03.350
So if you multiply those by 16,
you start getting nearly 60 amps

00:18:03.350 --> 00:18:06.720
and about 5,500 watts of power.

00:18:06.740 --> 00:18:10.060
Obviously much lower than that
in real world consumption.

00:18:10.060 --> 00:18:12.350
But you do have to plan
up for startup currents,

00:18:12.350 --> 00:18:15.380
which are actually close
to rated consumption.

00:18:15.480 --> 00:18:18.670
And one of the things that you can
do is actually stagger the startup

00:18:18.760 --> 00:18:24.700
in the cluster to prevent maximum
current load at power up time.

00:18:24.910 --> 00:18:29.290
And of course, over 7,000 BTUs an hour.

00:18:29.380 --> 00:18:33.240
So from an environmental space,
making sure you have adequate cooling for

00:18:33.410 --> 00:18:37.110
a room that's going to host a cluster is,
of course, critical.

00:18:37.510 --> 00:18:40.670
So, you know, the big things here is,
of course, that obviously you need to

00:18:40.780 --> 00:18:41.830
plan for these requirements.

00:18:41.860 --> 00:18:46.180
You know, if your, you know,
UPS becomes critical and being able

00:18:46.180 --> 00:18:47.960
to plan appropriately for that.

00:18:47.960 --> 00:18:51.150
One of the strategies that we're
starting to see is typically

00:18:51.150 --> 00:18:55.100
the most important element in a
compute cluster is providing power,

00:18:55.100 --> 00:18:58.090
backup power for the
head node in the storage.

00:18:58.320 --> 00:19:01.080
So,
very rarely do you actually put all the

00:19:01.250 --> 00:19:07.010
elements on protected backup power since
if you have resource management software,

00:19:07.010 --> 00:19:11.090
any terminated computation
will be restarted automatically

00:19:11.090 --> 00:19:13.290
when an element's available.

00:19:13.300 --> 00:19:16.900
So, the head node is really the
critical piece in this whole puzzle.

00:19:19.740 --> 00:19:22.980
So the next thing becomes
an issue of the networking.

00:19:22.980 --> 00:19:28.290
What kind of problems do you have and
what kind of interconnects are required

00:19:28.470 --> 00:19:30.420
to be able to manage this cluster?

00:19:30.420 --> 00:19:34.500
And so one of the big
factors becomes how much I.O.

00:19:34.500 --> 00:19:40.160
does the particular compute task do and
whether you get the bang for the buck

00:19:40.240 --> 00:19:46.700
deploying gigabit Ethernet as an example
across a series of compute elements.

00:19:47.360 --> 00:19:51.500
If there's heavy I.O., there might
be dramatic advantages to adding that

00:19:51.560 --> 00:19:53.590
kind of network behind the scenes.

00:19:53.600 --> 00:19:57.520
Other types of compute
jobs don't require that.

00:19:57.520 --> 00:20:01.960
It's very compute focused and sits in
processes on a small amount of data.

00:20:01.960 --> 00:20:05.280
And a great example of that,
kind of a more dramatic example of that,

00:20:05.280 --> 00:20:07.500
is, for example, SETI at Home, right?

00:20:07.500 --> 00:20:09.810
If you've ever run the
SETI at Home client,

00:20:09.810 --> 00:20:12.260
even over a very low
speed modem connection,

00:20:12.260 --> 00:20:16.300
it will download a block of data and
it will sit and churn on it for hours.

00:20:17.120 --> 00:20:20.410
So having a high performance
network back to the machine doling

00:20:20.410 --> 00:20:22.340
out the work is not real critical.

00:20:22.360 --> 00:20:24.830
However, other problems,
that's not the case.

00:20:24.980 --> 00:20:29.610
The other question
becomes that of latency.

00:20:29.760 --> 00:20:32.690
A lot of computational problems are
what you might call embarrassingly

00:20:32.750 --> 00:20:36.500
parallel in that a job can be sent
out across a whole bunch of nodes

00:20:36.500 --> 00:20:40.130
and with no dependencies on it,
they'll just sit and churn

00:20:40.130 --> 00:20:41.140
and then return the results.

00:20:41.210 --> 00:20:44.330
Other problems have very tight
dependencies that the results of

00:20:44.330 --> 00:20:47.680
one computation will get fed into
the results of another computation

00:20:47.680 --> 00:20:49.500
and they'll be tightly coupled.

00:20:49.500 --> 00:20:53.930
And so having low latency,
not necessarily bandwidth,

00:20:53.930 --> 00:20:58.010
but low latency between
the machines becomes very,

00:20:58.010 --> 00:20:59.450
very critical.

00:20:59.620 --> 00:21:03.550
And this is where solutions like
MirrorNet become important because

00:21:03.550 --> 00:21:05.060
of that low latency interconnect.

00:21:05.060 --> 00:21:07.150
And of course,
the other thing that you'll

00:21:07.230 --> 00:21:10.560
always want to manage is just
who can connect to this cluster.

00:21:10.560 --> 00:21:13.420
And typically,
access is provided through the head

00:21:13.420 --> 00:21:15.760
node and secured through the head node.

00:21:15.760 --> 00:21:19.510
So you authenticate into the head node,
submit your job to the head node,

00:21:19.510 --> 00:21:22.060
and the work begins to
be managed from there.

00:21:24.690 --> 00:21:27.130
Let's take a look at some
of the logical concerns,

00:21:27.130 --> 00:21:28.420
the management.

00:21:28.420 --> 00:21:32.700
And this is an area where we actually
have a lot of advantages with the tools

00:21:32.700 --> 00:21:34.400
that are provided in Mac OS X Server.

00:21:34.400 --> 00:21:37.090
What's interesting is that
a lot of the tools that we

00:21:37.090 --> 00:21:40.520
provide in Mac OS X Server for
desktop management become very

00:21:40.590 --> 00:21:42.730
applicable for cluster management.

00:21:42.740 --> 00:21:49.000
And so tools like cloning tools, Netboot,
and network install become very,

00:21:49.000 --> 00:21:51.960
very valuable in cluster management.

00:21:52.510 --> 00:21:56.170
The reality is that for a cluster,
more often than not,

00:21:56.300 --> 00:22:00.670
every compute element needs to
have the exact same system image,

00:22:00.700 --> 00:22:02.140
exact same access to tools.

00:22:02.140 --> 00:22:06.400
And so Netboot becomes a very,
very viable way to manage and

00:22:06.420 --> 00:22:09.160
have basically a single system
image across your entire cluster.

00:22:09.160 --> 00:22:12.550
And actually,
when we add some of the headless

00:22:12.550 --> 00:22:17.900
tools that we provide out of the box
with Mac OS X Server and with XServe,

00:22:17.900 --> 00:22:20.400
literally you can take a brand
new XServe out of the box,

00:22:20.400 --> 00:22:24.120
hold down a button on the front panel,
and have it Netboot off your head node.

00:22:24.120 --> 00:22:27.870
It really provides quick
out-of-the-box deployment for XServe.

00:22:29.160 --> 00:22:31.290
Of course,
remote management tools are essential.

00:22:31.300 --> 00:22:34.240
Again, clusters are meant to live,
interact somewhere.

00:22:34.240 --> 00:22:36.230
You should be able to
access them from anywhere.

00:22:36.400 --> 00:22:39.200
You may choose to provide
command line access with things

00:22:39.200 --> 00:22:42.380
like SSH or provide web tools,
so provide web interfaces.

00:22:42.380 --> 00:22:46.180
And you'll see some examples
of that in this session.

00:22:46.400 --> 00:22:49.080
And finally, user accessibility.

00:22:49.250 --> 00:22:51.730
I actually just touched on that.

00:22:51.740 --> 00:22:54.140
You know,
web interface and terminal access.

00:22:54.140 --> 00:22:55.080
And finally, backup.

00:22:55.160 --> 00:22:57.930
Being able to have a backup
strategy for the head node,

00:22:57.940 --> 00:22:59.690
being able to back up that critical data.

00:22:59.870 --> 00:23:02.880
Again, typically in these deployments,
the compute elements themselves

00:23:02.880 --> 00:23:04.410
are thought of as disposable.

00:23:04.550 --> 00:23:08.590
They're quick to replace or re-image.

00:23:08.720 --> 00:23:10.650
It's really the head node
that becomes critical in the

00:23:10.680 --> 00:23:13.970
storage that's provided there,
particularly the computation

00:23:13.970 --> 00:23:15.640
programs and the results.

00:23:18.020 --> 00:23:21.280
So before I introduce my next speaker,
I wanted to highlight and we'll

00:23:21.330 --> 00:23:23.920
kind of bring up some of the things
that we typically say for the end of

00:23:23.920 --> 00:23:25.420
these presentations up to the front.

00:23:25.420 --> 00:23:29.220
I wanted to really highlight some of the
key cluster resources that are available

00:23:29.340 --> 00:23:31.260
for Mac OS X and Mac OS X Server.

00:23:31.260 --> 00:23:37.180
In the SciTech lunch yesterday,
someone asked about Apple providing

00:23:37.180 --> 00:23:42.320
an MPI stack and whether they
thought that was important.

00:23:42.400 --> 00:23:45.780
I think one of the interesting things
I think you'll see here is that this

00:23:45.780 --> 00:23:49.180
is an area where we have actually a
wealth of solutions to choose from.

00:23:49.180 --> 00:23:53.030
And we actually prefer having a large
number of excellent open source and

00:23:53.030 --> 00:23:55.140
third-party solutions available.

00:23:55.140 --> 00:23:59.790
So if we look at some of the examples,
one of the keys of any cluster

00:23:59.810 --> 00:24:04.110
deployment is DRM software,
Digital Distributed Resource Management.

00:24:04.120 --> 00:24:07.230
And we have a number of
solutions to choose from.

00:24:07.240 --> 00:24:12.360
Platform LSF, SunGrid Engine, OpenPBS,
PBS Pro.

00:24:12.400 --> 00:24:14.640
Some of the top examples.

00:24:14.640 --> 00:24:17.560
High-performance computing tools.

00:24:17.560 --> 00:24:23.530
So again, in the MPI stack area, MPitch,
MPI Pro, LAMMPI, Lyndon Paradise,

00:24:23.530 --> 00:24:27.540
and Pooch from Dogger
Research are all great examples

00:24:27.540 --> 00:24:29.980
of solutions in this space.

00:24:32.080 --> 00:24:34.120
all available for Mac OS X.

00:24:34.120 --> 00:24:39.080
Grid Computing Tools, Gridiron Software,
a really excellent piece of

00:24:39.100 --> 00:24:41.880
distributed application resource.

00:24:41.880 --> 00:24:47.480
Globus Toolkit was actually ported
to Mac OS X from the bio team and

00:24:47.480 --> 00:24:50.550
has a port available from them.

00:24:51.100 --> 00:24:53.140
Science and Research Applications.

00:24:53.140 --> 00:24:56.160
You'll see a demo of Grid Mathematica
a little later in this session.

00:24:56.160 --> 00:25:00.220
Excellent tool for a number
of kinds of computation.

00:25:00.220 --> 00:25:05.900
Life Science Applications, Turbo Blast,
Turbo Hub, Turbo Bench from Turbo Works.

00:25:06.720 --> 00:25:09.670
Bioinformatics Toolkit,
which is a set of life

00:25:09.700 --> 00:25:13.280
sciences applications that
have been ported to Mac OS X,

00:25:13.280 --> 00:25:17.720
kind of a single-click,
double-clickable installer for Mac OS X.

00:25:17.720 --> 00:25:21.380
And Inquiry Bioinformatics,
those same solutions wrapped up into

00:25:21.390 --> 00:25:23.520
a really easy deployment solution.

00:25:23.550 --> 00:25:24.920
You'll hear more about that in a minute.

00:25:26.840 --> 00:25:30.330
So, with that said,
I'd like to introduce our next speaker,

00:25:30.370 --> 00:25:33.140
Michael Athanas from the bio team.

00:25:33.140 --> 00:25:35.610
Michael is a principal investigator
and a founding partner and is

00:25:35.610 --> 00:25:42.180
going to talk to you a little bit
more about accessible clustering.

00:25:42.180 --> 00:25:42.180
Michael.

00:25:46.520 --> 00:25:47.500
Thanks.

00:25:47.500 --> 00:25:49.000
Does this work?

00:25:49.000 --> 00:25:52.780
Again, my name is Michael Athanas,
and I'm a scientist and co-founder

00:25:52.780 --> 00:25:57.500
of a bioinformatic consulting
group called the BioTeam.

00:25:57.620 --> 00:26:01.510
One of the interesting things
I learned from one of my hosts here

00:26:01.520 --> 00:26:06.500
at the conference is that one out
of nine attendees is a scientist.

00:26:06.500 --> 00:26:07.500
Is that true?

00:26:07.620 --> 00:26:10.050
Scientists here?

00:26:10.820 --> 00:26:15.400
I'm actually quite impressed
at the way this platform is,

00:26:15.690 --> 00:26:19.600
it seems to resonate with
the scientific community.

00:26:19.600 --> 00:26:25.190
It's very well matched.

00:26:25.190 --> 00:26:25.190
Anyway,

00:26:26.600 --> 00:26:34.600
So what motivates me as a bioinformatics
consultant in the morning is how to

00:26:34.600 --> 00:26:39.400
take advantage of boundless computing.

00:26:39.400 --> 00:26:42.400
In this presentation,
I'm going to briefly talk about some of

00:26:42.400 --> 00:26:46.870
the pressures in life science computing,
just expand a little bit

00:26:46.950 --> 00:26:50.100
beyond what Doug talked about.

00:26:50.120 --> 00:26:55.260
Briefly talk about computing
solutions with emphasis on clustering,

00:26:55.260 --> 00:26:58.100
and then talk about instant clustering.

00:26:58.100 --> 00:27:01.090
I'll explain that as we go along.

00:27:01.600 --> 00:27:07.750
Quickly, the BioTeam is a group of
scientists focused upon

00:27:07.750 --> 00:27:11.500
delivering life science solutions.

00:27:11.500 --> 00:27:15.620
The group,
what makes us somewhat unique is

00:27:15.620 --> 00:27:19.500
that we're somewhat vendor agnostic.

00:27:19.500 --> 00:27:26.490
We work with all sorts of platforms,
including Apple platforms as well.

00:27:26.500 --> 00:27:30.810
The principals of BioTeam have
been working together on several

00:27:30.810 --> 00:27:35.240
projects over the past few years,
and most recently a great

00:27:35.270 --> 00:27:37.500
deal of projects with Apple.

00:27:37.500 --> 00:27:42.880
This list here shows some of the
clients that we've worked with,

00:27:42.940 --> 00:27:46.940
and actually I've bumped into
representatives of some of these

00:27:46.940 --> 00:27:50.200
organizations at this conference as well.

00:27:53.700 --> 00:27:57.710
So again,
what motivates me as a bioinformatics

00:27:57.710 --> 00:28:02.420
consultant is to get my clients
to think about what you could

00:28:02.420 --> 00:28:04.260
do with boundless computing.

00:28:04.300 --> 00:28:09.990
That is, what if CPU was not a limitation
in modeling and simulation?

00:28:10.050 --> 00:28:15.700
What if you had very fast access
to terabytes of information?

00:28:15.730 --> 00:28:22.790
And what's the most appropriate way
to visualize the knowledge that is

00:28:22.790 --> 00:28:25.820
derived from these data analyses?

00:28:26.800 --> 00:28:32.920
In doing so,
the computing has to be accessible.

00:28:33.020 --> 00:28:36.340
And in order to do that,
some level of abstraction

00:28:36.340 --> 00:28:38.690
has to be defined.

00:28:39.090 --> 00:28:42.140
Apple seems to be great
at this in terms of,

00:28:42.140 --> 00:28:46.170
for example, the finder,
the emphasis is on the user experience

00:28:46.170 --> 00:28:50.580
as opposed to the nuts and bolts
of the computing behind the scenes.

00:28:50.850 --> 00:28:53.740
It's true to enable
scientific computing as well.

00:28:53.740 --> 00:28:58.700
It's not about the computers,
it's about the applications and pipelines

00:28:59.430 --> 00:29:03.300
involved in the scientific computation.

00:29:03.720 --> 00:29:09.300
So what is important from a scientist
perspective is quick data access,

00:29:09.500 --> 00:29:14.220
reliable fast execution,
and application interoperability.

00:29:14.230 --> 00:29:18.600
What is not so important in
order to carry out science is the

00:29:18.600 --> 00:29:22.880
nuts and bolts of the computing,
you know, the details of the storage,

00:29:23.080 --> 00:29:27.320
how the storage is laid out,
or even what type of processor is used.

00:29:27.430 --> 00:29:31.160
It really doesn't matter,
and it shouldn't matter from

00:29:31.190 --> 00:29:33.260
a scientific perspective.

00:29:36.900 --> 00:29:44.300
[Transcript missing]

00:29:44.640 --> 00:29:49.480
computing problems and clustering
can be an appropriate solution.

00:29:49.480 --> 00:29:53.490
There are benefits to clustering,
as Doug pointed out,

00:29:53.490 --> 00:29:57.060
and perhaps some arguments
against clustering.

00:29:57.060 --> 00:30:03.360
And I'm going to augment what Doug was
talking about in terms of why clustering.

00:30:03.360 --> 00:30:08.170
Well, I think one of the most
compelling reasons to go for

00:30:08.210 --> 00:30:12.940
clustering is scalability,
because in terms of scientific research,

00:30:12.940 --> 00:30:17.520
it's very difficult to forecast what
you're going to be doing tomorrow.

00:30:19.430 --> 00:30:23.640
Clustering inherently is
a blueprint for growth.

00:30:23.640 --> 00:30:27.880
If you architect the system properly,
you can increase your computing power

00:30:27.880 --> 00:30:31.750
in step with your computational need.

00:30:34.730 --> 00:30:39.890
Another compelling reason is price
performance of commodity hardware.

00:30:39.950 --> 00:30:45.430
We've seen that, you know,
with the announcement of the G5

00:30:45.550 --> 00:30:50.050
dual processor box for only $3,000,

00:30:50.410 --> 00:30:53.200
That same computing power,
if I wanted to buy it

00:30:53.200 --> 00:30:55.540
four or five years ago,
would probably be tens,

00:30:55.540 --> 00:30:59.300
if not a hundred thousand
dollars for the same thing.

00:30:59.300 --> 00:31:02.300
Computing is getting ridiculously cheap.

00:31:02.300 --> 00:31:04.320
If you look at the curve,
they're going to be giving

00:31:04.320 --> 00:31:05.300
it away pretty soon.

00:31:05.300 --> 00:31:08.600
So the trick is,
how do we take advantage of it?

00:31:12.200 --> 00:31:22.600
[Transcript missing]

00:31:22.770 --> 00:31:27.570
Construct your architecture based
upon the scientific demands of the

00:31:27.570 --> 00:31:30.060
applications in your infrastructure.

00:31:30.380 --> 00:31:33.710
There are many parameters that you
can tweak from a hardware perspective.

00:31:33.840 --> 00:31:39.780
For example, as Doug was pointing out,
there are network alternatives.

00:31:39.930 --> 00:31:42.690
That decision is based
upon the applications that

00:31:42.760 --> 00:31:45.620
are within your workflow.

00:31:45.730 --> 00:31:48.140
Also, there are storage options as well.

00:31:48.310 --> 00:31:52.910
Do you take advantage of local
caching on the individual nodes,

00:31:52.920 --> 00:31:59.480
or do you use some kind of
network or SAN available storage?

00:31:59.540 --> 00:32:03.820
And even the processors,
there are some applications may take

00:32:03.820 --> 00:32:09.760
advantage of different processors
or accelerators better than others.

00:32:09.760 --> 00:32:16.900
And I think one of the interesting
flexibility issues that we're

00:32:16.900 --> 00:32:16.900
seeing in the industry is

00:32:17.140 --> 00:32:23.890
A compelling reason is clusters kind of
transcend the single vendor solution.

00:32:23.990 --> 00:32:29.210
You're allowed to build a cluster
of components that are optimal to

00:32:29.210 --> 00:32:34.770
your workflow as opposed to a single
vendor providing everything which

00:32:34.950 --> 00:32:38.280
some of the components may be ideal,
some may not.

00:32:39.920 --> 00:32:42.890
Reliability is another
reason for clustering,

00:32:42.990 --> 00:32:44.830
and this may not be so obvious.

00:32:44.940 --> 00:32:47.500
Doug pointed this out a little bit,
but with careful

00:32:47.500 --> 00:32:51.660
architecture of a cluster,
careful identification of

00:32:51.710 --> 00:32:54.890
single points of failure,
your cluster can have

00:32:54.890 --> 00:32:59.410
extremely high availability,
high uptime.

00:33:00.040 --> 00:33:04.610
The architecture can be such
that if a compute element dies,

00:33:04.610 --> 00:33:09.820
you just pop it out like you would
replace a light bulb in your home.

00:33:09.820 --> 00:33:13.300
You wouldn't have to shut down the
grid of your home and unsolder the

00:33:13.310 --> 00:33:15.320
light bulb and solder a new one in.

00:33:15.360 --> 00:33:18.700
You just unscrew it and plug it in.

00:33:21.280 --> 00:33:29.000
But clustering is not appropriate for
all applications or all workflows or...

00:33:30.000 --> 00:33:31.540
or types of scientific research.

00:33:31.580 --> 00:33:35.820
Not all applications map to
loosely coupled architectures.

00:33:35.820 --> 00:33:41.610
An example of this could be
relational database engines.

00:33:41.990 --> 00:33:46.290
Another reason why clustering
may not work out very well

00:33:46.330 --> 00:33:48.300
is management complexity.

00:33:48.350 --> 00:33:53.790
If you don't pay careful attention
to the initial architecture,

00:33:53.790 --> 00:33:58.340
the effort required to maintain your
system may scale with the number

00:33:58.400 --> 00:34:00.410
of elements within your cluster.

00:34:00.520 --> 00:34:02.740
That's a sign of failure.

00:34:02.960 --> 00:34:07.700
I think one of the more compelling
reasons why clustering can be difficult

00:34:07.800 --> 00:34:11.620
is user application complexity.

00:34:11.740 --> 00:34:16.550
You may have a really good application
that runs great on a single processor,

00:34:16.590 --> 00:34:19.280
but you need a thousand times that.

00:34:19.420 --> 00:34:22.600
How do you break that up
and run it in parallel?

00:34:22.820 --> 00:34:26.950
Really depends upon the application,
the tools that were

00:34:27.200 --> 00:34:28.240
used to construct that.

00:34:28.430 --> 00:34:32.200
There isn't a silver bullet that
you can use to automatically

00:34:32.240 --> 00:34:34.100
paralyze an application.

00:34:34.220 --> 00:34:37.300
It still takes special skill to do that.

00:34:37.410 --> 00:34:45.400
Reliability is also on this list because
if the architecture is not correct,

00:34:45.490 --> 00:34:49.470
you can have, and you don't pay attention
to single points of failure,

00:34:49.470 --> 00:34:52.370
it may be very difficult to maintain.

00:34:52.750 --> 00:34:58.600
Also, achieving high utilization seems
to be an important consideration.

00:34:58.600 --> 00:35:00.440
It comes back to the user application.

00:35:00.440 --> 00:35:04.680
If you don't get the degree
of parallelization necessary

00:35:04.680 --> 00:35:08.020
to utilize the cluster,
then you're kind of wasting

00:35:08.020 --> 00:35:10.510
all your compute elements.

00:35:13.350 --> 00:35:19.340
This is a different topological view of
what Doug showed earlier in terms of the,

00:35:19.350 --> 00:35:21.500
we call it the portal architecture.

00:35:21.520 --> 00:35:25.700
And again,
this is a great way of abstracting

00:35:26.290 --> 00:35:32.590
the computing resources from both
from an administrative perspective

00:35:32.630 --> 00:35:34.700
as well as a user perspective.

00:35:34.700 --> 00:35:39.070
Neither administrator or users are
allowed to access the individual

00:35:39.070 --> 00:35:43.990
nodes within the private subnet in
which the cluster elements reside on.

00:35:44.220 --> 00:35:48.980
And because you do this,
then the nodes become anonymous.

00:35:48.990 --> 00:35:53.850
That allows you to replace
them if something goes wrong.

00:35:53.980 --> 00:35:58.440
You guys skip this.

00:36:05.470 --> 00:36:08.350
So I mentioned scalability.

00:36:08.400 --> 00:36:16.340
Again, I think scalability is a crucial
issue that clustering can address.

00:36:16.340 --> 00:36:20.400
But there are many characteristics
of what scalability is.

00:36:20.400 --> 00:36:25.390
Scalability in terms of quantity of data
that you're distributing on the cluster.

00:36:25.440 --> 00:36:29.710
Scalability in terms of number of
users that are hitting the system.

00:36:31.060 --> 00:36:33.780
All those have to be addressed
in terms of the architecture.

00:36:33.890 --> 00:36:38.340
I think one of the more
significant components of achieving

00:36:38.340 --> 00:36:40.300
scalability is fault tolerance.

00:36:40.500 --> 00:36:45.330
How is your system going to be
aware of some kind of adverse

00:36:45.590 --> 00:36:47.990
event within your cluster?

00:36:48.240 --> 00:36:51.230
The flip side of fault
tolerance is automation.

00:36:51.520 --> 00:36:55.090
How are you going to respond to
that adverse condition so that

00:36:55.090 --> 00:37:00.570
you can continue to process so you
don't need extensive management

00:37:00.670 --> 00:37:06.090
or monitoring capability to ensure
completion of your workflow?

00:37:09.610 --> 00:37:14.930
Okay, so I wanted to contrast two
different approaches for computing.

00:37:15.100 --> 00:37:24.600
The mainframe SMP monolithic approach
compared to the clustering approach.

00:37:24.920 --> 00:37:32.530
Again, one thing that's going against
the clustering approach

00:37:32.530 --> 00:37:35.390
is application complexity.

00:37:35.930 --> 00:37:40.040
It's definitely more difficult to
make full usage of a cluster than

00:37:40.060 --> 00:37:42.680
if you had an SMP type machine.

00:37:42.810 --> 00:37:47.170
However, what's going against the
mainframe SMP approach to

00:37:47.180 --> 00:37:50.700
computing is the upfront cost.

00:37:51.010 --> 00:37:56.420
A mainframe type system with comparable
compute power of a cluster can be

00:37:56.590 --> 00:37:59.600
about 4 to 20 times more expensive.

00:38:00.840 --> 00:38:03.230
As I mentioned,
a cluster architecture can

00:38:03.230 --> 00:38:10.450
give you better scalability,
but countering the upfront cost of the

00:38:10.450 --> 00:38:16.260
mainframe system is the total cost of
ownership of maintaining that system.

00:38:16.350 --> 00:38:18.840
If you don't architect
your cluster correctly,

00:38:18.890 --> 00:38:21.990
then it can be extremely
expensive to maintain.

00:38:22.160 --> 00:38:29.720
So if you can address the application
complexity and the management complexity,

00:38:29.920 --> 00:38:33.880
then the clustering solution
can be very compelling.

00:38:36.770 --> 00:38:38.490
Okay,
I'm going to switch gears a little bit

00:38:38.500 --> 00:38:41.980
and talk about the BioTeam Inquiry.

00:38:42.070 --> 00:38:48.740
The Inquiry was an award last night,
an Apple Designer Award in

00:38:48.740 --> 00:38:51.610
the category of Server.

00:38:52.280 --> 00:38:55.890
and David So what is inquiry?

00:38:55.900 --> 00:39:00.360
The concept behind inquiry is
instant scalable informatics,

00:39:00.360 --> 00:39:02.540
just add hardware.

00:39:02.540 --> 00:39:09.230
So the concept is to provide a full
functioning informatics solution and

00:39:09.500 --> 00:39:12.420
you start out with an empty cluster.

00:39:12.420 --> 00:39:17.730
And the fun part is that we can
do this in about 20 minutes.

00:39:21.300 --> 00:39:27.290
So what we do with inquiry,
we've deployed many clusters,

00:39:27.460 --> 00:39:30.980
many types of clusters,
but there's some common denominator

00:39:31.040 --> 00:39:33.180
that we see in our deployments.

00:39:33.180 --> 00:39:37.430
We've taken these best practices
in terms of network configuration,

00:39:37.430 --> 00:39:44.040
OS configuration, various optimizations,
deploying the right administration tools,

00:39:44.040 --> 00:39:47.340
monitoring tools,
but we don't stop there.

00:39:47.520 --> 00:39:51.070
If we stop there,
that would be a fine cluster that

00:39:51.120 --> 00:39:54.900
you can use from an IT basis,
but we go beyond that.

00:39:54.990 --> 00:39:57.580
Our goal is to enable the scientists,
in this case,

00:39:57.580 --> 00:39:59.440
the bioinformatics scientists.

00:39:59.440 --> 00:40:05.380
The inquiry cluster is loaded with
more than 200 open source applications,

00:40:05.410 --> 00:40:08.850
which are all cluster enabled,
and we provide a

00:40:08.850 --> 00:40:11.930
consistent user interface,
a web interface to all

00:40:11.930 --> 00:40:13.040
these applications.

00:40:13.860 --> 00:40:19.560
And on top of that, we deploy about 100
gigabytes of genomic data.

00:40:19.560 --> 00:40:22.880
So as soon as the cluster is up,
you're ready to fire.

00:40:25.350 --> 00:40:30.150
So the idea is to go from the
many computer concept to a

00:40:30.150 --> 00:40:36.750
single virtual computing resource
that is usable by a scientist.

00:40:38.320 --> 00:40:39.950
But we go beyond that.

00:40:40.030 --> 00:40:43.190
Like I said before,
we don't want to necessarily train

00:40:43.190 --> 00:40:45.560
scientists to become computer scientists.

00:40:45.700 --> 00:40:51.320
We want to empower them to go from,
to abstract the command line into

00:40:51.330 --> 00:40:54.390
something that is more accessible.

00:40:58.200 --> 00:41:05.320
So Inquiry is an orchestration of
many open source tools and utilities.

00:41:05.320 --> 00:41:09.110
Just to mention a couple of
the components behind Inquiry.

00:41:09.160 --> 00:41:11.340
The first one is Pies.

00:41:11.480 --> 00:41:15.380
Pies is a very cool tool
from the Pasteur Institute.

00:41:15.380 --> 00:41:20.180
The heart of Pies is essentially
a collection of XML documents

00:41:20.320 --> 00:41:24.680
describing a bunch of command
line bioinformatic tools.

00:41:25.310 --> 00:41:31.880
Starting from this set of XML documents,
we can render an interface,

00:41:31.880 --> 00:41:38.070
whether it be a web interface
or a web services interface.

00:41:39.480 --> 00:41:43.100
So now that we've
presented the application,

00:41:43.190 --> 00:41:47.480
we connect the execution
of that application to the

00:41:47.480 --> 00:41:51.640
cluster using Sun Grid Engine,
or we can use platform LSF.

00:41:51.720 --> 00:41:55.430
And that's all completely
abstracted away from the user

00:41:55.490 --> 00:41:57.950
and integrated within Inquiry.

00:41:59.000 --> 00:42:04.510
We've also deployed several
monitoring and administrative

00:42:04.510 --> 00:42:08.900
tools that are commonly available
in the open source domain.

00:42:08.900 --> 00:42:13.900
For example, this is Ganglia,
and it provides a very nice snapshot

00:42:13.900 --> 00:42:19.900
of your cluster and allows you to drill
down to get more detail of the health

00:42:19.900 --> 00:42:22.700
of various nodes within your cluster.

00:42:24.080 --> 00:42:27.320
In addition,
we provide another perspective of your

00:42:27.380 --> 00:42:30.160
cluster from the load management system.

00:42:30.160 --> 00:42:33.360
So how are jobs running on your system?

00:42:33.360 --> 00:42:35.180
Who's running jobs?

00:42:35.180 --> 00:42:38.100
And what jobs are pending?

00:42:38.100 --> 00:42:43.510
And everything from the user
job submission perspective.

00:42:46.350 --> 00:42:49.570
Because we're using PI's,
we have a great deal of flexibility

00:42:49.590 --> 00:42:55.420
in terms of how the application
interfaces are presented to the user.

00:42:55.420 --> 00:43:01.650
For each application within Inquire,
we provide two interfaces.

00:43:01.720 --> 00:43:04.710
A simple view,
which gives you just the bare

00:43:04.710 --> 00:43:08.580
bones of what you need in order
to execute that application.

00:43:08.640 --> 00:43:12.340
In addition,
we provide an expert view with all the

00:43:12.350 --> 00:43:17.670
bells and whistles of that application,
along with complete documentation

00:43:17.810 --> 00:43:23.410
for each of those flags,
for every application within Inquire.

00:43:26.550 --> 00:43:30.470
Also within Inquiry,
we manage results that are generated

00:43:30.640 --> 00:43:36.410
from the various applications.

00:43:36.640 --> 00:43:40.260
Results calculated at different
times are accessible and can be

00:43:40.310 --> 00:43:47.820
retrieved and either piped into other
applications or just reexamined.

00:43:48.590 --> 00:43:53.220
Okay,
so one of the fun things about Inquiry,

00:43:53.220 --> 00:43:58.660
like I said, we can deploy this
within 15 or 20 minutes.

00:43:58.660 --> 00:44:00.340
And this is Inquiry.

00:44:00.340 --> 00:44:02.240
We deploy it on an iPod.

00:44:02.240 --> 00:44:02.920
Okay?

00:44:02.920 --> 00:44:06.530
And the idea behind that
is first we take the iPod,

00:44:06.550 --> 00:44:10.440
we plug it into the head node,
and we mount the iPod and

00:44:10.440 --> 00:44:14.860
run an application called the
Cluster Configuration Tool,

00:44:14.860 --> 00:44:16.280
as shown here.

00:44:16.940 --> 00:44:22.990
And this tool provides a way of setting
the number of nodes in the cluster,

00:44:23.130 --> 00:44:27.920
external IP addresses,
just the external things that is

00:44:27.920 --> 00:44:30.960
needed to describe that cluster.

00:44:33.950 --> 00:44:38.260
Step two in configuring your
cluster is then you boot off

00:44:38.260 --> 00:44:40.390
the iPod from the head node.

00:44:40.400 --> 00:44:44.130
And that takes about five or six minutes.

00:44:44.190 --> 00:44:47.450
And when that's done,
images for the entire cluster

00:44:47.610 --> 00:44:50.320
are loaded onto the head node.

00:44:50.390 --> 00:44:52.900
You're essentially done
with the iPod at that point.

00:44:52.900 --> 00:44:59.610
And the third step is to boot
each individual node of the

00:44:59.620 --> 00:45:01.580
cluster from the head node.

00:45:01.710 --> 00:45:08.160
And that can take anywhere from a
few minutes to up to ten minutes,

00:45:08.240 --> 00:45:11.240
because depending upon the network,
as Doug pointed out,

00:45:11.240 --> 00:45:13.480
that you've deployed within the cluster.

00:45:13.480 --> 00:45:15.840
But that can be done in parallel.

00:45:15.840 --> 00:45:19.620
So when you look at the aggregate,
it takes about, you know,

00:45:19.620 --> 00:45:23.260
15-20 minutes before you
have a fully working cluster.

00:45:23.260 --> 00:45:29.570
And that's it.

00:45:38.610 --> 00:45:39.640
Congratulations on your award.

00:45:39.640 --> 00:45:43.860
I'd now like to introduce Theodore Gray,
Director of User Interfaces for Wolfram,

00:45:43.910 --> 00:45:47.800
to talk a little bit about
Grid Mathematica and its solution.

00:45:54.900 --> 00:45:57.220
We have the demo machine.

00:45:57.230 --> 00:46:01.960
Mathematica is a presentation tool,
so we use it instead of Keynote.

00:46:01.960 --> 00:46:04.920
And I just typed in my presentation.

00:46:04.920 --> 00:46:06.640
Isn't that typical?

00:46:06.740 --> 00:46:13.680
Okay, so the first thing I should say is
that this is actually not my talk.

00:46:13.700 --> 00:46:16.390
Ordinarily it would be
given by Roger Germanson,

00:46:16.390 --> 00:46:18.920
our director of R&D,
but he could be here,

00:46:18.920 --> 00:46:23.070
so I'm giving the talk,
which should be interesting.

00:46:23.770 --> 00:46:24.870
At least I didn't have to prepare it.

00:46:24.950 --> 00:46:25.700
That's one plus.

00:46:25.700 --> 00:46:32.380
So Grid Mathematica is basically the
sort of grid version of Mathematica.

00:46:32.380 --> 00:46:34.440
Mathematica itself is
a desktop application.

00:46:34.440 --> 00:46:35.040
You can buy it.

00:46:35.090 --> 00:46:37.560
It's a very general purpose
programming language and

00:46:37.560 --> 00:46:39.160
system for doing mathematics.

00:46:39.160 --> 00:46:42.230
There's a product called
Network Mathematica,

00:46:42.230 --> 00:46:45.300
which is basically a
network license server.

00:46:45.300 --> 00:46:49.690
And there's an application package
called the Parallel Computing Toolkit,

00:46:49.690 --> 00:46:55.700
which is an application pack that lets,
one Mathematica session manage

00:46:55.700 --> 00:46:57.540
multiple ones on a network.

00:46:57.580 --> 00:47:00.620
And then Grid Mathematica
is essentially a marketing

00:47:01.150 --> 00:47:04.550
concept of those two together,
and it's cheaper per node than

00:47:04.560 --> 00:47:06.300
the regular copy of Mathematica.

00:47:06.300 --> 00:47:12.970
But you can actually put together
those different elements separately.

00:47:15.600 --> 00:47:21.570
So basically,
one of the goals of Grid Mathematica is,

00:47:21.760 --> 00:47:23.520
like has been mentioned
several times before here,

00:47:23.520 --> 00:47:29.120
to try to abstract as much as possible
the details of the configuration of

00:47:29.120 --> 00:47:32.120
your cluster and sort of what brand of
computer it is and things like that.

00:47:32.200 --> 00:47:36.610
So from a system point of view,
you think of a cluster in terms

00:47:36.650 --> 00:47:40.440
of you have some processors,
you start processes on them,

00:47:40.440 --> 00:47:42.760
you schedule them, and you exchange data.

00:47:43.520 --> 00:47:47.920
In the Mathematica view,
you think of having kernel processes,

00:47:47.920 --> 00:47:50.170
Mathematica kernel,
that's what we call the

00:47:50.230 --> 00:47:53.860
computational engine,
and you have expressions in

00:47:53.860 --> 00:47:56.780
the Mathematica language that
you want to have evaluated.

00:47:56.780 --> 00:48:02.500
And the sort of grid clustering element
is to distribute those processes,

00:48:02.520 --> 00:48:06.770
those Mathematica expressions,
to different kernels

00:48:06.850 --> 00:48:08.780
running on a cluster.

00:48:10.960 --> 00:48:15.000
And it's sort of,
we try to be buzzword compliant.

00:48:15.060 --> 00:48:19.600
And, you know, we're trying to be

00:48:19.730 --> 00:48:24.440
So we can handle,
I guess we have the same sort

00:48:24.440 --> 00:48:26.800
of general arrangement where
you have a head machine,

00:48:26.830 --> 00:48:30.100
which is the master,
and you have these multiple

00:48:30.100 --> 00:48:32.700
ones which are not accessible
from the outside world.

00:48:32.710 --> 00:48:38.020
And you have Mathematica handling
that communication strictly

00:48:38.020 --> 00:48:42.500
between Mathematica processes,
not involving any other sort of

00:48:42.500 --> 00:48:45.420
resource management software.

00:48:46.170 --> 00:48:50.320
The system is written entirely
in top-level Mathematica code,

00:48:50.320 --> 00:48:54.100
which means it's completely
machine-independent,

00:48:54.100 --> 00:48:58.170
completely platform-independent,
and you're not restricted

00:48:58.270 --> 00:49:01.420
to any of the sort of C data
types or anything like that.

00:49:01.420 --> 00:49:04.970
You can use arbitrary
Mathematica expressions,

00:49:05.220 --> 00:49:08.210
which could be numbers
or arrays of numbers,

00:49:08.210 --> 00:49:12.420
strings,
but also structured symbolic expressions

00:49:12.500 --> 00:49:18.240
that represent either mathematical
objects or protein structure or whatever.

00:49:18.240 --> 00:49:22.220
It's sort of a general thing.

00:49:23.440 --> 00:49:28.350
So it's not just for sort of numerical
or data analysis type things.

00:49:28.770 --> 00:49:34.190
You can do, well, abstract mathematical
sorts of things too.

00:49:34.750 --> 00:49:37.960
The communication between
processes is through MathLink,

00:49:37.960 --> 00:49:42.040
which is our sort of high
level communication protocol.

00:49:42.060 --> 00:49:46.850
It uses whichever of the
underlying protocols you'd like.

00:49:46.920 --> 00:49:50.880
I think in this case we have it
configured to use TCP between nodes,

00:49:50.900 --> 00:49:54.390
but if you have different, you know,
we have devices for various

00:49:54.450 --> 00:49:55.720
different kinds of networks.

00:49:55.750 --> 00:49:59.020
So you can have either relatively
tightly clustered things

00:49:59.020 --> 00:50:02.660
or you could have them on,
you know, distant,

00:50:02.660 --> 00:50:04.220
more loosely clustered things.

00:50:04.830 --> 00:50:09.280
Our,

00:50:09.900 --> 00:50:23.000
[Transcript missing]

00:50:23.870 --> 00:50:25.160
and you can do that.

00:50:25.180 --> 00:50:28.030
Which I have to admit,
I'm not an expert in cluster computing,

00:50:28.030 --> 00:50:31.160
but it sounds great.

00:50:31.160 --> 00:50:34.950
And it also,
the sort of concurrency controlling

00:50:34.950 --> 00:50:39.730
structures deal with a kernel that
dies or doesn't come back or whatever.

00:50:39.730 --> 00:50:44.150
You can sort of shuffle things around,
which we'll also see in a minute.

00:50:46.030 --> 00:50:49.360
Okay, so let's actually do this.

00:50:49.420 --> 00:50:52.320
We're gonna start Mathematica here, yes.

00:50:52.320 --> 00:50:57.800
So these evaluations are
running on the head machine.

00:50:57.800 --> 00:51:00.320
It just told us that the name
of the head machine is XSERV0.

00:51:00.320 --> 00:51:02.800
We'll kind of use this machine
name as a way of telling

00:51:02.800 --> 00:51:04.640
where a calculation is going.

00:51:04.670 --> 00:51:07.400
And this is an OS X version.

00:51:07.460 --> 00:51:10.400
So this is kind of some
configuration which took most

00:51:10.460 --> 00:51:13.160
of yesterday to get right,
but that's because

00:51:13.190 --> 00:51:15.120
I didn't know how to do it.

00:51:16.000 --> 00:51:20.960
The little bit about plugging
in an iPod and it's automatic,

00:51:20.970 --> 00:51:23.820
that would be great.

00:51:24.960 --> 00:51:29.080
And so now what we're going
to do is actually launch all,

00:51:29.200 --> 00:51:34.530
we're launching 10 kernels,
and that's because there's five

00:51:34.530 --> 00:51:37.430
machines with two processors each,
so we're kind of putting one

00:51:37.430 --> 00:51:38.660
process on each computer.

00:51:38.660 --> 00:51:40.620
So that's finished.

00:51:40.640 --> 00:51:41.920
Now we're going to do a little thing.

00:51:41.920 --> 00:51:45.960
This command says,
take this expression and evaluate

00:51:45.960 --> 00:51:48.060
it on each of the clients.

00:51:49.480 --> 00:51:54.530
So you see it's returned XSERV 1, 2, 3,
4, 5 twice,

00:51:54.530 --> 00:51:57.520
and each of them is a Mac OS version.

00:51:57.540 --> 00:52:00.350
So I should note at this
point that from here on out,

00:52:00.600 --> 00:52:04.510
absolutely nothing would be different
about any of these demos in any way,

00:52:04.510 --> 00:52:07.400
shape,
or form if this had returned a list of,

00:52:07.400 --> 00:52:12.340
you know, Sun and PC or Linux or,
you know, anything else.

00:52:12.450 --> 00:52:15.910
There's absolutely nothing
machine-dependent or hardware-dependent

00:52:15.910 --> 00:52:17.960
or platform-dependent or anything.

00:52:19.480 --> 00:52:22.440
Which, you know, it's sort of,
it's a nice advantage because

00:52:22.470 --> 00:52:25.130
if you've built some cluster
and then you find out that,

00:52:25.130 --> 00:52:28.120
oh, you built a big Linux cluster,
but now you can get Macs

00:52:28.120 --> 00:52:30.700
that are cheaper per,
you know, per CPU cycle,

00:52:30.700 --> 00:52:33.260
you could just add some
Macs to it or vice versa.

00:52:36.940 --> 00:52:44.830
So let me show you some simple examples
of how you actually use the parallels.

00:52:44.840 --> 00:52:47.930
So this is just running
on the local machine,

00:52:47.930 --> 00:52:53.080
and this says run the same
command on this particular node,

00:52:53.080 --> 00:52:56.960
number one,
and this means run it on all of them,

00:52:56.960 --> 00:52:58.280
so we can see.

00:52:58.280 --> 00:53:02.080
And obviously you could put
something more interesting,

00:53:02.080 --> 00:53:06.250
you could do 100 factorial on
each one and get that back.

00:53:06.810 --> 00:53:08.410
Um,

00:53:09.310 --> 00:53:11.530
For those of you who are
familiar with Mathematica,

00:53:11.530 --> 00:53:14.870
this probably will make somewhat
more sense than to those who aren't,

00:53:14.870 --> 00:53:17.950
but this is just showing some
basic Mathematica commands.

00:53:17.960 --> 00:53:22.140
Table builds a table of
expressions like this.

00:53:22.140 --> 00:53:27.400
And here we're building just sort
of to demonstrate a table of machine

00:53:27.400 --> 00:53:30.140
name always on the same machine.

00:53:30.380 --> 00:53:36.080
And now we're going to do it on
farming that out to the processors.

00:53:36.080 --> 00:53:39.900
Now you notice that it's used the
same machine over and over again.

00:53:39.900 --> 00:53:43.760
And that's actually because I discovered
that just a few minutes ago,

00:53:43.830 --> 00:53:45.910
because this command is too fast.

00:53:45.910 --> 00:53:51.260
So it's done before the load management
is basically saying it's done already,

00:53:51.260 --> 00:53:52.360
we'll just do it on the same machine.

00:53:52.360 --> 00:53:55.700
But if we slow this down a little bit,
if we put in, let's say,

00:53:55.710 --> 00:53:59.840
1000 factorial and suppress the output,
it's still too fast.

00:54:00.380 --> 00:54:01.550
Thank you.

00:54:03.180 --> 00:54:06.000
All right, so now it's,
you see it's now sort of distributing

00:54:06.000 --> 00:54:12.760
a little bit more because the processes
are not actually finishing instantly.

00:54:14.600 --> 00:54:16.350
Okay, so another function is map.

00:54:16.350 --> 00:54:21.430
Map takes a function and applies it
to each of the arguments in a list.

00:54:21.440 --> 00:54:24.170
We do the same thing here.

00:54:24.170 --> 00:54:27.840
And again, it's kind of boring
because it's just too fast.

00:54:27.840 --> 00:54:31.580
But you get the idea that many of the
sort of programming constructs that

00:54:31.730 --> 00:54:35.460
you have in Mathematica for building
tables or for applying functions to

00:54:35.460 --> 00:54:38.660
data can be parallelized very easily.

00:54:38.660 --> 00:54:43.050
And as long as there isn't, you know,
a data dependency between the

00:54:43.050 --> 00:54:47.060
instances of that function,
it'll just work.

00:54:47.160 --> 00:54:51.690
And there's a host of other sorts
of commands that are built in,

00:54:51.770 --> 00:54:58.260
dot products, inner product animations,
plotting, things like that,

00:54:58.260 --> 00:55:03.280
that are automatically -- or
where there's prepared sort

00:55:03.410 --> 00:55:06.370
of parallelized versions.

00:55:06.670 --> 00:55:10.640
This is an example of how
you distribute data and code.

00:55:10.640 --> 00:55:14.360
This is a Mathematica program.

00:55:14.360 --> 00:55:18.790
Actually in Roger's version of the talk,
it just added up the numbers 1 through n.

00:55:18.860 --> 00:55:20.250
I thought that was silly.

00:55:20.470 --> 00:55:23.900
What I had to do is add up the numbers
1 through n and then add the process

00:55:23.920 --> 00:55:31.110
ID and then take the factorial of that
just so we would get a better number.

00:55:31.340 --> 00:55:35.410
Also,
it proves that I have great confidence

00:55:35.430 --> 00:55:40.600
in the system because I have no idea
what the process IDs are going to be.

00:55:41.200 --> 00:55:43.580
So we execute this in the head machine.

00:55:43.650 --> 00:55:46.570
We've made that definition
on the head machine.

00:55:46.620 --> 00:55:48.100
And now we execute it.

00:55:48.250 --> 00:55:53.080
And we get a number that is involved
with the process ID in some way.

00:55:53.080 --> 00:55:56.960
And now what we're going to
do is export this definition.

00:55:56.960 --> 00:56:00.730
And that command took the definition
that we made in the head machine

00:56:00.730 --> 00:56:02.910
and distributed it to all the nodes.

00:56:02.960 --> 00:56:05.010
Which you can do because
it's not a C program.

00:56:05.010 --> 00:56:06.960
It's not something you have to compile.

00:56:06.960 --> 00:56:10.940
It's a Mathematica expression
that can be interpreted by

00:56:10.940 --> 00:56:12.960
the Mathematica interpreter.

00:56:12.960 --> 00:56:17.900
And now we'll go and evaluate
this machine name command.

00:56:17.950 --> 00:56:19.960
That will let us see which
one each one executed on.

00:56:19.960 --> 00:56:20.980
And we do that.

00:56:20.980 --> 00:56:24.860
And so now we have the ten results.

00:56:24.860 --> 00:56:29.740
And you'll see each number is
a little bit different because

00:56:29.740 --> 00:56:32.420
it had a different process ID.

00:56:35.340 --> 00:56:40.990
So, how are we doing for time?

00:56:42.960 --> 00:56:43.850
Let me skip these.

00:56:44.580 --> 00:56:51.030
And this one,
this is basically showing the

00:56:51.030 --> 00:56:54.600
lower level operations where
rather than just do a map,

00:56:54.600 --> 00:56:57.390
you can actually set
up a queue where you,

00:56:57.390 --> 00:57:00.850
I'm not going to go
through the details here,

00:57:00.890 --> 00:57:08.710
but you basically tell it to queue up
these processes and then you can ask for

00:57:08.710 --> 00:57:13.900
it to wait for certain ones to finish and
you can wait for a list in which everyone

00:57:13.900 --> 00:57:19.400
finishes first will return sort of like a
select call if you're familiar with Unix.

00:57:19.400 --> 00:57:21.760
And that allows you,
that's sort of a foundation in

00:57:21.760 --> 00:57:26.160
which you can build your own manual
more sophisticated load balancing

00:57:26.270 --> 00:57:29.170
and process managing things.

00:57:29.580 --> 00:57:31.300
So, now here's an example.

00:57:31.300 --> 00:57:33.360
And as I mentioned,
this is actually Roger's talk,

00:57:33.370 --> 00:57:36.400
and I don't actually know that
much about parallel computing,

00:57:36.400 --> 00:57:39.800
but I thought I would make a little
example and sort of whip something up

00:57:39.880 --> 00:57:41.660
for the demo to see if I could do it.

00:57:41.720 --> 00:57:46.840
And what this does is it recreates
the keynote demo fractal that

00:57:46.840 --> 00:57:52.240
I used a couple days ago,
and this is the code for that fractal.

00:57:53.600 --> 00:57:58.510
And so, here, this will run this example
now on the -- just on the head

00:57:58.810 --> 00:58:00.560
machine as a single process.

00:58:00.560 --> 00:58:02.890
And you see it goes through,
and this is a little sort of

00:58:03.060 --> 00:58:06.660
graphical animation progress monitor
thing that I wrote a while ago.

00:58:06.660 --> 00:58:09.260
And as you can see,
it's kind of poking along.

00:58:09.260 --> 00:58:13.120
You may notice it's not much
slower than the G5 demo.

00:58:13.120 --> 00:58:16.160
That's because I'm not computing
as many points and because

00:58:16.300 --> 00:58:19.740
it's also not doing the big num
calculation in the background.

00:58:19.740 --> 00:58:22.280
It's not, in fact,
the case that this is as fast as a G5.

00:58:23.600 --> 00:58:27.460
And there,
it's put the animation together.

00:58:27.460 --> 00:58:32.010
So, now let's run it on the grid.

00:58:32.100 --> 00:58:33.600
And for those of you
who can see the lights,

00:58:33.600 --> 00:58:36.760
look at the lights.

00:58:36.760 --> 00:58:37.760
Here we go.

00:58:37.760 --> 00:58:40.420
It's very important always to
watch the lights on your cluster.

00:58:40.420 --> 00:58:43.410
And here we go.

00:58:43.820 --> 00:58:45.970
Now, notice the first frame,
you don't get any

00:58:45.970 --> 00:58:47.700
faster because it takes,
you know,

00:58:47.700 --> 00:58:48.990
they're all doing it at the same time.

00:58:49.040 --> 00:58:54.510
But then once it gets going,
you basically get ten at a time.

00:59:02.320 --> 00:59:06.460
So if I'm reading my clock right,
I really need to hurry.

00:59:06.460 --> 00:59:10.670
So basically the advantages are,
it's much, much cheaper than buying

00:59:10.740 --> 00:59:15.600
separate copies of Mathematica,
if you buy the nodes.

00:59:16.280 --> 00:59:19.290
It works in a completely open-ended
heterogeneous environment,

00:59:19.290 --> 00:59:23.910
absolutely no restrictions at all
as long as it runs Mathematica.

00:59:24.530 --> 00:59:28.410
We have sort of high-level symbolic
representation of the parallel

00:59:28.460 --> 00:59:31.650
structures and the parallel
control that you need to do,

00:59:31.650 --> 00:59:34.700
or the controls to get good performance.

00:59:34.820 --> 00:59:38.780
It's pretty easy to take existing code,
and as long as it's suitable

00:59:38.800 --> 00:59:43.250
for parallelization,
it's easy to do that.

00:59:43.350 --> 00:59:45.620
And you can do it,
you can do this sort of in the

00:59:45.620 --> 00:59:49.980
rich world of Mathematica rather
than the sort of more limited,

00:59:49.980 --> 00:59:52.310
you know,
worlds of C and Java or whatever,

00:59:52.310 --> 00:59:54.650
where you have to do a lot
more sort of by yourself.

00:59:54.880 --> 00:59:58.930
And as a prototyping environment
for parallel algorithms,

00:59:58.990 --> 01:00:01.080
of course, it's very nice.

01:00:01.120 --> 01:00:02.030
And I guess that's about it.

01:00:02.040 --> 01:00:04.140
And we all have to
remember to close these,

01:00:04.140 --> 01:00:06.060
otherwise we leave things running.

01:00:06.150 --> 01:00:07.710
Thank you.

01:00:16.960 --> 01:00:19.510
Okay, well,
I would like to basically wrap up

01:00:19.640 --> 01:00:23.380
by pointing to quite a variety of
resources for more information.

01:00:23.400 --> 01:00:26.800
So, first of all,
there's been a lot of tracks and

01:00:26.930 --> 01:00:30.580
sessions that are relevant to this topic.

01:00:30.580 --> 01:00:32.970
And, unfortunately,
a lot of them were earlier in the week.

01:00:33.080 --> 01:00:35.630
So, hopefully,
you had a lot of opportunity to

01:00:35.630 --> 01:00:39.450
go see some of these sessions this
week on the Enterprise IT track.

01:00:39.500 --> 01:00:41.650
If you don't,
I encourage you to watch the videos

01:00:41.650 --> 01:00:43.800
since there were some excellent sessions.

01:00:44.930 --> 01:00:47.720
There will be a session tomorrow on
deploying Xserve RAID in the afternoon.

01:00:47.720 --> 01:00:51.610
I'm sorry, deploying Xserve tomorrow
afternoon and Friday afternoon

01:00:51.610 --> 01:00:53.230
on deploying Xserve RAID.

01:00:53.240 --> 01:00:55.690
I encourage you to attend those sessions.

01:00:55.700 --> 01:01:00.820
On the developer side,
there's some excellent sessions

01:01:00.900 --> 01:01:07.040
on development tools for the
Unix layer and performance tools.

01:01:07.040 --> 01:01:12.120
And those were, again,
either earlier today or yesterday.

01:01:12.630 --> 01:01:15.630
But, again, I encourage you to watch the
videos if you weren't able

01:01:15.630 --> 01:01:16.990
to attend those sessions.

01:01:17.080 --> 01:01:18.890
Who to contact?

01:01:18.990 --> 01:01:24.300
So, again, for information or follow-up,
you're welcome to contact me.

01:01:24.300 --> 01:01:26.060
Again, Doug Brooks, my email is up there.

01:01:26.060 --> 01:01:30.200
Michael and Theodore
have contacts as well.

01:01:30.200 --> 01:01:33.940
And Skip Levins,
who's our server technology evangelist,

01:01:34.020 --> 01:01:36.300
wasn't able to attend the session.

01:01:36.300 --> 01:01:39.430
But from a developer perspective,
he's your contact from a server

01:01:39.430 --> 01:01:40.880
technology point of view.

01:01:40.880 --> 01:01:41.990
So, he's a great person to talk to.

01:01:42.050 --> 01:01:44.570
technology perspective.

01:01:46.960 --> 01:01:48.030
Additional resources.

01:01:48.050 --> 01:01:51.090
So we gave you a list
of solutions earlier.

01:01:51.110 --> 01:01:54.480
I wanted to point you to a key
page that we've recently put up

01:01:54.540 --> 01:01:57.200
about two or three weeks ago,
which is the Compute

01:01:57.200 --> 01:01:58.890
Cluster Solutions page.

01:01:58.960 --> 01:02:01.840
If you go to the apple.com
slash server page,

01:02:01.850 --> 01:02:05.340
you'll find quite a number
of solution-specific pages,

01:02:05.340 --> 01:02:09.420
one specifically on clustering,
which highlighted all of the key

01:02:09.420 --> 01:02:12.200
solutions that were referenced today.

01:02:13.020 --> 01:02:17.100
Again, the same page can provide
product information and,

01:02:17.100 --> 01:02:21.840
of course, information on both the bio
team and Grid Mathematica.

01:02:24.950 --> 01:02:29.660
This is an area where there's a wealth
of community support from mailing lists.

01:02:29.660 --> 01:02:32.900
And so I wanted to make sure you're
well aware of several key mailing lists

01:02:32.900 --> 01:02:35.880
that Apple and some third parties host.

01:02:35.970 --> 01:02:38.600
Apple hosts the SciTech and
the Unix porting list,

01:02:38.600 --> 01:02:42.040
which are both very relevant
to the cluster space.

01:02:42.110 --> 01:02:47.940
And bioinformatics.org has some
excellent mailing lists for BioClusters

01:02:47.960 --> 01:02:50.940
and BioDarwin development under Darwin.