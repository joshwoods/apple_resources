---
Title:  Audio and QuickTime
Year:   2003
Web:    https://nonstrict.eu/wwdcindex/wwdc2003/711/

[!] This transcript was generated using Whisper, it has known transcription errors.
---

Good morning everyone. Welcome to the audio and QuickTime session. I hope you enjoyed that slight demo that we had of some of the things that we're going to be showing you today as well. So this session is covering changes to the QuickTime audio foundation. Basically the big changes, we're using Core Audio natively. We're removing the code that's depending on Sound Manager. This is bringing a lot of new capabilities to QuickTime's audio infrastructure, and as well we're going to talk a little bit about audio codec news.

So we're going to be focusing today on playback, export, some changes to DV audio that we made, and as well news from the audio codec team. And the technology frameworks that we're covering today are QuickTime and Core Audio, and without any further pause, I'm going to introduce Greg Chapman who's going to be talking about audio playback.

So we've been having a lot of fun, as you might imagine. So the main thing we wanted to do with this revision of QuickTime was to remove all those restrictions that the sound manager put on us. We wanted to have more than two channels per track. You've been listening to some 5.1, the Linkin Park and the Bugs Bunny was all 5.1, so you can hear center, right and left, right and left surround, and into the subs.

We wanted to have high sample rates. Sound Manager limited us to basically a 16-bit number for the sample rate. We're talking to Core Audio now, so we're doing much better on this front. We wanted more than 16 bits per integer sample. We could support playback of 24 and 32-bit integer, but we would always bring it back down to 16-bit to play it out, so you'd lose all that nice depth. We also wanted to support floating point samples. We used to be able to do this. Again, we'd convert to 16-bit. You'd lose all that nice depth.

The other thing we wanted to do was to be able to do multi-tracks around where you could have, for example, for 5.1, you could have six mono tracks, each one labeled as being left, right, center, whatever, and then have them mixed together, sample accurate, and come out all the appropriate speakers. So let me go to demo two and let's show you what we've got. I'm going to play this whole trailer. Tim played part of it in the QuickTime State of the Union. We want to see the whole thing. This is 5.1.

Proceed as you still are. Only human. All of our lives, we have fought this war. Tonight, I believe we can end it. That's a nice trigger. Oh, that's great. Mr. Anderson. Surprised to see me? So now he's found a way to code himself. Now he's more than one of them.

have to do to be able to play such content in your app? And the answer is absolutely nothing. Your app can play this as soon as we ship this version of QuickTime. Let me show you iTunes. iTunes doesn't know about QuickTime support for multi-channel. iTunes will play, iTunes does not use QuickTime to play MP3s or the new music store content, but if you hand it a movie that it doesn't know how to play, it'll use QuickTime to play it. Here's an example of this. This is a Linkin Park song. I think it's the same one we played when we came in. It takes a little bit.

and others. I'm not going to play that whole song again, as fun as it was. So that's an example of an app that, you know, we just installed a different QuickTime underneath it, plays back this movie, it gets surround sound. No worries. Okay, let's go back to slides.

So how did we do that? There's a couple of things. There's some file format things we had to do. There's a new version of the sound description that's in the soundtrack. It describes sample rate as a 64-bit float, for example. There's a new optional sound description extension that tells you what the channel layout is for the track. Is this left and right? Is it 5.1? Is it just left?

We've rewritten the sound media handler so that it can understand these things, and it talks to Core Audio instead of the sound manager, as you might imagine. And the new multi-track mixer that I mentioned that does sample accurate mix between soundtracks so that you can have your surround spread between tracks.

Let me talk a little bit about channel mapping and how that works. Historically, movies can have one or more soundtracks. Usually they're alternates, but you could have more than one soundtrack playing at the same time. Each track could only have one or two channels, and if you had two, we would just assume that it was left followed by right. And if you had a multi-track movie, it would always mix to mono or stereo depending what your output device was. So if you had three stereo tracks, you'd end up with stereo out. And it makes a certain amount of sense, but it's kind of limiting.

In this lovely new world, tracks can have any number of channels. They're assigned to speakers using a sound description extension, as I mentioned. And each track can contribute to the movie's channel layout. For example, 5.1 could be one track with six channels in it, could be six mono tracks labeled according to what speaker they should go to. Or it could be a five-channel track and then a one-channel LFE track or three stereo tracks or whatever. You just have to mark them appropriately and then we'll play them back correctly.

The order of the track in the movie is not important as long as the channel assignments are labeled. And your old movies that have unlabeled mono and stereo, they behave as before, as you would expect. And if you're playing to a stereo device, if I took that Matrix trailer and you played it back on your computer with no six-channel device plugged in, that would downmix to stereo and you would hear all the left side and the left, all the right side and the right, and you'd hear center and both sides. So let me show you a little bit about that. We'll go back to demo two.

I've got three movies here. These are actually quad. Let me show you what they look like. So the one change we made to player

[Transcript missing]

Left, right. Left, saran. Right, saran. Okay. Thank you, Kevin. Here's this one. This one has four tracks. Let's take a look at each one of those.

This first track is a mono track. It goes in right. Second one's left surround. It's a strange order, but right surround and then left. That's kind of cool. Is that going to go to the right places? Left, right. Left Saran, Right Saran. There's always that little hesitation before that one comes in that makes me nervous.

And this one, just to be different, it's also four tracks. This one goes left, right, left surround. Right surround as you might expect, and this one will play back correctly too. Left, right, left surround. There we go. Right surround. Okay. So you can see that we're actually following the labeling. We're not just doing something in the order of the tracks. Let's go back to slides.

OK, let me talk about a few interesting new APIs in QuickTime. Now that we're talking to Core Audio, there's some new things you may need to do. The first one is Qt Convert Sound Description. We now have three different versions of sound descriptions. If you look at sound descriptions yourself, this is starting to become a nightmare. So we put in an API where you can take any version of sound description, convert it to any other version of sound description.

We have a mode where you can convert it to the lowest possible version of sound description given the format that's described so that you can get the best possible backward compatibility when you're creating movies, so old versions of QuickTime can play them. And you can go to and from an audio stream basic description, which is core audio's struct that describes a stream of audio. So that's very useful for just taking a sound description, you convert it to an ASBD, and carry on with core audio.

These are the obvious ones you might want to have to set a movie to play to a particular audio output device. Device UIDs are a core audio concept. It's a CFString. It's a unique identifier for the device. We used to have media set sound output component where you could grab a sound manager sound output component and say, please play to this device.

If you call the old API, we will try underneath to determine what the actual core audio device is that that sound output component was going to talk to, and then we'll play to that guy. We'll try and do the right thing. We have two levels of API here. One is to set the movie to play, and then media set is for individual tracks. So you can, as before, set individual tracks in a movie to play to different devices.

Extract movie audio. This is something that people have asked for for a while. Have a way to take a movie and just get uncompressed audio out of it. Mix across tracks, decompress the tracks as necessary. This is the one that's going to allow you to do this. And it returns a core audio buffer list, which means you can then take that, put it in a queue for some AU graph to have a ball with and play it out or whatever you want to do. There's these APIs, Begin and I'm not really going to go through these. I'm just going to show you some sample code that will be posted that you can take a look at. Let's go to Demo 2.

Okay. Yes, it's Unix. I wrote a command line app and I'm going to show you the routine that I wrote. It just takes a movie, extracts the audio uncompressed, and writes it to an AIFF file, which is a boring thing to do, but I could imagine that you guys might think of something more interesting to do with the audio buffer list you get out of the movie. So it takes a movie and it takes the name of the AIFF file you want.

You come down, you begin movie audio extraction. You can ask the audio extraction session what its default output format is going to be. And that default is generally 32-bit float. It's all the channels in the movie from all the soundtracks in the movie. It does no mixdown by default. And I think it picks the highest sample rate in the movie.

if there are multiple sample rates, otherwise it's the sample rate. But that's not what I want. I want to put it in an AIFF file, so I want 16-bit Big Endian samples. So that's what I say here. I did that by hand. Core Audio has much nicer C++ classes for dealing with AudioStream basic descriptions. You tell the extraction session what you want, so I send in what I want. I wanted 16 bits per channel, and I wanted signed Big Endian, blah, blah, blah. Allocate your Core Audio buffer list. Mine's real simple.

I've only got one buffer because it's all interleaved, but you could get deinterleaved float32 buffers if you wanted. Set up my AIFF file, and I call extractMovieAudio. This is the interesting loop. You watch this flags word that comes out of extractMovieAudio for this bit to be set. When that bit is set, you're done, and each buffer you pull, you write it out to the file or pass it off to your AU graph, whatever you want to do, and then we do this. So let me just show you this running. I wrote a little batch script to do it because the command line tool is really ugly.

But let's see, I'm in the same folder here and you'll watch that show up. Do it. And over here we have, suddenly, an AIFF file. This is an AAC movie, which is actually, people who have tried to get audio out of movies by themselves, it's really hard when you're dealing with a VBR codec. There are seeming issues between buffers and you get pops and it's just a pain in the butt. But, here we are, we've got this and it plays beautifully.

[Transcript missing]

See, this is stereo, and you just heard what we do with stereo when we have a multi-channel output, which is we put it in right and left, as you would expect. All right. That was fun. Go back to slides. Now I'm going to bring up Brad Ford, and he's going to talk to you about exporting these new kinds of audio.

Thanks Greg, and thank all of you for caring enough to get out of bed early. Greg said that we've been having a lot of fun. I would use a different word to describe it, probably pain. But that's the whole mantra of QuickTime. We write the hard code so that you don't have to.

And a great deal of pain has gone into making these bits play back to the right place at the right time. And we would really hate it if all of this work were wasted by not giving you a way to author this content. So thus we needed to give you a way to export to these formats as well.

[Transcript missing]

So now in QuickTime 6.4, we have a new standard sound compression component. I'm kind of loath to say it's a new dialogue because it's oh so much more. It's a dialogue in one case, but it's also just a sound configurator. It can export to and from all the new formats. We can play it back. We'll let you export to it.

Standard sound works well with or without QuickTime's default dialogue, which makes it easy for you to put custom UI on top of it. So any application you might develop that would need to export audio, you can use your own beautiful UI and just use our standard sound component to drive it and let it keep track of all the state, ask it questions, and we make this possible with new property APIs, which I'll be talking about in a minute. Also, the default dialogue is now a lot smarter. We don't make the same mistakes that we made before. It's easier for me to show this to you than to talk about it. So let's go to demo two.

Okay. Here we've got the same lovely, riveting Warner Brothers sound effects content that you heard earlier. I'm just going to bring up the... Export dialog. I'm going to export to a QuickTime movie. And this is the new standard sound dialog. This is what the UI looks like. You'll notice a lot of changes from the old one.

First of all, gone are 32-bit floating point, 24-bit integer, and all the other uncompressed formats pretending that they were really compressed formats. Now, if you're uncompressed, you're uncompressed all the time. And you get to different flavors of yourself by going down to this codec-specific settings area where you can set your sample size to be what it really is and you can tell us whether you're floating point or unsigned or little endian, things like that. Also of note are -- and I expect some oohs and ahhs here. Ooh.

Ah. That's a lot more channel layouts than we ever -- -- supported before. You want to do mono or stereo, that's fine. You can still do that. But you can also tag the content in your movie. We'll let you export to as many tracks or as many channels as you have in your input movie.

So if you've got six tracks or six channels of audio in your input movie, we'll present, you know, all the five ohs, all the five dot ones. Makes sense. Also, here's another ooh-ahh moment. Wow. Sample rates that go up beyond 64. So we'll let you go to 24-bit, 96 kilohertz audio. No problem.

And as a first, let's go back to AAC where for the first time we'll let you export multichannel AAC, which opens up a lot of really interesting possibilities for web content delivery. You know, you can export AAC in the range of say 192 to 320 kilobits per second. That's low enough that you could transfer transfer it over the internet, surround sound. Pretty interesting.

Okay, so let's go ahead and save this out to the desktop. Okay, here we go. And let's quickly look at the info. Now we've taken all the tracks and we've put the audio in each track down into one track, but it's still got all the channels. They're just all in one track. So we can play this back.

You get the idea. AAC 5.1. You witnessed it first in public. Okay, let's go back to slides please. That actually was the first public demo of 5.1 AAC export in QuickTime. Feel special. So what application changes are necessary? Many of you probably use these old friends, these APIs, convert movie to file or convert movie to data ref.

If you do, you'll know that it has a lot of parameters and the last parameter is a component instance, which is the component instance of the exporter that you're going to use. And there are three ways that you can do this. You can pass in nil for the last parameter, which tells QuickTime, I don't care what exporter you use, just do the right thing.

You can also pass in a component, not a component instance, which means you find the right component of the type and subtype that you want, but you don't open it, and you say, I know I really want to use this component, but I don't care about special settings, so still do the right thing.

And the third one that you can do is you can actually open that component yourself, do some special configuration to it, and then pass it as that last parameter. If you do the first case, pass nil. Or the second case, pass nil. Or the second case, passing just the component, you don't have to make any changes.

Your export will work with the new settings. You'll get the new audio exporting by default. Now if you do open your own movie export directly, you must opt in. You must be explicit. Otherwise we don't know if you're prepared to handle the new properties and the new settings that we're going to give you. So the way that you opt in is you call this lovely new call, QT set component property.

I'll be talking about these component properties more in a minute. But you can see you just pass in the exporter that you're talking about, the class, and the property ID is KMovieExporterPropertyID enable 2003 audio features. Pretty long property name. And it's a Boolean. And if you say yes, then you are telling us that you're ready to handle these new settings.

Now if you have opted in, you may just need to opt in and you're done. You will only need to make further changes if you're ready to handle these new settings. So if you have opted in, you may just need to make further changes if you call MovieExport get settings as Atom container and parse the settings yourself.

If you do that, so I don't want to tell you not to do that, but it's kind of risky to do that because we might do things like we just did when we changed audio underneath you and give you all different settings. So if you have code to parse them, you need to be ready because you're going to get different settings.

[Transcript missing]

The property type is specified by a class as well as an ID. That means you get two 4-care codes or eight care codes to work with instead of just one 4-care code. Which is kind of good because it sort of gives you a little bit of a buffer in the area of namespace collision.

So if I'm a component and you're a component and I have sample rate property and you have sample rate property but yours is a float 64 and mine's an unsigned fixed, I can still protect myself if our property names are the same because I have a class 4-care code that puts mine in a different space than yours. Also, all of the data is explicitly typed in the get info call. So if you make a get info call and you don't have to know anything about me, you can just query me, what are your properties?

I have to tell you what the type of the property is and the size. So it's very explicit, even if you don't have the header file, what the property is. Okay, so the get info call also gives you some flags that tell you some important things. This is a logical extension of what they were doing in Core Audio as well.

They have a great call in get info that lets you see if the property is settable. So ours gives you a bitmap, a flags field that lets you see if perhaps... Yes, the property is settable but I can't set it now. Maybe my hardware is not ready. So maybe it'll return can set later or can get later.

So you can get meta information about the property by getting these flags out, including one of the most important ones, will notify listeners. And I should mention standard sound compression, the new dialog has, like I said, about 50 new properties and I think all of them but five are listenable. So you can build a very sophisticated app that's very dumb. underneath because all it has to do is listen for property changes.

What do they look like? So here are the actual calls. I've already talked about them so I won't stay on this long. It has a class, it has an ID, it has the type, which is also a 4-care code, it has a size, and then it has that flags field that tells you can get now, can get later. Here is how you would implement a property listener. Let's say you are listening for my available sample rate list.

My available sample rate may change based on whether or not the compressor changes. So you would set up my listener proc and you typically have a big switch statement in it in which you would handle all of the property IDs that you'd registered for. So I just did a simple little printf statement there and go and update my UI.

And then you would tell the component, like standard compression, you would first make a new component property listener UPP and then you would register for that notification by saying call component, add component property listener. Or if you're using the QuickTime version for Jaguar, you would say QT, add component property listener with the exact same parameters. And once you do that, you are listening for that property and you will continue listening for that property until you remove it.

There is one important thing I should mention about these property APIs. Due to a minor oversight, the glue code got into Panther, got into the seed CDs that you received on Monday, but the header file didn't make it there in time. It wasn't promoted to public status. So that was a little bit of a minor oversight.

Okay, here is a great little app that was written by Adrian Baerlacher, another member of the QuickTime engineering team. And I'm very proud of this app because he wrote it very quickly, and it's the most slick, under-thousand-line code app that one could write because it keeps no state.

All of its state is managed by the standard sound compression dialog. So maybe you're like Adrian, you think that the standard sound compression dialog is swell, but you really just can't get enough metal in your life, and you want more metal. Or maybe you want it to have your own custom UI look like he did. So you are able to use standard sound to do all of your configuration and put up your own UI on top of it. So let's take the same content we heard before that has the left.

Right. Left surround. Right surround. And we'll use his UI. Now remember, this is being driven entirely by the properties. So every time I tweak a setting here, it's doing a set component property call or it's listening for changes and updating the UI. So let's go ahead and do this export. I suppose I should select an output location.

Export. This is a pretty full-featured app, and it's under a thousand lines of code. Left, right. Left, surround. Right, surround. So there we took a quad and we mixed it down to stereo and it still plays in the right places. If you'd like to learn more about the component property APIs or standard sound compression in general, please come to our lab between 12:00 and 2:00 today where you'll hear more. Thanks. And now I'll bring back Tim.

So we thought this feature was really, really good. We worked really, really hard. We were a little bit confused. We might have been drinking strange things. So we want to show you a little bit what we are going through. Can we go back to demo two, please? This around's gone again.

Oh man! Wish I didn't have to work on audio. QuickTime is back to normal! Thank you! So that was a stupid movie that a lot of people on the QuickTime team worked really really hard to film and direct and dream up I guess. That was probably either the easiest or the hardest.

So I'm going to talk a little bit about some DV audio changes back to, you know, real content. So we made some changes in QuickTime 6.3. The issue with DV audio was that there's two basic ways that you can store DV audio inside of a QuickTime movie. The first way is you can store inside a track as PCM audio and basically extract it from the DV frames as you capture.

That's what Final Cut Pro does, that's what QuickTime Sequence Grabber does, and there's been generally no problems with long-term AV sync in that case. The other way you could do it is you could have the audio track index every frame of DV and reference the DV audio inside the DV frame.

And that's what iMovie's doing when it captures and that's what the QuickTime DV importer is doing. And the issue there is that audio in that case is represented as a fixed number of samples per DV frame, which is a problem because it isn't. If you look at the duration of a frame in NTSC, it's basically a variable number of audio samples per frame. And when we represent it as a fixed value, which is basically the maximum number that could occur, we're basically accumulating a drift in some cases.

So in playback it's a very small amount of audio, but in our export and our editing cases, we were accumulating about a one frame of video error in the audio for about every two and a half minutes of export or editing, depending on the case. So we had a fairly large problem with some AV sync in some applications. So the solution that we decided to do is represent DV frames having variable duration, which is really the correct thing to do.

So we have a new type, which is VDVA, and it's a variable duration DV, and that's new in QuickTime 6 3. It's also going to be in 6 4 and forever after. That's the default when you're importing a DV clip. When you use the QuickTime importer and in iMovie, it's now going to be representing them as variable duration.

So, a couple of reminders when you're looking at DV, if you're playing around and actually looking at DV audio in the tracks, you're going to see the new type, K-variable duration DV audio format. You may see the old type because files that were created before 6.3 will still have the old type. And the important thing is if you're playing with sound descriptions and you're using the new type, you must set the compression ID to variable compression.

And that's how we basically make this work, and so we've actually achieved a really good audio-video sync. And I just wanted to point out a couple of things. A lot of the support that you're seeing will be available when QuickTime 6.4 ships on Jaguar as well. That's something that we're working on to make sure that the multi-channel support, et cetera, is going to be working on that platform. We're using a mobile I.O. audio box to drive the audio, the six-channel audio. It's connected via FireWire. And, of course, we're using the Core.

We're using the Core audio support and their drivers to make that happen. And with that, I'd like to introduce, oh, one more thing. One more thing, which is for those interested in AV startup sync issues, we have a session this afternoon at 2 where we're going to be talking about some changes we're making for audio-video startup sync. We figure that's more advanced programming than audio, so we put it in that session. And now I'd like to introduce Eric Aldrich, who's going to talk about audio codec development. Thank you, Tim. Thank you, Eric.

This is wonderful. QuickTime 6.4 built on top of Core Audio. But now we're going to take a little journey to Core Audio land. We're going to talk about audio codecs. What is an audio codec? Essentially, it's the business end of QuickTime on the audio side. It's going to actually handle your encoding and your decoding of your audio format. Audio codecs are going to replace your SCOMs and your SDECs. They're now called ADECs and AANCs. And right now, in your seed CD, we have AAC, MP3, and IMA. And we've already shipped AAC before, since QuickTime 6.

Okay, so how do you go about writing an audio codec? Well, we make it a little bit easier for you. There's a lot of code involved, but we provide you an audio codec SDK. It's available on the connect.apple.com site, your core audio. We provide you a framework, basically just needed to implement a codec.

It's fairly basic, but you can do quite a bit with it. We give you some sample codec, which is IMA41, Important point to note is you do not have to implement both an encoder and a decoder. You can implement one and not the other. Example of this is that MP3, we've only implemented the decode. We have not implemented the encode.

Another thing that's important about audio codecs is we use a push and pull model. You push the data in and you pull the resulting transformation out. Keep that in mind when I'm talking about this. Required files for writing an audio codec. Fairly straightforward. A big one there is ACBaseCodec, which is pretty much your base class for everything.

You're going to use this in every codec you write. Basically, all these files provide the framework for what you will need to implement. Handles all the dispatching, all the nitty gritty dirty details of talking to the sound converter and the like, or excuse me, the audio converter, which was detailed in session 404.

We have an optional file, which is AC Simple Codec, and this implements a basic buffer. For very basic sound codecs, audio codecs, we provide buffer, the buffering implementation for you. You don't have to do it. But for things that are fairly elaborate, you're going to want to use something else.

And then you're going to have to deal with codec-specific files. For instance, if you're doing AAC or MP3, you're going to want an AAC codec file. You're going to inherit directly from either base or simple codec. Then you have several methods you'll have to implement, and these are analogous to a few things that were already in the sound codecs. Of course, you have your property management, which is get property info, get property, set property.

You have your initialized and uninitialized, which is basically lock down the codec and then unlock it. You have append input data, which is the push of the push-pull model, where you'll push your data in. Then you have produce output packets, which is your pull on the push-pull model. And that's where you're going to get your decoded or encoded data out of.

A few things you need to think about. Like before you had open a component. Now that is handled mostly in the constructor. Your input, your default formats will be handled in Constructor as well. You need to handle the encoder and decoder separately. A model that we have used is we create, you have a lot of common functionality in codecs, whether you're encoding or decoding, so a lot of that we put into a generic, here we say my codec, and then we inherit directly from that, we create my decoder and my encoder.

And that's the model you want to think about. That allows us to add, for instance, on MP3, we have an MP3 codec, then we inherit, the MP3 decoder inherits MP3 codec, and if we later choose to implement an encoder, for instance, we'll also inherit directly from the MP3 codec file.

Okay, when dealing with properties, I can't stress enough that this is probably the most One of the most important features of what you'll be dealing with, this is basically you're configuring your codec before you lock it all down. So you have your get property info. It tells you how much data is required for each property. And more importantly, it tells you whether or not you can set it. And there will be two cases of this. A, you cannot set it, or B, you can set it when the codec is not initialized. And we'll get into that a little bit.

Get property, not all that much to say. You just have to realize that at any time, this could be called. And it is all right to answer, I don't know. If someone asks you for your bit rate and your bit rate hasn't been configured, you could say, you know, zero, say, I don't know. Or you can say, just report a default.

But it's important to understand that this can be called at any time. And there are some properties that won't even be known until after runtime. So that's possible. You can just say, I don't know. Set property, again, is not always available. Depends if the codec's been initialized or not.

Initialize and uninitialize. Again, initialize is just basically locking down the codec. At that point, when someone calls initialize, whatever they have set, you take that and that's how you set up the audio codec. If they have not set all the appropriate properties, it is if you can use default values and if you can make something out of it, you can initialize at whatever settings have been set, or you can fail. That's perfectly legal. If they haven't provided you enough data, you can fail. And uninitialize just simply removes this lock. It's basically a Boolean. Am I initialized or am I not?

Append input data, fairly straightforward. You basically take your input data and you put it in an internal buffer. Important thing to note though is you have to deal with audio packet, you know, audio stream packet descriptions. All VBR codecs should handle these. Very important for MP3, AAC, and the like. CBR codecs, you don't have to. It's nice, you can handle them if you choose, but it's not really required.

Produce output packets, does the actual encoding or decoding. You don't have to have the actual encode, decode right there. You can call a separate function or a separate library or whatever to do it. That's where you actually handle it. Again, on the encode side, if you're a VBR codec, you have to fill out the audio stream packet descriptions.

Another thing to note here, this is where you're going to actually handle clearing your input buffer. Even though they're pen input data, you're going to deal with handling, handing the data in and produce output packets. You're going to update all your buffers. You have to remember to do that here.

and backward compatibility. Kind of an important thing. There's a lot of QuickTime 6. There are copies of QuickTime 6 out there and Panther hasn't shipped yet. We allow you, so you can deal with the old SCOM and SDEC world, we give you sound manager audio components which allows you to wrap an audio codec. So to old versions of QuickTime it looks just like an old sound manager component, SDEC and SCOM. And we're going to be providing an SDK that does this on our site. Anyway.

You will need to implement this if you want that support. That's kind of important. But very straightforward. And we've been shipping these for a while. This is how we implemented AAC from the get-go. So this is proven technology, and it's been working for quite a while. So, at this point, one important thing, some documentation on all of this. I'm going to read this here. The core audio overview, which you can get basically at connectonapple.com. Take a look at it, provide any feedback. We'd appreciate it. Thank you.

The non-technical stuff here. For the roadmap, these are the We have a bunch of sessions going on also for content developers on QuickTime. One thing that is not noted here is that if you really care deeply about audio, there is a feedback forum starting just after we finish this session across the aisle here. So if you have any comments that you want to provide to the audio team, this is the right place to go and provide that.

We have a QuickTime feedback forum tomorrow afternoon. We will welcome you all there if you have any questions or if you have any comments that you want to make about QuickTime. That's my name and my address that you can use if you have any further comments, any further questions after the conference is finished.

For more information, you can go to the developer site, QuickTime. If you have any, you want to see where QuickTime is in general, you can go to the Apple site, QuickTime. And for information, you can go to the documentation site. There is a QuickTime section on it that has been updated just for this conference. are all present here. There is documentation and sample code that is only available to you if you go to the ADC site, download, place, and go to the QuickTime stuff.

So it should be available right now for you. Also, we have mentioned this many times this morning, there is a QuickTime lab going on downstairs. So if you have any questions, for instance, today is the special day for any audio questions. All the engineers from the audio team will be down there at the lab today. All day you can go down there. It's in the first floor Sacramento room. There is also some assistance for content development if that's what you're interested in.
