WEBVTT

00:00:21.500 --> 00:00:23.990
Good morning and welcome.

00:00:24.060 --> 00:00:27.460
As we were setting up for the conference,
I had in the database that this was

00:00:27.460 --> 00:00:30.000
Keithley secret session number one.

00:00:30.000 --> 00:00:33.000
I had a number of people ask me if
I had a second secret session and,

00:00:33.000 --> 00:00:34.640
well, no, I wasn't so lucky.

00:00:35.920 --> 00:00:39.370
The reason why this was secret is
because we really wanted to make a

00:00:39.370 --> 00:00:44.160
dramatic statement about some new
features in our acceleration libraries.

00:00:44.160 --> 00:00:48.710
What we'll be talking about today
will be techniques that you can use to

00:00:48.710 --> 00:00:54.550
accelerate image processing using the
image as part of the Accelerate Library.

00:00:54.600 --> 00:00:59.280
So, to do that,
let me bring up Robert Murley.

00:01:04.560 --> 00:01:06.820
Thank you, Craig.

00:01:06.970 --> 00:01:10.980
I'm very happy to be here this
morning to introduce to all of you

00:01:10.980 --> 00:01:15.780
a major new technology from Apple,
our vector-accelerated image

00:01:15.890 --> 00:01:19.500
processing library called Vimage.

00:01:19.500 --> 00:01:22.640
Before I jump into that demonstration,
though,

00:01:22.640 --> 00:01:27.490
there's another message that I would
like to make right here at the outset.

00:01:27.500 --> 00:01:32.020
All of the vector libraries
starting with Panther are

00:01:32.020 --> 00:01:36.500
going to be contained in a new,
high-performance,

00:01:36.500 --> 00:01:40.500
state-of-the-art computing
framework called Accelerate.

00:01:40.500 --> 00:01:48.580
Accelerate contains not only Vimage,
the major subject of this talk today,

00:01:48.630 --> 00:01:52.500
but also all of the vector
libraries that have been available

00:01:52.500 --> 00:01:57.500
previously in OS versions in
the framework called VecLib.

00:01:57.500 --> 00:02:04.380
The Digital Signal Processing Library,
the Basic Linear Algebra Subroutines,

00:02:04.380 --> 00:02:09.500
LAPAC, the Math Libraries,
and the BigNum Library.

00:02:09.730 --> 00:02:15.490
So I wanted to make sure that
that was clear before we went on.

00:02:15.610 --> 00:02:20.480
Now, today we're getting into the main
part of the demonstration here.

00:02:20.660 --> 00:02:25.500
What you'll learn about Vimage, well,
that may be a little optimistic.

00:02:25.500 --> 00:02:28.490
I'm going to try to convey
about Vimage anyway.

00:02:28.600 --> 00:02:33.500
Functionality, data structures and API,
at least some of the examples of them.

00:02:33.500 --> 00:02:37.500
It's much too extensive
to go over completely.

00:02:37.610 --> 00:02:42.500
Some of the features that are not
included in the first two subjects.

00:02:42.500 --> 00:02:46.240
Then I'm going to bring up
one of my colleagues in the

00:02:46.310 --> 00:02:49.500
vector and numerics group,
Yann Olman,

00:02:49.500 --> 00:02:53.500
who will talk about implementation
techniques and performance.

00:02:53.500 --> 00:02:56.500
And then finally,
although it's not on this slide,

00:02:56.500 --> 00:02:59.780
there will be a section at
the end by Eric Miller of the

00:02:59.780 --> 00:03:03.440
Architecture and Performance Group on
an overview of the CHUD tools,

00:03:03.510 --> 00:03:06.540
the performance tools.

00:03:08.530 --> 00:03:12.550
So what is in the functionality
of the Vimage library?

00:03:12.720 --> 00:03:19.800
It can be broadly grouped into
these main topics listed here.

00:03:19.850 --> 00:03:22.450
I'm going to talk a little
bit about each one of them,

00:03:22.450 --> 00:03:25.160
so I won't belabor it at this point.

00:03:25.210 --> 00:03:29.360
But at this time,
I'd like to jump into the

00:03:29.390 --> 00:03:31.900
first demo of the morning.

00:03:31.960 --> 00:03:41.400
What I want to show you is an image
processing function called inversion,

00:03:41.460 --> 00:03:48.270
which is a very simple technique
where each pixel of an image,

00:03:48.340 --> 00:03:51.860
in fact each color
component of each pixel,

00:03:51.910 --> 00:03:54.940
the complement of that value is taken.

00:03:55.080 --> 00:03:59.360
So what you see here is a picture of
a bed and breakfast in Portugal that

00:03:59.360 --> 00:04:01.760
I happened to stay at earlier this year.

00:04:01.840 --> 00:04:08.920
And what I'm going to do is
perform an inverse operation on it.

00:04:09.030 --> 00:04:12.850
And what you see is essentially
the photographic negative.

00:04:12.970 --> 00:04:19.870
Now the functions that are comprised in
Vimage run quite a gamut in complexity

00:04:20.280 --> 00:04:23.500
from quite simple to quite complex.

00:04:23.600 --> 00:04:26.500
This one is way over on the simple side.

00:04:26.530 --> 00:04:29.030
It's very easy to do.

00:04:30.550 --> 00:04:35.930
The first subject,
first major category was convolution.

00:04:35.980 --> 00:04:40.500
I want to say a word
about area processes.

00:04:40.640 --> 00:04:45.630
In image processing,
area processes are processes that use

00:04:45.630 --> 00:04:52.500
both the source pixel and other pixels
nearby to generate the destination pixel.

00:04:52.520 --> 00:04:56.490
Both convolution and morphology,
the first two examples

00:04:56.490 --> 00:05:00.490
I'm going to give you,
are examples of area processes.

00:05:00.500 --> 00:05:06.200
Convolution, in particular,
creates an output pixel by

00:05:06.200 --> 00:05:12.500
taking a weighted sum of pixels
nearby the input or source pixel.

00:05:12.500 --> 00:05:17.170
And that weight,
and therefore the effect of the process,

00:05:17.170 --> 00:05:21.500
is determined by a matrix
called a convolution kernel.

00:05:21.500 --> 00:05:22.000
And that weight,
and therefore the effect of the process,

00:05:22.000 --> 00:05:22.500
is determined by a matrix
called a convolution kernel.

00:05:22.860 --> 00:05:25.020
So, um,

00:05:25.860 --> 00:05:28.800
I want to give you an example
of how convolution works.

00:05:28.930 --> 00:05:36.040
What I have here is a fairly
blurry and kind of low intensity

00:05:36.040 --> 00:05:38.800
image of downtown Lisbon.

00:05:38.800 --> 00:05:42.400
And I want to emphasize that
this was probably poor equipment

00:05:42.400 --> 00:05:45.800
and not the photographer's
skill that was involved here.

00:05:45.890 --> 00:05:53.050
But anyway, I'm going to operate on this
with a 5x5 sharpening kernel.

00:05:53.050 --> 00:05:56.790
And what the result of that is, is this.

00:05:56.800 --> 00:05:58.890
A lot of the blurriness is gone.

00:05:58.900 --> 00:06:01.790
Features are much sharper than they were.

00:06:01.790 --> 00:06:02.800
I'll go back.

00:06:03.100 --> 00:06:05.800
That's the blurry part
and the sharpening part.

00:06:05.800 --> 00:06:08.960
And you can see the kernel there is 5x5.

00:06:08.960 --> 00:06:10.760
It's not that complex.

00:06:10.800 --> 00:06:18.790
So all of convolution... ...
...operates the same way.

00:06:18.790 --> 00:06:20.800
It's all matrix multiplication
of areas of pixels.

00:06:20.800 --> 00:06:25.800
And the effect simply depends
on what the matrix is.

00:06:25.800 --> 00:06:28.800
Another example, this edifice.

00:06:28.900 --> 00:06:36.820
I'm going to use a rather extreme edge
definition process or edge detection

00:06:36.820 --> 00:06:42.400
process and produce an embossed image.

00:06:43.110 --> 00:06:47.710
This is, as you can see,
an extremely simple convolution kernel,

00:06:47.710 --> 00:06:52.340
3 by 3, to get a pretty dramatic effect.

00:06:53.940 --> 00:06:59.860
The second I'd like to talk about,
or the second major topic, morphology.

00:07:00.030 --> 00:07:05.140
Morphology in general adjusts the shape
of objects in the image to conform

00:07:05.140 --> 00:07:08.900
more closely to the shape of a probe.

00:07:08.980 --> 00:07:15.900
And the probe is defined also in a
matrix called a morphology matrix.

00:07:15.900 --> 00:07:20.220
In practice it can be
used to take objects,

00:07:20.380 --> 00:07:24.600
small or large objects,
in an image and lighten

00:07:24.600 --> 00:07:28.900
them or darken them,
make them larger or smaller.

00:07:28.900 --> 00:07:34.700
It can be used to alter their shape,
to remove fine details while

00:07:34.700 --> 00:07:38.900
preserving larger objects,
and so forth.

00:07:38.900 --> 00:07:42.320
So to demonstrate this,
I have here a couple

00:07:42.390 --> 00:07:46.900
of very simple images,
a small circle and a large circle.

00:07:46.900 --> 00:07:53.720
I'm going to do a morphology process
on these two images using a probe

00:07:53.880 --> 00:07:56.900
in the shape of a right triangle.

00:07:56.900 --> 00:08:01.870
And it's a fairly large matrix,
about the size of the smaller circle.

00:08:01.900 --> 00:08:05.760
So the result of that operation is this.

00:08:05.900 --> 00:08:08.870
As you can see, both the circles take
on a more typical shape.

00:08:08.930 --> 00:08:10.900
They have a more
triangular characteristic.

00:08:10.900 --> 00:08:14.900
And in fact, the smaller circle,
in the smaller circle,

00:08:14.900 --> 00:08:20.550
the center circle completely
disappears because of the size

00:08:20.590 --> 00:08:23.300
of the convolution kernel.

00:08:23.490 --> 00:08:27.650
Secondly, I'm going to take this image,
well that was called a

00:08:27.650 --> 00:08:30.700
dilate operation by the way.

00:08:30.730 --> 00:08:34.400
I'm going to now do what's
called an erode operation.

00:08:34.400 --> 00:08:41.160
I'm going to take a circular filter
about the same size as the kernel here,

00:08:41.160 --> 00:08:45.860
operate on these two images,
and I get this result.

00:08:45.950 --> 00:08:48.650
So you can see that in the
case of the smaller circle,

00:08:48.660 --> 00:08:53.340
all the circularity is gone,
it's turned completely into a triangle.

00:08:53.360 --> 00:08:57.930
And the smaller circle has taken
on some triangular structure and

00:08:57.930 --> 00:09:00.280
lost a lot of its circularity.

00:09:00.350 --> 00:09:05.500
So there is a couple of examples
of morphology in action.

00:09:07.090 --> 00:09:13.540
The class of functions in geometry
is pretty much self-explanatory.

00:09:13.540 --> 00:09:17.900
They perform some sort of a
geometric operation on the image,

00:09:17.900 --> 00:09:23.000
transform it, make it larger, smaller,
reflect it, whatever.

00:09:23.000 --> 00:09:29.700
For an example of geometry,
I'm going to take this picture of a J,

00:09:29.700 --> 00:09:34.500
and I'm going to transpose
it or translate it,

00:09:34.840 --> 00:09:38.340
make it bigger,
but only in the longitudinal direction.

00:09:38.360 --> 00:09:42.320
And that results in this image.

00:09:42.850 --> 00:09:47.260
Secondly, I'm going to go back to the
original picture and do a shearing

00:09:47.260 --> 00:09:51.560
operation off to the right side.

00:09:52.340 --> 00:09:55.300
and that results in this image.

00:09:55.300 --> 00:10:01.300
And is it my imagination or is that bird
getting more irritated with each picture?

00:10:01.300 --> 00:10:06.630
Maybe I've just been looking
at them a little bit too long.

00:10:07.540 --> 00:10:15.560
Histogram operations are those that
use an intensity distribution histogram

00:10:15.560 --> 00:10:18.400
of the image to perform some function.

00:10:18.400 --> 00:10:23.560
The example I'm going to use
is histogram equalization,

00:10:23.560 --> 00:10:28.400
a process whereby an image with a poor
non-uniform intensity distribution

00:10:28.400 --> 00:10:33.400
is modified so that intensities
are distributed more evenly.

00:10:33.480 --> 00:10:40.410
So I'm going to go back to this bed and
breakfast here in Portugal and perform

00:10:40.410 --> 00:10:46.400
this equalization operation on it,
and it results in this.

00:10:46.400 --> 00:10:49.640
Now what you can see is there's a
great more detail visible here in

00:10:49.640 --> 00:10:51.400
this image than in the original.

00:10:51.400 --> 00:10:55.460
Notice in particular the
weather stains below the

00:10:55.460 --> 00:10:58.280
windowsills on the second floor.

00:10:58.460 --> 00:11:03.400
They were virtually undetectable
in the original image.

00:11:03.440 --> 00:11:08.810
So this, the equalization operation
brings out a lot of detail that

00:11:08.940 --> 00:11:11.200
was absent in the original.

00:11:11.460 --> 00:11:15.230
This is probably a lot closer to
the way that building really looked,

00:11:15.260 --> 00:11:17.410
would be my guess.

00:11:18.490 --> 00:11:22.850
Here's an example,
or rather the before and after

00:11:22.870 --> 00:11:25.400
histograms of the intensity distribution.

00:11:25.400 --> 00:11:30.490
I've added all three color channels
into each bar to simplify it,

00:11:30.500 --> 00:11:36.400
although in practice the operation is
done on each color component separately.

00:11:36.400 --> 00:11:41.600
But you can see in the before
image there's a lot of white with

00:11:41.600 --> 00:11:46.060
some starkly contrasted black,
and in the after image

00:11:46.060 --> 00:11:50.200
it's much more uniform,
a lot of different grays.

00:11:52.170 --> 00:11:57.270
So I could go on quite a long
time actually about functionality,

00:11:57.270 --> 00:12:01.100
but we do have a limited amount of time,
so I want to proceed on to some

00:12:01.100 --> 00:12:05.500
examples of data structures and API.

00:12:07.300 --> 00:12:11.260
First I want to talk about data types
and layouts that we support in our

00:12:11.260 --> 00:12:14.580
initial incarnation of the image.

00:12:14.710 --> 00:12:17.630
There are two different
data types supported.

00:12:17.630 --> 00:12:22.660
One is the 8-bit integer per
color component or per channel.

00:12:22.660 --> 00:12:26.570
I'll use those terms interchangeably.

00:12:26.570 --> 00:12:33.840
And the second is a 32-bit floating point
value per color component or channel.

00:12:34.400 --> 00:12:40.470
We also support two
different data layouts.

00:12:41.420 --> 00:12:48.550
One is the planar layout whereby
each channel is in its own array.

00:12:48.550 --> 00:12:52.130
And if I'm using an
RGB image as an example,

00:12:52.130 --> 00:12:55.100
that simply means that the reds,
the greens, and the blues are all in

00:12:55.100 --> 00:12:56.300
their separate buffers.

00:12:56.300 --> 00:13:03.480
And if you're calling a convolution,
or excuse me, an image process to perform

00:13:03.490 --> 00:13:07.960
some function on this image,
you would need to call it three

00:13:07.960 --> 00:13:10.250
times for all three color components.

00:13:10.300 --> 00:13:15.990
The advantage though is if you don't want
to do the process on all three channels,

00:13:16.180 --> 00:13:19.300
you can do them on only
one or two as you wish.

00:13:21.810 --> 00:13:28.540
The second layout is what we
call the ARGB interleaved layout,

00:13:28.570 --> 00:13:33.040
where all the color channels are
interleaved into a single buffer.

00:13:33.090 --> 00:13:38.370
We support at the current time a
four-channel interleaved layout,

00:13:38.370 --> 00:13:47.780
which can be either four 8-bit integers
or four 32-bit floating-point values.

00:13:48.100 --> 00:14:47.400
[Transcript missing]

00:14:49.880 --> 00:14:54.160
We do supply data conversion utilities
to go between the different data

00:14:54.160 --> 00:14:57.350
layouts and different data types.

00:14:59.580 --> 00:15:04.770
And now I'd like to go on to
what is probably the single

00:15:04.770 --> 00:15:07.700
most important data structure,
almost the only public data

00:15:07.700 --> 00:15:12.330
structure we have in Vimage,
the Vimage buffer.

00:15:12.410 --> 00:15:15.790
As you can see,
it's a very simple data structure,

00:15:15.790 --> 00:15:17.370
only four elements.

00:15:17.450 --> 00:15:19.680
There's a pointer to
the start of the data,

00:15:19.680 --> 00:15:22.640
which would be the upper
left-hand corner of the image,

00:15:22.690 --> 00:15:27.970
a height, a number of pixels,
width in the number of pixels,

00:15:28.050 --> 00:15:30.480
and then a row bytes,
which is the number of bytes

00:15:30.480 --> 00:15:37.000
from one row to another,
or the stride from one row to the other.

00:15:38.270 --> 00:15:44.620
Pictorially, if the name of the
Vimage buffer is image,

00:15:44.620 --> 00:15:47.900
then we have image.data at
the upper left-hand corner.

00:15:47.900 --> 00:15:52.780
You have the height, the width,
and if you imagine that that white

00:15:52.790 --> 00:15:58.380
space to the right of the image is extra
memory that's not used in the image,

00:15:58.380 --> 00:16:00.920
but just sitting there
at the end of the row,

00:16:01.070 --> 00:16:04.230
then you can see that the
row bytes parameter includes

00:16:04.230 --> 00:16:06.100
that length in the stride.

00:16:06.770 --> 00:16:11.410
That comes in handy if you want to
do 16-byte alignment on each row,

00:16:11.410 --> 00:16:12.520
for example.

00:16:12.540 --> 00:16:16.100
That's not a requirement by the image,
but certainly may be

00:16:16.100 --> 00:16:17.830
helpful in your own work.

00:16:20.800 --> 00:17:32.200
[Transcript missing]

00:17:32.630 --> 00:17:37.470
This allows you to do tiling,
if you want to do that,

00:17:37.500 --> 00:17:41.140
to take advantage of caching,
although we will also do

00:17:41.140 --> 00:17:42.940
that for you if you wish.

00:17:42.940 --> 00:17:50.480
It has quite a number of
other advantages as well.

00:17:52.300 --> 00:17:57.340
So here's an example of equalization,
an example of a very

00:17:57.340 --> 00:18:02.370
simple call to Vimage,
image equalization.

00:18:02.620 --> 00:18:06.540
There's only three parameters,
it couldn't get too much more simple.

00:18:06.540 --> 00:18:11.230
The Vimage buffer source,
the Vimage buffer for the destination,

00:18:11.230 --> 00:18:15.570
and then the flags word,
which the information in the flags

00:18:15.590 --> 00:18:18.240
word varies with each function.

00:18:18.560 --> 00:18:24.200
You notice that you don't have to
specify what the data layout or

00:18:24.200 --> 00:18:28.930
the data type is because that's
implicit in the name of the function,

00:18:28.930 --> 00:18:31.500
in this case, planar8.

00:18:31.630 --> 00:18:35.720
So every function has
four different variants:

00:18:35.880 --> 00:18:42.880
planar8, bit, planarFloat,
interleaved8bit, and interleavedFloat.

00:18:45.900 --> 00:18:53.270
There are some functions that do require
us to know in the image both the full

00:18:53.270 --> 00:18:56.280
image buffer and the region of interest.

00:18:56.370 --> 00:19:00.070
And these are the functions
that I mentioned earlier

00:19:00.070 --> 00:19:02.370
I referred to as area processes.

00:19:03.140 --> 00:19:05.060
The components are all shown here.

00:19:05.060 --> 00:19:08.650
You have the full image buffer,
the source ROI,

00:19:08.650 --> 00:19:12.170
which may or may not be smaller
than the full image buffer,

00:19:12.170 --> 00:19:17.150
a convolution kernel,
a matrix shown by the yellow rectangle,

00:19:17.150 --> 00:19:19.860
and then the destination buffer,
the result.

00:19:19.980 --> 00:19:25.940
I'd like to go into the relationship
between these things a little bit more.

00:19:26.780 --> 00:19:31.080
So this is the discussion,
a further discussion of image

00:19:31.100 --> 00:19:33.840
buffers and regions of interest.

00:19:33.870 --> 00:19:37.700
Alright, I think we all know what
the full image buffer is.

00:19:38.910 --> 00:19:44.920
In a call to an area function,
morphology or convolution,

00:19:44.950 --> 00:19:48.900
the region of interest is not
specified by a second V-image buffer,

00:19:48.900 --> 00:19:54.160
but rather simply by X and Y offsets
from the beginning of the full buffer.

00:19:54.160 --> 00:19:58.730
So, as you can see here,
you would indicate the upper left-hand

00:19:58.730 --> 00:20:03.220
corner of the region of interest
by an X and Y offset from the upper

00:20:03.330 --> 00:20:06.300
left-hand corner of the full buffer.

00:20:07.340 --> 00:20:11.820
The Robites is the same in both cases.

00:20:12.770 --> 00:20:17.200
You also pass a V-image buffer
indicating the destination,

00:20:17.250 --> 00:20:22.340
which has a height and a width
and independent row bytes.

00:20:22.430 --> 00:20:27.850
And notice that we have not
specified as yet the source,

00:20:27.850 --> 00:20:31.060
the region of interest, height and width.

00:20:31.120 --> 00:20:32.760
And that's for a simple reason.

00:20:32.760 --> 00:20:36.060
It has to be the same height
and width as the destination,

00:20:36.140 --> 00:20:39.060
so we simply take it from there.

00:20:44.060 --> 00:20:47.130
This is an example of one
of these function calls,

00:20:47.130 --> 00:20:48.290
convolution.

00:20:48.410 --> 00:20:52.270
You have the source and
destination image buffers,

00:20:52.290 --> 00:20:56.900
the offsets to the region of interest,
and then some other information

00:20:56.900 --> 00:21:02.130
defining the kernel and a few
other things that we need to know.

00:21:02.330 --> 00:21:07.130
So this is probably one of
the more complicated calls

00:21:07.520 --> 00:21:10.710
that you're going to run into.

00:21:13.810 --> 00:21:18.710
We have three computational cases
that we need to worry about when

00:21:18.710 --> 00:21:22.260
we're doing these calculations.

00:21:22.380 --> 00:21:27.110
The first one is fairly simple,
and to explain this,

00:21:27.120 --> 00:21:33.060
just keep in mind the four different
elements that I'm talking about here.

00:21:33.160 --> 00:21:35.860
The full image buffer,
the region of interest,

00:21:35.970 --> 00:21:40.360
the convolution kernel,
which is simply a matrix,

00:21:40.480 --> 00:21:46.420
and then in this image the source pixel
shown by the tiny red rectangle there.

00:21:46.580 --> 00:21:50.100
So if we are going to calculate
a destination rectangle

00:21:50.100 --> 00:21:53.570
from that source pixel,
we need to do a matrix multiplication

00:21:53.620 --> 00:21:57.100
of the pixels in the region of
the source pixel as shown there.

00:21:57.250 --> 00:22:02.310
The first case is very simple because
the entire matrix is contained

00:22:02.340 --> 00:22:05.360
within the region of interest,
so there's no issue about

00:22:05.630 --> 00:22:07.720
where the data comes from.

00:22:07.870 --> 00:22:10.360
The second case is a little
bit more complicated.

00:22:10.360 --> 00:22:15.630
What happens if the computational
matrix extends out beyond

00:22:15.710 --> 00:22:17.720
the region of interest?

00:22:17.720 --> 00:22:21.760
And this is exactly why we need to
know what the full image buffer is,

00:22:21.800 --> 00:22:25.210
because if it still remains
in the full image buffer,

00:22:25.480 --> 00:22:29.550
then we can use that data
without further concern.

00:22:29.660 --> 00:22:34.040
The third case is the more complex case.

00:22:34.080 --> 00:22:39.390
What if the computational matrix goes
even beyond the full image buffer?

00:22:39.420 --> 00:22:40.160
And in that case, what happens?

00:22:40.160 --> 00:22:40.160
What happens if the computational matrix
goes even beyond the full image buffer?

00:22:40.160 --> 00:22:40.160
And in that case, what happens?

00:22:40.160 --> 00:22:40.260
What happens if the computational matrix
goes even beyond the full image buffer?

00:22:40.260 --> 00:22:44.990
In that case,
we have to do something to substitute

00:22:44.990 --> 00:22:47.760
for the pixels that are missing.

00:22:48.260 --> 00:22:52.710
So we have an edge case problem,
and we supply you in this instance

00:22:52.790 --> 00:22:56.430
with three different options
to deal with these edge cases:

00:22:56.530 --> 00:23:01.250
background, color, edge extend,
and copy in place.

00:23:01.740 --> 00:23:04.720
And to demonstrate these three,
I'm going to start with

00:23:04.720 --> 00:23:07.940
this as an original image.

00:23:07.940 --> 00:23:14.070
All the lines between the different
colors are clean and smooth,

00:23:14.070 --> 00:23:16.780
and the edges are clean.

00:23:16.820 --> 00:23:20.480
And I'm going to do a
blurring operation on it.

00:23:20.500 --> 00:23:26.050
And the first time I'm going to do this,
I'm going to specify that for the edges,

00:23:26.050 --> 00:23:27.980
the color to use is black.

00:23:27.980 --> 00:23:29.920
If we don't have a pixel
in the computation,

00:23:29.920 --> 00:23:31.570
we'll use a black pixel.

00:23:31.600 --> 00:23:34.820
So the result of that
comes out like this.

00:23:34.820 --> 00:23:38.530
You can see that the colors
merge together on the edges,

00:23:38.530 --> 00:23:43.780
and on the outside of the image,
it just fades off into black gradually.

00:23:43.830 --> 00:23:47.600
The other extreme of that is
a background color of white,

00:23:47.600 --> 00:23:50.220
which ends up looking like this.

00:23:50.250 --> 00:23:54.560
With a black background,
you can see quite a difference there.

00:23:55.410 --> 00:23:57.990
The second case,
so that was the background color,

00:23:58.000 --> 00:23:59.600
the first option that we give you.

00:23:59.600 --> 00:24:02.930
The second option we
give you is edge extend,

00:24:02.930 --> 00:24:08.360
which means that we take the
pixels at the outside border of

00:24:08.420 --> 00:24:12.600
the image and just extend them out,
copy them out as far as we need to,

00:24:12.600 --> 00:24:14.000
to perform the operation.

00:24:14.000 --> 00:24:17.650
So the result of that
blurring operation is this,

00:24:17.670 --> 00:24:22.120
and as you would expect,
you really don't see any change when

00:24:22.120 --> 00:24:26.220
you get to the edge of the image,
it just continues on as

00:24:26.250 --> 00:24:30.110
it does in the beginning,
or in the middle.

00:24:30.550 --> 00:24:33.600
The third case is copy in place.

00:24:33.740 --> 00:24:41.790
And what we are saying there is that
if we don't have all the data we need

00:24:41.870 --> 00:24:46.230
to do the computation at any point,
then we won't do it.

00:24:46.260 --> 00:24:51.390
We'll just copy the source pixel to the
destination pixel and be done with it.

00:24:51.470 --> 00:24:53.560
And this is what that looks like.

00:24:54.160 --> 00:24:58.690
You have to concentrate on the edges of
the image and you can see that towards

00:24:58.690 --> 00:25:01.080
the edges there is no blurring effect.

00:25:01.230 --> 00:25:04.440
Once the computational
matrix goes off the edge,

00:25:04.440 --> 00:25:06.840
we just do a copy from the source.

00:25:06.840 --> 00:25:15.710
So those are the various options that
we give you to handle the edge cases.

00:25:19.350 --> 00:25:21.730
A couple of features that
I haven't yet mentioned,

00:25:21.740 --> 00:25:24.130
or maybe I have.

00:25:24.710 --> 00:25:29.190
All of the Apple libraries,
the vector-accelerated libraries,

00:25:29.190 --> 00:25:31.380
are optimized for all Apple processors.

00:25:31.380 --> 00:25:36.980
So if you are, for example,
running on a G3,

00:25:36.980 --> 00:25:43.620
if the host system is a G3,
then a form of any given routine that

00:25:43.620 --> 00:25:48.710
is not vectorized but still highly
optimized for scalar will be chosen.

00:25:49.300 --> 00:25:53.790
If you're running on a G4 or a G5,
then an appropriately optimized

00:25:54.280 --> 00:25:56.700
vectorized version will be chosen.

00:25:56.830 --> 00:26:00.770
This is all done transparently to you,
the caller.

00:26:02.130 --> 00:26:06.450
Our system, our library is multi,
Vimage in particular here

00:26:06.450 --> 00:26:08.550
is multi-processor safe.

00:26:08.600 --> 00:26:12.960
I should also mention that it's
interrupt safe if you take some

00:26:12.960 --> 00:26:16.260
precautions to make it interrupt safe.

00:26:16.380 --> 00:26:22.640
There's a lot of routines in Vimage
that do call malloc to allocate memory.

00:26:22.820 --> 00:26:26.480
However, if you don't want it to do that,
we do give you the option

00:26:26.630 --> 00:26:28.200
to supply your own memory.

00:26:28.270 --> 00:26:34.590
The calls that need memory also
have an auxiliary call that returns

00:26:34.610 --> 00:26:39.240
to you the minimum buffer size that
we will need to do the operation.

00:26:39.340 --> 00:26:42.330
So you can call that,
allocate your own memory,

00:26:42.340 --> 00:26:48.590
and then there will be no system calls
during the course of the operation.

00:26:50.020 --> 00:26:52.900
The image is a standard part of Panther.

00:26:53.050 --> 00:26:58.500
The data structures are unencapsulated,
simple and flexible,

00:26:58.500 --> 00:27:02.200
and unlike a competitor
or two I could name,

00:27:02.290 --> 00:27:06.090
but won't, there is no license fees.

00:27:08.480 --> 00:27:11.460
Okay,
so that completes my portion of the talk.

00:27:11.460 --> 00:27:13.710
I'd like to bring up my colleague,
Ian Ollmann,

00:27:13.710 --> 00:27:19.020
who will talk about implementation
techniques and performance.

00:27:24.270 --> 00:27:24.700
Thank you.

00:27:24.700 --> 00:27:29.880
I wanted to touch on two subjects,
mostly what you can do to use Vimage

00:27:29.920 --> 00:27:34.200
most effectively in your app to get the
best possible performance out of it.

00:27:34.270 --> 00:27:38.860
And then just for your own curiosity,
some of the things we did to tune

00:27:38.860 --> 00:27:42.530
the functions that you can get
through the Vimage subframework

00:27:42.600 --> 00:27:45.890
under the acceleration framework.

00:27:46.980 --> 00:27:51.270
So a couple of things
that you can focus on,

00:27:51.340 --> 00:27:52.940
a lot you'll touch on.

00:27:52.980 --> 00:27:57.000
So there are some alignment,
memory alignment things that you can do.

00:27:57.000 --> 00:27:58.900
We don't require that you
do anything in particular,

00:27:58.900 --> 00:28:01.310
but some things help,
so I'll mention them.

00:28:01.350 --> 00:28:05.490
I'll briefly talk about tiling
and then also some multiprocessing

00:28:05.520 --> 00:28:07.660
or real-time considerations.

00:28:09.100 --> 00:30:20.400
[Transcript missing]

00:30:21.850 --> 00:30:24.140
Tiling, of course,
is a commonly used technique

00:30:24.140 --> 00:30:25.160
in image processing.

00:30:25.160 --> 00:30:30.930
Basic approach is you divide
up your image into smaller

00:30:30.930 --> 00:30:34.720
segments which are cache-sized,
and this allows you to operate

00:30:34.720 --> 00:30:39.210
on segments and keep them in the
caches while you're working on them.

00:30:39.240 --> 00:30:41.570
So, for example,
if you had several chain filters

00:30:41.570 --> 00:30:44.510
you wanted to do in series,
rather than apply one to the whole image,

00:30:44.510 --> 00:30:45.910
then do the next filter
to the whole image,

00:30:45.910 --> 00:30:48.730
then the third filter to the whole image,
you could pick a small

00:30:48.840 --> 00:30:51.040
subset of the image,
do all three to that,

00:30:51.080 --> 00:30:53.310
and that means that for the second and
third filters you'd be very likely to

00:30:53.380 --> 00:30:57.130
have the pixels already in the caches,
so you'd be less likely to

00:30:57.130 --> 00:30:59.920
pay any penalty for going
out to DRAM to get them.

00:31:03.350 --> 00:31:05.400
So a few tips on how to do that.

00:31:05.400 --> 00:31:08.560
We've found that tiling is
only helpful some of the time,

00:31:08.570 --> 00:31:09.200
not all the time.

00:31:09.200 --> 00:31:12.450
So don't waste your time if it isn't.

00:31:12.580 --> 00:31:17.000
And we found it's very easy to simply
assay to find out whether or not tiling

00:31:17.000 --> 00:31:22.350
is going to work for you by just pushing
a small image through your code as it is,

00:31:22.350 --> 00:31:24.660
unoptimized,
and then push a big one through.

00:31:24.730 --> 00:31:27.700
Take a look at how many pixels per second
you're able to calculate in each case.

00:31:27.700 --> 00:31:30.220
If there's a big difference,
then maybe tiling will pay off for you,

00:31:30.280 --> 00:31:32.900
and it's worth the time to go through it.

00:31:33.160 --> 00:31:35.890
In our experience,
we found that tile sizes that are

00:31:35.890 --> 00:31:41.290
the rough will fit in the L1 cache,
which are probably about 16K to 32K,

00:31:41.290 --> 00:31:42.360
work best.

00:31:42.360 --> 00:31:46.220
Wide is better than tall or square,
and it can be very wide.

00:31:46.220 --> 00:31:54.600
We found cases where only 16 pixels high,
but 1,024 is the optimal case.

00:31:54.600 --> 00:31:57.630
We also do some tiling
in some of our functions.

00:31:57.690 --> 00:32:00.200
If you're going to do your own tiling,
in certain cases we imagine,

00:32:00.200 --> 00:32:01.950
although we haven't
found any examples of it,

00:32:01.960 --> 00:32:03.040
that these two things work.

00:32:03.040 --> 00:32:05.230
These two things could
interact adversely,

00:32:05.230 --> 00:32:07.860
so we provided you with a
flag you can pass that says,

00:32:07.860 --> 00:32:11.120
"KV image do not tile," which
basically tells us not to tile.

00:32:11.120 --> 00:32:13.400
You're going to do it yourself.

00:32:14.500 --> 00:32:18.490
Another thing you can do is take
advantage of our planar data format.

00:32:18.610 --> 00:32:21.040
Originally we were thinking
of only providing planar,

00:32:21.090 --> 00:32:23.600
but we had so many requests
for ARGB that it's a feature.

00:32:23.600 --> 00:32:26.300
However,
there are many drawbacks to ARGB,

00:32:26.300 --> 00:32:30.000
and if you use planar data
formats you can get around them.

00:32:31.310 --> 00:32:33.510
First of all,
for ARGB you may not wish to

00:32:33.510 --> 00:32:38.210
operate on the alpha channel,
so it's 25% or 33% more work to use an

00:32:38.210 --> 00:32:43.480
ARGB format in that case compared to just
operating on the three color channels.

00:32:43.500 --> 00:32:47.160
So going with planar would allow
you to just do the work that you

00:32:47.160 --> 00:32:51.500
need to do and skip over the other
stuff and touch less memory as well.

00:32:51.500 --> 00:32:55.090
Another nice thing about planar is
that it's kind of a limited form

00:32:55.090 --> 00:32:58.710
of tiling in the sense that you've
now split up your image into three

00:32:58.810 --> 00:33:00.500
smaller or four smaller parts.

00:33:00.500 --> 00:33:03.630
So in certain cases this may allow
you to exist entirely in the cache

00:33:03.720 --> 00:33:05.500
rather than half in and half out.

00:33:05.500 --> 00:33:09.580
So that would allow you to push
through several filters with just red,

00:33:09.670 --> 00:33:13.480
for example, and then move on to just
green and do pretty well.

00:33:13.520 --> 00:33:15.600
One of the problems
with geometric tiling,

00:33:15.640 --> 00:33:18.000
which is what I presented
in the previous slide,

00:33:18.000 --> 00:33:21.610
is that if you've got something with a
kernel matrix that needs to be applied

00:33:21.700 --> 00:33:24.900
and where for each pixel you need
to look at all the pixels around it,

00:33:24.900 --> 00:33:27.500
that can make tiling a little bit tricky.

00:33:27.500 --> 00:33:30.500
So with this one, we're going to do that.

00:33:30.500 --> 00:33:35.610
And then finally,
a bit of an implementation detail,

00:33:35.610 --> 00:33:41.120
a lot of our ARGB code will
take the ARGB interleave format,

00:33:41.200 --> 00:33:46.280
convert it into planar, do the work,
convert it back,

00:33:46.380 --> 00:33:49.500
and then give you the result.

00:33:49.500 --> 00:33:52.210
All that happens in register,
so it's pretty fast,

00:33:52.210 --> 00:33:54.370
but it's nicer not to
have to do it at all.

00:33:54.500 --> 00:33:57.390
So if you use planar data,
you probably will get

00:33:57.390 --> 00:33:59.500
somewhat better performance.

00:34:00.560 --> 00:34:04.310
we often see the difference is about 30%.

00:34:05.000 --> 00:37:31.100
[Transcript missing]

00:37:32.750 --> 00:37:37.510
So in practice, in order to achieve that,
the simple things, you can unroll loops.

00:37:37.610 --> 00:37:39.100
We aren't doing that to get
rid of the loop overhead.

00:37:39.100 --> 00:37:43.400
We're getting that to make sure
that we have 8 or 12 or 50 or

00:37:43.400 --> 00:37:46.340
however many parallel calculations
we have going on concurrently

00:37:46.340 --> 00:37:48.020
so we can keep the processor full.

00:37:49.040 --> 00:37:52.340
We identify and eliminate
compiler aliasing.

00:37:52.340 --> 00:37:56.520
So if you have pointers
pointing to buffers,

00:37:56.520 --> 00:38:02.640
the compiler might not know how
these overlap and it might decide to

00:38:02.680 --> 00:38:08.680
keep the load store order from load,
do operation store, load,

00:38:08.760 --> 00:38:12.310
do operation store in strict order.

00:38:12.690 --> 00:38:14.240
We want to eliminate LSU bottlenecks.

00:38:14.240 --> 00:38:17.080
A lot of code just spends all of its
time loading data in and out of register.

00:38:17.080 --> 00:38:20.060
So we look for ways to
merge small operations,

00:38:20.060 --> 00:38:22.760
many small operations
into a few big ones.

00:38:22.760 --> 00:38:25.340
And that way we can spend most
of our time actually doing work.

00:38:25.360 --> 00:38:29.540
If you have certain instructions
that are spending a lot of time,

00:38:29.540 --> 00:38:31.840
they take six, eight,
ten cycles to get through.

00:38:31.840 --> 00:38:35.460
Then we try to find enough work to keep
us busy while we wait for that to happen.

00:38:35.460 --> 00:38:38.580
We avoid branching like the plague.

00:38:40.410 --> 00:38:44.200
So we use a lot of selects and other
kinds of things to make sure that

00:38:44.210 --> 00:38:46.380
our code flies in a straight line.

00:38:46.380 --> 00:38:49.400
As I mentioned earlier,
we try to keep all the execution

00:38:49.400 --> 00:38:51.080
units busy at the same time.

00:38:51.080 --> 00:38:52.710
So if we're busy doing something
in the floating point unit,

00:38:52.720 --> 00:38:55.460
this might be a good time to also
be loading data for the next loop.

00:38:55.460 --> 00:38:57.860
So we schedule things
pretty aggressively.

00:38:57.900 --> 00:39:00.770
And finally, we prefetch our data.

00:39:00.780 --> 00:39:03.810
Just to make sure it's in the
cache when we need it so we don't

00:39:03.810 --> 00:39:07.450
have to take a long stall waiting
for data to appear out of DRAM.

00:39:07.460 --> 00:39:11.980
Insofar as our... ...tiling goes,
we only did it for some

00:39:11.980 --> 00:39:14.480
functions because we only found
only some functions benefited.

00:39:14.530 --> 00:39:17.930
Generally what we did was we
took a look at... ...first the

00:39:17.930 --> 00:39:20.580
experiment I suggested earlier,
run a small image and a big one

00:39:20.580 --> 00:39:23.570
and see whether there is some
improvement for doing smaller images.

00:39:23.580 --> 00:39:26.380
We also took a look at
different tile shapes.

00:39:26.380 --> 00:39:30.770
So here you see a graph where I've
taken a 3x3 kernel and a 21x21

00:39:30.810 --> 00:39:32.790
kernel for the same function.

00:39:32.800 --> 00:39:36.110
And looked at how much time it
takes for different tile widths.

00:39:36.120 --> 00:39:37.320
The tiles are all the same size.

00:39:37.320 --> 00:39:41.820
It's just we widen them and... ...shrink
them vertically at the same time.

00:39:41.820 --> 00:39:44.960
So you can see that there is
some advantage to a particular

00:39:44.960 --> 00:39:46.010
tile width in this case.

00:39:46.060 --> 00:39:49.060
1,024,
2048 bytes is probably the optimal case.

00:39:49.060 --> 00:39:50.520
So that's what we choose.

00:39:52.490 --> 00:39:57.390
So, and then of course we tune
these things per processors.

00:39:57.390 --> 00:40:00.280
We actually end up running this
experiment several times to make

00:40:00.280 --> 00:40:05.220
sure that the tile sizes we pick for
G3 are optimal for G3 and the ones we

00:40:05.220 --> 00:40:08.270
pick for G4 and G5 are optimal there.

00:40:08.850 --> 00:40:11.860
Finally, just to stress,
like everything else in

00:40:11.870 --> 00:40:13.350
Accelerate framework,
we vectorize.

00:40:13.420 --> 00:40:16.790
So our intent is to use the
velocity engine across the

00:40:16.790 --> 00:40:18.220
board everywhere we can.

00:40:18.220 --> 00:40:23.880
So you'll see that in the final product,
we're going to have Altevec

00:40:23.880 --> 00:40:25.640
pretty much everywhere.

00:40:25.640 --> 00:40:27.430
The only exception is
going to be Histogram,

00:40:27.430 --> 00:40:29.400
which is a class of functions
that just don't work very

00:40:29.400 --> 00:40:30.180
well with the vector unit.

00:40:30.180 --> 00:40:35.000
Typical speedups we see over scalar
code for that is 4 to 10 times.

00:40:35.000 --> 00:40:37.020
If you haven't tried vectorization,
I suggest you do.

00:40:38.480 --> 00:40:41.220
That doesn't mean that our
scalar code is any sludge.

00:40:41.220 --> 00:40:42.730
We make sure that runs
as fast as possible,

00:40:42.730 --> 00:40:43.040
too.

00:40:43.040 --> 00:40:46.880
And in a couple of cases,
such as our sampling filters,

00:40:46.880 --> 00:40:49.980
we use the extra speed to deliver
a lot better image quality.

00:40:49.980 --> 00:40:52.650
So hopefully you'll like that.

00:40:52.660 --> 00:40:57.190
And I've got this as a beta release,
so I'm quite finished.

00:40:57.200 --> 00:40:59.030
Every bit of vectorization
I'd like to do,

00:40:59.030 --> 00:41:00.800
but certainly we're working hard at it.

00:41:00.920 --> 00:41:04.920
Finally,
experimentally driven optimization.

00:41:04.920 --> 00:41:07.560
We never guess.

00:41:08.350 --> 00:41:10.360
If we find we are guessing,
we try to figure out how to run

00:41:10.360 --> 00:41:13.450
the right experiment to find
out what's actually going on.

00:41:13.460 --> 00:41:15.900
So obviously, always profile.

00:41:15.900 --> 00:41:16.900
I'm sure you've heard that before.

00:41:16.900 --> 00:41:19.760
You can use tools like gprof and sampler,
but those only give you

00:41:19.760 --> 00:41:21.400
function-level information.

00:41:21.400 --> 00:41:24.560
It'll only tell you which
function is performing slowly.

00:41:24.560 --> 00:41:28.410
It won't tell you why or what
part of it or what instruction

00:41:28.410 --> 00:41:30.900
in particular is getting a stall.

00:41:30.900 --> 00:41:34.720
So actually, most of our work is done
using Chud or Shark,

00:41:34.720 --> 00:41:36.420
which they're going to
talk about later on today.

00:41:36.420 --> 00:41:38.140
And I'll invite Eric to
give us a short overview.

00:41:38.240 --> 00:41:39.380
Eric Chud: Thank you, Eric.

00:41:39.380 --> 00:41:41.480
So we're going to talk about what
we're doing with our CPU for.

00:41:41.480 --> 00:41:43.580
And we also use
CPU simulators like SIMG4.

00:41:43.580 --> 00:41:48.010
And so these things can be used
to actually narrow in and directly

00:41:48.120 --> 00:41:52.220
tell whether we're running into
cache misses or paging or any

00:41:52.220 --> 00:41:56.730
of a number of other problems,
which historically have been very

00:41:56.740 --> 00:41:59.680
hard to tell what exactly is going on.

00:41:59.680 --> 00:42:01.760
And you're just kind of
guessing what's going on.

00:42:01.780 --> 00:42:02.880
But we don't.

00:42:02.880 --> 00:42:04.980
We zero in on a problem and solve that.

00:42:04.980 --> 00:42:07.980
And that lets us very efficiently
get to the high-performance code.

00:42:08.020 --> 00:42:11.080
So, finally, if you aren't already,
I'll urge you to inspect our

00:42:11.080 --> 00:42:14.180
compiler output for functions
that really make a difference,

00:42:14.200 --> 00:42:18.950
since we are almost always surprised
by some of the mistakes we make.

00:42:19.020 --> 00:42:22.680
So with that,
I'll introduce Eric Miller from the

00:42:22.880 --> 00:42:27.060
Architecture Performance Group to
come up here and tell you

00:42:27.060 --> 00:42:31.000
a little bit about Chud,
which is the tool that

00:42:31.000 --> 00:42:33.330
we use to tune our code.

00:42:39.750 --> 00:42:42.900
Good morning, I'm Eric Miller with the
Architecture Performance Group.

00:42:42.900 --> 00:42:47.260
As Ian said, the CHUD tools are one
of his favorite toys,

00:42:47.260 --> 00:42:49.100
and I'm glad he put it on the list,
although I would like to see

00:42:49.100 --> 00:42:52.290
him reverse the order and put
it above GPROF and Sampler,

00:42:52.290 --> 00:42:53.770
but that's just me.

00:42:54.260 --> 00:42:57.260
So, what are CHUD tools?

00:42:57.260 --> 00:43:00.100
Well, there are a suite of
performance analysis tools.

00:43:00.100 --> 00:43:03.510
There are several that are interesting,
probably the most interesting

00:43:03.510 --> 00:43:06.570
we'll get to in a minute,
but the idea behind them is that

00:43:06.580 --> 00:43:11.390
they give you low-level access to the
performance monitor hardware counters in

00:43:11.590 --> 00:43:15.650
the processor and the memory controller,
and then we have implemented some

00:43:15.650 --> 00:43:19.870
software versions in the operating system
that behave exactly the same as the

00:43:19.870 --> 00:43:22.410
hardware performance monitor counters.

00:43:25.020 --> 00:43:29.890
The idea is to help you find problems
and improve your code performance.

00:43:29.930 --> 00:43:32.900
And the best part is they're
freely available on the web.

00:43:33.010 --> 00:43:34.900
And they're also on
the Developer Tools CD.

00:43:34.900 --> 00:43:38.740
One of the neat things about the
CD this year is that you'll be able

00:43:38.740 --> 00:43:42.490
to install the tools and immediately
we have a Chud updater which is

00:43:42.530 --> 00:43:44.930
very similar to Software Updater.

00:43:45.800 --> 00:44:01.900
[Transcript missing]

00:44:02.410 --> 00:44:08.100
So we generally will put out a
release every week at least during

00:44:08.100 --> 00:44:11.800
the beta period and probably
slightly reduce the frequency

00:44:11.800 --> 00:44:14.840
later once we have it gold master.

00:44:15.200 --> 00:44:16.990
So there are three main tools.

00:44:17.070 --> 00:44:19.810
The first tool is a
profiling tool called Shark,

00:44:19.810 --> 00:44:21.890
which Ian alluded to.

00:44:21.970 --> 00:44:24.390
It is an instruction level profiler.

00:44:24.510 --> 00:44:27.830
It can do many things that
we'll get to in a minute.

00:44:27.830 --> 00:44:33.850
And not the least of which is,
Ian mentioned that you can inspect your

00:44:34.000 --> 00:44:42.600
[Transcript missing]

00:44:42.700 --> 00:45:01.100
[Transcript missing]

00:45:01.750 --> 00:45:06.450
Saturn is a call-grab visualizer,
as it says.

00:45:06.510 --> 00:45:09.300
What the idea there is,
is it's kind of like using Gprof.

00:45:09.310 --> 00:45:11.600
It goes through and actually
instruments all your application

00:45:11.600 --> 00:45:17.670
code and then produces the results of
how often the functions get called.

00:45:17.720 --> 00:45:21.830
But you can also have auxiliary
information with regard to

00:45:21.830 --> 00:45:24.670
performance monitor counts.

00:45:25.170 --> 00:45:27.620
We also have several tracing tools.

00:45:27.640 --> 00:45:30.300
Amber,
which actually when you run Amber can

00:45:30.300 --> 00:45:35.390
collect every single instruction that is
executed on behalf of your application on

00:45:35.500 --> 00:45:38.280
the processor and put that into a file.

00:45:38.300 --> 00:45:41.980
And then those files
will be consumed by ACID,

00:45:41.980 --> 00:45:45.020
which is a tool that
we wrote in our group,

00:45:45.060 --> 00:45:49.200
and by SIMG4,
which is produced by Motorola, and SIMG5,

00:45:49.260 --> 00:45:51.580
which will be produced by IBM.

00:45:51.660 --> 00:45:54.680
Those are cycle-accurate CPU simulators.

00:45:54.680 --> 00:46:00.420
And of course Ian and his team use
the SIMG4 product quite readily.

00:46:00.970 --> 00:46:03.900
The other thing you can
do with the CHUD tools is

00:46:03.900 --> 00:46:06.240
instrument your applications.

00:46:06.240 --> 00:46:09.600
And along with that,
I'm running out of dots on the slide,

00:46:09.600 --> 00:46:12.940
but you can also create your own
application performance analysis

00:46:12.940 --> 00:46:16.320
tools using the CHUD framework because
that's the exact framework that we

00:46:16.320 --> 00:46:20.090
developed in order to create Shark,
Monster, and Saturn.

00:46:21.670 --> 00:46:24.210
So, I mentioned performance
counters several times.

00:46:24.340 --> 00:46:24.950
What are they?

00:46:25.160 --> 00:46:27.140
Well,
they're a series of dedicated special

00:46:27.210 --> 00:46:31.180
purpose registers actually in the
processor and in the memory controller

00:46:31.180 --> 00:46:36.120
that we create that's in the G4,
G5, and G3 systems.

00:46:36.500 --> 00:46:43.110
So what we can do with those is
set them up to count and record

00:46:43.120 --> 00:46:44.640
what we call performance events.

00:46:44.640 --> 00:46:48.810
Things like the number of L1 cache
misses or L2 cache misses or L3

00:46:48.940 --> 00:46:56.340
cache misses or instruction counts,
instruction misses, execution stalls,

00:46:56.340 --> 00:46:58.720
page faults in the operating system.

00:46:58.720 --> 00:47:01.220
There are a plethora of events.

00:47:01.240 --> 00:47:03.900
In fact, on G4,
you have something in the order of

00:47:03.900 --> 00:47:06.220
maybe 200 events you can measure.

00:47:06.400 --> 00:47:09.800
On G5, there are literally thousands
of events that can be measured.

00:47:12.730 --> 00:47:16.540
So we use the CHUD tools,
and in particular the CHUD framework,

00:47:16.540 --> 00:47:20.500
to configure and control all the PMCs.

00:47:21.170 --> 00:47:23.680
So, I'm not going to do any
demos this morning because

00:47:23.680 --> 00:47:26.000
we're pretty short on time,
but I just wanted to mention

00:47:26.000 --> 00:47:30.150
Shark because all you do to use
Shark is push the start button and

00:47:30.160 --> 00:47:32.100
it will profile the entire system.

00:47:32.100 --> 00:47:36.000
It defaults to a time profile and
what that will give you is in your

00:47:36.000 --> 00:47:42.220
application when you select it from the
list of profiled threads or processes,

00:47:42.220 --> 00:47:46.290
you'll see where in your application
in relation to your source code will

00:47:46.290 --> 00:47:48.940
highlight it for you and show you
this is where you spent your time.

00:47:48.940 --> 00:47:52.890
If you do an event profile,
suppose you selected CPU cycles,

00:47:52.890 --> 00:47:56.680
Shark can tell you exactly how
many cycles were spent in your

00:47:56.680 --> 00:47:59.150
code for a particular line of code.

00:47:59.180 --> 00:48:02.610
And Shark captures every
single thread on the system,

00:48:02.610 --> 00:48:05.660
the driver,
any drivers or kernel extensions,

00:48:05.670 --> 00:48:08.800
the kernel itself,
and all the applications that

00:48:08.800 --> 00:48:10.860
are running at any given time.

00:48:10.940 --> 00:48:13.500
The best thing about Shark is
it's very low overhead,

00:48:13.520 --> 00:48:14.930
as are all the Chud tools.

00:48:14.940 --> 00:48:16.720
You can actually set the time profile
down to a minimum of about 50 seconds.

00:48:16.780 --> 00:48:16.780
So, you can actually set the time profile
down to a minimum of about 50 seconds.

00:48:16.800 --> 00:48:18.760
So, you can actually set the time profile
down to a minimum of about 50 seconds.

00:48:18.880 --> 00:48:23.230
50 microseconds per time sample,
which is a couple of order magnitudes

00:48:23.230 --> 00:48:25.860
smaller than you can use with sampler.

00:48:26.940 --> 00:48:32.420
It also gives you an automated analysis
which will show up as this column of

00:48:32.470 --> 00:48:35.160
exclamation points beside your code.

00:48:35.160 --> 00:48:38.590
So we annotate your source code,
you click on these annotations,

00:48:38.680 --> 00:48:43.840
and it will tell you things like
this loop has a non-changing

00:48:43.840 --> 00:48:47.050
variable and it's serialized,
so you might want to move

00:48:47.050 --> 00:48:50.220
that variable out of the loop,
or if this loop is a good candidate

00:48:50.340 --> 00:48:54.120
for Altevec or parallelization because
there aren't any data dependencies.

00:48:55.260 --> 00:48:59.450
We do static analysis and this
can lead to the surprises that

00:48:59.530 --> 00:49:01.920
Ian mentioned from the compiler.

00:49:01.930 --> 00:49:04.770
It actually will show you the
disassembly with the compiler

00:49:04.800 --> 00:49:08.780
generated on your behalf and annotate
that as to how many stalls you'll have,

00:49:08.840 --> 00:49:14.730
how many delays might be
involved from other aspects.

00:49:14.900 --> 00:49:18.640
New features this year from, well,
let me say this,

00:49:18.650 --> 00:49:22.500
Shark was formerly called Shikari
in the Chud Tools from last year.

00:49:22.500 --> 00:49:25.460
So it's been renamed Shark with
a lot of new features.

00:49:25.530 --> 00:49:29.300
One of the features is that you can
now save and review all the sessions

00:49:29.300 --> 00:49:31.800
that you collect for later analysis.

00:49:31.800 --> 00:49:35.240
And there's also a command line
version that you can use to

00:49:35.240 --> 00:49:37.500
instrument with scripts and things.

00:49:37.500 --> 00:49:41.800
So we use this command line version
of Shark whenever we have a old

00:49:41.830 --> 00:49:45.500
Unix scientific application that
just runs in the command line.

00:49:45.690 --> 00:49:47.500
And it has a launch script.

00:49:47.500 --> 00:49:51.080
We can just script Shark to begin and
then run our command line application

00:49:51.080 --> 00:49:53.500
as normal and then script Shark to end.

00:49:55.430 --> 00:49:58.270
Here's a little photograph of SHARC.

00:49:58.270 --> 00:50:02.120
And what you can see in the
left-hand picture would be the

00:50:02.210 --> 00:50:04.400
result of actually a time profile.

00:50:04.400 --> 00:50:07.640
And in this particular picture,
we were running a test and it

00:50:07.700 --> 00:50:11.400
turned out that the square root
function was 42% of the time.

00:50:11.400 --> 00:50:15.000
At the bottom of that left-hand picture,
you can see there's a

00:50:15.170 --> 00:50:18.370
little process menu,
and that lists all the processes

00:50:18.390 --> 00:50:20.390
that were running on the
system when you did the trace.

00:50:20.400 --> 00:50:25.400
You can choose from any of those,
and normally you would choose your own.

00:50:25.400 --> 00:50:28.670
This is a screenshot
from last year's demo.

00:50:29.000 --> 00:51:10.200
[Transcript missing]

00:51:10.580 --> 00:51:13.890
The next tool is Monster,
which is the most direct way

00:51:13.900 --> 00:51:17.470
to configure and set up the
performance monitor counters.

00:51:19.320 --> 00:51:24.060
You can use a, in CHUD tools in general,
there are timed intervals so you

00:51:24.060 --> 00:51:27.680
can select a certain number of
milliseconds or microseconds or seconds

00:51:27.680 --> 00:51:33.540
for that matter that you would like
to collect per sample in the hardware.

00:51:33.540 --> 00:51:37.250
You can also collect data
based on other events.

00:51:37.250 --> 00:51:40.620
You can set it up to collect a
sample every so many cycles or

00:51:40.620 --> 00:51:44.900
every so many instructions completed
or every so many cache misses.

00:51:45.740 --> 00:51:50.260
There's also a third way that actually is
related to both which is called a hotkey.

00:51:50.260 --> 00:51:52.420
All the CHUD tools have a global hotkey.

00:51:52.420 --> 00:51:56.680
In the case of Shark it's Option Escape,
in the case of Monster it's

00:51:56.840 --> 00:51:57.800
Command Escape.

00:51:57.800 --> 00:52:03.090
And if you use those keys,
you don't actually have to have

00:52:03.090 --> 00:52:05.720
the application in front of the

00:52:06.350 --> 00:52:13.300
and David Koehn.

00:52:13.300 --> 00:52:13.300
The next session will be about
the application of the Monster.

00:52:13.300 --> 00:52:39.470
The information from the memory
controller about transactions,

00:52:39.470 --> 00:52:41.740
reads and writes,
and you know the amount of time

00:52:41.740 --> 00:52:44.260
because they're sampled over time,
for example.

00:52:44.280 --> 00:52:47.640
Then you can take those
transactions and apply a calculation

00:52:47.640 --> 00:52:49.440
to them we call shortcuts.

00:52:49.440 --> 00:52:53.140
So you can say every read is 16 bytes,
so I take the number of reads,

00:52:53.140 --> 00:52:56.190
multiply by 16 bytes,
I have the number of bytes,

00:52:56.190 --> 00:52:58.820
divide by the time, I have the bandwidth.

00:52:58.820 --> 00:53:03.230
So you can set up these calculations
in Monster and have additional

00:53:03.230 --> 00:53:05.490
columns in your spreadsheet.

00:53:05.790 --> 00:53:07.880
and these calculations
are just standard infix,

00:53:07.920 --> 00:53:10.980
mathematic notation with
parentheses and it's basically

00:53:11.080 --> 00:53:13.120
a four function calculator.

00:53:13.340 --> 00:53:16.850
There's a table and you can
also draw charts and Shark is

00:53:16.860 --> 00:53:20.300
also capable of drawing charts.

00:53:20.300 --> 00:53:22.800
Then you can also,
new in this version of Monster,

00:53:22.800 --> 00:53:25.990
you can save and review the sessions
and the nice thing about this is

00:53:26.150 --> 00:53:29.820
you can review sessions on a system
that you don't have in front of you.

00:53:29.820 --> 00:53:32.760
So you could do collections if
you had a G5 at your disposal.

00:53:32.760 --> 00:53:36.450
You could collect data with
Shark or Monster on your G5,

00:53:36.450 --> 00:53:41.830
then take it back to your laptop or
your desktop G4 or even your iMac and

00:53:42.100 --> 00:53:47.090
review those results and print off
the charts and those sorts of things.

00:53:47.100 --> 00:53:48.720
And there's also a scriptable
command line version of

00:53:48.720 --> 00:53:49.860
Monster which is new this year.

00:53:51.910 --> 00:53:53.560
Here's a screenshot of Monster.

00:53:53.560 --> 00:53:56.470
On the left of the first
of the leftmost image,

00:53:56.480 --> 00:54:00.090
there's a column that where you
if you click on those entries,

00:54:00.110 --> 00:54:01.800
will highlight those columns in the data.

00:54:01.800 --> 00:54:05.910
So,
and when you highlight columns of data,

00:54:05.940 --> 00:54:10.800
you can then just press the draw chart
button and it would result in a chart.

00:54:10.800 --> 00:54:12.800
And there's many options for charting.

00:54:12.800 --> 00:54:15.910
There's bar charts,
various colorizations,

00:54:16.020 --> 00:54:21.030
line charts with markers,
logarithmic scales, direct scales,

00:54:21.120 --> 00:54:25.280
samples over time,
and samples as a single

00:54:25.280 --> 00:54:29.800
x-axis just per sample plots.

00:54:29.800 --> 00:54:33.890
You can see in this particular
case that what's been highlighted

00:54:33.890 --> 00:54:35.800
are some of the shortcuts.

00:54:35.800 --> 00:54:38.210
So a load store session was done,
so all the load

00:54:38.210 --> 00:54:39.800
instructions were collected.

00:54:39.850 --> 00:54:40.800
All the store instructions
were collected.

00:54:40.800 --> 00:54:43.800
And all the regular
instructions were collected.

00:54:43.820 --> 00:54:47.110
And then percentages of each
were calculated along with that,

00:54:47.110 --> 00:54:48.740
you know, for every sample.

00:54:49.020 --> 00:54:52.680
Each sample is listed
horizontally in the table there.

00:54:52.800 --> 00:54:55.800
And so vertically is
each of these shortcuts.

00:54:55.800 --> 00:54:59.800
So then you just highlight those columns
of shortcuts and we plot the percentages,

00:54:59.800 --> 00:55:01.800
which is what you see
in the second picture.

00:55:02.170 --> 00:55:06.170
There is quite an extensive set
of sampling controls to configure

00:55:06.270 --> 00:55:10.510
the performance monitor counters
in both Shark and Monster.

00:55:12.180 --> 00:55:15.160
So the last thing is a
new tool we call Saturn,

00:55:15.160 --> 00:55:19.530
which record, like it says,
you record your function call history,

00:55:19.640 --> 00:55:24.090
and the way we do this is by
instrumenting the functions

00:55:24.180 --> 00:55:26.090
at entry and exit with GCC.

00:55:26.100 --> 00:55:29.500
There's a compiler flag that
you throw and do a build,

00:55:29.500 --> 00:55:34.630
and it'll inject all the Saturn entry
and exit prolog and epilog functions

00:55:34.640 --> 00:55:37.570
in every function in your application.

00:55:37.610 --> 00:55:41.190
Now, to be completely thorough,
you have to go through and recompile

00:55:41.190 --> 00:55:45.800
all of the frameworks and libraries,
and that's similar to Gprof,

00:55:45.800 --> 00:55:50.490
which is really not that fun to do,
so most of the time we like to focus

00:55:50.490 --> 00:55:52.700
just on actual application code.

00:55:52.700 --> 00:55:55.920
But the nice thing about Saturn is that
once you have this function call history,

00:55:55.920 --> 00:56:00.310
you can visualize that call tree,
and here in this image you can see

00:56:00.380 --> 00:56:04.890
that the call tree for CSE under
main has been highlighted,

00:56:04.890 --> 00:56:07.560
and you see the red dashes in
that stack of the call tree.

00:56:07.560 --> 00:56:08.830
And you can see the red bars there.

00:56:09.040 --> 00:56:11.660
That's where that function
is called and run.

00:56:11.660 --> 00:56:14.490
So what you would want
to use Saturn for is,

00:56:14.490 --> 00:56:18.200
in particular with C++,
you have a lot of call depth.

00:56:18.200 --> 00:56:20.560
So you want to -- things
are very skinny and tall.

00:56:20.560 --> 00:56:23.450
You're spending a lot of time calling
functions and not doing any work,

00:56:23.460 --> 00:56:24.840
so you want to try to avoid that.

00:56:24.850 --> 00:56:26.480
You want to have a nice flat profile.

00:56:26.480 --> 00:56:29.840
You can also collect call counts,
PMC event counts,

00:56:29.840 --> 00:56:33.970
and execution times by using the
performance monitor counters with

00:56:33.970 --> 00:56:37.330
your instrumented functions that
are injected at entry and exit

00:56:37.360 --> 00:56:39.580
of your each of your functions.

00:56:41.290 --> 00:56:44.380
So, as I mentioned on the first slide,
we've got the instruction

00:56:44.380 --> 00:56:45.500
tracing and simulation.

00:56:45.540 --> 00:56:48.150
AMBER is the instruction
tracing mechanism,

00:56:48.150 --> 00:56:52.260
and the resultant files are
in a format that's called TT6.

00:56:52.260 --> 00:56:57.440
These TT6 files are consumed by the
other programs mentioned on this slide,

00:56:57.460 --> 00:57:00.970
which is a trace -- ACID is
our internal trace analyzer,

00:57:00.970 --> 00:57:04.420
and actually,
the ACID trace analyzer is the

00:57:04.420 --> 00:57:09.080
parent of the code coach and the
parts in SHARC that explain why

00:57:09.220 --> 00:57:13.540
you have bottlenecks and what
you might do to change them.

00:57:13.540 --> 00:57:15.320
These come out of ACID.

00:57:15.320 --> 00:57:16.950
And it can also do a
couple things on its own,

00:57:16.950 --> 00:57:18.700
which is memory footprint
of your application.

00:57:18.700 --> 00:57:21.660
It'll give you a little GNU plot file.

00:57:21.660 --> 00:57:26.200
You can find your instruction
sequences that may be an issue and

00:57:26.200 --> 00:57:32.220
then try and remove those through the
informational notes that it gives you.

00:57:32.220 --> 00:57:36.980
SIMG4 is a cycle accurate
simulator for PPC7400,

00:57:36.990 --> 00:57:40.960
which is an old MAX processor
from the early G4 systems.

00:57:40.960 --> 00:57:45.910
And SIMG5 will be available in
the future -- in the near future,

00:57:45.910 --> 00:57:51.240
and that'll be a cycle accurate
simulator for the new PPC970.

00:57:51.240 --> 00:57:54.050
These can be quite handy
in tracing particularly

00:57:54.760 --> 00:57:59.730
complicated performance issues,
although the output to SIMG4 and

00:57:59.730 --> 00:58:02.390
SIMG5 requires a terminal window.

00:58:02.600 --> 00:58:07.720
Maybe -- maybe would require maybe
a 50 inch monitor that would work.

00:58:09.200 --> 00:58:11.910
Lastly,
the CHUD framework is available to,

00:58:11.910 --> 00:58:14.640
like I said, instrument your source code.

00:58:14.640 --> 00:58:17.180
One of the things you can do
with instrumentation is do

00:58:17.180 --> 00:58:21.320
one function call to start and
stop monster or shark sampling.

00:58:21.320 --> 00:58:26.780
So you can sort of put a caliper
around your interesting code.

00:58:26.780 --> 00:58:28.850
Suppose you find a piece of code
that Shark says is a hotspot and

00:58:29.020 --> 00:58:32.070
you want to get more detailed
and just trace through that,

00:58:32.190 --> 00:58:33.190
you can add code.

00:58:33.220 --> 00:58:37.110
It's CHUD start remote
performance monitor and CHUD stop

00:58:37.110 --> 00:58:39.220
remote performance monitor.

00:58:39.220 --> 00:58:41.950
And what happens is you set it,
you just click a key in monster or

00:58:41.950 --> 00:58:46.110
shark and it will be in remote mode
and be waiting for messages from your

00:58:46.110 --> 00:58:48.820
application and your application only.

00:58:48.820 --> 00:58:48.890
So you can just...

00:58:51.160 --> 00:58:51.750
and many others.

00:58:51.830 --> 00:58:54.000
You can just collect the data
for your interesting code.

00:58:54.000 --> 00:58:57.580
You can directly read and report on the
PMCs by writing small pieces of code,

00:58:57.690 --> 00:59:00.860
either instrumented in your
application or write a separate

00:59:00.860 --> 00:59:02.540
stand-alone application.

00:59:02.540 --> 00:59:05.540
As I mentioned,
you can write your own performance

00:59:05.710 --> 00:59:09.980
tools and do all the things that
need to be done in order to create

00:59:09.980 --> 00:59:12.680
a performance tool like SHARC,
which is control the

00:59:12.730 --> 00:59:15.590
performance monitor counters,
collect the information

00:59:15.590 --> 00:59:18.070
about the system hardware,
which can be handy in a lot of ways.

00:59:18.070 --> 00:59:21.020
You can know that you're on a G5,
you can know that you're on a G3,

00:59:21.050 --> 00:59:24.960
you can know the bus speed of the system,
the amount of memory in the system,

00:59:24.960 --> 00:59:26.400
number of processors.

00:59:26.400 --> 00:59:28.460
You can also modify some
of that information.

00:59:30.490 --> 00:59:34.320
And there also is an HTML reference
document online that describes all the

00:59:34.320 --> 00:59:37.110
various functions in the CHUD framework.

00:59:37.430 --> 00:59:40.100
Here's a small example of
code with the CHUD framework.

00:59:40.100 --> 00:59:42.100
And this is, I mentioned,
that instrument your code to

00:59:42.100 --> 00:59:44.580
start and stop Shark or Monster.

00:59:44.580 --> 00:59:48.330
So you just have to include
the CHUD H header file,

00:59:48.340 --> 00:59:52.740
initialize,
and then acquire the remote access,

00:59:52.800 --> 00:59:55.970
start Remote Performance Monitor with
a label that will show up in your

00:59:55.970 --> 00:59:59.800
output in Shark or Monster so you
know which instrumentation it was.

00:59:59.830 --> 01:00:03.180
Then you run through your important code,
stop the monitor,

01:00:03.180 --> 01:00:04.870
release the remote access.

01:00:05.010 --> 01:00:09.140
Secondly, a slightly more complex mode,
I mentioned you can write your

01:00:09.150 --> 01:00:11.120
own performance monitoring tool.

01:00:11.120 --> 01:00:13.140
You initialize,
acquire the sampling facility,

01:00:13.140 --> 01:00:17.130
you turn on some special filters,
maybe mark your process as

01:00:17.260 --> 01:00:20.890
the only one to be counted,
and then you set the events.

01:00:20.920 --> 01:00:23.780
In particular, you say both CPUs,
process performance

01:00:23.780 --> 01:00:26.240
monitor counter number one,
event number one,

01:00:26.240 --> 01:00:28.820
which happens to be cycles,
and event number two,

01:00:28.910 --> 01:00:30.740
which happens to be instructions.

01:00:30.960 --> 01:00:34.840
Clear the counters, start the counters,
your important function executes,

01:00:34.840 --> 01:00:37.660
stop the counters,
then you collect these results and you

01:00:37.660 --> 01:00:42.080
can perform a calculation and get cycles
per instruction in your own application.

01:00:44.140 --> 01:00:46.930
For more information about that stuff,
you can get your own

01:00:46.930 --> 01:00:52.850
download at this web address:
developer.apple.com/tools/debuggers.html

01:00:52.930 --> 01:00:56.110
And then you can always contact
myself and my colleagues on the

01:00:56.160 --> 01:00:59.340
Chud Tools Development Team at
this email address.

01:00:59.340 --> 01:01:02.440
And we try to be pretty responsive,
and that's probably the best

01:01:02.440 --> 01:01:06.170
way to get your feature requests
and complaints into our queue.

01:01:07.100 --> 01:01:07.940
Let's see what's next.

01:01:07.940 --> 01:01:08.800
Oh, I guess I'm done.

01:01:08.800 --> 01:01:10.820
So let me bring up Mr.

01:01:10.820 --> 01:01:11.340
Keithley.

01:01:11.340 --> 01:01:12.980
That'd be great.

01:01:20.930 --> 01:01:23.500
So the roadmap,
a couple more sessions today,

01:01:23.500 --> 01:01:28.720
obviously one specializing
in CHUD itself.

01:01:28.720 --> 01:01:30.280
We should move on to Q&A pretty quickly.

01:01:30.280 --> 01:01:32.520
We're into that time right now.

01:01:32.520 --> 01:01:36.140
Here's some contact info.

01:01:36.140 --> 01:01:37.780
Our reference library information.