---
Title:  Deploying Xserve RAID
Year:   2003
Web:    https://nonstrict.eu/wwdcindex/wwdc2003/626/

[!] This transcript was generated using Whisper, it has known transcription errors.
---

I'm really glad to be here today to talk to you about Xserve RAID and deploying it. My name is Alex Grossman. I'm a director of hardware storage for Apple. And today I think we're going to have a little bit of fun. Let me start with saying that there's an IT mantra that's actually out there.

There's a couple things that it's covered. The first thing regarding storage is that today we're seeing storage needs grow 100% yearly. And that's not only because everybody's downloading from the iTunes Music Store, but it's really because people are actually building more content. And a little bit because they're downloading from the iTunes Music Store.

The other thing we're seeing is that managing storage is becoming a full-time job. It used to be where you deployed a server and you had a hard drive in the server and maybe you had another hard drive for storage and you kept deploying servers and everyone had storage and as you managed the server, you managed the storage. But as things moved to more network-attached storage and more direct-attached storage and also storage-area networking, storage management becomes a full-time job.

And then the other thing is that people are starting to classify storage. So there's things coming up called storage classes, and we'll talk a little bit about what storage classes are. And what they're doing is they're, they're the attempt to reduce the cost and complexity in the IT world.

And of course, configuration and planning. That's the one thing that's key to efficient deployments. Because today, a lot of people deploy storage and they do it in a haphazard way. And it's not purposely, it's just that storage needs are growing so fast, how do we keep up? And then of course, storage cost. Well, Apple with Xserve RAID has really reduced the cost of storage and we've gotten a lot of heat from our competitors on that.

So what are we really doing about that? Well, the first thing is Xserve RAID is the highest density in a 3U space. In fact, I have about 17 terabytes of storage up here on the stage with me and it's a little warm, but it's powerful. Feels really good.

And of course, fast deployment and easy-to-use tools make storage installation and management a lot easier. So if you're going to deploy something, you want to deploy it fast. And the one thing you have to remember about storage is that you deploy it once and you don't touch it every day. I mean, most of you, you'll be in Word, you'll be in Excel. If you're coding, you're going to use tools every day, the same tools.

But storage, you set it up once, and the only time you go back to it is when you have a problem or when you want to add more. So if you have tools that are easy to use, it just makes that faster. And of course, Xserve RAID has been built to have the maximum versatility and flexibility to be able to get you over this storage class issue and use it in many different areas. And then of course, when it comes to configuring and planning, there's just no substitute for hard work.

You just got to do it. Now, there's methodology you should follow, but you just have to do it. And then of course, Xserve RAID, it's inexpensive. So what we're going to try to go through today is some really simple stuff. We're going to introduce you to Xserve RAID. If you haven't seen the product, get an idea why Apple did it.

I mean, it's kind of interesting. Apple has not really been in a high availability storage business until this year. In fact, Xserve RAID didn't launch. It only launched four months ago. And if you look at most other vendors of all kinds of hardware, including servers, they've been in a RAID business for many years. Then we're going to go through a little bit of speeds and feeds, not a lot, but just a little bit about the architecture. Because it's completely different than anything that anyone else has done.

And then we're going to talk about some of the applications in IT, some of the storage classing. Talk about planning considerations. And then of course, deployment and configurations. And then I'm going to give you a sneak peek at RAID admin, the newest, latest version, 1.1, which isn't out yet.

And so you get an idea of what we're doing to progress. And RAID admin is our management tool for Xserve RAID. You'll get an idea of how we're progressing that along. So the first thing is an intro. So what is Xserve RAID? Well, there's a lot of them here. These are the big boxes. So there are three U-high box. That means they're five and a quarter inches.

They're 19 inches tall. They're 19 inches wide. Fully loaded, they weigh about 110 pounds. So the first thing to remember when deploying one is don't do it yourself. It's the buddy system. And never put them in the top of the rack. It's always a good thing. And if you do, don't stand under it.

And it's massive in size, but it's also massive in capacity. So it boasts today the highest capacity in a 3U system. And of course, that changes every day. Hard drives get faster and bigger. And of course, Apple is always on the cutting edge with that. We did that with XServe.

We started with 60 gig hard drives, and now we have 180 gig hard drives, same as with XServe RAID. And then the other thing is that XServe RAID is a high availability storage system that uses an advanced architecture that is unlike what a lot of other people have done.

In fact, we really think it's a precursor to what we're seeing from competitors in the future. It uses ATA hard drives and a fiber channel back end to the host connection. It's something that you're hearing more and more about, but Apple was really the first tier one vendor to pioneer it and put it in to a high availability. And then of course, it's Apple's ease of use. Remote monitoring and management, similar to what we've done with XServe.

And in every generation, we think we're getting a little better. And I'm going to give you a sneak preview of our next generation of remote monitoring and management. And then of course, industry leading value. And we're talking two and a half terabytes of storage for around 10 grand. It's not too bad.

Now, we designed Xserve RAID for nonstop operation. A lot of definitions of what that means. Basically, it's a high availability, fault-tolerant architecture. That means if you have a power supply fail in the system, there's a redundant one to take over. That means if you have a cooling unit fail in the system, there's redundant cooling. You can build protected RAID sets if you have a hard drive fail, there's another hard drive to take over for it.

And then we even have hot sparing of hard drives so that if one fails, another one will take over, rebuild, and completely protect you. And that's the way we designed the system. And then, of course, we wanted to make it complete. The thing we realized is that fiber channel storage was pretty expensive. And it wasn't just the storage systems, it was the infrastructure.

So we lowered the cost, not only of the storage, but we lowered the cost of the infrastructure. We built a fiber channel PCI card that we sell at about a third of the price of what others sell it for. And we can't figure out why they charge so much. And then, of course, we include everything you need to complete it and to work with an Xserve, a G4, a G5.

And it's not just for the rack, because not everybody has a rack installation, and not everybody, although I think everyone should run an Xserve, not everybody's going to run an Xserve. And there are other deployment areas where Xserve RAID is great, such as in the video area. So you can take a product like the Extrovert from Extreme Mac that takes the Xserve RAID and puts it on its side. Two and a half terabytes on your desktop.

or under your desk. It's not terrible. Well, let's talk about speeds and feeds. Well, I was going to give you all the speeds and feeds, and I decided, you know what, let's really look at the numbers and see what they mean, because speeds and feeds are useless if you really can't figure out how they work in your application. So we have 14 independent drive channels. What does that mean? We say that all the time. There's 14 hard drives in an Xserver RAID. I'll save you the counting. There's 14 independent drive channels. Why do we do it?

Well, first of all, we didn't want any inner drive dependencies. Why? Because any time you have a dependency between one drive and another, you limit the reliability of the system. So if you've used SCSI before, most RAID systems are SCSI or fiber channel, all the drives have dependencies. A SCSI bus is a dependent bus. They all share a common bus. If something happens on one side of the bus, guess what?

It can affect the other side of the bus. Well, since every drive's on its own bus, there's 14 independent ones. People who do very high-end servers and storage systems and people who work in the video world have learned a long time ago that you put independent buses in for performance. It's the reason they have different independent PCI buses on very high-performance computer systems.

And also, the bandwidth. There's 1400 megabytes per second internal bandwidth. How do we get there? That's a speed and feed number. Well, if there's 14 independent channels and each channel is 100 megabytes a second, this is first-grade math, 100 times 14 is 1400 megs a second. And that's a burst rate. Right?

An individual spin up and spin down control. Who cares? Well, hard drives take a lot of power. You can imagine that it's costing us a lot to run 17 terabytes here. But hard drives take a lot of power, and to make it easier to deploy those in a rack, you want to limit the amount of power surge you have, or what they call the break current. So you don't want to blow a breaker every time you turn a rack of Xserve RAIDs on.

[Transcript missing]

and a passive data path through the midplane. What the heck is that? First of all, what's a midplane? Well, in RAID systems, unlike computer systems, computer systems have backplanes. What's cool about the backplane? Everything plugs into it. Well, on the RAID system, in the middle of it, there's a backplane. And because it's in the middle, we call it a midplane. And what plugs into it? Well, everything. The hard drives plug into it, the power supplies plug into it, the RAID controllers plug into it, the cooling units plug into it. It's all in the middle.

But what's nice about it is there's an independent path from every hard drive to the RAID controller going through the midplane, but it's passive. It's like a connector. And on other RAID systems, there's actually logic on the midplane. Is that good? Well, it's good until the logic fails, and then you have a single point of failure. So we've tried to eliminate single points of failure for reliability. And how about a dedicated 64-bit, 66 MHz PCI bus to the drives? So I talked about 1,400 megabytes a second internal bandwidth.

Well, I lied when we get to the RAID controllers, because each RAID controller has 533 megabytes a second of internal bandwidth. So it's really about 1,000 megabytes of internal bandwidth within a system. And because we have an independent coprocessing system in each one of the RAID controllers, events that happen on the PCI bus from the drive don't affect any of the performance of the system.

Now the other area that we talk about are dual independent RAID controllers. So what are they? Well, essentially it means that each controller only has to manage seven drives. So we have two of them, we have 14 drives, each one manages seven drives, it's less work for the RAID controller. They're happy, they get to work nine to five. But what that really means is that it leads to higher throughput.

[Transcript missing]

And then the other thing that I think is really important is that we can use Apple RAID that's built in our operating system. This is the nice thing about being close to the operating system. We can use Apple RAID to build compound RAID sets. What are they?

So that means if we take a RAID 5 and a RAID 5 and put them together and stripe them, we'll build a RAID 50. And we can get the maximum performance out of that because in the operating system we can tune that. And we're able to, so it's really exciting.

[Transcript missing]

So how about deploying it? Well, in talking about the applications, the first thing you should see is how do you deploy it physically? So I mentioned they need to be rack-mounted, or they need to have an enclosure that holds them. What's really important in the IT world today is performance and reliability. That's optimum, right? That's what you have to have. So how do you achieve that? The first thing I can tell you is spend the money on a good rack. Why? Well, I'm going to tell you why.

First, you want to make sure that with 14 spinning hard drives that spin at 7200 RPM, that you don't achieve what we call rotational vibration. What does that mean? We have desktops, they have hard drives, no problem. Well, a hard drive... In fact, this was told to me a long time ago and it's still true. A hard drive spinning with the head floating above it is like flying a 747 six feet off the ground at 600 miles an hour.

[Transcript missing]

is the founder of Xserve RAID. He's the founder of the Xserve RAID platform. He's the founder of the Xserve RAID platform. He's the founder of the Xserve RAID platform. Alex Grossman is the founder of the Xserve RAID platform. He's the founder of the Xserve RAID platform. Alex Grossman is the founder of the Xserve RAID platform. Alex Grossman is the founder of the Xserve RAID platform.

Alex Grossman is the founder of the Xserve RAID

[Transcript missing]

So there's this thing what we call captive and non-captive storage. And in the Apple world, there was no captive storage. That meant that the server company actually sold the storage along with the servers. And before Xserve RAID and before Xserve, it was really hard to get a lot of storage in an Apple platform. So if you wanted it, you were stacking FireWire drives or buying from some third-party company and hoping for the best.

But the other storage companies and server companies, they made it their business to sell storage in the servers. Because it was easier to deploy. You didn't have to worry about setting it up. And also, my server vendor knows what's best for me. Well, I still believe that one because we're the server vendor. Now, throughput is driven by cost.

If it costs more, it has to be faster. Ferraris cost more than Volkswagens. They have to be faster, unless you throw them off a cliff. If it costs more, it has to be better. How can you build it for less? If it costs more, it must be better supported. Those are traditional views. Throw them out the window. Technology changes everything. And that's what's happened.

So this is the traditional view of storage. This is the way most people deploy it. You have internal server, usually using software RAID. It's a good way to start. The throughput and the availability are low, and the cost is low. Now, there is an exception to that, and that's Xserve. The throughput's phenomenal. The cost is still low, but the throughput's phenomenal.

Internal server-based hardware RAID. Now, I would argue with this a little bit and say that usually the throughput's lower. In the Xserve case, it's much lower than our software RAID. And then there's external SCSI hardware RAID storage, the staple of the industry. It's easy, it's expensive. And then there's external SAN-attached fiber channel, storage area networking, the idea that I can centralize all my storage into one area. These are traditional views. These are the way people look at it.

So what's changing it? Something called storage classing. What is storage classing? Well, servers are moving to a scale-out model. We have more servers. People talk about server consolidation. I want to get rid of all the servers I have and do one. That works until you're asked to deliver more services. And as soon as you deliver more services, you buy more servers. And you're trying to scale them out, because you're not buying these monster servers anymore and deploying everything on them.

[Transcript missing]

And the other thing is that the management of having all these servers and all these storage devices is out of control. Because especially if you bought servers from a lot of different people, you manage them all differently. And a lot of different operating systems. It's really kind of crazy. And then direct-attached storage was the only alternate. Now I'll bet you that a lot of people out here have installed network-attached storage in some case. At least some of it. And probably even storage-area networking. And those numbers are increasing every day. That's one of the changes.

Now, the other thing about storage classes is that people spend a lot of money on storage. They always have. It's been an interesting thing. I buy a server for X dollars, I spend three times as much on storage. That was always the rule. And they always had to buy the same storage. They always bought really expensive SCSI storage, if that's what they bought, because they had to use it everywhere. Because it was the same, and it was there, and they had to keep buying it. And every time they needed a terabyte, they paid X dollars for it.

Or a gigabyte in the old days, they paid X dollars. Storage classes are changing that. They're realizing that you can deploy different types of storage for different needs. Gee. And also, this incredible thing happened. Anybody who's older like me, they'll remember something called HSM, hierarchical storage management. They renamed it nearline storage. What's old is new again, and it's really getting a lot of play, and I'll talk about that.

So as you deploy, you look at these traditional views, and something's changed. And this is what's changed. And you're going to see this from other vendors happening, but Apple was the first one to put the shot across the bow with this one. External ATA-based hardware RAID storage that's fast and affordable. And guess what?

It has the high availability needs that you use in business-critical applications as well. And you get that for free. And so when you look at this picture, you say, how can something that costs as little as internal storage, based on what people will do at SCSI, deliver the performance equivalent to what you get with external fiber-channel-based systems that cost five times as much?

Well, it can happen, and I'll show you why in a couple minutes. So here's what it's really all about. IT has to look at doing more with less. I don't think there's anyone in the room here who's had a boss who's said, you know what, let's deliver less services this year. Let me give you tons of money. Go buy whatever you want. And only work five days a week and six hours a day. I see a bunch of nods. Everybody's doing that, right? Okay.

It's not happening like that. It's like this department needs this. This department needs that. I need to deploy it now. It needs to be faster. It needs to be better. I'm bringing 10 people online. I need you to work Sunday. to install that server and while you're at it, put the storage in the top of the rack as far as you can, 'cause there's no room to buy a new rack, that's happening. And then there's backup. So how do you backup 17 terabytes of storage? So what I have on the stage, how long does that take?

Can you do it in the eight-hour window that no people are working at your company? Say yes, right? So people are looking at different ways of doing it. They're saying, you know what? What we can do is as we bring more storage online, we're looking at new ways. So things like nearline, disk-to-disk backup. Have we all heard that term, disk-to-disk backup? Why is that? Because it's fast, right?

And then I can back it up, I have another copy, and at my leisure, I can have those slow tape drives take it offline. Because I can tell you right now, you can back up all 17 terabytes that are on this stage in one hour for about a million dollars.

[Transcript missing]

Okay. I know this will work. Okay. So I mentioned storage classes. So there's three classes of storage most people identify. The first class is high availability, highly redundant, and very expensive. Where do you want to use this? Well, some people used to call this mission critical. I tend to call it mission critical, but everything you do is mission critical. If you're doing code and you lose that code, that was mission critical to you, especially if you had a deadline next week.

If you're deploying a web server and you lose all that content, that was mission critical. But where do you really see this used? I use my favorite example is the ATM machine. When I put my card in, I want to make sure that it has my balance or lies and gives me more money, but it has my balance that's in there. So that's mission critical.

I want the people who run the banks to spend a lot of money on storage. But you know, the transactions that I did the day before, they're not as important to me because it's my balance I care about. So I want to make sure that that becomes class two storage. It's still protected, but it's business critical.

It's not mission critical. And that's like running a website. That's business critical stuff. That's your email. It's business critical stuff. It's highly redundant, and it's available, but you can accept some downtime as long as you can save the data and the data's still there. business on it. It's a good thing.

So this is the way storage classes get deployed. And so in the enterprise apps, I use that big E word up there, in database financial, there's what they call online storage in class one. There's also what they call near-line storage. What the heck is near-line? Near-line means it's not as fast, it's not as redundant, but it's there and it's safe. And if it does have a little bit of downtime, I can recover from it quickly and I know my data's still there.

And then when you look at business apps, for the most part, you start to see that they're using different classes of storage. You'd never deploy an enterprise app on Nearline alone. You'd always have some online. But you would find a business app running on Nearline. And you'll find e-commerce and web apps running on Nearline. So this whole classing of storage is starting to do something to the world. Why? Well, imagine if I have to buy Brand X storage that I've used for all my mission-critical applications and it costs, and this is not unrealistic, $250,000 a terabyte.

And since I've deployed five terabytes of that in my organization, I give it to everyone. Everyone's home directory runs on that storage that costs a lot of money. That's crazy. When I can buy 2.5 terabytes of Xserve RAID, and I can let all my people have 10 megabytes of iTunes Music Store, I can have all my people have enough room to do their work, I don't have to bug them, and I can put their home directories out there. And it's safe and it's secure, but guess what? It's not the transactions that run the company. So that's what the classing of storage is really doing.

So how do you plan for this? Because we're going to talk about the real fun stuff in a minute, but how do you plan for it? Well, the first thing is you have to plan. Take into consideration the capacity, the throughput, and availability, and also the cost and value. So when you're doing capacity planning, my rule is buy more than you need. Because if you think you bought enough today, start working on that budget for next year because you're going to buy more.

and make sure you have a solution that scales. How about throughput? You really have to look at this. What is the throughput you need? Again, if you get throughput for free, buy the highest throughput you can. But it's all about configuration, and it's all about performance. The availability. Mission critical, no downtime. Business critical, manageable downtime. Archive and near align.

So you have to take all those things in consideration and build a big chart and decide what you really want. Because there are times when you want to buy storage that's even beyond what Xserve RAID can deploy. So here's what people usually will determine, use to determine their storage. The first metric that you always get, this is the financial people always say this one. What's the cost per gigabyte?

So currently IDC and Gartner, they say, if you believe them, they say it's $30 a gigabyte for high availability storage. Xserve RAID is $4.36. So it's chalking it up for value. What are the hidden costs that you get with other companies? Well, the hidden costs are things you have to look at.

What's What's the additional software you have to buy, both to set up and manage the system? What are the costs of cables? What are the costs of power, because it draws a lot of power? Those are all the hidden costs that you have. and also what about expansion? When I expand, does it cost me as much to expand as it cost me to buy in? Or does it cost more? Does it cost less? And will the company be there in a couple years?

Those are all storage planning tools. How about deployments? So here's the scenarios we always see. An Apple server-based deployment. A single server, a single RAID. I think we can all do that one. A single server, multiple RAIDs. I'd like everyone to do that one. Multiple server, single RAID, or multiple servers, multiple RAID. So let's look at the cases. How about a mixed OS platform? Is it possible to do that with Xserve RAID? You could have multiple servers, multiple RAIDs, single server, single RAIDs, all that's possible. How about a virtualized mixed platform? What is that?

Why would you want to virtualize? We're going to talk about that. So let's build some scenarios. You ready? I built a couple already here. So here's three typical scenarios that I built. The first one is I took Mac OS X server on an Xserve and an Xserve RAID. It was a pretty easy and simple thing to do.

The next scenario I did is I took that same Xserve RAID and I attached two Xserves to it. I took another scenario in the middle, kind of a funny one. I have this server from a company called IBM and one from a company called Dell with an Xserve, and I have two RAIDs. And I'm sharing the storage across all those platforms.

And in scenario four, it's kind of interesting. I have what I call virtualized storage. So I have a third-party product called a Chaparral RAID RPS, or a provisioning server. There's the word you want to remember, provisioning. I have a couple Xserves and a couple Xserve RAIDs, and I basically added additional capabilities to Xserve RAID and kept that low-cost storage deployment, but added true enterprise-class performance and reliability and scalability to the Xserve RAID without having to buy some very expensive storage that you could use to do this. So let me show you how they work. First thing is, let's look at an Xserve RAID. If you've seen one other than here, it's pretty standard.

Cleaned in the front. I mentioned that there's 14 hard drives. People always say we went a little overboard with lights. I say we need more. Drive activity and health activity on each drive. Each hard drive, what is health? We actually measure the drives for are they good, are they in a pre-fail condition?

That means are they going to fail or have they failed? So the light goes green, amber, and red. Red is dead. Green is good. Amber means that we've used a technology called SMART to actually look at the performance of the drive. We look at the drives and say what is the health of that drive?

If it's going to fail, we're going to pre-fail it and we're going to warn you. We're going to send you an email and say, hey, this drive is going to fail. We're going to, if you had a hot spare in the system, we're going to rebuild to the hot spare so you have full availability. It's an indicator on the front. It gives you a good idea. There's fiber channel link indicators. Just like on Xserve, we want to make sure that the links that you have to your server are easily available.

Why do we put those on the front? There's lights on the back, right? Well, this comes from the classic example of ease of use. Anybody ever have a rack? Anybody ever install a rack? Okay, a few people here, a lot of people. When you have a rack, you really want to make it really clean in the back and run all the cables down the side, and it's going to be beautiful. It doesn't happen, guys. It looks like a spaghetti jungle in there.

And so there's a chance, there's always that possibility that you could come along and unplug a cable by mistake. And if you don't have an indicator on the front to show you, you may not notice that until you walk to your desk and get emailed or paged. So it can happen, it can happen really easy. So we put a lot of indicators on the front, including link indicators for fiber channel. Because I will tell you, when you disconnect the fiber channel cable, you don't get called by our admin utility first. You get called by all those people who just lost their connection.

And so we put a lot of indicators on the front. And then we put a lot of indicators on the back. And then we put a lot of indicators on the front. And then we put a lot of indicators on the back. And then we put a lot of indicators on the front. And usually those pages are the bad ones.

So we put a lot on the front. You'll notice just like XServe, we have a system identifier button and enclosure lock, and we added something else called an alarm silencer. Because I can set up my XServe RAID that if I have a failure, and it's in a desk-side configuration, or under the desk or on the desk or in the CEO's office, it's going to be a failure.

I can actually have it beep at me beyond the visual indicators. And you notice this one didn't beep when I pulled that power cord out because I shut it off. That's the fun thing you can do. So let's look at the back of it because this is really how you deploy it, right? Pretty clean, Apple design.

Two big fans and power supplies. They're redundant. Two redundant cooling modules in the middle. Two independent RAID controllers. Two backup battery modules. A power indicator. An alarm mute button on the back as well. And a system identifier. And so you look at it and you say, pretty easy. There's nothing you can't get to. If I go back and look at the front, I can get to all the hard drives.

If I look at the back, I can get to all the components. So the design of Xserve RAID is a high availability design, meaning that the only thing that I can't change in the field is that midplane. But remember, that was a passive data flow. So the only way to break that is to physically bend the connectors. Bad thing, guys.

Or you're going to hurt the metal. Maybe look at the bezel on this. It's a little hard to hurt, right? It's aluminum. So you're really not going to be able to hurt anything, and you're going to be able to deploy this thing anywhere you need it in an easy manner. So most of the connections you're going to look at are going to be the fiber channel connection, the Ethernet connection, and the serial connection.

Why? Well, that's what we're going to talk about. So here's scenario one. I call this one standard operating procedure. You take an Xserve RAID, you take an Xserve. You put our fiber channel card in the Xserve. It has two ports on it. You take the two cables that come with the card, you plug them in the Xserve RAID, and you're done.

It's easy. Two and a half terabytes, ready to go. The Xserve RAID comes out of the box. It's configured for two RAID 5 sets. So you have about 2.1 terabytes available to you, protected RAID. You go to Disk Utility, just like you would any other hard drives. Either build one RAID 50 set by striping the two together or two RAID 5 sets that are on the desktop. You serve them to your clients, you're done. Pretty easy. You tell your boss it took you two days.

Software that comes with it, Apple RAID Admin. What is RAID Admin? It's a utility that Apple's designed to make it easy to deploy these systems. So you can install them using this utility, you can manage them using this utility. What's different about it? What's different about it, it's web-based, it's Java-based, so read that, Java-based, platform-independent-based. You can use it anywhere in the world.

And you know what's cool about it? You can deploy a RAID system literally in a minute. And I'm going to show you that. And then the other thing about this installation is that you just have to have a strong back, because you've got to lift the RAID. Okay, how about this one? Scenario two. Two Xserves and an Xserve RAID.

What did we do here? Why did we do this? Well, what we did is we centralized the storage. So that's what I did in this case. I took my Xserve RAID. I didn't use a switch in this case. I just went directly from the Xserve RAID to the two Xserves.

I took centralized, protected storage, high availability storage, and delivered over a terabyte to each Xserve. And what's really cool about this is that that storage is more redundant than the server in this case. And if I ever had a problem, I can move that storage over to another server. So I have a highly redundant configuration. And it's inexpensive.

How about scenario three? That's the one I built over here. Actually, some nice people here built it for me. We took an Xserve, we took an IBM 330 server, a Dell 1550 server, an Xserve RAID, we took Apple RAID admin, and I configured it with four RAID sets. Three RAID 5s and a RAID 1. And each server has a RAID 5 set, and the Xserve has an additional RAID 1 boot. So, contrary to popular belief, you can actually boot your Xserve off an Xserve RAID. It just works.

And I'll show you exactly how we did that. So what I did is I built a RAID set. This took me about 10 seconds. And I took three hard drives, and I built a 360 gig RAID set, fully protected, and I gave it to my Xserve. Then I built another RAID set and I gave it to the Dell dude. Sorry.

And then because someone said I had to keep that Dell and that IBM 330 and my budget for buying Xserves to replace those pigs took a little while, I also gave a 360 gig or 540 gig RAID set to my IBM 330. And then that last two hard drives that I had, I made a boot that was mirrored for my Xserve. Now I had two drives left over, and I made those hot spares so that all my RAID sets are globally protected against the failure of any drive.

[Transcript missing]

Well, hundreds of thousands of dollars. In the same capacity with some storage systems, you'd have to spend hundreds of thousands of dollars to get this capability. So we took two Xserves. We took two Xserve RAIDs. We took a VXL fiber channel switch, a 16-port VXL fiber channel switch. Unfortunately, I didn't have enough room to put it in the front. It's in the back.

I took a third-party product, a provisioning server from a good company called Chaparral, called the RAID RPS. I took Apple RAID admin and Chaparral's loan provisioning software, and I built a completely virtualized storage system. We call this a SAN. You might have heard the technology. And so what did it do? Well, the RAID RPS sees 4.32 terabytes.

And the RAID RPS allows me to build 1,024 LUNs. What's a LUN? A LUN is basically, think of it as a partition. So that if I had a lot of users, maybe not 1,024, but if I had 100 users, and they were all graphic artists, I could break that storage up to them equally. And I probably would leave a pool of that storage unused.

[Transcript missing]

So let's take that. Let's take that scenario. Let's decide that one server handles my graphics department and one server handles my video department. My graphic guys are slacking off. They're using a terabyte of storage. My video guys, on the other hand, they've been downloading QuickTime flicks for a long time here, and they really are maxed out. I'm running at 2.16 terabytes. I'm telling them every week, throw your stuff away. What can I do with RAID RPS and with Xserve RAID?

I can go ahead and re-partition or re-engineer that software so that I can re-provision it so that now my video people can have, let's say, three terabytes and my graphic artists only get one terabyte. And I can do that dynamically, essentially on the fly. With a restart of the Xserve, they can have a complete change in what they need.

And so that's really high-level capabilities that are very difficult to get with other systems. Now, the other thing, that's what we call dynamic growth of LUNs. The other thing we can do here is we can do things like snapshots. So I mentioned backup, right? If that graphics department is moving a terabyte a day, I probably don't have a tape drive to move a terabyte a day.

But I can snapshot that or make a point-in-time copy of that data so that someone can get it off there at their leisure. And that's all available with Xserve RAID using low-cost storage with a provisioning server. And again, the provisioning server we have here is a low-cost storage.

And again, the provisioning Here is the provisioning server from Chaparral. Now, managing LUNs and performance tuning with RAID Admin. So I'm going to give you a quick look at a sneak preview of a new version of RAID Admin that's coming out. Let me show you how this works. So can I have a demo two up, please?

Okay. So this is RAID admin. Our little piece of software to manage the RAID systems. So the first thing you're going to notice is that all my RAID systems here happen to be on my network and they're on a subnet. Now, to make it easy, because I didn't want to confuse myself, I took some of them off the subnet because I didn't want to see them. But you'll notice that RAID admin uses rendezvous. And so I can just grab one of those systems and I happen to know the password, so I'm not going to tell everybody it's public here.

Anybody who's on my subnet here. And I know the password and I'm going to log into it and it's going to go out there and it's going to hit one of my RAID systems and give me all the information about it. Now, remember, pretty cool Cocoa app, right?

[Transcript missing]

How about Fiber Channel? I have a connection to Fiber Channel. I can look. I have two links. I can see this, this little worldwide name down here. It's only 48 digits. It's really simple and easy to remember. and my network. So why do I need network?

Because we do what's called out-of-band management. What's connecting my RAID admin system to my RAID systems here is Ethernet. I'm not using the bandwidth on Fibre Channel. I'm doing this in what's called out-of-band, so it's really a simple connection. But you'll notice I have a link that's down, because I wanted to get an amber light in here to show you what it looks like.

I have a link that's down. It's a warning. It's warning me, telling me, Alex, you only connect to one of those Ethernet lines. So if your switch or router on your Ethernet goes down, you're not going to be able to see your RAID system when you're on vacation in Fuji next week. So you want to have them both on there. You'll notice I also have some speed and configuration on here, and then something brand new in RAID admin 1.1 is an event log.

So I can actually save the events and know what I did, because believe it or not, we get busy. It's one of those funny things that happens. So this is basically a system. Let's do something. Let's do something funny here. Let's create a RAID system. Is that cool? So everybody's used to Keychain, right? So I have that in my Java app here. So let's see if I remember my password.

[Transcript missing]

I'm an IT person, but I manage a lot of different things. I don't just manage my RAID systems every day. If I do, I've got a pretty cushy job if they're Xserve RAIDs. But I don't remember what the difference between RAID 3 and RAID 5 is.

So I just look up here and it tells me, and I can build a RAID set knowing I need three or more drives. How do I do it? This is really hard. So I pick a drive and I select them. Oh, my finger's getting tired. Oh, this is a lot of work. I select my drives, and then we have this little thing that's called RAID Now.

What does that mean? It's background initialization. So I can click that, and that allows me to actually use the RAID set instantly. So what I'm going to do is build a RAID set, and it's going to take a long time. It's actually going to take 24 hours to build a RAID set.

Because what I'm going to do is I'm going to check to a surface analysis on every single drive in the system to make sure there's no failures of those drives. Because you don't want to build a RAID set on bad drives. And you'd be surprised if you ever see other RAID systems and they build a RAID set in like five minutes or ten seconds. You go, oh, they did a lot of checking on those drives, didn't they? They made sure those were perfect.

They just looked at the signature and wrote it on there. We're going to do a complete surface analysis of the entire drive, back and forth, reads and writes. And we're going to spare out any bad areas. We're going to take care of it all. But in the meantime, you'll still be able to use it. And then I want high performance.

You know what it says here. It says that if I turn on drive cache, I should have batteries or UPS. That's a warning, guys, because you can lose data if you don't. And then I say create. I'm done. I just built a RAID 5 RAID set on the fly in a matter of seconds.

That's it. It's easy. So what else is really cool about RAID Admin? How about settings? So the one thing I can do in this system, in fact, what I'm going to do here is I'm going to grab another one that has some different stuff on it. So I can see some different settings.

This is actually the system I'm using right now in this configuration with the provisioning server. This one has some cool settings on it. So I'm going to go in and take a look at the settings. and what you're going to notice first is the system information. So this is the RAID RPS rack. That's this rack over here. And this is RAID 3.

I can synchronize my time. I can change my management passwords. Remember I mentioned about the audible system alerts? I can turn them on or turn them off. I can restart automatically when there's a power failure. Anybody like to get called in the middle of the night when there's a power failure because the RAID went down and you've got to go push the button back on? Forget that. No more doing that. I mentioned the network settings. I can change from manual to DHCP. Fiber channel. If you have an existing fiber channel network, this is the most important thing you're ever going to play with.

[Transcript missing]

Part of that is marketing, but... For the most part, it's because most RAID systems, you have to be a genius to tune them. So I mentioned there were automatic and there were manual settings for our RAID systems. So first thing we do is we do all the automatic stuff for you. We tune it based on the way you tell us. You tell us you have 512 megs of cache in there. You tell us that you have battery backup. You tell us you're going to do RAID 5, and we tune it.

But we also give you other settings. I mentioned Drive Cache. I can turn the Drive Cache on in the drives. So if I didn't have a UPS and I'm willing to take a performance hit, I don't want to turn Drive Cache on. "I'm scared that if I have a power failure, I could lose some data." Now we do have in Mac OS X, we do of course have journaling. Journaling protects you against these things. But there's still the possibility that you could lose a lot of data because there's 8 megabytes of cache per hard drive, times that by 14, and that's bigger than your spreadsheet.

or my spreadsheet. So we may want to turn it on or turn it off. I always suggest hooking to UPS and running DriveCache on, because the performance can be 100% better. How about WriteCache? What is WriteCache? This is kind of a misnomer, enable and disable. Because what we actually do is we have two levels of WriteCache.

One is write through. That means we don't really cache anything at the RAID controller. Everything that comes through, we just write it through it, and so the data flows quickly to the drives. So the computer, in this case, let's call it an Xserve, as soon as it sees that the data is at the drives, it's free to send more data.

So it gets this write complete. If we do something called write back cache, which is what we're enabling here, we actually tell the system, the Xserve in this case, that we've written the data to the drives when we really haven't. We've written it into a cache buffer. And that's why we have those battery backups, to back up that cache in case for some reason we lost power at that case.

But that could also increase your performance, because imagine if you want to have a lot of people hitting the RAID system at once, what do you expect it to do? You expect the Xserve to send a lot of commands to the Xserve RAID at once. And the only way you can accept those commands is to start caching them up.

And also you have to remember that the data that's coming off the hard drives can come either in order or out of order to the RAID controller. And so what we want to do is we want to cache all that data up so we can get as much data delivered to the Xserve at once. We want to open the floodgates, so this is important. Now I mentioned that we had dynamic prefetch in the system. So why do I have a prefetch setting for RAID? And what is prefetch?

Well, essentially what prefetch is, is that if I, let's take the alphabet, A through Z. If I tell you that I want to look at A and B, you're pretty much going to guess that the next thing I want to look at is C and D. So in a streaming application, or if you're like me, I'm probably going to look at Z next, but if you're in a streaming application, it's a pretty good guess that I'm going to prefetch a lot of data ahead because I'm going to anticipate what you're going to get next.

So I said that we're dynamic. So what we do is we look at the data that the driver is giving us from our Xserve, from the OS, and we say, what kind of data is it? If it's streaming data, what I do is I open my prefetch window and I start grabbing a lot of data because I know if I'm streaming a QuickTime movie, the rest of the QuickTime movie is what you're likely going to want to see. It's the data you want.

But, you notice here I have three settings for that. Why? If I just can dynamically change this. It's the size of the dynamic changes we can make. So if I set it to 128, you give me, the RAID controller, a lot of range. I can go anywhere I want. If I set it to 1, I can only do a little prefetch. Why? Why would I need to change it?

Why would you be smarter than me? Well, the reason is, you may not want to watch that whole QuickTime movie. You might be scrubbing a timeline, you might have a hundred users and everybody wants to share a little of that data. So you know your data patterns better than I do.

So what's cool about this? It's dynamic. You run a test based on your data patterns, you're using your users, you hammer it, you hit it a lot, you don't see the performance you want. What do you do? It's really hard here. You go over and you push the button. You go over and you push the button. You run the test again. So I'll give you some general rules for this. Anybody here have I/Os per second?

Small files are I/Os per second. Large files are usually megabytes. They usually work against each other. If you have large files, video content, put the prefetch as high as you can. If you have tons of users, lots of users, they do small files, turn the prefetch down. You'll help the system. And in all cases, keep write cache enabled. great admin. Now let me show you something else. Can I go to demo three, please?

So this is, I don't know what kind of system this is. This is, I can't remember one of these things. It's called a PC. So this is actually RAID admin running on Windows. Now, this isn't something we support, but I mentioned RAID admin is Java. Oh, I'll even show you something that I didn't show you on the Mac, which I could have shown you, is contextual menus. RAID admin actually has contextual menus as a Java app. So it's pretty cool. All the capabilities of RAID admin on the Mac are available on other platforms.

Such as Windows, such as Sun, anything that runs Java 1.31 or better. So you can actually manage your system, although not necessarily a recommendation, you can actually manage your system from a Windows system. So when you're stuck in Fiji on vacation, and you forgot to bring your PowerBook with you, you can still manage through RAID admin. Okay, great. Can we go back to slides, please?

Cool. So, here's a typical LUN management configuration. One of the things I like to talk about is how we actually can do basic SAN functionality in Xserve RAID with RAID admin without really having to spend any money on SAN software pieces and parts and still do what's true to a SAN, which is consolidating your storage and deploying it to many servers. So, I showed you how we had four servers basically deployed here, or three servers deploying four RAID sets. So, let me show you how it actually would look. We'd break it up into four Xserves in this case.

I'd build four RAID sets using that one masking I saw. And now, I get the advantage of global hot-sparing using that tool by just putting the worldwide port name of each one of those Xserves and addressing those to each one of the Xserves. So, I can deploy that to any one of the RAID sets. Simple. So, we can deploy that. You can do it in seconds. In this case, I used a fiber channel switch to allow me to go from two to four.

Now, the beauty of this is that not every computer can see every RAID set. Only the computer, or only the Xserve in this case, that is attached to its RAID set can see its RAID set. The other data is protected away. And easily, I can change who sees what data. So it's a really simple deployment. So let me wrap it up. There's five hints I'll give you for deployment. First thing to remember is it's easy as one, two, three, almost. The first one is read the manual.

What? There's two manuals for Xserve RAID. One's a hardware manual, it's 91 pages, and it's excellent. The second one is the software manual for RAID admin. Do you really need to read it? I would read it. Plan ahead. This is the biggest problem that we all have with deploying RAID, is we either buy too much or buy too little, but we never deploy it right. So that goes along with my next one. Buy too much, because you're going to ask for more later. Okay, that's a little marketing pitch, okay. Then, number four, read the manual again.

5. Deploy it. It's really simple. If you want to get more information, you can contact myself or Skip Levins. There's reference libraries for Xserve RAID. A lot of information online for that. I'd like to open it up to any questions that anybody has, if we can do that.

Oh, I did want to talk about one more thing, too, two more things, actually. In the XServe, in the XServe deploying XServe session yesterday, we were talking about ways of making it easy to deploy multiple servers, and Doug Brooks mentioned something called the introvert, which is a little device here.

I call it the little buddy for the XServe RAID, and it allows us to take a cartridge or what we call a drive carrier from an XServe or an XServe RAID and bring it up onto a G4, G5, or even another XServe if we want to, really easily because it makes it hot-pluggable using FireWire on the system. There's two really cool things you can do. One is I mentioned mirroring on the XServe RAID.

So if I mirror, let's say I have seven drives on one side of my XServe RAID, and I do a mirror of one of those drives, and I don't tell it that I want to just mirror one drive on the other. to the next drive, it will automatically mirror all seven drives.

Why would anybody ever use that? Well, how about if I have to make an image and deploy to multiple Xserves? I have seven boot drives. And if I want to check them, I take my extrovert, Am I introvert, actually? And I mounted up on my G5, G4, and I can actually see what I got. So I can have seven copies easily. So that's my little tips and tricks thing for Xserve RAID.
