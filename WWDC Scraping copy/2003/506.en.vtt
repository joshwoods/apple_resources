WEBVTT

00:00:17.830 --> 00:00:18.380
Good afternoon.

00:00:18.390 --> 00:00:20.500
My name is Mark Tozer-Vilches.

00:00:20.500 --> 00:00:23.330
I'm the Desktop Hardware
Evangelist in Apple Computer and

00:00:23.330 --> 00:00:24.670
Worldwide Developer Relations.

00:00:24.750 --> 00:00:29.700
Welcome to session 506,
performance optimization tools in depth.

00:00:29.700 --> 00:00:33.660
Now, optimization means a lot of things
to a lot of different people.

00:00:33.730 --> 00:00:37.700
It can be from trying to get your
application to launch faster,

00:00:37.700 --> 00:00:41.700
access the network faster,
get higher frame rates.

00:00:41.710 --> 00:00:44.700
Bottom line is, it's about speed,
it's about performance.

00:00:44.700 --> 00:00:47.270
It's about getting your
application to run faster than

00:00:47.270 --> 00:00:50.700
what it currently does today,
or faster than what maybe your

00:00:50.700 --> 00:00:52.700
competitive application does.

00:00:52.990 --> 00:00:55.700
Bottom line is,
there's a common denominator.

00:00:55.720 --> 00:00:58.990
You're looking to increase performance,
and you need to know where that

00:00:59.090 --> 00:01:00.700
performance can be increased.

00:01:00.700 --> 00:01:04.130
In order to do that,
you need tools to be able to allow you to

00:01:04.130 --> 00:01:06.700
understand where that work can be done.

00:01:06.700 --> 00:01:09.980
Apple has created a set of
tools for developers shipped

00:01:10.170 --> 00:01:13.700
freely on the developer tools
that you'll hear about today.

00:01:13.700 --> 00:01:17.290
I'd like to introduce up here
a member of that team that's

00:01:17.290 --> 00:01:20.320
involved in creating these tools,
Mr.

00:01:20.360 --> 00:01:23.700
Sanjay Patel of the
Architecture Performance Group.

00:01:30.060 --> 00:01:31.210
Thanks, Mark.

00:01:31.310 --> 00:01:32.400
So my name is Sanjay Patel.

00:01:32.400 --> 00:01:34.460
I'm in the Architecture and
Performance group.

00:01:34.460 --> 00:01:37.030
We're going to start off today
by talking a little bit about G5

00:01:37.030 --> 00:01:41.110
from a programmer's perspective,
some issues you may run into

00:01:41.110 --> 00:01:45.290
as you're moving code from G3
and G4 over to the new systems.

00:01:46.260 --> 00:01:49.960
So to start off with, first of all,
the 970, the PowerPC 970,

00:01:49.960 --> 00:01:54.170
it's the official chip name,
is a very super scaler,

00:01:54.470 --> 00:01:56.030
very wide and very deep machine.

00:01:56.030 --> 00:01:58.900
It's based on IBM's Power4 architecture.

00:01:58.990 --> 00:02:04.530
It's a true 64-bit chip of
the PowerPC AS architecture.

00:02:05.910 --> 00:02:10.580
It has the full Altevec instruction set,
the full 162 instructions,

00:02:10.580 --> 00:02:14.500
all implemented in hardware.

00:02:15.720 --> 00:02:18.060
Also has a high bandwidth
point-to-point interface,

00:02:18.080 --> 00:02:19.820
so this is a little different than a bus.

00:02:19.820 --> 00:02:24.010
What we actually have is direct
connections between the processor

00:02:24.010 --> 00:02:25.710
and the memory controller.

00:02:26.760 --> 00:02:29.240
And we also supply automated
hardware prefetch engines,

00:02:29.240 --> 00:02:33.860
so what these guys do is they'll
start detecting patterns of memory

00:02:33.860 --> 00:02:38.660
accesses and prefetching those memory
accesses into local caches for you.

00:02:40.110 --> 00:02:41.560
So here's a picture of the die.

00:02:41.560 --> 00:02:46.670
You might have seen this in the keynote
or at the 970 presentation yesterday.

00:02:46.770 --> 00:02:49.550
But what we have here
is two load store units,

00:02:49.550 --> 00:02:56.090
independent, two fixed point units,
two IEEE compliant independent

00:02:56.090 --> 00:02:57.720
floating point units,

00:02:58.520 --> 00:03:00.840
The full set of the
four Altevec subunits,

00:03:00.920 --> 00:03:03.770
the ALU as well as the Permute.

00:03:04.450 --> 00:03:10.990
A branch unit and a unit to handle
condition register logical operations.

00:03:12.190 --> 00:03:14.440
So here's another view
from the top there.

00:03:14.440 --> 00:03:17.140
You see instructions
would be in the L1 cache.

00:03:17.160 --> 00:03:19.770
They'll go into fetch,
sit around in some queues.

00:03:19.870 --> 00:03:22.380
They'll go to dispatch,
where they can be dispatched

00:03:22.380 --> 00:03:25.710
up to four instructions plus
one branch on every clock.

00:03:25.790 --> 00:03:27.840
So this is really a wide machine.

00:03:27.870 --> 00:03:32.550
There you can see you get fed into 10
issue queues to the 12 execution units.

00:03:32.610 --> 00:03:37.820
So again, this is just the widest machine
you've probably dealt with.

00:03:39.130 --> 00:03:40.950
How does this all compare against G4?

00:03:40.990 --> 00:03:45.140
Well, to keep everything--all
these units flowing,

00:03:45.350 --> 00:03:49.160
the core can actually have over
200 instructions in flight,

00:03:49.160 --> 00:03:53.610
versus a little over 30 in the G4
if you count the completion buffers

00:03:53.610 --> 00:03:55.000
as well as the various queues.

00:03:55.000 --> 00:03:59.630
The pipeline stages have been expanded,
so we're at 16 stages for a simple

00:03:59.630 --> 00:04:01.990
instruction versus 7 in the G4.

00:04:01.990 --> 00:04:06.000
As I mentioned,
we have two load store units versus one,

00:04:06.000 --> 00:04:09.000
as well as two floating point
units versus one in the G4.

00:04:09.000 --> 00:04:11.940
There are two general
purpose fixed point units,

00:04:12.040 --> 00:04:14.790
where in the G4 there were
three dedicated simple

00:04:15.090 --> 00:04:17.000
units and one complex unit.

00:04:17.000 --> 00:04:18.000
Vector is similar.

00:04:18.000 --> 00:04:20.750
There's the ALU,
which includes floating point,

00:04:20.900 --> 00:04:25.000
complex integer, simple integer,
and the permute unit.

00:04:27.190 --> 00:04:29.880
If we talk about the caches,
there's quite a few differences here.

00:04:30.060 --> 00:04:34.100
First and foremost for programmers
is the cache line size has changed.

00:04:34.200 --> 00:04:37.380
It's 128 bytes where
it used to be 32 bytes.

00:04:37.500 --> 00:04:41.200
The L1 data cache is the same size,
but it's a two-way associative

00:04:41.480 --> 00:04:45.780
design and write-through versus
eight-way and write-back on the G4.

00:04:45.860 --> 00:04:49.370
The instruction cache has been doubled,
so we're at 64K now,

00:04:49.480 --> 00:04:53.720
although it's a direct map design
versus eight-way associative on the G4.

00:04:55.770 --> 00:04:59.300
L2 cache is also doubled,
so now we're at a full half meg.

00:04:59.370 --> 00:05:02.640
Also an eight-way associative
in both G4 and G5.

00:05:02.970 --> 00:05:07.510
The replacement algorithm is
LRU versus random on the G4.

00:05:07.510 --> 00:05:12.130
There is no L3 cache on the G5,
whereas on the G4 you

00:05:12.130 --> 00:05:14.140
had up to two megabytes.

00:05:14.500 --> 00:05:18.120
Now that's partially made up for by
the fact that processor bandwidth is

00:05:18.120 --> 00:05:20.540
just tremendously higher on the G5.

00:05:20.620 --> 00:05:24.660
It's up to 3.5 gigabytes a second
effective in each direction,

00:05:24.800 --> 00:05:29.880
both to and from memory simultaneously,
versus a 1.3 gigabyte per

00:05:30.430 --> 00:05:32.370
second bus for the G4.

00:05:32.980 --> 00:05:34.550
On the other side of
the memory controller,

00:05:34.550 --> 00:05:37.760
we've doubled the width
of the DDR interface,

00:05:37.760 --> 00:05:40.080
as well as increased the clock frequency.

00:05:40.170 --> 00:05:44.270
So more than twice the bandwidth is here,
available from the DDR chips.

00:05:44.360 --> 00:05:48.510
6.4 gigs versus 2.7 gigs on the G4.

00:05:50.510 --> 00:05:53.600
So, what does this all mean from
a programmer's perspective?

00:05:53.640 --> 00:05:56.120
Well, there are going to be some things
you're going to have to look out

00:05:56.120 --> 00:05:58.400
for as you're porting your code
and optimizing it on this chip.

00:05:58.400 --> 00:06:02.400
And so the first thing you'll notice is
that there are more pipeline stages here,

00:06:02.400 --> 00:06:06.400
which means instruction
latencies have grown from G4.

00:06:06.400 --> 00:06:10.750
So,
how do you work around that in your code?

00:06:12.010 --> 00:06:14.500
Well, you should do more in parallel,
right?

00:06:14.610 --> 00:06:18.700
So manually unroll important
loops or try to use compiler flags

00:06:18.700 --> 00:06:21.500
such as unroll reloops with GCC.

00:06:21.630 --> 00:06:27.700
You can also schedule your code using
M2 and equal 970 with the new GCC 3.3.

00:06:31.730 --> 00:06:34.100
Now similarly,
because the pipeline is longer,

00:06:34.100 --> 00:06:36.910
branch mispredictions
are going to cost more.

00:06:37.840 --> 00:06:40.730
It's just going to take longer
to recover from a mispredict.

00:06:40.800 --> 00:06:42.590
So there are several
solutions you can use here.

00:06:42.690 --> 00:06:46.560
If you're coding in C,
GCC offers a built-in expect.

00:06:46.670 --> 00:06:49.550
So that's for a very
highly predictable branch,

00:06:49.550 --> 00:06:51.370
such as maybe exception code.

00:06:51.620 --> 00:06:54.140
You expect it not to be taken very much.

00:06:54.180 --> 00:06:57.400
You can use this macro built-in expect.

00:06:57.440 --> 00:07:00.950
If you're coding in assembly,
we have these new plus plus and minus

00:07:00.950 --> 00:07:03.490
minus suffixes for all branches.

00:07:03.560 --> 00:07:06.720
So either highly taken
or highly not taken.

00:07:07.100 --> 00:07:10.670
Best solution is to just not do branches,
right?

00:07:10.840 --> 00:07:14.230
So in floating points, you have fcell.

00:07:14.310 --> 00:07:17.010
That's enabled with FastMath,
and what that allows you to

00:07:17.010 --> 00:07:20.140
do is a conditional move in
floating point registers.

00:07:20.140 --> 00:07:23.160
In the vector domain, you have vcell,
very similar operation,

00:07:23.160 --> 00:07:24.320
you use it with masks.

00:07:24.320 --> 00:07:27.800
In the integer domain,
you have the carry bit.

00:07:27.860 --> 00:07:30.330
So this can be used for min
and max type of operations.

00:07:30.430 --> 00:07:36.160
You can also use masks to avoid
branches when you're doing integer,

00:07:36.160 --> 00:07:40.150
what effectively would
be conditional moves.

00:07:40.430 --> 00:07:45.220
And then feedback directed optimization
is something most programmers don't try,

00:07:45.240 --> 00:07:47.680
but this can be very effective on G5.

00:07:47.790 --> 00:07:50.920
Because if you can have a
representative run of your program,

00:07:50.920 --> 00:07:56.240
let the compiler annotate that run,
and then mark all branches with

00:07:56.460 --> 00:07:59.740
highly not taken or highly taken,
this can be very effective for improving

00:08:00.000 --> 00:08:02.760
performance on this long pipeline.

00:08:05.460 --> 00:08:08.840
So as I said, the data cache is quite
different than it was on G4.

00:08:08.840 --> 00:08:13.780
And the most important thing
here is that it's 128 by line.

00:08:14.060 --> 00:08:15.800
What can you do to work around that?

00:08:16.020 --> 00:08:18.400
Well,
that's either a win or a loss for you,

00:08:18.400 --> 00:08:20.030
depending on your code, right?

00:08:20.250 --> 00:08:24.230
If you have a lot of locality, well,
you're probably going to incur one miss,

00:08:24.230 --> 00:08:27.640
where you would have had
four misses on the G4 system.

00:08:27.720 --> 00:08:30.680
So,
you must design your algorithms and your

00:08:30.680 --> 00:08:34.590
data structures to move through data--to
move through memory sequentially,

00:08:34.660 --> 00:08:36.550
continuously, if possible.

00:08:36.600 --> 00:08:37.060
Okay?

00:08:37.130 --> 00:08:39.390
That's also going to trigger
the hardware prefetcher,

00:08:39.490 --> 00:08:42.530
and this is very powerful,
because it will amortize all

00:08:42.530 --> 00:08:44.590
of the latency up to memory.

00:08:46.170 --> 00:08:48.000
So that's the next topic.

00:08:48.000 --> 00:08:51.780
Because it is a point-to-point
interface to the memory controller,

00:08:51.910 --> 00:08:56.190
latency--effective latency may be higher
than what you've seen on a G4 system.

00:08:56.190 --> 00:08:59.960
And that's because to maintain coherency,
you have to go to the memory

00:08:59.960 --> 00:09:03.000
controller and then bounce
back to another processor.

00:09:03.000 --> 00:09:06.000
What can you do to avoid
any of those penalties?

00:09:06.080 --> 00:09:09.000
Well, software prefetching--so there
are several instructions.

00:09:09.000 --> 00:09:11.440
And of course,
the hardware prefetcher is the best

00:09:11.460 --> 00:09:15.990
solution because it's self-paced,
and it'll be synchronous with your code.

00:09:16.090 --> 00:09:18.800
As you miss,
the hardware detects those misses,

00:09:19.010 --> 00:09:21.800
detects the pattern,
and prefetches lines for you.

00:09:21.800 --> 00:09:24.000
You can also batch your loads.

00:09:24.000 --> 00:09:27.000
So say you need to access
several pieces of data,

00:09:27.000 --> 00:09:27.960
you know you're going
to need them in advance.

00:09:28.010 --> 00:09:31.270
Try to group those loads together,
because the bus can

00:09:31.270 --> 00:09:33.000
support several misses.

00:09:33.000 --> 00:09:33.840
simultaneously.

00:09:38.930 --> 00:09:43.900
So the Data Stream Touch instruction
from the Altevec instruction set

00:09:43.950 --> 00:09:48.030
is execution serializing on the
G5 because it's mapped onto the

00:09:48.030 --> 00:09:49.800
existing hardware prefetch mechanism.

00:09:49.800 --> 00:09:53.730
So what can you do to avoid DST?

00:09:53.730 --> 00:09:56.800
Well, first of all,
you can probably remove it.

00:09:56.800 --> 00:09:59.130
It is just a hint,
so there's no guarantee that

00:09:59.130 --> 00:10:01.800
DST is going to be effective
for you in the first place.

00:10:01.800 --> 00:10:04.960
The preferred solution is to
rely on the hardware prefetcher,

00:10:05.040 --> 00:10:07.790
so assuming you have
contiguous memory accesses,

00:10:07.810 --> 00:10:09.800
that's going to
automatically work for you.

00:10:09.800 --> 00:10:14.460
Now, if you have non-contiguous accesses,
we recommend that you replace a single

00:10:14.460 --> 00:10:16.790
DST with several DCBT instructions.

00:10:16.790 --> 00:10:18.570
That's a Data Cache Block Touch.

00:10:18.800 --> 00:10:23.020
So you would issue one
of those for each line.

00:10:27.380 --> 00:10:31.770
So, legacy code that uses DCBZ,
which is the zeroing of a cache block,

00:10:31.770 --> 00:10:35.890
or DCBA, the allocation of a cache block,
is going to perform very poorly on G5.

00:10:36.000 --> 00:10:36.990
Why is that?

00:10:37.100 --> 00:10:41.580
Well, DCBZ is emulated effectively
to only work on 32 bytes,

00:10:41.750 --> 00:10:46.390
and we had to do this to ensure backwards
compatibility with existing code.

00:10:47.000 --> 00:10:49.750
DCBA is not implemented on G5,
so that's just going to

00:10:49.750 --> 00:10:50.930
be an illegal instruction.

00:10:50.950 --> 00:10:54.580
You'll end up in an exception handler
and then bounce back to your code.

00:10:54.740 --> 00:10:59.590
So this is just going to be tremendously
bad for any performance-critical code,

00:10:59.610 --> 00:11:01.720
and the only reason you would
have used these instructions is

00:11:01.720 --> 00:11:04.110
because it's performance-critical.

00:11:04.530 --> 00:11:09.920
So the solution is get the
DCBZs and DCBAs out of the code.

00:11:10.200 --> 00:11:13.680
Again, DCBA is just a performance hint,
so that shouldn't affect

00:11:13.680 --> 00:11:15.160
any kind of functionality.

00:11:15.260 --> 00:11:19.140
If you do need to zero cache lines,
we would recommend that you use

00:11:19.140 --> 00:11:22.390
memset or b0 rather than trying to
roll your own zeroing functions.

00:11:22.400 --> 00:11:27.240
But if you do need a DCBZ type of
function or an allocate of a cache line,

00:11:27.240 --> 00:11:30.400
we have this new mnemonic called DCBZL.

00:11:30.400 --> 00:11:35.400
And that's going to effectively
zero out whatever the native cache

00:11:35.400 --> 00:11:40.170
line length is for any system,
whether it's G3, G4, or G5.

00:11:41.220 --> 00:11:44.470
So here's an example of how to use DCBZL.

00:11:44.470 --> 00:11:47.470
Now, for those of you who have used DCBZ,
you'd say, "Well,

00:11:47.520 --> 00:11:50.430
the original definition of
DCBZ was simply to zero out

00:11:50.430 --> 00:11:54.090
the native cache line length,
so what have we changed?" Well,

00:11:54.090 --> 00:11:56.600
the reason we have to have this
new mnemonic is because most

00:11:56.600 --> 00:11:58.400
programmers ignored that warning.

00:11:58.540 --> 00:12:02.580
They coded for 32 bytes,
and now they're going to get bitten.

00:12:02.720 --> 00:12:08.740
So what we'd much rather have you
do is code based on line size,

00:12:08.980 --> 00:12:12.610
so effectively stride through
memory based on the line size,

00:12:12.610 --> 00:12:14.540
which you can get from
the operating system,

00:12:14.640 --> 00:12:18.730
whatever the current line
size is on the system.

00:12:18.900 --> 00:12:21.130
And of course,
if you're just doing a memset operation,

00:12:21.130 --> 00:12:25.510
we'd much prefer that
you use memset or B0.

00:12:28.470 --> 00:12:30.730
So, synchronization primitives--
this would be locks,

00:12:30.880 --> 00:12:34.460
sinks, iSinks-- they are going to be
more costly on this architecture,

00:12:34.560 --> 00:12:36.900
on this chip, than they were on G4.

00:12:36.900 --> 00:12:40.100
And that's for two reasons: one,
the longer pipelines, and two,

00:12:40.110 --> 00:12:42.410
the longer latencies to memory.

00:12:42.770 --> 00:12:47.240
So, this is a tough one,
but what you have to do is just make sure

00:12:47.240 --> 00:12:50.300
all your locking is absolutely necessary.

00:12:50.330 --> 00:12:53.700
Minimize the lock hold time so you're
not contending for locks as much.

00:12:53.850 --> 00:12:56.360
And of course,
ensure that each lock is in its

00:12:56.360 --> 00:13:00.170
own cache line so you don't have
fighting between processors.

00:13:03.960 --> 00:13:07.570
So scheduling is crucial for this chip,
and it's going to require

00:13:07.580 --> 00:13:12.240
recompiling or even hand
scheduling for optimal performance.

00:13:13.610 --> 00:13:17.790
So what we recommend is you use GCC 3.3,
which has a pipeline model

00:13:17.810 --> 00:13:20.440
and scheduling model for 970.

00:13:20.480 --> 00:13:23.600
And the other thing you can do for really
performance critical code is understand

00:13:23.720 --> 00:13:26.640
dispatch group formation using SHARC.

00:13:26.650 --> 00:13:28.280
And for those of you who
don't know what SHARC is,

00:13:28.350 --> 00:13:30.710
we'll get to that in just a minute.

00:13:33.330 --> 00:13:36.830
So in summary,
this is a very parallel core.

00:13:37.140 --> 00:13:42.100
You have two, basically,
of each unit-- LSUs, FPUs, FXUs,

00:13:42.130 --> 00:13:45.020
lots of renames,
lots of instructions in flight.

00:13:45.080 --> 00:13:49.140
So if you have very synchronous code,
it's simply not gonna take

00:13:49.190 --> 00:13:50.990
advantage of this core.

00:13:52.470 --> 00:13:55.880
So what you want to do is, of course,
unroll and schedule.

00:13:55.880 --> 00:13:59.760
You can also use AlteVec to calculate
up to a theoretical peak of 32

00:13:59.840 --> 00:14:02.760
gigaflops on a 2 gigahertz system,
a dual.

00:14:04.910 --> 00:14:08.640
The 970 has the full precision
hardware square root,

00:14:08.640 --> 00:14:12.800
so you don't need to make calls to any
libm functions for square root anymore.

00:14:12.800 --> 00:14:18.810
If you're using GCC, we offer this flag,
PowerPC-GPopt.

00:14:20.950 --> 00:14:25.380
We also have native long long support
because this is a 64-bit chip.

00:14:25.390 --> 00:14:32.610
It can natively do double word lengths
in leaf functions using PowerPC 64.

00:14:37.520 --> 00:14:41.450
So again, the system and the chip are all
designed for high bandwidth.

00:14:41.520 --> 00:14:43.630
There's incredible
bandwidth to the L1 cache,

00:14:43.770 --> 00:14:46.080
between the caches, and out to memory.

00:14:46.180 --> 00:14:52.880
32, 64, and effectively 3.5 gigabytes per
second in each direction on the bus.

00:14:53.550 --> 00:14:55.690
Take advantage of that
by using streaming,

00:14:55.840 --> 00:15:00.370
using software streaming and
hardware streaming prefetch.

00:15:03.680 --> 00:15:06.000
So again,
the optimal cache control instruction,

00:15:06.000 --> 00:15:09.560
rather than a DST,
is to use DCBT to prefetch.

00:15:09.610 --> 00:15:11.820
If you have a DST that
covered a lot of ground,

00:15:12.010 --> 00:15:16.980
multiple cache lines will then
issue multiple DCBT's in its place.

00:15:16.980 --> 00:15:19.240
Don't use DCBZ, because that's emulated.

00:15:19.240 --> 00:15:23.090
Use the DCBZL instruction,
but be careful if you're using it.

00:15:23.090 --> 00:15:25.480
Make sure you account
for cache line size.

00:15:25.940 --> 00:15:29.950
And again, DCBA and DCBI are illegal,
so those just need to

00:15:29.950 --> 00:15:31.620
be removed from code.

00:15:33.270 --> 00:15:37.000
Okay,
so we've talked a lot of theory here.

00:15:37.410 --> 00:15:41.200
How you actually get down and dirty with
your code and figure out what's going on.

00:15:41.200 --> 00:15:44.230
Well that's where Chud comes in,
so I'd like to introduce

00:15:44.230 --> 00:15:45.670
Nathan Slingerland.

00:15:51.600 --> 00:15:54.220
Okay, thank you Sanjay.

00:15:54.590 --> 00:15:58.510
So hopefully a lot of you were introduced
to CHUD tools last year at WWDC,

00:15:58.600 --> 00:16:01.600
at least the version 2 tools.

00:16:01.800 --> 00:16:06.600
And this year we're happy to give
you the version 3 of the tools,

00:16:06.600 --> 00:16:09.600
and we have a lot of enhancements
and improvements to that.

00:16:09.600 --> 00:16:13.640
But basically the CHUD tools are
a suite of low-level performance

00:16:13.640 --> 00:16:17.810
analysis tools written by Apple's
Architecture and Performance Group.

00:16:17.810 --> 00:16:22.040
And they give you access to the
performance counters and the processor

00:16:22.040 --> 00:16:24.620
memory controller operating system.

00:16:24.980 --> 00:16:28.560
and using these counters with
CHUD you can find problems in

00:16:28.560 --> 00:16:30.970
your code and improve your code.

00:16:31.450 --> 00:16:32.400
And of course it's free.

00:16:32.400 --> 00:16:37.470
So it's on the Developer Tools CD,
and it's also on the website.

00:16:38.580 --> 00:16:43.360
So in 3.0,
we have several classes of tools,

00:16:43.500 --> 00:16:46.690
profiling tools,
so tools to find out where

00:16:46.690 --> 00:16:47.480
things are happening.

00:16:47.570 --> 00:16:49.500
These include Shark.

00:16:49.500 --> 00:16:53.500
So this is the successor to Shikari,
if you've used that.

00:16:53.700 --> 00:16:56.500
And we'll get to all the
great new features it has.

00:16:56.500 --> 00:16:59.120
Monster is a spreadsheet
for performance events,

00:16:59.120 --> 00:17:01.500
and it has a lot of great new features,
too.

00:17:01.660 --> 00:17:07.490
Saturn is a new tool for visualizing
function calling behavior.

00:17:08.590 --> 00:17:12.520
For tracing,
so if you've ever done Altevec or

00:17:12.520 --> 00:17:20.330
a very processor critical code,
sometimes it's useful to see how things

00:17:20.340 --> 00:17:23.100
are actually happening on the processor.

00:17:23.230 --> 00:17:28.930
So we have Amber to take an instruction
level trace of a particular program,

00:17:28.930 --> 00:17:32.190
and then Assets is a program
to analyze this trace,

00:17:32.190 --> 00:17:36.700
or SIMG4,
PowerPC 7400 cycle accurate simulator,

00:17:36.710 --> 00:17:43.050
and soon SIMG5, so you'll be able to
simulate for the PowerPC 970.

00:17:43.660 --> 00:17:46.120
And of course,
we provide the CHUD framework.

00:17:46.120 --> 00:17:48.710
So this is the API we
use in all our tools.

00:17:48.710 --> 00:17:53.490
And you can use this to make your
own tools or call into the CHUD tools

00:17:53.550 --> 00:17:56.910
and have them do what you need.

00:17:57.420 --> 00:17:58.800
Okay, so the performance counters.

00:17:58.820 --> 00:18:01.910
These are in the processor and memory
controller and operating system,

00:18:02.000 --> 00:18:02.560
as I mentioned.

00:18:02.560 --> 00:18:07.430
And what these do,
they count interesting low-level

00:18:07.570 --> 00:18:13.160
performance events that things such
as cache misses or instruction stall

00:18:13.160 --> 00:18:16.590
cycles that would otherwise you'd
have to use a simulator to find

00:18:16.590 --> 00:18:18.000
out what's happening at this level.

00:18:18.000 --> 00:18:21.700
Page faults in the operating system,
you can find out when those happen.

00:18:21.700 --> 00:18:25.220
And the Chud tools let you
configure these counters,

00:18:25.220 --> 00:18:27.560
tell them what to count,
record the counts,

00:18:27.710 --> 00:18:30.990
and then you can use the tools
to look at what the result is.

00:18:32.800 --> 00:18:35.140
Okay, so the first tool we're
going to talk about is Shark.

00:18:35.140 --> 00:18:36.790
Shark is a system-wide profiling tool.

00:18:36.790 --> 00:18:41.560
So you can use it to profile a process,
a thread, or the entire system if

00:18:41.650 --> 00:18:42.700
you want to look at that.

00:18:42.800 --> 00:18:46.700
The most basic usage
is just a time profile.

00:18:46.920 --> 00:18:49.800
So this will show you where
the hotspots are in the system,

00:18:49.800 --> 00:18:51.720
where the system is spending its time.

00:18:51.800 --> 00:18:55.630
You can also use any of the
performance counter events.

00:18:55.870 --> 00:19:01.470
So you can get an event profile
to see where hardware events

00:19:01.540 --> 00:19:02.790
relate to your source code.

00:19:02.810 --> 00:19:06.780
For example, where cache misses might be
coming from in your code.

00:19:06.780 --> 00:19:09.800
We capture everything: drivers, kernel,
applications.

00:19:09.870 --> 00:19:12.740
So if you're a driver or
kernel extension writer,

00:19:12.740 --> 00:19:15.820
you can also use CHUD to see
the call stack and find out

00:19:15.830 --> 00:19:17.700
where things are coming from.

00:19:17.800 --> 00:19:19.720
And of course, we're very low overhead.

00:19:19.810 --> 00:19:24.380
This is all handled inside of the--
inside of our own kernel extension.

00:19:24.920 --> 00:19:27.890
Once you've gathered this session
that you're interested in,

00:19:27.910 --> 00:19:29.800
take in the samples that
you want to look at.

00:19:29.820 --> 00:19:33.770
We provide automated analysis,
so we annotate your source code,

00:19:33.910 --> 00:19:37.040
disassembly of your source code,
give you optimization tips

00:19:37.130 --> 00:19:38.800
about how to improve your code.

00:19:38.800 --> 00:19:41.750
And there's also static analysis.

00:19:41.790 --> 00:19:45.170
You can use this to just look for,
for example,

00:19:45.170 --> 00:19:49.410
DCBA instructions in your code,
if you want to make sure you

00:19:49.410 --> 00:19:51.530
catch every instance of that.

00:19:51.800 --> 00:19:54.740
There's a command line version,
so this is both scriptable and

00:19:54.740 --> 00:19:58.350
you can also use it--you know,
you can telnet into a machine and

00:19:58.350 --> 00:20:00.440
use this from the command line.

00:20:01.150 --> 00:20:04.800
And of course we can save sessions and
you can give them to your colleagues,

00:20:04.910 --> 00:20:08.450
pass them around,
whatever you'd like to do.

00:20:09.150 --> 00:20:11.800
So Monster is a more direct
interface to the counters.

00:20:11.830 --> 00:20:14.850
This lets you look directly at
the results of the counters.

00:20:14.880 --> 00:20:18.950
You can configure them using Monster,
collect the PMC data

00:20:18.950 --> 00:20:23.340
based on timed intervals,
hotkey, event counts,

00:20:23.340 --> 00:20:26.100
every 10,000 cache misses, for example.

00:20:26.330 --> 00:20:29.890
And then you can view the result
in a spreadsheet or a chart.

00:20:30.160 --> 00:20:33.960
In addition to just the
raw performance counts,

00:20:33.960 --> 00:20:36.030
there is a built-in shortcut language.

00:20:36.190 --> 00:20:40.200
This uses an infix computational
language that you can program your

00:20:40.200 --> 00:20:44.100
own metrics that you're interested in,
or you can use the built-in ones,

00:20:44.120 --> 00:20:50.760
things like memory bandwidth,
bandwidth over the memory bus,

00:20:51.180 --> 00:20:53.070
or cycles per instruction,
a variety of things.

00:20:53.270 --> 00:20:57.260
There's a command line version of
Monster provided as well for your

00:20:57.260 --> 00:21:00.800
scripting and remote sessions,
and you can also save and

00:21:00.800 --> 00:21:02.820
review these sessions as well.

00:21:04.370 --> 00:21:09.170
Okay, so the best way to see how to use
these tools is with a demonstration.

00:21:09.190 --> 00:21:12.950
So what we're going to look at is a
program called the Noble Ape Simulation.

00:21:13.030 --> 00:21:16.690
This is written by Tom Barbele,
and he's simulating apes

00:21:17.010 --> 00:21:18.410
on a tropical island.

00:21:18.670 --> 00:21:21.430
And these apes can think,
and he's simulating the

00:21:21.430 --> 00:21:24.540
biological environment,
so the food and the other

00:21:24.540 --> 00:21:28.020
animals on the island,
as well as the cognitive

00:21:28.020 --> 00:21:29.860
processes of the apes.

00:21:30.000 --> 00:21:32.960
I mean, obviously,
simple cognitive processes such as

00:21:32.970 --> 00:21:35.380
desire and fear and those kind of things.

00:21:35.450 --> 00:21:38.140
So this is open source,
and for more information,

00:21:38.140 --> 00:21:42.100
please check out his
website at nobleape.com.

00:21:42.210 --> 00:21:45.130
So let's switch to the demo machine.

00:21:45.600 --> 00:21:47.700
and see this C-noble ape in action.

00:21:47.700 --> 00:21:49.990
Okay, so this is the map window.

00:21:50.280 --> 00:21:52.150
Here we just showed, this is the island,
right?

00:21:52.160 --> 00:21:55.730
And each red dot represents
an ape running around the

00:21:55.730 --> 00:21:57.100
island doing its thing.

00:21:57.110 --> 00:21:59.260
And we can select one ape at a time.

00:21:59.260 --> 00:22:02.300
That's the ape with the
red box around him there.

00:22:02.600 --> 00:22:07.110
And for this ape we can see his brain,
what's happening in his

00:22:07.180 --> 00:22:09.270
brain in the brain window,
right?

00:22:09.280 --> 00:22:13.520
And of course, any good performance study
requires a performance metric,

00:22:13.520 --> 00:22:16.180
and our metric is ape
thoughts per second.

00:22:16.300 --> 00:22:24.200
So this is the, for the original code,
we're getting, we have this metric.

00:22:24.320 --> 00:22:28.030
So around 1200, 1300 or so.

00:22:28.090 --> 00:22:32.840
Okay, so the first thing we'd like to do,
we're gonna launch Shark,

00:22:32.840 --> 00:22:35.000
and we'll see what's
happening in the system.

00:22:35.480 --> 00:22:40.140
Okay, so this is the main Shark window,
and it's really pared down and simple,

00:22:40.140 --> 00:22:43.280
just to let you start your work.

00:22:43.400 --> 00:22:46.400
By default,
we come up with the time profile,

00:22:46.400 --> 00:22:48.340
so this would be the most
common thing you'd use it for.

00:22:48.400 --> 00:22:52.400
We provide a bunch of other built-in
shortcuts and configurations.

00:22:52.400 --> 00:22:54.570
You can, of course,
you can create your own using

00:22:54.800 --> 00:22:56.400
any of the performance counters.

00:22:56.400 --> 00:22:58.400
But for now,
we're going to use time profile.

00:22:58.400 --> 00:23:01.480
There's a start button
here for starting sampling,

00:23:01.590 --> 00:23:06.280
but you can also use the-- there's a
global hotkey so that Shark doesn't

00:23:06.450 --> 00:23:08.400
have to be in the foreground.

00:23:08.400 --> 00:23:11.280
It can be, you know, in the background,
and you can start it.

00:23:11.400 --> 00:23:13.390
So we'll use that hotkey.

00:23:13.400 --> 00:23:16.050
We'll take a 5 or 10 seconds sample.

00:23:16.400 --> 00:23:18.470
See what's happening.

00:23:25.530 --> 00:23:26.830
So here's the profile.

00:23:27.040 --> 00:23:30.630
What we've done here,
this is just listing the functions

00:23:30.640 --> 00:23:36.000
that it sampled inside Noble Ape
from most sampled to least sampled.

00:23:36.090 --> 00:23:38.990
So when you're optimizing,
you wanna work on what's

00:23:39.070 --> 00:23:41.230
running the most of the time,
then you're gonna get the most

00:23:41.350 --> 00:23:43.100
benefit out of optimizing that code.

00:23:43.100 --> 00:23:46.660
So we see that Noble Ape
is 50% of the system.

00:23:46.660 --> 00:23:48.880
This is the process pop-up, right?

00:23:48.880 --> 00:23:52.660
And this is like top,
it lists what was running in the system.

00:23:52.660 --> 00:23:56.060
It's kind of strange, 50% of the time,
even though we know that we're CPU bound.

00:23:56.200 --> 00:24:00.930
Well, if we go to the thread pop-up here,
you can see that in fact,

00:24:00.960 --> 00:24:01.920
it's single threaded.

00:24:01.990 --> 00:24:05.400
So this is,
we're running Noble Ape on a Power Mac G5

00:24:05.400 --> 00:24:08.630
that has dual two gigahertz processors.

00:24:08.820 --> 00:24:14.600
So, all right, so next step,
we wanna thread this thing, right,

00:24:14.600 --> 00:24:19.160
since we wanna take
advantage of both processors.

00:24:19.220 --> 00:24:20.840
So this is the heavy view
that we're looking at.

00:24:20.860 --> 00:24:23.290
There's a heavy profile
of view and a tree view.

00:24:23.320 --> 00:24:25.680
The heavy view,
we can open up these disclosure

00:24:25.680 --> 00:24:28.850
triangles and see how we
got to this heavy function.

00:24:28.940 --> 00:24:31.120
So we started in main,
we called plat cycle,

00:24:31.130 --> 00:24:33.680
called control cycle,
called cycle ape troop,

00:24:33.690 --> 00:24:37.490
called the cycle troop brain scaler,
this important function.

00:24:38.010 --> 00:24:41.390
So we know our code,
and we know that we can't really

00:24:41.680 --> 00:24:45.180
split the processing of things
between simulation cycles,

00:24:45.190 --> 00:24:45.580
right?

00:24:45.580 --> 00:24:49.330
The way this app works, there's a,
you know, it does a simulation cycle,

00:24:49.330 --> 00:24:51.750
and, you know,
within each simulation cycle,

00:24:51.750 --> 00:24:54.590
it's processing a bunch
of ape simulation cycles.

00:24:54.620 --> 00:24:57.840
Well, the simulation cycles
themselves are not independent.

00:24:57.840 --> 00:24:59.220
They depend on one another, right?

00:24:59.220 --> 00:25:01.220
But we know that the
apes are independent.

00:25:01.220 --> 00:25:03.530
They're independent thinking apes, so,
you know,

00:25:03.530 --> 00:25:05.240
we can parallelize it at that level.

00:25:05.280 --> 00:25:07.700
We can process the apes in parallel.

00:25:07.900 --> 00:25:10.060
For each simulation cycle.

00:25:10.060 --> 00:25:11.490
So that's what we did.

00:25:11.520 --> 00:25:15.060
We threaded that to split
up the number of apes.

00:25:15.180 --> 00:25:17.820
We have 64 apes to split it
between two threads evenly.

00:25:17.820 --> 00:25:20.460
So you'll note that the brain
rate is originally around 1,200.

00:25:20.460 --> 00:25:24.770
When we do this, we get around 20,
almost 2,400, not quite.

00:25:24.800 --> 00:25:25.980
That's pretty good.

00:25:26.050 --> 00:25:27.940
We've gotten a nice speed
up just from threading,

00:25:27.970 --> 00:25:29.810
from taking advantage of
that second processor.

00:25:29.820 --> 00:25:32.590
So let's profile again
and see what's going on.

00:25:32.620 --> 00:25:36.160
Okay.

00:25:40.890 --> 00:25:42.850
So now we can see that
in the process pop-up,

00:25:42.900 --> 00:25:45.340
we can see that Noble Ape
is taking up a much more

00:25:45.340 --> 00:25:47.100
significant amount of the system.

00:25:47.130 --> 00:25:48.800
That's a good thing.

00:25:48.800 --> 00:25:52.230
And we can see from the thread
pop-up that we have our main thread,

00:25:52.230 --> 00:25:56.330
that's the 9.2%, which spun
up two computational threads,

00:25:56.330 --> 00:25:58.520
each about 40% of the time.

00:26:00.870 --> 00:26:04.900
So, the next thing we'd like to do is
actually optimize this function.

00:26:04.900 --> 00:26:06.560
This function is important to us, right?

00:26:06.560 --> 00:26:09.800
It's almost 90% of the
time is spent in here.

00:26:09.800 --> 00:26:13.830
So, if we double click on this,
Shark will present us with

00:26:13.830 --> 00:26:17.800
our source that's been
highlighted where we've sampled.

00:26:17.800 --> 00:26:22.800
So, this tells us where in our
source code we're spending time.

00:26:22.910 --> 00:26:25.760
So, it's actually inside of this
Cycle Troop Brain Scaler,

00:26:25.880 --> 00:26:28.890
it's just--it's this for loop.

00:26:29.780 --> 00:26:34.450
So we can see that this for loop
actually represents about 94%

00:26:34.600 --> 00:26:36.700
of the time in this function.

00:26:36.700 --> 00:26:38.450
Okay.

00:26:38.530 --> 00:26:40.680
So I should probably talk about a
couple of these other things here.

00:26:40.710 --> 00:26:44.700
Oh yeah, well so the scroll bar at
the side is like an overview.

00:26:44.700 --> 00:26:47.700
You can easily jump to the
hot spots in your code.

00:26:47.700 --> 00:26:48.110
Right.

00:26:48.110 --> 00:26:49.850
So it's colored accordingly, right.

00:26:49.850 --> 00:26:51.700
A brighter yellow means more samples.

00:26:51.700 --> 00:26:54.700
At the top we have a source file list.

00:26:54.700 --> 00:26:56.630
This is, you know,
sometimes you can have more than one

00:26:56.630 --> 00:27:00.690
source file contributing to a particular
function with header files and like that.

00:27:00.700 --> 00:27:04.700
And this function popup is like
what you have in Project Builder.

00:27:04.700 --> 00:27:06.700
You can easily jump to
different functions.

00:27:06.700 --> 00:27:08.600
And then we have the edit button.

00:27:08.830 --> 00:27:11.990
So what this allows you to do is
it will jump into Project Builder

00:27:12.120 --> 00:27:13.700
at the same selected line.

00:27:13.700 --> 00:27:14.240
Right.

00:27:14.240 --> 00:27:17.790
So you can easily go to where you
want to edit and change something

00:27:17.790 --> 00:27:19.700
once you know where the problem is.

00:27:19.750 --> 00:27:20.660
Okay.

00:27:20.780 --> 00:27:21.700
So let's go back to Shark.

00:27:23.110 --> 00:27:26.660
and what Shark does is it
provides us with advice.

00:27:26.760 --> 00:27:29.090
That's what this little
exclamation point button is.

00:27:29.090 --> 00:27:30.380
It's advice for us.

00:27:30.390 --> 00:27:32.000
It's calling something out.

00:27:32.000 --> 00:27:35.910
And so there's two bits of advice here,
but we'll focus just on the first one.

00:27:36.000 --> 00:27:39.000
So this loop contains
8-bit integer computation.

00:27:39.070 --> 00:27:41.370
And if you--obviously if you're
spending a lot of time in this

00:27:41.370 --> 00:27:45.210
8-bit integer computation,
it might be a good idea to use Altevec

00:27:45.240 --> 00:27:48.000
to really improve the speed of this code.

00:27:48.140 --> 00:27:50.000
So that was what we did.

00:27:50.000 --> 00:27:51.000
That was our next step.

00:27:51.090 --> 00:27:55.630
So let's, I guess, let's go try that out.

00:28:01.220 --> 00:28:05.740
"Get the ring." So,
that's a nice speed up,

00:28:05.760 --> 00:28:07.100
but we're not done yet.

00:28:07.210 --> 00:28:09.200
That's good.

00:28:09.200 --> 00:28:11.190
So, let's profile again.

00:28:11.200 --> 00:28:14.120
Let's see where we're spending time now.

00:28:19.500 --> 00:28:21.440
We'll double click again on this.

00:28:21.440 --> 00:28:24.810
We see the vector function shows
up at the top of the profile.

00:28:25.740 --> 00:28:27.600
And this is the vector code.

00:28:27.600 --> 00:28:30.600
So a lot of you are probably--if
you've used Shikari,

00:28:30.600 --> 00:28:33.560
you saw the assembly view.

00:28:33.600 --> 00:28:36.600
If you double-click on any
of these--any source line,

00:28:36.600 --> 00:28:39.600
it's going to jump to the assembly
view that you're familiar with.

00:28:39.600 --> 00:28:42.560
And if you double-click on the assembly,
it'll jump back.

00:28:42.620 --> 00:28:46.600
And it's going to highlight the
line--the instruction or instructions

00:28:46.600 --> 00:28:48.600
that correspond to that source line.

00:28:48.600 --> 00:28:50.850
So you can see--this can give
you an idea of how good the

00:28:50.850 --> 00:28:52.600
code gen is for your compiler,
right?

00:28:52.600 --> 00:28:54.130
right, how many,
what kind of instructions it's

00:28:54.170 --> 00:28:56.440
generating for each source line.

00:28:56.640 --> 00:29:01.190
And, okay, so if you've seen this before,
the columns here, we have samples,

00:29:01.190 --> 00:29:03.260
how many times we
sampled each instruction,

00:29:03.260 --> 00:29:05.500
address,
obviously the address and instruction,

00:29:05.500 --> 00:29:08.640
the instructions themselves,
and switch between various

00:29:09.150 --> 00:29:10.500
views of the address.

00:29:10.500 --> 00:29:14.200
Cycles is the latency and throughput
of a particular instruction,

00:29:14.200 --> 00:29:18.050
so these are for the 970, right,
that's the CPU model down at the

00:29:18.050 --> 00:29:21.500
right-- lower right-hand corner there,
tells you that.

00:29:21.570 --> 00:29:26.600
And in the comment column,
various things about this code,

00:29:26.650 --> 00:29:28.480
and of course the source file at the end.

00:29:28.500 --> 00:29:32.440
Now, one of the nice things that
we give you is an ability to

00:29:32.530 --> 00:29:34.500
visualize dispatch groups.

00:29:34.500 --> 00:29:39.120
So if we go to this option,
we can turn this on.

00:29:39.200 --> 00:29:46.520
If you remember the diagram that Sanjay
showed earlier with the dispatch,

00:29:46.650 --> 00:29:50.460
we can see here how they--usually
between four and five instructions,

00:29:50.570 --> 00:29:53.540
dispatch, this would be in each cycle.

00:29:53.540 --> 00:29:56.170
So this can give you a good
idea about how things are

00:29:56.170 --> 00:29:58.200
actually behaving on the machine.

00:29:58.360 --> 00:30:03.900
So the other thing we provide
too is this functional unit and

00:30:03.900 --> 00:30:07.960
dispatch slot utilization graph.

00:30:08.400 --> 00:30:10.200
So you want to talk a little bit to that?

00:30:10.200 --> 00:30:10.700
Right.

00:30:10.700 --> 00:30:15.270
So on 970, as a programmer,
the key bottleneck you'll have to face

00:30:15.430 --> 00:30:18.600
is maximizing dispatch group width,
because that's one of the

00:30:18.600 --> 00:30:22.180
narrower points in the core,
because it's four instructions

00:30:22.180 --> 00:30:23.340
wide plus a branch.

00:30:23.390 --> 00:30:26.860
So what we offer you here in dispatch
slot utilization over here on the

00:30:26.950 --> 00:30:28.600
right is the average group size.

00:30:28.600 --> 00:30:32.310
You can see how effectively is
your code taking advantage of

00:30:32.320 --> 00:30:36.920
this really wide issue machine,
wide dispatch machine.

00:30:36.920 --> 00:30:40.370
And dispatch defines where
instructions are issued to,

00:30:40.390 --> 00:30:41.410
which functional units.

00:30:41.440 --> 00:30:45.980
So here you see a map of the 12
functional units that I talked about.

00:30:45.980 --> 00:30:48.960
And you can see that the
units are symmetrical,

00:30:48.960 --> 00:30:50.900
like the two LSUs here.

00:30:50.900 --> 00:30:57.010
If there's a big imbalance between, say,
one of these, the LSUs-- one is doing a

00:30:57.010 --> 00:30:59.300
lot and one's not-- well,
that's something that you could

00:30:59.300 --> 00:31:01.940
probably correct with scheduling
or reordering your code,

00:31:01.940 --> 00:31:05.440
because what you want to do is
balance the execution units.

00:31:05.440 --> 00:31:05.930
Right?

00:31:05.940 --> 00:31:09.400
You don't want half the chip doing all
the work and the other half sitting idle.

00:31:09.400 --> 00:31:11.500
All of that is defined
by dispatch groups.

00:31:11.510 --> 00:31:15.150
So that's why we've put dispatch
group modeling into Shark.

00:31:16.400 --> 00:31:18.480
- Right, so you can select a few, right?

00:31:18.480 --> 00:31:20.650
I mean, you can do the dynamic selection,
right?

00:31:20.840 --> 00:31:22.440
- So this is dynamic.

00:31:22.480 --> 00:31:25.820
You select a few instructions and it'll
tell you where they got mapped to.

00:31:25.830 --> 00:31:29.510
The charts update and the
numbers will update it with it.

00:31:31.940 --> 00:31:35.900
Great.

00:31:35.970 --> 00:31:40.300
So this can help you, obviously,
on the Power Mac G5, tuning your code.

00:31:40.420 --> 00:31:44.250
But let's go back to the source view.

00:31:45.360 --> 00:31:47.800
And if we look a little
closer at our vector code,

00:31:47.810 --> 00:31:50.910
you remember we vectorized
this inner loop that we saw

00:31:50.910 --> 00:31:53.330
was taking up 94% of the time,
right?

00:31:53.430 --> 00:31:56.970
Well now that loop is still important,
but it's taking up a smaller portion

00:31:56.970 --> 00:31:59.170
of the time in this function.

00:31:59.250 --> 00:32:03.400
We can see also that up at the
top and bottom of this function,

00:32:03.400 --> 00:32:07.670
we're spending more time-- more time
relatively speaking-- in the scalar code,

00:32:07.730 --> 00:32:08.030
right?

00:32:08.190 --> 00:32:10.380
The two loops that we didn't touch.

00:32:10.680 --> 00:32:15.240
So, this code is very similar
to the other loop,

00:32:15.370 --> 00:32:19.600
it's almost exactly the same,
and I believe Sharpe will point this out,

00:32:19.600 --> 00:32:20.780
but yeah.

00:32:20.790 --> 00:32:22.970
Right, it's saying, yeah, hey,
vectorize this loop too,

00:32:22.970 --> 00:32:24.470
this is important now.

00:32:24.560 --> 00:32:28.120
So, that was the next step,
was to vectorize the rest of this,

00:32:28.180 --> 00:32:32.530
so to vectorize the entire function,
and as well as a few other,

00:32:32.580 --> 00:32:35.100
there are a few other
optimizations as well.

00:32:35.100 --> 00:32:37.820
So, let's try that out.

00:32:37.860 --> 00:32:40.800
starting around ten thousand or so

00:32:42.460 --> 00:32:47.940
So another 40 to 50% we can eke out by
vectorizing the rest of that function.

00:32:47.940 --> 00:32:50.540
You can see some of the gorillas
have gone off into the water.

00:32:50.540 --> 00:32:52.820
You can bring them back to life
by dragging them back to land.

00:32:52.920 --> 00:32:56.170
They're a bit suicidal.

00:32:56.180 --> 00:32:59.610
Yeah, they just like the beach.

00:33:00.450 --> 00:33:03.850
All right, so, uh,
we're gonna have 15x or 14x speedup,

00:33:03.970 --> 00:33:05.280
that's pretty decent.

00:33:05.280 --> 00:33:09.030
Um...

00:33:12.600 --> 00:33:15.520
We hope you can all do
that well in your code too.

00:33:15.700 --> 00:33:20.520
So the next step, okay, so, no,
don't do that.

00:33:20.520 --> 00:33:23.240
We have a few more things to show.

00:33:23.250 --> 00:33:25.310
There are a couple of other things
we didn't really talk about.

00:33:25.370 --> 00:33:28.100
Shark allows you to manage
the sampling session.

00:33:28.150 --> 00:33:31.510
So we've taken about four
sampling sessions here.

00:33:31.660 --> 00:33:35.210
You can either look at these in parallel,
right, in a multi-window mode,

00:33:35.240 --> 00:33:38.060
or you can put them back and deal
with them one window at a time.

00:33:38.060 --> 00:33:41.820
The multi-window mode is nice because
you can put them side by side.

00:33:41.820 --> 00:33:43.300
There's also a session drawer.

00:33:43.300 --> 00:33:45.980
You can quickly switch
between them in single,

00:33:45.980 --> 00:33:48.040
in the single window mode.

00:33:48.040 --> 00:33:50.940
And of course, as we mentioned,
you can save sessions.

00:33:51.150 --> 00:33:54.740
There's also ancillary
information included.

00:33:54.750 --> 00:33:58.000
So whenever you take a session,
it records what kind of machine

00:33:58.000 --> 00:34:00.800
it was and gives you some space
to write notes to yourself about

00:34:00.800 --> 00:34:02.340
what was happening on this.

00:34:02.420 --> 00:34:03.680
So this is archival.

00:34:03.680 --> 00:34:06.700
You can keep it around,
remember what happened.

00:34:06.700 --> 00:34:09.380
There's also an extensive
user guide included.

00:34:09.440 --> 00:34:11.040
So it's just online here.

00:34:11.200 --> 00:34:12.630
Please read it.

00:34:12.720 --> 00:34:16.450
There's lots more information and
features that are covered in there.

00:34:16.460 --> 00:34:18.960
Okay.

00:34:19.510 --> 00:34:26.150
So, one other thing we wanted to look at,
we want to use the Monster Tool,

00:34:26.150 --> 00:34:31.400
and we want to look at some more--some
of these performance counters in depth.

00:34:31.520 --> 00:34:35.400
So, this is the main Monster Window,
and this is the spreadsheet, right?

00:34:35.400 --> 00:34:39.870
So, on the left-hand side,
we can see the various performance

00:34:40.000 --> 00:34:42.960
counters that are on this system,
and on the right is

00:34:42.960 --> 00:34:45.400
the spreadsheet itself.

00:34:45.400 --> 00:34:52.400
And the shortcut pop-up is similar to
the sampling config selection on Shark.

00:34:52.460 --> 00:34:53.400
Same thing.

00:34:53.400 --> 00:34:57.360
But what we can do is we
can edit these shortcuts.

00:34:57.430 --> 00:34:59.780
So, if we go to the shortcut tab.

00:34:59.970 --> 00:35:03.040
And we're going to look
at DDR memory bandwidth.

00:35:03.120 --> 00:35:08.320
Right, so this is--it's just--it's
taking a few of the U3 counters,

00:35:08.420 --> 00:35:12.660
the memory controller
counters on this Power Mac G5,

00:35:12.760 --> 00:35:15.350
and it's going to calculate
the number of megabytes

00:35:15.480 --> 00:35:17.900
transferred over the memory bus.

00:35:17.900 --> 00:35:23.280
All right, so the way it does this is it
counts the number of beats,

00:35:23.390 --> 00:35:24.900
and each beat is 16
bytes in the data bus,

00:35:24.900 --> 00:35:28.900
so it can multiply out and figure
out for every 10 millisecond sample

00:35:28.900 --> 00:35:30.900
how many megabytes that means.

00:35:30.910 --> 00:35:31.900
So we have a session.

00:35:31.900 --> 00:35:34.520
Let's open that.

00:35:38.770 --> 00:35:40.700
It's on the desktop, yeah.

00:35:40.700 --> 00:35:42.700
There it is.

00:35:42.700 --> 00:35:47.650
Right, so if we pop open the run pop-up,

00:35:48.400 --> 00:36:53.300
[Transcript missing]

00:36:53.660 --> 00:36:56.690
So you can see how as
we optimize the code,

00:36:56.700 --> 00:36:59.340
we were able to take better and
better advantage of this bandwidth,

00:36:59.440 --> 00:37:03.190
the massive bandwidth that's
available on the Power Mac G5.

00:37:10.390 --> 00:37:15.300
So let's go back to the slides, I think.

00:37:15.300 --> 00:37:20.300
Right.

00:37:20.300 --> 00:37:23.300
Right, so you might wonder--
I guess-- I'll let Sanjay talk.

00:37:23.300 --> 00:37:26.300
So you might wonder,
how does this compare against G4?

00:37:26.300 --> 00:37:30.300
So we started out with
the regular scalar code.

00:37:30.300 --> 00:37:34.300
And on the G4, you get about 1,200 ape
thoughts per second.

00:37:34.300 --> 00:37:37.300
We were getting closer
to 1,300 on the G4.

00:37:37.300 --> 00:37:40.300
Well, you say, well,
the G4 is running much higher frequency,

00:37:40.300 --> 00:37:43.010
and the G4,
we're barely getting a little more

00:37:43.010 --> 00:37:45.290
than 10% faster performance here.

00:37:45.340 --> 00:37:47.300
So what is the bottleneck?

00:37:47.300 --> 00:37:50.300
Well, the initial bottleneck was
all integer performance.

00:37:50.300 --> 00:37:53.270
So with the longer latency,
longer pipeline instructions,

00:37:53.370 --> 00:37:56.290
you're just not going to get
the full frequency increase

00:37:56.360 --> 00:37:58.300
in your performance increase.

00:37:58.300 --> 00:38:02.130
So when we went to threaded,
we started to expose the

00:38:02.130 --> 00:38:04.300
better bandwidth on the bus.

00:38:04.370 --> 00:38:07.300
So because we have two processors,
and they each have independent

00:38:07.300 --> 00:38:10.280
point-to-point connections
to the memory controller,

00:38:10.300 --> 00:38:12.300
we're running much faster than the G4.

00:38:12.300 --> 00:38:15.300
And then we really break this
open when we go to vector.

00:38:15.300 --> 00:38:17.160
The G4 does well.

00:38:17.300 --> 00:38:21.300
We get a 2.5 or so x
speedup from using vector.

00:38:21.300 --> 00:38:24.300
But on G5,
we don't have any bandwidth limit yet.

00:38:24.300 --> 00:38:28.300
So we get a full 4x improvement
from going to vector.

00:38:28.300 --> 00:38:30.300
So we're going to go to vector.

00:38:30.300 --> 00:38:31.300
And we're going to go to vector.

00:38:31.670 --> 00:38:34.580
And then with Vector Optimized,
you can see again the

00:38:34.610 --> 00:38:35.930
G4 does pretty well.

00:38:36.010 --> 00:38:37.580
Nice speed up.

00:38:37.650 --> 00:38:40.300
And the G5 gets a 60% speed up.

00:38:40.680 --> 00:38:42.500
And that's--you know,
if you look at the back

00:38:42.500 --> 00:38:44.540
of that monster chart,
you can see we were getting

00:38:44.890 --> 00:38:47.580
peak bandwidths of 2.5
gigabytes a second on the bus.

00:38:47.600 --> 00:38:51.300
So we're still not done yet,
but we didn't have more time

00:38:51.360 --> 00:38:53.600
to optimize before this demo.

00:38:53.600 --> 00:38:56.720
But clearly, you know,
there are a lot of resources there.

00:38:56.740 --> 00:38:59.780
And if you start with basic code, well,
you might get a decent

00:38:59.800 --> 00:39:01.200
speed up over a G4.

00:39:01.200 --> 00:39:04.750
But if you put a little effort into it,
you can get very big speed ups

00:39:04.750 --> 00:39:07.880
if you take advantage of AlteVec,
take advantage of all the

00:39:07.880 --> 00:39:10.120
bandwidth that's available to you.

00:39:13.630 --> 00:39:17.220
Okay, so a third tool we'll talk a
little bit about is Saturn.

00:39:17.220 --> 00:39:22.100
Saturn, so the other tool,
so Shark is another profiling

00:39:22.100 --> 00:39:23.140
tool that we've talked about.

00:39:23.210 --> 00:39:26.030
That provides a statistical profile,
right?

00:39:26.040 --> 00:39:27.930
It's periodically
interrupting the system,

00:39:27.930 --> 00:39:30.540
recording where you are,
and then going on, right?

00:39:30.540 --> 00:39:33.160
And then afterwards, we say, well,
wherever we got the most samples,

00:39:33.160 --> 00:39:34.430
that's where the most time was spent.

00:39:34.460 --> 00:39:38.800
Well, Saturn is going to instrument
every function in your source

00:39:38.800 --> 00:39:41.190
code to give you an exact profile.

00:39:41.760 --> 00:39:45.420
So we'll show you, this allows you to
visualize the call tree.

00:39:45.420 --> 00:39:50.380
It uses GCC to instrument each
function at entry and exit.

00:39:51.830 --> 00:39:54.360
And it records this function
call history to a trace,

00:39:54.360 --> 00:39:57.820
and with this we can get call counts,
so how many times each

00:39:58.040 --> 00:40:01.280
function was called,
performance monitor counts,

00:40:01.280 --> 00:40:04.090
it can use those,
as well as the execution

00:40:04.090 --> 00:40:05.690
time of each function,
right, added up.

00:40:05.700 --> 00:40:08.750
So if we look at this, this, you know,
at the top we have the

00:40:08.750 --> 00:40:11.850
familiar call stack,
you know, call tree view that says,

00:40:11.850 --> 00:40:14.560
you know,
how--where we spent time in each function

00:40:14.560 --> 00:40:16.700
and its descendants and like that.

00:40:16.700 --> 00:40:20.070
And at the bottom we have something
that's--it's viewing the same

00:40:20.070 --> 00:40:21.700
data but in a different way.

00:40:21.700 --> 00:40:26.700
It's plotting call stack depth vertically
versus time on the horizontal axis.

00:40:26.700 --> 00:40:32.240
And what you can use this for,
if you see a very sharp, you know,

00:40:32.240 --> 00:40:34.980
narrow spike,
that means that you're spending a

00:40:34.980 --> 00:40:36.690
lot of time in calling overhead,
right?

00:40:36.700 --> 00:40:39.500
You're not getting--you're
going through many,

00:40:39.500 --> 00:40:42.830
many function calls and not
getting a lot of work done if it's

00:40:42.830 --> 00:40:44.700
not a wide call or wide stack.

00:40:44.700 --> 00:40:45.690
So, okay.

00:40:45.700 --> 00:40:46.700
That's it.

00:40:46.700 --> 00:40:48.400
Saturn.

00:40:48.890 --> 00:40:50.550
And of course the CHED framework.

00:40:50.560 --> 00:40:52.900
You can use the CHED framework
to instrument your source code.

00:40:53.280 --> 00:40:57.060
You can use this to start
and stop Monster or Shark.

00:40:57.130 --> 00:41:00.800
You can also write your
own performance tools.

00:41:00.800 --> 00:41:03.600
A lot of the functionality,
almost all of it that's in Shark and

00:41:03.600 --> 00:41:05.660
Monster is exposed in this framework.

00:41:05.800 --> 00:41:08.390
So you can set up and
start and stop the PMCs,

00:41:08.390 --> 00:41:13.060
collect information about the hardware,
a lot of things that otherwise you'd

00:41:13.060 --> 00:41:17.760
have to go through I/O Kit and it might
be some extra--a lot of code to get at.

00:41:17.800 --> 00:41:23.290
And of course an extensive
HTML reference guide is provided.

00:41:24.730 --> 00:41:28.490
Okay, so here's an example of using
the framework to remotely

00:41:28.490 --> 00:41:30.440
control either Shark or Monster.

00:41:30.500 --> 00:41:34.080
What you do is you pick the profile
that you're interested in and then place

00:41:34.140 --> 00:41:36.700
either of those tools in remote mode.

00:41:36.700 --> 00:41:40.670
That means allow other tools that
want to connect and control the start

00:41:40.680 --> 00:41:42.580
and stop of the counters to do so.

00:41:42.700 --> 00:41:47.700
So first we initialize, you know,
acquire remote access,

00:41:47.700 --> 00:41:51.700
I mean make sure that another tool is
actually waiting for us to do something,

00:41:51.700 --> 00:41:54.700
and this will block if the other
tool isn't currently waiting.

00:41:54.880 --> 00:41:58.270
Start the RemotePerfMonitor,
so for this function you can give it a

00:41:58.280 --> 00:42:00.700
label that's going to appear in the tool.

00:42:00.770 --> 00:42:03.700
Do whatever it is you're interested in,
whatever the code of interest is.

00:42:03.700 --> 00:42:07.700
Stop the RemotePerfMonitor and of
course release to be a good citizen.

00:42:07.700 --> 00:42:12.690
So that's one way to use the framework,
that's to instrument.

00:42:12.700 --> 00:42:15.080
The other way is just more directly,
you can set up the counters

00:42:15.260 --> 00:42:16.700
directly and read them directly.

00:42:16.700 --> 00:42:19.690
You know, initialize,
acquire the sampling facility.

00:42:19.700 --> 00:42:23.200
So there's only one set of
performance counters in the system,

00:42:23.440 --> 00:42:25.780
right,
because there's one set of hardware,

00:42:25.780 --> 00:42:27.460
there's one physical device.

00:42:27.700 --> 00:42:29.830
So you have to acquire
the sampling facility,

00:42:29.910 --> 00:42:32.620
the kernel extension that we
have manages access to this.

00:42:32.700 --> 00:42:36.010
Set up the counter events,
clear the counters, start the counters,

00:42:36.010 --> 00:42:38.790
do whatever it is you're interested in,
stop the counters,

00:42:38.850 --> 00:42:40.410
and then process the result.

00:42:43.170 --> 00:42:46.150
We also provide some lower level tools.

00:42:46.150 --> 00:42:49.310
So if you've ever done, as I mentioned,
AlteVec programming,

00:42:49.310 --> 00:42:54.150
or any kind of really intense tuning,
you'd like to know what's

00:42:54.170 --> 00:42:56.000
happening on the processor core.

00:42:56.000 --> 00:42:59.000
Why is it slower than you expect?

00:42:59.000 --> 00:43:00.000
What's happening?

00:43:00.000 --> 00:43:02.590
So with Amber,
this is a command line tool to

00:43:02.590 --> 00:43:05.000
record an instruction trace to disk.

00:43:05.030 --> 00:43:08.000
So this is for all the
threads in a given process.

00:43:08.000 --> 00:43:09.000
Record that to disk.

00:43:09.000 --> 00:43:13.000
And then you can run that
trace file through ACID.

00:43:13.000 --> 00:43:13.900
This is a trace analyzer.

00:43:13.980 --> 00:43:16.000
It gives you some
interesting trace statistics.

00:43:16.030 --> 00:43:20.600
You can plot the memory footprint
that this trace walks through.

00:43:20.600 --> 00:43:24.590
Point out problematic
instruction sequences.

00:43:25.050 --> 00:43:31.040
And then you can also run this trace
through SIMG4 for the 7400 processor,

00:43:31.040 --> 00:43:34.210
or eventually,
when this is available soon,

00:43:34.350 --> 00:43:39.840
the SIMG5 PowerPC 970 simulator
to know exactly what's happening.

00:43:41.130 --> 00:43:45.900
Okay, so at this time,
we'll turn it back over to Mark, I think,

00:43:45.900 --> 00:43:47.340
for session wrap-up.

00:43:47.340 --> 00:43:48.710
Thank you.

00:43:56.560 --> 00:43:59.510
So to give you a little bit of
a roadmap of other sessions that

00:43:59.510 --> 00:44:03.820
are going to be valuable to you,
the Tuning Software with

00:44:03.820 --> 00:44:06.220
Performance Tools,
session 305 this afternoon

00:44:06.220 --> 00:44:08.860
at 5:00 in Presidio,
and then the

00:44:08.920 --> 00:44:10.460
MacOSX High Performance Libraries.

00:44:10.460 --> 00:44:15.010
Again, another set of tools-- really,
libraries are not tools,

00:44:15.010 --> 00:44:19.320
but venues here for you to be
able to eke out performance

00:44:19.320 --> 00:44:20.980
out of the operating system.

00:44:20.980 --> 00:44:24.140
Again,
optimization-- just to go back on my

00:44:24.140 --> 00:44:29.480
introductory statements-- should not
always be a process or an afterthought.

00:44:29.510 --> 00:44:33.020
Optimization should begin with when
you first start writing your code.

00:44:33.030 --> 00:44:36.220
It should be part of the process of
how you want your code to be written.

00:44:36.290 --> 00:44:39.980
So you're not going back after the
application is written and think about,

00:44:39.980 --> 00:44:42.040
well,
maybe I should thread my application.

00:44:42.040 --> 00:44:47.330
I should utilize threading processes.

00:44:47.340 --> 00:44:50.300
An application like the Noble 8,
we can add threading because

00:44:50.300 --> 00:44:50.960
it's not a lot of code.

00:44:50.960 --> 00:44:53.780
But if you're talking about
a much larger application,

00:44:53.780 --> 00:44:58.890
a word processor,
a graphic imaging editing application,

00:44:58.960 --> 00:45:01.280
then you're looking at a whole redesign,
possibly,

00:45:01.280 --> 00:45:03.200
and then it becomes more frustrating.

00:45:03.200 --> 00:45:05.770
So again,
optimization should be something

00:45:05.770 --> 00:45:09.740
that is both at the beginning of your
project as well as an afterthought

00:45:09.740 --> 00:45:12.780
of once you've finished your project,
how do you get more

00:45:12.780 --> 00:45:15.980
performance out of it?

00:45:16.350 --> 00:45:18.700
You know,
the other thing I wanted to mention

00:45:18.730 --> 00:45:24.090
is that the G5 PowerPC processor
is a very unique architecture,

00:45:24.160 --> 00:45:27.940
much different than the G4,
as Sanjay pointed out in

00:45:27.940 --> 00:45:30.100
his session presentation.

00:45:30.130 --> 00:45:32.710
And for that reason,
we want to make sure that you have as

00:45:32.710 --> 00:45:36.100
many resources available for you to
understand what those differences are

00:45:36.100 --> 00:45:37.740
and how to take advantage of that.

00:45:37.770 --> 00:45:41.470
All week, we have been running a G5
optimization lab on the first

00:45:41.540 --> 00:45:43.480
floor in the California room.

00:45:43.480 --> 00:45:46.320
I urge you to visit,
talk to the many engineers that

00:45:46.340 --> 00:45:49.660
have made themselves available,
spending countless hours.

00:45:49.700 --> 00:45:52.240
Monday and Tuesday,
we were there until midnight.

00:45:52.240 --> 00:45:55.700
You'll be able to talk to Sanjay, Nathan,
and several other engineers,

00:45:55.750 --> 00:46:00.480
both from Apple and IBM,
throughout the week.

00:46:00.520 --> 00:46:04.660
As well, we'll have follow-on kitchens
available to you as developers in

00:46:04.660 --> 00:46:11.240
the developer program at Cupertino,
following the developers conference.