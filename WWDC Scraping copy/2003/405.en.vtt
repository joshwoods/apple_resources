WEBVTT

00:00:24.250 --> 00:00:25.500
Hello.

00:00:25.500 --> 00:00:28.980
So I thought as an introduction
to James' session here,

00:00:28.980 --> 00:00:32.040
he's going to talk about the
details of several of our

00:00:32.040 --> 00:00:36.890
APIs including the audio file APIs,
the audio converter APIs.

00:00:36.960 --> 00:00:40.880
I just show a quick demo program
I wrote which shows you how

00:00:40.880 --> 00:00:43.080
this looks from the outside.

00:00:43.080 --> 00:00:45.500
This doesn't use QuickTime.

00:00:45.500 --> 00:00:52.920
It's using all of our native audio
APIs to read and convert audio files.

00:00:52.920 --> 00:00:56.750
So here I've got a few
files I brought with me.

00:00:56.750 --> 00:01:01.120
This is a simple AIFF stereo 16-bit file.

00:01:01.120 --> 00:01:04.190
And I can listen to it.

00:01:11.200 --> 00:01:16.880
So that's just using the
Audio File API to recognize the file,

00:01:16.880 --> 00:01:21.960
read the samples out of it,
and pump it through an audio output unit.

00:01:21.960 --> 00:01:26.570
There's very little actual manipulating
of audio data in this program.

00:01:26.570 --> 00:01:28.470
It's all using our APIs.

00:01:29.100 --> 00:01:33.780
So another thing I can do here,
I can do a simple sample rate conversion.

00:01:33.900 --> 00:01:37.820
These are the same APIs that
QuickTime is based on top of

00:01:37.820 --> 00:01:40.290
in their upcoming version.

00:01:40.350 --> 00:01:45.580
This program also, by the way,
is going to be available as sample code.

00:01:45.580 --> 00:01:50.470
And we can see some of the things the
Audio File API provides in sample here,

00:01:50.470 --> 00:01:52.100
or in Panther here.

00:01:52.100 --> 00:01:55.460
We can see all the file
formats that are supported.

00:01:55.640 --> 00:01:59.590
I'm going to keep this in AIFF.

00:01:59.590 --> 00:02:04.020
I'll just say same as source.

00:02:04.020 --> 00:02:08.100
Actually if I were to choose AIFF here,
I could make it a 24-bit AIFF,

00:02:08.100 --> 00:02:11.190
which doesn't make sense
because it's 16 bits.

00:02:11.190 --> 00:02:12.980
I'll just say same as source.

00:02:12.980 --> 00:02:16.270
And I'll down sample it to 22K.

00:02:16.270 --> 00:02:18.060
And hit convert.

00:02:19.370 --> 00:02:22.130
This is going through our
sample rate converter,

00:02:22.130 --> 00:02:24.840
which is built into our
Audio Converter API.

00:02:24.840 --> 00:02:28.370
This is very heavily
AlteVec optimized code,

00:02:28.370 --> 00:02:34.060
as are all of our integer to
float conversions now in Panther.

00:02:34.060 --> 00:02:38.740
These are good reasons for using our
APIs for your int to float conversions.

00:02:38.740 --> 00:02:48.240
They're heavily optimized for both
G4 and G3 and soon on G5 as well.

00:02:48.240 --> 00:02:49.240
This is my converted file.

00:02:49.290 --> 00:02:52.190
It's going to sound about the same.

00:02:55.800 --> 00:03:01.830
Okay, so another thing you can do with
the Audio Converter is to use it to

00:03:01.830 --> 00:03:07.540
encode and decode formats such as AAC.

00:03:07.540 --> 00:03:14.420
Here I've got an example of a
six-channel AIFF file which one of my

00:03:14.420 --> 00:03:16.800
coworkers authored in Logic a year ago.

00:03:16.800 --> 00:03:20.910
Here it is in its AIFF format,
and I'll leave it playing

00:03:21.590 --> 00:03:24.700
while I convert it to an AAC.

00:03:25.700 --> 00:03:30.310
You might want to turn the volume down
just a little so I can keep talking,

00:03:30.310 --> 00:03:31.090
please.

00:03:32.890 --> 00:03:36.430
Okay,
so I can say I want to convert it to

00:03:36.430 --> 00:03:38.600
AAC and leave it to the same sample rate.

00:03:38.600 --> 00:03:42.190
There's some more parameters you
can put on the converter here,

00:03:42.190 --> 00:03:44.760
such as the bit rate, the quality.

00:03:44.760 --> 00:03:49.240
You can do channel remapping
in multi-channel AAC,

00:03:49.260 --> 00:03:50.760
for example.

00:03:50.760 --> 00:03:58.390
Depending on the channel layout,
it can save bits by putting a

00:03:58.390 --> 00:04:02.750
filter on the sub or LFE channel.

00:04:03.300 --> 00:04:05.970
So this example program
doesn't show that right now,

00:04:05.990 --> 00:04:08.270
but that is an option that
there is on the converter.

00:04:08.280 --> 00:04:13.980
So while it keeps playing,
you can turn the volume back up now.

00:04:24.800 --> 00:04:26.520
So this is a pretty substantial file.

00:04:26.520 --> 00:04:30.110
It's 40 megabytes as an AIFF.

00:04:33.370 --> 00:04:36.930
So we're encoding that
six-channel AAC file in a

00:04:36.930 --> 00:04:39.360
little less than real time here.

00:04:39.420 --> 00:04:41.360
It's

00:04:43.210 --> 00:04:46.280
And so here we have,
if we go to the finder,

00:04:46.280 --> 00:04:50.870
we'll see now it's a 3.1 megabyte file.

00:04:50.890 --> 00:04:54.010
And it's also in surround.

00:05:05.470 --> 00:05:06.100
This is kind of cool.

00:05:06.100 --> 00:05:08.480
I'm scrubbing around inside the AAC file.

00:05:08.480 --> 00:05:11.410
It's decoding multiple packets.

00:05:16.940 --> 00:05:18.540
Really nice and efficient.

00:05:18.540 --> 00:05:22.920
By the way,
our AAC decoder in our test is about

00:05:23.010 --> 00:05:26.830
three times as efficient as MP3 decoding.

00:05:26.900 --> 00:05:31.380
So if you're considering
putting sounds in games,

00:05:31.380 --> 00:05:35.900
you might take a close look at
using AAC encoding instead of MP3.

00:05:36.730 --> 00:05:39.340
Okay, so that's the sample program,
and I'll turn it back over to

00:05:39.360 --> 00:05:42.670
James to explain the APIs that
are underneath this program.

00:05:42.690 --> 00:05:44.700
Thank you.

00:05:44.780 --> 00:05:51.990
Okay, back to slides, please.

00:05:57.350 --> 00:05:59.300
Slides.

00:05:59.300 --> 00:06:03.740
Okay, there we go.

00:06:03.740 --> 00:06:06.690
Okay,
so what I'm going to talk about is how

00:06:06.690 --> 00:06:08.400
to handle audio formats with Core Audio.

00:06:08.400 --> 00:06:13.060
First I'll sort of review
the basics of how formats are

00:06:13.060 --> 00:06:14.900
represented in Core Audio.

00:06:14.900 --> 00:06:19.440
Then I'll talk about the Audio Converter
API and the Audio Format API,

00:06:19.440 --> 00:06:21.260
which is new in Panther.

00:06:21.260 --> 00:06:24.080
And I'll talk about some new
features for audio units for

00:06:24.080 --> 00:06:26.400
supporting multi-channel and surround.

00:06:27.320 --> 00:06:33.460
And I'll talk about the new
Audio File Get Global Info API and

00:06:33.460 --> 00:06:36.210
the new Matrix Mixer Audio Unit.

00:06:37.020 --> 00:06:42.520
Okay, so one thing that's been a source
of some confusion in Core Audio is

00:06:42.680 --> 00:06:46.720
what is the definition of frames
and packets and bytes and samples.

00:06:46.950 --> 00:06:49.350
And when you're dealing
with compressed formats,

00:06:49.350 --> 00:06:53.160
it gets pretty important to
take them all into account.

00:06:53.160 --> 00:07:02.280
So this graphic shows what a five-channel
interleaved 24-bit stream looks like.

00:07:02.280 --> 00:07:06.340
You can see that one sample is
three bytes and five channels

00:07:06.340 --> 00:07:08.760
are interleaved into one frame.

00:07:08.760 --> 00:07:15.550
And for linear PCM,
one frame equals one packet in the way

00:07:15.550 --> 00:07:20.640
we count things in the Core Audio APIs.

00:07:20.640 --> 00:07:27.980
And this information is the way you
describe a format is by specifying

00:07:27.980 --> 00:07:32.300
an audio stream basic description,
which is a structure that's used

00:07:32.300 --> 00:07:34.040
throughout the Core Audio API.

00:07:34.040 --> 00:07:40.220
It has the sample rate for
the stream and the format ID,

00:07:40.220 --> 00:07:48.380
which tells you whether it's a PCM stream
or it's some kind of compressed format.

00:07:48.380 --> 00:07:53.090
That's the format ID as a
four-character code for the format.

00:07:54.400 --> 00:07:59.260
Then there's flags that are specific to
that format and some fields that tell

00:07:59.260 --> 00:08:02.330
you the relationship between the bytes,
the packets,

00:08:02.330 --> 00:08:06.560
and the frames in that format and
the number of channels in that

00:08:07.120 --> 00:08:10.930
format and how many bits per channel.

00:08:11.610 --> 00:08:13.690
and David Koehn,
and I'm going to be talking

00:08:13.690 --> 00:08:14.740
about the different ways that
each sample is in that format.

00:08:14.740 --> 00:08:18.400
Okay, so here's an example of how to
fill one of these out for the

00:08:18.760 --> 00:08:24.420
five-channel 24-bit interleave stream
that was in the first graphic there.

00:08:24.420 --> 00:08:27.740
The format ID is linear PCM.

00:08:27.740 --> 00:08:32.920
The flags are set to
Big Indian signed integer packed.

00:08:32.920 --> 00:08:34.260
It's one frame per packet.

00:08:34.430 --> 00:08:39.920
All linear PCM is one frame per packet.

00:08:39.920 --> 00:08:43.150
And then for this interleave stream,
there's 15 bytes per packet

00:08:43.150 --> 00:08:47.110
and 15 bytes per frame,
five channels per frame,

00:08:47.110 --> 00:08:48.930
24 bits per channel.

00:08:49.550 --> 00:08:54.630
Okay, so this shows an example of
a non-interleaved stream.

00:08:54.760 --> 00:08:56.410
There's two buffers.

00:08:56.540 --> 00:08:59.740
Each of them holds
floating point samples.

00:09:00.060 --> 00:09:02.700
They're four bytes each.

00:09:02.700 --> 00:09:06.900
You see on the left there's a

00:09:07.290 --> 00:09:10.280
and I are going to be talking
about the different types of

00:09:10.500 --> 00:09:12.040
audio buffers that we use.

00:09:12.040 --> 00:09:18.330
We have these names, mbuffers,
subzero.mdata, that's fields in the audio

00:09:18.780 --> 00:09:22.110
buffer list structure,
which I'll show in a minute.

00:09:23.720 --> 00:09:25.850
Samples from one channel of the data.

00:09:25.910 --> 00:09:30.240
So this non-intermediate
stream has two channels,

00:09:30.250 --> 00:09:31.470
and so there's two buffers.

00:09:33.230 --> 00:09:37.760
Okay, and then here's the
Audio Stream Basic description for that.

00:09:37.760 --> 00:09:41.080
The one difference you'll see here, well,
for one thing, there's the flag for

00:09:41.090 --> 00:09:44.000
non-interleaved is set,
the format flag.

00:09:44.000 --> 00:09:46.280
And another difference you'll
see here is that bytes per packet

00:09:46.280 --> 00:09:52.380
and bytes per frame is four,
describes one of those buffers.

00:09:52.380 --> 00:09:55.680
So even though there's two channels,
it's not two times four bytes.

00:09:55.720 --> 00:10:00.810
It's just four bytes per packet
because they're split into two buffers.

00:10:01.820 --> 00:10:05.260
Okay, so now when you get
into compressed formats,

00:10:05.260 --> 00:10:09.200
the simplest kind of compressed
format is a constant bitrate format.

00:10:09.340 --> 00:10:13.930
This is, there's a constant number
of bytes per packet.

00:10:14.200 --> 00:10:17.050
The number of frames per packet
depends on the compressed

00:10:17.050 --> 00:10:20.760
format that you're dealing with.

00:10:21.380 --> 00:10:25.520
So the other kind will be
a variable bit rate data,

00:10:25.520 --> 00:10:30.400
and that's the number of
bytes per packet can vary.

00:10:30.490 --> 00:10:36.020
And in the AudioStream Basic Description,
the m bytes per packet is just set to

00:10:36.020 --> 00:10:38.890
zero because it's not a constant value.

00:10:39.710 --> 00:10:46.510
Now when you're dealing with some
formats like AAC that don't have

00:10:46.960 --> 00:10:50.090
information in the bitstream about
where the packet boundaries are,

00:10:50.090 --> 00:10:55.240
you need something,
an external piece of data to tell you

00:10:55.240 --> 00:10:56.680
where those packet boundaries are.

00:10:56.920 --> 00:11:02.770
And we use the Audio Stream Packet
Description in our APIs to tell you what

00:11:02.770 --> 00:11:08.560
the starting byte offset of a packet is
and the length in bytes of that packet.

00:11:08.560 --> 00:11:13.010
And when you're passing around AAC data,
you have to pass around an array of

00:11:13.010 --> 00:11:17.580
these packet descriptions to tell
you where the packet boundaries are.

00:11:20.670 --> 00:11:24.850
Okay, now,
audio data is stored in Audio Buffer

00:11:24.920 --> 00:11:28.440
Lists in Core Audio throughout our APIs.

00:11:28.440 --> 00:11:32.380
This is in the Audio Units,
the Audio Converter, and the How.

00:11:32.380 --> 00:11:37.630
The Audio Buffer List tells
you the number of buffers,

00:11:37.630 --> 00:11:41.160
and then there's an array
of buffer structures,

00:11:41.160 --> 00:11:44.600
and each buffer tells you
how many interleaved channels

00:11:44.600 --> 00:11:49.420
there are in that buffer,
and the size of the buffer,

00:11:49.420 --> 00:11:52.590
and then there's a pointer to the buffer.

00:11:53.880 --> 00:11:55.990
Okay,
all these structures that I'm talking

00:11:55.990 --> 00:12:02.640
about and some I'll talk about later
are defined in CoreAudioTypes.h.

00:12:02.720 --> 00:12:07.750
They're used everywhere in Core Audio,
so they're

00:12:08.060 --> 00:12:13.440
and I are the co-founders of the
Core Audio and there's public utility

00:12:13.440 --> 00:12:18.950
classes that are shipped with the
SDK that provide common operations on

00:12:18.950 --> 00:12:24.590
these structures and make it easier to
fill them out and do the things that

00:12:24.590 --> 00:12:27.320
you commonly need to do with them.

00:12:27.850 --> 00:12:31.900
Okay, so in Audio Units,
I'm going to talk about how

00:12:31.900 --> 00:12:39.460
frames and packets are used by
the various core audio APIs.

00:12:39.460 --> 00:12:42.830
Audio Units express their
buffer sizes in frames.

00:12:42.840 --> 00:12:47.250
By default, the format is 32-bit float,
non-interleaved.

00:12:47.320 --> 00:12:51.980
People have sort of gotten the
impression that that's the only

00:12:51.980 --> 00:12:57.870
format you could do with Audio Units,
but since there is a stream description

00:12:57.880 --> 00:13:02.100
that you can set on the inputs
and outputs of the Audio Units,

00:13:02.100 --> 00:13:10.820
you can actually put any kind of
audio data into those Audio Units.

00:13:13.150 --> 00:13:21.160
Okay, the HAL counts time in frames when
you get your I/O proc callback,

00:13:21.370 --> 00:13:26.000
the sample time is
provided there in frames.

00:13:26.000 --> 00:13:34.000
In PCM mode, the HAL tells you its
buffer size in frame.

00:13:34.000 --> 00:13:39.170
The buffer sizes are set in frames
and unless you tell it otherwise,

00:13:39.170 --> 00:13:42.460
the format will be 32-bit float.

00:13:43.110 --> 00:13:49.650
In non-linear PCM mode,
you're restricted to dealing one

00:13:49.650 --> 00:13:52.990
packet at a time with the data.

00:13:53.310 --> 00:13:59.980
The buffer frame size range is restricted
to the number of frames per packet.

00:14:03.340 --> 00:14:09.630
Okay, so for the Audio File API,
there's two calls,

00:14:09.870 --> 00:14:12.800
Audio File Read Bytes and Write Bytes,
Deals and Bytes,

00:14:12.800 --> 00:14:15.310
and then Audio File Read Packets
and Write Packets,

00:14:15.310 --> 00:14:16.430
Deals and Packets.

00:14:16.460 --> 00:14:20.650
Since frames equals packets for PCM,
then you can use

00:14:20.790 --> 00:14:26.200
Audio File Read Packets for PCM,
and that'll return you PCM frames.

00:14:26.740 --> 00:14:31.400
If you're dealing with compressed data,
then Read Packets will return

00:14:31.400 --> 00:14:33.900
you some number of packets
of that compressed data.

00:14:35.370 --> 00:14:38.700
Okay, so now you can describe
different formats.

00:14:38.700 --> 00:14:41.860
How do you convert from
one format to another?

00:14:41.860 --> 00:14:46.810
And for that we have the Audio Converter.

00:14:47.230 --> 00:14:51.730
It can do floating point integer,
various bit depth,

00:14:51.830 --> 00:14:55.090
sample rate conversion, interleaving,
deinterleaving, channel reordering,

00:14:55.090 --> 00:15:01.420
and new in Panther is that it can convert
between PCM and compressed formats for

00:15:01.420 --> 00:15:04.950
codecs that are installed in the system.

00:15:05.870 --> 00:15:09.160
In order to create an audio converter,
you use Audio Converter New.

00:15:09.160 --> 00:15:13.960
You give it an input and output format,
and it returns you an

00:15:13.960 --> 00:15:17.910
Audio Converter Ref,
which is your audio converter object.

00:15:17.940 --> 00:15:22.410
So in order to call it here,
you would fill out two

00:15:22.440 --> 00:15:25.440
audio stream descriptions.

00:15:25.440 --> 00:15:29.660
You can use CA Stream Basic Description
to help you do that,

00:15:29.660 --> 00:15:31.080
the SDK class.

00:15:32.340 --> 00:15:34.140
Then you have a decoder.

00:15:34.140 --> 00:15:38.500
Well, in this example,
I'm just showing a decoder,

00:15:38.500 --> 00:15:40.480
creating a decoder.

00:15:40.480 --> 00:15:44.180
I do Audio Converter New,
and I'll get my decoder

00:15:44.180 --> 00:15:46.320
instance out from that.

00:15:46.320 --> 00:15:51.300
In order to, say, after you've created
your audio converter,

00:15:51.300 --> 00:15:54.140
you want to convert audio with it.

00:15:54.200 --> 00:15:57.820
So you call Audio Converter
Fill Complex Buffer.

00:15:58.990 --> 00:16:03.840
That is a call that takes an
audio converter instance in.

00:16:03.840 --> 00:16:09.200
It takes a pointer to an input procedure,
which is the data source for getting

00:16:09.530 --> 00:16:11.920
input into the audio converter.

00:16:11.920 --> 00:16:19.540
A user data field for storing your
instance data for the audio converter.

00:16:19.540 --> 00:16:25.180
And then there's IO Output Data Packet
Size is the number of packets you want

00:16:25.250 --> 00:16:28.040
to get out of the audio converter.

00:16:28.040 --> 00:16:28.860
And on return,
you'll see the number of packets you

00:16:28.860 --> 00:16:28.860
want to get out of the audio converter.

00:16:28.880 --> 00:16:30.920
And on return, it will be the number of
packets you actually got out.

00:16:30.980 --> 00:16:36.210
That number could be less than what
you requested if you're at the end

00:16:36.210 --> 00:16:40.450
of the stream or there was an error.

00:16:41.160 --> 00:16:48.090
Then you pass in the audio buffer list
which you want to be the audio converter

00:16:48.580 --> 00:16:52.100
to write your converted data into.

00:16:52.100 --> 00:16:58.100
And then there's a pointer to
an array of packet descriptions.

00:16:58.100 --> 00:17:03.180
If you're converting AAC and you're
asking for some number of packets of AAC,

00:17:03.180 --> 00:17:05.530
you need to pass in an array of
packet descriptions so you'll

00:17:05.530 --> 00:17:10.100
know where the packet boundaries
of the data you get back is.

00:17:10.680 --> 00:17:14.050
Okay, so...

00:17:14.510 --> 00:17:18.090
In order to call Audio Converter
Fill Complex Buffer,

00:17:18.090 --> 00:17:19.860
you need to prepare
an output buffer list.

00:17:19.880 --> 00:17:23.410
If it's interleaved data,
then there will be one buffer

00:17:23.410 --> 00:17:25.380
in the buffer list array.

00:17:25.380 --> 00:17:30.530
And if it's non-interleaved,
you'll need multiple mono buffers.

00:17:30.540 --> 00:17:35.620
The mdata pointer contains the
pointer to the buffers that will

00:17:35.620 --> 00:17:37.850
have the audio data written into it.

00:17:37.940 --> 00:17:43.180
And the data byte size tells
the size of the buffer.

00:17:43.180 --> 00:17:49.670
And if you ask for more packets
that will fit into that size,

00:17:49.670 --> 00:17:54.010
it will get truncated down to whatever
the space that you provided to it.

00:17:55.660 --> 00:17:59.360
So in order to call Audio Converter
Fill Complex Buffer,

00:17:59.360 --> 00:18:03.280
basically this shows just
passing the arguments.

00:18:03.280 --> 00:18:07.860
There's the decoder,
the input procedure pointer,

00:18:07.860 --> 00:18:11.250
the user data,
the packet list I'm requesting,

00:18:11.450 --> 00:18:14.870
which is 8192 in this case,
and a buffer list,

00:18:14.930 --> 00:18:16.740
which I have filled out.

00:18:17.180 --> 00:18:21.320
And then I'm passing here null
for the packet descriptions

00:18:21.320 --> 00:18:25.540
because I'm probably dealing with,
well, I'm decoding,

00:18:25.540 --> 00:18:28.740
so I'm getting PCM out,
so I don't need packet

00:18:28.740 --> 00:18:30.990
descriptions in this case.

00:18:31.000 --> 00:18:38.500
So now when you use an audio converter,
you need to write an input procedure,

00:18:38.610 --> 00:18:46.170
and the input procedure implements
the source of the demand-driven model.

00:18:46.180 --> 00:18:47.040
So the audio converter.

00:18:47.630 --> 00:18:54.460
When you ask it for converted data,
it will call your input proc to get data,

00:18:54.460 --> 00:19:01.960
the input side data,
and it's demand-driven so that if

00:19:01.960 --> 00:19:07.750
you're doing a sample rate conversion,
then it might be pulling

00:19:08.420 --> 00:19:13.780
data at a different rate
than you're getting it out.

00:19:13.820 --> 00:19:21.340
And also there's... some internal
buffering that can happen for doing

00:19:21.340 --> 00:19:28.380
compression or sample rate conversion so
that the pulling needs to be decoupled

00:19:28.470 --> 00:19:32.860
from the... the pulling of input needs to
be decoupled from the pulling of output,

00:19:32.860 --> 00:19:36.390
and that's what the
input proc implements.

00:19:36.980 --> 00:19:41.060
So you need to set up your buffer list.

00:19:41.320 --> 00:19:46.140
Inside your input proc,
your job is to provide data

00:19:46.140 --> 00:19:47.140
to the audio converter.

00:19:47.140 --> 00:19:51.100
You don't have to copy data
into Audio Converter's buffers.

00:19:51.100 --> 00:19:53.840
You just give the audio
converter pointers to your data.

00:19:53.840 --> 00:19:59.170
And so it passes you a buffer
list and you fill out that buffer

00:19:59.170 --> 00:20:02.240
list with the pointers to the
data that it wants to convert.

00:20:02.880 --> 00:20:08.180
The Audio Converter in general goes
out of its way to eliminate copying

00:20:08.190 --> 00:20:12.440
and so it tries to just convert.

00:20:12.610 --> 00:20:16.110
I mean, if it can,
it will just convert from your

00:20:16.110 --> 00:20:22.700
input buffer and into your output
buffer without buffering internally.

00:20:22.700 --> 00:20:28.290
In certain cases, it cannot do that.

00:20:28.820 --> 00:20:32.880
In the input proc,
you provide pointers to your data,

00:20:32.880 --> 00:20:35.800
not copy it.

00:20:36.080 --> 00:20:39.690
And you need, in your input proc,
when you pass data to

00:20:39.740 --> 00:20:43.590
the audio converter,
you have to keep that data valid until

00:20:43.590 --> 00:20:46.020
the next time your input proc is called.

00:20:46.020 --> 00:20:50.640
And that might be across calls to
Audio Converter Fill Complex Buffer.

00:20:50.640 --> 00:20:53.620
So you may call Audio Converter
Fill Complex Buffer.

00:20:53.620 --> 00:20:56.240
It calls your input proc,
which returns data to

00:20:56.240 --> 00:20:57.580
the audio converter.

00:20:57.580 --> 00:21:01.200
And then you exit Audio Converter
Fill Complex Buffer,

00:21:01.200 --> 00:21:03.140
exit and returns you data.

00:21:03.750 --> 00:21:08.100
You still have to keep that input data
live until the next time you've called

00:21:08.100 --> 00:21:10.860
Audio Converter Fill Complex Buffer
and it's called your input proc.

00:21:10.880 --> 00:21:14.100
Because it's still looking at that data.

00:21:15.880 --> 00:21:22.510
Now, your input proc gets past the
number of packets that the

00:21:22.510 --> 00:21:29.360
Audio Converter wants from you,
but you're allowed to return either more

00:21:29.360 --> 00:21:31.100
or less data than it's asked you for.

00:21:31.100 --> 00:21:35.500
If you return less,
then it will be called again.

00:21:35.500 --> 00:21:39.100
The word again has been removed here.

00:21:40.820 --> 00:21:46.820
If you return more, then it will just ask
you less frequently.

00:21:46.820 --> 00:21:48.820
It will just ask you the
next time it needs data.

00:21:48.820 --> 00:21:54.820
The input proc,
you have an Audio Converter instance,

00:21:54.820 --> 00:21:58.820
and there's the number of data packets.

00:21:58.820 --> 00:22:02.820
On input, it's the number of data packets
that have been requested,

00:22:02.820 --> 00:22:05.820
and on output,
you return the number of data packets

00:22:05.820 --> 00:22:07.820
that you're actually returning.

00:22:07.820 --> 00:22:09.820
It passes you a buffer list,
which you need to fill

00:22:09.820 --> 00:22:09.820
out for the output.

00:22:09.920 --> 00:22:16.750
And then there's the
Audio Stream Packet Description.

00:22:16.860 --> 00:22:22.530
If you're returning AAC data,
then you need to set this

00:22:22.530 --> 00:22:27.020
pointer to the array of packet
descriptions that describe the

00:22:27.020 --> 00:22:29.270
packet boundaries of the data.

00:22:29.510 --> 00:22:32.220
And then there's the user
data that's passed in,

00:22:32.220 --> 00:22:38.360
which is your own instance data for use,
however you want to use

00:22:38.360 --> 00:22:39.320
it in your input proc.

00:22:39.320 --> 00:22:46.620
There's a couple special conditions that
you have to deal with in your input proc.

00:22:46.840 --> 00:22:50.360
One is when you reach the end of
the stream and you're out of data,

00:22:50.360 --> 00:22:54.990
what you need to do is set the
number of data packets you're

00:22:54.990 --> 00:22:57.660
returning to zero and return no error.

00:22:58.400 --> 00:23:02.540
Your input proc may be called
several more times and you

00:23:02.540 --> 00:23:03.840
just keep returning zero.

00:23:03.900 --> 00:23:08.150
And that will signal to the
Audio Converter that you're indeed out

00:23:08.150 --> 00:23:10.360
of data and it will flush its buffers.

00:23:10.910 --> 00:23:16.000
Another situation you could be in is
that if you're doing real-time streaming,

00:23:16.000 --> 00:23:19.160
you may be in a situation where you're
not at the end of the stream but you

00:23:19.170 --> 00:23:21.140
don't have any data available right now.

00:23:21.280 --> 00:23:26.360
So the Audio Converter needs to just
return whatever it's got converted.

00:23:27.400 --> 00:23:32.900
But what you do in that case is
you return no packets available

00:23:32.900 --> 00:23:34.400
and you return an error.

00:23:34.450 --> 00:23:38.740
And this error gets propagated back
to the caller and any data that

00:23:38.740 --> 00:23:43.740
had been converted up to that point
will be returned to the caller.

00:23:43.740 --> 00:23:49.240
But the Audio Converter will keep any
unconverted data that it has internally

00:23:49.240 --> 00:23:54.190
until the next time Audio Converter
fill complex buffer is called.

00:23:54.600 --> 00:25:36.900
[Transcript missing]

00:25:37.220 --> 00:25:41.690
So for Audio Stream Basic Description
Operations with the Audio Format API,

00:25:41.690 --> 00:25:43.380
you can get a format's name.

00:25:43.380 --> 00:25:45.100
You pass an
Audio Stream Basic Description

00:25:45.100 --> 00:25:50.340
to the Audio Format API,
and it will generate a name,

00:25:50.340 --> 00:25:55.670
either a name for a compressed format,
or if you pass it a

00:25:55.750 --> 00:26:00.950
certain linear PCM format,
it will tell you, generate a CFString and

00:26:00.950 --> 00:26:02.090
tell you what that is.

00:26:04.520 --> 00:26:10.100
You can also pass it a partially filled
out Audio Stream Basic Description.

00:26:10.100 --> 00:26:16.930
That's mostly useful for constant bit
rate data formats to find out what

00:26:17.040 --> 00:26:21.350
the bytes per packet is for IMA4,
for example.

00:26:21.360 --> 00:26:26.980
So you can ask,
is a format variable bit rate?

00:26:26.980 --> 00:26:29.100
Is it externally framed?

00:26:29.100 --> 00:26:34.500
You can ask what encoders
I have installed on the system.

00:26:34.520 --> 00:26:39.370
and what decoders I have
installed on the system.

00:26:40.820 --> 00:26:43.910
Okay, so this is what the
Audio Format API looks like.

00:26:44.040 --> 00:26:45.240
There's two calls.

00:26:45.240 --> 00:26:48.500
One is the Audio Format
Get Property Info.

00:26:48.500 --> 00:26:52.260
You use that to find out
the size of a property.

00:26:52.260 --> 00:26:57.480
You pass in a specifier,
which is some argument to the property.

00:26:57.480 --> 00:27:01.700
There's a property ID, which tells the
Audio Format API what API,

00:27:01.800 --> 00:27:03.980
what property you're asking about.

00:27:03.980 --> 00:27:06.470
And then there's the specifier,
which gives an argument.

00:27:06.470 --> 00:27:10.600
And then it returns you the size
of the property you're asking for.

00:27:10.600 --> 00:27:15.260
And then there's the Get Property call,
which you use to actually get

00:27:15.260 --> 00:27:17.300
the value of the property.

00:27:19.290 --> 00:27:24.890
All right,
so here's an example of using it.

00:27:25.320 --> 00:27:29.900
Here I'm finding out what encoders
are installed in the system.

00:27:29.900 --> 00:27:37.540
I make a get property info call to find
out the size of the array of encoders.

00:27:37.540 --> 00:27:43.990
It's going to return me a list of
OS types for these encode formats.

00:27:44.010 --> 00:27:49.590
Then I call audio format get property
to get the array of format IDs.

00:27:50.090 --> 00:27:54.420
Then I enter in a loop here
where I call the format API to

00:27:54.420 --> 00:27:56.950
give me a name for that format.

00:27:57.000 --> 00:28:00.430
I create a small audio stream
based description and get a name

00:28:00.440 --> 00:28:02.480
for the format and print them out.

00:28:02.890 --> 00:28:07.720
Then on my system this is
what I got printed out.

00:28:08.000 --> 00:28:17.200
[Transcript missing]

00:28:17.380 --> 00:28:19.860
describes the channel
ordering of a stream.

00:28:19.920 --> 00:28:23.810
Now, Audio Stream Basic Description tells
you the number of channels in a stream,

00:28:23.810 --> 00:28:25.160
but it doesn't really
tell you what they are.

00:28:25.160 --> 00:28:29.540
So,
if you have one channel or two channels,

00:28:29.540 --> 00:28:31.260
you can pretty much guess
that's mono or stereo.

00:28:31.260 --> 00:28:35.760
But if you have five channels,
it could be one of

00:28:35.760 --> 00:28:39.540
several orderings of 5.0.

00:28:39.540 --> 00:28:42.320
If you have six channels,
it could be 6.0 or it could be 5.1

00:28:42.320 --> 00:28:43.820
in several different orderings.

00:28:43.820 --> 00:28:47.280
So,
you need a way to find out what that is.

00:28:47.300 --> 00:28:48.910
on this.

00:28:49.070 --> 00:28:55.280
The Audio Channel Layout has several
ways of specifying orderings.

00:28:55.380 --> 00:28:59.000
There's an integer tag for a
bunch of predefined layouts.

00:28:59.000 --> 00:29:01.720
In a lot of cases,
you can just pass around these

00:29:01.720 --> 00:29:06.200
integer tags to tell you what
channel ordering you have.

00:29:06.200 --> 00:29:10.990
There's a bitmap for
USB WAV style layouts.

00:29:11.000 --> 00:29:14.720
So in WAV files or USB,
there's a bitmap to tell you which

00:29:14.800 --> 00:29:17.540
channels are present in a stream,
and then they have to be

00:29:17.670 --> 00:29:18.700
present in a certain order.

00:29:19.000 --> 00:29:22.980
And then there's an array of
channel descriptions which you can

00:29:23.050 --> 00:29:25.500
use to describe arbitrary layouts.

00:29:25.500 --> 00:29:28.380
So the structure looks like this.

00:29:28.550 --> 00:29:33.700
There's a channel layout tag,
which is one of these predefined layouts.

00:29:33.950 --> 00:29:35.300
There's a bitmap.

00:29:35.550 --> 00:29:39.300
And then there's an array
of channel descriptions.

00:29:40.710 --> 00:29:45.400
Lots of formats are predefined.

00:29:45.400 --> 00:29:54.230
We defined just about everything we
could think of or find references to.

00:29:54.920 --> 00:29:57.760
And then there's one thing you
can do with these integer tags

00:29:57.760 --> 00:30:00.240
is mask off the low 16 bits,
and that will tell you the

00:30:00.240 --> 00:30:04.590
number of channels in the format.

00:30:04.730 --> 00:30:11.740
The hold tag will be more specific and
tell you what kind of ordering there is.

00:30:11.740 --> 00:30:13.060
There's two special tags.

00:30:13.060 --> 00:30:17.360
One is used channel descriptions,
which means that you can't know

00:30:17.360 --> 00:30:18.770
anything from looking at the tag.

00:30:18.770 --> 00:30:21.300
You have to look at the array
of channel descriptions to find

00:30:21.300 --> 00:30:23.060
out what channels are present.

00:30:23.060 --> 00:30:26.690
And then there's used channel bitmaps,
or if you're dealing

00:30:26.690 --> 00:30:28.790
with a USB Wave style.

00:30:28.970 --> 00:30:33.800
and I are going to be talking
about the channel layout.

00:30:33.800 --> 00:30:39.690
You can see here that there's four
different kinds of layouts for 5.1.

00:30:40.050 --> 00:30:45.820
This sort of illustrates the kind of
problem that this is trying to solve.

00:30:46.120 --> 00:30:51.300
Various streams will be in
different ones of these formats.

00:30:51.300 --> 00:30:53.800
You have to be able
to differentiate them.

00:30:55.130 --> 00:30:59.340
The channel description struct for
the array of channel descriptions.

00:30:59.340 --> 00:31:01.690
There's a channel label that
tells you whether it's left,

00:31:01.820 --> 00:31:02.600
right, left, or around.

00:31:02.600 --> 00:31:06.930
And then there's some
optional coordinates.

00:31:06.990 --> 00:31:09.510
So if you wanted to specify
speaker positions using

00:31:10.230 --> 00:31:13.000
floating point coordinates,
you can do that in rectangular

00:31:13.000 --> 00:31:14.060
or spherical coordinates.

00:31:14.060 --> 00:31:17.010
Okay, the channel label is an integer.

00:31:17.050 --> 00:31:21.810
It tells you just basically which
channel you're dealing with.

00:31:21.890 --> 00:31:27.800
There's basic ones and then
there's lots of more esoteric ones.

00:31:27.800 --> 00:31:31.110
This is sort of channels,
some of the channels defined

00:31:31.160 --> 00:31:34.720
by the theater industry along
with my favorite channel,

00:31:34.720 --> 00:31:37.380
the left surround direct channel,
LSD channel.

00:31:37.420 --> 00:31:42.010
The audio channel layout operations,
you can get the number

00:31:42.010 --> 00:31:44.060
of channels in a layout.

00:31:44.060 --> 00:31:50.060
You can get a full description
from a layout tag or a bitmap.

00:31:50.060 --> 00:31:56.190
So if you have one of these
integer tags like 5.1a,

00:31:56.190 --> 00:32:00.120
you can have it give you an array
of the channel descriptions telling

00:32:00.120 --> 00:32:04.480
you what the channel label is all
filled out so you know which channel

00:32:04.480 --> 00:32:06.040
is where in a certain layout.

00:32:06.060 --> 00:32:11.710
You can get a matrix of coefficients
for using with the matrix mixer,

00:32:11.710 --> 00:32:14.060
which I'll go over in a bit,
for doing downmixing.

00:32:14.060 --> 00:32:16.300
So you can get a matrix of coefficients
for using with the matrix mixer,

00:32:16.300 --> 00:32:18.060
which I'll go over in a bit,
for doing downmixing.

00:32:18.270 --> 00:32:21.710
You can get a name for a layout.

00:32:21.710 --> 00:32:25.870
So if you have some layout,
you can pass it here to the audio format

00:32:25.870 --> 00:32:30.060
API and it'll give you a CFString,
which you can print out.

00:32:30.060 --> 00:32:32.060
You can get a name for a channel.

00:32:32.060 --> 00:32:36.060
So if you have a certain channel
and you want to find out a name

00:32:36.060 --> 00:32:38.060
you can print for the user.

00:32:38.410 --> 00:32:44.050
These are localized strings.

00:32:44.060 --> 00:32:48.040
This is what the left channel looks like.

00:32:48.110 --> 00:32:52.560
You can get that and
that'll be localized.

00:32:52.860 --> 00:32:55.470
And then there's a,
you can also put these audio

00:32:55.470 --> 00:33:02.100
channel layouts into AIF files,
so.

00:33:02.540 --> 00:33:06.490
All right, so with Audio Units,
you can get the channel layouts

00:33:06.680 --> 00:33:08.410
and Audio Unit supports.

00:33:08.470 --> 00:33:11.760
This is a new feature for Panther.

00:33:11.800 --> 00:33:14.400
So Audio Units can
support channel layouts,

00:33:14.520 --> 00:33:18.240
and you can get or set a channel
layout for an Audio Unit stream.

00:33:18.240 --> 00:33:23.470
For example,
the Matrix Reverb can have stereo, quad,

00:33:23.470 --> 00:33:27.120
or 5.0 channel layouts as output.

00:33:29.120 --> 00:33:32.300
Okay, you don't have to
support channel layouts.

00:33:32.300 --> 00:33:36.000
If you're doing an Audio Unit that's
only doing mono or stereo,

00:33:36.000 --> 00:33:38.980
or it doesn't really care about
spatial location like a filter,

00:33:38.980 --> 00:33:43.570
then you don't have to
support audio channel layouts.

00:33:43.580 --> 00:33:48.250
But if you're doing a Reverb or a
Panner or something that can deal

00:33:48.280 --> 00:33:53.210
with certain numbers of channels,
like if you're dealing with five

00:33:53.210 --> 00:33:58.780
channels and you want to be able to
support multiple channel orderings,

00:33:59.120 --> 00:34:05.240
you can do that with Audio Units.

00:34:05.700 --> 00:34:08.560
So the Audio Converter also
supports channel layouts.

00:34:08.600 --> 00:34:16.880
You can use the AAC Codec
to support various channel

00:34:16.880 --> 00:34:20.920
layouts when encoding to AAC.

00:34:21.300 --> 00:34:24.410
There's a property for getting
the available encode channel

00:34:24.470 --> 00:34:30.810
layouts and for setting the channel
layouts when you're encoding.

00:34:31.900 --> 00:34:36.380
So there's also the
Audio File Global Info API,

00:34:36.380 --> 00:34:40.730
which is related to Audio Format API,
but it gets information

00:34:40.730 --> 00:34:43.240
about audio files.

00:34:43.240 --> 00:34:46.040
You can ask it what
file types can be read,

00:34:46.040 --> 00:34:47.640
what can be written.

00:34:47.640 --> 00:34:54.020
You can get names for the file types,
and you can find out what stream formats

00:34:54.050 --> 00:34:56.450
a certain file type can have put into it.

00:34:56.500 --> 00:35:02.170
You can find out what file extensions

00:35:02.300 --> 00:35:08.020
and I will be presenting the
first part of this session.

00:35:08.950 --> 00:35:13.730
So it's basically symmetrical
to the Audio Format API.

00:35:13.920 --> 00:35:19.400
There's a property ID, a specifier,
and then for the info you

00:35:19.480 --> 00:35:24.240
get the size of the property,
and for the get property call

00:35:24.240 --> 00:35:29.520
you get the property data itself.

00:35:29.520 --> 00:35:34.670
So here's an example of finding out what
writable file types are on the system.

00:35:34.670 --> 00:35:38.600
It's almost identical to finding
out what encoders are on the system.

00:35:38.600 --> 00:35:46.430
I find out what the size of
the array I'm going to get back

00:35:46.600 --> 00:35:53.110
is using the info size call,
and then I get the array of

00:35:53.110 --> 00:35:57.690
file types that can be read,
and then I go into a loop and I use

00:35:57.690 --> 00:36:03.260
the file type name property to get
a CFString that I can print out.

00:36:03.260 --> 00:36:08.300
And so this is what I get.

00:36:08.300 --> 00:36:08.530
So in Panther Audio File,
I'm going to get the

00:36:08.530 --> 00:36:11.680
file type name property,
and then I'm going to go

00:36:11.680 --> 00:36:17.570
into a loop and I'm going to

00:36:18.960 --> 00:36:26.770
Okay, one new audio unit in
Panther is the Matrix Mixer.

00:36:27.130 --> 00:36:33.430
Audio Unit, it's an audio unit that
can take inputs to,

00:36:33.450 --> 00:36:35.670
or any number of inputs
to any number of outputs,

00:36:35.670 --> 00:36:40.000
and they can be bundled
in streams of any size.

00:36:40.000 --> 00:36:44.550
And the CPU usage depends
only on the number of non-zero

00:36:44.550 --> 00:36:49.100
cross points in the matrix,
not the size of the matrix.

00:36:49.100 --> 00:36:57.520
And you can get metering on inputs,
cross points, and outputs.

00:36:58.690 --> 00:37:03.530
So Matrix Mixer is useful
for signal routing,

00:37:03.850 --> 00:37:07.340
channel reordering, surround down mixing,
generalized panning,

00:37:07.340 --> 00:37:09.730
and generalized mixing.

00:37:10.630 --> 00:37:18.930
All the input buses are flattened
to an array of mono channels for

00:37:18.930 --> 00:37:21.180
all the input and output buses.

00:37:21.180 --> 00:37:25.240
It's a big matrix of mono channels.

00:37:25.240 --> 00:37:29.140
Four gains control each cross point.

00:37:29.140 --> 00:37:32.640
There's a gain on input,
a cross point gain, an output gain,

00:37:32.640 --> 00:37:35.020
and a master gain for the entire matrix.

00:37:35.020 --> 00:37:38.970
This is a...

00:37:39.500 --> 00:37:41.870
This is a game of
Go where black is losing.

00:37:42.610 --> 00:37:47.650
So this shows--

00:37:48.190 --> 00:37:53.000
This shows the input buses coming in.

00:37:53.000 --> 00:37:55.560
They're flattened to a
set of four channels.

00:37:55.600 --> 00:38:01.960
I have stereo buses,
so they're flattened to four channels.

00:38:02.030 --> 00:38:03.600
There's a gain on each input.

00:38:03.600 --> 00:38:10.100
And the channels get numbered across
all the buses in just a linear fashion.

00:38:10.100 --> 00:38:13.540
So they go 0, 1, 2,
3 for addressing each channel.

00:38:13.600 --> 00:38:21.410
And then I'm using a black
circle here to represent a cross

00:38:21.410 --> 00:38:24.600
point that has a non-zero gain.

00:38:24.600 --> 00:38:27.600
And then the open circles show zero gain.

00:38:27.600 --> 00:38:33.600
So here I've got input bus 0 as
being mixed to output buses 0 and 1.

00:38:33.630 --> 00:38:39.600
And input bus 1 is being
mixed just to output bus 0.

00:38:39.670 --> 00:38:41.600
So that's how it's laid out.

00:38:41.610 --> 00:38:43.600
And you're only paying CPU.

00:38:43.600 --> 00:38:46.600
And you're only paying
CPU cost for the black circle.

00:38:46.600 --> 00:38:54.560
So as you turn up more gains,
you'll have more CPU load.

00:38:54.750 --> 00:38:58.490
Okay,
so I'm going to demonstrate that now.

00:39:07.700 --> 00:39:12.740
Okay, so here's the Matrix Mixer,
or just my UI on top of the Matrix Mixer.

00:39:12.740 --> 00:39:19.480
I've got two stereo
input buses coming in,

00:39:19.480 --> 00:39:22.880
and I've got five channels
going out in one bus.

00:39:22.880 --> 00:39:28.630
So if I hit play here,
you can see on the left,

00:39:28.790 --> 00:39:32.430
these are the pre-fader input meters.

00:39:32.460 --> 00:39:35.380
I'll turn up my master gain here.

00:39:37.360 --> 00:39:41.450
So as I turn these input faders on...

00:39:42.790 --> 00:39:48.170
You can see this is the
post fade input meter.

00:39:48.170 --> 00:39:49.740
And I'll turn on these cross point gains.

00:39:49.740 --> 00:39:52.720
So I'm going to map this
channel to channel zero of

00:39:52.720 --> 00:39:55.300
the output and I'll map this.

00:39:55.400 --> 00:40:04.700
[Transcript missing]

00:40:08.300 --> 00:40:16.230
And then I can sort of put this backwards
into the surround channels here.

00:40:21.760 --> 00:40:25.790
Okay, so you can see the metering
on the cross points here and

00:40:25.880 --> 00:40:28.290
the metering on the outputs.

00:40:39.310 --> 00:40:43.800
So I'm mixing another sound in here.

00:40:43.800 --> 00:40:47.710
And I can mix it also
into the surrounds here.

00:40:54.000 --> 00:41:01.000
This is center here.

00:41:01.100 --> 00:41:08.320
Okay, so that shows basically setting
the levels and the metering.

00:41:08.320 --> 00:41:15.080
In addition,
you can enable and disable buses

00:41:15.120 --> 00:41:19.980
and that basically turns off
pulling for that branch of the,

00:41:20.110 --> 00:41:22.220
on that bus of input.

00:41:22.220 --> 00:41:25.580
So basically it's like pausing
a section of your input.

00:41:30.410 --> 00:41:35.410
Yeah, so, um, all right, now, just,
I implemented this so

00:41:35.450 --> 00:41:38.270
I can just demonstrate,
uh,

00:41:38.670 --> 00:41:44.220
You know, you can automate these
programmatically to do any kind of

00:41:44.220 --> 00:41:47.680
panning algorithm you want to do.

00:41:47.850 --> 00:41:52.430
One of the kind of interesting things
you can do with a matrix mixer is

00:41:52.590 --> 00:41:58.610
multi-channel panning in various manners.

00:42:09.900 --> 00:42:11.900
It's just the rear speakers.

00:42:11.910 --> 00:42:16.940
Okay.

00:42:16.940 --> 00:42:21.980
I can also disable the output.

00:42:21.980 --> 00:42:23.970
That just mutes it basically.

00:42:30.630 --> 00:42:32.600
Okay, so that's the Matrix Mixer.

00:42:32.600 --> 00:42:38.790
I'll go back to slides now.

00:42:47.380 --> 00:42:50.280
All right, so in order to set the
gains on the Matrix Mixer,

00:42:50.460 --> 00:42:54.140
you can set all gains from global scope.

00:42:54.290 --> 00:42:59.290
Audio units, when you set parameters,
you set parameters using

00:42:59.290 --> 00:43:02.200
either input scope,
output scope, or global scope.

00:43:04.340 --> 00:43:07.380
With the Matrix Mixer,
you can set everything from global scope,

00:43:07.650 --> 00:43:13.060
although you can set input gains and
output gains from input or output scope.

00:43:13.240 --> 00:43:18.440
But this shows how you would
specify a certain gain in the

00:43:18.440 --> 00:43:21.330
cross points of the matrix.

00:43:21.400 --> 00:43:27.100
You need to specify these
using the element argument

00:43:27.100 --> 00:43:30.170
to audio unit set parameter.

00:43:30.180 --> 00:43:33.870
And you do that by shifting
the input channel left by 16.

00:43:33.910 --> 00:43:43.170
And then you set output channel 2.

00:43:43.340 --> 00:43:48.770
to hex all Fs or four Fs,
or that with the input channel shift

00:43:48.770 --> 00:43:53.620
left 16 and then for the output
gain you do the converse operation

00:43:53.620 --> 00:43:56.260
and then master gain is all Fs.

00:43:56.320 --> 00:43:57.680
Okay.

00:43:57.680 --> 00:44:02.360
So, as you saw you can get metering
on prefade and postfade metering

00:44:02.360 --> 00:44:08.040
on inputs and postfade metering
on outputs and cross points.

00:44:08.040 --> 00:44:10.790
In order to get the metering
you need to set audio unit

00:44:10.940 --> 00:44:14.680
property metering mode to one.

00:44:14.680 --> 00:44:21.100
Metering does take some CPU so
you want to have the option to

00:44:21.100 --> 00:44:26.180
turn it off if you don't need it.

00:44:26.180 --> 00:44:31.610
And so and the parameters for metering
are accessed in the same method as the

00:44:31.610 --> 00:44:38.300
gains by shifting the input left 16 in
the element and ordering with the output.

00:44:42.760 --> 00:44:46.220
Okay,
so the bus enable that I showed you,

00:44:46.220 --> 00:44:51.840
Matrix Mixer parameter bus enable,
if input bus is disabled,

00:44:51.840 --> 00:44:53.540
then it won't be pulled.

00:44:53.540 --> 00:44:57.910
So that's one way you can
use to manage CPU load when

00:44:57.910 --> 00:45:00.900
you're using the Matrix Mixer.

00:45:00.900 --> 00:45:06.190
So if you disable input buses,
then you're basically turning

00:45:06.190 --> 00:45:09.270
that part of your input graph off.

00:45:12.120 --> 00:45:13.990
So, um...

00:45:15.170 --> 00:45:20.100
If you just set the input gain to zero,
that input will still be playing,

00:45:20.100 --> 00:45:21.290
you're just not hearing it.

00:45:21.310 --> 00:45:27.770
So it's just a different way
to do something like that.

00:45:28.100 --> 00:45:30.360
In order to set up the MatrixMixer,
before you can use it,

00:45:30.360 --> 00:45:33.040
you need to set the number of
input and output buses using

00:45:33.040 --> 00:45:36.000
Audio Unit Property Bus Count.

00:45:36.000 --> 00:45:38.450
And you need to set the
number of channels in the

00:45:38.450 --> 00:45:40.140
stream formats of each bus.

00:45:40.140 --> 00:45:47.070
This defines the size of the
matrix so that it can allocate

00:45:47.070 --> 00:45:50.140
itself to the proper size.

00:45:51.040 --> 00:45:57.040
Okay, also in Panther,
there's a new Panner unit,

00:45:57.120 --> 00:46:06.860
as Bill was saying,
a Panner unit class for doing mono,

00:46:06.860 --> 00:46:12.000
stereo, or end channel inputs
to end channel outputs,

00:46:12.000 --> 00:46:16.240
and possibly using audio channel layouts.

00:46:16.900 --> 00:46:22.040
You can use it to do panning
or you can use it to do inline

00:46:22.040 --> 00:46:24.600
faders for channel volumes.

00:46:27.260 --> 00:46:29.160
Alright, so that's about it.

00:46:29.160 --> 00:46:32.170
Wrap up.

00:46:32.300 --> 00:46:35.610
Okay, I think a couple of these are over.

00:46:35.830 --> 00:46:46.850
There's audio and
QuickTime tomorrow morning.

00:46:46.850 --> 00:46:47.000
And, uh, uh, quality documentation.