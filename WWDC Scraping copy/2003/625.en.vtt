WEBVTT

00:00:27.700 --> 00:00:30.770
So welcome to the last
Java session of the week.

00:00:30.790 --> 00:00:33.170
It's maximizing-- yeah.

00:00:33.230 --> 00:00:36.610
Maximizing Java performance for Mac OS X.

00:00:36.630 --> 00:00:38.420
My name is Victor Hernandez.

00:00:38.420 --> 00:00:41.200
This talk will be given by three of us,
Jim Laskey and myself.

00:00:41.210 --> 00:00:43.860
We're from the Java runtime
technologies team.

00:00:43.870 --> 00:00:47.420
And Gerard Ziemski, who is from the
Java platform classes team.

00:00:47.420 --> 00:00:50.070
And we're going to be splitting
the talk into three parts.

00:00:50.280 --> 00:00:53.340
But the goal of the overall talk is
to give you a better understanding

00:00:53.340 --> 00:00:57.700
of why your Java application
performs as it does on Mac OS X.

00:00:58.710 --> 00:01:02.490
Jim is going to be talking about
performance improvements that we've made,

00:01:02.490 --> 00:01:05.250
specifically targeting the G5 processor.

00:01:05.400 --> 00:01:09.600
Then I'll be talking about
performance opportunities that

00:01:09.600 --> 00:01:13.060
have arrived with Java 1.4.1.

00:01:13.200 --> 00:01:18.560
And then Gerard will be talking about
Java graphics performance on Mac OS X.

00:01:18.660 --> 00:01:20.140
And stay tuned
specifically for that part,

00:01:20.140 --> 00:01:22.560
because there's a lot of
great demos to be seen there.

00:01:22.650 --> 00:01:24.750
So we've got a lot of material,
so let's get right to it.

00:01:24.800 --> 00:01:26.630
Here's Jim.

00:01:33.730 --> 00:01:34.450
Thanks.

00:01:34.580 --> 00:01:37.460
So my part of the talk,
I'm going to talk specifically

00:01:37.460 --> 00:01:42.640
about what changes were made to
the Hotspot VM to target the G5,

00:01:42.640 --> 00:01:47.150
which was pretty exciting because
we only got to see some of these new

00:01:47.150 --> 00:01:49.810
machines a few weeks ago and play
around with some of the prototypes.

00:01:49.820 --> 00:01:55.470
First of all, I'll just give an overview
of my section of the talk.

00:01:55.530 --> 00:01:59.300
I want to talk specifically about some
of the details of the G5 to give you

00:01:59.300 --> 00:02:03.280
a sense of what sorts of things that
we could actually exploit on the G5.

00:02:03.920 --> 00:02:07.270
Then I'll do a little performance
comparison between the G4 and the

00:02:07.270 --> 00:02:11.170
G5 to give you some kind of sense,
as well as a benchmark can,

00:02:11.360 --> 00:02:15.590
of what kinds of improvements you
might see in your application.

00:02:15.950 --> 00:02:21.590
And then I'll go into some detail on some
of the changes that we made specifically

00:02:21.590 --> 00:02:24.720
to the Java VM interpreter and the Jitsi.

00:02:24.920 --> 00:02:28.480
And then quickly at the end,
I'll go through a couple of the changes

00:02:28.480 --> 00:02:30.780
that we made to the Hotspot runtime.

00:02:33.780 --> 00:02:37.700
Okay, so what does the G5
mean to Java developers?

00:02:37.740 --> 00:02:41.930
Well, the main thing you should note is,
I guess, or you should understand,

00:02:41.940 --> 00:02:45.280
or it should be obvious to you,
that the G5 is going to make your

00:02:45.340 --> 00:02:48.980
application generally run faster,
and you would expect that

00:02:49.120 --> 00:02:51.200
from a faster processor.

00:02:51.200 --> 00:02:55.630
It's about 40% faster than the
highest-end G4 currently shipping,

00:02:55.630 --> 00:02:57.160
faster bus structure.

00:02:57.190 --> 00:03:02.350
There's also been some architectural
changes to the way the G5

00:03:02.350 --> 00:03:07.180
processor works over the G4,
which actually improves the performance

00:03:07.200 --> 00:03:11.130
of various types of operations,
and very specifically, floating point.

00:03:11.200 --> 00:03:15.780
You'll find that floating point is
faster than that 40%, or typically

00:03:15.780 --> 00:03:20.930
faster than that 40%, projected by just
the change in gigahertz on the machine.

00:03:23.730 --> 00:03:28.740
Now, we could have left the VM alone
and not done anything to it,

00:03:28.880 --> 00:03:32.770
and you would have gotten a gain
in performance running on the G5,

00:03:32.950 --> 00:03:35.700
but we like to tinker,
and there's all these really cool

00:03:35.700 --> 00:03:39.660
instructions on the new processor
that we wanted to take advantage of.

00:03:40.170 --> 00:03:46.130
Specifically, there's the introduction
of 64-bit operations,

00:03:46.660 --> 00:03:53.220
and if you have any longs or long
ints in your Java application,

00:03:53.660 --> 00:03:59.880
we now use a much simpler and quicker set
of instructions to do those operations,

00:03:59.880 --> 00:04:04.870
and I'll go into some detail
on what was actually done.

00:04:05.100 --> 00:05:05.800
[Transcript missing]

00:05:07.290 --> 00:05:10.130
And finally,
the main thing that you can walk away

00:05:10.140 --> 00:05:14.580
from this session feeling is that
you don't have to do anything to your

00:05:14.580 --> 00:05:19.400
application to gain these benefits,
improvements in performance.

00:05:19.510 --> 00:05:21.160
We've modified the VM.

00:05:21.240 --> 00:05:28.100
As soon as you run your
application on a G5,

00:05:28.100 --> 00:05:31.790
you're going to gain all the
benefits of having 64-bit arithmetic,

00:05:31.900 --> 00:05:33.540
the faster processing yourself.

00:05:33.630 --> 00:05:37.400
So you have to make no
changes to your application.

00:05:39.190 --> 00:05:43.210
Now, just to start it off,
I want to show you some comparisons

00:05:43.250 --> 00:05:49.600
of running an application or some
applications on G4 versus G5.

00:05:49.720 --> 00:05:55.080
This is - I'll be using a spec - sorry,
a SCIMARC.

00:05:55.200 --> 00:05:59.280
We use several different benchmarks
internally to test various things.

00:05:59.550 --> 00:06:04.640
We would normally have used spec-JVM,
but there's a fair use policy

00:06:04.680 --> 00:06:09.340
which requires that you post
the scores on a public forum

00:06:09.340 --> 00:06:10.940
before you can actually use them.

00:06:11.110 --> 00:06:13.960
And we're still working with prototypes,
and we don't have our

00:06:13.960 --> 00:06:15.370
final values and whatnot.

00:06:15.650 --> 00:06:20.800
So we chose to use SCIMARC,
which is a fairly good benchmark,

00:06:21.270 --> 00:06:26.440
and it will give you a good
sense of where we're going.

00:06:26.440 --> 00:06:29.440
The other thing about SCIMARC is
that it's a scientific instrument.

00:06:29.440 --> 00:06:34.820
It's a very good engineering benchmark,
which people have often said, well,

00:06:34.830 --> 00:06:39.440
you know, like the client VM is very slow
when it comes to computation.

00:06:39.440 --> 00:06:42.880
Well,
this SCIMARC score - the SCIMARC score

00:06:42.950 --> 00:06:48.410
should give you a sense of where
we're headed with the computation.

00:06:49.560 --> 00:06:54.960
CIMARC can be found at
the National Institute for

00:06:54.960 --> 00:06:56.760
Standards and Technology website.

00:06:57.500 --> 00:06:59.150
There's the URL.

00:06:59.150 --> 00:07:04.500
And if you go there, there's a whole list
of current standings.

00:07:04.500 --> 00:07:05.800
They're fairly up to date.

00:07:05.800 --> 00:07:08.600
I think the most recent
one is May-June time frame.

00:07:08.600 --> 00:07:13.570
If you look at the list,
you'll see us way down there somewhere.

00:07:13.570 --> 00:07:16.700
Actually in the 61st position.

00:07:16.700 --> 00:07:21.650
This was run by somebody back
in the fall running on 1.3.1.

00:07:22.630 --> 00:07:30.900
1.3.1 version of the VM on a 1.2, sorry,
a 1.

00:07:31.230 --> 00:07:35.030
and the other two are going to be
talking about the new G4 processor,

00:07:35.100 --> 00:07:38.090
the G425 GHz dual processor G4.

00:07:38.100 --> 00:07:40.100
So note the score there.

00:07:40.130 --> 00:07:42.100
The score is 78253.

00:07:42.100 --> 00:07:45.100
That is what's called
the composite score.

00:07:45.100 --> 00:07:49.100
The side mark is actually
five separate tests,

00:07:49.100 --> 00:07:53.260
such as fast Fourier transform,
sparse matrix multiplication,

00:07:53.260 --> 00:07:55.100
Monte Carlo.

00:07:55.100 --> 00:08:01.100
So that's the composite score
prepared from those five results,

00:08:01.100 --> 00:08:02.100
and that's the composite score
which is used to actually rate

00:08:02.100 --> 00:08:06.810
how you're doing in the side mark.

00:08:10.030 --> 00:08:14.180
This graph will show a high-end G4,
the current high-end G4,

00:08:14.190 --> 00:08:22.860
which is a 1.42 GHz dual
G4 against a 2 GHz dual G5.

00:08:24.260 --> 00:08:28.700
Focus on the first column because that's
the one that the score is actually,

00:08:28.720 --> 00:08:29.960
the main score is based on.

00:08:30.000 --> 00:08:34.200
So currently,
our score would be around 111.

00:08:34.290 --> 00:08:36.790
Okay, composite score.

00:08:37.200 --> 00:08:41.200
And you can see each
of the subtests there.

00:08:41.200 --> 00:08:43.200
I'm not sure whether there's
a normalization of these.

00:08:43.200 --> 00:08:47.190
I haven't seen anybody actually
hitting 100 on any of those,

00:08:47.310 --> 00:08:50.030
but that's basically what
you would find currently.

00:08:50.200 --> 00:08:56.200
Now if you took a straight 40% increase
in performance on each of those tests,

00:08:56.200 --> 00:08:59.080
this would be the projection
that you would get.

00:08:59.200 --> 00:09:04.330
Okay, so we did this to sort of get a
sense of where we should be headed

00:09:04.330 --> 00:09:07.200
once we ran it on one of these G5s.

00:09:07.200 --> 00:09:08.780
Okay, so a score of 158.

00:09:08.780 --> 00:09:11.060
Again,
focus on the composite score because the

00:09:11.150 --> 00:09:13.200
other one is going to vary a little bit.

00:09:13.200 --> 00:09:17.170
So, as I said, this would have been the
best we would have expected.

00:09:17.250 --> 00:09:20.200
Well, we were kind of surprised
when we actually ran the test.

00:09:20.200 --> 00:09:24.940
And we got a 232,
which is pretty significant.

00:09:25.260 --> 00:09:27.200
So it's more than just gigahertz.

00:09:27.200 --> 00:09:31.200
It's also the system itself and the
changes that we've made to the VM.

00:09:31.200 --> 00:09:35.200
And here's an overlay of the projections
just to show you where we're at.

00:09:35.200 --> 00:09:40.200
So our score is basically
more than doubled on the G5.

00:09:42.540 --> 00:09:44.400
So where does this put us?

00:09:44.520 --> 00:09:48.240
Well, if we were to do this today,
this would put us about 12th position.

00:09:48.370 --> 00:09:52.630
And what's interesting is that this
is up in the high end there with all

00:09:52.700 --> 00:09:59.000
the high IBM servers running 3 GHz,
and we're running a client VM.

00:09:59.120 --> 00:10:03.790
So that gives you a sense of
the power of what the G5 is,

00:10:03.810 --> 00:10:06.150
and also the potential that
we could have as we make

00:10:06.270 --> 00:10:09.020
further improvements on the VM.

00:10:17.810 --> 00:10:20.850
Okay, so let's go over some of the
changes that took place in

00:10:20.850 --> 00:10:24.700
the interpreter and the JITC.

00:10:24.700 --> 00:10:28.470
We enhanced them with G5 instructions.

00:10:28.770 --> 00:10:33.160
We can do this because the interpreter,
the JITC compiler code generation,

00:10:33.160 --> 00:10:37.610
and also the runtime are all constructed
on the fly when you launch the VM.

00:10:37.700 --> 00:10:42.700
So this gives us an opportunity to choose
which instructions that we want to apply.

00:10:42.700 --> 00:10:45.610
So if we ran on a G3 versus G4,
we would choose different instructions.

00:10:45.700 --> 00:10:48.590
If we were running on a single
processor or a double processor,

00:10:48.590 --> 00:10:50.700
we would choose different instructions.

00:10:50.740 --> 00:10:53.290
And now that we're running on a G5,
we can actually choose

00:10:53.290 --> 00:10:54.620
64-bit instructions.

00:10:54.900 --> 00:10:59.910
So Java long in support is
now using 64-bit operations,

00:10:59.910 --> 00:11:03.700
and I'll show some of
the details of that.

00:11:03.960 --> 00:11:06.820
There's also been improvement in
float and double support using some of

00:11:06.820 --> 00:11:08.700
the new floating point instructions.

00:11:08.700 --> 00:11:10.280
And they're not actually
new instructions,

00:11:10.280 --> 00:11:12.260
they're just instructions
that are only made available

00:11:12.270 --> 00:11:13.700
because of the 64-bit support.

00:11:13.700 --> 00:11:15.810
64-bit support.

00:11:17.290 --> 00:11:22.380
So let's talk a little bit about
the details of what 64-bit means.

00:11:22.820 --> 00:11:27.360
In a G4, or G3 for that matter,
everything that goes through the

00:11:27.360 --> 00:11:30.290
processor has to go through in 32 bits.

00:11:30.290 --> 00:11:37.800
That's because the data bus and the width
of the registers is only 32 bit wide.

00:11:37.800 --> 00:11:43.940
So if you wanted to do an operation
on a 64 bit long integer value,

00:11:43.940 --> 00:11:49.460
you would require two registers
to deal with each of the

00:11:49.460 --> 00:11:51.290
operands and with the result.

00:11:51.370 --> 00:11:55.420
In this case, if we had foo = x + y,
foo would require two

00:11:55.440 --> 00:11:57.700
registers to hold the result.

00:11:57.780 --> 00:12:02.320
In this case, in this example, R3 and R4,
we'd also need two

00:12:02.410 --> 00:12:05.930
registers for x and for y,
so we'd need six registers just

00:12:05.930 --> 00:12:11.140
to perform a long add operation,
or a long subtract operation.

00:12:14.290 --> 00:12:18.760
In the G5 world,
our registers are 64-bit wide.

00:12:18.840 --> 00:12:21.240
We can still treat them as
though they're 32-bit wide,

00:12:21.260 --> 00:12:23.710
and some operations still
deal with them as 32-bit wide,

00:12:23.880 --> 00:12:27.960
but the long integer operations,
we can deal with them as full 64-bit.

00:12:28.090 --> 00:12:31.920
So in the previous example where
we had foo equals x plus y,

00:12:31.920 --> 00:12:35.760
foo only needs,
the result only needs one register.

00:12:35.860 --> 00:12:40.860
And x and y only need one register each,
so we cut down the number of registers

00:12:41.210 --> 00:12:45.170
that are needed for each operation,
and that makes more available

00:12:45.310 --> 00:12:47.280
for other operations.

00:12:47.360 --> 00:12:54.330
So we get a win from the
reduction or the lack of,

00:12:54.480 --> 00:13:00.210
sorry, the general win by having more
registers available for operations.

00:13:01.040 --> 00:13:06.180
So let's look at the specific
operations that we can improve on.

00:13:08.980 --> 00:13:14.590
So in your Java code you have
an expression along X equals Y.

00:13:14.970 --> 00:13:24.170
On a G4, this would actually require four
steps in order to perform this.

00:13:24.520 --> 00:13:29.200
We need two steps to load
each half of the 64-bit value,

00:13:29.200 --> 00:13:32.040
the high 32 bits, then the low 32 bits.

00:13:32.040 --> 00:13:36.380
And then we need two steps to
store those out back into memory.

00:13:36.380 --> 00:13:42.070
So in the 32-bit world,
we almost always have to use at least

00:13:42.090 --> 00:13:44.820
two instructions where one would do.

00:13:44.820 --> 00:13:48.900
And the G5, we only have one instruction
for each of those.

00:13:48.900 --> 00:13:53.120
So one instruction for load,
one instruction for store.

00:13:54.290 --> 00:13:57.230
This is also used for moving data.

00:13:57.320 --> 00:14:00.880
We have a 64-bit data bus,
we can get better throughput

00:14:00.980 --> 00:14:01.720
through the system.

00:14:01.720 --> 00:14:04.710
So when we're doing memory copies,
we're also getting

00:14:04.720 --> 00:14:06.890
performance boosts there.

00:14:08.150 --> 00:14:11.300
Let's look at some of the
simple operations like add,

00:14:11.380 --> 00:14:13.630
subtract, and negate.

00:14:15.490 --> 00:14:19.310
Again, because we only have
32-bit wide registers,

00:14:19.360 --> 00:14:22.830
we have to do everything
in two steps on the G4.

00:14:22.910 --> 00:14:27.400
So in this case,
if we want to add two long ints,

00:14:27.480 --> 00:14:32.300
we have to add the low
halves of the two operands,

00:14:32.360 --> 00:14:34.500
bring the carry forward,
and then add the two halves

00:14:34.500 --> 00:14:35.300
and add the carry in.

00:14:35.570 --> 00:14:38.900
So that would be the two steps
that are highlighted here.

00:14:38.960 --> 00:14:43.400
I've thrown in the load operations
as well to give you a sense of that.

00:14:43.460 --> 00:14:46.780
Well, it's not just the operation itself,
it's also the things

00:14:46.790 --> 00:14:48.400
that go on around it.

00:14:48.490 --> 00:14:51.680
So it takes eight
instructions to perform that.

00:14:52.260 --> 00:14:56.730
On the G5, it only takes one
instruction to do the add,

00:14:56.730 --> 00:15:01.100
and again, each of the loads only
takes one instruction,

00:15:01.100 --> 00:15:02.940
the store only takes one instruction.

00:15:03.010 --> 00:15:05.400
So we've cut the number of
instructions required in half,

00:15:05.440 --> 00:15:08.280
and you can think in terms
of fewer instructions,

00:15:08.280 --> 00:15:09.410
faster code.

00:15:11.360 --> 00:15:15.750
Now the more interesting things,
and this is the most trouble for

00:15:15.750 --> 00:15:20.300
us in implementing the Java VM,
has been dealing with longs and some

00:15:20.300 --> 00:15:23.890
of these more complex operations like
multiply and divide and remainder

00:15:23.890 --> 00:15:25.840
and shifts and even comparisons.

00:15:25.910 --> 00:15:30.860
They can take many instructions and long
and divide can literally take hundreds

00:15:30.860 --> 00:15:36.670
of instructions or hundreds of steps
in order to complete the operation.

00:15:36.780 --> 00:15:39.390
Remainder, just a few more.

00:15:39.400 --> 00:15:40.600
Shifts can take eight.

00:15:40.740 --> 00:15:43.420
Comparisons can take up to 12.

00:15:43.810 --> 00:15:46.820
They're fairly expensive operations.

00:15:47.140 --> 00:15:51.090
Each of these have been reduced to
one single operation and I'll take

00:15:51.090 --> 00:15:53.090
multiply as the simplest example.

00:15:55.360 --> 00:16:01.660
On the G4, to do a long int multiply,
it takes six steps to do the

00:16:01.660 --> 00:16:06.940
cross multiply of the low and
high parts of the operands.

00:16:07.000 --> 00:16:11.170
On the G5, it only takes one instruction.

00:16:12.360 --> 00:16:17.600
So you can see where this is going,
is that if you have a lot of long

00:16:17.600 --> 00:16:22.460
int computation in your code,
where it took many steps before,

00:16:22.460 --> 00:16:24.900
it's only going to take a few now.

00:16:28.200 --> 00:16:30.980
Let's take a look at float.

00:16:30.980 --> 00:16:35.100
When I say float,
I mean float and double.

00:16:35.100 --> 00:16:41.120
In the G5 implementation of the Java VM,
we have taken advantage of some

00:16:41.120 --> 00:16:44.740
of the newer instructions that
can convert longs to double,

00:16:44.770 --> 00:16:49.340
and doubles back to long,
and same is true with float.

00:16:49.570 --> 00:16:52.840
In the G4 implementation,
it has to make a library call,

00:16:52.840 --> 00:16:55.400
which takes several hundreds of steps.

00:16:55.430 --> 00:16:59.160
So it speeds up the performance
of casting or conversion

00:16:59.160 --> 00:17:00.450
of longs to doubles.

00:17:00.820 --> 00:17:04.740
There's also been some
improvements in the float and

00:17:04.740 --> 00:17:08.600
double bit extraction routines,
such as double to long bits.

00:17:08.670 --> 00:17:11.960
These are used primarily when
you're converting doubles

00:17:11.960 --> 00:17:13.690
to strings and back again.

00:17:14.140 --> 00:17:17.150
The most interesting of
the changes is square root.

00:17:17.540 --> 00:17:20.800
On the G5,
there's a built-in square root function.

00:17:20.960 --> 00:17:28.510
On the G4, the square root is implemented
as a trig library routine,

00:17:28.510 --> 00:17:30.800
and it can take several steps.

00:17:30.800 --> 00:17:34.790
It's in the order of about
40 steps to complete.

00:17:35.310 --> 00:17:40.580
So what I did was I took a little
micro benchmark where I'm iterating

00:17:40.580 --> 00:17:47.740
through 100 million data points and
applying a square root to each of the

00:17:47.740 --> 00:17:49.280
data points and producing a result.

00:17:49.300 --> 00:17:52.230
And just to make it interesting,
I took a little bit more complex

00:17:52.290 --> 00:17:57.420
operation where I had 100 million
xy points on a coordinate plane,

00:17:57.420 --> 00:18:00.320
and I wanted to compute the distance,
so it's a little bit more

00:18:00.320 --> 00:18:01.300
complicated equation.

00:18:01.500 --> 00:18:05.120
And just to see how long it would
take to do on each of the processors.

00:18:05.300 --> 00:18:11.300
The first processor is the
G4 running at 1.42 GHz.

00:18:11.300 --> 00:18:19.300
So it takes about 12 seconds
to do all those computations,

00:18:19.300 --> 00:18:22.590
and 13.5 for the distance formula.

00:18:24.890 --> 00:18:27.520
Now if I was just to take a
straight port over and use

00:18:27.520 --> 00:18:31.610
the library routine on the G5,
it would be reduced to 7.7

00:18:31.610 --> 00:18:33.890
seconds and 8.1 seconds.

00:18:34.010 --> 00:18:40.900
And this is actually better than 30% of
the projected time that it should take.

00:18:41.370 --> 00:18:44.700
So the floating point
processing is better on the G5,

00:18:44.700 --> 00:18:47.000
and you're going to get a better result.

00:18:47.710 --> 00:18:53.070
On the G5, running with the square root
instruction built into the

00:18:53.160 --> 00:18:55.600
code or inlined in the code,
it only takes two seconds.

00:18:55.600 --> 00:18:59.100
So you've got, say,
six times improvement in performance.

00:18:59.210 --> 00:19:00.800
This is a micro benchmark.

00:19:00.930 --> 00:19:05.600
It's just going to give you a sense
of the increase in performance

00:19:05.600 --> 00:19:07.200
of the square root itself.

00:19:07.590 --> 00:19:10.830
So your actual example may
take a little bit longer,

00:19:10.830 --> 00:19:15.310
but it gives you a sense of the
magnitude of the improvement there.

00:19:17.080 --> 00:19:19.990
Finally,
I just want to quickly run through

00:19:19.990 --> 00:19:22.420
some of the changes to the runtime.

00:19:22.680 --> 00:19:25.500
In a 32-bit world,
we have a little problem

00:19:25.500 --> 00:19:30.600
where two threads may want
to share a long int value,

00:19:30.690 --> 00:19:34.910
say a static or a field in an object.

00:19:35.040 --> 00:19:39.320
And while they're
writing to that long int,

00:19:39.590 --> 00:19:44.360
the upper and lower halves of
those values might get slammed

00:19:44.360 --> 00:19:47.420
by one or the other process,
depending on how the thread

00:19:47.420 --> 00:19:48.730
switching is going on.

00:19:48.980 --> 00:19:53.270
To avoid that problem,
you can annotate your field

00:19:53.270 --> 00:19:55.600
with the volatile keyword.

00:19:55.660 --> 00:20:02.600
And what that does is forces the VM to
coordinate how that field is accessed,

00:20:02.670 --> 00:20:05.620
and make sure that we don't
run into that problem.

00:20:05.700 --> 00:20:10.360
In the G4, we did a little fudge using
a 64-bit double register,

00:20:10.410 --> 00:20:14.860
and using that as an atomic access,
and copying it through some memory,

00:20:14.860 --> 00:20:15.600
and so on and so forth.

00:20:15.670 --> 00:20:20.400
So it took several steps
in order to make that work.

00:20:20.470 --> 00:20:24.660
In the G5,
64-bit loads and stores are atomic,

00:20:24.760 --> 00:20:26.520
so you don't have that problem.

00:20:26.610 --> 00:20:32.390
So there's no overhead when you're
dealing with volatile fields on the G5.

00:20:35.190 --> 00:20:40.500
One of the problems that the G5
introduces is the fact that the

00:20:40.500 --> 00:20:43.820
hardware itself is a little bit
more complex and has more stages

00:20:43.820 --> 00:20:45.300
when it's doing its computation.

00:20:45.300 --> 00:20:47.100
This is where it gets its speed.

00:20:47.100 --> 00:20:49.750
So when you're running
on a dual processor,

00:20:49.750 --> 00:20:54.100
there needs to be some coordination
on how memory is being accessed.

00:20:54.130 --> 00:20:58.100
In the G4 world, we use something called
the sync instruction,

00:20:58.100 --> 00:21:01.920
and this allowed the two processors
in the dual processor environment

00:21:01.940 --> 00:21:05.810
to sync up the data that's shared
between the two processors.

00:21:05.800 --> 00:21:10.800
But the problem with the sync instruction
is that it somewhat freezes the state

00:21:10.800 --> 00:21:14.800
of the processors until they're both
coordinated before it continues on.

00:21:14.930 --> 00:21:18.070
So there's a bit of an impact there,
and sometimes it can be

00:21:18.070 --> 00:21:19.800
actually fairly serious.

00:21:20.050 --> 00:21:23.380
With the introduction of the G5,
they brought in a new instruction

00:21:23.440 --> 00:21:27.440
called Lightweight Sync,
which doesn't require as much

00:21:27.440 --> 00:21:35.800
handshaking between the processors to
determine whether the data is in sync.

00:21:35.890 --> 00:21:38.870
And we use these when we're
doing memory allocation,

00:21:38.870 --> 00:21:40.800
when two threads are trying to
allocate memory at the same time,

00:21:40.800 --> 00:21:46.240
or when you're using a
synchronization of an object.

00:21:48.560 --> 00:21:52.390
And finally,
the last major change that we

00:21:52.390 --> 00:21:55.900
made in the runtime to deal with
the G5 is atomic long access.

00:21:56.170 --> 00:22:01.480
There's a class in SunMisc
called AtomicLongCS - sorry,

00:22:01.480 --> 00:22:07.100
CSImpl, which allows you to do
atomic access of long values.

00:22:07.100 --> 00:22:10.500
And this is primarily used
in the net operations,

00:22:10.540 --> 00:22:14.600
like when you're setting up
sockets and so on and so forth.

00:22:14.730 --> 00:22:19.390
In the G4, we had to actually use
full Java synchronization,

00:22:19.730 --> 00:22:23.600
and we just used the Java implementation
to provide the synchronization.

00:22:23.600 --> 00:22:28.600
So we lock out the access to
that particular object field,

00:22:28.720 --> 00:22:31.040
and then we make the assignment
and release it through

00:22:31.210 --> 00:22:32.540
normal synchronization.

00:22:32.600 --> 00:22:36.600
On the G5,
we use lightweight load and reserve,

00:22:36.600 --> 00:22:36.600
which is an instruction
that we use for the G5.

00:22:36.600 --> 00:22:39.830
It allows us to reserve
access to that word,

00:22:39.830 --> 00:22:42.600
and it can be done fairly quickly.

00:22:44.670 --> 00:22:53.830
So in summary, the Java that 1.14,
or sorry, 1.4.1 that ships with the G5s,

00:22:53.830 --> 00:22:57.210
once they start shipping,
will automatically adapt

00:22:57.210 --> 00:22:59.000
to the G5 processor.

00:22:59.080 --> 00:23:04.000
And we're only going to be shipping one
version of the VM from that point on.

00:23:04.000 --> 00:23:07.180
It's not one that runs on the G4,
one that runs on G5.

00:23:07.390 --> 00:23:11.400
It's one that runs on all
platforms but adapts to the G5.

00:23:11.460 --> 00:23:17.300
And this is one of the great
things about the Hotspot VM.

00:23:17.730 --> 00:23:20.200
You're going to get
significant performance changes

00:23:20.200 --> 00:23:23.950
somewhat across the board,
specifically in floating point.

00:23:23.950 --> 00:23:26.440
That's the main thing.

00:23:26.440 --> 00:23:30.100
If you're doing scientific computing,
you're going to see bigger wins there.

00:23:30.180 --> 00:23:33.600
And then also with the
long int arithmetic,

00:23:33.600 --> 00:23:35.320
if you're using it.

00:23:36.830 --> 00:23:41.780
The main thing that I want to point
out is that you don't have to make

00:23:41.780 --> 00:23:43.900
any code changes to your own code.

00:23:43.910 --> 00:23:45.960
The VM does the adoption for you.

00:23:45.960 --> 00:23:51.000
This is where you're one up on all
the C and Objective C programmers.

00:23:51.000 --> 00:23:54.940
Because if they want to run on G5 and
take advantage of the G5 processor,

00:23:55.040 --> 00:23:58.430
they're going to have to recompile
their application and they're

00:23:58.430 --> 00:24:00.700
going to have to ship a separate
version of their application

00:24:00.700 --> 00:24:04.000
for the G5 and one for the G4.

00:24:04.000 --> 00:24:06.960
So Java is automatically going
to take advantage of that.

00:24:07.030 --> 00:24:11.000
Okay, and that's all I have to say,
I guess.

00:24:11.000 --> 00:24:11.000
Okay, Victor?

00:24:16.000 --> 00:24:19.800
There you go.

00:24:19.800 --> 00:24:22.600
So for those of you that don't think
in terms of bits and instructions,

00:24:22.600 --> 00:24:24.640
we'll take it at a higher level now.

00:24:24.640 --> 00:24:26.740
My name is Victor Hernandez,
in case you don't remember.

00:24:26.740 --> 00:24:30.710
And here we go.

00:24:31.790 --> 00:24:34.880
So basically what I'm going to be
talking about is updates to Hotspot

00:24:34.960 --> 00:24:37.770
that have been made with Java 1.4.1.

00:24:37.770 --> 00:24:39.720
Specifically,
one of the features that we've added

00:24:39.740 --> 00:24:43.320
in being able to optimize your code,
and that's specifically

00:24:43.320 --> 00:24:44.400
aggressive inlining.

00:24:44.400 --> 00:24:48.060
And also,
one performance opportunity that you can

00:24:48.060 --> 00:24:53.800
take advantage of yourself in Java 1.4.1,
which is the new IO APIs.

00:24:53.800 --> 00:24:56.830
And finally,
I'm going to kind of wrap it up

00:24:56.920 --> 00:25:01.260
with a bunch of conclusions on
tips that you can take advantage

00:25:01.260 --> 00:25:03.760
of to improve your hot methods.

00:25:06.900 --> 00:25:11.900
Okay, so one of the performance
bottlenecks that has plagued Java,

00:25:11.960 --> 00:25:15.560
well, I don't know if plagued,
but that Java has encountered

00:25:15.560 --> 00:25:18.640
since the early days,
is the fact that there's a

00:25:18.640 --> 00:25:22.160
large cost in the overhead of
actually invoking a Java method.

00:25:22.160 --> 00:25:28.890
So our opportunity to minimize that
cost is actually done by dynamically

00:25:28.890 --> 00:25:34.630
inlining the method calls done by your
method when we compile your method.

00:25:34.700 --> 00:25:38.260
What is inlining?

00:25:38.260 --> 00:25:41.480
That should be pretty straightforward,
but I'll give a quick example.

00:25:41.700 --> 00:25:44.700
Here you've got average and sum,
average call sum.

00:25:44.700 --> 00:25:48.550
Of course,
this could avoid the call to sum if it

00:25:48.550 --> 00:25:52.710
just simply did the A plus B itself,
but of course you don't want to do

00:25:52.710 --> 00:25:57.190
that in your code because that limits
the reusability of that method.

00:25:57.190 --> 00:26:00.240
Good thing is that we're
able to do that for you.

00:26:00.240 --> 00:26:01.700
You don't need to change your code,
we just do it for you on the fly.

00:26:02.560 --> 00:26:05.760
In 1.3.1,
there was limited ability to do inlining.

00:26:05.830 --> 00:26:10.240
We were able to inline your
accessor methods to your fields.

00:26:10.280 --> 00:26:15.500
We were able to inline your call to
create new instances of your objects.

00:26:15.500 --> 00:26:17.500
And we were able to
inline certain intrinsics.

00:26:17.500 --> 00:26:23.500
Intrinsics being methods that we actually
don't need to look at the bytecodes

00:26:23.500 --> 00:26:25.500
to know what it's supposed to do.

00:26:25.500 --> 00:26:28.500
We actually know what it's
supposed to do and actually have a

00:26:28.500 --> 00:26:30.500
finely tuned implementation of it.

00:26:30.500 --> 00:26:36.490
For example, sine, cosine,
also the identity function.

00:26:36.500 --> 00:26:39.430
And then also,
but one of the main issues with

00:26:39.430 --> 00:26:44.020
inlining in Java 1.3.1 was the
fact that we were actually not

00:26:44.050 --> 00:26:46.360
able to inline virtual methods.

00:26:46.550 --> 00:26:48.440
Why are virtual methods
difficult to inline?

00:26:48.500 --> 00:26:50.490
Well,
the reason they're difficult to inline

00:26:50.490 --> 00:26:55.500
is because there could be possible
multiple implementations of that method.

00:26:55.680 --> 00:26:57.880
When you actually go to the invocation,
so we don't know which

00:26:57.880 --> 00:26:59.500
implementation to actually inline.

00:26:59.500 --> 00:27:04.450
So how do we go about inlining
those virtual methods?

00:27:05.810 --> 00:27:08.740
We do that with a technique
called class hierarchy analysis.

00:27:08.760 --> 00:27:15.410
The goal of class hierarchy analysis is
to determine if a method is monomorphic.

00:27:15.410 --> 00:27:18.970
And a method is monomorphic if
there is only one implementation

00:27:19.240 --> 00:27:21.840
of that particular virtual method
that has actually been loaded.

00:27:21.840 --> 00:27:24.770
If we know there's only
one that has been loaded,

00:27:24.770 --> 00:27:27.750
and you go to call it,
that's got to be the one.

00:27:28.590 --> 00:27:34.370
and Hotspot 141 attempts to aggressively
inline all monomorphic methods.

00:27:34.510 --> 00:27:39.500
That's the main feature
we've added beyond 1.3.1.

00:27:39.500 --> 00:27:40.500
So what are the benefits of this?

00:27:40.500 --> 00:27:43.500
Well, clearly the fact that now we can
actually inline virtual methods.

00:27:43.500 --> 00:27:47.600
There are certain situations where those
methods actually don't get inlined,

00:27:47.600 --> 00:27:51.820
and even in that case then,
we can avoid the virtual table

00:27:51.820 --> 00:27:55.460
lookup when invoking that method,
because we know that there's only

00:27:55.460 --> 00:27:57.500
one entry in the virtual table.

00:27:57.500 --> 00:28:01.210
This also provides us the ability
to do a faster implementation

00:28:01.210 --> 00:28:05.240
of certain bytecodes,
because the class hierarchy analysis has

00:28:05.240 --> 00:28:08.620
a data structure which actually tells
us the full hierarchy information of

00:28:08.620 --> 00:28:10.500
all the classes that have been loaded.

00:28:10.500 --> 00:28:14.270
So when you're doing things
like instance of or check cast,

00:28:14.360 --> 00:28:19.490
which are bytecodes used when casting
your objects between various classes,

00:28:19.500 --> 00:28:24.240
we can actually use that data structure,
and it actually performs a lot faster.

00:28:26.700 --> 00:28:33.460
OK, so what is another performance issue
that has affected Java in the past?

00:28:33.460 --> 00:28:35.360
Well,
this one actually has two parts to it.

00:28:35.360 --> 00:28:38.850
One is the fact that if you ever
wanted to operate on native data

00:28:38.880 --> 00:28:41.680
structures from your Java methods,
you actually had to have them

00:28:41.680 --> 00:28:43.580
residing in the Java heap.

00:28:43.690 --> 00:28:46.580
Why would you actually need to have
native data structures in your Java heap?

00:28:46.580 --> 00:28:49.500
Well, if you ever want to interact
with any system APIs,

00:28:49.530 --> 00:28:52.660
you actually need to have those
data structures to pass down once

00:28:52.660 --> 00:28:55.010
you drop down into native methods.

00:28:55.210 --> 00:28:59.540
That adds the other heavy cost,
which is the fact that those

00:28:59.540 --> 00:29:03.050
JNI transitions to do those method calls,
the native method calls,

00:29:03.050 --> 00:29:04.440
are quite expensive.

00:29:04.560 --> 00:29:08.280
I mean, in the previous section where
I was talking about the inlining,

00:29:08.280 --> 00:29:12.200
we're trying to minimize
the amount of method calls.

00:29:12.200 --> 00:29:16.240
And those method calls are even pretty
quick compared to these JNI transitions.

00:29:16.240 --> 00:29:19.200
Not only that, but these JNI transitions
definitely cannot be inlined at

00:29:19.200 --> 00:29:23.540
all because we're crossing ABIs,
and we don't totally control a lot of the

00:29:23.540 --> 00:29:28.440
issues between calling from Java to C.

00:29:28.440 --> 00:29:34.690
But those JNI transitions are
still necessary as of Java 1.3.1.

00:29:34.710 --> 00:29:37.530
The other thing to keep in mind is
that once you have all those native

00:29:37.530 --> 00:29:40.360
data structures in the Java heap,
they actually need to be copied

00:29:40.360 --> 00:29:42.730
around during garbage collection.

00:29:43.030 --> 00:29:45.540
And yet they don't contain
any actual Java pointers,

00:29:45.540 --> 00:29:49.890
which the garbage collection
algorithm needs to take track of.

00:29:50.490 --> 00:29:54.640
So what is our approach at
actually improving this bottleneck?

00:29:54.710 --> 00:29:59.070
We want to remove this JNI dependency
altogether by giving you the ability

00:29:59.070 --> 00:30:02.720
to actually access that native
memory from your Java methods.

00:30:02.920 --> 00:30:05.120
You might be familiar with this.

00:30:05.160 --> 00:30:09.000
This is basically the new
IO APIs that were provided in 1.4.1.

00:30:09.050 --> 00:30:12.920
They're available in
the Java and IO package.

00:30:12.970 --> 00:30:15.690
And there's basically a buffer
class for every single one

00:30:15.690 --> 00:30:20.840
of the Java scalar types,
and also for the actual byte scalar type.

00:30:20.930 --> 00:30:24.100
Actually,
all operations happen at the byte level,

00:30:24.110 --> 00:30:29.020
even though that's not a Java scalar.

00:30:29.570 --> 00:30:33.940
But you can actually cast to int buffer,
to long buffer,

00:30:33.940 --> 00:30:37.580
and actually operate at the Java level.

00:30:37.680 --> 00:30:41.650
And one of the things you need
to keep in mind here is that

00:30:41.650 --> 00:30:44.730
even though the goal is to have

00:30:44.960 --> 00:30:50.740
Direct access to native buffers that
are not located inside of the Java heap,

00:30:50.850 --> 00:30:54.500
you can actually trick yourself
into still basically having a

00:30:54.500 --> 00:30:58.010
copy residing in the Java heap,
accessing that and have it being

00:30:58.100 --> 00:31:01.760
copied over outside of the heap,
which even though it might be

00:31:01.850 --> 00:31:05.910
improved performance as before,
since you don't have to drop down

00:31:05.910 --> 00:31:11.010
into a JNI native method to do that,
it still is an added overhead,

00:31:11.010 --> 00:31:11.010
and you need to be careful about that.

00:31:12.410 --> 00:31:14.800
There's a few other
issues I want to bring up.

00:31:14.800 --> 00:31:19.180
This is a pretty straightforward
code example that shows an allocation

00:31:19.180 --> 00:31:23.720
of a byte buffer of size 400,
and you're basically zero

00:31:23.720 --> 00:31:25.510
filling it with a for loop.

00:31:25.600 --> 00:31:30.090
One of the things that actually you need
to be aware of right here is that that

00:31:30.270 --> 00:31:34.290
for loop is not as optimal as it can be,
because the call to the put method

00:31:34.590 --> 00:31:38.080
does not get in line because it's
not determined to be monomorphic.

00:31:38.100 --> 00:31:44.840
This is a caveat of the actual class
hierarchy of the Java and IO package,

00:31:44.860 --> 00:31:48.700
and it affects all your
calls to get and put.

00:31:48.700 --> 00:31:51.770
In the case of byte buffer,
if you're doing something like this,

00:31:51.970 --> 00:31:53.830
there is actually one way
you can get around it.

00:31:53.920 --> 00:31:57.350
And that's actually just simply
by using a map byte buffer.

00:31:57.380 --> 00:31:59.570
The map byte buffer get and
put methods are actually

00:31:59.620 --> 00:32:02.970
determined to be monomorphic,
and they get in line.

00:32:03.120 --> 00:32:04.450
But you need to keep that in mind.

00:32:04.460 --> 00:32:07.700
And this is something that we're
going to be tracking for in the

00:32:07.720 --> 00:32:10.100
future to see if this can be improved.

00:32:11.880 --> 00:32:18.260
So how do you actually do high
level I/O with the new I/O?

00:32:18.430 --> 00:32:22.540
That's using Java and I/O channels,
found in the Java and

00:32:22.540 --> 00:32:24.100
I/O channels package.

00:32:24.180 --> 00:32:27.440
And the main thing that it provides
beyond what was available in

00:32:27.470 --> 00:32:31.840
traditional Java I/O as of Java 1.3.1
is the ability to do non-blocking

00:32:31.840 --> 00:32:33.700
and interruptible operations.

00:32:33.800 --> 00:32:38.280
No longer is there any need to
actually have one thread per socket.

00:32:38.460 --> 00:32:41.780
That's the thing of the past.

00:32:41.850 --> 00:32:44.290
The other thing that it
provides is actually it provides

00:32:44.290 --> 00:32:47.560
improved file system support,
gives you a lot more of the system

00:32:47.560 --> 00:32:53.000
level primitives that you would come to
expect from a robust operating system.

00:32:53.080 --> 00:32:57.140
Things like file locking and
also memory mapped files.

00:32:57.200 --> 00:33:03.070
Just like in the case where you needed
to make sure that you had a direct

00:33:03.070 --> 00:33:07.570
buffer sitting behind your native buffer,
this is another example where

00:33:07.570 --> 00:33:09.400
you actually not only have
access to direct memory,

00:33:09.400 --> 00:33:11.760
but you actually are accessing
the memory mapped file itself.

00:33:11.760 --> 00:33:11.920
stuff.

00:33:14.350 --> 00:33:17.060
So let me go into a little more
detail about the socket channel.

00:33:17.120 --> 00:33:20.900
I don't want this to be a-- I'm not
going to go into enough depth for this

00:33:20.950 --> 00:33:24.380
to be a tutorial on this sort of thing,
but I do want to bring up a few issues

00:33:24.480 --> 00:33:26.580
that tutorials might miss on occasion.

00:33:26.750 --> 00:33:30.400
This is an example of how to
create a server socket channel

00:33:30.800 --> 00:33:35.650
and bind it to a particular
address for it to be listening on.

00:33:35.670 --> 00:33:39.550
One of the things is that, by default,
it is not set to be non-blocking.

00:33:39.560 --> 00:33:42.680
So you actually have to do that
by calling configure blocking

00:33:42.900 --> 00:33:46.060
and passing it a value of false.

00:33:46.070 --> 00:33:48.720
You can-- it's a pretty
straightforward thing,

00:33:48.770 --> 00:33:49.830
but it can be missed.

00:33:49.990 --> 00:33:53.330
And it definitely makes
a huge difference.

00:33:54.050 --> 00:33:57.480
And then actually,
how do you actually communicate

00:33:57.480 --> 00:34:00.720
with your clients using this model?

00:34:00.720 --> 00:34:04.280
You use the selector model,
which you might be familiar with

00:34:04.430 --> 00:34:08.910
from the programming patterns.

00:34:09.150 --> 00:34:12.300
You can see in the code right here,
basically what you're doing is you're

00:34:12.300 --> 00:34:16.190
registering for a particular key,
and then once you've done that,

00:34:16.220 --> 00:34:20.710
you can basically iterate
over all of your clients who

00:34:20.710 --> 00:34:27.190
communicate to you via keys,
and who pass the new channel you were

00:34:27.200 --> 00:34:31.170
communicating with them with in a...

00:34:31.260 --> 00:34:37.110
In a big wild loop, if you want,
by iterating over all of the keys.

00:34:37.110 --> 00:34:39.920
And that way you're abstracting
away all of the different sockets

00:34:39.920 --> 00:34:43.200
that you're actually talking to,
instead of doing the traditional having

00:34:43.200 --> 00:34:48.220
to block until your client talks back
to you along that particular socket.

00:34:48.630 --> 00:34:52.340
One of the things to keep in mind
is that the socket channel that is

00:34:52.340 --> 00:34:58.080
actually returned each of the times you
access one of these keys is different

00:34:58.080 --> 00:34:59.500
than the one you had originally.

00:34:59.500 --> 00:35:02.500
So if you want to continue
the non-blocking I/O,

00:35:02.500 --> 00:35:07.800
you actually need to state that you're
doing non-blocking I/O once again with

00:35:07.800 --> 00:35:10.640
the configure blocking set to false.

00:35:12.990 --> 00:35:16.580
Okay, so what do you need to keep
in mind when using NIO?

00:35:16.580 --> 00:35:18.500
Well, it's definitely not free.

00:35:18.500 --> 00:35:22.090
The cost of allocating those native
buffers is definitely much larger

00:35:22.460 --> 00:35:23.820
than allocating the Java arrays.

00:35:23.910 --> 00:35:27.480
It's pretty hard to reach our performance
of allocating Java arrays because

00:35:27.480 --> 00:35:31.860
we actually do a very good job at
doing that as quickly as possible.

00:35:31.950 --> 00:35:35.680
The other thing to keep in mind
is that the get-put methods of the

00:35:35.680 --> 00:35:37.900
native buffers are not inlined.

00:35:37.900 --> 00:35:41.900
You can use the trick to get at
least that fixed for a few cases,

00:35:41.900 --> 00:35:44.490
but there's nothing you can do
for int buffer and for some of

00:35:44.490 --> 00:35:46.680
the other scalar buffer types.

00:35:46.770 --> 00:35:51.790
But the cost definitely out - sorry,
the gains definitely outweigh the cost in

00:35:51.790 --> 00:35:55.080
the cases where I've been talking about,
where you have heavy use of system

00:35:55.080 --> 00:35:56.700
APIs with native data structures.

00:35:56.710 --> 00:36:01.020
One of the good examples of actually
taking advantage of that win is

00:36:01.020 --> 00:36:06.700
actually in the re-architecture of the
AWT done by our team for Java 1.4.1.

00:36:06.700 --> 00:36:15.000
We actually took advantage of the new
IO API to talk to core graphics and

00:36:15.000 --> 00:36:18.630
minimize the number of JNI transitions.

00:36:18.710 --> 00:36:22.410
Basically, we told the classes team,
try to minimize JNI transitions

00:36:22.460 --> 00:36:25.700
as much as possible,
and they did that as much as they could.

00:36:25.700 --> 00:36:29.520
You definitely see the
performance improvement there,

00:36:29.700 --> 00:36:34.880
and we're hoping that the shared classes
on the whole will be seeing more use

00:36:34.880 --> 00:36:36.700
of new IO wherever that can be done.

00:36:36.840 --> 00:36:38.850
in the future.

00:36:39.930 --> 00:36:45.630
The other thing also is clearly if you
have server I/O with multiple clients,

00:36:45.650 --> 00:36:50.000
you definitely want to
be using this because the

00:36:50.000 --> 00:36:55.730
overhead is definitely costly.

00:36:58.140 --> 00:37:00.570
So,
what can you take away from all of this?

00:37:00.570 --> 00:37:03.500
Well, the main thing you need to keep
in mind with the server I/O is

00:37:03.500 --> 00:37:05.100
just simply use it in those cases.

00:37:05.100 --> 00:37:09.910
And with what I told you about inlining,
what you need to do actually

00:37:09.910 --> 00:37:14.100
is maximize the opportunities
where we can inline your methods.

00:37:14.100 --> 00:37:16.100
This is mainly important
in your hot methods.

00:37:16.100 --> 00:37:17.840
When you do a profile,
you want to figure out what the

00:37:17.850 --> 00:37:22.660
methods that you're mainly calling are,
and make sure that the hottest method,

00:37:22.660 --> 00:37:27.100
all of the things that it's calling
are hopefully being inlined.

00:37:27.100 --> 00:37:30.100
It only can be done at a high level.

00:37:30.100 --> 00:37:38.290
There are actually no flags to have this
notified to you if your methods aren't

00:37:38.290 --> 00:37:40.100
being inlined and that sort of thing.

00:37:40.100 --> 00:37:42.040
But the general rules of
thumb are definitely if all

00:37:42.040 --> 00:37:44.600
those methods are small,
that helps because we have a

00:37:44.610 --> 00:37:49.550
certain limit at which point we
bail on any future inlines in the

00:37:49.550 --> 00:37:51.100
method we're trying to compile.

00:37:51.100 --> 00:37:52.870
Feel free to use accessor methods.

00:37:52.870 --> 00:37:56.100
Those have been inlined
definitely since Java 1.3.1.

00:37:56.100 --> 00:38:00.070
Also, there's no need to use the
final qualifier on your methods.

00:38:00.100 --> 00:38:02.100
That's superficial for
performance tuning.

00:38:02.100 --> 00:38:05.070
It's definitely not superficial
for object oriented programming,

00:38:05.120 --> 00:38:09.100
but we don't particularly
get any benefit out of that.

00:38:09.370 --> 00:38:14.280
And also, keep in mind that a lot of the
JDK methods do get inlined,

00:38:14.300 --> 00:38:19.300
so you can keep that in mind if that's a
lot of what your HOT methods are doing.

00:38:19.300 --> 00:38:22.300
There are a few things that
we're still unable to inline,

00:38:22.300 --> 00:38:24.250
and you've got to keep that in mind.

00:38:24.420 --> 00:38:28.690
Mainly synchronized methods,
obviously large methods,

00:38:28.690 --> 00:38:31.850
and if you have an exception
handler in your method,

00:38:31.930 --> 00:38:33.300
that can cause it not to be inlined.

00:38:33.300 --> 00:38:35.790
So keep that in mind.

00:38:36.010 --> 00:38:40.030
The last tips I want to leave you with
are ones I always like to reiterate,

00:38:40.100 --> 00:38:45.860
which are things that still
live on from the days of Java 1.

00:38:45.940 --> 00:38:46.870
Avoid object pools.

00:38:46.950 --> 00:38:50.810
There's absolutely no need
for them in modern Java.

00:38:51.110 --> 00:38:52.740
Our new is completely fast.

00:38:52.790 --> 00:38:53.860
It's also inlined.

00:38:53.940 --> 00:38:55.760
We also have thread local allocation.

00:38:55.840 --> 00:38:59.840
So now there's minimized contention
between multiple threads allocating

00:38:59.840 --> 00:39:02.000
in the Java heap at the same time.

00:39:02.120 --> 00:39:05.000
And we also, I mean,
we have precise garbage collection.

00:39:05.000 --> 00:39:08.190
So let us do the work for you
in terms of figuring out when

00:39:08.280 --> 00:39:10.000
an object needs to go away.

00:39:10.000 --> 00:39:13.330
You don't need to take care of it in
terms of the object pool and all that.

00:39:13.820 --> 00:39:15.950
And also, avoid programming by exception.

00:39:16.120 --> 00:39:20.020
There definitely are situations where
you want to program by exceptions.

00:39:20.160 --> 00:39:24.430
There's the case where you have, like,
you want to go down a tree and

00:39:24.430 --> 00:39:28.050
then go all the way back up to
certain branches in the tree.

00:39:28.240 --> 00:39:29.080
Sure.

00:39:29.120 --> 00:39:35.630
But Hotspot is definitely not optimized
to compile those cases as well.

00:39:35.850 --> 00:39:40.550
For example,
it can cause inlining to be prevented.

00:39:41.450 --> 00:39:46.190
And also, the actual creation of the
exceptions are expensive,

00:39:46.500 --> 00:39:49.900
but that creation only happens if
the exception is actually thrown.

00:39:50.000 --> 00:39:53.400
So, hope you gave away some
tips for your application,

00:39:53.400 --> 00:39:56.010
and now I'll bring up Gerard.

00:40:02.970 --> 00:40:04.120
Hello, welcome.

00:40:04.200 --> 00:40:05.440
My name is Gerard Ziemski.

00:40:05.520 --> 00:40:08.240
I'm an engineer on Java Classes team.

00:40:08.400 --> 00:40:11.790
And I'll be talking to you
about graphics performance.

00:40:16.170 --> 00:40:19.070
First,
I'll give you a short introduction of

00:40:19.070 --> 00:40:21.340
the state of Java graphics on Mac OS X.

00:40:21.360 --> 00:40:25.800
Then I'll give you a few actual
tips and techniques on what you

00:40:25.800 --> 00:40:30.540
can do to your application to
make your Java app run faster.

00:40:30.710 --> 00:40:37.410
And finally,
we'll have some cool demos to show you.

00:40:38.540 --> 00:40:40.620
Java 1.3.1.

00:40:40.620 --> 00:40:45.790
One really interesting thing that
we've done there was a Java 2D hardware

00:40:45.830 --> 00:40:49.660
accelerated implementation
that set on top of OpenGL.

00:40:49.740 --> 00:40:52.700
That was really terrific,
terrific implementation.

00:40:52.700 --> 00:40:53.500
It was fast.

00:40:53.500 --> 00:40:55.000
It was incredibly fast.

00:40:55.120 --> 00:40:59.730
However, the problem with it was
that when it worked,

00:40:59.730 --> 00:41:00.680
it worked.

00:41:00.680 --> 00:41:03.630
And it worked only 90% of the time.

00:41:03.690 --> 00:41:07.750
And to get the rest,
10%, is really difficult for us.

00:41:07.920 --> 00:41:09.160
We were making strides.

00:41:09.210 --> 00:41:10.320
We're continuing.

00:41:10.320 --> 00:41:11.820
And we're making progress.

00:41:11.830 --> 00:41:15.920
However, we really could not nail
down the correctness.

00:41:15.920 --> 00:41:22.950
So when we moved to Java 1.4.1,
we completely re-heterotectured our code.

00:41:23.180 --> 00:41:25.490
We moved from Carbon to Cocoa.

00:41:25.510 --> 00:41:29.520
And the lessons that we
learned in 1.3.1 was,

00:41:29.940 --> 00:41:33.300
first of all,
if we cannot do hardware acceleration,

00:41:33.300 --> 00:41:35.920
we need to have something
we can fall back on.

00:41:35.920 --> 00:41:37.890
And that is something
called a software renderer.

00:41:37.910 --> 00:41:42.650
So when we moved to 1.4.1, we decided,
let's start, let's nail it down.

00:41:42.650 --> 00:41:45.940
Let's have terrific software
implementation as far as

00:41:45.940 --> 00:41:47.700
correctness is concerned.

00:41:47.870 --> 00:41:51.120
Then, in the future,
once we have that done,

00:41:51.120 --> 00:41:55.320
we'll be looking for new technologies
emerging right here within Apple.

00:41:55.680 --> 00:41:57.600
And then we'll evaluate them.

00:41:57.600 --> 00:42:00.920
And then we'll see which
one works for us the best.

00:42:01.210 --> 00:42:03.590
And then we'll adopt that technology.

00:42:03.590 --> 00:42:07.660
So we are right now, at this point,
we are still at the transition

00:42:07.760 --> 00:42:10.060
point where we're in 1.4.1.

00:42:10.060 --> 00:42:11.050
We have brand new code.

00:42:11.320 --> 00:42:14.990
There is not even one line of
code that we share with 1.3.1.

00:42:14.990 --> 00:42:15.640
It's brand new.

00:42:15.640 --> 00:42:17.620
Everything is written from scratch.

00:42:17.750 --> 00:42:20.160
But we want to nail
the correctness first.

00:42:20.430 --> 00:42:22.900
And, of course,
we are keeping our eyes open on

00:42:22.900 --> 00:42:27.500
what is going on around us and
what technologies we can use later.

00:42:31.390 --> 00:42:36.100
So in 1.4.1, the Java update that
you guys have access to,

00:42:36.260 --> 00:42:39.540
first of all,
our priorities were correctness.

00:42:39.920 --> 00:42:44.210
Second,
we also didn't really want to neglect

00:42:44.490 --> 00:42:46.300
the Java graphics optimizations.

00:42:46.300 --> 00:42:49.220
As you all know,
1.4.1 is not a speed demon as

00:42:49.270 --> 00:42:51.040
far as graphics is concerned.

00:42:51.040 --> 00:42:56.240
So we worked on really on very
basic architectural optimization

00:42:56.240 --> 00:42:59.180
techniques that we could put in there.

00:42:59.180 --> 00:43:02.470
And right now,
we came up with three of them.

00:43:02.550 --> 00:43:06.020
That's lazy drawing,
lazy pixel conversion,

00:43:06.140 --> 00:43:07.300
and lazy state management.

00:43:09.110 --> 00:43:13.280
What lazy drawing is about
is we simply collect all your

00:43:13.280 --> 00:43:16.100
primitives that you want to draw.

00:43:16.100 --> 00:43:18.640
We put them aside in a queue in a cache.

00:43:18.640 --> 00:43:22.050
And when the time comes to draw them
to the screen or into your image,

00:43:22.050 --> 00:43:24.570
it's only then when we
transition from Java,

00:43:24.570 --> 00:43:27.230
we go to the native,
then we process that queue.

00:43:27.350 --> 00:43:31.140
The good thing about this lazy drawing
implementation that we have right

00:43:31.140 --> 00:43:34.340
now is that it's future compatible.

00:43:34.340 --> 00:43:36.970
Whatever technology
we choose to use next,

00:43:37.030 --> 00:43:40.820
this lazy drawing implementation
will work with it.

00:43:40.820 --> 00:43:43.970
And we work with Core Graphics guys,
and we make sure that whatever we do

00:43:43.970 --> 00:43:49.620
with our lazy drawing optimization
will not break them in any way.

00:43:50.260 --> 00:43:52.960
Second, lazy pixel conversion.

00:43:53.470 --> 00:43:57.590
There are certain image types
that Java provides the access to

00:43:57.680 --> 00:43:59.360
that are not supported natively.

00:43:59.480 --> 00:44:04.110
So what that means is if we want
to do something with that image,

00:44:04.350 --> 00:44:07.720
the pixels are in format that
are not understood natively.

00:44:07.720 --> 00:44:09.710
We have to convert them.

00:44:09.820 --> 00:44:13.500
If we didn't do this optimization,
drawing of images or drawing

00:44:13.500 --> 00:44:16.640
into images would be terribly,
terribly slow.

00:44:16.780 --> 00:44:22.110
So what lazy pixel conversion is,
is simply a technique of converting

00:44:22.110 --> 00:44:26.230
the pixels only when it's necessary.

00:44:27.620 --> 00:44:31.360
And then, thirdly, lazy state management.

00:44:31.360 --> 00:44:35.700
A graphics context has multiple
different states that you can set.

00:44:35.880 --> 00:44:38.220
Transformations, color.

00:44:38.250 --> 00:44:43.330
And what this is about,
this optimization techniques

00:44:43.330 --> 00:44:49.290
will simply let us set only those
states that have actually changed.

00:44:49.880 --> 00:44:53.840
We are not quite done
with this optimization.

00:44:53.970 --> 00:44:55.440
We are only part way.

00:44:55.790 --> 00:45:00.700
So unfortunately, at this point,
whenever you change most

00:45:00.700 --> 00:45:03.400
of the graphic states,
we have to slam all the other

00:45:03.400 --> 00:45:04.770
ones as well at that time.

00:45:05.070 --> 00:45:08.740
That is terribly inefficient,
but we are working on that.

00:45:09.760 --> 00:45:15.300
So here is one benchmark,
micro benchmark, to show you.

00:45:15.300 --> 00:45:20.650
This one,
the scores show simply basically

00:45:20.740 --> 00:45:25.020
performance of lazy drawing optimization.

00:45:25.020 --> 00:45:30.720
So what you have in your hands
with our initial one-for-one

00:45:31.090 --> 00:45:36.960
release was the base of 100,
and what you have right now,

00:45:36.960 --> 00:45:42.540
you have 175 score,
which is 75% increase.

00:45:42.540 --> 00:45:44.500
That's not too bad.

00:45:44.500 --> 00:45:47.140
We're not done with this by any means.

00:45:47.140 --> 00:45:48.140
And second, Robocode.

00:45:48.140 --> 00:45:50.720
That's real world application.

00:45:50.720 --> 00:45:54.860
There's interesting story behind this.

00:45:54.860 --> 00:45:57.380
At the time when we
were working on 1.3.1,

00:45:57.380 --> 00:45:59.440
Robocode was running pretty darn slow.

00:45:59.440 --> 00:46:02.980
And we went to the developer,
and I think we made a mistake,

00:46:03.050 --> 00:46:06.280
because we told him, look,
the image format that you're

00:46:06.280 --> 00:46:10.550
using is not fast with our
current implementation in 1.3.1.

00:46:10.670 --> 00:46:14.340
Why don't you use the image
format that we support natively,

00:46:14.540 --> 00:46:16.980
and we'll speed up your application?

00:46:17.060 --> 00:46:18.580
Well, they listened to us.

00:46:18.580 --> 00:46:23.220
They changed it, and yes,
they saw the performance

00:46:23.320 --> 00:46:26.630
improvement in 1.3.1.

00:46:26.820 --> 00:46:29.620
However, when we moved to 1.1,
underneath we used

00:46:29.620 --> 00:46:32.750
different implementation,
different techniques,

00:46:32.750 --> 00:46:34.540
different technology.

00:46:34.540 --> 00:46:38.120
And the Robocode score plummeted down.

00:46:38.210 --> 00:46:43.580
The problem was that the
image format was hard coded.

00:46:44.070 --> 00:46:49.300
So right now what we tried to do was,
there are two things that

00:46:49.300 --> 00:46:50.900
went wrong in Robocode.

00:46:50.900 --> 00:46:57.140
First of all, our lazy pixel conversion
had not very efficient

00:46:57.730 --> 00:46:59.600
and the rest of the team.

00:47:27.600 --> 00:47:41.600
We're getting much closer to full
frame rates that I've seen before.

00:47:41.600 --> 00:47:45.440
And if you remember on the 1.4.1 release,
we were getting about

00:47:45.440 --> 00:47:46.830
four frames a second.

00:47:46.840 --> 00:47:50.490
And that's because we were doing all
that conversion right on the fly.

00:47:50.500 --> 00:47:54.760
And now, if I just start this battle up,
we should be getting something closer

00:47:54.760 --> 00:47:57.220
to around 30 or so frames per second.

00:47:57.340 --> 00:48:00.620
We're getting about 32 and
we may even get above there.

00:48:00.620 --> 00:48:03.040
Right now it's hard locked to 30.

00:48:03.040 --> 00:48:07.470
So if I go up to maximum,
we might even break 30 and get some more.

00:48:07.550 --> 00:48:09.380
But we're right around 30.

00:48:09.380 --> 00:48:13.000
And you can see here that we're not
even using both of the processors.

00:48:13.000 --> 00:48:15.410
And there's something else going
on here that you don't actually

00:48:15.410 --> 00:48:18.150
know is that we're running one
of the really cool demos in the

00:48:18.160 --> 00:48:22.910
background that you'll see later in
the session on this same machine.

00:48:22.940 --> 00:48:24.320
So it's actually got a lot of extra time.

00:48:24.320 --> 00:48:27.040
I could start my word processor and
we'd still keep our 30 frames a second.

00:48:27.040 --> 00:48:30.840
And I also like to do this.

00:48:30.840 --> 00:48:33.320
Let's restart it again and
turn on my favorite option,

00:48:33.320 --> 00:48:36.160
which is showing where
the robots are scanning.

00:48:37.700 --> 00:48:56.200
[Transcript missing]

00:49:00.540 --> 00:49:03.400
And the future.

00:49:03.400 --> 00:49:06.160
First of all,
we have lazy state management to finish.

00:49:06.210 --> 00:49:08.400
This should give us
pretty nice improvement.

00:49:08.400 --> 00:49:11.220
Then there's more
optimization that we can do.

00:49:11.530 --> 00:49:16.400
We have already tried implementing some
of our lazy pixel conversion filters

00:49:16.400 --> 00:49:21.400
using the multiprocessors that are now
available in many of our computers.

00:49:21.400 --> 00:49:23.080
And we're not done with it.

00:49:23.080 --> 00:49:27.130
We're just testing, playing around,
seeing how much improvement

00:49:27.130 --> 00:49:28.400
that can give us.

00:49:28.400 --> 00:49:31.400
But that's one of the
things we're looking at.

00:49:31.400 --> 00:49:33.370
Also, all the back optimizations.

00:49:33.390 --> 00:49:38.240
So there's still quite a bit,
quite a few technologies that we can

00:49:38.240 --> 00:49:41.400
use to make Java graphics go in faster.

00:49:41.980 --> 00:49:45.780
And then we are talking
to science engineers.

00:49:45.920 --> 00:49:49.660
When we are working on our
lazy drawing optimization,

00:49:49.690 --> 00:49:51.380
we actually went to them.

00:49:51.380 --> 00:49:55.680
We went to Java 2D graphics engineers,
and we told them, listen, guys,

00:49:55.950 --> 00:49:57.670
this is what we're thinking of doing.

00:49:57.710 --> 00:49:58.700
What do you think?

00:49:58.700 --> 00:50:00.120
Is it going to work?

00:50:00.120 --> 00:50:01.320
Is it not going to work?

00:50:01.420 --> 00:50:05.520
Do you have any other
cool ideas that we may do?

00:50:05.520 --> 00:50:09.260
And they loved our ideas, and they said,
yeah, go for it.

00:50:09.260 --> 00:50:10.590
We don't even have it.

00:50:10.600 --> 00:50:14.770
So we've done something that
they wished they had it.

00:50:14.780 --> 00:50:17.010
So we are definitely doing
some interesting things.

00:50:18.600 --> 00:51:30.600
[Transcript missing]

00:51:30.880 --> 00:51:32.370
They're looking into OpenGL.

00:51:32.570 --> 00:51:38.790
They're looking into API,
2D API that sits on top of OpenGL.

00:51:39.230 --> 00:51:43.900
That's very similar to the hardware
acceleration that we had in 1.3.1,

00:51:43.900 --> 00:51:47.430
but better, because we are not the
only client of that.

00:51:47.460 --> 00:51:49.340
We don't have to support it.

00:51:49.550 --> 00:51:53.580
It would be system-wide,
and if they come through,

00:51:53.700 --> 00:51:56.920
if they have that working,
then that's definitely something

00:51:56.920 --> 00:52:01.070
that we would like our code work
with them and take advantage of that.

00:52:01.080 --> 00:52:05.240
So we'll be looking very closely at
what CoreGraphics guys will be doing,

00:52:05.280 --> 00:52:08.900
and certainly taking advantage
of any code technologies

00:52:08.910 --> 00:52:10.710
that they have to offer.

00:52:13.180 --> 00:52:18.790
If there's one thing that I would like
you guys to take out of this session is

00:52:19.310 --> 00:52:21.440
What to do about the images.

00:52:21.440 --> 00:52:26.770
If you have to draw into buffered image,
how do you determine what is the correct,

00:52:26.950 --> 00:52:32.950
what is the fastest image
type that you guys should use?

00:52:32.950 --> 00:52:32.950
And, you know,

00:52:36.780 --> 00:52:39.840
And this is the most important thing.

00:52:39.840 --> 00:52:43.690
Please do not hard code
proper image types.

00:52:43.710 --> 00:52:48.420
That's an example of a code
that would be hard coding it.

00:52:48.420 --> 00:52:52.500
What you can do is you
can ask the system.

00:52:54.170 --> 00:52:56.330
for Compatible Image.

00:52:56.420 --> 00:53:00.400
Now, if you do this,
then no matter what technology

00:53:00.400 --> 00:53:03.900
we'll use in the future,
you're guaranteed that you will be

00:53:03.900 --> 00:53:08.600
given a buffered image type that
will be the fastest on our platform.

00:53:08.600 --> 00:53:11.180
That is very, very important.

00:53:11.380 --> 00:53:13.790
And one more point here.

00:53:13.960 --> 00:53:15.730
If you have a choice,
you have the option,

00:53:15.800 --> 00:53:18.360
ask for a volatile image.

00:53:18.360 --> 00:53:21.660
This will be - will have them
part of Accelerated soon,

00:53:21.800 --> 00:53:22.790
hopefully.

00:53:22.810 --> 00:53:26.650
So if you have the choice,
get a volatile image.

00:53:27.450 --> 00:53:32.780
Now, there's one misconception
among some of you with respect

00:53:32.780 --> 00:53:35.820
to indexed color formats.

00:53:35.820 --> 00:53:39.080
On other platforms,
they're very fast and they're

00:53:39.080 --> 00:53:40.800
also conserved memory.

00:53:40.820 --> 00:53:44.380
So using indexed format is
a way of compressing the

00:53:44.380 --> 00:53:46.760
pixel data using less memory.

00:53:46.760 --> 00:53:49.990
Unfortunately, on Mac OS X,
they're not supported natively.

00:53:50.000 --> 00:53:54.860
So what we have to do internally
to support that image format is to

00:53:54.860 --> 00:53:59.740
create brand new buffer just to have
those pixels converted in a format

00:53:59.740 --> 00:54:02.140
that we can understand natively.

00:54:02.140 --> 00:54:04.180
And then that's the way we can use them.

00:54:04.180 --> 00:54:10.800
So indexed color format images on
Mac OS X do not use less memory.

00:54:10.800 --> 00:54:13.180
On the contrary,
they use more and they're slower.

00:54:14.970 --> 00:54:18.380
If you have to use them,
you have no choice.

00:54:18.420 --> 00:54:23.990
But if you do need to use them,
use other image formats.

00:54:23.990 --> 00:54:28.130
And it's very easy, very often,
for you just to see.

00:54:28.250 --> 00:54:29.450
You don't need to do a lot.

00:54:29.490 --> 00:54:32.260
Just change the buffer image format type.

00:54:32.280 --> 00:54:37.550
And second, and that's very important,
the most optimal image

00:54:37.550 --> 00:54:39.800
format is not hard coded.

00:54:39.950 --> 00:54:40.770
It can change.

00:54:40.770 --> 00:54:44.280
It can vary from machine to machine.

00:54:44.460 --> 00:54:48.600
If we were to move again to a
technology that uses OpenGL,

00:54:48.620 --> 00:54:54.270
that is very dependent on a video
graphics card you have in your system.

00:54:54.490 --> 00:54:58.220
Also, it is very important then to us
to see what is the resolution

00:54:58.220 --> 00:55:01.120
of the monitor you're running,
what is the depth of

00:55:01.120 --> 00:55:02.280
screen you're running on.

00:55:02.280 --> 00:55:06.260
So there will not be one and only
one image format that is the best,

00:55:06.260 --> 00:55:07.280
the fastest.

00:55:07.280 --> 00:55:08.260
It will change.

00:55:08.280 --> 00:55:14.410
And you need to keep that in mind
if you're writing for Mac OS X.

00:55:18.620 --> 00:55:22.170
Now, if you really need to know what
are the natively supported image

00:55:22.240 --> 00:55:25.620
formats at this particular time,
and this may not hold even

00:55:25.620 --> 00:55:30.110
in the next few months,
this may change, but at this point,

00:55:30.110 --> 00:55:32.900
only four image types
are supported natively.

00:55:32.900 --> 00:55:35.100
Those are the fastest.

00:55:35.100 --> 00:55:37.620
And those are the image types
into which you can draw,

00:55:37.620 --> 00:55:39.700
which means they're at the destination.

00:55:39.800 --> 00:55:43.030
You can create a context
by perfect image,

00:55:43.030 --> 00:55:46.910
create context, get context,
get graphics.

00:55:47.120 --> 00:55:49.690
So those only four are
natively supported.

00:55:50.050 --> 00:55:52.630
Those are the pastest at this point.

00:55:54.030 --> 00:55:57.200
If you need to draw an
image somewhere else,

00:55:57.230 --> 00:56:00.310
meaning the image you have, the pixels,
are source,

00:56:00.570 --> 00:56:07.240
then the natively supported image formats
are a separate set of the destination.

00:56:07.240 --> 00:56:10.830
We have one more image format that
we can support natively as a source,

00:56:10.830 --> 00:56:14.120
and that is ARGB alpha non-premultiplied.

00:56:14.120 --> 00:56:16.900
That is, by the way,
the image format that Robocode uses.

00:56:16.900 --> 00:56:21.850
And we have added special optimization
in our lazy pixel conversion that

00:56:21.850 --> 00:56:25.420
actually allows the pixels to
know whether they're in native

00:56:25.510 --> 00:56:27.400
format or they're in Java format.

00:56:27.450 --> 00:56:31.380
And then based on that,
we have two different CG image revs.

00:56:31.510 --> 00:56:34.380
And then we can just switch
very fast between two of them.

00:56:34.380 --> 00:56:40.410
And we can choose the
pixels that are up to date.

00:56:44.380 --> 00:56:49.480
And here is some techniques
for the rendering.

00:56:51.150 --> 00:56:53.760
This is important in
our platform as well.

00:56:53.890 --> 00:56:56.500
What we have missing,
one of our technologies

00:56:56.500 --> 00:57:02.390
that we had in 1.3.1,
allowed you to draw very...

00:57:02.800 --> 00:57:42.400
[Transcript missing]

00:57:44.640 --> 00:57:49.060
Now,
this is for those of you who really need

00:57:49.190 --> 00:57:53.160
the fastest access to the image pixels,
for whatever reason.

00:57:53.160 --> 00:57:58.650
If you're writing an image
manipulation program like Photoshop,

00:57:58.740 --> 00:58:02.600
something like that,
then if an image is supported natively,

00:58:02.600 --> 00:58:06.260
unfortunately for you,
there's no way to determine

00:58:06.490 --> 00:58:08.960
whether a certain image type
is supported natively or not.

00:58:09.020 --> 00:58:09.940
It may change in the future.

00:58:09.940 --> 00:58:11.540
It's constant.

00:58:11.540 --> 00:58:13.740
Only at this point, it may change.

00:58:13.740 --> 00:58:17.080
However,
if for some reason you need to do that,

00:58:17.080 --> 00:58:19.840
and you know the image
format is supported natively,

00:58:20.070 --> 00:58:24.790
what you can do is grab pixels
directly from data buffer.

00:58:26.060 --> 00:58:29.940
On a non-natively supported image,
you do not want to

00:58:29.940 --> 00:58:31.400
access pixels directly.

00:58:31.400 --> 00:58:35.470
If you do, we have to turn lazy pixel
conversion optimization off,

00:58:35.590 --> 00:58:38.820
because the time,
the second you touch pixels,

00:58:38.820 --> 00:58:40.840
those pixels are stolen.

00:58:40.840 --> 00:58:42.960
You can have access to them.

00:58:43.020 --> 00:58:46.010
We do not know when you look
at them or when you use them.

00:58:46.010 --> 00:58:50.200
We have to do the conversion from
native to Java every single operation.

00:58:50.460 --> 00:58:53.840
So you do not want to
touch pixels directly on a

00:58:53.840 --> 00:58:55.930
non-natively supported image.

00:58:55.940 --> 00:59:01.220
So go through graphics object
and draw to it that way.

00:59:01.220 --> 00:59:04.230
Auto optimisation tips and techniques.

00:59:04.230 --> 00:59:05.800
This comes from my old

00:59:07.140 --> 00:59:11.510
All the work I've done on application
was DNA sequencing application,

00:59:11.510 --> 00:59:15.640
and when I was trying to optimize it,
here are the few things that I found

00:59:15.700 --> 00:59:17.810
that helped that application.

00:59:20.870 --> 00:59:23.120
First of all, avoid creating new objects
in your paint method.

00:59:23.310 --> 00:59:25.100
Obvious, this applies to all platforms.

00:59:25.100 --> 00:59:26.440
Don't create new fonts.

00:59:26.440 --> 00:59:28.570
Don't create rectangles
if you need to use them,

00:59:28.720 --> 00:59:32.450
and manipulate them later
on for determining the clip.

00:59:32.630 --> 00:59:36.090
Don't create any objects in paint method.

00:59:36.420 --> 00:59:39.240
Use simple primitive instead of shapes.

00:59:39.400 --> 00:59:41.610
We have done our lazy
drawing optimization,

00:59:41.610 --> 00:59:44.300
actually attempts to do
that automatically for you.

00:59:44.620 --> 00:59:48.300
However, it would be,
if you have the choice on Mac OS,

00:59:48.300 --> 00:59:55.290
then it is faster to drop the
primitives directly using full reg 00,

00:59:55.300 --> 01:00:01.000
you know, xy, width, height,
as opposed to creating a rect 2D object.

01:00:01.320 --> 01:00:05.540
Use polyline instead of
drawing lines one at a time.

01:00:05.540 --> 01:00:09.590
It simply avoids the crossing of
the native from Java to native.

01:00:09.720 --> 01:00:14.600
And it's simply faster with current
core graphics implementation

01:00:14.600 --> 01:00:18.000
because we can build a complex
path if you have a polyline.

01:00:18.060 --> 01:00:21.350
Otherwise,
we have to draw lines one at a time,

01:00:21.400 --> 01:00:24.970
and core graphics is not
terribly good at that.

01:00:25.370 --> 01:00:27.960
This will not apply
probably to most of you,

01:00:27.960 --> 01:00:32.450
but if you have a limited alphabet
and you know you will not be

01:00:32.450 --> 01:00:35.560
drawing complex characters,
so if you're writing on text

01:00:35.660 --> 01:00:37.900
editor kind of application,
this will not apply.

01:00:37.900 --> 01:00:41.960
However, if you have a limited alphabet,
say, for letters,

01:00:41.960 --> 01:00:43.870
then maybe you can do that.

01:00:43.920 --> 01:00:46.420
And the optimization
is you can use bytes,

01:00:46.420 --> 01:00:47.200
not chars.

01:00:47.200 --> 01:00:50.770
Chars are 16-bit,
and we do not know whether it could

01:00:50.770 --> 01:00:52.890
be a Unicode character or not.

01:00:52.910 --> 01:00:55.320
If it is,
then we have to go through this more

01:00:55.440 --> 01:00:57.900
complex path to draw Unicode characters.

01:00:57.900 --> 01:01:02.060
If it's a byte, then we know it falls
within the NASKE range,

01:01:02.060 --> 01:01:05.990
and then we can just bypass some of
the complex text drawing routines,

01:01:06.010 --> 01:01:11.340
and we can go straight to
Core Graphics to blit those characters.

01:01:11.500 --> 01:01:16.320
Use double buffering for your static
portions of your applications.

01:01:16.320 --> 01:01:20.480
That is - that will
apply to all platforms.

01:01:21.520 --> 01:01:24.800
And we have added, with this release,
we have added tons of runtime

01:01:24.800 --> 01:01:28.190
options for you guys to play around,
to turn them on and off.

01:01:28.200 --> 01:01:31.200
You can turn off the optimizations
that we provided for you guys.

01:01:31.200 --> 01:01:36.580
You can turn on and off rendering
of lines or rectangles or shapes.

01:01:36.690 --> 01:01:39.730
You can use all of those runtime
options to narrow down and to

01:01:39.730 --> 01:01:44.500
find out what is the problem with
your application if you have one.

01:01:44.590 --> 01:01:48.790
So now for the demo, I'd like to welcome
Ken Russell from Sun Microsystem.

01:01:55.900 --> 01:01:59.540
A couple of weeks ago at Java 1,
Sun announced the new

01:01:59.540 --> 01:02:01.180
Java Gaming Initiative.

01:02:01.180 --> 01:02:05.530
And one of the products of this
initiative is a new OpenGL binding

01:02:05.530 --> 01:02:08.980
for the Java platform called JOGL.

01:02:08.980 --> 01:02:12.840
And JOGL is open source,
and you can download the source

01:02:12.840 --> 01:02:15.120
code right now on java.net.

01:02:15.140 --> 01:02:18.130
So just go to java.net,
search for the project name,

01:02:18.310 --> 01:02:19.290
and you can get it.

01:02:19.300 --> 01:02:22.920
And thanks to Gerard and
a couple of all-nighters,

01:02:22.920 --> 01:02:25.100
JOGL is now running on OS X.

01:02:25.100 --> 01:02:28.860
It's running on the developer preview
that you've got with your 10.3 CDs,

01:02:28.860 --> 01:02:32.300
and it's not going to run on any
earlier versions of Java for OS X,

01:02:32.320 --> 01:02:33.770
so keep that in mind.

01:02:33.830 --> 01:02:37.440
But going forward, it will work,
and it will be fast and robust.

01:02:37.440 --> 01:02:41.300
So we've got a couple of
very cool demos to show you.

01:02:41.300 --> 01:02:42.820
This one is very special.

01:02:42.820 --> 01:02:46.910
This is Doby the dog,
and Doby was developed by the Synthetic

01:02:46.910 --> 01:02:50.420
Characters Group at the MIT Media Lab.

01:02:50.420 --> 01:02:52.620
And Doby is completely autonomous.

01:02:52.620 --> 01:02:54.300
He perceives his environment.

01:02:54.300 --> 01:02:57.660
He has his own internal
motivations and desires.

01:02:57.660 --> 01:03:01.710
And you can actually train Doby in the
same way that you would train a real dog.

01:03:01.740 --> 01:03:05.640
You can sort of lure him around
and show him new motions to do.

01:03:05.670 --> 01:03:09.120
You can reward him by giving him
a little click with a clicker.

01:03:09.180 --> 01:03:11.540
You can, in some sense,
scold him by ignoring him when he

01:03:11.540 --> 01:03:15.580
does a behavior that you don't like.

01:03:15.580 --> 01:03:18.880
And basically,
Doby represents -- or at least it's a

01:03:18.900 --> 01:03:21.950
safe thing to say that Doby is pretty
much a state of the art in interactive

01:03:22.040 --> 01:03:23.500
animated characters that can learn.

01:03:23.500 --> 01:03:29.780
And you can read more about Doby in
the paper on him in SIGGRAPH 2002.

01:03:29.780 --> 01:03:32.860
Now, Doby, it turns out,
is written almost entirely in

01:03:32.890 --> 01:03:36.810
the Java programming language,
with a little bit of native

01:03:36.810 --> 01:03:41.620
code around the outside to
get the custom device inputs.

01:03:41.760 --> 01:03:44.740
He uses some of the more
advanced OpenGL techniques,

01:03:44.740 --> 01:03:47.470
like vertex shaders,
to do the shadow that you see here,

01:03:47.470 --> 01:03:49.610
and the cartoon-like shading
around the edges of the dog.

01:03:49.700 --> 01:03:55.840
This demo runs at over 50 frames
a second on a dual processor G4.

01:03:55.840 --> 01:03:58.290
And I should mention that the
synthetic characters group is

01:03:58.290 --> 01:04:01.400
a big OS X development house,
and so they do all of their development

01:04:01.780 --> 01:04:03.690
at this point with Java on OS X.

01:04:04.700 --> 01:04:07.160
This is the first demo that
they've actually had to slow down,

01:04:07.160 --> 01:04:08.700
because it was running too fast.

01:04:08.700 --> 01:04:11.700
So it's actually slowed
down to 30 frames a second,

01:04:11.700 --> 01:04:16.700
and because the G4s are so fast,
the G5s will be even better.

01:04:16.700 --> 01:04:19.080
So we're not actually going
to train Doby right now,

01:04:19.080 --> 01:04:22.700
he's just going through his paces,
but you can sort of see what's going on.

01:04:22.700 --> 01:04:27.680
There's skinning going on,
there's action selection,

01:04:27.700 --> 01:04:30.700
and this is running on top
of the JOGL binding for OS X.

01:04:30.700 --> 01:04:33.660
And also notice the CPU usage
in the bottom left corner.

01:04:33.700 --> 01:04:38.020
There's almost nothing going on in there,
everything goes through

01:04:38.030 --> 01:04:39.700
Video Graphics Card.

01:04:39.700 --> 01:04:40.480
Yep.

01:04:40.500 --> 01:04:42.500
So, cool stuff.

01:04:42.700 --> 01:04:44.700
Okay.

01:04:48.800 --> 01:04:52.000
Okay,
so now here's another demonstration.

01:04:52.140 --> 01:04:57.590
This is a demo by NVIDIA Corporation that
we've ported from C to Java.

01:05:08.000 --> 01:05:11.360
Okay,
now this is not real-time ray tracing.

01:05:11.460 --> 01:05:15.500
This is using a couple of tricks
to get hardware acceleration

01:05:15.500 --> 01:05:19.000
for this technique of rendering
glass with prismatic effects.

01:05:19.110 --> 01:05:22.260
So there is actually in some sense,
many of you I'm sure are familiar

01:05:22.260 --> 01:05:25.920
with the technique of ray tracing
where you send a ray of light

01:05:25.930 --> 01:05:28.000
out the camera into the scene.

01:05:28.000 --> 01:05:32.960
And that is in fact being done at
every vertex on this wireframe model,

01:05:33.030 --> 01:05:36.180
but the trick is that it's being
done on the graphics card by

01:05:36.180 --> 01:05:41.000
what's called a vertex shader,
or a vertex program.

01:05:41.050 --> 01:05:44.230
This is a tiny little assembly
language program that is actually

01:05:44.270 --> 01:05:47.740
uploaded to the graphics card when
the demo starts up that tells it,

01:05:47.880 --> 01:05:52.000
okay, we're going to take the camera's
position and the vertex's position

01:05:52.000 --> 01:05:56.000
and the surface normal and figure out
where should the reflected ray go,

01:05:56.100 --> 01:05:58.000
and where should the refracted
ray go through the surface normal.

01:05:58.000 --> 01:06:00.000
And then it's going to
go through the object.

01:06:00.000 --> 01:06:02.000
And basically it looks up in
the surrounding environment,

01:06:02.000 --> 01:06:06.860
this street scene,
where's the right texture coordinate

01:06:06.860 --> 01:06:08.990
for where the ray intersects the world.

01:06:09.000 --> 01:06:13.150
And basically what it's doing is
distorting the background texture in

01:06:13.150 --> 01:06:18.000
such a way on a per vertex basis that it
looks like the thing's made out of glass.

01:06:18.000 --> 01:06:21.050
So it's not doing it at every pixel,
it's doing it at every vertex,

01:06:21.050 --> 01:06:24.000
but it's close enough that
it's really indistinguishable.

01:06:24.000 --> 01:06:24.870
Another cool trick here is that
you notice that we just talked

01:06:24.890 --> 01:06:26.000
about the fact that the camera is
not actually doing the same thing.

01:06:26.000 --> 01:06:26.380
So it's not doing it at every pixel,
it's doing it at every vertex,

01:06:26.400 --> 01:06:26.990
but it's close enough that
it's really indistinguishable.

01:06:27.010 --> 01:06:27.390
Another cool trick here is that
you notice that we just talked

01:06:27.390 --> 01:06:28.000
about the fact that the camera is
not actually doing the same thing.

01:06:28.000 --> 01:06:31.360
turned off the fringe effects,
what's going on is that we're rendering

01:06:31.370 --> 01:06:35.430
the scene three different times,
each with a slightly different

01:06:35.430 --> 01:06:37.800
refractive index for the glass.

01:06:37.800 --> 01:06:40.590
And that makes the refracted ray go
into a slightly different position in

01:06:40.590 --> 01:06:42.530
the surrounding environment each time.

01:06:42.550 --> 01:06:44.830
Then those three things
are added together,

01:06:44.830 --> 01:06:47.740
again, on the graphics card,
and you get the -- what

01:06:47.790 --> 01:06:50.380
basically looks like a prism.

01:06:50.380 --> 01:06:52.770
So, I'd like to point out that
the same binary for this

01:06:52.770 --> 01:06:57.940
demonstration runs on OS X,
it runs on Linux, and it runs on Windows.

01:06:57.940 --> 01:07:01.320
And it runs at 100% of the
speed of the analogous C++ code.

01:07:01.320 --> 01:07:06.040
Remember, this was a port,
not a new demo.

01:07:06.040 --> 01:07:08.960
So, basically,
we are here with respect to

01:07:08.960 --> 01:07:13.380
OpenGL performance in Java,
and it's running on OS X.

01:07:13.450 --> 01:07:14.560
It looks great.

01:07:14.560 --> 01:07:17.080
So, go out and develop cool stuff.

01:07:24.720 --> 01:07:28.500
We don't have Java 3D for you guys yet,
but if you really need

01:07:28.600 --> 01:07:35.460
to use 3D graphics,
then please use Joggle.

01:07:35.460 --> 01:07:37.240
And some of you might be
familiar with Geo4Java,

01:07:37.420 --> 01:07:42.500
which is very similar product technology,
also Open Geo binding.

01:07:42.500 --> 01:07:45.690
Geo4Java doesn't support the
latest Open Geo standard,

01:07:45.690 --> 01:07:47.480
doesn't give you access to pixel shaders.

01:07:47.500 --> 01:07:48.850
Joggle does.

01:07:48.890 --> 01:07:52.730
So if you want to have 3D
graphics on Mac OS X using Java,

01:07:52.730 --> 01:07:53.420
you can.