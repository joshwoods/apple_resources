# Wwdc2022 10026

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Add Live Text interaction to your appLearn how you can bring Live Text support for still photos or paused video frames to your app. We'll share how you can easily enable text interactions, translation, data detection, and QR code scanning within any image view on iOS, iPadOS, or macOS. We'll also go over how to control interaction types, manage the supplementary interface, and resolve potential gesture conflicts.

To learn more about capturing and interacting with detected data in live camera feeds, watch "Capture machine-readable codes and text with VisionKit" from WWDC22.ResourcesEnabling Live Text interactions with imagesHD VideoSD VideoRelated VideosWWDC23What’s new in VisionKitWWDC22Capture machine-readable codes and text with VisionKitCreate a great video playback experience

Learn how you can bring Live Text support for still photos or paused video frames to your app. We'll share how you can easily enable text interactions, translation, data detection, and QR code scanning within any image view on iOS, iPadOS, or macOS. We'll also go over how to control interaction types, manage the supplementary interface, and resolve potential gesture conflicts.

To learn more about capturing and interacting with detected data in live camera feeds, watch "Capture machine-readable codes and text with VisionKit" from WWDC22.

Enabling Live Text interactions with images

HD VideoSD Video

HD Video

SD Video

What’s new in VisionKit

Capture machine-readable codes and text with VisionKit

Create a great video playback experience

Search this video…♪ Mellow instrumental hip-hop music ♪♪Hi! My name is Adam Bradford.I'm an engineer on the VisionKit team,and if you're looking to add Live Text to your app,you're in the right place.But first, what is Live Text?Live Text analyzes an image and provides features for the usersto interact with its content, such selecting and copying text,perform actions like lookup and translate,providing data-detection workflows,such as mapping an address, dialing a number,or jumping to a URL.Live Text even allows for QR code interaction.Imagine how you could put this to use in your app?You want to know more? Well, you're in the right place.For this session, I'm going to start with a general overviewof the Live Text API.Then I will explore how to implement this APIin an existing application.Next, I will dive into some tips and trickswhich may help you when adding Live Text to your app.Now for an overview of the Live Text API.At a high level, the Live Text API is available in Swift.It works beautifully on static imagesand can be adapted to be used for paused video frames.If you need to analyze video in a live camera streamto search for items like text or QR codes,VisionKit also has a data scanner available.Check out this session from my colleague Ron for more info.The Live Text API is available starting on iOS 16for devices with an Apple Neural Engine,and for all devices that support macOS 13.It consists of four main classes.To use it, first, you'll need an image.This image is then fed into an ImageAnalyzer,which performs the async analysis.Once the analysis is complete,the resulting ImageAnalysis objectis provided to either an ImageAnalysisInteractionor ImageAnalysisOverlayView, depending on your platform.Seems pretty straightforward so far, right?Now, I'm going to demonstrate how one would add itto an existing application.And here's our application.This is a simple image viewer,which has an image view inside of a scroll view.Notice, I can both zoom and pan.But try as I might, I cannot select any of this textor activate any of these data detectors.This simply will not do.Here's the project in Xcode.To add Live Text to this application,I'll be modifying a view controller subclass.First, I'm going to need an ImageAnalyzer,and an ImageAnalysisInteraction.Here, I'm simply overriding viewDidLoadand adding the interaction to the imageview.Next, I need to know when to perform the analysis.Notice that when a new image is set,I first reset the preferredInteractionTypesand analysis which were meant for the old image.Now everything is ready for a new analysis.Next, I'm going to create the function we will useand then check that our image exists.If so, then create a task.Next, create a configuration in order to tell the analyzerwhat it should be looking for.In this case, I'll go with text and machine-readable codes.Generating the analysis can throw,so handle that as appropriate.And now finally, I'm ready to call the methodanalyzeImageWithConfiguration,which will start the analysis process.Once the analysis is complete,an indeterminate amount of time has passed,and the state of the application may have changed,so I will check that both the analysis was successfuland that the displayed image has not changed.If all of these checks pass,I can simply set the analysis on the interactionand set the preferredInteractionTypes.I'm using .automatic here,which will give me the default system behavior.I think this is ready for a test.Oh, look at that!I see the Live Text button has appeared, and yep,I can now select text.Notice how these interface elementsare positioned for me automatically,and keep their position inside of both the image boundsand the visible area, with no work on my part.OK, notice that tapping the Live Text buttonwill both highlight any selectable items,underline data detectors, and show Quick Actions.I can easily tap this Quick Action to make a call,and even see more options by long-pressing.You have to admit, this is pretty cool.With just these few lines of code,I've taken an ordinary image and brought it to life.This simple application now has the abilityto select text on images, activate data detectors,QR codes, lookup, translate text, and more.Not too shabby from just this few lines of code,if you ask me.And now that you've have seen how to implement Live Text,I'm going to go over a few tips and tricksthat may help you with your adoption.I'll start by exploring interaction types.Most developers will want .automatic,which provides text selection, but will also highlightdata detectors if the Live Text button is active.This will draw a line underneath any applicable detected itemsand allows one-tap access to activate them.This is the exact same behavior you would seefrom built-in applications.If it makes sense for your app to only have text selectionwithout data detectors, you may set the type to .textSelectionand it will not change with the state of the Live Text button.If, however, it makes sense for your appto only have data detectors without text selection,set the type to .dataDetectors.Note that in this mode, since selection is disabled,you will not see a Live Text button,but data detectors will be underlined and readyfor one-tap access.Setting the preferredInteractionTypesto an empty set will disable the interaction.And also, a last note, with text selection or automatic mode,you'll find you can still activate data detectorsby long-pressing.This is controlled by theallowLongPressForDataDetectorsIn TextMode property,which will be active when set to true, which the default.Simply set to false to disable this if necessary.I would like to now take a momentand talk about these buttons at the bottom,collectively known as the supplementary interface.This consists of the Live Text button,which normally lives in the bottom right-hand corner,as well as Quick Actions which appear on the bottom left.Quick Actions represent any data detectors from the analysisand are visible when the Live Text button is active.The size, position, and visibilityare controlled by the interaction.And while the default position and look matches the system,your app may have custom interface elementswhich may interfere or utilize different fontsand symbol weights.Let's look at how you can customize this interface.First off, the isSupplementary InterfaceHidden property.If I wanted to allow my app to still select textbut I did not want to show the Live Text button,if I set the SupplementaryInterfaceHiddento true, you will not see any Live Text buttonor Quick Actions.We also have a content insets property available.If you have interface elements that would overlapthe supplementary interface,you may adjust the content insetsso the Live Text button and Quick Actions adapt nicelyto your existing app content when visible.If your app is using a custom fontyou'd like the interface to adopt,setting the supplementaryInterfaceFontwill cause the Live Text button and Quick Actionsto use the specified font for textand font weight for symbols.Please note that for button-sizing consistency,Live Text will ignore the point size.Switching gears for a moment,if you are not using UIImageview,you may discover that highlights do not match up with your image.This is because with UIImageView,VisionKit can use its ContentMode propertyto calculate the contentsRect automatically for you.Here, the interaction's view has a bounds that is biggerthan its image content but is usingthe default's content rect, which is a unit rectangle.This is easily solved by implementing the delegate methodcontentsRectForInteractionand return a rectangle in unit coordinate spacedescribing how the image content relatesto the interaction's bounds in order to correct this.For example, returning a rectangle with these valueswould correct the issue,but please return the correct normalized rectanglebased on your app's current content and layout.contentsRectForInteraction will be calledwhenever the interaction's bounds change, however,if your contentsRect has changedbut your interaction's bounds have not,you can ask the interaction to update by callingsetContentsRectNeedsUpdate().Another question you may have when adopting Live Text may be,Where is the best place to put this interaction?Ideally, Live Text interactions are placed directly on the viewthat hosts your image content.As mentioned before, UIImageView will handlethe contentsRect calculations for you,and while not necessary, is preferred.If you are using UIImageview,just set the interaction on the imageViewand VisionKit will handle the rest.However, if your ImageView is locatedinside of a ScrollView,you may be tempted to place the interaction on the ScrollView,however, this is not recommended and could be difficult to manageas it will have a continually changing contentsRect.The solution here is the same,place the interaction on the viewthat hosts your image content,even if it is inside a ScrollViewwith magnification applied.I'm going talk about gestures for a moment,Live Text has a very, very rich set of gesture recognizers,to say to least.Depending on how your app is structured,it's possible you may find the interactionresponding to gestures and eventsyour app should really handle or vice versa.Don't panic.Here are a few techniques you can use to help correctif you see these issues occur.One common way to correct thisis to implement the delegate methodinteractionShouldBeginAtPointFor InteractionType.If you return false, the action will not be performed.A good place to start is to check if the interactionhas an interactive item at the given pointor if it has an active text selection.The text selection check is used hereso you will be able to have the ability tap off of the textin order to deselect it.On the other hand, if you find your interactiondoesn't seem to respond to gestures,it may be because there's a gesture recognizerin your app that's handling them instead.In this case, you can craft a similar solutionusing your gestureRecognizer's gestureRecognizerShouldBegindelegate method.Here, I perform a similar check and return falseif there is an interactive item at the locationor there's an active text selection.On a side note.In this example, I'm first convertingthe gestureRecognizer's location to the window's coordinate spaceby passing in nil,and then converting it to the interaction's view.This may be necessary if your interactionis inside of a ScrollView with magnification applied.If you find your points aren't matching up,give this technique a try.Another similar option I have found to be usefulis to override UIView's hitTest:WithEvent.Here, once again, similar story,I perform the same types of checks as before,and in this case, return the appropriate view.As always, we want your app to be as responsive as possible,and while the Neural Engine makes analysisextremely efficient, there a few ImageAnalyzer tipsI'd like to share for best performance.Ideally, you want only one ImageAnalyzershared in your app.Also, we support several types of images.You should always minimize image conversionsby passing in the native type that you have;however, if you do happen to have a CVPixelBuffer,that would be the most efficient.Also, in order to best utilize system resources,you should begin your analysis only when, or just before,an image appears onscreen.If your app's content scrolls --for example, it has a timeline --begin analysis only once the scrolling has stopped.Now this API isn't the only place you'll see Live Text,support is provided automaticallyin a few frameworks across the system your app may already use.For example, UITextField or UITextViewhave Live Text support using Camera for keyboard input.And Live Text is also supported in WebKit and Quick Look.For more information, please check out these sessions.New this year for iOS 16,we've added Live Text support in AVKit.AVPlayerView and ViewController support Live Textin paused frames automaticallyvia the allowsVideoFrameAnalysis property,which is enabled by default.Please note, this is only availablewith non-FairPlay protected content.If you're using AVPlayerLayer,then you are responsible for managing the analysisand the interaction but it is very important to use thecurrentlyDisplayedPixelBuffer propertyto get the current frame.This is the only way to guaranteethe proper frame is being analyzed.This will only return a valid valueif the video play rate is zero, and this is a shallow copyand absolutely not safe to write to.And once again, only availablefor non-FairPlay protected content.We are thrilled to help bring Live Text functionalityto your app.On behalf of everybody on the Live Text team,thank you for joining us for this session.I am stoked to see how you use it for images in your app.And as always, have fun!♪

♪ Mellow instrumental hip-hop music ♪♪Hi! My name is Adam Bradford.I'm an engineer on the VisionKit team,and if you're looking to add Live Text to your app,you're in the right place.But first, what is Live Text?Live Text analyzes an image and provides features for the usersto interact with its content, such selecting and copying text,perform actions like lookup and translate,providing data-detection workflows,such as mapping an address, dialing a number,or jumping to a URL.Live Text even allows for QR code interaction.Imagine how you could put this to use in your app?You want to know more? Well, you're in the right place.For this session, I'm going to start with a general overviewof the Live Text API.Then I will explore how to implement this APIin an existing application.Next, I will dive into some tips and trickswhich may help you when adding Live Text to your app.Now for an overview of the Live Text API.At a high level, the Live Text API is available in Swift.It works beautifully on static imagesand can be adapted to be used for paused video frames.If you need to analyze video in a live camera streamto search for items like text or QR codes,VisionKit also has a data scanner available.Check out this session from my colleague Ron for more info.The Live Text API is available starting on iOS 16for devices with an Apple Neural Engine,and for all devices that support macOS 13.It consists of four main classes.To use it, first, you'll need an image.This image is then fed into an ImageAnalyzer,which performs the async analysis.Once the analysis is complete,the resulting ImageAnalysis objectis provided to either an ImageAnalysisInteractionor ImageAnalysisOverlayView, depending on your platform.Seems pretty straightforward so far, right?Now, I'm going to demonstrate how one would add itto an existing application.And here's our application.This is a simple image viewer,which has an image view inside of a scroll view.Notice, I can both zoom and pan.But try as I might, I cannot select any of this textor activate any of these data detectors.This simply will not do.Here's the project in Xcode.To add Live Text to this application,I'll be modifying a view controller subclass.First, I'm going to need an ImageAnalyzer,and an ImageAnalysisInteraction.Here, I'm simply overriding viewDidLoadand adding the interaction to the imageview.Next, I need to know when to perform the analysis.

Notice that when a new image is set,I first reset the preferredInteractionTypesand analysis which were meant for the old image.Now everything is ready for a new analysis.Next, I'm going to create the function we will useand then check that our image exists.

If so, then create a task.Next, create a configuration in order to tell the analyzerwhat it should be looking for.In this case, I'll go with text and machine-readable codes.Generating the analysis can throw,so handle that as appropriate.And now finally, I'm ready to call the methodanalyzeImageWithConfiguration,which will start the analysis process.Once the analysis is complete,an indeterminate amount of time has passed,and the state of the application may have changed,so I will check that both the analysis was successfuland that the displayed image has not changed.If all of these checks pass,I can simply set the analysis on the interactionand set the preferredInteractionTypes.I'm using .automatic here,which will give me the default system behavior.I think this is ready for a test.Oh, look at that!I see the Live Text button has appeared, and yep,I can now select text.Notice how these interface elementsare positioned for me automatically,and keep their position inside of both the image boundsand the visible area, with no work on my part.OK, notice that tapping the Live Text buttonwill both highlight any selectable items,underline data detectors, and show Quick Actions.I can easily tap this Quick Action to make a call,and even see more options by long-pressing.You have to admit, this is pretty cool.With just these few lines of code,I've taken an ordinary image and brought it to life.This simple application now has the abilityto select text on images, activate data detectors,QR codes, lookup, translate text, and more.Not too shabby from just this few lines of code,if you ask me.And now that you've have seen how to implement Live Text,I'm going to go over a few tips and tricksthat may help you with your adoption.I'll start by exploring interaction types.Most developers will want .automatic,which provides text selection, but will also highlightdata detectors if the Live Text button is active.This will draw a line underneath any applicable detected itemsand allows one-tap access to activate them.This is the exact same behavior you would seefrom built-in applications.If it makes sense for your app to only have text selectionwithout data detectors, you may set the type to .textSelectionand it will not change with the state of the Live Text button.If, however, it makes sense for your appto only have data detectors without text selection,set the type to .dataDetectors.Note that in this mode, since selection is disabled,you will not see a Live Text button,but data detectors will be underlined and readyfor one-tap access.Setting the preferredInteractionTypesto an empty set will disable the interaction.And also, a last note, with text selection or automatic mode,you'll find you can still activate data detectorsby long-pressing.This is controlled by theallowLongPressForDataDetectorsIn TextMode property,which will be active when set to true, which the default.Simply set to false to disable this if necessary.I would like to now take a momentand talk about these buttons at the bottom,collectively known as the supplementary interface.This consists of the Live Text button,which normally lives in the bottom right-hand corner,as well as Quick Actions which appear on the bottom left.Quick Actions represent any data detectors from the analysisand are visible when the Live Text button is active.The size, position, and visibilityare controlled by the interaction.And while the default position and look matches the system,your app may have custom interface elementswhich may interfere or utilize different fontsand symbol weights.Let's look at how you can customize this interface.First off, the isSupplementary InterfaceHidden property.If I wanted to allow my app to still select textbut I did not want to show the Live Text button,if I set the SupplementaryInterfaceHiddento true, you will not see any Live Text buttonor Quick Actions.We also have a content insets property available.If you have interface elements that would overlapthe supplementary interface,you may adjust the content insetsso the Live Text button and Quick Actions adapt nicelyto your existing app content when visible.If your app is using a custom fontyou'd like the interface to adopt,setting the supplementaryInterfaceFontwill cause the Live Text button and Quick Actionsto use the specified font for textand font weight for symbols.Please note that for button-sizing consistency,Live Text will ignore the point size.Switching gears for a moment,if you are not using UIImageview,you may discover that highlights do not match up with your image.This is because with UIImageView,VisionKit can use its ContentMode propertyto calculate the contentsRect automatically for you.Here, the interaction's view has a bounds that is biggerthan its image content but is usingthe default's content rect, which is a unit rectangle.This is easily solved by implementing the delegate methodcontentsRectForInteractionand return a rectangle in unit coordinate spacedescribing how the image content relatesto the interaction's bounds in order to correct this.For example, returning a rectangle with these valueswould correct the issue,but please return the correct normalized rectanglebased on your app's current content and layout.contentsRectForInteraction will be calledwhenever the interaction's bounds change, however,if your contentsRect has changedbut your interaction's bounds have not,you can ask the interaction to update by callingsetContentsRectNeedsUpdate().Another question you may have when adopting Live Text may be,Where is the best place to put this interaction?Ideally, Live Text interactions are placed directly on the viewthat hosts your image content.As mentioned before, UIImageView will handlethe contentsRect calculations for you,and while not necessary, is preferred.If you are using UIImageview,just set the interaction on the imageViewand VisionKit will handle the rest.However, if your ImageView is locatedinside of a ScrollView,you may be tempted to place the interaction on the ScrollView,however, this is not recommended and could be difficult to manageas it will have a continually changing contentsRect.The solution here is the same,place the interaction on the viewthat hosts your image content,even if it is inside a ScrollViewwith magnification applied.I'm going talk about gestures for a moment,Live Text has a very, very rich set of gesture recognizers,to say to least.Depending on how your app is structured,it's possible you may find the interactionresponding to gestures and eventsyour app should really handle or vice versa.Don't panic.Here are a few techniques you can use to help correctif you see these issues occur.One common way to correct thisis to implement the delegate methodinteractionShouldBeginAtPointFor InteractionType.If you return false, the action will not be performed.A good place to start is to check if the interactionhas an interactive item at the given pointor if it has an active text selection.The text selection check is used hereso you will be able to have the ability tap off of the textin order to deselect it.On the other hand, if you find your interactiondoesn't seem to respond to gestures,it may be because there's a gesture recognizerin your app that's handling them instead.In this case, you can craft a similar solutionusing your gestureRecognizer's gestureRecognizerShouldBegindelegate method.Here, I perform a similar check and return falseif there is an interactive item at the locationor there's an active text selection.On a side note.In this example, I'm first convertingthe gestureRecognizer's location to the window's coordinate spaceby passing in nil,and then converting it to the interaction's view.This may be necessary if your interactionis inside of a ScrollView with magnification applied.If you find your points aren't matching up,give this technique a try.Another similar option I have found to be usefulis to override UIView's hitTest:WithEvent.Here, once again, similar story,I perform the same types of checks as before,and in this case, return the appropriate view.As always, we want your app to be as responsive as possible,and while the Neural Engine makes analysisextremely efficient, there a few ImageAnalyzer tipsI'd like to share for best performance.Ideally, you want only one ImageAnalyzershared in your app.Also, we support several types of images.You should always minimize image conversionsby passing in the native type that you have;however, if you do happen to have a CVPixelBuffer,that would be the most efficient.Also, in order to best utilize system resources,you should begin your analysis only when, or just before,an image appears onscreen.If your app's content scrolls --for example, it has a timeline --begin analysis only once the scrolling has stopped.Now this API isn't the only place you'll see Live Text,support is provided automaticallyin a few frameworks across the system your app may already use.For example, UITextField or UITextViewhave Live Text support using Camera for keyboard input.And Live Text is also supported in WebKit and Quick Look.For more information, please check out these sessions.New this year for iOS 16,we've added Live Text support in AVKit.AVPlayerView and ViewController support Live Textin paused frames automaticallyvia the allowsVideoFrameAnalysis property,which is enabled by default.Please note, this is only availablewith non-FairPlay protected content.If you're using AVPlayerLayer,then you are responsible for managing the analysisand the interaction but it is very important to use thecurrentlyDisplayedPixelBuffer propertyto get the current frame.This is the only way to guaranteethe proper frame is being analyzed.This will only return a valid valueif the video play rate is zero, and this is a shallow copyand absolutely not safe to write to.And once again, only availablefor non-FairPlay protected content.We are thrilled to help bring Live Text functionalityto your app.On behalf of everybody on the Live Text team,thank you for joining us for this session.I am stoked to see how you use it for images in your app.And as always, have fun!♪

2:37 -Live Text Sample Adoption

## Code Samples

```swift
import
 UIKit

import
 VisionKit


class
 
LiveTextDemoController
: 
BaseController
, 
ImageAnalysisInteractionDelegate
, 
UIGestureRecognizerDelegate
 {
   
    
let
 analyzer 
=
 
ImageAnalyzer
()
    
let
 interaction 
=
 
ImageAnalysisInteraction
()
    
    
override
 
func
 
viewDidLoad
() {
        
super
.viewDidLoad()
        imageview.addInteraction(interaction)
    }
    
    
override
 
var
 image: 
UIImage
? {
        
didSet
 {
            interaction.preferredInteractionTypes 
=
 []
            interaction.analysis 
=
 
nil

            analyzeCurrentImage()
        }
    }
    
    
func
 
analyzeCurrentImage
() {
        
if
 
let
 image 
=
 image {
            
Task
 {
               
let
 configuration 
=
 
ImageAnalyzer
.
Configuration
([.text, .machineReadableCode])
                
do
 {
                    
let
 analysis 
=
 
try
 
await
 analyzer.analyze(image, configuration: configuration)
                    
if
 
let
 analysis 
=
 analysis, image 
==
 
self
.image {
                        interaction.analysis 
=
 analysis;
                        interaction.preferredInteractionTypes 
=
 .automatic
                    }
                }
                
catch
 {
                    
// Handle error…

                }
            }
        }
    }
}
```

