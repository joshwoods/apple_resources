# Wwdc2022 10155

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Take ScreenCaptureKit to the next levelDiscover how you can support complex screen capture experiences for people using your app with ScreenCaptureKit. We'll explore many of the advanced options you can incorporate including fine tuning content filters, frame metadata interpretation, window pickers, and more. We'll also show you how you can configure your stream for optimal performance.ResourcesCapturing screen content in macOSScreenCaptureKitHD VideoSD VideoRelated VideosWWDC23What’s new in ScreenCaptureKitWWDC22Meet ScreenCaptureKit

Discover how you can support complex screen capture experiences for people using your app with ScreenCaptureKit. We'll explore many of the advanced options you can incorporate including fine tuning content filters, frame metadata interpretation, window pickers, and more. We'll also show you how you can configure your stream for optimal performance.

Capturing screen content in macOS

ScreenCaptureKit

HD VideoSD Video

HD Video

SD Video

What’s new in ScreenCaptureKit

Meet ScreenCaptureKit

Search this video…♪ Mellow instrumental hip-hop music ♪♪Meng Yang: Hi, my name is Meng Yang,an engineer from GPU Software here at Apple.Today I am going cover a few advanced topicsabout ScreenCaptureKit and how it can takeyour app's screen sharing experience to the next level.Later, my colleague Drew will demonstratethis exciting new API in action.Screen capture is at the heart of screen sharing applicationssuch as Zoom, Google Meet, SharePlayand even popular game streaming services like Twitch,which have become the new norm of how we work, study,collaborate, and socialize over the past few years.ScreenCaptureKit is a brand-new,high-performance screen capture frameworkbuilt from ground up with a powerful feature set.The rich set of features includes highly customizablecontent control that allows you to easily pick and then chooseany combination of windows, applications,and displays to capture.Ability to capture up to the screen content'snative resolution and frame rate.Dynamic stream property controls like resolution,frame rate, pixel format.And these controls can be modified on the flywithout recreating the stream.Capture buffers that are GPU memory-backedto reduce memory copies.Hardware-accelerated content capture, scaling,pixel and color format conversionto achieve high-performance capture with reduced CPU usage.Last but not least, support for both video and audio capture.Before getting started,this talk assumes you are already familiarwith the basic concepts, building blocks,and workflow of how the framework works.Please visit the intro session "Meet ScreenCaptureKit"to learn more.In this session, I am going to talk abouthow to capture and display a single window.Next, how to add screen content to full display capture.How to remove content from display capture.I will then show you a few ways to configure the streamfor different use cases.And last, you will see a demo of how ScreenCaptureKittransformed the screen and audio capture experienceof OBS Studio, a popular open source screen capture app.Now, let's start with the first example,and probably the most common use case:capture a single window.This example is going to coverhow to set up a single window filter;what to expect from the stream outputwhen the captured window is being resized, occluded,moved off-screen, or minimized.You will also learn how to use per-frame metadataand how to properly display the captured window.Let's dive in.To capture a single window that's independentof which display it's on,you can start by using a single window filterand initialize the filter with just one window.In the example here,the filter is configured to include a single Safari window.The video output includes just that window and nothing else.No child, pop-up, or other windows from Safariwill be included.ScreenCaptureKit's audio capture policy on the other handalways works at the app level.When a single window filter is used,all the audio content from the applicationthat contains the window will be captured,even from those windows that are not present in the video output.Now let's take a look at the code sample.To create a stream with a single window,start by getting all available content to sharevia SCShareableContent.Next, get the window you want to share from SCShareableContentby matching the windowID.Then, create a SCContentFilter with the typedesktopIndependentWindow with the specified SCWindow.You can further configure the stream to include audioas part of the stream output.Now you are ready to create a streamwith contentFilter and streamConfig.You can then add a StreamOutput and start the stream.Let's take a look at the stream output next.In the example here, the source display is on the leftand the stream output is on the right.The stream filter includes a single Safari window.Now I am going to start to scroll the Safari windowthat's being captured.The stream output includes the live contentfrom the single Safari windowand is updating at the same cadence as the source window,up to the source display's native frame rate.For example, when the source window is constantly updatingon a 120Hz display, the stream outputcan also achieve up to 120 fps update.You might wonder what happens when the window resizes.Please keep in mind that frequently changingthe stream's output dimensioncan lead to additional memory allocationand therefore not recommended.The stream's output dimension is mostly fixedand it does not resize with the source window.Now let me start to resize the source windowand see what happens to the stream's output.ScreenCaptureKit always performs hardware scalingon the captured window so it never exceeds the frame outputas the source window resizes.How about windows that are covered by other windows?When the source window is occluded or partially occluded,the stream output always includesthe window's full content.And this also applies to the casewhen the window is completely off-screenor moved to other displays.And for minimized windows, when the source window is minimized,the stream output is paused,and it resumes when the source window is no longer minimized.Next, let's move to audio output.In this example here,there are two Safari windows with audio tracks,and the window on the left is being captured.The video output includes just the first window,and the audio tracks from both Safari windowswill be included in the audio output.Let's take a look and listen.♪ Electronic dance music ♪Chef: And I wrote down my favorite guacamole recipe.It calls for four avocados.Meng: With the stream up and running,your app receives a frame updatewhenever there's a new frame available.The frame's output includes IOSurfacerepresenting the captured frame and the per-frame metadata.I'd like to spend some time talking about metadata.I am going to show you examples of metadatathat can be quite useful for your app.And these include dirty rects, content rect,content scale, and scale factor.Let's start with dirty rects.Dirty rects indicate where the new content isfrom the previous frame.In the example here, the dirty rects are being highlightedto illustrate the regions of frame updates.Instead of always encoding the entire frame,or calculate the delta between two frames in the encoder,you can simply use dirty rects to only encodeand transmit the regions with new updatesand copy the updates onto the previous frameon the receiver side to generate a new frame.Dirty rects can be retrieved from the output CMSampleBuffer'smetadata dictionary using the matching key.Now let's move to content rect and the content scale.The source window to be captured is on the leftand the stream output is on the right.Since a window can be resized,the source window's native backing surface sizeoften doesn't match the stream output's dimension.In the example here, the captured window hasdifferent aspect ratio from the frame's output and is bigger.The captured window is scaled down to fit into the output.A content rect, which is highlighted in green here,indicates the region of interest of the captured contenton the stream output.And the content scale indicates how much the contentis scaled to fit.Here the captured Safari window is scaled downby 0.77 to fit inside the frame.Now you can use the metadata just discussedto correctly display the captured windowas close to its native appearance as possible.First, let's start by cropping the content from its outputusing the content rect.Next, scale the content back up by dividing the content scale.Now the captured content is scaled to match one-to-onein pixel size as the source window.But how is the captured window going to lookon the target display?To answer that question, I would like to startby describing how scale factor works.A display's scale factor indicates the scale ratiobetween a display or window's logical point sizeand its backing surface's pixel size.A scale factor 2, or a 2x mode, means every one point onscreenequals four pixels on the backing surface.A window can be moved from a Retina displaywith scale factor 2, such as in the example here,to a non-Retina display with scale factor 1while being captured.With scale factor 1, each one logical point onscreencorresponds to one pixel on the backing surface.In addition, the source display might havemismatched scale factor from the target displaywhere the captured content will be displayed.In this example, a window is being capturedfrom a Retina display on the left with a scale factor 2and to be displayed on a non-Retina display on the right.If the captured window is displayed as-is without scalingon the target non-Retina displaywith one point to one pixel mapping,the window will look four times as big.To fix this, you should always check the scale factorfrom the frame's metadata against the scale factorof the target display.When there's a mismatch, scale the size of the captured contentby the scale factor before displaying it.After scaling, the captured window on the target displaynow appears to be the same size as its source window.Now let's take a look at the code,and it's quite simple.Content rect, content scale, and scale factorcan also be retrieved from the output CMSampleBuffer'smetadata attachment.You can then use these metadatato crop and scale the captured content to display it correctly.To recap, a single window filteralways includes full window contenteven when the source window is off-screen or occluded.It's display and space independent.The output is always offset at the top-left corner.Pop-up or child windows are not included.Consider using metadata to best display the content.And the audio includes tracks from the entire containing app.Now that you have just learned about how to captureand display a single window,let me move to the next class of display-based content filters.In this next example,you will learn to create a display-based filterwith windows or apps, and I will demonstratesome differences between video- and audio-filtering rules.A display-based inclusion filter specifies which displayyou want to capture content from.By default, no windows are captured.You can choose the content you want to capture by window.In the example here, a Safari window and a Keynote windoware added to the display filter.The video output includes just these two windowsplaced in a display spaceand the audio output includes all the soundtracksfrom Keynote and Safari apps.This code sample demonstrates how to createdisplay-based filters with included windows.Start by creating a list of SCWindowsusing SCShareableContent and windowIDs.And then, create a display-based SCContentFilterwith a given display and a list of included windows.You can then create a streamusing the filter and configuration in the same wayas a desktop independent window and start the stream.With the stream up and running,let's take a look at the stream's output.The filter is configured to include two Safari windows,menu bar, and wallpaper windows.If a window is moved off-screen,it will be removed from the stream output.When a new Safari window is created,the new window doesn't show up in the stream outputbecause the new window is not in the filter.The same rule also applies to child or pop-up windows,which do not show up in the stream's output.If you want to ensure that child windowsare included automatically in your stream output,you can use a display-based filter with included apps.In this example, adding the Safari and Keynote appsto the filter ensures that the audio and video outputfrom all the windows and soundtracks from these two appsare included in the outputWindow exception filters are a powerful wayof excluding specific windows from your outputwhen the filter is specified as a display with included apps.For example, a single Safari windowis removed from the output.ScreenCaptureKit enables audio capture at the app level,so excluding audio from a single Safari windowis the equivalent to removing audio tracksfor all Safari apps.Although the stream's video outputstill includes a Safari window,all the sound tracks from Safari apps are removedand the audio output includes just the soundtrackfrom Keynote.In the code example here, we change the SCContentFilterto include a list of SCRunningApplicationsinstead of SCWindows.If there are individual windows you want to further exclude,build a list of SCWindows and then create an SCContentFilterusing the list of SCApplicationswith the list of excepting windows to exclude.Let's take a look at what the stream output looks like nowwhen new or child windows are createdby specifying included apps.This time, Safari app and system windows are added to the filter.A new Safari window is now automatically includedin the stream output and the same rule appliesto child and pop-up windows.This can be quite useful when you are doing a tutorialand want to demonstrate the full actionincluding invoking pop-up or new windows.I have just demonstrated how to add contentto the stream output through a few different ways.My next example will show youhow to remove content from the stream output.This example includes a test appthat emulates a video conferencing appthat contains a preview of the display being shared.Because the test app recursively shows itself in the preview,it's creating the so-called mirror hall effect.Even during full display share, it's common for screen sharingapplications to remove its own windows, capture preview,participant camera view to avoid the mirror hall effect,or other system UIs such as notification windows.ScreenCaptureKit provides youwith a set of exclusion-based filters that allow youto quickly remove content from display capture.An exclusion-based display filter captures all the windowsfrom the given display by default.You can then start to remove individual windows or appsby adding them to the exclusion filter.For example, you can add the content capture test appand Notification Center to the list of excluded applications.To create a display-based filterexcluding a list of applications,start by retrieving SCApplications to excludeby matching bundle ID.If there are individual windows you'd like to cherry-pickback to the stream output, you can also buildan optional list of excepting SCWindows.And then use a given display,the list of applications to exclude,and a list of excepting windows to create the content filter.Let's take a look at the result.The content capture test app that's causingthe mirror hall problem and the notification windowsare both removed from the stream output.New or child windows from these appswill be automatically removed as well.If these removed apps include any audio,their audio will be removed from the audio output.We've just seen how to capture a single window,how to add and remove windows from a display filter.Let's move to stream configuration next.In the next few examples, you will learn aboutdifferent stream properties you can configure,how to set up the stream for screen capture and streaming,and how to build a window picker with live preview.Let's start with configuration properties.These are some of the common stream propertiesyou can configure, such as stream output dimensions,source and destination rects,color space, color matrix, and pixel format,whether to include cursor, and frame rate control.We will take a look at each property in details next.Let's start with output dimension,which can be specified as width and height in pixels.The source display's dimension and aspect ratiodoesn't always match the output dimension.And when this mismatch happens while capturing a full display,there will be pillar or letterbox in the stream output.You can also specify a source rect that defines the regionto capture from and the result will be rendered and scaledto the destination rect on the frame output.ScreenCaptureKit supports hardware acceleratedcolor space, color matrix, and pixel format conversion.Common BGRA and YUV formats are supported.Please visit our developer page for the full list.When show cursor is enabled, the stream output includesa cursor prerendered into the frame.This applies to all system cursors,even custom cursor like the camera-shaped one here.You can use minimum frame intervalto control desired output frame rate.For example, when requesting 60 fps,set the minimal interval to 1/60.You will receive frame update no more than 60 fps,and no more than the content's native frame rate.Queue depth can be specified to determinethe number of surfaces in the server-side surface pool.More surfaces in the pool can lead to better frame rateand performance, but it results in higher system memory usageand potentially a latency trade-off,which I will discuss in more details later.ScreenCaptureKit accepts queue depth range betweenthree to eight with a default queue depth of three.In this example here, the surface pool is configuredto include four surfaces available for ScreenCaptureKitto render to.The current active surface is surface 1and ScreenCaptureKit is rendering the next frame to it.Once surface 1 is complete,ScreenCaptureKit sends surface 1 to your app.Your app is processing and holding surface 1,while ScreenCaptureKit is rendering to surface 2.Surface 1 is now marked as unavailable in the poolsince your app is still using it.When surface 2 is complete, it's sent to your appand ScreenCaptureKit now renders to surface 3.But if your app is still processing surface 1,it will start to fall behind as frames are now providedfaster than they can be processed.If the surface pool contains a large number of surfaces,new surfaces will start to pile upand you might need to consider starting to drop framesin order to keep up.In this case, more surfaces in the poolcan potentially lead to higher latency.The number of surfaces left in the poolfor ScreenCaptureKit to use, equals the queue depthminus the number of surfaces held by your app.In the example here,both surface 1 and 2 are still held by your app.There are 2 surfaces left in the surface pool.After surface 3 is complete and is sent to your app,the only available surface left in the pool is surface 4.If your app continues to hold on to surface 1, 2, and 3,ScreenCaptureKit will soon run out of surfaces to render toand you will start to see frame loss and glitch.Your app needs to finish and release surface 1before ScreenCaptureKit starts to render the next frameafter surface 4 in order to avoid frame loss.Now your app releases surface 1 and it's availablefor ScreenCaptureKit to use again.To recap: there are two rules your app needs to followin order avoid frame latency and frame loss.To avoid delayed frame, you need to be able to processa frame within the MinimumFrameInterval.To avoid frame loss, the time it takes your appto release the surfaces back to the pool must be less thanMinimumFrameInterval times QueueDepth minus 1,after which ScreenCaptureKit runs out of surfaces to use,enters a stall, and will start to miss new frames.Now that you've seen the various propertiesyou can configure, let's dive into some examplesto configure the stream for screen capture and streaming.Some screen content includes videos, games,or animations that are constantly updatingand that requires higher frame rate.While others include mostly static textlike the keynote window,which prioritize higher resolution over frame rate,you can live-adjust the stream's configuration based onthe content being shared and the networking condition.In this code example, you are going to seehow to configure the capture to stream 4K, 60-fps game.You can start by setting the stream output dimensionto 4K in pixel size.And then, set the output frame rate to 60 fpsby setting the minimum frame interval to 1/60.Next, use pixel format YUV420 for encoding and streaming.Set the optional source rectto just capture a portion of the screen.Next, change the background fill color to black,and then include a cursor in the frame output.Configure surface queue depth to five for optimal frame rateand performance.Last, enable audio on the output stream.All the stream configurations you've just seenin the previous example can be dynamically changed on the flywithout recreating the stream.For example, you can live adjust some properties such asoutput dimension, dynamically change the frame rate,and update stream filters.Here's an example to switch the output dimensionfrom 4K down to 720p.And downgrade the frame rate from 60 fps to 15 fps.You can then simply call updateConfigurationto apply the new settings on the flywithout interrupting the stream.In the last example, I'd like to walk you throughbuilding a window picker with live preview.Here is an exampleof what a typical window picker looks like.It's common for web conferencing screen sharing appsto offer users an optionto choose the exact window to share.ScreenCaptureKit provides an efficientand high-performance solution to creating large numberof thumbnail-sized streams with live content update,and it's simple to implement.Let's break it down to see what it takes to builda window picker like this using ScreenCaptureKit.To set up the picker, you can start by creatingone single window filter for each eligible windowthat your app allows the user to pickwith desktop independent window as the filter type.Next, set up the stream configurationthat's thumbnail-sized, 5 fps, with BGRA pixel formatfor onscreen display, default queue depth, no cursor or audio.Use single window filter and the stream configuration hereto create one stream for each window.To do this in code, you can start by gettingthe SCShareableContent by excluding desktopand system windows.Next, create a content filterof type desktop independent window for each eligible window.Then, move to the stream configuration part.Choose an appropriate thumbnail size --in this example, it's 284 by 182 --and then set the minimum frame interval to one over five.With a pixel format of BGRA for onscreen display,disable audio and cursor since we don't need themin the preview.And set queue depth to three because we don't expect updatesthat are too often.With the stream content filter and the configuration created,you are now ready to create the streams.Create one stream for each window,add stream output for each stream,and then start the stream.Last, append it to the stream list.This is the window picker with live previewcreated using the sample code we saw earlier.Each thumbnail is live updating and then backedby an individual stream with single-window filter.With ScreenCaptureKit, you can easily builda live preview picker like this, that allows youto concurrently capture so much live screen contentsimultaneously, without overburdening the system.Now let me hand it over to my colleague, Drew,who's going to give you an exciting demo aboutOBS adoption of ScreenCaptureKit.Drew Mills: Thanks, Meng.Hi, my name is Drew, and I'm a Partner Engineer here at Apple.OBS Studio is an open source application that allows usersto manage recording and streaming contentfrom their computer.It contains an implementation of ScreenCaptureKitthat we worked with the project on integrating this spring.ScreenCaptureKit was easy to implement thanks to utilizingsimilar code to OBS's existing CGDisplayStream-based capture.The ScreenCaptureKit implementation demonstratesmany of the features discussedin the "Meet ScreenCaptureKit" session.This includes: capturing an entire desktop,all of the windows of an application,or just one specific window.ScreenCaptureKit has lower overhead than OBS'sCGWindowListCreateImage-based capture.This means that when capturing a portion of your screen,you are left with more resources that you can usefor producing your content.Let's dive into a demoto see what we've been discussing in action.On the left, there is a worst case exampleof OBS's Window Capture.This capture uses the CGWindowListCreateImage API,and has significant stuttering.In our testing, we saw frame rates dip as low as 7 fps.Meanwhile, the ScreenCaptureKit implementation on the righthas a much smoother result, providing an output videowith significantly smoother motion.In this case, delivering 60 fps.All while OBS uses up to 15 percent less RAMthan Window Capture.And while OBS's CPU utilization is cut by up to halfwhen using ScreenCaptureKit instead of OBS's Window Capture.Let's look at the other improvementsthat ScreenCaptureKit has to offer OBS users.I'm still trying to track down all of the Gold Ranksin Sayonara Wild Hearts.I want to show off my best run,so I've been recording my gameplay.Thanks to ScreenCaptureKit, I can now capturedirect audio stream from the game,so when I get a notification on my Mac,it won't ruin my recording's audio or video.And this is possible without having to installany additional audio routing software.♪Now, using all of the enhancementsprovided by ScreenCaptureKit on Apple silicon,I can stream games like Taiko no Tatsujin Pop Tap Beatfrom my Mac to popular streaming services.A new constant bitrate option for Apple silicon'shardware encoder means that I can encodemy streaming content for services requiring constantbitrate without significantly impacting my game's performance.Now, thanks to ScreenCaptureKit'slower resource usage and encoding offloading,I have even more performance availablefor the content that matters.Back to you, Meng.Meng: Thank you, Drew.Through the demos and examples,you learned about advanced screen content filters.Several ways to configure the stream for different use cases.And how to use per-frame metadata and correctly displaythe captured content.Some best practices to help you achieve best performance.And finally, Drew showcased the significant capabilityand performance improvement ScreenCaptureKit brought to OBS.I can't wait to see how you redefine your app'sscreen sharing, streaming, and collaboration experienceusing ScreenCaptureKit.Thank you for watching!♪

♪ Mellow instrumental hip-hop music ♪♪Meng Yang: Hi, my name is Meng Yang,an engineer from GPU Software here at Apple.Today I am going cover a few advanced topicsabout ScreenCaptureKit and how it can takeyour app's screen sharing experience to the next level.Later, my colleague Drew will demonstratethis exciting new API in action.Screen capture is at the heart of screen sharing applicationssuch as Zoom, Google Meet, SharePlayand even popular game streaming services like Twitch,which have become the new norm of how we work, study,collaborate, and socialize over the past few years.ScreenCaptureKit is a brand-new,high-performance screen capture frameworkbuilt from ground up with a powerful feature set.The rich set of features includes highly customizablecontent control that allows you to easily pick and then chooseany combination of windows, applications,and displays to capture.Ability to capture up to the screen content'snative resolution and frame rate.Dynamic stream property controls like resolution,frame rate, pixel format.And these controls can be modified on the flywithout recreating the stream.Capture buffers that are GPU memory-backedto reduce memory copies.Hardware-accelerated content capture, scaling,pixel and color format conversionto achieve high-performance capture with reduced CPU usage.Last but not least, support for both video and audio capture.Before getting started,this talk assumes you are already familiarwith the basic concepts, building blocks,and workflow of how the framework works.Please visit the intro session "Meet ScreenCaptureKit"to learn more.In this session, I am going to talk abouthow to capture and display a single window.Next, how to add screen content to full display capture.How to remove content from display capture.I will then show you a few ways to configure the streamfor different use cases.And last, you will see a demo of how ScreenCaptureKittransformed the screen and audio capture experienceof OBS Studio, a popular open source screen capture app.Now, let's start with the first example,and probably the most common use case:capture a single window.This example is going to coverhow to set up a single window filter;what to expect from the stream outputwhen the captured window is being resized, occluded,moved off-screen, or minimized.You will also learn how to use per-frame metadataand how to properly display the captured window.Let's dive in.To capture a single window that's independentof which display it's on,you can start by using a single window filterand initialize the filter with just one window.In the example here,the filter is configured to include a single Safari window.The video output includes just that window and nothing else.No child, pop-up, or other windows from Safariwill be included.ScreenCaptureKit's audio capture policy on the other handalways works at the app level.When a single window filter is used,all the audio content from the applicationthat contains the window will be captured,even from those windows that are not present in the video output.Now let's take a look at the code sample.To create a stream with a single window,start by getting all available content to sharevia SCShareableContent.Next, get the window you want to share from SCShareableContentby matching the windowID.Then, create a SCContentFilter with the typedesktopIndependentWindow with the specified SCWindow.You can further configure the stream to include audioas part of the stream output.Now you are ready to create a streamwith contentFilter and streamConfig.You can then add a StreamOutput and start the stream.Let's take a look at the stream output next.In the example here, the source display is on the leftand the stream output is on the right.The stream filter includes a single Safari window.Now I am going to start to scroll the Safari windowthat's being captured.The stream output includes the live contentfrom the single Safari windowand is updating at the same cadence as the source window,up to the source display's native frame rate.For example, when the source window is constantly updatingon a 120Hz display, the stream outputcan also achieve up to 120 fps update.You might wonder what happens when the window resizes.Please keep in mind that frequently changingthe stream's output dimensioncan lead to additional memory allocationand therefore not recommended.The stream's output dimension is mostly fixedand it does not resize with the source window.Now let me start to resize the source windowand see what happens to the stream's output.ScreenCaptureKit always performs hardware scalingon the captured window so it never exceeds the frame outputas the source window resizes.How about windows that are covered by other windows?When the source window is occluded or partially occluded,the stream output always includesthe window's full content.And this also applies to the casewhen the window is completely off-screenor moved to other displays.And for minimized windows, when the source window is minimized,the stream output is paused,and it resumes when the source window is no longer minimized.Next, let's move to audio output.In this example here,there are two Safari windows with audio tracks,and the window on the left is being captured.The video output includes just the first window,and the audio tracks from both Safari windowswill be included in the audio output.Let's take a look and listen.♪ Electronic dance music ♪Chef: And I wrote down my favorite guacamole recipe.It calls for four avocados.Meng: With the stream up and running,your app receives a frame updatewhenever there's a new frame available.The frame's output includes IOSurfacerepresenting the captured frame and the per-frame metadata.I'd like to spend some time talking about metadata.I am going to show you examples of metadatathat can be quite useful for your app.And these include dirty rects, content rect,content scale, and scale factor.Let's start with dirty rects.Dirty rects indicate where the new content isfrom the previous frame.In the example here, the dirty rects are being highlightedto illustrate the regions of frame updates.Instead of always encoding the entire frame,or calculate the delta between two frames in the encoder,you can simply use dirty rects to only encodeand transmit the regions with new updatesand copy the updates onto the previous frameon the receiver side to generate a new frame.Dirty rects can be retrieved from the output CMSampleBuffer'smetadata dictionary using the matching key.Now let's move to content rect and the content scale.The source window to be captured is on the leftand the stream output is on the right.Since a window can be resized,the source window's native backing surface sizeoften doesn't match the stream output's dimension.In the example here, the captured window hasdifferent aspect ratio from the frame's output and is bigger.The captured window is scaled down to fit into the output.A content rect, which is highlighted in green here,indicates the region of interest of the captured contenton the stream output.And the content scale indicates how much the contentis scaled to fit.Here the captured Safari window is scaled downby 0.77 to fit inside the frame.Now you can use the metadata just discussedto correctly display the captured windowas close to its native appearance as possible.First, let's start by cropping the content from its outputusing the content rect.Next, scale the content back up by dividing the content scale.Now the captured content is scaled to match one-to-onein pixel size as the source window.But how is the captured window going to lookon the target display?To answer that question, I would like to startby describing how scale factor works.A display's scale factor indicates the scale ratiobetween a display or window's logical point sizeand its backing surface's pixel size.A scale factor 2, or a 2x mode, means every one point onscreenequals four pixels on the backing surface.A window can be moved from a Retina displaywith scale factor 2, such as in the example here,to a non-Retina display with scale factor 1while being captured.With scale factor 1, each one logical point onscreencorresponds to one pixel on the backing surface.In addition, the source display might havemismatched scale factor from the target displaywhere the captured content will be displayed.In this example, a window is being capturedfrom a Retina display on the left with a scale factor 2and to be displayed on a non-Retina display on the right.If the captured window is displayed as-is without scalingon the target non-Retina displaywith one point to one pixel mapping,the window will look four times as big.To fix this, you should always check the scale factorfrom the frame's metadata against the scale factorof the target display.When there's a mismatch, scale the size of the captured contentby the scale factor before displaying it.After scaling, the captured window on the target displaynow appears to be the same size as its source window.Now let's take a look at the code,and it's quite simple.Content rect, content scale, and scale factorcan also be retrieved from the output CMSampleBuffer'smetadata attachment.You can then use these metadatato crop and scale the captured content to display it correctly.To recap, a single window filteralways includes full window contenteven when the source window is off-screen or occluded.It's display and space independent.The output is always offset at the top-left corner.Pop-up or child windows are not included.Consider using metadata to best display the content.And the audio includes tracks from the entire containing app.Now that you have just learned about how to captureand display a single window,let me move to the next class of display-based content filters.In this next example,you will learn to create a display-based filterwith windows or apps, and I will demonstratesome differences between video- and audio-filtering rules.A display-based inclusion filter specifies which displayyou want to capture content from.By default, no windows are captured.You can choose the content you want to capture by window.In the example here, a Safari window and a Keynote windoware added to the display filter.The video output includes just these two windowsplaced in a display spaceand the audio output includes all the soundtracksfrom Keynote and Safari apps.This code sample demonstrates how to createdisplay-based filters with included windows.Start by creating a list of SCWindowsusing SCShareableContent and windowIDs.And then, create a display-based SCContentFilterwith a given display and a list of included windows.You can then create a streamusing the filter and configuration in the same wayas a desktop independent window and start the stream.With the stream up and running,let's take a look at the stream's output.The filter is configured to include two Safari windows,menu bar, and wallpaper windows.

If a window is moved off-screen,it will be removed from the stream output.When a new Safari window is created,the new window doesn't show up in the stream outputbecause the new window is not in the filter.The same rule also applies to child or pop-up windows,which do not show up in the stream's output.If you want to ensure that child windowsare included automatically in your stream output,you can use a display-based filter with included apps.In this example, adding the Safari and Keynote appsto the filter ensures that the audio and video outputfrom all the windows and soundtracks from these two appsare included in the outputWindow exception filters are a powerful wayof excluding specific windows from your outputwhen the filter is specified as a display with included apps.For example, a single Safari windowis removed from the output.ScreenCaptureKit enables audio capture at the app level,so excluding audio from a single Safari windowis the equivalent to removing audio tracksfor all Safari apps.Although the stream's video outputstill includes a Safari window,all the sound tracks from Safari apps are removedand the audio output includes just the soundtrackfrom Keynote.In the code example here, we change the SCContentFilterto include a list of SCRunningApplicationsinstead of SCWindows.If there are individual windows you want to further exclude,build a list of SCWindows and then create an SCContentFilterusing the list of SCApplicationswith the list of excepting windows to exclude.Let's take a look at what the stream output looks like nowwhen new or child windows are createdby specifying included apps.This time, Safari app and system windows are added to the filter.A new Safari window is now automatically includedin the stream output and the same rule appliesto child and pop-up windows.This can be quite useful when you are doing a tutorialand want to demonstrate the full actionincluding invoking pop-up or new windows.I have just demonstrated how to add contentto the stream output through a few different ways.My next example will show youhow to remove content from the stream output.This example includes a test appthat emulates a video conferencing appthat contains a preview of the display being shared.Because the test app recursively shows itself in the preview,it's creating the so-called mirror hall effect.Even during full display share, it's common for screen sharingapplications to remove its own windows, capture preview,participant camera view to avoid the mirror hall effect,or other system UIs such as notification windows.ScreenCaptureKit provides youwith a set of exclusion-based filters that allow youto quickly remove content from display capture.An exclusion-based display filter captures all the windowsfrom the given display by default.You can then start to remove individual windows or appsby adding them to the exclusion filter.For example, you can add the content capture test appand Notification Center to the list of excluded applications.To create a display-based filterexcluding a list of applications,start by retrieving SCApplications to excludeby matching bundle ID.If there are individual windows you'd like to cherry-pickback to the stream output, you can also buildan optional list of excepting SCWindows.And then use a given display,the list of applications to exclude,and a list of excepting windows to create the content filter.Let's take a look at the result.The content capture test app that's causingthe mirror hall problem and the notification windowsare both removed from the stream output.New or child windows from these appswill be automatically removed as well.If these removed apps include any audio,their audio will be removed from the audio output.We've just seen how to capture a single window,how to add and remove windows from a display filter.Let's move to stream configuration next.In the next few examples, you will learn aboutdifferent stream properties you can configure,how to set up the stream for screen capture and streaming,and how to build a window picker with live preview.Let's start with configuration properties.These are some of the common stream propertiesyou can configure, such as stream output dimensions,source and destination rects,color space, color matrix, and pixel format,whether to include cursor, and frame rate control.We will take a look at each property in details next.Let's start with output dimension,which can be specified as width and height in pixels.The source display's dimension and aspect ratiodoesn't always match the output dimension.And when this mismatch happens while capturing a full display,there will be pillar or letterbox in the stream output.You can also specify a source rect that defines the regionto capture from and the result will be rendered and scaledto the destination rect on the frame output.ScreenCaptureKit supports hardware acceleratedcolor space, color matrix, and pixel format conversion.Common BGRA and YUV formats are supported.Please visit our developer page for the full list.When show cursor is enabled, the stream output includesa cursor prerendered into the frame.This applies to all system cursors,even custom cursor like the camera-shaped one here.You can use minimum frame intervalto control desired output frame rate.For example, when requesting 60 fps,set the minimal interval to 1/60.You will receive frame update no more than 60 fps,and no more than the content's native frame rate.Queue depth can be specified to determinethe number of surfaces in the server-side surface pool.More surfaces in the pool can lead to better frame rateand performance, but it results in higher system memory usageand potentially a latency trade-off,which I will discuss in more details later.ScreenCaptureKit accepts queue depth range betweenthree to eight with a default queue depth of three.In this example here, the surface pool is configuredto include four surfaces available for ScreenCaptureKitto render to.The current active surface is surface 1and ScreenCaptureKit is rendering the next frame to it.Once surface 1 is complete,ScreenCaptureKit sends surface 1 to your app.Your app is processing and holding surface 1,while ScreenCaptureKit is rendering to surface 2.Surface 1 is now marked as unavailable in the poolsince your app is still using it.When surface 2 is complete, it's sent to your appand ScreenCaptureKit now renders to surface 3.But if your app is still processing surface 1,it will start to fall behind as frames are now providedfaster than they can be processed.If the surface pool contains a large number of surfaces,new surfaces will start to pile upand you might need to consider starting to drop framesin order to keep up.In this case, more surfaces in the poolcan potentially lead to higher latency.The number of surfaces left in the poolfor ScreenCaptureKit to use, equals the queue depthminus the number of surfaces held by your app.In the example here,both surface 1 and 2 are still held by your app.There are 2 surfaces left in the surface pool.After surface 3 is complete and is sent to your app,the only available surface left in the pool is surface 4.If your app continues to hold on to surface 1, 2, and 3,ScreenCaptureKit will soon run out of surfaces to render toand you will start to see frame loss and glitch.Your app needs to finish and release surface 1before ScreenCaptureKit starts to render the next frameafter surface 4 in order to avoid frame loss.Now your app releases surface 1 and it's availablefor ScreenCaptureKit to use again.To recap: there are two rules your app needs to followin order avoid frame latency and frame loss.To avoid delayed frame, you need to be able to processa frame within the MinimumFrameInterval.To avoid frame loss, the time it takes your appto release the surfaces back to the pool must be less thanMinimumFrameInterval times QueueDepth minus 1,after which ScreenCaptureKit runs out of surfaces to use,enters a stall, and will start to miss new frames.Now that you've seen the various propertiesyou can configure, let's dive into some examplesto configure the stream for screen capture and streaming.Some screen content includes videos, games,or animations that are constantly updatingand that requires higher frame rate.While others include mostly static textlike the keynote window,which prioritize higher resolution over frame rate,you can live-adjust the stream's configuration based onthe content being shared and the networking condition.In this code example, you are going to seehow to configure the capture to stream 4K, 60-fps game.You can start by setting the stream output dimensionto 4K in pixel size.And then, set the output frame rate to 60 fpsby setting the minimum frame interval to 1/60.Next, use pixel format YUV420 for encoding and streaming.Set the optional source rectto just capture a portion of the screen.Next, change the background fill color to black,and then include a cursor in the frame output.Configure surface queue depth to five for optimal frame rateand performance.Last, enable audio on the output stream.All the stream configurations you've just seenin the previous example can be dynamically changed on the flywithout recreating the stream.For example, you can live adjust some properties such asoutput dimension, dynamically change the frame rate,and update stream filters.Here's an example to switch the output dimensionfrom 4K down to 720p.And downgrade the frame rate from 60 fps to 15 fps.You can then simply call updateConfigurationto apply the new settings on the flywithout interrupting the stream.In the last example, I'd like to walk you throughbuilding a window picker with live preview.Here is an exampleof what a typical window picker looks like.It's common for web conferencing screen sharing appsto offer users an optionto choose the exact window to share.ScreenCaptureKit provides an efficientand high-performance solution to creating large numberof thumbnail-sized streams with live content update,and it's simple to implement.Let's break it down to see what it takes to builda window picker like this using ScreenCaptureKit.To set up the picker, you can start by creatingone single window filter for each eligible windowthat your app allows the user to pickwith desktop independent window as the filter type.Next, set up the stream configurationthat's thumbnail-sized, 5 fps, with BGRA pixel formatfor onscreen display, default queue depth, no cursor or audio.Use single window filter and the stream configuration hereto create one stream for each window.To do this in code, you can start by gettingthe SCShareableContent by excluding desktopand system windows.Next, create a content filterof type desktop independent window for each eligible window.Then, move to the stream configuration part.Choose an appropriate thumbnail size --in this example, it's 284 by 182 --and then set the minimum frame interval to one over five.With a pixel format of BGRA for onscreen display,disable audio and cursor since we don't need themin the preview.And set queue depth to three because we don't expect updatesthat are too often.With the stream content filter and the configuration created,you are now ready to create the streams.Create one stream for each window,add stream output for each stream,and then start the stream.Last, append it to the stream list.This is the window picker with live previewcreated using the sample code we saw earlier.Each thumbnail is live updating and then backedby an individual stream with single-window filter.With ScreenCaptureKit, you can easily builda live preview picker like this, that allows youto concurrently capture so much live screen contentsimultaneously, without overburdening the system.Now let me hand it over to my colleague, Drew,who's going to give you an exciting demo aboutOBS adoption of ScreenCaptureKit.Drew Mills: Thanks, Meng.Hi, my name is Drew, and I'm a Partner Engineer here at Apple.OBS Studio is an open source application that allows usersto manage recording and streaming contentfrom their computer.It contains an implementation of ScreenCaptureKitthat we worked with the project on integrating this spring.ScreenCaptureKit was easy to implement thanks to utilizingsimilar code to OBS's existing CGDisplayStream-based capture.The ScreenCaptureKit implementation demonstratesmany of the features discussedin the "Meet ScreenCaptureKit" session.This includes: capturing an entire desktop,all of the windows of an application,or just one specific window.ScreenCaptureKit has lower overhead than OBS'sCGWindowListCreateImage-based capture.This means that when capturing a portion of your screen,you are left with more resources that you can usefor producing your content.Let's dive into a demoto see what we've been discussing in action.On the left, there is a worst case exampleof OBS's Window Capture.This capture uses the CGWindowListCreateImage API,and has significant stuttering.In our testing, we saw frame rates dip as low as 7 fps.Meanwhile, the ScreenCaptureKit implementation on the righthas a much smoother result, providing an output videowith significantly smoother motion.In this case, delivering 60 fps.All while OBS uses up to 15 percent less RAMthan Window Capture.And while OBS's CPU utilization is cut by up to halfwhen using ScreenCaptureKit instead of OBS's Window Capture.Let's look at the other improvementsthat ScreenCaptureKit has to offer OBS users.I'm still trying to track down all of the Gold Ranksin Sayonara Wild Hearts.I want to show off my best run,so I've been recording my gameplay.Thanks to ScreenCaptureKit, I can now capturedirect audio stream from the game,so when I get a notification on my Mac,it won't ruin my recording's audio or video.And this is possible without having to installany additional audio routing software.♪Now, using all of the enhancementsprovided by ScreenCaptureKit on Apple silicon,I can stream games like Taiko no Tatsujin Pop Tap Beatfrom my Mac to popular streaming services.A new constant bitrate option for Apple silicon'shardware encoder means that I can encodemy streaming content for services requiring constantbitrate without significantly impacting my game's performance.Now, thanks to ScreenCaptureKit'slower resource usage and encoding offloading,I have even more performance availablefor the content that matters.Back to you, Meng.Meng: Thank you, Drew.Through the demos and examples,you learned about advanced screen content filters.Several ways to configure the stream for different use cases.And how to use per-frame metadata and correctly displaythe captured content.Some best practices to help you achieve best performance.And finally, Drew showcased the significant capabilityand performance improvement ScreenCaptureKit brought to OBS.I can't wait to see how you redefine your app'sscreen sharing, streaming, and collaboration experienceusing ScreenCaptureKit.Thank you for watching!♪

4:36 -Create a single window filter

9:38 -Get dirty rects

13:34 -Get content rect, content scale and scale factor

15:37 -Create display filter with included windows

18:13 -Create display filter with included apps

20:46 -Create display filter with excluded apps

28:46 -Configure 4k 60FPS capture for streaming

30:08 -Live downgrade 4k 60FPS to 720p 15FPS

31:57 -Build a window picker with live preview

## Code Samples

```swift
// Get all available content to share via SCShareableContent


let
 shareableContent 
=
 
try
 
await
 
SCShareableContent
.excludingDesktopWindows(
false
, 
       onScreenWindowsOnly: 
false
)


// Get window you want to share from SCShareableContent


guard
 
let
 window : [
SCWindow
] 
=
 shareableContent.windows.first( where: 
                                    { 
$0
.windowID 
==
 windowID }) 
else
 { 
return
 } 


// Create SCContentFilter for Independent Window


let
 contentFilter 
=
 
SCContentFilter
(desktopIndependentWindow: window)


// Create SCStreamConfiguration object and enable audio capture


let
 streamConfig 
=
 
SCStreamConfiguration
()
streamConfig.capturesAudio 
=
 
true



// Create stream with config and filter

stream 
=
 
SCStream
(filter: contentFilter, configuration: streamConfig, delegate: 
self
)
stream.addStreamOutput(
self
, type: .screen, sampleHandlerQueue: serialQueue)
stream.startCapture()
```

```swift
// Get dirty rects from CMSampleBuffer dictionary metadata



func
 
streamUpdateHandler
(
_
 
stream
: 
SCStream
, 
sampleBuffer
: 
CMSampleBuffer
)
 {
    
guard
 
let
 attachmentsArray 
=
 
CMSampleBufferGetSampleAttachmentsArray
(sampleBuffer, 
                                                          createIfNecessary: 
false
) 
as?
 
                                                         [[
SCStreamFrameInfo
, 
Any
]], 
        
let
 attachments 
=
 attachmentsArray.first 
else
 { 
return
 }
        
        
let
 dirtyRects 
=
 attachments[.dirtyRects]    
    }
}


// Only encode and transmit the content within dirty rects
```

```swift
/* Get and use contentRect, contentScale and scaleFactor (pixel density) to convert the captured window back to its native size and pixel density */



func
 
streamUpdateHandler
(
_
 
stream
: 
SCStream
, 
sampleBuffer
: 
CMSampleBuffer
)
 {

    
guard
 
let
 attachmentsArray 
=
 
CMSampleBufferGetSampleAttachmentsArray
(sampleBuffer, 
                                                          createIfNecessary: 
false
) 
as?
 
                                                         [[
SCStreamFrameInfo
, 
Any
]], 
        
let
 attachments 
=
 attachmentsArray.first 
else
 { 
return
 }

        
let
 contentRect 
=
 attachments[.contentRect]
        
let
 contentScale 
=
 attachments[.contentScale]
        
let
 scaleFactor 
=
 attachments[.scaleFactor]

        
/* Use contentRect to crop the frame, and then contentScale and 
        scaleFactor to scale it */


    }
}
```

```swift
// Get all available content to share via SCShareableContent


let
 shareableContent 
=
 
try
 
await
 
SCShareableContent
.excludingDesktopWindows(
false
, 
                                                            onScreenWindowsOnly: 
false
)


// Create SCWindow list using SCShareableContent and the window IDs to capture


let
 includingWindows 
=
 shareableContent.windows.filter { windowIDs.contains(
$0
.windowID)}


// Create SCContentFilter for Full Display Including Windows


let
 contentFilter 
=
 
SCContentFilter
(display: display, including: includingWindows)


// Create SCStreamConfiguration object and enable audio


let
 streamConfig 
=
 
SCStreamConfiguration
()
streamConfig.capturesAudio 
=
 
true



// Create stream

stream 
=
 
SCStream
(filter: contentFilter, configuration: streamConfig, delegate: 
self
)
stream.addStreamOutput(
self
, type: .screen, sampleHandlerQueue: serialQueue)
stream.startCapture()
```

```swift
// Get all available content to share via SCShareableContent


let
 shareableContent 
=
 
try
 
await
 
SCShareableContent
.excludingDesktopWindows(
false
, onScreenWindowsOnly: 
false
)


/* Create list of SCRunningApplications using SCShareableContent and the application 
IDs you’d like to capture */


let
 includingApplications 
=
 shareableContent.applications.filter { 
    appBundleIDs.contains(
$0
.bundleIdentifier)
}


// Create SCWindow list using SCShareableContent and the window IDs to except


let
 exceptingWindows 
=
 shareableContent.windows.filter { windowIDs.contains(
$0
.windowID) }


// Create SCContentFilter for Full Display Including Apps, Excepting Windows


let
 contentFilter 
=
 
SCContentFilter
(display: display, including: includingApplications,
                           exceptingWindows: exceptingWindows)
```

```swift
// Get all available content to share via SCShareableContent


let
 shareableContent 
=
 
try
 
await
 
SCShareableContent
.excludingDesktopWindows(
false
, 
                                         onScreenWindowsOnly: 
false
)


/* Create list of SCRunningApplications using SCShareableContent and the app IDs 
you’d like to exclude */


let
 excludingApplications 
=
 shareableContent.applications.filter { 
    appBundleIDs.contains(
$0
.bundleIdentifier)
}


// Create SCWindow list using SCShareableContent and the window IDs to except


let
 exceptingWindows 
=
 shareableContent.windows.filter { windowIDs.contains(
$0
.windowID) }


// Create SCContentFilter for Full Display Excluding Windows


let
 contentFilter 
=
 
SCContentFilter
(display: display, 
                        excludingApplications: excludingApplications, 
                        exceptingWindows: exceptingWindows)
```

```swift
let
 streamConfiguration 
=
 
SCStreamConfiguration
()


// 4K output size

streamConfiguration.width  
=
 
3840

streamConfiguration.height 
=
 
2160



// 60 FPS

streamConfiguration.minimumFrameInterval 
=
 
CMTime
(value: 
1
, timescale: 
CMTimeScale
(
60
))


// 420v output pixel format for encoding

streamConfiguration.pixelFormat 
=
 kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange


// Source rect(optional)

streamConfiguration.sourceRect 
=
 
CGRectMake
(
100
, 
200
, 
3940
, 
2360
)


// Set background fill color to black

streamConfiguration.backgroundColor 
=
 
CGColor
.black 


// Include cursor in capture 

streamConfiguration.showsCursor 
=
 
true



// Valid queue depth is between 3 to 8

streamConfiguration.queueDepth 
=
 
5



// Include audio in capture

streamConfiguration.capturesAudio 
=
 
true
```

```swift
// Update output dimension down to 720p

streamConfiguration.width  
=
 
1280

streamConfiguration.height 
=
 
720



// 15FPS

streamConfiguration.minimumFrameInterval 
=
 
CMTime
(value: 
1
, timescale: 
CMTimeScale
(
15
))


// Update the configuration


try
 
await
 stream.updateConfiguration(streamConfiguration)
```

```swift
// Get all available content to share via SCShareableContent


let
 shareableContent 
=
 
try
 
await
 
SCShareableContent
.excludingDesktopWindows(
false
, 
                                        onScreenWindowsOnly: 
true
)


// Create a SCContentFilter for each shareable SCWindows


let
 contentFilters 
=
 shareableContent.windows.map { 
    
SCContentFilter
(desktopIndependentWindow: 
$0
) 
}


// Stream configuration


let
 streamConfiguration 
=
 
SCStreamConfiguration
()


// 284x182 frame output

streamConfiguration.width  
=
 
284

streamConfiguration.height 
=
 
182


// 5 FPS

streamConfiguration.minimumFrameInterval 
=
 
CMTime
(value: 
1
, timescale: 
CMTimeScale
(
5
))

// BGRA pixel format for on screen display

streamConfiguration.pixelFormat 
=
 kCVPixelFormatType_32BGRA

// No audio

streamConfiguration.capturesAudio 
=
 
false


// Does not include cursor in capture 

streamConfiguration.showsCursor 
=
 
false


// Valid queue depth is between 3 to 8



// Create a SCStream with each SCContentFilter


var
 streams: [
SCStream
] 
=
 []

for
 contentFilter 
in
 contentFilters {
    
let
 stream 
=
 
SCStream
(filter: contentFilter, streamConfiguration: streamConfig, 
                        delegate: 
self
)
    
try
 stream.addStreamOutput(
self
, type: .screen, sampleHandlerQueue: serialQueue)
    
try
 
await
 stream.startCapture()
    streams.append(stream)
}
```

