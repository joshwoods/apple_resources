# Wwdc2022 10008

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

What's new in Nearby InteractionDiscover how the Nearby Interaction framework can help you easily integrate Ultra Wideband (UWB) into your apps and hardware accessories. Learn how you can combine the visual-spatial power of ARKit with the radio sensitivity of the U1 chip to locate nearby stationary objects with precision. We'll also show you how you can create background interactions using UWB accessories paired via Bluetooth.ResourcesCore BluetoothFinding devices with precisionNearby InteractionNearby Interactions with U1PHASEHD VideoSD VideoRelated VideosWWDC21Design for spatial interactionExplore Nearby Interaction with third-party accessoriesWWDC20Meet Nearby Interaction

Discover how the Nearby Interaction framework can help you easily integrate Ultra Wideband (UWB) into your apps and hardware accessories. Learn how you can combine the visual-spatial power of ARKit with the radio sensitivity of the U1 chip to locate nearby stationary objects with precision. We'll also show you how you can create background interactions using UWB accessories paired via Bluetooth.

Core Bluetooth

Finding devices with precision

Nearby Interaction

Nearby Interactions with U1

PHASE

HD VideoSD Video

HD Video

SD Video

Design for spatial interaction

Explore Nearby Interaction with third-party accessories

Meet Nearby Interaction

Search this video…♪ instrumental hip hop music ♪♪Hi, I'm Jon Schoenberg, and I'm an engineeron the Location Technologies team at Apple.In this session, I'll cover the new features we've broughtto Nearby Interaction, that are going to enable youto build richer and more diverse experienceswith spatial awareness.The Nearby Interaction framework makes it simpleto take advantage of the capabilities of U1 --Apple's chip for Ultra Wideband technology --and enables creating precise and spatially aware interactionsbetween nearby Apple devices or accessoriescompatible with Apple's U1 chip for Ultra Wideband.Let's get started with a quick reviewof what's been available to you over the last two years.When Nearby Interaction was introduced at WWDC 2020,the functionality focused on creatingand running a session between two iPhones with U1.At WWDC 2021, the functionality was extendedto support running sessions with Apple Watch and third-partyUltra Wideband-compatible accessories.If you're interested in a deep diveinto the Nearby Interaction framework's APIs,please review the WWDC talk "Meet Nearby Interaction"from 2020and "Explore Nearby Interaction with third-party accessories"from 2021.We've been blown away by the community's responseto Nearby Interaction, and in this session,I'm excited to unveil the new capabilitiesand improvements for you.I will focus on two main topics:enhancing Nearby Interaction with ARKitand background sessions.Along the way, I'll share some improvementsthat make it easier to use the Nearby Interaction frameworkand I'll conclude with an update on third-party hardware supportthat was announced last year.We're excited about what you will dowith the new capabilities,so let's dive right into the details.I'll start with an exciting new capabilitythat tightly integrates ARKit with Nearby Interaction.This new capability enhances Nearby Interactionby leveraging the device trajectory computed from ARKit.ARKit-enhanced Nearby Interactionleverages the same underlying technologythat powers Precision Finding with AirTag,and we're making it available to you via Nearby Interaction.The best use cases are experiences that guide a userto a specific nearby object such as a misplaced item,object of interest,or object that the user wants to interact with.By integrating ARKit and Nearby Interaction,the distance and direction informationis more consistently availablethan using Nearby Interaction alone,effectively broadening the Ultra Wideband field of view.Finally, this new capability is best usedfor interacting with stationary devices.Let's jump right into a demonstrationof the possibilities this new integration of ARKitand Nearby Interaction enable with your application.I've got an application here for my Jetpack Museumthat has Ultra Wideband accessoriesto help guide users to the exhibits.Let's go find the next jetpack.As the user selects to go to the next exhibit,the application discovers the Ultra Wideband accessoryand performs the necessary exchangesto start using Nearby Interaction.The application then instructs the userto move the phone side to side while it begins to determinethe physical location of the next exhibit,using the enhanced Nearby Interaction mode with ARKit.Now that the application understands the directionthat corresponds to the next exhibit,a simple arrow icon telling the userthe direction to head to check it out appears.This rich, spatially-aware informationutilizing the combination of ARKit and Nearby Interactioncan even indicate when the exhibit is behind the userand the user is heading in a direction away from the exhibit.Finally, the application can display,in the AR world, an overlay of the next exhibit's location,and the application prompts the user to move the iPhoneup and down slightly to resolve where the exhibit isin the AR world.Once the AR content is placed in the scene,the powerful combination of Nearby Interaction --with its Ultra Wideband measurements --and ARKit, allows the user to easily head overto check out the next jetpack.I may not have found a jetpack, but I found a queen.Let's turn now to how you can enablethis enhanced Nearby Interaction mode.With iOS 15, you probably have a method in your applicationthat accepts the NIDiscoveryTokenfrom a nearby peer, creates a session configuration,and runs the NISession.Enabling the enhanced mode with ARKit is easyon new and existing uses of Nearby Interaction with a newisCameraAssistanceEnabled property on the subclassesof NIConfiguration.Setting the isCameraAssistanceEnabledproperty is all that's required to leverage the enhanced modewith ARKit.Camera assistance is available when interactingbetween two Apple devices, and an Apple deviceto third-party Ultra Wideband accessories.Let's look at the details of what happenswhen an NISession is run with camera assistance enabled.When camera assistance is enabled,an ARSession is automatically createdwithin the Nearby Interaction framework.You are not responsible for creating this ARSession.Running an NISession with camera assistance enabledwill also run the ARSession that was automatically createdwithin the Nearby Interaction framework.The ARSession is running within the application process.As a result,the application must provide a camera usage description keywithin the application's Info.plist.Be sure to make this a useful string to inform your userswhy the camera is necessary to provide a good experience.Only a single ARSession can be running for a given application.This means that if you already have an ARKit experiencein your app, it is necessary to share the ARSession you createwith the NISession.To share the ARSession with the NISession,a new setARSession method is availableon the NISession class.When setARSession is called, prior to run, on the NISession,an ARSession will not be automatically createdwithin the Nearby Interaction frameworkwhen the session is run.This ensures the application ARKit experiencehappens concurrently to the camera assistancein Nearby Interaction.In this SwiftUI example, as part of the makeUIView function,the underlying ARSession within the ARViewis shared with the NISession via the new setARSession method.If you are using an ARSession directly,it is necessary to call run on the ARSessionwith an ARWorldTrackingConfiguration.In addition, several properties must be configuredin a specific manner within this ARConfigurationto ensure high-quality performancefrom camera assistance.The worldAlignment should be set to gravity,collaboration and userFaceTracking disabled,a nil initialWorldMap, and a delegatewhose sessionShouldAttempt Relocalization methodreturns false.Let's turn to some best practiceswhen sharing an ARSession you created.In your NISessionDelegate didInvalidateWith error method,always inspect the error code.If the ARConfiguration used to run the shared ARSessiondoes not conform to the outlined properties,the NISession will be invalidated.A new NIError code invalidARConfigurationwill be returned.To receive nearby object updates in your app,continue to use the didUpdateNearbyObjects methodin your NISessionDelegate.In your didUpdateNearbyObjects method,you probably check the nearby objects for your desired peerand update the UI based on distanceand direction properties of the NINearbyObject when available,always being careful to recall these can be nil.When camera assistance is enabled,two new properties are available within the NINearbyObject.The first is horizontalAngle.This is the 1D angle in radiansindicating the azimuthal direction to the nearby object.When unavailable, this value will be nil.Second, verticalDirectionEstimateis the positional relationship to the nearby objectin the vertical dimension.This is a new VerticalDirectionEstimate type.Distance and direction represent the key spatial relationshipbetween the user's device and a nearby object.Distance is measured in meters and direction is a 3D vectorfrom your device to the nearby object.Horizontal angle is defined as the angle between the devicerunning the NISession and the nearby objectwithin a local horizontal plane.This accounts for any vertical displacementoffset between the two devices and any horizontal rotationof the device itself.While direction is 3D, horizontal angleis a 1D representation of the heading between the two devices.This horizontal angle propertyis complimentary to the direction property,and if the direction cannot be resolved,the horizontal angle can be availableto help you guide your user to a nearby object.Vertical direction estimate is a qualitative assessmentof the vertical position information.You should use it to guide the user between floor levels.Let's look now at the new VerticalDirectionEstimate type.VerticalDirectionEstimate is a nested enumwithin the NINearbyObject and representsa qualitative assessment of the vertical relationshipto the nearby object.Be sure to check if the VerticalDirectionEstimateis unknown before using the property.The vertical relationship can be same, above, below,or a special aboveOrBelow valuethat represents the nearby object is not on the same level,but not clearly above or below the device.The Ultra Wideband measurements are subject to field of viewand obstructions.The field of view for direction informationcorresponds to a cone projecting from the rear of the device.The device trajectory computed from ARKitwhen camera assistance is enabled allows the distance,direction, horizontal angle, and vertical direction estimateto be available in more scenarios, effectively expandingthe Ultra Wideband sensor field of view.Let's turn now to leveraging this integration of ARKitand Nearby Interaction to place AR objects in your scene.To make it easier for you to overlay 3D virtual contentthat represents the nearby objectonto a camera feed visualization,we've added a helper method: worldTransform on NISession.This method returns a worldTransformin ARKit's coordinate space that representsthe given nearby object's positionin the physical environment when it's available.When not available, this method returns nil.We used this method in the demonstrationto place the floating spheres above the next exhibit.We want to make it as easy as possible for youto leverage Nearby Interaction positional outputto manipulate content in the AR world in your app.Two powerful systems in iOS, combined.Your users must sweep the device sufficiently in verticaland horizontal directions to allow the camera assistanceto adequately compute the world transform.This method can return nil when the user motionis insufficient to allow the camera assistanceto fully converge to an ARKit world transform.When this transform is important to your app experience,it is important to coach the user to take actionto generate this transform.Let's look now at some additions we've madeto the NISessionDelegate to make it possiblefor you to guide the user similar to what you sawin the demonstration.To aid in guiding the user towards your object,an NISessionDelegate callback provides informationabout the Nearby Interaction algorithm convergencevia the new didUpdateAlgorithmConvergencedelegate method.Algorithm convergence can help you understand whyhorizontal angle, vertical direction estimate,and worldTransform are unavailableand what actions the user can taketo resolve those properties.The delegate providesa new NIAlgorithmConvergence objectand an optional NINearbyObject.This delegate method is only called when you have enabledcamera assistance in your NIConfiguration.Let's look at the new NIAlgorithmConvergence type.NIAlgorithmConvergence has a single-status propertythat is an NIAlgorithm ConvergenceStatus type.The NIAlgorithmConvergenceStatus type is an enum that representswhether the algorithm is converged or not.If the algorithm is not converged,an array of associated valuesNIAlgorithmConvergenceStatus .Reasons is provided.Let's return to the new delegate methodand say you want to updatethe status of the camera assistance to the user,you can switch on the convergence statusand if unknown or converged,display that information to the user.Be sure to inspect the NINearbyObject.When the object is nil,the NIAlgorithmConvergence state applies to the session itself,rather than a specific NINearbyObject.When the status is notConverged,it also includes an associated value that describes the reasonsthe algorithm is not converged.A localized description is available for this reasonto help you communicate better with your users.Let's look next at how to use these values.Inspecting the notConverged case more closelyand the associated reasons value,it is possible to guide the user to take actionsthat helps produce the desired informationabout a nearby object.The associated value is an array ofNIAlgorithmConvergence StatusReasons.The reason can indicate there's insufficient total motion,insufficient motion in the horizontal or vertical sweep,and insufficient lighting.Be mindful that multiple reasons may exist at the same timeand guide the user sequentially through each actionbased on which is most important for your application.Recall how I moved the phone in the demonstrationand needed to sweep in both the horizontaland vertical direction to resolve the world transform.That's the most important bits aboutthe enhanced Nearby Interaction mode with camera assistance.We've made some additional changesto help you better leverage this mode.Previously, a single isSupported class variable on the NISessionwas all that was necessary to check if Nearby Interactionwas supported on a given device.This is now deprecated.With the addition of camera assistance,we've made the device capabilitiessupported by Nearby Interaction more descriptivewith a new deviceCapabilities class member on the NISessionthat returns a new NIDeviceCapability object.At a minimum, checking thesupportsPreciseDistance Measurement propertyis the equivalent of the now deprecatedisSupported class variable.Once you've established that the device supportsthe precise distance measurement,you should use NIDeviceCapabilityto fully understand the capabilities availablefrom Nearby Interaction on the device running your application.It is recommended you tailor your app experienceto the capabilities of the deviceby checking the additional supportsDirectionMeasurementand supportsCameraAssistance propertiesof the NIDeviceCapability object.Not all devices will support direction measurementsnor camera assistance,so be sure to include experiences that are tailoredto the capabilities of this device.In particular, be mindful to includedistance-only experiencesin order to best support Apple Watch.That's all for camera assistance as a way to enhanceNearby Interaction with ARKit. So let's turn our attention nowto accessory background sessions.Today, you can use Nearby Interaction in your appto allow users to point to other devices, find friends,and show controls or other UIbased on distance and direction to an accessory.However, when your app transitions to the backgroundor when the user locks the screen on iOS and watchOS,any running NISessions are suspendeduntil the application returns to the foreground.This means you needed to focus on hands-on user experienceswhen interacting with your accessory.Starting in iOS 16, Nearby Interaction has gone hands-free.You're now able to use Nearby Interactionto start playing music when you walk into a roomwith a smart speaker, turn on your eBike when you get on it,or trigger other hands-free actions on an accessory.You can do this even when the userisn't actively using your appvia accessory background sessions.Let's look at how you can accomplishthis exciting new capability.Let's spend just a minute reviewing the sequencefor how to configure and run an NISession with an accessory.You might recognize this sequencefrom last year's WWDC presentation.The accessory sends its Ultra Widebandaccessory configuration data over to your applicationvia a data channel, and you create anNINearbyAccessoryConfiguration from this data.You create an NISession, set an NISessionDelegateto get Ultra Wideband measurements from the accessory.You run the NISession with your configurationand the session will return sharable configuration datato setup the accessory to interoperatewith your application.After sending this sharable configuration databack to the accessory,you are now able to receive Ultra Wideband measurementsin your application and at the accessory.For all the details on configuring and runningNearby Interaction with third-party accessories,please review last year's WWDC session.Let's look now at how you set up the new background sessions.The previous sequence diagram showed data flowingbetween your application and the accessory.It is very common to have the communication channelbetween an accessory and your application use Bluetooth LE.When paired to the accessory using Bluetooth LE,you can enable Nearby Interactionto start and continue sessions in the background.Let's look closely at how this is possible.Today, you can configure your app to use Core Bluetoothto discover, connect to, and exchange datawith Bluetooth LE accessorieswhile your app is in the background.Check out the existing Core Bluetooth Programming Guideor the WWDC session from 2017 for more details.Taking advantage of the powerful background operationsfrom Core Bluetooth to efficiently discoverthe accessory and run your application in the background,your application can start an NISessionwith a Bluetooth LE accessorythat also supports Ultra Wideband in the background.Let's look now at how the sequence diagram updatesto reflect this new mode.To interact with this accessory,first, ensure that it is Bluetooth LE-paired.Then, connect to the accessory.When the accessory generatesits accessory Ultra Wideband configuration data,it should both send it to your applicationand populate the Nearby Interaction GATT service;more on this next.Finally, when your application receivesthe accessory's configuration data, construct anNINearbyAccessoryConfiguration object using a new initializerproviding both your accessory's UWB configuration dataand its Bluetooth peer identifier.Run your NISession with this configurationand ensure you complete the setup by receivingthe sharable configuration in your NISessionDelegateand send the sharable configuration to the accessory.In order for your accessory to create a relationshipbetween its Bluetooth identifierand the Ultra Wideband configuration,it must implement the new Nearby Interaction GATT service.The Nearby Interaction service contains a single encryptedcharacteristic called Accessory Configuration Data.It contains the same UWB configuration dataused to initialize the NINearbyAccessoryConfigurationobject.iOS uses this characteristic to verify the associationbetween your Bluetooth peer identifier and your NISession.Your app cannot read from this characteristic directly.You can find out more about the detailsof this new Nearby Interaction GATT serviceon developer.apple.com/ nearby-interaction.If your accessory supports multiple NISessions in parallel,create multiple instances of Accessory Configuration Data,each with a different NISession's UWB configuration.That's what's necessary on the accessory.Let's turn to what you need to implement in your applicationby diving into some code!Accessory background sessions require that the accessoryis LE-paired to the user's iPhone.Your app is responsible for triggering this process.To do this, implement methods to scan for your accessory,connect to it, and discover its services and characteristics.Then, implement a method to readone of your accessory's encrypted characteristics.You only need to do this once.It will show the user a prompt to accept pairing.Accessory background sessions also requirea Bluetooth connection to your accessory.Your app must be able to form this connectioneven when it's backgrounded.To do this, implement a method to initiate a connection attemptto your accessory.You should do this even if the accessoryis not within Bluetooth range.Then, implement CBManagerDelegate methodsto restore state after your app is relaunched by Core Bluetoothand handle when your connection is established.Now you're ready to run an accessory background session.Create an NINearbyAccessoryConfigurationobject by providing both the accessory'sUWB configuration data and its Bluetooth peer identifierfrom the CBPeripheral identifier.Run an NISession with that configurationand it will run while your app is backgrounded.That's it!Well, there is one more settingyou need to update for your app in Xcode.This background mode requires the Nearby Interaction stringin the UIBackgroundModes array in your app's Info.plist.You can also use Xcode capabilities editorto add this background mode.You will also want to ensure you have"Uses Bluetooth LE accessories" enabled to ensure your appcan connect with the accessory in the background.One important noteabout this new accessory background session.When your application is in the background,the NISession will continue to run and will not be suspended,so Ultra Wideband measurements are available on the accessory.You must consume and act on the Ultra Wideband measurementson the accessory.Your application will not receive runtime,and you will not receive didUpdateNearbyObjectdelegate callbacksuntil your application returns to the foreground.When using this new background mode,let's review the following best practices.Triggering LE pairing with your accessorywill show the user a prompt to accept the pairing.Do this at a time that is intuitive to the userwhy they want to pair the accessory.This could be in the setup flow that it creates the relationshipwith the accessory or when the user clearly indicatestheir desire to interact with the accessory.While your app is backgrounded,your NISession will not be suspended,but it will not receive didUpdateNearbyObjectdelegate callbacks.However, your accessory will receiveUltra Wideband measurements.Process these measurements directly on your accessoryto determine what action should happen for the user.Finally, manage battery usageby only sending data from your accessory to your appduring a significant user interaction;for example, to show a notification to the user.That's all you need to know on background sessionsand leads me to the last topic on third-party hardware support.Today, I'm happy to announce that the previously availablebeta U1-compatible development kitsare now out of beta and available for wider use.Please visit developer.apple.com /nearby-interactionto find out more aboutcompatible Ultra Wideband development kits.We've also updated the specificationfor accessory manufacturers to supportthe new accessory background sessions,including the Nearby Interaction GATT service,and it is available on the same website.So, let's summarize what we've discussed in this session.Nearby Interaction now includes a new camera-assisted modethat tightly integrates ARKit and Nearby Interactionto provide a seamless experiencefor you to create spatially aware experiencesthat guide users to a nearby object.The accessory background sessions enable you to initiateand extend sessions into the backgroundfor you to build a more hands-off experiencefor your users.We've announced exciting updatesto the third-party compatible Ultra Wideband hardware support.That's it for the Nearby Interaction updates this year.Download the demos,reach out with feedback on the updated capabilities,review the updated third-party specification,and go build amazing apps with spatial experiences.Thank you.♪

♪ instrumental hip hop music ♪♪Hi, I'm Jon Schoenberg, and I'm an engineeron the Location Technologies team at Apple.

In this session, I'll cover the new features we've broughtto Nearby Interaction, that are going to enable youto build richer and more diverse experienceswith spatial awareness.

The Nearby Interaction framework makes it simpleto take advantage of the capabilities of U1 --Apple's chip for Ultra Wideband technology --and enables creating precise and spatially aware interactionsbetween nearby Apple devices or accessoriescompatible with Apple's U1 chip for Ultra Wideband.

Let's get started with a quick reviewof what's been available to you over the last two years.

When Nearby Interaction was introduced at WWDC 2020,the functionality focused on creatingand running a session between two iPhones with U1.

At WWDC 2021, the functionality was extendedto support running sessions with Apple Watch and third-partyUltra Wideband-compatible accessories.

If you're interested in a deep diveinto the Nearby Interaction framework's APIs,please review the WWDC talk "Meet Nearby Interaction"from 2020and "Explore Nearby Interaction with third-party accessories"from 2021.

We've been blown away by the community's responseto Nearby Interaction, and in this session,I'm excited to unveil the new capabilitiesand improvements for you.

I will focus on two main topics:enhancing Nearby Interaction with ARKitand background sessions.

Along the way, I'll share some improvementsthat make it easier to use the Nearby Interaction frameworkand I'll conclude with an update on third-party hardware supportthat was announced last year.

We're excited about what you will dowith the new capabilities,so let's dive right into the details.

I'll start with an exciting new capabilitythat tightly integrates ARKit with Nearby Interaction.

This new capability enhances Nearby Interactionby leveraging the device trajectory computed from ARKit.

ARKit-enhanced Nearby Interactionleverages the same underlying technologythat powers Precision Finding with AirTag,and we're making it available to you via Nearby Interaction.

The best use cases are experiences that guide a userto a specific nearby object such as a misplaced item,object of interest,or object that the user wants to interact with.

By integrating ARKit and Nearby Interaction,the distance and direction informationis more consistently availablethan using Nearby Interaction alone,effectively broadening the Ultra Wideband field of view.

Finally, this new capability is best usedfor interacting with stationary devices.

Let's jump right into a demonstrationof the possibilities this new integration of ARKitand Nearby Interaction enable with your application.

I've got an application here for my Jetpack Museumthat has Ultra Wideband accessoriesto help guide users to the exhibits.

Let's go find the next jetpack.

As the user selects to go to the next exhibit,the application discovers the Ultra Wideband accessoryand performs the necessary exchangesto start using Nearby Interaction.

The application then instructs the userto move the phone side to side while it begins to determinethe physical location of the next exhibit,using the enhanced Nearby Interaction mode with ARKit.

Now that the application understands the directionthat corresponds to the next exhibit,a simple arrow icon telling the userthe direction to head to check it out appears.

This rich, spatially-aware informationutilizing the combination of ARKit and Nearby Interactioncan even indicate when the exhibit is behind the userand the user is heading in a direction away from the exhibit.

Finally, the application can display,in the AR world, an overlay of the next exhibit's location,and the application prompts the user to move the iPhoneup and down slightly to resolve where the exhibit isin the AR world.

Once the AR content is placed in the scene,the powerful combination of Nearby Interaction --with its Ultra Wideband measurements --and ARKit, allows the user to easily head overto check out the next jetpack.

I may not have found a jetpack, but I found a queen.

Let's turn now to how you can enablethis enhanced Nearby Interaction mode.

With iOS 15, you probably have a method in your applicationthat accepts the NIDiscoveryTokenfrom a nearby peer, creates a session configuration,and runs the NISession.

Enabling the enhanced mode with ARKit is easyon new and existing uses of Nearby Interaction with a newisCameraAssistanceEnabled property on the subclassesof NIConfiguration.

Setting the isCameraAssistanceEnabledproperty is all that's required to leverage the enhanced modewith ARKit.

Camera assistance is available when interactingbetween two Apple devices, and an Apple deviceto third-party Ultra Wideband accessories.

Let's look at the details of what happenswhen an NISession is run with camera assistance enabled.

When camera assistance is enabled,an ARSession is automatically createdwithin the Nearby Interaction framework.

You are not responsible for creating this ARSession.

Running an NISession with camera assistance enabledwill also run the ARSession that was automatically createdwithin the Nearby Interaction framework.

The ARSession is running within the application process.

As a result,the application must provide a camera usage description keywithin the application's Info.plist.

Be sure to make this a useful string to inform your userswhy the camera is necessary to provide a good experience.

Only a single ARSession can be running for a given application.

This means that if you already have an ARKit experiencein your app, it is necessary to share the ARSession you createwith the NISession.

To share the ARSession with the NISession,a new setARSession method is availableon the NISession class.

When setARSession is called, prior to run, on the NISession,an ARSession will not be automatically createdwithin the Nearby Interaction frameworkwhen the session is run.

This ensures the application ARKit experiencehappens concurrently to the camera assistancein Nearby Interaction.

In this SwiftUI example, as part of the makeUIView function,the underlying ARSession within the ARViewis shared with the NISession via the new setARSession method.

If you are using an ARSession directly,it is necessary to call run on the ARSessionwith an ARWorldTrackingConfiguration.

In addition, several properties must be configuredin a specific manner within this ARConfigurationto ensure high-quality performancefrom camera assistance.

The worldAlignment should be set to gravity,collaboration and userFaceTracking disabled,a nil initialWorldMap, and a delegatewhose sessionShouldAttempt Relocalization methodreturns false.

Let's turn to some best practiceswhen sharing an ARSession you created.

In your NISessionDelegate didInvalidateWith error method,always inspect the error code.

If the ARConfiguration used to run the shared ARSessiondoes not conform to the outlined properties,the NISession will be invalidated.

A new NIError code invalidARConfigurationwill be returned.

To receive nearby object updates in your app,continue to use the didUpdateNearbyObjects methodin your NISessionDelegate.

In your didUpdateNearbyObjects method,you probably check the nearby objects for your desired peerand update the UI based on distanceand direction properties of the NINearbyObject when available,always being careful to recall these can be nil.

When camera assistance is enabled,two new properties are available within the NINearbyObject.

The first is horizontalAngle.

This is the 1D angle in radiansindicating the azimuthal direction to the nearby object.

When unavailable, this value will be nil.

Second, verticalDirectionEstimateis the positional relationship to the nearby objectin the vertical dimension.

This is a new VerticalDirectionEstimate type.

Distance and direction represent the key spatial relationshipbetween the user's device and a nearby object.

Distance is measured in meters and direction is a 3D vectorfrom your device to the nearby object.

Horizontal angle is defined as the angle between the devicerunning the NISession and the nearby objectwithin a local horizontal plane.

This accounts for any vertical displacementoffset between the two devices and any horizontal rotationof the device itself.

While direction is 3D, horizontal angleis a 1D representation of the heading between the two devices.

This horizontal angle propertyis complimentary to the direction property,and if the direction cannot be resolved,the horizontal angle can be availableto help you guide your user to a nearby object.

Vertical direction estimate is a qualitative assessmentof the vertical position information.

You should use it to guide the user between floor levels.

Let's look now at the new VerticalDirectionEstimate type.

VerticalDirectionEstimate is a nested enumwithin the NINearbyObject and representsa qualitative assessment of the vertical relationshipto the nearby object.

Be sure to check if the VerticalDirectionEstimateis unknown before using the property.

The vertical relationship can be same, above, below,or a special aboveOrBelow valuethat represents the nearby object is not on the same level,but not clearly above or below the device.

The Ultra Wideband measurements are subject to field of viewand obstructions.

The field of view for direction informationcorresponds to a cone projecting from the rear of the device.

The device trajectory computed from ARKitwhen camera assistance is enabled allows the distance,direction, horizontal angle, and vertical direction estimateto be available in more scenarios, effectively expandingthe Ultra Wideband sensor field of view.

Let's turn now to leveraging this integration of ARKitand Nearby Interaction to place AR objects in your scene.

To make it easier for you to overlay 3D virtual contentthat represents the nearby objectonto a camera feed visualization,we've added a helper method: worldTransform on NISession.

This method returns a worldTransformin ARKit's coordinate space that representsthe given nearby object's positionin the physical environment when it's available.

When not available, this method returns nil.

We used this method in the demonstrationto place the floating spheres above the next exhibit.

We want to make it as easy as possible for youto leverage Nearby Interaction positional outputto manipulate content in the AR world in your app.

Two powerful systems in iOS, combined.

Your users must sweep the device sufficiently in verticaland horizontal directions to allow the camera assistanceto adequately compute the world transform.

This method can return nil when the user motionis insufficient to allow the camera assistanceto fully converge to an ARKit world transform.

When this transform is important to your app experience,it is important to coach the user to take actionto generate this transform.

Let's look now at some additions we've madeto the NISessionDelegate to make it possiblefor you to guide the user similar to what you sawin the demonstration.

To aid in guiding the user towards your object,an NISessionDelegate callback provides informationabout the Nearby Interaction algorithm convergencevia the new didUpdateAlgorithmConvergencedelegate method.

Algorithm convergence can help you understand whyhorizontal angle, vertical direction estimate,and worldTransform are unavailableand what actions the user can taketo resolve those properties.

The delegate providesa new NIAlgorithmConvergence objectand an optional NINearbyObject.

This delegate method is only called when you have enabledcamera assistance in your NIConfiguration.

Let's look at the new NIAlgorithmConvergence type.

NIAlgorithmConvergence has a single-status propertythat is an NIAlgorithm ConvergenceStatus type.

The NIAlgorithmConvergenceStatus type is an enum that representswhether the algorithm is converged or not.

If the algorithm is not converged,an array of associated valuesNIAlgorithmConvergenceStatus .Reasons is provided.

Let's return to the new delegate methodand say you want to updatethe status of the camera assistance to the user,you can switch on the convergence statusand if unknown or converged,display that information to the user.

Be sure to inspect the NINearbyObject.

When the object is nil,the NIAlgorithmConvergence state applies to the session itself,rather than a specific NINearbyObject.

When the status is notConverged,it also includes an associated value that describes the reasonsthe algorithm is not converged.

A localized description is available for this reasonto help you communicate better with your users.

Let's look next at how to use these values.

Inspecting the notConverged case more closelyand the associated reasons value,it is possible to guide the user to take actionsthat helps produce the desired informationabout a nearby object.

The associated value is an array ofNIAlgorithmConvergence StatusReasons.

The reason can indicate there's insufficient total motion,insufficient motion in the horizontal or vertical sweep,and insufficient lighting.

Be mindful that multiple reasons may exist at the same timeand guide the user sequentially through each actionbased on which is most important for your application.

Recall how I moved the phone in the demonstrationand needed to sweep in both the horizontaland vertical direction to resolve the world transform.

That's the most important bits aboutthe enhanced Nearby Interaction mode with camera assistance.

We've made some additional changesto help you better leverage this mode.

Previously, a single isSupported class variable on the NISessionwas all that was necessary to check if Nearby Interactionwas supported on a given device.

This is now deprecated.

With the addition of camera assistance,we've made the device capabilitiessupported by Nearby Interaction more descriptivewith a new deviceCapabilities class member on the NISessionthat returns a new NIDeviceCapability object.

At a minimum, checking thesupportsPreciseDistance Measurement propertyis the equivalent of the now deprecatedisSupported class variable.

Once you've established that the device supportsthe precise distance measurement,you should use NIDeviceCapabilityto fully understand the capabilities availablefrom Nearby Interaction on the device running your application.

It is recommended you tailor your app experienceto the capabilities of the deviceby checking the additional supportsDirectionMeasurementand supportsCameraAssistance propertiesof the NIDeviceCapability object.

Not all devices will support direction measurementsnor camera assistance,so be sure to include experiences that are tailoredto the capabilities of this device.

In particular, be mindful to includedistance-only experiencesin order to best support Apple Watch.

That's all for camera assistance as a way to enhanceNearby Interaction with ARKit. So let's turn our attention nowto accessory background sessions.

Today, you can use Nearby Interaction in your appto allow users to point to other devices, find friends,and show controls or other UIbased on distance and direction to an accessory.

However, when your app transitions to the backgroundor when the user locks the screen on iOS and watchOS,any running NISessions are suspendeduntil the application returns to the foreground.

This means you needed to focus on hands-on user experienceswhen interacting with your accessory.

Starting in iOS 16, Nearby Interaction has gone hands-free.

You're now able to use Nearby Interactionto start playing music when you walk into a roomwith a smart speaker, turn on your eBike when you get on it,or trigger other hands-free actions on an accessory.

You can do this even when the userisn't actively using your appvia accessory background sessions.

Let's look at how you can accomplishthis exciting new capability.

Let's spend just a minute reviewing the sequencefor how to configure and run an NISession with an accessory.

You might recognize this sequencefrom last year's WWDC presentation.

The accessory sends its Ultra Widebandaccessory configuration data over to your applicationvia a data channel, and you create anNINearbyAccessoryConfiguration from this data.

You create an NISession, set an NISessionDelegateto get Ultra Wideband measurements from the accessory.

You run the NISession with your configurationand the session will return sharable configuration datato setup the accessory to interoperatewith your application.

After sending this sharable configuration databack to the accessory,you are now able to receive Ultra Wideband measurementsin your application and at the accessory.

For all the details on configuring and runningNearby Interaction with third-party accessories,please review last year's WWDC session.

Let's look now at how you set up the new background sessions.

The previous sequence diagram showed data flowingbetween your application and the accessory.

It is very common to have the communication channelbetween an accessory and your application use Bluetooth LE.

When paired to the accessory using Bluetooth LE,you can enable Nearby Interactionto start and continue sessions in the background.

Let's look closely at how this is possible.

Today, you can configure your app to use Core Bluetoothto discover, connect to, and exchange datawith Bluetooth LE accessorieswhile your app is in the background.

Check out the existing Core Bluetooth Programming Guideor the WWDC session from 2017 for more details.

Taking advantage of the powerful background operationsfrom Core Bluetooth to efficiently discoverthe accessory and run your application in the background,your application can start an NISessionwith a Bluetooth LE accessorythat also supports Ultra Wideband in the background.

Let's look now at how the sequence diagram updatesto reflect this new mode.

To interact with this accessory,first, ensure that it is Bluetooth LE-paired.

Then, connect to the accessory.

When the accessory generatesits accessory Ultra Wideband configuration data,it should both send it to your applicationand populate the Nearby Interaction GATT service;more on this next.

Finally, when your application receivesthe accessory's configuration data, construct anNINearbyAccessoryConfiguration object using a new initializerproviding both your accessory's UWB configuration dataand its Bluetooth peer identifier.

Run your NISession with this configurationand ensure you complete the setup by receivingthe sharable configuration in your NISessionDelegateand send the sharable configuration to the accessory.

In order for your accessory to create a relationshipbetween its Bluetooth identifierand the Ultra Wideband configuration,it must implement the new Nearby Interaction GATT service.

The Nearby Interaction service contains a single encryptedcharacteristic called Accessory Configuration Data.

It contains the same UWB configuration dataused to initialize the NINearbyAccessoryConfigurationobject.

iOS uses this characteristic to verify the associationbetween your Bluetooth peer identifier and your NISession.

Your app cannot read from this characteristic directly.

You can find out more about the detailsof this new Nearby Interaction GATT serviceon developer.apple.com/ nearby-interaction.

If your accessory supports multiple NISessions in parallel,create multiple instances of Accessory Configuration Data,each with a different NISession's UWB configuration.

That's what's necessary on the accessory.

Let's turn to what you need to implement in your applicationby diving into some code!Accessory background sessions require that the accessoryis LE-paired to the user's iPhone.

Your app is responsible for triggering this process.

To do this, implement methods to scan for your accessory,connect to it, and discover its services and characteristics.

Then, implement a method to readone of your accessory's encrypted characteristics.

You only need to do this once.

It will show the user a prompt to accept pairing.

Accessory background sessions also requirea Bluetooth connection to your accessory.

Your app must be able to form this connectioneven when it's backgrounded.

To do this, implement a method to initiate a connection attemptto your accessory.

You should do this even if the accessoryis not within Bluetooth range.

Then, implement CBManagerDelegate methodsto restore state after your app is relaunched by Core Bluetoothand handle when your connection is established.

Now you're ready to run an accessory background session.

Create an NINearbyAccessoryConfigurationobject by providing both the accessory'sUWB configuration data and its Bluetooth peer identifierfrom the CBPeripheral identifier.

Run an NISession with that configurationand it will run while your app is backgrounded.

That's it!Well, there is one more settingyou need to update for your app in Xcode.

This background mode requires the Nearby Interaction stringin the UIBackgroundModes array in your app's Info.plist.

You can also use Xcode capabilities editorto add this background mode.

You will also want to ensure you have"Uses Bluetooth LE accessories" enabled to ensure your appcan connect with the accessory in the background.

One important noteabout this new accessory background session.

When your application is in the background,the NISession will continue to run and will not be suspended,so Ultra Wideband measurements are available on the accessory.

You must consume and act on the Ultra Wideband measurementson the accessory.

Your application will not receive runtime,and you will not receive didUpdateNearbyObjectdelegate callbacksuntil your application returns to the foreground.

When using this new background mode,let's review the following best practices.

Triggering LE pairing with your accessorywill show the user a prompt to accept the pairing.

Do this at a time that is intuitive to the userwhy they want to pair the accessory.

This could be in the setup flow that it creates the relationshipwith the accessory or when the user clearly indicatestheir desire to interact with the accessory.

While your app is backgrounded,your NISession will not be suspended,but it will not receive didUpdateNearbyObjectdelegate callbacks.

However, your accessory will receiveUltra Wideband measurements.

Process these measurements directly on your accessoryto determine what action should happen for the user.

Finally, manage battery usageby only sending data from your accessory to your appduring a significant user interaction;for example, to show a notification to the user.

That's all you need to know on background sessionsand leads me to the last topic on third-party hardware support.

Today, I'm happy to announce that the previously availablebeta U1-compatible development kitsare now out of beta and available for wider use.

Please visit developer.apple.com /nearby-interactionto find out more aboutcompatible Ultra Wideband development kits.

We've also updated the specificationfor accessory manufacturers to supportthe new accessory background sessions,including the Nearby Interaction GATT service,and it is available on the same website.

So, let's summarize what we've discussed in this session.

Nearby Interaction now includes a new camera-assisted modethat tightly integrates ARKit and Nearby Interactionto provide a seamless experiencefor you to create spatially aware experiencesthat guide users to a nearby object.

The accessory background sessions enable you to initiateand extend sessions into the backgroundfor you to build a more hands-off experiencefor your users.

We've announced exciting updatesto the third-party compatible Ultra Wideband hardware support.

That's it for the Nearby Interaction updates this year.

Download the demos,reach out with feedback on the updated capabilities,review the updated third-party specification,and go build amazing apps with spatial experiences.

Thank you.

♪

## Code Samples

