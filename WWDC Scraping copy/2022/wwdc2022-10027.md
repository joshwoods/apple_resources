# Wwdc2022 10027

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Optimize your Core ML usageLearn how Core ML works with the CPU, GPU, and Neural Engine to power on-device, privacy-preserving machine learning experiences for your apps. We'll explore the latest tools for understanding and maximizing the performance of your models. We'll also show you how to generate reports to easily understand your model performance characteristics, help you gain insight into your models with the Core ML Instrument, and take you through API enhancements to further optimize Core ML integration in your apps.

To get the most out of this session, be sure to watch “Tune your Core ML models” from WWDC21.ResourcesCore MLHD VideoSD VideoRelated VideosWWDC23Improve Core ML integration with async predictionWWDC22Explore the machine learning development experience

Learn how Core ML works with the CPU, GPU, and Neural Engine to power on-device, privacy-preserving machine learning experiences for your apps. We'll explore the latest tools for understanding and maximizing the performance of your models. We'll also show you how to generate reports to easily understand your model performance characteristics, help you gain insight into your models with the Core ML Instrument, and take you through API enhancements to further optimize Core ML integration in your apps.

To get the most out of this session, be sure to watch “Tune your Core ML models” from WWDC21.

Core ML

HD VideoSD Video

HD Video

SD Video

Improve Core ML integration with async prediction

Explore the machine learning development experience

Search this video…♪ Mellow instrumental hip-hop music ♪♪Hi, my name is Ben,and I'm an engineer on the Core ML team.Today I'm going to show some of the exciting new featuresbeing added to Core ML.The focus of these features is to help youoptimize your Core ML usage.In this session, I'll go over performance toolsthat are now available to give you the information you needto understand and optimize your model's performancewhen using Core ML.Then I'll go over some enhanced APIswhich will enable you to make those optimizations.And lastly, I'll give an overviewof some additional Core ML capabilitiesand integration options.Let me begin with the performance tools.To give some background,I'll start by summarizing the standard workflowwhen using Core ML within your app.The first step is to choose your model.This may be done in a variety of ways,such as using Core ML tools to convert a PyTorchor TensorFlow model to Core ML format,using an already-existing Core ML model,or using Create ML to train and export your model.For more details on model conversionor to learn about Create ML,I recommend checking out these sessions.The next step is to integrate that model into your app.This involves bundling the model with your applicationand using the Core ML APIs to load and run inferenceon that model during your app's execution.The last step is to optimize the way you use Core ML.First, I'll go over choosing a model.There are many aspects of a modelthat you may want to considerwhen deciding if you should use that model within your app.You also may have multiple candidates of modelsyou'd like to select from,but how do you decide which one to use?You need to have a model whose functionalitywill match the requirements of the feature you wish to enable.This includes understanding the model's accuracyas well as its performance.A great way to learn about a Core ML modelis by opening it in Xcode.Just double-click on any model,and it will bring up the following.At the top, you'll find the model type,its size, and the operating system requirements.In the General tab, it shows additional detailscaptured in the model's metadata,its compute and storage precision,and info, such as class labels that it can predict.The Preview tab is for testing out your modelby providing example inputs and seeing what it predicts.The Predictions tab displays the model's inputs and outputs,as well as the types and sizesthat Core ML will expect at runtime.And finally, the Utilities tab can help with model encryptionand deployment tasks.Overall, these views give you a quick overviewof your model's functionality and preview of its accuracy.But what about your model's performance?The cost of loading a model,the amount of time a single prediction takes,or what hardware it utilizes,may be critical factors for your use case.You may have hard targetsrelated to real-time streaming data constraintsor need to make key design decisions around user interfacedepending on perceived latency.One way to get insight into the model's performanceis to do an initial integration into your appor by creating a small prototypewhich you can instrument and measure.And since performance is hardware dependent,you would likely want to do these measurementson a variety of supported hardware.Xcode and Core ML can now help you with this taskeven before writing a single line of code.Core ML now allows you to create performance reports.Let me show you.I now have the Xcode model viewer openfor the YOLOv3 object detection model.Between the Predictions and Utilities tabs,there is now a Performance tab.To generate a performance report,I'll select the plus icon at the bottom left,select the device I'd like to run on --which is my iPhone -- click next,then select which compute units I'd like Core ML to use.I'm going to leave it on All,to allow Core ML to optimize for latencywith all available compute units.Now I'll finish by pressing Run Test.To ensure the test can run,make sure the selected device is unlocked.It shows a spinning icon while the performance reportis being generated.To create the report,the model is sent over to the device,then there are several iterations of compile,load, and predictions which are run with the model.Once those are complete,the metrics in the performance report are calculated.Now it's run the model on my iPhone,and it displays the performance report.At the top, it shows some detailsabout the device where the test was runas well as which compute units were selected.Next it shows statistics about the run.The median prediction time was 22.19 millisecondsand the median load time was about 400 ms.Also, if you plan to compile your model on-device,this shows the compilation time was about 940 ms.A prediction time of around 22 ms tells me that this modelcan support about 45 frames per secondif I want to run it in real time.Since this model contains a neural network,there's a layer view displayed towards the bottomof the performance report.This shows the name and type of all of the layers,as well as which compute unit each layer ran on.A filled-in checkmark means that the layer was executedon that compute unit.An unfilled checkmark means that the layer is supportedon that compute unit,but Core ML did not choose to run it there.And an empty diamond means that the layer is not supportedon that compute unit.In this case, 54 layers were run on the GPU,and 32 layers were run on the Neural Engine.You can also filter the layersby a compute unit by clicking on it.That was how you can use Xcode 14to generate performance reports for your Core ML models.This was shown for running on an iPhone,but it will allow you to test on multiple operating systemand hardware combinations,without having to write a single line of code.Now that you've chosen your model,the next step is to integrate this model into your app.This involves bundling the model with your appand making use of Core ML APIs to load the modeland make predictions with it.In this case, I've built an appthat uses Core ML style transfer models to perform style transferon frames from a live camera session.It's working properly; however, the frame rateis slower than I'd expect, and I'd like to understand why.This is where you'd move on to step three,which is to optimize your Core ML usage.Generating a performance report can show the performancea model is capable of achieving in a stand-alone environment;however, you also need a way to profile the performanceof a model that's running live in your app.For this, you can now use the Core ML Instrumentfound in the Instruments app in Xcode 14.This Instrument allows you to visualize the performanceof your model when it runs live in your app,and helps you identify potential performance issues.Let me show how it can be used.So I'm in Xcodewith my style transfer app workspace open,and I'm ready to profile the app.I'll force-click on the Run buttonand select Profile.This will install the latest version of the codeon my device and open Instruments for mewith my targeted device and app selected.Since I want to profile my Core ML usage,I'm going to select the Core ML template.This template includes the Core ML Instrument,as well as several other useful Instrumentswhich will help you profile your Core ML usage.To capture a trace, I'll simply press Record.The app is now running on my iPhone.I will let it run for a few secondsand use a few different styles.And now I'll end the trace by pressing the Stop button.Now I have my Instruments trace.I'm going to focus on the Core ML Instrument.The Core ML Instrument shows all of the Core ML eventsthat were captured in the trace.The initial view groups all of the events into three lanes:Activity, Data, and Compute.The Activity lane shows top-level Core ML eventswhich have a one-to-one relationshipwith the actual Core ML APIs that you would call directly,such as loads and predictions.The Data lane shows events in which Core MLis performing data checks or data transformationsto make sure that it can safely workwith the model's inputs and outputs.The Compute lane shows when Core ML sendscompute requests to specific compute units,such as the Neural Engine, or the GPU.You can also select the Ungrouped viewwhere there is an individual lane for each event type.At the bottom, there's the Model Activity Aggregation view.This view provides aggregate statistics for all of the eventsdisplayed in the trace.For example, in this trace,the average model load took 17.17 ms,and the average prediction took 7.2 ms.Another note is that it can sort the events by duration.Here, the list is telling me that more time is being spentloading the model than actually making predictions with it,at a total of 6.41 seconds of loads,compared to only 2.69 seconds of predictions.Perhaps this has something to with the low frame rate.Let me try to find where all of these loads are coming from.I am noticing that I am reloading my Core ML modelprior to calling each prediction.This is generally not good practiceas I can just load the model once and hold it in memory.I'm going to jump back into my code and try to fix this.I found the area of code where I load my model.The issue here is that this is a computed properly,which means that each time I reference thestyleTransferModel variable, it will recompute the property,which means reloading the model, in this case.I can quickly fix thisby changing this to be a lazy variable.Now I'll reprofile the app to check if this has fixedthe repeated loads issue.I'll once again select the Core ML templateand capture a trace.This is much more in line with what I'd expect.The count column tells methat there are five load events total,which matches the number of styles I used in the app,and the total duration of loads is much smallerthan the total duration of predictions.Also, as I scroll through......it correctly shows repeated prediction eventswithout loads in between each one.Another note is that so far, I've only looked at the viewsthat show all Core ML model activity.In this app, there is one Core ML model per style,so I may want to breakdown the Core ML activity by model.The Instrument makes this easy to do.In the main graph, you can click the arrow at the top left,and it will make one subtrackfor each model used in the trace.Here it displays all of the different style transfer modelsthat were used.The Aggregation view also offers similar functionalityby allowing you to break down the statistics by model.Next I'd like to dive into a prediction on one of my modelsto get a better idea of how it's being run.I'll look deeper into the Watercolor model.In this prediction, the Compute lane is telling methat my model was run on a combinationof the Neural Engine and the GPU.Core ML is sending these compute requests asynchronously,so if I'm interested to see when these compute unitsare actively running the model,I can combine the Core ML Instrumentwith the GPU Instrument and the new Neural Engine Instrument.To do this, I have the three Instruments pinned here.The Core ML Instrument shows me the entire regionwhere the model ran.And within this region, the Neural Engine Instrumentshows the compute first running on the Neural Engine,then the GPU Instrument shows the modelwas handed off from the Neural Engineto finish running on the GPU.This gives me a better idea of how my modelis actually being executed on the hardware.To recap, I used the Core ML Instrument in Xcode 14to learn about my model's performancewhen running live in my app.I then identified an issuein which I was too frequently reloading my model.I fixed the issue in my code, reprofiled the application,and verified that the issue had been fixed.I was also able to combine the Core ML, GPU,and new Neural Engine Instrument to get more detailson how my model was actually run on different compute units.That was an overview of the new toolsto help you understand performance.Next, I'll go over some enhanced APIsthat can help optimize that performance.Let me start by going over how Core MLhandles model inputs and outputs.When you create a Core ML model,that model has a set of input and output features,each with a type and size.At runtime, you use Core ML APIs to provide inputsthat conform with the model's interfaceand get outputs after running inference.Let me focus on images and MultiArraysin a bit more detail.For images, Core ML supports 8-bit grayscaleand 32-bit color images with 8 bits per component.And for multidimensional arrays,Core ML supports Int32, Double, and Float32as the scalar types.If your app is already working with these types,it's simply a matter of connecting them to the model.However, sometimes your types may differ.Let me show an example.I'd like to add a new filter to my image processingand style app.This filter works to sharpen imagesby operating on a single-channel image.My app has some pre- and post-processing operationson the GPU and represents this single channelin Float16 precision.To do this, I used coremltools to convertan image-sharpening torch model to Core ML format as shown here.The model was set up to use Float16 precision computation.Also, it takes image inputs and produces image outputs.I got a model that looks like this.Note that it takes grayscale imageswhich are 8-bit for Core ML.To make this work, I had to write some codeto downcast my input from OneComponent16Halfto OneComponent8 and then upcast the outputfrom OneComponent8 to OneComponent16Half.However, this isn't the whole story.Since the model was set up to perform computationin Float16 precision, at some point,Core ML needs to convert these 8-bit inputs to Float16.It does the conversion efficiently,but when looking at an Instruments tracewith the app running, it shows this.Notice the data steps Core ML is performingbefore and after Neural Engine computation.When zooming in on the Data lane,it shows Core ML is copying datato prepare it for computation on the Neural Engine,which means converting it to Float16, in this case.This seems unfortunate since the original datawas already Float16.Ideally, these data transformations can be avoidedboth in-app and inside Core ML by making the model workdirectly with Float16 inputs and outputs.Starting in iOS 16 and macOS Ventura,Core ML will have native supportfor one OneComponent16Half grayscale images,and Float16 MultiArrays.You can create a model that accepts Float16 inputsand outputs by specifying a new color layout for imagesor a new data type for MultiArrays,while invoking the coremltools convert method.In this case, I'm specifying the input and outputof my model to be grayscale Float16 images.Since Float16 support is availablestarting in iOS 16 and macOS Ventura,these features are only availablewhen the minimum deployment target is specified as iOS 16.This is how the reconverted version of the model looks.Note that the inputs and outputs are marked as Grayscale16Half.With this Float16 support,my app can directly feed Float16 images to Core ML,which will avoid the need for downcasting the inputsand upcasting the outputs in the app.This is how it looks in the code.Since I have my input data in the formof a OneComponent16Half CVPixelBuffer,I can simply send the pixel bufferdirectly to Core ML.This does not incur any data copy or transformation.I then get a OneComponent16Half CVPixelBuffer as the output.This results in simpler code,and no data transformations required.There's also another cool thing you can do,and that's to ask Core ML to fill your preallocated buffersfor outputs instead of having Core ML allocatea new buffer for each prediction.You can do this by allocating an output backing bufferand setting it on the prediction options.For my app, I wrote a function called outputBackingBufferwhich returns a OneComponent1 HalfCVPixelBuffer.I then set this on the prediction options,and finally call the prediction method on my modelwith those prediction options.By specifying output backings,you can gain better control over the buffer managementfor model outputs.So with those changes made, to recap,here's what was shown in the Instruments tracewhen using the original version of the modelthat had 8-bit inputs and outputs.And here's how the final Instruments trace looksafter modifying the codeto provide IOSurface-backed Float16 buffersto the new Float16 version of the model.The data transformations that were previously shownin the Data lane are now gone,since Core ML no longer needs to perform them.To summarize, Core ML now has end-to-end native supportfor Float16 data.This means you can provide Float16 inputs to Core MLand have Core ML give you back Float16 outputs.You can also use the new output backing APIto have Core ML fill up your preallocated output buffersinstead of making new ones.And lastly, we recommendusing IOSurface-backed buffers whenever possible,as this allows Core ML to transfer the databetween different compute units without data copiesby taking advantage of the unified memory.Next, I'll go through a quick tourof some of the additional capabilitiesbeing added to Core ML.First is weight compression.Compressing the weights of your model may allow youto achieve similar accuracy while having a smaller model.In iOS 12, Core ML introduced post-training weight compressionwhich allows you to reduce the sizeof Core ML neural network models.We are now extending 16- and 8-bit supportto the ML Program model type, and additionally,introducing a new option to store weightsin a sparse representation.With coremltools utilities,you will now be able to quantize, palettize,and sparsify the weights for your ML Program models.Next is a new compute unit option.Core ML always aims to minimize inference latencyfor the given compute unit preference.Apps can specify this preference by settingthe MLModelConfiguration computeUnits property.In addition to the three existing compute unit options,there is now a new one called cpuAndNeuralEngine.This tells Core ML to not dispatch computation on the GPU,which can be helpful when the app uses the GPUfor other computation and, hence, prefers Core MLto limit its focus to the CPU and the Neural Engine.Next, we are adding a new way to initializeyour Core ML model instance that provides additional flexibilityin terms of model serialization.This allows you to encrypt your model datawith custom encryption schemesand decrypt it just before loading.With these new APIs, you can compile and loadan in-memory Core ML model specificationwithout requiring the compiled model to be on disk.The last update is about Swift packagesand how they work with Core ML.Packages are a great way to bundle and distributereusable code.With Xcode 14, you can include Core ML modelsin your Swift packages,and now when someone imports your package,your model will just work.Xcode will compile and bundle your Core ML model automaticallyand create the same code-generation interfaceyou're used to working with.We're excited about this change,as it'll make it a lot easier to distribute your modelsin the Swift ecosystem.That brings us to the end of this session.Core ML performance reports and Instrument in Xcode 14are here to help you analyze and optimizethe performance of the ML-powered featuresin your apps.New Float16 support and output backing APIsgives you more control of how data flows in and outof Core ML.Extended support for weight compressioncan help you minimize the size of your models.And with in-memory models and Swift package support,you have even more options when it comesto how you represent, integrate, and share Core ML models.This was Ben from the Core ML team,and have a great rest of WWDC.♪

♪ Mellow instrumental hip-hop music ♪♪Hi, my name is Ben,and I'm an engineer on the Core ML team.Today I'm going to show some of the exciting new featuresbeing added to Core ML.The focus of these features is to help youoptimize your Core ML usage.In this session, I'll go over performance toolsthat are now available to give you the information you needto understand and optimize your model's performancewhen using Core ML.Then I'll go over some enhanced APIswhich will enable you to make those optimizations.And lastly, I'll give an overviewof some additional Core ML capabilitiesand integration options.Let me begin with the performance tools.To give some background,I'll start by summarizing the standard workflowwhen using Core ML within your app.The first step is to choose your model.This may be done in a variety of ways,such as using Core ML tools to convert a PyTorchor TensorFlow model to Core ML format,using an already-existing Core ML model,or using Create ML to train and export your model.For more details on model conversionor to learn about Create ML,I recommend checking out these sessions.The next step is to integrate that model into your app.This involves bundling the model with your applicationand using the Core ML APIs to load and run inferenceon that model during your app's execution.The last step is to optimize the way you use Core ML.First, I'll go over choosing a model.There are many aspects of a modelthat you may want to considerwhen deciding if you should use that model within your app.You also may have multiple candidates of modelsyou'd like to select from,but how do you decide which one to use?You need to have a model whose functionalitywill match the requirements of the feature you wish to enable.This includes understanding the model's accuracyas well as its performance.A great way to learn about a Core ML modelis by opening it in Xcode.Just double-click on any model,and it will bring up the following.At the top, you'll find the model type,its size, and the operating system requirements.In the General tab, it shows additional detailscaptured in the model's metadata,its compute and storage precision,and info, such as class labels that it can predict.The Preview tab is for testing out your modelby providing example inputs and seeing what it predicts.The Predictions tab displays the model's inputs and outputs,as well as the types and sizesthat Core ML will expect at runtime.And finally, the Utilities tab can help with model encryptionand deployment tasks.Overall, these views give you a quick overviewof your model's functionality and preview of its accuracy.But what about your model's performance?The cost of loading a model,the amount of time a single prediction takes,or what hardware it utilizes,may be critical factors for your use case.You may have hard targetsrelated to real-time streaming data constraintsor need to make key design decisions around user interfacedepending on perceived latency.One way to get insight into the model's performanceis to do an initial integration into your appor by creating a small prototypewhich you can instrument and measure.And since performance is hardware dependent,you would likely want to do these measurementson a variety of supported hardware.Xcode and Core ML can now help you with this taskeven before writing a single line of code.Core ML now allows you to create performance reports.Let me show you.

I now have the Xcode model viewer openfor the YOLOv3 object detection model.Between the Predictions and Utilities tabs,there is now a Performance tab.To generate a performance report,I'll select the plus icon at the bottom left,select the device I'd like to run on --which is my iPhone -- click next,then select which compute units I'd like Core ML to use.I'm going to leave it on All,to allow Core ML to optimize for latencywith all available compute units.Now I'll finish by pressing Run Test.To ensure the test can run,make sure the selected device is unlocked.It shows a spinning icon while the performance reportis being generated.To create the report,the model is sent over to the device,then there are several iterations of compile,load, and predictions which are run with the model.Once those are complete,the metrics in the performance report are calculated.Now it's run the model on my iPhone,and it displays the performance report.At the top, it shows some detailsabout the device where the test was runas well as which compute units were selected.Next it shows statistics about the run.The median prediction time was 22.19 millisecondsand the median load time was about 400 ms.Also, if you plan to compile your model on-device,this shows the compilation time was about 940 ms.A prediction time of around 22 ms tells me that this modelcan support about 45 frames per secondif I want to run it in real time.

Since this model contains a neural network,there's a layer view displayed towards the bottomof the performance report.This shows the name and type of all of the layers,as well as which compute unit each layer ran on.A filled-in checkmark means that the layer was executedon that compute unit.An unfilled checkmark means that the layer is supportedon that compute unit,but Core ML did not choose to run it there.And an empty diamond means that the layer is not supportedon that compute unit.In this case, 54 layers were run on the GPU,and 32 layers were run on the Neural Engine.You can also filter the layersby a compute unit by clicking on it.

That was how you can use Xcode 14to generate performance reports for your Core ML models.This was shown for running on an iPhone,but it will allow you to test on multiple operating systemand hardware combinations,without having to write a single line of code.Now that you've chosen your model,the next step is to integrate this model into your app.This involves bundling the model with your appand making use of Core ML APIs to load the modeland make predictions with it.In this case, I've built an appthat uses Core ML style transfer models to perform style transferon frames from a live camera session.It's working properly; however, the frame rateis slower than I'd expect, and I'd like to understand why.This is where you'd move on to step three,which is to optimize your Core ML usage.Generating a performance report can show the performancea model is capable of achieving in a stand-alone environment;however, you also need a way to profile the performanceof a model that's running live in your app.For this, you can now use the Core ML Instrumentfound in the Instruments app in Xcode 14.This Instrument allows you to visualize the performanceof your model when it runs live in your app,and helps you identify potential performance issues.Let me show how it can be used.So I'm in Xcodewith my style transfer app workspace open,and I'm ready to profile the app.I'll force-click on the Run buttonand select Profile.

This will install the latest version of the codeon my device and open Instruments for mewith my targeted device and app selected.Since I want to profile my Core ML usage,I'm going to select the Core ML template.This template includes the Core ML Instrument,as well as several other useful Instrumentswhich will help you profile your Core ML usage.To capture a trace, I'll simply press Record.

The app is now running on my iPhone.I will let it run for a few secondsand use a few different styles.And now I'll end the trace by pressing the Stop button.Now I have my Instruments trace.I'm going to focus on the Core ML Instrument.The Core ML Instrument shows all of the Core ML eventsthat were captured in the trace.The initial view groups all of the events into three lanes:Activity, Data, and Compute.The Activity lane shows top-level Core ML eventswhich have a one-to-one relationshipwith the actual Core ML APIs that you would call directly,such as loads and predictions.The Data lane shows events in which Core MLis performing data checks or data transformationsto make sure that it can safely workwith the model's inputs and outputs.The Compute lane shows when Core ML sendscompute requests to specific compute units,such as the Neural Engine, or the GPU.You can also select the Ungrouped viewwhere there is an individual lane for each event type.At the bottom, there's the Model Activity Aggregation view.This view provides aggregate statistics for all of the eventsdisplayed in the trace.For example, in this trace,the average model load took 17.17 ms,and the average prediction took 7.2 ms.Another note is that it can sort the events by duration.Here, the list is telling me that more time is being spentloading the model than actually making predictions with it,at a total of 6.41 seconds of loads,compared to only 2.69 seconds of predictions.Perhaps this has something to with the low frame rate.Let me try to find where all of these loads are coming from.

I am noticing that I am reloading my Core ML modelprior to calling each prediction.This is generally not good practiceas I can just load the model once and hold it in memory.I'm going to jump back into my code and try to fix this.

I found the area of code where I load my model.The issue here is that this is a computed properly,which means that each time I reference thestyleTransferModel variable, it will recompute the property,which means reloading the model, in this case.I can quickly fix thisby changing this to be a lazy variable.

Now I'll reprofile the app to check if this has fixedthe repeated loads issue.

I'll once again select the Core ML templateand capture a trace.This is much more in line with what I'd expect.The count column tells methat there are five load events total,which matches the number of styles I used in the app,and the total duration of loads is much smallerthan the total duration of predictions.Also, as I scroll through......it correctly shows repeated prediction eventswithout loads in between each one.

Another note is that so far, I've only looked at the viewsthat show all Core ML model activity.In this app, there is one Core ML model per style,so I may want to breakdown the Core ML activity by model.The Instrument makes this easy to do.In the main graph, you can click the arrow at the top left,and it will make one subtrackfor each model used in the trace.Here it displays all of the different style transfer modelsthat were used.The Aggregation view also offers similar functionalityby allowing you to break down the statistics by model.

Next I'd like to dive into a prediction on one of my modelsto get a better idea of how it's being run.I'll look deeper into the Watercolor model.

In this prediction, the Compute lane is telling methat my model was run on a combinationof the Neural Engine and the GPU.Core ML is sending these compute requests asynchronously,so if I'm interested to see when these compute unitsare actively running the model,I can combine the Core ML Instrumentwith the GPU Instrument and the new Neural Engine Instrument.To do this, I have the three Instruments pinned here.

The Core ML Instrument shows me the entire regionwhere the model ran.

And within this region, the Neural Engine Instrumentshows the compute first running on the Neural Engine,then the GPU Instrument shows the modelwas handed off from the Neural Engineto finish running on the GPU.This gives me a better idea of how my modelis actually being executed on the hardware.To recap, I used the Core ML Instrument in Xcode 14to learn about my model's performancewhen running live in my app.I then identified an issuein which I was too frequently reloading my model.I fixed the issue in my code, reprofiled the application,and verified that the issue had been fixed.I was also able to combine the Core ML, GPU,and new Neural Engine Instrument to get more detailson how my model was actually run on different compute units.That was an overview of the new toolsto help you understand performance.Next, I'll go over some enhanced APIsthat can help optimize that performance.Let me start by going over how Core MLhandles model inputs and outputs.When you create a Core ML model,that model has a set of input and output features,each with a type and size.At runtime, you use Core ML APIs to provide inputsthat conform with the model's interfaceand get outputs after running inference.Let me focus on images and MultiArraysin a bit more detail.For images, Core ML supports 8-bit grayscaleand 32-bit color images with 8 bits per component.And for multidimensional arrays,Core ML supports Int32, Double, and Float32as the scalar types.If your app is already working with these types,it's simply a matter of connecting them to the model.However, sometimes your types may differ.Let me show an example.I'd like to add a new filter to my image processingand style app.This filter works to sharpen imagesby operating on a single-channel image.My app has some pre- and post-processing operationson the GPU and represents this single channelin Float16 precision.To do this, I used coremltools to convertan image-sharpening torch model to Core ML format as shown here.The model was set up to use Float16 precision computation.Also, it takes image inputs and produces image outputs.I got a model that looks like this.Note that it takes grayscale imageswhich are 8-bit for Core ML.To make this work, I had to write some codeto downcast my input from OneComponent16Halfto OneComponent8 and then upcast the outputfrom OneComponent8 to OneComponent16Half.However, this isn't the whole story.Since the model was set up to perform computationin Float16 precision, at some point,Core ML needs to convert these 8-bit inputs to Float16.It does the conversion efficiently,but when looking at an Instruments tracewith the app running, it shows this.Notice the data steps Core ML is performingbefore and after Neural Engine computation.When zooming in on the Data lane,it shows Core ML is copying datato prepare it for computation on the Neural Engine,which means converting it to Float16, in this case.This seems unfortunate since the original datawas already Float16.Ideally, these data transformations can be avoidedboth in-app and inside Core ML by making the model workdirectly with Float16 inputs and outputs.Starting in iOS 16 and macOS Ventura,Core ML will have native supportfor one OneComponent16Half grayscale images,and Float16 MultiArrays.You can create a model that accepts Float16 inputsand outputs by specifying a new color layout for imagesor a new data type for MultiArrays,while invoking the coremltools convert method.In this case, I'm specifying the input and outputof my model to be grayscale Float16 images.Since Float16 support is availablestarting in iOS 16 and macOS Ventura,these features are only availablewhen the minimum deployment target is specified as iOS 16.This is how the reconverted version of the model looks.Note that the inputs and outputs are marked as Grayscale16Half.With this Float16 support,my app can directly feed Float16 images to Core ML,which will avoid the need for downcasting the inputsand upcasting the outputs in the app.This is how it looks in the code.Since I have my input data in the formof a OneComponent16Half CVPixelBuffer,I can simply send the pixel bufferdirectly to Core ML.This does not incur any data copy or transformation.I then get a OneComponent16Half CVPixelBuffer as the output.This results in simpler code,and no data transformations required.There's also another cool thing you can do,and that's to ask Core ML to fill your preallocated buffersfor outputs instead of having Core ML allocatea new buffer for each prediction.You can do this by allocating an output backing bufferand setting it on the prediction options.For my app, I wrote a function called outputBackingBufferwhich returns a OneComponent1 HalfCVPixelBuffer.I then set this on the prediction options,and finally call the prediction method on my modelwith those prediction options.By specifying output backings,you can gain better control over the buffer managementfor model outputs.So with those changes made, to recap,here's what was shown in the Instruments tracewhen using the original version of the modelthat had 8-bit inputs and outputs.And here's how the final Instruments trace looksafter modifying the codeto provide IOSurface-backed Float16 buffersto the new Float16 version of the model.The data transformations that were previously shownin the Data lane are now gone,since Core ML no longer needs to perform them.To summarize, Core ML now has end-to-end native supportfor Float16 data.This means you can provide Float16 inputs to Core MLand have Core ML give you back Float16 outputs.You can also use the new output backing APIto have Core ML fill up your preallocated output buffersinstead of making new ones.And lastly, we recommendusing IOSurface-backed buffers whenever possible,as this allows Core ML to transfer the databetween different compute units without data copiesby taking advantage of the unified memory.Next, I'll go through a quick tourof some of the additional capabilitiesbeing added to Core ML.First is weight compression.Compressing the weights of your model may allow youto achieve similar accuracy while having a smaller model.In iOS 12, Core ML introduced post-training weight compressionwhich allows you to reduce the sizeof Core ML neural network models.We are now extending 16- and 8-bit supportto the ML Program model type, and additionally,introducing a new option to store weightsin a sparse representation.With coremltools utilities,you will now be able to quantize, palettize,and sparsify the weights for your ML Program models.Next is a new compute unit option.Core ML always aims to minimize inference latencyfor the given compute unit preference.Apps can specify this preference by settingthe MLModelConfiguration computeUnits property.In addition to the three existing compute unit options,there is now a new one called cpuAndNeuralEngine.This tells Core ML to not dispatch computation on the GPU,which can be helpful when the app uses the GPUfor other computation and, hence, prefers Core MLto limit its focus to the CPU and the Neural Engine.Next, we are adding a new way to initializeyour Core ML model instance that provides additional flexibilityin terms of model serialization.This allows you to encrypt your model datawith custom encryption schemesand decrypt it just before loading.With these new APIs, you can compile and loadan in-memory Core ML model specificationwithout requiring the compiled model to be on disk.The last update is about Swift packagesand how they work with Core ML.Packages are a great way to bundle and distributereusable code.With Xcode 14, you can include Core ML modelsin your Swift packages,and now when someone imports your package,your model will just work.Xcode will compile and bundle your Core ML model automaticallyand create the same code-generation interfaceyou're used to working with.We're excited about this change,as it'll make it a lot easier to distribute your modelsin the Swift ecosystem.That brings us to the end of this session.Core ML performance reports and Instrument in Xcode 14are here to help you analyze and optimizethe performance of the ML-powered featuresin your apps.New Float16 support and output backing APIsgives you more control of how data flows in and outof Core ML.Extended support for weight compressioncan help you minimize the size of your models.And with in-memory models and Swift package support,you have even more options when it comesto how you represent, integrate, and share Core ML models.This was Ben from the Core ML team,and have a great rest of WWDC.♪

## Code Samples

