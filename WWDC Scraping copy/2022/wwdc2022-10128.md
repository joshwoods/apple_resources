# Wwdc2022 10128

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Bring your world into augmented realityFollow along as we demonstrate how you can use Object Capture and RealityKit to bring real-world objects into an augmented reality game. We'll show you how to capture detailed items using the Object Capture framework, add them to a RealityKit project in Xcode, apply stylized shaders and animations, and use them as part of an AR experience. We'll also share best practices when working with ARKit, RealityKit, and Object Capture.

To get the most out of this session, we recommend first watching "Dive into RealityKit 2" and "Create 3D models with Object Capture" from WWDC21.ResourcesBuilding an Immersive Experience with RealityKitCapturing photographs for RealityKit Object CaptureCreating a Photogrammetry Command-Line AppRealityKitTaking Pictures for 3D Object CaptureUsing object capture assets in RealityKitHD VideoSD VideoRelated VideosWWDC22Discover ARKit 6WWDC22 Day 1 recapWWDC21Create 3D models with Object CaptureDive into RealityKit 2Explore advanced rendering with RealityKit 2

Follow along as we demonstrate how you can use Object Capture and RealityKit to bring real-world objects into an augmented reality game. We'll show you how to capture detailed items using the Object Capture framework, add them to a RealityKit project in Xcode, apply stylized shaders and animations, and use them as part of an AR experience. We'll also share best practices when working with ARKit, RealityKit, and Object Capture.

To get the most out of this session, we recommend first watching "Dive into RealityKit 2" and "Create 3D models with Object Capture" from WWDC21.

Building an Immersive Experience with RealityKit

Capturing photographs for RealityKit Object Capture

Creating a Photogrammetry Command-Line App

RealityKit

Taking Pictures for 3D Object Capture

Using object capture assets in RealityKit

HD VideoSD Video

HD Video

SD Video

Discover ARKit 6

WWDC22 Day 1 recap

Create 3D models with Object Capture

Dive into RealityKit 2

Explore advanced rendering with RealityKit 2

Search this video…♪ Mellow instrumental hip-hop music ♪♪Hao Tang: Hi, my name is Hao.I'm an engineer on the Object Capture team.Today, my colleague Risa and I will be showing you how to usethe Object Capture API and RealityKitto create 3D models of real-world objectsand bring them into AR.Let's get started.First, I'll give you a recap of Object Capture,which we launched as a RealityKit APIon macOS last year.Then, I'll introduce youto a couple of camera enhancements in ARKit,which allow you to capture high-res photos of your objectand can help you better integrateObject Capture into your AR applications.After that, I will go through the best practice guidelinesof Object Capture so you can continueto make the best out of this technology.In the last section, Risa will take you throughan end-to-end workflow with Object Capture in RealityKitand demonstrate how you can bring real-world objectsinto an AR experience.Let's start with a quick recap of Object Capture.Object Capture is a computer vision technologythat you can leverage to easily turn imagesof real-world objects into detailed 3D models.You begin by taking photos of your objectfrom various angles with an iPhone, iPad, or DSLR.Then, you copy those photos to a Macwhich supports Object Capture.Using the Photogrammetry API,RealityKit can transform your photos into a 3D modelin just a few minutes.The output model includes both a geometric meshas well as various material maps, including textures,that are automatically applied to your model.For more details of the Object Capture API,I would highly recommend that you watchlast year's WWDC session on Object Capture.Many developers have created amazing 3D capture appsusing Object Capture: Unity, Cinema4D, Qlone,PolyCam, PhotoCatch, just to name a few.In addition to this, we have beautiful-looking modelsthat were created using this API.Here's a few models that were created by Ethan Saadiausing the power of Object Capturewithin the PhotoCatch app.And our friend Mikko Haapoja from Shopify also generateda bunch of great-looking 3D models using this API.The detailed quality of the output 3D modelsyou get with Object Capture is highly beneficial in e-commerce.Here's the GOAT app, for example,that lets you try on a variety of shoes on your feet.All of these shoe models have been createdwith the Object Capture API which has been designedto capture the finest level of detail on them.This can go a long way in helping youwith your purchase decision on a product,or even try out an accurate fit for an object in your space.For example, the Plant Story app lets you previewreal-looking 3D models of various plants in your space,all of which have been created with Object Capture.This can help you get a sense of how much spaceyou may need for a plant,or simply see them in your space in realistic detail.Speaking about realism,were you able to spot the real plant in this video?Yes, it's the one in the white planteron the left-most corner of the table.We are very thrilled to see such a stunningand widespread use of the Object Capture APIsince its launch in 2021.Now, let's talk about some camera enhancements in ARKit,which will greatly help the quality of reconstructionwith Object Capture.A great Object Capture experience startswith taking good photos of objects from all sides.To this end, you can use any high-resolution camera,like the iPhone or iPad,or even a DSLR or mirrorless camera.If you use the Camera app on your iPhone or iPad,you can take high-quality photos with depthand gravity information which lets the Object Capture APIautomatically recover the real-world scaleand orientation of the object.In addition to that, if you use an iPhone or iPad,you can take advantage of ARKit's tracking capabilitiesto overlay a 3D guidance UI on top of the modelto get good coverage of the object from all sides.Another important thing to noteis that the higher the image resolution from your capture,the better the quality of the 3D modelthat Object Capture can produce.To that end,with this year's ARKit release we are introducing a brand-newhigh-resolution background photos API.This API lets you capture photos at native camera resolutionwhile you are still running an ARSession.It allows you to use your 3D UI overlays on top of the objectwhile taking full advantage of the camera sensor on device.On an iPhone 13, that means the full 12 megapixelsnative resolution of the Wide camera.This API is nonintrusive.It does not interrupt the continuous video streamof the current ARSession, so your app will continueto provide a smooth AR experience for your users.In addition, ARKit makes EXIF metadata availablein the photos, which allows your appto read useful information about white balance, exposure,and other settings that can be valuable for post-processing.ARKit makes it extremely easy to use this new API in your app.You can simply query a video formatthat supports high-resolution frame capturingon ARWorldTrackingConfguration, and if successful,set the new video format and run the ARSession.When it comes to capturing a high-res photo,simply call ARSession's newcaptureHighResolutionFrame API function,which will return to you a high-res photovia a completion handler asynchronously.It is that simple.We have also recognized that there are use caseswhere you may prefer manual control over the camera settingssuch as focus, exposure, or white balance.So we are providing you with a convenient wayto access the underlying AVCaptureDevice directlyand change its properties for fine-grained camera control.As shown in this code example, simply callconfigurableCaptureDevice ForPrimaryCameraon your ARWorldTrackingConfigurationto get access to the underlying AVCaptureDevice.For more details on these enhancements,I highly recommend you to check outthe "Discover ARKit 6 session" from this year's WWDC.Now, let's go through some best practice guidelineswith Object Capture.First things first; we need to choose an objectwith the right characteristics for Object Capture.A good object has adequate texture on its surface.If some regions of the object are textureless or transparent,the details in those regions may not be reconstructed well.A good object should also be free of glare and reflections.If the object does not have a matte surface,you can try to reduce the specular on itusing diffuse lighting.If you would like to flip over the objectto capture its bottom,please ensure that your object stays rigid.In other words, it should not change its shape when flipped.And lastly, a good object can contain fine structureto some degree, but you will need to usea high-resolution camera and take close-up photosto recover the fine detail of the object.The next important thingis setting up an ideal capture environment.You will want to make sure that your capture environmenthas good, even, and diffuse lighting.It is important to ensure a stable backgroundand have sufficient space around the object.If your room is dark,you can make use of a well-lit turntable.Next, we'll look at some guidelinesfor capturing good photos of your object, which in turn,will ensure that you get a good quality 3D modelfrom Object Capture.As an example, I'll show you how my colleague Mauneshused his iPhone to capture the imagesof a beautiful pirate ship that was createdby our beloved ARKit engineer, Christian Lipski.Maunesh begins by placing the pirate shipin the middle of a clean table.This makes the ship clearly stand out in the photos.He holds his iPhone steadily with two hands.As he circles around the ship slowly,he captures photos at various heights.He makes sure that the ship is large enough in the centerof the camera's field of viewso that he can capture the maximum amount of detail.He also makes sure that he always maintainsa high degree of overlap between any two adjacent photos.After he takes a good number of photos --about 80 in this case -- he flips the ship on its side,so that he can also reconstruct its bottom.He continues to capture about 20 more photos of the shipin a flipped orientation.One thing to note is that he is holding the iPhonein landscape mode.This is because he is capturing a long object,and in this case, the landscape mode helps himcapture maximum amount of detail of the object.However, he may need to use the iPhone in portrait modeif he were to capture a tall object instead.That's it!The final step in the process is to copy those photosonto a Mac and process them using the Object Capture API.You can choose from four different detail levels,which are optimized for different use cases.The reduced and medium detail levels are optimizedfor use in web-based and mobile experiences,such as viewing 3D content in AR QuickLook.The reconstructed models for those detail levelshave fewer triangles and material channels,thereby consuming less memory.The full and raw detail levelsare intended for high-end interactive use cases,such as in computer games or post-production workflows.These models contain the highest geometric detailand give you the flexibility to choose between bakedand unbaked materials,but they require more memory to reconstruct.It is important to select the right output leveldepending on your use case.For our pirate ship, we chose the medium detail level,which only took a few minutes to process it on an M1 Mac.The output 3D model looked so stunningthat we actually put together an animated clipof the pirate ship sailing in high seas.And that's the power of Object Capture for you!Ahoy!Now I'll hand it off to Risa,who will be walking you through an end-to-end workflowwith Object Capture in RealityKit.Risa Yoneyama: Thanks, Hao.Now that we have gone over the Object Capture API,I am excited to go over an end-to-end developer workflow,to bring your real-life object into AR using RealityKit.We'll walk through each step in detailwith an example workflow,so let's dive straight into a demo.My colleague Zach is an occasional woodworkerand recently built six oversized wooden chess pieces --one for each unique piece.Looking at these chess pieces,I'm inspired to create an interactive AR chess game.Previously, you'd need a 3D modelerand material specialist to create high-quality 3D modelsof real-world objects.Now, with the Object Capture API,we can simply capture these chess pieces directlyand bring them into augmented reality.Let's start off by capturing the rook.My colleague Bryan will be using this professional setup,keeping in mind the best practices we coveredin the previous section.In this case, Bryan is placing the rook on this turntablewith some professional lighting to help avoid harsh shadowsin the final output.You can also use the iPhone camera with a turntable,which provides you with automatic scale estimationand gravity vector information in your output USDZ.Please refer to the Object Capture session from 2021for more details on this.Of course, if you do not have an elaborate setuplike Bryan does here, you can also simply hold your iOS deviceand walk around your object to capture the images.Now that we have all the photos of our rook piece,I'm going to transfer these over to the Mac.I'll process these photosusing the PhotogrammetrySession APIthat was introduced in 2021.Per the best practice guidelines,I'll use the reduced detail level to reconstruct,as we want to make sure our AR app performs well.The final output of the API will be a USDZ file type model.Here is our final output of the rook chess pieceI just reconstructed.To save us some time, I've gone ahead and capturedthe other five pieces ahead of time.You may be wondering how we will create a chess gamewith only one color scheme for the chess pieces.Let's duplicate our six unique piecesand drag them into Reality Converter.I have inverted the colors in the original textureand replaced the duplicated set with this new inverted texture.This way, we can have a lighter versionand a darker version of the chess pieces,one for each player.I'll be exporting the modelswith the compressed textures option turned onin the Export menu.This will help decrease the memory footprintof the textures.Now that we have our full set of chess pieces,we are ready to bring the models into our Xcode project.I've created a chessboard using RealityKitby scaling down primitive cubes on the y-axisand alternating the colors between black and white.Here are all the chess pieces I reconstructed,laid out on the chessboard.This is already exciting to seeour real-life objects in our application,but let's start adding some featuresto make it an actual interactive game.Throughout this part of the demo,I would like to showcase several different existing technologies,so we can provide examples of how you might wantto combine the technologies to accomplish your desired output.As we'll be going over some practical use casesof advanced topics in RealityKit,I would recommend checking outthe RealityKit sessions from 2021if you are not already familiar with the APIs.I want to start by adding a start-up animationwhen we first launch the application.I am imagining an animation where the checker tilesslowly fall into placefrom slightly above its final position,all while the chess pieces also translate in together.To replicate this effect in code,all it takes is two steps.The first step is to translate both our entitiesup along the y-axis,while also uniformly scaling down the chess piece.The second step and final step is to animate both entitiesback to its original transform.The code for this is quite simple.I'll start by iterating through the checker tile entities.For each entity,I'll save the current transform of the checker tileas this will be the final position it lands on.Then, I'll move each square up 10 cm on the y-axis.We can now take advantage of the move functionto animate this back to our original transform.I also happen to know that this border USDZthat outlines the checkerboard has a built-in animation.We can use the playAnimation APIto start the animation simultaneously.I've added the exact same animation to the chess piecesbut also modifying the scale as they translate.And here we have it:a simple startup animation with just a few lines of code.However, we won't actually be able to play chesswithout the ability to move the chess pieces.Let's work on that next.Before we can start moving the chess pieces,we'll need to be able to select one.I've already added a UITapGestureRecognizerto my ARView.When the users taps a specific location,we will define a ray that starts from the camera originand goes through that 2D point.We can then perform a raycast with that ray into the 3D sceneto see if we hit any entities.I've specified my chess piece collision group as a maskas I know I only want to be able to select the chess piecesin my scene.Be mindful that the raycast functionwill ignore all entities that do not havea CollisionComponent.If we do find a chess piece, we can finally select it.Now that we know which piece is selected,I want to add an effectthat will make the piece look like it is glowing.We can leverage custom materials to achieve this;more specifically, surface shaders.Surface shaders allow you to calculateor specify material parameters through Metal,which then gets called by RealityKit's fragment shaderonce per each pixel.We can write a surface shaderthat looks like this fire effect in Metal.Then apply a custom material,with this surface shader to our rectangular prismto have the shader affect how our entity looks.Let's write some code to achieve our desired effect.I've already added a noise texture to the projectto use in this surface shader.We'll sample the texture twice,once for the overall shape of the effectand another for detail.We then take the RGB value and remap it to lookjust the way we want.Then, with the processed value we just extracted,we'll calculate the opacity of the sample pointby comparing its y-position with the image value.To give the effect some movement,we'll be moving through the y-axis of the textureas a function of time.Additionally, we will also use the facing angleof each sample point in conjunctionwith the viewing direction of the camerato fade the effect at the sides.This will soften the edges and hide the regular natureof the underlying model.Finally, we'll set the color and opacity we just calculatedusing the surface parameter functions.And here are the chess pieceswith the selection shader applied to it.They really do look like they are glowing from the inside.Now, if we combine thatwith three separate translation animations,that will result in something that looks like this.With the functionality to move chess pieces implemented,we'll also be able to capture the opponent's pieces.Just like surface shaders, geometry modifierscan be implemented using custom materials.They are a very powerful tool, as you can change vertex datasuch as position, normals, texture coordinates, and more.Each of these Metal functions will be called once per vertexby RealityKit's vertex shader.These modifications are purely transientand do not affect the vertex informationof the actual entity.I'm thinking we could add a fun geometry modifierto squash the pieces when they are captured.I have this property on my chess piececalled capturedProgress to represent the progressof the capturing animation from 0 to 1.Since capturing is a user-initiated action,we somehow need to tell the geometry modifierwhen to start its animation.The good thing is you can do thisby setting the custom property on a customMaterial.This allows data to be shared between the CPU and the GPU.We will specifically use the custom value property hereand pass the animation progress to the geometry modifier.To extract the animation progress from the Metal side,we can use the custom parameter on uniforms.Since I want to scale the object vertically,as if it is being squashed by another piece,we will set the scale axis as the y-direction.To add some complexity to the animation,we will also change the geometry in the x-axisto create a wave effect.The offset of the vertex can be set using theset_model_position_ offset function.Here is the final product of our geometry modifier.You can see that it scales up a bit before collapsing down,while being stretched vertically along the x-axis.As a chess novice myself,I thought it might be helpful to add a featureto indicate where your selected piece can move toto help me learn the game.Since the checker pieces are each individual entitieswith their own Model Component,I can apply a pulsing effect using a surface shaderto potential moves to distinguish them from others.Next, I'll add a post-processing effect called "bloom"to accentuate the effect even more.Again, we're using the custom parameter herewe used in the surface shader for the glow effect.In this case, we are passing in a Boolean from the CPU sideto our Metal surface shader.If this checker is a possible move,I want to add a pulsing effect by changing the color.We'll specifically add the pulse to the emissive color here.Lastly, I'll add the bloom effect to the entire view.Bloom is a post-processing effect that producesfeathers of light from the borders of bright areas.We can accomplish this effect by taking advantageof the render callbacks property on ARView.We will write the bloom effect using the already built-inMetal performance shader functions.Next, we'll simply set the renderCallbacks.postProcessclosure as our bloom function we just defined.When we pulse our checkers, we are pulsing to a white colorwhich will now be further emphasizedwith the bloom effect.With the surface shader and bloom effect together,we can see exactly where we can move our pieces to.Finally, let's combine everything we have togetherto see our real-life chess pieces come to lifein our AR app.We can see how all the features we addedlook in our environment.For your convenience we have linkedthe Capture Chess sample project to the session resources.Please download it and try it out for yourselfto see it in your environment.And it's that simple.From capture to reconstruction of the oversized chess pieces,then into our augmented reality app.We've covered a lot in this session todayso let's summarize some of the key points.We first started off by recapping the Object Capture APIthat we announced in 2021.We then went over a new API in ARKitthat enables capturing photos on-demandat native camera resolution during an active ARSession.To help you get the most out of the Object Capture technology,we listed types of objects that are suited for reconstruction,ideal environments to get high-quality images,and the recommended flow to followwhile capturing your object.For the latter part of this session,we walked through an example end-to-end developer workflow.We captured photos of the oversized chess piecesand used the images as input to the PhotogrammetrySession APIto create 3D models of them.Then, we imported the models into Reality Converterto replace some textures.And finally, we slowly built up our chess gameto see our chess pieces in action in AR.And that's it for our session today.Thank you so much for watching.Ahoy!♪

♪ Mellow instrumental hip-hop music ♪♪Hao Tang: Hi, my name is Hao.I'm an engineer on the Object Capture team.Today, my colleague Risa and I will be showing you how to usethe Object Capture API and RealityKitto create 3D models of real-world objectsand bring them into AR.Let's get started.First, I'll give you a recap of Object Capture,which we launched as a RealityKit APIon macOS last year.Then, I'll introduce youto a couple of camera enhancements in ARKit,which allow you to capture high-res photos of your objectand can help you better integrateObject Capture into your AR applications.After that, I will go through the best practice guidelinesof Object Capture so you can continueto make the best out of this technology.In the last section, Risa will take you throughan end-to-end workflow with Object Capture in RealityKitand demonstrate how you can bring real-world objectsinto an AR experience.Let's start with a quick recap of Object Capture.Object Capture is a computer vision technologythat you can leverage to easily turn imagesof real-world objects into detailed 3D models.You begin by taking photos of your objectfrom various angles with an iPhone, iPad, or DSLR.Then, you copy those photos to a Macwhich supports Object Capture.Using the Photogrammetry API,RealityKit can transform your photos into a 3D modelin just a few minutes.The output model includes both a geometric meshas well as various material maps, including textures,that are automatically applied to your model.For more details of the Object Capture API,I would highly recommend that you watchlast year's WWDC session on Object Capture.Many developers have created amazing 3D capture appsusing Object Capture: Unity, Cinema4D, Qlone,PolyCam, PhotoCatch, just to name a few.In addition to this, we have beautiful-looking modelsthat were created using this API.Here's a few models that were created by Ethan Saadiausing the power of Object Capturewithin the PhotoCatch app.And our friend Mikko Haapoja from Shopify also generateda bunch of great-looking 3D models using this API.The detailed quality of the output 3D modelsyou get with Object Capture is highly beneficial in e-commerce.Here's the GOAT app, for example,that lets you try on a variety of shoes on your feet.All of these shoe models have been createdwith the Object Capture API which has been designedto capture the finest level of detail on them.This can go a long way in helping youwith your purchase decision on a product,or even try out an accurate fit for an object in your space.For example, the Plant Story app lets you previewreal-looking 3D models of various plants in your space,all of which have been created with Object Capture.This can help you get a sense of how much spaceyou may need for a plant,or simply see them in your space in realistic detail.Speaking about realism,were you able to spot the real plant in this video?Yes, it's the one in the white planteron the left-most corner of the table.We are very thrilled to see such a stunningand widespread use of the Object Capture APIsince its launch in 2021.Now, let's talk about some camera enhancements in ARKit,which will greatly help the quality of reconstructionwith Object Capture.A great Object Capture experience startswith taking good photos of objects from all sides.To this end, you can use any high-resolution camera,like the iPhone or iPad,or even a DSLR or mirrorless camera.If you use the Camera app on your iPhone or iPad,you can take high-quality photos with depthand gravity information which lets the Object Capture APIautomatically recover the real-world scaleand orientation of the object.In addition to that, if you use an iPhone or iPad,you can take advantage of ARKit's tracking capabilitiesto overlay a 3D guidance UI on top of the modelto get good coverage of the object from all sides.Another important thing to noteis that the higher the image resolution from your capture,the better the quality of the 3D modelthat Object Capture can produce.To that end,with this year's ARKit release we are introducing a brand-newhigh-resolution background photos API.This API lets you capture photos at native camera resolutionwhile you are still running an ARSession.It allows you to use your 3D UI overlays on top of the objectwhile taking full advantage of the camera sensor on device.On an iPhone 13, that means the full 12 megapixelsnative resolution of the Wide camera.This API is nonintrusive.It does not interrupt the continuous video streamof the current ARSession, so your app will continueto provide a smooth AR experience for your users.In addition, ARKit makes EXIF metadata availablein the photos, which allows your appto read useful information about white balance, exposure,and other settings that can be valuable for post-processing.ARKit makes it extremely easy to use this new API in your app.You can simply query a video formatthat supports high-resolution frame capturingon ARWorldTrackingConfguration, and if successful,set the new video format and run the ARSession.When it comes to capturing a high-res photo,simply call ARSession's newcaptureHighResolutionFrame API function,which will return to you a high-res photovia a completion handler asynchronously.It is that simple.We have also recognized that there are use caseswhere you may prefer manual control over the camera settingssuch as focus, exposure, or white balance.So we are providing you with a convenient wayto access the underlying AVCaptureDevice directlyand change its properties for fine-grained camera control.As shown in this code example, simply callconfigurableCaptureDevice ForPrimaryCameraon your ARWorldTrackingConfigurationto get access to the underlying AVCaptureDevice.For more details on these enhancements,I highly recommend you to check outthe "Discover ARKit 6 session" from this year's WWDC.Now, let's go through some best practice guidelineswith Object Capture.First things first; we need to choose an objectwith the right characteristics for Object Capture.A good object has adequate texture on its surface.If some regions of the object are textureless or transparent,the details in those regions may not be reconstructed well.A good object should also be free of glare and reflections.If the object does not have a matte surface,you can try to reduce the specular on itusing diffuse lighting.If you would like to flip over the objectto capture its bottom,please ensure that your object stays rigid.In other words, it should not change its shape when flipped.And lastly, a good object can contain fine structureto some degree, but you will need to usea high-resolution camera and take close-up photosto recover the fine detail of the object.The next important thingis setting up an ideal capture environment.You will want to make sure that your capture environmenthas good, even, and diffuse lighting.It is important to ensure a stable backgroundand have sufficient space around the object.If your room is dark,you can make use of a well-lit turntable.Next, we'll look at some guidelinesfor capturing good photos of your object, which in turn,will ensure that you get a good quality 3D modelfrom Object Capture.As an example, I'll show you how my colleague Mauneshused his iPhone to capture the imagesof a beautiful pirate ship that was createdby our beloved ARKit engineer, Christian Lipski.Maunesh begins by placing the pirate shipin the middle of a clean table.This makes the ship clearly stand out in the photos.He holds his iPhone steadily with two hands.As he circles around the ship slowly,he captures photos at various heights.He makes sure that the ship is large enough in the centerof the camera's field of viewso that he can capture the maximum amount of detail.He also makes sure that he always maintainsa high degree of overlap between any two adjacent photos.After he takes a good number of photos --about 80 in this case -- he flips the ship on its side,so that he can also reconstruct its bottom.He continues to capture about 20 more photos of the shipin a flipped orientation.One thing to note is that he is holding the iPhonein landscape mode.This is because he is capturing a long object,and in this case, the landscape mode helps himcapture maximum amount of detail of the object.However, he may need to use the iPhone in portrait modeif he were to capture a tall object instead.

That's it!The final step in the process is to copy those photosonto a Mac and process them using the Object Capture API.You can choose from four different detail levels,which are optimized for different use cases.The reduced and medium detail levels are optimizedfor use in web-based and mobile experiences,such as viewing 3D content in AR QuickLook.The reconstructed models for those detail levelshave fewer triangles and material channels,thereby consuming less memory.The full and raw detail levelsare intended for high-end interactive use cases,such as in computer games or post-production workflows.These models contain the highest geometric detailand give you the flexibility to choose between bakedand unbaked materials,but they require more memory to reconstruct.It is important to select the right output leveldepending on your use case.For our pirate ship, we chose the medium detail level,which only took a few minutes to process it on an M1 Mac.The output 3D model looked so stunningthat we actually put together an animated clipof the pirate ship sailing in high seas.And that's the power of Object Capture for you!Ahoy!Now I'll hand it off to Risa,who will be walking you through an end-to-end workflowwith Object Capture in RealityKit.Risa Yoneyama: Thanks, Hao.Now that we have gone over the Object Capture API,I am excited to go over an end-to-end developer workflow,to bring your real-life object into AR using RealityKit.We'll walk through each step in detailwith an example workflow,so let's dive straight into a demo.My colleague Zach is an occasional woodworkerand recently built six oversized wooden chess pieces --one for each unique piece.Looking at these chess pieces,I'm inspired to create an interactive AR chess game.Previously, you'd need a 3D modelerand material specialist to create high-quality 3D modelsof real-world objects.Now, with the Object Capture API,we can simply capture these chess pieces directlyand bring them into augmented reality.Let's start off by capturing the rook.My colleague Bryan will be using this professional setup,keeping in mind the best practices we coveredin the previous section.In this case, Bryan is placing the rook on this turntablewith some professional lighting to help avoid harsh shadowsin the final output.You can also use the iPhone camera with a turntable,which provides you with automatic scale estimationand gravity vector information in your output USDZ.Please refer to the Object Capture session from 2021for more details on this.Of course, if you do not have an elaborate setuplike Bryan does here, you can also simply hold your iOS deviceand walk around your object to capture the images.Now that we have all the photos of our rook piece,I'm going to transfer these over to the Mac.I'll process these photosusing the PhotogrammetrySession APIthat was introduced in 2021.Per the best practice guidelines,I'll use the reduced detail level to reconstruct,as we want to make sure our AR app performs well.The final output of the API will be a USDZ file type model.Here is our final output of the rook chess pieceI just reconstructed.To save us some time, I've gone ahead and capturedthe other five pieces ahead of time.You may be wondering how we will create a chess gamewith only one color scheme for the chess pieces.Let's duplicate our six unique piecesand drag them into Reality Converter.I have inverted the colors in the original textureand replaced the duplicated set with this new inverted texture.This way, we can have a lighter versionand a darker version of the chess pieces,one for each player.I'll be exporting the modelswith the compressed textures option turned onin the Export menu.This will help decrease the memory footprintof the textures.

Now that we have our full set of chess pieces,we are ready to bring the models into our Xcode project.I've created a chessboard using RealityKitby scaling down primitive cubes on the y-axisand alternating the colors between black and white.Here are all the chess pieces I reconstructed,laid out on the chessboard.This is already exciting to seeour real-life objects in our application,but let's start adding some featuresto make it an actual interactive game.Throughout this part of the demo,I would like to showcase several different existing technologies,so we can provide examples of how you might wantto combine the technologies to accomplish your desired output.As we'll be going over some practical use casesof advanced topics in RealityKit,I would recommend checking outthe RealityKit sessions from 2021if you are not already familiar with the APIs.I want to start by adding a start-up animationwhen we first launch the application.I am imagining an animation where the checker tilesslowly fall into placefrom slightly above its final position,all while the chess pieces also translate in together.To replicate this effect in code,all it takes is two steps.The first step is to translate both our entitiesup along the y-axis,while also uniformly scaling down the chess piece.The second step and final step is to animate both entitiesback to its original transform.The code for this is quite simple.I'll start by iterating through the checker tile entities.For each entity,I'll save the current transform of the checker tileas this will be the final position it lands on.Then, I'll move each square up 10 cm on the y-axis.We can now take advantage of the move functionto animate this back to our original transform.I also happen to know that this border USDZthat outlines the checkerboard has a built-in animation.We can use the playAnimation APIto start the animation simultaneously.I've added the exact same animation to the chess piecesbut also modifying the scale as they translate.And here we have it:a simple startup animation with just a few lines of code.However, we won't actually be able to play chesswithout the ability to move the chess pieces.Let's work on that next.Before we can start moving the chess pieces,we'll need to be able to select one.I've already added a UITapGestureRecognizerto my ARView.When the users taps a specific location,we will define a ray that starts from the camera originand goes through that 2D point.We can then perform a raycast with that ray into the 3D sceneto see if we hit any entities.I've specified my chess piece collision group as a maskas I know I only want to be able to select the chess piecesin my scene.Be mindful that the raycast functionwill ignore all entities that do not havea CollisionComponent.If we do find a chess piece, we can finally select it.Now that we know which piece is selected,I want to add an effectthat will make the piece look like it is glowing.We can leverage custom materials to achieve this;more specifically, surface shaders.Surface shaders allow you to calculateor specify material parameters through Metal,which then gets called by RealityKit's fragment shaderonce per each pixel.We can write a surface shaderthat looks like this fire effect in Metal.Then apply a custom material,with this surface shader to our rectangular prismto have the shader affect how our entity looks.Let's write some code to achieve our desired effect.I've already added a noise texture to the projectto use in this surface shader.We'll sample the texture twice,once for the overall shape of the effectand another for detail.We then take the RGB value and remap it to lookjust the way we want.Then, with the processed value we just extracted,we'll calculate the opacity of the sample pointby comparing its y-position with the image value.To give the effect some movement,we'll be moving through the y-axis of the textureas a function of time.Additionally, we will also use the facing angleof each sample point in conjunctionwith the viewing direction of the camerato fade the effect at the sides.This will soften the edges and hide the regular natureof the underlying model.Finally, we'll set the color and opacity we just calculatedusing the surface parameter functions.And here are the chess pieceswith the selection shader applied to it.They really do look like they are glowing from the inside.Now, if we combine thatwith three separate translation animations,that will result in something that looks like this.With the functionality to move chess pieces implemented,we'll also be able to capture the opponent's pieces.Just like surface shaders, geometry modifierscan be implemented using custom materials.They are a very powerful tool, as you can change vertex datasuch as position, normals, texture coordinates, and more.Each of these Metal functions will be called once per vertexby RealityKit's vertex shader.These modifications are purely transientand do not affect the vertex informationof the actual entity.I'm thinking we could add a fun geometry modifierto squash the pieces when they are captured.I have this property on my chess piececalled capturedProgress to represent the progressof the capturing animation from 0 to 1.Since capturing is a user-initiated action,we somehow need to tell the geometry modifierwhen to start its animation.The good thing is you can do thisby setting the custom property on a customMaterial.This allows data to be shared between the CPU and the GPU.We will specifically use the custom value property hereand pass the animation progress to the geometry modifier.To extract the animation progress from the Metal side,we can use the custom parameter on uniforms.Since I want to scale the object vertically,as if it is being squashed by another piece,we will set the scale axis as the y-direction.To add some complexity to the animation,we will also change the geometry in the x-axisto create a wave effect.The offset of the vertex can be set using theset_model_position_ offset function.Here is the final product of our geometry modifier.You can see that it scales up a bit before collapsing down,while being stretched vertically along the x-axis.As a chess novice myself,I thought it might be helpful to add a featureto indicate where your selected piece can move toto help me learn the game.Since the checker pieces are each individual entitieswith their own Model Component,I can apply a pulsing effect using a surface shaderto potential moves to distinguish them from others.Next, I'll add a post-processing effect called "bloom"to accentuate the effect even more.Again, we're using the custom parameter herewe used in the surface shader for the glow effect.In this case, we are passing in a Boolean from the CPU sideto our Metal surface shader.If this checker is a possible move,I want to add a pulsing effect by changing the color.We'll specifically add the pulse to the emissive color here.Lastly, I'll add the bloom effect to the entire view.Bloom is a post-processing effect that producesfeathers of light from the borders of bright areas.We can accomplish this effect by taking advantageof the render callbacks property on ARView.We will write the bloom effect using the already built-inMetal performance shader functions.Next, we'll simply set the renderCallbacks.postProcessclosure as our bloom function we just defined.When we pulse our checkers, we are pulsing to a white colorwhich will now be further emphasizedwith the bloom effect.With the surface shader and bloom effect together,we can see exactly where we can move our pieces to.Finally, let's combine everything we have togetherto see our real-life chess pieces come to lifein our AR app.We can see how all the features we addedlook in our environment.For your convenience we have linkedthe Capture Chess sample project to the session resources.Please download it and try it out for yourselfto see it in your environment.And it's that simple.From capture to reconstruction of the oversized chess pieces,then into our augmented reality app.We've covered a lot in this session todayso let's summarize some of the key points.We first started off by recapping the Object Capture APIthat we announced in 2021.We then went over a new API in ARKitthat enables capturing photos on-demandat native camera resolution during an active ARSession.To help you get the most out of the Object Capture technology,we listed types of objects that are suited for reconstruction,ideal environments to get high-quality images,and the recommended flow to followwhile capturing your object.For the latter part of this session,we walked through an example end-to-end developer workflow.We captured photos of the oversized chess piecesand used the images as input to the PhotogrammetrySession APIto create 3D models of them.Then, we imported the models into Reality Converterto replace some textures.And finally, we slowly built up our chess gameto see our chess pieces in action in AR.And that's it for our session today.Thank you so much for watching.Ahoy!♪

6:20 -HighRes capturing

17:00 -Chessboard animation

18:00 -select chess piece

21:16 -capture geometry modifier

23:00 -highlight potential moves using bloom

23:20 -Import MetalPerformanceShaders

## Code Samples

```swift
if
 
let
 hiResCaptureVideoFormat 
=
 
ARWorldTrackingConfiguration
.recommendedVideoFormatForHighResolutionFrameCapturing {
    
// Assign the video format that supports hi-res capturing.

config.videoFormat 
=
 hiResCaptureVideoFormat
}

// Run the session.

session.run(config)

session.captureHighResolutionFrame { (frame, error) 
in

   
if
 
let
 frame 
=
 frame {
      
// save frame.capturedImage 

      
// …   

   }
}
```

```swift
// Board Animation


class
 
Chessboard
: 
Entity
 {
    
func
 
playAnimation
() {
        checkers
            .forEach { entity 
in

                
let
 currentTransform 
=
 entity.transform
        
// Move checker square 10cm up

                entity.transform.translation 
+=
 
SIMD3
<
Float
>(
0
, 
0.1
, 
0
)
                entity.move(to: currentTransform,
                    relativeTo: entity.parent,
                    duration: 
BoardGame
.startupAnimationDuration)
            }
        
        
// Play built-in animation for board border

        border.availableAnimations.forEach {
            border.playAnimation(
$0
)
        }
    }
}
```

```swift
// Select chess piece


class
 
ChessViewport
: 
ARView
 {
    
@objc

    
func
 
handleTap
(
sender
: 
UITapGestureRecognizer
) {
        
guard
 
let
 ray 
=
 ray(through: sender.location(in: 
self
)) 
else
 { 
return
 }

        
// No piece is selected yet, we want to select one

        
guard
 
let
 raycastResult 
=
 scene.raycast(origin: ray.origin,
                                                direction: ray.direction,
                                                length: 
5
,
                                                query: .nearest,
                                                mask: .piece).first,
              
let
 piece 
=
 raycastResult.entity.parentChessPiece 
else
 {
            
return

        }
        boardGame.select(piece)
        gameManager.selectedPiece 
=
 piece
    }
}
```

```swift
// Capture Geometry Modifier


class
 
ChessPiece
: 
Entity
, 
HasChessPiece
 {
    
var
 capturedProgress: 
Float

        
get
 {
            (pieceEntity
?
.model
?
.materials.first 
as?
 
CustomMaterial
)
?
.custom.value[
0
] 
??
 
0

        }
        
set
 {
            pieceEntity
?
.modifyMaterials { material 
in

                
guard
 
var
 customMaterial 
=
 material 
as?
 
CustomMaterial
 
else
 {
                    
return
 material
                }
                customMaterial.custom.value 
=
 
SIMD4
<
Float
>(newValue, 
0
, 
0
, 
0
)
                
return
 customMaterial
            }
        }
    }
}
```

```swift
// Checker animation to show potential moves

void checkerSurface(realitykit::surface_parameters params,
                    float amplitude,
                    bool isBlack 
=
 
false
)
{
    
// ...

    bool isPossibleMove 
=
 params.uniforms().custom_parameter()[
0
];
    
if
 (isPossibleMove) {
        const float a 
=
 amplitude 
*
 sin(params.uniforms().time() 
*
 
M_PI_F
) 
+
 amplitude;
        params.surface().set_emissive_color(half3(a));
        
if
 (isBlack) {
            params.surface().set_base_color(half3(a));
        }
    }
}
```

```swift
import
 MetalPerformanceShaders


class
 
ChessViewport
: 
ARView
 {
    
init
(
gameManager
: 
GameManager
) {
        
/// ...

        renderCallbacks.postProcess 
=
 postEffectBloom
    }

    
func
 
postEffectBloom
(
context
: 
ARView
.
PostProcessContext
) {
        
let
 brightness 
=
 
MPSImageThresholdToZero
(device: context.device,
                                                 thresholdValue: 
0.85
,
                                                 linearGrayColorTransform: 
nil
)
        brightness.encode(commandBuffer: context.commandBuffer,
                          sourceTexture: context.sourceColorTexture,
                          destinationTexture: bloomTexture
!
)
        
/// ...

    }
}
```

