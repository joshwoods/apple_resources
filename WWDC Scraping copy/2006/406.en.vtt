WEBVTT

00:00:10.570 --> 00:00:13.600
So, performance.

00:00:13.650 --> 00:00:17.680
Along with user interface and
documentation improvements,

00:00:17.680 --> 00:00:22.270
it's probably the longest standing
feature on most of your to-do lists.

00:00:23.150 --> 00:00:24.860
certainly on mine.

00:00:24.870 --> 00:00:29.300
But before we talk a little
bit about performance and the

00:00:29.300 --> 00:00:32.120
black art of performance tuning,

00:00:32.860 --> 00:00:36.290
Probably a good idea to think a
little bit about why we care about

00:00:36.300 --> 00:00:37.900
performance in the first place.

00:00:37.900 --> 00:00:40.960
I mean,
performance tuning is a lot of work.

00:00:40.960 --> 00:00:43.740
It can sometimes lead to some
fairly uncertain results.

00:00:43.760 --> 00:00:44.800
We're all pretty busy anyway.

00:00:44.800 --> 00:00:48.410
I mean, if it's not a sure thing,
why bother at all?

00:00:48.860 --> 00:00:52.700
So first and foremost,
if you're competing in the marketplace

00:00:52.700 --> 00:00:56.140
against other applications,
superior application performance is

00:00:56.140 --> 00:00:58.550
a first-order competitive advantage.

00:00:58.580 --> 00:01:02.160
Your customers that are dutiful
at any rate will be comparing

00:01:02.160 --> 00:01:04.760
your app against others.

00:01:04.800 --> 00:01:08.960
Magazine reviews will also run
competitive comparisons in which

00:01:08.960 --> 00:01:12.430
a 1% or 2% performance margin may
mean the difference between the

00:01:12.430 --> 00:01:16.380
top and the bottom of the list of
their recommended applications.

00:01:16.780 --> 00:01:21.610
In addition to that,
your customers themselves are probably

00:01:21.610 --> 00:01:23.580
your most powerful advertising medium.

00:01:23.580 --> 00:01:26.230
If they like your application
because it performs well,

00:01:26.300 --> 00:01:28.460
they'll say nice things
about you to their coworkers,

00:01:28.500 --> 00:01:31.620
to other people in the industry
at conferences like this,

00:01:31.620 --> 00:01:35.570
and they'll also be more favorably
inclined towards your other products.

00:01:36.910 --> 00:01:40.640
You can impress your customers
a couple of different ways.

00:01:40.640 --> 00:01:44.430
With the push towards more
mobile systems in particular,

00:01:44.530 --> 00:01:47.030
a theme you'll be hearing
from me quite a bit today is

00:01:47.160 --> 00:01:49.520
efficiency versus performance.

00:01:49.560 --> 00:01:51.320
If your application
not only performs well,

00:01:51.320 --> 00:01:54.200
but is efficient on a portable system,
that means more battery

00:01:54.200 --> 00:01:55.440
life and less heat.

00:01:55.440 --> 00:01:59.710
Both of those things are things that
customers are very sensitive to.

00:02:00.220 --> 00:02:03.140
In addition to that,
if your application performs

00:02:03.140 --> 00:02:05.640
its basic tasks well,
you have effectively

00:02:05.640 --> 00:02:07.190
surplus performance left.

00:02:07.280 --> 00:02:12.340
You can add animations,
other user interface niceties that will

00:02:12.340 --> 00:02:15.100
again impress and please your customers.

00:02:15.100 --> 00:02:19.950
You can also potentially do more complex,
more sophisticated things in your

00:02:19.950 --> 00:02:22.100
application's regular workflow.

00:02:22.250 --> 00:02:25.100
And if you're trying to add a
new feature to your application,

00:02:25.100 --> 00:02:28.180
the performance of that feature is
often one of the gating criteria.

00:02:28.190 --> 00:02:29.580
If you can't make it fast enough,
it'll never make it

00:02:29.580 --> 00:02:30.100
in in the first place.

00:02:30.100 --> 00:02:33.820
And so performance doesn't
just adjust how your customers

00:02:33.870 --> 00:02:36.090
feel about existing features.

00:02:36.150 --> 00:02:40.090
It literally determines whether or not
you'll get a feature in the first place.

00:02:41.330 --> 00:02:47.510
We're talking today particularly
about resource-hungry applications,

00:02:47.510 --> 00:02:51.450
and so it's relevant and important
to think about what hunger actually

00:02:51.490 --> 00:02:56.560
is in this context and how that
affects the distribution of resources.

00:02:56.650 --> 00:03:01.440
Mac OS X's fairly free
with its resources.

00:03:01.460 --> 00:03:05.460
The overall philosophy is that if
an application wants something,

00:03:05.550 --> 00:03:10.290
the system will give
it to that application.

00:03:11.350 --> 00:03:13.800
So if we're talking about resources,
what sort of resources?

00:03:13.810 --> 00:03:17.500
We have CPU time,
time on I/O peripherals.

00:03:17.540 --> 00:03:21.300
We have access to and
allocation of memory,

00:03:21.350 --> 00:03:26.080
storage space,
mass storage in particular, power.

00:03:26.950 --> 00:03:31.240
And as I said, X is very free and giving
with those resources.

00:03:31.250 --> 00:03:36.200
And so you need to be aware of the
fact that when you're using a resource,

00:03:36.240 --> 00:03:37.830
there's a good chance that
you're actually taking it

00:03:37.840 --> 00:03:39.980
away from someone else.

00:03:40.530 --> 00:03:44.620
That actually brings up
something of a critical point,

00:03:44.620 --> 00:03:47.300
which is that hunger itself,
along with performance,

00:03:47.310 --> 00:03:48.500
is really a subjective thing.

00:03:48.500 --> 00:03:52.500
If there are abundant resources,
the fact that you're using a lot of them

00:03:52.500 --> 00:03:54.500
doesn't necessarily matter a great deal.

00:03:54.500 --> 00:03:56.970
But if you're competing either
with other applications,

00:03:56.970 --> 00:03:59.980
with the system, or with yourself,
despite the fact that your

00:04:00.030 --> 00:04:03.540
workload hasn't changed,
you are, in that context,

00:04:03.540 --> 00:04:05.450
a hungry application.

00:04:09.370 --> 00:04:13.940
It's fair to say that when
your application is doing work,

00:04:13.940 --> 00:04:15.700
everything takes time.

00:04:15.810 --> 00:04:19.430
Your code takes time to run,
your I/Os take time to perform,

00:04:19.480 --> 00:04:23.950
the system takes time to respond
to requests that you've made to it.

00:04:25.140 --> 00:04:28.860
And most of these resources,
these time-bounded resources,

00:04:28.860 --> 00:04:29.720
are single-use.

00:04:29.720 --> 00:04:32.260
If you're running code on a
particular core in the processor,

00:04:32.280 --> 00:04:34.800
no one else is running
it at the same time.

00:04:34.800 --> 00:04:37.320
If the disk subsystem is
doing work directly related

00:04:37.320 --> 00:04:40.280
to an I/O that you've issued,
no one else is getting that

00:04:40.300 --> 00:04:42.270
same work done at the same time.

00:04:44.060 --> 00:04:47.060
With regards to the actual
application of these resources,

00:04:47.060 --> 00:04:51.090
CPU scheduling is managed
in a fairly conventional

00:04:51.090 --> 00:04:53.840
priority-based round-robin scheme.

00:04:53.850 --> 00:04:56.540
We do have a number of tweaks
in the system that attempt

00:04:56.540 --> 00:04:58.040
to maintain responsiveness.

00:04:58.110 --> 00:05:00.670
From the application perspective,
you should mostly ignore those because

00:05:00.670 --> 00:05:02.260
we're trying to second-guess you.

00:05:02.300 --> 00:05:05.690
And if you second-guess us,
bad things happen.

00:05:05.830 --> 00:05:10.000
I/O scheduling varies
greatly with the device.

00:05:10.050 --> 00:05:12.440
If your application is
particularly sensitive to that,

00:05:12.450 --> 00:05:15.670
you need to actually spend some
time to understand how individual

00:05:15.670 --> 00:05:18.810
peripherals respond to multiple requests,
particularly in a

00:05:18.810 --> 00:05:20.520
heavily loaded situation.

00:05:20.570 --> 00:05:23.100
That's not really something I can
offer you any general guidance on here.

00:05:23.100 --> 00:05:26.080
It's very much situationally dependent.

00:05:30.260 --> 00:05:34.710
Memory is quite possibly the most
abused resource in our system.

00:05:34.750 --> 00:05:39.530
Virtual space,
most people think of as free,

00:05:39.530 --> 00:05:43.220
and to be sure, allocating virtual space
doesn't cost you a great deal.

00:05:43.220 --> 00:05:45.220
It's a relatively efficient operation.

00:05:45.220 --> 00:05:47.180
It does, however, have some overheads.

00:05:47.230 --> 00:05:50.980
Every allocation that you make
consumes wide kernel memory,

00:05:50.980 --> 00:05:56.100
and that wide kernel memory in turn
is a consumption of physical memory.

00:05:56.130 --> 00:06:00.810
It also consumes kernel virtual space.

00:06:01.290 --> 00:06:05.190
So whilst it's mostly free,
it's not entirely free.

00:06:05.210 --> 00:06:07.960
You find that you're competing
for virtual space against other

00:06:07.970 --> 00:06:10.880
portions of your own application,
as well as frameworks that

00:06:10.890 --> 00:06:14.190
you're linked against that
are doing work on your behalf.

00:06:14.940 --> 00:06:16.970
Of course, if you want to do anything
with our virtual space,

00:06:16.980 --> 00:06:18.500
you need physical memory to back it.

00:06:18.500 --> 00:06:23.050
And physical memory is
most certainly not free.

00:06:23.130 --> 00:06:25.990
Finding physical pages
requires substantial work,

00:06:26.150 --> 00:06:28.290
typically because we are trying
to satisfy the requests of

00:06:28.290 --> 00:06:31.900
everyone else in the system,
including the disk caching subsystem.

00:06:32.120 --> 00:06:34.900
Physical memory is
almost always spoken for.

00:06:34.900 --> 00:06:37.600
If you have customers who,
if you yourself have noticed

00:06:37.600 --> 00:06:41.990
that applications such as
Top or the Activity Monitor,

00:06:41.990 --> 00:06:44.340
once a system has been up for
any appreciable period of time,

00:06:44.340 --> 00:06:46.030
report no free memory.

00:06:46.030 --> 00:06:46.900
Well, that's good.

00:06:46.900 --> 00:06:48.390
We're using that memory for something.

00:06:48.520 --> 00:06:50.580
But it means that if your
application needs it,

00:06:50.590 --> 00:06:53.040
you're taking it away from
whoever currently has it.

00:06:54.410 --> 00:06:58.540
When thinking about the use of memory,
it's important to understand that

00:06:58.540 --> 00:07:01.000
memory is managed in page-sized quanta.

00:07:01.000 --> 00:07:04.140
On all of our currently shipping systems,
this is 4 kilobytes.

00:07:04.140 --> 00:07:09.480
It means that any time you touch
memory or allocate memory and use it,

00:07:09.610 --> 00:07:11.560
you're using a 4 kilobyte quantum.

00:07:11.580 --> 00:07:14.790
The fact that you've allocated
an 8-byte object notwithstanding,

00:07:14.820 --> 00:07:18.260
that's 4 kilobytes that you've
consumed that are currently

00:07:18.260 --> 00:07:23.160
being denied both the system and
potentially other applications.

00:07:23.850 --> 00:07:27.640
I mentioned that memory is taken away.

00:07:27.680 --> 00:07:31.600
So we use a modified LRU algorithm.

00:07:31.600 --> 00:07:34.920
Anyone who's taken a kindergarten-level
operating systems course will be familiar

00:07:34.920 --> 00:07:38.210
with the two-handed clock algorithm.

00:07:38.400 --> 00:07:41.820
We've stuck with it for a long time
for the simple reason that it works.

00:07:41.850 --> 00:07:46.760
We have a few modifications that help
us prioritize things such as disk

00:07:46.770 --> 00:07:52.900
cache pages are reclaimed slightly
more readily than other pages.

00:07:52.990 --> 00:07:55.150
But fundamentally,
we stick with this because it's simple,

00:07:55.160 --> 00:07:58.410
it's fast, and for the vast
majority of applications,

00:07:58.410 --> 00:08:00.460
it does a very good job.

00:08:00.850 --> 00:08:02.570
Unfortunately,
if you actually completed that

00:08:02.570 --> 00:08:05.160
kindergarten-level OS course,
you'll be well aware that in

00:08:05.550 --> 00:08:10.340
situations of heavy page use,
when the working set of the system

00:08:10.410 --> 00:08:14.110
actually outstrips the available
physical memory in the system,

00:08:14.110 --> 00:08:16.380
your algorithm breaks
down almost completely.

00:08:16.380 --> 00:08:19.650
And so there's a very sharp knee
in performance when you manage

00:08:19.650 --> 00:08:23.180
to push the working set beyond
the available physical memory.

00:08:23.180 --> 00:08:26.930
Virtually everything in the system
will come to an apparent standstill.

00:08:27.460 --> 00:08:30.750
We'll talk a little bit about degenerate
conditions later and why it's important

00:08:30.760 --> 00:08:35.390
for you to be able to recognize these
and perhaps moderate your behavior.

00:08:39.760 --> 00:08:44.190
Mass storage poses its own
set of interesting challenges

00:08:44.190 --> 00:08:45.600
when it comes to optimization.

00:08:45.600 --> 00:08:51.690
If you are using large
amounts of storage,

00:08:52.540 --> 00:08:55.060
You will have faced the
challenge of actually organizing

00:08:55.060 --> 00:08:56.600
the data in that storage.

00:08:56.600 --> 00:09:02.530
It's very popular to use the file
system namespace as a low-rent

00:09:02.530 --> 00:09:06.490
database to partition your data into
a collection of files and directories.

00:09:06.500 --> 00:09:09.590
Now, this can be fairly efficient,
particularly if you don't feel like

00:09:09.590 --> 00:09:11.800
writing a database or if you want
to take advantage of the fact that

00:09:11.800 --> 00:09:16.380
the file system will cache both the
directory metadata and the file data

00:09:16.520 --> 00:09:18.500
in order to speed up your application.

00:09:18.500 --> 00:09:22.500
But you need to understand that there
are costs associated with all of this.

00:09:22.500 --> 00:09:24.840
Caching that file system
metadata consumes,

00:09:24.840 --> 00:09:27.500
again, kernel, virtual,
and physical space.

00:09:27.500 --> 00:09:31.490
It also imposes overheads in
actually accessing these files.

00:09:31.500 --> 00:09:35.100
Each directory lookup may
potentially involve a disk access.

00:09:35.100 --> 00:09:39.090
It certainly involves several different
page references inside the kernel.

00:09:40.800 --> 00:09:42.850
As far as allocation
strategies are concerned,

00:09:42.850 --> 00:09:44.700
disk space is shared.

00:09:44.700 --> 00:09:48.210
You can certainly impose quotas,
but they're typically used

00:09:48.210 --> 00:09:50.970
in an administrative fashion
on a per-user basis rather

00:09:50.990 --> 00:09:52.700
than a per-application basis.

00:09:52.700 --> 00:09:55.600
So between applications,
it's fair to say that disk

00:09:55.600 --> 00:09:58.700
space is shared on a first-come,
first-served basis.

00:09:58.700 --> 00:10:02.050
It is possible for an application
to make disk space reservations

00:10:02.050 --> 00:10:05.700
if it's particularly critical that
disk space actually be available.

00:10:05.700 --> 00:10:07.680
There's a feature that's only
available on some file systems,

00:10:07.700 --> 00:10:11.690
but the vast majority of our
systems installed using HFS+

00:10:11.710 --> 00:10:13.700
as the local root file system.

00:10:13.700 --> 00:10:17.190
So you can be fairly certain that
if you need to make a reservation,

00:10:17.190 --> 00:10:18.970
you can do that on the root volume.

00:10:23.180 --> 00:10:27.590
As I pointed out to begin with,
performance tuning is considered

00:10:27.600 --> 00:10:30.280
something of a black art,
which is really kind of unfair

00:10:30.290 --> 00:10:32.120
because it's not super difficult.

00:10:32.500 --> 00:10:36.690
But to get folks started
and on the right foot,

00:10:36.700 --> 00:10:40.600
I'm proposing a five-step
program for better performance.

00:10:40.600 --> 00:10:43.520
You can come up with seven more
steps if you feel that's important,

00:10:43.540 --> 00:10:45.490
but I'm going to stick to five.

00:10:45.560 --> 00:10:48.220
The first is to avoid unnecessary work.

00:10:48.220 --> 00:10:50.220
I'm going to expand on these more,
so I'm not going to talk too

00:10:50.230 --> 00:10:51.530
much about them right now.

00:10:51.590 --> 00:10:53.800
The second,
which seems like it completely

00:10:53.800 --> 00:10:55.790
contradicts the first,
is to keep busy.

00:10:56.250 --> 00:10:57.860
Third is to be a good citizen.

00:10:57.860 --> 00:11:00.470
As I've already alluded,
your actions affect the

00:11:00.470 --> 00:11:03.960
behavior of the system,
and likewise, the actions of other

00:11:03.960 --> 00:11:06.020
applications will affect you.

00:11:06.040 --> 00:11:08.100
It's important to get along.

00:11:08.140 --> 00:11:11.980
When it comes to actually attempting
to assess your performance,

00:11:11.990 --> 00:11:12.450
don't guess.

00:11:12.510 --> 00:11:14.260
Don't say, "Hey, that looks snappy.

00:11:14.260 --> 00:11:18.540
That was a bit quicker than last time,
I'm sure." Measure it.

00:11:18.560 --> 00:11:21.300
And finally,
when you're actually looking to

00:11:21.380 --> 00:11:25.380
find things that you can improve,
take advantage of other people's work.

00:11:25.390 --> 00:11:28.140
Apple in particular provides
you with an enormous collection

00:11:28.140 --> 00:11:31.820
of ready-optimized algorithms,
code, and services.

00:11:31.820 --> 00:11:35.010
I'll talk very briefly about
those a little bit later on.

00:11:49.860 --> 00:11:52.540
too many steps involved.

00:11:52.560 --> 00:11:57.620
Modern software architecture
emphasizes modularity and code reuse.

00:11:57.640 --> 00:12:02.400
There's a great pressure on time
to market and time to feature.

00:12:04.200 --> 00:12:07.390
These are great techniques
for achieving those goals,

00:12:07.390 --> 00:12:11.390
but they introduce some
unfortunate overheads.

00:12:11.780 --> 00:12:15.260
and quite often,
it can lead to significant

00:12:15.270 --> 00:12:16.700
performance problems.

00:12:16.700 --> 00:12:20.400
As I mentioned before,
pretty much everything

00:12:20.400 --> 00:12:22.510
that you do takes time.

00:12:23.830 --> 00:12:27.920
The more time that you
spend doing something,

00:12:27.950 --> 00:12:30.700
the longer that in the end
ultimately it's going to take,

00:12:30.710 --> 00:12:37.390
and thus the less performance
is actually perceived.

00:12:39.050 --> 00:12:42.500
But this actually brings up
an interesting side point that

00:12:42.500 --> 00:12:45.330
I'm going to get back to a
couple times again later on,

00:12:45.330 --> 00:12:48.590
which is the difference between
performance and efficiency.

00:12:49.040 --> 00:12:52.070
Performance is typically measured
in the time it takes to perform some

00:12:52.070 --> 00:12:56.240
task or the number of tasks that
you can perform in some unit time.

00:12:56.260 --> 00:12:58.080
Efficiency you can measure
slightly differently,

00:12:58.080 --> 00:13:00.700
which is the amount of total
actual work done by the system

00:13:00.700 --> 00:13:03.780
in order to perform this task.

00:13:04.100 --> 00:13:06.730
In the simplest situation,
you have a chain of

00:13:06.760 --> 00:13:10.460
directly dependent steps,
and so performance and efficiency

00:13:10.460 --> 00:13:11.690
are ultimately the same thing.

00:13:11.700 --> 00:13:14.930
But with multi-core,
multi-threaded systems

00:13:14.930 --> 00:13:18.660
with asynchronous work,
with peripherals doing work for you,

00:13:18.660 --> 00:13:22.410
the dependency chain between
individual work units is not

00:13:22.470 --> 00:13:24.980
necessarily quite so straightforward.

00:13:24.980 --> 00:13:27.770
And so you can complete a
task in the same elapsed time,

00:13:27.770 --> 00:13:29.620
but with very different efficiency.

00:13:30.260 --> 00:13:33.580
Efficiency is particularly
critical when we talk about power,

00:13:33.580 --> 00:13:36.100
which I'll get to a little
bit later on as well.

00:13:38.450 --> 00:13:44.780
So doing less work seems obvious,
but after a few years actually analyzing

00:13:44.780 --> 00:13:48.200
the performance of various applications,
you realize that maybe this

00:13:48.200 --> 00:13:50.390
isn't necessarily quite the case.

00:13:51.500 --> 00:13:55.490
How much work an application does,
the amount of work that it ends up doing

00:13:55.490 --> 00:13:59.850
in order to perform some sort of task,
depends on a whole variety of things.

00:13:59.870 --> 00:14:02.380
The architecture chosen
for the application,

00:14:02.380 --> 00:14:05.820
the specific algorithms
used in its implementation,

00:14:05.860 --> 00:14:07.860
the tools,
the language in which it was written,

00:14:07.860 --> 00:14:11.380
the libraries,
all that sort of stuff plays into the

00:14:11.380 --> 00:14:15.920
overall performance of an application,
before we start talking about bugs,

00:14:15.940 --> 00:14:19.650
misunderstandings of components,
and so on and so forth.

00:14:20.240 --> 00:14:23.670
Talking about implementation decisions,
and as I've mentioned

00:14:23.670 --> 00:14:27.140
on the previous slide,
there's a great trend

00:14:27.140 --> 00:14:29.440
towards component reuse.

00:14:29.450 --> 00:14:33.170
And when you combine that with
a great deal of pressure on many

00:14:33.170 --> 00:14:35.800
of you to get things out the
door as quickly as possible,

00:14:35.810 --> 00:14:38.750
you often find components that
aren't really well-suited to the

00:14:38.750 --> 00:14:43.300
task being pressed in because they
get you past the feature point.

00:14:43.790 --> 00:14:47.180
There's also the fact that many
of these components are opaque.

00:14:47.200 --> 00:14:49.450
You're encouraged to
consider them as black boxes,

00:14:49.460 --> 00:14:52.700
and so you don't have a great deal of
visibility into their overall behavior.

00:14:52.700 --> 00:14:56.690
And sometimes the behavior of those
blocks is actually misrepresented,

00:14:56.700 --> 00:14:58.820
perhaps because someone
wants you to pay for them,

00:14:58.820 --> 00:15:02.890
perhaps because you simply misunderstood
the way that it's being described.

00:15:06.000 --> 00:15:09.880
So when you're putting together
a piece of software and you are

00:15:09.880 --> 00:15:13.900
using other people's components,
you're using system-provided services,

00:15:13.990 --> 00:15:15.540
maybe you're just doing
some stuff yourself,

00:15:15.540 --> 00:15:19.090
it's important to understand how much it
actually costs you to do these things.

00:15:19.100 --> 00:15:23.090
There are some
particularly relevant ones,

00:15:23.090 --> 00:15:25.820
given that I'm beating up on
modular software design right now.

00:15:25.850 --> 00:15:28.180
Cross-module costs.

00:15:29.380 --> 00:15:31.450
There's the obvious cost of
calling another function,

00:15:31.460 --> 00:15:34.070
the fact that it potentially
perturbs the code flow,

00:15:34.100 --> 00:15:35.740
it has impacts on the cache.

00:15:35.740 --> 00:15:37.900
The calling convention,
the setting up of arguments

00:15:37.900 --> 00:15:38.980
and tearing them down.

00:15:38.980 --> 00:15:40.860
If this is a function that
you call very frequently,

00:15:40.860 --> 00:15:43.980
you may spend a lot of meta-work
simply calling it in the first place.

00:15:43.980 --> 00:15:46.460
On the larger scale,
you often find yourself

00:15:46.610 --> 00:15:49.740
performing format conversions,
packing structures,

00:15:49.740 --> 00:15:52.690
building things up into XML plists
just to tear them down again on the

00:15:52.690 --> 00:15:54.340
other side of the call interface.

00:15:54.340 --> 00:15:57.790
All of these things are costs that
are directly associated with the

00:15:58.070 --> 00:16:01.260
choice of modular architecture,
and these things need to be

00:16:01.260 --> 00:16:03.380
considered when you're actually
designing your application.

00:16:03.380 --> 00:16:05.300
Or alternatively,
when you're reconsidering the

00:16:05.320 --> 00:16:07.860
design of your application in
order to meet a performance target.

00:16:09.810 --> 00:16:12.640
There are a couple of special
costs that are associated

00:16:12.640 --> 00:16:15.290
with calling system services.

00:16:15.560 --> 00:16:19.720
A lot of services on Mac OS X are
implemented as separate processes,

00:16:19.740 --> 00:16:24.060
and so you'll find that there is overhead
involved in inter-process communication.

00:16:24.110 --> 00:16:28.980
If you have the luxury of specifying
or deciding on an interface,

00:16:29.040 --> 00:16:34.180
you may find yourself making trade-offs
between convenience and performance.

00:16:35.120 --> 00:16:37.700
Calling the kernel directly
also imposes some overhead.

00:16:37.700 --> 00:16:40.400
There's a protection boundary crossing,
obviously.

00:16:40.400 --> 00:16:41.990
There's argument marshalling.

00:16:42.020 --> 00:16:44.310
There are potentially credential
operations because you may be

00:16:44.370 --> 00:16:47.730
performing something that's
actually an authorized operation.

00:16:48.100 --> 00:16:50.000
As a general rule,
it's good to assume that

00:16:50.000 --> 00:16:53.000
any time you invoke anything
else anywhere that it costs,

00:16:53.000 --> 00:16:57.250
factor that in over and above whatever
it may cost that other module to actually

00:16:57.250 --> 00:16:59.830
do the work for you in the first place.

00:17:01.200 --> 00:17:03.480
Some specific examples of
things that are good to avoid,

00:17:03.480 --> 00:17:05.510
as I mentioned, system calls.

00:17:05.530 --> 00:17:07.950
But particularly when
you're making system calls,

00:17:07.960 --> 00:17:10.100
often you're talking
about memory allocation,

00:17:10.100 --> 00:17:12.100
file system I/O.

00:17:12.100 --> 00:17:14.100
Frequent memory allocations.

00:17:14.100 --> 00:17:18.920
Some of you would certainly have
seen a number of articles in the

00:17:18.920 --> 00:17:23.370
popular hacker press about terrible
performance that Mac OS X has,

00:17:23.390 --> 00:17:25.100
which usually devolves into,
"My goodness,

00:17:25.100 --> 00:17:28.840
I'm calling Malik far too often."
And typically what that devolves

00:17:28.930 --> 00:17:32.090
into subsequently is that Malik
is making memory allocations

00:17:32.100 --> 00:17:36.680
in your process address space,
which involves calling into the kernel.

00:17:37.310 --> 00:17:39.150
That's something that
is very good to avoid.

00:17:39.160 --> 00:17:42.560
There's a degree of locking and
overhead involved in setting up and

00:17:42.560 --> 00:17:44.960
tearing down address space mappings.

00:17:44.960 --> 00:17:47.390
And if you're not doing very
much work with the virtual

00:17:47.390 --> 00:17:51.580
space that you've set up,
you're basically wasting your time.

00:17:51.730 --> 00:17:55.260
Likewise, small I/Os,
particularly if these I/Os are actually

00:17:55.260 --> 00:17:57.860
going to hit a physical device,
there's a good deal of set-up

00:17:57.920 --> 00:18:00.310
and tear-down involved in
almost any I/O operation.

00:18:00.380 --> 00:18:04.530
So a small I/O operation's overall
elapsed runtime is dominated

00:18:04.530 --> 00:18:06.150
by set-up and tear-down costs.

00:18:06.240 --> 00:18:11.090
You want to amortize those much as
you amortize memory allocation costs.

00:18:12.340 --> 00:18:15.990
In a process communication, once again,
we're talking about system

00:18:16.000 --> 00:18:17.020
call overhead costs here.

00:18:17.020 --> 00:18:20.530
If you're not communicating very much
information between the two processes,

00:18:20.580 --> 00:18:25.480
you're paying a lot of overhead in order
to get that information moved around.

00:18:25.900 --> 00:18:29.040
Shared memory operations,
particularly if you're working on

00:18:29.050 --> 00:18:32.790
a segment that you have previously
shared with another process,

00:18:32.790 --> 00:18:37.370
and you have lock-based or lock-free
algorithms for moving that data around,

00:18:37.400 --> 00:18:39.210
can be much more efficient.

00:18:39.960 --> 00:18:42.830
And obviously,
try to only ever do anything once.

00:18:42.860 --> 00:18:44.190
Sometimes this isn't possible.

00:18:44.200 --> 00:18:47.390
Caching the results of an
operation is just too expensive,

00:18:47.400 --> 00:18:50.220
consumes too much space,
so you may have to trade off

00:18:50.220 --> 00:18:53.330
the cost of caching your results
against simply re-performing

00:18:53.340 --> 00:18:54.920
the operation at a later time.

00:18:55.020 --> 00:18:57.590
But it's something to consider.

00:18:58.760 --> 00:19:02.300
This one gets a slider by itself because
this really sort of reaches into the

00:19:02.300 --> 00:19:05.680
practical implementation of the machine
itself and how your code runs on it.

00:19:05.700 --> 00:19:07.980
Touching memory.

00:19:08.410 --> 00:19:11.690
Anything that you do that
touches memory costs.

00:19:11.880 --> 00:19:16.070
If you're touching stuff in the L1 cache,
that's cheaper than touching

00:19:16.210 --> 00:19:17.970
data that's in the L2 cache.

00:19:17.970 --> 00:19:21.620
And if it's out in main memory and you
have to take a full-on cache hit for it,

00:19:21.620 --> 00:19:23.800
that's going to cost
you a great deal more.

00:19:23.800 --> 00:19:26.920
So anything that you can do
to avoid touching memory,

00:19:27.120 --> 00:19:30.760
and this goes all the way
from algorithm design,

00:19:30.830 --> 00:19:33.720
through data structure design,
through the way that you actually lay out

00:19:33.740 --> 00:19:35.800
the tasks that your application performs.

00:19:35.800 --> 00:19:37.790
Avoid copying things.

00:19:37.820 --> 00:19:40.470
I mean,
up aside from the fact that a straight-up

00:19:40.470 --> 00:19:44.260
copy wastes virtual and physical memory,
the act of actually reading and

00:19:44.260 --> 00:19:45.800
writing that data costs time.

00:19:45.800 --> 00:19:48.720
It also costs power.

00:19:48.960 --> 00:19:53.730
So think about allocating memory
for your data structures with the

00:19:53.730 --> 00:19:57.040
patterns that you actually access
those data structures in mind.

00:19:57.040 --> 00:19:59.580
Allocate memory,
put things in that memory that

00:19:59.580 --> 00:20:02.040
you're going to hit multiple
times with relatively good

00:20:02.040 --> 00:20:05.040
temporal locality of reference.

00:20:05.090 --> 00:20:08.440
Also consider physical
locality of reference.

00:20:10.150 --> 00:20:14.060
If you do that,
if you cluster things appropriately,

00:20:14.060 --> 00:20:18.380
you can reduce the number of cache
lines that your algorithms use.

00:20:18.380 --> 00:20:20.860
You can also reduce the number
of pages that you touch,

00:20:20.860 --> 00:20:23.850
and thus the size of your working set,
which incidentally also reduces the

00:20:23.850 --> 00:20:27.620
number of TLB entries that you'll
actually require to get the work done.

00:20:27.620 --> 00:20:29.750
TLB entries themselves tend
to end up in the cache,

00:20:29.810 --> 00:20:32.860
and so they compete with
your data for cache space.

00:20:32.860 --> 00:20:36.330
So once again,
using fewer pages saves you cache lines.

00:20:39.200 --> 00:20:42.440
If you're trying to keep the
number of pages that you use down,

00:20:42.440 --> 00:20:46.250
as I'm encouraging you to do,
think about the way that you

00:20:46.260 --> 00:20:47.960
actually access your data.

00:20:47.970 --> 00:20:49.540
Linked lists are super convenient.

00:20:49.540 --> 00:20:50.880
There are great macros for them.

00:20:50.880 --> 00:20:52.500
There are marvelous classes.

00:20:52.500 --> 00:20:54.780
They're nice and easy
to understand and debug.

00:20:54.790 --> 00:20:56.770
Unfortunately,
once you put a few things in them,

00:20:56.880 --> 00:20:59.160
they perform really badly.

00:20:59.560 --> 00:21:01.940
Typically, if you're looking for
something in a linked list,

00:21:01.950 --> 00:21:05.610
and you have no other indexing for it,
you're going to touch maybe half,

00:21:05.640 --> 00:21:07.640
depending on what data
you're actually looking at,

00:21:07.640 --> 00:21:09.810
maybe half of the entries in the list.

00:21:09.820 --> 00:21:12.340
If the list has been built dynamically,
they're probably scattered

00:21:12.340 --> 00:21:15.340
all over the place,
and so you're going to be touching

00:21:15.340 --> 00:21:17.190
an enormous number of pages.

00:21:17.240 --> 00:21:19.050
Now, it's fair to say that you've
allocated those pages,

00:21:19.060 --> 00:21:21.260
and so you're not paying the
cost for allocating them,

00:21:21.310 --> 00:21:22.950
but you are using those pages.

00:21:23.000 --> 00:21:25.580
And that means that if those pages
have been reclaimed and they're

00:21:25.580 --> 00:21:28.490
currently being used by someone else,
they have to be brought back.

00:21:29.080 --> 00:21:31.980
Any time that you touch a page
that isn't currently resident,

00:21:31.990 --> 00:21:35.710
the thread that is touching that
page will block synchronously

00:21:35.830 --> 00:21:38.000
until the page can be brought back.

00:21:38.030 --> 00:21:41.220
It's terrible for performance.

00:21:41.940 --> 00:21:45.230
If you have the luxury of allocating
your data structures up front,

00:21:45.380 --> 00:21:49.060
think about the patterns in which they're
accessed and try and group members,

00:21:49.070 --> 00:21:51.880
assuming your data structure
members are smaller than a page,

00:21:52.030 --> 00:21:55.800
within pages so that you have good
physical locality of reference.

00:21:55.820 --> 00:21:58.520
You can also consider breaking the
data structures down so that the

00:21:58.600 --> 00:22:02.330
portions of the data structure that you
actually access are kept separately,

00:22:02.330 --> 00:22:04.750
and once again,
cluster those within pages with

00:22:04.750 --> 00:22:08.700
good physical locality of reference.

00:22:08.710 --> 00:22:11.250
Most of you would have been exposed
to the concept of indexing your data.

00:22:11.260 --> 00:22:13.250
It's a great idea.

00:22:16.180 --> 00:22:20.090
Here's a really simple example of how
to reduce the cache line utilization

00:22:20.090 --> 00:22:26.020
if you are stuck with or if you're
considering a linked list implementation.

00:22:26.040 --> 00:22:28.880
In the first example,
traversing the list,

00:22:28.880 --> 00:22:32.240
where you're comparing the tag against
some key that you're searching for,

00:22:32.320 --> 00:22:35.770
you're going to hit two cache
lines for every entry in the list.

00:22:35.840 --> 00:22:39.010
One for the tag,
another one for the list entry.

00:22:39.190 --> 00:22:43.320
In the second example,
provided that the structure is

00:22:43.320 --> 00:22:46.170
allocated such that the first two
members don't cross a cache line,

00:22:46.180 --> 00:22:49.800
and this is typically the case,
structure this size is likely to be

00:22:49.800 --> 00:22:53.040
allocated on a size-aligned boundary
or something fairly close to it.

00:22:53.080 --> 00:22:56.120
And if not,
you can ensure that yourselves.

00:22:56.610 --> 00:22:58.880
Checking the tag will have
brought the pointer to the

00:22:58.880 --> 00:23:01.490
next list entry into the cache,
and so when you go looking

00:23:01.490 --> 00:23:03.850
for the next entry,
you're not going to stall

00:23:03.850 --> 00:23:05.390
waiting for a cache fill.

00:23:17.950 --> 00:23:22.230
So the second step that I brought up,
which does indeed sound like it's

00:23:22.230 --> 00:23:26.060
somewhat contrary to the first,
is to keep busy.

00:23:26.060 --> 00:23:27.190
I should perhaps qualify that.

00:23:27.260 --> 00:23:29.730
When you have something to do,
you should always be

00:23:29.730 --> 00:23:31.850
working on getting it done.

00:23:37.200 --> 00:23:41.500
All the modern Macintosh systems
you can buy are at least dual-core.

00:23:41.500 --> 00:23:43.100
Some of them have more.

00:23:43.100 --> 00:23:47.400
It's an industry trend that
I think we can see continuing.

00:23:48.440 --> 00:23:51.030
And so this means that if you
actually want to maximize the

00:23:51.030 --> 00:23:53.690
performance of your application,
if you want to utilize all

00:23:53.690 --> 00:23:56.370
of the performance that's
available to you in the system,

00:23:56.400 --> 00:24:00.290
you need to start thinking about
how to get concurrent work performed

00:24:00.300 --> 00:24:04.060
on your application's behalf,
whether your application itself does it,

00:24:04.060 --> 00:24:08.800
whether you are finding enough work
for other services in the system to do,

00:24:08.810 --> 00:24:12.030
that you can keep all of
those cores busy all the time.

00:24:12.040 --> 00:24:15.570
If you are working on a
single-threaded application,

00:24:15.590 --> 00:24:18.700
you're going to find that the overall
throughput of your application,

00:24:18.700 --> 00:24:23.530
and thus its potential performance,
is nowhere near up to the maximum

00:24:23.540 --> 00:24:26.030
potential of the system that you're on.

00:24:27.010 --> 00:24:29.460
There are a number of system services.

00:24:29.460 --> 00:24:34.190
In fact, the vast majority of them can be
used in asynchronous fashions.

00:24:34.300 --> 00:24:35.960
The POSIX asynchronous I/O.

00:24:35.960 --> 00:24:39.720
There are an enormous number of
callback-oriented operations in the

00:24:39.720 --> 00:24:42.300
Carbon APIs and so on and so forth.

00:24:42.320 --> 00:24:46.650
These are all great ways to
have things done on your behalf.

00:24:48.960 --> 00:24:54.430
If you are likely to actually block,
then you need to arrange one

00:24:54.430 --> 00:24:57.400
way or another for there to be
someone else in your application,

00:24:57.400 --> 00:25:00.190
another thread,
another process somewhere else,

00:25:00.190 --> 00:25:02.870
that's willing to pick up the slack,
that is likely to be ready to be

00:25:02.870 --> 00:25:06.170
able to run to get something done
so that your task is completed.

00:25:08.020 --> 00:25:10.330
There are a couple of
overheads associated with this.

00:25:10.360 --> 00:25:13.450
We'll talk about a few
of them in a little bit.

00:25:13.480 --> 00:25:15.630
These are really things that you
need to consider in the design

00:25:15.640 --> 00:25:18.570
process as you partition your work.

00:25:18.600 --> 00:25:22.740
A lot of applications-- this is
particularly the case with games--

00:25:22.870 --> 00:25:25.080
tend to have been implemented in a
single threaded fashion because that's

00:25:25.150 --> 00:25:26.790
the easy way to think about things.

00:25:26.790 --> 00:25:28.520
It's the way that we tend to do things.

00:25:28.590 --> 00:25:31.880
We'll do one task, and then another,
and then another.

00:25:32.360 --> 00:25:37.360
So there's a real body of design
work that needs to be done at the

00:25:37.370 --> 00:25:42.270
architectural level for most-- that's
probably unfair-- many applications.

00:25:42.510 --> 00:25:47.640
And so, thinking about these pitfalls
is fairly germane at this point.

00:25:51.300 --> 00:25:55.300
There's a lot of folklore out
there about threading and locking.

00:25:55.300 --> 00:25:58.480
Some of it is hard won
from long experience.

00:25:58.480 --> 00:26:01.580
Some of it is perhaps not so valuable.

00:26:01.690 --> 00:26:06.080
I'm going to bring up a couple of
quick starter points to think about.

00:26:06.500 --> 00:26:10.400
that combat a couple of
these common misconceptions.

00:26:10.440 --> 00:26:12.300
But overall,
the point that I raised earlier

00:26:12.300 --> 00:26:15.360
about how important it is to measure
and understand the behavior of

00:26:15.420 --> 00:26:19.000
the things that you're doing in
your application is paramount here.

00:26:21.360 --> 00:26:26.740
There's a very common conception
that locks are basically free

00:26:26.740 --> 00:26:30.380
unless they're contested,
and unfortunately that's just not true.

00:26:30.560 --> 00:26:33.940
Locking and unlocking imposes a fairly
significant overhead on the system.

00:26:33.940 --> 00:26:37.460
And that's exacerbated
even more by the fact that,

00:26:37.480 --> 00:26:41.180
particularly if you're pursuing a
very aggressive lock/unlock strategy,

00:26:41.180 --> 00:26:45.140
the party most likely to take a lock
that you're just about to release or

00:26:45.140 --> 00:26:47.930
you're considering holding for some
time is likely to be yourself again.

00:26:47.930 --> 00:26:52.440
So there is considerable value
in considering exactly how

00:26:52.440 --> 00:26:54.220
likely contention is to be.

00:26:54.220 --> 00:26:57.980
And then once you've actually implemented
measuring to ensure that your assumptions

00:26:57.980 --> 00:26:59.620
about contention are actually correct,

00:27:02.210 --> 00:27:06.360
If you're not contending on a lock,
but you are taking and

00:27:06.360 --> 00:27:09.820
releasing that lock a lot,
it suggests that maybe you could

00:27:09.820 --> 00:27:15.850
afford to hold it for longer and spend
less time taking it and releasing it.

00:27:19.400 --> 00:27:22.960
It's particularly important to bear
in mind that locks are serializing

00:27:22.960 --> 00:27:29.310
operations in order to actually provide
the guarantees that they do about access

00:27:29.310 --> 00:27:32.080
to data under and not under the lock.

00:27:32.460 --> 00:27:35.540
They take one of the major
advantages of modern microprocessors,

00:27:35.590 --> 00:27:38.450
that is the ability to perform
operations out of order,

00:27:38.510 --> 00:27:40.710
and they toss it all out the window.

00:27:42.210 --> 00:27:45.320
It's actually worse than that
because in order to maintain these

00:27:45.320 --> 00:27:49.510
guarantees in multiprocessor systems,
every other processor in the system has

00:27:49.510 --> 00:27:52.100
to be aware of the lock in some fashion.

00:27:52.150 --> 00:27:54.340
And so there are system-wide overheads.

00:27:54.340 --> 00:27:56.410
Anytime you take a lock,
you're affecting every other

00:27:56.420 --> 00:27:58.480
processor in the system.

00:28:02.510 --> 00:28:05.830
And so again,
there is considerable value to be had

00:28:05.940 --> 00:28:08.890
in amortizing your use of those logs.

00:28:10.460 --> 00:28:15.900
The cache is another resource that's
very heavily impacted by concurrency.

00:28:16.030 --> 00:28:20.000
There are many algorithms out there
that either attempt to automatically

00:28:20.000 --> 00:28:24.080
determine the size of the cache available
to you or query the system for that size,

00:28:24.080 --> 00:28:28.020
and then make assumptions about
their likely performance based

00:28:28.020 --> 00:28:30.010
on their usage of that cache.

00:28:31.280 --> 00:28:35.360
If you're running on one of our
current Intel-based systems,

00:28:35.360 --> 00:28:38.480
and if you've actually looked at
the architecture of these systems,

00:28:38.480 --> 00:28:42.840
you will have noticed
that cores share cache.

00:28:42.840 --> 00:28:46.430
This means that not only code that
you're actually aware of running,

00:28:46.430 --> 00:28:50.910
but code that you are completely unaware
of is competing with you for cache space.

00:28:51.910 --> 00:28:54.370
Given that you have no idea
what this code is doing,

00:28:54.370 --> 00:28:56.800
it may be making assumptions
about cache usage as well.

00:28:56.800 --> 00:29:01.170
You have to be much more careful
in algorithmic assumptions that you

00:29:01.170 --> 00:29:04.470
make about the usage of the cache.

00:29:05.560 --> 00:29:09.590
And you need to be aware of what's going
on while your algorithm is running,

00:29:09.630 --> 00:29:13.500
because you may find yourself needing
to adapt your behavior based on what you

00:29:13.520 --> 00:29:16.460
discern about other usage of the cache.

00:29:16.500 --> 00:29:19.130
It's also worth bearing in
mind that if you call out,

00:29:19.130 --> 00:29:21.100
going back to the modular
software issue again,

00:29:21.160 --> 00:29:25.680
if you call out from your algorithm
to some other section of code,

00:29:25.760 --> 00:29:29.690
that call and what that code does
will also affect your cache usage.

00:29:44.410 --> 00:29:49.480
Mac OS X is a multi-user,
multi-application, multi-processor,

00:29:49.480 --> 00:29:52.970
multi-just-about-everything
operating system.

00:29:53.680 --> 00:29:58.970
And this is particularly relevant
to your applications because your

00:29:58.970 --> 00:30:01.600
application will never own the system.

00:30:01.630 --> 00:30:04.710
Even if you're the only app running,
well, even if you think you're

00:30:04.720 --> 00:30:05.800
the only app running,
you're not.

00:30:05.800 --> 00:30:06.780
There's a whole bunch of other ones.

00:30:06.780 --> 00:30:08.780
They're all doing stuff.

00:30:08.780 --> 00:30:13.750
They all have expectations
about what the system may or may

00:30:13.750 --> 00:30:15.180
not do at at least some level.

00:30:15.420 --> 00:30:19.200
Anything that you do on the system,
regardless of what it is,

00:30:19.200 --> 00:30:22.660
has some impact on other
applications in the system.

00:30:22.660 --> 00:30:25.360
If you're consuming CPU cycles,
they're CPU cycles they

00:30:25.360 --> 00:30:26.420
might not have had.

00:30:26.420 --> 00:30:29.210
If you're doing disk I/O,
that's disk space,

00:30:29.210 --> 00:30:31.550
disk bandwidth that they own and have.

00:30:33.790 --> 00:30:36.640
And in fact,
what you do affects you as well.

00:30:36.740 --> 00:30:38.620
If your working set
is sufficiently large,

00:30:38.620 --> 00:30:40.700
you may find yourself
cannibalizing yourself.

00:30:40.700 --> 00:30:42.690
If you have a busy thread,
you may be taking processor

00:30:42.700 --> 00:30:44.040
time away from other threads.

00:30:44.040 --> 00:30:48.430
And of course,
what other applications do is

00:30:48.430 --> 00:30:50.400
going to affect you as well.

00:30:56.770 --> 00:31:02.580
There are a few applications that have
very consistent runtime environments.

00:31:02.580 --> 00:31:04.940
If you're building a large
scientific application,

00:31:04.940 --> 00:31:08.980
you have 500 machines,
a nice air-conditioned room,

00:31:08.980 --> 00:31:12.540
you can make some fairly steady-state
assumptions about what the system

00:31:12.540 --> 00:31:13.840
environment is going to look like.

00:31:13.840 --> 00:31:15.950
You've got a pretty good idea of
how much physical memory you've got.

00:31:16.010 --> 00:31:18.710
You've got a pretty good idea about
how likely you are to be competing

00:31:18.780 --> 00:31:21.330
with someone else for processor time,
so on and so forth.

00:31:22.550 --> 00:31:24.330
But this really isn't the general case.

00:31:24.430 --> 00:31:28.160
Most of your applications are going to
be run in a fairly uncertain environment.

00:31:28.270 --> 00:31:32.450
Even if your code starts out in a
controlled research environment,

00:31:32.490 --> 00:31:34.940
as we've seen over the years,
that code tends to migrate

00:31:34.940 --> 00:31:36.220
into general usage.

00:31:36.230 --> 00:31:38.220
What you're doing is
actually even vaguely useful.

00:31:38.320 --> 00:31:40.770
Someone is going to say,
"I can build a great app

00:31:40.770 --> 00:31:44.050
around that," and they're
either going to buy your code,

00:31:44.110 --> 00:31:47.170
use it if it's open source,
or they're going to reimplement something

00:31:47.170 --> 00:31:48.920
based on whatever you've published.

00:31:49.130 --> 00:31:52.520
So it's very important for
you to understand the runtime

00:31:52.520 --> 00:31:55.430
environment for your applications
and what your expectations about

00:31:55.430 --> 00:31:57.610
that runtime environment are.

00:32:02.680 --> 00:32:07.740
It's also really important for you
to make good use of your resources.

00:32:07.880 --> 00:32:11.640
Any use of a resource implies that
you're doing some sort of work.

00:32:11.640 --> 00:32:15.120
As we've already considered,
doing work is something to

00:32:15.120 --> 00:32:16.880
be avoided at all costs.

00:32:16.880 --> 00:32:18.980
So make sure that when you're
actually using a resource,

00:32:18.980 --> 00:32:20.900
you have a need for it.

00:32:22.000 --> 00:32:25.110
You're cannibalizing those
resources from someone else,

00:32:25.110 --> 00:32:26.400
in all likelihood.

00:32:26.410 --> 00:32:29.340
And so you want to consider the
impact that that has on them,

00:32:29.350 --> 00:32:32.940
and what impact may actually
come back from your doing that.

00:32:34.050 --> 00:32:37.840
And again, the memory point,
and a term that you may hear

00:32:37.840 --> 00:32:41.000
us bandy around a little bit,
memory pressure.

00:32:41.280 --> 00:32:44.310
Because pages are reclaimed
on an as-needed basis,

00:32:44.380 --> 00:32:47.360
and that need is driven by the
size of the working sets of the

00:32:47.360 --> 00:32:51.470
applications that are currently active,
we describe the desire for

00:32:51.640 --> 00:32:53.760
physical memory as pressure.

00:32:53.760 --> 00:32:59.280
It tends to affect the rate at which
pages flow through the LIU cache,

00:32:59.280 --> 00:33:01.120
which is where the
pressure term comes from.

00:33:01.120 --> 00:33:04.400
The greater the pressure,
the faster the recycle rate for pages,

00:33:04.400 --> 00:33:06.800
the greater the chance that
your application is going to

00:33:06.850 --> 00:33:09.200
hit a page that it needs that,
due to this pressure,

00:33:09.200 --> 00:33:10.880
has been reclaimed for use elsewhere.

00:33:11.200 --> 00:33:14.400
If you consume fewer pages,
you reduce that pressure.

00:33:14.400 --> 00:33:17.050
We've already talked about
some of the ways that you

00:33:17.050 --> 00:33:18.880
can reduce page consumption.

00:33:18.880 --> 00:33:22.500
Those things have a direct flow on to the
overall performance of your application

00:33:22.560 --> 00:33:24.400
and every application in the system.

00:33:25.290 --> 00:33:29.120
It's also good to consider
holding resources for the

00:33:29.120 --> 00:33:31.000
shortest possible period of time.

00:33:31.000 --> 00:33:33.180
There's obviously a trade-off here,
as I already mentioned.

00:33:33.220 --> 00:33:35.600
Allocating these resources takes time.

00:33:35.780 --> 00:33:37.190
Freeing them also takes time.

00:33:37.190 --> 00:33:40.060
But there's a cost to
your holding them idle,

00:33:40.060 --> 00:33:43.180
in that they are quite
often denied someone else.

00:33:46.940 --> 00:33:50.860
Given that other applications in
the system can consume resources and

00:33:50.860 --> 00:33:53.760
that we will give those resources
to them on an as-needed basis,

00:33:53.760 --> 00:33:56.390
you can't count.

00:33:56.570 --> 00:34:00.460
at any particular point in time on having
the resources that you actually need.

00:34:00.530 --> 00:34:03.940
I'm sorry if this sounds kind of unfair,
but if someone else needs them

00:34:03.940 --> 00:34:08.690
and we think that maybe they need
them more than you or they're

00:34:08.690 --> 00:34:13.390
just asking more frequently,
we're going to give them those

00:34:13.390 --> 00:34:13.390
resources because that means
they'll-- at least we believe that

00:34:13.390 --> 00:34:13.390
means they'll get their job done.

00:34:14.650 --> 00:34:18.830
You can generally assume that you get
a few CPU cycles and that you have some

00:34:18.840 --> 00:34:20.680
pages for your program text to be in.

00:34:20.690 --> 00:34:25.210
I mean, it's not realistic to expect
an application to adapt to its

00:34:25.210 --> 00:34:28.240
environment if it can't run at all.

00:34:28.240 --> 00:34:33.470
So, perhaps that makes you feel a little
bit better with the situation.

00:34:34.000 --> 00:34:37.190
But the availability of pretty much any
other resource in the system can change

00:34:37.190 --> 00:34:38.980
on a more or less instantaneous basis.

00:34:38.990 --> 00:34:41.000
Even while you're running,
even if you think that interrupts

00:34:41.000 --> 00:34:42.990
happen to be off because you're
in a device driver or something,

00:34:43.020 --> 00:34:46.730
someone else somewhere else in the
system can be running on another core

00:34:46.780 --> 00:34:51.400
and get their hands on a resource that
you think you ought to be able to have.

00:34:52.900 --> 00:34:55.580
Power management will
also play into this.

00:34:55.580 --> 00:35:00.110
Again, in order to keep the power
consumption on portable systems down,

00:35:00.110 --> 00:35:04.550
we will moderate the processor
and performance of various

00:35:04.560 --> 00:35:07.060
other components in the system.

00:35:07.070 --> 00:35:11.150
And so assumptions that you've made about
the overall throughput of the system,

00:35:11.160 --> 00:35:14.320
even just given straight-up
resource availability,

00:35:14.320 --> 00:35:16.130
aren't necessarily valid.

00:35:17.920 --> 00:35:24.660
All of this uncertainty means that you
need to think about your application's

00:35:24.660 --> 00:35:27.370
ability to deal with shortages.

00:35:31.580 --> 00:35:34.580
I don't want to make
this the normal case.

00:35:34.620 --> 00:35:38.000
Obviously,
from a performance perspective,

00:35:38.000 --> 00:35:40.690
you want to encourage your users to
run your application in an environment

00:35:40.700 --> 00:35:44.930
where the resources that the application
is going to need will be available.

00:35:45.900 --> 00:35:49.280
That isn't always the case,
and there are quite often situations

00:35:49.340 --> 00:35:51.940
where they'll want to do something else.

00:35:51.940 --> 00:35:54.690
They are willing to deal
with some degradation in the

00:35:54.710 --> 00:35:56.310
performance of your application.

00:35:56.380 --> 00:35:58.410
But if your application is
destroying the system by

00:35:58.480 --> 00:36:03.420
consuming too many physical pages,
or it's just performing impossibly badly,

00:36:03.460 --> 00:36:06.050
that's not necessarily
a great user experience.

00:36:06.250 --> 00:36:11.210
So it's important to think about how
you go about mitigating the effect of

00:36:11.210 --> 00:36:13.930
resource shortages on your application.

00:36:14.210 --> 00:36:18.500
This starts with understanding what your
application actually needs in order to

00:36:18.500 --> 00:36:22.800
get work done so that you can detect
when those resources aren't available.

00:36:23.190 --> 00:36:28.030
If you have critical performance
criteria with regards to resources,

00:36:28.120 --> 00:36:30.440
try reserving those resources.

00:36:30.500 --> 00:36:34.890
In certain very specific situations,
and I want to layer this particular

00:36:34.890 --> 00:36:38.740
point with as many caveats as I can,
you can wire down physical memory.

00:36:38.740 --> 00:36:41.100
That is, you can guarantee that
it will not be reclaimed,

00:36:41.100 --> 00:36:43.240
that those pages will always be present.

00:36:43.330 --> 00:36:46.930
Obviously, that's fairly uncivilized,
but if you can't afford to take a

00:36:46.930 --> 00:36:52.020
blocking fault for an absent page,
wiring down is your only alternative.

00:36:52.130 --> 00:36:55.210
You can use the thread scheduling
facilities we provide to ensure

00:36:55.210 --> 00:36:59.000
that you will get adequate CPU time,
although bear in mind that other

00:36:59.010 --> 00:37:00.730
people can use this technique as well.

00:37:00.730 --> 00:37:03.940
You can also obviously
pre-allocate disk files,

00:37:03.940 --> 00:37:05.470
as I mentioned earlier.

00:37:06.420 --> 00:37:11.540
If you can't meet your
acceptable performance target,

00:37:12.070 --> 00:37:15.240
It's good to have a
strategy to fall back on.

00:37:15.240 --> 00:37:18.590
Users don't much like applications
that spin the beach ball.

00:37:18.670 --> 00:37:22.250
They don't like applications that
take inexplicably long to do things.

00:37:22.340 --> 00:37:25.360
Having a fallback strategy that
perhaps performs half as well,

00:37:25.360 --> 00:37:30.240
but uses a quarter of the resources-- one
possible way to deal with the situation.

00:37:30.240 --> 00:37:32.980
Having a dialogue that says, look,
you're just doing too much.

00:37:32.980 --> 00:37:34.490
I can't actually get this done.

00:37:34.500 --> 00:37:36.280
You're not going to be happy.

00:37:36.320 --> 00:37:38.430
Do something to the system.

00:37:38.740 --> 00:37:39.460
Deal with this.

00:37:39.460 --> 00:37:41.980
Maybe another way of going about it.

00:37:41.980 --> 00:37:44.260
At the very least, you're being honest.

00:37:46.570 --> 00:37:50.930
I mentioned before that
power is a resource.

00:37:51.050 --> 00:37:52.580
Traditionally,
power has been considered a

00:37:52.580 --> 00:37:57.440
resource that the operating
systems really responsible for,

00:37:57.440 --> 00:37:57.440
but

00:37:58.010 --> 00:38:01.040
This goes all the way
up and down the stack.

00:38:01.050 --> 00:38:03.840
The hardware itself strives to be
as power-efficient as possible.

00:38:03.840 --> 00:38:06.910
The operating system strives to
be as power-efficient as possible.

00:38:06.970 --> 00:38:12.330
But the biggest consumers, by far,
of power in the system are applications,

00:38:12.330 --> 00:38:14.560
applications doing work.

00:38:14.640 --> 00:38:18.790
So you need to, once again,
consider efficiency.

00:38:28.500 --> 00:38:34.650
The system's power management
both helps and to a degree hinders

00:38:34.660 --> 00:38:36.740
your application's performance.

00:38:36.740 --> 00:38:40.280
As I've already mentioned,
the system may elect to reduce the

00:38:40.290 --> 00:38:44.690
performance of certain components
in order to meet power and thermal

00:38:44.700 --> 00:38:48.460
guarantees that are made by the system.

00:38:51.170 --> 00:38:53.630
That being said,
these algorithms do understand that

00:38:53.630 --> 00:38:56.270
your application needs to get work done.

00:38:56.850 --> 00:38:59.030
Generally, again, not a good idea to try
and second-guess them,

00:38:59.090 --> 00:39:02.320
but one key point that I can
make is that if your application

00:39:02.470 --> 00:39:05.410
can continue to do work,
if it can avoid performing

00:39:05.410 --> 00:39:08.880
work in a bursty fashion,
it will interact better with the

00:39:08.880 --> 00:39:12.160
system because that gives the
system a chance to recognize that

00:39:12.160 --> 00:39:13.690
it's trying to get work done.

00:39:24.500 --> 00:39:29.920
Change is a constant, what can I say?

00:39:29.920 --> 00:39:33.750
If you're building your application to
be aware of changes in its environment,

00:39:33.750 --> 00:39:39.720
to actually detect its performance,
to understand the availability

00:39:39.720 --> 00:39:41.580
of resources in the system,

00:39:41.930 --> 00:39:47.470
You're building in a good deal of
change-proofness in your application

00:39:47.530 --> 00:39:51.200
because you're no longer making
assumptions about the fact that you're

00:39:51.200 --> 00:39:54.570
running on a 1.8 gigahertz MacBook Pro.

00:39:54.920 --> 00:39:58.840
You are looking at your throughput and
the availability of resources to you,

00:39:58.840 --> 00:40:01.980
and so you can say,
whatever the system that I'm

00:40:01.980 --> 00:40:06.640
running on happens to be,
this is how I behave.

00:40:06.640 --> 00:40:10.600
This is another thing that will come
back to making your customers happy.

00:40:24.300 --> 00:40:31.910
The key to improving performance in
your applications is to measure them.

00:40:33.570 --> 00:40:36.430
Measurement isn't glamorous, it's boring,
it's a whole bunch of numbers,

00:40:36.440 --> 00:40:39.190
it's occasionally some
interesting looking graphs.

00:40:39.450 --> 00:40:41.400
but it's really effective.

00:40:41.430 --> 00:40:44.400
And it's effective because in
order to actually get anything

00:40:44.400 --> 00:40:46.860
done when it comes to tuning
performance in your application,

00:40:46.880 --> 00:40:48.850
you're going to need
to focus your efforts.

00:40:48.890 --> 00:40:51.170
You can't afford to spend

00:40:51.240 --> 00:40:55.240
However many years it will take you to
go over every line in your application,

00:40:55.240 --> 00:40:57.660
line by line,
contemplate its overall impact

00:40:57.660 --> 00:40:59.620
on the grand scheme of things.

00:40:59.620 --> 00:41:05.020
It's not a practical way to go
about optimizing performance.

00:41:06.970 --> 00:41:09.960
That being said,
if you do understand the interplay

00:41:09.960 --> 00:41:13.870
of components in your application,
that's a great place to start.

00:41:15.340 --> 00:41:16.730
But be careful.

00:41:16.800 --> 00:41:18.340
Guessing.

00:41:18.420 --> 00:41:20.500
Guessing is a great way to make
mistakes and spend an awful lot

00:41:20.500 --> 00:41:23.900
of time doing something that isn't
actually going to help you at all.

00:41:24.810 --> 00:41:26.900
Modern software tends to
be extremely complicated,

00:41:26.900 --> 00:41:30.230
and the interplay between
components is often very subtle.

00:41:31.520 --> 00:41:36.620
So if you're going to measure things,
try and measure in concrete terms.

00:41:36.620 --> 00:41:39.700
Elapsed time for a task,
total resources consumed

00:41:39.710 --> 00:41:41.890
for a particular task,
that sort of stuff.

00:41:41.900 --> 00:41:44.900
If you do this, and if you do this in
a repeatable fashion,

00:41:44.900 --> 00:41:47.500
you can actually integrate it
into your development workflow.

00:41:47.500 --> 00:41:51.610
It means that from build to build
you can monitor your performance,

00:41:51.640 --> 00:41:54.720
you can actually justify the time
that you spend on performance

00:41:54.730 --> 00:41:57.090
improvements if you happen to
be in a management situation

00:41:57.110 --> 00:41:58.500
that requires that sort of thing.

00:41:58.500 --> 00:42:02.420
And you can also say, "Look,
here is our benchmark here,

00:42:02.420 --> 00:42:06.750
here is our benchmark here,
we've improved performance 15%,

00:42:06.760 --> 00:42:08.610
that's a marketing bullet."

00:42:09.830 --> 00:42:14.430
What you actually choose to measure
is pretty much open to debate.

00:42:14.460 --> 00:42:17.670
Obviously, you need to measure the things
that your customers care about.

00:42:17.690 --> 00:42:20.510
If there are key tasks that are
performed by your application,

00:42:20.520 --> 00:42:22.200
those are great things to measure.

00:42:22.230 --> 00:42:25.020
When it comes to efficiency,
consider the things that we've

00:42:25.020 --> 00:42:29.790
been talking about already:
CPU cycles, pages used,

00:42:29.790 --> 00:42:31.680
that sort of thing.

00:42:35.390 --> 00:42:37.640
Measurement sounds like
an awful lot of work.

00:42:37.670 --> 00:42:38.800
And it is, I guess.

00:42:38.880 --> 00:42:40.670
But we give you a bunch of
tools to make life much,

00:42:40.680 --> 00:42:41.900
much easier.

00:42:41.950 --> 00:42:43.780
I'm not going to talk
too much about Shark.

00:42:43.780 --> 00:42:45.280
There have been other presentations here.

00:42:45.280 --> 00:42:47.780
There is an enormous amount
of documentation on the web.

00:42:47.780 --> 00:42:52.130
Shark is a great tool for understanding
what your code is doing in the system.

00:42:52.320 --> 00:42:54.820
It's also fairly helpful in
suggesting what you might do

00:42:54.880 --> 00:42:56.480
about things at the lower level.

00:42:56.490 --> 00:42:58.070
Obviously,
it doesn't know enough about what

00:42:58.080 --> 00:43:00.240
your application actually does
at the user level to give you

00:43:00.240 --> 00:43:02.360
high-level architectural suggestions.

00:43:02.380 --> 00:43:05.410
But when you're looking for
visibility and visibility into the

00:43:05.410 --> 00:43:09.570
system that understands the system,
Shark is a marvelous tool.

00:43:10.110 --> 00:43:13.100
This week we've been introducing
a new thing called DTrace.

00:43:13.100 --> 00:43:15.270
This comes from the good folks at Sun.

00:43:15.330 --> 00:43:19.050
DTrace is not, in and of itself,
a performance tool.

00:43:19.300 --> 00:43:22.300
It's an architecture and
infrastructure for building tools

00:43:22.300 --> 00:43:26.300
that can be used for both debugging
and for performance measurement.

00:43:26.300 --> 00:43:29.080
If you're developing-- if you're
interested in developing a specific

00:43:29.080 --> 00:43:32.680
performance evaluation harness
for your software that considers

00:43:32.680 --> 00:43:35.600
its use of resources and its
interaction with the system,

00:43:35.600 --> 00:43:39.780
DTrace gives you visibility from the top
of your application all the way through

00:43:39.780 --> 00:43:41.890
to the very lowest levels of the system.

00:43:43.320 --> 00:43:45.330
And there are a whole bunch
of more traditional tools,

00:43:45.350 --> 00:43:48.170
and it's in fact the traditional
tools I'm going to address because if

00:43:48.170 --> 00:43:51.150
we're talking about focusing effort,
the first thing that you want to be

00:43:51.150 --> 00:43:55.570
able to do is to start with a more
or less clean slate and narrow in

00:43:55.580 --> 00:43:59.490
on the one or two worst offenders
in any particular situation.

00:44:02.730 --> 00:44:04.940
Anyone who's worked on a Unix
system for any length of time

00:44:04.940 --> 00:44:06.210
will be familiar with Top.

00:44:06.220 --> 00:44:07.650
I'm not going to ask you
to read those numbers,

00:44:07.650 --> 00:44:09.980
although they're actually
pleasantly large.

00:44:10.010 --> 00:44:14.980
Top gives you a good view of
instantaneous overall system activity,

00:44:15.000 --> 00:44:18.050
which processes are doing things,

00:44:18.700 --> 00:44:24.460
This is the one I like the most because
it gives me a rapid summary of page-in,

00:44:24.460 --> 00:44:28.240
page-out activity,
which is very relevant to the

00:44:28.270 --> 00:44:32.650
overall system working set,
total I/Os, system calls,

00:44:32.650 --> 00:44:36.290
and it sorts the worst
offenders to the top.

00:44:40.630 --> 00:44:43.540
Understanding the use of the
file system by an application

00:44:43.540 --> 00:44:45.300
is actually fairly difficult.

00:44:45.300 --> 00:44:50.300
If you haven't instrumented the
application to tell you exactly what

00:44:50.300 --> 00:44:53.170
it's doing with the file system,
it can be quite difficult

00:44:53.170 --> 00:44:55.400
to actually-- well,
without this tool,

00:44:55.400 --> 00:44:57.690
I should say-- it can be quite
difficult to understand what's going on.

00:44:57.710 --> 00:45:00.000
FS usage will tell you what your
application is actually doing

00:45:00.000 --> 00:45:01.700
with regards to the file system.

00:45:01.720 --> 00:45:04.400
Paging, page add activity,
how long the I/Os actually took,

00:45:04.410 --> 00:45:06.290
how large they were.

00:45:06.810 --> 00:45:09.730
If you are beating on the file system
very hard and you don't understand

00:45:09.730 --> 00:45:13.370
why something is taking too long,
you just simply want to understand,

00:45:13.370 --> 00:45:14.690
FS Usage is where you start.

00:45:14.700 --> 00:45:21.290
Likewise, S Usage tells you the same sort
of thing about system calls.

00:45:22.270 --> 00:45:26.050
It's a little more specific than Top,
in that it actually breaks down

00:45:26.050 --> 00:45:30.510
the system calls by frequency,
gives you some other useful statistics.

00:45:30.780 --> 00:45:32.860
If you're not quite sure what
your application is doing because

00:45:32.860 --> 00:45:35.340
you don't necessarily have a
great deal of visibility into it,

00:45:35.540 --> 00:45:38.050
this is another good one to look at.

00:45:41.000 --> 00:45:43.700
I talked about virtual memory usage.

00:45:43.700 --> 00:45:47.210
VM Map will give you a detailed
breakdown of all of the map regions

00:45:47.210 --> 00:45:49.560
in your processor's address space.

00:45:49.560 --> 00:45:52.500
If you're trying to work out
where your virtual space is going,

00:45:52.500 --> 00:45:53.860
start here.

00:45:55.450 --> 00:46:00.200
goes a little bit further and actually
breaks down the internal data structures

00:46:00.200 --> 00:46:02.950
used by the system default malloc.

00:46:03.450 --> 00:46:08.580
and it can also break down
Objective-C object allocations.

00:46:08.730 --> 00:46:13.800
If you've decided from your use of
VM Map that malloc is the primary

00:46:13.800 --> 00:46:17.780
offender in consuming virtual space,
this is where you go next.

00:46:17.780 --> 00:46:21.620
There are a number of other
malloc-related debugging tools.

00:46:21.880 --> 00:46:25.960
Malik history in particular
that can be used to track that

00:46:25.960 --> 00:46:28.020
sort of thing even further.

00:46:28.050 --> 00:46:30.540
But typically I would say that
once you've ascertained that

00:46:30.540 --> 00:46:33.850
you have a mal-correlated issue,
what you actually have is an object

00:46:33.850 --> 00:46:36.320
management issue inside your application.

00:46:36.320 --> 00:46:38.420
And so you're going to need to look
at the way that you manage objects.

00:46:38.420 --> 00:46:40.490
And at that point,
you might want to consider either

00:46:40.510 --> 00:46:43.850
building a custom harness or using
DTrace in order to better understand

00:46:43.850 --> 00:46:46.200
how you're actually allocating objects.

00:46:46.200 --> 00:46:48.480
You may also simply be leaking.

00:46:50.290 --> 00:46:51.890
Okay,
so you've taken a bunch of measurements.

00:46:51.970 --> 00:46:52.960
Now what?

00:46:52.990 --> 00:46:55.690
Oop, come back here.

00:46:55.800 --> 00:46:56.890
There's a whole bunch of numbers.

00:46:56.940 --> 00:46:57.700
What do they actually mean?

00:46:57.700 --> 00:46:59.670
How do you use them?

00:47:09.100 --> 00:47:12.700
The first and most important thing is
to actually understand how these numbers

00:47:12.700 --> 00:47:15.430
relate to what your application is doing.

00:47:16.920 --> 00:47:19.420
Once you've done that,
you can start looking at particular

00:47:19.440 --> 00:47:21.600
portions of your application
that are responsible for those

00:47:21.600 --> 00:47:27.070
numbers and understanding how
the code is being used in order to

00:47:27.550 --> 00:47:29.250
in order to reach those numbers.

00:47:29.330 --> 00:47:31.780
Once you've achieved that sort of
understanding-- and you'll note

00:47:31.800 --> 00:47:34.030
that this is an understanding
of potentially a relatively

00:47:34.040 --> 00:47:36.250
small portion of the application,
rather than understanding

00:47:36.360 --> 00:47:38.170
the whole thing,
which is the whole objective of

00:47:38.170 --> 00:47:42.980
this focusing part-- you can start
thinking about how to improve it.

00:47:43.450 --> 00:47:47.180
Unfortunately,
it's at this point that performance

00:47:47.230 --> 00:47:49.300
tuning becomes very application-specific.

00:47:49.300 --> 00:47:53.310
We can talk about general rules
about resources and so forth,

00:47:53.310 --> 00:47:56.700
but the actual flow of data
inside your application,

00:47:56.700 --> 00:47:59.390
the flow of work inside the
application is what largely determines

00:47:59.390 --> 00:48:02.050
how all of that is consumed.

00:48:02.090 --> 00:48:04.350
And so it's at this point that you,
the owner of the application,

00:48:04.350 --> 00:48:06.800
really have to come up to the front.

00:48:06.900 --> 00:48:09.380
But there are some things
that you can think about.

00:48:10.860 --> 00:48:14.980
The numbers will typically tell you that
you are doing something a great deal.

00:48:14.980 --> 00:48:15.880
What are you doing?

00:48:15.880 --> 00:48:16.870
Are you doing it too often?

00:48:16.890 --> 00:48:18.790
Could you do it less?

00:48:19.000 --> 00:48:21.340
Are you doing this in
an efficient fashion?

00:48:21.340 --> 00:48:24.500
Specific things that you can
think about these actions.

00:48:26.020 --> 00:48:30.000
You can also use the results of your
performance measurements to understand,

00:48:30.000 --> 00:48:33.000
if you're not necessarily suffering
from performance problems,

00:48:33.000 --> 00:48:36.000
what resource expectations
you actually have.

00:48:36.000 --> 00:48:39.860
In order to perform a particular task,
how much of whatever this particular

00:48:39.860 --> 00:48:41.990
thing is measuring are you consuming?

00:48:43.680 --> 00:48:46.130
That leads, of course,
to the ability to deprive the application

00:48:46.130 --> 00:48:50.760
of those resources and to understand
its behavior in a degenerate case.

00:48:53.570 --> 00:48:56.650
I'm going to pull together
a really brief example here.

00:48:56.660 --> 00:48:58.740
This actually occurred while
I was writing the slides for this.

00:48:58.740 --> 00:49:01.700
I thought that I could use some time off,
and so I went out and bought a

00:49:01.700 --> 00:49:04.120
spiffy new game and installed it.

00:49:04.900 --> 00:49:07.660
I'm not going to finger the
developer of this game because

00:49:07.660 --> 00:49:11.990
they were very responsive without
any action on my part in producing

00:49:11.990 --> 00:49:14.170
a patch that addresses this issue.

00:49:15.820 --> 00:49:18.920
Having installed the game and, of course,
wanting to minimize it so that I could

00:49:18.920 --> 00:49:22.310
get back to working on my slides,
I noticed that the system

00:49:22.310 --> 00:49:24.210
performance was terrible.

00:49:24.330 --> 00:49:28.410
And so the obvious conclusion is that
this application is consuming resources,

00:49:28.420 --> 00:49:30.940
it's not necessarily
being a good citizen.

00:49:30.960 --> 00:49:33.680
So I start with top.

00:49:33.680 --> 00:49:36.800
Minus DUX,
the arguments that I like to use.

00:49:36.800 --> 00:49:41.510
And right there at the
top is my new game.

00:49:41.540 --> 00:49:45.180
It's using a great deal of CPU.

00:49:45.210 --> 00:49:47.640
And it's making an inordinate
number of BSD system calls.

00:49:47.640 --> 00:49:53.370
In this particular sample,
there are 474,000 BSD system calls.

00:49:53.460 --> 00:49:56.360
This is on a one second sample,
by the way.

00:49:59.420 --> 00:50:03.320
So, BSD system calls, lots of them, huh?

00:50:03.440 --> 00:50:07.910
So, let's have a look at what sort
of system calls it's making.

00:50:10.690 --> 00:50:13.300
Someone's taken away my screen resource.

00:50:13.330 --> 00:50:14.190
There we go.

00:50:14.340 --> 00:50:15.130
Wrong button.

00:50:15.170 --> 00:50:16.800
I always do that.

00:50:18.370 --> 00:50:22.610
And so we can see there,
about halfway down the screen,

00:50:22.610 --> 00:50:24.210
that in this particular
one-second sample,

00:50:24.210 --> 00:50:28.900
we've made nearly 680,000
k-event system calls.

00:50:28.950 --> 00:50:32.820
This is really not good for performance.

00:50:34.580 --> 00:50:37.240
Fortunately for me,
the developer of this application

00:50:37.240 --> 00:50:40.960
left all the symbols in it.

00:50:41.160 --> 00:50:45.880
I sprang out another one of my favorite
"What on Earth is Going On" tools,

00:50:45.880 --> 00:50:49.160
which I didn't mention before,
called Sample.

00:50:49.620 --> 00:50:52.440
Some of you will be familiar
with the sample application

00:50:52.520 --> 00:50:55.100
functionality in the Activity Monitor.

00:50:55.200 --> 00:50:57.700
This is the command line
foundation for that tool.

00:50:57.720 --> 00:51:00.690
It basically runs

00:51:01.200 --> 00:51:03.730
I've got a core graph sample on an
application on all of the threads in

00:51:03.730 --> 00:51:05.580
an application for a period of time.

00:51:05.580 --> 00:51:08.200
In this case, I asked it for 10 seconds
worth of sampling.

00:51:08.290 --> 00:51:10.280
There's an enormous amount of
other completely irrelevant

00:51:10.280 --> 00:51:12.200
rubbish in the backtrace.

00:51:12.200 --> 00:51:13.380
Like I said,
there are all the symbols in there,

00:51:13.380 --> 00:51:18.300
so I can see their direct 3D
emulation on top of OpenGL and

00:51:18.300 --> 00:51:19.200
all sorts of other stuff going on.

00:51:19.200 --> 00:51:20.790
But all that looks like real work.

00:51:20.790 --> 00:51:23.850
It looks like stuff that's actually
germane to the game running.

00:51:24.240 --> 00:51:27.930
But there's also this particular one.

00:51:27.960 --> 00:51:30.120
We're looking for k-event system calls,
right?

00:51:30.190 --> 00:51:36.190
So of the 930 samples, yeah,
930 samples for this particular thread,

00:51:36.200 --> 00:51:41.340
about 751 of them were inside k-event,
and about 150 of those were

00:51:41.360 --> 00:51:45.890
downstream of k-event in a
function called findChangeHandle.

00:51:47.440 --> 00:51:52.030
So something is going on that's causing
events to be sent to this application.

00:51:52.040 --> 00:51:55.070
And this bit more to the point
to this particular handler.

00:51:56.110 --> 00:51:58.680
At this point,
my ability to actually work out

00:51:58.680 --> 00:52:02.320
what's going on has pretty much run
out because I'm going to need to

00:52:02.320 --> 00:52:05.860
get the source to the application,
or I guess I could disassemble it.

00:52:05.900 --> 00:52:08.780
But I'm going to need to know what this
findChangeHandle function actually does

00:52:08.780 --> 00:52:10.920
in the larger context of the application.

00:52:11.000 --> 00:52:13.220
But in just a few commands,
I've narrowed it down from "my

00:52:13.220 --> 00:52:15.240
system performs really badly
because I'm running this game"

00:52:15.260 --> 00:52:18.050
to "what is this function doing?"

00:52:18.490 --> 00:52:22.270
And this is the beginning
of performance analysis.

00:52:22.360 --> 00:52:26.520
Incidentally, the OT Atomic--

00:52:26.970 --> 00:52:27.900
Was it?

00:52:27.900 --> 00:52:28.900
There we are.

00:52:29.000 --> 00:52:29.900
Yes.

00:52:29.900 --> 00:52:32.050
OT Atomic Test Bit,
a function that's called out there.

00:52:36.530 --> 00:52:41.440
Not really a great example
of performant coding.

00:52:41.450 --> 00:52:43.740
There's a function, a regular function.

00:52:43.740 --> 00:52:46.800
It's not inlined or anything like that.

00:52:46.830 --> 00:52:50.540
It masks a single bit in a byte.

00:52:50.640 --> 00:52:52.300
But in order to do this,
despite the fact that the

00:52:52.300 --> 00:52:54.830
access to the byte is atomic,

00:52:55.100 --> 00:52:57.520
and all the systems that this
code is likely to run on.

00:52:57.520 --> 00:52:59.100
It takes and releases a lock.

00:52:59.100 --> 00:53:01.060
Plus,
it also calls out to another function

00:53:01.130 --> 00:53:04.790
just in case that lock hasn't been
actually initialized in the first place.

00:53:04.840 --> 00:53:10.360
So this is a good example of things when
you're actually hunting in your code,

00:53:10.360 --> 00:53:13.720
looking for low-level performance things.

00:53:13.720 --> 00:53:17.750
If you see functions that are
implemented like this with

00:53:17.750 --> 00:53:18.710
very short amounts of work,
bracketed by locks,

00:53:18.710 --> 00:53:18.710
these are great low-hanging
fruit to attack.

00:53:31.290 --> 00:53:34.810
So onto the,
I believe this is the fifth step,

00:53:34.910 --> 00:53:37.800
taking advantage of work
that other people have done,

00:53:37.800 --> 00:53:42.190
or in the case of system
provider services,

00:53:42.200 --> 00:53:45.150
work that people will
continue to do in the future.

00:53:49.640 --> 00:53:51.400
Before you can actually take
advantage of this stuff,

00:53:51.400 --> 00:53:53.360
obviously,
you have to actually know what it is.

00:53:53.490 --> 00:53:56.680
We do a pretty good job, I think,
of advertising the functionality

00:53:56.680 --> 00:53:59.710
that the system exposes to
make life easier for you.

00:53:59.760 --> 00:54:04.560
It's not too hard for you to
browse the documentation that

00:54:04.560 --> 00:54:07.420
the good folks in DTS provide.

00:54:07.550 --> 00:54:10.290
And they're always willing to answer your
questions about how to actually improve

00:54:10.290 --> 00:54:12.800
the performance of your application,
leveraging system services.

00:54:12.860 --> 00:54:14.980
These are things that we
expect you to be using,

00:54:14.980 --> 00:54:17.830
so we're prepared to deal with
you asking questions about how to

00:54:17.830 --> 00:54:19.480
actually take advantage of them.

00:54:21.300 --> 00:54:25.200
The critical point I brought up talking
about the previous slide is that these

00:54:25.200 --> 00:54:28.020
services will continue to improve.

00:54:28.020 --> 00:54:31.800
Because we ship them,
because we use these services ourselves,

00:54:31.800 --> 00:54:34.060
we constantly optimize
them for new platforms.

00:54:34.060 --> 00:54:36.810
We constantly improve them
on existing platforms.

00:54:36.810 --> 00:54:40.100
They're basically a free
performance upgrade for your

00:54:40.140 --> 00:54:42.220
application if you're using them.

00:54:46.040 --> 00:54:50.030
In adopting a system service,
there is always a cost.

00:54:50.040 --> 00:54:51.940
I talked before about intermodule costs.

00:54:51.940 --> 00:54:54.930
Here I am advocating modular
software design and code reuse

00:54:54.940 --> 00:54:57.140
after just bashing it earlier on.

00:54:57.140 --> 00:55:00.590
You've got to understand there's a
trade-off involved in all of this.

00:55:01.270 --> 00:55:04.270
Typically, though,
leveraging a system service that

00:55:04.280 --> 00:55:07.550
does something even moderately
complex is going to be easier than

00:55:07.550 --> 00:55:10.150
implementing it from scratch yourself.

00:55:11.610 --> 00:55:14.440
On top of that,
these services typically maintain stable

00:55:14.440 --> 00:55:17.420
interfaces for long periods of time.

00:55:18.320 --> 00:55:21.110
So your adoption cost is
usually only paid once,

00:55:21.110 --> 00:55:24.070
provided of course your application's
internal architecture remains

00:55:24.080 --> 00:55:26.510
likewise relatively stable.

00:55:26.860 --> 00:55:31.360
If you duplicate some particular
piece of performance-sensitive

00:55:31.360 --> 00:55:34.700
functionality and the system changes,
your implementation will

00:55:34.710 --> 00:55:35.800
need to change as well.

00:55:35.800 --> 00:55:39.560
And that obviously imposes an
ongoing maintenance burden that

00:55:39.560 --> 00:55:41.580
you probably don't want to adopt.

00:55:42.200 --> 00:55:45.720
It is worth bearing in mind though
that system services with nice,

00:55:45.720 --> 00:55:50.430
stable, generalized interfaces often
impose some penalty in order to

00:55:50.440 --> 00:55:53.330
achieve that sort of generality.

00:55:54.300 --> 00:55:57.360
There's also potentially a loss
of critical differentiation.

00:55:57.360 --> 00:56:00.260
If there's a system service that does
something that you honestly believe

00:56:00.280 --> 00:56:03.710
and can measure that you do better,
if you adopt the system service,

00:56:03.740 --> 00:56:07.310
you are potentially losing
that critical differentiation.

00:56:07.950 --> 00:56:11.250
That being said,
if what you're doing is something that

00:56:11.250 --> 00:56:13.730
is heavily used by other applications,
we're going to be

00:56:13.800 --> 00:56:15.080
optimizing that as well.

00:56:15.080 --> 00:56:18.600
And so the cost that you're
paying in order to maintain

00:56:18.600 --> 00:56:22.390
that critical differentiation
may ultimately come to nothing.

00:56:26.260 --> 00:56:31.760
I started off with about eight slides
worth of examples of system services,

00:56:31.760 --> 00:56:36.160
which would make for a really
boring portion of this presentation.

00:56:36.160 --> 00:56:41.680
So I culled this down to a list
of just a few high-point examples.

00:56:41.800 --> 00:56:43.800
I've seen people
re-implement far too often.

00:56:43.800 --> 00:56:45.160
I really wish they wouldn't.

00:56:45.180 --> 00:56:46.840
MemCopy is right at the
very top of the list.

00:56:46.840 --> 00:56:50.280
I talked about copies
being evil earlier on.

00:56:50.280 --> 00:56:53.320
There are times when it
is simply inevitable that

00:56:53.320 --> 00:56:54.490
you must copy information.

00:56:54.490 --> 00:56:56.050
You're about to make a change to it.

00:56:56.050 --> 00:56:57.840
You're going to throw
away the old version,

00:56:57.850 --> 00:57:00.090
and the old version is going to change,
whatever.

00:57:00.090 --> 00:57:03.360
There are any number of reasons
why copying something is

00:57:03.360 --> 00:57:05.470
actually a necessary thing to do.

00:57:06.030 --> 00:57:08.620
It's also,
from a performance perspective,

00:57:08.620 --> 00:57:12.490
incredibly sensitive to the architecture
of the system that you're running on.

00:57:12.780 --> 00:57:17.920
And in order to address
this particular concern,

00:57:18.020 --> 00:57:20.660
we go to some fairly extensive lengths.

00:57:20.720 --> 00:57:24.420
There are custom implementations
of MemCopy for virtually every

00:57:24.420 --> 00:57:26.140
system that we've shipped.

00:57:26.140 --> 00:57:29.250
It's becoming a little more stable
with some of the Intel systems,

00:57:29.250 --> 00:57:32.220
but by and large,
those implementations of MemCopy

00:57:32.220 --> 00:57:35.320
have specific knowledge of the
architecture of the individual

00:57:35.320 --> 00:57:36.620
machine that they're running on.

00:57:36.620 --> 00:57:39.460
And they're implemented in such
a fashion that there is little

00:57:39.460 --> 00:57:42.460
or no overhead in determining
which version to actually use.

00:57:42.460 --> 00:57:44.950
And

00:57:45.320 --> 00:57:47.790
Keeping up with this, I mean,
this is a great example of why you

00:57:47.800 --> 00:57:49.110
just shouldn't re-implement memcopy.

00:57:49.160 --> 00:57:51.810
Keeping up with all of this work
that we've done to optimize this is

00:57:51.810 --> 00:57:53.270
just a burden you don't need to pay.

00:57:53.310 --> 00:57:57.500
There's very little to be gained in
terms of critical differentiation

00:57:57.510 --> 00:57:59.370
by having a different memcopy.

00:57:59.730 --> 00:58:04.860
XML parsers,
all sorts of mathematical functions,

00:58:04.860 --> 00:58:07.360
image processing,
string manipulation really

00:58:07.360 --> 00:58:09.250
belongs up there with memcopy.

00:58:09.330 --> 00:58:12.840
Once again,
these are very cache and processor

00:58:12.870 --> 00:58:15.760
architecture sensitive algorithms.

00:58:16.280 --> 00:58:18.570
We've taken the time
to optimize them much,

00:58:18.570 --> 00:58:21.570
much better for you just
to take advantage of that.

00:58:23.480 --> 00:58:27.390
At a slightly higher level,
you find things like XGrid,

00:58:27.390 --> 00:58:29.900
which allow you to build
distributed applications.

00:58:29.900 --> 00:58:36.710
The list is, as I said,
really quite expansive.

00:58:47.000 --> 00:58:53.880
We've made a little bit of hoo-ha
about 64-bit support in new hardware,

00:58:53.940 --> 00:58:57.320
and not only that,
but in the upcoming Leopard

00:58:57.320 --> 00:58:59.150
release of Mac OS X.

00:59:00.730 --> 00:59:05.140
There's a lot to be said both
to encourage you and to perhaps

00:59:05.140 --> 00:59:08.230
actually discourage you from
considering moving your applications

00:59:08.280 --> 00:59:09.700
to a 64-bit environment.

00:59:09.700 --> 00:59:12.100
Ultimately,
you and your customers are going

00:59:12.100 --> 00:59:15.700
to be the ones that will decide
what the right criteria are,

00:59:15.700 --> 00:59:18.870
when and how to actually make the switch.

00:59:21.320 --> 00:59:24.530
That being said,
there are some advantages to

00:59:24.610 --> 00:59:30.200
pursuing a 64-bit implementation at
the earliest possible convenience.

00:59:30.260 --> 00:59:33.220
The large virtual address space
that 64-bit applications have

00:59:33.240 --> 00:59:37.190
available to them does let you do
some fairly interesting things.

00:59:37.220 --> 00:59:39.820
You do need to bear in mind the
point that I raised earlier about

00:59:39.830 --> 00:59:42.260
virtual allocations costing.

00:59:42.280 --> 00:59:44.270
But that being said,
you can do some fairly neat stuff

00:59:44.340 --> 00:59:46.640
if you're no longer constrained
to a total of four gigs worth

00:59:46.640 --> 00:59:47.930
of virtual address space.

00:59:47.980 --> 00:59:50.360
You can consolidate things that
you may have previously had to

00:59:50.360 --> 00:59:54.260
do in multiple address spaces
or by partitioning your data.

00:59:55.260 --> 00:59:59.550
It also means that with more
heavily configured systems,

00:59:59.550 --> 01:00:03.450
you can take advantage of more
physical memory in a single process.

01:00:05.310 --> 01:00:10.320
For the Intel platform,
code performance does change.

01:00:10.320 --> 01:00:14.470
The 64-bit code generation
is quite a bit different.

01:00:14.620 --> 01:00:16.220
Just to begin with,
there are more registers.

01:00:16.300 --> 01:00:19.910
This has enormous
impact on use of memory.

01:00:19.920 --> 01:00:23.590
Algorithms that previously used to spill
into their one cache in particular can

01:00:23.600 --> 01:00:26.690
now be implemented entirely in registers.

01:00:27.110 --> 01:00:31.640
There's also considerable improvement in
the calling convention between functions,

01:00:31.640 --> 01:00:34.520
making intermodule calls
potentially quite a lot cheaper.

01:00:35.600 --> 01:01:00.800
[Transcript missing]

01:01:02.060 --> 01:01:05.430
As I just mentioned,
maintaining a very large virtual space

01:01:05.440 --> 01:01:08.640
has a cost that is not necessarily
paid directly by your application.

01:01:08.640 --> 01:01:14.600
A large number of those address spaces
will simply exacerbate the situation.

01:01:15.420 --> 01:01:18.330
There's also efficiency to be
considered if you're spreading

01:01:18.330 --> 01:01:20.940
your data out a great deal more.

01:01:20.940 --> 01:01:25.600
That effectively decreases the
multiplier effect of the cache size.

01:01:25.600 --> 01:01:28.480
If your data is more sparsely spread,
particularly if you're using

01:01:28.480 --> 01:01:30.630
a stride that is incompatible
with the way that the cache is

01:01:30.630 --> 01:01:34.250
organized on a particular machine,
you may find that you have very

01:01:34.250 --> 01:01:36.770
much negative caching implications.

01:01:37.490 --> 01:01:42.760
It also becomes very difficult to, well,
very difficult is probably

01:01:42.760 --> 01:01:43.800
the wrong way of putting it.

01:01:43.810 --> 01:01:47.650
Leaky applications in a 32-bit
virtual space tend to bring

01:01:47.650 --> 01:01:49.750
themselves to your attention by
running out of virtual address

01:01:49.750 --> 01:01:51.160
space fairly quickly and crashing.

01:01:51.160 --> 01:01:53.960
This doesn't happen with
64-bit applications.

01:01:53.960 --> 01:01:57.130
You can leak until the heat death
of the universe and you're not

01:01:57.130 --> 01:01:58.900
going to run out of virtual space.

01:01:59.270 --> 01:02:02.010
The kernel will probably crash
because it'll run out of virtual

01:02:02.040 --> 01:02:04.570
space to store those mappings,
but your app won't.

01:02:05.750 --> 01:02:10.270
Some of the code performance changes
that I mentioned before are not so great.

01:02:10.660 --> 01:02:13.700
CodeGen for 64-bit
Intel processors is still maturing,

01:02:13.700 --> 01:02:16.530
and our understanding of that
code generation is still maturing.

01:02:16.610 --> 01:02:18.490
This is something where--

01:02:18.630 --> 01:02:20.950
Time will certainly
improve the situation.

01:02:20.970 --> 01:02:23.100
By the time the leopard
is actually released,

01:02:23.100 --> 01:02:25.440
we expect that we will understand
this a lot better and that much

01:02:25.440 --> 01:02:26.600
of this will be ironed out.

01:02:26.610 --> 01:02:29.600
Something that you need to measure,
however.

01:02:30.400 --> 01:02:33.490
Every pointer in a 64-bit
application doubles in size.

01:02:33.490 --> 01:02:37.650
If you have large, complex,
interlinked data structures,

01:02:37.720 --> 01:02:40.300
this means that those
data structures will grow.

01:02:40.300 --> 01:02:44.060
Any work that you've put in on a 32-bit
application for optimizing the structure

01:02:44.060 --> 01:02:50.300
layout for cache line size will have to
be reconsidered because the association

01:02:50.300 --> 01:02:53.300
of members inside the structure
will change because of this change.

01:02:53.320 --> 01:03:09.080
On top of that, some opcodes also grow.

01:03:09.080 --> 01:03:09.080
This means that if you've optimized
code for cache residency or for

01:03:09.080 --> 01:03:09.080
timing related to memory fill,
that may also be perturbed by this.

01:03:09.580 --> 01:03:12.000
And there is the unfortunate fact
that some of our machines will

01:03:12.000 --> 01:03:14.740
simply never run 64-bit code.

01:03:14.700 --> 01:03:18.020
If you want to run your application,
or if you want your application to be

01:03:18.020 --> 01:03:20.730
able to run on one of those systems,
you need to consider building

01:03:20.730 --> 01:03:23.700
both a 32-bit and a 64-bit
version of the application.

01:03:23.700 --> 01:03:26.690
We make this pretty
straightforward with Xcode,

01:03:26.700 --> 01:03:28.710
but there are algorithmic
and performance-related

01:03:28.750 --> 01:03:29.700
implications for this.

01:03:29.700 --> 01:03:33.050
In particular,
if you are considering re-optimizing your

01:03:33.120 --> 01:03:37.480
application for a 64-bit environment,
some of those changes are likely

01:03:37.480 --> 01:03:40.870
to perturb the way that it
performs in a 32-bit environment.

01:03:47.860 --> 01:03:50.260
So going back over all of this,
I can really sum it all up with,

01:03:50.260 --> 01:03:52.830
there is no magic bullet.

01:03:52.840 --> 01:03:57.680
Making your application faster involves
you understanding it and doing real work.

01:03:57.680 --> 01:04:00.100
Sorry.

01:04:00.100 --> 01:04:02.510
But the five-step plan will help.

01:04:03.840 --> 01:04:08.260
Because doing work takes time,
you should try and do less of it.

01:04:08.260 --> 01:04:11.820
When you have something to do,
get on with it.

01:04:12.520 --> 01:04:16.060
Treat other applications in the system
like you'd like them to treat you.

01:04:16.080 --> 01:04:18.960
Don't just blow them off.

01:04:20.540 --> 01:04:23.920
Performance tuning is not a black art.

01:04:23.920 --> 01:04:24.700
It's a science.

01:04:24.700 --> 01:04:26.780
It's a science that you can master.

01:04:26.780 --> 01:04:29.030
We provide a bunch of
tools to help you with it.

01:04:29.050 --> 01:04:32.460
We provide some great folks who
will help you if you need help.

01:04:34.130 --> 01:04:37.740
Don't ever despair of understanding
what your application is doing,

01:04:37.740 --> 01:04:39.730
why it's performing the way it is.

01:04:39.750 --> 01:04:42.340
If you can't understand it
based on the data you have,

01:04:42.340 --> 01:04:43.640
look for more data.

01:04:43.640 --> 01:04:47.100
Consider in particular the enormous
advantage that DTrace gives you in

01:04:47.100 --> 01:04:51.250
understanding specifically what your
application and what the environment

01:04:51.250 --> 01:04:53.400
around your application is doing.

01:04:53.400 --> 01:04:56.940
And take advantage of the
stuff that we're offering you.

01:04:57.620 --> 01:05:00.030
The real value that you give
to your applications is the

01:05:00.100 --> 01:05:01.500
neat stuff that you guys do.

01:05:01.500 --> 01:05:06.250
Reimplementing code, algorithms,
functionality that Apple provides

01:05:06.260 --> 01:05:08.800
to you is just wasting the time
that could be much better spent

01:05:08.800 --> 01:05:11.000
making your customers happy.