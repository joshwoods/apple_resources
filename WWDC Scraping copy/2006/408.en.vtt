WEBVTT

00:00:10.250 --> 00:00:12.930
Good morning.

00:00:12.980 --> 00:00:13.690
It's amazing.

00:00:13.810 --> 00:00:15.440
If you had asked me a
year and a half ago,

00:00:15.440 --> 00:00:20.560
would I be on stage talking to a
bunch of Apple developers at WWDC,

00:00:20.560 --> 00:00:22.870
I thought you would have been crazy.

00:00:23.220 --> 00:00:27.730
But it's here,
and I'm overwhelmed but excited.

00:00:27.800 --> 00:00:30.190
I've had a wonderful
time actually attending,

00:00:30.200 --> 00:00:34.530
talking to a lot of people,
doing a lot of great things on the Mac,

00:00:34.630 --> 00:00:40.060
and I think it's fantastic that it's
finally materialized on Intel hardware.

00:00:40.130 --> 00:00:41.850
In fact,
it wasn't until about a year and a half

00:00:41.930 --> 00:00:46.100
ago that I had first walked into an
Apple retail store for the first time.

00:00:46.210 --> 00:00:49.090
And I have to tell you, I'm a convert.

00:00:49.220 --> 00:00:51.320
So when I was at--

00:00:55.080 --> 00:00:58.150
So when we came up with
the topic for performance

00:00:58.270 --> 00:01:02.870
optimization for Intel-based Macs,

00:01:03.270 --> 00:01:06.800
I took a lot of time to think
about exactly what that meant.

00:01:06.860 --> 00:01:10.020
There's a lot of different
avenues that that can take,

00:01:10.020 --> 00:01:12.950
top to bottom,
from algorithms to implementation

00:01:13.170 --> 00:01:21.000
to instruction set,
down to the actual hardware and

00:01:21.060 --> 00:01:22.710
how the hardware architecture
is actually implemented.

00:01:22.940 --> 00:01:26.870
And I thought, well, first of all,
we should focus on things that are

00:01:26.870 --> 00:01:29.420
unique to Intel versus what's to Mac.

00:01:29.480 --> 00:01:32.300
So that would rule out
things like algorithms.

00:01:32.380 --> 00:01:37.130
Most of those are going to be
common across both architectures.

00:01:37.280 --> 00:01:39.250
And I thought that, well,
I could talk about the

00:01:39.250 --> 00:01:41.900
hardware and the architecture,
but there's another talk that

00:01:41.900 --> 00:01:44.400
actually talks about the architecture.

00:01:44.470 --> 00:01:47.240
Plus, I don't think that you actually
get the bang for the buck

00:01:47.240 --> 00:01:48.980
if you look that low level.

00:01:49.210 --> 00:01:51.440
The first part is you have
to look at algorithms,

00:01:51.500 --> 00:01:54.730
then look at implementation,
then you get down to the hardware

00:01:54.800 --> 00:01:57.460
and the actual architecture and
tuning for the architecture.

00:01:57.560 --> 00:01:59.620
So I wanted to pick someplace in between.

00:01:59.820 --> 00:02:03.750
And one of the unique places where
you can get one of the biggest bangs

00:02:03.750 --> 00:02:07.540
for the buck is actually looking
at your AltaVec implementation

00:02:07.550 --> 00:02:10.920
and converting that to SSE.

00:02:12.110 --> 00:02:14.590
So that's really what we're
going to focus on today,

00:02:14.590 --> 00:02:19.260
is really demonstrating the steps
to optimizing something that's

00:02:19.260 --> 00:02:22.700
already been well thought out
of in terms of the algorithm,

00:02:22.850 --> 00:02:27.550
has already been implemented in AlteVec,
and really looking at the steps

00:02:27.550 --> 00:02:33.110
of what do we need to do to get to
SSC quickly and gain the performance

00:02:33.340 --> 00:02:36.060
from the SSC implementation.

00:02:36.110 --> 00:02:40.040
I'll highlight, hopefully mostly,
similarities, because if you look at

00:02:40.110 --> 00:02:43.080
the instruction set,
there's a lot of overlap.

00:02:43.190 --> 00:02:46.640
It's almost straightforward,
one-for-one implementation,

00:02:46.680 --> 00:02:48.360
with a few differences.

00:02:48.480 --> 00:02:52.680
And then, you know,
GCC has been optimized for the

00:02:52.680 --> 00:02:55.320
PowerPC for a very long time.

00:02:55.370 --> 00:02:58.280
Apple's done a great job
for tuning for PowerPC.

00:02:58.430 --> 00:03:00.510
And it's only been a year,
year and a half,

00:03:00.540 --> 00:03:03.400
that we've actually looked
at the Intel processor.

00:03:03.400 --> 00:03:06.060
And so that's where the
low-level architecture comes in.

00:03:06.060 --> 00:03:09.930
And the experience from the
Intel compiler group at Intel in terms

00:03:09.930 --> 00:03:12.370
of optimizing for our architecture.

00:03:12.430 --> 00:03:15.240
So we'll look at, you know,
what are the benefits of,

00:03:15.240 --> 00:03:18.570
in this particular case,
of using the Intel compiler on

00:03:18.610 --> 00:03:20.670
this particular application.

00:03:21.480 --> 00:03:22.850
So we're going to introduce Noble Apes.

00:03:22.960 --> 00:03:28.020
If you were here at WWDC last year,
I know that it was a topic of using

00:03:28.020 --> 00:03:30.130
Shark and how to do optimization.

00:03:30.440 --> 00:03:34.180
In that presentation,
they started out with a

00:03:34.180 --> 00:03:38.320
scalar implementation,
converted that to AlteVac,

00:03:38.450 --> 00:03:41.830
got significant speedups,
and then actually focused

00:03:41.830 --> 00:03:42.950
on optimizing that.

00:03:43.020 --> 00:03:47.360
So I thought that would be a great
place to start because the application's

00:03:47.360 --> 00:03:49.680
already been optimized for PowerPC.

00:03:49.880 --> 00:03:53.320
It's already gone through the
implementation to AlteVac.

00:03:53.410 --> 00:03:55.250
It's been optimized for AlteVac.

00:03:55.540 --> 00:03:57.400
We'll take that as a starting base.

00:03:57.450 --> 00:04:00.120
And now we get into something
that's more unique to Intel,

00:04:00.180 --> 00:04:03.410
which would be the SSE or the
Streaming SIMD instruction

00:04:03.510 --> 00:04:06.510
or extension instructions.

00:04:06.720 --> 00:04:08.780
So we'll translate that for Noble Apes.

00:04:08.950 --> 00:04:12.680
We'll look at the performance,
and then we'll apply the Intel compiler

00:04:12.780 --> 00:04:16.070
and then close with questions at the end.

00:04:16.250 --> 00:04:17.800
So what is Noble Apes?

00:04:17.800 --> 00:04:22.500
Noble Apes is a very simple
application that simulates a bunch

00:04:22.500 --> 00:04:24.360
of apes running around an island.

00:04:24.380 --> 00:04:29.340
There's one particular ape
that has a view of his terrain.

00:04:29.730 --> 00:04:32.560
That happens to be the one
that's actually with the red dot.

00:04:32.760 --> 00:04:36.480
And when I get into the demonstration,
I'll point it out a little bit more.

00:04:36.490 --> 00:04:40.160
And then the AlteVec part is
actually focused on the brain

00:04:40.250 --> 00:04:45.080
of the ape as it's thinking,
as it goes into its sleep cycle,

00:04:45.080 --> 00:04:49.390
as it wakes up, as it eats,
as it wanders around during the day.

00:04:49.400 --> 00:04:52.720
And then we measure how many thoughts
or how many calculations we're

00:04:52.720 --> 00:04:59.100
doing per second for that ape to
actually think about what it's doing.

00:04:59.120 --> 00:05:03.120
And this is an application that ships
with the Chud tools in order to be

00:05:03.120 --> 00:05:07.660
able to use with Shark and follow along
with the tutorial that goes with that.

00:05:07.750 --> 00:05:08.900
It's also multithreaded.

00:05:09.100 --> 00:05:13.040
So that was another thing that
I didn't really feel like we needed to

00:05:13.120 --> 00:05:14.680
talk about in terms of optimization.

00:05:14.680 --> 00:05:18.980
Even though Intel has multi-core,

00:05:19.270 --> 00:05:23.200
Apple Mac has had had multi-core
for a long time as well,

00:05:23.400 --> 00:05:26.700
and so that's not something that
we were going to necessarily cover.

00:05:26.730 --> 00:05:29.120
But it's great for this
particular application.

00:05:29.220 --> 00:05:32.610
So let me go ahead and switch to
Noble Apes just to give you a little

00:05:32.610 --> 00:05:34.490
preview of what it looks like.

00:05:51.100 --> 00:05:51.510
OK.

00:05:51.700 --> 00:05:57.340
So see if I can zoom in
on this a little bit,

00:05:57.340 --> 00:05:59.860
if that helps.

00:05:59.860 --> 00:06:00.360
OK.

00:06:00.360 --> 00:06:04.620
So up here in this corner,
you can see that the-- oops,

00:06:04.620 --> 00:06:08.660
my mouse is moving as I move
along-- that the daytime,

00:06:08.750 --> 00:06:11.240
nighttime is changing.

00:06:11.240 --> 00:06:13.420
The apes are wandering around the screen.

00:06:13.480 --> 00:06:16.650
This is the particular ape that we're
actually looking at that happens

00:06:16.650 --> 00:06:18.720
to be wandering around the screen.

00:06:18.720 --> 00:06:22.300
This is the terrain view that
that particular ape is looking at.

00:06:22.310 --> 00:06:25.570
And these are the thoughts that the ape,
or actually what the brain is

00:06:25.680 --> 00:06:29.840
looking like as it's wandering
around this particular island.

00:06:29.840 --> 00:06:31.480
And this is what we're looking at.

00:06:31.480 --> 00:06:34.260
So we have op thoughts per second.

00:06:34.270 --> 00:06:41.220
Right now, we're at about 1,300 or so
for the scalar implementation.

00:06:41.230 --> 00:06:45.880
There was also a vectorized
implementation that got a

00:06:45.880 --> 00:06:47.320
significant improvement.

00:06:47.320 --> 00:06:52.340
We're up to-- over 3,000 op thoughts.

00:06:52.690 --> 00:06:56.880
There's a vectorized optimized version,
which gets even more.

00:06:57.050 --> 00:07:02.370
We're up around 48, 49, 5,000 timeframe.

00:07:02.370 --> 00:07:06.290
And then it's also threaded.

00:07:07.310 --> 00:07:09.950
So I thought this was a good
application to start with.

00:07:10.070 --> 00:07:14.000
It kind of covered a lot of the ideas
for general optimization techniques.

00:07:14.120 --> 00:07:16.630
It had multi-threading in it,
was already AlteVec,

00:07:16.730 --> 00:07:19.830
and then I would take a look at it
and see what we could do from an

00:07:20.220 --> 00:07:25.050
Intel perspective for optimizing
this particular application.

00:07:32.500 --> 00:07:43.500
[Transcript missing]

00:07:45.600 --> 00:07:48.950
So a real quick brief
overview of what SSC means.

00:07:49.200 --> 00:07:54.060
It's the streaming SIMD instructions
or extensions for the Intel processor.

00:07:54.060 --> 00:07:56.150
We started out with the
multimedia extensions,

00:07:56.200 --> 00:08:01.820
which was focused on 64-bit
versions for integers.

00:08:02.240 --> 00:08:05.300
We added SSC,
which was the first incantation

00:08:05.300 --> 00:08:09.610
of the SSC instructions,
which was focused on single precision

00:08:09.740 --> 00:08:12.280
floating point instruction set.

00:08:12.540 --> 00:08:18.270
SSC2 went back and added the 64-bit
integer versions that were similar

00:08:18.270 --> 00:08:24.100
to the MMX version but were actually
implemented in the 128-bit registers.

00:08:24.140 --> 00:08:28.200
And then SSC3 added some additional
complex math and some horizontal

00:08:28.600 --> 00:08:32.740
add/subtract instructions,
a very much smaller delta.

00:08:33.040 --> 00:08:37.800
All three of these instruction
sets are supported on the

00:08:37.810 --> 00:08:41.170
Intel Mac-based processors today.

00:08:44.470 --> 00:08:46.320
So as I was going through the porting,
I thought, well,

00:08:46.320 --> 00:08:50.430
maybe it's good to put together a little
bit of a recipe of what I did in terms

00:08:50.440 --> 00:08:52.920
of actually porting the application.

00:08:52.980 --> 00:08:57.620
The first one was I wanted to make sure
that we supported universal binaries.

00:08:57.710 --> 00:09:00.690
We've already done all the work,
or Apple has already done all the work,

00:09:00.800 --> 00:09:03.400
to implement the AlteVec version.

00:09:03.400 --> 00:09:07.740
There's still a lot of platforms out
there that are going to be PowerPC-based.

00:09:07.840 --> 00:09:11.590
So we definitely don't want
to take that away from them.

00:09:11.640 --> 00:09:16.060
So we want to make sure that
we support universal binaries.

00:09:16.110 --> 00:09:19.480
Then we really need to look at the
data structures and the data and

00:09:19.480 --> 00:09:25.460
modify the types that are being
used between AlteVec and SSC.

00:09:25.610 --> 00:09:28.160
Then it was fairly straightforward for,
I would say,

00:09:28.160 --> 00:09:31.900
almost 90% of the instructions
to go through and just implement

00:09:31.900 --> 00:09:35.700
or transcribe the one-to-one
correspondence between the AlteVec

00:09:35.760 --> 00:09:39.430
instruction to the SSC instruction.

00:09:39.640 --> 00:09:41.480
There's a few instructions
that we don't support.

00:09:41.490 --> 00:09:45.860
We don't support on a one-to-one basis,
but with a combination of two, three,

00:09:45.900 --> 00:09:48.020
or four instructions,
you can achieve the same thing.

00:09:48.020 --> 00:09:51.630
It's pretty much of a recipe
type of implementation.

00:09:51.960 --> 00:09:55.640
There's really not a great
alternative unless you can modify

00:09:55.960 --> 00:09:57.900
your algorithm to avoid it.

00:09:57.960 --> 00:10:00.960
So there are some just very
standard ones that you just go in

00:10:01.000 --> 00:10:03.380
and replace with the single one.

00:10:03.470 --> 00:10:06.300
And then I would say there
is nonstandard combinations,

00:10:06.380 --> 00:10:09.990
things that use
instructions like permute,

00:10:10.030 --> 00:10:11.270
some of the multiplies.

00:10:11.360 --> 00:10:11.500
There are some just very standard
ones that you just go in and

00:10:11.500 --> 00:10:11.720
replace with the single one.

00:10:11.720 --> 00:10:12.490
And then I would say there
is nonstandard combinations,

00:10:12.490 --> 00:10:13.100
things that use
instructions like permute,

00:10:13.140 --> 00:10:13.460
some of the multiplies.

00:10:13.630 --> 00:10:17.600
So there's a little bit
more harder part to it.

00:10:17.970 --> 00:10:22.010
Not all of the data type
variants are supported,

00:10:22.010 --> 00:10:28.160
so you start having to think about
what is your precision requirements?

00:10:28.240 --> 00:10:31.080
Why did you do the algorithm
the way that you did?

00:10:31.130 --> 00:10:33.600
So there's a little bit
more thinking to it.

00:10:33.620 --> 00:10:38.400
And then actually looking to see how
you can optimize the implementation to

00:10:38.400 --> 00:10:41.240
take advantage of the SSC instructions.

00:10:41.240 --> 00:10:42.860
So the AlteVec implementation was done.

00:10:42.860 --> 00:10:45.960
They looked at the set of
best instructions to use.

00:10:45.980 --> 00:10:48.900
And the same is true for SSC.

00:10:48.900 --> 00:10:51.820
You can look at the full set and see
if there's an alternative approach

00:10:52.200 --> 00:10:55.570
that you might be able to take
better advantage on SSC that you

00:10:55.600 --> 00:10:58.910
might not have done with AlteVec.

00:10:59.050 --> 00:11:02.830
And then we'll apply the Intel compiler
and see what the speedup is.

00:11:03.310 --> 00:11:10.810
So the first step was really just to go
into the project and flip the switch from

00:11:11.110 --> 00:11:14.990
PowerPC on the architecture type to I386.

00:11:15.060 --> 00:11:16.010
Try compiling it.

00:11:16.290 --> 00:11:17.390
You'll get a lot of errors.

00:11:17.480 --> 00:11:19.740
Wherever there's an
alt-devec instruction,

00:11:19.740 --> 00:11:21.210
you're certainly going to get an error.

00:11:21.440 --> 00:11:26.230
And go in there and actually weed
out that part or partition that

00:11:26.380 --> 00:11:31.740
part into conditional compilation
by using the standard defines

00:11:31.740 --> 00:11:34.600
that the compiler supports.

00:11:34.700 --> 00:11:40.950
So on alt-devec or on the PowerPC side,
the alt-defec define is automatically

00:11:40.990 --> 00:11:42.800
enabled by the compiler.

00:11:42.840 --> 00:11:47.690
On Intel compiler and
GCC running on I386,

00:11:47.760 --> 00:11:50.930
SSC and SSC2 are automatically defined.

00:11:51.220 --> 00:11:54.820
SS3 is turned off by default,
but you can easily turn it on.

00:11:54.940 --> 00:11:58.940
All of the Intel-based Macs support SS3.

00:11:58.980 --> 00:12:01.400
From the laptop on up.

00:12:01.750 --> 00:12:13.500
And so let's go ahead and just kind
of show what we actually did there.

00:12:13.520 --> 00:12:17.370
Let me take this back out of zoom mode.

00:12:28.200 --> 00:12:33.210
I have a lot of code that's in here,
but we'll go up to the first

00:12:33.210 --> 00:12:37.030
vectorized implementation.

00:12:37.900 --> 00:12:39.280
start at the beginning.

00:12:39.280 --> 00:12:45.740
Now one of the things that I wanted to
do as part of partitioning was to make

00:12:45.740 --> 00:12:51.660
sure that I kept as much common code
common between the two implementations.

00:12:51.660 --> 00:12:54.840
There's no sense just
partitioning the full function,

00:12:54.840 --> 00:12:59.070
because there's a lot of C code,
setup code at the beginning, prolog,

00:12:59.210 --> 00:13:03.310
epilog at the beginning that was common
that we didn't need to comment out,

00:13:03.310 --> 00:13:06.110
and if you made changes to one,
then you'd have to remember

00:13:06.110 --> 00:13:08.340
to make changes to the other.

00:13:08.420 --> 00:13:14.380
Here's the beginning of the function
for the scalar implementation.

00:13:14.380 --> 00:13:19.300
Let's go ahead and search through for...

00:13:20.040 --> 00:13:21.400
The vector.

00:13:21.490 --> 00:13:22.390
Okay.

00:13:22.540 --> 00:13:25.660
And so here's the vector implementation.

00:13:25.680 --> 00:13:32.310
We have some initial declarations, okay,
that we kept exactly the same.

00:13:33.040 --> 00:13:38.240
There's one instruction here that we had
to modify for initializing the vector.

00:13:38.250 --> 00:13:43.000
You'll notice that I'm actually using
the data type declared here using

00:13:43.000 --> 00:13:46.190
the Accelerate Framework data type.

00:13:46.360 --> 00:13:50.080
And so I decided to just partition
out these two into the implementation

00:13:50.190 --> 00:13:52.760
for initializing the variable.

00:13:52.780 --> 00:13:57.700
But for the rest of this function,
a lot of this setup code at the beginning

00:13:58.050 --> 00:14:00.960
is just straight standard C code.

00:14:02.020 --> 00:14:03.900
It's all within the function itself.

00:14:03.940 --> 00:14:10.180
Didn't have to do anything different
until we got to the alt-vect section.

00:14:10.440 --> 00:14:12.130
In the original code,
they had commented out

00:14:12.240 --> 00:14:14.410
the scalar implementation,
so you could see what that

00:14:14.470 --> 00:14:15.890
scalar loop would look like.

00:14:16.010 --> 00:14:19.780
But then immediately,
we start getting into these

00:14:19.780 --> 00:14:22.370
vect unsigned char declarations.

00:14:23.720 --> 00:14:29.540
A bunch of more declarations
and initialization.

00:14:29.540 --> 00:14:32.640
So I just went to the beginning of
where I saw all the errors to begin

00:14:32.790 --> 00:14:38.760
with for this routine and put an
ifdef around the full block of code.

00:14:38.990 --> 00:14:45.280
Cut and pasted that so I would
have a reference to go through.

00:14:45.360 --> 00:14:47.910
Let me scan down here.

00:14:49.200 --> 00:14:51.740
So this is all the AlteVec code.

00:14:51.770 --> 00:14:53.590
We get down there some debug.

00:14:53.750 --> 00:14:55.140
And now we get into SSC.

00:14:55.250 --> 00:14:59.460
So I ended up just cutting
and pasting the original code.

00:14:59.890 --> 00:15:04.660
So that I could use that as the template
to start my Alphabet implementation.

00:15:04.780 --> 00:15:07.410
So if we go back to the slides.

00:15:13.130 --> 00:15:16.090
So the next step after having done
that was to really start looking

00:15:16.190 --> 00:15:20.140
at the data types and modifying
them to fit the Intel architecture.

00:15:20.140 --> 00:15:25.360
There's three basic
intrinsic data types to SSC.

00:15:25.380 --> 00:15:28.770
That's 128-bit,
which is an integer-based.

00:15:28.940 --> 00:15:32.840
It doesn't matter whether
it's a vector of char,

00:15:32.840 --> 00:15:35.880
shorts, ints, longs.

00:15:35.880 --> 00:15:36.900
It's all the same.

00:15:36.990 --> 00:15:39.890
It's just a vector of integers,
and that's the way the

00:15:39.890 --> 00:15:41.660
instructions look at them.

00:15:41.690 --> 00:15:47.060
There is a M128,
which was the original SSC data type,

00:15:47.130 --> 00:15:51.240
which was a vector of four floats,
and then an M128d.

00:15:51.310 --> 00:15:58.090
So it's underscore, underscore,
M128d for a vector of doubles.

00:15:58.580 --> 00:16:01.800
Now, you could stick with those data
types and actually just replace the

00:16:01.800 --> 00:16:09.180
vector unsigned int or unsigned char
declarations that were in the AltaVec.

00:16:09.820 --> 00:16:12.380
But one of the problems that you
have if you use those particular

00:16:12.390 --> 00:16:15.650
data types is that you can lose track
of exactly what your variable type is

00:16:15.760 --> 00:16:17.790
and what you're actually operating on.

00:16:17.890 --> 00:16:21.870
So one of the recommendations is
to actually use the AlteVec--or

00:16:21.870 --> 00:16:24.550
the Accelerate framework
and their vector types,

00:16:24.550 --> 00:16:32.590
because they've already been type-deafed
for the Intel SSE data types.

00:16:32.680 --> 00:16:36.890
And it also helps support
the readability of the code.

00:16:37.970 --> 00:16:41.340
One of the other things that
you'll run into is the fact that

00:16:41.340 --> 00:16:43.370
the SSE instructions are C-style.

00:16:43.400 --> 00:16:45.940
That means that no matter
what variables you give,

00:16:45.940 --> 00:16:50.100
as long as they're M128s,
the functions--you have to specify

00:16:50.100 --> 00:16:53.890
exactly what the function is going
to operate on within that vector.

00:16:54.010 --> 00:16:58.750
For example, here,
this is a compare-equal extended

00:16:58.750 --> 00:17:01.470
packed integer of 8 bits.

00:17:01.580 --> 00:17:07.500
So it's no longer a C++ style where
you could just give it the data type,

00:17:07.500 --> 00:17:10.400
the arguments will be over--or
the functions will be overloaded,

00:17:10.400 --> 00:17:13.850
take those--the arguments to the
functions and know exactly which

00:17:13.850 --> 00:17:16.000
function to actually implement.

00:17:16.090 --> 00:17:19.900
So that was the next step,
was really just to go through

00:17:19.900 --> 00:17:21.800
and modify the data types.

00:17:26.600 --> 00:17:30.980
And so here you can see we
have vector-unsigned chars.

00:17:30.980 --> 00:17:35.840
We just went and basically
declared this table.

00:17:35.840 --> 00:17:40.440
In this particular case,
it was a table of M128Is.

00:17:40.440 --> 00:17:41.590
It was a lookup table.

00:17:41.680 --> 00:17:45.380
It didn't seem like it was necessary to
specify exactly what their data type.

00:17:45.520 --> 00:17:49.110
This really wasn't being used in
any of the particular calculations.

00:17:49.400 --> 00:17:53.400
But then I have a special
initializer that allows me to

00:17:53.420 --> 00:17:58.130
set all of the vector types,
all of the values within the vector

00:17:58.430 --> 00:18:04.660
to the same value using a set1,
extended, packed, integer 8 instruction.

00:18:04.680 --> 00:18:08.000
And that's the equivalent to
the initialization that we see

00:18:08.010 --> 00:18:10.050
with the vector-unsigned char.

00:18:12.060 --> 00:18:15.760
Did that for a couple of lookup tables.

00:18:15.850 --> 00:18:19.470
Here, there were a couple of
constants that were defined,

00:18:19.470 --> 00:18:22.580
which, because you would have to,
in all of the functions,

00:18:22.580 --> 00:18:25.960
actually cast them to M28s if
I defined them differently,

00:18:26.210 --> 00:18:29.130
I decided to also keep
these as M128s as well,

00:18:29.210 --> 00:18:31.640
just because I'm not writing to them.

00:18:31.640 --> 00:18:34.140
They're going to be constant throughout,
and I knew exactly how they

00:18:34.140 --> 00:18:36.600
were going to be operated on.

00:18:36.700 --> 00:18:38.360
There was no sense actually casting them.

00:18:40.460 --> 00:18:43.530
There were a couple of constants
that actually get generated

00:18:43.530 --> 00:18:46.670
on the fly in the AlteVec,
but in this case I used constants and

00:18:46.670 --> 00:18:51.990
declared them up front so that we would
load them from cache when we needed them.

00:18:52.360 --> 00:18:56.460
It was cheaper to load them from
cache than to actually try to

00:18:56.460 --> 00:18:58.760
generate the data on the fly.

00:19:00.680 --> 00:19:02.910
Keep going down through the routine.

00:19:03.100 --> 00:19:05.560
And now we start getting
into the inner loop.

00:19:05.630 --> 00:19:07.940
Okay,
and this is where I just took exactly

00:19:07.940 --> 00:19:10.200
what the AltaVec instruction said.

00:19:10.240 --> 00:19:15.720
If it said vector signed short,
I replaced it with the Accelerate

00:19:15.720 --> 00:19:19.920
Framework data type vector signed in 16.

00:19:20.080 --> 00:19:22.240
And in fact,
if you were actually implementing

00:19:22.240 --> 00:19:26.540
this between a PowerPC with
AlteVec and the Intel architecture,

00:19:26.540 --> 00:19:29.510
you could actually move these
declarations out such that

00:19:29.510 --> 00:19:33.080
you would have one set of
declarations for both architectures.

00:19:33.080 --> 00:19:39.040
You wouldn't need to separate them
out in the if-def between the two,

00:19:39.080 --> 00:19:41.210
AlteVec versus SSC.

00:19:41.320 --> 00:19:43.100
Make the code a little bit shorter.

00:19:43.180 --> 00:19:46.710
Puts all that stuff in a common area,
and you're not rewriting the code.

00:19:46.720 --> 00:19:50.910
or if you make a change in one,
you don't have to modify it in the other.

00:19:51.510 --> 00:19:54.440
And so that was, you know,
for all of the declarations,

00:19:54.440 --> 00:19:57.800
was very straightforward
to actually implement.

00:19:57.860 --> 00:20:02.420
There was just a few cases where
I decided to go ahead and use the

00:20:02.420 --> 00:20:07.740
M128 because of what it appeared
to be more applicable for.

00:20:08.060 --> 00:20:12.520
In this particular case,
this happened to be differences between

00:20:12.520 --> 00:20:15.700
the brain versus the old brain settings.

00:20:15.780 --> 00:20:19.680
And so it was actually a bit--a string of
bits that you actually compared against,

00:20:19.710 --> 00:20:23.010
and so I kept it as an M128i.

00:20:26.670 --> 00:20:30.880
Let's go ahead and switch back.

00:20:31.000 --> 00:20:33.750
So after we had the data types
all declared and transferred and

00:20:33.760 --> 00:20:37.170
got rid of all of those warnings,
then I looked at all of the

00:20:37.170 --> 00:20:40.300
instructions that were in the
AlteVec and did the transcribe from

00:20:40.310 --> 00:20:42.550
one instruction to the other one.

00:20:42.560 --> 00:20:43.910
It's just a one-for-one.

00:20:44.050 --> 00:20:45.640
There's a significant overlap.

00:20:45.890 --> 00:20:48.290
You'll find that the modulo, integer,
add,

00:20:48.350 --> 00:20:51.080
subtracts are fairly straightforward.

00:20:51.270 --> 00:20:53.680
Pretty much all of the
compare instructions,

00:20:53.770 --> 00:20:59.100
the packs, unpacks, integer,
floating point conversions are all there.

00:20:59.100 --> 00:21:02.190
Floating point operations,
pretty straightforward.

00:21:02.200 --> 00:21:06.400
There's a little bit less
limited in terms of the overlap

00:21:06.510 --> 00:21:09.020
in the min/max instructions.

00:21:09.020 --> 00:21:12.150
So if you look at it,
we're not quite as orthogonal in terms

00:21:12.160 --> 00:21:15.880
of the data types that we support.

00:21:15.880 --> 00:21:18.210
But in any case,
all of those were pretty much

00:21:18.210 --> 00:21:19.860
a one-to-one translation.

00:21:19.860 --> 00:21:24.720
Again, the SSE intrinsic instructions
have to be data type aware,

00:21:24.720 --> 00:21:27.930
so you actually have to understand
essentially what the data type

00:21:27.940 --> 00:21:31.080
that you're operating on to
the end of the instruction.

00:21:31.140 --> 00:21:33.800
So let's go ahead and look at a
couple of those instructions as well.

00:21:33.800 --> 00:21:35.590
We switch back.

00:21:41.430 --> 00:21:44.040
Okay,
so some of the first instructions that

00:21:44.040 --> 00:21:48.300
we actually run into are these Vector
Merge High and Vector Merge Lows.

00:21:48.630 --> 00:21:52.930
Again, we do the exact same thing,
only now instead of using

00:21:52.930 --> 00:21:55.920
the Merge High and Merge Low,
we're using the Unpack

00:21:56.050 --> 00:21:57.200
High and Unpack Low.

00:21:57.300 --> 00:22:00.300
I'm just casting the value
here because if I didn't,

00:22:00.300 --> 00:22:07.520
I would get a bunch of warnings
about vector signed int 16 values not

00:22:07.520 --> 00:22:10.300
knowing how to convert that to an M128.

00:22:10.300 --> 00:22:13.660
So I just go ahead and do it so
that if you actually--you know,

00:22:13.660 --> 00:22:15.290
in my code,
I like to get rid of all the warnings

00:22:15.290 --> 00:22:19.490
that I possibly can so that if this is
something that I know that I need to do,

00:22:19.500 --> 00:22:21.930
I just go ahead and do the casting.

00:22:22.160 --> 00:22:24.600
So we had a whole series of unpacks.

00:22:24.600 --> 00:22:26.700
It was very straightforward.

00:22:26.700 --> 00:22:28.640
We came down to the vector adds.

00:22:29.190 --> 00:22:29.600
Okay.

00:22:29.730 --> 00:22:32.600
Again, a series of two adds together.

00:22:32.640 --> 00:22:33.840
Exactly the same thing.

00:22:33.840 --> 00:22:36.000
We have the two adds together.

00:22:36.030 --> 00:22:40.680
One-to-one correspondence.

00:22:40.680 --> 00:22:40.680
Again.

00:22:41.150 --> 00:22:45.820
So it's a very straightforward
translation up to this point.

00:22:45.840 --> 00:22:48.650
Even these particular unpacks

00:22:50.010 --> 00:22:53.600
And these subtracts,
still again very straightforward,

00:22:53.620 --> 00:23:02.060
up until we get to the vec-multi or
the multiply-even and multiply-odd.

00:23:02.060 --> 00:23:06.330
So that's where we ran into the first
difference that was not really a

00:23:06.330 --> 00:23:10.600
one-to-one translation between the two.

00:23:16.020 --> 00:23:19.280
And so that's the first one where we hit
where it's not really a non-standard one.

00:23:19.280 --> 00:23:22.060
But the standard translations,
we also hit a little bit

00:23:22.060 --> 00:23:24.100
further down in the code,
which I'll show you.

00:23:24.270 --> 00:23:29.480
And one of them is the vector select,
which is essentially the

00:23:29.480 --> 00:23:33.000
implementation is here in AltaVec.

00:23:33.100 --> 00:23:38.610
It's just a straight vector select, A, B,
and C, for the C, B, and the mask,

00:23:38.680 --> 00:23:41.140
on which you want to select
out of the two registers,

00:23:41.140 --> 00:23:43.200
and you're just picking
the bits between each one.

00:23:43.700 --> 00:23:49.420
The implementation here for SSE is
just doing an and not and an and,

00:23:49.420 --> 00:23:51.460
and then or-ing those values together.

00:23:51.460 --> 00:24:00.280
A great resource for migrating AltaVec
to SSE is actually in Apple's own

00:24:00.280 --> 00:24:02.360
documentation on this translation.

00:24:02.360 --> 00:24:06.580
They have a lot of these very
standard types of translations

00:24:06.580 --> 00:24:09.200
between AltaVec and SSE.

00:24:14.360 --> 00:24:17.840
So here's basically a graphical display.

00:24:18.060 --> 00:24:23.950
You see that the C variable
has some 1s and 0s.

00:24:24.010 --> 00:24:26.000
Those are just the selector values.

00:24:26.090 --> 00:24:28.540
In this particular case,
we just represent them as 0, 1,

00:24:28.540 --> 00:24:32.020
but they would actually be
8-bit values or 16-bit values,

00:24:32.030 --> 00:24:34.450
depending on the input.

00:24:34.530 --> 00:24:39.770
And here we have the same
implementation in SSE.

00:24:39.950 --> 00:24:43.080
We're doing the AND, the AND NOT,
and then OR-ing the two to

00:24:43.530 --> 00:24:49.800
get basically the same result
that you get from the SELECT.

00:24:49.800 --> 00:24:53.800
So let me go ahead and just show
you that instruction real quick.

00:24:53.800 --> 00:25:02.130
Down here.

00:25:08.900 --> 00:25:11.780
Okay,
so here's the VEX Select instruction,

00:25:11.820 --> 00:25:13.280
taking the three arguments.

00:25:13.410 --> 00:25:18.170
Again, we have our AND NOT with
our AND instruction.

00:25:18.740 --> 00:25:20.100
And then we're ORing them together.

00:25:20.280 --> 00:25:25.340
So that's just a, you know,
straight recipe formula implementation

00:25:25.340 --> 00:25:27.970
for that particular instruction.

00:25:29.230 --> 00:25:32.320
But now I'm going to go back to
the non-standard implementation,

00:25:32.320 --> 00:25:42.600
which is using the mole-high and
the mole-even and the mole-odd.

00:25:45.780 --> 00:25:47.820
Which is right here.

00:25:47.820 --> 00:25:50.400
Okay.

00:25:50.460 --> 00:25:53.820
So let's go back to the slides again.

00:25:59.520 --> 00:26:03.120
So this is really the hardest
part of the non-standard part

00:26:03.120 --> 00:26:06.020
of translating AltaVec to SSE.

00:26:06.050 --> 00:26:08.760
There are some integer
multiplications that have limited

00:26:08.760 --> 00:26:09.960
support for the data type.

00:26:10.040 --> 00:26:12.240
So you have to look at
what your precisions are.

00:26:12.240 --> 00:26:16.770
You may have to scale to support the
precision that you're looking for.

00:26:16.830 --> 00:26:20.990
The PERMUTE instruction is a very
significant instruction that's

00:26:20.990 --> 00:26:26.620
supported on AltaVec that is not
currently supported on Intel processors.

00:26:26.930 --> 00:26:31.480
And you really have to implement a
series of packs and unpacks and shuffles

00:26:31.480 --> 00:26:35.560
in order to be able to get basically
the same equivalent implementation

00:26:35.560 --> 00:26:37.840
to the PERMUTE instruction.

00:26:37.870 --> 00:26:40.640
Then there's some also
miscellaneous instructions that

00:26:40.640 --> 00:26:46.110
are non-standard between the two
that make it a little bit tougher.

00:26:46.120 --> 00:26:48.410
And that's the hard part where you
have to actually think about the

00:26:48.410 --> 00:26:50.240
algorithm that you might implement.

00:26:50.290 --> 00:26:54.180
And then there's also some unique
instructions that you might take

00:26:54.520 --> 00:26:58.870
advantage of for Intel that you might
not actually have on the AltaVec that'll

00:26:58.940 --> 00:27:01.000
get you some improvement as well.

00:27:01.000 --> 00:27:05.110
So here's the MUL-E and the MUL-ODD,
or even and odd.

00:27:05.120 --> 00:27:08.700
And essentially,
they multiply the two vectors.

00:27:08.700 --> 00:27:11.880
In the case of the even,
you take the even

00:27:12.390 --> 00:27:18.300
vector input and you come up in this
case it's a 16-bit implementation you

00:27:18.300 --> 00:27:23.460
end up with a multiplication of the
two 16-bit values for a 32-bit value

00:27:23.510 --> 00:27:29.660
and you get them in an all odds or all
even vector and this is very different

00:27:29.660 --> 00:27:34.990
from the way the intel style works in
that we tend to always do everything

00:27:34.990 --> 00:27:40.340
in order so you get all of the vectors
zero through seven if you happen to

00:27:40.340 --> 00:27:45.210
be doing this particular operation
instead of the even and odds but we

00:27:45.210 --> 00:27:48.060
take it in the upper half and lower half

00:27:51.370 --> 00:27:58.980
So here's the MOL low.

00:27:59.000 --> 00:28:01.730
And this is a case where we
actually do the same multiplication,

00:28:01.730 --> 00:28:04.400
but we take the lower 16 bits.

00:28:04.440 --> 00:28:06.330
And we put that in a vector.

00:28:06.600 --> 00:28:10.390
And if we do that to the high,
then we get the upper

00:28:10.390 --> 00:28:12.640
16 bits in the high.

00:28:12.830 --> 00:28:16.270
If we unpack those together,
then we get the 32-bit values.

00:28:16.340 --> 00:28:18.800
So we do an unpack
high and an unpack low,

00:28:18.820 --> 00:28:20.280
and we can get the 32-bit values.

00:28:20.400 --> 00:28:24.060
But they're still in the same
vector order that they originally

00:28:24.080 --> 00:28:25.900
were in the original vectors.

00:28:25.960 --> 00:28:32.480
So we're still going from, in this case,
0 to 4, or 0 to 3, and then from 4 to 7

00:28:32.490 --> 00:28:36.510
in terms of the vector,
which is very different

00:28:36.510 --> 00:28:39.050
than the MOL even and odd.

00:28:39.150 --> 00:28:43.580
So let's go ahead and
switch back to the code.

00:28:45.050 --> 00:28:52.390
So here is a case where we implemented
the mole high and mole low,

00:28:52.390 --> 00:28:56.530
and the high unpack and the low unpack,
and this basically gives

00:28:56.530 --> 00:28:58.060
us our 32-bit values.

00:28:58.080 --> 00:29:00.670
If we wanted to model
the AltaVec exactly,

00:29:00.760 --> 00:29:04.130
one-for-one instruction,
we would have to do a further

00:29:04.130 --> 00:29:07.030
unpack of these instructions
in order to get all the evens

00:29:07.030 --> 00:29:08.800
together and all the odds together.

00:29:08.800 --> 00:29:10.970
But when we actually
looked at the algorithm,

00:29:11.030 --> 00:29:13.460
it turned out that further
down in the algorithm,

00:29:14.540 --> 00:29:20.070
after they did a bunch of calculations,
they ended up doing that same

00:29:20.070 --> 00:29:24.160
packing and unpacking in the result.

00:29:24.200 --> 00:29:27.840
So right here,
they ended up doing the merge at the

00:29:27.880 --> 00:29:33.580
end between the high and the low to get
them back into vector order from the

00:29:33.580 --> 00:29:37.400
original straight linear vector sequence.

00:29:37.400 --> 00:29:43.010
So we decided that instead of
trying to mirror the odd and even,

00:29:43.010 --> 00:29:47.280
we would go ahead and just
leave them in our particular

00:29:47.280 --> 00:29:49.500
format and wait to the end.

00:29:49.500 --> 00:29:52.250
And then if we still needed to do that,
we would do that at the end,

00:29:52.250 --> 00:29:55.240
and it turned out that we actually
could avoid a couple of instructions

00:29:55.250 --> 00:29:56.390
by not having to do that.

00:30:01.200 --> 00:30:06.900
[Transcript missing]

00:30:13.680 --> 00:30:17.660
So that was pretty much
the full implementation.

00:30:18.020 --> 00:30:21.960
It actually vectorized very
well following the standard

00:30:21.960 --> 00:30:24.970
implementation that we had for AlteVec.

00:30:25.130 --> 00:30:27.190
But there were some additional
things that we could do.

00:30:27.330 --> 00:30:30.440
One was we didn't have to do
the additional unpacking on

00:30:30.480 --> 00:30:33.600
the unpacks in order to get it
into the even and odd order,

00:30:33.600 --> 00:30:37.250
then to further have to go
back and do packs to get it

00:30:37.250 --> 00:30:39.590
back into the normal sequence.

00:30:39.910 --> 00:30:45.090
And so we could take advantage of
that SSC feature and avoid having to

00:30:45.090 --> 00:30:48.600
do that extra packing and unpacking.

00:30:49.080 --> 00:30:53.870
We also looked at re-evaluating
the precision requirements.

00:30:53.900 --> 00:30:57.240
A lot of this stuff is done in 16-bit,
goes back to 32-bit.

00:30:57.240 --> 00:31:00.040
It's possible that maybe
in this particular case the

00:31:00.050 --> 00:31:02.300
precision isn't quite necessary.

00:31:02.300 --> 00:31:03.890
You could actually keep it all in 16-bit.

00:31:04.000 --> 00:31:09.100
But to be compatible with
the implementation as it was,

00:31:09.100 --> 00:31:14.380
and to be fair in terms of the
comparison between the two,

00:31:14.380 --> 00:31:14.530
we decided to keep it
at 32-bit precision.

00:31:15.330 --> 00:31:18.320
There are a couple other things that you
have to be aware of that we looked at.

00:31:18.530 --> 00:31:21.780
Actually, in this particular case was
looked at memory alignment,

00:31:21.830 --> 00:31:25.590
watch for cache line splits,
because when you're loading

00:31:25.650 --> 00:31:28.310
data and storing data,
you have to be careful of that

00:31:28.320 --> 00:31:30.040
on the Intel architecture.

00:31:30.150 --> 00:31:32.830
There was no floating point,
but if you were doing floating point,

00:31:32.840 --> 00:31:38.420
you would want to make sure that you
looked at or had caution for denormals.

00:31:38.560 --> 00:31:42.390
On the Intel architecture,
denormals are enabled by default.

00:31:42.500 --> 00:31:46.800
On PowerPC, they are disabled.

00:31:46.920 --> 00:31:48.860
And then we looked at
partial register stalls.

00:31:48.860 --> 00:31:51.270
It turned out there was no
partial register stalls in

00:31:51.270 --> 00:31:53.700
this particular application.

00:31:53.820 --> 00:31:58.170
And then I already showed you that
we actually generated--instead of

00:31:58.170 --> 00:32:00.960
generating constants on the fly,
we actually declared them and

00:32:00.960 --> 00:32:03.150
initialized them up front.

00:32:03.280 --> 00:32:05.960
But there was an additional instruction
that we could take advantage of that,

00:32:06.030 --> 00:32:09.900
once we looked at the algorithm,
to actually improve the performance

00:32:09.900 --> 00:32:11.970
of this particular sample app.

00:32:12.050 --> 00:32:15.850
And that was the packed fused
multiply add instruction.

00:32:15.920 --> 00:32:21.810
And what that does is basically
takes the two 16-bit values,

00:32:22.130 --> 00:32:28.320
multiplies them to get a 32-bit result,
and then adds it to its neighbor.

00:32:28.390 --> 00:32:32.970
And it turned out that's
exactly what this application

00:32:32.970 --> 00:32:35.450
was doing towards the end.

00:32:35.840 --> 00:32:42.410
So let me go ahead and show you
that particular piece of code.

00:32:43.470 --> 00:32:51.870
So here, we did all our multiply, even,
and odds--in our case, high and lows.

00:32:52.050 --> 00:32:56.390
But down here, we had this addition.

00:32:57.500 --> 00:33:03.190
Okay, that happened to take the
result of that multiplication.

00:33:04.840 --> 00:33:06.290
For the two.

00:33:06.330 --> 00:33:09.600
And so what we decided to do was
instead of going through all of

00:33:09.690 --> 00:33:14.460
these multiplies and unpacks,
we were able to combine that

00:33:14.460 --> 00:33:17.820
into the multiply-add--the
fuse multiply-add instruction.

00:33:17.960 --> 00:33:20.700
We were able to get rid of the unpacks.

00:33:20.780 --> 00:33:28.860
Bring it down here to the implementation.

00:33:28.920 --> 00:33:32.210
We're actually in the
optimized version now.

00:33:41.070 --> 00:33:45.180
So here's the equivalent that
we were able to do instead.

00:33:45.180 --> 00:33:51.740
Okay, so we did a couple of unpacks,
but now we did the multiply and the add,

00:33:51.760 --> 00:33:54.360
and we got the 32-bit
value that we wanted,

00:33:54.360 --> 00:33:58.050
plus we were able to complete
the addition that we needed to do

00:33:58.170 --> 00:34:01.090
in the instruction further down.

00:34:01.580 --> 00:34:07.300
So we replaced all of those mole-highs
and mole-lows with the multiply-add

00:34:07.310 --> 00:34:12.030
on all four implementations here.

00:34:12.430 --> 00:34:18.060
And then down here at the bottom,
we no longer had to do the addition.

00:34:18.080 --> 00:34:20.330
All we're doing is the
subtraction that was actually

00:34:20.330 --> 00:34:22.910
implemented in the original code,
which is up here.

00:34:22.990 --> 00:34:29.000
So originally,
we had this shift-right arithmetic,

00:34:29.260 --> 00:34:30.880
the subtraction, and the add.

00:34:31.110 --> 00:34:35.110
Now we just had the shift
and the subtraction.

00:34:35.440 --> 00:34:38.530
We've already done the addition
in the multiplication part,

00:34:38.530 --> 00:34:40.460
in the multiply-add instruction.

00:34:40.460 --> 00:34:43.610
And it gave it to us in 32-bit values.

00:34:44.480 --> 00:34:47.400
Once we made that optimization,
then we were doing pretty well

00:34:47.400 --> 00:34:51.310
in terms of the performance.

00:34:54.900 --> 00:34:59.340
Let me go ahead and switch back.

00:34:59.370 --> 00:35:02.140
And so I'm going to
bring up Justin Landon,

00:35:02.140 --> 00:35:04.940
my colleague,
who will go through the Intel compiler.

00:35:05.180 --> 00:35:08.860
He'll talk about the actual
performance that we got for

00:35:08.860 --> 00:35:11.530
this implementation of Noble 8s.

00:35:18.420 --> 00:35:20.280
Thank you, Phil.

00:35:20.410 --> 00:35:23.280
Now that Phil's done all the
hard work of translating this

00:35:23.310 --> 00:35:26.800
application over to use SSE,
let's take a look at what the

00:35:26.800 --> 00:35:29.560
performance of the application is.

00:35:30.500 --> 00:35:34.310
This is Noble Ape,
and this was built with a GCC compiler

00:35:34.560 --> 00:35:36.640
that ships with Xcode 2.3.

00:35:36.730 --> 00:35:38.250
And as you can see,
we have three different

00:35:38.250 --> 00:35:39.190
computation modes.

00:35:39.270 --> 00:35:43.290
We have a Scalar, a Vector,
and a Vector Optimize.

00:35:43.390 --> 00:35:50.100
What we're comparing here is G5 iMac
versus the 2.0 GHz Intel Core Duo iMac.

00:35:50.230 --> 00:35:54.430
And we basically have the three-- Scalar,
Vector, and Vector Optimize.

00:35:54.450 --> 00:35:57.980
And these are the ape thoughts per second
that we're seeing on each platform.

00:35:58.070 --> 00:36:00.070
As you can see,
we're doing quite well on the Scalar.

00:36:00.230 --> 00:36:03.030
We're doing pretty good with the Vector,
which is just basically

00:36:03.350 --> 00:36:06.000
a Vector-optimized-- or
not an optimized version,

00:36:06.000 --> 00:36:09.400
but just a scale-- a Vector
implementation of the Scalar version.

00:36:09.520 --> 00:36:10.900
And then we have the
Vector-optimized version,

00:36:10.900 --> 00:36:13.320
which, you know,
we take math tricks and try to

00:36:13.360 --> 00:36:15.300
optimize as much as possible.

00:36:15.380 --> 00:36:19.050
As you can see, we're not doing so great
on the last case there.

00:36:19.080 --> 00:36:19.840
But fear not.

00:36:20.020 --> 00:36:22.990
Before we take a look at the algorithm
and try to change things around,

00:36:23.000 --> 00:36:25.470
we'll try to use the
Intel compiler to see if that

00:36:25.470 --> 00:36:28.160
can improve performance for us.

00:36:28.160 --> 00:36:29.920
So if we can switch over
to the demo machine.

00:36:29.920 --> 00:36:31.580
machine.

00:36:36.690 --> 00:36:39.910
So what I'm going to show here
is the ease in which you can use

00:36:39.940 --> 00:36:43.420
the Intel compiler to recompile
your application and try to obtain

00:36:43.420 --> 00:36:45.470
additional performance gains.

00:36:45.480 --> 00:36:48.820
This is as simple as going to the target.

00:36:50.020 --> 00:36:52.800
Going to the Rules section here.

00:36:52.870 --> 00:36:56.900
And under the System C rule,
we can change the compiler.

00:36:56.910 --> 00:36:59.590
We can just change it
to the Intel compiler.

00:37:03.460 --> 00:37:07.080
It will ask me to make a copy,
and I can change it again here.

00:37:07.180 --> 00:37:08.600
And now we're all set there.

00:37:08.650 --> 00:37:11.700
The other thing that we need to take
a look at is we want to make sure that

00:37:11.700 --> 00:37:15.870
we evoke certain optimization settings.

00:37:16.570 --> 00:37:19.690
So I'm going to go
ahead and change those.

00:37:19.780 --> 00:37:22.000
And you can see right now,
now that we've had the

00:37:22.000 --> 00:37:24.570
Intel compiler selected,
we have the ability to set some

00:37:24.570 --> 00:37:27.570
switches within the Intel compiler.

00:37:28.680 --> 00:37:31.200
The biggest ones that we want to take
a look at here is we want to make sure

00:37:31.200 --> 00:37:34.240
the optimization level is set to O3.

00:37:34.590 --> 00:37:38.250
We also want to make sure that we have
inline function expansion turned on,

00:37:38.260 --> 00:37:40.530
and OB2 is the highest setting for that.

00:37:40.690 --> 00:37:45.880
And we also want to have the IP flag,
and this is a checkbox right here,

00:37:45.950 --> 00:37:50.610
and this allows function
expansion within single files.

00:37:50.840 --> 00:37:52.080
And that's all we're going to do.

00:37:52.140 --> 00:37:54.510
So I'm going to go ahead and close that.

00:37:54.510 --> 00:37:58.370
And I'm going to clean and build.

00:38:07.660 --> 00:38:09.600
So we're going to go ahead and
let that build for a second.

00:38:09.600 --> 00:38:16.940
And as you can see on the bottom,
you can tell that the Intel compiler

00:38:16.940 --> 00:38:20.820
is being used because it says
"compile ICC." So you'll know that the

00:38:21.000 --> 00:38:22.910
compiler is actually being invoked.

00:38:23.050 --> 00:38:27.100
Now, if you're doing a universal build,
what will happen is the Intel part

00:38:27.100 --> 00:38:31.620
of it will build the I native code,
and then it will use GCC to

00:38:31.620 --> 00:38:33.830
build the PowerPC code.

00:38:35.850 --> 00:38:40.460
So I'm going to go ahead and switch
the Vector Optimize mode here.

00:38:40.480 --> 00:38:42.150
And as you can see,
we've gotten a substantial

00:38:42.150 --> 00:38:44.390
speed-up just from doing that.

00:38:44.540 --> 00:38:46.830
This is on the MacBook Pro laptop.

00:38:47.020 --> 00:38:50.190
So if we switch back to the slide set,

00:38:53.890 --> 00:38:59.000
This is now what we're seeing as
far as GCC versus ICC performance.

00:38:59.110 --> 00:39:01.100
So as you can see in the
vector-optimized case,

00:39:01.130 --> 00:39:03.740
we went somewhere around
8,800 apethots per second,

00:39:03.770 --> 00:39:07.300
and we've increased that to
almost 12,000 apethots per second.

00:39:07.390 --> 00:39:11.450
So that's given us a 34% performance
gain just by recompilation alone.

00:39:11.530 --> 00:39:15.490
So there was no extra engineering
requirements or time investment

00:39:15.490 --> 00:39:18.820
needed in order to get us
that extra performance gain.

00:39:18.930 --> 00:39:22.370
We've also gotten a substantial
increase on the scalar and

00:39:22.420 --> 00:39:24.570
vector portions as well.

00:39:24.690 --> 00:39:28.140
So this is a pretty good case
for using the Intel compiler.

00:39:28.140 --> 00:39:31.400
It's easy to use,
and you can get substantial

00:39:31.400 --> 00:39:33.240
performance gains.

00:39:33.290 --> 00:39:37.460
I'm going to go back to this slide
so we can take a look at how we're

00:39:37.480 --> 00:39:39.050
going to look at the G5 comparison.

00:39:39.140 --> 00:39:42.920
So now if we're comparing against
the previous iMac model using the G5,

00:39:42.950 --> 00:39:46.440
we're about 19% better on
the vector-optimized case.

00:39:46.570 --> 00:39:51.160
So this is indeed showing that
we can translate the code over

00:39:51.190 --> 00:39:54.560
to use our SC instruction set,
and at the same time get

00:39:54.560 --> 00:39:56.580
better performing code.

00:39:58.640 --> 00:40:01.330
I'm going to go back to the demo
and show you one more optimization

00:40:01.330 --> 00:40:04.100
switch that can easily be thrown.

00:40:07.970 --> 00:40:12.950
So if we go back to the target
here and make sure that we have

00:40:12.950 --> 00:40:15.060
the Intel compiler selected,

00:40:15.800 --> 00:40:19.320
If we scroll all the way to
the bottom--looks like it's

00:40:19.320 --> 00:40:23.640
already been set--this additional
flag that you can set is IPO.

00:40:23.640 --> 00:40:26.800
And this is basically set in
the additional C options flag.

00:40:26.820 --> 00:40:32.080
And what this does is it allows inline
expansion within multiple files.

00:40:32.200 --> 00:40:35.840
So this will give you
increased performance.

00:40:35.930 --> 00:40:41.540
So since I already built it with that,
there won't be really, really need,

00:40:41.650 --> 00:40:44.110
but we'll do a clean anyways.

00:40:47.600 --> 00:40:53.990
While it's building,
we'll go back to the slide deck here.

00:40:57.800 --> 00:41:00.400
So now that I've thrown the IPO switch,
this is what we were

00:41:00.400 --> 00:41:02.100
seeing on the 2 GHz iMac.

00:41:02.230 --> 00:41:05.050
So as you can see,
I have the three columns here,

00:41:05.240 --> 00:41:06.830
the first one being GCC.

00:41:07.030 --> 00:41:12.400
The second column there is what
we just compiled with the O3 and

00:41:12.580 --> 00:41:14.760
OB2 switch and the IP switch.

00:41:14.830 --> 00:41:17.300
And then the last one
is where we have the O3,

00:41:17.370 --> 00:41:18.350
OB2, and IPO.

00:41:18.530 --> 00:41:22.010
And as you can see,
we've gained an additional 6%

00:41:22.010 --> 00:41:24.480
just from throwing that switch.

00:41:24.590 --> 00:41:29.190
So that really is a really simple way to
get additional performance within your

00:41:29.190 --> 00:41:31.080
application with minimal time investment.

00:41:34.570 --> 00:41:39.410
Here again, we're comparing against the
previous iMac model using the G5.

00:41:39.410 --> 00:41:41.720
And as you can see, now we're 24% better.

00:41:41.870 --> 00:41:45.530
So this is a clear indication that,
you know, with a little bit of time

00:41:45.570 --> 00:41:49.010
and using the Intel tools,
you can really get good performance

00:41:49.010 --> 00:41:51.070
gains out of your applications.

00:41:54.740 --> 00:41:58.460
So, in summary, you basically want to
use care in translating.

00:41:58.670 --> 00:42:00.680
You don't want to really
translate verbatim.

00:42:00.750 --> 00:42:03.790
Going one-to-one will work sometimes,
but you want to take some thought

00:42:03.790 --> 00:42:07.500
and care into how you're exactly
translating your application over

00:42:07.540 --> 00:42:09.960
to use the Intel instruction sets.

00:42:09.990 --> 00:42:12.160
You may want to re-architect the data.

00:42:12.180 --> 00:42:15.100
You want to try to avoid using the
permute since we don't have a direct

00:42:15.110 --> 00:42:17.310
analogous instruction that uses that.

00:42:17.550 --> 00:42:22.130
And sometimes just reorienting the data
to use our instruction set can save

00:42:22.130 --> 00:42:24.570
you some overhead of data shuffling.

00:42:24.920 --> 00:42:27.810
We highly recommend using the
ADC Reference Guide that's available

00:42:27.810 --> 00:42:29.040
online on the Apple website.

00:42:29.250 --> 00:42:33.230
That's very helpful,
and they have some good tips and tricks

00:42:33.240 --> 00:42:35.420
in there to help you translate your code.

00:42:35.670 --> 00:42:37.650
And utilize the Intel compiler.

00:42:37.870 --> 00:42:40.100
It's really easy to use.

00:42:40.320 --> 00:42:41.980
You let the compiler do the work,
and there's no time

00:42:41.980 --> 00:42:42.920
investment on your part.