---
Title:  IT State of the Union
Year:   2006
Web:    https://nonstrict.eu/wwdcindex/wwdc2006/500/

[!] This transcript was generated using Whisper, it has known transcription errors.
---

So, Apple product strategy for business. And as I mentioned, we've been getting a little bit more attention lately around our enterprise use of our products. This is from Federal Computer Week. This is, when you think about Apple computer and Macintosh systems, you probably don't think about servers, but think again.

And this increased attention is partly, and honestly, I think due to the fact that we've transitioned to Intel, and for whatever reason, I think there are people who feel maybe more comfortable in the IT space with Intel. But really, I think what's driving this is that we have a complete platform that is tuned for business. Starting from Mac OS X, which is of course Unix-based, fits in very well in the server room.

As well as on the desktop. On the desktop, we are really the only environment that combines the power of Unix, plus the ability to run Microsoft productivity applications on that very same desktop. A very nice suite of pro applications for companies that either have it as their business to manage media, or as more and more is true, it's part of your business to manage media. And the Apple applications for managing media are really second to none.

Servers and storage, of course, since we've introduced our One-U Server product, and now as you've seen, are transitioning that to a Quad Intel product. That is going to be, I think, of increasing interest to people who are looking for that, you know, not just a nicely designed, nice appearing product, but a product that actually fits into the server room and gives you the price performance that really drives everything there. Especially when you start looking at heating issues. And you saw some of the heating information earlier today. We're going to go a little bit deeper into that.

Service, of course, Apple provides world class service for our products. Sales, pro sales service and support that really is tuned for businesses and what businesses need. And then finally, none of this works without the software and the software produced by the people who attend this conference. We're at 22,000 titles and growing. In addition, of course, we have a great number of other software products in the world. We have a lot of software products.

We have a lot of software products. We have a large number of IT solution partners, many of whom are represented here at this conference. People like Oracle, SAP, TIBCO, Cisco System. Some nice point solutions, for example, Backbone for enterprise level backup of data. A lot of partners that were helping us bring the Mac into the enterprise.

[Transcript missing]

So talk a bit now about Apple's established markets. And as we move out into the IT space, there are some markets where we've been present all along and actually provide great examples of how IT challenges have been met and met with Apple solutions. And we're going to have some examples of that today.

Examples of our established markets: education, design, pro audio and video, science and technology, government. Some of these are enterprise level deployments. For example, education has some of the largest institutional deployments around that would give any Fortune 500 deployment a run for their money. And of course, government tends to be large as well. I need to say no more.

Design, pro audio and video, science and technology. Both examples of smaller medium-sized businesses, but also are oftentimes a portion of a large enterprise. So in pharma, you have the science and technology desktops where they do these days genetic engineering. Or in many Fortune 500 companies, you have the IT shops.

First experience with Macs was in the audio and video in the creative services in that shop. And they had to make those fit in. And so we're going to have some examples of how we can fit in with the rest of the corporate IT infrastructure. So the first one we're going to talk about is Apple on campus. And as I mentioned, the campus IT administrator has all of the same issues that any IT administrator has, plus some.

And they are dealing with the mission of educating their students, not just making money. They have to be on the leading edge in order to take advantage of the technology that's out there. The kinds of products we produce make creation by the students and the teachers very simple. Even things like iLife add to the ability to create for the students.

Broad distribution, campuses are highly networked. They need to get their content that's created, whether it's courseware or student work, broadly distributed on the campus. So that means XSERVs and XSERV raids. And then finally, mobility. And this is an area where education is a big part of our lives.

Education is a leading indicator. Education was the first place we saw that laptop volume was going to surpass, and it already has by a large margin, desktop volume. And this will hit the rest of our world as well, and is in the process of hitting the rest of our world.

But even beyond that, hypermobility with the iPod. And you find large numbers of students and teachers innovating with how to integrate the MacBook, the MacBook Pro, and the iPod for a complete learning environment. So here to tell us a little bit about that is I'd like to introduce Anton C. Harfman, who is Associate Dean for Technology and Facilities at the College of Design, Architecture, Art, and Planning at University of Cincinnati. So welcome. Thank you, Bud.

Thank you so much, Bud. I really appreciate the honor. It's such an honor to present here. And let me start off by apologizing for the tie. The College of Design, Architecture, Art, and Planning is four schools with a total of 13 programs. The College of Design, the College of Architecture and Interior Design, the School of Art, and the School of Planning. I'm sorry, those are all schools. It must be the tie. This is a quick tour of our building as well. As you can see, there are no right angles. If you're drunk, I'm told the building does look straight.

It's the first building on the University's master plan. We have 2,400 students in our college with 110 full-time faculty and 30 plus staff. The University of Cincinnati itself is a Research I institution with 35,000 students. It's a very design-rich environment. We have a really broad collection of contemporary signature architecture on campus that makes this a really incredible environment. This has all recently been done in the last 10 years.

The University of Cincinnati has a history of Apple at our college. The history begins in 1999 when we were unable to keep up with the pace of technology and we couldn't afford to maintain all the labs, so we asked our students to bring their own personal computers, which they did. They brought every variety of, so you can imagine, Gateway, Heathkit, and so on. Only four students in our program in architecture chose to bring an Apple.

Huge monitors sitting on these drafting tables made it impossible to build a computer. It was impossible to build conventional study models or do large drawings. And then furthermore, the studio faculty, of which I was one, were spending all of our time solving IT and network issues and no time teaching design.

Well, I marched right into the chair's office and said, "We're either going to drop computers from the curriculum or we're going to move to a laptop environment, and we're going to do it with one platform only." It was a hard sell at first, but as I made the argument, they seemed to go along. Architecture, graphic, and digital design all went that way, and that was a good chunk of our student population in the college. And all first-year students in those programs came with PowerBooks. The others in the college came with other stuff.

The argument was pretty easy to make. They're minimal IT. We didn't want to invest in IT in the school. We'd rather have another faculty member. They were very intuitive and very powerful and graphically interesting to use. And moreover, they were well-designed. If we're a college of design and we're interested and curious about design, we ought to be using well-designed products as well. Practice what you preach, in other words.

The first wave of this laptop tsunami, of all of these laptops arriving on our doorstep, forced us to change some things critically right away. The administrative structure just wasn't working. We had these silos with individuals reporting to other individuals, and when somebody in their area wasn't working, they were picking their nose. We immediately moved to a horizontal structure, made it more team-like, and fit a little bit better with the academic model.

Physically, we were able to reduce our labs one per year. We moved from six mediocre labs to three high-end labs. The third one I didn't show a picture of because it's the other one. So these are two of our high-end labs. The other thing that's interesting with this many laptops in the building is any classroom is by default now a computer lab. The students all have really pretty good computers, and wherever we are, we can make this a lab. We teach a lot of classes that way. In architecture, it allowed us to get smaller desks because no longer are they doing these huge drawings.

They're doing all of their stuff in Photoshop or Illustrator on their small laptops and printing out really huge plots and pinning those up. So we could get by with smaller desks, increase enrollment, more revenue. Yay. So the curricular changes that we found necessary to make was to teach IT fundamentals across the board.

And we teach it across the college at the freshman level as well. So in our first year, all 520 of our freshmen take the freshman class. They all sit in this room. It's quite amusing. It cuts across all 13 disciplines, and they learn basic things for the full year.

They're learning things like basic fundamental graphic skills, 3D motion, and web applications. This obviously is not a first-year project, but this student was once a first-year student. Other curricular changes. In architecture, we teach this immersion studio. We force 18 credits, totally integrated content. So the only way to manage all of this was we forced our students to subscribe to iCale so that they could tell what's going on every week or every day or every hour for that matter. As you can see, that second week is pretty intense. The second wave of the tsunami allowed us to make some critical changes in the infrastructure and the virtual space.

In the infrastructure, we did an evaluation and decided to support all these laptops. It would be advantageous to move to XServe and XServe Raids. So we chose them because of the hardware monitoring, the headless installation and management, as well as the swappability of the hard drives and the flexibility that gave us and the availability. The XServe X Raids, we have currently 22 terabytes of available storage.

We give each of our students 4 gigabytes of storage. The video students, it's basically on demand. Most of them get about 100 or so. Some of them get about 200 or 200 gigabytes. They're very powerful and very easy to set up. Furthermore, they're managed by one IT person for all 1,200 students in the building as well as all the Macs in the building. He's sitting right over here. His name is Pete Akins. If you have questions for him.

At the IT reception, he'll be there to answer all the technical questions since I have no clue what I'm doing. This has allowed us also to develop a portal and management infrastructure as well. We use out-of-the-box solutions like Samba, OpenLDAP, Kerberos, and so on. And most importantly, we use an alter Apache, MySQL, and PHP which come native on the machines. And we use those basically to develop our own intranet.

We are not really totally satisfied with the university's purchase of the web portal for the student body at large, so we've been developing our own home folder and web management and course management system. We call this DAP space. DAP is the acronym for Design, Architecture, Art, and Planning.

We have our own discussion and bulletin board and wiki-based web content management and a whole bunch of other features to integrate other databases like facility asset enrollment management as well. And we custom develop these things as we go and as we need a new tool. Culturally, as you can imagine, this has had a huge impact on our college. Seventy percent of our students come with iPods.

100% of them, though, have laptops, so they're all podcasting, so it's just a question of mobility. But they all have laptops and they're all downloading podcasts of lectures and music and so on. Wireless, sort of. I say that because there's a slew of people on the side here who are all plugged in. We have the same plague. Even though we can download anywhere in the building, you can't recharge anywhere in the building. While we're speaking, the electricians are adding about 100 outlets to the public spaces of our building so that the students with their laptops can recharge.

And I'd like to end with this last two slides here. If you draw your eye to the right side, upper right-hand corner, you'll see one of our newest students. His name's Ethan, and he is a sculptor that resides as the concierge in that great hall in our building.

And the interesting story here is that the dean, when he entered the building after the weekend that it was installed, really thought it was real and yelled at him and said, Young man, you come down right now. And went and called security. security. Until someone told him he's not real. Even with a faux plug, the students tried to plug into the false plug there for quite some time, which is why we're adding these other ones. So thank you very much.

Thanks a lot. This next customer is going to be very interesting. This is actually all of it says Apple in the studio and it is Apple in a studio production environment, but it's actually a story about availability and scalability. And of course most CIOs understand that when you're near the end of the quarter and you have to close the book, uptime becomes super important.

And if you're down, you hear from the CFO, the CEO. But nothing compares with a production studio environment where you're deadline based and that's what we're going to hear about. 24/7 availability is an absolute necessity as well as scalability so that you can have appropriate fault tolerance and appropriate performance to meet the needs.

So of course Apple is well entrenched in the studio market with production. We have products like Final Cut Studio and Logic Pro for sound and rock solid SAN servers and XServe and XServe RAID for meeting the needs of that market. I'd like to introduce JJ Franzen who is CIO and technology supervisor at South Park Studios. So JJ.

Thank you, Bud. As Bud said, I'm the technology supervisor for South Park Studios. Some of you are probably asking, what does that mean? That means I have the coolest job on the planet. I get to play with all the toys. The render farm, the file servers, the workstations, even Matt and Trey's Xboxes. I touch it all. I get to play with everything. Another cool aspect of my job is I get to know how everyone else does their job.

I get to look over the shoulders of the artists while they sketch new characters, while they animate the other characters, while they record audio on the audio booths. I get to be part of every single thing and see how the episodes evolve over the course of a production.

I'm also the guy who writes all those little pieces of glue that production requires to make things flow smoothly, to get data from one part to the next, and so on. But if I've done my job properly while we're in actual production, I'm the digital equivalent of the Maytag repairman. I sit there and do nothing, waiting for something to break. Hopefully nothing ever does. So, let's go over the production process of making a South Park episode.

South Park began as a cardboard cutout, stop motion animated short that then was decided to make a TV series out of. But the pilot for that TV series took over two and a half months to develop. Obviously they couldn't do 13 episodes a year at that scale, so they had to use computers. So we developed over the course of nine years now a production process that is very streamlined, very organized, and very, very fast.

First stage, the script. Most animated series, or most TV series in general, start with a full script before they do anything. We get a few pages. We get those few pages Thursday before the Wednesday it airs. And we get a few more pages every day after that, so we literally don't know what's going to happen from day to day. Not until Tuesday around 9 or 10 when we get the final pages of script.

After we get that script, we begin the storyboard process. And that's where we take those texts and we actually convert them into images. We'll do a comic book version of the episode to see how things looked, how things are staged. This is also where we design our characters, our backgrounds, and color design. While that's going on, Matt and Trey will go into the audio booth and record all the dialogue for the show. They even do the females because you never know if the audio is going to change, so we don't bring our females in until the very last minute.

Once those two stages are done, we begin the animatic stage. And that's where we take the audio and those individual frames of the storyboard and time out the still frames to the audio. This gives the mat and tray a chance to see how things are flowing. It also gives us a chance to see how long each shot needs to be for animation. While the animatic stage is going on, we have the setup stage. That's where we take those character designs and background designs and locations and model them. We get them rigged, get them going, getting prepared for animation.

After that's done, we go to lip sync. Lip sync is the thankless job. This is the one job where they sit there, listen to the audio, and animate just the mouths. That's all they do. It's a thankless task, but we're very dedicated to it. After all of that is done, we can finally get to animation, where a shot spends most of its time.

This is where they'll do everything else. The bobbing heads, the blinking eyes, the dancing, the talking, everything else is done there. And this basically is the primary part of the iterative process where we'll do a first pass at a shot, send it through the pipeline, Trey will look at it if he likes it, great, if he doesn't he goes back through.

Our render queue has evolved over the years to the point that it's a completely automated process now. The artist clicks a button, and from then on out, the editor gets a signal saying it's ready to be brought in. No one has to touch anything. It's a very smooth system. We're using Cube, XN, NFS. It's all very slick. We're using, in fact, Shake to convert every single individual frame into a QuickTime movie for import. So for those of you who like Shake can say that Shake is used on every shot in South Park.

After that, it goes to the editing bay where Trey will actually sit down and edit every part of every show. He's very directly involved in the entire process. The retake process, as I mentioned, we go through. It's a very iterative process. If Trey likes something, he's great. If he doesn't, he'll make notes.

There have been times where a shot will come through that completely changes where he's going with the show. Just because of one little extra thing an animator did, it completely inspires him in another direction. After we finish all of that, we get video locked, which means no more animation is done. That's usually around 7:00 or 8:00 AM, Wednesday, the day it airs.

After we get video lock, audio still has another couple hours synchronizing audio to the final animation. Once all that's done, we then roll off the videotape, take it for color correction, and then upload it from Comedy Central's Los Angeles office to the New York office where they broadcast it. That usually happens about an hour, hour and a half before it goes on the air. Now, on very rare occasion, if everyone's really kicking butt and we're all doing great, there's tenth stage we reach.

Okay, so this is how we do it. This is what we use. We use all the standard protocols. We have servers dedicated for every individual job. We have a server dedicated just to serving data to the artists. Another one just serving data to the render farm. Another one is just dedicated to converting those frames to QuickTimes. We have another one dedicated just doing backup. You can see all the tools we use. And from the entire staff and crew of South Park, I'd like to thank all you developers for making those tools and enabling us to make the show we do. Thank you.

Thanks a lot. Okay, finally we have Apple in the laboratory, Apple in science. And this is a great example of a challenge which is to build a very highly collaborative environment and to leverage open source. In the sciences, Mac has actually taken off in the last few years. Part of that is due to the interest in visualization and of course the great job we do at providing visualization tools. And the Cinema HD display has helped a lot there.

As well as groundbreaking applications from third parties, but also open source. Especially in the bioinformatics field where most of the applications that are used in that field were written in the era of open source. That means in the 1990s or later. And so what this has led to is a great amount of sharing in that community between researchers, both commercial researchers as well as researchers at institutions. StructureBiologyGrid.org, I think associated with the Harvard Medical School, but actually a distributed collaborative environment is going to be talked about by Dr. David Goharra from Washington University School of Medicine Center for Computational Biology. So, David.

Thank you, Bud. It's really a great pleasure to be here today and for the opportunity to be able to share with you some of the work that we've been doing and some of the ways that we've integrated Apple technology in the lab. But before I do that, I'd like to give you a little bit of background about SB Grid and some of the things that we do.

SBGrid started in the lab of Steve Harrison at Harvard Medical School out of a need to unify and standardize the software and computing environments that his lab members were using, with a specific focus on x-ray crystallography, electron microscopy, and NMR. My primary responsibility as part of SBGrid is to identify performance bottlenecks in our users' calculations and then to develop solutions to help them get those calculations done faster. Since its inception, SBGrid has expanded to 14 other institutions across the country, and we now have over 50 labs and support over 500 users with software.

Almost all of the applications we use are open source software and can be run from the command line. And so any system we use has to provide a UNIX environment. And because we're dealing with open source applications, we need developer tools and performance tools. We need everything that's required for porting, building, and optimizing open source applications.

As structural biologists, molecular visualization is very, very important to us. We want to be able to relate structure to function, and so we need to see what's going on with large molecular assemblies. And so having access to high definition displays, fast graphics cards, and stereo graphics hardware is not only important for us, it's an absolute requirement.

We typically tend to turn over or upgrade our hardware faster than we decommission it. And so anytime we bring new system types into the mix, they have to be able to work well in a mixed computing environment. So interoperability is really key. But probably the most important thing for us is the fact that the types of problems that we deal with are extremely computationally expensive. I'm going to give you an example of that in a little bit.

What's really key to us is the fact that we need systems that are priced well, that are fast, and that are reliable. And so what we really strive to do is we want to minimize the bottlenecks that are introduced by the calculation itself, and have the science and the understanding of that science be the rate-limiting step in everything that we do. And so having systems that ship with multiple CPUs, vector units, and highly tuned vector numeric libraries that we can use out of the box are very important features for us.

Our first entry into high-performance computing solutions from Apple was a 14-node XSERV G5 cluster. And that system was commissioned primarily for some of the EM work that was being done in the Harvard labs. On the desktop, we're using primarily Power Mac G5s and iMac G5s. We've seen some of the Intel Macs coming in as well.

And in terms of our graphics workstations, we're still using the Power Mac G5s, now with dual LCD displays. And in the case of stereo graphics, we'll attach CRT monitors to those. So let me give you just one quick example of how we've used the technologies that ship with these systems to be able to speed up some of our users' calculations.

We had a user that was performing a calculation that would take about 10 days per iteration. And at the end of that, he'd sit in front of the graphics workstation, move some things around, analyze the results, of course. That's what we were referred to as analyzing it. And then he'd resubmit it for calculation again. And so what this really meant was that he was spending about three days out of the month on this actual portion of the project, and the rest of the time he was simply waiting for results.

And so the performance tools that are provided with Xcode Shark told us immediately that we're spending, with this particular application, we're spending about 30% of our time allocating and deallocating memory. And that this memory allocation turned out to just simply be for scratch space. And so we could move that allocation to the beginning of the program's execution, and we could immediately get the runtime down to about seven days.

Continuing on, we could see that the program was doing a lot of two things. It was moving data around, it was transposing it, three-dimensional data set, and it was calculating a lot of FFTs. And if you're on a Mac and you want to do either one of those two things, the first place you really want to look is the Accelerate framework. Some of the fastest FFTs available anywhere are in Accelerate. And so after integrating Accelerate, we were able to get the runtime down to about 5.6 days.

And so then continuing on, what we could see was that the application was doing a lot of similar operations on these large chunks of data, and that the operations that were occurring over here could be done independent of the operations that were over here. And so then after multi-threading the application, we could get the runtime down to about 3.7 days. And so at this point, I say, "Okay, hey, that's not bad. Take it back to the user and to the user's boss." And the user's boss happened to actually be my boss.

And the user's boss, who was my boss, was happy, and so the user was happy, and so I was happy. Great. And so normally we just sort of leave it there and move on to something else, but we didn't. In fact, the story gets a little bit better because what I realized is that in thinking about the problem a bit more, the particular calculation that this user was doing was inherently a parallel calculation that was masquerading itself as a serial one. And so we backed off on the multi-threading code, and instead I wrote a Cocoa app that would break the job up into chunks, send it off to XGrid, and then XGrid could farm it out.

And so what we ended up with is a calculation that was taking 5.6 days on one CPU and originally 10 days on one CPU. We could farm it out to 26 CPUs and get the calculation down to 0.2 days, or roughly about five hours. And so what this meant was that the user could start the job in the evening before he left, and when he got back in in the morning, the results were now waiting for him. So it was a complete opposite. And this is just one example of how we've been able to take advantage of some of the technologies that ship with Apple hardware and software to really get some great performance out of our applications.

And so when people come up and ask me, you know, "Well, why OS X and why Apple?" I usually respond with, "Well, why not Apple?" Almost everything we need comes in the package with the system, ready to go out of the box. And if we add up all the features that we use and factor in price and performance, reliability, interoperability, Unix, it simply makes sense for us. And given the announcement that was made this morning, it makes more sense, given the hardware that's now going to become available to us. And so this is why we use Apple. This is why we continue to use Apple. And that's it. Thank you very much. Great. Thanks.

Thanks a lot. Before we get into our next session, I just want to say thank you to these customers. And really, customers like these are worth their weight in gold. Because what they do is they push us on issues of price performance, scalability, availability, manageability, and really help us to focus on innovative ways to solve those problems. And also give examples to the rest of the industry as to innovative ways to solve those problems. And again, thank you for coming to speak here.

Next, we're going to talk about XSERV as well as OS X Server Leopard. For XSERV, the hardware, we're going to ask Michael Culbert, who's Distinguished Engineer, Scientist, Technologist in Apple Hardware Engineering. Michael, you're going to talk about XSERV. Thanks a lot. Let you take it over. Thank you. Hello.

So we're done. It's been quite a year. XServe is the last product that we'll be shipping as part of the transition, but it's also the fastest and the coolest, and was one of the most fun. It has two dual-core Intel Xeon processors in it, 64-bit throughout, and of course supports tremendous amount of performance and memory. We're going to go through that.

The dual-core Intel Xeon that's been codenamed Woodcrest is the new Core 2 microarchitecture. It's full 64 bits throughout, 1.33 GHz front side bus, this thing screams. Four megabytes of shared L2, which is really, really important for these high performance, particularly in the science and video industries where a lot of rendering and a lot of manipulation of data needs to happen. It's also got the first full 128-bit vector engine that Intel has done. It's optimized for power efficiency, as you'll see in a little bit, and it's built using their most advanced 65 nanometer process.

Now, it's over five times faster than the XSERV G5. 5.4 times on integer specint rate and about 3.7 times on floating point specFP rate. has superior power efficiency, 3.88 times more compute per watt than the Intel Xeon that it replaces. And as you notice, the PowerPC G5 was actually not much better than that original Intel Xeon in this metric. So it's a tremendous improvement in density that you can achieve in a data center or in compute in the same physical volume.

It's a very high bandwidth server architecture. We focused a lot of time and effort making sure this thing can really behave well as a server as well as a compute engine. So we have 21.3 gigabytes a second of processor bandwidth to those two processors. We have a 256-bit wide, 667 megahertz memory system.

Again, very, very well balanced, 21 gigabytes a second of main memory bandwidth. Dual 8x PCI Express expansion slots. That's two gigabytes a second per slot. So you can get quite a bit of file system bandwidth if you're using this directly with an XServe RAID as part of an XAN or anything like that. Very, very high I/O bandwidth.

And of course the built-in drives. We now support SATA and SAS drives. So you can actually get tremendous performance with the drives built in. As you can see, scientific benchmarks, LINPACK is used. To demonstrate the overall system throughput. So you have tremendous memory system, processor, and disk performance that lead up to a very, very good LINPACK result. Almost three times faster in double precision and 3.2 times faster in single precision.

Super fast memory back set up, fully buffered DIMMs, 2x the capacity, 32GB in a single 1U system, increased high availability features to keep these things up and running, ECC with on-demand scrubbing, CRC protection of the data, so the transactions between the memory DIMMs and the systems are protected with CRCs, giving the system the ability to recover from transient errors rather than coming to a halt.

There's massive memory bandwidth in this machine, 2.1 times more than the original XServe G5. And that is what helps reflect the benchmarks you saw earlier. Tremendously flexible storage. The SATA, of course, is no small feat. 2.25 terabytes of internal hot-plug storage in this 1U machine. 24x7 server rated drives, very very important. And of course, if you want really really high performance, with the SAS drives you get 125 MB/s per drive, 3.5 ms of seek. These are ideal for large database applications, anything that really has very very I/O intense operation.

As you can see, this is just comparing SATA and SAS in the same machine. Both of them are quite fast, but the SAS is really unbelievable. 125 megabytes a second. Remember, you have three of those drives, so you actually have three times that bandwidth if you stripe them. And the seek time means that for large data sets with random access behavior like a database, you have tremendously high performance right in the box.

And then of course the I/O. We have two, as I said, two by eight PCI Express card slots. One is a standard slot for 6.8 inch cards, so fiber channel cards and the like. And one is actually a full 9 inch slot which allows you to put fairly large graphics cards and other things in that slot. There's 50 watts of total power available for the slots. And the long slot can actually support PCI X cards, which gives you the ability to use cards that haven't yet transitioned to PCI Express in the same server form factor.

So, there were a lot of customer requested features which you guys have been waiting for for a while. We have dual core processors, we have redundant power supplies, we have remote lights out management, which I'll demonstrate in a minute, and built-in graphics. The built-in graphics does not occupy one of those two slots, so you get the graphics and the two slots.

So, take the cover off this thing. There's some other very, very subtle but important improvements as well. This machine has a tremendous amount of thought put into the thermal design. It's actually quite a bit quieter than its predecessor and, very importantly, has a very direct clean airflow path through the machine.

So, even in a non-ideal machine room, which a lot of these find their way into, it can tolerate pretty adverse environments with ease. Of course, we have the dual redundant power supplies. The dual Intel processors are in the center. As you can see, the airflow is straight through the three drives in the front.

32 gigabytes of RAM on the side there. Not in front of the processor, which a lot of other servers do. This is again making the thermal environment substantially better. The two PCI Express slots, dual onboard gigabit Ethernet ports, the built-in graphics, mini DVI, and we didn't take anything off either. FireWire 800, USB 2, the DB9 serial are all still there.

Of course, this is really, really important. Redundant power supplies, hot swappable, fully load sharing, you only need one of them to run the machine. Again, the ECC, on-demand scrubbing, embedded diagnostics, and integrated software RAID. So the lights-out management. It's based on the IPMI 2.0 protocol. We can remotely power on/off and restart the server. It's Ethernet access with encryption, and the server monitor application, which you're already familiar with, will control this. So I'm going to demo it here.

We have this rack over on the side that's basically, this laptop of course is connected to the projector, it's got wires hanging off of it, but running the server monitor app, you can see one of the machines here, this little pip is reflecting that it's on the net, but it's off. See, it's half full. So we're going to go ahead and power that machine on.

And while we're at it, we're going to take the top one here, and if you watch the rack there, you'll see the lights come on. We're going to take the top one here, which is running, and we're going to just go ahead and shut that one down. So you'll see, I don't know if probably most of you can't see it, but we did put them in the top of the rack. You can see the lights on. One of them go on and the other one went off as it shut down. So, that's the demo. And it actually worked!

So anyway, and of course, it's really the software that makes this an amazing machine. We've built this incredibly fast piece of hardware, but guess what? It has all of Mac OS X Tiger Server in the box, which makes it an incredible server story. and to come tell us a lot more about what's new in Mac OS X Server for Leopard, Greg Burns, our Senior Director of Server and Software Storage Solutions.

Thanks, Michael. So we've got three new updates today actually in server software. And the first one is Tiger Server Universal. This is an update to the existing Tiger Server that we shipped today on PowerPC. And as the name says, it's universal. It runs on the new Intel boxes.

It runs on the Mac Pro and the XServe as well as the other Intel Macs. And it's actually the first version of the Mac OS X operating system that's universal in a sense. And it runs on a single image. What does that mean? That means that a single image on disk will boot Mac OS X server and run it on both Intel and PowerPC.

If you have a lot of servers and you need to image them, it makes your job vastly simpler. Of course, it gives you access to the power and the performance of the new Mac Pro and the XServe and the 64-bit address space of the Xeon processor. And we've made a few enhancements in the OS. We've improved some of the locking and data structure allocation in the kernel to allow it to scale better as a server on the new hardware. So that's Tiger Server Universal. Mac OS X servers completed the transition to Intel, and that's available now.

Second product is XAN 1.4. This is our storage area networking product. And if you're doing a video workflow, if you're consolidating storage behind your servers, it's the fastest, it's the highest performance way to connect all your Macs into a single storage pool. It scales up to two petabytes and it provides uptime through redundant metadata controllers.

So XAN does all this today. So what's new? Well, I'm sure you've guessed it's universal. So it runs on the new Mac Pro hardware and AXSERVs and you can mix and match Intel and PowerPC systems on the same SAN. And we have one more feature that's new in XAN 1.4 and that's Access Control List support.

Up till now, XAN has been limited by Unix access privileges, and that's fairly difficult to map into your workflows in your shared workgroups, especially in video workgroups. And so we've removed that limitation, and it's now fully compatible with Tiger Access Control List. So you can map it to your workflow and not have to map your workflow into your access privileges. So XAN 1.4 has also made the transition to Intel.

So our biggest release today that we're working on now is Leopard Server. This is going to be shipping in the spring of 2007, and we have a lot of new features coming. So here's just a few. The first is the iCal Server. This is a complete calendaring server.

And it's got everything that you'd expect from a full-fledged calendaring server. So, users can schedule events and meetings, you can send invitations, receive them, you can search free busy time, proxies, access control, all the features you'd expect. And the most important feature is that it's built on an open standard, CalDAV. So it not only works with the iCal client in Leopard, but it also works with other clients such as Mozilla's Sunbird or OSAF's Chandler.

We've designed it from the ground up to be scalable. So you can start simple with one server and then grow as you need to with multiple servers all sharing the same calendaring database on the back end with XAN. And because we want to promote the adoption of this server as well as CalDAV, we're actually releasing the complete open source, the complete source to the server as open source as the Darwin Calendar Server and that will be available this week under the Apache license.

So another new service in Leopard is Teams. Teams is a wiki and a set of web services that easily allows groups or project teams to create and share information using Leopard's server. We use wikis a lot inside of Apple and they're really powerful, but sometimes the usage model can be really arcane and some of the markup language is hard to use. We've solved all that in Teams. Teams uses Ajax. It's all truly what you see is what you get. It's direct editing. If you can use a word processor, you can use Teams.

And so Teams is new. It's a wiki service in Leopard's server. And it's more than just that. It also has a lot of other services as well. So there's shared group calendars. You can publish information through blogs and podcasts. So it's truly a rich set of services for Teams to create and share content using Leopard's server through the web.

Now both Teams and iCal Server actually use open directory. They're actually integrated there. So all of the information they need, such as groups and shared contacts, resources, rooms, locations, other things like that, are all now stored in open directory. And so we've introduced another application as well called the Teams directory that actually lets normal users who need to work with these entries create and manage these. So you can create project teams or groups. You can manage their membership, rooms and locations, resources that are used by iCal. All of these can be created using Teams directory. using Teams directory.

Another new feature in Leopard Server is streamlined setup. Tiger Server was already very easy to set up, and we've made it even simpler. So it's very quick to get the server up and running. But also, it's not just the server, it's also the clients as well. We've automated the client setup, so all your applications in the client that work with the server, such as mail, chat, calendaring, time machine, etc., can all be easily configured to work with the server in just a few clicks.

So if you need to set up a server with a collection of clients, it's very easy, very quick, and there's truly no day-to-day management that goes into it. And in addition, as part of our server management enhancements, we've also integrated backup. So now the server uses time machine to backup your server settings and data as well.

Spotlight Server allows you to easily search for content both locally and now on your server as well. So in Leopard, when you do a Spotlight search, it will transparently search for all the content on your mounted server volumes. There's no configuration required and it'll support all the features, the new features of Spotlight such as advanced search queries. On the server, everything happens automatically. You don't need to configure it.

The indexing happens automatically for all your content. It's done in the background at a lower priority so it doesn't interfere with the performance of your services. And of course, as you would expect, it adheres to the file access privileges so people only see the results for the files that they're actually allowed to access.

We have another new service called Podcast Producer. Podcast Producer is an end-to-end solution for capturing, encoding, and publishing content on the web as a podcast. It's great for people who quickly and easily need to be able to capture audio or video feeds and publish them out. You can set up an automatic workflow so that you can capture video or audio content on a Leopard client. It's automatically uploaded to the server where it can be encoded in any number of formats that QuickTime supports. And then it's published out to the web as a podcast via RSS.

It's integrated with Open Directory to control who can publish content through your services. And all the encoding is hosted on top of XGrid. So you can start with a single server, but then you can grow as your encoding needs grow. So it's a great end-to-end solution for rapidly capturing content and getting it out to the web.

So the iCal Server, Teams Wiki, Spotlight Server, Podcast Producer, these are a few of the major new services in Leopard Server. There's a whole bunch more. We have clustering in the mail services. We have a federation of instant messaging servers with iChat Server 2. We have the ability to host distributed computing without a dedicated controller in XSquare 2. As well as in the CoreOS foundations, we have the ability to perform 64-bit computing now at all layers of the system.

So there's a lot of stuff that's new. We don't have time to go into all of it right now, but the good news is there's another session right after this, the Mac OS X Server overview session. We're going to spend that whole session going more into Leopard Server, so I hope I'll see you there. Thank you.

Thanks Greg. So, to summarize, things we're focused on for business: standards-based, that's been true all along, interoperable, especially with Windows, best price performance, open source, and of course security. Now, at this WWDC we actually have a lot of things going on that are of interest to the IT professional.

We have sessions, or actually labs downstairs, right across from the IT lab. Apple in the Lab, which is scientific computing, Apple in the Studio, Apple on the Campus. You should especially go take a look at Apple in the Lab. One of the things we have there is a smaller version of the IT lab.

If you heard about the Hyper Wall at UC Irvine, we've got a smaller version of that, which is a huge number of pixels done with a mosaic of displays. We also have community areas in digital media, system administration, and scientific computing, and as I mentioned, the IT lab across from the Apple Lab Studio and Campus.
