WEBVTT

00:00:10.960 --> 00:00:14.600
You've heard that any
sufficiently advanced technology

00:00:14.600 --> 00:00:17.390
is indistinguishable from magic.

00:00:18.320 --> 00:00:20.460
Well,
there's been some technology lurking

00:00:20.460 --> 00:00:26.390
under the covers in Mac OS X for
a while that some people have

00:00:26.390 --> 00:00:26.390
said seems pretty close to magic.

00:00:27.370 --> 00:00:31.950
It's called Latent Semantic Mapping.

00:00:32.550 --> 00:00:37.790
And today,
we're going to release that magic

00:00:37.790 --> 00:00:37.790
so that you guys can build that
magic into your applications.

00:00:38.000 --> 00:00:42.190
We've applied Latent Semantic
Mapping to quite a range of problems.

00:00:42.340 --> 00:00:49.850
We'll show you junk mail filtering.

00:00:49.850 --> 00:00:52.490
We'll show you how it enabled
us to achieve the world's best

00:00:52.490 --> 00:00:52.490
Japanese text input method.

00:00:53.220 --> 00:00:55.100
Thank you.

00:00:55.200 --> 00:00:58.210
Kida-san, Kida-san, the guy.

00:00:58.720 --> 00:01:01.200
So let's get into it.

00:01:01.200 --> 00:01:04.240
I'm delighted to be here today,
and I'm delighted to be able to

00:01:04.350 --> 00:01:10.200
introduce the senior engineer who
is the architect of the code base,

00:01:10.220 --> 00:01:10.700
Dr.

00:01:10.700 --> 00:01:12.450
Matthias Neeracher.

00:01:18.100 --> 00:01:20.660
Thank you, Kim.

00:01:20.670 --> 00:01:21.790
Good afternoon, everybody.

00:01:21.820 --> 00:01:29.810
I'm really thrilled to be
here today and talk to you

00:01:29.810 --> 00:01:30.890
about Latent Semantic Mapping,
or as we affectionately call it, LSM.

00:01:32.200 --> 00:01:35.360
We're unfortunately not quite
important enough to be renamed to

00:01:35.360 --> 00:01:38.490
Core Semantics or something like that.

00:01:39.900 --> 00:01:45.510
So this afternoon I would like to talk
to you about what LSM is and I actually

00:01:45.830 --> 00:01:51.380
am going to start with a toy example just
to give you an idea of what it could do.

00:01:51.730 --> 00:01:56.610
Then we're going to bring up somebody who
can explain you how it actually works.

00:01:56.820 --> 00:02:01.900
I'm going to talk about how to use
the LSM API or APIs that we have.

00:02:01.960 --> 00:02:03.920
We're going to present
some internal case studies,

00:02:03.940 --> 00:02:05.720
how we've used it.

00:02:05.780 --> 00:02:09.240
Then we're going to talk about
what's really interesting for you,

00:02:09.300 --> 00:02:12.960
what it can do for you,
how you can use it in your applications.

00:02:12.970 --> 00:02:17.920
And we're going to have extended time
for questions and answers at the end.

00:02:18.010 --> 00:02:20.800
So if you have questions,
you're going to get a chance

00:02:20.860 --> 00:02:23.760
to ask them at the end.

00:02:24.340 --> 00:02:26.510
So what is LateN Semantic Mapping?

00:02:26.650 --> 00:02:31.560
In the most general and abstract terms,
LateN Semantic Mapping is a

00:02:31.560 --> 00:02:35.800
technology to analyze data and
classify it into categories.

00:02:35.940 --> 00:02:38.200
Now that's awfully abstract and general.

00:02:38.200 --> 00:02:43.840
So in fact,
it's probably going to be easier if

00:02:43.970 --> 00:02:48.650
I just give you a brief example of this.

00:02:48.740 --> 00:02:49.790
So could I have the
demo machine on screen?

00:02:50.940 --> 00:02:52.180
Thank you very much.

00:02:52.260 --> 00:02:55.900
So one of the things we've done with
the Latent Semantic Mapping framework

00:02:55.900 --> 00:03:00.470
is we've tried to make pretty much
the whole API available as a shell,

00:03:00.490 --> 00:03:02.620
as a command line tool, as well.

00:03:02.620 --> 00:03:06.270
So you can prototype your
applications very easily,

00:03:06.270 --> 00:03:08.440
especially if they are text-based.

00:03:08.440 --> 00:03:13.400
And you can quickly find out whether
an ID is going to work out or not.

00:03:13.400 --> 00:03:16.310
And in fact,
certain parts of your application,

00:03:16.370 --> 00:03:20.080
if you do pre-training,
you might never have to write to

00:03:20.080 --> 00:03:21.800
the C API for an app to do that.

00:03:21.800 --> 00:03:23.820
You could just do it in a shell script.

00:03:23.930 --> 00:03:27.730
So I'm going to demonstrate how
to use the Latent Semantic Mapping

00:03:27.730 --> 00:03:31.420
command line tool to do a simple
categorization task to be able

00:03:31.830 --> 00:03:34.720
to tell apart Objective-C header.

00:03:34.720 --> 00:03:38.440
If you see a .h file,
is it Objective-C or is it C?

00:03:38.630 --> 00:03:45.320
So we're going to train up a
little map to tell those apart.

00:03:55.400 --> 00:04:10.800
[Transcript missing]

00:04:13.570 --> 00:04:18.200
So first thing we need
to do is to create a map.

00:04:18.200 --> 00:04:19.890
And the command line
tool is just called lsm.

00:04:21.210 --> 00:04:26.110
We want to create a map
and we have two categories:

00:04:26.110 --> 00:04:27.940
Objective C versus C.

00:04:28.120 --> 00:04:30.490
Each of those has a number of files.

00:04:30.500 --> 00:04:36.770
We separate it to two groups
of files with a semicolon,

00:04:36.770 --> 00:04:36.770
so we say categories.

00:04:40.210 --> 00:04:51.460
are separated by a group delimiter,
a semicolon, and we're going to call the

00:04:51.460 --> 00:04:51.460
map Objective C versus C.

00:04:52.070 --> 00:04:57.590
So let's pick a rich, juicy source of
Objective-C headers to train up.

00:04:57.710 --> 00:05:00.700
Let's pick the foundation headers.

00:05:08.170 --> 00:05:09.670
So we have the semicolon.

00:05:09.930 --> 00:05:12.620
Now we're going to bring
in a source of C headers.

00:05:12.760 --> 00:05:16.480
Let's pick Audio Toolbox.

00:05:25.280 --> 00:05:31.670
Now, it's going to parse a couple
dozen headers for each side.

00:05:31.690 --> 00:05:33.480
It's going to put these into a map.

00:05:33.680 --> 00:05:37.720
It's going to compile the map,
which is a complex mathematical process.

00:05:37.860 --> 00:05:43.190
So it's going to take some time,
let's say a couple hundred milliseconds.

00:05:43.230 --> 00:05:45.870
Okay, we're done.

00:05:48.010 --> 00:05:52.540
We have a map that, in its 300 kilobytes,
incorporates everything there is

00:05:52.570 --> 00:05:56.780
to know about telling apart an
objective C header from a C header.

00:05:56.920 --> 00:05:59.730
Now I'm going to prove that it works.

00:06:02.870 --> 00:06:05.460
For that,
we are going to do LSM Evaluate.

00:06:05.750 --> 00:06:10.000
And now let's pick a framework
where you might not know in advance

00:06:10.000 --> 00:06:12.800
whether it's Objective-C or C.

00:06:12.910 --> 00:06:17.400
So for that, let's pick Core Data.

00:06:17.520 --> 00:06:20.010
Yes?

00:06:20.020 --> 00:06:22.450
Sure.

00:06:22.490 --> 00:06:23.720
Thanks, Kim.

00:06:23.720 --> 00:06:32.110
That was a little bit too big.

00:06:40.880 --> 00:06:45.170
Now let's evaluate all of these
headers and core data and see

00:06:45.180 --> 00:06:49.870
whether LateN Semantic Mapping
places them in category 1,

00:06:49.890 --> 00:06:52.700
Objective C, or in category 2, C.

00:06:56.800 --> 00:06:59.030
Oops.

00:06:59.070 --> 00:07:01.910
And it would help to type this correctly.

00:07:03.830 --> 00:07:08.290
OK, so we have core data dot h.

00:07:08.480 --> 00:07:12.080
As you see,
category one wins quite decisively.

00:07:12.130 --> 00:07:15.690
We're going to talk a little bit
later about what these course means.

00:07:15.780 --> 00:07:18.730
What's important to know
is category one wins.

00:07:18.900 --> 00:07:21.980
Now, core data defines.

00:07:22.070 --> 00:07:26.110
Blueprint semantic mapping unfortunately
thinks that this is a plain C file.

00:07:26.120 --> 00:07:28.000
Category two wins.

00:07:28.090 --> 00:07:29.520
However, this is not unfortunate.

00:07:29.620 --> 00:07:31.120
This is, in fact, a plain C file.

00:07:31.120 --> 00:07:33.830
It just has a couple of defines in it.

00:07:34.020 --> 00:07:38.060
And all the other headers you see,
category one, category one,

00:07:38.060 --> 00:07:41.390
all of these are Objective-C.

00:07:41.450 --> 00:07:46.320
Now, you might be saying,
I could have done that with a grep.

00:07:46.380 --> 00:07:48.690
Or I could have told that by
looking at the file name and

00:07:48.750 --> 00:07:51.060
saying that it starts with ns.

00:07:51.120 --> 00:07:54.730
And you would be absolutely
right if you said that.

00:07:56.050 --> 00:07:59.330
The point here is not that this
is a particularly hard task.

00:07:59.330 --> 00:08:02.490
The point here is that we were
able to train this up without

00:08:02.490 --> 00:08:04.500
any domain-specific knowledge.

00:08:04.500 --> 00:08:08.390
We just fed the LateN Semantic
Mapping system a bunch of

00:08:08.420 --> 00:08:12.190
files from each category,
and it was able to pick out

00:08:12.290 --> 00:08:14.280
which file belonged where.

00:08:14.360 --> 00:08:17.640
And as my colleague is
going to explain later,

00:08:17.670 --> 00:08:21.760
this was not even a particularly
well-chosen example.

00:08:21.760 --> 00:08:24.740
Could we go back to the slides, please?

00:08:24.740 --> 00:08:25.880
Thank you.

00:08:28.150 --> 00:08:31.620
So you've seen what it does.

00:08:31.750 --> 00:08:34.180
Now, that was the black box for you.

00:08:34.330 --> 00:08:36.480
We're going to pry that box open.

00:08:36.780 --> 00:08:43.350
And for that,
I would like to bring up my colleague,

00:08:43.350 --> 00:08:46.170
Jerome Bellegarda,
the Apple's distinguished scientist

00:08:46.170 --> 00:08:46.170
who brought this technology to Apple.

00:08:46.170 --> 00:08:46.170
Jerome?

00:08:51.060 --> 00:08:53.160
Thank you, Matthias.

00:08:53.160 --> 00:08:53.940
Good afternoon.

00:08:53.940 --> 00:08:55.060
Delighted to be here.

00:08:55.250 --> 00:08:59.570
Actually, my job is very simple,
because how it works,

00:08:59.680 --> 00:09:02.080
it's all in the name.

00:09:02.080 --> 00:09:03.440
Latent semantic mapping.

00:09:03.440 --> 00:09:06.120
So first of all, it's a mapping.

00:09:06.260 --> 00:09:11.810
And what we're going to do is we're
going to go from discrete words and

00:09:11.810 --> 00:09:19.050
documents into a mathematical space
which is multidimensional in general.

00:09:19.230 --> 00:09:22.760
In the examples that I'm going
to go through in this session,

00:09:22.760 --> 00:09:25.900
that's all going to be low dimensional,
typically three

00:09:25.900 --> 00:09:27.260
dimensional so we can see.

00:09:27.520 --> 00:09:30.020
But in general,
it's going to be multidimensional.

00:09:30.060 --> 00:09:34.690
And this space is going to be--
the particularity of this space is

00:09:34.690 --> 00:09:37.650
that it's going to be continuous,
which makes it easy to

00:09:37.650 --> 00:09:40.540
compare things in this space.

00:09:40.550 --> 00:09:46.160
LSM also has the word semantic in it,
which means that this mapping is going

00:09:46.160 --> 00:09:51.630
to be based on the overall content and
meaning buried inside the document.

00:09:51.760 --> 00:09:57.210
Sometimes not even in an obvious way,
which is the part that is latent.

00:09:57.480 --> 00:10:00.900
And the way we're going to
uncover those relationships,

00:10:00.920 --> 00:10:05.020
which are sometimes hidden,
is we're going to look at words

00:10:05.060 --> 00:10:07.470
that co-occur with each other.

00:10:07.500 --> 00:10:09.380
Exactly which co-occur with what.

00:10:09.630 --> 00:10:12.880
So for example,
if you look at the document and you

00:10:12.880 --> 00:10:18.210
see the words maybe bank and interest,
it doesn't tell you much about

00:10:18.220 --> 00:10:20.060
what the document is all about.

00:10:20.150 --> 00:10:25.540
If you further see rate and money,
then you know that it's probably having

00:10:25.540 --> 00:10:27.330
something to do with your finances.

00:10:27.560 --> 00:10:29.500
If on the other hand,
you see bank and interest,

00:10:29.510 --> 00:10:33.020
and then later on river and trout,
then it's probably some sort

00:10:33.020 --> 00:10:36.550
of article about the outdoors
and fishing in particular.

00:10:36.620 --> 00:10:41.640
And so that's really the basis
of latent semantic mapping.

00:10:41.640 --> 00:10:46.060
We're going to try to uncover those
hidden relationships between the words.

00:10:46.140 --> 00:10:49.360
So in the next few slides,
I'm going to go through each

00:10:49.360 --> 00:10:54.340
of those three aspects of LSM,
mapping, semantic, and latent.

00:10:54.480 --> 00:10:56.590
Let's start with the mapping.

00:10:56.750 --> 00:11:00.350
So as I mentioned at the beginning,
we're going to have some documents.

00:11:00.460 --> 00:11:03.920
Maybe we have the finance documents
and some fishing documents,

00:11:03.920 --> 00:11:06.530
like I was talking just the slide before.

00:11:06.690 --> 00:11:10.420
We may have some other things,
like computer science documents,

00:11:10.420 --> 00:11:12.510
maybe some hunting documents, and so on.

00:11:12.700 --> 00:11:17.680
And what we want to do is we want
to map those documents onto a space,

00:11:17.710 --> 00:11:18.880
multidimensional space.

00:11:18.880 --> 00:11:21.640
In that case,
it's going to be just three-dimensional.

00:11:21.760 --> 00:11:25.170
And so imagine in that space,
we have corners, so to speak,

00:11:25.180 --> 00:11:27.410
regions in that space,
which corresponds to

00:11:27.540 --> 00:11:28.540
what we're interested in.

00:11:28.540 --> 00:11:31.750
So there will be a finance corner
and maybe a computer science corner.

00:11:31.830 --> 00:11:35.870
So what we want to do is take each
of those documents and map them into

00:11:35.870 --> 00:11:38.580
the appropriate corner of the space.

00:11:38.600 --> 00:11:40.660
And that's a process called training.

00:11:40.680 --> 00:11:43.170
That's what happens
when you create a map,

00:11:43.260 --> 00:11:46.660
just like Mathias did a few minutes ago.

00:11:46.670 --> 00:11:47.230
And so we're trying to do that.

00:11:47.300 --> 00:11:48.090
We're training.

00:11:48.220 --> 00:11:51.240
So we are mapping all those
documents on two points in the space.

00:11:51.240 --> 00:11:53.460
And hopefully,
once we are done with that,

00:11:53.680 --> 00:11:58.490
we have all those little triangles here,
the red triangles,

00:11:58.520 --> 00:12:03.060
which are representation of each of
the documents that we started out with.

00:12:03.060 --> 00:12:07.110
Now, the problem is now we
have a new document.

00:12:07.120 --> 00:12:08.840
And we haven't seen it before.

00:12:08.840 --> 00:12:12.900
And what we want to do is we want
to classify the document against the

00:12:12.900 --> 00:12:15.080
LSM space that we've just created.

00:12:15.110 --> 00:12:20.720
And that's what the lsmevaluate command
that Mathias just presented to you does.

00:12:20.760 --> 00:12:25.420
So in this case, we are just mapping that
document onto the space.

00:12:25.520 --> 00:12:28.890
And in this particular case,
as you can see, it's aligned with the

00:12:28.990 --> 00:12:30.920
computer science cluster.

00:12:31.100 --> 00:12:36.180
So we can reasonably conclude that this
document has to do with computer science.

00:12:36.230 --> 00:12:37.340
That's the mapping part.

00:12:37.380 --> 00:12:39.540
Now, let's take a look at the semantic.

00:12:39.540 --> 00:12:44.780
And for this, I would like to just
bring out a case study.

00:12:44.810 --> 00:12:47.000
And that's the junk mail filter,
the one that you have on

00:12:47.000 --> 00:12:49.350
your machines right now.

00:12:49.430 --> 00:12:52.040
And so when we started
working on a junk mail filter,

00:12:52.040 --> 00:12:56.130
we decided to do something a
little bit different from what was

00:12:56.130 --> 00:12:58.450
the state of the art at the time.

00:12:58.570 --> 00:13:01.340
And at the time,
the state of the art was, well,

00:13:01.360 --> 00:13:03.290
you look for keywords in your message.

00:13:03.310 --> 00:13:06.000
So if you see the keyword
"sex," for example,

00:13:06.000 --> 00:13:10.720
then you might possibly see some
sort of junk message or whatever.

00:13:10.830 --> 00:13:14.560
But in fact,
we wanted to go beyond that and try

00:13:15.090 --> 00:13:19.240
to assess whether the whole topic,
not just a few keywords,

00:13:19.240 --> 00:13:23.080
but the whole topic of the message
was consistent with the user interest.

00:13:23.090 --> 00:13:25.040
Because after all,
you could have sex education,

00:13:25.040 --> 00:13:26.860
which might be a legitimate topic.

00:13:26.860 --> 00:13:33.670
So to do that, we used an LSM strategy,
which is to encapsulate what the user

00:13:33.670 --> 00:13:39.960
likes and what he dislikes inside of
the LSM space that you saw before.

00:13:39.960 --> 00:13:43.080
And so in that particular case,
we used two categories,

00:13:43.080 --> 00:13:46.560
one for junk and one for legitimate.

00:13:46.560 --> 00:13:49.400
And the implementation
in this particular case,

00:13:49.400 --> 00:13:50.960
of course, the space is very simple.

00:13:50.960 --> 00:13:53.400
It has just two dimensions.

00:13:53.400 --> 00:13:56.460
And so we defined within
that space two points.

00:13:56.460 --> 00:14:01.840
We call that anchor, semantic anchors,
one that represents the

00:14:01.840 --> 00:14:05.540
junkness of the collection,
training collection,

00:14:05.690 --> 00:14:09.220
and one that represents the
legitimacy of the collection.

00:14:09.220 --> 00:14:13.560
And then what we have to do now is
every time we have a message coming in,

00:14:13.740 --> 00:14:16.440
we just evaluate whether it's
closer to one or the other.

00:14:16.440 --> 00:14:20.760
So let's take a look at what
happens in this simple case.

00:14:20.800 --> 00:14:23.370
So we have this space,
which is two dimensional.

00:14:23.550 --> 00:14:25.760
And we have three kinds
of entities in that space.

00:14:25.810 --> 00:14:31.530
We have the legitimate semantic anchor,
which might be somewhere in here.

00:14:31.720 --> 00:14:35.000
And you can think of it as the
centroid of all the messages that

00:14:35.000 --> 00:14:39.900
we've seen in the training collection
that were marked as legitimate.

00:14:39.920 --> 00:14:43.260
Then we have the junk anchor,
which might fall somewhere here.

00:14:43.300 --> 00:14:45.140
And again,
you can think of it as the centroid

00:14:45.140 --> 00:14:47.820
of all those messages that were junk.

00:14:47.910 --> 00:14:51.310
And then we have the incoming message,
which falls somewhere in that space.

00:14:51.440 --> 00:14:56.280
And so now what LSM does, very simple,
it looks at distances.

00:14:56.280 --> 00:15:00.150
It looks at which
anchor is it closest to.

00:15:00.440 --> 00:15:02.070
In this particular case,
you can see that it's

00:15:02.070 --> 00:15:04.100
closer to the junk anchor.

00:15:04.190 --> 00:15:08.860
And so LSM would assign to that
particular junk category a higher score.

00:15:08.980 --> 00:15:13.930
And that's what you saw in the simple
example that Mathias went through.

00:15:14.040 --> 00:15:15.330
So that's the semantic part.

00:15:15.450 --> 00:15:16.200
Now let's take a look.

00:15:16.200 --> 00:15:20.040
Oh, before we go on,
I'd like to just mention,

00:15:20.040 --> 00:15:25.980
a little bit of bragging here,
the performance of our junk mail filter.

00:15:25.980 --> 00:15:29.870
We did quite a number of
experiments along the years to

00:15:29.870 --> 00:15:33.490
validate that this LSM approach,
which again was a little bit

00:15:33.520 --> 00:15:36.180
different from the state of the art,
is actually competitive

00:15:36.180 --> 00:15:37.270
with the state of the art.

00:15:37.420 --> 00:15:41.030
And as you can see,
we got on different databases,

00:15:41.120 --> 00:15:46.080
including some very standard databases
that are used in the literature.

00:15:46.080 --> 00:15:50.190
We got performance that is
equal or better than the

00:15:50.190 --> 00:15:54.290
competitive methods out there,
which are typically based

00:15:54.290 --> 00:15:58.760
on what you may have seen
referred to as Bayesian methods.

00:15:58.760 --> 00:16:04.340
And an advantage of the junk mail filter,
in addition, is that it's adaptive.

00:16:04.340 --> 00:16:08.480
It's very easy to reconfigure the
LSM space in order to track the

00:16:08.480 --> 00:16:10.480
user's interest as they evolve.

00:16:10.480 --> 00:16:14.910
They may be interested in another topic,
or they may lose interest in a topic,

00:16:14.910 --> 00:16:15.960
and so on and so forth.

00:16:15.960 --> 00:16:17.720
And we are able to model all of this.

00:16:17.720 --> 00:16:21.780
And of course, as you know,
junk mail filter has been integrated

00:16:21.780 --> 00:16:24.590
into Mac OS X since Jaguar.

00:16:24.770 --> 00:16:27.150
So now let me say a word about latent.

00:16:27.310 --> 00:16:31.280
And for this, I thought I would go
through an actual example,

00:16:31.280 --> 00:16:33.160
the kind of example that
Mathias went through,

00:16:33.160 --> 00:16:35.720
but a little bit,
perhaps a little bit richer,

00:16:35.730 --> 00:16:39.120
where we have four
categories in that example.

00:16:39.120 --> 00:16:42.460
So think of it as a four
feature calendar management.

00:16:42.460 --> 00:16:45.840
It's a calendar where you
can only do four things.

00:16:45.840 --> 00:16:50.790
It's elementary, but I think it will
make the point nicely.

00:16:50.840 --> 00:16:57.650
So in that particular calendar system,
you can issue four commands, or queries,

00:16:57.750 --> 00:16:59.090
if you will.

00:16:59.190 --> 00:17:01.520
You can ask what time
it is at the moment.

00:17:01.600 --> 00:17:03.590
You can ask what day it is.

00:17:03.660 --> 00:17:06.600
You can ask what time
is a particular meeting,

00:17:06.600 --> 00:17:08.100
and you can cancel that meeting.

00:17:08.100 --> 00:17:10.250
And that's all you can say.

00:17:10.370 --> 00:17:14.520
And so we're going to use an LSM strategy
to try to approach that problem.

00:17:14.540 --> 00:17:18.030
And so we're going to use one category,
one category per feature,

00:17:18.040 --> 00:17:20.150
which means in that space,
we're going to have

00:17:20.150 --> 00:17:21.130
four semantic anchors.

00:17:21.140 --> 00:17:23.840
And again,
because it's a very simple problem,

00:17:23.840 --> 00:17:27.800
it's going to be a
two-dimensional LSM space.

00:17:27.820 --> 00:17:30.040
But instead of having
two semantic anchors,

00:17:30.040 --> 00:17:32.620
like in the junk mail filter,
now we have four.

00:17:32.650 --> 00:17:35.660
And then again,
we're going to evaluate whatever the user

00:17:36.200 --> 00:17:40.660
decides to issue as a query or a command.

00:17:41.030 --> 00:17:45.600
We're going to evaluate that
input against the four anchors.

00:17:45.600 --> 00:17:49.110
will tell us actually what we need to do.

00:17:49.190 --> 00:17:50.140
how LSM works.

00:17:50.260 --> 00:17:53.450
So why did I choose
this particular example,

00:17:53.450 --> 00:17:55.780
which is not very realistic?

00:17:55.850 --> 00:17:58.220
Well, as I mentioned,
it's got four categories.

00:17:58.220 --> 00:18:04.300
Now let's assume the user wants
to know what time is the meeting,

00:18:04.300 --> 00:18:06.120
but he messes up.

00:18:06.120 --> 00:18:07.640
You know, users always mess up.

00:18:07.640 --> 00:18:10.740
He messes up,
and instead of inputting what time is

00:18:10.740 --> 00:18:14.470
the meeting as the application demands,
he says something like

00:18:14.470 --> 00:18:15.810
when is the meeting.

00:18:16.040 --> 00:18:20.500
So we have to classify that input
against the four categories.

00:18:20.770 --> 00:18:25.140
So let me point out why
this is a difficult task.

00:18:25.170 --> 00:18:28.190
Well, first of all,
different words in this example have

00:18:28.330 --> 00:18:29.920
very different predictive power.

00:18:30.070 --> 00:18:35.920
For example, the appears everywhere,
which means it has zero predictive power.

00:18:35.940 --> 00:18:37.060
All you know is the.

00:18:37.060 --> 00:18:41.340
You cannot tell which command
it is or which query it is.

00:18:41.420 --> 00:18:47.370
However, they and console each predict a
particular query or command well.

00:18:47.440 --> 00:18:52.760
And so those two words
have high predictive power.

00:18:52.950 --> 00:18:56.910
There are all kinds of all manners
of co-occurrences and overlaps.

00:18:56.920 --> 00:19:02.700
What is the, for example,
occur in three of the four categories?

00:19:02.730 --> 00:19:07.730
And then, of course, we have ambiguities,
because meeting, for example,

00:19:07.730 --> 00:19:10.300
occurs also in two of the categories.

00:19:10.480 --> 00:19:13.050
And then, of course,
the big one is that when

00:19:13.830 --> 00:19:15.460
has never been seen.

00:19:15.540 --> 00:19:18.760
In fact, it's not even part of the
underlying vocabulary.

00:19:18.780 --> 00:19:22.620
And so we don't know what
the system will do with that.

00:19:22.620 --> 00:19:24.710
Well, let's take a look.

00:19:25.020 --> 00:19:30.320
So in this particular example, again,
it's so simple that we in two dimensions.

00:19:30.320 --> 00:19:34.200
We have the first LSM dimension
and the second LSM dimension.

00:19:34.210 --> 00:19:38.630
And then we have one thing
that I didn't make clear,

00:19:38.630 --> 00:19:42.580
perhaps, is that you can map the
categories in that space.

00:19:42.580 --> 00:19:44.350
You can also map the words.

00:19:44.490 --> 00:19:45.300
So let's take a look at.

00:19:45.320 --> 00:19:47.830
How the words map in that space.

00:19:48.190 --> 00:19:49.520
Those are the words.

00:19:49.560 --> 00:19:51.480
And I mentioned the.

00:19:51.480 --> 00:19:54.220
As you remember,
the occurred in all the categories,

00:19:54.230 --> 00:19:56.290
which means it has zero predictive power.

00:19:56.300 --> 00:19:59.560
That's why it sits right
there at the origin.

00:19:59.580 --> 00:20:02.360
I also mentioned they, a console.

00:20:02.390 --> 00:20:04.900
I mentioned that those words
had high predictive power.

00:20:04.900 --> 00:20:08.480
And in fact,
that's why they align along-- they have

00:20:08.490 --> 00:20:15.740
high coordinate along one dimension
and a low coordinate against the other.

00:20:15.740 --> 00:20:18.140
They align against the
principal component,

00:20:18.140 --> 00:20:18.920
if you will.

00:20:18.920 --> 00:20:22.180
And then the rest of the words
fall somewhere in the middle.

00:20:22.180 --> 00:20:26.660
And I'd like to point out that
what and is fall right on top

00:20:26.660 --> 00:20:32.040
of each other in that space,
because of course, they always co-occur.

00:20:32.040 --> 00:20:37.260
And here are the queries and commands,
the four categories, if you will,

00:20:37.440 --> 00:20:41.120
what we would refer to as the
semantic anchors in that case.

00:20:41.150 --> 00:20:44.860
So not surprisingly,
what is the day falls very close to day.

00:20:44.860 --> 00:20:49.940
Because that's the command that
is predicted by it the best.

00:20:50.070 --> 00:20:53.400
And similarly, with console.theMeeting,
the other ones fall

00:20:53.400 --> 00:20:54.500
somewhere in the middle.

00:20:54.680 --> 00:20:58.630
You'll see that they fall
within the vicinity-- well,

00:20:58.660 --> 00:21:01.800
in the vicinity of
their constituent words.

00:21:01.800 --> 00:21:05.100
Now the question is,
what about this new wording,

00:21:05.100 --> 00:21:07.510
this variant that the user introduced?

00:21:07.750 --> 00:21:11.400
What can LSM do to classify it?

00:21:11.410 --> 00:21:13.950
Well, first of all,
it maps it into that space,

00:21:13.950 --> 00:21:15.640
and that's what it's going to do.

00:21:15.740 --> 00:21:17.640
It's going to map it
to the data that falls.

00:21:17.860 --> 00:21:19.640
Again, that's an actual example.

00:21:19.640 --> 00:21:22.610
And so you can see now that
when we look at the distances

00:21:22.640 --> 00:21:26.520
to the various semantic anchors,
we see that the query that would be

00:21:26.520 --> 00:21:31.260
predicted on the basis of this mapping
would be what time is the meeting.

00:21:31.260 --> 00:21:35.000
That's where the latent part of
latent semantic analysis comes in.

00:21:35.000 --> 00:21:38.760
Because in a sense, that means that in a
very data-driven way,

00:21:38.760 --> 00:21:42.820
what the system has figured
out is that when is a synonym

00:21:43.410 --> 00:21:47.800
So that's pretty close to magic.

00:21:47.800 --> 00:21:51.740
But as-- Thank you.

00:21:54.070 --> 00:21:58.490
But as Kim mentioned at the beginning,
it's not all magic.

00:21:58.570 --> 00:22:04.460
There is some very sound mathematical
principles behind all of this.

00:22:04.460 --> 00:22:08.380
And I'm going to try to
give you a flavor of that.

00:22:08.410 --> 00:22:13.440
The basic information that we rely
on when we do this maneuvering is

00:22:13.440 --> 00:22:19.030
we are trying to see how often each
word appears in each category and

00:22:19.030 --> 00:22:23.600
contrast this with how often each word
appears in the entire training data.

00:22:23.650 --> 00:22:26.040
That's kind of the
bread and butter of LSM.

00:22:26.260 --> 00:22:28.840
And we arrange that information.

00:22:29.040 --> 00:22:31.840
So we go through each of the
words in the training data,

00:22:31.840 --> 00:22:34.380
and we try to keep track
of this information.

00:22:34.490 --> 00:22:37.800
And then we arrange it in a matrix,
which looks something like this.

00:22:37.900 --> 00:22:44.650
So here we have M is the size
of the underlying vocabulary.

00:22:44.660 --> 00:22:48.300
That's how many unique words do you
have in your training collection.

00:22:48.390 --> 00:22:53.200
And then you arrange
them in rows like this.

00:22:53.200 --> 00:22:55.760
Then N would be the number
of categories you have.

00:22:55.770 --> 00:22:58.890
So in a previous example, I had four.

00:22:58.910 --> 00:23:03.660
And so you arrange now your
categories in columns like this.

00:23:03.660 --> 00:23:08.620
And so each category now is
sort of a vector of numbers,

00:23:08.620 --> 00:23:15.290
each number representing roughly the
number of times that this particular

00:23:15.290 --> 00:23:22.800
word occurred in this particular
category with some appropriate weighting.

00:23:22.800 --> 00:23:29.020
So this gives us a representation
of each word and each document.

00:23:29.020 --> 00:23:34.240
Each word is sort of a row of numbers,
and each category is a column of number.

00:23:34.260 --> 00:23:38.100
However, you'll notice they don't evolve
in the same spaces because they

00:23:38.100 --> 00:23:39.360
all have the same dimensions.

00:23:39.360 --> 00:23:43.580
And furthermore,
those numbers are mostly-- I mean,

00:23:43.580 --> 00:23:46.650
many of them are zero because, after all,
most words don't appear

00:23:46.650 --> 00:23:49.340
in most documents,
in most categories.

00:23:49.340 --> 00:23:53.300
And so we're going to go
through a particular trick here,

00:23:53.300 --> 00:23:56.490
which is from linear algebra,
which is called singular

00:23:56.490 --> 00:23:59.050
value decomposition,
which we're going to take that matrix--

00:23:59.080 --> 00:24:07.220
let's call it W-- and then we're going to
decompose it onto different sub-matrices.

00:24:07.680 --> 00:24:11.210
And the important thing to
realize is that each word,

00:24:11.210 --> 00:24:18.500
which was at the beginning represented
by a long vector here of sparse vectors,

00:24:18.510 --> 00:24:22.930
is now going to be associated
with a vector of now dense values.

00:24:22.950 --> 00:24:26.730
And similarly,
each category here is now associated

00:24:26.730 --> 00:24:29.540
with a vector of dense values here.

00:24:29.560 --> 00:24:31.640
That's all that matters.

00:24:31.700 --> 00:24:35.920
And the important thing is now r,
which is the number of dimensions

00:24:36.000 --> 00:24:41.460
that are retained by this procedure,
is now much smaller than either m,

00:24:41.470 --> 00:24:45.120
the size of a vocabulary, or n,
the number of categories.

00:24:45.120 --> 00:24:50.720
That's really the power of this mapping,
if you will.

00:24:50.750 --> 00:24:55.320
And so when we do this-- the LSM space
that we had before-- remember,

00:24:55.320 --> 00:24:57.600
we had a bunch of documents,
and we were mapping it

00:24:57.600 --> 00:24:59.220
into categories and so on.

00:24:59.370 --> 00:25:01.140
And then we had a new document.

00:25:01.140 --> 00:25:03.100
We were classifying it and so on.

00:25:03.110 --> 00:25:05.600
Well,
it turns out we can also extract all

00:25:05.600 --> 00:25:07.200
the words appearing in the document.

00:25:07.200 --> 00:25:13.100
And as part of this decomposition,
we also have values for the words,

00:25:13.100 --> 00:25:14.440
which are embedded in that space.

00:25:14.440 --> 00:25:17.120
So now we can ask ourselves
several questions.

00:25:17.120 --> 00:25:23.520
Given this representation
for the new-- document here,

00:25:23.630 --> 00:25:28.900
we can say, well,
which is the topic of this new document?

00:25:29.010 --> 00:25:34.040
And the answer would be
the closest category,

00:25:34.070 --> 00:25:35.700
just like we saw for junk mail.

00:25:35.820 --> 00:25:39.890
But we could also ask ourselves,
what is the single word that

00:25:39.890 --> 00:25:41.870
best describes that document?

00:25:41.950 --> 00:25:48.170
And the answer would be the
closest word in the LSM space.

00:25:48.540 --> 00:25:50.380
That's in a nutshell.

00:25:50.400 --> 00:25:51.040
Thank you.

00:25:51.380 --> 00:25:57.260
That's what LSM is all about-- computing
distances in a mathematical space.

00:25:57.630 --> 00:26:03.800
Now, of course,
that sounds like pretty neat technology,

00:26:03.800 --> 00:26:05.180
and so on, so forth.

00:26:05.180 --> 00:26:07.220
However, there are some caveats.

00:26:07.380 --> 00:26:10.360
This is not going to solve
all the problems of the world.

00:26:10.550 --> 00:26:15.400
For example, LSM has a somewhat
limited descriptive power,

00:26:15.400 --> 00:26:18.410
in the sense that, for example,
the semantic part of

00:26:18.620 --> 00:26:22.510
it is heavily tied to,
well, co-occurrences between words.

00:26:22.690 --> 00:26:27.590
So that's what we call shallow in an
artificial intelligence kind of way,

00:26:27.590 --> 00:26:31.860
as opposed to deep parsing
and things of that nature.

00:26:31.860 --> 00:26:36.800
The second limiting factor,
or characteristic of LSM,

00:26:36.840 --> 00:26:39.630
turns out it could be a
strength also in some ways,

00:26:39.760 --> 00:26:45.270
is that word order is ignored,
meaning that it doesn't matter that

00:26:45.500 --> 00:26:50.430
two words co-occur near each other,
or one is at the beginning and the

00:26:50.430 --> 00:26:53.080
other one is at the end of a document.

00:26:53.420 --> 00:26:55.380
It doesn't matter as
long as they co-occur.

00:26:55.380 --> 00:26:55.880
So.

00:26:56.060 --> 00:26:59.330
In fact-- In fact,
they could appear in reversed order,

00:26:59.330 --> 00:27:00.320
and so on, so forth.

00:27:00.320 --> 00:27:01.440
That wouldn't matter.

00:27:01.570 --> 00:27:05.000
And of course, this decomposition
that I've talked about,

00:27:05.000 --> 00:27:08.690
the singular value decomposition,
that's a linear algebra operation.

00:27:08.770 --> 00:27:11.480
So it assumes that everything
is nice and linear.

00:27:11.600 --> 00:27:13.160
And of course, the world is not linear.

00:27:13.160 --> 00:27:19.690
So you could also bump
into some limitation there.

00:27:20.160 --> 00:27:25.060
Another thing to note is that
LSM is a statistical method.

00:27:25.130 --> 00:27:30.860
And as-- As such-- As such,
it works as well as the training

00:27:30.860 --> 00:27:32.860
data on which it is trained.

00:27:33.090 --> 00:27:40.300
And so, for example,
I mentioned the bank and interest and

00:27:40.300 --> 00:27:43.040
river at the beginning of the talk.

00:27:43.070 --> 00:27:46.230
Well, imagine now that we have
a single document in which

00:27:46.480 --> 00:27:52.000
river bank appears and Bank of
America appears in the same document.

00:27:52.030 --> 00:27:56.250
That would be very difficult to
infer from that document which

00:27:56.290 --> 00:28:00.700
bank we are talking about here,
because the two senses would co-occur.

00:28:00.880 --> 00:28:04.440
So again, it's very important to have
good training data so that the

00:28:04.440 --> 00:28:06.700
categories are well separated.

00:28:06.900 --> 00:28:12.890
And it turns out that this method is
also somewhat sensitive to writing style.

00:28:12.920 --> 00:28:16.400
You can certainly appreciate that
the style in which you write an email

00:28:16.400 --> 00:28:20.580
would be very different from the
style in which Mac OS Help is written,

00:28:20.580 --> 00:28:21.310
for example.

00:28:21.590 --> 00:28:29.110
And this is also-- this influences
the information that LSM finds.

00:28:29.630 --> 00:28:35.120
And finally, I just want to mention
that in some applications,

00:28:35.200 --> 00:28:41.740
there is some training cost in doing
this singular value decomposition,

00:28:41.750 --> 00:28:46.090
especially if the
underlying matrix is large.

00:28:46.090 --> 00:28:49.880
So that's also something to keep in mind.

00:28:49.880 --> 00:28:50.610
Of course,
in an application like junk mail,

00:28:50.610 --> 00:28:50.610
that wasn't an issue because
the dimension was so small.

00:28:50.920 --> 00:28:56.850
So in the rest of my contribution here,
I just would like to go through

00:28:57.000 --> 00:29:03.820
a couple of guidelines to let you
know what you can do's and don'ts

00:29:03.820 --> 00:29:07.070
of being successful with LSM.

00:29:07.210 --> 00:29:12.500
So the first, and perhaps more important,
is that you have this cool technology

00:29:12.500 --> 00:29:15.320
that you've learned about at WWDC.

00:29:15.320 --> 00:29:19.300
You want to go back and try it
on the application you have.

00:29:19.300 --> 00:29:23.470
Well,
is it really the right tool for the job?

00:29:23.740 --> 00:29:24.650
That's the first question.

00:29:24.760 --> 00:29:25.860
Can LSM handle the task?

00:29:26.040 --> 00:29:30.360
If the problem you're trying to
solve is syntactic in nature,

00:29:30.400 --> 00:29:33.920
such as finding dates or
times or email addresses,

00:29:34.120 --> 00:29:39.980
or whether it's Objective-C versus C,
right, Matthias?

00:29:39.980 --> 00:29:43.090
Then that's probably
not the right approach,

00:29:43.150 --> 00:29:46.400
because you can use grep.

00:29:46.450 --> 00:29:49.300
So you have to make sure that the
problem is semantic in nature.

00:29:49.300 --> 00:29:52.820
So if your problem is,
I have a bunch of documents.

00:29:52.820 --> 00:29:54.540
I'd like to sort them by topic.

00:29:54.600 --> 00:29:56.050
Now that's semantic.

00:29:56.200 --> 00:30:02.090
So that's a problem for which
LSM would be well indicated.

00:30:02.230 --> 00:30:03.720
So that's the first guideline.

00:30:04.010 --> 00:30:10.800
Ask yourself,
is the task really suitable for LSM?

00:30:11.470 --> 00:30:14.450
Second is-- and I've already
touched upon that-- are the

00:30:14.530 --> 00:30:16.420
categories distinct enough?

00:30:16.420 --> 00:30:19.520
After all, you're trying to classify,
which means you're trying to have

00:30:19.530 --> 00:30:23.600
the categories as different as
possible in order to do a good job.

00:30:23.600 --> 00:30:26.330
If they overlap, you can't do a good job.

00:30:26.350 --> 00:30:28.560
And so, for example,
if you had a bunch of documents

00:30:28.560 --> 00:30:31.970
talking about economy and a bunch
of documents talking about business,

00:30:31.970 --> 00:30:34.640
and you would like to separate them,
yeah,

00:30:34.860 --> 00:30:37.580
probably can't do a very good job of it.

00:30:37.610 --> 00:30:41.590
However, if you have a bunch of documents
talking about economy versus a bunch of

00:30:41.590 --> 00:30:45.340
documents talking about entertainment,
well, sure enough,

00:30:45.360 --> 00:30:48.320
you'll have the odd document
that sort of overlaps.

00:30:48.320 --> 00:30:52.200
But by and large, you'll do a much better
job of classifying.

00:30:52.330 --> 00:30:55.780
So here, that's the second guideline.

00:30:55.800 --> 00:30:59.780
Now, I'd like to say a few
words about training.

00:31:00.310 --> 00:31:05.040
In terms of both the quality
of the training data,

00:31:05.070 --> 00:31:07.910
that is, the training data,
as I've already mentioned

00:31:07.910 --> 00:31:11.530
it a little bit,
it has to be representative of the

00:31:11.730 --> 00:31:14.640
variety that you can find in the domain.

00:31:14.840 --> 00:31:17.970
So if you forget something,
like in the example

00:31:17.970 --> 00:31:20.580
I had at the beginning,
if you don't have a category

00:31:20.700 --> 00:31:22.540
for computer science,
well,

00:31:22.740 --> 00:31:28.950
obviously you're not going to be able to
classify documents on computer science.

00:31:29.140 --> 00:31:31.920
The data has to be clean for
the purpose of classification,

00:31:31.920 --> 00:31:35.680
meaning that you have to remove
things that don't matter,

00:31:35.680 --> 00:31:37.740
for example, HTML tags.

00:31:37.740 --> 00:31:40.320
If you happen to have them,
they are not germane

00:31:40.320 --> 00:31:42.790
to the classification,
why keep them?

00:31:42.980 --> 00:31:48.250
Fixing typos is sometimes important,
sometimes not, depending.

00:31:48.260 --> 00:31:52.940
In the Jack May filter,
we kept the typos, of course.

00:31:53.030 --> 00:31:58.380
And also,
the various categories have to be--

00:31:58.380 --> 00:32:01.650
the training data-- the training data
has to be balanced in each of them.

00:32:01.660 --> 00:32:05.220
That's important because, remember,
you're trying to compare the number

00:32:05.220 --> 00:32:10.300
of times that words appear in each
category versus the number of times

00:32:10.300 --> 00:32:12.560
they appear in the entire training data.

00:32:12.710 --> 00:32:17.760
So the more balanced the training
data is across all the categories,

00:32:17.830 --> 00:32:20.180
the better,
the more unbiased you're going

00:32:20.180 --> 00:32:22.600
to be in the classification.

00:32:23.300 --> 00:32:25.500
So that was the quality.

00:32:25.500 --> 00:32:28.680
It's also important to talk
about quantity of training data.

00:32:28.840 --> 00:32:33.120
Of course, it has to be large enough
to cover all the variability

00:32:33.120 --> 00:32:35.240
that you expect in your data.

00:32:35.310 --> 00:32:39.980
And for a junk mail filter localization,
for example,

00:32:39.980 --> 00:32:46.580
our rule of thumb was that we wanted
to have about 30,000 unique words

00:32:46.850 --> 00:32:50.970
in the underlying vocabulary across
the entire training collection.

00:32:51.310 --> 00:32:56.800
That's the M, the size of the vocabulary,
the M value that I was

00:32:56.830 --> 00:32:59.000
talking about before.

00:32:59.110 --> 00:33:02.940
Now, of course,
this value has to be larger

00:33:02.940 --> 00:33:07.720
as more categories are added,
and larger still if the

00:33:07.720 --> 00:33:12.440
data changes over time,
because again, like news,

00:33:12.440 --> 00:33:12.440
you have new information
coming in all the time.

00:33:12.440 --> 00:33:12.440
You want to be able to
take advantage of it.

00:33:13.350 --> 00:33:16.180
Now in terms of testing,
it's also important.

00:33:16.180 --> 00:33:19.450
Again, this is a statistical method,
so you have to test the

00:33:19.580 --> 00:33:22.110
performance of the technique.

00:33:22.170 --> 00:33:25.750
And the way to do that is
to use validation data.

00:33:26.260 --> 00:33:29.940
So the way we like to do things at
Apple is we take the training data,

00:33:29.940 --> 00:33:32.820
we partition it into chunks,
typically 10,

00:33:32.820 --> 00:33:38.900
and then we train on the first nine and
test on the last and see what happens.

00:33:39.100 --> 00:33:41.950
The last chunk is called
the held-out data.

00:33:41.960 --> 00:33:45.060
It's important to have held-out data
because if you test on the training,

00:33:45.060 --> 00:33:47.910
you cannot predict the performance
you're going to get on something

00:33:47.920 --> 00:33:49.210
you haven't seen before.

00:33:49.220 --> 00:33:52.930
And in fact,
we even go further than that.

00:33:53.030 --> 00:33:55.950
We also repeat sequentially
on all those chunks.

00:33:56.100 --> 00:33:57.120
It's called round-robin.

00:33:57.120 --> 00:34:00.990
So now you train on the last nine
and you test on the first and so on,

00:34:00.990 --> 00:34:02.400
and you permutate.

00:34:02.400 --> 00:34:05.020
And then we average all the results,
and that gives us a better

00:34:05.020 --> 00:34:06.520
predictor of the performance.

00:34:08.820 --> 00:34:13.900
now what if you do all those things and
the outcome looks still somewhat weird

00:34:13.910 --> 00:34:18.130
well first of all check the previous
guidelines you know was was the task

00:34:18.140 --> 00:34:23.080
appropriate for lsm and so on so forth
another thing is to try with a short

00:34:23.080 --> 00:34:28.860
stop word list there are words like the
or in or of that are likely to appear

00:34:28.950 --> 00:34:35.050
pretty much in the same way across all
the categories and they tend to not be

00:34:35.050 --> 00:34:39.640
very informative and furthermore they
tend to muddy the results so sometimes

00:34:39.670 --> 00:34:44.100
we get better results by removing them
try experimenting with the number of

00:34:44.100 --> 00:34:51.380
dimensions that are retained because
sometimes like the the default that we

00:34:51.500 --> 00:34:57.360
set in the in the in the framework is the
number r that i was talking about before

00:34:57.360 --> 00:35:01.600
defaults to the number of categories so
in the junkbait filter for example that

00:35:01.600 --> 00:35:06.010
would be two but for natural language
problems like the ones that kidasana

00:35:06.010 --> 00:35:13.020
is going to talk about later typically
we want r to be between 100 and 300.

00:35:13.580 --> 00:35:18.400
Finally, because remember LSM doesn't
keep track of world order,

00:35:18.400 --> 00:35:23.980
so sometimes it's valuable to keep
track of the most frequent world pairs.

00:35:24.130 --> 00:35:27.330
For example,
you can appreciate that a blowtorch,

00:35:27.330 --> 00:35:30.510
for example,
may occur in a context which is quite

00:35:30.510 --> 00:35:33.380
different from just blow and just torch.

00:35:33.380 --> 00:35:36.350
I had actually a better
example with blow,

00:35:36.350 --> 00:35:38.980
but it was forbidden to use it here.

00:35:41.200 --> 00:35:45.880
Same thing, if it's important to use
New York City as a single entity,

00:35:46.190 --> 00:35:48.580
then by all means add it to the data.

00:35:48.580 --> 00:35:53.240
But make sure that you do this sparingly,
because obviously this will impact

00:35:53.410 --> 00:35:56.990
the dimension of your matrix,
and therefore it could impact

00:35:57.060 --> 00:35:58.550
the computational cost.

00:36:00.220 --> 00:36:02.870
And that's pretty much
all I wanted to say.

00:36:03.120 --> 00:36:05.000
One more thing.

00:36:05.120 --> 00:36:10.510
And that is that it's often to your
advantage to integrate LSM with

00:36:10.510 --> 00:36:12.160
other sources of knowledge.

00:36:12.320 --> 00:36:15.630
Because of the semantic
approach that LSM takes,

00:36:15.720 --> 00:36:19.380
it tends to complement other techniques
that are not based on semantics.

00:36:19.470 --> 00:36:23.600
And so it can often improve the
robustness of the overall system if

00:36:23.680 --> 00:36:25.840
you combine LSM with other things.

00:36:25.970 --> 00:36:27.920
And I'd like to offer two examples.

00:36:28.090 --> 00:36:31.590
The junk mail filter complements,
instead of replacing,

00:36:31.590 --> 00:36:35.930
things that are normally
done in a spam filtering,

00:36:35.930 --> 00:36:38.730
like whitelist and blacklist
and unwritten rule.

00:36:38.870 --> 00:36:43.240
The junk mail filter comes
after all of those things,

00:36:43.340 --> 00:36:44.890
not instead of.

00:36:45.040 --> 00:36:46.900
It's just an additional
source of knowledge.

00:36:46.990 --> 00:36:50.000
And similarly,
in the Kanata-Kanji conversion,

00:36:50.000 --> 00:36:55.600
which we're going to talk to
you about in a little bit,

00:36:55.600 --> 00:36:58.020
LSM is just one of several
sources of information.

00:36:58.020 --> 00:37:01.470
information that is being exploited.

00:37:01.690 --> 00:37:08.180
And so that concludes what I wanted
to say about how LSM works,

00:37:08.790 --> 00:37:14.390
and I'd like to turn it
over to Matthias once again,

00:37:14.390 --> 00:37:14.390
who's going to talk to us
about how to use the API.

00:37:14.390 --> 00:37:14.390
Thank you.

00:37:15.100 --> 00:37:17.100
Thank you very much, Jerome.

00:37:17.100 --> 00:37:19.900
Thank you very much, Jerome.

00:37:19.900 --> 00:37:24.440
Now you've heard how it works.

00:37:24.440 --> 00:37:25.590
Now we are going to hear
how to make it work for you.

00:37:26.880 --> 00:37:29.320
Now, the first point I would
like to reiterate,

00:37:29.320 --> 00:37:33.550
it's always cheaper not to
use the API than to use it.

00:37:33.600 --> 00:37:37.410
It's often a lot easier to
use the command line tool to

00:37:37.470 --> 00:37:40.360
prototype instead of using the API.

00:37:40.360 --> 00:37:44.110
However, at some point,
if it works for you, you're going to want

00:37:44.200 --> 00:37:45.680
to use the actual API.

00:37:45.680 --> 00:37:50.490
Our API is core foundation-based,
so our main data types

00:37:50.490 --> 00:37:53.000
are core foundation types.

00:37:53.120 --> 00:37:54.930
You can retain them,
you can release them,

00:37:54.930 --> 00:37:56.540
you can put them into containers.

00:37:56.800 --> 00:38:04.210
Our three types in the
API are the LSM map,

00:38:04.630 --> 00:38:06.350
so you see...

00:38:06.830 --> 00:38:11.310
Latent Semantic Mapping really revolves
around the lifecycle of an LSM map.

00:38:11.450 --> 00:38:20.390
You create the map,
you train it by adding data to it,

00:38:20.490 --> 00:38:22.180
you compile it,
then you evaluate it with data,

00:38:22.180 --> 00:38:22.180
you can archive it to disk,
load it back in.

00:38:22.430 --> 00:38:27.410
Whenever you're dealing with data,
whether it's for training

00:38:27.410 --> 00:38:27.410
or for evaluation,

00:38:28.530 --> 00:38:35.140
You're using an LSM text to represent
the document that you're going to

00:38:35.190 --> 00:38:37.040
use for training or for evaluation.

00:38:37.180 --> 00:38:40.750
And finally, on evaluation,
you're getting back an

00:38:40.750 --> 00:38:43.080
LSM result from the evaluation.

00:38:43.230 --> 00:38:48.230
So let's start at the
beginning of the lifecycle.

00:38:51.220 --> 00:39:00.180
You create an LSM map,
and here we finally come to the

00:39:00.180 --> 00:39:00.180
all-important blue boxes on the slides.

00:39:00.180 --> 00:39:00.180
You create an LSM map.

00:39:00.270 --> 00:39:04.720
By calling LSM Map Create,
and like any creation API in

00:39:04.740 --> 00:39:09.390
the core foundation space,
you can pass in any allocator,

00:39:09.440 --> 00:39:14.080
but for most purposes it's good
to pass in the default allocator,

00:39:14.080 --> 00:39:17.200
also affectionately known
by its friends as null.

00:39:17.200 --> 00:39:22.450
You can optionally pass
in a number of flags.

00:39:22.600 --> 00:39:27.280
Here, as an example,
we've passed in the flag map pairs,

00:39:27.280 --> 00:39:33.920
which will allow you to map a
pair of words like blowtorch as a

00:39:33.920 --> 00:39:36.240
single unit and keep track of that.

00:39:36.320 --> 00:39:38.980
In most cases,
you don't actually need these flags,

00:39:39.140 --> 00:39:40.590
so it's okay to pass in zero.

00:39:43.030 --> 00:39:45.300
Now you've created an empty map.

00:39:45.460 --> 00:39:47.500
Your next job is to fill it up.

00:39:47.740 --> 00:39:52.650
You add categories by
calling lsm_map_add_category.

00:39:52.650 --> 00:39:56.810
You're going to get back an integer
which identifies the category.

00:39:56.980 --> 00:40:04.050
You then add data to the
map by creating an lsm_text,

00:40:04.380 --> 00:40:08.690
by adding a number of words to
the text and having them parsed.

00:40:08.840 --> 00:40:12.330
And then by adding the text to the
map in one particular category.

00:40:12.480 --> 00:40:15.730
Now let's look at that in
a little bit more detail.

00:40:15.890 --> 00:40:20.060
The text you add is
passed in as a CFString.

00:40:20.220 --> 00:40:26.900
You also can pass in a locale identifier,
but we would not necessarily

00:40:26.900 --> 00:40:28.640
advise you to do so.

00:40:28.640 --> 00:40:32.630
The default is going
to work out quite well.

00:40:32.940 --> 00:40:37.200
And furthermore,
like most of the LSM calls,

00:40:37.200 --> 00:40:40.390
you can pass in a number of flags.

00:40:40.390 --> 00:40:45.440
Here we're passing in a flag,
LSM text preserve case,

00:40:45.530 --> 00:40:50.730
which means instead of just
mapping everything to lowercase

00:40:51.270 --> 00:40:55.490
as we normally would in this case,
we would like to preserve the case.

00:40:55.490 --> 00:40:55.490
We're going to talk about
some other flags later.

00:40:55.790 --> 00:40:58.830
Once you're done adding words,
you add the text to the map.

00:40:58.980 --> 00:41:03.540
Here we say,
"This text is one of the good guys.

00:41:03.570 --> 00:41:07.780
This belongs in the good category."
So we add it to the good category.

00:41:07.860 --> 00:41:12.400
Once we've done that,
we have no more need for the text,

00:41:12.400 --> 00:41:12.400
we can release it.

00:41:12.670 --> 00:41:16.320
Now, this is the case where
LSM parses the text for you.

00:41:16.320 --> 00:41:20.140
There might be more subtle
situations where you would like

00:41:20.140 --> 00:41:22.660
to do the parsing yourself,
in which case,

00:41:22.660 --> 00:41:26.090
instead of calling add words,
you call add word,

00:41:26.160 --> 00:41:30.580
which doesn't do any case mapping,
doesn't do any transformation whatsoever.

00:41:30.580 --> 00:41:35.920
It just takes the CFString you
pass into it as the gospel truth,

00:41:36.030 --> 00:41:38.300
adds it to the map,
or adds it to the text.

00:41:38.300 --> 00:41:41.030
You don't even have to use words.

00:41:41.320 --> 00:41:45.160
You can also, if necessary,
you can use binary tokens,

00:41:45.180 --> 00:41:49.130
because deep down inside,
LSM doesn't care what you

00:41:49.130 --> 00:41:51.210
give it as identifiers.

00:41:51.220 --> 00:41:53.910
All that matters is an
identity comparison.

00:41:53.980 --> 00:41:58.280
Does it compare equal
to another token or not?

00:41:58.400 --> 00:42:01.880
For that,
you have to call LSM text add token.

00:42:05.180 --> 00:42:06.990
Once you're done training,
you compile the map.

00:42:07.130 --> 00:42:08.640
This is LSM map compile.

00:42:08.640 --> 00:42:13.320
The example I've shown to you,
this takes a couple hundred milliseconds,

00:42:13.320 --> 00:42:15.030
or maybe a couple milliseconds.

00:42:15.140 --> 00:42:18.150
In mail,
after each message gets evaluated,

00:42:18.150 --> 00:42:22.190
when you click the chunk button,
the map gets recompiled.

00:42:23.040 --> 00:42:26.370
I very much doubt that any of you
have ever sat in front of your

00:42:26.540 --> 00:42:30.190
computer and twiddled your thumbs
while the map was recompiled.

00:42:30.770 --> 00:42:34.550
However, I should point out that in
natural language problems,

00:42:34.550 --> 00:42:38.700
when there are large and sparse maps,
that can actually take an

00:42:38.890 --> 00:42:40.930
appreciable amount of time.

00:42:43.620 --> 00:42:47.330
Now you have a compiled map,
now you're in a situation to evaluate.

00:42:47.500 --> 00:42:50.740
For evaluating,
first thing you do is create

00:42:50.830 --> 00:42:53.500
a text that looks exactly
the same way as for training.

00:42:53.500 --> 00:42:58.250
You create the text, you add words to it,
you add single words to it,

00:42:58.250 --> 00:42:59.500
you add tokens to it.

00:42:59.500 --> 00:43:03.330
And finally,
the all-important evaluation result

00:43:03.330 --> 00:43:07.500
you get by calling lsm_result_create.

00:43:07.500 --> 00:43:11.810
You pass to it the map,
you pass the text you

00:43:11.910 --> 00:43:13.500
want to evaluate against,

00:43:14.570 --> 00:43:19.130
You pass the maximum number of
categories you would like in the results.

00:43:19.240 --> 00:43:21.950
If you're kind of have a
winner-takes-all mentality,

00:43:21.950 --> 00:43:24.490
then passing in one would
be the right thing to do.

00:43:24.700 --> 00:43:27.030
If you're always
second-guessing yourself,

00:43:27.030 --> 00:43:28.310
you're passing in two.

00:43:28.480 --> 00:43:34.380
And if you want to know
the total rankings,

00:43:34.420 --> 00:43:35.870
you're maybe passing in a thousand
if you have a thousand categories.

00:43:36.520 --> 00:43:39.920
Again,
there is a final argument for flags,

00:43:40.090 --> 00:43:44.430
which in most cases are
not going to be used,

00:43:44.560 --> 00:43:49.710
but we are going to present one situation
where they are going to be used.

00:43:50.310 --> 00:43:52.600
At that point you have no
longer a need for the text,

00:43:52.600 --> 00:43:55.830
but you're going to have to extract
what the result actually says.

00:43:55.840 --> 00:43:59.840
So, you're calling routines
like get_category,

00:43:59.840 --> 00:44:04.900
which takes the result and it
takes the index into the result.

00:44:05.120 --> 00:44:11.170
So, get_category result 0 gives you
back the number of the index of the

00:44:11.170 --> 00:44:14.440
best category of that evaluation.

00:44:14.580 --> 00:44:19.320
You can also get the floating point score
that we've seen here in this example,

00:44:19.470 --> 00:44:25.320
again with an index of 0 to
get the best of those scores.

00:44:25.490 --> 00:44:28.840
Passing non-zero indexes to
get second best and so on.

00:44:29.020 --> 00:44:33.330
And finally, when you get tired of that,
you throw away the result.

00:44:34.870 --> 00:44:38.350
If you feel after evaluating
that you want to add more data,

00:44:38.450 --> 00:44:44.440
for instance, in the junk mail example,
you're all trained up and yet you see

00:44:44.450 --> 00:44:49.760
another spam or you see a false positive,
you click the junk or not junk button.

00:44:49.820 --> 00:44:55.980
The map we call LSM Map Start Training to
put the map back into training state so

00:44:55.980 --> 00:44:58.900
you can add more training data to it.

00:44:59.060 --> 00:45:02.380
And once you're done,
you call compile again.

00:45:02.810 --> 00:45:08.320
Similarly, in many situations,
you don't want to compute

00:45:08.360 --> 00:45:11.380
the map on the fly.

00:45:11.400 --> 00:45:17.790
Once it's computed, you want to store it.

00:45:17.800 --> 00:45:17.800
So you write it out with lsm_map_write
to URL and load it back in

00:45:17.800 --> 00:45:17.800
with lsm_map_create from URL.

00:45:18.570 --> 00:45:22.440
In some situations,
you know exactly that the only reason

00:45:22.550 --> 00:45:27.600
you're loading the map is because you
immediately want to add more data,

00:45:27.600 --> 00:45:32.040
in which case you can pass in
the flag KLSM Map Load Mutable.

00:45:32.080 --> 00:45:35.750
Normally, when you load from a URL,
you're getting back the

00:45:35.750 --> 00:45:38.830
map in evaluation state,
where you can evaluate.

00:45:39.090 --> 00:45:44.320
If you say KLSM Map Load Mutable,
you're getting it back in training state,

00:45:44.400 --> 00:45:48.500
which in some situations may
save you time and memory.

00:45:50.560 --> 00:45:51.980
So that's all there is to the API.

00:45:52.040 --> 00:45:55.660
There are a number more calls,
there are properties you can set,

00:45:55.950 --> 00:46:00.890
but all of that is pretty easily
discoverable from the documentation

00:46:00.900 --> 00:46:04.820
you have received and you should
have with your seed on Xcode.

00:46:04.900 --> 00:46:08.960
So how have we used
LateN Semantic Mapping internally?

00:46:08.990 --> 00:46:11.940
Our first application was
using LateN Semantic Mapping

00:46:12.040 --> 00:46:14.600
for junk mail filtering,
which was an application

00:46:14.630 --> 00:46:18.980
designed by my colleagues Devang
Naik and Jerome Bellegarda.

00:46:19.140 --> 00:46:21.620
So here, in a way,
it's a very simple application.

00:46:21.620 --> 00:46:22.830
You just have two categories.

00:46:22.910 --> 00:46:26.500
You have legitimate
mail versus junk mail.

00:46:26.560 --> 00:46:28.250
There are a couple of subtleties here.

00:46:28.450 --> 00:46:35.540
First of all, getting a false positive,
having a mail that is terribly important

00:46:35.540 --> 00:46:39.420
to you classified as junk mail,
is a worse situation than having the

00:46:39.420 --> 00:46:41.460
occasional junk mail slip through.

00:46:41.500 --> 00:46:44.050
Or at least for many people it is.

00:46:44.210 --> 00:46:48.270
So therefore we're not strictly
going by the top result,

00:46:48.270 --> 00:46:49.000
being junk.

00:46:49.060 --> 00:46:52.570
We're also going to test that
the top result has a confidence

00:46:52.570 --> 00:46:56.440
score that is higher than just
the 50% it would need to win.

00:46:56.550 --> 00:46:59.940
We want a little bit
level of extra confidence.

00:46:59.940 --> 00:47:03.700
Second, as Jerome said,
we're going to use multiple

00:47:03.970 --> 00:47:05.360
sources of knowledge.

00:47:05.410 --> 00:47:09.740
And one of these sources is if the sender
of a mail appears in your address book,

00:47:09.740 --> 00:47:12.790
then we're always going to
err on the side of caution and

00:47:12.830 --> 00:47:14.580
think it's a legitimate mail.

00:47:14.580 --> 00:47:17.140
Which, as we all know,
is not always true.

00:47:17.140 --> 00:47:18.870
But we certainly don't want that.

00:47:18.900 --> 00:47:24.000
We don't want to toss a mail
and afterwards it turns out that

00:47:24.000 --> 00:47:28.370
you have actually won a prize,
or you were supposed

00:47:28.400 --> 00:47:32.820
to come to a meeting,
you have finally secured your mortgage.

00:47:32.900 --> 00:47:37.020
So we're going to use the
address book as an indicator

00:47:37.100 --> 00:47:39.860
whether a mail is legitimate.

00:47:40.900 --> 00:47:45.420
Now, junk mail filtering is also
especially tricky in one regard.

00:47:45.470 --> 00:47:49.100
In many semantic applications,
the semantics are just

00:47:49.190 --> 00:47:51.040
waiting to be discovered.

00:47:51.040 --> 00:47:53.360
This is not the case
in junk mail filtering.

00:47:53.360 --> 00:47:58.230
The junk mail senders are going to
great lengths to disguise the fact

00:47:58.360 --> 00:48:00.060
that they are sending unwanted mail.

00:48:00.060 --> 00:48:03.220
They're not going to proclaim,
this mail here is unwanted.

00:48:03.300 --> 00:48:10.040
So, one of the things they do is they
insert punctuation into their words.

00:48:10.790 --> 00:48:14.320
You might have seen this,
a dot after each character.

00:48:14.320 --> 00:48:19.390
Or they use, for an accent,
they use what we call in the trade

00:48:19.650 --> 00:48:22.620
the heavy metal band syndrome.

00:48:22.620 --> 00:48:28.020
You insert a couple of extra umlauts and
accents in the hope that your users are

00:48:28.020 --> 00:48:31.960
still going to be able to read the text,
but that the junk mail filter

00:48:31.960 --> 00:48:33.120
is going to be thrown off.

00:48:34.460 --> 00:48:45.240
For that, to counteract that,
one of the flags we have for LSM text ad

00:48:45.240 --> 00:48:45.240
words is LSM text apply spam heuristics.

00:48:45.270 --> 00:48:48.680
goes through the text with a somewhat,
with an eye to treating

00:48:48.740 --> 00:48:52.530
the text as hostile text,
it's going to make a guess that if

00:48:52.530 --> 00:48:56.780
it's seeing periods between characters,
it's a longer word that is

00:48:56.920 --> 00:48:58.640
just broken up this way.

00:48:58.640 --> 00:49:03.020
Or if it sees weird characters that are,
kind of look like letters,

00:49:03.020 --> 00:49:06.500
like the copyright sign,
but are not usually treated as letters,

00:49:06.610 --> 00:49:08.820
it treats them as letters anyway.

00:49:08.820 --> 00:49:11.810
Nice thing is,
this makes the mail even better

00:49:11.840 --> 00:49:15.940
trainable because these weird
spellings are going to stand out

00:49:15.940 --> 00:49:18.150
like a sore thumb in training.

00:49:18.290 --> 00:49:21.930
Another problem we're dealing with
here is that the map potentially

00:49:21.930 --> 00:49:26.530
contains a lot of offensive words,
and we don't want a third grader armed

00:49:26.670 --> 00:49:30.280
with cat or less to look at the map and
discover all these words that aren't

00:49:30.410 --> 00:49:32.020
supposed to be in his vocabulary yet.

00:49:32.200 --> 00:49:35.520
So we have a simple flag,
KLSM Map Hash Text,

00:49:35.520 --> 00:49:39.190
which is not industrial-grade
cryptography,

00:49:39.220 --> 00:49:46.200
but it does the job to secure the
map from the most casual prying eyes.

00:49:46.400 --> 00:49:50.010
Finally, you're not really restricted to
using just the text of the mail.

00:49:50.200 --> 00:49:55.070
You could use meta information
if you have a tiny one-by-one

00:49:55.190 --> 00:50:00.200
picture pointed to by a URL,
what is referred to as a web bug,

00:50:00.200 --> 00:50:02.200
I think,
to track whether the message gets read.

00:50:02.460 --> 00:50:05.630
That's a piece of information
you can just add to the map

00:50:05.630 --> 00:50:09.310
as a pseudo-word tiny picture,
because that's something

00:50:09.310 --> 00:50:11.120
that is very trainable.

00:50:11.790 --> 00:50:14.380
Overall,
this was still a relatively simple

00:50:14.380 --> 00:50:17.060
application of Latent Semantic Mapping.

00:50:17.060 --> 00:50:23.120
For a much more involved application,
I would like to bring up

00:50:23.120 --> 00:50:29.160
my colleague Kira San,
the manager of OS Engineering Tokyo,

00:50:29.210 --> 00:50:32.490
to talk about Japanese input.

00:50:32.490 --> 00:50:32.490
Kira San?

00:50:35.260 --> 00:50:37.700
Good morning, no, good afternoon.

00:50:37.700 --> 00:50:42.900
My name is Yasuo Kida,
and everybody calls me Kidasan.

00:50:42.900 --> 00:50:47.500
I'm a manager of OS Engineering Tokyo of
Frameworks Group.

00:50:47.510 --> 00:50:53.190
I'll first explain a little bit about
how Japanese input method work first,

00:50:53.430 --> 00:50:58.030
and the challenge we faced,
and how we took the advantage

00:50:58.030 --> 00:51:01.310
of LSM to tackle the challenge.

00:51:02.440 --> 00:51:07.530
You might know Japanese language
uses thousands of characters,

00:51:07.530 --> 00:51:10.840
and it doesn't fit in a keyboard.

00:51:10.840 --> 00:51:12.970
So how we enter them?

00:51:13.040 --> 00:51:16.710
Well,
Japanese input method uses a process

00:51:17.730 --> 00:51:20.570
called kana-kanji conversion.

00:51:20.570 --> 00:51:26.990
You enter pronunciation using
phonetic characters called kana,

00:51:26.990 --> 00:51:28.920
and the input method converts
the input into kanji.

00:51:29.540 --> 00:51:35.280
An issue is there's so many
possible conversions and you

00:51:35.280 --> 00:51:37.000
need to pick the correct one.

00:51:37.200 --> 00:51:41.620
For example,
when you enter the pronunciation,

00:51:41.690 --> 00:51:49.490
those are actual possibilities and the
input method chooses the correct one.

00:51:49.720 --> 00:51:54.840
This is similar to voice recognition
or handwriting recognition.

00:51:55.570 --> 00:52:01.030
It can never be 100% correct,
but accuracy is critical for

00:52:01.030 --> 00:52:03.480
the usability of the system.

00:52:05.610 --> 00:52:17.850
So, a Japanese input method called Koteri
calculates the probability like this.

00:52:17.850 --> 00:52:17.850
It uses a model called Class N-gram.

00:52:18.430 --> 00:52:30.200
It looks up each word in the possible
conversion from the dictionary

00:52:30.200 --> 00:52:30.200
and get the frequency of it.

00:52:30.590 --> 00:52:36.210
And also, it looks at part of speech,
probability of part of speech

00:52:36.750 --> 00:52:41.970
connecting each other in a conversion,
and multiply them together to get

00:52:41.970 --> 00:52:44.840
the probability of a conversion.

00:52:44.860 --> 00:52:49.940
In actual calculation,
we use a value called cost,

00:52:50.320 --> 00:52:54.010
which takes logarithm of
the probability value.

00:52:54.160 --> 00:52:57.720
So by doing that,
we can add integer together,

00:52:57.720 --> 00:53:01.260
add integer together,
instead of multiplying

00:53:01.640 --> 00:53:03.480
floating point numbers.

00:53:05.740 --> 00:53:07.870
This model works pretty well.

00:53:08.000 --> 00:53:17.920
However, there are ambiguities that
cannot be solved by Engram,

00:53:17.920 --> 00:53:17.920
because Engram looks
at neighboring words.

00:53:18.230 --> 00:53:23.770
For example, in English, if you say,
"What a beautiful tail!"

00:53:23.890 --> 00:53:28.700
Is it about the most part of
an animal or about the story?

00:53:29.100 --> 00:53:30.770
You don't know.

00:53:32.000 --> 00:53:44.250
This ambiguity can be resolved by
knowing a topic of the discourse.

00:53:44.250 --> 00:53:44.250
This is where Latent
Semantic Mapping comes in.

00:53:45.500 --> 00:53:57.990
For example,
if a word "princess" appears in the text,

00:53:58.600 --> 00:54:00.630
most likely the tale is about the
story instead of the tale of an animal.

00:54:02.320 --> 00:54:07.420
So we built an LSM map that does this.

00:54:07.480 --> 00:54:14.130
We first prepared a Japanese corpus
containing 300,000 documents,

00:54:14.130 --> 00:54:17.950
and we clustered them
into 3,000 categories.

00:54:18.210 --> 00:54:24.860
The purpose is to capture the
topic in appropriate resolution.

00:54:25.210 --> 00:54:31.580
And then we removed words that are
low frequency and removed words that

00:54:31.690 --> 00:54:34.660
appear equally in many categories.

00:54:34.670 --> 00:54:39.900
For example,
word this or that gives little

00:54:39.900 --> 00:54:42.390
information about the topic.

00:54:42.570 --> 00:54:52.680
And we set the dimension of
the LSM map 200 like this code.

00:54:52.680 --> 00:54:57.150
We called LSM map set
properties with the number 200.

00:54:58.810 --> 00:55:03.280
You might wonder how we came
up with those magic numbers,

00:55:03.320 --> 00:55:09.120
like category 3,000 categories
or 200 dimensions and so on.

00:55:09.270 --> 00:55:15.680
We iterated measurements,
number of categories, 2,000 to 5,000,

00:55:15.680 --> 00:55:19.420
threshold for removing
words and dimensions,

00:55:19.430 --> 00:55:25.140
and some other parameters in order
to optimize the conversion accuracy

00:55:25.280 --> 00:55:28.180
as well as the size of the LSM map.

00:55:28.220 --> 00:55:35.470
The size is important because the
size of the LSM map tends to become

00:55:35.530 --> 00:55:39.210
very big when you have a large corpus.

00:55:39.330 --> 00:55:43.300
So you want to make the
map space efficient.

00:55:45.760 --> 00:55:53.990
At runtime, Kotori captures what
you have typed so far,

00:55:54.140 --> 00:56:00.580
and that history is
kept for each document.

00:56:00.580 --> 00:56:07.390
And when it converts an input stream,
it uses LSM to obtain words that

00:56:07.390 --> 00:56:07.390
best match the word's history.

00:56:09.140 --> 00:56:12.330
For example,
when you have typed "performance",

00:56:12.490 --> 00:56:19.270
"violin", and so on so far,
the LSM map might return those words

00:56:19.270 --> 00:56:27.760
like "music", "competition", and
"soprano", and "cella", and so on.

00:56:27.880 --> 00:56:31.940
This is a real example that was
returned from Kotori LSM map,

00:56:31.940 --> 00:56:31.940
of course translated to English.

00:56:34.070 --> 00:56:37.970
And we adjusted the cost of
probable words that appear

00:56:38.030 --> 00:56:42.570
in the possible conversions.

00:56:43.190 --> 00:56:48.180
And by doing that,
the probability of each word is now

00:56:48.180 --> 00:56:51.410
aligned with the topic of the discourse.

00:56:51.540 --> 00:56:57.400
And consequently,
the probability of the whole

00:56:57.400 --> 00:57:01.080
conversion reflects the topic.

00:57:01.200 --> 00:57:09.140
So in a word, LSM boosts the probability
of words that most align with

00:57:09.410 --> 00:57:11.500
the topic of the discourse.

00:57:11.650 --> 00:57:15.090
And this is exactly what humans do.

00:57:15.820 --> 00:57:19.010
And the result?

00:57:19.140 --> 00:57:27.920
Kotori version 4 that took advantage
of LSM was ranked number one in

00:57:27.920 --> 00:57:35.410
conversion accuracy on a benchmark
conducted by an independent research.

00:57:35.410 --> 00:57:35.410
So, thank you.

00:57:39.150 --> 00:57:48.290
At the end of the day,
the improvement achieved by

00:57:48.290 --> 00:57:56.400
adding LSM is relatively small,
but it was still enough

00:57:56.400 --> 00:57:56.400
to pass the competitions.

00:57:57.100 --> 00:58:03.340
I hope this gives you a hint of how
you could take advantage of LSM.

00:58:03.370 --> 00:58:08.580
Thank you.

00:58:08.580 --> 00:58:08.580
Now, Matthias, welcome to the stage.

00:58:09.000 --> 00:58:11.150
Thank you very much.

00:58:11.190 --> 00:58:13.400
Thank you, Kida-san.

00:58:13.530 --> 00:58:15.450
So what has LSM done for you lately?

00:58:15.680 --> 00:58:18.420
Maybe it has thrown away
a junk mail for you.

00:58:18.420 --> 00:58:22.080
Maybe it has converted some
kana into kanji for you.

00:58:22.110 --> 00:58:25.480
But more importantly,
what can LSM do for you?

00:58:25.510 --> 00:58:28.320
What can it do for your application?

00:58:28.530 --> 00:58:31.570
Latent Semantic Mapping can
categorize lots of things.

00:58:31.570 --> 00:58:32.910
Think of bookmarks.

00:58:33.200 --> 00:58:34.990
Right now,

00:58:35.410 --> 00:58:40.090
In Safari, you're hitting "Add this to my
bookmarks folder" and you're

00:58:40.090 --> 00:58:44.300
going to get a pop-up menu that
always starts at the same place.

00:58:44.490 --> 00:58:54.230
How about if your web browser fetches
all the web pages in your bookmarks menu,

00:58:54.340 --> 00:58:57.690
categorizes them,
and uses that to propose the

00:58:57.690 --> 00:58:57.690
category to add your bookmark into?

00:58:57.860 --> 00:58:59.690
You could do the same
thing for RSS feeds.

00:58:59.720 --> 00:59:04.250
RSS readers are, I think,
a very fluent category right now,

00:59:04.530 --> 00:59:06.500
fluid category right now.

00:59:06.550 --> 00:59:08.800
You could use it if you
had a library application.

00:59:08.800 --> 00:59:15.500
You could use it to categorize books
or CDs or DVDs by fetching book

00:59:15.550 --> 00:59:19.350
reviews from a site to categorize.

00:59:19.370 --> 00:59:22.750
If you're a yuppie and would like to keep
track of your wine and cheese database,

00:59:23.310 --> 00:59:23.870
Be welcome.

00:59:23.920 --> 00:59:33.820
If you're a biologist and would like
to categorize your DNA sequence,

00:59:33.820 --> 00:59:33.820
are you a man or a mouse?

00:59:33.820 --> 00:59:33.820
LateN Semantic Mapping
might be able to tell you.

00:59:34.710 --> 00:59:38.330
You're limited only by your
imagination for what you can

00:59:38.450 --> 00:59:42.320
use LateN Semantic Mapping for,
and people often ask us,

00:59:42.320 --> 00:59:47.830
"Is this a suitable field to apply
LateN Semantic Mapping for?" Ultimately,

00:59:47.920 --> 00:59:51.270
it comes down to the tools
are relatively easy to try.

00:59:51.570 --> 00:59:55.590
Try it for yourself,
find out whether it works for you.

00:59:57.310 --> 01:00:02.460
For more information,
talk to John Galainzi,

01:00:02.580 --> 01:00:05.450
our technology manager.

01:00:05.450 --> 01:00:09.100
There is a public mailing
list as of today or tomorrow

01:00:09.100 --> 01:00:10.980
that you can subscribe to.

01:00:11.150 --> 01:00:13.670
And as usual, we have sample code.

01:00:13.820 --> 01:00:20.490
We have a brand new sample code
application written by my colleague,

01:00:20.490 --> 01:00:20.490
Jia Pu.