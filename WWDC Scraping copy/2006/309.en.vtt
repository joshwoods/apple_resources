WEBVTT

00:00:19.580 --> 00:00:23.360
And we'd like to tell you today
about the Performance Analysis and

00:00:23.360 --> 00:00:25.900
Debugging Tools in Mac OS X.

00:00:25.950 --> 00:00:30.300
We'll show you some case studies of
using those tools to do optimization,

00:00:30.350 --> 00:00:32.630
and we'll talk about some
exciting new directions that

00:00:32.630 --> 00:00:35.110
we're taking the tools this year.

00:00:35.820 --> 00:00:39.460
So first off, why are we here?

00:00:39.460 --> 00:00:40.900
Why worry about performance?

00:00:41.020 --> 00:00:43.970
It's real easy to forget
about it in that last rush to

00:00:43.970 --> 00:00:48.100
get the product out the door,
but it is a competitive selling point.

00:00:48.100 --> 00:00:50.160
Hopefully we all care about it.

00:00:50.280 --> 00:00:55.340
I'm on the Xcode team,
I certainly hear about it.

00:00:55.530 --> 00:01:01.610
It's also easy for performance problems
for you not to really notice them and

00:01:01.610 --> 00:01:05.410
maybe your customers use the product
in a slightly different way than you

00:01:05.410 --> 00:01:07.070
did and they start seeing issues.

00:01:07.080 --> 00:01:11.120
It can be hard to fix major
performance issues at the very

00:01:11.120 --> 00:01:13.620
end of your development cycle.

00:01:13.620 --> 00:01:19.010
So you want to be sure that you're
using a development process that allows

00:01:19.010 --> 00:01:23.620
you to monitor and fix performance
throughout your development cycle.

00:01:23.620 --> 00:01:27.620
There's a lot of stuff going
on in the systems these days.

00:01:27.620 --> 00:01:33.620
FS event monitoring for time machine,
spotlight in the background.

00:01:33.620 --> 00:01:38.380
Users want a lot of capabilities so
we want to make sure that we're using

00:01:38.380 --> 00:01:40.620
the system to its best capacity.

00:01:40.620 --> 00:01:42.620
And it's not just speed.

00:01:42.620 --> 00:01:45.390
Users want a responsive system.

00:01:45.890 --> 00:01:47.620
They want scalability.

00:01:47.620 --> 00:01:51.620
If they toss ten times more
data at it than you ever did,

00:01:51.620 --> 00:01:53.620
are your N squared algorithms
going to be able to do that?

00:01:53.620 --> 00:01:56.620
Are they going to go out of control?

00:01:56.620 --> 00:01:58.570
Memory use is a big deal.

00:01:58.690 --> 00:02:03.620
If you are testing on a two gigabyte
maxed out development system and then

00:02:03.620 --> 00:02:08.390
your users work with it on 512 megabytes,
are you going to be paging a lot

00:02:08.390 --> 00:02:10.620
and thus having a lot of problems?

00:02:10.830 --> 00:02:12.600
And battery life.

00:02:12.680 --> 00:02:16.620
It's a huge issue now that we're
selling more and more laptop computers.

00:02:16.630 --> 00:02:18.430
You saw the sales numbers.

00:02:18.640 --> 00:02:22.130
We want those batteries to last through
an entire plane flight and play all

00:02:22.130 --> 00:02:23.620
our movies and listen to iTunes.

00:02:23.620 --> 00:02:25.580
So, important things.

00:02:25.620 --> 00:02:30.620
So in this session we're going to look
at Shark for profiling your applications.

00:02:30.730 --> 00:02:33.620
We'll look at how to
analyze 64 bit binaries.

00:02:33.620 --> 00:02:37.620
And we'll talk some more about
X-Ray that you heard about on Monday.

00:02:37.620 --> 00:02:40.720
And specifically in this session,
how it makes use of

00:02:40.760 --> 00:02:42.620
D-Trace under the covers.

00:02:44.470 --> 00:02:48.300
So the flagship analysis
applications that we've got at

00:02:48.300 --> 00:02:51.400
this point are Shark and X-Ray.

00:02:51.400 --> 00:02:54.390
Shark,
hopefully you're all familiar with it.

00:02:54.580 --> 00:02:59.330
It's great for profiling your
application or your entire system

00:02:59.440 --> 00:03:04.460
to show where the hotspots in the
application are and help you fix those,

00:03:04.660 --> 00:03:07.380
including giving
suggestions for fixing them.

00:03:07.400 --> 00:03:12.310
X-Ray is a new direction for us that
really lets you go in and peek beneath

00:03:12.560 --> 00:03:17.630
the covers to understand what's going
on in a way that we haven't been

00:03:17.630 --> 00:03:20.680
able to look at processes before.

00:03:21.620 --> 00:03:26.730
These are part of the overall
performance tools suite for Mac OS X that

00:03:26.730 --> 00:03:28.160
we've been shipping for a while.

00:03:28.160 --> 00:03:29.620
X-Ray is new this year.

00:03:29.620 --> 00:03:32.980
With these tools,
we can monitor performance so we

00:03:33.100 --> 00:03:36.410
can see when there's an issue,
and we can analyze it to

00:03:36.470 --> 00:03:38.420
understand why there's an issue.

00:03:38.420 --> 00:03:42.000
So they cover various areas
like execution behavior,

00:03:42.100 --> 00:03:43.560
CPU speed, I.O.

00:03:43.560 --> 00:03:48.690
They cover memory use,
and there's tools for analyzing graphics.

00:03:49.060 --> 00:03:51.810
So hopefully most of you are
familiar with most of these.

00:03:51.820 --> 00:03:56.150
The tools like Top, Big Top,
and Activity Monitor for looking

00:03:56.150 --> 00:03:58.710
at overall system performance.

00:03:58.720 --> 00:04:02.290
Quartz Debug to help you figure out,
are you drawing too many

00:04:02.290 --> 00:04:03.640
times to the screen?

00:04:03.640 --> 00:04:05.240
Really help visualize that.

00:04:05.280 --> 00:04:09.810
And then the best tools on
the system now for analyzing,

00:04:09.820 --> 00:04:11.720
Shark and now X-Ray.

00:04:11.720 --> 00:04:16.200
For memory use,
Object Alloc is still probably the best.

00:04:16.200 --> 00:04:18.120
Malloc Debug is still there too.

00:04:18.900 --> 00:04:21.180
X-Ray is getting some of
these capabilities now,

00:04:21.270 --> 00:04:23.540
and we've got command line tools as well.

00:04:23.640 --> 00:04:27.840
These ship with the
X-Code tools releases,

00:04:28.450 --> 00:04:32.640
providing full support for Mac OS X,
all of our major environments, Cocoa,

00:04:32.640 --> 00:04:37.120
Carbon, Unix apps,
and all the major languages.

00:04:37.140 --> 00:04:42.470
Objective C, C, C++, Java,
and Shark supports Fortran as well.

00:04:42.540 --> 00:04:45.380
The graphical apps are
integrated with X-Code,

00:04:45.410 --> 00:04:48.740
so you can make this part
of your development process.

00:04:48.740 --> 00:04:53.340
So here to tell you more about Shark,
I'd like to bring on Rick Galthor.

00:04:53.340 --> 00:04:54.610
Rick Galthor: Hi, Rick.

00:05:05.100 --> 00:05:10.200
We're going to cover briefly what
Shark is and what it can be used for,

00:05:10.200 --> 00:05:13.220
and how you actually might go
about using it in your workflow,

00:05:13.220 --> 00:05:15.480
and some new features for this year.

00:05:17.190 --> 00:05:17.850
So what is Shark?

00:05:17.920 --> 00:05:19.200
Well, it's a profiling tool.

00:05:19.200 --> 00:05:20.480
So what does that mean?

00:05:20.480 --> 00:05:25.160
Well, it's a tool to help you find the
performance characteristics of

00:05:25.160 --> 00:05:29.560
your application as well as the
performance bottlenecks within it.

00:05:29.690 --> 00:05:34.430
So you can actually find why you
aren't running as fast as you could.

00:05:34.430 --> 00:05:37.760
Shark supports a wide
variety of languages.

00:05:37.760 --> 00:05:41.250
Almost every language that
compiles down to the native

00:05:41.250 --> 00:05:43.400
machine we can do analysis on.

00:05:43.400 --> 00:05:47.210
We actually support source line
information for many of the compilers.

00:05:47.360 --> 00:05:50.380
We also offer things as
both GUI and command line.

00:05:50.430 --> 00:05:52.740
This is really helpful.

00:05:52.740 --> 00:05:56.440
We actually, the command line version is
especially useful for doing things

00:05:56.530 --> 00:05:58.360
like automated regression suites.

00:05:58.490 --> 00:06:01.160
You can actually have it
collect Shark sessions on these

00:06:01.160 --> 00:06:05.410
suites and come back and look
at them if you see problems.

00:06:06.020 --> 00:06:10.100
So we make a big deal about
Shark being very simple to use.

00:06:10.420 --> 00:06:13.510
Performance shouldn't be painful.

00:06:13.550 --> 00:06:16.550
To this end, we actually like to call
it one-click profiling.

00:06:16.630 --> 00:06:18.680
You really have one button
that you need to worry about,

00:06:18.680 --> 00:06:21.330
and that's a start button.

00:06:21.380 --> 00:06:23.670
Pushing this button,
you instantly are collecting

00:06:23.670 --> 00:06:24.620
profile information.

00:06:24.660 --> 00:06:26.660
You don't even have to do anything else.

00:06:26.730 --> 00:06:29.120
30 seconds later, a session will appear.

00:06:29.150 --> 00:06:30.930
It'll show you what was happening.

00:06:31.150 --> 00:06:34.210
And from there, you can go on your way.

00:06:34.310 --> 00:06:37.950
We also realize that not every
time do you need to look at

00:06:37.960 --> 00:06:39.860
just one type of information.

00:06:39.860 --> 00:06:42.200
So we also offer many
different configurations.

00:06:42.350 --> 00:06:45.180
This lets you change
what you're looking at,

00:06:45.180 --> 00:06:47.950
what type of characteristics
you're interested in.

00:06:48.090 --> 00:06:52.000
And we also offer a number of bundled
presets so that you don't have to

00:06:52.220 --> 00:06:53.850
actually create your own configurations.

00:06:53.860 --> 00:06:57.580
You can go for the most common
types of things right off the bat.

00:06:58.720 --> 00:07:00.960
And we also let you narrow your scope.

00:07:01.060 --> 00:07:02.860
Sometimes you want to
look at the whole system,

00:07:02.900 --> 00:07:04.060
sometimes you don't.

00:07:04.150 --> 00:07:07.780
So we'll cover this in a little
bit more detail in a little bit.

00:07:08.100 --> 00:07:10.730
So it's really simple to use,
but one of the important things is

00:07:10.810 --> 00:07:12.440
that it's actually really powerful,
too.

00:07:12.440 --> 00:07:15.240
You can have an easy tool
that doesn't help a whole lot.

00:07:15.400 --> 00:07:17.930
In this case,
we end up with Shark brings the

00:07:17.930 --> 00:07:21.120
performance bottlenecks right
to the top of your session.

00:07:21.120 --> 00:07:25.160
It shows you this is where
you have something happening.

00:07:25.160 --> 00:07:28.320
We do this so you don't
have to go hunting for it.

00:07:28.360 --> 00:07:29.670
It brings it right to you.

00:07:29.680 --> 00:07:35.400
We also find the code within each
function that is the important part.

00:07:35.520 --> 00:07:37.940
For example,
you can see this line of source

00:07:37.940 --> 00:07:41.720
code has 45% of the samples,
and we actually change the background

00:07:41.720 --> 00:07:43.790
color to bring this up and highlight it.

00:07:43.890 --> 00:07:47.260
In fact, when you open the function,
it'll jump right to that line

00:07:47.260 --> 00:07:50.780
to get you looking at where are
you spending a lot of your time.

00:07:50.780 --> 00:07:56.840
We also offer tips, performance tips,
in the sessions so that you

00:07:56.910 --> 00:08:00.020
can simply look at these tips,
and they encapsulate a lot

00:08:00.020 --> 00:08:02.700
of the common performance
bottlenecks that you run into,

00:08:02.700 --> 00:08:05.420
and they offer suggestions as to
how you might go about fixing this.

00:08:05.460 --> 00:08:07.360
in your application.

00:08:07.820 --> 00:08:09.980
And the really important
thing with Shark is it's very,

00:08:09.980 --> 00:08:11.200
very, very low overhead.

00:08:11.200 --> 00:08:13.740
Most of the time,
you won't even know that Shark was

00:08:13.790 --> 00:08:15.150
running during the session.

00:08:15.160 --> 00:08:16.140
It won't even show up.

00:08:16.320 --> 00:08:19.200
Because you don't really want
to look at what Shark's doing.

00:08:19.200 --> 00:08:20.600
You want to look at what
your application's doing.

00:08:20.600 --> 00:08:23.640
So we strive to keep our
overhead as low as possible.

00:08:25.460 --> 00:08:29.740
So let's talk a little bit about how
you might use Shark in your application.

00:08:29.760 --> 00:08:33.600
The first thing you need to think about
is what exactly do you want to look at?

00:08:33.600 --> 00:08:35.600
What type of information
do you want to collect?

00:08:35.600 --> 00:08:39.240
So three of our most common
profile types are time profile,

00:08:39.280 --> 00:08:41.320
system trace, and static analysis.

00:08:41.320 --> 00:08:43.740
So let's go ahead and
take a look at these.

00:08:43.740 --> 00:08:47.700
Time profile is what you might
consider a classic profile.

00:08:47.700 --> 00:08:51.260
It's a look at what was actually
executing on the processor

00:08:51.260 --> 00:08:53.050
at given time intervals.

00:08:53.680 --> 00:08:57.160
And we aggregate that and give you a
session and show you exactly where your

00:08:57.160 --> 00:08:58.740
time was spent within your application.

00:08:58.740 --> 00:09:01.220
We show you not only
what function it was in,

00:09:01.220 --> 00:09:04.390
which is what the self column is,
it shows how many samples

00:09:04.390 --> 00:09:06.720
were in that function,
but we also aggregate

00:09:06.820 --> 00:09:08.150
it through call stacks.

00:09:08.210 --> 00:09:11.440
So you can actually look and see, yes,
there were 79% of the

00:09:11.440 --> 00:09:15.300
samples in this function,
but what different call paths did it come

00:09:15.400 --> 00:09:17.910
from and which one contributes the most?

00:09:17.920 --> 00:09:22.760
Because some functions may only
actually see a lot of samples

00:09:22.760 --> 00:09:23.550
in one particular function.

00:09:23.580 --> 00:09:26.000
invocation area.

00:09:26.410 --> 00:09:30.210
So this is really helpful for
finding CPU-bound problems,

00:09:30.210 --> 00:09:33.520
but sometimes you're actually
running into performance bottlenecks

00:09:33.520 --> 00:09:34.530
interacting with the system.

00:09:34.540 --> 00:09:41.810
So I have a thing we call System Trace,
and it looks at the boundary between

00:09:41.810 --> 00:09:44.190
the system and your application.

00:09:44.200 --> 00:09:47.860
System calls, VM faults,
a lot of things that can

00:09:47.860 --> 00:09:50.970
be performance bottlenecks,
and they're usually

00:09:50.970 --> 00:09:51.900
pretty tricky to find.

00:09:52.600 --> 00:09:55.100
So we offer a couple views.

00:09:55.100 --> 00:09:58.200
The first one is the summary view,
and it shows where are you actually

00:09:58.200 --> 00:09:59.700
spending time during this trace?

00:09:59.700 --> 00:10:01.070
Is it in the kernel?

00:10:01.100 --> 00:10:02.380
Is it in your code?

00:10:02.380 --> 00:10:03.840
Is it in VM faults?

00:10:03.840 --> 00:10:05.820
Where are we actually
spending a lot of this time?

00:10:07.260 --> 00:10:10.000
But sometimes that's not sufficient,
and you actually want to

00:10:10.000 --> 00:10:12.010
see what really happened.

00:10:12.090 --> 00:10:15.500
So we have the timeline view,
which actually shows what

00:10:15.540 --> 00:10:19.720
CPU was executing what
thread in a graphical format,

00:10:19.850 --> 00:10:23.120
as well as what events occurred
during this time sequence.

00:10:23.220 --> 00:10:26.710
So you can actually look
at each individual event.

00:10:26.820 --> 00:10:28.520
And not only do we show
you that an event happened,

00:10:28.520 --> 00:10:31.180
but for many of them,
we show you a call stack of what in

00:10:31.180 --> 00:10:33.700
your application caused that event.

00:10:33.880 --> 00:10:36.120
Now, this doesn't work on
everything-- for example,

00:10:36.120 --> 00:10:36.840
VM faults.

00:10:36.950 --> 00:10:38.500
But for system calls
and things like that,

00:10:38.500 --> 00:10:41.500
you can see how you got to it,
because the system call name itself

00:10:41.500 --> 00:10:43.720
might not be what you actually called.

00:10:43.720 --> 00:10:47.170
For example, in this case,
you see that our application, Flurry,

00:10:47.430 --> 00:10:51.210
actually made a call through Foundation,
which ended up in LibSystem,

00:10:51.210 --> 00:10:53.070
which then made a system call.

00:10:54.840 --> 00:10:58.020
And static analysis is yet
another way of looking at things.

00:10:58.140 --> 00:11:03.360
It's a way to look at the binary
without actually running it.

00:11:03.540 --> 00:11:08.440
So you may have an isolated section
of your executable where you're not

00:11:08.490 --> 00:11:11.350
tracking down the runtime problems.

00:11:11.360 --> 00:11:13.230
You're just trying to figure out,
how can I optimize this

00:11:13.310 --> 00:11:16.670
function to the maximum?

00:11:17.030 --> 00:11:20.420
And what this does is it runs our
analysis engine and pulls up the

00:11:20.420 --> 00:11:23.220
same performance tips that you
would see in the runtime behavior

00:11:23.550 --> 00:11:24.520
without actually running it.

00:11:24.630 --> 00:11:27.750
It just looks at the file on disk.

00:11:28.830 --> 00:11:32.650
So you've picked what
you want to look at,

00:11:32.650 --> 00:11:35.570
and now you have to pick what on
the system you want to look at.

00:11:35.600 --> 00:11:37.560
You know the type of information.

00:11:37.690 --> 00:11:42.100
So we have three different ways of--
or three different scopes of sampling.

00:11:42.400 --> 00:11:46.390
Some of the configurations work
better with certain types of sampling.

00:11:46.720 --> 00:11:49.520
For example,
System Trace is very oriented towards

00:11:49.520 --> 00:11:50.750
looking at the system as a whole.

00:11:50.760 --> 00:11:53.540
You don't particularly want to
look at an individual process.

00:11:53.540 --> 00:11:55.230
You might after you
collect the information,

00:11:55.260 --> 00:11:58.300
but it actually collects what
happens with the OS as a whole.

00:11:58.300 --> 00:12:03.420
Time Profile can actually be used in both
everything and an individual process.

00:12:03.420 --> 00:12:06.130
You can pinpoint it on just your
application and see what happens.

00:12:06.140 --> 00:12:09.580
File, on the other hand,
lets you actually look at

00:12:09.580 --> 00:12:11.280
just the file and disk.

00:12:11.340 --> 00:12:14.060
And this is really useful for static
analysis where you're looking at

00:12:14.190 --> 00:12:17.760
potentially what the compiler output
is or if we can see any problems

00:12:17.760 --> 00:12:21.720
with custom rolled assembler,
various types of things like that.

00:12:24.400 --> 00:13:58.900
[Transcript missing]

00:14:05.100 --> 00:14:06.660
Thanks, Rick.

00:14:06.750 --> 00:14:08.650
So, as he said,
my name is Ryan Du Bois and I'm here

00:14:08.650 --> 00:14:10.730
to tell you what's new with Shark.

00:14:11.070 --> 00:14:15.240
We spent the last year adding some
scalability and robustness to Shark as

00:14:15.240 --> 00:14:18.920
well as a host of new features and
I'm going to highlight five of them.

00:14:20.180 --> 00:14:23.340
First feature I'd like to
talk about is UTF-8 support.

00:14:23.340 --> 00:14:25.300
As some of you may know,
the old Shark had some

00:14:25.300 --> 00:14:29.030
trouble with source files,
application names, and framework names

00:14:29.030 --> 00:14:30.560
with UTF-8 characters.

00:14:30.600 --> 00:14:32.430
That is now a thing of the past.

00:14:32.500 --> 00:14:35.830
The new Shark supports all
these things out of the box.

00:14:36.270 --> 00:14:38.190
The next really exciting
feature I'd like to talk about:

00:14:38.290 --> 00:14:40.900
64-bit profiling.

00:14:40.950 --> 00:14:45.000
We've added completely universal
support for profiling 64-bit binaries.

00:14:45.040 --> 00:14:49.600
It allows you to get above the
4-gigabyte line for symbols and code.

00:14:49.640 --> 00:14:54.340
This will be instrumental in performance
profiling your Leopard applications

00:14:54.340 --> 00:14:59.490
because the frameworks and the code are
going to be loaded up above 4 gigabytes.

00:14:59.840 --> 00:15:02.990
We've also added
EM64T disassembly support.

00:15:03.010 --> 00:15:08.300
You can see the syntax in Intel or AT&T,
whichever is more comfortable for you.

00:15:08.460 --> 00:15:11.790
And there's even an integrated
instruction set reference.

00:15:12.000 --> 00:15:15.250
As you'll notice from this
screenshot of a time profile,

00:15:15.250 --> 00:15:18.080
the addresses here are
above the 4 gigabyte line,

00:15:18.080 --> 00:15:21.170
and it's using 64-bit registers.

00:15:21.390 --> 00:15:24.760
So I'd like to give you a
quick demo of this in action.

00:15:24.860 --> 00:15:27.500
So we can switch over
to the demo machine.

00:15:35.150 --> 00:15:37.900
So this is a little
application called Fractality.

00:15:38.010 --> 00:15:41.600
And what it does is it draws fractals
and allows you to animate them in motion.

00:15:41.730 --> 00:15:44.300
We contacted the developer
of this application.

00:15:44.300 --> 00:15:48.890
He was gracious enough to let us port
it to 64-bit on Intel hardware and

00:15:48.890 --> 00:15:50.810
push it above the 4-gigabyte line.

00:15:51.080 --> 00:15:54.470
Even allowed us to change the
name to Uber Fractality to

00:15:54.470 --> 00:15:57.240
demonstrate the UTF-8 support.

00:15:57.470 --> 00:15:59.670
So I'm going to go ahead and
launch Shark and take you

00:15:59.720 --> 00:16:00.640
through kind of the workflow.

00:16:00.640 --> 00:16:03.180
We're going to pick a time profile.

00:16:03.180 --> 00:16:06.160
We're going to just target fractality.

00:16:06.250 --> 00:16:09.550
And what we're going to do is
go ahead and start it animating.

00:16:10.780 --> 00:16:13.660
So that's what Fractality does.

00:16:13.770 --> 00:16:16.760
Keep your eye on the frame
rate while I'm profiling this.

00:16:16.790 --> 00:16:17.640
I'm just going to press Start.

00:16:17.640 --> 00:16:22.330
You'll notice it doesn't
drop terribly much at all.

00:16:22.360 --> 00:16:24.950
I could have let that run all the way,
but I decided to stop it early.

00:16:25.010 --> 00:16:26.400
And here we go.

00:16:26.420 --> 00:16:29.250
This is the Shark profile.

00:16:29.470 --> 00:16:30.960
If I look at this and
try to interpret it,

00:16:30.960 --> 00:16:33.640
I'll notice there's a
lot of GL stuff going on.

00:16:33.640 --> 00:16:37.500
And that's because this
particular set of drivers does

00:16:37.500 --> 00:16:39.640
not have hardware acceleration.

00:16:39.640 --> 00:16:43.200
However, I do have a save session of
Fractality from a machine

00:16:43.200 --> 00:16:45.730
with hardware acceleration.

00:16:45.780 --> 00:16:47.900
So if we go ahead and
poke open this function,

00:16:47.900 --> 00:16:50.000
the one that comes to the top,

00:16:52.910 --> 00:16:54.760
Make this a little bit bigger.

00:16:54.760 --> 00:16:58.860
You'll notice we're spending most of
our time calculating sine and cosine.

00:16:58.860 --> 00:17:03.360
It takes us right to the line number
of the source code and highlights it,

00:17:03.380 --> 00:17:04.900
as Rick was saying, with the colors.

00:17:04.900 --> 00:17:09.510
If I click over on Assembly,
you'll notice the addresses are

00:17:09.510 --> 00:17:14.780
above the 4 gigabyte line and
it's using 64-bit registers.

00:17:19.020 --> 00:17:24.740
Can we switch back to the slides?

00:17:24.750 --> 00:17:28.200
So the next feature I want to talk
about is Dwarf Debugging Support.

00:17:28.210 --> 00:17:30.640
We've added support for
Dwarf Debugging Format,

00:17:30.640 --> 00:17:33.240
both 32 and 64 bit binaries.

00:17:33.240 --> 00:17:36.670
You can mix and match with STABs,
and we've even added support

00:17:36.740 --> 00:17:38.650
for DSIM files along with it.

00:17:38.660 --> 00:17:41.810
Fractality, as you just saw,
was built using Dwarf symbols,

00:17:41.810 --> 00:17:44.810
so all the source line
information came from Dwarf.

00:17:46.990 --> 00:17:50.900
The next feature I want to talk about is
something we like to call Symbolication.

00:17:50.900 --> 00:17:53.670
So let's say, for example,
you write an application and it

00:17:53.670 --> 00:17:57.350
goes to your QA team before you
notice a performance bottleneck.

00:17:57.420 --> 00:18:01.000
The problem here is the QA team has
it as it's going to be released,

00:18:01.020 --> 00:18:04.900
so they don't have any symbols,
any of the debugging information.

00:18:04.940 --> 00:18:06.870
But they go ahead and take
a Shark session anyway,

00:18:06.870 --> 00:18:08.900
trying to be helpful,
and they email it to you.

00:18:09.130 --> 00:18:12.310
With the old Shark, you would have had no
idea what was going on.

00:18:12.400 --> 00:18:14.850
With Symbolication,
it allows you to point Shark at

00:18:14.850 --> 00:18:18.110
a symbol-rich version of that
same binary and discover the

00:18:18.110 --> 00:18:20.370
symbol names after the fact.

00:18:22.330 --> 00:18:28.200
Next feature I want to talk about is
called the windowed time facility.

00:18:28.200 --> 00:18:31.600
So for another example,
let's say you're writing a video

00:18:31.600 --> 00:18:33.560
player and you're testing it.

00:18:33.560 --> 00:18:34.640
You go to play videos.

00:18:34.640 --> 00:18:37.310
Every once in a while under
certain conditions you'll

00:18:37.310 --> 00:18:38.920
notice you're dropping frames.

00:18:38.940 --> 00:18:40.040
Not quite sure why.

00:18:40.040 --> 00:18:42.550
And the problem is it's hard to repeat.

00:18:42.840 --> 00:18:45.670
With the old Shark,
you would have had to press start,

00:18:45.670 --> 00:18:50.090
hope that the frame dropping was
going to happen in your sample window,

00:18:50.250 --> 00:18:52.400
press stop, and hopefully you got it.

00:18:52.400 --> 00:18:53.330
Kind of a pain.

00:18:53.600 --> 00:18:55.480
That all is now a thing of the past.

00:18:55.480 --> 00:18:59.910
What windowed time facility allows
you to do is press start and forget.

00:19:00.010 --> 00:19:03.720
Shark will continuously collect a
certain limited sample window in

00:19:03.720 --> 00:19:07.520
the background and you press stop
after your target event happens.

00:19:07.520 --> 00:19:10.680
So with the new Shark,
you can just press start,

00:19:10.680 --> 00:19:14.820
set your sample window,
you're watching your movie, going along,

00:19:14.820 --> 00:19:17.260
boom, you notice you're dropping frames.

00:19:17.260 --> 00:19:18.160
What do you do?

00:19:18.160 --> 00:19:20.050
Wait till it's over, you press stop.

00:19:20.050 --> 00:19:23.640
Shark gives you that last window
of samples as your session and you

00:19:23.640 --> 00:19:26.120
can pinpoint exactly what happened.

00:19:32.240 --> 00:19:35.960
We've added support for this in
Time Profile and System Trace.

00:19:35.960 --> 00:19:38.310
There is a windowed time
facility checkbox that allows

00:19:38.350 --> 00:19:42.050
you to set the sample window for
both of these configurations.

00:19:42.060 --> 00:19:45.540
So it should be readily apparent
to you when you go to use it.

00:19:45.650 --> 00:19:49.640
Here to demo that, I have Rob Barris from
Blizzard Entertainment.

00:19:49.640 --> 00:19:51.200
Rob?

00:19:59.220 --> 00:20:00.200
Okay.

00:20:00.200 --> 00:20:03.620
I'm just going to switch
the demo machine here.

00:20:03.670 --> 00:20:05.720
Can everyone hear me okay?

00:20:08.710 --> 00:20:10.850
Fantastic.

00:20:10.930 --> 00:20:16.310
Hi, I'm Robert Barris,
18-time WDC attendee, first-time speaker.

00:20:20.120 --> 00:20:22.490
I run the Mac software group
at Blizzard Entertainment.

00:20:22.500 --> 00:20:25.010
Our most recent Mac product
is called World of Warcraft.

00:20:25.040 --> 00:20:27.890
This is a massively multiplayer
online role-playing game.

00:20:28.060 --> 00:20:32.140
It is a large and complex product,
and we ship updates to it regularly.

00:20:32.170 --> 00:20:36.860
In fact, over 40 updates since it first
came out in the fall of 2004.

00:20:36.900 --> 00:20:41.610
And part of that regular update
process is identifying and correcting

00:20:41.620 --> 00:20:42.960
any performance regressions.

00:20:42.960 --> 00:20:46.070
As this is a product where new
features and capabilities are

00:20:46.070 --> 00:20:49.860
being added with every release,
it's always possible that some new

00:20:49.860 --> 00:20:52.970
behavior sneaks in that is unpleasant.

00:20:53.120 --> 00:20:55.720
So we like to catch those things
and fix them as quickly as we can.

00:20:55.740 --> 00:20:59.360
And Shark has been extremely
valuable to us in identifying

00:20:59.430 --> 00:21:00.660
those types of problems.

00:21:03.300 --> 00:21:06.700
With the new window time facility,
the task of finding transient

00:21:06.790 --> 00:21:11.640
hiccup-type events during
gameplay has become a lot easier.

00:21:11.730 --> 00:21:14.500
I think of the window time
facility as TiVo for profiling.

00:21:14.500 --> 00:21:17.340
As Ryan just explained,
the ability to have the sliding

00:21:17.340 --> 00:21:21.770
window of most recent events kept
stored continuously and to let you

00:21:21.770 --> 00:21:27.040
stop when you identify a problem
really frees you up to go tackle

00:21:27.040 --> 00:21:32.290
some of the more surprise issues
that may occur in your application.

00:21:34.600 --> 00:21:37.760
So let's have a look at World of Warcraft
and see if we can spot one of the little

00:21:37.760 --> 00:21:39.000
hiccups that we'd like to eliminate.

00:21:39.000 --> 00:21:42.160
Due to time limitations today,
we won't be able to actually

00:21:42.160 --> 00:21:45.520
dig into the code and make the
fix and rebuild it and so forth.

00:21:45.560 --> 00:21:49.300
But I think we'll demonstrate the
strength of the window of time

00:21:49.300 --> 00:21:53.040
facility and how it can help you zero
in on something that may be elusive.

00:21:56.780 --> 00:21:58.700
Okay, so I'm going to start up Shark.

00:21:58.700 --> 00:22:02.390
You are visible here, excellent.

00:22:02.860 --> 00:22:04.330
There it is.

00:22:04.350 --> 00:22:07.820
OK, I have it set to the window
time profile version.

00:22:07.920 --> 00:22:09.850
You'll notice that's at
the bottom of the menu.

00:22:09.890 --> 00:22:14.500
It's distinct from the regular time
profile and system trace at the top.

00:22:15.500 --> 00:22:19.310
And the one thing to look out
for is the new sample window.

00:22:19.490 --> 00:22:22.040
So what we have set up here is
we've asked to maintain a rolling

00:22:22.040 --> 00:22:26.280
window of 10,000 recent samples,
each one being a stack crawl done on

00:22:26.280 --> 00:22:29.100
your whole machine at a given moment.

00:22:29.130 --> 00:22:30.870
And they're spaced one millisecond apart.

00:22:30.930 --> 00:22:34.080
So this works out to a 10-second
trailing window of activity,

00:22:34.130 --> 00:22:36.560
which basically means that
if you see something strange,

00:22:36.560 --> 00:22:39.160
you have 10 seconds to hit the hot key,
stop it,

00:22:39.190 --> 00:22:40.580
and have a look at what happened.

00:22:44.960 --> 00:22:46.690
That's not good.

00:22:59.600 --> 00:23:00.740
Log in to our character here.

00:23:00.840 --> 00:23:05.540
Excellent.

00:23:05.540 --> 00:23:07.700
There's my teleporter character.

00:23:07.700 --> 00:23:12.200
She can go all over the world and
see various interesting issues.

00:23:13.150 --> 00:23:14.100
Okay, here we are.

00:23:14.100 --> 00:23:14.700
This is the city of Ironforge.

00:23:14.700 --> 00:23:18.860
It's one of the central hub cities
in our game of World of Warcraft.

00:23:18.880 --> 00:23:21.710
And we're just standing in
front of the griffin trainer.

00:23:21.860 --> 00:23:25.180
This is an in-game character
who can let you hop on a

00:23:25.180 --> 00:23:27.450
griffin and fly to another city.

00:23:27.560 --> 00:23:28.890
So I'm just going to run down there.

00:23:29.000 --> 00:23:31.050
Oh, let's start up the Shark profile.

00:23:31.120 --> 00:23:33.440
This is the fun part.

00:23:33.530 --> 00:23:36.450
So Shark is now continuously
sampling and keeping the last 10

00:23:36.470 --> 00:23:38.250
seconds of activity in its buffer.

00:23:39.020 --> 00:23:41.250
Now, I have seen several times,
and I'm going to try to

00:23:41.250 --> 00:23:44.820
reproduce it right now,
a hiccup when we go through

00:23:44.820 --> 00:23:46.350
the very first little portals.

00:23:46.370 --> 00:23:50.820
We fly up and out of this mining
area and start to leave the city.

00:23:50.820 --> 00:23:52.890
There'll be a little hiccup,
and I'm hoping to catch that

00:23:52.890 --> 00:23:53.880
and show it to you live.

00:23:55.880 --> 00:23:56.790
Here's the trainer.

00:23:56.800 --> 00:23:58.700
I'm going to talk to him.

00:23:58.770 --> 00:24:01.490
Say,
"I would like to fly to this other city."

00:24:01.790 --> 00:24:02.940
And he puts me on the
bird and we start flying.

00:24:02.940 --> 00:24:05.140
And I think it will happen
in the next few seconds.

00:24:05.240 --> 00:24:06.090
There was one.

00:24:06.100 --> 00:24:09.300
I'm expecting another
one right about here.

00:24:09.660 --> 00:24:11.290
There it was.

00:24:11.670 --> 00:24:12.590
Hey, it happened.

00:24:12.590 --> 00:24:13.890
I hit stop on Shark.

00:24:13.900 --> 00:24:18.210
I can actually just hide the game
now because we're done there.

00:24:19.600 --> 00:24:21.200
Shark is analyzing the samples.

00:24:21.200 --> 00:24:24.640
To save time in the demo,
I repeated this process earlier,

00:24:24.640 --> 00:24:29.190
and I have a snapshotted
version of it right here.

00:24:29.640 --> 00:24:31.150
What I want to show is
when it first comes up,

00:24:31.260 --> 00:24:32.200
you'll see something like this.

00:24:32.200 --> 00:24:34.330
You'll see the percentages
spent in your various routines.

00:24:34.340 --> 00:24:36.840
And a lot of these are
very familiar to me.

00:24:36.840 --> 00:24:38.000
They have to do with
rendering the terrain,

00:24:38.000 --> 00:24:41.920
routines inside OpenGL,
internal data structure updating.

00:24:41.920 --> 00:24:45.020
This is like the bread and butter of
what the game is doing frame after frame.

00:24:45.020 --> 00:24:47.900
And when we're running 40,
50 frames a second,

00:24:47.900 --> 00:24:50.680
that's somewhere around
20 milliseconds per frame,

00:24:50.680 --> 00:24:55.840
which corresponds to 20 samples that
Shark took for an average frame.

00:24:55.840 --> 00:24:58.050
And we can go see that right
now in the timeline view,

00:24:58.050 --> 00:24:59.020
one of my favorites.

00:25:00.560 --> 00:25:02.460
If I click on one of these
columns in the timeline view,

00:25:02.460 --> 00:25:04.870
what that's showing is that
the spikes are the depth of

00:25:04.870 --> 00:25:06.610
the stack of your thread.

00:25:06.620 --> 00:25:09.860
You can actually view various threads,
but we're watching the main thread here.

00:25:09.860 --> 00:25:11.830
Each one shows the depth of
your thread at that moment in

00:25:11.830 --> 00:25:13.020
time when the sample was took.

00:25:13.090 --> 00:25:16.080
And if you tap the arrow keys,
you can go from sample to sample.

00:25:16.080 --> 00:25:17.220
Can you see that okay?

00:25:17.220 --> 00:25:18.840
Can everyone see that okay?

00:25:18.840 --> 00:25:23.200
And it will show you the stack
crawl evolving on the right.

00:25:23.220 --> 00:25:26.360
Take notes quickly.

00:25:26.360 --> 00:25:27.600
You'll see the names of all our routines.

00:25:30.360 --> 00:25:33.480
So basically one frame
should be about 20 of these.

00:25:33.610 --> 00:25:35.140
That's like a normal frame.

00:25:35.180 --> 00:25:36.530
There's another normal frame.

00:25:36.600 --> 00:25:37.900
So what we're going to do is we're
going to scroll to the right.

00:25:37.900 --> 00:25:39.380
We noticed that hiccup happen.

00:25:39.420 --> 00:25:43.370
And what I'd like to do is if you think
you see something that kind of breaks

00:25:43.380 --> 00:25:48.020
the pattern of the activity here,
shout out the word bingo.

00:25:50.800 --> 00:25:55.360
StatCrawl is all pretty
much the same height,

00:25:55.360 --> 00:25:57.450
pretty familiar.

00:26:02.500 --> 00:26:02.960
So there it is.

00:26:03.030 --> 00:26:05.000
That's the event I was looking for.

00:26:05.000 --> 00:26:09.090
And what we discover here from
reading the routine names is

00:26:09.400 --> 00:26:10.630
I'm going to highlight this one here.

00:26:10.700 --> 00:26:13.990
This is the post-initialization phase
for a new player coming into view.

00:26:14.150 --> 00:26:17.200
And it's actually jumping into
our Lua interpreter to run a

00:26:17.200 --> 00:26:19.020
script to extract the guild name.

00:26:19.020 --> 00:26:22.000
If you're familiar with the game,
each character has their name and their

00:26:22.010 --> 00:26:23.820
guild name displayed over their heads.

00:26:23.840 --> 00:26:27.200
And what happens when we fly through
that little porthole is you fly over

00:26:27.200 --> 00:26:30.500
a very crowded area of the game where
there's maybe 100 people standing around.

00:26:30.500 --> 00:26:34.330
And when they come into range,
the engine is-- oh, what did that have?

00:26:34.500 --> 00:26:36.930
The engine is-- oh,
that's the real profile.

00:26:36.940 --> 00:26:38.830
It just finished processing.

00:26:40.590 --> 00:26:45.180
The engine is pulling all of
those characters and running a

00:26:45.260 --> 00:26:47.620
Lua script on each one to find
out what is your guild name.

00:26:47.640 --> 00:26:48.680
It's doing it all in one frame.

00:26:48.680 --> 00:26:53.120
Whereas before we had this nice even flow
and the frames were taking 20 samples,

00:26:53.180 --> 00:26:56.510
20 ticks each,
this goes on and on and on,

00:26:56.520 --> 00:26:58.120
and we're just spending a lot
of time in the Lua interpreter.

00:26:58.120 --> 00:27:00.410
So what I've done in the
span of a single run,

00:27:00.420 --> 00:27:03.910
a single capture, is I've identified
exactly what's going on,

00:27:03.940 --> 00:27:06.900
and just by reading the routine names,
I can kind of get an idea for what

00:27:06.900 --> 00:27:08.770
I might be able to do to improve this.

00:27:08.820 --> 00:27:11.790
One idea would be to stagger the
guild updates so we only do a certain

00:27:11.790 --> 00:27:15.250
number per frame and smooth out that
load and you would avoid that hiccup.

00:27:15.300 --> 00:27:18.900
That basically concludes the demo.

00:27:18.900 --> 00:27:20.660
That's it.

00:27:29.130 --> 00:27:30.240
That feature makes me really happy.

00:27:30.240 --> 00:27:33.400
We asked for that about a year ago
and it makes me a happy programmer.

00:27:33.600 --> 00:27:35.500
Okay.

00:27:35.500 --> 00:27:37.500
Thank you, Rob.

00:27:37.520 --> 00:27:38.980
Thank you.

00:27:40.260 --> 00:27:44.360
I'd like to finish up the Shark portion
of the presentation with a quick summary.

00:27:44.360 --> 00:27:47.600
As you've seen,
Shark is easy enough for beginners,

00:27:47.600 --> 00:27:49.620
yet powerful enough for
the seasoned professionals.

00:27:49.620 --> 00:27:52.220
It offers many customizable workflows.

00:27:52.220 --> 00:27:55.870
It even gives you complete profiling
analysis down to hardware events,

00:27:55.870 --> 00:27:57.500
should you so choose to use it.

00:27:57.670 --> 00:28:01.940
We do have some exciting new features,
including the windowed time facility,

00:28:01.940 --> 00:28:03.470
which you just saw demoed live.

00:28:03.660 --> 00:28:06.010
There's also a new
version of the Chud tools,

00:28:06.010 --> 00:28:09.620
of which Shark is a part,
on the ADC website,

00:28:09.720 --> 00:28:10.960
and it should be available today.

00:28:10.960 --> 00:28:13.800
So after the session,
please go download the latest version,

00:28:13.800 --> 00:28:15.880
try it out,
and feel free to give us feedback.

00:28:15.880 --> 00:28:20.650
And I'd like to turn it back over to
Dave Payne for the rest of the session.

00:28:20.660 --> 00:28:22.140
Oh wait, I lied.

00:28:22.140 --> 00:28:27.700
More info, there are links to performance
documentation on the ADC webpage.

00:28:27.700 --> 00:28:32.470
And there's also a Debugging
with Shark lab later tonight,

00:28:32.480 --> 00:28:35.210
downstairs in the Performance lab,
6 to 8.

00:28:35.220 --> 00:28:39.700
All of the Shark team will be there,
so feel free to come on down.

00:28:39.700 --> 00:28:43.870
Use it live, ask us all your questions,
and we'll be happy to answer them.

00:28:43.880 --> 00:28:46.080
Thank you.

00:28:53.950 --> 00:28:55.150
All right, thank you.

00:28:55.230 --> 00:28:59.840
So I'm going to switch gears a little
bit and talk about other ways that we can

00:28:59.840 --> 00:29:03.370
look at 64-bit processes on the system.

00:29:03.780 --> 00:29:04.960
All right, thank you.

00:29:04.960 --> 00:29:09.640
So I'm going to switch gears a little
bit and talk about other ways that we can

00:29:09.640 --> 00:29:13.150
look at 64-bit processes on the system.

00:29:33.720 --> 00:29:36.930
Somebody gets a hold of the
system and is not a developer,

00:29:36.940 --> 00:29:40.560
they can run sample and help
figure out what's going on.

00:29:40.760 --> 00:29:45.600
VM Map is a process that can look
at the memory map of your app,

00:29:45.690 --> 00:29:46.760
and we'll actually look at that.

00:29:46.840 --> 00:29:53.900
And Gard Malik will help find memory
corruption on 64-bit apps now.

00:29:53.900 --> 00:29:58.120
So you may have seen this if you've
attended some of the 64-bit sessions,

00:29:58.120 --> 00:30:01.680
what the memory map of a
64-bit process looks like.

00:30:02.240 --> 00:30:06.560
So the intent is that to help,
as you're developing,

00:30:06.560 --> 00:30:12.240
to help catch pointer issues and
transition from 32 to 64-bit,

00:30:12.240 --> 00:30:16.100
we'll by default have
a 4-gigabyte page zero.

00:30:16.100 --> 00:30:20.260
So that's the entire memory
space of a 32-bit process will

00:30:20.580 --> 00:30:23.260
not be used by 64-bit processes.

00:30:23.260 --> 00:30:29.730
User space starts at the 4-gigabyte
line and goes up to 128 terabytes.

00:30:29.740 --> 00:30:32.120
So.

00:30:32.240 --> 00:30:33.650
Please fill that up real soon.

00:30:34.000 --> 00:30:41.530
The shared libraries, DYLD,
and the system comm pages will

00:30:41.530 --> 00:30:42.620
go at the high end of that.

00:30:42.710 --> 00:30:45.100
You may not see that
on your systems today.

00:30:45.100 --> 00:30:50.380
Then we've got some invalid range
there based on hardware limitations,

00:30:50.380 --> 00:30:54.640
and we'll be loading the
kernel up in high memory space.

00:30:54.640 --> 00:31:00.120
So I talked about that the VM Map tool
can help visualize what the memory map

00:31:00.120 --> 00:31:02.670
actually looks like in your process.

00:31:02.770 --> 00:31:06.110
So with this command line tool,
you can look at where the binary

00:31:06.110 --> 00:31:08.040
image sections get loaded.

00:31:08.040 --> 00:31:12.560
You can look at what the file names
of files that are mapped into your

00:31:12.560 --> 00:31:15.640
process are and where they're mapped.

00:31:15.640 --> 00:31:20.760
You can look at what virtual memory
regions compose the malloc heap

00:31:20.780 --> 00:31:28.200
and look at VM allocated regions
that get created outside of malloc.

00:31:28.260 --> 00:31:31.340
Some subsystems,
some libraries do actually

00:31:31.340 --> 00:31:35.910
call VM malloc or VM allocate,
and you may also be doing that,

00:31:35.910 --> 00:31:38.630
possibly in special circumstances.

00:31:38.730 --> 00:31:42.080
And we can see where the stacks are.

00:31:44.690 --> 00:31:48.450
Now I talked about GuardMalloc
a couple of slides ago.

00:31:48.510 --> 00:31:50.160
So what does GuardMalloc do?

00:31:50.160 --> 00:31:56.300
This helps force a crash if
you are using memory in unsafe

00:31:56.300 --> 00:31:58.240
ways in your application.

00:31:58.240 --> 00:32:02.400
It can be real easy to miss these
things in standard operations.

00:32:02.400 --> 00:32:07.380
You've got small buffer overruns
and maybe that's not catastrophic

00:32:07.380 --> 00:32:09.680
in the situations you've seen.

00:32:09.880 --> 00:32:13.800
Or maybe it's that elusive crash
or it happens once in a blue

00:32:13.800 --> 00:32:15.880
moon and you don't know why.

00:32:15.910 --> 00:32:19.820
So with GuardMalloc you can help
trigger some of these conditions.

00:32:19.950 --> 00:32:24.570
Buffer overruns, accessing memory that
you've already freed,

00:32:24.570 --> 00:32:27.400
or possibly uninitialized memory.

00:32:27.510 --> 00:32:32.700
So to use that,
again this is 64-bit aware now,

00:32:32.750 --> 00:32:37.260
because it's causing a crash in
your application it's best to

00:32:37.260 --> 00:32:39.880
always run that in the debugger.

00:32:39.950 --> 00:32:43.150
So it's most convenient
to do it from Xcode.

00:32:43.160 --> 00:32:46.920
Just go down to the bottom of
the debug menu and turn on the

00:32:46.920 --> 00:32:49.680
"Enable GuardMalloc" menu item.

00:32:49.880 --> 00:32:54.700
You can also do it from GDB in Terminal
by setting an environment variable

00:32:54.810 --> 00:32:59.800
to insert the guard malloc library
into your process as it launches.

00:32:59.860 --> 00:33:02.790
That's essentially what
Xcode was doing too.

00:33:02.850 --> 00:33:06.800
So you can read the man page
for lots more information.

00:33:06.900 --> 00:33:08.800
How does it work?

00:33:08.800 --> 00:33:11.820
Well, first off,
it puts every allocation on a

00:33:11.820 --> 00:33:13.800
separate virtual memory page.

00:33:13.800 --> 00:33:17.920
And then to catch buffer overruns,
it actually aligns the end

00:33:18.100 --> 00:33:21.800
of your allocation at the end
of the virtual memory page.

00:33:21.880 --> 00:33:24.790
It's more common to overrun
a buffer than to underrun.

00:33:24.950 --> 00:33:27.800
So by default, it's set up to do that.

00:33:27.800 --> 00:33:29.700
So we have a 4K page.

00:33:29.810 --> 00:33:32.800
If you allocate 100 bytes,
the 100 bytes is at the very end.

00:33:32.800 --> 00:33:36.360
You walk one byte over,
and we crash because

00:33:36.470 --> 00:33:40.800
we've got a protected,
unallocated guard page after that,

00:33:40.800 --> 00:33:43.800
and you've got a memory
violation trying to access that.

00:33:43.800 --> 00:33:46.800
If you think you might be
getting buffer underruns,

00:33:46.800 --> 00:33:51.190
you can set another environment variable,
which will then restructure it so

00:33:51.190 --> 00:33:55.800
that the memory allocation gets
aligned at the start of the page,

00:33:55.800 --> 00:33:59.490
and the protected page is at the start.

00:34:00.080 --> 00:34:04.380
When you free memory,
it VM deallocates the entire page.

00:34:04.400 --> 00:34:06.140
So the memory is gone now.

00:34:06.140 --> 00:34:09.300
So if you try to access
that after you freed it,

00:34:09.340 --> 00:34:11.420
boom, you crash immediately.

00:34:11.420 --> 00:34:15.030
So you can see immediately
when it happens why your bad

00:34:15.080 --> 00:34:17.220
memory access is happening.

00:34:17.220 --> 00:34:22.980
And to help try to catch
uninitialized memory,

00:34:22.980 --> 00:34:27.000
there's another environment variable
to set that fills memory with a

00:34:27.050 --> 00:34:30.330
pattern that's not going to be

00:34:30.800 --> 00:34:32.700
Typically not a valid pointer.

00:34:32.700 --> 00:34:36.010
It'll end in 5.5.

00:34:36.010 --> 00:34:41.170
So, in fact,
when I first got this working for 64-bit,

00:34:41.210 --> 00:34:43.950
I thought it wasn't working
because the app was crashing.

00:34:43.950 --> 00:34:48.510
But that's because it was working,
and we had a size mismatch in

00:34:48.510 --> 00:34:52.770
one field in a library that
was causing buffer overrun.

00:34:55.220 --> 00:35:00.830
So let's take another look at
the Fractality application.

00:35:00.940 --> 00:35:02.580
If I can switch to demo machine.

00:35:02.700 --> 00:35:13.690
So what I want to do is--

00:35:22.670 --> 00:35:27.560
Well,
I think Factality is still running here.

00:35:27.580 --> 00:35:30.270
I can run VM Map.

00:35:31.640 --> 00:35:35.740
And again, that had the UTF-8 name in it,
so I can just give a partial name

00:35:35.740 --> 00:35:41.220
with standard characters that
I can type in on the terminal.

00:35:42.050 --> 00:35:43.530
I'm able to run this.

00:35:43.670 --> 00:35:48.760
So if we go up to the very
top here-- let me clear this,

00:35:48.760 --> 00:35:49.930
try it again.

00:35:52.990 --> 00:35:57.140
Up to the very top, you might ask, well,
how do I know whether my process

00:35:57.140 --> 00:35:58.670
is running 64-bit or not?

00:35:58.700 --> 00:36:03.560
As a developer, you can run this tool,
and it shows right here that this

00:36:03.650 --> 00:36:06.430
is a 64-bit process up the top.

00:36:06.440 --> 00:36:09.320
We also report the output format.

00:36:09.320 --> 00:36:12.740
We've got some large developers who
actually use VM Map as a part of

00:36:12.820 --> 00:36:17.640
their standard development process
to see when memory changes in major

00:36:17.650 --> 00:36:20.580
ways and then go hunt people down.

00:36:21.140 --> 00:36:23.040
Why did you put that there?

00:36:23.040 --> 00:36:27.860
So then we see all the non-writable
regions of the process.

00:36:27.860 --> 00:36:31.310
We can see that we start at
the 4-gigabyte mark with the

00:36:31.360 --> 00:36:35.360
text of the application itself,
then the link edit segment.

00:36:35.360 --> 00:36:39.190
And then in this Leopard preview,
the libraries are actually

00:36:39.190 --> 00:36:40.980
loaded right after that.

00:36:40.980 --> 00:36:46.000
So we see OpenGL loaded right after
the link edit segment of Fractality.

00:36:48.460 --> 00:36:51.680
And we see all the libraries
that got loaded in.

00:36:51.980 --> 00:36:56.400
We have a few shared
libraries on our system.

00:36:56.400 --> 00:36:59.620
We also see shared memory segments.

00:37:01.300 --> 00:37:06.600
[Transcript missing]

00:37:08.070 --> 00:37:13.920
Core Graphics Areas and we
see DYLD loaded up in the

00:37:13.920 --> 00:37:16.310
high memory space up here.

00:37:17.260 --> 00:37:21.160
Then by default,
the writable regions are split

00:37:21.190 --> 00:37:23.520
out from the non-writable,
but you can interleave them with a

00:37:23.520 --> 00:37:25.130
command line argument if you would like.

00:37:25.180 --> 00:37:29.890
So the data segments are
from the shared libraries,

00:37:29.890 --> 00:37:33.060
the data that comes in with them.

00:37:33.060 --> 00:37:37.520
And we can see various malloc heap area,
font support,

00:37:37.530 --> 00:37:42.100
the actual names of some map
files that get brought in.

00:37:43.700 --> 00:37:46.450
So this really gives you some
visibility into how memory is

00:37:46.450 --> 00:37:47.840
being used in your process.

00:37:47.840 --> 00:37:50.700
And down here we see
where the stack space is.

00:37:50.700 --> 00:37:55.400
Okay, back to slides please.

00:38:01.510 --> 00:38:03.520
So we haven't quite finished the job.

00:38:03.620 --> 00:38:05.800
There's some more memory
analysis tools that we've got

00:38:05.800 --> 00:38:08.010
that are not 64-bit aware yet.

00:38:08.080 --> 00:38:13.320
So the ObjectAlloc application is a
great way of looking at the dynamic

00:38:13.420 --> 00:38:15.960
memory use of an application.

00:38:15.960 --> 00:38:19.640
It can show the objects
in the malloc heap,

00:38:19.640 --> 00:38:25.460
so core foundation and Cocoa objects,
sorts them by object type.

00:38:25.480 --> 00:38:28.160
You can see which ones
you've got a lot of,

00:38:28.160 --> 00:38:30.900
which ones are growing and shrinking.

00:38:31.400 --> 00:38:37.530
You can look at specific ones
and the retains and releases so

00:38:37.530 --> 00:38:41.520
you can kind of help see where
memory leaks are occurring.

00:38:41.760 --> 00:38:45.810
There's also a set of command line tools,
the Leaks, Heap,

00:38:45.910 --> 00:38:51.460
and Malloc History tools that
for when we ship Leopard GM,

00:38:51.460 --> 00:38:53.700
those will be available with 64-bit also.

00:38:56.550 --> 00:38:59.870
So now let's switch gears and
talk about the new tracing tools

00:39:00.140 --> 00:39:02.120
that we're bringing into Leopard.

00:39:02.180 --> 00:39:05.390
So X-Ray and D-Trace.

00:39:06.060 --> 00:39:09.410
X-Ray, as I mentioned before,
is a way to let you peek beneath

00:39:09.440 --> 00:39:13.590
the covers of an application or your
entire system to really get greater

00:39:13.640 --> 00:39:15.520
insights into what's going on.

00:39:15.560 --> 00:39:19.890
We're adding some of the functionality
of some of the other tools,

00:39:19.890 --> 00:39:22.260
like ObjectAlloc, into X-Ray.

00:39:22.260 --> 00:39:27.760
And this really leverages the power of
an underlying facility called D-Trace.

00:39:27.760 --> 00:39:32.540
So I'm sort of setting up two
additional sessions that are coming up.

00:39:32.720 --> 00:39:34.880
We've got a full session
on X-Ray tomorrow,

00:39:34.880 --> 00:39:39.160
and then a session on
D-Trace on Friday morning

00:39:39.160 --> 00:39:41.680
for D-Trace kernel logging.

00:39:41.680 --> 00:39:44.600
But there's so much to talk
about here and with X-Ray that

00:39:44.600 --> 00:39:46.400
we need to get a start on it now.

00:39:48.730 --> 00:39:50.590
So what is D-Trace?

00:39:50.600 --> 00:39:53.000
It stands for Dynamic Tracing.

00:39:53.140 --> 00:39:56.840
This is open source technology
that was ported from open Solaris.

00:39:56.840 --> 00:40:02.290
We've been working with the
Sun engineers who had created this.

00:40:02.290 --> 00:40:07.860
This lets you trace activity in either
the whole system or a specific process.

00:40:07.880 --> 00:40:11.170
And I like to think of it as
kind of a middle ground between

00:40:11.170 --> 00:40:13.360
a debugger and print statements.

00:40:13.820 --> 00:40:16.560
So with a debugger,
I can go in and put breakpoints in,

00:40:16.560 --> 00:40:20.340
I can put conditional breakpoints,
but it's a little bit laborious.

00:40:20.380 --> 00:40:23.240
I can compile print statements in,
but oh,

00:40:23.240 --> 00:40:27.900
I should have some way to turn them
off for my shipping application.

00:40:27.900 --> 00:40:32.790
But D-Trace was designed to
be able to dynamically turn

00:40:32.800 --> 00:40:35.670
it on on production systems.

00:40:35.680 --> 00:40:40.100
With Solaris, they actually tell
their server customers,

00:40:40.100 --> 00:40:42.560
if you're seeing slowdowns
on your web pages,

00:40:43.200 --> 00:40:46.170
your web server,
turn on D-Trace dynamically

00:40:46.240 --> 00:40:48.320
on the processes,
see what's going on,

00:40:48.320 --> 00:40:51.510
and then you can turn it back off safely.

00:40:51.700 --> 00:40:56.360
It's a preview release,
so we're still a little bit

00:40:56.360 --> 00:41:00.680
working on the safe part.

00:41:03.000 --> 00:41:07.180
So, but if you're not using DTrace,
there is no overhead.

00:41:07.180 --> 00:41:09.700
And the overhead when you are
using it kind of depends on how

00:41:09.700 --> 00:41:11.840
many probe points you've enabled.

00:41:11.860 --> 00:41:17.380
A lot of the power of DTrace comes in
from the D script language that filters

00:41:17.380 --> 00:41:20.720
and aggregates data at collection time.

00:41:20.760 --> 00:41:24.290
So you could be having a
long running system that's

00:41:24.290 --> 00:41:28.720
collecting data for a month,
and then you come back in and

00:41:28.890 --> 00:41:32.310
see aggregate statistics on that.

00:41:33.330 --> 00:41:36.100
So how does X-Ray use D-Trace?

00:41:36.100 --> 00:41:38.420
Well,
X-Ray has a lot of instruments in it.

00:41:38.420 --> 00:41:41.540
And for what we're shipping
the Leopard Preview,

00:41:41.540 --> 00:41:46.170
about 2/3 of them probably
are based on D-Trace.

00:41:46.240 --> 00:41:49.650
This lets us look at
file system activity.

00:41:49.680 --> 00:41:54.450
So for all of these operations,
we can get arguments and backtraces

00:41:54.450 --> 00:41:56.590
for where these are occurring.

00:41:56.640 --> 00:42:00.730
So where are my opens and closes,
and what are the file descriptor

00:42:00.730 --> 00:42:04.360
names and the file descriptor numbers,
file names,

00:42:04.450 --> 00:42:09.440
what are the sizes of reads and
writes of I/O that are happening?

00:42:09.610 --> 00:42:12.960
Who's changing permission on
that particular file there?

00:42:13.000 --> 00:42:15.080
Or who's setting a lock on a file?

00:42:15.230 --> 00:42:17.240
When does this directory get created?

00:42:17.330 --> 00:42:19.910
Who's putting that file in /tmp?

00:42:19.990 --> 00:42:23.410
X-Ray uses D-Trace to
figure all this out.

00:42:23.590 --> 00:42:28.270
It also has an instrument to
look at shared memory regions.

00:42:28.270 --> 00:42:33.320
And we've added instruments for looking
for messages to zombie objects for

00:42:33.330 --> 00:42:38.340
Objective-C and to monitor when garbage
collection happens with Objective-C.

00:42:38.340 --> 00:42:43.320
But D-Trace with that D script
language is extensible.

00:42:43.380 --> 00:42:47.600
There's a graphical instrument
builder in X-Ray that you can hear

00:42:47.660 --> 00:42:52.580
about tomorrow that lets you build
your own instruments using D-Trace.

00:42:53.500 --> 00:42:55.790
So let's look at the architecture.

00:42:55.980 --> 00:43:01.240
So as I mentioned, X-Ray uses a set of
instruments or plug-ins.

00:43:01.240 --> 00:43:08.600
So some of them are resource use
plug-ins that look at things like

00:43:08.600 --> 00:43:12.780
CPU use and IO activity on the system,
kind of summarizing the kind of

00:43:12.780 --> 00:43:16.350
information you would see in TOP,
but also putting it in graphical

00:43:16.390 --> 00:43:17.940
format like Big Top does.

00:43:17.940 --> 00:43:23.480
Then we've got some other plug-ins
like Objectalloc that uses a library.

00:43:23.500 --> 00:43:25.750
So that's a library
that gets inserted in.

00:43:25.970 --> 00:43:30.040
But then a good number of
the instruments use D-Trace.

00:43:30.260 --> 00:43:36.290
What they have is a specification of
what the D-Trace script should be.

00:43:36.340 --> 00:43:40.580
When you go to run with
that instrument enabled,

00:43:40.800 --> 00:43:47.360
X-Ray dynamically generates the
D script and passes that down to

00:43:47.360 --> 00:43:50.600
the user Sbin D-Trace command.

00:43:50.850 --> 00:44:00.300
Now the DTrace command parses the script
into a bytecode format which then passes

00:44:00.300 --> 00:44:02.940
into the DTrace engine in the kernel.

00:44:03.130 --> 00:44:07.040
The DTrace engine inserts all the
probe points that you've specified

00:44:07.590 --> 00:44:12.200
and actually interprets the script
in the context of the kernel,

00:44:12.330 --> 00:44:15.990
collects the data there,
and passes it back to

00:44:15.990 --> 00:44:18.020
user space as it goes.

00:44:18.440 --> 00:44:23.300
But,
so X-Ray is using D-Trace in the same

00:44:23.300 --> 00:44:29.420
way that X-Code uses GCC and GDB,
leveraging the power of

00:44:29.420 --> 00:44:31.300
open source technology.

00:44:31.380 --> 00:44:35.640
But as with GCC and GDB,
you can use D-Trace standalone.

00:44:35.710 --> 00:44:39.540
So, for example,
you could remotely log into a machine

00:44:39.540 --> 00:44:46.520
and run a D-Trace script to see what's
going on on the other side of the world.

00:44:46.700 --> 00:44:50.100
So that's kind of what
I'll talk about here.

00:44:50.180 --> 00:44:52.370
So what is a Descript?

00:44:52.560 --> 00:44:58.570
A Descript is a set of clauses
that compose three parts.

00:44:58.590 --> 00:45:03.460
There's a probe specifier that says,
these are the points I'm

00:45:03.460 --> 00:45:05.350
interested in monitoring.

00:45:05.790 --> 00:45:11.090
You can have a predicate that's optional
that says what are the conditions

00:45:11.090 --> 00:45:14.040
under which I want this probe to fire.

00:45:14.160 --> 00:45:19.020
So that's part of the scalability
of this is that you don't have to

00:45:19.020 --> 00:45:22.880
have it fire and collect data for
every time a routine gets hit,

00:45:22.920 --> 00:45:25.730
but only the times you're interested in.

00:45:25.870 --> 00:45:30.130
And you can have actions that
provide much of the power here,

00:45:30.290 --> 00:45:37.230
actions that are commands for
what to do when the probe fires.

00:45:37.250 --> 00:45:41.370
So there's a simple little script here,
but I'll show you some more

00:45:41.370 --> 00:45:44.480
in detail as we do the demo.

00:45:45.150 --> 00:45:48.160
So I talked about you
specify a probe name.

00:45:48.260 --> 00:45:52.530
So it's important to understand
how we specify probe names.

00:45:52.690 --> 00:45:58.450
Consists of four fields, and they are,
in the current language,

00:45:58.470 --> 00:46:00.700
separated by colons.

00:46:00.790 --> 00:46:04.660
The first field is a provider name
that specifies what instrumentation

00:46:04.660 --> 00:46:08.010
mechanism does this probe use.

00:46:08.260 --> 00:46:11.980
Basically a way of splitting
the probes up into sets,

00:46:12.030 --> 00:46:14.610
kind of based on how they're implemented.

00:46:14.670 --> 00:46:18.860
Then a module name,
which might be a library,

00:46:18.860 --> 00:46:22.880
like core foundation,
or it could be an Objective C class name,

00:46:22.880 --> 00:46:25.330
or a kernel module name.

00:46:25.440 --> 00:46:27.370
Function name is pretty self-explanatory.

00:46:27.380 --> 00:46:29.740
This is the function I want to look at.

00:46:29.750 --> 00:46:36.390
And name is kind of a semantic thing,
like entry or exit.

00:46:36.470 --> 00:46:41.150
Entry or return for when
the function gets hit.

00:46:41.330 --> 00:46:47.640
So some examples here, if you specify the
DTRACE colon colon colon begin,

00:46:47.640 --> 00:46:51.060
or just begin,
that would be the first one that gets

00:46:51.060 --> 00:46:54.600
executed when your script starts up.

00:46:54.920 --> 00:47:03.190
The next one, syscall::entry says,
"I want to probe on every system call."

00:47:03.280 --> 00:47:06.540
And that's actually quite doable.

00:47:06.650 --> 00:47:10.200
And for the PID provider,
the third one here,

00:47:10.220 --> 00:47:12.480
you have to specify a process ID.

00:47:12.530 --> 00:47:19.800
Say, for process number 759,
I want to break on the core foundation's

00:47:19.920 --> 00:47:23.400
CF retain when we enter that.

00:47:24.720 --> 00:47:26.470
So what set of providers are there?

00:47:26.500 --> 00:47:30.940
If you want to look at a
specific user-level process,

00:47:30.940 --> 00:47:34.360
there's the PID provider,
where you specify a process ID.

00:47:34.360 --> 00:47:41.700
We also, because Objective-C and
Cocoa are very important for us,

00:47:41.700 --> 00:47:45.840
and you'll notice that the D language
has colon-separated fields,

00:47:45.840 --> 00:47:49.810
we actually implemented a
variant of the PID provider

00:47:49.810 --> 00:47:54.290
that we call the OBJC provider,
that lets you specify class

00:47:54.300 --> 00:47:57.960
name and method name rather
than library and function.

00:47:59.820 --> 00:48:02.580
Then there are some system
providers that we've got.

00:48:02.580 --> 00:48:07.070
So Syscall,
this works today with both enter

00:48:07.140 --> 00:48:09.400
and return of system calls.

00:48:09.550 --> 00:48:11.220
Profile is timed events.

00:48:11.370 --> 00:48:15.150
So you can specify every 893
milliseconds I want to fire a

00:48:15.150 --> 00:48:17.160
probe and see what's going on.

00:48:17.320 --> 00:48:19.260
And then FBT is really interesting.

00:48:19.260 --> 00:48:22.280
It's function boundary
tracing inside the kernel.

00:48:22.280 --> 00:48:27.230
So tell me every time I enter
and return from a specific

00:48:27.230 --> 00:48:29.600
probe point in the kernel.

00:48:29.600 --> 00:48:33.760
Device driver writers, for example,
might find that really interesting

00:48:33.760 --> 00:48:37.020
to see how the kernel is
interacting with their driver.

00:48:37.840 --> 00:48:39.870
Now there's a lot to DTrace.

00:48:39.990 --> 00:48:42.270
We don't have it all done yet.

00:48:42.330 --> 00:48:47.220
There's a number of providers that we
don't have yet for looking at kernel

00:48:47.220 --> 00:48:49.910
lock statistics or user space locks.

00:48:49.920 --> 00:48:55.280
Sysinfo, VMinfo,
the scheduler information, IO.

00:48:55.280 --> 00:48:59.860
PROC would be user space for when
does fork and exec get called.

00:48:59.860 --> 00:49:02.320
But it's quite usable
with what we've got today.

00:49:05.200 --> 00:49:08.220
One additional thing that we have
running in the labs and we don't have

00:49:08.220 --> 00:49:14.180
in the preview yet is what's called
User-Land Statically Defined Tracing.

00:49:14.330 --> 00:49:17.800
This allows you to specify
where you want static probe

00:49:17.800 --> 00:49:20.070
points in your application code.

00:49:20.120 --> 00:49:27.460
So there are various things out
in existence that use these today.

00:49:27.460 --> 00:49:34.540
There will be a demo tomorrow morning in
the Java 64-bit VM talk where they show

00:49:34.540 --> 00:49:37.980
the use of static probe points in Java.

00:49:37.980 --> 00:49:44.160
Demo where it's not available
in your preview DVDs right now.

00:49:44.200 --> 00:49:48.200
So this is kind of that
middle ground again.

00:49:48.200 --> 00:49:50.200
You can dynamically turn
these probe points on.

00:49:50.200 --> 00:49:54.340
So they're like the printfs
that you put into your code,

00:49:54.340 --> 00:49:57.030
except you don't have to have #, ifdef,
etc.

00:49:57.230 --> 00:50:00.200
When you're not using them,
they're just no-ops.

00:50:00.200 --> 00:50:02.200
So very lightweight.

00:50:02.200 --> 00:50:06.160
So let's see some demo here.

00:50:15.470 --> 00:50:20.510
Alright, this will be the graphically
flashy demo on this one.

00:50:22.310 --> 00:50:27.370
Okay, so the first basic thing we can
do is list the probe points I've

00:50:27.450 --> 00:50:29.190
got available to me in the system.

00:50:29.400 --> 00:50:33.590
So I do that with the -l
argument to the DTRS command.

00:50:33.840 --> 00:50:36.840
18,000 probe points available.

00:50:36.840 --> 00:50:42.980
So if we look through,
we see that I've already

00:50:42.980 --> 00:50:45.890
looked at PID 440,
which happens to be the

00:50:45.890 --> 00:50:47.360
text edit that I'm running.

00:50:47.360 --> 00:50:56.010
We've got all the system calls here,
P thread, entry and exit, semaphores,

00:50:56.010 --> 00:50:57.890
etc., etc.

00:50:59.420 --> 00:51:04.380
And then FBT is the
functions within the kernel.

00:51:04.380 --> 00:51:08.480
So that's already a pretty
big menu of choices.

00:51:08.480 --> 00:51:10.560
Let me clear the screen from that.

00:51:10.600 --> 00:51:14.060
And let's say that I wanted
to look at what's available

00:51:14.730 --> 00:51:17.080
to me with core foundation.

00:51:17.080 --> 00:51:20.350
So again, I type in the process
number I'm interested in,

00:51:20.350 --> 00:51:23.880
which is 440 for the text edit
that I had already launched.

00:51:25.160 --> 00:51:32.040
So I can say the PID provider,
core foundation library, all functions,

00:51:32.040 --> 00:51:34.480
so I'll just leave that one blank.

00:51:34.490 --> 00:51:36.280
And I want to see the entry points.

00:51:37.870 --> 00:51:46.600
Actually, let me restart TextEdit here.

00:51:56.320 --> 00:51:58.860
I'll hide the finder.

00:51:58.860 --> 00:52:03.970
And let's get the process
number of text edit again.

00:52:06.560 --> 00:52:08.720
4.7.7.

00:52:08.720 --> 00:52:13.660
So going back to the
previous D-Trace command,

00:52:13.830 --> 00:52:17.370
change that to 4.7.7.

00:52:17.400 --> 00:52:18.520
There.

00:52:21.090 --> 00:52:28.290
For some reason I'm not
getting core foundation lines.

00:52:29.690 --> 00:52:33.350
Sorry, I need a dash end.

00:52:33.400 --> 00:52:36.890
I'm specifying the name
of a provider here.

00:52:43.600 --> 00:52:55.300
[Transcript missing]

00:53:05.320 --> 00:53:07.200
There we go, that's better.

00:53:07.350 --> 00:53:10.790
So these are all the entry
points on core foundation.

00:53:11.010 --> 00:53:12.550
So there's a lot of flexibility here.

00:53:12.560 --> 00:53:14.940
But what can we do with that?

00:53:15.000 --> 00:53:20.200
Let's say I wanted to see
all the graphics call,

00:53:20.200 --> 00:53:25.990
all the draw calls that are
being made in Objective-C in...

00:53:28.360 --> 00:53:31.660
In that text edit run.

00:53:31.660 --> 00:53:36.180
So that starts up and we see
that this script has matched

00:53:36.390 --> 00:53:38.940
350 different probe points.

00:53:39.020 --> 00:53:41.470
So when I bring text
edit to the foreground,

00:53:41.490 --> 00:53:46.920
I'm starting to see probes
dynamically firing the background.

00:53:46.920 --> 00:53:50.830
If I put it in background
and bring it up again,

00:53:50.830 --> 00:53:52.600
more of them fire.

00:53:52.600 --> 00:53:57.970
So what we're seeing over here is the
CPU number that the probe executed on.

00:53:58.350 --> 00:54:04.040
The probe identifier,
which isn't too useful to humans really.

00:54:04.100 --> 00:54:08.110
And the probe name,
then some information that

00:54:08.110 --> 00:54:10.200
we custom printed out.

00:54:10.290 --> 00:54:14.170
So a nice formatting of the probe point.

00:54:14.180 --> 00:54:17.020
If I hit control C,
then I'm printing out a

00:54:17.020 --> 00:54:18.790
summary of what we had.

00:54:18.800 --> 00:54:23.440
So here we see that we had 80
calls to NSImage drawing rect,

00:54:23.490 --> 00:54:27.100
42 calls to NSClipView draws background.

00:54:27.100 --> 00:54:30.600
So how did we do that?

00:54:30.600 --> 00:54:33.510
So let's look at the script there.

00:54:34.540 --> 00:54:39.180
So this script has two probe points,
or two clauses.

00:54:39.180 --> 00:54:46.160
One we see is an Objective-C probe
point with $1 as the provider name.

00:54:46.300 --> 00:54:49.610
$1 is the first argument
that I passed to the probe.

00:54:49.660 --> 00:54:52.500
So that was the 475 process number.

00:54:52.500 --> 00:54:54.450
Then I'm saying, I don't care what class.

00:54:54.460 --> 00:55:00.500
Tell me about all classes and the
draw method with any suffix on it.

00:55:00.500 --> 00:55:02.140
Tell me when that gets entered.

00:55:02.140 --> 00:55:04.620
Then I'm doing a custom print statement.

00:55:04.620 --> 00:55:11.170
I'm also then setting up an aggregate,
which is basically an associative

00:55:11.170 --> 00:55:15.780
array that's using the probe module,
so the class name, the function name,

00:55:15.780 --> 00:55:18.720
as keys, and keeping a count.

00:55:18.720 --> 00:55:21.310
Then at the end,
there's a default way that

00:55:21.320 --> 00:55:24.320
aggregates will get printed,
but I've done some formatting

00:55:24.320 --> 00:55:29.340
here so it shows up a little
bit nicer with an end probe.

00:55:29.770 --> 00:55:32.780
Okay, so let's look at another one.

00:55:32.840 --> 00:55:35.700
Let's look at file opens.

00:55:35.700 --> 00:55:39.100
So we only matched three
probes on that one.

00:55:39.260 --> 00:55:42.370
So let's create a couple of files.

00:55:42.930 --> 00:55:44.840
Do some things.

00:55:44.870 --> 00:55:46.200
And it's just collecting data.

00:55:46.350 --> 00:55:52.220
When I control C, then we see that it's
opened some files here.

00:55:52.220 --> 00:55:54.890
Again, this is a count of the files.

00:55:54.890 --> 00:55:56.940
We see that it's opened text.

00:55:56.940 --> 00:56:01.110
The process text edit has opened
a preferences file ten times.

00:56:01.110 --> 00:56:05.610
So that's interesting.

00:56:07.050 --> 00:56:14.550
So what we're doing here is breaking
on the entry to the open system call,

00:56:14.730 --> 00:56:20.310
checking the executable name.

00:56:25.180 --> 00:56:29.370
: Breaking it open,
getting the file name, the pointer to it,

00:56:29.370 --> 00:56:30.660
recording that.

00:56:30.660 --> 00:56:35.400
Then when the open system call returns,
if the file name has been set,

00:56:35.400 --> 00:56:39.940
then we record the executable,
the file name, and account.

00:56:40.930 --> 00:56:45.770
We can also get a little bit more
information about why it's happening.

00:56:45.820 --> 00:56:48.230
Oops,
I need the... In this case I've written

00:56:48.230 --> 00:56:52.160
this script to take an executable name

00:56:57.940 --> 00:56:58.760
A couple of things.

00:56:58.900 --> 00:57:04.850
And what this does is it gives me the
file descriptor that was returned,

00:57:04.940 --> 00:57:09.310
the file name, it recorded the time it
took to open that file,

00:57:09.310 --> 00:57:14.160
so 191,000 nanoseconds,
and the full backtrace

00:57:14.160 --> 00:57:15.400
of where it happened.

00:57:15.400 --> 00:57:20.490
So this one's kind of interesting
as to how this is working.

00:57:21.590 --> 00:57:26.720
So again,
breaking on open entry and return,

00:57:26.720 --> 00:57:28.790
but checking for a
specific executable name,

00:57:28.790 --> 00:57:31.880
recording both the file
name and the timestamp.

00:57:31.920 --> 00:57:35.380
And if the timestamp is non-zero,
then we check,

00:57:35.380 --> 00:57:38.440
is it a file name that I'm interested in?

00:57:38.440 --> 00:57:45.910
So check whether the path name has the
substring slash preferences slash in it.

00:57:46.540 --> 00:57:53.220
If so, then print out information,
arg1 is the return value from the open,

00:57:53.220 --> 00:57:58.470
and use the useStack function
to print the backtrace.

00:57:58.540 --> 00:58:02.010
So back to slides, please.

00:58:06.130 --> 00:58:10.660
So what's our status with
DTrace in the Leopard Preview?

00:58:10.870 --> 00:58:14.720
So the platform that we recommend
that you work with us on for

00:58:14.720 --> 00:58:18.400
now is a 32-bit Intel Macintosh.

00:58:18.420 --> 00:58:24.700
That's the one that currently supports
the PID and Objective-C providers.

00:58:24.710 --> 00:58:26.990
Based on the code that we've
ported from OpenSolaris,

00:58:27.060 --> 00:58:29.070
that was significantly easier for us.

00:58:29.150 --> 00:58:33.320
We'll have to do some real
work to support PowerPC.

00:58:33.320 --> 00:58:38.120
And because of this,
X-Ray works best on that platform also.

00:58:38.140 --> 00:58:39.990
And I need to tell you about UStack.

00:58:40.090 --> 00:58:44.240
You saw a symbolicated backtrace
there on my 32-bit Intel machine.

00:58:44.470 --> 00:58:48.600
In the Leopard Preview that you have,
unfortunately,

00:58:48.600 --> 00:58:52.300
DTrace doesn't give you symbols,
but X-Ray's use of it

00:58:52.340 --> 00:58:54.090
does give you symbols.

00:58:54.170 --> 00:59:01.260
So this is basically telling what you can
do with DTrace under the covers of X-Ray.

00:59:01.590 --> 00:59:04.000
But be careful with it,
with your PowerBooks,

00:59:04.100 --> 00:59:07.410
because it will hang,
if you call a use stack,

00:59:07.410 --> 00:59:09.180
it will hang immediately on a G4.

00:59:09.180 --> 00:59:12.440
We've fixed that in the labs.

00:59:12.500 --> 00:59:16.160
There's a lot of Solera-specific
concepts that we haven't

00:59:16.160 --> 00:59:20.340
yet mapped over to Mac OS X,
and as you saw, there's some providers

00:59:20.340 --> 00:59:22.100
that we don't support.

00:59:22.480 --> 00:59:29.730
To get more information about DTrace,
the best source is the Sun Big Admin

00:59:29.850 --> 00:59:36.040
DTrace website and the 400-page
Solera's Dynamic Tracing Guide.

00:59:36.040 --> 00:59:40.800
So what we've found easiest is
to download the PDF and to use

00:59:40.930 --> 00:59:46.460
Preview's search capabilities
to search through that manual.

00:59:46.460 --> 00:59:50.170
Unfortunately, it's not all relevant for
our platform at this point,

00:59:50.230 --> 00:59:52.450
so we'll be working with the Sun Guide.

00:59:52.480 --> 00:59:54.230
on that.

00:59:54.990 --> 00:59:57.010
So we've covered a lot in this session.

00:59:57.150 --> 01:00:01.420
To summarize, we've got some powerful
tools for you on Mac OS X.

01:00:01.420 --> 01:00:05.930
Shark keeps getting better and better,
more powerful all the time to

01:00:05.930 --> 01:00:10.920
help profile the hot spots,
and now to use the windowed time facility

01:00:10.920 --> 01:00:15.420
to really kind of start it and forget
about it until your problem occurs.

01:00:15.420 --> 01:00:19.940
With X-Ray, you can understand your
application and system behavior,

01:00:19.940 --> 01:00:22.480
and it uses D-Trace that you've seen.

01:00:22.800 --> 01:00:27.850
Again, there's an X-Ray session tomorrow,
the Java 64-bit VM session

01:00:27.850 --> 01:00:31.680
will discuss it,
and the D-Trace session on Friday.

01:00:31.680 --> 01:00:36.830
We've added 64-bit support,
and we're continuing to expand on that.

01:00:36.900 --> 01:00:41.640
We're trying to put a lot of effort in
to help you create great applications.

01:00:41.680 --> 01:00:44.800
So we'd really like your feedback
about what works for you,

01:00:44.800 --> 01:00:48.250
what doesn't work,
what additional tools you'd like to see,

01:00:48.320 --> 01:00:48.710
etc.