WEBVTT

00:00:10.370 --> 00:00:11.270
I'm Chris Kane.

00:00:11.460 --> 00:00:14.760
I'm a software engineer
on the Cocoa team.

00:00:14.760 --> 00:00:17.820
You, hopefully,
are here to hear about something

00:00:17.820 --> 00:00:20.290
to do with multithreading in Cocoa.

00:00:20.290 --> 00:00:23.800
I'm going to be talking a
little bit today about a few

00:00:23.800 --> 00:00:29.100
general multithreading topics,
and then I'll be getting into

00:00:29.100 --> 00:00:33.410
some of the Cocoa APIs that we
provide for multithreading and to

00:00:33.540 --> 00:00:36.020
support you in your multithreading.

00:00:36.100 --> 00:02:01.300
[Transcript missing]

00:02:02.380 --> 00:02:04.820
Well, of course, you know,
at the beginning we have

00:02:04.930 --> 00:02:06.310
to give our definitions.

00:02:06.330 --> 00:02:10.550
And so here's my definition
of what is multithreading.

00:02:10.670 --> 00:02:13.740
When I talk about multithreading,
I'm talking about executing

00:02:14.140 --> 00:02:17.960
multiple chunks of code
simultaneously in your program.

00:02:17.960 --> 00:02:24.300
Now, on a single processor machine,
you can do this by using the

00:02:24.350 --> 00:02:27.600
preemptive multitasking that
the operating system provides.

00:02:27.600 --> 00:02:30.100
And so it's simulated concurrency.

00:02:30.760 --> 00:02:33.990
But as we are moving
forward here in the future,

00:02:33.990 --> 00:02:37.180
and in the future,
we're going to be seeing a lot more true

00:02:37.180 --> 00:02:41.620
concurrency from these dual-processor,
now quad-processor machines

00:02:41.620 --> 00:02:43.520
that we've been shipping.

00:02:45.690 --> 00:02:48.680
Now,
another way to achieve multithreading is,

00:02:48.680 --> 00:02:51.200
of course, to spawn other processes,
but I'm not going to

00:02:51.200 --> 00:02:52.290
be talking about that.

00:02:52.310 --> 00:02:55.360
Of course,
the user is running many apps while

00:02:55.360 --> 00:03:00.380
they're logged in and using the system,
and each of those apps has their own set

00:03:00.560 --> 00:03:03.660
of threads and is doing its own thing.

00:03:03.660 --> 00:03:07.860
I'm not going to be talking about
concurrency or multithreading

00:03:07.860 --> 00:03:12.790
via separate processes,
but rather just about threads

00:03:12.790 --> 00:03:15.290
within the same process.

00:03:16.540 --> 00:03:21.320
Now, the traditional approach,
or the traditional reason

00:03:21.770 --> 00:03:25.740
for multithreading was to
avoid blocking the event loop.

00:03:25.740 --> 00:03:29.980
That is, if you had a long-running
task that you wanted to do,

00:03:29.980 --> 00:03:33.440
say when the user hit a
certain button or whatever,

00:03:33.490 --> 00:03:37.420
you could spawn off another thread,
do that long-running

00:03:37.420 --> 00:03:42.290
processing in the background,
and remain responsive to the user as the

00:03:42.290 --> 00:03:45.420
user continued to fiddle with your UI.

00:03:46.940 --> 00:03:50.790
That was a traditional
reason for multithreading.

00:03:50.930 --> 00:03:55.700
Sometimes multithreading is useful
because it can simplify the code.

00:03:55.820 --> 00:04:00.830
Another approach to achieving
apparent concurrency,

00:04:00.830 --> 00:04:03.900
I'll call it,
to the user is to use asynchronous

00:04:03.900 --> 00:04:09.190
APIs and register to receive callbacks
when things happen and so on.

00:04:09.280 --> 00:04:17.590
But that can cause you to have to
chunk up the logic of your program

00:04:17.590 --> 00:04:21.540
across several different types of
callback functions and whatnot.

00:04:21.540 --> 00:04:27.440
And so sometimes multithreading
can simplify code.

00:04:27.440 --> 00:04:33.200
But the main reason that we're going to
be seeing now and going into the future,

00:04:33.200 --> 00:04:37.320
it seems,
is to achieve performance improvement.

00:04:37.460 --> 00:04:40.620
There was a time when
machines ran at 100 megahertz.

00:04:40.620 --> 00:04:44.200
And then the machines,
the next generation of machines came

00:04:44.200 --> 00:04:46.760
out and they ran at 150 megahertz.

00:04:46.820 --> 00:04:52.140
And so the existing applications got a
little performance boost out of that.

00:04:52.710 --> 00:05:13.840
And eventually there were 500
MHz machines and GHz machines

00:05:13.840 --> 00:05:13.840
and 3 GHz machines and so on.

00:05:13.840 --> 00:05:13.840
And the existing apps,
the apps which didn't change, I mean,

00:05:13.840 --> 00:05:13.840
were getting performance improvements
sort of for free by way of the

00:05:13.840 --> 00:05:13.840
processor speed increasing.

00:05:14.990 --> 00:05:17.560
What we see,
it looks like going into the future,

00:05:17.560 --> 00:05:21.900
is that processors are going
to be gaining multiple cores.

00:05:21.900 --> 00:05:27.270
And in order to take advantage
of the multiple cores,

00:05:27.270 --> 00:05:29.520
you have to use threads.

00:05:29.520 --> 00:05:33.630
Because threads are the way
that the operating system

00:05:33.630 --> 00:05:37.360
allows you to address each core,
if you will.

00:05:37.380 --> 00:05:41.910
And so to put work on a core,
you have to use multiple threads.

00:05:43.460 --> 00:05:46.510
I'm going to do a quick demo here.

00:05:46.930 --> 00:05:51.790
Now this demo,
if we can go to the demo machine,

00:05:51.790 --> 00:05:51.790
oh we already have.

00:05:53.500 --> 00:05:56.210
All I've set up here
is a very trivial app.

00:05:56.360 --> 00:06:00.890
And what I have is a bunch of work
to be done on a bunch of images,

00:06:00.890 --> 00:06:03.080
an array of images I have.

00:06:03.290 --> 00:06:08.900
And the work is-- maybe it's
some sort of warp or some sort

00:06:09.130 --> 00:06:13.590
of color changing operation to
each image that I'm going to do.

00:06:13.720 --> 00:06:16.620
The details don't matter.

00:06:16.740 --> 00:06:22.910
What I have here in the left column,
what I want to point

00:06:22.910 --> 00:06:26.550
out is in parentheses,
I've put in an estimate based

00:06:26.550 --> 00:06:31.120
on the size of each image of the
amount of work it's going to take.

00:06:31.200 --> 00:06:35.150
So first what I'm going to do,
I've got a 1 up there in that top box.

00:06:35.170 --> 00:06:38.680
And what that top box is is the
number of threads that I'm going

00:06:38.760 --> 00:06:40.960
to use to process these images.

00:06:41.060 --> 00:06:42.140
So let me go ahead.

00:06:42.300 --> 00:06:43.970
I'm going to start this up.

00:06:44.190 --> 00:06:47.160
And of course, it begins chunking away.

00:06:47.180 --> 00:06:52.900
As each image is finished,
it shows up in the rightmost column.

00:06:53.090 --> 00:06:55.760
Now at the same time now,
I'm going to show what happens

00:06:55.760 --> 00:06:57.670
if I use instead three threads.

00:06:57.680 --> 00:06:59.380
And of course, the obvious thing happens.

00:06:59.380 --> 00:07:04.290
I'm doing things three times-- well,
maybe roughly three times faster.

00:07:04.430 --> 00:07:10.490
And of course, using three threads,
I can process the array that much faster.

00:07:10.840 --> 00:07:15.230
Now, in some sense,
I've faked up this demo,

00:07:15.230 --> 00:07:19.480
and I'm not really doing a lot of
serious work underneath the covers.

00:07:19.600 --> 00:07:24.180
But what this allows me to do is
I'm going to pretend I have 10

00:07:24.180 --> 00:07:32.320
cores and spawn off 10 different
threads to process these images.

00:07:32.510 --> 00:07:34.280
And of course, I'm going

00:07:36.200 --> 00:07:45.000
[Transcript missing]

00:07:47.550 --> 00:07:51.390
The way that you got processor
speed up in the past with

00:07:51.390 --> 00:07:58.300
increasing processor speed,
clock speed, is going to be changed if we

00:07:58.350 --> 00:07:59.470
can go back to the slides.

00:07:59.480 --> 00:08:05.150
And now we're going to be going back,
going into a world in which

00:08:05.150 --> 00:08:10.610
multiple threads are going to
be a key tool to get performance

00:08:10.610 --> 00:08:14.210
improvement in your applications.

00:08:15.240 --> 00:08:24.000
Now, unfortunately,
not everything works out just so nicely.

00:08:24.000 --> 00:08:27.210
You can't just create
threads and go do the work.

00:08:27.420 --> 00:08:30.500
There's a little problem.

00:08:30.680 --> 00:08:33.860
There are many problems
that come about when you go

00:08:34.000 --> 00:08:36.350
starting to do multithreading.

00:08:37.740 --> 00:08:43.840
By and large, most of them devolve
down to one key issue.

00:08:44.090 --> 00:08:49.300
That is,
changes to shared data by multiple

00:08:49.300 --> 00:08:55.930
threads ends up being the cause
of most multithreading issues.

00:08:56.220 --> 00:09:00.740
Now,
there's actually two flavors of this.

00:09:00.740 --> 00:09:05.120
One is that the obvious one,
and hopefully, you know,

00:09:05.120 --> 00:09:07.140
I'm assuming here that
you have some experience,

00:09:07.140 --> 00:09:10.960
so that you've run into this one or
you've heard about this one before.

00:09:11.700 --> 00:09:15.960
If you are changing data in
multiple threads at the same time,

00:09:16.060 --> 00:09:20.160
of course, you can produce invalid data,
invalid results,

00:09:20.160 --> 00:09:22.220
invalid states in the data.

00:09:22.220 --> 00:09:25.910
That is,
the threads can conflict with one

00:09:25.910 --> 00:09:29.350
another in producing their changes.

00:09:30.000 --> 00:11:45.900
[Transcript missing]

00:11:47.460 --> 00:11:50.750
The second thread gets done with
instant increment and writes

00:11:50.840 --> 00:11:55.320
the value out to its value,
its incremented value, out to memory.

00:11:55.360 --> 00:11:58.150
And what's happened?

00:11:58.550 --> 00:12:01.370
The change that thread
one made has been lost.

00:12:01.550 --> 00:12:03.920
This is a classic example.

00:12:04.040 --> 00:12:07.810
B only got incremented once,
which in some sense could be

00:12:07.810 --> 00:12:09.680
an invalid state in your data.

00:12:09.810 --> 00:12:15.010
You've lost the work that
the first thread tried to do.

00:12:16.420 --> 00:12:20.720
Let me show quickly an example
of a visibility hazard.

00:12:20.800 --> 00:12:23.070
So we start with the
same integer in memory.

00:12:23.180 --> 00:12:26.930
And the first-- both threads,
we're going to assume,

00:12:26.930 --> 00:12:30.230
see the value as 5 out in memory.

00:12:30.330 --> 00:12:33.480
The first thread is going
to increment the value.

00:12:34.070 --> 00:12:37.600
And it writes out six out to my memory.

00:12:37.670 --> 00:12:41.320
At some later point,
perhaps it's briefly later,

00:12:41.320 --> 00:12:46.680
or it might be quite a bit later,
quite a bit in terms of the

00:12:46.680 --> 00:12:50.340
CPU processor time at least,
the second thread is gonna

00:12:50.340 --> 00:12:54.650
come along and it's gonna load
the value at that address,

00:12:54.650 --> 00:12:55.780
that B value.

00:12:55.780 --> 00:13:01.420
But it may only load it from, say,
its L1 cache.

00:13:01.630 --> 00:13:04.000
And it's gonna print that value out.

00:13:04.100 --> 00:13:07.940
But it prints out five because
it still sees the value five.

00:13:08.030 --> 00:13:12.200
The first process,
the first thread on the first

00:13:12.200 --> 00:13:17.100
processor has not broadcast the
information that has changed

00:13:17.100 --> 00:13:20.450
B out to the other processors.

00:13:20.480 --> 00:13:23.540
And so that is the source
of this visibility problem

00:13:23.540 --> 00:13:25.710
in this particular case.

00:13:26.940 --> 00:13:31.660
So because there are these problems
that come about when you try to

00:13:31.780 --> 00:13:35.920
execute code at the same time,
this notion of thread

00:13:36.240 --> 00:13:37.660
safety is introduced.

00:13:37.660 --> 00:13:45.340
And for this particular purpose,
I'm focusing on intrinsic thread safety.

00:13:45.980 --> 00:13:53.410
So what I mean by intrinsic thread safety
here is that a caller does not have to

00:13:53.500 --> 00:14:00.040
worry about the activities of that API,
for example, or that functionality,

00:14:00.040 --> 00:14:00.700
that feature.

00:14:02.210 --> 00:14:06.620
When the caller is using that
functionality on multiple threads,

00:14:06.640 --> 00:14:10.040
that is, the caller does not have to
take any additional action.

00:14:10.040 --> 00:14:13.630
And that is a piece of functionality,
a piece of software,

00:14:13.650 --> 00:14:16.170
which is intrinsically thread-safe.

00:14:16.250 --> 00:14:18.090
It's, in some sense, built-in.

00:14:18.090 --> 00:14:19.680
The thread safety is built-in.

00:14:19.680 --> 00:14:24.250
Now, typically, though,
intrinsic thread safety does

00:14:24.250 --> 00:14:26.980
not protect compound operations.

00:14:27.300 --> 00:14:30.130
So that if you make one call,
and then you make another call,

00:14:30.130 --> 00:14:33.070
and then you make another call
into the same functionality,

00:14:33.080 --> 00:14:37.570
its intrinsic thread safety does
not necessarily provide safety

00:14:37.570 --> 00:14:40.260
across all of those separate calls.

00:14:41.790 --> 00:14:45.620
That is,
the first call may happen safely,

00:14:45.800 --> 00:14:49.290
then the second call may happen safely,
but things may change

00:14:49.290 --> 00:14:51.470
in between those calls.

00:14:51.560 --> 00:14:54.340
Now the Cocoa documentation
talks about what things are

00:14:54.360 --> 00:14:58.050
intrinsically thread-safe,
and I'll be mentioning this

00:14:58.050 --> 00:14:59.810
again a little bit later.

00:15:00.120 --> 00:15:03.390
Many things in Cocoa are not
intrinsically thread-safe,

00:15:03.390 --> 00:15:12.990
but most can be used in a thread-safe
way by taking additional action.

00:15:13.700 --> 00:15:23.800
[Transcript missing]

00:15:24.750 --> 00:15:27.430
and his team at the
University of California,

00:15:27.430 --> 00:15:31.560
California, have been working on a new
application that will help you

00:15:31.560 --> 00:15:33.400
get the most out of your Mac.

00:15:36.280 --> 00:15:38.030
Now, how do you avoid these problems?

00:15:38.080 --> 00:15:41.200
Well,
if the main problem is that multiple

00:15:41.280 --> 00:15:44.450
threads changing shared data,

00:15:44.700 --> 00:16:03.500
[Transcript missing]

00:16:04.410 --> 00:16:07.740
In practice, of course,
this is not realistic.

00:16:07.760 --> 00:16:13.480
And most actual functional
types of solutions devolve,

00:16:13.480 --> 00:16:16.490
again, here we have two types.

00:16:16.830 --> 00:16:21.100
The first is you can change the way
that the code or data is structured.

00:16:21.230 --> 00:16:24.340
And the second is you can
augment the code with more code.

00:16:24.350 --> 00:16:26.900
So let me look at the first of those.

00:16:27.000 --> 00:16:29.730
Well,
if you have data and it's immutable,

00:16:29.730 --> 00:16:34.540
say you're using an immutable
collection or you're simply treating

00:16:34.600 --> 00:16:38.860
it as immutable is constant,
then of course there's no issue

00:16:38.860 --> 00:16:41.630
because you're not changing data.

00:16:41.780 --> 00:16:46.680
And changing data, changing shared data,
is one of the controls that you can use.

00:16:46.700 --> 00:16:50.930
contributing factors to
most threading issues.

00:16:51.410 --> 00:16:53.960
The second approach
is thread confinement,

00:16:54.060 --> 00:16:56.900
that I'm going to talk about at least,
is thread confinement.

00:16:56.900 --> 00:17:00.890
Thread confinement means

00:17:01.940 --> 00:17:07.000
Only one thread is going to look at
or use or change a particular value.

00:17:07.040 --> 00:17:10.580
So the most trivial example is,
of course, all the local variables on the

00:17:10.580 --> 00:17:14.700
stack are generally confined
to that particular thread.

00:17:14.700 --> 00:17:18.330
You don't have to worry about
the thread safety of writing

00:17:18.330 --> 00:17:20.460
and reading local variables.

00:17:22.730 --> 00:17:27.920
Second type of thread confinement
is thread-specific data.

00:17:27.940 --> 00:17:31.930
Most thread packages offer some
sort of thread-specific data,

00:17:32.040 --> 00:17:34.660
and of course Cocoa does as well.

00:17:34.850 --> 00:17:37.790
Another approach is to
use dedicated threads.

00:17:37.820 --> 00:17:41.240
Suppose you have a very
complex data structure,

00:17:41.240 --> 00:17:46.820
and it would be very complicated,
perhaps, to try to allow multiple threads

00:17:46.920 --> 00:17:50.790
to access that complicated
data structure thread safely.

00:17:50.870 --> 00:17:55.820
What you can do is you can dedicate
a thread to be responsible for

00:17:55.820 --> 00:18:00.460
accessing that data structure,
and then funnel all requests to

00:18:00.480 --> 00:18:04.160
actually process or work with that
data structure through that thread

00:18:04.160 --> 00:18:09.670
and make that thread actually do the
work and look at that data structure.

00:18:10.170 --> 00:18:16.100
If you have a dedicated thread,
you may need to communicate with it.

00:18:16.100 --> 00:18:21.010
Another form of changing how your
data or your code is structured is

00:18:21.100 --> 00:18:24.100
to add communication between threads.

00:18:24.100 --> 00:18:28.090
One has to be a little careful
with thread confinement because

00:18:28.100 --> 00:18:31.100
you have to be careful not to
publish to global data structures.

00:18:31.100 --> 00:18:35.140
You can publish, say,
something to a global data

00:18:35.160 --> 00:18:43.100
structure by assigning it to a
static global in a source file.

00:18:43.100 --> 00:18:49.250
Or there are certainly global structures
like the NS Notification Center,

00:18:49.250 --> 00:18:53.100
which if you put an observer
in the Notification Center,

00:18:53.100 --> 00:18:57.980
the Notification Center is public,
it's global to the entire process,

00:18:57.980 --> 00:19:00.100
not just any particular thread.

00:19:00.100 --> 00:19:01.100
And so when you put an observer
in the Notification Center,

00:19:01.100 --> 00:19:01.100
it's not just a thread,
it's a whole thread.

00:19:01.100 --> 00:19:02.070
And so when you put an observer
in the Notification Center,

00:19:02.120 --> 00:19:06.100
you've published that
object to all the threads.

00:19:06.100 --> 00:19:11.090
And so you've lost any hope of thread
confinement for that particular object.

00:19:12.900 --> 00:19:15.910
Object confinement is another approach.

00:19:16.070 --> 00:19:18.540
This is one where you have,
perhaps again,

00:19:18.540 --> 00:19:23.160
a complex data structure that
you don't want to get into trying

00:19:23.160 --> 00:19:26.130
to make thread safe itself.

00:19:26.130 --> 00:19:29.330
What you can do is create
a facade type object,

00:19:29.330 --> 00:19:32.690
a front object,
some sort of proxy through which

00:19:32.700 --> 00:19:37.220
you funnel all your requests
to that complex data structure.

00:19:37.220 --> 00:19:40.400
So you create an object which
acts as a front and you can

00:19:40.460 --> 00:19:42.460
make that object thread safe.

00:19:42.950 --> 00:19:46.620
Without necessarily making
a whole big gnarly object

00:19:46.620 --> 00:19:48.810
graph behind it thread safe.

00:19:48.810 --> 00:19:52.710
And there are also other special
purpose data structures and various

00:19:52.720 --> 00:19:54.490
sorts of lockless techniques.

00:19:54.580 --> 00:19:58.100
I don't have nearly enough time
to go into those kind of issues,

00:19:58.140 --> 00:20:00.130
so I'm going to gloss over that.

00:20:02.000 --> 00:20:06.080
The second approach to
making something thread-safe,

00:20:06.550 --> 00:20:10.620
second general approach I should say,
is to augment the code.

00:20:10.620 --> 00:20:14.500
And this is the usual technique
that most of you would be,

00:20:14.500 --> 00:20:17.120
I hope,
familiar with coming into this talk.

00:20:17.210 --> 00:20:23.910
You add locks around the code
that is doing the augmentation.

00:20:24.120 --> 00:20:27.130
shows a dangerous operation.

00:20:27.330 --> 00:20:28.960
So locks are one form.

00:20:29.040 --> 00:20:34.140
Conditioned objects are
another way to protect data,

00:20:34.240 --> 00:20:38.280
protect chunks of code from
executing simultaneously.

00:20:38.280 --> 00:20:45.210
There are also atomic operations or
barrier instructions that one can insert.

00:20:45.640 --> 00:20:51.060
The nice thing about these is that
they take care of both of the issues,

00:20:51.060 --> 00:20:54.540
both the change conflict type
issues and the visibility issues.

00:20:54.540 --> 00:20:59.020
That is, if you use a lock,
the operating system has,

00:20:59.020 --> 00:21:01.650
the people working on
the operating system,

00:21:01.660 --> 00:21:04.910
I should say,
have written the locks so that both the

00:21:04.910 --> 00:21:11.090
visibility issues are addressed and the
change conflict issues are addressed.

00:21:13.100 --> 00:21:20.340
Now, once one solves the threading
issues with very simple,

00:21:20.340 --> 00:21:26.880
you know, sort of does the simple changes
to solve the threading issues,

00:21:26.880 --> 00:21:31.760
one might introduce secondary problems.

00:21:31.760 --> 00:21:36.580
And so there are whole classes of
secondary issues that come about by

00:21:36.580 --> 00:21:39.270
trying to make things thread safe.

00:21:39.340 --> 00:21:41.760
You know, deadlock and livelock
are classic examples.

00:21:43.080 --> 00:21:45.720
Performance problems can occur.

00:21:45.720 --> 00:21:51.880
One in particular would be
if many things are trying,

00:21:51.880 --> 00:21:56.820
many threads are trying to get access
to a particular data structure,

00:21:56.820 --> 00:22:00.110
you often run into issues
like lock contention,

00:22:00.120 --> 00:22:03.830
where if only one thing can change
the data structure at a time,

00:22:03.830 --> 00:22:06.420
all the other threads
are blocked waiting,

00:22:06.420 --> 00:22:09.900
trying to get in,
to get access to that data structure.

00:22:09.900 --> 00:22:11.900
And you've lost a...

00:22:13.200 --> 00:22:17.170
possibly lost some of the performance
improvement you could have had

00:22:17.350 --> 00:22:19.850
if there was no contention.

00:22:20.400 --> 00:22:32.100
[Transcript missing]

00:22:32.700 --> 00:22:35.980
It comes about when you
use per-thread data.

00:22:36.120 --> 00:22:42.060
If you use per-thread data and you very
carefully set up a data structure on a

00:22:42.060 --> 00:22:49.000
particular thread in a particular way,
and the code then is going to

00:22:49.000 --> 00:22:52.880
come along later and use that
per-thread data structure,

00:22:53.640 --> 00:22:56.840
What happens is that
the code now is coupled,

00:22:56.950 --> 00:23:01.870
is bound in some sense to that very
carefully initialized data structure.

00:23:01.870 --> 00:23:07.020
And so that code can't be just
arbitrarily run on another thread.

00:23:07.050 --> 00:23:12.540
Another thread will have a
different copy of that per-thread

00:23:12.540 --> 00:23:15.200
data if it has any at all.

00:23:15.310 --> 00:23:19.130
It doesn't necessarily have the
copy that exists on the thread

00:23:19.520 --> 00:23:21.580
the code was supposed to run on.

00:23:21.600 --> 00:23:26.340
And so this limits, this being execution
environment coupling,

00:23:26.340 --> 00:23:30.400
limits your ability to move
code around and run code on new

00:23:30.400 --> 00:23:35.340
threads when you're trying to
take advantage of multithreading.

00:23:35.340 --> 00:23:39.530
A classic example of per-thread
data that runs into this is,

00:23:39.530 --> 00:23:44.320
of course, the run loop system,
where you put sources in the run loop

00:23:44.320 --> 00:23:47.360
and the run loop is a per-thread object.

00:23:47.360 --> 00:23:50.660
And then you try to run some
code on a different thread.

00:23:50.680 --> 00:23:54.490
And those sources aren't registered
with that run loop on that thread.

00:23:57.240 --> 00:23:59.850
So those are some of the general issues.

00:23:59.860 --> 00:24:03.700
I'm going to get into a little
bit more about Cocoa now.

00:24:03.760 --> 00:24:10.520
And I'm going to cover some NSThread
APIs and some of the lock APIs that

00:24:10.520 --> 00:24:15.310
we have and some of the ways that
you can communicate between threads.

00:24:15.470 --> 00:24:19.970
Cocoa also has immutable objects
like immutable arrays and immutable

00:24:20.030 --> 00:24:21.720
data structures and so on.

00:24:21.850 --> 00:24:23.790
I'm not going to talk
any more about that,

00:24:23.920 --> 00:24:27.530
but of course, as I said before,
using immutable objects is a way of

00:24:27.530 --> 00:24:31.850
achieving a form of thread safety because
immutable objects can't be changed,

00:24:31.930 --> 00:24:35.540
and so you know you're, in some sense,
you know you're safe.

00:24:37.080 --> 00:24:41.600
Cocoa also has immutable objects
like immutable arrays and immutable

00:24:41.600 --> 00:24:43.400
data structures and so on.

00:24:43.400 --> 00:24:45.320
I'm not going to talk
any more about that,

00:24:45.420 --> 00:24:49.130
but of course, as I said before,
using immutable objects is a way of

00:24:49.130 --> 00:24:53.480
achieving a form of thread safety because
immutable objects can't be changed,

00:24:53.490 --> 00:24:57.140
and so you know you're, in some sense,
you know you're safe.

00:25:07.000 --> 00:25:11.500
I'm not going to be talking
about those functionality in

00:25:11.500 --> 00:25:13.830
any more detail in this talk.

00:25:16.330 --> 00:25:21.420
NSThread is the way you create
new threads and new threads

00:25:21.420 --> 00:25:26.880
are the way you separate work
to be done by different cores.

00:25:26.880 --> 00:25:34.440
You can create a new thread using the
DetachNewThreadSelectorToTargetWithObject

00:25:34.440 --> 00:25:35.330
method.

00:25:35.410 --> 00:25:41.780
In Leopard we now allow you to create
NSThread objects without creating

00:25:41.840 --> 00:25:45.300
the underlying operating code.

00:25:46.300 --> 00:25:54.300
This allows you to create
threads which aren't yet running.

00:25:54.380 --> 00:25:59.290
When you want the thread to run,
you use the thread method start.

00:25:59.290 --> 00:26:02.980
That creates the underlying
operating system thread and starts

00:26:02.990 --> 00:26:04.910
the thread off doing its work.

00:26:05.010 --> 00:26:06.360
What is the thread doing?

00:26:06.360 --> 00:26:12.030
The thread has called the
main method in NSThread.

00:26:12.030 --> 00:26:15.810
This is the point where if
you're subclassing NSThread,

00:26:16.230 --> 00:26:16.280
you would have to create a
thread that is not yet running.

00:26:16.280 --> 00:26:23.230
You would override main and in main do
the work that you want the thread to do.

00:26:23.540 --> 00:26:29.280
If you were doing a dedicated thread,
for example,

00:26:29.280 --> 00:26:34.040
you would probably approach that by
subclassing NSThread and overriding

00:26:34.040 --> 00:26:35.590
main to do the body of the work.

00:26:35.680 --> 00:26:39.960
When the start method is called,
what it does is it creates

00:26:39.960 --> 00:26:44.300
a new underlying operating
system thread and causes main,

00:26:44.430 --> 00:26:45.870
the main method.

00:26:46.360 --> 00:26:50.570
to be invoked in the
context of that new threat.

00:26:50.820 --> 00:26:54.790
If you have an existing
method on an existing object,

00:26:55.000 --> 00:26:59.420
we also have a convenience method
on NSThread now where you can sort

00:26:59.420 --> 00:27:04.540
of ramp an NSThread around that
method on that object and have

00:27:04.540 --> 00:27:09.450
that method be your thread body,
the work that the thread is doing.

00:27:10.980 --> 00:27:13.540
We've added a few more
features to NSThread,

00:27:13.590 --> 00:27:15.990
like cancellation.

00:27:16.100 --> 00:27:19.860
But this isn't cancellation in the
sense of POSIX Pthread cancellation.

00:27:19.860 --> 00:27:25.520
Rather, all it is is an advisory state,
a Boolean, which you can set by calling

00:27:25.600 --> 00:27:28.200
the cancel method on a thread.

00:27:28.200 --> 00:27:33.710
And what's supposed to happen is that
the code running on the thread can pull

00:27:33.710 --> 00:27:39.340
the cancellation state once in a while,
at safe points, presumably,

00:27:39.450 --> 00:27:42.980
and decide that it
should shut itself down.

00:27:42.980 --> 00:27:48.110
So it's a sort of requesting
that the thread cancel itself,

00:27:48.110 --> 00:27:51.640
that the work being done by main stop.

00:27:53.320 --> 00:27:57.110
We've added other state like
is executing and is finished.

00:27:57.200 --> 00:28:00.670
So you can find out what
the threads are doing.

00:28:00.740 --> 00:28:04.340
And NS threads are going
to be KVO compliant.

00:28:04.340 --> 00:28:09.050
I don't think they're fully KVO compliant
in the leopard seed that you have.

00:28:09.080 --> 00:28:12.980
But the intent is that
threads will be KVO compliant.

00:28:13.000 --> 00:28:16.450
And so you can hook them up
with bindings and say observe

00:28:16.450 --> 00:28:18.610
an array of threads if you wish.

00:28:18.650 --> 00:28:23.180
And perhaps even display a
UI like the activity view.

00:28:23.180 --> 00:28:28.710
And then you can use the user and mail
to show what the threads are doing.

00:28:29.190 --> 00:28:31.660
Now, of course, Cocoa also has locks.

00:28:31.660 --> 00:28:37.300
These are lock classes wrapping
the underlying Pthread,

00:28:37.300 --> 00:28:39.960
POSIX Pthread implementation.

00:28:39.960 --> 00:28:46.280
The Objective-C language also
offers the atSynchronized directive,

00:28:46.280 --> 00:28:52.660
which creates a block which is
synchronized by the object that you give,

00:28:52.660 --> 00:28:54.040
the atSynchronized directive.

00:28:54.040 --> 00:29:02.280
And all this means is that the lock
is essentially the object itself,

00:29:02.280 --> 00:29:05.220
or the object acts like the lock,
perhaps would be a

00:29:05.220 --> 00:29:06.920
better way to put that.

00:29:07.440 --> 00:29:10.730
and again, I can't talk about these
things in any detail.

00:29:10.730 --> 00:29:13.980
You have to go to the documentation
to read more about them.

00:29:13.980 --> 00:29:20.300
Condition objects wrap the underlying
POSIX Pthread condition objects.

00:29:20.300 --> 00:29:27.920
And we have a new one that we've
exposed called NSCondition in Leopard,

00:29:27.920 --> 00:29:32.280
but it exists in Tiger and
all previous releases as well.

00:29:32.280 --> 00:29:34.760
It simply wasn't exposed as an API.

00:29:36.280 --> 00:29:40.220
NSCondition is a, since it's new,
I'll talk about it a little bit,

00:29:40.260 --> 00:29:45.740
is a more powerful, I would say,
way of using condition objects

00:29:45.740 --> 00:29:47.240
than NSCondition lock was.

00:29:47.240 --> 00:29:51.280
You can do more complicated
predicates using NSConditions.

00:29:53.170 --> 00:29:56.380
But again,
I can't talk about these things in any

00:29:56.380 --> 00:29:58.260
detail because I'm very time constrained.

00:29:58.260 --> 00:30:00.260
The clock is ticking here.

00:30:01.000 --> 00:30:04.410
I talked about dedicated
threads and one way,

00:30:04.410 --> 00:30:09.420
one thing we have to support use
of dedicated threads is to be able

00:30:09.420 --> 00:30:12.080
to perform selector on thread.

00:30:12.080 --> 00:30:17.280
So you can send this message to
any object and tell that object to,

00:30:17.280 --> 00:30:22.230
in the context of the given thread,
perform that method.

00:30:22.230 --> 00:30:27.340
This is a very powerful way to
communicate information between threads.

00:30:27.340 --> 00:30:31.690
It's also a powerful way to,
if you have a dedicated thread,

00:30:31.690 --> 00:30:34.810
to give that dedicated thread work to do.

00:30:34.810 --> 00:30:40.550
You can call the work the selector,
the method that you're

00:30:40.620 --> 00:30:42.400
telling it to invoke.

00:30:42.400 --> 00:30:46.290
And, you know,
and of course the object receiver

00:30:46.290 --> 00:30:51.520
of this method is the implicit
receiver of that message as well.

00:30:51.520 --> 00:30:57.310
And so you can give that dedicated
thread work to do by using perform

00:30:57.310 --> 00:31:02.290
selector on thread as well as the
more typical use which would be

00:31:02.290 --> 00:31:05.100
to just communicate information.

00:31:05.100 --> 00:31:09.810
The perform selector on main
thread method that existed,

00:31:09.810 --> 00:31:14.220
well, since Panther at least,
I forget if we added it to

00:31:14.320 --> 00:31:19.180
Jaguar or if it was Panther,
that method is now sort of a special

00:31:19.180 --> 00:31:24.030
case of this where the thread parameter
is simply the main thread object.

00:31:26.330 --> 00:31:32.720
Now what is one way in which you can
use this perform selector mechanism?

00:31:32.720 --> 00:31:32.720
Well,

00:31:33.220 --> 00:31:36.040
There's something I like to
call the receptionist pattern,

00:31:36.060 --> 00:31:42.500
which is you can create objects
that act as a proxy or a front

00:31:43.320 --> 00:31:45.760
for a real intended receiver.

00:31:47.220 --> 00:31:51.960
And this is useful when you have
an object you want to message

00:31:51.960 --> 00:31:57.070
from the context of some thread,
but it isn't safe for that object

00:31:57.070 --> 00:32:01.360
to actually receive and do that,
invoke that method on that

00:32:01.590 --> 00:32:03.690
or any particular thread.

00:32:03.700 --> 00:32:07.120
For example,
you have an object which is only

00:32:07.120 --> 00:32:09.640
safe to use on the main thread.

00:32:09.640 --> 00:32:15.700
You can use something like a
receptionist to get the information over.

00:32:17.360 --> 00:32:21.330
So what a proxy,
the receptionist does is it's simply an

00:32:21.370 --> 00:32:26.480
object that records the messages that
it receives and arranges to have them

00:32:26.480 --> 00:32:29.160
delivered on the main thread instead.

00:32:30.160 --> 00:32:33.900
So for example,
I have a set title method here that

00:32:33.900 --> 00:32:39.160
I implemented that I want to invoke
on the real destination object.

00:32:40.160 --> 00:32:42.160
Maybe it's a window object.

00:32:43.230 --> 00:32:46.020
What I do is I implement set
title on the receptionist.

00:32:46.240 --> 00:32:49.150
And I have a set title for
the receptionist object class.

00:32:50.160 --> 00:32:54.160
And what the receptionist simply does
is it turns around and tells the target,

00:32:54.160 --> 00:32:56.780
its real target,
which I've had to initialize

00:32:56.780 --> 00:33:02.160
the receptionist with,
to perform set title on the main thread.

00:33:03.160 --> 00:33:04.160
So this is fairly straightforward.

00:33:06.150 --> 00:33:10.250
What you would do then is use the
receptionist object in place of

00:33:10.250 --> 00:33:15.640
the real object whenever you needed
to address some message to the,

00:33:15.640 --> 00:33:16.730
of course, real object.

00:33:17.960 --> 00:33:21.290
Now sometimes you have methods
which have more complicated

00:33:21.300 --> 00:33:24.980
parameters than a simple object.

00:33:24.980 --> 00:33:27.840
And you may need then
to create a memo object,

00:33:27.870 --> 00:33:35.560
which just is a data-bearing object
that remembers all the arguments.

00:33:35.560 --> 00:33:35.560
In this particular example,

00:33:36.230 --> 00:33:41.820
I'm creating a memo object where
I'm passing the ID and the int,

00:33:41.820 --> 00:33:47.420
the two arguments, to the complex method,
to the memo for it to remember.

00:33:47.420 --> 00:33:51.180
And I'm going to tell the memo,
instead of the real target,

00:33:51.240 --> 00:33:57.080
I'm going to tell the memo to
dispatch itself on the main thread,

00:33:57.080 --> 00:34:00.420
in this particular case,
with the given target.

00:34:00.420 --> 00:34:03.910
And what will happen is
that on the main thread,

00:34:03.910 --> 00:34:07.590
at some point,
the dispatch to target method will

00:34:07.590 --> 00:34:10.500
be called on the little memo object.

00:34:10.600 --> 00:34:15.040
And all the memo object does is call
complex method then on the real target

00:34:15.040 --> 00:34:17.780
that I intended to receive the message.

00:34:17.780 --> 00:34:21.970
And so that has gotten the
invocation of the complex

00:34:22.110 --> 00:34:24.950
method over to the main thread.

00:34:24.980 --> 00:34:28.780
Of course,
the logical extension of this is you

00:34:28.780 --> 00:34:33.820
can... have a receptionist object,
which is actually a proxy,

00:34:33.820 --> 00:34:36.520
doesn't implement any methods.

00:34:36.550 --> 00:34:40.050
And NS invocation would
be your memo object.

00:34:40.080 --> 00:34:45.040
The NS invocation and the forward
invocation method becomes the memo,

00:34:45.040 --> 00:34:49.380
and you can tell the invocation to
invoke itself over on the main thread by

00:34:49.380 --> 00:34:52.740
using perform selector on main thread.

00:34:56.450 --> 00:35:01.290
Now, how do you go about
making work concurrent?

00:35:01.330 --> 00:35:03.290
Let me take a little sidebar here.

00:35:03.460 --> 00:35:07.760
Well,
the traditional approach was to use,

00:35:07.930 --> 00:35:10.100
to do several completely
different things.

00:35:10.100 --> 00:35:14.060
That was a typical approach.

00:35:15.190 --> 00:35:17.920
In the future,
we're going to be seeing more of what

00:35:17.920 --> 00:35:19.620
I've highlighted in the second bullet.

00:35:19.620 --> 00:35:24.190
That is decomposing a chunk of work,
a large chunk of work,

00:35:24.240 --> 00:35:26.290
into multiple pieces.

00:35:27.500 --> 00:35:33.810
The most simple type of this
example would be where you have

00:35:33.820 --> 00:35:37.750
an array of the same type of
object as in my demo program.

00:35:37.760 --> 00:35:39.700
I had an array of images.

00:35:39.700 --> 00:35:43.910
An array where you want to do the same
thing to every object in the array.

00:35:43.970 --> 00:35:45.380
That's a very simple approach.

00:35:45.380 --> 00:35:51.740
There are many systems like OpenMP and
so on that some of you may be familiar

00:35:52.260 --> 00:35:58.160
with that help facilitate splitting up,
doing work over an array of

00:35:58.170 --> 00:36:01.990
something across multiple threads.

00:36:02.330 --> 00:36:07.460
Another thing,
another type of decomposition is where

00:36:07.460 --> 00:36:10.910
you have a big thing that you want
to work on as sort of a logically

00:36:11.240 --> 00:36:16.300
single unit where you can split it
up in some way and then successfully

00:36:16.300 --> 00:36:17.800
merge the result back together.

00:36:17.800 --> 00:36:20.030
For example,
you might have a large image which

00:36:20.030 --> 00:36:25.380
you want to color process in some
way and perhaps it's possible for

00:36:25.380 --> 00:36:29.970
you to split the image in half,
do the processing of the two

00:36:29.980 --> 00:36:35.470
halves on separate threads and then
stitch the results back together.

00:36:35.550 --> 00:36:38.110
Of course,
if what you're doing is rotating

00:36:38.110 --> 00:36:42.070
the image or applying some very
complex mesh warp algorithm

00:36:42.070 --> 00:36:45.570
or whatever it happens to be,
then it may be nearly impossible

00:36:45.570 --> 00:36:49.070
to do that on two different
threads and successfully stitch

00:36:49.070 --> 00:36:50.830
the results back together.

00:36:50.840 --> 00:36:56.420
So, of course, this is a technique that
only applies in some cases.

00:36:56.510 --> 00:36:59.920
One case would be, for example,
sorting an array.

00:36:59.920 --> 00:37:03.590
If you have a large array
and you want to sort it,

00:37:03.810 --> 00:37:07.440
well, sorting is sort of a
logically single operation.

00:37:07.440 --> 00:37:10.640
That is, you're operating on all
the elements of the array.

00:37:10.640 --> 00:37:14.150
But you can divide an
array up into two pieces,

00:37:14.320 --> 00:37:18.320
sort the two halves,
and apply a merge sort type merge

00:37:18.320 --> 00:37:22.070
operation to the two sorted arrays
to produce your final array.

00:37:22.080 --> 00:37:26.990
So there's a way to go about doing that.

00:37:29.560 --> 00:37:35.960
In Leopard, we've added an abstraction
for a task called NSOperation.

00:37:35.960 --> 00:37:40.720
And all this, and what I mean by task is
just a bit of work to do,

00:37:40.720 --> 00:37:44.210
not task in the sense of a new process.

00:37:45.240 --> 00:37:51.520
The intent here is to offer
an abstraction and a way,

00:37:51.830 --> 00:37:55.600
an approach perhaps,
to decomposing your code,

00:37:55.600 --> 00:37:58.100
to structuring your code.

00:37:58.220 --> 00:38:03.780
So the intent is to help
you design your programs.

00:38:04.960 --> 00:38:09.700
So operations can be
themselves concurrent or not.

00:38:09.780 --> 00:38:11.610
That's a key element.

00:38:12.210 --> 00:38:15.720
An operation which is concurrent
does its own threading,

00:38:15.720 --> 00:38:18.000
but operations do not
have to be concurrent.

00:38:18.050 --> 00:38:21.160
They can just be straight line
pieces of code without doing

00:38:21.310 --> 00:38:23.680
any particular threading at all.

00:38:25.710 --> 00:38:30.840
Operations also introduce
a concept of readiness.

00:38:30.970 --> 00:38:35.170
So in the base class, NSOperation,

00:38:35.570 --> 00:38:38.780
An operation is ready when
all of its dependencies,

00:38:38.780 --> 00:38:43.240
all of its dependent, the operations it
depends on have finished.

00:38:43.420 --> 00:38:46.900
So operations can depend
on other operations.

00:38:46.900 --> 00:38:50.100
Of course, the classic example of
this would be linking.

00:38:50.100 --> 00:38:54.230
When you hit the Xcode hammer button
and it goes off and builds things,

00:38:54.230 --> 00:38:58.240
well, it can't do the link until it's
done all the compile steps.

00:38:58.240 --> 00:39:02.780
And so the link step depends
on all the compile steps having

00:39:02.780 --> 00:39:05.340
finished before it can begin.

00:39:05.500 --> 00:39:07.300
work.

00:39:09.590 --> 00:39:13.790
Another point I should make, of course,
is that NS operations are KVO compliant,

00:39:13.820 --> 00:39:15.700
so you can observe them through bindings.

00:39:15.700 --> 00:39:20.620
You can observe them through key
value observing and also bind to

00:39:20.710 --> 00:39:23.320
them and display them in the UI.

00:39:24.930 --> 00:39:29.180
And as operation Q,
we introduce in Leopard to allow you

00:39:29.280 --> 00:39:33.330
to apply some form of flow control,
basically,

00:39:33.560 --> 00:39:37.240
to your execution of operations.

00:39:37.300 --> 00:39:41.160
So you may have, for example,
a thousand operations.

00:39:41.230 --> 00:39:43.850
The user hits a button and now
there's a thousand operations

00:39:43.860 --> 00:39:45.960
that need to be invoked.

00:39:46.030 --> 00:39:48.700
And of course,
the Xcode example applies here as well.

00:39:48.700 --> 00:39:52.900
You hit build and all your
source files need to be compiled.

00:39:52.950 --> 00:39:57.400
Well, obviously,
somebody at some point tried that.

00:39:57.440 --> 00:40:04.360
Spawn off a thousand GCC processes and
build them all simultaneously and then

00:40:04.360 --> 00:40:06.520
do the link step when they were all done.

00:40:06.850 --> 00:40:13.790
And they tried that and it was
an abysmal failure because...

00:40:14.760 --> 00:40:18.120
Well, there's any number of reasons,
of course.

00:40:18.120 --> 00:40:22.710
But, you know, if each GCC is using, say,
just 50 megabytes and you

00:40:23.040 --> 00:40:25.600
spawn off 1,000 of them,
well,

00:40:25.600 --> 00:40:31.870
now you have 50,000 megabytes of RAM that
are desired by all the processes running.

00:40:31.890 --> 00:40:35.180
So, you know,
it just doesn't work to spawn off and

00:40:35.180 --> 00:40:37.390
run all the GCCs at the same time.

00:40:37.390 --> 00:40:41.180
So what Xcode does, of course,
is spawns off two at a time if

00:40:41.180 --> 00:40:43.980
you're on a dual-processor Mac.

00:40:44.000 --> 00:40:46.170
So you can either use a dual-core
machine or four at a time if

00:40:46.170 --> 00:40:47.730
you're on a four-core machine.

00:40:47.820 --> 00:40:53.800
And that's the rule of thumb they've
chosen for applying concurrency.

00:40:54.100 --> 00:40:59.240
What NSOperationQ does is you can
tune the amount of concurrency you

00:40:59.240 --> 00:41:02.440
get in execution of the operations
that you put in the queue.

00:41:02.440 --> 00:41:08.150
And so if you want two at a time to run,
you can turn the knob and set

00:41:08.150 --> 00:41:13.820
two as the amount of concurrency,
or four at a time, and so on.

00:41:13.820 --> 00:41:21.060
And what happens is that operations that
are put in the queue that are ready are

00:41:21.060 --> 00:41:25.180
started as previous operations finish.

00:41:25.180 --> 00:41:28.690
And so what the OperationQ is
doing is it's churning through

00:41:28.690 --> 00:41:32.130
the operations for you,
and you can sort of turn it loose and

00:41:32.260 --> 00:41:36.350
let it do its thing once you've put
all the operations that you have in it.

00:41:36.400 --> 00:41:41.420
And OperationQ is also KVO compliant,
and so you can bind up

00:41:41.420 --> 00:41:43.410
UI like a table view.

00:41:43.820 --> 00:41:47.150
And to it,
and display all the operations that

00:41:47.260 --> 00:41:50.830
are going on in the background,
much again like the

00:41:50.830 --> 00:41:52.630
activity viewer in mail.

00:41:56.510 --> 00:41:58.230
So let me give you a simple example.

00:41:58.320 --> 00:42:01.030
This example comes from the demo.

00:42:01.160 --> 00:42:04.160
So I have a warp an image
operation which I'm going to

00:42:04.160 --> 00:42:07.880
apply over an array of images.

00:42:07.960 --> 00:42:12.750
So for each image,
I'm warping it and I'm adding the object,

00:42:12.940 --> 00:42:16.620
the new image, to the new images array.

00:42:22.560 --> 00:42:27.900
What I'm going to do is I'm going to
create a new class called Image Warp to

00:42:28.000 --> 00:42:33.090
abstract the loop, basically.

00:42:33.110 --> 00:42:36.980
I'm going to make it a non-concurrent
subclass of NSOperation.

00:42:37.060 --> 00:42:40.560
So I'm not going to worry about
doing the threading myself.

00:42:40.590 --> 00:42:45.690
What happens when a non-concurrent
operation is discovered in the

00:42:46.030 --> 00:42:50.940
queue by NSOperation queue,
it creates the thread for the

00:42:51.030 --> 00:42:56.600
operation and runs the operation in the
context of that new temporary thread.

00:42:57.070 --> 00:43:02.400
I'm also going to move that loop to a
new class method on the ImageWarp class.

00:43:02.550 --> 00:43:04.840
I'm going to call that
method ProcessImages.

00:43:04.840 --> 00:43:11.540
And what this move allows me to do is,
well, for one thing, it reduces the one,

00:43:11.540 --> 00:43:15.760
two, three,
six lines above down to one line.

00:43:15.760 --> 00:43:19.310
But it also allows me to do
something different in that loop.

00:43:19.400 --> 00:43:23.330
So I'm going to embed the logic of
the loop off in a method so I can

00:43:23.330 --> 00:43:25.960
change it in the future if I wanted to.

00:43:31.470 --> 00:43:33.400
What does process images look like then?

00:43:33.400 --> 00:43:36.830
Well,
I'm going to create an operation queue,

00:43:36.930 --> 00:43:38.090
fairly straightforward.

00:43:38.120 --> 00:43:42.490
I'm going to create an array to receive
all the new images like I did before.

00:43:42.600 --> 00:43:45.740
And for each image,
I'm going to create instead an

00:43:45.740 --> 00:43:47.810
image warp operation object.

00:43:47.810 --> 00:43:51.190
I'm going to tell it which
image it's supposed to process,

00:43:51.190 --> 00:43:54.350
and I'm going to tell it the
array in which it's supposed

00:43:54.350 --> 00:43:56.240
to add the resulting image.

00:43:56.240 --> 00:44:01.190
Then I'm going to add that
operation to the operation queue.

00:44:01.400 --> 00:44:04.810
And as soon as I begin adding
operations to the operation queue,

00:44:04.810 --> 00:44:07.500
it's going to start
churning on them right away.

00:44:07.700 --> 00:44:10.400
Meanwhile, my for loop will finish.

00:44:10.400 --> 00:44:13.580
I'll put all the operations in the queue.

00:44:13.580 --> 00:44:18.410
I'm going to wait until all
the operations are finished.

00:44:18.410 --> 00:44:22.510
I'm going to release the queue,
and I'm going to return

00:44:22.510 --> 00:44:24.580
the array of new images.

00:44:25.140 --> 00:44:28.980
Now, of course,
there was a call to warp an

00:44:28.980 --> 00:44:30.570
image in my previous loop.

00:44:30.680 --> 00:44:34.580
Well, that moves to the main
method of NSOperation.

00:44:34.580 --> 00:44:40.580
So the body of the loop becomes
the core of the operation.

00:44:41.480 --> 00:44:46.330
: So what the main method is going to do,
it's going to warp the image that it

00:44:46.480 --> 00:44:51.660
was given when I initialized the object,
and it's going to add the result to

00:44:51.660 --> 00:44:56.790
the result array that I gave it when
I initialized the operation object.

00:44:56.790 --> 00:45:00.590
Now of course, many objects,
many operation objects

00:45:00.590 --> 00:45:04.950
are poking at this array,
so I have to add some synchronization

00:45:04.950 --> 00:45:07.500
around that to protect that.

00:45:08.900 --> 00:45:13.320
Now NSOperation,
we're introducing it as a tool

00:45:13.320 --> 00:45:14.840
to help you structure your code.

00:45:14.840 --> 00:45:19.000
It's not the be-all and
end-all of threading.

00:45:19.080 --> 00:45:24.900
There are many approaches
to doing multithreading,

00:45:24.900 --> 00:45:31.000
many abstractions one
can use to assist in

00:45:31.350 --> 00:45:37.180
writing or decomposing programs
and making them concurrent.

00:45:37.790 --> 00:45:41.420
Now, of course, you can still use threads
as you have before.

00:45:41.420 --> 00:45:43.060
We haven't changed that.

00:45:43.060 --> 00:45:47.230
One thing I'm going to point out, though,
is that an operation queue

00:45:47.230 --> 00:45:50.060
is a place where you put an
operation to happen later.

00:45:50.060 --> 00:45:52.820
So if you want something
to occur right away,

00:45:52.880 --> 00:45:56.990
say you want to start an animation,
you wouldn't necessarily create

00:45:56.990 --> 00:46:02.110
an operation for that animation,
you know, create by,

00:46:02.120 --> 00:46:05.380
by which I mean structure
your code into an operation.

00:46:06.500 --> 00:46:10.880
And, you know, do that operation later by
putting in an operation queue.

00:46:10.880 --> 00:46:13.420
You want the animation
probably to start right away.

00:46:14.750 --> 00:46:20.600
Operations then don't have to be
used in the context of a queue.

00:46:20.600 --> 00:46:22.700
You can call start on one right away.

00:46:22.700 --> 00:46:26.280
If you put it in the queue,
of course at some later point

00:46:26.280 --> 00:46:29.980
the queue will get to it and call
start and start that operation.

00:46:29.980 --> 00:46:31.690
But you don't have to do that.

00:46:34.040 --> 00:46:39.110
So the key point I'm trying to
make here is that NSOperation and

00:46:39.430 --> 00:46:45.820
NSOperationQ are just one way to approach
task decomposition within your programs.

00:46:48.300 --> 00:47:56.700
[Transcript missing]

00:47:56.860 --> 00:48:02.460
So I could give you a rule of thumb
like in splitting up the pieces to say,

00:48:02.460 --> 00:48:04.740
well, how small should work be?

00:48:04.860 --> 00:48:10.750
I could give you an example,
which is don't make your work--

00:48:11.300 --> 00:48:14.350
The thing that's going to be
done by that thread less than

00:48:14.410 --> 00:48:16.300
10 microseconds of computation.

00:48:16.300 --> 00:48:18.440
And there's your answer.

00:48:20.500 --> 00:48:21.670
10 microseconds.

00:48:21.670 --> 00:48:25.410
How do I know how long this work is
going to take before I actually do it?

00:48:25.550 --> 00:48:26.260
That's one problem.

00:48:26.260 --> 00:48:30.320
If it takes 10 microseconds or
100 microseconds on this machine,

00:48:30.320 --> 00:48:32.400
well,
it might take a completely different

00:48:32.680 --> 00:48:34.900
amount of time on this other machine.

00:48:34.900 --> 00:48:42.370
And so there's no simple answer that
applies across various architectures.

00:48:42.370 --> 00:48:45.200
And of course,
as machines speed up in the future,

00:48:45.200 --> 00:48:46.760
all your answers change.

00:48:46.760 --> 00:48:50.480
And so what I would suggest is
don't worry too much about the time.

00:48:50.500 --> 00:48:54.160
Don't worry too much about
decomposing your work into

00:48:54.190 --> 00:48:56.490
two fine pieces at this point.

00:48:56.610 --> 00:49:00.700
It's just a waste of time.

00:49:00.710 --> 00:49:03.860
In a sense, you'd be over-optimizing.

00:49:03.860 --> 00:49:10.820
And tuning to a particular processor
or particular architecture like your

00:49:10.960 --> 00:49:17.250
dual-core at home won't necessarily
improve things for the user.

00:49:18.930 --> 00:49:22.480
Let me get back to
multithreading in Cocoa.

00:49:22.590 --> 00:49:25.680
Well,
I started by talking about thread safety.

00:49:25.850 --> 00:49:28.550
Well, unfortunately, at this point,
I have to just point you to the

00:49:28.560 --> 00:49:32.690
documentation to answer the question,
what APIs are thread safe

00:49:32.690 --> 00:49:34.550
in Cocoa and which aren't.

00:49:34.550 --> 00:49:40.100
There's no way I can cover that
in the timeframe allowed here.

00:49:40.370 --> 00:49:44.540
Now, as I pointed out at the beginning,
many things aren't

00:49:44.550 --> 00:49:47.300
intrinsically thread-safe.

00:49:47.300 --> 00:49:51.400
And if you don't see that a class says
it's thread-safe in the documentation,

00:49:51.400 --> 00:49:54.360
you have to assume that it's
not intrinsically thread-safe.

00:49:54.360 --> 00:49:57.590
But of course,
you can use locks and other safety

00:49:58.110 --> 00:50:02.590
measures that I've discussed
already to use these things in

00:50:02.590 --> 00:50:05.770
a multi-threaded environment.

00:50:06.610 --> 00:50:10.690
I talked earlier about transferring work.

00:50:10.690 --> 00:50:16.240
This is what I call what happens when
you use perform select on main thread.

00:50:16.240 --> 00:50:20.160
You're transferring the work that you
wanted to do on the main thread over to,

00:50:20.160 --> 00:50:22.120
say, the main thread.

00:50:22.120 --> 00:50:25.750
I mean, you're transferring the work
you wanted to do on the current

00:50:25.820 --> 00:50:29.200
thread over to a different
thread to have it do it instead.

00:50:29.660 --> 00:50:33.070
And the app kit, of course,
in some places does this

00:50:33.070 --> 00:50:34.620
sort of thing for you.

00:50:34.620 --> 00:50:39.220
If you tell a view to display,
for example, it doesn't actually do the

00:50:39.220 --> 00:50:40.940
drawing on that thread.

00:50:40.940 --> 00:50:45.060
What the view does is it arranges
for the view to be redrawn off in

00:50:45.060 --> 00:50:47.680
the context of the main thread.

00:50:49.550 --> 00:50:57.100
But again, not all is ideal here.

00:50:57.100 --> 00:50:59.860
Transferring work can
cause its own issues,

00:50:59.870 --> 00:51:02.170
and I'm going to illustrate that.

00:51:02.620 --> 00:51:07.700
So what I have here is a do work
method which only wants to run in

00:51:07.700 --> 00:51:09.330
the context of the main thread.

00:51:09.360 --> 00:51:13.100
So it tests to see if it's running
in the context of the main thread.

00:51:13.150 --> 00:51:18.180
And if it's not,
it tells itself to perform do work

00:51:18.180 --> 00:51:24.660
in the context of the main thread by
using perform selector on main thread.

00:51:25.980 --> 00:51:30.500
If the thread is the main
thread that DoWork is called on,

00:51:30.580 --> 00:51:33.580
it will actually just go ahead and do it.

00:51:33.620 --> 00:51:39.330
Now we introduce a subclass,
and the subclass wants to enhance

00:51:39.460 --> 00:51:42.890
DoWork to do some more work.

00:51:42.970 --> 00:51:46.760
So what the subclass does
is it calls super DoWork.

00:51:47.690 --> 00:51:53.400
And it does its own thing,
whatever the subclass wants to do.

00:51:53.440 --> 00:51:56.620
But this has caused two
problems to be introduced,

00:51:56.710 --> 00:51:59.160
two very subtle problems.

00:51:59.280 --> 00:52:04.340
The first is that the superclass
has not actually done its work when

00:52:04.340 --> 00:52:07.900
the subclass goes to do more stuff.

00:52:08.450 --> 00:52:13.200
The superclass pushed its
work over to the main thread

00:52:13.200 --> 00:52:14.550
to be done at a later time.

00:52:14.620 --> 00:52:17.210
Well,
what does that mean for the subclass?

00:52:17.220 --> 00:52:21.710
Has it assumed that the superclass,
because it called super do work,

00:52:21.770 --> 00:52:22.870
has done that work?

00:52:22.880 --> 00:52:26.320
Has all the data structures
that the superclass was

00:52:26.320 --> 00:52:28.760
supposed to update been updated?

00:52:28.760 --> 00:52:29.160
No.

00:52:29.160 --> 00:52:31.770
They're going to be updated
later on the main thread.

00:52:31.780 --> 00:52:37.240
The second problem is that the subclass
is going to do its thing twice.

00:52:37.480 --> 00:52:39.940
At least in this particular formulation.

00:52:39.940 --> 00:52:43.520
Do work has been called
on the background thread.

00:52:43.520 --> 00:52:47.460
And the subclass called super do work.

00:52:47.460 --> 00:52:49.610
And then it did the work.

00:52:49.640 --> 00:52:51.540
Its own work.

00:52:52.800 --> 00:52:56.750
Well,
when that do work method is re-invoked

00:52:56.880 --> 00:53:00.690
on the object in the main thread,

00:53:00.790 --> 00:53:02.830
The subclass is going
to receive that method.

00:53:02.870 --> 00:53:04.300
It's going to call super do work.

00:53:04.300 --> 00:53:07.250
And super is actually going to do
it because it's the main thread.

00:53:07.250 --> 00:53:10.820
And then it's going to do its
more stuff thing again in the

00:53:10.820 --> 00:53:12.980
context of the main thread.

00:53:12.980 --> 00:53:14.360
Well, what does that mean?

00:53:14.360 --> 00:53:15.390
Who knows?

00:53:15.390 --> 00:53:20.380
Is it okay that the subclass's
work is going to be done twice?

00:53:20.380 --> 00:53:22.590
Is the subclass prepared for that?

00:53:22.590 --> 00:53:23.370
Who knows?

00:53:23.370 --> 00:53:28.280
It depends on the particular example,
the particular thing that's going on.

00:53:28.300 --> 00:53:34.850
So transferring work like this is not
a panacea for making things thread

00:53:35.030 --> 00:53:37.740
safe or approaching thread safety.

00:53:39.400 --> 00:55:04.100
[Transcript missing]

00:55:05.150 --> 00:55:09.690
But what's not made thread-safe
thereby is the operations that occur

00:55:09.780 --> 00:55:12.430
outside the context of the set method.

00:55:12.440 --> 00:55:18.890
KVO is called that for you before
your set foo method is invoked.

00:55:18.940 --> 00:55:22.620
And those are not being wrapped
by that lock you introduced.

00:55:22.620 --> 00:55:25.600
So those things are not thread-safe.

00:55:25.690 --> 00:55:30.780
And what happens then is that in
the did change value for key method,

00:55:30.780 --> 00:55:33.640
when the observers are
being all notified,

00:55:34.380 --> 00:55:38.300
the KVO subsystem goes down the line,
tells all the observers,

00:55:38.300 --> 00:55:40.560
observe value at key path and so on.

00:55:40.560 --> 00:55:44.150
All that is happening outside
the context of a lock.

00:55:44.160 --> 00:55:49.260
And it's all happening in
some sort of arbitrary order.

00:55:49.260 --> 00:55:50.980
So if an observer...

00:55:52.280 --> 00:55:56.660
believes the information that it
receives in the change dictionary,

00:55:56.730 --> 00:56:00.600
that information may be out
of date if multiple objects,

00:56:00.600 --> 00:56:02.890
multiple threads rather,
are changing this

00:56:02.900 --> 00:56:05.260
property of this object.

00:56:05.350 --> 00:56:10.280
Those observances,
those notifications are being received in

00:56:10.280 --> 00:56:13.890
essentially kind of an arbitrary order.

00:56:15.310 --> 00:56:20.240
Well, one solution is, of course,
to use manual notification and put

00:56:20.330 --> 00:56:25.160
the lock around the will change
value for key and the change in

00:56:25.160 --> 00:56:26.540
the did change value for key.

00:56:26.540 --> 00:56:29.560
And what this does is it
makes sure that nobody else,

00:56:29.650 --> 00:56:34.540
no other thread,
changes that foo property until all

00:56:34.830 --> 00:56:40.800
the observers have been notified
of the change as a result of

00:56:41.140 --> 00:56:43.770
this current thread changing it.

00:56:44.730 --> 00:56:53.020
Of course, having to use manual
KVO notification is kind of a bummer.

00:56:53.020 --> 00:56:57.330
The automatic notification is
a very convenient mechanism.

00:56:57.770 --> 00:57:03.360
One approach to fixing that would be
to use a receptionist type pattern to

00:57:03.360 --> 00:57:12.480
push the actual change and the resulting
KVO observances over to the main thread.

00:57:12.920 --> 00:57:19.290
Another class of things
that arise relatedly is,

00:57:19.290 --> 00:57:21.140
of course, bindings in Cocoa.

00:57:21.180 --> 00:57:26.400
Cocoa views and currently
Cocoa controllers are not

00:57:26.560 --> 00:57:28.760
particularly thread-safe.

00:57:29.480 --> 00:57:34.370
And so what one has to do then is
get those KVO notifications over

00:57:34.380 --> 00:57:40.660
to the main thread in some way to
be invoked in the safe context.

00:57:40.720 --> 00:57:44.130
That is,
views should be generally accessed

00:57:44.130 --> 00:57:48.320
in the main thread for the
work that the KVO notification

00:57:48.340 --> 00:57:50.460
is going to cause in them.

00:57:52.330 --> 00:57:56.060
The problem is that it's not clear how
to do that with bindings at this point.

00:57:56.060 --> 00:57:59.520
And this is an active area
of investigation for us.

00:57:59.520 --> 00:58:04.280
Cocoa is doing most of the work
for you underneath the covers.

00:58:04.280 --> 00:58:09.210
That is,
it's doing the KVO ad observer calls,

00:58:09.210 --> 00:58:11.000
the KVO registration.

00:58:11.000 --> 00:58:15.240
You don't really have a hook into that.

00:58:15.280 --> 00:58:17.660
So you can't substitute
a different object.

00:58:17.800 --> 00:58:21.220
You can't substitute, say,
a receptionist-type object to

00:58:21.490 --> 00:58:25.630
capture those KVO messages and
send them over to the main thread.

00:58:27.330 --> 00:58:31.730
So as I said, this is an active area
of investigation for us,

00:58:31.730 --> 00:58:38.340
how to enable multithreading
with the bindings,

00:58:38.350 --> 00:58:40.990
or at least make it more convenient.

00:58:41.740 --> 00:58:44.460
Core data, another aspect of Cocoa.

00:58:44.460 --> 00:58:46.860
Well, of course,
if you're on the Cocoa dev mailing list,

00:58:46.860 --> 00:58:49.360
you've seen any number
of discussions on this.

00:58:49.360 --> 00:58:53.790
And I can't go into any detail,
but they recommend that if

00:58:53.800 --> 00:58:58.440
you use a separate managed
object context for each thread,

00:58:58.440 --> 00:59:02.840
the locking will be taken
care of for you by core data.

00:59:03.580 --> 00:59:07.480
Some classes in core data
implement NS locking and can be

00:59:07.600 --> 00:59:11.530
locked explicitly if you need to,
but you really have to go

00:59:11.530 --> 00:59:15.400
see the documentation to
find out more about this.

00:59:17.570 --> 00:59:22.490
Sometimes Cocoa already
offers built-in concurrency.

00:59:22.490 --> 00:59:27.420
You can tell a file handle, for example,
to read in background and notify,

00:59:27.420 --> 00:59:32.480
and Cocoa will spawn off the thread
and do the read in the background.

00:59:32.480 --> 00:59:36.480
The pulsing button animations that occur,
say when you bring up an open

00:59:36.480 --> 00:59:39.740
panel and the OK button is pulsing,
those are occurring in

00:59:39.740 --> 00:59:42.130
a background thread,
and those things are

00:59:42.130 --> 00:59:43.700
taken care of for you.

00:59:44.540 --> 00:59:47.550
The ideal situation
for you as a developer,

00:59:47.550 --> 00:59:51.660
of course, would be for Cocoa to
do a lot of concurrency,

00:59:51.660 --> 00:59:55.900
to do a lot of use of the separate
cores available in the machine,

00:59:55.900 --> 00:59:58.990
so you can just write your
straight-line code without having

00:59:58.990 --> 01:00:00.980
to think about any of these issues.

01:00:02.650 --> 01:00:09.340
The problem comes about because
when we need to call back into you,

01:00:09.340 --> 01:00:11.480
we don't know that you're thread safe.

01:00:11.620 --> 01:00:14.710
For example,
I talked about sorting an array earlier.

01:00:14.940 --> 01:00:18.720
We could split the array in
the sorting array function.

01:00:18.800 --> 01:00:22.990
We can split the array into two,
sort the two halves,

01:00:22.990 --> 01:00:25.840
and merge the result back together.

01:00:25.900 --> 01:00:30.820
But the sorting process involves
calling your compare function.

01:00:31.280 --> 01:00:37.140
Now, it seems like compare functions,
compare methods, whatever,

01:00:37.140 --> 01:00:39.040
should be immutable.

01:00:39.040 --> 01:00:45.370
They shouldn't be changing things
as a result of being called.

01:00:45.380 --> 01:00:48.480
That is,
nothing should change out in memory

01:00:48.480 --> 01:00:51.280
as a result of the compare operation.

01:00:51.430 --> 01:00:53.380
But we don't know that.

01:00:53.490 --> 01:00:55.390
That's a problem.

01:00:56.410 --> 01:01:01.890
Another example would be, say,
in computing TableView's row update

01:01:02.540 --> 01:01:06.730
information or in saving documents.

01:01:07.150 --> 01:01:13.010
But in the case of a table view,
it needs to compute its row information,

01:01:13.030 --> 01:01:17.670
row updates, and so on,
what goes in the boxes, for example,

01:01:17.750 --> 01:01:20.430
by talking to the data source.

01:01:20.490 --> 01:01:21.720
The data source comes from you.

01:01:21.720 --> 01:01:26.410
We don't know that it would be safe
to start calling the data source

01:01:26.410 --> 01:01:29.270
concurrently in multiple threads.

01:01:29.340 --> 01:01:32.570
Saving a document, another example.

01:01:32.910 --> 01:01:37.230
But we don't know that the delegate
won't be surprised when it's

01:01:37.240 --> 01:01:41.020
called on a background thread
to actually go and package up

01:01:41.020 --> 01:01:42.730
the data and do the save operation.

01:01:42.740 --> 01:01:48.850
So one thing we want you to think about
going forward here is making perhaps

01:01:49.330 --> 01:01:55.740
chunks of your code like delegates and
data sources or compare operations.

01:01:55.740 --> 01:02:01.260
Things where you give,
you sort of hook in to the Cocoa system.

01:02:02.020 --> 01:02:04.420
Think about making those threads safe.

01:02:04.420 --> 01:02:10.700
And so in the future, we can,
when we add, say,

01:02:10.700 --> 01:02:15.840
suppose we add a functionality
like set thread safe delegate

01:02:15.850 --> 01:02:18.060
on a particular class.

01:02:18.060 --> 01:02:22.830
Well, you can then at that point
immediately take advantage of

01:02:23.010 --> 01:02:25.450
that kind of new capability.

01:02:28.220 --> 01:02:32.020
Think about making those threads safe.

01:02:32.020 --> 01:02:38.560
And so in the future, we can,
when we add, say,

01:02:38.690 --> 01:02:38.690
suppose we add a functionality
like set thread safe delegate

01:02:38.690 --> 01:02:38.690
on a particular class.

01:02:38.690 --> 01:02:38.690
Well, you can then at that point
immediately take advantage of

01:02:38.690 --> 01:02:38.690
that kind of new capability.

01:02:38.910 --> 01:02:44.450
But it seems like in the future,
multiple cores are the way

01:02:44.460 --> 01:02:47.050
that processors are going.

01:02:47.640 --> 01:02:59.820
And to use multiple cores,
you have to use multiple threads

01:02:59.820 --> 01:02:59.820
because thread is the concept that
the operating system provides to

01:02:59.820 --> 01:02:59.820
address work to those different cores.

01:03:00.210 --> 01:03:03.580
But there are a lot of complex issues.

01:03:03.630 --> 01:03:08.640
Cocoa has some APIs to help you
deal with these multi-threading

01:03:08.640 --> 01:03:11.620
problems and the various issues.

01:03:11.650 --> 01:03:16.980
And in the future,
we expect to have more and do more.

01:03:17.010 --> 01:03:24.900
And we're ourselves investigating
these things ongoing.

01:03:24.900 --> 01:03:24.900
But to get

01:03:25.600 --> 01:03:28.710
Real nice performance
improvement in the future.

01:03:28.710 --> 01:03:34.900
You're going to have to start doing
threading and being multi-threaded.

01:03:35.180 --> 01:03:42.380
and perhaps after a few years when we
have these eight processor systems,

01:03:42.500 --> 01:03:48.100
your apps will be just so much
better than they are today.

01:03:48.100 --> 01:03:48.100
Thank you.