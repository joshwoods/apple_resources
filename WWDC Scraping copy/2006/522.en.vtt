WEBVTT

00:00:10.520 --> 00:00:11.510
Good afternoon.

00:00:11.680 --> 00:00:14.300
Welcome to our session today,
Scientific Computing

00:00:14.300 --> 00:00:15.990
Clusters on Mac OS X.

00:00:15.990 --> 00:00:17.280
My name is Todd Dailey.

00:00:17.280 --> 00:00:20.510
I'm a consulting engineer on the
business side of Apple and we've got

00:00:20.510 --> 00:00:22.140
a few other presenters for you today.

00:00:22.240 --> 00:00:26.000
We've got Warner Yuen who's a consulting
engineer on the education side,

00:00:26.000 --> 00:00:30.350
Yuusuf Abdulghani who's a performance
architect both doing their benchmarks

00:00:30.350 --> 00:00:34.370
and working on the performance tools,
and Josh Durham who's a technical

00:00:34.370 --> 00:00:38.400
services consultant who's been
involved in the Virginia Tech Cluster.

00:00:38.460 --> 00:00:40.940
We'll have a couple other
people coming up for Q&A later.

00:00:40.940 --> 00:00:42.400
I'll introduce them to you then.

00:00:42.400 --> 00:00:46.580
But what we're going to do today is
we're going to work up the stack of

00:00:46.640 --> 00:00:51.390
high performance computing cluster
and show you everything that you need,

00:00:51.420 --> 00:00:53.380
give you an update on
all the different pieces.

00:00:53.410 --> 00:00:55.400
Obviously we've done
some updates this week.

00:00:55.400 --> 00:00:57.400
We've introduced a great new server.

00:00:57.400 --> 00:01:00.590
We've introduced some other great
new tools and we've got some

00:01:00.590 --> 00:01:02.400
great stuff coming in Leopard.

00:01:02.400 --> 00:01:06.400
So we'll kind of use this as an
architecture to work through and show

00:01:06.400 --> 00:01:06.400
you everything that's going on currently.

00:01:06.400 --> 00:01:07.510
We'll kind of use this as an
architecture to work through and show

00:01:07.510 --> 00:01:08.400
you everything that's going on currently.

00:01:08.400 --> 00:01:12.220
with clusters and how you can use that.

00:01:12.380 --> 00:01:15.490
So the first thing we want to do is
give you an update on Apple deployments

00:01:15.500 --> 00:01:19.970
and talk about what we've done lately,
what people have been doing with our

00:01:19.970 --> 00:01:24.240
XServe G5 products and expanding those
and how they've used those today in

00:01:24.240 --> 00:01:28.800
deployments that they're using for
real science for real purposes today.

00:01:28.800 --> 00:01:32.960
One of those that we've recently
done is the Mayo Clinic and the,

00:01:32.960 --> 00:01:35.290
oh I'm sorry, let me step back.

00:01:35.300 --> 00:01:39.800
One of those that, so why would you use
Apple for scientific clusters?

00:01:39.800 --> 00:01:42.710
There's a lot of tools that
are built in there that are

00:01:42.710 --> 00:01:46.030
useful to you that you might,
that provide a lot of benefits

00:01:46.040 --> 00:01:47.800
to you for clusters today.

00:01:47.840 --> 00:01:54.160
Obviously we've got Mac OS X Server,
a powerful Unix based and Leopard Unix

00:01:54.160 --> 00:01:58.300
operating system that provides you with
a lot of tools that are built in there.

00:01:58.300 --> 00:02:00.720
We've got all the built
in tools like Python,

00:02:00.720 --> 00:02:04.260
like Ruby coming up in Leopard tools
like Ruby on Rails that are all

00:02:04.260 --> 00:02:06.280
just built in ready for you to use.

00:02:06.300 --> 00:02:07.300
We've got Xgrid which
we've done some updates to.

00:02:07.300 --> 00:02:08.300
We've got the XServe G5,
which we've done some updates to.

00:02:08.300 --> 00:02:09.290
We've got the XServe G5,
which we've done some updates to.

00:02:09.300 --> 00:02:09.860
We've got the XServe G5,
which we've done some updates

00:02:09.860 --> 00:02:10.300
to with Xgrid 2 for Leopard.

00:02:10.300 --> 00:02:11.490
We've got the XServe G5,
which we've done some updates

00:02:11.490 --> 00:02:12.300
to with Xgrid 2 for Leopard.

00:02:12.300 --> 00:02:16.300
But it's a great easy to use clustering
tool that allows you to run jobs.

00:02:16.300 --> 00:02:20.300
Josh will be showing you a
demo later of how that works.

00:02:20.300 --> 00:02:24.330
We've got all the great tools
in Xcode 2.4 and Yusuf will

00:02:24.330 --> 00:02:26.300
be showing you some of that.

00:02:26.300 --> 00:02:30.300
And it's a popular and a well understood
platform within the science community.

00:02:30.300 --> 00:02:32.300
And that's a very important point.

00:02:32.300 --> 00:02:36.420
If you go down to the Apple and
Science Lab downstairs,

00:02:36.420 --> 00:02:38.800
you can see all of the different
applications that are built in there.

00:02:38.800 --> 00:02:40.710
You can see all of the different
applications that people

00:02:40.720 --> 00:02:41.800
are using our platform for.

00:02:41.800 --> 00:02:45.160
You can see the hyper wall,
the big multi-monitor wall

00:02:45.160 --> 00:02:46.800
that people are using.

00:02:46.800 --> 00:02:49.800
Just amazing innovations
going on on the platform.

00:02:49.800 --> 00:02:53.010
So one of those people that's
using our cluster and expanding

00:02:53.010 --> 00:02:54.790
that today is the Mayo Clinic.

00:02:54.800 --> 00:02:56.800
The Mayo Clinic is doing
molecular modeling.

00:02:56.800 --> 00:02:58.800
They're developing drug therapies.

00:02:58.800 --> 00:03:01.930
So they're doing kind of
theoretical therapies trying to

00:03:01.930 --> 00:03:05.800
figure out if a drug could work,
for example, as a cure for cancer.

00:03:05.800 --> 00:03:08.300
And this is very complicated.

00:03:08.300 --> 00:03:11.300
They have a 295 XServe
cluster that's running.

00:03:11.300 --> 00:03:14.300
This replaced an old Xeon
cluster that they had.

00:03:14.300 --> 00:03:16.590
And with that,
they were able to reduce the time to

00:03:16.590 --> 00:03:20.300
run one of their models through from
27 minutes down to just five minutes.

00:03:20.300 --> 00:03:24.200
So a lot of benefits for them
immediately with the XServe G5

00:03:24.200 --> 00:03:26.300
by being able to deploy that.

00:03:26.430 --> 00:03:30.300
Another customer that I'm
sure you've heard of is Colsa.

00:03:30.300 --> 00:03:35.160
And we'll have Eric Wages who is
involved in running that cluster

00:03:35.160 --> 00:03:37.800
up to talk to you later on the Q&A.

00:03:37.800 --> 00:03:42.800
So Colsa provides computational
resources basically under contract

00:03:42.800 --> 00:03:47.790
to whoever would like to have
their cluster housed with them.

00:03:47.800 --> 00:03:51.800
They're number 21 currently
on the supercomputing top 500.

00:03:51.800 --> 00:03:57.790
They've got 1,536 of our
cluster nodes deployed today.

00:03:57.800 --> 00:04:00.620
And some things that they've done,
you've probably heard

00:04:00.630 --> 00:04:01.800
about their cluster before.

00:04:01.800 --> 00:04:04.800
But some things that they've
done since last year,

00:04:04.800 --> 00:04:07.300
they've upgraded the cluster from
gigabit Ethernet on the cluster.

00:04:07.300 --> 00:04:09.300
From Ethernet on the
backbone to mirror net.

00:04:09.300 --> 00:04:11.300
This provides them with a lot more speed.

00:04:11.300 --> 00:04:15.290
Warner will be showing you later
some benchmarks that show that.

00:04:15.300 --> 00:04:17.300
They've got XSAN deployed now.

00:04:17.300 --> 00:04:21.300
They've got 41 XSAN clients
going back to XServe RAID.

00:04:21.300 --> 00:04:23.300
And they use this cluster
to do missile research.

00:04:23.300 --> 00:04:27.300
So they do a lot of research
about hypersonic missiles.

00:04:27.300 --> 00:04:31.300
And to do that, they use something called
computational flow dynamics.

00:04:31.300 --> 00:04:35.560
A very complicated way of
modeling that to basically see

00:04:35.560 --> 00:04:36.800
theoretically where things are going.

00:04:36.800 --> 00:04:40.800
So it's a very productive
cluster for them.

00:04:40.800 --> 00:04:43.270
And we've got some other
customers out there that have

00:04:43.270 --> 00:04:44.800
very large clusters as well.

00:04:44.800 --> 00:04:48.930
So besides Colsa up there on the top,
you can see as we go down,

00:04:48.930 --> 00:04:51.800
we've got some other big clusters
that are already out there.

00:04:51.800 --> 00:04:55.790
We've got Virginia Tech with 1,100 nodes.

00:04:55.800 --> 00:04:57.800
We've got UIUC with 512.

00:04:57.800 --> 00:05:00.780
We've got UCLA with 256.

00:05:00.800 --> 00:05:02.790
Bowie State with 224.

00:05:02.820 --> 00:05:06.300
And we've got hundreds of deployments
that are out there that are still going.

00:05:06.300 --> 00:05:09.290
And we've got a lot of other clusters
out there that are smaller than that.

00:05:09.300 --> 00:05:14.510
And those people in many cases are
using our prebuilt configurations that

00:05:14.510 --> 00:05:16.290
we saw as the Apple Workgroup cluster.

00:05:16.300 --> 00:05:21.300
And so we've got a lot of 8, 16, 32,
64 node clusters out there.

00:05:21.300 --> 00:05:25.300
A lot of those in education where
people are using these for educational

00:05:25.300 --> 00:05:30.540
or departmental or teaching purposes
to be able to take their cluster and

00:05:30.550 --> 00:05:33.300
do research on a departmental level.

00:05:33.300 --> 00:05:35.800
In many cases, in addition to larger
clusters that they might have.

00:05:35.800 --> 00:05:40.770
You can get into one of these Workgroup
clusters pretty inexpensively.

00:05:40.800 --> 00:05:43.910
So we see a lot of departments that
buy them just because it's within

00:05:43.980 --> 00:05:45.800
their discretionary budget to do so.

00:05:45.810 --> 00:05:49.550
And they can have their own cluster
to play with versus having to

00:05:49.550 --> 00:05:53.800
compete for resources on a campus
or on an entire enterprise level.

00:05:53.800 --> 00:05:55.790
You can get this prebuilt.

00:05:55.800 --> 00:05:58.410
Well,
you can get this with qualified for many

00:05:58.410 --> 00:06:00.800
other applications that are out there.

00:06:00.800 --> 00:06:02.790
We qualify that with MATLAB.

00:06:02.850 --> 00:06:05.300
We qualify it with Grid Mathematica.

00:06:05.300 --> 00:06:08.300
We qualify it with
Inquiry from the Bio team,

00:06:08.300 --> 00:06:09.800
which does bioinformatics.

00:06:09.800 --> 00:06:12.790
And it's also a great platform
for just open source code.

00:06:12.800 --> 00:06:16.040
There's hundreds of open source
projects that are out there that

00:06:16.040 --> 00:06:17.800
just compile and run just fine.

00:06:17.800 --> 00:06:22.180
Clustered code projects that
just run fine on our clusters.

00:06:24.140 --> 00:06:28.540
So talking about the HPC stack
itself and starting to work up that,

00:06:28.550 --> 00:06:32.420
we're going to start at the bottom and
start looking at the hardware platform.

00:06:32.420 --> 00:06:35.770
Obviously you've got to have some
servers before you can start building

00:06:35.770 --> 00:06:38.870
the interconnects and building the
applications to run on top of that.

00:06:39.340 --> 00:06:43.170
So to start with servers,
we've got a great new server to

00:06:43.170 --> 00:06:45.170
talk about at this conference.

00:06:45.190 --> 00:06:49.030
You won't be able to get it for a little
while but it's still a great platform

00:06:49.040 --> 00:06:54.000
and we're really excited about the,
now the rechristened XServe and the quad

00:06:54.000 --> 00:06:56.500
Xeon processor capabilities that it has.

00:06:56.500 --> 00:06:59.010
This thing is just a computing monster.

00:06:59.010 --> 00:07:01.780
You've got four Xeon
cores that are in there.

00:07:01.780 --> 00:07:04.790
You've got an immense
cache on the processor.

00:07:04.790 --> 00:07:08.540
You've got very fast hard drive
options that you have on that.

00:07:08.920 --> 00:07:10.970
You've got an immense RAM capability.

00:07:10.980 --> 00:07:14.570
If you haven't seen the new XServe yet,
many of you may not know,

00:07:14.570 --> 00:07:17.420
we've got those down in
the IT lab downstairs.

00:07:17.420 --> 00:07:20.500
If you do want to see that,
that lab closes today at 5.

00:07:20.500 --> 00:07:23.000
But if you want to see one up close,
there's,

00:07:23.000 --> 00:07:25.940
I know there's a few around different
places but the IT lab is one place

00:07:25.970 --> 00:07:27.580
that you can go and see it today.

00:07:27.580 --> 00:07:29.720
So this is just a fantastic product.

00:07:29.730 --> 00:07:33.310
We've got the server out on
Intel now so that you can see that.

00:07:35.700 --> 00:07:39.450
And on top of that of course you've
got to run an operating system.

00:07:39.450 --> 00:07:46.000
With the XServe Intel announcement we
also announced Mac OS X Server running

00:07:46.000 --> 00:07:49.200
on Intel 1047 that's available today.

00:07:49.200 --> 00:07:52.200
Although the server is not going to
be available for a little while yet.

00:07:52.200 --> 00:07:56.050
But that's got a 64-bit memory
address space for command line

00:07:56.050 --> 00:07:57.700
apps for compiled Unix apps.

00:07:57.700 --> 00:08:01.060
It's got just announced Xcode 2.4.

00:08:01.180 --> 00:08:04.420
Yuusuf will talk about some of the tools
and some of the updates that we've done

00:08:04.440 --> 00:08:06.200
in that for performance optimization.

00:08:06.220 --> 00:08:09.700
And that can act as an Xgrid controller.

00:08:09.780 --> 00:08:14.690
So again you can right out of the bat
with that control up to 128 nodes without

00:08:14.690 --> 00:08:19.660
going to any job managers or any of the
things that Josh is going to show you.

00:08:20.480 --> 00:08:23.300
And then in Leopard Server the
story gets even better.

00:08:23.300 --> 00:08:27.520
In Leopard Server we've extended
the 64-bitness from the command

00:08:27.520 --> 00:08:28.900
line all the way up the stack.

00:08:28.970 --> 00:08:32.330
Now that may not seem to be too
important for a cluster job,

00:08:32.340 --> 00:08:34.640
a lot of that stuff operates
at the command line.

00:08:34.700 --> 00:08:37.280
But we expect that some of
our partners out there who

00:08:37.280 --> 00:08:40.510
make tools for small clusters,
it's very conceivable that you

00:08:40.510 --> 00:08:43.880
could have GUI applications that
might need 64-bit access to memory,

00:08:43.880 --> 00:08:45.300
to large amounts of memory.

00:08:45.370 --> 00:08:48.200
And we've got a huge memory capacity
as you know on the new XServe.

00:08:48.200 --> 00:08:53.100
So you can access all of that memory when
Leopard comes out from any application

00:08:53.100 --> 00:08:54.790
that you can build with Xcode.

00:08:54.870 --> 00:08:59.040
We'll have Xcode 3,
we'll have the Xgrid 2 controller.

00:08:59.070 --> 00:09:03.160
There's a session tomorrow on Xgrid 2 if
you want to learn some more about that.

00:09:03.220 --> 00:09:06.190
And one important feature that we
haven't talked about much at the

00:09:06.190 --> 00:09:10.000
conference is that Leopard Server will
be conformant with the UNIX spec.

00:09:10.040 --> 00:09:13.500
So we've always said that
Mac OS X is UNIX based.

00:09:13.590 --> 00:09:17.100
But now we can say with
Leopard that OS X is UNIX.

00:09:17.100 --> 00:09:19.940
It conforms with the POSIX specs,
it's been certified.

00:09:20.010 --> 00:09:22.140
It'll be an actual UNIX platform.

00:09:22.170 --> 00:09:25.900
And so the code will be much
more compatible with any sort of

00:09:25.900 --> 00:09:29.250
random UNIX code that you want to
throw on the machine and compile.

00:09:29.300 --> 00:09:33.100
And we've also added a DTrace
facility within Leopard Server now.

00:09:33.100 --> 00:09:36.890
So there's other sessions on that
this week and you can see about that.

00:09:36.930 --> 00:09:40.460
But that provides you with a very
low-level debugging function.

00:09:40.500 --> 00:09:42.960
And so with that intro,
I'm going to hand things over to Warner.

00:09:42.960 --> 00:09:45.300
And Warner's going to talk
about the interconnection.

00:09:45.300 --> 00:09:47.690
Thanks.

00:09:51.670 --> 00:09:52.600
Thanks Todd.

00:09:52.600 --> 00:09:53.500
I'm Warner Yuen.

00:09:53.500 --> 00:09:56.590
I'm a consulting engineer with
the education division of Apple.

00:09:56.600 --> 00:10:02.280
And I'm going to be talking
about interconnects.

00:10:02.280 --> 00:10:02.280
Oops, wrong way.

00:10:02.750 --> 00:10:04.460
And what are high
performance interconnects?

00:10:04.460 --> 00:10:09.230
So these are specialized interconnects
that don't use the TCP/IP stack but

00:10:09.230 --> 00:10:13.880
instead the data flows directly from
the network to the memory on the

00:10:13.930 --> 00:10:17.880
computer and that way the processors
don't actually have to wait for

00:10:17.980 --> 00:10:19.820
the data in order to act upon it.

00:10:19.820 --> 00:10:24.850
And also these interconnects have
a high bandwidth capability so it's

00:10:24.850 --> 00:10:29.250
not just having the data flow fast
to the processors but also you can

00:10:29.440 --> 00:10:31.260
flow lots of data to the processors.

00:10:31.260 --> 00:10:35.280
In general these are external
interface cards and the link is

00:10:35.300 --> 00:10:39.660
either copper or fiber optic and
they are connected to purpose built

00:10:39.670 --> 00:10:45.000
hardware switches designed especially
for high performance computing.

00:10:45.240 --> 00:10:48.870
A little bit on when to use and when to
choose high performance interconnects

00:10:48.870 --> 00:10:51.200
for your high performance computer.

00:10:51.200 --> 00:10:58.550
So the idea is that interconnect
selection can actually drastically affect

00:10:58.550 --> 00:11:00.200
the performance of your application.

00:11:00.200 --> 00:11:05.200
These days often times a gigabit Ethernet
provides pretty good performance.

00:11:05.210 --> 00:11:07.930
However there are times when
there is a parallel computing

00:11:07.930 --> 00:11:13.200
code that passes lots of messages
from one machine to the other.

00:11:13.200 --> 00:11:16.230
And you don't want it to spend
a lot of time communicating

00:11:16.340 --> 00:11:18.200
back from machine to machine.

00:11:18.200 --> 00:11:22.200
So you need a high performance
interconnect with low latency.

00:11:22.200 --> 00:11:25.650
And then there are other times
there is code that has very large

00:11:25.660 --> 00:11:29.200
messages that it's trying to pass
from one machine to the next.

00:11:29.200 --> 00:11:33.200
And you don't want to saturate the fabric
just by sending messages back and forth.

00:11:33.200 --> 00:11:35.200
And of course a combination of the two.

00:11:35.200 --> 00:11:38.200
Lots of messages and very large messages.

00:11:38.200 --> 00:11:41.150
So those are times to choose a
high performance interconnect.

00:11:41.200 --> 00:11:46.960
Interestingly another time that you
want to select a high performance

00:11:46.960 --> 00:11:51.200
interconnect is if you are in
a shared computing resource.

00:11:51.200 --> 00:11:54.530
So the idea is if you build
a shared compute resource and

00:11:54.530 --> 00:11:58.720
you want a variety of users,
a high performance interconnect can often

00:11:58.750 --> 00:12:01.200
attract more users to your computer.

00:12:01.200 --> 00:12:05.170
And thus offering better
utilization of your resources.

00:12:05.190 --> 00:12:09.180
And the reason for that of course is
you have a variety of users out there

00:12:09.190 --> 00:12:09.200
with a broad range of requirements.

00:12:09.200 --> 00:12:14.200
And you can't predict what each
person is going to require.

00:12:14.200 --> 00:12:18.490
So if you can offer a high
performance interconnect you

00:12:18.490 --> 00:12:21.190
can make more customers happy.

00:12:21.830 --> 00:12:25.320
So let's take a look at what some
of these interconnects are like.

00:12:25.320 --> 00:12:27.750
So there's the MirrorNet 2000 components.

00:12:27.750 --> 00:12:32.800
This Miracom and MirrorNet are one of
the earliest interconnects out there.

00:12:32.800 --> 00:12:34.960
And they are PCIX interfaces.

00:12:34.960 --> 00:12:39.770
They include offload engines with memory,
processor and firmware.

00:12:39.830 --> 00:12:41.800
And the switches scale
to thousands of nodes.

00:12:41.800 --> 00:12:46.820
In fact the majority of the high
performance interconnects on the top 500

00:12:46.820 --> 00:12:49.800
lists are currently MirrorNet 2000 based.

00:12:49.800 --> 00:12:54.790
The LINQ cables are nice fiber optic
pairs up to 200 meters in length.

00:12:54.800 --> 00:12:58.830
And the software stack includes a driver,
interface firmware,

00:12:58.830 --> 00:13:01.800
libraries and middleware
to run all of this.

00:13:03.200 --> 00:13:05.910
About two months ago,
MirrorNet actually introduced

00:13:05.980 --> 00:13:09.060
a new interface card,
the MirrorNet 10G components

00:13:09.060 --> 00:13:10.700
for high performance computing.

00:13:10.700 --> 00:13:15.720
So this is a newer PCI Express based
10 gigabit Ethernet card from

00:13:15.740 --> 00:13:21.500
MirrorCom that offers not just
a 10 gigabit Ethernet capability,

00:13:21.500 --> 00:13:25.620
but it also offers the ability
to use it in a high performance,

00:13:25.620 --> 00:13:28.300
low latency computing environment.

00:13:28.410 --> 00:13:31.800
The switches are standard
gigabit Ethernet switches that

00:13:31.800 --> 00:13:37.300
work in conjunction with the
PCI Express Mirror10G cards.

00:13:37.390 --> 00:13:42.270
The link cables are either copper
or fiber optic and it also includes

00:13:42.270 --> 00:13:47.130
all of the software drivers
required to run it from MirrorCom.

00:13:47.590 --> 00:13:52.850
And a third form of interconnect probably
first made famous by Virginia Tech with

00:13:52.870 --> 00:13:57.240
their System 10 cluster is InfiniBand.

00:13:57.240 --> 00:13:58.740
So the SilverStorm InfiniBand.

00:13:58.740 --> 00:14:02.040
InfiniBand is currently
offered in two forms.

00:14:02.060 --> 00:14:05.830
Here you see the PCI-X version
and the PCI-Express version.

00:14:05.970 --> 00:14:11.040
The main difference being the
PCI-Express offers a dual 4x bandwidth

00:14:11.040 --> 00:14:16.580
or much higher bandwidth with
the same latency as the 4x bandwidth.

00:14:16.580 --> 00:14:20.980
Switches up to 288 ports in a
single chassis so you can build a

00:14:20.980 --> 00:14:25.380
fairly good sized cluster with just
a single large switch and it also

00:14:25.380 --> 00:14:27.930
scales up to thousands of nodes.

00:14:27.930 --> 00:14:31.580
And the connection on
this is copper based.

00:14:31.580 --> 00:14:36.290
And for more information on all
of these and the websites provide

00:14:36.290 --> 00:14:38.710
a lot of really good resources.

00:14:38.810 --> 00:14:44.290
Miracom, both their Miranet 2000 and
Miranet 10G solutions you can get

00:14:44.290 --> 00:14:45.660
information on the miracom.com.

00:14:45.660 --> 00:14:51.140
Miracom.com website and InfiniBand
from either SmallTree Communications or

00:14:51.140 --> 00:14:53.420
SilverStorm Technologies.

00:14:53.420 --> 00:14:58.920
You can check out their websites
for Mac OS X based solutions.

00:15:00.400 --> 00:16:47.400
[Transcript missing]

00:16:47.830 --> 00:16:50.630
So the first one from
Argonne National Labs,

00:16:50.630 --> 00:16:54.580
MPitch 1.2.7 is a current
iteration of this from Argonne.

00:16:54.580 --> 00:17:00.260
MPitch 1 is one of the first
MPI middlewares that came out and

00:17:00.260 --> 00:17:02.810
there have been spin-outs of that.

00:17:02.830 --> 00:17:08.020
There are mirror net enabled MPI stacks,
MPitch GM and MPitch MX.

00:17:08.020 --> 00:17:11.700
There's InfiniBand enabled
versions known as MPapitch.

00:17:11.700 --> 00:17:15.940
And one of the newest versions
from Argonne is MPitch 2.1,

00:17:15.940 --> 00:17:17.700
it's the latest from them.

00:17:17.700 --> 00:17:22.700
In addition there are other ones, LAMMPI.

00:17:22.700 --> 00:17:28.780
LAMMPI is from Indiana University among
a couple of other labs I believe.

00:17:28.780 --> 00:17:33.680
And it provides very good Ethernet
performance as well as mirror net and

00:17:33.680 --> 00:17:38.770
InfiniBand support out of the box.

00:17:39.090 --> 00:17:42.380
Of note the LAMMPI is
probably the most widely used

00:17:42.380 --> 00:17:46.820
MPI stack on Mac OS X clusters
based on its performance.

00:17:46.820 --> 00:17:50.430
But one of the newest more
interesting ones is OpenMPI.

00:17:50.440 --> 00:17:56.210
It's a joint venture by several of
the national labs and universities

00:17:56.210 --> 00:17:58.000
and international labs as well.

00:17:58.040 --> 00:18:05.690
It is Xgrid enabled.

00:18:05.690 --> 00:18:06.000
You'll see a demonstration on that
from Josh when he comes up in a bit.

00:18:06.150 --> 00:18:11.280
It includes not only Ethernet
support but also native mirror

00:18:11.280 --> 00:18:13.750
net and InfiniBand support.

00:18:14.990 --> 00:18:19.880
So with all of those MPI middlewares
software stacks out there one

00:18:19.880 --> 00:18:22.790
of the things we wanted to
look at was well how do these

00:18:22.790 --> 00:18:25.900
MPI middlewares perform on Mac OS X.

00:18:25.900 --> 00:18:30.690
So one of the things that we
did was we decided to run the

00:18:30.690 --> 00:18:34.900
MPI Ping Pong Benchmark on Mac OS X.

00:18:34.900 --> 00:18:38.900
The idea here is to measure the
MPI software and fabric performance.

00:18:38.900 --> 00:18:44.900
The software itself is designed not
to run on two cores of the same node

00:18:44.900 --> 00:18:47.900
but to run on across the fabric.

00:18:47.920 --> 00:18:52.900
So from one core to another core
sitting on another compute node.

00:18:52.900 --> 00:18:59.170
The message or the ping is sent from
the client to the server process and

00:18:59.190 --> 00:19:04.900
the server process bounces back the
message to the client or the Pong.

00:19:04.900 --> 00:19:07.620
And that performance time,
the time that it takes to make that

00:19:07.620 --> 00:19:08.900
trip is measured from the Pong.

00:19:08.980 --> 00:19:10.900
So that's what we did for
our performance benchmark.

00:19:10.900 --> 00:19:14.180
In addition you can actually
vary the size of the message or

00:19:14.180 --> 00:19:17.900
the ping that you send back and
forth to measure the performance.

00:19:18.080 --> 00:19:24.820
Beyond just testing with the
MPI Ping Pong we also decided to see what

00:19:24.820 --> 00:19:28.870
that meant for real-world applications.

00:19:28.960 --> 00:19:32.910
So we tested the Ping Pong Benchmark
and we also chose a couple of different

00:19:32.910 --> 00:19:37.900
real-world software applications
that our customers often run.

00:19:37.920 --> 00:19:38.890
So let's take a look at some
of the performance benchmarks.

00:19:38.920 --> 00:19:42.160
So as I mentioned there are
various message sizes that

00:19:42.250 --> 00:19:43.900
you can pass back and forth.

00:19:43.900 --> 00:19:47.900
So here on this chart we have
one byte up to 16 kilobytes.

00:19:47.900 --> 00:19:51.880
Latency is measured in
microseconds and shorter is better.

00:19:51.960 --> 00:19:57.350
One thing that I did do on this
particular slide is put up a gold

00:19:57.350 --> 00:20:00.900
standard benchmark just as a baseline.

00:20:00.900 --> 00:20:05.240
So we chose one of the high
performance interconnects and showed

00:20:05.240 --> 00:20:07.900
the baseline on the performance
of that with the different messages.

00:20:07.900 --> 00:20:15.360
So as we compare that to MPitch
1 over gigabit ethernet you

00:20:15.360 --> 00:20:20.900
can see that it does scale.

00:20:21.400 --> 00:20:25.320
Similarly to the mirror net but the
latency is about 10 times worse or

00:20:25.320 --> 00:20:28.400
an order of magnitude worse than
the high performance interconnect.

00:20:28.400 --> 00:20:32.960
But as we switch over to some of the
newer MPI layers we can look at the

00:20:32.960 --> 00:20:34.400
performance and what happens there.

00:20:34.400 --> 00:20:40.280
So as you see here it's interesting
with the MPitch 2 there's a drastic

00:20:40.280 --> 00:20:42.400
improvement over the original MPitch 1.

00:20:42.400 --> 00:20:47.570
The latency is almost
significantly better on MPitch

00:20:47.580 --> 00:20:50.400
2 or the newer MPI software.

00:20:50.420 --> 00:20:53.460
and then LAMMPI.

00:20:53.780 --> 00:20:57.300
Better still yet, not significantly,
but it is still a little bit

00:20:57.300 --> 00:20:59.640
faster with the message sizes.

00:20:59.640 --> 00:21:04.150
So this showed us that any
of the newer stacks seemed to

00:21:04.270 --> 00:21:06.010
look better on performance.

00:21:06.010 --> 00:21:08.150
So how did this affect
real-world applications?

00:21:08.200 --> 00:21:12.910
So one of the applications
we chose to run was GrowMax.

00:21:12.950 --> 00:21:17.890
It is a C-based molecular dynamics code.

00:21:19.230 --> 00:21:25.580
And in this case we ran a specific number
of processors on XServe G5s and measured

00:21:25.580 --> 00:21:28.170
the time for the benchmark performance.

00:21:28.170 --> 00:21:31.100
So how long did it take to
run a particular simulation.

00:21:31.100 --> 00:21:35.710
In this case you can see we
also ran the high performance

00:21:35.710 --> 00:21:42.100
interconnect as a baseline to just
study and look at the scaling.

00:21:42.190 --> 00:21:46.020
And then we ran MPitch2,
so one of the newer MPI software

00:21:46.020 --> 00:21:48.300
stacks over Gigabit Ethernet.

00:21:48.300 --> 00:21:51.530
And it scales fairly similarly
with the MirrorNet just

00:21:51.530 --> 00:21:53.100
edging it out a little bit.

00:21:53.100 --> 00:21:56.100
But Gigabit Ethernet's
pretty good performance.

00:21:56.100 --> 00:22:00.380
And of note you can see that this
particular application scales very

00:22:00.380 --> 00:22:05.080
well up to about 24 nodes and it
starts to tend to taper off so that

00:22:05.090 --> 00:22:11.100
you might be looking at a plateau in
terms of how many nodes to run per job.

00:22:11.100 --> 00:22:15.380
Now let's compare that to MPitch1.

00:22:15.380 --> 00:22:20.120
So as you recall the MPitch1 on
the Ping Pong showed very poor

00:22:20.210 --> 00:22:23.100
results as message sizes got larger.

00:22:23.220 --> 00:22:27.100
And this shows very odd results as well.

00:22:27.100 --> 00:22:32.310
It clearly shows MPitch1 results as
not something you want to choose for

00:22:32.390 --> 00:22:35.100
running this particular application.

00:22:35.100 --> 00:22:36.650
So it gets very slow.

00:22:36.650 --> 00:22:40.100
It doesn't scale even after
the first eight nodes.

00:22:40.100 --> 00:22:45.100
So just to be sure we decided
to run another application.

00:22:45.100 --> 00:22:50.100
This time a Fortran code weather
forecasting simulation benchmark.

00:22:50.100 --> 00:22:53.080
And again number of
CPUs and execution time.

00:22:53.120 --> 00:22:57.170
So running the simulation,
the baseline high performance

00:22:57.170 --> 00:22:59.100
interconnect benchmark.

00:22:59.100 --> 00:23:02.100
MPitch2.

00:23:02.100 --> 00:23:05.100
So very good scaling on
both of these fabrics.

00:23:05.100 --> 00:23:08.990
Again the high performance
interconnect edging out the GIF.

00:23:09.180 --> 00:23:19.100
And now if we look at the MPitch1
performance with the same gigabit fabric.

00:23:19.100 --> 00:23:26.100
You can see here there are no results
shown for the 8 and 14 CPU benchmarks.

00:23:26.100 --> 00:23:30.210
And the reason for that is when
we ran it the scaling would have

00:23:30.210 --> 00:23:34.100
pretty much flattened out all
of the other graphs out there.

00:23:34.100 --> 00:23:38.100
So we didn't even decide not to even show
those performance benchmarks on this.

00:23:38.100 --> 00:23:40.920
on this slide.

00:23:42.090 --> 00:23:43.710
So what's that mean?

00:23:43.870 --> 00:23:49.740
Basically what we learned from this is
as users of high performace computers you

00:23:49.740 --> 00:23:52.000
need to choose your MPI software wisely.

00:23:52.000 --> 00:23:56.720
It's not just about hardware selection
and switching fabrics but you need

00:23:56.720 --> 00:23:59.000
to choose your MPI software wisely.

00:23:59.000 --> 00:24:02.950
What that really means
is don't use MPitch 1.

00:24:02.990 --> 00:24:07.960
The exceptions are the specific
fabric designed MPI's MPitch for

00:24:08.220 --> 00:24:11.000
MirrorNet or MPitch for InfiniBand.

00:24:11.000 --> 00:24:17.000
If you are going to use MPitch strongly
are suggesting that you use MPitch 2.

00:24:17.000 --> 00:24:19.000
The performance is much better.

00:24:19.000 --> 00:24:22.000
LAMMPI provides excellent performance.

00:24:22.000 --> 00:24:27.320
It's a little bit better even than MPitch
2 and again it's compatible with all

00:24:27.320 --> 00:24:30.000
the different communications fabrics.

00:24:30.130 --> 00:24:34.990
And the latest version OpenMPI and
at the end of the session we'll

00:24:34.990 --> 00:24:40.000
have Brian Barrett from the
OpenMPI group come up for Q&A as well.

00:24:40.000 --> 00:24:42.990
But excellent alternative to
any of the other MPI softwares.

00:24:43.000 --> 00:24:47.150
It runs on Gigabit Ethernet
and also the other fabrics as

00:24:47.160 --> 00:24:49.000
I mentioned InfiniBand and MirrorNet.

00:24:49.000 --> 00:24:54.440
And what's unique about this is that it
automatically selects the best fabric

00:24:54.580 --> 00:24:57.000
to run your application at run time.

00:24:57.000 --> 00:25:02.040
So if you have MirrorNet or InfiniBand
enabled it will know that and it

00:25:02.040 --> 00:25:05.000
will select that fabric at run time.

00:25:05.000 --> 00:25:09.000
And as a bonus it can integrate
with XGrid as a basic job scheduler.

00:25:09.000 --> 00:25:13.830
job scheduler,
making setting up a cluster much simpler.

00:25:15.030 --> 00:25:18.660
Of note,
I wanted to put this little additional

00:25:18.660 --> 00:25:24.590
information here integrating Mac OS X G5
based machines and Intel based machines,

00:25:24.590 --> 00:25:26.580
so heterogeneous clusters.

00:25:26.580 --> 00:25:33.230
So again, MPitch, it cannot be used on a
heterogeneous G5 and Intel cluster.

00:25:33.230 --> 00:25:39.900
Whereas, LAMMPI and OpenMPI can be used
in a mixed cluster environment.

00:25:40.030 --> 00:25:45.220
The reason is both LAMMPI and
OpenMPI handle the byte order swapping,

00:25:45.220 --> 00:25:49.720
the big Indian, little Indian issues
of the architectures,

00:25:49.720 --> 00:25:51.340
whereas MPitch does not.

00:25:51.440 --> 00:25:58.460
So the idea to run this mixed environment
is you'll want to compile the MPI layer

00:25:58.760 --> 00:26:01.780
for both PowerPC and for Intel.

00:26:01.840 --> 00:26:06.910
And you can lipo and build a
universal from that if you wish

00:26:06.910 --> 00:26:09.900
and install it on all of the nodes.

00:26:09.900 --> 00:26:14.370
And then from there,
make sure that you compile compatible

00:26:14.370 --> 00:26:19.210
binaries that are accessible for
each of the architectures and

00:26:19.210 --> 00:26:21.900
then just do the MPI run as usual.

00:26:22.410 --> 00:26:26.910
So one other thing that I decided
to put up was since I was showing

00:26:26.910 --> 00:26:31.440
a lot of performance differences
between the various MPI layers,

00:26:31.440 --> 00:26:36.120
I also decided to just show a couple of
slides on gigabit performance versus one

00:26:36.120 --> 00:26:39.300
of the high performance interconnects,
in this case, MirrorNet.

00:26:39.300 --> 00:26:43.270
As you recall,
we ran the Ping Pong benchmark

00:26:43.330 --> 00:26:47.300
and this time I'm just going to
show you the gigabit results,

00:26:47.300 --> 00:26:51.300
again, various message sizes and
latency in microseconds.

00:26:51.300 --> 00:26:54.680
And the high performance
MirrorNet interconnect,

00:26:54.700 --> 00:26:59.300
about an order of magnitude better
for these particular message sizes.

00:26:59.300 --> 00:27:03.980
And then we decided to look at it again,
what's it look like on the

00:27:03.980 --> 00:27:06.290
real-world applications.

00:27:06.300 --> 00:27:09.510
So again,
the weather forecasting model will

00:27:09.510 --> 00:27:15.300
show the gigabit results and follow
that up by the MirrorNet results.

00:27:15.460 --> 00:27:20.300
In this case, from 8 to 28 CPUs,
they both scale fairly linearly.

00:27:20.300 --> 00:27:23.240
So this shows that the high
performance interconnect,

00:27:23.240 --> 00:27:27.400
although faster, you might not be able to
justify purchasing a high

00:27:27.500 --> 00:27:29.300
performance interconnect.

00:27:29.360 --> 00:27:32.430
It looks like here you'd
be better off purchasing,

00:27:32.430 --> 00:27:36.300
possibly purchasing more XSERVs,
which would make us happy.

00:27:36.300 --> 00:27:40.040
And that way,
it is a large investment to spend

00:27:40.040 --> 00:27:43.260
on a high performance interconnect.

00:27:43.300 --> 00:27:47.300
So what's it look like when
maybe you actually do need it?

00:27:47.300 --> 00:27:49.300
So VASP is another example.

00:27:49.300 --> 00:27:53.650
This is another molecular dynamics,
it's a quantum mechanical molecular

00:27:53.650 --> 00:27:58.300
dynamics chemistry software package,
a large Fortran code.

00:27:58.300 --> 00:28:01.300
And we look at the gigabit Ethernet.

00:28:01.300 --> 00:28:05.300
You can see it scales between 6, 8,
and 16 CPUs fairly well,

00:28:05.300 --> 00:28:09.180
but there's almost no scaling when
you double the number of CPUs.

00:28:09.330 --> 00:28:12.500
And this is with gigabit Ethernet.

00:28:12.740 --> 00:28:16.210
But here you can see the
mirror net performance scales

00:28:16.280 --> 00:28:18.280
almost perfectly linearly.

00:28:18.400 --> 00:28:21.440
So doubling the number of nodes
you get double the performance.

00:28:21.440 --> 00:28:25.350
So in this case it does make a
lot of sense to look into adding a

00:28:25.350 --> 00:28:27.600
high performance computing fabric.

00:28:27.600 --> 00:28:29.600
So what's that mean?

00:28:29.600 --> 00:28:32.570
So in summary the high performance
interconnects are usually faster

00:28:32.570 --> 00:28:37.210
than gigabit ethernet and when
applications can benefit the performance

00:28:37.320 --> 00:28:39.600
gains are usually significant.

00:28:39.600 --> 00:28:43.490
So in the case of VASP it was clearly
a significant win to implement

00:28:43.490 --> 00:28:46.600
a high performance interconnect.

00:28:46.600 --> 00:28:50.600
Though not all applications can justify
the higher performance interconnects.

00:28:50.600 --> 00:28:55.210
The first slide with the Wharf
application showed that you're probably

00:28:55.210 --> 00:28:57.760
better off just adding more servers.

00:28:57.760 --> 00:29:00.600
And then there's other strategies
that you can look to evaluate

00:29:00.600 --> 00:29:03.600
whether it's worthwhile to
add the interconnect or not.

00:29:03.600 --> 00:29:06.790
But one of the important things
is performing your own tests

00:29:06.790 --> 00:29:08.600
with the various interconnects.

00:29:08.600 --> 00:29:14.580
That is very valuable in determining
the performance of your applications.

00:29:14.600 --> 00:29:21.600
So that's the summary I have on
interconnects and software middleware.

00:29:21.600 --> 00:29:26.600
And now I'd like to introduce
the Josh on job scheduling.

00:29:26.600 --> 00:29:28.600
Thank you Warner.

00:29:31.220 --> 00:29:34.290
So we're going to overview job
schedules a little bit and then go

00:29:34.290 --> 00:29:38.600
into some more specific details about
the job schedules that are available

00:29:38.600 --> 00:29:41.200
and we'll also do a little Xgrid demo.

00:29:41.200 --> 00:29:45.200
So what do job schedulers do?

00:29:45.200 --> 00:29:48.390
They basically work with the resource
manager to maximize resource usage.

00:29:48.390 --> 00:29:52.190
So if you have a bunch of computers
you want to make sure you effectively

00:29:52.200 --> 00:29:56.200
use all of them and none of them are
wasted by just sitting there idle.

00:29:56.450 --> 00:29:59.630
They help alleviate the political
issues that are sometimes come

00:29:59.630 --> 00:30:01.150
up when you have a cluster.

00:30:01.230 --> 00:30:05.380
So let's say you have two teams
both want to run on the cluster.

00:30:05.380 --> 00:30:08.190
Well obviously they probably
want to be able to use the

00:30:08.200 --> 00:30:09.200
cluster as much as they can.

00:30:09.200 --> 00:30:13.100
So this kind of balances
out between the two.

00:30:13.100 --> 00:30:16.290
Most resource managers come
with a basic job scheduler.

00:30:16.470 --> 00:30:20.800
So most of the times it's
either like round robin or first

00:30:20.880 --> 00:30:23.200
in first out sort of thing.

00:30:23.200 --> 00:30:25.200
If you have more complex requirements
or more complex requirements

00:30:25.200 --> 00:30:25.200
you can use the job scheduler.

00:30:25.200 --> 00:30:25.820
So if you have more complex requirements
or more complex requirements

00:30:25.840 --> 00:30:26.200
you can use the job scheduler.

00:30:26.200 --> 00:30:30.360
more requirements than just that.

00:30:30.360 --> 00:30:31.950
There are more complex scheduling.

00:30:32.080 --> 00:30:35.630
An example of one of those
complex schedulers is called Mali.

00:30:35.630 --> 00:30:37.340
It is free, which is nice.

00:30:37.340 --> 00:30:41.150
A lot of people like that,
but it is also very complicated.

00:30:41.150 --> 00:30:44.030
Lots of different things
you can do with it.

00:30:44.100 --> 00:30:47.390
You can schedule policies,
do different priorities,

00:30:47.390 --> 00:30:49.310
reservations and fair share.

00:30:49.310 --> 00:30:51.840
So, for example,
if you have a bioinformatics

00:30:51.840 --> 00:30:55.110
cluster that's also letting the
physics people get on the computer,

00:30:55.110 --> 00:30:57.940
you want to make sure the
bioinformatics people get a higher

00:30:57.940 --> 00:31:00.180
priority when they want to run.

00:31:00.180 --> 00:31:03.940
So, something like this will help you
kind of balance out things like that.

00:31:05.830 --> 00:31:09.030
Resource management is basically
the thing that tracks what

00:31:09.030 --> 00:31:15.510
is happening on each node,
makes sure that system loads are correct.

00:31:15.510 --> 00:31:19.230
And they are also making sure that
jobs don't exceed their allocated time.

00:31:19.250 --> 00:31:22.310
So if someone says, you know,
submits a job for one hour and it's

00:31:22.310 --> 00:31:25.640
still running after that one hour,
it's responsible for killing it

00:31:25.800 --> 00:31:31.680
and then allocating the resources
appropriately after that.

00:31:33.000 --> 00:31:36.430
So some available resource managers and
this list is nowhere near all inclusive.

00:31:36.440 --> 00:31:39.520
There's definitely a lot more
than this and I probably missed

00:31:39.520 --> 00:31:41.680
someone's favorite resource manager.

00:31:41.690 --> 00:31:47.240
But obviously Xgrid
is available in Tiger.

00:31:47.260 --> 00:31:50.700
It allows basically distributed
processing of what we call

00:31:50.700 --> 00:31:53.680
grid jobs which are a little
bit loosely interconnected

00:31:53.680 --> 00:31:58.450
jobs to make work go faster.

00:31:58.460 --> 00:32:04.190
I don't know if anyone saw the
podcasting utility that we're doing but

00:32:04.190 --> 00:32:10.880
obviously Xgrid is a big part of that
because now you can use Xgrid to do the

00:32:10.950 --> 00:32:15.340
podcasting formation and moving it along.

00:32:15.340 --> 00:32:18.940
So Tiger Client comes with the Xgrid.

00:32:18.940 --> 00:32:22.310
You do need Tiger Server though to
do the controller which basically

00:32:22.370 --> 00:32:27.070
brings all those clients together and
lets you allocate resources on them.

00:32:27.340 --> 00:32:30.140
There's Grid Engine as well.

00:32:30.140 --> 00:32:33.580
They do provide OS X binders
which is really nice.

00:32:33.580 --> 00:32:36.800
It is free and open source
so available to anyone.

00:32:36.800 --> 00:32:40.670
It is distributed by Sun and
Sun does have I think a commercial

00:32:40.680 --> 00:32:44.340
product that Space Health is
so if you need it actually support

00:32:44.460 --> 00:32:46.040
you can get that through Sun.

00:32:46.040 --> 00:32:49.460
And the great thing about Grid Engine is
it's actively being developed.

00:32:49.570 --> 00:32:55.500
It's constantly being updated
and that's really important.

00:32:55.550 --> 00:32:58.040
Another one is Torque
which is very popular.

00:32:58.040 --> 00:33:03.190
It's actually based off of PBS and
Torque is basically the free

00:33:03.190 --> 00:33:05.900
version once it branched off.

00:33:05.940 --> 00:33:11.480
And another one is Platform LSF which
is a commercial resource manager.

00:33:11.540 --> 00:33:14.240
So we're going to just
focus on a couple of these,

00:33:14.240 --> 00:33:16.000
Xgrid and Grid Engine.

00:33:16.000 --> 00:33:17.310
- Yeah.

00:33:18.770 --> 00:33:21.760
So there's different kinds of
resource management and you

00:33:21.760 --> 00:33:24.580
always you don't want to you know

00:33:24.990 --> 00:33:29.460
Deploy the one that is going to
be a lot more than what you need.

00:33:29.460 --> 00:33:32.720
So the most basic form of
resource management is what we

00:33:32.720 --> 00:33:34.580
call social resource management.

00:33:34.690 --> 00:33:38.680
Basically you usually have a couple guys,
maybe a few people that can handle

00:33:38.680 --> 00:33:42.900
it among themselves who's going to
run when and what just by talking.

00:33:42.900 --> 00:33:47.900
So this guy will have nodes 1 through 8,
this guy will have nodes 9 through 16 and

00:33:47.900 --> 00:33:51.070
they can usually cooperate pretty nicely.

00:33:51.260 --> 00:33:55.140
On top of that we have basic
resource management with round

00:33:55.140 --> 00:33:58.200
robin allocation so that the first
job will get the first few nodes,

00:33:58.200 --> 00:34:01.200
the next job will get the next few,
so on and so forth.

00:34:01.200 --> 00:34:03.990
And Xgrid is an example of that.

00:34:04.130 --> 00:34:07.440
So if you need to step up
and start actually looking at

00:34:07.450 --> 00:34:10.810
what are each system is doing,
then we can do resource management

00:34:11.220 --> 00:34:13.020
with load balancing as well.

00:34:13.020 --> 00:34:17.150
So Grid Engine out of
the box can do this.

00:34:17.500 --> 00:34:19.430
Now if you need something a
little bit more complex like that,

00:34:19.520 --> 00:34:21.280
like what we were talking
about before with reservations,

00:34:21.340 --> 00:34:25.070
fair share, dynamic priorities,
then you can basically take MAUI,

00:34:25.130 --> 00:34:27.740
the scheduler,
and plug that into something

00:34:27.740 --> 00:34:32.950
like Grid Engine and be able to
start doing those sort of things.

00:34:34.200 --> 00:34:36.040
So Xgrid.

00:34:36.040 --> 00:34:39.430
Xgrid generally is used for serial
applications where you can take

00:34:39.460 --> 00:34:45.010
a task and chunk it out into very
loose integrated things and deploy

00:34:45.010 --> 00:34:49.200
it across a bunch of systems.

00:34:49.240 --> 00:34:52.630
So when Xgrid first came out I didn't
like it because you had to use the

00:34:52.630 --> 00:34:57.590
GUI and I was always a command line
sort of person and there's actually a

00:34:57.590 --> 00:35:03.200
lot of really good command line support
in Xgrid that I didn't know about.

00:35:03.200 --> 00:35:07.190
So for example all you have to do is
set a couple of environment variables

00:35:07.190 --> 00:35:13.850
to tell the system where you want to
run on and then you can launch any

00:35:13.940 --> 00:35:16.200
job with using the Xgrid command line.

00:35:16.200 --> 00:35:20.250
And you can also use this command line
to retrieve the results and delete

00:35:20.260 --> 00:35:22.390
the results once the job is done.

00:35:23.230 --> 00:35:25.320
And so basically again
what we said before,

00:35:25.320 --> 00:35:27.470
Xgrid is round robin.

00:35:27.470 --> 00:35:31.100
You do have your,
in this case the Xgrid clients are

00:35:31.100 --> 00:35:36.600
on the right there and you can see
that they're working on something.

00:35:37.580 --> 00:35:41.260
Now you can actually
integrate Xgrid with MPI.

00:35:41.260 --> 00:35:46.890
The OpenMPI team has actually done
a lot of really neat work making

00:35:46.890 --> 00:35:48.500
this as seamless as possible.

00:35:48.500 --> 00:35:53.690
If anyone's ever done any sort of
MPI integration with a resource

00:35:53.720 --> 00:35:56.890
manager it's kind of a little bit
difficult because you have to make

00:35:56.890 --> 00:35:59.500
sure that the MPI knows which nodes
it can run on which is dynamic.

00:35:59.500 --> 00:36:03.390
You know you don't know beforehand
which nodes you're going to get but

00:36:03.400 --> 00:36:07.500
they've done a lot of work integrating
with Xgrid to get that to work.

00:36:07.500 --> 00:36:12.970
Same thing as before,
same the two environment variables

00:36:12.970 --> 00:36:16.190
and then you can use OpenMPI to run.

00:36:16.500 --> 00:36:19.500
There are a couple of requirements
that get that working.

00:36:19.630 --> 00:36:22.500
OpenMPI has to be installed
on every single node.

00:36:22.500 --> 00:36:25.640
You have to have a shared
workspace where the user nobody

00:36:25.670 --> 00:36:27.500
can read and write permissions.

00:36:27.500 --> 00:36:29.500
So right now Xgrid is a little bit
more complicated than you might think.

00:36:29.500 --> 00:36:32.160
It doesn't run as the current
user who submitted it,

00:36:32.160 --> 00:36:33.500
it runs as the user nobody.

00:36:33.500 --> 00:36:38.500
So you have to make sure that user can
read and write to that shared workspace.

00:36:38.500 --> 00:36:43.500
Set the path so that MPI is
in your path on each system.

00:36:43.500 --> 00:36:47.500
And then you submit an
Xgrid MPI job using MPIRUN.

00:36:47.560 --> 00:36:50.960
And we're going to try to demo this.

00:36:54.820 --> 00:36:58.140
Great, so did a little bit of
cook and show magic here.

00:36:58.180 --> 00:37:01.900
We've got Xgrid Admin already running,
deployed.

00:37:01.900 --> 00:37:03.100
We have a cluster actually.

00:37:03.100 --> 00:37:05.170
This is a cluster over
at the Apple campus.

00:37:05.200 --> 00:37:09.680
It's 16 nodes but we've got
four of them on the Xgrid.

00:37:09.700 --> 00:37:12.240
So this isn't the cluster
here in the front.

00:37:12.240 --> 00:37:14.700
It's actually over at the Apple campus.

00:37:14.700 --> 00:37:19.630
So if you haven't seen Xgrid before
we've got the nice little speedometer

00:37:19.710 --> 00:37:20.700
that tells you how much work it's doing.

00:37:20.710 --> 00:37:22.370
You can see the agents here.

00:37:22.370 --> 00:37:25.700
You can see the different
nodes for example that we have.

00:37:25.700 --> 00:37:26.630
So we have four nodes.

00:37:26.690 --> 00:37:28.700
Yellow isn't bad in this case.

00:37:28.700 --> 00:37:34.180
It just means it's not doing
anything at the moment.

00:37:34.280 --> 00:37:37.860
So what we're going to do is we are
going to use the command line just

00:37:37.860 --> 00:37:41.800
to prove that we can actually use the
command line to submit jobs and we're

00:37:41.800 --> 00:37:44.960
going to do an open MPI job GROMACS.

00:37:45.100 --> 00:37:49.750
So like we said before we have
to set the environment variables.

00:37:50.320 --> 00:37:55.760
And these variables aren't that short
and they're pretty easy to mistype.

00:37:55.900 --> 00:37:59.060
Let's see if I get this right.

00:38:04.120 --> 00:38:09.130
You can put these in your startup
script or something like that so you

00:38:09.130 --> 00:38:12.160
don't have to type this in every time.

00:38:13.740 --> 00:38:17.030
So there you can see the two
environment variables are now set.

00:38:17.060 --> 00:38:22.400
So we have the application GROMACS and
again this is the cooking show magic.

00:38:22.410 --> 00:38:25.380
We already have the NFS deployed,
the shared space,

00:38:25.380 --> 00:38:27.200
the setup across all the nodes.

00:38:27.200 --> 00:38:34.900
And from here it's simply just using the
OpenMPIRUN command to run the GROMACS.

00:38:34.910 --> 00:38:38.850
So we'll do the nice little
gauge here so you can see that.

00:38:51.050 --> 00:38:55.250
So once we run it we should see in
the Xgrid admin the jobs actually

00:38:55.390 --> 00:38:57.220
start queuing up and running there.

00:38:57.220 --> 00:39:01.000
So then I can see the little gauge
is kind of cranking up there.

00:39:01.090 --> 00:39:03.700
And we can go to the agents and
now instead of yellow you can

00:39:03.700 --> 00:39:05.400
see all the agents are now green.

00:39:05.400 --> 00:39:08.300
You can see they all think
they're working on something.

00:39:08.350 --> 00:39:12.820
And you can see now here's the
job which wasn't there before.

00:39:13.050 --> 00:39:15.240
So this demo takes a
little bit of time to run.

00:39:15.240 --> 00:39:20.310
You can actually see now it's getting
the nodes and actually starting to run.

00:39:20.310 --> 00:39:22.290
And this is GROMACS.

00:39:22.430 --> 00:39:27.990
It does take a while to run so I'm just
going to go and kill it with Control C.

00:39:28.020 --> 00:39:30.940
And you can see job disappeared.

00:39:31.000 --> 00:39:32.930
Agents think they're available again.

00:39:33.070 --> 00:39:35.340
And that's it.

00:39:35.400 --> 00:39:39.430
So that was Xgrid from the
command line using OpenMPI.

00:39:42.940 --> 00:39:48.940
Again, another resource manager that we
like to talk about is Grid Engine.

00:39:48.940 --> 00:39:51.900
It's actually deployed on a lot
of different systems because,

00:39:51.900 --> 00:39:56.900
for example, BioTeams Inquiry uses
Grid Engine by default.

00:39:56.900 --> 00:40:00.900
So if you ever installed BioTeams
Inquiry you kind of get this for free.

00:40:00.900 --> 00:40:04.150
So the pros on it,
it supports a lot of MPI libraries.

00:40:04.160 --> 00:40:07.830
All the ones that Warner went
over I definitely know that they

00:40:07.830 --> 00:40:09.890
support and you can use those.

00:40:09.900 --> 00:40:13.700
It runs on a wide variety of platforms
and these are the officially supported

00:40:13.800 --> 00:40:17.470
ones that you can download binaries for
and there's a lot more beyond that that

00:40:17.470 --> 00:40:18.900
are probably not officially supported.

00:40:18.900 --> 00:40:23.240
But you can definitely download binaries
for OS X and Linux and Solaris and

00:40:23.240 --> 00:40:27.840
probably a lot of other Unix operating
systems that you may have to run into.

00:40:27.900 --> 00:40:29.900
And like I said before,
active development community.

00:40:29.900 --> 00:40:36.900
It's up to version 6 now and
it's a company sponsored project.

00:40:36.900 --> 00:40:39.830
It's a company sponsored project.

00:40:39.940 --> 00:40:42.880
But definitely a lot of
community development on this.

00:40:42.900 --> 00:40:44.900
The con is it's not that easy to set up.

00:40:44.900 --> 00:40:47.900
You know, if you ever set up an
X Grid controller it's pretty easy.

00:40:47.900 --> 00:40:49.970
You just go out and say, "Hey,
I want to grab these

00:40:50.020 --> 00:40:52.900
X Grid clients." And go from there.

00:40:52.900 --> 00:40:54.900
With this you have to install
software on each system.

00:40:54.900 --> 00:40:58.900
You have to make sure that certain
things are running when you start it up.

00:40:58.900 --> 00:41:01.900
And, you know, a little bit more work
required to do that.

00:41:01.900 --> 00:41:04.900
And it's all command line based,
of course.

00:41:04.900 --> 00:41:06.900
It doesn't require direct
job submission of a binary.

00:41:06.900 --> 00:41:08.890
And I'll kind of show you on
the next slide what that is.

00:41:08.900 --> 00:41:11.900
On the next slide what's required
to get something running.

00:41:11.900 --> 00:41:15.820
And generally it's been my experience
that it requires more maintenance

00:41:15.930 --> 00:41:17.900
than something like X Grid.

00:41:17.910 --> 00:41:19.870
But, you know,
it's because it's more complex.

00:41:19.910 --> 00:41:22.770
But, you know,
you have to make sure that the

00:41:22.770 --> 00:41:24.900
daemon is running on each system.

00:41:24.900 --> 00:41:26.900
The grid engine daemon.

00:41:26.900 --> 00:41:29.860
And make sure that the queues
aren't messed up somehow.

00:41:29.910 --> 00:41:31.900
So most of the time it works.

00:41:31.900 --> 00:41:34.810
But, you know, it does require a little
bit more maintenance.

00:41:34.900 --> 00:41:40.350
So here's an example of a...
How to do some... Summary

00:41:40.350 --> 00:41:41.900
of a job with grid engine.

00:41:41.900 --> 00:41:43.900
I'm not going to read
out the script there.

00:41:43.900 --> 00:41:45.900
But that's what it looks like.

00:41:46.800 --> 00:41:49.270
So basically you have to create
a script that we call a wrapper

00:41:49.270 --> 00:41:51.270
script that will submit your job.

00:41:51.490 --> 00:41:56.860
This is just setting up some environment
variables for Grid Engine and then

00:41:56.860 --> 00:42:01.700
basically running it there at the end
using that user local mpish126 line.

00:42:01.700 --> 00:42:05.080
So again it's what we were talking
about before though is that this script,

00:42:05.180 --> 00:42:08.120
you know,
when you submit it you're not going

00:42:08.120 --> 00:42:11.760
to know what machines you're going
to run on so that's why you see

00:42:11.760 --> 00:42:13.700
variables there like tempdura machines.

00:42:13.720 --> 00:42:18.090
That way when the job gets run it
gets handed by Grid Engine which

00:42:18.090 --> 00:42:19.590
nodes it can run on.

00:42:19.700 --> 00:42:23.700
And then to run the command it basically
gets qsub and then the script name

00:42:23.700 --> 00:42:26.930
and then it gets put in the queue
and then the resource manager and

00:42:26.930 --> 00:42:29.700
the job scheduler handle when to run it.

00:42:29.700 --> 00:42:31.700
And that's it.

00:42:31.700 --> 00:42:34.700
This script did come from
the Bio Team website.

00:42:34.700 --> 00:42:37.700
They do a lot of integration
with Grid Engine and OS X.

00:42:37.720 --> 00:42:39.440
So if you want to look at
that in a little bit more

00:42:39.440 --> 00:42:40.700
detail definitely go there.

00:42:40.700 --> 00:42:43.680
And next is the framework.

00:42:43.700 --> 00:42:46.150
Frighteningly smart, Yousuf.

00:42:52.550 --> 00:42:55.150
Hi, my name is Yusef Abdulghani.

00:42:55.160 --> 00:42:57.200
I'm from the Architecture and
Performance team at Apple.

00:42:57.200 --> 00:43:01.720
As Apple makes the transition
from PowerPC based systems

00:43:01.720 --> 00:43:05.380
to Intel based systems,
we wanted to make sure that we provide

00:43:05.380 --> 00:43:09.190
you the right tools to make that
transition easy and also provide you

00:43:09.190 --> 00:43:13.860
with tools which can actually give you
better performance and you can use the

00:43:13.860 --> 00:43:17.180
underlying architecture to the maximum.

00:43:17.230 --> 00:43:21.530
So with that in mind,
I'm going to talk about a few tools that

00:43:21.610 --> 00:43:23.150
are available on the Apple platform.

00:43:23.150 --> 00:43:25.440
There are several others which
I'm not going to talk about.

00:43:25.440 --> 00:43:31.100
These are the Accelerated Framework,
the Intel C++ and Fortran compilers,

00:43:31.280 --> 00:43:34.000
the Intel's performance
libraries and Shark.

00:43:34.000 --> 00:43:37.260
So let's get started with
the Accelerated Framework.

00:43:37.280 --> 00:43:40.880
The Accelerated Framework is
one-stop shopping for your

00:43:40.880 --> 00:43:42.740
computational performance.

00:43:42.870 --> 00:43:47.480
It comes built in with every copy of
Mac OS X and it is highly optimized

00:43:47.720 --> 00:43:53.050
and in some cases hand-tuned to
run on the underlying architecture.

00:43:53.050 --> 00:43:56.400
It knows what you are running it
on and then selects the highly

00:43:56.400 --> 00:43:59.980
optimized library to actually execute.

00:43:59.980 --> 00:44:02.580
It has got several libraries.

00:44:02.730 --> 00:44:06.350
The VDSP library is a library
which has got routines for

00:44:06.350 --> 00:44:08.130
digital signal processing.

00:44:08.400 --> 00:44:11.640
The image contains libraries
for digital image processing.

00:44:11.950 --> 00:44:14.960
There is Blast Level 1,
2 and 3 and LA Pack routines.

00:44:14.960 --> 00:44:18.670
VForce library,
this is a new library that was introduced

00:44:18.670 --> 00:44:20.480
in Tiger and we also have VMathLiv.

00:44:20.580 --> 00:44:23.190
So how do you use the
Accelerated Framework?

00:44:23.270 --> 00:44:24.540
It's pretty easy.

00:44:24.790 --> 00:44:30.250
In your source code, you just do #include
accelerator/accelerator.h and on

00:44:30.250 --> 00:44:33.870
the command line for the linker,
you provide the library

00:44:34.320 --> 00:44:36.640
using the -wl command.

00:44:36.640 --> 00:44:40.640
So what's new with Accelerated
Framework in Leopard?

00:44:40.640 --> 00:44:45.010
It is a 64-bit,
it's a four-way FAT binary.

00:44:45.170 --> 00:44:48.630
You can create a four-way FAT binary.

00:44:48.630 --> 00:44:54.560
So you can create either 32-bit or 64-bit
applications and your applications can

00:44:54.620 --> 00:44:57.960
either run on PPC or x86 architecture.

00:44:57.960 --> 00:45:00.520
Having said that,
the work on the Accelerated

00:45:00.520 --> 00:45:02.000
Framework is not done yet.

00:45:02.000 --> 00:45:05.430
We are still optimizing it
for the Intel architecture.

00:45:05.430 --> 00:45:08.630
And as we draw closer
to the Leopard release,

00:45:08.640 --> 00:45:11.280
you will see much better
performance improvements in

00:45:11.350 --> 00:45:13.840
the Accelerated Framework.

00:45:14.740 --> 00:45:17.960
The Intel C++ and the
Intel Fortran compilers,

00:45:17.960 --> 00:45:21.700
these are one of the best compilers
on the Intel architectures.

00:45:22.020 --> 00:45:26.830
They've got support for the
Intel C++ compiler has got

00:45:26.830 --> 00:45:29.700
ANSI ISO support or compatibility.

00:45:29.700 --> 00:45:38.700
The Fortran compiler supports Fortran 77,
Fortran 90 and Fortran 95 features.

00:45:38.700 --> 00:45:43.900
And there are some Fortran 2000
and 2003 features which are also

00:45:43.900 --> 00:45:46.700
included in the current version of the
Intel Fortran compiler on Mac OS X.

00:45:46.700 --> 00:45:51.750
The Fortran compiler also has
support for common extensions.

00:45:51.960 --> 00:45:56.460
Here are some of the key performance
features for the Intel compilers.

00:45:56.470 --> 00:45:59.110
Auto vectorization,
this is really important because

00:45:59.110 --> 00:46:03.580
you don't want to spend time hand
coding and making use of the vector

00:46:03.580 --> 00:46:05.960
engines on the Intel processors.

00:46:05.970 --> 00:46:08.270
Auto parallelization,
we'll talk about these

00:46:08.270 --> 00:46:11.360
features in a little bit more
detail in the next few slides.

00:46:11.390 --> 00:46:16.470
It has got full support for OpenMP and
it is capable of doing whole program

00:46:16.470 --> 00:46:20.660
analysis through the inter-procedural,
through the IPO switch.

00:46:20.710 --> 00:46:24.840
It has facility to do profile
guided optimization and it has

00:46:24.840 --> 00:46:29.530
also certain switches to control
the precision of the floating point

00:46:29.530 --> 00:46:32.140
arithmetic on your application.

00:46:32.180 --> 00:46:36.500
As you turn on and go to
higher levels of optimization,

00:46:36.530 --> 00:46:42.880
a lot of compilers tend to go
away from the IEEE 754 compliance

00:46:43.200 --> 00:46:44.740
and give you lower precision.

00:46:44.760 --> 00:46:49.500
But if your application is
dependent on the 754 precision

00:46:49.500 --> 00:46:53.060
model for floating point,
then you can actually control that and

00:46:53.160 --> 00:46:58.540
limit the optimizations which change
the precision of the floating point

00:46:58.540 --> 00:47:00.940
arithmetic using some of these switches.

00:47:00.940 --> 00:47:05.840
Such as strict, precise, fast, or accept.

00:47:06.100 --> 00:47:09.720
So as far as compatibility goes,
the Intel C++ compiler is

00:47:09.720 --> 00:47:13.580
source and binary compatible
with the Apple's GCC and G++

00:47:13.580 --> 00:47:16.000
compiler for most of the cases.

00:47:16.000 --> 00:47:21.340
There's no support for Objective-C,
but it is binary compatible

00:47:21.340 --> 00:47:27.000
with the Objective-C objects.

00:47:27.000 --> 00:47:31.110
If you want to create universal binaries,
the Intel compiler,

00:47:31.110 --> 00:47:34.000
you can do that either
using Xcode or LiPo.

00:47:34.000 --> 00:47:39.140
So you want to create a binary
on x86 using the Intel compiler,

00:47:39.250 --> 00:47:43.950
but for the PowerPC binary you
might want to use the GCC compiler.

00:47:44.700 --> 00:47:48.100
Some note on Xcode integration.

00:47:48.100 --> 00:47:53.000
The Intel C++ compiler fully
integrates with Xcode 2.3 or later

00:47:53.000 --> 00:47:58.900
and it requires Mac OS X 10.4.4.

00:47:58.900 --> 00:48:02.440
The Fortran compiler integration
is a preview release.

00:48:02.440 --> 00:48:07.760
The Intel's compiler team is working
with the Xcode team to make that

00:48:07.760 --> 00:48:12.730
more streamlined and give full
integration for the Fortran compiler.

00:48:13.160 --> 00:48:16.460
There's a 30-day evaluation copy that
you can download from the Intel website,

00:48:16.460 --> 00:48:17.350
give it a shot.

00:48:17.480 --> 00:48:23.700
And there's also some coupons that
Intel is giving out for 50% discount.

00:48:23.700 --> 00:48:27.420
So if you go and want to buy the
compiler you can go downstairs

00:48:27.420 --> 00:48:30.970
to the performance lab,
talk to some of the Intel engineers

00:48:30.970 --> 00:48:33.130
and they can hand you out that coupon.

00:48:33.400 --> 00:48:36.400
So let's talk about some
of the compiler switches.

00:48:36.400 --> 00:48:40.690
What are the optimization switches
that Intel compiler provides?

00:48:40.810 --> 00:48:43.970
At Dash 01,
this switch actually optimizes

00:48:44.060 --> 00:48:45.680
your binary for size.

00:48:45.740 --> 00:48:51.850
So if you have got a lot of applications
which you're going to deploy on a server,

00:48:51.850 --> 00:48:56.690
you might want to use this switch
because it optimizes for the size.

00:48:56.830 --> 00:49:01.300
Dash02, this is the default optimization
level and it optimizes for speed.

00:49:01.440 --> 00:49:05.600
So when you turn on this optimization
the compiler actually looks at

00:49:05.600 --> 00:49:08.390
your loops and tries to convert,
vectorize,

00:49:08.410 --> 00:49:16.150
auto vectorize the loop and use the
SSE or SSE2 or SSE3 instruction set.

00:49:16.740 --> 00:49:20.420
As you turn on the
optimization level to -03,

00:49:20.530 --> 00:49:26.640
this optimization actually optimizes
your code for data cache and it

00:49:26.640 --> 00:49:28.190
does a lot of loop transform.

00:49:28.380 --> 00:49:35.650
One thing to be concerned about at this
level is that you might not get the

00:49:35.650 --> 00:49:39.200
accurate 754 compliant FP arithmetic.

00:49:39.200 --> 00:49:43.830
So you might want to use another
switch to make sure that your

00:49:43.830 --> 00:49:48.590
application performs correctly
at this particular switch.

00:49:48.960 --> 00:49:54.620
Dash IPO, this instantiates the
interprocedural optimization.

00:49:54.620 --> 00:49:58.020
The Intel compiler is capable
of doing whole program analysis.

00:49:58.020 --> 00:50:02.480
One caveat with this particular switch is
that your compile time increases a lot.

00:50:02.480 --> 00:50:07.480
So if you have a big application with
lots of files and lots of objects,

00:50:07.500 --> 00:50:11.660
you will see that the compile time
of your application jumps up a lot.

00:50:11.660 --> 00:50:14.420
But because it looks
at the whole program,

00:50:14.420 --> 00:50:21.360
it can find out much more optimization,
much more things to do,

00:50:21.360 --> 00:50:24.510
and then optimizes your code really well.

00:50:26.360 --> 00:50:31.030
The Profile Guided Optimization, the PGO,
is also done and it's also

00:50:31.060 --> 00:50:32.240
available for you to use.

00:50:32.370 --> 00:50:39.300
You can use it using the
PROFGEN and PROFUSE flags.

00:50:39.350 --> 00:50:43.160
This is very useful for branching code,
but one of the problems with this,

00:50:43.160 --> 00:50:46.840
and this is like a double-edged sword,
is that the output is going

00:50:46.840 --> 00:50:48.300
to be as good as the input.

00:50:48.300 --> 00:50:51.710
So if you're using a training
workload for profile guided

00:50:51.710 --> 00:50:55.300
optimization and that training
workload is not really representative

00:50:55.300 --> 00:50:59.140
of what you're going to run,
then it will give wrong hints to the

00:50:59.140 --> 00:51:03.150
compiler and then the resulting binary
that you generate with the profile

00:51:03.160 --> 00:51:08.300
guided optimization might give you
regressions in terms of performance.

00:51:08.300 --> 00:51:11.300
So you have to use this
optimization carefully.

00:51:11.450 --> 00:51:15.300
But if the training workload is
good and it is representative,

00:51:15.300 --> 00:51:18.300
you can get really good
results for code which are,

00:51:18.300 --> 00:51:18.300
which, which are really good.

00:51:18.300 --> 00:51:20.310
branching codes.

00:51:20.830 --> 00:51:27.560
Dash Parallel is a new feature that they
have introduced in the Intel compilers.

00:51:27.560 --> 00:51:30.950
Most of the products that
Apple ships has at least,

00:51:31.040 --> 00:51:33.950
well, have got multiple cores in them.

00:51:33.950 --> 00:51:36.610
And Dash Parallel actually takes
the onus on the compiler to

00:51:36.610 --> 00:51:38.330
actually parallelize their code.

00:51:38.340 --> 00:51:40.020
You don't have to do
anything inside the code.

00:51:40.020 --> 00:51:42.610
But again,
I've played with it and sometimes it

00:51:42.610 --> 00:51:44.760
works fine and sometimes it doesn't.

00:51:44.760 --> 00:51:47.720
So you have to really test it
out and see how good it is for

00:51:47.720 --> 00:51:49.760
your particular application.

00:51:50.800 --> 00:51:54.460
From HPC's point of view,
because a lot of time people

00:51:54.460 --> 00:51:57.740
use MPI to do parallel stuff,
the Dash Parallel switch

00:51:57.740 --> 00:51:59.000
is not really useful.

00:51:59.090 --> 00:52:03.640
So try it out and see whether it
works for you or not and then gauge

00:52:03.640 --> 00:52:06.360
the performance benefit of that.

00:52:06.600 --> 00:52:10.140
And finally,
the Intel C++ and Fortran compilers,

00:52:10.140 --> 00:52:13.210
they both support the OpenMP standards.

00:52:13.210 --> 00:52:16.700
And if you have got OpenMP pragmas
in your application,

00:52:16.700 --> 00:52:22.110
you can instantiate and make use
of that using the -OpenMP switch.

00:52:22.820 --> 00:52:25.700
So let's talk about the
Intel performance libraries.

00:52:25.870 --> 00:52:29.770
Intel ships two types of
performance libraries on Mac OS X.

00:52:29.770 --> 00:52:32.700
The Intel MKL and the Intel IPP.

00:52:32.700 --> 00:52:35.390
The Intel MKL or the
Math Kernel libraries have

00:52:35.390 --> 00:52:38.730
got routines for scientific,
engineering and financial

00:52:38.800 --> 00:52:45.700
applications such as BLAST 1,
2 and 3 levels, LAPAC, sparse solvers,

00:52:45.700 --> 00:52:49.700
FFTs,
vector math and random number generators.

00:52:49.740 --> 00:52:52.340
On the other hand,
the Intel's IPP library

00:52:52.340 --> 00:52:57.220
contains image processing,
digital signal processing, cryptography,

00:52:57.340 --> 00:53:01.700
video, audio, speech coding and speech
recognition software.

00:53:01.700 --> 00:53:05.870
Both these libraries are highly tuned
and optimized for the underlying

00:53:05.930 --> 00:53:10.700
Intel chipsets and the chips and
they are also multi-core aware.

00:53:10.700 --> 00:53:15.800
So you can control on how many cores
you want these libraries to run and

00:53:15.800 --> 00:53:18.700
they can give you better performance.

00:53:21.800 --> 00:53:27.880
So when we talk about LAPAC and LINPAC,
so LINPAC numbers,

00:53:27.880 --> 00:53:30.020
it's obligatory for us to see.

00:53:30.110 --> 00:53:33.850
So what we have here,
I ran LINPAC numbers on a

00:53:33.920 --> 00:53:37.040
2.16 GHz Core Duo Processor.

00:53:37.080 --> 00:53:43.430
Now this is one of the Apple 17-inch
laptops with 2 GB DDR RAM.

00:53:43.550 --> 00:53:46.700
On the X-axis we have got,
we're rating the matrix size from

00:53:46.700 --> 00:53:50.060
1000 by 1000 to 12,000 by 12,000.

00:53:50.110 --> 00:53:53.900
And on the Y-axis we have
got measurements in M-flops.

00:53:53.990 --> 00:53:59.810
So the first curve shows the results
for Intel's MKL using single precision.

00:53:59.880 --> 00:54:05.720
So it gives you about 4.2 Giga Flops
on a 2.16 GHz Core Duo Processor.

00:54:05.780 --> 00:54:10.500
And this is only using
one core of the processor.

00:54:10.740 --> 00:54:14.080
If you run the same benchmark with
Apple's accelerated framework you

00:54:14.080 --> 00:54:16.600
get something close to 5 gigaflops.

00:54:16.600 --> 00:54:20.400
A pretty good improvement
over the Intel's MKL library.

00:54:20.460 --> 00:54:25.220
If you are interested in looking
at double precision you get about

00:54:25.220 --> 00:54:31.060
1.4 gigaflops using the Intel's
MKL library and with Apple's accelerated

00:54:31.140 --> 00:54:33.300
framework it is pretty much the same.

00:54:33.300 --> 00:54:47.300
There's not much performance differences.

00:54:47.300 --> 00:54:47.300
So depending on your application and
what kind of primitives do you want to

00:54:47.300 --> 00:54:47.300
use you might want to choose accelerated
framework or the Intel's MKL library.

00:54:47.740 --> 00:54:50.600
Moving forward,
once you write your application

00:54:50.600 --> 00:54:56.240
and you have ported it,
we give you a very nice tool called

00:54:56.260 --> 00:55:00.700
a SHARK to actually profile and
identify performance bottleneck.

00:55:00.700 --> 00:55:01.700
So what is SHARK?

00:55:01.700 --> 00:55:04.700
It's a simple and fast profiling tool.

00:55:04.700 --> 00:55:07.940
It identifies performance
problems in your code.

00:55:07.940 --> 00:55:10.930
It works with a wide
variety of languages,

00:55:10.930 --> 00:55:15.280
C, C++, Fortran,
and any compiler which is actually

00:55:15.280 --> 00:55:21.040
capable of generating stabs or dwarf
debug symbols can be used to profile,

00:55:21.110 --> 00:55:23.500
can be used with SHARK.

00:55:23.600 --> 00:55:27.860
We give you a GUI as well as command
line SHARK and the command line SHARK is

00:55:27.860 --> 00:55:30.570
really good for scripting purposes.

00:55:30.770 --> 00:55:33.770
Shark is part of Chud Tools.

00:55:33.800 --> 00:55:38.000
Chud Tools also have applications
like Big Top and Reggie and Processor

00:55:38.000 --> 00:55:42.890
Control Panel which are really useful
tools and the most important part about

00:55:42.950 --> 00:55:45.480
Chud is that it's available for free.

00:55:46.050 --> 00:55:48.230
So what's new in Shark?

00:55:48.240 --> 00:55:53.520
We've spent last year working
with Shark and improving

00:55:53.520 --> 00:55:55.660
it in several aspects.

00:55:55.760 --> 00:55:59.940
One of the things that we have
done is you can do 64-bit profiling

00:56:00.080 --> 00:56:03.620
not only on PowerPC based systems
but also on Intel architecture

00:56:03.690 --> 00:56:05.000
or Intel based systems.

00:56:05.000 --> 00:56:10.750
We have got UTF-8 support and we have
added the DWARF and DSIM file support

00:56:11.030 --> 00:56:13.500
and one of the features is Symbolication.

00:56:13.500 --> 00:56:17.000
I'll talk to you about
Symbolication in the later slide.

00:56:17.080 --> 00:56:19.560
And finally, Window Time Facility.

00:56:19.560 --> 00:56:22.280
Again, I'll explain what that means.

00:56:22.840 --> 00:56:26.960
So complete 64-bit support,
the Shark binary itself is

00:56:26.960 --> 00:56:30.650
universal and you can also profile
your universal applications.

00:56:30.720 --> 00:56:33.460
It shows symbols,
code and leopard frameworks

00:56:33.460 --> 00:56:34.800
above the 4GB line.

00:56:34.800 --> 00:56:39.870
It has got EM64T support,
you can look at the assembly either

00:56:39.870 --> 00:56:45.800
in Intel or AT&T syntax and it has
also got integrated ISA reference.

00:56:45.800 --> 00:56:51.220
So for those of us who are not familiar
with the Intel assembly language can

00:56:51.350 --> 00:56:54.180
actually learn what those things mean.

00:56:55.100 --> 00:57:00.380
So for those of us who are not familiar
with the Intel assembly language can

00:57:00.380 --> 00:57:03.380
actually learn what those things mean.

00:57:06.940 --> 00:57:14.370
If you have a binary which is
compiled with Dorf information

00:57:14.740 --> 00:57:16.140
then you can use it with Shark.

00:57:16.160 --> 00:57:18.600
And it works with 32 and 64 bit binaries.

00:57:18.650 --> 00:57:22.980
You can mix and match steps and Dorf
information to look up the symbols.

00:57:23.080 --> 00:57:27.310
And Shark also provides the DSIM support.

00:57:28.230 --> 00:57:29.960
Symbolication,
this was one of the feature

00:57:29.960 --> 00:57:32.930
requested by our developers.

00:57:32.970 --> 00:57:38.170
A lot of times when you ship your binary
or you strip away the symbols from it,

00:57:38.170 --> 00:57:42.430
when it comes to, say,
if it goes to the QA or your user base

00:57:42.580 --> 00:57:47.110
and they find any performance issue,
they will actually take a profile

00:57:47.200 --> 00:57:49.040
and probably send it back to you.

00:57:49.070 --> 00:57:51.880
But the profile that they send back to
you does not have symbol information.

00:57:51.880 --> 00:57:53.380
It's all addresses.

00:57:53.440 --> 00:57:57.640
So if Shark allows you to actually
reference a symbol rich binary

00:57:57.640 --> 00:58:01.230
and then automatically apply
symbol information after the

00:58:01.350 --> 00:58:05.190
fact on the profile that you got,
so that now you can see symbol

00:58:05.200 --> 00:58:08.330
information in the profile that
you have received which did

00:58:08.330 --> 00:58:09.850
not have symbol information.

00:58:09.910 --> 00:58:14.150
So that is Symbolication in Shark.

00:58:15.700 --> 01:00:25.000
[Transcript missing]

01:00:30.300 --> 01:00:34.570
Okay, so we're just going to wrap up here
and then we'll have some time for Q&A.

01:00:34.690 --> 01:00:37.550
The window time facility has the best
name ever because you know once you're

01:00:37.580 --> 01:00:40.170
sitting there running and you want
to know what just happened you can

01:00:40.170 --> 01:00:42.780
press the WTF button and then find out,
right?

01:00:42.780 --> 01:00:45.480
So anyway,
so for more information about some of

01:00:45.480 --> 01:00:49.840
the stuff that we've talked about today,
if you have questions about the stuff

01:00:49.840 --> 01:00:54.220
that Yusef was covering you can send some
email to the performance tools group.

01:00:55.060 --> 01:00:59.600
Warner and myself both handle
basically cluster pre-sales.

01:00:59.600 --> 01:01:03.240
I handle the business side and
Warner handles the education side.

01:01:03.240 --> 01:01:06.820
If you're not sure we'd be happy to
direct you to the right people so email

01:01:06.820 --> 01:01:10.530
either of us if you're international
or you know research funded and aren't

01:01:10.530 --> 01:01:11.740
quite sure where you classify in.

01:01:11.740 --> 01:01:13.220
We're friendly, we know each other.

01:01:13.240 --> 01:01:16.220
So we'll be happy to get
you to the right person.

01:01:16.240 --> 01:01:19.000
If you're new to clusters this was
probably pretty intimidating going

01:01:19.010 --> 01:01:21.480
through some of this technical
information but we've got a

01:01:21.480 --> 01:01:23.050
couple good resources for you.

01:01:23.110 --> 01:01:25.020
These are kind of long URLs but
you can find them on the web.

01:01:25.020 --> 01:01:28.970
You can find both of these off of the
just general apple.com/science site which

01:01:28.970 --> 01:01:33.460
is kind of a collector site for all of
this but there you'll find two PDFs.

01:01:33.540 --> 01:01:37.460
One which is evaluating, acquiring,
and deploying clusters which kind

01:01:37.490 --> 01:01:41.660
of is a soup to nuts kind of walk
through of everything that we talked

01:01:41.660 --> 01:01:44.030
about today on an even lower level.

01:01:44.080 --> 01:01:49.420
And then you also can find the high
performance computing solutions

01:01:49.440 --> 01:01:53.220
PDF which is kind of a cookbook
that takes you through the whole

01:01:53.220 --> 01:01:54.980
process of setting up a cluster.

01:01:54.980 --> 01:02:00.090
Literally at the screenshot level showing
you everything that you would need to do.

01:02:00.650 --> 01:02:02.750
and a couple of those
resources I talked about.

01:02:02.760 --> 01:02:04.210
There's a couple mailing lists.

01:02:04.280 --> 01:02:06.140
The SciTech list and the HPC list.

01:02:06.140 --> 01:02:10.360
You can go to lists.apple.com and
find those in the master list there.

01:02:10.360 --> 01:02:13.080
Those are both good,
fairly high volume lists.

01:02:13.170 --> 01:02:21.010
And then as I said the
apple.com/sciencecollector site gets you

01:02:21.010 --> 01:02:21.010
to everything having to do with clusters.

01:02:21.520 --> 01:02:24.140
And to end this up,
some related sessions.

01:02:24.140 --> 01:02:25.710
We're towards the end of the
conference but there are a

01:02:25.760 --> 01:02:26.950
couple more things going on.

01:02:27.000 --> 01:02:30.760
There's developing and porting
Unix applications on Mac OS X which

01:02:30.760 --> 01:02:32.770
is happening right after this.

01:02:32.770 --> 01:02:35.940
And then tomorrow there's develop
and deploy with Xgrid 2 which is

01:02:35.990 --> 01:02:37.720
actually right here in this room.

01:02:37.720 --> 01:02:41.940
A couple other things, Apple in the lab,
Apple Science Connection are ongoing.

01:02:41.940 --> 01:02:44.020
And also if you wanted
to see the new Xserve,

01:02:44.020 --> 01:02:47.490
I'm sure there's a few floating around
the conference but one place I know

01:02:47.490 --> 01:02:49.320
there's one is down in the IT lab.

01:02:49.320 --> 01:02:51.220
That does close at 5 o'clock today.

01:02:51.420 --> 01:02:54.430
It's not open tomorrow so if you want
to see it you're going to have to go

01:02:54.470 --> 01:02:56.000
now and go see it after this show.