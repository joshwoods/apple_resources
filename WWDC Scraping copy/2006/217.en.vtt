WEBVTT

00:00:10.280 --> 00:00:12.790
Okay,
I think I'm going to start off this talk.

00:00:12.940 --> 00:00:15.320
So I'm Chris Niederauer.

00:00:15.380 --> 00:00:20.200
I am a senior 3D graphics software
engineer in the OpenGL team at Apple,

00:00:20.260 --> 00:00:23.660
and I'm going to be talking today
about maximizing your performance

00:00:23.680 --> 00:00:28.500
of your OpenGL applications
on the Mac OS X platform.

00:00:30.500 --> 00:00:34.280
So, in introduction, what I'm going to be
talking about in this talk,

00:00:34.280 --> 00:00:38.660
I'm going to be talking about basically
moving your applications code,

00:00:38.660 --> 00:00:41.190
your OpenGL usage, into modern API.

00:00:41.190 --> 00:00:45.600
And as part of this,
I'm going to be talking about how

00:00:45.600 --> 00:00:51.070
to basically optimize your vertex
and your texture throughput.

00:00:51.190 --> 00:00:55.700
And then I'm also going to
go over basically how to find

00:00:55.700 --> 00:00:56.250
bottleneck and the pipeline,
the OpenGL pipeline.

00:00:56.930 --> 00:01:01.920
So the main point I want to make today
is that there's always a fast path,

00:01:02.030 --> 00:01:03.480
and I'm going to hopefully
tell you how to get there.

00:01:05.190 --> 00:01:08.200
So part of getting there is
knowing which tools to use,

00:01:08.200 --> 00:01:10.830
I mean,
the performance tools that we have.

00:01:10.950 --> 00:01:14.600
And Apple provides a really great
suite of some tools that can help you

00:01:14.920 --> 00:01:19.400
basically pinpoint where the problems
may be in your application and maximize

00:01:19.400 --> 00:01:22.360
your performance on our platform.

00:01:22.640 --> 00:01:27.680
So the first tool, OpenGL Profiler,
is a tool that lets you

00:01:27.680 --> 00:01:31.560
basically look at all of the
OpenGL state in your application.

00:01:31.560 --> 00:01:34.110
You can set breakpoints, look at state.

00:01:34.180 --> 00:01:37.160
You can also...

00:01:37.580 --> 00:01:41.940
Get statistics like the amount of time
that's being spent in certain functions.

00:01:42.090 --> 00:01:43.020
It's a very useful tool.

00:01:43.020 --> 00:01:46.880
The second tool we're going to go
over is the OpenGL Driver Monitor.

00:01:46.880 --> 00:01:51.700
And this gives you more specifics
about the GPU such as the amount of

00:01:51.700 --> 00:01:54.220
time the CPU is waiting for the GPU.

00:01:54.220 --> 00:01:57.830
Also, it tells you how much
free VRAM is available,

00:01:57.830 --> 00:01:59.250
things like that.

00:01:59.320 --> 00:02:00.990
And then finally, there's Shark.

00:02:01.060 --> 00:02:03.680
And I'm not going to be
going over Shark today.

00:02:04.530 --> 00:02:07.800
But there's a lot of great
documentation on that on the web.

00:02:07.800 --> 00:02:10.470
And basically,
that tool lets you pinpoint hot

00:02:10.470 --> 00:02:14.020
spots in your source code of where
you can hopefully improve the

00:02:14.230 --> 00:02:16.340
performance of your application.

00:02:19.170 --> 00:02:21.260
So some key concepts about OpenGL.

00:02:21.260 --> 00:02:22.550
OpenGL is a state machine.

00:02:22.560 --> 00:02:27.170
And what this means is that
you've got a state that you set.

00:02:27.180 --> 00:02:29.330
You do multiple draw calls.

00:02:29.360 --> 00:02:31.430
And throughout those draw calls,
your state will remain

00:02:31.440 --> 00:02:32.730
constant through those calls.

00:02:32.730 --> 00:02:36.300
And you have to change
the state at some point.

00:02:36.370 --> 00:02:39.830
And when you're changing the state,
there's actually an

00:02:39.830 --> 00:02:41.490
overhead to doing that.

00:02:41.620 --> 00:02:47.320
And so basically, I wanted to say today,
try and minimize the amount of times

00:02:47.320 --> 00:02:50.050
that you're changing the state.

00:02:50.220 --> 00:02:52.440
Because it is overhead,
it has to be done, yes,

00:02:52.440 --> 00:02:53.690
but try and minimize it.

00:02:53.740 --> 00:02:57.380
And so you do this by grouping
similar state draw calls together.

00:02:57.380 --> 00:03:02.280
And I also want to mention that if you're
going to be changing a lot of state,

00:03:02.290 --> 00:03:04.890
there's also objects
can encapsulate state.

00:03:04.900 --> 00:03:08.240
So you should be using objects like if
you're changing a lot of vertex pointers,

00:03:08.490 --> 00:03:11.110
vertex color texture
pointers and whatnot,

00:03:11.110 --> 00:03:14.970
you can use the vertex array
objects extension and also frame

00:03:15.000 --> 00:03:21.490
buffer objects or another state,
another object that encapsulates state.

00:03:22.360 --> 00:03:25.330
One of the most important
things about this talk,

00:03:25.330 --> 00:03:30.250
however, is the fact that OpenGL acts
in a client-server model.

00:03:30.300 --> 00:03:35.500
And basically what this means is
that you're going to be having

00:03:35.500 --> 00:03:39.660
to balance the CPU and the
GPU usage of your applications.

00:03:39.660 --> 00:03:44.400
And one way of doing this is,
since OpenGL is a pipeline,

00:03:44.650 --> 00:03:47.540
round-tripping can be expensive.

00:03:47.560 --> 00:03:50.430
So doing like gets of both
data as well as state can

00:03:50.430 --> 00:03:52.500
cause stalls in this pipeline.

00:03:52.500 --> 00:03:56.290
So what we want to do is we
want to maximize asynchronicity.

00:04:00.120 --> 00:04:06.350
So here we see the most common
way that we might think about

00:04:06.360 --> 00:04:10.180
OpenGL as a client-server model
is between the CPU and the GPU.

00:04:10.350 --> 00:04:14.080
And basically,
we're trying to always send data one way.

00:04:14.180 --> 00:04:17.080
In the OpenGL pipeline,
it works most efficiently when

00:04:17.090 --> 00:04:19.100
you're trying to send data one way.

00:04:20.720 --> 00:04:25.870
But OpenGL also is a client-server
model between your application

00:04:25.880 --> 00:04:28.130
and the OpenGL API itself.

00:04:28.130 --> 00:04:32.890
Doing gets and so forth will cause
potential stalls in this pipeline,

00:04:32.890 --> 00:04:37.320
especially important with the
new multi-threaded OpenGL engine

00:04:37.320 --> 00:04:40.820
on Mac OS X Leopard and
available on the Mac Pros.

00:04:42.820 --> 00:04:46.980
So what is a multi-threaded
OpenGL engine?

00:04:46.980 --> 00:04:51.620
It's basically a switch you can
set that will automatically start

00:04:51.800 --> 00:04:57.830
running the OpenGL engine on a
separate thread than your main thread.

00:04:57.840 --> 00:05:03.850
And what this allows you to do is
if you're using asynchronous calls,

00:05:03.950 --> 00:05:09.510
maximizing the use of your pipeline,
you can basically get very large

00:05:09.560 --> 00:05:14.600
performance increases simply by turning
on this multi-threaded OpenGL engine.

00:05:15.630 --> 00:05:20.740
So I was saying to take advantage
of the OpenGL multi-threaded engine,

00:05:20.740 --> 00:05:23.500
the most important part to
that is basically maintaining

00:05:23.500 --> 00:05:28.190
asynchronous behavior,
trying to keep the client's server

00:05:28.750 --> 00:05:30.850
model in a straightforward pipeline.

00:05:30.860 --> 00:05:34.800
So obviously we've been
saying past forever,

00:05:34.800 --> 00:05:38.670
we always have been telling developers,
never call GL Finish.

00:05:38.680 --> 00:05:42.980
There's no reason whatsoever that your
application should be calling GL Finish.

00:05:44.620 --> 00:05:50.200
And then as part of that,
GL Flush as well is usually unnecessary.

00:05:51.990 --> 00:05:55.910
So there's only select cases where
you want to be using geoflush.

00:05:55.910 --> 00:05:59.000
And if you don't know that
you're supposed to be using it,

00:05:59.000 --> 00:06:01.190
then you probably are not supposed
to be using it at that point.

00:06:01.190 --> 00:06:04.750
Basically, whenever you call geoflush,
it's going to cause a

00:06:04.750 --> 00:06:06.300
stall in the pipeline.

00:06:06.320 --> 00:06:12.360
GeoGet and other calls like that
where you're trying to get backstate

00:06:12.360 --> 00:06:18.180
will also cause synchronization
between OpenGL and your application.

00:06:19.220 --> 00:06:21.490
And so what you want to try
and do is shadow as much

00:06:21.490 --> 00:06:23.060
state as you can when you can.

00:06:23.070 --> 00:06:25.810
And if you do the shadowing,
you don't necessarily have

00:06:25.810 --> 00:06:26.610
to do these gets back.

00:06:26.640 --> 00:06:30.380
So only use GeoGet in the cases
where you can't shadow state,

00:06:30.530 --> 00:06:34.890
such as when you are looking up the
extension string for the first time on

00:06:34.890 --> 00:06:37.590
your video cards that you may be using.

00:06:40.090 --> 00:06:44.430
Also, to maximize asynchronous behavior,
there are points at which you do

00:06:44.490 --> 00:06:48.540
need to synchronize using fences or
occlusion query or things like that.

00:06:48.600 --> 00:06:52.380
So when you are going to be
using something like fences

00:06:52.390 --> 00:06:55.080
or occlusion query testing,
what you want to do is

00:06:55.080 --> 00:06:57.880
you want to try and defer,
use the most time that you can to

00:06:58.350 --> 00:07:01.840
defer the actual set and the test.

00:07:02.220 --> 00:07:06.200
If you do this, you're able to,
the more time that you

00:07:06.320 --> 00:07:10.510
utilize outside of this set,
between the set and this test,

00:07:10.510 --> 00:07:14.680
the more time that,
the less time that you'll likely

00:07:14.740 --> 00:07:16.790
stall the pipeline by doing so.

00:07:17.670 --> 00:07:19.320
Let's see.

00:07:19.320 --> 00:07:23.290
Then there's also, again,
I'm going to mention, you know,

00:07:23.290 --> 00:07:26.840
state setting in general takes overhead,
so trying to avoid that.

00:07:26.890 --> 00:07:30.600
Again, group similar state
draws to cause together.

00:07:30.600 --> 00:07:37.820
And if you do this,
you're basically optimizing

00:07:37.820 --> 00:07:37.820
for the multi-threaded engine.

00:07:37.820 --> 00:07:37.820
And...

00:07:38.290 --> 00:07:40.620
Hopefully you'll get a
big benefit out of that.

00:07:40.690 --> 00:07:44.980
So later I'm going to go over how to
do dynamic vertex and texture data,

00:07:44.980 --> 00:07:47.700
and then I'm also going to go
over frame buffer readback,

00:07:47.760 --> 00:07:51.950
how to do that in an efficient
and asynchronous method.

00:07:52.920 --> 00:07:56.740
So I was talking about call overhead.

00:07:56.770 --> 00:07:59.920
Every time you call into OpenGL,
there's a slight amount of overhead

00:07:59.970 --> 00:08:03.630
that has to occur because simply you're
calling a function at that point.

00:08:03.680 --> 00:08:07.040
But there's also a little bit more
that's involved in the process where

00:08:07.040 --> 00:08:09.010
it has to look up the current context.

00:08:09.010 --> 00:08:14.740
So one way you can work around
this is you can use macros.

00:08:14.740 --> 00:08:18.230
So by simply including
CGL macro and Glue macro,

00:08:18.230 --> 00:08:22.520
if you're using Glue code,
you can avoid this overhead.

00:08:22.800 --> 00:08:24.680
And so I recommend that you use these.

00:08:24.680 --> 00:08:31.900
Also, make sure that you move,
try and use the OpenGL calls

00:08:31.980 --> 00:08:34.230
that move a lot of data with
the fewest number of calls.

00:08:34.230 --> 00:08:36.670
So for instance,
you want to use drawRays or

00:08:36.670 --> 00:08:40.030
drawRangeElements with VBO as
opposed to doing immediate mode

00:08:40.030 --> 00:08:45.400
calls where you have GL begin,
GL vertex, GL vertex, GL vertex, GL end.

00:08:45.900 --> 00:08:48.580
Obviously, it's a lot easier to just
call one GL drawRays call.

00:08:48.580 --> 00:08:52.480
And secondly, as an example, the EXT.

00:08:52.800 --> 00:08:56.720
GPU program parameters extension,
which is rather new,

00:08:56.720 --> 00:09:01.040
allows you to upload multiple
parameters for a fragment program

00:09:01.040 --> 00:09:03.580
or vertex program in a single call.

00:09:03.590 --> 00:09:08.430
And the same API, equivalent API is
available already for GLSO.

00:09:09.910 --> 00:09:12.410
And then, again,
I can't stress this enough,

00:09:12.540 --> 00:09:15.810
try and group your similar
state objects together.

00:09:17.100 --> 00:09:19.440
So, vertex buffer objects.

00:09:19.440 --> 00:09:23.200
I wanted to go over vertex performance.

00:09:23.200 --> 00:09:27.500
And basically, for getting the most out
of your vertex performance,

00:09:27.500 --> 00:09:30.740
you want to be using,
instead of compiled vertex arrays,

00:09:30.740 --> 00:09:32.840
you want to be using
vertex buffer objects.

00:09:32.840 --> 00:09:37.760
And what vertex buffer objects are,
are a way to encapsulate,

00:09:37.770 --> 00:09:42.940
basically an efficient method
for uploading the vertex data.

00:09:43.430 --> 00:09:46.270
And so I have an example here
of using vertex buffer objects.

00:09:46.500 --> 00:09:48.710
Hopefully,
a lot of you are already using this,

00:09:48.710 --> 00:09:50.960
and so probably very
familiar with this code.

00:09:50.980 --> 00:09:54.400
But we have in this code sample,
we just have a,

00:09:54.410 --> 00:09:56.620
we bind to this vertex buffer object.

00:09:56.640 --> 00:10:01.420
We say how big it is with
the buffer data call,

00:10:01.430 --> 00:10:02.820
and then we map it.

00:10:02.830 --> 00:10:06.820
And at this map,
we can then put in some data in there,

00:10:06.830 --> 00:10:09.350
unmap it, and then draw with that.

00:10:09.840 --> 00:10:13.040
So Apple Vertex Array Range is
what we used to be saying

00:10:13.040 --> 00:10:15.550
that you should be using,
and that is a good way

00:10:15.550 --> 00:10:16.990
to push vertex data up.

00:10:17.300 --> 00:10:19.680
However,
it does cause synchronization points

00:10:20.100 --> 00:10:22.970
in the multi-threaded OpenGL engine,
and so we're recommending now that

00:10:22.970 --> 00:10:24.500
you switch to vertex buffer objects.

00:10:26.340 --> 00:10:29.250
And so one last thing about
vertex buffer objects.

00:10:29.310 --> 00:10:35.340
I wanted to make sure that you guys know
that if you know that you're going to be,

00:10:35.340 --> 00:10:38.310
you want to make sure that you
provide the minimal hints that

00:10:38.320 --> 00:10:41.550
you can for these buffer objects
when you're allocating them.

00:10:41.620 --> 00:10:45.530
So if you know you're going to be
only reading a vertex buffer object,

00:10:45.660 --> 00:10:49.640
as opposed to reading and writing it,
then do map it as read-only.

00:10:49.640 --> 00:10:52.320
And if you know, similarly,
if you know that your vertex

00:10:52.320 --> 00:10:55.090
data is only going one way,
you want to be using geostatic

00:10:55.090 --> 00:10:56.270
draw rather than draw.

00:10:56.330 --> 00:11:01.570
So you can do geostatic copy and
so forth with those parameters.

00:11:02.700 --> 00:13:47.600
[Transcript missing]

00:13:49.200 --> 00:13:53.040
So I wanted to turn it
over to Dave Springer here,

00:13:53.040 --> 00:13:57.700
who's going to go and show some
examples of using vertex buffer objects.

00:13:59.080 --> 00:14:00.340
All right.

00:14:00.340 --> 00:14:02.080
Thanks, Chris.

00:14:03.400 --> 00:14:08.200
I'm going to show a
quick demo of using VBOs.

00:14:08.300 --> 00:14:13.130
And... What happened?

00:14:13.800 --> 00:14:15.190
Okay.

00:14:17.240 --> 00:14:21.580
I, uh, no rabbits were harmed in this.

00:14:23.370 --> 00:14:26.560
I found these,
and I saw these rabbits in the hay,

00:14:26.560 --> 00:14:30.850
and I won't tell you what I was doing,
but these...

00:14:31.470 --> 00:14:34.840
This is not a shader.

00:14:34.850 --> 00:14:39.340
So what you're looking at here
is every frame I'm updating all

00:14:39.340 --> 00:14:41.400
the colors at all the vertices.

00:14:41.400 --> 00:14:45.880
So this is immediate mode
and you've got a GL begin,

00:14:45.880 --> 00:14:51.080
GL color, GL vertex, GL normal,
texture coordinate.

00:14:51.080 --> 00:14:52.590
So that's what's happening here.

00:14:52.590 --> 00:14:53.630
No shaders involved.

00:14:53.660 --> 00:14:58.260
Immediate mode,
I'm getting about 26 frames per second.

00:14:58.260 --> 00:15:00.040
I don't know if you guys can
see that number up there.

00:15:01.570 --> 00:15:04.480
Let me flip on CGL macros.

00:15:04.850 --> 00:15:08.390
And you'll see that I do get a
slight increase because I'm not

00:15:08.390 --> 00:15:10.380
making those function calls.

00:15:10.380 --> 00:15:15.070
We're going through about, we figured,
a million or so GL calls to render

00:15:15.150 --> 00:15:19.900
these four rabbits because there's
the number of vertices and so on.

00:15:20.050 --> 00:15:24.530
So that's with CGL macros on.

00:15:24.530 --> 00:15:24.530
Now, with VBOs,

00:15:25.200 --> 00:15:29.410
I jumped to about 110 frames per second.

00:15:29.410 --> 00:15:32.900
So huge, huge increase, like 5x.

00:15:33.520 --> 00:15:35.700
So that's the advantage
of using the VBOs.

00:15:35.700 --> 00:15:40.060
Again,
I've taken the static parts of the data,

00:15:40.060 --> 00:15:43.660
pushed it all onto the card using VBOs,
and then I've taken the

00:15:43.660 --> 00:15:45.360
dynamic part of the data,
the color,

00:15:45.420 --> 00:15:48.820
which I'm refreshing every frame,
and that's another separate VBO.

00:15:48.820 --> 00:15:51.150
And so that's how this is working.

00:15:51.150 --> 00:15:54.560
And you can see the kind of
performance improvement we're getting.

00:15:55.030 --> 00:16:00.010
Also, there's a CPU monitor down here,
and you can see the difference

00:16:00.070 --> 00:16:03.640
in what's going on in my
balance between CPU and GPU.

00:16:03.640 --> 00:16:07.450
When I'm in immediate mode,
you can see that the CPU is,

00:16:07.450 --> 00:16:09.860
one of them anyway, is pegged.

00:16:09.880 --> 00:16:12.660
So it's working really,
really hard all the time,

00:16:12.660 --> 00:16:15.520
forming these commands,
setting the buffers down,

00:16:15.560 --> 00:16:16.720
so on and so forth.

00:16:16.830 --> 00:16:21.600
When I flip over to VBOs,
my CPU usage drops almost to nothing.

00:16:21.600 --> 00:16:24.790
So it's showing me that
I really am offloading.

00:16:24.850 --> 00:16:29.270
everything onto the GPU to
render these rabbits.

00:16:29.690 --> 00:16:32.030
So that means my CPU is freed
up to do a lot of other things.

00:16:32.390 --> 00:16:35.530
Like I could do a physics
simulator and make the rabbits hop.

00:16:35.730 --> 00:16:38.600
I don't have that in this demo,
but I could.

00:16:38.600 --> 00:16:41.430
Okay, that's it.

00:16:41.430 --> 00:16:45.470
Turn it back over to Chris.

00:16:54.280 --> 00:16:55.410
Thanks, Dave.

00:16:55.570 --> 00:16:58.900
So now that we've gone over
vertex performance and modern

00:16:58.900 --> 00:17:01.790
API being vertex buffer objects,
I wanted to go over textures.

00:17:01.800 --> 00:17:04.280
So optimizing texture throughput.

00:17:04.280 --> 00:17:08.430
The four parts that I'm going
to go over are I'm going to

00:17:08.510 --> 00:17:11.930
talk about using the fast,
you want to make sure that you're using

00:17:11.930 --> 00:17:13.390
fast texture type and format pairs.

00:17:13.400 --> 00:17:17.570
You want to minimize the number
of texture copies that you have

00:17:17.620 --> 00:17:20.580
when you can by using like,
for instance,

00:17:20.580 --> 00:17:23.860
using the Apple Texture Range and
Apple Client Storage Extensions.

00:17:24.200 --> 00:17:28.840
And I'm going to go over a little
bit over our pixel buffer object and

00:17:28.840 --> 00:17:33.530
API very similar to vertex buffer
objects but for textures instead.

00:17:33.700 --> 00:17:37.330
And then after that,
I'm going to go over asynchronous,

00:17:37.330 --> 00:17:42.640
basically speeding up your frame buffer
readback by doing it asynchronously.

00:17:42.880 --> 00:17:49.040
So the format and type pairs with
textures are very important that

00:17:49.150 --> 00:17:53.010
you pick something that is capable
by the hardware to try and get

00:17:53.090 --> 00:17:54.490
the most performance out of it.

00:17:54.620 --> 00:17:59.040
So I have here three examples of
texture type format pairs that

00:17:59.040 --> 00:18:02.350
are supported by the hardware.

00:18:02.400 --> 00:18:07.560
And so they are basically
BGRA with unsigned 8888 rev,

00:18:07.700 --> 00:18:11.640
BGRA with short 1555 rev.

00:18:11.720 --> 00:18:18.050
And for YCBR textures,
you need to use unsigned short 8888

00:18:18.060 --> 00:18:22.920
either with or without the rev apple,
depending on your usage of it.

00:18:22.920 --> 00:18:26.660
And I have a picture here
of the shark application.

00:18:26.660 --> 00:18:31.150
And you can see here we basically did
a time profile of an application that

00:18:31.210 --> 00:18:33.660
was not using one of these types.

00:18:33.660 --> 00:18:36.360
And it just happened to be
going down a type that the

00:18:36.370 --> 00:18:38.080
GPU was unable to deal with.

00:18:38.120 --> 00:18:41.440
And so what we see in this case
is we see GLG process pixels.

00:18:41.560 --> 00:18:43.860
And occurring at this point.

00:18:43.930 --> 00:18:46.770
So we can get rid of this completely
by simply using one of these

00:18:46.970 --> 00:18:49.480
texture type and format pairs.

00:18:50.010 --> 00:18:52.850
So, second part about
optimizing texture format,

00:18:53.000 --> 00:18:57.210
or textures, is texture performance,
is by trying to minimize the number

00:18:57.210 --> 00:18:58.650
of texture copies that you are using.

00:18:58.660 --> 00:19:03.020
So, using the normal OpenGL engine,
geotech image, and so forth,

00:19:03.020 --> 00:19:10.510
in the Mac OS X implementation of OpenGL,
we actually keep around three

00:19:11.030 --> 00:19:15.120
copies of the texture in addition
to potentially your fourth copy of

00:19:15.120 --> 00:19:16.760
the texture held by the application.

00:19:17.490 --> 00:19:21.210
And so,
by using some Apple-specific extensions,

00:19:21.330 --> 00:19:25.220
we can get rid of two of
these copies very easily.

00:19:25.220 --> 00:19:28.100
And basically,
you have it so that the application

00:19:28.260 --> 00:19:30.450
and the VRAM are the only copies of it.

00:19:30.540 --> 00:19:34.030
So, here, what we've done here,
and this is basically

00:19:34.080 --> 00:19:35.730
showing what happens.

00:19:35.740 --> 00:19:40.250
There's some sample code on the
developer webpage called Texture

00:19:40.250 --> 00:19:43.240
Range that has an example of doing this.

00:19:43.240 --> 00:19:47.120
I'm going to also show a little bit
more about this sample code later.

00:19:47.240 --> 00:19:52.160
And basically, what it does is it's using
these three extensions,

00:19:52.490 --> 00:19:55.120
Apple Client Storage,
Apple Texture Range,

00:19:55.120 --> 00:19:56.940
and EXT Texture Rectangle.

00:19:57.000 --> 00:19:59.440
And by using these three together,
we're able to get rid

00:19:59.450 --> 00:20:00.700
of these extra textures.

00:20:00.760 --> 00:20:02.620
So, how does this work?

00:20:02.850 --> 00:20:09.590
Well, the Apple Client Storage extension
basically allows the application

00:20:09.750 --> 00:20:17.000
to tell OpenGL that it is going to
keep around a copy of the textures.

00:20:17.000 --> 00:20:19.680
That OpenGL can look at at any point.

00:20:19.840 --> 00:20:22.400
So, this means that after
you call your tech image,

00:20:22.460 --> 00:20:28.360
you have to keep your copy of the texture
in your application's memory around and

00:20:28.420 --> 00:20:35.300
try and not modify it while the GPU is
using it or while OpenGL is using it.

00:20:36.110 --> 00:20:36.870
Let's see.

00:20:36.870 --> 00:20:41.280
So basically, that is different from the
normal behavior in that normally

00:20:41.280 --> 00:20:45.270
when you call geotech image,
you can free your texture immediately

00:20:45.270 --> 00:20:49.020
afterwards because of the fact
that OpenGL is making for you.

00:20:49.080 --> 00:20:51.360
So that's what client storage does.

00:20:51.360 --> 00:20:54.940
And then we have, in addition to that,
we have Apple Texture Range and

00:20:54.960 --> 00:20:56.480
EXD Texture Rectangle.

00:20:56.480 --> 00:20:59.050
And by using those two together,
we then get rid of the

00:20:59.050 --> 00:21:00.240
OpenGL driver copy.

00:21:01.080 --> 00:21:05.470
And here, in particular,
we have Apple Texture Range specified

00:21:05.910 --> 00:21:08.000
with a storage cache tint.

00:21:08.020 --> 00:21:11.300
And by using this cache tint,
this is telling it that

00:21:11.390 --> 00:21:12.710
we want to copy in VRAM.

00:21:12.740 --> 00:21:17.020
And we can actually get rid of this
copy as well and have it so that

00:21:17.490 --> 00:21:22.220
the video card is texturing directly
from our system memory copy if we

00:21:22.320 --> 00:21:26.400
were to replace that geostorage cache
tint with a geostorage shared tint.

00:21:30.880 --> 00:21:37.240
So now that we're texturing from client
storage and using all those extensions,

00:21:37.240 --> 00:21:41.230
we are going to have the issue
of now we need to maintain that

00:21:41.230 --> 00:21:44.850
we're not modifying the texture
data while the OpenGL is using it.

00:21:44.940 --> 00:21:47.480
So this means that we're going
to be potentially stalling

00:21:47.480 --> 00:21:49.020
on our data synchronization.

00:21:49.020 --> 00:21:53.710
So the way that we can get around
this is by double buffering our

00:21:53.760 --> 00:21:55.900
data that's going through OpenGL.

00:21:56.640 --> 00:22:01.820
And we will use the fences and
object testing as needed in order to

00:22:01.820 --> 00:22:06.600
make sure that we're not modifying
this texture data while it's in use.

00:22:06.600 --> 00:22:09.640
So here, again,
this is showing stuff from

00:22:09.780 --> 00:22:12.150
the texture range sample code.

00:22:12.180 --> 00:22:18.190
And here we have the CPU is
generating a texture and then

00:22:18.330 --> 00:22:24.630
drawing it and basically drawing it
on the GPU and then modifying it for

00:22:24.630 --> 00:22:26.260
a second frame and then drawing it.

00:22:26.640 --> 00:22:27.800
And then drawing it again to the GPU.

00:22:27.800 --> 00:22:33.710
And what we see here is that the
CPU is unable to modify the copy

00:22:33.710 --> 00:22:39.520
of the texture at this point,
circled in red, because the GPU is still

00:22:39.520 --> 00:22:41.000
using it at that point.

00:22:41.040 --> 00:22:45.420
And so we basically get this stall in
the pipeline right here while as we wait,

00:22:45.420 --> 00:22:49.100
we call geofinish object,
waiting on that texture object.

00:22:49.160 --> 00:22:51.680
We wait until the GPU is done
and we have this huge stall here.

00:22:51.700 --> 00:22:55.940
And also we get the stall on the GPU as
a result of that stall on the CPU.

00:22:56.640 --> 00:23:00.620
So how do we get rid of this
synchronization or at least make sure

00:23:00.880 --> 00:23:03.260
that it's as efficient as possible?

00:23:03.260 --> 00:23:05.450
We simply double buffer it.

00:23:05.560 --> 00:23:09.960
So here we see that we have the first
texture modifying it on the CPU,

00:23:09.960 --> 00:23:11.230
drawing with it.

00:23:11.400 --> 00:23:15.720
Second texture is modified
with it and drawn.

00:23:15.720 --> 00:23:19.140
And at that point, we call finish object
on the first texture.

00:23:19.140 --> 00:23:23.820
And the first texture is able to,
at that point, we've minimized the gap in

00:23:23.820 --> 00:23:25.710
the pipeline at that point.

00:23:25.760 --> 00:23:26.620
By deferring the gap.

00:23:26.620 --> 00:23:29.780
By deferring that testing of
the fence until a later point.

00:23:29.840 --> 00:23:32.620
And by double buffering,
we've filled in the gap

00:23:32.630 --> 00:23:34.370
that was otherwise there.

00:23:38.280 --> 00:23:41.440
So now I'm going to defer to Dave again.

00:23:41.500 --> 00:23:45.910
And he's going to show an example of
texturing performance using texture

00:23:45.910 --> 00:23:48.310
range and pixel buffer objects.

00:23:48.760 --> 00:23:49.150
Okay.

00:23:49.150 --> 00:23:54.580
What I have here is a demo that really
shows different texturing procedures.

00:23:54.580 --> 00:23:58.660
Texturing performance is
really application specific.

00:23:58.660 --> 00:24:02.980
So, your texture performance is
going to vary depending on how

00:24:02.980 --> 00:24:05.090
you're actually using them.

00:24:05.090 --> 00:24:09.560
There's no real one boilerplate
method for saying if you do this,

00:24:09.560 --> 00:24:12.580
your textures will just always go faster.

00:24:12.580 --> 00:24:15.240
It really depends on your usage.

00:24:15.780 --> 00:24:17.910
Although,
I can tell you that nearly always

00:24:17.910 --> 00:24:19.770
immediate mode will be the slowest.

00:24:19.920 --> 00:24:21.920
So, here we are in immediate mode.

00:24:21.920 --> 00:24:24.120
I'm uploading a new texture.

00:24:24.120 --> 00:24:26.910
There's five frames
of--this is Danny's stove.

00:24:26.920 --> 00:24:33.120
And we're uploading five
frames--five different textures,

00:24:33.120 --> 00:24:35.870
one, two, three, four,
five in a tight loop like that.

00:24:35.930 --> 00:24:36.920
So, this is immediate mode.

00:24:36.920 --> 00:24:38.910
We're getting about 43 frames per second.

00:24:38.990 --> 00:24:44.910
So, Chris talked about the
texture range cached.

00:24:45.040 --> 00:24:46.500
attributes.

00:24:46.590 --> 00:24:52.120
With that turned on,
my frames per second jumps to about 300,

00:24:52.120 --> 00:24:56.340
and I'm also pushing about
1.3 gigabytes across the bus.

00:24:56.340 --> 00:25:00.240
Now, what's happening here is
that I'm telling the GPU,

00:25:00.240 --> 00:25:01.960
here's your texture bits.

00:25:01.960 --> 00:25:03.450
Go ahead and render it.

00:25:03.450 --> 00:25:05.360
I'm not going to touch them.

00:25:05.360 --> 00:25:10.720
And it doesn't make those extra copies
that Chris was talking about earlier.

00:25:10.720 --> 00:25:15.970
So this is pushing the
texture straight into VRAM.

00:25:15.970 --> 00:25:19.300
Now, if I turn cached to shared,
I'm going to

00:25:19.670 --> 00:25:22.390
You'll notice my frames per
second now jumps up again,

00:25:22.390 --> 00:25:28.540
and I'm almost at the theoretical
limit of the throughput in megabytes

00:25:28.540 --> 00:25:34.530
per second of the bus because what's
happening here is I'm telling the

00:25:34.530 --> 00:25:36.920
GPU there's going to be one copy of this.

00:25:36.920 --> 00:25:39.740
The GPU can access it directly,
and so can the CPU.

00:25:39.740 --> 00:25:46.760
Now, what's interesting about the shared
attribute is that if you're making

00:25:46.760 --> 00:25:51.260
a very small amount of accesses
to the texture across the bus,

00:25:51.260 --> 00:25:54.410
you're going to get
huge performance wins.

00:25:54.420 --> 00:25:56.740
So I've jumped up to
1,200 frames per second.

00:25:56.740 --> 00:26:00.400
I'm at 5 gigabytes per
second across the bus.

00:26:00.400 --> 00:26:03.600
But this is because I made
the texture really small.

00:26:03.600 --> 00:26:05.610
So this is a 1K by 1K texture originally.

00:26:05.620 --> 00:26:09.620
Now I'm grabbing about every, oh,
I don't know, 20th texel or something.

00:26:09.740 --> 00:26:13.070
So I'm not really making a lot
of accesses across the bus.

00:26:13.220 --> 00:26:17.190
So that's where this
method really shines.

00:26:17.680 --> 00:26:21.100
The final method I want to
show you is static PBOs.

00:26:21.100 --> 00:26:22.940
I happen to really like PBOs.

00:26:22.980 --> 00:26:24.930
I didn't really know how to
do texture mapping very well

00:26:24.930 --> 00:26:26.020
before I wrote this demo.

00:26:26.020 --> 00:26:29.590
And let me tell you something,
the PBOs are a snap.

00:26:29.590 --> 00:26:31.860
So if you guys haven't
played with PBOs yet,

00:26:31.860 --> 00:26:34.870
I really recommend that you get
in there and do that because

00:26:34.870 --> 00:26:36.680
they're easy to set up and use.

00:26:36.680 --> 00:26:38.940
You get great performance wins.

00:26:38.940 --> 00:26:44.520
And understanding where your texture
data is relative to the CPU and

00:26:44.520 --> 00:26:47.600
the GPU and across the bus is easy.

00:26:48.420 --> 00:26:52.980
So here we are, static PBO,
the contract I'm making between the

00:26:52.980 --> 00:26:58.830
CPU and the GPU is here's your data,
GPU, I'm not going to modify it.

00:26:58.830 --> 00:27:02.220
So I upload it all to the
GPU and the GPU just spins.

00:27:02.310 --> 00:27:05.100
And again, I'm getting great performance,
500 frames.

00:27:06.500 --> 00:27:10.650
So over here in Driver Monitor,
I just want to show this tool really fast

00:27:10.830 --> 00:27:16.440
because this is a way for you to look at
what's happening with your texture data.

00:27:16.440 --> 00:27:20.040
So if you're noticing that you're
getting texture performance problems,

00:27:20.100 --> 00:27:25.260
Driver Monitor is a great tool
to help pinpoint where those are.

00:27:25.750 --> 00:27:29.110
If I flip down to immediate mode,

00:27:29.150 --> 00:27:33.260
You can see there's a green line there,
right here.

00:27:33.280 --> 00:27:37.790
And this is the amount of texture
data that I'm slamming onto the GPU.

00:27:37.800 --> 00:27:39.130
So this, you can see it jump.

00:27:39.200 --> 00:27:41.340
It jumped right here when
I went to immediate mode.

00:27:41.950 --> 00:27:49.000
Also,
the CPU right now is not doing its...

00:27:49.970 --> 00:27:52.690
Idling waiting for the GPU.

00:27:52.990 --> 00:27:56.680
So if I go now to static PBOs,

00:27:57.770 --> 00:28:01.940
Here, I've got this drop in
texture page on data.

00:28:01.950 --> 00:28:05.380
So I'm not slamming a lot of
texture data across the bus,

00:28:05.380 --> 00:28:08.040
which is better for this particular case.

00:28:08.040 --> 00:28:10.420
And again, guys,
this is really application specific.

00:28:10.420 --> 00:28:13.960
So when you're going to do
your texture optimizations,

00:28:13.960 --> 00:28:16.720
you need to be prepared
to really look into it.

00:28:16.720 --> 00:28:22.470
There is no just boilerplate solution
to making your textures go faster.

00:28:22.530 --> 00:28:25.600
But you always can make
your textures go faster,

00:28:25.600 --> 00:28:29.060
depending on what your usage is.

00:28:29.060 --> 00:28:29.620
That's it.

00:28:29.620 --> 00:28:31.740
Turn it back over to Chris.

00:28:35.200 --> 00:28:36.960
Thank you.

00:28:37.630 --> 00:28:38.850
Thanks.

00:28:39.090 --> 00:28:44.000
So now that we've just gone over how
to upload your textures pretty fast,

00:28:44.160 --> 00:28:48.420
we still haven't gone over downloading,
so reading back the pixel

00:28:48.420 --> 00:28:50.500
data from the video card.

00:28:50.510 --> 00:28:53.560
So those of you familiar with
the old method of doing this,

00:28:53.560 --> 00:28:58.460
using async read pixels,
will be happy to hear there's a

00:28:58.460 --> 00:29:02.670
much easier way to do it just by
simply using pixel buffer objects.

00:29:02.710 --> 00:29:08.240
And so using a vanilla geo read
is not the right thing to do.

00:29:08.240 --> 00:29:09.860
This will cause a synchronization.

00:29:09.860 --> 00:29:15.480
It will cause a stall, basically,
in that it stops everything until--

00:29:15.480 --> 00:29:19.340
you can see in this line of source here
that we're calling geo read pixels.

00:29:19.340 --> 00:29:23.180
And as soon as we enter
this line of source code,

00:29:23.190 --> 00:29:28.100
we don't get back from OpenGL until
the pixel data has been read back.

00:29:28.140 --> 00:29:31.460
So like I was talking about earlier,
you want to try and defer the amount

00:29:31.590 --> 00:29:32.660
of time between your settings.

00:29:32.660 --> 00:29:34.520
So if you're cutting your
testing of your fences,

00:29:34.520 --> 00:29:38.250
and you want to do the same
thing with your read back,

00:29:38.260 --> 00:29:40.500
and you can do this by
using pixel buffer objects.

00:29:40.540 --> 00:29:44.130
So here with pixel buffer objects,
what I've done is I've bound

00:29:44.130 --> 00:29:46.080
to a pixel buffer object.

00:29:46.080 --> 00:29:49.940
I stated the size of the pixel
buffer object and told it that

00:29:49.940 --> 00:29:54.020
I wanted a static read to say that
I was going to read this data back.

00:29:54.020 --> 00:29:58.560
And then in the read pixels,
I pass it a offset into that pixel

00:29:58.560 --> 00:30:02.640
buffer object instead of a pointer,
like we were doing before.

00:30:02.640 --> 00:30:06.380
And we notice we don't synchronize
at this point with geo read pixels.

00:30:06.480 --> 00:30:08.760
But instead,
we get time to do other work on the CPU,

00:30:08.760 --> 00:30:12.490
such as physics or other types
of things that are unrelated

00:30:12.500 --> 00:30:14.400
to this particular read back.

00:30:14.400 --> 00:30:16.920
And then when we're
done with all our work,

00:30:16.920 --> 00:30:19.620
we can go back,
ask when we need this data back,

00:30:19.620 --> 00:30:24.900
we ask OpenGL to map that pixel
buffer object as read only.

00:30:24.900 --> 00:30:28.400
And then we can get the
data out and map it.

00:30:28.400 --> 00:30:29.480
And that's it.

00:30:29.480 --> 00:30:31.320
It's pretty straightforward.

00:30:31.320 --> 00:30:32.620
And it was a lot more complex.

00:30:32.620 --> 00:30:37.310
asynchronous pixel readback before.

00:30:38.320 --> 00:30:43.510
Yeah, so now that I've gone over
texture and vertex throughput,

00:30:43.540 --> 00:30:46.290
I want to talk about finding
the bottleneck in your

00:30:46.290 --> 00:30:48.060
application's performance.

00:30:48.060 --> 00:30:51.600
So as I was saying earlier,
OpenGL is a pipeline.

00:30:51.720 --> 00:30:53.080
And it's made up of certain parts.

00:30:53.080 --> 00:30:55.570
And if you stall any one of
those parts of the pipeline,

00:30:55.720 --> 00:31:00.260
you will cause your application
to slow down as a result of that.

00:31:00.260 --> 00:31:04.100
So I'm going to go over,
trying to identify if that

00:31:04.100 --> 00:31:09.240
bottleneck may be on the CPU,
if it's due to the bus bandwidth,

00:31:09.240 --> 00:31:12.690
also if it's due to the
GPU vertex processing,

00:31:12.700 --> 00:31:17.120
and finally,
if it's due to the fill rate of the GPU.

00:31:17.130 --> 00:31:20.060
So I'm going to go over how
to identify all these things.

00:31:20.060 --> 00:31:23.660
And basically, when you are trying to
identify these things,

00:31:23.660 --> 00:31:24.980
there's two parts.

00:31:24.980 --> 00:31:26.560
There's two ways of doing this.

00:31:26.560 --> 00:31:30.530
Either you can try and change the
workload of that stage to minimize

00:31:30.530 --> 00:31:34.400
the amount of work that it's doing,
lower the amount.

00:31:34.400 --> 00:31:36.800
And if you see an increase in speed,
then you know that.

00:31:36.800 --> 00:31:38.100
That's likely the bottleneck.

00:31:38.100 --> 00:31:41.060
Conversely,
if you know that it does not change

00:31:41.060 --> 00:31:44.720
the frame rate by lowering the
amount of work being done on a stage,

00:31:44.720 --> 00:31:47.670
you can rule out that particular stage.

00:31:48.190 --> 00:31:50.560
So first, CPU bound.

00:31:50.560 --> 00:31:55.040
This tends to be pretty common
for a lot of the applications

00:31:55.040 --> 00:31:56.410
that are being written these days.

00:31:56.590 --> 00:32:01.330
So here, the way to identify if you've
got a CPU bound application

00:32:01.330 --> 00:32:02.700
is pretty straightforward.

00:32:02.730 --> 00:32:06.040
You can simply look at the CPU monitor,
for instance,

00:32:06.040 --> 00:32:10.150
in Top or in Activity Monitor.

00:32:10.270 --> 00:32:14.240
And you can basically see if you're
using 100% of the CPU at that point.

00:32:14.450 --> 00:32:18.200
And another way you could do
it as well is by if you take

00:32:18.200 --> 00:32:22.540
out some sort of processing
step that's unnecessary for the

00:32:22.540 --> 00:32:28.290
OpenGL particulars of the application,
such as taking out, say, the sound.

00:32:28.310 --> 00:32:32.070
If you see that your frame rate
goes up as a result of that,

00:32:32.220 --> 00:32:34.000
then you're likely CPU bound.

00:32:34.060 --> 00:32:37.600
So what's the trick to fixing this?

00:32:37.630 --> 00:32:39.990
Well,
other than using Shark and trying to

00:32:39.990 --> 00:32:43.830
optimize as much of your application,
and then also trying to follow the

00:32:43.830 --> 00:32:47.680
steps I've been going over to try and
optimize your throughput with OpenGL,

00:32:47.920 --> 00:32:50.640
what you want to try and
do now is probably paralyze

00:32:50.810 --> 00:32:52.700
your work as much as you can.

00:32:52.700 --> 00:32:55.610
So if you have things like the
sound engine or physics engine,

00:32:55.790 --> 00:32:57.900
if you can offload those
onto other threads,

00:32:57.950 --> 00:32:58.760
then do that.

00:32:58.800 --> 00:33:04.040
Alternatively, you can try and offload
OpenGL onto another thread.

00:33:04.140 --> 00:33:07.890
But do keep in mind that when you
offload OpenGL onto another thread,

00:33:07.950 --> 00:33:10.720
OpenGL itself is not a thread-safe API.

00:33:11.000 --> 00:33:16.520
So what this means, basically,
you can only access one context,

00:33:16.520 --> 00:33:20.260
from one-- you can only access a
context from one thread at a time.

00:33:20.280 --> 00:33:26.430
Doing otherwise would be a violation
of this and is not acceptable in the

00:33:26.700 --> 00:33:30.750
OpenGL-- in OpenGL implementation.

00:33:31.210 --> 00:33:33.650
So the way that you can work
around this if you do need to

00:33:33.650 --> 00:33:36.540
use it for multiple threads,
but I don't recommend this

00:33:36.580 --> 00:33:39.330
because it's very difficult,
is you can use a CGL lock

00:33:39.390 --> 00:33:40.880
and unlock context API.

00:33:40.900 --> 00:33:44.240
But here we have an
example of an application.

00:33:44.240 --> 00:33:48.730
We see two CPUs here,
and in green we have the application,

00:33:48.740 --> 00:33:53.240
the time being spent in application code,
and in blue we have OpenGL being

00:33:53.260 --> 00:33:56.870
squished on that first CPU,
and we see that it's not, you know,

00:33:56.870 --> 00:34:00.500
we've got an entire CPU there
that's not being utilized.

00:34:01.100 --> 00:34:04.970
So what we simply do is have our
application try and offload some

00:34:04.970 --> 00:34:06.730
of that work on another thread.

00:34:06.740 --> 00:34:09.950
Here we have the main CPU doing
some work that doesn't have

00:34:09.950 --> 00:34:13.050
anything to do with OpenGL,
and then it's synchronizing,

00:34:13.050 --> 00:34:16.020
you see the green arrow pointing down,
it's synchronizing with another

00:34:16.020 --> 00:34:19.010
thread that's doing the OpenGL work,
and then we call OpenGL from

00:34:19.160 --> 00:34:22.480
that second thread,
and OpenGL gets a lot more time as well

00:34:22.480 --> 00:34:25.080
as the application gets a lot more time.

00:34:25.080 --> 00:34:28.990
We're just taking much more advantage
of the dual processors that we've

00:34:28.990 --> 00:34:34.430
been shipping in our systems for,
almost six years, about six years.

00:34:36.030 --> 00:34:40.300
So, alternatively, new to Leopard,
there's the multi-threaded engine.

00:34:40.300 --> 00:34:45.380
So, the multi-threaded engine, you know,
as long as you can apply those

00:34:45.380 --> 00:34:49.510
techniques I've been talking about,
try and maximize your

00:34:49.510 --> 00:34:54.120
asynchronicity of the application
and the client-server model,

00:34:54.120 --> 00:34:57.060
can hopefully get you some big wins.

00:34:57.500 --> 00:35:00.810
But do keep in mind that
this will add more work.

00:35:00.890 --> 00:35:04.070
You should only be doing this
on dual processor systems.

00:35:04.070 --> 00:35:08.100
It would probably be slower on a
single processor system because of

00:35:08.100 --> 00:35:10.370
the fact that it's doing more work.

00:35:10.400 --> 00:35:14.930
But the reason this is so good
is that it's doing the work for

00:35:14.930 --> 00:35:18.720
the synchronization for you,
and so you don't have

00:35:18.910 --> 00:35:20.950
to do any of this stuff.

00:35:21.040 --> 00:35:26.140
All you have to do is call a CGL set
parameter with a KCGL MP Engine.

00:35:26.760 --> 00:35:27.590
And that turns it on.

00:35:27.660 --> 00:35:30.780
And as I was saying,
this only works... Well,

00:35:30.780 --> 00:35:33.120
this works really well with
some well-behaved applications.

00:35:35.930 --> 00:35:41.100
So here we saw the application
was using only one CPU before.

00:35:41.100 --> 00:35:44.540
And simply by flipping a switch,
we see now that OpenGL adds a little

00:35:44.540 --> 00:35:46.480
bit of work on the first thread.

00:35:46.500 --> 00:35:50.000
It moves the work that OpenGL was
doing on the first CPU is now

00:35:50.000 --> 00:35:51.940
being done on the second CPU.

00:35:51.940 --> 00:35:56.680
And we have a small little portion
on the first CPU that the application

00:35:57.110 --> 00:36:00.680
is interfacing with that then
synchronizes with that second

00:36:00.680 --> 00:36:02.980
thread automatically through OpenGL.

00:36:03.790 --> 00:36:08.000
And all the application did was
call this one CGL set parameter and

00:36:08.000 --> 00:36:10.740
turned on the multi-threaded engine.

00:36:13.110 --> 00:36:18.220
So I wanted to give Dave a look
at-- he's going to show you how to

00:36:18.250 --> 00:36:22.260
basically use the OpenGL Profiler
to see how your application performs

00:36:22.260 --> 00:36:24.640
with the multi-threaded engine.

00:36:25.290 --> 00:36:26.920
All right, thanks, Chris.

00:36:27.170 --> 00:36:30.820
New for Leopard and
actually in your seed,

00:36:30.850 --> 00:36:33.360
the DVD that you guys
got at the show here,

00:36:33.520 --> 00:36:37.720
the version of Profiler has a new feature
which allows you to turn multi-threading

00:36:37.720 --> 00:36:39.530
on and off right from Profiler.

00:36:39.700 --> 00:36:41.560
So you don't have to do
anything to your app.

00:36:41.740 --> 00:36:45.220
In fact, you can take somebody else's
app and see what happens.

00:36:45.340 --> 00:36:48.990
So I'm going to do that right now.

00:36:49.370 --> 00:36:51.130
Okay, so this is Doom 3.

00:36:51.350 --> 00:36:53.860
I'm going to run it first
with multithreading off.

00:36:53.860 --> 00:36:56.980
So this is just out-of-the-box Doom 3.

00:36:56.980 --> 00:37:01.340
And what I'm going to do is run the time
demo because I really suck at Doom 3.

00:37:01.340 --> 00:37:03.180
You don't want to watch
me play it up here.

00:37:03.180 --> 00:37:04.300
That would be brutal.

00:37:05.060 --> 00:37:10.140
What's happening here in this
demo is Doom renders about 2,000,

00:37:10.170 --> 00:37:13.260
2,100 frames and then times it.

00:37:13.340 --> 00:37:15.180
It has its own frame counter.

00:37:15.180 --> 00:37:18.540
Profiler also has a frame counter
which is counting frames too.

00:37:18.540 --> 00:37:21.720
But in this case,
we're just going to use Doom's built-in

00:37:21.720 --> 00:37:25.710
frame counter to see what kind of
frame rate it thinks it's getting.

00:37:25.720 --> 00:37:28.840
And, uh...

00:37:30.000 --> 00:37:34.630
This again is on the
single-threaded engine.

00:37:34.750 --> 00:37:37.580
OK, so 62 frames per second.

00:37:37.750 --> 00:37:40.550
Is that legible to you guys up there?

00:37:40.910 --> 00:37:45.970
While this was running,
you noticed the CPU monitor

00:37:46.030 --> 00:37:47.240
as well was pegged.

00:37:47.300 --> 00:37:51.060
So now what I'm going to do
is flip over to Profiler.

00:37:51.190 --> 00:37:54.600
And to use this new feature,
what you have to do is-- as

00:37:54.600 --> 00:37:57.240
with many things in Profiler,
you have to be stopped at a

00:37:57.240 --> 00:38:01.230
breakpoint in order to change
any kind of state in the app,

00:38:01.330 --> 00:38:02.300
which is only fair to the app.

00:38:02.400 --> 00:38:04.340
I mean, really,
you don't want Profiler in there

00:38:04.340 --> 00:38:07.020
mucking around with your app
while you're live changing states.

00:38:07.060 --> 00:38:09.550
So it's better to stop at first,
then change the state.

00:38:09.600 --> 00:38:12.530
So I'm going to stop the app
here at CGL flush drawable,

00:38:12.810 --> 00:38:16.950
go down here to multi-threaded control,
say Force On.

00:38:17.120 --> 00:38:21.470
Then I remove the breakpoint,
say Continue, and that's it.

00:38:21.770 --> 00:38:24.230
Doom is now multi-threaded.

00:38:24.730 --> 00:38:26.200
So let's see what happens.

00:38:26.230 --> 00:38:29.100
I'm going to run that time demo again.

00:38:29.100 --> 00:38:33.440
I think you can already see that
it looks like it's going faster.

00:38:33.440 --> 00:38:39.430
The kind of speed-ups that we have been
seeing in apps where all we did was

00:38:39.430 --> 00:38:43.900
do this kind of a trick with Profiler,
just go turn the multi-threading

00:38:43.900 --> 00:38:48.020
on behind the scenes on it,
we're getting anywhere from 20 to

00:38:48.020 --> 00:38:50.530
40%. And that's just a freebie.

00:38:50.590 --> 00:38:52.480
So, you know, why not?

00:38:53.580 --> 00:38:56.540
Chris has talked about some
different times when you

00:38:56.570 --> 00:39:00.720
don't want multi-threading,
but here we're getting, boy,

00:39:00.720 --> 00:39:04.990
that's hard to read,
88.5 frames per second, up from about 60.

00:39:04.990 --> 00:39:09.240
So about a 40% increase
just by flipping a switch.

00:39:09.240 --> 00:39:11.120
So that's it.

00:39:11.120 --> 00:39:12.400
Thank you.

00:39:18.400 --> 00:39:23.140
Thanks, Dave.

00:39:23.160 --> 00:39:27.460
So as you saw, Doom 3, you know,
we just turned the switch on.

00:39:27.460 --> 00:39:31.230
We didn't work with the developer at all,
and, you know, we're already seeing

00:39:31.250 --> 00:39:32.680
a 20 to 40% increase.

00:39:32.680 --> 00:39:36.200
If they were to follow some
of the synchronization,

00:39:36.200 --> 00:39:38.080
asynchronicity stuff that
I've been talking about,

00:39:38.100 --> 00:39:40.610
potentially we can get even
more performance out of it.

00:39:40.900 --> 00:39:42.740
At least that's the hope.

00:39:42.740 --> 00:39:47.040
I don't know, but we did the same thing
with World of Warcraft,

00:39:47.040 --> 00:39:50.980
and we had a 90% increase after
working with the developer.

00:39:50.980 --> 00:39:54.210
So it just shows that you can get
some big wins if you are CPU-bound

00:39:54.210 --> 00:39:58.240
like Doom 3 and World of Warcraft are
just by simply turning on the switch.

00:40:01.080 --> 00:40:04.710
So next part of the pipeline
I want to talk about is when you

00:40:04.800 --> 00:40:06.640
get bound by the bus bandwidth.

00:40:06.660 --> 00:40:11.810
So the way that you can identify
if your bus bandwidth is the

00:40:11.810 --> 00:40:16.630
limitation of your pipeline,
you can basically try and

00:40:16.630 --> 00:40:20.340
use smaller data sizes,
such as if you have large textures that

00:40:20.350 --> 00:40:23.680
you're trying to send up to the GPU,
you can try sending up smaller textures

00:40:23.680 --> 00:40:29.140
that are simply like one-by-one textures
as opposed to 1,000 by 1,000 textures and

00:40:29.140 --> 00:40:30.820
see if that increases your frame rate.

00:40:30.820 --> 00:40:36.200
If that does increase your frame rate,
then you know you're bus bandwidth bound.

00:40:36.240 --> 00:40:40.850
So ways to address this,
use smaller data sets in general,

00:40:41.130 --> 00:40:47.040
smaller textures, smaller vertex data,
that type of stuff.

00:40:47.040 --> 00:40:53.090
Use shaders to generate your texture
and your vertex data when you can.

00:40:53.350 --> 00:40:57.440
reduce your texture and vertex
uploads with data caching and VRAM.

00:40:57.440 --> 00:41:02.200
So you can use the pixel buffer object
and vertex buffer object extensions to

00:41:02.200 --> 00:41:07.490
basically say that you want to have a
static vertex buffer object and so forth.

00:41:07.520 --> 00:41:10.880
Or with the texture range,
you tell it that you want it to be cached

00:41:10.880 --> 00:41:16.360
and that will try and keep it in the
GPU as opposed to having a streaming

00:41:16.800 --> 00:41:19.160
or dynamics texture or vertex data.

00:41:19.160 --> 00:41:22.020
That would obviously be going
over the bus every single time.

00:41:22.020 --> 00:41:25.830
And then conversely,
I wanted to mention if

00:41:25.830 --> 00:41:30.890
you do see VRAM paging,
say running out of video memory,

00:41:31.060 --> 00:41:35.330
the way that you want to address this
is by trying to reduce the amount

00:41:35.440 --> 00:41:38.510
of data that's being cached in VRAM.

00:41:38.950 --> 00:41:44.140
And you do this by using the stream
or the shared texture types instead

00:41:44.140 --> 00:41:46.890
of the static and the cached ones.

00:41:47.850 --> 00:41:49.310
I overheard this in the hallways.

00:41:49.530 --> 00:41:50.930
The graphics bus is not a big truck.

00:41:50.990 --> 00:41:53.300
It's a series of tubes.

00:41:53.560 --> 00:41:55.400
Keep that in mind.

00:41:55.400 --> 00:41:59.370
You know,
you can't just take everything at once.

00:41:59.370 --> 00:42:00.760
It is limited.

00:42:03.600 --> 00:42:07.270
So now that we're past the bus bandwidth,
let's take a look at the

00:42:07.290 --> 00:42:09.130
GPU vertex transformation.

00:42:09.140 --> 00:42:13.430
And so how do you address this,
or how do you identify this, excuse me?

00:42:13.430 --> 00:42:16.680
You can try using some trivial
vertex shaders and see if

00:42:16.690 --> 00:42:18.460
that increases your speed.

00:42:18.460 --> 00:42:21.810
Alternatively,
you can try and submit less geometry.

00:42:21.870 --> 00:42:25.330
But do remember that if you're
submitting less geometry,

00:42:25.330 --> 00:42:29.940
you're potentially affecting other stages
of the pipeline such as your fill rate

00:42:30.110 --> 00:42:33.200
if you're only drawing part of the cube.

00:42:33.630 --> 00:42:36.460
You're not drawing the fill
rate as well at that point.

00:42:36.530 --> 00:42:42.050
So that's a little bit
harder to figure out,

00:42:42.200 --> 00:42:44.640
but that's a couple ways to identify it.

00:42:44.790 --> 00:42:49.020
And ways to address it,
you can reduce the number of vertices.

00:42:49.360 --> 00:42:53.440
And so you could, for instance,
you could use normal and bump mapping.

00:42:53.440 --> 00:42:56.020
Or if you were at Nick's
talk earlier with GLSL,

00:42:56.020 --> 00:42:59.660
you saw how ATI was using
parallax occlusion bump mapping

00:42:59.660 --> 00:43:01.620
or ray tracing or something.

00:43:01.620 --> 00:43:03.400
I don't know.

00:43:04.110 --> 00:43:07.760
And, you know, with a single quad,
they were able to have these

00:43:07.760 --> 00:43:12.250
tremendously high-resolution-looking...

00:43:12.260 --> 00:43:13.960
that looked like they
were millions of vertices,

00:43:13.960 --> 00:43:15.500
but it was really only a few.

00:43:15.500 --> 00:43:20.100
Another technique, you know,
very common to Quake, Quake 3,

00:43:20.240 --> 00:43:22.800
those types of games,
they use multipass rendering so that

00:43:22.820 --> 00:43:24.860
they only submit the vertex data once.

00:43:24.860 --> 00:43:29.800
And they use multiple textures and
do the multiple passes with only

00:43:29.800 --> 00:43:31.520
one transformation of the vertices.

00:43:31.520 --> 00:43:36.490
You can also try and reduce
the processing by your

00:43:36.490 --> 00:43:38.240
vertex shader processing.

00:43:38.710 --> 00:43:42.400
And finally,
by caching your vertices in VRAM,

00:43:42.400 --> 00:43:44.520
you get faster access to those vertices.

00:43:44.520 --> 00:43:49.700
So this will have a little bit less
stall on that vertex processing.

00:43:51.350 --> 00:43:55.060
And then finally,
I want to go over the fill rate,

00:43:55.090 --> 00:43:58.400
basically the drawing of
the pixels themselves.

00:43:58.580 --> 00:44:01.120
And so one way to identify this,
the main way,

00:44:01.170 --> 00:44:04.360
the easiest way to identify this
that I've found is simply by

00:44:04.360 --> 00:44:06.140
trying to reduce the frame size.

00:44:06.140 --> 00:44:08.990
And so if you reduce frame size,
you see the frame rate go up,

00:44:08.990 --> 00:44:10.860
you know that your fill rate pounds.

00:44:10.860 --> 00:44:14.420
So ways that you can address this,
first you want to try and

00:44:14.420 --> 00:44:17.840
reduce the depth complexity,
both from an algorithmic

00:44:17.840 --> 00:44:20.860
standpoint as well as,
well, for instance,

00:44:21.310 --> 00:44:23.620
not using GeoClear unnecessarily.

00:44:23.770 --> 00:44:26.690
For instance,
if you are drawing over the entire scene,

00:44:26.710 --> 00:44:29.860
there's no reason to call
GeoClear with the color buffer

00:44:29.860 --> 00:44:33.470
every frame since you're already
overriding it every frame anyways.

00:44:34.070 --> 00:44:37.960
Another way is to call
the back-facing polygons.

00:44:38.180 --> 00:44:45.800
You want to remove pixels as early
as possible from the pipeline.

00:44:45.860 --> 00:44:49.220
So you can use the GeoAlphaThunk,
GeoDepthTest.

00:44:49.220 --> 00:44:55.580
Using these sort of tests,
you can trivially reject fragments

00:44:55.690 --> 00:45:00.140
before they've been actually doing
fragment processing or so forth

00:45:00.140 --> 00:45:02.010
that may occur on those fragments.

00:45:02.040 --> 00:45:05.540
Alpha testing is really useful,
by the way,

00:45:05.570 --> 00:45:10.030
for if you have a texture that has
a lot -- say you have a texture of

00:45:10.030 --> 00:45:13.140
a star where you have only stuff
in the center of the texture,

00:45:13.140 --> 00:45:16.210
but then the outside is zero alpha
and not actually being drawn.

00:45:16.340 --> 00:45:19.630
By using alpha test,
you can trivially reject the

00:45:19.760 --> 00:45:25.710
outsides of those textiles and speeds
up the processing tremendously,

00:45:25.850 --> 00:45:27.930
the fill rate at least.

00:45:28.400 --> 00:45:29.300
Let's see.

00:45:29.300 --> 00:45:33.340
You can reduce fragment processing,
shader processing, obviously,

00:45:33.340 --> 00:45:34.860
if you have a complex shader.

00:45:34.860 --> 00:45:37.380
Try and do something a
little bit more simple.

00:45:37.380 --> 00:45:41.570
Again, reducing the frame size.

00:45:41.570 --> 00:45:45.010
And this is especially important
if you have off-screen buffers.

00:45:45.010 --> 00:45:46.480
Don't forget about those.

00:45:46.480 --> 00:45:49.710
If you have an off-screen buffer that's
being rendered at a huge resolution,

00:45:49.710 --> 00:45:54.270
and then you're only seeing it on,
you know, far away distance at, you know,

00:45:54.270 --> 00:45:57.340
say you were to be using frame
buffer objects and rendering a tree,

00:45:57.340 --> 00:46:00.620
you wouldn't want to be rendering
that at 1024 by 1024 if they're

00:46:00.800 --> 00:46:03.460
only going to have those
billboards in the background at,

00:46:03.650 --> 00:46:05.050
you know, 20 by 20.

00:46:05.050 --> 00:46:06.890
Try and use a smaller texture for that.

00:46:08.370 --> 00:46:10.470
and David Koehn.

00:46:10.470 --> 00:46:17.220
And finally,
you want to use simple texture filtering.

00:46:17.220 --> 00:46:20.230
As simple as you can get.

00:46:20.230 --> 00:46:27.110
Like, for instance, don't use tri-linear
and isotropic filtering.

00:46:27.110 --> 00:46:27.110
It's not free.

00:46:27.110 --> 00:46:27.110
If it was free, it would probably be a
lot easier to pronounce.

00:46:30.400 --> 00:46:35.100
So just gone over all the bottlenecks
that are very common to run into.

00:46:35.100 --> 00:46:39.580
And there's a few more things I want
to go over before I end my talk.

00:46:39.580 --> 00:46:43.540
So one of the points I want to
go over is software fallback.

00:46:43.630 --> 00:46:47.850
Software fallback, it's not something you
want to be hitting.

00:46:47.850 --> 00:46:50.100
It is a sink point for one thing.

00:46:50.100 --> 00:46:53.900
And so what software fallback
is is when you're using,

00:46:53.900 --> 00:46:58.400
say, a shader that is too complex
for a video card to support.

00:47:00.300 --> 00:47:03.190
And you may actually fall back to
the software render at that point.

00:47:03.190 --> 00:47:06.730
And then the software is doing
that rasterization for you because

00:47:06.730 --> 00:47:11.510
the GPU was unable to do that,
fulfill that program at that point.

00:47:11.690 --> 00:47:16.310
So you want to try and avoid this
by basically testing it on all your

00:47:16.310 --> 00:47:20.110
supported graphics cards that you have.

00:47:20.220 --> 00:47:26.210
And the way that you get around this is
you simply reduce your shader complexity

00:47:26.210 --> 00:47:28.520
if you are falling back to software.

00:47:28.520 --> 00:47:33.520
So for instance,
the noise function is not supported

00:47:33.520 --> 00:47:38.170
by any hardware at this point,
this shipping.

00:47:38.280 --> 00:47:41.330
And even though there's a noise
function specific to GLSL,

00:47:41.380 --> 00:47:45.500
so whenever you call that noise function,
it has to unfortunately fall back to

00:47:45.500 --> 00:47:47.480
software in order to support that.

00:47:47.480 --> 00:47:50.790
So instead of using that
noise function call,

00:47:50.790 --> 00:47:57.480
you can simply try and use a texture such
as we use in the GLSL showpiece example.

00:47:57.480 --> 00:48:01.830
And by using noise textures instead of
-- like a texture to simulate the noise,

00:48:02.230 --> 00:48:05.300
instead of just calling
noise from the shader itself,

00:48:05.300 --> 00:48:09.210
we can reduce that complexity of
that shader to the point that it's

00:48:09.210 --> 00:48:12.870
hopefully going to be accelerated
in the hardware at that point.

00:48:13.040 --> 00:48:16.160
So then one other thing
I wanted to mention.

00:48:16.160 --> 00:48:19.680
If you see GeoRenderer float
as a library name in Shark,

00:48:19.830 --> 00:48:23.910
this means you're hitting the
software renderer at some point.

00:48:23.980 --> 00:48:31.580
Also, one thing new to Leopard,
the Leopard Profiler version 4.0,

00:48:31.610 --> 00:48:34.740
is that it's going to allow you to,
similar to how you can set a

00:48:34.780 --> 00:48:39.220
breakpoint on if a GeoGet error is set,
you can set a breakpoint on if

00:48:39.220 --> 00:48:41.850
you ever fall back to software.

00:48:47.100 --> 00:48:50.660
I also wanted to mention a little
bit about framebuffer objects.

00:48:50.720 --> 00:48:53.940
Hopefully you saw some of the really
cool demos that you could do this morning

00:48:53.940 --> 00:48:56.340
at John Rososki's talk with Alex Eddy.

00:48:56.340 --> 00:48:59.500
He wrote some really cool applications
using framebuffer objects.

00:48:59.500 --> 00:49:05.560
Basically what they are is some simple,
intuitive, fast render-to-texture

00:49:06.100 --> 00:49:07.600
methods for OpenGL.

00:49:07.600 --> 00:49:11.440
It's similar to pbuffers,
not to be confused with

00:49:11.480 --> 00:49:13.480
pixel buffer objects.

00:49:14.500 --> 00:49:19.030
Pbuffers is an older way of creating
a context that has a drawable,

00:49:19.460 --> 00:49:23.320
and so you can basically render
into a texture using pbuffers.

00:49:23.320 --> 00:49:29.070
But framebuffer objects are an actual
new GL state piece that is cross-platform

00:49:29.070 --> 00:49:33.940
and very simple to use that will do the
same thing without the complexities of,

00:49:34.060 --> 00:49:35.600
for instance,
having multiple contexts because

00:49:36.050 --> 00:49:37.800
it's simply a state object.

00:49:37.880 --> 00:49:43.120
So because it is a state object,
it's not another context.

00:49:43.580 --> 00:49:46.910
It also avoids requiring any
flushing that pbuffers did

00:49:47.000 --> 00:49:48.740
require for synchronizing.

00:49:48.800 --> 00:49:51.510
And of course,
we want to try and make our applications

00:49:51.620 --> 00:49:53.270
as asynchronous as possible.

00:49:54.940 --> 00:49:58.330
And then one really cool part
about framebuffer objects as

00:49:58.420 --> 00:50:02.750
well is that they are able to
be used in sort of a trifecta.

00:50:03.110 --> 00:50:06.660
Framebuffer objects with vertex buffer
objects and pixel buffer objects.

00:50:06.660 --> 00:50:10.850
You can basically render directly
to a pixel buffer object,

00:50:10.920 --> 00:50:13.690
use that pixel buffer object
as a vertex buffer object,

00:50:13.760 --> 00:50:16.800
and you can do this all on the
GPU seamlessly without having

00:50:16.960 --> 00:50:18.720
to round-trip any of that data.

00:50:18.740 --> 00:50:23.440
But one thing to keep in mind if you
are going to use framebuffer objects,

00:50:24.040 --> 00:50:30.140
which are much preferred over pbuffers,
but the one thing that's different is

00:50:30.140 --> 00:50:34.590
although pbuffers work on everything,
framebuffer objects currently are only

00:50:34.720 --> 00:50:38.740
supported on hardware that's capable
of running ARB fragment programs.

00:50:38.740 --> 00:50:45.160
So that means like the
ATI 9600 and above,

00:50:45.180 --> 00:50:50.670
the GeForce FX 5200 and above,
and of course the

00:50:50.670 --> 00:50:53.600
Intel Embedded graphics.

00:50:54.040 --> 00:50:58.280
So, those all support fanbuffer objects.

00:51:00.400 --> 00:51:06.240
Finally,
I wanted to make sure that you guys try

00:51:06.240 --> 00:51:07.580
and optimize for the user's hardware.

00:51:07.580 --> 00:51:13.840
If a user buys a high-end graphics card,
you've got a very capable GPU in

00:51:13.980 --> 00:51:17.400
addition to the capable CPU,
but you might as well try and take

00:51:17.400 --> 00:51:23.220
advantage of that GPU resource for things
that might not even be related to OpenGL.

00:51:24.000 --> 00:51:26.680
So if you aren't using much of
the GPU in your application,

00:51:26.680 --> 00:51:28.480
try and offload some of that work.

00:51:28.480 --> 00:51:31.810
There's some work being
done in that field,

00:51:31.890 --> 00:51:36.580
like just a lot of research in
terms of trying to use these GPUs

00:51:36.580 --> 00:51:40.920
for things other than graphics,
so check that out if you

00:51:40.920 --> 00:51:43.590
haven't looked into that yet.

00:51:44.610 --> 00:51:46.400
I wanted to talk about
integrated graphics.

00:51:46.400 --> 00:51:51.310
So the Intel integrated graphics,
it's a very capable hardware.

00:51:51.310 --> 00:51:53.220
Core image is capable.

00:51:54.080 --> 00:51:55.140
It has fragment programming.

00:51:55.140 --> 00:51:58.680
It runs all the really cool apps.

00:51:58.780 --> 00:52:04.720
It runs like front row and the
little special effects when you

00:52:04.860 --> 00:52:07.360
drop things into your dashboard.

00:52:07.360 --> 00:52:09.020
It runs all those things.

00:52:09.020 --> 00:52:13.820
But do keep in mind that it
does not have hardware TCL.

00:52:13.820 --> 00:52:18.380
And so whenever you're doing complex,
if you're doing a lot of vertex

00:52:18.450 --> 00:52:22.290
data or complex vertex shaders,
it will be going through

00:52:22.290 --> 00:52:23.900
the CPU on those types.

00:52:24.000 --> 00:52:29.450
So keep that in mind when trying to
create the content that you're going

00:52:29.530 --> 00:52:32.070
to be delivering on those systems.

00:52:32.120 --> 00:52:34.570
And then finally,
I just want to mention it does not

00:52:34.580 --> 00:52:36.570
have floating point texture support.

00:52:40.950 --> 00:52:44.920
Also, next part, I wanted to talk about,
you know,

00:52:44.920 --> 00:52:49.670
now that we're shipping the Mac Pros,
we're currently able-- we're shipping

00:52:49.680 --> 00:52:55.170
actually built order now with multiple
video cards in a single machine.

00:52:55.390 --> 00:53:00.220
You could, in theory, have both ATI and
NVIDIA cards on your system,

00:53:00.220 --> 00:53:03.040
and you want to make sure that
your application is able to

00:53:03.040 --> 00:53:05.030
switch between these two GPUs,
say,

00:53:05.050 --> 00:53:09.590
if the user were to drag a window from
one video card to the other video card.

00:53:09.620 --> 00:53:12.360
And so when you do do this
checking to make sure you're

00:53:12.360 --> 00:53:15.950
supporting both the video cards,
do remember only try and check

00:53:16.010 --> 00:53:18.320
those capabilities once per render.

00:53:18.320 --> 00:53:23.020
And I finally want to mention, you know,
don't turn off features just

00:53:23.020 --> 00:53:24.900
by checking one feature.

00:53:25.360 --> 00:53:27.810
Just don't turn off
features just by checking,

00:53:27.810 --> 00:53:30.280
like, one extension or things like that.

00:53:30.280 --> 00:53:33.280
If you're trying to use one extension,
check for that one in particular.

00:53:35.780 --> 00:53:40.860
Okay, so before you guys go,
I've got a huge grab bag.

00:53:40.860 --> 00:53:45.390
There's quite a few hints I just
wanted to throw out there.

00:53:45.390 --> 00:53:49.120
And so I'll just take my go at it.

00:53:49.120 --> 00:53:50.200
Let's see.

00:53:50.200 --> 00:53:51.100
So vertex data.

00:53:51.100 --> 00:53:55.040
For optimizing the amount of vertex data,
you want to try to minimize the

00:53:55.040 --> 00:53:58.920
state changes that you make between
those calls to the vertex data.

00:53:58.920 --> 00:54:02.160
You want to do things like
try and maximize the vertices

00:54:02.160 --> 00:54:03.850
that you do per draw call.

00:54:05.140 --> 00:54:07.620
Let's see.

00:54:07.850 --> 00:54:08.600
Display lists.

00:54:08.600 --> 00:54:12.290
Display lists are still useful
even though I've been talking

00:54:12.340 --> 00:54:13.640
about vertex buffer objects.

00:54:13.930 --> 00:54:17.720
Display lists are still pretty good for
when you have objects that are static.

00:54:17.990 --> 00:54:23.440
And it has an internal optimization
scheme that it does that will

00:54:23.440 --> 00:54:26.330
optimize whatever you pass it.

00:54:26.370 --> 00:54:29.510
Well, not whatever you pass it,
but if you pass it some good stuff,

00:54:29.510 --> 00:54:31.000
it'll be able to optimize it.

00:54:31.010 --> 00:54:35.740
So for instance, with immediate mode,
it can optimize your immediate mode

00:54:35.750 --> 00:54:38.050
if you pass it consistent vertex data.

00:54:38.170 --> 00:54:42.770
So I have an example here of some code
that actually would not be optimized

00:54:42.770 --> 00:54:44.820
by the display lists optimizer.

00:54:44.820 --> 00:54:49.410
And as you can see here,
it's calling geo begin

00:54:49.410 --> 00:54:53.660
followed by vertex,
vertex, color, vertex, and then ending.

00:54:53.660 --> 00:54:56.620
If you were to try and
make a similar draw call,

00:54:56.960 --> 00:55:01.200
there would not be a similar way to
convert that into a geo draw call.

00:55:01.210 --> 00:55:07.960
What I mean by that,
this is a code that's going to

00:55:07.980 --> 00:55:13.400
be set to one of those colors.

00:55:13.800 --> 00:55:21.640
So instead of doing this,
calling this color in the middle of it,

00:55:21.640 --> 00:55:25.840
we add a color at the very beginning
of it as well to specify that we know

00:55:25.840 --> 00:55:29.380
what the color is going to be as we
enter that immediate mode drawing.

00:55:29.380 --> 00:55:33.460
And that makes it so that that
second color call is acceptable.

00:55:33.460 --> 00:55:35.760
is acceptable at that point.

00:55:36.380 --> 00:55:38.280
Another hint, vertex data.

00:55:38.280 --> 00:55:41.770
You can try and use degenerate
triangles to connect your strips,

00:55:41.770 --> 00:55:43.560
your triangle and quad strips.

00:55:43.620 --> 00:55:46.930
This helps you, as I was saying earlier,
you can maximize the

00:55:46.930 --> 00:55:50.020
vertices per draw command,
and this helps with that.

00:55:50.050 --> 00:55:52.220
Look it up if you aren't
familiar with that.

00:55:52.390 --> 00:55:53.740
It's a pretty neat trick.

00:55:55.580 --> 00:55:59.980
You can provide,
the most efficient method

00:55:59.980 --> 00:56:03.420
I've seen of providing color
data is actually as bytes,

00:56:03.450 --> 00:56:04.120
as unsigned bytes.

00:56:04.180 --> 00:56:05.910
So when you're able to do that.

00:56:05.950 --> 00:56:11.830
And finally, for your vertex data,
try and align it to the 16 byte,

00:56:11.850 --> 00:56:16.740
because by doing so,
SSE and AltaVec can take advantage of,

00:56:16.740 --> 00:56:23.140
you can do some faster processing of
that data if the CPU needs to work on it.

00:56:24.840 --> 00:56:29.900
So, some grab bag hints for the tools.

00:56:29.900 --> 00:56:31.930
You can find out where you
might be blocking in your

00:56:32.020 --> 00:56:35.520
application by using Sharks,
Time Profile, All Thread States,

00:56:35.610 --> 00:56:37.040
and also System Trace.

00:56:37.040 --> 00:56:39.700
They work pretty well for
helping you find those blocks.

00:56:40.410 --> 00:56:45.620
And also, my favorite way of doing it is
by using OpenGL Profiler's Trace,

00:56:45.620 --> 00:56:48.620
and you can actually see
timings on a per-call basis.

00:56:48.620 --> 00:56:52.180
So, if you see, for instance,
that your Geo Map Buffer is taking

00:56:52.510 --> 00:56:55.770
a very long time at some point,
you know that you might,

00:56:55.770 --> 00:56:59.590
and if you were not using the
Apple Flush Buffer Range extension,

00:56:59.650 --> 00:57:02.430
and you see the Map Buffer's
taking a long time,

00:57:02.430 --> 00:57:05.990
you know that the GPU must still
be using that data at the point

00:57:06.000 --> 00:57:08.270
where you're trying to map it.

00:57:08.340 --> 00:57:10.140
So,
you might try and push that Map Buffer.

00:57:10.160 --> 00:57:13.370
down until a later point.

00:57:14.030 --> 00:57:17.320
Another bit of information.

00:57:17.320 --> 00:57:21.720
The driver symbols that you get
in Shark can often be misleading.

00:57:21.720 --> 00:57:26.100
Usually you'll see GLD page-off buffered,
GLD get string.

00:57:26.100 --> 00:57:28.560
Symbols like this,
those are usually false alarms.

00:57:28.560 --> 00:57:34.490
And if you see a symbol in the driver,
keep in mind that it might not be

00:57:34.490 --> 00:57:40.500
in doing the work you have because
obviously we can't have the symbols

00:57:40.500 --> 00:57:41.960
available of what's in the drivers.

00:57:42.240 --> 00:57:44.640
Also, some things to look out
for when you're in Shark.

00:57:44.720 --> 00:57:49.080
If you see GLG process pixels, again,
this means that you're swizzling

00:57:49.080 --> 00:57:51.400
textual data to something
that the GPU can handle.

00:57:51.400 --> 00:57:54.320
If you see georenderer
float as a library,

00:57:54.320 --> 00:57:56.740
this means that you're falling
back to the software render.

00:57:56.740 --> 00:58:00.180
And finally,
with the multithreaded engine,

00:58:00.180 --> 00:58:02.140
if you ever see
GLI finish command buffer,

00:58:02.140 --> 00:58:06.570
this means that somewhere in your code,
you are synchronizing with the

00:58:06.570 --> 00:58:08.970
multithreaded OpenGL engine.

00:58:09.400 --> 00:58:13.100
So hopefully you can optimize
these parts out or minimize the

00:58:13.100 --> 00:58:14.800
parts where you have to do this.

00:58:16.770 --> 00:58:19.290
Some hints for textures.

00:58:19.650 --> 00:58:23.420
You want to try and reduce the
VRAM to avoid any texture paging,

00:58:23.420 --> 00:58:23.880
obviously.

00:58:23.880 --> 00:58:27.660
But some ways to do this are by,
you know, you always want to try and

00:58:27.660 --> 00:58:29.610
use minimal texture formats.

00:58:29.660 --> 00:58:34.240
If you're only using, say,
8-bit per pixel textures

00:58:34.240 --> 00:58:38.520
as your source data,
there's no reason to use a float

00:58:38.520 --> 00:58:43.500
data as the type unless you're
doing some computations after

00:58:43.500 --> 00:58:46.220
the fact that require that.

00:58:46.700 --> 00:58:49.020
extra precision.

00:58:49.020 --> 00:58:51.580
Also,
you can compress your textures at the

00:58:51.580 --> 00:58:55.200
slight cost that it will take a little
bit longer for the GPU to render it.

00:58:55.200 --> 00:58:59.570
Decreases the fill rate very slightly,
but it will increase the amount that

00:58:59.570 --> 00:59:03.940
you can actually upload to the video
card by a large significant amount.

00:59:03.960 --> 00:59:07.040
And then finally,
you can always use the shared and

00:59:07.040 --> 00:59:11.470
streaming textures types with pixel
buffer objects and the texture range

00:59:11.540 --> 00:59:19.410
stuff to have large textures simply being
drawn straight out of your system copy.

00:59:20.230 --> 00:59:23.750
Finally, for textures,
if you ever have large textures

00:59:23.790 --> 00:59:30.930
that need to be uploaded over time,
during a real-time application,

00:59:31.240 --> 00:59:35.630
you can space out those uploads
by only uploading sub-images

00:59:35.660 --> 00:59:37.040
of it using GeoTech sub-images.

00:59:37.040 --> 00:59:40.580
Otherwise,
you may actually see stalls where

00:59:41.020 --> 00:59:46.320
you have too much data going up
to the GPU in between your frames,

00:59:46.340 --> 00:59:47.650
and you may see hiccups
as a result of that.

00:59:48.260 --> 00:59:52.870
So with shaders, make sure you only
compile your shaders once.

00:59:52.870 --> 00:59:58.810
And then try and use your attributes
when you're changing the data per vertex,

00:59:58.810 --> 01:00:01.200
but use uniforms otherwise.

01:00:04.730 --> 01:00:07.180
And then finally some
hints on compile time.

01:00:07.330 --> 01:00:09.420
So again, use CGL macros.

01:00:09.650 --> 01:00:14.050
Dave showed you it was getting
like 10% or 20%-- or excuse me,

01:00:14.220 --> 01:00:20.320
like few percent increase in frame
rate in an application that had a very

01:00:20.320 --> 01:00:23.150
significant amount of OpenGL calls.

01:00:23.210 --> 01:00:26.540
And so if you can use that, great.

01:00:26.570 --> 01:00:29.870
And then finally,
for your release builds, again,

01:00:29.870 --> 01:00:33.340
never try and call a geo get
error in your release builds.

01:00:33.360 --> 01:00:36.380
You can use OpenGL Profiler for that.

01:00:36.380 --> 01:00:41.180
You can use it-- same thing with seeing
if you're falling back to software.

01:00:41.200 --> 01:00:44.760
You can use OpenGL Profiler
for that if need be.

01:00:45.050 --> 01:00:47.820
Try and minimize on your
synchronizing calls.

01:00:47.840 --> 01:00:51.460
Also, if you're going to have--
make sure that your VBL is

01:00:51.550 --> 01:00:53.240
turned on when you release it.

01:00:53.270 --> 01:00:56.370
But you may want to have
VBL turned off for development

01:00:56.370 --> 01:00:58.740
purposes simply for benchmarking.

01:00:58.790 --> 01:01:04.450
But do remember that the reason that
people tend to like Mac OS X a lot better

01:01:04.450 --> 01:01:09.600
than some other platforms is we tend
to make things look nice in VBL sync,

01:01:09.600 --> 01:01:11.510
not tear.

01:01:11.890 --> 01:01:17.190
And then also,
if you're using the OpenGL Profiler APIs,

01:01:17.290 --> 01:01:20.560
which are basically there's a
header in the OpenGL framework

01:01:20.620 --> 01:01:23.840
that has some ways that your
application can interact with OpenGL.

01:01:23.840 --> 01:01:28.690
If you are using those,
try not to use those in the release

01:01:28.690 --> 01:01:31.980
builds as every little bit counts.

01:01:31.980 --> 01:01:35.510
And then finally,
just wanted to mention that if

01:01:35.510 --> 01:01:39.840
you're profiling your application,
if you're in O0,

01:01:39.920 --> 01:01:41.880
you may get different results.

01:01:41.880 --> 01:01:44.930
And if you're, say,
in your optimized OS or O2 or whatever,

01:01:44.980 --> 01:01:46.790
you may actually have
some release builds.

01:01:46.840 --> 01:01:48.970
So sometimes you don't
want to profile that way.

01:01:50.360 --> 01:01:56.200
So, in conclusion,
I just hope that you guys can move all

01:01:56.200 --> 01:02:01.170
your applications to be using some of
the more modern techniques in OpenGL that

01:02:01.170 --> 01:02:04.820
we make available on Mac OS X,
such as the vertex buffer objects,

01:02:04.820 --> 01:02:08.920
pixel buffer objects, and texture range,
and also the frame buffer objects.

01:02:08.920 --> 01:02:13.600
And I want to make sure that you guys
all use the tools because they are very,

01:02:13.600 --> 01:02:15.420
very good at what they do.

01:02:15.420 --> 01:02:18.960
And if you get good at them,
you can make your applications scream.

01:02:20.040 --> 01:02:20.560
So...

01:02:22.530 --> 01:02:28.810
More information, Alan is our 2D and 3D
graphics evangelist.

01:02:28.810 --> 01:02:35.190
Also, we have Michelle Castajon,
he's our OpenGL DATS engineer.

01:02:35.340 --> 01:02:41.330
We have a lab right after this,
an OpenGL lab that will be in

01:02:41.330 --> 01:02:45.010
the Graphics and Media Lab.

01:02:45.010 --> 01:02:50.370
And also,
a OpenGL Quartz Composer drop-in

01:02:50.370 --> 01:02:51.130
lab will be tomorrow morning in the
Graphics and Media Lab at 9:00 AM.

01:02:51.130 --> 01:02:51.130
So stop on by if you have any questions.