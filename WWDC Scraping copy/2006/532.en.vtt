WEBVTT

00:00:11.460 --> 00:00:12.890
Good afternoon everybody.

00:00:13.010 --> 00:00:17.200
Welcome to WebObjects
deployments for Leopard Server.

00:00:17.200 --> 00:00:20.390
And my name is Mankit Szhe.

00:00:20.400 --> 00:00:22.400
I'm a senior WebObjects engineer.

00:00:22.610 --> 00:00:27.040
Today I'm very happy to be here with you
guys and we're going to talk about some

00:00:27.040 --> 00:00:29.280
new features on our WebObjects runtime.

00:00:29.400 --> 00:00:34.100
So today I'm going to talk
about four brand new WebObjects

00:00:34.100 --> 00:00:35.700
features for Leopard Server.

00:00:35.950 --> 00:00:40.910
The first and very important one
which is the performance boost

00:00:41.000 --> 00:00:44.400
enabled by using Java new I/O.

00:00:44.530 --> 00:00:48.160
And we're going to introduce a new
threading model and we're going to

00:00:48.300 --> 00:00:52.060
show you what do we mean by performance
boost and let you experience it.

00:00:52.180 --> 00:00:56.640
So the second features
we're going to talk about is

00:00:57.040 --> 00:01:01.900
Load Balancing with
Apache 2.2 Mod Proxy Balancer.

00:01:01.950 --> 00:01:05.350
So, starting in Leopard,
we are deprecating the Mod WebObjects

00:01:05.430 --> 00:01:10.880
proprietary load balancing
module in favor of using standard

00:01:11.090 --> 00:01:12.940
Apache 2.2 Mod Proxy Balancer.

00:01:13.020 --> 00:01:16.330
And the third feature is
we're going to talk about load

00:01:16.380 --> 00:01:19.260
balancing with Tomcat 5.5 Adapter.

00:01:19.260 --> 00:01:22.880
We are doing some experimentations,
exploring options,

00:01:23.010 --> 00:01:25.300
and we'll talk about this very soon.

00:01:26.210 --> 00:01:29.050
The fourth feature,
which is also very important,

00:01:29.050 --> 00:01:33.860
which is I'm very happy to tell
you guys that starting Leopard,

00:01:33.860 --> 00:01:38.540
WebObjects is going to have
built-in JMX support with every

00:01:38.540 --> 00:01:40.360
world application instances.

00:01:40.360 --> 00:01:42.980
And I'm going to talk about
some new APIs and how you guys

00:01:42.980 --> 00:01:44.600
can leverage it and do a demo.

00:01:44.600 --> 00:01:47.470
So, to begin with...

00:01:49.910 --> 00:01:55.870
Let's talk about the performance
analysis we have done with JMX.

00:01:56.110 --> 00:02:01.500
So what we have done is that we tried to,
in that part we introduced a new

00:02:01.840 --> 00:02:07.160
threading model and we tried to
adopt the Java NIO and we do a quick

00:02:07.160 --> 00:02:10.880
performance analysis back in Cupertino.

00:02:10.880 --> 00:02:15.940
And what we did was that we just
loaded the standard file upload

00:02:15.940 --> 00:02:22.420
examples that were shipped with
the Xcode DVD and then we load the

00:02:22.420 --> 00:02:31.280
server on a MacBook 1.83 core-dued
Intel processor with 2 gigs of RAM.

00:02:31.280 --> 00:02:35.400
And the interesting thing is
that we are actually using the

00:02:35.400 --> 00:02:40.120
load generating on a Mac Pro quad
processor with 4 gigs of RAM.

00:02:40.240 --> 00:02:40.880
Why we do that?

00:02:40.880 --> 00:02:45.000
Because we just need to generate a lot
of loads to try to kill the NIO server,

00:02:45.100 --> 00:02:46.220
but we just cannot.

00:02:46.320 --> 00:02:49.320
So let's take a look
at what numbers we got.

00:02:49.520 --> 00:02:56.290
So in terms of response time performance,
and here's what we got with KASIC,

00:02:56.470 --> 00:02:57.720
which is the old world.

00:02:57.840 --> 00:03:00.640
The average response
time was 61 milliseconds.

00:03:00.640 --> 00:03:06.990
The minimum response times was
47 milliseconds and the 90% line,

00:03:07.150 --> 00:03:10.880
that means that 90% of the requests
are being handled by the asynchronous.

00:03:10.880 --> 00:03:14.320
So we can actually show WebObjects
instances within 63 milliseconds.

00:03:14.320 --> 00:03:19.260
So what about -- that was
using the WebObjects 5.3,

00:03:19.260 --> 00:03:22.240
what you guys shipped,
what we shipped with and

00:03:22.240 --> 00:03:23.760
what you already have.

00:03:23.800 --> 00:03:28.730
And with the new threading
model and the NIO runtime,

00:03:28.730 --> 00:03:30.260
we have this.

00:03:30.260 --> 00:03:34.170
So -- hey, thank you.

00:03:36.400 --> 00:03:39.870
So our average response time was
cut from 61 milliseconds to 11

00:03:39.870 --> 00:03:46.290
milliseconds and our minimum response
time is cut from 47 milliseconds to 7.

00:03:46.370 --> 00:03:49.540
Most of our requests,
90% is cut from 63 milliseconds

00:03:49.540 --> 00:03:51.190
to 12 milliseconds.

00:03:51.190 --> 00:03:52.680
So who can do the math?

00:03:52.700 --> 00:03:57.190
That's six times faster.

00:04:01.200 --> 00:04:02.280
Thanks.

00:04:02.460 --> 00:04:07.290
So after we talk about the response time,
what about scalability, right?

00:04:07.290 --> 00:04:11.020
So when you develop applications
from a customer perspective,

00:04:11.020 --> 00:04:15.480
your customer want to get
the response really quick.

00:04:15.600 --> 00:04:17.500
That's why we talk about
response time first.

00:04:17.500 --> 00:04:20.720
But from an application
administrator perspective,

00:04:20.960 --> 00:04:23.810
you want to have a server that's
super scalable so that you don't

00:04:23.810 --> 00:04:27.020
have to buy tons and tons of
machines to serve your customer.

00:04:27.020 --> 00:04:29.710
So let's take a look at the throughput
performance that we have done and

00:04:29.740 --> 00:04:31.130
measured in the last few minutes.

00:04:31.330 --> 00:04:33.320
back in Cupertino.

00:04:33.540 --> 00:04:37.490
In terms of throughput performance,
using the KASIC worlds,

00:04:37.490 --> 00:04:40.850
which is a stream-based blocking
IO using the WebObjects 5.3

00:04:40.850 --> 00:04:45.300
World Worker Threads model,
we get...

00:04:45.660 --> 00:04:50.790
We were able to handle 30 requests per
second and using the new threading model,

00:04:50.880 --> 00:04:55.710
DND, using the Java NIO, we get...

00:04:56.890 --> 00:05:08.120
Mankit Szeghi: 119.

00:05:08.120 --> 00:05:08.120
So how much more requests we
can handle with the new I/O?

00:05:08.120 --> 00:05:08.120
It's four times more requests per second.

00:05:14.150 --> 00:05:17.430
So now what's next?

00:05:17.870 --> 00:05:22.200
So now we have showed you this
performance boost using the new

00:05:22.200 --> 00:05:24.440
NIO and the new threading model.

00:05:24.470 --> 00:05:30.420
Now the next thing that we are going to
talk about is how could that be possible?

00:05:30.450 --> 00:05:33.470
I mean, how does it work with
the new threading model?

00:05:33.510 --> 00:05:35.600
Now we all live with
world worker threads.

00:05:35.720 --> 00:05:39.100
Now you're telling me that these world
worker threads is not relevant anymore?

00:05:39.130 --> 00:05:39.980
What's the deal?

00:05:40.010 --> 00:05:41.860
Why do we get rid of the worker threads?

00:05:41.880 --> 00:05:43.710
And what is so special about the NIO?

00:05:43.750 --> 00:05:47.990
So with that,
we're going to do a dive into the NIO,

00:05:48.440 --> 00:05:53.160
the new threading model,
and the Java NIO architecture.

00:05:53.160 --> 00:05:56.940
So to start with, using Java NIO,
that's something called

00:05:56.940 --> 00:05:58.580
Connection Watcher.

00:05:58.580 --> 00:06:02.160
What it is,
is it's a single thread that's just

00:06:02.160 --> 00:06:05.300
watching for incoming connection.

00:06:05.300 --> 00:06:07.030
So what is so special about this?

00:06:07.080 --> 00:06:10.390
It is special because
unlike the classic model,

00:06:10.540 --> 00:06:13.380
you have one thread watching one socket.

00:06:13.380 --> 00:06:17.000
With the Java NIO and
the new threading model,

00:06:17.170 --> 00:06:21.790
you actually have one thread that's
playing the role of a connection

00:06:21.790 --> 00:06:26.240
watcher and now it's watching hundreds
and thousands of socket channels.

00:06:26.240 --> 00:06:31.520
So that means that you actually
save a lot of server resources.

00:06:31.630 --> 00:06:38.660
There's no more one thread per socket,
which was the case in the classic world.

00:06:38.660 --> 00:06:43.020
So what happened when the
connection watcher discovered us?

00:06:43.020 --> 00:06:44.420
So ready, socket channel.

00:06:44.420 --> 00:06:47.340
It is going to...

00:06:47.500 --> 00:06:49.980
heading off the connections
to a worker thread pool.

00:06:50.100 --> 00:06:51.770
And then what happened to
the worker thread pool?

00:06:51.910 --> 00:06:58.490
The worker thread pool would select its
managed worker thread and distributing

00:06:58.490 --> 00:07:00.730
the connections to each of them.

00:07:00.800 --> 00:07:03.660
And then what happened to this
three worker threads pool?

00:07:03.740 --> 00:07:04.950
Now three keywords.

00:07:05.230 --> 00:07:08.570
One, they work directly with clients.

00:07:08.700 --> 00:07:10.910
Two, they work independently.

00:07:11.230 --> 00:07:15.220
Three, they work independently without
supervision of its thread pool.

00:07:15.220 --> 00:07:17.980
So there's no overhead
like paying back and forth.

00:07:17.980 --> 00:07:21.920
So three is that they
actually work in parallel.

00:07:21.920 --> 00:07:26.790
And later in the slides,
we will see why that's so important

00:07:26.790 --> 00:07:29.930
that they were in parallel independence.

00:07:30.110 --> 00:07:33.730
Because we need that to
achieve maximum concurrency.

00:07:33.940 --> 00:07:37.900
So now, as you see,
all these three workers for us,

00:07:37.900 --> 00:07:41.320
the one in purple and the one in red,
the one in green, they're all doing good,

00:07:41.400 --> 00:07:45.180
serving as clients,
and clients are on different networks.

00:07:45.270 --> 00:07:47.490
So everything seems to be good.

00:07:47.730 --> 00:07:50.370
Now, what happens when trouble comes?

00:07:50.580 --> 00:07:51.350
Like that.

00:07:53.570 --> 00:07:54.490
You see that?

00:07:54.490 --> 00:07:58.060
The worker thread that's in
purple is servicing a client

00:07:58.060 --> 00:07:59.950
on a slow upstream network.

00:07:59.950 --> 00:08:01.380
So what does it mean?

00:08:01.380 --> 00:08:05.460
That means that your server is
actually on a very fast network.

00:08:05.460 --> 00:08:10.140
It's ready to receive lots and lots
and lots of data in a very fast manner.

00:08:10.140 --> 00:08:11.260
But it's too bad.

00:08:11.470 --> 00:08:14.420
Your client is just on
a slow upstream network.

00:08:14.560 --> 00:08:17.380
It cannot give you data quick enough.

00:08:17.420 --> 00:08:21.270
So what happens in the classic world?

00:08:21.470 --> 00:08:24.320
In the classic world,
then the worker threads

00:08:24.320 --> 00:08:28.020
will just sit still,
standing still, blocked on read,

00:08:28.020 --> 00:08:32.230
and then consuming the server
resources and do no useful work.

00:08:32.720 --> 00:08:35.660
Because in the old model,
in the old world, a request,

00:08:35.970 --> 00:08:39.650
once a request comes in,
we have a worker thread assigned to it.

00:08:39.710 --> 00:08:42.250
It grabs really tight to the request.

00:08:42.250 --> 00:08:45.390
And then when the client is not ready,
it will still hold it

00:08:45.530 --> 00:08:47.010
and it will never let go.

00:08:47.020 --> 00:08:50.390
But in the new world, what did they do?

00:08:51.360 --> 00:08:54.850
They actually tell the,
they actually return itself

00:08:54.980 --> 00:08:55.880
to the worker thread pool.

00:08:55.880 --> 00:08:58.180
And then what would the worker,
and then what happened

00:08:58.180 --> 00:09:00.200
to the other two threads,
the red and green?

00:09:00.200 --> 00:09:01.480
Remember three keywords?

00:09:01.560 --> 00:09:02.840
The first one was direct.

00:09:02.910 --> 00:09:05.510
I show you from the previous
slides that they were direct.

00:09:05.590 --> 00:09:06.280
Like, take a look.

00:09:06.410 --> 00:09:07.730
They were directly working with clients.

00:09:07.790 --> 00:09:10.720
And now I'm going to show
you that they're working like

00:09:10.750 --> 00:09:12.580
independently of each other.

00:09:12.580 --> 00:09:16.730
The first worker threads in purple,
because there's no more work for them,

00:09:16.810 --> 00:09:18.800
so they return itself to the thread pool.

00:09:18.800 --> 00:09:20.680
But the other two, it's continued.

00:09:21.110 --> 00:09:22.630
It's handling the request.

00:09:22.720 --> 00:09:24.160
It's processing the business logic.

00:09:24.180 --> 00:09:28.960
So how the worker thread pool
deal with the worker threads?

00:09:28.960 --> 00:09:30.300
It's going to do this.

00:09:30.320 --> 00:09:33.110
It's going to make a note to
the connection watcher and say,

00:09:33.110 --> 00:09:34.200
hey, you know what?

00:09:34.270 --> 00:09:35.400
Watch that client.

00:09:35.400 --> 00:09:38.940
We have to watch them because the
client is on a snow upstream network.

00:09:39.000 --> 00:09:40.930
There's no more work for us.

00:09:40.940 --> 00:09:43.630
So as you see,
the worker threads in purple,

00:09:43.630 --> 00:09:47.170
it's go back to the worker thread
pool and it's getting ready

00:09:47.170 --> 00:09:50.820
to do useful work rather than,
unlike the classic model,

00:09:50.820 --> 00:09:53.190
that the worker thread
would just sit idling,

00:09:53.540 --> 00:09:56.620
blocked, consuming server resources
and doing no useful work.

00:09:56.740 --> 00:09:58.570
So now...

00:10:00.430 --> 00:10:06.000
What happened when the slow
client is finally catch up and

00:10:06.000 --> 00:10:09.960
is ready to send more data,
but we have nobody home,

00:10:10.130 --> 00:10:12.710
nobody home to receive the data.

00:10:12.780 --> 00:10:18.100
So what happened will be the worker
thread would selecting a worker threads.

00:10:18.130 --> 00:10:22.250
Now, what's so special about the
new threading model is that

00:10:23.650 --> 00:10:26.570
This worker thread could be
the same worker thread that

00:10:26.570 --> 00:10:30.760
has surfaced the client before,
or it could be a totally different one.

00:10:30.800 --> 00:10:33.570
Because, as I said before,
unlike the old model,

00:10:33.590 --> 00:10:36.970
when the transactions come in,
a worker thread will hold on to

00:10:36.970 --> 00:10:40.760
those transactions and will hold
on to them until it completes.

00:10:40.840 --> 00:10:42.750
But in the new model, they don't.

00:10:42.840 --> 00:10:46.200
When there's no more work,
the worker thread will return

00:10:46.200 --> 00:10:49.550
itself to the worker thread
pool and doing useful work.

00:10:49.880 --> 00:10:54.090
That means that a worker thread could
be reading in one minute and then

00:10:54.140 --> 00:10:58.580
writing response for another minute
and then processing requests during

00:10:58.790 --> 00:11:00.750
business logic in another minute.

00:11:00.800 --> 00:11:01.770
And guess what?

00:11:01.840 --> 00:11:03.620
They're all independent.

00:11:03.620 --> 00:11:05.840
They're all swappable.

00:11:05.840 --> 00:11:08.950
And they can be serving totally
three different transactions

00:11:09.090 --> 00:11:12.060
at various different times,
doing different things.

00:11:12.060 --> 00:11:14.540
That's how we achieve
maximum concurrency,

00:11:14.590 --> 00:11:17.940
by breaking the tie between
one thread per transaction.

00:11:19.970 --> 00:11:22.570
So now,

00:11:25.100 --> 00:11:55.300
[Transcript missing]

00:11:55.880 --> 00:11:57.880
It is on a slow downstream network.

00:11:58.140 --> 00:11:58.790
What does it mean?

00:11:59.120 --> 00:12:03.540
That means that your server, again,
is on a very fast network.

00:12:03.540 --> 00:12:06.300
It wants to push data really,
really fast to its clients,

00:12:06.310 --> 00:12:08.380
but the client just cannot keep up.

00:12:08.530 --> 00:12:11.700
It doesn't receive the data quick enough.

00:12:11.880 --> 00:12:14.580
As a result, the client is not ready.

00:12:14.670 --> 00:12:16.080
The worker threads have nothing to do.

00:12:16.300 --> 00:12:20.540
Unlike the blocking I/O,
the worker thread will just sit still,

00:12:20.770 --> 00:12:23.210
idling, blocked,
consuming server resources.

00:12:23.400 --> 00:12:27.720
With the new threading model and NIO,
the worker threads will return

00:12:27.720 --> 00:12:31.590
itself to the thread pool,
and then the rest of the worker

00:12:31.590 --> 00:12:36.990
threads will work independently in
parallel without any interference.

00:12:37.000 --> 00:12:41.400
How does the worker thread
pool handle the worker thread?

00:12:41.400 --> 00:12:44.810
It would recycle it,
and it would get it ready to

00:12:44.840 --> 00:12:46.890
handle a different client.

00:12:47.010 --> 00:12:50.440
And while it's doing that,
it will make a note to the

00:12:50.440 --> 00:12:53.840
connection watcher and say,
"Hey, watch that guy.

00:12:53.880 --> 00:12:56.450
That guy is on a slow downstream network.

00:12:56.540 --> 00:12:59.450
When he becomes available,
let us know." And I will let the

00:12:59.450 --> 00:13:03.890
worker thread pool to select one of its
managed worker threads to handle it.

00:13:06.540 --> 00:13:10.380
Now finally,
the clients on a slow downstream

00:13:10.380 --> 00:13:15.400
network has become available,
but there's nobody home to send his data,

00:13:15.400 --> 00:13:16.600
to send him any data.

00:13:16.600 --> 00:13:19.190
So what's going to happen?

00:13:19.500 --> 00:13:20.720
Mankit Szeghi:
So the worker thread pool is

00:13:20.720 --> 00:13:22.990
going to select a worker thread.

00:13:23.130 --> 00:13:25.580
It could be the same thread.

00:13:25.640 --> 00:13:27.650
It could be a totally different thread.

00:13:27.800 --> 00:13:29.310
So in this case, it's a different thread.

00:13:29.460 --> 00:13:30.320
Before it was red.

00:13:30.440 --> 00:13:32.430
Now it was yellow.

00:13:32.580 --> 00:13:35.880
So after they select the worker threads,
they continue servicing

00:13:35.880 --> 00:13:40.400
the request in a direct,
independent, and in parallel manner.

00:13:40.580 --> 00:13:42.020
They're writing response.

00:13:42.250 --> 00:13:47.160
So now the next thing is
that you see the green worker

00:13:47.160 --> 00:13:48.880
threads is fast on both ways.

00:13:48.880 --> 00:13:51.970
It's fast on upstream and
it's fast on downstream.

00:13:52.190 --> 00:13:54.500
So of course it's going to finish first.

00:13:54.650 --> 00:13:57.320
So when it's finished,
it will return itself to worker thread

00:13:57.320 --> 00:13:59.380
pool and then the transaction completes.

00:13:59.430 --> 00:14:00.800
What happened to the other two?

00:14:00.910 --> 00:14:04.460
The other two will continue
working with its clients.

00:14:04.500 --> 00:14:09.260
And then finally when they were done,
then they returned

00:14:09.260 --> 00:14:11.170
themselves to the pool and

00:14:13.010 --> 00:14:13.840
There we go.

00:14:13.840 --> 00:14:18.910
We complete an illustration of
how life looks like with the new

00:14:19.020 --> 00:14:21.970
threading model and Java NIO.

00:14:22.060 --> 00:14:24.070
Let me get some water.

00:14:33.680 --> 00:14:40.000
So, after we have talked about,
after we have shown you some

00:14:40.100 --> 00:14:43.970
performance analysis we have
done in Cupertino headquarters,

00:14:44.030 --> 00:14:47.140
after we have talked about some
theories and understand how things work,

00:14:47.230 --> 00:14:49.750
so the next question you
are going to ask me is,

00:14:49.890 --> 00:14:52.050
Mankit, is this relevant to me?

00:14:52.050 --> 00:14:54.870
Can I replicate this kind of
performance enhancement in my

00:14:54.900 --> 00:14:56.660
home production environment?

00:14:57.560 --> 00:15:00.500
To enable you to do that,
let's talk about some

00:15:00.570 --> 00:15:04.010
configuration settings,
something that you actually

00:15:04.010 --> 00:15:06.230
need to do to enable this.

00:15:07.720 --> 00:15:11.680
So let's talk about
how do you turn it on.

00:15:11.860 --> 00:15:14.400
To turn on the NIO adapter
is really easy.

00:15:14.400 --> 00:15:18.390
You set it to wo nio enable
equal to true and notice that

00:15:18.860 --> 00:15:21.270
that's a stashd in front of it.

00:15:21.400 --> 00:15:22.060
So what does that mean?

00:15:22.060 --> 00:15:25.120
That means that you can use,
you can pass this command line

00:15:25.120 --> 00:15:28.990
using standard Java notations and
do the stashd on command line when

00:15:28.990 --> 00:15:32.280
you launch your Java applications
or if you are a big time,

00:15:32.310 --> 00:15:36.180
long time WebObjects fan,
you would like to use wo properties file,

00:15:36.230 --> 00:15:37.520
that's fine.

00:15:37.740 --> 00:15:39.740
You can just specify that
in the property files.

00:15:39.940 --> 00:15:40.980
Both would work.

00:15:40.980 --> 00:15:45.060
So the moment you turn on this
wo nio enable flag to true,

00:15:45.430 --> 00:15:47.290
you're running with the
new threading model,

00:15:47.350 --> 00:15:53.550
you're running based on NIO and your
WoW worker threads becomes irrelevant.

00:15:53.770 --> 00:15:55.620
So that's no hybrid mode.

00:15:55.670 --> 00:16:00.250
You cannot say, OK, I want an I/O on,
but then I also want a worker

00:16:00.250 --> 00:16:02.150
threads on at the same time.

00:16:02.170 --> 00:16:03.940
No, there's no such thing.

00:16:03.940 --> 00:16:07.120
And the reason we don't do this thing
is because if you turn an I/O on,

00:16:07.230 --> 00:16:11.300
you have one worker threads running
the connection watcher and watching

00:16:11.300 --> 00:16:12.790
hundreds of thousands of requests.

00:16:13.030 --> 00:16:15.530
But if you have at the same time,
if you're on the same server,

00:16:15.530 --> 00:16:18.740
same instances, running the blocking I/O,
it's going to spawn a lot

00:16:18.810 --> 00:16:21.500
of working threads up front,
and it will consume a

00:16:21.500 --> 00:16:22.830
lot of server resources.

00:16:23.030 --> 00:16:25.220
This is something I would try to avoid.

00:16:25.270 --> 00:16:27.910
That's why there's no hybrid mode.

00:16:28.000 --> 00:16:28.760
So it's a big switch.

00:16:29.020 --> 00:16:31.330
You either move or you
either stick around.

00:16:31.460 --> 00:16:35.940
So next, this will be,
will an I/O blocking enable?

00:16:36.140 --> 00:16:38.940
So that seems a little bit confusing.

00:16:39.020 --> 00:16:40.670
So what is it?

00:16:41.360 --> 00:16:44.970
So, NIO is basically two parts.

00:16:44.980 --> 00:16:50.200
One is doing non-blocking
I/O and you have a multiplexer.

00:16:50.200 --> 00:16:53.380
In Java,
terminology is having a selector to

00:16:53.400 --> 00:16:56.540
watch the register socket channel.

00:16:56.910 --> 00:17:00.200
There's another side of
it is that with the NIO,

00:17:00.200 --> 00:17:03.600
it's actually operating
in terms of bytes buffers.

00:17:03.700 --> 00:17:08.700
So, the reason I introduced this flag
is because when I'm developing this,

00:17:08.700 --> 00:17:12.430
I get into a situation that, hey,
that something goes wrong.

00:17:12.870 --> 00:17:18.700
Is it some bugs in the connection
watcher or is it the way it handles its,

00:17:18.700 --> 00:17:20.570
remember,
the worker's report would notify the

00:17:20.660 --> 00:17:26.700
connection watcher every time that this
client is not ready to process data.

00:17:27.280 --> 00:17:31.720
So we turn this blocking enabled to
true and then that would make the

00:17:32.130 --> 00:17:36.310
worker threads just call thread yield.

00:17:36.340 --> 00:17:44.030
So instead of notifying its thread pool,
which we in turn notify its connection

00:17:44.030 --> 00:17:47.050
watcher to watch that slow client.

00:17:47.060 --> 00:17:49.780
So this is basically a debugging thing.

00:17:49.880 --> 00:17:54.100
I'm pretty sure you guys may run into it
when you try to vote and I are enabled,

00:17:54.100 --> 00:17:56.440
you get a true and something breaks.

00:17:56.730 --> 00:18:00.140
Try that with blocking enabled to go
to true and see if it fix your problem.

00:18:00.140 --> 00:18:03.430
And please file the bugs
in bugreport.apple.com.

00:18:03.550 --> 00:18:05.330
We need you guys to help
mature the frameworks.

00:18:05.370 --> 00:18:07.210
Okay, that's enough digression.

00:18:07.220 --> 00:18:10.720
Next, let's talk about woe-nio
bytes buffer factories.

00:18:10.740 --> 00:18:14.810
In Lepre 5.4, we introduced a woe-nio
bytes buffer factories.

00:18:14.840 --> 00:18:20.920
And the most important flag is
called direct bytes buffer enabled.

00:18:20.920 --> 00:18:22.150
So what is it?

00:18:22.240 --> 00:18:23.590
What is direct bytes buffer?

00:18:23.640 --> 00:18:25.840
Okay.

00:18:26.480 --> 00:18:28.550
Direct bytes, okay,
that's two types of buffer.

00:18:28.660 --> 00:18:30.860
One is bytes buffer,
the other one is direct bytes buffer.

00:18:30.940 --> 00:18:33.200
For bytes buffer,
it's consuming heap space.

00:18:33.430 --> 00:18:37.680
It's managed by the VM and
with direct byte buffer,

00:18:37.680 --> 00:18:39.440
it's using non-heap space.

00:18:39.440 --> 00:18:41.230
So what's the benefits of that?

00:18:41.450 --> 00:18:43.820
The benefit is if you're
using non-heap space,

00:18:43.930 --> 00:18:45.730
it's closer to the low level stack.

00:18:45.900 --> 00:18:51.980
So that means that when you do networked
I/O operations or file I/O operations

00:18:52.140 --> 00:18:55.580
using non-heap space memory,
it's going, it's because it's going to

00:18:55.580 --> 00:18:55.580
be a lot more efficient.

00:18:55.700 --> 00:18:59.250
Because it's closer to
the native system call,

00:18:59.390 --> 00:19:02.140
the I/O is going to be faster.

00:19:02.300 --> 00:19:03.390
So what's the catch?

00:19:03.400 --> 00:19:12.230
The catch is the creation cost of the
direct bytes buffer is going to be

00:19:12.620 --> 00:19:16.270
larger than is traditional bytes buffer.

00:19:16.300 --> 00:19:20.860
So to address this concern,
we introduced second

00:19:20.860 --> 00:19:24.300
configuration setting,
which is woe-nio preheated

00:19:24.300 --> 00:19:25.040
bytes buffer pool size.

00:19:25.040 --> 00:19:27.900
So what is it?

00:19:29.970 --> 00:19:35.020
So this is, so let's say if you are
running a small business,

00:19:35.020 --> 00:19:38.670
if you have an intranet,
and at the most you only have 10

00:19:39.280 --> 00:19:43.100
clients accessing your application.

00:19:43.100 --> 00:19:45.670
You don't want to set this
preheated by its buffer pool

00:19:45.670 --> 00:19:50.580
size to a really large number,
because at the most there will be 10.

00:19:50.580 --> 00:19:55.280
So setting it up really big,
that means that the consequences is the

00:19:55.280 --> 00:19:58.540
launch time will be relatively increased.

00:19:59.780 --> 00:20:04.630
So we want to make sure that you set this
number to a reasonable size so that you

00:20:04.630 --> 00:20:07.350
can pay the construction costs up front.

00:20:07.430 --> 00:20:11.390
So when your server is up and running,
then the user would have

00:20:11.420 --> 00:20:14.940
quick response time,
because it would take benefits

00:20:14.940 --> 00:20:20.250
of using non-HEAP memory,
and the cost of using non-HEAP memory

00:20:20.350 --> 00:20:24.490
is taken care of during launch time.

00:20:25.380 --> 00:20:28.980
So the next thing is
NIO defaults by buffer size.

00:20:29.250 --> 00:20:32.280
The default is 8192.

00:20:32.620 --> 00:20:34.000
So what is it?

00:20:34.300 --> 00:20:37.980
So we have a bucket and a water tab.

00:20:38.460 --> 00:20:44.110
So the water tab, in this metaphor,
the water tab is actually your networked

00:20:44.100 --> 00:21:00.100
[Transcript missing]

00:21:01.480 --> 00:21:02.220
The water is dripping.

00:21:02.220 --> 00:21:04.290
Now it's done.

00:21:04.300 --> 00:21:05.680
But take a look.

00:21:05.760 --> 00:21:09.390
The bytes buffer, you allocate it.

00:21:09.500 --> 00:21:11.560
You told the WebObjects
runtime to allocate it.

00:21:11.680 --> 00:21:12.600
It's huge.

00:21:12.600 --> 00:21:15.420
But it's never getting
used to its full capacity.

00:21:15.420 --> 00:21:17.440
The utilization is so low.

00:21:17.440 --> 00:21:18.980
You're wasting a lot of server resources.

00:21:18.980 --> 00:21:20.910
That's the way to kill scalability.

00:21:20.920 --> 00:21:22.200
So please don't do that.

00:21:22.320 --> 00:21:27.800
If you know that you only have a
very small data coming in burst

00:21:27.800 --> 00:21:31.220
mode and your network is slow,
use a small bucket.

00:21:31.220 --> 00:21:35.440
So that when the data comes,
water dripping in,

00:21:35.710 --> 00:21:37.340
and it will use the full size.

00:21:37.340 --> 00:21:41.020
So it's all about,
in terms of performance on deployment,

00:21:41.020 --> 00:21:43.920
it's all about efficient
use of server resources.

00:21:46.600 --> 00:21:56.800
[Transcript missing]

00:21:56.900 --> 00:22:00.220
As you've seen before,
your buckets fill up really fast,

00:22:00.220 --> 00:22:02.430
and what happens when it fills up?

00:22:02.430 --> 00:22:02.430
Huh.

00:22:02.770 --> 00:22:05.370
Your networks want to
push a lot of data to you,

00:22:05.370 --> 00:22:08.590
but you don't have enough
bytes per size to hold it.

00:22:08.600 --> 00:22:09.830
So you have to stop the tab.

00:22:09.920 --> 00:22:11.600
You have to pause the network.

00:22:11.600 --> 00:22:13.100
And then you have to replace the buckets.

00:22:13.150 --> 00:22:16.970
So if you're on a fast network
and expect lots of data coming,

00:22:16.970 --> 00:22:18.810
then you use a larger bucket.

00:22:19.160 --> 00:22:22.640
So set the bytes per size to larger.

00:22:22.830 --> 00:22:24.480
So like that.

00:22:25.500 --> 00:22:33.900
[Transcript missing]

00:22:40.010 --> 00:22:42.970
Remember the connection watcher?

00:22:42.970 --> 00:22:48.960
The connection watcher is implemented
with a class called WoW NIO Multiplexer.

00:22:48.960 --> 00:22:55.340
And we introduced a configuration
setting called Multiplexer Timeout.

00:22:55.340 --> 00:22:56.280
So what is it done?

00:22:56.280 --> 00:22:57.560
Why do we need to do this?

00:22:57.560 --> 00:23:03.040
Because, again, it's about efficient use
of several resources.

00:23:03.040 --> 00:23:05.680
You want to set the timeout, let's say,
to 30 minutes.

00:23:05.680 --> 00:23:07.230
So you can track it.

00:23:08.640 --> 00:23:12.000
We're going to have a host to
the WoW NIO Multiplexer for you.

00:23:12.110 --> 00:23:13.710
So you can do something like this.

00:23:13.720 --> 00:23:17.320
If within 30 minutes,
if you don't see any

00:23:17.320 --> 00:23:19.870
ready socket channels,
that means there's no clients

00:23:19.870 --> 00:23:23.060
hitting your server for 30 minutes,
then you may want to do something.

00:23:23.060 --> 00:23:25.560
Like, let's say,
email the system application

00:23:25.580 --> 00:23:29.560
system administrator saying that,
hey, our server is busy.

00:23:29.560 --> 00:23:31.960
Our server has been idling.

00:23:31.960 --> 00:23:36.360
Maybe we should use the server
and deploy different instances

00:23:36.360 --> 00:23:38.040
and retire these instances.

00:23:38.820 --> 00:23:40.760
and something like that.

00:23:42.390 --> 00:23:43.010
Right.

00:23:43.530 --> 00:23:46.520
And the next thing is,
is adapt to listening queue size.

00:23:46.520 --> 00:23:50.400
Now, for those people,
how many people who comes to

00:23:50.490 --> 00:23:55.090
previous years of WebObjects
performance optimization session?

00:23:55.100 --> 00:23:56.490
Can I raise a hand?

00:23:56.770 --> 00:24:02.480
Now, last year, we talked about, okay,
the default listening

00:24:02.480 --> 00:24:04.640
queue size was four.

00:24:04.640 --> 00:24:06.960
Now, this is not relevant anymore.

00:24:08.370 --> 00:24:11.700
Because with the new threading models,
we do some performance testing.

00:24:11.700 --> 00:24:16.900
Four doesn't seem to be the optimal
for the new threading model.

00:24:16.900 --> 00:24:21.920
And I encourage you guys to try it out,
characterize it, and file a box report.

00:24:21.920 --> 00:24:26.700
Tell us how it works in your production,
in your station environments,

00:24:26.730 --> 00:24:29.000
and we would mature these
frameworks together.

00:24:31.500 --> 00:24:36.400
So the defaults for the new NIO,
it will be 128.

00:24:36.400 --> 00:24:41.890
So next, there's a preheated
worker thread pool size.

00:24:42.170 --> 00:24:43.130
So what does it mean?

00:24:43.140 --> 00:24:49.900
So if you have a Mac Pro,
you want to set this at least

00:24:49.900 --> 00:24:53.290
to 4 because you have 4 cores.

00:24:54.160 --> 00:25:01.110
And so this is basically you
want to see how many concurrent

00:25:01.110 --> 00:25:03.340
requests you want to handle,
you expect to handle.

00:25:03.460 --> 00:25:07.160
If you set it too big, again,
it's consuming a lot of server resources.

00:25:07.160 --> 00:25:13.720
If you set it too small,
you're not fully utilizing your CPU.

00:25:13.730 --> 00:25:15.420
So the default will be 16.

00:25:15.420 --> 00:25:16.420
It may change.

00:25:16.420 --> 00:25:17.410
It's just a DP.

00:25:17.500 --> 00:25:23.740
The good news is that you can actually
specify this yourself based on your need.

00:25:23.740 --> 00:25:25.280
Oops, sorry.

00:25:25.560 --> 00:25:29.190
Okay, so after we have talked
about these configurations,

00:25:30.000 --> 00:25:49.100
[Transcript missing]

00:25:49.310 --> 00:26:04.490
Mankit Szeghi:
I'm going to talk about two graphs.

00:26:04.490 --> 00:26:04.490
One is throughput load graph.

00:26:04.490 --> 00:26:04.490
The other one is the response time graph.

00:26:04.490 --> 00:26:04.490
Now, I encourage you guys to do something
similar to your production environments,

00:26:04.490 --> 00:26:04.490
which is now when you start adding load,

00:26:04.700 --> 00:26:06.410
The throughput is going to increase.

00:26:06.680 --> 00:26:12.050
The response time is going to increase,
but not that much because your

00:26:12.050 --> 00:26:15.540
server is still under utilization.

00:26:15.700 --> 00:26:39.200
[Transcript missing]

00:26:39.690 --> 00:26:43.120
Then, increasing more load,
the throughputs instead

00:26:43.130 --> 00:26:44.540
of increase is decrease.

00:26:44.540 --> 00:26:45.280
Why?

00:26:45.280 --> 00:26:51.480
Because you have reached a very high
utilization and you should do something.

00:26:51.480 --> 00:26:54.840
Like,
think about deploying more instances,

00:26:54.840 --> 00:26:56.280
using more server.

00:26:56.280 --> 00:27:00.160
What happens if we don't do anything?

00:27:00.160 --> 00:27:03.360
Or we keep just monitoring it,
assuming that,

00:27:03.510 --> 00:27:05.750
don't worry about this main kit,
don't worry about it.

00:27:05.760 --> 00:27:07.760
It will just work, crossing our fingers.

00:27:08.270 --> 00:27:13.430
Then if you hit this buckle point,
when more user comes in, boom, it's dead.

00:27:13.460 --> 00:27:18.990
Your throughputs will decrease
exponentially and then your response

00:27:18.990 --> 00:27:20.910
time will increase exponentially.

00:27:20.920 --> 00:27:24.370
So basically, you're just thrashing,
not doing any useful work.

00:27:24.460 --> 00:27:27.840
So after that...

00:27:29.880 --> 00:27:33.530
Let's do a quick summary of what
we have talked about so far.

00:27:33.580 --> 00:27:35.800
We talked about the performance number.

00:27:35.800 --> 00:27:38.980
One,
we talked about the performance number.

00:27:38.980 --> 00:27:42.560
Two, we talked about the theories
like how the new threading model

00:27:42.560 --> 00:27:44.800
works and how the NIO works.

00:27:44.800 --> 00:27:47.790
And now we talked about
configuration setting.

00:27:47.810 --> 00:27:51.800
We talked about analyzing the
performance on a production environment.

00:27:51.800 --> 00:27:54.800
Next thing I'm going to
talk about load balancing.

00:27:54.800 --> 00:27:58.800
As we said, once it hit buckle points,
your instances are dead.

00:27:58.800 --> 00:28:02.680
So we need to start thinking about
load balancing while you hit your

00:28:02.750 --> 00:28:06.760
saturation point or way before
you hit saturation point at best.

00:28:06.850 --> 00:28:09.490
So with that,
I'm going to talk about load

00:28:09.490 --> 00:28:12.630
balancing with Apache 2.2.

00:28:26.400 --> 00:28:30.610
With this, starting in Leopard,
we are going to use Mod Proxy

00:28:30.610 --> 00:28:38.710
Balancer instead of the Mod WebObjects
that were Apple proprietary.

00:28:38.890 --> 00:28:42.000
And I think it's a good thing that we
move on to Apache Community because

00:28:42.010 --> 00:28:47.180
you can always get a nice new view for
Apache and it's actively being developed.

00:28:48.020 --> 00:28:51.920
So in Leopard,
some of you guys probably see

00:28:51.920 --> 00:28:53.420
this slide in another session.

00:28:53.420 --> 00:28:59.380
So we actually have a
GUI in Leopard server,

00:28:59.380 --> 00:29:03.810
the server admin application,
so that you can specify your application

00:29:03.890 --> 00:29:07.130
cluster using load mod proxy balancer.

00:29:07.140 --> 00:29:12.560
So yes, you can use Apache frontend to
distribute load to WebObjects backend.

00:29:14.600 --> 00:29:18.470
Right, and by the way,
WebObjects is running faster

00:29:18.570 --> 00:29:21.760
than Ruby and RHEL and is much
more scalable than Ruby and RHEL.

00:29:22.010 --> 00:29:25.120
WebObjects is still alive
and is moving forward.

00:29:31.530 --> 00:29:36.020
So after this,
I'm going to-- so we talk about the GUI.

00:29:36.070 --> 00:29:39.820
So what about those people
who actually use KMLM?

00:29:39.830 --> 00:29:42.040
How many people-- let's
do a quick survey.

00:29:42.040 --> 00:29:46.790
How many people here would prefer
using a GUI piece where you still have?

00:29:48.100 --> 00:29:56.700
Mankit Szeghi: Okay,
so how many people who would actually,

00:29:56.700 --> 00:29:56.700
I do all my system administration
with command lines.

00:29:56.700 --> 00:29:56.700
I do SSH in a data center.

00:29:57.230 --> 00:30:01.130
Okay, so in Leopard,
starting in Leopard Server,

00:30:01.130 --> 00:30:05.340
WebObjects is going to
take care of both audience.

00:30:05.340 --> 00:30:09.100
We have the fun end and we have the,
you can do,

00:30:09.290 --> 00:30:13.700
you can configure and administer your
application on command line using SSH.

00:30:13.700 --> 00:30:15.220
So how do we do that?

00:30:15.270 --> 00:30:16.170
Five step.

00:30:16.450 --> 00:30:20.350
The first step is you need to
use SCP securely copy your board

00:30:20.350 --> 00:30:22.950
executable into your remote host.

00:30:22.950 --> 00:30:27.020
Step two,
you generate the launch e.plist file.

00:30:27.100 --> 00:30:31.720
So let's talk about
launch e for a second.

00:30:31.770 --> 00:30:36.040
In WebObjects 5.3,
there's something called vote test e.

00:30:36.040 --> 00:30:39.150
So what it does,
it's actually trying to keep track of

00:30:39.150 --> 00:30:43.590
the healthiness of your applications
and they send live bits and if you don't

00:30:43.590 --> 00:30:47.950
get the live bits in a timely manner,
then it will thought that it was dead,

00:30:47.950 --> 00:30:51.730
it will recycle, it will restart it and
something like that.

00:30:51.790 --> 00:30:54.830
And Apple is pushing launch e.

00:30:54.840 --> 00:30:57.080
So and we move, we move to the next step.

00:30:57.080 --> 00:30:58.440
We're getting along with them.

00:30:58.470 --> 00:31:00.180
So we're jumping into launch e.

00:31:00.200 --> 00:31:02.830
So and the benefits of
this is that you have,

00:31:02.890 --> 00:31:05.600
you get rid of a lot of
problems that we have,

00:31:05.600 --> 00:31:09.450
that you guys may have been
experiencing with vote test e.

00:31:11.600 --> 00:31:27.600
[Transcript missing]

00:31:27.820 --> 00:31:31.900
Your WebObjects instances are starting
and running and then if it dies,

00:31:32.560 --> 00:31:35.780
the launch day will pick
it up and restart it again.

00:31:35.890 --> 00:31:38.920
There's no more WOTAS days,
so that means that there's

00:31:38.920 --> 00:31:40.440
no more close rate problem.

00:31:40.440 --> 00:31:42.930
If you're deployed on Apache.

00:31:45.930 --> 00:31:48.040
Thanks.

00:31:48.040 --> 00:31:50.370
So it seems like people do
experience close-range problems

00:31:50.370 --> 00:31:53.080
with WebObjects 5.3 in Apache 1.3.

00:31:53.080 --> 00:31:55.780
But good thing is that we
are moving to Apache 2.2.

00:31:57.720 --> 00:32:04.890
So, and then the fourth step is to
create a XTDBD-webobjects.conf file.

00:32:05.200 --> 00:32:20.800
[Transcript missing]

00:32:24.100 --> 00:32:29.170
Okay, so in the example source,
there's actually a new tab that

00:32:29.300 --> 00:32:30.880
I put together really quick.

00:32:31.090 --> 00:32:34.160
Then it would,
basically what it does is if you have

00:32:34.160 --> 00:32:38.450
existing deployments environments,
they all deploy with Wotacity,

00:32:38.550 --> 00:32:41.320
they all deploy on Apache 1.3 Adapter.

00:32:41.320 --> 00:32:45.570
Now, to move to Apache 2.2,
there's one quick shortcut for you guys.

00:32:45.620 --> 00:32:50.140
Fire up your Java monitor and you
click on a tab called Migration Tab.

00:32:50.160 --> 00:32:52.490
And when you,
the moment you click on that one,

00:32:52.490 --> 00:32:58.480
you will see that the
httpd-webobjects.conf

00:32:58.480 --> 00:33:02.020
file will be generated,
will be spirited out for you on the web

00:33:02.020 --> 00:33:08.380
page and you can copy and save it on to
local disk and do SCP and use that file.

00:33:11.700 --> 00:33:12.460
Thanks.

00:33:12.460 --> 00:33:16.750
There's also some other features in
the migration tab in Java Monitor,

00:33:16.750 --> 00:33:20.370
which is you can specify
a SSH identity file,

00:33:20.450 --> 00:33:22.920
and then it would actually
move the files for you.

00:33:22.970 --> 00:33:25.400
So it would make it even easier.

00:33:27.020 --> 00:33:30.790
So the last step, the fifth step,
is of course called Apache Start,

00:33:30.880 --> 00:33:33.740
or if you haven't started,
or Apache Graceful.

00:33:33.740 --> 00:33:39.640
So that's concluded with
Apache 2.2 Low Balancing Stories.

00:33:39.680 --> 00:33:44.400
Let me get some water.

00:33:48.560 --> 00:33:51.660
So, what next?

00:33:51.670 --> 00:33:54.260
We are probably talking about
load balancing strategy.

00:33:54.260 --> 00:33:55.810
What are the other load
balancing strategies?

00:33:55.910 --> 00:33:58.490
What about using hardware load balancer?

00:33:58.700 --> 00:34:05.610
Well, we are not going to talk about load
balancing using hardware load balancer,

00:34:05.900 --> 00:34:08.270
but it would certainly work.

00:34:10.100 --> 00:34:12.780
And so instead of talking
about hardware low balancer,

00:34:12.780 --> 00:34:16.100
we're going to talk about low
balancing with Tomcat's 5.5.

00:34:17.620 --> 00:34:21.820
So let's talk about it for a few minutes.

00:34:21.820 --> 00:34:24.280
So why are we doing this?

00:34:24.340 --> 00:34:27.670
Well, because we're

00:34:28.050 --> 00:34:31.000
That was people, so what is it?

00:34:31.000 --> 00:34:33.480
Blow balancing with Tomcat 5.5?

00:34:33.490 --> 00:34:37.480
Well, basically,
it is a filter that is set onto Tomcat,

00:34:37.500 --> 00:34:39.060
and it doesn't have to set onto Tomcat.

00:34:39.060 --> 00:34:41.260
It can set into any J2EE container.

00:34:41.320 --> 00:34:44.410
I knew that a certain customer
that was actually trying to

00:34:44.410 --> 00:34:49.100
server to your management and
using WebObjects for a J2EE shop,

00:34:49.120 --> 00:34:52.240
like if they use WebSphere or WebLogix,
and you just,

00:34:52.240 --> 00:34:56.510
because they don't know about the power
of WebObjects and how easy it's used,

00:34:56.720 --> 00:35:01.430
and you want to just develop one part of
the applications and to use WebObjects,

00:35:01.530 --> 00:35:03.360
yes, you can do that.

00:35:03.380 --> 00:35:05.780
We are doing this experiment,
experiments,

00:35:05.790 --> 00:35:10.100
and please tell us if you like this,
tell us that you like this,

00:35:10.190 --> 00:35:13.110
then we'll see what we can do.

00:35:13.590 --> 00:35:16.210
So one very important point
that my manager always

00:35:16.210 --> 00:35:21.500
reminds me during rehearsal,
which is make sure that we understand

00:35:21.500 --> 00:35:25.700
this node balancing adapter is not
the same thing as Surflets deployment.

00:35:25.700 --> 00:35:29.010
It is not the same thing as
you develop your application,

00:35:29.090 --> 00:35:32.000
you web your application
into a tool wall bundle,

00:35:32.300 --> 00:35:37.400
and then every draft framework
gets shoved into the bundle,

00:35:37.400 --> 00:35:40.460
and then the whole WebObjects
application is in a Surflets.

00:35:40.520 --> 00:35:43.930
No, that's not what we're
experimenting right there.

00:35:44.140 --> 00:35:50.140
So instead, we actually have a filter,
which will talk to different instances

00:35:50.140 --> 00:35:53.120
running outside of the container.

00:35:55.170 --> 00:35:57.240
So how do we do that?

00:35:57.270 --> 00:35:59.820
There's five steps,
very similar to doing load

00:35:59.890 --> 00:36:01.510
balancing with Apache.

00:36:01.700 --> 00:36:07.960
The first step is you need to use
SCP to copy your world bundle--

00:36:08.230 --> 00:36:11.680
war executable into your remote host.

00:36:12.050 --> 00:36:15.570
Step two,
you generate the launchd.plist files.

00:36:15.580 --> 00:36:20.390
And that's - and in Leopard,
I believe that in Leopard

00:36:20.390 --> 00:36:25.730
you can actually - how do you
generate this launchd.plist file?

00:36:25.730 --> 00:36:28.020
That's open source tool called link girl.

00:36:28.060 --> 00:36:30.440
You can use it with a
GUI and configure it.

00:36:30.440 --> 00:36:33.720
You can use a VI or you can
grab your WebObjects application

00:36:33.720 --> 00:36:35.740
instances and you do this.

00:36:35.750 --> 00:36:39.320
Let's say,
hollow world and then you do stashd,

00:36:39.320 --> 00:36:40.600
roll launchd enabled.

00:36:40.660 --> 00:36:47.260
It will - and if you launch it with
the appropriate access privileges,

00:36:47.330 --> 00:36:52.690
it will generate the file for you
and dump it in the correct place.

00:36:52.770 --> 00:36:56.270
So it will constantly elaborate,
but I believe that that portion of

00:36:56.270 --> 00:36:58.900
the code might be commented out.

00:36:59.500 --> 00:37:04.380
So the first step is that
you need to call launch,

00:37:04.380 --> 00:37:08.520
control, load the library,
and then launch the plist.

00:37:08.570 --> 00:37:13.470
The fourth step is that you need to
create a row-adapter-config.xml file

00:37:13.470 --> 00:37:20.530
so that the load balancer filter would
know how many instances were out there.

00:37:20.540 --> 00:37:23.760
Last step, you start your container.

00:37:23.760 --> 00:37:26.560
So with that, let's do a quick demo.

00:37:35.300 --> 00:37:52.450
Mankit Szeghi: Demo machine, please.

00:37:52.450 --> 00:37:52.450
Okay, so what do we get here?

00:37:52.450 --> 00:37:52.450
So we're going to show you how to
use the Tomcat adapter to do sticky

00:37:52.450 --> 00:37:52.450
session -- to do round robin load
balancing with sticky session support.

00:37:56.180 --> 00:38:00.290
So with that,
I would bring up three terminal

00:38:00.440 --> 00:38:05.890
for my three instances,
and then I would CD into that directory.

00:38:10.820 --> 00:38:13.030
and then I will launch them.

00:38:13.040 --> 00:38:15.080
Now watch this.

00:38:15.470 --> 00:38:20.710
Do you see that's a stashd
jSessionId equal to the number?

00:38:21.030 --> 00:38:23.340
And, okay,
let me open up another terminal

00:38:23.360 --> 00:38:24.960
and show you something.

00:38:25.110 --> 00:38:27.780
So that's here.

00:38:32.740 --> 00:38:36.600
So I am sitting into the library
WebObjects configuration folder and

00:38:36.600 --> 00:38:39.080
then I'm going to catch you this.

00:38:39.080 --> 00:38:39.400
You see that?

00:38:39.480 --> 00:38:40.400
That's an instance ID.

00:38:40.400 --> 00:38:43.200
When you launch your instances,
let me bring this up.

00:38:43.280 --> 00:38:47.730
So when you launch your instances,
stashd jsession ID up on the top

00:38:47.830 --> 00:38:53.140
screen equal to 1 must be matched
with your world configured .xml file.

00:38:53.280 --> 00:38:56.200
It says instance ID equal to 1.

00:38:56.230 --> 00:38:59.110
If you specify it as 1
and you specify it to 10,

00:38:59.110 --> 00:38:59.960
I'm sorry.

00:38:59.960 --> 00:39:01.780
It's not that smart.

00:39:01.780 --> 00:39:02.580
WebObjects is cool.

00:39:02.600 --> 00:39:03.470
But not that smart.

00:39:03.830 --> 00:39:05.130
That they can reach your mind.

00:39:05.170 --> 00:39:07.500
So you have to make sure that they match.

00:39:09.060 --> 00:39:11.180
Okay, let's create this.

00:39:11.180 --> 00:39:14.500
And then, now I'm starting one instance.

00:39:16.040 --> 00:39:23.830
Mankit Szeghi: Let's move it aside.

00:39:23.830 --> 00:39:23.830
And I'm starting the second instances.

00:39:23.830 --> 00:39:23.830
Move it aside.

00:39:23.830 --> 00:39:23.830
Now I'm going to start the--

00:39:24.280 --> 00:39:32.900
Mankit Szeghi: The third instance is,
move this aside,

00:39:32.900 --> 00:39:32.900
and after I start three instances,
it's starting.

00:39:34.610 --> 00:39:37.160
Okay,
so that's some networking issues here.

00:39:37.160 --> 00:39:40.000
So let's do logo.

00:39:47.100 --> 00:40:00.300
[Transcript missing]

00:40:11.210 --> 00:40:16.680
So as you see, I started Tomcat and see
now I try to access it,

00:40:16.680 --> 00:40:18.380
which is here.

00:40:19.360 --> 00:40:23.190
Now I started transaction and see
and take a look at I want you guys

00:40:23.230 --> 00:40:25.240
to pay attention at the upper.

00:40:25.320 --> 00:40:26.030
Let me bring it down.

00:40:26.030 --> 00:40:29.750
I want you to pay attention right here.

00:40:29.940 --> 00:40:30.660
Do you see that?

00:40:30.730 --> 00:40:32.860
J session ID equal to 2.

00:40:32.860 --> 00:40:35.400
And then when I click this,

00:40:35.800 --> 00:40:55.800
[Transcript missing]

00:40:56.360 --> 00:40:57.700
Now, updated a few times.

00:40:57.740 --> 00:40:58.380
It's doing one.

00:40:58.480 --> 00:41:02.010
So it's actually doing round robin
successfully with sticky session support.

00:41:02.700 --> 00:41:06.710
And that concludes my demo for Tomcat.

00:41:06.940 --> 00:41:07.480
Slide, please.

00:41:16.170 --> 00:41:21.240
So what next?

00:41:21.310 --> 00:41:24.390
The next thing is JMX.

00:41:24.610 --> 00:41:29.720
So then that's part of the
reason why we do Tomcat because

00:41:29.720 --> 00:41:35.610
Tomcat is Java is more close to,
Tomcat is Java and we have built

00:41:35.610 --> 00:41:41.250
in JMX to pop up WebObjects and
we can do a lot of interesting

00:41:41.990 --> 00:41:42.330
things if we move to Tomcat.

00:41:44.100 --> 00:41:48.150
Like such as using the advanced
class loading mechanisms to

00:41:48.450 --> 00:41:51.230
do some interesting things.

00:41:51.230 --> 00:41:52.830
We have a demo.

00:41:56.750 --> 00:42:01.680
So let's talk about
troubleshooting with JMX.

00:42:01.680 --> 00:42:04.370
So in JMX--

00:42:05.410 --> 00:42:10.210
Okay, in JMX we introduced some new API.

00:42:10.290 --> 00:42:13.470
The first new API we introduced
is the role application.

00:42:13.700 --> 00:42:15.860
In role application is register and bin.

00:42:15.990 --> 00:42:19.820
Why do we want to do that?

00:42:19.820 --> 00:42:19.820
Because

00:42:20.090 --> 00:42:27.380
Mankit Szeghi: Like in the past,
you used and used all these light

00:42:27.480 --> 00:42:32.170
bit things to see whether your
instances is healthy and running.

00:42:32.170 --> 00:42:33.960
And I believe that we
can do better than that.

00:42:34.070 --> 00:42:40.000
You can actually define your own
healthiness of your instances using JMX.

00:42:40.000 --> 00:42:43.040
You can specify how much
memories that your instances

00:42:43.040 --> 00:42:44.800
should have and stuff like that.

00:42:44.940 --> 00:42:48.200
And then you can track different users,
set up different conditions.

00:42:48.200 --> 00:42:52.330
If certain assertion fails and
you want to lock that transactions

00:42:52.330 --> 00:42:56.410
to JConsole and stuff like that,
we'll have a demo for that.

00:42:57.060 --> 00:43:01.360
And then the next API we
introduce is Unregistered Nbin.

00:43:01.410 --> 00:43:02.180
Yeah, obvious.

00:43:02.270 --> 00:43:04.580
You have register, you have unregister.

00:43:04.610 --> 00:43:08.130
And then here, get Nbin server.

00:43:08.410 --> 00:43:12.480
So every WebObjects application instances
would have a built-in ambient server.

00:43:12.550 --> 00:43:14.480
That means that you can
do some interesting thing.

00:43:14.640 --> 00:43:21.790
Let's say, for example,
if you have a web page,

00:43:21.790 --> 00:43:24.100
if you have web applications,
you have one page called...

00:43:24.300 --> 00:43:29.400
[Transcript missing]

00:43:29.700 --> 00:43:56.000
[Transcript missing]

00:43:56.630 --> 00:44:01.800
But that feature is not a part
of the Leopard preview disk.

00:44:01.850 --> 00:44:07.800
So another API we introduced is the
Get JMX Domain and Set JMX Domain.

00:44:07.870 --> 00:44:14.160
So let's have a demo with JMX.

00:44:17.400 --> 00:44:19.640
You can see.

00:44:19.650 --> 00:44:19.990
OK.

00:44:20.100 --> 00:44:24.080
So with that, we're going to start a
world JMX demo machine.

00:44:24.080 --> 00:44:27.360
And Chuck at Wall,
our intern who does cool photo

00:44:27.880 --> 00:44:32.680
viewer applications from yesterday,
is going to help us demo this.

00:44:32.760 --> 00:44:34.400
So to do this, take a look.

00:44:34.400 --> 00:44:37.980
I want you guys to pay attention at here.

00:44:38.080 --> 00:44:42.120
Stashd.com.sun.management.jmx.remote.ec
ode.show.

00:44:42.120 --> 00:44:45.420
If you're running Java 1.5,
you want to turn this on.

00:44:45.420 --> 00:44:50.920
If you're running Java JDK 1.4.2,
you want to turn this on.

00:44:51.040 --> 00:44:53.070
Plus,
you want to download an optional package

00:44:53.310 --> 00:44:56.630
from Sun so that you can use the JMX.

00:44:56.900 --> 00:45:01.680
And with Java 1.6, when they shift,
this flag is not less than Siri.

00:45:01.680 --> 00:45:04.900
So, Chuck, can you launch this for us?

00:45:08.800 --> 00:45:10.320
So the application is launching.

00:45:10.320 --> 00:45:10.680
OK.

00:45:10.820 --> 00:45:15.700
OK, we have some network problems here.

00:45:15.700 --> 00:45:19.380
OK, so what does this application does?

00:45:20.020 --> 00:45:22.330
This application is a simple application.

00:45:22.340 --> 00:45:24.630
It asks users for inputs.

00:45:24.890 --> 00:45:28.500
So it expects the numbers from 0 to 10.

00:45:28.730 --> 00:45:32.370
So Chuck, can you launch JConsole?

00:45:34.130 --> 00:45:39.390
So you see in JConsole,
you can actually do it locally, remotely,

00:45:39.390 --> 00:45:41.840
and that's in this case because
we're running on the same machine,

00:45:41.840 --> 00:45:42.660
let's do it locally.

00:45:42.660 --> 00:45:44.160
Can we click connect?

00:45:44.160 --> 00:45:48.340
Bottom.

00:45:48.340 --> 00:45:49.200
Lower.

00:45:49.200 --> 00:45:50.300
Yep, there you go.

00:45:59.000 --> 00:46:18.900
[Transcript missing]

00:46:19.590 --> 00:46:23.990
In this, you see there's a heap
space measurement thing,

00:46:23.990 --> 00:46:25.800
meter, and there's a non-heap space.

00:46:25.950 --> 00:46:29.680
With the new threading model and NIO,
you'll see non-heap space go up a lot,

00:46:29.690 --> 00:46:31.050
but not using as much heap space.

00:46:31.050 --> 00:46:33.440
And that's your application code,
use lots of heap memory.

00:46:33.440 --> 00:46:36.180
And then can you click on the threads?

00:46:36.260 --> 00:46:38.650
So here's the interesting thing.

00:46:38.980 --> 00:46:42.930
Now, in this pane,
you can show all the worker threads,

00:46:42.930 --> 00:46:44.540
what are they doing.

00:46:44.540 --> 00:46:47.080
And can you click on one of them?

00:46:48.890 --> 00:46:53.910
So as you see, like before JMX,
when your application hangs or does

00:46:53.910 --> 00:46:56.890
not respond as quick as you expect,
what do you do?

00:46:56.900 --> 00:46:59.910
You're just guessing what it does,
build lots of logs.

00:47:00.000 --> 00:47:03.580
But with JMX, you actually know exactly
what state it is on,

00:47:03.590 --> 00:47:05.350
and you have a stack trace.

00:47:05.400 --> 00:47:09.340
So next time when your
application has run into issues,

00:47:09.340 --> 00:47:13.470
you actually can fire up
JMX and look at all the threads,

00:47:13.720 --> 00:47:17.330
and then you will know whether this EOF,
Enterprise Objects Frameworks,

00:47:17.400 --> 00:47:20.190
is deadlocking on you,
or you have some problem with

00:47:20.190 --> 00:47:23.830
Apache adapter that they have
all this thing that's hanging

00:47:24.010 --> 00:47:25.910
all your WebObjects threads.

00:47:25.980 --> 00:47:29.650
So, okay, so class, can we go to class?

00:47:31.240 --> 00:47:35.170
So this will show you how many
classes you will load on your runtime,

00:47:35.170 --> 00:47:36.230
and let's talk about nbin.

00:47:36.240 --> 00:47:38.900
Things will get more interesting.

00:47:40.370 --> 00:47:43.040
So here we have three things.

00:47:43.070 --> 00:47:44.290
Thanks, make it bigger.

00:47:44.520 --> 00:47:46.900
So nothing seems to be exciting.

00:47:46.900 --> 00:47:49.460
So Chuck, can you go back to the
application please?

00:47:49.510 --> 00:47:53.350
Let's start of inputting
some good numbers like three,

00:47:53.450 --> 00:47:55.140
five or something.

00:47:55.650 --> 00:47:58.570
Okay,
we said that the user submitted a five,

00:47:58.640 --> 00:48:00.140
go back to JConsole.

00:48:00.170 --> 00:48:01.300
Nothing happened.

00:48:01.320 --> 00:48:04.090
Try now number seven, let's say.

00:48:05.520 --> 00:48:06.520
Okay, go back to JConsole.

00:48:06.730 --> 00:48:07.280
Nothing happened.

00:48:07.530 --> 00:48:09.910
Now, here's a user that's bad.

00:48:09.960 --> 00:48:14.220
Let's try click like,
give a number out of range.

00:48:14.220 --> 00:48:16.210
Let's say 11.

00:48:16.870 --> 00:48:18.240
And then look at the JConsole.

00:48:18.300 --> 00:48:21.520
Boom, you see it locked that as a
transaction coming with that host.

00:48:21.740 --> 00:48:23.960
Can we make this bigger, please?

00:48:25.360 --> 00:48:27.600
that it is actually
submits the number 11.

00:48:27.600 --> 00:48:31.000
So in this simple application,
we are trying to show you

00:48:31.000 --> 00:48:35.960
that with JamX we can do
conditionally registration.

00:48:35.960 --> 00:48:39.760
In this trivia example,
it illustrated how much you

00:48:39.850 --> 00:48:41.550
can do when it comes to EOF.

00:48:41.730 --> 00:48:46.400
Let's say if you have EOF applications
that you expect that you would

00:48:46.500 --> 00:48:51.440
only fetch like each user have
a default editing context in session

00:48:51.800 --> 00:48:58.430
and you expect that each user
should have only let's say 10 toys,

00:48:58.430 --> 00:48:59.360
right?

00:48:59.410 --> 00:49:03.280
But all of a sudden that the user
fetching all the toys in the database.

00:49:03.280 --> 00:49:08.160
Now, with JamX,
you can do this conditionally.

00:49:08.510 --> 00:49:14.160
You can only track the user that
trying to do something that you

00:49:14.160 --> 00:49:19.340
specify that is not expected behavior.

00:49:19.920 --> 00:49:24.780
and we encourage you to try this
out and that concludes the JMX demo.

00:49:24.800 --> 00:49:29.800
Slide please.

00:49:34.100 --> 00:49:37.380
Okay, oh,
there's one little slide missing.

00:49:37.380 --> 00:49:38.480
That's okay, I'll talk about it.

00:49:38.530 --> 00:49:41.580
So there's four new features that
we introduced in LibreServer.

00:49:41.580 --> 00:49:46.680
One, it's an NIO and the performance
enhancement and the new threading model.

00:49:46.680 --> 00:49:50.120
Two,
it's a load balancing with Apache 2.2

00:49:50.340 --> 00:49:53.160
adapter using Mod Proxy Balancer.

00:49:53.160 --> 00:49:56.760
Three,
it's an experimental Tomcat adapter.

00:49:56.760 --> 00:50:00.280
You can use it on
different J2E container.

00:50:00.280 --> 00:50:03.930
Again, as my manager always
reminds me to say this,

00:50:03.990 --> 00:50:04.910
make it explicit.

00:50:04.940 --> 00:50:09.700
It is not the same thing as bundling your
application into a true raw bundle and

00:50:09.700 --> 00:50:13.370
running the entire WebObjects application
as a server because it's heavyweight.

00:50:13.470 --> 00:50:17.600
It is running as a,
so with our experimental Tomcat adapter,

00:50:17.600 --> 00:50:21.580
it's actually a filter that's
living on a J2E container and then

00:50:21.580 --> 00:50:25.920
all your WebObjects application is
running outside of the container.

00:50:26.030 --> 00:50:29.600
So in terms of performance,
it would be very similar to what

00:50:29.600 --> 00:50:32.290
you get without using the container.

00:50:32.300 --> 00:50:36.380
The last feature, the fourth, the last,
a very important feature is the JMX.

00:50:36.700 --> 00:50:41.460
It is just a beginning and we plan
on having some other interesting

00:50:41.460 --> 00:50:43.620
JMX stuff like more standard ambience.