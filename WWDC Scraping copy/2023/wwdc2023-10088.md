# Wwdc2023 10088

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Create immersive Unity appsExplore how you can use Unity to create engaging and immersive experiences for visionOS. We'll share how Unity integrates seamlessly with Apple frameworks, take you through the tools you can use to build natively for the platform, and show you how volume cameras can bring your existing scenes into visionOS windows, volumes, and spaces. 

Discover how to incorporate visionOS features like passthrough and scene understanding, customize your visuals with Shader Graph, and adapt your interactions to work with spatial input.Chapters0:07 -Intro2:13 -Achieve your visual look6:18 -Play to device7:48 -Explore volume cameras10:00 -Build interaction13:49 -Prepare for the platform15:04 -Wrap-UpResourcesApply for the Unity betaHD VideoSD VideoRelated VideosWWDC23Bring your Unity VR app to a fully immersive spaceBuild great games for spatial computingGet started with building apps for spatial computing

Explore how you can use Unity to create engaging and immersive experiences for visionOS. We'll share how Unity integrates seamlessly with Apple frameworks, take you through the tools you can use to build natively for the platform, and show you how volume cameras can bring your existing scenes into visionOS windows, volumes, and spaces. 

Discover how to incorporate visionOS features like passthrough and scene understanding, customize your visuals with Shader Graph, and adapt your interactions to work with spatial input.

0:07 -Intro

2:13 -Achieve your visual look

6:18 -Play to device

7:48 -Explore volume cameras

10:00 -Build interaction

13:49 -Prepare for the platform

15:04 -Wrap-Up

Apply for the Unity beta

HD VideoSD Video

HD Video

SD Video

Bring your Unity VR app to a fully immersive space

Build great games for spatial computing

Get started with building apps for spatial computing

Search this video…♪ Mellow instrumental hip-hop ♪♪John Calsbeek: Welcome!I'm John, and I work on RealityKit.Vladimir Vukićević: And I'm Vlad, from Unity.John: I'm thrilled to introduce Unity supportfor immersive apps.Unity has worked with Apple to bringthe full Unity experience to this new platform.Unity is used by tens of thousands of apps,and now you can use Unity to build immersive apps.Triband brought their Apple Arcade title What The Golf?,which is built in Unity, to this platform.It's really fun to play on an iPhone,and it feels great to play it this way.There are two main approaches for creatingimmersive experiences on this platform with Unity.You can create experiences which mix your contentwith real-world objects using passthrough,either as an immersive experienceor in the Shared Space alongside other apps.You can also bring a fully immersive Unity experienceto the platform.If you're interested in this approach,I recommend you check out"Bring your Unity VR app to a fully immersive space."Creating experiences for the shared space with Unityopens up exciting opportunities for your apps.Here's Vlad to tell you more.Vladimir: Thanks, John.Unity and Apple have been working togetherfor the past two years to make sureyour Unity content looks great on the platform.Whether you're starting with an existing project,or building something completely new,Unity is a great tool for creating immersive experiencesusing familiar tools and some new capabilities.On this platform, you can achieve the visual look you wantusing Unity's shaders and materials.We're introducing the ability to enter play modedirectly to device, improving your iteration time.There's also a new concept called the volume camera,which controls how content from your Unity sceneis brought into the real world.Input on this new devicecan be as simple as a look-and-tap gestureor involve more complex interactions.And there are a few things you can do todayto prepare your Unity content for spatial computing.Here's an example of some of these elements working together.This scene uses materials built with Unity's Shader Graph,and it's being displayed in the shared spacein the Simulator with passthrough.There are fully rigged and animated characters,like the ogre in the back.Physics interactions work just like you're used to.All of the residents of this townare using character navigation to move around,and custom dynamic scripted behaviorsare used to make this scene feel alive.We put this together in two weeks with the helpof the Asset Store, and it looks greatwhen viewed in your space, where you can get up closeand look at a scene from any angle.All content in the shared space is rendered using RealityKit.Your Unity materials and shaders need to be translatedto this new environment.Unity has created PolySpatial, which takes careof this translation for you and brings many Unity featuresover to this environment.PolySpatial translates materials,regular and skinned mesh rendering,as well as particle effects and sprites.Unity simulation features are supported,and you continue to use MonoBehaviours,scriptable objects, and other standard tools.Three categories of materials are translated.They are the physically based materials, custom materials,and some special effect materials.Materials based on Unity's physically based shaderstranslate directly to RealityKit.If you're using the Universal Render Pipeline,you can use any of the Lit, Simple Lit,or Complex Lit Shaders in your materials.With the built-in pipeline, you can use the Standard Shader.All of these are translatedto a RealityKit PhysicallyBasedMaterial.Custom shader and material types are supportedthrough Unity Shader Graph.Unity Shader Graphs are converted to MaterialX,a standard interchange format for complex materials.MaterialX shaders become a ShaderGraphMaterialin RealityKit.Many Unity Shader Graph nodes are supported,so you can create complex and interesting effects.Handwritten shaders are not supportedfor rendering through RealityKit,but you can use them with RenderTextures in Unity.You can then use that RenderTextureas a texture input to a Shader Graphfor displaying through RealityKit.Two additional material shader types are supported.First is the Unlit Shader, which lets you create objectsthat take on a solid color or texture,unaffected by lighting.The second is the Occlusion Shader,which lets passthrough show through the object.You can use the Occlusion Shader with world mesh datato help your content feel more integrated with the real world.Unity MeshRenderers and SkinnedMeshRenderersare supported and are the primary wayof bringing visual content into real space.Rigged characters and animation are available.You can use either the Universal or the Built-in Render Pipeline,and your content will be translated to RealityKitvia Unity PolySpatial.Rendering features, such as postprocessing effectsand custom pipeline stages, are not available,as RealityKit performs the final rendering.Particle effects using Unity's Shuriken systemare either translated to RealityKit's particle system,if they are compatible,or are translated to baked meshes.Sprites become 3D meshes, though you should considerhow you're using them in a spatial context.PolySpatial works to optimize and translate renderingbetween Unity and RealityKit.Simulation features in Unity work just like you're used to,such as Physics, Animation and Timeline,Pathfinding and NavMesh, your custom MonoBehaviours,and other non-rendering features.To help you fine-tune your look and to speed up iteration,Unity PolySpatial enables "Play to device."It can take time to go through the build processto see what your content looks like on your device.With PolySpatial, you have access to Play to devicefor the first time.Play to device lets you see an instant previewof your scene and make live changes to it.It works in the simulator and works great on device as well.You can use Play to device to rapidly explorethe placement and size of your content,including adding and removing elements.You can change materials, textures,and even Shader Graphs to fine-tune your lookwhile seeing your content in place with passthrough.You can test out interactionbecause events are sent back to the editor.Your simulation continues to run,so it's easy to debug just by attaching to the editor.Here's the same castle scene you saw earlier.I have it open in Unity on the left,and with Play to device, I can see it runningin the simulator on the right.I can add more ogres just by dragging them into my scene.They're instantly visible in the simulator or device.If I want to see how pink or neon green ogres look, I can.Play to device is a really efficient workflowfor iterating on your content,and it's currently available in Unityonly for creating content in the shared space.Because you're using Unity to create volumetric contentthat participates in the shared space,a new concept called a volume camera lets you controlhow your scene is brought into the real world.A volume camera can create two types of volumes,bounded and unbounded, each with different characteristics.Your application can switch between the two at any time.Bounded volumes exist in the shared spaceas volumes, alongside other apps and games.They have dimensions and a transform in Unity,as well as a specific real-world size.They can be repositioned, but people cannot resize them.The dimensions and the transform of the volume cameradefine the region of your scene that your app will displayin a volume.They're specified in scene units.You can see a preview of the volume in greenin Unity's scene view.By manipulating the dimensions and the transformof the volume camera, different parts of the scenecan be brought into the volume.If I move or rotate the camera, new objects become visiblein my space.If I scale up its size, more of the scene comes into view.In both cases, the volume remains the same size.Only the content visible inside of it changes.Notice that in the initial placement of the volume camera,the spring intersects the side of the volume;content is clipped by RealityKit.If you will have content intersecting the edgesof your volume, consider placing the same meshin your scene a second time with a back-facing materialto fill in the clipped sections.Your unbounded volume displays in a full space on this platformand allows your content to fully blend with passthroughfor a more immersive experience.It has no dimensions because it selects your entire scene,and its transform specifies how your scene unitsare mapped to real-world units.There can only be one unbounded volume camera active at a time.You'll see an example of an unbounded volumewhen we talk about interaction.Unity supports multiple input types for apps on this platform.On this platform, people use their eyes and handsto look at content and tap their fingers together to select it.Full hand tracking as well as head pose datalets you create realistic interactions.Augmented reality data from ARKit is available,as are Bluetooth devicessuch as keyboards and game controllers.The tap gesture is the most common wayof interacting with content on this platform.In order for your objects to receive these events,they must have input colliders configured.You can look and tap to select an object from a distance,or you can reach out and directly touch an objectwith a finger.Up to two simultaneous tap actions can be in progress.In Unity, taps are available as WorldTouch events.They are similar to 2D tap events,but have a full 3D position.Hand and head pose tracking gives your applicationprecise information about each hand jointand the viewer's head positionrelative to the global tracking origin.Low-level hand data is provided via Unity's Hands package,and head pose is provided through the Input System.Both of these are available in unbounded volumes only,and accessing hand tracking requires your applicationto request permission to receive the data.Augmented reality data such as detected planes,the world mesh, and image markersare available through ARKit and Unity's AR Foundation.Like hands and head pose, AR data is only availablein unbounded volumes and requires extra permissions.Finally, Bluetooth devices such as keyboards, controllers,and other supported devices are availablefor you to access through Unity's Input System.Because some types of input are only availablein unbounded volumes, you'll need to decidewhat type of interaction you would like to build.Using look and tap will allow your content to workin a bounded volume that can livealongside other applications, but if you need accessto hand tracking or augmented reality data,you'll need to use an unbounded volumeand request permissions.Each of these is delivered to your Unity applicationvia an appropriate mechanism.This sample uses tapping, hand tracking,and plane detection in an unbounded volume scene.You can look at a surface found via ARKit plane detectionand create flowers by dragging your finger along it.The flowers are painted using hand tracking,and you can tap to grow flowers.Flowers that you grow react to hand movementusing Unity's physics system.By incorporating the real world into your content in this way,you can create a much deeper sense of immersion.The best way to adapt your existing interactionsdepends on the type.If you are already working with touch,such as on an iPhone,you can add appropriate input collidersand continue to use tap as your primary input mechanism.If you are using VR controllers, you will have to redefineyour interactions in terms of either tap or hand-based input,depending on how complex they are.Existing hand-based input should work without changes.And if you have existing UI panelsusing one of Unity's UI systems,you can bring them to this platform.User interface elements built using uGUI,as well as UI Toolkit, are supported.If you are using other UI systems,they will work as long as they use meshesand MeshRenderers or draw to a RenderTexture,which is then placed on a mesh.Support for spatial computing on Apple platformswill be coming soon in beta based on Unity 2022.However, you can start getting your content ready today.If you're starting a new project,use Unity 2022 or later.If you have an existing project, start upgrading to 2022.If you have handwritten shaders in your project,start converting them to Shader Graph.Consider adopting the Universal Render Pipeline.While the built-in graphics pipeline is supported,all future improvements will be on the Universal pipeline.If you haven't yet, start using the Input System package.Mixed-mode input is supported, but platform eventsare only delivered through the Input System.Finally, start thinking about how you can bringan existing app or game to spatial computing,or what new experience you'd like to create.Consider whether your idea fits in the shared spaceto give people more flexibility, or if your app needs the powerof a full space.To get more information about Unity's supportfor this platform,and to sign up for early beta access,please visit unity.com/spatial.I'm excited to see all the amazing thingsthat you'll create with Unity and this new device.John: Unity is an amazing way to hit the ground runningand build immersive apps.And it works great with RealityKit on this new platform.You can get started today preparing your projects.If you want to create a fully immersive experiencewith Unity, I recommend the session"Bring your Unity VR app to a fully immersive space."And don't miss "Build great games for spatial computing"to get an overview of game development technologiesfor this platform.We can't wait to see what you create.Vladimir: Thanks for watching.♪

♪ Mellow instrumental hip-hop ♪♪John Calsbeek: Welcome!I'm John, and I work on RealityKit.

Vladimir Vukićević: And I'm Vlad, from Unity.

John: I'm thrilled to introduce Unity supportfor immersive apps.

Unity has worked with Apple to bringthe full Unity experience to this new platform.

Unity is used by tens of thousands of apps,and now you can use Unity to build immersive apps.

Triband brought their Apple Arcade title What The Golf?,which is built in Unity, to this platform.

It's really fun to play on an iPhone,and it feels great to play it this way.

There are two main approaches for creatingimmersive experiences on this platform with Unity.

You can create experiences which mix your contentwith real-world objects using passthrough,either as an immersive experienceor in the Shared Space alongside other apps.

You can also bring a fully immersive Unity experienceto the platform.

If you're interested in this approach,I recommend you check out"Bring your Unity VR app to a fully immersive space."Creating experiences for the shared space with Unityopens up exciting opportunities for your apps.

Here's Vlad to tell you more.

Vladimir: Thanks, John.

Unity and Apple have been working togetherfor the past two years to make sureyour Unity content looks great on the platform.

Whether you're starting with an existing project,or building something completely new,Unity is a great tool for creating immersive experiencesusing familiar tools and some new capabilities.

On this platform, you can achieve the visual look you wantusing Unity's shaders and materials.

We're introducing the ability to enter play modedirectly to device, improving your iteration time.

There's also a new concept called the volume camera,which controls how content from your Unity sceneis brought into the real world.

Input on this new devicecan be as simple as a look-and-tap gestureor involve more complex interactions.

And there are a few things you can do todayto prepare your Unity content for spatial computing.

Here's an example of some of these elements working together.

This scene uses materials built with Unity's Shader Graph,and it's being displayed in the shared spacein the Simulator with passthrough.

There are fully rigged and animated characters,like the ogre in the back.

Physics interactions work just like you're used to.

All of the residents of this townare using character navigation to move around,and custom dynamic scripted behaviorsare used to make this scene feel alive.

We put this together in two weeks with the helpof the Asset Store, and it looks greatwhen viewed in your space, where you can get up closeand look at a scene from any angle.

All content in the shared space is rendered using RealityKit.

Your Unity materials and shaders need to be translatedto this new environment.

Unity has created PolySpatial, which takes careof this translation for you and brings many Unity featuresover to this environment.

PolySpatial translates materials,regular and skinned mesh rendering,as well as particle effects and sprites.

Unity simulation features are supported,and you continue to use MonoBehaviours,scriptable objects, and other standard tools.

Three categories of materials are translated.

They are the physically based materials, custom materials,and some special effect materials.

Materials based on Unity's physically based shaderstranslate directly to RealityKit.

If you're using the Universal Render Pipeline,you can use any of the Lit, Simple Lit,or Complex Lit Shaders in your materials.

With the built-in pipeline, you can use the Standard Shader.

All of these are translatedto a RealityKit PhysicallyBasedMaterial.

Custom shader and material types are supportedthrough Unity Shader Graph.

Unity Shader Graphs are converted to MaterialX,a standard interchange format for complex materials.

MaterialX shaders become a ShaderGraphMaterialin RealityKit.

Many Unity Shader Graph nodes are supported,so you can create complex and interesting effects.

Handwritten shaders are not supportedfor rendering through RealityKit,but you can use them with RenderTextures in Unity.

You can then use that RenderTextureas a texture input to a Shader Graphfor displaying through RealityKit.

Two additional material shader types are supported.

First is the Unlit Shader, which lets you create objectsthat take on a solid color or texture,unaffected by lighting.

The second is the Occlusion Shader,which lets passthrough show through the object.

You can use the Occlusion Shader with world mesh datato help your content feel more integrated with the real world.

Unity MeshRenderers and SkinnedMeshRenderersare supported and are the primary wayof bringing visual content into real space.

Rigged characters and animation are available.

You can use either the Universal or the Built-in Render Pipeline,and your content will be translated to RealityKitvia Unity PolySpatial.

Rendering features, such as postprocessing effectsand custom pipeline stages, are not available,as RealityKit performs the final rendering.

Particle effects using Unity's Shuriken systemare either translated to RealityKit's particle system,if they are compatible,or are translated to baked meshes.

Sprites become 3D meshes, though you should considerhow you're using them in a spatial context.

PolySpatial works to optimize and translate renderingbetween Unity and RealityKit.

Simulation features in Unity work just like you're used to,such as Physics, Animation and Timeline,Pathfinding and NavMesh, your custom MonoBehaviours,and other non-rendering features.

To help you fine-tune your look and to speed up iteration,Unity PolySpatial enables "Play to device."It can take time to go through the build processto see what your content looks like on your device.

With PolySpatial, you have access to Play to devicefor the first time.

Play to device lets you see an instant previewof your scene and make live changes to it.

It works in the simulator and works great on device as well.

You can use Play to device to rapidly explorethe placement and size of your content,including adding and removing elements.

You can change materials, textures,and even Shader Graphs to fine-tune your lookwhile seeing your content in place with passthrough.

You can test out interactionbecause events are sent back to the editor.

Your simulation continues to run,so it's easy to debug just by attaching to the editor.

Here's the same castle scene you saw earlier.

I have it open in Unity on the left,and with Play to device, I can see it runningin the simulator on the right.

I can add more ogres just by dragging them into my scene.

They're instantly visible in the simulator or device.

If I want to see how pink or neon green ogres look, I can.

Play to device is a really efficient workflowfor iterating on your content,and it's currently available in Unityonly for creating content in the shared space.

Because you're using Unity to create volumetric contentthat participates in the shared space,a new concept called a volume camera lets you controlhow your scene is brought into the real world.

A volume camera can create two types of volumes,bounded and unbounded, each with different characteristics.

Your application can switch between the two at any time.

Bounded volumes exist in the shared spaceas volumes, alongside other apps and games.

They have dimensions and a transform in Unity,as well as a specific real-world size.

They can be repositioned, but people cannot resize them.

The dimensions and the transform of the volume cameradefine the region of your scene that your app will displayin a volume.

They're specified in scene units.

You can see a preview of the volume in greenin Unity's scene view.

By manipulating the dimensions and the transformof the volume camera, different parts of the scenecan be brought into the volume.

If I move or rotate the camera, new objects become visiblein my space.

If I scale up its size, more of the scene comes into view.

In both cases, the volume remains the same size.

Only the content visible inside of it changes.

Notice that in the initial placement of the volume camera,the spring intersects the side of the volume;content is clipped by RealityKit.

If you will have content intersecting the edgesof your volume, consider placing the same meshin your scene a second time with a back-facing materialto fill in the clipped sections.

Your unbounded volume displays in a full space on this platformand allows your content to fully blend with passthroughfor a more immersive experience.

It has no dimensions because it selects your entire scene,and its transform specifies how your scene unitsare mapped to real-world units.

There can only be one unbounded volume camera active at a time.

You'll see an example of an unbounded volumewhen we talk about interaction.

Unity supports multiple input types for apps on this platform.

On this platform, people use their eyes and handsto look at content and tap their fingers together to select it.

Full hand tracking as well as head pose datalets you create realistic interactions.

Augmented reality data from ARKit is available,as are Bluetooth devicessuch as keyboards and game controllers.

The tap gesture is the most common wayof interacting with content on this platform.

In order for your objects to receive these events,they must have input colliders configured.

You can look and tap to select an object from a distance,or you can reach out and directly touch an objectwith a finger.

Up to two simultaneous tap actions can be in progress.

In Unity, taps are available as WorldTouch events.

They are similar to 2D tap events,but have a full 3D position.

Hand and head pose tracking gives your applicationprecise information about each hand jointand the viewer's head positionrelative to the global tracking origin.

Low-level hand data is provided via Unity's Hands package,and head pose is provided through the Input System.

Both of these are available in unbounded volumes only,and accessing hand tracking requires your applicationto request permission to receive the data.

Augmented reality data such as detected planes,the world mesh, and image markersare available through ARKit and Unity's AR Foundation.

Like hands and head pose, AR data is only availablein unbounded volumes and requires extra permissions.

Finally, Bluetooth devices such as keyboards, controllers,and other supported devices are availablefor you to access through Unity's Input System.

Because some types of input are only availablein unbounded volumes, you'll need to decidewhat type of interaction you would like to build.

Using look and tap will allow your content to workin a bounded volume that can livealongside other applications, but if you need accessto hand tracking or augmented reality data,you'll need to use an unbounded volumeand request permissions.

Each of these is delivered to your Unity applicationvia an appropriate mechanism.

This sample uses tapping, hand tracking,and plane detection in an unbounded volume scene.

You can look at a surface found via ARKit plane detectionand create flowers by dragging your finger along it.

The flowers are painted using hand tracking,and you can tap to grow flowers.

Flowers that you grow react to hand movementusing Unity's physics system.

By incorporating the real world into your content in this way,you can create a much deeper sense of immersion.

The best way to adapt your existing interactionsdepends on the type.

If you are already working with touch,such as on an iPhone,you can add appropriate input collidersand continue to use tap as your primary input mechanism.

If you are using VR controllers, you will have to redefineyour interactions in terms of either tap or hand-based input,depending on how complex they are.

Existing hand-based input should work without changes.

And if you have existing UI panelsusing one of Unity's UI systems,you can bring them to this platform.

User interface elements built using uGUI,as well as UI Toolkit, are supported.

If you are using other UI systems,they will work as long as they use meshesand MeshRenderers or draw to a RenderTexture,which is then placed on a mesh.

Support for spatial computing on Apple platformswill be coming soon in beta based on Unity 2022.

However, you can start getting your content ready today.

If you're starting a new project,use Unity 2022 or later.

If you have an existing project, start upgrading to 2022.

If you have handwritten shaders in your project,start converting them to Shader Graph.

Consider adopting the Universal Render Pipeline.

While the built-in graphics pipeline is supported,all future improvements will be on the Universal pipeline.

If you haven't yet, start using the Input System package.

Mixed-mode input is supported, but platform eventsare only delivered through the Input System.

Finally, start thinking about how you can bringan existing app or game to spatial computing,or what new experience you'd like to create.

Consider whether your idea fits in the shared spaceto give people more flexibility, or if your app needs the powerof a full space.

To get more information about Unity's supportfor this platform,and to sign up for early beta access,please visit unity.com/spatial.

I'm excited to see all the amazing thingsthat you'll create with Unity and this new device.

John: Unity is an amazing way to hit the ground runningand build immersive apps.

And it works great with RealityKit on this new platform.

You can get started today preparing your projects.

If you want to create a fully immersive experiencewith Unity, I recommend the session"Bring your Unity VR app to a fully immersive space."And don't miss "Build great games for spatial computing"to get an overview of game development technologiesfor this platform.

We can't wait to see what you create.

Vladimir: Thanks for watching.

♪

## Code Samples

