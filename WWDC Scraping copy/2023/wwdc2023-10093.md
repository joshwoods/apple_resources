# Wwdc2023 10093

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Bring your Unity VR app to a fully immersive spaceDiscover how you can bring your existing Unity VR apps and games to visionOS. We'll explore workflows that can help you get started and show you how to build for eyes and hands in your apps and games with the Unity Input System. Learn about Unity's XR Interaction Toolkit, tips for foveated rendering, and best practices.Chapters0:01 -Intro2:28 -Build and run workflow3:15 -Prepare your graphics5:56 -Input options13:43 -Wrap-UpResourcesApply for the Unity betaHD VideoSD VideoRelated VideosWWDC23Build great games for spatial computingCreate immersive Unity apps

Discover how you can bring your existing Unity VR apps and games to visionOS. We'll explore workflows that can help you get started and show you how to build for eyes and hands in your apps and games with the Unity Input System. Learn about Unity's XR Interaction Toolkit, tips for foveated rendering, and best practices.

0:01 -Intro

2:28 -Build and run workflow

3:15 -Prepare your graphics

5:56 -Input options

13:43 -Wrap-Up

Apply for the Unity beta

HD VideoSD Video

HD Video

SD Video

Build great games for spatial computing

Create immersive Unity apps

Search this video…♪ Mellow instrumental hip-hop ♪♪Christopher Figueroa: Hello, I'm Christopher Figueroafrom the ARKit team at Apple.Peter Kuhn: And I'm Peter Kuhn from Unity.Christopher: Unity has brought their engine and XR ecosystemto this new platform,making it simple for a Unity developer like yourselfto easily bring your project over.Peter and I will show you how to builda fully immersive experience, like Rec Room here,using Unity workflows you're already familiar with.You'll start by creating an Immersive Spacewith a Full Immersive style.This allows your app to hide passthroughand transport someone to another world.In a fully immersive experience,Unity utilizes Compositor Services,and gives your app the power of Metal rendering capabilities.Unity also takes advantage of ARKitto recognize your body position and the surroundings,including skeletal hand tracking.Unity builds upon these technologiesto provide the same services in the Unity Engine.There are two main approachesfor creating immersive experienceson this platform with Unity.You can bring a fully immersive Unity experienceto this platform, replacing the player'ssurroundings with your own environments.Or you can mix your content with passthroughto create immersive experiences that blend inwith their surroundings.If you're interested in the second approach,I recommend you check out "Create immersive Unity apps."Now, Peter will go over how these new Apple technologieshelp Unity developers bring fully immersive VR gamesto this platform.Peter: Thanks, Christopher.First, I wanted to start off by showing you Rec Roomfrom Against Gravity.It's a popular VR social platform that allows usersto create and play games and experiences with othersfrom around the world.It's built on the Unity game engine,which provides a powerful and flexible platformfor game development.I'll introduce you to some of those tools and technologiesthat will to make it easier for you to bring your VR contentto this new platform, just like Rec Room.I'll tell you about some things to keep in mind when planningto bring your Unity content to this new platform.First I'll cover the workflow you'll use to deploy contentfrom Unity to the device.There are a few things you'll need to keep in mindrelated to graphics on this platform.And finally, I'll talk about how to adapt controller inputsto hand input, and some of the toolsthat Unity provides to help with this transition.To start, there's a build and run workflowthat you should already be familiar with.We've built full support for this platform into Unity,so you can see your projects running on this devicein just a few steps.The first is to select the build target for this platform.Then, like you would for any other VR platform,enable the XR Plug-in.If your app relies on native plug-ins,they'll need to be recompiled for this platform.On the other hand, if you are usingraw source code or .mm files, you're already good to go.Building from Unity will now generate an Xcode project,just like it does for an iOS, Mac, or Apple TV target.Then, from within Xcode, you can build and runto either the device or the device simulatorfor faster iteration.The graphics pipeline you'll use to transformsomeone's surroundings into a fully immersive experienceis likely to be familiar to you as well.But there are a few new concepts that are importantto understand.One choice every project makes right in the beginningis which rendering pipeline to use.The Universal Render Pipeline is an ideal choice.It enables a special feature unique to this platformcalled Foveated Rendering.Foveated Rendering is a techniquethat concentrates more pixel density in the centerof each lens where the eyes are likely to be focused,and less detail on the peripherals of the screenwhere the eyes are less sensitive to detail.This results in a much higher-quality experiencefor the person using the device.When you use the Universal Render Pipeline,Static Foveated Rendering is appliedthroughout the entire pipeline.And it works with all URP features,including post-processing, camera stacking, HDR, and more.If you have custom render passes that would benefitfrom Foveated Rendering, there are new APIs in Unity 2022that can take advantage of this technology.Since rendering now happens in a nonlinear space,there are also shader macros to handle that remapping.Taking advantage of Static Foveated Renderingmeans you'll spend resources on the pixels that matterand produce a higher-quality visual experience.Another way to optimize your graphics on this platformis by using Single-Pass Instanced Rendering.In Unity, Single-Pass Instanced Renderingnow supports the Metal graphics API,and it will be enabled by default.With Single-Pass Instanced Rendering,the engine submits only one draw call for both eyes,and reduces the overhead of certain partsof the rendering pipeline like culling and shadows.This reduces the CPU overhead of rendering your scenes in stereo.The good news is, if your app already renders correctlyon other VR platforms using Single-Pass Instanced Rendering,shader macros ensure it should work here as well.There's one last thing to consider.Make sure your app is writing to the depth bufferfor every pixel correctly.The system compositor uses the depth buffer for reprojection.Wherever the depth information is missing,the system will render an error color as an indication.One example is the skybox which normally is infinitely far awayfrom the user, so it writes a depth of zero with reverse Z.This requires modification to appear on the device.We've fixed all of Unity's shadersto write correct values to the depth buffer,but if you have any custom effects such as a custom skybox,or perhaps a water effect or transparency effects,ensure that some value is written to depth for each pixel.Now that you've rendered your graphics to the device,it's time to make them interactive.Interaction on this device is unique.People will use their hands and their eyesto interact with content.There are a few ways you will be ableto add interaction to your Unity apps on this platform.The XR Interaction Toolkit adds hand trackingto make it easier for you to adapt existing projects.You can also react to the built-in system gestureswith the Unity Input System.And you can access the raw hand joint datafor custom interactions with the Unity Hands Package.The XR Interaction Toolkit, also known as XRI,provides a high-level interaction system.The toolkit is designed to make it easyto translate input into interactions.It works with both 3D and UI objects.XRI abstracts away the type of input,like hand tracking, and translates that inputinto actions that your app can respond to.This means your input code can work across platformsthat accept different types of input.XRI makes it easy to respond to common interactionslike hover, grab, and select, both in 3D spaceand in the UI for 3D spatial worlds.The toolkit also includes a locomotion systemso people can travel through a fully immersive spacemore comfortably.As people interact with your world,visual feedback is important for immersion.XRI enables you to define the visual reactionsfor each input constraint.The core of XRI is a set of base Interactableand Interactor components.Interactables are objects in your scenethat can receive input.You define Interactors, which specify how peoplecan interact with your Interactables.The Interaction Manager ties these components together.The first step is to decide which objects in the scenecan be interacted with, and how to reactwhen those interactions occur.We do this by adding an Interactable componentto the object.There are three built-in types.Simple marks the object as receiving interactions.You can subscribe to events like SelectEnteredor SelectExited with this component.With Grab, when the object is selected or grabbed,it will follow the Interactor aroundand inherit its velocity when released.Teleport interactables like TeleportArea and TeleportAnchorenable you to define areas or points for the playerto teleport to.And you can create your own custom Interactables.Interactors are responsible for selectingor interacting with the objects you've tagged as Interactable.They define a list of Interactablesthat they could potentially hover over or select each frame.There are several types of Interactors.Direct Interactors select Interactablesthat are touching it.You would use one of these when you want to knowwhen a person's hands touch an interactable object,or when they are close to interactable objects.Ray Interactors are used for interacting from far away.This Interactor is highly configurablewith curved and straight lines, and customizable visualizationsto help you adapt it to the visual style of your project.Once the user starts the interaction,you have options on how that interaction works.For example, if it's a grab interaction,you may want to move the object to the user's hand.And the Ray Interactor makes it possibleto limit the degrees of freedom for the grabin order to match your gameplay needs.A common interaction in a fully immersive experienceis grabbing an object and placing itsomewhere contextual to that object.For example, placing a battery in a socket.The Socket Interactor shows the playerthat a certain area can accept an object.These Interactors are not attached to the hands.Instead they live somewhere in the world.With hand tracking or even controllers,a common type of interaction that users naturally wantto perform is the poke interaction.This is similar to a direct Interactor,except that it includes direction filteringso that correct motion must be performedin order to trigger an interaction.If you want people to interact by looking,the Gaze Interactor provides some extensionsto the Ray Interactor to make gaze a bit easier to deal with.For example, Gaze Interactors can automaticallymake the colliders larger for Interactablesso that they're easier to select.To bring it all together, the Interaction Manager servesas a middleman between the Interactors and Interactables,facilitating the exchange of interactions.Its primary role is to initiate changesin the interaction state within a designated groupof registered Interactors and Interactables.Usually, a single Interaction Manager is establishedto enable the possibility of all Interactorsaffecting all Interactables.Alternatively, multiple complementaryInteraction Managers can be utilized,each with their own unique assortmentof Interactors and Interactables.These managers can be activated or deactivatedto enable or disable specific sets of interactions.For example, you may have a different setof Interactables per scene, or in your menus.Finally, the XR Controller component helps you make senseof the input data you'll receive.It takes input actions from the hands or a tracked deviceand passes it to the Interactors so that they can decideto select or activate something based on that input.You will need to bind Input Action Referencesfor each of the XR Interaction States, such as Select.You're not limited to just one XR Controller componentper hand or controller, which gives us the flexibilityto support both hands and controllers independently.Sample code that is bundled with XRIshows you how you can do this.In addition to the advanced features of XRI,you've also got the optionof simply using the system gesture inputsdirectly from the Unity Input System.You can then map the platform's built-in interactions,like tap gestures, to your own interaction system.You can use the binding paths from the Unity Input Systemto access and respond to these system gestures.The pinch gesture, for example, comes through as a valuewhen active, with position and rotation.These can be bound to input actions.Where the person is directing their focus comes throughin the same frame as a pinch gesture,with position and rotation.For even more flexibility, you can usethe Unity Hands Subsystem to accessall of the raw hand joint data from the systemthrough the Unity Hands Package.The Unity Hands Package provides accessto low-level hand joint datathat are consistent across platforms.For example, you can write code to look at each jointand determine how close the pose is to a certain gesture,like a thumbs up or a pointing index finger,and translate those into gameplay actions.This is powerful but can be challenging to get rightsince everyone's hands are different sizesand people have a variety of range of motions.This code defines a method which tells youif the index finger is extended.You can call this method from the OnHandUpdate eventand pass in one of the hands.First, get a few specific joints to checkif the index finger is extended.If any of them are invalid, it will return false.If all joints are valid, do a simple checkto make sure that the index finger isn't curled.You can extend this logic to other fingersto start to implement some basic gesture detections.Another use for the raw hand joint datais mapping it to a custom hand mesh visual.This can help make the hands fit moreinto the art style of your game.For example, Rec Room used the raw hand joint datato show a stylized hand model that fits their visual style.They also show other player hand models for more immersion.The Unity Hand package has some sample code to get you startedif you want to explore more about raw hand joint access.I'm excited to see your VR experiencescome to this new platform.To get more information about Unity's supportfor this platform and to sign up for early beta access,please visit unity.com/spatial.Christopher: Those are the tools you can useto bring a fully immersive VR experience to this new platformusing Unity workflows you're already familiar with.Peter: To recap, this session introduced youto some of the tools and technologiesthat will make it easier for you to bring your VR contentto this new platform, just like Rec Room.If you're starting a new project,use Unity 2022 or later.If you have an existing project,start upgrading to 2022.Consider adopting the Universal Render Pipeline.While the built-in graphics pipeline is supported,all future improvements will be on the Universal Pipeline.Start adapting any controller-based interactionsto hands.You can start today with the XR Interaction Toolkitand the Unity Hands package.Christopher: And finally, to learn more abouthow you can use Unityto create immersive experiences with passthrough,I recommend "Create immersive Unity apps."And check out "Build great games for spatial computing"to get an overview of what's possiblefor game developers on this platform.Peter: We're excited to see what you bring to the platform.Christopher: Thanks for watching.♪

♪ Mellow instrumental hip-hop ♪♪Christopher Figueroa: Hello, I'm Christopher Figueroafrom the ARKit team at Apple.Peter Kuhn: And I'm Peter Kuhn from Unity.Christopher: Unity has brought their engine and XR ecosystemto this new platform,making it simple for a Unity developer like yourselfto easily bring your project over.Peter and I will show you how to builda fully immersive experience, like Rec Room here,using Unity workflows you're already familiar with.You'll start by creating an Immersive Spacewith a Full Immersive style.This allows your app to hide passthroughand transport someone to another world.In a fully immersive experience,Unity utilizes Compositor Services,and gives your app the power of Metal rendering capabilities.Unity also takes advantage of ARKitto recognize your body position and the surroundings,including skeletal hand tracking.Unity builds upon these technologiesto provide the same services in the Unity Engine.There are two main approachesfor creating immersive experienceson this platform with Unity.You can bring a fully immersive Unity experienceto this platform, replacing the player'ssurroundings with your own environments.Or you can mix your content with passthroughto create immersive experiences that blend inwith their surroundings.If you're interested in the second approach,I recommend you check out "Create immersive Unity apps."Now, Peter will go over how these new Apple technologieshelp Unity developers bring fully immersive VR gamesto this platform.Peter: Thanks, Christopher.First, I wanted to start off by showing you Rec Roomfrom Against Gravity.It's a popular VR social platform that allows usersto create and play games and experiences with othersfrom around the world.It's built on the Unity game engine,which provides a powerful and flexible platformfor game development.I'll introduce you to some of those tools and technologiesthat will to make it easier for you to bring your VR contentto this new platform, just like Rec Room.I'll tell you about some things to keep in mind when planningto bring your Unity content to this new platform.First I'll cover the workflow you'll use to deploy contentfrom Unity to the device.There are a few things you'll need to keep in mindrelated to graphics on this platform.And finally, I'll talk about how to adapt controller inputsto hand input, and some of the toolsthat Unity provides to help with this transition.To start, there's a build and run workflowthat you should already be familiar with.We've built full support for this platform into Unity,so you can see your projects running on this devicein just a few steps.The first is to select the build target for this platform.Then, like you would for any other VR platform,enable the XR Plug-in.If your app relies on native plug-ins,they'll need to be recompiled for this platform.On the other hand, if you are usingraw source code or .mm files, you're already good to go.Building from Unity will now generate an Xcode project,just like it does for an iOS, Mac, or Apple TV target.Then, from within Xcode, you can build and runto either the device or the device simulatorfor faster iteration.The graphics pipeline you'll use to transformsomeone's surroundings into a fully immersive experienceis likely to be familiar to you as well.But there are a few new concepts that are importantto understand.One choice every project makes right in the beginningis which rendering pipeline to use.The Universal Render Pipeline is an ideal choice.It enables a special feature unique to this platformcalled Foveated Rendering.

Foveated Rendering is a techniquethat concentrates more pixel density in the centerof each lens where the eyes are likely to be focused,and less detail on the peripherals of the screenwhere the eyes are less sensitive to detail.This results in a much higher-quality experiencefor the person using the device.When you use the Universal Render Pipeline,Static Foveated Rendering is appliedthroughout the entire pipeline.And it works with all URP features,including post-processing, camera stacking, HDR, and more.If you have custom render passes that would benefitfrom Foveated Rendering, there are new APIs in Unity 2022that can take advantage of this technology.Since rendering now happens in a nonlinear space,there are also shader macros to handle that remapping.Taking advantage of Static Foveated Renderingmeans you'll spend resources on the pixels that matterand produce a higher-quality visual experience.Another way to optimize your graphics on this platformis by using Single-Pass Instanced Rendering.In Unity, Single-Pass Instanced Renderingnow supports the Metal graphics API,and it will be enabled by default.With Single-Pass Instanced Rendering,the engine submits only one draw call for both eyes,and reduces the overhead of certain partsof the rendering pipeline like culling and shadows.This reduces the CPU overhead of rendering your scenes in stereo.The good news is, if your app already renders correctlyon other VR platforms using Single-Pass Instanced Rendering,shader macros ensure it should work here as well.There's one last thing to consider.Make sure your app is writing to the depth bufferfor every pixel correctly.The system compositor uses the depth buffer for reprojection.Wherever the depth information is missing,the system will render an error color as an indication.One example is the skybox which normally is infinitely far awayfrom the user, so it writes a depth of zero with reverse Z.This requires modification to appear on the device.We've fixed all of Unity's shadersto write correct values to the depth buffer,but if you have any custom effects such as a custom skybox,or perhaps a water effect or transparency effects,ensure that some value is written to depth for each pixel.Now that you've rendered your graphics to the device,it's time to make them interactive.Interaction on this device is unique.People will use their hands and their eyesto interact with content.There are a few ways you will be ableto add interaction to your Unity apps on this platform.The XR Interaction Toolkit adds hand trackingto make it easier for you to adapt existing projects.You can also react to the built-in system gestureswith the Unity Input System.And you can access the raw hand joint datafor custom interactions with the Unity Hands Package.The XR Interaction Toolkit, also known as XRI,provides a high-level interaction system.The toolkit is designed to make it easyto translate input into interactions.It works with both 3D and UI objects.XRI abstracts away the type of input,like hand tracking, and translates that inputinto actions that your app can respond to.This means your input code can work across platformsthat accept different types of input.XRI makes it easy to respond to common interactionslike hover, grab, and select, both in 3D spaceand in the UI for 3D spatial worlds.The toolkit also includes a locomotion systemso people can travel through a fully immersive spacemore comfortably.As people interact with your world,visual feedback is important for immersion.XRI enables you to define the visual reactionsfor each input constraint.The core of XRI is a set of base Interactableand Interactor components.Interactables are objects in your scenethat can receive input.You define Interactors, which specify how peoplecan interact with your Interactables.The Interaction Manager ties these components together.The first step is to decide which objects in the scenecan be interacted with, and how to reactwhen those interactions occur.We do this by adding an Interactable componentto the object.There are three built-in types.Simple marks the object as receiving interactions.You can subscribe to events like SelectEnteredor SelectExited with this component.With Grab, when the object is selected or grabbed,it will follow the Interactor aroundand inherit its velocity when released.Teleport interactables like TeleportArea and TeleportAnchorenable you to define areas or points for the playerto teleport to.And you can create your own custom Interactables.Interactors are responsible for selectingor interacting with the objects you've tagged as Interactable.They define a list of Interactablesthat they could potentially hover over or select each frame.There are several types of Interactors.Direct Interactors select Interactablesthat are touching it.You would use one of these when you want to knowwhen a person's hands touch an interactable object,or when they are close to interactable objects.Ray Interactors are used for interacting from far away.This Interactor is highly configurablewith curved and straight lines, and customizable visualizationsto help you adapt it to the visual style of your project.Once the user starts the interaction,you have options on how that interaction works.For example, if it's a grab interaction,you may want to move the object to the user's hand.And the Ray Interactor makes it possibleto limit the degrees of freedom for the grabin order to match your gameplay needs.A common interaction in a fully immersive experienceis grabbing an object and placing itsomewhere contextual to that object.For example, placing a battery in a socket.The Socket Interactor shows the playerthat a certain area can accept an object.These Interactors are not attached to the hands.Instead they live somewhere in the world.With hand tracking or even controllers,a common type of interaction that users naturally wantto perform is the poke interaction.This is similar to a direct Interactor,except that it includes direction filteringso that correct motion must be performedin order to trigger an interaction.If you want people to interact by looking,the Gaze Interactor provides some extensionsto the Ray Interactor to make gaze a bit easier to deal with.For example, Gaze Interactors can automaticallymake the colliders larger for Interactablesso that they're easier to select.To bring it all together, the Interaction Manager servesas a middleman between the Interactors and Interactables,facilitating the exchange of interactions.Its primary role is to initiate changesin the interaction state within a designated groupof registered Interactors and Interactables.Usually, a single Interaction Manager is establishedto enable the possibility of all Interactorsaffecting all Interactables.Alternatively, multiple complementaryInteraction Managers can be utilized,each with their own unique assortmentof Interactors and Interactables.These managers can be activated or deactivatedto enable or disable specific sets of interactions.For example, you may have a different setof Interactables per scene, or in your menus.Finally, the XR Controller component helps you make senseof the input data you'll receive.It takes input actions from the hands or a tracked deviceand passes it to the Interactors so that they can decideto select or activate something based on that input.

You will need to bind Input Action Referencesfor each of the XR Interaction States, such as Select.You're not limited to just one XR Controller componentper hand or controller, which gives us the flexibilityto support both hands and controllers independently.Sample code that is bundled with XRIshows you how you can do this.In addition to the advanced features of XRI,you've also got the optionof simply using the system gesture inputsdirectly from the Unity Input System.You can then map the platform's built-in interactions,like tap gestures, to your own interaction system.You can use the binding paths from the Unity Input Systemto access and respond to these system gestures.The pinch gesture, for example, comes through as a valuewhen active, with position and rotation.These can be bound to input actions.Where the person is directing their focus comes throughin the same frame as a pinch gesture,with position and rotation.For even more flexibility, you can usethe Unity Hands Subsystem to accessall of the raw hand joint data from the systemthrough the Unity Hands Package.The Unity Hands Package provides accessto low-level hand joint datathat are consistent across platforms.For example, you can write code to look at each jointand determine how close the pose is to a certain gesture,like a thumbs up or a pointing index finger,and translate those into gameplay actions.This is powerful but can be challenging to get rightsince everyone's hands are different sizesand people have a variety of range of motions.This code defines a method which tells youif the index finger is extended.You can call this method from the OnHandUpdate eventand pass in one of the hands.First, get a few specific joints to checkif the index finger is extended.If any of them are invalid, it will return false.If all joints are valid, do a simple checkto make sure that the index finger isn't curled.You can extend this logic to other fingersto start to implement some basic gesture detections.Another use for the raw hand joint datais mapping it to a custom hand mesh visual.This can help make the hands fit moreinto the art style of your game.For example, Rec Room used the raw hand joint datato show a stylized hand model that fits their visual style.They also show other player hand models for more immersion.The Unity Hand package has some sample code to get you startedif you want to explore more about raw hand joint access.I'm excited to see your VR experiencescome to this new platform.To get more information about Unity's supportfor this platform and to sign up for early beta access,please visit unity.com/spatial.Christopher: Those are the tools you can useto bring a fully immersive VR experience to this new platformusing Unity workflows you're already familiar with.Peter: To recap, this session introduced youto some of the tools and technologiesthat will make it easier for you to bring your VR contentto this new platform, just like Rec Room.If you're starting a new project,use Unity 2022 or later.If you have an existing project,start upgrading to 2022.Consider adopting the Universal Render Pipeline.While the built-in graphics pipeline is supported,all future improvements will be on the Universal Pipeline.Start adapting any controller-based interactionsto hands.You can start today with the XR Interaction Toolkitand the Unity Hands package.Christopher: And finally, to learn more abouthow you can use Unityto create immersive experiences with passthrough,I recommend "Create immersive Unity apps."And check out "Build great games for spatial computing"to get an overview of what's possiblefor game developers on this platform.Peter: We're excited to see what you bring to the platform.Christopher: Thanks for watching.♪

12:46 -Translate raw joints into gameplay actions

## Code Samples

```swift
// Translate raw joints into gameplay actions



static
 
bool
 
IsIndexExtended
(
XRHand hand
)

{
    
if
 (!(hand.GetJoint(XRHandJointID.Wrist).TryGetPose(
out
 
var
 wristPose) &&
          hand.GetJoint(XRHandJointID.IndexTip).TryGetPose(
out
 
var
 tipPose) &&
          hand.GetJoint(XRHandJointID.IndexIntermediate).TryGetPose(
out
 
var
 intermediatePose)))
    {
        
return
 
false
;
    }

    
var
 wristToTip = tipPose.position - wristPose.position;
    
var
 wristToIntermediate = intermediatePose.position - wristPose.position;
    
return
 wristToTip.sqrMagnitude > wristToIntermediate.sqrMagnitude;
}
```

