# Wwdc2023 10091

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Evolve your ARKit app for spatial experiencesDiscover how you can bring your app's AR experience to visionOS. Learn how ARKit and RealityKit have evolved for spatial computing: We'll highlight conceptual and API changes for those coming from iPadOS and iOS and guide you to sessions with more details to help you bring your AR experience to this platform.ResourcesHD VideoSD VideoRelated VideosWWDC23Build spatial experiences with RealityKitMeet ARKit for spatial computingMeet Reality Composer ProRun your iPad and iPhone apps in the Shared Space

Discover how you can bring your app's AR experience to visionOS. Learn how ARKit and RealityKit have evolved for spatial computing: We'll highlight conceptual and API changes for those coming from iPadOS and iOS and guide you to sessions with more details to help you bring your AR experience to this platform.

HD VideoSD Video

HD Video

SD Video

Build spatial experiences with RealityKit

Meet ARKit for spatial computing

Meet Reality Composer Pro

Run your iPad and iPhone apps in the Shared Space

Search this video…♪ Mellow instrumental hip-hop ♪♪Omid Khalili: Hello! My name is Omid.Oliver and I are engineers on the ARKit teamand we are thrilled to review the concepts --some familiar and some new --that you'll need know about when bringing your iOS AR appto our new platform.ARKit was introduced on iOS in 2017and with it, we introduced three key conceptsto building augmented reality applications.With world tracking,ARKit is able to track your device's position in the worldwith six degrees of freedom.This allows anchoring virtual contentwith a position and orientation to the real world.Scene understanding provides insightabout the real world around you.Using the provided geometry and semantic knowledge,your content can be intelligently placedand realistically interact with surroundings.Finally, rendering engines can correctly register and compositeyour virtual content over captured imagesutilizing camera transformsand intrinsics provided by ARKit.Initially, we started with a SceneKit viewto use ARKit's camera transformsand render 3D content on iOS.We then introduced RealityKit,laying out the foundation for an enginecapable of highly realistic physically-based renderingand accurate object simulation with your surroundings.To enable spatial computing,ARKit and RealityKit have maturedand are deeply integrated into the operating system.For example, ARKit's tracking and Scene Understandingare now running as system services,backing everything from window placementto spatial audio.The system takes on responsibilitiesthat used to belong to applications.Camera pass-through and matting of the user's handsare now built-in,so your application gets these capabilities for free.Another built-in capability is that ARKit world mapsare continuously persisted by a system service,so your application doesn't need to do it anymore.We believe that this will free you upto focus on building the best application and content possiblefor this platform.Here is an example demonstrating these capabilities,along with new ones introduced with this platform.For example, ARKit now provides hand tracking to your app,which allows people to reach outand directly interact with virtual contentthat can then interact with their surroundings.In order to take advantage of all the new capabilitiesand immersive experiences this new platform offers,you'll need to update your iOS ARKit-based experience.This is a great opportunity to reimagine your appand AR experience for spatial computing.As a part of this transition,you'll be using familiar conceptsthat we've introduced with ARKit and RealityKit.We're going to cover how these concepts have carried over,how they've evolved,and how you can take advantage of them.Let's get started!First, we'll explore some new ways you can present your appfor spatial computing,and introduce new content tools available to you.Next, we will talk about Reality Kit,which is the engine to use to renderand interact with your content.We'll see how RealityView lets your appleverage spatial computing similar to ARView on iOS.Then, we'll talk about the different waysyour app can bring content into people's surroundings.Raycasting is something many iOS applications useto place content.We'll show an example of how to combine ARKit dataand RealityKit to enable raycastingfor spatial computing.And finally, we'll review the updates to ARKitand see the new ways to utilize familiar concepts from iOS.Lets get into preparing to migrate your experiencefor spatial computing.Spatial computing allows you to take your iOS AR experienceand expand it beyond the window.This platform offers new ways to present your applicationthat you'll want to consideras you bring your iOS experience over.Here is an example from our Hello World sample app.You can now display UI,including windows and three-dimensional content,anywhere around you.By default, applications on this platformlaunch into the Shared Space.The Shared Space is where apps exist side by side,much like multiple apps on a Mac desktop.Inside a Shared Space, your app can open one or more windowsto display content.Additionally, your app can createa three-dimensional volume.For example, now you can show a list of available board gamesin one window, the rules in another,and open the selected game in its own volume.The game can be played while keeping a Safari window opento read up on winning strategies.The content you add to the window and volumestays contained within its boundsto allow sharing the space with other applications.In some cases, you may want your app to have more controlover the level of immersion in your experience --maybe to play a game that interacts with your room.For this, your app can open a dedicated Full Spacein which only your app's windows,volumes, and 3D objects appear.Once in a Full Space,your application has access to more features.Using RealityKit's anchor entities,you can target and attach objects to the surroundingslike tables, floors,and even parts of your hands like the palm or wrist.Anchor entities work without requiring user permission.ARKit data is something elseyour app can only access in a Full Space.With permission, ARKit will provide dataabout the real-world surfaces, scene geometry,and skeletal hand tracking,expanding your app's ability for realistic physicsand natural interactions.Windows, volumes, and spaces are all SwiftUI scene types.There's so much more for you to learn about these.For starters, you can go to the session mentioned here.Next, let's review the main steps needed to prepareyour content for bringing it to spatial computing.Memorable AR experiences on iOS begin with great 3D content;the same is true for spatial experiences on this platform.And when it comes to 3D content,it's great to rely on an open standard likeUniversal Scene Description, or USD for short.USD is production proven,and scales from creators making single assetsto large studios working on AAA games and films.Apple was an early adopter of USD,adding it to our platforms in 2017and growing support since.Today, USD is at the heart of 3D contentfor spatial computing.With USD assets ready, you can bring them intoour new developer tool, Reality Composer Pro,to compose, edit, and preview your 3D content.If you're using CustomMaterials for your 3D content on iOS,then you will need to rebuild them using its shader graph.You also have the ability to edit your RealityKit componentsdirectly through the UI.And finally, you can import your Reality Composer Pro projectdirectly into Xcode,allowing you to easily bundle all of your USD assets,materials, and custom components into your Xcode project.We have some great sessionsto help you learn more about Reality Composer Proand how to build your own custom materials for spatial computing.Now that we've seen the different waysto present your application,let's learn more about the features RealityView offersas you bring your experience over.We just saw how spatial computingallows apps to display content in your space.One of the key differences coming from iOSis how different elements can be presented side by side.Notice how your 3D content and 2D elements can appearand work along side each other.Coming from iOS, you're going to usefamiliar frameworks to create each of these.You'll use SwiftUI to build the best 2D UIand get system gestures events like the ones on iOS.And you'll use RealityKit to render your 3D contentfor spatial experiences.The way to interface with both of these at the same timeis through RealityView -a new SwiftUI view that we're introducingto cater to the unique needs of spatial computing.RealityView truly bridges SwiftUI and RealityKit,allowing you to combine 2D and 3D elementsand create a memorable spatial experience.You'll be using the RealityView to hold all the entitiesyou wish to display and interact with.You can get gesture events and connect themto the entities in your view to control them.And with access to ARKit's scene understanding,you can enable realistic simulationswith people's surroundings and even their handsusing RealityKit's collision components.Before we look at how using RealityKitcarries over from iOS, let's do a quick refresheron how to work with RealityKit's Entity Component System.In the Reality Kit Entity Component System,each entity is a container for 3D content.Different components are added to an entityto define its look and behavior.This can include a model componentfor how it should render;a collision component,for how it can collide with other entities;and many more.You can use RealityComposer Proto prepare RealityKit components like collision componentsand get them added to your entities.Systems contain code to act on entitiesthat have the required components.For example, the system required for gesture supportonly operates on entities that have a CollisionComponentand InputTargetComponent.A lot of the concepts used by RealityViewfor spatial computing carry over from those of ARView on iOS.Lets see how these two stack up.Both views are event-aware containersto hold the entities you wish to display in your app.You can add Gesture Support to your viewsto enable selection and interaction with entities.With SwiftUI for spatial computing,you can reach out to select or drag your entities.Both ARView and RealityViewprovide a collection of your entities.ARView uses a Scene for this.RealityView has a Content to add your entities to.You can add AnchorEntities to them,allowing you to anchor your content to the real world.On both platforms, you create an entityto load your content model and an AnchorEntity to place it.One main difference between the platformsis in the behavior of anchor entities.ARView on iOS uses an ARSessionand your app must receive permissionto run scene understanding algorithmsnecessary for anchor entities to work.RealityView is using System Servicesto enable anchorEntities.This means that spatial experiencescan anchor content to your surroundingswithout requiring permissions.Apps using this approach do not receivethe underlying scene understanding dataor transforms.Not having transform data for your app to place contenthas some implications that Oliver will talk about laterin his section.As we've seen, there are many familiar conceptsthat carry over coming from iOS,but there are also new capabilitiesthat RealityKit provides for spatial computing.We've only scratched the surface of what's possiblewith RealityKit on this new platform,and you may want to check out the session belowto follow up on more.Now over to Oliver, who will talk more about RealityViewand how to bring in your content from iOS.Oliver Dunkley: Thanks, Omid!Let's continue by exploring the different waysyou can bring in your existing contentto spatial computing.Lets start in the Shared Space.We can add 3D content to a window or volume,and use system gestures to interact with it.To display your assets, you just add them directlyto RealityView's Content.You do this by creating an entityto hold your model component and position itby setting the transform component.You can also set up gesture supportto modify the transform component.Note that all entities added to the view's contentexist in the same space relative to the space's originand can therefore interact with one another.In the Shared Space,content cannot be anchored to your surroundings.Let's consider our optionsif we transition our app to a Full Space.One of the key differences coming from the Shared Spaceis that apps can now additionally anchor contentto people's surroundings.Anchoring your content here can happen in two ways.Lets first look at using RealityKit's AnchorEntityto place content without requiring permissionsto use ARKit data in your app.RealityKit's AnchorEntitiesallow you to specify a target for the system to findand automatically anchor your content to.So for example, in order to place a 3D modelon a table surface in front of you,you can use a RealityKit AnchorEntitywith a target set to table.Different from iOS, AnchorEntities can be usedwithout having to prompt for user permissions.People's privacy is preserved by not sharingthe underlying transforms of the AnchorEntitywith your application.Note: this implies children of different anchor entitiesare not aware of one another.New to anchorEntities, you can target hands,which opens up a whole new realmof interesting interaction opportunities.For example, you could anchor content to a person's palmand have it follow their hands as they move them.This is all done by the system,without telling your app where the persons hands actually are.AnchorEntitys provide a quick,privacy-friendly way for your appto anchor content to people's surroundings.Coming back to a Full Space,we can also leverage ARKit to incorporatesystem-level knowledge of the people's surroundings.This enables you to build your own custom placement logic.Let's take a look at how this works.Similar to iOS, your application receives anchoring updatesfor scene understanding data.You can integrate this anchor data into your app logicto achieve all sorts of amazing experiences.For example, you could use the bounds of a planeto center and distribute your content onto.Or, you could use planes and their classificationsto find the corner of a roomby looking for the intersection of two walls and a floor.Once you've decided where to place your content,you add a world anchor for ARKit to trackand use it to update your entity's transform component.This not only allows your contentto remain anchored to the real world,as the underlying world map is updated,but it also opens the door to anchor persistence,which we will explore shortly.All the entities added to your spacecan interact with one another as well as with the surroundings.This all works because scene understanding anchorsare delivered with transforms relative to the space's origin.User Permission is required to use ARKit capabilities.You just saw how integrating ARKit data into your app logiccan enable more advanced features.So far we have talked about letting your app place content.Let's explore how we can let people guide placement.On iOS, you can use raycastingto translate 2D input to a 3D position.But with this new platform,we don't need this 2D-3D bridge anymore,as we can use handsto naturally interact with experiences directly.Raycasting remains powerful;it lets people reach out beyond arms length.There are various ways to set up raycasting.Fundamentally, you need to setup RealityKit'scollision components to raycast against.Collision components can also be createdfrom ARKit's mesh anchorsto raycast against people's surroundings.Let's explore two examples of how to raycastfor spatial computing:first, using system gestures,and the second using hands data.After obtaining a position,we can place an ARKit worldAnchorto keep our content anchored.Let's consider the following example.Imagine our app revolves aroundplacing inspirational 3D assets for modelers.Maybe in this particular scenario,a person wants to use our app to place a virtual shipon their workbench for some modeling project.Here is our workbench we want to place our ship on.We'll start with an empty RealityView.ARKit's scene understanding provides mesh anchorsthat we'll use to represent the surroundings.They provide geometry and semantic information we can use.Remember that meshes for scene reconstruction dataare delivered as a series of chunks.We'll create an entity to represent this mesh chunk,and we'll correctly place this entity in a full spaceusing the mesh anchor's transform.Our entity then needs a collision componentto hit test against.We'll use RealityKit's ShapeResources methodto generate a collision shape from the meshAnchorfor our entity.We'll then add our correctly placed entitywhich supports hit testing.We'll build an entity and collision componentfor each mesh chunk we receiveto represent all the surroundings.As scene reconstruction is refined,we may get updates to meshes or have chunks removed.We should be ready to update our entitieson these changes as well.We now have a collection of entitiesrepresenting the surroundings.All these entities have collision componentsand can support a raycast test.Let's first explore raycasting using system gestures,and then continue the example using hands data.We can raycast and get a position to place our shipusing system gestures.Gestures can only interact with entitiesthat have both Collision and InputTarget components,so we add one to each of our mesh entities.By adding a SpatialTapGesture to the RealityView,people can raycast by looking at entities and tapping.This resulting event holds a position in world spacerepresenting the place people looked at when tapping.Instead of using system gestures,we could also have used ARKit's hand anchorsto build a ray.Lets take a step back and explore this option.To know where people point,we first need a representation of the person's hand.ARkit's new hand anchors gives us everything we need.We can use finger joint information to build the originand direction of the ray for our query.Now that we have the origin and direction of our ray,we can do a raycast against the entities in our scene.The resulting CollisionCastHitprovides the entity that was hit,along with its position and a surface normal.Once we identify a position in the world to place our content,we'll add a world anchorfor ARKit to continuously track this position for us.ARKit will update this world anchor's transformas the world map is refined.We can create a new entity to load our ship's model,and set its transform using the world anchor update,positioning it where the user wanted.Finally, we can add the entity to our contentto render it over the workbench.Whenever ARKit updates the world anchor we added,we update the transform component of our ship entity,making sure it stays anchored to the real world.And that's it!We used our hands to point to a location in our surroundingsand placed content there.Raycasting is not only useful for placing content,but also for interacting with it.Let's see what it takesto raycast against our virtual ship.RealityKit collision components are very powerful.We can let the ship entity participate in collisionsby simply adding an appropriate collision component to it,which Reality Composer Pro can help us with.After enabling the ship's collision componentand building a new ray from the latest hand joint positions,we can do another raycastand tell if the user is pointing at the ship or the table.The previous examples demonstratedthe power and versatility of combining RealityKit's featureswith ARKit's scene understandingto build truly compelling experiences.Lets see how using ARkit has changed for spatial computing.Fundamentally, just as on iOS,ARKit still works by running a sessionto receive anchor updates.How you configure and run your session,receive anchors updates, and persist world anchorshas changed on this new platform.Let's take a look!On iOS, ARKit provides different configurations to chose from.Each configuration bundles capabilitiesnecessary for your experience.Here, for example,we selected ARWorldTrackingConfiguration,and will enable sceneReconstruction for meshesand planeDetection for planes.We can then create our ARSessionand run it with the selected configuration.On this new platform, ARKit now exposes a data providerfor each scene understanding capability.Hand tracking is a new capability offered by ARKitand gets its own provider as well.Each data provider's initializer takes the parametersneeded to configure that provider instance.Now instead of choosing from a catalogueof preset configurations, you get an à la carte selectionof the providers you need for your application.Here for example, we choose a SceneReconstructionProviderto receive mesh anchorsand a PlaneDetectionProvider to receive plane anchors.We create the providersand initialize the mesh classificationand plane types we wish to receive.Then we create an ARKitSessionand run it with the instantiated providers.Now that we have seen how configuring your sessionhas been simplified, let's go and understandwhich way these new data providerschange the way your app actually receives ARKit data.On iOS, a single delegatereceives anchor and frame updates.Anchors are aggregated and delivered with ARFramesto keep camera frames and anchors in sync.Applications are responsiblefor displaying the camera pixel buffer,and using camera transformsto register and render tracked virtual content.Mesh and plane anchors are delivered as base anchors,and it is up to you to disambiguate themto figure out which is which.On our new platform,it is the data providers that deliver anchor updates.Here are the providers we previously configured.Once you run an ARKitSession,each provider will immediately beginasynchronously publishing anchor updates.The SceneReconstructionProvider gives meshAnchors,and the planeDetectionProvider gives us PlaneAnchors.No disambiguation is necessary!Anchor updates come as soon as they're available,and are decoupled from updates of other data providers.It is important to note that ARFrames are no longer provided.Spatial computing applications do not need frame or camera datato display content,since this is now done automatically by the system.Without having to package anchor updates with an ARFrame,ARKit can now deliver them immediately,reducing latency, allowing your applicationto quickly react to updates in the person's surroundings.Next, let's talk about worldAnchor persistence.You will love these changes!In our raycasting examples,we used world anchors to place and anchor virtual contentto real-world positions.Your app can persist these anchors,enabling it to automatically receive them again,when the device returns to the same surroundings.Let's first quickly recap how persistence worked on iOS.On iOS, it is the application's responsibilityto handle world map and anchor persistence.This included requesting and saving ARKit's world mapwith your added anchor,adding logic to reload the correct world mapat the right time, then waiting for relocalization to finishbefore receiving previously persisted anchorsand continuing the application experience.On this new platform, the system continuously persiststhe world map in the background,seamlessly loading, unloading,creating, and relocalizing to existing mapsas people move around.Your application does not have to handle maps anymore,the system now does it for you!You simply focus on using world anchorsto persist locations of virtual content.When placing content,you'll be using the new WorldTrackingProviderto add WorldAnchors to the world map.The system will automatically save these for you.The WorldTrackingProvider will update the tracking statusand transforms of these world anchors.You can use the WorldAnchor identifierto load or unload the corresponding virtual content.We just highlighted a few updates to the ARKit principlesthat you knew from iOS,but there is so much more to explore!For a deeper dive, with code examples,we recommend you watch"Meet ARKit for spatial computing."Let's conclude this session!In this session, we provided a high-level understandingof how ARKit and RealityKit conceptshave evolved from iOS,the changes you need to consider,and which sessions to watch for more details.This platform takes on many tasksyour iOS app had to handle, allowing you to really focuson building beautiful content and experiencesusing frameworks and concepts you're already familiar with.We are thrilled to see how you leverage spatial computingand all of its amazing capabilities to evolve your app!Thank you for watching!♪

♪ Mellow instrumental hip-hop ♪♪Omid Khalili: Hello! My name is Omid.

Oliver and I are engineers on the ARKit teamand we are thrilled to review the concepts --some familiar and some new --that you'll need know about when bringing your iOS AR appto our new platform.

ARKit was introduced on iOS in 2017and with it, we introduced three key conceptsto building augmented reality applications.

With world tracking,ARKit is able to track your device's position in the worldwith six degrees of freedom.

This allows anchoring virtual contentwith a position and orientation to the real world.

Scene understanding provides insightabout the real world around you.

Using the provided geometry and semantic knowledge,your content can be intelligently placedand realistically interact with surroundings.

Finally, rendering engines can correctly register and compositeyour virtual content over captured imagesutilizing camera transformsand intrinsics provided by ARKit.

Initially, we started with a SceneKit viewto use ARKit's camera transformsand render 3D content on iOS.

We then introduced RealityKit,laying out the foundation for an enginecapable of highly realistic physically-based renderingand accurate object simulation with your surroundings.

To enable spatial computing,ARKit and RealityKit have maturedand are deeply integrated into the operating system.

For example, ARKit's tracking and Scene Understandingare now running as system services,backing everything from window placementto spatial audio.

The system takes on responsibilitiesthat used to belong to applications.

Camera pass-through and matting of the user's handsare now built-in,so your application gets these capabilities for free.

Another built-in capability is that ARKit world mapsare continuously persisted by a system service,so your application doesn't need to do it anymore.

We believe that this will free you upto focus on building the best application and content possiblefor this platform.

Here is an example demonstrating these capabilities,along with new ones introduced with this platform.

For example, ARKit now provides hand tracking to your app,which allows people to reach outand directly interact with virtual contentthat can then interact with their surroundings.

In order to take advantage of all the new capabilitiesand immersive experiences this new platform offers,you'll need to update your iOS ARKit-based experience.

This is a great opportunity to reimagine your appand AR experience for spatial computing.

As a part of this transition,you'll be using familiar conceptsthat we've introduced with ARKit and RealityKit.

We're going to cover how these concepts have carried over,how they've evolved,and how you can take advantage of them.

Let's get started!First, we'll explore some new ways you can present your appfor spatial computing,and introduce new content tools available to you.

Next, we will talk about Reality Kit,which is the engine to use to renderand interact with your content.

We'll see how RealityView lets your appleverage spatial computing similar to ARView on iOS.

Then, we'll talk about the different waysyour app can bring content into people's surroundings.

Raycasting is something many iOS applications useto place content.

We'll show an example of how to combine ARKit dataand RealityKit to enable raycastingfor spatial computing.

And finally, we'll review the updates to ARKitand see the new ways to utilize familiar concepts from iOS.

Lets get into preparing to migrate your experiencefor spatial computing.

Spatial computing allows you to take your iOS AR experienceand expand it beyond the window.

This platform offers new ways to present your applicationthat you'll want to consideras you bring your iOS experience over.

Here is an example from our Hello World sample app.

You can now display UI,including windows and three-dimensional content,anywhere around you.

By default, applications on this platformlaunch into the Shared Space.

The Shared Space is where apps exist side by side,much like multiple apps on a Mac desktop.

Inside a Shared Space, your app can open one or more windowsto display content.

Additionally, your app can createa three-dimensional volume.

For example, now you can show a list of available board gamesin one window, the rules in another,and open the selected game in its own volume.

The game can be played while keeping a Safari window opento read up on winning strategies.

The content you add to the window and volumestays contained within its boundsto allow sharing the space with other applications.

In some cases, you may want your app to have more controlover the level of immersion in your experience --maybe to play a game that interacts with your room.

For this, your app can open a dedicated Full Spacein which only your app's windows,volumes, and 3D objects appear.

Once in a Full Space,your application has access to more features.

Using RealityKit's anchor entities,you can target and attach objects to the surroundingslike tables, floors,and even parts of your hands like the palm or wrist.

Anchor entities work without requiring user permission.

ARKit data is something elseyour app can only access in a Full Space.

With permission, ARKit will provide dataabout the real-world surfaces, scene geometry,and skeletal hand tracking,expanding your app's ability for realistic physicsand natural interactions.

Windows, volumes, and spaces are all SwiftUI scene types.

There's so much more for you to learn about these.

For starters, you can go to the session mentioned here.

Next, let's review the main steps needed to prepareyour content for bringing it to spatial computing.

Memorable AR experiences on iOS begin with great 3D content;the same is true for spatial experiences on this platform.

And when it comes to 3D content,it's great to rely on an open standard likeUniversal Scene Description, or USD for short.

USD is production proven,and scales from creators making single assetsto large studios working on AAA games and films.

Apple was an early adopter of USD,adding it to our platforms in 2017and growing support since.

Today, USD is at the heart of 3D contentfor spatial computing.

With USD assets ready, you can bring them intoour new developer tool, Reality Composer Pro,to compose, edit, and preview your 3D content.

If you're using CustomMaterials for your 3D content on iOS,then you will need to rebuild them using its shader graph.

You also have the ability to edit your RealityKit componentsdirectly through the UI.

And finally, you can import your Reality Composer Pro projectdirectly into Xcode,allowing you to easily bundle all of your USD assets,materials, and custom components into your Xcode project.

We have some great sessionsto help you learn more about Reality Composer Proand how to build your own custom materials for spatial computing.

Now that we've seen the different waysto present your application,let's learn more about the features RealityView offersas you bring your experience over.

We just saw how spatial computingallows apps to display content in your space.

One of the key differences coming from iOSis how different elements can be presented side by side.

Notice how your 3D content and 2D elements can appearand work along side each other.

Coming from iOS, you're going to usefamiliar frameworks to create each of these.

You'll use SwiftUI to build the best 2D UIand get system gestures events like the ones on iOS.

And you'll use RealityKit to render your 3D contentfor spatial experiences.

The way to interface with both of these at the same timeis through RealityView -a new SwiftUI view that we're introducingto cater to the unique needs of spatial computing.

RealityView truly bridges SwiftUI and RealityKit,allowing you to combine 2D and 3D elementsand create a memorable spatial experience.

You'll be using the RealityView to hold all the entitiesyou wish to display and interact with.

You can get gesture events and connect themto the entities in your view to control them.

And with access to ARKit's scene understanding,you can enable realistic simulationswith people's surroundings and even their handsusing RealityKit's collision components.

Before we look at how using RealityKitcarries over from iOS, let's do a quick refresheron how to work with RealityKit's Entity Component System.

In the Reality Kit Entity Component System,each entity is a container for 3D content.

Different components are added to an entityto define its look and behavior.

This can include a model componentfor how it should render;a collision component,for how it can collide with other entities;and many more.

You can use RealityComposer Proto prepare RealityKit components like collision componentsand get them added to your entities.

Systems contain code to act on entitiesthat have the required components.

For example, the system required for gesture supportonly operates on entities that have a CollisionComponentand InputTargetComponent.

A lot of the concepts used by RealityViewfor spatial computing carry over from those of ARView on iOS.

Lets see how these two stack up.

Both views are event-aware containersto hold the entities you wish to display in your app.

You can add Gesture Support to your viewsto enable selection and interaction with entities.

With SwiftUI for spatial computing,you can reach out to select or drag your entities.

Both ARView and RealityViewprovide a collection of your entities.

ARView uses a Scene for this.

RealityView has a Content to add your entities to.

You can add AnchorEntities to them,allowing you to anchor your content to the real world.

On both platforms, you create an entityto load your content model and an AnchorEntity to place it.

One main difference between the platformsis in the behavior of anchor entities.

ARView on iOS uses an ARSessionand your app must receive permissionto run scene understanding algorithmsnecessary for anchor entities to work.

RealityView is using System Servicesto enable anchorEntities.

This means that spatial experiencescan anchor content to your surroundingswithout requiring permissions.

Apps using this approach do not receivethe underlying scene understanding dataor transforms.

Not having transform data for your app to place contenthas some implications that Oliver will talk about laterin his section.

As we've seen, there are many familiar conceptsthat carry over coming from iOS,but there are also new capabilitiesthat RealityKit provides for spatial computing.

We've only scratched the surface of what's possiblewith RealityKit on this new platform,and you may want to check out the session belowto follow up on more.

Now over to Oliver, who will talk more about RealityViewand how to bring in your content from iOS.

Oliver Dunkley: Thanks, Omid!Let's continue by exploring the different waysyou can bring in your existing contentto spatial computing.

Lets start in the Shared Space.

We can add 3D content to a window or volume,and use system gestures to interact with it.

To display your assets, you just add them directlyto RealityView's Content.

You do this by creating an entityto hold your model component and position itby setting the transform component.

You can also set up gesture supportto modify the transform component.

Note that all entities added to the view's contentexist in the same space relative to the space's originand can therefore interact with one another.

In the Shared Space,content cannot be anchored to your surroundings.

Let's consider our optionsif we transition our app to a Full Space.

One of the key differences coming from the Shared Spaceis that apps can now additionally anchor contentto people's surroundings.

Anchoring your content here can happen in two ways.

Lets first look at using RealityKit's AnchorEntityto place content without requiring permissionsto use ARKit data in your app.

RealityKit's AnchorEntitiesallow you to specify a target for the system to findand automatically anchor your content to.

So for example, in order to place a 3D modelon a table surface in front of you,you can use a RealityKit AnchorEntitywith a target set to table.

Different from iOS, AnchorEntities can be usedwithout having to prompt for user permissions.

People's privacy is preserved by not sharingthe underlying transforms of the AnchorEntitywith your application.

Note: this implies children of different anchor entitiesare not aware of one another.

New to anchorEntities, you can target hands,which opens up a whole new realmof interesting interaction opportunities.

For example, you could anchor content to a person's palmand have it follow their hands as they move them.

This is all done by the system,without telling your app where the persons hands actually are.

AnchorEntitys provide a quick,privacy-friendly way for your appto anchor content to people's surroundings.

Coming back to a Full Space,we can also leverage ARKit to incorporatesystem-level knowledge of the people's surroundings.

This enables you to build your own custom placement logic.

Let's take a look at how this works.

Similar to iOS, your application receives anchoring updatesfor scene understanding data.

You can integrate this anchor data into your app logicto achieve all sorts of amazing experiences.

For example, you could use the bounds of a planeto center and distribute your content onto.

Or, you could use planes and their classificationsto find the corner of a roomby looking for the intersection of two walls and a floor.

Once you've decided where to place your content,you add a world anchor for ARKit to trackand use it to update your entity's transform component.

This not only allows your contentto remain anchored to the real world,as the underlying world map is updated,but it also opens the door to anchor persistence,which we will explore shortly.

All the entities added to your spacecan interact with one another as well as with the surroundings.

This all works because scene understanding anchorsare delivered with transforms relative to the space's origin.

User Permission is required to use ARKit capabilities.

You just saw how integrating ARKit data into your app logiccan enable more advanced features.

So far we have talked about letting your app place content.

Let's explore how we can let people guide placement.

On iOS, you can use raycastingto translate 2D input to a 3D position.

But with this new platform,we don't need this 2D-3D bridge anymore,as we can use handsto naturally interact with experiences directly.

Raycasting remains powerful;it lets people reach out beyond arms length.

There are various ways to set up raycasting.

Fundamentally, you need to setup RealityKit'scollision components to raycast against.

Collision components can also be createdfrom ARKit's mesh anchorsto raycast against people's surroundings.

Let's explore two examples of how to raycastfor spatial computing:first, using system gestures,and the second using hands data.

After obtaining a position,we can place an ARKit worldAnchorto keep our content anchored.

Let's consider the following example.

Imagine our app revolves aroundplacing inspirational 3D assets for modelers.

Maybe in this particular scenario,a person wants to use our app to place a virtual shipon their workbench for some modeling project.

Here is our workbench we want to place our ship on.

We'll start with an empty RealityView.

ARKit's scene understanding provides mesh anchorsthat we'll use to represent the surroundings.

They provide geometry and semantic information we can use.

Remember that meshes for scene reconstruction dataare delivered as a series of chunks.

We'll create an entity to represent this mesh chunk,and we'll correctly place this entity in a full spaceusing the mesh anchor's transform.

Our entity then needs a collision componentto hit test against.

We'll use RealityKit's ShapeResources methodto generate a collision shape from the meshAnchorfor our entity.

We'll then add our correctly placed entitywhich supports hit testing.

We'll build an entity and collision componentfor each mesh chunk we receiveto represent all the surroundings.

As scene reconstruction is refined,we may get updates to meshes or have chunks removed.

We should be ready to update our entitieson these changes as well.

We now have a collection of entitiesrepresenting the surroundings.

All these entities have collision componentsand can support a raycast test.

Let's first explore raycasting using system gestures,and then continue the example using hands data.

We can raycast and get a position to place our shipusing system gestures.

Gestures can only interact with entitiesthat have both Collision and InputTarget components,so we add one to each of our mesh entities.

By adding a SpatialTapGesture to the RealityView,people can raycast by looking at entities and tapping.

This resulting event holds a position in world spacerepresenting the place people looked at when tapping.

Instead of using system gestures,we could also have used ARKit's hand anchorsto build a ray.

Lets take a step back and explore this option.

To know where people point,we first need a representation of the person's hand.

ARkit's new hand anchors gives us everything we need.

We can use finger joint information to build the originand direction of the ray for our query.

Now that we have the origin and direction of our ray,we can do a raycast against the entities in our scene.

The resulting CollisionCastHitprovides the entity that was hit,along with its position and a surface normal.

Once we identify a position in the world to place our content,we'll add a world anchorfor ARKit to continuously track this position for us.

ARKit will update this world anchor's transformas the world map is refined.

We can create a new entity to load our ship's model,and set its transform using the world anchor update,positioning it where the user wanted.

Finally, we can add the entity to our contentto render it over the workbench.

Whenever ARKit updates the world anchor we added,we update the transform component of our ship entity,making sure it stays anchored to the real world.

And that's it!We used our hands to point to a location in our surroundingsand placed content there.

Raycasting is not only useful for placing content,but also for interacting with it.

Let's see what it takesto raycast against our virtual ship.

RealityKit collision components are very powerful.

We can let the ship entity participate in collisionsby simply adding an appropriate collision component to it,which Reality Composer Pro can help us with.

After enabling the ship's collision componentand building a new ray from the latest hand joint positions,we can do another raycastand tell if the user is pointing at the ship or the table.

The previous examples demonstratedthe power and versatility of combining RealityKit's featureswith ARKit's scene understandingto build truly compelling experiences.

Lets see how using ARkit has changed for spatial computing.

Fundamentally, just as on iOS,ARKit still works by running a sessionto receive anchor updates.

How you configure and run your session,receive anchors updates, and persist world anchorshas changed on this new platform.

Let's take a look!On iOS, ARKit provides different configurations to chose from.

Each configuration bundles capabilitiesnecessary for your experience.

Here, for example,we selected ARWorldTrackingConfiguration,and will enable sceneReconstruction for meshesand planeDetection for planes.

We can then create our ARSessionand run it with the selected configuration.

On this new platform, ARKit now exposes a data providerfor each scene understanding capability.

Hand tracking is a new capability offered by ARKitand gets its own provider as well.

Each data provider's initializer takes the parametersneeded to configure that provider instance.

Now instead of choosing from a catalogueof preset configurations, you get an à la carte selectionof the providers you need for your application.

Here for example, we choose a SceneReconstructionProviderto receive mesh anchorsand a PlaneDetectionProvider to receive plane anchors.

We create the providersand initialize the mesh classificationand plane types we wish to receive.

Then we create an ARKitSessionand run it with the instantiated providers.

Now that we have seen how configuring your sessionhas been simplified, let's go and understandwhich way these new data providerschange the way your app actually receives ARKit data.

On iOS, a single delegatereceives anchor and frame updates.

Anchors are aggregated and delivered with ARFramesto keep camera frames and anchors in sync.

Applications are responsiblefor displaying the camera pixel buffer,and using camera transformsto register and render tracked virtual content.

Mesh and plane anchors are delivered as base anchors,and it is up to you to disambiguate themto figure out which is which.

On our new platform,it is the data providers that deliver anchor updates.

Here are the providers we previously configured.

Once you run an ARKitSession,each provider will immediately beginasynchronously publishing anchor updates.

The SceneReconstructionProvider gives meshAnchors,and the planeDetectionProvider gives us PlaneAnchors.

No disambiguation is necessary!Anchor updates come as soon as they're available,and are decoupled from updates of other data providers.

It is important to note that ARFrames are no longer provided.

Spatial computing applications do not need frame or camera datato display content,since this is now done automatically by the system.

Without having to package anchor updates with an ARFrame,ARKit can now deliver them immediately,reducing latency, allowing your applicationto quickly react to updates in the person's surroundings.

Next, let's talk about worldAnchor persistence.

You will love these changes!In our raycasting examples,we used world anchors to place and anchor virtual contentto real-world positions.

Your app can persist these anchors,enabling it to automatically receive them again,when the device returns to the same surroundings.

Let's first quickly recap how persistence worked on iOS.

On iOS, it is the application's responsibilityto handle world map and anchor persistence.

This included requesting and saving ARKit's world mapwith your added anchor,adding logic to reload the correct world mapat the right time, then waiting for relocalization to finishbefore receiving previously persisted anchorsand continuing the application experience.

On this new platform, the system continuously persiststhe world map in the background,seamlessly loading, unloading,creating, and relocalizing to existing mapsas people move around.

Your application does not have to handle maps anymore,the system now does it for you!You simply focus on using world anchorsto persist locations of virtual content.

When placing content,you'll be using the new WorldTrackingProviderto add WorldAnchors to the world map.

The system will automatically save these for you.

The WorldTrackingProvider will update the tracking statusand transforms of these world anchors.

You can use the WorldAnchor identifierto load or unload the corresponding virtual content.

We just highlighted a few updates to the ARKit principlesthat you knew from iOS,but there is so much more to explore!For a deeper dive, with code examples,we recommend you watch"Meet ARKit for spatial computing."Let's conclude this session!In this session, we provided a high-level understandingof how ARKit and RealityKit conceptshave evolved from iOS,the changes you need to consider,and which sessions to watch for more details.

This platform takes on many tasksyour iOS app had to handle, allowing you to really focuson building beautiful content and experiencesusing frameworks and concepts you're already familiar with.

We are thrilled to see how you leverage spatial computingand all of its amazing capabilities to evolve your app!Thank you for watching!♪

## Code Samples

