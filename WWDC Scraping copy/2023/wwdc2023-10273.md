# Wwdc2023 10273

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Work with Reality Composer Pro content in XcodeLearn how to bring content from Reality Composer Pro to life in Xcode. We'll show you how to load 3D scenes into Xcode, integrate your content with your code, and add interactivity to your app. We'll also share best practices and tips for using these tools together in your development workflow.

To get the most out of this session, we recommend first watching “Meet Reality Composer Pro” and “Explore materials in Reality Composer Pro" to learn more about creating 3D scenes.Chapters0:00 -Introduction2:37 -Load 3D content6:27 -Components12:00 -User Interface27:51 -Play audio30:18 -Material properties33:25 -Wrap-upResourcesHD VideoSD VideoRelated VideosWWDC23Build spatial experiences with RealityKitDevelop your first immersive appEnhance your spatial computing app with RealityKitExplore materials in Reality Composer ProMeet Reality Composer ProWWDC21Dive into RealityKit 2

Learn how to bring content from Reality Composer Pro to life in Xcode. We'll show you how to load 3D scenes into Xcode, integrate your content with your code, and add interactivity to your app. We'll also share best practices and tips for using these tools together in your development workflow.

To get the most out of this session, we recommend first watching “Meet Reality Composer Pro” and “Explore materials in Reality Composer Pro" to learn more about creating 3D scenes.

0:00 -Introduction

2:37 -Load 3D content

6:27 -Components

12:00 -User Interface

27:51 -Play audio

30:18 -Material properties

33:25 -Wrap-up

HD VideoSD Video

HD Video

SD Video

Build spatial experiences with RealityKit

Develop your first immersive app

Enhance your spatial computing app with RealityKit

Explore materials in Reality Composer Pro

Meet Reality Composer Pro

Dive into RealityKit 2

Search this video…♪ Mellow instrumental hip-hop ♪♪Hi, my name is Amanda,and I'm an engineer working on RealityKitand Reality Composer Pro.In this session, we'll learn how to make spatial experiencesusing 3D content you assemble in Reality Composer Pro.Reality Composer Pro is a developer tool for preparingRealityKit content to be used in your spatial computing app.In this session, we'll continue iterating on a projectthat my colleagues Eric and Niels created in their sessions,and we'll learn how to make it interactive in code.If you haven't watched their sessions already,I'd recommend you familiarize yourselfwith the editor UI and features of Reality Composer Procovered in their sessions.First, let's look at the finished product we built,and then I'll take you through how we created each part.We're looking at a topographical mapof Yosemite National Park.Seeing it in headset really gives you a sense of vastnessthat wasn't possible before without going there in person.In the previous Reality Composer Pro sessions,Eric assembled this scene, and Niels created the materialswe're using on the topography.Here, we've added a sliderto morph between two different California landmarks.Now we're looking at Catalina Islandoff the coast of Los Angeles.We also have hovering 2D SwiftUI buttonspositioned in 3D space that let you learn more aboutvarious points of interest in that map.In this session, we'll explore how we arranged this contentin Reality Composer Pro so that we could use itto drive the experience.I'll show how we hooked up this sliderand the point-of-interest buttonsso that they affect the scene we madein Reality Composer Pro.We'll start by programmatically loading 3D contentfrom our Reality Composer Pro project.We'll explore how RealityKit components work,and how we can make use of them in code,including creating our own Custom Components.We'll learn about the new RealityView API in SwiftUIand discover how we can add user interface elementsto our scene using the Attachments API.And we'll learn how to work with audio that we set upin Reality Composer Pro.Then, we'll pick up from where Niels left offby connecting our custom material that we madeusing Shader Graph and drive elements of it from our code.Let's get started.In Eric's session, we made a Reality Composer Pro projectthat contains all the assets for our dioramaarranged the way we want them.These tabs at the top each represent one root entitythat we can load at runtime.We can put a lot of things into a sceneand treat that as our fully assembled scene.Or we can put just a fewand then treat that scene like a little reusable assemblage.We can make as many as we want.Let's see how we load this scene named DioramaAssembledat runtime.We use entity's asynchronous initializerto make us an entity with the contentsfrom our Reality Composer Pro package.We specify which entity we want to load using its string name,and we give it the bundle that our package produces.It will throw if it can't find anythingin our Reality Composer Pro project by that name.realityKitContentBundle is a constant value thatwe autogenerate for you in your Reality Composer Pro package.This goes in a RealityView make closure.A RealityView is a new kind of SwiftUI view.It is your entrée into RealityKit.It's the bridge between the worldsof SwiftUI and RealityKit.We'll delve deeper into this RealityViewlater in this session.If there are USD assets you're using in your Xcode projectthat you're not adding to a Reality Composer Pro project,we strongly encourage you to put those assetsinto a Swift Package,with an .rkassets directory inside it, like this.Xcode compiles the .rkassets folder into a formatthat's faster to load at runtime.The entity we just loaded is actually the rootof a larger entity hierarchy.It has child entities and they in turn have child entities.It's everything we arranged in our Reality Composer Pro scene.If we wanted to address one of the entities lower downin the hierarchy, we could give it a namein Reality Composer Pro,and then at runtime, we could ask the sceneto find that entity by its name.Entities are a part of ECS,which stands for Entity Component System.ECS is what powers RealityKit and Reality Composer Pro.Let's take a step back and understand ECS.ECS has some close parallels to object-oriented programmingbut is different in some key ways.In the object-oriented programming world,the object has properties which are attributesthat define its nature,and it has its own functionality.You write these properties and functionsin a class that defines the object.In the ECS world, an entity is any thing you see in the scene.They can also be invisible.They don't hold attributes or data though.We put our data into components instead.Components can be added to or removed from entitiesat any time during app execution,which provides a way to dynamically changethe nature of an entity.A system is where our behavior lives.It has an update function that's called once per frame.That's where you put your ongoing logic.In your system, you query for all entities that havea certain component on them, or configuration of components,and then you perform some action and store the updated databack into those components.For a more in-depth discussion on ECS,check out the "Dive into RealityKit 2" sessionfrom 2021, and this year's"Build spatial experiences with RealityKit."Now let's learn about components.We're going to see how to add components to entitiesin our Reality Composer Pro project,and then we'll learn how to create custom componentsfor making location markers on our diorama.To add a component to an entity in Swift,you'd say entity.components.set()and provide the component value.To do the same in Reality Composer Pro,select the entity you wanteither in the viewport or in the hierarchy.Then, at the bottom of the Inspector Panel,click the Add Component button to bring up a listof all RealityKit's available components.We can add as many components to an entity as we want,and we can only add one of each type;it's a set.You'll also see any custom components you've madein this list as well.Let's see how we can use Reality Composer Proto create our own custom components.We're going to make those floating buttons that hoverover specific points on our terrain so you can select themto see more information about that spot.We'll prepare a lot of that UI and functionality in code,but I want to show you how to mark these entitiesin Reality Composer Pro as the positionsat which we want to show those floating buttons.To do this, we're going to add entities at locationsabove our terrain map, which will signify to the appthat these are the places we want to showour floating buttons.Then we'll create a point of interest componentto house our information about each place.Then we open the PointOfInterestComponent.swiftin Xcode to edit it, adding properties like a nameand a description.In Reality Composer Pro, we'll add our newPointOfInterestComponent to each of our new entities,and then we'll fill in the properties' values.Let's make our first location marker entity, Ribbon Beach,which is a place on Catalina Island.We click the plus menu and select Transformto make us a new invisible entity.We can name our entity Ribbon_Beach.Let's put this entity where Ribbon Beachactually is on the island.We click on the Add Component button,but this time, we select New Componentbecause we're going to make our own.Let's give it a name, PointOfInterest.Now it shows up in the Inspector Paneljust like our other components do.But what's this count property?Let's open our new component in Xcode.In Xcode, we see that Reality Composer Pro createdPointOfInterestComponent.swift for us.Reality Composer Pro projects are Swift packages,and the Swift code we just generatedlives here in the package.Looking at the template code, we see that that'swhere the count property came from.Let's have another property instead.We want each point of interestto know which map it's associated withso that when you change maps,we can fade out the old points of interestand fade in the appropriate ones.So we add an enumeration property,var region.Let's make our enum region up here......and give it two cases,since we're only building two maps right now:Catalina and Yosemite.It can serialize as a string.We also conform it to the Codable protocolso that Reality Composer Pro can see itand serialize instances of it.Back in Reality Composer Pro, the count property has gone awayand our new region property shows up.It has a default value of yosemitebecause that's what we initialized in the code,but we can override it here for this particular entity.If we override it, this value will only take effecton this particular entity.The rest of the point of interest componentswill have the default value of yosemiteunless we override them too.We're using our PointOfInterestComponentlike a signifier, a marker that we stick on these entities.These entities act like placeholdersfor where we'll put our SwiftUI buttons at runtime.We add our other Catalina Island points of interestthe same way we just added Ribbon Beach.Let's run our app and see what our new custom component does.Oh! It doesn't do anything.That's because we haven't written any codeto handle these point of interest components yet.Let's do that.We have a new way of putting SwiftUI contentinto a RealityKit scene.This is called the Attachments API.We're going to combine attachments withour PointOfInterestComponent to create hovering buttonswith custom data at runtime.Let's first see this in code and then I'll walk you throughthe data flow.Attachments are a part of the RealityView.Let's first look at a simplified exampleto show the structure of a RealityViewso we can see how SwiftUI views get into a RealityKit scene.The RealityView initializer that we're going to usetakes three parameters:a make closure, an update closure,and an attachments ViewBuilder.Fleshing this out a little, let's add a bare-minimumimplementation of creating an Attachment View,a green SwiftUI button,and adding it to our RealityKit scene.In the Attachments ViewBuilder, we make a normal SwiftUI view;we can use view modifiers and gesturesand all the rest that SwiftUI gives us.We tag our View with any unique hashable.I've chosen to tag this button view with a fish emoji.Later, when SwiftUI invokes our update closure,our button View has become an entity.It's stored in the attachments parameter to this closure,and we fish it out using the tag we gave it before.We can then treat it like any other entity.We can add it as a child of any existing entity in our scene,or we can add it as a new top-level entityin the content's entities collection.And since it's become a regular entity,we can set its position so it shows up where we want in 3D,and we can add any components we want as well.Here's how data flows from one part of the RealityViewto another.Let's look at the three parametersto this RealityView initializer.The first is make, which is where you loadyour initial set-up scene from your Reality Composer Pro bundleas an entity and then add it to the RealityKit scene.The second is update,which is a closure that will be calledwhen there are changes to your view's state.Here, you can change things about your entities,like properties in their components, their position,and even add or remove entities from the scene.This update closure is not executed every frame.It's called whenever the SwiftUI view state changes.The third is the attachments ViewBuilder.This is where you can make SwiftUI viewsto put into your RealityKit scene.Your SwiftUI views start out in the attachments ViewBuilder,then they are delivered to you in the update closurein the attachments parameter.Here, you ask the attachments parameter if it has an entityfor you using the same tag you gave to your buttonin the attachments ViewBuilder.If there is one, it vends you a RealityKit entity.In your update closure, you set its 3D positionand add it to your RealityKit sceneso you can see it floating in space wherever you want.Here, I've added my button entityas a child of a sphere entity.I positioned it 0.2 meters above its parent.The make closure also has an attachments parameter.This one is for adding any attachments that you haveready to go at the time this view is first evaluated,because the make closure is only run once.Now that we have understood the general flow of a RealityView,let's get further into the update closure.The parameter to your make and update closuresis a RealityKitContent.When you add an entity to your RealityKit content,it becomes a top-level entity in your scene.Likewise, from your update function,adding an entity to your contentgives you a new top-level entity in your scene.While the make closure will only be called once,the update closure will be called more than once.If you create a new entity in your update closureand add it to your content there,you'll get duplicates of that entity,which might not be what you want.To guard against that, you should only add entitiesto your content that are created somewhere that's only run once.You don't need to check if the content.entitiesalready contains your entity.It's a no-op if you call add with the same entity twice,like a set.It's the same when you parent an entity to an existing entityin your scene -- it won't be added twice.Attachment entities are not created by you;they're created by the RealityViewfor each attachment view that you providein your attachments ViewBuilder.That means it's safe to add them to the contentin your update closure without checking if it's already there.So, that was how we'd write our attachments codeif we wanted to hardcode our points of interestin the attachments ViewBuilder.But since we want to let the datain our Reality Composer Pro project drive the experience,let's make it more flexible.That way, a designer or producer can createthe points of interest in the Reality Composer Pro project,and our code can accommodate whatever data they add.To make it data-driven, we need our code to readthe data that we set up in our Reality Composer Pro scene.We'll be creating our attachment views dynamically.High level, here's what we're going to do.In Reality Composer Pro, we already set upour placeholder entity for Ribbon Beach,and we'll do the same for the other points of interestthat we want to highlight in our diorama.We'll fill out all the info each one needs,like their name and which map they belong on.Now, in code, we'll query for those entitiesand create a new SwiftUI button for each one.In order to get SwiftUIto invoke our attachments ViewBuilderevery time we add a new button to our collection,we'll add the @State property wrapper to this collection.We'll serve those buttons up to the attachments ViewBuilder.Then finally, in the update closure of our RealityView,we'll receive our buttons as entitiesand add those new button entities to the scene.We'll add each one as a child of the marker entitieswe set up in Reality Composer Pro.Let's understand these six steps through a more detailed diagram,and then we'll look at the code.First, we add invisible entitiesin our Reality Composer Pro scene.We position our invisible entities where we wantour buttons to show up, on the x-, y-, and z-axes.We're making use of the Transform Component here,which all entities have by default.Then we add our PointOfInterestComponentto each of them.In our code, we get references to these entitiesby querying for all entities in the scene that havethe PointOfInterestComponent on them.The query returns the three invisible entitieswe set up in Reality Composer Pro.We create a new SwiftUI view for each oneand store them in a collection.To get our buttons into our RealityView,we'll make use of the SwiftUI view-updating flow.This means adding the property wrapper @Stateto the collection of buttons in our View.The @State property wrapper tells SwiftUIthat when we add items to this collection,SwiftUI should trigger a view update on our ImmersiveView.That will cause SwiftUI to evaluateour attachments ViewBuilder and our update closure again.The RealityView's attachments ViewBuilderis where we'll declare to SwiftUIthat we want these buttons to be made into entities.Our RealityView's update closure will be called next,and our buttons will be delivered to us as entities.They're no longer SwiftUI Views now.That's why we can add them to our entity hierarchy.In the update closure, we add our attachment entitiesto the scene, positioned floating aboveeach of our invisible entities.Now they will show up visuallywhen we look at our diorama scene.Let's see how each of these steps is done.First, we mark our invisible entitiesin our Reality Composer Pro scene.To find our entities that we marked,we'll make an EntityQuery.We'll use it to ask for all entities that havea PointOfInterestComponent on them.We'll then iterate through our QueryResultand create a new SwiftUI View for each entity in our scenethat has a PointOfInterestComponent on it.We'll fill it in with informationwe grab from the component,the data we entered in Reality Composer Pro.That view is going to be one of our attachments,so we put a tag on it.In this case, we're getting serious,so we'll use an ObjectIdentifier rather than a fish emoji.Here's the part where we make our collection of SwiftUI Views.We'll call it attachmentsProvidersince it will provide our attachmentsto the RealityView's attachments ViewBuilder.We'll then store our view in the attachmentsProvider.Let's take a look at that collection type.AttachmentsProvider has a dictionaryof attachment tags to views.We type-erased our view so we can putother kinds of views in there besides our LearnMoreView.We have a computed property called sortedTagViewPairsthat returns an array of tuples --tags and their corresponding views --in the same order every time.Then, in the attachments ViewBuilder,we'll ForEach over our collection of attachmentsthat we made.This tells SwiftUI that we want one viewfor each of the pairs we've given it,and we provide our views from our collection.We're letting the ObjectIdentifierdo double duty here as both an attachment tag for the view,and as an identifier for the ForEach structure.So, why didn't we just add a tag propertyto our PointOfInterestComponent instead?Attachment tags need to be unique,both for the ForEach structand the attachments mechanism to work.And since all the properties in our custom componentwill be shown in Reality Composer Pro's Inspector Panelwhen you add the component to an entity,that means the attachmentTag would show up there too.We don't want to burden ourselveswith having to remember to uniquify all the tagswhen we're adding each point of interestin Reality Composer Pro.But, conveniently for us, entities conformto the Identifiable protocol, so they have identifiersthat are unique automatically.We can get this identifier at runtime from the entitywithout needing to know it ahead of timewhen we're designing our scene in Reality Composer Pro.To have the attachmentTag propertynot show up in Reality Composer Pro,we use a technique that I call"design-time versus runtime components."We'll separate our data into two different components,one for design-time data that we want to arrangein Reality Composer Pro, and one for runtime datathat we will attach to those same entitiesdynamically at runtime.This is for properties that we don't want to showin our Inspector Panel in Reality Composer Pro.So we'll define a new component,PointOfInterestRuntimeComponent,and move our attachment tag inside it.Reality Composer Pro automatically buildsthe component UI for you based on what it readsin your Swift package.It inspects the Swift code in your packageand makes any codable components it findsavailable for you to use in your scenes.Here we're showing four components.Components A and Bare in our Xcode project, but they are not insidethe Reality Composer Pro package,so they won't be available for you to attachto your entities in Reality Composer Pro.Component C is inside the package but it is not codable,so Reality Composer Pro will ignore it.Of the four components shown here,only Component D will be shown in the listin Reality Composer Pro because it is within the Swift packageand it is a codable component.That one is our design-time component,while all the others may be used as runtime components.Design-time components are for housing simpler data,such as ints, strings, and SIMD values,things that 3D artists and designers will make use of.You'll see an error in your Xcode projectif you add a property to your custom componentthat's of a type that Reality Composer Pro won't serialize.Now, let's get back to our code.We'll first add our PointOfInterestruntime component to our entityand then use the runtime componentto help us match up our attachment entitieswith their corresponding points of interest on the diorama.Here's where our runtime component comes in.We're at the part where we're reading inour PointOfInterest entities and creating our attachment views.We queried for all our design-time components,and now we'll make a new corresponding runtime componentfor each of them.We store our attachmentTag in our runtime component,and we store our runtime component on that same entity.In this way, the design-time component is like a signifier.It tells our app that it wants an attachment made for it.The runtime component handles any other kinds of data we needduring app execution, but don't want to storein the design-time component.In our RealityView, we have one more stepbefore we see our attachment entities show up in our scene.Once we've provided our SwiftUI Viewsin the attachments ViewBuilder, SwiftUI will callour RealityView's update closure and give us our attachmentsas RealityKit entities.But if we just add them to the contentwithout positioning them, they'll all show up sittingat the origin of the scene, position 0, 0, 0.That's not where we want them.We want them to float above each point of intereston the terrain.We need to match up our attachment entitieswith our invisible point of interest entitiesthat we set up in Reality Composer Pro.The runtime component we put on the invisible entityhas our tag in it.That's how we'll match up which attachmentEntitygoes with each point of interest entity.We query for all our PointOf InterestRuntimeComponents,we get that runtime component from each entityreturned by the query,then we use the component's attachmentTag propertyto get our attachmentEntity from the attachments parameterto the update closure.Now we add our attachmentEntity to the content and position ithalf a meter above the point of interest entity.Let's run our app again and see what these look like.Hey, they look great!We can see each place name floating above the spotwhere we put them in our Reality Composer Pro project.Next let's find out how we play audiothat we set up in Reality Composer Pro.To set up something that plays audio in Reality Composer Pro,you can bring in an audio entity by clicking the plus button,selecting Audio, and then selecting Ambient Audio.This creates a regular invisible entitywith an AmbientAudioComponent on it.Let's name our entity OceanEmitterbecause we're going to use it to play ocean soundsfor Catalina Island.You need to add an audio file to your scene as well.Let's bring in our ocean sound.You can preview your audio component by selecting a soundin the Preview menu of the componentin the Inspector Panel,but this won't automatically play the selected soundwhen the entity is loaded in your app.For that, we need to load the audio resourceand tell it to play.To play this sound, we'll get a reference to the entitythat we put the audio component on.We've named our entity OceanEmitter,so we'll find our entity by that name.We load the sound file using the AudioFileResource initializer,passing it the full path to the audio file resource primin our scene.We give it the name of the .usda file that contains itin our Reality Composer Pro project.In our case, that's our main scenenamed DioramaAssembled.usda.We create an audioPlaybackControllerby calling entity.prepareAudioso we can play, pause, and stop this sound.Now we're ready to call play on it.Here's the ocean sound playing in our app.The slider in our appmorphs between our two different terrain maps,Yosemite and Catalina Island.Now that we've introduced audio into our scene,we're going to crossfade between two audio sources.We add a forest audio emitter the same way we addedour ocean emitter entity.Let's take a look at how we're morphing our terrainusing the slider,and we'll also include audio in this transition.We'll use a property from our Shader Graph materialto morph between the two terrains.Let's see how we do that.In Niels's session, he created this beautiful geometry modifierfor us using Reality Composer Pro's Shader Graph.Now we can hook it up to our sceneand drive some of the parameters at runtime.We want to connect this Shader Graph material with a slider.To do that, we need to promote an input node.Command-click on the node and select Promote.This tells the project that we intend to supply dataat runtime to this part of the material.We'll name this promoted node Progress,so we can address it by that name at runtime.We can now dynamically change this value in code.We get a reference to the entity that our material is on.Then we get its ModelComponent, which isthe RealityKit component that houses materials.From the ModelComponent, we get its first material.There's only one on this particular entity.We cast it to type ShaderGraphMaterial.Now, we can set a new value for our parameterwith name Progress.Finally, we store the material back onto the ModelComponent,and the ModelComponent back onto the terrain entity.Now we'll hook that up to our SwiftUI slider.Whenever the slider's value changes,we'll grab that value, which is in the range 0 to 1,and feed it into our ShaderGraphMaterial.Next, let's crossfade between the ambient audio tracksfor our two terrains.Because we've also put an AmbientAudioComponenton our two audio entities, ocean and forest,we can adjust how loud the sound playsusing the gain property on them.We'll query for all entities --all two at this point, our ocean and our forest --that have the AmbientAudioComponent on them.Plus, we added another custom componentcalled RegionSpecificComponent so we can mark the entitiesthat go with one region or the other.We get a mutable copy of our audioComponentbecause we're going to change it and store itback onto our entity.We call a function that calculateswhat the gain should be given a region and a sliderValue.We set the gain value onto the AmbientAudioComponent,and then we store the component back onto the entity.Let's see that in action.Great!When we move the slider, we can seeour Shader Graph material changing the geometryof the terrain map, and we can hearthe forest sound fading out and the ocean sound coming in.We covered a lot of information today.Let's recap.We've learned how to load our Reality Composer Pro contentinto our app in Xcode.We looked at how to create your own custom componentsin Reality Composer Pro.We explored how the SwiftUI Attachments API worksand how they are delivered to us as entities.We saw how to set up audio and then play that audio in code.And finally, we saw how to take a promoted material propertyand drive it from code.These workflows will help youbring your spatial experiences to life.I look forward to seeing all the amazing thingsyou'll build on our new platform.Thank you.♪

♪ Mellow instrumental hip-hop ♪♪Hi, my name is Amanda,and I'm an engineer working on RealityKitand Reality Composer Pro.In this session, we'll learn how to make spatial experiencesusing 3D content you assemble in Reality Composer Pro.Reality Composer Pro is a developer tool for preparingRealityKit content to be used in your spatial computing app.In this session, we'll continue iterating on a projectthat my colleagues Eric and Niels created in their sessions,and we'll learn how to make it interactive in code.If you haven't watched their sessions already,I'd recommend you familiarize yourselfwith the editor UI and features of Reality Composer Procovered in their sessions.First, let's look at the finished product we built,and then I'll take you through how we created each part.We're looking at a topographical mapof Yosemite National Park.Seeing it in headset really gives you a sense of vastnessthat wasn't possible before without going there in person.In the previous Reality Composer Pro sessions,Eric assembled this scene, and Niels created the materialswe're using on the topography.Here, we've added a sliderto morph between two different California landmarks.Now we're looking at Catalina Islandoff the coast of Los Angeles.We also have hovering 2D SwiftUI buttonspositioned in 3D space that let you learn more aboutvarious points of interest in that map.In this session, we'll explore how we arranged this contentin Reality Composer Pro so that we could use itto drive the experience.I'll show how we hooked up this sliderand the point-of-interest buttonsso that they affect the scene we madein Reality Composer Pro.We'll start by programmatically loading 3D contentfrom our Reality Composer Pro project.We'll explore how RealityKit components work,and how we can make use of them in code,including creating our own Custom Components.We'll learn about the new RealityView API in SwiftUIand discover how we can add user interface elementsto our scene using the Attachments API.And we'll learn how to work with audio that we set upin Reality Composer Pro.Then, we'll pick up from where Niels left offby connecting our custom material that we madeusing Shader Graph and drive elements of it from our code.Let's get started.In Eric's session, we made a Reality Composer Pro projectthat contains all the assets for our dioramaarranged the way we want them.These tabs at the top each represent one root entitythat we can load at runtime.We can put a lot of things into a sceneand treat that as our fully assembled scene.Or we can put just a fewand then treat that scene like a little reusable assemblage.We can make as many as we want.Let's see how we load this scene named DioramaAssembledat runtime.We use entity's asynchronous initializerto make us an entity with the contentsfrom our Reality Composer Pro package.We specify which entity we want to load using its string name,and we give it the bundle that our package produces.It will throw if it can't find anythingin our Reality Composer Pro project by that name.realityKitContentBundle is a constant value thatwe autogenerate for you in your Reality Composer Pro package.This goes in a RealityView make closure.A RealityView is a new kind of SwiftUI view.It is your entrée into RealityKit.It's the bridge between the worldsof SwiftUI and RealityKit.We'll delve deeper into this RealityViewlater in this session.If there are USD assets you're using in your Xcode projectthat you're not adding to a Reality Composer Pro project,we strongly encourage you to put those assetsinto a Swift Package,with an .rkassets directory inside it, like this.Xcode compiles the .rkassets folder into a formatthat's faster to load at runtime.The entity we just loaded is actually the rootof a larger entity hierarchy.It has child entities and they in turn have child entities.It's everything we arranged in our Reality Composer Pro scene.If we wanted to address one of the entities lower downin the hierarchy, we could give it a namein Reality Composer Pro,and then at runtime, we could ask the sceneto find that entity by its name.Entities are a part of ECS,which stands for Entity Component System.ECS is what powers RealityKit and Reality Composer Pro.Let's take a step back and understand ECS.ECS has some close parallels to object-oriented programmingbut is different in some key ways.In the object-oriented programming world,the object has properties which are attributesthat define its nature,and it has its own functionality.You write these properties and functionsin a class that defines the object.In the ECS world, an entity is any thing you see in the scene.They can also be invisible.They don't hold attributes or data though.We put our data into components instead.Components can be added to or removed from entitiesat any time during app execution,which provides a way to dynamically changethe nature of an entity.A system is where our behavior lives.It has an update function that's called once per frame.That's where you put your ongoing logic.In your system, you query for all entities that havea certain component on them, or configuration of components,and then you perform some action and store the updated databack into those components.For a more in-depth discussion on ECS,check out the "Dive into RealityKit 2" sessionfrom 2021, and this year's"Build spatial experiences with RealityKit."Now let's learn about components.We're going to see how to add components to entitiesin our Reality Composer Pro project,and then we'll learn how to create custom componentsfor making location markers on our diorama.To add a component to an entity in Swift,you'd say entity.components.set()and provide the component value.To do the same in Reality Composer Pro,select the entity you wanteither in the viewport or in the hierarchy.

Then, at the bottom of the Inspector Panel,click the Add Component button to bring up a listof all RealityKit's available components.

We can add as many components to an entity as we want,and we can only add one of each type;it's a set.You'll also see any custom components you've madein this list as well.Let's see how we can use Reality Composer Proto create our own custom components.We're going to make those floating buttons that hoverover specific points on our terrain so you can select themto see more information about that spot.We'll prepare a lot of that UI and functionality in code,but I want to show you how to mark these entitiesin Reality Composer Pro as the positionsat which we want to show those floating buttons.To do this, we're going to add entities at locationsabove our terrain map, which will signify to the appthat these are the places we want to showour floating buttons.Then we'll create a point of interest componentto house our information about each place.Then we open the PointOfInterestComponent.swiftin Xcode to edit it, adding properties like a nameand a description.In Reality Composer Pro, we'll add our newPointOfInterestComponent to each of our new entities,and then we'll fill in the properties' values.Let's make our first location marker entity, Ribbon Beach,which is a place on Catalina Island.We click the plus menu and select Transformto make us a new invisible entity.We can name our entity Ribbon_Beach.Let's put this entity where Ribbon Beachactually is on the island.We click on the Add Component button,but this time, we select New Componentbecause we're going to make our own.

Let's give it a name, PointOfInterest.Now it shows up in the Inspector Paneljust like our other components do.

But what's this count property?Let's open our new component in Xcode.In Xcode, we see that Reality Composer Pro createdPointOfInterestComponent.swift for us.Reality Composer Pro projects are Swift packages,and the Swift code we just generatedlives here in the package.Looking at the template code, we see that that'swhere the count property came from.Let's have another property instead.We want each point of interestto know which map it's associated withso that when you change maps,we can fade out the old points of interestand fade in the appropriate ones.So we add an enumeration property,var region.Let's make our enum region up here......and give it two cases,since we're only building two maps right now:Catalina and Yosemite.It can serialize as a string.We also conform it to the Codable protocolso that Reality Composer Pro can see itand serialize instances of it.Back in Reality Composer Pro, the count property has gone awayand our new region property shows up.It has a default value of yosemitebecause that's what we initialized in the code,but we can override it here for this particular entity.If we override it, this value will only take effecton this particular entity.The rest of the point of interest componentswill have the default value of yosemiteunless we override them too.We're using our PointOfInterestComponentlike a signifier, a marker that we stick on these entities.These entities act like placeholdersfor where we'll put our SwiftUI buttons at runtime.We add our other Catalina Island points of interestthe same way we just added Ribbon Beach.Let's run our app and see what our new custom component does.

Oh! It doesn't do anything.That's because we haven't written any codeto handle these point of interest components yet.Let's do that.We have a new way of putting SwiftUI contentinto a RealityKit scene.This is called the Attachments API.We're going to combine attachments withour PointOfInterestComponent to create hovering buttonswith custom data at runtime.Let's first see this in code and then I'll walk you throughthe data flow.Attachments are a part of the RealityView.Let's first look at a simplified exampleto show the structure of a RealityViewso we can see how SwiftUI views get into a RealityKit scene.The RealityView initializer that we're going to usetakes three parameters:a make closure, an update closure,and an attachments ViewBuilder.Fleshing this out a little, let's add a bare-minimumimplementation of creating an Attachment View,a green SwiftUI button,and adding it to our RealityKit scene.In the Attachments ViewBuilder, we make a normal SwiftUI view;we can use view modifiers and gesturesand all the rest that SwiftUI gives us.We tag our View with any unique hashable.I've chosen to tag this button view with a fish emoji.Later, when SwiftUI invokes our update closure,our button View has become an entity.It's stored in the attachments parameter to this closure,and we fish it out using the tag we gave it before.We can then treat it like any other entity.We can add it as a child of any existing entity in our scene,or we can add it as a new top-level entityin the content's entities collection.And since it's become a regular entity,we can set its position so it shows up where we want in 3D,and we can add any components we want as well.Here's how data flows from one part of the RealityViewto another.Let's look at the three parametersto this RealityView initializer.The first is make, which is where you loadyour initial set-up scene from your Reality Composer Pro bundleas an entity and then add it to the RealityKit scene.The second is update,which is a closure that will be calledwhen there are changes to your view's state.Here, you can change things about your entities,like properties in their components, their position,and even add or remove entities from the scene.This update closure is not executed every frame.It's called whenever the SwiftUI view state changes.The third is the attachments ViewBuilder.This is where you can make SwiftUI viewsto put into your RealityKit scene.Your SwiftUI views start out in the attachments ViewBuilder,then they are delivered to you in the update closurein the attachments parameter.Here, you ask the attachments parameter if it has an entityfor you using the same tag you gave to your buttonin the attachments ViewBuilder.If there is one, it vends you a RealityKit entity.In your update closure, you set its 3D positionand add it to your RealityKit sceneso you can see it floating in space wherever you want.Here, I've added my button entityas a child of a sphere entity.I positioned it 0.2 meters above its parent.The make closure also has an attachments parameter.This one is for adding any attachments that you haveready to go at the time this view is first evaluated,because the make closure is only run once.Now that we have understood the general flow of a RealityView,let's get further into the update closure.The parameter to your make and update closuresis a RealityKitContent.When you add an entity to your RealityKit content,it becomes a top-level entity in your scene.Likewise, from your update function,adding an entity to your contentgives you a new top-level entity in your scene.While the make closure will only be called once,the update closure will be called more than once.If you create a new entity in your update closureand add it to your content there,you'll get duplicates of that entity,which might not be what you want.To guard against that, you should only add entitiesto your content that are created somewhere that's only run once.You don't need to check if the content.entitiesalready contains your entity.It's a no-op if you call add with the same entity twice,like a set.It's the same when you parent an entity to an existing entityin your scene -- it won't be added twice.Attachment entities are not created by you;they're created by the RealityViewfor each attachment view that you providein your attachments ViewBuilder.That means it's safe to add them to the contentin your update closure without checking if it's already there.So, that was how we'd write our attachments codeif we wanted to hardcode our points of interestin the attachments ViewBuilder.But since we want to let the datain our Reality Composer Pro project drive the experience,let's make it more flexible.That way, a designer or producer can createthe points of interest in the Reality Composer Pro project,and our code can accommodate whatever data they add.To make it data-driven, we need our code to readthe data that we set up in our Reality Composer Pro scene.We'll be creating our attachment views dynamically.High level, here's what we're going to do.In Reality Composer Pro, we already set upour placeholder entity for Ribbon Beach,and we'll do the same for the other points of interestthat we want to highlight in our diorama.We'll fill out all the info each one needs,like their name and which map they belong on.Now, in code, we'll query for those entitiesand create a new SwiftUI button for each one.In order to get SwiftUIto invoke our attachments ViewBuilderevery time we add a new button to our collection,we'll add the @State property wrapper to this collection.We'll serve those buttons up to the attachments ViewBuilder.Then finally, in the update closure of our RealityView,we'll receive our buttons as entitiesand add those new button entities to the scene.We'll add each one as a child of the marker entitieswe set up in Reality Composer Pro.Let's understand these six steps through a more detailed diagram,and then we'll look at the code.First, we add invisible entitiesin our Reality Composer Pro scene.We position our invisible entities where we wantour buttons to show up, on the x-, y-, and z-axes.We're making use of the Transform Component here,which all entities have by default.Then we add our PointOfInterestComponentto each of them.In our code, we get references to these entitiesby querying for all entities in the scene that havethe PointOfInterestComponent on them.The query returns the three invisible entitieswe set up in Reality Composer Pro.We create a new SwiftUI view for each oneand store them in a collection.To get our buttons into our RealityView,we'll make use of the SwiftUI view-updating flow.This means adding the property wrapper @Stateto the collection of buttons in our View.The @State property wrapper tells SwiftUIthat when we add items to this collection,SwiftUI should trigger a view update on our ImmersiveView.That will cause SwiftUI to evaluateour attachments ViewBuilder and our update closure again.The RealityView's attachments ViewBuilderis where we'll declare to SwiftUIthat we want these buttons to be made into entities.Our RealityView's update closure will be called next,and our buttons will be delivered to us as entities.They're no longer SwiftUI Views now.That's why we can add them to our entity hierarchy.In the update closure, we add our attachment entitiesto the scene, positioned floating aboveeach of our invisible entities.Now they will show up visuallywhen we look at our diorama scene.Let's see how each of these steps is done.First, we mark our invisible entitiesin our Reality Composer Pro scene.To find our entities that we marked,we'll make an EntityQuery.We'll use it to ask for all entities that havea PointOfInterestComponent on them.We'll then iterate through our QueryResultand create a new SwiftUI View for each entity in our scenethat has a PointOfInterestComponent on it.We'll fill it in with informationwe grab from the component,the data we entered in Reality Composer Pro.That view is going to be one of our attachments,so we put a tag on it.In this case, we're getting serious,so we'll use an ObjectIdentifier rather than a fish emoji.Here's the part where we make our collection of SwiftUI Views.We'll call it attachmentsProvidersince it will provide our attachmentsto the RealityView's attachments ViewBuilder.We'll then store our view in the attachmentsProvider.Let's take a look at that collection type.AttachmentsProvider has a dictionaryof attachment tags to views.We type-erased our view so we can putother kinds of views in there besides our LearnMoreView.We have a computed property called sortedTagViewPairsthat returns an array of tuples --tags and their corresponding views --in the same order every time.Then, in the attachments ViewBuilder,we'll ForEach over our collection of attachmentsthat we made.This tells SwiftUI that we want one viewfor each of the pairs we've given it,and we provide our views from our collection.We're letting the ObjectIdentifierdo double duty here as both an attachment tag for the view,and as an identifier for the ForEach structure.So, why didn't we just add a tag propertyto our PointOfInterestComponent instead?Attachment tags need to be unique,both for the ForEach structand the attachments mechanism to work.And since all the properties in our custom componentwill be shown in Reality Composer Pro's Inspector Panelwhen you add the component to an entity,that means the attachmentTag would show up there too.We don't want to burden ourselveswith having to remember to uniquify all the tagswhen we're adding each point of interestin Reality Composer Pro.But, conveniently for us, entities conformto the Identifiable protocol, so they have identifiersthat are unique automatically.We can get this identifier at runtime from the entitywithout needing to know it ahead of timewhen we're designing our scene in Reality Composer Pro.To have the attachmentTag propertynot show up in Reality Composer Pro,we use a technique that I call"design-time versus runtime components."We'll separate our data into two different components,one for design-time data that we want to arrangein Reality Composer Pro, and one for runtime datathat we will attach to those same entitiesdynamically at runtime.This is for properties that we don't want to showin our Inspector Panel in Reality Composer Pro.So we'll define a new component,PointOfInterestRuntimeComponent,and move our attachment tag inside it.Reality Composer Pro automatically buildsthe component UI for you based on what it readsin your Swift package.It inspects the Swift code in your packageand makes any codable components it findsavailable for you to use in your scenes.Here we're showing four components.Components A and Bare in our Xcode project, but they are not insidethe Reality Composer Pro package,so they won't be available for you to attachto your entities in Reality Composer Pro.Component C is inside the package but it is not codable,so Reality Composer Pro will ignore it.Of the four components shown here,only Component D will be shown in the listin Reality Composer Pro because it is within the Swift packageand it is a codable component.That one is our design-time component,while all the others may be used as runtime components.Design-time components are for housing simpler data,such as ints, strings, and SIMD values,things that 3D artists and designers will make use of.You'll see an error in your Xcode projectif you add a property to your custom componentthat's of a type that Reality Composer Pro won't serialize.Now, let's get back to our code.We'll first add our PointOfInterestruntime component to our entityand then use the runtime componentto help us match up our attachment entitieswith their corresponding points of interest on the diorama.Here's where our runtime component comes in.We're at the part where we're reading inour PointOfInterest entities and creating our attachment views.We queried for all our design-time components,and now we'll make a new corresponding runtime componentfor each of them.We store our attachmentTag in our runtime component,and we store our runtime component on that same entity.In this way, the design-time component is like a signifier.It tells our app that it wants an attachment made for it.The runtime component handles any other kinds of data we needduring app execution, but don't want to storein the design-time component.In our RealityView, we have one more stepbefore we see our attachment entities show up in our scene.Once we've provided our SwiftUI Viewsin the attachments ViewBuilder, SwiftUI will callour RealityView's update closure and give us our attachmentsas RealityKit entities.But if we just add them to the contentwithout positioning them, they'll all show up sittingat the origin of the scene, position 0, 0, 0.That's not where we want them.We want them to float above each point of intereston the terrain.We need to match up our attachment entitieswith our invisible point of interest entitiesthat we set up in Reality Composer Pro.The runtime component we put on the invisible entityhas our tag in it.That's how we'll match up which attachmentEntitygoes with each point of interest entity.We query for all our PointOf InterestRuntimeComponents,we get that runtime component from each entityreturned by the query,then we use the component's attachmentTag propertyto get our attachmentEntity from the attachments parameterto the update closure.Now we add our attachmentEntity to the content and position ithalf a meter above the point of interest entity.Let's run our app again and see what these look like.Hey, they look great!We can see each place name floating above the spotwhere we put them in our Reality Composer Pro project.Next let's find out how we play audiothat we set up in Reality Composer Pro.To set up something that plays audio in Reality Composer Pro,you can bring in an audio entity by clicking the plus button,selecting Audio, and then selecting Ambient Audio.This creates a regular invisible entitywith an AmbientAudioComponent on it.Let's name our entity OceanEmitterbecause we're going to use it to play ocean soundsfor Catalina Island.You need to add an audio file to your scene as well.Let's bring in our ocean sound.

You can preview your audio component by selecting a soundin the Preview menu of the componentin the Inspector Panel,but this won't automatically play the selected soundwhen the entity is loaded in your app.For that, we need to load the audio resourceand tell it to play.To play this sound, we'll get a reference to the entitythat we put the audio component on.We've named our entity OceanEmitter,so we'll find our entity by that name.We load the sound file using the AudioFileResource initializer,passing it the full path to the audio file resource primin our scene.We give it the name of the .usda file that contains itin our Reality Composer Pro project.In our case, that's our main scenenamed DioramaAssembled.usda.We create an audioPlaybackControllerby calling entity.prepareAudioso we can play, pause, and stop this sound.Now we're ready to call play on it.Here's the ocean sound playing in our app.The slider in our appmorphs between our two different terrain maps,Yosemite and Catalina Island.

Now that we've introduced audio into our scene,we're going to crossfade between two audio sources.We add a forest audio emitter the same way we addedour ocean emitter entity.Let's take a look at how we're morphing our terrainusing the slider,and we'll also include audio in this transition.We'll use a property from our Shader Graph materialto morph between the two terrains.Let's see how we do that.In Niels's session, he created this beautiful geometry modifierfor us using Reality Composer Pro's Shader Graph.Now we can hook it up to our sceneand drive some of the parameters at runtime.We want to connect this Shader Graph material with a slider.To do that, we need to promote an input node.Command-click on the node and select Promote.This tells the project that we intend to supply dataat runtime to this part of the material.We'll name this promoted node Progress,so we can address it by that name at runtime.We can now dynamically change this value in code.We get a reference to the entity that our material is on.Then we get its ModelComponent, which isthe RealityKit component that houses materials.From the ModelComponent, we get its first material.There's only one on this particular entity.We cast it to type ShaderGraphMaterial.Now, we can set a new value for our parameterwith name Progress.Finally, we store the material back onto the ModelComponent,and the ModelComponent back onto the terrain entity.Now we'll hook that up to our SwiftUI slider.Whenever the slider's value changes,we'll grab that value, which is in the range 0 to 1,and feed it into our ShaderGraphMaterial.Next, let's crossfade between the ambient audio tracksfor our two terrains.Because we've also put an AmbientAudioComponenton our two audio entities, ocean and forest,we can adjust how loud the sound playsusing the gain property on them.We'll query for all entities --all two at this point, our ocean and our forest --that have the AmbientAudioComponent on them.Plus, we added another custom componentcalled RegionSpecificComponent so we can mark the entitiesthat go with one region or the other.We get a mutable copy of our audioComponentbecause we're going to change it and store itback onto our entity.We call a function that calculateswhat the gain should be given a region and a sliderValue.We set the gain value onto the AmbientAudioComponent,and then we store the component back onto the entity.Let's see that in action.Great!When we move the slider, we can seeour Shader Graph material changing the geometryof the terrain map, and we can hearthe forest sound fading out and the ocean sound coming in.We covered a lot of information today.Let's recap.We've learned how to load our Reality Composer Pro contentinto our app in Xcode.We looked at how to create your own custom componentsin Reality Composer Pro.We explored how the SwiftUI Attachments API worksand how they are delivered to us as entities.We saw how to set up audio and then play that audio in code.And finally, we saw how to take a promoted material propertyand drive it from code.These workflows will help youbring your spatial experiences to life.I look forward to seeing all the amazing thingsyou'll build on our new platform.Thank you.♪

3:12 -Loading an entity

6:39 -Adding a component

12:21 -Attachments data flow

15:48 -Adding attachments

20:43 -Adding point of interest attachment entities

21:40 -AttachmentsProvider

22:31 -Design-time and Run-time components

25:38 -Adding a run-time component for each design-time component

26:19 -Adding and positioning the attachment entities

28:55 -Audio Playback

31:02 -Terrain material transition using the slider

31:57 -Audio transition using the slider

## Code Samples

```swift
RealityView
 { content 
in

    
do
 {
        
let
 entity 
=
 
try
 
await
 
Entity
(named: 
"DioramaAssembled"
, in: realityKitContentBundle)
        content.add(entity)
    } 
catch
 {
        
// Handle error

    }
}
```

```swift
let
 component 
=
 
MyComponent
()
entity.components.set(component)
```

```swift
RealityView
 { 
_
, 
_
 
in

    
// load entities from your Reality Composer Pro package bundle

} update: { content, attachments 
in

           
   
if
 
let
 attachmentEntity 
=
 attachments.entity(for: 
"🐠"
) {
     		content.add(attachmentEntity)
 	 }
           
} attachments: {
    
Button
 { 
...
 }
       .background(.green)
       .tag(
"🐠"
)
}
```

```swift
let
 myEntity 
=
 
Entity
()


RealityView
 { content, 
_
 
in

    
if
 
let
 entity 
=
 
try?
 
await
 
Entity
(named: 
"MyScene"
, in: realityKitContentBundle) {
        content.add(entity)
    }
} update: { content, attachments 
in


    
if
 
let
 attachmentEntity 
=
 attachments.entity(for: 
"🐠"
) {
        content.add(attachmentEntity)
    }

    content.add(myEntity)

} attachments: {
    
Button
 { 
...
 }
       .background(.green)
       .tag(
"🐠"
)
}
```

```swift
static
 
let
 markersQuery 
=
 
EntityQuery
(where: .has(
PointOfInterestComponent
.
self
))

@State
 
var
 attachmentsProvider 
=
 
AttachmentsProvider
()

rootEntity.scene
?
.performQuery(
Self
.markersQuery).forEach { entity 
in

  
guard
 
let
 pointOfInterest 
=
 entity.components[
PointOfInterestComponent
.
self
] 
else
 { 
return
 }
  
  
let
 attachmentTag: 
ObjectIdentifier
 
=
 entity.id

  
let
 view 
=
 
LearnMoreView
(name: pointOfInterest.name, description: pointOfInterest.description)
                           .tag(attachmentTag)

   attachmentsProvider.attachments[attachmentTag] 
=
 
AnyView
(view)
}
```

```swift
@Observable
 
final
 
class
 
AttachmentsProvider
 {
    
var
 attachments: [
ObjectIdentifier
: 
AnyView
] 
=
 [:]
    
var
 sortedTagViewPairs: [(tag: 
ObjectIdentifier
, view: 
AnyView
)] { 
...
 }
}


...



@State
 
var
 attachmentsProvider 
=
 
AttachmentsProvider
()


RealityView
 { 
_
, 
_
 
in


} update: { 
_
, 
_
 
in


} attachments: {
    
ForEach
(attachmentsProvider.sortedTagViewPairs, id: \.tag) { pair 
in

        pair.view
    }
}
```

```swift
// Design-time component


public
 
struct
 
PointOfInterestComponent
: 
Component
, 
Codable
 {
    
public
 
var
 region: 
Region
 
=
 .yosemite
    
public
 
var
 name: 
String
 
=
 
"Ribbon Beach"

    
public
 
var
 description: 
String
?
}


// Run-time component


public
 
struct
 
PointOfInterestRuntimeComponent
: 
Component
 {
    
public
 
let
 attachmentTag: 
ObjectIdentifier

}
```

```swift
static
 
let
 markersQuery 
=
 
EntityQuery
(where: .has(
PointOfInterestComponent
.
self
))

@State
 
var
 attachmentsProvider 
=
 
AttachmentsProvider
()

rootEntity.scene
?
.performQuery(
Self
.markersQuery).forEach { entity 
in

  
guard
 
let
 pointOfInterest 
=
 entity.components[
PointOfInterestComponent
.
self
] 
else
 { 
return
 }
  
  
let
 attachmentTag: 
ObjectIdentifier
 
=
 entity.id

  
let
 view 
=
 
LearnMoreView
(name: pointOfInterest.name, description: pointOfInterest.description)
                           .tag(attachmentTag)

   attachmentsProvider.attachments[attachmentTag] 
=
 
AnyView
(view)
   
let
 runtimeComponent 
=
 
PointOfInterestRuntimeComponent
(attachmentTag: attachmentTag)
   entity.components.set(runtimeComponent)
}
```

```swift
static
 
let
 runtimeQuery 
=
 
EntityQuery
(where: .has(
PointOfInterestRuntimeComponent
.
self
))


RealityView
 { 
_
, 
_
 
in


} update: { content, attachments 
in
 x
   
    rootEntity.scene
?
.performQuery(
Self
.runtimeQuery).forEach { entity 
in

        
guard
 
let
 component 
=
 entity.components[
PointOfInterestRuntimeComponent
.
self
],
              
let
 attachmentEntity 
=
 attachments.entity(for: component.attachmentTag) 
else
 { 
            
return
 
        }        
        content.add(attachmentEntity)
        attachmentEntity.setPosition([
0
, 
0.5
, 
0
], relativeTo: entity)
    }
} attachments: {
    
ForEach
(attachmentsProvider.sortedTagViewPairs, id: \.tag) { pair 
in

        pair.view
    }
}
```

```swift
func
 
playOceanSound
() {
        
    
guard
 
let
 entity 
=
 entity.findEntity(named: 
"OceanEmitter"
),
        
let
 resource 
=
 
try?
 
AudioFileResource
(named: 
"/Root/Resources/Ocean_Sounds_wav"
,
                                   from: 
"DioramaAssembled.usda"
,
                                   in: 
RealityContent
.realityContentBundle) 
else
 { 
return
 }
        
    
let
 audioPlaybackController 
=
 entity.prepareAudio(resource)
    audioPlaybackController.play()
}
```

```swift
@State
 
private
 
var
 sliderValue: 
Float
 
=
 
0.0



Slider
(value: 
$sliderValue
, in: (
0.0
)
...
(
1.0
))
    .onChange(of: sliderValue) { 
_
, 
_
 
in

        
guard
 
let
 terrain 
=
 rootEntity.findEntity(named: 
"DioramaTerrain"
),
                
var
 modelComponent 
=
 terrain.components[
ModelComponent
.
self
],
                
var
 shaderGraphMaterial 
=
 modelComponent.materials.first 
                    
as?
 
ShaderGraphMaterial
 
else
 { 
return
 }
        
do
 {
            
try
 shaderGraphMaterial.setParameter(name: 
"Progress"
, value: .float(sliderValue))
            modelComponent.materials 
=
 [shaderGraphMaterial]
            terrain.components.set(modelComponent)
        } 
catch
 { }
    }
}
```

```swift
@State
 
private
 
var
 sliderValue: 
Float
 
=
 
0.0


static
 
let
 audioQuery 
=
 
EntityQuery
(where: .has(
RegionSpecificComponent
.
self
) 
                                    
&&
 .has(
AmbientAudioComponent
.
self
))


Slider
(value: 
$sliderValue
, in: (
0.0
)
...
(
1.0
))
    .onChange(of: sliderValue) { 
_
, 
_
 
in

        
// ... Change the terrain material property ...

                                
        rootEntity
?
.scene
?
.performQuery(
Self
.audioQuery).forEach({ audioEmitter 
in

            
guard
 
var
 audioComponent 
=
 audioEmitter.components[
AmbientAudioComponent
.
self
],
                  
let
 regionComponent 
=
 audioEmitter.components[
RegionSpecificComponent
.
self
]
            
else
 { 
return
 }

            
let
 gain 
=
 regionComponent.region.gain(forSliderValue: sliderValue)
            audioComponent.gain 
=
 gain
            audioEmitter.components.set(audioComponent)
        })
    }
}
```

