# Wwdc2023 10089

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Discover Metal for immersive appsFind out how you can use Metal to render fully immersive experiences for visionOS. We'll show you how to set up a rendering session on the platform and create a basic render loop, and share how you can make your experience interactive by incorporating spatial input.Chapters0:06 -Intro1:50 -App Architecture5:58 -Render configuration11:29 -Render Loop13:00 -Render a frame17:28 -User input20:22 -Wrap-UpResourcesDrawing fully immersive content using MetalMetalHD VideoSD VideoRelated VideosWWDC23Build great games for spatial computingGo beyond the window with SwiftUIMeet ARKit for spatial computingWWDC21Explore HDR rendering with EDR

Find out how you can use Metal to render fully immersive experiences for visionOS. We'll show you how to set up a rendering session on the platform and create a basic render loop, and share how you can make your experience interactive by incorporating spatial input.

0:06 -Intro

1:50 -App Architecture

5:58 -Render configuration

11:29 -Render Loop

13:00 -Render a frame

17:28 -User input

20:22 -Wrap-Up

Drawing fully immersive content using Metal

Metal

HD VideoSD Video

HD Video

SD Video

Build great games for spatial computing

Go beyond the window with SwiftUI

Meet ARKit for spatial computing

Explore HDR rendering with EDR

Search this video…♪ Mellow instrumental hip-hop ♪♪Hey, I'm Pau Sastre Miguel, a Software Engineer at Apple.Today I'm going to talk about how to createimmersive experiences with Metal on xrOS.This year, with the launch of xrOS,you will be able to create immersive experienceswith familiar technologies in the Apple ecosystem.With RealityKit you will be able to create experiencesthat blend your virtual content with the real world.On the other hand,if your application will take the userinto a fully immersive experience,xrOS also enables you to completely replacethe real world content with your own virtual content.When creating fully immersive experiences,you have a few choices when it comes to your rendering method.You can still use RealityKit,or if you prefer, you can choose Metal and ARKit APIs.RecRoom is a great example of an applicationthat provides a fully immersive experienceusing CompositorServices to create a rendering session,Metal APIs to render frames,and ARKit to get world and hand tracking.They were able to bring support to all these technologiesthanks to the Unity editor.If you'd like to write your own engine,the CompositorServices API gives you accessto Metal rendering on xrOS.You can combine it with ARKit,which adds world tracking and hand tracking,to create a fully immersive experience.CompositorServices is the key to configure your engineto work on xrOS.I'll show you how to setup the render loopand then how to render one frame.Finally, I'll show you how to use ARKitto make your experience interactive.Start with the architecture of an xrOS app.You'll get the most out of today's sessionif you have previous experience with Metal APIsand Metal rendering techniques.If you haven't used Metal before,check out the code samples and documentationover in developer.apple.com/Metal.When you create immersive experiences on xrOS with Metal,you'll start with SwiftUI to create the applicationand the rendering session.After creating the rendering session,you can switch to a language you might be more familiar with,like C or C++, to define the inner parts of the engine.You start by creating a type that conformsto the SwiftUI app protocol.To conform to this protocol,you will define the list of scenes in your app.On xrOS, there are three main scenes types.The window type provides an experience similarto 2D platforms like macOS.The volume type renders content within its boundsand it coexists in the Shared Space with other applications.And ImmersiveSpace allows you to render content anywhere.Whenever you render fully immersive experienceswith Metal, you will choose the ImmersiveSpace type.ImmersiveSpace is a new SwiftUI Scene typeavailable on xrOS.It serves as the container for fully immersive experiences.To learn how to use ImmersiveSpace,check out the session "Go beyond the window with SwiftUI."When you create an ImmersiveSpace scene,your application provides the content by using a typethat conforms to the ImmersiveSpaceContent protocol.Often, when creating content for an ImmersiveSpace Scene,applications will use RealityKit.It uses CoreAnimation and MaterialX under the hood.But if instead, you'd like to use the power of Metalto render the content of your application,you've got another option.The CompositorServices API uses Metal and ARKitto provide immersive rendering capabilitiesto your application.The new CompositorServices API, introduced in xrOS,provides a Metal rendering interfaceto be able to render the contents of an ImmersiveSpace.With CompositorServices, applications can renderdirectly into the compositor server.It has a low IPC overhead to minimize latency,and it is built from the ground upto support both C and Swift APIs.When using CompositorServices,the ImmersiveSpaceContent is called CompositorLayer.To create a CompositorLayeryou will need to provide two parameters.The first one is theCompositorLayerConfiguration protocol.This protocol defines the behavior and capabilitiesof your rendering session.The second is the LayerRenderer.This is the interface to the layer rendering session.Your application will use this objectto schedule and render new frames.When writing an immersive experience with Metal,start by defining the app type.As the scene type, use ImmersiveSpace.For the content type, use a CompositorLayer.Once the CompositorLayer is ready to render content,the system will call the applicationwith the instance of the rendering session.Here is a good place to create the instanceof your custom engine.Now that you have the engine instance,you can create the render thread and run the render loopby calling start.One thing to take into accountwhen defining the scene list in your application,is that by default SwiftUI creates a window scene,even if the first scene in your app is an ImmersiveSpace.To change that default behavior,you can modify the info plist of your app.You can add the keyUIApplicationPreferred DefaultSceneSessionRoleto your application scene manifestto change the default scene type of your application.If you are using a space with a Compositor SpaceContent,you will useCPSceneSessionRole ImmersiveSpaceApplication.After setting up the application,and before getting in the render loop,you'll tell CompositorServices how to configurethe LayerRenderer.To provide a configuration to the CompositorLayer,you will create a new type that conforms to theCompositorLayerConfiguration protocol.This protocol allows you to modify the setupand some of the behavior of the rendering session.The CompositorLayerConfiguration provides you two parameters.The first one is the layer capabilities.It enables you to query what features are availableon the device.Use the capabilities to create a valid configuration.And the second parameter is the LayerRenderer Configuration.This type defines the configurationof your rendering session.With the configuration, you can definehow your engine maps its content into the layer,enable foveated rendering,and define the color management of your pipeline.Now, I'll talk about how each of these propertieswill affect your engine.The first one is foveated rendering.The main goal of this feature is to allow you to render contentat a higher pixel-per-degree densitywithout using a bigger texture size.In a regular display pipeline,the pixels are distributed linearly in a texture.xrOS optimizes this workflow by creating a mapthat defines what regions in a displaycan use a lower sampling rate.This helps reduce the power required to render your framewhile maintaining the visual fidelity of the display.Using foveation whenever possible is important,as it will result in a better visual experience.A great way to visualize how foveation affectsyour rendering pipeline is using Xcode's Metal Debugger.With Metal Debugger, you can inspect the target texturesand rasterization rate mapsbeing used in the render pipeline.This capture shows the contents of the texturewithout scaling for the rasterization rate map.You can notice the different sample rates by focusingin the regions of the texture that are more compressed.With the attachment viewer options in Metal Debugger,you can scale the image to visualize the final resultthat the display will show.Compositor provides the foveation mapusing an MTLRasterizationRateMap for each frame.It is a good practice to always checkif foveation is supported.This will change depending on the platform.For example, in the xrOS simulator,foveation is not available.To enable foveation,you can set isFoveationEnabled on the configuration.The second property is LayerRenderer layout.This property is one of the most important configurationsfor your engine.It defines how each display from the headsetgets mapped into the rendered content of your application.Each eye first maps into a Metal textureprovided by Compositor.Then Compositor provides the index of which sliceto use within that texture.And finally, Compositor provides the viewport to usewithin that texture slice.The LayerRenderer layout lets you choose different mappingsbetween the texture slice and viewport.With layered, Compositor will use one texturewith two slices and two viewports.With dedicated, Compositor will use two textureswith one slice and one viewport each.And finally with shared, Compositor will use one texture,one slice, and two different viewports for that slice.Choosing which layout to use will dependon how you set up your rendering pipeline.For example, with layered and shared, you will be ableto perform your rendering in one single pass,so you can optimize your rendering pipeline.With shared layout, it might be easierto port existing code baseswhere foveated rendering is not an option.Layered layout is the optimal layoutsince it allows you to render your scene in a single passwhile still maintaining foveated rendering.The last configuration property to discuss is color management.Compositor expects the content to be renderedwith extended linear display P3 color space.xrOS supports an EDR headroom of 2.0.That is two times the SDR range.By default, Compositor does not use a pixel formatthat is HDR renderable,but if your application supports HDR,you can specify rgba16Float in the layer configuration.If you want to know more about how to render HDR with EDR,checkout the session "Explore HDR rendering with EDR."To create a custom configuration in your application,start by defining a new type that conforms to theCompositorLayerConfiguration protocol.To conform to this protocol,add the makeConfiguration method.This method provides the layer capabilitiesand a configuration you can modify.To enable the three properties I mentioned before,first check if foveation is supported.Then check what layouts are supported in this device.With this information, you can set a valid layoutin the configuration.In some devices like the simulator,where the Compositor only renders one view,layered won't be available.For foveation, set it to true if the device supports it.And finally, set the colorFormat to rgba16Floatto be able to render HDR content.Returning to the code that created the Compositor layer,you can now add the configuration typeyou just created.Now that the rendering session is configured,you can set up the render loop.You'll start by using the LayerRenderer objectfrom the CompositorLayer.First, you'll load the resources and initialize any objectsthat your engine will need to render frames.Then check the state of the layer.If the layer is paused, wait until the layer is running.Once the layer is unblocked from the wait,check the layer state again.If the layer is running,you'll be able to render a frame.And once that frame is rendered,check the layer state again before rendering the next frame.If the layer state is invalidated,free the resources you created for the render loop.Now, it's time to define the main functionof the render_loop.Until now I've been using Swift since the ImmersiveSpace APIis only available in Swift.But from here I will switch to C to write the render loop.As I mentioned, the first step in the render loopis to allocate and initialize all the objects you'll needto render frames.You'll do this by calling the setup functionin your custom engine.Next, is the main section of the loop.The first step is to check the layerRenderer state.If the state is paused,the thread will sleep until the layerRenderer is running.If the layer state is running,the engine will render one frame.And finally, if the layer has been invalidated,the render loop will finish.The last step of the render_loop functionwill be to clear any used resources.Now that the app is going through the render loop,I'll explain how to render one frame.Rendering content in xrOS is alwaysfrom the point of view of the device.You can use ARKit to obtain the device orientationand translation.ARKit is already available on iOS,and now xrOS is introducing a whole new API,which has additional features that can help you createimmersive experiences.With ARKit, you can add world tracking, hand tracking,and other world sensing capabilitiesto your application.The new ARKit API is also built from the ground upto support C and Swift APIs,which will allow for an easier integrationwith existing rendering engines.To learn more about ARKit on xrOS,check out "Meet ARKit for spatial computing."Within the render loop, it's time to render one frame.When rendering a frame,Compositor defines two main sections.The first one is the update.Here's where you will doany work that is not input-latency critical.This can be things like updating the animations in your scene,updating your characters, or gathering inputsin your system like hand skeleton poses.The second section of the frame is the submission section.Here's where you will perform any latency-critical work.You'll also render any contentthat is headset-pose dependent here.In order to define the timing for each of those sections,Compositor provides a timing object.This diagram defines how the timing affectsthe different frame sections.The CPU and GPU tracks represent the work that is being doneby your application.And the Compositor track represents the work doneby the Compositor server to display your frame.The timing type from Compositor Servicesdefines three main time values.First is the optimal input time.That is the best time to query the latency-critical inputand start rendering your frame.Second is the rendering deadline.That is the time by when your CPU and GPU workto render a frame should be finished.And third is presentation time.That is the time when your frame will be presented on display.In the two sections of your frame,The update section should happen before the optimal input time.After the update, you will wait for the optimal input timebefore starting the frame submission.Then you will perform the frame submission,which will submit the render work to the GPU.It is important to notice that the CPU and GPU workneeds to finish before the rendering deadline,otherwise the Compositor server won't be able to use this frameand will use a previous one instead.Finally, on the rendering deadline,the Compositor server will composite this framewith the other layers in the system.Back to the render loop code,it's time to define the render_new_frame function.In your engine's render_new_frame function,you will first query a frame from the layerRenderer.With the frame object, you will be able to predictthe timing information.Use that timing information to scope the updateand submit intervals.Next, implement the update section.Define this section by calling the start and end updateon the frame.Inside, you will gather the device inputsand update the contents of the frame.Once the update is finished,wait for the optimal input time before starting the submission.After the wait, define the submission sectionby calling start and end submission.Inside this section, first query the drawable object.Similar to CAMetalLayer, the drawable objectcontains the target texture and the informationthat you will need to set up the render pipeline.Now that you have your drawable,you can get the final timing informationthat Compositor will use to render this frame.With the final timing, you can query the ar_pose.It is important to set the pose in the drawablesince it will be used by the Compositorto perform reprojection on the frame.Here I'm getting the poseby calling the get_ar_pose function in my engine object.But you will need to implement the contents of this functionusing the ARKit world tracking APIs.The last step of the function will be to encodeall the GPU work and submit the frame.Inside the submit_frame, use the drawableto render the contents of the frame as usual.Now that the render loop is rendering frames,it's time to make your immersive experience interactive.This video shows how RecRoom using Unityis already taking advantage of the ARKit and Compositor APIsto add interactivity to their application.There are two main input sources driving this interaction.ARKit's HandTracking is providing the hand skeletonto render virtual hands.And pinch events from the LayerRendererare driving the user interactions.In order to make the experience interactive,you'll first gather the user inputand then apply it to the contents of your scene.All this work will happen in the update section of the frame.There are two main input sources,the LayerRenderer and the ARKit HandTracking provider.With the LayerRenderer, you will get updates every timethe application receives a pinch event.These updates are exposed in the form of spatial events.These events contains three main properties.The phase will tell you if the event is active,if it finished, or if it got canceled.The selection ray will allow you to determinethe content of the scene that had the attentionwhen the event began.And the last event property is the manipulator pose.This is the pose of the pinchand gets updated every frame for the duration of the event.From the HandTracking API, you will be able to get skeletonfor both the left and right hands.Now, it's time to add input support in the code.Before gathering the input, you will decideif your application is rendering virtual handsor if it uses passthrough hands.Add the upperLimbVisibility scene modifierto the ImmersiveSpace to make the passthrough handsvisible or hidden.To access the spatial events, go back to where you definedthe CompositorLayer render handler.Here, register a block in the layerRendererto get updates every time there is a new spatial event.If you are writing your engine code in C,you'll map the SwiftUI spatial event to a C type.Inside the C code,you can now receive the C event collection.One thing to keep in mindwhen handling the spatial event updatesis that the updates get delivered in the main thread.This means that you will use some synchronization mechanismwhen reading and writing the events in your engine.Now that the events are stored in the engine,it's time to implement the gather input function.The first step is to create an objectto store the current input state for this frame.This input state will store the events that you receivedfrom the LayerRenderer.Make sure that you are accessing your internal storagein a safe way.As for the hand skeleton,you can use the hand tracking provider API from ARKitto get the latest hand anchors.And now that your application has input support,you have all the tools at your disposalto create fully immersive experiences on xrOS.To recap, with SwiftUI, you will define the application.With CompositorServices and Metal, you will set upthe render loop and display 3D content.And finally, with ARKit,you will be able to make your experience interactive.Thank you for watching!♪

♪ Mellow instrumental hip-hop ♪♪Hey, I'm Pau Sastre Miguel, a Software Engineer at Apple.

Today I'm going to talk about how to createimmersive experiences with Metal on xrOS.

This year, with the launch of xrOS,you will be able to create immersive experienceswith familiar technologies in the Apple ecosystem.

With RealityKit you will be able to create experiencesthat blend your virtual content with the real world.

On the other hand,if your application will take the userinto a fully immersive experience,xrOS also enables you to completely replacethe real world content with your own virtual content.

When creating fully immersive experiences,you have a few choices when it comes to your rendering method.

You can still use RealityKit,or if you prefer, you can choose Metal and ARKit APIs.

RecRoom is a great example of an applicationthat provides a fully immersive experienceusing CompositorServices to create a rendering session,Metal APIs to render frames,and ARKit to get world and hand tracking.

They were able to bring support to all these technologiesthanks to the Unity editor.

If you'd like to write your own engine,the CompositorServices API gives you accessto Metal rendering on xrOS.

You can combine it with ARKit,which adds world tracking and hand tracking,to create a fully immersive experience.

CompositorServices is the key to configure your engineto work on xrOS.

I'll show you how to setup the render loopand then how to render one frame.

Finally, I'll show you how to use ARKitto make your experience interactive.

Start with the architecture of an xrOS app.

You'll get the most out of today's sessionif you have previous experience with Metal APIsand Metal rendering techniques.

If you haven't used Metal before,check out the code samples and documentationover in developer.apple.com/Metal.

When you create immersive experiences on xrOS with Metal,you'll start with SwiftUI to create the applicationand the rendering session.

After creating the rendering session,you can switch to a language you might be more familiar with,like C or C++, to define the inner parts of the engine.

You start by creating a type that conformsto the SwiftUI app protocol.

To conform to this protocol,you will define the list of scenes in your app.

On xrOS, there are three main scenes types.

The window type provides an experience similarto 2D platforms like macOS.

The volume type renders content within its boundsand it coexists in the Shared Space with other applications.

And ImmersiveSpace allows you to render content anywhere.

Whenever you render fully immersive experienceswith Metal, you will choose the ImmersiveSpace type.

ImmersiveSpace is a new SwiftUI Scene typeavailable on xrOS.

It serves as the container for fully immersive experiences.

To learn how to use ImmersiveSpace,check out the session "Go beyond the window with SwiftUI."When you create an ImmersiveSpace scene,your application provides the content by using a typethat conforms to the ImmersiveSpaceContent protocol.

Often, when creating content for an ImmersiveSpace Scene,applications will use RealityKit.

It uses CoreAnimation and MaterialX under the hood.

But if instead, you'd like to use the power of Metalto render the content of your application,you've got another option.

The CompositorServices API uses Metal and ARKitto provide immersive rendering capabilitiesto your application.

The new CompositorServices API, introduced in xrOS,provides a Metal rendering interfaceto be able to render the contents of an ImmersiveSpace.

With CompositorServices, applications can renderdirectly into the compositor server.

It has a low IPC overhead to minimize latency,and it is built from the ground upto support both C and Swift APIs.

When using CompositorServices,the ImmersiveSpaceContent is called CompositorLayer.

To create a CompositorLayeryou will need to provide two parameters.

The first one is theCompositorLayerConfiguration protocol.

This protocol defines the behavior and capabilitiesof your rendering session.

The second is the LayerRenderer.

This is the interface to the layer rendering session.

Your application will use this objectto schedule and render new frames.

When writing an immersive experience with Metal,start by defining the app type.

As the scene type, use ImmersiveSpace.

For the content type, use a CompositorLayer.

Once the CompositorLayer is ready to render content,the system will call the applicationwith the instance of the rendering session.

Here is a good place to create the instanceof your custom engine.

Now that you have the engine instance,you can create the render thread and run the render loopby calling start.

One thing to take into accountwhen defining the scene list in your application,is that by default SwiftUI creates a window scene,even if the first scene in your app is an ImmersiveSpace.

To change that default behavior,you can modify the info plist of your app.

You can add the keyUIApplicationPreferred DefaultSceneSessionRoleto your application scene manifestto change the default scene type of your application.

If you are using a space with a Compositor SpaceContent,you will useCPSceneSessionRole ImmersiveSpaceApplication.

After setting up the application,and before getting in the render loop,you'll tell CompositorServices how to configurethe LayerRenderer.

To provide a configuration to the CompositorLayer,you will create a new type that conforms to theCompositorLayerConfiguration protocol.

This protocol allows you to modify the setupand some of the behavior of the rendering session.

The CompositorLayerConfiguration provides you two parameters.

The first one is the layer capabilities.

It enables you to query what features are availableon the device.

Use the capabilities to create a valid configuration.

And the second parameter is the LayerRenderer Configuration.

This type defines the configurationof your rendering session.

With the configuration, you can definehow your engine maps its content into the layer,enable foveated rendering,and define the color management of your pipeline.

Now, I'll talk about how each of these propertieswill affect your engine.

The first one is foveated rendering.

The main goal of this feature is to allow you to render contentat a higher pixel-per-degree densitywithout using a bigger texture size.

In a regular display pipeline,the pixels are distributed linearly in a texture.

xrOS optimizes this workflow by creating a mapthat defines what regions in a displaycan use a lower sampling rate.

This helps reduce the power required to render your framewhile maintaining the visual fidelity of the display.

Using foveation whenever possible is important,as it will result in a better visual experience.

A great way to visualize how foveation affectsyour rendering pipeline is using Xcode's Metal Debugger.

With Metal Debugger, you can inspect the target texturesand rasterization rate mapsbeing used in the render pipeline.

This capture shows the contents of the texturewithout scaling for the rasterization rate map.

You can notice the different sample rates by focusingin the regions of the texture that are more compressed.

With the attachment viewer options in Metal Debugger,you can scale the image to visualize the final resultthat the display will show.

Compositor provides the foveation mapusing an MTLRasterizationRateMap for each frame.

It is a good practice to always checkif foveation is supported.

This will change depending on the platform.

For example, in the xrOS simulator,foveation is not available.

To enable foveation,you can set isFoveationEnabled on the configuration.

The second property is LayerRenderer layout.

This property is one of the most important configurationsfor your engine.

It defines how each display from the headsetgets mapped into the rendered content of your application.

Each eye first maps into a Metal textureprovided by Compositor.

Then Compositor provides the index of which sliceto use within that texture.

And finally, Compositor provides the viewport to usewithin that texture slice.

The LayerRenderer layout lets you choose different mappingsbetween the texture slice and viewport.

With layered, Compositor will use one texturewith two slices and two viewports.

With dedicated, Compositor will use two textureswith one slice and one viewport each.

And finally with shared, Compositor will use one texture,one slice, and two different viewports for that slice.

Choosing which layout to use will dependon how you set up your rendering pipeline.

For example, with layered and shared, you will be ableto perform your rendering in one single pass,so you can optimize your rendering pipeline.

With shared layout, it might be easierto port existing code baseswhere foveated rendering is not an option.

Layered layout is the optimal layoutsince it allows you to render your scene in a single passwhile still maintaining foveated rendering.

The last configuration property to discuss is color management.

Compositor expects the content to be renderedwith extended linear display P3 color space.

xrOS supports an EDR headroom of 2.0.

That is two times the SDR range.

By default, Compositor does not use a pixel formatthat is HDR renderable,but if your application supports HDR,you can specify rgba16Float in the layer configuration.

If you want to know more about how to render HDR with EDR,checkout the session "Explore HDR rendering with EDR."To create a custom configuration in your application,start by defining a new type that conforms to theCompositorLayerConfiguration protocol.

To conform to this protocol,add the makeConfiguration method.

This method provides the layer capabilitiesand a configuration you can modify.

To enable the three properties I mentioned before,first check if foveation is supported.

Then check what layouts are supported in this device.

With this information, you can set a valid layoutin the configuration.

In some devices like the simulator,where the Compositor only renders one view,layered won't be available.

For foveation, set it to true if the device supports it.

And finally, set the colorFormat to rgba16Floatto be able to render HDR content.

Returning to the code that created the Compositor layer,you can now add the configuration typeyou just created.

Now that the rendering session is configured,you can set up the render loop.

You'll start by using the LayerRenderer objectfrom the CompositorLayer.

First, you'll load the resources and initialize any objectsthat your engine will need to render frames.

Then check the state of the layer.

If the layer is paused, wait until the layer is running.

Once the layer is unblocked from the wait,check the layer state again.

If the layer is running,you'll be able to render a frame.

And once that frame is rendered,check the layer state again before rendering the next frame.

If the layer state is invalidated,free the resources you created for the render loop.

Now, it's time to define the main functionof the render_loop.

Until now I've been using Swift since the ImmersiveSpace APIis only available in Swift.

But from here I will switch to C to write the render loop.

As I mentioned, the first step in the render loopis to allocate and initialize all the objects you'll needto render frames.

You'll do this by calling the setup functionin your custom engine.

Next, is the main section of the loop.

The first step is to check the layerRenderer state.

If the state is paused,the thread will sleep until the layerRenderer is running.

If the layer state is running,the engine will render one frame.

And finally, if the layer has been invalidated,the render loop will finish.

The last step of the render_loop functionwill be to clear any used resources.

Now that the app is going through the render loop,I'll explain how to render one frame.

Rendering content in xrOS is alwaysfrom the point of view of the device.

You can use ARKit to obtain the device orientationand translation.

ARKit is already available on iOS,and now xrOS is introducing a whole new API,which has additional features that can help you createimmersive experiences.

With ARKit, you can add world tracking, hand tracking,and other world sensing capabilitiesto your application.

The new ARKit API is also built from the ground upto support C and Swift APIs,which will allow for an easier integrationwith existing rendering engines.

To learn more about ARKit on xrOS,check out "Meet ARKit for spatial computing."Within the render loop, it's time to render one frame.

When rendering a frame,Compositor defines two main sections.

The first one is the update.

Here's where you will doany work that is not input-latency critical.

This can be things like updating the animations in your scene,updating your characters, or gathering inputsin your system like hand skeleton poses.

The second section of the frame is the submission section.

Here's where you will perform any latency-critical work.

You'll also render any contentthat is headset-pose dependent here.

In order to define the timing for each of those sections,Compositor provides a timing object.

This diagram defines how the timing affectsthe different frame sections.

The CPU and GPU tracks represent the work that is being doneby your application.

And the Compositor track represents the work doneby the Compositor server to display your frame.

The timing type from Compositor Servicesdefines three main time values.

First is the optimal input time.

That is the best time to query the latency-critical inputand start rendering your frame.

Second is the rendering deadline.

That is the time by when your CPU and GPU workto render a frame should be finished.

And third is presentation time.

That is the time when your frame will be presented on display.

In the two sections of your frame,The update section should happen before the optimal input time.

After the update, you will wait for the optimal input timebefore starting the frame submission.

Then you will perform the frame submission,which will submit the render work to the GPU.

It is important to notice that the CPU and GPU workneeds to finish before the rendering deadline,otherwise the Compositor server won't be able to use this frameand will use a previous one instead.

Finally, on the rendering deadline,the Compositor server will composite this framewith the other layers in the system.

Back to the render loop code,it's time to define the render_new_frame function.

In your engine's render_new_frame function,you will first query a frame from the layerRenderer.

With the frame object, you will be able to predictthe timing information.

Use that timing information to scope the updateand submit intervals.

Next, implement the update section.

Define this section by calling the start and end updateon the frame.

Inside, you will gather the device inputsand update the contents of the frame.

Once the update is finished,wait for the optimal input time before starting the submission.

After the wait, define the submission sectionby calling start and end submission.

Inside this section, first query the drawable object.

Similar to CAMetalLayer, the drawable objectcontains the target texture and the informationthat you will need to set up the render pipeline.

Now that you have your drawable,you can get the final timing informationthat Compositor will use to render this frame.

With the final timing, you can query the ar_pose.

It is important to set the pose in the drawablesince it will be used by the Compositorto perform reprojection on the frame.

Here I'm getting the poseby calling the get_ar_pose function in my engine object.

But you will need to implement the contents of this functionusing the ARKit world tracking APIs.

The last step of the function will be to encodeall the GPU work and submit the frame.

Inside the submit_frame, use the drawableto render the contents of the frame as usual.

Now that the render loop is rendering frames,it's time to make your immersive experience interactive.

This video shows how RecRoom using Unityis already taking advantage of the ARKit and Compositor APIsto add interactivity to their application.

There are two main input sources driving this interaction.

ARKit's HandTracking is providing the hand skeletonto render virtual hands.

And pinch events from the LayerRendererare driving the user interactions.

In order to make the experience interactive,you'll first gather the user inputand then apply it to the contents of your scene.

All this work will happen in the update section of the frame.

There are two main input sources,the LayerRenderer and the ARKit HandTracking provider.

With the LayerRenderer, you will get updates every timethe application receives a pinch event.

These updates are exposed in the form of spatial events.

These events contains three main properties.

The phase will tell you if the event is active,if it finished, or if it got canceled.

The selection ray will allow you to determinethe content of the scene that had the attentionwhen the event began.

And the last event property is the manipulator pose.

This is the pose of the pinchand gets updated every frame for the duration of the event.

From the HandTracking API, you will be able to get skeletonfor both the left and right hands.

Now, it's time to add input support in the code.

Before gathering the input, you will decideif your application is rendering virtual handsor if it uses passthrough hands.

Add the upperLimbVisibility scene modifierto the ImmersiveSpace to make the passthrough handsvisible or hidden.

To access the spatial events, go back to where you definedthe CompositorLayer render handler.

Here, register a block in the layerRendererto get updates every time there is a new spatial event.

If you are writing your engine code in C,you'll map the SwiftUI spatial event to a C type.

Inside the C code,you can now receive the C event collection.

One thing to keep in mindwhen handling the spatial event updatesis that the updates get delivered in the main thread.

This means that you will use some synchronization mechanismwhen reading and writing the events in your engine.

Now that the events are stored in the engine,it's time to implement the gather input function.

The first step is to create an objectto store the current input state for this frame.

This input state will store the events that you receivedfrom the LayerRenderer.

Make sure that you are accessing your internal storagein a safe way.

As for the hand skeleton,you can use the hand tracking provider API from ARKitto get the latest hand anchors.

And now that your application has input support,you have all the tools at your disposalto create fully immersive experiences on xrOS.

To recap, with SwiftUI, you will define the application.

With CompositorServices and Metal, you will set upthe render loop and display 3D content.

And finally, with ARKit,you will be able to make your experience interactive.

Thank you for watching!♪

4:45 -App architecture

10:32 -CompositorLayer Configuration

12:20 -Render loop

15:56 -Render new frame

18:57 -App architecture + input support

18:57 -Push spatial events

19:57 -Gather inputs

## Code Samples

```swift
@main


struct
 
MyApp
: 
App
 {
    
var
 body: 
some
 
Scene
 {
        
ImmersiveSpace
 {
            
CompositorLayer
 { layerRenderer 
in

                
let
 engine 
=
 my_engine_create(layerRenderer)
                
let
 renderThread 
=
 
Thread
 {
                    my_engine_render_loop(engine)
                }
                renderThread.name 
=
 
"Render Thread"

                renderThread.start()
            }
        }
    }
}
```

```swift
// CompositorLayer configuration



struct
 
MyConfiguration
: 
CompositorLayerConfiguration
 {
    
func
 
makeConfiguration
(
capabilities
: 
LayerRenderer
.
Capabilities
,
                           
configuration
: 
inout
 
LayerRenderer
.
Configuration
) {

        
let
 supportsFoveation 
=
 capabilities.supportsFoveation
        
let
 supportedLayouts 
=
 capabilities.supportedLayouts(options: supportsFoveation 
?

                                                             [.foveationEnabled] : [])

        configuration.layout 
=
 supportedLayouts.contains(.layered) 
?
 .layered : .dedicated

        configuration.isFoveationEnabled 
=
 supportsFoveation

        
// HDR support

        configuration.colorFormat 
=
 .rgba16Float
   }
}
```

```swift
void
 my_engine_render_loop(my_engine *engine) {
    my_engine_setup_render_pipeline(engine);

    
bool
 is_rendering = 
true
;
    
while
 (is_rendering) 
@autoreleasepool
 {
        
switch
 (cp_layer_renderer_get_state(engine->layer_renderer)) {
            
case
 cp_layer_renderer_state_paused:
                cp_layer_renderer_wait_until_running(engine->layer_renderer);
                
break
;
            
case
 cp_layer_renderer_state_running:
                my_engine_render_new_frame(engine);
                
break
;
            
case
 cp_layer_renderer_state_invalidated:
                is_rendering = 
false
;
                
break
;
        }
    }

    my_engine_invalidate(engine);
}
```

```swift
void
 my_engine_render_new_frame(my_engine *engine) {
    
    cp_frame_t frame = cp_layer_renderer_query_next_frame(engine->layer_renderer);
    
if
 (frame == nullptr) { 
return
; }
    
    cp_frame_timing_t timing = cp_frame_predict_timing(frame);
    
if
 (timing == nullptr) { 
return
; }

    cp_frame_start_update(frame);

    my_input_state input_state = my_engine_gather_inputs(engine, timing);
    my_engine_update_frame(engine, timing, input_state);

    cp_frame_end_update(frame);

    
// Wait until the optimal time for querying the input

    cp_time_wait_until(cp_frame_timing_get_optimal_input_time(timing));

    cp_frame_start_submission(frame);

    cp_drawable_t drawable = cp_frame_query_drawable(frame);
    
if
 (drawable == nullptr) { 
return
; }

    cp_frame_timing_t final_timing = cp_drawable_get_frame_timing(drawable);
    ar_pose_t pose = my_engine_get_ar_pose(engine, final_timing);
    cp_drawable_set_ar_pose(drawable, pose);

    my_engine_draw_and_submit_frame(engine, frame, drawable);

    cp_frame_end_submission(frame);
}
```

```swift
@main


struct
 
MyApp
: 
App
 {
    
var
 body: 
some
 
Scene
 {
        
ImmersiveSpace
 {
            
CompositorLayer
(configuration: 
MyConfiguration
()) { layerRenderer 
in

                
let
 engine 
=
 my_engine_create(layerRenderer)
                
let
 renderThread 
=
 
Thread
 {
                    my_engine_render_loop(engine)
                }
                renderThread.name 
=
 
"Render Thread"

                renderThread.start()
                layerRenderer.onSpatialEvent 
=
 { eventCollection 
in

                    
var
 events 
=
 eventCollection.map { my_spatial_event(
$0
) }
                    my_engine_push_spatial_events(engine, 
&
events, events.count)
                }
            }
        }
        .upperLimbVisibility(.hidden)
    }
}
```

```swift
void
 my_engine_push_spatial_events(my_engine *engine,
                                   my_spatial_event *spatial_event_collection,
                                   size_t event_count) {
    os_unfair_lock_lock(&engine->input_event_lock);
    
    
// Copy events into an internal queue

    
    os_unfair_lock_unlock(&engine->input_event_lock);
}
```

```swift
my_input_state my_engine_gather_inputs(my_engine *engine,
                                       cp_frame_timing_t timing) {
    my_input_state input_state = my_input_state_create();

    os_unfair_lock_lock(&engine->input_event_lock);
    input_state.current_pinch_collection = my_engine_pop_spatial_events(engine);
    os_unfair_lock_unlock(&engine->input_event_lock);

    ar_hand_tracking_provider_get_latest_anchors(engine->hand_tracking_provider,
                                                 input_state.left_hand,
                                                 input_state.right_hand);

    
return
 input_state;
}
```

