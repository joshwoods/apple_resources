# Wwdc2023 10082

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Meet ARKit for spatial computingDiscover how you can use ARKit's tracking and scene understanding features to develop a whole new universe of immersive apps and games. Learn how visionOS and ARKit work together to help you create apps that understand a person's surroundings — all while preserving privacy. Explore the latest updates to the ARKit API and follow along as we demonstrate how to take advantage of hand tracking and scene geometry in your apps.ResourcesHD VideoSD VideoRelated VideosWWDC23Build spatial experiences with RealityKitDevelop your first immersive appDiscover Metal for immersive appsEnhance your spatial computing app with RealityKitEvolve your ARKit app for spatial experiencesMeet SwiftUI for spatial computingOptimize app power and performance for spatial computingWhat’s new in App Store Connect

Discover how you can use ARKit's tracking and scene understanding features to develop a whole new universe of immersive apps and games. Learn how visionOS and ARKit work together to help you create apps that understand a person's surroundings — all while preserving privacy. Explore the latest updates to the ARKit API and follow along as we demonstrate how to take advantage of hand tracking and scene geometry in your apps.

HD VideoSD Video

HD Video

SD Video

Build spatial experiences with RealityKit

Develop your first immersive app

Discover Metal for immersive apps

Enhance your spatial computing app with RealityKit

Evolve your ARKit app for spatial experiences

Meet SwiftUI for spatial computing

Optimize app power and performance for spatial computing

What’s new in App Store Connect

Search this video…♪ Mellow instrumental hip-hop ♪♪Ryan Taylor: Hello! My name is Ryan.Conner Brooks: And I'm Conner.Ryan: In this session, we are going to introduce youto ARKit for spatial computing.We will discuss the critical role that it playson this new platform and how you can leverage itto build the next generation of apps.ARKit uses sophisticated computer vision algorithmsto construct an understanding of the world around you,as well as your movements.We first introduced this technology in iOS 11as a way for developers to createamazing augmented reality experiencesthat you can use in the palm of your hand.On this platform, ARKit has maturedinto a full-blown system service,rebuilt from the ground up with a new real-time foundation.ARKit is deeply woven into the fabricof the entire operating system, powering everythingfrom interacting with a window, to playing an immersive game.As part of this journey, we have also given our APIa complete overhaul.The new design is a result of everythingthat we learned on iOS, plus the unique needsof spatial computing,and we think you are going to love it.ARKit provides a variety of powerful featuresthat you can combine to do incredible things,such as place virtual content on a table.You can reach out and touch the content,as if it is really there, and then watchas the content interacts with the real world.It is truly a magical experience.Now that you have seen a glimpse of what can be accomplishedusing ARKit on this new platform,let me walk you through our agenda.We will begin with an overview of the fundamental conceptsand building blocks that make up our API.Next, we will dive into world tracking,which is essential for placing virtual contentrelative to the real world.Then, we will explore our scene understanding features,which provide useful information about your surroundings.After that, we will introduce you to our newest feature,hand tracking, an exciting new addition that you can leveragefor placing virtual content relative to your handsor building other types of bespoke interactions.And lastly, we will come full circle and take a lookat a practical application of some of these featuresby examining code from the video that we showed you a moment ago.All right, let's get started!Our new API has been meticulously craftedin two invigorating flavors, modern Swift and classic C.All ARKit features are now provided à la carte.We wanted developers to have as much flexibility as possible,so that you can simply pick and choose what you needto build your experience.Access to ARKit data has been designedwith a privacy-first approach.We have put safeguards into placeto protect people's privacy,while also maintaining simplicity for developers.The API consists of three fundamental building blocks:sessions,data providers,and anchors.Let's begin with anchors and then work our wayback up to sessions.An anchor represents a position and orientationin the real world.All anchors include a unique identifier,as well as a transform.Some types of anchors are also trackable.When a trackable anchor is not being tracked,you should hide any virtual contentthat you have anchored with it.A data provider represents an individual ARKit feature.Data providers allow you to poll foror observe data updates, such as anchor changes.Different types of data providersprovide different types of data.A session represents a combined set of ARKit featuresthat you would like to use togetherfor a particular experience.You run a session by providing it with a set of data providers.Once the session is running,the data providers will begin receiving data.Updates arrive asynchronously and at different frequencies,depending on the type of data.Let's move on now and talk about privacyand how your app gets access to ARKit data.Privacy is a fundamental human right.It is also one of our core values.ARKit's architecture and API have been thoughtfully designedto protect people's privacy.In order for ARKit to construct an understandingof the world around you, the device has many camerasand other types of sensors.Data from these sensors, such as camera frames,is never sent to client space.Instead, sensor data is sent to ARKit's daemonfor secure processing by our algorithms.The resulting data that is produced by these algorithmsis then carefully curated before being forwardedto any clients that are requesting the data,such as your app.There are a few prerequisites to accessing ARKit data.First, your app must enter a Full Space.ARKit does not send data to apps that are in the Shared Space.Second, some types of ARKit data require permission to access.If the person does not grant permission,then we will not send that type of data to your app.To facilitate this, ARKit providesa convenient authorization API for handling permission.Using your session, you can request authorizationfor the types of data that you would like to access.If you do not do this, ARKit will automatically promptthe person for permission when you run the session,if necessary.Here, we are requesting access to hand tracking data.You can batch all of the authorization typesthat you need together in a single request.Once we have the authorization results,we iterate over them and check the statusfor each authorization type.If the person has granted permission,the status will be allowed.Attempting to run a session with a data providerthat provides data that the person has denied access towill result in the session failing.Now, let's take a closer look at each of the featuresthat ARKit supports on this platform,starting with world tracking.World tracking allows you to anchor virtual contentin the real world.ARKit tracks the device's movementin six degrees of freedom and updates each anchor,so that they stay in the same placerelative to your surroundings.The type of DataProvider that world tracking usesis called WorldTrackingProvider,and it gives you several important capabilities.It allows you to add WorldAnchors,which ARKit will then update to remain fixedrelative to people's surroundingsas the device moves around.WorldAnchors are an essential toolfor virtual content placement.Any WorldAnchors that you add are automatically persistedacross app launches and reboots.If this behavior is undesirable for the experiencethat you are building, you can simply removethe anchors when you are done with them,and they will no longer be persisted.It is important to note that there are some caseswhere persistence will not be available.You can also use a WorldTrackingProviderto get the device's pose relative to the app origin,which is necessary if you are doing your own renderingusing Metal.Let's begin by taking a closer look at what a WorldAnchor isand why you would want to use one.A WorldAnchor is a TrackableAnchorwith an initializer that takes a transform,which is the position and orientationthat you would like to place the anchor at,relative to the app's origin.We have prepared an example to help visualize the differencebetween virtual content that is not anchoredversus content that is anchored.Here we have two cubes.The blue cube on the left is not being updated by a WorldAnchor,while the red cube on the right is being updatedby a WorldAnchor.Both cubes were placed relative to the app's originwhen the app was launched.As the device moves around,both cubes remain where they were placed.You can press and hold the crown to recenter the app.When recentering occurs,the app's origin will be moved to your current location.Notice that the blue cube, which is not anchored,relocates to maintain its relative placementto the app's origin;while the red cube, which is anchored,remains fixed relative to the real world.Let's take a look at how WorldAnchor persistence works.As the device moves around,ARKit builds a map of your surroundings.When you add WorldAnchors, we insert them into the mapand automatically persist them for you.Only WorldAnchor identifiers and transforms are persisted.No other data, such as your virtual content,is included.It is up to you to maintain a mappingof WorldAnchor identifiers to any virtual contentthat you associate with them.Maps are location based, so when you take your deviceto a new location --for instance, from home to the office --the map of your home will be unloaded,and then a different map will be localized for the office.Any anchors that you add at this new locationwill go into that map.When you leave the office at the end of the dayand head home, the map that ARKithas been building at the office, along with any anchorsthat you placed there, will be unloaded.Once again, though, we have beenautomatically persisting the map along with your anchors.Upon returning home,ARKit will recognize that the location has changed,and we will begin the process of relocalizingby checking for an existing map for this location.If we find one, we will localize with it,and all of the anchors that you previously added at homewill become tracked once again.Let's move on to the device pose.Along with adding and removing WorldAnchors,you can also use a WorldTrackingProviderto get the pose of the device.The pose is the position and orientation of the devicerelative to the app's origin.Querying the pose is required if you are doingyour own rendering with Metal and CompositorServicesin a fully immersive experience.This query is relatively expensive.Exercise caution when querying for the device posefor other types of app logic, such as content placement.Let's quickly walk through a simplified rendering exampleto demonstrate how you can provide device posesfrom ARKit to CompositorServices.We have a Renderer struct that will hold our session,world tracking provider, and latest pose.When initializing the Renderer, we start by creating a session.Next, we create a world tracking provider,which we will use to query for the device posewhen we render each frame.Now, we can go ahead and run our sessionwith any data providers that we need.In this case, we are only using a world tracking provider.We also create a pose to avoid allocatingin the render function.Let's jump over to our render function now,which we will be calling at frame rate.Using the drawable from CompositorServices,we fetch the target render time.Next, we use the target render timeto query for the pose of the device.If successful, we can extract a transform of the poserelative to the app's origin.This is the transform to use for rendering your content.Lastly, before we submit the frame for compositing,we set the pose on the drawable,so that the compositor knows which pose we usedto render content for the frame.For more information on doing your own rendering,see the dedicated session for using Metalto create immersive apps.Additionally, there is a great sessionon spatial computing performance considerationsthat we encourage you to check out as well.Next, let's take a look at scene understanding.Scene understanding is a category of featuresthat inform you about your surroundings in different ways.Let's begin with plane detection.Plane detection provides anchors for horizontaland vertical surfaces that ARKit detects in the real world.The type of DataProvider that plane detection usesis called PlaneDetectionProvider.As planes are detected in your surroundings,they are provided to you in the form of PlaneAnchors.PlaneAnchors can be used to facilitate content placement,such as placing a virtual object on a table.Additionally, you can use planes for physics simulationswhere basic, flat geometry, such as a floor or wall,is sufficient.Each PlaneAnchor includes an alignment,which is horizontal or vertical; the geometry of the plane;and a semantic classification.Planes can be classified as a varietyof different types of surfaces, such as floor or table.If we are unable to identify a particular surface,the provided classification will be marked as either unknown,undetermined, or not available, depending on the circumstances.Now, let's move on to scene geometry.Scene geometry provides anchors containing a polygonal meshthat estimates the shape of the real world.The type of DataProvider that scene geometry usesis called SceneReconstructionProvider.As ARKit scans the world around you,we reconstruct your surroundings as a subdivided mesh,which is then provided to you in the form of MeshAnchors.Like PlaneAnchors, MeshAnchors can be usedto facilitate content placement.You can also achievehigher-fidelity physics simulationsin cases where you need virtual content to interactwith objects that are not just simple, flat surfaces.Each MeshAnchor includes geometry of the mesh.This geometry contains vertices, normals, faces,and semantic classifications, which are per face.Mesh faces can be classified as a varietyof different types of objects.If we are unable to identify a particular object,the provided classification will be none.Lastly, let's take a look at image tracking.Image tracking enables you to detect 2D imagesin the real world.The type of DataProvider that image tracking usesis called ImageTrackingProvider.You configure ImageTrackingProviderwith a set of ReferenceImages that you want to detect.These ReferenceImages can be created in a few different ways.One option is to load them from an AR resource groupin your project's asset catalog.Alternatively, you can also initializea ReferenceImage yourself by providing a CVPixelBufferor CGImage.When an image is detected,ARKit provides you with an ImageAnchor.ImageAnchors can be used to place contentat known, statically placed images.For instance, you can display some informationabout a movie next to a movie poster.ImageAnchors are TrackableAnchorsthat include an estimated scale factor,which indicates how the size of the detected imagecompares to the physical size that you specifiedand the ReferenceImage that the anchor corresponds to.Now, to tell you about our new feature, hand tracking,and then walk you through the example,here is Conner.Conner: Howdy. Let's take a look at hand tracking,a brand-new addition to ARKit.Hand tracking provides you with anchorscontaining skeletal data for each of your hands.The type of DataProvider that hand tracking usesis called HandTrackingProvider.When your hands are detected, they are provided to youin the form of HandAnchors.A HandAnchor is a TrackableAnchor.HandAnchors include a skeleton and a chirality.The chirality tells us whether this is a left or a right hand.A HandAnchor's transform is the wrist's transformrelative to the app origin.The skeleton consists of joints, which can be queried by name.A joint contains its parent joint; its name;a localTransform, which is relative to its parent joint;a rootTransform, which is relative to the root joint;and finally, each joint contains a bool,which indicates whether or not this joint is tracked.Here we enumerate all of the available jointsin the hand skeleton.Let's walk through a subset of the joint's hierarchy.The wrist is the root joint for the hand.For each finger, the first joint is parented to the wrist;for example, 1 is parented to 0.Subsequent finger joints are parented to the previous joint;for example, 2 is parented to 1, and so on.HandAnchors can be used to place contentrelative to your hands or detect custom gestures.There are two options for receiving HandAnchors --you can either poll for updates or receive anchorsasynchronously when they are available.We'll take a look at asynchronous updatesin our Swift example later on, so let's add hand anchor pollingto our renderer from earlier.Here's our updated struct definition.We've added a hand tracking provider,along with a left and right hand anchor.In our updated init function, we createour new hand tracking provider and add it to the listof providers that we run; we then createthe left and right hand anchors that we'll need when we poll.Note, we create these ahead of timein order to avoid allocating in the render loop.With our struct updated and initialized,we can call get_latest_anchors in our render function.We pass the provider and our preallocated hand anchors.Our anchors will be populated with the latest available data.With our latest anchors populated,we can now use their data in our experience.Very cool.Now it's time to revisit the example we showed you earlier.We used a combination of ARKit and RealityKit featuresto build this experience.Scene geometry was used as colliders for physicsand gestures, while hand tracking was usedto directly interact with the cube entities.Let's take a look at how we built this example.First, we'll check out the app struct and view model.Next, we'll initialize the ARKit session.Then we'll add colliders for our fingertipsand colliders from scene reconstruction.Finally, we'll look at how to add cubes with gestures.Let's just jump right into it.Here is our app, TimeForCube.We have a relatively standard SwiftUI app and scene setup.Within our scene, we declare an ImmersiveSpace.The IimmersiveSpace is required as we'll need to moveto a Full Space in order to get access to ARKit data.Within the ImmersiveSpace, we define a RealityViewwhich will present the content from our view model.The view model is where most of our app's logic will live.Let's take a quick look.The view model holds onto the ARKit session;the data providers we'll be using;our content entity, which will containall other entities that we create;and both our scene and hand collider maps.Our view model also provides various functionsthat we'll call from the app.We'll go through each of these in context from the app.The first function we'll call is within our RealityView'smake closure to set up the contentEntity.We'll add this entity to the content of our RealityView,so that the view model can add entities to the view's content.setupContentEntity simply adds all the finger entitiesin our map as children of the contentEntityand then returns it.Nice!Let's move on to session initialization.Our session initialization runs in one of three tasks.Our first task calls the runSession function.This function simply runs the session with our two providers.With the session running,we can start receiving anchor updates.Let's create and update our fingertip colliderswe'll use to interact with cubes.Here is our task for processing hand updates.Its function iterates over the async sequenceof anchor updates on the provider.We ensure that the hand anchor is tracked,get the index fingertip joint,and check that the joint itself is also tracked.We then compute the transform of the tip of the index fingerrelative to the app origin.Finally, we look up which finger entity we should updateand set its transform.Let's revisit our finger entity map.We create an entity per hand via an extension to ModelEntity,This extension creates a 5mm sphere with a collision shape.We add a kinematic physics body componentand hide this entity by adding an opacity component.Though we'll hide these for our use case,it'd be nice to visualize our fingertip entitiesto verify that everything is working as expected.Let's temporarily set our opacity to oneand make sure our entities are in the right place.Great!We can see spheres right where our fingertips are!Notice, our hands are partially covering up the spheres.This is called hand occlusion, a system feature that allowsa person to see their hands on top of virtual content.This is enabled by default,but if we'd like to see our sphere a little more clearly,we can configure hand occlusion visibilityby using the upperLimbVisibility setter on our scene.If we set limb visibility to hidden,we'll see the entire sphereregardless of where our hands are.For our example, we'll leave the upper limb visibilityas the default value and set the opacity back to zero.Neat! Now let's add scene colliders --we'll use these for physics and as gesture targets.Here's the task that calls the function on our model.We iterate over the async sequence of anchor updateson the provider, attempt to generatea ShapeResource from the MeshAnchor,then switch on the anchor update's event.If we're adding an anchor, we create a new entity,set its transform,add a collision and physics body component,then add an input target componentso that this collider can be a target for gestures.Finally, we add a new entity to our mapand as a child of our content entity.To update an entity, we retrieve it from the map,then update its transform and collision component shape.For removal, we remove the corresponding entityfrom its parent and the map.Now that we have hand and scene colliders,we can use gestures to add cubes.We add a SpatialTapGesture targeted to any entity,which will let us know if someone has tappedon any entity in our RealityView's content.When that tap has ended, we receive a 3D locationwe convert from global to scene coordinates.Let's visualize this location.Here's what we'd see if we added a sphereat the location of the tap.Now, we tell our view model to add a cuberelative to this location.To add a cube, we first calculate a placement locationthat's 20 centimeters above the tap location.We then create the cube and set its positionto our calculated placement location.We add an InputTargetComponent, which allows us setwhich types of gestures our entity will respond to.For our use case, we'll allow only indirect input typesfor these cubes, as our fingertip colliderswill provide direct interaction.We add a PhysicsBodyComponent with custom parametersto make the physics interactions a bit nicer.Last, we add our cube to the content entity,which means it is finally time for cube.Let's take one last look at our example, end to end.Every time we tap on the scene colliders or a cube,a new cube is added above the tap location.The physics system causes the cubeto drop onto the scene colliders,and our hand colliders allow us to interact with the cubes.For more information about RealityKit,check out the introductory session on using RealityKitfor spatial computing.And, if you already have an existing ARKit experienceon iOS that you're interested in bringing overto this platform, be sure to watchthe dedicated session on this topic for further guidance.Our entire team is incredibly thrilled for youto get your hands on the new version of ARKit.We cannot wait to see all of the groundbreaking appsthat you will create for this exciting new platform.Ryan: Thanks for watching!♪

♪ Mellow instrumental hip-hop ♪♪Ryan Taylor: Hello! My name is Ryan.Conner Brooks: And I'm Conner.Ryan: In this session, we are going to introduce youto ARKit for spatial computing.We will discuss the critical role that it playson this new platform and how you can leverage itto build the next generation of apps.ARKit uses sophisticated computer vision algorithmsto construct an understanding of the world around you,as well as your movements.We first introduced this technology in iOS 11as a way for developers to createamazing augmented reality experiencesthat you can use in the palm of your hand.On this platform, ARKit has maturedinto a full-blown system service,rebuilt from the ground up with a new real-time foundation.ARKit is deeply woven into the fabricof the entire operating system, powering everythingfrom interacting with a window, to playing an immersive game.As part of this journey, we have also given our APIa complete overhaul.The new design is a result of everythingthat we learned on iOS, plus the unique needsof spatial computing,and we think you are going to love it.ARKit provides a variety of powerful featuresthat you can combine to do incredible things,such as place virtual content on a table.You can reach out and touch the content,as if it is really there, and then watchas the content interacts with the real world.It is truly a magical experience.Now that you have seen a glimpse of what can be accomplishedusing ARKit on this new platform,let me walk you through our agenda.We will begin with an overview of the fundamental conceptsand building blocks that make up our API.Next, we will dive into world tracking,which is essential for placing virtual contentrelative to the real world.Then, we will explore our scene understanding features,which provide useful information about your surroundings.After that, we will introduce you to our newest feature,hand tracking, an exciting new addition that you can leveragefor placing virtual content relative to your handsor building other types of bespoke interactions.And lastly, we will come full circle and take a lookat a practical application of some of these featuresby examining code from the video that we showed you a moment ago.All right, let's get started!Our new API has been meticulously craftedin two invigorating flavors, modern Swift and classic C.All ARKit features are now provided à la carte.We wanted developers to have as much flexibility as possible,so that you can simply pick and choose what you needto build your experience.Access to ARKit data has been designedwith a privacy-first approach.We have put safeguards into placeto protect people's privacy,while also maintaining simplicity for developers.The API consists of three fundamental building blocks:sessions,data providers,and anchors.Let's begin with anchors and then work our wayback up to sessions.An anchor represents a position and orientationin the real world.All anchors include a unique identifier,as well as a transform.Some types of anchors are also trackable.When a trackable anchor is not being tracked,you should hide any virtual contentthat you have anchored with it.A data provider represents an individual ARKit feature.Data providers allow you to poll foror observe data updates, such as anchor changes.Different types of data providersprovide different types of data.A session represents a combined set of ARKit featuresthat you would like to use togetherfor a particular experience.You run a session by providing it with a set of data providers.Once the session is running,the data providers will begin receiving data.Updates arrive asynchronously and at different frequencies,depending on the type of data.Let's move on now and talk about privacyand how your app gets access to ARKit data.Privacy is a fundamental human right.It is also one of our core values.ARKit's architecture and API have been thoughtfully designedto protect people's privacy.In order for ARKit to construct an understandingof the world around you, the device has many camerasand other types of sensors.Data from these sensors, such as camera frames,is never sent to client space.Instead, sensor data is sent to ARKit's daemonfor secure processing by our algorithms.The resulting data that is produced by these algorithmsis then carefully curated before being forwardedto any clients that are requesting the data,such as your app.There are a few prerequisites to accessing ARKit data.First, your app must enter a Full Space.ARKit does not send data to apps that are in the Shared Space.Second, some types of ARKit data require permission to access.If the person does not grant permission,then we will not send that type of data to your app.To facilitate this, ARKit providesa convenient authorization API for handling permission.Using your session, you can request authorizationfor the types of data that you would like to access.If you do not do this, ARKit will automatically promptthe person for permission when you run the session,if necessary.Here, we are requesting access to hand tracking data.You can batch all of the authorization typesthat you need together in a single request.Once we have the authorization results,we iterate over them and check the statusfor each authorization type.If the person has granted permission,the status will be allowed.Attempting to run a session with a data providerthat provides data that the person has denied access towill result in the session failing.Now, let's take a closer look at each of the featuresthat ARKit supports on this platform,starting with world tracking.World tracking allows you to anchor virtual contentin the real world.ARKit tracks the device's movementin six degrees of freedom and updates each anchor,so that they stay in the same placerelative to your surroundings.The type of DataProvider that world tracking usesis called WorldTrackingProvider,and it gives you several important capabilities.It allows you to add WorldAnchors,which ARKit will then update to remain fixedrelative to people's surroundingsas the device moves around.WorldAnchors are an essential toolfor virtual content placement.Any WorldAnchors that you add are automatically persistedacross app launches and reboots.If this behavior is undesirable for the experiencethat you are building, you can simply removethe anchors when you are done with them,and they will no longer be persisted.It is important to note that there are some caseswhere persistence will not be available.You can also use a WorldTrackingProviderto get the device's pose relative to the app origin,which is necessary if you are doing your own renderingusing Metal.Let's begin by taking a closer look at what a WorldAnchor isand why you would want to use one.A WorldAnchor is a TrackableAnchorwith an initializer that takes a transform,which is the position and orientationthat you would like to place the anchor at,relative to the app's origin.We have prepared an example to help visualize the differencebetween virtual content that is not anchoredversus content that is anchored.Here we have two cubes.The blue cube on the left is not being updated by a WorldAnchor,while the red cube on the right is being updatedby a WorldAnchor.Both cubes were placed relative to the app's originwhen the app was launched.As the device moves around,both cubes remain where they were placed.You can press and hold the crown to recenter the app.When recentering occurs,the app's origin will be moved to your current location.Notice that the blue cube, which is not anchored,relocates to maintain its relative placementto the app's origin;while the red cube, which is anchored,remains fixed relative to the real world.Let's take a look at how WorldAnchor persistence works.As the device moves around,ARKit builds a map of your surroundings.When you add WorldAnchors, we insert them into the mapand automatically persist them for you.Only WorldAnchor identifiers and transforms are persisted.No other data, such as your virtual content,is included.It is up to you to maintain a mappingof WorldAnchor identifiers to any virtual contentthat you associate with them.Maps are location based, so when you take your deviceto a new location --for instance, from home to the office --the map of your home will be unloaded,and then a different map will be localized for the office.Any anchors that you add at this new locationwill go into that map.When you leave the office at the end of the dayand head home, the map that ARKithas been building at the office, along with any anchorsthat you placed there, will be unloaded.Once again, though, we have beenautomatically persisting the map along with your anchors.Upon returning home,ARKit will recognize that the location has changed,and we will begin the process of relocalizingby checking for an existing map for this location.If we find one, we will localize with it,and all of the anchors that you previously added at homewill become tracked once again.Let's move on to the device pose.Along with adding and removing WorldAnchors,you can also use a WorldTrackingProviderto get the pose of the device.The pose is the position and orientation of the devicerelative to the app's origin.Querying the pose is required if you are doingyour own rendering with Metal and CompositorServicesin a fully immersive experience.This query is relatively expensive.Exercise caution when querying for the device posefor other types of app logic, such as content placement.Let's quickly walk through a simplified rendering exampleto demonstrate how you can provide device posesfrom ARKit to CompositorServices.We have a Renderer struct that will hold our session,world tracking provider, and latest pose.When initializing the Renderer, we start by creating a session.Next, we create a world tracking provider,which we will use to query for the device posewhen we render each frame.Now, we can go ahead and run our sessionwith any data providers that we need.In this case, we are only using a world tracking provider.We also create a pose to avoid allocatingin the render function.Let's jump over to our render function now,which we will be calling at frame rate.Using the drawable from CompositorServices,we fetch the target render time.Next, we use the target render timeto query for the pose of the device.If successful, we can extract a transform of the poserelative to the app's origin.This is the transform to use for rendering your content.Lastly, before we submit the frame for compositing,we set the pose on the drawable,so that the compositor knows which pose we usedto render content for the frame.For more information on doing your own rendering,see the dedicated session for using Metalto create immersive apps.Additionally, there is a great sessionon spatial computing performance considerationsthat we encourage you to check out as well.Next, let's take a look at scene understanding.Scene understanding is a category of featuresthat inform you about your surroundings in different ways.Let's begin with plane detection.Plane detection provides anchors for horizontaland vertical surfaces that ARKit detects in the real world.The type of DataProvider that plane detection usesis called PlaneDetectionProvider.As planes are detected in your surroundings,they are provided to you in the form of PlaneAnchors.PlaneAnchors can be used to facilitate content placement,such as placing a virtual object on a table.Additionally, you can use planes for physics simulationswhere basic, flat geometry, such as a floor or wall,is sufficient.Each PlaneAnchor includes an alignment,which is horizontal or vertical; the geometry of the plane;and a semantic classification.Planes can be classified as a varietyof different types of surfaces, such as floor or table.If we are unable to identify a particular surface,the provided classification will be marked as either unknown,undetermined, or not available, depending on the circumstances.Now, let's move on to scene geometry.Scene geometry provides anchors containing a polygonal meshthat estimates the shape of the real world.The type of DataProvider that scene geometry usesis called SceneReconstructionProvider.As ARKit scans the world around you,we reconstruct your surroundings as a subdivided mesh,which is then provided to you in the form of MeshAnchors.Like PlaneAnchors, MeshAnchors can be usedto facilitate content placement.You can also achievehigher-fidelity physics simulationsin cases where you need virtual content to interactwith objects that are not just simple, flat surfaces.Each MeshAnchor includes geometry of the mesh.This geometry contains vertices, normals, faces,and semantic classifications, which are per face.Mesh faces can be classified as a varietyof different types of objects.If we are unable to identify a particular object,the provided classification will be none.Lastly, let's take a look at image tracking.Image tracking enables you to detect 2D imagesin the real world.The type of DataProvider that image tracking usesis called ImageTrackingProvider.You configure ImageTrackingProviderwith a set of ReferenceImages that you want to detect.These ReferenceImages can be created in a few different ways.One option is to load them from an AR resource groupin your project's asset catalog.Alternatively, you can also initializea ReferenceImage yourself by providing a CVPixelBufferor CGImage.When an image is detected,ARKit provides you with an ImageAnchor.ImageAnchors can be used to place contentat known, statically placed images.For instance, you can display some informationabout a movie next to a movie poster.ImageAnchors are TrackableAnchorsthat include an estimated scale factor,which indicates how the size of the detected imagecompares to the physical size that you specifiedand the ReferenceImage that the anchor corresponds to.Now, to tell you about our new feature, hand tracking,and then walk you through the example,here is Conner.Conner: Howdy. Let's take a look at hand tracking,a brand-new addition to ARKit.Hand tracking provides you with anchorscontaining skeletal data for each of your hands.The type of DataProvider that hand tracking usesis called HandTrackingProvider.When your hands are detected, they are provided to youin the form of HandAnchors.A HandAnchor is a TrackableAnchor.HandAnchors include a skeleton and a chirality.The chirality tells us whether this is a left or a right hand.A HandAnchor's transform is the wrist's transformrelative to the app origin.The skeleton consists of joints, which can be queried by name.A joint contains its parent joint; its name;a localTransform, which is relative to its parent joint;a rootTransform, which is relative to the root joint;and finally, each joint contains a bool,which indicates whether or not this joint is tracked.Here we enumerate all of the available jointsin the hand skeleton.Let's walk through a subset of the joint's hierarchy.The wrist is the root joint for the hand.For each finger, the first joint is parented to the wrist;for example, 1 is parented to 0.Subsequent finger joints are parented to the previous joint;for example, 2 is parented to 1, and so on.HandAnchors can be used to place contentrelative to your hands or detect custom gestures.There are two options for receiving HandAnchors --you can either poll for updates or receive anchorsasynchronously when they are available.We'll take a look at asynchronous updatesin our Swift example later on, so let's add hand anchor pollingto our renderer from earlier.Here's our updated struct definition.We've added a hand tracking provider,along with a left and right hand anchor.In our updated init function, we createour new hand tracking provider and add it to the listof providers that we run; we then createthe left and right hand anchors that we'll need when we poll.Note, we create these ahead of timein order to avoid allocating in the render loop.With our struct updated and initialized,we can call get_latest_anchors in our render function.We pass the provider and our preallocated hand anchors.Our anchors will be populated with the latest available data.With our latest anchors populated,we can now use their data in our experience.Very cool.Now it's time to revisit the example we showed you earlier.We used a combination of ARKit and RealityKit featuresto build this experience.Scene geometry was used as colliders for physicsand gestures, while hand tracking was usedto directly interact with the cube entities.Let's take a look at how we built this example.First, we'll check out the app struct and view model.Next, we'll initialize the ARKit session.Then we'll add colliders for our fingertipsand colliders from scene reconstruction.Finally, we'll look at how to add cubes with gestures.Let's just jump right into it.Here is our app, TimeForCube.We have a relatively standard SwiftUI app and scene setup.Within our scene, we declare an ImmersiveSpace.The IimmersiveSpace is required as we'll need to moveto a Full Space in order to get access to ARKit data.Within the ImmersiveSpace, we define a RealityViewwhich will present the content from our view model.The view model is where most of our app's logic will live.Let's take a quick look.The view model holds onto the ARKit session;the data providers we'll be using;our content entity, which will containall other entities that we create;and both our scene and hand collider maps.Our view model also provides various functionsthat we'll call from the app.We'll go through each of these in context from the app.The first function we'll call is within our RealityView'smake closure to set up the contentEntity.We'll add this entity to the content of our RealityView,so that the view model can add entities to the view's content.setupContentEntity simply adds all the finger entitiesin our map as children of the contentEntityand then returns it.Nice!Let's move on to session initialization.Our session initialization runs in one of three tasks.Our first task calls the runSession function.This function simply runs the session with our two providers.With the session running,we can start receiving anchor updates.Let's create and update our fingertip colliderswe'll use to interact with cubes.Here is our task for processing hand updates.Its function iterates over the async sequenceof anchor updates on the provider.We ensure that the hand anchor is tracked,get the index fingertip joint,and check that the joint itself is also tracked.We then compute the transform of the tip of the index fingerrelative to the app origin.Finally, we look up which finger entity we should updateand set its transform.

Let's revisit our finger entity map.We create an entity per hand via an extension to ModelEntity,This extension creates a 5mm sphere with a collision shape.We add a kinematic physics body componentand hide this entity by adding an opacity component.Though we'll hide these for our use case,it'd be nice to visualize our fingertip entitiesto verify that everything is working as expected.Let's temporarily set our opacity to oneand make sure our entities are in the right place.Great!We can see spheres right where our fingertips are!Notice, our hands are partially covering up the spheres.This is called hand occlusion, a system feature that allowsa person to see their hands on top of virtual content.This is enabled by default,but if we'd like to see our sphere a little more clearly,we can configure hand occlusion visibilityby using the upperLimbVisibility setter on our scene.If we set limb visibility to hidden,we'll see the entire sphereregardless of where our hands are.For our example, we'll leave the upper limb visibilityas the default value and set the opacity back to zero.Neat! Now let's add scene colliders --we'll use these for physics and as gesture targets.Here's the task that calls the function on our model.We iterate over the async sequence of anchor updateson the provider, attempt to generatea ShapeResource from the MeshAnchor,then switch on the anchor update's event.If we're adding an anchor, we create a new entity,set its transform,add a collision and physics body component,then add an input target componentso that this collider can be a target for gestures.Finally, we add a new entity to our mapand as a child of our content entity.To update an entity, we retrieve it from the map,then update its transform and collision component shape.For removal, we remove the corresponding entityfrom its parent and the map.Now that we have hand and scene colliders,we can use gestures to add cubes.We add a SpatialTapGesture targeted to any entity,which will let us know if someone has tappedon any entity in our RealityView's content.When that tap has ended, we receive a 3D locationwe convert from global to scene coordinates.Let's visualize this location.Here's what we'd see if we added a sphereat the location of the tap.Now, we tell our view model to add a cuberelative to this location.To add a cube, we first calculate a placement locationthat's 20 centimeters above the tap location.We then create the cube and set its positionto our calculated placement location.We add an InputTargetComponent, which allows us setwhich types of gestures our entity will respond to.For our use case, we'll allow only indirect input typesfor these cubes, as our fingertip colliderswill provide direct interaction.We add a PhysicsBodyComponent with custom parametersto make the physics interactions a bit nicer.Last, we add our cube to the content entity,which means it is finally time for cube.Let's take one last look at our example, end to end.Every time we tap on the scene colliders or a cube,a new cube is added above the tap location.The physics system causes the cubeto drop onto the scene colliders,and our hand colliders allow us to interact with the cubes.For more information about RealityKit,check out the introductory session on using RealityKitfor spatial computing.And, if you already have an existing ARKit experienceon iOS that you're interested in bringing overto this platform, be sure to watchthe dedicated session on this topic for further guidance.Our entire team is incredibly thrilled for youto get your hands on the new version of ARKit.We cannot wait to see all of the groundbreaking appsthat you will create for this exciting new platform.Ryan: Thanks for watching!♪

5:20 -Authorisation API

10:20 -World Tracking Device Pose Render Struct

10:21 -World Tracking Device Pose Render function

16:00 -Hand tracking joints

17:00 -Hand tracking with Render struct

17:25 -hand tracking call in render function

18:00 -Demo app TimeForCube

18:50 -Demo app View Model

20:00 -function HandTrackingProvider

21:20 -function SceneReconstruction

22:20 -add cube at tap location

## Code Samples

```swift
// Privacy


// Authorization


session 
=
 
ARKitSession
()


Task
 {
    
let
 authorizationResult 
=
 
await
 session.requestAuthorization(for: [.handTracking])

    
for
 (authorizationType, authorizationStatus) 
in
 authorizationResult {
        
print
(
"Authorization status for 
\(authorizationType)
: 
\(authorizationStatus)
"
)

        
switch
 authorizationStatus {
        
case
 .allowed:
            
// All good!

            
break

        
case
 .denied:
            
// Need to handle this.

            
break

        
...

        }
    }
}
```

```swift
// World tracking


// Device pose


#include 
<
ARKit
/
ARKit
.h
>

#include 
<
CompositorServices
/
CompositorServices
.h
>



struct
 
Renderer
 {
    ar_session_t                 session;
    ar_world_tracking_provider_t world_tracking;
    ar_pose_t                    pose;

    
...

};

void renderer_init(
struct
 
Renderer
 
*
renderer) {
    renderer->session 
=
 ar_session_create();

    ar_world_tracking_configuration_t config 
=
 ar_world_tracking_configuration_create();
    renderer->world_tracking 
=
 ar_world_tracking_provider_create(config);

    ar_data_providers_t providers 
=
 ar_data_providers_create();
    ar_data_providers_add_data_provider(providers, renderer->world_tracking);
    ar_session_run(renderer->session, providers);

    renderer->pose 
=
 ar_pose_create();

    
...

}
```

```swift
// World tracking


// Device pose


void render(
struct
 
Renderer
 
*
renderer,
            cp_layer_t       layer,
            cp_frame_t       frame_encoder,
            cp_drawable_t    drawable) {
    const cp_frame_timing_t timing_info 
=
 cp_drawable_get_frame_timing(drawable);
    const cp_time_t presentation_time 
=
 cp_frame_timing_get_presentation_time(timing_info);
    const 
CFTimeInterval
 target_render_time 
=
 cp_time_to_cf_time_interval(presentation_time);

    simd_float4x4 pose 
=
 matrix_identity_float4x4;

    const ar_pose_status_t status 
=

        ar_world_tracking_provider_query_pose_at_timestamp(renderer->world_tracking,
                                                           target_render_time,
                                                           renderer->pose);

    
if
 (status 
==
 ar_pose_status_success) {
        pose 
=
 ar_pose_get_origin_from_device_transform(renderer->pose);
    }

    
...


    cp_drawable_set_ar_pose(drawable, renderer->pose);

    
...

}
```

```swift
/
 
Hand
 tracking


@available
(xrOS 
1.0
, 
*
)

public
 
struct
 
Skeleton
 : @
unchecked
 
Sendable
, 
CustomStringConvertible
 {

    
public
 
func
 
joint
(
named
: 
SkeletonDefinition
.
JointName
) -> 
Skeleton
.
Joint
 

    
public
 
struct
 
Joint
 : 
CustomStringConvertible
, @
unchecked
 
Sendable
 {

        
public
 
var
 parentJoint: 
Skeleton
.
Joint
? { 
get
 }

        
public
 
var
 name: 
String
 { 
get
 }

        
public
 
var
 localTransform: simd_float4x4 { 
get
 }

        
public
 
var
 rootTransform: simd_float4x4 { 
get
 }

        
public
 
var
 isTracked: 
Bool
 { 
get
 }
    }
}
```

```swift
// Hand tracking


// Polling for hands



struct
 
Renderer
 {
    ar_hand_tracking_provider_t  hand_tracking;
    
struct
 {
        ar_hand_anchor_t left;
        ar_hand_anchor_t right;
    } hands;

    
...

};

void renderer_init(
struct
 
Renderer
 
*
renderer) {
    
...


    ar_hand_tracking_configuration_t hand_config 
=
 ar_hand_tracking_configuration_create();
    renderer->hand_tracking 
=
 ar_hand_tracking_provider_create(hand_config);

    ar_data_providers_t providers 
=
 ar_data_providers_create();
    ar_data_providers_add_data_provider(providers, renderer->world_tracking);
    ar_data_providers_add_data_provider(providers, renderer->hand_tracking);
    ar_session_run(renderer->session, providers);

    renderer->hands.left 
=
 ar_hand_anchor_create();
    renderer->hands.right 
=
 ar_hand_anchor_create();

    
...

}
```

```swift
// Hand tracking


// Polling for hands


void render(
struct
 
Renderer
 
*
renderer,
            
...
 ) {
    
...


    ar_hand_tracking_provider_get_latest_anchors(renderer->hand_tracking,
                                                 renderer->hands.left,
                                                 renderer->hands.right);

    
if
 (ar_trackable_anchor_is_tracked(renderer->hands.left)) {
        const simd_float4x4 origin_from_wrist 
            
=
 ar_anchor_get_origin_from_anchor_transform(renderer->hands.left);

        
...

    }

    
...

}
```

```swift
// App



@main


struct
 
TimeForCube
: 
App
 {
   
@StateObject
 
var
 model 
=
 
TimeForCubeViewModel
()

    
var
 body: 
some
 
SwiftUI
.
Scene
 {
        
ImmersiveSpace
 {
            
RealityView
 { content 
in

                content.add(model.setupContentEntity())
            }
            .task {
                
await
 model.runSession()
            }
            .task {
                
await
 model.processHandUpdates()
            }
            .task {
                
await
 model.processReconstructionUpdates()
            }
            .gesture(
SpatialTapGesture
().targetedToAnyEntity().onEnded({ value 
in

                
let
 location3D 
=
 value.convert(value.location3D, from: .global, to: .scene)
                model.addCube(tapLocation: location3D)
            }))
        }
    }
}
```

```swift
// View model



@MainActor
 
class
 
TimeForCubeViewModel
: 
ObservableObject
 {
    
private
 
let
 session 
=
 
ARKitSession
()
    
private
 
let
 handTracking 
=
 
HandTrackingProvider
()
    
private
 
let
 sceneReconstruction 
=
 
SceneReconstructionProvider
()

    
private
 
var
 contentEntity 
=
 
Entity
()

    
private
 
var
 meshEntities 
=
 [
UUID
: 
ModelEntity
]()

    
private
 
let
 fingerEntities: [
HandAnchor
.
Chirality
: 
ModelEntity
] 
=
 [
        .left: .createFingertip(),
        .right: .createFingertip()
    ]

    
func
 
setupContentEntity
() { 
...
 }

    
func
 
runSession
() 
async
 { 
...
 }

    
func
 
processHandUpdates
() 
async
 { 
...
 }

    
func
 
processReconstructionUpdates
() 
async
 { 
...
 }

    
func
 
addCube
(
tapLocation
: 
SIMD3
<
Float
>) { 
...
 }
}
```

```swift
class
 
TimeForCubeViewModel
: 
ObservableObject
 {
    
...

    
private
 
let
 fingerEntities: [
HandAnchor
.
Chirality
: 
ModelEntity
] 
=
 [
        .left: .createFingertip(),
        .right: .createFingertip()
    ]

    
...

    
func
 
processHandUpdates
() 
async
 {
        
for
 
await
 update 
in
 handTracking.anchorUpdates {
            
let
 handAnchor 
=
 update.anchor

            
guard
 handAnchor.isTracked 
else
 { 
continue
 }

            
let
 fingertip 
=
 handAnchor.skeleton.joint(named: .handIndexFingerTip)

            
guard
 fingertip.isTracked 
else
 { 
continue
 }

            
let
 originFromWrist 
=
 handAnchor.transform
            
let
 wristFromIndex 
=
 fingertip.rootTransform
            
let
 originFromIndex 
=
 originFromWrist 
*
 wristFromIndex

            fingerEntities[handAnchor.chirality]
?
.setTransformMatrix(originFromIndex,             relativeTo: 
nil
)
        }
```

```swift
func
 
processReconstructionUpdates
() 
async
 {
        
for
 
await
 update 
in
 sceneReconstruction.anchorUpdates {
            
let
 meshAnchor 
=
 update.anchor
            
            
guard
 
let
 shape 
=
 
try?
 
await
 
ShapeResource
.generateStaticMesh(from: meshAnchor)             
else
 { 
continue
 }
            
            
switch
 update.event {
            
case
 .added:
                
let
 entity 
=
 
ModelEntity
()
                entity.transform 
=
 
Transform
(matrix: meshAnchor.transform)
                entity.collision 
=
 
CollisionComponent
(shapes: [shape], isStatic: 
true
)
                entity.physicsBody 
=
 
PhysicsBodyComponent
()
                entity.components.set(
InputTargetComponent
())

                meshEntities[meshAnchor.id] 
=
 entity
                contentEntity.addChild(entity)
            
case
 .updated:
                
guard
 
let
 entity 
=
 meshEntities[meshAnchor.id] 
else
 { 
fatalError
(
"..."
) }
                entity.transform 
=
 
Transform
(matrix: meshAnchor.transform)
                entity.collision
?
.shapes 
=
 [shape]
            
case
 .removed:
                meshEntities[meshAnchor.id]
?
.removeFromParent()
                meshEntities.removeValue(forKey: meshAnchor.id)
            
@unknown
 
default
:
                
fatalError
(
"Unsupported anchor event"
)
            }
        }
    }
```

```swift
class
 
TimeForCubeViewModel
: 
ObservableObject
 {
    
func
 
addCube
(
tapLocation
: 
SIMD3
<
Float
>) {
        
let
 placementLocation 
=
 tapLocation 
+
 
SIMD3
<
Float
>(
0
, 
0.2
, 
0
)

        
let
 entity 
=
 
ModelEntity
(
            mesh: .generateBox(size: 
0.1
, cornerRadius: 
0.0
),
            materials: [
SimpleMaterial
(color: .systemPink, isMetallic: 
false
)],
            collisionShape: .generateBox(size: 
SIMD3
<
Float
>(repeating: 
0.1
)),
            mass: 
1.0
)

        entity.setPosition(placementLocation, relativeTo: 
nil
)
        entity.components.set(
InputTargetComponent
(allowedInputTypes: .indirect))

        
let
 material 
=
 
PhysicsMaterialResource
.generate(friction: 
0.8
, restitution: 
0.0
)
        entity.components.set(
PhysicsBodyComponent
(shapes: entity.collision
!
.shapes,
                                                   mass: 
1.0
,
                                                   material: material,
                                                   mode: .dynamic))

        contentEntity.addChild(entity)
    }
}
```

