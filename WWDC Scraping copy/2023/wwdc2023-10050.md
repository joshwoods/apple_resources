# Wwdc2023 10050

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Optimize machine learning for Metal appsDiscover the latest enhancements to accelerated ML training in Metal. Find out about updates to PyTorch and TensorFlow, and learn about Metal acceleration for JAX. We'll show you how MPS Graph can support faster ML inference when you use both the GPU and Apple Neural Engine, and share how the same API can rapidly integrate your Core ML and ONNX models.

For more information on using Metal for machine learning, check out “Accelerate machine learning with Metal” from WWDC22.ResourcesCustomizing a PyTorch operationFiltering Images with MPSGraph FFT OperationsHD VideoSD VideoRelated VideosWWDC22Accelerate machine learning with Metal

Discover the latest enhancements to accelerated ML training in Metal. Find out about updates to PyTorch and TensorFlow, and learn about Metal acceleration for JAX. We'll show you how MPS Graph can support faster ML inference when you use both the GPU and Apple Neural Engine, and share how the same API can rapidly integrate your Core ML and ONNX models.

For more information on using Metal for machine learning, check out “Accelerate machine learning with Metal” from WWDC22.

Customizing a PyTorch operation

Filtering Images with MPSGraph FFT Operations

HD VideoSD Video

HD Video

SD Video

Accelerate machine learning with Metal

Search this video…♪ ♪Denis: Hi, my name is Denis Vieriu, and I'm a software engineer in the GPU,Graphics, and Display Software group at Apple.Today I will present to you all the new features and enhancementsintroduced to machine learning this year in Metal.I'll first recap the existing machine learning backends.The Metal machine learning APIs are exposedthrough Metal Performance Shaders framework.MPS is a collection of high-performance GPU primitives for various fields,like image processing, linear algebra, and machine learning.MPSGraph is a general purpose compute graph,which sits on top of the MPS frameworkand extends support to multi-dimensional tensors.machine learning inference frameworks, like CoreML,build on top of the MPSGraph backend.MPSGraph also supports training frameworks,like TensorFlow and PyTorch.To learn more about MPSGraphand ML Frameworks, please referto the previous Metal WWDC talks listed here.This session focuses on the updates and enhancements added to PyTorchand TensorFlow Metal backends, the new GPU acceleration for JAX,and the features added this year to MPSGraph for ML Inference.PyTorch and TensorFlow Metal acceleration enable you to usethe highly efficient kernels from MPS to get the best performance on your Mac.PyTorch Metal acceleration has been available since version 1.12through the MPS backend.This was introduced last year into the PyTorch ecosystem,and since then, multiple improvements have been madefor optimizing memory usage and view tensors.This year, PyTorch 2.0 MPS Backend made a great leap forwardand has been qualified for the Beta Stage.But these were not all the improvements.Latest PyTorch builds contain lots of new updates,such as MPS operation profiling, custom kernel,and Automatic Mixed precision support.Before covering all the nightly build features,I'll start with what is new in PyTorch 2.0.There is support for the top 60 most used Torch operators,including ops such as grid sampler, triangular solve, topk, and many more.The testing coverage has greatly improved.This includes tests for most of the Torch operators,gradient testing, and ModuleInfo based testing.Since release, the network coverage has expandedas multiple popular models adopted MPS as their official backend on macOS.This includes foundation models, such as WhisperAI,object detection models such as YOLO, stable diffusion models, and many more.Let's check one of these models in action using latest PyTorch 2.0.For this example, I am using YoloV5,an object detection network running on an M2 Max.On the left side, I have the network runningand generating live images using the PyTorch MPS backend,while on the right, I have the exact same model,but running on the CPU.The left side, using the MPS backend, is running at noticeably higher framerate.And furthermore, the developers not only adoptedthe PyTorch MPS backend in their external networks,but also have contributed code for multiple new operators,including histogram, group_norm, signbit, and more.Next, I'll cover the new featuresavailable in the latest PyTorch builds,starting with profiling supportfor MPS operations.PyTorch nightly builds have profiling supportthat uses OS signposts to show the exact running timefor operation executions, copies between CPU and GPU,and fallbacks to the CPU caused by unsupported operators.You will be able to visualize the profiling data in a very familiar tool,Metal System Trace, which is part of Instruments.To learn more about profiling ML applications using Metal System Trace,I recommend you watch the session from last year,"Accelerate machine learning with Metal."Using the profiler is a very simple process.Call the start method on the MPS profiler package to enable tracing,and, at the end of your script, use the stop method to end profiling.Now I will walk through the profiler to debug an example.This sample network uses a Sequential model composedof linear transformations and Softshrink activation functionswith a total of seven layers in the model.The current performance of this model is not satisfying.In this case, the profiler can be usedto find the bottleneck.In the Metal System Trace, first, make sure to enable the os_signpost.This will allow you to capture the PyTorch operator information.Next, check that the device and the right executable are set,in this case, the Python binary.Then click on the record button.Instruments is now recording the PyTorch execution.I'll let it run for couple of seconds to make sure I capture enough data.Then I click Stop.In the os_signpost tab,disclose the PyTorch Intervals timeline.This timeline displays the execution time of an operator,alongside PyTorch Metadata, such as string identifiers,data types, and copy lengths.Zooming into the timeline reveals PyTorch operators used by this example.The pattern from this trace can be easily identifiedto the custom Sequential model made of seven layers.From the trace, it's clear that the bottleneckis in the Softshrink fallback to the CPU.This process is very inefficient.The model incurs the overhead from the CPU executionof the Softshrink operator and the additional copies,while the GPU is starved.Most of the gaps in the GPU timeline are comingfrom the Softshrink activation function falling back to CPU.In order to fix this, I'll write a custom kernel to improve the performance.There are four steps to write a custom operation.First, implement the operation in Objective-C and Metal.Next, create the Python bindings for your Objective-C codeand compile your extension.Finally, once your extension is built,import the operation into your training script and begin using it.I'll start with the operation implementation.Start by importing the Torch extension header.This includes all the necessary PyTorch bits to write C++ extensions.Then define the compute functionand use the get_command_buffer MPS backend API to get a referenceto the MPSStream Command Buffer.Similarly, use the get_dispatch_queue API to get a reference to the serial queue.Next, create an encoder using the command buffer and define the custom GPU kernel.You encode the kernel inside the dispatch queueto ensure that submissions from multiple threads are serialized.After all the work is encoded, use the synchronize API to waituntil the current command buffer is done running,so you can observe serialized submissions.Or if you don't need serialization, use the commit API.Next, bind your custom functions.You can use PYBIND11 to bind the Objective-C functionsinto Python in a very simple manner.For this extension, the necessary binding code spans only two lines.After binding, compile your extension.First import torch.utils.cpp_extension.This provides a load function which you can use to compile your extension.Next, pass the name of your extension to build,then a list of relative or absolute paths to the source code files.Optionally, you can list additional compiler flags to forward to the build.The load function will compile the source files into a shared library,which will be subsequently loaded into the current Python process as a module.Finally, import the operator into your script to begin using it.Start by importing the compiled library and change the previous Sequential modelto use the custom Softshrink kernel.Let's run the same model again and check the result.With the newly added custom operator, the model runs much more efficiently.All the copies and intermediate tensors created by the fallbackto the CPU are gone, and the Sequential model runs much faster.Now let's explore more ways your network can be further improved.PyTorch MPS backend now supports automatic mixed precision,which allows you to train faster using less memoryand without loss in quality.To understand mixed precision,I will first review the supported data types.Mixed precision training is a mode that allows trainingdeep learning models with a mix of single precision floating pointand half precision floating point.Starting with macOS Sonoma,MPSGraph adds support for a new data type, bfloat16.bfloat16 is a 16-bit floating point format for deep learning.It is comprised of 1 sign bit, 8 exponent bits,and 7 mantissa bits.This is different from the standard IEEE 16-bit floating point format,which was not designed with deep learning applications in mind.Automatic Mixed Precision will be enabled for both float16 and bfloat16.Automatic mixed precision chooses the right precision per layerby measuring the performance of the network in default precision,then it runs again, with mixed precision settingsto optimize the performance without impacting the accuracy.Some layers of the neural networks can be executed at lower precision,such as convolutional or linear layers.Other layers such as reductions will often require a higher precision level.Adding Automatic Mixed Precision support to your network is a very easy process.First, add autocast.Both float16 and bfloat16 are supported.Autocast serves as a context manager that allows a region of the scriptto run in mixed precision.In this region, MPS ops run in a data type chosen by autocastto improve performance while maintaining accuracy.The MPS backend has also been significantly optimized.With PyTorch 2.0 and macOS Sonoma,the MPS backend is up to five times fastercompared to our previous release.That's it for PyTorch. Now let's move on to TensorFlow.The TensorFlow Metal backend has matured to a stable 1.0 release version.In this release, a grappler remapping optimizer passhas been added to the plugin.The Metal plugin also gets mixed precision support,and the installation process is now simpler than before.The performance of the TensorFlow Metal backend has been improvedthrough addition of automatic fusion of recognized computation patterns.These computations include fused convolutionsand matrix multiplications, optimizer operations, and RNN cells.This optimization happens automatically through the grappler passwhen the computation graph is created.Here I have an example of a common computationof a two-dimensional convolution operation.The convolution is often followed by an addition function,a common pattern in convolutional neural networks.By identifying this pattern,the grappler pass can remap the computation.This allows you to use a more optimized kernel to achieve the same output,leading to better performance.Like in PyTorch, TensorFlow also gets mixed precision support.TensorFlow allows setting mixed precision globally.This enables all network layers to be automatically createdwith the requested data type policy, so enabling this changein your standard workflow requires minimal changes to existing code.Global policy can be set to use either Float16 or BFloat16.In addition to improvements in performance,the user experience in enabling the Metal acceleration has been streamlined.From now on, simply following the usual pathof installing the TensorFlow wheel and the TensorFlow-Metal pluginthrough a package manager will enable the Metal acceleration.For those who want to stay on the bleeding edge of TensorFlow development,the Metal acceleration support is now also availableon the nightly releases of TensorFlow.Now let's talk about the new GPU acceleration for JAX.This year, JAX GPU acceleration will be supportedthrough the Metal backend, similar to PyTorch and TensorFlow.JAX is a Python library for high-performance numerical computingand machine learning research.It is based on the popular NumPy framework for working with large arrays,with three key extensions for machine learning research.First, it supports automatic differentiation using the grad function.It can differentiate through a large subset of Python's features,and it can even take high order derivatives.JAX also supports fast and efficient vectorization.Given a function apply_matrix,you could loop over a batch dimension in Python,but it may run at sub-optimal performance.In this case, vmap can be used to add batching support automatically.And further, JAX lets you just-in-time compileyour function into optimized kernels using an API called jit.In the same case, jit is used to transform the functionon top of vmap to make it run faster.On a MacBook Pro with M2 Max,JAX Metal acceleration provides amazing speedups,with an average of ten times faster than the CPU across these networks.For more details on environment setup and installation of JAX,please refer to the Metal Developer Resources web page.Let's switch gears and move to ML inference.I will start by introducing a new serialization formatfor MPSGraph that you use to optimize your load times.This new serialization format can be generatedfrom your existing serialized networks from other frameworks.Finally, I will show you how to optimize the memory footprintof your network by leveraging 8-bit integer quantization.Let's begin.An MPSGraph can be created using the high level APIs with full flexibility,layer by layer.Please refer to the video on building customized ML modelswith Metal Performance Shaders Graph for details.After defining and compiling your custom graph,it will then execute through the MPSGraphExecutable to get results.Normally, this process works great.However, in complex graphs with many layers,this initial compilation can lead to high application launch times.MPSGraph has a new serialization format called MPSGraphPackage,to address exactly this problem.This new serialization format allows you to createthe MPSGraphExecutable ahead of time.Once created, the optimized MPSGraphExecutablecan be loaded directly from a MPSGraphPackage file.Creating a MPSGraphPackage is very simple.All you need to do is to create a serialization descriptorand pass it to the serialize functionof the MPSGraphExecutable you want to serialize.You'll also need to pass a path to store it.After creating the package,this is how you load the graph into your app.You need a compilation descriptor and the path to your stored package.Then use them to initialize the MPSGraphExecutable.If you've been already using MPSGraph, you can easily adoptthe new serialization format using the APIs we have presented.But if you're coming from other frameworks,you can now easily migrate to MPSGraphPackageusing the new MPSGraphTool.For users of CoreML,you can pass your ML Programstp the MPSGraphTool, which will createa MPSGraphPackage for you.The same goes for ONNX,where you can use your ONNX fileas the input.This new tool lets you quickly include your existing modelsto your MPSGraph application without the needto encode the inference model manually.Here's how you use the command line tool.You give the MPSGraphTool a flag to declare the input model type,in this case, CoreML Package.You also provide it with the path to your output destinationand the name of your output model.Additionally, you define the target platformand minimum OS version.After conversion, the produced MPSGraphPackagescan be loaded to your app and executed directly.Next, lets discuss how you can improve the efficiencyof your computations using the 8-bit integer quantizations.It's common to use floating point formats to do training and inference,such as 16-bit floating point format.However, at inference, these models may take a longer time to predict results.Instead, it's better in many cases to use reduced precisionor 8-bit integer numbers.This will help you in saving memory bandwithand reduce the memory footprint of your model.For 8-bit integer formats, there are two types of quantization:symmetric and asymmetric.MPSGraph now supports APIs for both of them.Compared to the symmetric quantization, the asymmetric one lets you specifya quantization bias, denoted by zeroPoint here.Now let's delve into using quantized computations through an example,starting with activation and weights in an Int8 format as the inputs.These inputs are dequantized to floating point formatusing the dequantizeTensor op in MPSGraph.Now the floating point inputs can be fedinto a convolution operation.The resulting floating point tensorcan then be quantized back to Int8using the quantizeTensor op.MPSGraph will automatically fuse all of these kernels into a single operation,therefore saving memory bandwidth and potentially improving performance.And this is how you can use the quantization support in MPSGraph.In addition to the previous new features,MPSGraph supports even more machine learning operators.Starting this year, complex types are supported for most graph operations.You can use complex numbers either with single precisionor half precision floating point formats.Building on the complex data type,MPSGraph adds operators for computing Fast Fourier Transformations.You can apply complex to complex, complex to real,and real to complex transformations up to four dimensions.These are very common in audio, video, and image processing applications.Furthermore, using MPSGraph,you can now perform three-dimensional convolutions,grid sampling, Sort and ArgSort, and cumulative operations,including sums, products, minima, and maxima.And this concludes the discussion about the new features in MPSGraph.Let's review what was presented today in this session.I went over the improvements in accelerating popular ML frameworkslike PyTorch and TensorFlow through Metal.Now you can also take advantage of the new Metal accelerated JAX framework.We also discussed how to seamlessly integrate your existing modelsfrom other frameworks to MPSGraph using the new serialization tools.And this concludes our talk.We can't wait to see the amazing content that you will createusing all of these features.Thanks for watching.♪ ♪

♪ ♪Denis: Hi, my name is Denis Vieriu, and I'm a software engineer in the GPU,Graphics, and Display Software group at Apple.Today I will present to you all the new features and enhancementsintroduced to machine learning this year in Metal.I'll first recap the existing machine learning backends.The Metal machine learning APIs are exposedthrough Metal Performance Shaders framework.MPS is a collection of high-performance GPU primitives for various fields,like image processing, linear algebra, and machine learning.MPSGraph is a general purpose compute graph,which sits on top of the MPS frameworkand extends support to multi-dimensional tensors.machine learning inference frameworks, like CoreML,build on top of the MPSGraph backend.MPSGraph also supports training frameworks,like TensorFlow and PyTorch.To learn more about MPSGraphand ML Frameworks, please referto the previous Metal WWDC talks listed here.

This session focuses on the updates and enhancements added to PyTorchand TensorFlow Metal backends, the new GPU acceleration for JAX,and the features added this year to MPSGraph for ML Inference.

PyTorch and TensorFlow Metal acceleration enable you to usethe highly efficient kernels from MPS to get the best performance on your Mac.PyTorch Metal acceleration has been available since version 1.12through the MPS backend.This was introduced last year into the PyTorch ecosystem,and since then, multiple improvements have been madefor optimizing memory usage and view tensors.This year, PyTorch 2.0 MPS Backend made a great leap forwardand has been qualified for the Beta Stage.But these were not all the improvements.Latest PyTorch builds contain lots of new updates,such as MPS operation profiling, custom kernel,and Automatic Mixed precision support.Before covering all the nightly build features,I'll start with what is new in PyTorch 2.0.

There is support for the top 60 most used Torch operators,including ops such as grid sampler, triangular solve, topk, and many more.

The testing coverage has greatly improved.This includes tests for most of the Torch operators,gradient testing, and ModuleInfo based testing.

Since release, the network coverage has expandedas multiple popular models adopted MPS as their official backend on macOS.This includes foundation models, such as WhisperAI,object detection models such as YOLO, stable diffusion models, and many more.Let's check one of these models in action using latest PyTorch 2.0.For this example, I am using YoloV5,an object detection network running on an M2 Max.On the left side, I have the network runningand generating live images using the PyTorch MPS backend,while on the right, I have the exact same model,but running on the CPU.The left side, using the MPS backend, is running at noticeably higher framerate.

And furthermore, the developers not only adoptedthe PyTorch MPS backend in their external networks,but also have contributed code for multiple new operators,including histogram, group_norm, signbit, and more.

Next, I'll cover the new featuresavailable in the latest PyTorch builds,starting with profiling supportfor MPS operations.PyTorch nightly builds have profiling supportthat uses OS signposts to show the exact running timefor operation executions, copies between CPU and GPU,and fallbacks to the CPU caused by unsupported operators.You will be able to visualize the profiling data in a very familiar tool,Metal System Trace, which is part of Instruments.To learn more about profiling ML applications using Metal System Trace,I recommend you watch the session from last year,"Accelerate machine learning with Metal."Using the profiler is a very simple process.Call the start method on the MPS profiler package to enable tracing,and, at the end of your script, use the stop method to end profiling.Now I will walk through the profiler to debug an example.

This sample network uses a Sequential model composedof linear transformations and Softshrink activation functionswith a total of seven layers in the model.

The current performance of this model is not satisfying.In this case, the profiler can be usedto find the bottleneck.

In the Metal System Trace, first, make sure to enable the os_signpost.This will allow you to capture the PyTorch operator information.Next, check that the device and the right executable are set,in this case, the Python binary.Then click on the record button.

Instruments is now recording the PyTorch execution.I'll let it run for couple of seconds to make sure I capture enough data.Then I click Stop.

In the os_signpost tab,disclose the PyTorch Intervals timeline.

This timeline displays the execution time of an operator,alongside PyTorch Metadata, such as string identifiers,data types, and copy lengths.

Zooming into the timeline reveals PyTorch operators used by this example.The pattern from this trace can be easily identifiedto the custom Sequential model made of seven layers.

From the trace, it's clear that the bottleneckis in the Softshrink fallback to the CPU.This process is very inefficient.The model incurs the overhead from the CPU executionof the Softshrink operator and the additional copies,while the GPU is starved.Most of the gaps in the GPU timeline are comingfrom the Softshrink activation function falling back to CPU.In order to fix this, I'll write a custom kernel to improve the performance.There are four steps to write a custom operation.First, implement the operation in Objective-C and Metal.Next, create the Python bindings for your Objective-C codeand compile your extension.Finally, once your extension is built,import the operation into your training script and begin using it.I'll start with the operation implementation.

Start by importing the Torch extension header.This includes all the necessary PyTorch bits to write C++ extensions.

Then define the compute functionand use the get_command_buffer MPS backend API to get a referenceto the MPSStream Command Buffer.Similarly, use the get_dispatch_queue API to get a reference to the serial queue.

Next, create an encoder using the command buffer and define the custom GPU kernel.

You encode the kernel inside the dispatch queueto ensure that submissions from multiple threads are serialized.

After all the work is encoded, use the synchronize API to waituntil the current command buffer is done running,so you can observe serialized submissions.Or if you don't need serialization, use the commit API.

Next, bind your custom functions.You can use PYBIND11 to bind the Objective-C functionsinto Python in a very simple manner.For this extension, the necessary binding code spans only two lines.

After binding, compile your extension.First import torch.utils.cpp_extension.

This provides a load function which you can use to compile your extension.Next, pass the name of your extension to build,then a list of relative or absolute paths to the source code files.Optionally, you can list additional compiler flags to forward to the build.The load function will compile the source files into a shared library,which will be subsequently loaded into the current Python process as a module.

Finally, import the operator into your script to begin using it.

Start by importing the compiled library and change the previous Sequential modelto use the custom Softshrink kernel.

Let's run the same model again and check the result.

With the newly added custom operator, the model runs much more efficiently.

All the copies and intermediate tensors created by the fallbackto the CPU are gone, and the Sequential model runs much faster.Now let's explore more ways your network can be further improved.

PyTorch MPS backend now supports automatic mixed precision,which allows you to train faster using less memoryand without loss in quality.To understand mixed precision,I will first review the supported data types.Mixed precision training is a mode that allows trainingdeep learning models with a mix of single precision floating pointand half precision floating point.Starting with macOS Sonoma,MPSGraph adds support for a new data type, bfloat16.

bfloat16 is a 16-bit floating point format for deep learning.It is comprised of 1 sign bit, 8 exponent bits,and 7 mantissa bits.This is different from the standard IEEE 16-bit floating point format,which was not designed with deep learning applications in mind.Automatic Mixed Precision will be enabled for both float16 and bfloat16.

Automatic mixed precision chooses the right precision per layerby measuring the performance of the network in default precision,then it runs again, with mixed precision settingsto optimize the performance without impacting the accuracy.Some layers of the neural networks can be executed at lower precision,such as convolutional or linear layers.Other layers such as reductions will often require a higher precision level.

Adding Automatic Mixed Precision support to your network is a very easy process.First, add autocast.Both float16 and bfloat16 are supported.Autocast serves as a context manager that allows a region of the scriptto run in mixed precision.

In this region, MPS ops run in a data type chosen by autocastto improve performance while maintaining accuracy.

The MPS backend has also been significantly optimized.With PyTorch 2.0 and macOS Sonoma,the MPS backend is up to five times fastercompared to our previous release.That's it for PyTorch. Now let's move on to TensorFlow.The TensorFlow Metal backend has matured to a stable 1.0 release version.In this release, a grappler remapping optimizer passhas been added to the plugin.The Metal plugin also gets mixed precision support,and the installation process is now simpler than before.

The performance of the TensorFlow Metal backend has been improvedthrough addition of automatic fusion of recognized computation patterns.These computations include fused convolutionsand matrix multiplications, optimizer operations, and RNN cells.This optimization happens automatically through the grappler passwhen the computation graph is created.

Here I have an example of a common computationof a two-dimensional convolution operation.The convolution is often followed by an addition function,a common pattern in convolutional neural networks.By identifying this pattern,the grappler pass can remap the computation.

This allows you to use a more optimized kernel to achieve the same output,leading to better performance.Like in PyTorch, TensorFlow also gets mixed precision support.TensorFlow allows setting mixed precision globally.This enables all network layers to be automatically createdwith the requested data type policy, so enabling this changein your standard workflow requires minimal changes to existing code.

Global policy can be set to use either Float16 or BFloat16.

In addition to improvements in performance,the user experience in enabling the Metal acceleration has been streamlined.From now on, simply following the usual pathof installing the TensorFlow wheel and the TensorFlow-Metal pluginthrough a package manager will enable the Metal acceleration.For those who want to stay on the bleeding edge of TensorFlow development,the Metal acceleration support is now also availableon the nightly releases of TensorFlow.Now let's talk about the new GPU acceleration for JAX.This year, JAX GPU acceleration will be supportedthrough the Metal backend, similar to PyTorch and TensorFlow.

JAX is a Python library for high-performance numerical computingand machine learning research.It is based on the popular NumPy framework for working with large arrays,with three key extensions for machine learning research.

First, it supports automatic differentiation using the grad function.It can differentiate through a large subset of Python's features,and it can even take high order derivatives.JAX also supports fast and efficient vectorization.Given a function apply_matrix,you could loop over a batch dimension in Python,but it may run at sub-optimal performance.In this case, vmap can be used to add batching support automatically.

And further, JAX lets you just-in-time compileyour function into optimized kernels using an API called jit.In the same case, jit is used to transform the functionon top of vmap to make it run faster.

On a MacBook Pro with M2 Max,JAX Metal acceleration provides amazing speedups,with an average of ten times faster than the CPU across these networks.For more details on environment setup and installation of JAX,please refer to the Metal Developer Resources web page.

Let's switch gears and move to ML inference.I will start by introducing a new serialization formatfor MPSGraph that you use to optimize your load times.This new serialization format can be generatedfrom your existing serialized networks from other frameworks.Finally, I will show you how to optimize the memory footprintof your network by leveraging 8-bit integer quantization.Let's begin.An MPSGraph can be created using the high level APIs with full flexibility,layer by layer.Please refer to the video on building customized ML modelswith Metal Performance Shaders Graph for details.After defining and compiling your custom graph,it will then execute through the MPSGraphExecutable to get results.Normally, this process works great.However, in complex graphs with many layers,this initial compilation can lead to high application launch times.

MPSGraph has a new serialization format called MPSGraphPackage,to address exactly this problem.This new serialization format allows you to createthe MPSGraphExecutable ahead of time.Once created, the optimized MPSGraphExecutablecan be loaded directly from a MPSGraphPackage file.Creating a MPSGraphPackage is very simple.

All you need to do is to create a serialization descriptorand pass it to the serialize functionof the MPSGraphExecutable you want to serialize.You'll also need to pass a path to store it.After creating the package,this is how you load the graph into your app.You need a compilation descriptor and the path to your stored package.Then use them to initialize the MPSGraphExecutable.If you've been already using MPSGraph, you can easily adoptthe new serialization format using the APIs we have presented.But if you're coming from other frameworks,you can now easily migrate to MPSGraphPackageusing the new MPSGraphTool.For users of CoreML,you can pass your ML Programstp the MPSGraphTool, which will createa MPSGraphPackage for you.The same goes for ONNX,where you can use your ONNX fileas the input.This new tool lets you quickly include your existing modelsto your MPSGraph application without the needto encode the inference model manually.

Here's how you use the command line tool.You give the MPSGraphTool a flag to declare the input model type,in this case, CoreML Package.You also provide it with the path to your output destinationand the name of your output model.Additionally, you define the target platformand minimum OS version.After conversion, the produced MPSGraphPackagescan be loaded to your app and executed directly.

Next, lets discuss how you can improve the efficiencyof your computations using the 8-bit integer quantizations.It's common to use floating point formats to do training and inference,such as 16-bit floating point format.However, at inference, these models may take a longer time to predict results.Instead, it's better in many cases to use reduced precisionor 8-bit integer numbers.This will help you in saving memory bandwithand reduce the memory footprint of your model.

For 8-bit integer formats, there are two types of quantization:symmetric and asymmetric.MPSGraph now supports APIs for both of them.Compared to the symmetric quantization, the asymmetric one lets you specifya quantization bias, denoted by zeroPoint here.

Now let's delve into using quantized computations through an example,starting with activation and weights in an Int8 format as the inputs.These inputs are dequantized to floating point formatusing the dequantizeTensor op in MPSGraph.Now the floating point inputs can be fedinto a convolution operation.The resulting floating point tensorcan then be quantized back to Int8using the quantizeTensor op.MPSGraph will automatically fuse all of these kernels into a single operation,therefore saving memory bandwidth and potentially improving performance.

And this is how you can use the quantization support in MPSGraph.

In addition to the previous new features,MPSGraph supports even more machine learning operators.Starting this year, complex types are supported for most graph operations.You can use complex numbers either with single precisionor half precision floating point formats.

Building on the complex data type,MPSGraph adds operators for computing Fast Fourier Transformations.You can apply complex to complex, complex to real,and real to complex transformations up to four dimensions.These are very common in audio, video, and image processing applications.Furthermore, using MPSGraph,you can now perform three-dimensional convolutions,grid sampling, Sort and ArgSort, and cumulative operations,including sums, products, minima, and maxima.And this concludes the discussion about the new features in MPSGraph.Let's review what was presented today in this session.

I went over the improvements in accelerating popular ML frameworkslike PyTorch and TensorFlow through Metal.Now you can also take advantage of the new Metal accelerated JAX framework.We also discussed how to seamlessly integrate your existing modelsfrom other frameworks to MPSGraph using the new serialization tools.And this concludes our talk.We can't wait to see the amazing content that you will createusing all of these features.Thanks for watching.♪ ♪

## Code Samples

