# Wwdc2023 10073

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Design for spatial inputLearn how to design great interactions for eyes and hands. We'll share the design principles for spatial input, explore best practices around input methods, and help you create spatial experiences that are comfortable, intuitive, and satisfying.Chapters0:00 -Introduction2:22 -Eyes12:21 -Hands18:36 -ConclusionResourcesHD VideoSD VideoRelated VideosWWDC23Design considerations for vision and motionDesign for spatial user interfacesDesign spatial SharePlay experiencesExplore immersive sound designPrinciples of spatial design

Learn how to design great interactions for eyes and hands. We'll share the design principles for spatial input, explore best practices around input methods, and help you create spatial experiences that are comfortable, intuitive, and satisfying.

0:00 -Introduction

2:22 -Eyes

12:21 -Hands

18:36 -Conclusion

HD VideoSD Video

HD Video

SD Video

Design considerations for vision and motion

Design for spatial user interfaces

Design spatial SharePlay experiences

Explore immersive sound design

Principles of spatial design

Search this video…♪ Mellow instrumental hip-hop ♪♪Israel Pastrana Vicente: Hello and welcometo "Design for Spatial Input."My name is Israel, and I'm here with Eugene.We are designers on the Apple Design team.Eugene Krivoruchko: Today, we'll talk aboutdesigning interactions for eyes and hands.We'll cover what's special about these new input methods,and how to make the best use of them on our platform.Israel: Let's take a quick lookat all the available input modalities.With spatial input, you can simply look at a buttonand tap your fingers together to select it,keeping your arm relaxed on your lap.Our system is designed to interact with UIcomfortably at a distance.In some cases, you can also interact with elements directly.For example, typing on a virtual keyboardusing your fingertips.Holding your hands in the air can cause fatigue,but we will see that some tasks are better suitedto interact directly.Now, eyes and hands are the new spatial inputs,but you can also use other familiar inputslike voice to search without needing to type.Or keyboard and trackpad, which are greatfor getting things done.Lastly, you can also connect a game controllerto play your favorite games.We are going to focus on the most newand exciting spatial inputs: eyes and hands.Using your eyes and hands to interactis distinct in a few ways.First, it's personal.Your eye movements and hand gestures are unique to you.An array of cameras inside and outside the devicecapture all the details of your natural movementsin a privacy-respectful way.Next, it's comfortable.You can keep your hands resting beside youbecause the device sees a wide area around you.Lastly, it makes spatial interactions precise.The device filters all the data and translates itinto accurate interactions that you can use in your apps.So spatial input, it's a personal inputthat feels incredibly comfortable while providing yougreat precision to control your interactions.Today, we'll be going over how to use your eyes and handsto interact naturally with your apps.Let's start with eyes.Eyes are the primary targeting mechanismfor spatial experiences.All the interfaces in the system react to where you look.And we can effortlessly target any elementjust by looking at them, no matter how far away they are.Now, I'll talk about how to make appsthat are comfortable to interact with;how to make them easy to target with your eyes;how to make interfaces that respond to where you lookwhile respecting privacy;and finally, how eye intent simplifies our layoutsand offers unique assistive options.In order to build apps that are comfortable for the eyes,we need to consider how your content shows up in the device.Here's the first thing to consider.Even though you have an infinite canvas for your apps,you only see the content inside the field of view.Within the field of view,it's most comfortable to look in the center,and it's less comfortable to look at the edges.So, design apps that fit inside the field of view,minimizing neck and body movement.Try to keep the main content of the app in the centerof the field of view, the most comfortable area for your eyes.Looking at the edges of the field of viewcan be tiring for your eyes, so use these areasfor content that you don't need all the time,like secondary actions, which remain accessibleand don't interfere with the main content.Always try to maximize eye and neck comfort in your appsby placing the content inside the field of view.Now, we should also consider depthwhen thinking about eye comfort.Depth is a unique feature of spatial experiences.Placing your content near or far awaycreates different feelings in your projects.But our eyes focus on one distance at a time,and changing the focus depth frequentlycan create eye strain.Look to keep interactive content at the same depthto make it feel effortless to switch between UI.For example, presenting a modal viewpushes the main view in the z-axis,and the modal is placed at the original distance.By maintaining the same Z position,your eyes don't need to adapt to the new distance.Now, you can use subtle changes in depthto communicate hierarchy, like in this example,with a tab bar on the left and a segmented controlat the bottom.This way, you're using depth meaningfully,while avoiding eye discomfort.Now that we've seen how to make an app comfortable,we also need to make it easy to use with your eyes.Eyes are very precise, but there are certain qualitiesthat help our eyes target successfully on UI elements.Our eyes naturally focus on shapes that guide our attentionto the middle of an object.To help our eyes, use round shapeslike circles, pills, and rounded rectangles.Avoid using shapes with sharp edges.When you use sharp edges, your eyes tend to focuson the outside, decreasing eye precision.Also, keep the shapes flat and avoid thick outlinesor effects that call attention to the edges.And lastly, make sure to center the text and the glyphsin your elements using generous padding.So, always make sure that your UI is designedto guide the eyes to the center of the element.Now that our attention is in the middle of our elements,let's look at the right size for your controls.The minimum area that your element needs for eye targetis 60 points.But the element can be smaller than 60 points.You can achieve the minimum target areacombining size and spacing.Use generous spacing between elements in your layouts.This will help you target quickly and accuratelywith your eyes.Again, it's very important to respect the minimum target areaof 60 points, combining size and spacingto make your UI look great and easy to use with your eyes.Out of the box, standard componentshave sizes that are easy to target.Use these components as much as you can.And if you use your own,make sure to follow our guidelines on sizing.To learn more about points and layout,check out the session "Design for spatial user interfaces."Now that we've learned about how important the target area isfor your eyes, we have to make sure thatwe maintain this target area in any position in a space.For that, we need to understand how to scale your UI.Let's look at two different scale mechanisms.The system provides dynamic scale for app windows.You can see how the window scales larger as it moves away,and smaller as it moves close.Dynamic scale makes your UI fill the same field of viewand preserve the size of the target areas,no matter where the window is positioned.If you use fixed scale instead, your UI becomes smalleras it moves away.Fixed scale changes the size of the interfaceand makes your app difficult to use with your eyes.Let's look at this side by side.Dynamic scale keeps your UI and the target areasat the same size, while fixed scalechanges the size and makes the target areas too small.When you create custom UI, use dynamic scale to ensurethat your eyes can always target all the controls.Besides scale, orientation also affectsthe usability of your app.If the interface is at an angle,it's difficult to read and hard to use.That's why system windows are always oriented to face people.But if you create custom windows in your app,always make sure to keep your UI facing the viewer.As we've just seen, correct scale and orientationof windows and UI are fundamentalto ensure accuracy with your eyes.To learn more about how windows on this platform behave,check out the session "Principles of spatial design."Eyes are a very novel input. and it's really importantto make your interfaces respond to your eyes.When interactive elements highlight,you understand that your eyes are driving the interaction.Let's see what happens when you look at a group of buttons.See how they highlight as you look at each one.All interactive elements should be highlighted,and we do this with a hover effect.But because your eyes move quickly,the effect needs to be subtle and work on top of any content,like when looking at your favorite photos,reinforcing intention without being prominent.Thanks to the hover effect, all the system-provided controlshighlight when you look at them.If you create custom elements for your apps,use hover effects to add eye feedbackand make your elements feel responsive.Now, eye intention is very sensitive information.Privacy is our top priority when dealing with eye data.The hover effect happens out of your app's process,so you will only get the informationof which element is focused when there is an interactionon the element triggered by a gesture.Hovering on an element with your eyesis a signal for intention.When you look at something for a long time,we know that you are interested in it.It is a great opportunityto show you more information about it.For example, buttons can have tooltipsthat reveal as you look at them.Also, tab bars expand when you focus on them,showing a label for each tab.Lastly, focusing on the microphone glyphinside a system-provided search fieldwill trigger Speak to Search, revealing this layerand allowing you to perform a searchusing just eyes and voice.All these system elementsgive extra information when you need it,while keeping a clean UI when not in focus.Take advantage of them when creating your apps.They are also built with privacy in mind to ensurethat no focus information is being sent to the app.Eye intent also provides great opportunitiesfor assistive technology.For example, using the Dwell Control feature,you can select content just with your eyes.In this example, focusing on a button for a short timewill show the Dwell Control UI and will select the buttonwithout needing to perform a tap gesture with your hand.So, what did we just learnabout designing interactions for eyes?We learned about how to make your apps comfortablefor your eyes by placing content in front of the viewer,inside the field of view, and using depth responsibly.Then we looked at how to design interfaces that are easy to useand how to guide your eyes to the center of the elements.We also reinforced how important it isto respect the minimum target area of 60 pointsin your controls.How we should always communicate interactivityand reinforce targeting by adding hover effectsto your elements.And lastly, how we can take advantage of UI elementsthat reveal extra information on eye intent.I think this was great, and there's even more.We've seen that eyes are a fantastic target mechanism,and they become much more powerfulwhen you combine them with hands.Now, I'll hand it over to Eugene to talk about it.Eugene: Thanks, Israel.Let's talk about hands.Combined with eyes for targeting,hand gestures are the primary wayto interact across the system.Pinching your fingers together is an equivalentof pressing on the screen of your phone.The system supports other familiar gestures.For example, you can pinch and drag to scroll,and perform two-handed gestures like zoom and rotate.Notice how in all these cases, UI feedback continuesthe motion of the hand, which really helps it feelconnected to the gesture.The gestures work the same way across the systemand follow the logic similar to the Multi-Touch gestures.This means that people can really focus on the experience,instead of having to think about how to perform the interaction.This is why you should lean on these familiar patternswhen designing your experience,and make sure to respond to gesturesin a way that matches people's expectations.In some cases, part of your experiencemight be a unique behaviorthat can't easily be expressed with standard gestures.In this case, you may want to define a custom one.Here are some tips on howto make a successful custom gesture.First, make sure that the gesture is easy to explainand perform so that people can learn how to use it quickly.It is also important to avoid gesture conflicts.Your custom gesture needs to be distinctly differentfrom the standard system set or common hand movementspeople might use in the conversation.This has to be a gesture that people can consistently repeatwithout strain or fatigue,and that has a low rate of false activations.Be mindful of people who are using assistive technologiesto interact across the systemand consider how your gesture will work in those cases.To learn more about accessibility,check out the session "Create accessible spatial experiences."Gestures can also mean different thingsto different people, so make sure your custom gesturedoesn't send messages you didn't intend.All of this may be a tricky balance to hit,so it's always worth considering a fallbackin the form of a UI affordance.One of the most exciting aspects of our input modelis the opportunity to use eyes as a signal of intent.Using eye direction combined with hand gestures,we can create precise and satisfying interactionsthat are not possible on other platforms.Let's look at the zoom gesture again to see what I mean.At the start of the gesture, the origin point of the zoomis determined by where within the imageyour eyes are focused at that moment.This results in that particular area to be magnifiedand centered as you zoom in.As a result, you can navigate the image easilyjust by looking around and performing this simple gesture.This feels really magical and 100 percent expectedat the same time.The point you are looking at naturally indicatesthe intent of that interaction.Another example of this behavior is pointer movement in Markup.To draw, you control the brush cursor with your hand,similar to a mouse pointer, but then if you lookto the other side of the canvas and tap,the cursor jumps there landing right where you're looking.This creates a sense of accuracyand helps to cover the large canvas quickly.These are examples of interactionsthat use eye direction to make simple behaviorsmore precise and satisfying.Eyes are used not only to target elements,but to implicitly provide a more granular locationfor that interaction.This is a really powerful aspect of our input modelthat allows us to respond to interactionin a much more intelligent way.Now let's talk about direct touch.Across the system, we support being able to reach outand use your fingertips to interact.For example, you can bring Safari close to youand scroll the page directly.You can also use both your handsto type on the virtual keyboardand even have more spatial experiences,manipulating 3D content within your arm's reach.Interactions at a distance stay comfortable for a long timebecause it's easy to target controls with your eyes,and your hands can stay restedwhile performing minimal gestures.When designing for direct interaction,we have to keep in mind that holding hands in the airwill cause fatigue after a while.Still, certain apps will benefit from placing contentwithin arm's reach for direct touch,like experiences that invite up close inspectionor object manipulation;or any interactive mechanic that builds on topof the muscle memory from real-world experiences;and generally, whenever physical activityis at the center of the experience.Lack of tactile response is another thing to considerwhen designing for direct interaction.Every time we touch something in the physical world,our hands receive lots of multisensory feedback,which is essential to our perception.None of this is happening when we reach outand touch virtual content.And to make that interaction work,we need to compensate for the missing sensory informationwith other types of feedback.Let's look at how we approached this challengewith keyboard buttons.The buttons are actually raised above the platterto invite pushing them directly.While the finger is above the keyboard,buttons display a hover state and a highlightthat gets brighter as you approach the button surface.It provides a proximity cueand helps guide the finger to target.At the moment of contact, the state changeis quick and responsive, and is accompaniedby matching spatial sound effect.These additional layers of feedback are really importantto compensate for missing tactile information,and to make direct interactions feel reliable and satisfying.Audio plays a special role in connecting inputwith virtual content across the system.To learn more about it, check out the session called"Explore immersive sound design."To recap, here are the takeawaysfor designing interactions with hands.Use gesture language consistent with the systemso people can focus on the contentinstead of the interaction.Be careful to only introduce custom gestureswhen the desired behavior can't be achievedwith the standard set.Look for ways to improve your interactionusing eyes as a signal of intent.Only use direct interactionwhen it's at the core of your experience.And if you do, provide extensive feedbackto compensate for missing sensory information.Today, we have talked about some of the design principlesfor spatial interactions with eyes and hands.We talked a lot about comfort and ergonomics.With so many ways for how software can look,behave, and react to input on this platform,there is more responsibility on designers and developersto make sure these experiences are comfortable and accessible.By running your app on device, people welcome your workinto their space and give it their full attention.Software is no longer contained within a screen.Instead, it's allowed to occupy a more significant portionof people's physical surroundingsand react to their natural body movements.Using hands to interact with virtual contentis also something very new for most people.That's why it's so important to guide themby providing clear feedback and to rely onfamiliar interaction patterns where possible.As we know from designing for other platforms,great input experience is the onethat you don't have to think about.Software response becomes a natural continuationof your body movementand perfectly matches the interaction intent.Our ability to use eyes as the foundationof the input model opens up opportunitiesto respond to interaction with magical precision.We think it's really powerful, and we hope you will use itto create delightful and novel interactionsfor the spatial medium.Please make sure to check out the sessionsthat we have referenced throughout the talk.Thanks for watching!Israel: Adios!♪

♪ Mellow instrumental hip-hop ♪♪Israel Pastrana Vicente: Hello and welcometo "Design for Spatial Input."My name is Israel, and I'm here with Eugene.

We are designers on the Apple Design team.

Eugene Krivoruchko: Today, we'll talk aboutdesigning interactions for eyes and hands.

We'll cover what's special about these new input methods,and how to make the best use of them on our platform.

Israel: Let's take a quick lookat all the available input modalities.

With spatial input, you can simply look at a buttonand tap your fingers together to select it,keeping your arm relaxed on your lap.

Our system is designed to interact with UIcomfortably at a distance.

In some cases, you can also interact with elements directly.

For example, typing on a virtual keyboardusing your fingertips.

Holding your hands in the air can cause fatigue,but we will see that some tasks are better suitedto interact directly.

Now, eyes and hands are the new spatial inputs,but you can also use other familiar inputslike voice to search without needing to type.

Or keyboard and trackpad, which are greatfor getting things done.

Lastly, you can also connect a game controllerto play your favorite games.

We are going to focus on the most newand exciting spatial inputs: eyes and hands.

Using your eyes and hands to interactis distinct in a few ways.

First, it's personal.

Your eye movements and hand gestures are unique to you.

An array of cameras inside and outside the devicecapture all the details of your natural movementsin a privacy-respectful way.

Next, it's comfortable.

You can keep your hands resting beside youbecause the device sees a wide area around you.

Lastly, it makes spatial interactions precise.

The device filters all the data and translates itinto accurate interactions that you can use in your apps.

So spatial input, it's a personal inputthat feels incredibly comfortable while providing yougreat precision to control your interactions.

Today, we'll be going over how to use your eyes and handsto interact naturally with your apps.

Let's start with eyes.

Eyes are the primary targeting mechanismfor spatial experiences.

All the interfaces in the system react to where you look.

And we can effortlessly target any elementjust by looking at them, no matter how far away they are.

Now, I'll talk about how to make appsthat are comfortable to interact with;how to make them easy to target with your eyes;how to make interfaces that respond to where you lookwhile respecting privacy;and finally, how eye intent simplifies our layoutsand offers unique assistive options.

In order to build apps that are comfortable for the eyes,we need to consider how your content shows up in the device.

Here's the first thing to consider.

Even though you have an infinite canvas for your apps,you only see the content inside the field of view.

Within the field of view,it's most comfortable to look in the center,and it's less comfortable to look at the edges.

So, design apps that fit inside the field of view,minimizing neck and body movement.

Try to keep the main content of the app in the centerof the field of view, the most comfortable area for your eyes.

Looking at the edges of the field of viewcan be tiring for your eyes, so use these areasfor content that you don't need all the time,like secondary actions, which remain accessibleand don't interfere with the main content.

Always try to maximize eye and neck comfort in your appsby placing the content inside the field of view.

Now, we should also consider depthwhen thinking about eye comfort.

Depth is a unique feature of spatial experiences.

Placing your content near or far awaycreates different feelings in your projects.

But our eyes focus on one distance at a time,and changing the focus depth frequentlycan create eye strain.

Look to keep interactive content at the same depthto make it feel effortless to switch between UI.

For example, presenting a modal viewpushes the main view in the z-axis,and the modal is placed at the original distance.

By maintaining the same Z position,your eyes don't need to adapt to the new distance.

Now, you can use subtle changes in depthto communicate hierarchy, like in this example,with a tab bar on the left and a segmented controlat the bottom.

This way, you're using depth meaningfully,while avoiding eye discomfort.

Now that we've seen how to make an app comfortable,we also need to make it easy to use with your eyes.

Eyes are very precise, but there are certain qualitiesthat help our eyes target successfully on UI elements.

Our eyes naturally focus on shapes that guide our attentionto the middle of an object.

To help our eyes, use round shapeslike circles, pills, and rounded rectangles.

Avoid using shapes with sharp edges.

When you use sharp edges, your eyes tend to focuson the outside, decreasing eye precision.

Also, keep the shapes flat and avoid thick outlinesor effects that call attention to the edges.

And lastly, make sure to center the text and the glyphsin your elements using generous padding.

So, always make sure that your UI is designedto guide the eyes to the center of the element.

Now that our attention is in the middle of our elements,let's look at the right size for your controls.

The minimum area that your element needs for eye targetis 60 points.

But the element can be smaller than 60 points.

You can achieve the minimum target areacombining size and spacing.

Use generous spacing between elements in your layouts.

This will help you target quickly and accuratelywith your eyes.

Again, it's very important to respect the minimum target areaof 60 points, combining size and spacingto make your UI look great and easy to use with your eyes.

Out of the box, standard componentshave sizes that are easy to target.

Use these components as much as you can.

And if you use your own,make sure to follow our guidelines on sizing.

To learn more about points and layout,check out the session "Design for spatial user interfaces."Now that we've learned about how important the target area isfor your eyes, we have to make sure thatwe maintain this target area in any position in a space.

For that, we need to understand how to scale your UI.

Let's look at two different scale mechanisms.

The system provides dynamic scale for app windows.

You can see how the window scales larger as it moves away,and smaller as it moves close.

Dynamic scale makes your UI fill the same field of viewand preserve the size of the target areas,no matter where the window is positioned.

If you use fixed scale instead, your UI becomes smalleras it moves away.

Fixed scale changes the size of the interfaceand makes your app difficult to use with your eyes.

Let's look at this side by side.

Dynamic scale keeps your UI and the target areasat the same size, while fixed scalechanges the size and makes the target areas too small.

When you create custom UI, use dynamic scale to ensurethat your eyes can always target all the controls.

Besides scale, orientation also affectsthe usability of your app.

If the interface is at an angle,it's difficult to read and hard to use.

That's why system windows are always oriented to face people.

But if you create custom windows in your app,always make sure to keep your UI facing the viewer.

As we've just seen, correct scale and orientationof windows and UI are fundamentalto ensure accuracy with your eyes.

To learn more about how windows on this platform behave,check out the session "Principles of spatial design."Eyes are a very novel input. and it's really importantto make your interfaces respond to your eyes.

When interactive elements highlight,you understand that your eyes are driving the interaction.

Let's see what happens when you look at a group of buttons.

See how they highlight as you look at each one.

All interactive elements should be highlighted,and we do this with a hover effect.

But because your eyes move quickly,the effect needs to be subtle and work on top of any content,like when looking at your favorite photos,reinforcing intention without being prominent.

Thanks to the hover effect, all the system-provided controlshighlight when you look at them.

If you create custom elements for your apps,use hover effects to add eye feedbackand make your elements feel responsive.

Now, eye intention is very sensitive information.

Privacy is our top priority when dealing with eye data.

The hover effect happens out of your app's process,so you will only get the informationof which element is focused when there is an interactionon the element triggered by a gesture.

Hovering on an element with your eyesis a signal for intention.

When you look at something for a long time,we know that you are interested in it.

It is a great opportunityto show you more information about it.

For example, buttons can have tooltipsthat reveal as you look at them.

Also, tab bars expand when you focus on them,showing a label for each tab.

Lastly, focusing on the microphone glyphinside a system-provided search fieldwill trigger Speak to Search, revealing this layerand allowing you to perform a searchusing just eyes and voice.

All these system elementsgive extra information when you need it,while keeping a clean UI when not in focus.

Take advantage of them when creating your apps.

They are also built with privacy in mind to ensurethat no focus information is being sent to the app.

Eye intent also provides great opportunitiesfor assistive technology.

For example, using the Dwell Control feature,you can select content just with your eyes.

In this example, focusing on a button for a short timewill show the Dwell Control UI and will select the buttonwithout needing to perform a tap gesture with your hand.

So, what did we just learnabout designing interactions for eyes?We learned about how to make your apps comfortablefor your eyes by placing content in front of the viewer,inside the field of view, and using depth responsibly.

Then we looked at how to design interfaces that are easy to useand how to guide your eyes to the center of the elements.

We also reinforced how important it isto respect the minimum target area of 60 pointsin your controls.

How we should always communicate interactivityand reinforce targeting by adding hover effectsto your elements.

And lastly, how we can take advantage of UI elementsthat reveal extra information on eye intent.

I think this was great, and there's even more.

We've seen that eyes are a fantastic target mechanism,and they become much more powerfulwhen you combine them with hands.

Now, I'll hand it over to Eugene to talk about it.

Eugene: Thanks, Israel.

Let's talk about hands.

Combined with eyes for targeting,hand gestures are the primary wayto interact across the system.

Pinching your fingers together is an equivalentof pressing on the screen of your phone.

The system supports other familiar gestures.

For example, you can pinch and drag to scroll,and perform two-handed gestures like zoom and rotate.

Notice how in all these cases, UI feedback continuesthe motion of the hand, which really helps it feelconnected to the gesture.

The gestures work the same way across the systemand follow the logic similar to the Multi-Touch gestures.

This means that people can really focus on the experience,instead of having to think about how to perform the interaction.

This is why you should lean on these familiar patternswhen designing your experience,and make sure to respond to gesturesin a way that matches people's expectations.

In some cases, part of your experiencemight be a unique behaviorthat can't easily be expressed with standard gestures.

In this case, you may want to define a custom one.

Here are some tips on howto make a successful custom gesture.

First, make sure that the gesture is easy to explainand perform so that people can learn how to use it quickly.

It is also important to avoid gesture conflicts.

Your custom gesture needs to be distinctly differentfrom the standard system set or common hand movementspeople might use in the conversation.

This has to be a gesture that people can consistently repeatwithout strain or fatigue,and that has a low rate of false activations.

Be mindful of people who are using assistive technologiesto interact across the systemand consider how your gesture will work in those cases.

To learn more about accessibility,check out the session "Create accessible spatial experiences."Gestures can also mean different thingsto different people, so make sure your custom gesturedoesn't send messages you didn't intend.

All of this may be a tricky balance to hit,so it's always worth considering a fallbackin the form of a UI affordance.

One of the most exciting aspects of our input modelis the opportunity to use eyes as a signal of intent.

Using eye direction combined with hand gestures,we can create precise and satisfying interactionsthat are not possible on other platforms.

Let's look at the zoom gesture again to see what I mean.

At the start of the gesture, the origin point of the zoomis determined by where within the imageyour eyes are focused at that moment.

This results in that particular area to be magnifiedand centered as you zoom in.

As a result, you can navigate the image easilyjust by looking around and performing this simple gesture.

This feels really magical and 100 percent expectedat the same time.

The point you are looking at naturally indicatesthe intent of that interaction.

Another example of this behavior is pointer movement in Markup.

To draw, you control the brush cursor with your hand,similar to a mouse pointer, but then if you lookto the other side of the canvas and tap,the cursor jumps there landing right where you're looking.

This creates a sense of accuracyand helps to cover the large canvas quickly.

These are examples of interactionsthat use eye direction to make simple behaviorsmore precise and satisfying.

Eyes are used not only to target elements,but to implicitly provide a more granular locationfor that interaction.

This is a really powerful aspect of our input modelthat allows us to respond to interactionin a much more intelligent way.

Now let's talk about direct touch.

Across the system, we support being able to reach outand use your fingertips to interact.

For example, you can bring Safari close to youand scroll the page directly.

You can also use both your handsto type on the virtual keyboardand even have more spatial experiences,manipulating 3D content within your arm's reach.

Interactions at a distance stay comfortable for a long timebecause it's easy to target controls with your eyes,and your hands can stay restedwhile performing minimal gestures.

When designing for direct interaction,we have to keep in mind that holding hands in the airwill cause fatigue after a while.

Still, certain apps will benefit from placing contentwithin arm's reach for direct touch,like experiences that invite up close inspectionor object manipulation;or any interactive mechanic that builds on topof the muscle memory from real-world experiences;and generally, whenever physical activityis at the center of the experience.

Lack of tactile response is another thing to considerwhen designing for direct interaction.

Every time we touch something in the physical world,our hands receive lots of multisensory feedback,which is essential to our perception.

None of this is happening when we reach outand touch virtual content.

And to make that interaction work,we need to compensate for the missing sensory informationwith other types of feedback.

Let's look at how we approached this challengewith keyboard buttons.

The buttons are actually raised above the platterto invite pushing them directly.

While the finger is above the keyboard,buttons display a hover state and a highlightthat gets brighter as you approach the button surface.

It provides a proximity cueand helps guide the finger to target.

At the moment of contact, the state changeis quick and responsive, and is accompaniedby matching spatial sound effect.

These additional layers of feedback are really importantto compensate for missing tactile information,and to make direct interactions feel reliable and satisfying.

Audio plays a special role in connecting inputwith virtual content across the system.

To learn more about it, check out the session called"Explore immersive sound design."To recap, here are the takeawaysfor designing interactions with hands.

Use gesture language consistent with the systemso people can focus on the contentinstead of the interaction.

Be careful to only introduce custom gestureswhen the desired behavior can't be achievedwith the standard set.

Look for ways to improve your interactionusing eyes as a signal of intent.

Only use direct interactionwhen it's at the core of your experience.

And if you do, provide extensive feedbackto compensate for missing sensory information.

Today, we have talked about some of the design principlesfor spatial interactions with eyes and hands.

We talked a lot about comfort and ergonomics.

With so many ways for how software can look,behave, and react to input on this platform,there is more responsibility on designers and developersto make sure these experiences are comfortable and accessible.

By running your app on device, people welcome your workinto their space and give it their full attention.

Software is no longer contained within a screen.

Instead, it's allowed to occupy a more significant portionof people's physical surroundingsand react to their natural body movements.

Using hands to interact with virtual contentis also something very new for most people.

That's why it's so important to guide themby providing clear feedback and to rely onfamiliar interaction patterns where possible.

As we know from designing for other platforms,great input experience is the onethat you don't have to think about.

Software response becomes a natural continuationof your body movementand perfectly matches the interaction intent.

Our ability to use eyes as the foundationof the input model opens up opportunitiesto respond to interaction with magical precision.

We think it's really powerful, and we hope you will use itto create delightful and novel interactionsfor the spatial medium.

Please make sure to check out the sessionsthat we have referenced throughout the talk.

Thanks for watching!Israel: Adios!♪

## Code Samples

