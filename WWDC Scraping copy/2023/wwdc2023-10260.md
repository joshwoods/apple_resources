# Wwdc2023 10260

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Get started with building apps for spatial computingGet ready to develop apps and games for visionOS! Discover the fundamental building blocks that make up spatial computing — windows, volumes, and spaces — and find out how you can use these elements to build engaging and immersive experiences.ResourcesHD VideoSD VideoRelated VideosWWDC23Build spatial experiences with RealityKitCreate immersive Unity appsDevelop your first immersive appMeet Reality Composer ProMeet RealityKit TraceMeet SwiftUI for spatial computingPrinciples of spatial design

Get ready to develop apps and games for visionOS! Discover the fundamental building blocks that make up spatial computing — windows, volumes, and spaces — and find out how you can use these elements to build engaging and immersive experiences.

HD VideoSD Video

HD Video

SD Video

Build spatial experiences with RealityKit

Create immersive Unity apps

Develop your first immersive app

Meet Reality Composer Pro

Meet RealityKit Trace

Meet SwiftUI for spatial computing

Principles of spatial design

Search this video…♪ Mellow instrumental hip-hop ♪♪Jim Tilander: Hi, I'm Jim, an engineeron the RealityKit team.Today, my colleague Christopher from the ARKit teamwill join me in guiding you through how toget started with building apps for spatial computing.Let's dive in!We are excited about our new platform for spatial computing.This platform is built on familiar foundations for peopleto use and for you to develop apps on.It opens up new and exciting possibilities to blendreal and virtual content, as well as using natural inputto interact with your app -and the whole system has been designedto protect people's privacy, giving you the peace of mindto focus on your app's experience.Let's talk a bit about the fundamentals to build upour vocabulary and concepts of spatial computing.After that, we will go over the different waysto get started with your app.Then, my colleague Christopher will walk us throughhow to build your app, diving deeperinto the details of spatial computing.Now, let's take a look at some of the fundamentals.First let's cover what bothfamiliar and new UI concepts mean in spatial computing.By default, apps launch into the Shared Space.This is where apps exist side-by-side,much like multiple apps on a Mac desktop.People remain connected to their surroundingsthrough passthrough.Each app can have one or more windows.These are SwiftUI scenes that can be resized and reflowedlike you would expect of a normal macOS window.They can contain traditional views and controls,as well as 3D content, allowing you to mix and match 2D and 3D.People can reposition a window to their likingin their current space, just as one might expect.Volumes allow an app to display 3D content in defined bounds,sharing the space with other apps.Volumes are great for showcasing 3D content,for example, a chess board.People can reposition volumes in space,and they can be viewed from different angles.Volumes are SwiftUI scenes, allowing you to do layoutin familiar ways, and they use the power of RealityKitto display your 3D content.Sometimes you might want to have more controlof the level of immersion in your app…maybe to focus while watching a video or to play a game.You can do this by opening a dedicated Full Space,where your app's windows, volumes, and 3D objectsare the only ones appearing across the view.In a Full Space, you can also take advantage of ARKit's APIs.For example, in addition to system-provided gestures,you can get more detailed Skeletal Hand Trackingto really incorporate the structure of people's handsinto your experience.Your app can use a Full Space in different ways.You can use passthrough to ground contentin the real world and keep people connectedwith their surroundings.And when you play Spatial Audio and render 3D through RealityKityou will automatically take advantage of the factthat the device will continually updateunderstanding of the room to blend visuals and soundinto people's surroundings, making them feelthat these virtual objects really belong in their room.You can also choose to render to a fully-immersive spaceto fill up the entire field of view.This allows your app flexibilityto deliver on creative intent of your appby customizing the lighting of virtual objects,as well as the ability to choose audio characteristics.These are the foundational elements of spatial computing:windows, volumes, and spaces.They give you a flexible toolset to build appsthat can span the continuum of immersion.Christopher will talk more about this later.Now that we've introduced the foundational elementsof spatial computing, let's explore the wayswe can interact with windows, volumes, and spaces.On this platform, we can interact with appsby simply using our eyes and hands.People can, for example,interact with a button by looking at itand tapping their fingers together to select.People can also reach outand physically touch the same button in 3D space.For both these kinds of interactions,there is a variety of gestures that are possible, like taps,long presses, drags, rotations, zooms, and a lot more.The system detects these automaticallyand generates touch events for your app to respond to.Gestures are integrated well with SwiftUI.The same gesture API works seamlesslywith RealityKit entities.This allows people to easily interactdirectly with your 3D scene elements.For example, this could be useful to place a flagdirectly onto this 3D model,or imagine controlling a virtual zipperor perhaps you want to interact and pick upvirtual chess pieces.Now if you want to do a game of bowlingor transform people's hands into a virtual club,you can do this through ARKit's Skeletal Hand Tracking.Here we can see an example how you can stack cubeson a table using tapsand then smashing them onto the table with your hands.This is a powerful way that you can bringapp-specific hands input into the experience.And finally, the system automatically brings inputfrom wireless keyboards, trackpads,and accessibility hardware right into your app,and the Game Controller framework lets you add supportfor wireless game controllers as well.Collaborating and exploring things togetheris a fundamental part of spatial computing.We do this through SharePlayand the Group Activities framework.On this platform, as on macOS, people can share any window,like this Quick Look experience.When people share a Quick Look 3D model,we sync the orientation, scale and animationsbetween participants, making it easy to collaboratewhile being in different locations.When people are collaborating on something that is shownin their space and that they physically point at,it is important that everyone in the SharePlay sessionhave the same experience.This enables natural references such as gesturing to an objectand reinforces the feeling of being physically together.We've added the concept of shared context to the system.The system manages this shared context for youhelping make sure that participantsin a SharePlay session can all experience contentin the same way.You can use Spatial Persona Templatesto further customize how people experience your content.To learn more, watch our sessions about designingand building spatial SharePlay experiences for this platform.Given that the device has a lot of intimate knowledgeof the surroundings and people, we put a lot of architecturein place to protect people's privacy.Let's dive into that.Privacy is a core principle for guiding the designof this platform, while making it easy for you as a developerto leverage APIs to take advantageof the many capabilities of the device.Instead of allowing apps to access datafrom the sensors directly, the system does that for youand provides apps with events and visual cues.For example, the system knows the eye position and gesturesof somebody's hands in 3D spaceand delivers that as touch events.Also, the system will render a hover effect on a viewwhen it is the focus of attentionbut does not communicate to the app where the person is looking.For many situations, the system-provided behaviorsare sufficient for your app to respond to interactions.In cases where you actually do need accessto more sensitive information, the system will ask the peoplefor their permission first.An example would be asking user permissionto access scene understanding to detect walls and furnitureor access to Skeletal Hand Trackingto bring custom interactions into your app.Now that we've seen some of the capabilitiesavailable for apps, let's move onto exploringhow we are developing those apps.Everything starts with Xcode,Apple's integrated development environment.Xcode offers a complete set of tools for developing apps,including project management support,visual editors for your UI, debugging tools, a Simulator,and much more.And most importantly, Xcode also comes withthe platform SDK, which provides the complete set of frameworksand APIs you'll use for developing your app.If your source file contains a SwiftUI preview provider,the preview canvas will automatically open up in Xcode.The preview canvas has been extended to support 3D,allowing you to visualize RealityKit code for your scene,including animations and custom code.This enables shorter iteration times,finding the right look and feel for your appas you edit live code and see the resultsof changes and tweaks directly.Let's experiment a little bit herewith how the satellite looks orbiting the Earthby changing the orbital speed and the size of the satellite.Notice the preview reflects the code changes,making it easy to see the resultsof quick experimentation in the code.Xcode Previews also has an object modethat allows for quick previews of 3D layouts -for example, seeing if your layout fitsinside the bounds of the view.This is great for building tightly-integrated sceneswith both traditional UI and new 3D visuals.Xcode Preview gives you a fantastic wayto get the layout right before you run your app.The Simulator is a great way of testing interactivitywith your app.You can move and look around in the scene using a keyboard,mouse or compatible game controller.And it's easy to interact with your appby using simulated system gestures.The Simulator comes with three different simulated scenes,each with a day and night lighting.This makes it easy to see your app under different conditions.The Simulator is a great way to run and debug most appsand to quickly iterate during developmentwith a very predictable environment.We've also extended Xcode to support a numberof runtime visualizations while you are debuggingto help you quickly understand and track down bugsby simply looking at the scene.Here we have plane estimation visible,including semantic meaning of those planesand the collision shapes in the scene.It's easy to toggle visualizationsyou would like to focus on from the debugger in Xcode.These visualizations works great both in the Simulatorand in the device.When it becomes time to polish your application's performanceand responsiveness, we've got familiar tools like Instruments.Instruments is a powerful performance analysis toolincluded with Xcode.You can use Instruments to provide youwith actionable insights of your running app.And for spatial computing, Instruments 15includes a new template, RealityKit Trace,providing even more and deeper insightsinto new behaviors on the platform.The RealityKit Trace template has new instrumentsallowing developers to understand GPU, CPU,and system power impact of their app,and identify performance hotspots.You can easily observe and understand frame bottlenecksand trace them back to vital metrics like total trianglessubmitted or number of RealityKit entities simulated.This lets you quickly find and addresspotential performance issues.For more details, check out the session "Meet RealityKit Trace."We've also introduced a new developer toolcalled Reality Composer Pro.It allows you to preview and prepare3D content for your apps.Reality Composer Pro helps you get an overviewof all your assets and how they fit together in your scene.A new feature that we added to RealityKit is particles,and you can use a workflow in Reality Composer Proto author and preview them.Adding particles into your scene provides movement, lifeand endless possibilities.Clouds, rain and sparks are just a few effectsthat you can build in a short amount of time.Adding audio into your scenesand associating them with objects is a breeze.You can also spatially preview audio,which takes into account the shape and contextof your entire scene.Most virtual objects will use RealityKit's physically-basedmaterial to represent a variety of real world materials.RealityKit uses sensor data to feedreal-world lighting information into these materials,grounding them in people's surroundings.RealityKit also has a couple of additional standard materialsavailable for your app to use in common scenarios.For those times when you have a very specific need,perhaps to convey a creative intent,you can author custom materials in Reality Composer Prowith the open standard MaterialX.You can do this through an easy-to-use node graph,without touching any code,and quickly preview them directly in the viewport.You can learn more about this in the session"Explore materials in Reality Composer Pro."When you're feeling good about your 3D content,you can send your scenes to your deviceand test your content directly.This is great for iteration times since you don't evenhave to build an app.To learn more, watch the session"Meet Reality Composer Pro."Another option that is available is Unity.Unity is bringing the ability for you to write appsfor spatial computing with familiar workflowsand without any plugins required.You can bring your existing content overto power new immersive experiences.To learn more, watch these sessionscovering how to write immersive apps with Unity.Now that we understand some of the fundamental conceptsand tools available to us,let's see how we can start building apps.There are two ways to get started -either you design a brand-new appfrom the ground up to be spatialor perhaps you have an existing appthat you want to bring into this new spatial platform.Let's explore how we build a new app.Designing an application from the ground up to be spatialhelps you to quickly embrace the new unique capabilitiesof spatial computing.To get started, you can use new app templatefor this platform.The app template has two new important options.First, you can choose your Initial Scene Typeto be either a ‘Window’ or a ‘Volume’.This generates the initial starting code for you,and it's easy to add additional scenes later.The second option lets you add an entry pointfor an immersive space to your app.By default, your app will launch into the Shared Space.If you select Immersive Scene Type to ‘Space’,a second scene will be added to your app,along with an example button showing how to launchinto this Full Space.And when you finish the assistant,you are presented with an initial working app in SwiftUIthat shows familiar buttons mixed in with a 3D objectrendered with RealityKit.To learn more, watch the session"Develop your first immersive app."We are also publishing code samples,each one of them illustrating different topicsto quickly get you up to speed.Destination Video shows how to builda shared, immersive playback experiencethat incorporates 3D video and Spatial Audio.Happy Beam is an example of how you can create a gamethat leverages an Immersive Space,including custom hand gestures,to create a fun game with friends.And Hello World shows how to transitionbetween different visual modes with a 3D globe.Christopher will talk more in detail about Hello World later.Building and designing your app from the ground upon this platform offers opportunities to easily embracespatial computing concepts.However, some of you might have existing appsthat you want to bring to spatial computing.From the start, iPad and iPhone apps look and feel great.If your app supports iPad,that variant will be preferred over iPhonethough iPhone-only apps are fully supported.Let's take a look at the recipes app shown here in the Simulator.While this platform has its own darker style,iPad and iPhone apps retain a light mode style.Windows can scale to allow for ease of use,and rotations for apps are handled,allowing you to see different layouts.To find out more, watch the session"Run your iPad and iPhone apps in the Shared Space"to learn about the system's built-in behaviors,functional differences and how to test with the Simulator.However, running an existing iPad or iPhone appis just the beginning.It's easy to add a destination in your Xcode projectfor this platform with just a click.And after that, we can simply select our target device,recompile and run.Once you recompile, you get native spacing,sizing and relayout.Your windows and materials will all automatically moveto the platform's look and feel, ensuring legibilityin any light condition,and your app can take advantage of built-in capabilitieslike highlighting for your custom controls.Now here’s Christopher to show us how we can evolve our appsfurther using the concepts we've covered so far.Thanks Jim.I'm going to walk you through how to build an applicationthat incorporates the elements you've learned previously.Let's start with Hello World to explore some ofthe great functionalities you can integrate into your app.Here’s our sample in action.Upon running the app in the Simulator,Hello World launches with a window into the Shared Space,right in front of us.This is a familiar-looking window made in SwiftUI,and it contains different elementssuch as text, images and buttons.Using tap gestures allows the navigation within the app.Observe how our new view has embedded 3D content.SwiftUI and 3D content now work together seamlessly.Going back to our main window and selecting Planet Earthbrings us to a new view.A new element appears. This is a volume.It contains a 3D model of the Earth,alongside a few UI elements.By moving the window bar, the volume's positioncan be adjusted anywhere in the surroundings.Going back to our main window againand selecting View Outer Space brings up an invitation for usto enter the solar system.From here, we can enter space, which is shown herewith an immersion style of ‘full’.Our example renders Planet Earth and dims passthrough,allowing us to focus on the contentwith no distractions of the surroundings.Now that we have seen how this looks in action,let's break down some of the functionalities of Hello Worldand show you how to use these concepts in your own apps.As you've learned from Jim,there are multiple elements: windows, volumes and spaces.You can look at this as a spectrum that your app can useto flex up and down, depending on what is bestfor people using your app in a specific moment.You can choose to present one or several windowsin the Shared Space, allowing people to be more present.They can see passthrough and have a choiceto have other apps side by side.Or… you can choose to increase the immersion levelby having your app take over the space entirely.Finding the most suitable elementsfor your app's experience in a given momentand flexing between them is an important considerationwhen you design your app for spatial computing.Next, Let's look further into how to use windowsas part of your experience.Windows serve as a starting point for your app.They are built with SwiftUI using scenes,and they contain traditional views and controls.Windows on this platform support mixing 2D and 3D content.This means that your 3D content can be presentedalongside 2D UI in a window.Windows can be resized and repositioned in space.People can arrange them as per their liking.Let's go back to our example.In Hello World, the content view holds our SwiftUI images,text and buttons, along with a call to actionto get more immersive content.Creating a window is as easyas adding a WindowGroup to a scene.Inside the WindowGroup, we will display our Content View.Our Content View can add 3D content,bringing a new dimension of depth to your app.To do that, you can use the new Model3D view.Model3D is similar to an image, making it easy to loadand display beautiful 3D content in your appthat is rendered by RealityKit.To add Model3D to your view,we initialize Model3D by passing the name of the satellite model.With this, Model3D will find and load the model,and place it into your view hierarchy.Now this window has the satellite embeddedinto the view and can be seen coming out of the z-axis,adding a new dimension of depth to your app.Now that we have added a satellite,we can add interactions.Interactions are fundamentally built into the systemand provided by SwiftUI.SwiftUI provides the gesture recognizersyou are already familiar with on Apple platforms,such as Tap, onHover, and RotateGesture.The platform provides new gesture recognizersthat are made for 3D interactions,like rotations in 3D space, taps on 3D objects and more.Let's look at the code that enablesinteractions with the satellite.We are going to enable a spatial tap gestureso we can grab and move the satellite around.Starting from Model3D, we can now add a gesture.Inside we add a DragGesturetargeted to the satellite entity.We can then use the values passed infrom the update closure to move the satellite.Lets see what that looks like.Back in our satellite view, where our satellite is rendered,note the DragGesture allows me to tap and drag the model,moving with my interactions.As we've just seen, it's easyto mix 2D and 3D content together with Model3D.These are just a few things you can do with a window.Now let's look at another type of element, volume.Lets see what a volume has to offer.Volume is an extension of a window,giving you similar functionality.A volume is a new style of windowthat is ideal for 3D content.They can host multiple SwiftUI viewscontaining your 2D or 3D content.Although volumes can be used in a Full Space,they are really built for the Shared Space,therefore content must remain within the bounds of the volume.Let's look at how to add a volume to your scene.You will start by creating a new WindowGroupand setting its windowStyle to volumetric.Then, you need to give it a defaultSizewith the properties width, height and depth.The units of a volume can be specified in points or meters.Let's look at this running in the Simulator.When the application is presented,the volume is placed in front of the person.This volume has the dimensions we specified,along with the platform controls:the application title bar, which displays our app name,making it easy to identify which app this volume belongs to;the window bar, enabling the volume to be positioned;and the close button, suspending the appwhen tapped, closing the volume.Currently, our volume renders the 3D model of the Earth,but you might want to start adding more contentand different behaviors.In order to do this, you can adopt RealityViewas part of your app.RealityView is a new view that can be added to your scene,allowing for any number of entities to be manageddirectly within SwiftUI.SwiftUI and RealityView let you easily integrate your app byconnecting to SwiftUI's managed state and entity properties.This makes it easy to drive the behavior of 3D modelswith a source of truth from your app's data model.Conversion between coordinate spaces is easywith conversion functions provided by RealityView,and RealityView offers a way to position SwiftUI elementsinside your 3D scene through attachments.Let's take a moment to look at how we can use attachmentsinside RealityView.The RealityView initializer that we're going to usetakes three parameters: a make closure,an update closure, and an attachments ViewBuilder.The make closure allows you to create entitiesand attach them to the root entity.The update closure, which is called wheneverthe state of the view changes.And lastly, the attachments closureis where we add our SwiftUI views with a tag propertythat allows RealityView to translate our viewsinto entities.Now, let's work through an exampleof how to use attachments with RealityView.Adding an attachment is as easy as putting your SwiftUI viewinside the attachment closure of RealityView.Let's use this icon of a delicious pastryto represent a location on our 3D globe.For each attachment, you must add a tagthat gives the attachment a name.I'll name this one ‘pin’.To display the attachment, I'll add it to the contentof my RealityView.I'll do that in the update closure by adding itto the root entity of the scene.Here, we can see the attachment we made previously,rendering on the globe above my favorite bakery location.As we've just seen, using RealityKitunleashes powerful features such as Model3D, RealityView,attachments and so many more.These can be easily integrated into your app.This is only scratching the surfaceof what RealityKit can do.If you want to know more, I encourage you to go and watch"Build spatial experiences with RealityKit"and "Enhance your spatial computing app with RealityKit."Let's recap what we went through so far.A volume is a container that is ideal for 2D and 3D content.Volumes are built for the Shared Space,can coexist with windows,and are bounded to specified dimensions.Next, let's dive into our last type of element, spaces.Once your app is opening a dedicated Full Space,the system hides all other apps,leaving only your app visible.Now you can place your app's window, volumesand content anywhere around you.Thanks to ARKit and RealityKit, your virtual contentcan even interact with your surroundings.You could throw a virtual ball into the room and watchas it bounces off of the wall and then rolls on the floor.And with the addition of hand tracking,you can build custom gestures and interactionsor place the content relative to people's hands.Many of these capabilities are coming from ARKit.To go into more depth and learn how you can leverage themin your app, be sure to check out"Meet ARKit for spatial computing" session.With spaces, your app can also offerdifferent levels of immersion, depending on which styleis chosen at creation time.Jim talked a bit about the spectrum of immersionavailable in a Full Space.Let's dive in and learn more abouthow you can add more immersion into your app.Immersion style is a parameterthat can be passed in your Full Space.There are two basic stylescalled .mixed and .full.Mixed style layers your app's content on top of passthrough.Full style hides passthrough and displays your content only.You can also combine the two by choosing progressive.This style allows some passthrough initially,but the person can change the level of immersionall the way up to full by turning the Digital Crownlocated on the top of the device.Let's go back to our example to explore immersion style.I'll start with the mixed style and see how that looks.And because Full Space is a SwiftUI scene,I can use RealityView to display the Earth.Here's the Earth viewed from high orbit…and here's how I displayed the scene in my app.Notice I didn't actually specify the immersion style.That's because when you create an immersive space,SwiftUI assumes mixed style by default.Let's also take your app completely immersiveby adding a different immersion style.This time, I'll use immersion style ‘full’.Adding an immersive styleto the end of our ImmersiveSpace is easy.We store the immersive style in our state variableand then set the type to full.Because we want to give people the choiceof when they enter an immersive experience,it's a good idea to add a button to allow the person to decideif they want to enter this immersive style.Now let's see the new immersive style in action.Back in our app, I've taken Hello Worldfrom a single window to fully immersed,allowing us to view Planet Earth from any angle.And that's just the beginning of what you can dowith your spatial app.Let's see where you can go from here.In this session, we've covered the fundamentals:how to get started,and then took you through the basics of building an app.We have some great sessions that should be your next stop -about the principles of spatial design,or to learn about building appswith SwiftUI and with RealityKit,or to begin creating your 3D content.With spatial computing, your app creationcan venture into new, exciting avenuesguided by your ingenuity.Thanks for watching!♪

♪ Mellow instrumental hip-hop ♪♪Jim Tilander: Hi, I'm Jim, an engineeron the RealityKit team.Today, my colleague Christopher from the ARKit teamwill join me in guiding you through how toget started with building apps for spatial computing.Let's dive in!We are excited about our new platform for spatial computing.This platform is built on familiar foundations for peopleto use and for you to develop apps on.It opens up new and exciting possibilities to blendreal and virtual content, as well as using natural inputto interact with your app -and the whole system has been designedto protect people's privacy, giving you the peace of mindto focus on your app's experience.Let's talk a bit about the fundamentals to build upour vocabulary and concepts of spatial computing.After that, we will go over the different waysto get started with your app.Then, my colleague Christopher will walk us throughhow to build your app, diving deeperinto the details of spatial computing.Now, let's take a look at some of the fundamentals.First let's cover what bothfamiliar and new UI concepts mean in spatial computing.By default, apps launch into the Shared Space.This is where apps exist side-by-side,much like multiple apps on a Mac desktop.People remain connected to their surroundingsthrough passthrough.Each app can have one or more windows.These are SwiftUI scenes that can be resized and reflowedlike you would expect of a normal macOS window.They can contain traditional views and controls,as well as 3D content, allowing you to mix and match 2D and 3D.People can reposition a window to their likingin their current space, just as one might expect.Volumes allow an app to display 3D content in defined bounds,sharing the space with other apps.Volumes are great for showcasing 3D content,for example, a chess board.People can reposition volumes in space,and they can be viewed from different angles.Volumes are SwiftUI scenes, allowing you to do layoutin familiar ways, and they use the power of RealityKitto display your 3D content.Sometimes you might want to have more controlof the level of immersion in your app…maybe to focus while watching a video or to play a game.You can do this by opening a dedicated Full Space,where your app's windows, volumes, and 3D objectsare the only ones appearing across the view.In a Full Space, you can also take advantage of ARKit's APIs.For example, in addition to system-provided gestures,you can get more detailed Skeletal Hand Trackingto really incorporate the structure of people's handsinto your experience.Your app can use a Full Space in different ways.You can use passthrough to ground contentin the real world and keep people connectedwith their surroundings.And when you play Spatial Audio and render 3D through RealityKityou will automatically take advantage of the factthat the device will continually updateunderstanding of the room to blend visuals and soundinto people's surroundings, making them feelthat these virtual objects really belong in their room.You can also choose to render to a fully-immersive spaceto fill up the entire field of view.This allows your app flexibilityto deliver on creative intent of your appby customizing the lighting of virtual objects,as well as the ability to choose audio characteristics.These are the foundational elements of spatial computing:windows, volumes, and spaces.They give you a flexible toolset to build appsthat can span the continuum of immersion.Christopher will talk more about this later.Now that we've introduced the foundational elementsof spatial computing, let's explore the wayswe can interact with windows, volumes, and spaces.On this platform, we can interact with appsby simply using our eyes and hands.People can, for example,interact with a button by looking at itand tapping their fingers together to select.People can also reach outand physically touch the same button in 3D space.For both these kinds of interactions,there is a variety of gestures that are possible, like taps,long presses, drags, rotations, zooms, and a lot more.The system detects these automaticallyand generates touch events for your app to respond to.Gestures are integrated well with SwiftUI.The same gesture API works seamlesslywith RealityKit entities.This allows people to easily interactdirectly with your 3D scene elements.For example, this could be useful to place a flagdirectly onto this 3D model,or imagine controlling a virtual zipperor perhaps you want to interact and pick upvirtual chess pieces.Now if you want to do a game of bowlingor transform people's hands into a virtual club,you can do this through ARKit's Skeletal Hand Tracking.Here we can see an example how you can stack cubeson a table using tapsand then smashing them onto the table with your hands.This is a powerful way that you can bringapp-specific hands input into the experience.And finally, the system automatically brings inputfrom wireless keyboards, trackpads,and accessibility hardware right into your app,and the Game Controller framework lets you add supportfor wireless game controllers as well.Collaborating and exploring things togetheris a fundamental part of spatial computing.We do this through SharePlayand the Group Activities framework.On this platform, as on macOS, people can share any window,like this Quick Look experience.When people share a Quick Look 3D model,we sync the orientation, scale and animationsbetween participants, making it easy to collaboratewhile being in different locations.When people are collaborating on something that is shownin their space and that they physically point at,it is important that everyone in the SharePlay sessionhave the same experience.This enables natural references such as gesturing to an objectand reinforces the feeling of being physically together.We've added the concept of shared context to the system.The system manages this shared context for youhelping make sure that participantsin a SharePlay session can all experience contentin the same way.You can use Spatial Persona Templatesto further customize how people experience your content.To learn more, watch our sessions about designingand building spatial SharePlay experiences for this platform.Given that the device has a lot of intimate knowledgeof the surroundings and people, we put a lot of architecturein place to protect people's privacy.Let's dive into that.Privacy is a core principle for guiding the designof this platform, while making it easy for you as a developerto leverage APIs to take advantageof the many capabilities of the device.Instead of allowing apps to access datafrom the sensors directly, the system does that for youand provides apps with events and visual cues.For example, the system knows the eye position and gesturesof somebody's hands in 3D spaceand delivers that as touch events.Also, the system will render a hover effect on a viewwhen it is the focus of attentionbut does not communicate to the app where the person is looking.For many situations, the system-provided behaviorsare sufficient for your app to respond to interactions.In cases where you actually do need accessto more sensitive information, the system will ask the peoplefor their permission first.An example would be asking user permissionto access scene understanding to detect walls and furnitureor access to Skeletal Hand Trackingto bring custom interactions into your app.Now that we've seen some of the capabilitiesavailable for apps, let's move onto exploringhow we are developing those apps.Everything starts with Xcode,Apple's integrated development environment.Xcode offers a complete set of tools for developing apps,including project management support,visual editors for your UI, debugging tools, a Simulator,and much more.And most importantly, Xcode also comes withthe platform SDK, which provides the complete set of frameworksand APIs you'll use for developing your app.If your source file contains a SwiftUI preview provider,the preview canvas will automatically open up in Xcode.The preview canvas has been extended to support 3D,allowing you to visualize RealityKit code for your scene,including animations and custom code.This enables shorter iteration times,finding the right look and feel for your appas you edit live code and see the resultsof changes and tweaks directly.Let's experiment a little bit herewith how the satellite looks orbiting the Earthby changing the orbital speed and the size of the satellite.Notice the preview reflects the code changes,making it easy to see the resultsof quick experimentation in the code.Xcode Previews also has an object modethat allows for quick previews of 3D layouts -for example, seeing if your layout fitsinside the bounds of the view.This is great for building tightly-integrated sceneswith both traditional UI and new 3D visuals.Xcode Preview gives you a fantastic wayto get the layout right before you run your app.The Simulator is a great way of testing interactivitywith your app.You can move and look around in the scene using a keyboard,mouse or compatible game controller.And it's easy to interact with your appby using simulated system gestures.The Simulator comes with three different simulated scenes,each with a day and night lighting.This makes it easy to see your app under different conditions.The Simulator is a great way to run and debug most appsand to quickly iterate during developmentwith a very predictable environment.We've also extended Xcode to support a numberof runtime visualizations while you are debuggingto help you quickly understand and track down bugsby simply looking at the scene.Here we have plane estimation visible,including semantic meaning of those planesand the collision shapes in the scene.It's easy to toggle visualizationsyou would like to focus on from the debugger in Xcode.These visualizations works great both in the Simulatorand in the device.When it becomes time to polish your application's performanceand responsiveness, we've got familiar tools like Instruments.Instruments is a powerful performance analysis toolincluded with Xcode.You can use Instruments to provide youwith actionable insights of your running app.And for spatial computing, Instruments 15includes a new template, RealityKit Trace,providing even more and deeper insightsinto new behaviors on the platform.The RealityKit Trace template has new instrumentsallowing developers to understand GPU, CPU,and system power impact of their app,and identify performance hotspots.You can easily observe and understand frame bottlenecksand trace them back to vital metrics like total trianglessubmitted or number of RealityKit entities simulated.This lets you quickly find and addresspotential performance issues.For more details, check out the session "Meet RealityKit Trace."We've also introduced a new developer toolcalled Reality Composer Pro.It allows you to preview and prepare3D content for your apps.Reality Composer Pro helps you get an overviewof all your assets and how they fit together in your scene.A new feature that we added to RealityKit is particles,and you can use a workflow in Reality Composer Proto author and preview them.Adding particles into your scene provides movement, lifeand endless possibilities.Clouds, rain and sparks are just a few effectsthat you can build in a short amount of time.Adding audio into your scenesand associating them with objects is a breeze.You can also spatially preview audio,which takes into account the shape and contextof your entire scene.Most virtual objects will use RealityKit's physically-basedmaterial to represent a variety of real world materials.RealityKit uses sensor data to feedreal-world lighting information into these materials,grounding them in people's surroundings.RealityKit also has a couple of additional standard materialsavailable for your app to use in common scenarios.For those times when you have a very specific need,perhaps to convey a creative intent,you can author custom materials in Reality Composer Prowith the open standard MaterialX.You can do this through an easy-to-use node graph,without touching any code,and quickly preview them directly in the viewport.You can learn more about this in the session"Explore materials in Reality Composer Pro."When you're feeling good about your 3D content,you can send your scenes to your deviceand test your content directly.This is great for iteration times since you don't evenhave to build an app.To learn more, watch the session"Meet Reality Composer Pro."Another option that is available is Unity.Unity is bringing the ability for you to write appsfor spatial computing with familiar workflowsand without any plugins required.You can bring your existing content overto power new immersive experiences.To learn more, watch these sessionscovering how to write immersive apps with Unity.Now that we understand some of the fundamental conceptsand tools available to us,let's see how we can start building apps.There are two ways to get started -either you design a brand-new appfrom the ground up to be spatialor perhaps you have an existing appthat you want to bring into this new spatial platform.Let's explore how we build a new app.Designing an application from the ground up to be spatialhelps you to quickly embrace the new unique capabilitiesof spatial computing.To get started, you can use new app templatefor this platform.The app template has two new important options.First, you can choose your Initial Scene Typeto be either a ‘Window’ or a ‘Volume’.This generates the initial starting code for you,and it's easy to add additional scenes later.The second option lets you add an entry pointfor an immersive space to your app.By default, your app will launch into the Shared Space.If you select Immersive Scene Type to ‘Space’,a second scene will be added to your app,along with an example button showing how to launchinto this Full Space.And when you finish the assistant,you are presented with an initial working app in SwiftUIthat shows familiar buttons mixed in with a 3D objectrendered with RealityKit.To learn more, watch the session"Develop your first immersive app."We are also publishing code samples,each one of them illustrating different topicsto quickly get you up to speed.Destination Video shows how to builda shared, immersive playback experiencethat incorporates 3D video and Spatial Audio.Happy Beam is an example of how you can create a gamethat leverages an Immersive Space,including custom hand gestures,to create a fun game with friends.And Hello World shows how to transitionbetween different visual modes with a 3D globe.Christopher will talk more in detail about Hello World later.Building and designing your app from the ground upon this platform offers opportunities to easily embracespatial computing concepts.However, some of you might have existing appsthat you want to bring to spatial computing.From the start, iPad and iPhone apps look and feel great.If your app supports iPad,that variant will be preferred over iPhonethough iPhone-only apps are fully supported.Let's take a look at the recipes app shown here in the Simulator.While this platform has its own darker style,iPad and iPhone apps retain a light mode style.Windows can scale to allow for ease of use,and rotations for apps are handled,allowing you to see different layouts.To find out more, watch the session"Run your iPad and iPhone apps in the Shared Space"to learn about the system's built-in behaviors,functional differences and how to test with the Simulator.However, running an existing iPad or iPhone appis just the beginning.It's easy to add a destination in your Xcode projectfor this platform with just a click.And after that, we can simply select our target device,recompile and run.

Once you recompile, you get native spacing,sizing and relayout.Your windows and materials will all automatically moveto the platform's look and feel, ensuring legibilityin any light condition,and your app can take advantage of built-in capabilitieslike highlighting for your custom controls.Now here’s Christopher to show us how we can evolve our appsfurther using the concepts we've covered so far.Thanks Jim.I'm going to walk you through how to build an applicationthat incorporates the elements you've learned previously.Let's start with Hello World to explore some ofthe great functionalities you can integrate into your app.Here’s our sample in action.Upon running the app in the Simulator,Hello World launches with a window into the Shared Space,right in front of us.This is a familiar-looking window made in SwiftUI,and it contains different elementssuch as text, images and buttons.Using tap gestures allows the navigation within the app.Observe how our new view has embedded 3D content.SwiftUI and 3D content now work together seamlessly.Going back to our main window and selecting Planet Earthbrings us to a new view.A new element appears. This is a volume.It contains a 3D model of the Earth,alongside a few UI elements.By moving the window bar, the volume's positioncan be adjusted anywhere in the surroundings.

Going back to our main window againand selecting View Outer Space brings up an invitation for usto enter the solar system.

From here, we can enter space, which is shown herewith an immersion style of ‘full’.Our example renders Planet Earth and dims passthrough,allowing us to focus on the contentwith no distractions of the surroundings.Now that we have seen how this looks in action,let's break down some of the functionalities of Hello Worldand show you how to use these concepts in your own apps.As you've learned from Jim,there are multiple elements: windows, volumes and spaces.You can look at this as a spectrum that your app can useto flex up and down, depending on what is bestfor people using your app in a specific moment.You can choose to present one or several windowsin the Shared Space, allowing people to be more present.They can see passthrough and have a choiceto have other apps side by side.Or… you can choose to increase the immersion levelby having your app take over the space entirely.Finding the most suitable elementsfor your app's experience in a given momentand flexing between them is an important considerationwhen you design your app for spatial computing.Next, Let's look further into how to use windowsas part of your experience.Windows serve as a starting point for your app.They are built with SwiftUI using scenes,and they contain traditional views and controls.Windows on this platform support mixing 2D and 3D content.This means that your 3D content can be presentedalongside 2D UI in a window.Windows can be resized and repositioned in space.People can arrange them as per their liking.Let's go back to our example.In Hello World, the content view holds our SwiftUI images,text and buttons, along with a call to actionto get more immersive content.Creating a window is as easyas adding a WindowGroup to a scene.Inside the WindowGroup, we will display our Content View.Our Content View can add 3D content,bringing a new dimension of depth to your app.To do that, you can use the new Model3D view.Model3D is similar to an image, making it easy to loadand display beautiful 3D content in your appthat is rendered by RealityKit.To add Model3D to your view,we initialize Model3D by passing the name of the satellite model.With this, Model3D will find and load the model,and place it into your view hierarchy.Now this window has the satellite embeddedinto the view and can be seen coming out of the z-axis,adding a new dimension of depth to your app.Now that we have added a satellite,we can add interactions.Interactions are fundamentally built into the systemand provided by SwiftUI.SwiftUI provides the gesture recognizersyou are already familiar with on Apple platforms,such as Tap, onHover, and RotateGesture.The platform provides new gesture recognizersthat are made for 3D interactions,like rotations in 3D space, taps on 3D objects and more.Let's look at the code that enablesinteractions with the satellite.We are going to enable a spatial tap gestureso we can grab and move the satellite around.Starting from Model3D, we can now add a gesture.Inside we add a DragGesturetargeted to the satellite entity.We can then use the values passed infrom the update closure to move the satellite.Lets see what that looks like.Back in our satellite view, where our satellite is rendered,note the DragGesture allows me to tap and drag the model,moving with my interactions.As we've just seen, it's easyto mix 2D and 3D content together with Model3D.These are just a few things you can do with a window.Now let's look at another type of element, volume.Lets see what a volume has to offer.Volume is an extension of a window,giving you similar functionality.A volume is a new style of windowthat is ideal for 3D content.They can host multiple SwiftUI viewscontaining your 2D or 3D content.Although volumes can be used in a Full Space,they are really built for the Shared Space,therefore content must remain within the bounds of the volume.Let's look at how to add a volume to your scene.You will start by creating a new WindowGroupand setting its windowStyle to volumetric.Then, you need to give it a defaultSizewith the properties width, height and depth.The units of a volume can be specified in points or meters.Let's look at this running in the Simulator.When the application is presented,the volume is placed in front of the person.This volume has the dimensions we specified,along with the platform controls:the application title bar, which displays our app name,making it easy to identify which app this volume belongs to;the window bar, enabling the volume to be positioned;and the close button, suspending the appwhen tapped, closing the volume.Currently, our volume renders the 3D model of the Earth,but you might want to start adding more contentand different behaviors.In order to do this, you can adopt RealityViewas part of your app.RealityView is a new view that can be added to your scene,allowing for any number of entities to be manageddirectly within SwiftUI.SwiftUI and RealityView let you easily integrate your app byconnecting to SwiftUI's managed state and entity properties.This makes it easy to drive the behavior of 3D modelswith a source of truth from your app's data model.Conversion between coordinate spaces is easywith conversion functions provided by RealityView,and RealityView offers a way to position SwiftUI elementsinside your 3D scene through attachments.Let's take a moment to look at how we can use attachmentsinside RealityView.The RealityView initializer that we're going to usetakes three parameters: a make closure,an update closure, and an attachments ViewBuilder.The make closure allows you to create entitiesand attach them to the root entity.The update closure, which is called wheneverthe state of the view changes.And lastly, the attachments closureis where we add our SwiftUI views with a tag propertythat allows RealityView to translate our viewsinto entities.Now, let's work through an exampleof how to use attachments with RealityView.Adding an attachment is as easy as putting your SwiftUI viewinside the attachment closure of RealityView.Let's use this icon of a delicious pastryto represent a location on our 3D globe.For each attachment, you must add a tagthat gives the attachment a name.I'll name this one ‘pin’.To display the attachment, I'll add it to the contentof my RealityView.I'll do that in the update closure by adding itto the root entity of the scene.Here, we can see the attachment we made previously,rendering on the globe above my favorite bakery location.As we've just seen, using RealityKitunleashes powerful features such as Model3D, RealityView,attachments and so many more.These can be easily integrated into your app.This is only scratching the surfaceof what RealityKit can do.If you want to know more, I encourage you to go and watch"Build spatial experiences with RealityKit"and "Enhance your spatial computing app with RealityKit."Let's recap what we went through so far.A volume is a container that is ideal for 2D and 3D content.Volumes are built for the Shared Space,can coexist with windows,and are bounded to specified dimensions.Next, let's dive into our last type of element, spaces.Once your app is opening a dedicated Full Space,the system hides all other apps,leaving only your app visible.Now you can place your app's window, volumesand content anywhere around you.Thanks to ARKit and RealityKit, your virtual contentcan even interact with your surroundings.You could throw a virtual ball into the room and watchas it bounces off of the wall and then rolls on the floor.And with the addition of hand tracking,you can build custom gestures and interactionsor place the content relative to people's hands.Many of these capabilities are coming from ARKit.To go into more depth and learn how you can leverage themin your app, be sure to check out"Meet ARKit for spatial computing" session.With spaces, your app can also offerdifferent levels of immersion, depending on which styleis chosen at creation time.Jim talked a bit about the spectrum of immersionavailable in a Full Space.Let's dive in and learn more abouthow you can add more immersion into your app.Immersion style is a parameterthat can be passed in your Full Space.There are two basic stylescalled .mixed and .full.Mixed style layers your app's content on top of passthrough.Full style hides passthrough and displays your content only.You can also combine the two by choosing progressive.This style allows some passthrough initially,but the person can change the level of immersionall the way up to full by turning the Digital Crownlocated on the top of the device.Let's go back to our example to explore immersion style.I'll start with the mixed style and see how that looks.And because Full Space is a SwiftUI scene,I can use RealityView to display the Earth.Here's the Earth viewed from high orbit…and here's how I displayed the scene in my app.Notice I didn't actually specify the immersion style.That's because when you create an immersive space,SwiftUI assumes mixed style by default.Let's also take your app completely immersiveby adding a different immersion style.This time, I'll use immersion style ‘full’.Adding an immersive styleto the end of our ImmersiveSpace is easy.We store the immersive style in our state variableand then set the type to full.Because we want to give people the choiceof when they enter an immersive experience,it's a good idea to add a button to allow the person to decideif they want to enter this immersive style.Now let's see the new immersive style in action.Back in our app, I've taken Hello Worldfrom a single window to fully immersed,allowing us to view Planet Earth from any angle.And that's just the beginning of what you can dowith your spatial app.Let's see where you can go from here.In this session, we've covered the fundamentals:how to get started,and then took you through the basics of building an app.We have some great sessions that should be your next stop -about the principles of spatial design,or to learn about building appswith SwiftUI and with RealityKit,or to begin creating your 3D content.With spatial computing, your app creationcan venture into new, exciting avenuesguided by your ingenuity.Thanks for watching!♪

## Code Samples

