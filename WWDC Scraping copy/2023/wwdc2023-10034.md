# Wwdc2023 10034

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Create accessible spatial experiencesLearn how you can make spatial computing apps that work well for everyone. Like all Apple platforms, visionOS is designed for accessibility: We'll share how we've reimagined assistive technologies like VoiceOver and Pointer Control and designed features like Dwell Control to help people interact in the way that works best for them. Learn best practices for vision, motor, cognitive, and hearing accessibility and help everyone enjoy immersive experiences for visionOS.Chapters0:42 -Overview2:04 -Vision16:10 -Motor20:15 -Cognitive22:15 -HearingResourcesAccessibilityDioramaImproving accessibility support in your visionOS appMedia AccessibilityUIAccessibilityHD VideoSD VideoRelated VideosWWDC23Meet SwiftUI for spatial computingWWDC22Create accessible Single App Mode experiencesWWDC21SwiftUI Accessibility: Beyond the basicsTailor the VoiceOver experience in your data-rich appsWWDC20Make your app visually accessibleVoiceOver efficiency with custom rotorsWWDC19Accessibility in SwiftUIMaking Apps More Accessible With Custom Actions

Learn how you can make spatial computing apps that work well for everyone. Like all Apple platforms, visionOS is designed for accessibility: We'll share how we've reimagined assistive technologies like VoiceOver and Pointer Control and designed features like Dwell Control to help people interact in the way that works best for them. Learn best practices for vision, motor, cognitive, and hearing accessibility and help everyone enjoy immersive experiences for visionOS.

0:42 -Overview

2:04 -Vision

16:10 -Motor

20:15 -Cognitive

22:15 -Hearing

Accessibility

Diorama

Improving accessibility support in your visionOS app

Media Accessibility

UIAccessibility

HD VideoSD Video

HD Video

SD Video

Meet SwiftUI for spatial computing

Create accessible Single App Mode experiences

SwiftUI Accessibility: Beyond the basics

Tailor the VoiceOver experience in your data-rich apps

Make your app visually accessible

VoiceOver efficiency with custom rotors

Accessibility in SwiftUI

Making Apps More Accessible With Custom Actions

Search this video…♪ Mellow instrumental hip-hop ♪♪Dan Golden: Hi, I'm Dan from the Accessibility team.I am thrilled to talk about accessibilityin spatial computing, alongside my colleague Drew.In this talk, I'll give you an overviewof some of the accessibility featuresavailable on this platform.Next, I'll dive into some of the specificsof what you can do in your appsto support people who are blind or low vision.Then, I'll hand it off to Drew to discuss motor, cognitive,and hearing accessibility in spatial computing.Let's get started!We've designed this immersive platform for everyone.While spatial computing experiences are often builtwith stunning visual features and a variety of hand inputs,that doesn't mean that vision or physical movementare required to engage with them.In fact, these experiences have the potential to beincredibly impactful to people who are blind or low vision,have limited mobility, or limb differences.For example, someone who is blindcould interact with the real worldwithout having to see what's on the displays.Therefore, it's important to keep people of all abilitiesin mind when you're building out your app,so that everyone can enjoy and benefit from them.At Apple, we recognize that access to technologyis a fundamental human right, and this platformcontains the largest list of accessibility featureswe've ever included in the first generation of a product.You'll recognize many features you already know and love,like Dynamic Type support, Increase Contrast,and Spoken Content features.And we've reimagined our flagship assistive technologiesspecifically for spatial computing.We are so excited about these features,and as a developer, you can help by making sure the experiencesthat you're building include everyone.Let's start by talking about the ways you can support peoplewho are blind or low vision in your apps.There are a few things to considerwhen discussing vision accessibility:VoiceOver support, visual design, and motion.Let's start by talking about VoiceOver support.VoiceOver is the built-in screen readeravailable on all Apple platforms,and I'm excited to say we've brought it to this one as well.Drew and I have been working on a really fun appcalled Happy Beam that utilizes ARKit and RealityKit.In the app, you make heart gestures with your handsto turn grumpy clouds happy.Let's take a look at some of the wayswe can improve the VoiceOver experience in this app.We've gone ahead and added VoiceOverto the Accessibility Shortcutin Settings > Accessibility > Accessibility Shortcut,so that whenever we triple-press the Digital Crown,VoiceOver will toggle on or off.This is a great tool when we're testingthe accessibility of our app.Let's open the app and toggle VoiceOver onwith a triple-press of the Digital Crown.♪ Ethereal instrumental music ♪VoiceOver: Happy Beam.Choose how you will cheer up grumpy clouds.Dan: On this platform, VoiceOver uses different finger pincheson different hands to perform different actions.By default, you can move focus to the next itemby pinching your right index finger.♪VoiceOver: Make a heart with two hands, button.Use a pinch gesture or a compatible device, button.Dan: To move focus to the previous item,pinch your right middle finger.VoiceOver: Make a heart with two hands, button.Dan: To activate an item, pinch your right ring fingeror your left index finger.Now that we're familiar with some ofVoiceOver's basic controls,let's explore the rest of the app.VoiceOver: Three, two, one.Happy beam. Back button. Score zero.Stop music, button. Twenty-nine seconds remaining.Pause, button.Dan: Things are looking pretty good here.This app is built using SwiftUI,so a lot of the standard controls we're usingare already accessible,and we've taken care to adopt SwiftUI'saccessibility modifiers on the rest of the viewsto ensure appropriate accessibility informationis being provided to VoiceOver.To learn more, check out our talks on SwiftUI Accessibility.Let's take a look at the rest of the appand see how we can interact with the clouds.VoiceOver: Close, button. Pop-up. App placement bar.App placement --  App placement --App placement bar.Dan: That sound that you hear indicates that VoiceOvercan't find any other items to interact with,so the clouds are inaccessible to VoiceOver.The clouds are generated using RealityKit,so we can fix this using the new accessibility componentavailable in RealityKit.The accessibility component allows you to specifyaccessibility properties on RealityKit entities.You can configure accessibility labels, values, and traits,as well as custom rotors, custom content,and custom actions.You can also configure system accessibility actions,such as the activate and adjustable actions.To make the clouds accessible,let's start by creating a new accessibility component,and then set isAccessibilityElement to trueto indicate to assistive technologiesthat the cloud should be navigable.Next, we'll give the cloud the button trait,so that technologies like Switch Controland Voice Control understand that this cloudis a user-interactive item.We'll also give it the playsSound trait,since the clouds play a sound when they turn happy.Next, we can give the cloud a name via the label property,and a value describing its grumpy or happy state.Any time the state in your app is updated,make sure you update the relevant propertyon the accessibility component.Here, in the didSet handler of our isHappy variable,we're using one of the convenience propertiesto update the accessibilityValue on the cloud.Any time you use a convenience property,the cooresponding property on the accessibility componentis updated accordingly.Labels and values are stored as LocalizedStringResources,which can be created using string literals,but automatically resolve to a localized valuethat you've provided at runtime.Lastly, we'll set the component on the entities component list.Let's take a look at our app now that the cloudshould be accessible.♪VoiceOver: App pl-- Cloud, grumpy. Button.Dan: Great! We can navigate to the cloudsand their state is communicated to us.VoiceOver uses Spatial Audio to provide cuesas to where objects are located.So let's try to make a heart gesture at one of our cloudsto turn it happy.♪What's going on?We just showed that cloud so much love,but it still didn't turn happy.That's because when VoiceOver is enabled,your app won't receive hand input by default.This is to ensure that your appwon't inadvertently perform an actionwhile someone is performing VoiceOver gestures,enabling them to explore safely.On this platform, VoiceOver includesa new Direct Gesture Mode, and when it's enabled,VoiceOver won't process its standard gesturesand instead allow your app to directly process hand input.People can choose to run your appboth in Direct Gesture Modeand in VoiceOver's default interaction mode,and there are accessibility considerationsto keep in mind in each case.Let's start by talking aboutVoiceOver's Default Interaction Mode.Let's add an activate actionso that we can turn the clouds happy using VoiceOver.To do this, we'll add the activate actionto the systemActions property on the accessibilityComponent.Then, we'll subscribe to the activate eventon the content inside our RealityView.Any time we receive an activate event in the callback,we'll update the game model so that the stateof the relevant cloud is updated accordingly.Let's take a look at our app again with actions added.VoiceOver: App placement -- Close --Cloud, grumpy.Cloud, grumpy.Dan: Awesome!Now we can turn the clouds happy using VoiceOver.The AccessibilityComponent also offers additional APIssuch as custom actions, custom rotors,and custom content.These are great tools for improvingthe accessibility experience in your app.To learn more about, check out our talkson the corresponding topics.Next, let's talk about Direct Gesture Mode.This is a new way to interact with appswhile using VoiceOver on this platform.VoiceOver's activate action isn't availablein Direct Gesture Mode, so we'll need to provide feedbackto the hand-input interactions that we'll be using instead.Let's start by posting an announcementwhen the clouds appear,describing them and their placement in the world.To do this, we'll create a new AccessibilityNotificationwith an Announcement type, pass in the stringwe'd like to have spoken, and then call the post functionon the announcement.In spatial experiences, it's critical to provide informationabout what items are available and where they are located,so this will be a crucial announcement in our app.Let's also post an announcementany time a heart gesture is recognizedor when a cloud changes from grumpy to happy.Always announce any meaningful event to VoiceOver,so that it's clear what is happening,and what interactions are being performed.For example, in a fully immersive app,any time you enter a new room or environment,make sure to announce that change in context to VoiceOver,and describe any new items that are available in the world.Also consider utilizing sounds when actions are performed.The sounds that play in Happy Beamwhen a cloud turns happy are a great wayto keep the app feeling fun and spatial,even if we can't see the visual transformation of the cloud.Let's take a look at our app one last timewith some announcements added.When the game starts, we'll enable Direct Gesture Modewith a left index finger triple-pinch and hold.Then, we can make heart gestures to turn the clouds happyand get feedback about all of the interactions.VoiceOver: Three, two, one.Happy Beam.Three clouds above and in front of you to the right.Direct Gestures enabled.Press the crown to ac-- Casting beam.♪Grumpy cloud hit.Hiding beam.Dan: Awesome!We received some great feedback thereabout all of the interactions as they were being performed.Our app is shaping up to have some great VoiceOver support.But there's still a lot you can doto support people with low visionwho aren't using your app with VoiceOver,especially if you're building any custom componentsor controls.Just like on all other Apple platforms,make sure your app responds to changes in Dynamic Type,especially at the largest sizes availablein accessibility settings.Audit your app for any UI that might benefitfrom being laid out vertically instead of horizontallyat these larger sizes.Also ensure you're usingat least a four-to-one contrast ratiobetween foreground and background colors.To learn more, check out "Make your apps visually accessible."In spatial experiences, anchors can be usedto place content relative to different anchor points,such as a hand or a specific position in the world.You can also configure content to be anchoredto the virtual camera, so that it appearson the same spot on the displays.While you may be familiar with camera anchorsin Reality Kit on other Apple platforms,on this platform, the content follows your headas you look around, which has the potentialto impact people with low vision differently.Head anchors should be avoided and used sparingly,so that people with low vision can get closer to contentto read it or view its details.Additionally, people using the accessibility Zoom featurewon't be able to easily position head-anchored contentinside of the Zoom lens,since the Zoom lens is also head anchored.Instead, consider using a world anchoror lazily move your content after a delay.In the rare cases where it's absolutely necessaryto use head anchors, keep the content decorative.Critical information should not only be accessiblethrough head-anchored content.Always provide alternatives for head anchors,even when they might be the best mainstream experiencein your app.The new accessibilityPrefers HeadAnchorAlternativeEnvironment variable in SwiftUI,and the AX PrefersHeadAnchor Alternative APIin the Accessibility framework,let you know when you should use alternate anchors.Observe these APIs anywhere your app employs head anchors.We've taken care to adopt this API in the system ourselves.By default, Control Center is head anchoredin its collapsed state.You can see here that, as you look around,Control Center follows you.While this design makes it easy to access Control Centerfrom anywhere,we knew it might be challenging for some people.I mentioned earlier that Zoom is also head anchored.This is a feature that magnifies contentfor people with low vision.When Zoom is enabled or someone has otherwise indicatedthat they prefer alternatives to head anchors,Control Center moves freely about the Y axis.Here, you can see that, as you tilt your head up,the Zoom lens follows your head, but Control Center does not.Enabling you to position Control Centerinside of the Zoom lens and interact with it.It's also important to be mindfulof the usage of motion in your app.Motion can be dizzying for some people,and wearing a headset can be an especially jarring experience,even when subtle motion effects are used.Avoid the use of motionthat move a person rapidly through your app,or involve a bouncing or wave-like movement.Zooming animations, animations that involve movementalong more than one axis, spinning or rotating effects,and persistent background effectsshould all also be avoided.Always provide alternatives for these types of animationswhen Reduce Motion is enabled.You can check whether Reduce Motion is active with theaccessibilityReduceMotion Environment variable in SwiftUI.In UIKit, you can queryUIAccessibility. isReduceMotionEnabled,and observe changes to the preferencewith the corresponding notification.If you're having trouble finding a suitable replacementfor the motion in your app, consider utilizing a crossfade.Here's an example of how we've adopted Reduce Motionin the system.Check out the water here in the Mount Hood Environment,as the water persistently ripples in the background.When we toggle Reduce Motion on, the water is changedto statically show a ripple effect,which achieves a similar visual effectwithout requiring the usage of motion.That's an overview of some of the ways you can improvethe vision accessibility in your apps,but there's still a lot to consider when discussingmotor, cognitive, and hearing accessibility.And for that, here's Drew to tell you more.Drew Haas: Thanks, Dan. Really great work!My name is Drew Haas and I'm an engineeron the Accessibility team.Now that we've learned about meaningful waysto improve the visual accessibilityof your spatial experiences, there is so much I want to shareabout how to create your apps to be inclusiveto people with disabilities which affectphysical and motor function, cognition, and hearing.Let's start first with motor!The default input system is drivenby a combination of eyes and hands.For example, looking at a button with your eyes,and pinching with your hand sends a selection eventto activate the button.However, not everyone can perform these physical actions.Our accessibility features provide alternate input methodsfor people with disabilities which impact their use of eyes,hands, or both.The Dwell Control accessibility feature allows peopleto select and interact with the UI without using their hands.Dwell Control supports gestures like tap, scroll,long press, and drag.You should design your app to have full functionalitywith this gesture set so people using Dwell Controlaren't excluded.It's a breeze to switch gesture modesby using the Dwell Control menu, allowing people to operatetheir device using accommodationswithout sacrificing efficiency.This is by design: giving people a frictionless experienceeven if they're using nondefault inputs.Let's see how our Happy Beam appis designed to support a variety of inputs,which makes it fully playable using Dwell Control.When Happy Beam is launched, the player will first choosehow they'll cheer up those grumpy clouds.You saw the first option earlier in this session:using two hands in the shape of a heartand aiming it at the clouds.The second option supports Bluetooth accessorieslike keyboards and game controllers.When playing with these inputs, the happy beam is firedusing this heart turret.The turret also responds to tap-and-drag gestures,which means you can play with one hand.And, people playing with Dwell Controlhave full functionality over the turret.So plan and design for your app to support different inputslike Happy Beam does.This is the best way to ensureyou don't accidentally exclude people.There is another accessibility featurethat works really well with Dwell.Enter Pointer Control,one of my favorite accessibility features.This feature transforms the input experience,allowing people to use different input sourcesto control the system focus instead of using their eyes.Eyes is the default, but here people can changethe system focus to be driven by head position,wrist position, or index finger.Since Pointer Control can change the input signalto follow head position,remember to use camera-anchored content sparingly.This is another reason you should preferto use world anchors or provide alternativesto camera-anchored content.Both Dwell Control and Pointer Control --either on their own or utilizing their feature sets combined --provide tons of flexibility with how peopleinteract with their device.These features accommodate the physical requirementsto use the system.Allow multiple avenues for physical interaction,because you never know what kind of disabilitiessomeone using your app may have.Spatial experiences enable new,dimensional ways to interact with content.Switch Control has new menu optionsfor adjusting the camera's position in world space.Here we're using a keyboard with Switch Controlto activate the new camera position modifiers.This moves your spatial position downwardwithout you physically moving your body.Not everyone will be able to move comfortably or freelyin their environment.While these camera position options are availablefor Switch Control, if you have experiencesthat require people to position themselvesin certain ways, provide options to bypass them.Next, I want to talk about Cognitive accessibility,and how you can support people with disabilities that affectthe way they learn, remember, and process information.Guided Access is a cognitive accessibility featurethat promotes focus by restricting the systemto a single app.It aims to minimize distractions by backgrounding other apps,removing ornamental UI which may be distracting,and by suppressing hardware button eventsthat could take someone out of their experience.Being able to adjust the system in this way can make it easierfor someone to stay focused on their current task,without distractions or easy ways to get sidetracked.To learn more about how to use Guided Accessand implement the custom restrictions APIs,check out my talk from last year,"Create accessible Single App Mode experiences."By following just a few best practicesfor cognitive accessibility, you make your app easierto use for everyone,but especially people with disabilities.Some people need a little more helpbreaking down the complexity of your app.Interactions which require complex hand gesturescan be hard for people to pick up on and retain.You can help create a consistent and familiar visual experienceby using Apple's UI frameworks like SwiftUI.This reduces the amount of time someone may needto feel comfortable using your app,because it's likely they've used other appsbuilt using the same UI framework.And finally, allow people to take their timeimmersing themselvesand experiencing all that you have to offer.There is no need to rush people through an experience.Immersive content can promote focus and attention,which is a fantastic way to create a comfortable environmentfor someone with sensory processing disorders.Remember that not everyone processes informationat the same speed, so some people may preferor need a little extra time to work through an experience.Finally, I want to share some of the best waysto provide access and accommodationto people that are deaf or hard of hearing.It's common to use audio and speechas a way to immerse people in a spatial experience.For people with hearing loss or auditory processing disorders,one of the most impactful things you can dois provide quality captions so they can access your content.A comfortable reading experience is easy to createby using pop-on captions --which render the phrase all at once and are easy to read --instead of using roll-up captions,which appear word-by-word and can cause reading fatigueand nausea when reading for a long duration.Did you know that people can customizethe visual appearance of captions on their device?Captions can be widely customized,modifying things like text size, font, and color,as well as stroke outlines or backgrounds.These options allow people to customize their captionsso they're easy to see and read.AVKit and AVFoundation provide built-in supportfor supplying captions in your app.These frameworks automatically handlethe caption appearance and visual style.If you are not using AVFoundationbecause you're implementing your own captions system,there are two APIs that you should know about.First, the isClosedCaptioningEnabled API.Use this to check whether someone already hasClosed Captions turned on in Accessibility settings.If you have separate caption settings in your app,you should use this APIto inform the default state of captioning.This way, people who rely on captionsget access to them right away.The second can be found in the Media Accessibility framework,which has APIs to access each style attribute individually.You should check these styles and apply them to your captionsto keep a consistent reading experienceacross the entire system.No matter which way you choose to provide captions,you should have a high standard for their quality.Captions should represent all audio content,including music and sound effects.It's also helpful to indicate where in spacethe audio is coming from if directionalityis important to your experience, basically telling the userto "keep in mind, the nearest audio source may be behind you."An impressive accessibility experiencecomes from considering all people and their needs.Provide rich RealityKit experiencesby setting accessibility properties on your entities.This is the foundation for your app's accessibilityfor technologies like VoiceOver, Voice Control,and Switch Control.Be flexible and provide options for physical interactionlike we saw in Happy Beam to include all playersand their play styles.Strive to remove ambiguity and provide clarityand focus for people with cognitive disabilities,and spend time and care on captioned contentfor audio experiences so that people that are deafor hard of hearing can enjoy your creation.If you're not sure where to start,turn on some of these accessibility featuresand open up your app!Trying out these features for yourselfis a great way to dive right in.This platform is designed for everyone,and with all of the considerationsDan and I have shared today, you are equippedto create accessible and inclusive spatial experiences.Thank you!♪

♪ Mellow instrumental hip-hop ♪♪Dan Golden: Hi, I'm Dan from the Accessibility team.I am thrilled to talk about accessibilityin spatial computing, alongside my colleague Drew.In this talk, I'll give you an overviewof some of the accessibility featuresavailable on this platform.Next, I'll dive into some of the specificsof what you can do in your appsto support people who are blind or low vision.Then, I'll hand it off to Drew to discuss motor, cognitive,and hearing accessibility in spatial computing.Let's get started!We've designed this immersive platform for everyone.While spatial computing experiences are often builtwith stunning visual features and a variety of hand inputs,that doesn't mean that vision or physical movementare required to engage with them.In fact, these experiences have the potential to beincredibly impactful to people who are blind or low vision,have limited mobility, or limb differences.For example, someone who is blindcould interact with the real worldwithout having to see what's on the displays.Therefore, it's important to keep people of all abilitiesin mind when you're building out your app,so that everyone can enjoy and benefit from them.At Apple, we recognize that access to technologyis a fundamental human right, and this platformcontains the largest list of accessibility featureswe've ever included in the first generation of a product.You'll recognize many features you already know and love,like Dynamic Type support, Increase Contrast,and Spoken Content features.And we've reimagined our flagship assistive technologiesspecifically for spatial computing.We are so excited about these features,and as a developer, you can help by making sure the experiencesthat you're building include everyone.Let's start by talking about the ways you can support peoplewho are blind or low vision in your apps.There are a few things to considerwhen discussing vision accessibility:VoiceOver support, visual design, and motion.Let's start by talking about VoiceOver support.VoiceOver is the built-in screen readeravailable on all Apple platforms,and I'm excited to say we've brought it to this one as well.Drew and I have been working on a really fun appcalled Happy Beam that utilizes ARKit and RealityKit.In the app, you make heart gestures with your handsto turn grumpy clouds happy.Let's take a look at some of the wayswe can improve the VoiceOver experience in this app.We've gone ahead and added VoiceOverto the Accessibility Shortcutin Settings > Accessibility > Accessibility Shortcut,so that whenever we triple-press the Digital Crown,VoiceOver will toggle on or off.This is a great tool when we're testingthe accessibility of our app.Let's open the app and toggle VoiceOver onwith a triple-press of the Digital Crown.♪ Ethereal instrumental music ♪VoiceOver: Happy Beam.Choose how you will cheer up grumpy clouds.Dan: On this platform, VoiceOver uses different finger pincheson different hands to perform different actions.By default, you can move focus to the next itemby pinching your right index finger.♪VoiceOver: Make a heart with two hands, button.Use a pinch gesture or a compatible device, button.Dan: To move focus to the previous item,pinch your right middle finger.VoiceOver: Make a heart with two hands, button.Dan: To activate an item, pinch your right ring fingeror your left index finger.Now that we're familiar with some ofVoiceOver's basic controls,let's explore the rest of the app.VoiceOver: Three, two, one.Happy beam. Back button. Score zero.Stop music, button. Twenty-nine seconds remaining.Pause, button.Dan: Things are looking pretty good here.This app is built using SwiftUI,so a lot of the standard controls we're usingare already accessible,and we've taken care to adopt SwiftUI'saccessibility modifiers on the rest of the viewsto ensure appropriate accessibility informationis being provided to VoiceOver.To learn more, check out our talks on SwiftUI Accessibility.Let's take a look at the rest of the appand see how we can interact with the clouds.VoiceOver: Close, button. Pop-up. App placement bar.App placement --  App placement --App placement bar.Dan: That sound that you hear indicates that VoiceOvercan't find any other items to interact with,so the clouds are inaccessible to VoiceOver.The clouds are generated using RealityKit,so we can fix this using the new accessibility componentavailable in RealityKit.The accessibility component allows you to specifyaccessibility properties on RealityKit entities.You can configure accessibility labels, values, and traits,as well as custom rotors, custom content,and custom actions.You can also configure system accessibility actions,such as the activate and adjustable actions.To make the clouds accessible,let's start by creating a new accessibility component,and then set isAccessibilityElement to trueto indicate to assistive technologiesthat the cloud should be navigable.Next, we'll give the cloud the button trait,so that technologies like Switch Controland Voice Control understand that this cloudis a user-interactive item.We'll also give it the playsSound trait,since the clouds play a sound when they turn happy.Next, we can give the cloud a name via the label property,and a value describing its grumpy or happy state.Any time the state in your app is updated,make sure you update the relevant propertyon the accessibility component.Here, in the didSet handler of our isHappy variable,we're using one of the convenience propertiesto update the accessibilityValue on the cloud.Any time you use a convenience property,the cooresponding property on the accessibility componentis updated accordingly.Labels and values are stored as LocalizedStringResources,which can be created using string literals,but automatically resolve to a localized valuethat you've provided at runtime.Lastly, we'll set the component on the entities component list.Let's take a look at our app now that the cloudshould be accessible.♪VoiceOver: App pl-- Cloud, grumpy. Button.Dan: Great! We can navigate to the cloudsand their state is communicated to us.VoiceOver uses Spatial Audio to provide cuesas to where objects are located.So let's try to make a heart gesture at one of our cloudsto turn it happy.♪What's going on?We just showed that cloud so much love,but it still didn't turn happy.That's because when VoiceOver is enabled,your app won't receive hand input by default.This is to ensure that your appwon't inadvertently perform an actionwhile someone is performing VoiceOver gestures,enabling them to explore safely.On this platform, VoiceOver includesa new Direct Gesture Mode, and when it's enabled,VoiceOver won't process its standard gesturesand instead allow your app to directly process hand input.People can choose to run your appboth in Direct Gesture Modeand in VoiceOver's default interaction mode,and there are accessibility considerationsto keep in mind in each case.Let's start by talking aboutVoiceOver's Default Interaction Mode.Let's add an activate actionso that we can turn the clouds happy using VoiceOver.To do this, we'll add the activate actionto the systemActions property on the accessibilityComponent.Then, we'll subscribe to the activate eventon the content inside our RealityView.Any time we receive an activate event in the callback,we'll update the game model so that the stateof the relevant cloud is updated accordingly.Let's take a look at our app again with actions added.VoiceOver: App placement -- Close --Cloud, grumpy.Cloud, grumpy.Dan: Awesome!Now we can turn the clouds happy using VoiceOver.The AccessibilityComponent also offers additional APIssuch as custom actions, custom rotors,and custom content.These are great tools for improvingthe accessibility experience in your app.To learn more about, check out our talkson the corresponding topics.Next, let's talk about Direct Gesture Mode.This is a new way to interact with appswhile using VoiceOver on this platform.VoiceOver's activate action isn't availablein Direct Gesture Mode, so we'll need to provide feedbackto the hand-input interactions that we'll be using instead.Let's start by posting an announcementwhen the clouds appear,describing them and their placement in the world.To do this, we'll create a new AccessibilityNotificationwith an Announcement type, pass in the stringwe'd like to have spoken, and then call the post functionon the announcement.In spatial experiences, it's critical to provide informationabout what items are available and where they are located,so this will be a crucial announcement in our app.Let's also post an announcementany time a heart gesture is recognizedor when a cloud changes from grumpy to happy.Always announce any meaningful event to VoiceOver,so that it's clear what is happening,and what interactions are being performed.For example, in a fully immersive app,any time you enter a new room or environment,make sure to announce that change in context to VoiceOver,and describe any new items that are available in the world.Also consider utilizing sounds when actions are performed.The sounds that play in Happy Beamwhen a cloud turns happy are a great wayto keep the app feeling fun and spatial,even if we can't see the visual transformation of the cloud.Let's take a look at our app one last timewith some announcements added.When the game starts, we'll enable Direct Gesture Modewith a left index finger triple-pinch and hold.Then, we can make heart gestures to turn the clouds happyand get feedback about all of the interactions.VoiceOver: Three, two, one.Happy Beam.Three clouds above and in front of you to the right.Direct Gestures enabled.Press the crown to ac-- Casting beam.♪Grumpy cloud hit.Hiding beam.Dan: Awesome!We received some great feedback thereabout all of the interactions as they were being performed.Our app is shaping up to have some great VoiceOver support.But there's still a lot you can doto support people with low visionwho aren't using your app with VoiceOver,especially if you're building any custom componentsor controls.Just like on all other Apple platforms,make sure your app responds to changes in Dynamic Type,especially at the largest sizes availablein accessibility settings.Audit your app for any UI that might benefitfrom being laid out vertically instead of horizontallyat these larger sizes.Also ensure you're usingat least a four-to-one contrast ratiobetween foreground and background colors.To learn more, check out "Make your apps visually accessible."In spatial experiences, anchors can be usedto place content relative to different anchor points,such as a hand or a specific position in the world.You can also configure content to be anchoredto the virtual camera, so that it appearson the same spot on the displays.While you may be familiar with camera anchorsin Reality Kit on other Apple platforms,on this platform, the content follows your headas you look around, which has the potentialto impact people with low vision differently.Head anchors should be avoided and used sparingly,so that people with low vision can get closer to contentto read it or view its details.Additionally, people using the accessibility Zoom featurewon't be able to easily position head-anchored contentinside of the Zoom lens,since the Zoom lens is also head anchored.Instead, consider using a world anchoror lazily move your content after a delay.In the rare cases where it's absolutely necessaryto use head anchors, keep the content decorative.Critical information should not only be accessiblethrough head-anchored content.Always provide alternatives for head anchors,even when they might be the best mainstream experiencein your app.The new accessibilityPrefers HeadAnchorAlternativeEnvironment variable in SwiftUI,and the AX PrefersHeadAnchor Alternative APIin the Accessibility framework,let you know when you should use alternate anchors.Observe these APIs anywhere your app employs head anchors.We've taken care to adopt this API in the system ourselves.By default, Control Center is head anchoredin its collapsed state.You can see here that, as you look around,Control Center follows you.

While this design makes it easy to access Control Centerfrom anywhere,we knew it might be challenging for some people.I mentioned earlier that Zoom is also head anchored.This is a feature that magnifies contentfor people with low vision.When Zoom is enabled or someone has otherwise indicatedthat they prefer alternatives to head anchors,Control Center moves freely about the Y axis.Here, you can see that, as you tilt your head up,the Zoom lens follows your head, but Control Center does not.Enabling you to position Control Centerinside of the Zoom lens and interact with it.It's also important to be mindfulof the usage of motion in your app.Motion can be dizzying for some people,and wearing a headset can be an especially jarring experience,even when subtle motion effects are used.Avoid the use of motionthat move a person rapidly through your app,or involve a bouncing or wave-like movement.Zooming animations, animations that involve movementalong more than one axis, spinning or rotating effects,and persistent background effectsshould all also be avoided.Always provide alternatives for these types of animationswhen Reduce Motion is enabled.You can check whether Reduce Motion is active with theaccessibilityReduceMotion Environment variable in SwiftUI.In UIKit, you can queryUIAccessibility. isReduceMotionEnabled,and observe changes to the preferencewith the corresponding notification.If you're having trouble finding a suitable replacementfor the motion in your app, consider utilizing a crossfade.Here's an example of how we've adopted Reduce Motionin the system.Check out the water here in the Mount Hood Environment,as the water persistently ripples in the background.When we toggle Reduce Motion on, the water is changedto statically show a ripple effect,which achieves a similar visual effectwithout requiring the usage of motion.That's an overview of some of the ways you can improvethe vision accessibility in your apps,but there's still a lot to consider when discussingmotor, cognitive, and hearing accessibility.And for that, here's Drew to tell you more.Drew Haas: Thanks, Dan. Really great work!My name is Drew Haas and I'm an engineeron the Accessibility team.Now that we've learned about meaningful waysto improve the visual accessibilityof your spatial experiences, there is so much I want to shareabout how to create your apps to be inclusiveto people with disabilities which affectphysical and motor function, cognition, and hearing.Let's start first with motor!The default input system is drivenby a combination of eyes and hands.For example, looking at a button with your eyes,and pinching with your hand sends a selection eventto activate the button.However, not everyone can perform these physical actions.Our accessibility features provide alternate input methodsfor people with disabilities which impact their use of eyes,hands, or both.The Dwell Control accessibility feature allows peopleto select and interact with the UI without using their hands.Dwell Control supports gestures like tap, scroll,long press, and drag.You should design your app to have full functionalitywith this gesture set so people using Dwell Controlaren't excluded.It's a breeze to switch gesture modesby using the Dwell Control menu, allowing people to operatetheir device using accommodationswithout sacrificing efficiency.This is by design: giving people a frictionless experienceeven if they're using nondefault inputs.Let's see how our Happy Beam appis designed to support a variety of inputs,which makes it fully playable using Dwell Control.When Happy Beam is launched, the player will first choosehow they'll cheer up those grumpy clouds.You saw the first option earlier in this session:using two hands in the shape of a heartand aiming it at the clouds.The second option supports Bluetooth accessorieslike keyboards and game controllers.When playing with these inputs, the happy beam is firedusing this heart turret.The turret also responds to tap-and-drag gestures,which means you can play with one hand.And, people playing with Dwell Controlhave full functionality over the turret.So plan and design for your app to support different inputslike Happy Beam does.This is the best way to ensureyou don't accidentally exclude people.There is another accessibility featurethat works really well with Dwell.Enter Pointer Control,one of my favorite accessibility features.This feature transforms the input experience,allowing people to use different input sourcesto control the system focus instead of using their eyes.Eyes is the default, but here people can changethe system focus to be driven by head position,wrist position, or index finger.Since Pointer Control can change the input signalto follow head position,remember to use camera-anchored content sparingly.This is another reason you should preferto use world anchors or provide alternativesto camera-anchored content.Both Dwell Control and Pointer Control --either on their own or utilizing their feature sets combined --provide tons of flexibility with how peopleinteract with their device.These features accommodate the physical requirementsto use the system.Allow multiple avenues for physical interaction,because you never know what kind of disabilitiessomeone using your app may have.Spatial experiences enable new,dimensional ways to interact with content.Switch Control has new menu optionsfor adjusting the camera's position in world space.Here we're using a keyboard with Switch Controlto activate the new camera position modifiers.This moves your spatial position downwardwithout you physically moving your body.Not everyone will be able to move comfortably or freelyin their environment.While these camera position options are availablefor Switch Control, if you have experiencesthat require people to position themselvesin certain ways, provide options to bypass them.Next, I want to talk about Cognitive accessibility,and how you can support people with disabilities that affectthe way they learn, remember, and process information.Guided Access is a cognitive accessibility featurethat promotes focus by restricting the systemto a single app.It aims to minimize distractions by backgrounding other apps,removing ornamental UI which may be distracting,and by suppressing hardware button eventsthat could take someone out of their experience.Being able to adjust the system in this way can make it easierfor someone to stay focused on their current task,without distractions or easy ways to get sidetracked.To learn more about how to use Guided Accessand implement the custom restrictions APIs,check out my talk from last year,"Create accessible Single App Mode experiences."By following just a few best practicesfor cognitive accessibility, you make your app easierto use for everyone,but especially people with disabilities.Some people need a little more helpbreaking down the complexity of your app.Interactions which require complex hand gesturescan be hard for people to pick up on and retain.You can help create a consistent and familiar visual experienceby using Apple's UI frameworks like SwiftUI.This reduces the amount of time someone may needto feel comfortable using your app,because it's likely they've used other appsbuilt using the same UI framework.And finally, allow people to take their timeimmersing themselvesand experiencing all that you have to offer.There is no need to rush people through an experience.Immersive content can promote focus and attention,which is a fantastic way to create a comfortable environmentfor someone with sensory processing disorders.Remember that not everyone processes informationat the same speed, so some people may preferor need a little extra time to work through an experience.Finally, I want to share some of the best waysto provide access and accommodationto people that are deaf or hard of hearing.It's common to use audio and speechas a way to immerse people in a spatial experience.For people with hearing loss or auditory processing disorders,one of the most impactful things you can dois provide quality captions so they can access your content.A comfortable reading experience is easy to createby using pop-on captions --which render the phrase all at once and are easy to read --instead of using roll-up captions,which appear word-by-word and can cause reading fatigueand nausea when reading for a long duration.Did you know that people can customizethe visual appearance of captions on their device?Captions can be widely customized,modifying things like text size, font, and color,as well as stroke outlines or backgrounds.These options allow people to customize their captionsso they're easy to see and read.AVKit and AVFoundation provide built-in supportfor supplying captions in your app.These frameworks automatically handlethe caption appearance and visual style.If you are not using AVFoundationbecause you're implementing your own captions system,there are two APIs that you should know about.First, the isClosedCaptioningEnabled API.Use this to check whether someone already hasClosed Captions turned on in Accessibility settings.If you have separate caption settings in your app,you should use this APIto inform the default state of captioning.This way, people who rely on captionsget access to them right away.The second can be found in the Media Accessibility framework,which has APIs to access each style attribute individually.You should check these styles and apply them to your captionsto keep a consistent reading experienceacross the entire system.No matter which way you choose to provide captions,you should have a high standard for their quality.Captions should represent all audio content,including music and sound effects.It's also helpful to indicate where in spacethe audio is coming from if directionalityis important to your experience, basically telling the userto "keep in mind, the nearest audio source may be behind you."An impressive accessibility experiencecomes from considering all people and their needs.Provide rich RealityKit experiencesby setting accessibility properties on your entities.This is the foundation for your app's accessibilityfor technologies like VoiceOver, Voice Control,and Switch Control.Be flexible and provide options for physical interactionlike we saw in Happy Beam to include all playersand their play styles.Strive to remove ambiguity and provide clarityand focus for people with cognitive disabilities,and spend time and care on captioned contentfor audio experiences so that people that are deafor hard of hearing can enjoy your creation.If you're not sure where to start,turn on some of these accessibility featuresand open up your app!Trying out these features for yourselfis a great way to dive right in.This platform is designed for everyone,and with all of the considerationsDan and I have shared today, you are equippedto create accessible and inclusive spatial experiences.Thank you!♪

5:28 -Use AccessibilityComponent with RealityKit

8:04 -Add an activate action

9:23 -Announce meaningful events and changes in context

13:15 -Provide alternatives to head anchored content

15:04 -Provide alternatives when Reduce Motion is enabled

23:35 -Check whether captions are enabled

## Code Samples

```swift
var
 accessibilityComponent 
=
 
AccessibilityComponent
()
accessibilityComponent.isAccessibilityElement 
=
 
true

accessibilityComponent.traits 
=
 [.button, .playsSound]
accessibilityComponent.label 
=
 
"Cloud"

accessibilityComponent.value 
=
 
"Grumpy"

cloud.components[
AccessibilityComponent
.
self
] 
=
 accessibilityComponent


// ...



var
 isHappy: 
Bool
 {
    
didSet
 {
        cloudEntities[id].accessibilityValue 
=
 isHappy 
?
 
"Happy"
 : 
"Grumpy"

    }
}
```

```swift
var
 accessibilityComponent 
=
 
AccessibilityComponent
()
accessibilityComponent.isAccessibilityElement 
=
 
true

accessibilityComponent.traits 
=
 [.button, .playsSound]
accessibilityComponent.label 
=
 
"Cloud"

accessibilityComponent.value 
=
 
"Grumpy"

accessibilityComponent.systemActions 
=
 [.activate]
cloud.components[
AccessibilityComponent
.
self
] 
=
 accessibilityComponent


// ...


content.subscribe(to: 
AccessibilityEvents
.
Activate
.
self
, componentType: 
nil
) { activation 
in

    handleCloudCollision(for: activation.entity, gameModel: gameModel)
}
```

```swift
AccessibilityNotification
.
Announcement
(
"8 clouds in front of you"
).post()
```

```swift
// SwiftUI


@Environment
(\.accessibilityPrefersHeadAnchorAlternative)

private
 
var
 accessibilityPrefersHeadAnchorAlternative


// UIKit


AXPrefersHeadAnchorAlternative
()

NSNotification
.
Name
.
AXPrefersHeadAnchorAlternativeDidChange
```

```swift
// SwiftUI


@Environment
(\.accessibilityReduceMotion)

private
 
var
 accessibilityReduceMotion


// UIKit


UIAccessibility
.isReduceMotionEnabled

UIAccessibility
.reduceMotionStatusDidChangeNotification
```

```swift
UIAccessibility
.isClosedCaptioningEnabled

UIAccessibility
.closedCaptioningStatusDidChangeNotification
```

