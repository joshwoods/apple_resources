# Wwdc2023 10071

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Deliver video content for spatial experiencesLearn how to prepare and deliver video content for visionOS using HTTP Live Streaming (HLS). Discover the current HLS delivery process for media and explore how you can expand your delivery pipeline to support 3D content. Get up to speed with tips and techniques for spatial media streaming and adapting your existing caption production workflows for 3D. And find out how to share audio tracks across video variants and add spatial audio to make your video content more immersive.Chapters0:41 -Deliver 2D content5:20 -Deliver 3D contentResourcesApple HEVC Stereo Video Interoperability ProfileVideo Contour Map Payload Metadata within the QuickTime Movie File FormatHD VideoSD VideoRelated VideosWWDC23Create a great spatial playback experienceEnhance your spatial computing app with RealityKit

Learn how to prepare and deliver video content for visionOS using HTTP Live Streaming (HLS). Discover the current HLS delivery process for media and explore how you can expand your delivery pipeline to support 3D content. Get up to speed with tips and techniques for spatial media streaming and adapting your existing caption production workflows for 3D. And find out how to share audio tracks across video variants and add spatial audio to make your video content more immersive.

0:41 -Deliver 2D content

5:20 -Deliver 3D content

Apple HEVC Stereo Video Interoperability Profile

Video Contour Map Payload Metadata within the QuickTime Movie File Format

HD VideoSD Video

HD Video

SD Video

Create a great spatial playback experience

Enhance your spatial computing app with RealityKit

Search this video…♪ Mellow instrumental hip-hop ♪♪Howdy, I'm Chris.I'm an engineer on the AVFoundation team,and I'd like to welcome you to our session.In this talk, we're going to look at how to prepareand deliver streaming content for spatial experiences.We'll start with a brief review of the current stepsin producing, preparing, and delivering 2D mediausing HTTP Live Streaming.also known as HLS.With 2D content preparation and delivery covered,we'll turn to 3D video content --what's supported and updates to the steps just described.Considering the content pipeline,we'll start with media encoding of video, audio, and captions.Then, those media resources need to be packaged,ready for HLS delivery.This is how 2D content is delivered today.The goal of delivering 3D contentis to build upon current 2D processes.HLS adds new support for fragmented MP4 timed metadatathat allows an important adaptation.Please note the HTTP Live Streaming pageon the Apple Developer website, which provides linksto documentation, tools, example streams,developer forums, and other resources.This is where more details covered in this talkwill be made available over time.Our goal is that delivering 2D audiovisual contentto this platform should be the same as all our other platforms.This is achieved by building upon Apple Media technologysuch as HTTP Live Streaming, AVFoundation, Core Media,and standards-based formatssuch as the ISO-based media file format,often thought of as MPEG-4.This is done all while supportinga new spatial experiences paradigm.For a deep dive into how to best support playbackof audiovisual media, see the session"Create a great spatial experience for video playback."For video, encode the source video.Edit it to the right length and color correct itfor the bitrate tiers that matter to you.Here you'll make choices of how you configureand use video encoders such as HEVC,short for High Efficiency Video Coding.While support for existing 2D audiovisual mediayou deliver to other Apple platforms is fully supported,take note of these playback capabilities.This platform supports playback of up to 4K resolution,allowing your highest-quality video to be experienced.The display's refresh rate is 90 Hertz,And for 24-frames-per-second video,a special 96-hertz mode may be used automatically.There's support for standard and high dynamic range.For your video's corresponding audio,identify and produce the number of source audio streamsyou need.The number depends upon the set of spoken languagesyou are targeting and the roles of that audio.One role might be main dialogue,another, audio descriptions.Encode these sources for delivery with HLS in mind.You may want to deliver Spatial Audio,along with a fallback stereo audio track.This ensures a great experience for those devicessupporting Spatial Audio and reliable playback everywhere.The HLS Developer page has links to documentationon preparing audio.And then there are captions.Here, captions includes both subtitles and closed captionsto cover different languages and roles.The term "subtitles" is used for transcriptionsof spoken text providing translationsin different languages for viewersthat may not speak the language or for establishingthe settings time and place.Closed captions is like subtitles but is intendedwhen the audio can't be heard by the viewer.Close captions provide a transcriptionof not only the dialogue but also of sound effectsand other relevant audio cues.There might also be subtitlesfor the Deaf and hard of hearing, SDH,serving the same purpose.Akin to video and audio encoding,you should produce caption files and formatssupported by HLS, most commonly WebVTT.With source video, audio, and captions in hand,next comes packaging.Packaging is a process that transforms the source mediainto various types of segments for reliable delivery.This can be done with Apple's HLS toolsavailable at the earlier HLS streaming page.Some content providers might use their own production tools,hardware, or workflows.Others might be vendors delivering those servicesand tools to the first group.The goal of packaging is to produce a set of media segments,the media playlists that drive their use,and a multivariant playlist that ties them all together.Two kinds of HLS media segments are most typically used today.Fragmented MP4 media segments are produced by startingwith an already encoded movie file of video or audioand generating a number of resources.These resources are known as media segments.It is these segments that are retrievedby client devices during playback.Subtitle files also require segmenting.This is done with a subtitle-segmenting toolto generate media segments.A source WebVTT file may be split into any numberof WebVTT files for the target segment duration.Finally, the collection of HLS resourcesis hosted on a web server for HTTP delivery.This might be to one server that serves clients directlyor to an origin server used with a content delivery network,or a CDN.Either way, it is these resourcesthat are delivered to client devices for playback.Now that we've reviewed the 2D productionand delivery pipeline, let's turn to 3D contentand the differences taking advantageof new spatial capabilities.We will again look at source encoding,packaging, and delivery, focusing on differencesbetween 2D content and 3D stereoscopic content.So, we're talking about 3D video.Let's deconstruct this term.First, it's video, so a sequence of framesin a movie track or a network stream.The "3D" in "3D video" is used interchangeablywith stereoscopic, which provides an imagefor the left eye, and another very similar imagefrom a slightly different perspective for the right eye.These differences between the left and right images,called parallax, causes you to perceivethree-dimensional depth in the video when presented.While there are choices in how 3D video frames might be carriedthere are some guiding principles that seem useful.By using a single video track for all stereo frames,traditional production with 2D video tracks is preserved.Both the left and right images or views,for any display time is in a single compressed frame.If you have a frame in your hands,you have both views or the stereo pair.It should be efficient,ideally, it's supported by Apple silicon,and to the greatest degree possible,it should be decodable by non-3D-aware playback,allowing the video to be auditioned in 2D workflows.To deliver stereo frames, we introduce the useof multiview HEVC, also called "MV-HEVC."It's an extension of HEVC.The "MV" is multiview.Carrying more than one view in each frame,each frame has a pairof compressed left and right images.Because MV-HEVC is HEVC at its heart,Apple silicon supports it.In MV-HEVC stores the base HEVC 2D viewin each compressed frame.Encoding determines a difference, or delta,between the left and right images.This technique, known as 2D Plus Delta,means that 2D decoders can find and usethe base 2D view, for example the left eye.But 3D decoders can calculate the other viewto present both views to the corresponding eyes.Efficiency is achieved because the differencesbetween base 2D images uses standard HEVC techniques,and just the differences between the left and right eye viewsare described in the stereo frames.The video format description,or the visual sample entry in MPEG-4,indicates the coding type, the codec,the dimensions of each view,and other details necessary to decode the video frames.A new extension to the video format descriptionis introduced.Termed the Video Extended Usage box,it serves as a lightweight, easily discoverable signalthat the video is stereoscopic,and which stereo eye views are present.For HLS delivery, this will be both left and right.A specification describing this new VEXU boxis available with the SDK.Its structure will evolve, and that will be describedin the specification.Like 2D content, 3D video uses HEVC,except this time, the variation called MV-HEVC.This is required to carry the stereoscopic views.Like with 2D production, local movies with MV-HEVCcan be used and should behave like other 2D video.Having both a left and a right imagepresented to the corresponding eyeproduces a perception of stereoscopic depth,providing a sense of relative depth.An object in the video scene might be perceived neareror farther than another due to the differing amountsof parallax.Three primary zones of stereoscopic depthcan be defined.They are the screen plane with no parallax cues;negative parallax, which will cause objectsto be perceived in front of the screen plane;and positive parallax, which will cause objectsto be perceived behind the screen plane.If an element like a caption is rendered with no parallaxin the same area of the frame as negative parallax cues,then a depth conflict will be createdand cause discomfort when viewing.Question.Given stereoscopic parallax and potential for depth conflict,how involved is producing captions for 3D video?Can we support the following?Playback works for horizontal captions,playback works across languages,including for vertical captions,and playback works when accessibility settingsis used to adjust the user's preferred caption sizing.Well, the answer is yes.With stereoscopic video using the approach I'll next describe,captions should just work as is,while also allowing the same 2D caption assetsto be shared between 2D and 3D experiences.This is possible by including the new timed metadataI mentioned earlier.With stereoscopic video, avoiding depth conflictand visual elements overlaying the video is important.Instead of requiring new caption formatsor changes to existing formats,we offer a way to characterize each video frame's parallax.This can vary across the frame with some areasapparently closer and some farther from the viewer.We call this a parallax contour,and it is recorded as metadata in a metadata trackthat is synchronized with a video track's frames.If we tile the 3D video and indicate the depth parallaxfor each tile,we can use that to ensure that captions never interferewith elements in the stereo video.During playback, the parallax of the captionwill be automatically adjusted to avoid depth conflict.Each metadata item with such a parallax video contourdescribes a 2D tiling of the associated videowith the minimum parallax value associated with each tile.Each video frame's presentation should be associatedwith a metadata item describing the video frame's contour.We recommend a 10 by 10 tiling as a good balancebetween storage and resolution to characterizedifferent areas of parallax in the video.Considering how this parallax metadata is produced,start with left and right views for each frame.This can be done in productionwith two synchronized video tracksand doesn't require MV-HEVC.Then, perform parallax or disparity analysis to createparallax information suitable for describing the tiling.For each stereo frame, this is then packagedin a metadata payload for the next step.A specification describing the format of this metadatais available with the SDK.This parallax information is packaged in metadata samplesand written into a timed metadata track.The metadata track will be associatedwith the corresponding video it describes.The metadata and video track should be multiplexedwith the video so that HLS packaging will producevideo segments with both the video and the parallax metadata.Captions you might already produce for 2Dcan be reused with 3D.This means the processes used todayor the vendor you might work withcan continue to work in 2D with your 3D production.Also, this means your 3D content is agnosticto the choice of languages, horizontal and vertical layout,or potential use of accessibility subtitlepreferences by users.By adding the described parallax metadata,the platform adapts dynamically to the parallax metadatayou build.As for audio use with 3D video, you can use the same audio usefor 2D delivery.As the platform supports head tracking,consider using a Spatial Audio format.To share the same audio between 2D and 3D experiences,the video should match timingwise,having the same edits.If they differ, you will need to separate audio tracksbetween the 2D and 3D assets.Turning to packaging of 3D, updated HLS toolstake care of the details, with 3D assetsmaking the process nearly identical to that with 2D.Most production systems, which do not use Apple's tools,will be able to use the new specs that are being releasedto build equivalent functionality.If you're building your own playlists or inspecting them,take note of a few changes.REQ-VIDEO-LAYOUT is a new tag for video streamsto indicate video is stereoscopic.The attribute value indicates if the video is stereo or not.Note that if your asset is loaded as 3D,it won't switch to 2D or vice versa.2D Video is unchanged and can be mixed with 3D videoin the same playlist.REQ-VIDEO-LAYOUT requires a new version of the HLS spec,so the version is updated to 12.This is documented with the SDK.Here's an example multivariant playlistwith the change of the version number to 12,and using REQ-VIDEO-LAYOUT for the 3D video stream.For the best navigation experience,you should include a 2D iFrame streamto the multivariant playlist to support thumbnail scrubbing.Finally, HLS delivery works the same with 3D assets.Delivering 3D assets is largely the sameas delivering 2D assets, but there are some thingsyou can do to optimize the experience.Prepare your source assets, noting to use MV-HEVCfor 3D video,and including the new parallax contour metadata.Audio and caption production can be the same.Use updated packaging to produce the relevant segmentsand playlists.Hosting remains the same.Before closing, I want to emphasizethat visual comfort is a key content-design goalfor 3D experiences.3D content should be comfortable to watchfor sufficiently long durations.Some 3D content characteristics that could potentiallycause comfort issues include extreme parallax,both negative and positive,high motion in the content causing focusing difficulties;as well as depth conflictsresulting from something termed "window violations."Screen size may affect viewing comfort,depending on how much of the screenis in the viewer's horizontal field of view.Note that the user can affect the screen sizeby positioning it nearer or farther.So in our journey, we've looked at 2D and 3D deliverywith HTTP Live Streaming.For video, I introduced MV-HEVC.For audio, we noted that the same audio streamscan be used across 2D and 3D.For captions, the same streams can likewise be usedacross 2D and 3D.Finally, a new timed metadata formatis introduced to characterize the 3D videos' parallax,allowing the same captions to be used.To wrap up, we've made it as easy as possibleto bring your existing 2D content to a spatial experience.With some small modifications to your current 2D pipeline,you can support 3D content using MV-HEVC.You can even continue to use all your existing captionsfrom 2D assets.But if you provide timed metadata,those captions can be unobscuredand provide a comfortable viewing experience.Watch our companion session for considerationsin implementing playback of video.We look forward to the great new content you'll be delivering.Thanks for joining us today.♪

♪ Mellow instrumental hip-hop ♪♪Howdy, I'm Chris.I'm an engineer on the AVFoundation team,and I'd like to welcome you to our session.In this talk, we're going to look at how to prepareand deliver streaming content for spatial experiences.We'll start with a brief review of the current stepsin producing, preparing, and delivering 2D mediausing HTTP Live Streaming.also known as HLS.With 2D content preparation and delivery covered,we'll turn to 3D video content --what's supported and updates to the steps just described.Considering the content pipeline,we'll start with media encoding of video, audio, and captions.Then, those media resources need to be packaged,ready for HLS delivery.This is how 2D content is delivered today.The goal of delivering 3D contentis to build upon current 2D processes.HLS adds new support for fragmented MP4 timed metadatathat allows an important adaptation.Please note the HTTP Live Streaming pageon the Apple Developer website, which provides linksto documentation, tools, example streams,developer forums, and other resources.This is where more details covered in this talkwill be made available over time.Our goal is that delivering 2D audiovisual contentto this platform should be the same as all our other platforms.This is achieved by building upon Apple Media technologysuch as HTTP Live Streaming, AVFoundation, Core Media,and standards-based formatssuch as the ISO-based media file format,often thought of as MPEG-4.This is done all while supportinga new spatial experiences paradigm.For a deep dive into how to best support playbackof audiovisual media, see the session"Create a great spatial experience for video playback."For video, encode the source video.Edit it to the right length and color correct itfor the bitrate tiers that matter to you.Here you'll make choices of how you configureand use video encoders such as HEVC,short for High Efficiency Video Coding.While support for existing 2D audiovisual mediayou deliver to other Apple platforms is fully supported,take note of these playback capabilities.This platform supports playback of up to 4K resolution,allowing your highest-quality video to be experienced.The display's refresh rate is 90 Hertz,And for 24-frames-per-second video,a special 96-hertz mode may be used automatically.There's support for standard and high dynamic range.For your video's corresponding audio,identify and produce the number of source audio streamsyou need.The number depends upon the set of spoken languagesyou are targeting and the roles of that audio.One role might be main dialogue,another, audio descriptions.Encode these sources for delivery with HLS in mind.You may want to deliver Spatial Audio,along with a fallback stereo audio track.This ensures a great experience for those devicessupporting Spatial Audio and reliable playback everywhere.The HLS Developer page has links to documentationon preparing audio.And then there are captions.Here, captions includes both subtitles and closed captionsto cover different languages and roles.The term "subtitles" is used for transcriptionsof spoken text providing translationsin different languages for viewersthat may not speak the language or for establishingthe settings time and place.Closed captions is like subtitles but is intendedwhen the audio can't be heard by the viewer.Close captions provide a transcriptionof not only the dialogue but also of sound effectsand other relevant audio cues.There might also be subtitlesfor the Deaf and hard of hearing, SDH,serving the same purpose.Akin to video and audio encoding,you should produce caption files and formatssupported by HLS, most commonly WebVTT.With source video, audio, and captions in hand,next comes packaging.Packaging is a process that transforms the source mediainto various types of segments for reliable delivery.This can be done with Apple's HLS toolsavailable at the earlier HLS streaming page.Some content providers might use their own production tools,hardware, or workflows.Others might be vendors delivering those servicesand tools to the first group.The goal of packaging is to produce a set of media segments,the media playlists that drive their use,and a multivariant playlist that ties them all together.Two kinds of HLS media segments are most typically used today.Fragmented MP4 media segments are produced by startingwith an already encoded movie file of video or audioand generating a number of resources.These resources are known as media segments.It is these segments that are retrievedby client devices during playback.Subtitle files also require segmenting.This is done with a subtitle-segmenting toolto generate media segments.A source WebVTT file may be split into any numberof WebVTT files for the target segment duration.Finally, the collection of HLS resourcesis hosted on a web server for HTTP delivery.This might be to one server that serves clients directlyor to an origin server used with a content delivery network,or a CDN.Either way, it is these resourcesthat are delivered to client devices for playback.Now that we've reviewed the 2D productionand delivery pipeline, let's turn to 3D contentand the differences taking advantageof new spatial capabilities.We will again look at source encoding,packaging, and delivery, focusing on differencesbetween 2D content and 3D stereoscopic content.So, we're talking about 3D video.Let's deconstruct this term.First, it's video, so a sequence of framesin a movie track or a network stream.The "3D" in "3D video" is used interchangeablywith stereoscopic, which provides an imagefor the left eye, and another very similar imagefrom a slightly different perspective for the right eye.These differences between the left and right images,called parallax, causes you to perceivethree-dimensional depth in the video when presented.While there are choices in how 3D video frames might be carriedthere are some guiding principles that seem useful.By using a single video track for all stereo frames,traditional production with 2D video tracks is preserved.Both the left and right images or views,for any display time is in a single compressed frame.If you have a frame in your hands,you have both views or the stereo pair.It should be efficient,ideally, it's supported by Apple silicon,and to the greatest degree possible,it should be decodable by non-3D-aware playback,allowing the video to be auditioned in 2D workflows.To deliver stereo frames, we introduce the useof multiview HEVC, also called "MV-HEVC."It's an extension of HEVC.The "MV" is multiview.Carrying more than one view in each frame,each frame has a pairof compressed left and right images.Because MV-HEVC is HEVC at its heart,Apple silicon supports it.In MV-HEVC stores the base HEVC 2D viewin each compressed frame.Encoding determines a difference, or delta,between the left and right images.This technique, known as 2D Plus Delta,means that 2D decoders can find and usethe base 2D view, for example the left eye.But 3D decoders can calculate the other viewto present both views to the corresponding eyes.Efficiency is achieved because the differencesbetween base 2D images uses standard HEVC techniques,and just the differences between the left and right eye viewsare described in the stereo frames.The video format description,or the visual sample entry in MPEG-4,indicates the coding type, the codec,the dimensions of each view,and other details necessary to decode the video frames.A new extension to the video format descriptionis introduced.Termed the Video Extended Usage box,it serves as a lightweight, easily discoverable signalthat the video is stereoscopic,and which stereo eye views are present.For HLS delivery, this will be both left and right.A specification describing this new VEXU boxis available with the SDK.Its structure will evolve, and that will be describedin the specification.Like 2D content, 3D video uses HEVC,except this time, the variation called MV-HEVC.This is required to carry the stereoscopic views.Like with 2D production, local movies with MV-HEVCcan be used and should behave like other 2D video.Having both a left and a right imagepresented to the corresponding eyeproduces a perception of stereoscopic depth,providing a sense of relative depth.

An object in the video scene might be perceived neareror farther than another due to the differing amountsof parallax.Three primary zones of stereoscopic depthcan be defined.They are the screen plane with no parallax cues;negative parallax, which will cause objectsto be perceived in front of the screen plane;and positive parallax, which will cause objectsto be perceived behind the screen plane.If an element like a caption is rendered with no parallaxin the same area of the frame as negative parallax cues,then a depth conflict will be createdand cause discomfort when viewing.Question.Given stereoscopic parallax and potential for depth conflict,how involved is producing captions for 3D video?Can we support the following?Playback works for horizontal captions,playback works across languages,including for vertical captions,and playback works when accessibility settingsis used to adjust the user's preferred caption sizing.Well, the answer is yes.With stereoscopic video using the approach I'll next describe,captions should just work as is,while also allowing the same 2D caption assetsto be shared between 2D and 3D experiences.This is possible by including the new timed metadataI mentioned earlier.With stereoscopic video, avoiding depth conflictand visual elements overlaying the video is important.Instead of requiring new caption formatsor changes to existing formats,we offer a way to characterize each video frame's parallax.This can vary across the frame with some areasapparently closer and some farther from the viewer.We call this a parallax contour,and it is recorded as metadata in a metadata trackthat is synchronized with a video track's frames.If we tile the 3D video and indicate the depth parallaxfor each tile,we can use that to ensure that captions never interferewith elements in the stereo video.During playback, the parallax of the captionwill be automatically adjusted to avoid depth conflict.Each metadata item with such a parallax video contourdescribes a 2D tiling of the associated videowith the minimum parallax value associated with each tile.Each video frame's presentation should be associatedwith a metadata item describing the video frame's contour.We recommend a 10 by 10 tiling as a good balancebetween storage and resolution to characterizedifferent areas of parallax in the video.Considering how this parallax metadata is produced,start with left and right views for each frame.This can be done in productionwith two synchronized video tracksand doesn't require MV-HEVC.Then, perform parallax or disparity analysis to createparallax information suitable for describing the tiling.For each stereo frame, this is then packagedin a metadata payload for the next step.A specification describing the format of this metadatais available with the SDK.This parallax information is packaged in metadata samplesand written into a timed metadata track.The metadata track will be associatedwith the corresponding video it describes.The metadata and video track should be multiplexedwith the video so that HLS packaging will producevideo segments with both the video and the parallax metadata.Captions you might already produce for 2Dcan be reused with 3D.This means the processes used todayor the vendor you might work withcan continue to work in 2D with your 3D production.Also, this means your 3D content is agnosticto the choice of languages, horizontal and vertical layout,or potential use of accessibility subtitlepreferences by users.By adding the described parallax metadata,the platform adapts dynamically to the parallax metadatayou build.As for audio use with 3D video, you can use the same audio usefor 2D delivery.As the platform supports head tracking,consider using a Spatial Audio format.To share the same audio between 2D and 3D experiences,the video should match timingwise,having the same edits.If they differ, you will need to separate audio tracksbetween the 2D and 3D assets.Turning to packaging of 3D, updated HLS toolstake care of the details, with 3D assetsmaking the process nearly identical to that with 2D.Most production systems, which do not use Apple's tools,will be able to use the new specs that are being releasedto build equivalent functionality.If you're building your own playlists or inspecting them,take note of a few changes.REQ-VIDEO-LAYOUT is a new tag for video streamsto indicate video is stereoscopic.The attribute value indicates if the video is stereo or not.Note that if your asset is loaded as 3D,it won't switch to 2D or vice versa.2D Video is unchanged and can be mixed with 3D videoin the same playlist.REQ-VIDEO-LAYOUT requires a new version of the HLS spec,so the version is updated to 12.This is documented with the SDK.Here's an example multivariant playlistwith the change of the version number to 12,and using REQ-VIDEO-LAYOUT for the 3D video stream.For the best navigation experience,you should include a 2D iFrame streamto the multivariant playlist to support thumbnail scrubbing.Finally, HLS delivery works the same with 3D assets.Delivering 3D assets is largely the sameas delivering 2D assets, but there are some thingsyou can do to optimize the experience.Prepare your source assets, noting to use MV-HEVCfor 3D video,and including the new parallax contour metadata.Audio and caption production can be the same.Use updated packaging to produce the relevant segmentsand playlists.Hosting remains the same.Before closing, I want to emphasizethat visual comfort is a key content-design goalfor 3D experiences.3D content should be comfortable to watchfor sufficiently long durations.Some 3D content characteristics that could potentiallycause comfort issues include extreme parallax,both negative and positive,high motion in the content causing focusing difficulties;as well as depth conflictsresulting from something termed "window violations."Screen size may affect viewing comfort,depending on how much of the screenis in the viewer's horizontal field of view.Note that the user can affect the screen sizeby positioning it nearer or farther.So in our journey, we've looked at 2D and 3D deliverywith HTTP Live Streaming.For video, I introduced MV-HEVC.For audio, we noted that the same audio streamscan be used across 2D and 3D.For captions, the same streams can likewise be usedacross 2D and 3D.Finally, a new timed metadata formatis introduced to characterize the 3D videos' parallax,allowing the same captions to be used.To wrap up, we've made it as easy as possibleto bring your existing 2D content to a spatial experience.With some small modifications to your current 2D pipeline,you can support 3D content using MV-HEVC.You can even continue to use all your existing captionsfrom 2D assets.But if you provide timed metadata,those captions can be unobscuredand provide a comfortable viewing experience.Watch our companion session for considerationsin implementing playback of video.We look forward to the great new content you'll be delivering.Thanks for joining us today.♪

## Code Samples

