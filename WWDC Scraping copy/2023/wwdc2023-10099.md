# Wwdc2023 10099

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Meet RealityKit TraceDiscover how you can use RealityKit Trace to improve the performance of your spatial computing apps. Explore performance profiling guidelines for this platform and learn how the RealityKit Trace template can help you optimize rendering for your apps. We'll also provide guidance on profiling various types of content in your app to help pinpoint performance issues.Chapters0:44 -Rendering2:17 -Profiling spatial apps3:34 -Introduction to RealityKit Trace7:38 -Optimizing offscreen passes11:30 -Optimizing asset rendering14:02 -Optimizing system power impact17:55 -Overview of optimized World app19:01 -RecommendationsResourcesAnalyzing the performance of your visionOS appHD VideoSD VideoRelated VideosWWDC23Analyze hangs with InstrumentsGet started with building apps for spatial computingMeet Reality Composer ProOptimize app power and performance for spatial computingWWDC21Discover Metal debugging, profiling, and asset creation toolsTech TalksDemystify and eliminate hitches in the render phase

Discover how you can use RealityKit Trace to improve the performance of your spatial computing apps. Explore performance profiling guidelines for this platform and learn how the RealityKit Trace template can help you optimize rendering for your apps. We'll also provide guidance on profiling various types of content in your app to help pinpoint performance issues.

0:44 -Rendering

2:17 -Profiling spatial apps

3:34 -Introduction to RealityKit Trace

7:38 -Optimizing offscreen passes

11:30 -Optimizing asset rendering

14:02 -Optimizing system power impact

17:55 -Overview of optimized World app

19:01 -Recommendations

Analyzing the performance of your visionOS app

HD VideoSD Video

HD Video

SD Video

Analyze hangs with Instruments

Get started with building apps for spatial computing

Meet Reality Composer Pro

Optimize app power and performance for spatial computing

Discover Metal debugging, profiling, and asset creation tools

Demystify and eliminate hitches in the render phase

Search this video…♪ Mellow instrumental hip-hop ♪♪Sarina Wu: Hello! My name is Sarina,and I'm a Software Engineer on the RealityKit Tools team.Harjas Monga: And I am Harjas, a Profiling Tools Engineer.Sarina: Today, Harjas and I will be introducingthe RealityKit Trace template in Instruments.We'll show you how this template can help you optimizethe performance of your spatial experiences.Performance is essential to the user experiencein spatial computing.To learn how to optimize spatial experiences,we will briefly cover how rendering workson this platform,show you how to profile using the RealityKit Trace templatein Instruments,and briefly cover the other great toolsavailable to optimize your content.This platform has unique performance constraints.To understand them, you first need to understandhow rendering works.Rendering includes your app process,the render server,and the compositor.How your app interacts with these componentswill depend on the types of experiences you create.Let's take a look at the types of experiences you can createfor your spatial apps and how they are rendered.Apps on the platform can enter either the Shared Spaceor a Full Space.These have different performance implicationsto consider based on how they render.When multiple apps run side by side,they are all rendered in the same space,which is one of the reasons we call it the Shared Space.This means that the performance of your appcan be affected by the work that the render server is doingto render the other apps.Then the render server works with the compositorto generate the final frames.When your app enters a Full Space,all other visible apps are hidden.This means that the performance of your appis no longer affected by the rendering workfor the apps that are now hidden.To learn more about how to enter a Full Space,check out the session "Go beyond the window with SwiftUI."Based on what was just covered, we have two recommended waysto profile your app.Whenever you are investigating performance issuesor analyzing system power impact,you should profile your app in isolationto understand your app's impact on system performance.When you expect your app to work alongside other apps,you should profile your app with those other apps.This is important to understandhow the user would experience your app.Let's profile a spatial app to show youhow you can optimize your app's performance in isolationusing the RealityKit Trace template.We've been working on Hello World,and we want to make sure that there are no performance issues.Sarina: This is the Start screen of the app,which is a SwiftUI View.This view has an Objects in Orbit button.We can tap on that button to learn moreabout objects orbiting Earth.The button opens a new view that lists examplesof different objects that are orbiting Earth.This view has 3D models of these objects,including a satellite,the Moon,and a telescope.In this view, there is also a View Orbits button.We can explore this by tapping the button,which will open an immersive experience showing Earthand a satellite orbiting around it.We've used detailed assets for these models,and I suspect that they're affecting the performanceof this app.In the immersive experience,we can see the path of the satellite animateas it orbits Earth.We can even scale the Earth up to see it in more detail.This interaction is incredibly choppy,so I think there's a performance issue here.Harjas and I profiled that experienceusing the RealityKit Trace template.Harjas, could you walk us through it?Harjas: Of course, let's walk throughall the features available in RealityKit Trace.RealityKit Trace is available as a new templatein Instruments 15.It can be used to profile both a real device and a simulator.To get the most accurate and actionable information,you should profile a real device.When profiling against the simulatornot all the timing information will be accuratebecause of the hardware and software differencesbetween your Mac and on-device.But you could still use it for quick iterationand improving some of the statisticsthat are not based on time.The RealityKit Trace template contains several instruments.The first instrument you will want to look atis the RealityKit Frames instrument.This instrument tracks each framebeing rendered by the device.You can zoom in on these frames to checkhow long each frame took to render.With this, you can check how long each stageof the frame took to render.This gives you a high-level idea of what portionof the render pipeline could be causing performance problems.In order to achieve a smooth user experience,your application should be able to achieve 90 frames per second.However, the OS may not always be targeting 90 fps.It will render at the frame rate most appropriate for the contentbeing displayed and the environment the device is in.Because the frame rate can change,every frame has a deadline in which it hasto complete rendering so that the devicecan hit whatever the current target frame rate is.The frames are classified into three groups:frames that are completing well within their deadline,frames that are just barely finishing within their deadline,and frames than run past their deadlineand result in frame drops.These classifications are color codedgreen, orange, and red, respectively.Frames that are running past the deadlinewill negatively impact the user experience.If you zoom out and check the frames from a high level,the color coding allows you to quickly findthe problematic parts of the trace.So, you can narrow down any performance investigationsto the areas where there are the most frame drops.In addition to the individual frames,the instrument also visualizes the average amount of timethe system spent on CPU or GPU work to render each frame.The next instrument you will want to checkis the RealityKit Metrics instrument.At a top level, the instrument drawsall the bottlenecks that it detected.These bottlenecks are generated by looking atcomprehensive timing information from the entire render pipeline.Prioritize the bottlenecks that occur during the same timethat frames exceed their deadline.In the detail view below, you will findthat these RealityKit bottlenecksare summarized by severity and type.You can dig in further to see exactly what kind of bottleneckthe instrument found and how much it affectedyour overall performance.In the extended detail view, the instrument providesrecommendations on how to diagnosethese bottlenecks further and what steps you can taketo mitigate them.By expanding the RealityKit Metrics track,you will be presented with several types of metricsfrom different components of the render pipeline.These statistics can help you understandthe full complexity of the scene your app is presenting.Some of the key metrics will have associated thresholdsto help inform you on reasonable expectations for those metrics.Use the metrics to help guide you furtherin diagnosing a bottleneck or why a frameis not hitting its deadline.RealityKit Metrics will visualizehow much time is being spent in each frameto run the application's RealityKit systems.This includes all the built-in systemsand all the custom systems your application may implement.This information is best combined with Time Profilerso you can optimize your RealityKit system code.Lastly, review the System Power Impact laneshown in RealityKit Metrics to understandthe power envelope your application needsto work within to provide a greatand consistent user experience.Now let's take a look at some traces we tookwhile we were stepping through the world experience.The first scene in the app was the Start screen,which is implemented in SwiftUI.In the frames instrument, there are quite a fewdropped frames throughout this trace.These dropped frames may not seem significant,but they can really damage the user experience.I can use Option-drag to zoom inon one of the more problematic areas.And by adjusting the time range, I can check what bottlenecksthe RealityKit Metrics instrument foundduring these long-running frames.The Instrument found that the largest bottleneckin this time was Core Animation Encoding.So I'm going to check the Core Animation statistics,which can be found by clicking on the disclosure trianglenext to RealityKit Metrics Instrumentand selecting the track labeled Core Animation.These Core Animation metrics can help inform uson what might have caused these frame drops.When investigating these metrics,you will notice that some of themhave context of how severe the metric is.In the timeline, this is reflected in the color coding.This is to guide you on what are reasonable thresholdsfor these key metrics.Based on the timeline visualization,it is clear that the applicationis exceeding the recommended thresholdfor the number of offscreen prepares.The summary at the bottom shows that the average numberof offscreen prepares here are 180,which is quite a high average.When considering the Core Animation statistics,there are three types of work you want to keep in mind.Firstly, transparency and blur effectsare very expensive operations for the system.Use these effects when they delivermost impact to the user, otherwise use them sparingly.The number of render passes is determined by how many layersCore Animation has to render individuallyfor the entire image.And finally, there are offscreen passes.So as the name implies, an offscreen passis a render pass that is rendered offscreenand not to the display.An offscreen pass requires the rendering passto pause what it's currently doingand do some work that won't be shown to the user.However, the output of the offscreen passis needed to continue the regular rendering pass.Offscreen passes are particularly impactfulfor spatial apps.Unlike other app platforms, this platform continuouslyrenders spatial apps because every single frameneeds to account for environment factors,such as the user's head movements.Therefore, your static UI needs to be efficient enoughthat it can be rendered at the system's target frame rate.There are four main types of workthat can cause an offscreen pass:shadows,masking,rounded rectangles,and visual effects.To learn more about offscreen passes,watch our tech talkon "Demystify and eliminate hitches in the render phase."Since there were a lot of offscreen passes,I am going to check the SwiftUI code for this viewto find what could have caused them.In the SwiftUI code, this view is not doingany masking or visual effects.But there are instances of shadows being applied.For example, in the SwiftUI View item,shadows are being applied to several buttons.Shadows are a particularly expensive operation,especially when combined with transparency.While shadows are a useful UI idiom,for spatial apps, you should use themwhen they deliver a significant effect to the user.I'm going to disable these shadowsand take look at a new trace.With the shadows disabled,in the RealityKit Frames Instrument,there are few frame issues and RealityKit Metrics reportsthat offscreen passes have reduced by four times.Now, the next scene that we saw in the World appwas the objects in orbit view.I am going to open up a trace from that sceneto see if there is anything that can be optimized.In the Frames Instrument,there are a scattering of dropped framesthroughout the trace with lots of bottlenecks.The detail view for RealityKit Metricsprovides a summary of those bottlenecks.In the summary, most of these bottlenecksare related to GPU Work Stalls.Because the bottleneck type reported most frequentlyare GPU stalls,I am again going to expand RealityKit Metrics.But this time, I'll investigate using the 3D Render track.I'm going to select the area of the tracethat has a high number of frame drops.In this time selection,the 3D Render metrics reports that the triangleand vertex counts are far exceedingthe recommended thresholds.Next, I am going to highlight the area of the tracewhere there aren't nearly as many frame drops.And according to the rendering metrics,the triangle and vertex countsare within the recommended thresholds.This means you should really be evaluating the numberand quality of the assets the app is using in the scene.When optimizing asset rendering, first check the triangles,vertices, and draw calls from the 3D Rendering groupin RealityKit Metrics.To optimize these metrics, use simple shape mesheswhen possible.Take advantage of instancing when utilizing assetswith the same mesh.Check the complexity of assets using the statisticsin Reality Composer Pro, which is a new developer toolthat allows you to assemble, edit, and preview 3D content.That content could later be accessed through codedirectly in your Xcode project.To learn more about this tool and how to create great assets,check out the session "Meet Reality Composer Pro."I went ahead and swapped out the assets I was usingwith those that used fewer polygonsand captured a new trace.In this trace, the Frames Instrument reportsthat all the frames are hitting their deadlines.And if I check the 3D rendering statistics again,it reports that the triangle and vertex countsare reduced substantially.While these assets did use fewer polygons,there was no loss in quality of the experience.The next trace is for when we were interactingwith the Earth model.During this scene, resizing the globewas actually quite jittery.RealityKit Metrics reports that the System Power Impact lanewas very high for a substantial amount of time.This is indicating that some part of your applicationis being very inefficient and the user experiencecould be impacted.You should target for your application to work wellwhile keeping the device's system power impactin the nominal state for as much time as possible.When profiling to reduce system power impact,always profile with your application in isolationto ensure you get the most actionable information.You can lower the system power impact using several approaches.First, make sure that the statisticsfrom RealityKit Metrics are within expectations.If these are exceeding expectations,the device could be operating at higher power statesfor long stretches of time to deliver a smooth experience.Next, check what work the CPU and GPU are doing.For the CPU, check if Time Profiler reports high CPU usageduring your high-power draw regions.And if it does, optimize your CPU-bound codeusing Time Profiler.For the GPU, we have performance states.When the GPU is in the maximum stage,it draws a considerable amount of power.In that case, we should use the Metal System Trace templatein Instruments to see what work is being done on the GPU.That way, we can understand what could be optimized.Going back to the trace,Time Profiler tells us that the CPU usagewas averaging 100 percent in this region,and the GPU performance stateswere minimum during most of this time.Using Time Profiler, I can check what caused the high CPU usage.The heaviest stack trace is in the extended detail view.This is a very useful feature of the Time Profiler,as it allows you to quickly find the most expensive partsof your code in the call tree.Looking at these frames,it appears that Entity.makeModel is using a lot of CPU time.The next frame down is calling Entity.generateCollisionShapes.Therefore, the performance issue appears to being causedby constantly generating models and collision shapes,which is an expensive operation.I'm going to open Xcode to see what I can do about this.This is the Entity.makeModel function callthat the call tree showed was taking a lot of CPU time.This is getting invoked within the makeGlobe function.I can Control-click on the makeGlobe functionto see who is invoking it.It's getting invoked from the Orbit SwiftUI view body.This is antipattern that should be avoidedbecause the view body needs to be computed very quickly.You should avoid doing model loadingor any other expensive operationsin the body of your SwiftUI viewsbecause any time the state of the view changes,all those expensive operations need to be recomputed.So, what I am going to do is remove this callfrom the view body.Next, in the ViewModel, I will add a reusable versionof the Earth entity.And finally, I am going to use that reusable Earth entityin the Orbit View.Now, when the view body is recomputed,the app is not wasting time reloading the same model.Looking at the trace after our fix,the power impact is brought back down to the nominal state.And Time Profiler reports that the CPU usagehas dropped from 100 percent to 10 percent.After all these optimizations,there are few reported bottlenecks,almost every frame is hitting its deadline,and power is within expectations.Now, the World app is a well-optimized appfor this platform.Now that we've reduced the number of offscreen passes,replaced the high-polygon assets with reasonable ones,and lowered CPU and power usage,we're going to step through the optimized version of this app.The start screen looks great, and since the shadowsweren't adding much to the user experience,this was a good optimization.Next, let's open up the Objects in Orbit.These models look great, even though we are using assetswith fewer polygons.So that extra detail was just wasting resources.And finally, we're going to open up the Earth model againand try resizing.Now this interaction is as smooth as butter.That was a brief overview of how to use RealityKit Traceto optimize your apps for this new platform.Hey, Sarina, what other tools are available for developers?Sarina: There are several tools availableto help you optimize your apps for spatial computing.For optimizing SwiftUI content,there are domain-specific instrumentsin the Instruments app for analyzing SwiftUI,Core Animation, and hangs.You can learn more about the Hangs instrument in the session"Analyze hangs with Instruments."There are also several tools availableto optimize your 3D asset-based content.The Time Profiler Instrument can help you find areaswhere your app is taking the most time,such as when a large amount of time is spent loading assets.The RealityKit Metrics Instrumentcan help you diagnose when scenes have too many assetsor assets that are too complex.Finally, you can also check the complexity of your assetsas you're assembling a scene using Reality Composer Pro.To learn more about Reality Composer Pro,watch the session "Meet Reality Composer Pro."If you are using Metal in your app,the most useful tool will be the Metal System Trace templatein Instruments.This template has key metrics, such as the GPU timeline,GPU counters, and GPU performance state.To learn more about this template and other toolsfor profiling Metal content, check out the session"Discover Metal debugging, profiling,and asset creation tools."To recap, performance is essential for this platform.Apps need to be well optimizedto deliver the best possible user experience.You can use the RealityKit Trace templateto find performance bottlenecks in your app.Profiling proactively with other instrumentsand checking your content in Reality Composer Procan also help you find and resolve performance issues.To learn more about how to use the RealityKit Trace templateto optimize your apps,please check out the developer documentation.And to get a better understanding of performancefor this platform, watch the session"Optimize app power and performancefor spatial computing."Enjoy optimizing your spatial computing apps,whatever the trace may be.Harjas: Thank you for watching.♪

♪ Mellow instrumental hip-hop ♪♪Sarina Wu: Hello! My name is Sarina,and I'm a Software Engineer on the RealityKit Tools team.Harjas Monga: And I am Harjas, a Profiling Tools Engineer.Sarina: Today, Harjas and I will be introducingthe RealityKit Trace template in Instruments.We'll show you how this template can help you optimizethe performance of your spatial experiences.Performance is essential to the user experiencein spatial computing.To learn how to optimize spatial experiences,we will briefly cover how rendering workson this platform,show you how to profile using the RealityKit Trace templatein Instruments,and briefly cover the other great toolsavailable to optimize your content.This platform has unique performance constraints.To understand them, you first need to understandhow rendering works.Rendering includes your app process,the render server,and the compositor.How your app interacts with these componentswill depend on the types of experiences you create.Let's take a look at the types of experiences you can createfor your spatial apps and how they are rendered.Apps on the platform can enter either the Shared Spaceor a Full Space.These have different performance implicationsto consider based on how they render.When multiple apps run side by side,they are all rendered in the same space,which is one of the reasons we call it the Shared Space.This means that the performance of your appcan be affected by the work that the render server is doingto render the other apps.Then the render server works with the compositorto generate the final frames.When your app enters a Full Space,all other visible apps are hidden.This means that the performance of your appis no longer affected by the rendering workfor the apps that are now hidden.To learn more about how to enter a Full Space,check out the session "Go beyond the window with SwiftUI."Based on what was just covered, we have two recommended waysto profile your app.Whenever you are investigating performance issuesor analyzing system power impact,you should profile your app in isolationto understand your app's impact on system performance.When you expect your app to work alongside other apps,you should profile your app with those other apps.This is important to understandhow the user would experience your app.Let's profile a spatial app to show youhow you can optimize your app's performance in isolationusing the RealityKit Trace template.We've been working on Hello World,and we want to make sure that there are no performance issues.Sarina: This is the Start screen of the app,which is a SwiftUI View.This view has an Objects in Orbit button.We can tap on that button to learn moreabout objects orbiting Earth.The button opens a new view that lists examplesof different objects that are orbiting Earth.This view has 3D models of these objects,including a satellite,the Moon,and a telescope.In this view, there is also a View Orbits button.We can explore this by tapping the button,which will open an immersive experience showing Earthand a satellite orbiting around it.We've used detailed assets for these models,and I suspect that they're affecting the performanceof this app.In the immersive experience,we can see the path of the satellite animateas it orbits Earth.We can even scale the Earth up to see it in more detail.This interaction is incredibly choppy,so I think there's a performance issue here.Harjas and I profiled that experienceusing the RealityKit Trace template.Harjas, could you walk us through it?Harjas: Of course, let's walk throughall the features available in RealityKit Trace.RealityKit Trace is available as a new templatein Instruments 15.It can be used to profile both a real device and a simulator.To get the most accurate and actionable information,you should profile a real device.When profiling against the simulatornot all the timing information will be accuratebecause of the hardware and software differencesbetween your Mac and on-device.But you could still use it for quick iterationand improving some of the statisticsthat are not based on time.The RealityKit Trace template contains several instruments.The first instrument you will want to look atis the RealityKit Frames instrument.This instrument tracks each framebeing rendered by the device.You can zoom in on these frames to checkhow long each frame took to render.With this, you can check how long each stageof the frame took to render.This gives you a high-level idea of what portionof the render pipeline could be causing performance problems.In order to achieve a smooth user experience,your application should be able to achieve 90 frames per second.However, the OS may not always be targeting 90 fps.It will render at the frame rate most appropriate for the contentbeing displayed and the environment the device is in.Because the frame rate can change,every frame has a deadline in which it hasto complete rendering so that the devicecan hit whatever the current target frame rate is.The frames are classified into three groups:frames that are completing well within their deadline,frames that are just barely finishing within their deadline,and frames than run past their deadlineand result in frame drops.These classifications are color codedgreen, orange, and red, respectively.Frames that are running past the deadlinewill negatively impact the user experience.If you zoom out and check the frames from a high level,the color coding allows you to quickly findthe problematic parts of the trace.So, you can narrow down any performance investigationsto the areas where there are the most frame drops.In addition to the individual frames,the instrument also visualizes the average amount of timethe system spent on CPU or GPU work to render each frame.The next instrument you will want to checkis the RealityKit Metrics instrument.At a top level, the instrument drawsall the bottlenecks that it detected.These bottlenecks are generated by looking atcomprehensive timing information from the entire render pipeline.Prioritize the bottlenecks that occur during the same timethat frames exceed their deadline.In the detail view below, you will findthat these RealityKit bottlenecksare summarized by severity and type.You can dig in further to see exactly what kind of bottleneckthe instrument found and how much it affectedyour overall performance.In the extended detail view, the instrument providesrecommendations on how to diagnosethese bottlenecks further and what steps you can taketo mitigate them.By expanding the RealityKit Metrics track,you will be presented with several types of metricsfrom different components of the render pipeline.These statistics can help you understandthe full complexity of the scene your app is presenting.Some of the key metrics will have associated thresholdsto help inform you on reasonable expectations for those metrics.Use the metrics to help guide you furtherin diagnosing a bottleneck or why a frameis not hitting its deadline.RealityKit Metrics will visualizehow much time is being spent in each frameto run the application's RealityKit systems.This includes all the built-in systemsand all the custom systems your application may implement.This information is best combined with Time Profilerso you can optimize your RealityKit system code.Lastly, review the System Power Impact laneshown in RealityKit Metrics to understandthe power envelope your application needsto work within to provide a greatand consistent user experience.Now let's take a look at some traces we tookwhile we were stepping through the world experience.The first scene in the app was the Start screen,which is implemented in SwiftUI.In the frames instrument, there are quite a fewdropped frames throughout this trace.These dropped frames may not seem significant,but they can really damage the user experience.I can use Option-drag to zoom inon one of the more problematic areas.And by adjusting the time range, I can check what bottlenecksthe RealityKit Metrics instrument foundduring these long-running frames.The Instrument found that the largest bottleneckin this time was Core Animation Encoding.So I'm going to check the Core Animation statistics,which can be found by clicking on the disclosure trianglenext to RealityKit Metrics Instrumentand selecting the track labeled Core Animation.These Core Animation metrics can help inform uson what might have caused these frame drops.When investigating these metrics,you will notice that some of themhave context of how severe the metric is.In the timeline, this is reflected in the color coding.This is to guide you on what are reasonable thresholdsfor these key metrics.Based on the timeline visualization,it is clear that the applicationis exceeding the recommended thresholdfor the number of offscreen prepares.The summary at the bottom shows that the average numberof offscreen prepares here are 180,which is quite a high average.When considering the Core Animation statistics,there are three types of work you want to keep in mind.Firstly, transparency and blur effectsare very expensive operations for the system.Use these effects when they delivermost impact to the user, otherwise use them sparingly.The number of render passes is determined by how many layersCore Animation has to render individuallyfor the entire image.And finally, there are offscreen passes.So as the name implies, an offscreen passis a render pass that is rendered offscreenand not to the display.An offscreen pass requires the rendering passto pause what it's currently doingand do some work that won't be shown to the user.However, the output of the offscreen passis needed to continue the regular rendering pass.Offscreen passes are particularly impactfulfor spatial apps.Unlike other app platforms, this platform continuouslyrenders spatial apps because every single frameneeds to account for environment factors,such as the user's head movements.Therefore, your static UI needs to be efficient enoughthat it can be rendered at the system's target frame rate.There are four main types of workthat can cause an offscreen pass:shadows,masking,rounded rectangles,and visual effects.To learn more about offscreen passes,watch our tech talkon "Demystify and eliminate hitches in the render phase."Since there were a lot of offscreen passes,I am going to check the SwiftUI code for this viewto find what could have caused them.In the SwiftUI code, this view is not doingany masking or visual effects.But there are instances of shadows being applied.For example, in the SwiftUI View item,shadows are being applied to several buttons.Shadows are a particularly expensive operation,especially when combined with transparency.While shadows are a useful UI idiom,for spatial apps, you should use themwhen they deliver a significant effect to the user.I'm going to disable these shadowsand take look at a new trace.With the shadows disabled,in the RealityKit Frames Instrument,there are few frame issues and RealityKit Metrics reportsthat offscreen passes have reduced by four times.Now, the next scene that we saw in the World appwas the objects in orbit view.I am going to open up a trace from that sceneto see if there is anything that can be optimized.In the Frames Instrument,there are a scattering of dropped framesthroughout the trace with lots of bottlenecks.The detail view for RealityKit Metricsprovides a summary of those bottlenecks.

In the summary, most of these bottlenecksare related to GPU Work Stalls.Because the bottleneck type reported most frequentlyare GPU stalls,I am again going to expand RealityKit Metrics.But this time, I'll investigate using the 3D Render track.

I'm going to select the area of the tracethat has a high number of frame drops.In this time selection,the 3D Render metrics reports that the triangleand vertex counts are far exceedingthe recommended thresholds.Next, I am going to highlight the area of the tracewhere there aren't nearly as many frame drops.

And according to the rendering metrics,the triangle and vertex countsare within the recommended thresholds.This means you should really be evaluating the numberand quality of the assets the app is using in the scene.When optimizing asset rendering, first check the triangles,vertices, and draw calls from the 3D Rendering groupin RealityKit Metrics.To optimize these metrics, use simple shape mesheswhen possible.Take advantage of instancing when utilizing assetswith the same mesh.Check the complexity of assets using the statisticsin Reality Composer Pro, which is a new developer toolthat allows you to assemble, edit, and preview 3D content.That content could later be accessed through codedirectly in your Xcode project.To learn more about this tool and how to create great assets,check out the session "Meet Reality Composer Pro."I went ahead and swapped out the assets I was usingwith those that used fewer polygonsand captured a new trace.In this trace, the Frames Instrument reportsthat all the frames are hitting their deadlines.And if I check the 3D rendering statistics again,it reports that the triangle and vertex countsare reduced substantially.While these assets did use fewer polygons,there was no loss in quality of the experience.The next trace is for when we were interactingwith the Earth model.During this scene, resizing the globewas actually quite jittery.RealityKit Metrics reports that the System Power Impact lanewas very high for a substantial amount of time.This is indicating that some part of your applicationis being very inefficient and the user experiencecould be impacted.You should target for your application to work wellwhile keeping the device's system power impactin the nominal state for as much time as possible.When profiling to reduce system power impact,always profile with your application in isolationto ensure you get the most actionable information.You can lower the system power impact using several approaches.First, make sure that the statisticsfrom RealityKit Metrics are within expectations.If these are exceeding expectations,the device could be operating at higher power statesfor long stretches of time to deliver a smooth experience.Next, check what work the CPU and GPU are doing.For the CPU, check if Time Profiler reports high CPU usageduring your high-power draw regions.And if it does, optimize your CPU-bound codeusing Time Profiler.For the GPU, we have performance states.When the GPU is in the maximum stage,it draws a considerable amount of power.In that case, we should use the Metal System Trace templatein Instruments to see what work is being done on the GPU.That way, we can understand what could be optimized.Going back to the trace,Time Profiler tells us that the CPU usagewas averaging 100 percent in this region,and the GPU performance stateswere minimum during most of this time.Using Time Profiler, I can check what caused the high CPU usage.The heaviest stack trace is in the extended detail view.This is a very useful feature of the Time Profiler,as it allows you to quickly find the most expensive partsof your code in the call tree.Looking at these frames,it appears that Entity.makeModel is using a lot of CPU time.The next frame down is calling Entity.generateCollisionShapes.Therefore, the performance issue appears to being causedby constantly generating models and collision shapes,which is an expensive operation.I'm going to open Xcode to see what I can do about this.This is the Entity.makeModel function callthat the call tree showed was taking a lot of CPU time.This is getting invoked within the makeGlobe function.I can Control-click on the makeGlobe functionto see who is invoking it.It's getting invoked from the Orbit SwiftUI view body.This is antipattern that should be avoidedbecause the view body needs to be computed very quickly.You should avoid doing model loadingor any other expensive operationsin the body of your SwiftUI viewsbecause any time the state of the view changes,all those expensive operations need to be recomputed.So, what I am going to do is remove this callfrom the view body.Next, in the ViewModel, I will add a reusable versionof the Earth entity.And finally, I am going to use that reusable Earth entityin the Orbit View.Now, when the view body is recomputed,the app is not wasting time reloading the same model.Looking at the trace after our fix,the power impact is brought back down to the nominal state.And Time Profiler reports that the CPU usagehas dropped from 100 percent to 10 percent.After all these optimizations,there are few reported bottlenecks,almost every frame is hitting its deadline,and power is within expectations.Now, the World app is a well-optimized appfor this platform.Now that we've reduced the number of offscreen passes,replaced the high-polygon assets with reasonable ones,and lowered CPU and power usage,we're going to step through the optimized version of this app.The start screen looks great, and since the shadowsweren't adding much to the user experience,this was a good optimization.Next, let's open up the Objects in Orbit.These models look great, even though we are using assetswith fewer polygons.So that extra detail was just wasting resources.And finally, we're going to open up the Earth model againand try resizing.

Now this interaction is as smooth as butter.That was a brief overview of how to use RealityKit Traceto optimize your apps for this new platform.Hey, Sarina, what other tools are available for developers?Sarina: There are several tools availableto help you optimize your apps for spatial computing.For optimizing SwiftUI content,there are domain-specific instrumentsin the Instruments app for analyzing SwiftUI,Core Animation, and hangs.You can learn more about the Hangs instrument in the session"Analyze hangs with Instruments."There are also several tools availableto optimize your 3D asset-based content.The Time Profiler Instrument can help you find areaswhere your app is taking the most time,such as when a large amount of time is spent loading assets.The RealityKit Metrics Instrumentcan help you diagnose when scenes have too many assetsor assets that are too complex.Finally, you can also check the complexity of your assetsas you're assembling a scene using Reality Composer Pro.To learn more about Reality Composer Pro,watch the session "Meet Reality Composer Pro."If you are using Metal in your app,the most useful tool will be the Metal System Trace templatein Instruments.This template has key metrics, such as the GPU timeline,GPU counters, and GPU performance state.To learn more about this template and other toolsfor profiling Metal content, check out the session"Discover Metal debugging, profiling,and asset creation tools."To recap, performance is essential for this platform.Apps need to be well optimizedto deliver the best possible user experience.You can use the RealityKit Trace templateto find performance bottlenecks in your app.Profiling proactively with other instrumentsand checking your content in Reality Composer Procan also help you find and resolve performance issues.To learn more about how to use the RealityKit Trace templateto optimize your apps,please check out the developer documentation.And to get a better understanding of performancefor this platform, watch the session"Optimize app power and performancefor spatial computing."Enjoy optimizing your spatial computing apps,whatever the trace may be.Harjas: Thank you for watching.♪

10:50 -SwiftUI View with High Offscreens

16:33 -EarthEntity Factory

16:53 -Orbit SwiftUI View Body

17:26 -SwiftUI ViewModel

17:33 -SwiftUI Orbit View Body

## Code Samples

```swift
private
 
struct
 
Item
: 
View
 {
    
var
 module: 
Module


    
// The corner radius of the item's hightlight when selected or hovering.

    
let
 cornerRadius 
=
 
20.0


    
var
 body: 
some
 
View
 {
        
NavigationLink
(value: module) {
            
VStack
(alignment: .leading, spacing: 
3
) {
                
Text
(module.eyebrow)
                    .font(.titleHeading)
                    .foregroundStyle(.secondary)
                
VStack
(alignment: .leading, spacing: 
7
) {
                    
Text
(module.heading)
                        .font(.largeTitle)
                    
Text
(module.abstract)
                }
            }
            .padding(.horizontal, 
5
)
            .padding(.vertical, 
20
)
        }
        .buttonStyle(.bordered)
        .shadow(radius: 
10
)
        .buttonBorderShape(.roundedRectangle(radius: cornerRadius))
        .frame(minWidth: 
150
, maxWidth: 
280
)
    }
}
```

```swift
class
 
EarthEntity
: 
Entity
 {
    
static
 
func
 
makeGlobe
() -> 
EarthEntity
 {
        
EarthEntity
(earthModel: 
Entity
.makeModel(
            name: 
"Earth"
,
            filename: 
"Globe"
,
            radius: 
0.35
,
            color: .blue)
        )
    }

    
static
 
func
 
makeCloudyEarth
() -> 
EarthEntity
 {
        
let
 earthModel 
=
 
Entity
()
        earthModel.name 
=
 
"Earth"


        
Task
 {
            
if
 
let
 scene 
=
 
await
 loadFromRealityComposerPro(
                named: 
WorldAssets
.rootNodeName,
                fromSceneNamed: 
WorldAssets
.sceneName
            ) {
                earthModel.addChild(scene)
            } 
else
 {
                
fatalError
(
"Unable to load earth model"
)
            }
        }

        
return
 
EarthEntity
(earthModel: earthModel)
    }
}
```

```swift
struct
 
Orbit
: 
View
 {
    
@EnvironmentObject
 
private
 
var
 model: 
ViewModel


    
var
 body: 
some
 
View
 {
        
Earth
(
            world: 
EarthEntity
.makeGlobe(),
            earthConfiguration: model.orbitEarth,
            satelliteConfiguration: [model.orbitSatellite],
            moonConfiguration: model.orbitMoon,

            showSun: 
true
,
            sunAngle: model.orbitSunAngle,

            animateUpdates: 
true

        )
        .place(
            initialPosition: 
Point3D
([
475
, 
-
1200.0
, 
-
1200.0
]),
            useCustomGesture: model.useCustomGesture,
            handOffset: model.customGestureHandOffset,
            isCustomGestureAnimated: model.isCustomGestureAnimated,
            debugCustomGesture: model.debugCustomGesture,
            scale: 
$model
.orbitEarth.scale)
    }
}
```

```swift
class
 
ViewModel
: 
ObservableObject
 {
    
// MARK: - Navigation

    
@Published
 
var
 navigationPath: [
Module
] 
=
 []
    
@Published
 
var
 titleText: 
String
 
=
 
""

    
@Published
 
var
 isTitleFinished: 
Bool
 
=
 
false

    
var
 finalTitle: 
String
 
=
 
"Hello World"


    
// MARK: - Globe

    
@Published
 
var
 globeEarthEntity: 
EarthEntity
 
=
 .makeGlobe()

    
@Published
 
var
 isShowingGlobe: 
Bool
 
=
 
false

    
@Published
 
var
 globeEarth: 
EarthEntity
.
Configuration
 
=
 .globeEarthDefault
    
@Published
 
var
 globeEarthOffset: 
SIMD3
<
Double
> 
=
 [
0
, 
0
, 
0
]
    
@Published
 
var
 globePanelOffset: 
SIMD3
<
Double
> 
=
 [
0
, 
-
50
, 
30
]

    
@Published
 
var
 showSatelliteButton: 
Bool
 
=
 
false

    
@Published
 
var
 isShowingSatellite: 
Bool
 
=
 
false


    
// MARK: - Orbit

    
@Published
 
var
 orbitEarthEntity: 
EarthEntity
 
=
 .makeGlobe()

    
@Published
 
var
 useCustomGesture: 
Bool
 
=
 
true

    
@Published
 
var
 customGestureHandOffset: 
SIMD3
<
Float
> 
=
 [
0
, 
0.21
, 
-
0.07
]
    
@Published
 
var
 isCustomGestureAnimated: 
Bool
 
=
 
false

    
@Published
 
var
 debugCustomGesture: 
Bool
 
=
 
false


    
@Published
 
var
 orbitSatelliteScale: 
Float
 
=
 
0.9

    
@Published
 
var
 orbitMoonScale: 
Float
 
=
 
0.9

    
@Published
 
var
 orbitTelescopeScale: 
Float
 
=
 
0.8

    
@Published
 
var
 orbitSatelliteZOffset: 
Double
 
=
 
100

    
@Published
 
var
 orbitMoonZOffset: 
Double
 
=
 
100

    
@Published
 
var
 orbitTelescopeZOffset: 
Double
 
=
 
100


    
@Published
 
var
 isShowingOrbit: 
Bool
 
=
 
false

    
@Published
 
var
 orbitImmersionStyle: 
ImmersionStyle
 
=
 .mixed
    
@Published
 
var
 orbitEarth: 
EarthEntity
.
Configuration
 
=
 .orbitEarthDefault
    
@Published
 
var
 orbitSatellite: 
SatelliteEntity
.
Configuration
 
=
 .orbitSatelliteDefault
    
@Published
 
var
 orbitMoon: 
SatelliteEntity
.
Configuration
 
=
 .orbitMoonDefault

    
@Published
 
var
 orbitSunAngle: 
Angle
 
=
 .degrees(
150
)
    
var
 orbitSunAngleBinding: 
Binding
<
Float
> {
        
Binding
<
Float
>(
            get: { 
Float
(
self
.orbitSunAngle.degrees) },
            set: { 
self
.orbitSunAngle 
=
 .degrees(
Double
(
$0
)) }
        )
    }

    
// MARK: - Solar System

    
@Published
 
var
 solarEarthEntity: 
EarthEntity
 
=
 .makeCloudyEarth()

    
@Published
 
var
 isShowingSolar: 
Bool
 
=
 
false

    
@Published
 
var
 solarImmersionStyle: 
ImmersionStyle
 
=
 .full
    
@Published
 
var
 solarEarth: 
EarthEntity
.
Configuration
 
=
 .solarEarthDefault
    
@Published
 
var
 solarSatellite: 
SatelliteEntity
.
Configuration
 
=
 .solarTelescopeDefault
    
@Published
 
var
 solarMoon: 
SatelliteEntity
.
Configuration
 
=
 .solarMoonDefault

    
@Published
 
var
 solarSunDistance: 
Double
 
=
 
700

    
@Published
 
var
 solarSunAngle: 
Angle
 
=
 .degrees(
280
)
    
@Published
 
var
 solarSunSpotIntensity: 
Float
 
=
 
10.5

    
@Published
 
var
 solarSunEmissionIntensity: 
Float
 
=
 
10.5

    
var
 solarSunPosition: 
SIMD3
<
Float
> {
        [
Float
(solarSunDistance 
*
 sin(solarSunAngle.radians)),
         
0
,
         
Float
(solarSunDistance 
*
 cos(solarSunAngle.radians))]
    }
}
```

```swift
struct
 
Orbit
: 
View
 {
    
@EnvironmentObject
 
private
 
var
 model: 
ViewModel


    
var
 body: 
some
 
View
 {
        
Earth
(
            world: model.globeEarthEntity,
            earthConfiguration: model.orbitEarth,
            satelliteConfiguration: [model.orbitSatellite],
            moonConfiguration: model.orbitMoon,

            showSun: 
true
,
            sunAngle: model.orbitSunAngle,

            animateUpdates: 
true

        )
        .place(
            initialPosition: 
Point3D
([
475
, 
-
1200.0
, 
-
1200.0
]),
            useCustomGesture: model.useCustomGesture,
            handOffset: model.customGestureHandOffset,
            isCustomGestureAnimated: model.isCustomGestureAnimated,
            debugCustomGesture: model.debugCustomGesture,
            scale: 
$model
.orbitEarth.scale)
    }
}
```

