WEBVTT

00:00:07.516 --> 00:00:18.500
[ Music ]

00:00:25.516 --> 00:00:31.546
[ Applause ]

00:00:32.046 --> 00:00:32.716
>> So, hello everyone.

00:00:33.026 --> 00:00:35.416
My name is Fiona and this
is my colleague Alex.

00:00:36.146 --> 00:00:40.776
And I work on the iOS GPU
complier team and our job is

00:00:40.776 --> 00:00:43.356
to make your shaders run
on the latest iOS devices,

00:00:43.436 --> 00:00:46.176
and to make them run as
efficiently as possible.

00:00:46.636 --> 00:00:49.056
And I'm here to talk
about our presentations,

00:00:49.166 --> 00:00:52.136
Advanced Metal Shader
Optimization, that is Forging

00:00:52.136 --> 00:00:53.666
and Polishing your
Metal shaders.

00:00:54.006 --> 00:00:56.286
Our compiler is based on LVM.

00:00:56.906 --> 00:00:58.546
And we work with the
Open Source committee

00:00:58.546 --> 00:01:01.636
to make LVM more suitable
for use on GPUs by everyone.

00:01:03.806 --> 00:01:06.006
Here's a quick overview of
the other Metal session,

00:01:06.006 --> 00:01:06.856
in case you missed them,

00:01:06.856 --> 00:01:08.966
and don't worry you can
watch the recordings online.

00:01:09.416 --> 00:01:11.736
Yesterday we had part one
and two of adopting Metal

00:01:12.326 --> 00:01:14.556
and earlier today we had part
one and two of what's new

00:01:14.616 --> 00:01:16.516
in Metal, because there's quite
a lot that's new in Metal.

00:01:17.326 --> 00:01:18.946
And of course here's
the last one,

00:01:19.586 --> 00:01:20.786
the one you're watching
right now.

00:01:22.206 --> 00:01:24.646
So in this presentation we're
going to be going over a number

00:01:24.646 --> 00:01:26.956
of things you can do to
work with the compiler

00:01:27.266 --> 00:01:28.696
to make your code faster.

00:01:29.566 --> 00:01:32.606
And some of this stuff is
going to be specific to A8

00:01:32.606 --> 00:01:35.046
and later GPUs including
some information

00:01:35.046 --> 00:01:36.636
that has never been
made public before.

00:01:36.816 --> 00:01:39.176
And some of it will
also be more general.

00:01:39.466 --> 00:01:42.306
And we'll be noting that with
the A8 icon you can see there

00:01:42.506 --> 00:01:44.996
for slides that are
more A8 specific.

00:01:46.056 --> 00:01:50.006
And additionally, we'll be
noting some potential pitfalls.

00:01:50.496 --> 00:01:53.036
That is things that may not
come up as often as the kind

00:01:53.036 --> 00:01:55.466
of micro optimizations
you're used to looking for,

00:01:55.826 --> 00:01:58.026
but if you run into these,
you're likely to lose

00:01:58.026 --> 00:02:01.486
so much performance, nothing is
going to matter by comparison.

00:02:01.776 --> 00:02:04.226
So it's always worth making
sure you don't run into those.

00:02:04.566 --> 00:02:05.386
And those will be marked

00:02:05.386 --> 00:02:07.036
with the triangle icon,
as you can see there.

00:02:08.205 --> 00:02:10.586
Before we go on, this
is not the first step.

00:02:10.996 --> 00:02:12.136
This is the last step.

00:02:12.886 --> 00:02:15.446
There's no point to doing
low-level shader optimization

00:02:15.446 --> 00:02:17.756
until you've done the
high-level optimizations before,

00:02:17.896 --> 00:02:19.466
like watching the
other Metal talks

00:02:19.466 --> 00:02:21.766
from optimizing your
draw calls, the structure

00:02:21.836 --> 00:02:22.976
of your engine and so forth.

00:02:23.656 --> 00:02:26.226
Optimizing your later shader
should be roughly the last thing

00:02:26.226 --> 00:02:28.036
you do.

00:02:28.036 --> 00:02:30.356
And, this presentation
is primarily

00:02:30.436 --> 00:02:31.936
for experienced shader authors.

00:02:32.506 --> 00:02:34.766
Perhaps you've worked on Metal
a whole lot and you're looking

00:02:34.766 --> 00:02:37.686
to get more into optimizing your
shaders, or perhaps your new

00:02:37.686 --> 00:02:40.286
to Metal, but you've done a
lot of shader optimization

00:02:40.286 --> 00:02:42.346
on other platforms and
you'd like to know how

00:02:42.346 --> 00:02:44.276
to optimize better
for A8 and later GPUs,

00:02:44.276 --> 00:02:46.606
this is the presentation
for you.

00:02:48.396 --> 00:02:50.886
So you may have seen this
pipeline if you watched any

00:02:50.886 --> 00:02:51.916
of the previous Metal talks.

00:02:52.426 --> 00:02:54.436
And we will be focusing
of course

00:02:54.496 --> 00:02:56.456
on the programmable
stages of this pipeline,

00:02:56.456 --> 00:02:58.316
as you can see there,
the shader course.

00:02:59.336 --> 00:03:00.596
So first, Alex is going to go

00:03:00.596 --> 00:03:02.506
over some shader
performance fundamentals

00:03:02.506 --> 00:03:03.536
and higher level issues.

00:03:03.916 --> 00:03:06.846
After which, I'll return
for some low-level,

00:03:06.846 --> 00:03:09.096
down and dirty shader
optimizations.

00:03:12.516 --> 00:03:17.546
[ Applause ]

00:03:18.046 --> 00:03:18.456
>> Thanks, Fiona.

00:03:18.456 --> 00:03:19.956
Let me start by explaining
the idea

00:03:19.956 --> 00:03:21.256
of shader performance
fundamentals.

00:03:21.776 --> 00:03:22.986
These are the things that
you want to make sure

00:03:22.986 --> 00:03:24.616
that you have right
before you start digging

00:03:24.616 --> 00:03:26.116
into source level optimizations.

00:03:26.546 --> 00:03:27.776
Usually the impact of the kind

00:03:27.776 --> 00:03:29.716
of changes you'll
make here can dwarf

00:03:29.716 --> 00:03:32.176
or potentially hide other
more targeted changes

00:03:32.176 --> 00:03:32.966
that you make elsewhere.

00:03:33.336 --> 00:03:34.936
So I'm going to talk
about four of these today.

00:03:34.936 --> 00:03:36.716
Address space selection
for buffer arguments,

00:03:37.226 --> 00:03:39.056
buffer preloading, dealing

00:03:39.056 --> 00:03:40.436
with fragment function
resource writes,

00:03:40.436 --> 00:03:42.276
and how to optimize
your computer kernels.

00:03:42.276 --> 00:03:45.676
So, let's start with
addresses spaces.

00:03:46.246 --> 00:03:48.886
So since this functionality
doesn't exist

00:03:48.886 --> 00:03:50.596
in all shading languages,
I'll give a quick primer.

00:03:50.866 --> 00:03:53.816
So, GPUs have multiple paths
for getting date from memory.

00:03:53.816 --> 00:03:56.936
And these paths are optimized
for different use cases,

00:03:57.086 --> 00:03:59.846
and they have different
performance characteristics.

00:04:00.706 --> 00:04:03.806
In Metal, we expose control
over which path is used

00:04:03.956 --> 00:04:06.926
to the developer by requiring
that they qualify all buffers,

00:04:07.356 --> 00:04:09.386
arguments and pointers
in the shading language

00:04:09.386 --> 00:04:11.096
with which address
space they want to use.

00:04:11.766 --> 00:04:14.726
So a couple of the address
spaces specifically apply

00:04:14.726 --> 00:04:16.786
to getting information
from memory.

00:04:17.055 --> 00:04:20.416
The first of which is
the device address space.

00:04:21.086 --> 00:04:23.296
This is an address space with
relatively few restrictions.

00:04:23.296 --> 00:04:25.016
You can read and write data
through this address space,

00:04:25.016 --> 00:04:28.076
you can pass as much data as
you want, and the buffer offsets

00:04:28.076 --> 00:04:30.756
that you specify at the API
level have relatively flexible

00:04:30.756 --> 00:04:31.486
alignment requirements.

00:04:32.156 --> 00:04:35.866
On the other end of things, you
have the constant address space.

00:04:36.086 --> 00:04:38.246
As the name implies, this is
a read only address space,

00:04:38.246 --> 00:04:39.856
but there are a couple of
additional restrictions.

00:04:40.316 --> 00:04:42.136
There are limits on how
much data you can pass

00:04:42.256 --> 00:04:44.426
through this address space, and
additionally the buffer offsets

00:04:44.426 --> 00:04:46.966
that you specify at the API
level have more stringent

00:04:46.966 --> 00:04:47.716
alignment requirements.

00:04:48.296 --> 00:04:51.016
However, this is the address
space that's optimized for cases

00:04:51.016 --> 00:04:52.036
with a lot of data reuse.

00:04:52.036 --> 00:04:53.016
So you want to take advantage

00:04:53.016 --> 00:04:54.766
of this address space
whenever it makes sense.

00:04:55.786 --> 00:04:58.336
Figuring out whether or not the
constant address space makes

00:04:58.336 --> 00:05:00.116
sense for your buffer
argument is typically a matter

00:05:00.116 --> 00:05:01.846
of asking yourself
two questions.

00:05:02.636 --> 00:05:05.176
The first question is, do I
know how much data I have.

00:05:05.176 --> 00:05:07.786
And if you have a potentially
variable amount of data,

00:05:07.786 --> 00:05:08.946
this is usually a
sign that you need

00:05:08.946 --> 00:05:10.716
to be using the device
address space.

00:05:11.466 --> 00:05:14.656
Additionally, you want to
look at how much each item

00:05:15.056 --> 00:05:16.526
in your buffer is being read.

00:05:16.576 --> 00:05:20.606
And if these items can
potentially be read many times,

00:05:21.046 --> 00:05:22.756
this is usually a sign
that you want to put them

00:05:22.756 --> 00:05:23.866
into the constant address space.

00:05:24.536 --> 00:05:27.016
So let's put this into practice
with a couple of examples

00:05:27.016 --> 00:05:28.726
from some vertex shaders.

00:05:29.496 --> 00:05:31.886
First, you have regular,
old vertex data.

00:05:32.396 --> 00:05:36.376
So as you can see, each vertex
has its own piece of data.

00:05:36.706 --> 00:05:39.046
And each vertex is the only one
that reads that piece of data.

00:05:39.046 --> 00:05:40.536
So there's essentially
no reuse here.

00:05:40.536 --> 00:05:42.646
This is the kind of thing
that really needs to be

00:05:42.646 --> 00:05:45.396
in the device address space.

00:05:46.816 --> 00:05:51.626
Next, you have projection
matrices, another matrices.

00:05:51.926 --> 00:05:55.426
Now, typically what you have
here is that you have one

00:05:55.426 --> 00:05:59.926
of these objects, and they're
read by every single vertex.

00:06:00.686 --> 00:06:03.206
So with this kind of complete
data reuse, you really want this

00:06:03.206 --> 00:06:04.636
to be in the constant
address space.

00:06:04.826 --> 00:06:09.036
Let's mix things up a little bit

00:06:09.036 --> 00:06:11.276
and take a look at
standing matrices.

00:06:11.276 --> 00:06:14.816
So hopefully in this case
you have some maximum number

00:06:14.816 --> 00:06:15.836
of bones that you're handling.

00:06:16.706 --> 00:06:20.186
But if you look at each
bone that matrix may be read

00:06:20.186 --> 00:06:21.956
by every vertex that
references that bone,

00:06:21.956 --> 00:06:25.046
and that also is a potential
for a large amount of reuse.

00:06:25.046 --> 00:06:26.086
And so this really ought to be

00:06:26.086 --> 00:06:29.116
on the constant address
space as well.

00:06:29.316 --> 00:06:31.176
Finally, let's look
at per instance data.

00:06:31.756 --> 00:06:35.266
As you can see all vertices

00:06:35.266 --> 00:06:37.756
in the instance will read
this particular piece of data,

00:06:38.396 --> 00:06:40.916
but on the other hand you have
a potentially variable number

00:06:40.916 --> 00:06:42.396
of instances, so this
actually needs to be

00:06:42.396 --> 00:06:44.226
in the device address
space as well.

00:06:45.616 --> 00:06:48.356
For an example of why address
space selection matters

00:06:48.356 --> 00:06:49.406
for performance, let's move

00:06:49.406 --> 00:06:51.206
on to our next topic,
buffer preloading.

00:06:52.746 --> 00:06:55.226
So Fiona will spend some
time talking about how

00:06:55.226 --> 00:06:57.396
to actually optimize loads and
stores within your shaders,

00:06:57.396 --> 00:06:59.856
but for many cases the best
thing that you can do is

00:06:59.856 --> 00:07:02.186
to actually off load this
work to dedicated hardware.

00:07:02.676 --> 00:07:04.906
So we can do this
for you in two cases,

00:07:05.456 --> 00:07:06.856
context buffers and
vertex buffers.

00:07:07.616 --> 00:07:11.086
But this relies on knowing
things about the access patterns

00:07:11.086 --> 00:07:13.936
in your shaders and what address
space you've placed them into.

00:07:14.866 --> 00:07:17.056
So let's start with
constant buffer preloading.

00:07:17.426 --> 00:07:20.136
So the idea here is
that rather than loading

00:07:20.136 --> 00:07:21.156
through the constant
address space,

00:07:21.156 --> 00:07:23.016
what we can actually do is
take your data and put it

00:07:23.016 --> 00:07:25.456
into special constant
registers that are even faster

00:07:25.456 --> 00:07:26.536
for the ALU to access.

00:07:27.286 --> 00:07:28.116
So we can do this as long

00:07:28.116 --> 00:07:29.826
as we know exactly
what data will be read.

00:07:30.726 --> 00:07:32.776
If your offsets are
known a compile time,

00:07:32.776 --> 00:07:33.786
this is straightforward.

00:07:34.036 --> 00:07:34.936
But if your offsets aren't known

00:07:34.936 --> 00:07:37.976
until run time then we need a
little bit of extra information

00:07:37.976 --> 00:07:39.416
about how much data
that you're reading.

00:07:39.986 --> 00:07:42.346
So indicating this

00:07:42.346 --> 00:07:44.876
to the compiler is usually
a matter of two steps.

00:07:45.016 --> 00:07:46.476
First, you need to make
sure that this data is

00:07:46.476 --> 00:07:47.676
in the constant address space.

00:07:48.696 --> 00:07:50.256
And additionally
you need to indicate

00:07:50.256 --> 00:07:51.696
that your accesses are
statically bounded.

00:07:53.296 --> 00:07:55.436
The best way to do this
is to pass your arguments

00:07:55.696 --> 00:07:57.726
by reference rather than
pointer where possible.

00:07:58.056 --> 00:08:00.676
If you're passing only a
single item or a single struct,

00:08:00.676 --> 00:08:02.396
this is straightforward, you
can just change your pointers

00:08:02.396 --> 00:08:05.106
to references and change
your accesses accordingly.

00:08:05.296 --> 00:08:07.976
This is a little different
if you're passing an array

00:08:07.976 --> 00:08:09.006
that you know is bounded.

00:08:09.666 --> 00:08:12.656
So what you do in this case is
you can embed that size array

00:08:12.656 --> 00:08:14.946
and pass that struct
by reference rather

00:08:14.946 --> 00:08:16.176
than passing the
original pointer.

00:08:16.936 --> 00:08:18.966
So we can put this into
practice with an example

00:08:19.536 --> 00:08:21.426
at a forward lighting
fragment shader.

00:08:21.776 --> 00:08:23.176
So as you can see in sort

00:08:23.176 --> 00:08:26.246
of the original version what we
have are a bunch of arguments

00:08:26.246 --> 00:08:27.756
that are passed as
regular device pointers.

00:08:27.756 --> 00:08:29.806
And this doesn't expose the
information that we want.

00:08:30.616 --> 00:08:31.586
So we can do better than this.

00:08:32.306 --> 00:08:33.676
Instead if we note the number

00:08:33.676 --> 00:08:36.946
of lights is bonded what we can
do is we can put the light data

00:08:36.946 --> 00:08:39.056
and the count together into
a single struct like this.

00:08:40.356 --> 00:08:43.116
And we can pass that struct
in the constant address space

00:08:43.116 --> 00:08:43.986
as a reference like this.

00:08:44.616 --> 00:08:46.526
And so that gets us
constant buffer preloading.

00:08:48.366 --> 00:08:49.346
Let's look at another example

00:08:49.346 --> 00:08:51.086
of how this can affect
you in practice.

00:08:52.156 --> 00:08:55.206
So, there are many ways to
implement a deferred render,

00:08:55.206 --> 00:08:57.786
but what we find is that the
actually implementation choices

00:08:57.786 --> 00:09:00.036
that you make can have a big
impact on the performance

00:09:00.036 --> 00:09:01.236
that you achieve in practice.

00:09:01.696 --> 00:09:05.496
One pattern that's common
now is to use a single shader

00:09:05.496 --> 00:09:07.456
to accumulate the
results of all lights.

00:09:08.456 --> 00:09:10.776
And what you can see form the
declaration of this function,

00:09:10.776 --> 00:09:13.436
is that it can potentially read
any or all lights in the scene

00:09:13.436 --> 00:09:15.306
and that means that your
input size is unbounded.

00:09:15.306 --> 00:09:20.026
Now, on the other hand if you're
able to structure your rendering

00:09:20.026 --> 00:09:21.246
such that each light is handled

00:09:21.246 --> 00:09:24.006
in its own draw call
then what happens is

00:09:24.006 --> 00:09:27.206
that each light only needs
to read that light's data

00:09:27.206 --> 00:09:29.506
and it's shader and that
means that you can pass it

00:09:29.536 --> 00:09:31.666
in the constant address space

00:09:32.026 --> 00:09:33.476
and take advantage
of buffer preloading.

00:09:34.336 --> 00:09:36.546
In practice we see
that on A8 later GPUs

00:09:36.546 --> 00:09:38.176
that this is a significant
performance win.

00:09:38.176 --> 00:09:42.416
Now let's talk about
vertex buffer preloading.

00:09:42.776 --> 00:09:44.386
The idea of vertex
buffer preloading is

00:09:44.386 --> 00:09:46.656
to reuse the same dedicated
hardware that we would use

00:09:46.656 --> 00:09:47.936
for a fix function
vertex fetching.

00:09:48.226 --> 00:09:51.666
And we can do this for regular
buffer loads as long as the way

00:09:51.666 --> 00:09:53.216
that you access your
buffer looks just

00:09:53.216 --> 00:09:54.456
like fix function
vertex fetching.

00:09:54.876 --> 00:09:55.896
So what that means
is that you need

00:09:55.896 --> 00:09:58.976
to be indexing using the
vertex or instance ID.

00:09:59.046 --> 00:10:01.316
Now we can handle a couple
additional modifications

00:10:01.316 --> 00:10:04.216
to the vertex or instance IDs
such as applying a deviser

00:10:04.216 --> 00:10:06.916
and that's with or
without any base vertex

00:10:06.916 --> 00:10:09.216
or instance offsets you might
have applied at the API level.

00:10:09.876 --> 00:10:12.206
Of course the easiest way to
take advantage of this is just

00:10:12.206 --> 00:10:15.116
to use the Metal vertex
descriptor functionality

00:10:15.116 --> 00:10:15.926
wherever possible.

00:10:15.926 --> 00:10:18.156
But if you are writing
your own indexing code,

00:10:18.826 --> 00:10:20.496
we strongly suggest that
you layout your data

00:10:20.496 --> 00:10:23.126
so that vertexes fetch linearly
to simplify buffer indexing.

00:10:23.366 --> 00:10:26.196
Note that this doesn't preclude
you from doing fancier things,

00:10:26.196 --> 00:10:28.996
like if you were rendering quads
and you want to pass one value

00:10:29.446 --> 00:10:33.256
to all vertices in the quad,
you can still do things

00:10:33.256 --> 00:10:35.526
like indexing by vertex
ID divided by four

00:10:35.526 --> 00:10:36.926
because this just
looks like a divider.

00:10:38.106 --> 00:10:42.086
So now let's move on to a couple
shader stage specific concerns.

00:10:42.686 --> 00:10:47.016
In iOS 10 we introduced the
ability to do resource writes

00:10:47.016 --> 00:10:48.436
from within your
fragment functions.

00:10:48.626 --> 00:10:50.186
And this has interesting
implications

00:10:50.186 --> 00:10:51.156
for hidden surface removal.

00:10:52.246 --> 00:10:54.446
So prior to this you might have
been accustomed to the behavior

00:10:54.446 --> 00:10:57.386
that a fragment wouldn't
need to be shaded as long

00:10:57.386 --> 00:10:59.286
as an opaque fragment
came in and occluded it.

00:10:59.956 --> 00:11:02.176
So this is no longer
true specifically

00:11:02.176 --> 00:11:03.926
if your fragment function
is doing resource writes,

00:11:03.926 --> 00:11:05.656
because those resource
writes still need to happen.

00:11:06.836 --> 00:11:09.046
So instead your behavior
really only depends

00:11:09.346 --> 00:11:10.346
on what's come before.

00:11:10.346 --> 00:11:12.956
And specifically what
happens depends on whether

00:11:12.956 --> 00:11:14.776
or not you've enabled
early fragment tests

00:11:14.776 --> 00:11:15.686
on your fragment function.

00:11:16.116 --> 00:11:19.236
If you have enabled
early fragment tests,

00:11:19.916 --> 00:11:21.296
once it's rasterized as long

00:11:21.296 --> 00:11:23.666
as it also passes the early
depth and stencil tests.

00:11:24.066 --> 00:11:26.366
If you haven't specified
early fragment tests,

00:11:26.496 --> 00:11:28.146
then your fragment
will be shaded

00:11:28.146 --> 00:11:29.306
as long as it's rasterized.

00:11:30.136 --> 00:11:32.276
So from a perspective of
minimizing your shading,

00:11:32.276 --> 00:11:34.376
what you want to do is
use early fragment tests

00:11:34.376 --> 00:11:35.116
wherever possible.

00:11:35.116 --> 00:11:36.466
But there are a couple
additional things

00:11:36.466 --> 00:11:38.536
that you can do to improve
the rejection that you get.

00:11:39.676 --> 00:11:41.796
And most of these boil
down to draw order.

00:11:41.836 --> 00:11:44.406
You want to draw these objects,

00:11:44.466 --> 00:11:47.046
the objects where the fragment
functions do resource writes

00:11:47.376 --> 00:11:48.516
after opaque objects.

00:11:48.716 --> 00:11:50.586
And if you're using these
objects to update your depth

00:11:50.586 --> 00:11:52.536
and stencil buffers,
we strongly suggest

00:11:52.536 --> 00:11:54.766
that you sort these
buffer from front to back.

00:11:55.646 --> 00:11:57.816
Note that this guidance
should sound fairly familiar

00:11:57.816 --> 00:11:59.506
if you've been dealing
with fragment functions

00:11:59.506 --> 00:12:01.676
that do discard or modify
your depth per pixel.

00:12:01.676 --> 00:12:05.536
Now let's talk about
compute kernels.

00:12:05.736 --> 00:12:08.546
Since the defining
characters of a compute kernels

00:12:08.546 --> 00:12:10.366
that you can structure your
computation however you want.

00:12:11.036 --> 00:12:14.736
Let's talk about what factors
influence how you do this

00:12:14.796 --> 00:12:16.386
on iOS.

00:12:17.076 --> 00:12:19.226
First we have computer
thread launch overhead.

00:12:20.576 --> 00:12:24.856
So on A8 and later GPUs
there's a certain amount of time

00:12:24.856 --> 00:12:27.486
that it takes to launch a
group of compute threads.

00:12:27.586 --> 00:12:28.836
So if you don't do enough work

00:12:28.836 --> 00:12:31.376
from within a single compute
thread you can potentially,

00:12:31.376 --> 00:12:33.036
it leaves the hardware
underutilized

00:12:33.036 --> 00:12:34.266
and leave performance
on the table.

00:12:36.046 --> 00:12:38.716
And a good way to deal with
this and actually a good pattern

00:12:38.716 --> 00:12:41.116
for writing computer
kernels on iOS in general is

00:12:41.116 --> 00:12:43.466
to actually process multiple
conceptual work items

00:12:43.466 --> 00:12:44.666
in a single compute threat.

00:12:45.166 --> 00:12:48.276
And in particular a pattern
that we find works well is

00:12:48.276 --> 00:12:50.416
to reuse values not
by passing them

00:12:50.416 --> 00:12:53.586
through thread group memory, but
rather by reusing values loaded

00:12:53.586 --> 00:12:57.106
for one work item when you're
processing the next work item

00:12:57.296 --> 00:12:58.426
in the same compute thread.

00:12:58.426 --> 00:13:00.916
And it's best to illustrate
this with an example.

00:13:01.696 --> 00:13:04.226
So this is a syllable
filter kernel, this is sort

00:13:04.226 --> 00:13:06.226
of the most straightforward
version of it, as you see,

00:13:06.226 --> 00:13:07.966
it reads as a three-
[inaudible] region of its source

00:13:08.366 --> 00:13:10.576
and produces one output pixel.

00:13:11.146 --> 00:13:15.696
So if instead we
apply the pattern

00:13:15.696 --> 00:13:17.416
of processing multiple
work items

00:13:17.876 --> 00:13:18.876
in a single compute thread,

00:13:18.876 --> 00:13:20.476
we get something
that looks like this.

00:13:21.086 --> 00:13:24.536
Notice now that we're striding
by two pixels at a time.

00:13:24.916 --> 00:13:27.126
So processing the first pixel
looks much as it did before.

00:13:27.126 --> 00:13:28.556
We read the 3 by 3 region.

00:13:29.046 --> 00:13:31.246
We apply the filter and
we write up the value.

00:13:31.786 --> 00:13:34.716
But now let's look at
how pixel 2 is handled.

00:13:35.006 --> 00:13:38.516
So stents are striding by
two pixels at a time we need

00:13:38.516 --> 00:13:40.426
to make sure that there is
a second pixel to process.

00:13:41.486 --> 00:13:42.546
And now we read its data.

00:13:43.386 --> 00:13:45.536
Note here that a 2 by 3 region

00:13:45.536 --> 00:13:47.576
of what this pixel
wants was already loaded

00:13:47.576 --> 00:13:48.416
by the previous pixel.

00:13:48.416 --> 00:13:49.376
So we don't need
to load it again,

00:13:49.376 --> 00:13:50.776
we can reuse those old values.

00:13:51.016 --> 00:13:52.396
All we need to load now is the 1

00:13:52.396 --> 00:13:54.646
by 3 region that's
new to this pixel.

00:13:55.786 --> 00:13:58.166
After which, we can apply
the filter and we're done.

00:13:58.916 --> 00:14:02.496
Note that as a result we're
not doing 12 texture reads,

00:14:02.496 --> 00:14:05.276
instead of the old 9, but
we're producing 2 pixels.

00:14:05.276 --> 00:14:07.816
So this is a significant
reduction in the amount

00:14:07.816 --> 00:14:09.126
of texture reads per pixel.

00:14:09.866 --> 00:14:13.466
Of course this pattern doesn't
work for all compute use cases.

00:14:14.026 --> 00:14:16.086
Sometimes you do still
need to pass data

00:14:16.086 --> 00:14:16.856
through thread group memory.

00:14:17.456 --> 00:14:20.396
And in that case, when you're
synchronizing between threads

00:14:20.396 --> 00:14:24.286
in a thread group, an important
thing to keep in mind is

00:14:24.286 --> 00:14:26.906
that you want to use the barrier
with the smallest possible scope

00:14:26.906 --> 00:14:28.476
for the threads that
you need to synchronize.

00:14:29.326 --> 00:14:33.436
In particular, if your thread
group fits within a single SIMD,

00:14:34.116 --> 00:14:35.966
the regular thread
group barrier function

00:14:35.966 --> 00:14:37.156
in Metal is unnecessary.

00:14:37.626 --> 00:14:41.346
What you can use instead is the
new SIMD group barrier function

00:14:41.346 --> 00:14:42.536
introduced in iOS 10.

00:14:43.476 --> 00:14:47.076
And what we find is actually
the targeting your thread group

00:14:47.076 --> 00:14:48.336
to fit within a single SIMD

00:14:48.336 --> 00:14:52.426
and using SIMD group barrier
is often faster than trying

00:14:52.426 --> 00:14:54.406
to use a larger thread
group in order to squeeze

00:14:54.406 --> 00:14:55.266
that additional reuse,

00:14:55.266 --> 00:14:57.146
but having to use thread
group barrier as a result.

00:14:57.706 --> 00:15:01.136
So that wraps things up
for me, in conclusion,

00:15:01.946 --> 00:15:04.596
make sure you're using the
appropriate address space

00:15:04.596 --> 00:15:06.106
for each of your buffer
arguments according

00:15:06.106 --> 00:15:07.386
to the guidelines
that we described.

00:15:08.546 --> 00:15:09.916
Structure your data
and rendering

00:15:09.916 --> 00:15:11.696
to take maximal advantage
of constant

00:15:11.696 --> 00:15:12.996
and vertex buffer preloading.

00:15:14.526 --> 00:15:16.426
Make sure you're using early
fragment tests to reject

00:15:16.426 --> 00:15:18.576
as many fragments as possible

00:15:18.646 --> 00:15:19.796
when you're doing
resource writes.

00:15:20.336 --> 00:15:23.096
Put enough work in
each compute thread

00:15:23.096 --> 00:15:24.386
so you're not being limited

00:15:24.386 --> 00:15:26.506
by your compute thread
launch overhead.

00:15:27.056 --> 00:15:29.336
And use the smallest barrier
for the job when you need

00:15:29.336 --> 00:15:31.126
to synchronize between
threads in a thread group.

00:15:31.446 --> 00:15:33.926
And with that I'd like to pass
it back to Fiona to dive deeper

00:15:33.926 --> 00:15:34.796
into tuning shader code.

00:15:35.516 --> 00:15:40.956
[ Applause ]

00:15:41.456 --> 00:15:42.006
>> Thank you, Alex.

00:15:43.306 --> 00:15:46.476
So, before jumping into the
specifics here, I want to go

00:15:46.476 --> 00:15:48.686
over some general
characteristics of GPUs

00:15:48.686 --> 00:15:50.846
and the bottlenecks
you can encounter.

00:15:50.846 --> 00:15:52.336
And all of you may be
familiar with this,

00:15:52.536 --> 00:15:54.146
but I figure I should
just do a quick review.

00:15:54.836 --> 00:15:58.596
So with GPUs typically you
have a set of resources.

00:15:58.696 --> 00:16:01.116
And it's fairly common for
a shader to be bottlenecked

00:16:01.116 --> 00:16:02.336
by one of those resources.

00:16:02.596 --> 00:16:04.186
And so for example if
you're bottlenecked

00:16:04.186 --> 00:16:06.406
by memory bandwidth,
improving other things

00:16:06.406 --> 00:16:08.836
in your shader will often
not give any apparent

00:16:08.886 --> 00:16:09.966
performance improvement.

00:16:10.616 --> 00:16:13.036
And while it is important to
identify these bottlenecks

00:16:13.036 --> 00:16:15.846
and focus on them to
improve performance,

00:16:16.356 --> 00:16:18.796
there is actually still
benefit to improving things

00:16:18.796 --> 00:16:19.826
that aren't bottlenecks.

00:16:19.826 --> 00:16:22.336
For example, in that example
if you are bottlenecked

00:16:22.336 --> 00:16:25.356
at memory usage, but then
you improve your arithmetic

00:16:25.356 --> 00:16:28.866
to be more efficient, you
will still save power even

00:16:28.866 --> 00:16:30.566
if you are not improving
your frame rate.

00:16:30.726 --> 00:16:32.046
And of course being on mobile,

00:16:32.346 --> 00:16:34.016
saving power is always
important.

00:16:34.286 --> 00:16:35.996
So it's not something to ignore,

00:16:36.106 --> 00:16:38.536
just because your frame rate
doesn't go up in that case.

00:16:38.696 --> 00:16:41.256
So there's four typical
bottlenecks to keep

00:16:41.256 --> 00:16:43.506
in mind in shaders here.

00:16:43.656 --> 00:16:45.596
The first is fairly
straightforward, ALU bandwidth.

00:16:45.846 --> 00:16:47.696
The amount of math
that the GPU can do.

00:16:48.626 --> 00:16:50.936
The second is memory bandwidth,
again, fairly straightforward,

00:16:50.966 --> 00:16:53.746
the amount of data that the GPU
can load from system memory.

00:16:54.116 --> 00:16:55.846
The other two are
little more subtle.

00:16:55.846 --> 00:16:57.716
The first one is
memory issue rate.

00:16:58.076 --> 00:17:00.466
Which represents the
number of memory operations

00:17:00.466 --> 00:17:01.486
that can be performed.

00:17:01.946 --> 00:17:04.026
And this can come up in the case

00:17:04.066 --> 00:17:06.086
where you have smaller
memory operations,

00:17:06.086 --> 00:17:08.346
or you're using a lot of thread
group memory and so forth.

00:17:09.096 --> 00:17:11.236
And the last one, which I'll
go into detail a bit more

00:17:11.236 --> 00:17:13.816
about later is latency
occupancy register usage.

00:17:13.816 --> 00:17:15.146
You may have heard about that,

00:17:15.226 --> 00:17:17.236
but I will save that
until the end.

00:17:18.616 --> 00:17:20.656
So to try to alleviate
some of these bottlenecks,

00:17:20.656 --> 00:17:23.116
and improve overall shader
performance and efficiency,

00:17:23.455 --> 00:17:25.136
we're going to look
at four categories

00:17:25.165 --> 00:17:26.766
of optimization opportunity
here.

00:17:27.665 --> 00:17:28.996
And the first one is data types.

00:17:29.076 --> 00:17:31.376
And the first thing to consider

00:17:31.376 --> 00:17:34.366
when optimizing your shader
is choosing your data types.

00:17:34.846 --> 00:17:36.726
And the most important
thing to remember

00:17:36.726 --> 00:17:38.726
when you're choosing
data types is that A8

00:17:38.726 --> 00:17:42.206
and later GPUs have
16-bit register units,

00:17:42.856 --> 00:17:45.846
which means that for example if
you're using a 32-bit data type,

00:17:45.916 --> 00:17:49.226
that's twice the register
space, twice the bandwidth,

00:17:49.756 --> 00:17:51.916
potentially twice the
power and so-forth,

00:17:52.316 --> 00:17:53.826
it's just twice as much stuff.

00:17:54.626 --> 00:17:57.116
So, accordingly you
will save registers,

00:17:57.116 --> 00:18:00.026
you will get faster performance,
you'll get lower power

00:18:00.266 --> 00:18:01.706
by using smaller data types.

00:18:02.006 --> 00:18:04.496
Use half and short for
arithmetic wherever you can.

00:18:05.276 --> 00:18:07.226
Energy wise, half is
cheaper than float.

00:18:07.766 --> 00:18:09.526
And float is cheaper
than integer,

00:18:09.886 --> 00:18:12.676
but even among integers,
smaller integers are cheaper

00:18:12.676 --> 00:18:13.296
than bigger ones.

00:18:13.866 --> 00:18:17.136
And the most effective thing
you can do to save registers is

00:18:17.136 --> 00:18:20.816
to use half for texture reads
and interpolates because most

00:18:20.816 --> 00:18:23.036
of the time you really do
not need float for these.

00:18:23.446 --> 00:18:26.166
And note I do not mean
your texture formats.

00:18:26.226 --> 00:18:29.006
I mean the data types you're
using to store the results

00:18:29.006 --> 00:18:30.806
of a texture sample
or an interpolate.

00:18:32.116 --> 00:18:36.656
And one aspect of A8 in later
GPUs that is fairly convenient

00:18:37.146 --> 00:18:39.346
and makes using smaller
data types easier

00:18:39.346 --> 00:18:40.656
than on some other GPUs is

00:18:40.986 --> 00:18:43.986
that data type conversions
are typically free,

00:18:44.166 --> 00:18:48.226
even between float and half,
which means that you don't have

00:18:48.256 --> 00:18:51.336
to worry, oh am I introducing
too many conversions in this

00:18:51.336 --> 00:18:53.006
by trying to use half here?

00:18:53.266 --> 00:18:54.496
Is this going to cost too much?

00:18:54.496 --> 00:18:55.286
Is it worth it or not?

00:18:55.746 --> 00:18:58.266
No it's probably fast because
the conversions are free,

00:18:58.626 --> 00:19:00.576
so you can use half wherever
you want and not worry

00:19:00.576 --> 00:19:01.626
about that part of it.

00:19:01.766 --> 00:19:03.526
The one thing to keep
in mind here though is

00:19:03.586 --> 00:19:05.356
that half-precision numerics

00:19:05.356 --> 00:19:08.106
and limitations are
different from float.

00:19:08.996 --> 00:19:11.496
And a common bug
that can come up here

00:19:11.496 --> 00:19:16.386
for example is people will
write 65,535 as a half,

00:19:17.506 --> 00:19:19.766
but that is actually infinity.

00:19:20.236 --> 00:19:22.666
Because that's bigger
than the maximum half.

00:19:22.666 --> 00:19:25.116
And so by being aware of
what these limitations are,

00:19:25.116 --> 00:19:27.146
you'll better be able to
know where you perhaps should

00:19:27.146 --> 00:19:28.486
and shouldn't use half.

00:19:28.486 --> 00:19:31.116
And less likely to encounter
unexpected bugs in your shaders.

00:19:32.076 --> 00:19:34.216
So one example application

00:19:34.216 --> 00:19:38.766
for using smaller integer
data types is thread IDs.

00:19:39.356 --> 00:19:41.976
And as those of you who worked
on computer kernels will know,

00:19:41.976 --> 00:19:44.506
thread IDs are used
all over your programs.

00:19:45.076 --> 00:19:47.916
And so making them smaller
can significantly increase the

00:19:47.966 --> 00:19:51.796
performance of arithmetic, and
can save registers and so forth.

00:19:52.696 --> 00:19:57.076
And so local thread IDs, there's
no reason to ever use uint

00:19:57.176 --> 00:19:58.616
for them as in this case,

00:19:58.936 --> 00:20:01.436
because local thread IDs can't
have that many thread IDs.

00:20:02.076 --> 00:20:04.836
For global thread IDs, usually
you can get away with a ushort

00:20:05.316 --> 00:20:06.946
because most of the
time you don't have

00:20:06.946 --> 00:20:08.316
that many global tread IDs.

00:20:08.316 --> 00:20:09.646
Of course it depends
on your program.

00:20:10.056 --> 00:20:13.786
But in most cases, you won't
go over 2 to the 16 minus 1,

00:20:14.276 --> 00:20:15.526
so it is said you can do this.

00:20:16.186 --> 00:20:19.426
And this is going to be lower
power, it's going to be faster

00:20:19.426 --> 00:20:22.326
because all of the arithmetic
involving your thread ID is now

00:20:22.326 --> 00:20:23.296
going to be faster.

00:20:23.576 --> 00:20:28.076
So I highly recommend
this wherever possible.

00:20:28.636 --> 00:20:31.696
Additionally, keep in mind
that in C like languages,

00:20:31.696 --> 00:20:33.856
which of course includes
Metal, the precision

00:20:33.856 --> 00:20:37.126
of an operation is defined by
the larger of the input types.

00:20:37.886 --> 00:20:39.946
For example, if you're
multiplying a float by a half,

00:20:40.276 --> 00:20:43.566
that's a float operation not a
half operation, it's promoted.

00:20:44.156 --> 00:20:47.456
So accordingly, make sure
not to use float literals

00:20:47.456 --> 00:20:51.386
when not necessary, because
that will turn here what appears

00:20:51.436 --> 00:20:53.786
to be a half operation, it
takes a half and returns a half,

00:20:54.256 --> 00:20:55.546
into a float operation.

00:20:55.586 --> 00:20:57.126
Because by the language
semantics,

00:20:57.126 --> 00:20:59.886
that's actually a float
operation since at least one

00:20:59.886 --> 00:21:01.056
of the inputs is float.

00:21:01.856 --> 00:21:03.776
And so you probably
want to do this.

00:21:04.636 --> 00:21:06.516
This will actually
be a half operation.

00:21:06.516 --> 00:21:08.116
This will actually be faster.

00:21:08.696 --> 00:21:10.206
This is probably what you mean.

00:21:10.556 --> 00:21:11.586
So be careful not

00:21:11.586 --> 00:21:14.066
to inadvertently introduce
float precision arithmetic

00:21:14.066 --> 00:21:19.216
into your code when
that's not what you meant.

00:21:19.216 --> 00:21:21.836
And while I did mention that
smaller data types are better,

00:21:21.836 --> 00:21:24.556
there's one exception to
this rule and that is char.

00:21:25.156 --> 00:21:27.606
Remember as I said that
native data type size on A8

00:21:27.606 --> 00:21:30.686
and later GPUs is
16-bit, not 8-bit.

00:21:31.466 --> 00:21:34.806
And so char is not going to
save you any space or power

00:21:34.806 --> 00:21:35.606
or anything like that

00:21:35.606 --> 00:21:38.126
and furthermore there's no
native 8-bit arithmetic.

00:21:38.506 --> 00:21:40.006
So it sort of has
to be emulated.

00:21:40.246 --> 00:21:43.296
It's not overly expensive if you
need it, feel free to use it.

00:21:43.616 --> 00:21:45.616
But it may result in
extra instructions.

00:21:45.836 --> 00:21:48.746
So don't unnecessarily
shrink things to char

00:21:48.746 --> 00:21:53.706
that don't actually need it.

00:21:53.966 --> 00:21:57.216
So next we have arithmetic
optimizations,

00:21:57.436 --> 00:21:58.826
and pretty much everything

00:21:58.826 --> 00:22:00.946
in this category
affects ALU bandwidth.

00:22:01.416 --> 00:22:05.106
The first thing you can do
is always use Metal built-ins

00:22:05.106 --> 00:22:05.856
whenever possible.

00:22:06.336 --> 00:22:07.846
They're optimized
implementations

00:22:07.846 --> 00:22:09.016
for a variety of functions.

00:22:09.206 --> 00:22:11.006
They're already optimized
for the hardware.

00:22:11.316 --> 00:22:13.426
It's generally better than
implementing them yourself.

00:22:14.426 --> 00:22:18.076
And in particular,
there are some of these

00:22:18.156 --> 00:22:20.256
that are usually
free in practice.

00:22:21.446 --> 00:22:24.416
And this is because GPUs
typically have modifiers.

00:22:24.526 --> 00:22:27.176
Operations that can be
performed for free on the input

00:22:27.176 --> 00:22:28.406
and output of instructions.

00:22:28.956 --> 00:22:31.876
And for A8 and later GPUs
these typically include negate,

00:22:32.246 --> 00:22:35.176
absolute value, and
saturate as you can see here,

00:22:35.176 --> 00:22:36.686
these three operations in green.

00:22:37.076 --> 00:22:41.666
So, there's no point to trying
to "be clever" and speed

00:22:41.666 --> 00:22:44.046
up your code by avoiding
those, because again,

00:22:44.046 --> 00:22:45.386
they're almost always free.

00:22:45.716 --> 00:22:48.426
And because they're free,
you can't do better than fee.

00:22:48.426 --> 00:22:50.306
There's no way to
optimize better than free.

00:22:50.956 --> 00:22:54.486
A8 and later GPUs, like a lot

00:22:54.486 --> 00:22:56.556
of others nowadays,
are scalar machines.

00:22:57.226 --> 00:22:59.586
And while shaders are
typically written with vectors,

00:22:59.636 --> 00:23:02.386
the compiler is going to split
them all apart internally.

00:23:02.866 --> 00:23:05.256
Of course, there's no downside
to writing vector code,

00:23:05.906 --> 00:23:08.996
I mean often it's clearer,
often it's more maintainable,

00:23:09.026 --> 00:23:12.496
often it fits what you're trying
to do, but it's also no better

00:23:12.496 --> 00:23:15.076
than writing scaler code
from a compiler perspective

00:23:15.076 --> 00:23:15.936
and the code you're
going to get.

00:23:16.546 --> 00:23:19.456
So there's no point in
trying to vectorize code

00:23:19.456 --> 00:23:23.126
that doesn't really fit a vector
format, because it's just going

00:23:23.126 --> 00:23:24.776
to end up the same
thing in the end,

00:23:24.866 --> 00:23:27.066
and you're kind of
wasting your time.

00:23:27.326 --> 00:23:29.216
However, as a side
note, which I'll go

00:23:29.216 --> 00:23:32.146
into more detail a lot later,
in later A8 and later GPUs,

00:23:32.146 --> 00:23:35.186
do have vector load in store
even though they do not have

00:23:35.186 --> 00:23:36.156
vector arithmetic.

00:23:36.596 --> 00:23:38.926
So this only applies
to arithmetic here.

00:23:41.006 --> 00:23:43.626
Instruction Level Parallelism
is something that some

00:23:43.626 --> 00:23:45.326
of you may have used
optimizing for,

00:23:45.326 --> 00:23:47.336
especially if you've
done work on CPUs.

00:23:47.866 --> 00:23:51.896
But on A8 and later GPUs this
is generally not a good thing

00:23:51.896 --> 00:23:54.176
to try to optimize for
because it typically works

00:23:54.176 --> 00:23:55.266
against registry usage,

00:23:55.266 --> 00:23:57.376
and registry usage
typically matters more.

00:23:57.966 --> 00:24:01.096
So a common pattern you may
have seen is a kind of loop

00:24:01.096 --> 00:24:03.406
where you have multiple
accumulators in order

00:24:03.406 --> 00:24:07.016
to better deal with
latency on a CPU.

00:24:07.816 --> 00:24:11.376
But on A8 and later GPUs this
is probably counterproductive.

00:24:11.776 --> 00:24:13.906
You'd be better off just
using one accumulator.

00:24:14.156 --> 00:24:16.606
Of course this applies to
much more complex examples

00:24:16.656 --> 00:24:18.686
than the artificial
simple ones here.

00:24:19.256 --> 00:24:21.986
Just write what you mean, don't
try to restructure your code

00:24:21.986 --> 00:24:23.346
to get more ILP out of it.

00:24:23.346 --> 00:24:26.166
It's probably not going to
help you at best, and at worst,

00:24:26.166 --> 00:24:28.906
you just might get worse code.

00:24:29.686 --> 00:24:33.306
So one fairly nice feature
of A8 and later GPUs is

00:24:33.386 --> 00:24:35.916
that they have very
fast select instructions

00:24:35.916 --> 00:24:37.746
that is the ternary operator.

00:24:38.476 --> 00:24:40.966
And historically it's
been fairly common

00:24:40.966 --> 00:24:43.536
to use clever tricks,
like this to try

00:24:43.536 --> 00:24:46.536
to perform select
operations in ternaries

00:24:46.916 --> 00:24:49.066
to avoid those branches
or whatever.

00:24:49.596 --> 00:24:52.986
But on modern GPUs this is
usually counterproductive,

00:24:52.986 --> 00:24:56.846
and especially on A8 later GPUs
because the compiler can't see

00:24:56.846 --> 00:24:57.786
through this cleverness.

00:24:57.786 --> 00:25:00.116
It's not going to figure
out what you actually mean.

00:25:00.646 --> 00:25:02.776
And really, this is really ugly.

00:25:03.586 --> 00:25:04.756
You could just have
written this.

00:25:04.756 --> 00:25:07.596
And this is going to be faster,
shorter, and it's actually going

00:25:07.596 --> 00:25:08.246
to show what you mean.

00:25:08.966 --> 00:25:12.776
Like before, being overly clever
will often obfuscate what you're

00:25:12.776 --> 00:25:14.656
trying to do and
confuse the compiler.

00:25:16.846 --> 00:25:18.676
Now, this is a potential
major pitfall,

00:25:18.676 --> 00:25:20.116
hopefully this won't
come up too much.

00:25:21.066 --> 00:25:26.106
On modern GPUs most of them
do not have integer division

00:25:26.106 --> 00:25:28.796
or modulus instructions,
integer not float.

00:25:29.616 --> 00:25:33.876
So avoid divisional
modulus by denominators

00:25:33.906 --> 00:25:36.666
that are not literal
or function consonants,

00:25:36.746 --> 00:25:38.876
the new feature mentioned in
some of the earlier talks.

00:25:39.556 --> 00:25:43.036
So in this example, what we
have over here, this first one

00:25:43.036 --> 00:25:45.426
where the denominator
is a variable,

00:25:45.736 --> 00:25:47.756
that will be very, very slow.

00:25:47.756 --> 00:25:49.356
Think hundreds of clock seconds.

00:25:50.346 --> 00:25:52.506
But these other two examples,
those will be very fast.

00:25:52.736 --> 00:25:53.256
Those are fine.

00:25:53.626 --> 00:25:56.926
So don't feel like you
have to avoid that.

00:25:57.616 --> 00:25:59.976
So, finally the topic
of fast-math.

00:26:00.996 --> 00:26:04.166
So in Metal, fast-math
is on by default.

00:26:04.416 --> 00:26:07.286
And this is because compiler
fast-math optimizations are

00:26:07.576 --> 00:26:09.616
critical to performance
Metal shaders.

00:26:10.046 --> 00:26:13.156
They can give off in 50%
performance gain or more

00:26:13.396 --> 00:26:14.946
over having fast-math off.

00:26:14.946 --> 00:26:16.466
So it's no wonder
it's on be default.

00:26:17.236 --> 00:26:20.226
And so what exactly do
we do in fast-math mode?

00:26:20.846 --> 00:26:22.396
Well, the first is that some

00:26:22.546 --> 00:26:25.206
of the Metal built-in functions
have different precision

00:26:25.206 --> 00:26:27.646
guarantees between
fast-math and non fast-math.

00:26:27.646 --> 00:26:30.146
And so in some of them they will
have slightly lower precision

00:26:30.436 --> 00:26:33.236
in fast-math mode to
get better performance.

00:26:34.576 --> 00:26:37.626
The compiler may increase
the intermediate precision

00:26:37.626 --> 00:26:39.886
of your operations like
by forming a fuse multiple

00:26:39.886 --> 00:26:40.686
add instructions.

00:26:41.306 --> 00:26:44.206
It will not decrease the
intermediate precision.

00:26:44.696 --> 00:26:48.086
So for example if you write a
float operation you will get an

00:26:48.086 --> 00:26:50.296
operation that is at
least a float operation.

00:26:50.376 --> 00:26:51.566
Not a math operation.

00:26:52.096 --> 00:26:54.386
So if you want to write half
operations you better write

00:26:54.466 --> 00:26:56.576
that, the compiler will
not do that for you,

00:26:56.576 --> 00:26:57.556
because it's not allowed to.

00:26:57.626 --> 00:27:00.456
It can't your precision
like that.

00:27:00.976 --> 00:27:03.896
We do ignore strict if not
a number, infinity steal,

00:27:03.896 --> 00:27:06.686
and sign zero semantics,
which is fairly important,

00:27:06.686 --> 00:27:08.986
because without that
you can't actually prove

00:27:08.986 --> 00:27:10.926
that x times zero
is equal to zero.

00:27:11.906 --> 00:27:16.246
But we will not introduce a new
not at new NaNs, not a number

00:27:16.666 --> 00:27:20.466
because in practice
that's a really nice way

00:27:20.466 --> 00:27:22.496
to annoy developers,
and break their code

00:27:22.496 --> 00:27:25.256
and we don't want to do that.

00:27:25.256 --> 00:27:28.576
And the compiler will perform
arithmetic re-association,

00:27:28.846 --> 00:27:30.676
but it will not do
arithmetic distribution.

00:27:30.676 --> 00:27:34.046
And really this just comes
down to what doesn't break code

00:27:34.046 --> 00:27:36.356
and makes it faster versus
what does break code.

00:27:36.526 --> 00:27:38.406
And we don't want to break code.

00:27:39.476 --> 00:27:44.006
So if you absolutely cannot use
fast-math for whatever reason,

00:27:44.446 --> 00:27:47.116
there are some ways to recover
some of that performance.

00:27:48.156 --> 00:27:51.866
Metal has a fused multiply-add
built in which you can see here.

00:27:52.286 --> 00:27:54.266
Which allows you to
directly request a fused

00:27:54.266 --> 00:27:55.496
multiply-add instructions.

00:27:55.806 --> 00:27:57.146
And of course if
fast-math is off,

00:27:57.146 --> 00:27:59.336
the compiler is not even
allowed to make those,

00:27:59.386 --> 00:28:02.866
it cannot change one bit of
your rounding, it is prohibited.

00:28:03.366 --> 00:28:05.886
So if you want to use
fused multiply-add

00:28:05.886 --> 00:28:07.296
and fast-math is
off, you're going

00:28:07.296 --> 00:28:08.406
to have to use the built-in.

00:28:08.526 --> 00:28:11.086
And that will regain
some of the performance,

00:28:11.476 --> 00:28:14.896
not all of it, but
at least some.

00:28:15.136 --> 00:28:17.386
So, on our third
topic, control flow.

00:28:18.456 --> 00:28:21.116
Predicated GP control flow
is not a new topic and some

00:28:21.116 --> 00:28:22.856
of you may already
be familiar with it.

00:28:22.896 --> 00:28:24.826
But here's a quick review
of what it means for you.

00:28:25.606 --> 00:28:28.046
Control flow that is
uniform across the SIMD,

00:28:28.046 --> 00:28:30.016
that is every thread is
doing the same thing,

00:28:30.496 --> 00:28:31.326
is generally fast.

00:28:31.856 --> 00:28:35.056
And this is true even if
the compiler can't see that.

00:28:35.566 --> 00:28:39.776
So if your program doesn't
appear uniform, but just happens

00:28:39.826 --> 00:28:44.786
to be uniform when it runs,
that's still just as fast.

00:28:44.786 --> 00:28:46.936
And similarly, the
opposite of this divergence,

00:28:46.936 --> 00:28:50.476
different lanes doing different
things, well in that case,

00:28:50.476 --> 00:28:52.346
it potentially may
have to run all

00:28:52.346 --> 00:28:55.476
of the different paths
simultaneously unlike a CPU

00:28:55.476 --> 00:28:57.746
which only takes
one path at a time.

00:28:58.386 --> 00:29:01.876
And as a result it does more
work, which of course means

00:29:01.906 --> 00:29:04.126
that inefficient control
flow can affect any

00:29:04.126 --> 00:29:06.696
of the bottlenecks, because it
just outright means the GPU is

00:29:06.756 --> 00:29:09.396
doing more stuff, whatever
that stuff happens to be.

00:29:11.246 --> 00:29:16.176
So, the one suggestion I'll make
on the topic of control flow is

00:29:16.176 --> 00:29:18.376
to avoid switch fall-throughs.

00:29:18.666 --> 00:29:20.786
And these are fairly
common in CPU code.

00:29:21.096 --> 00:29:24.006
But on GPUs they can potentially
be somewhat inefficient,

00:29:24.266 --> 00:29:28.446
because the compiler has to do
fairly nasty transformations

00:29:28.446 --> 00:29:30.806
to make them fit within the
control flow model of GPUs.

00:29:30.806 --> 00:29:34.006
And often this will involve
duplicating code and all sort

00:29:34.006 --> 00:29:36.636
of nasty things you probably
would rather not be happening.

00:29:37.216 --> 00:29:39.966
So if you can find a nice way to
avoid these switch fall-throughs

00:29:39.966 --> 00:29:41.636
in your code, you'll
probably be better off.

00:29:42.526 --> 00:29:45.076
So now we're on to
our final topic.

00:29:45.366 --> 00:29:46.166
Memory access.

00:29:46.446 --> 00:29:48.296
And we'll start with
the biggest pitfall

00:29:48.526 --> 00:29:50.156
that people most
commonly run into

00:29:50.156 --> 00:29:54.946
and that is dynamically indexed
non-constant stack arrays.

00:29:55.116 --> 00:29:56.516
Now that's quite a mouthful,

00:29:56.516 --> 00:29:59.506
but a lot of you probably
are familiar with code

00:29:59.506 --> 00:30:00.466
that looks vaguely like this.

00:30:01.046 --> 00:30:04.626
You have an array that consist
of values that are defined

00:30:04.626 --> 00:30:08.226
in runtime and vary between each
thread or each function call.

00:30:08.226 --> 00:30:10.986
And you index it to the
array with another value

00:30:10.986 --> 00:30:12.076
that is also a variable.

00:30:12.416 --> 00:30:15.026
That is a dynamically indexed
non-constant stack array.

00:30:15.846 --> 00:30:18.436
Now before we go on, I'm
not going to ask you to take

00:30:18.436 --> 00:30:20.966
for grabs at the idea that
stacks are slow on GPUs.

00:30:20.966 --> 00:30:22.456
I'm going to explain why.

00:30:23.306 --> 00:30:26.446
So, on CPUs typically you
have like a couple threads,

00:30:26.546 --> 00:30:29.506
maybe a dozen threads, and you
have megabytes of cache split

00:30:29.546 --> 00:30:30.276
between those threads.

00:30:30.396 --> 00:30:33.126
So every thread can have
hundreds of kilobytes

00:30:33.126 --> 00:30:35.366
of stack space before they
get really slow and have

00:30:35.406 --> 00:30:36.506
to head off to main memory.

00:30:37.336 --> 00:30:40.956
On a GPU you often have tens of
thousands of threads running.

00:30:41.016 --> 00:30:43.976
And they're all sharing
a much smaller cache too.

00:30:44.256 --> 00:30:46.356
So when it comes down to
it each thread has very,

00:30:46.356 --> 00:30:48.436
very little space
for data for a stack.

00:30:49.206 --> 00:30:52.176
It's just not meant for that,
it's not efficient and so

00:30:52.176 --> 00:30:54.966
as a general rule,
for most GPU programs,

00:30:54.966 --> 00:30:57.276
if you're using the
stack, you've already lost.

00:30:57.326 --> 00:31:00.786
It's so slow that almost
anything else would have

00:31:00.786 --> 00:31:01.156
been better.

00:31:02.696 --> 00:31:06.856
And an example for a real
world app is at the start

00:31:06.856 --> 00:31:09.586
of the program it needed
to select one of two float

00:31:09.586 --> 00:31:12.376
for vectors, so it
used a 32-byte array,

00:31:12.376 --> 00:31:14.836
an array of two float
fours and tried to select

00:31:14.916 --> 00:31:16.336
between them using
this stack array.

00:31:16.336 --> 00:31:18.996
And that caused a
30% performance loss

00:31:18.996 --> 00:31:21.076
in this program even though it's
only done once at the start.

00:31:21.546 --> 00:31:24.036
It can be pretty significant.

00:31:24.716 --> 00:31:27.516
And of course every time we
improve the compiler we are

00:31:27.516 --> 00:31:30.936
going to try harder and harder
to avoid, do anything we can

00:31:31.546 --> 00:31:34.706
to avoid generating these stack
access because it is that bad.

00:31:35.806 --> 00:31:38.186
Now I'll show you two
examples here that are okay.

00:31:39.636 --> 00:31:42.876
This other one, you can
see those are constants,

00:31:42.976 --> 00:31:43.666
not variables.

00:31:44.026 --> 00:31:46.706
It's not a non-constant
stack array and that's fine

00:31:47.266 --> 00:31:50.706
because the values don't vary
per threads, they don't need

00:31:50.706 --> 00:31:51.856
to be duplicated per thread.

00:31:52.426 --> 00:31:54.506
So that's okay.

00:31:54.856 --> 00:31:56.406
And this one is also okay.

00:31:56.616 --> 00:31:57.136
Wait, why?

00:31:57.136 --> 00:31:59.486
It's still a dynamically indexed
non-constant stack array.

00:32:00.116 --> 00:32:02.646
But it's only done dynamically
indexed because of this loop.

00:32:03.266 --> 00:32:06.116
And the compiler is going
to unroll that loop.

00:32:06.476 --> 00:32:09.526
In fact, your compiler
aggressively unrolls any loop

00:32:09.526 --> 00:32:12.426
that is accessing the stack to
try to make it stop doing that.

00:32:13.256 --> 00:32:15.836
So in this case after it's
unrolled it will no longer be

00:32:15.836 --> 00:32:17.326
dynamically indexed,
so it will be fast.

00:32:17.326 --> 00:32:18.796
And this is worth mentioning,

00:32:18.796 --> 00:32:20.706
because this is a fairly
common pattern in a lot

00:32:20.706 --> 00:32:23.336
of graphics code and I don't
want to scare you into not doing

00:32:23.336 --> 00:32:24.676
that when it's probably fine.

00:32:25.516 --> 00:32:27.986
So now that we've gone
over the topic of how

00:32:27.986 --> 00:32:30.236
to not do certain types
of loads and stores,

00:32:30.536 --> 00:32:32.426
let's go on to making
the loads and stores

00:32:32.426 --> 00:32:34.556
that we do actually fast.

00:32:35.416 --> 00:32:38.066
Now while A8 and later
GPUs use scalar arithmetic,

00:32:38.066 --> 00:32:41.236
as I went over earlier, they
do have vector memory units.

00:32:42.006 --> 00:32:45.546
And one big vector loading
source of course faster

00:32:45.546 --> 00:32:48.746
than multiple smaller ones
that sum up to the same size.

00:32:49.616 --> 00:32:52.266
And this typically effects the
memory issue rate bottleneck

00:32:52.326 --> 00:32:53.266
because if you're running

00:32:53.266 --> 00:32:55.186
through a loads,
that's fewer loads.

00:32:56.266 --> 00:33:00.136
And, so as of iOS 10, one of
our new compiler optimizations,

00:33:00.136 --> 00:33:03.186
is we will try to vectorize
some loads and stores that go

00:33:03.186 --> 00:33:05.396
to neighboring memory
locations wherever we can,

00:33:05.626 --> 00:33:07.766
because again it can give
good performance improvements.

00:33:08.956 --> 00:33:12.656
But nevertheless, this is one
of the cases where working

00:33:12.656 --> 00:33:14.286
with the compiler
can be very helpful,

00:33:14.536 --> 00:33:15.436
and I'll give an example.

00:33:16.436 --> 00:33:18.336
So as you can see here,
here's a simple loop

00:33:18.436 --> 00:33:21.016
that does some arithmetic and
reads in an array of structures,

00:33:21.876 --> 00:33:25.066
but on each iteration,
it reads just two loads.

00:33:25.506 --> 00:33:27.696
Now we would want that
to be one if we could,

00:33:27.696 --> 00:33:29.856
because one is better than two.

00:33:29.856 --> 00:33:32.096
And the compiler wants that too.

00:33:32.096 --> 00:33:34.976
It wants to try to vectorize
this but it can't, because A

00:33:34.976 --> 00:33:36.886
and C aren't next to
each other in memory

00:33:36.886 --> 00:33:37.896
so there's nothing it can do.

00:33:37.896 --> 00:33:39.976
The compiler's not allowed
to rearrange your structs,

00:33:40.256 --> 00:33:41.156
so we've got two loads.

00:33:42.186 --> 00:33:43.436
There's two solutions to this.

00:33:44.036 --> 00:33:46.536
Number one, of course,
just make it a float to,

00:33:46.536 --> 00:33:47.966
now it's a vector
load, you're done.

00:33:48.696 --> 00:33:49.996
One load, a set of
two, we're all good.

00:33:51.176 --> 00:33:54.946
Also, as of iOS 10, this
should also be equally fast,

00:33:55.276 --> 00:33:56.986
because here, we've
reordered our struct

00:33:56.986 --> 00:33:58.486
to put the values
next to each other,

00:33:58.836 --> 00:34:00.936
so the compiler can
now vectorize the loads

00:34:00.936 --> 00:34:01.936
when it's doing it.

00:34:02.256 --> 00:34:05.286
And this is an example again
of working with the compiler,

00:34:05.636 --> 00:34:08.616
you've allowed the compiler to
do something it couldn't before,

00:34:08.726 --> 00:34:11.295
because you understand
what's going on.

00:34:11.295 --> 00:34:13.766
You understand how the
patterns need to be

00:34:13.766 --> 00:34:15.246
to make the compiler happy

00:34:15.516 --> 00:34:18.916
and make it able to
do a [inaudible].

00:34:19.835 --> 00:34:23.216
So, another thing to keep in
mind with loads and stores is

00:34:23.216 --> 00:34:26.235
that A8 and later GPUs
have dedicated hardware

00:34:26.235 --> 00:34:31.735
for device memory addressing,
but this hardware has limits.

00:34:32.406 --> 00:34:35.065
The offset for accessing
device memory must fit

00:34:35.065 --> 00:34:36.406
within a signed integer.

00:34:36.876 --> 00:34:39.335
Smaller types like short
and ushort are also okay,

00:34:39.335 --> 00:34:40.916
in fact they're highly
encouraged,

00:34:41.416 --> 00:34:43.545
because those do also fit
within a signed integer.

00:34:44.366 --> 00:34:47.786
However, of course uint does
not because it can have values

00:34:47.786 --> 00:34:49.076
out of range of signed integer.

00:34:49.696 --> 00:34:54.076
And so if the compiler
runs into a situation

00:34:54.076 --> 00:34:56.716
where the offset is a
uint and it cannot prove

00:34:56.775 --> 00:34:58.896
that it will safely fit
within a signed integer,

00:34:59.266 --> 00:35:01.596
it has to manually
calculate the address,

00:35:02.156 --> 00:35:04.086
rather than letting the
dedicated hardware do it.

00:35:04.366 --> 00:35:05.656
And that can waste power,

00:35:05.786 --> 00:35:08.886
it can waste ALU
performance and so forth.

00:35:09.006 --> 00:35:10.066
It's not good.

00:35:10.626 --> 00:35:15.396
So, change your offset to
int, now the problem's solved.

00:35:15.396 --> 00:35:16.426
And of course taking advantage

00:35:16.426 --> 00:35:18.606
to this will typically
save you ALU bandwidth.

00:35:21.496 --> 00:35:23.626
So now on to our final
topic that I sort of glossed

00:35:23.626 --> 00:35:25.476
over earlier, latency
and occupancy.

00:35:26.266 --> 00:35:28.496
So one of the core
design tenants

00:35:28.496 --> 00:35:30.316
of modern GPUs is
they hide latency

00:35:30.316 --> 00:35:32.146
by using large scale
multithreading.

00:35:32.646 --> 00:35:34.866
So when they're waiting for
something slow to finish,

00:35:34.866 --> 00:35:36.856
like a texture read,
they just go

00:35:36.856 --> 00:35:37.896
and run another thread instead

00:35:37.896 --> 00:35:39.336
of sitting there doing
nothing while waiting.

00:35:39.336 --> 00:35:40.536
And this is fairly important

00:35:40.536 --> 00:35:43.426
because texture reads typically
take a couple hundred cycles

00:35:43.426 --> 00:35:44.656
to complete on average.

00:35:47.306 --> 00:35:49.536
And so the more latency
you have in a shader,

00:35:49.536 --> 00:35:52.066
the more threads you need
to hide that latency,

00:35:52.606 --> 00:35:53.776
and how many threads
can you have?

00:35:54.236 --> 00:35:56.526
Well it's limited by the fact
that you have a fixed set

00:35:56.526 --> 00:35:58.116
of resources that are shared

00:35:58.116 --> 00:35:59.696
between threads in
a thread group.

00:36:00.016 --> 00:36:02.706
So clearly depending on
how much each thread uses,

00:36:02.706 --> 00:36:04.596
you have a limitation on
the number of threads.

00:36:04.846 --> 00:36:07.006
And the two things that
are split are the number

00:36:07.006 --> 00:36:08.766
of registers and
thread group memory.

00:36:09.316 --> 00:36:11.136
So if you use more
registers per thread,

00:36:11.306 --> 00:36:12.606
now you can't have
as many threads.

00:36:12.716 --> 00:36:13.126
Simple enough.

00:36:13.606 --> 00:36:17.626
And if you use more thread group
memory per thread, again you run

00:36:17.626 --> 00:36:18.646
into the same problem,

00:36:18.806 --> 00:36:20.936
more thread your memory per
thread means to your threads.

00:36:21.726 --> 00:36:25.316
And you can actually check out
the occupancy of your shader

00:36:25.636 --> 00:36:29.416
by using MTLComputePipeLineState
incurring

00:36:29.416 --> 00:36:31.306
maxTotalThreadsPerThreadgroup,

00:36:31.646 --> 00:36:33.826
which will tell you what
the actual occupancy

00:36:33.826 --> 00:36:36.776
of your shader is based
on the register usage

00:36:36.846 --> 00:36:39.486
and the thread group
memory usage.

00:36:40.006 --> 00:36:42.346
And so when we say a
shader is latency limited,

00:36:42.626 --> 00:36:44.426
it means you have
too few threads

00:36:44.496 --> 00:36:45.806
to hide the latency of a shader.

00:36:45.806 --> 00:36:47.436
And there's two things
you can do there,

00:36:47.696 --> 00:36:49.616
you can either reduce the
latency of your shader,

00:36:50.016 --> 00:36:52.296
your save registers
or whatever else it is

00:36:52.476 --> 00:36:54.546
that is preventing you
from having more threads.

00:36:57.066 --> 00:37:02.236
So, since it's kind of
hard to go over latency

00:37:02.236 --> 00:37:03.856
in a very large complex shader.

00:37:04.246 --> 00:37:06.356
I'll go over a little bit
of a pseudocode example

00:37:06.356 --> 00:37:08.516
that will hopefully give you
a big of an intuition of how

00:37:08.516 --> 00:37:10.496
to think about latency
and how to sort

00:37:10.496 --> 00:37:12.926
of mentally model
in your shaders.

00:37:14.166 --> 00:37:16.666
So, here's an example
of a REAL dependency.

00:37:17.066 --> 00:37:19.686
We have a texture sample,
and then we use the operative

00:37:19.686 --> 00:37:21.786
of that texture sample
to run an if statement

00:37:21.786 --> 00:37:24.306
and then we do another texture
sample inside that x statement.

00:37:24.986 --> 00:37:26.056
We have to wait twice.

00:37:26.056 --> 00:37:28.706
Because we have to wait once
before doing the if statement.

00:37:29.016 --> 00:37:31.566
And we have to wait again
before using the value

00:37:31.566 --> 00:37:32.836
from the second texture sample.

00:37:32.836 --> 00:37:36.566
So that's two serial
texture accesses

00:37:37.026 --> 00:37:38.716
for a total of twice
the latency.

00:37:40.326 --> 00:37:42.416
Now here's an example
of a false dependency.

00:37:42.416 --> 00:37:43.456
It looks a lot like the other,

00:37:43.456 --> 00:37:45.716
except we're not using
a in the if statement.

00:37:46.906 --> 00:37:51.086
But typically, we can't
wait across control flow.

00:37:51.346 --> 00:37:54.366
The if statement acts an
effective barrier in this case.

00:37:54.706 --> 00:37:56.746
So, we automatically have

00:37:56.886 --> 00:37:59.796
to wait here anyways even though
there's no data dependency.

00:38:00.116 --> 00:38:01.636
So we still get twice
the latency.

00:38:02.006 --> 00:38:04.386
As you noticed the GPU
does not actually care

00:38:04.456 --> 00:38:05.656
about your data dependencies.

00:38:06.046 --> 00:38:09.406
It only cares about what the
dependencies appear to be

00:38:09.966 --> 00:38:13.106
and so the second one will
be just as long latency

00:38:13.106 --> 00:38:15.196
as the first one, even
though there isn't a data

00:38:15.196 --> 00:38:16.016
dependency there.

00:38:16.846 --> 00:38:19.426
And then finally
here's a simple one

00:38:19.426 --> 00:38:21.326
where you just have two
texture reads at the top,

00:38:21.946 --> 00:38:23.826
and they can both
be done in parallel

00:38:24.456 --> 00:38:26.626
and then we can have
a single wait.

00:38:26.746 --> 00:38:29.186
So it's 1 x instead
of 2 x for latency.

00:38:30.226 --> 00:38:32.386
So, what are you going to
do with this knowledge?

00:38:32.696 --> 00:38:36.146
So in many real world
shaders you have opportunities

00:38:36.146 --> 00:38:38.466
to tradeoff between
latency and throughput.

00:38:39.046 --> 00:38:41.946
And a common example of this
might be that you have some code

00:38:41.946 --> 00:38:45.956
where based on one texture read
you can decide, oh we don't need

00:38:45.956 --> 00:38:48.776
to do anything in this shader,
we're going to quit early.

00:38:48.776 --> 00:38:50.436
And that can be very useful.

00:38:50.436 --> 00:38:53.366
Because now all that work
that's being done in the cases

00:38:53.366 --> 00:38:54.546
where you don't need
it to be done,

00:38:55.026 --> 00:38:56.056
you're saving all that work.

00:38:56.346 --> 00:38:57.036
That's great.

00:38:57.386 --> 00:39:01.816
But now you're increasing
your throughput

00:39:02.596 --> 00:39:04.156
by reducing the amount
of work you need to do.

00:39:05.026 --> 00:39:09.256
But you're also increasing
your latency because now it has

00:39:09.316 --> 00:39:12.866
to do the first texture read,
then wait for that texture read,

00:39:13.306 --> 00:39:14.956
then do your early
termination check,

00:39:15.336 --> 00:39:18.616
and then do whatever other
texture reads you have.

00:39:18.996 --> 00:39:20.626
And well is it faster?

00:39:20.626 --> 00:39:21.106
Is it not?

00:39:21.476 --> 00:39:23.976
Often you just have to test.

00:39:24.386 --> 00:39:26.756
Because which is faster
is really going to depend

00:39:26.756 --> 00:39:28.966
on your shader, but it's
a thing worth being aware

00:39:28.966 --> 00:39:32.026
of that often is a real
tradeoff and you often have

00:39:32.096 --> 00:39:33.436
to experiment to
see what's right.

00:39:34.246 --> 00:39:35.976
Now, while there isn't
a universal rule,

00:39:35.976 --> 00:39:39.056
there is one particular
guideline I can give for A8

00:39:39.056 --> 00:39:43.016
and later GPUs and that is
typically the hardware needs

00:39:43.016 --> 00:39:45.556
at least two texture
reads at a time

00:39:45.556 --> 00:39:47.706
to get full ability
to hide latency.

00:39:48.206 --> 00:39:49.086
One is not enough.

00:39:49.896 --> 00:39:51.656
If you have to do
one, no problem.

00:39:51.706 --> 00:39:53.816
But if you have some choice

00:39:53.816 --> 00:39:55.726
in how you arrange your
texture reads in your shader,

00:39:55.946 --> 00:39:58.066
if you allow it to do
at least two at a time,

00:39:58.266 --> 00:39:59.416
you may get better performance.

00:40:01.326 --> 00:40:02.236
So, in summary.

00:40:03.576 --> 00:40:06.436
Make sure you pick the correct
address spaces, data structures,

00:40:06.436 --> 00:40:09.556
layouts and so forth, because
getting this wrong is going

00:40:09.556 --> 00:40:11.676
to hurt so much that often
none of the other stuff

00:40:11.676 --> 00:40:12.816
in the presentation will matter.

00:40:14.326 --> 00:40:15.646
Work with the compiler.

00:40:15.646 --> 00:40:16.506
Write what you mean.

00:40:17.086 --> 00:40:18.476
Don't try to be too clever,

00:40:18.476 --> 00:40:20.966
or the compiler won't know what
you mean and will get lost,

00:40:21.406 --> 00:40:22.786
and won't be able to do its job.

00:40:23.656 --> 00:40:25.456
Plus, it's easier to
write what you mean.

00:40:26.976 --> 00:40:28.566
Keep an eye out for
the big pitfalls,

00:40:28.566 --> 00:40:30.156
not just the
micro-optimizations.

00:40:30.386 --> 00:40:32.796
They're often not as obvious,
and they often don't come

00:40:32.796 --> 00:40:35.326
up as often, but when
they do, they hurt.

00:40:35.456 --> 00:40:37.716
And they will hurt so
much that no number

00:40:37.716 --> 00:40:39.346
of micro-optimizations
will save you.

00:40:40.986 --> 00:40:42.466
And feel free to experiment.

00:40:42.696 --> 00:40:44.926
There's a number of rule
tradeoffs that happen,

00:40:44.926 --> 00:40:47.646
where there's simply
no single rule.

00:40:48.076 --> 00:40:49.946
And try them both,
see what's faster.

00:40:51.936 --> 00:40:54.486
So, if you want more
information, go online.

00:40:54.486 --> 00:40:55.886
The video of the talk
will be up there.

00:40:55.886 --> 00:40:59.946
Here are the other session if
you missed them earlier, again,

00:40:59.946 --> 00:41:01.406
the videos will be online.

00:41:02.936 --> 00:41:03.246
Thank you.