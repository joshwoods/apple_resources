WEBVTT

00:00:07.516 --> 00:00:17.500
[ Music ]

00:00:21.516 --> 00:00:26.846
[ Applause ]

00:00:27.346 --> 00:00:27.986
>> Welcome.

00:00:28.846 --> 00:00:31.776
This is Part 2 of our
What's New in Metal session.

00:00:32.485 --> 00:00:36.746
My name is Charles Brissart,
and I'm a GPU Software Engineer,

00:00:36.746 --> 00:00:41.046
and together with my colleague,
Dan Omachi and Ana Tikhonova,

00:00:41.046 --> 00:00:43.726
I will be telling you about
some of our new features.

00:00:45.076 --> 00:00:47.876
But first, let's take a look

00:00:48.326 --> 00:00:50.756
at the other Metal
session at the WWDC.

00:00:51.716 --> 00:00:56.456
The first two sessions I call
Adopting Metal uncovered some

00:00:56.456 --> 00:00:59.386
of the basic concepts
of Metal as well

00:00:59.386 --> 00:01:01.336
as some more advanced
considerations.

00:01:02.956 --> 00:01:06.806
The What's New in Metal session
covered our new features.

00:01:08.046 --> 00:01:12.596
Finally, the Advanced Shadow
Optimization session will tell

00:01:12.596 --> 00:01:15.056
you how to get the best
performance out of your shaders.

00:01:15.916 --> 00:01:21.746
So this morning you were
told about tessellation,

00:01:21.916 --> 00:01:25.466
resource heaps, memoryless
render targets as well

00:01:25.466 --> 00:01:27.856
as some improvement
for GPU tools.

00:01:29.636 --> 00:01:32.776
This afternoon we'll tell you
about function specialization,

00:01:33.096 --> 00:01:38.076
function resource read-writes,
wide color, texture assets,

00:01:38.526 --> 00:01:41.736
as well as some addition to
the Metal Performance Shaders.

00:01:41.736 --> 00:01:46.726
So let's get started with
function specialization.

00:01:48.856 --> 00:01:51.566
It is a common pattern
in a rendering engine

00:01:52.336 --> 00:01:55.546
to define a few complex
master functions

00:01:55.816 --> 00:01:58.876
and then use those master
functions to generator minimum

00:01:58.876 --> 00:02:00.646
of specialized simple functions.

00:02:01.706 --> 00:02:05.586
The idea is that the
master function allow you

00:02:05.586 --> 00:02:10.485
to avoid duplicating card while
the specialized function are

00:02:10.485 --> 00:02:14.086
simpler on those as a result
of better performance.

00:02:15.796 --> 00:02:17.056
So let's take an example.

00:02:17.306 --> 00:02:20.386
If we are trying to write a
material function you could

00:02:20.386 --> 00:02:24.306
write a master function
that implements every aspect

00:02:24.536 --> 00:02:26.376
of any material that
you might need.

00:02:27.536 --> 00:02:30.466
But then, if you are trying
to implement a shiny --

00:02:30.736 --> 00:02:32.176
a simple shiny material,

00:02:32.646 --> 00:02:34.336
you would probably
not need reflection,

00:02:34.466 --> 00:02:36.286
but you will need a
specular highlight.

00:02:37.876 --> 00:02:39.986
If you implement a
reflected material

00:02:39.986 --> 00:02:42.676
on the other hand you will
need to add reflection

00:02:42.986 --> 00:02:44.756
on also the specular highlights.

00:02:45.606 --> 00:02:48.856
Our transition material will
need subsurface scattering,

00:02:48.896 --> 00:02:50.196
but probably no reflection

00:02:50.196 --> 00:02:53.536
or may be no specular
highlights either, and so on.

00:02:53.946 --> 00:02:54.766
You get the idea.

00:02:55.986 --> 00:02:59.766
So this is typically implemented
using preprocessor macros.

00:03:00.216 --> 00:03:05.566
The master function is
compiled with a set of values

00:03:05.566 --> 00:03:07.766
for the macro to create
a specialized function.

00:03:08.286 --> 00:03:12.146
This can be done at runtime,
but this is expensive.

00:03:12.146 --> 00:03:15.926
You can also try to
precompile every single variant

00:03:15.926 --> 00:03:20.356
of the precompiled function, but
-- and then store them in Metal,

00:03:20.406 --> 00:03:22.376
but this requires
a lot of storage

00:03:22.376 --> 00:03:24.166
because you can have
many, many variants,

00:03:24.316 --> 00:03:25.876
or maybe you don't know
which one you will need.

00:03:27.676 --> 00:03:30.486
Another approach is to
use runtime constants.

00:03:31.626 --> 00:03:34.776
Runtime constants avoid the need
to recompile your functions.

00:03:35.066 --> 00:03:37.586
However, you need to
evaluate the values

00:03:37.586 --> 00:03:39.146
of the constant at runtime.

00:03:40.516 --> 00:03:43.106
That will impact the
performance of your shaders.

00:03:43.106 --> 00:03:46.536
So we are proposing a new way

00:03:46.536 --> 00:03:49.736
to create specialized
functions using what we call

00:03:49.736 --> 00:03:50.656
function constants.

00:03:51.446 --> 00:03:53.886
So function constants
are constants

00:03:54.506 --> 00:03:57.286
that are defined directly in
the Metal shading language

00:03:57.676 --> 00:04:01.116
and can be compiled into IR
and stored in the Metal lib.

00:04:01.946 --> 00:04:05.516
Then at runtime you can provide
the value of the constant

00:04:05.896 --> 00:04:07.846
to create a specialized
function.

00:04:08.746 --> 00:04:11.526
The advantage of
this approach is

00:04:11.526 --> 00:04:13.866
that you can compile the
master function offline

00:04:13.866 --> 00:04:15.096
and store it in the Metal lib.

00:04:15.816 --> 00:04:17.166
The storage requirement is small

00:04:17.166 --> 00:04:19.356
because you only store
the master functions.

00:04:20.076 --> 00:04:23.146
And since we run a
quick optimization pass

00:04:23.146 --> 00:04:24.926
when we create the
specialized function,

00:04:25.446 --> 00:04:27.216
you still get the
best performance.

00:04:28.026 --> 00:04:31.036
So let's look at an example.

00:04:31.716 --> 00:04:34.146
This is what a master
function could look

00:04:34.146 --> 00:04:35.716
like using a preprocessor macro.

00:04:36.176 --> 00:04:38.566
Of course, this is
a simple example.

00:04:38.926 --> 00:04:40.696
A real one would be
much more complex.

00:04:41.266 --> 00:04:45.426
As you can see, different parts
of the code surrounded by what

00:04:45.426 --> 00:04:49.076
if statements so that
you can eliminate

00:04:49.146 --> 00:04:50.256
that section of the code.

00:04:51.766 --> 00:04:54.206
Here is what it would look
like with function constant.

00:04:54.796 --> 00:04:57.696
As you can see at the top,
we are defining a number

00:04:57.696 --> 00:04:59.886
of constants, and then
we use them in the code.

00:05:00.696 --> 00:05:04.326
To define the constants, you use
the constant keyword followed

00:05:04.326 --> 00:05:08.476
by the type, in this case
Boolean, and finally the name

00:05:08.476 --> 00:05:12.236
of the constant and the
function constant attribute.

00:05:12.806 --> 00:05:16.026
The function constant attribute
specifies that the value

00:05:16.026 --> 00:05:18.846
of the constant is not going
to be provided at compile time

00:05:19.156 --> 00:05:20.926
but will be provided at runtime

00:05:20.926 --> 00:05:22.616
when we create the
specialized function.

00:05:23.386 --> 00:05:25.546
You should also note that
we are passing an index.

00:05:26.166 --> 00:05:28.926
That index can be used
in addition to the name

00:05:28.926 --> 00:05:32.646
to identify the constant when we
create the specialized function

00:05:32.646 --> 00:05:33.326
at runtime.

00:05:34.846 --> 00:05:37.676
You can then use the
constant anywhere in your code

00:05:37.676 --> 00:05:38.876
like your normal constant.

00:05:39.426 --> 00:05:42.046
Here we have a simple if
statement that is used

00:05:42.046 --> 00:05:44.136
to conditionalize
part of the code.

00:05:45.426 --> 00:05:48.946
So once you've created your
master function and compiled it

00:05:48.946 --> 00:05:50.226
and stored it in a Metal lib,

00:05:50.706 --> 00:05:53.706
you need to at runtime
create specialized functions.

00:05:53.706 --> 00:05:56.506
So you need to provide the
values of the constant.

00:05:57.456 --> 00:06:00.946
To do that, we use an MTL
function constant values object

00:06:01.276 --> 00:06:03.396
that will solve the values
of multiple constants.

00:06:04.106 --> 00:06:07.956
Once we created the object,
we can then set the values

00:06:07.956 --> 00:06:13.106
of a constant either by
name, by index, or by name.

00:06:15.006 --> 00:06:17.456
Once we have created an object,

00:06:17.726 --> 00:06:20.396
we can then create the
specialized function

00:06:20.396 --> 00:06:22.746
by simply coding the
new function with names

00:06:23.176 --> 00:06:27.496
and constant values on the
library, providing the name

00:06:27.496 --> 00:06:32.446
of the master function as well
as the values we just filled.

00:06:33.016 --> 00:06:36.986
This will return a regular MTL
function that can then be used

00:06:36.986 --> 00:06:40.856
to create compute pipeline
or render pipeline depending

00:06:40.856 --> 00:06:41.946
on the type of the function.

00:06:42.456 --> 00:06:45.696
So to better understand
how this works,

00:06:45.696 --> 00:06:47.906
let's look at the
compilation pipeline.

00:06:48.576 --> 00:06:52.646
So at build time, you use the
source of your master function

00:06:53.336 --> 00:06:55.666
and compile it and
store into a Metal lib.

00:06:56.586 --> 00:07:01.146
At runtime you load
the Metal lib

00:07:01.146 --> 00:07:06.816
and create a new function using
the MTL function constant values

00:07:06.996 --> 00:07:08.366
to specialize the function.

00:07:09.316 --> 00:07:11.596
At this point, we
run some optimization

00:07:11.596 --> 00:07:13.716
to eliminate any code
that's not used anymore,

00:07:14.366 --> 00:07:17.096
and then we have an interior
function that we can use

00:07:17.666 --> 00:07:21.246
to create a render pipeline
or a compute pipeline.

00:07:21.826 --> 00:07:27.976
You can declare constants
of any scalar or vector type

00:07:27.976 --> 00:07:30.216
that is [inaudible]
in Metal , so float,

00:07:30.216 --> 00:07:32.326
half, int, uint, and so on.

00:07:32.886 --> 00:07:35.576
Here we are defining
half4 color.

00:07:37.076 --> 00:07:41.936
You can also create intermediate
constants using the value

00:07:41.936 --> 00:07:43.036
of function constants.

00:07:43.406 --> 00:07:45.906
Here we're defining
a Boolean constant

00:07:45.906 --> 00:07:48.456
that has the opposite value
of a function constant a.

00:07:49.496 --> 00:07:51.936
Here we are calculating a
value based on the value

00:07:51.936 --> 00:07:53.506
of the value function constant.

00:07:54.396 --> 00:07:58.376
We can also have
optional constants.

00:07:58.996 --> 00:08:01.596
Optional constants are constants
for which you don't need

00:08:01.596 --> 00:08:04.346
to always provide the value when
you specialize the function.

00:08:04.876 --> 00:08:07.616
This is exactly the same
thing as using a what ifdef

00:08:07.616 --> 00:08:09.746
in your code when using
preprocessor macros.

00:08:10.866 --> 00:08:14.066
To do this, you use the if
function constant defined built

00:08:14.066 --> 00:08:17.106
in that will return true if
the value has been provided

00:08:17.466 --> 00:08:20.576
and false if otherwise.

00:08:22.356 --> 00:08:24.876
You can also use
function constant to add

00:08:24.876 --> 00:08:27.176
or eliminate arguments
from function.

00:08:27.986 --> 00:08:31.796
This is useful to avoid, to
making sure you don't have

00:08:31.796 --> 00:08:34.135
to bind a buffer or texture

00:08:34.135 --> 00:08:35.696
if you know it's not
going to be used.

00:08:36.346 --> 00:08:39.806
It's also useful to replace
the type of an argument,

00:08:40.336 --> 00:08:41.506
and we'll talk about --

00:08:41.506 --> 00:08:43.706
we'll talk more about this
in the next couple of slides.

00:08:44.916 --> 00:08:46.286
So here we have an example.

00:08:46.906 --> 00:08:51.906
This is a vertex function that
can implement skinning depending

00:08:51.906 --> 00:08:54.126
of the value of the
doSkinning constant.

00:08:55.166 --> 00:09:00.986
The first argument of the
function is the matrices buffer

00:09:01.156 --> 00:09:02.996
that will exist depending

00:09:02.996 --> 00:09:06.336
on whether the doSkinning
constant is true or false.

00:09:06.806 --> 00:09:09.806
We use the function
constant attribute to qualify

00:09:09.806 --> 00:09:11.676
that argument as being optional.

00:09:12.806 --> 00:09:17.076
In the code, you still need to
use the same function constant

00:09:17.336 --> 00:09:19.446
to protect the code
that's using that argument.

00:09:20.306 --> 00:09:23.106
So here we use doSkinning
in the if statement,

00:09:23.696 --> 00:09:29.706
and then we can use the
matrices safely in our code.

00:09:30.296 --> 00:09:34.626
You can as well use function
constant to eliminate arguments

00:09:34.626 --> 00:09:35.786
from the stage in struct.

00:09:37.176 --> 00:09:39.046
Here, we have two
color arguments.

00:09:39.476 --> 00:09:44.696
The first color argument
as type float4 on these use

00:09:44.696 --> 00:09:46.886
for attributes, that
is attribute 1.

00:09:48.206 --> 00:09:54.096
The second lowp color is a
lower precision color half4

00:09:55.146 --> 00:09:57.926
but is overriding the
same attribute index.

00:09:59.046 --> 00:10:00.976
So you can have either
one or the other.

00:10:01.046 --> 00:10:04.016
These are used to
specifically change the type

00:10:04.506 --> 00:10:07.066
of the color attributes
in your code.

00:10:08.206 --> 00:10:12.196
There are some limitations with
function constants, namely,

00:10:12.196 --> 00:10:15.746
you cannot really change the
layout of a struct in memory,

00:10:16.856 --> 00:10:18.966
and that can be a problem
because you might want

00:10:18.966 --> 00:10:22.696
to have different constants for
different shaders and so on.

00:10:23.956 --> 00:10:25.336
But you can work around that

00:10:25.576 --> 00:10:28.446
but adding multiple arguments
with different types.

00:10:29.086 --> 00:10:31.746
So in this example, we
have two buffer arguments

00:10:32.566 --> 00:10:34.616
that are using buffer index 1.

00:10:35.336 --> 00:10:37.036
They are controlled
by function constants,

00:10:37.036 --> 00:10:38.806
use ConstantA and ConstantB.

00:10:41.606 --> 00:10:46.626
So these are used to
select one or the other.

00:10:46.626 --> 00:10:50.406
Note that we have -- we use
an intermediate constant

00:10:50.406 --> 00:10:52.756
that is the opposite
of the first constant

00:10:52.886 --> 00:10:54.116
to make sure only one

00:10:54.116 --> 00:10:56.196
of the arguments will
exist at a given time.

00:10:56.656 --> 00:10:59.756
So in summary, you can
use function constant

00:11:00.196 --> 00:11:02.626
to create specialized
function at runtime.

00:11:03.226 --> 00:11:07.546
It avoids front end compilation,
and because we only use --

00:11:07.546 --> 00:11:09.796
and it only uses fast
optimization phase

00:11:09.796 --> 00:11:11.066
to eliminate unused code.

00:11:11.606 --> 00:11:14.346
The storage is compact
because you only need

00:11:14.346 --> 00:11:16.506
to store the master
function in your library.

00:11:17.356 --> 00:11:18.756
You don't have to
ship your source.

00:11:18.756 --> 00:11:20.246
It can only ship the IR.

00:11:20.246 --> 00:11:23.376
And finally, the unused
code is eliminated,

00:11:23.376 --> 00:11:24.806
which gives you the
best performance.

00:11:25.796 --> 00:11:31.036
So let's now talk about
function resource read-writes.

00:11:31.596 --> 00:11:35.076
So we're introducing
two new features,

00:11:35.686 --> 00:11:37.536
function buffered read-writes

00:11:37.536 --> 00:11:39.486
and function texture
read-writes.

00:11:40.826 --> 00:11:44.006
Function buffered read-writes
is the ability to read and write

00:11:44.196 --> 00:11:49.146
to a buffer from any function
type and also the ability

00:11:49.146 --> 00:11:51.636
to use atomic operations
on those buffers

00:11:51.636 --> 00:11:52.646
from any function type.

00:11:53.436 --> 00:11:56.276
As you guessed, function texture
read-writes is the ability

00:11:56.276 --> 00:12:00.166
to read and write to texture
from any function type.

00:12:01.526 --> 00:12:05.996
Function buffer read-writes
is available on iOS

00:12:06.306 --> 00:12:09.446
with a 9 processor and macOS.

00:12:10.116 --> 00:12:13.256
Function texture read-writes
is available on macOS.

00:12:13.256 --> 00:12:17.766
So let's talk about function
buffered read-writes.

00:12:18.276 --> 00:12:19.306
So what's new here?

00:12:19.596 --> 00:12:21.896
What's new is the
ability to write to buffer

00:12:21.896 --> 00:12:25.816
from fragment function as well
as using an atomic operation

00:12:25.816 --> 00:12:27.466
in the text and fragment
function.

00:12:28.226 --> 00:12:31.026
These can be used to
implement such things

00:12:31.026 --> 00:12:35.186
as order-independent
transparency, building lists

00:12:35.186 --> 00:12:37.586
of lights that affect
the given tile,

00:12:38.036 --> 00:12:39.546
or simply to debug your shaders.

00:12:41.306 --> 00:12:43.676
So let's look at
the simple example.

00:12:44.016 --> 00:12:47.196
Let's say we want to
write the position

00:12:47.516 --> 00:12:49.566
of the visible fragments
we are rendering.

00:12:50.096 --> 00:12:51.706
It could look like this.

00:12:52.506 --> 00:12:54.006
So we have a fragment function

00:12:54.596 --> 00:12:56.286
to which we pass
an output buffer.

00:12:56.286 --> 00:12:58.856
The output buffer is
where we are going

00:12:58.856 --> 00:13:02.376
to store the position
of the fragments.

00:13:02.596 --> 00:13:07.346
Then we have a counter, so
another buffer that we start

00:13:07.346 --> 00:13:10.106
after [inaudible] that we
use to find the position

00:13:10.426 --> 00:13:12.046
into the buffer,
the first buffer,

00:13:12.046 --> 00:13:14.126
to which we want to write.

00:13:15.166 --> 00:13:18.616
We can then use an atomic
preparation to count the number

00:13:18.616 --> 00:13:20.806
of fragments with that
has been already written

00:13:20.806 --> 00:13:22.396
to get an index in the buffer.

00:13:22.926 --> 00:13:25.886
And then we can write into
the buffer the position

00:13:25.886 --> 00:13:26.606
of the fragments.

00:13:27.016 --> 00:13:30.926
So this looks pretty good,
but there is a small problem.

00:13:32.416 --> 00:13:36.296
The depth and stencil
test when you're writing

00:13:36.296 --> 00:13:38.786
to buffer is actually
always exhibited

00:13:38.786 --> 00:13:39.956
after the fragment shader.

00:13:40.646 --> 00:13:44.426
So this is a problem
because we are going

00:13:44.426 --> 00:13:46.406
to still perform the
rights to the buffer,

00:13:46.406 --> 00:13:47.566
which is not what we want.

00:13:47.566 --> 00:13:49.206
We only want the
visible fragments.

00:13:50.116 --> 00:13:52.436
It's also something to be aware

00:13:52.436 --> 00:13:54.426
of because it will
impact your performance.

00:13:54.496 --> 00:13:57.336
That means we don't have any
early Z optimization here,

00:13:57.416 --> 00:13:59.686
so we are going to
exhibit fragment shader

00:13:59.686 --> 00:14:01.756
when we probably
wouldn't want to.

00:14:02.936 --> 00:14:06.326
Fortunately we have a new
function qualifier early

00:14:06.326 --> 00:14:09.846
fragment test that can be
used to force the depth

00:14:09.846 --> 00:14:12.906
and stencil test to appear
before the fragment shader.

00:14:12.906 --> 00:14:18.446
As a result, if the depth test
fail, we will skip the execution

00:14:18.446 --> 00:14:22.096
of the fragment shader and
thus not write to the buffer.

00:14:22.096 --> 00:14:26.526
So this is what we need here,
to reach the final function

00:14:27.126 --> 00:14:30.296
with the early fragment test
attribute which otherwise

00:14:30.296 --> 00:14:34.766
to only execute the function
when the fragments are visible.

00:14:34.766 --> 00:14:39.976
Now let's talk about
function texture read-writes.

00:14:40.826 --> 00:14:45.426
So what's new is the ability to
write to texture from the vertex

00:14:45.426 --> 00:14:49.806
and fragment functions as well
as the ability to read and write

00:14:50.516 --> 00:14:52.606
to a texture from
a single function.

00:14:53.376 --> 00:14:55.836
This can be used, for
instance, to save memory

00:14:55.836 --> 00:14:57.916
when implementing post
processing effects

00:14:58.376 --> 00:15:01.046
by using the same texture
on both input and output.

00:15:04.156 --> 00:15:06.056
So writing to texture
is fairly simple.

00:15:06.476 --> 00:15:09.866
You just define your texture
with the access qualifier write,

00:15:09.866 --> 00:15:12.656
and then you can
write to your texture.

00:15:13.266 --> 00:15:18.786
Read-write texture, a texture
to which you can both --

00:15:18.786 --> 00:15:21.486
that you can both read
and write in your shader.

00:15:21.486 --> 00:15:25.486
Only a limited number of formats
is reported for those textures.

00:15:26.316 --> 00:15:29.466
To use the read-write texture
you will use the access

00:15:29.466 --> 00:15:34.376
qualifier of read-write, and
then you can read to the texture

00:15:35.236 --> 00:15:39.146
and write to it in your shader.

00:15:39.146 --> 00:15:41.546
However, you have to be careful
when you write to the texture

00:15:41.546 --> 00:15:44.376
if you want to read the results,

00:15:44.376 --> 00:15:47.566
if you want to read the same
pixel again in your shader.

00:15:48.046 --> 00:15:51.086
In this case, you need
to use a texture fence.

00:15:51.596 --> 00:15:53.186
The texture fence will ensure

00:15:53.186 --> 00:15:56.106
that the writes have
been committed to memory

00:15:56.106 --> 00:15:58.146
so that you can read
the proper value.

00:15:59.366 --> 00:16:06.086
Here, we write to a given pixel,
and then we use a texture fence

00:16:06.216 --> 00:16:08.916
to make sure we can
read that value again

00:16:09.176 --> 00:16:11.546
and then we can finally
read the value.

00:16:11.546 --> 00:16:14.996
We should also be
careful with texture fence

00:16:14.996 --> 00:16:17.686
because they only apply
on a single SIMD thread,

00:16:18.786 --> 00:16:20.886
which means that if you have
two threads that are writing

00:16:20.886 --> 00:16:23.876
to a texture and the
second thread is trying

00:16:23.956 --> 00:16:28.386
to read the value that was
written by the first thread,

00:16:29.666 --> 00:16:32.806
even after a texture
fence, this will not work.

00:16:33.786 --> 00:16:38.886
What will work is if each thread
is reading the pixel values

00:16:38.886 --> 00:16:41.426
that it was writing
to but not the ones

00:16:41.426 --> 00:16:42.576
that are written
by other threads.

00:16:43.736 --> 00:16:47.376
So one note about reading,
we talked a lot about writing

00:16:47.376 --> 00:16:48.806
to buffers and textures.

00:16:49.526 --> 00:16:51.706
With vertex and fragment
functions,

00:16:51.786 --> 00:16:52.826
you have to be careful.

00:16:53.996 --> 00:16:56.736
In this example, fragment
function is trying to write --

00:16:56.736 --> 00:16:58.646
is writing to a buffer

00:16:58.646 --> 00:17:01.066
and a vertex function is
trying to read the results.

00:17:01.066 --> 00:17:02.476
However, this is
not going to work

00:17:02.476 --> 00:17:05.076
because of having the
same RenderCommandEncoder.

00:17:05.965 --> 00:17:10.336
To fix this, we need to use
two RenderCommandEncoder.

00:17:11.435 --> 00:17:13.326
The fragment function
writes to the buffer

00:17:13.326 --> 00:17:16.246
in the first
RenderCommandEncoder while the

00:17:16.246 --> 00:17:17.386
texture -- the vertex function

00:17:17.386 --> 00:17:19.056
in the second
RenderCommandEncoder can finally

00:17:19.056 --> 00:17:20.996
read the result and
get proper results.

00:17:21.606 --> 00:17:24.816
You should note that
with compute shader,

00:17:24.816 --> 00:17:26.715
this is not necessary.

00:17:26.715 --> 00:17:29.036
It can be done the same
compute CommandEncoder.

00:17:30.256 --> 00:17:32.876
So in summary, we
introduced two new features,

00:17:33.406 --> 00:17:36.626
function buffer read-writes and
function texture read-writes.

00:17:36.956 --> 00:17:40.686
You can use early fragment
tests to make sure the depth

00:17:40.686 --> 00:17:44.086
and stencil test is done
because the execution

00:17:44.086 --> 00:17:44.856
of the fragment shader.

00:17:45.856 --> 00:17:48.616
You should use a texture fence
if you are trying to read data

00:17:48.616 --> 00:17:52.526
from a read-write texture
that you have been writing to.

00:17:52.826 --> 00:17:56.276
And finally, when using vertex
and fragment shader to write

00:17:56.276 --> 00:17:58.716
to buffers, you need
to make sure

00:17:58.716 --> 00:18:00.486
to use a different
RenderCommandEncoder

00:18:00.486 --> 00:18:01.926
when you want to
read the results.

00:18:03.276 --> 00:18:08.416
So with this, I will hand the
stage to Dan Omachi to talk

00:18:08.416 --> 00:18:09.596
to you about wide color.

00:18:10.516 --> 00:18:13.546
[ Applause ]

00:18:14.046 --> 00:18:14.466
>> Thank you, Charles.

00:18:14.466 --> 00:18:14.776
Thank you.

00:18:15.646 --> 00:18:17.416
As Charles mentioned,
my name is Dan Omachi.

00:18:17.766 --> 00:18:20.746
I work as an engineer in Apple's
GPU Software Frameworks Team

00:18:21.336 --> 00:18:22.896
and I'd like to start
off talking to you

00:18:22.896 --> 00:18:25.716
about color management,
which isn't a topic

00:18:27.256 --> 00:18:31.016
that all developers are
actually familiar with.

00:18:32.266 --> 00:18:36.476
So if you are an
artist at either the --

00:18:36.476 --> 00:18:39.926
either a texture artist
creating assets for a game

00:18:40.416 --> 00:18:43.336
or a photographer editing
photos for distribution,

00:18:44.156 --> 00:18:46.276
you would have a particular
color scheme in mind,

00:18:46.276 --> 00:18:49.506
and you'd choose
colors pretty carefully.

00:18:50.366 --> 00:18:55.376
And you'd want consistency
regardless of the display

00:18:55.796 --> 00:18:57.556
on which your content is viewed.

00:18:58.576 --> 00:19:00.686
Now it's our responsibility
as developers

00:19:00.686 --> 00:19:04.106
and software engineers to
guarantee that consistency.

00:19:04.386 --> 00:19:08.616
If you're using a high level
framework like SceneKit,

00:19:08.846 --> 00:19:12.866
SpriteKit, or Core Graphics,
much of this work is done

00:19:12.866 --> 00:19:14.416
for you, and you

00:19:14.416 --> 00:19:17.336
as app developers don't
need to think about it.

00:19:17.946 --> 00:19:20.786
Metal, however, is a
much lower level API.

00:19:22.806 --> 00:19:26.086
This offers increased
performance and some flexibility

00:19:26.086 --> 00:19:30.086
but also places some of this
responsibility in your hands.

00:19:32.096 --> 00:19:32.696
So why now?

00:19:33.826 --> 00:19:36.066
You've been able to
use different displays

00:19:36.446 --> 00:19:37.916
with different color spaces

00:19:38.456 --> 00:19:40.456
with Apple devices
for many years now.

00:19:41.786 --> 00:19:46.246
Well, late last year, Apple
introduced a couple of iMacs

00:19:46.656 --> 00:19:50.256
with a display capable
of rendering colors

00:19:50.536 --> 00:19:52.296
in the P3 color space.

00:19:52.846 --> 00:19:57.956
And in April, we introduced
the 9.7-inch iPad Pro,

00:19:58.436 --> 00:20:00.566
which also has a P3 display.

00:20:00.566 --> 00:20:03.726
So what is the P3 color space?

00:20:04.196 --> 00:20:06.326
Well, this is a chromaticity
diagram,

00:20:06.496 --> 00:20:10.266
and conceptually this
represents all of the colors

00:20:10.346 --> 00:20:13.746
in the visual spectrum, in
other words, all the colors

00:20:13.906 --> 00:20:15.816
that the normal human
eye can see.

00:20:17.666 --> 00:20:20.786
Of that, within this
triangle are colors

00:20:21.116 --> 00:20:25.706
that a standard sRGB
display can represent.

00:20:26.336 --> 00:20:32.406
The P3 display is able
to represent colors

00:20:32.406 --> 00:20:35.106
of a much broader variety.

00:20:36.676 --> 00:20:39.146
So here's how it works on macOS.

00:20:41.186 --> 00:20:44.116
We want you to be able to
render in any color space

00:20:45.696 --> 00:20:50.076
and as I mentioned, high level
frameworks take care of this,

00:20:50.076 --> 00:20:52.116
this job of color
management for you

00:20:52.396 --> 00:20:54.756
by performing an operation
called color matching

00:20:55.066 --> 00:20:58.516
where your color and one
color space is matched to that

00:20:58.516 --> 00:21:01.816
of the display color space
so that the same intensity

00:21:02.106 --> 00:21:04.286
on the display regardless
of the color space

00:21:04.286 --> 00:21:06.716
that you're working
in is displayed.

00:21:08.276 --> 00:21:13.626
Now, Metal views by default
are not color managed.

00:21:14.516 --> 00:21:16.176
This color match
operation is skipped,

00:21:16.286 --> 00:21:20.546
and this generally offers
increased performance.

00:21:21.596 --> 00:21:25.966
So by default, you're
ignoring the color profile

00:21:25.966 --> 00:21:28.806
of the display, and therefore,

00:21:28.806 --> 00:21:33.476
the display will interpret
colors in its own color space.

00:21:34.966 --> 00:21:38.326
Now, this means that sRGB
colors will be interpreted

00:21:38.326 --> 00:21:42.296
as P3 colors, and rendering will
be inconsistent between the two.

00:21:42.516 --> 00:21:46.616
So if this is your application
with an sRGB drawable

00:21:47.576 --> 00:21:52.916
and this is the display, well,
when you call present drawable,

00:21:53.556 --> 00:21:56.356
these colors become
much saturated.

00:21:57.036 --> 00:21:57.906
So why does this happen?

00:21:58.166 --> 00:22:01.086
Well, let's go back to
our chromaticity diagram.

00:22:02.076 --> 00:22:06.476
This is the most green
color that you can represent

00:22:06.526 --> 00:22:11.356
in the sRGB color space,
and in a fragment shader,

00:22:12.156 --> 00:22:15.636
you'd represent this as
0.0 in the red channel,

00:22:15.906 --> 00:22:20.036
1.0 in the green channel
and 0.0 in the blue channel.

00:22:20.856 --> 00:22:24.746
Well, the P3 Display
just takes that raw value

00:22:24.746 --> 00:22:25.926
and interprets it,

00:22:25.926 --> 00:22:29.716
and it basically thinks
that it's a P3 color.

00:22:30.316 --> 00:22:34.806
So you're getting the most
green color of a P3 Display,

00:22:35.086 --> 00:22:37.416
which happens to be a
different green color.

00:22:38.326 --> 00:22:43.546
Now, for content creation
apps, it's pretty critical

00:22:43.546 --> 00:22:48.266
that you get this right because
artists have used careful

00:22:48.266 --> 00:22:51.246
consideration to
render their colors.

00:22:51.246 --> 00:23:00.696
For games, the effect is more
subtle, but if your designers

00:23:01.066 --> 00:23:07.406
and artists are looking for this
dark and gritty theme, well,

00:23:07.406 --> 00:23:10.296
they're going to be disappointed
when it looks much more cheerful

00:23:10.296 --> 00:23:12.616
and happy when you
plug in a P3 Display.

00:23:13.816 --> 00:23:16.636
Also, this problem can get worse

00:23:16.976 --> 00:23:20.966
as the industry moves towards
even wider gamut displays.

00:23:22.816 --> 00:23:27.956
So, the solution is
really quite simple.

00:23:28.926 --> 00:23:33.696
You enable color management
on the NSWindow or CAMetal

00:23:33.806 --> 00:23:36.696
by setting the color space
to your working color space,

00:23:36.696 --> 00:23:38.776
probably the sRGB color space.

00:23:39.446 --> 00:23:43.146
This causes the OS to
perform a color match as part

00:23:43.146 --> 00:23:48.656
of its window server's
normal compositing pass.

00:23:48.786 --> 00:23:51.136
So if here's your
display, or excuse me,

00:23:51.136 --> 00:23:53.286
here's your application
with sRGB drawable

00:23:53.696 --> 00:23:54.746
and here's the display,

00:23:56.506 --> 00:23:59.406
the window server takes your
drawable when you call present

00:23:59.406 --> 00:24:06.936
and performs the color match
before slapping it on the glass.

00:24:07.056 --> 00:24:09.656
Now, all right, so now
you've got that consistency.

00:24:09.776 --> 00:24:12.346
What if you want to
adopt wide color?

00:24:13.066 --> 00:24:18.876
You want to purposefully render
those more intense colors a wide

00:24:18.996 --> 00:24:21.116
gamut display is only
capable of rendering.

00:24:21.746 --> 00:24:25.206
Well, first of all, you
need to create some content.

00:24:25.206 --> 00:24:27.716
You need your artist to
create wider content,

00:24:28.856 --> 00:24:33.826
and for that we recommend
using the extended range sRGB

00:24:33.896 --> 00:24:36.166
color space.

00:24:37.436 --> 00:24:41.796
This allows existing assets that
aren't offered for wide color

00:24:42.036 --> 00:24:44.106
to continue working
as they have,

00:24:44.666 --> 00:24:47.776
and your shader pipelines don't
need to do anything different.

00:24:49.186 --> 00:24:53.896
However, your artists can
create new wider color assets

00:24:54.346 --> 00:24:56.606
that will provide much
more intense colors.

00:24:57.946 --> 00:25:03.356
So what exactly is the
extended range sRGB?

00:25:03.356 --> 00:25:08.666
Well here's the sRGB
triangle and here's P3.

00:25:10.946 --> 00:25:13.726
Extended range sRGB
just goes out infinitely

00:25:14.116 --> 00:25:19.436
in all directions, meaning
values outside of 0 to 1

00:25:19.736 --> 00:25:24.076
in your shader represent
values that can only be viewed

00:25:24.466 --> 00:25:28.316
on a wider than sRGB
color display.

00:25:30.516 --> 00:25:33.966
So I mentioned values
outside of 0 to 1.

00:25:34.406 --> 00:25:37.826
This means that you will need to
use floating point pixel formats

00:25:38.066 --> 00:25:43.016
to express such values, and for
source textures we recommend a

00:25:43.436 --> 00:25:44.516
couple of formats.

00:25:45.136 --> 00:25:48.156
You can use the BC6H
floating point format.

00:25:48.486 --> 00:25:50.916
It's a compressed format
offering high performance

00:25:51.116 --> 00:25:54.456
as well as the pack float
and shared exponent formats.

00:25:55.406 --> 00:25:59.216
For your render targets, you
can use this pack float format

00:25:59.866 --> 00:26:04.436
or the RGBA half-float
format, allowing you

00:26:04.596 --> 00:26:07.256
to specify these
more intense colors.

00:26:09.346 --> 00:26:12.016
Color management on
iOS is a bit simpler.

00:26:12.956 --> 00:26:15.816
You always render in
the sRGB color space,

00:26:17.446 --> 00:26:19.526
even when targeting
a P3 Display.

00:26:19.526 --> 00:26:22.846
Colors are automatically matched
with no performance penalty.

00:26:24.206 --> 00:26:28.346
And if you want to use wide
colors, you can make use

00:26:28.406 --> 00:26:29.856
of some new pixel formats

00:26:31.046 --> 00:26:33.516
that are natively
readable by the display.

00:26:34.106 --> 00:26:36.466
There's no compositing
operation that needs to happen.

00:26:37.826 --> 00:26:40.656
They can be gamma encoded,
offering better blacks

00:26:40.656 --> 00:26:43.716
and allowing you to do linear
blending in your shaders,

00:26:44.566 --> 00:26:47.796
and they're efficient for
use as source textures.

00:26:48.106 --> 00:26:49.176
All right.

00:26:49.176 --> 00:26:52.096
Here are the bit layouts
of these new formats.

00:26:52.306 --> 00:26:56.386
So, there are -- there
is a 32-bit RGB format

00:26:56.806 --> 00:27:01.556
with 10 bits per channel
and also an RGBA format

00:27:01.876 --> 00:27:05.096
with 10 bits per channel
spread across 64 bits.

00:27:05.876 --> 00:27:11.016
Now, this, the values
of this 10 bits are --

00:27:11.016 --> 00:27:13.276
can express values
outside of 0 to 1.

00:27:13.686 --> 00:27:20.276
Values from 0 to 384 represent
negative values, 384 to 894,

00:27:20.576 --> 00:27:24.506
the next 510 values, represent
values between 0 and 1

00:27:24.826 --> 00:27:28.416
and those greater than
894 represent these more

00:27:28.416 --> 00:27:29.186
intense values.

00:27:30.386 --> 00:27:37.426
Now, note here that the RGBA
pixel format is twice as large

00:27:37.426 --> 00:27:40.406
and therefore uses twice
as much memory and twice

00:27:40.406 --> 00:27:43.756
as much bandwidth
as this RGB format.

00:27:44.746 --> 00:27:49.096
So, in general, we recommend
that you use this only

00:27:49.096 --> 00:27:53.396
in the CAMetal Layer if
you need destination alpha.

00:27:54.056 --> 00:27:57.986
All right, so you've made
the decision that you want

00:27:57.986 --> 00:28:00.296
to create some wide
gamut content.

00:28:00.946 --> 00:28:03.456
How can you do this?

00:28:03.456 --> 00:28:04.686
Well, you have an artist --

00:28:05.606 --> 00:28:09.156
author using image
editor on macOS,

00:28:09.156 --> 00:28:13.376
which supports the P3 color
space, such as Adobe Photoshop.

00:28:14.256 --> 00:28:19.496
You can save that image as
a 16-bit per channel PNG

00:28:19.496 --> 00:28:24.376
or JPEG using the
display P3 color profile.

00:28:24.776 --> 00:28:26.546
Now, once you've got this image,

00:28:27.026 --> 00:28:28.926
how do you create
textures from it?

00:28:29.766 --> 00:28:30.976
Well, you've got
two solutions here.

00:28:31.566 --> 00:28:35.616
The first is you can create your
own asset conditioning tool,

00:28:36.306 --> 00:28:42.596
and from that 16-bit per channel
Display P3 image you can convert

00:28:43.046 --> 00:28:49.556
using the extended sRGB floating
point color space using either

00:28:49.556 --> 00:28:51.526
the ImageIO or vImage
frameworks.

00:28:52.106 --> 00:28:55.836
And then from that on
macOS, you'd convert to one

00:28:55.836 --> 00:28:58.186
of those floating point pixel
formats I mentioned earlier,

00:28:58.726 --> 00:29:00.576
and on iOS you'd convert to one

00:29:00.576 --> 00:29:03.196
of those extended range pixel
formats I just mentioned.

00:29:03.196 --> 00:29:06.206
All right, so that's option one

00:29:06.206 --> 00:29:08.396
if you really want
explicit control

00:29:08.396 --> 00:29:09.746
of how your textures are built.

00:29:11.636 --> 00:29:14.546
The next option is
to use Xcode support

00:29:14.746 --> 00:29:16.806
for textures in asset
catalogues.

00:29:17.536 --> 00:29:21.246
With that, will automatically
create extended range sRGB

00:29:21.246 --> 00:29:24.236
textures for devices
with a P3 Display,

00:29:24.236 --> 00:29:26.756
and I'll talk a little bit more

00:29:26.756 --> 00:29:28.886
about asset catalogues
right now.

00:29:29.506 --> 00:29:36.006
So for a while now you've been
able to put icons and images

00:29:36.006 --> 00:29:39.336
into an asset catalogue
within your Xcode project.

00:29:40.706 --> 00:29:45.156
Last year, we introduced app
thinning whereby you can create

00:29:45.156 --> 00:29:46.406
a specialized version

00:29:46.806 --> 00:29:50.076
for various devices based
upon device capability

00:29:50.076 --> 00:29:53.396
such as the amount of memory,
the graphics features set,

00:29:54.186 --> 00:30:00.746
or the type of device, whether
it be an iPad, Mac or TV

00:30:00.866 --> 00:30:03.676
or watch or even
phone, of course.

00:30:05.006 --> 00:30:08.906
And when your app was
downloaded, you download

00:30:08.906 --> 00:30:12.486
and install only the single
version of that assess made

00:30:12.486 --> 00:30:15.806
for that device with the
capabilities you specified.

00:30:16.446 --> 00:30:20.476
The asset was compressed over
the wire and on the device,

00:30:20.766 --> 00:30:24.556
saving a lot of storage
on the user's device,

00:30:25.366 --> 00:30:28.856
and there were numerous APIs,

00:30:28.856 --> 00:30:31.676
which offer efficient
access to those assets.

00:30:32.696 --> 00:30:36.856
So now we've added texture
sets to these asset catalogues.

00:30:37.726 --> 00:30:39.446
So what does this offer?

00:30:39.446 --> 00:30:41.796
Well, storage for mipmap levels.

00:30:42.136 --> 00:30:45.466
Textures are more
than just 2D images.

00:30:46.656 --> 00:30:50.646
You can perform offline mipmap
generation within Xcode,

00:30:51.986 --> 00:30:55.106
will automatically color
match this texture.

00:30:55.416 --> 00:30:59.656
So if it's a wide gamut texture
in some different color space,

00:30:59.996 --> 00:31:04.616
will perform a color
matching operation to the sRGB

00:31:04.616 --> 00:31:07.246
or extended range
sRGB color space.

00:31:07.776 --> 00:31:13.306
And I think the most important
feature of this ability here is

00:31:13.536 --> 00:31:16.706
that we can choose the
most optimal pixel format

00:31:17.136 --> 00:31:20.446
for every device on
which your app can run.

00:31:20.756 --> 00:31:25.026
So on newer devices that support
ASTC texture compression,

00:31:25.756 --> 00:31:27.346
we can use that format.

00:31:27.866 --> 00:31:29.956
On older devices which
don't support that,

00:31:30.176 --> 00:31:33.406
we can choose either
a noncompressed format

00:31:33.756 --> 00:31:35.526
or some other compressed format.

00:31:36.526 --> 00:31:38.956
Additionally, we can
choose a wide color format

00:31:39.556 --> 00:31:44.786
for devices with a P3 Display.

00:31:44.916 --> 00:31:46.136
So here's the basic workflow.

00:31:47.486 --> 00:31:51.156
You create texture
sets within Xcode.

00:31:51.606 --> 00:31:54.996
You assign a name to the
set, a unique identifier.

00:31:56.236 --> 00:31:59.116
You'll add an image and
indicate basically how

00:31:59.116 --> 00:32:01.676
that texture will be used,
whether it's a color texture

00:32:02.136 --> 00:32:07.516
or some other type of data like
a normal map or a height map.

00:32:07.516 --> 00:32:11.616
Then, you'll -- can
create this texture.

00:32:11.616 --> 00:32:13.966
Xcode will build this texture

00:32:13.966 --> 00:32:15.526
and deliver it to
your application.

00:32:15.656 --> 00:32:19.646
Now, you can create these
texture sets via the Xcode UI

00:32:19.646 --> 00:32:21.296
or programmatically.

00:32:21.836 --> 00:32:26.876
Once your texture is on the
device, you can supply the name

00:32:27.026 --> 00:32:30.966
to MetalKit, and MetalKit
will build a texture,

00:32:31.086 --> 00:32:34.666
a Metal texture,
from that asset.

00:32:35.576 --> 00:32:38.806
So I'd like to walk you
through the Xcode workflow

00:32:38.986 --> 00:32:41.436
to introduce some of
these concepts to you.

00:32:43.136 --> 00:32:48.386
So, you'll first select
the asset catalogue

00:32:48.526 --> 00:32:51.736
in your projects
navigator sidebar

00:32:52.326 --> 00:32:56.196
and then hit this plus button
here, which brings up this menu.

00:32:56.606 --> 00:33:00.436
Now, here's where you can create
the various types of sets.

00:33:00.436 --> 00:33:06.686
There are image sets, icon
sets, generic data sets,

00:33:06.686 --> 00:33:11.506
as well as texture and
cube map texture sets.

00:33:12.806 --> 00:33:15.366
So once you've created
your texture set,

00:33:15.956 --> 00:33:17.726
you need to name it.

00:33:18.076 --> 00:33:20.956
Now, your naming
hierarchy need not be flat.

00:33:21.386 --> 00:33:24.526
If you have a number of textures
that are called base texture,

00:33:24.716 --> 00:33:27.956
one for each object, you can
create a folder for each object

00:33:28.246 --> 00:33:31.776
and stuff your base texture
for that object in that folder,

00:33:32.806 --> 00:33:36.326
and your hierarchy can be
as complex as you'd like.

00:33:37.266 --> 00:33:43.166
You add your image, and then
you set the interpretation.

00:33:43.716 --> 00:33:45.226
Now there are three
options here.

00:33:45.756 --> 00:33:49.866
Color, in color NonPremultiplied
perform this color

00:33:49.866 --> 00:33:51.106
match operation.

00:33:52.066 --> 00:33:56.136
The NonPremultiplied option
will multiply the alpha channel

00:33:56.196 --> 00:33:57.726
by your R, B, and G --

00:33:57.936 --> 00:34:01.196
RGB channels before
building the texture.

00:34:02.096 --> 00:34:08.716
The data option here will
-- is used for normal maps,

00:34:08.716 --> 00:34:15.005
height maps, roughness maps,
textures of noncolor type.

00:34:15.916 --> 00:34:17.576
Now, this is all you need to do.

00:34:18.136 --> 00:34:21.136
Xcode will go off and
build various versions

00:34:21.136 --> 00:34:26.536
of this texture, and it
will pick the most optimal

00:34:26.536 --> 00:34:27.206
pixel format.

00:34:28.966 --> 00:34:33.466
You can, however, have
more explicit control.

00:34:33.826 --> 00:34:35.846
You can select any number
of these traits here,

00:34:37.366 --> 00:34:39.516
which will open up
a number of buckets

00:34:39.516 --> 00:34:40.946
that you can select
to customize.

00:34:41.906 --> 00:34:44.826
You can add different
images for each version.

00:34:45.196 --> 00:34:47.516
You probably wouldn't
use a different image,

00:34:47.616 --> 00:34:49.906
but may be a different
size of an image.

00:34:49.906 --> 00:34:53.005
So on a device with
lots of memory,

00:34:53.036 --> 00:34:55.536
you can use a bigger
texture, and a device

00:34:55.536 --> 00:34:58.526
with a smaller memory, you would
use a much smaller texture.

00:34:58.986 --> 00:35:05.226
And then you can specify how
or whether you want mipmaps.

00:35:05.896 --> 00:35:08.826
The all option will
generate mipmaps all the way

00:35:08.826 --> 00:35:14.436
down to the 1 by 1 level and the
fixed option here will give you

00:35:14.436 --> 00:35:17.866
some more explicit control,
such as whether you want

00:35:17.866 --> 00:35:21.406
to use a max level and
also whether you want

00:35:21.406 --> 00:35:24.366
to have different
images for each level.

00:35:24.936 --> 00:35:29.456
And finally, you can override
our automatic selection

00:35:30.026 --> 00:35:33.226
of pixel formats.

00:35:33.226 --> 00:35:37.266
Now I mentioned that you can
programmatically create these

00:35:37.266 --> 00:35:38.036
texture sets.

00:35:38.856 --> 00:35:40.886
You don't really want to
go through the Xcode UI

00:35:41.396 --> 00:35:44.036
if you've got thousands
of assets.

00:35:44.926 --> 00:35:48.336
So there's a pretty
simple directory structure,

00:35:48.336 --> 00:35:50.686
and within that directory
structure are a number

00:35:50.686 --> 00:35:52.036
of JSON files.

00:35:52.636 --> 00:35:57.806
Now these files and directory
structure is fully documented

00:35:58.056 --> 00:36:02.146
on the asset catalogue
reference.

00:36:02.656 --> 00:36:05.216
So you can create your own
asset conditioning tool

00:36:05.446 --> 00:36:08.906
to set up your texture set.

00:36:09.876 --> 00:36:12.726
So once you've got this
asset on the device,

00:36:12.726 --> 00:36:14.156
how do you make use of it?

00:36:14.326 --> 00:36:19.086
Well, you create a MetalKit
texture loader supplying your

00:36:19.086 --> 00:36:24.136
Metal device, and then
you supply the name along

00:36:24.136 --> 00:36:26.796
with its hierarchy
to the texture loader

00:36:27.216 --> 00:36:29.396
and MetalKit will go off
and build that texture.

00:36:29.696 --> 00:36:32.636
You can supply a couple
of other options here

00:36:32.636 --> 00:36:34.976
such as scale factor if
you have different versions

00:36:34.976 --> 00:36:39.976
of the texture for different
scale factors or the bundle

00:36:40.046 --> 00:36:42.446
if the asset catalogue is

00:36:42.496 --> 00:36:44.266
in something other
than the main bundle.

00:36:44.266 --> 00:36:45.406
There are also a couple

00:36:45.406 --> 00:36:49.356
of options here that
you can specify.

00:36:49.726 --> 00:36:53.226
So I'd really like you to
pay attention to color space

00:36:53.586 --> 00:36:55.586
and set your apps apart

00:36:55.676 --> 00:36:59.896
by creating content
with wide color.

00:37:00.886 --> 00:37:04.306
Asset catalogues can help
you achieve that goal.

00:37:04.306 --> 00:37:07.556
As well, they provide a
number of other features

00:37:07.826 --> 00:37:09.556
which you can make use of,

00:37:09.646 --> 00:37:11.876
such as optimal pixel
format selection.

00:37:12.786 --> 00:37:18.386
I'd like to have my colleague
Anna Tikhonova up here to talk

00:37:18.386 --> 00:37:19.876
about some exciting improvements

00:37:20.146 --> 00:37:21.976
to the Metal Performance
Shaders framework.

00:37:22.516 --> 00:37:27.796
[ Applause ]

00:37:28.296 --> 00:37:29.616
>> Hi. Good afternoon.

00:37:30.196 --> 00:37:31.726
Thank you, Dan, for
the introduction.

00:37:31.846 --> 00:37:33.266
As Dan said, my name is Anna.

00:37:33.366 --> 00:37:35.416
I'm an engineer on
the GPU Software Team.

00:37:35.506 --> 00:37:37.386
So let's talk about
some new additions

00:37:37.456 --> 00:37:38.686
to the Metal Performance
Shaders.

00:37:40.686 --> 00:37:43.336
We introduced the Metal
Performance Shaders framework

00:37:43.336 --> 00:37:45.766
last year in the What's
New in Metal Part 2 talk.

00:37:46.006 --> 00:37:47.636
If you haven't seen
that session,

00:37:47.746 --> 00:37:49.256
you should definitely
check out the video.

00:37:50.336 --> 00:37:52.096
But just to give
you a quick recap,

00:37:52.626 --> 00:37:55.176
the Metal Performance Shaders
framework is the framework

00:37:55.176 --> 00:37:58.436
of optimized high performance
data parallel algorithms

00:37:58.436 --> 00:37:59.536
for the GPU in Metal .

00:38:00.896 --> 00:38:02.866
The algorithms are
optimized for iOS,

00:38:03.056 --> 00:38:06.036
and they have been available
for you since iOS 9, for the A8

00:38:06.036 --> 00:38:07.836
and now the A9 processors.

00:38:08.846 --> 00:38:12.066
The framework is designed
to integrate easily

00:38:12.176 --> 00:38:15.386
into your Metal applications
and be very simple to use.

00:38:16.536 --> 00:38:19.436
It should be as simple as
calling a library function.

00:38:20.066 --> 00:38:23.776
So last year, we talked
about following a list

00:38:23.776 --> 00:38:27.796
of supported image operations,
and you should watch the video

00:38:27.986 --> 00:38:30.066
for lots of details
and examples.

00:38:30.716 --> 00:38:32.996
But this year, we've added
some more cool stuff for you.

00:38:34.396 --> 00:38:36.846
We've added wide color
conversion, which you can use

00:38:36.846 --> 00:38:39.596
to convert your Metal textures
between different color spaces.

00:38:40.456 --> 00:38:45.326
You can convert between
RGB, sRGB, grayscale, CMYK,

00:38:45.636 --> 00:38:49.106
C3 and any color
space you define.

00:38:49.826 --> 00:38:53.536
We've also added Gaussian
pyramids, which you can use

00:38:53.536 --> 00:38:56.146
to create multiscaler
presentations of image data

00:38:56.386 --> 00:38:58.796
on the GPU to enable
multiscale algorithms.

00:38:59.726 --> 00:39:03.226
They can also be used for
common optical flow algorithms,

00:39:03.886 --> 00:39:06.666
image blending, and
high-quality mipmap generation.

00:39:08.006 --> 00:39:11.196
And finally, we've added
convolutional neural networks,

00:39:11.406 --> 00:39:13.166
or CNNs, which are used

00:39:13.356 --> 00:39:15.476
to accelerate deep
learning algorithms.

00:39:16.376 --> 00:39:18.286
This is going to be the
main topic of this talk.

00:39:18.586 --> 00:39:20.046
So let's just dive right in.

00:39:20.636 --> 00:39:23.026
First of all, what
is deep learning?

00:39:24.336 --> 00:39:27.436
Deep learning is a field of
machine learning which goal is

00:39:27.436 --> 00:39:28.356
to answer this question.

00:39:28.666 --> 00:39:31.896
Can a machine do the same
task that a human can do?

00:39:32.346 --> 00:39:34.346
Well, what types of
tasks am I talking about?

00:39:35.176 --> 00:39:36.946
Each one of you has an
iPhone in your pocket.

00:39:37.116 --> 00:39:38.996
You probably took a
few pictures today,

00:39:39.436 --> 00:39:42.896
and all of us are constantly
exposed to images and videos

00:39:42.896 --> 00:39:46.076
on the Web every day, on
news sites, on social media.

00:39:47.436 --> 00:39:50.866
When you see an image, you
know instantly what is depicted

00:39:50.866 --> 00:39:51.136
on it.

00:39:51.796 --> 00:39:53.006
You can detect faces.

00:39:53.496 --> 00:39:55.206
If you know these
people, you can tag them.

00:39:55.366 --> 00:39:56.606
You can annotate this image.

00:39:56.936 --> 00:39:59.076
And this works well
for a single image,

00:40:00.006 --> 00:40:03.246
but what if you have more
images and even more images?

00:40:03.416 --> 00:40:06.946
Think about all of the images
uploaded to the Web every day.

00:40:07.716 --> 00:40:10.116
No human can hand
annotate this many images.

00:40:10.656 --> 00:40:12.766
So deep learning is a technique

00:40:12.846 --> 00:40:14.256
for solving these
kinds of problems.

00:40:15.566 --> 00:40:18.176
It can be used for sifting
through large amounts of data

00:40:18.356 --> 00:40:21.536
and for answering questions
such as, "Who's in this image?"

00:40:21.536 --> 00:40:22.426
And "Where was it taken?"

00:40:23.026 --> 00:40:25.926
But I'm using image-based
examples in this talk

00:40:25.926 --> 00:40:26.906
because they are visual.

00:40:27.096 --> 00:40:29.846
So they are a great fit for
this type of a presentation,

00:40:30.246 --> 00:40:31.466
but I just want to mention

00:40:31.746 --> 00:40:33.656
that deep learning
algorithms can be used

00:40:33.656 --> 00:40:34.736
for other types of data.

00:40:34.956 --> 00:40:39.356
For example, other types
of signal like audio

00:40:39.506 --> 00:40:41.576
to do speech recognition
and haptics

00:40:41.666 --> 00:40:42.886
to create the sense of touch.

00:40:45.316 --> 00:40:47.436
Deep learning algorithms
have two phases.

00:40:48.186 --> 00:40:49.916
The first one is
the training phase.

00:40:50.486 --> 00:40:53.426
So let's talk about it,
give a specific example.

00:40:53.846 --> 00:40:56.296
So image that you
want train your system

00:40:56.386 --> 00:40:58.176
to categorize images
into classes.

00:40:58.696 --> 00:40:59.626
This is an image of a cat.

00:40:59.946 --> 00:41:01.026
This is an image of a dog.

00:41:01.136 --> 00:41:02.186
This is the image of a rabbit.

00:41:03.346 --> 00:41:07.186
This is a labor intensive task
that requires a large number

00:41:07.186 --> 00:41:10.596
of images, hand-labeled
annotated images

00:41:10.596 --> 00:41:12.706
for each one of these
categories.

00:41:13.846 --> 00:41:16.706
So for example, if you
want to train your system

00:41:16.706 --> 00:41:20.396
to recognize cats, you need to
feed it a large number of images

00:41:20.396 --> 00:41:24.146
of cats all labeled, and
same for your rabbits

00:41:24.226 --> 00:41:26.356
and all the other animals
that you want your system

00:41:26.356 --> 00:41:29.666
to be able to recognize.

00:41:30.256 --> 00:41:34.436
This is a one-time
computationally expensive step.

00:41:34.836 --> 00:41:36.466
It's usually done offline,
and there are plenty

00:41:36.466 --> 00:41:38.116
of training packages
available out there.

00:41:38.596 --> 00:41:42.156
The result of the training
phase is trained parameters.

00:41:42.956 --> 00:41:45.186
So I will not talk
about them right now,

00:41:45.186 --> 00:41:46.596
but we will get back
to them later.

00:41:48.246 --> 00:41:51.266
The trained parameters are
required for the next phase,

00:41:51.396 --> 00:41:52.456
which is the inference phase.

00:41:53.536 --> 00:41:56.686
This is the phase where
your system is presented

00:41:56.686 --> 00:41:59.496
with a new image that has
never seen before, and it needs

00:41:59.496 --> 00:42:00.716
to classify in real-time.

00:42:00.936 --> 00:42:03.966
So in this example, the system
correctly classified this image

00:42:03.996 --> 00:42:05.096
as an image of a cat.

00:42:06.636 --> 00:42:09.446
We provide GPU acceleration
for the inference phase.

00:42:10.076 --> 00:42:12.646
Specifically, we give
you the building blocks

00:42:12.996 --> 00:42:15.686
to build your inference
networks for the GPU.

00:42:17.266 --> 00:42:20.246
So let's now talk about what
are the convolutional neural

00:42:20.246 --> 00:42:24.976
networks and what are these
building blocks we provide?

00:42:25.496 --> 00:42:27.456
The convolutional
neural networks, or CNNs,

00:42:27.696 --> 00:42:30.706
are biologically
inspired and designed

00:42:30.706 --> 00:42:32.146
to resemble the visual cortex.

00:42:33.336 --> 00:42:36.826
When our brain processes visual
input, the first hierarchy

00:42:36.826 --> 00:42:38.686
of neurons that receive
information

00:42:38.826 --> 00:42:42.526
in the visual cortex are
sensitive to specific edges

00:42:42.526 --> 00:42:45.296
or blobs of color, while
the brain regions further

00:42:45.296 --> 00:42:49.256
down the visual pipeline respond
to more complex structures

00:42:49.496 --> 00:42:51.066
like faces or kinds of animals.

00:42:51.676 --> 00:42:53.326
So in a very similar way,

00:42:54.146 --> 00:42:57.246
the convolutional neural
networks are organized

00:42:57.486 --> 00:43:00.286
into layers of neurons
which are trained

00:43:00.356 --> 00:43:02.646
to recognize increasingly
complex features.

00:43:04.846 --> 00:43:08.426
So the first layers are trained
to recognize low level features

00:43:08.836 --> 00:43:11.216
like edges and blobs of color,

00:43:11.636 --> 00:43:13.566
while the subsequent
layers are trained

00:43:13.566 --> 00:43:15.156
to recognize higher
level features.

00:43:15.156 --> 00:43:17.216
So for example, if we
are doing face detection,

00:43:17.566 --> 00:43:20.026
then will have layers that will
recognize features like noses,

00:43:20.326 --> 00:43:23.716
eyes, cheeks, and then
combination of these features,

00:43:23.816 --> 00:43:24.966
and then finally faces.

00:43:26.586 --> 00:43:30.116
And then the final few layers
combine all the generated

00:43:30.116 --> 00:43:33.106
information to produce the
final output for the network,

00:43:33.346 --> 00:43:35.886
such as the probability that
there is a face in the image.

00:43:36.986 --> 00:43:38.336
And I keep mentioning features.

00:43:39.026 --> 00:43:43.626
Think of a feature as a
filter that filters the input

00:43:43.626 --> 00:43:45.066
for that feature,
such as a nose,

00:43:45.786 --> 00:43:48.816
and if that information is
found, it's passed along.

00:43:49.686 --> 00:43:52.966
If that feature is found, this
information is passed along

00:43:53.356 --> 00:43:54.366
to the subsequent layers.

00:43:54.916 --> 00:43:57.246
And, of course, we need to
look for many such features.

00:43:57.246 --> 00:43:59.946
So if we're doing face
detection, then looking

00:43:59.946 --> 00:44:01.946
for just noses is
simply not enough.

00:44:02.306 --> 00:44:04.906
We also need to look for other
facial features like cheeks,

00:44:05.406 --> 00:44:07.486
eyes, and then combinations
of such features.

00:44:07.916 --> 00:44:09.816
So we need many of
these feature filters.

00:44:11.736 --> 00:44:14.306
So now that I've covered
convolutional neural networks,

00:44:14.886 --> 00:44:16.646
let's talk about the
building blocks we'll provide.

00:44:17.436 --> 00:44:19.866
The first building
block is your data.

00:44:20.746 --> 00:44:24.176
We want you to use MPS images
and MPS temporary images,

00:44:24.266 --> 00:44:27.516
which we added specifically to
support convolutional networks.

00:44:28.246 --> 00:44:31.346
They provide and optimize layout
for your data, for your input

00:44:31.346 --> 00:44:32.436
and intermediate results.

00:44:32.436 --> 00:44:37.986
Think of MPS temporary images
as light-weight MPS images,

00:44:39.146 --> 00:44:41.716
which we want you to
use for image data

00:44:41.836 --> 00:44:43.046
with a transient lifetime.

00:44:44.286 --> 00:44:49.216
MPS temporary images are built
using the Metal resource heaps,

00:44:49.386 --> 00:44:54.536
which were described in the
Part 1 of these sessions.

00:44:55.546 --> 00:44:58.146
They address some of
the reused cache memory,

00:44:59.196 --> 00:45:02.866
and they avoid expensive
allocation

00:45:02.866 --> 00:45:04.686
and deallocation of
texture resources.

00:45:05.116 --> 00:45:06.936
So the goal is to save
you lots of memory

00:45:07.546 --> 00:45:10.196
and to help you manage
intermediate resources.

00:45:11.056 --> 00:45:14.946
We also provide a collection
of layers, which you can use

00:45:15.266 --> 00:45:17.376
to create your inference
networks.

00:45:17.986 --> 00:45:20.436
But you may be thinking
right now, "How do I know

00:45:20.756 --> 00:45:22.496
which building blocks
I actually need

00:45:22.756 --> 00:45:24.496
to build my own inference
network?"

00:45:25.856 --> 00:45:28.666
So the answer is
trained parameters.

00:45:29.626 --> 00:45:32.806
The trained parameters, I
mentioned them previously

00:45:32.806 --> 00:45:34.456
when we talked about
the training phase.

00:45:34.666 --> 00:45:37.786
The trained parameters give
you a complete recipe for how

00:45:37.786 --> 00:45:39.216
to build your inference
networks.

00:45:39.926 --> 00:45:42.976
They tell you how many
layers you will have,

00:45:43.276 --> 00:45:45.756
what kind they will be, in
which order they will appear,

00:45:45.756 --> 00:45:48.236
and you also get all those
feature filters for every layer.

00:45:50.286 --> 00:45:53.586
So we take care of everything
under the hood to make sure

00:45:53.586 --> 00:45:56.406
that the networks you build
using these building blocks have

00:45:56.496 --> 00:45:59.156
the best possible
performance on all iOS GPUs.

00:45:59.516 --> 00:46:01.736
All you have to do
is to mine your data

00:46:02.206 --> 00:46:04.536
into this optimized
layout that we provide

00:46:05.276 --> 00:46:08.216
and to call library
functions to create the layers

00:46:08.216 --> 00:46:09.026
that make up your network.

00:46:10.636 --> 00:46:14.106
So now let's discuss all these
building blocks in more detail,

00:46:14.386 --> 00:46:17.006
but let's do it in a context
of a specific example.

00:46:20.416 --> 00:46:25.516
So in this demo, I have a system

00:46:25.576 --> 00:46:27.726
that has been trained
to detect smiles.

00:46:28.736 --> 00:46:30.256
And what we'll have is

00:46:30.256 --> 00:46:33.716
in real-time the system will
detect whether I am smiling

00:46:33.716 --> 00:46:33.956
or not.

00:46:34.066 --> 00:46:35.736
So I will first smile,
and then I will frown,

00:46:36.306 --> 00:46:37.976
and you will see the
system report just that.

00:46:44.516 --> 00:46:46.946
[ Laughter ]

00:46:47.446 --> 00:46:48.446
All right.

00:46:48.446 --> 00:46:48.976
So that [inaudible] my demo.

00:46:49.516 --> 00:46:54.646
[ Applause ]

00:46:55.146 --> 00:46:58.376
Okay. So now let's take a
look at the building blocks

00:46:58.376 --> 00:47:00.726
that I needed to build
this kind of a network.

00:47:01.236 --> 00:47:03.206
So the first building
block we're going to talk

00:47:03.206 --> 00:47:04.836
about is the convolution layer.

00:47:05.666 --> 00:47:08.206
It's the core building block of
convolutional neural networks,

00:47:08.536 --> 00:47:11.156
and its goal is to
recognize features and input.

00:47:11.156 --> 00:47:12.946
And it's called a
convolutional layer

00:47:13.256 --> 00:47:15.356
because it performs a
convolution on the input.

00:47:15.746 --> 00:47:18.536
So let's recall how
regular convolution works.

00:47:18.626 --> 00:47:21.176
You have your input and your
output and in this case a 5

00:47:21.176 --> 00:47:22.966
by 5 pixel filter
with some weight.

00:47:23.716 --> 00:47:26.086
And in order to compute
the value of this pixel

00:47:26.526 --> 00:47:27.476
in your output, you need

00:47:27.476 --> 00:47:29.246
to convolve the filter
with the input.

00:47:30.636 --> 00:47:31.176
Pretty easy.

00:47:32.036 --> 00:47:34.616
The convolution layer
is a generalization

00:47:34.646 --> 00:47:35.776
of regular convolution.

00:47:36.746 --> 00:47:38.476
It allows you to have
multiple filters.

00:47:39.286 --> 00:47:42.306
The different filters are
applied to the input separately,

00:47:42.486 --> 00:47:44.106
resulting in different
output channels.

00:47:44.206 --> 00:47:45.706
So if you have 16 filters.

00:47:45.706 --> 00:47:47.596
That means you have
16 output channels.

00:47:48.106 --> 00:47:51.426
So in order to get the value of
this pixel in the first channel

00:47:51.426 --> 00:47:54.216
of the output, you need
to take the first filter

00:47:54.456 --> 00:47:56.446
and convolve it with the input.

00:47:56.446 --> 00:48:00.416
And in order to get the value of
this pixel in the second channel

00:48:00.416 --> 00:48:02.836
of the output, you need
to take the second filter

00:48:03.116 --> 00:48:04.336
and convolve it with your input.

00:48:05.166 --> 00:48:07.546
Of course, in our examples,

00:48:07.596 --> 00:48:09.896
mild detection we are
dealing with color images.

00:48:10.226 --> 00:48:13.376
So that means that your input
actually has three separate

00:48:13.376 --> 00:48:16.856
channels, and just because
of how convolutional neural

00:48:16.856 --> 00:48:20.886
networks work, you need
three sets of 16 filters

00:48:21.326 --> 00:48:23.846
where you have one set
for each input channel.

00:48:24.956 --> 00:48:28.166
And then you apply
the different filters

00:48:29.096 --> 00:48:37.106
to separate input channels
and combine the results

00:48:37.856 --> 00:48:42.616
to get a single output value.

00:48:43.366 --> 00:48:45.426
So this is how you
would create one

00:48:45.426 --> 00:48:48.006
of these convolution
layers in our framework.

00:48:48.756 --> 00:48:51.596
You first create a descriptor
and specify such parameters

00:48:51.706 --> 00:48:54.516
as the width and height of the
filters you're going to use

00:48:54.906 --> 00:48:57.086
and then the number of
input and output channels.

00:48:57.846 --> 00:49:02.236
And then you create
a convolution layer

00:49:02.236 --> 00:49:06.876
from this descriptor and
provide the actual data

00:49:07.226 --> 00:49:08.786
for the feature filters,
which you get

00:49:08.836 --> 00:49:09.866
from the trained parameters.

00:49:12.846 --> 00:49:14.896
The next layer we are going to
talk about is the pooling layer.

00:49:15.866 --> 00:49:17.906
The function of the
pooling layer is

00:49:17.906 --> 00:49:21.546
to progressively reduce the
spatial size of the network,

00:49:21.686 --> 00:49:23.306
which reduces the
amount of competition

00:49:23.306 --> 00:49:24.296
for the subsequent layers.

00:49:24.686 --> 00:49:26.626
And it's common to
insert a pooling of the

00:49:26.626 --> 00:49:28.726
in between successive
convolution layers.

00:49:29.646 --> 00:49:33.906
Another function of the
pooling layer is to summarize

00:49:33.956 --> 00:49:37.546
or condense information
in a region of the input,

00:49:37.546 --> 00:49:41.896
and it would provide two pooling
operations, maximum and average.

00:49:42.816 --> 00:49:48.496
So in this example, we take a 2
by 2 pixel region of the input.

00:49:49.536 --> 00:49:53.786
We take the maximum value
and store it as our output.

00:49:54.356 --> 00:49:58.306
And this is the API
you need to use

00:49:58.306 --> 00:50:00.706
in the Metal Performance
Shaders framework to create one

00:50:00.706 --> 00:50:01.586
of these pooling layers.

00:50:02.086 --> 00:50:04.516
It's common to use
the max operation

00:50:06.226 --> 00:50:10.926
with a filter size of 2 by 2.

00:50:11.316 --> 00:50:14.416
The fully connected layer is
a layer where every neuron

00:50:14.446 --> 00:50:17.466
in the input is connected to
every neuron in the output.

00:50:18.206 --> 00:50:20.926
But think about it as a special
type of a convolution layer

00:50:21.556 --> 00:50:24.836
where the filter size is
the same as your input size.

00:50:24.836 --> 00:50:28.306
So in this example, we have
a filter of the same size

00:50:28.306 --> 00:50:30.466
as the input, and
we convolve them

00:50:30.546 --> 00:50:32.036
to get a single output value.

00:50:32.256 --> 00:50:35.736
So in this architecture,
the convolution

00:50:35.736 --> 00:50:38.666
and pooling layers operate
on regions of input,

00:50:38.976 --> 00:50:41.286
while the fully connected
layer can be used

00:50:41.496 --> 00:50:44.846
to aggregate information
from across the entire input.

00:50:45.506 --> 00:50:47.986
It's usually one of the
last layers in your network,

00:50:48.096 --> 00:50:50.746
and this is where your final
decision-making is taking place

00:50:51.216 --> 00:50:55.346
and you create -- you generate
the output for the network,

00:50:55.846 --> 00:50:58.276
such as the probability that
there's a smile in the image.

00:50:58.896 --> 00:51:03.276
And this is how you
would create one

00:51:03.276 --> 00:51:04.506
of these fully connected layers

00:51:04.676 --> 00:51:06.496
in the Metal Performance
Shaders framework.

00:51:07.196 --> 00:51:08.826
You create a convolution
descriptor

00:51:08.826 --> 00:51:11.016
because this is a special
type of a convolution layer,

00:51:11.446 --> 00:51:13.416
and then you create a
fully connected layer

00:51:14.176 --> 00:51:15.026
from this descriptor.

00:51:15.616 --> 00:51:18.156
We'll also provide
some additional layers,

00:51:18.156 --> 00:51:20.636
which I'm not going to cover
in detail in this presentation

00:51:21.156 --> 00:51:23.266
but they are described
in our documentation.

00:51:23.526 --> 00:51:26.666
We provide the neural
layer, which is usually used

00:51:26.666 --> 00:51:28.516
in conjunction with
the convolution layer,

00:51:28.756 --> 00:51:31.796
and we also provide the soft
max and normalization layers.

00:51:32.816 --> 00:51:35.196
So now that we've
covered all of the layers,

00:51:35.606 --> 00:51:36.626
let's talk about your data.

00:51:37.386 --> 00:51:39.396
I mentioned that you
should be using MPS images.

00:51:40.006 --> 00:51:40.856
So what are they really?

00:51:42.356 --> 00:51:45.386
Most of you are already
familiar with Metal textures.

00:51:45.526 --> 00:51:50.596
So this is a 2D Metal
texture with multiple channels

00:51:50.906 --> 00:51:53.326
where every channel corresponds
to a color channel and alpha.

00:51:54.276 --> 00:51:56.906
And I mentioned in my
previous examples that we need

00:51:56.906 --> 00:52:00.026
to create images with
multiple channels,

00:52:00.706 --> 00:52:01.946
for example, 32 channels.

00:52:02.286 --> 00:52:04.046
If we have 32 feature filters,

00:52:04.296 --> 00:52:05.706
we need to create
an output channel --

00:52:05.706 --> 00:52:08.486
an output image that
has 32 channels.

00:52:08.726 --> 00:52:09.466
So how do we do this?

00:52:10.386 --> 00:52:15.456
So an MPS image is really
a Metal 2D array texture

00:52:15.456 --> 00:52:16.376
with multiple slices.

00:52:16.946 --> 00:52:18.736
And when you're creating
an MPS image,

00:52:19.206 --> 00:52:21.016
all you really should
care about is

00:52:21.016 --> 00:52:25.006
that you are creating an image
with 32 -- with 32 channels.

00:52:25.546 --> 00:52:29.606
But sometimes you may need to
reach the MPS image data back

00:52:29.636 --> 00:52:31.586
to the CPU, or you may want

00:52:31.586 --> 00:52:35.496
to use an existing Metal 2D
array texture as your MPS image.

00:52:35.916 --> 00:52:37.636
So for those cases,
you need to know

00:52:37.846 --> 00:52:41.656
that we use a special
packed layout for your data.

00:52:42.146 --> 00:52:44.106
So every pixel in a slice

00:52:44.106 --> 00:52:47.156
of the structure contains
the data for four channels.

00:52:48.726 --> 00:52:52.516
So a 32-channel image would
really just have eight slices.

00:52:53.356 --> 00:52:57.046
And this is the API you
need to use to create one

00:52:57.046 --> 00:52:59.066
of the MPS images
in our framework.

00:52:59.606 --> 00:53:02.616
You first create a descriptor
and specify such parameters

00:53:02.656 --> 00:53:07.016
as the channel for data format
with the height of the image

00:53:07.266 --> 00:53:08.686
and the number of channels.

00:53:09.886 --> 00:53:11.456
And then you create an MPS image

00:53:11.606 --> 00:53:13.146
from this descriptor,
pretty simple.

00:53:14.376 --> 00:53:18.066
Of course, if you have
small input images,

00:53:18.136 --> 00:53:20.476
then you should batch them
to better utilize the GPU,

00:53:21.156 --> 00:53:24.066
and we provide a simple
mechanism for you to do this.

00:53:24.546 --> 00:53:28.696
So in this example, we create
an array of 100 MPS images.

00:53:30.596 --> 00:53:33.136
Okay, so now that we've
covered all the layers,

00:53:33.136 --> 00:53:35.866
we've covered data, and
now let's take a look

00:53:35.866 --> 00:53:39.426
at the actual network you need
to build to do smile detection.

00:53:40.166 --> 00:53:42.666
So we start with our
inputs, and now we're going

00:53:42.666 --> 00:53:45.126
to use the trained parameters
that I keep mentioning

00:53:46.096 --> 00:53:47.766
to help us build this network.

00:53:48.346 --> 00:53:51.446
So the trained parameters
tell us that the first layer

00:53:51.546 --> 00:53:53.786
in this network is going
to be a convolution layer,

00:53:54.056 --> 00:53:56.066
which takes a three-channel
images input

00:53:56.426 --> 00:53:58.286
and outputs a 16-channel image.

00:53:59.656 --> 00:54:03.456
The trained parameters also give
us the three sets of 16 filters

00:54:03.896 --> 00:54:07.796
for this layer, and these
colorful blue images show you

00:54:08.726 --> 00:54:12.136
the visualization of
the output channels

00:54:12.436 --> 00:54:14.496
after the filters have
been applied to the input.

00:54:16.686 --> 00:54:18.596
The next layer is
a pooling layer,

00:54:18.796 --> 00:54:22.336
which reduces the spatial
resolution of the output

00:54:22.336 --> 00:54:25.406
of the convolution layer by a
factor of two in each dimension.

00:54:27.166 --> 00:54:28.346
The trained parameters tell us

00:54:28.346 --> 00:54:30.706
that the next layer is
another convolution layer,

00:54:31.056 --> 00:54:33.116
which takes a 16-channel
images input

00:54:33.336 --> 00:54:37.046
and outputs a 16-channel image,
which is further down reduced

00:54:37.046 --> 00:54:38.596
in size by the next
pooling layer,

00:54:39.206 --> 00:54:42.396
and so on until we
get to our output.

00:54:43.616 --> 00:54:46.656
As you can see, this
network has a series

00:54:46.656 --> 00:54:49.576
of convolution layers
followed by the pooling layers,

00:54:50.146 --> 00:54:53.136
and the last two layers are
the fully connected layers,

00:54:53.296 --> 00:54:56.546
which generate the final
output for your network.

00:54:58.126 --> 00:55:00.516
So now that we know what this
network should look like,

00:55:00.516 --> 00:55:03.936
and this is very common for a
convolutional neural network

00:55:03.936 --> 00:55:07.036
for inference, so now
let's write the code

00:55:07.366 --> 00:55:08.596
to create it in our framework.

00:55:09.746 --> 00:55:12.866
So the first step is
to create the layers.

00:55:13.576 --> 00:55:15.736
Once again, the trained
parameters tell us that we need

00:55:15.916 --> 00:55:20.376
to have four convolution layers
in our network and I'm showing

00:55:20.376 --> 00:55:23.026
that the code had to create
one of them for simplicity

00:55:23.406 --> 00:55:25.966
but as you can see, I'm
using exactly the same API

00:55:26.586 --> 00:55:27.636
that I've showed you before.

00:55:28.356 --> 00:55:30.596
Then we need to create
our pooling layer.

00:55:31.426 --> 00:55:33.886
We just need one because
we're always going

00:55:33.886 --> 00:55:37.136
to be using the max operation
with a filter size of 2 by 2.

00:55:38.166 --> 00:55:40.716
And we also need to create
two fully connected layers,

00:55:41.036 --> 00:55:42.896
and once again I'm only
showing you the code

00:55:42.896 --> 00:55:44.416
for one for simplicity.

00:55:45.226 --> 00:55:48.276
And now, we need to take
care of our input and output.

00:55:48.726 --> 00:55:51.546
In this particular
example, I'm assuming

00:55:51.546 --> 00:55:55.286
that we have an existing Metal
app and you have some textures

00:55:55.286 --> 00:55:57.196
that you would like to use
for your input and output,

00:55:57.616 --> 00:56:01.396
and this is the API that you
need to use to create MPS images

00:56:01.766 --> 00:56:03.226
from existing Metal textures.

00:56:03.846 --> 00:56:08.336
And so the last step is
to encode all your layers

00:56:08.716 --> 00:56:11.556
into an existing command
buffer in the order prescribed

00:56:11.616 --> 00:56:12.596
by the trained parameters.

00:56:14.686 --> 00:56:18.256
So we have our input and our
outputs, and now we notice

00:56:18.256 --> 00:56:21.156
that we need one more
thing to take care of.

00:56:21.156 --> 00:56:24.306
We need to store the output
of the first layer somewhere.

00:56:24.476 --> 00:56:28.276
So let's use MPS
temporary images for that.

00:56:28.466 --> 00:56:31.106
This is how you would create
an MPS temporary image.

00:56:31.526 --> 00:56:33.306
As you can see, this
is very similar

00:56:33.306 --> 00:56:35.596
to the way you would
create a regular MPS image.

00:56:36.706 --> 00:56:40.286
And now we immediately use it
when we encode the first layer.

00:56:40.896 --> 00:56:43.756
And the temporary image
will go away as soon

00:56:43.756 --> 00:56:45.096
as the command buffer
is submitted.

00:56:45.986 --> 00:56:47.496
And then we continue.

00:56:47.496 --> 00:56:50.396
We create another temporary
image to store the output

00:56:50.396 --> 00:56:54.516
of the second layer, and so
on until we get to our output.

00:56:56.026 --> 00:56:56.446
That's it.

00:56:56.446 --> 00:56:59.586
And just to tie it
all back together,

00:57:00.146 --> 00:57:03.606
the order in which you encode
the layers matches the network

00:57:03.606 --> 00:57:06.016
diagram that I showed
you earlier exactly,

00:57:06.596 --> 00:57:09.016
so starting from the input
and all the way to the output.

00:57:10.156 --> 00:57:12.876
So now we worked through
a pretty simple example.

00:57:13.356 --> 00:57:15.146
Let's look at a more
complex one.

00:57:17.006 --> 00:57:18.956
We've ported the
inception inference network

00:57:18.956 --> 00:57:21.746
from tensor flow to run
using the Metal Performance

00:57:21.746 --> 00:57:22.426
Shaders framework.

00:57:23.506 --> 00:57:26.056
This is a very commonly
used inference network

00:57:26.056 --> 00:57:28.776
for object detection, and
this is the full diagram

00:57:29.626 --> 00:57:30.216
for this network.

00:57:31.366 --> 00:57:34.516
As you can see, this
network is a lot more complex

00:57:34.546 --> 00:57:35.846
that the previous
one I showed you.

00:57:36.686 --> 00:57:37.916
It has over 100 layers.

00:57:38.296 --> 00:57:40.296
But just to remind you,
all you have to do is

00:57:40.296 --> 00:57:42.466
to call some library functions
to create these layers.

00:57:43.676 --> 00:57:45.976
And now first, let's take a
look at this network in action.

00:57:51.046 --> 00:57:53.726
So here I have a collection of
images of different objects,

00:57:54.226 --> 00:57:56.086
and as soon as I
tap on this image,

00:57:56.446 --> 00:58:00.446
we will run the inference
network in real-time

00:58:00.446 --> 00:58:02.686
and it will report
the top five guesses

00:58:02.686 --> 00:58:04.476
for what it thinks
this object is.

00:58:05.236 --> 00:58:07.836
So the top guess is
that it's a zebra.

00:58:09.606 --> 00:58:14.306
Then this is a pickup
truck, and this a volcano.

00:58:14.646 --> 00:58:16.886
So that looks pretty good
to me, but of course,

00:58:17.606 --> 00:58:20.256
let's do a real live demo
right here on this stage.

00:58:20.946 --> 00:58:24.816
And we'll take a picture
of this water bottle,

00:58:24.816 --> 00:58:30.866
and let's use this
image, water bottle.

00:58:31.516 --> 00:58:39.746
[ Applause ]

00:58:40.246 --> 00:58:42.996
So what I wanted to show
you with this live demo is

00:58:42.996 --> 00:58:46.966
that even a large network
with over 100 layers can run

00:58:47.016 --> 00:58:49.616
in real-time using the Metal
Performance Shaders framework,

00:58:50.166 --> 00:58:50.976
but this is not all.

00:58:51.766 --> 00:58:54.866
I also want to talk about
the memory savings we got

00:58:54.966 --> 00:58:57.446
from using MPS temporary
images in this demo.

00:58:58.286 --> 00:59:01.996
So in the first version of
this demo, we used MPS images

00:59:02.146 --> 00:59:04.726
to store intermediate
results, and we ended

00:59:04.726 --> 00:59:08.796
up needing 74 MPS
images totaling in size

00:59:09.256 --> 00:59:12.076
over 80 megabytes for
the entire network.

00:59:12.786 --> 00:59:15.356
And of course, you don't
have to use 74 images.

00:59:15.356 --> 00:59:18.466
You can come up with your
own clever scheme for how

00:59:18.466 --> 00:59:22.766
to reuse these images, but
this means more stuff to manage

00:59:22.766 --> 00:59:25.306
in your code, and we want to
make sure that our framework is

00:59:25.306 --> 00:59:27.126
as easy for you to
use as possible.

00:59:28.006 --> 00:59:29.586
So in the second
version of the demo,

00:59:30.066 --> 00:59:33.936
we replaced all the MPS images
with MPS temporary images,

00:59:34.876 --> 00:59:36.566
and this gave us
several advantages.

00:59:37.016 --> 00:59:40.076
The first one is reduced
CPU cost in terms of time

00:59:40.076 --> 00:59:45.486
and energy, but also creating
74 temporary images resulted

00:59:45.526 --> 00:59:50.286
in just 5 underlying memory
allocations, totaling just

00:59:50.286 --> 00:59:53.986
over 20 megabytes and this
is 76% of memory savings.

00:59:54.616 --> 00:59:55.296
That's pretty huge.

00:59:56.516 --> 01:00:00.446
So what I showed you with
these two live demos is

01:00:00.486 --> 01:00:02.816
that the Metal Performance
Shaders framework provides

01:00:03.216 --> 01:00:06.356
complete support for building
convolutional neural networks

01:00:06.466 --> 01:00:09.836
for inference, and it's
optimized iOS GPU use.

01:00:10.296 --> 01:00:12.606
So please, use the
convolutional neural networks

01:00:13.086 --> 01:00:15.266
to build some cool apps.

01:00:15.776 --> 01:00:18.806
So this is the end of
What's New in Metal talks,

01:00:19.006 --> 01:00:22.656
and if you haven't seen the
first session, please check

01:00:22.656 --> 01:00:25.496
out the video so you can learn
about such cool new features

01:00:25.536 --> 01:00:29.306
as tessellation, resource heaps,
and memoryless render targets

01:00:29.626 --> 01:00:30.986
and improvements to our tools.

01:00:31.886 --> 01:00:37.136
In this session, we talked
about function specialization

01:00:37.136 --> 01:00:39.636
and function resource
read-writes, white color

01:00:39.686 --> 01:00:42.026
and texture assets,
and new additions

01:00:42.026 --> 01:00:44.166
to the Metal performance
tools, concentrating

01:00:44.166 --> 01:00:45.506
on convolutional
neural networks.

01:00:46.816 --> 01:00:50.136
For more information about this
session, please go to this URL.

01:00:51.656 --> 01:00:53.946
You can catch the
video and get links

01:00:53.996 --> 01:00:56.206
to related documentation
and sample code.

01:00:57.696 --> 01:00:59.806
And here's some information
on the related sessions.

01:01:01.066 --> 01:01:03.006
You could always
check out the videos

01:01:03.456 --> 01:01:05.656
of the past Metal
sessions online,

01:01:06.006 --> 01:01:08.836
but you can also catch
an advanced Metal shader

01:01:08.836 --> 01:01:12.576
optimization talk later today,
and just note the location

01:01:12.576 --> 01:01:14.406
of this talk has
changed to Knob Hill.

01:01:15.986 --> 01:01:18.456
Tomorrow, you have an
opportunity to catch the Working

01:01:18.456 --> 01:01:21.046
with White Color talk
and the Neural Networks

01:01:21.046 --> 01:01:22.976
and Accelerate talk
where you can learn how

01:01:22.976 --> 01:01:23.956
to create neural networks

01:01:23.956 --> 01:01:26.546
for the CPU using the
Accelerate framework.

01:01:27.316 --> 01:01:28.826
So thank you very
much for coming,

01:01:28.936 --> 01:01:30.916
and I hope you have
a great WWDC.

01:01:31.508 --> 01:01:33.508
[ Applause ]