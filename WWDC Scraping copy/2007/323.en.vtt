WEBVTT

00:00:20.010 --> 00:00:26.070
>> So welcome to session 323, Third Party
Solutions for Fortran Development

00:00:26.070 --> 00:00:33.020
and High Performance Computing on Mac OS X.

00:00:33.020 --> 00:00:37.690
Like gall, this session is divided into three parts.

00:00:37.689 --> 00:00:45.390
I'll start off talking about the Intel Fortran
Compiler and then I'll turn the floor over to David

00:00:45.390 --> 00:00:54.200
and Drew of Mac research and they will talk about the
services they provide to the Mac OS scientific community.

00:00:54.200 --> 00:01:03.150
They will also go in depth into advancements in
the gFortran Compiler and in particular, OpenMP.

00:01:03.149 --> 00:01:11.269
And finally, Wood from Absoft will come up and speak about
their latest versions of their Fortran development tools.

00:01:11.269 --> 00:01:21.840
So my name is Dave Kreitzer and for the past ten
years, I've been a developer in the Intel Compiler lab

00:01:21.840 --> 00:01:28.210
and I'd like to spend the next few minutes first,
giving you an introduction to the Intel Fortran Compiler

00:01:28.209 --> 00:01:36.309
and then to illustrate via a brief demo, how the
Compiler's advanced optimization techniques can help deliver

00:01:36.310 --> 00:01:38.930
performance to your applications.

00:01:43.310 --> 00:01:50.430
During the ten years that I've been working on the Intel
Compiler, the development team has grown significantly.

00:01:50.430 --> 00:01:56.510
Intel is investing heavily in it's compiler
resources and the primary goal has always been

00:01:56.510 --> 00:02:03.810
to deliver outstanding software performance on
Intel architecture and with the advent of technology

00:02:03.810 --> 00:02:14.020
such as hyper threading and multi core, providing threading
features has become a major focus contributing to that goal.

00:02:14.020 --> 00:02:19.370
Our industry leading performance has
been proven for many years on the Windows

00:02:19.370 --> 00:02:24.069
and Linux platforms across a wide spectrum of applications.

00:02:24.069 --> 00:02:29.370
Including those from the scientific
and engineering computing fields.

00:02:29.370 --> 00:02:35.319
And my key message to you today is that
Intel's thrilled to have the opportunity

00:02:35.319 --> 00:02:39.609
to bring that proven performance to the Mac.

00:02:39.610 --> 00:02:45.480
And as I'll show you in the demo, the Intel
Compiler is designed to integrate easily

00:02:45.479 --> 00:02:49.979
into the development environment that you're used to using.

00:02:49.979 --> 00:02:56.829
Intel and Apple have worked closely together
to ensure seamless integration into Xcode

00:02:56.830 --> 00:03:04.680
and the Compiler's inter operable with the other
tools that you're used to using such as GCC and GDC.

00:03:04.680 --> 00:03:10.939
Additionally the Compiler supports the Fortran standards,
so that this all means is that you can take advantage

00:03:10.939 --> 00:03:14.150
of our outstanding performance
without having to spend the time

00:03:14.150 --> 00:03:19.300
and energy porting your applications
to a completely new tool suite.

00:03:19.300 --> 00:03:26.380
And if you have problems with the Compiler,
have questions or just have suggestions

00:03:26.379 --> 00:03:34.049
about possible future feature enhancements, Intel
has an extremely responsive support team of engineers

00:03:34.050 --> 00:03:41.760
who have the expertise to help you extract the most
performance out of Intel architecture for your Fortran apps.

00:03:43.110 --> 00:03:47.440
So at this time, I'd like to introduce our latest product,

00:03:47.439 --> 00:03:54.579
the Intel Fortran Compiler Professional
Edition, Version 10.0.

00:03:54.580 --> 00:03:57.730
This is a single bundle package of three separate tools.

00:03:57.729 --> 00:04:03.909
The Intel Fortran Compiler, the Intel Math
Kernel Library and the Intel Debugger.

00:04:03.909 --> 00:04:12.500
This is Intel's first 64 bit Fortran Compiler product
for the Mac and we're very excited about that.

00:04:12.500 --> 00:04:13.870
( applause )

00:04:13.870 --> 00:04:15.240
Thank you.

00:04:16.759 --> 00:04:23.360
The Compiler will help take your Fortran source code
and convert it into highly optimized machine code

00:04:23.360 --> 00:04:27.330
with improved threading behavior and performance.

00:04:27.329 --> 00:04:35.129
The Intel Math Kernel Library contains a set of
math functions that are aggressively multi threaded

00:04:35.129 --> 00:04:39.189
and meticulously tuned to Intel architecture.

00:04:39.189 --> 00:04:49.240
MKL contains BLAS functions, LAPACK functions, FFT,
vector math functions and vector statistics functions.

00:04:49.240 --> 00:04:56.740
And finally, the Intel Debugger is designed to work well
with the Intel Compiler and it contains features to aid

00:04:56.740 --> 00:05:01.720
in the debugging of optimized and multi threaded code.

00:05:04.949 --> 00:05:12.959
So I'd like to emphasize again, that the primary
feature delivered by Intel's Compilers is performance

00:05:12.959 --> 00:05:18.620
and there are a number of features and underlying
technologies that help deliver that performance.

00:05:18.620 --> 00:05:25.170
Auto-parallelization and OpenMP support
help you to take advantage of multi core.

00:05:26.560 --> 00:05:30.500
Vectorization helps you take advantage
of instruction set enhancements

00:05:30.500 --> 00:05:35.519
such as the streaming SIMD
extensions and it's descendants.

00:05:35.519 --> 00:05:39.120
Advanced co generation optimizations
help you to take advantage

00:05:39.120 --> 00:05:44.930
of low level performance details of our microprocessors.

00:05:44.930 --> 00:05:53.180
Inter procedural optimizations or IPO, helps expose
to the compiler optimization opportunities that exist

00:05:53.180 --> 00:06:00.569
across module boundaries and profile
guided optimizations or PGO,

00:06:00.569 --> 00:06:06.009
enables the Compiler to make optimization
decisions based on concrete information

00:06:06.009 --> 00:06:09.659
about the runtime and behavior of your program.

00:06:09.660 --> 00:06:16.180
And since we integrate easily into Xcode, you can take
advantage of all these performance features without having

00:06:16.180 --> 00:06:19.240
to go through an expensive tools migration.

00:06:20.769 --> 00:06:25.699
So what's new specifically in our 10.0 version.

00:06:25.699 --> 00:06:28.620
Well the big bang new performance feature is HPO.

00:06:28.620 --> 00:06:37.899
HPO is a high performance parallel optimizer
that combines in one phase, vectorization,

00:06:37.899 --> 00:06:43.120
auto-parallelization and aggressive loop transformations.

00:06:43.120 --> 00:06:51.300
HPO essentially finds the parallelism that exists in
your program and then categorizes it for vectorization

00:06:51.300 --> 00:06:58.470
on the one hand in the form of SSE instruction or
auto-parallelization on the other hand in the form

00:06:58.470 --> 00:07:06.660
of multi threading for multi-core
or perhaps a combination of both.

00:07:06.660 --> 00:07:10.189
There are new verification and diagnostic capabilities.

00:07:10.189 --> 00:07:18.569
The Compiler can instrument your code so that at runtime,
it can detect stack integrity problems and buffer overflows.

00:07:18.569 --> 00:07:25.230
Additionally its whole program analysis capabilities
help to detect a whole new class of problems

00:07:25.230 --> 00:07:29.689
at compile time such as OpenMP usage errors.

00:07:31.439 --> 00:07:34.550
There's support for a new Fortran 2003 features.

00:07:34.550 --> 00:07:44.449
Asynchronous I/O is a new multi threaded I/O library
that will help to improve your application performance.

00:07:44.449 --> 00:07:52.329
There are new C interoperability features that help
you to build portable mixed language applications.

00:07:52.329 --> 00:07:59.399
There's improved standard checking and many more features
that really are aimed at increasing developer productivity.

00:08:00.769 --> 00:08:07.079
Improvements in the Intel Debugger make it easier than
ever to debug parallel programs, programs that use MPI

00:08:07.079 --> 00:08:16.120
and OpenMP and I'd like to emphasize again, that this
is Intel's first 64 bit Fortran Compiler for the Mac.

00:08:16.120 --> 00:08:24.740
You have the opportunity to direct the Compiler at Compiler
time to either produce a 64 bit binary or a 32 bit binary

00:08:24.740 --> 00:08:27.829
or a universal binary that contains both.

00:08:27.829 --> 00:08:36.049
So at this point, I'd like to go
ahead and go to the demo machine.

00:08:36.049 --> 00:08:46.479
The application that we're using in this demo is
the same square charge application that was used

00:08:46.480 --> 00:08:51.180
in Monday's tool state of the union if you
were here for that and I'll tell you more

00:08:51.179 --> 00:08:53.599
about what this application does in a moment.

00:08:53.600 --> 00:08:59.290
But let me first draw your attention to the
fact that this Xcode project contains a number

00:08:59.289 --> 00:09:09.009
of Fortran modules and a number of Objective-C modules.

00:09:09.009 --> 00:09:16.590
So it's easy in Xcode to mix C, C++,
Objective-C and Fortran altogether in the same project.

00:09:16.590 --> 00:09:26.870
And here now I'll show you how to select the Intel Fortran
Compiler as a default for building Fortran source modules.

00:09:26.870 --> 00:09:33.840
You just need to open up the inspector for the target,
click on the rules tab which is already selected here

00:09:33.840 --> 00:09:39.129
and then select the Fortran Compiler from the pull down menu.

00:09:39.129 --> 00:09:42.000
That's it, all there is to it.

00:09:42.000 --> 00:09:45.679
So now I'd like to go ahead and build this application.

00:09:45.679 --> 00:09:55.219
And the thing I'd like you to notice right now is these
diagnostic messages which are coming from the vectorizer.

00:09:55.220 --> 00:10:01.120
I did that again.

00:10:01.120 --> 00:10:05.460
There. Loop was not vectorized,
existence of vector dependents

00:10:05.460 --> 00:10:08.320
I'll talk a little bit more about that in a moment.

00:10:08.320 --> 00:10:10.890
But let's go ahead and run the application.

00:10:10.889 --> 00:10:26.490
Now what this application is doing
is a numerical integration.

00:10:26.490 --> 00:10:33.659
That's a common problem in scientific computing and this
particular application was written by our resident physicist

00:10:33.659 --> 00:10:40.259
and it's from the field of electrostatics
and it's computing the electric potential due

00:10:40.259 --> 00:10:47.899
to a uniformly distributed electric charge in a unit square
at various points outside the square but in the same plane.

00:10:47.899 --> 00:10:51.319
If you were at the tools talk on Monday, you
might have heard the more technical terms

00:10:51.320 --> 00:10:53.680
for what this program was computing.

00:10:53.679 --> 00:10:57.019
I think it was Electra whosiemawhatsit.

00:10:57.019 --> 00:11:02.389
But so anyway, this took about 23 seconds.
#

00:11:02.389 --> 00:11:09.500
Let's go ahead and quit and take a look at the
inner loop of this program that failed to vectorize.

00:11:09.500 --> 00:11:12.389
So this loop right here.

00:11:12.389 --> 00:11:20.939
Now you'll notice, this loop contains a call to an external
function, a function that's defined in a different module.

00:11:20.940 --> 00:11:25.140
The vectorizer does not have the context
to know that the stuff contained inside

00:11:25.139 --> 00:11:30.389
that function is independent from
one loop iteration to the next.

00:11:30.389 --> 00:11:36.809
We can give it that context by enabling inter procedural
optimizations and for the purpose of this demonstration,

00:11:36.809 --> 00:11:44.239
I'd like to isolate the effect of IPO and the effect of
vectorization so the next configuration that I'm going

00:11:44.240 --> 00:11:49.490
to select, enables IPO but disables the vectorizer.

00:11:54.250 --> 00:12:06.309
So now we're going ahead and rebuilding and running.

00:12:06.309 --> 00:12:13.729
Now remember the first run that took 23 seconds, that
was with your vanilla 02 optimizations and this run has

00:12:13.730 --> 00:12:19.990
in addition to those vanilla 02 optimizations,
inter procedural optimizations but no vectorization

00:12:19.990 --> 00:12:24.580
and we'll see how long this takes to run.

00:12:24.580 --> 00:12:26.060
18 seconds.

00:12:26.059 --> 00:12:30.419
So a speed up of about 5 seconds
or in the range of 20 percent.

00:12:30.419 --> 00:12:36.139
So even without vectorization, we're seeing some
benefit from inter procedural optimizations and that's

00:12:36.139 --> 00:12:41.120
because the Compiler is able to in line
the code from that integration function,

00:12:41.120 --> 00:12:44.480
into the inner loop, eliminating the call overhead.

00:12:44.480 --> 00:12:47.409
Now let's enable vectorization.

00:12:47.409 --> 00:13:01.370
Okay and the thing to note here is
that now the Compiler is telling us

00:13:01.370 --> 00:13:10.970
that that inner loop has been vectorized
and we'll see how that performs.

00:13:10.970 --> 00:13:18.820
So you've gone from 18 seconds to now about
seven tenths of a second, 30x ish or so speed up.

00:13:18.820 --> 00:13:23.370
There's one more thing I'd like to do with
this application and that is multi thread it

00:13:23.370 --> 00:13:35.470
and what we've done is taken an outer loop,
right here and added an OpenMP directive to it.

00:13:35.470 --> 00:13:42.019
You actually need a command line option to tell the
Compiler to go ahead and process these directives

00:13:42.019 --> 00:13:45.689
and generate parallel code for the loops that they modify.

00:13:45.690 --> 00:13:58.090
So that's what this last configuration is doing.

00:13:58.090 --> 00:14:06.830
Now you can see that in addition to the message that tells
us that loop was vectorized, there's a diagnostic message

00:14:06.830 --> 00:14:12.060
that tells us that the OpenMP loop was parallelized.

00:14:12.059 --> 00:14:14.809
And we'll go ahead and see how that performs.

00:14:19.490 --> 00:14:27.460
So a little under 2 seconds or a speed up of 4x ish.

00:14:29.649 --> 00:14:35.889
Makes me wonder if this is a 4 core machine and not
the 8 core machine we were running on yesterday.

00:14:35.889 --> 00:14:42.639
Okay, can we cut back to the slides please.

00:14:44.750 --> 00:14:46.889
Thank you.

00:14:46.889 --> 00:14:49.480
So what did we do in this demo?

00:14:49.480 --> 00:14:54.279
We first showed how easily the
Compiler integrates into Xcode.

00:14:54.279 --> 00:15:00.149
Then we built the square charge application
in under a number of different configurations.

00:15:00.149 --> 00:15:06.669
We saw the diagnostic messages coming from the
vectorizer and the parallelizer and most importantly,

00:15:06.669 --> 00:15:09.649
we saw the improved performance that was achievable

00:15:09.649 --> 00:15:14.970
by applying the more advanced optimization
capabilities of the Compiler.

00:15:14.970 --> 00:15:23.129
We went from an execution time of about 23 seconds to
an execution time of about 17 hundredths of a second.

00:15:23.129 --> 00:15:27.830
So it's an improvement of over 100x.

00:15:27.830 --> 00:15:33.340
Now this demo of course was designed to showcase
the optimization capabilities of the Compiler.

00:15:33.340 --> 00:15:39.009
We don't expect everybody to be able to just take
the Compiler and achieve 100x performance improvement

00:15:39.009 --> 00:15:46.120
on their application, but you can imagine that if you
have one hot kernel of your application and are able

00:15:46.120 --> 00:15:49.409
to achieve just a fraction of the speed up on that kernel,

00:15:49.409 --> 00:15:56.480
you can achieve some very impressive application level
performance improvements just by switching to our Compiler.

00:15:56.480 --> 00:16:03.730
So hopefully by this point I've peaked your
interest about the Compiler and conveyed the idea

00:16:03.730 --> 00:16:13.509
that we think it's an invaluable tool for helping build high
performing, multi threaded, Fortran applications on the Mac.

00:16:13.509 --> 00:16:19.179
And if you would like more information or
would like to download a free evaluation copy,

00:16:19.179 --> 00:16:22.719
I'll direct you to the Intel software products website.

00:16:22.720 --> 00:16:42.310
And with that I'd like to thank
you and ask David to come up.

00:16:42.309 --> 00:16:42.959
( applause )

00:16:42.960 --> 00:16:43.950
>> Thank you.

00:16:43.950 --> 00:16:50.810
So my name's David Gohara and I am with MacResearch.org,
it's an online community for scientists

00:16:50.809 --> 00:16:55.159
who are using Apple hardware and software in their research.

00:16:55.159 --> 00:17:03.519
MacResearch.org helps scientists by aggregating
the community together and providing resources

00:17:03.519 --> 00:17:07.829
for scientists such as news, reviews, tutorials.

00:17:07.829 --> 00:17:17.559
We also have initiatives to help scientists do
more with their, with their work on the platform

00:17:17.559 --> 00:17:23.359
and we also do another important thing, which is we serve
as advocates for scientists directly with Apply by meeting

00:17:23.359 --> 00:17:29.059
with Apple, talking with Apple, engaging with then and
letting them know what scientists need on the platform

00:17:29.059 --> 00:17:33.319
to really use their hardware and software in their work.

00:17:33.319 --> 00:17:35.839
So what are some of the resources?

00:17:35.839 --> 00:17:42.929
Early on, one of the resources we came up with was
our script repository, which scientists could take any

00:17:42.930 --> 00:17:50.830
of the tools that they've developed and we'd essentially
just host them for them and they're freely available

00:17:50.829 --> 00:17:52.759
to any other scientist to use in their own work.

00:17:52.759 --> 00:18:02.190
And these could be extensions, applications that scientists
might commonly use such as Octave or R for example.

00:18:02.190 --> 00:18:08.130
And the repository of course is free and
it's searchable by programming language

00:18:08.130 --> 00:18:12.430
or by scientific discipline and it's freely available.

00:18:12.430 --> 00:18:16.509
You can upload whatever you have
and make it available to others.

00:18:16.509 --> 00:18:22.379
The other area of that we've really
invested heavily our time in is tutorials.

00:18:22.380 --> 00:18:25.040
These are usually user driven tutorials.

00:18:25.039 --> 00:18:31.089
People saying well how do I use XYZ
technology in Mac OS X, in my work.

00:18:31.089 --> 00:18:37.329
And so we go out and Drew McCormack has written an excellent
series called Cocoa for Scientists, I know you're all sick

00:18:37.329 --> 00:18:42.819
and tired of hearing about Cocoa this week, but
you know, that's one of the one of the series.

00:18:42.819 --> 00:18:47.759
We also have Apple Script for Scientists,
Xcode, tutorials, Xgrade etcetera and so forth.

00:18:47.759 --> 00:18:53.430
Again, these are all free and not only do we develop
these tutorials, but other scientists put them,

00:18:53.430 --> 00:18:58.950
write their own tutorials and we make
them available and host them on the site.

00:18:58.950 --> 00:19:02.470
Another recent initiative that we've done is OpenMacGrid.

00:19:02.470 --> 00:19:07.269
Which is a community Xgrid cluster for
scientists and so if, and it's operating right now

00:19:07.269 --> 00:19:14.019
at about 2.2 THz and if you're a scientist and
you have an application that's amendable to grid computing,

00:19:14.019 --> 00:19:22.180
you can get access to these resources free of charge and
you can, if you need extra computational power on demand,

00:19:22.180 --> 00:19:28.830
but you don't happen to have the computational resources
wherever you're located, you can just submit a proposal

00:19:28.829 --> 00:19:35.679
and if it's approved then you can go
ahead and get access to OpenMacGrid.

00:19:35.680 --> 00:19:41.009
But the real reason why we're standing up here of course
is Fortran and one of the things that we all know is

00:19:41.009 --> 00:19:47.359
that there's a large body of scientific code that has been
developed in Fortran, is maintained in Fortran and continues

00:19:47.359 --> 00:19:55.599
to get updated in Fortran and so we want to make it
easy for scientists to use Fortran within Mac OS X.

00:19:55.599 --> 00:20:00.909
Now of course they're the Intel tools and the
Absoft tools and there are also open source tools

00:20:00.910 --> 00:20:06.340
and scientists have different needs and
sometimes they just want something right away,

00:20:06.339 --> 00:20:13.539
sometimes they want a commercial application with
well you know, with additional support or what not

00:20:13.539 --> 00:20:20.690
and so we have focused on gFortran as
one alternative for scientists to use.

00:20:20.690 --> 00:20:24.570
And to make it easy for scientists to use Fortran on Mac OS X,

00:20:24.569 --> 00:20:27.950
to use gFortran I should say on Mac
OS X, we've done two things.

00:20:27.950 --> 00:20:33.819
We've made prepackaged installers that are simple double
clickable installers, so you can within a matter of seconds,

00:20:33.819 --> 00:20:38.859
get gFortran installed on your system, power
pc or Intel based Mac's and you'll be up

00:20:38.859 --> 00:20:40.740
and running at the command line ready to go.

00:20:40.740 --> 00:20:46.450
But at the same time, we also wanted scientists to be
able to use a lot of the more advanced tools that come

00:20:46.450 --> 00:20:54.850
with Apple's developer tools such as Xcode, the graphical
debugger and what not and to make that easy for them,

00:20:54.849 --> 00:21:02.139
we commissioned a contest essentially, that would
allow people to use gFortran within Xcode and this plug

00:21:02.140 --> 00:21:05.900
in works also PowerPC and Intel based Macs.

00:21:05.900 --> 00:21:09.150
It's free, it's also part of this installer package.

00:21:09.150 --> 00:21:10.310
You double click install.

00:21:10.309 --> 00:21:11.009
It's all set up.

00:21:11.009 --> 00:21:13.009
It comes with templates, it's ready to go.

00:21:13.009 --> 00:21:17.470
So you can develop your Fortran projects
within Xcode, you can develop mixed C

00:21:17.470 --> 00:21:20.049
in Fortran and what not and you're ready to go.

00:21:20.049 --> 00:21:22.200
And of course the plug in is open source as well.

00:21:22.200 --> 00:21:27.720
So if you wanna extend the plug in and prove it,
make modifications to it, you're free to do that

00:21:27.720 --> 00:21:32.000
and it's all available at MacResearch.org.

00:21:32.000 --> 00:21:34.589
So at this point what I'd like to do
is turn this over to my colleague,

00:21:34.589 --> 00:21:40.809
Drew McCormack who will give you
an overview of OpenMP and gFortran.

00:21:40.809 --> 00:21:41.309
( applause )

00:21:41.309 --> 00:21:47.559
>> Okay thanks Dave.

00:21:47.559 --> 00:21:50.899
So what is OpenMP?

00:21:50.900 --> 00:21:59.370
OpenMP is a parallel programming API,
particularly for shared memory architectures.

00:21:59.369 --> 00:22:04.469
Now it's supported with C and Fortran
and it's actually not a new API at all.

00:22:04.470 --> 00:22:07.380
It's been around, for around 10 years I guess.

00:22:07.380 --> 00:22:15.470
But it's been used primarily on super-computers, large
super-computers, SMP machines such as the SGI Origin series.

00:22:15.470 --> 00:22:19.870
But it hasn't been used very much on the
desktop and the main reason for that is

00:22:19.869 --> 00:22:26.659
that you simply haven't had multiprocessor machines on
the desktop or at least not significantly many calls.

00:22:26.660 --> 00:22:32.009
That of course is all changing and that makes OpenMP
more interesting to people developing on the desktop

00:22:32.009 --> 00:22:37.559
and so OpenMP is becoming a bit
more relevant to most programmers.

00:22:37.559 --> 00:22:42.690
So if you wanna know more about the spec
of OpenMP, what OpenMP can do for you,

00:22:42.690 --> 00:22:46.039
I suggest taking at look at the openmp.org website.

00:22:46.039 --> 00:22:56.129
But I wanna go through now and sort of introduce you
to OpenMP and in particular, how it's used in gFortran.

00:22:56.130 --> 00:22:59.970
I assume that many of you have heard of MPI.

00:22:59.970 --> 00:23:04.620
MPI is a popular way of parallelizing
programs and I think a good way

00:23:04.619 --> 00:23:10.879
to introduce OpenMP is to contrast
it with MPI for that reason.

00:23:10.880 --> 00:23:19.710
So MPI is the message passing interface and
basically what you have there is separate processors,

00:23:19.710 --> 00:23:27.650
so effectively separate programs running
with their own memory address space

00:23:27.650 --> 00:23:31.220
and of course they can communicate
with one another with message passing.

00:23:31.220 --> 00:23:36.200
So sending buffers of data to one another.

00:23:36.200 --> 00:23:42.360
MPI works very well on distributed memory
computers like clusters or grids even,

00:23:42.359 --> 00:23:45.019
but it can also be used on shared memory machines.

00:23:45.019 --> 00:23:54.740
Usually there's a small cost in memory, but you can
at least run on a, on a say an 8-core octo Mac Pro.

00:23:54.740 --> 00:23:58.250
That's an advantage of MPI, it runs just about everywhere.

00:23:58.250 --> 00:24:05.390
OpenMP in contrast, is a threaded model, so it's not
separate processors, but separate threads in one process.

00:24:05.390 --> 00:24:13.120
It's shared memory so all of those threads of course can
read and write the same memory and it doesn't generally run

00:24:13.119 --> 00:24:17.159
on a cluster, it's generally for SMP machines.

00:24:17.160 --> 00:24:22.790
Now there's a footnote there because Intel does
actually make something called Cluster OpenMP.

00:24:22.789 --> 00:24:24.329
Unfortunately it's not available on the Mac.

00:24:24.329 --> 00:24:28.839
So if you'd like Intel to bring that to the
Mac, I think it's only on Linux at the moment,

00:24:28.839 --> 00:24:32.849
if you'd like Intel to bring it to the Mac, I
suggest you that you get in contact with Intel.

00:24:34.059 --> 00:24:36.950
So let's go into the memory architecture of these things.

00:24:36.950 --> 00:24:40.930
Imagine we've got a serial program,
this is a Fortran serial program.

00:24:40.930 --> 00:24:45.170
Imagine these blocks here are the
different, are different arrays in memory.

00:24:45.170 --> 00:24:48.490
So a green array and a blue array and a pink array.

00:24:48.490 --> 00:25:01.529
So what happens when we move to MPI, well then we have
of course multiple processes and typically what you have

00:25:01.529 --> 00:25:07.720
when you parallelize a code with MPI is
that you'll duplicate some of those arrays,

00:25:07.720 --> 00:25:13.430
particularly the smaller ones, you don't wanna distribute
those necessarily so you'll have a small memory cost.

00:25:13.430 --> 00:25:17.019
You'll have more memory, you'll require some more
memory because you'll in this case for example,

00:25:17.019 --> 00:25:21.519
we'd duplicate the green square or the
green array, we'd duplicated the blue one

00:25:21.519 --> 00:25:27.940
and maybe the pink one was particularly big and so we'd
split that in half, put have on CPU one, half on CPU two.

00:25:27.940 --> 00:25:33.880
Of course, the accounting for where that data
is, when you split an array in half like that,

00:25:33.880 --> 00:25:38.250
things get a bit more complicated in your program, because
you have to do all the accounting for indexing and things.

00:25:38.250 --> 00:25:39.240
Where is that data?

00:25:39.240 --> 00:25:41.750
If you need it on a certain processor, how do I get it?

00:25:41.750 --> 00:25:44.390
You have to request it from another processor.

00:25:44.390 --> 00:25:49.670
So that can complicate your program quite a lot.

00:25:49.670 --> 00:25:55.029
So this is distributed memory, some arrays
are duplicated and some are distributed.

00:25:55.029 --> 00:26:02.329
If we go to OpenMP now, we're back at the serial
case basically, we've got the same memory structure

00:26:02.329 --> 00:26:09.069
as a serial case, but we're two processors or two threads
should I say in this case, sharing the same memory

00:26:09.069 --> 00:26:11.740
and so they can read and write
exactly the same logical data.

00:26:11.740 --> 00:26:19.730
And that means you're using less memory in general and but
it also makes complicates things as we'll see a bit later.

00:26:19.730 --> 00:26:25.809
So memory is shared and each processor can
read and write exactly the same address space.

00:26:25.809 --> 00:26:29.359
So let's get into how OpenMP actually works.

00:26:29.359 --> 00:26:36.549
It's basically a loop centric, so your
program typically remains largely serial.

00:26:36.549 --> 00:26:39.230
So when you first compile your
program with an OpenMP Compiler,

00:26:39.230 --> 00:26:44.019
it wall typically, it will just simply run serially.

00:26:44.019 --> 00:26:48.519
So you don't have to do anything to step to OpenMP.

00:26:48.519 --> 00:26:51.490
But then of course you won't get any performance
gain because it's just running serially,

00:26:51.490 --> 00:26:55.779
so what you need menu to do is say profile your
code, maybe with Shark or something like that

00:26:55.779 --> 00:27:03.210
and find out where those hot kernels or hot loops are
in your code and then you wanna go in and use OpenMP

00:27:03.210 --> 00:27:08.210
to parallelize those, only those parts
of the code, leaving the rest serial.

00:27:08.210 --> 00:27:16.549
So the way that works is you'll if you got some loop,
some hot loop, you'll put in some OpenMP directives

00:27:16.549 --> 00:27:23.889
for that loop and what happens is then the Compiler
will insert code into your executable that when it hits

00:27:23.890 --> 00:27:32.200
that loop, some threads will be spawned and then each thread
will work on a certain subset of the iterations of the loop

00:27:32.200 --> 00:27:37.640
and then at the end of the loop all the threads will
exit except for of course, the one master thread.

00:27:37.640 --> 00:27:40.920
Now it's very important to realize
that this happens implicitly.

00:27:40.920 --> 00:27:48.160
You never actually say I wanna create now, I wanna create
ten threads, you don't do that explicitly like you would

00:27:48.160 --> 00:27:54.670
in say pthreads, you simply tell the Compiler,
this loop can be parallelized in this manner

00:27:54.670 --> 00:27:58.490
and the Compiler will do it for you, it
will inject the code that's necessary

00:27:58.490 --> 00:28:05.990
to create those threads and delete them at the end.

00:28:05.990 --> 00:28:11.839
Now as this here will be about gFortran
and OpenMP just come to gFortran,

00:28:11.839 --> 00:28:17.449
actually if you get a very recent
version of gFortran or GCC, you can use it.

00:28:17.450 --> 00:28:21.000
I think you need version 4.2 or later.

00:28:22.049 --> 00:28:23.539
And it's very easy to use.

00:28:23.539 --> 00:28:28.349
You simply compile your code as normal,
but you add the -fopenmp option.

00:28:28.349 --> 00:28:36.159
And then at run time, you typically have to tell the program
how many threads it should spawn when it enters a loop

00:28:36.160 --> 00:28:42.170
and the way you can do that is just simply set an
environment variable, the OMP number of threads.

00:28:42.170 --> 00:28:45.330
And there are other ways of doing it as well.

00:28:45.329 --> 00:28:49.960
You can do it actually dynamically
inside your program if you like.

00:28:49.960 --> 00:28:55.950
So let's get away from all this abstract talk
and into an actual example, concrete example.

00:28:55.950 --> 00:29:00.080
Something that should be familiar to most Fortran programmers.

00:29:00.079 --> 00:29:05.980
This is a full Fortran 90 program and all
it does is do a matrix multiplication.

00:29:05.980 --> 00:29:12.000
Now of course you wouldn't do it in this way in a
real production code, you would hopefully use MKL

00:29:12.000 --> 00:29:15.450
or you would use accelerate or
some high performance library.

00:29:15.450 --> 00:29:19.340
But of course everyone knows what a matrix matrix
multiplication is so we can use that as an example.

00:29:19.339 --> 00:29:26.509
So in the middle there in gold, is the
actual loops that perform the multiplication.

00:29:26.509 --> 00:29:30.119
Double loop with a dot_product in the middle.

00:29:30.119 --> 00:29:33.169
So how do we parallelize this in OpenMP.

00:29:33.170 --> 00:29:35.840
Well it's actually very easy.

00:29:35.839 --> 00:29:42.639
You add a single directive just in
front of the loop, the first loop.

00:29:42.640 --> 00:29:48.690
And an OpenMP directive is just actually in
Fortran, it's just a comment, a Fortran comment.

00:29:48.690 --> 00:29:55.700
So the nice thing about that is if then go and compile
this code on a serial compiler that's for serial use,

00:29:55.700 --> 00:30:00.450
it doesn't understand OpenMP, it
will still compile and run serially.

00:30:00.450 --> 00:30:02.140
So that's a real advantage of OpenMP.

00:30:02.140 --> 00:30:07.680
You don't have to, you don't have to jump
in, you can literally just dip your toe in

00:30:07.680 --> 00:30:14.470
and just so your program can be
run serially or with OpenMP.

00:30:14.470 --> 00:30:21.220
Now a directive has the $OMP that
indicates to the Compiler that it's a directive

00:30:21.220 --> 00:30:24.180
and in this case we're doing a parallel do, which means

00:30:24.180 --> 00:30:31.620
that basically tells the Compiler the loop that's following
this one, the one with the I index, parallelize that

00:30:32.829 --> 00:30:40.699
and the private(j), what that means is that each thread
that's produced, should have it's own private copy

00:30:40.700 --> 00:30:46.549
of the j variable and that's actually important because
otherwise all of these threads would be trying to ride

00:30:46.549 --> 00:30:51.970
into the j variable and you'll get what's called a race
condition which I'll talk a little bit about shortly.

00:30:56.930 --> 00:31:03.279
Okay, so what happens when you do
this, when you run this, run this code?

00:31:03.279 --> 00:31:09.740
Imagine this is a thread, this blue line is a thread,
single thread so your program initially starts off serial,

00:31:09.740 --> 00:31:17.039
it will run along serial and in the middle hit
this loop and then it will spawn multiple threads

00:31:17.039 --> 00:31:23.799
and these threads will go through the loop and at the
end of the loop, it will return to a serial application.

00:31:26.160 --> 00:31:31.590
So one really nice aspect of OpenMP is that you
can parallelize your code one loop at a time.

00:31:31.589 --> 00:31:36.659
You don't have to do the big bang change to five
hundred thousand lines of code, you can literally go

00:31:36.660 --> 00:31:41.090
and find those hot spots, parallelize them
and leave the rest serial.

00:31:41.089 --> 00:31:45.539
And over time, as you think oh I need
another performance boost, you can go back

00:31:45.539 --> 00:31:48.039
and do another loop or another section of the code.

00:31:48.039 --> 00:31:51.819
But you can do it incrementally
and that's an important part of it.

00:31:51.819 --> 00:31:56.809
Now just to return to what actually
happens with these threads, as I said,

00:31:56.809 --> 00:31:58.690
each thread takes a subset of these directives.

00:31:58.690 --> 00:32:06.380
Can you imagine this is the matrix C thread one,
say we had twenty threads, three thousand rows,

00:32:06.380 --> 00:32:11.030
then thread one will simply do iterations one
to a hundred, thread two will do one hundred

00:32:11.029 --> 00:32:12.829
and one to two hundred, etcetera, etcetera.

00:32:12.829 --> 00:32:15.699
So it's, in the default case it's very simple.

00:32:15.700 --> 00:32:18.430
They simply split up the work, share the work.

00:32:18.430 --> 00:32:28.310
Now just to show you that it's not totally, it's
very simple, but it can also produce good results,

00:32:28.309 --> 00:32:35.669
here is a test I did on, of exactly this code on a 3
gigahertz Intel Mac Pro 4 core and what you can see is

00:32:35.670 --> 00:32:42.330
that on 4 processes or 4 cores, you can get
a speed of around 3.6 with this code.

00:32:42.329 --> 00:32:47.129
Which is actually quite acceptable, quite
reasonable for such a simple change.

00:32:47.130 --> 00:32:52.030
At the bottom there are the Compiler options that
I used, but they're not really of that important

00:32:52.029 --> 00:32:57.069
in this case, we're just interested in the scaling.

00:32:57.069 --> 00:33:02.159
So I mentioned before race conditions
and I wanna return to that a bit.

00:33:02.160 --> 00:33:08.640
Here we've got the same loop and imagine that we forgot to
include the private(j), then we will have introduced a bug,

00:33:08.640 --> 00:33:14.110
a race condition and what we'll, as I said, what will
happen is, all of those threads will try to write

00:33:14.109 --> 00:33:21.179
to this j variable at one time, so one might be writing one
into j, another might be writing five into J and the results

00:33:21.180 --> 00:33:25.920
that you get out will be completely
dependent on a race between the threads.

00:33:25.920 --> 00:33:33.140
It's very undesirable and in your best case if
you're really lucky, your program will crash.

00:33:33.140 --> 00:33:35.160
That's a good situation.

00:33:35.160 --> 00:33:38.769
You'll know that there's a problem
and you'll be able to fix it.

00:33:38.769 --> 00:33:43.900
In the, in a worst case, you'll simply get the wrong
answer and you won't know that it's the wrong answer

00:33:43.900 --> 00:33:49.269
and in a very bad case, but still
possible, it might run well, properly,

00:33:49.269 --> 00:33:55.069
999 times and on the thousandth time it
will crash or produce the wrong answer.

00:33:55.069 --> 00:33:58.149
So this makes debugging very difficult in OpenMP.

00:33:58.150 --> 00:34:01.330
You have to be very careful with what you're doing.

00:34:01.329 --> 00:34:04.259
And that I guess is the flip side to the ease of use.

00:34:04.259 --> 00:34:10.789
It's easy to learn, easy to introduce, but the
flip side, debugging can become very difficult.

00:34:12.269 --> 00:34:14.250
So what else is in there?

00:34:14.250 --> 00:34:15.250
What else is in OpenMP?

00:34:15.250 --> 00:34:19.239
I've shown you a very simple parallel loop construction,
I'll just go through a few of the other things you can do,

00:34:19.239 --> 00:34:21.739
just to give you an idea of what's possible.

00:34:21.739 --> 00:34:23.329
First of all you've got things like scheduling.

00:34:23.329 --> 00:34:28.940
You can change the scheduling from static to
dynamic, so imagine that you've got a loop

00:34:28.940 --> 00:34:33.720
where some iteration take a lot
longer to run than other iterations.

00:34:33.719 --> 00:34:37.929
In that case, it would be better to do some sort of
dynamic load balancing and that's possible in OpenMP.

00:34:37.929 --> 00:34:42.629
You can have each of the threads go away, do
some work, come back and ask for more work

00:34:42.630 --> 00:34:45.690
and effectively that balances things out.

00:34:47.210 --> 00:34:53.530
You've also got runtime routines, actual subroutines you
can call, to query things that the OpenMP system has,

00:34:53.530 --> 00:34:57.890
such as how many threads are running at the
moment and what thread am I, that sort of query.

00:34:57.889 --> 00:35:00.179
You can also do things like locking.

00:35:00.179 --> 00:35:03.559
But typically don't need to use these routines.

00:35:03.559 --> 00:35:10.170
You can get away with the directives for most things, but
if you really wanna get into the nitty gritty and low level,

00:35:10.170 --> 00:35:13.750
low level threading, you can use these routines.

00:35:13.750 --> 00:35:17.929
Plus there are a few other things
that I'll just demonstrate now.

00:35:17.929 --> 00:35:21.399
So we mentioned there was a parallel
loop that was a single block,

00:35:21.400 --> 00:35:25.750
but you can actually make these parallel
sections go over much wider area of the code.

00:35:25.750 --> 00:35:33.869
So in this case, there's two loops and a
parallel section right around those loops.

00:35:33.869 --> 00:35:39.730
So what happens is you come in serially, you hit
this parallel section and it spawns these threads,

00:35:39.730 --> 00:35:43.990
they go through the first loop, do some
work and then there's actually a barrier,

00:35:43.989 --> 00:35:47.189
a synchronization at the end of
the first loop that's implicit.

00:35:47.190 --> 00:35:50.880
You can actually get around that
by setting a no wait directive.

00:35:50.880 --> 00:35:57.090
And then they will continue on do the
second loop and before returning to serial.

00:35:57.090 --> 00:36:00.329
So the message is there that you
can actually make whole sections

00:36:00.329 --> 00:36:02.460
of your code parallel, not just, not just a single loop.

00:36:02.460 --> 00:36:07.139
Now you can do the other way around as well.

00:36:07.139 --> 00:36:12.250
If you've got a very big parallel section, you
can also do a serial section embedded inside that.

00:36:12.250 --> 00:36:16.860
So here we've got a parallel loop
and then we've got this master block

00:36:16.860 --> 00:36:20.410
and the master block is a serial section of the code.

00:36:20.409 --> 00:36:23.279
So we wanna do summation just on one thread.

00:36:23.280 --> 00:36:27.210
So what happens here in this case, the master
thread continues, the others just wait.

00:36:27.210 --> 00:36:30.970
And then we go through the second loop
in parallel again and back to serial.

00:36:30.969 --> 00:36:38.809
So you can have parallel sections in serial sections
and you can have serial sections in parallel sections.

00:36:38.809 --> 00:36:47.039
Now you might be thinking oh it's a shame to do that
summation serially, that's gonna be a performance problem

00:36:47.039 --> 00:36:50.320
and you can get around that sort of
thing with something like a reduction.

00:36:50.320 --> 00:36:53.880
So a reduction is like an arithmetic
operation that you can do in parallel.

00:36:53.880 --> 00:37:00.300
So this is exactly the same code, but I've
replaced that master block with a reduction.

00:37:00.300 --> 00:37:07.140
So what happens is you enter the, enter the
parallel section, you do the first loop parallel,

00:37:07.139 --> 00:37:12.609
then there's a barrier and then you go through this
reduction serially and what in parallel as well

00:37:12.610 --> 00:37:18.519
and what happens there is that each thread has it's
own sum variable and adds up all of the array elements,

00:37:18.519 --> 00:37:26.579
but at the end of the loop, all of those partial sums are
added together and put back in to the global sum variable.

00:37:26.579 --> 00:37:33.309
So this is a way of doing summation in parallel
and then we do the last loop as before.

00:37:33.309 --> 00:37:38.980
So a lesson there is you can do basic mathematical
operations in parallel as well, reductions for example.

00:37:38.980 --> 00:37:46.909
Now I've said that everything's in OpenMP is
pretty much loop centric, that's not entirely true.

00:37:46.909 --> 00:37:51.069
You can do other types of threading
as well which are not loop based.

00:37:51.070 --> 00:37:57.400
For example, here's a section, a
parallel sections, a construction.

00:37:57.400 --> 00:38:06.539
So in this case what happens is, you split up the loops
and then the first thread will do the first section,

00:38:06.539 --> 00:38:09.779
the second thread will simultaneously do the second section

00:38:09.780 --> 00:38:14.660
and the third will do the third section
and then continue on in parallel.

00:38:14.659 --> 00:38:18.839
Now of course, your sections have
to be basically independent code.

00:38:18.840 --> 00:38:25.539
You basically have to have, this will only work if set up
A, set up B and set up C are independent of one another

00:38:25.539 --> 00:38:29.059
and you also notice that this will only scale

00:38:29.059 --> 00:38:33.699
to three threads before this particular
part of the code will not scale any further.

00:38:33.699 --> 00:38:37.289
So that's, that's a disadvantage, but this
could be useful for certain parts of the code,

00:38:37.289 --> 00:38:41.000
which are not so easy to do with parallelizing loops.

00:38:41.000 --> 00:38:46.650
So independent sections of the code can run concurrently.

00:38:46.650 --> 00:38:50.820
So that's pretty much all I've got
to say, just a few points in summary.

00:38:50.820 --> 00:38:53.880
OpenMP is easy to learn.

00:38:53.880 --> 00:38:59.110
It only really works on shared memory machines, and
so that's certainly something you should consider,

00:38:59.110 --> 00:39:01.789
but if you're writing desktop apps,
now is a good time to learn it.

00:39:01.789 --> 00:39:06.519
Because you really can take advantage
of those 8 core Mac Pros quite easily.

00:39:07.840 --> 00:39:16.180
You can convert from serial to OpenMP one loop at a time,
iteratively, which is a very useful attribute of OpenMP.

00:39:16.179 --> 00:39:20.879
It's not the case for example with MPI where
typically you need to do a big bang change

00:39:20.880 --> 00:39:24.519
to your whole code to get it to parallelize.

00:39:24.519 --> 00:39:30.210
So with OpenMP you can, you can just make one part of
the code fast and then three months later you can go back

00:39:30.210 --> 00:39:35.210
and make another part fast and in the
interim time, you can be using that code.

00:39:36.449 --> 00:39:41.299
On flip side of course is that race
conditions can make debugging very difficult.

00:39:42.510 --> 00:39:46.250
To use OpenMP and gFortran just use a -fopenmp flag

00:39:46.250 --> 00:39:51.739
and you'll need a very new version
of gFortran, I think it's 4.2.

00:39:51.739 --> 00:39:56.659
Okay now I'll just pass back to Dave to finish
off and talk about the future of Mac research.

00:39:56.659 --> 00:40:03.789
( applause )

00:40:03.789 --> 00:40:08.300
>> So just really quickly, a couple points.

00:40:08.300 --> 00:40:16.490
We are at MacResearch, having a new or redesigning the site
from the ground up and to make it easier for people to use

00:40:16.489 --> 00:40:22.549
to get information and it'll be the same great
site, actually it'll be a better site I think,

00:40:22.550 --> 00:40:28.720
but it'll be easier to find what you want, do what you want
on the site and to really contribute to the site as well

00:40:28.719 --> 00:40:33.119
and you'll notice that real programmers do code in Fortran.

00:40:33.119 --> 00:40:36.819
So keep that in mind and Fortran is
always welcome at MacResearch.

00:40:36.820 --> 00:40:46.620
But the other thing I wanted to announce as well is we are
partnering up with the guys at Mac ports and to bring a new,

00:40:46.619 --> 00:40:52.609
new initiative called OpenMacForge, it's
gonna be, it's gonna come out in two phases,

00:40:52.610 --> 00:40:59.110
but the first phase of course is pre-compiled binaries with
installer packages and we are hoping to get that rolled

00:40:59.110 --> 00:41:04.230
out really soon and what it'll allow you to do is if you're
a scientist and there are common applications that you'd

00:41:04.230 --> 00:41:09.809
like to use, but you don't wanna go through the trouble of
having to build them all yourselves, we're going to help,

00:41:09.809 --> 00:41:15.670
we're gonna build them for you and you can just
double click, install, get them up and running

00:41:15.670 --> 00:41:20.340
and get your work going in no time,
so and of course it free as well.

00:41:20.340 --> 00:41:22.220
That service will be provided free as well.

00:41:22.219 --> 00:41:28.549
So you can look forward to that in the very near
future and so with that I will pass it off to Wood

00:41:28.550 --> 00:41:33.800
from Absoft, from Absoft and he will continue.

00:41:33.800 --> 00:41:34.610
( applause )

00:41:34.610 --> 00:41:36.670
>> Thank you very much.

00:41:40.429 --> 00:41:48.690
Hello I'm Wood Lotz from Absoft and many of you may
know, we've been doing tools for the Mac since 1984.

00:41:48.690 --> 00:41:55.079
We did have a 64 bit Fortran Cdompiler for the
power based machines and today I'm here to talk

00:41:55.079 --> 00:41:59.849
about a brand new product we have
for the Intel based machines.

00:42:01.539 --> 00:42:07.300
The product is called Absoft Pro Fortran
version 10 and it's a completely new product.

00:42:07.300 --> 00:42:12.269
It's not just port of our previous
product that we had on Power.

00:42:12.269 --> 00:42:19.610
The two biggest differences is it
includes a lot of new compiler technology,

00:42:19.610 --> 00:42:25.579
which is all oriented towards improved
performance and we have introduced a new version

00:42:25.579 --> 00:42:31.219
of our Fx Debugger, Fx3 which is included with the product.

00:42:31.219 --> 00:42:33.719
We began shipping this about three months ago.

00:42:33.719 --> 00:42:40.549
We've gotten excellent feedback and the
keys that we tried to work with to develop

00:42:40.550 --> 00:42:47.640
for this new product were the performance and reliability
and ease of use for the Compiler and the basic tools,

00:42:47.639 --> 00:42:53.429
but also all Pro Fortran products, at
least we consider a complete solution

00:42:53.429 --> 00:42:58.649
and that means they include not only
the standard Compiler and Debugger,

00:42:58.650 --> 00:43:03.230
but also development environment
designed for Fortran programmers.

00:43:03.230 --> 00:43:07.349
Again, this is shipping now, it
installs on both Tiger and Leopard.

00:43:07.349 --> 00:43:12.380
So what I'm gonna talk about here is an overview
of the product and everything it includes

00:43:12.380 --> 00:43:15.260
and then we're gonna talk about performance.

00:43:16.809 --> 00:43:25.329
Okay an overview here summary of the Pro Fortran version 10
product is obviously we wanna generate really fast code,

00:43:25.329 --> 00:43:27.029
32 and 64 bit.

00:43:27.030 --> 00:43:34.030
It's a Compiler that we have been using for quite awhile,
uses a lot of technology from Cray Research.

00:43:34.030 --> 00:43:43.100
We licensed that in 1993 I think and Sun and SGI use
licenses as well, so our Compilers are source compatible

00:43:43.099 --> 00:43:48.469
with Cray and Sun and Absoft across
the board with no problems.

00:43:48.469 --> 00:43:53.039
Since that time we've also added quite a
few extensions from all the work stations

00:43:53.039 --> 00:43:59.070
and big iron and several from Fortran in 2003.

00:43:59.070 --> 00:44:07.860
Of note with the new version 10 Compiler, are the optimizers
in the back end which we'll get to in a little bit.

00:44:07.860 --> 00:44:14.860
Support for threading is obviously important and we
make that available as well as automatic vectorization

00:44:14.860 --> 00:44:19.490
and the auto-vectorization is a
completely new feature with this product.

00:44:19.489 --> 00:44:27.329
Building mixed Fortran and C, as other people have stated as
well, is critical to pretty much everybody in the HPC space

00:44:27.329 --> 00:44:34.940
and so that's been something we've spent a lot of time
with and that is easy to do but in the documentation,

00:44:34.940 --> 00:44:40.920
we also spend a little extra time with examples and
instructions on just how to do that in different ways.

00:44:40.920 --> 00:44:53.590
The FX3 debugger which we introduced new here like
I said, is that we've been doing Fortran debuggers

00:44:53.590 --> 00:44:56.930
for probably 15 years, starting out with the Fx debugger.

00:44:56.929 --> 00:45:05.639
Fx3 is the latest iteration and this debugger adds,
includes all the functionality that you would need

00:45:05.639 --> 00:45:08.809
for most Fortran debugging but also supports C as well.

00:45:08.809 --> 00:45:12.070
So mixed Fortran and C debugging is important.

00:45:12.070 --> 00:45:20.510
Something that was important that we did with this product
is we use a standard code base across Mac, Windows and Linux

00:45:20.510 --> 00:45:25.160
and that's really good for you guys because
it means we can keep the product reliable,

00:45:25.159 --> 00:45:30.199
easier and also add features faster
across all those platforms.

00:45:30.199 --> 00:45:33.239
It has the same look at feel on all the platforms.

00:45:33.239 --> 00:45:35.659
It also is better for us because it supports easier.

00:45:35.659 --> 00:45:44.829
The development environment that we include
is complete and it provides benefits

00:45:44.829 --> 00:45:48.750
that aren't available elsewhere for most Fortran programs.

00:45:48.750 --> 00:45:57.010
Some of those benefits include the fact that it supports
multiple compilers both Fortran and C as plug ins.

00:45:57.010 --> 00:46:02.620
So you can mix and match different compilers
and build your codes as you want to.

00:46:02.619 --> 00:46:07.079
We also include an application
framework, which we call MRWE,

00:46:07.079 --> 00:46:12.779
which stands for Macintosh Runtime Window Environment
and that automatically puts a Mac style front end

00:46:12.780 --> 00:46:15.140
on your application if you choose to do so.

00:46:15.139 --> 00:46:20.879
The application framework's written in Fortran,
all the sources included if you're really daring

00:46:20.880 --> 00:46:25.230
and wanna do some programming with Mac toolbox from Fortran.

00:46:25.230 --> 00:46:33.039
We also support a variety of third party products
which can plug into the development environment.

00:46:33.039 --> 00:46:38.820
We included Programmer's Editor for example, but
if you wanna use BBEdit or something that's fine.

00:46:38.820 --> 00:46:41.930
We also support different math libraries.

00:46:41.929 --> 00:46:47.489
For example, IMSL will plug in and if you have
some underlying core routines you wanna use,

00:46:47.489 --> 00:46:53.159
you can just select those options and then automatically
everything hooks together properly at build time.

00:46:53.159 --> 00:46:55.319
Just to make it simple for everybody.

00:46:55.320 --> 00:47:02.550
And again, this environment looks the same on
Windows, Mac and Linux so for people that program

00:47:02.550 --> 00:47:07.789
in multiple environments, the learning curve
is just one, it's simple and easy to do.

00:47:07.789 --> 00:47:16.960
As far as math libraries, we include the BLAS,
LAPACK, atlas, FFT's and things like that.

00:47:16.960 --> 00:47:21.030
Which are already pre-built, preconfigured and ready to run.

00:47:21.030 --> 00:47:26.090
We also include some 2D and 3D
graphics as well as HDF libraries.

00:47:26.090 --> 00:47:28.380
So this is all a part of the package.

00:47:28.380 --> 00:47:30.380
Excuse me.

00:47:30.380 --> 00:47:36.980
Okay first, let me take a look just at a screen, so
we can get an idea what we're talking about here.

00:47:36.980 --> 00:47:38.579
Oh, just put nothing else to buy or learn.

00:47:38.579 --> 00:47:41.519
Cool. Thanks guys.

00:47:41.519 --> 00:47:41.980
Very good.

00:47:41.980 --> 00:47:46.389
Okay, here's a screen and you can
see across the top, this is a sample.

00:47:46.389 --> 00:47:52.710
We support different Fortran Compilers and
the GNU C Compiler on the Intel Macs.

00:47:52.710 --> 00:48:01.380
On the power Macs we support of course the Absoft
Compilers, the GNU Apple Compilers and the IBM XL Fortran

00:48:01.380 --> 00:48:07.110
and XLC Compilers in this environment and
depending on which compiler you would choose,

00:48:07.110 --> 00:48:10.530
you can set a whole bunch of options
which makes things simple.

00:48:10.530 --> 00:48:18.500
You can build as you can see there, MRWE applications,
whatever you wanna do, position independent code.

00:48:18.500 --> 00:48:24.940
On the right hand side we have a thing called speed math,
which is a really aggressive floating point optimizer,

00:48:24.940 --> 00:48:30.579
so if your applications can trade a little
accuracy out there in the far digits

00:48:30.579 --> 00:48:33.840
for some speed, then you might wanna give it a try.

00:48:33.840 --> 00:48:36.100
Also there's command lines available.

00:48:36.099 --> 00:48:38.739
Options at the bottom so you can do that.

00:48:38.739 --> 00:48:44.859
Anyway, it's a typical environment but it's
designed for Fortran building as well as C

00:48:44.860 --> 00:48:49.240
and most of the environments are designed
primarily for C. So that's why we did it.

00:48:49.239 --> 00:48:52.519
We'll come back to that slide in a second.

00:48:52.519 --> 00:48:59.750
Here is the screen shot of the Debugger
and again, it includes standard things

00:48:59.750 --> 00:49:02.559
that you need for graphical debugging environment.

00:49:02.559 --> 00:49:09.940
This particular sample is an example of array elements
and you can you know, open up a lot of screens,

00:49:09.940 --> 00:49:13.159
whatever you wanna do for looking at that.

00:49:13.159 --> 00:49:19.619
Okay, that's a quick summary of the environment
itself and the features that we include.

00:49:19.619 --> 00:49:28.339
But performance is key in the Fortran world of
course so that's something we'll talk about next.

00:49:28.340 --> 00:49:34.660
And because this is a completely new compiler that
we moved to the or built for the Intel machines

00:49:34.659 --> 00:49:40.019
and we didn't port our other compiler
for comparisons, we're gonna be comparing

00:49:40.019 --> 00:49:44.199
against some other compilers available in the space.

00:49:44.199 --> 00:49:46.609
Oops, just a second.

00:49:46.610 --> 00:49:54.180
Okay, what we're using for the basis of our comparison here
is the Polyhedron in Benchmarks and for those of you

00:49:54.179 --> 00:49:59.849
who may not be familiar with Polyhedron, it's a
company in the UK, they're a Fortran specialty shop

00:49:59.849 --> 00:50:06.360
and over the years they've collected a suite of sixteen
programs which represents a good benchmark that a lot

00:50:06.360 --> 00:50:10.480
of people use and basically, you run
the programs, take the geometric mean

00:50:10.480 --> 00:50:14.760
and that's in seconds, so less time is better.

00:50:14.760 --> 00:50:23.700
So as you can see here on 32 bit, we're looking at Tiger
here, the Absoft version 10 ran in 16 seconds and change.

00:50:23.699 --> 00:50:29.859
Right next to that we have our compiler
version 9 from Power running under emulation,

00:50:29.860 --> 00:50:36.590
we just included that for a comparison for people in
the event that some people are still using Rosetta

00:50:36.590 --> 00:50:40.039
to run power developed apps on their Intel boxes.

00:50:40.039 --> 00:50:44.409
There's a huge performance gain available
if you switch to native compilers.

00:50:44.409 --> 00:50:50.089
And then we used gFortran and G95 for comparison here as well.

00:50:50.090 --> 00:51:02.340
In the 64 bit space, we used Absoft Intel, that's 9.1
and also gFortran and again, we're looking good.

00:51:02.340 --> 00:51:10.170
Caveat supply of course because this is just a benchmark,
your mileage will vary and but we did wanna do this to show

00:51:10.170 --> 00:51:14.639
that the new version 10 Compiler
is a very, very good performer.

00:51:14.639 --> 00:51:19.599
And 64 bit code is what we're gonna be
focusing on in the future and that's

00:51:19.599 --> 00:51:26.710
where most of the continued development will be.

00:51:26.710 --> 00:51:36.630
Okay, I've talked about auto-vectorization as a
new addition to the version 10 Compiler and again,

00:51:36.630 --> 00:51:43.269
as was discussed earlier, the goal of auto-vectorization
is certainly to execute multiple loops at the same time.

00:51:43.269 --> 00:51:50.980
We also have a vectorization report which you can
print out and it shows loops that were not vectorized.

00:51:50.980 --> 00:51:58.050
You can identify them and then you can go
back and if you want to and look at the loops

00:51:58.050 --> 00:52:02.640
and see if maybe a little massaging
of code could change the loop

00:52:02.639 --> 00:52:06.929
so it could be vectorized and improve
performance even further.

00:52:08.000 --> 00:52:10.280
Again, how well does this work?

00:52:10.280 --> 00:52:19.420
Here's a sample here where we took another benchmark and
on the left the unoptimized code and then we optimized it

00:52:19.420 --> 00:52:23.909
with different compilers and came out very well.

00:52:23.909 --> 00:52:31.989
So the new version 10 Compiler
is a very, very good performer.

00:52:31.989 --> 00:52:39.309
So basically that's my snapshot of what
we've got for the new Intel based Macs.

00:52:39.309 --> 00:52:47.750
So to summarize, we have 32 and 64 bit Compiledrs
so you can install in Tiger or Leopard, just runs,

00:52:47.750 --> 00:52:54.710
auto-parallelization, auto-vectorization so you can
take care, take advantage of the multi-core machines.

00:52:54.710 --> 00:52:58.380
We have a brand new debugger we introduced for this.

00:52:58.380 --> 00:53:01.950
Fortran and mixed C is easy to do.

00:53:01.949 --> 00:53:10.289
The IDE is common across a variety of
platforms and the price starts at just $299.

00:53:10.289 --> 00:53:16.139
Another thing we wanted to announce, actually just
happened this week, so I'm happy to announce it here is

00:53:16.139 --> 00:53:22.699
that in addition to shipping the first 64
bit commercial Compiler for Intel Macs,

00:53:22.699 --> 00:53:30.429
we will be shipping IMSL beginning next week and I
believe we're the first company to do that as well.

00:53:30.429 --> 00:53:36.809
And in addition, for anybody who has
the Power based Mac and absoft and IMSL,

00:53:36.809 --> 00:53:39.070
we will be offering special upgrade pricing.

00:53:39.070 --> 00:53:43.900
This is something we worked out with VNI
to extend to our existing customer base.

00:53:43.900 --> 00:53:49.250
There'll be details on our site about that shortly.

00:53:49.250 --> 00:53:55.239
Okay well that's the product that we have and
of course we make it, but we want you to try it.

00:53:55.239 --> 00:54:03.259
So that's the whole goal and we have,
we like to, I'd like to encourage you

00:54:03.260 --> 00:54:06.810
to come to our website, that's absoft.com.

00:54:06.809 --> 00:54:13.889
We have a free thirty day demo that you can
download, try it on your code, try it on your machine

00:54:13.889 --> 00:54:18.319
and if you have any questions, email us or call us.

00:54:18.320 --> 00:54:27.010
We prefer email as the method of support question
handling, but we do maintain real live knowledgeable people

00:54:27.010 --> 00:54:32.480
that will answer questions and help you solve your
problems if you wanna call us during business hours.

00:54:32.480 --> 00:54:34.429
So that's it, thank you very much.