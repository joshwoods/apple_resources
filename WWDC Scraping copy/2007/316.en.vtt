WEBVTT

00:00:20.170 --> 00:00:24.429
>>First off since you have choose to
come to this session today instead of one

00:00:24.429 --> 00:00:29.960
of the other very interesting ones happening simultaneously
we suspect that you, like us, believe that performance

00:00:29.960 --> 00:00:35.679
of your applications is a very important and key
part of how you actually create those applications.

00:00:35.679 --> 00:00:40.479
First and foremost because, well, nobody likes to
sit around and wait for their computer to compute;

00:00:40.479 --> 00:00:43.750
they actually want to really be doing productive work.

00:00:43.750 --> 00:00:47.799
There're also obviously a lot of smaller
and more subtle details like the fact

00:00:47.799 --> 00:00:53.589
that humans actually work more efficiently and better
if after they click on a button like a real button

00:00:53.590 --> 00:00:59.790
in the real world it responds immediately to
their actions and doesn't have a noticeable delay.

00:00:59.789 --> 00:01:07.769
That's also a performance issue or also when you think about
it as you optimize your code and make it run faster you sort

00:01:07.769 --> 00:01:11.989
of squish down the amount of time it takes to
execute you'll give yourself additional head room

00:01:11.989 --> 00:01:17.329
where the processor isn't doing anything that you can use
in order to provide additional features to your application.

00:01:17.329 --> 00:01:23.079
To give a basic example several years back everyone
started doing background spelling check in word processors

00:01:23.079 --> 00:01:28.500
because they had the head room in order to do
that and finally if you are running an application

00:01:28.500 --> 00:01:33.359
like a game well you know you can always use more
performance because you can use that additional performance

00:01:33.359 --> 00:01:40.329
in order to provide, you know, higher frame rates,
additional quality and detail to your graphics and so on.

00:01:40.329 --> 00:01:45.209
Well there's another way to look at performance
as well in that it's very economical.

00:01:45.209 --> 00:01:51.030
Now once you've optimized your software and crunched it down
so to do a certain fixed amount of work it requires lessons

00:01:51.030 --> 00:01:56.250
and fewer instructions and so on; well if you think
about it this actually means it's taking less power to do

00:01:56.250 --> 00:02:03.480
that same amount of work and if you have to do that work in
a finite amount of time you'll be able to use less hardware,

00:02:03.480 --> 00:02:08.090
smaller system, fewer systems and so on to do
that same amount of work in your fixed time

00:02:08.090 --> 00:02:13.550
and in the real world this means that say, you know,
with portables well you'll be able to do more work

00:02:13.550 --> 00:02:20.960
on a battery charge if you're running optimized code or in
the server space well, you know, to do that fixed amount

00:02:20.960 --> 00:02:26.740
of work and provide your customers with a certain level of
service for their application well you'll be able to do it

00:02:26.740 --> 00:02:33.300
with fewer servers in your rack which take up less
space and less power in your expensive server farms.

00:02:34.430 --> 00:02:36.460
Okay, so how do we get optimized code?

00:02:36.460 --> 00:02:40.780
Well, you know, as developers I'm sure you're
all quite familiar with the programming cycle.

00:02:40.780 --> 00:02:46.729
You write your code, you build it, you test and well if
you're like me you haven't done it perfect the first time

00:02:46.729 --> 00:02:51.500
so something goes wrong, you go back
around and you fix it and go through again.

00:02:51.500 --> 00:02:58.759
Well of course in reality we don't just simply say, "Well
it failed and oh gee, maybe I should try doing something."

00:02:58.759 --> 00:03:05.079
No, what we really do is we go in and we
apply debugger such as say GVD built an X code

00:03:05.080 --> 00:03:07.950
so it can tell us exactly where we failed.

00:03:07.949 --> 00:03:13.349
We failed at say function foo line 25 and not
just well it crashed somewhere in general.

00:03:13.349 --> 00:03:16.379
Also we can do auxiliary things like
say displaying the programs state

00:03:16.379 --> 00:03:19.840
around where we have our crash and so on as well.

00:03:20.879 --> 00:03:25.969
So, performance semi-cycle is actually
very similar to this process.

00:03:25.969 --> 00:03:32.400
Now we're still going around the same loop of writing our
code, building it, testing it only now instead of crashing

00:03:32.400 --> 00:03:38.770
and failing well our users will come back and say, "this
is to slow, I really want it to be faster, you know,

00:03:38.770 --> 00:03:41.490
I'm getting tired of waiting for my computer."

00:03:41.490 --> 00:03:48.450
You know, we could in theory go in and manually tune and
optimize this but unfortunately it's even more difficult

00:03:48.449 --> 00:03:54.489
to close this loop manually than it is with
the original just fixing your bugs loop

00:03:54.490 --> 00:03:59.490
because well first off do you even have a clue as to where
you might want to go in and do your performance tuning,

00:03:59.490 --> 00:04:04.110
you have to have pretty intimate knowledge of the
code and probably only the person who wrote it

00:04:04.110 --> 00:04:10.630
or someone who's worked with them very closely would have
even a clue as to where to start and secondly it's real easy

00:04:10.629 --> 00:04:16.959
to become side tracked and your intuition, your intuition
tells you to go in and look at one part of the code

00:04:16.959 --> 00:04:22.979
but actually it's something which you weren't even thinking
about in another part of the code which is a real problem

00:04:22.980 --> 00:04:25.580
and it's way to easy to get caught in a situation.

00:04:25.579 --> 00:04:29.419
You can spend a couple of weeks trying to optimize and
really beat the heck out of one portion of your code

00:04:29.420 --> 00:04:35.660
that you think is really the problem only to
have your coworker come in and go, "Sorry,

00:04:35.660 --> 00:04:38.230
these were not the functions you're looking for."

00:04:38.230 --> 00:04:45.240
So as a result, what you want to do is
use a tool to actually help you focus

00:04:45.240 --> 00:04:49.670
in on your activity and that's what we have in Shark.

00:04:49.670 --> 00:04:55.259
Shark is effectively a performance debugger to tell
you that which part of your program is slowest;

00:04:55.259 --> 00:04:59.959
foo 25 is the slow part of your program;
that's where you should go in and optimize.

00:05:01.000 --> 00:05:06.060
So, effectively when you think about it what
Shark helps you do is it helps you go in

00:05:06.060 --> 00:05:10.990
and find the performance bottle necks in your code and
it does this by as you're running your code it goes in

00:05:10.990 --> 00:05:15.400
and measures and analyzes, it pokes and
prods your program to see what it's doing

00:05:15.399 --> 00:05:18.000
and gets this information and presents it back to you.

00:05:18.000 --> 00:05:22.170
As it's presenting it back it might also give you some
hints on how to optimize and what you might want to look

00:05:22.170 --> 00:05:30.110
at in your program and we really think that this
is a great and optimal performance tool solution.

00:05:30.110 --> 00:05:32.050
First off because it's really easy.

00:05:32.050 --> 00:05:39.810
I'll be able to show you basically how to use Shark in just
a few slides here coming up next and once we've taken this

00:05:39.810 --> 00:05:46.050
and analyzed your program in a very easy manner it
presents the information acquired with very simple

00:05:46.050 --> 00:05:50.400
and straight forward graphical interface displays
instead of saying making you go through reams

00:05:50.399 --> 00:05:55.239
and reams of textual output like you
would say with something like gprof.

00:05:55.240 --> 00:06:00.670
Now once you've started to use Shark for a
while you realize that it's also quite powerful.

00:06:00.670 --> 00:06:05.890
It offers many different analyzing profiling techniques
you can use to go in and look at your application

00:06:05.889 --> 00:06:11.219
in a wide variety of ways in order to look at different
types of performance issues you may encounter.

00:06:11.220 --> 00:06:16.800
In each one of these options has many different
configuration options to go and play with it in order

00:06:16.800 --> 00:06:21.050
to fine tune it to look at just
your issues that you may have.

00:06:21.050 --> 00:06:25.560
Also it works seamlessly with both 32
and 64 bit applications so you can look

00:06:25.560 --> 00:06:28.490
at any application you might write for Leopard.

00:06:30.050 --> 00:06:33.720
So, I just said Shark was easy to use;
well here let me try to prove it to you.

00:06:33.720 --> 00:06:39.100
In order to actually use Shark well, the first thing
you want to do is debug your program because trying

00:06:39.100 --> 00:06:44.640
to actually fix performance problems when your program
is still crashing is usually kind of pointless.

00:06:44.639 --> 00:06:47.579
Then once you've got your program
working well you want to go in

00:06:47.579 --> 00:06:49.689
and set the build options like you're planning to release.

00:06:49.689 --> 00:06:53.620
You want to get all the optimization
flags at the full optimization and so on.

00:06:53.620 --> 00:06:58.160
So, you're looking at just as your users will
look at your code with one big exception.

00:06:58.160 --> 00:07:02.880
You want to make sure you have all the symbols
enabled in the code and you're not stripping anything

00:07:02.879 --> 00:07:05.389
out because you actually want to
build and use that information

00:07:05.389 --> 00:07:09.039
to allow Shark to tell you where to go in and look.

00:07:09.040 --> 00:07:14.210
So, now that we've got this what you want to do is
get ready to run your program and when you do you want

00:07:14.209 --> 00:07:17.870
to choose the part you reprogrammed, you
want to optimize at any point in time.

00:07:17.870 --> 00:07:21.600
You know, do you want to speed up the
response to button clicks on your interface?

00:07:21.600 --> 00:07:26.280
Do you want to improve the wait times
when you're doing long computation?

00:07:26.279 --> 00:07:28.769
Do you want, if you're a game, do
you want to increase the frame rate

00:07:28.769 --> 00:07:31.349
or do you want to go for higher resolution graphics?

00:07:31.350 --> 00:07:33.189
You know, what is your measurement?

00:07:33.189 --> 00:07:38.709
And once you've got this go in and take a base line
measurement so you kind of have a place to start;

00:07:38.709 --> 00:07:44.180
this is where we're starting our measurement; we
want to try and improve it and you know, you want,

00:07:44.180 --> 00:07:48.490
a key part of this is you need to build,
take this measurement in a repeatable manner

00:07:48.490 --> 00:07:53.530
which can sometimes be very difficult if it's something
that's very user interface intensive because you don't want

00:07:53.529 --> 00:07:58.529
to take different measurements just depending on
how fast a person actually clicked on their mouse

00:07:58.529 --> 00:08:03.579
and once you got this picked out you can go ahead and
use Shark on your program while it's running, you know,

00:08:03.579 --> 00:08:10.689
to analyze it and to do Shark; well when you start
it up this little window which I can fit on a slide

00:08:10.689 --> 00:08:15.810
and actually have to magnify to make it look like it's
filling up a slide, control is really you need to use

00:08:15.810 --> 00:08:20.790
to get going and what you'll need to do is first
pick a way for Shark to look at your program

00:08:20.790 --> 00:08:25.210
and there are several methods and we'll
talk about several of these here today.

00:08:25.209 --> 00:08:27.689
Once you've picked a sample configuration from this list,

00:08:27.689 --> 00:08:31.660
you go in and figure out how you
want to aim Shark at your program.

00:08:31.660 --> 00:08:36.899
Do you want to just have Shark slurp up what's happening
in the entire system while it's going and executing

00:08:36.899 --> 00:08:41.019
and you know it can do this and this is
actually surprising useful in many cases.

00:08:41.019 --> 00:08:45.980
You can say, "Look at how you're interacting with
other applications with the system and so on"

00:08:45.980 --> 00:08:51.240
but you might also want to go in and focus it on say just
your application or you know, have it actually it can go in

00:08:51.240 --> 00:08:57.360
and start an executable file and look
at your file starting from binary disk.

00:08:57.360 --> 00:09:02.700
Now once you've told Shark what you want to
look at well pull the trigger, press start.

00:09:02.700 --> 00:09:05.840
It'll be off and running and doing
some measurements of your application.

00:09:05.840 --> 00:09:11.509
Alternatively you can use option escape the hot key listed
right here above start which will allow you to do this

00:09:11.509 --> 00:09:15.559
without actually touching your mouse
and you can actually change this hot key

00:09:15.559 --> 00:09:19.439
to be anything you want should this a
problem with your particular program.

00:09:19.440 --> 00:09:26.190
So, you're on your way and after a little while later you
can just press this button again, it changes conveniently

00:09:26.190 --> 00:09:29.220
to stop or similarly use option escape to stop

00:09:29.220 --> 00:09:34.330
and you'll stop Shark's measurement
and it can start its analysis phase.

00:09:34.330 --> 00:09:40.550
Alternatively Shark will also stop automatically
after a preset time limit by default.

00:09:40.549 --> 00:09:47.209
Now once you've got all this information collected
and ready to go well, boom, up pops this window all

00:09:47.210 --> 00:09:53.519
at once containing a very concise summary of what happened
in your application during the time it was measured

00:09:53.519 --> 00:09:56.879
and pointing you right at the area
you want to optimize first.

00:09:56.879 --> 00:10:00.789
We'll talk more on how to interpret this in just a moment.

00:10:00.789 --> 00:10:07.779
So, what you can see here is in just a few slides I
was able to show you that how to use Shark essentially.

00:10:07.779 --> 00:10:18.799
So basically it's real easy to use but it can give you a lot
of data really fast and this is...so the real key is trying

00:10:18.799 --> 00:10:26.769
to get Shark to focus in on the key data of importance
to you and so the data it produces is really useful

00:10:26.769 --> 00:10:32.159
and so the rest of this talk we're going to go in and
try to give you a basic tutorial on how you might want

00:10:32.159 --> 00:10:37.409
to choose the analysis options from the analysis
options menu and how to go in and focus it in on the part

00:10:37.409 --> 00:10:41.579
of your program that's of most interest to
you in order to allow you to actually see

00:10:41.580 --> 00:10:43.700
and measure just those items of interest.

00:10:43.700 --> 00:10:49.050
So, expanding upon this a bit what we're
going to do is we're going to show you how

00:10:49.049 --> 00:10:51.979
to use Shark to examine a variety of issues.

00:10:51.980 --> 00:10:56.730
First is your basic CPU bound job, you just need
to speed up something that's processor bound

00:10:56.730 --> 00:11:00.529
and for this we use a time profile technique.

00:11:00.529 --> 00:11:08.439
Another common thing you may face is dealing with how
your application deals with Mac OS 10 and how it interacts

00:11:08.440 --> 00:11:11.670
and for this we suggest the system trace tool.

00:11:11.669 --> 00:11:16.009
If you have a multi threaded application and are
concerned with how the different threads are interacting

00:11:16.009 --> 00:11:21.470
with each other we would also let you consider the system
trace or you might want to consider a slight variation

00:11:21.470 --> 00:11:23.750
on time profile called all thread states.

00:11:23.750 --> 00:11:30.250
We have a Malloc trace for memory allocation
issues, a variety of options for Java tracing

00:11:30.250 --> 00:11:34.799
and if you are encountering actual
problems with how your application interacts

00:11:34.799 --> 00:11:39.889
with the hardware below it well we allow access
to the actual hardware performance counters

00:11:39.889 --> 00:11:46.789
and OS performance counters in order to see what's really
working on a low level and then to wrap up we're going to go

00:11:46.789 --> 00:11:52.069
in and look at how to actually go in and aim Shark at
your code, going beyond just the start and stop buttons

00:11:52.070 --> 00:11:57.450
which I showed you here a moment ago and at the several
other ways you control how Shark is pointed directly

00:11:57.450 --> 00:12:00.280
at issues that may be of interest to you.

00:12:00.279 --> 00:12:07.240
So, for the first part here I'm going to go
and talk about how we can use Shark to look

00:12:07.240 --> 00:12:11.950
at CPU bound jobs using the time profiling technique.

00:12:11.950 --> 00:12:16.100
So, first off just how would we really
want to address a CPU bound job,

00:12:16.100 --> 00:12:20.290
for those of you who don't have
experience with performance tuning?

00:12:20.289 --> 00:12:28.339
Well to give a quick example, let's say we have an
application here where we're currently spending 100%

00:12:28.340 --> 00:12:36.550
of our time for the key area of interest doing some activity
and so we say that's our time baseline and we're just going

00:12:36.549 --> 00:12:39.689
to call it, however many seconds, going to call it 100%.

00:12:39.690 --> 00:12:44.150
Well the first thing you really want to do in order
to do performance optimization is we don't want

00:12:44.149 --> 00:12:49.409
to simply see this block of time, we want to actually
see what was happening; so this is where Shark comes in.

00:12:49.409 --> 00:12:56.329
It takes this time and breaks it down and shows what code
was actually executing over the duration of this time

00:12:56.330 --> 00:13:02.920
so we can see all the different parts of what that time was
actually consisting of and the key thing you should see take

00:13:02.919 --> 00:13:09.459
from this graph is that, "Well hey, we're spending half of
our time in function foo; this is probably the best place

00:13:09.460 --> 00:13:14.379
for us to spend our time and going and optimizing
because we'll get the most bang for our buck."

00:13:14.379 --> 00:13:19.269
So just to give a quick example let's say we go
in and look at the, and interloop of function foo

00:13:19.269 --> 00:13:21.840
and spending a long time on that interloop.

00:13:21.840 --> 00:13:28.780
So we can go in and look at that loop and if we go in and
optimize it, let's say we manage to get a 5 times speed

00:13:28.779 --> 00:13:31.649
up by looking at this little bit of code on foo.

00:13:31.649 --> 00:13:36.559
We'll just by going in and tweaking one little
bit of code here we can cut the amount of time

00:13:36.559 --> 00:13:45.199
that our application has taken during this area of interest
down almost in half, with one little application of Shark.

00:13:45.200 --> 00:13:51.300
Now of course what'll typically happen here is that your
boss will say, "Oh great, well it's still not fast enough.

00:13:51.299 --> 00:13:53.049
You should optimize some more."

00:13:53.049 --> 00:14:00.219
So then what you can do is look at your application again
and what you'll see here is that the second bar has popped

00:14:00.220 --> 00:14:07.410
up and it's now the biggest because we've pushed down
the first one so far and you know I actually like to kind

00:14:07.409 --> 00:14:09.250
of compare those to the game of whack a mole.

00:14:09.250 --> 00:14:12.029
If you've ever gone to the arcade and you know,
seen the little moles popping out of the hole

00:14:12.029 --> 00:14:15.909
and you take the big huge hammer and you go and
beat them down and of course as soon as you beat one

00:14:15.909 --> 00:14:19.439
down the next one pops up and you go
and beat it down and so on and so forth.

00:14:19.440 --> 00:14:24.070
Well performance optimization is the same
way; you beat one down the next one pops

00:14:24.070 --> 00:14:26.730
up and is your next challenge to go after.

00:14:26.730 --> 00:14:35.550
So let's say we're able to go in and tune an interloop
here on bar and we manage to beat that one down as well

00:14:35.549 --> 00:14:40.109
and well you can see here now just by going in
and tuning a couple of interloops in our program

00:14:40.110 --> 00:14:46.039
which Shark basically pointed us at we're able to get a two
and a half times performance speed up on this application

00:14:46.039 --> 00:14:50.279
without even looking at the rest of
the code or working with it in any way.

00:14:50.279 --> 00:14:55.569
Now you can continue on this path ad infin item,
you know, continue to try to get better performance

00:14:55.570 --> 00:14:59.760
but at some point either your product will ship or you'll
get tired of this; your boss will say, "You know, hey,

00:14:59.759 --> 00:15:05.330
that's good enough; we're fast enough" and usually by this
point in time what you get is a distribution more like this

00:15:05.330 --> 00:15:11.070
where you have a variety of functions that are more or less
equal because you've beaten down all the big heavy hitters

00:15:11.070 --> 00:15:15.840
and so it becomes more of an issue of going in and
picking out which ones are the easiest to optimize

00:15:15.840 --> 00:15:21.240
but still even looking at something like this
picking from one among three functions to maybe look

00:15:21.240 --> 00:15:26.899
at optimizing it's much better than, say, to have to look
over the hundreds you may have in your entire program.

00:15:26.899 --> 00:15:32.590
So, how does Shark actually allow us
to go in and see inside our program

00:15:32.590 --> 00:15:35.790
and actually get this information to
see where we want to go and optimize.

00:15:35.789 --> 00:15:42.480
Well what Shark does is it goes in and takes periodical
samples of our program execution; so over the course of time

00:15:42.480 --> 00:15:49.460
as our application executes Shark will go in and peek in
and see what we're doing on an even rate and it's going

00:15:49.460 --> 00:15:52.320
to see everything that our application
is doing; there's no hiding from it here.

00:15:52.320 --> 00:15:57.430
It's going to see the kernel time, drivers, you name it;
whatever is happening it's going to peek in there and see it

00:15:57.429 --> 00:16:00.469
and it can do this with a configured
goal and variable sample rate.

00:16:00.470 --> 00:16:05.360
By default we start with a thousand samples a second
because that works pretty well for most applications

00:16:05.360 --> 00:16:10.899
but you can change it if you want if your application
needs a higher resolution or lower resolution sampling

00:16:10.899 --> 00:16:15.429
and then at each sample point we
actually don't do very much work.

00:16:15.429 --> 00:16:21.449
All we do is we go in and slurp out the call stack of what
your program is actually doing at that particular point

00:16:21.450 --> 00:16:28.930
in time and a few other bits of information such as the
program counter of where the processor is executing,

00:16:28.929 --> 00:16:33.139
processor thread IDs, and a couple of other
numbers and we do this on all the processors

00:16:33.139 --> 00:16:36.319
in the system for today's multi processors systems.

00:16:36.320 --> 00:16:41.150
So, this had a lot of key advantages.

00:16:41.149 --> 00:16:45.220
First and most importantly it has
very low system overhead in order

00:16:45.220 --> 00:16:47.370
to get the amount of information we're actually getting.

00:16:47.370 --> 00:16:54.100
If you actually look at a whole system trace
with Shark you can see that its impact on the CPU

00:16:54.100 --> 00:17:01.500
with typical sample rates is usually less
than 1% of the CPU load; very little factor.

00:17:01.500 --> 00:17:10.119
Similarly in order to record a typical length
traces of maybe 30 seconds, a mi0nute or so on,

00:17:10.119 --> 00:17:15.009
what it actually only requires a few megabits
of RAM in order to store all this information

00:17:15.009 --> 00:17:20.900
and in today's multi gigabyte system this a kind
of a drop in the bucket and not really a big deal.

00:17:20.900 --> 00:17:26.660
So a key factor to think about all this is that
Shark does not affect your results and with any sort

00:17:26.660 --> 00:17:33.210
of performance optimization tool this is very important
because much as with you know, particle physics, you know,

00:17:33.210 --> 00:17:34.680
Heisenberg's uncertainty principle says

00:17:34.680 --> 00:17:39.150
"you know our measuring tool can actually affect what
we're measuring and make our measurement invalid."

00:17:39.150 --> 00:17:42.620
This is also a very applicable
to performance analysis tools.

00:17:42.619 --> 00:17:47.289
You don't want to end up measuring what your performance
analysis tool did you want to measure your application

00:17:47.289 --> 00:17:53.710
and so Shark gets out of the way and lets
you measure your application and not Shark.

00:17:53.710 --> 00:17:59.170
Now going even a step further in this, sort of a
corollary is that Shark doesn't require you to instrument

00:17:59.170 --> 00:18:03.730
or adjust your code or screw it up in any
way in order to take your measurements.

00:18:03.730 --> 00:18:08.549
No code modifications are required in order to use
Shark; there are a few optional ones you can add

00:18:08.549 --> 00:18:14.480
but in most cases you won't need to modify your code
in any way and this even goes to the compilation stage;

00:18:14.480 --> 00:18:19.269
you don't need to recompile with any special
options such as say with gprof you have

00:18:19.269 --> 00:18:27.109
to actually add additional code to get your information
from that and as a result sort of an interesting thing

00:18:27.109 --> 00:18:32.459
about Shark is that even end users can Shark your
application with their own data sets after it's

00:18:32.460 --> 00:18:36.009
out in the field and subsequently
send this data back to you.

00:18:36.009 --> 00:18:41.180
Now I'll talk a little bit more about this a little later.

00:18:41.180 --> 00:18:47.320
So one final issue here which is of key importance;
it's one of our key differences from Xray if you went

00:18:47.319 --> 00:18:52.579
to that talk yesterday is that Shark can get the
execution profile down to the program counter level;

00:18:52.579 --> 00:18:57.740
we can tell you the exact instructions you're
working on and we're able to use that information

00:18:57.740 --> 00:19:01.910
to really aim you in exactly where you want to optimize.

00:19:01.910 --> 00:19:07.769
Now of course all these great advantages do come
with some disadvantages but really they're not

00:19:07.769 --> 00:19:10.079
that bad when you really think about them.

00:19:10.079 --> 00:19:17.389
So if we have some execution going along here, our main
function is running and it's (inaudible) is very small foo

00:19:17.390 --> 00:19:25.550
and bar functions popping up and then at the white
points, white lines where we're actually sampling;

00:19:25.549 --> 00:19:31.259
so what Shark will see when it samples
this is actually this measurement instead

00:19:31.259 --> 00:19:36.019
which looks quite a bit different
and for a couple of reasons.

00:19:36.019 --> 00:19:40.440
Over here on the left side well the foo and
bars are falling between our sample points

00:19:40.440 --> 00:19:44.680
so we're just missing them completely;
we're not getting them at all.

00:19:44.680 --> 00:19:49.980
Over on the right side in contrast the foo and bar
functions are falling right on the sample lines

00:19:49.980 --> 00:19:54.789
and so we're getting them and thinking
that they're larger than they actually are.

00:19:54.789 --> 00:20:01.099
Now the nice effect about this is that statistically
speaking when you look over a large enough set of samples

00:20:01.099 --> 00:20:04.490
which for a typical function is if
you get more than 10 or 20 samples

00:20:04.490 --> 00:20:10.670
in a function that's usually statistically significant
is that these 2 effects will average out over time

00:20:10.670 --> 00:20:13.140
and therefore for all the important heavy hitter functions

00:20:13.140 --> 00:20:21.850
in your program these problems will essentially
completely cancel out and be meaningless.

00:20:21.849 --> 00:20:27.240
Okay, so we've established how Shark can actually
get this information, well how can we get that how

00:20:27.240 --> 00:20:29.990
that time is breakdown and feed it back to you?

00:20:29.990 --> 00:20:35.210
Shark is going to take all these samples
and it's going to grab all of the functions

00:20:35.210 --> 00:20:39.600
where we were executing off the top of the stack,
leaf functions and it just with those it's going to go

00:20:39.599 --> 00:20:45.579
and compile the exact graph that I showed you before,
showing exactly where we were executing over the span

00:20:45.579 --> 00:20:52.199
of time while we were taking these samples broken
down into the various functions we were executing.

00:20:52.200 --> 00:20:56.049
Now we can take this and this looks
really good for 5 samples.

00:20:56.049 --> 00:21:01.990
In the real world of course you're going to have a lot more
samples and probably a whole lot more functions than just 3

00:21:01.990 --> 00:21:08.779
so what we do in order to make this readable is we take this
graph we tilt it over on it's side and then we present it

00:21:08.779 --> 00:21:14.750
in this textual format but it's the same thing; what
you're seeing is instead of the big heavy hitters going

00:21:14.750 --> 00:21:21.250
over to the left is they're just coming up to the top
and being right there obvious and so effectively just

00:21:21.250 --> 00:21:32.410
with this one window we're telling you that, "Hey,
optimize this; you can ignore these down at the bottom."

00:21:32.410 --> 00:21:37.320
Now, this view is what we call the heavy
view; it shows you the leaf functions,

00:21:37.319 --> 00:21:39.439
the functions that are actually executing code.

00:21:39.440 --> 00:21:43.590
It's usually the best place to start your
analysis of your program for the simple reason

00:21:43.589 --> 00:21:47.449
in that it answers the question, you
know, what code is actually giving?

00:21:47.450 --> 00:21:52.950
What code should I look at...and effectively
what we're doing though if you really step back

00:21:52.950 --> 00:21:57.009
and think about this is we're providing
sort of a bottom up view of your code.

00:21:57.009 --> 00:22:00.170
We're starting from the codes actually
executing at the bottom of the call stack,

00:22:00.170 --> 00:22:02.670
the leaf functions and then going back up.

00:22:02.670 --> 00:22:07.720
You can see up here in the upper part I've actually
opened up some of these little disclosure triangles

00:22:07.720 --> 00:22:13.990
that appear next to the function name so we
can actually see the call stacks backing up.

00:22:13.990 --> 00:22:18.400
Now this is really great for going and finding out
say an interloop which is executing all the time

00:22:18.400 --> 00:22:21.570
but sometimes it's not always what you want.

00:22:21.569 --> 00:22:26.669
So in other cases what you may actually
want to do is go in and look at your code

00:22:26.670 --> 00:22:29.570
and at the samples in more of a top down fashion.

00:22:29.569 --> 00:22:30.710
Shark can handle this as well.

00:22:30.710 --> 00:22:38.769
What we can do is take your call stacks and we can walk
them down starting from the bottom and build up a whole tree

00:22:38.769 --> 00:22:43.960
of what is being executed and what
is being called by other functions.

00:22:43.960 --> 00:22:49.480
Once we've built up this call stack tree
well we then can go in and see patterns

00:22:49.480 --> 00:22:52.440
on how these different calls are related to each other.

00:22:52.440 --> 00:22:58.690
For example, in this case well we're calling
square root here in 2 completely different ways

00:22:58.690 --> 00:23:04.920
through 2 completely different code paths and as a result
we may want to optimize those 2 different code paths

00:23:04.920 --> 00:23:12.820
in 2 completely different ways; even assuming that we can't
actually optimize inside this square root function itself.

00:23:12.819 --> 00:23:19.099
So basically what we're answering here is how do we reach
executing code in case we want to consider, you know,

00:23:19.099 --> 00:23:26.750
backing up a level or 2 in the tree and optimizing a higher
level function instead of down, clear down to the bottom.

00:23:26.750 --> 00:23:35.069
So in order to present this again we present it in sort
of the textual format, the kind of columnar chart format

00:23:35.069 --> 00:23:40.480
and this looks exactly the same as what you saw before
except we've just simply flipped over everything

00:23:40.480 --> 00:23:48.029
so now the start function is up a the top; we go
to underscore start main and then we branch off

00:23:48.029 --> 00:23:50.119
into the various functions that occurring.

00:23:50.119 --> 00:23:54.969
This is a very simple little program which nicely,
but a real one, which nicely fits on a slide

00:23:54.970 --> 00:23:58.180
but with your programs it looks very similar.

00:24:00.970 --> 00:24:06.839
So even using the little menu in the lower right
hand corner there we can choose between those 2

00:24:06.839 --> 00:24:12.779
or hey we can split the difference look at them both side
by side and do some comparing contrast with the heavy view

00:24:12.779 --> 00:24:20.799
up on top, the tree view down in the bottom and
see, we can look at our code both ways at once.

00:24:20.799 --> 00:24:25.200
Okay, well for a little simple toy
example like what I can show you

00:24:25.200 --> 00:24:27.779
in this particular session, you know, this looks great.

00:24:27.779 --> 00:24:31.299
We can just look at the whole tree; see
all the patterns right there, no problem.

00:24:31.299 --> 00:24:36.879
Of course with the real application which may have
hundreds of thousands of functions well the tree can get

00:24:36.880 --> 00:24:39.940
so big that it can be really hard to follow.

00:24:39.940 --> 00:24:42.799
So we need to simplify this down.

00:24:42.799 --> 00:24:43.940
Shark can help us out here too.

00:24:43.940 --> 00:24:49.570
What we can simply do is eliminate parts of the tree.

00:24:49.569 --> 00:24:55.439
For example lets say we want to go in and eliminate all the
math library calls; you know this is code supplied to you

00:24:55.440 --> 00:24:59.580
from apple; it's not like you're going to be able to
go in and optimize the internal guts of square root

00:24:59.579 --> 00:25:02.799
because we haven't provided you with the source.

00:25:03.809 --> 00:25:09.869
So by eliminating those what you'll typically want
to do is not just simply throw them out completely

00:25:09.869 --> 00:25:15.479
but take their samples and move them up into
the your function which is calling them;

00:25:15.480 --> 00:25:20.480
so therefore it's basically telling you that, "hey I want
to look at my function that's calling them because I want

00:25:20.480 --> 00:25:24.079
to try to optimize like the number
of times I'm calling those functions

00:25:24.079 --> 00:25:28.289
which is a place I can actually
make an adjustment to the program."

00:25:28.289 --> 00:25:35.099
Once we do that now you'll see we have a much
simpler and easier to read tree without the stuff

00:25:35.099 --> 00:25:40.809
which we can't actually have any control over and we
can now see the appropriate waiting of all the functions

00:25:40.809 --> 00:25:48.149
that we do have control over and this is just one simple
example of what we call Shark's data mining options

00:25:48.150 --> 00:25:53.110
which can go in and allow you to simplify these
large complicated sets of potential call trees

00:25:53.109 --> 00:25:57.269
by doing things like, the example here,
I charged the library to the caller;

00:25:57.269 --> 00:26:03.369
it can completely chop off tree branches which he just
knows are not of interest to you and any other possibilities

00:26:03.369 --> 00:26:09.269
and then going with Shark's ethos of a fairly simple
GUI user interface in order to do this all you need

00:26:09.269 --> 00:26:15.970
to do is select a function in a browser window, go up to the
data mining menu at the top and choose one of these options

00:26:15.970 --> 00:26:20.259
and it'll immediately start chopping off the
functions that aren't of interest to you.

00:26:20.259 --> 00:26:25.059
Now there's a second way as well in that at any
point in time you can go up to the window menu

00:26:25.059 --> 00:26:30.970
and choose show advance settings and this little
drawer will pop out of the side of the window

00:26:30.970 --> 00:26:36.980
Now in this little drawer I have highlighted the area
summarizing all the call stack information which you can go

00:26:36.980 --> 00:26:40.130
and do pretty much the same operation you can from the menu.

00:26:40.130 --> 00:26:46.420
There are also other controls in here too and basically they
control various ways that Shark is displaying information

00:26:46.420 --> 00:26:52.039
in the browser to allow you to customize that display
to highlight the areas that are of interest to you

00:26:52.039 --> 00:26:56.579
and the controls in this drawer are actually very similar
over the course of time depending on what you're looking

00:26:56.579 --> 00:27:02.009
at in order to provide you with the options that could be
helpful with that particular view and we'll actually talk

00:27:02.009 --> 00:27:06.970
about a few more of these options
over the course of the talk.

00:27:06.970 --> 00:27:13.380
Okay, so the browser view profile heavy are
the most common ways to look at your code.

00:27:13.380 --> 00:27:17.730
There are other ways too; for example
we have what is called the chart view.

00:27:17.730 --> 00:27:24.339
It kind of gives you an overview of the code; it graphically
displays all the samples in our code on the basis

00:27:24.339 --> 00:27:27.669
of call stack depths; that's what the height of each bar is

00:27:27.670 --> 00:27:34.519
and what you can see here is very much how your program
changes over time and in many programs we may measure

00:27:34.519 --> 00:27:38.319
or sort of steady state for the time of
measurement and doing pretty much the same thing

00:27:38.319 --> 00:27:40.730
but if you're actually doing multiple
different things over the course

00:27:40.730 --> 00:27:43.730
of time the chart view can make them very obvious.

00:27:43.730 --> 00:27:49.450
For example, we can see over here on the left
we're doing what looks like one thing and over here

00:27:49.450 --> 00:27:54.029
on the right well this looks completely
different and then in the middle we have kind

00:27:54.029 --> 00:28:00.740
of these big funny red spikes popping up; so, you
know, maybe we want to know what the heck are those;

00:28:00.740 --> 00:28:08.839
so we can go in and actually go in and magnify the
interesting sections using this little slider at the bottom

00:28:08.839 --> 00:28:13.159
which controls the magnification
factor on the display and go in

00:28:13.160 --> 00:28:17.570
and zero in on those interesting
parts that we want to look at.

00:28:17.569 --> 00:28:22.549
So in this case we want to go in and zero in
on this red big spike which is the left one

00:28:22.549 --> 00:28:26.119
of the two you just saw in order
to see what that really was.

00:28:26.119 --> 00:28:32.699
So we can magnify it and we can go in and if you
actually click on the graph what will happen is over here

00:28:32.700 --> 00:28:38.990
on the right side we'll show you the call stack
from that particular sample that was taken for you

00:28:38.990 --> 00:28:44.339
and as a result we can see just from looking
and examining this call stack that, "Oh, okay,

00:28:44.339 --> 00:28:50.589
these 2 big red spikes are getting a lot of VM
under square functions, virtual memory functions,

00:28:50.589 --> 00:28:52.379
we were allocating a whole bunch of memory here

00:28:52.380 --> 00:28:58.440
and the kernel actually mapping those
pages in for us so we could use them."

00:28:58.440 --> 00:29:02.700
You can also go in and double click
on the various samples here to go in

00:29:02.700 --> 00:29:06.830
and go back to the profile exactly where you were.

00:29:06.829 --> 00:29:13.089
Now everything I've shown you so far has basically
allowed you to see down to say the function level

00:29:13.089 --> 00:29:17.429
but as I mentioned before, well sampling
allows you to see down to the instruction level

00:29:17.430 --> 00:29:19.900
where you were executing, what you might want to optimize.

00:29:19.900 --> 00:29:22.080
How can we show you that?

00:29:22.079 --> 00:29:24.000
Well, it's really simple.

00:29:24.000 --> 00:29:30.869
We'll go beyond those function names, just come in double
click anywhere in the source name, the window changes.

00:29:30.869 --> 00:29:35.329
What it does is it shows you the code which
is executing and not only are we going

00:29:35.329 --> 00:29:39.359
to show you the code here we've also color coded it.

00:29:39.359 --> 00:29:44.669
So you can see here that the code which is actually
executing the most has been color coded to,

00:29:44.670 --> 00:29:49.509
in these hot colors so you can see that,
"Hey, this is where you should look.

00:29:49.509 --> 00:29:55.539
This particular line of code is what you should optimize"
and if you happen to have multiple of these spots scattered

00:29:55.539 --> 00:30:00.170
about your function well over here in the scroll
bar you can see this isn't quite your typical run

00:30:00.170 --> 00:30:03.960
of the mill scroll bar; it actually has the
coloring extended out and on to the scroll bar

00:30:03.960 --> 00:30:09.049
so you can grab the thumb and just simply move
to these different yellow points and to see all

00:30:09.049 --> 00:30:13.500
of the different hot spots in your function.

00:30:13.500 --> 00:30:19.690
Now once you pick out the part you want to go in and you
thought of all of it, "Hey, I think I can optimize this

00:30:19.690 --> 00:30:25.640
in a new way" and you can pick that spot and go in and
press the edit button, take you right back to your editor

00:30:25.640 --> 00:30:30.270
and you can start the cycle of optimization again.

00:30:30.269 --> 00:30:34.670
So, now that's the way code browser usually works.

00:30:34.670 --> 00:30:39.529
Now there are some cases you'll try double
clicking and this will come up instead.

00:30:39.529 --> 00:30:45.819
You'll see a bunch of assembly code from your code and
it'll happen if you say double click on a library function,

00:30:45.819 --> 00:30:51.659
any sort of kernel function, or if your end users
try to Shark your code and they double click

00:30:51.660 --> 00:30:57.720
on anything this is what they'll tend to see or you may just
simply, you know, switch over; you can actually bounce back

00:30:57.720 --> 00:31:00.710
and forth and look at both source and assembly if you like

00:31:00.710 --> 00:31:04.460
and even in this view we can offer
a lot of useful information

00:31:04.460 --> 00:31:09.190
We can offer you optimization hints simply by
clicking on these little exclamation point buttons

00:31:09.190 --> 00:31:15.490
which can tell you how you might want to consider optimizing
that code, even on an instruction by instruction level

00:31:15.490 --> 00:31:18.440
or potentially give hints to the compiler
on how I might want to generate code.

00:31:18.440 --> 00:31:23.779
In fact this is really just a good way to check
on the compiler in order to make sure, well,

00:31:23.779 --> 00:31:27.440
first off that you actually did enable an
optimization for that particular function

00:31:27.440 --> 00:31:33.470
because un-optimized code looks a lot different than
optimized code once you've seen it a couple of times

00:31:33.470 --> 00:31:40.200
and also, you know, make sure that a particular way you
wrote your code wasn't a pathological case for the compiler

00:31:40.200 --> 00:31:43.140
that caused it to generate something
that was really ugly and slow just

00:31:43.140 --> 00:31:46.100
because of the way you happen to write your syntax.

00:31:46.099 --> 00:31:55.769
Now of course assembly view is great but I for one don't
bother to memorize every last one of those assembly line

00:31:55.769 --> 00:31:59.589
which is pneumonics and I actually
do use assembly on a regular basis.

00:31:59.589 --> 00:32:05.179
So for most of us what you're going to want
to do is pick an instruction of interest

00:32:05.180 --> 00:32:11.039
and click on this little ASM help button down in the lower
right corner and what will happen is this window will pop up

00:32:11.039 --> 00:32:15.769
and it will have the actual pages from
the architectural reference manual paged

00:32:15.769 --> 00:32:18.099
up to the page describing the instruction you're looking

00:32:18.099 --> 00:32:23.519
at so you can actually tell what that
move SS really means in real life.

00:32:23.519 --> 00:32:32.769
Now I'm going to take a small tangent in here for
a moment to mention one other key feature of Shark is

00:32:32.769 --> 00:32:38.069
that once you have these session windows as we
call them you can actually save them at anytime.

00:32:38.069 --> 00:32:45.220
The most obviously just too simply save all the samples but
optionally we can also embed all the symbols and source code

00:32:45.220 --> 00:32:49.279
from your program into the file
right along with sample information.

00:32:49.279 --> 00:32:52.149
Now this is optional if you're
worried about security or so on

00:32:52.150 --> 00:32:57.360
when you try to do this savings circle will
actually prominent you, "Do you actually really want

00:32:57.359 --> 00:33:02.269
to save the symbols with it" and so you can so no if you're
concerned about someone else seeing your code or symbols

00:33:02.269 --> 00:33:09.319
or what not but if you do put them in you can
then examine them later or if you don't put them

00:33:09.319 --> 00:33:16.189
in you can maybe decide later on that, "Yea, I actually
did want to see my symbols, that person is legitimate,

00:33:16.190 --> 00:33:20.850
they can see the code;" so you can go up to
the file menu choose the symbolic 8 command

00:33:20.849 --> 00:33:24.399
and actually go and add them back in later.

00:33:24.400 --> 00:33:33.240
Shark will take them from your binary and beyond the
basic obvious just saving a session away so you can look

00:33:33.240 --> 00:33:38.019
at it next week this has many other
very useful applications.

00:33:38.019 --> 00:33:43.369
For example you may have a performance problem which
you find in the code written by someone else halfway

00:33:43.369 --> 00:33:49.279
across your company; well you can take the Shark
session, package it up, send it to them in e-mail saying,

00:33:49.279 --> 00:33:57.220
"Hey this is your code, could you tell me why this so slow
and maybe tweak it and optimize it for me" or you can,

00:33:57.220 --> 00:34:02.130
if you have an issue say that you don't time
to do it right now but for the next dot release

00:34:02.130 --> 00:34:05.670
of your product you'll have some time to
deal with it, you can save this as sort

00:34:05.670 --> 00:34:10.430
of a documentation of a future thing you might want to do.

00:34:10.429 --> 00:34:16.039
Another great application is that if your
users go and use Shark on your application

00:34:16.039 --> 00:34:20.670
after they've run their own data set with it they
may have a data set which of course you didn't have

00:34:20.670 --> 00:34:25.940
at the time you were developing the application; they may
use Shark and find a performance problem that you missed

00:34:25.940 --> 00:34:32.030
and they can actually take the Shark sessions, package them
up, send them into you, you can add the symbols afterwards

00:34:32.030 --> 00:34:37.620
and figure out your performance problem using
the data from their code, their data set.

00:34:37.619 --> 00:34:45.019
Also the Shark sessions are great to put in bug reports and
one last note is that these sessions are crossed platform

00:34:45.019 --> 00:34:51.219
so if you have a mixed house with both X86 and power PC Macs
anybody can look at the session made on any of those Mac,

00:34:51.219 --> 00:34:55.929
no problem; they go back and forth seamlessly.

00:34:55.929 --> 00:35:01.289
So in order to illustrate how time profile works
I'm going to go and give you a quick demo here

00:35:01.289 --> 00:35:06.920
on how we can use time profile to optimize
a very simple MPEG decode application.

00:35:06.920 --> 00:35:14.059
In order to make this I just simply went to the internet and
got the reference in MPEG 2 decode off of the internet site.

00:35:14.059 --> 00:35:22.019
So going here to the first demo machine, here's the
simple application, here's Shark's window up here above.

00:35:22.019 --> 00:35:30.989
So what I'm going to do with my little reference
application here is I'm going to start up a little movie

00:35:30.989 --> 00:35:36.250
of a guy pedaling around his lab and you
know, "hey this is going pretty fast."

00:35:36.250 --> 00:35:42.340
One thing I did was unlike most video decoders I just
simply let the processor go as fast as it possibly could;

00:35:42.340 --> 00:35:49.789
it's not stopping at a fixed frame rate and so we're seeing
how fast the processor can actually decode this information

00:35:49.789 --> 00:35:52.539
at full blast and he's going pretty fast.

00:35:52.539 --> 00:35:57.019
You might think, "Oh great, no problem we don't really
need to optimize this" but you need to really think

00:35:57.019 --> 00:36:01.670
about it well 1 this is a pretty small movie,
fits in on a small portion of the screen

00:36:01.670 --> 00:36:08.849
and 2 this is a really high end 3 giga hertz Mac
Pro down here and well a lot of people out there

00:36:08.849 --> 00:36:14.429
in the field are going to have their laptops or older
machines and so on and this is occupying an entire processor

00:36:14.429 --> 00:36:18.000
on this 3 gigahertz machine in order to
get this guy riding around like this.

00:36:18.000 --> 00:36:25.519
So in reality we probably want to optimize this to account
for those other systems or for potentially larger movies.

00:36:25.519 --> 00:36:30.460
So what we're going to want to do is use Shark
on this; so I'll go up here to the Shark window

00:36:30.460 --> 00:36:37.670
and I've choose time profile option, first one on the menu,
and I'm going to choose to look in at just at my process,

00:36:37.670 --> 00:36:43.180
the MPEG 2 decode process which I can
choose from this menu off to the right.

00:36:43.179 --> 00:36:46.559
I've chosen that and press start.

00:36:46.559 --> 00:36:52.940
Shark makes this little beep and you can see
down here at the bottom the icon turn bright red

00:36:52.940 --> 00:36:59.470
when it's sampling and...this is a pretty simply
application...I think I've probably sampled enough

00:36:59.469 --> 00:37:01.429
so I'm going to go ahead and stop it.

00:37:01.429 --> 00:37:06.969
So you can see that bar went up and Shark
in order to avoid impacting the performance

00:37:06.969 --> 00:37:13.799
of your application only does the analysis in actually
sorting out what was executed when after we pressed stop;

00:37:13.800 --> 00:37:18.769
it just simply takes the samples while we're doing the
measurement and all the heavy analysis work is saved

00:37:18.769 --> 00:37:24.599
until afterwards; this is purely to keep your
program, to keep the overhead as low as possible.

00:37:24.599 --> 00:37:36.400
So now we've got this time profile window brought up and we
can go in and zoom in on that and what we can see here is

00:37:36.400 --> 00:37:41.349
that we've got an obvious heavy hitter routine, this
reference IDC routine that came right up to the top.

00:37:41.349 --> 00:37:47.440
So it looks like that's probably where we're going to
want to spend most of our time doing our optimization

00:37:47.440 --> 00:37:53.030
but before I do that what might be good is to
go ahead, Shark put up this little button here

00:37:53.030 --> 00:37:58.090
with an exclamation point on it; it's offering a
hint and this is in a function that's pretty high up;

00:37:58.090 --> 00:38:05.880
it's only the fourth one down taking almost 11% of the
time; so you might want to go ahead and check out this hint

00:38:05.880 --> 00:38:12.510
and basically what Shark is offering here is that this is
the fore function of the math library and well, you know,

00:38:12.510 --> 00:38:17.110
here's a little short bit of code you might
consider in-lining in your program in order

00:38:17.110 --> 00:38:22.510
to avoid calling the floor function so much and all
the overhead associated with making a function call.

00:38:22.510 --> 00:38:28.590
So, hey it's pretty simple and quick and easy for us to
cut and paste a little bit of code in our application so,

00:38:28.590 --> 00:38:31.820
heck, it's not the top function but why not.

00:38:31.820 --> 00:38:39.370
So if we can go back and in our application we've already
done this and I've got several copies of the same movie

00:38:39.369 --> 00:38:47.500
so that we can show them simultaneously here and
showing these two, well you can see that when we put

00:38:47.500 --> 00:38:53.429
in this fast floor, we changed the floor command, we're
getting a little performance boost, you know, 10 or 15%,

00:38:53.429 --> 00:38:58.849
somewhere in that vicinity which is not too impressive
but on the other hand all we had to do is copy

00:38:58.849 --> 00:39:06.139
and paste a little bit of code so for just a few minutes of
work this is pretty good but if we actually want to go back

00:39:06.139 --> 00:39:10.449
and look at that reference site,
(inaudible) I think we can do better.

00:39:10.449 --> 00:39:13.710
So go ahead and close this.

00:39:13.710 --> 00:39:19.440
Going back in and looking at our profile again let's
go in and actually look at the reference IDCT routine.

00:39:19.440 --> 00:39:23.420
I'll go and double click on that, we'll
go to the code browser and look at that;

00:39:23.420 --> 00:39:30.119
so what we're seeing here it this routine is our code
and you can see how the, in the whole function here,

00:39:30.119 --> 00:39:35.369
there's one area over here in the scroll bar
where we're having our various hot lines;

00:39:35.369 --> 00:39:40.250
it's all in this function reference IDCT and
we can go in; it has a couple of hot loops here

00:39:40.250 --> 00:39:45.070
where we're spending our time but it's
all in this very one small function.

00:39:45.070 --> 00:39:51.220
So we went in and spent a day or so looking at this
and you know what is happening is it's taking a bunch

00:39:51.219 --> 00:39:57.359
of short integers in, converting them to floating
point doubles, doing a whole bunch of math

00:39:57.360 --> 00:40:05.970
on it's floating point doubles and then outputting back
to the short integers, the same variable it came in.

00:40:05.969 --> 00:40:12.369
So, you know, there's a whole lot of overhead involved with
converting back and forth between integer and floating point

00:40:12.369 --> 00:40:16.449
and also floating point math is slower than integer math.

00:40:16.449 --> 00:40:22.039
So what if we can convert this to integer math
in order to avoid all this excess overhead?

00:40:22.039 --> 00:40:27.779
So this is the really hot place in the program
where a little bit of overhead goes a long way.

00:40:27.780 --> 00:40:36.920
So what we can do, what we did here is let's go ahead and
we went in and changed this code to integer code instead

00:40:36.920 --> 00:40:41.050
and the code is a little uglier with integers
because integer math is not quite as,

00:40:41.050 --> 00:40:45.800
you have to do a little bit more work with
that, but after we just change this one function

00:40:45.800 --> 00:40:52.390
to use integers all the way you can see that now we're
going a frame rate which is nearly twice as fast.

00:40:52.389 --> 00:41:00.699
He's now boogying around that lab with pretty good
abandon and we can see that just by changing one function

00:41:00.699 --> 00:41:07.399
in the whole program and this is code I didn't write
so the rest of it I don't even know what it does.

00:41:07.400 --> 00:41:09.809
I didn't pay any attention to it.

00:41:09.809 --> 00:41:14.779
You know, that's pretty good for going in
and tweaking one function but if we go in

00:41:14.780 --> 00:41:22.540
and look at this a little more...you
can see that in...let's see,

00:41:22.539 --> 00:41:33.759
if we go in and look at the code browser here is the actual
assembly code and...on this particular Mac it's not wanting

00:41:33.760 --> 00:41:41.540
to show it to me; back at the office when I did this right
up here on this function it had a hint here and so I clicked

00:41:41.539 --> 00:41:47.880
on it and it said, "Hey you might want to consider
using SIMD functions in this particular function;

00:41:47.880 --> 00:41:50.860
so it's doing a lot of very regular mathematics.

00:41:50.860 --> 00:41:55.769
So I went ahead and took that hint and say, "Well this
does look like a good opportunity for SIMD instructions,"

00:41:55.769 --> 00:42:01.559
they're kind of designed for video coding work and
I said, "Well sure, let's go ahead and put those in"

00:42:01.559 --> 00:42:07.980
and so let's make a third copy of him
with this converted to SIMD instructions.

00:42:07.980 --> 00:42:18.889
So now the third version you can see here, well he's
going a little bit faster but it's really not that much.

00:42:18.889 --> 00:42:26.849
It's maybe another 10% or something which considering
all the what we heard about the SIMD instructions it's

00:42:26.849 --> 00:42:31.690
like they're doing like 4 rate operations at
once, sometimes even 16 operations at once,

00:42:31.690 --> 00:42:36.200
shouldn't we be seeing like a 4 rate
or 16X improvement not just 10%.

00:42:36.199 --> 00:42:39.199
This is where our intuition is starting to fail us again.

00:42:39.199 --> 00:42:43.559
So this is where intuition starts to fail us,
it's a good time to go back and use Shark.

00:42:43.559 --> 00:42:49.889
So we can go back and do another Shark profile...yea,

00:42:49.889 --> 00:42:59.819
that's probably enough...Shark will analyze our
samples for us now and toss up a new window.

00:42:59.820 --> 00:43:12.870
So, I go in and zoom in on this...well the first problem
I'm seeing is that I actually left all those windows open

00:43:12.869 --> 00:43:21.259
so when I sampled my decoder I actually saw everything
and I was leaving those up there for the demo.

00:43:21.260 --> 00:43:27.250
Actually, no, Shark has a solution to this; down here at the
bottom you'll see we actually have these menus so as well

00:43:27.250 --> 00:43:31.989
as focusing in before we start we can
actually do some post run focusing.

00:43:31.989 --> 00:43:36.829
So I can actually choose both the process
and in this case a thread of interest.

00:43:36.829 --> 00:43:41.539
So we can see we have our main thread which is doing
the GUI application and we have a thread for each

00:43:41.539 --> 00:43:44.949
of the different decoders, each of
which is getting a balanced amount

00:43:44.949 --> 00:43:47.079
of time, about a third of the execution time.

00:43:47.079 --> 00:43:55.579
So I can go in and look at these and that is the
integer version...this is the vectorized version

00:43:55.579 --> 00:44:00.809
and here is the...oh yea here is our original one
and you can see that the original one has a breakdown

00:44:00.809 --> 00:44:08.159
if we just look at the thread, almost exactly identical
to what we saw before with the original analysis

00:44:08.159 --> 00:44:11.009
with the reference ICD routine running up to the top.

00:44:11.010 --> 00:44:14.510
Let's go back and look at that vectorized one.

00:44:14.510 --> 00:44:19.960
So if we go back and look at just this vectorized
one the problem that immediately rears it's ugly head;

00:44:19.960 --> 00:44:27.840
here is the IDCT down here; it's number 6 in line
now; so basically we were playing whack a mole again;

00:44:27.840 --> 00:44:34.070
we'd beaten down this operation a whole
bunch; it's now nice and short and efficient.

00:44:34.070 --> 00:44:39.710
So instead what's the problem is these other routines
which popped up and are now the new heavy hitters

00:44:39.710 --> 00:44:41.730
in our program since we optimized the IDCT.

00:44:41.730 --> 00:44:45.590
So in order to actually optimize our
program and get it to go faster we're going

00:44:45.590 --> 00:44:49.090
to have to go in and optimize these instead.

00:44:49.090 --> 00:44:55.740
So, I will skip forward through all the different potential
steps there of actually going and optimizing those

00:44:55.739 --> 00:45:01.649
and we went and most of all these were also
amendable to vectorization just like the IDCT was.

00:45:01.650 --> 00:45:09.920
So, I went in and applied vectorization to pretty much all
these heavy hitter routines and after doing all those

00:45:09.920 --> 00:45:20.680
about 4 I believe, went in and got the motion compensation,
color space conversion, pixel interpolation and so on,

00:45:20.679 --> 00:45:31.049
all these vectors; now we can see that with just these
top 4 or 5 routines optimized he is now really boogying

00:45:31.050 --> 00:45:38.690
around that lab; I don't know, someone clearly lit a
fire under him and you can see just with by going in

00:45:38.690 --> 00:45:44.179
and using Shark to find and isolate and give us
hints on how to optimize just a handful of functions

00:45:44.179 --> 00:45:50.659
in this actually fairly large program, most of which consist
of code which is executed at start up and never again;

00:45:50.659 --> 00:45:57.569
we were able to go in and just with one engineer
for a few days go and optimize this program

00:45:57.570 --> 00:46:09.340
so it's nearly a 6X performance boost and if we could switch
back to the slides; you can see here that with our baseline

00:46:09.340 --> 00:46:15.680
of 1 time the fast floor operation just
in a few minutes we got an additional 15%.

00:46:15.679 --> 00:46:23.629
When we switched to the integer IDCT with about a days
work or so we were able to get nearly 2X performance boost;

00:46:23.630 --> 00:46:28.530
vector IDCT added a little bit but as it turns out
that we had already gotten most of the benefit just

00:46:28.530 --> 00:46:33.420
by converting integer and then back taking that and
going to just a few more functions, about 4 or so,

00:46:33.420 --> 00:46:38.800
we were able to get almost 6X performance
improvement just with a few days, one person;

00:46:38.800 --> 00:46:43.590
thanks to the fact that Shark was able
to point us right in at the issues.

00:46:43.590 --> 00:46:47.519
So that's how to use Shark to look
at your CPU bound problems.

00:46:47.519 --> 00:46:54.199
Next we're going to go in and look at how you can use it to
go in and look at system and threading interaction in order

00:46:54.199 --> 00:46:56.960
to speed up how you're dealing with Mac OS X

00:46:56.960 --> 00:47:01.349
or your multi threaded applications using
a tool we call system trace analysis.

00:47:01.349 --> 00:47:04.659
For this we're going to bring up Mr. Ryan Du Bois.

00:47:04.659 --> 00:47:12.019
( Applause )

00:47:12.019 --> 00:47:16.659
>>Thank you Lance; as you said my name is Ryan
Dubois and I'm here to talk about system trace.

00:47:16.659 --> 00:47:20.500
Let me get out of your way here.

00:47:20.500 --> 00:47:24.639
So you guys just saw time profile, you saw all
about it, how it helps you boost the performance

00:47:24.639 --> 00:47:27.500
of your CPU bound jobs; that's all well and good.

00:47:27.500 --> 00:47:30.590
What if your application is more just I/O intensive

00:47:30.590 --> 00:47:35.150
or maybe you just added multi threading
after you optimized the CPU work?

00:47:35.150 --> 00:47:39.800
How about if you want to understand the virtual memory
behavior of your application; what do you use then?

00:47:39.800 --> 00:47:45.740
You could probably use time profile but it might not
be as helpful as system trace; let's find out why.

00:47:47.139 --> 00:47:50.599
So time profile as Lance showed
you interrupts your application

00:47:50.599 --> 00:47:54.110
at a predetermined interval; it figures out what it's doing.

00:47:54.110 --> 00:47:57.430
Let's say for example here this
blue block of code on the left there

00:47:57.429 --> 00:48:01.139
of execution what if that was VM call or system call.

00:48:01.139 --> 00:48:03.839
You'll notice that time profile completely missed it.

00:48:03.840 --> 00:48:09.269
To this end we've created system trace which is
an exact trace of operating system entry and exit;

00:48:09.269 --> 00:48:13.079
this will catch everything like
system calls, VM faults, interrupts,

00:48:13.079 --> 00:48:15.849
and it'll catch all the threads running on your system.

00:48:15.849 --> 00:48:19.969
The really beautiful part about this is you don't have
to add any instrumentation to your application just

00:48:19.969 --> 00:48:27.489
like with time profile but if you did want to you could add
sign posts and we'll talk a little bit about those later.

00:48:27.489 --> 00:48:30.750
When you first take a system trace you have 3 views.

00:48:30.750 --> 00:48:36.599
The first view here is a summary view, there's
also the trace view and the timeline view.

00:48:36.599 --> 00:48:39.019
Let's dive into each one of these.

00:48:39.019 --> 00:48:40.820
You look at the summary view, the top right there,

00:48:40.820 --> 00:48:43.789
it gives you kind of an overall
breakdown; how many CPUs are on the system?

00:48:43.789 --> 00:48:45.509
How much total CPU time?

00:48:45.510 --> 00:48:49.040
Was it user, was it system, was it busy, was it idle?

00:48:49.039 --> 00:48:54.750
To go even further the pie chart in the upper left breaks
it down even more to user code, system calls, VM faults,

00:48:54.750 --> 00:48:59.150
interrupts and idle time and down below
that you'll notice there's 3 more tabs.

00:48:59.150 --> 00:49:01.579
These are more detailed summaries.

00:49:01.579 --> 00:49:03.289
First one here is the scheduler summary.

00:49:03.289 --> 00:49:08.500
This is a basic overall statistical
summary of the thread scheduling behavior.

00:49:08.500 --> 00:49:11.710
Down on the bottom right you kind pick the
different metrics you're interested in;

00:49:11.710 --> 00:49:14.820
there's busy time, user time, system time, and priority.

00:49:14.820 --> 00:49:21.490
This will give you kind of a min/average/max breakdown of
how much time your threads are getting and in what metric.

00:49:21.489 --> 00:49:27.429
The next tab over is the system call; you'll notice this
looks a lot like a time profile and in fact it kind of is.

00:49:27.429 --> 00:49:29.649
It's a profile of your system calls.

00:49:29.650 --> 00:49:35.500
Down at the bottom just like with the time profile
you can switch the view between tree and heavy

00:49:35.500 --> 00:49:41.690
and opening the disclosure triangles allows you to tie the
system call time directly to your user space application.

00:49:41.690 --> 00:49:50.039
The same thing for VM faults over here;
again open the disclosure triangles,

00:49:50.039 --> 00:49:54.650
tie your VM fault time directly to your user space code.

00:49:54.650 --> 00:50:00.700
The next main view that we talked about was the trace
view; this is the complete view of traced events.

00:50:00.699 --> 00:50:03.730
It allows you to inspect a call
stack, the arguments and return values

00:50:03.730 --> 00:50:09.320
of every system call that's made during the system trace and
similarly for VM faults allows you to inspect the address

00:50:09.320 --> 00:50:12.260
and the size and the call stack where they happened.

00:50:12.260 --> 00:50:15.340
If you were to double click on any one of
these events it will take you to that event

00:50:15.340 --> 00:50:18.030
in the timeline view; so it looks like this.

00:50:18.030 --> 00:50:21.769
Timeline view and the most obvious
part of it is the timeline;

00:50:21.769 --> 00:50:25.619
basically every row there corresponds
to one thread of execution.

00:50:25.619 --> 00:50:30.389
Total colored bars are time that that
thread was actually executing on a CPU;

00:50:30.389 --> 00:50:36.369
within these colored bars we'll draw icons
for system calls, the M faults, interrupts.

00:50:36.369 --> 00:50:41.409
Speaking of system calls if you were to click on one like
this one you'd get an inspector that looks like this, list,

00:50:41.409 --> 00:50:44.920
the system call name, the call stack, how you got there.

00:50:44.920 --> 00:50:49.820
The first 4 or 5 arguments and the
return value and in the lower left

00:50:49.820 --> 00:50:53.789
of that inspector you'll see how much
time was actually executed on the CPU

00:50:53.789 --> 00:50:58.110
and how much time it was waiting, possibly on resources.

00:50:58.110 --> 00:51:04.289
You can zoom in on this using the top slider there
just like you can with the chart view in time profile

00:51:04.289 --> 00:51:07.659
and the bottom scroll bar behaves just like
you would expect, just scroll through time.

00:51:07.659 --> 00:51:13.629
You can filter this display just like with
time profile based on the process, the thread,

00:51:13.630 --> 00:51:18.950
the CPU and you can even turn on and
off the different classes of icons.

00:51:18.949 --> 00:51:22.969
In case you're not interested in
system calls you can turn them off.

00:51:22.969 --> 00:51:27.959
Now we offer 3 different ways to color
this view that allows a more kind

00:51:27.960 --> 00:51:32.909
of instantly visually interpretable way to look at this.

00:51:32.909 --> 00:51:39.219
The first one seen here is colored by CPU so each on of
the colored rectangles is colored according to the CPU

00:51:39.219 --> 00:51:44.539
on which it was executing and what you can do with
this is at a glance identify your work or threads

00:51:44.539 --> 00:51:47.579
and make sure they're showing the processes correctly.

00:51:47.579 --> 00:51:49.829
So you'll notice here these top 4 threads are NobleApe.

00:51:49.829 --> 00:51:54.349
The processor called NobleApe and just
by looking over here in this section

00:51:54.349 --> 00:51:57.049
and seeing the 4 different colors we can verify visually

00:51:57.050 --> 00:52:02.590
that all 4 worker threads are showing
the CPUs as we would expect.

00:52:02.590 --> 00:52:06.530
Now if you look a little bit further to the right
you notice they're not sharing as well anymore;

00:52:06.530 --> 00:52:10.570
2 of them are kind of arguing over which
CPU they want and just by looking down

00:52:10.570 --> 00:52:16.150
and finding the color that's missing we can see that
one of the hour glass threads has stolen that CPU.

00:52:16.150 --> 00:52:18.960
The other way that you can color this is by priority.

00:52:18.960 --> 00:52:22.940
This is that same system trace; we
just switched the coloring to priority.

00:52:22.940 --> 00:52:28.289
The red down here are for the iTunes and
NobleApe threads, denotes high priority.

00:52:28.289 --> 00:52:35.550
For these hourglass threads the blue denotes low priority
and finally you can color by the context which reason

00:52:35.550 --> 00:52:39.830
so this allows you to kind of visually
interpret why these threads switched.

00:52:39.829 --> 00:52:44.279
Let's go break down this timeline view even more.

00:52:44.280 --> 00:52:46.870
Like a said, these colored rectangles,
these are run intervals.

00:52:46.869 --> 00:52:50.650
The little gray lines on the ends of
the run interval represent a context

00:52:50.650 --> 00:52:54.360
which to a different thread on the same CPU.

00:52:54.360 --> 00:52:59.430
Within these run intervals we draw the icons like
I said; here you can see a normal system call;

00:52:59.429 --> 00:53:03.579
a little black underline represents how
much time that system call actually took.

00:53:03.579 --> 00:53:07.799
If you look up above you can see a system
call that causes a thread to block;

00:53:07.800 --> 00:53:13.840
again that black underline represents how much time
that system call took and similarly we'll draw icons

00:53:13.840 --> 00:53:19.269
for page faults; again black underlines
how much time that page fault took.

00:53:19.269 --> 00:53:24.710
If you were to click on one of these run intervals you'd
see an inspector much like this with list of thread,

00:53:24.710 --> 00:53:29.980
the process to which it belonged, how much time
was user time, how much time was system time

00:53:29.980 --> 00:53:32.670
and importantly the context which reasons.

00:53:32.670 --> 00:53:36.769
So you can click on each one of
these and inspect it individually.

00:53:36.769 --> 00:53:43.050
Now we'll add icons to this; like I said, you saw
some system calls but what do they really mean.

00:53:43.050 --> 00:53:46.580
Well there's 4 system call icons
that you'll see in a timeline

00:53:46.579 --> 00:53:53.819
The red phone denotes BSD system calls; these are you
POSIX APIs, things that allow you to talk to devices

00:53:53.820 --> 00:53:55.860
and an abstract way, that sort of a thing.

00:53:55.860 --> 00:54:02.829
The blue phone are mock system calls basic
services like ports, locks, tasks and threads.

00:54:02.829 --> 00:54:06.819
Speaking of locks a subset of these
will be represented with the lock icon;

00:54:06.820 --> 00:54:12.370
this will happen for any pthread mutex
lock call that ends up in a trap to the kernel.

00:54:12.369 --> 00:54:19.739
So you can visually at a glance identify lock intention in
your system and the green phone here represent MIG messages;

00:54:19.739 --> 00:54:23.949
these are mock interface generator
routines more low level stuff.

00:54:23.949 --> 00:54:27.509
We'll also draw events for virtual memory.

00:54:27.510 --> 00:54:31.110
For those of you not familiar with virtual
memory it takes your physical memory divides it

00:54:31.110 --> 00:54:34.289
into small 4 kilobit chunks called pages and moves these in

00:54:34.289 --> 00:54:37.710
and out of physical memory on demand
in a process called faulting.

00:54:37.710 --> 00:54:40.250
There are 5 types of faults on OS 10.

00:54:40.250 --> 00:54:46.019
The page in, the page is brought from the disk into
physical memory, that's represented by a green up arrow.

00:54:46.019 --> 00:54:52.349
Page out with a red down arrow, pages taken from physical
memory pushed out to disk because it's no longer used.

00:54:52.349 --> 00:54:56.549
Zero fill has a little 0 there, just a
new page you got marked 0 fill on demand;

00:54:56.550 --> 00:54:59.170
the first time you touch it, it fills it with 0.

00:54:59.170 --> 00:55:10.240
Copy on right fault with the clever cow icon...these
are shared pages so say you do a process fork,

00:55:10.239 --> 00:55:13.539
all your pages are marked copy on right
so when the new process goes to write

00:55:13.539 --> 00:55:19.539
onto those pages it gets a whole new copy and
finally the check mark is the page cache hit.

00:55:19.539 --> 00:55:23.929
This means that the page is actually resident in physical
memory but it's just not mapped into your process.

00:55:23.929 --> 00:55:32.539
We'll also display icons for hardware interrupts from
anything of from I/O operations, DNA operations,

00:55:32.539 --> 00:55:38.519
even the clock interrupts that enforce the time
quantum and as I said before you can add signposts

00:55:38.519 --> 00:55:40.920
to your system trace; it would show up like this.

00:55:40.920 --> 00:55:46.450
There are 2 main types of signposts; the first type is a
point signpost; you can think of this like a tracer bullet

00:55:46.449 --> 00:55:50.419
or a print F, you application kind of raises
its hand and says, "Hey this is what I'm doing."

00:55:50.420 --> 00:55:54.260
The other type is an interval signpost;
these have a beginning and an end record;

00:55:54.260 --> 00:55:59.110
they'll be represented with the little black under bar
just like the system calls them in VM faults you just saw.

00:55:59.110 --> 00:56:05.110
With each one of these events you can record auxiliary data
values, anything you want and you can add these directly

00:56:05.110 --> 00:56:09.140
to your code both user land and
driver code anywhere you want.

00:56:09.139 --> 00:56:14.379
You can come find us after the talk in the labs and
check out the Shark manual to find out how to do that.

00:56:14.380 --> 00:56:22.720
So now I'd like to expand upon Lance's demo where we've
already optimized an application using time profile

00:56:22.719 --> 00:56:27.339
and we added some multi threading as kind of
a first pass; let's take it one step further

00:56:27.340 --> 00:56:33.320
and let's see what system trace tells us, see if
we can squeeze out a little bit more performance.

00:56:34.659 --> 00:56:41.869
So we're going to be working with an application
today called NobleApe; it looks like this.

00:56:41.869 --> 00:56:49.819
It's just a basic simulation, each one of the little
red dots represents an ape running around on an island.

00:56:49.820 --> 00:56:55.300
Up right here you have the little simulation
of the ape brain; you'll notice it keeps track

00:56:55.300 --> 00:56:58.380
of ape thoughts per second; that's
are metric we're going to focus on.

00:56:58.380 --> 00:57:03.300
So when we originally looked at this code it was a
completely single threaded scale in implementation,

00:57:03.300 --> 00:57:09.380
some of you may be familiar with this
already, gets about 2008 thoughts per second.

00:57:09.380 --> 00:57:15.410
So in the past we've gone through and we've optimized it
with vector code using time profile just as Lance showed you

00:57:15.409 --> 00:57:22.529
and that gets us up to around 12,008 thoughts per second and
we even went one step further and these are 4 core machines

00:57:22.530 --> 00:57:24.960
up here so the other 3 right now are just kind of idle.

00:57:24.960 --> 00:57:31.050
So adding multi threading that gets us up around
the neighborhood of 25,000 ape thoughts per second;

00:57:31.050 --> 00:57:36.010
that's pretty good coming from 2,000 but I have a suspicion
that we might be able to take it a little bit further.

00:57:36.010 --> 00:57:42.950
So what we're going to do now is just leave this
running; we're going to take a system trace with Shark;

00:57:42.949 --> 00:57:51.309
again all I do is poke the start button, let
it go for a little bit, poke the stop button.

00:57:51.309 --> 00:57:53.549
We'll go ahead and pause NobleApe here.

00:57:53.550 --> 00:57:56.789
So this system trace comes up; it's got
the same things I just walked you through.

00:57:56.789 --> 00:58:00.489
It has the basic summary up here in the
right; on the left you have the pie chart.

00:58:00.489 --> 00:58:05.750
You notice most of the time is in the user code; this
is expected and you know we want it to be calculating

00:58:05.750 --> 00:58:11.659
and not doing a bunch of blocking stuff but you also notice
the second major chunk there, it's just sitting idle;

00:58:11.659 --> 00:58:13.789
that's the time we're going to and break into.

00:58:13.789 --> 00:58:16.159
We're going to try and get some more idle time.

00:58:16.159 --> 00:58:18.579
Let's go look at the timeline.

00:58:18.579 --> 00:58:26.099
Here's the timeline; the window menu show advanced
settings allows you to play with the coloring

00:58:26.099 --> 00:58:29.360
so you can see the different coloring schemes here.

00:58:29.360 --> 00:58:32.579
You can zoom in just by clicking and dragging.

00:58:32.579 --> 00:58:35.360
So we're going to go color by CPU.

00:58:35.360 --> 00:58:41.410
Now these are the worker threads here, these top 4
and we can look like I said at a glance right here

00:58:41.409 --> 00:58:45.899
and just determine that they're sharing all
4 CPUs as we would expect; so that's good;

00:58:45.900 --> 00:58:48.940
that's not the problem; they're not blocking on each other.

00:58:48.940 --> 00:58:51.670
Let's go back to the summary view here.

00:58:51.670 --> 00:58:54.150
Let's focus in on NobleApe.

00:58:54.150 --> 00:58:58.119
You'll notice down on the right
here we've selected busy time.

00:58:59.630 --> 00:59:05.510
These 4 here, these are the worker threads that are
doing the calculation and upon first inspection one thing

00:59:05.510 --> 00:59:10.980
that really jumps out to me is the average amount of busy
time that each one of these gets is less than a millisecond.

00:59:10.980 --> 00:59:16.730
Now the time slicing quantum on OS X is 10
milliseconds so these are CPU bound threads;

00:59:16.730 --> 00:59:20.789
they are using less than one-tenth of
the amount time they've been given.

00:59:20.789 --> 00:59:24.849
So if we can break into that, get some
more busy time on average for each one

00:59:24.849 --> 00:59:31.029
of these threads we could probably
get some more ape thoughts per second.

00:59:31.030 --> 00:59:36.019
So we're going to use that, the average
busy time as one of our metrics.

00:59:36.019 --> 00:59:38.349
Let's go figure out why they're blocking.

00:59:38.349 --> 00:59:43.029
We're going to go to system calls
here; we're going to open this,

00:59:43.030 --> 00:59:48.920
this little icon in the bottom right allows
us to see the heaviest call stack there.

00:59:51.630 --> 00:59:57.780
I'm a little bit familiar with this code because I've looked
at it now but once you get down to NobleApe you'll see,

00:59:57.780 --> 01:00:00.540
you'll get to something called NPtaskWrapper.

01:00:00.539 --> 01:00:07.599
So I went in, I found that code over in NobleApe
and this was the original implementation;

01:00:07.599 --> 01:00:13.170
kind of like what you'd expect basic worker thread
model, you know, wait on a request queue do some stuff

01:00:13.170 --> 01:00:16.889
and post to a results queue, makes sense.

01:00:16.889 --> 01:00:21.489
Inspect it a little bit further, walk through the
code a little bit and this variable here for each one

01:00:21.489 --> 01:00:28.369
of these worker threads they're 8 troops globally and there
are 4 worker threads and we just divide the work up evenly.

01:00:28.369 --> 01:00:30.940
So each one of these worker threads in only cycling

01:00:30.940 --> 01:00:35.539
through 2 troops before it's posting
to the queue and going to sleep again.

01:00:35.539 --> 01:00:40.400
That's not a lot of work for these guys to do and obviously
as you just saw it's not filling up the time quan.

01:00:40.400 --> 01:00:47.260
So this is the first pass, all I simply
did was just go ahead and double the amount

01:00:47.260 --> 01:00:49.760
of apes on the island and see what that does.

01:00:49.760 --> 01:00:58.390
Get it back in to vector threaded
mode...that's a little bit higher now;

01:00:58.389 --> 01:01:02.549
it's breaking into the 28s, hanging around 26 mostly.

01:01:02.550 --> 01:01:05.130
We got a little bit of a boost there.

01:01:05.130 --> 01:01:06.930
Let's go ahead and take another system trace.

01:01:06.929 --> 01:01:21.440
( Pause )
Let Shark do its thing; again we're going to focus on
NobleApe, open up these and look at the worker threads.

01:01:21.440 --> 01:01:26.760
Look at that; we took it from the around 700 or
800 microseconds on average up to 1.3 milliseconds;

01:01:26.760 --> 01:01:28.510
so we're breaking into it; this is a good start.

01:01:28.510 --> 01:01:34.750
A couple of other things to try are if
you remember this loop here we can go

01:01:34.750 --> 01:01:37.289
through that loop twice before we post the results;

01:01:37.289 --> 01:01:40.829
you're just calculating thoughts and
changing some parameters on the ape.

01:01:40.829 --> 01:01:47.000
So let's go ahead and do that; let's walk through
this loop a couple of times over our amount of apes

01:01:47.000 --> 01:01:50.389
that we've been given and then post the results after that.

01:01:50.389 --> 01:01:57.679
So I went ahead and did that; went back to the original
amount of apes, doubled the amount of times we go

01:01:57.679 --> 01:02:01.549
through that loop; let's see what that
does to our ape thoughts per second.

01:02:01.550 --> 01:02:01.740
( Pause )

01:02:01.739 --> 01:02:12.619
It looks like it's getting a little
bit higher there, hovering around 28,

01:02:12.619 --> 01:02:18.449
29 breaking up into the 30s; that's not bad.

01:02:18.449 --> 01:02:20.269
Similarly we do another system trace.

01:02:20.269 --> 01:02:21.230
( Pause )

01:02:21.230 --> 01:02:29.130
Let it process the samples and let's
go look at that other metric again.

01:02:29.130 --> 01:02:38.200
Still about the same; okay so that's...that's not too
bad but we could even break into this even further.

01:02:38.199 --> 01:02:44.079
So I went through this cycle a bit more, found out that
I could double the number of apes and double the number

01:02:44.079 --> 01:02:51.719
of cycles and do them simultaneously and get a big
boost but I just went ahead and I went crazy with it.

01:02:51.719 --> 01:02:55.179
I said, "You know, why don't we
just do 8 of these synchronization,

01:02:55.179 --> 01:02:58.909
8 of these thought cycles per synchronization
and let's double the number of apes

01:02:58.909 --> 01:03:02.559
and let's see how many ape thoughts per second we can get.

01:03:02.559 --> 01:03:04.949
( Pause )

01:03:04.949 --> 01:03:18.210
So we went ahead and did that, let this computer average for
a while...there we go...and we've broken into the 40s now;
@

01:03:18.210 --> 01:03:23.809
we're hovering around forty two, 41,000
ape thoughts per second up from about 25.
%

01:03:23.809 --> 01:03:30.199
Let's go ahead and take one final system trace
and let's see what our other metric is doing;

01:03:30.199 --> 01:03:34.329
let's see how those run intervals are going.

01:03:34.329 --> 01:03:40.579
Focus in on NobleApe, go check the worker
threads, look at that; on average we're up in 5

01:03:40.579 --> 01:03:43.500
to 6 millisecond range, up from 600 microseconds.

01:03:43.500 --> 01:03:50.320
So here what you've seen is how to build upon what you've
done with time profile in your multi threaded applications.

01:03:50.320 --> 01:03:55.910
You've gone in and you've optimized all your
CPU work loops to make them as fast as possible.

01:03:55.909 --> 01:03:58.529
System trace allows you to do that for your multi threading.

01:03:58.530 --> 01:04:01.630
It allows you to identify where the overhead is,

01:04:01.630 --> 01:04:05.980
see how much time you worker threads are
getting on average and increase that.

01:04:08.809 --> 01:04:15.329
Let's go ahead and go back to the slides.

01:04:15.329 --> 01:04:21.900
( Pause )
So in review when I did this Mac at work these
are the results that I got; so what do we do?

01:04:21.900 --> 01:04:27.460
Well first we ran NobleApe, we identified our one
metric we wanted to improve, the ape thoughts per second.

01:04:27.460 --> 01:04:33.920
We went in with system trace, we saw what it was doing, we
identified the short run interval for the worker threads.

01:04:33.920 --> 01:04:39.809
From there we used the system called summary view
there to identify key point in our code to rethink,

01:04:39.809 --> 01:04:45.799
attempted a few really simple, really quick
optimizations; we got around a 1.7X speed up in

01:04:45.800 --> 01:04:50.960
about 15 minutes of actual work; that's not bad.

01:04:50.960 --> 01:04:55.550
So now you've seen time profile
for optimizing your CPU bound jobs.

01:04:55.550 --> 01:04:59.539
You've seen system trace for optimizing
your multi threaded and system interaction.

01:04:59.539 --> 01:05:04.889
These are just 2 of the many options that Shark allows
you to do for profiling your applications and here to talk

01:05:04.889 --> 01:05:08.089
about some of the other options for
profiling with Shark is Rick Altherr.

01:05:08.090 --> 01:05:09.329
( Applause )

01:05:09.329 --> 01:05:15.519
>>Thanks Ryan.

01:05:15.519 --> 01:05:22.989
So like Ryan said we've talked about system trace, we've
talked about time profile, let's talk about a few others.

01:05:22.989 --> 01:05:25.699
So what's a common theme in applications today?

01:05:25.699 --> 01:05:30.489
Well they start using a lot of memory; this can
have some pretty adverse affects; you can, you know,

01:05:30.489 --> 01:05:35.939
run out of memory, you can be using enough memory
that the operating system has a hard time managing it.

01:05:35.940 --> 01:05:41.050
Well how can we take a look and
see what's going on memory wise?

01:05:41.050 --> 01:05:43.760
Well for Shark we have Malloc tracing.

01:05:43.760 --> 01:05:48.410
So we can actually trace all the allocation
events that happen, this is both allocations

01:05:48.409 --> 01:05:54.019
and frees so we trace both C, C++,
new Malloc free,

01:05:54.019 --> 01:05:58.989
delete and we actually create an
exact trace of all these events.

01:05:58.989 --> 01:06:01.969
This way we can look at a variety of statistics.

01:06:01.969 --> 01:06:07.219
Now what you get when you collect one of
these is what looks like a time profile.

01:06:07.219 --> 01:06:13.779
In fact we even sort it by the same type of statistics; the
number of allocation events that happen in each function.

01:06:13.780 --> 01:06:21.170
You can also go to the source level and look at allocations
per source line but we also added an extra column

01:06:21.170 --> 01:06:25.010
and this tells you the actual size of the allocations.

01:06:25.010 --> 01:06:29.790
Now this is a summation of all of the
allocations that happened in this function.

01:06:29.789 --> 01:06:36.329
This is really useful to find runaway functions where
you're allocating things in a loop and you can say, "Well,

01:06:36.329 --> 01:06:42.750
you know, I allocated 50 megs in one function; perhaps
I should go through and think about that for a second."

01:06:42.750 --> 01:06:50.349
You can also go to the chart view and look at this in
more of a behavioral sense; instead of looking at "well

01:06:50.349 --> 01:06:55.139
where am I doing lots of allocations" you can
look at what's the overall behavior of my program.

01:06:55.139 --> 01:07:00.339
So you get the normal chart view that time profile
gives you but you also get a secondary view

01:07:00.340 --> 01:07:03.420
that illustrates the change in heap size.

01:07:03.420 --> 01:07:11.010
So bars going above the zero line are allocations and
lines going below the zero line are actually frees.

01:07:11.010 --> 01:07:16.190
So you can look and at a glance see the pattern
of my application is allocate, allocate, allocate,

01:07:16.190 --> 01:07:26.019
free and you can also see I allocate a whole bunch and
I freed very little which indicates that you have leaks.

01:07:26.019 --> 01:07:33.429
Now another area that has become more interesting
over time is Java and virtual machines.

01:07:33.429 --> 01:07:39.029
Traditional profilers like time profile
and system trace look at compiled code.

01:07:39.030 --> 01:07:44.430
They look at what is the native machine actually
executing; this is great for lots of things;

01:07:44.429 --> 01:07:48.679
in fact it's great for looking at the
virtual machine itself but what if you want

01:07:48.679 --> 01:07:51.519
to look at what the virtual machine is running?

01:07:51.519 --> 01:07:59.699
Well with Shark we've actually implemented hooks into the VM
so that we can actually lift you out of the native machine

01:07:59.699 --> 01:08:05.149
into the virtual machine and see
this is what my code is doing.

01:08:05.150 --> 01:08:08.530
So what do we offer in terms of Java support?

01:08:08.530 --> 01:08:14.420
Well for time profile looks exactly like an old
time profile, has the same features, same display,

01:08:14.420 --> 01:08:22.130
same data mining capabilities; it just happens to show you,
"Hey this is the Java class that I'm actually working with,

01:08:22.130 --> 01:08:25.020
this is the method I'm in, this is the package it's in."

01:08:25.020 --> 01:08:30.790
This also will take you directly to the source code;
you get all the exact same features just for Java.

01:08:30.789 --> 01:08:36.939
Well for another type that we haven't talked about
before but this is what we call a call trace.

01:08:36.939 --> 01:08:40.359
Now this is an exact trace of every method, entry

01:08:40.359 --> 01:08:43.869
and exit that happens while you
actually take a trace of your program.

01:08:43.869 --> 01:08:48.739
Now this is quite invasive and will make your application
considerably slower while you do it but if you really want

01:08:48.739 --> 01:08:51.109
to know what your application does this will tell you.

01:08:51.109 --> 01:08:57.259
It shows it in the exact same format; it's a time
profile style format and you can look and say, "Well,

01:08:57.260 --> 01:09:03.210
I make lots of calls as function, I spend lots of
time in these functions and I know it exactly."

01:09:03.210 --> 01:09:06.670
We also offer allocation tracing.

01:09:06.670 --> 01:09:11.210
We watch for you memory allocations
by allocating objects in Java.

01:09:11.210 --> 01:09:16.050
We also catch garbage collection events to catch
the frees and we display it in the exact same way

01:09:16.050 --> 01:09:18.630
that you're used to with the Malloc trace.

01:09:18.630 --> 01:09:24.750
We add the allocation size so you can even see
I'm allocating lots in this particular method.

01:09:24.750 --> 01:09:32.449
We also offer even more techniques than what I just talked
about and we're going to cover these really briefly.

01:09:32.449 --> 01:09:36.220
Sometimes you want to deal with early testing of binaries.

01:09:36.220 --> 01:09:41.090
You know, perhaps you really haven't debugged your
application completely; it's really hard to get into a case

01:09:41.090 --> 01:09:46.930
where you actually exhibit a performance
issue but you want to take a look anyway.

01:09:46.930 --> 01:09:52.880
Well you can point Shark right at your binary on a disk,
not even run it and Shark can come back and give you a lot

01:09:52.880 --> 01:10:00.029
of those hints about what you might be able to do
such as vectorizing your code or things like that

01:10:00.029 --> 01:10:06.539
and you can do this without even running
the application; does it all statically.

01:10:09.199 --> 01:10:18.800
Now in today's world you have lots of threads for doing lots
of different things; sometimes you think of doing everything

01:10:18.800 --> 01:10:23.420
in parallel and sometimes they're
not, like you saw in system trace.

01:10:23.420 --> 01:10:29.500
Well sometimes you're interested in what they're
not doing; specifically what they're blocked on

01:10:29.500 --> 01:10:34.710
and system trace can be very useful for this but
another way of looking at this is saying, "Well okay,

01:10:34.710 --> 01:10:41.340
it took 10 seconds on my watch and of that 10 seconds
how long was I blocked and where was I blocked?"

01:10:41.340 --> 01:10:46.350
Well for profile called time profile all threads
states it's just like a normal time profile except

01:10:46.350 --> 01:10:53.430
that we record not only execution time, time where you
were actually executing on the CPU but we also take time

01:10:53.430 --> 01:10:59.369
where you were not executing in every
thread on the system simultaneously.

01:10:59.369 --> 01:11:03.819
This is displayed as a normal time profile and
you can look at this for things like figuring

01:11:03.819 --> 01:11:06.469
out why is my launch time slow for my application?

01:11:06.470 --> 01:11:10.659
What am I actually waiting on to
happen during my application launch?

01:11:11.819 --> 01:11:16.479
We also offer performance counter recording; we
can get OS X events from the operating system;

01:11:16.479 --> 01:11:22.000
we can get counts from the processor, from
memory controllers on power PC systems

01:11:22.000 --> 01:11:25.100
and we can actually look at things like L2 cache misses.

01:11:25.100 --> 01:11:29.360
We can look at memory bandwidth; this way
if you suspect that you're actually running

01:11:29.359 --> 01:11:33.009
into a hardware limitation you can
actually go in and look at this.

01:11:33.010 --> 01:11:38.289
You can look at this in both a time profile type format
and also in a spreadsheet type format with charting

01:11:38.289 --> 01:11:41.060
so that you can actually look at bandwidth overtime.

01:11:41.060 --> 01:11:47.370
For more information about any of these techniques
please come see us in the lab after the session

01:11:47.369 --> 01:11:52.989
and tomorrow we're looking at Shark documentation which
has full descriptions of all these different techniques.

01:11:52.989 --> 01:11:59.609
Now for some other ways to start
and stop Shark back to Ryan.

01:11:59.609 --> 01:12:01.779
( Applause )

01:12:01.779 --> 01:12:04.159
Thank you Rick.

01:12:04.159 --> 01:12:10.170
Alright, so by now hopefully you're all familiar with
the normal start button, the way to start and stop Shark;

01:12:10.170 --> 01:12:14.440
super easy, you've got configurable hot key, really awesome.

01:12:14.439 --> 01:12:20.479
The only down side here is it requires UI interaction
and it might show up in a really short time profile.

01:12:20.479 --> 01:12:23.639
So let's talk about some other ways to
use Shark, some other ways to point it

01:12:23.640 --> 01:12:26.560
at different processes and ways to trigger it.

01:12:26.560 --> 01:12:29.370
( Pause )

01:12:29.369 --> 01:12:39.090
An extension of process attach I last showed you is
process launch; say you want to profile the launch time

01:12:39.090 --> 01:12:46.600
of your application; let it profile say preference
loading, plug in loading, that sort of a thing.

01:12:46.600 --> 01:12:50.210
Go to process menu as you just saw,
pick launch, you'll get this menu here.

01:12:50.210 --> 01:12:53.230
It has many options you can start sampling immediately;

01:12:53.229 --> 01:12:57.259
this will start time profile before
the first instruction is ever executed.

01:12:57.260 --> 01:13:01.150
You can give it user-supplied arguments
working directory and environment variables.

01:13:01.149 --> 01:13:04.819
It's really perfect for really short live
applications like command mine tools.

01:13:04.819 --> 01:13:10.409
It's also great for really large applications like
Photoshop; say you want to profile the plug in loading

01:13:10.409 --> 01:13:14.729
and the preference checking at
start of time, that sort of a thing.

01:13:14.729 --> 01:13:16.500
Some other things we can do.

01:13:16.500 --> 01:13:22.100
What if the machine you're interested in is half way around
the world; no problem there; run Shark on the machine,

01:13:22.100 --> 01:13:27.890
put it in network mode; you can point another Shark
at it on different computer using Bonjour or TCP/IP.

01:13:27.890 --> 01:13:34.510
This allows you to simultaneously profile
multiple machines say in a parallel cluster.

01:13:34.510 --> 01:13:37.060
You can have Shark running on all the different machines.

01:13:37.060 --> 01:13:40.940
You can the remote configurations
on the fly; it's great for games,

01:13:40.939 --> 01:13:44.729
headless servers, parallel clusters, that sort of thing.

01:13:44.729 --> 01:13:47.629
We're all sadly familiar with the spinning pizza of death.

01:13:47.630 --> 01:13:51.100
Why does it happen, how to get rid of
it, lot of times that's hard to answer.

01:13:51.100 --> 01:13:53.850
The combination of time profiles all threads states

01:13:53.850 --> 01:13:57.310
and unresponsive application profiling
is here to solve that question for you.

01:13:57.310 --> 01:14:02.680
Anytime the spinning pizza of death pops up,
Shark will start taking a session for you.

01:14:02.680 --> 01:14:06.659
A couple of awesome features; you can give it
application name filters, partial application name

01:14:06.659 --> 01:14:11.399
or minimum hang threshold; say you're
only interested in stuff over a second.

01:14:11.399 --> 01:14:18.039
What about if you've got it down to one function
or one hot loop in your program; that's cool too;

01:14:18.039 --> 01:14:21.470
go in and add a couple of lines of
API to your code then you can start

01:14:21.470 --> 01:14:24.600
and stop Shark from directly within your application.

01:14:24.600 --> 01:14:28.060
There's a full example of this in the
Shark manual or you can find us in the lab.

01:14:28.060 --> 01:14:32.690
This is really great for really fine grain
control of sampling; it's got the easy API.

01:14:32.689 --> 01:14:38.099
The only downside here is you have to link with the CHUD
framework so it's not always the best for release stuff.

01:14:38.100 --> 01:14:45.600
Similarly we have command line; so same kind of
API in the command line, put Shark in remote mode,

01:14:45.600 --> 01:14:49.810
run these from the command line; it's good
for scripting stuff in remote log ins.

01:14:49.810 --> 01:14:52.350
What if you don't have any UI?

01:14:52.350 --> 01:14:53.970
There's also consult Shark.

01:14:53.970 --> 01:14:55.740
Quickly we're running out of time.

01:14:55.739 --> 01:15:00.399
I want to talk about something we introduced
last year called the window time facility.

01:15:00.399 --> 01:15:06.979
You're normal profiling, you start, you stop, you get
all the samples in between; it's great for normal stuff.

01:15:06.979 --> 01:15:09.979
What happens if you don't know
when your problems going to occur?

01:15:09.979 --> 01:15:14.549
To that end we've made the window time facility;
it allows you to press stop or press start, sorry,

01:15:14.550 --> 01:15:19.500
pick a sampling window and forget; go about trying
to reproduce your bug, as soon as it happens,

01:15:19.500 --> 01:15:23.380
wait until it's done, you press stop and
Shark gives you that last window samples.

01:15:23.380 --> 01:15:28.619
This is selectable with time profile, they're marked
in the config menu as you see here and it works

01:15:28.619 --> 01:15:31.899
with any other methods that we've talked about before.

01:15:31.899 --> 01:15:37.819
So in summary Shark is really easy; it allows you to
quickly identify exactly what code you need to optimize.

01:15:37.819 --> 01:15:43.409
There's many different options available for different
types of tasks including hardware performance counters

01:15:43.409 --> 01:15:48.739
and as you've just seen there's many different
ways to trigger and target these operations.

01:15:48.739 --> 01:15:50.569
For more information send us an e-mail.