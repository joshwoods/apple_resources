WEBVTT

00:00:12.220 --> 00:00:18.120
>> My name is Phil Kerly and I'm from
the Intel Software Solutions group.

00:00:18.120 --> 00:00:23.789
Today Mike and I are going to be presenting
on threading for performance using OpenMP

00:00:23.789 --> 00:00:29.299
in the Intel threading building blocks on Mac OS X.

00:00:29.300 --> 00:00:33.670
This is session 102 so hopefully you are in the right spot.

00:00:33.670 --> 00:00:41.859
Wanted to give a little bit about some of the
motivation for this talk, if you look back in the history

00:00:41.859 --> 00:00:55.030
of the Apple products the first 20 years we were basically
on a single threaded processing execution type CPU.

00:00:55.030 --> 00:01:05.390
After about 1976 started hittivng Duo Core or dual
processors, within about eight years we got into Quad Core

00:01:05.390 --> 00:01:10.489
which was just two years ago and
right now we are hitting eight cores.

00:01:12.250 --> 00:01:21.260
Intel is extremely enthusiastic about trying to help
software engineers improve their productivity in terms

00:01:21.260 --> 00:01:24.219
of threading because this is going to keep increasing.

00:01:24.219 --> 00:01:26.719
We're not going to slow down in
terms of the number of threads

00:01:26.719 --> 00:01:31.329
of execution we are going to be providing to developers.

00:01:31.329 --> 00:01:42.959
And I talk about threads of process or execution instead of
processors because it use to be that we had individual CPU's

00:01:42.959 --> 00:01:51.069
and then in the old Pentium 4 days we added something called
hyper threading which enabled multiple threads of execution.

00:01:51.069 --> 00:02:00.519
Then we moved that to multiple cores and so there is many
kinds of permutations that this really falls into for Intel.

00:02:00.519 --> 00:02:05.979
But in any case we need to be able to increase our
productivity in terms of threading our applications.

00:02:05.980 --> 00:02:09.069
So that's really what today is all about.

00:02:09.069 --> 00:02:19.329
So I'm going to talk a little bit about OpenMP, the
specification and I am going to talk a little bit

00:02:19.330 --> 00:02:24.380
about Intel threading building
blocks which we also call TBB.

00:02:24.379 --> 00:02:28.519
But the reality is in an hour,
hour and half talk there is no way

00:02:28.520 --> 00:02:33.600
that we can cover the full breath
and dept of these two technologies.

00:02:33.599 --> 00:02:40.460
SO what I am really hoping to do is that by the end of
this talk I will have at least covered enough material

00:02:40.460 --> 00:02:49.180
to influence you to actually go and investigate these
technologies, maybe we'll see a little spy kit or downloads

00:02:49.180 --> 00:02:54.040
on our Intel complier or our TBB product.

00:02:54.039 --> 00:02:57.909
Hopefully that is what will happen over the next week.

00:02:57.909 --> 00:03:05.150
So the agenda that I had for today is really to talk
about what I consider to be the fundamental principals

00:03:05.150 --> 00:03:11.010
of threading for performance, just
so that we have a common base line.

00:03:11.009 --> 00:03:15.489
Cover OpenMP with the specification
with a high level overview.

00:03:15.490 --> 00:03:22.129
And them Mike Lewis is going to come up and do a little
demo for us in terms of implementing an application

00:03:22.129 --> 00:03:30.599
that was already threaded but it was threaded for a
functional perspective and apply OpenMP to that application

00:03:30.599 --> 00:03:36.629
and then compare that implementation
to a POSIX treaded implementation

00:03:36.629 --> 00:03:39.990
and look at the performance and the implementation.

00:03:39.990 --> 00:03:47.480
Then we'll move on to TBB and again Mike will give
us a little bit of insight into what it took to port

00:03:47.479 --> 00:03:51.579
that same application to TBB, look at it's performance.

00:03:51.580 --> 00:04:01.620
And then we'll talk a little bit towards the end about where
these tools leave off and where the real engineering begins,

00:04:01.620 --> 00:04:08.789
the hard part, the work we all have to do.

00:04:08.789 --> 00:04:15.340
So I really have four fundamental principals and
the first one I have up here is parallel algorithms.

00:04:15.340 --> 00:04:23.709
Without having parallel algorithms, whether they
be functionally decomposed or dated decomposed,

00:04:23.709 --> 00:04:31.089
if we can come up with a solution for how we would
conceptually implement the algorithm then we are not going

00:04:31.089 --> 00:04:34.469
to achieve performance by threading.

00:04:34.470 --> 00:04:45.080
We clearly have to minimize the overhead of threading,
if you look at the old days, three years ago.

00:04:45.079 --> 00:04:51.879
If you threaded your application for a Dual
processor platform, you know it's fairly easy to get

00:04:51.879 --> 00:04:57.769
or to minimize the threading in overhead compared to the
amount of work that you are actually going to accomplish.

00:04:57.769 --> 00:05:06.289
But with eight cores that challenge becomes a lot tougher
as we start subdividing or data decomposing our application

00:05:06.290 --> 00:05:14.189
and our task, then the overhead becomes a little bit harder,
becomes a little bit larger compared to that level of work.

00:05:14.189 --> 00:05:24.129
Just because we have a parallel algorithm and we
minimize the overhead, we also have to balance

00:05:24.129 --> 00:05:27.189
that workload across those execution units.

00:05:27.189 --> 00:05:34.009
As we go through the talk today we will go through
a few examples where that is a little bit tricky

00:05:34.009 --> 00:05:40.750
and how both OpenMP and TBB can
help with balancing those workloads.

00:05:40.750 --> 00:05:48.389
And finally anything that has to do with performance has
to taken in to consideration the hardware architecture.

00:05:48.389 --> 00:05:54.079
And all of these are actually very
intertwined with one another.

00:05:54.079 --> 00:06:02.680
As you try to make your algorithm more parallel and
you spread them out among more cores the relative ratio

00:06:02.680 --> 00:06:13.090
of the work you accomplish for your task verses the overhead
that ratio changes, the threading overhead gets larger.

00:06:13.089 --> 00:06:21.489
As we start subdividing that work and we put it across more
cores, more threads of execution, balance in that workload

00:06:21.490 --> 00:06:26.470
across those multiple threads of
execution becomes more challenging.

00:06:26.470 --> 00:06:35.110
And as we increase the differences in the hardware and we
move from a true symmetric multi processing type of platform

00:06:35.110 --> 00:06:41.449
to something that's a kin to more of a newma
type architecture where not every execution,

00:06:41.449 --> 00:06:50.089
engine that you are executing on necessarily has
the same power or performance as every other core

00:06:50.089 --> 00:06:53.469
that you might be running on, there's
differences between those execution units.

00:06:53.470 --> 00:07:00.680
We clearly saw that with hyper threading with today
processors or today's Mac Pro platform with eight threads

00:07:00.680 --> 00:07:06.400
of execution or eight cores, some of those cores share
the same cache and some of them sit on the same bus

00:07:06.399 --> 00:07:10.519
and some don't and so there are
differences in the performance capabilities

00:07:10.519 --> 00:07:14.740
of those individual cores as they relate to one another.

00:07:14.740 --> 00:07:23.199
So as I look at this list of fundamental principals for
threading for performance, its clear to me that some

00:07:23.199 --> 00:07:32.319
of the things we do, we tend to do repetitively
because there more automatic, there more mechanical

00:07:32.319 --> 00:07:36.560
and less about the algorithm or
what we're trying to achieve.

00:07:36.560 --> 00:07:42.730
For example using the hardware architecture we
have to detect the number of detection units

00:07:42.730 --> 00:07:46.870
that we are actually going to run on, how many
cores or how many processors or how many threads

00:07:46.870 --> 00:07:50.720
of execution does the hardware actually support.

00:07:50.720 --> 00:07:58.590
We need to detect that right and that's pretty standard
process of doing that and anyone that's doing multithreading

00:07:58.589 --> 00:08:04.939
for performance is going to have some
technique or some natural way of doing that.

00:08:08.240 --> 00:08:17.560
If you do any kind of cache blocking where you are going
to detect the size of the cache and modify the perimeters

00:08:17.560 --> 00:08:24.399
by which you are doing your parallel data
decomposition you have to detect the size of the cache

00:08:24.399 --> 00:08:27.599
from the hardware in order to make that, make that change.

00:08:27.600 --> 00:08:30.540
So those kinds of things tend to be more automatic,

00:08:30.540 --> 00:08:38.320
tend to be things that you'll do repeatedly
from implementation to implementation.

00:08:38.320 --> 00:08:51.050
So what I hope to show is that with OpenMP and with Intel
threading building block that some of that mechanical stuff

00:08:51.049 --> 00:08:56.740
that we are use to doing can actually be
taken over by these technologies and allow you

00:08:56.740 --> 00:09:05.620
to actually improve your productivity by focusing on
the problem at hand and focusing on the constraints

00:09:05.620 --> 00:09:09.000
under which your application are running.

00:09:09.000 --> 00:09:11.580
So let's talk a little but about OpenMP.

00:09:11.580 --> 00:09:19.670
First its not a standard but actually a specification
from a group of industry companies that were interested

00:09:19.669 --> 00:09:28.360
in promoting threading and they really
specifically targeted shared memory multiprocessing,

00:09:28.360 --> 00:09:34.129
not necessarily symmetric multiprocessing
but shared memory multiprocessing.

00:09:34.129 --> 00:09:40.259
The other thing they did is that they kind of looked at
that motivational slide and said well we have a lot of code

00:09:40.259 --> 00:09:46.019
out there that is actually serial code and we
want to move people and get them moving more

00:09:46.019 --> 00:09:50.129
and the developers more and more
into a threading environment.

00:09:50.129 --> 00:09:56.269
So it really was started to try to get
encouraged threading serial code incrementally.

00:09:56.269 --> 00:10:00.579
So it's not a big bang type of approach to threading.

00:10:00.580 --> 00:10:09.020
You can actually go to the OpenMP org website and actually
see the full specification and it will actually allow

00:10:09.019 --> 00:10:14.529
for vendor specific implementation
or extensions to the specification.

00:10:15.620 --> 00:10:21.669
OpenMP is something that has to be supported by
a complier and in this case Intel's C++

00:10:21.669 --> 00:10:29.169
and Fortran compliers do support OpenMP on Mac OS X.

00:10:29.169 --> 00:10:38.019
SO what is OpenMP, it's really a set of programming
directives supported by some library routines.

00:10:38.019 --> 00:10:45.860
There are some environment variables that
help you control the runtime environment,

00:10:45.860 --> 00:10:48.070
great for debugging and performance tuning.

00:10:48.070 --> 00:10:56.750
And then of course there are vendor extensions and we'll
talk about Intel's extensions as part of this session today.

00:11:01.210 --> 00:11:09.710
Like I said before, I'm hoping to just provide
an overview and not the complete specification,

00:11:09.710 --> 00:11:16.769
it would just take much longer then an hour and
half talk but kind of the main points to OpenMP

00:11:16.769 --> 00:11:22.000
and the programming directives are
the four that I have listed here.

00:11:22.000 --> 00:11:27.009
There is what's called a parallel for
which allows you to using this pragma,

00:11:27.009 --> 00:11:38.669
parallelize for loops have a defined start and ending
index and parallelize that naturally using the pragma's.

00:11:38.669 --> 00:11:46.370
That's more of a day to decomposition approach but OpenMP
also supports what's called parallel sections or sections

00:11:46.370 --> 00:11:51.129
which allow you to do more of a
functional type of threading.

00:11:51.129 --> 00:11:56.340
If you actually wrote serial code in
which you called function A and function B

00:11:56.340 --> 00:12:00.200
and there was really no dependences
between the two and you could reorder them,

00:12:00.200 --> 00:12:05.960
then parallel sections might be a quick
way to actually thread that between them.

00:12:05.960 --> 00:12:11.650
And then there were some other kind of natural things
that could be done with pragma's that did not have

00:12:11.649 --> 00:12:19.110
to be implemented using function calls that allowed and
supported threading as well, such as critical sections

00:12:19.110 --> 00:12:26.480
and variable in which all threads have to achieve
that variable before they can move beyond it.

00:12:28.200 --> 00:12:36.610
You augment these work sharing constructs
with data sharing clauses appended to them.

00:12:36.610 --> 00:12:43.259
And here are a few examples, as part of that
for loop, the data you have within that loop,

00:12:43.259 --> 00:12:49.240
you can define those as being shared
type data, can be private type data.

00:12:49.240 --> 00:12:57.080
You can have first private in which all the threads
get the first value at initialization is all the same

00:12:57.080 --> 00:13:02.300
and then after the threads start executing they
are actually private to the threads themselves.

00:13:02.299 --> 00:13:05.259
And then it supports such things as reduction.

00:13:05.259 --> 00:13:10.899
Reduction is an example of a reduction would be if you
had an array that you just wanted to sum up all the values

00:13:10.899 --> 00:13:17.829
in the array, you could logically construct
a parallel for algorithm that could operate

00:13:17.830 --> 00:13:21.780
on that array by subdividing it into smaller pieces.

00:13:21.779 --> 00:13:28.860
Doing subtotals on each of those sections and then at
the end doing the total reduction and just taking the sum

00:13:28.860 --> 00:13:31.519
of the sub values and adding them together.

00:13:31.519 --> 00:13:34.139
So OpenMP can support that type of paradigm as well.

00:13:34.139 --> 00:13:45.799
Talked a little bit about workload balancing and it was
clear that not just one type of scheduling algorithm worked

00:13:45.799 --> 00:13:52.659
for all types of workloads and so Open MP
supports three types of scheduling algorithm,

00:13:52.659 --> 00:14:01.909
one is static another is dynamic and then we have guided
and as we go through the talk I will actually show examples

00:14:01.909 --> 00:14:04.750
of these individual type of scheduling algorithms.

00:14:04.750 --> 00:14:18.019
The pragma themselves are not sufficient to actually give
a complete or a robust set of tools in order to be able

00:14:18.019 --> 00:14:21.860
to implement threading even on simple for loops.

00:14:21.860 --> 00:14:29.080
There's often application specific activities
that require more then just a critical section.

00:14:29.080 --> 00:14:38.509
And so the OpenMP standard or specification allows for
some of these threads synchronization type routines

00:14:38.509 --> 00:14:42.919
and in this case setting lock,
unlocking locks and testing locks.

00:14:44.000 --> 00:14:51.870
It also allows you to query and set some of
the run time execution routines for the values.

00:14:51.870 --> 00:14:56.980
For example you can actually set the number of
threads, if you don't set the number of threads,

00:14:56.980 --> 00:15:01.789
OpenMP will actually detect it from
the hardware and it will implement

00:15:01.789 --> 00:15:08.299
or use that many threads across the parallel section.

00:15:08.299 --> 00:15:15.539
But you can actually change that, not all applications
are well suited to complete well balanced environments

00:15:15.539 --> 00:15:21.849
where the number of threads equaling the
number of executions units is sufficient.

00:15:21.850 --> 00:15:29.180
And then you could also query, you know what thread am I, so
you may have some specific implementation in your algorithm

00:15:29.179 --> 00:15:37.000
that you want to know and assign a particular
thread to some particular thread of execution.

00:15:38.679 --> 00:15:40.309
And then there are timing routines.

00:15:40.309 --> 00:15:48.609
If we're threading for performance you know we want to know
how long do things take and so there were some standard type

00:15:48.610 --> 00:15:54.370
of routines that were added to support just
actually being able to query the system time

00:15:54.370 --> 00:15:57.139
and be able to do those types of evaluations.

00:15:57.139 --> 00:16:08.689
So the pragma's, the libraries and now is supported
by the environment variables so that in addition

00:16:08.690 --> 00:16:15.100
to what you are doing programmatically you have
the option of actually dynamically changing these,

00:16:15.100 --> 00:16:20.580
some of these parameters without actually
having to recompile your code and executing.

00:16:20.580 --> 00:16:29.480
Great for supporting you know performance evaluation,
so here OpenMP schedule basically allows you

00:16:29.480 --> 00:16:37.980
to define exactly what kind of the three scheduling types,
what scheduler do you want and what is the chunk size.

00:16:37.980 --> 00:16:44.769
The chunk size is basically how do you want to divide up in
that for loop that your using, how do you want to divide

00:16:44.769 --> 00:16:51.110
up that chunk, in terms of how much
chunk should each thread operate on.

00:16:51.110 --> 00:16:54.750
And we'll cover more about that as we proceed.

00:16:54.750 --> 00:16:59.330
You can change the number of threads, so we had the
routine that will allow you to change it in the code.

00:16:59.330 --> 00:17:03.960
But we also have an environment variable
that allows you to change it on the fly.

00:17:03.960 --> 00:17:09.600
So you could actually write a script dynamically change
the number of threads, gather your performance data

00:17:09.599 --> 00:17:17.109
and see if that's you know what is the
optimal number of threads that you need.

00:17:17.109 --> 00:17:23.309
We have dynamic, which is basically
enables or allows the OpenMP runtime

00:17:23.309 --> 00:17:27.059
to dynamically change the number of
threads that you are going to use.

00:17:27.059 --> 00:17:32.460
Some OpenMP implementation support this and some don't.

00:17:32.460 --> 00:17:37.620
This allows you to enable and disable
that type of feature and then nest it.

00:17:37.619 --> 00:17:42.919
So its conceivable that you can have a pragma
on a for loop and then somewhere in the body

00:17:42.920 --> 00:17:52.779
of that for loop you have another for loop, so that would
be a condition of where you have nested OpenMP directives

00:17:52.779 --> 00:17:59.740
and the question is really, you know if your running it
on a eight way system do you really want eight threads

00:17:59.740 --> 00:18:05.819
of execution or do you want, or is your
implementation such that you want to run 16 threads.

00:18:05.819 --> 00:18:10.230
And so you can actually enable or
disable parallel constructs in which

00:18:10.230 --> 00:18:15.759
that second parallel for would be ignored.

00:18:15.759 --> 00:18:21.960
And all of these, so you don't get too concerned that you
know if you implement this using OpenMP in your application

00:18:21.960 --> 00:18:26.240
that somebody can actually go in and start
fiddling with the behavior of your application.

00:18:26.240 --> 00:18:32.519
If you actually write these and use the library
routines, they override the environment variables.

00:18:34.220 --> 00:18:46.259
So Intel's extension to OpenMP, we allow
you to modify and control the stack size.

00:18:46.259 --> 00:18:48.930
Some people are concerned about the
foot print of their application.

00:18:48.930 --> 00:18:59.380
Some you know are very knowledgeable of exactly how large
the stack is and want to be able to control that capability.

00:18:59.380 --> 00:19:07.190
We have threaded local memory,
allocation routines so that you can modify

00:19:07.190 --> 00:19:12.210
or manage your own memory allocation within the routines.

00:19:12.210 --> 00:19:21.279
And then Intel also implemented something
called the KMP block time and this is really

00:19:21.279 --> 00:19:25.379
to control the spin waits of how
the locks operate with an OpenMP.

00:19:25.380 --> 00:19:32.110
And we'll talk a little about that when I show the example.

00:19:32.109 --> 00:19:36.229
So here's an OpenMP for loop, its fairly basic.

00:19:36.230 --> 00:19:46.480
It's a for loop, it has a defined starting index
and it has a defined end, happens to be a parameter

00:19:46.480 --> 00:19:51.029
but it is a defined end and then
there is a for loop within that.

00:19:51.029 --> 00:20:00.529
So this could e some serial code, and by simply adding
a pragma OpenMP parallel for assignment or pragma,

00:20:00.529 --> 00:20:04.319
we can actually parallelize this loop very simply.

00:20:04.319 --> 00:20:08.149
Now the private's required because
if you look at the second for loop,

00:20:08.150 --> 00:20:15.340
the second for loop is actually updating the J
variable and if we allow, if we didn't control access

00:20:15.339 --> 00:20:23.069
to that J variable then any thread could actually
modify that and so here we have to make it private.

00:20:23.069 --> 00:20:28.470
Now an OpenMP it's possible that maybe most
of your threads or variables are private.

00:20:28.470 --> 00:20:35.329
So you may want to actually make the default
for how variables are handled as shared,

00:20:35.329 --> 00:20:39.799
or you could make the default as private.

00:20:39.799 --> 00:20:47.220
Currently the default or the standard default is shared,
you can also specify it to none which means that you have

00:20:47.220 --> 00:20:52.980
to specify every variable and sometimes its actually good to
do that because then you can look at the compiler will spit

00:20:52.980 --> 00:20:58.079
out all of the information about which variables you
have not defined as shared or private and you have

00:20:58.079 --> 00:21:02.740
to make a determination on exactly
how you want those shared.

00:21:02.740 --> 00:21:06.450
So in this case A, B and N are all shared by default,

00:21:06.450 --> 00:21:12.090
the variable I which is the index for
the parallel for loop is private.

00:21:12.089 --> 00:21:20.939
It has to be private in that case but what's interesting
if you look at this for loop is that its not,

00:21:20.940 --> 00:21:29.690
it is also dependent on the index I, which means that this
for loop is not going to be a well balanced for loop.

00:21:29.690 --> 00:21:38.519
The work that happens when I is one that means J is going
to go from zero to one where as what I is a 100 then

00:21:38.519 --> 00:21:45.740
that for loop is going to go from zero
to 99 or whatever, 100 in this case.

00:21:45.740 --> 00:21:54.789
So what would happen if we actually ran this using
the Open MP static scheduler which is the default,

00:21:54.789 --> 00:22:00.599
we would get something that looks like this, we didn't
specify this scheduler and we didn't specify the chunk size

00:22:00.599 --> 00:22:07.819
so OpenMP is going to take the total workload, the total
number or indices that are being operated on within I

00:22:07.819 --> 00:22:13.960
and its going to statically allocate them and divide that
by the number of threads that its actually going to use.

00:22:13.960 --> 00:22:16.250
This example I use for threads.

00:22:16.250 --> 00:22:21.640
But because a workload is not balanced what
will happen is that threads will execute.

00:22:21.640 --> 00:22:27.480
First one you know may take a long time to run, the
second one a little bit shorter and a little bit shorter.

00:22:27.480 --> 00:22:35.079
So we already violated the balanced
workload fundamental principal

00:22:35.079 --> 00:22:40.069
for performing, implementing threads for performance.

00:22:40.069 --> 00:22:48.480
And actually if you opened up Shark, this is a Shark view
of system trace if you haven't used Shark I'd encourage you

00:22:48.480 --> 00:22:54.990
to do so, it's defiantly very good for
analyzing your threading behavior especially

00:22:54.990 --> 00:22:57.599
if you are not familiar with your application.

00:22:57.599 --> 00:23:05.269
And what we see in this view, this was taken on an eight
way a Mac Pro eight way system, what we see is that all

00:23:05.269 --> 00:23:12.519
of the threads are busy and what we see you can see
the color coding and what the color coding means that,

00:23:12.519 --> 00:23:19.369
that's a particular core or a particular
execution CPU for the OS.

00:23:19.369 --> 00:23:25.819
And so in this case you see some hopping around of those
threads, sometimes it runs on you know the green runs

00:23:25.819 --> 00:23:31.309
on green and then it moves lighter or some other
core that's a lighter green and it runs on pink

00:23:31.309 --> 00:23:35.059
and so we see these threads moving
around but there fully busy.

00:23:35.059 --> 00:23:40.490
And yet I just showed you that the workflow wasn't balanced.

00:23:40.490 --> 00:23:47.380
So of you use that Intel extension that allows you
to modify the KMP block time and you set it to zero.

00:23:47.380 --> 00:23:54.800
In other words, don't spin at all if you come up with a
lock or you hit a point at which you can't make progress,

00:23:54.799 --> 00:23:59.609
don't spin for a while hoping that lock's going to
be released and you are going to be able to move on.

00:23:59.609 --> 00:24:06.899
Actually go ahead and go to sleep and if you look
at that then this is the kind of view you get.

00:24:06.900 --> 00:24:11.070
You see their stair stepping a fact and that's
because the second for loop wasn't balanced.

00:24:11.069 --> 00:24:22.889
SO what we could do, is we could still use static
OpenMP, scheduler and we could set the chunk size instead

00:24:22.890 --> 00:24:31.790
of being N divided by the number of CPU's, we could
just in this case for example define it as one.

00:24:31.789 --> 00:24:36.289
Okay and essentially want this allows it to do is
instead of statically allocating all of the indexes

00:24:36.289 --> 00:24:40.059
to a particular thread it allows the thread to run.

00:24:40.059 --> 00:24:44.619
When it's done it then goes and pulls the next one.

00:24:44.619 --> 00:24:51.409
Okay now it's still static in the sense that all of these
threads are going to get them in order but at least now we,

00:24:51.410 --> 00:24:58.210
instead of having all of the long workload running on
one single thread of execution it's kind of spread it

00:24:58.210 --> 00:25:01.390
out over the multiple threads of execution.

00:25:01.390 --> 00:25:07.550
But its still not extremely well balanced here you
still have a little bit of gap towards the end.

00:25:07.549 --> 00:25:18.419
So what we can do is we can take that same parallel for
with the private J variable and change the scheduling

00:25:18.420 --> 00:25:22.960
to dynamic and in this case define
it as a chunk size of one.

00:25:22.960 --> 00:25:28.970
We could of chosen different chunk sizes but
we know that the second for loop is going

00:25:28.970 --> 00:25:34.170
to be decreasingly you know less work as we go along
there is no reason to define it as anything larger.

00:25:34.170 --> 00:25:40.269
And in that case what OpenMP will do is allow
the static scheduling to look something like,

00:25:40.269 --> 00:25:42.720
or the dynamic scheduling to look like this.

00:25:42.720 --> 00:25:49.880
In which case every thread pulls for the next work
unit that it needs, so if it takes longer to execute

00:25:49.880 --> 00:25:57.590
or even if the thread gets interrupted by the OS to
go off and do some other networking task or whatever

00:25:57.589 --> 00:26:07.109
at least the threads will balance out just by simply adding
that one scheduling pragma clause to the pragma statement.

00:26:07.109 --> 00:26:15.649
So that was kind of parallel for with the different types
of scheduling algorithms, but you could also implement more

00:26:15.650 --> 00:26:24.670
of a functional decomposition and again simply by
just adding the pragma OpenMP parallel sections

00:26:24.670 --> 00:26:31.440
and then defining the sections you can actually make each
of those as independent tasks that will run concurrently.

00:26:31.440 --> 00:26:37.670
Now the way OpenMP works is that
all of the threads of execution

00:26:37.670 --> 00:26:44.300
within the parallel sections have an implicit
join at the end of the parallel section.

00:26:44.299 --> 00:26:50.369
It's not always what you want to happen, sometimes these are
independent threads and you may actually want to continue on

00:26:50.369 --> 00:26:53.349
and do something else and not necessarily wait.

00:26:53.349 --> 00:27:02.829
So OpenMP allows you to add a no wait
statement to the end of that pragma directive.

00:27:02.829 --> 00:27:05.869
So what we're going to do now is
we're going to show a little demo.

00:27:05.869 --> 00:27:11.849
But first I am going to have Mike come up and explain a
little bit about the demo and what we've done in terms

00:27:11.849 --> 00:27:17.199
of analyzing the application and
show a little performance results

00:27:17.200 --> 00:27:21.509
that of using OpenMP and comparing that to POSIX threads.

00:27:21.509 --> 00:27:29.109
( Pause in speaking. )

00:27:29.109 --> 00:27:30.419
>> Mike: Thank you Phil.

00:27:30.420 --> 00:27:32.600
We put together a demo.

00:27:32.599 --> 00:27:37.009
I think it shows this technology off really well.

00:27:37.009 --> 00:27:43.039
I'll first start by showing you an introduction
to what or demo does and how it functions

00:27:43.039 --> 00:27:47.000
so you have a little background
before we show you actual code.

00:27:47.000 --> 00:27:48.740
Here we have some pseudo code.

00:27:48.740 --> 00:27:57.359
It shows you pretty much what would
happen in a typical particle system.

00:27:57.359 --> 00:28:05.429
In this loop we have everything attracted to one
gravity point it's an order and complexity loop.

00:28:05.430 --> 00:28:12.850
Even thought there are a lot of operations that go on
when it recalculates the momentum and velocity vector,

00:28:12.849 --> 00:28:18.539
its still order and so you can have a lot of particle's
you don't need a whole lot of computing power.

00:28:18.539 --> 00:28:23.809
But if you want something that's a little more complex,
say you wanted to simulate particles in space,

00:28:23.809 --> 00:28:27.690
there all attracted to each other,
that's sort of N squared complexity.

00:28:27.690 --> 00:28:29.840
You would probably have a loop like this.

00:28:29.839 --> 00:28:36.230
In this loop we have a little optimization because
Newton's third law we can update two velocity particles

00:28:36.230 --> 00:28:39.589
at the same time.

00:28:39.589 --> 00:28:42.720
Unfortunately we wanted to make this thread safe.

00:28:42.720 --> 00:28:47.370
We wanted to implement OpenMP for
it, so we had to make a compromise.

00:28:47.369 --> 00:28:54.549
If we split this up with an OpenMP parallel for
right now particle J could have a raise condition

00:28:54.549 --> 00:28:59.059
when we decrement it and this isn't good.

00:28:59.059 --> 00:29:03.470
There is other algorithms we could split
it off and still have this optimization

00:29:03.470 --> 00:29:08.470
but for simplicity sake we decided
to just make it this and refactor it.

00:29:08.470 --> 00:29:14.610
So it gets about 60 percent of the
performance and we can thread it with OpenMP

00:29:14.609 --> 00:29:19.509
and I'll show you the demo we worked on right now.

00:29:19.509 --> 00:29:33.129
( Pause in speaking. )

00:29:33.130 --> 00:29:38.450
>> Mike: So here we have a simulation, you see
particles floating around in space and when they collide

00:29:38.450 --> 00:29:43.319
with each other, they absorb each other
and they are all attracted to each other.

00:29:43.319 --> 00:29:53.509
This is similar to the previous loop I showed you
in serial code and I'll show you the code right now.

00:29:53.509 --> 00:30:04.170
( Pause in speaking. )

00:30:04.170 --> 00:30:09.860
>> Mike: Here we have the original loop which is
similar to the first more complex loop I showed you

00:30:09.859 --> 00:30:14.009
with the optimization, you have the outer loop right here

00:30:14.009 --> 00:30:28.519
and we have the inner loop right
around, it's right around here.

00:30:28.519 --> 00:30:37.940
We had to refactor is so we got rid of a potential
risk condition here and I'll quickly show you

00:30:37.940 --> 00:30:42.390
that this is the hot loop by using Shark.

00:30:44.839 --> 00:30:51.839
So we open up Shark, we start a time profile, we have our
process selected, we have a time limit set to five seconds

00:30:51.839 --> 00:30:59.589
because if you have a pretty simple application,
you can get a really good sample in five seconds.

00:30:59.589 --> 00:31:01.509
So I start Shark.

00:31:01.509 --> 00:31:08.569
( Pause in speaking. )

00:31:08.569 --> 00:31:10.819
>> Mike: And we get a profile right here.

00:31:10.819 --> 00:31:19.250
Most of the work is done in this manual loop right here as
you can see and there's actually other accessor functions

00:31:19.250 --> 00:31:25.829
for vector class but there all in line so
you'll see most of the work in this one loop.

00:31:25.829 --> 00:31:31.329
The names kind of obfuscated because the
optimizations that are going on but if you double click

00:31:31.329 --> 00:31:34.509
on it you can see some of the assembling.

00:31:34.509 --> 00:31:44.680
You can see that the square root right here is taking up a
lot of processing power on the float division and all that.

00:31:44.680 --> 00:31:52.750
So I'll show you how OpenMP can make a really huge
performance difference after I show you some performance

00:31:52.750 --> 00:31:55.609
that we are getting right now with the original loop.

00:31:55.609 --> 00:31:58.969
So right here you can see our updates for a second,

00:31:58.970 --> 00:32:03.789
this is the benchmark that we are using is
how many times the velocity vectors of each

00:32:03.789 --> 00:32:05.970
of the particles are updated each second.

00:32:05.970 --> 00:32:15.819
So they can be updated between frames, its all concurrent
and it could be slower then each frame, after each frame,

00:32:15.819 --> 00:32:19.829
they'll still move around but it won't
be as precise if it updates slower.

00:32:19.829 --> 00:32:24.309
So this is our original code that's not thread safe.

00:32:24.309 --> 00:32:28.740
I'll tab to a loop that shows our thread safe code.

00:32:28.740 --> 00:32:36.130
As you can see the update per second, it's about
60 percent of the original but this is okay

00:32:36.130 --> 00:32:42.020
because when we're threading for this we're
going to be able to get theoretically four

00:32:42.019 --> 00:32:46.720
or eight times improvement and it's a trade off.

00:32:52.690 --> 00:33:00.200
So here's a copy of the thread safe original loop
just so I can show the different loops side by side.

00:33:00.200 --> 00:33:04.680
Simply I'll add a pragma statement as Phil showed.

00:33:11.589 --> 00:33:19.849
And a reduction since we are incrementing this variable
right here each loop we have to add our reduction

00:33:19.849 --> 00:33:34.740
so it doesn't create a raised condition
and try to increment it at the same time.

00:33:34.740 --> 00:33:47.450
And we'll build it and here we have our original
loop again, getting about 50 updates per second.

00:33:47.450 --> 00:33:57.460
We have our thread safe original getting about 30
updates per 0second and we have our OpenMP version.

00:33:57.460 --> 00:34:00.730
We'll just wait a second for the updates to propagate.

00:34:03.460 --> 00:34:10.940
We hit about 215, 220 and that's a really huge performance
increase added one line of code, did the refactoring

00:34:10.940 --> 00:34:18.289
and get a lot more precision and accuracy
in simulations that are real time.

00:34:19.590 --> 00:34:26.900
Now you could do this in native pthreads as well and we
actually did this just to show you how complex it could be

00:34:26.900 --> 00:34:36.329
and it wasn't that fun but I'll show you a little
of the code so you can see what we went through.

00:34:36.329 --> 00:34:46.869
So we decided to implement a thread pool and you
can see we have pthread, pthread cons, mutexes.

00:34:46.869 --> 00:34:52.489
We have a job class and we have all this stuff going on.

00:34:52.489 --> 00:35:00.750
I am not going to go into the nitty gritty because there's
too much to go over but I'll show you the size of the code.

00:35:00.750 --> 00:35:05.260
And we also have a worker thread
that's implemented right here.

00:35:05.260 --> 00:35:10.370
Still a lot of concurrency that you have to
worry about, debugging we had a lot of deadlock

00:35:10.369 --> 00:35:15.539
and we just have to work out all those bugs.

00:35:15.539 --> 00:35:21.809
And here's where we set it up, we have a structure we have
to make it so it passes a void star pointer to the threads

00:35:21.809 --> 00:35:27.980
so we have to figure out a good way to pass
it so we have to set everything up here

00:35:27.980 --> 00:35:33.050
and you can get a little more performance
out of using native pthread

00:35:33.050 --> 00:35:36.010
so I'll show you a little bit of
the performance that we get.

00:35:36.010 --> 00:35:50.800
So we get about 210, 215 with OpenMP and then
with our pthreads implementation we squeeze

00:35:50.800 --> 00:35:55.950
out a little bit more performance, its
not significant as about 10 percent

00:35:55.949 --> 00:36:01.619
and whatever threading model you choose
to depends on how important the code is.

00:36:01.619 --> 00:36:07.299
If you need that 10 percent by all means go for the
pthreads, but if you're just looking for an easy fix

00:36:07.300 --> 00:36:10.620
to get most of there performance
possible you can go with OpenMP.

00:36:10.619 --> 00:36:21.509
Now I am going to bring you back to Phil he has a
little more to go over with TBB I think and here he is.

00:36:21.510 --> 00:36:28.660
( Applause )

00:36:28.659 --> 00:36:30.129
>> Phil: Thank you, Mike.

00:36:35.070 --> 00:36:41.320
So one of the things we talked about initially was kind of
the four principals and I think you can actually see it,

00:36:41.320 --> 00:36:45.670
hopefully saw it applied with Mike's demo.

00:36:45.670 --> 00:36:49.010
First of all he needed that parallel algorithm.

00:36:49.010 --> 00:36:55.330
The reality is, that the code that he has
received which was implemented as threaded

00:36:55.329 --> 00:37:02.670
because it supported a functional thread in terms
of managing a user input and those kinds of threads

00:37:02.670 --> 00:37:09.550
and the display thread and then a computational thread.

00:37:09.550 --> 00:37:18.440
The serial implementation was optimal for a serial
implementation but it clearly was an optimal for threading.

00:37:18.440 --> 00:37:27.559
So even though he had to refactor the code so that it
would be more suited to threading and actually went slower.

00:37:27.559 --> 00:37:38.049
In the serial case he was able to you know focus on the
parallelism of the work that he was trying to accomplish

00:37:38.050 --> 00:37:42.230
and not on the mechanics of implementing the threading.

00:37:42.230 --> 00:37:52.619
So it's kind of amazing that for comparable
performance you know its, you know one line of code,

00:37:52.619 --> 00:37:58.839
the few changes that had to happen because he had to
refactor the code to support parallelism versus hundreds

00:37:58.840 --> 00:38:03.220
of lines of code to do it in POSIX threads.

00:38:04.530 --> 00:38:12.030
So there's huge productivity improvement,
a lot fewer changes, less debugging,

00:38:12.030 --> 00:38:17.090
spent a lot less time on actually figuring
out where the synchronization problems were.

00:38:17.090 --> 00:38:26.250
We had lots of issues with the POSIX's version initially,
and he focuses on the behavior okay and the constraints.

00:38:26.250 --> 00:38:31.760
So initially when we first looked at that threading
algorithm we actually took the serial implementation

00:38:31.760 --> 00:38:38.610
and just threw OpenMP at it and we didn't get the
performance that we were expecting, one because it had a bug

00:38:38.610 --> 00:38:45.329
in it and two because we needed to take care
of the fact that we had this extra constraint

00:38:45.329 --> 00:38:50.049
that we were updating two variables
in the array verses the single one.

00:38:50.050 --> 00:38:56.950
And so putting any kind of OpenMP synchronization or
critical sections around that really impacted performance

00:38:56.949 --> 00:39:06.529
but still we got to those kinds of decisions much quicker
and then we were able to focus on the actual performance

00:39:06.530 --> 00:39:13.230
and so clearly with OpenMP, okay we only got 90 percent but
you know what it took 10 percent of the time more or less

00:39:13.230 --> 00:39:20.380
and actually got along way to where we needed
to be versus the POSIX's implementation.

00:39:20.380 --> 00:39:28.530
So let's contrast that a little bit with
the Intel threading building blocks.

00:39:29.800 --> 00:39:33.390
It's another approach to implementing threading.

00:39:33.389 --> 00:39:40.829
Again tries to hide some of the mechanics
much like OpenMP does from the developer

00:39:40.829 --> 00:39:44.190
and allow you to focus on the problem space.

00:39:45.329 --> 00:39:50.929
And so one thing it does is because it's because
it's an optimized library we're not constrained

00:39:50.929 --> 00:39:53.649
by just implementing or using pragmas.

00:39:53.650 --> 00:40:02.619
There's a lot complex threading models that are supported
by Intel's threading building blocks that OpenMP.

00:40:02.619 --> 00:40:11.139
OpenMP supports the parallel for in parallel sections
implementation where as we'll see once we get into TBB

00:40:11.139 --> 00:40:19.269
that there's a lot more threading models that
can be applied much, much easier using TBB.

00:40:19.269 --> 00:40:22.000
It's C++ so there's no Fortran,

00:40:22.000 --> 00:40:28.369
so OpenMP is supported by the Intel
complier both on C, C++ and Fortran.

00:40:28.369 --> 00:40:31.900
We do not have the equivalent library in Fortran on TBB.

00:40:31.900 --> 00:40:39.300
It is based on templates where
as OpenMP is based on directives.

00:40:40.570 --> 00:40:47.140
But in both cases there both optimized for Intel platforms.

00:40:47.139 --> 00:40:56.299
But TBB is not limited to Intel compliers.

00:40:56.300 --> 00:41:03.550
You can use this with any compatible
complier, so you can use it with GCC as well.

00:41:03.550 --> 00:41:11.130
Where OpenMP the specification you know it depends
on whether the complier actually supports it.

00:41:11.130 --> 00:41:19.289
And then there are some developers that need cross OS
platform support, in fact Apple delivers Safari now

00:41:19.289 --> 00:41:27.949
on Windows and so OpenMP is supported on
all of Intel's compliers and so is TBB.

00:41:27.949 --> 00:41:35.949
So TBB is supported across the
OS, Windows, Linux and Mac OS X.

00:41:35.949 --> 00:41:40.980
You can actually download those
compliers and tools from Intel's website.

00:41:40.980 --> 00:41:43.679
I believe that most of them have like a 30 day trial period.

00:41:43.679 --> 00:41:47.449
I encourage you to actually go
ahead and download and try them.

00:41:47.449 --> 00:41:49.359
So what is TBB offer?

00:41:49.360 --> 00:41:52.610
It basically offers parallels constructs as well.

00:41:52.610 --> 00:41:59.000
It offers the parallel for and the parallel reduction
which we actually saw in operation with Mike's demo.

00:41:59.000 --> 00:42:08.000
But it also supports a parallel while so we no longer
have to have a fix to find termination index point

00:42:08.000 --> 00:42:12.639
for the parallel while implementation or paradigm.

00:42:12.639 --> 00:42:17.879
In OpenMP you had to define that last index and
it couldn't change, you couldn't change the value

00:42:17.880 --> 00:42:23.789
within the for loop itself, while, in the
parallel while you can actually make those changes,

00:42:23.789 --> 00:42:28.829
you can have that updated and operated
on within the parallel while.

00:42:28.829 --> 00:42:35.480
There is also support for parallel scan, sort
and then the concepts of liner pipelining

00:42:35.480 --> 00:42:38.900
so you can actually have multiple
threads that are pipelined and add

00:42:38.900 --> 00:42:45.950
and insert basically filter stages
that can be operated on the data.

00:42:47.510 --> 00:42:55.170
In addition to the parallel constructs its supports
container classes, so there is a lot of defined classes

00:42:55.170 --> 00:43:04.840
that allow you to have threaded safe implementations of
something like a hashmap vector or a queue class as well.

00:43:04.840 --> 00:43:13.670
TBB supports task space scheduling so
vary similar in so respects to Open MP

00:43:13.670 --> 00:43:16.840
where you could have the parallel sections.

00:43:16.840 --> 00:43:25.340
In this case you could define the task space
somewhat like the task Q in Apple's implementation

00:43:25.340 --> 00:43:31.600
and then you can define your own task
scheduler to operate on that, those tasks.

00:43:31.599 --> 00:43:37.610
It supports the synchronization both atomic
operation as well as mutual exclusion,

00:43:37.610 --> 00:43:40.940
there are several flavors of mutual
exclusions that are supported.

00:43:40.940 --> 00:43:49.760
There is the kind of the unfair mutual exclusion,
you try it, if you don't get it the your try again

00:43:49.760 --> 00:43:55.540
and maybe you get it but not necessarily fair your not,
just cause you tried it the first time does not mean

00:43:55.539 --> 00:44:00.289
that you are the first person that's going to
be able to get the, in there the second time.

00:44:00.289 --> 00:44:06.079
So there's queuing mutexes which allow you
actually queue up the request to the mutex

00:44:06.079 --> 00:44:10.880
so it's a more fair first come
first serve type of implementation.

00:44:10.880 --> 00:44:19.769
There's also read-write mutexes, queuing read-write as
well as spin versions of mutexes so when I was talking

00:44:19.769 --> 00:44:28.849
about OpenMP and the fact that the synchronization all
have spin waits to hopefully avoid context switches.

00:44:28.849 --> 00:44:33.099
You also have spin mutexes here as well in TBB.

00:44:33.099 --> 00:44:42.019
And then just like OpenMP there are memory allocation
support for thread allocation and timing routines.

00:44:42.019 --> 00:44:47.309
Now the benefit of theses, you know both of these
technologies is that because they are implemented either

00:44:47.309 --> 00:44:56.049
as a library or as a pragma or a set of library routines
across platforms, across OS's is that now

00:44:56.050 --> 00:45:01.740
if you implement it you get the
same functionality across the board.

00:45:01.739 --> 00:45:04.619
Doesn't matter which OS you happen to be running on.

00:45:04.619 --> 00:45:09.179
So let's do an example of a parallel for.

00:45:09.179 --> 00:45:15.619
This is the same loop that we had before
implemented but what we're going to find its going

00:45:15.619 --> 00:45:24.690
to be a little bit more complex to actually implement
TBB but hopefully by the end of this talk you'll see

00:45:24.690 --> 00:45:35.519
that the complexity that TBB adds to your development pays
off in the performance that you could also get out of TBB.

00:45:35.519 --> 00:45:38.019
So first of all you have to be
able to initialize the library,

00:45:38.019 --> 00:45:45.199
you do this by including essentially the task
scheduler init.h and then using the name space

00:45:45.199 --> 00:45:52.129
and then defining the task scheduler and that
initializes the library because it's a class bases object,

00:45:52.130 --> 00:46:00.849
oriented based library you actually get the destructor and
you can terminate the task scheduler earlier if you like

00:46:00.849 --> 00:46:05.449
but if you just exit out of the
main, it will clean up itself.

00:46:05.449 --> 00:46:14.929
And then what we have to do it's we have to basically
implement the function okay in terms of an operator

00:46:14.929 --> 00:46:19.669
that will operate on a variable range of tasks.

00:46:19.670 --> 00:46:30.230
So now before where we define the full set of the data
or the full routine was from I equals 0 to I less then N,

00:46:30.230 --> 00:46:38.750
we now have to kind of parameterize that so that the task
scheduler can now give that function or that routine kind

00:46:38.750 --> 00:46:40.809
of the indexes on where to start and where to end.

00:46:40.809 --> 00:46:44.639
And that can be dynamic and you'll
see why we actually do that

00:46:44.639 --> 00:46:50.869
So here's the code that we actually have to add,
we actually put it in a class form and then we have

00:46:50.869 --> 00:46:57.639
to implement the operator with kind of this
variable range class that goes along with it.

00:46:57.639 --> 00:47:09.159
And then we actually implement the range class and
so this is kind of right out of the TBB's format

00:47:09.159 --> 00:47:12.109
but this is actually implementing a custom range class.

00:47:12.110 --> 00:47:17.200
You could actually use the default that is included
in TBB and that would mirror very much kind

00:47:17.199 --> 00:47:23.169
of like what OpenMP does in terms of a dynamic scheduler.

00:47:23.170 --> 00:47:30.349
Okay were it basically the default is a
divisible will be you know its divisible by one

00:47:30.349 --> 00:47:39.239
and so it just gives you the next iteration that you are
actually going to have to operate on and that's the range.

00:47:39.239 --> 00:47:47.179
But in this case I specifically picked this particular
implementation because it allows me to customize how

00:47:47.179 --> 00:47:51.109
that gets divided, so I can change the beginning.

00:47:51.110 --> 00:47:54.230
I can change how we're actually dividing the workload.

00:47:54.230 --> 00:48:02.969
And so once we define the range class and we define
the function, now we can implement the parallel for.

00:48:02.969 --> 00:48:09.679
Somewhat similar techniques are applied to the parallel
while, parallel sort, parallel scan type constructs.

00:48:09.679 --> 00:48:15.169
But what makes this a little more different then
the OpenMP is because I was able to define it.

00:48:15.170 --> 00:48:23.139
Now you can see that there's a custom range and I waited
it in the implementation range of the custom class instead

00:48:23.139 --> 00:48:31.949
of it being equal, divided equally among the threads I
gave just two of the first two indexes to the first thread

00:48:31.949 --> 00:48:39.399
and then three indexes to the second thread, four
to the third and five, and that kind of help waited

00:48:39.400 --> 00:48:46.710
or you know did the load balancing my self based
on exactly knowing what the calculation was

00:48:46.710 --> 00:48:51.369
and some of the performance that I expected it to have.

00:48:51.369 --> 00:49:00.079
So what we are going to do is have Mike com up
and show the implementation of the demo using TBB

00:49:00.079 --> 00:49:03.509
and we'll look at the performance of that as well.

00:49:03.510 --> 00:49:07.600
( Pause in speaking. )

00:49:07.599 --> 00:49:10.460
>> Mike: So we have one last demo for
you guys that will be pretty quick.

00:49:10.460 --> 00:49:17.970
I'll first show you the TBB code that it took
to implement when I did OpenMP and pthreads.

00:49:19.510 --> 00:49:27.980
So I'll first show you the problem, the problem
is right here, well it's not really a problem.

00:49:27.980 --> 00:49:34.090
It's where we put the workload in a split it up,
similar to what Phil showed you in the slides.

00:49:34.090 --> 00:49:38.660
We basically called our update sub range for loop,

00:49:38.659 --> 00:49:44.819
we called and we have it set up
to take the blocked range class.

00:49:44.820 --> 00:49:52.670
And we call a parallel reduce right here, this
right here is similar to parallel for in TBB

00:49:52.670 --> 00:49:55.530
but it does the reduction as well like in OpenMP.

00:49:55.530 --> 00:50:01.000
And we get really pleasing results cause we didn't have
to do all the work we had with pthreads but we still had

00:50:01.000 --> 00:50:04.809
to do a little more then we did with OpenMP.

00:50:04.809 --> 00:50:17.829
So I showed all this already, I'll skip to the OpenMP
where we have the threaded getting decent results there.

00:50:17.829 --> 00:50:33.509
I'll switch to POSIX threads, we're getting about 241
updates a second, and it'll just take a second to propagate.

00:50:33.510 --> 00:50:46.680
We're getting about 248 updates with TBB or 242 it depends
just a little bit on what is going on in the screen.

00:50:46.679 --> 00:50:49.349
But overall we got really good results with TBB.

00:50:49.349 --> 00:50:57.230
We're not doing all the effort, a few threads it takes
care of, the task scheduling, how many threads we have

00:50:57.230 --> 00:51:00.909
and all that stuff for not that much effort.

00:51:00.909 --> 00:51:04.509
And I think Phil is going to end.

00:51:04.510 --> 00:51:12.040
( Pause in speaking. )

00:51:12.039 --> 00:51:12.509
>> Phil: Thank you, Mike.

00:51:12.510 --> 00:51:21.540
If we can go back to the slides, so again what
we saw was, was a little bit more effort in TBB,

00:51:21.539 --> 00:51:29.349
certainly understanding the class structure and the
implementation for TBB instead of the one or two lines

00:51:29.349 --> 00:51:36.599
of code to get 90 percent of the performance
it probably was another 10 or 20 lines of code.

00:51:36.599 --> 00:51:47.329
But we actually got slightly better
performance then the POSIX implementation.

00:51:47.329 --> 00:51:52.199
We actually didn't go back to try to analyze
clearly if you want it to work a little bit harder

00:51:52.199 --> 00:51:59.309
on the POSIX implementation you could certainly implement
and get equivalent or even better performance then TBB

00:51:59.309 --> 00:52:05.329
but the important part is that it's the improvement
in the additional productivity that you achieve

00:52:05.329 --> 00:52:12.519
from using these technologies, over doing the
implementation yourself and all the debugging that goes on.

00:52:12.519 --> 00:52:19.719
So TBB certainly offers more common
threading model options then OpenMP.

00:52:19.719 --> 00:52:24.730
There's still less debugging, more performance focus,

00:52:24.730 --> 00:52:30.949
we're still focused on the performance we can
implement custom type workload implementations

00:52:30.949 --> 00:52:35.269
and ranges to help balance out the workload.

00:52:35.269 --> 00:52:42.880
But there is a higher learning code then OpenMP and again
you can achieve the same performance or better with POSIX,

00:52:42.880 --> 00:52:45.950
it's just going to take a lot more effort.

00:52:45.949 --> 00:52:50.049
So clearly there's still software engineering

00:52:50.050 --> 00:52:55.130
that is required you know you have to
modify and think about the algorithm.

00:52:55.130 --> 00:53:00.550
The first example with the, the example
where we had to change the serial code

00:53:00.550 --> 00:53:05.650
to support better improved parallelism,
that part is still required.

00:53:05.650 --> 00:53:14.780
OpenMP, TBB or any other application or technology
is not going to remove that from the equation.

00:53:14.780 --> 00:53:21.250
You have to understand that data sharing and
synchronization that goes on, again OpenMP allows you

00:53:21.250 --> 00:53:26.909
to express it a little more cleanly and
concisely but it doesn't do it for you.

00:53:26.909 --> 00:53:36.149
You have to analyze the workload behavior and
balance that workload again if you are so focused

00:53:36.150 --> 00:53:42.230
on the POSIX implementation, you are spending a lot of
time debugging and figuring out exactly where your bugs are

00:53:42.230 --> 00:53:50.090
and not focused on the performance side, you know the
productivity is lost to that, so OpenMP and TBB allow you

00:53:50.090 --> 00:53:56.190
to focus more on that workload balancing behavior.

00:53:56.190 --> 00:54:00.590
And certainly you have to take into
account the hardware platform constraints.

00:54:00.590 --> 00:54:05.940
For example, one thing that we didn't
encounter in this particular demo is the fact

00:54:05.940 --> 00:54:10.360
that you know hardware had this certain cache line size.

00:54:10.360 --> 00:54:17.690
If you have data that you happen to share on the same
cache line, even though you may not be updating the data,

00:54:17.690 --> 00:54:19.740
or modifying the data or reading the data

00:54:19.739 --> 00:54:25.099
between two threads you can cause implicit
false sharing between those cache lines.

00:54:25.099 --> 00:54:31.319
That's a hardware platform issue that you have to be aware
of, OpenMP and TBB or any other technology is not going

00:54:31.320 --> 00:54:36.539
to help do that, if you laid out your data that way.

00:54:36.539 --> 00:54:43.019
So there's still a lot of effort that has to go in
but hopefully with something like OpenMP or TBB,

00:54:43.019 --> 00:54:52.259
the focus is actually on the threading part and so in
summary you know we really want to be aggressive in terms

00:54:52.260 --> 00:54:57.810
of threading the number of threads of execution are
going to increase, they have already, its accelerating.

00:54:57.809 --> 00:55:02.650
It's going to be harder and harder to implement threading.

00:55:03.719 --> 00:55:12.029
Open MP and TBB are alternative to improve your productivity
and your application performance clearly we have a demo

00:55:12.030 --> 00:55:16.340
that we showed where we actually
exceeded the initial POSIX implementation.

00:55:16.340 --> 00:55:21.470
Certainly they are all comparable in overall
performance but the level of effort and work required

00:55:21.469 --> 00:55:25.959
to achieve that was significantly different.

00:55:25.960 --> 00:55:31.269
And with greater productivity what we're really hoping
is that there is more opportunities for threading.

00:55:31.269 --> 00:55:38.030
So if you have for loops that are only taking 10 or
15 percent of your applications time maybe its time

00:55:38.030 --> 00:55:42.530
to start investigating those areas as
well for opportunities for threading.

00:55:42.530 --> 00:55:50.580
If you can add a pragma, OpenMP pragma statement to a four
loop and reduce 15 percent down to you know down to two,

00:55:50.579 --> 00:55:56.860
three, four percent you've already gained a 10 percent
improvement on your application for very little effort.