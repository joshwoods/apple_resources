WEBVTT

00:00:20.210 --> 00:00:21.519
Hi. My name is Chris Emura.

00:00:21.519 --> 00:00:26.219
I'm the Filesystem Engineering Manager
with an Apple's Core OS organization.

00:00:26.219 --> 00:00:30.779
And today we'll be talking a little about
Mac OS X filesystems and related technologies.

00:00:30.780 --> 00:00:35.370
This is obviously targeted to both halves as
far as the developers and the IT professionals.

00:00:35.369 --> 00:00:39.969
But there's really a little something in
here for everyone that's just an end user.

00:00:39.969 --> 00:00:43.539
Our agenda is pretty compact.

00:00:43.539 --> 00:00:47.509
We'll be doing a very quick overview.

00:00:47.509 --> 00:00:53.059
( Applause )

00:00:53.060 --> 00:00:56.910
I'll talk about some things that are specific
to our platform as far as, you know, Mac OS X-isms.

00:00:56.909 --> 00:00:58.239
What's changed since Tiger.

00:00:58.240 --> 00:00:59.620
What's new in Leopard.

00:00:59.619 --> 00:01:02.829
A few words on the ZFS developer preview.

00:01:02.829 --> 00:01:06.840
And then some time at the very end may
be 10, 15 minutes for Q&A. All right.

00:01:06.840 --> 00:01:09.510
Let's get started.

00:01:09.510 --> 00:01:10.340
So quick overview.

00:01:10.340 --> 00:01:15.650
In all the years we've done this session it kind of occurred
to me that we've never had a master slide that just kind

00:01:15.650 --> 00:01:17.960
of dumped out all the filesystems on our platform.

00:01:17.959 --> 00:01:20.890
And so, put something together really quickly.

00:01:20.890 --> 00:01:27.200
Divided into 3 basic buckets, the local filesystems,
the network file sharing protocols,

00:01:27.200 --> 00:01:30.240
both client and server, and then some third party mentions.

00:01:30.239 --> 00:01:33.890
XSAN is not really a local file
systems I had nowhere else to put it.

00:01:33.890 --> 00:01:37.310
There's going to be a number of sessions during
this week that talk about XSAN in much more details.

00:01:37.310 --> 00:01:42.120
So if you are interested in that technology, you
know, by all means please attend those sessions.

00:01:42.120 --> 00:01:48.920
On the local filesystem front, as most of you know, our
primary filesystem is going to be Journaled HFS+.

00:01:48.920 --> 00:01:52.640
We also have FAT 32, FAT 16, all the variance for a lot

00:01:52.640 --> 00:01:59.700
of the consumer-level devices whether they be your
camera, your flash cards, things of that nature.

00:01:59.700 --> 00:02:03.859
UDF, the Universal Disk Format, has undergone
some significant changes in Leopard.

00:02:03.859 --> 00:02:05.980
We have a dedicated slide for that.

00:02:05.980 --> 00:02:10.150
It's typically associated though with optical media.

00:02:10.150 --> 00:02:15.319
NTFS, some kind of under the radar, but it's
actually been completely rewritten in Leopard.

00:02:15.319 --> 00:02:20.680
Read-only support still at this point but watch this space.

00:02:20.680 --> 00:02:23.860
There's going to be some things happening in that area.

00:02:23.860 --> 00:02:29.640
ISO9660 and CDDAFS kind of round things off in optical media.

00:02:29.639 --> 00:02:32.549
On the file sharing side again, both client and server.

00:02:32.550 --> 00:02:36.500
We've got the Windows file sharing as far as SMB and CIFS.

00:02:36.500 --> 00:02:39.569
Those of you familiar on the UNIX
side, of course, NFS V2 and V3.

00:02:39.569 --> 00:02:46.909
And then for us on the internal side, the Apple
File Sharing Protocol which you'll probably see

00:02:46.909 --> 00:02:49.829
as a default for Mac-to-Mac communication.

00:02:49.830 --> 00:02:55.640
Third party, there's obviously many more third-party
products as far as filesystems and file sharing protocols.

00:02:55.639 --> 00:03:01.259
I just mentioned AFS and Amit Singh's MacFUSE because
it's just getting some great press recently.

00:03:01.259 --> 00:03:04.389
Good examples of technology on our platform.

00:03:04.389 --> 00:03:08.409
With all those filesystems you've got various capabilities.

00:03:08.409 --> 00:03:14.419
And some of them are pretty mundane as far as limits,
maximum file size, maximum filesystem size and so on.

00:03:14.419 --> 00:03:16.030
These are things you guys can look up on your own.

00:03:16.030 --> 00:03:18.659
We won't talk about them in any great detail here.

00:03:18.659 --> 00:03:22.500
When it comes to filesystem-specific fcntl()s or capabilities,

00:03:22.500 --> 00:03:25.979
things are pretty much documented in the fcntl() man page.

00:03:25.979 --> 00:03:31.239
These are things, of course, you should feel free to use,
but at the same time as developers, you know, the more --

00:03:31.240 --> 00:03:35.659
the deeper your dependencies are on these
type of specific capabilities, in general,

00:03:35.659 --> 00:03:37.180
the less portable your code's going to be.

00:03:37.180 --> 00:03:40.629
So just take care in those situations.

00:03:40.629 --> 00:03:44.019
Crash protection is not something you're
going to code against necessarily.

00:03:44.020 --> 00:03:45.920
But just as an end user something to be aware of.

00:03:45.919 --> 00:03:49.559
In terms of our local filesystems,
the only local filesystem

00:03:49.560 --> 00:03:53.719
with formal crash protection is going
to be Journaled HFS, of course.

00:03:53.719 --> 00:03:57.750
That said on the FAT side of things
we've gone through great lengths

00:03:57.750 --> 00:04:02.240
to balance crash protection as
far as robustness and performance.

00:04:02.240 --> 00:04:07.420
And so with our FAT implementation I think you'll see
we've headed quite nicely in terms of finding that mix

00:04:07.419 --> 00:04:13.699
or that balance between being able to survive
plug pulls and so on versus just raw performance.

00:04:13.699 --> 00:04:19.870
Going on with the various capabilities I wanted
to call this one out specifically this year.

00:04:19.870 --> 00:04:26.590
Sparse file support, we don't have native sparse
file support on our primary filesystem HFS.

00:04:26.589 --> 00:04:30.009
So you will pay that zero fill "cost".

00:04:30.009 --> 00:04:32.110
If that's going to be a problem
for any of you as developers,

00:04:32.110 --> 00:04:35.569
come talk to us either after this
session or during the lab tomorrow.

00:04:35.569 --> 00:04:38.959
There are definitely a number of tricks
or workarounds we've used in the past

00:04:38.959 --> 00:04:43.289
that have made other vendors happy with this limitation.

00:04:45.310 --> 00:04:46.519
Case sensitivity.

00:04:46.519 --> 00:04:50.539
This one almost never gets talked
about, at least in any kind of detail.

00:04:50.540 --> 00:04:56.390
For starters we offer case sensitive HFS as
a first-class filesystem on our platform.

00:04:56.389 --> 00:05:01.579
Something we tend to steer people towards as
far as those looking for case sensitivity.

00:05:01.579 --> 00:05:03.769
But as developers a lot of time we do C-code.

00:05:03.769 --> 00:05:11.479
It's internal too. We make the same mistakes as far as people
hard coding case incorrectness into their actual, their code.

00:05:11.480 --> 00:05:15.810
They have to hard code info.plist, when,
in fact, it's a capital I, for example.

00:05:15.810 --> 00:05:21.300
So they get away with it on their primary filesystem
a lot of times when there's case insensitivity.

00:05:21.300 --> 00:05:25.069
But in the case sensitive side these type of
look ups, these types of operations will fail.

00:05:25.069 --> 00:05:32.430
So the general message here is, as developers, please,
test your code on a case-sensitive format as well.

00:05:32.430 --> 00:05:37.319
Another subtle point involves people that come
from other platforms often times with tarballs

00:05:37.319 --> 00:05:39.490
for their home directories, whatever it may be.

00:05:39.490 --> 00:05:42.449
And they come from let's say UFS
or something along those lines.

00:05:42.449 --> 00:05:46.909
They are basically going from a case-
sensitive format to a case-insensitive one.

00:05:46.910 --> 00:05:51.780
And with something like tar, it's very easy to kind
of lose some data here in terms of having two files

00:05:51.779 --> 00:05:54.359
that basically collide that only differ in case.

00:05:54.360 --> 00:05:59.230
And on the case-insensitive filesystem
you'll overwrite one.

00:05:59.230 --> 00:06:04.120
And that can be silent for a while and won't
notice until you start poking around.

00:06:04.120 --> 00:06:06.769
So just something to be mindful of.

00:06:08.040 --> 00:06:09.840
Access control lists.

00:06:09.839 --> 00:06:13.569
This is very prominent now on our platform.

00:06:13.569 --> 00:06:17.610
Whenever I talk about access control lists
or ACLs I like to start off by comparing it

00:06:17.610 --> 00:06:21.550
with the old-school UNIX user ID and mode_t bits.

00:06:21.550 --> 00:06:25.520
I think everyone in this room is probably pretty
familiar with this as far as the read-write execute bits

00:06:25.519 --> 00:06:29.849
for yourself, groups you belong to,
and pretty much the rest of the world.

00:06:29.850 --> 00:06:34.720
There's other limitations, of course,
with regards to 16 groups and no nesting.

00:06:34.720 --> 00:06:40.850
But, you know, this has served most of us pretty well
as far as being a very simple, easy-to-understand model.

00:06:40.850 --> 00:06:45.370
That said, there's a lot of times when you do
want to express yourself in terms of permissions,

00:06:45.370 --> 00:06:49.709
a little bit finer granularity,
and with that, again, the access control lists.

00:06:49.709 --> 00:06:56.659
I've highlighted a couple here I feel that are pretty much
difficult or impossible to express the traditional mode_t.

00:06:56.660 --> 00:07:00.530
As far as appending to a file or changing
the security information of a file.

00:07:00.529 --> 00:07:05.229
And you'll note for everyone of these entries here
there's an allow or deny flag.

00:07:05.230 --> 00:07:07.629
So the list itself is actually pretty long.

00:07:07.629 --> 00:07:12.819
You can kind of see how much finer granularity you have,
how much expressive power you have with regard

00:07:12.819 --> 00:07:17.469
to permissions with access control lists.

00:07:17.470 --> 00:07:20.360
Extended Attributes.

00:07:20.360 --> 00:07:23.699
I should kind of start of describing what they are.

00:07:23.699 --> 00:07:28.089
As most you know, extended attributes are
simply arbitrary bits of data that don't live

00:07:28.089 --> 00:07:31.139
in a natural file itself--they don't
live in the data fork, if you will.

00:07:31.139 --> 00:07:35.430
They can be quite useful in situations
where you are working with the file format.

00:07:35.430 --> 00:07:38.829
That's got no allowance for tucking away a little bit

00:07:38.829 --> 00:07:42.459
of extended metadata, you can just
go ahead and use the EAs there.

00:07:42.459 --> 00:07:46.449
On our files, or on our platform, they are
going to be supported in all filesystems.

00:07:46.449 --> 00:07:49.990
I think at first that sounds a bit
odd but I'll explain a little further.

00:07:49.990 --> 00:07:55.120
Obviously for cases where the filesystem supports
EAs natively, we will use the native method.

00:07:55.120 --> 00:07:56.780
That's pretty straight forward.

00:07:56.779 --> 00:08:02.189
In situations where the filesystem does not support EAs
natively, we will synthesize support at the VFS level

00:08:02.189 --> 00:08:05.719
and store the actual EAs in what's
called an Apple Double File.

00:08:05.720 --> 00:08:08.710
I'll talk about that a little bit more later.

00:08:08.709 --> 00:08:12.000
The convention we use for the actual
EA tags is pretty simple.

00:08:12.000 --> 00:08:16.769
We just use the reverse-DNS name
convention, the FinderInfo is listed here.

00:08:16.769 --> 00:08:19.039
We don't actually enforce this,
but it's something that, you know,

00:08:19.040 --> 00:08:23.050
we encourage you using that keeps
things nice and clean, very manageable.

00:08:23.050 --> 00:08:26.360
And there's a good past precedent for this as well.

00:08:26.360 --> 00:08:28.759
Also want to call out current limitations.

00:08:28.759 --> 00:08:32.519
The main one I'm talking about today is size.

00:08:32.519 --> 00:08:38.889
The current lowest common denominator of size
per EA is roughly just under 4K, about 3803.

00:08:38.889 --> 00:08:41.309
I can't remember the exact number but it's around there.

00:08:41.309 --> 00:08:44.509
This is really something we should put
in some kind of volume capability bit

00:08:44.509 --> 00:08:46.689
and advertise a little bit more strongly.

00:08:46.690 --> 00:08:51.720
But at this point just be aware that the current
lowest common denominator is in this neighborhood.

00:08:51.720 --> 00:08:56.110
I call out current and I don't know if
any of your browsed the open source code.

00:08:56.110 --> 00:09:00.820
We do have some work in place for extent-
based EAs that hasn't been turned on just yet.

00:09:00.820 --> 00:09:06.110
But just so you know this is something, you know,
that could be lifted at some point in the future.

00:09:06.110 --> 00:09:10.340
Preservation when it comes to extended
metadata or EAs in general.

00:09:10.340 --> 00:09:16.399
Definitely something you want to be mindful of even if you
don't use EAs directly, it's very, very easy for you guys

00:09:16.399 --> 00:09:19.429
to strip off -- just as end users, just to strip off EAs.

00:09:19.429 --> 00:09:25.529
Doing very basic operations, operations with
utilities or applications that may not be EA aware.

00:09:25.529 --> 00:09:28.319
And so, you know, again something for you to be mindful of.

00:09:28.320 --> 00:09:30.800
I'll talk about some alternate methods to do copies.

00:09:30.799 --> 00:09:32.049
We don't have to worry about this.

00:09:32.049 --> 00:09:35.419
But if for some reason you have
to roll your own copy engine,

00:09:35.419 --> 00:09:39.679
your own copy functionality do moves or some kind of archive.

00:09:39.679 --> 00:09:43.370
When it comes to preservation we have an
internal monitor that focuses on three things.

00:09:43.370 --> 00:09:45.190
Always the same three things.

00:09:45.190 --> 00:09:50.510
It's the data, of course, I mean the data itself, the extended
attributes, including the resource fork which shows

00:09:50.509 --> 00:09:53.929
up in the EA name space, and then
the security information itself.

00:09:53.929 --> 00:09:57.859
Each one of these buckets or these categories
has its own corresponding API

00:09:57.860 --> 00:10:00.810
or family of APIs to manipulate this data.

00:10:00.809 --> 00:10:04.769
So if you have to roll your own, those are
the three things you need to focus on.

00:10:04.769 --> 00:10:09.100
This is a slide we actually use every year.

00:10:09.100 --> 00:10:09.840
It's a good one.

00:10:09.840 --> 00:10:14.060
It kind of gives you the lay of the land of
the various frameworks and how they fit in.

00:10:14.059 --> 00:10:21.539
Those of you coming from other flavors of UNIX you'll be
very familiar with the BSD or the POSIX-level API.

00:10:21.539 --> 00:10:24.089
You know, it's again, it's pretty much the same thing.

00:10:24.090 --> 00:10:29.700
The problem or the limitation with the BSD API, is that
they don't show the limitations at the various filesystems,

00:10:29.700 --> 00:10:34.370
they don't distinguish much between the two as far
as -- and everything will be exposed at that level.

00:10:34.370 --> 00:10:36.250
So as a general rule,

00:10:36.250 --> 00:10:41.659
we ask people to kind of move up in terms of using the
Carbon file manager or the Core Services file manager

00:10:41.659 --> 00:10:46.629
and the NSFileManager to kind of protect you from
the differences between the various filesystems.

00:10:46.629 --> 00:10:50.929
Just a general suggestion as far as not
having to worry about my new show or any kind

00:10:50.929 --> 00:10:56.209
of specific details behind each different
filesystem and its capabilities.

00:10:56.210 --> 00:11:01.639
Okay. Let's switch gears a little bit and
talk about things that are more or less unique

00:11:01.639 --> 00:11:05.819
to our platform, especially useful to those of you
who are new to our platform and kind of familiar

00:11:05.820 --> 00:11:09.200
with the LINUX side, or the Solaris side or
whatever it may be.

00:11:09.200 --> 00:11:12.310
First one is those Apple Double Files.

00:11:12.309 --> 00:11:15.059
This always gets an incredible amount of attention.

00:11:15.059 --> 00:11:19.469
There's all kind of different terms for this.

00:11:19.470 --> 00:11:25.500
As I mentioned earlier they are use to back these extended
attributes in cases where native support isn't available.

00:11:25.500 --> 00:11:30.000
So you would think in the future as, you know, you
look at future filesystems and future protocols

00:11:30.000 --> 00:11:32.039
in general most of those come with native supports.

00:11:32.039 --> 00:11:37.329
So the idea hopefully is that in the future these
Apple Double Files will become more and more rare.

00:11:37.330 --> 00:11:40.280
There's legacy usage withstanding, of course.

00:11:40.279 --> 00:11:45.720
The convention is pretty simple for any given file
foo you have a corresponding ._ file

00:11:45.720 --> 00:11:50.590
which again backs all of those EAs and so on.

00:11:50.590 --> 00:11:54.649
When you use the upper level APIs as I mentioned
earlier, it tends to abstract away things.

00:11:54.649 --> 00:11:58.159
These ._ files, these Apple Double Files
will not show up in that particular --

00:11:58.159 --> 00:11:59.850
they won't show up at all in the native space.

00:11:59.850 --> 00:12:02.149
And so you won't be aware of them.

00:12:02.149 --> 00:12:07.490
For better or for worse, my opinion, these things should be
actually taken out completely as far as the name space.

00:12:07.490 --> 00:12:15.340
But at the BSD level, these things are exposed not just at
the API level but also command line, things like find or LS.

00:12:15.340 --> 00:12:17.269
You will see these ._ files.

00:12:17.269 --> 00:12:21.029
So please resist the temptation to
operate on these things directly.

00:12:21.029 --> 00:12:23.259
You should never have to do that.

00:12:23.259 --> 00:12:28.710
The kernel manages these things completely as
far as creation, renames, and unlinks, or deletes.

00:12:28.710 --> 00:12:32.090
And so there really isn't any need
for you to manipulate these ._

00:12:32.090 --> 00:12:35.450
files, these Apple Double Files directly.

00:12:35.450 --> 00:12:42.000
getattrlist(). What can I say?

00:12:42.000 --> 00:12:45.970
For those of you coming from the UNIX side
it sure don't smell like a UNIX function.

00:12:45.970 --> 00:12:47.379
But it is a full fledged VNOP.

00:12:47.379 --> 00:12:53.059
It's a fancy way of saying it's here to stay,
it's a first-class function on our platform.

00:12:53.059 --> 00:12:57.479
The best way to describe it for those of you on the
UNIX side is this is pretty much a superset of stat().

00:12:57.480 --> 00:13:01.039
You get attributes of a file,
everything that stat provides and much more.

00:13:01.039 --> 00:13:06.579
Those of you from the Carbon side of the house, this is
pretty much how FSGetCatalogInfo() is implemented.

00:13:06.580 --> 00:13:10.560
So if you are familiar with that call,
you'll be familiar with this mechanism.

00:13:10.559 --> 00:13:17.189
It does have, in my opinion at least, one kind of
really nice quality as far as this selection bitmap.

00:13:17.190 --> 00:13:21.250
And I think the best way to describe that is to
compare it to something like stat which operates

00:13:21.250 --> 00:13:24.740
with this fixed struct stat buffer, if you will.

00:13:24.740 --> 00:13:29.019
With stat, as you know, you get everything whether or not
you need it or not, you are going to get everything,

00:13:29.019 --> 00:13:31.000
you're going to have to pay for everything.

00:13:31.000 --> 00:13:37.259
With the selection bitmap and getattrlist() you can only specify
the attributes you want and only pay for the ones you want.

00:13:37.259 --> 00:13:41.059
So right off the bat there is a nice
semantic there with regard to costs.

00:13:41.059 --> 00:13:46.119
Especially in the future if you see some of these
attributes costing much more than others in a lot of cases

00:13:46.120 --> 00:13:48.379
where you don't really need them or don't care.

00:13:48.379 --> 00:13:52.570
The other nice property with this selection
bitmap and getattrlist() is extensibility.

00:13:52.570 --> 00:13:56.910
I think it's pretty obvious in terms of a well-
define structure like stat in general and you have

00:13:56.909 --> 00:14:00.559
to add new fields or may be blow out the width of a field,

00:14:00.559 --> 00:14:05.439
the inode 64-bit from 32-bit is a classic example.

00:14:05.440 --> 00:14:10.710
With getattrlist() you at least have the option with the
selection bitmap to define a new field or nview type

00:14:10.710 --> 00:14:15.540
and not have to worry as much about
extensibility and backwards compatibility.

00:14:15.539 --> 00:14:20.750
MNT_IGNORE_OWNERSHIP.

00:14:22.070 --> 00:14:28.870
This is a much higher level model or construct here.

00:14:28.870 --> 00:14:30.909
I'll start with this motivation.

00:14:30.909 --> 00:14:33.059
Motivation is actually pretty simple.

00:14:33.059 --> 00:14:35.909
I think a lot of you guys in this
room can't or don't relate as much

00:14:35.909 --> 00:14:37.889
because you can figure out other ways to get around it.

00:14:37.889 --> 00:14:43.750
But motivation is such that you have two computers
or two users, both with completely different UIDs,

00:14:43.750 --> 00:14:46.220
different UID spaces, if you will, of directly services.

00:14:46.220 --> 00:14:48.050
They are not bound to the same directory service.

00:14:48.049 --> 00:14:49.169
This is completely different.

00:14:49.169 --> 00:14:55.019
And you want to move or share files from one to
the other with say an external FireWire drive.

00:14:55.019 --> 00:15:01.079
So we have Joe here, user ID 501, Sam
on another machine, UID 502.

00:15:01.080 --> 00:15:07.900
All those files there are kind of laid out with Joe's
UID, brings it over to Sam's computer, and again for a lot

00:15:07.899 --> 00:15:11.480
of less technical users, on the read-
only side you are typically okay

00:15:11.480 --> 00:15:16.279
but, you know, even then there can be some problems
just because Sam owns none of those files, you know,

00:15:16.279 --> 00:15:19.299
with the traditional kind of UNIX
model, if you will, for permissions.

00:15:19.299 --> 00:15:23.339
This can be a little bit overwhelming
for a less technical user like Sam.

00:15:23.340 --> 00:15:27.530
So by default we go ahead and mount
all those external drives

00:15:27.529 --> 00:15:30.740
with this nondurable state, this
MNT_IGNORE_OWNERSHIP flag.

00:15:30.740 --> 00:15:32.529
It's intended to be transparent.

00:15:32.529 --> 00:15:37.620
But the bottom line here is it's pretty
self explanatory in terms of all these processes

00:15:37.620 --> 00:15:41.710
on Sam's machine, any nonroot process
will see their own UID when looking

00:15:41.710 --> 00:15:44.530
at the directory ownership or file ownership of those files.

00:15:44.529 --> 00:15:51.240
So even though Joe's UID is what's on disk, whenever Sam
does an LS or takes a look at those files, he will see,

00:15:51.240 --> 00:15:53.340
you know, those files as being owned by him.

00:15:53.340 --> 00:15:56.790
And so clearly on the read-only side you are
really good, I mean there's no problems at all.

00:15:56.789 --> 00:15:59.189
It's as if he owns those files.

00:15:59.190 --> 00:16:05.410
On the write side so long as you are in this mode,
so long as you are mounting in this mode regardless

00:16:05.409 --> 00:16:10.469
of what your UID is, anything you
write out to disk will be UID 99.

00:16:10.470 --> 00:16:14.480
So you don't -- better way to phrase
this -- but you won't pollute, you know,

00:16:14.480 --> 00:16:18.750
the name space when you bring it back into Joe's computer.

00:16:18.750 --> 00:16:20.899
UID 99 is kind of a wild card here.

00:16:20.899 --> 00:16:25.529
Any time we see it on disk it will be
interpreted as the calling process UID.

00:16:25.529 --> 00:16:29.000
So that's how these things kind of work out if you will.

00:16:29.000 --> 00:16:30.669
It's not a perfect solution.

00:16:30.669 --> 00:16:35.689
And for those of you thinking about security
implications about how I can possibly tell the difference

00:16:35.690 --> 00:16:39.460
between UID 99 versus the real UID so to speak.

00:16:39.460 --> 00:16:45.389
Root processes will always see the truth and that's
kind of how we mitigate any exposure in that area.

00:16:45.389 --> 00:16:49.159
Just log in as root and take a look.
You should see the truth there.

00:16:49.159 --> 00:16:53.600
The Finder's Get Info panel, by the way, also kind
of lets you know what state you are in.

00:16:53.600 --> 00:17:00.350
If you look at the Ignore ownership on this volume check
box in the Sharing pane, this corresponds directly

00:17:00.350 --> 00:17:03.670
to whether or not that flag is set or clear.

00:17:05.599 --> 00:17:07.659
volfs. Okay.

00:17:07.660 --> 00:17:10.890
It's got a pretty clear message.

00:17:10.890 --> 00:17:14.770
We have to give this message every year but I
wanted to go a little bit more into what it actually is.

00:17:14.769 --> 00:17:19.799
I'll explain what it is and I am not going to debate
the merits of look up by path vs. look up by tuple

00:17:19.799 --> 00:17:22.329
or look up by some kind of inode number.

00:17:22.329 --> 00:17:27.480
You know the message is the same year after year,
please do not depend on volfs directly.

00:17:27.480 --> 00:17:29.750
Please do not depend on volfs directly.

00:17:29.750 --> 00:17:32.759
Let's kind of go into what it is.

00:17:32.759 --> 00:17:38.289
It's pretty simple. volfs is a filesystem
that implements an alternate look up method.

00:17:38.289 --> 00:17:42.149
I mean you guys are all familiar with regular
paths whether they be absolute or relative.

00:17:42.150 --> 00:17:45.130
I've got an example here where I just cat foo.text.

00:17:45.130 --> 00:17:47.340
Very simple look up method if you will.

00:17:47.339 --> 00:17:52.000
volfs simply implements this alternate look up
method where you can pass in a unique tuple.

00:17:52.000 --> 00:17:55.180
In this case some kind of FS ID and then some file ID,

00:17:55.180 --> 00:17:59.890
or an inode number. It's an alternate method
to get to that particular file.

00:17:59.890 --> 00:18:03.650
That's all it is, as far as mechanism
was implemented as a filesystem.

00:18:03.650 --> 00:18:07.490
We just ask that you do not use this directly, that's all.

00:18:10.259 --> 00:18:12.529
Okay. Let's change gears again really quickly.

00:18:12.529 --> 00:18:17.250
What's changed or what's new in Leopard?

00:18:17.250 --> 00:18:18.190
Start off with a copy engine.

00:18:18.190 --> 00:18:20.690
FSCopyObject. As far as this API,

00:18:20.690 --> 00:18:24.650
it's easily been one of the most requested
features, very robust in terms of being able

00:18:24.650 --> 00:18:29.840
to preflight high performance, cross volume
format so you don't have to be aware

00:18:29.839 --> 00:18:32.619
of what filesystem you are copying from and to.

00:18:32.619 --> 00:18:34.619
And it is the same engine that the Finder uses.

00:18:34.619 --> 00:18:35.899
So you know it's quite robust.

00:18:35.900 --> 00:18:38.580
It's got quite a bit of soak time if you will
and exposure.

00:18:38.579 --> 00:18:40.970
It's quite well tested, quite well used.

00:18:40.970 --> 00:18:45.210
And, of course, it takes care of all of those
things about the preservation I mentioned earlier.

00:18:45.210 --> 00:18:47.799
The data, the EAs, and the security information.

00:18:47.799 --> 00:18:52.119
You don't have to familiarize yourself with
the APIs that are required to copy the EAs

00:18:52.119 --> 00:18:53.909
and security information. This will do it for you.

00:18:53.910 --> 00:18:56.480
So it's something we recommend you use.

00:18:56.480 --> 00:18:59.549
You see the synchronous/asynchronous versions of the call below.

00:18:59.549 --> 00:19:02.419
It's something that you can look up
on the ADC site for sample code.

00:19:02.420 --> 00:19:04.570
We did talk about this in more detail last year.

00:19:04.569 --> 00:19:10.839
So go ahead and refresh yourselves
by looking up that reference.

00:19:10.839 --> 00:19:11.649
Safe saves.

00:19:11.650 --> 00:19:15.970
The mechanism itself certainly isn't new, but
this year given some feedback from last year,

00:19:15.970 --> 00:19:18.059
I kind of wanted to talk about what safe saves were.

00:19:18.059 --> 00:19:20.099
I think all of you guys know what this is in general.

00:19:20.099 --> 00:19:26.139
Terminology may be different but all safe saves are, are a
way to guarantee a very nice semantic in terms of --

00:19:26.140 --> 00:19:30.000
you want to modify a document and you
want it to only end up in one of two states.

00:19:30.000 --> 00:19:33.720
Either the original document is untouched
or all of your changes have made it over.

00:19:33.720 --> 00:19:34.600
All are nothing, right.

00:19:34.599 --> 00:19:37.629
Never a situation where the document was half modified.

00:19:37.630 --> 00:19:38.730
Something along those lines.

00:19:38.730 --> 00:19:43.500
And so with any safe save mechanism we start
off with the original document or file,

00:19:43.500 --> 00:19:47.809
we go ahead and make a complete copy, and
you only work and modify on that file.

00:19:47.809 --> 00:19:51.769
When you are all done you go ahead and close,
you sync and you check for errors. of course.

00:19:51.769 --> 00:19:57.119
And then you go ahead and do an atomic rename, or
exchangedata, the HFS case and you get your semantic.

00:19:57.119 --> 00:20:01.159
You'll notice that during that entire flow
there's no point where I can introduce a fault

00:20:01.160 --> 00:20:03.140
and then be left in an inconsistent state.

00:20:03.140 --> 00:20:05.280
Either all the changes have made
it over or none of them at all.

00:20:05.279 --> 00:20:08.849
And that's pretty much what the safe save mechanism is.

00:20:08.849 --> 00:20:14.250
New in Leopard we have the ReplaceObject API
that implements this as far as safe save.

00:20:14.250 --> 00:20:20.259
And just like with the copy engine there's no need for you
to kind of be aware of what API or what calls I should use

00:20:20.259 --> 00:20:23.930
for the preservation exchange data versus rename.

00:20:23.930 --> 00:20:28.170
It's a port across all filesystems and it does
the right thing with regard to preservation.

00:20:28.170 --> 00:20:32.630
You may have already seen it in use
by NSDocument as far as that class.

00:20:32.630 --> 00:20:35.420
You'll see the temporary subdirectory there.

00:20:35.420 --> 00:20:39.310
If you need more information just like last year
this is talked about in a little more detail,

00:20:39.309 --> 00:20:45.769
look at the AppKit Release Notes or go ahead and grap
around for the API there on the ADC website. Take a look

00:20:45.769 --> 00:20:51.730
at that and download that, play with it a bit.

00:20:51.730 --> 00:20:54.400
Oh, there's that VolFS again.

00:20:54.400 --> 00:20:56.910
So this year in Leopard no more volfs--period.

00:20:56.910 --> 00:20:59.190
As far as the filesystem itself
has been completely removed.

00:20:59.190 --> 00:21:04.160
Of course, none of you were depending on it in
the first place, so this should not be a problem.

00:21:04.160 --> 00:21:08.110
But seriously speaking you won't
see them in the filesystem listing.

00:21:08.109 --> 00:21:12.129
There was a pragmatic side as well
for those of you tools like FS usage.

00:21:12.130 --> 00:21:16.410
I know I get a lot of hate mail about how
annoying it is to look at volfs paths all day.

00:21:16.410 --> 00:21:19.060
You'll finally see full blown paths in traces like this

00:21:19.059 --> 00:21:21.539
which is a little bit more readable,
a little bit more helpful.

00:21:21.539 --> 00:21:24.309
So there's a pragmatic side as well.

00:21:24.309 --> 00:21:27.349
So no more volfs at least the filesystem.

00:21:29.029 --> 00:21:33.609
Deprecation of UFS is a phased deprecation.

00:21:33.609 --> 00:21:37.359
Starting with Leopard you won't be able to
install on a UFS root and you won't be able

00:21:37.359 --> 00:21:41.729
to create a UFS filesystem at least not from the GUI.

00:21:41.730 --> 00:21:47.400
Eventually the end game here is to get to a
point where we only support read-only UFS.

00:21:47.400 --> 00:21:51.700
Most of the people, seriously, you
know, this is nothing personal.

00:21:51.700 --> 00:21:55.269
Most people that basically use UFS are
really after the case sensitivity, at least

00:21:55.269 --> 00:21:57.180
in my experience wherever I ask this question.

00:21:57.180 --> 00:21:59.730
So again for case sensitivity if that's what you are

00:21:59.730 --> 00:22:06.259
after right now, please go ahead and
switch over to case-sensitive HFS.

00:22:07.529 --> 00:22:12.109
UDF. Universal disk format from
OSTA that's an open filesystem.

00:22:12.109 --> 00:22:16.039
Like I said earlier this is completely rewritten in Leopard.

00:22:16.039 --> 00:22:18.059
And we've got read/write support.

00:22:18.059 --> 00:22:20.779
Burning is always going to be
there as far as the optical side.

00:22:20.779 --> 00:22:25.779
I think the nice change now with UDF
is that we fully support block storage devices,

00:22:25.779 --> 00:22:33.700
block addressable devices, whether they be your flash thumb
drive, CF/SD card, or even just a portable USB drive.

00:22:33.700 --> 00:22:36.039
You'll see that Vista's got corresponding support as well.

00:22:36.039 --> 00:22:38.430
Kind of moving in that direction as much.

00:22:38.430 --> 00:22:42.060
It's not going to be a replacement
for FAT overnight that's for sure.

00:22:42.059 --> 00:22:45.490
I mean FATs pretty much the de facto
standard for this kind of operability.

00:22:45.490 --> 00:22:51.059
But the device vendors are very much aware of this as
far as the consumer guys and other vendors, you know,

00:22:51.059 --> 00:22:54.819
UNIX and ourselves and Microsoft are
kind of bumping up their support of UDF.

00:22:54.819 --> 00:22:58.789
So something to be aware of, something
to, in my opinion, be excited about.

00:22:58.789 --> 00:23:03.440
It doesn't have the same limitations as
far as metadata and sizes that FAT does.

00:23:03.440 --> 00:23:06.120
So it's, you know, there's some potential here.

00:23:09.279 --> 00:23:10.799
Enhancements to ls.

00:23:10.799 --> 00:23:15.980
The hang around on the command line or the
shell--you always, I think, are already familiar

00:23:15.980 --> 00:23:19.630
with the -E option for ACL or the access control lists.

00:23:19.630 --> 00:23:23.000
You'll see that little + sign next
to the mode_t bit which lets you know

00:23:23.000 --> 00:23:25.990
that this file has at least one access control entry.

00:23:25.990 --> 00:23:29.240
When you use the -L option in
conjunction, you get the full listing.

00:23:29.240 --> 00:23:35.099
Similarly with extended attributes if I only have a main
stream command at least in the command line, right,

00:23:35.099 --> 00:23:37.459
to give you the same type of information for EAs,

00:23:37.460 --> 00:23:41.950
you have a little @ sign here next to
the mode_t bits which lets you know there's

00:23:41.950 --> 00:23:44.630
at least one extended attribute for this file.

00:23:44.630 --> 00:23:48.670
And when used the conjunction with the
-L option you go ahead and see everything

00:23:48.670 --> 00:23:51.009
as far as the tags and the size and bytes.

00:23:51.009 --> 00:23:55.859
And you can see that the tags conform to that
reverse-DNS convention I mentioned earlier.

00:23:55.859 --> 00:24:02.209
Access control lists, you know, certainly not new in Leopard

00:24:02.210 --> 00:24:05.829
but what's changing here is that
they are on by default for one.

00:24:05.829 --> 00:24:06.819
Got no choice at this point.

00:24:06.819 --> 00:24:10.470
They are on by default and they are
actually used in a very mainstream way.

00:24:10.470 --> 00:24:14.509
If you look at this listing here this is
basically your default home directory.

00:24:14.509 --> 00:24:18.400
And you'll see there are a bunch of access
control entries for the well-known folders.

00:24:18.400 --> 00:24:21.720
The everyone deny delete I believe, yes.

00:24:21.720 --> 00:24:26.500
This is basically the "do not shoot
yourself in the foot" access control entry.

00:24:26.500 --> 00:24:30.400
Of course, you can go ahead and remove that explicitly
and go ahead and remove the directory if you need to.

00:24:30.400 --> 00:24:34.000
But it's a nice level of protection,
a little safety net there.

00:24:34.000 --> 00:24:36.420
That's the ls listing from the shell.

00:24:36.420 --> 00:24:41.940
The Finders Get Info panel's been modified and updated
to kind of give you the same information in terms

00:24:41.940 --> 00:24:45.110
of viewing the access control lists and also modifying them.

00:24:45.109 --> 00:24:49.439
So something to check out and start seeing a lot
more usage here in terms of access control lists.

00:24:49.440 --> 00:24:55.390
You'll be able do the same thing with your applications in
terms of sharing or providing these kind of safety nets here.

00:24:57.559 --> 00:24:59.159
FSEvents.

00:24:59.160 --> 00:25:04.130
This is our filesystem event notification
or change notification framework.

00:25:04.130 --> 00:25:06.530
It's a nice, it's a really nice
framework in terms of being very simple.

00:25:06.529 --> 00:25:08.440
It's the same thing -- not the same thing.

00:25:08.440 --> 00:25:13.240
It's what underlies Spotlight so it's quite powerful,
yet it's all user land unlike kauth

00:25:13.240 --> 00:25:17.230
which is also quite powerful but
much more difficult to program too.

00:25:17.230 --> 00:25:20.750
So simple user land file change notification mechanism.

00:25:20.750 --> 00:25:22.700
We actually talked about a little bit last year.

00:25:22.700 --> 00:25:24.259
Kind of at the same session.

00:25:24.259 --> 00:25:28.349
This year we have a dedicated session for
this that goes over this in much more detail.

00:25:28.349 --> 00:25:30.469
Provides that same watcher sample code.

00:25:30.470 --> 00:25:33.049
It's a good one to go to tomorrow at 3:30.

00:25:33.049 --> 00:25:38.329
A lot of times I get questions about
how is this different than kqueues?

00:25:38.329 --> 00:25:43.579
For starters it's certainly much more
sophisticated than kqueues while being able to use

00:25:43.579 --> 00:25:49.699
but with kqueues the most succinct example I can give is if
you are just going to monitor one file or just a few files

00:25:49.700 --> 00:25:54.279
or may be a directory, and just its immediate
children, kqueue is usually pretty nice in that context.

00:25:54.279 --> 00:25:55.950
Kind of what it's designed for.

00:25:55.950 --> 00:25:59.600
But if you want to monitor an entire directory
hierarchy all the way down to the leaves,

00:25:59.599 --> 00:26:03.000
changes anywhere in that hierarchy
kqueue is not going to be adequate

00:26:03.000 --> 00:26:05.410
and you should really be using something like FSEvents.

00:26:05.410 --> 00:26:11.070
So if you want to learn more about that in much
more detail, please attend that session tomorrow.

00:26:11.069 --> 00:26:16.929
Filesystem corruption detection
infrastructure or framework.

00:26:16.930 --> 00:26:21.250
This is something that we've introduced
specifically in Leopard or new in Leopard.

00:26:21.250 --> 00:26:22.329
It's very dynamic.

00:26:22.329 --> 00:26:26.139
There's nothing you have to do
explicitly in terms of offline your disk,

00:26:26.140 --> 00:26:29.620
or unmount your disk, downgrade
mount your disk to read only.

00:26:29.619 --> 00:26:30.279
Nothing like that.

00:26:30.279 --> 00:26:31.029
It's all dynamic.

00:26:31.029 --> 00:26:33.289
It happens in the runtime.

00:26:33.289 --> 00:26:37.420
At very little there's little performance
penalty here at all, it's always running.

00:26:37.420 --> 00:26:42.200
It's basically triggered by physical disk problems,
any kind of physical disk I/O errors or a number

00:26:42.200 --> 00:26:45.860
of data integrity checks that we're doing in the runtime.

00:26:45.859 --> 00:26:49.559
And so it's always running and kind
of keeping track of things there.

00:26:49.559 --> 00:26:53.419
In the event that you do have a problem
and NS fsck is going to be triggered,

00:26:53.420 --> 00:26:58.769
is reboot and hopefully everything will be
fixed, hopefully it's not a physical problem.

00:26:58.769 --> 00:27:01.680
Hopefully everything will be fixed and you
will be along your way.

00:27:01.680 --> 00:27:05.640
In the cases where we cannot fix the
drive, it will not be mounted as read only.

00:27:05.640 --> 00:27:08.940
Not going to look the other way and let
you read/write and do whatever you want.

00:27:08.940 --> 00:27:14.789
It's one of those things where we want -- and the motivation
here in read-only mounts is not to do anymore damage.

00:27:14.789 --> 00:27:16.899
Kind of let you know something is wrong.

00:27:16.900 --> 00:27:20.060
You can still salvage things from a read-
only context as far as copying things over,

00:27:20.059 --> 00:27:24.519
but don't want to mounted read/write certainly not
by default just in case something is seriously wrong

00:27:24.519 --> 00:27:26.809
and you are going to make the situation worse.

00:27:26.809 --> 00:27:28.919
Where possible.

00:27:28.920 --> 00:27:29.670
Where possible.

00:27:29.670 --> 00:27:34.240
We are going to try to provide specific
paths or file names so kind of get a feel

00:27:34.240 --> 00:27:37.890
for what's been damaged or what's been impacted.

00:27:37.890 --> 00:27:40.650
Kind of as a side note it's not
really directly tied to this project.

00:27:40.650 --> 00:27:47.140
But we also had improved fsck in terms of
providing a -c or cache size option.

00:27:47.140 --> 00:27:52.060
Kind of realize that these days there are a lot of
you out there with huge, huge memory configurations.

00:27:52.059 --> 00:27:54.809
And we are not taking advantage of that by default in fsck.

00:27:54.809 --> 00:27:57.679
So we provide this explicit option for those of you who hang

00:27:57.680 --> 00:28:01.950
out in the command line, you can start
passing much larger values for the block cache

00:28:01.950 --> 00:28:04.580
and make your performance actually jump quite a bit.

00:28:04.579 --> 00:28:05.329
So check that out.

00:28:05.329 --> 00:28:07.710
That's actually documented in the man page for fsck.

00:28:07.710 --> 00:28:10.319
It should be documented anyway.

00:28:10.319 --> 00:28:16.230
Okay. How do we get started here?

00:28:16.230 --> 00:28:17.240
HFS Directory Hard Links.

00:28:17.240 --> 00:28:24.349
HFS Directory Hard Link is specifically
created for Time Machine.

00:28:24.349 --> 00:28:27.279
It's specific to Journaled HFS+.

00:28:27.279 --> 00:28:32.819
As you know with Time Machine or most backup infrastructures
you've got some kind of level zero where you back everything

00:28:32.819 --> 00:28:36.200
up and then incrementals from that
point on that keep track of the changes.

00:28:36.200 --> 00:28:41.720
For the incrementals you want to complete a
view of the tree but not have to incur the costs

00:28:41.720 --> 00:28:43.180
of copying things that haven't been changed.

00:28:43.180 --> 00:28:45.509
So it's pretty obvious how the hard links fit in.

00:28:45.509 --> 00:28:49.769
You basically create a hard link, in this case for
subdirectory 1 where things haven't changed

00:28:49.769 --> 00:28:52.190
and you kind of get the same view, if you will.

00:28:52.190 --> 00:28:56.190
There's a reason why hard links
aren't very popular in the industry.

00:28:56.190 --> 00:28:57.350
Actually there's a number of reasons.

00:28:57.349 --> 00:29:00.909
This is probably one of the biggest
ones as far as either maliciously

00:29:00.910 --> 00:29:03.529
or accidentally creating a cycle in the name space.

00:29:03.529 --> 00:29:06.289
A lot of bad things happen because
it's very difficult to detect this.

00:29:06.289 --> 00:29:08.339
At least in a timely manner.

00:29:08.339 --> 00:29:13.769
And so you know a cycle is kind of represented by this red
line where it created a hard link from one of the trees

00:29:13.769 --> 00:29:16.029
in level zero up to the root of incremental 1.

00:29:16.029 --> 00:29:20.910
If I were to transcend to one of these
directories you'd loop for a long time, if not forever.

00:29:20.910 --> 00:29:22.140
Bad things happen.

00:29:22.140 --> 00:29:28.060
So we put very conservative limits on link creations to
prevent these cycles from occurring in the first place.

00:29:28.059 --> 00:29:33.960
So with that all that said, kind of get the feeling that
this is definitely not intended for general purpose use.

00:29:33.960 --> 00:29:35.660
It's very specific.

00:29:35.660 --> 00:29:39.640
Future changes may certainly occur not, just
with the on-disk format, but also with the SPI,

00:29:39.640 --> 00:29:42.580
as far as how we do these in the first place.

00:29:42.579 --> 00:29:46.069
With that said I'm sure all of you are
going to start poking around in the backing

00:29:46.069 --> 00:29:51.849
store anyway. Just be aware we've had some problems
here already with regard to this very basic scenario.

00:29:51.849 --> 00:29:56.740
Again the level zero in the incremental from the name
space point of view this is basically what you see.

00:29:56.740 --> 00:29:58.160
It looks like they are completely isolated.

00:29:58.160 --> 00:30:02.470
And you guys know better now, but it looks like from
the utility's point of view ones that aren't aware

00:30:02.470 --> 00:30:04.740
of directory hard links this is what they see.

00:30:04.740 --> 00:30:08.240
In the level zero hierarchy that's
completely separate from an incremental.

00:30:08.240 --> 00:30:13.589
And it seems natural that if you want to blow away the
incremental version, you should just be able to point

00:30:13.589 --> 00:30:16.099
at the root there and it will do the right thing.

00:30:16.099 --> 00:30:21.049
Of course, because it's implemented this way
as soon as you're naive RM command goes in there,

00:30:21.049 --> 00:30:26.589
depth first goes to subdirectory one, you will actually
be on the level zero side, start unrolling there

00:30:26.589 --> 00:30:28.500
and start deleting things from the originals.

00:30:28.500 --> 00:30:31.180
So this view isn't what it seems, so to speak.

00:30:31.180 --> 00:30:32.529
You've got this implementation.

00:30:32.529 --> 00:30:34.629
So a lot of utilities are not going to be aware.

00:30:34.630 --> 00:30:39.510
Just be careful, this is just one of
many I'm sure as far as gotchas.

00:30:39.509 --> 00:30:48.119
( Pause )

00:30:48.119 --> 00:30:50.049
Okay. Let's completely switch gears.

00:30:50.049 --> 00:30:57.009
Talk a little bit about the ZFS on Mac
OS X Server, the Developer Preview.

00:30:57.009 --> 00:30:59.259
Let's start really quickly describing what it is.

00:30:59.259 --> 00:31:03.410
I think a lot of you guys already know
what it is but kind of bear with me here.

00:31:03.410 --> 00:31:10.330
ZFS is the next-generation filesystem from
Sun Microsystems that's been ported to our platform.

00:31:10.329 --> 00:31:15.549
In my opinion it has no equal in terms of the data
integrity side of things for any production filesystem.

00:31:15.549 --> 00:31:21.750
And by data integrity I just don't mean the cases where
you have a huge number of disks and a lot of resources even

00:31:21.750 --> 00:31:26.710
down if you look at this kind of slider, all they way
down the single spindle cases ZSF can really do a good job

00:31:26.710 --> 00:31:30.220
in those cases as well when it comes to data integrity.

00:31:30.220 --> 00:31:34.089
It uses a pooled storage model
which is a great departure away

00:31:34.089 --> 00:31:37.720
of from traditional LVM, logical
volume managers and partitions.

00:31:37.720 --> 00:31:39.250
Talk a little bit more about that.

00:31:39.250 --> 00:31:43.339
It introduces RAID-Z, a great improvement
over something like or RAID 5

00:31:43.339 --> 00:31:46.519
and if we have time, I'll dig into that a little bit.

00:31:46.519 --> 00:31:49.960
Something that I like a lot about it is
kind of built in live disk scrubbing.

00:31:49.960 --> 00:31:55.740
It's well beyond anything like an fsck as
far as checking everything, all the blocks,

00:31:55.740 --> 00:31:57.390
metadata and data.

00:31:57.390 --> 00:32:02.440
And most importantly it can do this live on a system;
you don't have to again downgrade, mount or unmount,

00:32:02.440 --> 00:32:07.640
or boot in to some other OS to go
ahead and do this, so it can do it live.

00:32:07.640 --> 00:32:10.040
And it's really got first-class snapshots and clones.

00:32:10.039 --> 00:32:12.099
Where clones are read only or read/write snapshots.

00:32:12.099 --> 00:32:16.959
First class, I mean not just in terms
of functionality, in terms of no limits,

00:32:16.960 --> 00:32:18.100
but also implementation.

00:32:18.099 --> 00:32:24.369
It's not a bolt on architecture as far as snapshots
being very fundamental to designers of ZFS

00:32:24.369 --> 00:32:28.569
And of all the other kind of features,
if you will, it's going to end it here

00:32:28.569 --> 00:32:31.679
with compression capability, built-in compression capability.

00:32:31.680 --> 00:32:36.350
And this is great not just for
disk savings, which is important,

00:32:36.349 --> 00:32:40.929
but also for generating much higher I/O throughput
for workloads that compress very well.

00:32:40.930 --> 00:32:44.039
Just because of the amount of work you have
to do from disk is diminished quite a bit.

00:32:44.039 --> 00:32:48.000
When it expands out you can synthetize
these huge I/O rates and so.

00:32:48.000 --> 00:32:51.690
This is just a little bit, this is a
scratch in the service with regard to ZFS.

00:32:51.690 --> 00:32:53.700
There's much more out there that you guys
can read up about.

00:32:53.700 --> 00:32:57.380
I'm only going to talk about two things actually today.

00:32:57.380 --> 00:33:01.680
Talk a little bit more about the data integrity
model, and then the pooled storage model.

00:33:01.680 --> 00:33:06.140
Let's go ahead and start with the
data integrity side of things.

00:33:06.140 --> 00:33:08.300
With ZFS everything is copy on write.

00:33:08.299 --> 00:33:10.639
So you never overwrite live data.

00:33:10.640 --> 00:33:13.759
This is a great property you have
from an integrity point of view.

00:33:13.759 --> 00:33:18.879
It's very similar, in fact, to the whole safe save mechanism,
instead of for documents think of it as a safe save

00:33:18.880 --> 00:33:22.060
for blocks, if you will, with some
kind of transactional switch.

00:33:22.059 --> 00:33:27.309
In this case we'd over simplify and represent
a ZFS filesystem as a tree of blocks.

00:33:27.309 --> 00:33:30.869
Kind of represented by a uberblock
or head pointer of sorts.

00:33:30.869 --> 00:33:32.809
And you want to make a modification.

00:33:32.809 --> 00:33:35.809
You go ahead and copy and write, you know, your changes.

00:33:35.809 --> 00:33:41.710
Then I can copy and write the indirect nodes of the tree,
indirect blocks, and when you are all done we can go ahead

00:33:41.710 --> 00:33:43.980
and atomically update the uberblock.

00:33:43.980 --> 00:33:48.579
Update that version there and you'll notice just
like with the safe save there is no point in time

00:33:48.579 --> 00:33:52.429
where you can eject the fault, would
it be a power outage, plug pull, panic,

00:33:52.430 --> 00:33:56.370
whatever it is there's no point you can introduce
a fault and be left in an inconsistent state.

00:33:56.369 --> 00:33:59.549
This is very simple but it's also
very powerful and very provable.

00:33:59.549 --> 00:34:03.930
It's a very nice departure from traditional,
you know, overwriting things live and hoping

00:34:03.930 --> 00:34:07.759
for things to work out with some kind of replay.

00:34:07.759 --> 00:34:12.619
Another aspect of ZFS data integrity involves checksums.
Checksums, I think, are a very natural extension

00:34:12.619 --> 00:34:15.000
of any kind of data integrity goal.

00:34:15.000 --> 00:34:20.340
But I wanted to start off with conventional checksums
as far as checks sums that are local or very close

00:34:20.340 --> 00:34:22.430
to data, block global checksums, for example.

00:34:22.429 --> 00:34:27.789
They are certainly better than nothing as far as, at least
you're doing something to help ensure data integrity.

00:34:27.789 --> 00:34:29.840
It will catch bit rot, for example.

00:34:29.840 --> 00:34:36.250
But anytime you have block local checksums
you are vulnerable to a number of problems.

00:34:36.250 --> 00:34:41.320
These are problems that are common enough such that
they have industry standard terms or terminology.

00:34:41.320 --> 00:34:45.860
I'm going to talk about phantom writes and misdirection
that's because I've seen those myself personally.

00:34:45.860 --> 00:34:47.690
I can vouch for them, if you will.

00:34:47.690 --> 00:34:52.159
Phantom writes are simply a case where you do a
write, the driver, the device acknowledges the write

00:34:52.159 --> 00:34:55.079
and it just basically drops it, kind of disappears.

00:34:55.079 --> 00:34:59.409
Checksums, at least local checksums, are
not going to help you on that situation.

00:34:59.409 --> 00:35:06.420
Misdirection is just a fancy term for, let's say, you use a
filesystem asking the driver to drive itself or block 17.

00:35:06.420 --> 00:35:08.590
The driver returns blocks 71.

00:35:08.590 --> 00:35:10.630
The checksums check out and I keep on going.

00:35:10.630 --> 00:35:13.059
So we are not going to help you in that situation either.

00:35:13.059 --> 00:35:17.210
So again, better than nothing but
there are some vulnerabilities here.

00:35:17.210 --> 00:35:22.659
So compare that to how ZFS uses
checksums, kind of this indirect model.

00:35:22.659 --> 00:35:27.000
For starters everything in ZFS, at least
by default, everything is checksummed,

00:35:27.000 --> 00:35:28.840
not just the metadata but the data as well.

00:35:28.840 --> 00:35:33.690
There's this kind of fault isolation or indirection
between the actual data and the checksums.

00:35:33.690 --> 00:35:37.980
So in this kind of simple layout here
this tree of blocks, if you will,

00:35:37.980 --> 00:35:41.309
anytime I modify the data and go ahead
and calculate the checksum store

00:35:41.309 --> 00:35:44.099
that in the address away from it
as far as an apparent porter,

00:35:44.099 --> 00:35:49.569
calculate the checksum for those kind of transitively
kind of roll up here all the way to the uberblock.

00:35:49.570 --> 00:35:55.180
Get a nice end to end, you know, this is a very
secure way to do things as far as data integrity goes.

00:35:55.179 --> 00:35:59.399
For those of you who want a more formal definition
you can go ahead and look up Merkle tree or Hash Trees,

00:35:59.400 --> 00:36:02.710
it's got some nice properties that are exploited.

00:36:02.710 --> 00:36:07.780
But all those previous problems I mentioned earlier
as far as the misdirection, the phantom writes,

00:36:07.780 --> 00:36:13.040
and everything, those are all covered at least they'll
be detected by this particular checksum scheme.

00:36:13.039 --> 00:36:16.789
So quite an improvement over block local checksums.

00:36:16.789 --> 00:36:23.400
And finally, there's actually a bit more but this is the
last leg I'm going to talk about regarding data integrity.

00:36:23.400 --> 00:36:25.490
Let's call it variable redundancy.

00:36:25.489 --> 00:36:29.179
If you are looking at any kind of block
tree layout again over simplified here.

00:36:29.179 --> 00:36:31.199
There's the triangle or the tree itself.

00:36:31.199 --> 00:36:34.969
You'll have to agree that it's a lot
worse to lose blocks further up the tree.

00:36:34.969 --> 00:36:39.489
Meaning, it's bad to lose blocks, in general to have
something bad happen to a block, but if something bad has

00:36:39.489 --> 00:36:41.559
to happen, you want it to be at the leaf level.

00:36:41.559 --> 00:36:42.539
Where that's all you'll lose.

00:36:42.539 --> 00:36:43.489
You've just lost the leaf.

00:36:43.489 --> 00:36:47.889
If you lose an interior node you start
to lose many more -- much more data, and

00:36:47.889 --> 00:36:52.509
this kind of logically includes the uberblock where
that's probably the most catastrophic block to lose.

00:36:52.510 --> 00:36:58.460
So with ZFS, the more important metadata, the further
up the tree you go, the more highly replicated you are.

00:36:58.460 --> 00:37:03.710
So it's a nice semantic or nice design
principle to have with regard to data integrity.

00:37:03.710 --> 00:37:14.630
Okay. Let's talk about the pool model, at least a
little bit as far as how it impacts you as users.

00:37:14.630 --> 00:37:18.910
All of you are familiar with this basic
configuration as far as a drive and a filesystem.

00:37:18.909 --> 00:37:22.730
That's what we all have basically or most of us have.

00:37:22.730 --> 00:37:25.429
It works out pretty well, it's worked out for a long time.

00:37:25.429 --> 00:37:27.559
The problem is you fill this drive up.

00:37:27.559 --> 00:37:30.710
I'm going to have to buy more storage.

00:37:30.710 --> 00:37:35.970
And for a lot of you the first step, which probably
be just to create another filesystem, it's fine.

00:37:35.969 --> 00:37:40.059
Something you're going to have to manage though
as far as maybe new files go on the new filesystem,

00:37:40.059 --> 00:37:42.299
old files you have to remember that are
on the old filesystem.

00:37:42.300 --> 00:37:45.260
A little bit of management overhead
here, a little bit more clumsy,

00:37:45.260 --> 00:37:46.400
but it's not the end of the world.

00:37:46.400 --> 00:37:51.539
It's certainly something that's served
all of us fairly well for a long time.

00:37:51.539 --> 00:37:54.869
A lot of you can bring up a logical
volume manager, at least a traditional one

00:37:54.869 --> 00:37:58.159
where I concatenate the two drives
together, and that's fine.

00:37:58.159 --> 00:38:01.429
That's actually a lot better in terms of
management in the previous situation.

00:38:01.429 --> 00:38:04.889
Of course, if I started out with the previous
situation I'd have to back everything up,

00:38:04.889 --> 00:38:07.400
create my volume and then restore everything on there.

00:38:07.400 --> 00:38:09.500
You know, again, not the end of the world.

00:38:09.500 --> 00:38:14.280
This is certainly a better model but it's still pretty
rigid as far as the association between my filesystem,

00:38:14.280 --> 00:38:21.260
the volume, and my initial choice to create this
particular -- this particular concatenated volume in this case.

00:38:21.260 --> 00:38:24.860
In summary, there's always going to be
this association between a partition

00:38:24.860 --> 00:38:28.240
or a volume, some kind of dev node in every filesystem.

00:38:28.239 --> 00:38:29.929
Sure you can grow and shrink.

00:38:29.929 --> 00:38:32.710
I shrink, most of the time any way, by hand or explicitly.

00:38:32.710 --> 00:38:38.289
So there's -- there's ways you can get around this as
far as growing into new storage as it becomes available.

00:38:38.289 --> 00:38:42.550
I think the main point here is that you have
these kind of stranded storage islands, as far

00:38:42.550 --> 00:38:46.690
as with this layout here, anything in
filesystem 3 that's excess or extra,

00:38:46.690 --> 00:38:52.320
whether it be extra bandwidth or extra space
cannot be utilized by filesystems 1 and 2.

00:38:52.320 --> 00:38:56.750
Once you've made up your mind it becomes
a little bit more manual or impossible

00:38:56.750 --> 00:38:58.940
in some cases to kind of rebalance things around.

00:38:58.940 --> 00:39:00.909
You have to do a very manual process.

00:39:00.909 --> 00:39:01.799
Things are very rigid.

00:39:01.800 --> 00:39:04.450
There's a very rigid association between the layers here.

00:39:04.449 --> 00:39:08.009
And so things are so much grounded
once you make up your mind.

00:39:08.010 --> 00:39:15.000
Looking at the ZFS pool model we start off with
filesystems are built on top of storage pools.

00:39:15.000 --> 00:39:18.360
And just like with LVMs, pools do
abstract away the physical storage.

00:39:18.360 --> 00:39:20.210
That's what they have in common.

00:39:20.210 --> 00:39:23.720
But unlike traditional logical
volume managers, all available space

00:39:23.719 --> 00:39:28.759
and bandwidth in the pool is actually
shared by all the filesystems on top of it.

00:39:28.760 --> 00:39:29.870
And here's what's kind of interesting.

00:39:29.869 --> 00:39:35.250
The filesystems themselves are very lightweight in terms
of being able to create and destroy as many as you want.

00:39:35.250 --> 00:39:38.039
They are very similar semantically to folders
and directories.

00:39:38.039 --> 00:39:41.400
Not quite the same but they are very similar
especially from an end-user point of view.

00:39:41.400 --> 00:39:45.130
You guys don't give any second thought
to creating folders or directories.

00:39:45.130 --> 00:39:50.660
You don't have to worry about how much space is available in
general or associating it with a partition or a filesystem

00:39:50.659 --> 00:39:53.049
or anything along those lines. You
just go ahead and create them.

00:39:53.050 --> 00:39:55.990
Same thing with the filesystems here.

00:39:55.989 --> 00:39:59.379
Pools themselves can be configured
by mirroring, concatenation,

00:39:59.380 --> 00:40:03.329
or this RAIDZ configuration or a combination of the three.

00:40:03.329 --> 00:40:07.360
And to kind of explain this a little bit better I'll
go ahead and talk about that same scenario earlier.

00:40:07.360 --> 00:40:09.090
We have a single drive.

00:40:09.090 --> 00:40:12.660
We create a ZFS pool or a Z pool there
and put a filesystem on top of it.

00:40:12.659 --> 00:40:16.539
That kind of looks just like the logical volume scenario.

00:40:16.539 --> 00:40:21.009
But with the pool model you can create as many
filesystems as you want, as many or as few.

00:40:21.010 --> 00:40:23.600
Very dynamic, you are not stuck with your initial decision.

00:40:23.599 --> 00:40:25.230
Go ahead and create as many as you want.

00:40:25.230 --> 00:40:29.050
They all share the bandwidth and the space of that drive.

00:40:29.050 --> 00:40:33.650
In this case I kind of gave them folder-
like names as far as Movies and so on.

00:40:33.650 --> 00:40:35.519
Very dynamic model here.

00:40:35.519 --> 00:40:40.639
When you run out of space, go ahead and add a new
drive, the pool expands, you don't have to do anything

00:40:40.639 --> 00:40:45.409
with regard to all those files in those filesystems or
those folders, the Movies, Pictures, Documents and so on.

00:40:45.409 --> 00:40:46.309
You don't have to do anything.

00:40:46.309 --> 00:40:48.380
There's no explicit action you have to take.

00:40:48.380 --> 00:40:53.360
They all immediately see all that extra space,
can take advantage of that extra bandwidth there.

00:40:53.360 --> 00:40:56.300
So again a very dynamic model.

00:40:56.300 --> 00:40:57.769
Growing and shrinking is automatic.

00:40:57.769 --> 00:41:01.789
There is no explicit anything here and
like I keep on reiterating all the space

00:41:01.789 --> 00:41:05.759
of all the underlying storage devices
is all shared space and bandwidth.

00:41:05.760 --> 00:41:10.860
For those of you who are thinking, you
know, it's nice in a multi-spindle type

00:41:10.860 --> 00:41:15.349
of configurations, these dynamic pool properties do
apply to single spindle configurations as well.

00:41:15.349 --> 00:41:21.259
I know a lot of you as developers may have some
big drives for your desktop or your laptops.

00:41:21.260 --> 00:41:25.400
And, you know, you may have a couple
boot partitions, a big data partition.

00:41:25.400 --> 00:41:29.820
And once you kind of set on that or you make that
configuration choice, you are somewhat stuck.

00:41:29.820 --> 00:41:33.840
Meaning if you kind of misjudged, miscalculated,
you want to move things around you can do it.

00:41:33.840 --> 00:41:38.450
Actually most of you in this room could probably
do it mechanically but it can be a bit of pain.

00:41:38.449 --> 00:41:42.759
With ZFS just have a pool on top of there,
create as many filesystems as you want.

00:41:42.760 --> 00:41:46.250
You don't have to worry about preallocating
or precalculating size

00:41:46.250 --> 00:41:49.409
and such, things work out very nicely there.

00:41:49.409 --> 00:41:52.239
How we doing on time here?

00:41:52.239 --> 00:41:54.569
We have a lot of time.

00:41:54.570 --> 00:41:56.190
So RAIDZ. I want to talk about this.

00:41:56.190 --> 00:42:01.260
There's also a mirror configuration but it's a little bit,
it's actually pretty interesting, I don't have any slides

00:42:01.260 --> 00:42:03.150
for that at this point in time.

00:42:03.150 --> 00:42:05.530
RAIDZ is something that's specific to ZFS.

00:42:05.530 --> 00:42:10.440
And the best way I like to introduce
it as an improvement over RAID-5.

00:42:10.440 --> 00:42:14.070
And I think most of you are familiar with
traditional RAID-5. It's an N plus 1 model.

00:42:14.070 --> 00:42:18.059
It's pretty simple, it works well today.

00:42:18.059 --> 00:42:24.079
Basic idea, of course, is to kind of stripe your data across
one parity disk and distributing your parity out there,

00:42:24.079 --> 00:42:28.480
but kind of the key here is that value
of N is always going to be fixed.

00:42:28.480 --> 00:42:30.849
It's what drives the detection in the first place.

00:42:30.849 --> 00:42:36.279
Let's just say we're going to use some exclusive war
function for our detection or function itself.

00:42:36.280 --> 00:42:40.420
The end value, that fixed value, is what drives
the error detection and also the reconstruction.

00:42:40.420 --> 00:42:44.300
And in this example and further examples
I'm going use a fixed value of 3.

00:42:44.300 --> 00:42:48.420
3 data block or 3 data disk and 1 parity disk.

00:42:48.420 --> 00:42:53.639
Problem is with any RAID-5 configuration any value
of N, you are always going to have a situation

00:42:53.639 --> 00:42:57.650
where you have a write, an outstanding write, that
does not fill the full width of that stripe.

00:42:57.650 --> 00:42:58.840
That's always going to be the case.

00:42:58.840 --> 00:43:02.460
There's never a situation where you
can always have complete stripe writes.

00:43:02.460 --> 00:43:09.300
And so in this case kind of represent that N
plus or 3 plus 1, 3 data blocks, 1 parity block,

00:43:09.300 --> 00:43:12.660
let's just say my workload right now is
just 1 data block.

00:43:12.659 --> 00:43:18.339
I go ahead and write that data out,and at that exact
moment in time you notice the parity is no longer valid.

00:43:18.340 --> 00:43:21.700
If I were to crash at this point in time,
you'd be in a little bit of trouble just

00:43:21.699 --> 00:43:23.639
because things are just out of
sync at this point.

00:43:23.639 --> 00:43:28.029
Of course, if nothing happens, you go ahead and
update the new parity and things kind of work out.

00:43:28.030 --> 00:43:34.040
But, like I said earlier, because you cannot atomically
update all those disks at that same time, if a fault occurs

00:43:34.039 --> 00:43:38.400
after the data is updated, the new data, but
before the parity update, you'll have corruption.

00:43:38.400 --> 00:43:41.610
And this is known in the industry as RAID-5 write hole.

00:43:41.610 --> 00:43:44.720
The side effect of the fact that you are updating
data in place.

00:43:44.719 --> 00:43:49.019
There are certainly workarounds for
this as far as on the enterprise side.

00:43:49.019 --> 00:43:51.460
Battery backed NVRAM and so on.

00:43:51.460 --> 00:43:55.440
Typically associated again with
the enterprise space, however.

00:43:55.440 --> 00:44:01.539
So on to RAIDZ and how RAIDZ gets around this problem or
solves this problem, if you will, is variable stripe width.

00:44:01.539 --> 00:44:04.420
And that's something that's very
well captured by this diagram.

00:44:04.420 --> 00:44:09.500
The colors basically represent the stripe
width, and you can see that they're variable.

00:44:09.500 --> 00:44:13.869
You have some very wide stripes, some
very small ones, and things in between.

00:44:13.869 --> 00:44:18.440
Point being is that you always, because you're
variable, everything becomes a full stripe write.

00:44:18.440 --> 00:44:23.889
So right off the bat there's not going to be any read-
modify-write in terms of the performance side of things.

00:44:23.889 --> 00:44:28.170
When you copy that with copy on write, like I
said earlier where you never overwrite live data,

00:44:28.170 --> 00:44:30.380
the RAID-5 write hole is eliminated completely.

00:44:30.380 --> 00:44:36.220
And so you solve the problem without any need
for NVRAM, special hardware, or special expenses.

00:44:36.219 --> 00:44:40.949
The ZFS guys have a saying about ZFS loving cheap disks.

00:44:40.949 --> 00:44:49.859
Here's our current status as far as --
as far as ZFS on Mac OS X Server.

00:44:49.860 --> 00:44:54.019
In Leopard, I guess 10.5.0, the initial version of Leopard,

00:44:54.019 --> 00:44:56.949
somebody already discovered this--
there's a read-only default.

00:44:56.949 --> 00:44:58.859
Not very interesting, not much you can do.

00:44:58.860 --> 00:45:01.510
It's the read-only version of this as far as the kext.

00:45:01.510 --> 00:45:05.870
Over the next few weeks or so, I'd say two weeks

00:45:05.869 --> 00:45:11.909
on the outside, hopefully, we will be introducing a
read-write beta download for all of you as ADC members.

00:45:11.909 --> 00:45:15.599
Somebody can download for free, check
it out, play with it, kick it around.

00:45:15.599 --> 00:45:17.920
At this point it's going to be mostly command line.

00:45:17.920 --> 00:45:24.920
You don't have any need to integrate it with any
of the higher level GUI apps and so on.

00:45:24.920 --> 00:45:27.139
We have not worked on performance at all at this point.

00:45:27.139 --> 00:45:32.039
So something that we have to get going
as far as spending some time there.

00:45:32.039 --> 00:45:32.829
And snapshots.

00:45:32.829 --> 00:45:34.139
Snapshots actually do work.

00:45:34.139 --> 00:45:39.859
We just haven't ported something called GFS or it's
equivalent, some type of equivalent functionality.

00:45:39.860 --> 00:45:45.480
So you won't be able to browse them as easily
as far as mounting them on /ZFS or .ZFS.

00:45:45.480 --> 00:45:52.400
So you can take snapshots but browsing one until we get
that component in place is not going to be as easy.

00:45:52.400 --> 00:45:54.800
That's basically where we're at.

00:45:54.800 --> 00:45:59.690
For more information there's a ZFS mailing list
you can kind of send feedback comments to.

00:45:59.690 --> 00:46:05.730
There's also ZFS radar component for those
of you who have radar access and want to file bugs.

00:46:05.730 --> 00:46:12.130
For any questions in general, ZFS, filesystem in general,
please talk to our technology evangelist Deric Horn.

00:46:12.130 --> 00:46:14.000
His e-mail address is up there.

00:46:14.000 --> 00:46:18.320
And you can interact with us tomorrow
at the Filesystem Lab, hopefully 10:30

00:46:18.320 --> 00:46:20.360
in the morning is not too early for most of you.

00:46:20.360 --> 00:46:23.099
We'll be there for, I think, an hour and a half.

00:46:23.099 --> 00:46:27.630
And then there's that Bash on Thursday
where most of us will be attending.

00:46:27.630 --> 00:46:30.840
You can interact with us there as
well or even after this session here.