WEBVTT

00:00:18.070 --> 00:00:23.589
>> And welcome to Session 139,
Partitioning your Cocoa Application.

00:00:23.589 --> 00:00:28.559
My name is Chris Kane, and I'm a
software engineer with the Cocoa team.

00:00:29.809 --> 00:00:35.850
So what I'm going to be talking about today - I'm going to
define that term partitioning that you saw in the title.

00:00:35.850 --> 00:00:45.490
I'm going to give several examples of when and why you
would be using this, what really is a design technique.

00:00:45.490 --> 00:00:55.310
I'm going to talk about a few of the pitfalls, I'm going to
look at several you know, sort of aspects of partitioning

00:00:55.310 --> 00:01:02.380
in a little bit more detail, and I'm going to finally
at the end give you some concrete code examples.

00:01:02.380 --> 00:01:07.480
So if you find yourself you know, just
really want to see code examples and zoning

00:01:07.480 --> 00:01:11.120
out during the first half of the talk, hang in there.

00:01:11.120 --> 00:01:14.840
We'll get to some code examples eventually.

00:01:14.840 --> 00:01:21.310
I'm going to have to assume in this talk that
you have some knowledge of multi-threading or,

00:01:21.310 --> 00:01:28.060
and inter-process communication,
and I'm going to use terms like thread,

00:01:28.060 --> 00:01:30.439
and I'm going to have to assume you know what that is.

00:01:30.439 --> 00:01:35.459
Or I'm going to use a term like socket
and assume you know what that is.

00:01:35.459 --> 00:01:43.579
I'm not going to define these things, or go into more
detail about how you do socket based communication,

00:01:43.579 --> 00:01:48.509
I'm going to assume you can, after the
talk if you don't know what they are,

00:01:48.510 --> 00:01:54.150
can go and investigate more about these things on your own.

00:01:54.150 --> 00:02:01.969
Finally we're just going to talk about some multi-process
and multi-threading reasons for partitioning today.

00:02:01.969 --> 00:02:09.819
Although there are good reasons for using
partitioning, or other reasons as well.

00:02:09.819 --> 00:02:13.259
So what do I mean by partitioning?

00:02:13.259 --> 00:02:17.929
Well, first and foremost partitioning is a design technique.

00:02:17.930 --> 00:02:21.200
One might call it factoring.

00:02:21.199 --> 00:02:25.030
We almost did, but we decided against
that because we didn't want everybody

00:02:25.030 --> 00:02:28.169
to think oh this was going to be a talk about re-factoring.

00:02:28.169 --> 00:02:41.089
No, what we mean by partitioning is the segregation of parts
of an application into well defined and independent pieces.

00:02:41.090 --> 00:02:48.849
And then those pieces have to
in some way communicate with one another.

00:02:48.849 --> 00:02:56.620
So what we're talking about is a design technique
where basically you're trying to either build walls

00:02:56.620 --> 00:03:03.480
because you want to keep things independent,
or you have walls in place for some reason,

00:03:03.479 --> 00:03:09.699
like you have multiple processes and you
have to deal with those walls in some way.

00:03:09.699 --> 00:03:18.539
So to summarize right away, there are two main aspects
to partitioning, the segregation into independent parts,

00:03:18.539 --> 00:03:21.909
and then the communication between the parts.

00:03:23.129 --> 00:03:31.370
So to illustrate what I mean in more concrete
terms, I'm going to go through a series of examples of,

00:03:31.370 --> 00:03:34.069
reasons for and examples of partitioning.

00:03:34.069 --> 00:03:38.060
So the first is the Unix command line tool.

00:03:38.060 --> 00:03:44.900
We want to make use, well the first is we want
to make use of other applications or executables,

00:03:44.900 --> 00:03:49.870
applications or executables which somebody
else has written, somebody else is providing,

00:03:49.870 --> 00:03:52.890
and the Unix command line tool is my first example.

00:03:52.889 --> 00:03:59.529
If you want to invoke a Unix command line tool from within
your application, you're going to be dealing with the kind

00:03:59.530 --> 00:04:04.259
of issues that you're, arise in partitioning.

00:04:04.259 --> 00:04:10.099
So when you, after you have created
the command line tool, you know,

00:04:10.099 --> 00:04:13.530
started it in some way, you have to communicate with it.

00:04:13.530 --> 00:04:20.240
And there are several different ways you might communicate
with the command line tool, such as passing arguments to it

00:04:20.240 --> 00:04:25.170
when you start it, you might use a
pipe or a socket, or Unix signals.

00:04:25.170 --> 00:04:30.569
And the file system can also be a way to
communicate with the command line tool.

00:04:30.569 --> 00:04:32.459
So this is the kind of thing I'm talking about.

00:04:32.459 --> 00:04:39.339
You have a separate process, in this case a command line
tool, and you have to communicate with it in some way.

00:04:39.339 --> 00:04:43.750
Another example is the services menu in your application.

00:04:43.750 --> 00:04:51.620
When you have a services menu, you have things
like mail the selection to somebody else.

00:04:51.620 --> 00:04:59.459
If you have a selection of text in your code,
and you, the user chooses the mail to service,

00:04:59.459 --> 00:05:03.779
the mail application gets that
data, and puts up a compose window.

00:05:03.779 --> 00:05:12.009
So that's an example again, of a form of
partitioning, where the services menu is making use

00:05:12.009 --> 00:05:16.209
of services provided by other applications.

00:05:16.209 --> 00:05:21.829
In that particular case, most of the
communication is going on via the pace board.

00:05:21.829 --> 00:05:28.979
So the selected text for example, is put on
the pace board, and the mail app reads it off.

00:05:28.980 --> 00:05:35.520
But mach ports and the file system can also
be involved in the services communication.

00:05:35.519 --> 00:05:40.870
So those are other examples of the
communication going on in that case.

00:05:40.870 --> 00:05:49.189
When you have a scriptable application, you are
invoking services, or capabilities I should say,

00:05:49.189 --> 00:05:59.339
often that other scripted, scriptable application, and you
are effectively communicating with the scripting engine,

00:05:59.339 --> 00:06:06.619
that is the scripting libraries, and whatever mechanism
it uses to do the actual communication back and forth.

00:06:06.620 --> 00:06:11.030
You know, Apple events at the high level.

00:06:11.029 --> 00:06:15.659
Another reason you might use other
applications and other executables is

00:06:15.660 --> 00:06:18.810
to use services on the local network or on the Internet.

00:06:18.810 --> 00:06:25.569
So if you're say talking to some API
that is vended by Amazon dot com,

00:06:25.569 --> 00:06:32.339
this is another type of partitioning that's going
on, where you are making use of something going

00:06:32.339 --> 00:06:37.729
on on another machine, in this case off at Amazon dot com.

00:06:37.730 --> 00:06:45.270
In this case you're probably communicating with sockets,
and possibly a higher level protocol, like SOAP or HTTP.

00:06:45.269 --> 00:06:52.109
If you're communicating between multiple Mac OS X
machines you can use of course, distributed objects.

00:06:52.110 --> 00:06:58.800
If you're you know, communicating with
something that you've written on the other end.

00:06:58.800 --> 00:07:04.449
Another reason to use partitioning is to
make use of multiple platform architectures.

00:07:04.449 --> 00:07:12.120
So there are several different kinds of platform
choices that we now have available in the system.

00:07:12.120 --> 00:07:18.350
For example, there is Intel or
PowerPC, the microprocessor choice.

00:07:18.350 --> 00:07:22.200
There is 64-bit process or 32-bit process.

00:07:22.199 --> 00:07:25.969
There is garbage collected and non-garbage collected.

00:07:25.970 --> 00:07:28.660
And of course there are different operating systems.

00:07:28.660 --> 00:07:36.439
You know, Mac OS X and, or Windows I list here, of course
or Linux, probably should have added that in the slide.

00:07:36.439 --> 00:07:45.439
And these are properties of the entire process, so
that is if you want to do something in 64-bit,

00:07:45.439 --> 00:07:48.980
the entire process has to be 64-bit.

00:07:48.980 --> 00:07:54.400
You cannot have you know, most of it be 32-bit,
and you got a little bit of it to be 64-bit.

00:07:54.399 --> 00:07:56.839
The entire process has to be 64-bit.

00:07:56.839 --> 00:08:05.449
So if you need to do something that involves being,
using 32-bit code and using 64-bit code,

00:08:05.449 --> 00:08:08.209
you have to have multiple processes involved.

00:08:08.209 --> 00:08:10.489
Well why would you want to do that?

00:08:10.490 --> 00:08:17.879
Well if you have for example, existing plug-ins that
are 32-bit and, or perhaps they're Power PC,

00:08:17.879 --> 00:08:25.939
and you want to continue using them in some way,
then you have to create, but you want your app for,

00:08:25.939 --> 00:08:35.179
in this example say, to you know, be converted
to 64-bit, you have to continue,

00:08:35.179 --> 00:08:40.919
well you have to create a new process, a
32-bit process to load those plug-ins,

00:08:40.919 --> 00:08:50.669
and then communicate with that 32-bit process 2in
order to make use of that, those old plug-ins in some way.

00:08:50.669 --> 00:08:56.719
Another example might be suppose
you're using an open source library,

00:08:56.720 --> 00:09:02.529
and it just isn't ready for being
compiled as 64-bit.
d

00:09:02.529 --> 00:09:09.589
You know, perhaps it assumes that ints and pointers
are the same size, and they're interchangeable.

00:09:09.590 --> 00:09:15.050
So when you go and try to you know, compile it
as 64-bit, and you try to run, you know,

00:09:15.049 --> 00:09:17.689
it just crashes all over the place, or something like that.

00:09:17.690 --> 00:09:25.450
And you know, you just don't have the resources to
port it yourself, you may have to use partitioning.

00:09:25.450 --> 00:09:33.140
That is create a 32-bit proces2s which links
with that open source library, and then define some sort

00:09:33.139 --> 00:09:41.110
of protocol or communication channel with that thirty two
bit process in order to continue using the capabilities

00:09:41.110 --> 00:09:48.550
of that presumably important open
source library that you're using.

00:09:48.549 --> 00:09:50.009
So what do you do?

00:09:50.009 --> 00:09:55.860
Well, as I said before, you're going to have to
create a separate helper process, which you know,

00:09:55.860 --> 00:10:02.500
you're going to launch when you first need it, and
then you're going to communicate with it via some sort

00:10:02.500 --> 00:10:10.200
of communication channel, inter-process communication
channel, like pipes or sockets, or the file system again,

00:10:10.200 --> 00:10:16.840
can be a form of communication channel by writing a
file that that other thing picks up off the file system.

00:10:16.840 --> 00:10:22.930
Distributed objects here is mentioned, and I mention
this in particular at this point because we have made

00:10:22.929 --> 00:10:27.599
in Leopard distributed objects 64-bit capable,d
because we you know,

00:10:27.600 --> 00:10:29.629
had to make the whole thing 64-bit.

00:10:29.629 --> 00:10:37.539
But distributed objects can also talk to 32-bit
processes from 64-bit processeds, and vice versa.

00:10:37.539 --> 00:10:49.049
And in fact we will do some forms of argument type coercion
and what not to you know, be able to continue to talk

00:10:49.049 --> 00:10:54.599
between the 64-bit and 32-bit processes
where method type signatures may change slightly.

00:10:54.600 --> 00:10:57.840
Because you know, one thing it may now be using a long,

00:10:57.840 --> 00:11:01.269
but in a 32-bit space it's
using an int, and that kind of thing.

00:11:01.269 --> 00:11:09.250
And so as long as the long value fits into the 32-bit
integer,2 we'll you know, convert back and forth.

00:11:10.570 --> 00:11:15.340
Another reason to use partitioning
is to make use of another machine.

00:11:15.340 --> 00:11:18.870
So you may want to use another machine's operating system,

00:11:18.870 --> 00:11:21.620
there may be something that an
operating system can do for you.

00:11:21.620 --> 00:11:26.299
Or it may have software, other software on that
operating system that you need to make use of.

00:11:26.299 --> 00:11:28.959
You may want to simply use its harbor resources.

00:11:28.960 --> 00:11:36.340
For example, if you use distributed builds and Xcode,
you're simply making use of the other machine's you know,

00:11:36.340 --> 00:11:42.019
CPU, its disk space, its memory to do your compilation.

00:11:42.019 --> 00:11:47.699
Another reason is you may have special hardware
which is you know, somewhere off on a network,

00:11:47.700 --> 00:11:53.120
hooked up to that particular machine, and you
want, if you want to access that special hardware,

00:11:53.120 --> 00:11:57.269
it might be you know, a simple case would be a printer.

00:11:57.269 --> 00:12:03.059
Then you would be using a form of
partitioning in order to talk to that printer.

00:12:03.059 --> 00:12:08.399
Now you generally wouldn't talk to a printer
yourself, you would go through a library

00:12:08.399 --> 00:12:10.659
to you know, do the printing to the other machine.

00:12:10.659 --> 00:12:17.649
So that's other subsystem, that printing
subsystem is doing the inter-process communication

00:12:17.649 --> 00:12:19.799
and the partitioning for you.

00:12:21.759 --> 00:12:28.480
So you have to create and use a server process in
that other machine in order to be able to you know,

00:12:28.480 --> 00:12:33.259
make use of that other machine's resources,
its hardware, whatever it happens to be.

00:12:33.259 --> 00:12:37.779
And again, example of communication
include pipes and sockets,

00:12:37.779 --> 00:12:45.659
distributed objects if you have a Cocoa server running on
the other side, or you know, some sort of custom protocol.

00:12:45.659 --> 00:12:51.230
It might be SOAP or HTTP based, or it might
just be a custom protocol of you know,

00:12:51.230 --> 00:12:54.810
a custom byte stream over the sockets that you're using.

00:12:54.809 --> 00:12:59.719
Or you might use a library which is provided
for you from you know, somewhere else,

00:12:59.720 --> 00:13:06.820
like a third party might provide a library that talks
to its server, which you run on the other machine.

00:13:06.820 --> 00:13:12.560
A fourth reason for partitioning is to increase safety.

00:13:12.559 --> 00:13:21.889
The first example might be just data
security, second might be privilege level.

00:13:21.889 --> 00:13:28.360
So if you want to run something at a higher privilege,
or a lower privilege, or simply as a different user,

00:13:28.360 --> 00:13:32.710
you may want to create a separate
process in order to do that.

00:13:32.710 --> 00:13:41.470
A second process can also give you a separate protected
address space in which to you know, run code in,

00:13:41.470 --> 00:13:46.750
a separate address space which is different
from your main applications address space.

00:13:46.750 --> 00:13:52.440
And so that can give you some additional safety,
and I'll get to some examples in a second.

00:13:52.440 --> 00:14:00.140
A separate address space, a separate process,
is another way to achieve multi threading,

00:14:00.139 --> 00:14:04.399
that is each of those processes
will have at least one threat.

00:14:04.399 --> 00:14:15.090
But because it's often a separate address space, you can
in some sense avoid many of the multi threading issues

00:14:15.090 --> 00:14:23.120
that may arise if you're trying to do actual multiple
threads within your own application directly.

00:14:23.120 --> 00:14:29.149
So what are some examples?

00:14:29.149 --> 00:14:34.179
Well, I think the most compelling
example might be again, the plug in.

00:14:34.179 --> 00:14:41.620
Suppose you have a desire to have a plug in model,
or you have a plug in model for your application,

00:14:41.620 --> 00:14:45.919
but you don't want to load those
plug-ins right into your application.

00:14:45.919 --> 00:14:51.949
That is maybe there's information or things that you
don't want the plug-ins to be able to access, you know,

00:14:51.950 --> 00:14:55.670
if they were to be loaded right into your application.

00:14:55.669 --> 00:15:01.559
Well, creating a separate process, you can
load the plug-ins into the separate process,

00:15:01.559 --> 00:15:06.299
and have them off in their own little world
so they can't get access to your data.

00:15:06.299 --> 00:15:12.059
And if they happen to be malfunctioning,
or malicious in some way, they you know,

00:15:12.059 --> 00:15:16.169
cannot take down your application,
they'll take down the helper process.

00:15:16.169 --> 00:15:20.449
In fact, this is what Spotlight does.

00:15:20.450 --> 00:15:26.490
If you've ever done a process listing
while Spotlight is indexing your machine,

00:15:26.490 --> 00:15:30.049
you would have seen these MD worker processes running.

00:15:30.049 --> 00:15:38.769
And the MD workers are separate processes which are
loading the Spotlight plug-ins, and running them,

00:15:38.769 --> 00:15:47.049
but they're separate from the actual Spotlight daemon,
which you know, sort of controls the Spotlight database.

00:15:47.049 --> 00:15:48.579
Let's see.

00:15:48.580 --> 00:15:54.629
And again, well as I said before, you have
to create the helper process for this.

00:15:54.629 --> 00:16:01.659
A fifth and final reason that I'm going to cover
is separation or encapsulation of functionality.

00:16:01.659 --> 00:16:05.870
And this is a little abstract, a little
vague, so let me give some examples.

00:16:05.870 --> 00:16:12.000
Well one reason for this is simply to
improve the structure of your application.

00:16:12.000 --> 00:16:20.870
If you have independent parts you might have a better
structured application that gives you more you know,

00:16:20.870 --> 00:16:23.350
more flexibility in making changes or what not.

00:16:23.350 --> 00:16:25.029
So this, that's still a little vague.

00:16:25.029 --> 00:16:29.509
You may want to separate the computation,

00:16:29.509 --> 00:16:35.490
and the data involved in some computation,
from its presentation in the UI.

00:16:35.490 --> 00:16:42.250
One example might be in the you
know, the new Leopard system.

00:16:42.250 --> 00:16:46.409
We have introduced NS operation that
I'll talk about in a little bit.

00:16:46.409 --> 00:16:53.839
And so that's an example of you taking a computation and
the data it needs, and trading a separate entity for it

00:16:53.840 --> 00:16:58.780
so that it can be run on another
thread in the background usually.

00:16:58.779 --> 00:17:01.870
You may want to create separate data domains.

00:17:01.870 --> 00:17:05.970
For example, you may want to say
okay, I'm going to create a thread,

00:17:05.970 --> 00:17:14.039
and I'm going to make that thread responsible
for accessing say this core data file.

00:17:14.039 --> 00:17:18.769
And doing all, that thread is going
to do all of the core data stuff.

00:17:18.769 --> 00:17:28.210
And so I don't have to worry then about multiple threads
trying to access that core data file, that core data store.

00:17:28.210 --> 00:17:30.840
I'm going to make that thread responsible.

00:17:30.839 --> 00:17:36.419
But once I've done that then, I need to define
some way to communicate with that thread,

00:17:36.420 --> 00:17:40.850
tell that thread to do things with that core data store.

00:17:40.849 --> 00:17:45.439
So I've you know, you can avoid
some of the thread safety issues,

00:17:45.440 --> 00:17:50.009
and some of the threading issues
by doing that kind of thing.

00:17:50.009 --> 00:17:58.519
But you need to then define the communication channel,
because you've built that wall between you know,

00:17:58.519 --> 00:18:02.480
the rest of your app and that particular subsystem.

00:18:02.480 --> 00:18:09.779
So again, you know, another reason to use
partitioning is to make use of multiple cores,

00:18:09.779 --> 00:18:15.000
but also potentially to help you increase
the thread safety at the same time.

00:18:15.000 --> 00:18:19.619
That is if you make things, if
you can make things independent,

00:18:19.619 --> 00:18:26.099
then you don't have the particular thread
safety issues that generally arise.

00:18:26.099 --> 00:18:33.379
The primary reason for thread safety issues is of
course, you know, if you were at my talk last year,

00:18:33.380 --> 00:18:43.540
or have you know, read books on the subject,
changes to shared data on multiple threads.

00:18:43.539 --> 00:18:48.440
So if you avoid changes to shared data on multiple threads,

00:18:48.440 --> 00:18:55.680
you can avoid a lot of the thread,
threading issues that arise.

00:18:55.680 --> 00:19:04.380
I talked so far about a lot of examples of
communicating between multiple processes.

00:19:04.380 --> 00:19:10.330
Let me give a few examples of communicating when
you have multiple threads within the same process.

00:19:10.329 --> 00:19:16.029
Well the first, and very simplest
communication channel is of course memory.

00:19:16.029 --> 00:19:21.399
You can store something in memory, and another thread
can come along and read memory, and look that up.

00:19:21.400 --> 00:19:22.410
The threads share memory.

00:19:22.410 --> 00:19:28.660
More formalized abstractions include queues for example.

00:19:28.660 --> 00:19:37.560
If you have a producer and a consumer thread, they may
communicate via a queue to maintain their independence.

00:19:37.559 --> 00:19:42.399
The queue then becomes the communication
channel between them.

00:19:42.400 --> 00:19:50.259
Arguments or initial data, and return values or
result data can also be a form of communication.

00:19:50.259 --> 00:19:56.890
That is if you create a new thread and you pass it you
know, a selector to run, and the argument to run with,

00:19:56.890 --> 00:20:03.480
you're passing it its initial data, and
that can be a way to, that is, not can be,

00:20:03.480 --> 00:20:07.519
it is a way to communicate with that other thread.

00:20:07.519 --> 00:20:12.660
The inter-process communication mechanisms
I already mentioned usually work here too,

00:20:12.660 --> 00:20:18.890
so you can use sockets between threads if you want,
you can use distributed objects, you can you know,

00:20:18.890 --> 00:20:21.759
use the file system as a communication channel.

00:20:21.759 --> 00:20:26.109
But those are generally not going to be as efficient.

00:20:26.109 --> 00:20:34.929
We used to recommend distributed objects between threads,
but our recommendation now is not to use distributed objects

00:20:34.930 --> 00:20:42.420
between threads, because it's generally heavier
weight mechanism than the other alternatives.

00:20:42.420 --> 00:20:47.539
That is if distributed objects between threads
works for you that's fine, but you know,

00:20:47.539 --> 00:20:55.980
distributed objects has its own setup, and you
know, behavioral costs that you have to deal with.

00:20:55.980 --> 00:21:03.240
So what I would recommend instead is using the new perform
selector on thread mechanism that we've added in Leopard,

00:21:03.240 --> 00:21:11.240
and that's the bullet right above the big red X. So I have
a big red X there, and I don't want to say don't you know,

00:21:11.240 --> 00:21:18.009
absolutely don't use distributed objects between
threads, it's no longer a recommendation.

00:21:18.009 --> 00:21:22.849
You're better off if you can choosing something
lighter weight, you'll get better speedup

00:21:22.849 --> 00:21:27.099
out of your multiple core multi threading process.

00:21:27.099 --> 00:21:31.469
What are some of the pitfalls in partitioning?

00:21:31.470 --> 00:21:35.079
I warned you I would talk about
this at the beginning of the talk.

00:21:35.079 --> 00:21:39.769
Well the first is performance can be a pitfall

00:21:39.769 --> 00:21:45.210
The extra layering and communication
that's involved in communicating

00:21:45.210 --> 00:21:51.519
between these independent parts can slow things down.

00:21:51.519 --> 00:21:54.460
That's interesting.

00:21:54.460 --> 00:21:59.829
It's, the text is clipped for me, it's not for you.

00:21:59.829 --> 00:22:06.879
If you're you know, having to serialize some
objects into some sort of byte representation,

00:22:06.880 --> 00:22:13.870
send it across to another process which unpacks that,
and then you know, you know, recreates all the objects,

00:22:13.869 --> 00:22:17.409
obviously that extra layering involves overhead.

00:22:17.410 --> 00:22:23.950
You're not simply messaging the objects, you're
going through this extra transportation layer.

00:22:23.950 --> 00:22:28.480
Responsiveness though might increase
with the use of additional cores.

00:22:28.480 --> 00:22:37.390
So if you're doing multi threading, and you create
you know, operations, or you create threads,

00:22:37.390 --> 00:22:46.930
or whatever mechanism you happen to choose,
the overall efficiency I'll say might decrease.

00:22:46.930 --> 00:22:51.910
That is it costs you more to be
multi threaded in any context.

00:22:51.910 --> 00:23:00.450
However, getting to the end goal by using
multiple cores is faster, generally.

00:23:00.450 --> 00:23:07.660
So that is the reason for using the multiple cores to
say split up work or do something, things in parallel.

00:23:07.660 --> 00:23:09.880
You get to the end result faster.

00:23:09.880 --> 00:23:16.450
But overall, if you summed up you know, all
of the cost, the overall efficiency is less.

00:23:17.549 --> 00:23:22.539
A second pitfall in partitioning
is what I'm calling versionitis.

00:23:22.539 --> 00:23:30.319
When you have a communication channel, the senders and
the receivers have to agree on the communication protocol.

00:23:30.319 --> 00:23:34.809
If you write some bytes out to a socket,
and the other side reads those bytes,

00:23:34.809 --> 00:23:38.089
the other side has to know how to interpret those bytes.

00:23:38.089 --> 00:23:46.000
And so if you change things in the sender, in your
application, and recompile, and rerun, but you don't rerun,

00:23:46.000 --> 00:23:54.930
recompile and change you know, and recompile your server
and re-launch it, then you know, to match of course,

00:23:54.930 --> 00:23:58.070
then of course you have a communication problem.

00:23:58.069 --> 00:24:02.659
Because the other side is not going to interpret
those bytes correctly, or it's not going to know how

00:24:02.660 --> 00:24:06.350
to interpret those bytes that you've sent.

00:24:06.349 --> 00:24:10.069
So that's an example of versionitis.

00:24:10.069 --> 00:24:17.129
Changes have to be coordinated on both sides,
which adds to the complexity of things.

00:24:17.130 --> 00:24:19.690
A third, and the final example I'm going to give,

00:24:19.690 --> 00:24:24.680
of pitfalls and partitioning is what
I call execution context dependencies.

00:24:24.680 --> 00:24:31.799
And I think this is my favorite, because
it's, can be very, very insidious.

00:24:32.859 --> 00:24:38.009
Code may expect to run within a
certain process or in a certain thread.

00:24:38.009 --> 00:24:41.549
That is it is dependent on its context.

00:24:41.549 --> 00:24:47.680
When you write the code, you may
unintentionally make assumptions that for example,

00:24:47.680 --> 00:24:52.220
the code may assume it can access the NS application object.

00:24:52.220 --> 00:24:58.450
Well if you take that code, and you put it off in a separate
process, and it tries to access the application object,

00:24:58.450 --> 00:25:04.410
well you know, maybe you got a link failure or some compiler
warnings that you dealt with, and realized it at that point.

00:25:04.410 --> 00:25:09.120
But if it does manage to talk to some
application object in that other process,

00:25:09.119 --> 00:25:16.899
it's obviously not the same application object that it was
used to talking to when the code was in your application.

00:25:16.900 --> 00:25:20.660
So that's an example of context dependence.

00:25:20.660 --> 00:25:23.330
Another example would be per thread data.

00:25:23.329 --> 00:25:29.879
If some code runs, and it puts something into per
thread data, and then you know, other things run,

00:25:29.880 --> 00:25:37.680
and later the code runs again, that function or that method
runs again, and it expects to find that same per thread data

00:25:37.680 --> 00:25:44.519
that it put in there before in the per thread
data data store, it may not if the thread,

00:25:44.519 --> 00:25:48.019
if that function is now running on a different thread.

00:25:48.019 --> 00:25:52.750
Let me give you an example.

00:25:52.750 --> 00:25:59.579
We had an engineer at Apple who wanted to
load the plug-ins that his app was running,

00:25:59.579 --> 00:26:03.409
or loading, at launch, all from a separate thread.

00:26:03.410 --> 00:26:08.430
And the hope was well I'll increase launch time if
I you know, put these off in a background thread,

00:26:08.430 --> 00:26:15.610
then my main thread you know, can do the other launch
activities, and get some performance out of that.

00:26:15.609 --> 00:26:16.409
Well he did that.

00:26:16.410 --> 00:26:22.360
And what he found was well those, nothing
happened then with those plug-ins,

00:26:22.359 --> 00:26:29.809
because those plug-ins were accessing the current thread's
run loop, and installing their own run loop sources.

00:26:29.809 --> 00:26:36.859
Well so when he put the loading of the plug-ins off on
a background thread, they were installing their sources

00:26:36.859 --> 00:26:41.629
on the background thread's run loop, which
of course the background thread went away

00:26:41.630 --> 00:26:44.900
after the loading was done, you
know, it didn't stick around.

00:26:44.900 --> 00:26:51.920
And so the plug-ins had installed their sources, but
that got torn down when the thread got torn down,

00:26:51.920 --> 00:26:54.710
and the plug-ins were not able to get any input.

00:26:54.710 --> 00:27:02.170
So he had to go back and put the plug-ins back
in the main thread, the loading of the plug-ins.

00:27:02.170 --> 00:27:10.550
So this you know, illustrates how context
dependence can be really invisible, really insidious,

00:27:10.549 --> 00:27:17.829
until you've actually gone and done the work,
and then find out maybe oh, I can't do that.

00:27:17.829 --> 00:27:23.720
So if you remember back at the start
of the talk, we had this slide.

00:27:23.720 --> 00:27:32.400
And I talked about there being two aspects to partitioning,
segregation and communication between the parts.

00:27:32.400 --> 00:27:35.790
Let's talk a little bit about segregation.

00:27:35.789 --> 00:27:44.170
So multi, multiple processes is one way
to do segregation, or to get segregation.

00:27:44.170 --> 00:27:51.070
The only way to handle multiple platform
architectures is to use multiple processes.

00:27:51.069 --> 00:27:55.059
Those binary choices I talked about,
32-bit versus 64-bit,

00:27:55.059 --> 00:28:01.519
garbage collected versus non-garbage
collected, are all process wide, properties,

00:28:01.519 --> 00:28:10.629
and so you have to use multiple processes
if you want to use multiple of those.

00:28:10.630 --> 00:28:11.430
Let's see.

00:28:11.430 --> 00:28:17.490
I already talked about you know, multiple processes
giving you more address space to work with.

00:28:17.490 --> 00:28:22.000
You know, suppose your app cannot
be 32-bit at this point.

00:28:22.000 --> 00:28:28.569
You may be able to create a helper process to do you
know, the heavy work where you need 64-bit,

00:28:28.569 --> 00:28:35.409
and so you can communicate with the 64-bit process
to do that specific work that needs to be 64-bit.

00:28:35.410 --> 00:28:42.090
But your app itself doesn't have to be
converted yet to be 64-bit capable.

00:28:42.089 --> 00:28:46.799
However, when you have multiple
processes, getting the data back and forth,

00:28:46.799 --> 00:28:51.029
communicating between them is more labor intensive.

00:28:51.029 --> 00:28:57.039
It requires more code for you to write, but
it is also more costly in terms of CPU usage.

00:28:57.039 --> 00:29:02.920
And you get less intimate access you
know, there's of course less sharing.

00:29:02.920 --> 00:29:10.440
And that's part of the point of partitioning, that
there's less sharing between the multiple pieces.

00:29:12.140 --> 00:29:17.400
When you need the other process,
you have to go and start it.

00:29:17.400 --> 00:29:24.920
We recommend of course, as we do every year, that
you use the highest level API that suits your needs.

00:29:24.920 --> 00:29:34.240
NS task and NS workspace can both be used,
their Cocoa APIs to start other applications.

00:29:34.240 --> 00:29:38.579
Launch services is what a lot of NS workspace is based on.

00:29:38.579 --> 00:29:44.759
If you need to drop down and you know, get some of the
extra capabilities and launch services, you can do that.

00:29:44.759 --> 00:29:49.150
In Leopard the core OS guys have
added posick (assumed spelling) spawn,

00:29:49.150 --> 00:29:55.650
which is a call to create a process directly
without going through the fork and exec pair.

00:29:55.650 --> 00:30:00.570
But of course as you know, in other Unix
systems you can still call fork and exec.

00:30:00.569 --> 00:30:07.899
And of course there's launch D, too, which is
often appropriate if you have a server situation,

00:30:07.900 --> 00:30:11.790
to launch that server when you first need it.

00:30:11.789 --> 00:30:16.970
And then you have to communicate using one
of the techniques I talked about before.

00:30:16.970 --> 00:30:24.289
Maybe you've provided arguments, or maybe you
used pipes and sockets, or something else.

00:30:24.289 --> 00:30:32.470
Multiple threads is a second form of part, segregation,
partitioning that I'm going to talk about in this talk.

00:30:32.470 --> 00:30:39.839
It's often simpler to start up and shut down threads than
it is processes, so that's one benefit of using threads.

00:30:39.839 --> 00:30:42.559
And it's easier and faster to communicate back and forth,

00:30:42.559 --> 00:30:48.200
because of course they're sharing a memory
address space, and so you can just use memory.

00:30:48.200 --> 00:30:54.910
But using multiple threads requires more
discipline in order to maintain the segregation.

00:30:54.910 --> 00:31:01.440
If you're going to use multiple threads
within Cocoa, we say just use NS thread.

00:31:01.440 --> 00:31:09.650
NS thread has been improved in Leopard to be much more
sub-classable, if you actually want to sub-class NS thread,

00:31:09.650 --> 00:31:12.900
and we've given it some more methods as well.

00:31:12.900 --> 00:31:22.360
To communicate between the threads you have to you know,
use one of the communication channels I've talked about.

00:31:22.359 --> 00:31:28.219
The best way is to give the thread, or
other objects that will be doing the work

00:31:28.220 --> 00:31:32.500
on that thread, all of the data up front.

00:31:32.500 --> 00:31:38.809
But you can also do inter-thread communication
if you need to do some communication later.

00:31:38.809 --> 00:31:48.839
Or if objects are thread-safe, each of the threads of
course can talk directly to those thread-safe objects.

00:31:48.839 --> 00:31:59.789
NS operation is a new API in Leopard that is
one form of partitioning, or encapsulation,

00:31:59.789 --> 00:32:03.819
computation encapsulation maybe I should say,

00:32:03.819 --> 00:32:15.879
that we have invented to help you do
multi threading, what would be the word?

00:32:15.880 --> 00:32:17.010
Programming.

00:32:17.009 --> 00:32:21.500
>> Multi threading for dummies?

00:32:22.549 --> 00:32:30.859
>> Well no, it's not multi threading for dummies, but it's
a, the point of a NS operation is to provide an abstraction

00:32:30.859 --> 00:32:38.809
around which you can you know, think about
your multiple, use of multiple threads.

00:32:41.700 --> 00:32:48.410
When you're going to use an NS operation, what you do
is you generally create an NS operation sub-class.

00:32:48.410 --> 00:32:56.080
And in the simple case you simply have to
override its main method, the method called main.

00:32:56.079 --> 00:33:07.109
We provide an NS operation, sorry, NS invocation
operation sub-class of NS operation, which you know,

00:33:07.109 --> 00:33:13.799
has the capability of taking a target object and a
selector to invoke, and, or an NS invocation to invoke

00:33:13.799 --> 00:33:18.109
on the other thread, if you have some
existing method that you simply want

00:33:18.109 --> 00:33:23.439
to invoke from, in the context of an operation.

00:33:23.440 --> 00:33:29.190
The benefit of using an NS operation is
that you can put them in an operation queue,

00:33:29.190 --> 00:33:35.090
and the operation queue will create
threads for you, and it will start jobs,

00:33:35.089 --> 00:33:39.750
and run you know, a few jobs at a time, hopefully.

00:33:39.750 --> 00:33:45.359
The intent is that we're running
enough jobs to keep the CPUs busy.

00:33:45.359 --> 00:33:48.809
But you can also start NS operations manually.

00:33:48.809 --> 00:33:50.139
You know, it's just an API.

00:33:50.140 --> 00:33:56.210
You could create one for example, and pass it
off to some other sub-system in your application,

00:33:56.210 --> 00:33:59.900
and that other sub-system when it was ready
for that operation to run could you know,

00:33:59.900 --> 00:34:04.190
start it manually at that time, or put it in a queue.

00:34:05.309 --> 00:34:09.019
The key thing is that NS operations must be KVO compliant.

00:34:09.019 --> 00:34:14.920
And as long as they're KVO compliant, they
can be put into an NS operation queue.

00:34:14.920 --> 00:34:21.650
Simple operations, those that just override main,
have all the KVO compliance taken care of for them

00:34:21.650 --> 00:34:27.470
by the NS operation abstract super class.

00:34:27.469 --> 00:34:32.730
Once you have an operation that's
running, or have an operation,

00:34:32.730 --> 00:34:36.670
again the best thing to do is to
give it all its parameters up front.

00:34:36.670 --> 00:34:47.889
For example, if you had an operation object which was you
know, the intent was to say warp an image in some way,

00:34:47.889 --> 00:34:52.349
the best thing to do is to give it the
image when you create the NS operation.

00:34:52.349 --> 00:35:01.039
You might give the NS operation sub-class an init with image
initializer for example, give it all the data up front,

00:35:01.039 --> 00:35:05.099
then it can go off later and you know, play with that data.

00:35:05.099 --> 00:35:10.380
But of course you can do interthread communication,
or message thread-safe objects directly.

00:35:10.380 --> 00:35:17.740
And I'm going to point you to the documentation, which is
fairly complete in your Leopard seed to learn more

00:35:17.739 --> 00:35:20.879
about NS operation, and NS operation queue.

00:35:20.880 --> 00:35:26.849
The second aspect of partitioning
was the communication channel.

00:35:26.849 --> 00:35:33.489
The purpose of the communication channel is to act as a
mediary between the independent parts

00:35:33.489 --> 00:35:38.989
in order to help maintain the separation
independence of those parts.

00:35:38.989 --> 00:35:46.309
Usually communication is done via one way messages,
and usually it's between simply two endpoints.

00:35:46.309 --> 00:35:53.980
If you want to have any sort of reply or return value,
that's usually implemented with a second one way message

00:35:53.980 --> 00:36:01.329
that goes back to the original sender, or original
thread, original process, whatever it happens to be.

00:36:01.329 --> 00:36:07.829
The sender, once it sends a one way message, usually
immediately continues with you know, doing other work.

00:36:07.829 --> 00:36:15.559
But if the sender needs to get a reply back from
the other side, the sender can immediately choose

00:36:15.559 --> 00:36:17.969
to block, waiting for a response message.

00:36:17.969 --> 00:36:21.099
If you use DO, this is exactly what DO is doing.

00:36:21.099 --> 00:36:25.339
It sends a one way message off, and
then immediately chooses to block,

00:36:25.340 --> 00:36:29.019
waiting for the reply when you have a two way message.

00:36:29.019 --> 00:36:42.530
Beta containment handoff is an, a particular topic within
communication channel, the communication channel subject

00:36:42.530 --> 00:36:48.600
that I want to call out here as you
know, something worthy of your attention

00:36:48.599 --> 00:36:52.190
as you think about partitioning your application.

00:36:52.190 --> 00:37:02.970
The point of data containment and handoff is to keep objects
from being referenced outside of the independent parts.

00:37:02.969 --> 00:37:10.319
That is if the parts are independent, it's nice to
hide as much as you can within those independent parts

00:37:10.320 --> 00:37:15.410
to keep other parts from unintentionally
getting around your communication channel

00:37:15.409 --> 00:37:19.210
in some way, and talking to them directly.

00:37:19.210 --> 00:37:25.610
So the point is to hide references if you can
within the separate communication channels.

00:37:25.610 --> 00:37:31.960
The best way to do this is when you send information to
the other side, you send it and then you forget about it.

00:37:31.960 --> 00:37:39.630
It's sort of like when you have a relay race, and
one runner hands off the baton to the next runner.

00:37:39.630 --> 00:37:45.349
At one point one runner has it, and at
another point the other runner has it.

00:37:45.349 --> 00:37:49.360
The goal is to not access objects
or data from multiple threads.

00:37:49.360 --> 00:37:54.849
When you have sharing, you have access
from multiple threads to the same data,

00:37:54.849 --> 00:38:01.289
you have the potential for the usual
multi threading issues that arise.

00:38:01.289 --> 00:38:09.980
When references to objects do get exposed across a
communication channel, that's called reference leakage.

00:38:09.980 --> 00:38:15.199
So there are many forms of reference
leakage, and some of them are very subtle.

00:38:16.820 --> 00:38:21.860
When you put objects in global memory,
that can be a form of reference leakage,

00:38:21.860 --> 00:38:25.760
because another thread could come along
and read that reference out of memory.

00:38:25.760 --> 00:38:30.010
An example of this is the NS notification center.

00:38:30.010 --> 00:38:38.130
If you add an object as an observer to the NS
notification center, that has put it into global memory.

00:38:38.130 --> 00:38:44.740
And if any thread posts notifications
that that observer has registered for,

00:38:44.739 --> 00:38:47.519
those other threads will be messaging that object.

00:38:47.519 --> 00:38:54.300
And you may not expect that object to be being messaged
from multiple threads, but if you've put it in,

00:38:54.300 --> 00:39:01.010
any observer within the notification center
may receive messages on multiple threads.

00:39:01.010 --> 00:39:11.500
If you give the object that you want to keep hidden to a
non-contained object, you know, an object which is not part

00:39:11.500 --> 00:39:15.239
of that independent piece, then you may have leaked it.

00:39:15.239 --> 00:39:18.669
You know, an example would just be like a set method.

00:39:18.670 --> 00:39:23.670
If you set it in another object, you may have leaked it.

00:39:23.670 --> 00:39:29.769
Because other objects could get the
reference from that non-contained object.

00:39:31.099 --> 00:39:38.389
One example of this is suppose you're going to
send an array to the, to another thread.

00:39:38.389 --> 00:39:45.759
And so what you do is you say well I'm going to copy
the array and give the copy to the other thread.

00:39:45.760 --> 00:39:52.940
But the copy may not have copied all the objects
inside of the array, and so all of those objects,

00:39:52.940 --> 00:39:56.010
references now, have been leaked to that other thread.

00:39:56.010 --> 00:40:01.970
That other thread will access the array, and all
of the objects inside of it are now, you know,

00:40:01.969 --> 00:40:06.219
the references to them are shared between those two threads.

00:40:06.219 --> 00:40:12.689
And finally, another form of reference leakage is to
simply pass an argument and a message to some sub-system.

00:40:12.690 --> 00:40:18.599
You may not know what that other method
is going to do to that object reference,

00:40:18.599 --> 00:40:23.420
and so that may be a vector for leaking references.

00:40:23.420 --> 00:40:30.710
And so that's you know, this is a much deeper topic
really than I can go into, but I'm trying to give you sort

00:40:30.710 --> 00:40:35.230
of a sense of some of the complexity that can arise here.

00:40:35.230 --> 00:40:43.860
When you have multi threading and a communication channel
between multiple threads, the channel should be thread-safe.

00:40:43.860 --> 00:40:48.940
The independent parts potentially
do not have to be thread-safe.

00:40:48.940 --> 00:40:54.659
The point of the segregation is that you know,
you're not sharing data or anything across them,

00:40:54.659 --> 00:40:59.190
but the channel itself does need to be thread-safe usually.

00:40:59.190 --> 00:41:02.250
Channels used between processes may not have

00:41:02.250 --> 00:41:08.909
to be thread-safe unless you have multiple threads
using the same channel to get to another process.

00:41:10.150 --> 00:41:15.420
Okay, finally, finally we're going to
get to some data, some examples.

00:41:15.420 --> 00:41:21.430
My particular example here, I'm going to
focus on using the new perform selector

00:41:21.429 --> 00:41:26.259
on thread mechanism that has been added in Leopard to Cocoa.

00:41:26.260 --> 00:41:33.790
So this method takes four arguments, the selector
to invoke the thread on which to invoke it,

00:41:33.789 --> 00:41:39.570
an argument object for the method,
and a wait until done parameter

00:41:39.570 --> 00:41:46.390
which specifies whether you want the perform
method to block until the method has been performed

00:41:46.389 --> 00:41:50.519
in the other thread, or return immediately.

00:41:50.519 --> 00:41:54.469
And this can be used to send one
way messages back and forth.

00:41:54.469 --> 00:42:01.859
So you, what you do is you, when you have a method
you want an object to perform, you send that object,

00:42:01.860 --> 00:42:07.220
the target object the perform method,
and that will become then the receiver

00:42:07.219 --> 00:42:11.639
in that other, the context of that other thread.

00:42:13.190 --> 00:42:20.619
Now perform selector of course is very useful, and many
of you may be thinking oh finally they've added this.

00:42:20.619 --> 00:42:28.659
You know, they've had perform on main thread for many
releases now, but I haven't been able to target my methods

00:42:28.659 --> 00:42:32.170
to a specific thread, what if it's not the main thread.

00:42:32.170 --> 00:42:35.730
But there are a couple potential issues with this.

00:42:35.730 --> 00:42:41.969
The first is that you have to know that
you need to use perform selector on thread.

00:42:41.969 --> 00:42:45.269
That is you have to know you can't
just message the object directly

00:42:45.269 --> 00:42:47.969
with obscene messaging
as you normally would.

00:42:47.969 --> 00:42:54.029
You have to know that that object only wants
to receive messages on that other thread.

00:42:54.030 --> 00:42:55.840
So that's the first issue.

00:42:55.840 --> 00:42:58.740
The second issue is that second argument up there.

00:42:58.739 --> 00:42:59.739
The thread.

00:42:59.739 --> 00:43:03.629
You have to know what thread to send the message on.

00:43:03.630 --> 00:43:07.150
The third issue is the argument.

00:43:07.150 --> 00:43:08.769
There's only one of them.

00:43:08.769 --> 00:43:13.730
So if you have multiple arguments that
you need to get to that other method,

00:43:13.730 --> 00:43:18.949
what you end up having to do is creating a package
of the arguments, and create a cover method

00:43:18.949 --> 00:43:26.329
which can unpack the arguments, and then send a
real message in the context of that other thread.

00:43:26.329 --> 00:43:31.670
And the fourth issue of course, is
the versionitis that can happen.

00:43:31.670 --> 00:43:36.269
The senders all need to know how to
package up the arguments in a way

00:43:36.269 --> 00:43:44.619
that matches what the receiver
object is going to do in unpacking them.

00:43:44.619 --> 00:43:50.650
One solution is what I'm going to call a bridge
object, and you could also call this a proxy object.

00:43:50.650 --> 00:43:56.289
It may sound like a proxy object, but I'm
going to use the term bridge in this case.

00:43:56.289 --> 00:44:01.199
The point of the bridge will be
to hide all of that knowledge

00:44:01.199 --> 00:44:04.500
that I just talked about, all that
knowledge that was needed.

00:44:04.500 --> 00:44:10.000
It's going to hide the actual destination
thread, and potentially the destination object,

00:44:10.000 --> 00:44:17.250
it's going to hide the packing and unpacking work that
needs to go on if there are multiple arguments.

00:44:17.250 --> 00:44:21.329
So this is just you know, object oriented encapsulation.

00:44:21.329 --> 00:44:26.130
I've had a, I have a bunch of information, a
bunch of policy, whatever you want to call it,

00:44:26.130 --> 00:44:34.599
and so I've created a class to encapsulate all
of that knowledge and work that needs to go on.

00:44:34.599 --> 00:44:42.599
This bridge object then is going to be this, all
the senders' endpoint in the communication channel,

00:44:42.599 --> 00:44:46.429
to get to that other object in that other thread.

00:44:46.429 --> 00:44:51.399
They're going to talk to the bridge object, rather
than to talk to that other object directly.

00:44:51.400 --> 00:44:57.160
And of course this applies equally to inter-process
communication, where if you're communicating bytes

00:44:57.159 --> 00:45:04.639
over a socket, you talk, it'd be very convenient
to encapsulate all the serialization work,

00:45:04.639 --> 00:45:11.309
and the unserialization work on the other side into a
bridge object that you know, knows about those things.

00:45:11.309 --> 00:45:16.199
So every send point does not have
to replicate that information.

00:45:16.199 --> 00:45:24.199
In my particular case I'm talking between two threads,
and I'm going to use perform on thread to talk between them.

00:45:24.199 --> 00:45:30.149
As a twist, I am going to now eliminate this bridge object

00:45:30.150 --> 00:45:33.769
that I just talked about, in one
sense I'm going to eliminate it.

00:45:33.769 --> 00:45:41.820
That is I have an object which only wants to process
its messages in the context of another thread.

00:45:41.820 --> 00:45:46.200
It sort of wants to only be used on that other thread.

00:45:46.199 --> 00:45:48.849
But it has a public API.

00:45:48.849 --> 00:45:57.380
So what the object is going to do is it's going to bridge
to itself, and so it's going to act as its own bridge.

00:45:59.260 --> 00:46:02.410
What's going to happen is the object
is going to receive messages

00:46:02.409 --> 00:46:07.210
from various senders in the source side on the left.

00:46:07.210 --> 00:46:14.909
The object is going to tell itself to perform a slightly
different internal private message on its thread,

00:46:14.909 --> 00:46:20.000
it knows about the thread of course that it wants to run on.

00:46:20.000 --> 00:46:27.159
The perform queue is our communication channel in this
case, and it eventually is going to deliver the message,

00:46:27.159 --> 00:46:31.190
and the object is going to do the work on that other thread.

00:46:31.190 --> 00:46:35.400
So that's you know, fairly straight
forward, should be fairly straight forward.

00:46:35.400 --> 00:46:39.780
So I have a method here in the API
of this bridge object called do work,

00:46:39.780 --> 00:46:45.080
and it returns one way void to
indicate that it's asynchronous.

00:46:45.079 --> 00:46:49.900
It's going to call perform selector on itself, and tell itself

00:46:49.900 --> 00:46:54.829
to do this private internal method
called ox do work in this case.

00:46:54.829 --> 00:47:01.389
It's going to send that, it's going to ask that
that message be sent to itself on the my thread,

00:47:01.389 --> 00:47:07.980
which is presumably some sort of instance variable of
the object, and it's going to say wait until done no.

00:47:07.980 --> 00:47:11.519
So this is going to return immediately.

00:47:11.519 --> 00:47:18.829
Eventually the other thread will, the perform
queue on that other thread will call the ox do work

00:47:18.829 --> 00:47:24.239
on that object, and the object will do its work.

00:47:24.239 --> 00:47:31.309
If you have multiple arguments, the changes
in gold here are what change in that process.

00:47:31.309 --> 00:47:37.239
I've given the do work method an integer
argument, and what's going to happen is

00:47:37.239 --> 00:47:41.149
that I'm going to use an NS dictionary
to package up the arguments.

00:47:41.150 --> 00:47:42.430
So that's fairly straight forward, you know.

00:47:42.429 --> 00:47:47.250
The dictionary will have the string
object, and the integer argument has values

00:47:47.250 --> 00:47:49.780
for some well defined keys that I'll define.

00:47:49.780 --> 00:47:56.590
I'm going to pass that argument dictionary as the
argument to the perform selector on thread method.

00:47:56.590 --> 00:48:02.070
And the perform selector on thread method
by the way, retains the target object,

00:48:02.070 --> 00:48:08.140
and it retains the argument object until the
method has been performed by that other thread.

00:48:08.139 --> 00:48:12.889
So my dictionary, which has auto
released in this case let's say, is safe,

00:48:12.889 --> 00:48:18.659
it's held by the perform queue
until that method has been invoked.

00:48:18.659 --> 00:48:25.389
Eventually the method is invoked on the other thread,
and it gets the dictionary as its argument this time.

00:48:25.389 --> 00:48:29.920
First it has to unpack the arguments, but
then it proceeds and does its you know,

00:48:29.920 --> 00:48:33.950
the work that it did before in our previous example.

00:48:33.949 --> 00:48:38.710
So that has solved most of our sending issues.

00:48:38.710 --> 00:48:40.920
What about return values?

00:48:40.920 --> 00:48:47.780
Suppose I want to effect, have the effect
of return values or reply messages,

00:48:47.780 --> 00:48:53.940
but communicate via perform selector on thread.

00:48:53.940 --> 00:49:01.760
Another issue that I'll point out at this point is
that the sender may need the result of a computation,

00:49:01.760 --> 00:49:08.300
and may need to wait in some way for that reply
message, that reply information to come back.

00:49:08.300 --> 00:49:12.510
And so we'd be looking at both of those things.

00:49:12.510 --> 00:49:19.840
Well the first solution is to use two
perform selector on thread invocations,

00:49:19.840 --> 00:49:23.269
two one way messages as I indicated before.

00:49:23.269 --> 00:49:30.829
In the first case you, you know, start, and the
first object, object A, and you send it a message.

00:49:30.829 --> 00:49:33.679
And then the object might you know, continue on doing work,

00:49:33.679 --> 00:49:36.639
or it might have to block, waiting
for the reply immediately.

00:49:36.639 --> 00:49:45.339
And eventually the other object sends a message
back in some way using perform selector on thread.

00:49:45.340 --> 00:49:50.600
Well, this has some of the same
sending issues that I started with.

00:49:50.599 --> 00:49:58.599
Now, it's the receiver object, that remote object
in that other thread, yeah, sits on the right,

00:49:58.599 --> 00:50:03.719
which has to send messages back to the original object.

00:50:03.719 --> 00:50:10.859
And so it has to know what thread the original object wants
those messages on, it has to know what the selector is

00:50:10.860 --> 00:50:17.579
to send, and basically it has to
bridge back to the original sender.

00:50:17.579 --> 00:50:24.650
So using two one way messages, you have to
do bridging then in the both directions.

00:50:24.650 --> 00:50:29.940
The sender may eventually have to
wait for the return perform to occur

00:50:29.940 --> 00:50:34.019
on its thread, and so it needs to wait in some way.

00:50:34.019 --> 00:50:40.500
A second solution is a little bit better.

00:50:40.500 --> 00:50:45.469
In this case we're going to have
the bridge object do the waiting,

00:50:45.469 --> 00:50:51.269
and get the return value back to the original thread.

00:50:51.269 --> 00:50:56.079
So what I'm going to do is I'm going to say this
do work method say returns an NS string.

00:50:56.079 --> 00:50:58.529
And so I've changed the return value to NS string.

00:50:58.530 --> 00:51:06.490
And this method, although it's going to do the work in the
other thread, is going to return the reply information,

00:51:06.489 --> 00:51:13.549
the return value, the result string, whatever
you want to call it as its return value.

00:51:13.550 --> 00:51:19.360
So what I'm going to do is I'm going to allocate some
memory on the stack, the local stack of this thread

00:51:19.360 --> 00:51:27.829
in which the result is going to be stored, I'm going to create
an NS value with the pointer to that storage location,

00:51:27.829 --> 00:51:35.569
and I'm going to package that return, that NS value up with
the arguments in the NS dictionary that I'm going to send

00:51:35.570 --> 00:51:39.630
over to the other side, just as in my previous example.

00:51:39.630 --> 00:51:43.840
This time I'm going to say wait until done yes.

00:51:43.840 --> 00:51:52.000
So the perform selector method here is going to block
until the method has been performed in the other thread,

00:51:52.000 --> 00:51:56.690
and the method invocation is finished in the other thread.

00:51:56.690 --> 00:51:59.460
So eventually the other thread will execute the method,

00:51:59.460 --> 00:52:05.079
and it has to unpack the arguments and
the result address, and do the work.

00:52:05.079 --> 00:52:12.440
As its last act, it's going to take that
address, it's going to extract the address

00:52:12.440 --> 00:52:17.789
out of the value, and store the result string in there.

00:52:17.789 --> 00:52:22.730
It's going to give the result string also, and retain.

00:52:22.730 --> 00:52:32.659
So its given the retain to keep that NS string result
object alive until the other thread can get at it.

00:52:32.659 --> 00:52:39.759
So what I've done basically here is, the other thread,
the background thread has written into the stack

00:52:39.760 --> 00:52:46.090
at the storage location I told
it to, the result string object.

00:52:46.090 --> 00:52:52.079
So eventually the perform selector method then,
which has blocked, will return because the method

00:52:52.079 --> 00:52:59.860
in the other thread returns, and the result on
the stack has been filled in by that other thread.

00:52:59.860 --> 00:53:05.570
So what I'm going to do is I'm simply going to return
it with an auto release so that it becomes you know,

00:53:05.570 --> 00:53:14.000
auto released in the context of this thread, just like
any other API, you know, it normally returns an NS string.

00:53:14.000 --> 00:53:19.820
So that solution works pretty well, and
pretty neatly has solved the problem

00:53:19.820 --> 00:53:21.950
of getting the information back to theg thread.

00:53:21.949 --> 00:53:28.379
It's all been encapsulated quite
nicely within the bridge object itself.

00:53:28.380 --> 00:53:31.050
But we've lost something.

00:53:31.050 --> 00:53:36.830
We've lost the potential for parallel work.

00:53:36.829 --> 00:53:42.079
In the first example, the method was
sent off to the background thread,

00:53:42.079 --> 00:53:45.279
and then the original sender could continue on and do work.

00:53:45.280 --> 00:53:50.790
And that kind of parallelism might
get you a performance increase.

00:53:50.789 --> 00:54:00.259
But we've lost that here because when you call
the do work method, the sender then has to block,

00:54:00.260 --> 00:54:07.750
waiting for that return result, and the blocking of
course is done by the wait until done yes argument.

00:54:08.800 --> 00:54:15.340
So a third solution will allow us
to restore that asynchronicity,

00:54:15.340 --> 00:54:19.100
and that is to use what I'm going to call a future value.

00:54:19.099 --> 00:54:23.940
A future value is simply an object
which holds another value.

00:54:23.940 --> 00:54:28.220
It's like an NS value but it's mutable.

00:54:28.219 --> 00:54:35.059
What the future value's going to do is if you
try to get the value out of the future value,

00:54:35.059 --> 00:54:38.829
it's going to block you if the value hasn't been set yet.

00:54:38.829 --> 00:54:43.889
If the value has been set, it's going to
return the value that's been set right away.

00:54:43.889 --> 00:54:48.489
But if you try to get the value
that hasn't been set, it will block.

00:54:48.489 --> 00:54:54.189
Now this is not a new concept, this
concept goes back thirty years or more.

00:54:54.190 --> 00:55:03.599
It was called eventual values in the literature long
ago, and it's very similar to a concept in parallel,

00:55:03.599 --> 00:55:07.179
or concurrent functional programming called a future.

00:55:07.179 --> 00:55:12.269
Here's what the interface to this
future value might look like.

00:55:12.269 --> 00:55:17.099
Of course there's a getter and a setter, and
there's an IVAR which will store our value.

00:55:17.099 --> 00:55:24.750
And I'm going to use an NS condition lock to
implement the synchronization that's needed.

00:55:24.750 --> 00:55:29.099
So in the bridge, what the bridge
is going to do is it's going

00:55:29.099 --> 00:55:34.099
to return the future, a future
value instead of the NS string.

00:55:34.099 --> 00:55:41.759
And the future value is, acts like a token for
the actual eventual result of the computation.

00:55:41.760 --> 00:55:47.040
So the do work method is going to create
a future value to hold the result,

00:55:47.039 --> 00:55:52.119
and pass that as the arguments in
the dictionary to the other thread.

00:55:52.119 --> 00:55:55.420
We're going to go back to wait until done no.

00:55:55.420 --> 00:56:02.329
So we're going to get back the return immediately
from the perform selector on thread method.

00:56:02.329 --> 00:56:06.139
And we're going to return to future value right away.

00:56:06.139 --> 00:56:12.359
Eventually on the other thread, this internal
private method in the bridge object will be invoked,

00:56:12.360 --> 00:56:16.390
and it will unpack its arguments and do the work as before.

00:56:16.389 --> 00:56:19.449
And instead of writing to the other thread stack,

00:56:19.449 --> 00:56:25.039
it's going to simply call set value on
the future value to store its result.

00:56:25.039 --> 00:56:29.820
So the result is now being stored in the
future value rather than on the stack.

00:56:29.820 --> 00:56:35.690
What does this look like from the
point of view from the caller?

00:56:35.690 --> 00:56:42.309
Well the caller has of course, a handle on the
bridging object, and it calls the do work method there.

00:56:42.309 --> 00:56:48.289
It gets back the future value, and it's going to store it,
it's going to retain it and store it somewhere for later use.

00:56:48.289 --> 00:56:54.170
Because in my example here, the caller
does not need the result right away.

00:56:54.170 --> 00:56:59.099
So it's going to go off, having stored
the value, and do some other stuff.

00:56:59.099 --> 00:57:06.329
Eventually, presumably, it needs the return value, why
else you know, you have the other thread do the work?

00:57:06.329 --> 00:57:07.449
So it needs the return value.

00:57:07.449 --> 00:57:11.639
So it asks the stored future value for its value.

00:57:11.639 --> 00:57:16.199
If the value has been set, that is the
other thread finished its computation,

00:57:16.199 --> 00:57:21.069
this method will return right away
with the NS string result.

00:57:21.070 --> 00:57:31.180
If this value has not been set in the future value, the
value method, the getter will block until it is set.

00:57:31.179 --> 00:57:39.500
Eventually, presumably, the value is set, this method
will return, and the result has been retrieved.

00:57:39.500 --> 00:57:45.860
And at some later point we remember to you
know, release the stored future value as well.

00:57:45.860 --> 00:57:50.200
What does the future value implementation look like?

00:57:50.199 --> 00:57:52.500
That's the most interesting part here.

00:57:52.500 --> 00:57:57.480
Well this implementation that I get
here is not complete in and of itself.

00:57:57.480 --> 00:58:03.340
It's obviously missing a dialic (assumed spelling)
method, and there are a number of things you can do

00:58:03.340 --> 00:58:05.980
to improve this, you know, to do better than this.

00:58:05.980 --> 00:58:11.460
But this is the amount of code that happens to
fit on a slide, so that's what I included here.

00:58:11.460 --> 00:58:16.449
When we initialize the future value, all
we need to do is set up the condition lock.

00:58:16.449 --> 00:58:23.259
So we do that by creating a condition lock,
and initializing it with condition zero.

00:58:23.260 --> 00:58:28.070
What this condition lock is going to
do is act as a simple Boolean latch.

00:58:28.070 --> 00:58:32.360
That is it's going to be no, and
eventually the value's going to be set,

00:58:32.360 --> 00:58:35.760
and then the latch is going to have the true value.

00:58:35.760 --> 00:58:44.550
So zero is going to represent no the value has not been
set, and one will represent yes the value has been set.

00:58:44.550 --> 00:58:54.480
In set value, all the future does is lock the condition
unconditionally, store the value that its given,

00:58:54.480 --> 00:59:01.269
and unlock the condition with value one,
which means yes, now I have the value.

00:59:01.269 --> 00:59:04.679
So the condition lock in this case
is acting also as a Boolean

00:59:04.679 --> 00:59:08.949
to say you know, yes or no, I have the value been set.

00:59:08.949 --> 00:59:17.909
When this unlock condition happens, all the waiters
who are waiting for condition one will wake up.

00:59:17.909 --> 00:59:22.529
What the getters does is lock when condition one.

00:59:22.530 --> 00:59:26.550
That is the getter doesn't want to
proceed unless the value has been set.

00:59:26.550 --> 00:59:30.190
The getter doesn't want to return
something that isn't there yet.

00:59:30.190 --> 00:59:36.530
So the getter potentially blocks here in
lock with condition until the value is set.

00:59:36.530 --> 00:59:39.780
If the condition's already at one, this immediately returns.

00:59:39.780 --> 00:59:48.920
If it isn't at one yet, the condition lock won't allow the
lock, and will block this thread until it has been set.

00:59:48.920 --> 00:59:51.809
We unlock the lock to balance the lock.

00:59:51.809 --> 00:59:54.679
And we simply unlock it, we don't change the condition,

00:59:54.679 --> 01:00:00.539
because of course the getter is not changing
whether the future value has or has not you know,

01:00:00.539 --> 01:00:03.840
gotten the object, it's you know, value yet.

01:00:03.840 --> 01:00:07.019
And we return the value IVAR.

01:00:08.570 --> 01:00:11.070
So fairly straight forward.

01:00:12.750 --> 01:00:18.929
Now that's a very simple sort of
example, but it's a very powerful concept.

01:00:18.929 --> 01:00:24.809
One way to extend it might be that a future value could
instead hold many values, there could be many results.

01:00:24.809 --> 01:00:29.440
You don't have to necessarily package
them all up into a single value.

01:00:29.440 --> 01:00:33.539
And of course there are other ways
to you know, make use of this.

01:00:33.539 --> 01:00:41.940
The future value could be itself passed around
as a token for the eventual return value.

01:00:41.940 --> 01:00:47.280
But until somebody actually needs that
return value, nobody needs to block.

01:00:47.280 --> 01:00:53.960
So it sort of acts as a stand in for the eventual
value that will be computed by that other thread.

01:00:53.960 --> 01:00:59.880
And this has helped restore the
potential for parallelism in the API.

01:00:59.880 --> 01:01:07.220
That is if the caller really does need the return value
right away, it can call the getter that it you know,

01:01:07.219 --> 01:01:12.809
on the future value that it got from the
do work method, and it will block there,

01:01:12.809 --> 01:01:15.099
and you know, wait for the return value to happen.

01:01:15.099 --> 01:01:21.880
But if it doesn't need it right away, it can go off and
do other things waiting for the result to be filled in.

01:01:21.880 --> 01:01:28.599
This kind of blocking though can still be problematic
for the UI, because it's not a type of blocking

01:01:28.599 --> 01:01:32.369
that is running the run loop or
allowing events to be processed.

01:01:32.369 --> 01:01:41.219
So you know, that's one way again, in which potentially
that future value that I presented here could be improved.

01:01:42.300 --> 01:01:44.980
Well let's see.

01:01:44.980 --> 01:01:51.820
I think it was Steve Jobs who introduced the word
wow to us on Monday, so I'll use that again, wow.

01:01:51.820 --> 01:01:54.460
Let's wrap up.

01:01:55.869 --> 01:02:01.250
Partitioning is an important design tool, and
there are two main aspects to partitioning,

01:02:01.250 --> 01:02:06.900
the segregation into independent parts,
and the communication between the parts.

01:02:06.900 --> 01:02:12.139
And there are times when you're going to have to use
partitioning because you simply have to do something

01:02:12.139 --> 01:02:17.289
that can only be done by say using
multiple threads or multiple processes.

01:02:17.289 --> 01:02:27.039
But it can also be used as a you know, voluntarily as a way
to help structure your code, or increase your thread safety.

01:02:27.039 --> 01:02:31.849
We looked at several reasons for
partitioning, and gave several examples.

01:02:31.849 --> 01:02:38.549
And we looked at you know, some issues,
in one example of a communication channel

01:02:38.550 --> 01:02:43.789
which is using the new perform selector on thread mechanism.

01:02:43.789 --> 01:02:51.860
But as always, this talk has simply scratched
the surface of the potentially very deep subject.

01:02:51.860 --> 01:02:57.300
And so the next time you have a problem which
could potentially be solved using partitioning,

01:02:57.300 --> 01:03:03.760
I encourage you to go out and look at some of these
things that I've mentioned in this talk in more depth.

01:03:03.760 --> 01:03:05.510
Thank you.

01:03:05.510 --> 01:03:13.200
( applause )

01:03:13.199 --> 01:03:16.439
Now you've seen this before, but it always bears repeating.

01:03:16.440 --> 01:03:20.119
Deric Horn is the Application Technologies evangelist,

01:03:20.119 --> 01:03:27.799
so he's the evangelist perhaps mostly
responsible for, most responsible for Cocoa.

01:03:27.800 --> 01:03:31.440
And of course the usual documentation link.

01:03:31.440 --> 01:03:40.950
And the documentation in your WWDC seed as I mentioned
before, is fairly complete with respect to NS operation.

01:03:40.949 --> 01:03:46.139
And there's also a nice multi threading document
that the documentation people have written in there.