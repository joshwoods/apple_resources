WEBVTT

00:00:20.010 --> 00:00:28.260
>> I'm here basically because we know that there's a lot
of attendees here at WWDC who this is their first time.

00:00:28.260 --> 00:00:33.000
And so I'm just going to spend maybe about
five minutes giving those of you who are new

00:00:33.000 --> 00:00:35.869
to Core Image a quick overview, very, very quick.

00:00:35.869 --> 00:00:39.669
And then we're going to bring the engineering
guys up to dive very deep on it.

00:00:39.670 --> 00:00:41.890
So the basics of Core Image.

00:00:41.890 --> 00:00:47.149
This is an image processing framework that we first
distributed in Tiger and now it's updated in Leopard.

00:00:47.149 --> 00:00:51.140
And the whole idea is to be able to
do image processing very, very fast.

00:00:51.140 --> 00:00:57.530
And it does this by leveraging the GPU that's shipped
in all the machines that we, that we support in Leopard.

00:00:57.530 --> 00:01:05.280
Under the hood it's actually using OpenGL to load
fragment programs that encapsulate the image filters,

00:01:05.280 --> 00:01:08.060
and the image processing effects that we want to do.

00:01:08.060 --> 00:01:13.659
And so you know, the idea there is that there's a, there's
some of you in the audience who have done image processing,

00:01:13.659 --> 00:01:18.929
you've written it yourself in the past,
and typically done that work in the CPU.

00:01:18.930 --> 00:01:22.520
And so, and perhaps you're doing
something where you're just you know,

00:01:22.519 --> 00:01:26.089
in a four loop running a convolution filter over an image.

00:01:26.090 --> 00:01:32.790
But by moving that onto the GPU, you can essentially kind of
push all of the pixels through in a very parallel fashion,

00:01:32.790 --> 00:01:37.570
and they run on the back end with this fragment program,
and so basically lets us do some very heavy lifting

00:01:37.569 --> 00:01:39.859
for you, and use the GPU to do the hard work.

00:01:39.859 --> 00:01:45.620
Of course the whole pipeline is floating point, it's
color managed, there's a lot of image processing filters

00:01:45.620 --> 00:01:48.130
that are included in the bundle that you get in Leopard.

00:01:48.129 --> 00:01:51.759
And then now also you can extend the base set.

00:01:51.760 --> 00:01:55.380
If you have a custom filter that you
would like to write, you can do that,

00:01:55.379 --> 00:02:00.509
and then you bundle it up in a plug in called a image unit.

00:02:00.510 --> 00:02:05.530
So the basic idea here, just the very, very
basic concept of course is that you're,

00:02:05.530 --> 00:02:08.259
you're doing per pixel operations on an image.

00:02:08.259 --> 00:02:11.329
And so here on the left we have a picture of Copenhagen.

00:02:11.330 --> 00:02:17.040
We're running all of the pixels through a hue
adjustment filter, which is basically just,

00:02:17.039 --> 00:02:20.509
if you think of the color wheel, just sort
of rotating the whole color wheel around.

00:02:20.509 --> 00:02:26.939
And so you know, blues kind of change in this case
to oranges, oranges sort of rotate around to blues.

00:02:26.939 --> 00:02:31.359
But you know, like if you look at the boats for
example, there's a white boat and a black boat,

00:02:31.360 --> 00:02:35.910
and those are staying white and black in the final
image, because you know, what are you doing there?

00:02:35.909 --> 00:02:39.400
You're kind of rotating a gray
around different grays, right?

00:02:39.400 --> 00:02:42.300
So that's what a hue adjustment filter is doing.

00:02:42.300 --> 00:02:46.030
But so the output of this of course
is the image on the right.

00:02:46.030 --> 00:02:48.330
But it goes a lot further than that.

00:02:48.330 --> 00:02:51.730
The real idea is that the filters over the you know,

00:02:51.729 --> 00:02:55.449
a hundred different filters, or
you can repeat filters as well.

00:02:55.449 --> 00:03:00.439
They can be chained together to kind of stack
different operations on top of each other.

00:03:00.439 --> 00:03:03.569
And so we start again the same
way, original image of Copenhagen,

00:03:03.569 --> 00:03:05.949
we're running that through a hue adjustment filter.

00:03:05.949 --> 00:03:11.459
And then as a second step we get, we take that
image and run it through a blending filter,

00:03:11.460 --> 00:03:16.610
and that's how we're adding this sort of
starburst effect onto the final image on the right.

00:03:16.610 --> 00:03:19.360
But it actually works a lot smarter than that.

00:03:19.360 --> 00:03:25.700
It's not doing these step by step processes,
sort of completing a whole image and then going

00:03:25.699 --> 00:03:29.060
through to the next part, and completing
another image, and so on.

00:03:29.060 --> 00:03:34.520
What it'll do is, these filters are kind of like
recipes, and it'll chain the filters together,

00:03:34.520 --> 00:03:39.780
compile them essentially into a single
unit so that each pixel only has to run

00:03:39.780 --> 00:03:42.949
through kind of to the, you know, one set of recipes.

00:03:42.949 --> 00:03:48.780
So by chaining these filters together, we still get the
same final image, but it's a much more efficient approach.

00:03:48.780 --> 00:03:55.080
Now the reason why this is so important is because
of the optimization possibilities in this case.

00:03:55.080 --> 00:03:59.719
So here we have just a lot of filters kind
of going from a start image to an end image.

00:03:59.719 --> 00:04:03.210
You know, same thing, hue adjustment,
we're adding that blending filter,

00:04:03.210 --> 00:04:06.950
it looks like we have a pointelized filter being applied.

00:04:06.949 --> 00:04:14.969
And so, you know, but the thing is like let's just
imagine if that fourth dot on the right was a crop filter.

00:04:14.969 --> 00:04:22.110
Then by optimizing this all together into sort of
one big recipe, we're able to eliminate the work

00:04:22.110 --> 00:04:24.750
of the pixels that are going to be cropped out.

00:04:24.750 --> 00:04:29.009
There's no reason to run those pixels through
the hue adjustment, through the blend,

00:04:29.009 --> 00:04:32.509
if they're just going to get discarded at the end of the chain.

00:04:32.509 --> 00:04:39.699
Okay? So that's just, that's the very basic idea of how
core image is working, and now to show a demo of that

00:04:39.699 --> 00:04:43.110
and go much deeper in the session, I'd like
to bring up Frank Doepke

00:04:43.110 --> 00:04:45.600
who is going to take us the rest of the way.

00:04:45.600 --> 00:04:46.510
>> Thank you Allan.

00:04:46.509 --> 00:04:51.349
( applause )

00:04:51.350 --> 00:04:55.790
Can you go back to slides please?

00:04:55.790 --> 00:04:57.490
Thank you.

00:04:57.490 --> 00:04:59.460
Okay, good afternoon.

00:04:59.459 --> 00:05:02.029
My name is Frank Doepke,
working on the Core Image team.

00:05:02.029 --> 00:05:04.409
I hope you all had a nice lunch.

00:05:04.410 --> 00:05:08.020
Let's take bets if Paris Hilton's lunch was better.

00:05:08.019 --> 00:05:12.500
So as Allan showed already nicely how to use Core
Image to demonstrate the effects of global warming

00:05:12.500 --> 00:05:15.160
on Copenhagen, although I heard it's raining there.

00:05:15.160 --> 00:05:17.160
I would like to dive a little bit deeper.

00:05:17.160 --> 00:05:20.420
He gave us a nice overview, but let's look
a little bit more what we can really do

00:05:20.420 --> 00:05:22.759
with Core Image, and how all this stuff works.

00:05:22.759 --> 00:05:25.879
And I'm picking actually one of
the demos that we actually have

00:05:25.879 --> 00:05:29.740
on your Leopard disk, it's the image unit UI demo, IUUI.

00:05:29.740 --> 00:05:32.829
And so let's have a look actually what we can do with this.

00:05:32.829 --> 00:05:39.279
And I would like to go back to the demo machine.

00:05:39.279 --> 00:05:41.969
So this is a really, really small application.

00:05:41.970 --> 00:05:46.940
If you look at the code base, if you want to take that
code and read it to your child to put it to sleep at night,

00:05:46.939 --> 00:05:49.339
you might take some extra reading, it's really small.

00:05:49.339 --> 00:05:54.729
Plus it's riveting, so he would not fall asleep over that.

00:05:54.730 --> 00:05:56.800
( laughter )

00:05:56.800 --> 00:05:59.129
We are on the wrong machines, thank you.

00:05:59.129 --> 00:06:01.540
Ha, now you see the same thing as I see, good.

00:06:01.540 --> 00:06:06.140
Okay. So what this is, we have a little
image here, and I already brought up now,

00:06:06.139 --> 00:06:08.469
oh let me do it again so that you see where it comes from.

00:06:08.470 --> 00:06:13.080
We have a little filter browser, and now
you can see I have a whole set of filters.

00:06:13.079 --> 00:06:17.779
We have like all these different adjustments,
we have blur filters, I can pick one,

00:06:17.779 --> 00:06:21.119
I can see a little preview of it here, Gaussian blur.

00:06:21.120 --> 00:06:22.920
As Allan said, there's a set of over a hundred.

00:06:22.920 --> 00:06:27.350
Unfortunately he's not up to date, it's over
125 filters that %we have already.

00:06:27.350 --> 00:06:33.310
And this browser's actually part of our new
NetWare that we already provide you for it.

00:06:33.310 --> 00:06:37.399
So I can see what the filter does, and
now let's make some image adjustments.

00:06:37.399 --> 00:06:40.849
So I'm not a graphic artist, so
bear with me, I'm not the best one.

00:06:40.850 --> 00:06:43.110
And, I want to do something
that looks like an old photo,

00:06:43.110 --> 00:06:46.910
let me start with something like sepia, always a favorite.

00:06:46.910 --> 00:06:51.960
So I added to it, now I have a sepia
filter, I can adjust here on the image.

00:06:51.959 --> 00:06:57.409
Now next I want to give it a little
of a gloomy effect, and there it is.

00:06:57.410 --> 00:06:59.240
You see, just typing, I find my gloom filter.

00:06:59.240 --> 00:07:05.530
Now it looks like almost like a faded image
already, and although there's like multiple filters,

00:07:05.529 --> 00:07:11.569
I can adjust each of them, and you see this actually
happening in real time, going through all the pixels.

00:07:11.569 --> 00:07:17.180
Now let me add actually in between those
two filter, like a gamma adjust filter.

00:07:17.180 --> 00:07:20.949
Oops, if I can type here, there we go.

00:07:20.949 --> 00:07:27.659
So, now I have like some nice effect and make it like, well,
looks like an old photo that you probably find in your attic

00:07:27.660 --> 00:07:30.320
from your grandmother or something like that.

00:07:30.319 --> 00:07:37.250
Now what I can do is I can actually export this
filter, and let me save it, I can call this old photo.

00:07:37.250 --> 00:07:44.300
And I can give it some description,
I'll skip this for the moment, yeah.

00:07:44.300 --> 00:07:50.110
And on the sepia tone filter I want to export
that you can later on use to input intensity.

00:07:50.110 --> 00:07:53.210
And I do the same on the gloom, and
actually leave both the same name.

00:07:53.209 --> 00:07:58.789
What this means actually is I'll take both of
them and adjust them later on as one control.

00:07:58.790 --> 00:08:03.850
And let me also from the Gamma Adjust,
I want to have the input power.

00:08:03.850 --> 00:08:12.189
So I'll export this now, and you go, and the nice
part now is I can just bring this into our library,

00:08:12.189 --> 00:08:19.949
we go into the graphics folder, into image units,
and save it right there.

00:08:19.949 --> 00:08:25.219
So let's quit for now the image in the demo application, and
for those who've already worked with Core Image on Tiger,

00:08:25.220 --> 00:08:31.400
they probably know the Core Image funhouse
application, this is our old school way of doing things.

00:08:31.399 --> 00:08:38.279
And now I can go in Copenhagen, must be
our city favorite today, and I can go,

00:08:38.279 --> 00:08:41.839
this is now the old way how we did the
browser there that was all done by hand.

00:08:41.840 --> 00:08:44.540
If you want to do it by hand, so
this is the sample to look at.

00:08:44.539 --> 00:08:48.039
And now I can go into my stylized
filter and I see old photo.

00:08:48.039 --> 00:08:49.559
I can apply this one.

00:08:49.559 --> 00:08:51.500
It looks like one filter that I just put together.

00:08:51.500 --> 00:08:54.860
And now you can see the intensity takes a sepia
together with subdue.

00:08:54.860 --> 00:08:55.700
( applause )

00:08:55.700 --> 00:09:01.820
Thank you.

00:09:01.820 --> 00:09:06.580
Okay. Instead I would like to go back to the slides please.

00:09:08.490 --> 00:09:11.389
So how did we do all this stuff?

00:09:11.389 --> 00:09:14.199
So let's get to know Core Image a little bit more.

00:09:14.200 --> 00:09:17.629
Core Image is all about doing this stuff on the GPU.

00:09:17.629 --> 00:09:20.350
So we do it very fast on the graphics card.

00:09:20.350 --> 00:09:25.840
We have, as Allan already said, a full floating-point,
color-managed pipeline, and I will go into some details on that later.

00:09:25.840 --> 00:09:31.690
As I said, we have over 125 filters,
so there's plenty for you already to play around with.

00:09:31.690 --> 00:09:36.220
And if you have filter ideas and you want to write
your own effects, go ahead, write an image unit.

00:09:36.220 --> 00:09:41.370
And you can, as you can see, you can
like make it available for everybody.

00:09:41.370 --> 00:09:44.490
The key point about these filters is the filter kernel.

00:09:44.490 --> 00:09:48.310
And this filter kernel actually is architecture independent.

00:09:48.309 --> 00:09:51.529
And that allows you that, for instance as we can do it here.

00:09:51.529 --> 00:09:58.789
If the GPU, so it has to be fragment program L. Or if this
is not the case, that means you will fall back to the CPU.

00:09:58.789 --> 00:10:05.689
And there we can actually go to the SSE, we can do the
velocity engine for the PowerPC chip, and we can also do,

00:10:05.690 --> 00:10:09.630
as you saw already in the keynote,
is the whole thing in 64-bit.

00:10:09.629 --> 00:10:13.740
So those are the key points about what Core Image is about.

00:10:13.740 --> 00:10:18.090
Now what do you need to work with Core Image?

00:10:18.090 --> 00:10:20.149
First of all, you need an image.

00:10:20.149 --> 00:10:24.230
And that is really the object that
represents really what we have to render.

00:10:24.230 --> 00:10:27.000
Now image, we always think of it
more like as a pixel bucket.

00:10:27.000 --> 00:10:29.879
This is not the case, this is more rendering recipes.

00:10:29.879 --> 00:10:36.629
It really just captures like all the steps that
it has to do to really later on render onto it.

00:10:36.629 --> 00:10:38.279
And we have the filters.

00:10:38.279 --> 00:10:43.879
And the filters actually have the processing kernels,
and those will be the ones that apply the effects.

00:10:43.879 --> 00:10:47.250
They typically have a set of parameters.

00:10:47.250 --> 00:10:54.440
Some of them actually don't, but most of them have like
input images, and radius as we saw already in the demo.

00:10:54.440 --> 00:10:58.600
And they produce one output image, and
that's what you want to then later on draw.

00:10:58.600 --> 00:11:01.129
You need to draw it, you need a context.

00:11:01.129 --> 00:11:02.279
And that's a CIContext.

00:11:02.279 --> 00:11:06.809
So this is where all the rendering of CI later on goes.

00:11:06.809 --> 00:11:11.539
It can be based on an OpenGL context, or a CGContext.

00:11:11.539 --> 00:11:16.139
There is something special about this core graphics
context, I will go into some of these details later,

00:11:16.139 --> 00:11:21.879
especially for those who have already
worked with it on Tiger.

00:11:21.879 --> 00:11:25.000
Okay. Let's do it once more and step by step.

00:11:25.000 --> 00:11:27.580
We start with a CIContext.

00:11:28.870 --> 00:11:31.629
Can somebody behind the stage shut up?

00:11:31.629 --> 00:11:35.669
I hear some voices, and somebody's
talking on the phone, that's not too good.

00:11:35.669 --> 00:11:40.610
Okay, so we start with a context, then
we have an image from some image data,

00:11:40.610 --> 00:11:43.659
from a file or that you have from data in memory.

00:11:43.659 --> 00:11:53.969
Next we create a filter object, and if you
need more than one filter you just repeat that.

00:11:53.970 --> 00:11:57.629
And next step is we need to set
the parameters on the filter.

00:11:57.629 --> 00:12:02.090
So now you set your radius and your
intensity, and stuff like this.

00:12:02.090 --> 00:12:06.889
And last step but not least, I
have to click it once, there we go.

00:12:06.889 --> 00:12:09.519
We draw the whole output into the context.

00:12:09.519 --> 00:12:10.189
That's it.

00:12:10.190 --> 00:12:14.330
That's all what you need to do to draw this core image.

00:12:14.330 --> 00:12:16.240
How does it work?

00:12:17.340 --> 00:12:20.590
One thing that's important to know is Core Image is lazy.

00:12:20.590 --> 00:12:24.230
We all are, but Core Image is specifically lazy.

00:12:24.230 --> 00:12:26.409
The reason for this, we have a lazy valuation.

00:12:26.409 --> 00:12:32.469
This allows us actually to defer all the
rendering really at the, until the draw time.

00:12:32.470 --> 00:12:35.710
So we are not like when you apply your
filter immediately rendering anything.

00:12:35.710 --> 00:12:40.250
Nothing's happening, so this is
pretty much like for free I would say.

00:12:40.250 --> 00:12:44.730
With this deferred rendering, what we can
do is we can concatenate this further.

00:12:44.730 --> 00:12:50.240
So we will look at all these filters, and then really see
okay, we really only need to touch these and these pixels,

00:12:50.240 --> 00:12:55.000
and we can also combine some of these
filters together to get a combined effect.

00:12:55.000 --> 00:12:58.360
So this is happening at run time
with a just-in-time compiler.

00:12:58.360 --> 00:13:03.590
And the benefits of course is first of all that we have
way better performance, because we can do multiple things

00:13:03.590 --> 00:13:07.480
in one pipeline rather than doing
it step by step by step by step.

00:13:07.480 --> 00:13:13.149
And also it allows us way better precision, cause we do
not (inaudible) between any of the intermediate steps.

00:13:13.149 --> 00:13:17.269
As we said, we do color management.

00:13:17.269 --> 00:13:19.120
So what does this mean?

00:13:19.120 --> 00:13:21.659
The whole output is color management.

00:13:21.659 --> 00:13:27.549
We go from an input image, and bring this into our
working space in which we do all our rendering.

00:13:27.549 --> 00:13:28.879
This working space is Generic Linear RGB.

00:13:28.879 --> 00:13:33.750
It's light linear, and it has an infinite gamut.

00:13:33.750 --> 00:13:39.610
This way allows us to do all the further processing, will
not create any banning because we run out of a gamut space.

00:13:39.610 --> 00:13:42.430
And then we go to the output space.

00:13:42.429 --> 00:13:45.529
In the diagram this looks like this.

00:13:45.529 --> 00:13:49.980
You see a bunch of images basically coming
in, all the filters doing the work.

00:13:49.980 --> 00:13:54.029
And in the end it gets put into
the target output, target context.

00:13:54.029 --> 00:13:57.879
So you might set up that output context
to whatever color space you want.

00:13:57.879 --> 00:14:01.840
And the images now may have a color space attached
to them, or if you have something that you rendered

00:14:01.840 --> 00:14:06.860
on your own, you might have to set that yourself.

00:14:06.860 --> 00:14:09.710
So we say we can do this really, really fast.

00:14:09.710 --> 00:14:11.879
Now you saw in the demo that was already pretty fast.

00:14:11.879 --> 00:14:15.620
But let's talk a little bit more about the performance.

00:14:15.620 --> 00:14:20.830
First of all, we have a little tool that we use internally
to measure if we are really doing everything correctly.

00:14:20.830 --> 00:14:22.560
It's called CIBenchmark.

00:14:22.559 --> 00:14:27.799
And what we show here is, we did a little trick.

00:14:27.799 --> 00:14:33.909
Normally Core Image will use, depending on how many
cores you have available, the right amount of threads

00:14:33.909 --> 00:14:35.659
and scale nicely across (inaudible) scaling.

00:14:35.659 --> 00:14:42.029
So we did a little trick that we can actually turn
off threads and actually see how well do we scale.

00:14:42.029 --> 00:14:47.809
And as you can see there with the first thread, you need
about 8 Mpixels up to 33 Mpixels
3

00:14:47.809 --> 00:14:52.139
when we use really all eight threads
on an eight core machine.

00:14:52.139 --> 00:14:55.340
Now how does the graphic card stack up to this?

00:14:55.340 --> 00:14:58.070
And this was actually a run of
the middle graphics card almost,

00:14:58.070 --> 00:15:01.540
it's a 7300 from NVIDIA,
nothing really too fancy,

00:15:01.539 --> 00:15:06.009
and it still beats the eight core
machine, and twice on Sunday.

00:15:06.009 --> 00:15:10.019
So let's look at another filter here,
we'll just pick up something special here.

00:15:10.019 --> 00:15:12.620
It's the ExposureAdjust. It's just a really simple filter.

00:15:12.620 --> 00:15:18.560
See again, we scale nicely over the eight
course, let's look at the graphics card, boom.

00:15:18.559 --> 00:15:19.759
As Steve would say.

00:15:19.759 --> 00:15:23.549
( laughter )

00:15:23.549 --> 00:15:28.379
Now CIConics, this is a really computation
intensive filter in this (inaudible) bound.

00:15:28.379 --> 00:15:31.689
We compare that with the graphics card,
it's still faster, but not as much.

00:15:31.690 --> 00:15:37.870
So your mileage varies a little bit depending on
which filters you use and how you combine them.

00:15:37.870 --> 00:15:42.169
Now one part that we are proud of is that we
are faster than ever than we've been in Tiger.

00:15:42.169 --> 00:15:46.379
So let's compare like the CIGaussianBlur specifically.

00:15:46.379 --> 00:15:50.029
Now it's fast enough actually like even
on our low end platform, like the MacBook,

00:15:50.029 --> 00:15:54.769
that you can actually run effects in real
time on the GPU with the Gaussian blur,

00:15:54.769 --> 00:16:00.549
because we are oh about over nine times
faster than what we've been in Tiger.

00:16:00.549 --> 00:16:03.669
( applause )

00:16:03.669 --> 00:16:05.000
Thank you.

00:16:05.000 --> 00:16:06.950
So those were charts.

00:16:06.950 --> 00:16:08.879
Let's show it off.

00:16:08.879 --> 00:16:10.110
And now comes the tricky part.

00:16:10.110 --> 00:16:18.950
We had some technical problems with this
machine, so let's pray that this will work.

00:16:18.950 --> 00:16:24.700
Okay, where is my mouse, okay there.

00:16:24.700 --> 00:16:29.550
And I'm missing something here.

00:16:32.450 --> 00:16:40.629
Yeah, there should have been actually
something that's on this machine.

00:16:40.629 --> 00:16:51.710
(inaudible), sorry.

00:16:51.710 --> 00:16:54.090
Ahh, okay.

00:16:54.090 --> 00:16:57.899
This was actually the person responsible
to make the filters much faster, so.

00:16:57.899 --> 00:16:59.299
( laughter )

00:16:59.299 --> 00:17:01.709
All right.

00:17:01.710 --> 00:17:06.390
So we have a little image here,
let's apply some effects to this.

00:17:06.390 --> 00:17:12.720
And now what we are doing here is, you see this is now
with one thread, basically on this eight core machine.

00:17:12.720 --> 00:17:22.559
And we will now be going up and make this a little
bit faster by using two, going forward three,

00:17:22.559 --> 00:17:30.200
going forward to four cores by now, and you
see how this effect slowly goes most around?

00:17:30.200 --> 00:17:34.830
But it gets faster and faster by the more cores
we're actually using by using our threads.

00:17:34.829 --> 00:17:39.220
So just (inaudible), it's nothing that you have to do in
your code, this is just a sample actually by something

00:17:39.220 --> 00:17:42.930
that we can do by turning off threads in Core Image.

00:17:42.930 --> 00:17:49.730
And you see it scales nicely, going all the way pretty fast.

00:17:49.730 --> 00:17:51.519
Let's see what the GPU can do.

00:17:51.519 --> 00:17:52.369
There we go.

00:17:52.369 --> 00:17:57.889
( laughter )

00:17:57.890 --> 00:17:59.810
And instead we'd like to go back to the slides please.

00:17:59.809 --> 00:18:00.389
( applause )

00:18:00.390 --> 00:18:02.650
Thank you.

00:18:02.650 --> 00:18:13.120
>> At that one point that was better GPU.

00:18:13.119 --> 00:18:18.859
>> Yeah, in all fairness this is a high end GPU,
course almost as small as a small car.

00:18:18.859 --> 00:18:25.639
I don't know if that's quite true, but it is the Quadro
card from NVIDIA, so this is really a high end one.

00:18:25.640 --> 00:18:30.930
But even the 7300
was still faster than the 8 cores.

00:18:30.930 --> 00:18:37.730
So the one part that I showed you in the image unit demo
application is that I was able to take a bunch of filters,

00:18:37.730 --> 00:18:44.079
chain them together, and export them as one new filter
that we used in the funhouse, my old photo filter.

00:18:44.079 --> 00:18:46.109
So this is what we call the CIFilterGenerator.

00:18:46.109 --> 00:18:48.689
This is new in Leopard.

00:18:48.690 --> 00:18:52.930
It allows you to concatenate multiple
filters and wrap them into one filter.

00:18:52.930 --> 00:18:56.920
And it can be stored so that you can reuse it as a macro.

00:18:56.920 --> 00:18:59.279
Now how does it work?

00:18:59.279 --> 00:19:01.309
It's all about connecting the filters.

00:19:01.309 --> 00:19:05.970
So when you want to do this, you need to
connect basically a source to a target filter.

00:19:05.970 --> 00:19:09.829
The source can actually be already an image,
this is something that I didn't show in this demo.

00:19:09.829 --> 00:19:12.599
But you can just provide a path,
and then you have a default image.

00:19:12.599 --> 00:19:16.969
That's important when you use a filter
that needs an environment map for instance.

00:19:16.970 --> 00:19:21.390
Now if you want to declare basically, like as I
set the little checkbox, this means like okay,

00:19:21.390 --> 00:19:27.330
this input of this filter I want to later
on export to the customer to be used.

00:19:27.329 --> 00:19:32.980
And I can actually with this way also set my own
parameters, and I can combine as we showed already,

00:19:32.980 --> 00:19:37.180
like I take the same input key on multiple
filters and actually channel it into one,

00:19:37.180 --> 00:19:41.090
so I can control multiple aspects through one control.

00:19:41.089 --> 00:19:42.949
You can set your own attributes.

00:19:42.950 --> 00:19:48.470
This is important when you for instance sometimes
have this particular problem that you have a filter,

00:19:48.470 --> 00:19:51.400
but it has a way too high maximum value for your production.

00:19:51.400 --> 00:19:57.330
So you can actually set a normal now
default where the maximum value is smaller.

00:19:57.329 --> 00:20:00.809
And one thing that's important is you
always need to declare an output image.

00:20:00.809 --> 00:20:04.769
As I said, every filter has at least an output image.

00:20:04.769 --> 00:20:08.619
And then you write this for the generators
until it's actually a plist document.

00:20:08.619 --> 00:20:15.659
And every application that supports image units
will automatically support this new filter.

00:20:15.660 --> 00:20:21.740
Now you saw that we have UI, as I showed already the filter
browser, and all the stuff that happened in the filter panel

00:20:21.740 --> 00:20:26.440
in the image UI demo application, hence the name,

00:20:26.440 --> 00:20:31.370
now has in a way of like actually using an
automatically created UI from the filter.

00:20:31.369 --> 00:20:36.069
So this was a big request that we got from
you from those work requirements in Tiger,

00:20:36.069 --> 00:20:39.960
where when you looked at like the old
funhouse application had lots of code in there

00:20:39.960 --> 00:20:43.549
to create all these panels, now this is all there for you.

00:20:43.549 --> 00:20:48.930
And you actually get a view based
on the filter with all its controls.

00:20:48.930 --> 00:20:54.600
You can configure the size, if you want
small, medium, or the regular size controls.

00:20:54.599 --> 00:20:59.419
And you can exclude also certain keys,
like if you look in the (inaudible),

00:20:59.420 --> 00:21:03.610
all of these filters do have an input
image, but they didn't show up in the panel

00:21:03.609 --> 00:21:07.229
because it's already the input
image coming from the document.

00:21:07.230 --> 00:21:11.440
And if you write image units, and you say well
I want to have, really have my own look to it,

00:21:11.440 --> 00:21:14.299
you can provide it with your own branding as well.

00:21:14.299 --> 00:21:17.190
And then we have the filter browser.

00:21:17.190 --> 00:21:21.509
The filter browser allows you nicely to pick the
filters, you can get them as a view so you can embed it

00:21:21.509 --> 00:21:26.150
into your windows, you can get it as a
sheet or as a panel, just as you like it.

00:21:26.150 --> 00:21:28.870
And you see all the filters, you get the description.

00:21:28.869 --> 00:21:31.659
And one part that I didn't show was
very much like the font panel.

00:21:31.660 --> 00:21:32.960
You can collect your favorites.

00:21:32.960 --> 00:21:36.650
If you have a favorite set of filters, you can put
them into the favorite and find them very easily.

00:21:36.650 --> 00:21:41.330
And all of this is actually in the image kit framework.

00:21:41.329 --> 00:21:45.210
So since this is a little bit higher level
abstraction which depends a lot on AppKit,

00:21:45.210 --> 00:21:50.430
you have to go into image kit and
find the headers for it there.

00:21:50.430 --> 00:21:53.019
Now in general we have some new API.

00:21:53.019 --> 00:21:56.099
Also, for those who already worked with
it, let's have a quick look over that.

00:21:56.099 --> 00:22:03.119
We did some refinements all over the place based on your
feedback again, and we have now convenience functions

00:22:03.119 --> 00:22:05.829
for some of the common tasks, for
instance to create an empty image,

00:22:05.829 --> 00:22:09.059
this was one of the things that was not so easy before.

00:22:09.059 --> 00:22:15.200
Now we have also constants for all the common keys in
the filters, so this allows you actually to just type

00:22:15.200 --> 00:22:19.190
in KCI input image, and export should
do the code completion for you.

00:22:19.190 --> 00:22:22.180
That makes life a little bit easier there.

00:22:22.180 --> 00:22:25.850
And you can also now provide documentation for your filters.

00:22:25.849 --> 00:22:29.709
If you write your own filter, and you want to of
course explain to the user what this filter does,

00:22:29.710 --> 00:22:35.610
you can actually provide a UL, and actually
see a video documentation for the filter.

00:22:35.609 --> 00:22:42.429
Now clip in your bindings, we are not going skiing, but
what this means is Core Image is using key-value-coding.

00:22:42.430 --> 00:22:48.940
And one thing that didn't work in Tiger, but that works
in Leopard now, is that you can observe the output image.

00:22:48.940 --> 00:22:54.029
What that means is for instance if I have filter A,
and I can take the output image, put it into filter B,

00:22:54.029 --> 00:23:01.369
and then that output image into filter C. When I do
this, and I changed in Tiger something on filter A,

00:23:01.369 --> 00:23:05.129
I would have to again, take the output image,
put it into filter B, take that output image,

00:23:05.130 --> 00:23:08.130
put it into filter C. You don't have to do that any more.

00:23:08.130 --> 00:23:13.790
In Leopard it would automatically observe if something
changed on filter A, it propagates all the way through,

00:23:13.789 --> 00:23:17.859
and filter C is the only one where I have
to observe the output image and draw.

00:23:17.859 --> 00:23:20.969
Makes life much, much easier that way.

00:23:20.970 --> 00:23:24.450
So this allows you to always automatically
update your filter chain.

00:23:24.450 --> 00:23:31.750
And as I said, we have new filters, let me just
showcase a little bit of what we have here in filters.

00:23:31.750 --> 00:23:36.109
So this is just a little overview,
and might be even some more coming.

00:23:36.109 --> 00:23:40.869
So try those out, play around with them,
and I give you a tip how to try those out.

00:23:40.869 --> 00:23:43.269
Comes in a moment.

00:23:43.269 --> 00:23:51.759
Now I already mentioned that we can either create an
OpenGL context, or a CGContext as the base for CI.

00:23:53.019 --> 00:23:59.599
With Quartz GL, which is new in Leopard, we have
actually something special on the CGContext.

00:23:59.599 --> 00:24:04.059
In the past when you used the CG-based context,
what happened was CI did all its rendering

00:24:04.059 --> 00:24:08.700
up on the graphics card really spiffy and
fast, but since CG was of course bitmap based,

00:24:08.700 --> 00:24:13.809
it had to read all the stuff back,
and then composite in the CG space.

00:24:13.809 --> 00:24:19.299
Now this reading back from the graphic card is not really
coming for free, this is actually relatively costly.

00:24:19.299 --> 00:24:25.799
Now when you use Leopard, and your application opts
in to use OpenGL, you can create a CGContext,

00:24:25.799 --> 00:24:31.049
you don't have to write any OpenGL
code, and everything stays on the GPU.

00:24:31.049 --> 00:24:36.230
And we can show this already, like if you look at
the HazeFilter sample, the TransitionSelector,

00:24:36.230 --> 00:24:41.890
and even the Exposure sample, those three have
already been converted by actually just using

00:24:41.890 --> 00:24:48.290
in this case the any reports GL key and the info plist
as also a way to do that on a window by window base.

00:24:48.289 --> 00:24:52.369
Now let's see actually what is the
performance difference between those two.

00:24:52.369 --> 00:24:57.219
So on the left side you see it took, and I just
did this as a quick test, the CIHazeFilter.

00:24:57.220 --> 00:25:04.440
It took about 150 milliseconds to render the
old way when I had a CGContext and used a CIFilter.

00:25:04.440 --> 00:25:12.710
If I turn on Quartz GL and opt into this, it takes
four milliseconds, a little bit of a difference there.

00:25:12.710 --> 00:25:18.960
So this is it. There are some tools also that makes life
a little bit easier for you to develop this core image.

00:25:18.960 --> 00:25:20.360
First of all, debugging.

00:25:20.359 --> 00:25:22.279
We all have to do that at one point.

00:25:22.279 --> 00:25:25.470
And for that we can use Quartz Debug now.

00:25:25.470 --> 00:25:30.319
It knows which filters get executed, and you
can actually see which filters are in there.

00:25:30.319 --> 00:25:34.629
Sometimes you might be surprised, there might be a filter
in your pipeline you didn't think that you had in there.

00:25:34.630 --> 00:25:38.260
And you can see how long it takes, especially
for those who do like video applications,

00:25:38.259 --> 00:25:40.849
you need to find out if everything happens in real time.

00:25:40.849 --> 00:25:44.199
You can figure out okay, what is
the time spent on those filters.

00:25:44.200 --> 00:25:47.090
So look at Quartz Debug for debugging.

00:25:47.089 --> 00:25:50.000
Next we have a widget.

00:25:50.000 --> 00:25:53.640
This is actually coming with Leopard.

00:25:53.640 --> 00:25:59.130
And it's in the developer tools, I think right
now it's installed under extras Core Image.

00:25:59.130 --> 00:26:04.090
It should later on actually even like be right
in the dock when you have Xcode installed.

00:26:04.089 --> 00:26:09.179
You can browse the filters for image like
the (inaudible), you can find filters.

00:26:09.180 --> 00:26:12.690
But this is more in tune for the developers
because you actually can get to know the filters.

00:26:12.690 --> 00:26:18.190
You see the, all the keys set out there for
the filters, and all the parameters for those.

00:26:18.190 --> 00:26:22.029
And you can actually check as you can see, oh
well it's probably a little bit hard to read.

00:26:22.029 --> 00:26:27.039
But you see there's a little line that tells you if this
filter is available in 10.5 or 10.4.

00:26:27.039 --> 00:26:30.250
You can try them out, because you
see this preview down there?

00:26:30.250 --> 00:26:33.470
You can drag in your own images and
see actually how that looks like.

00:26:33.470 --> 00:26:37.420
That's the way how to test out
these new filters that we have.

00:26:37.420 --> 00:26:40.590
And one neat feature is that you can copy the code.

00:26:40.589 --> 00:26:48.199
So if you found the filter that you like in the widget,
hit command C, go back into Xcode, and just paste it in,

00:26:48.200 --> 00:26:54.420
and it gives you the completion already like scenes filter
equals CIFilter with name, and all the parameters in there.

00:26:54.420 --> 00:26:57.230
That saves you a lot of typing,
that's actually very convenient.

00:26:57.230 --> 00:27:02.349
Now as Allan already pointed out,
you can be part of Core Image,

00:27:02.349 --> 00:27:05.889
right image units, we have lots of documentations for this.

00:27:05.890 --> 00:27:11.300
There's a whole set of like how to do this,
and it guides us to all these little steps.

00:27:11.299 --> 00:27:15.250
And we have an Image Unit logo program, so you
can actually put this little icon on your box

00:27:15.250 --> 00:27:19.539
and saying I actually support
image units, or upgraded image units.

00:27:19.539 --> 00:27:25.480
And now, this was all a little bit dry,
let's bring up mister nine espressos

00:27:25.480 --> 00:27:35.490
for the day, Ralph Brunner.

00:27:35.490 --> 00:27:36.319
Thank you.

00:27:36.319 --> 00:27:36.539
( applause )

00:27:36.539 --> 00:27:40.519
>> I do know for a fact there are
people from Copenhagen in the audience.

00:27:42.259 --> 00:27:46.920
So let me start with pointing out
a bunch of places in the US

00:27:46.920 --> 00:27:50.440
where we use Core Image essentially for UI visual effects.

00:27:50.440 --> 00:27:56.000
One you know very well from Tiger is the
Dashboard ripple can have pretty obvious effect,

00:27:56.000 --> 00:27:59.130
so I'm not going to spend a lot of time on that.

00:27:59.130 --> 00:28:02.230
New in Leopard is the menu bar.

00:28:02.230 --> 00:28:10.269
And it was mentioned in the Keynote, and kind of was not
what, wasn't mentioned was why this is actually interesting.

00:28:10.269 --> 00:28:13.009
So let me try to explain that a little.

00:28:13.009 --> 00:28:20.379
So what the new menu bar does, it has a stack of filters
that examines what is underneath the menu bar.

00:28:20.380 --> 00:28:25.550
It computes an average color, and then based
on that average color, starts to adjust values,

00:28:25.549 --> 00:28:30.220
like how transparent the bar is, how
much light gets pushed in from the back,

00:28:30.220 --> 00:28:34.640
the inside glow, and all these kind of things.

00:28:34.640 --> 00:28:38.340
You might think that is, well the purpose
of this is to make sure it looks good

00:28:38.339 --> 00:28:42.539
on as many backgrounds as people can possibly have.

00:28:42.539 --> 00:28:50.309
You might think that is a bit excessive for something that's
22 pixels high, but the point I'm kind of trying

00:28:50.309 --> 00:28:56.149
to make here is a kind of first step to
go beyond standard alpha compositing.

00:28:56.150 --> 00:29:00.680
So by actually putting a bit of logic into the filter
that kind of measures what's underneath

00:29:00.680 --> 00:29:03.750
and adapt appropriately, you know,
a little bit of that knowledge

00:29:03.750 --> 00:29:09.250
that the designer have applied before
is now you know, written in code.

00:29:09.250 --> 00:29:16.150
And I expect we are going to do more of that, and that is
something to think about you know, how can you use filters

00:29:16.150 --> 00:29:21.810
to gather statistics, and then react,
kind of make your visuals really match.

00:29:21.809 --> 00:29:26.200
A second example I'd like to make is in Leopard

00:29:26.200 --> 00:29:32.080
underneath the sheet dialog, there
is a small and pretty subtle blur.

00:29:32.079 --> 00:29:37.839
It's not clear you can see that in there, if
you floated away from the protected surface.

00:29:37.839 --> 00:29:44.089
And again, this is a pretty subtle effect, and kind of
there's a lesson there, is most filters that you would,

00:29:44.089 --> 00:29:50.730
would use most special effects you do, you know, they're
just a small touch to kind of enhance the experience.

00:29:50.730 --> 00:29:56.730
And that you know, sometimes if you're in an application,
if you build an application that does something you know,

00:29:56.730 --> 00:29:59.059
that's really in your face, like think Omni Dazzle.

00:29:59.059 --> 00:30:04.139
Then the effect really has to be at the forefront,
but for most applications you really want you know,

00:30:04.140 --> 00:30:08.610
subtle touches that just overall improve things.

00:30:08.609 --> 00:30:15.750
So with that I'm going to switch gears, and talk
a bit about more advanced use of Core Image.

00:30:15.750 --> 00:30:23.299
The example I'm going to explain now is essentially
how do you do color tracking on the GPU.

00:30:23.299 --> 00:30:28.129
What we're doing, we're going to take and build
a mask based on a specific color in the image,

00:30:28.130 --> 00:30:33.950
we're looking for a specific color, and then compute
you know, the center of gravity of that object,

00:30:33.950 --> 00:30:39.019
and then apply an effect based on the
parameters we found at that location.

00:30:39.019 --> 00:30:55.119
And that makes much more sense if I actually
show a demo, so let me do that first.

00:30:58.019 --> 00:31:03.420
Okay. So what we have here is a video
of a remarkably handsome young man -

00:31:03.420 --> 00:31:05.670
( laughter )

00:31:05.670 --> 00:31:09.500
>> - waving a pink ball around,
which is kind of silly, but you know,

00:31:09.500 --> 00:31:14.049
bear with me, you will see how silly this really gets.

00:31:14.049 --> 00:31:16.809
On the right side you see the mask that was found.

00:31:16.809 --> 00:31:21.759
Essentially we're looking for pixels that are
looking somewhat like the pink on the right side

00:31:21.759 --> 00:31:24.490
in the color well, and then we display that mask.

00:31:24.490 --> 00:31:27.970
I can actually overlay that mask
to make this a bit more visible.

00:31:27.970 --> 00:31:33.759
So just the white area in the video
is what the filter thinks the mask is.

00:31:33.759 --> 00:31:40.019
And based on that mask, we are computing
the center of gravity of that object.

00:31:40.019 --> 00:31:43.480
And I will explain in a minute how that works.

00:31:43.480 --> 00:31:52.829
And after that we are taking that position and feeding
it into a compositing filter, which does the following.

00:31:52.829 --> 00:31:55.619
It takes a duck and puts it in the same place.

00:31:55.619 --> 00:32:04.029
( laughter )

00:32:04.029 --> 00:32:09.869
And you will notice that there is actually a depth, a
distance estimate going on, so when the ball comes closer,

00:32:09.869 --> 00:32:17.449
the duck gets bigger and gets smaller, and that is
done by not only measuring the center of gravity

00:32:17.450 --> 00:32:20.059
of the object, it's also measured the visible area.

00:32:20.059 --> 00:32:27.970
And based on the visible area, you can compute a distance
estimate, which is essentially one over (inaudible) area.

00:32:27.970 --> 00:32:33.509
Okay. So with that I'm going to close
this demo and go back to the slides.

00:32:33.509 --> 00:32:41.740
( applause )

00:32:41.740 --> 00:32:44.390
So to summarize what's going on here.

00:32:44.390 --> 00:32:49.790
So first the mask image by color, and that's
really just essentially a Euclidian distance.

00:32:49.789 --> 00:32:53.839
So you take current pixel color
minus pink color,

00:32:53.839 --> 00:32:56.730
measure how the distance is in
you know, three dimensional space.

00:32:56.730 --> 00:33:00.890
And if it's further away than a certain
threshold then we consider it outside the ball,

00:33:00.890 --> 00:33:03.009
otherwise it's inside the ball.

00:33:03.009 --> 00:33:04.859
One subtle detail there.

00:33:04.859 --> 00:33:08.759
Before the, before that distance
computation, we take the pixel,

00:33:08.759 --> 00:33:13.470
and take the pixel's color, and divide it by the luminance.

00:33:13.470 --> 00:33:17.980
And the reason is the ball has a shading on it
because the illumination isn't completely uniform.

00:33:17.980 --> 00:33:23.720
So by dividing by the luminance you get a solid
pink color, and that makes for a better match.

00:33:23.720 --> 00:33:30.990
The next step is for every pixel we take its XY
coordinate and multiply that by that mask, and store that,

00:33:30.990 --> 00:33:37.819
and this is the kind of the funky part, store the XY
coordinate that you know, you multiply XY coordinate

00:33:37.819 --> 00:33:40.809
as in the pixel as the red and the green components.

00:33:40.809 --> 00:33:44.960
So this is where you know, having
floating point images is really useful.

00:33:44.960 --> 00:33:51.140
And so at this point you have an image
which is zero everywhere except for the ball

00:33:51.140 --> 00:33:55.790
which has you know, little tuples of XY all over it.

00:33:55.789 --> 00:33:59.379
And then we use a new filter in
Leopard, the CIAreaAverage,

00:33:59.380 --> 00:34:03.660
which computes the average color essentially of an image.

00:34:03.660 --> 00:34:07.279
In this case it computes the average
XY coordinate over all the coordinates

00:34:07.279 --> 00:34:10.659
in that ball, which gives you the center of gravity.

00:34:10.659 --> 00:34:17.639
Now another interesting detail there is that the CIAreaAverage
filter, the output is a single pixel image,

00:34:17.639 --> 00:34:20.519
it's an image that's one pixel wide and one pixel high.

00:34:20.519 --> 00:34:26.949
And the reason for that is on GPUs it's really hard to
get data from one stage to the next if it's not an image.

00:34:26.949 --> 00:34:28.549
So well, let's make it an image.

00:34:28.550 --> 00:34:31.170
Floating points should just work.

00:34:31.170 --> 00:34:37.250
So the output of that filter is a single pixel, and
that pixel has in the first two components a coordinate,

00:34:37.250 --> 00:34:43.030
and in the alpha component the visible area,
which we use for the distance estimate.

00:34:43.030 --> 00:34:47.250
And the last step is well, the duck.

00:34:47.250 --> 00:34:53.309
So the last step has a filter which takes the video
frame, and it does a secondary image which is that 1x1

00:34:53.309 --> 00:34:56.039
pixel image, which is the found coordinate.

00:34:56.039 --> 00:34:59.929
And then it scales and composites the duck appropriately.

00:34:59.929 --> 00:35:07.149
Oops. So if you would like to learn a bit
more about that, there is actually a chapter

00:35:07.150 --> 00:35:11.950
in the upcoming GPU Gems 3
book that describes how this works.

00:35:11.949 --> 00:35:16.429
In particular, it also describes how the
CIAreaAverage filter is implemented.

00:35:16.429 --> 00:35:22.649
So if you want to do other statistics gathering on the
GPU, which isn't min, max and average which we added

00:35:22.650 --> 00:35:26.410
in Leopard, then there you can find how to do this.

00:35:26.409 --> 00:35:30.250
So if interested in these kind of
things, it's available in August,

00:35:30.250 --> 00:35:34.409
look in your favorite bookstore
for the cover with the friendly face.

00:35:34.409 --> 00:35:36.759
( laughter )

00:35:36.760 --> 00:35:43.240
Okay. So a second example I would like to
show is kind of similar to the first one,

00:35:43.239 --> 00:35:48.779
but now instead of tracking something by
color, we are tracking it by geometry.

00:35:48.780 --> 00:35:56.980
And so that just kind of came up in our, as an internal
usage for us because we have to support a large number

00:35:56.980 --> 00:36:01.610
of cameras for (inaudible) support which
actually we'll hear about a bit later.

00:36:01.610 --> 00:36:05.079
And that involves calibrating these cameras.

00:36:05.079 --> 00:36:11.549
And one part, and that's the only part I'm going to
talk about here, is once you have an image with one

00:36:11.550 --> 00:36:16.860
of these color charts which have known color values
where you can build your calibration information off,

00:36:16.860 --> 00:36:22.559
the part I'm talking about is you have an
image, where in that image is the chart.

00:36:22.559 --> 00:36:25.460
And yeah, it's kind of the (inaudible)
version of Where's Waldo?

00:36:25.460 --> 00:36:27.039
( laughter )

00:36:27.039 --> 00:36:33.400
So the goal is to find the chart, we need the
location, XY coordinate, we need the size, width, height,

00:36:33.400 --> 00:36:38.940
and we also would like to have orientation, the rotation
angle, simply because it's sometimes really hard

00:36:38.940 --> 00:36:43.369
to make it perfectly straight, so it would
be nice if the filter would handle this.

00:36:43.369 --> 00:36:48.809
And we want to do this by looking for geometry and not
for color, because well, do using it for color calibration

00:36:48.809 --> 00:36:51.849
so at the beginning color isn't particularly reliable.

00:36:51.849 --> 00:36:57.559
So that's why we are looking at tracking geometry.

00:36:57.559 --> 00:37:11.230
So with that I have a demo.

00:37:11.230 --> 00:37:16.909
Okay. And this involves live video, so that's
usually the place where things go horribly wrong.

00:37:16.909 --> 00:37:19.129
( laughter )

00:37:19.130 --> 00:37:22.289
So let me stand in front of this camera, okay.

00:37:22.289 --> 00:37:24.389
That's by the way the only reason
why I'm wearing this shirt.

00:37:24.389 --> 00:37:25.609
( laughter )

00:37:25.610 --> 00:37:33.000
So what I'm having here is this color chart, and
where I'm moving this color chart into the frame.

00:37:33.000 --> 00:37:42.849
Here is the chart, yeah, it tracks it you know, size
should work reasonably well, angle works most of the time.

00:37:42.849 --> 00:37:45.519
Yeah, one interesting thing here is
there's not that much light here,

00:37:45.519 --> 00:37:50.759
so if I move the chart fast I get motion
blur, then the detection completely fails.

00:37:50.760 --> 00:37:53.940
Interestingly even though there
is actually no code in there,

00:37:53.940 --> 00:37:58.320
tilting it away from the projection
plane is still working reasonably well.

00:37:58.320 --> 00:38:02.769
But you know, after about 20 degrees or so tracking will fail.

00:38:02.769 --> 00:38:14.980
Okay. So one thing I would like to draw your attention to
is that little quarter circle histogram on the right side.

00:38:14.980 --> 00:38:19.240
And this is essentially a histogram
over all angles in the image.

00:38:19.239 --> 00:38:25.209
And because all of these squares have a,
essentially angles that are ninety degrees apart.

00:38:25.210 --> 00:38:28.860
So as I move the chart, you see this
spike moving to different places.

00:38:28.860 --> 00:38:34.710
And that gives us you know, the
rotation estimate, how that is done.

00:38:34.710 --> 00:38:39.610
Essentially we're running a gradient filter, then a
histogram over the gradients, and then we look for the spike

00:38:39.610 --> 00:38:43.200
in that histogram to figure out what
the orientation of the chart is.

00:38:43.199 --> 00:38:46.509
Okay, with that I will go back to the slides.

00:38:46.510 --> 00:38:55.650
( applause )

00:38:55.650 --> 00:38:59.070
Okay. So first step.

00:38:59.070 --> 00:39:04.530
As I said, color is not reliable in this application, so
the first thing we're going to do, we convert to grayscale.

00:39:04.530 --> 00:39:08.640
And there's kind of the straight forward
way, which is convert to grayscale based

00:39:08.639 --> 00:39:12.440
on luminance, and that's what you're seeing here.

00:39:12.440 --> 00:39:16.429
So the left side you see the original,
the middle image is based on luminance.

00:39:16.429 --> 00:39:22.649
There's also a filter, which is probably the most simple
filter we ship in Leopard, it's the CI maximum component,

00:39:22.650 --> 00:39:27.460
which converts to grayscale based
on the maximum red, green, or blue.

00:39:27.460 --> 00:39:30.059
And for this particular application, this is perfect.

00:39:30.059 --> 00:39:35.960
Because we really want, we want to find these squares,
and we want to you know, differentiate between the squares

00:39:35.960 --> 00:39:38.690
and the gaps, and the gaps happen to be black.

00:39:38.690 --> 00:39:43.610
And some of these colors on the chart are just
saturated red, saturated green, saturated blue,

00:39:43.610 --> 00:39:49.920
so by using the maximum component, I get a more, an
image which has more white or light gray patches,

00:39:49.920 --> 00:39:52.619
as you can see here, which helps the detection.

00:39:52.619 --> 00:39:57.639
So that's kind of a lesson here if you
want to do these kind of things, you know,

00:39:57.639 --> 00:40:03.379
any kind of domain specific knowledge you have, you know,
exploit it, because that makes these things much easier.

00:40:04.429 --> 00:40:08.129
Okay. So the next step is called a row projection.

00:40:08.130 --> 00:40:12.150
What's happening is we are computing
the average over each scan line.

00:40:12.150 --> 00:40:15.930
There is a CRO projection filter in
Leopard, so you can use that one.

00:40:15.929 --> 00:40:19.909
And because showing, so what's happening,
you take this image and we create an image

00:40:19.909 --> 00:40:22.409
which is one pixel wide and the full height of the image.

00:40:22.409 --> 00:40:27.429
And for each pixel will contain
the average of each scan line.

00:40:27.429 --> 00:40:32.379
Because showing a one pixel wide
image is really hard on the projector,

00:40:32.380 --> 00:40:39.180
what I'm showing here is instead a luminance
plot of that you know, really narrow image.

00:40:39.179 --> 00:40:44.639
And what you're seeing in that luminance
plot is there are 10 pulses in there.

00:40:44.639 --> 00:40:49.829
Essentially for each row of tiles you get a high
value, and then you have a black gap in between them,

00:40:49.829 --> 00:40:56.599
so get a low value and then so on, ten times.

00:40:56.599 --> 00:40:59.279
So what we are going to do now is match a pulse signal.

00:40:59.280 --> 00:41:02.390
We know we have to look for 10 pulses at 2D.

00:41:02.389 --> 00:41:06.420
It's essentially kind of an auto
correlation that we're doing here.

00:41:06.420 --> 00:41:09.260
And we try out every start position
and every pulse width.

00:41:09.260 --> 00:41:12.890
But because it's a 1D image it's
actually really fast to do this.

00:41:12.889 --> 00:41:20.059
And the best fit of those 10 pulses to this signal will
tell us where the chart is in the vertical dimension.

00:41:20.059 --> 00:41:26.920
So we know where it starts and how tall it is.

00:41:26.920 --> 00:41:29.680
So the next step is called a column projection.

00:41:29.679 --> 00:41:35.789
But before we apply that, because we currently just
gathered some information, we know the vertical extent

00:41:35.789 --> 00:41:41.139
of the chart, so we can actually crop this image, and
remove everything outside which we know is not chart,

00:41:41.139 --> 00:41:45.009
so we're not really interested in
it, and then do a column projection,

00:41:45.010 --> 00:41:47.020
which is pretty much equivalent what we did before.

00:41:47.019 --> 00:41:54.539
The row projection is just an average
over each scanned row this time.

00:41:54.539 --> 00:42:03.380
You will notice that the luminance plot down there,
the pulses and the gaps are now much more pronounced.

00:42:03.380 --> 00:42:08.099
And that's the result of that cropping, because we're no
longer diluting the average with stuff that doesn't belong

00:42:08.099 --> 00:42:11.219
to the chart, we're getting a really clean signal.

00:42:11.219 --> 00:42:17.829
And we do the same thing again with before, we fit
14 pulses on this, we find the start position.

00:42:17.829 --> 00:42:20.869
And we also kind of reduce the search space now,

00:42:20.869 --> 00:42:25.509
because from the previous pass we know
how tall one of these squares were.

00:42:25.510 --> 00:42:29.890
And well, they're supposed to be square, so actually
the width has to be pretty much in the same range.

00:42:29.889 --> 00:42:39.009
So we can only search for a very small number of widths
to find to match the pulse signal to this spot.

00:42:39.010 --> 00:42:46.150
Now the other creation also gives you kind of a goodness
indicator into how well pulses fit to this chart.

00:42:46.150 --> 00:42:51.800
And we can just use a threshold to kind of
determine empirically, to figure out well,

00:42:51.800 --> 00:42:54.680
chances are there isn't charting
here at all, it's just you know,

00:42:54.679 --> 00:42:59.909
a local maxima that doesn't have any kind of pulse nature.

00:42:59.909 --> 00:43:02.069
Okay. And with that we're essentially done.

00:43:02.070 --> 00:43:06.150
We found the angle in the beginning,
and rotated the image to make sure

00:43:06.150 --> 00:43:08.369
that you know, this row and column projection works.

00:43:08.369 --> 00:43:13.159
And we found a vertical extent, the
horizontal extent, and the start positions.

00:43:13.159 --> 00:43:16.469
So now we have our chart.

00:43:16.469 --> 00:43:22.089
Okay. So with that I would like to kind of
give some conclusions out of that experiment.

00:43:22.090 --> 00:43:24.660
So first of all why implement that in a Core Image filter?

00:43:24.659 --> 00:43:29.599
Because clearly you could write some C code that
just goes over arrays and does the same thing.

00:43:29.599 --> 00:43:35.769
Well some of the advantages you get out
of it is the Core Image runtime will go

00:43:35.769 --> 00:43:42.090
and build SSE code velocity engine code, R fragment
(inaudible) GPU for you essentially off the same code base.

00:43:42.090 --> 00:43:44.039
So that's really convenient.

00:43:44.039 --> 00:43:50.050
The second, as Frank was already mentioning
before, it will scale to eight core machines,

00:43:50.050 --> 00:43:53.950
and whatever gets released in the
future, it's probably going to work.

00:43:53.949 --> 00:43:58.259
And another aspect is that you don't
have to worry about image formats.

00:43:58.260 --> 00:44:05.760
So I did this demo with video which comes in as eight
bit per component by UE, but we actually used that stuff

00:44:05.760 --> 00:44:11.990
that comes from still image cameras, and that is
16 bits per component RGB, and the code just works,

00:44:11.989 --> 00:44:18.829
because Code Image will convert the proper pieces to
you know, into the space that the filter expects it.

00:44:18.829 --> 00:44:27.789
The video playback via Quartz GL point here kind of seems
innocent, but to me that's actually a really profound part.

00:44:27.789 --> 00:44:33.769
What you just saw was essentially app
kit in the drawRect function,

00:44:33.769 --> 00:44:37.920
and grabbing a video frame and drawing it into CGContext.

00:44:37.920 --> 00:44:40.550
And that is possible because of QuartzGL.

00:44:40.550 --> 00:44:45.400
Because as the video frame comes in, it gets
uploaded to the GPU, and all the processing

00:44:45.400 --> 00:44:48.160
and all the rendering gets done in the GPU.

00:44:48.159 --> 00:44:52.079
And this is kind of, this is new in
Leopard, we could never do that before.

00:44:52.079 --> 00:44:56.069
You now can actually draw text on top of
video without having to do with child windows

00:44:56.070 --> 00:44:58.600
and all these kind of things that are really ugly.

00:44:58.599 --> 00:45:03.589
So, I mean to drawRect for this
demo was literally 20 lines of code,

00:45:03.590 --> 00:45:08.660
and most of it was to draw the
little bar charts on the side.

00:45:08.659 --> 00:45:12.699
And the last point I would like to
make is yeah, mind the color matching.

00:45:12.699 --> 00:45:19.319
Turns out when I was debugging this, I had some issues
with you know, trying to find bugs in bin filters.

00:45:19.320 --> 00:45:23.670
And what I did, I used the best
debugging tool ever invented, printf.

00:45:23.670 --> 00:45:24.539
( laughter )

00:45:24.539 --> 00:45:33.009
And I just took the value out of the orientation angle
and printed that in the console, and well it was wrong.

00:45:33.010 --> 00:45:37.670
And you know, when you move the chart it
went up and down properly, it was monotonic,

00:45:37.670 --> 00:45:39.190
you know, so I'm thinking something was right.

00:45:39.190 --> 00:45:43.470
But clearly scale was wrong, and I was wondering
did I forget to multiply by pi or whatever.

00:45:43.469 --> 00:45:44.250
( laughter )

00:45:44.250 --> 00:45:53.239
And it turns out it's really important to understand
the color matching part of Core Image, because that,

00:45:53.239 --> 00:45:56.989
it tries to do the right thing in
terms of you know, processing images.

00:45:56.989 --> 00:46:03.059
But once you go and put angles or XY coordinates into
pixels, you probably don't want to color match those.

00:46:03.059 --> 00:46:05.110
( laughter )

00:46:05.110 --> 00:46:08.730
So a gamma correction on an angle has
really surprising results I can say.

00:46:08.730 --> 00:46:10.690
( laughter )

00:46:10.690 --> 00:46:15.510
Okay. And with that, the weather.

00:46:15.510 --> 00:46:22.570
( applause )

00:46:22.570 --> 00:46:23.470
>> Thank you Ralph.

00:46:23.469 --> 00:46:30.279
Ralph did a great job of explaining how you can tackle some
very complex image processing problems using Core Image.

00:46:30.280 --> 00:46:34.500
I want to talk about another one today,
which is sort of an interesting problem.

00:46:34.500 --> 00:46:41.840
So imagine you have an RGB image, and it's missing
50% of its green pixels, 75%,

00:46:41.840 --> 00:46:46.360
three quarters of its red pixels, and
three quarters of its blue pixels,

00:46:46.360 --> 00:46:49.760
and your job is to recover all the missing pixels.

00:46:49.760 --> 00:46:51.390
Sounds impossible.

00:46:51.389 --> 00:46:56.619
But in fact this is a fundamental problem
that's, needs to be solved to produce images

00:46:56.619 --> 00:46:59.489
out of most image sensors that we see today.

00:46:59.489 --> 00:47:05.129
Ranging from disposable cameras, to
high quality video cameras all need

00:47:05.130 --> 00:47:09.970
to solve this fundamental problem of
how to recover all this missing data.

00:47:09.969 --> 00:47:16.319
As a bonus challange, you need to give those images
to the most discriminating eyes that are available,

00:47:16.320 --> 00:47:20.820
and they're going to try to see if there's anything missing.

00:47:20.820 --> 00:47:25.539
A visual way of looking at this same problem is
that while your eyes can see an image like this,

00:47:25.539 --> 00:47:29.099
what most bare sensors see is an image like this.

00:47:29.099 --> 00:47:36.610
And the idea is to be able to recover the full image
in a way that's pleasing and undetectable to the user.

00:47:36.610 --> 00:47:39.670
So this problem is generally known as the debayering,
and Apple has been working

00:47:39.670 --> 00:47:41.690
on this problem since 2004.

00:47:41.690 --> 00:47:46.470
Our algorithms fully leverage SMP and
GPU programming wherever possible.

00:47:46.469 --> 00:47:51.159
And we have some new developments coming in this for
Leopard that give us some of the best results we've seen.

00:47:51.159 --> 00:47:54.149
So we're very, very happy with
the direction we're going on this.

00:47:54.150 --> 00:47:58.070
And as always, we're very concerned about
getting the best performance and the best quality

00:47:58.070 --> 00:48:02.610
to meet the demands of the most demanding users.

00:48:02.610 --> 00:48:05.769
So let me talk a little bit about
what's involved in RAW image processing.

00:48:05.769 --> 00:48:09.079
First of all, we need to be able to decode
the file to get the actual bare sensor data.

00:48:09.079 --> 00:48:15.340
And then we need to be able to extract metadata
from that file in order to handle it correctly.

00:48:15.340 --> 00:48:20.990
Then we do spatial reconstruction, which involves
de-bayering, interpolating the pixels that are missing,

00:48:20.989 --> 00:48:26.939
compensating for noise, chroma blur and luma
sharpening to make the image more pleasing.

00:48:26.940 --> 00:48:31.700
Then there's a whole litany of color processing that's
also involved, highlight recovery, adjusting exposure

00:48:31.699 --> 00:48:36.230
and temperature/tint, converting from the
scene-referred nature of the sensor values

00:48:36.230 --> 00:48:41.199
to the output-referred color space
that is desired for rendering.

00:48:41.199 --> 00:48:46.189
And lastly, part of image processing is making
the image not just accurate, but pleasing.

00:48:46.190 --> 00:48:51.539
And so there's an additional amount of steps that is often
very desirable to adjust the image to make it look pleasing

00:48:51.539 --> 00:48:55.170
to the eye, increasing contrast for example.

00:48:56.329 --> 00:49:01.840
So this is a very complex process, and requires
dozens of parameters in order to handle it correctly.

00:49:01.840 --> 00:49:08.610
And so for each camera make and model that we support
we have default parameters for all these cameras.

00:49:08.610 --> 00:49:11.620
We are also constantly improving our RAW processing methods.

00:49:11.619 --> 00:49:16.309
And so the system maintains a set of method
versions that have been supported in the past,

00:49:16.309 --> 00:49:21.900
so that applications that need to be able to
reproduce old results can continue to do so.

00:49:21.900 --> 00:49:27.220
And lastly as I mentioned before, Leopard has a
new and improved method that greatly includes,

00:49:27.219 --> 00:49:36.449
improves handling of moire noise, high ISO noise,
and also adds broader support for D and G formats.

00:49:36.449 --> 00:49:42.909
The last thing I want to say though is that even though
we've done all this work to get good default values

00:49:42.909 --> 00:49:48.359
for all the cameras, the default parameters are never
going to be perfect for all photographs or all photographers.

00:49:48.360 --> 00:49:53.970
There's a high amount of taste and personal
preference that's involved in debayering images.

00:49:53.969 --> 00:49:58.299
And so ideally you want to give your
users control over the rendering

00:49:58.300 --> 00:50:00.800
of an image to get the best results for your users.

00:50:00.800 --> 00:50:06.160
And this is the main benefit of shooting RAW on
cameras, is that you can give your users this control,

00:50:06.159 --> 00:50:10.309
because the image has not already
been processed as an in camera JPEG.

00:50:12.719 --> 00:50:16.799
In order to provide this as a feature that
you can take advantage of in your application,

00:50:16.800 --> 00:50:25.610
we have a new filter in Leopard call CIRAWFilter,
which is designed to give your applications the ability

00:50:25.610 --> 00:50:30.050
to give your users control over the RAW processing pipeline.

00:50:30.050 --> 00:50:33.660
And also it's designed to give fast interactive performance.

00:50:33.659 --> 00:50:37.349
So here's roughly how this works in the process.

00:50:37.349 --> 00:50:42.480
You start out with a RAW image, either
URL or data, and today you know,

00:50:42.480 --> 00:50:45.920
you can do this in either Tiger or
Leopard using the core graphics APIs.

00:50:45.920 --> 00:50:50.950
If you use the core graphics APIs,
we'll use the default parameters,

00:50:50.949 --> 00:50:54.869
for example, exposure and temperature and adjustment.

00:50:54.869 --> 00:50:58.469
From that URL or data you create a CGImageSource.

00:50:58.469 --> 00:51:03.679
From that CGImageSource you can get a CGImageRef.

00:51:03.679 --> 00:51:07.119
And then from that CGImageRef
you can draw that to the display.

00:51:07.119 --> 00:51:11.559
And this works great and gives you a good default image.

00:51:11.559 --> 00:51:15.900
However, if you want to give your users
control over the rendering of this RAW image,

00:51:15.900 --> 00:51:20.230
then we have a new ability which
is to use the new CIRAWFilter.

00:51:20.230 --> 00:51:26.090
Again, you start with a RAW image, URL or data,
and we're going to create a RAW filter where we add

00:51:26.090 --> 00:51:30.200
into it the default user adjustments as input parameters.

00:51:30.199 --> 00:51:33.669
This can also be done with key value
bindings, which is really convenient.

00:51:33.670 --> 00:51:41.650
From that RAW filter we can extract a CIImage, and from
that C image, CIImage we can display it to the screen.

00:51:41.650 --> 00:51:48.079
The great advantage of this way of rendering RAW is that now
you can give the users the ability to control the sliders,

00:51:48.079 --> 00:51:51.759
which then get fed back back into the
pipeline, adjusting the input parameters,

00:51:51.760 --> 00:51:54.900
and therefore altering the output CIImage.

00:51:54.900 --> 00:51:57.510
And this can be done very quickly.

00:51:57.510 --> 00:52:02.330
Of course you don't always want to go to the display,
sometimes you need to go to the, an output file to be able

00:52:02.329 --> 00:52:05.769
to convert for example a RAW file to a TIFF or a JPEG.

00:52:05.769 --> 00:52:08.139
This can be done also using Core Image.

00:52:08.139 --> 00:52:15.259
From the CIImage you can create a CGImageRef, and from
the CGImageRef you can use CGImage destination to write

00:52:15.260 --> 00:52:18.380
to a file or TIFF, or whatever format you wish.

00:52:20.159 --> 00:52:22.769
This is how simple the code is to use this new filter.

00:52:22.769 --> 00:52:26.289
It's basically three things that we're
showing how to do in this little code snippet.

00:52:26.289 --> 00:52:32.239
We're creating a instance of the CIFilter
by calling filterWithImageURL,

00:52:32.239 --> 00:52:35.869
we're then setting in this example
just one option, which is the exposure,

00:52:35.869 --> 00:52:40.719
and we're setting the value to
minus one to darken the image a bit.

00:52:40.719 --> 00:52:47.989
And lastly, once we've set the input parameters, we get
the output CIImage by asking for the OutputImageKey.

00:52:49.489 --> 00:52:53.839
So now let me give a demo of this, and
show you how this works in practice.

00:52:58.789 --> 00:53:03.710
So. So one of the key advantages of RAW is that
because you're, you get to do the development yourself,

00:53:03.710 --> 00:53:07.750
you get to get a lot more of the
original sensor data out than you would

00:53:07.750 --> 00:53:10.820
if you were using an in-camera generated JPEG.

00:53:10.820 --> 00:53:16.660
So I want to show a couple examples of images
where you can really see the benefit of that.

00:53:16.659 --> 00:53:21.369
So I'm going to go to open here, and I've got pairs of images.

00:53:21.369 --> 00:53:26.489
I have an in-camera generated JPEG
and an in-camera generated RAW file.

00:53:26.489 --> 00:53:28.389
And we can open these both up.

00:53:28.389 --> 00:53:34.909
I should mention real quickly that this application we
have right here is a newer version of what's available

00:53:34.909 --> 00:53:37.629
on the SDK, but the principal is the same.

00:53:37.630 --> 00:53:42.360
All these sliders and controls that I'll be showing
are also available in the sample code that's

00:53:42.360 --> 00:53:46.110
on the disk, and there'll be updates to it soon.

00:53:46.110 --> 00:53:51.490
So what we have here is a picture, a
really great picture of the space shuttle

00:53:51.489 --> 00:53:54.899
as it's coming out of the vehicle assembly building.

00:53:54.900 --> 00:54:00.490
And you can see parts of the image are very bright, and
parts of the images are very dark and in the shadows.

00:54:00.489 --> 00:54:07.059
And this is a typical image where you can really benefit
from having controls over how the image is developed.

00:54:07.059 --> 00:54:12.000
If we look at this image here, we can see kind of in this
area that there's some strange, I hope you can see it,

00:54:12.000 --> 00:54:16.090
strange scion, it's like I don't know what kind of
clouds those are, I don't even really know what's there.

00:54:16.090 --> 00:54:18.110
Is that outside or inside?

00:54:18.110 --> 00:54:23.720
And if we bring the exposure down to see if it's blown out,
we see that there's nothing, we don't really learn anything

00:54:23.719 --> 00:54:26.959
by looking at the exposure, as we bring the exposure down.

00:54:26.960 --> 00:54:31.869
If I go and zoom in, you can see it's still
kind of hard to tell what's going on here.

00:54:31.869 --> 00:54:38.079
And that's because when this image was converted to JPEG by
the camera, all the values were capped at 255.

00:54:38.079 --> 00:54:43.019
So as we bring the exposure down,
there's no content there to reveal.

00:54:43.019 --> 00:54:44.719
That's entirely different with RAW files.

00:54:44.719 --> 00:54:51.109
With RAW files as you bring the exposure down, you can
actually see the content that was really in the scene,

00:54:51.110 --> 00:54:54.130
which is this scion tarp that was present.

00:54:54.130 --> 00:55:00.710
And if I zoom in here, not that far, you
can really see the detail in that tarp.

00:55:00.710 --> 00:55:07.000
So this is a textbook example of the benefits
you get by manipulating an image, a RAW image.

00:55:07.000 --> 00:55:11.030
And again as you see, we're doing that, we're
getting very interactive performance on these things.

00:55:11.030 --> 00:55:14.620
So let me set this exposure to minus one exactly.

00:55:14.619 --> 00:55:20.009
And now as we look at the image,
let me go back to zoom to fit.

00:55:20.010 --> 00:55:24.640
Because we brought the exposure down,
a lot of the dark areas got darker.

00:55:24.639 --> 00:55:29.559
And oftentimes that's desirable, it can
make an image look more contrasty.

00:55:29.559 --> 00:55:33.090
But other times you want to show detail in those areas.

00:55:33.090 --> 00:55:38.600
And this is one of the things we've spent a lot of time
on for the new version of RAW in Leopard, is to give new

00:55:38.599 --> 00:55:42.960
and better controls to adjust the shadow areas of an image.

00:55:42.960 --> 00:55:47.809
This is parts of images a lot of discriminating
photographers really want to be able to see,

00:55:47.809 --> 00:55:51.480
and to gather detail out of the darkest areas of the image.

00:55:51.480 --> 00:55:56.179
So as we zoom in here, and let me make sure I'm at the
right level, I'm hoping you can see this on the display.

00:55:56.179 --> 00:56:03.419
But I've got two new sliders here, one is one that's
sort of a bias on the exposure that we can bring down.

00:56:03.420 --> 00:56:06.409
And as we go down it will make the darks lighter.

00:56:06.409 --> 00:56:15.899
And then also there is another adjustment here which also
affects the shadows, and affects how much they're toned.

00:56:15.900 --> 00:56:21.660
So if we were to try to do the same thing on the JPEG,
A we don't have some of these controls available,

00:56:21.659 --> 00:56:25.719
and oftentimes because it's only an 8-bit image,

00:56:25.719 --> 00:56:31.359
making the shadows darker will just leave
you with a very posturized looking image.

00:56:31.360 --> 00:56:39.099
Let me look at, see if there's one more image to look at.

00:56:40.309 --> 00:56:43.349
Here's another fun set.

00:56:45.619 --> 00:56:50.619
Again, with the JPEG we don't have enough controls, if
we want to bring the exposure down we lose a lot of,

00:56:50.619 --> 00:56:56.579
we don't get any extra content in the cap clouds,
and we lose all of our detail in the shadows.

00:56:56.579 --> 00:57:02.929
When we go into a RAW file we can bring the
exposure down, and continue to see the cloud detail.

00:57:02.929 --> 00:57:10.129
And as we bring the shadows up we can
see all the details in the scaffolding.

00:57:10.130 --> 00:57:16.760
If I go in, zoom in, you can see all
the corrugated metal on the sides.

00:57:16.760 --> 00:57:18.080
Hopefully you can see that up there.

00:57:18.079 --> 00:57:20.619
No, it's not quite as visible for you.

00:57:20.619 --> 00:57:23.829
So that's what I'd like to show about the RAW.

00:57:23.829 --> 00:57:25.769
Again, this is the CIRAWFilter sample.

00:57:25.769 --> 00:57:31.039
It's a very simple CIFilter to use, and it'll allow
you to give your users all the controls for this.

00:57:31.039 --> 00:57:32.509
So back to the slides.

00:57:32.510 --> 00:57:40.040
( applause )

00:57:40.039 --> 00:57:41.789
So that's our discussion for today.

00:57:41.789 --> 00:57:44.309
I just want to have a few final thoughts.

00:57:44.309 --> 00:57:49.750
As we learned from today, you can use Core Image
to process more than just general purpose data.

00:57:49.750 --> 00:57:57.400
As Ralph was talking about earlier, you can put information
like XY coordinates and angles in a pixel buffer,

00:57:57.400 --> 00:58:00.349
and give it to Core Image to process your data.

00:58:00.349 --> 00:58:07.110
Another good bit of advice is you can use Quartz Composer
as a prototyping tool for developing new CIKernels.

00:58:07.110 --> 00:58:12.500
Once you've developed a kernel and debugged it in their
interactive debugger, you can then take that kernel

00:58:12.500 --> 00:58:17.369
and patch it, and package it as an image unit.

00:58:17.369 --> 00:58:23.710
Next you can use CIFilters together with core animation,
which work together very nicely, to spice up your interface,

00:58:23.710 --> 00:58:29.300
to add transition, roll-over effects, or subtle transparencies.

00:58:29.300 --> 00:58:33.170
Also keep in mind that many of our Macs
these days ship with built-in iSights

00:58:33.170 --> 00:58:37.240
with are a great input device to
add interesting effects over.

00:58:37.239 --> 00:58:42.189
And most important, amaze your users,
and give them great dynamic control

00:58:42.190 --> 00:58:49.039
over images that will wow them and amaze all of us.

00:58:49.039 --> 00:58:50.349
So that's the end of our conversation.

00:58:50.349 --> 00:58:56.519
We have a lab that follows this, which will be a great time
to talk to all of us for questions and answers periods.

00:58:56.519 --> 00:59:02.360
And for more information, contact Allan,
who's our Graphics and Imaging Evangelist,

00:59:02.360 --> 00:59:06.160
or check out our documentation
and other resources on the web.