WEBVTT

00:00:14.760 --> 00:00:15.820
Okay, so thank you.

00:00:15.820 --> 00:00:19.830
Welcome to session 404,
and we're going to be talking about

00:00:20.010 --> 00:00:22.860
three primary APIs in Core Audio.

00:00:22.860 --> 00:00:24.420
Oh, look, it even works.

00:00:24.420 --> 00:00:29.190
Audio queues, audio streams,
and extended audio files.

00:00:29.190 --> 00:00:32.430
And it's sort of like
a high-level overview,

00:00:32.430 --> 00:00:36.460
and I think you'll find the
information very useful.

00:00:36.460 --> 00:00:39.840
And without any more ado,
I'm going to bring up Doug Wyatt,

00:00:39.840 --> 00:00:42.790
and he's going to get us
started with audio queues.

00:00:42.810 --> 00:00:43.960
Thank you, Doug.

00:00:43.960 --> 00:00:44.700
Doug?

00:00:47.530 --> 00:00:52.840
So this is actually a hands-on session,
meaning that if you've got the code

00:00:52.840 --> 00:00:58.030
off of the developer attendee site,
the code for the program I'm going to

00:00:58.030 --> 00:01:01.060
show you in this session is on that site.

00:01:01.060 --> 00:01:01.760
You can look at it now.

00:01:01.760 --> 00:01:02.620
You can look at it later.

00:01:02.620 --> 00:01:06.540
But in either case,
I'm going to walk through a little

00:01:06.540 --> 00:01:09.180
program there called AQ Record.

00:01:09.180 --> 00:01:11.790
So in the session,
what I'm going to cover pertaining

00:01:11.850 --> 00:01:17.030
to the audio queue is what it is,
how it's constructed, how you set one up,

00:01:17.030 --> 00:01:19.270
how you manage buffers of audio.

00:01:19.270 --> 00:01:23.800
And in this example program,
we'll record some audio buffers

00:01:23.800 --> 00:01:26.320
from the hardware to a file.

00:01:26.320 --> 00:01:29.730
And then there's also an
example program called AQ Play,

00:01:29.730 --> 00:01:34.250
which we won't actually step through,
but I'll speak briefly about

00:01:34.260 --> 00:01:39.460
its similarities and differences
compared to AQ Record.

00:01:40.110 --> 00:01:42.020
So I'd like to go to the demo machine.

00:01:42.170 --> 00:01:45.740
We'll start from the outside,
and I'll just show you the simple

00:01:45.740 --> 00:01:49.600
little command line program here.

00:01:49.600 --> 00:01:51.860
And what screen are we on?

00:01:51.900 --> 00:01:55.390
Do we have any windows?

00:01:55.400 --> 00:01:59.360
Is that a separate -- ah, okay.

00:02:01.120 --> 00:02:03.340
Now I can't see it.

00:02:03.340 --> 00:02:06.550
But fortunately,
I typed my command line a little earlier.

00:02:06.630 --> 00:02:12.360
So here-- OK, let me actually start
the iPod before I record.

00:02:12.990 --> 00:02:18.060
Okay, so this is some music
I recorded a couple years ago.

00:02:18.060 --> 00:02:21.340
And I'm playing it from
my iPod into the Mac.

00:02:21.340 --> 00:02:23.770
And now we should have an audio file.

00:02:23.990 --> 00:02:28.560
And if I just say AQ Play,
hopefully we will hear

00:02:28.890 --> 00:02:31.450
what I just recorded,
but maybe not.

00:02:31.500 --> 00:02:34.560
Do you know if the audio
is up on this machine?

00:02:36.530 --> 00:02:36.920
Okay.

00:02:36.920 --> 00:02:39.460
Well, in any case,
you see the basic usage here.

00:02:39.460 --> 00:02:40.970
It's a very simple command line app.

00:02:41.040 --> 00:02:43.760
We just specify a file name.

00:02:43.760 --> 00:02:47.900
There's,
if I run AQ record without options,

00:02:47.900 --> 00:02:52.850
just in the spirit of format agnosticism,
as Jeff was talking about

00:02:52.880 --> 00:02:56.610
in the last session,
you can see that I can do things

00:02:56.610 --> 00:03:00.870
like AQ record dash D AAC,
and it'll make an AAC file.

00:03:00.870 --> 00:03:05.340
That would be an AAC file.

00:03:06.500 --> 00:03:09.070
OK, back to slides, please.

00:03:14.000 --> 00:03:17.400
Okay, so as we walk through the source
code to this little program,

00:03:17.400 --> 00:03:21.050
we'll see that here are the
steps that it goes through.

00:03:21.240 --> 00:03:25.690
First, we'll figure out what audio
format we want to record into.

00:03:25.780 --> 00:03:29.140
We'll create the audio file to
actually hold those samples.

00:03:29.160 --> 00:03:34.340
We'll create an audio queue to get
audio asynchronously from the hardware.

00:03:34.390 --> 00:03:40.520
And as we receive buffers from the queue,
then we'll write them

00:03:40.520 --> 00:03:40.520
out to the audio file.

00:03:42.350 --> 00:03:47.330
So in terms of what pieces of
API this program is built on top of,

00:03:47.380 --> 00:03:48.980
there's not only the audio queue.

00:03:48.980 --> 00:03:52.740
There's also the audio
file and audio format APIs,

00:03:52.740 --> 00:03:55.980
which Jeff Moore described
in the last session.

00:03:55.980 --> 00:03:59.230
Those in green are the
audio toolbox APIs,

00:03:59.230 --> 00:04:01.380
which this program uses.

00:04:01.380 --> 00:04:05.910
Underneath the audio queue,
we're using the Core Audio HAL or

00:04:05.910 --> 00:04:11.040
the audio hardware APIs,
but only indirectly through the queue.

00:04:11.300 --> 00:04:13.060
And actually,
this example program does have one little

00:04:13.070 --> 00:04:17.110
place where it talks to the HAL directly,
but I'll show you that in a sec.

00:04:18.020 --> 00:04:22.500
So what the queue does, as I said,
it receives buffers from

00:04:22.500 --> 00:04:24.380
your audio hardware.

00:04:24.380 --> 00:04:28.680
And as it receives those buffers from
the hardware in the hardware format,

00:04:28.680 --> 00:04:33.000
then it runs them through an
internal audio converter to convert

00:04:33.000 --> 00:04:36.860
those buffers of audio to whatever
your desired recording format is,

00:04:36.860 --> 00:04:40.460
whether that be 16-bit integers or AAC.

00:04:41.460 --> 00:04:46.390
Any encoder that is on the
system can be used to encode

00:04:46.480 --> 00:04:49.690
your audio to whatever format.

00:04:49.790 --> 00:04:53.710
So your audio will receive buffers in
that encoded format from the queue.

00:04:57.140 --> 00:05:00.650
Okay,
so stepping into the code for AQ record,

00:05:00.650 --> 00:05:05.060
here,
this is about halfway down through main,

00:05:05.060 --> 00:05:06.910
after it's parsed all the
command line arguments,

00:05:06.910 --> 00:05:10.110
so it's getting ready to
actually do some work.

00:05:10.150 --> 00:05:13.360
It's going to default to
the calf audio file type.

00:05:13.360 --> 00:05:16.060
That's that constant there.

00:05:16.060 --> 00:05:21.370
Then we're going to try to figure out if
the user specified some other extension

00:05:21.370 --> 00:05:23.360
on his file name on the command line.

00:05:23.360 --> 00:05:27.130
For instance, if he entered a file
name that ended in .aif,

00:05:27.210 --> 00:05:32.390
then we'd like to create
an aiff audio file instead.

00:05:32.520 --> 00:05:35.110
So that's going to be
done by this function,

00:05:35.110 --> 00:05:37.620
infer audio file format from file name.

00:05:37.780 --> 00:05:41.330
To get there,
we just need to convert the command line

00:05:41.380 --> 00:05:45.580
argument record file name to a CFString,
CF record file name.

00:05:45.750 --> 00:05:51.850
Then we can pass it to our function
and get back the audio file type ID.

00:05:51.880 --> 00:05:55.650
for whatever might have been specified.

00:05:56.470 --> 00:05:59.360
So the guts of that function,
I won't go through all

00:05:59.360 --> 00:06:02.380
the mechanics there,
but the essential point here is

00:06:02.490 --> 00:06:07.040
that it calls an audio file function
called audio file get global info.

00:06:07.040 --> 00:06:13.970
Audio file get global info when given
an extension in the form of a CF ref.

00:06:14.540 --> 00:06:20.400
And so, for example, if that were .aif,
then this function will know, oh,

00:06:20.400 --> 00:06:27.100
that's an extension for aif, aiff,
rather, and give us back that constant.

00:06:29.000 --> 00:06:32.810
So that's an example of one of the
nice little bits of information

00:06:32.810 --> 00:06:35.780
the AudioFalc API can give us.

00:06:36.820 --> 00:06:38.340
Okay, getting into a bit more code.

00:06:38.340 --> 00:06:44.940
So this is setting up the audio
stream basic description for the

00:06:44.940 --> 00:06:49.190
format which we want to receive from
the queue and write to the file.

00:06:49.270 --> 00:06:53.270
So we're going to call that
record format in this program.

00:06:53.800 --> 00:06:57.310
Now, the way the program is set up,
it's trying to just have

00:06:57.310 --> 00:07:00.380
some reasonable defaults.

00:07:00.380 --> 00:07:03.330
If you look in the code,
record format is just filled

00:07:03.330 --> 00:07:05.140
out completely with zeros.

00:07:05.250 --> 00:07:09.220
So what we'll do first, we'll say, well,
if the sample rate is still

00:07:09.220 --> 00:07:11.880
zero-- in other words,
it wasn't specified on the command

00:07:11.910 --> 00:07:16.700
line-- then we'll get the sample
rate from the hardware device.

00:07:16.790 --> 00:07:19.520
Then the next if statement
will default to stereo,

00:07:19.520 --> 00:07:20.950
two channels.

00:07:20.950 --> 00:07:24.860
Then that last large block,
if it's a ref block.

00:07:24.920 --> 00:07:28.420
If the record format
is zero or linear PCM,

00:07:28.420 --> 00:07:32.190
then we've got to elaborate and say,
what kind of linear PCM do we want?

00:07:32.280 --> 00:07:34.890
So we're going to default
to 16-bit integers.

00:07:35.050 --> 00:07:39.650
We'll decide whether to use bigger
little endian 16-bit integers,

00:07:39.650 --> 00:07:42.140
depending on the file format.

00:07:42.140 --> 00:07:45.400
We'll set up bytes per channel,
bytes per frame as functions of bits

00:07:45.420 --> 00:07:48.990
per channel and channels per frame,
and so on.

00:07:49.000 --> 00:07:52.310
So by the end of this,
we've got a full specification

00:07:52.310 --> 00:07:53.620
of the audio stream.

00:07:53.660 --> 00:07:56.860
Basically, we're going to have a full
description record format.

00:07:58.540 --> 00:08:02.030
So going back, the function,
here's the function that we

00:08:02.030 --> 00:08:05.450
called to figure out what
the hardware sample rate is.

00:08:05.450 --> 00:08:09.430
This is the one place we have to
sort of duck underneath to ask.

00:08:09.460 --> 00:08:12.570
Actually, in this example,
I'm using Audio Hardware Services,

00:08:12.570 --> 00:08:15.400
and I'm not talking directly to the HAL,
but I might as well,

00:08:15.400 --> 00:08:18.730
if I were on a Tiger system where
Audio Hardware Services didn't exist,

00:08:18.730 --> 00:08:21.680
I could call the HAL directly
here to make these calls.

00:08:21.680 --> 00:08:25.120
But in fact, on Leopard,
I could call Audio Hardware Services,

00:08:25.120 --> 00:08:27.780
which is a way of avoiding
talking directly to the HAL.

00:08:27.780 --> 00:08:31.040
But in any case,
those APIs are very similar.

00:08:31.040 --> 00:08:34.750
I can ask,
so what is the default input device?

00:08:34.750 --> 00:08:37.940
I can get back an audio device ID for it.

00:08:37.940 --> 00:08:41.800
Given that audio device ID,
it's the local variable device ID.

00:08:41.800 --> 00:08:44.940
I can ask the device for
its nominal sample rate,

00:08:44.940 --> 00:08:49.300
be it 44,100, 48,000, or whatever.

00:08:49.300 --> 00:08:51.660
And that will come back in.

00:08:51.680 --> 00:09:02.810
And that's the function that we use here,
my get default input sample rate.

00:09:04.160 --> 00:09:05.290
Okay, moving on.

00:09:05.290 --> 00:09:07.360
Now we know the record format.

00:09:07.410 --> 00:09:10.280
We've got it fully specified.

00:09:10.280 --> 00:09:14.840
So we can ask the queue to -- well,
we can create a new queue, rather,

00:09:14.840 --> 00:09:18.900
and ask it to provide
us that format of audio.

00:09:19.000 --> 00:09:24.920
So the record format's the first
argument to audio queue new input.

00:09:25.670 --> 00:09:30.420
The queue will use the
default input device.

00:09:30.420 --> 00:09:34.130
We're passing Audio Queue new
input the address of a function,

00:09:34.130 --> 00:09:35.890
my input buffer handler.

00:09:36.010 --> 00:09:39.810
It's going to call this function
every time new data arrives.

00:09:39.900 --> 00:09:44.240
And it's going to pass the
address of the local AQR,

00:09:44.360 --> 00:09:46.850
which is my user data for
the callback function,

00:09:46.860 --> 00:09:51.700
just so that in that callback,
I've got access to data

00:09:51.700 --> 00:09:54.200
that was declared in main.

00:09:55.180 --> 00:09:59.260
I've got two null arguments
to Audio Queue New Input,

00:09:59.260 --> 00:10:04.260
specifying the run loop and mode on which
I want my callback function to be called.

00:10:04.260 --> 00:10:06.220
This is a slightly more advanced feature.

00:10:06.220 --> 00:10:09.680
There are some situations
where you might want to say,

00:10:09.680 --> 00:10:11.870
OK, here's my disk I/O thread.

00:10:11.900 --> 00:10:17.160
I want to receive the incoming
buffers of audio on that thread.

00:10:17.320 --> 00:10:19.850
But in this case,
this is a simple program.

00:10:19.850 --> 00:10:22.680
I can just say null,
and the queue will construct

00:10:22.780 --> 00:10:26.900
its own thread internally,
and it will deliver me

00:10:26.900 --> 00:10:29.460
buffers on that thread.

00:10:30.030 --> 00:10:34.250
And so, the last argument,
the address of AQR.queue is my

00:10:34.340 --> 00:10:37.570
newly created Audio Queue object.

00:10:40.700 --> 00:10:42.600
Okay,
there's one more little detail here.

00:10:42.600 --> 00:10:46.820
So this variable record
format that I passed,

00:10:46.820 --> 00:10:50.220
that's my audio stream basic description,
and I've passed it to

00:10:50.220 --> 00:10:51.560
Audio Queue New Input.

00:10:51.620 --> 00:10:55.900
In the case of a
compressed format like AAC,

00:10:55.900 --> 00:11:03.070
all I had to specify was AAC,
two channels, 44,100 hertz,

00:11:03.140 --> 00:11:07.380
and that was enough for the queue to
be able to create an audio converter.

00:11:08.200 --> 00:11:13.190
But that's not necessarily a
sufficiently accurate audio stream basic

00:11:13.340 --> 00:11:15.580
description for the audio file API.

00:11:15.580 --> 00:11:19.880
So what I'm doing now is I'm
turning around and asking the queue,

00:11:19.880 --> 00:11:23.150
so, okay,
I gave you this sort of half-baked,

00:11:23.300 --> 00:11:25.200
maybe, record format.

00:11:25.200 --> 00:11:27.540
Can you really fill it
out completely for me?

00:11:27.540 --> 00:11:30.800
And that's what this call here does.

00:11:30.850 --> 00:11:33.970
I'm asking the queue,
please give me your underlying audio

00:11:34.040 --> 00:11:36.450
converter's output stream description.

00:11:37.850 --> 00:11:40.400
And if it were PCM,
then I'm just going to get

00:11:40.400 --> 00:11:44.760
back exactly what I gave it,
but if I gave it sort of

00:11:44.770 --> 00:11:48.200
that half-baked AAC format,
then I'll get a full

00:11:48.200 --> 00:11:52.890
description of the AAC format,
for example, with 1024 frames per packet.

00:11:52.900 --> 00:11:57.890
And now my local variable record
format is fully specified.

00:12:01.100 --> 00:12:03.850
So given that full specification
of the record format,

00:12:03.860 --> 00:12:07.590
now I'm ready to turn around and
create my audio file using that

00:12:07.590 --> 00:12:09.940
audio data format specification.

00:12:09.940 --> 00:12:14.640
I've got my audio file type,
which I determined earlier to

00:12:14.700 --> 00:12:19.980
be defaulted to CAF or .aif,
would have specified an .aiff audio file.

00:12:21.170 --> 00:12:25.990
So at this point,
now I've created an empty audio

00:12:26.150 --> 00:12:32.770
file containing exactly the
audio data format that I want.

00:12:35.420 --> 00:12:38.790
Now, in the case of a compressed format,
Jeff Moore spoke a bit about

00:12:38.790 --> 00:12:41.940
Magic Cookies in the previous session.

00:12:41.940 --> 00:12:44.300
Magic Cookie, again,
for those of you who weren't there,

00:12:44.300 --> 00:12:47.960
is just a little bit of private
data from the encoder that the

00:12:47.960 --> 00:12:50.440
decoder needs to decode the stream.

00:12:51.300 --> 00:12:56.300
And many audio file formats
will hold Magic Cookies.

00:12:56.300 --> 00:12:59.920
In the case of all the
Core Audio APIs that

00:13:00.000 --> 00:13:02.540
deal with Magic Cookies,
all you really need to know

00:13:02.540 --> 00:13:07.050
is that the Core Audio API,
you'll get the cookie from one place

00:13:07.050 --> 00:13:08.400
and you can deliver it to another.

00:13:08.400 --> 00:13:12.650
So in this case, we just need to get the
cookie from the audio queue,

00:13:12.650 --> 00:13:16.700
say, so what's your converter's
compression Magic Cookie?

00:13:16.860 --> 00:13:19.240
And then we turn around,
assuming there is one.

00:13:19.240 --> 00:13:21.030
That's what that first
if statement is checking.

00:13:21.040 --> 00:13:23.300
If it were PCM,
there wouldn't be a magic cookie.

00:13:23.300 --> 00:13:26.910
But if there is a magic cookie,
then we fetch it from the queue,

00:13:26.910 --> 00:13:29.910
and we set the cookie on the audio file.

00:13:33.530 --> 00:13:37.730
Okay, before we go further in the code,
I thought a little picture

00:13:37.730 --> 00:13:40.400
might be worth a bunch of words.

00:13:40.400 --> 00:13:44.500
This is what's going on inside the queue.

00:13:44.500 --> 00:13:49.750
We've got the audio hardware symbolized
by a microphone here on the left,

00:13:49.900 --> 00:13:52.780
providing input data.

00:13:52.810 --> 00:13:58.470
That's going through an audio converter,
converting from whatever format the

00:13:58.470 --> 00:14:02.960
hell is delivering the samples in
to the format I want to record in.

00:14:03.460 --> 00:14:08.010
Now, the interesting concept here is
the idea of the buffer queue,

00:14:08.010 --> 00:14:11.300
and that's the object from which
the queue derives its name.

00:14:11.300 --> 00:14:15.840
In the case of an input queue like this,
the buffer queue is simply

00:14:15.840 --> 00:14:19.460
a sequence of buffers that
the queue will fill in order.

00:14:19.460 --> 00:14:23.780
So from the application's point of view,
we're going to enqueue

00:14:23.780 --> 00:14:27.520
a series of buffers,
and then as audio comes in,

00:14:27.520 --> 00:14:32.080
the queue will fill those
buffers and return them to us.

00:14:32.800 --> 00:14:35.170
So in the diagram,
you'll see that the converter's filling

00:14:35.170 --> 00:14:36.980
buffer number one in the buffer queue.

00:14:36.980 --> 00:14:39.380
When that buffer gets
full or nearly full,

00:14:39.390 --> 00:14:42.940
that buffer gets delivered to
the queue's callback function,

00:14:42.940 --> 00:14:47.380
and we'll see what this example program's
callback looks like in a moment.

00:14:49.690 --> 00:14:54.070
From the callback function
in our example program,

00:14:54.120 --> 00:14:59.330
we'll write that audio data to the file,
and we'll turn around and call

00:14:59.330 --> 00:15:03.200
audio queue and queue buffer again
to effectively recycle the buffer.

00:15:03.200 --> 00:15:09.240
We're done writing data from it,
and we're ready to have it refilled.

00:15:11.410 --> 00:15:16.190
So that was a good introduction
to the process of dealing

00:15:16.190 --> 00:15:17.920
with buffers and the queue.

00:15:17.920 --> 00:15:21.780
So the buffers are actually
owned by the queue,

00:15:21.780 --> 00:15:26.700
which saves you some memory management
problems and addresses some shortcomings

00:15:26.700 --> 00:15:30.500
and typical usage errors we had with
the sound manager all those years ago.

00:15:31.180 --> 00:15:35.370
So what we'll do is we'll ask the
queue to allocate our buffers for us.

00:15:35.370 --> 00:15:39.370
But before we do that,
first we're going to have to

00:15:39.370 --> 00:15:44.780
figure out how many bytes in
size we wish these buffers to be,

00:15:44.850 --> 00:15:48.530
just to try to strike a
right balance between various

00:15:48.620 --> 00:15:51.080
performance characteristics.

00:15:51.080 --> 00:15:55.270
We want the buffers to be long enough
in time that we're not spending

00:15:55.270 --> 00:15:59.640
all of our time just going back
and forth between the file system.

00:15:59.640 --> 00:16:00.260
But we also want the buffers to be
long enough in time that we're not

00:16:00.350 --> 00:16:01.150
spending all of our time just going
back and forth between the file system.

00:16:01.160 --> 00:16:03.740
We want the buffers not to be
so enormously large that we're

00:16:03.740 --> 00:16:05.140
chewing up lots of memory.

00:16:05.140 --> 00:16:10.440
And to be format agnostic,
a half second of audio,

00:16:10.440 --> 00:16:17.300
which is a reasonable compromise in
terms of not hitting the disk too much,

00:16:17.300 --> 00:16:22.450
a half second of audio could range from
being rather small to fairly substantial.

00:16:22.460 --> 00:16:25.180
So we'll look at that
function in a moment,

00:16:25.180 --> 00:16:27.400
my compute record buffer size.

00:16:27.400 --> 00:16:31.080
So what we'll do is we'll,
once we know that,

00:16:31.140 --> 00:16:31.140
we'll look at the function in a moment,
my compute record buffer size.

00:16:31.150 --> 00:16:31.330
So what we'll do is we'll look
at the function in a moment,

00:16:31.340 --> 00:16:31.520
my compute record buffer size.

00:16:31.520 --> 00:16:34.840
Then we can allocate some
number of buffers of that size.

00:16:34.840 --> 00:16:36.260
Three is a good typical number.

00:16:36.260 --> 00:16:40.150
And we'll enqueue those buffers,
meaning that once the queue starts,

00:16:40.150 --> 00:16:44.260
the hardware and audio starts coming in,
that buffer will get filled,

00:16:44.360 --> 00:16:46.200
as we saw in the last diagram.

00:16:50.600 --> 00:18:38.300
( Transcript missing )

00:18:40.650 --> 00:18:43.130
Okay,
so we've enqueued all these buffers.

00:18:43.370 --> 00:18:47.500
We've got a file all set up
and ready to record into.

00:18:47.530 --> 00:18:50.970
I don't think there's anything
left to do now but set this member

00:18:50.970 --> 00:18:54.170
variable AQR.running on my structure.

00:18:54.260 --> 00:18:56.840
We'll see how that's used in a moment.

00:18:56.840 --> 00:19:00.670
Call audio queue start,
which starts the underlying hardware

00:19:00.670 --> 00:19:06.510
and starts the audio converter as it
receives the buffers from the hardware.

00:19:06.640 --> 00:19:10.290
And then we can basically
sleep for five seconds.

00:19:10.760 --> 00:19:15.480
I'm using CFRunLoop run-in mode,
but I think I have no RunLoop

00:19:15.480 --> 00:19:20.100
sources on this thread,
so I'll just sleep for five seconds.

00:19:22.450 --> 00:19:28.320
Okay, so as the buffers of encoded
audio are filled by the queue,

00:19:28.320 --> 00:19:30.900
then they're delivered to this function.

00:19:30.900 --> 00:19:34.440
And you'll remember this is the one,
this is the function I passed

00:19:34.440 --> 00:19:36.060
to audio queue new input.

00:19:36.060 --> 00:19:38.860
So this is my input buffer handler.

00:19:38.860 --> 00:19:42.100
The first thing it
does is just make sure,

00:19:42.100 --> 00:19:44.650
kind of a sanity check,
that we received at least

00:19:44.700 --> 00:19:45.940
one packet of audio.

00:19:46.830 --> 00:19:49.790
And assuming we did,
then we can just turn around and

00:19:49.890 --> 00:19:54.300
call audio file write packets,
given the record file that

00:19:54.300 --> 00:19:56.640
we set up early in main.

00:19:56.640 --> 00:20:01.140
And using the length of the buffer,
that's in buffer arrow

00:20:01.140 --> 00:20:05.000
M audio data byte size,
we've been given possibly

00:20:05.000 --> 00:20:09.480
some packet descriptions,
depending on the format.

00:20:09.520 --> 00:20:13.270
If it's a compressed format that
requires packet descriptions,

00:20:13.270 --> 00:20:16.530
then in packet desk,
that pointer will be non-null.

00:20:16.760 --> 00:20:17.820
Thank you.

00:20:18.740 --> 00:20:23.960
We also have the argument, or I'm sorry,
the variable AQR error record packet.

00:20:23.960 --> 00:20:27.330
That's, if we go look at the arguments
to audio file write packet,

00:20:27.330 --> 00:20:30.080
that's just saying the packet
number in the file that we

00:20:30.090 --> 00:20:31.610
want to start writing at.

00:20:31.640 --> 00:20:34.010
Earlier in the program,
I didn't show you this detail,

00:20:34.010 --> 00:20:35.460
but we started at packet zero.

00:20:35.460 --> 00:20:40.280
So the first buffer that comes in will
get written as packet zero to the file.

00:20:40.500 --> 00:20:43.790
And then after we call
audio file write packets,

00:20:43.800 --> 00:20:47.660
then we're going to advance
AQR error record packet by the

00:20:47.660 --> 00:20:50.500
number of packets that we just wrote.

00:20:50.900 --> 00:20:54.200
Okay, so we received a buffer
of audio from the queue.

00:20:54.200 --> 00:20:55.660
We wrote it to the file.

00:20:55.810 --> 00:21:01.410
And now, the only thing left to do is,
unless we're in the middle of stopping,

00:21:01.410 --> 00:21:06.170
in other words, if we're still running,
then we're going to turn around

00:21:06.190 --> 00:21:10.210
and re-enqueue the buffer,
telling the queue, okay,

00:21:10.270 --> 00:21:14.000
put this at the end of the
queue of buffers and fill it

00:21:14.000 --> 00:21:16.560
when it comes around again.

00:21:18.240 --> 00:21:21.550
So that's the input buffer handler.

00:21:23.320 --> 00:21:31.400
So having done all that,
when that five seconds expires in main,

00:21:31.400 --> 00:21:35.340
we simply have to stop the queue
after setting our member variable

00:21:35.340 --> 00:21:38.520
running to false so we don't keep
trying to recycle the buffers.

00:21:38.520 --> 00:21:40.100
So we stop the queue.

00:21:40.100 --> 00:21:42.140
The true argument means yes immediately.

00:21:43.120 --> 00:21:48.560
We call this function to copy the magic
cookie from the queue to the record file.

00:21:48.560 --> 00:21:53.330
Again, sometimes a cookie can change
as the codec is running,

00:21:53.330 --> 00:21:59.580
so it's best to set the cookie
both before and after recording.

00:22:00.700 --> 00:22:02.700
Okay,
so we've written the cookie to the file.

00:22:02.700 --> 00:22:04.160
The file's just about done.

00:22:04.160 --> 00:22:05.320
We're disposing the queue.

00:22:05.320 --> 00:22:06.280
We're closing the file.

00:22:06.280 --> 00:22:09.280
And that's pretty much
all there is to recording.

00:22:13.940 --> 00:22:16.850
So having walked through
all that about recording,

00:22:16.860 --> 00:22:20.880
that's the AQ Record program,
the queue's anatomy for

00:22:21.050 --> 00:22:24.160
audio playback is similar,
very similar,

00:22:24.160 --> 00:22:27.480
just different and a few little wrinkles.

00:22:27.480 --> 00:22:33.400
So instead of enqueuing buffers
to be filled from incoming audio,

00:22:33.480 --> 00:22:38.500
your program's job is to
supply buffers to be played.

00:22:39.800 --> 00:22:43.480
And just like with a recording queue,
there's an audio converter so

00:22:43.480 --> 00:22:47.300
you can supply your buffers
in a compressed format.

00:22:47.300 --> 00:22:52.570
As the buffer is completely played,
it's passed to a callback

00:22:52.570 --> 00:22:58.250
function whose job is to refill
it before re-enqueuing it.

00:23:00.350 --> 00:23:03.900
So this is pretty much
what I just told you,

00:23:03.900 --> 00:23:06.860
the similarities for playback.

00:23:06.940 --> 00:23:08.600
You can create and configure the queue.

00:23:08.600 --> 00:23:10.090
You enqueue your buffers the same way.

00:23:10.100 --> 00:23:12.090
You start the queue the same way.

00:23:12.210 --> 00:23:15.260
The completed buffers arrive
at your callback function.

00:23:15.370 --> 00:23:23.080
And in many cases,
your callback function will just

00:23:23.080 --> 00:23:23.080
re-enqueue the completed buffer.

00:23:24.040 --> 00:23:27.360
And the main difference really is
what happens in the callback function,

00:23:27.360 --> 00:23:32.040
since in the recording case,
you're just going to be reading

00:23:32.040 --> 00:23:34.500
the audio out of the buffer
and writing it to the file,

00:23:34.500 --> 00:23:37.720
whereas in the playback case,
you'll be reading it from the file

00:23:37.720 --> 00:23:41.720
and writing it into the queues buffer.

00:23:44.100 --> 00:23:48.580
Where this actually shows up
most distinctly as a difference

00:23:48.580 --> 00:23:52.100
in your programs is if you
look in the AQ play example,

00:23:52.100 --> 00:23:57.280
when we first start playing,
we have to call our own callback function

00:23:57.280 --> 00:24:02.520
manually to fill the buffers from disk
and enqueue them for the first time.

00:24:02.520 --> 00:24:05.030
That's a minor wrinkle,
and it makes sense when

00:24:05.030 --> 00:24:06.260
you walk through it.

00:24:06.340 --> 00:24:09.870
So I just wanted to give you
that brief look at how similar

00:24:10.040 --> 00:24:11.740
playing is to recording.

00:24:11.740 --> 00:24:14.080
So what I've shown you here is a little
bit of a difference between the two.

00:24:14.080 --> 00:24:18.070
What I've shown you here is that using
queues for recording and playing back

00:24:18.070 --> 00:24:20.360
audio is pretty simple and efficient.

00:24:20.360 --> 00:24:23.830
The queue takes care of
your memory management,

00:24:24.160 --> 00:24:28.200
converting between encoded
audio formats and linear PCM.

00:24:28.200 --> 00:24:30.280
There's a volume control on the queue.

00:24:30.280 --> 00:24:31.500
I didn't show you that, but it's there.

00:24:31.500 --> 00:24:35.530
It manages all of those lower-level
interactions with the audio hardware,

00:24:35.530 --> 00:24:37.840
including, of course, doing the I.O.

00:24:37.850 --> 00:24:39.000
to the hardware.

00:24:40.650 --> 00:24:45.060
And because it does that for you,
it leaves your application free to

00:24:45.060 --> 00:24:49.760
do its processing of incoming audio,
whether it's recording it to a

00:24:49.760 --> 00:24:52.240
file or doing signal processing.

00:24:52.240 --> 00:24:55.010
You can do all of your work in
less time-critical contexts,

00:24:55.010 --> 00:24:58.120
and that's the benefit of the queue.

00:24:58.120 --> 00:25:00.130
And with that,
I'll bring up James McCartney.

00:25:00.160 --> 00:25:01.740
Thank you.

00:25:08.000 --> 00:25:09.710
JAMES MCCARTHY: OK.

00:25:09.860 --> 00:25:11.650
I'm James McCartney,
and I'm going to talk about

00:25:11.650 --> 00:25:15.280
the Audio File Stream API.

00:25:15.410 --> 00:25:18.630
It's a new API in Leopard,
and it's for dealing with

00:25:19.140 --> 00:25:22.600
non-random access audio streams.

00:25:22.600 --> 00:25:25.630
The basic mode of operation
is a three-step process.

00:25:25.660 --> 00:25:30.730
You open an audio file stream,
passing it callback functions that will

00:25:30.980 --> 00:25:38.740
call you back with properties and audio
data packets that it finds in the stream.

00:25:39.020 --> 00:25:42.460
The second step is to parse the data.

00:25:42.460 --> 00:25:46.240
You enter a loop,
and you pass buffers to the

00:25:46.900 --> 00:25:52.250
Audio File Stream parser,
and it will call your callbacks.

00:25:52.470 --> 00:25:56.790
And then when you're finished,
you close the stream.

00:25:56.990 --> 00:26:00.900
Audio File Stream API doesn't
handle networking or disk streaming.

00:26:00.900 --> 00:26:02.830
It's a file parsing API.

00:26:02.840 --> 00:26:07.160
It's up to you to provide--
provide the audio data.

00:26:07.160 --> 00:26:11.480
And it currently supports
all these formats.

00:26:13.500 --> 00:26:17.140
Okay,
now the difference between a stream and

00:26:17.140 --> 00:26:20.290
a file is a stream is not random access.

00:26:20.290 --> 00:26:23.720
It has a pass that is
no longer accessible.

00:26:23.720 --> 00:26:28.740
So we're assuming you're not
writing the entire pass to a file.

00:26:28.740 --> 00:26:32.400
It could be an indefinitely long stream.

00:26:32.400 --> 00:26:37.840
And the future is not yet accessible,
so you can't access into the

00:26:37.840 --> 00:26:43.480
future to find out any kind of
data that might not be accessible.

00:26:43.500 --> 00:26:50.080
So the basic problem in dealing
with streams that you don't have

00:26:50.080 --> 00:26:57.090
with files is that you're getting
your data from some source,

00:26:57.440 --> 00:27:04.840
like from a socket or some other source,
and it's chunked up into buffers.

00:27:04.840 --> 00:27:10.620
And those buffer boundaries may
bisect chunks in the file so that

00:27:10.620 --> 00:27:15.090
you may have read part of a header,
but you don't have even the rest

00:27:15.090 --> 00:27:18.800
of the header yet to find out
how long this next chunk is.

00:27:18.800 --> 00:27:23.000
So your parser kind of has
to suspend in mid-operation.

00:27:23.030 --> 00:27:27.140
And that's what the
Audio File Stream parser handles for you.

00:27:27.300 --> 00:27:32.050
It remembers the necessary
state from previous buffers,

00:27:32.050 --> 00:27:35.800
and it doesn't ask for a state
that's not yet accessible.

00:27:36.420 --> 00:27:42.730
And as data arrives in the buffers,
it will call your callbacks telling

00:27:42.730 --> 00:27:48.160
you that I found the data format
or I found the Magic Cookie.

00:27:48.160 --> 00:27:52.040
And then here's audio data
and you can begin decoding

00:27:52.160 --> 00:27:54.640
and playing the audio data.

00:27:54.640 --> 00:27:59.480
So as the buffers arrive,
it's calling your callbacks.

00:27:59.480 --> 00:28:05.200
So I'm going to show a small
client server audio stream example.

00:28:05.200 --> 00:28:08.780
The server is a very simple
program that just f opens a file,

00:28:08.960 --> 00:28:12.900
just treats it as a file of bytes and
doesn't even know it's an audio file and

00:28:12.900 --> 00:28:15.780
writes its content into a TCP stream.

00:28:15.780 --> 00:28:19.420
It's really simple and I'm
not going to go over it.

00:28:19.420 --> 00:28:26.930
So the interesting stuff happens in the
client where I connect to that socket,

00:28:26.930 --> 00:28:30.490
I allocate data that I need
to manage the stream,

00:28:30.490 --> 00:28:35.020
and I create an audio file stream parser,
then I enter a main loop.

00:28:35.020 --> 00:28:41.580
And that will read data from the socket
and then call audio file parse data.

00:28:41.580 --> 00:28:48.060
And an audio file parse data in turn
will call the callbacks saying we've

00:28:48.730 --> 00:28:51.640
gotten these properties or audio data.

00:28:51.640 --> 00:28:54.420
And when it's inside
one of these callbacks,

00:28:54.420 --> 00:28:58.950
we're going to create an audio queue
and then start in-queuing buffers on the

00:28:58.950 --> 00:29:01.140
audio queue to have them played back.

00:29:01.140 --> 00:29:03.310
And then after that,
once the data is exhausted,

00:29:03.310 --> 00:29:04.800
we'll call back the callbacks.

00:29:04.840 --> 00:29:08.100
and we'll exit the loop and clean up.

00:29:09.720 --> 00:29:13.900
Some other utility functions
that are necessary for this

00:29:13.970 --> 00:29:18.220
are the property callback,
which tells you it was called

00:29:18.220 --> 00:29:23.570
whenever properties are discovered,
and the audio packet callback,

00:29:23.570 --> 00:29:27.920
and then there's the Audio Queue
Output Buffer Callback,

00:29:27.920 --> 00:29:33.920
which is called when the audio queue
is finished decoding an output buffer.

00:29:34.120 --> 00:29:39.330
And then there's a My Enqueue Buffer,
which is a helper function that just

00:29:39.330 --> 00:29:42.360
enqueues buffers onto the audio queue.

00:29:42.360 --> 00:29:55.480
So now I'm going to go to the
demo machine and run the demo.

00:29:58.040 --> 00:30:04.140
We have two terminal windows,
one for the server and

00:30:04.140 --> 00:30:04.140
one for the client.

00:30:22.200 --> 00:30:27.310
Okay, so on the left terminal window,
I'm going to run the server.

00:30:28.130 --> 00:30:33.500
I'm going to play back an AAC file.

00:30:33.500 --> 00:30:35.850
So now it's waiting for a connection.

00:30:35.930 --> 00:30:42.940
So then on this terminal,
I'm going to run the client.

00:30:43.700 --> 00:30:48.810
So now we're streaming data
from the server to the client.

00:30:55.700 --> 00:31:01.540
You can see these printouts are happening
in the client from the callback,

00:31:01.540 --> 00:31:07.280
so I'll show that in a bit.

00:31:07.280 --> 00:31:07.280
All right, so I'll just kill that.

00:31:09.530 --> 00:31:12.760
You can see here's the property
callbacks being called.

00:31:12.760 --> 00:31:16.790
Okay, so back to slides.

00:31:21.690 --> 00:31:27.590
Okay, so this is the main program
in the client program,

00:31:27.590 --> 00:31:29.920
the main function.

00:31:29.920 --> 00:31:33.160
The first thing I do is I allocate
a structure called MyData,

00:31:33.160 --> 00:31:37.880
and it's just a structure I'm going to
use to hold all of the bits and pieces

00:31:38.350 --> 00:31:41.400
that I need to manage the stream.

00:31:41.400 --> 00:31:44.440
You can see a declaration
of it in the sample code.

00:31:46.230 --> 00:31:52.890
Then I will initialize a Pthread mutex
in Convr because the audio queue is

00:31:52.890 --> 00:31:57.500
running a separate thread from my client
that's reading data from the socket,

00:31:57.500 --> 00:32:04.380
so I need to synchronize on signaling
when buffers are no longer in use.

00:32:04.380 --> 00:32:07.940
And then I connect to the socket.

00:32:07.940 --> 00:32:12.410
That's just a standard
connecting Unix sockets.

00:32:12.450 --> 00:32:15.880
And then I'm going to
allocate a buffer for reading.

00:32:15.880 --> 00:32:17.480
So I'm going to use the buffer to
read the stream from the socket.

00:32:17.480 --> 00:32:22.690
Okay, and then the next thing I'm going
to do is create an audio file stream

00:32:22.820 --> 00:32:25.480
parser with audio file stream open.

00:32:25.480 --> 00:32:32.110
I pass it MyData,
which is just a pointer to my local data,

00:32:32.300 --> 00:32:40.580
and I pass it to my property listener
and audio packets proc callbacks.

00:32:40.580 --> 00:32:24.950
And there's a file type hint,
which I'm going to tell it

00:32:24.950 --> 00:32:45.500
I'm going to be streaming A,
C, D, E, and E.

00:32:45.940 --> 00:32:53.680
And then I'm going to add a pointer
to my audio file stream parser,

00:32:53.680 --> 00:32:59.710
and it's going to put that into a
field in MyData struct so I'll have it.

00:33:00.890 --> 00:33:03.730
Okay, so this is the declaration
of audio file stream open.

00:33:03.780 --> 00:33:07.760
It's got the void star
for the client data,

00:33:07.960 --> 00:33:11.080
which can be in any of your
data that you want to have kept

00:33:11.100 --> 00:33:13.530
around by the audio file stream.

00:33:13.800 --> 00:33:18.020
And then the two callbacks
and a file type hint,

00:33:18.020 --> 00:33:24.180
which tells the Audio File Stream parser
what format it can expect,

00:33:24.190 --> 00:33:28.110
because there's some binary formats
that aren't self-identifying,

00:33:28.110 --> 00:33:32.210
so it may not have an easy way to figure
out from the bits that you're giving

00:33:32.210 --> 00:33:34.740
it what it's supposed to be parsing.

00:33:34.740 --> 00:33:38.970
And then it will pass you
back the Audio File Stream.

00:33:39.410 --> 00:33:44.020
Okay, so then the main loop,
this is where everything happens,

00:33:44.020 --> 00:33:44.870
basically.

00:33:45.020 --> 00:33:49.730
I just receive data from
the socket into a buffer,

00:33:49.900 --> 00:33:52.730
and then I call
Audio File Stream Parse Bytes,

00:33:52.820 --> 00:33:58.710
giving it my audio file stream,
how many bytes are in the buffer,

00:33:58.710 --> 00:34:02.270
and the pointer to the buffer,
and then a flags field.

00:34:03.000 --> 00:34:08.140
This is the declaration for
audio file stream parsed bytes.

00:34:08.140 --> 00:34:10.560
The flags field,
in the case that you had a

00:34:10.560 --> 00:34:14.220
discontinuity in your stream,
you could signal that here,

00:34:14.220 --> 00:34:18.370
and it would ignore anything
that happened to be in the

00:34:18.380 --> 00:34:21.020
middle of parsing at the time,
at the buffer boundary.

00:34:21.020 --> 00:34:27.280
So it would just say, "Okay,
I don't know where I am again,

00:34:27.280 --> 00:34:27.280
so I'll start parsing."

00:34:28.710 --> 00:34:29.610
From Xero.

00:34:29.700 --> 00:34:35.020
Okay, so and then the property listener
callback is the first one I'm going

00:34:35.020 --> 00:34:39.150
to show the implementation of,
which is the declaration

00:34:39.150 --> 00:34:41.950
is it passes you,
this is the audio file

00:34:41.980 --> 00:34:44.230
stream files are calling you.

00:34:44.340 --> 00:34:49.470
So it passes you a client data pointer,
the audio file stream instance,

00:34:49.480 --> 00:34:51.930
the property ID that it's found.

00:34:52.000 --> 00:34:56.320
So it just tells you,
I found this property in your stream.

00:34:56.320 --> 00:34:58.670
And then it flags fields.

00:34:58.680 --> 00:35:02.760
And the flag can be that this
tells you that this property is

00:35:02.760 --> 00:35:05.260
either cached or it's not cached.

00:35:05.260 --> 00:35:10.000
And that means if it's not cached,
you need to ask for it now or signal

00:35:10.030 --> 00:35:14.970
that you want it to start caching the
property now because you won't have

00:35:15.310 --> 00:35:20.010
access to it because it will have
fallen into the past in a past buffer.

00:35:20.010 --> 00:35:25.970
And the audio file stream parser doesn't
buffer indefinitely into the past.

00:35:26.350 --> 00:35:33.920
So, in this property listener callback,
I'm only going to be interested

00:35:34.260 --> 00:35:38.190
actually in one property,
which is the ready-to-produce

00:35:38.190 --> 00:35:39.690
packets property.

00:35:39.700 --> 00:35:44.130
And in that property,
I'm going to create an audio queue,

00:35:44.130 --> 00:35:50.640
allocate the audio queue output buffers,
and get a magic cookie from the

00:35:50.640 --> 00:35:50.640
stream and set it on the audio queue.

00:35:51.050 --> 00:35:55.900
So this is my property listener, Proc.

00:35:55.900 --> 00:36:00.540
So when I'm called,
the first thing I'm going to do

00:36:00.540 --> 00:36:04.820
is cast the client data to my,
a pointer to my data struct,

00:36:04.820 --> 00:36:05.960
so I have access to all my data.

00:36:05.960 --> 00:36:09.230
And then I'm just going to print
out what property I'm finding.

00:36:10.240 --> 00:36:14.080
At the bottom on the left,
you can see the properties it finds,

00:36:14.080 --> 00:36:17.720
and they're all four-character codes,
which indicate that it's

00:36:17.760 --> 00:36:20.520
found the file format,
the magic cookie, the data format,

00:36:20.600 --> 00:36:21.180
the data offset.

00:36:21.180 --> 00:36:25.300
And then the last one is
ready-to-produce output packets.

00:36:25.300 --> 00:36:32.550
So once we have gotten ready-to-produce
output packets property called on us,

00:36:32.550 --> 00:36:36.130
that means we're at the point
where we've received all the

00:36:36.320 --> 00:36:40.180
necessary file and data formats.

00:36:40.240 --> 00:36:43.540
So we're going to get the data
format information that is required

00:36:43.540 --> 00:36:45.000
to begin decoding the data.

00:36:45.000 --> 00:36:51.040
So at that point,
we can start reading audio packets

00:36:51.040 --> 00:36:57.900
or getting them passed to us
and being able to decode them.

00:36:59.100 --> 00:37:59.800
( Transcript missing )

00:38:01.110 --> 00:38:06.100
Okay, so now this is the declaration for
audio file stream get property,

00:38:06.100 --> 00:38:08.200
which I use to get the ASBD.

00:38:08.200 --> 00:38:14.980
It's just I pass the audio file
stream the property ID I want,

00:38:14.980 --> 00:38:20.910
and I pass it the byte size
for the buffer that I'm passing

00:38:20.980 --> 00:38:25.290
it to fill the property,
and then it will pass me back

00:38:25.290 --> 00:38:25.290
the property data in that buffer.

00:38:25.600 --> 00:38:28.890
So then I allocate the
audio queue buffers.

00:38:28.910 --> 00:38:34.770
I just have a loop where I allocate
a few audio queue buffers.

00:38:34.940 --> 00:38:38.400
I pass it the audio queue,
the buffer size I'm going to use,

00:38:38.650 --> 00:38:43.490
and then it will pass me back a
reference to the audio queue buffer,

00:38:43.490 --> 00:38:48.090
and I'm going to keep that in
an array in my data struct.

00:38:50.770 --> 00:38:54.540
Okay, so then I need to get the magic
cookie from the audio file stream.

00:38:54.540 --> 00:38:58.100
So what I do is I call audio
file stream get property info to

00:38:58.100 --> 00:38:59.740
find out how big the cookie is.

00:38:59.740 --> 00:39:03.140
Then I'm going to C-Alec a
buffer to hold the cookie,

00:39:03.140 --> 00:39:05.850
and then I'll get the cookie
from the audio file stream using

00:39:05.850 --> 00:39:07.540
audio file stream get property.

00:39:07.540 --> 00:39:11.650
And then I'm going to call
audio queue set property to set

00:39:11.650 --> 00:39:14.160
the cookie on the audio queue.

00:39:14.160 --> 00:39:17.400
So this way the audio queue now
has a cookie so that its audio

00:39:17.750 --> 00:39:20.040
converter can decode the stream.

00:39:22.050 --> 00:39:25.560
Okay,
so audio file stream get property info,

00:39:25.560 --> 00:39:29.900
which I just used to get
the size of the cookie,

00:39:29.960 --> 00:39:34.940
pass it the file stream instance,
the property ID,

00:39:34.970 --> 00:39:39.550
and it will pass you back a size for
that property and a writable flag.

00:39:39.810 --> 00:39:44.590
And currently there are no writable
properties for audio file stream.

00:39:46.200 --> 00:39:52.500
So then the next thing I'll
show is the audio data callback.

00:39:52.500 --> 00:39:56.320
So once we're ready
to use output packets,

00:39:56.320 --> 00:39:59.260
we've created a queue,
we know the format and the magic cookie,

00:39:59.510 --> 00:40:04.180
then we're going to start
getting packet callbacks from

00:40:04.180 --> 00:40:07.210
audio file stream parse bytes.

00:40:07.210 --> 00:40:07.210
And I

00:40:07.260 --> 00:40:11.200
So it's going to give me the
pointer to my client data,

00:40:11.360 --> 00:40:15.690
which is my data,
and it's going to tell me how many

00:40:15.690 --> 00:40:23.140
bytes are in this bunch of packets,
how many packets there are,

00:40:23.140 --> 00:40:27.200
this void star pointer
to the actual input data,

00:40:27.450 --> 00:40:30.660
which is my audio data,
and then an array of

00:40:30.660 --> 00:40:35.790
AudioStream packet descriptions,
which tell me where the packet

00:40:35.790 --> 00:40:37.800
boundaries are in the input data.

00:40:40.360 --> 00:40:47.770
So this is the actual implementation
of myPackets proc in this demo.

00:40:47.900 --> 00:40:52.850
So the first thing I'm going to do is,
well, first I get my pointer to my data,

00:40:52.980 --> 00:40:55.540
and then I just print
out how much data I got,

00:40:55.540 --> 00:40:58.090
how many bytes, and how many packets.

00:40:59.370 --> 00:41:04.750
And then I go into a loop
for each packet that I got.

00:41:04.850 --> 00:41:10.410
I'm going to get the packet offset and
the size from the packet descriptions.

00:41:10.410 --> 00:41:14.770
And if I've -- now, I'm currently filling
an audio queue buffer.

00:41:14.770 --> 00:41:19.280
I may have been from a previous callback,
so I have to find out whether there's

00:41:19.280 --> 00:41:23.480
enough buffer space remaining in this
audio queue buffer to fill the -- or

00:41:23.480 --> 00:41:25.580
to completely put this packet in there.

00:41:25.580 --> 00:41:28.260
So if there's not enough
buffer space remaining,

00:41:28.260 --> 00:41:31.530
I'm going to go ahead and enqueue
the buffer on the audio queue so

00:41:31.530 --> 00:41:35.580
that it'll go ahead and play it back,
and then it'll step to the next buffer,

00:41:35.630 --> 00:41:40.020
and I can begin -- continue copying data.

00:41:40.020 --> 00:41:43.670
So then the next thing I do is
just mem copy that packet of

00:41:43.870 --> 00:41:45.640
data into the audio queue buffer.

00:41:48.430 --> 00:41:54.310
Okay, then I need to fill out a packet
description for the audio queue to

00:41:54.310 --> 00:41:56.400
tell it where this packet boundary is.

00:41:56.560 --> 00:42:03.850
And so first I just copy the packet
description from the stream that the --

00:42:04.200 --> 00:42:10.530
The packets callback has given me,
and then I'm going to...

00:42:10.650 --> 00:42:18.480
Copy that into the Audio Queues output
buffer array of packet descriptions.

00:42:18.480 --> 00:42:24.410
And I need to change the offset because
it had one offset in the input stream,

00:42:24.410 --> 00:42:28.290
and I'm going to change it to the
offset where I am currently in

00:42:28.290 --> 00:42:30.560
the Audio Queues output buffer.

00:42:30.560 --> 00:42:36.290
And then I just keep track of
the bytes filled and the packets

00:42:37.490 --> 00:42:41.380
filled on the current queue,
Audio Queue buffer.

00:42:41.380 --> 00:42:44.760
And then if that was the
last free packet description,

00:42:44.770 --> 00:42:47.320
so I have a limited size array
of packet descriptions that

00:42:47.320 --> 00:42:51.070
I have allocated in my data,
so if I've reached the end of that,

00:42:51.370 --> 00:42:55.490
then I need to enqueue the buffer
because I don't have another packet

00:42:55.870 --> 00:42:58.490
description left in that array.

00:42:58.550 --> 00:43:04.060
And that will enqueue the buffer
and give me a new -- I can

00:43:04.060 --> 00:43:07.110
begin filling a new buffer.

00:43:07.390 --> 00:43:13.690
Okay, so the helper function
that does the enqueuing

00:43:13.840 --> 00:43:19.620
First, I need to set an in-use flag
so that I don't try to use a

00:43:19.620 --> 00:43:21.100
buffer that's already in use.

00:43:21.100 --> 00:43:24.100
So I set the in-use flag to true.

00:43:24.220 --> 00:43:29.100
And then I'm going to call
audio queue in-queue buffer to

00:43:29.150 --> 00:43:31.300
in-queue the buffer on the stream.

00:43:31.470 --> 00:43:38.010
And I pass up the audio queue,
the audio queue buffer.

00:43:38.280 --> 00:43:43.310
The number of packets filled and
an array of packet descriptions.

00:43:45.620 --> 00:43:49.740
And then, if I haven't already
started my audio queue,

00:43:49.740 --> 00:43:53.780
so now I've potentially
enqueued the first buffer,

00:43:53.780 --> 00:43:58.080
and I'm going to call audio
queue start to go ahead and start

00:43:58.080 --> 00:44:00.410
the audio queue playing back.

00:44:02.970 --> 00:44:08.270
Alright, so then I need to go
to the next buffer and,

00:44:08.270 --> 00:44:08.270
uh...

00:44:08.540 --> 00:44:12.740
So I'm going to reset the bytes
filled and the packets filled to zero,

00:44:12.770 --> 00:44:17.260
and then I'm going to take a
Pthread mutex so I can wait until

00:44:17.280 --> 00:44:20.850
there's a buffer that's become free.

00:44:20.890 --> 00:44:26.090
So I enter a loop waiting for the next
buffer's in-use flag to be set to false.

00:44:27.990 --> 00:44:31.950
Okay, then the other callback I need
to implement is the AudioCues

00:44:32.270 --> 00:44:35.880
output buffer callback.

00:44:35.880 --> 00:44:41.100
This gets called whenever the
AudioCues finish decoding a stream,

00:44:41.100 --> 00:44:42.740
a buffer of audio.

00:44:42.930 --> 00:44:48.400
The main thing I need to do here is
take this mutex again and signal that

00:44:48.400 --> 00:44:52.200
the buffer is now no longer in use.

00:44:53.070 --> 00:44:58.500
And then basically that's everything.

00:44:58.500 --> 00:45:01.760
Once the data has been
exhausted from the socket,

00:45:01.760 --> 00:45:06.250
we're going to exit the loop and
call audio file stream close,

00:45:06.260 --> 00:45:10.270
audio key dispose, and free all our data.

00:45:11.900 --> 00:45:14.240
This is the declaration for
audio file stream closed.

00:45:14.240 --> 00:45:15.440
You just pass it to file stream.

00:45:15.440 --> 00:45:19.740
There's one other issue.

00:45:19.740 --> 00:45:23.860
You can actually do some random access.

00:45:23.860 --> 00:45:28.660
This is to handle the situation
where a user has a play bar and

00:45:28.660 --> 00:45:32.940
they've changed the playback position.

00:45:32.940 --> 00:45:36.470
You can call audio file stream seek
to find out what is the byte position

00:45:36.470 --> 00:45:40.940
I need to play back the packet where
the user moved the play bar to.

00:45:40.940 --> 00:45:45.600
So audio file stream seek will
return that byte position,

00:45:45.720 --> 00:45:49.490
and then you're actually
responsible for seeking in the data.

00:45:49.770 --> 00:45:52.260
And if you seek to a
different place in the data,

00:45:52.260 --> 00:45:55.350
you need to set the audio file
stream discontinuity flag to

00:45:55.400 --> 00:45:57.890
signal that that's happened.

00:46:00.850 --> 00:46:04.150
This is the declaration
for audio file stream seek.

00:46:04.390 --> 00:46:09.480
You pass the audio file stream the
packet offset that you want to go to,

00:46:09.500 --> 00:46:13.300
and it will pass you back the
byte offset in your stream

00:46:13.300 --> 00:46:14.800
where that packet is located.

00:46:14.960 --> 00:46:20.190
Now, in some formats,
that may be in -- some file types,

00:46:20.190 --> 00:46:23.040
that may be an estimation.

00:46:23.040 --> 00:46:26.890
And in that case,
it will set the I/O flag

00:46:27.000 --> 00:46:30.800
parameter to offset as estimated.

00:46:32.150 --> 00:46:35.680
Okay, so that's audio file streams,
and now I'm going to bring

00:46:35.680 --> 00:46:38.540
up Bob Aron to talk about
Extended Audio File and OpenAL.

00:46:43.220 --> 00:46:44.200
Hi, I'm Bob Aron.

00:46:44.200 --> 00:46:46.710
I'm a member of the
Core Audio team here at Apple,

00:46:46.710 --> 00:46:49.500
and I'm going to talk a little bit,
as James mentioned,

00:46:49.500 --> 00:46:56.510
about OpenAL and using the Extended
Audio file in conjunction with OpenAL.

00:46:56.790 --> 00:46:59.700
So first, just a little bit of review if
you're not familiar with OpenGL.

00:46:59.700 --> 00:47:05.060
It's an open-source cross-platform
API for doing spatialized 3D audio.

00:47:05.060 --> 00:47:09.500
It has some real OpenGL-like conventions,
uses the same coordinate system.

00:47:09.500 --> 00:47:12.390
It actually complements
OpenGL very well if you're doing

00:47:12.400 --> 00:47:14.080
your graphics in GL already.

00:47:14.080 --> 00:47:17.380
As Jeff mentioned earlier,
it's primarily used for game development,

00:47:17.380 --> 00:47:20.070
although there are developers
using it for other things as well.

00:47:20.240 --> 00:47:23.930
And I'm not going to go too much
into detail of the overall API set.

00:47:23.940 --> 00:47:26.660
You can get more info on the website
there that's up on the slide.

00:47:26.660 --> 00:47:31.830
As far as Apple's commitment to OpenGL,
we delivered an implementation

00:47:31.830 --> 00:47:35.920
on the system in OpenGL framework
when we shipped Tiger.

00:47:36.030 --> 00:47:40.320
That was based on the 1.0
specification of OpenGL.

00:47:40.320 --> 00:47:45.580
And then when the 1.1 spec was released,
we released a new implementation with

00:47:45.580 --> 00:47:48.620
our 10.4.7 system software update.

00:47:48.620 --> 00:47:52.880
And then at the same time,
we also added some new OpenGL extensions

00:47:52.880 --> 00:47:56.670
for doing reverb occlusion
and a bunch of other things.

00:47:56.700 --> 00:47:58.730
So we're really happy
with the overall API set.

00:47:58.730 --> 00:48:02.100
And then we're going to go into a little
bit more detail of OpenGL construction.

00:48:02.100 --> 00:48:03.480
It's purely a core audio stack.

00:48:03.480 --> 00:48:06.310
It uses a lot of the same pieces that
we've been talking about for this

00:48:06.320 --> 00:48:07.860
session and the previous session.

00:48:07.860 --> 00:48:10.650
And it really takes advantage of
the 3D mixer audio unit for doing

00:48:10.650 --> 00:48:12.270
all that hard spatial rendering.

00:48:12.280 --> 00:48:15.250
I also wanted to touch briefly on ALUT.

00:48:15.250 --> 00:48:20.880
There's been some confusion on the
lists about the OpenGL ALUT library.

00:48:20.880 --> 00:48:26.600
ALUT is basically a companion library
for doing some utility things.

00:48:26.700 --> 00:48:30.700
And when the 1.0 spec was done
originally with OpenAL 1.0,

00:48:30.700 --> 00:48:35.230
it was primarily used for getting
audio data out of WAV files or

00:48:35.230 --> 00:48:40.260
from data into a form that you
could pass into the OpenAL buffers.

00:48:40.260 --> 00:48:44.380
And we implemented those APIs when
we did the initial release.

00:48:44.380 --> 00:48:49.710
But when OpenAL 1.1 was finished,
actually the 1.0

00:48:49.710 --> 00:48:52.160
ALUT spec was deprecated.

00:48:52.160 --> 00:48:55.590
So we've removed the ALUT.h
header from the framework.

00:48:55.600 --> 00:48:56.680
But we're still using it.

00:48:56.760 --> 00:48:59.090
We've still implemented those APIs.

00:48:59.180 --> 00:49:02.160
And so we won't break any
of your binaries at runtime.

00:49:02.220 --> 00:49:04.530
And you actually can still
use them in your code.

00:49:04.540 --> 00:49:08.090
But you just have to define
the prototypes for them

00:49:08.100 --> 00:49:10.250
yourself when you're building.

00:49:10.310 --> 00:49:14.200
As far as ALUT 1.1,
there is a new specification for ALUT.

00:49:14.240 --> 00:49:16.420
It's an expanded set of APIs.

00:49:16.420 --> 00:49:21.620
We did not implement that in the current
OpenAL framework that's on Tiger.

00:49:21.790 --> 00:49:25.120
And it won't be in the
Leopard version either.

00:49:25.120 --> 00:49:27.480
But if those are APIs that you need,
you can go, again,

00:49:27.580 --> 00:49:31.830
get more information about
those on the OpenAL website.

00:49:32.560 --> 00:49:34.740
So what's new?

00:49:34.760 --> 00:49:36.760
With Leopard,
we have a couple of new effect

00:49:36.810 --> 00:49:39.460
audio units on the system
for doing a Roger Beep effect

00:49:39.580 --> 00:49:40.840
and a distortion effect.

00:49:40.910 --> 00:49:45.540
And so we decided to add
OpenAL extensions to take advantage of

00:49:45.540 --> 00:49:50.060
these so that you could use them through
your normal OpenAL calling mechanisms.

00:49:50.060 --> 00:49:52.580
So we've got a couple there,
Roger Beep and Distortion,

00:49:52.580 --> 00:49:54.280
and I'll talk about these as we go.

00:49:55.140 --> 00:49:57.760
The Roger Beep is basically
an effect so you can do

00:49:57.760 --> 00:49:59.820
walkie-talkie type simulations.

00:49:59.820 --> 00:50:06.180
Basically what it does is when you're
playing your audio in your OpenAL buffer,

00:50:06.180 --> 00:50:10.250
whenever the audio signal dips
below a certain dB threshold

00:50:10.250 --> 00:50:14.490
for some amount of time,
it replaces the audio with a tone.

00:50:14.490 --> 00:50:17.300
Now it could be a static
tone like a walkie-talkie,

00:50:17.300 --> 00:50:19.400
like a sound or a beep or whatever.

00:50:19.400 --> 00:50:22.350
It just depends on which
Roger Beep setting you're using.

00:50:22.360 --> 00:50:25.040
It'll be available.

00:50:25.140 --> 00:50:28.320
It'll be available at runtime
whenever the Roger Beep audio

00:50:28.330 --> 00:50:29.760
unit is present on your system.

00:50:29.760 --> 00:50:32.390
And we've got some predefined settings.

00:50:32.390 --> 00:50:36.560
Also, you can create your own Roger Beep
audio unit preset files and load

00:50:36.670 --> 00:50:39.160
those at runtime in your application.

00:50:39.160 --> 00:50:41.950
And there are several properties
that go along with it.

00:50:42.030 --> 00:50:46.240
And we set the properties with
the ALC ASA set source API.

00:50:46.240 --> 00:50:49.120
That's something we introduced
with one of the extensions that

00:50:49.150 --> 00:50:53.080
shipped with the 10.4.7 release
of OpenAL with the 1.1 spec.

00:50:53.080 --> 00:50:55.110
And again, you can find all the
details in the openAL.

00:50:55.120 --> 00:50:59.190
And you can find all those properties
and things in the MacOSX OIO extension

00:50:59.190 --> 00:51:01.230
header that's in the framework.

00:51:01.740 --> 00:51:04.570
So just to give you a
visual representation of

00:51:04.570 --> 00:51:07.480
what Roger Beep does is,
let's say this is the audio data

00:51:07.490 --> 00:51:09.660
that's in one of your OpenAL buffers.

00:51:09.740 --> 00:51:12.310
You'll see that there's a couple
of points there in the data

00:51:12.340 --> 00:51:14.780
where the signal's not as loud,
so that you can see those

00:51:14.780 --> 00:51:16.140
designated in the blue.

00:51:16.140 --> 00:51:19.190
Well,
if that area of the signal was dipping

00:51:19.260 --> 00:51:24.250
below the dB threshold that your
sensitivity is set at for the effect,

00:51:24.250 --> 00:51:28.320
then what happens is this
other waveform will be played,

00:51:28.320 --> 00:51:31.490
and that gets done by
the audio unit itself.

00:51:34.440 --> 00:51:38.020
So here's our source properties.

00:51:38.420 --> 00:51:42.400
Basically, all these properties are set
on an OpenAL source basis.

00:51:42.730 --> 00:51:44.690
And what I mean by that,
if you're not familiar

00:51:44.690 --> 00:51:46.690
with OpenAL at all,
is an OpenAL source is your

00:51:46.690 --> 00:51:49.580
object that you're moving
around your spatial environment.

00:51:49.580 --> 00:51:54.640
So each thing that's making sound,
so to speak, is your OpenAL source.

00:51:55.640 --> 00:51:57.240
So we have some properties here.

00:51:57.240 --> 00:51:59.810
The first one is the
RogerBeepEnable property.

00:51:59.930 --> 00:52:02.690
You're going to want to call this
first when you're using this effect

00:52:02.700 --> 00:52:05.600
so that you can tell the library,
do some initialization,

00:52:05.610 --> 00:52:08.490
get the audio unit ready so
when I want to use the effect,

00:52:08.490 --> 00:52:09.800
it'll be ready to go.

00:52:09.800 --> 00:52:11.830
The next one is the RogerBeepOn.

00:52:11.830 --> 00:52:14.210
This is basically an on and off toggle.

00:52:14.210 --> 00:52:18.550
And you'll want to explicitly turn
that on because it's off by default.

00:52:18.550 --> 00:52:22.400
Now, you can do that while the source
is rendering or not rendering,

00:52:22.400 --> 00:52:24.880
but you'll want to explicitly turn it on.

00:52:25.640 --> 00:52:28.610
The next property we have is
the RogerBeepGain property.

00:52:28.680 --> 00:52:31.780
This allows you to set
how loud the waveform that

00:52:31.780 --> 00:52:36.160
replaces your original data,
how loud you want it to be played.

00:52:36.170 --> 00:52:40.410
So a setting of zero means you're
not doing any attenuation at all,

00:52:40.410 --> 00:52:44.760
and then you can attenuate it
down to a setting of minus 80 dB.

00:52:45.060 --> 00:52:47.090
Again, I mentioned there are several
different RogerBeep types.

00:52:47.210 --> 00:52:49.980
We have some preset
ones that you can use.

00:52:50.000 --> 00:52:52.530
Those are in the header file.

00:52:52.830 --> 00:52:54.300
RogerBeep sensitivity.

00:52:54.360 --> 00:52:58.000
This is so you can fine-tune
how the effect kicks in.

00:52:58.000 --> 00:53:01.470
You may be using -- you may be
capturing data off a microphone,

00:53:01.470 --> 00:53:05.700
so to -- and you have a particular
level of audio coming in,

00:53:05.700 --> 00:53:06.990
or you may have files.

00:53:07.090 --> 00:53:09.480
So there's some different
sensitivity settings,

00:53:09.480 --> 00:53:13.000
and you can fine-tune it for how
you're using it in your application.

00:53:13.120 --> 00:53:16.000
And then lastly,
we have the RogerBeep preset.

00:53:16.000 --> 00:53:17.680
As I mentioned,
you can create your own audio unit

00:53:17.680 --> 00:53:20.900
presets and load those at runtime.

00:53:21.790 --> 00:53:24.940
So if you've had a chance to look at the
code that's associated with this session,

00:53:24.940 --> 00:53:29.580
there's an OpenAL Tools project,
and there's a RogerBeep test source file.

00:53:29.580 --> 00:53:32.650
And so here's some of the source
for setting up the RogerBeep.

00:53:32.810 --> 00:53:35.360
I've kind of trimmed it out a little bit
so that we could get it all on the slide

00:53:35.360 --> 00:53:37.000
without all the error checking and stuff.

00:53:37.060 --> 00:53:40.860
So the first thing you'll
want to do is call the ALC,

00:53:40.940 --> 00:53:42.920
Is Extension Present?

00:53:42.920 --> 00:53:46.440
Now, this is the normal mechanism in
OpenAL for determining whether an

00:53:46.440 --> 00:53:50.380
extension that you want to use is
present when your application is running.

00:53:51.030 --> 00:53:55.420
So we're going to pass in
that string for our extension.

00:53:55.590 --> 00:53:57.340
And then if that's true,
then we can go and do some

00:53:57.380 --> 00:53:58.740
setup on our source object.

00:53:58.930 --> 00:54:00.900
So we're going to enable it.

00:54:00.980 --> 00:54:03.740
We'll pass it a value of true.

00:54:03.740 --> 00:54:05.400
We're going to explicitly turn it on.

00:54:05.480 --> 00:54:07.310
As I mentioned,
you could do this while the

00:54:07.340 --> 00:54:10.410
source is rendering or not.

00:54:10.720 --> 00:54:13.530
We're going to set our gain to zero,
which means we don't want any

00:54:13.530 --> 00:54:17.830
attenuation at all on the Roger Beep
tone that's going to be played.

00:54:18.200 --> 00:54:20.320
We'll set our sensitivity
to light in this case.

00:54:20.320 --> 00:54:22.750
When I demo it,
this seems to work well with

00:54:22.820 --> 00:54:25.480
the program because I'm in the
way that I'm capturing my data.

00:54:25.480 --> 00:54:28.760
And lastly, we're going to use the
walkie-talkie setting.

00:54:28.760 --> 00:54:31.940
This is one of the preset
sets that you'll want to use.

00:54:31.940 --> 00:54:35.810
So just to summarize the demo code,
if you've had a chance to look at it or

00:54:35.810 --> 00:54:40.190
you can look at it after the session,
basically what we're going to do is

00:54:40.240 --> 00:54:45.270
use the OpenAL Capture APIs to capture
some data off of an input device

00:54:45.300 --> 00:54:46.270
on the machine that we're running.

00:54:46.760 --> 00:54:50.300
We're going to capture that data
and put it in some OpenAL buffers,

00:54:50.300 --> 00:54:54.710
and then we'll attach those buffers
to our source object and play it back.

00:54:54.720 --> 00:54:56.660
And as you just saw,
we just went through all

00:54:56.660 --> 00:54:57.950
the RogerBeep settings.

00:54:57.960 --> 00:55:00.360
So if we could go to the demo machine.

00:55:03.130 --> 00:55:08.690
I'll set this guy up here,
because we don't need this.

00:55:08.800 --> 00:55:17.830
So I have an iPod here that has a file,
and I'm going to use

00:55:17.830 --> 00:55:22.420
that into the line input.

00:55:22.420 --> 00:55:22.420
Let's see, we don't need this.

00:55:22.420 --> 00:55:22.420
OK.

00:55:24.920 --> 00:55:30.840
Okay, so we want to do a Roger Peep test.

00:55:30.990 --> 00:55:35.900
This tool takes a parameter for the
duration for how long we want to run.

00:55:35.900 --> 00:55:37.830
Okay, and I've got my iPod here.

00:55:37.830 --> 00:55:41.520
And so I'll start it running,
and then I'll start sending it

00:55:41.520 --> 00:55:43.460
some data on the line input.

00:55:43.460 --> 00:55:47.610
Welcome to session 404.

00:55:48.330 --> 00:55:53.400
You are listening to a test of
the OpenAL Roger Beep effect.

00:55:53.520 --> 00:55:58.390
To use Roger Beep,
simply verify at runtime that

00:55:58.390 --> 00:56:00.670
the extension is present.

00:56:01.320 --> 00:56:07.210
Use the ASA SetSource API to set
the enable and on properties.

00:56:07.960 --> 00:56:11.910
This test is currently capturing
data in real-time and filling

00:56:11.910 --> 00:56:14.490
OpenAL buffers for playback.

00:56:15.320 --> 00:56:19.980
If the test is running properly,
you should hear the Roger beep tone

00:56:19.980 --> 00:56:22.970
at each pause in this dialogue.

00:56:24.470 --> 00:56:26.160
All right, if we can go back to slides.

00:56:26.210 --> 00:56:29.980
So what you heard right there
was some data coming off my iPod,

00:56:30.000 --> 00:56:32.630
and all of those beeps that
were at the end of each pause,

00:56:32.740 --> 00:56:35.800
those were actually being generated
by the RogerBeep audio unit.

00:56:35.840 --> 00:56:44.000
So you can see how that might be useful
in your game if you've got characters

00:56:44.230 --> 00:56:45.190
talking on a walkie-talkie or whatever.

00:56:45.190 --> 00:56:45.190
That's primarily what this effect is for.

00:56:46.800 --> 00:56:47.460
Let's go to the next one.

00:56:47.570 --> 00:56:49.620
Okay, so let's talk about the distortion.

00:56:49.620 --> 00:56:53.940
Again, this is an extension that's
used in a very similar fashion

00:56:53.980 --> 00:56:55.320
to the RogerBeep extension.

00:56:55.320 --> 00:56:58.910
It'll be available on the system
at runtime if the distortion audio

00:56:58.910 --> 00:57:00.780
unit is present on your system.

00:57:00.780 --> 00:57:02.810
So that'll start with Leopard 10.5.

00:57:02.810 --> 00:57:05.070
We have some predefined settings.

00:57:05.070 --> 00:57:09.140
Again, you can create your own audio
unit presets for distortion,

00:57:09.210 --> 00:57:12.380
load those at runtime,
and we set the properties with

00:57:12.580 --> 00:57:16.740
the same mechanism that I was just
showing you with the RogerBeep.

00:57:18.830 --> 00:57:21.000
So here's our distortion property.

00:57:21.000 --> 00:57:23.060
So see again some similarities.

00:57:23.230 --> 00:57:26.650
First the distortion enable
property so that you can tell the

00:57:26.650 --> 00:57:29.910
library to set up the audio unit,
do some initialization,

00:57:30.230 --> 00:57:31.920
get it ready for use.

00:57:31.920 --> 00:57:36.960
The distortion on property for
toggling the effect on and off.

00:57:37.100 --> 00:57:38.690
Again, it's off by default.

00:57:38.720 --> 00:57:40.790
You want to explicitly turn it on.

00:57:40.900 --> 00:57:44.560
The distortion mix is so that you can
designate how much of the distortion

00:57:44.570 --> 00:57:47.850
effect you want applied to your
audio when it's being played back.

00:57:47.940 --> 00:57:52.290
So a setting of zero means you're not
applying any distortion and 100 means

00:57:52.310 --> 00:57:55.100
you're getting full maximum distortion.

00:57:55.100 --> 00:57:57.280
We have some preset distortion types.

00:57:57.280 --> 00:58:00.270
Now, there will be some more
types added when we finish up

00:58:00.270 --> 00:58:01.810
Leopard and we ship it to you.

00:58:01.870 --> 00:58:05.280
So if you look in the header file now,
you'll see some and there will

00:58:05.320 --> 00:58:06.980
be some more that are added.

00:58:07.010 --> 00:58:11.220
And we have the distortion preset if
you want to create your own audio unit

00:58:11.260 --> 00:58:12.900
presets and load those at runtime.

00:58:15.280 --> 00:58:18.080
So I want to talk a little bit
about how we can use Extended

00:58:18.080 --> 00:58:20.260
Audio File with OpenAL.

00:58:20.260 --> 00:58:23.050
Extended Audio File,
as Jeff mentioned a little earlier,

00:58:23.050 --> 00:58:26.610
is sort of a combination of the
Audio File and Audio Converter

00:58:26.610 --> 00:58:31.390
APIs in that it allows you to read
and write audio files of any of the

00:58:31.390 --> 00:58:34.900
audio file types that we support
with our normal audio file APIs,

00:58:34.960 --> 00:58:38.170
whether that's WAV or AIFF or MPEG-4,
MP3,

00:58:38.170 --> 00:58:44.730
any of the files that we support with
any of the data formats that we support.

00:58:44.790 --> 00:58:48.280
It combines -- and so it --
basically what it does is it

00:58:48.280 --> 00:58:51.400
allows you to open any file,
but then designate the format

00:58:51.400 --> 00:58:54.920
that you want to receive when
you're asking for packets of data.

00:58:54.920 --> 00:58:57.040
So there's the header file there,
and you'll find that in the

00:58:57.040 --> 00:58:58.910
Audio Toolbox framework.

00:58:58.980 --> 00:59:02.590
And let me just run
through some code here.

00:59:02.670 --> 00:59:06.120
So in the distortion test,
also in the sample code,

00:59:06.120 --> 00:59:09.940
I have a function here
called MyGetOpenALAudioData.

00:59:09.940 --> 00:59:13.530
And the reason we call it that
is OpenAL has a limited number

00:59:13.530 --> 00:59:14.280
of formats that it wants.

00:59:14.320 --> 00:59:17.260
And so we want a function here
that's going to return us back a

00:59:17.310 --> 00:59:19.220
format that OpenAL understands.

00:59:19.400 --> 00:59:23.800
In this case, we're going to ask for
16-bit integer data.

00:59:23.800 --> 00:59:27.210
So the first thing we'll do is we'll
take the URL that we've been passed in

00:59:27.210 --> 00:59:30.140
and call extended audio file OpenURL.

00:59:30.140 --> 00:59:32.950
Now, if it successfully opens the
file that we've provided,

00:59:32.950 --> 00:59:36.440
we'll get back an extended audio
file reference that we can pass

00:59:36.440 --> 00:59:39.870
to the subsequent APIs in the set.

00:59:41.630 --> 00:59:44.620
Next, we're going to call Extended
Audio File Get Property,

00:59:44.620 --> 00:59:49.030
and we're going to use the
File Data Format property so that we

00:59:49.030 --> 00:59:51.920
can get the original format of the
data that's actually in the file.

00:59:51.920 --> 00:59:55.960
Now, even if we don't want that format
to pass when we pull packets out,

00:59:55.960 --> 01:00:00.200
we do want a couple of fields out of
that Audio Stream Basic Description.

01:00:01.190 --> 01:00:03.380
Specifically,
we want to keep the sample rate and

01:00:03.540 --> 01:00:05.260
the number of channels the same.

01:00:05.260 --> 01:00:10.360
So even if we've got MP3 data that's
stereo 44K and we want 16-bit,

01:00:10.360 --> 01:00:12.680
we still want it to be 44K stereo.

01:00:12.680 --> 01:00:17.180
So we're going to fill out
this output format struct here,

01:00:17.390 --> 01:00:20.820
which is an
Audio Stream Basic Description,

01:00:20.820 --> 01:00:24.720
and we're going to maintain our sample
rates and our channels per frame.

01:00:24.720 --> 01:00:27.990
And then we're going to fill out
the remaining fields to designate

01:00:28.030 --> 01:00:30.130
that we want 16-bit integer data.

01:00:30.760 --> 01:00:35.000
So that first field, the Format ID,
we're going to pass in Linear PCM.

01:00:35.000 --> 01:00:37.940
And then the next field,
the Bytes Per Packet,

01:00:37.990 --> 01:00:40.130
and then two fields down,
the Bytes Per Frame,

01:00:40.140 --> 01:00:44.580
we're going to designate that to be
the size of each packet of PCM data.

01:00:44.580 --> 01:00:47.400
So two being a 16-bit sample,
so two bytes,

01:00:47.520 --> 01:00:50.530
times the number of channels we have,
either mono,

01:00:50.540 --> 01:00:53.240
it's probably going to be mono or stereo.

01:00:53.480 --> 01:00:56.760
And then the frames per packet,
as Jeff also mentioned earlier,

01:00:56.760 --> 01:01:00.590
packets of PCM are always
one frame at a time.

01:01:00.620 --> 01:01:03.990
And then the bits per channel, 16,
we want 16-bit.

01:01:03.990 --> 01:01:07.120
And then the last field there,
the format flags,

01:01:07.120 --> 01:01:09.300
this is somewhat important if
you're dealing with OpenAL.

01:01:09.300 --> 01:01:13.700
OpenAL expects to get integer
data in whatever Endian format

01:01:13.720 --> 01:01:17.780
you're running on at the time,
because there's no way to

01:01:17.850 --> 01:01:21.880
designate what the data,
the format is at runtime.

01:01:22.420 --> 01:01:26.100
So if you're running on a PowerPC,
it expects to get big

01:01:26.100 --> 01:01:27.740
Endian integer data.

01:01:27.740 --> 01:01:29.140
And if you're running
on an Intel machine,

01:01:29.140 --> 01:01:30.820
it's going to expect
to get little Endian.

01:01:30.820 --> 01:01:32.200
So here's where we would set that.

01:01:32.220 --> 01:01:36.870
Now that we have our AudioStream
basic description filled out,

01:01:37.010 --> 01:01:42.110
we're going to pass it to the Extended
Audio Set Property API to designate

01:01:42.120 --> 01:01:44.720
what our client data format is.

01:01:44.720 --> 01:01:47.450
In other words,
this is the format we want to get,

01:01:47.880 --> 01:01:51.260
receive our audio data in when we
ask to read packets from the file.

01:01:51.440 --> 01:01:52.440
Thank you.

01:01:53.240 --> 01:01:54.590
Okay, so we've done our format stuff.

01:01:54.680 --> 01:01:58.180
Now, the next thing we need to do
is determine how many frames

01:01:58.180 --> 01:02:00.020
of data we have in the file.

01:02:00.130 --> 01:02:02.120
For our simple example,
we're just going to read all

01:02:02.120 --> 01:02:03.250
the data out in one shot.

01:02:03.320 --> 01:02:05.400
Now, if you have a big file,
you may want to break this up,

01:02:05.500 --> 01:02:08.150
but just for the purposes
of this demonstration,

01:02:08.240 --> 01:02:10.520
we're just going to pull
it all out at one time.

01:02:10.520 --> 01:02:13.450
So we get the total number of frames,
and then we're going to

01:02:13.460 --> 01:02:16.840
allocate some memory,
which is the total number of frames

01:02:16.840 --> 01:02:20.030
times the size of our packets,
which we know from filling out

01:02:20.130 --> 01:02:22.560
the AudioStream basic description.

01:02:22.650 --> 01:02:24.210
Once we do that,
we're going to read all of

01:02:24.220 --> 01:02:28.600
the packets out of the file
into our audio buffer list,

01:02:28.600 --> 01:02:32.940
which is pointing to our
allocated memory that we just did.

01:02:33.480 --> 01:02:36.290
And if that's successful,
we no longer need our

01:02:36.290 --> 01:02:41.300
extended audio file reference,
so we can dispose it and pass the data

01:02:41.300 --> 01:02:44.910
back to the caller of the function.

01:02:45.440 --> 01:02:48.740
So for the distortion demo,
it's very similar.

01:02:48.830 --> 01:02:50.840
If you look at the code,
we're not going to use

01:02:50.840 --> 01:02:52.550
capture APIs to get the data.

01:02:52.550 --> 01:02:56.590
We're going to get our data out of
a file via the extended audio files.

01:02:56.620 --> 01:03:00.000
We're going to take that data,
fill OpenAL buffers,

01:03:00.120 --> 01:03:04.630
attach it to a source for playback,
and we're going to apply the distortion

01:03:04.980 --> 01:03:09.570
effect much the same way that we applied
all the settings for the Roger Beat.

01:03:09.580 --> 01:03:14.310
So if we could go back to
-- there you beat me to it.

01:03:14.310 --> 01:03:17.970
I'll run this one.

01:03:17.970 --> 01:03:17.970
Let's see.

01:03:28.200 --> 01:03:32.200
Okay, so there's our test,
and we're going to provide it a file.

01:03:32.280 --> 01:03:35.340
Now, you might recognize the dialogue,
but keep in mind we're going to hear

01:03:35.340 --> 01:03:38.370
it through the distortion audio unit.

01:03:38.900 --> 01:03:41.060
You can see it is time
to play Choose a Vista.

01:03:41.060 --> 01:03:42.060
Oh, what's going on?

01:03:42.150 --> 01:03:44.170
Well,
Vista comes in six different versions,

01:03:44.170 --> 01:03:45.810
but I don't know which to choose.

01:03:46.200 --> 01:04:00.600
( Transcript missing )

01:04:05.300 --> 01:04:09.290
So that's the distortion unit.

01:04:09.290 --> 01:04:12.140
So we could go back to slides.

01:04:16.220 --> 01:04:18.100
So that about wraps up our session.

01:04:18.100 --> 01:04:25.690
Here's a couple of contacts for anybody
that's interested in contacting us later.

01:04:26.150 --> 01:04:29.590
I want to remind you that we have a
Core Audio session tomorrow at 2:00,

01:04:29.710 --> 01:04:32.550
from 2:00 to 6:00,
so you can come and ask us

01:04:32.630 --> 01:04:37.090
questions about anything that
we've talked about today.

01:04:41.850 --> 01:04:42.800
There you go.

01:04:42.890 --> 01:04:43.800
Sorry.

01:04:43.880 --> 01:04:48.600
I'd like to bring up
Bill Stewart now to host some Q&A.

01:04:48.600 --> 01:04:49.800
And thanks very much.