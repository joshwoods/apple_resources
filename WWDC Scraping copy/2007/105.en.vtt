WEBVTT

00:00:14.130 --> 00:00:15.770
>> Adam: Good morning.

00:00:15.769 --> 00:00:20.939
And welcome to Session 105; Optimizing
Your Core Data Application.

00:00:20.940 --> 00:00:25.320
I'm Adam Swift, I am an engineer on the Core Data team.

00:00:25.320 --> 00:00:31.170
And two years ago we shipped the
first version of Core Data on Tiger,

00:00:31.170 --> 00:00:33.120
and we love seeing what you guys have done with it.

00:00:33.119 --> 00:00:38.989
We've gotten a lot of great feedback, and
we've been busy for the last two years working

00:00:38.990 --> 00:00:43.560
on adding new APIs, new features, and tools.

00:00:43.560 --> 00:00:47.450
So today I'm going talk about some
optimization techniques that you can use,

00:00:47.450 --> 00:00:51.179
and some general information about Core Data performance.

00:00:52.880 --> 00:01:02.750
Specifically, we've -- we've added a new Atomic Store API,
a lot of new performance APIs, performance-tuning APIs,

00:01:02.750 --> 00:01:08.709
and we've learned a lot about optimizing
Core Data applications for your applications.

00:01:08.709 --> 00:01:16.069
Later on Ben will come up and he will be talking about
multithreading, Core Data performance tools and analysis,

00:01:16.069 --> 00:01:21.869
and some of the raw speed improvements
we've been able to achieve in the framework.

00:01:21.870 --> 00:01:24.130
Excuse me.

00:01:24.129 --> 00:01:31.659
So let's start off talking about some of the tools -- sorry
-- the types of stores you can use and the tradeoffs.

00:01:31.659 --> 00:01:33.299
This should be familiar to most of you.

00:01:33.299 --> 00:01:36.519
But we're going to cover it again.

00:01:36.519 --> 00:01:38.939
There are three basic categories of stores.

00:01:38.939 --> 00:01:44.939
The SQLite store, in-memory store,
and the Atomic Store.

00:01:46.569 --> 00:01:50.839
SQLite is your best choice when
you're dealing with performance.

00:01:50.840 --> 00:01:54.189
It scales to large data sets extremely well.

00:01:54.189 --> 00:01:58.349
It uses a real relational database as its backing,

00:01:58.349 --> 00:02:03.679
and as a result you're only loading the
data you actually want to work with.

00:02:03.680 --> 00:02:07.770
This means you're going to have a smaller
in-memory footprint, and due to the efficient way

00:02:07.769 --> 00:02:13.219
that SQLite stores data on the disc, you
actually get a smaller store on the file system

00:02:13.219 --> 00:02:16.819
than you would with the other store types.

00:02:16.819 --> 00:02:20.939
SQLitealso offers the most performance-tuning options.

00:02:20.939 --> 00:02:24.139
Things that the other store types can't really offer.

00:02:25.560 --> 00:02:30.250
The in-memory store is unique, it
doesn't offer any persistence, really.

00:02:30.250 --> 00:02:32.240
It's all stored in memory, in RAM.

00:02:32.240 --> 00:02:34.469
So why would you want to use it?

00:02:34.469 --> 00:02:35.490
There's two good reasons.

00:02:35.490 --> 00:02:43.780
One is Object Graph Management, integration with Cocoa
Technologies, and free support for Undo and Redo.

00:02:43.780 --> 00:02:49.159
The other thing is in Tiger, this is
the only way to get your data into

00:02:49.159 --> 00:02:53.030
and out of a custom file format, or Legacy file format.

00:02:53.030 --> 00:02:58.469
You had to import or export your data directly
into Managed Objects and work with it that way.

00:02:58.469 --> 00:03:02.479
The good news is we've added a new Atomic Store API.

00:03:02.479 --> 00:03:07.369
So you can now support your custom
file format directly within Core Data.

00:03:07.370 --> 00:03:09.740
It's a first-class persistent store.

00:03:09.740 --> 00:03:14.469
We still support the XML and the
binary stores, and all the store types,

00:03:14.469 --> 00:03:20.859
all the Atomic Store types handle
persistence with Atomic file reads and writes.

00:03:20.860 --> 00:03:23.920
So when you load your store, you're reading in every record.

00:03:23.919 --> 00:03:27.269
When you save your store, you're writing out every record.

00:03:27.270 --> 00:03:31.550
Performance-wise you're not going to be able to
scale as well as you can with the SQLite store,

00:03:31.550 --> 00:03:37.870
but the Atomic Stores offer some nice development time
benefits since you can -- with the XML store, for example,

00:03:37.870 --> 00:03:40.300
you can inspect the data in your store type.

00:03:40.300 --> 00:03:44.620
So during development time can be
handy to use the XML store type.

00:03:44.620 --> 00:03:52.360
And then when you're ready for deployment make the switch
to SQLite or something -- if performance is your issue.

00:03:52.360 --> 00:03:57.210
So what about building your own Atomic Store?

00:04:01.879 --> 00:04:03.549
The API works really nicely.

00:04:03.550 --> 00:04:07.530
We take care of all the interfacing with the coordinator.

00:04:07.530 --> 00:04:11.990
All you need to do is build a support
for your custom file format.

00:04:11.990 --> 00:04:15.490
You might need to do this if you've
got a Legacy file format,

00:04:15.490 --> 00:04:20.790
or if you want to be able to interchange data
with other technologies or platforms.

00:04:20.790 --> 00:04:24.920
you're building an Atomic Store, it's
a first class persistent store.

00:04:24.920 --> 00:04:31.699
As far as Core Data's concerned it's just like
any other -- you don't lose out on anything there.

00:04:31.699 --> 00:04:34.909
There are some file requirements though.

00:04:34.910 --> 00:04:38.980
You -- you need to be able to store
not just the record data,

00:04:38.980 --> 00:04:43.100
not just your attribute values and relationship states.

00:04:43.100 --> 00:04:48.250
But also the unique IDs for those objects,
and also some of the store information.

00:04:48.250 --> 00:04:50.949
There's a metadata dictionary that's required.

00:04:50.949 --> 00:04:55.750
And you also need to store your unique identifier.

00:04:55.750 --> 00:05:03.519
Or UUID. So let's take a look at
how these actually work in practice.

00:05:03.519 --> 00:05:07.949
In the diagram here you can see we've
got the Persistent Store Coordinator.

00:05:07.949 --> 00:05:10.990
And a picture of your file type.

00:05:10.990 --> 00:05:12.600
Your custom file type.

00:05:12.600 --> 00:05:17.590
All the interactions with the Atomic Store
occur between the coordinator around the store.

00:05:17.589 --> 00:05:22.389
There's no interactions with the Managed
Object context or -- or objects at that level.

00:05:22.389 --> 00:05:24.789
So it starts off when you open your store file.

00:05:24.790 --> 00:05:29.220
Persistent Store Coordinator receives
the addPersistentStoreWithType method.

00:05:29.220 --> 00:05:36.260
Looks up the type for the class of Atomic Store
that you've registered and creates an instance.

00:05:36.259 --> 00:05:44.019
At that time the Coordinator checks the metadata for your
store file to make sure it's compatible with the data model.

00:05:44.019 --> 00:05:48.629
And then issues a request to load all of
the record data from your Atomic Store.

00:05:48.629 --> 00:05:53.329
At that time your Atomic Store needs
to register all the record data,

00:05:53.329 --> 00:05:58.060
reading it in from the external file
and representing it as cache nodes.

00:05:59.750 --> 00:06:05.740
Once that's done, the Coordinator returns control,
your store returns control to the Coordinator,

00:06:05.740 --> 00:06:12.379
and it's up to the user using your application to insert
objects, update, delete, make changes, however they will.

00:06:12.379 --> 00:06:20.420
And none of this effects your store or cache nodes until
the coordinator receives a request to save those changes.

00:06:20.420 --> 00:06:27.750
At that time any newly inserted objects
need to get permanent object IDs.

00:06:27.750 --> 00:06:35.240
When you first insert an object they have temporary IDs, and
your store is responsible for providing that unique data.

00:06:36.600 --> 00:06:41.600
So the coordinator asks your store for the
unique IDs for all the newly inserted objects.

00:06:41.600 --> 00:06:45.410
Once it has them it updates the objects
with their permanent IDs and then requests

00:06:45.410 --> 00:06:49.980
that the store add new cache nodes
for each newly inserted object.

00:06:49.980 --> 00:06:55.840
Any updated Managed Objects need to have those values
propagated to the cache nodes to synchronize their state.

00:06:55.839 --> 00:07:00.239
And any Managed Objects that have been deleted
need their cache nodes deleted in your store.

00:07:00.240 --> 00:07:04.000
So the Coordinator will ask your store to delete those.

00:07:04.000 --> 00:07:06.730
Finally the Coordinator issues the command to save changes.

00:07:06.730 --> 00:07:14.610
And at that time your Atomic Store will have
only the cache nodes that are current according

00:07:14.610 --> 00:07:17.319
to the state from the Saved Managed Object context.

00:07:17.319 --> 00:07:23.740
And so you simply walk your set of registered cache
nods and write those out to your external file.

00:07:23.740 --> 00:07:27.090
You're done.

00:07:27.089 --> 00:07:31.919
So all of these interactions come
down to interfacing with two classes.

00:07:31.920 --> 00:07:35.390
The AtomicStore, and the AtomicStoreCacheNode.

00:07:35.389 --> 00:07:39.769
The great sample code available associated
with this session on the ADC website.

00:07:39.769 --> 00:07:46.490
Take a look at the CustomAtomicStoreSubClass example.

00:07:46.490 --> 00:07:53.800
When you're doing your Atomic Store class it's an
atomic -- um, abstract class that you need to sub class.

00:07:53.800 --> 00:07:59.819
And what you need to do is provide implementations
for all of the work that has to happen to archive data

00:07:59.819 --> 00:08:04.120
in your proprietary format, or your
custom format, and read it back in.

00:08:04.120 --> 00:08:07.879
As well as managing the cache nodes
that are representing that data.

00:08:07.879 --> 00:08:11.300
Representing the records for the Coordinator.

00:08:11.300 --> 00:08:16.780
One of the subtle details in managing those
cache nodes is when you remove the store

00:08:16.779 --> 00:08:20.869
any relationships between cache
nodes will retain those cache nodes.

00:08:20.870 --> 00:08:22.590
And you can wind up with a retain cycle.

00:08:22.589 --> 00:08:28.169
So you need to make sure to break those
up when it's time to remove the store.

00:08:28.170 --> 00:08:33.529
You also need to provide access to metadata
and the store's unique identifier.

00:08:35.559 --> 00:08:46.089
The AtomicStoreCacheNode represents the
record data for all the records in your store.

00:08:46.090 --> 00:08:50.420
And this is the intermediary representation
that the Coordinator talks to.

00:08:50.419 --> 00:08:58.829
When you've got a Managed Object that's a fault and you want
to get the property values faulted into that Managed Object,

00:08:58.830 --> 00:09:05.830
the Coordinator talks to the cache
nodes to get those actual values.

00:09:05.830 --> 00:09:07.430
Particularly with the Atomic Stores.

00:09:07.429 --> 00:09:13.509
We'll get into that a little more later.

00:09:13.509 --> 00:09:18.789
So once you've decided how you're actually going
to store your data file, where it's going to live,

00:09:18.789 --> 00:09:22.809
the next step is how are you going
to design your data model?

00:09:22.809 --> 00:09:25.199
How are you going to keep your information?

00:09:25.200 --> 00:09:27.750
This is a complicated issue.

00:09:27.750 --> 00:09:30.929
This is something that people get tripped up in a lot.

00:09:30.929 --> 00:09:36.959
You can do it in a simple way, but to really get
good performance it's a little bit more involved.

00:09:36.960 --> 00:09:38.680
There is no single right answer.

00:09:38.679 --> 00:09:41.599
It really depends on your application's workflow.

00:09:41.600 --> 00:09:45.389
And the application-specific data.

00:09:45.389 --> 00:09:50.199
So how do you start designing your model?

00:09:50.200 --> 00:10:00.840
If you saw Melissa's talk earlier today -- sorry
-- she gave you a basic idea of how you handle --

00:10:00.840 --> 00:10:03.899
how you can do the strategy of database normalization.

00:10:03.899 --> 00:10:09.889
This is a good starting point for breaking up all the
data that you want to work with in your application.

00:10:09.889 --> 00:10:14.590
There's great examples and details on Wikipedia if you
missed Melissa's talk, or if you want to learn more

00:10:14.590 --> 00:10:17.500
about it, I highly recommend you take a look there.

00:10:17.500 --> 00:10:25.899
But it basically comes down to normalizing your data model,
avoiding data duplication, modeling the real relationships

00:10:25.899 --> 00:10:31.470
between data that's -- as it's
represented in the real world.

00:10:31.470 --> 00:10:37.730
I would also say database normalization is sort of
a -- sometimes you can take it a little too far.

00:10:37.730 --> 00:10:42.420
There's about six levels you can
go of database normalization.

00:10:42.419 --> 00:10:46.370
Practically speaking, you are only
interested in the first one or two levels.

00:10:46.370 --> 00:10:50.600
And that's avoiding the data duplication issues.

00:10:50.600 --> 00:10:56.850
The other part of it is sort of an
art of database -- data modeling.

00:10:56.850 --> 00:11:03.680
You want to look at how your application workflow goes,
group information according to how it will be displayed

00:11:03.679 --> 00:11:08.379
in your interface, and then you want to turn that
around and when you're designing your interface,

00:11:08.379 --> 00:11:13.299
consider how much information you can
really present to the user at one time.

00:11:13.299 --> 00:11:16.899
The user won't benefit if you throw
too much information at them,

00:11:16.899 --> 00:11:20.470
and it's also going to lead to bloat in your data model.

00:11:22.909 --> 00:11:26.809
So relationships are a powerful tool in data modeling.

00:11:26.809 --> 00:11:36.989
And they allow you to defer loading up some data, minimize
unnecessary data loading, and objects that are on --

00:11:36.990 --> 00:11:44.090
at the destination of a relationship can be compared
as IDs rather than actually comparing the attribute values.

00:11:44.090 --> 00:11:48.070
This can be a lot faster, and in a lot of cases,
especially if the attributes don't compare quickly.

00:11:48.070 --> 00:11:57.310
The other aspect of that is if you are frequently accessing
some data that's at the destination of a relationship

00:11:57.309 --> 00:12:02.739
that involves reading and data from two tables,
you'll be faster to put that right into your main --

00:12:02.740 --> 00:12:06.259
as an attribute right into your main entity.

00:12:06.259 --> 00:12:11.529
Additionally, it takes additional -- it takes
some time to update values across a relationship.

00:12:11.529 --> 00:12:17.759
One of the great new additions to the
data modeling tool and the framework

00:12:17.759 --> 00:12:23.110
in Leopard is we now support transformable attributes.

00:12:23.110 --> 00:12:29.009
In the past you would have to represent a runtime
representation in your data model and then archive it

00:12:29.009 --> 00:12:37.250
out as binary data and with the new -- new support in
Leopard you can simply mark attributes as transformable.

00:12:37.250 --> 00:12:41.139
They are automatically transformed
using the value transformer class

00:12:41.139 --> 00:12:43.819
that you specify in the data modeling tool.

00:12:43.820 --> 00:12:46.790
And they are archived out as binary data.

00:12:46.789 --> 00:12:52.789
It's a huge time saver, and let's you focus
on the work that you really want to do.

00:12:52.789 --> 00:12:58.230
So let's take a little moment to talk about binary data.

00:12:58.230 --> 00:13:03.090
This is another area where you can
get tripped up with performance.

00:13:03.090 --> 00:13:08.810
There's no hard and fast rules about where
and how you want to model your binary data.

00:13:08.809 --> 00:13:19.969
But we have some good rules of thumb, and basically it comes
down to the size that the binary data's going to tend to be.

00:13:19.970 --> 00:13:26.930
For example, if you've got something that's small, on
the order of 1K, something like -- and it's color --

00:13:26.929 --> 00:13:29.949
that's a perfect example for a transformable attribute.

00:13:29.950 --> 00:13:35.280
Something that's written directly into
your entity, your main entity table.

00:13:35.279 --> 00:13:37.299
And performance shouldn't suffer at all.

00:13:37.299 --> 00:13:41.339
I mean, you would actually probably see a
performance hit by moving that into a relationship.

00:13:41.340 --> 00:13:47.740
On the other hand, if you're looking at something a
little bit bigger like a thumbnail image or an icon,

00:13:47.740 --> 00:13:52.789
something on the order of 100 kilobytes, you don't
want to load that data in when you're loading

00:13:52.789 --> 00:13:56.029
in your primary data -- your primary entity.

00:13:56.029 --> 00:14:00.159
It's better to push that off, defer
loading that until it's needed.

00:14:00.159 --> 00:14:06.969
And so you want to put it at the end of a 2-1
relationship, so you can just get that data on demand.

00:14:06.970 --> 00:14:12.570
On the other hand, if you are looking at something
truly large, something on the order of a megabyte,

00:14:12.570 --> 00:14:17.550
like a video file -- it's just -- you're better
off not putting it in the database at all.

00:14:17.549 --> 00:14:20.889
Keep it on the file system.

00:14:20.889 --> 00:14:25.269
Make a reference and its URL, and look it up on demand.

00:14:25.269 --> 00:14:27.870
It's going to bog down your performance.

00:14:30.129 --> 00:14:40.129
So once you're done normalizing your database it's time
to start thinking about performance considerations.

00:14:40.129 --> 00:14:45.399
Specifically, there are times when it
makes sense to add derived attributes.

00:14:45.399 --> 00:14:51.000
Derived attributes are a way of referring
to attributes that can be recalculated

00:14:51.000 --> 00:14:55.149
from other attributes intrinsic to your model.

00:14:55.149 --> 00:15:00.929
We've used derivative attributes
for benefits and search performance,

00:15:00.929 --> 00:15:05.299
there are other cases where there
will be an expensive calculated value

00:15:05.299 --> 00:15:07.849
that you want to be able to look up quickly.

00:15:07.850 --> 00:15:13.000
And derived attributes -- you pay
a cost by having some redundant data,

00:15:13.000 --> 00:15:17.090
some redundancy in what you're
storing, additional memory usage.

00:15:17.090 --> 00:15:23.519
However -- and additionally you're having to maintain
this data any time the underlined value changes.

00:15:23.519 --> 00:15:30.789
But there are a lot of circumstances where it will solve
your performance problems better than any other solution.

00:15:30.789 --> 00:15:40.699
So let's take a look at an example of a Calendar
application where event data has some --

00:15:40.700 --> 00:15:44.320
well, you're displaying events in, say, a week view.

00:15:44.320 --> 00:15:55.030
And each of the events has associations with
people or alarms, notes, and other display badges.

00:15:55.029 --> 00:16:01.889
In a well normalized model, this is how it might look.

00:16:01.889 --> 00:16:08.509
You've got your event entity, you've got person, alarm, note --

00:16:08.509 --> 00:16:13.399
but the problem is when you go to display all
of these events into a single place in a window,

00:16:13.399 --> 00:16:18.120
you're having to walk all of these key paths
to calculate are there any people associated

00:16:18.120 --> 00:16:19.899
with this event? Should I put the badge on?

00:16:19.899 --> 00:16:20.850
Are there any alarms?

00:16:20.850 --> 00:16:23.509
Are there any notes?

00:16:23.509 --> 00:16:31.700
So if you're having -- if your display is getting bogged
down, it's a perfect time to consider de-normalization.

00:16:31.700 --> 00:16:36.340
Add some derived attributes right
on the event table that indicate --

00:16:36.340 --> 00:16:40.420
does this event have alarms? Does this event
have attendees? Does this event have notes?

00:16:40.419 --> 00:16:46.319
That way you can search, you can load up your event
data without having to walk any key paths and be able

00:16:46.320 --> 00:16:50.720
to provide those visual cues very quickly.

00:16:52.580 --> 00:16:57.660
Another confusing point in data
modeling is entity inheritance.

00:16:57.659 --> 00:17:01.519
This has nothing to do with class inheritance.

00:17:01.519 --> 00:17:04.420
It's something that trips up a lot of people.

00:17:07.390 --> 00:17:13.320
The right reason to use entity inheritance, the best
reason to use entity inheritance, is if you want to be able

00:17:13.319 --> 00:17:16.869
to fetch a heterogeneous collection of entities.

00:17:17.960 --> 00:17:27.559
One of the new API features is you can now specify that for
a particular fetch you don't want to include subentities.

00:17:27.559 --> 00:17:32.700
But I -- I want to tell you to consider inheritance --

00:17:32.700 --> 00:17:36.299
entity inheritance carefully when
you're designing your model.

00:17:36.299 --> 00:17:40.109
The reason why -- it has a lot to do with how it works.

00:17:40.109 --> 00:17:47.729
All of the subentities of the core root entity
wind up getting folded into the root entity table.

00:17:47.730 --> 00:17:52.680
So you get an aggregation of all of the
properties, and not only that you get an aggregation

00:17:52.680 --> 00:17:56.630
of all of the different entity instances.

00:17:56.630 --> 00:18:01.680
So your table's wider and longer than it would
be if it were broken up into separate entities.

00:18:01.680 --> 00:18:04.940
This is, again, mindful of performance
-- this can be a problem.

00:18:04.940 --> 00:18:06.170
You want to keep it shallow.

00:18:06.170 --> 00:18:10.110
Let's take sort of a quick look at how this gets realized.

00:18:10.109 --> 00:18:19.309
We've got sort of an example model showing an
inheritance of a collection of six entities.

00:18:19.309 --> 00:18:25.679
And you can see in the table at the bottom all of these
entities's properties are written into a single table

00:18:25.680 --> 00:18:27.910
and you wind up with a lot of fragmented information.

00:18:27.910 --> 00:18:37.100
If you're only using entity B, you're still paying the
cost to have all of those columns unused in your database.

00:18:37.099 --> 00:18:41.449
So can't emphasize this enough.

00:18:41.450 --> 00:18:43.660
Optimize your data model.

00:18:43.660 --> 00:18:48.490
All of the performance work you want to do with
your application is really grounded in coming

00:18:48.490 --> 00:18:51.849
up with the right model to allow you that flexibility.

00:18:51.849 --> 00:18:57.609
The great news is if you didn't get it just right
the first time you've got a lot of tools in Leopard

00:18:57.609 --> 00:19:02.349
with the version migration support to
change your model and migrate your stores.

00:19:02.349 --> 00:19:07.299
I encourage you all to go to Miguel
and Ron's session at 2 o'clock.

00:19:07.299 --> 00:19:09.220
They'll be talking about the tools.

00:19:09.220 --> 00:19:13.519
And they'll show you how you can
map from one scheme into another.

00:19:13.519 --> 00:19:18.869
In many cases using simple mappings;
with key paths, not even using any code,

00:19:18.869 --> 00:19:22.919
and if there are complicated transformations we've
got all the hooks in there for you to be able

00:19:22.920 --> 00:19:27.759
to make those changes in your own custom subclasses.

00:19:27.759 --> 00:19:33.359
So now let's look at actually getting
the data into your application.

00:19:33.359 --> 00:19:35.719
Fetching and faulting.

00:19:35.720 --> 00:19:44.150
A lot of the new APIs that we've offered
in terms of performance tuning allow you

00:19:44.150 --> 00:19:48.130
to fine-tune how you do your fetching and faulting.

00:19:48.130 --> 00:19:53.910
There are -- when we talk about records
that you're storing in your data store,

00:19:53.910 --> 00:19:56.870
we're talking about a couple different aspects.

00:19:56.869 --> 00:20:01.689
There is an object ID, which is a very
lightweight reference to an object

00:20:01.690 --> 00:20:05.470
that contains no property data, no relationship information.

00:20:05.470 --> 00:20:09.890
And then there's at a slightly more
full level we've got the fault.

00:20:09.890 --> 00:20:11.830
The fault knows about the object ID.

00:20:11.829 --> 00:20:14.539
A fault is a Managed Object that's an empty shell.

00:20:14.539 --> 00:20:19.069
It doesn't have its property data yet; it's a placeholder.

00:20:19.069 --> 00:20:29.009
So when you ask a fault -- for one of its properties because
it looks like a Managed Object, it will automatically go

00:20:29.009 --> 00:20:34.240
to the database to load up the data for that Managed Object.

00:20:34.240 --> 00:20:42.569
Something that's happening behind the scenes that tends not
to be called out very often, but you saw it with our talk

00:20:42.569 --> 00:20:47.029
about the Atomic Store API is the snapshot of property data.

00:20:47.029 --> 00:20:50.430
In the Atomic Store API that was the cache node.

00:20:50.430 --> 00:20:56.190
And what that is, is that is at the Coordinator
level a representation for every record

00:20:56.190 --> 00:21:00.970
that has been loaded of its state of property values.

00:21:00.970 --> 00:21:02.890
So we're going to get back to that in a minute.

00:21:02.890 --> 00:21:10.650
But first I'd like to show you a quick
demo of the headstart example GoFetch.

00:21:10.650 --> 00:21:19.519
This -- this will give us a chance to look at some of
the new APIs and how they can impact fetch performance.

00:21:19.519 --> 00:21:21.910
So this is the GoFetch example.

00:21:21.910 --> 00:21:26.430
And along the left side here you can see
we've got a number of different controls

00:21:26.430 --> 00:21:33.820
to select whether we want Managed Objects,
or if we want to fetch Managed Object IDs.

00:21:33.819 --> 00:21:39.019
Whether we want to include subentities,
property types, or return objects as fault.

00:21:39.019 --> 00:21:43.049
So I'm just going to real quickly go through this.

00:21:43.049 --> 00:21:53.599
And I'll show you the actual commands, SQL commands
being issued to the database for these fetches.

00:21:53.599 --> 00:22:05.719
So first off, if we're just fetching Managed
Object IDs we can get those back pretty quickly.

00:22:05.720 --> 00:22:06.680
Five milliseconds.

00:22:06.680 --> 00:22:11.880
We're not getting our SQL logging, unfortunately.

00:22:11.880 --> 00:22:14.940
Well, that's too bad.

00:22:14.940 --> 00:22:20.320
But the timing should still be interesting.

00:22:20.319 --> 00:22:25.849
So we can do a fetch for Managed
Object IDs in about five milliseconds.

00:22:25.849 --> 00:22:34.169
Just to take it to the next extreme, if I were to
fetch Managed Objects including the property values

00:22:34.170 --> 00:22:42.830
and run the same fetch, we're looking
at 50 -- 53 millisecondsS.

00:22:42.829 --> 00:22:45.490
So I'm afraid I might be running a little short on time.

00:22:45.490 --> 00:22:49.329
And without the SQL logging the rest
of this demo's kind of hard to do.

00:22:49.329 --> 00:22:51.569
But I encourage you to download the example, play with it.

00:22:51.569 --> 00:22:55.849
There's great documentation within the
source files, and also just in the interface.

00:22:55.849 --> 00:23:01.299
There is a lot of discussion of how the --
how these options impact your performance.

00:23:01.299 --> 00:23:07.309
So let's go back to the slides.

00:23:07.309 --> 00:23:11.009
And let's talk about these APIs.

00:23:11.009 --> 00:23:17.240
So something that isn't present in the GoFetch
example but is a really powerful new piece of API

00:23:17.240 --> 00:23:23.960
on the fetch request is you can now fetch
the count of objects that match any fetch.

00:23:23.960 --> 00:23:27.549
So if you're not actually going to use the
objects, if you just need to know how many exist --

00:23:27.549 --> 00:23:31.779
whether it's to size a scroller or to put a number up on --

00:23:31.779 --> 00:23:36.779
on your interface, you can specify for
any fetch request that you want the count.

00:23:36.779 --> 00:23:43.690
It won't fetch any of the IDs, it won't fetch any of the
property values, and it won't fetch any of the records.

00:23:43.690 --> 00:23:47.299
Extremely fast, if all you need to know is how many.

00:23:47.299 --> 00:23:52.480
The next granularity level -- fetching just the IDs.

00:23:52.480 --> 00:23:59.750
This is a really powerful way to deal
with large collections in fetching.

00:23:59.750 --> 00:24:04.130
So you can fetch the IDs, they're very
lightweight, they're thread-safe.

00:24:04.130 --> 00:24:10.990
And what that means is that if you wanted to
do a long-running fetch, something expensive,

00:24:10.990 --> 00:24:13.809
you can perform that fetch on a second thread.

00:24:13.809 --> 00:24:15.659
You'll get back the object IDs.

00:24:15.660 --> 00:24:20.740
The thread's safe, so you can hand those back directly
to your main thread, and that context can work with those

00:24:20.740 --> 00:24:25.710
to fetch in -- to fault in Managed Objects.

00:24:25.710 --> 00:24:32.019
Something that's -- isn't necessarily obvious about
that is one of the options on the fetch request.

00:24:32.019 --> 00:24:37.859
Even if you're fetching object IDs, you can
specify that you want to fetch the property values.

00:24:37.859 --> 00:24:39.629
You want to the fetch to include property values.

00:24:39.630 --> 00:24:45.900
So as I told you before, objects IDs don't
have any representation for property values.

00:24:45.900 --> 00:24:48.090
So why would you want to do that?

00:24:48.089 --> 00:24:51.869
Well, the reason why is because it allows
you to warm up the snapshot cache.

00:24:51.869 --> 00:24:54.500
Those cache nods I was talking about before.

00:24:54.500 --> 00:24:59.250
That actually -- at the Coordinator level
represent the property values for every record.

00:24:59.250 --> 00:25:08.940
So the sort of the most common, frequent pattern for using
this is you boot -- you do your fetch on a second thread.

00:25:08.940 --> 00:25:10.289
You include the property values.

00:25:10.289 --> 00:25:12.950
You hand the object IDs over to your main thread.

00:25:12.950 --> 00:25:17.150
And then as the main thread walks through
and faults in those Managed Objects,

00:25:17.150 --> 00:25:19.210
all of the property values have already been loaded up.

00:25:19.210 --> 00:25:23.380
So there's no disk I/O required to get at them.

00:25:23.380 --> 00:25:28.780
Really powerful way to handle performance
in that circumstance.

00:25:28.779 --> 00:25:34.829
Now, on the other hand, if you're on your main
thread and you know you're going to be X --

00:25:34.829 --> 00:25:39.079
if you're going to be working with
all of the property data immediately.

00:25:39.079 --> 00:25:45.559
Whether you're exporting to a different file format or
you're producing a report that touches all the properties

00:25:45.559 --> 00:25:47.929
of the records that you're interacting with,

00:25:47.930 --> 00:25:53.980
you can now specify that you don't want
your fetch results managed as faults.

00:25:53.980 --> 00:25:58.279
But you want your fetch results to come
back as fully populated Managed Objects.

00:25:58.279 --> 00:26:02.980
So that -- that means if you're going to walk through
a huge collection of Managed Objects you don't need

00:26:02.980 --> 00:26:11.680
to fire faults individually for them, and
that saves you on performance in a big way.

00:26:11.680 --> 00:26:13.150
Going one step further.

00:26:13.150 --> 00:26:21.250
If you know you're going need objects related to the core
entity that you're fetching on, in the same kind of example

00:26:21.250 --> 00:26:27.440
whether you're producing a report or exporting to
another file format, prefetch your neighbors.

00:26:27.440 --> 00:26:33.500
Specify some prefetching key pads on your fetch request
and all of those objects will be fetched in immediately.

00:26:33.500 --> 00:26:40.559
And you won't, again, incur that penalty of doing many
round trips as you walk the key path to fetch the values

00:26:40.559 --> 00:26:44.599
that you need for whatever goal you're trying to achieve.

00:26:44.599 --> 00:26:49.279
One side note on this is you don't need
to worry if your key paths have overlap.

00:26:49.279 --> 00:26:55.529
We do a good job of figuring out the
minimal set and just working that out.

00:26:55.529 --> 00:27:02.389
So -- we've seen a lot of great uses of Core
Data, we've seen a lot of common mistakes.

00:27:02.390 --> 00:27:07.710
And this is going to be a tour of a lot
of a couple of patterns that we've seen

00:27:07.710 --> 00:27:10.670
that get people tripped up in performance.

00:27:10.670 --> 00:27:17.840
And a lot of it comes down to one simple
thing, which is making many round trips

00:27:17.839 --> 00:27:21.659
to the database to fetch data is expensive.

00:27:21.660 --> 00:27:23.600
It's very costly.

00:27:23.599 --> 00:27:30.129
Firing faults individually is the root cause of so
many performance problems in Core Data applications.

00:27:30.130 --> 00:27:39.960
The -- the way I'd like to think of it is consider
how much slower a disc is than your CPU or RAM.

00:27:39.960 --> 00:27:47.299
In the time for run fast disc revolution, about
6 milliseconds, think about how many processes --

00:27:47.299 --> 00:27:51.700
how many cycles an 8-core, 3GHz
processor can get done.

00:27:51.700 --> 00:27:56.870
And you don't want that processor to sit
there and wait for disc I/O all the time.

00:27:56.869 --> 00:27:58.309
So you want to batch things up

00:27:58.309 --> 00:28:04.549
So one real common pattern is you're
importing data into your application.

00:28:04.549 --> 00:28:08.710
So you're doing a lot of inserts and you're
doing a lot of uniquing to find out if the data

00:28:08.710 --> 00:28:13.890
that you're importing is already present in the database.

00:28:13.890 --> 00:28:20.990
The frequent problem is you hit the database
repeatedly to find out if an object already exists.

00:28:20.990 --> 00:28:25.319
Every time you issue a fetch request, every
time you execute a fetch request it's going

00:28:25.319 --> 00:28:27.649
to go out and check the database.

00:28:27.650 --> 00:28:35.680
So keep a local cache, build up the cache as you do your
import, and avoid doing that round trip as much as possible.

00:28:35.680 --> 00:28:41.690
The other side of it is while you're doing all of these
inserts you don't want to save after every insert,

00:28:41.690 --> 00:28:47.559
but you don't want to save after
10,000 inserts or 100,000 inserts.

00:28:47.559 --> 00:28:52.629
Depending -- and these are all kind of rules of thumb.

00:28:52.630 --> 00:28:55.340
There's no hard and fast numbers on this.

00:28:55.339 --> 00:28:59.859
But you want to try and batch into manageable
chunks and that's really sort of a question of looking

00:28:59.859 --> 00:29:02.519
at your application data and analyzing performance.

00:29:02.519 --> 00:29:04.809
Don't try and save a batch that's too big.

00:29:04.809 --> 00:29:08.549
Don't save after every insert.

00:29:08.549 --> 00:29:09.649
Deletions.

00:29:09.650 --> 00:29:11.880
This is a surprise to a lot of people.

00:29:11.880 --> 00:29:14.910
Why would deletion -- why would deleting objects get slow?

00:29:14.910 --> 00:29:16.940
I'm just getting rid of data.

00:29:16.940 --> 00:29:23.759
The reason why deletions can get bogged
down is because of relationship maintenance.

00:29:23.759 --> 00:29:29.129
If you have relationships from the
object you're deleting to other tables,

00:29:29.130 --> 00:29:31.790
the inverse relationships need to be maintained.

00:29:31.789 --> 00:29:36.500
And if you have various delete rules,
depending on the delete rule you're using,

00:29:36.500 --> 00:29:44.230
if you specify a cascading delete rule, deleting
a single object could walk -- a significant --

00:29:44.230 --> 00:29:48.809
over a significant portion of your graph,
fault in and fetch in a lot of objects.

00:29:48.809 --> 00:29:50.409
How do you deal with this?

00:29:50.410 --> 00:29:51.740
Prefetching.

00:29:51.740 --> 00:29:56.029
Take a look at what you're going to
delete before you start the delete.

00:29:56.029 --> 00:29:58.139
Look at the relationships that are going to be affected.

00:29:58.140 --> 00:30:05.420
Set up some relationship key paths, prefetching
relationship key paths, and do a fetch to get all that data

00:30:05.420 --> 00:30:09.700
and ready -- warm up the cache before you do it.

00:30:09.700 --> 00:30:18.500
Again, just as with the inserts, don't save after every
delete, and don't try and do batches that are too big.

00:30:18.500 --> 00:30:20.150
Searching.

00:30:20.150 --> 00:30:22.240
You want to keep your searches focused.

00:30:22.240 --> 00:30:30.200
Predicates that get out of control, crossing a lot
of key paths and relationships can get bogged down.

00:30:30.200 --> 00:30:35.009
They -- they tend not to perform well
when they get past a certain size.

00:30:35.009 --> 00:30:37.210
So what can you do?

00:30:37.210 --> 00:30:39.360
Order the simpler parts first in your predicate.

00:30:39.359 --> 00:30:41.019
That's one thing that helps.

00:30:41.019 --> 00:30:46.289
And the other thing that you can do is you can break up
one large predicate into a number of smaller predicates

00:30:46.289 --> 00:30:53.339
by taking the results of one search, using
those object IDs to feed in to an in predicate --

00:30:53.339 --> 00:31:00.189
or as an in-operator in your next predicate to cut
down the collection that you're searching against.

00:31:00.190 --> 00:31:05.390
Text. Searching against text.

00:31:05.390 --> 00:31:15.090
Core Data provides incredibly complete, excellent
support for Unicode searching, very powerful.

00:31:15.089 --> 00:31:20.379
But it can sometimes get to be a performance
problem if you're dealing with large --

00:31:20.380 --> 00:31:24.790
large blocks of text, or if you're
doing complicated text searches.

00:31:24.789 --> 00:31:30.659
For example, Unicode regex is a
complicated and expensive operation.

00:31:30.660 --> 00:31:34.110
More flexibility, more work.

00:31:34.109 --> 00:31:40.759
One thing that we've used is -- as I was talking about
before with derived attributes, if you take rich text

00:31:40.759 --> 00:31:50.210
and you flatten it to case normalized diacritic normalized
representations you can make great us of indexes.

00:31:50.210 --> 00:31:53.150
Something I didn't mention before.

00:31:53.150 --> 00:31:59.730
But now in the modeling tool you can mark
any attribute as being an indexed attribute.

00:31:59.730 --> 00:32:07.160
So for this SQLite store this means it's going to be
build up an index and do fast look-ups against those values.

00:32:07.160 --> 00:32:10.170
But indexes can only be used for some types of searches.

00:32:10.170 --> 00:32:15.630
So that gets back to the derived values.

00:32:17.180 --> 00:32:22.299
And for the truly pathological case where
you've got something where you want to do, like,

00:32:22.299 --> 00:32:28.529
a Google-style search with complicated
text rules and that sort of thing,

00:32:28.529 --> 00:32:33.079
supplement the search technology with
something like SearchKit indexes.

00:32:33.079 --> 00:32:35.019
We've used this as well.

00:32:35.019 --> 00:32:40.210
One of the benefits there -- well, the way
you make that work is you can make reference

00:32:40.210 --> 00:32:50.319
to your object IDs in the SearchKit indexes.

00:32:50.319 --> 00:32:54.750
And let's get to some of the other things
that I didn't get a chance to cover in detail.

00:32:54.750 --> 00:33:00.160
There's some new API in the Managed Object context.

00:33:00.160 --> 00:33:02.950
When you're working with a couple
of different Managed Object context

00:33:02.950 --> 00:33:08.400
in a single application you can
now listen for new notifications.

00:33:08.400 --> 00:33:10.340
One is the context will save.

00:33:10.339 --> 00:33:15.149
This gives you an opportunity to add
application behaviors before a save.

00:33:15.150 --> 00:33:20.700
And the other one is merging context changes after a save.

00:33:20.700 --> 00:33:24.360
So a common problem that would
happen is you have two contexts.

00:33:24.359 --> 00:33:26.539
One has a set of changes that have been saved.

00:33:26.539 --> 00:33:28.589
The other context isn't aware of those changes.

00:33:28.589 --> 00:33:30.730
And you get inconsistencies.

00:33:30.730 --> 00:33:35.230
Now, you have your second context or your
second thread listening for the notification,

00:33:35.230 --> 00:33:39.539
and you can integrate changes from the context that's saved.

00:33:39.539 --> 00:33:46.079
There's also new API for getting permanent object IDs
so newly inserted objects in one context can be passed

00:33:46.079 --> 00:33:49.819
over to another context with permanent IDs.

00:33:49.819 --> 00:33:53.829
You can't pass objects if they don't have permanent IDs.

00:33:54.920 --> 00:33:58.090
And these are some biggies here.

00:33:58.089 --> 00:33:59.639
Dynamic accessors.

00:33:59.640 --> 00:34:01.640
These are huge performance win.

00:34:01.640 --> 00:34:06.330
You can now call on any NS Managed Object or its subclass.

00:34:06.329 --> 00:34:11.000
The attribute names as direct accessors,
rather than using value

00:34:11.000 --> 00:34:15.050
for key. These are much better performing
ways to access your data.

00:34:15.050 --> 00:34:19.769
And there's also support for dynamic primitive accessors.

00:34:19.769 --> 00:34:26.869
So if you want to be able to access those primitives, again,
don't need to use value for key, or primitive value for key,

00:34:26.869 --> 00:34:30.889
you can call the primitive accessors directly.

00:34:31.920 --> 00:34:37.970
Also, we are huge fans of the new Objective C 2.0 syntax.

00:34:37.969 --> 00:34:40.609
There's the property syntax we fully support.

00:34:40.610 --> 00:34:45.190
And Core Data has been optimized for garbage collection.

00:34:45.190 --> 00:34:48.360
We're also supporting all the new
platform initiatives in Core Data.

00:34:48.360 --> 00:34:52.460
32-bit, 64-bit, multicore, Intel, PowerPC.

00:34:52.460 --> 00:34:56.500
Lot of -- a lot of performance tuning has gone into that.

00:34:56.500 --> 00:35:02.469
And the last point I'm going touch on is there's a new
version of SQLite and Core Data shipping with Leopard.

00:35:02.469 --> 00:35:05.759
It's been updated to 3.3.17.

00:35:05.760 --> 00:35:13.390
And one of the best benefits you will see is
that it has the same default configuration now

00:35:13.389 --> 00:35:15.509
that other platforms have been using.

00:35:15.510 --> 00:35:19.760
So your performance will out of
the box be a lot better in saves.

00:35:19.760 --> 00:35:25.250
There's some new pragmas for controlling disc
synchronization in SQLite.

00:35:25.250 --> 00:35:29.599
Take a look at the Release Notes for Leopard and Core Data.

00:35:29.599 --> 00:35:34.509
And for more information I recommend
you take a look at sqlite.org,

00:35:34.510 --> 00:35:39.240
where there's tons of information
about what you can do with SQLite.

00:35:39.239 --> 00:35:42.349
So -- with that I'm going turn it over to Ben.

00:35:42.349 --> 00:35:46.380
He's going to talk to you about multithreading.

00:35:46.380 --> 00:35:48.010
Thank you very much.

00:35:48.010 --> 00:35:48.710
( Applause )

00:35:48.710 --> 00:35:50.010
>> Ben: Thank you, (Inaudible) --

00:35:53.559 --> 00:35:54.259
>> Ben: Morning.

00:35:54.260 --> 00:35:56.620
I am Ben Trumbull,
I'm the manager of the Core Data team.

00:35:56.619 --> 00:35:59.500
And we're going to get into some issues in multithreading.

00:35:59.500 --> 00:36:03.039
This is a pretty common question
these days about using Core Data.

00:36:03.039 --> 00:36:07.759
And at the end I will wrap up with some
tips about threading with Cocoa in general.

00:36:07.760 --> 00:36:10.690
That turns out to be pretty important to us.

00:36:10.690 --> 00:36:13.360
So there's some good motivations to using threads.

00:36:13.360 --> 00:36:18.849
Responsiveness is probably the key reason to keep
the user working with the UI while you go off

00:36:18.849 --> 00:36:21.809
and do more complicated operation
yourself in the background.

00:36:21.809 --> 00:36:24.340
Or you have some long-running operation.

00:36:24.340 --> 00:36:26.150
You want to leverage additional cores.

00:36:26.150 --> 00:36:29.690
This is one of the reasons that I do
most of my multithreading is, you know,

00:36:29.690 --> 00:36:34.659
no point leaving the machine idle while
it can be used for something to my ends.

00:36:34.659 --> 00:36:39.519
I don't really need it to share well with others.

00:36:39.519 --> 00:36:41.940
And then improving batch save performance.

00:36:41.940 --> 00:36:45.369
If you're running a long import
operation, you're migrating data

00:36:45.369 --> 00:36:50.210
from another app and you're consuming a
large amount of XML or something like that --

00:36:50.210 --> 00:36:53.340
and you're creating a lot of objects
and you need to do a lot of saves.

00:36:53.340 --> 00:36:56.350
That's something you can do pretty well in the background.

00:36:56.349 --> 00:36:59.869
But first there's some bad reasons to be using threads.

00:36:59.869 --> 00:37:02.109
And this is sort of a word of warning here.

00:37:02.110 --> 00:37:06.380
This is a very simple app that I
wrote that just allocates memory.

00:37:06.380 --> 00:37:07.110
That's all it does.

00:37:07.110 --> 00:37:08.579
It just mallocs memory.

00:37:08.579 --> 00:37:15.809
So you can see in the top bar, the green bar, that's
if you have four threads on a quad core machine

00:37:15.809 --> 00:37:19.809
and they're all using the same pool of memory,
and they're all contending over a single lock.

00:37:19.809 --> 00:37:26.599
So here it's more than ten times slower than the same
four threads allocating the same amount of memory,

00:37:26.599 --> 00:37:29.110
with four separate pools and four separate locks.

00:37:29.110 --> 00:37:32.670
They're still doing the same locking, but
they're not contending over the same lock.

00:37:32.670 --> 00:37:37.039
So it's something to keep in mind that when you started
adding threads you need to have something useful

00:37:37.039 --> 00:37:39.190
for those threads to do, where they're not going

00:37:39.190 --> 00:37:44.730
to start impeding the work that other threads
you already have are trying to do.

00:37:44.730 --> 00:37:49.909
So in terms of Core Data itself and our
thread safety, the object IDs are thread safe.

00:37:49.909 --> 00:37:53.920
They are mutable objects and you
can pass them around without worry.

00:37:53.920 --> 00:37:59.059
And then we really want you to have separate
Managed Object context in each of these threads.

00:37:59.059 --> 00:38:01.860
And so that's going to be the basic
pattern we're going to talk

00:38:01.860 --> 00:38:04.940
about today is different threads
are using different context.

00:38:04.940 --> 00:38:11.929
And when they want, they'll--you pass object IDs over to other
threads and they'll pull up those objects themselves.

00:38:11.929 --> 00:38:18.099
The Managed Objects, they're controlled, they're owned by
the same thread that owns their Managed Object context.

00:38:18.099 --> 00:38:23.259
So this pattern we're discussing
is called thread confinement.

00:38:23.260 --> 00:38:27.370
You can read about it online or in
some references I'll bring up later.

00:38:27.369 --> 00:38:33.319
And here the Managed Objects are all being used by
one thread, and they don't migrate between threads.

00:38:33.320 --> 00:38:35.330
This is much more manageable for you to debug.

00:38:35.329 --> 00:38:40.000
If you have an idea of what thread created
those Managed Objects you can put a little stamp

00:38:40.000 --> 00:38:42.199
on it, and they don't move around.

00:38:42.199 --> 00:38:47.819
You don't have multiple threads turning a lock over an individual
fine-grained objects, but you have an opportunity

00:38:47.820 --> 00:38:52.789
to add debugging code to the larger granularity
interactions where the threads start communicating

00:38:52.789 --> 00:38:58.119
to each other, where they start passing object IDs
or other messages, notifications between each other.

00:38:58.119 --> 00:39:00.190
And so that you can focus your debugging efforts.

00:39:00.190 --> 00:39:01.789
And there's also less locking.

00:39:01.789 --> 00:39:07.090
As I showed you, if you have a lot of threads
locking on the same locks that can get slow.

00:39:07.090 --> 00:39:10.100
And so locks are resources that you might contend over.

00:39:10.099 --> 00:39:11.779
Less locking is faster.

00:39:11.780 --> 00:39:16.610
And here the thread confinement pattern is duplicating
a small amount of data, not too much, but a little bit.

00:39:16.610 --> 00:39:20.480
In order to cut back on extraneous locking.

00:39:20.480 --> 00:39:27.309
So if you use this pattern and you have Managed
Objects context separate for each thread,

00:39:27.309 --> 00:39:32.650
then Core Data is automatically going to handle
a lot of the locking needs that the framework has

00:39:32.650 --> 00:39:36.309
so some of our classes implement NS
locking and they need to be locked.

00:39:36.309 --> 00:39:41.599
But if you just create a Managed Object context on one
thread and you don't move it to any other threads,

00:39:41.599 --> 00:39:43.509
and Core Data can handle locking it for you,

00:39:43.510 --> 00:39:46.870
because it knows which thread created
your context and it's not going anywhere.

00:39:46.869 --> 00:39:50.420
It will also handle all the locking
between the context and the coordinator.

00:39:50.420 --> 00:39:55.639
So when the context has a fetch request, or the
context needs to save, it needs to lock the coordinator

00:39:55.639 --> 00:39:58.259
to get its operation, so that another Managed Object context

00:39:58.260 --> 00:40:01.420
on a different (Inaudible) doesn't come
in and interrupt that save operation.

00:40:01.420 --> 00:40:07.320
And Core Data can handle all that locking for you as long
as you have each context confined to a single thread.

00:40:07.320 --> 00:40:12.519
Now, you're going to want to be able
to move information between threads.

00:40:12.519 --> 00:40:16.670
Otherwise it's a really very simple
app and not terribly interesting.

00:40:16.670 --> 00:40:20.700
So as I keep mentioning you can
move the object IDs between threads.

00:40:20.699 --> 00:40:24.829
And the other context on that separate
thread can use the object

00:40:24.829 --> 00:40:28.549
with ID method to create a local clone of that object.

00:40:28.550 --> 00:40:33.240
Now when you do this you can get all
the objects that you'vd previously saved,

00:40:33.239 --> 00:40:36.429
and you can get the updated objects and the deleted objects.

00:40:36.429 --> 00:40:37.879
Because those objects still exist.

00:40:37.880 --> 00:40:41.700
But you can't pass inserted objects to
another context or to another thread,

00:40:41.699 --> 00:40:43.529
because those objects haven't been saved yet.

00:40:43.530 --> 00:40:50.810
So they don't exist outside the scope that
their originating context represents.

00:40:50.809 --> 00:40:56.690
Now like I said, when you pull these clones of
objects into new Managed Objects contexts

00:40:56.690 --> 00:40:58.700
on separate threads you're duplicating a little data.

00:40:58.699 --> 00:41:00.769
But you're not duplicating very much data.

00:41:00.769 --> 00:41:05.469
So you don't have to worry about duplicating all of
your image data or all of your string data or what not.

00:41:05.469 --> 00:41:07.399
The coordinator's providing captions.

00:41:07.400 --> 00:41:12.099
So all the contexts using the same
cache on that coordinator.

00:41:12.099 --> 00:41:16.130
And these are basically analogous to the cache nodes
that Adam's talked about for the custom Atomic Store.

00:41:16.130 --> 00:41:18.480
And the SQLite store has something similar.

00:41:18.480 --> 00:41:22.420
So those -- that data is basically the column
data that we fetched from the database.

00:41:22.420 --> 00:41:28.210
And that's all going to get shared copy on write between
all the Managed Object contexts that use that coordinator.

00:41:28.210 --> 00:41:33.010
So the Managed Objects themselves are fairly lightweight
little wrappers around that, that are provided for you

00:41:33.010 --> 00:41:36.540
to make changes to, or you to work with,
provide you all that Cocoa integration.

00:41:36.539 --> 00:41:42.710
Now you don't have to lock the coordinator when
you're working in the Managed Object context.

00:41:42.710 --> 00:41:48.019
But if you start messaging the store coordinator directly
you will need to lock it to run other threads, from say,

00:41:48.019 --> 00:41:52.539
trying to save while you at the same time
are trying to add a new store.

00:41:52.539 --> 00:41:56.719
Or you want to create a new Managed Object ID from a URI.

00:41:56.719 --> 00:42:02.329
That's a method on the coordinators that take a URI
representation and give you back a Managed Object ID.

00:42:02.329 --> 00:42:05.569
So you have to lock the coordinator
when you interact with it directly.

00:42:05.570 --> 00:42:09.300
You might also want to lock the coordinator
if you have a number of different operations.

00:42:09.300 --> 00:42:11.150
And you want to have a single scope around them.

00:42:11.150 --> 00:42:12.680
So you want a little bit less concurrency.

00:42:12.679 --> 00:42:17.489
Right. So a Managed Object context needs to do a fetch,
and a save, and maybe another fetch and a save.

00:42:17.489 --> 00:42:19.109
Or some other set of operations

00:42:19.110 --> 00:42:22.970
That you want to appear to all the
other threads as a single operation.

00:42:22.969 --> 00:42:26.699
You can lock the coordinator to
basically make the other contexts using

00:42:26.699 --> 00:42:31.109
that coordinator hold off on their interactions.

00:42:31.110 --> 00:42:33.800
Finally, you might want more concurrency.

00:42:33.800 --> 00:42:35.390
In which case you use multiple coordinators.

00:42:35.389 --> 00:42:40.879
And here you're making a performance memory tradeoff,
where you defeat some of the caching we're trying to do.

00:42:40.880 --> 00:42:45.160
So each of those coordinators is going to build
up its own cache of the records it fetches.

00:42:45.159 --> 00:42:52.309
But at the same time when a Managed Objects context locks the
coordinator to do a fetch it's not impacted by any contexts

00:42:52.309 --> 00:42:54.570
that are using different coordinators,
because they are different locks

00:42:54.570 --> 00:42:58.230
and they're a whole different --
stacks all the way down to the bottom.

00:42:58.230 --> 00:43:01.699
So the only locks you then have
are at the file system level.

00:43:01.699 --> 00:43:06.269
And so if you have an SQLite store, there will be a
POSIX file lock, basically is what's going on there.

00:43:06.269 --> 00:43:12.730
So the different coordinators will contend at that
level, but all the computation stuff done in memory.

00:43:12.730 --> 00:43:17.090
You can get more concurrency out of it.

00:43:17.090 --> 00:43:21.850
So there's an example, and in the Developers/Examples
folder on background fetching.

00:43:21.849 --> 00:43:27.009
And this example there's one shared coordinator that's
providing caching between two contexts, one in each thread.

00:43:27.010 --> 00:43:29.950
One's in the main thread and it's bound in the Cocoa UI.

00:43:29.949 --> 00:43:34.739
So Cocoa bindings basically only wants
to interact with the main thread.

00:43:34.739 --> 00:43:36.949
That's the safest way to work with it.

00:43:36.949 --> 00:43:41.509
And so what ends up happening here is the
background thread fetches a whole bunch

00:43:41.510 --> 00:43:44.650
of object IDs and includes the property values.

00:43:44.650 --> 00:43:49.590
And then passes those object IDs to the main thread, which
then tells Cocoa bindings, hey, there are a whole bunch

00:43:49.590 --> 00:43:52.190
of new objects I would like you to
register with the URI controller.

00:43:52.190 --> 00:43:54.630
And here's an example of threading for responsiveness.

00:43:54.630 --> 00:44:03.200
It's a pretty simple app, and it defers loading all of the
words in this dictionary database until the user's done.

00:44:03.199 --> 00:44:09.669
So it pops up the UI quickly and starts fetching
in the separate words individually by letter.

00:44:09.670 --> 00:44:15.170
So validating for correctness, Core Data
does have this threading debug default.

00:44:15.170 --> 00:44:19.559
There is a debug image of Core Data, and
it will log multithreading assertions

00:44:19.559 --> 00:44:22.639
So if you use the debug image you will get these assertions.

00:44:22.639 --> 00:44:29.879
Unfortunately it's not quite ready from developer.apple.com
yet, but it will be available for Leopard.

00:44:29.880 --> 00:44:36.220
And this is a way where you can run your app in the debugger
and you'll get NS assertions whenever you violate one

00:44:36.219 --> 00:44:41.379
of the rules that Core Data has about multithreading,
about what things you need to be locking

00:44:41.380 --> 00:44:44.289
or using the thread confinement
pattern that I've talked about.

00:44:44.289 --> 00:44:49.989
So the thread confinement pattern not only makes it
easier for you to debug, but it also makes it easy for us

00:44:49.989 --> 00:44:55.689
to add a little bit of extra auditing code
to track how you're using these objects.

00:44:55.690 --> 00:44:58.450
So here's some basic Cocoa threading tips.

00:44:58.449 --> 00:45:06.219
And just touching upon a bunch of stuff that's relevant
to how you might be using multiple threads with Core Data.

00:45:06.219 --> 00:45:08.689
First is Undo on background threads.

00:45:08.690 --> 00:45:11.940
The default Undo manager settings are
not compatible with background threads.

00:45:11.940 --> 00:45:13.700
They only work on the main thread.

00:45:13.699 --> 00:45:18.789
So most of the time when you're working with a background
thread and Managed Object contexts in the background,

00:45:18.789 --> 00:45:22.050
typically you just disable the
Undo management on that context.

00:45:22.050 --> 00:45:27.250
If you do want to use Undo on a background thread,
you're going to need to turn groupsByEvent off,

00:45:27.250 --> 00:45:30.699
and manage the under grouping yourself
using the foundation API.

00:45:30.699 --> 00:45:34.669
It's not hard, but it's just a little bit extra work there.

00:45:34.670 --> 00:45:42.250
And finally, if you're creating NS threads, all NS threads
are detached and what this means is that they're optional.

00:45:42.250 --> 00:45:47.420
So the application doesn't actually have to
finish the work they represent in order to quit.

00:45:47.420 --> 00:45:53.500
So the system will let the app quit even
though those threads are busy doing something.

00:45:53.500 --> 00:45:58.329
So if you need the application to wait for one
of the detached threads, one of your NS threads,

00:45:58.329 --> 00:46:01.039
then you're going to have to add some manual synchronization

00:46:01.039 --> 00:46:06.179
to make sure the app doesn't quit while you're
still in the midst of an important operation.

00:46:06.179 --> 00:46:08.859
An important operation might be saving the user's data.

00:46:08.860 --> 00:46:14.550
You might decide to save something in the background,
or you might be importing something in the background --

00:46:14.550 --> 00:46:17.410
anything -- you might be writing to disk in the background.

00:46:17.409 --> 00:46:22.599
And if you're going that you really need to add
extra code to make sure your app doesn't quit

00:46:22.599 --> 00:46:25.589
in the middle of the thread writing out this data.

00:46:25.590 --> 00:46:28.269
Otherwise you risk losing that data.

00:46:28.269 --> 00:46:31.340
The SQLite database will just
roll the transaction back.

00:46:31.340 --> 00:46:34.000
But some of the other data files, right, an XML file,

00:46:34.000 --> 00:46:38.650
if you get halfway through writing
it -- that's going to be really bad.

00:46:38.650 --> 00:46:41.280
Now being a detached thread isn't all bad.

00:46:41.280 --> 00:46:44.230
There are a bunch of things you could
be doing in the background

00:46:44.230 --> 00:46:46.400
where you don't actually care if the app quits.

00:46:46.400 --> 00:46:48.680
In fact, it might even be good for the app to quit.

00:46:48.679 --> 00:46:50.839
Say you're in the midst of a large clean-up operation.

00:46:50.840 --> 00:46:52.700
You're freeing up a lot of in-memory resources.

00:46:52.699 --> 00:46:57.210
Well, if the process quits -- well,
you want the background thread to quit.

00:46:57.210 --> 00:47:00.289
You don't want it to finish because
you're releasing the same resources

00:47:00.289 --> 00:47:03.269
that the kernel would as soon as your process terminates.

00:47:03.269 --> 00:47:07.070
You might also have speculative operations
you could be doing in the background.

00:47:07.070 --> 00:47:09.400
Or you could be doing increments of batch operations.

00:47:09.400 --> 00:47:13.820
If you're doing some kind of batch import that I
mentioned, right -- you could be doing it in batches.

00:47:13.820 --> 00:47:18.769
And you might decide that it's a better UI
experience to just recompute the last batch.

00:47:18.769 --> 00:47:19.989
Just kind of throw it away.

00:47:19.989 --> 00:47:23.369
And the next time the app starts
up, pick up where you left off.

00:47:24.500 --> 00:47:31.480
So in Leopard, the Cocoa team has provided new API, NSOperationQueue.
I'm not going to talk too, too much about this.

00:47:31.480 --> 00:47:35.980
But I just want to make sure you're aware of it, because
it does offer some really nice behaviors that are

00:47:35.980 --> 00:47:39.349
in contrast to the detached NS thread behavior.

00:47:39.349 --> 00:47:42.130
This is a convenient Cocoa API for managing tasks.

00:47:42.130 --> 00:47:49.230
And one of the advantages it has is it allows you to
specify dependencies between these tasks very easily,

00:47:49.230 --> 00:47:51.159
which is something that threads don't do.

00:47:51.159 --> 00:47:53.480
Right? I actually like working with P threads,

00:47:53.480 --> 00:47:58.110
but coordinating between these
threads is a -- it's really tedious.

00:47:58.110 --> 00:48:00.220
So there's a very convenient Cocoa API on this.

00:48:00.219 --> 00:48:02.259
And I recommend you check that out.

00:48:02.260 --> 00:48:05.790
There's also API to wait for the
queue to finish all these operations.

00:48:05.789 --> 00:48:09.750
I keep mentioning you need to add manual
synchronization to prevent the app

00:48:09.750 --> 00:48:11.500
from quitting while something's happening in the background.

00:48:11.500 --> 00:48:15.440
Well, the NSOperationQueue has a method to just
wait for everything to finish and just be done.

00:48:15.440 --> 00:48:18.269
And it also provides ways to suspend and cancel events.

00:48:18.269 --> 00:48:23.320
So your operations could check back with their queue
and realize, uh-uh, the application's trying to quit.

00:48:23.320 --> 00:48:26.090
Maybe I should cancel myself.

00:48:26.090 --> 00:48:29.309
One of the things that, like I said, I really

00:48:29.309 --> 00:48:33.289
like the way it allows you specify
dependencies between these background tasks.

00:48:33.289 --> 00:48:36.590
And it's really tedious to write that code myself.

00:48:36.590 --> 00:48:39.890
So to give you an example of what this is like,

00:48:39.889 --> 00:48:45.599
I've provided in a single slide a fully
multithreaded merged sort implementation.

00:48:45.599 --> 00:48:50.360
So this is the kind of thing where basically
-- it's a little busy, but it's pretty simple.

00:48:50.360 --> 00:48:55.809
So basically the NSOperation is allowing
you to add these operations to the queue.

00:48:55.809 --> 00:49:00.049
You specify which operations depend
on which other operations to finish.

00:49:00.050 --> 00:49:02.580
And then you just wait for the queue to be done executing.

00:49:02.579 --> 00:49:05.829
So there's a sorting operation, which
is a sub class of NSOperation.

00:49:05.829 --> 00:49:12.610
And actually in my code all it did was
call the queue sort function that's available

00:49:12.610 --> 00:49:14.970
in standard (Inaudible) so that's really simple.

00:49:14.969 --> 00:49:17.959
And then the merge operation is another subclass.

00:49:17.960 --> 00:49:22.289
It merges two continuous ranges together.

00:49:22.289 --> 00:49:26.739
I experimented with a couple different things and
finally just asked Google to give me an answer.

00:49:26.739 --> 00:49:29.389
So that was actually pretty simple to write.

00:49:29.389 --> 00:49:34.000
And then these dependencies they're
sort of towards the bottom.

00:49:34.000 --> 00:49:40.159
Let you specify that the merge operations need to wait for
the sorting operations to finish, and which ones to wait.

00:49:40.159 --> 00:49:43.399
And then the queue processes those.

00:49:43.400 --> 00:49:47.780
And then you can add a final one that merges
the two halves together into a single whole.

00:49:47.780 --> 00:49:51.140
Add that, and then you're done.

00:49:51.139 --> 00:49:58.879
So this is a very nice way of specifying -- you have certain
correspondences between threads going on in the background.

00:49:58.880 --> 00:50:06.170
And it's pretty straight forward to organize entire
clusters of workers together with the NSOperationQueue API.

00:50:06.170 --> 00:50:09.829
There's also some new API on the NSProcessInfo.

00:50:09.829 --> 00:50:12.670
So you can get the active processor count, physical memory.

00:50:12.670 --> 00:50:19.309
And then there's the sysctlbyname() function
that's also available at the lower level to query

00:50:19.309 --> 00:50:23.829
about what kind of machine are you running on,
what kind of resources are available to you.

00:50:23.829 --> 00:50:25.909
Some basic stuff.

00:50:25.909 --> 00:50:31.230
And finally, there's nothing quite like having a great reference.

00:50:31.230 --> 00:50:32.960
And I keep looking for them.

00:50:32.960 --> 00:50:34.809
There are a lot of API references about threading.

00:50:34.809 --> 00:50:40.019
It's pretty easy to get information about
P threads or other threading architectures.

00:50:40.019 --> 00:50:42.679
So I realized the first one is about Java.

00:50:42.679 --> 00:50:49.649
That said, the Java Concurrency in Practice book
is the best nuts-and-bolts engineering book

00:50:49.650 --> 00:50:51.869
that I've actually read about multithreading.

00:50:51.869 --> 00:50:57.179
So this book is actually more about how do I build
objects that are working in a multithreaded environment.

00:50:57.179 --> 00:51:03.759
And it's really -- it's a great book that talks about
how do I build up things that can be cancelled safely.

00:51:03.760 --> 00:51:08.240
Why are most UIs single threaded,
what's the reasoning behind that.

00:51:08.239 --> 00:51:10.939
How do I build up notification patterns.

00:51:10.940 --> 00:51:17.150
And it's really about object-oriented software
programming, engineering, in a multithreaded environment.

00:51:17.150 --> 00:51:21.010
So that's actually quite a stark difference
from a lot of multithreading books.

00:51:21.010 --> 00:51:21.860
And I recommend it highly.

00:51:21.860 --> 00:51:23.340
It's only about 30 bucks.

00:51:23.340 --> 00:51:28.140
The second book is sort of in contrast to that.

00:51:28.139 --> 00:51:31.230
It's more academic text, it's not very long.

00:51:31.230 --> 00:51:33.159
And it's again, also about $30.

00:51:33.159 --> 00:51:38.449
And it's more about design aspects
of large multithreading systems.

00:51:38.449 --> 00:51:45.039
So the authors are trying to do sort of a similar book
compared to, like, the design patterns Gang of Four book.

00:51:45.039 --> 00:51:48.820
And finally, for Mac OS X specific information,

00:51:48.820 --> 00:51:52.840
there's the multithreading programming
topics offered by ADC downloadable PDF.

00:51:52.840 --> 00:51:55.400
And it provides a great review of a lot

00:51:55.400 --> 00:52:02.340
of different multithreading architectures
available as Mac OS X specific technologies.

00:52:02.340 --> 00:52:07.420
Finally, Core Data is trying --
is going to be trying to use some

00:52:07.420 --> 00:52:10.650
of those extra cores for you on your behalf automatically.

00:52:10.650 --> 00:52:14.010
Whenever we can infer that there's
enough work to make that worth doing.

00:52:14.010 --> 00:52:19.160
So if you have a large fetch operation, you're
fetching thousands of objects, we're going to do that.

00:52:19.159 --> 00:52:24.599
We do some extra work when you're adding stores, some
of the versioning stuff we can do in the background.

00:52:24.599 --> 00:52:29.610
If you're tearing down large stacks, again, we try to
leverage the fact that the attach threads are optional.

00:52:29.610 --> 00:52:33.519
So if you're tearing down a stack and opening
up a new one, that happens in the background.

00:52:33.519 --> 00:52:37.860
But if you're tearing down the stack and
you quit the app it -- it's basically no op.

00:52:37.860 --> 00:52:44.910
And the background fetching, if you have a large
enough fetch request, we're going to do this for you.

00:52:44.909 --> 00:52:48.649
So you mostly want to focus on responsiveness in the UI.

00:52:48.650 --> 00:52:52.180
And you don't need to do that for out
of -- sort of absolute performance.

00:52:52.179 --> 00:52:54.859
But the out-of-box behavior will scale quite well.

00:52:54.860 --> 00:53:02.059
So I'm going give you a quick overview of some of
the tools on Leopard to explore the performance

00:53:02.059 --> 00:53:06.969
in a Core Data example with the GoFetch.

00:53:06.969 --> 00:53:11.349
Sorry. First I'm going take a look at this slide.

00:53:11.349 --> 00:53:16.219
So here's a basic example of Core Data is working.

00:53:16.219 --> 00:53:20.929
And in the middle we're showing you that being
able to use those extra cores is quite an advantage

00:53:20.929 --> 00:53:24.000
over just doing SQLite work yourself by hand.

00:53:24.000 --> 00:53:29.519
And as a reference, you can see that there is a
malloc time to malloc the amount of memory we're using,

00:53:29.519 --> 00:53:36.670
and vm_allocate so you can also see that memory
utilization is going to be a bottleneck there.

00:53:36.670 --> 00:53:40.139
The memory bus is something of a limiting factor.

00:53:40.139 --> 00:53:48.349
So Core Data gets down to about the same order
of magnitude as allocating the memory itself.

00:53:48.349 --> 00:53:55.449
And on -- on a mid range quad core machine we're
going to be fetching 800,000 objects per second.

00:53:55.449 --> 00:53:57.649
You can save about 20,000.

00:53:57.650 --> 00:54:02.220
And if you're fetching just the object IDs, you just want
to identify the objects that match a particular query,

00:54:02.219 --> 00:54:04.849
you're looking at about 3 million object IDs per second.

00:54:04.849 --> 00:54:07.349
You can pull back.

00:54:07.349 --> 00:54:11.380
So like I mentioned, that memory
bus is a real limiting factor.

00:54:11.380 --> 00:54:14.420
Particularly if you have an 8-way box.

00:54:14.420 --> 00:54:16.920
The memory bus is not even running at a gigahertz.

00:54:16.920 --> 00:54:22.000
So you want to keep as much data as possible on
disk, and you want to be focused on your working set.

00:54:22.000 --> 00:54:27.619
So even if you cache the memory, cache the data
in memory, you'll still have an issue if you try

00:54:27.619 --> 00:54:29.539
to have a lot of threads going at the same time.

00:54:29.539 --> 00:54:31.400
They're contending over that memory bus.

00:54:31.400 --> 00:54:35.170
So you want to focus on specific working
sets at a time, and you want to be kind

00:54:35.170 --> 00:54:39.039
to those UI widgets, and not overload them.

00:54:39.039 --> 00:54:46.449
Now we're going to take a look.

00:54:46.449 --> 00:54:55.789
So one of the things we have here is the new
Xray app, which offers a very nice collection

00:54:55.789 --> 00:55:00.000
of instruments in sort of a GarageBand style.

00:55:00.000 --> 00:55:06.079
And Core Data has some static probes built into the
framework to, as you might have guessed, monitor fetching,

00:55:06.079 --> 00:55:12.400
faulting -- and when a fault actually has to
go to disk because this isn't cached in memory.

00:55:12.400 --> 00:55:16.619
And then we have a default I/O tool.

00:55:16.619 --> 00:55:55.179
So if we launch an executable -- and we do a fetch.

00:55:55.179 --> 00:55:59.009
We come back.

00:55:59.010 --> 00:56:01.630
Change the scale here.

00:56:01.630 --> 00:56:08.789
So we can see we did some I/O here, and
take a look, we're basically loading

00:56:08.789 --> 00:56:11.219
up some of the frameworks towards the beginning.

00:56:12.250 --> 00:56:20.650
Here we have -- bring up the detailed
view -- so we have a fetch.

00:56:20.650 --> 00:56:25.410
And in here we can see we fetched against the person entity.

00:56:25.409 --> 00:56:29.679
And at the end we returned 9611 results.

00:56:29.679 --> 00:56:32.859
In a duration about 0.4 seconds.

00:56:32.860 --> 00:56:34.260
And we had a stack trace for us here.

00:56:34.260 --> 00:56:44.040
And if we bump up the scale a little bit more we can see
that a bunch of work started happening after the fetch

00:56:44.039 --> 00:56:50.259
where Managed Objects got instantiated and we
can even see which objects got instantiated.

00:56:50.260 --> 00:56:55.130
If it will cooperate with me.

00:56:55.130 --> 00:57:01.849
So we basically started faulting in both people and
icons to get the view that we showed you earlier.

00:57:01.849 --> 00:57:04.920
Just fetching and the little icon
that one associated with it.

00:57:04.920 --> 00:57:09.849
And in the cache missing, we can see that
the fetch request only fetched the people,

00:57:09.849 --> 00:57:11.940
and we didn't fetch any of the icons.

00:57:11.940 --> 00:57:21.059
So the table view then, here in the stack trace, comes down
and starts asking for the table data for that icon column.

00:57:21.059 --> 00:57:24.309
And then asks the Managed Objects that are all the people.

00:57:24.309 --> 00:57:26.320
And then the people come through.

00:57:26.320 --> 00:57:28.440
Value for key. You can see that there's some faulting.

00:57:28.440 --> 00:57:32.190
And then we have this dtrace method here.

00:57:32.190 --> 00:57:34.420
That registers a static probe.

00:57:34.420 --> 00:57:39.480
And you can see then we go back
to disk with the read-write tool.

00:57:39.480 --> 00:57:47.750
And there actually is some logging, apparently,
as we can see from the stack trace.

00:57:47.750 --> 00:57:56.679
And there's some faulting going on here, and you
can see that we're reading from the database.

00:57:56.679 --> 00:57:58.519
So you can add more tools.

00:57:58.519 --> 00:58:13.829
Like the ObjectAlloc tool.

00:58:13.829 --> 00:58:19.650
And -- and basically you can start seeing what kind of
objects get allocated and where those allocations happen.

00:58:19.650 --> 00:58:30.619
So you can see some of the places where these get fetched.

00:58:30.619 --> 00:58:31.489
Stuff like this.

00:58:31.489 --> 00:58:33.759
So

00:58:33.760 --> 00:58:35.700
this is an Xray tool there's a whole session on this.

00:58:35.699 --> 00:58:40.429
And I just wanted to give you an overview --
that you can use the custom instruments for.

00:58:44.590 --> 00:58:50.160
And if you give me one second, I'm afraid that
it took a little bit longer to get the demo set

00:58:50.159 --> 00:58:57.379
up in between sessions than we had hoped.

00:59:03.489 --> 00:59:12.579
So -- there we go.

00:59:12.579 --> 00:59:17.469
So we have two short examples.

00:59:17.469 --> 00:59:22.399
And one of the things that the GoFetch example
does is, it's fetching all these people,

00:59:22.400 --> 00:59:26.760
and it issues a fetch request which captures the
data -- but I wanted to show you something in Shark.

00:59:26.760 --> 00:59:31.280
And so I wrote a little tool that basically
asked for each of those objects one by one.

00:59:31.280 --> 00:59:33.980
So there's a lot of faulting going on in here.

00:59:33.980 --> 00:59:42.380
And one of the things that is nice to notice
here in Shark is that there's both a time sample

00:59:42.380 --> 00:59:45.490
and there's a time sample with all thread states.

00:59:45.489 --> 00:59:51.369
And if we bring up -- the Mini Config Editor.

00:59:51.369 --> 00:59:52.670
You can see there's a pop up.

00:59:52.670 --> 00:59:56.019
When you take Shark samples and
it says all thread states option.

00:59:56.019 --> 01:00:00.050
And what that's doing is it's going to record
events even had the thread is blocked on I/O,

01:00:00.050 --> 01:00:02.700
it's waiting on a lock, or some other reason.

01:00:02.699 --> 01:00:06.629
And this regular time sample is only
charging when the CPU is actually in use.

01:00:06.630 --> 01:00:07.099
So we can see

01:00:07.099 --> 01:00:16.130
there's some differences here when we do this fetch
and we fault in all these objects individually.

01:00:16.130 --> 01:00:22.530
And we can see in all thread states we spend an awful
lot of time in this system called here, Fcntl,

01:00:22.530 --> 01:00:24.960
which turns out to actually be file locking.

01:00:24.960 --> 01:00:29.970
And here in the regular time sample,
it's not up towards the top.

01:00:29.969 --> 01:00:41.049
So if we look in the tree view -- don't click
too much -- we see that in main, in value for key,

01:00:41.050 --> 01:00:52.810
which I was using to trip all those faults, we spent
about 1.6 seconds in the regular time sample in Shark.

01:00:52.809 --> 01:00:56.659
But in all thread states example, it recorded 2.6 seconds.

01:00:56.659 --> 01:01:01.849
And so the difference is how much time you were waiting
on I/O. So I just want to keep that in mind as you work

01:01:01.849 --> 01:01:06.539
with the performance tools that you have a lot of
different options with both the new Xray tool in Leopard,

01:01:06.539 --> 01:01:12.099
as well as in Shark, that you can record both the CPU time
and where you're spending time in your application code.

01:01:12.099 --> 01:01:16.539
But also where you're blocked on the system,
waiting for certain resources to become available.

01:01:16.539 --> 01:01:23.059
If we can cut back to slides.

01:01:23.059 --> 01:01:26.170
Great. So just a recap.

01:01:26.170 --> 01:01:31.000
The user default for SQL debugging is available now.

01:01:31.000 --> 01:01:32.789
You can use that on both Tiger and Leopard.

01:01:32.789 --> 01:01:37.360
And that is just logging the SQL. You can also use Xray.

01:01:37.360 --> 01:01:39.030
And we have instruments to find faulting.

01:01:39.030 --> 01:01:43.080
There are a couple of other instruments for
saving and for faulting too many relationships

01:01:43.079 --> 01:01:45.539
that I didn't show you -- that are available.

01:01:45.539 --> 01:01:51.039
And you can also find disk I/O. And you can use
that garage venue to correlate certain events.

01:01:51.039 --> 01:01:56.969
Both with the stack traces, but also when an event
like, say, when a fault goes off and trips file I/O,

01:01:56.969 --> 01:02:00.469
you can see that, and you can correlate
those two event last together.

01:02:00.469 --> 01:02:02.609
Which is something that's quite difficult to do in Shark.

01:02:02.610 --> 01:02:06.840
So these two tools are providing you
different views of the performance data.

01:02:06.840 --> 01:02:08.450
And Shark is providing you a sampling view.

01:02:08.449 --> 01:02:12.879
So it doesn't tell you how many events occurred, but it
provides a very nice aggregation of all the information

01:02:12.880 --> 01:02:18.079
about where you're spending time or where
your threads are blocked and waiting.

01:02:18.079 --> 01:02:21.049
And now we're going to wrap up.

01:02:21.050 --> 01:02:23.600
More information, there's a lot
of documentation available for us.

01:02:23.599 --> 01:02:27.610
There's a Core Data Programming Guide which
keeps getting expanded with new sections

01:02:27.610 --> 01:02:30.519
as people file requests for enhancements.

01:02:30.519 --> 01:02:35.000
There's an Atomic Store Programming
Guide for Leopard, NSPersistentDocuments,

01:02:35.000 --> 01:02:39.639
Low-level tutorials, and all that good stuff.

01:02:39.639 --> 01:02:45.460
Some related sessions, so after lunch at 2 p.m. in this room
there will be an entire session devoted to our new feature

01:02:45.460 --> 01:02:49.309
on schema versioning and migration
that Miguel and Ron will be doing.

01:02:49.309 --> 01:02:54.360
And then later on there will be Getting Started with
Cocoa Bindings session, for those of you who want to brush

01:02:54.360 --> 01:02:58.650
up on Cocoa bindings and working with Interface
Builder that Malcolm Crawford will be giving.

01:02:58.650 --> 01:03:05.260
And finally, there will be the Core Data Lab tomorrow
at 3:30 in the Mac OS X Lab where you're welcome

01:03:05.260 --> 01:03:07.700
to bring either problems or ask us questions.

01:03:07.699 --> 01:03:09.039
The whole team will be there.

01:03:09.039 --> 01:03:11.969
And we look forward to meeting with you.

01:03:11.969 --> 01:03:19.079
And Matt Formica is the Cocoa evangelist,
Cocoa Dev mailing lists -- the usual suspects.