WEBVTT

00:00:16.339 --> 00:00:17.690
>> Jeff: So I am Jeff Moore.

00:00:17.690 --> 00:00:21.510
I am software engineer with the Core Audio Group.

00:00:21.510 --> 00:00:25.040
( Applause )

00:00:25.039 --> 00:00:27.099
>> Jeff: This is, in fact, my presentation.

00:00:27.100 --> 00:00:36.210
I'm here to introduce you to the Core Audio architecture and
some of the general concepts that have gone into its design.

00:00:36.210 --> 00:00:43.079
In particular, I want to focus in on the property mechanism
that's used throughout most of the Core Audio APIs as well

00:00:43.079 --> 00:00:47.390
as to talk a little bit about a concept
that we call format agnosticism,

00:00:47.390 --> 00:00:52.240
which is another thing that runs throughout our APIs.

00:00:52.240 --> 00:00:59.789
After that, I want to introduce you to some specific
Core Audio APIs by going over some simple usage cases,

00:00:59.789 --> 00:01:07.000
including converting audio from one format to another,
reading and writing audio from a file, playing an effect,

00:01:07.000 --> 00:01:12.000
applying an effect to some data,
and playing audio data to hardware.

00:01:12.000 --> 00:01:21.810
Now for those of you that have taken a look at our API set,
you've noticed that there are an awful lot of headers spread

00:01:21.810 --> 00:01:26.359
across several frameworks, but there
is some method there to that madness.

00:01:26.359 --> 00:01:34.739
Now each individual Core Audio API is very focused
on the specific task that it was designed to handle.

00:01:34.739 --> 00:01:41.819
It doesn't, it tries to not veer outside of the lines,
so to speak, as far as dealing with its own task.

00:01:41.819 --> 00:01:50.349
But even though they are focused, all the APIs are also
cooperative in that they were designed to work well

00:01:50.349 --> 00:02:02.329
with each other so that you can basically pick and choose
amongst the APIs to accomplish what you need to get done.

00:02:02.329 --> 00:02:06.670
Now the API set is also a very layered architecture.

00:02:06.670 --> 00:02:12.150
We kind of break it down into three layers.

00:02:12.150 --> 00:02:20.310
At the lowest layer we have the APIs that deal primarily
with hardware, such as the I/O Audio driver family,

00:02:20.310 --> 00:02:27.360
the audio how for talking to those
drivers, core midi and a few others.

00:02:27.360 --> 00:02:35.000
And then we have the middle tier of our APIs and that's
where you're going to find all the workhorse APIs

00:02:35.000 --> 00:02:39.669
in the Core Audio set and that includes
things such as the audio converter,

00:02:39.669 --> 00:02:44.629
the audio file API, audio units, AU graph, etcetera.

00:02:44.629 --> 00:02:49.620
Most of our APIs are going to fall
into this middle ware layer.

00:02:49.620 --> 00:02:52.629
And finally we have a few high level APIs.

00:02:52.629 --> 00:03:02.289
And when we talk about high level APIs, basically we just
mean an API that combines some of the lower level APIs

00:03:02.289 --> 00:03:06.489
to synthesize a solution to a particular problem.

00:03:06.490 --> 00:03:13.510
And APIs like that are the extended audio
file, Open AL and the audio queue API.

00:03:13.509 --> 00:03:18.039
( Windows log off music followed by laughter.

00:03:18.039 --> 00:03:19.530
>> Somebody rebooting Windows.

00:03:19.530 --> 00:03:21.879
( Laughter )

00:03:21.879 --> 00:03:29.979
Now, in the Core Audio API set, we do have this
concept called properties and the property mechanism

00:03:29.979 --> 00:03:36.889
that we have is kind of a unification device,
because properties occur throughout our API set.

00:03:36.889 --> 00:03:45.159
What a property is is basically a way to
access and manipulate some particular piece

00:03:45.159 --> 00:03:48.340
of the state for a particular API object.

00:03:48.340 --> 00:03:56.920
Now because the API objects more than likely have many, many
properties, each property has to have an address so that you

00:03:56.919 --> 00:03:59.349
and the object know what you're talking about.

00:03:59.349 --> 00:04:05.359
Now most of the APIs use just a simple
32 bit integer selector for this address,

00:04:05.360 --> 00:04:13.340
although several of the APIs further qualify the
address using scope and element selectors as well.

00:04:13.340 --> 00:04:20.730
Now the value of a property can be pretty much
any kind of data that the API has a need for.

00:04:20.730 --> 00:04:28.490
In fact, in the APIs property values are actually
represented as a block of memory and a size

00:04:28.490 --> 00:04:32.509
that says how much space that that memory is occupying.

00:04:32.509 --> 00:04:40.569
Now, some of the APIs also provide what we call a listener
mechanism and that's just a way to install a callback

00:04:40.569 --> 00:04:46.199
that you can get called whenever the
value of a particular property changes.

00:04:46.199 --> 00:04:51.779
Now as I said, the property mechanism is a bit
of a unification device in our various APIs.

00:04:51.779 --> 00:04:59.299
And as such, you're going to run into several common
routines in pretty much all the APIs that use properties.

00:04:59.300 --> 00:05:04.240
Now the first routine that you're going to
run into is the get property info routine.

00:05:04.240 --> 00:05:12.699
The get property info routine is used to discover whether or
not a property at that address exists, how big its value is,

00:05:12.699 --> 00:05:16.599
and whether or not you can actually
change the value of that property.

00:05:16.600 --> 00:05:22.740
Now the next pair of functions you're going to run into
quite a bit is the get property and set property functions.

00:05:22.740 --> 00:05:30.129
And as their name implies, these are your basic accesser
methods for getting and setting the value of a property.

00:05:30.129 --> 00:05:35.680
Now in the APIs that support listeners,
you will also see the add property listener

00:05:35.680 --> 00:05:37.970
and remove property listener functions.

00:05:37.970 --> 00:05:42.960
And those are there specifically to allow you to
install your callback so that it can get called

00:05:42.959 --> 00:05:46.629
when a property you're interested in changes value.

00:05:48.160 --> 00:05:52.920
Now I'd like to talk a little bit about
another concept that we have in the design

00:05:52.920 --> 00:05:56.069
of Core Audio and that's formatting agnosticism.

00:05:56.069 --> 00:06:03.079
Now by format agnosticism, all we really mean is that
you write your code in such a way that you're prepared

00:06:03.079 --> 00:06:10.189
to handle pretty much any kind of data that the system
might throw at you, even data formats that aren't

00:06:10.189 --> 00:06:13.259
on the system when you were writing that code.

00:06:13.259 --> 00:06:20.000
And all of the Core Audio APIs are
designed with format agnosticism in mind.

00:06:20.000 --> 00:06:23.600
They will go out of their way to
help you out in this regard.

00:06:23.600 --> 00:06:30.720
And the reason why you want to do all of this is because,
as I said, you really want to be able to deal with any kind

00:06:30.720 --> 00:06:33.170
of data your users might want to throw at you.

00:06:33.170 --> 00:06:38.310
So it's best to kind of keep this in
mind when you're writing your code.

00:06:38.310 --> 00:06:47.500
Now even though we try to be agnostic about what formats
we're using, the Core Audio APIs do designate linear PCM

00:06:47.500 --> 00:06:52.459
with 32 bit floating point samples
as the canonical sample format.

00:06:52.459 --> 00:06:56.139
Now the canonical sample format's job is really two fold.

00:06:56.139 --> 00:07:03.209
First, it's going to be the default format you get in
absence of any other specified preference for a format.

00:07:03.209 --> 00:07:06.339
In other words, whenever you bring up a new audio unit,

00:07:06.339 --> 00:07:10.149
you're going to get the canonical
sample format, for example.

00:07:12.269 --> 00:07:23.199
The other big job that the canonical sample format provides
is to be the intermediate format when doing conversions.

00:07:23.199 --> 00:07:32.449
In other words, if you're trying to transform data in format
A into format B and there isn't a direct transformation,

00:07:32.449 --> 00:07:39.909
the transformation is going to go from format A into
the canonical format and then from the canonical format

00:07:39.910 --> 00:07:46.689
into format B. The canonical format
is always chosen to be big enough

00:07:46.689 --> 00:07:51.410
to hold pretty much any kind of
data without losing any precision.

00:07:52.470 --> 00:07:58.670
Now as I said, format agnosticism tries to
know as little about the data as possible,

00:07:58.670 --> 00:08:02.879
but there is still a small amount of
information that you're going to need to know

00:08:02.879 --> 00:08:06.719
in order to properly handle formatted data.

00:08:06.720 --> 00:08:13.260
And in the Core Audio APIs we've kind of
boiled down a good portion of this information

00:08:13.259 --> 00:08:17.039
into the structure called the audio
stream basic description.

00:08:17.040 --> 00:08:23.189
Now audio stream basic description is a bit of a
mouthful and quite often you're going to see written

00:08:23.189 --> 00:08:28.540
and in my talk you're going to
hear me use the abbreviation ASBD

00:08:28.540 --> 00:08:33.830
and I really just mean an audio stream
basic description when I say that.

00:08:33.830 --> 00:08:41.530
Now here you can see the declaration of an audio stream
basic description and there are quite a few fields in here.

00:08:41.529 --> 00:08:45.769
Some of their usage should be obvious,
such as the sample rate field.

00:08:45.769 --> 00:08:48.740
But some of them are a little less obvious.

00:08:48.740 --> 00:08:55.500
Now the format ID and the format flags field are
there so that you can identify the general category

00:08:55.500 --> 00:09:03.539
of audio data you're dealing with, such as
AAC or Linear PCM or MP3 or what have you.

00:09:03.539 --> 00:09:08.289
The other fields that should be pretty
obvious are like the channels per frame field.

00:09:08.289 --> 00:09:12.740
That just basically tells you how many
channels of audio are in this stream of data.

00:09:12.740 --> 00:09:18.049
And then you have the bits per channel field,
the bytes per frame fields, the frames per packet

00:09:18.049 --> 00:09:21.769
and bytes per packet field, and
these are just fields that are used

00:09:21.769 --> 00:09:27.879
to describe how the audio data is
broken down in the stream in memory.

00:09:27.879 --> 00:09:37.200
Now there are a lot of fields in an
ASBD and not every format actually needs

00:09:37.200 --> 00:09:40.640
to use all those fields in order to describe itself.

00:09:40.639 --> 00:09:49.649
Now in such a case, you will find that the fields that are
unknown or that are not applicable will be set to zero.

00:09:49.649 --> 00:09:53.189
Now before we move on and look at a
few audio stream basic descriptions,

00:09:53.190 --> 00:09:58.490
I just want to take a few minutes
to talk about some terminology.

00:09:58.490 --> 00:10:03.070
Now the words sample frame and packet
are thrown about in computer science

00:10:03.070 --> 00:10:07.610
and signal processing literature and
are very terribly overloaded terms.

00:10:07.610 --> 00:10:12.050
But when we're talking about the core
audio APIs, we mean something very,

00:10:12.049 --> 00:10:18.079
very specific when we say sample frame and packet.

00:10:18.080 --> 00:10:25.009
A sample for us is a single data point
for one channel in an audio stream.

00:10:25.009 --> 00:10:32.919
And then a frame is a collection of samples that
are all coincident at the same instant in time.

00:10:32.919 --> 00:10:40.399
Now one interesting aspect of a frame is that when
you're talking about relating a data format to time,

00:10:40.399 --> 00:10:48.299
the frame is really the smallest unit at which
you could measure time accurately with audio data.

00:10:48.299 --> 00:10:56.029
Now the final term I want to talk a
little bit about is a very important term.

00:10:56.029 --> 00:11:00.069
And it's important enough that we're going to
talk a lot more about it here in a few minutes.

00:11:00.070 --> 00:11:04.280
But I just wanted to say that a
packet is, at it's simplest terms,

00:11:04.279 --> 00:11:07.839
just a collection of frames that
go together for some purpose.

00:11:07.840 --> 00:11:12.450
Now we're, like I said, we're going to talk a
little bit more about packets in a few moments.

00:11:13.809 --> 00:11:21.969
Now here you see an audio stream basic description
structure filled out to represent a four channel stream

00:11:21.970 --> 00:11:26.450
in the canonical sample format at a 44.1 sample rate.

00:11:26.450 --> 00:11:32.430
And as you can go through, you can see each of the
fields are filled out to represent what that format is.

00:11:32.429 --> 00:11:39.009
The sample rate is 44.1, the format ID and
format flags are specifically set for linear PCM

00:11:39.009 --> 00:11:46.080
and the canonical sample format tags, which
are, as we recall, 32 bit floating2 point.

00:11:46.080 --> 00:11:49.340
And then you have the bits per channel, which is 32.

00:11:49.340 --> 00:11:51.879
And the channels per frame is four.

00:11:51.879 --> 00:11:59.269
Now when it comes to the bytes per frame, as I said,
that's a collection of all the samples in that time,

00:11:59.269 --> 00:12:05.759
so there are four channels, so that's
going to be four times four, which is 16.

00:12:05.759 --> 00:12:11.470
Now one interesting aspect I want to point out
about linear PCM is that the number of frames

00:12:11.470 --> 00:12:15.950
in a packet of linear PCM is always going to be one.

00:12:15.950 --> 00:12:22.320
Now that seems kind of arbitrary, but it'll make a
little bit more sense when we talk about packets later.

00:12:22.320 --> 00:12:32.640
Now here you see an audio stream basic description filled
out for a slightly more complicated linear PCM format.

00:12:32.639 --> 00:12:42.360
Now this one has 24 bit samples, but they're carried within
a 32 bit word and 2you have two channels of them at 96K.

00:12:42.360 --> 00:12:46.550
Now you can go through this and you
can see how all the fields are filled

00:12:46.549 --> 00:12:51.479
out to represent pretty much exactly
what I said in the English here.

00:12:51.480 --> 00:12:56.570
But I do want to point out a couple of
interesting things about this format.

00:12:56.570 --> 00:13:04.720
Because you have a format that is 24 valid bits represented
in a 32 bit word, 2you can see some little discrepancies

00:13:04.720 --> 00:13:09.450
between this ASBD and the previous ASBD I showed you.

00:13:09.450 --> 00:13:15.980
In particular, you see that the bits per channel is
set to 24, but the bytes per frame is actually eight.

00:13:15.980 --> 00:13:27.529
Now this is because each sample is actually 32 bits
wide, but only 24 of those bits are actually valid,

00:13:27.529 --> 00:13:33.980
and so that comes out to 4 bytes per sample,
times two, which is eight bytes per frame.

00:13:33.980 --> 00:13:44.289
Now the final ASBD I wanted to show you is
one here for AAC at a stereo AAC stream.

00:13:44.289 --> 00:13:51.230
Now AAC is one of those formats that doesn't require
all the fields in an ASBD in order to describe itself.

00:13:51.230 --> 00:13:57.860
So as you can see, several of the fields
here are set to zero to indicate this.

00:13:57.860 --> 00:14:04.919
In fact, the only fields that are really interesting
to talk about width with AAC are the number of channels

00:14:04.919 --> 00:14:08.559
in the channels per frame field
and the frames per packet field,

00:14:08.559 --> 00:14:15.489
which tells you how much to expect
in each packet of AAC data.

00:14:15.490 --> 00:14:23.960
Now in addition to audio stream basic descriptions,
some formats also require what we call a magic cookie.

00:14:23.960 --> 00:14:30.009
Now a magic cookie is just a little blob of data
that provides some extra out of band information

00:14:30.009 --> 00:14:34.549
about the specific instance of the
format of the data in the stream.

00:14:34.549 --> 00:14:41.709
Now magic cookies, as I said, are very
particular to their stream of data so they have

00:14:41.710 --> 00:14:44.470
to be paired with the stream at all times.

00:14:44.470 --> 00:14:52.080
In fact, a magic cookie comes about as part of the encoding
process to create the stream of data and you absolutely have

00:14:52.080 --> 00:14:56.870
to pass the magic cookie along in
order to properly decode the data.

00:14:56.870 --> 00:15:02.600
Now as you might imagine, the magic cookie does
have some interesting bits of information in it.

00:15:02.600 --> 00:15:08.460
But the contents of the magic cookie should be considered
opaque and you should treat it like a black box.

00:15:08.460 --> 00:15:12.030
You shouldn't be dereferencing or
trying to parse a magic cookie.

00:15:12.029 --> 00:15:18.079
Instead you should use the APIs that Core Audio provides
that allow you to probe the contents of a magic cookie

00:15:18.080 --> 00:15:22.680
and extract much of the interesting bits of information.

00:15:22.679 --> 00:15:31.309
Now I mentioned, we talked a little bit earlier about
packets in passing, and at that point I defined a packet

00:15:31.309 --> 00:15:34.509
as a collection of frames that go together.

00:15:34.509 --> 00:15:40.350
I want to strengthen that definition
a little bit here and define a packet

00:15:40.350 --> 00:15:45.149
to be the smallest indivisible unit of a data format.

00:15:45.149 --> 00:15:52.879
That is to say you can't really break that data
format down into pieces smaller than a packet.

00:15:52.879 --> 00:15:59.500
Now from that definition you can pretty
much see why the number of frames per packet

00:15:59.500 --> 00:16:02.720
in a linear PCM format is always going to be one.

00:16:02.720 --> 00:16:09.310
Because that's the smaller unit at which
you can break a linear PCM stream up with.

00:16:09.309 --> 00:16:17.000
Now there are basically three kinds of packetizations
that you're going to run into with various data formats.

00:16:17.000 --> 00:16:19.750
Now the first is constant bit rate.

00:16:19.750 --> 00:16:26.620
Now in a constant bit rate format each packet is going
to have both the same number of frames and it's going

00:16:26.620 --> 00:16:30.730
to occupy the same number of bytes
in memory for each packet.

00:16:30.730 --> 00:16:35.509
Now the most common constant bit
rate packet format is linear PCM.

00:16:35.509 --> 00:16:42.629
You know, linear PCM you always knows how big a
packet's going to be by the definition of the format.

00:16:42.629 --> 00:16:48.659
Now the next packetization you're going to
run into is what we call variable bit rate.

00:16:48.659 --> 00:16:55.100
Now in a variable bit rate packetization, each
packet will have the same number of frames in it,

00:16:55.100 --> 00:17:00.730
but the number of bytes that the packet
will occupy will vary from packet to packet.

00:17:00.730 --> 00:17:08.849
In fact, the variable bit rate packetization scheme is
flexible enough and useful enough that it's used in most,

00:17:08.849 --> 00:17:21.139
if not all, of the modern codecs that we use on the system,
such as AAC, Apple Lossless, MPEG1 Layer 3, etcetera.

00:17:21.140 --> 00:17:26.980
Most of your common formats these days
are, in fact, variable bit rate in nature.

00:17:26.980 --> 00:17:32.579
Now the final packetization type I want
to talk about is variable frame rate.

00:17:32.579 --> 00:17:39.250
Now variable frame rate packetization has, each
packet in this format will have a different,

00:17:39.250 --> 00:17:45.420
potentially a different number of frames and it will
potentially occupy a different number of bytes in memory.

00:17:45.420 --> 00:17:55.450
Now this is a very generalized packet scheme and
the only commonly used codec that you're going to find

00:17:55.450 --> 00:17:59.880
that uses this style of packetization
is the Ogg Vorbis codec.

00:17:59.880 --> 00:18:12.030
Now in the Core Audio APIs we use the structure audio stream
packet description in order to identify a packet in memory.

00:18:12.029 --> 00:18:17.629
But it's very unusual to run into an audio
stream packet description on its own.

00:18:17.630 --> 00:18:24.520
More likely you're going to run into them in the
form of an array of packet descriptions that are used

00:18:24.519 --> 00:18:28.849
to describe how another buffer of
memory is broken up into packets.

00:18:28.849 --> 00:18:33.919
Now packet descriptions are absolutely
required when you're dealing

00:18:33.920 --> 00:18:37.289
with a variable bit rate format
or a variable frame rate format.

00:18:37.289 --> 00:18:44.500
And the reason should be obvious, when you see them in
memory there's no way to know where one packet begins

00:18:44.500 --> 00:18:50.789
and the next packet ends without having a packet
description that tells you this information.

00:18:50.789 --> 00:18:59.089
Now at the bottom of this slide you can see the declaration
of the audio stream packet description structure.

00:18:59.089 --> 00:19:05.069
It has a field that has a byte
offset for the start of the packet.

00:19:05.069 --> 00:19:09.379
It has a field to indicate how
many frames are in shit packet.

00:19:09.380 --> 00:19:16.610
Now this field is going to be set to zero except
for variable frame rate packetization formats,

00:19:16.609 --> 00:19:22.609
in which case they'll be set to the number
of frames that are in that particular packet.

00:19:22.609 --> 00:19:31.009
And finally the audio stream packet description also has
a field for how many bytes the packet occupies in memory.

00:19:31.009 --> 00:19:38.279
Now one interesting property that this definition of
a packet description gives you is that it allows you

00:19:38.279 --> 00:19:45.440
to describe a buffer of memory where the audio
packets might be intermingled with other data,

00:19:45.440 --> 00:19:53.930
such as ID3 tags or video frames or what have you.

00:19:53.930 --> 00:20:02.100
Now to wrap up the first part of this talk, I'd like to talk
a little bit about how you'll go about filling out an ASBD,

00:20:02.099 --> 00:20:05.240
because from what I've said so far,
you're probably looking to go wow.

00:20:05.240 --> 00:20:09.970
You kind of have to know something about the format
you're dealing with in order to fill out the ASBD.

00:20:09.970 --> 00:20:16.250
And in some cases that's true, but there are
some techniques you can use that can help you out

00:20:16.250 --> 00:20:21.470
and kind of lift some of the burden of filling the ASBD out.

00:20:23.369 --> 00:20:28.239
Now the first technique to use should be pretty obvious.

00:20:28.240 --> 00:20:32.440
Just use the ASBDs that the Core Audio APIs hand to you.

00:20:32.440 --> 00:20:39.870
Every Core Audio API uses an audio stream basic description
to describe the data that it wants and the data it produces.

00:20:39.869 --> 00:20:47.159
So you should feel free to just ask the APIs your
interacting with to fill out your ASBDs for you.

00:20:47.160 --> 00:20:51.160
Now in some cases you're still going
to have to fill them out by hand

00:20:51.160 --> 00:20:56.400
and the APIs you're using aren't
going to be able to help you much.

00:20:56.400 --> 00:21:05.410
In that case, we do have another API, the audio
format API, which can help you still fill out an ASBD,

00:21:05.410 --> 00:21:10.060
provided that you have some other
bits of information to help it out.

00:21:10.059 --> 00:21:18.859
Now the audio format API is just a collection of global
properties and its purpose is there to give you kind

00:21:18.859 --> 00:21:26.479
of a handle on what formats, in terms of encoders
and decoders are installed on the system,

00:21:26.480 --> 00:21:30.660
as well as other bits of information
about the encoders and decoders,

00:21:30.660 --> 00:21:36.230
such as the name of the format, and a bunch of other things.

00:21:36.230 --> 00:21:43.130
But there are also properties there that will help,
that will fill out an ASBD for you when all you happen

00:21:43.130 --> 00:21:49.430
to have is the format ID of the format and a magic cookie.

00:21:49.430 --> 00:21:56.960
Now the final technique I wanted to mention kind of in
passing, is to use the CA stream basic description class.

00:21:56.960 --> 00:22:00.809
Now the CA stream basic description
is part of the Core Audio SDK.

00:22:00.809 --> 00:22:09.839
As such, you'll find it in the developer
examples folder on your install right now.

00:22:09.839 --> 00:22:14.259
And even though it is C++ code, and
if you're uncomfortable with C++,

00:22:14.259 --> 00:22:21.049
I really advice you to take a look at the code anyway
and even if you're not going to use it directly,

00:22:21.049 --> 00:22:27.049
there's probably a lot of good information in that
code that you can make use of in your own code.

00:22:27.049 --> 00:22:33.159
Even to the point of just copying and pasting to
where ever you need it, as are all of the SDK codes.

00:22:33.160 --> 00:22:36.210
They're really there to help you,
even if they are in C++.

00:22:36.210 --> 00:22:43.610
I do encourage you to at least take a look
at them and be familiar with how they work.

00:22:43.609 --> 00:22:47.309
Wrong way.

00:22:47.309 --> 00:22:53.049
So that kind of wraps up the first part of
the talk where I've been kind of talking

00:22:53.049 --> 00:22:56.190
in generalities about the Core Audio APIs.

00:22:56.190 --> 00:23:05.170
Here for the second part of this talk, I'd like to move more
into some concrete examples of doing something and through

00:23:05.170 --> 00:23:11.150
that introduce you to several of the
more commonly used Core Audio APIs.

00:23:11.150 --> 00:23:18.230
Now the first task I want to talk about is
converting data from one format to another.

00:23:18.230 --> 00:23:26.690
And the Core Audio API that you're going to use
to do this is the aptly named audio converter API.

00:23:26.690 --> 00:23:33.009
In order to do a conversion, the first thing you
need to do is instantiate a new audio converter.

00:23:33.009 --> 00:23:40.410
Now in order to do that, all you do is call audio
converter new and you have to pass the input format

00:23:40.410 --> 00:23:43.870
and the output format to the audio converter new function.

00:23:43.869 --> 00:23:53.849
Now one thing to be aware of is that the one or both
of the input or output formats has to be linear PCM.

00:23:53.849 --> 00:24:03.569
In other words, you can use an encoder to go from an encoded
format to linear PCM or from linear PCM to an encoded format

00:24:03.569 --> 00:24:07.829
or convert between the various linear PCM formats.

00:24:07.829 --> 00:24:14.799
Now after you've instantiated your audio converter,
all the other settings that you need to make

00:24:14.799 --> 00:24:17.960
on the converter are handled by setting properties.

00:24:17.960 --> 00:24:23.259
Now some of the properties you're definitely going
to want to set include the magic cookie in case

00:24:23.259 --> 00:24:26.710
of your dealing with a format that requires them.

00:24:26.710 --> 00:24:34.840
And when you're doing an encoding, you're also going to want
to configure the encoder for such things as what quality

00:24:34.839 --> 00:24:41.679
of encoding, what bit rate to run the encoder at, etcetera.

00:24:41.680 --> 00:24:48.269
So when you have an audio converter, you
need to know how to get data out of it.

00:24:48.269 --> 00:24:56.930
And the audio converter API provides to function
that you're going to want to use to do this.

00:24:56.930 --> 00:25:02.210
Now the first function is a specialist
function, audio converter convert buffer.

00:25:02.210 --> 00:25:08.259
It specializes in doing linear PCM to
linear PCM transformations provided

00:25:08.259 --> 00:25:10.369
that they don't have any rate conversion.

00:25:10.369 --> 00:25:17.469
This is useful in the cases where you are dealing with low
level stuff and you need to massage the linear PCM format

00:25:17.470 --> 00:25:25.980
and you can just access directly the high
performance blisters in the audio converter.

00:25:25.980 --> 00:25:31.849
But aside from that one special case, the general
routine that you're going to want to use for getting data

00:25:31.849 --> 00:25:36.379
out of an audio converter is audio
converter fill complex buffer.

00:25:36.380 --> 00:25:42.350
This is the routine that can handle every
kind of data that the system supports.

00:25:42.349 --> 00:25:45.539
As such, along with the concept of format agnosticism,

00:25:45.539 --> 00:25:50.159
this is really the routine you're going
to want to organize your code around.

00:25:50.160 --> 00:25:55.310
So presuming you're using audio
converter fill complex buffer,

00:25:55.309 --> 00:25:58.579
you're going to need to write an input data callback.

00:25:58.579 --> 00:26:01.759
Now this is the function that the audio converter is going

00:26:01.759 --> 00:26:06.259
to call whenever it needs to get
some new input data from you.

00:26:06.259 --> 00:26:11.160
Now your input data callback has two basic jobs.

00:26:11.160 --> 00:26:18.529
The first job is to provide the data in
order for the input side of the conversion.

00:26:18.529 --> 00:26:24.200
Now the audio converter has been specifically
plumbed so that it minimizes the number

00:26:24.200 --> 00:26:29.000
of copies involved in doing a particular transformation.

00:26:29.000 --> 00:26:34.819
As such, your input data callback does not copy data.

00:26:34.819 --> 00:26:41.200
Instead it has to provide a pointer to a buffer
containing the input data you want converted.

00:26:41.200 --> 00:26:48.289
Now this buffer has to remain valid for the entire
duration of a call to audio converter fill complex buffer

00:26:48.289 --> 00:26:54.920
and further has to be valid until the next time
you call audio converter fill complex buffer.

00:26:54.920 --> 00:27:02.130
Now the other big job that you're input callback data has
to do is that it has to fill out the packet descriptions

00:27:02.130 --> 00:27:06.660
in the case where you're dealing with
a format of data that requires them.

00:27:06.660 --> 00:27:14.940
So you can get data in, you can get data out, of an
audio converter, what do you do when you're done?

00:27:14.940 --> 00:27:16.799
How do you even know when you're done?

00:27:16.799 --> 00:27:23.109
Well there are two cases that you really
ought to know about that signal the end

00:27:23.109 --> 00:27:26.019
of the stream in a bit of a different way.

00:27:26.019 --> 00:27:33.359
Now the first way is that in your input
callback, you return zero packets of data,

00:27:33.359 --> 00:27:37.389
but you also return no error from the function.

00:27:37.390 --> 00:27:44.210
Doing this will signify to the audio converter
that you are out of data and this is, in fact,

00:27:44.210 --> 00:27:50.140
the logical end of the stream and that the converter
should not expect you to give it any more data.

00:27:50.140 --> 00:27:58.350
Now the second mechanism that's used to indicate kind of an
end of stream situation is that if you return no data again,

00:27:58.349 --> 00:28:02.369
but that you return an error code
from your input data callback.

00:28:02.369 --> 00:28:08.879
Now by returning the error, you're indicating to the
converter that you don't have any input data to give it

00:28:08.880 --> 00:28:16.630
at this point and time but you are not actually at the
logical end of stream and so that the converter should keep

00:28:16.630 --> 00:28:25.140
on trying to get more data from you in the future.

00:28:25.140 --> 00:28:31.660
So once you've signified the end of stream
and you're done with the conversion,

00:28:31.660 --> 00:28:35.080
you still have a little bit more that you need to do.

00:28:35.079 --> 00:28:39.799
In the case where you're going to reuse
that audio converter for something,

00:28:39.799 --> 00:28:45.579
you have to make sure that you call audio converter
reset before you try to do a new conversion with it.

00:28:45.579 --> 00:28:54.429
Now audio converter reset basically discards all
the internally cached data in the audio converter

00:28:54.430 --> 00:28:58.360
and basically returns everything to its ground state.

00:28:58.359 --> 00:29:03.129
Now another situation where you're
going to call audio converter reset is

00:29:03.130 --> 00:29:06.120
if you're seeking around in a given data stream.

00:29:06.119 --> 00:29:11.369
Now the reason why you want to do this
even if you're decoding the same string is

00:29:11.369 --> 00:29:20.289
that most data formats have some kind of inter packet
dependency and by moving around in the packet stream,

00:29:20.289 --> 00:29:23.789
you're going to need to make sure that the codec is returned

00:29:23.789 --> 00:29:27.980
to its ground state before it tries
to decode anything further.

00:29:29.789 --> 00:29:38.289
Now when you're dealing with audio formats that does imply
audio codec's and I've kind of mentioned codec's in passing.

00:29:38.289 --> 00:29:44.190
Officially the audio codec's API,
they constitute the plug in mechanism

00:29:44.190 --> 00:29:49.000
for both the audio converter API and the audio format API.

00:29:49.000 --> 00:29:51.569
Now codecs come in two flavors.

00:29:51.569 --> 00:29:53.240
You have encoders.

00:29:53.240 --> 00:30:01.390
They take in linear PCM data and output their
encoded format and then you have decoders,

00:30:01.390 --> 00:30:07.030
which take in the encoded format
and spit out the linear PCM data.

00:30:07.029 --> 00:30:11.109
Now one of the other rules of an
audio codec is that it's expected

00:30:11.109 --> 00:30:16.299
to be the expert format on the data format it works with.

00:30:16.299 --> 00:30:27.990
And so the audio converter and the audio format API will
always defer questions about the formats to the audio codec.

00:30:27.990 --> 00:30:33.039
Now in the case where you have both an encoder
and a decoder for a given format present,

00:30:33.039 --> 00:30:37.859
the decoder is always consulted
prior to consulting any encoders.

00:30:39.049 --> 00:30:45.700
Now when you're looking at what kind of
audio data to do for a particular job,

00:30:45.700 --> 00:30:51.490
there are several factors you're going
to want to think about in the choosing.

00:30:51.490 --> 00:30:55.059
Now the first factor is bit rate.

00:30:55.059 --> 00:30:57.549
How much compression do you actually need?

00:30:57.549 --> 00:31:01.379
And this is going to be governed
by the specific application.

00:31:01.380 --> 00:31:04.570
Like the case of a telecommunications application,

00:31:04.569 --> 00:31:09.500
you may only have a certain allocated
amount of bandwidth that you can use.

00:31:09.500 --> 00:31:15.759
And in those cases you're going to need to
use a codec that can fit into that bandwidth.

00:31:15.759 --> 00:31:23.879
Now along with the amount of compression, that's always
a trade off in terms of quality, so you're going to want

00:31:23.880 --> 00:31:31.330
to balance the amount of compression that you need with
the audio signal quality that you get out of an encoder.

00:31:31.329 --> 00:31:36.339
Most of the time you're going to
be trading one for the other.

00:31:36.339 --> 00:31:42.049
Now another big factor in your decision is going
to be how much CPU does it cost to use this format?

00:31:42.049 --> 00:31:48.149
If you're encoding, you're going to want to look
at how much CPU does it take to encode the format.

00:31:48.150 --> 00:31:50.650
The same with decoding.

00:31:50.650 --> 00:31:55.660
And another factor that a lot of people
don't necessarily think about when they think

00:31:55.660 --> 00:31:58.970
about codecs is the latency in the codec.

00:31:58.970 --> 00:32:06.940
Now all of these modern codecs are based on all
sorts of great signal processing and cool math tricks.

00:32:06.940 --> 00:32:11.730
But all of these things have some inherent
amount of latency in the calculation.

00:32:11.730 --> 00:32:16.799
That is, you'll stick some data in
but you'll have some extra silence put

00:32:16.799 --> 00:32:20.339
out before you get your actual bits back out.

00:32:20.339 --> 00:32:28.720
Now latency becomes a really important thing for certain
classes of applications, again, such as telecommunications.

00:32:28.720 --> 00:32:32.120
And when you fall into that category you usually know it.

00:32:32.119 --> 00:32:40.189
And the Core Audio system does provide codec's that
have excellent support for low latency characteristics.

00:32:40.190 --> 00:32:45.779
Now, the final factor you're probably
going to look at is compatibility.

00:32:45.779 --> 00:32:50.980
You're going to deploy your app in potentially
across several platforms and you're going to want

00:32:50.980 --> 00:32:55.549
to make sure you pick a format that is
accessible and usable across all those formats.

00:32:55.549 --> 00:33:09.700
So here's a little table I made that kind of compares and
contrasts some of the more common formats on the system.

00:33:09.700 --> 00:33:13.100
Now the first format is 16 bit linear PCM.

00:33:13.099 --> 00:33:17.849
I put that up there kind of to
give you a baseline for comparison.

00:33:17.849 --> 00:33:27.369
And it's pretty much the uncompressed format of the data
and it doesn't actually incur any CPU cost to use it.

00:33:27.369 --> 00:33:31.479
Now the next format on the chart is Apple Lossless.

00:33:31.480 --> 00:33:39.700
Apple lossless, as it's name applies, doesn't
have any generation loss when you do the encoding.

00:33:39.700 --> 00:33:45.870
In other words, the bits that you encode are
exactly the bits you get back when you decode them.

00:33:45.869 --> 00:33:53.879
Now the cost for lossless is that it's somewhat more
expensive to code and decode, in terms of CPU cost,

00:33:53.880 --> 00:33:58.440
and it doesn't support particularly low bit rates either.

00:33:58.440 --> 00:34:05.600
Now Apple lossless is available across Mac OS
X and Windows, provided you have QuickTime.

00:34:05.599 --> 00:34:09.009
The next format I want to talk about is IMA.

00:34:09.010 --> 00:34:12.110
Now IMA is a fairly old codec standard.

00:34:12.110 --> 00:34:14.390
It's been with us for a long time.

00:34:14.389 --> 00:34:19.469
It basically provides a four to one
compression ratio and the quality,

00:34:19.469 --> 00:34:22.829
though while not lossless, is still pretty high.

00:34:22.829 --> 00:34:30.630
And the CPU cost of IMA is next to nothing,
it's basically a couple of multiplies and adds.

00:34:30.630 --> 00:34:39.090
And the other great thing about IMA is that it
has absolutely no latency in it's encoding format.

00:34:39.090 --> 00:34:45.289
Not the final three formats are
all variations of MPEG formats.

00:34:45.289 --> 00:34:47.309
The first one is M-PEG4,AAC.

00:34:47.309 --> 00:34:51.219
This is the format we use on the iTunes store.

00:34:51.219 --> 00:34:56.789
This is the format that we've been
promoting in the system for a long time now.

00:34:56.789 --> 00:35:05.679
Now much like these, all three of these
formats all can trade bit rate for quality.

00:35:06.760 --> 00:35:13.740
But it's interesting to know when you compared and
contrast these three codecs together you will get,

00:35:13.739 --> 00:35:19.079
they kind of fall in about that order in
terms of quality at the same bit rate.

00:35:19.079 --> 00:35:23.900
I kind of want to point out a little bit about M-PEG4,AAC-LD.

00:35:23.900 --> 00:35:26.490
This is a new codec in Leopard.

00:35:26.489 --> 00:35:35.189
You might have heard it mentioned yesterday in the keynote,
as iChat has delivered a significant quality improvement

00:35:35.190 --> 00:35:41.320
to its users because they've switched
over to using the AAC-LD codec.

00:35:41.320 --> 00:35:49.570
Now AAC-LD is a modified form of AAC and it has a slightly
worse quality at the same bit rate as regular AAC.

00:35:49.570 --> 00:35:58.010
But what you get for that slight trade off is a
significantly improved latency performance in the codec,

00:35:58.010 --> 00:36:03.010
which was a key feature for getting
iChat on board for using it.

00:36:03.010 --> 00:36:07.340
And the final entry in the table is MPEG1 layer 3, aka KA MP3.

00:36:07.340 --> 00:36:16.050
And I threw that up there because it pretty much everybody
has a file somewhere in MP3 and it's good to look at that

00:36:16.050 --> 00:36:23.300
because it's so common throughout the
various platforms you're likely to use.

00:36:23.300 --> 00:36:32.390
Now I'd like to move on to talking about
reading and writing audio data to a file.

00:36:32.389 --> 00:36:40.429
The aptly named audio file API is the API
basically you're going to use for doing all of that.

00:36:40.429 --> 00:36:48.849
The audio file API provides a set of global properties
that allow you to find out all the readable file types,

00:36:48.849 --> 00:36:56.159
all the writable file types, and then for each of those file
types, what kinds of data you can put in each one of them.

00:36:56.159 --> 00:37:02.429
These properties are kind of the audio file
equivalent to what the audio format API provides

00:37:02.429 --> 00:37:09.710
with the audio converter and
describing the actual data formats.

00:37:09.710 --> 00:37:17.670
When you specify a file to the audio file API,
you can do that either using a CF URL that points

00:37:17.670 --> 00:37:26.079
to a file system object, in other words a file
calling URL, or you can use a Carbon FS Ref.

00:37:26.079 --> 00:37:32.630
Now when you want to create a new file with the audio
file API, you need to have the audio file type ID.

00:37:32.630 --> 00:37:40.450
That basically says what kind of file you want to
create, such as AIFF or MPEG4 or what have you.

00:37:40.449 --> 00:37:45.409
You need to know the ASBD of the format of
the data you're going to put in the file.

00:37:45.409 --> 00:37:53.299
And then any other information you need to include
in the file such as magic cookies or meta data,

00:37:53.300 --> 00:37:57.340
such as regions or markers or what have you.

00:37:57.340 --> 00:38:04.340
All of that stuff is taken care of after creating
the file by setting properties on the file object.

00:38:06.210 --> 00:38:11.519
Now when you're accessing an existing
file, much like most file system APIs,

00:38:11.519 --> 00:38:15.780
you just simply open those APIs, or open those files.

00:38:15.780 --> 00:38:24.030
And then you can find out all the information about the
file by accessing the various properties, such as the ASBD

00:38:24.030 --> 00:38:32.010
and the magic cookie for the format, the channel layout for
a multi channel situation, the various packet descriptions

00:38:32.010 --> 00:38:40.210
for the data and other kinds of Meta data, markers,
regions, lyrics, album covers, what have you.

00:38:40.210 --> 00:38:47.800
Now one word of warning, when you're trying to
access the packet table of certain kinds of files,

00:38:47.800 --> 00:38:51.640
you're potentially going to have
a pretty large performance hit.

00:38:51.639 --> 00:39:00.489
The reason for this is that some formats of
data such as MPEG1 files and ADTS and AC-3 files,

00:39:00.489 --> 00:39:07.609
they don't have a packet table, per say, rather
that they have an implicit packet table is signaling

00:39:07.610 --> 00:39:12.910
in the stream itself so that in order to
discover the packet table, you have to go through

00:39:12.909 --> 00:39:16.139
and parse every single packet in the data stream.

00:39:16.139 --> 00:39:19.739
And as you can imagine, for even small files,

00:39:19.739 --> 00:39:24.369
that can be a significant performance hit
in order to calculate that information.

00:39:24.369 --> 00:39:34.269
As such, the audio file API tries to provide as much
estimations for the sorts of things you're going to want

00:39:34.269 --> 00:39:41.360
to access the packet table for, such as the upper
bound on the size of a packet in the stream,

00:39:41.360 --> 00:39:46.890
as well as an estimate of how long,
in terms of real time, the file takes.

00:39:46.889 --> 00:39:54.059
And you should use these alternatives if you can,
rather than necessarily incurring the overhead of having

00:39:54.059 --> 00:39:58.880
to parse the entire file to get the packet table.

00:39:58.880 --> 00:40:05.740
Now, in the audio file API, reading and
writing data work basically the same way.

00:40:05.739 --> 00:40:10.929
In fact, the API calls all have
basically the same set of arguments.

00:40:10.929 --> 00:40:16.179
And they all have the same characteristics
in terms of behavior.

00:40:16.179 --> 00:40:20.019
Both reading and writing are blocking calls.

00:40:20.019 --> 00:40:27.000
That means that when you make the audio file read or
audio file write call, that file is not going to return

00:40:27.000 --> 00:40:32.119
until either there's some kind of system
error, in which case that'll be reported

00:40:32.119 --> 00:40:36.099
to you, or the request you made is satisfied.

00:40:36.099 --> 00:40:40.670
So you should keep that in mind
when you're making these calls.

00:40:40.670 --> 00:40:47.220
Now another thing that you can do with these calls is you
can say whether or not you want the audio file API to read

00:40:47.219 --> 00:40:49.579
or write the data through the system cache.

00:40:49.579 --> 00:40:56.440
This is a performance optimization, because you
may or may not want to incur the extra overhead

00:40:56.440 --> 00:41:01.519
of sticking the data in the file
in the general system cache.

00:41:01.519 --> 00:41:09.759
Usually you don't want to do this in the case where
you're just reading a packet of data from the disk once,

00:41:09.760 --> 00:41:14.010
potentially maybe to play it to
the hardware or what have you.

00:41:14.010 --> 00:41:18.880
Now both the audio file read and
write calls come in two flavors.

00:41:18.880 --> 00:41:25.300
There's a form that works in terms of bytes and
there's another form that works in terms of packets.

00:41:25.300 --> 00:41:32.170
Now while generally you can do it in terms of bytes for
pretty much any format, we've been talking a little bit

00:41:32.170 --> 00:41:38.829
about format agnosticism and this is another place where
that creeps in to things, you're rally going to want to deal

00:41:38.829 --> 00:41:42.440
with these formats in terms of whole packets.

00:41:42.440 --> 00:41:46.940
It's just the only way to handle
variable bit rate data, for example.

00:41:46.940 --> 00:41:51.610
But generally speaking, if you're dealing with things
in terms of whole packets, it's just generally easier

00:41:51.610 --> 00:41:56.230
to compute things like durations and
to keep track of the data in memory.

00:41:56.230 --> 00:42:04.960
Now after you're done writing to the file, the
audio file API has an optimization routine.

00:42:04.960 --> 00:42:11.099
And you can call that routine on a file to
cause the file to be rewritten in such a way

00:42:11.099 --> 00:42:18.769
that it can improve the actually access
performance to the data in the file.

00:42:18.769 --> 00:42:29.039
Now the audio file API is all well and good,
it's fairly robust and has a lot of stuff in it,

00:42:29.039 --> 00:42:37.759
but we also have another API called the extended audio file
API and it unites an audio file with an audio converter.

00:42:37.760 --> 00:42:45.790
And this allows you to easily read and write
files in whatever format you want while being able

00:42:45.789 --> 00:42:50.250
to do all the book keeping an what
not in terms of linear PCM.

00:42:50.250 --> 00:42:56.469
This is a conceptual thing that can simplify
the code that you have to write in terms

00:42:56.469 --> 00:43:00.379
of handling all the various kinds of formats.

00:43:00.380 --> 00:43:07.380
Just like the audio file API, the extended audio
file API can read and write to existing files.

00:43:07.380 --> 00:43:09.300
It can create new files.

00:43:09.300 --> 00:43:17.030
In fact, creating an opening with the extended audio
file is very analogous to the regular audio file API

00:43:17.030 --> 00:43:23.530
and everything I've talked about so far
applies to the extended audio file as well.

00:43:23.530 --> 00:43:30.660
And once you've created or opened a file with the
extended audio file API, you have to also set a property

00:43:30.659 --> 00:43:36.309
after the fact that will tell the extended
audio file what data format you want to give it,

00:43:36.309 --> 00:43:43.210
or if you're reading what format you want it to give you.

00:43:43.210 --> 00:43:51.900
Now when you're using the extended audio file API,
because it deals with everything as if it were linear PCM,

00:43:51.900 --> 00:44:00.910
all the reading, writing, file positions, sizes, etcetera,
are always handled in terms of whole sample frames.

00:44:00.909 --> 00:44:08.259
And much like the regular audio file
API, the extended audio file reads block.

00:44:08.260 --> 00:44:11.290
But the rights come in two flavors.

00:44:11.289 --> 00:44:17.170
There's the blocking version, but there's also a non
blocking asynchronous version that's pretty useful

00:44:17.170 --> 00:44:20.210
in lots of debugging context.

00:44:20.210 --> 00:44:27.400
You can, for example, you can use the non blocking
form of extended audio file write from an I/O context,

00:44:27.400 --> 00:44:31.210
which is a place where timing of
everything is very critical.

00:44:31.210 --> 00:44:39.690
Now I'd like to talk a little bit about
adding an effect to some audio data.

00:44:39.690 --> 00:44:44.220
And when you do that in the context of
Core Audio, you're going to be talking

00:44:44.219 --> 00:44:48.119
about the audio units that we shit on the system.

00:44:48.119 --> 00:44:54.589
Now an audio unit is a plug-in for encapsulating
some kind of signal processing operation.

00:44:54.590 --> 00:45:02.370
Audio units can live on their own, but their
generally can be, assembled together in either manually

00:45:02.369 --> 00:45:12.079
or using the AU graph API to make a collection of them
that can perform more complex signal processing operations.

00:45:12.079 --> 00:45:18.440
Now when you're dealing with I/O in an audio unit, all
the I/O has always done in a canonical sample format,

00:45:18.440 --> 00:45:23.070
it can be at any sample rate, but it's always
going to be at the canonical sample format.

00:45:23.070 --> 00:45:25.090
There are a couple of exceptions.

00:45:25.090 --> 00:45:31.769
Converter audio units and output audio units can be a
little more flexible with the kind of data they can take in,

00:45:31.769 --> 00:45:36.099
and we'll talk a little more about that in a minute.

00:45:36.099 --> 00:45:43.190
Audio units also can provide one or more GUI
components that your application can show to the user

00:45:43.190 --> 00:45:49.220
so that the user can interact with the audio
unit and can shape the sound that it creates.

00:45:50.550 --> 00:45:58.100
And in addition to properties, audio
units also make use of parameters.

00:45:58.099 --> 00:46:02.809
Now a parameter differs from a
property in a couple of key ways.

00:46:02.809 --> 00:46:11.699
First, parameters only apply to things that modify
the real time aspect of the signal processing.

00:46:11.699 --> 00:46:16.689
Further, parameters are always 32 bit floating2 point values.

00:46:16.690 --> 00:46:23.010
Now the range and the meaning of that 32 bit
flow is going to vary from parameter to parameter

00:46:23.010 --> 00:46:27.680
and you can get the audio unit to fill
out an audio unit parameter infrastructure

00:46:27.679 --> 00:46:31.739
that can describe the usage of the parameter to you.

00:46:31.739 --> 00:46:37.569
Now parameters can be, the changes to a
parameter can be scheduled in advance,

00:46:37.570 --> 00:46:44.640
and the changes can also be ramped smoothly from
one value to another over some length of time.

00:46:44.639 --> 00:46:51.039
Both of those operations are absolutely key to using an
audio unit in a digital audio workstation environment

00:46:51.039 --> 00:47:00.929
where you might be automating the
process of the data over time.

00:47:00.929 --> 00:47:06.129
Now the I/O of an audio unit is organized into a bus.

00:47:06.130 --> 00:47:10.900
Now each bus on an audio unit has some number of channels

00:47:10.900 --> 00:47:17.590
and each audio unit can have potentially
many input busses and output busses.

00:47:17.590 --> 00:47:22.510
Now each bus will also have an audio
channel layout that can describe the usage

00:47:22.510 --> 00:47:26.620
of each channel in a multi channel context.

00:47:26.619 --> 00:47:33.259
And when you're dealing with the audio unit API itself and
you're talking about properties that relate to the input

00:47:33.260 --> 00:47:41.400
and output busses, the actual bus number is passed to
the audio unit as the element in the property address.

00:47:42.590 --> 00:47:49.150
Now in order to set up an audio unit to render
some data, the first thing you have to do is locate

00:47:49.150 --> 00:47:53.110
and open the audio unit and this is
done using the component manager.

00:47:53.110 --> 00:47:59.190
This means you're going to use routines like find
next component and open a component, etcetera,

00:47:59.190 --> 00:48:04.460
in order to locate the audio unit
component that you're interested in.

00:48:04.460 --> 00:48:09.329
Once you've opened the audio unit,
you configure the audio unit by again,

00:48:09.329 --> 00:48:13.809
just like every other Core Audio
object, you set properties on it.

00:48:13.809 --> 00:48:20.019
Things such as the sample rate and the maximum number of
frames that you're going to ask the audio unit to render

00:48:20.019 --> 00:48:23.039
at a given time have to be set at this time.

00:48:23.039 --> 00:48:32.869
You also set up any algorithm specific attributes, such as
rendering quality or pointing the audio unit at data files

00:48:32.869 --> 00:48:37.929
such as samples or impulse responses or what have you.

00:48:37.929 --> 00:48:43.489
And then at this point you're also going to want to set up
whatever the audio unit has to whatever other audio units

00:48:43.489 --> 00:48:49.189
that you're using and you're also going to use this
point to install the various callbacks you're going

00:48:49.190 --> 00:48:53.630
to need in order to do the various processing.

00:48:53.630 --> 00:48:58.510
Now in order to get processed data out of the audio unit,

00:48:58.510 --> 00:49:02.830
the first step after you configure
it is to call audio unit initialize.

00:49:02.829 --> 00:49:08.719
This tells the audio unit that you're done configuring
it and that it's okay for the audio unit to go ahead

00:49:08.719 --> 00:49:18.809
and finalize it's setup and allocate buffers, set up delay
lines, basically get itself ready in order to do processing.

00:49:18.809 --> 00:49:24.380
Now once you've initialized the audio
unit, you call audio unit render in order

00:49:24.380 --> 00:49:28.470
to have the audio unit provide you with data.

00:49:28.469 --> 00:49:35.500
Now when you call audio unit render, you pass in a
time stamp that indicates the position in the stream

00:49:35.500 --> 00:49:38.159
that you want the audio unit to render for.

00:49:38.159 --> 00:49:42.179
You also have to tell the audio
unit how many frames to render.

00:49:42.179 --> 00:49:47.440
And recall that this number that you tell it
can't be larger than the maximum frame size

00:49:47.440 --> 00:49:50.710
that you set earlier in the configuration step.

00:49:50.710 --> 00:49:58.039
Now you will also pass in an audio buffer list
that can either be populated with buffers,

00:49:58.039 --> 00:50:04.320
in which case you're telling the audio unit that you
would like it to render into the buffers that you provide,

00:50:04.320 --> 00:50:11.330
or you can populate the audio buffer list with nulls, in
which case you're telling the audio unit that you want it

00:50:11.329 --> 00:50:16.259
to render into it's own buffers and
then provide you with pointers to them.

00:50:16.260 --> 00:50:22.740
Now both of those options are important for when you're
trying to optimize the signal flow in your application

00:50:22.739 --> 00:50:29.329
so that you can minimize the number
of copies that occur in the operation.

00:50:29.329 --> 00:50:35.650
And finally, much like the audio converter, the
audio units have a reset method that is there

00:50:35.650 --> 00:50:38.250
to return the audio unit to it's ground state.

00:50:38.250 --> 00:50:44.190
And you're going to call this to do things like
quiet reverb tails, stop delay lines, etcetera.

00:50:44.190 --> 00:50:49.960
Basically return the audio unit to its
just after being initialized state.

00:50:49.960 --> 00:51:01.150
So you can get data out of the audio unit,
you also have the task of getting the data

00:51:01.150 --> 00:51:03.960
into the audio unit that you want to process.

00:51:03.960 --> 00:51:09.420
Now there are two ways that you
can get audio into the audio unit.

00:51:09.420 --> 00:51:13.460
The first way is that the audio
can come from another audio unit.

00:51:13.460 --> 00:51:19.949
You can manually use the make connection property
on the audio unit to directly connect and input bus

00:51:19.949 --> 00:51:23.710
of the audio unit to another audio unit's output bus.

00:51:23.710 --> 00:51:29.030
When you make that connection, whenever you
ask the audio unit to render on that bus,

00:51:29.030 --> 00:51:36.260
he's going to go pull on the output bus of
the other audio unit to get his input data.

00:51:36.260 --> 00:51:41.660
Now manual connections are good, but
the AU graph API is there specifically

00:51:41.659 --> 00:51:50.029
to help you manage all the complexity involved with
dealing with the connectiveness of your audio units.

00:51:50.030 --> 00:51:56.610
Now another way you can get input data into
the audio unit is to use the render callback.

00:51:56.610 --> 00:52:03.800
The render callback is called by the audio unit
whenever it needs new input data to process.

00:52:03.800 --> 00:52:10.110
And you install your render callback
using the set render callback property.

00:52:10.110 --> 00:52:16.930
Now one thing to note about these two methods is that
they are mutually exclusive on a bus by bus basis.

00:52:16.929 --> 00:52:25.500
That is on a given bus you can either connect it to
another audio unit or you can install an input callback

00:52:25.500 --> 00:52:30.650
to provide the data, but you can't do both.

00:52:30.650 --> 00:52:40.369
Now OS X ships a lot of audio units and this is a
partial list of some of them and it's a list of also some

00:52:40.369 --> 00:52:44.710
of the general categories of audio
units that are in the system.

00:52:44.710 --> 00:52:50.949
You have effects, music effects, which are things
like delays and filters, EQs, what have you.

00:52:50.949 --> 00:53:00.539
The different between an effect and music effect is music
effects can process their control information from midi.

00:53:00.539 --> 00:53:01.779
And you have panner units.

00:53:01.780 --> 00:53:11.860
Panner units are there to manipulate the spatialization
of a signal and we have units that do HRTF panning,

00:53:11.860 --> 00:53:15.260
vector panning, sound field panning, etcetera.

00:53:15.260 --> 00:53:17.780
Then we have format converter audio units.

00:53:17.780 --> 00:53:22.880
Now I mentioned format converter audio units
earlier in that they're one of the kinds

00:53:22.880 --> 00:53:27.019
of audio units that can take non canonical format data.

00:53:27.019 --> 00:53:32.579
And the reason for that is some of them actually have
an audio converter built into them, but not all of them.

00:53:32.579 --> 00:53:41.210
But mostly the key aspect of a format converter is how the
cadence at which they're going to pull for their input data.

00:53:41.210 --> 00:53:44.820
Then you have generators and music devices.

00:53:44.820 --> 00:53:49.730
Generators and music devices represent
sources of audio in a graph of audio units.

00:53:49.730 --> 00:53:56.320
And the difference between a music device and a
generator is that music devices are representative

00:53:56.320 --> 00:54:00.990
of software synthesizers, whereas
generators provide a much wider range

00:54:00.989 --> 00:54:04.599
of things that can provide sound into the system.

00:54:04.599 --> 00:54:08.849
And then you have mixers, which are pretty
standard signal processing operations.

00:54:08.849 --> 00:54:16.699
We have several mixers on the system, including a stereo
mixer, a mixer that does 3D mixing, the matrix mixer,

00:54:16.699 --> 00:54:24.409
which is for handling for routing and other forms of
complex mixing operations and there are a couple of others.

00:54:24.409 --> 00:54:29.259
Finally you have output units,
AU Hal being the primary example.

00:54:29.260 --> 00:54:32.070
Output units are the synchs in a graph.

00:54:32.070 --> 00:54:37.070
They're basically the place where audio goes to
exit the graph and to be handed off to places

00:54:37.070 --> 00:54:40.300
like the hardware or to be written to a file.

00:54:40.300 --> 00:54:44.880
And then the final kind of audio that
I want to mention is the offline unit.

00:54:44.880 --> 00:54:49.099
An offline unit is a special kind of audio unit

00:54:49.099 --> 00:54:57.059
where the signal processing operation involved
requires either random access to the audio stream data,

00:54:57.059 --> 00:55:03.250
or it requires to be able to see all of the data
ahead of time before it can do it's processing.

00:55:03.250 --> 00:55:06.579
An example of an offline unit is normalization.

00:55:06.579 --> 00:55:15.199
Now I'd like to wrap up this part of the talk by
talking about how you output data to the hardware

00:55:15.199 --> 00:55:17.649
and actually get it to come out of the speaker.

00:55:17.650 --> 00:55:25.119
Now in the Core Audio API set there are probably more
ways to output data to the speaker than there are

00:55:25.119 --> 00:55:27.679
to do just about any other kind of tasks.

00:55:27.679 --> 00:55:31.519
I'm going to talk about four basic ways right now, in fact.

00:55:31.519 --> 00:55:36.400
The first one I want to talk about
is using the audio how directly.

00:55:36.400 --> 00:55:41.470
Now the Audio HAL, as you might recall
from earlier, is a very low level API.

00:55:41.469 --> 00:55:49.909
The Audio HAL's job is to be focused on providing
access to the audio hardware on the audio devices terms.

00:55:49.909 --> 00:55:56.670
That is the Audio HAL does not provide
convenience functions and it doesn't emulate things

00:55:56.670 --> 00:55:59.079
that they audio device doesn't actually do.

00:55:59.079 --> 00:56:07.639
For example, the HAL will not provide volume control on
a channel if the hardware does not actually support it.

00:56:07.639 --> 00:56:16.069
Now as such, the HAL's a pretty complex beast
and applications that use it are expected to deal

00:56:16.070 --> 00:56:23.730
with all the various notifications and all the
changes in state that the HAL will provide that relate

00:56:23.730 --> 00:56:28.280
to the ever evolving state of the
audio hardware on the system.

00:56:28.280 --> 00:56:34.930
Applications are also expected to handle all their
own mixing and also all their own format conversions

00:56:34.929 --> 00:56:40.869
so that they can provide the data to the hardware
in the format that the hardware is expecting.

00:56:40.869 --> 00:56:48.690
Now one other aspect of using the how, is that when you're
inside your I/O Proc, which is the callback the HAL will call

00:56:48.690 --> 00:56:55.110
when it wants to get some data for output from you,
you have to make sure that you're not going to block.

00:56:55.110 --> 00:57:01.400
This is very important because this thread has to meet
a deadline and when you block there's no guarantee

00:57:01.400 --> 00:57:07.300
that you're going to get back to the thread in
any time reasonable for making the deadline.

00:57:07.300 --> 00:57:12.620
Now given all the complexities of using
the how and being a proper how client,

00:57:12.619 --> 00:57:18.079
generally it's not recommended for
most applications to use the how.

00:57:18.079 --> 00:57:27.269
Instead, most applications should probably look at using AU
HAL, which is the output audio unit that talks to the how

00:57:27.269 --> 00:57:33.480
in order to send the data on a graph
of audio units out to the hardware.

00:57:33.480 --> 00:57:38.949
Now AU HAL is an output audio unit and
everything I've mentioned up to now

00:57:38.949 --> 00:57:43.889
about dealing with audio units definitely applies.

00:57:43.889 --> 00:57:50.839
The main difference between output units and the rest
of the audio units is that output units add to routines,

00:57:50.840 --> 00:57:54.480
to the audio output start and stop methods.

00:57:54.480 --> 00:57:56.980
And these routines are used to start and stop IO.

00:57:56.980 --> 00:58:08.289
Now AU HAL is there to do all the heavy lifting and all that
complex code for being a proper how client on your behalf

00:58:08.289 --> 00:58:10.860
so that your application doesn't have to.

00:58:10.860 --> 00:58:18.410
one of the big jobs that it does is it takes the wide
variety of formats you're liked to get from an audio device

00:58:18.409 --> 00:58:27.559
and massages it down so that it appears in the audio
unit world as just one input bus on the audio unit,

00:58:27.559 --> 00:58:32.079
and this saves you a lot of hassle and
it kind of normalizes things out so

00:58:32.079 --> 00:58:38.079
that it makes it easy to feed data to the hardware.

00:58:38.079 --> 00:58:40.860
Now AU HAL comes in two basic flavors.

00:58:40.860 --> 00:58:46.809
There is the basic AU how flavor and it
tracks the device that you tell it to.

00:58:46.809 --> 00:58:57.519
You can go to the how and say I want to use the built in
speaker, and it will track that device across any changes

00:58:57.519 --> 00:59:00.699
or whether it gets unplugged or what have you.

00:59:00.699 --> 00:59:06.889
Now the other kind of AU how output
unit is the default output unit.

00:59:06.889 --> 00:59:13.379
Now the default output units only differ from the regular
AU HAL class in terms of what device that they track.

00:59:13.380 --> 00:59:15.680
The default output units come in two flavors.

00:59:15.679 --> 00:59:21.629
There's the regular default output unit that
tracks the how's content default device,

00:59:21.630 --> 00:59:29.550
which is the device that the system has designated
for all content output such as the out put

00:59:29.550 --> 00:59:33.280
for iTunes, or QuickTime Player or what have you.

00:59:33.280 --> 00:59:39.290
And then there's the system default device and
that's the device that's used for interface sounds.

00:59:39.289 --> 00:59:43.690
And they may not be the same device, so you have
to make sure in the case where you want to deal

00:59:43.690 --> 00:59:47.710
with default device that you open the right component.

00:59:49.309 --> 00:59:57.789
Now one other interesting aspect of AU how is that
it provides an audio converter on its input bus,

00:59:57.789 --> 01:00:01.590
but this audio converter supports only linear PCM data.

01:00:01.590 --> 01:00:07.390
But this does give you an added bonus of flexibility
in terms of what kind of data you can supply

01:00:07.389 --> 01:00:12.099
to the output unit in order to map it onto the hardware.

01:00:12.099 --> 01:00:17.049
Now when you're dealing with AU HAL,
you can deal with mixing in two ways.

01:00:17.050 --> 01:00:25.230
You can either install a render callback and do it
yourself, or you can connect a mixer unit to the input bus

01:00:25.230 --> 01:00:31.889
and connect your input data to the
various inputs of the mixer audio unit.

01:00:31.889 --> 01:00:38.279
Now one thing to be aware of with AU HAL is that
when you're inside your render callback for AU HAL,

01:00:38.280 --> 01:00:44.100
you're also on the HAL's IO Proc so all
those rules I mentioned earlier still apply.

01:00:44.099 --> 01:00:50.849
Now the next method I want to talk about for
outputting data to the hardware is for using OpenAL.

01:00:50.849 --> 01:00:59.860
OpenAL is a high level API that
provides cross platform 3D audio mixing.

01:00:59.860 --> 01:01:04.490
And while OpenAL has mostly been
tailored for the needs of game developers,

01:01:04.489 --> 01:01:10.589
we found that it can be very useful
in a wide variety of context.

01:01:10.590 --> 01:01:14.900
And if you need cross platform support,
OpenAL is a pretty good place to start.

01:01:14.900 --> 01:01:24.800
Now the Mac OS X implementation of OpenAL is built on
top of the 3D mixer audio unit that I mentioned earlier.

01:01:24.800 --> 01:01:30.560
And I'm not going to go into too much more detail about Open
AL because we're going to have a great demo about OpenAL

01:01:30.559 --> 01:01:34.590
in the very next session here, in
the Core Audio hands-on session.

01:01:34.590 --> 01:01:43.630
And the final way I'd like to talk about for outputting
data to the hardware is using the Audio Queue API.

01:01:43.630 --> 01:01:51.000
The audio queue is a new API in Leopard that provides
a very high level API for playing audio data.

01:01:51.000 --> 01:02:00.409
It can handle both linear PCM and compressed audio data,
but one of the interesting aspects about the queue is

01:02:00.409 --> 01:02:05.879
that it owns the buffers that you're
going to be filling up to play.

01:02:05.880 --> 01:02:15.190
Now the queue also supports very rich timing mechanisms and
it also has support for scheduled playback of your sound.

01:02:15.190 --> 01:02:19.440
And again, much like OpenAL we're going
to talk a lot more about the audio queue

01:02:19.440 --> 01:02:23.619
in the next session, the audio hands-on session.

01:02:23.619 --> 01:02:32.639
So to kind of think back I've been talking about the core
audio architecture in general, what sort of APIs we have,

01:02:32.639 --> 01:02:39.879
and I've gone into some usage cases at kind of a
10,000 foot level to kind of introduce you to some

01:02:39.880 --> 01:02:43.289
of the Core Audio APIs you're going to end up using.

01:02:43.289 --> 01:02:50.509
And the next session, as I said earlier, is the
Core Audio hands on session where you're going

01:02:50.510 --> 01:02:56.990
to get some more practical experience with the Core Audio
APIs, in particular we're going to talk about playing

01:02:56.989 --> 01:03:03.449
and recording with the audio queue, using
the audio file and audio file streams APIs,

01:03:03.449 --> 01:03:08.289
using the extended audio file API and more about OpenAL.

01:03:08.289 --> 01:03:15.579
Now there are a lot of resources out there that you can use
to find out more about Core Audio and its architecture.

01:03:15.579 --> 01:03:22.539
There's documents, including the Core Audio Overview,
Getting Started With Audio, and various programming guides,

01:03:22.539 --> 01:03:27.769
including The Audio Unit Programming Guide, and
the new Audio Queue Services Programming Guide,

01:03:27.769 --> 01:03:31.659
which you can get from the WWDC attendee site.

01:03:31.659 --> 01:03:41.049
You'll also find tech notes and sample code
on the developer website at the URL here.

01:03:41.050 --> 01:03:47.519
And again, here are some more resources I'd like
to particularly plug, the Core Audio SDK again.

01:03:47.519 --> 01:03:54.019
There's a lot of code in there, and even though a lot
of it is C++, you can adapt it pretty much

01:03:54.019 --> 01:04:01.409
to whatever you're trying to do, and there's a
lot of knowledge kind of coded up in that SDK.

01:04:01.409 --> 01:04:04.539
And finally, I just want to plug our mailing list.

01:04:04.539 --> 01:04:08.590
The Core Audio API mailing list is probably
the best way to get in touch with us.

01:04:08.590 --> 01:04:15.490
All of the Core Audio engineering staff monitors
that list and we answer questions all the time there.