WEBVTT

00:00:15.529 --> 00:00:21.320
>> Before we get started, have a look up here
for some first instructions to get started.

00:00:21.320 --> 00:00:26.240
What we are going to do during the hands-on session is
actually set-up some video conferences with each other

00:00:26.239 --> 00:00:30.969
so that we can try out iChat Theater
as we build the project.

00:00:30.969 --> 00:00:35.750
So find somebody near by and set up a
computer-to-computer Bonjour network.

00:00:35.750 --> 00:00:38.369
Once you have that set up, when you
log into Bonjour you should be able

00:00:38.369 --> 00:00:42.619
to see the other person there and set up a video chat.

00:00:42.619 --> 00:00:46.509
I'll give you guys just a minute to do that.

00:00:46.509 --> 00:00:56.259
( Background noise )

00:00:56.259 --> 00:00:58.799
>> Okay. Let's get started.

00:01:00.310 --> 00:01:04.890
This is session 202, Sharing Your
Application's Content with iChat Theater.

00:01:04.890 --> 00:01:05.849
My name is Peter Westen.

00:01:05.849 --> 00:01:07.649
I'm an engineer on the iChat team.

00:01:07.650 --> 00:01:15.420
And we're going to talk about the demos that we saw during
Steve's Keynote, where Phil was able to share content

00:01:15.420 --> 00:01:21.549
in his Keynote presentations or in iPhoto or even
directly out of Finder in a middle of a video chat.

00:01:21.549 --> 00:01:24.829
So what's iChat Theater?

00:01:26.450 --> 00:01:32.950
It begins with content in another application, whether it's
Keynote or iPhoto or QuickLook running inside of Finder.

00:01:32.950 --> 00:01:40.409
And then when you have a running video chat with another
buddy, iChat Theater allows that application to send video

00:01:40.409 --> 00:01:45.049
and audio through that video chat so that it
can be shared with the person on the other side,

00:01:45.049 --> 00:01:48.920
who doesn't need to have the same
software on their computer.

00:01:50.489 --> 00:01:54.879
Example, clients would be iPhoto
and Keynote, which we saw on Monday,

00:01:54.879 --> 00:01:59.159
QuickTime Player can, of course, also send
movies directly into iChat Theater.

00:01:59.159 --> 00:02:05.299
Other applications could send 3D
content into =a video chat and hopefully

00:02:05.299 --> 00:02:09.370
after today you're application's content as well.

00:02:09.370 --> 00:02:14.090
Before we get into the meat of things, there
are a few things iChat Theater does not do.

00:02:14.090 --> 00:02:16.080
First thing is screen sharing.

00:02:16.080 --> 00:02:17.290
There's no shared control.

00:02:17.289 --> 00:02:21.949
It's a presentation of the content you have
on your computer with a buddy.

00:02:21.949 --> 00:02:30.549
So events, mouse clicks, any of that stuff does not
come back into your application from the other side.

00:02:30.550 --> 00:02:33.810
It's not video chat in your application either.

00:02:33.810 --> 00:02:36.890
Sorry, that's not what we are doing today.

00:02:36.889 --> 00:02:41.819
This is about getting the content in your
applications and putting it into iChat.

00:02:41.819 --> 00:02:47.489
It also not Quick Look, although it works with Quick Look
and that's something that was also shown on Monday.

00:02:47.490 --> 00:02:52.420
So before we go any further, let's take a
brief tangent and talked about Quick Look does.

00:02:52.419 --> 00:03:00.619
It's a new technology in Leopard that allows small bundle
to be written for your application;s document type in order

00:03:00.620 --> 00:03:04.349
to present a preview without actually
having to launch your applications.

00:03:04.349 --> 00:03:10.180
It's file based, which means you need to have a document
on disk that can be used to generate a preview.

00:03:10.180 --> 00:03:13.670
If your application's content is stored in documents,

00:03:13.669 --> 00:03:18.399
then you might want to consider writing a
Quick Look preview generator and leaving it at that,

00:03:18.400 --> 00:03:24.860
because iChat can use that preview generator to
produce content that gets shared in the video chat.

00:03:24.860 --> 00:03:27.320
But it does have limited interactivity.

00:03:27.319 --> 00:03:33.129
Quick Look only supports the most basic of operations,
like a dancing forward or back player, pause.

00:03:33.129 --> 00:03:38.689
So if you have more complicated manipulations that you
can do with your contents such as changing the perceptive

00:03:38.689 --> 00:03:43.370
in a 3D scene, then that's not going to work.

00:03:43.370 --> 00:03:44.830
There's also limited fidelity.

00:03:44.830 --> 00:03:50.990
Keynote slides, for example, don't have any of the
transitions that are supported in the Quick Look preview.

00:03:50.990 --> 00:03:56.020
It's the most bare bones simplification
of the content that's in that document.

00:03:56.020 --> 00:03:58.350
But the biggest advantage is that there's no extra work

00:03:58.349 --> 00:04:01.810
if you've written a Quick Look preview
generator for your document type,

00:04:01.810 --> 00:04:04.750
iChat can use that, the user can
even drop the document directly

00:04:04.750 --> 00:04:08.039
onto a video chat in order to share it with iChat Theater.

00:04:08.039 --> 00:04:10.039
The session I believe is right after this one.

00:04:10.039 --> 00:04:14.840
It's 213 and it talks about how to
write Quick Look preview generators.

00:04:14.840 --> 00:04:18.000
So with that out of the way, let's
talk about the iChat Theater API.

00:04:18.000 --> 00:04:22.689
It's a Cocoa API and it's divided into three major parts.

00:04:22.689 --> 00:04:24.879
The first is session control.

00:04:24.879 --> 00:04:30.430
Of course, we need to be able to start and stop these
sessions, but more importantly your application needs

00:04:30.430 --> 00:04:35.180
to integrate those actions in the workflow of
that application in the way that's most appropriate

00:04:35.180 --> 00:04:39.120
and that's the decision that's
left up to you and your designers.

00:04:39.120 --> 00:04:42.459
Video can be provided to iChat
Theater in two different ways.

00:04:42.459 --> 00:04:46.120
Last year we talked about implemented
low-level callbacks to provide video.

00:04:46.120 --> 00:04:52.060
We heard a lot of feedback that there should be an easier
way so, this time around we've got support for NSViews,

00:04:52.060 --> 00:04:56.990
so you can just set an NSView as
the source content for iChat Theater

00:04:56.990 --> 00:05:00.310
and that will get sent directly into the video conference.

00:05:00.310 --> 00:05:05.589
And then audio, there's also two ways
providing audio content into your video chat.

00:05:05.589 --> 00:05:12.799
The first is a very simple, it just uses NSSound, which can
be configured to send its audio directly into iChat Theater

00:05:12.800 --> 00:05:17.480
and there are low-level Core Audio callbacks
if you want more fine grain control.

00:05:17.480 --> 00:05:20.650
So first let's talk about session control.

00:05:20.649 --> 00:05:26.899
The most important part of the
entire API is the IMAVManager class

00:05:30.310 --> 00:05:36.040
and there's a single shared instance
that you will do everything through.

00:05:36.040 --> 00:05:44.260
All you have to do is import the IMAVManager.h
header into your code file and then ask

00:05:44.259 --> 00:05:48.959
for the sharedManager out of that class.

00:05:48.959 --> 00:05:54.159
And that object is going to be the one place
where you get state information to find

00:05:54.160 --> 00:06:01.280
out what state the iChat Theater session is in as well
ask it to start or stop and configure audio and video.

00:06:01.279 --> 00:06:05.619
It's very important that you register for notifications.

00:06:05.620 --> 00:06:11.100
The iChat Theater session depends on iChat and so if
iChat isn't running you won't be able to start a session.

00:06:11.100 --> 00:06:17.260
When it is running, you begin a session and there's
some extra states that tell you when the conference is up

00:06:17.259 --> 00:06:22.189
and running and you should be providing
frames of video to the conference.

00:06:22.189 --> 00:06:25.310
So there are two important parts to
registering for that notification.

00:06:25.310 --> 00:06:27.829
The first is, of course, the notification name itself.

00:06:27.829 --> 00:06:32.810
IMAVManagerStateChangedNotification,
but also it's important that you register

00:06:32.810 --> 00:06:37.560
for this notification using the notifications
center published by the IMService class.

00:06:37.560 --> 00:06:43.629
That's another part of the InstantMessage framework and
the reason we do is that iChat Theater wants to be lazy

00:06:43.629 --> 00:06:47.159
about connecting to iChat and getting
information about iChat Theater.

00:06:47.160 --> 00:06:51.780
So subscribing to this notification is actually
a hint to the framework that you're interested

00:06:51.779 --> 00:06:56.269
and if you do not subscribe to the notification,
we won't know and we won't connect to iChat.

00:06:56.269 --> 00:07:03.349
So the state will always be not available. And then each
time you are interested in the starting or stopping

00:07:03.350 --> 00:07:07.420
of iChat Theater, you'll generally want to get
the state to find out what you're able to do

00:07:07.420 --> 00:07:11.379
and that's simple accessor to get the state.

00:07:11.379 --> 00:07:14.240
So let's talks about those starts, there are six.

00:07:14.240 --> 00:07:17.090
The first is IMAVNotAvailable.

00:07:17.089 --> 00:07:22.889
When you launch your application, this is what the states
going to be because it has not yet connected to iChat.

00:07:22.889 --> 00:07:27.000
Once you subscribe to that notification,
we'll connect to iChat, get the state

00:07:27.000 --> 00:07:31.060
and it will then generally transition to Stopped.

00:07:31.060 --> 00:07:33.110
That means that iChat Theater can be started.

00:07:33.110 --> 00:07:37.389
Now it's important to note that this
is the state of the IMAVManager,

00:07:37.389 --> 00:07:42.169
not the state of the current iChat
Theater session the user might be having.

00:07:42.170 --> 00:07:46.189
For instance, if another application is in
the middle of sharing through iChat Theater,

00:07:46.189 --> 00:07:51.300
the state from the perspective of your application is
going to be stopped, because if the user were to start

00:07:51.300 --> 00:07:56.770
from your application the other one would be
cutoff and your application would then step in.

00:07:56.769 --> 00:08:02.459
Once you start a session it will transition
through StartingUp and from there to Pending

00:08:02.459 --> 00:08:06.680
and what this means your application is
connected to iChat, we're ready to go,

00:08:06.680 --> 00:08:13.250
but the video conference itself either isn't running
or the extra video channel for sending that video

00:08:13.250 --> 00:08:15.879
from your application hasn't been set up yet.

00:08:15.879 --> 00:08:22.209
So it will be in the Pending state for some period of
time and from there it will transition to IMAVRunning,

00:08:22.209 --> 00:08:27.539
which means that the session is active, the video frames
that you're providing will be sent to the other user

00:08:27.540 --> 00:08:32.200
and then when the session shuts down
it will go through ShuttingDown

00:08:32.200 --> 00:08:36.560
and from there back to Stopped and you can start over again.

00:08:38.159 --> 00:08:42.409
So running a session only has a few steps.

00:08:42.409 --> 00:08:44.449
The first is to set a video data source.

00:08:44.450 --> 00:08:50.720
IChat Theater needs a way to get the
video and this can either be an NSView

00:08:50.720 --> 00:08:57.450
or custom object you've written yourself. And that's
simply done through these setVideoDataSource method.

00:08:57.450 --> 00:09:05.350
And then you have a couple of options, this is a
bit mask and you can set two options right now.

00:09:05.350 --> 00:09:12.980
The first is called IMVideoOptimizationStills and
what this is, is a hint to the encoder for the video chat

00:09:12.980 --> 00:09:16.379
that you're content will remain largely static.

00:09:16.379 --> 00:09:21.299
And what that allows us to do is increase the
size of the buffer used for the video chat

00:09:21.299 --> 00:09:27.459
so that image quality can be greater, because we
know the bandwidth requirements will be lower,

00:09:27.460 --> 00:09:34.920
well, encode and send a video frame across the other side
and then it won't use up very much bandwidth after that.

00:09:34.919 --> 00:09:39.490
So, more bandwidth can be devoted
to video from the user's iSight.

00:09:39.490 --> 00:09:44.990
It's important that you only set this option if you know
that your video will remain static for periods of time,

00:09:44.990 --> 00:09:50.769
many seconds for a Slideshow or even an extended
period of time if it's some other application,

00:09:50.769 --> 00:09:53.779
if you're looking at a picture or some other image.

00:09:53.779 --> 00:09:59.209
If you set this and you do have a, you know,
video that's changing every single frame,

00:09:59.210 --> 00:10:04.580
the result is going to be pretty poor performance
and the quality of the video won't be very good.

00:10:04.580 --> 00:10:05.720
There's also another option.

00:10:05.720 --> 00:10:14.290
IMVideoOptimizationReplacement and what this does, is it
forces iChat Theater to replace the local user's iSight video

00:10:14.289 --> 00:10:20.509
with your content instead of sending both side by side, and
the result of that, of course, is that we can devote every bit

00:10:20.509 --> 00:10:26.110
of available CPU and bandwidth
resources to that video that you've sent.

00:10:26.110 --> 00:10:30.960
For the most part, you won't bother sending this because
the person's video is nice to have alongside the content,

00:10:30.960 --> 00:10:35.570
but if you have really important content that
you want to have as high fidelity as possible,

00:10:35.570 --> 00:10:37.140
you may want to set this option.

00:10:37.139 --> 00:10:43.939
If you don't set it though, it's not a guarantee that the
video will be side by side, because if we're in a video chat

00:10:43.940 --> 00:10:48.080
with somebody running Tiger for example,
it doesn't support iChat Theater,

00:10:48.080 --> 00:10:52.500
that's what we'll do automatically as a compatibility mode.

00:10:53.830 --> 00:10:56.540
Then once the options are set and
you have a video data source,

00:10:56.539 --> 00:11:01.879
you begin the session by simply calling
start and when you are done you call stop.

00:11:01.879 --> 00:11:06.789
Now that's the basic part of session control,

00:11:06.789 --> 00:11:11.939
the hands on project that we've got is just
going to talk about that part of the API.

00:11:11.940 --> 00:11:15.510
So we'll get started with that.

00:11:15.509 --> 00:11:22.069
Michael Estee also from the iChat
team will be my buddy here during our video chats.

00:11:23.409 --> 00:11:29.909
And so if you got the iChat Theater Headstart
project downloaded, go ahead and open that up.

00:11:29.909 --> 00:11:32.149
Let's switch over to the demo machine.

00:11:32.149 --> 00:11:36.259
So there are two folders in there, Finish
is the end state, we're going to want to be in

00:11:36.259 --> 00:11:41.240
and Start is our beginning state for this application.

00:11:41.240 --> 00:11:47.049
So the Slideshow project is pretty straight forward.

00:11:47.049 --> 00:11:49.750
First off, let's just see what this does.

00:11:49.750 --> 00:11:57.940
It's a simple application which loads images out
of your desktop Pictures folder and displays them

00:11:57.940 --> 00:12:01.180
in a Slideshow, but it doesn't do much else.

00:12:01.179 --> 00:12:02.129
It's not very interesting.

00:12:02.129 --> 00:12:04.320
So let's combine this with iChat Theater.

00:12:04.320 --> 00:12:06.900
So does everybody have this project
loaded up on their computers?

00:12:06.899 --> 00:12:09.840
Raise your hand if you got that there
and you'll be able to follow along.

00:12:09.840 --> 00:12:15.120
Great. You'll be able to get a decent amount even
if you don't have this loaded up on your machine.

00:12:15.120 --> 00:12:20.560
So we'll quit that and have a look
at the code in this application.

00:12:20.559 --> 00:12:26.209
It's very simple in the starting case.

00:12:26.210 --> 00:12:34.170
All this does so far is have one method to start and stop
the Slideshow in a custom view, which displays the images

00:12:34.169 --> 00:12:38.289
and it changes the image on the
start and stop button accordingly.

00:12:38.289 --> 00:12:45.230
A command from the main menu of the application
to toggle whether it's running or not

00:12:45.230 --> 00:12:53.399
and then a validateMenuItem method, which will enable
and disable the start and stop Slideshow menu item.

00:12:53.399 --> 00:13:00.340
And then an application delegate method to make the
application terminate when you close the window.

00:13:00.340 --> 00:13:05.879
This class will be instantiated once in the main
menu .nib and is the application's delegate,

00:13:05.879 --> 00:13:08.509
but right now it doesn't do anything else.

00:13:08.509 --> 00:13:14.480
So in order to make this work with iChat Theater, first
thing we need to do is add the framework to the project.

00:13:14.480 --> 00:13:21.879
So open up your Frameworks group on the side and then
in the Project menu, we're going to say Add to Project.

00:13:21.879 --> 00:13:24.019
Right there.

00:13:24.019 --> 00:13:33.419
And in your Group Volumes System Library Frameworks
folder, you'll find the InstantMessage framework.

00:13:33.419 --> 00:13:40.269
In addition to iChat Theater this framework also supports
getting presence information about buddies from iChat.

00:13:40.269 --> 00:13:43.120
That session was yesterday.

00:13:43.120 --> 00:13:50.279
So go ahead and add that framework to your project
and choose the target and then we'll go back

00:13:50.279 --> 00:13:56.509
to our main controller class and in here
we'll go ahead and import the header.

00:13:56.509 --> 00:14:02.189
Now, to keep things simple we've just put all of the code
into the starter project we need and we're just going

00:14:02.190 --> 00:14:07.680
to remove comment lines so we can activate
different portions of our implementations as we go.

00:14:07.679 --> 00:14:13.259
So just go ahead and remove the
comment line surrounding the import.

00:14:13.259 --> 00:14:17.970
And build to make sure that we've done everything we need to.

00:14:17.970 --> 00:14:20.970
Build succeeded, but it still doesn't do anything yet.

00:14:20.970 --> 00:14:25.200
So the next thing we need to do
is subscribe to that notification,

00:14:25.200 --> 00:14:28.740
as I've mentioned before it's important
that you do this because without

00:14:28.740 --> 00:14:32.909
that notification subscription the instant
message framework will never get the hint

00:14:32.909 --> 00:14:35.120
that you're interested in iChat Theater.

00:14:35.120 --> 00:14:43.990
So in the applicationDidFinishLaunching method just
below, we'll remove those two comment lines and again check

00:14:43.990 --> 00:14:47.649
that we've subscribed to the right stateChangedNotification

00:14:47.649 --> 00:14:54.149
and that we'll also using the IMService notificationCenter,
which is our custom center that we use in order

00:14:54.149 --> 00:14:58.919
to tell when people are subscribing
and unsubscribing to these notifications.

00:14:58.919 --> 00:15:03.279
So again, still we're not doing
anything very interesting yet,

00:15:03.279 --> 00:15:07.909
we need to actually have a command
to start and stop iChat Theater.

00:15:07.909 --> 00:15:14.929
So for that we'll go to the header file for the
controller where we have the toggleSlideshow action method

00:15:14.929 --> 00:15:18.579
and right above it we'll have a new
one we add called toggleTheater.

00:15:18.580 --> 00:15:21.660
Go ahead and remove those two comment lines.

00:15:21.659 --> 00:15:28.949
Save that file and now let's open MainMenu.nib
and hook up a new action.

00:15:28.950 --> 00:15:33.379
So in the Resources group, MainMenu.nib is right there.

00:15:33.379 --> 00:15:42.250
Now the new version of InterfaceBuilder you don't
need to reimport headers if you create your main menu,

00:15:42.250 --> 00:15:46.830
open up the main menu here under
File menu we'll add a new menu item.

00:15:46.830 --> 00:15:57.389
I'll do this just by duplicating Play Slideshow and we'll
retitle that, Start iChat Theater, and then we're just going

00:15:57.389 --> 00:16:03.220
to control drag from this menu item to our
controller object in order to hook up the action.

00:16:03.220 --> 00:16:07.870
All this is pretty elementary Cocoa development.

00:16:07.870 --> 00:16:10.840
And InterfaceBuilder has seen
that we added a new action method

00:16:10.840 --> 00:16:14.840
to the header and we'll just connect that right there.

00:16:14.840 --> 00:16:21.090
Go ahead and save that and back to our
project and we'll make one more change

00:16:21.090 --> 00:16:24.810
to actually add the implementation of
that method before we try things out.

00:16:24.809 --> 00:16:33.139
So if you scroll down a little ways, there's a section
called Headstart Step 3 and this is our action method.

00:16:33.139 --> 00:16:40.289
So go ahead and remove those two comment lines and we'll
talk for a little minute about what's in this method.

00:16:40.289 --> 00:16:48.360
So the first thing we're going to do is grab the
shared AVManager and then ask for its state.

00:16:48.360 --> 00:16:51.800
Since we are going to start and stop based on that state.

00:16:51.799 --> 00:16:54.559
If the state is stopped then we need to start it.

00:16:54.559 --> 00:16:56.599
And as we said, there are three steps you need to do.

00:16:56.600 --> 00:17:02.330
You need to set your video data source, need to set your
optimization options, and then you need to start the session.

00:17:02.330 --> 00:17:08.710
So that's what we've done here, we have an outlet
to the slideShowView, a custom NSView subclass.

00:17:08.710 --> 00:17:14.340
We're going to set the VideoOtimizationStills option,
because we're going to have an image that remains static

00:17:14.339 --> 00:17:18.399
for a very long time, so this particular
video stream isn't going to require a lot

00:17:18.400 --> 00:17:21.640
of bandwidth and then we start the session.

00:17:21.640 --> 00:17:25.290
And as soon as that's started we'll
also make the Slideshow itself begin.

00:17:25.289 --> 00:17:32.069
This is something Objective-C too that sets
the state of slidesShowView to running.

00:17:32.069 --> 00:17:39.139
If the state of the manager is not stopped, then
we'll stop the session and stop the Slideshow as well.

00:17:39.140 --> 00:17:46.580
So let's see how that works.

00:17:46.579 --> 00:17:51.189
So now we've got our app, the menu item is still there.

00:17:51.190 --> 00:18:03.509
It is now there and we'll start up the video
chat with Mike here and see how it works.

00:18:03.509 --> 00:18:15.390
( Background noise )

00:18:15.390 --> 00:18:16.290
>> Where are you?

00:18:16.289 --> 00:18:25.180
Hey, Mike. I want to show you some really cool photos I got.

00:18:25.180 --> 00:18:33.039
So instead of just beginning the Slideshow, we're going to
choose Start iChat Theater from the menu and there we go.

00:18:33.039 --> 00:18:35.200
That's all it takes.

00:18:35.200 --> 00:18:42.720
As you can see, even the transitions between the slides
are rendered perfectly into the shared video stream.

00:18:42.720 --> 00:18:50.160
So that Mike can see them. And to stop we'll
just choose that same menu item again.

00:18:50.160 --> 00:18:52.810
We'll talk about a little bit more
about configuring that menu item.

00:18:52.809 --> 00:19:00.799
Now I want to talk about another possible
way to begin an iChat Theater session.

00:19:00.799 --> 00:19:04.740
You don't have to have a video chat running first.

00:19:04.740 --> 00:19:09.089
The user can either choose who they are going to talk
to first or they can choose what they are going to share.

00:19:09.089 --> 00:19:15.079
So they can go into your application, even if a video
chat isn't running, the state will be IMAVStopped,

00:19:15.079 --> 00:19:17.659
which means you can start it if you want to.

00:19:17.660 --> 00:19:21.340
So if the user chooses share with
iChat Theater at that point.

00:19:21.339 --> 00:19:25.849
iChat will come to the front and
prompt the user to begin a video chat.

00:19:25.849 --> 00:19:31.990
In addition, this particular order of operations is
important if the user doesn't have a video camera at all.

00:19:31.990 --> 00:19:35.259
If you don't have an iSight connected to your computer,

00:19:35.259 --> 00:19:38.869
you go into the other application
choose Share with iChat Theater.

00:19:38.869 --> 00:19:44.599
iChat will come to the front and then it will be
as if a camera had been connected to your computer.

00:19:44.599 --> 00:19:50.759
You're then able to invite someone to a video chat,
which you can't normally do without a camera and as soon

00:19:50.759 --> 00:19:57.579
as they connect the iChat Theater state will move to running
and will start sending frames across to the other side.

00:19:57.579 --> 00:20:00.220
So let's just try out that opposite order here.

00:20:00.220 --> 00:20:03.269
We'll go back into our application.

00:20:04.470 --> 00:20:09.019
Choose Start iChat Theater and iChat comes
to the front and says iChat Theater is ready.

00:20:09.019 --> 00:20:14.220
And then I'll go ahead and invite
Mike to another video chat.

00:20:14.220 --> 00:20:15.470
Well, what's the problem?

00:20:15.470 --> 00:20:19.650
Our Slideshow is running, so Mike's missing
out on the whole beginning of this Slideshow.

00:20:19.650 --> 00:20:22.530
Maybe even the whole end if this thing connects slowly.

00:20:25.309 --> 00:20:28.509
I'll just go ahead and cancel that for now.

00:20:28.509 --> 00:20:31.750
So how do we synchronize those two actions?

00:20:31.750 --> 00:20:37.970
Go ahead and quit the application and let's look
back at our implementation of this action method.

00:20:37.970 --> 00:20:42.170
What we did was start the session and
then immediately start the Slideshow.

00:20:42.170 --> 00:20:47.440
So that means when the session state moves from
stopped through starting up and to pending,

00:20:47.440 --> 00:20:50.240
we've already started showing our slides.

00:20:50.240 --> 00:20:55.329
So the way to do that is to use the notifications that
we subscribed to in order to synchronize the beginning

00:20:55.329 --> 00:20:58.460
of the Slideshow with the iChat Theater session.

00:20:58.460 --> 00:21:06.450
So the next thing you should do is select these lines, which
start the Slideshow right away and remove them entirely.

00:21:06.450 --> 00:21:11.720
Now if we left things as they are, the user would
have to say, begin sharing with iChat Theater

00:21:11.720 --> 00:21:18.700
and then once the video chat had connected, they'd have
to go back to the Slideshow application and then hit play

00:21:18.700 --> 00:21:23.130
to actually begin the Slideshow, but we don't want
them to actually go through that amount of work.

00:21:23.130 --> 00:21:29.320
So, we'll look at the method which
we subscribe to that notification.

00:21:29.319 --> 00:21:33.129
If you go down a little ways, the stateChanged method,

00:21:33.130 --> 00:21:38.490
it's implemented, but there's, the
contents of it are commented out.

00:21:38.490 --> 00:21:43.930
So if you look at what's in here, all we
have to do, remove those two comment lines

00:21:43.930 --> 00:21:52.330
and see that the playSlideshow method, which is the same
method invoked from the toggleSlideshow action method.

00:21:52.329 --> 00:21:56.949
We're going to call from the stateChanged, so
what this means is, instead of starting a session

00:21:56.950 --> 00:22:02.789
and starting your Slideshow at the same time, we're going
to start the session, wait for it to begin, and as soon

00:22:02.789 --> 00:22:07.730
as it is actually connected and we know video can
be shared, then we'll start the Slideshow.

00:22:07.730 --> 00:22:17.980
So all this does is grab the manager as well as the state
if it's running, play the Slideshow, otherwise stop it.

00:22:17.980 --> 00:22:23.970
So let's see what that does.

00:22:23.970 --> 00:22:32.039
So this time when we say Start iChat Theater, iChat comes
to the front, prompts me to invite Mike to a conference,

00:22:32.039 --> 00:22:34.769
you'll see that the Slideshow is still black.

00:22:34.769 --> 00:22:36.509
It's waiting for us to begin.

00:22:36.509 --> 00:22:46.309
( Background noise )

00:22:46.309 --> 00:22:55.079
>> Bonjour here seems to be a little bit slow.

00:22:55.079 --> 00:23:00.980
So as soon as Mike connects, we negotiate the extra
video stream and than the Slideshow knows to begin.

00:23:04.109 --> 00:23:06.789
So the only other thing we need to add to this application

00:23:06.789 --> 00:23:11.009
to make it a little bit more polished
is to validate that menu item.

00:23:11.009 --> 00:23:12.740
This again is very straight forward.

00:23:12.740 --> 00:23:18.160
If you back to controller.m and scroll up a
little bit ways, scroll up a little bit there,

00:23:18.160 --> 00:23:23.170
you'll see the validateMenuItem method,

00:23:23.170 --> 00:23:28.140
go ahead and remove those two comment
lines and look at what we are doing here.

00:23:28.140 --> 00:23:33.009
So, if the action that we're validating
is toggle theater from the menu item,

00:23:33.009 --> 00:23:38.599
simply grab the state from the shared AV manager,
if it's greater than or equal to starting up,

00:23:38.599 --> 00:23:45.789
which means starting up, pending, or running then the menu
items title should be Stop Sharing With iChat Theater,

00:23:45.789 --> 00:23:52.789
otherwise it should be Share With iChat Theater. And then the
menu item should be enabled if the state is not available

00:23:52.789 --> 00:23:59.190
and that goes for enabling a menu item if you also
want to have a button or even a check box in your UI

00:23:59.190 --> 00:24:03.259
to determine whether or not you're
going to use iChat Theater.

00:24:03.259 --> 00:24:10.230
Simply use that single state to know whether
or not you're able to begin or end a session.

00:24:10.230 --> 00:24:18.819
And with that last change, we'll
try our application out again

00:24:18.819 --> 00:24:20.639
Share With iChat Theater.

00:24:20.640 --> 00:24:28.230
Once it begins the menu item will change
to Stop Sharing With iChat Theater

00:24:28.230 --> 00:24:34.549
and if I end my video chat, we know that
the session stopped.

00:24:34.549 --> 00:24:43.149
So our state immediately reacts and stops the
Slideshow and if I quit iChat entirely right now,

00:24:43.150 --> 00:24:45.620
the InstantMessage framework will let us know

00:24:45.619 --> 00:24:49.159
and this menu item will now be disabled
because the user can't start sharing.

00:24:49.160 --> 00:24:54.680
And that's everything you need to do.

00:24:54.680 --> 00:24:55.650
Let's go back to the slides.

00:24:55.650 --> 00:24:58.509
(Clapping) Thanks, Mike.

00:24:58.509 --> 00:25:06.549
( Clapping )

00:25:06.549 --> 00:25:11.200
>> So that's the most important part of
integrating iChat Theater with your application.

00:25:11.200 --> 00:25:16.730
Beginning and ending a session needs to be integrated
smoothly with the workflow of your particular application

00:25:16.730 --> 00:25:18.509
and you want to give it a lot of thought.

00:25:18.509 --> 00:25:23.859
For iPhoto and Keynote, for example, we
made sharing with iChat Theater an option

00:25:23.859 --> 00:25:26.299
that you have during an actual presentation.

00:25:26.299 --> 00:25:29.940
So with iPhoto you choose a Slideshow, when you say play one

00:25:29.940 --> 00:25:34.750
of the options you choose is also to
send that Slideshow to iChat Theater.

00:25:34.750 --> 00:25:38.450
With Keynote they have a special menu
item to begin presenting the slides,

00:25:38.450 --> 00:25:41.519
but immediately send them to iChat
Theater and begin that session.

00:25:41.519 --> 00:25:44.170
In your applications it maybe modal.

00:25:44.170 --> 00:25:52.160
You might have single option on your entire application
to share the content you have immediately live

00:25:52.160 --> 00:25:57.570
or you might have a specific task within your
application that will be integrated with iChat Theater.

00:25:57.569 --> 00:25:59.339
So, make sure you give that a lot of thought.

00:25:59.339 --> 00:26:07.230
We gave this element of the API a lot of
flexibility as far as what the developer can do.

00:26:07.230 --> 00:26:10.549
So that's session control.

00:26:10.549 --> 00:26:14.639
The next thing to talk about is how
you get video into iChat Theater.

00:26:14.640 --> 00:26:19.640
Again there are two different ways, you can either
specify in NSView and let us take care of it for you

00:26:19.640 --> 00:26:26.100
or you can implement a couple of low-level callbacks
so that you can provide exactly the content you want.

00:26:26.099 --> 00:26:29.589
The easiest way is to use an NSView.

00:26:29.589 --> 00:26:33.409
If you subclass a standard NSView then you can simply pass

00:26:33.410 --> 00:26:42.420
that into iChat Theater using the setVideoDataSource
method and when we need a frame we will render what's

00:26:42.420 --> 00:26:47.380
in your view on the main thread, so you
don't need to worry about thread safety.

00:26:47.380 --> 00:26:51.510
And we're going to do this by calling
-diplayRectIgnoringOpacity:inContext:.

00:26:51.509 --> 00:26:54.539
So this is a standard AppKit method.

00:26:54.539 --> 00:27:00.149
We'll have set up our own context, and we'll,
and this method will render not only your view

00:27:00.150 --> 00:27:07.550
but any subviews it has, taking into account
any opacity they have below it and taking

00:27:07.549 --> 00:27:14.559
that image and sending it into iChat Theater.

00:27:14.559 --> 00:27:21.019
One thing to remember though is that the size of the video
isn't necessarily going to correspond to the size of your view.

00:27:21.019 --> 00:27:26.990
And the way we get around that is by applying a transform
to the context we're asking the view to render

00:27:26.990 --> 00:27:34.470
into that will scale its bounds into the size of the
actual video frame and the result is exactly the same

00:27:34.470 --> 00:27:42.019
as running your entire application at
a different resolution scale factor.

00:27:42.019 --> 00:27:48.799
There's a session, I believe it's right after this one
on Making Your Custom Controls Resolution Independent,

00:27:48.799 --> 00:27:54.700
basically what this means in a nutshell, is that
instead of focusing on pixels you'll focus on points.

00:27:54.700 --> 00:27:59.870
So make sure that all of your lines, your text are all
aligned to each other according to the coordinates

00:27:59.869 --> 00:28:06.959
in the AppKit coordinates system instead of assuming
that pixels and points are equal to each other.

00:28:06.960 --> 00:28:13.140
What this means is, for instance, you could have a view that's
very, very small when it's rendered into the video buffer

00:28:13.140 --> 00:28:20.040
for iChat Theater the quality of the content
actually goes up and text will have more detail

00:28:20.039 --> 00:28:23.389
and your images can have more pixels
that actually get shared

00:28:23.390 --> 00:28:26.950
in the iChat Theater buffer then that
are actually displayed onscreen.

00:28:26.950 --> 00:28:34.769
There's also a lab, I believe it's on
Friday to actually tune your applications

00:28:34.769 --> 00:28:39.240
to make sure that they are resolution independent.

00:28:39.240 --> 00:28:44.799
And the only other thing we need from you, if you are
using a regular NSView is to -setNeedsDisplay.

00:28:44.799 --> 00:28:51.250
Again we don't want to render this image more often then
necessary, so make sure that you use the regular Cocoa view

00:28:51.250 --> 00:28:55.400
and validation mechanism, -setNeedsDisplay
will see that that's happened

00:28:55.400 --> 00:28:58.769
and then rerender the view into our buffers.

00:28:58.769 --> 00:29:06.329
If you do any other weird shenanigans inside drawRect or
use other ways to display an update of the views onscreen,

00:29:06.329 --> 00:29:10.389
we won't know about it and we'll never ask for a new frame.

00:29:10.390 --> 00:29:16.160
There are a couple of special cases, QTMovieView
is probably the easiest one to use.

00:29:16.160 --> 00:29:20.660
It will be able to render off the main thread
because the QTMovie object that's underneath

00:29:20.660 --> 00:29:26.620
that QTMovieView can control access to
the video frames that are inside of it.

00:29:26.619 --> 00:29:32.039
So we're able to render simultaneously
onscreen and for iChat Theater.

00:29:32.039 --> 00:29:40.769
-setNeedsDisplayP:YES is not required, because if the
movie is playing we'll know to get new frames from it

00:29:40.769 --> 00:29:45.599
and we won't rerender frames if
the movie is paused, because again,

00:29:45.599 --> 00:29:49.649
we can just get at the information in the QTMovie.

00:29:49.650 --> 00:29:55.490
Audio comes for free, if you set a QTMovieView
as the video data source for iChat Theater,

00:29:55.490 --> 00:30:01.900
then any audio that's in that movie will find its way
into the conference without you doing any extra work.

00:30:01.900 --> 00:30:06.960
NSOpenGLView is also another special case.

00:30:06.960 --> 00:30:11.769
It will take a snapshot on the main
thread. Because we don't have any control

00:30:11.769 --> 00:30:15.500
over the OpenGL context and the rendering that you do,

00:30:15.500 --> 00:30:19.369
when we need a frame, we will go onto
the main thread and take a snapshot

00:30:19.369 --> 00:30:24.009
of whatever has most recently been
rendered and then use that.

00:30:24.009 --> 00:30:30.029
-setNeedsDisplay:YES is also required for this one, even
though you don't strictly need it with an NSOpenGLView,

00:30:30.029 --> 00:30:36.149
again it's our only hook to know whether
or not we should capture a new snapshot.

00:30:36.150 --> 00:30:44.519
QCView is very similar to NSOpenGLView;
we'll also take a snapshot on the main thread.

00:30:44.519 --> 00:30:51.440
In the case of QCView though, -setNeedsDisplay is not
required because we're going to go ahead and capture a frame

00:30:51.440 --> 00:30:53.890
from that view every single time we need one.

00:30:53.890 --> 00:30:57.650
QCViews generally have animation
that's going pretty constantly,

00:30:57.650 --> 00:31:02.450
so we'll just grab a frame every
time we are able to get one,

00:31:02.450 --> 00:31:11.549
which does mean that the IMVideoOptimizationStills option
is not settable for this particular video data source.

00:31:11.549 --> 00:31:17.649
If you do want to have fine grain control you can
drop down to using our low-level video callbacks.

00:31:17.650 --> 00:31:22.030
So let's talk about those.

00:31:22.029 --> 00:31:23.720
What are the advantages?

00:31:23.720 --> 00:31:29.610
The first advantage is that when you use the low-level video
callbacks that we offer, they render off the main thread,

00:31:29.609 --> 00:31:31.799
which means that any updates that are happening

00:31:31.799 --> 00:31:37.099
onscreen can be done simultaneously
while filling a buffer for iChat Theater.

00:31:37.099 --> 00:31:40.879
You have complete control over the shared content.

00:31:40.880 --> 00:31:42.900
When we ask you to fill a
buffer, there's nothing in it.

00:31:42.900 --> 00:31:44.940
You can put exactly what you want in there.

00:31:44.940 --> 00:31:50.930
For instance if you want some UI that's visible on
screen to the user who's giving the presentation,

00:31:50.930 --> 00:31:55.230
but don't want that to be visible
to the other side, you can do that.

00:31:55.230 --> 00:32:01.599
There's been with optimization that is under your control
now, instead of relying on tricks using setNeedsDisplay,

00:32:01.599 --> 00:32:05.029
the video callbacks that ask for frame
have a simple return value, yes or no.

00:32:05.029 --> 00:32:06.940
You can say no, nothing has changed,

00:32:06.940 --> 00:32:09.450
use the last frame that I've gave you.

00:32:09.450 --> 00:32:16.230
And the last thing is that you will have is fine grain
control over frame scheduling, so that audio synchronization

00:32:16.230 --> 00:32:23.160
and the synchronization between the user's video
and your video can be as accurate as possible.

00:32:23.160 --> 00:32:27.190
So let's talk a little bit about
how this video architecture works.

00:32:31.009 --> 00:32:37.869
The InstantMessage framework, which is inside
your application space will create a frame.

00:32:37.869 --> 00:32:42.109
Pass that to a render callback, which you'll implement.

00:32:42.109 --> 00:32:46.549
When you've filled that buffer, you'll hand it back to us.

00:32:46.549 --> 00:32:50.529
The InstantMessage framework will then
take that frame and send it to iChat

00:32:50.529 --> 00:32:55.460
where it then can be encoded using the
video encoder and sent across the other side

00:32:55.460 --> 00:33:00.250
to the buddy that you're video chatting with.

00:33:00.250 --> 00:33:03.960
Video callbacks, there are two different kinds.

00:33:03.960 --> 00:33:11.840
You can either choose to implement CVPixelBuffer-
based callbacks, which are in main memory bitmaps

00:33:11.839 --> 00:33:17.929
or you can implement CVOpenGLBuffers, which
allow you to render OpenGL content into the buffer.

00:33:17.930 --> 00:33:21.789
And you should choose whichever buffer
type is most appropriate for your content.

00:33:21.789 --> 00:33:27.619
It will probably be pretty obvious which one you need
to do based on the kind of rendering that you do.

00:33:27.619 --> 00:33:31.419
Then for each kind of buffer
type, there are two callbacks.

00:33:31.420 --> 00:33:36.590
There's one callback which will set options
for the kind of buffer you want to fill

00:33:36.589 --> 00:33:41.319
and that will be called once immediately
after you -setVideoDataSource:.

00:33:41.319 --> 00:33:46.000
So choose your video data source, it will implement
this method, as soon as you pass it into iChat Theater,

00:33:46.000 --> 00:33:52.759
we'll ask it what kind of buffers it wants, and
then, of course, there's the actual frame callback.

00:33:53.910 --> 00:34:00.800
So you'll implement one pair of these methods, either the
option and frame callback methods for CVPixelBuffers

00:34:00.799 --> 00:34:06.200
or the options and frame callback
method for OpenGLBuffers.

00:34:06.200 --> 00:34:09.880
So first let's talk about pixel buffers.

00:34:12.989 --> 00:34:17.419
The first method is the pixel format that we need to know.

00:34:17.420 --> 00:34:20.340
We'll create pixel buffers in whatever format you want.

00:34:20.340 --> 00:34:26.800
So, implement this one method, it will get called
after -setVideoDataSource and pass out into

00:34:26.800 --> 00:34:29.010
that parameter the pixel format you want.

00:34:29.010 --> 00:34:36.730
This one is kCVPixelFormatType_32ARGB, which
is the kind of pixel format you want if you want

00:34:36.730 --> 00:34:44.050
to derive a graphic context out of the pixel
buffers that we hand to you in the frame callback.

00:34:44.050 --> 00:34:47.980
Not every single pixel format is
supported by CVPixelBuffers.

00:34:47.980 --> 00:34:51.769
So you'll have to choose one that does work for those.

00:34:51.769 --> 00:34:58.840
And then in the actual frame callback we will
give you the CVPixelBuffer that you should fill

00:34:58.840 --> 00:35:04.840
and then also a timeStamp that you should be filling
for and we'll talk a little bit more about that.

00:35:04.840 --> 00:35:07.760
And then in each one of these callbacks,
whenever it's made,

00:35:07.760 --> 00:35:11.040
if nothing is changed since the last frame, return no.

00:35:11.039 --> 00:35:13.199
This means, of course, that the very first time

00:35:13.199 --> 00:35:18.500
that we make this call you shouldn't return
no because we do need a starting frame.

00:35:19.570 --> 00:35:21.080
Make sure you don't retain the buffer.

00:35:21.079 --> 00:35:25.759
These are being recycled in a buffer pool and
we don't want to rapidly fill up memories.

00:35:25.760 --> 00:35:30.000
So make sure you let this buffer go one you filled it.

00:35:30.000 --> 00:35:36.639
And then lock the base address as you would
in order to actually fill the buffer directly.

00:35:36.639 --> 00:35:40.449
The other thing you can do and this
is in the sample code that we have.

00:35:40.449 --> 00:35:46.980
In the Headstart if you look in Slideshow view it
takes the CVPixelBuffer and uses the base address

00:35:46.980 --> 00:35:52.139
and the pixel format to create a CGGraphics
context so that you can issue Quartz

00:35:52.139 --> 00:35:57.469
or Cocoa drawing commands in order to fill it.

00:35:57.469 --> 00:36:02.489
And then unlock the base address, very
important, before you return from the method.

00:36:02.489 --> 00:36:05.859
So let's look at a sample implementation.

00:36:05.860 --> 00:36:09.250
The first thing is to determine whether
or not the content is changed or not.

00:36:09.250 --> 00:36:10.639
That's something you know.

00:36:10.639 --> 00:36:17.150
So you'll need to determine that in
order to return no when necessary.

00:36:17.150 --> 00:36:20.940
Then, lock the base address.

00:36:20.940 --> 00:36:27.420
When that succeeds, make sure that you
get the dimensions of the pixel buffer.

00:36:27.420 --> 00:36:30.250
They are not guaranteed to be the same for every session

00:36:30.250 --> 00:36:33.760
and they are not even guaranteed
to be the same for every callback.

00:36:33.760 --> 00:36:41.270
iChat is going to dynamically scale these buffers as the
bandwidth availability to the conference changes over time.

00:36:41.269 --> 00:36:43.900
So you want to make sure you get
the dimensions every single time

00:36:43.900 --> 00:36:47.300
and render appropriately for whatever size that we give you.

00:36:48.989 --> 00:36:54.609
Then get the base address, fill the
buffer in whatever way is easiest for you,

00:36:54.610 --> 00:37:00.710
unlock and return yes to signal that
the frame is ready for us to use.

00:37:00.710 --> 00:37:03.599
So, OpenGL buffers work similarly.

00:37:03.599 --> 00:37:10.789
The initial callback that we will make when you
set the video data source is to specify the context

00:37:10.789 --> 00:37:14.679
and the pixel format that you're going to rendering with.

00:37:14.679 --> 00:37:16.539
And that's looks like this.

00:37:16.539 --> 00:37:19.480
It's parallel to the other method.

00:37:19.480 --> 00:37:27.099
There are two parameters that you need to pass out; one is
the CGLContextObj and the other CGLPixelFormatObj.

00:37:27.099 --> 00:37:33.319
Notice that, at least in this little sample here we're using
mySharedContext and we'll talk about why it's important

00:37:33.320 --> 00:37:37.990
to use a shared context if you're going
to be rendering off the main thread.

00:37:37.989 --> 00:37:42.369
And then in the frame callback
method it works very similarly.

00:37:42.369 --> 00:37:48.630
We'll hand you a CVOpenGLBuffer to fill as well
as the virtual screen number you should be using.

00:37:48.630 --> 00:37:52.329
I'll talk about that in just a
second and the timeStamp as usual.

00:37:52.329 --> 00:37:57.710
So, just as with the pixel buffer
callback, if nothing has changed, return no.

00:37:57.710 --> 00:38:00.019
Again, don't retain the buffer.

00:38:01.030 --> 00:38:08.400
Then you'll attach that buffer to your context using the
virtual screen number that we give you, render your scene

00:38:08.400 --> 00:38:12.880
and then when you are done call
glFlush(); and then return yes.

00:38:12.880 --> 00:38:21.019
So the sample implementation here is pretty
similar, again, if nothing is changed return no

00:38:22.090 --> 00:38:26.800
and then take the shared context that you are
using for this rendering and we're going to sign it

00:38:26.800 --> 00:38:31.580
to a special value called CGL_MACRO_CONTEXT
and this is very important if you want to be able

00:38:31.579 --> 00:38:35.699
to render off the main thread while you
are still rendering on the main thread.

00:38:35.699 --> 00:38:40.529
So let's talk about how that works for a minute.

00:38:40.530 --> 00:38:44.110
Multithreaded OpenGL actually is possible.

00:38:44.110 --> 00:38:49.160
And the key is to drop the idea of
having a current OpenGL context.

00:38:49.159 --> 00:38:55.609
If you have two threads that want to be able to render at
the same time and one sets its context as the current one,

00:38:55.610 --> 00:39:00.990
starts drawing, and then the other thread
sets its current thread and starts drawing,

00:39:00.989 --> 00:39:07.179
well, it's changed the context that you have and then suddenly
OpenGL functions are going to get start getting dispatched

00:39:07.179 --> 00:39:13.379
from two different threads to the same
context and things go south very quickly.

00:39:13.380 --> 00:39:16.829
So in order to make this work, if
you import this special header,

00:39:16.829 --> 00:39:24.029
OpenGL/CGLMacro.h, into every source
file that does OpenGL rendering,

00:39:24.030 --> 00:39:31.340
it will redeclare every single OpenGL
function to invisibly take an extra parameter.

00:39:31.340 --> 00:39:35.510
And that parameter is the CGL_MACRO_CONTEXT.

00:39:35.510 --> 00:39:42.370
So any commands that you issue from that point
forward are going to go to the CGL_MACRO_CONTEXT

00:39:42.369 --> 00:39:45.549
that you have declared no matter what scope
that it's in.

00:39:45.550 --> 00:39:52.490
You can declare in a global, you can declare it in an
ivar of a class, you can declare it in a method parameter

00:39:52.489 --> 00:39:56.219
or even just use a local variable in your function.

00:39:56.219 --> 00:39:59.869
So the way that this will work is if you have two
threads, the callback thread that is rendering

00:39:59.869 --> 00:40:02.440
for the iChat Theater and the main
thread rendering onscreen.

00:40:02.440 --> 00:40:09.010
In each one you would first set the CGL_MACRO_CONTEXT
to the appropriate context even

00:40:09.010 --> 00:40:11.680
if these are going to be calling the same methods.

00:40:11.679 --> 00:40:17.359
In order to render the scene, all you have to do is pass
that CGL_MACRO_CONTEXT into each one of those methods

00:40:17.360 --> 00:40:20.720
so that they get directed to the appropriate one.

00:40:20.719 --> 00:40:27.359
So the offscreen context is rendering for iChat Theater
gets the shared context and the main context will be used

00:40:27.360 --> 00:40:34.120
for the main thread and from that point on they can
each issue commands to the GPU that will go directly

00:40:34.119 --> 00:40:36.329
to the context that they are intended for.

00:40:36.329 --> 00:40:39.250
So that's how that works.

00:40:39.250 --> 00:40:48.409
So once you've declared the appropriate context you want
to do your drawing in, attach to the buffer and make sure

00:40:48.409 --> 00:40:51.739
that you use the screenInOut parameter we've passed in.

00:40:51.739 --> 00:40:57.009
This is the OpenGL virtual screen number,
99 out of 100 of you are going to be able

00:40:57.010 --> 00:41:02.920
to just use this value directly in the attached function.

00:41:02.920 --> 00:41:08.909
In very rare cases you're going to want do
rendering on a specific rendering device.

00:41:08.909 --> 00:41:14.949
Cases are very rare, perhaps you have a machine
with five-year-old Rage 128 Pro and you also want

00:41:14.949 --> 00:41:22.460
to support having a modern OpenGL, a
modern graphics card in the same machine.

00:41:22.460 --> 00:41:24.880
You might want to do rendering on a specific device.

00:41:24.880 --> 00:41:28.170
That's not going to apply to very many
of you, but if you do want to render

00:41:28.170 --> 00:41:31.329
to a different device then the one
we've passed into the function.

00:41:31.329 --> 00:41:36.710
It's important that you write that value back into
that parameter so that we know what's happened.

00:41:36.710 --> 00:41:45.690
The CVOpenGLBuffers don't have a width and height the
way a CVPixelBuffer does, so the appropriate function

00:41:45.690 --> 00:41:51.490
to use for this one is the CleanRect, which will give
you the bounds that you should be using and you can pass

00:41:51.489 --> 00:41:57.069
that directly to glViewport in order to configure
the rendering that you do from that point on.

00:41:57.070 --> 00:42:02.170
Any other GL commands you dispatch
after that will go into your context,

00:42:02.170 --> 00:42:06.840
get rendered, call GL flush and then return yes.

00:42:06.840 --> 00:42:11.930
And at that point iChat will be able to take that
OpenGLBuffer and read it back into main memory

00:42:11.929 --> 00:42:18.569
for the video conference encoder to use
and it will get across to the other side.

00:42:18.570 --> 00:42:21.920
Now the other parameter that we haven't
talked about yet is timestamps.

00:42:21.920 --> 00:42:31.920
Both of the frame callback methods have this extra
parameter and we only talked about it briefly last year.

00:42:33.099 --> 00:42:38.579
So there's a very important way to use this
when you are implementing your callbacks.

00:42:38.579 --> 00:42:43.909
This is a CVTimeStamp, the relevant field in here is
called hostTime.

00:42:43.909 --> 00:42:48.079
We'll have just called CVhostTimeGetCurrent
before making the callback into your code.

00:42:48.079 --> 00:42:50.500
So you want to pull that value out.

00:42:51.519 --> 00:42:58.670
Do your normal rendering of the frame and then before you
return from the method you'll write an adjusted value back

00:42:58.670 --> 00:43:05.900
into that hostTime before giving the buffer
back to the InstantMessage framework.

00:43:05.900 --> 00:43:10.090
So the question is, how do you get from
the timeValue that goes into the method

00:43:10.090 --> 00:43:14.559
to the adjustedTimeValue you'll pass back out?

00:43:14.559 --> 00:43:17.190
Here's the problem that we're trying to solve.

00:43:17.190 --> 00:43:24.110
Suppose iChat is making callbacks into
your video data source at 20 frames per second,

00:43:24.110 --> 00:43:29.430
but your content is, for example, a
movie which has 30 frames per second.

00:43:29.429 --> 00:43:31.389
What's going to happen?

00:43:31.389 --> 00:43:38.719
Of course, we're going to have to drop any frame that
comes and goes in between two subsequent frame callbacks.

00:43:38.719 --> 00:43:46.399
If the timestamp parameter is ignored and you leave it
as it is, then iChat will take those frames, encode them,

00:43:46.400 --> 00:43:51.309
send them to the other side and they'll
be displayed at 20 frames a second.

00:43:51.309 --> 00:43:57.570
But the video won't look quite right, because
these frames are being displayed uniformly in time,

00:43:57.570 --> 00:44:02.110
but they weren't sampled from your
content at a uniform rate.

00:44:02.110 --> 00:44:04.280
So how do we fix this?

00:44:04.280 --> 00:44:13.200
Instead, whenever you receive a frame callback adjust
that timestamp backwards in time to correspond to when

00:44:13.199 --> 00:44:18.009
that frame first appeared in your application.

00:44:18.010 --> 00:44:22.750
When the next callback comes in, because our
callback is coming in at the tail end of when

00:44:22.750 --> 00:44:26.230
that frame is visible, the adjustment will be larger.

00:44:26.230 --> 00:44:32.119
If you do this for every single frame, iChat will
be able to use that information and when the decoder

00:44:32.119 --> 00:44:38.299
on the opposite side of the conference displays those
frames, it can actually schedule them for display

00:44:38.300 --> 00:44:43.730
at the frame rate that is consistent with
when they were sampled from your content.

00:44:43.730 --> 00:44:49.990
And this corresponds more accurately to the situation that
we are really in, which is dropping every third frame.

00:44:51.280 --> 00:44:57.230
And when you do this, this will allow us to synchronize the
audio from the user's microphone with the audio provided

00:44:57.230 --> 00:45:04.579
by your application and we're able to then synchronize
that audio with both video streams much more accurately.

00:45:05.969 --> 00:45:07.949
In our Slideshow sample it's a little bit different.

00:45:07.949 --> 00:45:13.359
It doesn't have frames, per se, instead it shows
a still image for an extended period of time,

00:45:13.360 --> 00:45:18.150
it does a cross-dissolve between two different
images and then it shows the next slide

00:45:18.150 --> 00:45:20.480
for a period of time and then cross-dissolves again.

00:45:20.480 --> 00:45:24.360
So what's the right behavior for this kind of pattern?

00:45:24.360 --> 00:45:29.340
The first callback, of course, gets adjusted
back to the beginning of the Slideshow.

00:45:29.340 --> 00:45:35.519
Any callbacks that come in during one of the cross-
dissolves can be rendered for exactly that time,

00:45:35.519 --> 00:45:42.469
because you can just use that timestamp to determine
where in that crossdissolve you need to render.

00:45:42.469 --> 00:45:51.709
So those can actually go straight through and render
for exactly those periods of progress in the transition.

00:45:51.710 --> 00:45:58.510
Each callback that comes in after a transition has
completed should be adjusted back to the end

00:45:58.510 --> 00:46:05.750
of the previous transition so that on the remote side that
cross-dissolve ends at the time it's really supposed to.

00:46:05.750 --> 00:46:10.679
If that timestamp were left as it were, then the last frame

00:46:10.679 --> 00:46:14.619
of the cross-dissolve would last a
little bit too long on the other side.

00:46:14.619 --> 00:46:22.069
Now an added convenience is that you can use this adjustment
to determine whether you should be returning no right

00:46:22.070 --> 00:46:26.670
at the beginning of this frame callback and
this is exactly what the Slideshow view sample

00:46:26.670 --> 00:46:29.320
in the iChat Theater Headstart does.

00:46:30.630 --> 00:46:41.320
Whoop. Too far. Let's go back to where we were.

00:46:41.320 --> 00:46:49.809
So, the same code that adjusts the timestamp back to the
beginning, to the end of the previous dissolve can be used

00:46:49.809 --> 00:46:53.150
to compare to the previous timestamp that you rendered for.

00:46:53.150 --> 00:46:56.800
So what the Slideshow view sample does is
it adjusts the timestamp back to figure

00:46:56.800 --> 00:47:03.060
out whether it should be displaying a single image or
a cross-dissolve and sees that it's the same timestamp

00:47:03.059 --> 00:47:08.840
that it returned from the previous call and says,
I can just bailout and return no in that situation.

00:47:08.840 --> 00:47:12.710
So that's how video works.

00:47:12.710 --> 00:47:16.170
And the last part is audio.

00:47:16.170 --> 00:47:22.360
Again there are two ways to provide
sound to an iChat Theater session.

00:47:22.360 --> 00:47:24.849
The first is to use NSSound.

00:47:24.849 --> 00:47:29.699
This is a new bit of API added
to Leopard and with any NSSound

00:47:29.699 --> 00:47:34.669
that you can have you can have it send its
audio directly into the iChat Theater session.

00:47:34.670 --> 00:47:39.190
First thing you need to do with the iChat
Theater session itself is configure the number

00:47:39.190 --> 00:47:42.360
of audio channels you want to have.

00:47:42.360 --> 00:47:44.030
Typically you'll just set one.

00:47:44.030 --> 00:47:47.450
The only allowable values for this are 0, 1, and 2.

00:47:47.449 --> 00:47:50.849
0 indicates you're not interested
in sending audio to the session at all.

00:47:50.849 --> 00:47:54.759
1 means you have a single channel
of sound you liked to provide.

00:47:54.760 --> 00:47:58.400
2 is really just a convenience
in case you have stereo sound

00:47:58.400 --> 00:48:01.320
and you don't want to have to do any mixing on your own.

00:48:01.320 --> 00:48:06.760
At the end of the day, all of the audio that gets sent
into iChat Theater is mixed down into a single channel

00:48:06.760 --> 00:48:11.760
with the user's microphone input
and sent across the conference.

00:48:11.760 --> 00:48:21.800
So then once your iChat Theater session has connected and
the state is IMAVRunning, you can get an audio device

00:48:21.800 --> 00:48:25.460
and a channel layout used for sending
audio into the session.

00:48:25.460 --> 00:48:31.030
And the two methods for that are audioDeviceUID
and audioDeviceChannels.

00:48:31.030 --> 00:48:39.360
The first is an NSString which identifies a special audio
device, which is used for getting sound into iChat Theater.

00:48:39.360 --> 00:48:43.960
And then an array of channels that will
be the same number that you've asked for.

00:48:43.960 --> 00:48:49.059
So if you asked for one channel, that array will have
one NS number in it and if you've asked for two channels,

00:48:49.059 --> 00:48:51.409
it will be an array of two and its numbers.

00:48:51.409 --> 00:48:54.819
You can't count on the channels being zero and one.

00:48:54.820 --> 00:49:00.559
They might be three and four or seven and nine.

00:49:00.559 --> 00:49:05.059
So once you have the NSSound you'd like to play, all
you need to do is set the audio device that you've got

00:49:05.059 --> 00:49:07.639
out of the IMAVAudioManager and the method

00:49:07.639 --> 00:49:13.299
for that is called setPlaybackDeviceIdentifier
and that's a new API in Leopard.

00:49:13.300 --> 00:49:15.940
And then you need to set the channel mapping.

00:49:15.940 --> 00:49:21.820
What the NSSound needs to know how to do is
which, is direct certain channels of its sound

00:49:21.820 --> 00:49:25.010
into the channels that have been set up in iChat Theater.

00:49:25.010 --> 00:49:33.100
So if the sound has one channel, then that channel sound
should be sent into all the channels you've set up

00:49:33.099 --> 00:49:36.529
for iChat Theater, which is probably also just one.

00:49:36.530 --> 00:49:40.420
It is important that you know how many
channels that particular sound has.

00:49:40.420 --> 00:49:47.309
Ninety-nine percent of the time it will just be a
single channel sound and this is how you set it up.

00:49:47.309 --> 00:49:56.429
So the mapping works as an array, so the first item in
that array corresponds to channel zero of the NSSound.

00:49:56.429 --> 00:50:03.239
If you only have a single channel sound you
take the single channel from the IMAVManager,

00:50:03.239 --> 00:50:10.179
which is in the channels array there, you package
that inside an array which maps index zero

00:50:10.179 --> 00:50:16.659
to the NS number inside the other array which is
the channel of the audio device that we've got

00:50:16.659 --> 00:50:19.769
and set that as the channel mapping on the sound.

00:50:19.769 --> 00:50:21.869
So again, for a single channel it's direct.

00:50:21.869 --> 00:50:27.289
That mapping array maps to a single,
either NS number or an array of NS numbers.

00:50:27.289 --> 00:50:34.460
If you have a two channel sound, index zero should contain
the first channel you got out of the channels array

00:50:34.460 --> 00:50:37.900
from the IMAVManager and the second
should contain the second.

00:50:37.900 --> 00:50:44.480
And then simply play the sound and its
contents will go into the iChat Theater session

00:50:44.480 --> 00:50:48.630
and be heard on the other side of the conference.

00:50:49.789 --> 00:50:55.840
One thing that is very convenient, if you download the
full iChat Theater sample from the developer site,

00:50:55.840 --> 00:51:02.380
it also includes examples for using OpenGLViews,
threaded and nonthreaded, that has a pair of files

00:51:02.380 --> 00:51:05.570
which declare a category method on NS Sound,

00:51:05.570 --> 00:51:10.910
adds two simple methods, play mono for iChat
Theater and play stereo for iChat Theater

00:51:10.909 --> 00:51:16.440
and that has the most correct implementation of
how you should play your sounds into iChat Theater.

00:51:16.440 --> 00:51:23.789
I recommend that you just download that project, dump
those files straight into your application and use those.

00:51:23.789 --> 00:51:27.969
That way you don't need to worry about the details.

00:51:27.969 --> 00:51:36.489
So, low-level Core Audio callbacks are also available
if you want to have finetune control over the content.

00:51:36.489 --> 00:51:41.189
Again, you'll use the same audio device and
channel layout that you get from the IMAVManager,

00:51:41.190 --> 00:51:45.650
but you need to convert those into a
Core Audio device that you can then use.

00:51:45.650 --> 00:51:47.559
So it's a little bit involved.

00:51:47.559 --> 00:51:51.630
Bear with me, we can get through this bit of code.

00:51:51.630 --> 00:51:55.530
The first thing you need is an
AudioValueTranslation structure.

00:51:55.530 --> 00:52:01.519
This is used for Core Audio function
to convert the NSString we gave you

00:52:01.519 --> 00:52:05.989
to the audio device ID that you'll use with Core Audio.

00:52:05.989 --> 00:52:12.939
The input to that is the deviceUID, the string that you got
from the IMAVManager and the output is the AudioDeviceID

00:52:12.940 --> 00:52:21.170
that you want to have at the end of the function
call, which is AudioHardwareGetProperty.

00:52:21.170 --> 00:52:27.659
You'll pass in this parameter here,
kAudioHardwarePropertyDeviceForUID that says,

00:52:27.659 --> 00:52:38.269
I'm going to give you a struct, which has an NSString for
an audio device and I'd like to get the AudioDeviceID out.

00:52:38.269 --> 00:52:46.159
Pass in that AVTranslation struct and when that call is
completed, you will have the AudioDeviceID that you need.

00:52:46.159 --> 00:52:49.869
And what do you do after that, that's
outside the scope of this session.

00:52:49.869 --> 00:52:53.929
Using the Core Audio device is
a whole other topic, of course.

00:52:53.929 --> 00:53:00.719
I think those sessions have already taken place, so you
may want to get the DVDs from the conference to check

00:53:00.719 --> 00:53:03.909
out what was in those, if you didn't
get a chance to see them.

00:53:03.909 --> 00:53:12.069
Those two sessions were 402 and the lab I think
has also taken place, so that might not matter too.

00:53:12.070 --> 00:53:15.850
So that's how you send audio into iChat Theater.

00:53:15.849 --> 00:53:19.460
So in summary, again there are three parts.

00:53:19.460 --> 00:53:22.050
Most important part for your application is session control.

00:53:22.050 --> 00:53:27.050
I want you to think very hard about how you are going
to integrate starting and stopping iChat Theater

00:53:27.050 --> 00:53:33.100
with manipulating content inside your own applications,
synchronizing the presentation of that content

00:53:33.099 --> 00:53:37.409
with the state of the iChat Theater session.

00:53:37.409 --> 00:53:40.139
Two ways to provide video, the first is to use NSViews.

00:53:40.139 --> 00:53:48.480
They do have some pitfalls, and then using low-level Core
Audio callbacks if you want the maximum amount of control.

00:53:48.480 --> 00:53:52.650
And then for audio, NSSound and Core
Audio are both available to you in order

00:53:52.650 --> 00:53:55.809
to provide sound into the iChat Theater session.

00:53:55.809 --> 00:54:01.049
If you'd like to get more information, our
evangelist is Matt Drance.

00:54:01.050 --> 00:54:05.130
He works on all the sharing technologies
that we have in Leopard.

00:54:05.130 --> 00:54:09.760
We have a new mailing list called
ichat-dev@lists.apple.com.

00:54:09.760 --> 00:54:15.780
Until Leopard ships we won't be able to talk in too
much detail on that list about the new Leopard APIs,

00:54:15.780 --> 00:54:22.200
but that will be ready for general consumption as soon
as Leopard is ready. And of course, all the documentation

00:54:22.199 --> 00:54:26.469
and sample code that we talked about
today is on the attendee's site.

00:54:26.469 --> 00:54:33.129
And with that the iChat Lab is actually
immediately after this session down in Lab D

00:54:33.130 --> 00:54:38.869
and the entire iChat engineering team will be there
to help you guys if you like to get started trying

00:54:38.869 --> 00:54:42.009
to see how you can integrate iChat
Theater into your applications.