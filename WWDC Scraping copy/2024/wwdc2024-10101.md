# Wwdc2024 10101

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Explore object tracking for visionOSFind out how you can use object tracking to turn real-world objects into virtual anchors in your visionOS app. Learn how you can build spatial experiences with object tracking from start to finish. Find out how to create a reference object using machine learning in Create ML and attach content relative to your target object in Reality Composer Pro, RealityKit or ARKit APIs.Chapters0:00 -Introduction5:07 -Create reference object9:28 -Anchor virtual contentResourcesExploring object tracking with ARKitForum: Spatial ComputingImplementing object tracking in your visionOS appHD VideoSD VideoRelated VideosWWDC24Build a spatial drawing app with RealityKitCompose interactive 3D content in Reality Composer ProCreate enhanced spatial computing experiences with ARKitWhat’s new in Create MLWWDC23Meet Object Capture for iOS

Find out how you can use object tracking to turn real-world objects into virtual anchors in your visionOS app. Learn how you can build spatial experiences with object tracking from start to finish. Find out how to create a reference object using machine learning in Create ML and attach content relative to your target object in Reality Composer Pro, RealityKit or ARKit APIs.

0:00 -Introduction

5:07 -Create reference object

9:28 -Anchor virtual content

Exploring object tracking with ARKit

Forum: Spatial Computing

Implementing object tracking in your visionOS app

HD VideoSD Video

HD Video

SD Video

Build a spatial drawing app with RealityKit

Compose interactive 3D content in Reality Composer Pro

Create enhanced spatial computing experiences with ARKit

What’s new in Create ML

Meet Object Capture for iOS

Search this video…Hello, and welcome to “Explore object tracking for visionOS”.I’m Henning, an engineer on the Object Tracking team.Today, I’ll show you how to turn real world objectsinto virtual anchors that your visionOS appcan bring to life using our new Object Tracking technology.You may have already created spatial experiences using Reality Composer Proor our RealityKit and ARKit frameworks.You may also be familiar with the concept of anchoring virtual objectsin people’s surroundings.For example, your visionOS app can use our RealityKit and ARKit APIsto place content relative to planes, images, or hands.Anchors are a great starting point to immersive experiencesthat blur the line between what’s real and what’s virtual.With object tracking, we’re now adding support for using real worlditems as anchors in your app.Imagine everyday objects that reveal useful informationonce you glance at them.Or household appliances and devices that already come with a virtual manual.Or collectables and toys that come to life and launch you intoan immersive storytelling experience.Sounds like magic? Well, let me show you!Here is a view of my space and I have a few items sitting on my desk.There is a globe, a microscope, and an oscilloscope.With Object Tracking, my app can get the position and orientationof each of these items, represented by a coordinate systemand bounding box as shown here in my example.Now that my app knows exactlywhere my objects are sitting in spatial coordinates,I can take this one step further and augment them with interesting content.Here is the view of my globe again,but this time there is a virtual label attached to it,telling me to tap on the globe for more information.Doing that reveals some objects orbiting around my globe.Oh, and there is a space shuttle launching into my room!Notice how the virtual moon and the space station disappear behind the real globe.I love this part, as it makes the experience feel even more immersive.But wait, there is more! When I tap the globe again,I can see the inner core of the earth shown on top of my globe.Wow, this looks great!This is only one example I created using an object from my collection.But I’m sure you have objects of your own that you can bring to life, too.So let’s see how you can use Object Tracking for that.We’ve made it really simple to use Object Tracking in your app.There are just 3 steps you need to follow.First, you’ll need to provide a 3D modelrepresenting the real-world object that you want to track.There are some easy-to-use toolsto help you get a 3D model if you don’t have one yet.You’re going use it to train a machine learning model,required by Object Tracking.For the training part, you import this 3D asset into the Create ML app.Once training is completed, the result is a reference object,a new file type that we’re introducing for Object Tracking this year.Finally, you use this reference object in your appto anchor your virtual content and create your experience.There are several tools and frameworks available for youto enable this last step, and I will cover a few examples later in this session.So, let’s first talk about the 3D model needed for object tracking.As mentioned before, the Create ML apprequires a 3D asset to train a machine learning model for your object.For this, your 3D asset needs to be in the USDZ file format.To ensure best tracking quality,your asset should be as photorealistic as possible.Essentially you’re looking for a digital twin of your real world object.A simple way for you to obtain a photorealistic 3D modelis by using our Object Capture technology.All you need for this is an iPhone or an iPad.For objects that contain glossy or transparent partsyou may also provide a multi-material assetobtained from any other acquisition workflow.If you’re interested in learning more about the detailsand best practices for Object Capture,please checkout our “Meet Object Capture for iOS” session.Now let’s look at what objects are supported by Object Tracking.It works best for objects that are mostly stationary in the surrounding.Also, aim for objects that have a rigid appearance in both shape and texture.Lastly, your objects should be non-symmetrical,meaning that they have a distinct appearance from all views.Earlier, I showed you my globewhich works great because it has a non-symmetrical textureon top of it’s spherical shape.For capturing the globe, I removed the stand to ensureonly the rigid part of my object is considered for setting up the tracking.Let’s dive further into what it takes to make this workwith your objects and your content.I’ve already covered some of the basics,but I’ll go into much more detail in this next section.I’ll begin by showing you how to create a reference objectand then I’ll walk you through an example on how to use our existing toolsto anchor virtual content to your real world items.Let’s start with creating a reference object.As mentioned earlier,Object Tracking requires individual machine learning trainingfor each target object.We made this part really easy for you by integrating it in the Create ML app,which is the perfect fit for this task.All the machine learning training will run locally on your Mac.When you launch the Create ML app, you can choose from a variety of templates,such as Image Classification or 2D Object Detection.This year, we’re introducing a category called Spatialwith our new Object Tracking template.The training workflow has three simple steps:First, configure the training session with your USDZ assets.Then, train your ML model locally on your Mac.And finally, save the reference objectto build your spatial experience on Apple Vision Pro.Let’s see how this was done for the example that I’ve shownearlier in the talk.When creating a new project with the Object Tracking template,CreateML launches a training configuration view with an empty 3D viewport.My next step is to simply drag and drop my USDZ filefrom my desktop into this viewport.Here, I’m using the 3D asset of my globe that you’ve seen earlier.I find the 3D viewport very usefulbecause I can view my 3D model from different anglesand confirm that it actually matches the real-world object.It’s a good idea to verify that the scale displayed in the bottom right cornermatches the actual dimensions of the object.You can add multiple objects to your project to have them tracked,similar to the example I showed you earlier.I can simply click the plus-icon next to model sources in the left menuand import another USDZ asset.There is one more configuration step neededbefore I can start training my model,selecting the most suitable viewing angle for my items.This helps to optimize the Object Tracking experience,depending on the type of object, and how they’re typically placed in your space.For example, many stationary objects may only be seenfrom an upright or front position.I can utilize this information to guide the machine learning training,in order to achieve the best tracking quality.There are three viewing angle categories for you to chose from:All Angles, Upright, and Front.Let’s understand each of them in detail.You can find the viewing angle selection right below the 3D viewport.For the globe asset that I’ve added earlier,let’s take a look at the actual item.We can see that my globe is mounted on a standand I can rotate it around an axis that is not aligned with gravity.Because it can be seen from any angle,I select the “All Angles” option in my configuration view.When using this option,keep in mind that your object should have a distinct and unique appearancefrom all views in order to achieve high tracking quality.Here is another item from my collection,a microscope, which is expected to be standing on a surface,aligned with gravity.For that reason, I’m choosing the “Upright” setting,which excludes bottom viewing angles.Lastly, let’s take a look at my 3rd item - an oscilloscope.Again, I’m assuming this object will be standing on a surfacebut it also doesn’t need to be tracked from backside views.So I’m choosing the ‘Front’ option.This mode excludes backside and bottom viewing anglesto limit Object Tracking support to only what matters for my spatial experience.Notice that the 3D viewport displays a ground plane and a back planeas well as two axes pointing in the assumed up and frontdirection of the 3D model.If your 3D model appears to be in the wrong orientation,for example facing backwards, you can use Reality Composer Proto correct this before running the training.Now that my configuration is done, I’m ready to start the training.Going back to my project, I simply click the Train button in the top left corner.Training starts immediately and there is a progress barto help me keep track of my project status.Training a reference object can take a few hours.The exact duration depends on your Mac configuration.And please note that this is only supported on Apple Silicon Macs.Once the training is done,I can go to the output tab and save the resulting reference object.You can find the Create ML app in the developer tools menu in Xcode.Create ML works for a variety of tasksand it allows you to train machine models beyond Object Tracking.If you’re interested in learning more about Create ML,please check out our “What’s new in Create ML” talk.Now, let’s move on to anchoring virtual content to your reference objects.There are multiple ways how you can createan immersive experience with a tracked object.You can anchor your virtual content with Reality Composer Proand also use our new RealityKit and ARKit APIs.I like to begin my authoring process with Reality Composer Prowhich offers me an intuitive way to edit and place my virtual content.I’ll start off by creating a new Xcode project using the visionOS app template.This will automatically create a default scenethat I can open in Reality Composer Pro.Switching over to Reality Composer Pro.I’ll find the same default scene and I can delete the default sphere.In this scene, I’ll first create an empty Transform entityand add an anchoring component to it.This entity serves as the container for my object anchor.To facilitate object tracking,we've introduced a new target named "Object", which I'll go ahead choose.Next, I’ll import the reference object generated with Create MLand associate it with my AnchoringComponent.While I’m using Reality Composer Pro in this example,it’s worth noting that I can also use RealityKit APIsto create my AnchoringComponent at runtime.Moving on,you’ll notice that a semi-transparent visual cueof the original USDZ model appears in the viewport.This is especially helpful when I need to place content accuratelywith respect to specific parts of my target object.Let’s explore the scenes used in my globe experience.I’d like to show you how I’ve set this up to create some of the immersive effectsin this example.Remember the space shuttle that launched directly from my globe?Turns out I’ve chosen a distinct launch location:Cape Canaveral in Florida.Using the visual cue in my viewport makes it easy for me to locate this spoton the globe and setup my space shuttle entity.Onto another immersive effect,how did I make the virtual moon and space station disappearas they orbit around my globe?Let’s take a look!I’m using a timeline animation for the moon and space stationto make them orbit around the globe.My scene contains a separate USDZ globe entity attached as a child nodeto the Anchor Entity.This serves as the occluding shape in my scene.Since object tracking updates the transform of the parent anchor entity,the occluder will be aligned with the physical globe.To complete this part, I can apply an occlusion materialon the USDZ globe entity using the ShaderGraph editor.This makes the orbiting objects disappear once they move behind my globe entity.Finally, I’ve also added a tap gesture to this occluder entityto switch between the two experiences using the Behaviors component.Let’s checkout what I’ve built so far in action on Apple Vision Pro.I can tap to play my first animation that will launch my orbiting objectsand I can also see them disappear behind my globe.Great!And here is my second animationafter I tapped again - which works just as expected.Nice. But so far my app is missing a way to tell me how to start this experience,especially if I’m new to this, not knowing which objects I’m looking for.To improve this, I’ll add a coaching UI to my appthat displays a preview of the target objectup to the point when it gets detected by Object Tracking.And I’ll also add a virtual label to explain how to interact with my globe.The RealityKit APIprovides an extensive toolset to help me achieve all of this and more.Let me go over the steps to implement my coaching UI.First, I’d like to display a preview 3D modelto help me find the right object in my space.My coaching UI should react to changes in the AnchorState,so I need to check for that in my code.Once the object is tracked, I’d like show a transition where the displayed 3D modeltransforms to the position of the anchorEntity.After that, I’ll add a virtual label with instructions to tap on the globe to begin.This code sample showshow to display the 3D model of my target object in the coaching UI.I retrieve it from the reference object filewith a little help from the ARKit API,then load the USDZ file just like any other model entity.I’ve also set it’s opacity to 50% to indicate this is a preview entity.And lastly, add it to my scene for display.To know whether the object is being tracked,let’s first find the object anchor entityI had earlier created in Reality Composer Pro.Then in the update loop, I can check for the entity’s isAnchored flag stateand decide what to display in both cases.For my transition animation,I want the object preview to move towards the tracked object’s positionwhen tracking starts.For this, I need to get the anchor’s transform data.I use a SpatialTrackingSession to ask for the correct authorizations.Then I can access the transform of my object anchorand implement an animation that uses it.Finally, I’m adding a virtual label near the globetelling me how to start this experience.With RealityView attachments, it’s easy to place SwiftUI elementson RealityKit anchor entities.First, I define the SwiftUI elementsin the attachments section under RealityView.Then in the scene setup, I can find this UI entity and add it as a child noteof a reference transform previously defined in Reality Composer Pro.Let’s checkout these additions on Apple Vision Pro.This time, before my globe gets detected,the app displays a preview of the target objectfor me to know what I’m looking for.Once the tracking begins,this preview moves towards the target object, guiding my view to follow it.And I can see a virtual label to start the experience.Great!This is a good moment to wrap up my example.We’re also releasing a new ARKit API for Object Tracking this year.It gives you access to your tracked objects’ bounding boxesalong with the corresponding USDZ files, as we’ve seen in the previous section.The API delivers refined information about when your objects are ready to track,or whether there were any issues,so that your app can react to such events in a controlled manner.Along with the new API, we’re also publishing an object tracking sample appfor you to download and try out on your device.To learn more, please check out the“Create enhanced spatial computing experiences with ARKit” session.And that’s all there is to explore for object tracking on visionOS.Object tracking can unlock new use cases for spatial computingthat require precise placement of virtual content on real-world items.I’ve only shown you a few examples of what’s possible with this new technology,but there is so much more to discover.Reality Composer Pro and RealityKitoffer a large variety of exciting features for you to build great spatial experiencesbeyond what I’ve covered here.I highly recommend that you check out these sessions to learn more.My entire team and I are really excited to seewhich ideas and objects you will bring to life.

Hello, and welcome to “Explore object tracking for visionOS”.I’m Henning, an engineer on the Object Tracking team.Today, I’ll show you how to turn real world objectsinto virtual anchors that your visionOS appcan bring to life using our new Object Tracking technology.You may have already created spatial experiences using Reality Composer Proor our RealityKit and ARKit frameworks.You may also be familiar with the concept of anchoring virtual objectsin people’s surroundings.For example, your visionOS app can use our RealityKit and ARKit APIsto place content relative to planes, images, or hands.Anchors are a great starting point to immersive experiencesthat blur the line between what’s real and what’s virtual.

With object tracking, we’re now adding support for using real worlditems as anchors in your app.Imagine everyday objects that reveal useful informationonce you glance at them.Or household appliances and devices that already come with a virtual manual.Or collectables and toys that come to life and launch you intoan immersive storytelling experience.Sounds like magic? Well, let me show you!Here is a view of my space and I have a few items sitting on my desk.There is a globe, a microscope, and an oscilloscope.With Object Tracking, my app can get the position and orientationof each of these items, represented by a coordinate systemand bounding box as shown here in my example.Now that my app knows exactlywhere my objects are sitting in spatial coordinates,I can take this one step further and augment them with interesting content.

Here is the view of my globe again,but this time there is a virtual label attached to it,telling me to tap on the globe for more information.Doing that reveals some objects orbiting around my globe.Oh, and there is a space shuttle launching into my room!Notice how the virtual moon and the space station disappear behind the real globe.I love this part, as it makes the experience feel even more immersive.

But wait, there is more! When I tap the globe again,I can see the inner core of the earth shown on top of my globe.Wow, this looks great!This is only one example I created using an object from my collection.But I’m sure you have objects of your own that you can bring to life, too.So let’s see how you can use Object Tracking for that.

We’ve made it really simple to use Object Tracking in your app.

There are just 3 steps you need to follow.

First, you’ll need to provide a 3D modelrepresenting the real-world object that you want to track.There are some easy-to-use toolsto help you get a 3D model if you don’t have one yet.You’re going use it to train a machine learning model,required by Object Tracking.For the training part, you import this 3D asset into the Create ML app.Once training is completed, the result is a reference object,a new file type that we’re introducing for Object Tracking this year.

Finally, you use this reference object in your appto anchor your virtual content and create your experience.There are several tools and frameworks available for youto enable this last step, and I will cover a few examples later in this session.So, let’s first talk about the 3D model needed for object tracking.

As mentioned before, the Create ML apprequires a 3D asset to train a machine learning model for your object.For this, your 3D asset needs to be in the USDZ file format.To ensure best tracking quality,your asset should be as photorealistic as possible.Essentially you’re looking for a digital twin of your real world object.

A simple way for you to obtain a photorealistic 3D modelis by using our Object Capture technology.

All you need for this is an iPhone or an iPad.For objects that contain glossy or transparent partsyou may also provide a multi-material assetobtained from any other acquisition workflow.

If you’re interested in learning more about the detailsand best practices for Object Capture,please checkout our “Meet Object Capture for iOS” session.

Now let’s look at what objects are supported by Object Tracking.It works best for objects that are mostly stationary in the surrounding.Also, aim for objects that have a rigid appearance in both shape and texture.Lastly, your objects should be non-symmetrical,meaning that they have a distinct appearance from all views.Earlier, I showed you my globewhich works great because it has a non-symmetrical textureon top of it’s spherical shape.For capturing the globe, I removed the stand to ensureonly the rigid part of my object is considered for setting up the tracking.

Let’s dive further into what it takes to make this workwith your objects and your content.I’ve already covered some of the basics,but I’ll go into much more detail in this next section.I’ll begin by showing you how to create a reference objectand then I’ll walk you through an example on how to use our existing toolsto anchor virtual content to your real world items.

Let’s start with creating a reference object.

As mentioned earlier,Object Tracking requires individual machine learning trainingfor each target object.We made this part really easy for you by integrating it in the Create ML app,which is the perfect fit for this task.All the machine learning training will run locally on your Mac.

When you launch the Create ML app, you can choose from a variety of templates,such as Image Classification or 2D Object Detection.This year, we’re introducing a category called Spatialwith our new Object Tracking template.

The training workflow has three simple steps:First, configure the training session with your USDZ assets.Then, train your ML model locally on your Mac.And finally, save the reference objectto build your spatial experience on Apple Vision Pro.

Let’s see how this was done for the example that I’ve shownearlier in the talk.When creating a new project with the Object Tracking template,CreateML launches a training configuration view with an empty 3D viewport.My next step is to simply drag and drop my USDZ filefrom my desktop into this viewport.

Here, I’m using the 3D asset of my globe that you’ve seen earlier.I find the 3D viewport very usefulbecause I can view my 3D model from different anglesand confirm that it actually matches the real-world object.It’s a good idea to verify that the scale displayed in the bottom right cornermatches the actual dimensions of the object.

You can add multiple objects to your project to have them tracked,similar to the example I showed you earlier.I can simply click the plus-icon next to model sources in the left menuand import another USDZ asset.

There is one more configuration step neededbefore I can start training my model,selecting the most suitable viewing angle for my items.This helps to optimize the Object Tracking experience,depending on the type of object, and how they’re typically placed in your space.For example, many stationary objects may only be seenfrom an upright or front position.I can utilize this information to guide the machine learning training,in order to achieve the best tracking quality.

There are three viewing angle categories for you to chose from:All Angles, Upright, and Front.Let’s understand each of them in detail.You can find the viewing angle selection right below the 3D viewport.

For the globe asset that I’ve added earlier,let’s take a look at the actual item.

We can see that my globe is mounted on a standand I can rotate it around an axis that is not aligned with gravity.

Because it can be seen from any angle,I select the “All Angles” option in my configuration view.When using this option,keep in mind that your object should have a distinct and unique appearancefrom all views in order to achieve high tracking quality.Here is another item from my collection,a microscope, which is expected to be standing on a surface,aligned with gravity.For that reason, I’m choosing the “Upright” setting,which excludes bottom viewing angles.Lastly, let’s take a look at my 3rd item - an oscilloscope.Again, I’m assuming this object will be standing on a surfacebut it also doesn’t need to be tracked from backside views.So I’m choosing the ‘Front’ option.This mode excludes backside and bottom viewing anglesto limit Object Tracking support to only what matters for my spatial experience.

Notice that the 3D viewport displays a ground plane and a back planeas well as two axes pointing in the assumed up and frontdirection of the 3D model.

If your 3D model appears to be in the wrong orientation,for example facing backwards, you can use Reality Composer Proto correct this before running the training.

Now that my configuration is done, I’m ready to start the training.

Going back to my project, I simply click the Train button in the top left corner.Training starts immediately and there is a progress barto help me keep track of my project status.Training a reference object can take a few hours.The exact duration depends on your Mac configuration.And please note that this is only supported on Apple Silicon Macs.

Once the training is done,I can go to the output tab and save the resulting reference object.

You can find the Create ML app in the developer tools menu in Xcode.Create ML works for a variety of tasksand it allows you to train machine models beyond Object Tracking.If you’re interested in learning more about Create ML,please check out our “What’s new in Create ML” talk.

Now, let’s move on to anchoring virtual content to your reference objects.

There are multiple ways how you can createan immersive experience with a tracked object.You can anchor your virtual content with Reality Composer Proand also use our new RealityKit and ARKit APIs.I like to begin my authoring process with Reality Composer Prowhich offers me an intuitive way to edit and place my virtual content.

I’ll start off by creating a new Xcode project using the visionOS app template.

This will automatically create a default scenethat I can open in Reality Composer Pro.

Switching over to Reality Composer Pro.

I’ll find the same default scene and I can delete the default sphere.

In this scene, I’ll first create an empty Transform entityand add an anchoring component to it.

This entity serves as the container for my object anchor.To facilitate object tracking,we've introduced a new target named "Object", which I'll go ahead choose.

Next, I’ll import the reference object generated with Create MLand associate it with my AnchoringComponent.

While I’m using Reality Composer Pro in this example,it’s worth noting that I can also use RealityKit APIsto create my AnchoringComponent at runtime.Moving on,you’ll notice that a semi-transparent visual cueof the original USDZ model appears in the viewport.This is especially helpful when I need to place content accuratelywith respect to specific parts of my target object.Let’s explore the scenes used in my globe experience.I’d like to show you how I’ve set this up to create some of the immersive effectsin this example.

Remember the space shuttle that launched directly from my globe?Turns out I’ve chosen a distinct launch location:Cape Canaveral in Florida.

Using the visual cue in my viewport makes it easy for me to locate this spoton the globe and setup my space shuttle entity.Onto another immersive effect,how did I make the virtual moon and space station disappearas they orbit around my globe?Let’s take a look!I’m using a timeline animation for the moon and space stationto make them orbit around the globe.My scene contains a separate USDZ globe entity attached as a child nodeto the Anchor Entity.This serves as the occluding shape in my scene.Since object tracking updates the transform of the parent anchor entity,the occluder will be aligned with the physical globe.To complete this part, I can apply an occlusion materialon the USDZ globe entity using the ShaderGraph editor.

This makes the orbiting objects disappear once they move behind my globe entity.

Finally, I’ve also added a tap gesture to this occluder entityto switch between the two experiences using the Behaviors component.Let’s checkout what I’ve built so far in action on Apple Vision Pro.I can tap to play my first animation that will launch my orbiting objectsand I can also see them disappear behind my globe.

Great!And here is my second animationafter I tapped again - which works just as expected.

Nice. But so far my app is missing a way to tell me how to start this experience,especially if I’m new to this, not knowing which objects I’m looking for.To improve this, I’ll add a coaching UI to my appthat displays a preview of the target objectup to the point when it gets detected by Object Tracking.And I’ll also add a virtual label to explain how to interact with my globe.

The RealityKit APIprovides an extensive toolset to help me achieve all of this and more.Let me go over the steps to implement my coaching UI.First, I’d like to display a preview 3D modelto help me find the right object in my space.My coaching UI should react to changes in the AnchorState,so I need to check for that in my code.Once the object is tracked, I’d like show a transition where the displayed 3D modeltransforms to the position of the anchorEntity.After that, I’ll add a virtual label with instructions to tap on the globe to begin.

This code sample showshow to display the 3D model of my target object in the coaching UI.I retrieve it from the reference object filewith a little help from the ARKit API,then load the USDZ file just like any other model entity.I’ve also set it’s opacity to 50% to indicate this is a preview entity.And lastly, add it to my scene for display.

To know whether the object is being tracked,let’s first find the object anchor entityI had earlier created in Reality Composer Pro.Then in the update loop, I can check for the entity’s isAnchored flag stateand decide what to display in both cases.

For my transition animation,I want the object preview to move towards the tracked object’s positionwhen tracking starts.For this, I need to get the anchor’s transform data.I use a SpatialTrackingSession to ask for the correct authorizations.Then I can access the transform of my object anchorand implement an animation that uses it.

Finally, I’m adding a virtual label near the globetelling me how to start this experience.With RealityView attachments, it’s easy to place SwiftUI elementson RealityKit anchor entities.First, I define the SwiftUI elementsin the attachments section under RealityView.Then in the scene setup, I can find this UI entity and add it as a child noteof a reference transform previously defined in Reality Composer Pro.Let’s checkout these additions on Apple Vision Pro.

This time, before my globe gets detected,the app displays a preview of the target objectfor me to know what I’m looking for.Once the tracking begins,this preview moves towards the target object, guiding my view to follow it.And I can see a virtual label to start the experience.

Great!This is a good moment to wrap up my example.

We’re also releasing a new ARKit API for Object Tracking this year.It gives you access to your tracked objects’ bounding boxesalong with the corresponding USDZ files, as we’ve seen in the previous section.The API delivers refined information about when your objects are ready to track,or whether there were any issues,so that your app can react to such events in a controlled manner.

Along with the new API, we’re also publishing an object tracking sample appfor you to download and try out on your device.

To learn more, please check out the“Create enhanced spatial computing experiences with ARKit” session.And that’s all there is to explore for object tracking on visionOS.Object tracking can unlock new use cases for spatial computingthat require precise placement of virtual content on real-world items.I’ve only shown you a few examples of what’s possible with this new technology,but there is so much more to discover.Reality Composer Pro and RealityKitoffer a large variety of exciting features for you to build great spatial experiencesbeyond what I’ve covered here.I highly recommend that you check out these sessions to learn more.

My entire team and I are really excited to seewhich ideas and objects you will bring to life.

13:55 -Coaching UI - display object USDZ preview

14:13 -Coaching UI - check anchor state

14:31 -Coaching UI - Transform space with SpatialSession

## Code Samples

```swift
// Display object USDZ


struct
 
ImmersiveView
: 
View
 {
   
@State
 
var
 globeAnchor: 
Entity
? 
=
 
nil

    
var
 body: 
some
 
View
 {
        
RealityView
 { content 
in

            
// Load the reference object with ARKit API

            
let
 refObjURL 
=
 
            
Bundle
.main.url(forResource: 
"globe"
, withExtension: 
".referenceobject"
)
            
let
 refObject 
=
 
try?
 
await
 
ReferenceObject
(from: refObjURL
!
)

            
// Load the model entity with USDZ path extracted from reference object

            
let
 globePreviewEntity 
=
 
            
try?
 
await
 
Entity
.
init
(contentsOf: (refObject
?
.usdzFile)
!
)

            
// Set opacity to 0.5 and add to scene

            globePreviewEntity
!
.components.set(
OpacityComponent
(opacity: 
0.5
))
            content.add(globePreviewEntity
!
)
        }
    }
}
```

```swift
// Check anchor state


struct
 
ImmersiveView
: 
View
 {
   
@State
 
var
 globeAnchor: 
Entity
? 
=
 
nil

    
var
 body: 
some
 
View
 {
        
RealityView
 { content 
in

            
if
 
let
 scene 
=
 
try?
 
await
 
Entity
(named: 
"Immersive"
, in: realityKitContentBundle) {
                globeAnchor 
=
 scene.findEntity(named: 
"GlobeAnchor"
)
                content.add(scene)
            }
            
let
 updateSub 
=
 content.subscribe(to: 
SceneEvents
.
AnchoredStateChanged
.
self
) { event 
in

                
if
 
let
 anchor 
=
 globeAnchor, event.anchor 
==
 anchor {
                    
if
 event.isAnchored {
                        
// Object anchor found, trigger transition animation

                    } 
else
 {
                        
// Object anchor not found, display coaching UI

                    }
                }
            }
        }
    }
}
```

```swift
// Transform space


struct
 
ImmersiveView
: 
View
 {
   
@State
 
var
 globeAnchor: 
Entity
? 
=
 
nil

    
var
 body: 
some
 
View
 {
        
RealityView
 { content 
in

            
// Setup anchor transform space for object and world anchor

            
let
 trackingSession 
=
 
SpatialTrackingSession
()
            
let
 config 
=
 
SpatialTrackingSession
.
Configuration
(tracking: [.object, .world])
            
if
 
let
 result 
=
 
await
 trackingSession.run(config) {
                
if
 result.anchor.contains(.object) {
                    
// Tracking not authorized, adjust experience accordingly

                }
            }
           
// Get tracked object's world transform, identity if tracking not authorized

            
let
 objectTransform 
=
 globeAnchor
?
.transformMatrix(relativeTo: 
nil
)
            
// Implement animation

            
...

        }
    }
}
```

