WEBVTT

00:00:01.602 --> 00:00:03.604
Jason Fielder: Hi,
my name's Jason Fielder,

00:00:03.604 --> 00:00:07.241
and I'm with GPU Software
Engineering team here at Apple.

00:00:07.241 --> 00:00:08.976
We're going to learn
how to take advantage

00:00:08.976 --> 00:00:11.478
of the awesome graphics
processing capabilities

00:00:11.478 --> 00:00:14.648
of our new M1 Pro
and M1 Max notebooks,

00:00:14.648 --> 00:00:17.217
and we'll be exploring the best
practices you can adopt

00:00:17.217 --> 00:00:22.389
to make your applications
perform great on our GPU.

00:00:22.389 --> 00:00:25.158
Our latest MacBook Pros
feature the most powerful chips

00:00:25.158 --> 00:00:27.160
we have ever created.

00:00:27.160 --> 00:00:30.397
The M1 Pro has
up to 16 GPU cores,

00:00:30.397 --> 00:00:33.867
and the M1 Max
doubles that to 32.

00:00:33.867 --> 00:00:37.137
This, coupled with much higher
DRAM bandwidth,

00:00:37.137 --> 00:00:38.772
greatly increases
the performance

00:00:38.772 --> 00:00:40.474
of the MacBook Pro.

00:00:40.474 --> 00:00:42.776
The system memory
is now available to the GPU

00:00:42.776 --> 00:00:45.078
thanks to our
Unified Memory Architecture,

00:00:45.078 --> 00:00:47.748
and with up to 64GB available,

00:00:47.748 --> 00:00:50.384
our GPU application is able
to access more memory

00:00:50.384 --> 00:00:53.120
than ever before.

00:00:53.120 --> 00:00:55.656
These MacBook Pros will open up
a whole new world

00:00:55.656 --> 00:00:58.792
of possibilities for developers
and creative pros,

00:00:58.792 --> 00:01:01.461
enabling workflows that were
previously only targeting

00:01:01.461 --> 00:01:04.197
desktop machines.

00:01:04.197 --> 00:01:07.601
So how do we open up this
new world of GPU potential?

00:01:07.601 --> 00:01:09.870
We'll start with a recap
of Metal Compute,

00:01:09.870 --> 00:01:12.940
which enables work
to be scheduled on the GPU.

00:01:12.940 --> 00:01:14.274
Then, with an understanding

00:01:14.274 --> 00:01:16.643
of the API and the GPU
architecture,

00:01:16.643 --> 00:01:18.145
we will look at
the best practices

00:01:18.145 --> 00:01:20.347
for each stage of your app.

00:01:20.347 --> 00:01:23.617
Then we'll conclude with some
specific kernel optimizations

00:01:23.617 --> 00:01:25.519
that you can apply.

00:01:25.519 --> 00:01:29.022
Let's start with a quick refresh
of Metal Compute.

00:01:29.022 --> 00:01:31.725
Metal is Apple's modern
low-overhead API

00:01:31.725 --> 00:01:34.361
for performing GPU work.

00:01:34.361 --> 00:01:37.297
It is designed to be as thin
and as efficient as possible

00:01:37.297 --> 00:01:41.034
and provides a unified graphics
and compute interface.

00:01:41.034 --> 00:01:43.603
Metal is multithread friendly,
enabling work to be

00:01:43.603 --> 00:01:46.206
trivially queued from multiple
CPU threads,

00:01:46.206 --> 00:01:48.475
and gives you, the developer,
flexibility to use

00:01:48.475 --> 00:01:53.313
an offline or online
shader compilation pipeline.

00:01:53.313 --> 00:01:55.082
Under the hood
in our hardware layer,

00:01:55.082 --> 00:01:56.683
we have a CPU and GPU

00:01:56.683 --> 00:01:59.353
both connected
to the same physical memory.

00:01:59.353 --> 00:02:03.223
The CPU creates GPU resources
within the unified memory block,

00:02:03.223 --> 00:02:04.958
and both the GPU and CPU

00:02:04.958 --> 00:02:09.229
are able to read and write
to these resources.

00:02:09.229 --> 00:02:11.365
We've a few layers
of API to work through

00:02:11.365 --> 00:02:14.201
to see a kernel executed
on the GPU.

00:02:14.201 --> 00:02:16.603
The top layer
is the command queue.

00:02:16.603 --> 00:02:19.539
As per its name, this object
allows an application

00:02:19.539 --> 00:02:23.810
to queue work for execution
at some points on the GPU.

00:02:23.810 --> 00:02:26.446
The commands
are batched up on the CPU

00:02:26.446 --> 00:02:28.181
through a command buffer.

00:02:28.181 --> 00:02:31.151
These objects are transient,
and you'll create a lot of these

00:02:31.151 --> 00:02:33.887
at the granularity that
makes sense for your app.

00:02:33.887 --> 00:02:36.189
You may find this is imposed
by requirements around

00:02:36.189 --> 00:02:38.191
CPU and GPU
synchronization,

00:02:38.191 --> 00:02:41.094
but in short, you'll want
to ensure you have enough work

00:02:41.094 --> 00:02:44.431
to keep the GPU fully busy.

00:02:44.431 --> 00:02:46.733
To put instructions
into the command buffer,

00:02:46.733 --> 00:02:48.535
we'll need
a command encoder.

00:02:48.535 --> 00:02:50.270
There are different types
of command encoders

00:02:50.270 --> 00:02:52.039
targeting different types
of work.

00:02:52.039 --> 00:02:54.207
There's a graphics encoder
for 3D draws,

00:02:54.207 --> 00:02:56.643
a blit encoder for copying
resources around,

00:02:56.643 --> 00:02:59.613
but for this talk, we'll focus
on the compute encoder

00:02:59.613 --> 00:03:01.915
for kernel dispatches.

00:03:01.915 --> 00:03:03.850
With a compute encoder in place,

00:03:03.850 --> 00:03:06.653
we're now ready to encode
a kernel dispatch.

00:03:06.653 --> 00:03:08.555
Along with the kernel
function itself,

00:03:08.555 --> 00:03:10.891
the encoder is how resources
needed by the kernel

00:03:10.891 --> 00:03:12.392
are bound to it.

00:03:12.392 --> 00:03:15.328
In fact we can encode
multiple kernel dispatches

00:03:15.328 --> 00:03:16.930
to the same encoder.

00:03:16.930 --> 00:03:18.165
We can change kernel,

00:03:18.165 --> 00:03:20.567
or bound resources
between each dispatch

00:03:20.567 --> 00:03:22.736
and also inform Metal
whether a dispatch

00:03:22.736 --> 00:03:24.871
can be executed concurrently

00:03:24.871 --> 00:03:26.873
or should be serialized
and executed

00:03:26.873 --> 00:03:30.744
after the previous dispatch
has completed.

00:03:30.744 --> 00:03:32.279
With our encoding complete,

00:03:32.279 --> 00:03:34.347
we end the encoder
which makes the command buffer

00:03:34.347 --> 00:03:37.150
available to either start
encoding a new encoder

00:03:37.150 --> 00:03:41.922
or to commit the command buffer
to the GPU for execution.

00:03:41.922 --> 00:03:44.724
Here we've encoded a total
of three compute encoders

00:03:44.724 --> 00:03:46.226
to the GPU.

00:03:46.226 --> 00:03:48.829
This represents a body of work
from start to finish,

00:03:48.829 --> 00:03:52.899
and we're ready to instruct
the GPU to start executing it.

00:03:52.899 --> 00:03:55.035
A call to commit
returns immediately,

00:03:55.035 --> 00:03:57.337
and Metal will ensure
that the work is scheduled

00:03:57.337 --> 00:03:59.039
and executed on the GPU

00:03:59.039 --> 00:04:02.676
once all other work before it
in the queue has completed.

00:04:02.676 --> 00:04:04.444
The CPU thread is now available

00:04:04.444 --> 00:04:06.480
to start building
a new command buffer,

00:04:06.480 --> 00:04:08.615
or perform any other app work
that's appropriate

00:04:08.615 --> 00:04:10.951
while the GPU is busy.

00:04:10.951 --> 00:04:12.752
The CPU is likely going
to need to know

00:04:12.752 --> 00:04:15.021
when a body of work has been
completed, though,

00:04:15.021 --> 00:04:18.391
so that its results
can be read back.

00:04:18.391 --> 00:04:21.495
For this, the command buffer
has two techniques for us.

00:04:21.495 --> 00:04:23.697
Here, before committing
the work,

00:04:23.697 --> 00:04:26.233
the application can add
a completion handler function

00:04:26.233 --> 00:04:29.369
that will be invoked by Metal
once the work has completed.

00:04:29.369 --> 00:04:30.804
For simple cases,

00:04:30.804 --> 00:04:33.607
there is a synchronous
method called waitUntilComplete

00:04:33.607 --> 00:04:35.842
that will block
the calling CPU thread,

00:04:35.842 --> 00:04:38.879
but here we're using
the asynchronous method.

00:04:38.879 --> 00:04:41.581
So this is our basic
execution model.

00:04:41.581 --> 00:04:45.118
One final feature of the API
is that multiple command buffers

00:04:45.118 --> 00:04:47.921
can be encoded simultaneously.

00:04:47.921 --> 00:04:49.256
Multiple CPU threads

00:04:49.256 --> 00:04:51.525
can encode multiple
command buffers at once,

00:04:51.525 --> 00:04:54.895
and commit the work
once encoding is complete.

00:04:54.895 --> 00:04:56.563
If ordering is important,

00:04:56.563 --> 00:04:59.766
either reserve the command
buffer's place for execution

00:04:59.766 --> 00:05:01.401
in the command queue
by calling enqueue,

00:05:01.401 --> 00:05:03.436
or alternatively,
simply call commit

00:05:03.436 --> 00:05:05.272
in the desired order.

00:05:05.272 --> 00:05:08.508
Use whichever approach
fits your application best.

00:05:08.508 --> 00:05:11.845
With the possibility of creating
multiple command queues too,

00:05:11.845 --> 00:05:16.183
the flexibility of Metal enables
an app to encode work to the GPU

00:05:16.183 --> 00:05:20.053
in a pattern most efficient
for its needs.

00:05:20.053 --> 00:05:23.523
So that's the recap of the
execution model Metal exposes.

00:05:23.523 --> 00:05:24.724
Let's build on that

00:05:24.724 --> 00:05:28.295
and take a look
at how to optimize for it.

00:05:28.295 --> 00:05:29.863
We've a handful
of recommendations

00:05:29.863 --> 00:05:31.731
on how your app
should access GPU memory

00:05:31.731 --> 00:05:34.935
to take advantage of
our Unified Memory Architecture,

00:05:34.935 --> 00:05:38.205
how to submit work to the GPU
to align with the compute model

00:05:38.205 --> 00:05:40.640
we just went over,
and how to choose

00:05:40.640 --> 00:05:45.445
which resources to allocate
to align best with our UMA.

00:05:45.445 --> 00:05:47.514
First up to talk about
is definitely

00:05:47.514 --> 00:05:50.183
the Unified Memory Architecture.

00:05:50.183 --> 00:05:52.619
This set of best practices
is about minimizing

00:05:52.619 --> 00:05:55.922
the amount of work
necessary on the GPU.

00:05:55.922 --> 00:05:57.724
With a Unified Memory
Architecture,

00:05:57.724 --> 00:06:00.827
the traditional management
of copies between system RAM

00:06:00.827 --> 00:06:02.896
and video RAM goes away.

00:06:02.896 --> 00:06:05.665
Metal exposes the UMA
through shared resources

00:06:05.665 --> 00:06:11.137
that allow the GPU and CPU to
read and write the same memory.

00:06:11.137 --> 00:06:13.940
Resource management is then
about synchronizing the access

00:06:13.940 --> 00:06:18.111
between the CPU and GPU to
happen safely at the right time,

00:06:18.111 --> 00:06:20.213
rather than duplicating
or shadowing data

00:06:20.213 --> 00:06:22.983
between system memory
and video memory.

00:06:22.983 --> 00:06:26.019
Working from a single instance
of a resource in memory

00:06:26.019 --> 00:06:28.688
drastically reduces the
requirements on memory bandwidth

00:06:28.688 --> 00:06:33.560
your app may have, enabling
large performance gains.

00:06:33.560 --> 00:06:35.462
Where there is
possible contention --

00:06:35.462 --> 00:06:37.998
such as the CPU
needing to update a buffer

00:06:37.998 --> 00:06:39.866
for a second batch
of work while the GPU

00:06:39.866 --> 00:06:42.068
is still executing
the first batch --

00:06:42.068 --> 00:06:45.972
an explicit multi-buffer model
is necessary.

00:06:45.972 --> 00:06:48.275
The CPU prepares content
in buffer n,

00:06:48.275 --> 00:06:51.177
while the GPU is reading
from buffer n-1

00:06:51.177 --> 00:06:53.980
and then increments n
for the next batch.

00:06:53.980 --> 00:06:56.116
This allows you
as the app developer

00:06:56.116 --> 00:06:58.885
to tune for your memory
overheads and access patterns,

00:06:58.885 --> 00:07:04.557
and avoid unnecessary
CPU/GPU stalls or copies.

00:07:04.557 --> 00:07:08.094
The limit on amount of GPU
resources an app can allocate

00:07:08.094 --> 00:07:10.563
has two values to be aware of.

00:07:10.563 --> 00:07:13.500
The total amount of GPU
resources that can be allocated,

00:07:13.500 --> 00:07:15.568
and more critically,

00:07:15.568 --> 00:07:17.704
the amount of memory
a single command encoder

00:07:17.704 --> 00:07:20.507
can reference at any one time.

00:07:20.507 --> 00:07:24.377
This limit is known
as the working set limit.

00:07:24.377 --> 00:07:26.579
It can be fetched from
the Metal device at runtime

00:07:26.579 --> 00:07:30.450
through reading
recommendedMaxWorkingSetSize.

00:07:30.450 --> 00:07:32.519
We recommend you
make use of this in your app

00:07:32.519 --> 00:07:34.554
to help control how much memory
you look to use

00:07:34.554 --> 00:07:37.157
and rely on being available.

00:07:37.157 --> 00:07:40.093
While a single command encoder
has this working set limit,

00:07:40.093 --> 00:07:43.363
Metal's able to allocate
further resources beyond this.

00:07:43.363 --> 00:07:47.300
Metal manages the residency
of these resources for you,

00:07:47.300 --> 00:07:49.302
and just like
system memory allocations,

00:07:49.302 --> 00:07:52.238
the GPU allocations
are also virtually allocated

00:07:52.238 --> 00:07:55.542
and made resident
before execution.

00:07:55.542 --> 00:07:57.444
By breaking up
your resource usage

00:07:57.444 --> 00:07:59.279
across multiple
command encoders,

00:07:59.279 --> 00:08:01.481
an application can use
total resources

00:08:01.481 --> 00:08:04.150
in excess
of the working set size

00:08:04.150 --> 00:08:06.052
and avoid
the traditional constraints

00:08:06.052 --> 00:08:10.724
associated with
hard VRAM limits.

00:08:10.724 --> 00:08:12.359
For the new MacBook Pros,

00:08:12.359 --> 00:08:16.196
the GPU working set size
is shown in this table.

00:08:16.196 --> 00:08:21.201
Now, for an M1 Pro or M1 Max
with 32GB of system RAM,

00:08:21.201 --> 00:08:24.938
the GPU can access
21GB of memory,

00:08:24.938 --> 00:08:28.608
and for an M1 Max
with 64GB of RAM,

00:08:28.608 --> 00:08:32.812
the GPU can access
48GB of memory.

00:08:32.812 --> 00:08:35.148
This is by far the highest
amount of memory

00:08:35.148 --> 00:08:37.817
we've ever made available
to the GPU in a Mac,

00:08:37.817 --> 00:08:39.419
and the new MacBook Pro lineup

00:08:39.419 --> 00:08:42.889
offers greatly expanded
capabilities to users.

00:08:42.889 --> 00:08:45.191
We're really excited to see
what experiences

00:08:45.191 --> 00:08:47.527
you'll be able
to empower users with

00:08:47.527 --> 00:08:51.865
and to experience
what they'll create with it.

00:08:51.865 --> 00:08:54.367
That's our best practices
for working with UMA,

00:08:54.367 --> 00:08:58.271
and we're ready
for our next topic.

00:08:58.271 --> 00:08:59.672
At the command buffer level,

00:08:59.672 --> 00:09:02.175
there's latency
with submitting it.

00:09:02.175 --> 00:09:03.309
Small amounts of work

00:09:03.309 --> 00:09:06.780
can lead to more time
spent waiting than working.

00:09:06.780 --> 00:09:08.615
Look to batch
more encoders together

00:09:08.615 --> 00:09:13.086
into each command buffer before
making that call to commit.

00:09:13.086 --> 00:09:15.488
If your app spends time
waiting for GPU results

00:09:15.488 --> 00:09:18.024
to inform what should be
dispatched next,

00:09:18.024 --> 00:09:21.094
then bubbles will appear
in the GPU timeline.

00:09:21.094 --> 00:09:23.363
In these bubbles,
the GPU is sitting idle,

00:09:23.363 --> 00:09:26.466
waiting for the next
dispatch to arrive.

00:09:26.466 --> 00:09:29.536
To hide this, consider using
multiple CPU threads

00:09:29.536 --> 00:09:31.538
working on multiple pieces
of work

00:09:31.538 --> 00:09:33.706
and keep the GPU busy.

00:09:33.706 --> 00:09:35.842
Either by creating
multiple command buffers,

00:09:35.842 --> 00:09:40.847
or by creating multiple
command queues.

00:09:40.847 --> 00:09:42.916
For the kernel dispatches
themselves,

00:09:42.916 --> 00:09:46.219
the GPU is kept busy by having
enough threads to work on,

00:09:46.219 --> 00:09:47.687
and with enough work
within each thread

00:09:47.687 --> 00:09:50.957
to justify the overheads
of launching it.

00:09:50.957 --> 00:09:53.226
In our image processing
example here,

00:09:53.226 --> 00:09:57.530
each pixel is processed
by a single thread.

00:09:57.530 --> 00:10:00.433
When you can, increase
the number of total threads

00:10:00.433 --> 00:10:03.403
in the kernel dispatches
to ensure all processing cores

00:10:03.403 --> 00:10:06.072
of the GPU can be utilized.

00:10:06.072 --> 00:10:08.007
Here, a single kernel dispatch

00:10:08.007 --> 00:10:10.143
is used to process
the entire image

00:10:10.143 --> 00:10:13.246
allowing Metal and the GPU
to optimally distribute the work

00:10:13.246 --> 00:10:17.450
across all available
processing cores.

00:10:17.450 --> 00:10:20.920
Finally, where you do have
and need smaller thread counts,

00:10:20.920 --> 00:10:22.856
use the concurrent
dispatch model

00:10:22.856 --> 00:10:25.358
instead of the default
serialize model.

00:10:25.358 --> 00:10:28.361
We've observed many applications
that run great on M1,

00:10:28.361 --> 00:10:30.096
but fall short
of their potential

00:10:30.096 --> 00:10:32.098
on M1 Pro and M1 Max.

00:10:32.098 --> 00:10:35.602
Submitting work in larger
volumes using these techniques

00:10:35.602 --> 00:10:38.438
is an easy way
for your application to scale

00:10:38.438 --> 00:10:43.142
and reach its potential.

00:10:43.142 --> 00:10:47.113
The next consideration I want
to talk about are L1 caches.

00:10:47.113 --> 00:10:50.283
Apple Silicon GPUs
contain separate L1 caches

00:10:50.283 --> 00:10:53.186
for texture reads
and buffer reads.

00:10:53.186 --> 00:10:57.056
With Metal being a unified API
across graphics and compute,

00:10:57.056 --> 00:10:59.392
the full suite of texture
objects and samplers

00:10:59.392 --> 00:11:02.295
are available to apps.

00:11:02.295 --> 00:11:04.063
So if your application
is only using buffers

00:11:04.063 --> 00:11:05.832
for its data sources,

00:11:05.832 --> 00:11:07.634
there are performance benefits
to be had

00:11:07.634 --> 00:11:10.870
from moving some of these
resources to textures.

00:11:10.870 --> 00:11:13.506
This will allow
better utilization of the GPU's

00:11:13.506 --> 00:11:16.643
high performance caches,
reduce traffic from RAM,

00:11:16.643 --> 00:11:19.812
and increase performance.

00:11:19.812 --> 00:11:21.981
Let's see what that looks like.

00:11:21.981 --> 00:11:25.451
While the GPU accesses RAM
for reads of all resources,

00:11:25.451 --> 00:11:27.153
there's a cache for improving
the performance

00:11:27.153 --> 00:11:31.424
of future buffer reads to
the same local area of memory.

00:11:31.424 --> 00:11:34.594
The cache is of a limited size,
though, and will fill quickly,

00:11:34.594 --> 00:11:36.963
so older data that hasn't been
read for a while

00:11:36.963 --> 00:11:41.000
will be discarded
to make way for newer reads.

00:11:41.000 --> 00:11:44.070
Hypothetically, if our kernel
operated on a small enough

00:11:44.070 --> 00:11:47.140
set of data,
once the cache is populated,

00:11:47.140 --> 00:11:49.108
all future reads
would hit the cache

00:11:49.108 --> 00:11:51.878
and complete with no stalls
or delays caused by

00:11:51.878 --> 00:11:54.914
waiting for a system memory
loads to complete.

00:11:54.914 --> 00:11:57.850
The bandwidth to the cache
is significantly higher,

00:11:57.850 --> 00:12:02.855
and with lower latency
than of that to system RAM.

00:12:02.855 --> 00:12:06.025
When reads miss the cache,
the calling threads will stall

00:12:06.025 --> 00:12:07.560
while the read
is fetched from RAM

00:12:07.560 --> 00:12:09.596
and placed into the cache.

00:12:09.596 --> 00:12:12.465
Data reads become limited
by the system memory bandwidth

00:12:12.465 --> 00:12:15.234
rather than the on-chip cache
bandwidth.

00:12:15.234 --> 00:12:17.804
A kernel accessing large
amounts of data from buffers

00:12:17.804 --> 00:12:19.672
can thrash the cache
in this way

00:12:19.672 --> 00:12:22.709
and result in reduced
performance.

00:12:22.709 --> 00:12:25.445
Apple silicon GPUs
contains a second cache

00:12:25.445 --> 00:12:27.313
alongside the buffer cache

00:12:27.313 --> 00:12:30.350
that's dedicated
to texture reads.

00:12:30.350 --> 00:12:32.652
Applications can move
some of their source data

00:12:32.652 --> 00:12:35.989
from Metal buffer objects
to Metal texture objects,

00:12:35.989 --> 00:12:38.424
and effectively increase
the amount of cache space

00:12:38.424 --> 00:12:41.494
and increase performance.

00:12:41.494 --> 00:12:43.963
Also, texture data
can be twiddled,

00:12:43.963 --> 00:12:47.367
and Metal will do this for you
automatically on upload.

00:12:47.367 --> 00:12:50.269
Twiddling means the texels
are ordered more optimally

00:12:50.269 --> 00:12:52.071
for a random access pattern

00:12:52.071 --> 00:12:54.607
and can help improve
cache efficiency further,

00:12:54.607 --> 00:12:57.977
giving another performance gain
over a regular buffer.

00:12:57.977 --> 00:13:00.346
This is transparent
to the kernel when reading,

00:13:00.346 --> 00:13:03.916
so it doesn't add complexity
to your kernel source.

00:13:03.916 --> 00:13:06.486
Actually, textures are
a gift that keeps on giving.

00:13:06.486 --> 00:13:08.287
Apple Silicon can also perform

00:13:08.287 --> 00:13:10.590
a lossless compression
of a texture --

00:13:10.590 --> 00:13:12.859
after it's been created
and when possible --

00:13:12.859 --> 00:13:15.395
to further reduce the memory
bandwidth of reading from it,

00:13:15.395 --> 00:13:18.331
and again, increase performance.

00:13:18.331 --> 00:13:21.434
This, too, is transparent
to the shader kernel

00:13:21.434 --> 00:13:23.403
as the decompression
happens automatically

00:13:23.403 --> 00:13:25.772
on the texture read or sample.

00:13:25.772 --> 00:13:28.074
A Metal texture will be
compressed by default

00:13:28.074 --> 00:13:31.444
if it's private to the GPU,
but shared and managed textures

00:13:31.444 --> 00:13:33.880
can explicitly be compressed
after upload

00:13:33.880 --> 00:13:37.450
through a call to
optimizeContentsForGPUAccess

00:13:37.450 --> 00:13:41.320
on a blit command encoder.

00:13:41.320 --> 00:13:43.623
For lossless texture compression
to be available,

00:13:43.623 --> 00:13:45.558
textures need
to have their usage set

00:13:45.558 --> 00:13:48.227
to either shaderRead
or renderTarget.

00:13:48.227 --> 00:13:49.896
Ensure this is set
on your descriptor

00:13:49.896 --> 00:13:53.199
when creating
the texture object.

00:13:53.199 --> 00:13:56.235
And if your texture data
is actual image data,

00:13:56.235 --> 00:13:58.371
or being used in a way
where lossy compression

00:13:58.371 --> 00:13:59.739
would be acceptable,

00:13:59.739 --> 00:14:03.142
then consider higher
ratio lossy compression formats

00:14:03.142 --> 00:14:06.179
such as ASTC or BC.

00:14:06.179 --> 00:14:08.414
This will further reduce
both the memory footprint

00:14:08.414 --> 00:14:10.450
and bandwidth utilization,

00:14:10.450 --> 00:14:13.152
increasing the kernel's
performance.

00:14:13.152 --> 00:14:17.924
BC and ASTC can both be
generated using offline tools,

00:14:17.924 --> 00:14:19.659
giving a great image quality,

00:14:19.659 --> 00:14:22.562
and have compression ratios
ranging from 4:1

00:14:22.562 --> 00:14:25.465
through to 36:1.

00:14:25.465 --> 00:14:27.400
With our work
now batched optimally,

00:14:27.400 --> 00:14:30.236
utilizing buffers and textures
for our data input,

00:14:30.236 --> 00:14:31.871
and being UMA aware

00:14:31.871 --> 00:14:34.640
to reduce the amount
of copy work we're performing,

00:14:34.640 --> 00:14:37.577
we're ready to look
at kernel optimizations.

00:14:37.577 --> 00:14:38.745
All of these best practices

00:14:38.745 --> 00:14:41.881
are aimed at increasing
your kernel's performance.

00:14:41.881 --> 00:14:44.617
Let's take a look
at some of them.

00:14:44.617 --> 00:14:46.686
I'm going to focus
on memory indexing,

00:14:46.686 --> 00:14:48.488
global atomics, and occupancy

00:14:48.488 --> 00:14:51.124
as areas of opportunity
in your kernels.

00:14:51.124 --> 00:14:54.427
We'll also take a look at where
to look in our profiling tools

00:14:54.427 --> 00:14:56.496
to understand
your kernel's bottlenecks

00:14:56.496 --> 00:14:58.131
and how to measure
any improvements

00:14:58.131 --> 00:15:01.667
any optimizations
may have.

00:15:01.667 --> 00:15:05.505
At last year's WWDC,
our GPU software team

00:15:05.505 --> 00:15:08.141
released a video on
Metal optimization techniques

00:15:08.141 --> 00:15:09.909
for Apple silicon.

00:15:09.909 --> 00:15:12.612
I'll recap briefly some
of that talk's content here,

00:15:12.612 --> 00:15:14.413
but for complete details
and examples,

00:15:14.413 --> 00:15:18.551
please do watch
that presentation.

00:15:18.551 --> 00:15:22.255
First up, I want to talk
about memory indexing.

00:15:22.255 --> 00:15:23.790
When indexing into an array,

00:15:23.790 --> 00:15:27.527
use signed integer types
over unsigned types.

00:15:27.527 --> 00:15:28.828
Here we have a for loop,

00:15:28.828 --> 00:15:32.598
with count variable i
declared as unsigned.

00:15:32.598 --> 00:15:34.734
Due to the wrapping
characteristics of uint

00:15:34.734 --> 00:15:38.671
in the shader language spec,
this disables vectorized loads.

00:15:38.671 --> 00:15:40.406
Usually, this is not
what you want,

00:15:40.406 --> 00:15:42.575
and the extra code generated
can be avoided

00:15:42.575 --> 00:15:45.144
by using a signed type.

00:15:45.144 --> 00:15:49.148
And here, as the wrapping
behavior of int is undefined,

00:15:49.148 --> 00:15:50.449
the load will be vectorized

00:15:50.449 --> 00:15:54.854
and likely give
a performance improvement.

00:15:54.854 --> 00:15:56.823
With the increase of GPU cores
and memory bandwidth

00:15:56.823 --> 00:15:58.858
in the new MacBook Pros,
we have seen

00:15:58.858 --> 00:16:01.494
the primary bottlenecks
of some GPU workloads

00:16:01.494 --> 00:16:06.265
shift from ALU or memory
bandwidth usage to other areas.

00:16:06.265 --> 00:16:08.868
One of those areas
is global atomics.

00:16:08.868 --> 00:16:10.503
Our recommendation
is to minimize

00:16:10.503 --> 00:16:13.239
the use of atomic operations
in your kernels,

00:16:13.239 --> 00:16:17.109
or use techniques built around
thread-group atomics instead.

00:16:17.109 --> 00:16:19.278
As with all good optimization
workflows,

00:16:19.278 --> 00:16:21.047
profile your shaders first
to understand

00:16:21.047 --> 00:16:23.282
if this is a problem
you're experiencing,

00:16:23.282 --> 00:16:27.153
as moderate use of atomics
won't be a problem.

00:16:27.153 --> 00:16:30.323
So how do we get this
vital profiling information?

00:16:30.323 --> 00:16:33.693
By using the GPU frame
debugger inside Xcode.

00:16:33.693 --> 00:16:36.796
It is a great tool
for this evaluation work.

00:16:36.796 --> 00:16:38.598
It provides a wealth
of insight into the work

00:16:38.598 --> 00:16:41.300
occurring on the GPU
and once we have our capture,

00:16:41.300 --> 00:16:43.336
we're able to browse it.

00:16:43.336 --> 00:16:46.205
The timeline view gives us a
great overview of our workload,

00:16:46.205 --> 00:16:48.007
and shows us graphs
visualizing

00:16:48.007 --> 00:16:52.278
the major performance counters
of the GPU.

00:16:52.278 --> 00:16:54.680
Many of these counters
give both a utilization

00:16:54.680 --> 00:16:56.415
and a limiter value.

00:16:56.415 --> 00:16:58.584
Using ALU as an example here,

00:16:58.584 --> 00:17:00.453
the utilization number
is telling us

00:17:00.453 --> 00:17:02.722
that the kernel used
about 27 percent

00:17:02.722 --> 00:17:07.660
of the GPU's ALU capabilities
during execution.

00:17:07.660 --> 00:17:09.762
Other time was spent
doing other tasks,

00:17:09.762 --> 00:17:11.497
such as data reads and writes,

00:17:11.497 --> 00:17:14.533
making control logic decisions,
et cetera.

00:17:14.533 --> 00:17:16.469
The limiter figure
means the GPU

00:17:16.469 --> 00:17:19.071
is bottlenecked
by ALU utilization

00:17:19.071 --> 00:17:23.376
for about 31 percent
of the kernel's execution time.

00:17:23.376 --> 00:17:25.211
So how can the GPU be utilizing

00:17:25.211 --> 00:17:28.581
27 percent of the GPU's
ALU capabilities

00:17:28.581 --> 00:17:32.585
but be bottlenecked
by ALU for 31 percent?

00:17:32.585 --> 00:17:34.720
The limiter can be thought of
as the efficiency

00:17:34.720 --> 00:17:37.423
of the ALU work that was done.

00:17:37.423 --> 00:17:40.159
It is the time spent
doing actual work,

00:17:40.159 --> 00:17:43.729
plus the time spent on
internal stalls or inefficiency.

00:17:43.729 --> 00:17:45.665
In best case,
these times are equal,

00:17:45.665 --> 00:17:47.934
but in practice,
there is a difference.

00:17:47.934 --> 00:17:51.337
A large difference indicates
that the GPU has work to do,

00:17:51.337 --> 00:17:53.673
but is unable to do it
for some reason.

00:17:53.673 --> 00:17:56.742
Complex ALU operations
such as log() for instance,

00:17:56.742 --> 00:17:58.911
or using expensive
texture formats,

00:17:58.911 --> 00:18:01.013
can result in underutilization,

00:18:01.013 --> 00:18:02.682
and signify that
there could be scope

00:18:02.682 --> 00:18:07.553
to optimize the kernel's math.

00:18:07.553 --> 00:18:09.288
These two figures work
hand in hand

00:18:09.288 --> 00:18:10.690
to help you understand

00:18:10.690 --> 00:18:13.259
the general makeup of work
your kernels are performing,

00:18:13.259 --> 00:18:17.096
and how efficient each category
of work is to perform.

00:18:17.096 --> 00:18:18.597
With this particular kernel,

00:18:18.597 --> 00:18:22.568
we can see that occupancy
is at 37 percent.

00:18:22.568 --> 00:18:23.669
This value looks low

00:18:23.669 --> 00:18:25.371
and certainly worthy
of investigation

00:18:25.371 --> 00:18:27.707
to understand
if it can be increased.

00:18:27.707 --> 00:18:31.043
Let's take a closer look
at occupancy.

00:18:31.043 --> 00:18:33.546
It is the measure of how many
threads are currently active

00:18:33.546 --> 00:18:37.683
on the GPU, relative to
the maximum that could be.

00:18:37.683 --> 00:18:41.187
When this figure is low, it is
important to understand why,

00:18:41.187 --> 00:18:45.458
to determine if this is expected
or signifies a problem.

00:18:45.458 --> 00:18:48.361
Low occupancy sometimes
isn't surprising nor avoidable,

00:18:48.361 --> 00:18:50.029
if for instance
your submitted work

00:18:50.029 --> 00:18:52.365
has a comparatively
low number of threads

00:18:52.365 --> 00:18:55.434
because the work to perform
is simply small.

00:18:55.434 --> 00:18:58.337
It can also be okay if the GPU
is limited by other counters,

00:18:58.337 --> 00:19:00.106
such as ALU.

00:19:00.106 --> 00:19:01.640
However a low occupancy

00:19:01.640 --> 00:19:03.843
coupled with low limiter
counters

00:19:03.843 --> 00:19:06.012
signifies that the GPU
has capacity

00:19:06.012 --> 00:19:10.750
to execute more threads
simultaneously.

00:19:10.750 --> 00:19:14.186
So what may be causing
problematic low occupancy?

00:19:14.186 --> 00:19:16.255
A common reason for this
is exhausting thread

00:19:16.255 --> 00:19:18.691
or thread-group memory.

00:19:18.691 --> 00:19:21.660
Both of these resources
are finite on the GPU

00:19:21.660 --> 00:19:24.497
and shared among
the running threads.

00:19:24.497 --> 00:19:26.599
Thread memory
is backed by registers,

00:19:26.599 --> 00:19:28.801
and as the pressure
for registers increases,

00:19:28.801 --> 00:19:32.405
occupancy can be decreased
to accommodate.

00:19:32.405 --> 00:19:34.573
With a high thread-group
memory usage,

00:19:34.573 --> 00:19:37.043
the only way
to increase occupancy

00:19:37.043 --> 00:19:39.745
is to reduce the amount
of shared memory used.

00:19:39.745 --> 00:19:42.782
Reducing threrad-group memory
can also help reduce the impact

00:19:42.782 --> 00:19:45.084
of thread-register pressure.

00:19:45.084 --> 00:19:46.519
There is scope for the compiler

00:19:46.519 --> 00:19:48.854
to spill registers
more efficiently

00:19:48.854 --> 00:19:51.557
when the maximum thread count
in a thread group is known

00:19:51.557 --> 00:19:54.994
at pipeline state creation time.

00:19:54.994 --> 00:19:57.430
These optimizations
can be enabled by setting

00:19:57.430 --> 00:19:59.532
either maxThreadsPerThreadgroup

00:19:59.532 --> 00:20:02.134
on the compute pipeline
state descriptor

00:20:02.134 --> 00:20:03.969
or by using the Metal Shader
langauge

00:20:03.969 --> 00:20:06.372
max_total_threads_per
threadgroup attribute

00:20:06.372 --> 00:20:09.341
directly in your kernel source.

00:20:09.341 --> 00:20:10.776
Tune this value
to find the balance

00:20:10.776 --> 00:20:12.912
that works best for your
kernels.

00:20:12.912 --> 00:20:15.147
Aim for a value that is
the smallest multiple

00:20:15.147 --> 00:20:20.920
of the thread execution width
that works for your algorithm.

00:20:20.920 --> 00:20:23.522
Let's go deeper
into register pressure.

00:20:23.522 --> 00:20:24.623
When this is high,

00:20:24.623 --> 00:20:29.361
we will see register spill
in Xcode's GPU profiler.

00:20:29.361 --> 00:20:32.031
With this kernel example,
we can see that our occupancy

00:20:32.031 --> 00:20:36.335
is at 16 percent,
and this is really low.

00:20:36.335 --> 00:20:38.938
Looking at the compiler
statistics for this kernel

00:20:38.938 --> 00:20:41.507
shows the relative
instruction costs of it,

00:20:41.507 --> 00:20:45.544
including the spilled bytes.

00:20:45.544 --> 00:20:48.180
This spill, along with
the temporary registers,

00:20:48.180 --> 00:20:50.950
is likely the cause
of our poor occupancy.

00:20:50.950 --> 00:20:52.551
We are exhausting
the thread memory,

00:20:52.551 --> 00:20:55.354
and occupancy is reduced
to free up more registers

00:20:55.354 --> 00:20:58.524
for the threads that will run.

00:20:58.524 --> 00:21:01.527
Registers are allocated
to a kernel in register blocks,

00:21:01.527 --> 00:21:03.496
and as such,
you'll need to reduce the usage

00:21:03.496 --> 00:21:08.667
by up to the block size to see a
potential increase in occupancy.

00:21:08.667 --> 00:21:11.804
Optimizing for minimal register
usage is a great way

00:21:11.804 --> 00:21:15.141
to make impactful performance
improvements to complex kernels,

00:21:15.141 --> 00:21:18.010
but how do we do this?

00:21:18.010 --> 00:21:21.180
Preferring 16bit types
over 32bit types

00:21:21.180 --> 00:21:22.648
increases the number
of registers

00:21:22.648 --> 00:21:25.217
available to other parts
of the kernel.

00:21:25.217 --> 00:21:27.653
Conversion between these types
to their 32bit counterparts

00:21:27.653 --> 00:21:29.755
is usually free.

00:21:29.755 --> 00:21:31.891
And reducing the data
stored on the stack --

00:21:31.891 --> 00:21:33.826
for example,
large arrays or structs --

00:21:33.826 --> 00:21:35.761
can consume
a large number of registers

00:21:35.761 --> 00:21:39.231
and reducing them
is an effective tool.

00:21:39.231 --> 00:21:40.766
Look to tune your shader inputs

00:21:40.766 --> 00:21:44.170
to make the best use
of the constant address space.

00:21:44.170 --> 00:21:46.138
This can drastically
reduce the number

00:21:46.138 --> 00:21:51.310
of general purpose registers
being used unnecessarily.

00:21:51.310 --> 00:21:53.679
And final tip is to avoid
indexing into arrays

00:21:53.679 --> 00:21:55.648
stored on the stack
or in constant data

00:21:55.648 --> 00:21:58.551
with a dynamic index.

00:21:58.551 --> 00:22:00.152
An example of this
is shown here,

00:22:00.152 --> 00:22:02.621
where an array is initialized
at runtime.

00:22:02.621 --> 00:22:05.558
If index is not known
to the compiler at compile time,

00:22:05.558 --> 00:22:08.661
the array will likely
spill to memory.

00:22:08.661 --> 00:22:10.529
In this second example,
though,

00:22:10.529 --> 00:22:13.299
index is known at compile time,
and the compiler will likely

00:22:13.299 --> 00:22:17.603
unroll the loop and be able
to optimize away any spill.

00:22:17.603 --> 00:22:18.704
Each of these techniques

00:22:18.704 --> 00:22:21.907
will reduce the allocation
of registers, reduce spill,

00:22:21.907 --> 00:22:26.812
and help increase occupancy
for more performant kernels.

00:22:26.812 --> 00:22:29.215
For more insights into Metal
optimization techniques

00:22:29.215 --> 00:22:33.419
for Apple Silicon,
do watch the WWDC 2020 video,

00:22:33.419 --> 00:22:37.623
"Optimize Metal Performance
for Apple Silicon Macs."

00:22:37.623 --> 00:22:38.857
And there you have it.

00:22:38.857 --> 00:22:41.660
Let's review
what we covered today.

00:22:41.660 --> 00:22:44.563
We started with a review in
the role of command queues,

00:22:44.563 --> 00:22:46.999
command buffers,
and command encoders

00:22:46.999 --> 00:22:49.201
to remind ourselves
about the submission model,

00:22:49.201 --> 00:22:52.238
and how work is queued
to the GPU in Metal,

00:22:52.238 --> 00:22:54.206
and we explored how to encode
Metal commands

00:22:54.206 --> 00:22:58.978
from multiple threads to reduce
the CPU encoding time and cost.

00:22:58.978 --> 00:23:01.614
With that knowledge,
we looked at recommendations

00:23:01.614 --> 00:23:03.849
on how to tune
your applications;

00:23:03.849 --> 00:23:06.352
avoiding unnecessary copies
to take advantage

00:23:06.352 --> 00:23:07.987
of the unified memory
architecture;

00:23:07.987 --> 00:23:10.256
submitting larger quantities
of work;

00:23:10.256 --> 00:23:12.691
and using Metal textures
as well as Metal buffers

00:23:12.691 --> 00:23:15.628
for our kernel's resources.

00:23:15.628 --> 00:23:17.263
And finally,
we took a walkthrough

00:23:17.263 --> 00:23:18.864
of how to use
our tools to identify

00:23:18.864 --> 00:23:20.633
performance bottlenecks.

00:23:20.633 --> 00:23:23.168
We understood how to interpret
the GPU's utilization

00:23:23.168 --> 00:23:25.571
and limiter values,
and how we can tackle

00:23:25.571 --> 00:23:30.409
problematic low occupancy
if we discover it.

00:23:30.409 --> 00:23:31.443
Thank you very much,

00:23:31.443 --> 00:23:33.946
and I hope you're as excited
as I am by what's possible

00:23:33.946 --> 00:23:37.182
with the most powerful
MacBook Pro lineup ever.