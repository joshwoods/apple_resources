WEBVTT

00:00:02.700 --> 00:00:07.600
Hello, I'm Shreya Jain,
an engineer from the Create ML team.

00:00:07.620 --> 00:00:09.990
Today,
we're going to explore enhancements

00:00:10.270 --> 00:00:14.760
in the Object Detection template and
leverage them to create better models.

00:00:14.840 --> 00:00:20.310
If you're not already familiar
with object detection in Create ML,

00:00:20.490 --> 00:00:25.900
I encourage you to watch
this video from WWDC 2019.

00:00:26.770 --> 00:00:31.040
Object detection can enable
some engaging app experiences.

00:00:31.090 --> 00:00:35.300
You can build an app to help
people sort their trash.

00:00:35.370 --> 00:00:40.480
Try virtual classes on their pet cat.

00:00:40.680 --> 00:00:46.600
and even an app that can recommend
recipes based on detected ingredients.

00:00:46.690 --> 00:00:51.680
Building a model for this app is
a great way to see Create ML and

00:00:51.690 --> 00:00:54.480
its new features in action.

00:00:54.960 --> 00:00:58.150
Significant improvements have
been made to object detection.

00:00:58.160 --> 00:01:04.390
You can train accurate and smaller
models with less training data

00:01:04.390 --> 00:01:10.790
and exposing more configuration
options to customize training.

00:01:11.500 --> 00:01:13.860
So let's jump right in.

00:01:13.860 --> 00:01:17.880
To get started,
I'll open the Create ML app

00:01:18.360 --> 00:01:19.070
from Spotlight.

00:01:19.070 --> 00:01:23.480
The first thing I see
is a template picker,

00:01:23.480 --> 00:01:26.630
where I'll select Object Detection.

00:01:26.620 --> 00:01:32.560
This opens a dialog box for entering
details about the Create ML project.

00:01:32.610 --> 00:01:37.380
I’ll name this project "Find
My Recipe" and add a description

00:01:37.890 --> 00:01:40.320
that it helps detect ingredients.

00:01:40.380 --> 00:01:45.740
I can choose to change the project's
location before creating it.

00:01:45.790 --> 00:01:48.920
Next, I land on the Settings tab.

00:01:48.980 --> 00:01:54.420
Data and configuration options can
be tweaked here before training.

00:01:54.480 --> 00:01:58.110
Before loading the data,
I’ll take you through how

00:01:58.110 --> 00:01:59.940
this data is prepared.

00:02:00.830 --> 00:02:04.210
Object detection data must
be stored in a folder,

00:02:04.390 --> 00:02:09.720
which contains all training images
and the annotations.json file.

00:02:09.880 --> 00:02:15.020
Contents of annotations.json
can be understood by taking

00:02:15.020 --> 00:02:18.850
this image as an example,
which has two objects,

00:02:18.970 --> 00:02:22.140
a slice of bread and a tomato.

00:02:22.280 --> 00:02:26.850
Each object annotation consists
of the object's label and

00:02:26.850 --> 00:02:29.190
its location in the image.

00:02:29.310 --> 00:02:34.230
The location is based on the
top left corner of the image.

00:02:34.250 --> 00:02:39.680
All objects in the images of the training
data can be annotated in this way.

00:02:41.550 --> 00:02:46.400
All these annotations are added to
a single JSON file in this format.

00:02:46.400 --> 00:02:52.400
I’ll use this information
to prepare my training data.

00:02:54.200 --> 00:02:59.410
After the data is prepared,
I can load it in the Create ML app.

00:03:00.140 --> 00:03:04.670
Clicking on the View button shows
the class distribution of my dataset.

00:03:04.670 --> 00:03:12.000
As you can see, my classes are Tomato,
Cheese, Bread, and Basil.

00:03:12.100 --> 00:03:17.670
Moving back to the Settings tab,
validation data can be provided

00:03:17.670 --> 00:03:23.600
optionally to ensure that the
model performs well on unseen data.

00:03:23.740 --> 00:03:29.570
Here, I set validation data to automatic,
letting Create ML use a

00:03:29.690 --> 00:03:32.560
small portion of my dataset.

00:03:34.710 --> 00:03:39.260
There are also new training
parameters that allow you to better

00:03:39.260 --> 00:03:41.800
control training of your model.

00:03:41.850 --> 00:03:49.540
They are: Algorithm, Iterations,
Batch Size, and Grid Size.

00:03:49.590 --> 00:03:52.680
There's two algorithms for training.

00:03:53.450 --> 00:03:55.600
First is the full network.

00:03:55.790 --> 00:03:59.720
Let's look deeper into the
full network algorithm.

00:04:00.510 --> 00:04:06.170
Full network was introduced in
Create ML in 2019 and has been the

00:04:06.170 --> 00:04:08.390
default training algorithm since.

00:04:08.650 --> 00:04:14.500
This algorithm is based on
the YOLOv2 architecture.

00:04:15.120 --> 00:04:20.110
All parameters of this network
are trained using your data.

00:04:20.950 --> 00:04:26.780
The resulting Core ML model
encodes all the learned parameters.

00:04:27.130 --> 00:04:33.540
This Core ML model has been quantized
to store weights with 16-bit precision.

00:04:33.710 --> 00:04:38.540
The resulting model size is
half of what we got earlier.

00:04:38.740 --> 00:04:46.390
So a model that was previously
around 65 MB will now be 33 MB.

00:04:47.340 --> 00:04:52.020
This algorithm is recommended when you
have large amounts of training data,

00:04:52.020 --> 00:04:56.760
like over 200 bounding boxes per class.

00:04:56.980 --> 00:05:00.580
The resulting model is
backwards compatible,

00:05:00.690 --> 00:05:04.420
going all the way back to iOS 12.

00:05:04.460 --> 00:05:10.700
We wanted to enable you to build highly
accurate models with less training data,

00:05:10.740 --> 00:05:15.110
which is why we’re introducing
the Transfer Learning Algorithm

00:05:15.370 --> 00:05:17.220
for Object Detection.

00:05:18.120 --> 00:05:22.300
Transfer learning leverages
machine learning models already

00:05:22.300 --> 00:05:24.300
in the operating system.

00:05:24.370 --> 00:05:31.790
For example, Photo’s app includes models
that power search and memories.

00:05:32.160 --> 00:05:37.410
One of the pre-trained backbones that
Photos uses is called Object Print.

00:05:37.540 --> 00:05:42.500
This is trained on huge
amounts of diverse data.

00:05:42.640 --> 00:05:46.590
With Transfer Learning,
you can take advantage of this

00:05:46.660 --> 00:05:49.370
to reduce your data requirement.

00:05:50.270 --> 00:05:54.410
Transfer Learning Algorithm
in Create ML uses object print

00:05:54.410 --> 00:05:56.670
along with the head network.

00:05:56.670 --> 00:06:00.840
Only the head network
is trained on your data,

00:06:00.990 --> 00:06:05.750
reducing the number of parameters
that need to be learned.

00:06:06.320 --> 00:06:09.600
As a result,
the Core ML model contains only

00:06:09.740 --> 00:06:12.890
the head network parameters.

00:06:13.150 --> 00:06:16.900
which makes your model five times
smaller than the full network.

00:06:16.930 --> 00:06:24.480
The same model that was 65 megabytes
in 2019 and 33 megabytes after

00:06:24.480 --> 00:06:30.420
quantization will be just 7 megabytes
using the transfer learning algorithm.

00:06:32.630 --> 00:06:36.880
Transfer learning is a great
option when you have limited data

00:06:37.480 --> 00:06:38.790
and want a lightweight model.

00:06:38.800 --> 00:06:44.840
It does well with as few as 80
training examples per class.

00:06:44.960 --> 00:06:50.130
The resulting model requires
iOS 14 to leverage the object

00:06:50.340 --> 00:06:52.720
print shipped with the OS.

00:06:54.900 --> 00:06:58.900
Algorithm is just one of
the new configurations.

00:06:58.980 --> 00:07:04.430
Parameters like iterations and
batch size have also been added.

00:07:05.270 --> 00:07:09.920
Iterations is the number of times
the model's parameters are updated.

00:07:09.920 --> 00:07:15.200
A default value is picked
based on the dataset size.

00:07:15.350 --> 00:07:19.740
For your particular use case,
you can increase iterations if

00:07:19.740 --> 00:07:23.980
the model has not yet converged,
or reduce them if the

00:07:23.980 --> 00:07:26.370
model is doing well early.

00:07:27.400 --> 00:07:31.780
Batch size refers to the
number of training examples

00:07:31.780 --> 00:07:34.360
utilized in one iteration.

00:07:34.360 --> 00:07:38.810
A default value is chosen
based on hardware restrictions.

00:07:38.940 --> 00:07:43.900
Although higher batch size is better,
you may want to use the default

00:07:43.900 --> 00:07:48.670
value or reduce it based on
performance restrictions.

00:07:49.140 --> 00:07:54.040
Finally, for the full network,
you can customize grid size.

00:07:54.080 --> 00:07:58.400
Understanding grid size requires
knowledge of how predictions

00:07:58.870 --> 00:08:00.800
work for the full network.

00:08:00.830 --> 00:08:03.300
Let’s take a closer look.

00:08:04.190 --> 00:08:09.800
Starting with this input image,
passing it to a trained full

00:08:09.800 --> 00:08:16.380
network model results in a number of
predicted objects with bounding boxes.

00:08:16.740 --> 00:08:21.720
To find the objects in the image,
the model leverages a grid

00:08:21.720 --> 00:08:24.340
and a set of anchor boxes.

00:08:24.340 --> 00:08:29.680
The specified grid defines the
aspect ratio of the input image,

00:08:29.710 --> 00:08:33.920
as well as where the model
will look for detected objects.

00:08:33.920 --> 00:08:36.810
For example,
let's see how the model will behave

00:08:37.420 --> 00:08:41.140
with a grid dimension of 5 by 5.

00:08:41.490 --> 00:08:44.640
The image will be
resized to fit the grid,

00:08:44.860 --> 00:08:49.660
in this case to a square image,
and then broken up into the

00:08:49.660 --> 00:08:52.210
defined number of cells.

00:08:53.470 --> 00:08:58.960
The network then produces predictions,
one for each grid cell.

00:08:59.420 --> 00:09:03.400
Each prediction contains
the following information:

00:09:03.400 --> 00:09:07.900
whether a cell has an object in it,
the class of the object,

00:09:07.920 --> 00:09:09.600
and its bounding box.

00:09:09.640 --> 00:09:14.320
YOLO works fine for multiple
objects where each object is

00:09:14.320 --> 00:09:17.090
associated with one grid cell.

00:09:17.370 --> 00:09:21.750
As you can see in this image,
the centers of banana and

00:09:22.030 --> 00:09:24.400
dog fall in the same cell.

00:09:25.110 --> 00:09:28.500
Since each cell can
only predict one class,

00:09:28.690 --> 00:09:32.670
it is forced to pick either
the banana or the dog.

00:09:34.170 --> 00:09:37.680
In order to predict
both the banana and dog,

00:09:37.830 --> 00:09:39.200
anchor boxes are defined.

00:09:39.200 --> 00:09:44.630
Anchor boxes have a set aspect
ratio and detect multiple

00:09:44.630 --> 00:09:47.600
objects within a grid cell.

00:09:48.580 --> 00:09:54.330
Create ML uses a default
grid dimension of 13 by 13,

00:09:54.330 --> 00:09:57.740
totaling 169 cells.

00:09:57.770 --> 00:10:05.620
A fixed set of 15 anchor boxes of varying
aspect ratio are evaluated for each cell.

00:10:05.660 --> 00:10:09.970
Therefore,
the default model is doing a total

00:10:10.000 --> 00:10:14.860
of 2,535 predictions per image.

00:10:15.830 --> 00:10:20.120
Consider this image of dice,
and how object detection would

00:10:20.330 --> 00:10:23.950
work with a grid dimension of 3x3.

00:10:24.600 --> 00:10:29.940
As multiple dyes of similar aspect
ratio are present in a single cell,

00:10:29.940 --> 00:10:34.540
only one of them would be detected.

00:10:35.190 --> 00:10:38.860
With a large grid size,
more dice are detected.

00:10:38.860 --> 00:10:45.910
However, this will increase the number
of predictions per image.

00:10:46.080 --> 00:10:50.190
It's important to consider
the computational cost when

00:10:50.190 --> 00:10:52.520
altering the grid size.

00:10:53.290 --> 00:11:00.100
For a non-square input image like
this one with dimensions 1500x800,

00:11:00.170 --> 00:11:05.610
using an 8x8 grid on this image
results in loss of information and

00:11:05.610 --> 00:11:08.640
distorts the natural shape of objects.

00:11:08.710 --> 00:11:12.330
This prevents the model from
capturing more fine-grained

00:11:12.330 --> 00:11:17.700
patterns while training,
as well as hinders its predictive power.

00:11:18.630 --> 00:11:24.500
Choosing a grid size of 15 by 8
preserves the original aspect ratio

00:11:24.700 --> 00:11:29.210
of the input image and results
in a model that has learned more

00:11:29.210 --> 00:11:32.670
information and can give better results.

00:11:33.210 --> 00:11:37.810
Getting back to training the model
for the Find My Recipe project,

00:11:37.910 --> 00:11:44.360
I can pick transfer learning algorithm,
set 1000 iterations,

00:11:44.360 --> 00:11:46.530
and auto for batch size.

00:11:46.790 --> 00:11:51.580
On clicking the play button,
the model starts training.

00:11:51.620 --> 00:11:55.600
The training tab shows that
the batch is being prepared.

00:11:55.650 --> 00:11:59.450
This step performs a set of
standard image augmentations

00:11:59.960 --> 00:12:05.120
to help with robustness and
generalizability to real-world data.

00:12:05.210 --> 00:12:11.590
Soon, a graph appears plotting loss
values for each iteration.

00:12:17.400 --> 00:12:21.380
As training proceeds,
the snapshot button can be clicked

00:12:21.910 --> 00:12:24.400
to obtain a model at that time.

00:12:24.400 --> 00:12:29.620
Snapshots are helpful in
checking on training progress.

00:12:29.720 --> 00:12:35.580
I can use this model to preview
predictions on a few images.

00:12:35.860 --> 00:12:41.810
For every image, model predictions are
shown in the preview tab.

00:12:41.930 --> 00:12:46.480
I can click on these bounding
boxes to see confidence for

00:12:46.480 --> 00:12:48.450
each class at the bottom.

00:12:48.460 --> 00:12:53.610
A snapshot can also be used
to experiment within an app.

00:12:55.110 --> 00:12:59.850
After training is complete,
evaluation metrics on training

00:12:59.850 --> 00:13:04.460
and validation data can be
seen in the Evaluation tab.

00:13:04.510 --> 00:13:07.440
What do these numbers mean?

00:13:08.480 --> 00:13:12.640
Evaluation for object detection
models needs to be twofold.

00:13:12.640 --> 00:13:17.300
Not only do we want correct labels,
but they must also be

00:13:17.400 --> 00:13:19.200
at the right locations.

00:13:19.200 --> 00:13:25.040
Getting the bounding boxes to exactly
match the annotated box is hard.

00:13:25.040 --> 00:13:30.870
A number that captures how
close predicted boxes are to

00:13:30.870 --> 00:13:34.630
annotated boxes becomes necessary.

00:13:35.760 --> 00:13:40.670
This is measured by a score
called Intersection Over Union.

00:13:40.670 --> 00:13:50.090
It is a value between 0% which is no
overlap to 100% which is perfect overlap.

00:13:51.050 --> 00:13:53.530
For a prediction to
be considered correct,

00:13:53.710 --> 00:13:59.530
it needs to have the correct class label
and an intersection over union score

00:13:59.530 --> 00:14:02.610
greater than a predefined threshold.

00:14:03.330 --> 00:14:07.300
If either the intersection over union
score is less than the threshold,

00:14:07.410 --> 00:14:14.140
or the predicted class is incorrect,
the overall prediction is incorrect.

00:14:14.580 --> 00:14:19.560
This information is used
to compute a metric called

00:14:19.690 --> 00:14:22.170
Mean Average Precision or MAP.

00:14:23.690 --> 00:14:28.780
I’ll go back to the Evaluations
tab to see these numbers.

00:14:30.370 --> 00:14:34.300
These numbers represent
average precision per class,

00:14:34.500 --> 00:14:36.460
calculated at two thresholds.

00:14:36.460 --> 00:14:43.180
One is fixed at 50% and the other
varied at multiple thresholds.

00:14:43.290 --> 00:14:47.460
The overall mean average
precision for our dataset can

00:14:47.460 --> 00:14:50.120
be seen at the top right corner.

00:14:50.280 --> 00:14:54.680
Higher MEP reflects more
correct predictions.

00:14:55.320 --> 00:14:59.300
The MAPFR model looks good overall.

00:14:59.320 --> 00:15:03.720
I’ll preview the model on a few
examples to make sure that the

00:15:03.720 --> 00:15:06.240
model predictions look right.

00:15:10.100 --> 00:15:12.000
Everything looks great.

00:15:12.070 --> 00:15:15.820
I can now drop this model into our app.

00:15:16.290 --> 00:15:19.320
With the additional
features that you just saw,

00:15:19.320 --> 00:15:25.720
creating an object detection
model using Create ML is simple.

00:15:25.860 --> 00:15:32.000
Create ML helps you customize models by
providing more control over training.

00:15:32.040 --> 00:15:38.760
It helps you build accurate models
with less data and smaller output size.

00:15:38.810 --> 00:15:43.560
We can't wait to see the wonderful
ideas that you bring to the table

00:15:43.560 --> 00:15:47.100
using these brand new features.