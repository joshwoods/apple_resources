WEBVTT

00:00:23.200 --> 00:00:27.140
Hi, I'm Eryk Vershen,
Media Technologies Evangelist.

00:00:27.140 --> 00:00:30.270
The AV Foundation framework
provides facilities for capture,

00:00:30.620 --> 00:00:34.090
playback, reading, writing,
and editing videos.

00:00:34.500 --> 00:00:39.070
Introduced in iOS 4,
it was brought to OS X in 10.7 Lion.

00:00:39.100 --> 00:00:43.290
AV Foundation is a modern framework
designed on the basis of our many

00:00:43.380 --> 00:00:46.090
years' experience with video at Apple.

00:00:46.230 --> 00:00:50.100
In this video, you'll learn how to use
AV Foundation to playback video,

00:00:50.100 --> 00:00:54.240
to edit videos together,
and also handle title animations,

00:00:54.240 --> 00:00:59.040
essential knowledge for incorporating
video in your iOS or OS X application.

00:01:00.280 --> 00:01:03.550
AV Foundation is our powerful
low-level framework for working

00:01:03.550 --> 00:01:05.400
with timed audiovisual media.

00:01:05.530 --> 00:01:09.410
It's available on iOS as well as OS X.

00:01:09.630 --> 00:01:12.860
The first thing you want to do
with a movie or song is play it.

00:01:12.960 --> 00:01:15.340
AV Foundation has great
support for playback.

00:01:15.550 --> 00:01:21.110
You can control which tracks are played,
stop, start, seek, and get fine-grain

00:01:21.110 --> 00:01:23.450
notifications during playback.

00:01:24.160 --> 00:01:28.000
As we'll see,
AV Foundation is below the UI layer.

00:01:28.150 --> 00:01:32.400
While this may require more work,
it allows you to completely customize

00:01:32.540 --> 00:01:35.290
the controls you provide to your users.

00:01:36.020 --> 00:01:40.120
AV Foundation allows you to capture
from the camera and microphone providing

00:01:40.120 --> 00:01:45.370
you with control over features like
auto focus and the camera flash.

00:01:45.910 --> 00:01:49.390
Support for editing is there,
from simple cuts-only editing

00:01:49.680 --> 00:01:53.790
to compositing operations
like fades and wipes,

00:01:53.840 --> 00:01:57.980
and the mixing of multiple audio
tracks like music or commentary

00:01:57.990 --> 00:01:59.870
with the original audio.

00:02:01.060 --> 00:02:04.790
You can both read and write
at the frame or sample level.

00:02:04.790 --> 00:02:08.670
For example,
to apply a sepia filter to a video.

00:02:09.440 --> 00:02:14.690
AV Foundation has a lot of functionality,
and these are only some of the classes.

00:02:14.820 --> 00:02:16.770
AV Foundation is a great framework.

00:02:17.080 --> 00:02:19.150
When should you use it?

00:02:19.310 --> 00:02:22.360
When you want to inspect
properties like duration,

00:02:22.450 --> 00:02:27.530
media type, or metadata,
to implement custom playback behavior,

00:02:27.750 --> 00:02:31.610
when you need to edit multiple
pieces of media together,

00:02:32.020 --> 00:02:35.220
to transcode a video
into some other format,

00:02:35.220 --> 00:02:38.680
and when you want detailed
control over capture.

00:02:40.450 --> 00:02:45.180
AV Foundation is, as its name implies,
a foundation-level framework.

00:02:45.320 --> 00:02:47.540
It is underneath the UI.

00:02:47.700 --> 00:02:51.180
There are no classes in
AV Foundation that are part of the UI.

00:02:51.350 --> 00:02:55.900
Instead, there are model classes
that provide data to the UI.

00:02:56.070 --> 00:03:01.070
AV Foundation sits on a number of
core foundation frameworks for audio,

00:03:01.070 --> 00:03:03.600
video, media, and animation.

00:03:03.670 --> 00:03:06.980
I'll mention some things
about these as I go along.

00:03:08.560 --> 00:03:11.550
Now, AV Foundation can be broken
up in a number of ways.

00:03:11.700 --> 00:03:17.420
For example, into playback, capture,
editing, and read and write.

00:03:17.630 --> 00:03:19.770
And I really wanted to talk
about the whole framework today,

00:03:19.770 --> 00:03:24.600
but you would be listening to me for
several hours instead of just an hour.

00:03:24.920 --> 00:03:26.960
So I'm going to have
to leave out Capture,

00:03:27.110 --> 00:03:30.460
but there's an excellent
presentation called Capturing from

00:03:30.460 --> 00:03:36.640
the Camera Using AV Foundation on
iOS 5 from the 2011 WWDC.

00:03:36.810 --> 00:03:41.050
And there's also another talk from
that same conference introducing

00:03:41.050 --> 00:03:43.360
AV Foundation Capture for Lion.

00:03:43.390 --> 00:03:46.110
So that topic's covered pretty well.

00:03:46.950 --> 00:03:50.640
It turns out that Read&Write
is also well covered.

00:03:50.720 --> 00:03:52.760
At the same conference
there was another talk,

00:03:52.770 --> 00:03:57.170
Working with Media in the AV Foundation,
that went into some detail

00:03:57.170 --> 00:03:59.340
on the Read&Write classes.

00:03:59.820 --> 00:04:03.550
So instead, I'm going to talk about how
Core Animation interacts

00:04:03.550 --> 00:04:04.940
with AV Foundation.

00:04:05.120 --> 00:04:08.960
These two areas, Edit and Core Animation,
were areas that I found

00:04:08.990 --> 00:04:10.300
confusing at first.

00:04:10.380 --> 00:04:14.670
I hope I can dispel any confusion
you might have about them.

00:04:15.310 --> 00:04:17.760
Let's start with playback.

00:04:17.880 --> 00:04:20.610
In playback,
we're taking data from a movie file,

00:04:20.820 --> 00:04:23.690
playing through the file,
and displaying the result

00:04:23.700 --> 00:04:25.440
on our device's screen.

00:04:25.540 --> 00:04:27.960
There are three principal
classes involved:

00:04:28.040 --> 00:04:31.500
AV Asset, that is the movie file,
AV Player,

00:04:31.760 --> 00:04:35.030
that controls the playback itself,
and AV Player Layer,

00:04:35.090 --> 00:04:38.700
that is the main class involved
in displaying the images.

00:04:38.800 --> 00:04:41.880
We'll start by discussing AV Asset.

00:04:42.870 --> 00:04:48.230
AV Asset is an abstract base class that
represents any timed audio-visual media,

00:04:48.230 --> 00:04:53.310
be it a video, a movie, a song,
a podcast, or whatever.

00:04:53.420 --> 00:04:56.700
Assets might be local,
that is on the iOS device,

00:04:56.840 --> 00:04:59.800
or they may be remote,
fetched over the network.

00:04:59.910 --> 00:05:03.310
While assets are typically finite,
they may actually be streams of data,

00:05:03.360 --> 00:05:07.690
such as from a webcam,
that have no real beginning or end.

00:05:08.810 --> 00:05:10.330
Where do you get an asset from?

00:05:10.460 --> 00:05:14.110
Well, they can be built into your
application in its bundle.

00:05:14.110 --> 00:05:18.210
They can come from either the iPod media
library or the user's photo library.

00:05:18.430 --> 00:05:22.340
In these cases, you don't have arbitrary
access to the files.

00:05:22.570 --> 00:05:27.770
You ask the library for a special
URL that you can pass to AV Foundation.

00:05:29.520 --> 00:05:33.000
They can also come from
the network as an HTTP URL.

00:05:33.000 --> 00:05:36.620
An asset has a structure.

00:05:36.940 --> 00:05:39.880
It's made of tracks,
and the tracks consist of segments.

00:05:40.000 --> 00:05:44.500
A track is a single kind of media,
a video track or an audio track.

00:05:44.610 --> 00:05:48.040
And you may have more than
one track of a given kind,

00:05:48.040 --> 00:05:51.960
say an English language audio track,
a German audio track,

00:05:51.960 --> 00:05:53.080
or some other language.

00:05:53.210 --> 00:05:55.170
And tracks have segments.

00:05:55.880 --> 00:05:57.730
While many tracks will
be a single segment,

00:05:57.730 --> 00:06:01.500
we will see later on that
tracks can have many segments.

00:06:03.440 --> 00:06:08.600
Now, merely because we've created an
asset object does not mean that

00:06:08.600 --> 00:06:12.150
the tracks and other properties
are immediately accessible.

00:06:12.220 --> 00:06:16.840
Getting values may take a substantial
time due to the file format,

00:06:16.910 --> 00:06:21.880
the sheer size of the file,
or it's being located across the network.

00:06:21.950 --> 00:06:25.400
To deal with the need to wait,
AV Foundation implements a protocol

00:06:25.670 --> 00:06:29.960
on media objects which allows you to
ask for values to be loaded and be

00:06:29.960 --> 00:06:33.200
notified when the loading has occurred.

00:06:33.240 --> 00:06:36.480
The second major class is AV Player.

00:06:36.750 --> 00:06:39.100
AVPlayer is a controller object.

00:06:39.250 --> 00:06:42.390
Besides being the object that
you tell to play and pause,

00:06:42.500 --> 00:06:46.390
it also has methods for
monitoring time passing.

00:06:47.300 --> 00:06:52.540
Now, we can't directly connect
AV Player with an AV Asset.

00:06:52.660 --> 00:06:56.910
Doing so would cause either the
Asset to know too much about the

00:06:56.910 --> 00:07:01.190
playback state or the Player to
know too much about the Asset.

00:07:01.300 --> 00:07:06.510
So instead, we use an AV Player item
to hold that state.

00:07:07.010 --> 00:07:10.690
Now just as assets have tracks,
so too player items have

00:07:10.790 --> 00:07:15.050
player item tracks that control
whether the tracks are enabled.

00:07:16.770 --> 00:07:20.760
The last major class is AV Player Layer.

00:07:20.760 --> 00:07:22.400
Here's our diagram again.

00:07:22.630 --> 00:07:25.950
Now, in order to display anything,
it needs to be part of a

00:07:25.970 --> 00:07:28.780
UIView or a subclass of UIView.

00:07:28.900 --> 00:07:31.900
But UIView doesn't know
anything about AV Player.

00:07:31.970 --> 00:07:34.360
It's not the kind of class it expects.

00:07:35.140 --> 00:07:38.860
UI views expect to be backed by
core animation layer subclasses.

00:07:38.990 --> 00:07:44.080
So AV Foundation supplies
a CA layer subclass,

00:07:44.080 --> 00:07:45.810
AV Player Layer.

00:07:46.420 --> 00:07:49.940
So now our picture of
playback classes is complete.

00:07:50.030 --> 00:07:55.170
But what do we actually need to
do to cause playback to happen?

00:07:55.290 --> 00:07:56.340
There are six steps.

00:07:56.970 --> 00:08:00.300
Create the asset,
ask it to load its tracks,

00:08:00.510 --> 00:08:04.180
then create a player item,
attach that to a player,

00:08:04.290 --> 00:08:08.280
attach the player to a layer,
then when the item is ready to play,

00:08:08.480 --> 00:08:09.050
play it.

00:08:09.050 --> 00:08:11.700
So let's look at each
of these steps in turn.

00:08:11.780 --> 00:08:14.260
How do I create an asset?

00:08:14.260 --> 00:08:18.010
The concrete subclass is AVURLAsset.

00:08:18.010 --> 00:08:21.230
In this example,
I'm calling the class method

00:08:21.230 --> 00:08:25.430
URLAssetWithURL to create an
asset object from some URL,

00:08:25.650 --> 00:08:27.450
which is either local or remote.

00:08:27.750 --> 00:08:30.990
Prior to iOS 4.3,
you needed to instantiate

00:08:30.990 --> 00:08:35.720
remote assets differently,
and you can refer to the documentation if

00:08:35.720 --> 00:08:38.520
you need to know what to do in that case.

00:08:39.750 --> 00:08:43.740
In order to ask for tracks or
other properties to be loaded,

00:08:43.740 --> 00:08:48.600
you need to create an array of
strings that are the property names.

00:08:48.830 --> 00:08:53.600
Then you pass that array to load
values asynchronously for keys.

00:08:53.860 --> 00:08:57.210
Sometimes later,
the completion handler block you

00:08:57.360 --> 00:08:59.470
passed will get executed on some queue.

00:08:59.600 --> 00:09:03.490
Now, since we want to operate
on AVPlayer in that block,

00:09:03.490 --> 00:09:07.500
and AVPlayer must only be
accessed from the main queue,

00:09:07.610 --> 00:09:11.210
we need to trampoline
onto the main queue.

00:09:12.800 --> 00:09:15.900
So here's the completion
block we're going to execute.

00:09:16.000 --> 00:09:19.200
The first thing you do is
check the status of the tracks.

00:09:19.210 --> 00:09:22.080
If the tracks are loaded,
then you can create a

00:09:22.080 --> 00:09:24.930
player item for the asset.

00:09:25.030 --> 00:09:28.700
Starting with iOS 5.0,
you only need to load properties

00:09:28.720 --> 00:09:32.270
that you directly access,
but you should still explicitly

00:09:32.270 --> 00:09:35.590
load tracks if you are
supporting earlier versions.

00:09:35.690 --> 00:09:39.130
Once we have a player item,
we can attach it to a player.

00:09:39.260 --> 00:09:43.040
While we are creating the AV player here,
it could be that the player

00:09:43.040 --> 00:09:46.980
already exists and we only
set the player's player item.

00:09:47.460 --> 00:09:52.100
The player needs to be attached to
a player layer in order to display.

00:09:52.160 --> 00:09:55.140
In a few slides,
I'll show you how this view is

00:09:55.140 --> 00:09:59.540
hooked up to the player layer so
that this does the right thing.

00:10:00.710 --> 00:10:04.070
Now we have our objects hooked up,
but the player item isn't

00:10:04.070 --> 00:10:06.600
necessarily ready to play yet.

00:10:06.780 --> 00:10:11.190
We need to key-value observe the
status property of the player item so

00:10:11.190 --> 00:10:14.290
we can detect when the item is ready.

00:10:14.700 --> 00:10:19.540
In our observed value for keypath method,
we look for the right context

00:10:19.650 --> 00:10:24.060
and trampoline to the main queue
to adjust our UI based on the

00:10:24.060 --> 00:10:26.010
value of the player item status.

00:10:26.040 --> 00:10:30.240
Now we can actually accept
requests to play from the user and

00:10:30.240 --> 00:10:32.810
subsequently requests to pause.

00:10:32.840 --> 00:10:36.030
I said I'd tell you how setting
the player views player,

00:10:36.270 --> 00:10:39.480
which we did in our completion handler,
caused the player

00:10:39.480 --> 00:10:41.640
layers player to be set.

00:10:41.640 --> 00:10:45.350
What we have here is the simplest
way of getting an AV player

00:10:45.350 --> 00:10:47.640
layer associated with a UI view.

00:10:47.640 --> 00:10:53.480
We make a UI view subclass and we make
its layer class be AV player layer.

00:10:53.900 --> 00:10:58.380
This means that the UI view
will instantiate an AV player

00:10:58.830 --> 00:11:00.480
layer object for us.

00:11:01.940 --> 00:11:06.010
When we set the views player,
what we do is set the

00:11:06.010 --> 00:11:08.870
player of the player layer.

00:11:09.480 --> 00:11:14.210
And when you fetch the view's player,
what we do is return the player

00:11:14.210 --> 00:11:16.640
associated with the player layer.

00:11:16.750 --> 00:11:21.820
So the player view is just mediating
our access to the player layer,

00:11:21.820 --> 00:11:25.780
so we have one less object
to explicitly worry about.

00:11:27.280 --> 00:11:30.180
Another detail to worry about:
when playback ends,

00:11:30.260 --> 00:11:33.600
you may need to do something,
like rewind the video.

00:11:33.720 --> 00:11:38.010
So you want to observe the "AV
Player Item Did Play to End Time"

00:11:38.010 --> 00:11:41.720
notification so you can respond.

00:11:41.830 --> 00:11:46.070
And then you need to implement the
method that receives the notification.

00:11:47.170 --> 00:11:49.660
That got our basic playback working.

00:11:49.690 --> 00:11:53.720
A common thing you'll want to do is
have a scrubber or slider that the user

00:11:53.720 --> 00:11:56.600
can use for random access to the movie.

00:11:56.750 --> 00:11:58.800
Let's talk about how you do that.

00:11:59.000 --> 00:12:02.110
When the movie is playing normally,
you need to keep the scrubber

00:12:02.110 --> 00:12:03.840
in sync with the current time.

00:12:04.050 --> 00:12:07.410
You do this with a
periodic time observer.

00:12:08.500 --> 00:12:12.520
You tell the player you want a block
called every tenth of a second or

00:12:12.520 --> 00:12:15.160
whatever time interval is appropriate.

00:12:15.290 --> 00:12:20.590
You want the block called on the main
queue because it will update UI elements,

00:12:20.660 --> 00:12:24.220
and that block will get past the
current time so you can move the

00:12:24.320 --> 00:12:27.140
scrubber thumb to the correct place.

00:12:27.720 --> 00:12:29.980
When the user starts to
move the scrubber around,

00:12:30.100 --> 00:12:31.520
you need to do a bit more work.

00:12:32.460 --> 00:12:36.400
You need to pause playback
and remove that time observer.

00:12:36.540 --> 00:12:40.680
And as the scrubber is moved around,
you need to move around in the movie.

00:12:40.790 --> 00:12:44.010
Since seeks can take a while,
and because only one seek

00:12:44.010 --> 00:12:48.040
can be in progress at a time,
you want to chain seeks together.

00:12:48.100 --> 00:12:52.120
I haven't shown all the details,
but basically, if no seek is in progress,

00:12:52.390 --> 00:12:54.120
then you should initiate a seek.

00:12:54.230 --> 00:12:57.140
And if one is in progress,
you should remember the time the

00:12:57.140 --> 00:13:02.150
user chose so your completion
handler can start the next seek.

00:13:03.570 --> 00:13:07.830
Lastly, when the user stops scrubbing,
you want to recreate your time observer

00:13:08.020 --> 00:13:10.850
and continue your normal playback.

00:13:11.970 --> 00:13:14.200
We've seen a lot of
multi-threading so far:

00:13:14.440 --> 00:13:20.170
completion handlers, notifications,
key value observers, and time observers.

00:13:20.280 --> 00:13:24.630
You want to remember that your
UI classes and your AV player need to

00:13:24.630 --> 00:13:27.820
be operated on only from the main queue.

00:13:27.890 --> 00:13:32.150
And because virtually all of the
properties of an AV asset are non-atomic,

00:13:32.250 --> 00:13:34.870
you need to make sure you
aren't manipulating the same

00:13:34.870 --> 00:13:37.560
asset on more than one queue.

00:13:39.300 --> 00:13:40.800
We've covered a bunch of stuff here.

00:13:40.940 --> 00:13:44.660
Remember, the player is about control,
the player item is about

00:13:44.880 --> 00:13:48.530
presentation state,
you can't see it without a player layer,

00:13:48.720 --> 00:13:51.350
and you need to stay on the right cue.

00:13:52.230 --> 00:13:53.700
Let's turn to editing.

00:13:53.850 --> 00:13:56.140
What is editing?

00:13:56.320 --> 00:14:00.510
Combining pieces of two or more tracks
to make a piece of a single track.

00:14:00.670 --> 00:14:04.700
There are three kinds of editing
operations we do in AV Foundation.

00:14:04.830 --> 00:14:09.780
Placing segments adjacent to each other,
that is what is called cuts-only editing.

00:14:09.940 --> 00:14:14.810
Mixing audio tracks together
and compositing video tracks.

00:14:15.130 --> 00:14:18.700
Let's look at the way we place
segments adjacent to each other,

00:14:18.700 --> 00:14:21.910
which is with AV composition.

00:14:22.810 --> 00:14:27.240
An AV composition object
is just a kind of asset.

00:14:27.370 --> 00:14:32.040
It's composed of composition tracks,
and the tracks are composed of segments,

00:14:32.160 --> 00:14:35.480
each of which can come
from a different asset.

00:14:35.600 --> 00:14:39.450
Typically, we will be using an
AV mutable composition and

00:14:39.560 --> 00:14:43.710
mutable composition track,
but segments are always

00:14:43.710 --> 00:14:45.730
immutable objects.

00:14:46.690 --> 00:14:49.600
Let's look at a picture
to make it a bit clearer.

00:14:49.750 --> 00:14:52.900
We have three videos,
three assets to start with.

00:14:53.080 --> 00:14:55.290
We take portions of each asset.

00:14:55.620 --> 00:14:59.390
The portion I take from each
asset doesn't have to be

00:14:59.390 --> 00:15:01.560
the same across all tracks.

00:15:01.590 --> 00:15:04.780
I could take very different
portions of each track.

00:15:04.780 --> 00:15:08.740
Let's look at what this looks
like at the object level.

00:15:09.250 --> 00:15:13.250
We have a composition object
that contains two track objects,

00:15:13.280 --> 00:15:17.740
and each track object contains
several segment objects.

00:15:17.900 --> 00:15:23.500
Looking at one of the segments,
it indicates which movie is the source,

00:15:23.670 --> 00:15:29.940
which track within the movie,
and the time range within that movie.

00:15:29.940 --> 00:15:34.500
I want to talk briefly about how
AV Foundation represents time.

00:15:34.770 --> 00:15:38.410
We don't use floating
point in AV Foundation.

00:15:38.410 --> 00:15:44.200
Time scales like 29.97 don't mesh
well with binary floating point.

00:15:44.320 --> 00:15:54.270
Remember 29.97 is actually precisely
30,000 divided by 1,001 or 29.9700299700,

00:15:54.270 --> 00:15:54.900
etc.

00:15:55.020 --> 00:15:59.400
When you mix this with other time scales,
it's easy to lose precision.

00:15:59.990 --> 00:16:02.850
AV Foundation uses core media time.

00:16:03.010 --> 00:16:05.730
This is not an object, but a C struct.

00:16:05.860 --> 00:16:08.500
It is a value over a time scale.

00:16:08.610 --> 00:16:10.660
The value is 64 bits.

00:16:10.740 --> 00:16:13.340
The time scale is 32 bits.

00:16:13.490 --> 00:16:18.980
The EPAC is used to distinguish values
associated with distinct timelines,

00:16:18.980 --> 00:16:22.100
and EPOC0 is also used for durations.

00:16:22.620 --> 00:16:28.560
We also have CM Time Range,
which is a start plus a duration.

00:16:28.600 --> 00:16:32.630
And especially for compositions,
CM Time Mapping

00:16:33.200 --> 00:16:38.560
which maps a time range in the source
asset to a time range in the composition.

00:16:38.620 --> 00:16:39.860
That's compositions.

00:16:39.910 --> 00:16:42.690
Now let's talk about mixing audio.

00:16:43.280 --> 00:16:48.300
As an example, let's take adding a
commentary track to a video.

00:16:48.440 --> 00:16:51.350
When we add a shorter
piece like this to a track,

00:16:51.580 --> 00:16:56.510
the framework will implicitly add
empty segments to fill out the track.

00:16:57.340 --> 00:17:00.230
What we want to do is have
the volume of the main audio

00:17:00.230 --> 00:17:02.200
drop during the commentary.

00:17:02.390 --> 00:17:04.060
This is called ducking the main track.

00:17:04.200 --> 00:17:09.340
We will find that the only part of this
that needs to be described is denoted

00:17:09.340 --> 00:17:11.060
by the part I've highlighted in red.

00:17:11.400 --> 00:17:16.200
That is, we only need to describe
when the audio level changes.

00:17:16.750 --> 00:17:20.470
Looking at this as objects,
we have an AV composition that includes

00:17:20.470 --> 00:17:24.770
the video and audio tracks from an asset,
plus the commentary track

00:17:24.770 --> 00:17:26.640
that comes from another asset.

00:17:26.910 --> 00:17:32.420
The audio mix object only needs to
describe what happens to the one audio

00:17:32.520 --> 00:17:38.200
track because the commentary track will
be implicitly mixed at normal volume.

00:17:39.110 --> 00:17:44.840
So audio mix objects contain a list
of audio mix input parameter objects.

00:17:44.990 --> 00:17:48.270
Each input parameter object
describes how the volume of one

00:17:48.270 --> 00:17:51.090
track should be adjusted over time.

00:17:51.110 --> 00:17:55.660
And tracks that are not
mentioned are implicitly mixed.

00:17:55.870 --> 00:17:59.540
Let's turn to video compositing,
which is a bit different.

00:18:00.920 --> 00:18:05.680
A video composition object describes
how to composite video tracks together.

00:18:05.720 --> 00:18:10.940
So it describes the compositing order
and what should be done with each layer.

00:18:10.970 --> 00:18:15.460
In this example, we want to take track
A and then composite track

00:18:15.620 --> 00:18:19.720
A over B with A fading out,
and then use track B by

00:18:19.830 --> 00:18:23.790
itself for a while,
and then instead layer track B in

00:18:23.790 --> 00:18:28.840
front of A and fade B out to get to A,
so that we get a result that

00:18:28.840 --> 00:18:30.850
looks something like this.

00:18:31.100 --> 00:18:34.180
What you need to do is
for every time range,

00:18:34.180 --> 00:18:39.160
make a composition instruction and then
make a composition layer instruction for

00:18:39.160 --> 00:18:40.980
each track that you need to composite.

00:18:41.000 --> 00:18:45.500
You order the layer instructions
into the composition instruction in

00:18:45.500 --> 00:18:50.340
the compositing order and then order
the composition instructions into

00:18:50.340 --> 00:18:52.990
the video composition in time order.

00:18:54.030 --> 00:18:57.000
Audio and video are very
different as we can see now.

00:18:57.000 --> 00:19:00.930
Audio is organized by tracks first
and then we describe how the volume

00:19:00.930 --> 00:19:03.000
of the track changes over time.

00:19:03.000 --> 00:19:07.060
Video on the other hand is
organized by time ranges and then

00:19:07.140 --> 00:19:09.990
describes the order of compositing.

00:19:09.990 --> 00:19:13.040
Also,
audio has implicit inclusion but with

00:19:13.070 --> 00:19:16.000
video everything needs to be explicit.

00:19:18.140 --> 00:19:21.480
These merge objects,
audio mix and video composition,

00:19:21.480 --> 00:19:22.450
are not assets.

00:19:22.790 --> 00:19:26.240
They are operations to
be performed on assets.

00:19:26.410 --> 00:19:31.020
What you do is pass one or both of
these objects to something else in

00:19:31.020 --> 00:19:34.120
order to have the operations applied.

00:19:35.400 --> 00:19:38.340
For example,
a player item can take an audio

00:19:38.340 --> 00:19:42.700
mix and a video composition,
which describe how the asset should

00:19:42.700 --> 00:19:45.300
be modified before being displayed.

00:19:45.300 --> 00:19:49.870
An export session can do the same thing,
describing how the asset track

00:19:49.940 --> 00:19:52.300
should be mixed as you save a copy.

00:19:52.300 --> 00:19:56.370
For still images, we can describe how to
composite the video tracks

00:19:56.370 --> 00:19:58.300
before the still is generated.

00:19:58.300 --> 00:20:02.280
And when reading either
audio or video from an asset,

00:20:02.500 --> 00:20:06.290
you can also have a mix
applied as you are reading.

00:20:07.840 --> 00:20:11.500
The things to remember:
a composition is an asset.

00:20:11.680 --> 00:20:15.700
Audio mixes are by track and
include other tracks implicitly,

00:20:15.870 --> 00:20:19.670
whereas video compositions
are by time range and must

00:20:19.670 --> 00:20:22.160
mention everything explicitly.

00:20:23.760 --> 00:20:25.940
Now let's turn to core animation.

00:20:25.990 --> 00:20:29.420
Your first question is probably,
"What does core animation

00:20:29.420 --> 00:20:30.990
have to do with videos?"

00:20:32.100 --> 00:20:36.740
Core animation underpins
all UI kit rendering in iOS.

00:20:36.760 --> 00:20:39.680
You may not notice this because
we hide it for the simple cases,

00:20:39.680 --> 00:20:41.910
for example, DrawRect.

00:20:42.350 --> 00:20:45.410
Core animation has tremendous
functionality for graphical

00:20:45.410 --> 00:20:48.320
operations and rather than
create some new interface,

00:20:48.500 --> 00:20:51.490
we decided to leverage core
animation directly to enable

00:20:51.570 --> 00:20:54.390
things like fancy title sequences.

00:20:56.010 --> 00:21:01.740
Since UIView uses CA layer subclasses,
AV Foundation provides CA layer

00:21:01.740 --> 00:21:07.730
subclasses for the UI to use:
Player Layer for playback and Capture

00:21:07.730 --> 00:21:10.170
Preview for looking at capture.

00:21:10.900 --> 00:21:12.380
Here's a class diagram.

00:21:12.440 --> 00:21:16.190
You can see that AV Player layer
knows about a player and

00:21:16.190 --> 00:21:20.490
AV Capture Video Preview layer
knows about a capture session.

00:21:20.650 --> 00:21:24.720
The capture preview layer has some
extra functionality to be able

00:21:24.720 --> 00:21:26.860
to easily mirror the video input.

00:21:27.950 --> 00:21:32.700
What if I'm using a real core animation,
say adding some title to my video?

00:21:32.750 --> 00:21:35.840
Let's take an example we used
at the developers conference.

00:21:35.900 --> 00:21:39.640
We want to add a title and some stars,
and we want to spin the stars

00:21:39.640 --> 00:21:44.920
around and have the whole thing
fade out after 10 seconds.

00:21:45.170 --> 00:21:50.320
We want it to look something like this.

00:21:50.390 --> 00:21:54.970
There we have our stars spinning,
and then it all fades out.

00:22:02.060 --> 00:22:07.180
So we need a top-level layer,
and that layer needs to have an animation

00:22:07.570 --> 00:22:09.570
on it that will do the fade-out.

00:22:09.570 --> 00:22:13.390
And then we need a layer underneath
to hold that title string,

00:22:13.690 --> 00:22:17.240
and another layer to
hold our ring of stars.

00:22:17.280 --> 00:22:21.490
The ring of stars will need an
animation on it to do the spin,

00:22:21.490 --> 00:22:23.370
but there's a problem.

00:22:24.120 --> 00:22:28.000
Our UI view is going to
reference an AV player layer,

00:22:28.050 --> 00:22:31.270
or maybe some layer tree that
contains that player layer,

00:22:31.290 --> 00:22:32.990
and that will bring in the video.

00:22:33.110 --> 00:22:38.200
But there's a distinction between times
above the line and below the line.

00:22:38.360 --> 00:22:41.040
Above the line,
things happen in real time,

00:22:41.100 --> 00:22:43.260
time since boot.

00:22:43.310 --> 00:22:46.740
Below the line,
things happen in movie time,

00:22:46.860 --> 00:22:51.160
which can go forward or backward.

00:22:51.840 --> 00:22:55.780
So how do we make our
animation happen in movie time?

00:22:55.930 --> 00:23:01.020
We add an AV synchronized layer,
which fixes the layer tree underneath

00:23:01.020 --> 00:23:03.020
it to happen in movie time.

00:23:04.650 --> 00:23:08.230
AV synchronized layer needs
to reference a specific player

00:23:08.230 --> 00:23:12.620
item which is associated with
the asset we want to sync with.

00:23:12.900 --> 00:23:15.940
But this only works in the
context of an AV player.

00:23:16.050 --> 00:23:19.880
So we need something else:
an AV Video Composition

00:23:19.940 --> 00:23:24.040
Core Animation Tool,
which we use to sync otherwise.

00:23:24.180 --> 00:23:26.840
This is attached to a video composition.

00:23:26.950 --> 00:23:30.110
That is,
I have to have a video composition

00:23:30.110 --> 00:23:33.020
in order to sync in other contexts.

00:23:34.860 --> 00:23:39.540
Now both of these objects are
constructed only with class methods.

00:23:39.540 --> 00:23:44.040
AV synchronized layer is created
with a "with player" item.

00:23:44.140 --> 00:23:44.900
That's a shortened name.

00:23:45.100 --> 00:23:48.870
It's actually called synchronized
layer with player item.

00:23:49.420 --> 00:23:52.300
Core Animation Tool has two methods.

00:23:52.390 --> 00:23:57.580
The first method is "With
Additional Layer as Track ID." Now,

00:23:57.580 --> 00:23:59.300
I've shortened that name again.

00:23:59.300 --> 00:24:03.080
It's actually "Video Composition
Core Animation Tool with

00:24:03.080 --> 00:24:05.290
Additional Layer as Track ID."

00:24:05.650 --> 00:24:09.540
What this does is add the first argument,
a CA layer,

00:24:09.580 --> 00:24:15.100
as a virtual track to the asset that
the video composition is operating on.

00:24:15.150 --> 00:24:19.800
The second method is with
post-processing as video layer.

00:24:20.610 --> 00:24:24.040
It renders its second argument,
a CA layer,

00:24:24.210 --> 00:24:28.690
and replaces the first argument,
a contained CA layer, with the results of

00:24:28.690 --> 00:24:30.480
the video composition.

00:24:30.880 --> 00:24:35.310
So either what we are doing is
we're making a layer tree a track

00:24:35.310 --> 00:24:39.270
within our video composition,
or instead we make the video composition

00:24:39.540 --> 00:24:42.240
a layer within our layer tree.

00:24:42.370 --> 00:24:46.040
Now, rendering a layer tree is
an expensive operation,

00:24:46.130 --> 00:24:50.680
so we can use the enablePostProcessing
property to tell a composition

00:24:50.680 --> 00:24:54.000
instruction that is a particular
time range of the video that we

00:24:54.000 --> 00:24:58.110
don't want to render the layer
tree during that time range since we,

00:24:58.110 --> 00:25:00.780
as programmers,
know it will have no effect.

00:25:01.810 --> 00:25:05.300
Another point that's important
has to do with coordinate systems.

00:25:05.390 --> 00:25:10.140
The coordinate system for UIView
has its origin in the upper left.

00:25:10.610 --> 00:25:15.320
But CAA layers default to
having origin lower left.

00:25:15.330 --> 00:25:19.090
Now,
when I place a CAA layer in a UI view,

00:25:19.090 --> 00:25:22.050
then the coordinate
system will get flipped.

00:25:22.180 --> 00:25:25.290
But this only happens
when I'm in a UI view.

00:25:25.430 --> 00:25:28.580
So what do we do when we're
using the core animation tool?

00:25:28.740 --> 00:25:30.890
Because we have no UI view there.

00:25:31.000 --> 00:25:34.740
Instead, we have to tell the layer
to flip its geometry.

00:25:34.830 --> 00:25:38.140
This property is inherited,
so we only need to set it

00:25:38.140 --> 00:25:39.890
at the root of the tree.

00:25:42.360 --> 00:25:46.330
So, for display,
remember to use AV Player Layer or

00:25:46.330 --> 00:25:49.080
AV Capture Video Preview Layer.

00:25:49.250 --> 00:25:51.790
For timing,
use Synchronize Layer if you're

00:25:51.790 --> 00:25:56.750
in an AV Player context and use
Core Animation Tool everywhere else.

00:25:57.720 --> 00:26:01.690
In closing,
I talked about three groups of classes:

00:26:01.810 --> 00:26:05.370
playback, edit, and core animation.

00:26:05.680 --> 00:26:08.580
I didn't talk about ReadWrite or Capture.

00:26:08.710 --> 00:26:12.240
There are also classes
around metadata and audio.

00:26:12.370 --> 00:26:15.300
If you have questions about
any part of AV Foundation,

00:26:15.450 --> 00:26:16.550
feel free to email me.

00:26:16.600 --> 00:26:20.600
I mentioned several videos from
the Developers Conference 2011.

00:26:20.600 --> 00:26:24.230
If you're interested in Capture,
you should also look at the Capture

00:26:24.230 --> 00:26:26.570
talk from the 2010 conference.

00:26:26.730 --> 00:26:29.360
I wanted to call out several
sample code programs:

00:26:29.450 --> 00:26:32.780
ROSI Writer,
if you're manipulating frames;

00:26:32.790 --> 00:26:35.980
AV Player Demo for information
on proper queue handling and

00:26:35.980 --> 00:26:37.700
also how to deal with metadata.

00:26:37.700 --> 00:26:40.700
Also,
you should look at the OS X samples.

00:26:40.700 --> 00:26:44.680
Remember that AV Foundation is almost
identical on the two platforms.

00:26:44.790 --> 00:26:49.700
Look especially at the Reader Writer
sample and the AV Metadata Editor.

00:26:50.930 --> 00:26:51.980
So there you have it.

00:26:52.250 --> 00:26:55.800
I look forward to seeing what you
do with video in your application.