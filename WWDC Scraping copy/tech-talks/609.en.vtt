WEBVTT

00:00:00.651 --> 00:00:04.121
Hello, and welcome to Advance
Scene Understanding in AR.

00:00:04.121 --> 00:00:07.157
In this video I'll introduce
the new capabilities

00:00:07.157 --> 00:00:09.142
of ARKit and RealityKit

00:00:09.142 --> 00:00:13.146
enabled by the LiDAR Scanner
on the new iPad Pro.

00:00:13.146 --> 00:00:15.399
iOS and ipadOS
provide developers

00:00:15.399 --> 00:00:19.870
with two powerful frameworks
to help you build apps for AR.

00:00:19.870 --> 00:00:22.689
ARKit combines
positional tracking,

00:00:22.689 --> 00:00:24.074
scene understanding,

00:00:24.074 --> 00:00:25.976
and integration
with rendering technologies

00:00:25.976 --> 00:00:28.862
to enable a variety
of AR experiences

00:00:28.862 --> 00:00:32.299
using either the back-facing
or front-facing camera.

00:00:32.299 --> 00:00:33.634
And RealityKit --

00:00:33.634 --> 00:00:37.020
a new high-level framework
built specifically for AR --

00:00:37.020 --> 00:00:40.073
provides photorealistic
rendering and special effects,

00:00:40.073 --> 00:00:43.744
scalable performance,
and a Swift-based API,

00:00:43.744 --> 00:00:48.098
making it easy to prototype
and build great AR experiences.

00:00:48.098 --> 00:00:50.150
Today we're excited
to talk about advances

00:00:50.150 --> 00:00:52.769
in both frameworks,
which are only made possible

00:00:52.769 --> 00:00:57.140
by the hardware capabilities
on the new iPad Pro.

00:00:57.140 --> 00:01:00.911
The new iPad Pro comes equipped
with a LiDAR Scanner.

00:01:00.911 --> 00:01:02.946
This is used
to determine distance

00:01:02.946 --> 00:01:05.465
by measuring
at nanosecond speeds

00:01:05.465 --> 00:01:08.118
how long it takes
for light to reach an object

00:01:08.118 --> 00:01:10.437
in front of you
and reflect back.

00:01:10.437 --> 00:01:13.090
This is effective
up to five meters away

00:01:13.090 --> 00:01:16.243
and operates both indoors
and outdoors.

00:01:16.243 --> 00:01:19.313
And coupling this capability
with information captured

00:01:19.313 --> 00:01:21.965
by the Wide and Ultra Wide
cameras gives you

00:01:21.965 --> 00:01:25.135
an incredible understanding
of your environment.

00:01:25.135 --> 00:01:28.438
So let's see how this improves
AR experiences

00:01:28.438 --> 00:01:32.509
built with each of these
frameworks, starting with ARKit.

00:01:32.509 --> 00:01:35.629
The new update to ARKit
version 3.5

00:01:35.629 --> 00:01:37.898
delivers a number of new
features and improvements

00:01:37.898 --> 00:01:40.233
that take full advantage
of the LiDAR Scanner

00:01:40.233 --> 00:01:42.185
on the new iPad Pro.

00:01:42.185 --> 00:01:45.555
Scene Geometry is a new API
that provides your apps

00:01:45.555 --> 00:01:49.960
with a detailed topological map
of your surrounding environment.

00:01:49.960 --> 00:01:52.362
The LiDAR Scanner
also simplifies

00:01:52.362 --> 00:01:55.549
the AR onboarding experience
by more quickly

00:01:55.549 --> 00:01:58.385
and more accurately
detecting surfaces.

00:01:58.385 --> 00:02:01.705
And existing ARKit features --
including motion capture,

00:02:01.705 --> 00:02:03.907
people occlusion,
and raycasting --

00:02:03.907 --> 00:02:06.109
also benefit without requiring

00:02:06.109 --> 00:02:08.745
any additional
application changes.

00:02:08.745 --> 00:02:11.348
So let's start
with Scene Geometry.

00:02:11.348 --> 00:02:14.668
Scene Geometry provides you
with a triangle mesh,

00:02:14.668 --> 00:02:18.405
representing a topological
mapping of your environment.

00:02:18.405 --> 00:02:22.693
And optionally, that mesh
can include semantic information

00:02:22.693 --> 00:02:24.728
that classifies
what's being seen.

00:02:24.728 --> 00:02:27.998
This includes things like tables
and chairs, floors, walls,

00:02:27.998 --> 00:02:30.283
ceilings, the windows,
and so on.

00:02:30.283 --> 00:02:33.186
All this information could be
used to allow occlusion

00:02:33.186 --> 00:02:36.173
of virtual content
by real-world objects,

00:02:36.173 --> 00:02:40.160
environment-dependent physics,
and illumination of both real

00:02:40.160 --> 00:02:42.829
and virtual objects
in your scene.

00:02:42.829 --> 00:02:46.133
So let's see this mapping
in action.

00:02:46.133 --> 00:02:49.469
Here's an example being taken
from an indoor scene.

00:02:49.469 --> 00:02:52.689
We're overlaying
the AR frame image with a mesh

00:02:52.689 --> 00:02:56.510
being generated by ARKit,
using the LiDAR sensor.

00:02:56.510 --> 00:02:58.311
You can see
as we sweep around the room

00:02:58.311 --> 00:03:01.598
how quickly we're able to detect
the shape of the furniture

00:03:01.598 --> 00:03:03.917
and the layout
of the environment.

00:03:03.917 --> 00:03:06.203
And the colors are based
on a classification

00:03:06.203 --> 00:03:09.890
of what the mesh overlays.

00:03:09.890 --> 00:03:12.492
So let's take a look at the API.

00:03:12.492 --> 00:03:13.927
Scene Geometry is enabled

00:03:13.927 --> 00:03:16.580
through a new scene
reconstruction property

00:03:16.580 --> 00:03:19.149
on
ARWorldTtrackingConfiguration.

00:03:19.149 --> 00:03:20.100
Now, there's two options,

00:03:20.100 --> 00:03:22.919
depending on what data you'd
like to have generated.

00:03:22.919 --> 00:03:25.839
The first option is to generate
just the mesh,

00:03:25.839 --> 00:03:29.459
meaning only the topological
information will be surfaced.

00:03:29.459 --> 00:03:32.529
This is intended for apps doing
things like object placement,

00:03:32.529 --> 00:03:34.798
where they don't depend on
a classification

00:03:34.798 --> 00:03:36.967
of the surrounding objects.

00:03:36.967 --> 00:03:39.820
The other option is
.meshWithClassification.

00:03:39.820 --> 00:03:41.138
And as the name implies,

00:03:41.138 --> 00:03:43.290
this adds semantic
classification

00:03:43.290 --> 00:03:45.258
for all the Scene Geometry.

00:03:45.258 --> 00:03:47.794
This is helpful for apps
that want different behaviors,

00:03:47.794 --> 00:03:49.663
depending on what's
in the scene,

00:03:49.663 --> 00:03:51.565
such as different lighting
on the floor

00:03:51.565 --> 00:03:53.183
versus on the table.

00:03:53.183 --> 00:03:55.285
And here down below,
you see the code;

00:03:55.285 --> 00:03:56.386
it's pretty simple.

00:03:56.386 --> 00:03:57.904
We're using world tracking,

00:03:57.904 --> 00:04:00.340
and we test to verify
that we're running on a device

00:04:00.340 --> 00:04:02.325
that supports
scene reconstruction.

00:04:02.325 --> 00:04:05.061
If so, then we select
the mesh option here

00:04:05.061 --> 00:04:07.147
and start running the session.

00:04:07.147 --> 00:04:10.851
Once the session is running with
scene reconstruction enabled,

00:04:10.851 --> 00:04:13.520
the AR session will return
AR mesh anchors.

00:04:13.520 --> 00:04:15.422
These are just like
any other anchor,

00:04:15.422 --> 00:04:17.023
and changes come through
the usual

00:04:17.023 --> 00:04:19.976
AR session delegate methods,
like did add(anchor:),

00:04:19.976 --> 00:04:22.629
did update(anchor:),
and did remove(anchor:).

00:04:22.629 --> 00:04:27.317
And each mesh anchor represents
a local area of mesh geometry.

00:04:27.317 --> 00:04:29.569
It's described with a transform
of the anchor

00:04:29.569 --> 00:04:32.105
and a mesh geometry object.

00:04:32.105 --> 00:04:35.075
An ARMeshGeometry object holds
all the information

00:04:35.075 --> 00:04:38.228
that you'll need to represent
the surrounding environment.

00:04:38.228 --> 00:04:40.664
Each object contains
a list of vertices,

00:04:40.664 --> 00:04:43.800
normals, faces,
and a semantic classification

00:04:43.800 --> 00:04:45.852
if enabled for each phase.

00:04:45.852 --> 00:04:47.804
These are all provided
as MTLBuffers

00:04:47.804 --> 00:04:51.791
to allow them to be directly
integrated into your renderers.

00:04:51.791 --> 00:04:54.060
Now there's some interesting
interplay that happens

00:04:54.060 --> 00:04:56.863
between Scene Geometry
and plane detection.

00:04:56.863 --> 00:04:58.999
When scene reconstruction
and plane detection

00:04:58.999 --> 00:05:00.367
are both enabled,

00:05:00.367 --> 00:05:02.419
the mesh that's constructed
will be flattened

00:05:02.419 --> 00:05:04.621
to match overlapping planes.

00:05:04.621 --> 00:05:06.723
This is useful
for an object placement

00:05:06.723 --> 00:05:09.142
where surfaces need to be
consistently flat

00:05:09.142 --> 00:05:11.828
to allow for a smooth object
movement.

00:05:11.828 --> 00:05:14.564
On the other hand,
if you're using Scene Geometry,

00:05:14.564 --> 00:05:16.416
and plane detection
is not enabled,

00:05:16.416 --> 00:05:18.168
the mesh will no longer be
flattened.

00:05:18.168 --> 00:05:19.619
But this combination

00:05:19.619 --> 00:05:22.989
will preserve more detail
in the meshed surfaces.

00:05:22.989 --> 00:05:25.075
OK, so that's Scene Geometry.

00:05:25.075 --> 00:05:27.160
Moving on, there's several
more improvements

00:05:27.160 --> 00:05:29.663
enabled by the LiDAR Scanner.

00:05:29.663 --> 00:05:33.300
The first is much simpler
and faster onboarding.

00:05:33.300 --> 00:05:36.419
With the LiDAR Scanner,
planer surfaces are detected

00:05:36.419 --> 00:05:39.639
almost instantly
and more accurately as well.

00:05:39.639 --> 00:05:42.125
This is true even
on low-features surfaces,

00:05:42.125 --> 00:05:43.743
like white walls.

00:05:43.743 --> 00:05:46.346
So the result is that
the plane-detection onboarding

00:05:46.346 --> 00:05:48.215
that previously took
a few seconds

00:05:48.215 --> 00:05:50.951
and required some amount
of user guidance

00:05:50.951 --> 00:05:53.853
can now occur
completely seamlessly.

00:05:53.853 --> 00:05:55.639
And no changes are necessary.

00:05:55.639 --> 00:05:57.891
All ARKit apps will benefit
from this

00:05:57.891 --> 00:06:00.327
when running
on the new iPad Pro.

00:06:00.327 --> 00:06:04.564
Existing apps will also benefit
from improved raycasting.

00:06:04.564 --> 00:06:07.267
The enhanced scene understanding
of ARKit

00:06:07.267 --> 00:06:09.936
allows quicker
and more accurate raycasting

00:06:09.936 --> 00:06:12.339
against horizontal
and vertical planes.

00:06:12.339 --> 00:06:15.108
Additionally, the new iPad Pro
can raycast

00:06:15.108 --> 00:06:18.428
against a wider range
of surfaces than ever before.

00:06:18.428 --> 00:06:21.731
Just set the allowing target
to estimatedPlane

00:06:21.731 --> 00:06:25.352
and data from the LiDAR Scanner
will provide raycasting results

00:06:25.352 --> 00:06:28.021
that match the surrounding
environment.

00:06:28.021 --> 00:06:31.024
For example, objects can now
be placed on all surfaces

00:06:31.024 --> 00:06:34.127
of a large chair
or a couch as seen here.

00:06:34.127 --> 00:06:36.229
Motion Capture
and people occlusion

00:06:36.229 --> 00:06:38.682
are also improved
due to the LiDAR Scanner

00:06:38.682 --> 00:06:41.284
providing more accurate
depth information.

00:06:41.284 --> 00:06:43.103
Apps using Motion Capture
will benefit

00:06:43.103 --> 00:06:45.438
from a more accurate
scale estimation,

00:06:45.438 --> 00:06:47.574
and the depth values
for people occlusion

00:06:47.574 --> 00:06:49.643
are more accurate as well.

00:06:49.643 --> 00:06:52.562
In addition, people occlusion
and the Scene Geometry API

00:06:52.562 --> 00:06:55.482
can work together
when both features are enabled.

00:06:55.482 --> 00:06:57.684
The very dynamic
geometry of people

00:06:57.684 --> 00:07:00.553
can be excluded
from the scene reconstruction,

00:07:00.553 --> 00:07:03.356
which in turn,
provides a more stable mesh

00:07:03.356 --> 00:07:05.859
of the real-world environment.

00:07:05.859 --> 00:07:08.561
So that's a quick look
at ARKit 3.5

00:07:08.561 --> 00:07:10.263
on the new iPad Pro.

00:07:10.263 --> 00:07:12.782
Scene Geometry provides
a topological map

00:07:12.782 --> 00:07:14.434
of the environment around you.

00:07:14.434 --> 00:07:17.120
Planer surfaces are detected
almost instantly

00:07:17.120 --> 00:07:20.340
and more accurately,
which simplifies onboarding.

00:07:20.340 --> 00:07:22.242
Raycasting is more accurate

00:07:22.242 --> 00:07:24.778
and can take Scene Geometry
into account,

00:07:24.778 --> 00:07:29.065
and Motion Capture and people
occlusion are improved as well.

00:07:29.065 --> 00:07:31.484
ARKit is also tightly integrated

00:07:31.484 --> 00:07:34.621
with our higher-level
AR framework called RealityKit.

00:07:34.621 --> 00:07:37.440
RealityKit provides
photo-realistic rendering,

00:07:37.440 --> 00:07:41.761
camera effects, animations,
physics, and a lot more.

00:07:41.761 --> 00:07:45.215
It was built from the ground up
specifically for AR.

00:07:45.215 --> 00:07:49.719
RealityKit takes advantage
of the new ARKit 3.5 features

00:07:49.719 --> 00:07:52.288
and makes it really easy for you
to integrate them

00:07:52.288 --> 00:07:56.176
into either new
or existing RealityKit apps.

00:07:56.176 --> 00:07:58.178
These capabilities
can be accessed

00:07:58.178 --> 00:08:00.997
through the new
scene understanding API.

00:08:00.997 --> 00:08:04.601
It provides options for enabling
LiDAR-enhanced physics,

00:08:04.601 --> 00:08:06.469
occlusion, and lighting,

00:08:06.469 --> 00:08:09.773
and it's all access through some
simple settings on the ARView,

00:08:09.773 --> 00:08:11.508
so let's take a look.

00:08:11.508 --> 00:08:13.026
With the new iPad Pro,

00:08:13.026 --> 00:08:16.096
RealityKit is able to determine
physics interactions

00:08:16.096 --> 00:08:18.982
between virtual objects
in the Scene Geometry

00:08:18.982 --> 00:08:22.118
from surfaces we detect
in the real world.

00:08:22.118 --> 00:08:23.887
So you can have a virtual ball

00:08:23.887 --> 00:08:26.756
bounce off of
your real-world furniture.

00:08:26.756 --> 00:08:28.875
To do this, first you'll
generate collision shapes

00:08:28.875 --> 00:08:31.194
for your virtual content,
the ModelEntity,

00:08:31.194 --> 00:08:33.496
and initialize its physics body.

00:08:33.496 --> 00:08:35.565
Then just add the physics option

00:08:35.565 --> 00:08:38.685
to your ARView's
sceneUnderstanding.options set,

00:08:38.685 --> 00:08:41.321
and RealityKit
will take care of the rest.

00:08:41.321 --> 00:08:43.440
Likewise with occlusion,

00:08:43.440 --> 00:08:45.558
RealityKit uses
the Scene Geometry

00:08:45.558 --> 00:08:47.560
that's detected
from real-world objects,

00:08:47.560 --> 00:08:50.096
like doorways, and tables,
chairs, and so on

00:08:50.096 --> 00:08:53.316
to occlude the virtual objects
in your scene.

00:08:53.316 --> 00:08:55.085
This is totally automatic.

00:08:55.085 --> 00:08:57.687
All it takes to enable this
is to add occlusion

00:08:57.687 --> 00:08:59.589
to the
sceneUnderstanding.options set

00:08:59.589 --> 00:09:01.357
for your ARView.

00:09:01.357 --> 00:09:03.843
RealityKit will take care of
everything else under the hood,

00:09:03.843 --> 00:09:05.779
and your virtual content
will be occluded

00:09:05.779 --> 00:09:08.531
by all the major objects
in the environment.

00:09:08.531 --> 00:09:10.533
Here's an example of that.

00:09:10.533 --> 00:09:13.536
Our virtual robot
is walking around on the floor.

00:09:13.536 --> 00:09:16.456
But you'll see as the camera
moves behind the column,

00:09:16.456 --> 00:09:17.991
the robot is occluded.

00:09:17.991 --> 00:09:20.794
Its geometry is disappearing
from the rendering,

00:09:20.794 --> 00:09:22.996
and that helps maintain
the illusion

00:09:22.996 --> 00:09:26.449
of proper depth in the scene.

00:09:26.449 --> 00:09:28.918
Now the third piece
is for lighting.

00:09:28.918 --> 00:09:30.303
With a new RealityKit,

00:09:30.303 --> 00:09:34.007
your virtual light sources can
illuminate real-world surfaces.

00:09:34.007 --> 00:09:37.193
This is because we're able to
illuminate the Scene Geometry,

00:09:37.193 --> 00:09:40.146
which is very accurately fitted
to those surfaces

00:09:40.146 --> 00:09:42.081
with the help
of the LiDAR Scanner.

00:09:42.081 --> 00:09:43.166
And like before,

00:09:43.166 --> 00:09:46.286
enabling this is just as simple
as adding receivesLighting

00:09:46.286 --> 00:09:49.656
to the ARView's
sceneUnderstanding.options set.

00:09:49.656 --> 00:09:52.942
And finally, support for these
features extends to scenes

00:09:52.942 --> 00:09:55.411
built in Reality Composer
as well.

00:09:55.411 --> 00:09:57.814
Physics can be configured
to collide virtual objects

00:09:57.814 --> 00:10:00.600
with the real world using
the Scene Geometry mesh,

00:10:00.600 --> 00:10:03.686
and occlusion of virtual content
by real-world objects

00:10:03.686 --> 00:10:06.289
can be enabled
in the Inspector panel.

00:10:06.289 --> 00:10:10.276
For more information,
please visit developer.apple.com

00:10:10.276 --> 00:10:13.229
where you'll find links
to documentation, sample code,

00:10:13.229 --> 00:10:16.850
or developer videos
like this one, and a lot more.

00:10:16.850 --> 00:10:18.852
Thank you for watching!