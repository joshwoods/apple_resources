WEBVTT

00:00:02.280 --> 00:00:06.450
Ruiwei: Hello, my name is Ruiwei, and I am a software engineer

00:00:06.450 --> 00:00:08.970
working on Metal developer tools.

00:00:08.970 --> 00:00:12.270
Today, with my colleague, Irfan, we are going to show you

00:00:12.270 --> 00:00:14.283
the latest Metal profiling tools.

00:00:15.690 --> 00:00:20.220
The Apple family 9 GPUs, the M3 and A17 Pro,

00:00:20.220 --> 00:00:22.893
feature a completely new shader core architecture.

00:00:24.000 --> 00:00:28.350
We took this opportunity to reimagine the profiling tools,

00:00:28.350 --> 00:00:30.933
and build state-of-the-art new workflows.

00:00:32.190 --> 00:00:34.470
To learn more about this new architecture,

00:00:34.470 --> 00:00:37.350
please check out "Explore GPU advancements

00:00:37.350 --> 00:00:39.477
in M3 and A17 Pro".

00:00:40.770 --> 00:00:43.380
In this talk, I will start by showing you

00:00:43.380 --> 00:00:45.933
the amazing new tools in Xcode 15.

00:00:47.820 --> 00:00:51.390
Then, as occupancy management is very important

00:00:51.390 --> 00:00:53.850
for achieving the best performance,

00:00:53.850 --> 00:00:56.610
Irfan will show you how to profile occupancy

00:00:56.610 --> 00:00:58.713
with a new set of performance counters.

00:01:00.000 --> 00:01:03.240
Lastly, you will learn how to profile ray tracing

00:01:03.240 --> 00:01:04.623
on this new architecture.

00:01:06.060 --> 00:01:08.463
Let's start with the new profiling tools.

00:01:10.050 --> 00:01:12.270
I am working on rendering a street

00:01:12.270 --> 00:01:14.523
with a modern GPU-driven pipeline.

00:01:15.540 --> 00:01:17.910
The rendered image looks beautiful,

00:01:17.910 --> 00:01:19.960
but I'm noticing some performance issues.

00:01:21.780 --> 00:01:24.360
There are two major approaches for charging

00:01:24.360 --> 00:01:27.513
the performance bottlenecks for these kind of applications.

00:01:28.530 --> 00:01:32.130
The first is identify the most expensive shaders,

00:01:32.130 --> 00:01:34.050
and start from there to understand

00:01:34.050 --> 00:01:36.663
what are the costly functions and lines.

00:01:37.950 --> 00:01:39.990
Another approach is start by finding

00:01:39.990 --> 00:01:42.870
the expensive objects or pixels.

00:01:42.870 --> 00:01:46.140
If we are dealing with shaders, they may behave differently,

00:01:46.140 --> 00:01:49.083
depending on the fragment position or thread ID.

00:01:50.550 --> 00:01:55.550
Thanks to the new profiling architecture in M3 and A17 Pro,

00:01:55.552 --> 00:01:57.810
Xcode 15 includes multiple new tools

00:01:57.810 --> 00:02:00.180
to simplify these tasks.

00:02:00.180 --> 00:02:03.420
Today, I'm going to use the new tools and walk you through

00:02:03.420 --> 00:02:06.363
how I found the performance issue of my workload.

00:02:08.880 --> 00:02:12.480
First, let me introduce Shader Cost Graph.

00:02:12.480 --> 00:02:15.180
It is a new tool that can help you find

00:02:15.180 --> 00:02:17.103
and triage expensive shaders.

00:02:18.720 --> 00:02:21.360
Here is a GPU capture of the workload

00:02:21.360 --> 00:02:24.030
I just profiled in Xcode 15.

00:02:24.030 --> 00:02:26.190
By navigating to the performance view,

00:02:26.190 --> 00:02:28.860
the GPU timeline shows the execution

00:02:28.860 --> 00:02:30.993
and performance counters of the workload.

00:02:32.820 --> 00:02:34.950
To view the Shader Cost Graph,

00:02:34.950 --> 00:02:37.023
I can switch to the new Shaders tab.

00:02:39.240 --> 00:02:42.420
The performance navigator on the left shows me a list

00:02:42.420 --> 00:02:45.543
of passes and pipeline states, sorted by cost.

00:02:48.210 --> 00:02:51.870
I can see that the GBuffer Pass is about 50%

00:02:51.870 --> 00:02:55.323
of the total cost, which is more than what I'm expecting.

00:02:56.880 --> 00:02:59.490
To investigate, I will start by looking

00:02:59.490 --> 00:03:03.303
at the expensive pipeline states used in the GBuffer Pass.

00:03:04.410 --> 00:03:07.050
Selecting a pipeline state in the navigator

00:03:07.050 --> 00:03:08.943
will reveal the Shader Cost Graph.

00:03:10.590 --> 00:03:12.660
For random pipelines, Fragment Shader

00:03:12.660 --> 00:03:14.403
will be selected by default.

00:03:18.999 --> 00:03:23.670
Shader Cost Graph is divided into two major parts.

00:03:23.670 --> 00:03:26.580
On top is a flame graph visualizing the most

00:03:26.580 --> 00:03:28.413
expensive shader function cost.

00:03:29.700 --> 00:03:32.583
Below the graph is the corresponding shader source code.

00:03:34.909 --> 00:03:37.440
This is the first time we have a flame graph

00:03:37.440 --> 00:03:40.353
for Metal shaders, and it looks amazing.

00:03:41.190 --> 00:03:44.130
By using the flame graph, I can easily identify

00:03:44.130 --> 00:03:47.820
the most expensive functions of my Fragment Shader.

00:03:47.820 --> 00:03:50.430
Selecting a function in the graph will make

00:03:50.430 --> 00:03:53.223
the source editor jump right to where the source is.

00:03:54.630 --> 00:03:57.900
The source code is annotated with performance annotations

00:03:57.900 --> 00:04:01.803
in the left sidebar, showing how expensive each line is.

00:04:03.330 --> 00:04:06.600
This is a full-screen shader that applies lighting

00:04:06.600 --> 00:04:08.553
to every pixel of the image.

00:04:09.570 --> 00:04:12.450
With the performance annotations in the left sidebar,

00:04:12.450 --> 00:04:13.680
I can quickly identify

00:04:13.680 --> 00:04:16.053
the most expensive shader source lines.

00:04:17.070 --> 00:04:18.780
And hover over the pie chart

00:04:18.780 --> 00:04:20.650
shows me the performance popover.

00:04:21.777 --> 00:04:25.080
The popover provides a detailed breakdown of the line,

00:04:25.080 --> 00:04:27.630
such as exactly how many instructions

00:04:27.630 --> 00:04:29.167
were executed by the GPU,

00:04:30.242 --> 00:04:32.692
and the cost of different instruction categories.

00:04:34.050 --> 00:04:36.750
Because this is a full-screen shader,

00:04:36.750 --> 00:04:38.310
the behavior can vary,

00:04:38.310 --> 00:04:40.470
depending on the fragment location,

00:04:40.470 --> 00:04:43.230
it is possible the performance bottleneck is caused

00:04:43.230 --> 00:04:45.483
by specific areas or pixels.

00:04:46.380 --> 00:04:48.270
To fully understand the issue,

00:04:48.270 --> 00:04:51.033
I need to find the pixels that are more expensive,

00:04:51.960 --> 00:04:54.180
and this is a great opportunity

00:04:54.180 --> 00:04:57.393
to use the next new tool, Performance Heat Maps.

00:04:59.400 --> 00:05:02.640
Performance Heat Maps is a way of visualizing pixel,

00:05:02.640 --> 00:05:06.750
or compute thread information, and performance metrics.

00:05:06.750 --> 00:05:09.780
They are built using the fragment position,

00:05:09.780 --> 00:05:12.603
or compute thread ID of the GPU threads.

00:05:14.550 --> 00:05:16.290
Let's take a look at the different types

00:05:16.290 --> 00:05:18.993
of Performance Heat Maps of the GBuffer pass.

00:05:22.290 --> 00:05:25.173
First is the Shader Execution Cost Heat Map.

00:05:26.010 --> 00:05:29.130
The cost is calculated by looking at the execution time

00:05:29.130 --> 00:05:31.263
and the latency hiding of GPU threads.

00:05:32.520 --> 00:05:35.430
I can easily notice that the pixels in the right half

00:05:35.430 --> 00:05:37.980
of the image are shown in red,

00:05:37.980 --> 00:05:39.873
which means they are more expensive.

00:05:41.130 --> 00:05:44.370
Next is the Thread Divergence Heat Map.

00:05:44.370 --> 00:05:47.640
It visualizes the amount of GPU thread divergence

00:05:47.640 --> 00:05:48.873
in the SIMD groups.

00:05:50.040 --> 00:05:53.010
Divergence increases with control flow differences

00:05:53.010 --> 00:05:57.180
among threads, which can occur with conditional branches

00:05:57.180 --> 00:06:00.153
or inactive threads due to the shape of the geometry.

00:06:02.250 --> 00:06:06.090
Overdraw Heat Map visualizes pixels that have been rendered

00:06:06.090 --> 00:06:07.760
by more than one GPU thread.

00:06:09.210 --> 00:06:11.730
This could be caused by overlapping geometry,

00:06:11.730 --> 00:06:15.840
with blending enabled from one or more render commands.

00:06:15.840 --> 00:06:18.330
Be sure to group your GPU commands

00:06:18.330 --> 00:06:21.570
so that opaque objects are rendered first,

00:06:21.570 --> 00:06:23.820
and then render the transparent ones

00:06:23.820 --> 00:06:26.733
to achieve the best performance on Apple GPUs.

00:06:28.650 --> 00:06:31.410
Instruction Count Heat Map shows exactly

00:06:31.410 --> 00:06:34.830
how many instructions were executed on the GPU

00:06:34.830 --> 00:06:37.040
for each pixel or SIMD group.

00:06:39.240 --> 00:06:42.000
And lastly, Draw ID Heat Map

00:06:42.000 --> 00:06:44.790
color codes different GPU commands.

00:06:44.790 --> 00:06:47.460
In this case, I can see most of the workload

00:06:47.460 --> 00:06:50.160
is rendered by a single command.

00:06:50.160 --> 00:06:52.683
Only the transparent window is separate.

00:06:54.570 --> 00:06:58.560
Now that you know what heat maps are and how they look,

00:06:58.560 --> 00:07:01.743
let's take a look how to access them in Xcode 15.

00:07:03.360 --> 00:07:05.910
To access the Performance Heat Maps,

00:07:05.910 --> 00:07:08.733
I can click the Heat Map tab on the top bar.

00:07:10.770 --> 00:07:14.100
By default, the Shader Execution Cost Heat Map

00:07:14.100 --> 00:07:16.053
and the first attachment are shown.

00:07:17.430 --> 00:07:19.800
Notice that the street part of the scene

00:07:19.800 --> 00:07:22.380
has a much higher execution cost.

00:07:22.380 --> 00:07:25.143
Let's add more heat maps to investigate further.

00:07:27.960 --> 00:07:30.990
By clicking the plus button on the bottom bar,

00:07:30.990 --> 00:07:32.913
the heat map popover will show up.

00:07:33.930 --> 00:07:36.240
This allows me to quickly enable or disable

00:07:36.240 --> 00:07:37.953
different types of heat maps.

00:07:42.360 --> 00:07:45.002
The Instruction Count Heat Map confirms

00:07:45.002 --> 00:07:47.490
that the GPU executes many more instructions

00:07:47.490 --> 00:07:49.410
for pixels of the street,

00:07:49.410 --> 00:07:51.423
which could explain the high cost.

00:07:53.790 --> 00:07:57.060
I can move the pointer to hover over the pixels

00:07:57.060 --> 00:08:01.620
and look at the details, such as the percentile of the cost

00:08:01.620 --> 00:08:03.873
and exact number of instructions.

00:08:05.400 --> 00:08:07.740
The heat maps already gives me ideas

00:08:07.740 --> 00:08:10.443
of why those pixels are more expensive.

00:08:12.000 --> 00:08:14.520
I can also take it further and see exactly

00:08:14.520 --> 00:08:16.710
how my shader renders those pixels

00:08:16.710 --> 00:08:20.973
with the next new tool, Shader Execution History.

00:08:23.370 --> 00:08:25.800
Clicking on the pixel in the performance heat map

00:08:25.800 --> 00:08:28.173
will select the underlying SIMD group.

00:08:29.880 --> 00:08:32.850
This will reveal the shader execution history

00:08:32.850 --> 00:08:35.103
for the SIMD group below the heat maps.

00:08:37.020 --> 00:08:41.370
Shader Execution History is divided into two major parts,

00:08:41.370 --> 00:08:44.703
a timeline on top, and the shader source code below it.

00:08:47.040 --> 00:08:50.040
The timeline from left to right shows the progress

00:08:50.040 --> 00:08:51.930
of the selected SIMD group.

00:08:51.930 --> 00:08:55.320
From top to bottom, the full shader call stack is shown

00:08:55.320 --> 00:08:57.033
at each point of execution.

00:08:58.080 --> 00:09:01.800
This is the first time I can see exactly how SIMD groups

00:09:01.800 --> 00:09:03.990
are executed by the Apple GPUs

00:09:03.990 --> 00:09:05.913
with this powerful visualization.

00:09:07.050 --> 00:09:10.140
By inspecting the timeline, I can immediately identify

00:09:10.140 --> 00:09:13.890
shader functions that takes most of the execution time.

00:09:13.890 --> 00:09:16.920
Metal Debugger also automatically detects loops

00:09:16.920 --> 00:09:19.070
to help you understand the progress better.

00:09:20.070 --> 00:09:22.410
Under my most expensive shader function,

00:09:22.410 --> 00:09:24.780
there's a loop with 12 iterations,

00:09:24.780 --> 00:09:28.773
and is 79% of the total SIMD group execution time.

00:09:29.970 --> 00:09:33.870
In each iteration, apply spotlight is called.

00:09:33.870 --> 00:09:35.280
And there are even more loops

00:09:35.280 --> 00:09:37.773
within the function call, sampling a texture.

00:09:38.970 --> 00:09:42.090
This is odd, there shouldn't be 12 spotlights

00:09:42.090 --> 00:09:43.790
lighting the pixels of the street.

00:09:44.790 --> 00:09:47.280
After checking my workload, I notice

00:09:47.280 --> 00:09:51.060
that there's a misconfiguration duplicating the spotlights.

00:09:51.060 --> 00:09:53.010
After removing the extra lights,

00:09:53.010 --> 00:09:55.803
the performance of the GBuffer Pass is much better.

00:09:57.600 --> 00:10:00.300
To recap, Shader Execution History

00:10:00.300 --> 00:10:03.723
visualizes how SIMD groups were executed by the GPU.

00:10:05.250 --> 00:10:07.800
This includes the state of the threads,

00:10:07.800 --> 00:10:10.773
function call stacks, and loops.

00:10:12.150 --> 00:10:14.670
This provides unprecedented understanding

00:10:14.670 --> 00:10:17.733
of the shader execution that was not possible before.

00:10:19.140 --> 00:10:22.560
So these are the new profiling tools in Xcode 15

00:10:22.560 --> 00:10:26.220
available for M3 and A17 pro.

00:10:26.220 --> 00:10:28.893
I cannot wait to see what you can do with them.

00:10:30.510 --> 00:10:33.000
Now, let me hand over to my colleague, Irfan,

00:10:33.000 --> 00:10:35.643
to tell you all about profiling occupancy.

00:10:36.990 --> 00:10:39.390
Irfan: Thank you, Ruiwei, for showing the brand new tools

00:10:39.390 --> 00:10:43.860
and workflows for the GPU in M3 and A17 Pro.

00:10:43.860 --> 00:10:47.070
Hello, I'm Irfan, and I will start by showing you

00:10:47.070 --> 00:10:51.030
how occupancy profiling works on the new GPU architecture,

00:10:51.030 --> 00:10:53.640
as well as new counters to help you profile

00:10:53.640 --> 00:10:55.323
hardware ray tracing workloads.

00:10:56.820 --> 00:10:59.490
Before I show you how to profile occupancy,

00:10:59.490 --> 00:11:02.400
I recommend watching "Explore GPU advancements

00:11:02.400 --> 00:11:05.100
in M3 and A17 Pro."

00:11:05.100 --> 00:11:07.170
That will help you better understand

00:11:07.170 --> 00:11:09.180
what I will be covering.

00:11:09.180 --> 00:11:12.180
Let's start by recapping some key concepts

00:11:12.180 --> 00:11:14.463
that are most relevant for this section.

00:11:16.890 --> 00:11:20.883
Apple family 9 GPUs include the M3 and A17 Pro.

00:11:21.987 --> 00:11:25.110
The GPU in both chips have various components.

00:11:25.110 --> 00:11:28.260
Each shader core has multiple execution pipelines

00:11:28.260 --> 00:11:30.810
for executing different types of instructions,

00:11:30.810 --> 00:11:35.810
such as FP32, FP16, and also reads and writes

00:11:35.970 --> 00:11:38.043
to texture and buffer resources.

00:11:39.570 --> 00:11:42.360
It also has on-chip memory for storing

00:11:42.360 --> 00:11:45.450
different types of data a shader program may use,

00:11:45.450 --> 00:11:49.170
such as registers for storing the values of variables,

00:11:49.170 --> 00:11:51.390
thread group and tile memory for storing data

00:11:51.390 --> 00:11:54.030
shared across a compute thread group,

00:11:54.030 --> 00:11:57.243
or color attachment data shared across a tile.

00:11:59.040 --> 00:12:02.040
These on-chip memories share L1 cache,

00:12:02.040 --> 00:12:05.703
and are backed by GPU last level cache and device memory.

00:12:07.110 --> 00:12:10.320
Now, let's talk about how GP performance and occupancy

00:12:10.320 --> 00:12:11.733
are related to each other.

00:12:13.200 --> 00:12:15.900
Suppose your Metal shader, after executing

00:12:15.900 --> 00:12:20.340
some math operations using the ALU execution pipelines,

00:12:20.340 --> 00:12:24.633
reads a buffer, whose result will be used immediately after.

00:12:26.280 --> 00:12:29.670
Accessing the buffer may require going all the way out

00:12:29.670 --> 00:12:33.540
to device memory, which is a long latency operation.

00:12:33.540 --> 00:12:36.210
During this time, the SIMD group

00:12:36.210 --> 00:12:38.280
can't execute other operations,

00:12:38.280 --> 00:12:41.493
which causes the ALU pipelines to go unused.

00:12:42.900 --> 00:12:46.530
To mitigate this, the shader core can execute instructions

00:12:46.530 --> 00:12:48.510
from a different SIMD group,

00:12:48.510 --> 00:12:51.513
which may have some ALU instructions of its own.

00:12:53.580 --> 00:12:57.600
This reduces the amount of time the ALUs go unused,

00:12:57.600 --> 00:12:59.370
and allows the SIMD groups

00:12:59.370 --> 00:13:02.523
to run in parallel, thereby improving performance.

00:13:04.080 --> 00:13:06.150
If there are additional SIMD groups running

00:13:06.150 --> 00:13:10.230
on the shader core, this can be done many times over,

00:13:10.230 --> 00:13:13.500
until the ALUs and other execution pipelines

00:13:13.500 --> 00:13:16.413
are never starved of instructions to execute.

00:13:17.790 --> 00:13:20.280
The number of SIMD groups that are concurrently running

00:13:20.280 --> 00:13:23.403
on a shader core is called its occupancy.

00:13:24.360 --> 00:13:27.630
To achieve optimal performance, you should increase

00:13:27.630 --> 00:13:30.960
the occupancy until the ALUs on the shader core

00:13:30.960 --> 00:13:33.543
are kept busy as much as possible.

00:13:35.130 --> 00:13:38.580
Next, let me quickly provide the motivation

00:13:38.580 --> 00:13:42.003
for occupancy management on Apple family 9 GPU.

00:13:43.650 --> 00:13:45.990
Registers, threadgroup, tile stack,

00:13:45.990 --> 00:13:49.560
and other shader core memory types are assigned dynamically

00:13:49.560 --> 00:13:52.470
from L1 cache, which is then backed

00:13:52.470 --> 00:13:55.173
by GPU last level cache and device memory.

00:13:57.000 --> 00:13:59.550
Each SIMD group may use a large amount of various

00:13:59.550 --> 00:14:02.160
on-chip shader program memories.

00:14:02.160 --> 00:14:04.650
With the increase in number of SIMD groups,

00:14:04.650 --> 00:14:08.790
they may come a point when your workload uses more memory

00:14:08.790 --> 00:14:11.820
than what is available in on-chip storage,

00:14:11.820 --> 00:14:14.373
which causes spills to next cache levels.

00:14:15.450 --> 00:14:17.640
The shader cores balance thread occupancy

00:14:17.640 --> 00:14:21.453
and cache utilization to prevent memory cache thrashing.

00:14:22.590 --> 00:14:26.010
This results in the shader data staying on-chip

00:14:26.010 --> 00:14:29.010
and this will keep the execution pipelines busy,

00:14:29.010 --> 00:14:31.143
resulting in better shader performance.

00:14:34.680 --> 00:14:38.070
Xcode 15 has new set of performance counters

00:14:38.070 --> 00:14:41.520
that can help you to easily identify and address

00:14:41.520 --> 00:14:44.250
the cause of low occupancy in your workload,

00:14:44.250 --> 00:14:45.903
and achieve great performance.

00:14:47.610 --> 00:14:51.180
Next, I will show you a workflow that can help you meet

00:14:51.180 --> 00:14:54.693
the workload performance target by increasing occupancy.

00:14:55.950 --> 00:14:59.070
The first thing you need to see is how your Metal workload

00:14:59.070 --> 00:15:01.320
is running on the GPU,

00:15:01.320 --> 00:15:04.683
and what is the occupancy throughout its execution.

00:15:05.670 --> 00:15:09.840
Let me show you how you can do that, using Metal Debugger.

00:15:09.840 --> 00:15:14.070
Here you will see the workload execution on the GPU

00:15:14.070 --> 00:15:15.903
when the Timeline tab is selected.

00:15:17.610 --> 00:15:20.610
It will show the duration of all the workload encoders

00:15:20.610 --> 00:15:22.620
for each shader stage.

00:15:22.620 --> 00:15:25.650
You can also view execution of shader pipelines

00:15:25.650 --> 00:15:28.353
in the section for each shader stage.

00:15:29.910 --> 00:15:33.390
Below the encoder section is a counter section,

00:15:33.390 --> 00:15:36.180
where you can view top level performance limiters

00:15:36.180 --> 00:15:38.370
and utilizations and other helpful

00:15:38.370 --> 00:15:40.443
performance counters, like occupancy.

00:15:41.700 --> 00:15:44.430
These counters are collected periodically,

00:15:44.430 --> 00:15:47.283
while their workload executed on the GPU.

00:15:48.660 --> 00:15:51.750
I will be frequently mentioning performance utilization

00:15:51.750 --> 00:15:53.613
and limiter during this section,

00:15:54.660 --> 00:15:57.063
so let me briefly review what it means.

00:15:58.350 --> 00:16:02.400
Work is the number of items processed in the hardware block,

00:16:02.400 --> 00:16:04.983
like arithmetic instructions in the ALU,

00:16:05.910 --> 00:16:09.213
address translation requests in the MMU, et cetera.

00:16:10.200 --> 00:16:14.400
Stall is the number of times an available item is held off

00:16:14.400 --> 00:16:15.783
by a downstream block.

00:16:16.770 --> 00:16:20.790
For example, memory instruction request stalled by cache,

00:16:20.790 --> 00:16:22.740
waiting for the request to come back

00:16:22.740 --> 00:16:25.890
from next level cache or device memory.

00:16:25.890 --> 00:16:29.040
Here are the math equations to calculate utilization

00:16:29.040 --> 00:16:31.113
and limiter for a hardware block.

00:16:32.070 --> 00:16:35.310
Utilization is the work done in the sample period

00:16:35.310 --> 00:16:38.040
by the hardware block, as the percentage

00:16:38.040 --> 00:16:40.920
of the hardware block's peak processing rate

00:16:40.920 --> 00:16:42.333
times the sample period.

00:16:43.170 --> 00:16:45.303
Limiter is calculated similarly,

00:16:46.200 --> 00:16:50.223
it includes both work and stalls in the sample period.

00:16:52.080 --> 00:16:56.193
Next, I will show you how you can triage low occupancy.

00:16:58.290 --> 00:16:59.913
Let me check the counter track,

00:17:01.740 --> 00:17:04.800
where total occupancy looks to be low.

00:17:04.800 --> 00:17:08.370
Let me also take a look at other performance limiters

00:17:08.370 --> 00:17:09.753
when the occupancy is low.

00:17:11.220 --> 00:17:13.620
While the total occupancy is low,

00:17:13.620 --> 00:17:16.353
you can see performance limiter for FP16,

00:17:17.910 --> 00:17:22.050
which is an ALU subunit, is around 100%.

00:17:22.050 --> 00:17:26.640
And that means FP16 was busy throughout the interval.

00:17:26.640 --> 00:17:30.390
In this scenario, if you try to increase the occupancy,

00:17:30.390 --> 00:17:33.210
you may not see any performance improvement

00:17:33.210 --> 00:17:35.220
if the newly added SIMD groups

00:17:35.220 --> 00:17:38.013
primarily wanted to do FP16 work.

00:17:39.330 --> 00:17:42.750
Reducing the FP16 instructions in your shader

00:17:42.750 --> 00:17:45.603
will most likely improve overall shader performance.

00:17:47.370 --> 00:17:50.250
Here is a different workload, where you can see

00:17:50.250 --> 00:17:54.150
both occupancy and all ALU limiters are low.

00:17:54.150 --> 00:17:57.210
And that means the occupancy is not high enough

00:17:57.210 --> 00:17:59.073
to avoid ALU starvation.

00:17:59.940 --> 00:18:03.510
Once I establish that occupancy is making my ALU units

00:18:03.510 --> 00:18:08.190
to starve, which is in fact contrary to my optimization goal

00:18:08.190 --> 00:18:09.753
of keeping the ALUs busy,

00:18:11.280 --> 00:18:15.960
I will show you how to triage the reasons of low occupancy

00:18:15.960 --> 00:18:19.470
and increase it enough to make the workload either

00:18:19.470 --> 00:18:24.423
limited by ALU or memory bandwidth, rather than occupancy.

00:18:25.800 --> 00:18:29.130
Shader Launch Limited Counter includes both the work done

00:18:29.130 --> 00:18:32.100
for launching the threads in the shader core,

00:18:32.100 --> 00:18:35.010
and the stalls when threads cannot be launched

00:18:35.010 --> 00:18:36.153
due to back pressure.

00:18:37.110 --> 00:18:39.750
A low value for this counter indicates

00:18:39.750 --> 00:18:42.180
that not enough threads are getting launched

00:18:42.180 --> 00:18:44.160
due to small workload size.

00:18:44.160 --> 00:18:47.433
Conversely, a high value indicates otherwise.

00:18:48.600 --> 00:18:51.390
First, I will start by checking

00:18:51.390 --> 00:18:53.790
if enough shader threads are getting launched

00:18:53.790 --> 00:18:57.600
into the shader cores, by inspecting this counter value

00:18:57.600 --> 00:18:59.400
in the counters track.

00:18:59.400 --> 00:19:00.840
Here, you can see

00:19:00.840 --> 00:19:05.460
the Compute Shader Launch limiter is just 0.07%.

00:19:05.460 --> 00:19:10.020
As I mentioned earlier, a small counter value indicates

00:19:10.020 --> 00:19:12.600
that the shader cores are getting starved

00:19:12.600 --> 00:19:15.633
because this workload is not large enough to fill the GPU.

00:19:17.280 --> 00:19:20.073
Now, let's look at a different workload I profiled.

00:19:21.780 --> 00:19:25.620
And here you can see shader launch limiters are high.

00:19:25.620 --> 00:19:29.580
This means that either enough threads are getting launched,

00:19:29.580 --> 00:19:32.880
or thread launch is getting stalled due to back pressure,

00:19:32.880 --> 00:19:36.030
possibly due to running out of memory resources

00:19:36.030 --> 00:19:37.503
needed by those threads.

00:19:38.820 --> 00:19:41.310
Let's understand what to do next

00:19:41.310 --> 00:19:43.683
in order to continue the investigation.

00:19:45.240 --> 00:19:48.720
There can be a couple of reasons of low occupancy

00:19:48.720 --> 00:19:51.990
when Shader Launch Limiter counter is high.

00:19:51.990 --> 00:19:55.140
First, I will check if any compute dispatch

00:19:55.140 --> 00:19:57.480
that is executing during this time

00:19:57.480 --> 00:20:00.120
is using a large amount of thread group memory.

00:20:00.120 --> 00:20:03.240
If that is the case, then shader cores will stall

00:20:03.240 --> 00:20:06.270
launching new threads due to unavailability

00:20:06.270 --> 00:20:09.933
of thread group memory, resulting in low occupancy.

00:20:11.490 --> 00:20:14.850
Here I profiled a different and simpler workload,

00:20:14.850 --> 00:20:18.060
consisting of just one compute pass.

00:20:18.060 --> 00:20:20.670
On GPU timeline, you can see the dispatches

00:20:20.670 --> 00:20:23.193
that were executing at any given time.

00:20:24.150 --> 00:20:27.480
With a compute encoder selected in the GPU timeline,

00:20:27.480 --> 00:20:30.810
you can see how much thread group memory is being set

00:20:30.810 --> 00:20:33.870
for each dispatch in the encoder.

00:20:33.870 --> 00:20:36.120
As thread group memory usage for the dispatch

00:20:36.120 --> 00:20:40.230
is just two kilobyte, which is low, I can rule out

00:20:40.230 --> 00:20:42.993
thread group memory causing shader launch stalls.

00:20:43.950 --> 00:20:47.940
Shader cores can set a target on the maximum occupancy

00:20:47.940 --> 00:20:51.150
to balance thread utilization and cache thrashing,

00:20:51.150 --> 00:20:52.863
using the occupancy manager.

00:20:53.910 --> 00:20:55.380
For the current workload,

00:20:55.380 --> 00:20:58.320
that leaves me with Occupancy Manager Target Counter

00:20:58.320 --> 00:21:01.743
to check if the GPU is restricting occupancy.

00:21:02.940 --> 00:21:04.800
This is done to keep the registers,

00:21:04.800 --> 00:21:08.253
threadgroup, tile, and stack memory on-chip.

00:21:09.150 --> 00:21:11.700
I can check Occupancy Manager Target Counter

00:21:11.700 --> 00:21:13.980
in the timeline counter track.

00:21:13.980 --> 00:21:17.370
As you can see, Occupancy Manager Target Counter

00:21:17.370 --> 00:21:19.620
is lower than 100%.

00:21:19.620 --> 00:21:24.360
This indicates that occupancy manager is engaged by the GPU

00:21:24.360 --> 00:21:27.960
to keep various shader data memory types to stay on-chip,

00:21:27.960 --> 00:21:31.230
which would otherwise spill to the GPU, loss level cache,

00:21:31.230 --> 00:21:33.213
or, worse, to device memory.

00:21:34.860 --> 00:21:37.920
Here is a flow chart that you can use to triage

00:21:37.920 --> 00:21:41.970
low occupancy when Occupancy Manager Target Counter is low.

00:21:41.970 --> 00:21:45.333
I will start by inspecting L1 eviction rate counter.

00:21:46.200 --> 00:21:49.080
This will provide a measure of how much registers,

00:21:49.080 --> 00:21:51.930
threadgroup, tile, and stack memories

00:21:51.930 --> 00:21:53.370
are able to stay on-chip,

00:21:53.370 --> 00:21:56.073
instead of being spilled to next level cache.

00:21:57.000 --> 00:21:59.910
Here in the L1 eviction rate counter track,

00:21:59.910 --> 00:22:03.180
I can see the counter show high spikes,

00:22:03.180 --> 00:22:06.960
which indicates L1 cash is being thrashed due to heavy

00:22:06.960 --> 00:22:09.993
shader core memory accesses, and getting evicted.

00:22:12.030 --> 00:22:14.730
Now, let me show you how you can figure out

00:22:14.730 --> 00:22:16.530
which of these shader core memories

00:22:16.530 --> 00:22:18.483
are the cause of these evictions.

00:22:19.590 --> 00:22:23.220
To figure out which L1 backed on-chip shader core memory

00:22:23.220 --> 00:22:27.150
is causing the evictions, we need to see which memory type

00:22:27.150 --> 00:22:29.700
is accessing L1 most frequently.

00:22:29.700 --> 00:22:32.280
And which memory has allocated the largest

00:22:32.280 --> 00:22:34.113
percentage of cache lines.

00:22:35.340 --> 00:22:38.910
If you inspect L1 load and store bandwidth counter track

00:22:38.910 --> 00:22:42.210
in GPU timeline, you can see L1 bandwidths

00:22:42.210 --> 00:22:46.080
of various on-chip L1 backed memories.

00:22:46.080 --> 00:22:48.960
As can be seen here, image block L1

00:22:48.960 --> 00:22:51.543
has the highest L1 memory store bandwidth.

00:22:52.860 --> 00:22:56.250
Similarly, image block L1 has the highest

00:22:56.250 --> 00:22:59.010
L1 load bandwidth, and it is causing

00:22:59.010 --> 00:23:00.783
most of the L1 evictions.

00:23:02.580 --> 00:23:06.120
L1 residency counter track will show you the breakdown

00:23:06.120 --> 00:23:10.020
of L1 cache allocations among various on-chip memories,

00:23:10.020 --> 00:23:12.750
and you can find out which shader core memory

00:23:12.750 --> 00:23:15.033
has the largest allocation in L1.

00:23:16.410 --> 00:23:19.770
Here you can see, again, image block L1 memory

00:23:19.770 --> 00:23:22.470
has the largest working set size,

00:23:22.470 --> 00:23:26.073
and is most likely the cause of high L1 eviction rate.

00:23:27.090 --> 00:23:29.610
In this case, you can reduce the L1 eviction rate

00:23:29.610 --> 00:23:32.010
by using smallest pixel formats.

00:23:32.010 --> 00:23:34.080
If you're using MSAA,

00:23:34.080 --> 00:23:37.260
and your workload has highly complex geometry,

00:23:37.260 --> 00:23:39.870
then reducing sample count will help reduce

00:23:39.870 --> 00:23:41.313
the L1 eviction rate.

00:23:42.840 --> 00:23:45.450
After reducing the frequency of accesses

00:23:45.450 --> 00:23:48.150
and the allocation size of on-chip memory

00:23:48.150 --> 00:23:51.150
causing L1 evictions, I needed to make sure

00:23:51.150 --> 00:23:53.463
that my changes have the desired effect.

00:23:54.630 --> 00:23:58.560
After optimizing memory and reprofiling, I will make sure,

00:23:58.560 --> 00:24:02.580
if my workload is not ALU or memory bandwidth limited,

00:24:02.580 --> 00:24:04.830
I will check other limiters first.

00:24:04.830 --> 00:24:07.350
If that is the case, then the workload

00:24:07.350 --> 00:24:09.420
is not occupancy limited,

00:24:09.420 --> 00:24:12.150
and I don't need to triage low occupancy.

00:24:12.150 --> 00:24:15.810
If workload is not limited by ALU or memory bandwidth,

00:24:15.810 --> 00:24:17.820
I will check values of occupancy

00:24:17.820 --> 00:24:20.520
and Occupancy Manager Target Counter again,

00:24:20.520 --> 00:24:22.200
and keep repeating this process

00:24:22.200 --> 00:24:24.093
till the L1 eviction rate is low.

00:24:26.370 --> 00:24:28.860
Here, L1 eviction rate is low.

00:24:28.860 --> 00:24:30.510
So occupancy manager target,

00:24:30.510 --> 00:24:33.360
in this case, seems to be engaged

00:24:33.360 --> 00:24:37.140
due to GPU last level cache or MMU stalls.

00:24:37.140 --> 00:24:40.230
This can happen when device perform memory accesses,

00:24:40.230 --> 00:24:44.400
thrash loss level cache, or generate DLB misses.

00:24:44.400 --> 00:24:46.710
Let me show you how to see these stalls

00:24:46.710 --> 00:24:48.003
for a different workload.

00:24:48.840 --> 00:24:52.380
GPU last level cache utilization measures the time

00:24:52.380 --> 00:24:55.710
during which it serviced, read, and write requests

00:24:55.710 --> 00:24:59.430
as a percentage of peak loss level cache bandwidth.

00:24:59.430 --> 00:25:03.000
Loss level cache limiter includes its utilization time,

00:25:03.000 --> 00:25:05.460
and the time during which it is stalled

00:25:05.460 --> 00:25:09.030
due to cache thrashing or back pressure from main memory.

00:25:09.030 --> 00:25:12.300
If you see GPU last level cache limiter is much higher

00:25:12.300 --> 00:25:15.180
than its utilization, then that indicates

00:25:15.180 --> 00:25:18.510
it is getting stalled a lot due to cache thrashing.

00:25:18.510 --> 00:25:21.720
You can reduce these stalls by reducing your buffer size,

00:25:21.720 --> 00:25:24.603
improving spatial and temporal locality.

00:25:25.710 --> 00:25:28.230
Similarly, in the MMU counter track,

00:25:28.230 --> 00:25:32.850
if you see MMU limiter is much higher than MMU utilization,

00:25:32.850 --> 00:25:35.820
then device buffer accesses are causing TLB misses

00:25:35.820 --> 00:25:37.950
and MMU is thrashing.

00:25:37.950 --> 00:25:40.890
Reducing incoherent memory accesses to buffers

00:25:40.890 --> 00:25:42.903
can help reduce these stalls.

00:25:44.310 --> 00:25:47.100
Once I have optimized device memory accesses

00:25:47.100 --> 00:25:49.863
and updated the workload, I will profile again.

00:25:50.910 --> 00:25:52.830
If other limiters are high,

00:25:52.830 --> 00:25:55.650
then I will focus on reducing those limiters,

00:25:55.650 --> 00:25:59.220
as workload is not occupancy limited anymore.

00:25:59.220 --> 00:26:02.430
If other limiters are low, I will keep repeating

00:26:02.430 --> 00:26:04.950
the triaging process, as I showed earlier,

00:26:04.950 --> 00:26:07.983
till the workload is not limited due to low occupancy.

00:26:09.090 --> 00:26:11.580
Using these newly added performance counters,

00:26:11.580 --> 00:26:12.780
you will be able to keep

00:26:12.780 --> 00:26:15.390
the instruction execution pipelines busy,

00:26:15.390 --> 00:26:18.540
resulting in great shader performance.

00:26:18.540 --> 00:26:22.470
Now, let's move over to ray tracing profiling.

00:26:22.470 --> 00:26:24.780
With the new ray tracing hardware accelerator

00:26:24.780 --> 00:26:27.540
built into Apple family 9 GPUs,

00:26:27.540 --> 00:26:29.790
you can render incredibly realistic

00:26:29.790 --> 00:26:32.250
life-like scenes in real time.

00:26:32.250 --> 00:26:35.070
Let me show you how Metal Debugger in Xcode

00:26:35.070 --> 00:26:37.380
can help you optimize the performance

00:26:37.380 --> 00:26:39.033
for your ray tracing workloads.

00:26:40.470 --> 00:26:43.200
I've been working on this app that renders a truck,

00:26:43.200 --> 00:26:44.310
and I've used ray racing

00:26:44.310 --> 00:26:46.830
to render some pretty nice reflections.

00:26:46.830 --> 00:26:49.620
It is already rendering incredibly fast

00:26:49.620 --> 00:26:52.620
with the new hardware, but I'm curious to see

00:26:52.620 --> 00:26:54.513
if I can make it even faster.

00:26:55.560 --> 00:26:58.620
So to help you achieve the best ray tracing performance,

00:26:58.620 --> 00:27:01.530
Xcode includes a new set of ray tracing counters,

00:27:01.530 --> 00:27:04.380
in addition to the Acceleration Structure Viewer

00:27:04.380 --> 00:27:05.763
you know and love.

00:27:06.600 --> 00:27:08.670
You can also use the Shader Cost Graph

00:27:08.670 --> 00:27:11.820
that Ruiwei showed earlier for analyzing your shaders

00:27:11.820 --> 00:27:14.190
and custom intersection functions.

00:27:14.190 --> 00:27:15.933
Let's start with the counters.

00:27:17.430 --> 00:27:19.590
I have captured a frame of my renderer

00:27:19.590 --> 00:27:22.440
and opened up the performance timeline.

00:27:22.440 --> 00:27:25.320
Xcode now includes a new ray tracing group,

00:27:25.320 --> 00:27:27.900
which contains a comprehensive set of tracks

00:27:27.900 --> 00:27:30.780
to help you understand how your workload is running

00:27:30.780 --> 00:27:32.463
on the new ray tracing hardware.

00:27:34.440 --> 00:27:36.003
Let's inspect each one.

00:27:38.250 --> 00:27:40.440
The first track shows ray occupancy.

00:27:40.440 --> 00:27:43.650
The hardware is capable of executing a large number

00:27:43.650 --> 00:27:46.170
of rays concurrently, and ray occupancy shows

00:27:46.170 --> 00:27:48.510
the percentage that are active.

00:27:48.510 --> 00:27:50.610
And just like with thread occupancy,

00:27:50.610 --> 00:27:52.770
the Apple family 9 GPU shader core

00:27:52.770 --> 00:27:55.710
also automatically optimizes ray occupancy

00:27:55.710 --> 00:27:59.193
to ensure your app run with maximum performance.

00:28:01.170 --> 00:28:03.780
Assuming the workload I have is not starved

00:28:03.780 --> 00:28:05.790
by the number of rays,

00:28:05.790 --> 00:28:08.703
I will start by checking the occupancy manager target.

00:28:10.260 --> 00:28:12.600
Follow the same process as before,

00:28:12.600 --> 00:28:14.460
but pay particular attention

00:28:14.460 --> 00:28:17.070
to the ray tracing scratch category

00:28:17.070 --> 00:28:19.533
within L1 residency and bandwidth.

00:28:20.640 --> 00:28:23.790
The ray tracing unit uses a sizable portion of L1

00:28:23.790 --> 00:28:27.330
as a scratch buffer, which you can reduce

00:28:27.330 --> 00:28:29.343
by optimizing your payload size.

00:28:30.540 --> 00:28:33.210
Re-profile and repeat the triaging process

00:28:33.210 --> 00:28:35.043
I showed you in the last section.

00:28:37.230 --> 00:28:40.650
The next set of tracks provide a percentage breakdown

00:28:40.650 --> 00:28:43.650
of what the active rays are working on,

00:28:43.650 --> 00:28:46.140
which can help you gain a better understanding

00:28:46.140 --> 00:28:48.180
of areas to improve.

00:28:48.180 --> 00:28:51.300
For example, at this point in time,

00:28:51.300 --> 00:28:55.740
75% of my active rays were performing instance transform.

00:28:55.740 --> 00:28:58.200
That seems quite high, considering my scene

00:28:58.200 --> 00:29:00.500
should only have two instances of truck in it.

00:29:02.220 --> 00:29:05.550
Now, if you notice something like this in your own workload,

00:29:05.550 --> 00:29:08.100
it might be worth investigating your scene

00:29:08.100 --> 00:29:10.983
to make sure there's minimal instance overlap.

00:29:11.910 --> 00:29:13.740
I'll dig deeper into that later,

00:29:13.740 --> 00:29:16.470
using the acceleration structure viewer.

00:29:16.470 --> 00:29:18.033
So let's move on for now.

00:29:19.560 --> 00:29:22.260
Finally, the intersection test tracks

00:29:22.260 --> 00:29:24.570
show a percentage breakdown

00:29:24.570 --> 00:29:26.970
for the primitive intersections being performed.

00:29:28.020 --> 00:29:30.210
For my renderer, it shows that the hardware

00:29:30.210 --> 00:29:33.723
is only running opaque triangle tests without any motion.

00:29:34.590 --> 00:29:36.780
To achieve the best performance,

00:29:36.780 --> 00:29:39.510
try to maximize opaque triangle tests,

00:29:39.510 --> 00:29:43.440
and only use custom intersection functions on geometries

00:29:43.440 --> 00:29:47.583
that need them, like objects requiring alpha testing.

00:29:48.990 --> 00:29:51.990
And that concludes the new ray tracing counters.

00:29:51.990 --> 00:29:54.720
They are fantastic for gaining an understanding

00:29:54.720 --> 00:29:58.170
of how the hardware is executing your workload,

00:29:58.170 --> 00:30:01.173
and are a great place to start triaging performance.

00:30:02.188 --> 00:30:05.220
In this case, I found the instance transforms

00:30:05.220 --> 00:30:08.340
were suspiciously high, which indicates

00:30:08.340 --> 00:30:10.443
a potential issue with a scene.

00:30:11.610 --> 00:30:14.730
To triage scene issues like potential instance overlap,

00:30:14.730 --> 00:30:17.220
you can use Acceleration Structure Viewer,

00:30:17.220 --> 00:30:18.690
let's take a look.

00:30:18.690 --> 00:30:20.730
First, let's find a dispatch

00:30:20.730 --> 00:30:23.310
where I use the acceleration structure.

00:30:23.310 --> 00:30:25.983
I'll click on the encoder in the timeline,

00:30:27.450 --> 00:30:28.893
then select the dispatch.

00:30:32.130 --> 00:30:33.390
Finally, double click

00:30:33.390 --> 00:30:35.733
on the instance, acceleration structure.

00:30:37.830 --> 00:30:40.473
This opens the Acceleration Structure Viewer,

00:30:41.940 --> 00:30:44.490
which provides a breakdown of my acceleration structure

00:30:44.490 --> 00:30:47.553
on the left, and a preview on the right.

00:30:48.570 --> 00:30:50.760
I can also highlight different aspects

00:30:50.760 --> 00:30:52.443
of my acceleration structure.

00:30:54.360 --> 00:30:57.390
Now, I want to investigate transforms,

00:30:57.390 --> 00:31:00.633
so let me turn on instance traversals highlight mode.

00:31:01.590 --> 00:31:03.630
The hotspots are shown in blue.

00:31:03.630 --> 00:31:07.350
From the preview, it already seems like there are way more

00:31:07.350 --> 00:31:09.963
than the two instances I was expecting.

00:31:10.860 --> 00:31:13.680
I'll hover my mouse over this dark blue area

00:31:13.680 --> 00:31:17.673
to inspect exactly how many instances the ray is traversing.

00:31:18.540 --> 00:31:21.240
Eight, that means my ray

00:31:21.240 --> 00:31:23.250
needs to traverse eight instances

00:31:23.250 --> 00:31:26.550
before it finds the closest intersection.

00:31:26.550 --> 00:31:27.963
That's way more than two.

00:31:28.800 --> 00:31:32.370
This explains why the active rays were mostly working

00:31:32.370 --> 00:31:36.303
on instance transforms, but why are there so many?

00:31:37.560 --> 00:31:40.890
Let me switch over to the instance highlight mode,

00:31:40.890 --> 00:31:44.433
which gives each instance its own distinct color.

00:31:50.430 --> 00:31:52.800
Ah, it looks like different parts

00:31:52.800 --> 00:31:55.110
of the truck are different instances,

00:31:55.110 --> 00:31:58.050
and they're all on top of each other.

00:31:58.050 --> 00:32:01.290
To achieve the best performance in this case,

00:32:01.290 --> 00:32:04.140
I should concatenate these instances together

00:32:04.140 --> 00:32:06.720
into a single primitive acceleration structure.

00:32:06.720 --> 00:32:10.710
However, that might not be a code change.

00:32:10.710 --> 00:32:13.890
The issue with my acceleration structure could be a symptom

00:32:13.890 --> 00:32:16.263
of a problem in my asset pipeline.

00:32:17.370 --> 00:32:19.470
So I did some investigation,

00:32:19.470 --> 00:32:20.853
and got a new truck asset,

00:32:22.410 --> 00:32:23.553
which fixed the issue.

00:32:24.570 --> 00:32:27.873
Notice how the instance traversal are much better now.

00:32:29.730 --> 00:32:33.660
To recap, you can use both the new ray tracing counters

00:32:33.660 --> 00:32:36.300
and the Acceleration Structure Viewer to unlock

00:32:36.300 --> 00:32:38.610
the incredible ray tracing performance

00:32:38.610 --> 00:32:40.353
of Apple family 9 GPUs.

00:32:41.370 --> 00:32:43.560
You should also continue to follow

00:32:43.560 --> 00:32:45.513
the best practices for ray tracing,

00:32:47.100 --> 00:32:50.223
which you can learn more about in these other talks.

00:32:52.590 --> 00:32:55.533
Now, let's recap everything that was covered today.

00:32:56.790 --> 00:33:00.930
Xcode 15 added new state-of-the-art GPU profiling tools

00:33:00.930 --> 00:33:03.513
that are available for Apple family 9 GPU.

00:33:04.350 --> 00:33:07.680
You can use Shader Cost Graph to immediately find

00:33:07.680 --> 00:33:09.603
and triage expensive shaders.

00:33:11.430 --> 00:33:15.450
Using Performance Heat Maps, you can determine exactly

00:33:15.450 --> 00:33:18.180
which object or pixels cause the shader

00:33:18.180 --> 00:33:19.473
to be more expensive.

00:33:20.760 --> 00:33:23.100
With the Shader Execution History tool,

00:33:23.100 --> 00:33:26.250
identifying shader functions that take most

00:33:26.250 --> 00:33:28.353
of the execution time is a breeze.

00:33:29.850 --> 00:33:32.310
You can triage reasons of low occupancy,

00:33:32.310 --> 00:33:36.150
using the newly added performance counter in Xcode 15

00:33:36.150 --> 00:33:37.863
and get the best performance.

00:33:39.150 --> 00:33:42.360
Using the new performance counters for ray tracing,

00:33:42.360 --> 00:33:44.610
in addition to Acceleration Structure Viewer,

00:33:44.610 --> 00:33:46.860
you can get the best ray tracing performance.

00:33:48.390 --> 00:33:49.443
Thanks for watching.

00:33:51.151 --> 00:33:53.401
(no audio)