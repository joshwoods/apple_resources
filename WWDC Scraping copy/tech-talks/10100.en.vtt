WEBVTT

00:00:01.034 --> 00:00:02.402
Alejandro Isaza: Hi.
I'm Alejandro.

00:00:02.402 --> 00:00:04.738
Today David and I will introduce
TabularData,

00:00:04.738 --> 00:00:08.508
a new framework for data
manipulation and exploration.

00:00:08.508 --> 00:00:10.277
I'll start with a quick
introduction

00:00:10.277 --> 00:00:12.446
and then we'll talk
about data exploration,

00:00:12.446 --> 00:00:15.115
data transformations,
best practices,

00:00:15.115 --> 00:00:16.550
and end with a recap.

00:00:16.550 --> 00:00:17.751
Let's jump in.

00:00:17.751 --> 00:00:20.687
First let me talk about,
what is tabular data?

00:00:20.687 --> 00:00:23.023
In the simplest terms,
tabular data is data

00:00:23.023 --> 00:00:25.092
that is organized
in rows and columns,

00:00:25.092 --> 00:00:26.393
similar to a spreadsheet.

00:00:26.393 --> 00:00:28.095
But imagine you had
hundreds of columns

00:00:28.095 --> 00:00:29.563
and millions of rows.

00:00:29.563 --> 00:00:32.366
This is where the TabularData
framework comes in.

00:00:32.366 --> 00:00:33.734
So what is it?

00:00:33.734 --> 00:00:36.069
It is a brand-new framework
we've been working on.

00:00:36.069 --> 00:00:38.705
It's already available
in macOS, iOS,

00:00:38.705 --> 00:00:40.908
tvOS, and watchOS.

00:00:40.908 --> 00:00:44.177
It will help you explore and
manipulate unstructured data.

00:00:44.177 --> 00:00:46.113
When I say "unstructured data",
I mean data

00:00:46.113 --> 00:00:49.016
that has not been arranged
in a predefined manner.

00:00:49.016 --> 00:00:50.584
For example, when you
download a dataset

00:00:50.584 --> 00:00:52.920
that doesn't have
a technical specification;

00:00:52.920 --> 00:00:56.490
say, weather data
or population statistics.

00:00:56.490 --> 00:00:57.958
And exploration
is the first thing you do

00:00:57.958 --> 00:00:59.726
when you encounter
a new dataset.

00:00:59.726 --> 00:01:01.628
You want to know what sort
of information is there.

00:01:01.628 --> 00:01:03.030
Things like,
what are the values?

00:01:03.030 --> 00:01:05.532
What are the types?
How is the data represented?

00:01:05.532 --> 00:01:07.301
Are there missing values?
And so on.

00:01:07.301 --> 00:01:08.702
You need to be able
to answer these questions

00:01:08.702 --> 00:01:10.304
to properly understand
a dataset

00:01:10.304 --> 00:01:11.905
and to be able to move
to the next step,

00:01:11.905 --> 00:01:14.007
which is manipulation.

00:01:14.007 --> 00:01:16.143
Manipulation is where
you transform the dataset

00:01:16.143 --> 00:01:17.344
into a form
that is best suited

00:01:17.344 --> 00:01:19.279
for the problem
you are trying to solve.

00:01:19.279 --> 00:01:20.914
For example, you may
want to represent dates

00:01:20.914 --> 00:01:22.716
as a date type
instead of a string,

00:01:22.716 --> 00:01:24.618
or you may want to combine
x and y coordinates

00:01:24.618 --> 00:01:26.954
into a point type, and so on.

00:01:26.954 --> 00:01:28.689
The TabularData framework
is great

00:01:28.689 --> 00:01:30.324
for dealing with large datasets.

00:01:30.324 --> 00:01:32.059
Here are some common use cases.

00:01:32.059 --> 00:01:34.061
Grouping data according
to some criteria;

00:01:34.061 --> 00:01:36.563
for example,
grouping people by age.

00:01:36.563 --> 00:01:38.598
Joining datasets
on common values.

00:01:38.598 --> 00:01:41.168
For example,
joining a table of transactions

00:01:41.168 --> 00:01:43.637
with the purchaser's
information.

00:01:43.637 --> 00:01:46.473
Splitting or segmenting data
for processing incrementally

00:01:46.473 --> 00:01:48.842
or filtering to a subset
of the whole dataset.

00:01:48.842 --> 00:01:50.777
And building data pipelines;
for instance,

00:01:50.777 --> 00:01:53.814
when doing feature engineering
for machine learning.

00:01:53.814 --> 00:01:55.048
In the context of the framework,

00:01:55.048 --> 00:01:57.050
a table is known
as a DataFrame.

00:01:57.050 --> 00:01:58.752
A DataFrame contains
rows and columns,

00:01:58.752 --> 00:02:00.020
similar to a spreadsheet.

00:02:00.020 --> 00:02:01.288
But unlike a spreadsheet,

00:02:01.288 --> 00:02:04.024
every column can contain
only a specific type of value.

00:02:04.024 --> 00:02:06.693
But this also means that columns
can hold any type,

00:02:06.693 --> 00:02:08.328
even your own custom types.

00:02:08.328 --> 00:02:10.664
For example, dictionaries,
GPS coordinates,

00:02:10.664 --> 00:02:12.065
or raw audio samples.

00:02:12.065 --> 00:02:14.668
Know that whenever we show a
table representing a DataFrame,

00:02:14.668 --> 00:02:18.305
or a DataFrame slice, we include
an index column on the left.

00:02:18.305 --> 00:02:20.440
This is relevant for when
you need to access rows

00:02:20.440 --> 00:02:21.875
by data index.

00:02:21.875 --> 00:02:24.745
Some operations like filter,
the data is unchanged,

00:02:24.745 --> 00:02:26.279
while other operations like sort

00:02:26.279 --> 00:02:29.149
may change the indices
on the resulting data frame.

00:02:29.149 --> 00:02:31.451
Let's break it down
and zoom in on a column.

00:02:31.451 --> 00:02:33.987
As I mentioned, a column
has a specific element type;

00:02:33.987 --> 00:02:35.155
in this case, Int.

00:02:35.155 --> 00:02:37.924
It also has a name that must
be unique in the data frame.

00:02:37.924 --> 00:02:39.860
A column is represented
by the Column type,

00:02:39.860 --> 00:02:41.862
which is a collection,
just like array.

00:02:41.862 --> 00:02:43.430
You can refer to a column
by name

00:02:43.430 --> 00:02:45.966
but in most cases,
you need the type as well.

00:02:45.966 --> 00:02:48.101
There is a struct
called ColumnID

00:02:48.101 --> 00:02:49.870
that holds the name and type
of a column,

00:02:49.870 --> 00:02:51.972
and can be used to refer
to a specific column.

00:02:51.972 --> 00:02:54.408
Whenever possible, I recommend
using predefined ColumnIDs

00:02:54.408 --> 00:02:56.843
over string literals
to refer to columns.

00:02:56.843 --> 00:02:58.779
Also note that
all columns in a DataFrame

00:02:58.779 --> 00:03:00.981
must have the same
number of elements.

00:03:00.981 --> 00:03:02.616
But there can always
be missing elements,

00:03:02.616 --> 00:03:04.584
represented as nil values.

00:03:04.584 --> 00:03:06.787
Similar to Column,
there is a Row type.

00:03:06.787 --> 00:03:08.288
You can access
each element of a row

00:03:08.288 --> 00:03:10.190
by column name or by index.

00:03:10.190 --> 00:03:12.025
You can think of Row
as a proxy.

00:03:12.025 --> 00:03:14.127
It doesn't actually contain
the elements in the row;

00:03:14.127 --> 00:03:15.929
instead, it's a reference
pointing back

00:03:15.929 --> 00:03:17.898
to a row in the DataFrame.

00:03:17.898 --> 00:03:19.566
So how do you create
a DataFrame?

00:03:19.566 --> 00:03:20.600
Let me show you.

00:03:20.600 --> 00:03:22.903
You can create a DataFrame
from a dictionary literal

00:03:22.903 --> 00:03:25.005
or by building one column
at a time.

00:03:25.005 --> 00:03:28.041
This is an example of building
from a dictionary literal.

00:03:28.041 --> 00:03:29.910
Note that when using
a dictionary literal,

00:03:29.910 --> 00:03:31.878
you are constrained
to using basic Swift types

00:03:31.878 --> 00:03:35.715
such as strings, numbers,
boolean values, and dates.

00:03:35.715 --> 00:03:37.384
Also keep in mind
that every column

00:03:37.384 --> 00:03:39.419
must have the same
number of elements.

00:03:39.419 --> 00:03:41.555
A more general way
of building a DataFrame

00:03:41.555 --> 00:03:43.356
is to build one column at a time

00:03:43.356 --> 00:03:45.759
and then append the columns
to the DataFrame.

00:03:45.759 --> 00:03:46.827
Here's an example.

00:03:46.827 --> 00:03:48.929
First, I create
an empty DataFrame.

00:03:48.929 --> 00:03:50.297
Then I create a Column.

00:03:50.297 --> 00:03:52.232
And then I append the column
to the DataFrame.

00:03:52.232 --> 00:03:54.401
I can repeat the process
for all the columns,

00:03:54.401 --> 00:03:56.236
but again, make sure
that every column

00:03:56.236 --> 00:03:57.737
has the same number of elements

00:03:57.737 --> 00:04:00.006
and that column names
are unique.

00:04:00.006 --> 00:04:01.942
Now that you know
what a DataFrame is,

00:04:01.942 --> 00:04:03.577
let's do some data exploration.

00:04:03.577 --> 00:04:05.879
The first thing you want to do
is load a dataset.

00:04:05.879 --> 00:04:09.382
TabularData supports reading
comma-separated values, CSV,

00:04:09.382 --> 00:04:10.417
and JSON files.

00:04:10.417 --> 00:04:14.254
And it's as simple as calling
the initializer with a file URL,

00:04:14.254 --> 00:04:16.323
which will load
using all default options.

00:04:16.323 --> 00:04:18.225
Let's explore some
of the options available

00:04:18.225 --> 00:04:19.960
when loading a CSV.

00:04:19.960 --> 00:04:21.728
If you have used CSV before,
you may know

00:04:21.728 --> 00:04:23.930
that there are not always
commas.

00:04:23.930 --> 00:04:26.433
The separator may be
a tab or a semicolon.

00:04:26.433 --> 00:04:28.301
Or there may or may not
be a header row

00:04:28.301 --> 00:04:29.469
with the column names.

00:04:29.469 --> 00:04:31.605
There are also variations
of how strings are escaped

00:04:31.605 --> 00:04:32.672
for special characters

00:04:32.672 --> 00:04:34.941
and how missing values
are represented.

00:04:34.941 --> 00:04:37.477
TabularData can handle
all the variations.

00:04:37.477 --> 00:04:40.447
In this example, I am specifying
that there is no header row,

00:04:40.447 --> 00:04:43.183
using a custom nil encoding,
ignoring empty lines,

00:04:43.183 --> 00:04:45.352
and using a semicolon delimiter.

00:04:45.352 --> 00:04:46.486
Please refer
to the documentation

00:04:46.486 --> 00:04:49.990
to see the full set of options
as well as the defaults.

00:04:49.990 --> 00:04:51.825
If you have a larger file,
you may want to load

00:04:51.825 --> 00:04:54.027
only a subset of rows
at a time.

00:04:54.027 --> 00:04:55.962
You can do this
with the rows option.

00:04:55.962 --> 00:04:59.166
For example, this will only load
the first 100 rows.

00:04:59.166 --> 00:05:02.235
Similarly, you can select
a subset of the columns.

00:05:02.235 --> 00:05:04.437
To do this,
use the columns argument.

00:05:04.437 --> 00:05:07.641
Note that this will also let you
reorganize the columns.

00:05:09.643 --> 00:05:11.945
Let me briefly talk about
how type inference works

00:05:11.945 --> 00:05:13.713
when loading a CSV file.

00:05:13.713 --> 00:05:15.815
CSV files are all text based.

00:05:15.815 --> 00:05:17.584
But having every column
be of type string

00:05:17.584 --> 00:05:18.885
is not very convenient.

00:05:18.885 --> 00:05:21.188
So when loading a CSV file,
TabularData will attempt

00:05:21.188 --> 00:05:24.224
to convert values to numbers,
boolean values, and dates

00:05:24.224 --> 00:05:26.092
before defaulting to string.

00:05:26.092 --> 00:05:28.161
If you want to force values
to a specific type,

00:05:28.161 --> 00:05:29.729
or want to speed
loading up a bit,

00:05:29.729 --> 00:05:32.132
you can explicitly specify
the type of a column.

00:05:32.132 --> 00:05:34.134
You do this with
the types argument.

00:05:34.134 --> 00:05:36.336
In this example,
we're specifying integer type

00:05:36.336 --> 00:05:37.304
for the id column

00:05:37.304 --> 00:05:39.206
and string type
for the name column.

00:05:39.206 --> 00:05:41.208
This will not only speed up
the loading process,

00:05:41.208 --> 00:05:43.343
but it will also throw an error
if there is a value

00:05:43.343 --> 00:05:45.845
that cannot be converted
to the specified type.

00:05:45.845 --> 00:05:48.215
This is good because it lets you
catch problems early

00:05:48.215 --> 00:05:50.884
and handle them appropriately
instead of ending up with

00:05:50.884 --> 00:05:52.819
a column of a type
that's not what you'd expect,

00:05:52.819 --> 00:05:56.089
which can lead to crashes
later in your app.

00:05:56.089 --> 00:05:58.091
Lastly, let me mention
date parsing.

00:05:58.091 --> 00:06:00.594
TabularData, by default,
will detect and parse dates

00:06:00.594 --> 00:06:02.596
in ISO8601 format.

00:06:02.596 --> 00:06:05.332
If your CSV file contains dates
in a different format,

00:06:05.332 --> 00:06:08.101
you will need to specify
a custom date parsing strategy.

00:06:08.101 --> 00:06:10.403
I'll let David talk about this
and show you an example

00:06:10.403 --> 00:06:12.472
when we go into the demo.

00:06:12.472 --> 00:06:15.208
Now let's switch gears and
talk about writing data out.

00:06:15.208 --> 00:06:18.211
The first and simplest choice is
to use Swift's print function.

00:06:18.211 --> 00:06:21.181
This will generate a nicely
printed table in Terminal.

00:06:21.181 --> 00:06:23.750
The printed output
includes the row index,

00:06:23.750 --> 00:06:26.152
the column names,
the column types,

00:06:26.152 --> 00:06:27.988
the first few rows of data,

00:06:27.988 --> 00:06:29.990
and the number
of rows and columns.

00:06:29.990 --> 00:06:32.592
It also indicates that not all
rows fit on the screen

00:06:32.592 --> 00:06:34.828
and that not all columns
fit on the screen.

00:06:34.828 --> 00:06:38.465
In this case, there are 10 more
columns that aren't displayed.

00:06:38.465 --> 00:06:40.233
Print is great for exploring
and debugging,

00:06:40.233 --> 00:06:42.702
but obviously not great
for storing data.

00:06:42.702 --> 00:06:45.272
If you want to save
the DataFrame as a CSV file,

00:06:45.272 --> 00:06:47.007
use the writeCSV method.

00:06:47.007 --> 00:06:49.542
One important thing to note
is that writeCSV

00:06:49.542 --> 00:06:51.945
will use every value's
default string conversion.

00:06:51.945 --> 00:06:54.314
Be careful when using
custom types in your columns,

00:06:54.314 --> 00:06:55.782
because the generated CSV

00:06:55.782 --> 00:06:58.051
may not be something
that you can read back.

00:06:58.051 --> 00:07:00.453
As a general rule,
use only the basic Swift types

00:07:00.453 --> 00:07:02.589
in your columns
when writing to CSV,

00:07:02.589 --> 00:07:05.258
which may require transforming
some of your columns.

00:07:05.258 --> 00:07:08.495
writeCSV has some options
similar to the reading options.

00:07:08.495 --> 00:07:11.398
They let you customize
how the CSV data is written.

00:07:11.398 --> 00:07:13.633
Here is an example
where I'm disabling headers

00:07:13.633 --> 00:07:16.970
using a custom nil encoding
and using a custom delimiter.

00:07:16.970 --> 00:07:18.605
To access a specific row,

00:07:18.605 --> 00:07:20.507
you can just use
the row subscript.

00:07:20.507 --> 00:07:23.009
Then you can access
a specific column of that row.

00:07:23.009 --> 00:07:24.444
But whenever possible,
you should instead

00:07:24.444 --> 00:07:26.546
access the column first
and then the row.

00:07:26.546 --> 00:07:28.315
This is how you access a column.

00:07:28.315 --> 00:07:30.583
In the case of accessing
a column by name,

00:07:30.583 --> 00:07:33.453
you can omit the column: label
from the subscript.

00:07:33.453 --> 00:07:35.522
You can also access
a subset of rows.

00:07:35.522 --> 00:07:37.724
In this case,
you get a DataFrame slice.

00:07:37.724 --> 00:07:40.126
A DataFrame slice is
very similar to a DataFrame.

00:07:40.126 --> 00:07:42.529
It is basically a reference
to the original DataFrame

00:07:42.529 --> 00:07:44.531
In most situations,
you don't even need to know

00:07:44.531 --> 00:07:47.233
whether it's a full DataFrame
or a slice.

00:07:47.233 --> 00:07:50.203
And lastly, you can also
select a subset of the columns.

00:07:50.203 --> 00:07:51.371
This will return
a new DataFrame

00:07:51.371 --> 00:07:53.707
that includes
only those columns.

00:07:53.707 --> 00:07:55.208
You can also select
a subset of the rows

00:07:55.208 --> 00:07:56.576
with the filter method.

00:07:56.576 --> 00:07:59.179
The result of the filter
operation is a DataFrame slice,

00:07:59.179 --> 00:08:00.914
similar to selecting
a range of rows.

00:08:00.914 --> 00:08:02.882
But unlike a range of rows,

00:08:02.882 --> 00:08:05.485
filter can return
discontiguous rows.

00:08:05.485 --> 00:08:06.953
You need to be careful
when dealing with

00:08:06.953 --> 00:08:08.688
DataFrame slice indices.

00:08:08.688 --> 00:08:09.923
Similar to array slices,

00:08:09.923 --> 00:08:12.726
their indices reflect the
indices of the original rows.

00:08:12.726 --> 00:08:15.061
Specifically, the first index
may not be zero,

00:08:15.061 --> 00:08:18.498
and the next index may not be
the current index plus one.

00:08:18.498 --> 00:08:19.999
As with string indices,
you want to use

00:08:19.999 --> 00:08:23.636
startIndex instead of zero,
endIndex instead of count,

00:08:23.636 --> 00:08:27.907
and index(after:)
instead of adding one.

00:08:27.907 --> 00:08:29.175
Now that I've covered
the basics,

00:08:29.175 --> 00:08:31.978
let's put it into practice
by building an app.

00:08:31.978 --> 00:08:34.114
Finding parking
in San Francisco is hard.

00:08:34.114 --> 00:08:35.949
David and I want to build
an iPhone app

00:08:35.949 --> 00:08:38.651
that shows nearby parking spots
on the street.

00:08:38.651 --> 00:08:40.387
We want to use data
published by the city

00:08:40.387 --> 00:08:42.522
to identify parking meters
that are close by

00:08:42.522 --> 00:08:44.424
where parking
is currently allowed.

00:08:44.424 --> 00:08:45.492
We know there is a dataset

00:08:45.492 --> 00:08:47.660
but we don't know
exactly what it contains.

00:08:47.660 --> 00:08:50.163
So the first step is going
to be exploring the dataset

00:08:50.163 --> 00:08:51.598
to understand what we have.

00:08:51.598 --> 00:08:53.633
I'll hand it off to David.

00:08:53.633 --> 00:08:54.834
David Findlay: Thanks,
Alejandro.

00:08:54.834 --> 00:08:57.137
Hi. I'm David,
a framework engineer.

00:08:57.137 --> 00:08:58.738
In this demo,
I'll go through an example

00:08:58.738 --> 00:09:01.674
of how you can use TabularData
to explore a dataset.

00:09:01.674 --> 00:09:05.245
I'll start by exploring
a CSV file of parking policies.

00:09:05.245 --> 00:09:07.080
The first step
is to load the data,

00:09:07.080 --> 00:09:09.349
which I can do easily
by passing the file url

00:09:09.349 --> 00:09:11.251
into the DataFrame initializer.

00:09:11.251 --> 00:09:13.386
Note that the initializer
is throwable,

00:09:13.386 --> 00:09:14.554
which I find useful

00:09:14.554 --> 00:09:16.689
when handling
potential parsing errors.

00:09:16.689 --> 00:09:18.224
Next, with a simple print,

00:09:18.224 --> 00:09:22.796
I can explore the first few
rows and columns.

00:09:26.800 --> 00:09:27.901
Loading took a few seconds,

00:09:27.901 --> 00:09:30.770
and that's because the DataFrame
loaded over a million rows

00:09:30.770 --> 00:09:32.972
and 15 columns into memory.

00:09:32.972 --> 00:09:34.974
When I'm exploring a dataset
for the first time,

00:09:34.974 --> 00:09:37.010
I don't usually need
the whole dataset,

00:09:37.010 --> 00:09:39.412
so I'll specify a row range
when loading the data

00:09:39.412 --> 00:09:41.481
to speed up my exploration.

00:09:43.917 --> 00:09:46.052
Next, I'll take a look
at the columns.

00:09:46.052 --> 00:09:48.154
Notice that two of them
are hidden to the right,

00:09:48.154 --> 00:09:50.056
since they wouldn't
fit on the screen.

00:09:50.056 --> 00:09:54.594
Let me show you how to fix that
with formatting options.

00:09:56.062 --> 00:09:57.664
Formatting options
let me configure

00:09:57.664 --> 00:09:59.399
how the data is presented.

00:09:59.399 --> 00:10:03.236
In this case, I'll increase
my maximumLineWidth to 250,

00:10:03.236 --> 00:10:07.373
reduce my column width to 15,
and reduce the rows to five

00:10:07.373 --> 00:10:09.909
to prevent scrolling through
the printed result.

00:10:09.909 --> 00:10:12.045
Then I can simply
add the formattingOptions

00:10:12.045 --> 00:10:16.182
to my print statement
using the description method.

00:10:21.855 --> 00:10:24.524
Great! Now that I can explore
all of my columns,

00:10:24.524 --> 00:10:26.426
I'll pick a few interesting ones
to keep.

00:10:26.426 --> 00:10:28.995
It's also a good opportunity
to reorganize the columns

00:10:28.995 --> 00:10:31.331
by listing them in the order
that I want.

00:10:31.331 --> 00:10:34.100
I have HourlyRate,
DayOfWeek,

00:10:34.100 --> 00:10:37.704
start and end time,
StartDate, and PostID.

00:10:37.704 --> 00:10:40.640
All I need to do next is to add
these columns as a parameter

00:10:40.640 --> 00:10:43.776
when loading my DataFrame.

00:10:45.778 --> 00:10:48.781
All right, this is already
so much easier to explore.

00:10:48.781 --> 00:10:50.216
Take a look
at the StartDate column,

00:10:50.216 --> 00:10:51.684
which has a string type.

00:10:51.684 --> 00:10:54.320
That's because
only ISO8601 dates

00:10:54.320 --> 00:10:55.855
are detected automatically.

00:10:55.855 --> 00:10:59.192
I'll need to explicitly specify
any other date format.

00:10:59.192 --> 00:11:01.561
I can fix that by using
CSVReadingOptions

00:11:01.561 --> 00:11:03.363
that Alejandro
explained earlier.

00:11:03.363 --> 00:11:05.732
Using Foundation
Date Parsing API,

00:11:05.732 --> 00:11:07.700
I'll add a date parsing
strategy.

00:11:07.700 --> 00:11:10.937
I'll specify the format
as year, month, day;

00:11:10.937 --> 00:11:12.972
the locale as US English;

00:11:12.972 --> 00:11:15.775
and the time zone
as Pacific Standard Time.

00:11:15.775 --> 00:11:17.977
Then I'll pass
the CSVReadingOptions

00:11:17.977 --> 00:11:20.179
when loading my DataFrame.

00:11:23.516 --> 00:11:25.685
Now that the StartDate column
has the right type,

00:11:25.685 --> 00:11:27.320
I can easily filter
the DataFrame

00:11:27.320 --> 00:11:30.256
so that I have only active
parking policies.

00:11:30.256 --> 00:11:34.360
Starting with the variable
to represent the current date,

00:11:34.360 --> 00:11:37.730
then I'll filter the DataFrame
using the filter method.

00:11:37.730 --> 00:11:39.265
The filter method
takes a column name --

00:11:39.265 --> 00:11:43.770
in this case, StartDate --
and a type -- Date.

00:11:43.770 --> 00:11:46.272
In the closure,
I unwrap the optional date,

00:11:46.272 --> 00:11:48.508
returning false
when the date value is nil

00:11:48.508 --> 00:11:51.044
so that it doesn't appear
in my filter result.

00:11:51.044 --> 00:11:52.745
And lastly,
I keep the start dates

00:11:52.745 --> 00:11:55.982
that are less than or equal to
my current date.

00:11:55.982 --> 00:11:57.450
I'll go ahead
and change my print

00:11:57.450 --> 00:11:59.886
to show you the filtered result.

00:12:04.223 --> 00:12:06.225
From now on, I won't be
needing the StartDate column,

00:12:06.225 --> 00:12:07.760
so I'll go ahead and remove it.

00:12:07.760 --> 00:12:08.728
But I need to be careful,

00:12:08.728 --> 00:12:11.764
since I can't remove a column
from a DataFrame slice.

00:12:11.764 --> 00:12:13.733
I'll first have to convert
to DataFrame

00:12:13.733 --> 00:12:15.868
and make filteredPolicies
of var,

00:12:15.868 --> 00:12:18.705
since removing a column
is a mutating method.

00:12:18.705 --> 00:12:22.108
Now I can remove the column
using the removeColumn method

00:12:22.108 --> 00:12:27.480
and specify the StartDate column
as the column to remove.

00:12:27.480 --> 00:12:29.115
All right, that's all
I wanted to explore

00:12:29.115 --> 00:12:31.284
in the parking policies dataset.

00:12:31.284 --> 00:12:33.920
In the next section,
Alejandro will discuss ways

00:12:33.920 --> 00:12:36.255
that you can augment
your tabular data.

00:12:36.255 --> 00:12:38.424
Back to you, Alejandro!

00:12:38.424 --> 00:12:39.158
Alejandro: Thanks, David.

00:12:39.158 --> 00:12:41.694
Now I now have some great
insights into the dataset.

00:12:41.694 --> 00:12:44.864
The next step is going to be
transforming and augmenting it

00:12:44.864 --> 00:12:46.599
to match our needs.

00:12:46.599 --> 00:12:47.867
The simplest kind
of transformation

00:12:47.867 --> 00:12:50.003
is changing the values
within a column.

00:12:50.003 --> 00:12:51.738
This can take the form
of a map operation

00:12:51.738 --> 00:12:53.673
where each value
is mapped to a new value,

00:12:53.673 --> 00:12:55.375
possibly of a different type.

00:12:55.375 --> 00:12:57.877
TabularData provides
an in-place version of map

00:12:57.877 --> 00:13:00.246
as a convenience:
transformColumn.

00:13:00.246 --> 00:13:01.981
In this example,
I am transforming

00:13:01.981 --> 00:13:04.617
the DayOfWeek column
from a string to an integer

00:13:04.617 --> 00:13:06.953
representing the weekday.

00:13:06.953 --> 00:13:08.554
This what the code
would look like.

00:13:08.554 --> 00:13:12.659
For each element, we convert
the string to an Int.

00:13:14.661 --> 00:13:15.895
Similar to transformColumn,

00:13:15.895 --> 00:13:18.564
the decode method
handles decoding of data.

00:13:18.564 --> 00:13:19.899
When dealing with CSV files,

00:13:19.899 --> 00:13:21.434
you may encounter
arrays or dictionaries

00:13:21.434 --> 00:13:24.103
embedded within the CSV
as JSON values.

00:13:24.103 --> 00:13:26.773
TabularData provides
a decode method for this.

00:13:26.773 --> 00:13:28.841
Here is an example
where the DataFrame on the left

00:13:28.841 --> 00:13:30.777
has an embedded
JSON data blob.

00:13:30.777 --> 00:13:32.912
Decode lets you
use a JSONDecoder

00:13:32.912 --> 00:13:35.114
to transform the column
into your own type;

00:13:35.114 --> 00:13:37.216
in this example,
Preferences.

00:13:37.216 --> 00:13:39.152
And this what the code
would look like.

00:13:39.152 --> 00:13:40.853
Keep in mind that
the Preferences type

00:13:40.853 --> 00:13:43.122
needs to conform
to the Decodable protocol,

00:13:43.122 --> 00:13:45.658
and the column needs to
contain elements of type Data,

00:13:45.658 --> 00:13:48.861
which is what JSONDecoder
expects as input.

00:13:48.861 --> 00:13:51.297
Another useful operation
is the filled method.

00:13:51.297 --> 00:13:53.166
It lets your replace
all missing values in a column

00:13:53.166 --> 00:13:54.767
with a default value.

00:13:54.767 --> 00:13:56.602
And to finish the list
of column operations,

00:13:56.602 --> 00:13:58.104
I like to mention summary.

00:13:58.104 --> 00:13:59.672
Summary gives you
a quick overview

00:13:59.672 --> 00:14:01.641
of the contents of a column.

00:14:01.641 --> 00:14:03.910
The summary method returns
a categorical summary.

00:14:03.910 --> 00:14:05.511
It includes the number
of elements,

00:14:05.511 --> 00:14:07.346
shown in the description
as someCount;

00:14:07.346 --> 00:14:10.249
the number of missing elements,
shown as noneCount;

00:14:10.249 --> 00:14:11.617
the number of unique elements;

00:14:11.617 --> 00:14:14.954
and the most frequent values,
known as the mode.

00:14:14.954 --> 00:14:17.056
There is also numericSummary,
which is only available

00:14:17.056 --> 00:14:18.991
for columns containing
numeric values.

00:14:18.991 --> 00:14:22.261
It includes counts as well,
plus mean, standard deviation,

00:14:22.261 --> 00:14:23.830
and other statistics.

00:14:23.830 --> 00:14:25.998
Here, I'm showing
the print result of summary.

00:14:25.998 --> 00:14:27.767
But you can also
use the summary struct directly

00:14:27.767 --> 00:14:29.235
to access the statistics.

00:14:29.235 --> 00:14:31.204
For instance,
if you want to filter scores

00:14:31.204 --> 00:14:34.040
to the ones
in the 75th percentile.

00:14:34.040 --> 00:14:35.608
OK, that was a lot of column
transformations,

00:14:35.608 --> 00:14:38.578
but column transformations
are not the most interesting.

00:14:38.578 --> 00:14:39.645
DataFrame transformation

00:14:39.645 --> 00:14:41.414
is where it really gets
interesting.

00:14:41.414 --> 00:14:43.683
Unlike column transformations,
DataFrame transformations

00:14:43.683 --> 00:14:45.918
manipulate multiple
columns at once.

00:14:45.918 --> 00:14:47.386
A simple example is sorting.

00:14:47.386 --> 00:14:48.454
We all know how sorting works,

00:14:48.454 --> 00:14:50.590
but let me illustrate
for clarity.

00:14:50.590 --> 00:14:52.225
Let's sort this table by score.

00:14:52.225 --> 00:14:53.893
This affects all columns.

00:14:53.893 --> 00:14:59.565
Also note that row indices
change when sorting.

00:14:59.565 --> 00:15:01.400
Another interesting
DataFrame transformation

00:15:01.400 --> 00:15:02.802
is combineColumns.

00:15:02.802 --> 00:15:03.936
The combineColumns method

00:15:03.936 --> 00:15:06.072
lets you combine
multiple columns into one.

00:15:06.072 --> 00:15:08.107
For example, imagine that
you have separate columns

00:15:08.107 --> 00:15:09.542
for latitude and longitude,

00:15:09.542 --> 00:15:12.712
but you want to combine them
into a CLLocation type.

00:15:12.712 --> 00:15:14.447
Here is an example
of doing this.

00:15:14.447 --> 00:15:17.083
First, I specify the columns
I want to combine.

00:15:17.083 --> 00:15:18.951
Then I give a name
to the new column.

00:15:18.951 --> 00:15:21.120
Then I specify the types
of the input columns

00:15:21.120 --> 00:15:22.054
and the new column,

00:15:22.054 --> 00:15:23.756
and note that everything
has to be optional.

00:15:23.756 --> 00:15:28.394
I handle the missing value case,
and I build the new value.

00:15:28.394 --> 00:15:31.297
Similar to column, there is also
a summary method for DataFrame.

00:15:31.297 --> 00:15:33.666
It returns summary statistics
for every column.

00:15:33.666 --> 00:15:36.302
Note that this may be costly
if you have a large DataFrame;

00:15:36.302 --> 00:15:37.436
it may be better to summarize

00:15:37.436 --> 00:15:39.839
only the columns
you are interested in.

00:15:39.839 --> 00:15:42.008
Another interesting method
is explode.

00:15:42.008 --> 00:15:44.644
It takes a column that contains
an array of elements

00:15:44.644 --> 00:15:47.680
and creates a new row
for every element in the array.

00:15:47.680 --> 00:15:49.682
Let's look at this example.

00:15:49.682 --> 00:15:51.851
This time, the scores column
contains an embedded array

00:15:51.851 --> 00:15:53.786
of scores for each person.

00:15:53.786 --> 00:15:55.788
If I apply the explode operation
to the DataFrame,

00:15:55.788 --> 00:15:57.390
each of these
becomes a new row.

00:15:57.390 --> 00:16:00.159
We have repeated names for
people who had multiple scores.

00:16:00.159 --> 00:16:02.595
This is useful when filtering
or doing other operations

00:16:02.595 --> 00:16:05.898
that require looking
at each individual score.

00:16:05.898 --> 00:16:08.067
With these tools in our arsenal
I'll turn it back to David,

00:16:08.067 --> 00:16:09.502
who will help us
get the meter data

00:16:09.502 --> 00:16:11.470
into the form that we need it.

00:16:11.470 --> 00:16:13.005
David, show us some code.

00:16:13.005 --> 00:16:14.140
David: Thanks, Alejandro.

00:16:14.140 --> 00:16:15.608
I don't know about you,
but for me,

00:16:15.608 --> 00:16:18.711
the most important part
about parking is the location.

00:16:18.711 --> 00:16:21.881
Luckily, I have another CSV file
that has just what I need.

00:16:21.881 --> 00:16:23.716
Let me show you
what's in there.

00:16:23.716 --> 00:16:25.952
Similar to before,
I'll start by loading the data,

00:16:25.952 --> 00:16:28.955
but this time, I already know
the columns I'm interested in.

00:16:28.955 --> 00:16:31.057
I have POST_ID,
STREET_NAME,

00:16:31.057 --> 00:16:33.159
STREET_NUM, LATITUDE,
and LONGITUDE,

00:16:33.159 --> 00:16:35.895
and I'll print the result
using the same formattingOptions

00:16:35.895 --> 00:16:37.930
from the previous demo.

00:16:37.930 --> 00:16:39.632
The first augmentation I want

00:16:39.632 --> 00:16:42.134
is to combine the latitude
and longitude columns

00:16:42.134 --> 00:16:44.637
into a new column
with a core location type.

00:16:44.637 --> 00:16:47.106
The combineColumns method
is perfect for the job.

00:16:47.106 --> 00:16:49.742
Here, I'm combining the latitude
and longitude columns

00:16:49.742 --> 00:16:52.278
into a new column
named location.

00:16:52.278 --> 00:16:54.580
In the closure, I specify
the latitude and longitude

00:16:54.580 --> 00:16:58.017
parameter types and
the core location return type.

00:16:58.017 --> 00:17:01.020
Next, I unwrap the optional
latitude and longitude values,

00:17:01.020 --> 00:17:03.990
returning nil in the case
that either one is nil.

00:17:03.990 --> 00:17:06.559
And finally, passing the
latitude and longitude values

00:17:06.559 --> 00:17:08.995
to a core location initializer.

00:17:08.995 --> 00:17:10.296
With location in my DataFrame,

00:17:10.296 --> 00:17:12.698
I can start building
the first feature for my app:

00:17:12.698 --> 00:17:15.801
given a location, search
for the closest parking meters.

00:17:15.801 --> 00:17:17.770
I'll write a function
named closestParking

00:17:17.770 --> 00:17:19.405
that takes a location,
a DataFrame,

00:17:19.405 --> 00:17:22.174
and the amount of parking meters
to include in the search result.

00:17:22.174 --> 00:17:24.710
I'll start with a local copy.

00:17:24.710 --> 00:17:26.679
And then using
the transformColumn method

00:17:26.679 --> 00:17:28.514
that Alejandro introduced
earlier,

00:17:28.514 --> 00:17:30.850
I'll transform location
to distance.

00:17:30.850 --> 00:17:34.553
And then, of course, I'll rename
the location column to distance.

00:17:34.553 --> 00:17:36.122
Finally, I'll sort
the distance column

00:17:36.122 --> 00:17:40.192
in ascending order and limit
the number of spots to return.

00:17:40.192 --> 00:17:41.961
Just for fun,
let's test this out

00:17:41.961 --> 00:17:44.397
using the Apple Store
in San Francisco.

00:17:44.397 --> 00:17:46.799
I'll plug in the coordinates
that I found on Apple Maps,

00:17:46.799 --> 00:17:47.800
the meters DataFrame,

00:17:47.800 --> 00:17:51.437
and limit the search result
to five parking spots.

00:17:51.437 --> 00:17:53.706
Perfect! It looks like
there's lots of parking

00:17:53.706 --> 00:17:56.475
close to the Apple Store
on Post Street.

00:17:56.475 --> 00:17:58.644
The first feature of the app
is working great,

00:17:58.644 --> 00:18:01.847
but what if all of the closest
parking is already taken?

00:18:01.847 --> 00:18:03.015
The next feature of the app

00:18:03.015 --> 00:18:05.418
is to find streets
with the most parking.

00:18:05.418 --> 00:18:06.919
But before I implement
this feature,

00:18:06.919 --> 00:18:10.423
let me introduce a new concept
called grouping.

00:18:10.423 --> 00:18:12.291
Grouping splits your data
into groups,

00:18:12.291 --> 00:18:13.693
given a grouping column.

00:18:13.693 --> 00:18:15.594
For example,
the STREET_NAME column.

00:18:15.594 --> 00:18:17.296
The group method
first identifies

00:18:17.296 --> 00:18:18.998
unique street name values --

00:18:18.998 --> 00:18:22.601
Post Street, California Street,
and Mission Street --

00:18:22.601 --> 00:18:25.404
and then splits the rows
into corresponding groups.

00:18:25.404 --> 00:18:27.640
Each group is a DataFrame slice.

00:18:27.640 --> 00:18:30.776
Now, let's jump back
into the code.

00:18:30.776 --> 00:18:32.545
I'll group the meters
by street name

00:18:32.545 --> 00:18:34.280
using the grouped method.

00:18:34.280 --> 00:18:36.215
Then I can count
how many parking meters

00:18:36.215 --> 00:18:37.516
each street group has

00:18:37.516 --> 00:18:39.919
and serve the result
in descending order.

00:18:39.919 --> 00:18:41.020
The streets
with the most parking

00:18:41.020 --> 00:18:44.490
are at the top of the result,
which is what I need for my app.

00:18:44.490 --> 00:18:45.725
This is so awesome!

00:18:45.725 --> 00:18:47.760
Two great features for my app.

00:18:47.760 --> 00:18:48.594
But wait a minute,

00:18:48.594 --> 00:18:50.830
I just realized there's a bug
in the first feature.

00:18:50.830 --> 00:18:53.866
The closest parking meters only
consider the meters DataFrame.

00:18:53.866 --> 00:18:56.235
What I really need
is the closest parking meters

00:18:56.235 --> 00:18:58.337
with active parking policies.

00:18:58.337 --> 00:18:59.405
This is getting interesting,

00:18:59.405 --> 00:19:02.675
since that information
is in the data from demo one.

00:19:02.675 --> 00:19:05.378
Let me show you how to join data
from two different sources

00:19:05.378 --> 00:19:08.414
so that I can fix the bug.

00:19:08.414 --> 00:19:09.715
You may be familiar with joining

00:19:09.715 --> 00:19:12.084
if you've used
a relational database before.

00:19:12.084 --> 00:19:14.186
It allows you to combine
two DataFrames together

00:19:14.186 --> 00:19:15.221
using a key.

00:19:15.221 --> 00:19:18.124
The key is a value that appears
in both DataFrames.

00:19:18.124 --> 00:19:19.558
In the meters and policies
DataFrames,

00:19:19.558 --> 00:19:21.127
the key is the POST_ID,

00:19:21.127 --> 00:19:23.362
which uniquely identifies
a parking meter.

00:19:23.362 --> 00:19:25.998
The join operation results
in a DataFrame with rows

00:19:25.998 --> 00:19:27.700
where the POST_ID from meters

00:19:27.700 --> 00:19:30.002
matches the POST_ID
from policies.

00:19:30.002 --> 00:19:32.271
The rows are composed
of the matching data

00:19:32.271 --> 00:19:35.074
from the left DataFrame
and the right DataFrame.

00:19:35.074 --> 00:19:36.242
Notice that the column names

00:19:36.242 --> 00:19:38.778
have either a left
or right prefix.

00:19:38.778 --> 00:19:40.646
This indicates
which side of the join

00:19:40.646 --> 00:19:41.981
the column came from.

00:19:41.981 --> 00:19:45.317
The prefixes help avoid naming
collisions in the join result.

00:19:45.317 --> 00:19:46.752
This operation
was an inner join,

00:19:46.752 --> 00:19:48.020
which is the default.

00:19:48.020 --> 00:19:49.655
There are three other
join kinds:

00:19:49.655 --> 00:19:52.491
left outer, right outer,
and full outer.

00:19:52.491 --> 00:19:53.959
I won't go into
the details here,

00:19:53.959 --> 00:19:56.762
but please refer to the
documentation to learn more.

00:19:56.762 --> 00:19:58.264
All right, that's all
I wanted to cover

00:19:58.264 --> 00:20:00.132
in tabular data augmentations.

00:20:00.132 --> 00:20:01.033
In the next section,

00:20:01.033 --> 00:20:03.969
Alejandro will talk about
best practices.

00:20:03.969 --> 00:20:05.037
Alejandro: Thanks, David.

00:20:05.037 --> 00:20:07.139
Now that we have all the data
in the form we need it,

00:20:07.139 --> 00:20:08.374
it's time to build our app.

00:20:08.374 --> 00:20:10.976
Let me talk about how you can
reuse your exploration code

00:20:10.976 --> 00:20:13.446
while making it
production ready.

00:20:13.446 --> 00:20:15.014
Let me go back
to the code we started with

00:20:15.014 --> 00:20:17.149
for loading a CSV file.

00:20:17.149 --> 00:20:19.785
If you do this, the columns
are going to have unknown types.

00:20:19.785 --> 00:20:22.855
This is problematic for
operations like filter or join

00:20:22.855 --> 00:20:25.424
where you need to know the type
ahead of time.

00:20:25.424 --> 00:20:27.760
If you are loading the data
from a user-supplied source,

00:20:27.760 --> 00:20:29.862
making assumptions
about the type is risky.

00:20:29.862 --> 00:20:31.330
It may lead
to your app crashing.

00:20:31.330 --> 00:20:33.566
Instead, you should declare
the types that you expect

00:20:33.566 --> 00:20:36.735
when loading data,
like in this example.

00:20:36.735 --> 00:20:39.738
Here, I'm defining column IDs
for every column I care about.

00:20:39.738 --> 00:20:41.974
And then I'm providing
both the column names

00:20:41.974 --> 00:20:44.610
and the column types
to the CSV initializer.

00:20:44.610 --> 00:20:45.511
Remember that you can use

00:20:45.511 --> 00:20:47.246
the column ID
instead of a string

00:20:47.246 --> 00:20:49.448
in any method
that refers to a column.

00:20:49.448 --> 00:20:50.783
Now, if there are
invalid values,

00:20:50.783 --> 00:20:52.651
you will get an exception
that you can handle,

00:20:52.651 --> 00:20:55.254
for instance, by presenting
an error to the user.

00:20:55.254 --> 00:20:57.123
This way you can ensure
you have the columns

00:20:57.123 --> 00:20:58.691
and column types you expect.

00:20:58.691 --> 00:21:01.694
This is particularly important
when using custom date formats

00:21:01.694 --> 00:21:02.895
because if you don't specify

00:21:02.895 --> 00:21:04.697
that your column
is of type date,

00:21:04.697 --> 00:21:06.332
the date parsing
may fail silently

00:21:06.332 --> 00:21:08.534
and produce a string column
instead.

00:21:08.534 --> 00:21:10.603
Forcing it to be a date
will throw an exception

00:21:10.603 --> 00:21:13.239
that includes the contents
of the cell that failed,

00:21:13.239 --> 00:21:15.574
which will help you debug
the problem.

00:21:15.574 --> 00:21:17.309
Speaking of errors,
these are the sort of errors

00:21:17.309 --> 00:21:19.979
you should expect
when loading a CSV file.

00:21:19.979 --> 00:21:21.480
Failed to parse
will be generated

00:21:21.480 --> 00:21:24.416
when using a custom date parser
and a cell fails to parse.

00:21:24.416 --> 00:21:26.519
The other ones
are self-explanatory,

00:21:26.519 --> 00:21:28.854
but please refer
to the documentation.

00:21:28.854 --> 00:21:31.357
And to finish, let me
briefly mention performance.

00:21:31.357 --> 00:21:33.559
Most of the time, you should not
need to worry about performance,

00:21:33.559 --> 00:21:34.560
but there are a few cases

00:21:34.560 --> 00:21:35.794
where you will see
a large impact

00:21:35.794 --> 00:21:37.997
when working
with large datasets.

00:21:37.997 --> 00:21:40.999
The first one is date parsing
when loading a CSV.

00:21:40.999 --> 00:21:43.802
Date parsing has a lot of
special cases and considerations

00:21:43.802 --> 00:21:45.471
and as such,
it tends to be slow.

00:21:45.471 --> 00:21:48.040
If your CSV file takes more than
a couple of seconds to load,

00:21:48.040 --> 00:21:49.308
this is the first place
you should look

00:21:49.308 --> 00:21:50.509
to make improvements.

00:21:50.509 --> 00:21:52.244
One option is to delay parsing.

00:21:52.244 --> 00:21:54.346
This works particularly well
when you don't need

00:21:54.346 --> 00:21:56.115
the date information
right away.

00:21:56.115 --> 00:21:57.049
For instance,
you want to perform

00:21:57.049 --> 00:21:58.851
filtering or grouping first.

00:21:58.851 --> 00:21:59.952
If that's not an option,

00:21:59.952 --> 00:22:01.654
consider handcrafting
a date parser

00:22:01.654 --> 00:22:04.790
that optimizes performance
for your date strings.

00:22:04.790 --> 00:22:06.859
When grouping, always use
a column containing

00:22:06.859 --> 00:22:08.861
a basic Swift type
as the group column,

00:22:08.861 --> 00:22:10.095
such as string or Int.

00:22:10.095 --> 00:22:12.464
This will speed up
grouping performance.

00:22:12.464 --> 00:22:14.333
If you are grouping
by more than one column,

00:22:14.333 --> 00:22:17.002
consider first combining the
columns into a single column

00:22:17.002 --> 00:22:19.505
of a simple type first
and then grouping.

00:22:19.505 --> 00:22:20.739
For example,
if you want to group

00:22:20.739 --> 00:22:22.608
by day of week and meter type,

00:22:22.608 --> 00:22:25.077
consider combining those two
properties into a string.

00:22:25.077 --> 00:22:26.946
For instance, day-type.

00:22:26.946 --> 00:22:28.847
Similarly, when joining,

00:22:28.847 --> 00:22:32.785
consider joining on a column
containing a basic Swift type.

00:22:32.785 --> 00:22:34.386
With this, we are ready
to finish the app.

00:22:34.386 --> 00:22:36.989
David, let's wrap it up.

00:22:36.989 --> 00:22:38.624
David: Using TabularData
best practices,

00:22:38.624 --> 00:22:41.026
I'll write the app's
search functionality.

00:22:41.026 --> 00:22:43.229
The Parking struct
will store the joined meters

00:22:43.229 --> 00:22:44.630
and policies DataFrame.

00:22:44.630 --> 00:22:46.398
And I defined
a location ColumnID,

00:22:46.398 --> 00:22:48.400
since multiple methods need it.

00:22:48.400 --> 00:22:51.437
Let's dive into the details
in the loadMeters method.

00:22:51.437 --> 00:22:53.372
At the top,
I have the Column IDs

00:22:53.372 --> 00:22:57.509
that I need for loading
the meters.

00:22:57.509 --> 00:22:58.611
Then, I load the meters

00:22:58.611 --> 00:23:02.081
and specify the expected
types of each column.

00:23:02.081 --> 00:23:03.849
This will throw
if there's any mismatch

00:23:03.849 --> 00:23:06.151
in the provided CSV file.

00:23:06.151 --> 00:23:07.987
Next I verify
the resolved columns

00:23:07.987 --> 00:23:09.622
are exactly what I expected,

00:23:09.622 --> 00:23:12.858
and I'll throw a custom
ParkingError otherwise.

00:23:12.858 --> 00:23:16.195
Lastly, I refactored
the combineColumns operation

00:23:16.195 --> 00:23:20.032
to use the latitude, longitude,
and location column IDs.

00:23:20.032 --> 00:23:21.734
With that, the app's
search functionality

00:23:21.734 --> 00:23:22.935
is production-ready.

00:23:22.935 --> 00:23:24.737
I'll hand it back to Alejandro
for a recap

00:23:24.737 --> 00:23:27.106
of the TabularData framework.

00:23:27.106 --> 00:23:29.308
Alejandro: Thanks, David.
Let's recap.

00:23:29.308 --> 00:23:31.810
Today we showed you how
TabularData lets you explore

00:23:31.810 --> 00:23:33.846
an unknown dataset,
manipulate it,

00:23:33.846 --> 00:23:35.481
and bring it into your app.

00:23:35.481 --> 00:23:36.915
We explored a dataset,

00:23:36.915 --> 00:23:39.051
looked at some column
and data transformations,

00:23:39.051 --> 00:23:40.986
and we finished
with some best practices

00:23:40.986 --> 00:23:43.422
around error handling
and performance.

00:23:43.422 --> 00:23:45.391
I can't wait to see
how you use TabularData

00:23:45.391 --> 00:23:46.625
to make great apps.

00:23:46.625 --> 00:23:48.627
Thank you!