WEBVTT

00:00:01.370 --> 00:00:02.940
Hello, my name is Steve.

00:00:03.070 --> 00:00:05.110
I'm an engineer at Apple.

00:00:05.400 --> 00:00:07.240
Hi, I'm Paul.

00:00:07.280 --> 00:00:09.400
I'm also an engineer.

00:00:09.440 --> 00:00:12.920
In this video, we are going to walk you
through a deep dive into one

00:00:12.920 --> 00:00:18.700
of the new aspects of Core ML,
converting PyTorch models to Core ML.

00:00:19.900 --> 00:00:30.500
[Transcript missing]

00:00:31.070 --> 00:00:34.440
We've expanded support for the
libraries most commonly used

00:00:34.600 --> 00:00:37.000
by the deep learning community.

00:00:37.060 --> 00:00:41.580
We've redesigned the converter
architecture to improve user experience,

00:00:41.580 --> 00:00:45.140
leveraging a new
in-memory representation.

00:00:45.200 --> 00:00:49.000
And we've unified the API so
there's a single call to invoke

00:00:49.000 --> 00:00:51.520
conversion from any model source.

00:00:51.570 --> 00:00:55.150
If you haven't already seen it,
I definitely recommend you check out

00:00:55.150 --> 00:01:00.750
the video that goes into the details
of this new converter architecture.

00:01:01.260 --> 00:01:05.080
But in this video,
I'm going to focus on model conversion,

00:01:05.130 --> 00:01:10.350
starting with the model built in
the PyToRch deep learning framework.

00:01:10.800 --> 00:01:24.200
[Transcript missing]

00:01:24.790 --> 00:01:28.030
Now the question is,
how do you convert that PyTorch

00:01:28.030 --> 00:01:30.970
model into a Core ML model?

00:01:31.230 --> 00:01:35.640
Well, the old Core ML Converter required
you to export your model to

00:01:35.800 --> 00:01:37.740
ONNX as a step in the process.

00:01:37.930 --> 00:01:40.910
And if you've used that converter,
you might have run into

00:01:40.910 --> 00:01:42.410
some of its limitations.

00:01:42.420 --> 00:01:46.100
ONNX is an open standard,
and so it can be slow to evolve

00:01:46.100 --> 00:01:48.260
and introduce new features.

00:01:48.260 --> 00:01:52.560
Compounding that,
ML frameworks like PyTorch need

00:01:52.650 --> 00:01:57.240
time to add support for exporting
their latest model features to ONNX.

00:01:58.370 --> 00:02:01.180
So with the old converter,
you might have found yourself

00:02:01.560 --> 00:02:04.540
with a PyTorch model that
you couldn't export to ONNX,

00:02:05.080 --> 00:02:08.210
blocking its conversion to Core ML.

00:02:08.700 --> 00:02:16.300
[Transcript missing]

00:02:17.000 --> 00:02:23.400
[Transcript missing]

00:02:23.540 --> 00:02:29.160
will walk through the different ways of
converting a PyTorch model into Core ML,

00:02:29.240 --> 00:02:32.780
including some real-world
conversion examples.

00:02:32.810 --> 00:02:41.730
And finally, I'll share some helpful tips
for you to follow if you run

00:02:41.730 --> 00:02:44.680
into trouble along the way.

00:02:44.680 --> 00:02:44.680
So now let's dive into the
new conversion process.

00:02:45.730 --> 00:02:48.120
Starting with the PyToRch
model you want to convert,

00:02:48.190 --> 00:02:52.670
you'll use PyToRch's JIT module
to convert to a representation

00:02:52.670 --> 00:02:54.470
called TorchScript.

00:02:54.630 --> 00:02:57.600
If you're curious,
JIT is an acronym that stands

00:02:57.600 --> 00:03:02.160
for "just in time." Then with
a TorchScript model in hand,

00:03:02.160 --> 00:03:07.250
you'll invoke the new Core ML Converter
to generate an ML model,

00:03:07.490 --> 00:03:08.770
which you can drop into your app.

00:03:09.170 --> 00:03:11.460
Later in the video,
I'll dig into what that

00:03:11.460 --> 00:03:14.160
TorchScript conversion
process looks like,

00:03:14.850 --> 00:03:19.510
But now let's look at how the
new Core ML Converter works.

00:03:20.230 --> 00:03:24.140
The converter is written in Python,
and invoking it only takes

00:03:24.140 --> 00:03:25.840
a couple lines of code.

00:03:25.870 --> 00:03:32.100
You simply provide it with a model,
which can either be a TorchScript object

00:03:32.100 --> 00:03:34.640
or the path to one saved on disk,
and a description of

00:03:34.640 --> 00:03:36.240
the inputs to the model.

00:03:36.240 --> 00:03:41.440
You can also include some information
about the outputs of the model,

00:03:41.440 --> 00:03:41.440
but that's optional.

00:03:42.230 --> 00:03:58.570
The converter works by iterating over
the operations in the TorchScript graph

00:03:58.980 --> 00:04:06.350
and converting them one by one
to their Core ML equivalent.

00:04:06.350 --> 00:04:06.350


00:04:09.200 --> 00:04:12.700
Now, sometimes a model might include
a custom operation that the

00:04:12.710 --> 00:04:14.700
converter doesn't understand.

00:04:14.820 --> 00:04:15.900
But that's okay.

00:04:15.980 --> 00:04:18.290
The converter is designed
to be extensible,

00:04:18.440 --> 00:04:21.490
so it's easy to add
definitions for new operations.

00:04:21.620 --> 00:04:24.720
In many cases,
you can express that operation as

00:04:24.720 --> 00:04:28.890
a combination of existing ones,
which we call a composite op.

00:04:28.960 --> 00:04:32.340
But if that isn't sufficient,
you can also write a custom

00:04:32.380 --> 00:04:36.070
Swift implementation and
target that during conversion.

00:04:36.760 --> 00:04:39.990
I won't get into the details of
how to do that in this video,

00:04:40.150 --> 00:04:44.140
but please check out our online
resources for examples and walkthroughs.

00:04:45.750 --> 00:04:49.500
Now that I've given an overview
of the whole conversion process,

00:04:49.550 --> 00:04:53.970
it's time to circle back and dig
into how to get a TorchScript model

00:04:53.970 --> 00:04:55.940
from your PyToRch model.

00:04:56.000 --> 00:04:58.460
There are two ways PyToRch can do this.

00:04:58.510 --> 00:05:04.060
The first is called tracing,
and the second is called scripting.

00:05:05.100 --> 00:05:25.800
[Transcript missing]

00:05:26.120 --> 00:05:29.000
So what does this call actually do?

00:05:29.070 --> 00:05:34.170
The act of tracing runs an example
input through a forward pass of the

00:05:34.210 --> 00:05:38.900
model and captures the operations
that are invoked as the input makes

00:05:38.950 --> 00:05:41.670
its way through the model's layers.

00:05:41.800 --> 00:05:45.490
The collection of all those
operations then becomes the

00:05:45.570 --> 00:05:48.710
TorchScript representation of the model.

00:05:49.000 --> 00:06:17.800
[Transcript missing]

00:06:18.100 --> 00:06:22.640
Let's make all of this a little more
concrete by working through an example.

00:06:22.710 --> 00:06:27.350
I'd like to introduce my colleague, Paul,
who will take you through the full

00:06:27.350 --> 00:06:31.750
process of converting a segmentation
model from PyTorch to Core ML.

00:06:31.970 --> 00:06:33.660
Thanks, Steve.

00:06:33.700 --> 00:06:38.700
Suppose I have a segmentation model,
and I would like it to run on-device.

00:06:38.740 --> 00:06:41.960
If you aren't familiar with
what a segmentation model does,

00:06:42.000 --> 00:06:45.490
it takes an image and assigns
a class probability score

00:06:45.720 --> 00:06:48.230
to each pixel of that image.

00:06:48.310 --> 00:06:51.500
So how would I get my
model running on-device?

00:06:51.540 --> 00:06:55.380
I'm going to convert my
model into a Core ML model.

00:06:55.530 --> 00:06:58.300
To do this,
I first trace my PyToRch model

00:06:58.400 --> 00:07:04.390
to turn it into TorchScript form
using PyToRch's JIT tracing module.

00:07:04.610 --> 00:07:10.260
Then, I use the new Core ML Converter
to convert the TorchScript model

00:07:10.260 --> 00:07:11.790
into a Core ML model.

00:07:12.170 --> 00:07:15.550
Finally, I will show off how the
resulting Core ML model

00:07:15.670 --> 00:07:17.860
integrates seamlessly into Xcode.

00:07:17.860 --> 00:07:22.080
Let's see what this
process looks like in code.

00:07:23.200 --> 00:07:42.900
[Transcript missing]

00:07:46.800 --> 00:07:51.760
Next, I load in the ResNet101
segmentation model from TorchVision,

00:07:51.760 --> 00:07:53.300
and a sample input.

00:07:53.300 --> 00:07:56.710
In this case, an image of a dog and cat.

00:08:04.840 --> 00:08:10.340
PyTorch models take in tensor objects,
not pill image objects.

00:08:10.370 --> 00:08:15.430
So I convert the image to a
tensor with transforms.toTensor.

00:08:15.430 --> 00:08:20.630
The model also expects an extra dimension
in the tensor denoting the batch size,

00:08:20.630 --> 00:08:22.330
so I add that in as well.

00:08:27.000 --> 00:08:30.120
As mentioned in the slides,
the Core ML Converter

00:08:30.120 --> 00:08:32.240
accepts a TorchScript model.

00:08:32.260 --> 00:08:36.840
To obtain this, I use the torch.jit
module's trace method,

00:08:36.870 --> 00:08:40.690
which converts a PyTorch
model to a TorchScript model.

00:08:49.130 --> 00:08:51.600
Tracing has thrown an exception.

00:08:51.610 --> 00:08:56.020
As it says in the exception method,
only tensors or tuples of tensors

00:08:56.260 --> 00:08:59.300
can be output from traced functions.

00:08:59.560 --> 00:09:03.940
This is a limitation of
PyToRch's JIT module.

00:09:03.990 --> 00:09:08.580
The problem here is that my
model is returning a dictionary.

00:09:08.800 --> 00:09:17.700
[Transcript missing]

00:09:18.310 --> 00:09:24.150
Here, I declare my class wrapper that
inherits from PyToRch's module class.

00:09:26.330 --> 00:09:31.000
I define a model attribute
which contains ResNet-101,

00:09:31.070 --> 00:09:32.120
as used above.

00:09:32.400 --> 00:09:42.800
[Transcript missing]

00:09:43.740 --> 00:09:47.490
Now that the model returns a
tensor and not a dictionary,

00:09:47.490 --> 00:09:49.440
it will successfully trace.

00:09:57.800 --> 00:10:06.000
[Transcript missing]

00:10:09.140 --> 00:10:14.180
I define my input as an image type,
with preprocessing that normalizes the

00:10:14.180 --> 00:10:20.460
image with ImageNet statistics and scales
its values down to lie between 0 and 1.

00:10:20.490 --> 00:10:24.770
This preprocessing is
what ResNet 101 expects.

00:10:26.600 --> 00:10:34.800
[Transcript missing]

00:10:44.030 --> 00:10:47.320
After conversion,
I'll set the metadata of my model so

00:10:47.350 --> 00:10:51.200
it can be understood by other programs,
such as Xcode.

00:10:51.250 --> 00:10:55.150
I set the type of my model to
segmentation and enumerate the

00:10:55.150 --> 00:10:57.370
classes in my model's order.

00:11:06.000 --> 00:11:09.400
So, does my converted model work?

00:11:09.430 --> 00:11:13.400
I can easily visualize my
model's output through Xcode.

00:11:13.470 --> 00:11:15.850
First, I'll save my model.

00:11:20.200 --> 00:11:24.580
Now all I need to do is click
on my saved model in Finder,

00:11:24.580 --> 00:11:27.360
and it will be opened by Xcode.

00:11:27.630 --> 00:11:33.000
Here I can view its metadata,
including input shapes and types.

00:11:33.710 --> 00:11:38.850
To visualize the model's output,
I'll go to the Preview tab and drag

00:11:38.850 --> 00:11:42.130
in my sample image of a dog and cat.

00:11:43.460 --> 00:11:47.190
Looks like my model is successfully
segmenting the pets in this image.

00:11:47.620 --> 00:11:54.700
ResNet-101 was able to be traced,
but some models cannot just be traced.

00:11:54.810 --> 00:11:58.030
To explain how to get these
other models to convert,

00:11:58.200 --> 00:12:01.120
I'll kick it back to Steve.

00:12:02.270 --> 00:12:03.810
Thanks, Paul.

00:12:03.810 --> 00:12:05.840
Okay,
I think we have a pretty good handle

00:12:05.840 --> 00:12:08.040
on how conversion works using tracing.

00:12:08.190 --> 00:12:11.720
But PyTorch offers a second
way to get TorchScript.

00:12:11.800 --> 00:12:14.890
So now let's dig into that one,
which is called scripting.

00:12:14.970 --> 00:12:19.820
Scripting works by taking a PyTorch
model and directly compiling

00:12:19.880 --> 00:12:22.490
into TorchScript operations.

00:12:22.910 --> 00:12:26.830
Remember, tracing captured the model
as data flowed through it.

00:12:27.810 --> 00:12:31.360
But like tracing,
scripting a model is also really easy.

00:12:31.960 --> 00:12:36.300
Simply invoke the "script"
method of PyTorch's JIT module

00:12:36.680 --> 00:12:39.170
and provide it with a model.

00:12:39.510 --> 00:12:44.050
OK, I've shown you two different ways
to get a Tor script representation,

00:12:44.180 --> 00:12:48.410
and you might be wondering when
to use one versus the other.

00:12:48.510 --> 00:12:53.740
One case where you must use scripting
is if the model includes control flow.

00:12:53.740 --> 00:12:56.920
Let's look at an example
to understand why.

00:12:57.090 --> 00:13:01.780
Here, this model has branches and loops,
and scripting will capture

00:13:01.780 --> 00:13:05.260
all of it because it is
directly compiling the model.

00:13:05.460 --> 00:13:09.630
If we traced the model,
what we get is only the path through

00:13:09.630 --> 00:13:13.410
the model for the given input,
which you can see doesn't

00:13:13.410 --> 00:13:15.490
capture the whole model.

00:13:15.980 --> 00:13:19.870
If you do need to script a model,
you'll usually get the best result

00:13:20.150 --> 00:13:24.490
if you trace as much of the model
as possible and script only the

00:13:24.490 --> 00:13:26.950
parts of the model that need it.

00:13:27.050 --> 00:13:30.450
This is because tracing
usually produces a simpler

00:13:30.450 --> 00:13:33.300
representation than scripting does.

00:13:33.580 --> 00:13:37.510
Let's see how to apply this
idea by looking at some code.

00:13:37.650 --> 00:13:40.180
In this example,
I've got a model that runs

00:13:40.180 --> 00:13:44.140
some chunk of code a fixed
number of times inside a loop.

00:13:44.240 --> 00:13:47.560
I've isolated the body of the
loop into something that can

00:13:47.560 --> 00:13:51.580
easily be traced on its own,
and then I can apply scripting

00:13:51.580 --> 00:13:53.250
to the model as a whole.

00:13:53.500 --> 00:13:56.900
What we're basically doing is
limiting the scripting to just the

00:13:57.030 --> 00:14:01.960
bits of control flow that need it,
and then tracing everything else.

00:14:02.030 --> 00:14:05.370
This mixing of tracing and
scripting works because they both

00:14:05.370 --> 00:14:10.420
will skip over code that's already
been converted to TorchScript.

00:14:11.780 --> 00:14:15.700
Now it's time to go through a
concrete example that uses scripting.

00:14:15.760 --> 00:14:18.840
I'll hand it back over to Paul,
who will walk you through

00:14:18.840 --> 00:14:20.700
converting a language model.

00:14:21.020 --> 00:14:22.640
Hey there!

00:14:22.850 --> 00:14:26.850
Suppose I have a sentence completion
model that I want to convert into a

00:14:26.850 --> 00:14:29.050
Core ML model so it can run on device.

00:14:29.120 --> 00:14:33.960
For some context,
sentence completion is a task that

00:14:33.960 --> 00:14:38.490
involves taking a sentence fragment
and using a model to predict the words

00:14:38.490 --> 00:14:40.000
that are likely to come after it.

00:14:40.080 --> 00:14:44.390
So what does this look like
in terms of computation steps?

00:14:45.410 --> 00:14:48.760
I'll start with a few words of a
sentence fragment and pass them

00:14:48.760 --> 00:14:53.000
through what's called an encoder
that translates those words into a

00:14:53.000 --> 00:14:55.530
representation my model can understand.

00:14:55.530 --> 00:14:59.010
In this case,
a sequence of integer tokens.

00:14:59.360 --> 00:15:02.820
Next, I'll pass that sequence
of tokens into my model,

00:15:02.820 --> 00:15:06.400
which will predict the
next token in the sequence.

00:15:06.510 --> 00:15:11.130
I will continue feeding my model
the partially constructed sentence,

00:15:11.390 --> 00:15:15.240
appending new tokens to the end,
until my model predicts a

00:15:15.330 --> 00:15:20.960
special end-of-sentence token,
which means my sentence is completed.

00:15:21.200 --> 00:15:30.400
[Transcript missing]

00:15:30.520 --> 00:15:34.640
The middle part of this diagram,
completing the list of tokens,

00:15:34.660 --> 00:15:38.140
is what I will convert
into a Core ML model.

00:15:38.190 --> 00:15:42.270
The encoder and decoder
are handled separately.

00:15:42.560 --> 00:15:47.100
Let's make sure we understand what's
going on by looking at some pseudocode.

00:15:47.180 --> 00:15:51.000
The core of my model is
my next token predictor.

00:15:51.120 --> 00:15:55.500
For this,
I will use Hugging Face's GPT-2 model.

00:15:55.570 --> 00:15:59.620
The predictor takes a list of
tokens as inputs and gives me a

00:15:59.690 --> 00:16:02.080
prediction for the next token.

00:16:02.330 --> 00:16:07.240
Next, I'll wrap some control flow around
the predictor to continue until

00:16:07.240 --> 00:16:09.870
I see the end-of-sentence token.

00:16:10.800 --> 00:16:14.050
Inside the loop,
I append the predicted token to the

00:16:14.050 --> 00:16:19.100
running list and use that as the
input to my predictor on every loop.

00:16:19.270 --> 00:16:22.520
When my predictor returns
the end-of-sentence token,

00:16:22.580 --> 00:16:25.860
I'll return the complete
sentence for decoding.

00:16:26.180 --> 00:16:31.410
Now to see this whole process in code,
let's dive into the Jupyter Notebook.

00:16:31.600 --> 00:16:47.600
[Transcript missing]

00:16:53.830 --> 00:16:57.900
My model inherits from Torch.module
and contains attributes for

00:16:58.010 --> 00:17:02.100
the end-of-sentence token,
the next token predictor model,

00:17:02.140 --> 00:17:06.210
and the default token,
denoting the beginning of a sentence.

00:17:06.750 --> 00:17:09.840
In its forward method,
just like in the slides,

00:17:09.870 --> 00:17:14.840
I've written a loop body that takes a
list of tokens and predicts the next one.

00:17:14.900 --> 00:17:19.400
The loop continues until the end
of sentence token is generated.

00:17:19.450 --> 00:17:23.080
When this happens,
we will return the sentence.

00:17:24.330 --> 00:17:28.860
As mentioned,
my next token predictor will be GPT-2,

00:17:28.950 --> 00:17:31.590
which will reside in the loop body.

00:17:33.750 --> 00:17:37.140
I'm going to follow the practice
of tracing the loop body,

00:17:37.170 --> 00:17:39.940
separate from scripting the whole model.

00:17:40.000 --> 00:17:44.080
So I'll run the JIT tracer on
only my next token predictor.

00:17:44.130 --> 00:17:49.200
It takes a list of tokens as inputs,
so for tracing, I'll just pass in a

00:17:49.200 --> 00:17:51.170
list of random tokens.

00:17:58.460 --> 00:18:00.820
I can see that the
tracer emitted a warning,

00:18:01.030 --> 00:18:04.900
telling me this trace might
not generalize to other inputs.

00:18:05.000 --> 00:18:09.290
Note that this warning is
from the PyTorch's JIT tracer,

00:18:09.290 --> 00:18:10.800
and not Core ML.

00:18:10.910 --> 00:18:14.800
It will be explained what's going on here
in the troubleshooting section later,

00:18:14.940 --> 00:18:19.510
but for now, I'll ignore this warning,
since there isn't actually a problem.

00:18:20.020 --> 00:18:24.140
With the bulk of the loop body traced,
I can instantiate my sentence finishing

00:18:24.140 --> 00:18:29.490
model and apply the JIT scripter to
prepare it for conversion to Core ML.

00:18:32.950 --> 00:18:36.500
Now with my TorchScript model,
I convert it to a Core ML model

00:18:36.630 --> 00:18:39.550
just like in the segmentation demo.

00:18:46.970 --> 00:18:50.500
Now I'll see if my model
can finish a sentence.

00:18:50.540 --> 00:18:52.500
I create a sentence fragment.

00:18:52.560 --> 00:18:56.650
In this case,
"The Manhattan Bridge is..." Then I run

00:18:56.800 --> 00:19:01.840
it through GPT-2's included encoder
to get the fragment's encoding,

00:19:01.840 --> 00:19:06.090
and then convert that list of
tokens into a Torch tensor.

00:19:12.500 --> 00:19:22.500
[Transcript missing]

00:19:34.300 --> 00:19:35.440
Nice.

00:19:35.510 --> 00:19:38.680
The Core ML model was able
to complete the sentence.

00:19:38.840 --> 00:19:42.900
Looks like it generated a statement
about the Manhattan Bridge.

00:19:43.650 --> 00:19:47.640
You might run into bumps along the road
as you trace and script your models

00:19:47.960 --> 00:19:50.540
to get them into a Core ML format.

00:19:50.660 --> 00:19:54.220
I'll hand it back to Steve to
help you along the way.

00:19:55.920 --> 00:19:59.400
Before we wrap up,
I want to review the snags we hit when

00:19:59.530 --> 00:20:04.480
converting PyTorch models to Core ML,
and go over some troubleshooting

00:20:04.480 --> 00:20:06.390
tips and best practices.

00:20:07.170 --> 00:20:10.950
Thinking back to the segmentation demo,
remember we encountered

00:20:10.950 --> 00:20:12.640
an error during tracing.

00:20:12.690 --> 00:20:15.700
This was because our model
returned to dictionary,

00:20:15.720 --> 00:20:21.050
and JIT tracing can only handle
tensors or tuples of tensors.

00:20:21.470 --> 00:20:26.200
The solution we showed in the demo was
to create a thin wrapper around the model

00:20:26.520 --> 00:20:29.240
that unpacks the model's native outputs.

00:20:29.330 --> 00:20:33.280
Remember, in this example,
the model returned a dictionary,

00:20:33.280 --> 00:20:37.600
so here we're accessing the dictionary
key that represents the inference

00:20:37.600 --> 00:20:40.540
results and returning that tensor.

00:20:40.630 --> 00:20:43.170
Of course,
this idea also works if we want

00:20:43.170 --> 00:20:47.140
to access and return multiple
items from the dictionary,

00:20:47.140 --> 00:20:51.150
or if we need to unpack
other types of containers.

00:20:52.290 --> 00:20:56.200
Now during the language model demo,
we encountered a tracer warning

00:20:56.200 --> 00:21:00.290
that said the trace might not
generalize to other inputs,

00:21:00.290 --> 00:21:04.670
and we see the tracer helpfully
print the troublesome line of code.

00:21:05.180 --> 00:21:07.400
So what's actually going on?

00:21:07.440 --> 00:21:10.580
If we look at the model source
code to understand the warning,

00:21:10.590 --> 00:21:14.100
we see that the model is
slicing one tensor based on

00:21:14.100 --> 00:21:15.920
the size of another tensor.

00:21:15.980 --> 00:21:20.200
Getting the size of a tensor
results in a bare Python value,

00:21:20.220 --> 00:21:25.620
in other words, not a PyTorch tensor,
and the tracer is warning that it

00:21:25.620 --> 00:21:30.500
can't trace the math operations being
performed on these bare Python values.

00:21:31.320 --> 00:21:33.870
However, in this case,
the tracer is a little too

00:21:33.870 --> 00:21:37.960
aggressive in emitting this warning,
and there isn't actually a problem.

00:21:40.040 --> 00:21:44.190
A good rule of thumb when it comes
to tracing code that operates on

00:21:44.190 --> 00:21:48.880
bare Python values is that only
built-in Python operations will be

00:21:49.050 --> 00:21:51.580
captured correctly by the tracer.

00:21:52.020 --> 00:21:55.340
Here are a few examples
to help explain this idea.

00:21:55.380 --> 00:21:59.450
Let's think through these and figure out,
based on that rule of thumb,

00:21:59.450 --> 00:22:02.710
if they will be traced correctly or not.

00:22:03.200 --> 00:22:07.100
The first example is very similar
to what we saw during the demo,

00:22:07.100 --> 00:22:15.000
and will result in a correct trace,
since a built-in operation, in this case,

00:22:15.000 --> 00:22:15.000
addition, is being applied.

00:22:15.600 --> 00:22:24.900
[Transcript missing]

00:22:25.590 --> 00:22:29.130
But the third example
won't trace correctly.

00:22:29.250 --> 00:22:35.470
The JIT tracer doesn't know what the
library function math.squareRoot does,

00:22:35.470 --> 00:22:40.180
and the traced graph will have a
constant value recorded instead

00:22:40.180 --> 00:22:44.890
of the operations to compute
the tensor size and square root.

00:22:45.270 --> 00:22:49.240
But with a simple fix to the model
to replace math.square root with

00:22:49.440 --> 00:22:55.290
Python's built-in power operator,
this will result in a correct trace.

00:22:55.340 --> 00:22:59.100
Now let's look at a case where
scripting a model can fail.

00:22:59.150 --> 00:23:03.150
This model starts with an empty
list and successively appends

00:23:03.150 --> 00:23:05.280
a fixed set of integers to it.

00:23:05.550 --> 00:23:08.620
Keep in mind,
this isn't a terribly useful model.

00:23:08.650 --> 00:23:12.060
I'm just using it to
illustrate a failure condition.

00:23:12.220 --> 00:23:15.720
If I script this model,
I'll get a runtime error that

00:23:15.720 --> 00:23:18.010
hints at a type mismatch.

00:23:18.130 --> 00:23:22.300
The JIT scripter needs type information
to turn a model into TorchScript,

00:23:22.410 --> 00:23:26.400
and does a pretty good job
inferring object types from context.

00:23:26.520 --> 00:23:29.700
However,
there are times when that's not possible.

00:23:29.700 --> 00:23:32.720
And if the scripter can't
figure out an object's type,

00:23:32.830 --> 00:23:35.440
it assumes the object is a tensor.

00:23:35.570 --> 00:23:39.510
In this case,
it's assuming this list is a list

00:23:39.590 --> 00:23:44.360
of tensors while it's actually
being built as a list of integers.

00:23:44.360 --> 00:23:47.140
So what can I do to
help the scripter out?

00:23:47.270 --> 00:23:51.860
Well, I can either include meaningful
initialization of the variable,

00:23:51.860 --> 00:23:54.280
or I can use type annotations.

00:23:54.480 --> 00:23:59.000
Here, I've adjusted the model
to show examples of both.

00:23:59.940 --> 00:24:02.560
There's one last thing I want to mention.

00:24:02.690 --> 00:24:07.270
You always want to make sure your model
is in evaluation mode before tracing.

00:24:07.400 --> 00:24:10.960
This ensures that all the
layers are configured for

00:24:10.960 --> 00:24:13.420
inference rather than training.

00:24:13.520 --> 00:24:15.540
For most layers, this doesn't matter.

00:24:15.630 --> 00:24:19.120
But for example, if you have a dropout
layer in your model,

00:24:19.570 --> 00:24:22.330
setting evaluation mode will
make sure it's disabled.

00:24:22.480 --> 00:24:27.830
And when the converter encounters
operations that have been disabled,

00:24:27.830 --> 00:24:31.350
it will treat them as
pass-through operations.

00:24:31.670 --> 00:24:34.100
We've covered a lot of
material in this video,

00:24:34.200 --> 00:24:38.800
but you can find even more information
in the links associated with the video,

00:24:38.800 --> 00:24:42.240
including the Core ML Converter
documentation,

00:24:42.330 --> 00:24:48.810
information about custom op conversion,
and many detailed TorchScript examples.

00:24:49.550 --> 00:24:52.160
We're really excited to
provide first-class support

00:24:52.370 --> 00:24:54.400
for converting PyTorch models.

00:24:54.450 --> 00:24:58.450
I hope you'll find that the new
Core ML Converter will enable broader

00:24:58.450 --> 00:25:02.880
support for your PyTorch models,
empower you to have optimized

00:25:02.880 --> 00:25:06.720
on-device model execution,
and really provide you with

00:25:06.720 --> 00:25:10.460
maximum support to get your
model converted easily.

00:25:10.540 --> 00:25:12.280
Thanks for watching.