WEBVTT

00:00:02.002 --> 00:00:04.705
Pierre Morf: Welcome
to the session on how to tune

00:00:04.705 --> 00:00:08.475
CPU job scheduling
for Apple silicon games.

00:00:08.475 --> 00:00:12.479
I'm Pierre Morf, working
in the Metal Ecosystem team.

00:00:12.479 --> 00:00:15.282
I've been helping several
third-party developers

00:00:15.282 --> 00:00:21.021
to optimize their GPU and CPU
workloads on Apple platforms.

00:00:21.021 --> 00:00:23.190
With the help
of the CoreOS team,

00:00:23.190 --> 00:00:25.926
I gathered here information
and guidelines

00:00:25.926 --> 00:00:28.729
to achieve better
CPU performance

00:00:28.729 --> 00:00:31.832
and efficiency in games.

00:00:31.832 --> 00:00:33.367
We are focusing on games,

00:00:33.367 --> 00:00:35.836
because they are usually
highly demanding

00:00:35.836 --> 00:00:39.106
in terms of
hardware resources.

00:00:39.106 --> 00:00:42.609
Also, their typical workload
requires hundreds,

00:00:42.609 --> 00:00:45.479
if not thousands,
of CPU jobs to be processed

00:00:45.479 --> 00:00:47.214
every frame.

00:00:47.214 --> 00:00:50.550
To get those done
in 16 milliseconds or less,

00:00:50.550 --> 00:00:54.054
jobs must be tailored
for maximum CPU throughput,

00:00:54.054 --> 00:00:58.058
and their submission overhead
must be minimized.

00:00:58.058 --> 00:01:00.260
First, I'll go through
an overview

00:01:00.260 --> 00:01:05.165
of the Apple silicon CPU
and its unique architecture.

00:01:05.165 --> 00:01:08.001
Then, I'll provide you
with fundamental guidance

00:01:08.001 --> 00:01:13.674
on how to organize your work
to maximize the CPU efficiency.

00:01:13.674 --> 00:01:16.977
Finally, we'll discuss
useful APIs to leverage,

00:01:16.977 --> 00:01:21.381
once those guidelines
are implemented.

00:01:21.381 --> 00:01:25.185
Let's get started with
the Apple CPU architecture.

00:01:25.185 --> 00:01:27.421
Apple has been
designing its own chips

00:01:27.421 --> 00:01:29.423
for more than a decade.

00:01:29.423 --> 00:01:32.159
They are at the core
of Apple devices.

00:01:32.159 --> 00:01:34.728
Apple silicon offers
high performance

00:01:34.728 --> 00:01:37.130
and unmatched efficiency.

00:01:37.130 --> 00:01:40.267
Last year,
Apple introduced the M1 chip.

00:01:40.267 --> 00:01:42.369
That was the first
Apple silicon chip

00:01:42.369 --> 00:01:44.871
to be made available
to Mac computers.

00:01:44.871 --> 00:01:47.107
And this year...

00:01:47.107 --> 00:01:50.510
...we introduced M1 Pro,
and M1 Max.

00:01:50.510 --> 00:01:53.647
Their new design is a huge
leap for Apple silicon

00:01:53.647 --> 00:01:56.516
and makes them able
to efficiently tackle

00:01:56.516 --> 00:01:59.152
very demanding workloads.

00:01:59.152 --> 00:02:03.457
The M1 chip gathers in a single
package many components.

00:02:06.460 --> 00:02:12.632
It contains a CPU, a GPU,
Neural Engine, and many more.

00:02:12.632 --> 00:02:17.137
It also has a high-bandwidth,
low-latency unified memory;

00:02:17.137 --> 00:02:19.506
it is accessible to all
the chip's components

00:02:19.506 --> 00:02:21.641
through Apple Fabric.

00:02:21.641 --> 00:02:25.479
That means the CPU and GPU
may work on the same data

00:02:25.479 --> 00:02:27.848
without copying it.

00:02:27.848 --> 00:02:31.385
Let's zoom into the CPU.

00:02:31.385 --> 00:02:36.323
On M1, the CPU contains cores
of two different types:

00:02:36.323 --> 00:02:40.927
the performance cores
and the efficiency cores.

00:02:40.927 --> 00:02:45.732
Those are physically different,
the E cores being smaller.

00:02:45.732 --> 00:02:48.935
The efficiency cores
are meant to process work

00:02:48.935 --> 00:02:52.172
with a very low
energy consumption.

00:02:52.172 --> 00:02:54.941
There is a very important
takeaway point:

00:02:54.941 --> 00:02:59.012
the P and E cores use
a similar microarchitecture,

00:02:59.012 --> 00:03:02.382
which is really
their inner working.

00:03:02.382 --> 00:03:05.919
They've been designed so that
developers don't need to care

00:03:05.919 --> 00:03:09.890
whether a thread runs
on a P or E core.

00:03:09.890 --> 00:03:13.794
If a program is optimized to
perform well on a type of core,

00:03:13.794 --> 00:03:18.532
it is expected to perform
well on the other.

00:03:18.532 --> 00:03:22.436
Those cores are physically
grouped together into clusters,

00:03:22.436 --> 00:03:25.338
at least according
to their type.

00:03:25.338 --> 00:03:30.077
On M1, each cluster has
a last level cache -- a L2 --

00:03:30.077 --> 00:03:32.546
shared by all of its cores.

00:03:32.546 --> 00:03:36.917
Cross-cluster communication
goes through Apple Fabric.

00:03:36.917 --> 00:03:42.122
The CPU topology shown here
is specific to the M1 chip.

00:03:42.122 --> 00:03:44.991
Other devices may have
a different CPU layout.

00:03:44.991 --> 00:03:50.564
For example, an iPhone XS
has one cluster of two P cores,

00:03:50.564 --> 00:03:54.868
and one cluster
of four E cores.

00:03:54.868 --> 00:03:57.237
This architecture allows
the system to optimize

00:03:57.237 --> 00:04:01.908
for performance when needed
or to optimize for efficiency

00:04:01.908 --> 00:04:04.211
instead,
improving the battery life,

00:04:04.211 --> 00:04:06.980
when performance
is not a priority.

00:04:06.980 --> 00:04:09.950
Each cluster may be
independently activated

00:04:09.950 --> 00:04:13.820
or have its frequency adjusted
by the kernel's scheduler --

00:04:13.820 --> 00:04:15.922
depending on
the current workload,

00:04:15.922 --> 00:04:18.492
the current thermal pressure
for that cluster,

00:04:18.492 --> 00:04:20.560
and other factors.

00:04:20.560 --> 00:04:25.398
Finally, note the availability
of P cores is not guaranteed.

00:04:25.398 --> 00:04:28.468
The system reserves the right
to make them unavailable

00:04:28.468 --> 00:04:32.772
under critical
thermal scenarios.

00:04:32.772 --> 00:04:35.575
Here is an overview
of the different API layers

00:04:35.575 --> 00:04:38.712
interacting with the CPU.

00:04:38.712 --> 00:04:41.248
First we have XNU --

00:04:41.248 --> 00:04:44.217
the kernel running
macOS and iOS.

00:04:44.217 --> 00:04:46.019
That is where
the scheduler lives,

00:04:46.019 --> 00:04:50.457
deciding what runs
on the CPU and when.

00:04:50.457 --> 00:04:53.293
On top of it,
we have two libraries:

00:04:53.293 --> 00:04:57.063
POSIX with pthreads
and Mach objects.

00:04:57.063 --> 00:04:59.132
They both provide
fundamental threading

00:04:59.132 --> 00:05:03.503
and synchronization primitives
to the application.

00:05:03.503 --> 00:05:07.374
On top of that we have
higher-level libraries.

00:05:07.374 --> 00:05:11.444
Thread-related NSObjects
encapsulate POSIX handles.

00:05:11.444 --> 00:05:12.679
In example,

00:05:12.679 --> 00:05:17.017
an NSLock encapsulates
a pthread_mutex_lock,

00:05:17.017 --> 00:05:21.888
a NSThread encapsulates
a pthread, among other things.

00:05:21.888 --> 00:05:24.324
That is also where GCD sits.

00:05:24.324 --> 00:05:26.593
GCD is an advanced
job manager.

00:05:26.593 --> 00:05:28.662
We will cover it later.

00:05:28.662 --> 00:05:31.798
In this session,
we will work our way up

00:05:31.798 --> 00:05:36.303
starting from the low level,
finishing with API features.

00:05:36.303 --> 00:05:40.273
Let's start by focusing on
what works best for the CPU,

00:05:40.273 --> 00:05:44.744
and how to lighten the
workload put on the scheduler.

00:05:44.744 --> 00:05:48.315
This will be our fundamental
efficiency guidelines.

00:05:48.315 --> 00:05:51.585
They apply to any job
manager implementation,

00:05:51.585 --> 00:05:55.288
are API-agnostic,
and apply to many platforms --

00:05:55.288 --> 00:06:01.194
including both Apple silicon
and Intel-based Macs.

00:06:01.194 --> 00:06:03.797
Let's imagine we are
in an ideal world

00:06:03.797 --> 00:06:06.999
and we have this job.

00:06:06.999 --> 00:06:10.470
If we spread it
over four cores,

00:06:10.470 --> 00:06:14.274
it should be processed exactly
four times quicker, right?

00:06:14.274 --> 00:06:18.678
Unfortunately, that's not as
straightforward in a real CPU.

00:06:18.678 --> 00:06:21.514
There are many bookkeeping
operations going on,

00:06:21.514 --> 00:06:24.718
each costing
some execution time.

00:06:24.718 --> 00:06:29.055
There should be three costs
to keep in mind for efficiency.

00:06:29.055 --> 00:06:32.158
Look at the cores
1, 2, and 3.

00:06:32.158 --> 00:06:35.795
They were not doing anything
before processing our job.

00:06:35.795 --> 00:06:38.131
Well, when a CPU core
has nothing to do

00:06:38.131 --> 00:06:43.003
for quite some time,
it goes idle to save energy.

00:06:43.003 --> 00:06:48.174
And reactivating an idle core
takes a little bit of time.

00:06:48.174 --> 00:06:53.213
That is our first cost,
the core wake-up cost.

00:06:53.213 --> 00:06:55.348
Here is another one.

00:06:55.348 --> 00:06:59.452
CPU work is first initiated
by the OS scheduler.

00:06:59.452 --> 00:07:02.122
It decides which process and
thread should be running next,

00:07:02.122 --> 00:07:03.723
and on which core.

00:07:03.723 --> 00:07:08.695
The CPU core then switches
to that execution context.

00:07:08.695 --> 00:07:11.998
We'll call that
the scheduling cost.

00:07:11.998 --> 00:07:15.969
Now, the third and last
type of cost.

00:07:15.969 --> 00:07:19.039
Let's consider the thread
running on Core 0

00:07:19.039 --> 00:07:22.375
signals the ones
on cores 1, 2, and 3.

00:07:22.375 --> 00:07:26.146
For example, with a semaphore.

00:07:26.146 --> 00:07:28.982
That signaling
is not instantaneous.

00:07:28.982 --> 00:07:32.218
During this interval,
the kernel has to identify

00:07:32.218 --> 00:07:35.055
which thread is waiting
on the primitive;

00:07:35.055 --> 00:07:36.923
and in case the thread
was not active,

00:07:36.923 --> 00:07:40.694
it needs to schedule it.

00:07:40.694 --> 00:07:44.664
This delay is called
the synchronization latency.

00:07:44.664 --> 00:07:48.101
Those costs appear,
in one form or another,

00:07:48.101 --> 00:07:50.670
in most CPU architectures.

00:07:50.670 --> 00:07:52.472
They are not an issue
by themselves,

00:07:52.472 --> 00:07:54.407
as they are very, very short.

00:07:54.407 --> 00:07:56.776
But they may become
a performance hit

00:07:56.776 --> 00:07:58.545
if they accumulate,

00:07:58.545 --> 00:08:01.848
appearing repeatedly
and frequently.

00:08:01.848 --> 00:08:06.186
What do those costs
look like in real life?

00:08:06.186 --> 00:08:10.156
This is an instruments trace
of a game running on M1.

00:08:10.156 --> 00:08:12.926
That game exhibits
a problematic pattern,

00:08:12.926 --> 00:08:15.228
repeated most of its frames.

00:08:15.228 --> 00:08:20.800
It tries to parallelize jobs at
an extremely fine granularity.

00:08:20.800 --> 00:08:24.938
We already zoomed
a lot into its timeline.

00:08:24.938 --> 00:08:26.239
To give you an idea,

00:08:26.239 --> 00:08:30.176
that section only takes
18 microseconds.

00:08:30.176 --> 00:08:34.414
Let's focus on that CPU core,
and those two threads.

00:08:34.414 --> 00:08:37.217
Those two threads could
have been running in parallel,

00:08:37.217 --> 00:08:41.788
but they ended up running
serially on that same core.

00:08:41.788 --> 00:08:43.723
Let's see why.

00:08:43.723 --> 00:08:47.660
They synchronize
with each other very often.

00:08:47.660 --> 00:08:51.398
The first one signals
the second one to start

00:08:51.398 --> 00:08:56.035
and very quickly
waits for its peer to be done.

00:08:56.035 --> 00:09:00.573
The second starts working,
quickly signals the first one,

00:09:00.573 --> 00:09:03.309
and waits very shortly after.

00:09:03.309 --> 00:09:07.347
This pattern repeats
over and over.

00:09:07.347 --> 00:09:10.016
We can see two issues here:

00:09:10.016 --> 00:09:13.453
first, synchronization
primitives are used

00:09:13.453 --> 00:09:15.688
at an extremely
high frequency.

00:09:15.688 --> 00:09:19.726
That interrupts work
and introduces overhead.

00:09:19.726 --> 00:09:22.695
We can see that overhead
with the red sections.

00:09:22.695 --> 00:09:26.299
Second, the active work --
the blue sections --

00:09:26.299 --> 00:09:27.834
is extremely brief.

00:09:27.834 --> 00:09:32.872
it only lasts between four
and 20 microseconds.

00:09:32.872 --> 00:09:36.042
This duration is so small,
it is barely shorter

00:09:36.042 --> 00:09:40.780
than the time it takes
to wake up a CPU core.

00:09:40.780 --> 00:09:43.383
During those red sections,
the OS scheduler

00:09:43.383 --> 00:09:47.220
was mostly waiting
for a CPU core to wake up.

00:09:47.220 --> 00:09:49.556
But right before that happens,

00:09:49.556 --> 00:09:52.358
a thread blocks
and frees up the core.

00:09:52.358 --> 00:09:55.728
The second thread
then runs on that same core

00:09:55.728 --> 00:09:57.664
instead of waiting
a little bit longer

00:09:57.664 --> 00:10:00.033
for another one to wake up.

00:10:00.033 --> 00:10:03.369
That's how those two threads
lost a tiny opportunity

00:10:03.369 --> 00:10:05.672
to run in parallel.

00:10:05.672 --> 00:10:07.207
Just from this observation,

00:10:07.207 --> 00:10:10.410
we can already define
two guidelines.

00:10:10.410 --> 00:10:13.947
First, choose
the right job granularity.

00:10:13.947 --> 00:10:19.085
We can achieve it by merging
tiny jobs into larger ones.

00:10:19.085 --> 00:10:21.788
Scheduling a thread
takes a little bit of time,

00:10:21.788 --> 00:10:24.224
no matter what.

00:10:24.224 --> 00:10:26.693
If a job becomes tiny,
the scheduling cost

00:10:26.693 --> 00:10:30.497
will take a relatively larger
part of the thread's timeline.

00:10:30.497 --> 00:10:34.100
The CPU will be underutilized.

00:10:34.100 --> 00:10:37.837
On the contrary, larger jobs
amortize the scheduling cost

00:10:37.837 --> 00:10:39.839
by running for longer.

00:10:39.839 --> 00:10:42.208
We have seen
a professional application --

00:10:42.208 --> 00:10:46.212
submitting lots of
30-microseconds work items --

00:10:46.212 --> 00:10:48.515
drastically increasing
their performance

00:10:48.515 --> 00:10:51.584
when they merged them.

00:10:51.584 --> 00:10:56.923
Second, line up enough work
before leveraging threads.

00:10:56.923 --> 00:10:58.791
This can be done every frame,

00:10:58.791 --> 00:11:02.128
by getting most jobs
ready at once.

00:11:02.128 --> 00:11:04.297
When you signal
and wait for threads,

00:11:04.297 --> 00:11:07.066
that generally means
some will be scheduled

00:11:07.066 --> 00:11:09.836
on the CPU core
and some will be blocked

00:11:09.836 --> 00:11:12.605
and moved off core.

00:11:12.605 --> 00:11:16.175
Doing that many times
is a performance pitfall.

00:11:16.175 --> 00:11:18.278
Waking and pausing
threads repeatedly

00:11:18.278 --> 00:11:21.814
adds more of the costs
we just talked about.

00:11:21.814 --> 00:11:25.385
Conversely, making threads
process more jobs

00:11:25.385 --> 00:11:30.456
without interruption
removes synchronization points.

00:11:30.456 --> 00:11:34.360
As an example, when dealing
with nested for-loops,

00:11:34.360 --> 00:11:38.231
it is a much better idea
to parallelize the outer for

00:11:38.231 --> 00:11:40.700
at a coarser granularity.

00:11:40.700 --> 00:11:43.570
That leaves the inner loops
uninterrupted.

00:11:43.570 --> 00:11:46.172
That gives them
a better coherency,

00:11:46.172 --> 00:11:49.142
a better cache utilization,
and overall,

00:11:49.142 --> 00:11:53.112
fewer of those
synchronization points.

00:11:53.112 --> 00:11:55.281
Before leveraging
more threads,

00:11:55.281 --> 00:11:58.685
determine if it
is worth the cost.

00:11:58.685 --> 00:12:02.288
Let's now have a look
at another game trace.

00:12:02.288 --> 00:12:05.658
That one was running
on an iPhone XS.

00:12:05.658 --> 00:12:09.028
We'll focus
on those helper threads.

00:12:09.028 --> 00:12:13.633
We can see the
synchronization latency here.

00:12:13.633 --> 00:12:15.835
This is the time
it took the kernel

00:12:15.835 --> 00:12:19.772
to signal those
different helpers.

00:12:19.772 --> 00:12:21.975
There are two issues here:

00:12:21.975 --> 00:12:26.746
first, the actual work
is extremely small again --

00:12:26.746 --> 00:12:29.148
roughly 11 microseconds --

00:12:29.148 --> 00:12:32.752
especially in comparison
to the entire overhead.

00:12:32.752 --> 00:12:34.387
Merging those jobs together

00:12:34.387 --> 00:12:37.624
would have been
more energy efficient.

00:12:37.624 --> 00:12:41.461
Second issue:
in that timespan,

00:12:41.461 --> 00:12:45.198
80 different threads
were scheduled on three cores.

00:12:45.198 --> 00:12:47.634
We can see
context switches here,

00:12:47.634 --> 00:12:50.937
those tiny gaps
between active work.

00:12:50.937 --> 00:12:53.506
In this example,
it isn't a problem yet --

00:12:53.506 --> 00:12:56.709
but with more threads,
context switching time

00:12:56.709 --> 00:13:01.114
may accumulate and hinder
the CPU performance.

00:13:01.114 --> 00:13:04.217
How can we minimize all those
different kinds of overhead

00:13:04.217 --> 00:13:10.356
when a typical game has at least
hundreds of jobs every frame?

00:13:10.356 --> 00:13:14.594
The best way of doing so
is by using a job pool.

00:13:14.594 --> 00:13:19.766
Worker threads consume them
through job stealing.

00:13:19.766 --> 00:13:22.101
Scheduling a thread
is done by the kernel;

00:13:22.101 --> 00:13:24.037
we saw it takes some time.

00:13:24.037 --> 00:13:26.005
And the CPU
also has to do some work,

00:13:26.005 --> 00:13:28.241
like context switching.

00:13:28.241 --> 00:13:31.878
On the other hand, starting
a new job in user-space

00:13:31.878 --> 00:13:33.980
is much cheaper.

00:13:33.980 --> 00:13:37.650
Generally, a worker just has
to decrement an atomic counter,

00:13:37.650 --> 00:13:40.920
and grab a pointer to a job.

00:13:40.920 --> 00:13:42.455
Second point:

00:13:42.455 --> 00:13:45.224
avoid interacting
with predetermined threads,

00:13:45.224 --> 00:13:49.696
since using workers will reduce
the amount of context switches.

00:13:49.696 --> 00:13:52.365
And as they grab more jobs,

00:13:52.365 --> 00:13:54.500
you leverage
an already active thread

00:13:54.500 --> 00:13:57.370
on an already active core.

00:13:57.370 --> 00:14:00.173
Finally, use your pool wisely.

00:14:00.173 --> 00:14:04.911
Wake up just enough workers
for the work being queued up.

00:14:04.911 --> 00:14:08.214
And the previous rule
applies here too:

00:14:08.214 --> 00:14:10.516
ensure enough work is lined up

00:14:10.516 --> 00:14:13.252
to justify waking
a worker thread,

00:14:13.252 --> 00:14:16.456
and keeping it busy.

00:14:16.456 --> 00:14:18.324
We reduced our overhead;

00:14:18.324 --> 00:14:22.495
now, we must make the most
out of our CPU cycles.

00:14:22.495 --> 00:14:26.032
Here are some
patterns to avoid.

00:14:26.032 --> 00:14:27.767
Avoid busy waits.

00:14:27.767 --> 00:14:29.936
They potentially
lock a P core,

00:14:29.936 --> 00:14:32.672
instead of doing something
useful with it.

00:14:32.672 --> 00:14:36.542
They also prevents the scheduler
from promoting a thread

00:14:36.542 --> 00:14:39.378
from and E to a P core.

00:14:39.378 --> 00:14:43.750
You are also wasting energy
and producing unnecessary heat,

00:14:43.750 --> 00:14:48.254
eating away
your thermal headroom.

00:14:48.254 --> 00:14:51.924
Second, the definition
of the yield function is loose

00:14:51.924 --> 00:14:54.794
across platforms
and even OSes.

00:14:54.794 --> 00:14:56.629
On Apple platforms, it means,

00:14:56.629 --> 00:14:58.865
"try to cede
the core I'm running on

00:14:58.865 --> 00:15:00.800
to any other thread
on the system,

00:15:00.800 --> 00:15:04.437
anything else,
whatever their priority is."

00:15:04.437 --> 00:15:08.608
It effectively tanks the current
thread priority to zero.

00:15:08.608 --> 00:15:11.878
A yield also has
a system-defined duration.

00:15:11.878 --> 00:15:16.315
It may be very long --
up to 10 milliseconds.

00:15:16.315 --> 00:15:20.353
Third, calls to sleep
are discouraged as well.

00:15:20.353 --> 00:15:24.390
Waiting for a specific event
is much more efficient.

00:15:24.390 --> 00:15:26.626
Also, note on Apple platforms,

00:15:26.626 --> 00:15:31.497
sleep(0) has a no meaning
and that call is even discarded.

00:15:31.497 --> 00:15:33.900
Those patterns
are generally a sign

00:15:33.900 --> 00:15:38.070
a fundamental scheduling error
happened in the first place.

00:15:38.070 --> 00:15:40.506
Instead,
wait on explicit signals

00:15:40.506 --> 00:15:45.878
with a semaphore
or a conditional variable.

00:15:45.878 --> 00:15:48.581
Final guideline:
scale the thread count

00:15:48.581 --> 00:15:51.284
to match the CPU core count.

00:15:51.284 --> 00:15:54.787
Avoid recreating new thread
pools in each framework

00:15:54.787 --> 00:15:57.456
or middleware you are using.

00:15:57.456 --> 00:16:01.761
Do not scale your thread count
based on your workload either.

00:16:01.761 --> 00:16:03.863
If your workload
increases dramatically,

00:16:03.863 --> 00:16:06.199
so will your thread count.

00:16:06.199 --> 00:16:09.035
Instead, query CPU information

00:16:09.035 --> 00:16:11.337
to size your thread pool
appropriately,

00:16:11.337 --> 00:16:13.873
and maximize
parallelization opportunities

00:16:13.873 --> 00:16:16.209
for the current system.

00:16:16.209 --> 00:16:20.313
Let's see how to query
this information.

00:16:20.313 --> 00:16:23.416
Starting with macOS Monterey
and iOS 15,

00:16:23.416 --> 00:16:26.719
you can query advanced details
about the CPU layout

00:16:26.719 --> 00:16:29.488
with the sysctl interface.

00:16:29.488 --> 00:16:33.292
In addition to get an
overall count of all CPU cores,

00:16:33.292 --> 00:16:38.231
you can now query how many
types of cores a machine has

00:16:38.231 --> 00:16:40.700
with nperflevels.

00:16:40.700 --> 00:16:44.670
On M1, we have two
types of cores: P and E.

00:16:44.670 --> 00:16:48.341
Use this range to query
data per core type,

00:16:48.341 --> 00:16:51.978
zero being the
most performant.

00:16:51.978 --> 00:16:56.315
For example,
perflevel{N}.logicalcpu

00:16:56.315 --> 00:17:00.453
tells how many P cores
the current CPU has.

00:17:00.453 --> 00:17:02.021
This is just an overview.

00:17:02.021 --> 00:17:04.056
You can also query
many other details,

00:17:04.056 --> 00:17:08.794
like how many cores
share the same L2.

00:17:08.794 --> 00:17:12.031
For more details,
refer to the sysctl man page,

00:17:12.031 --> 00:17:16.035
or the documentation webpage.

00:17:16.035 --> 00:17:18.070
When profiling your CPU usage,

00:17:18.070 --> 00:17:21.474
two instruments tracks
are very useful.

00:17:21.474 --> 00:17:25.578
They are available in the
Game Performance template.

00:17:25.578 --> 00:17:28.214
The first one, System Load,

00:17:28.214 --> 00:17:32.718
gives the number of active
threads per CPU core.

00:17:32.718 --> 00:17:36.389
The second one
is Thread State Trace.

00:17:36.389 --> 00:17:39.292
By default, the detail pane
shows the amount

00:17:39.292 --> 00:17:44.697
of thread state changes
and their duration per process.

00:17:44.697 --> 00:17:49.402
It can be changed to
the Context Switches view.

00:17:49.402 --> 00:17:53.339
This will give you a count
of context switches per process

00:17:53.339 --> 00:17:55.608
in the selected time range.

00:17:55.608 --> 00:17:59.545
Context switches count
is a useful metric to measure

00:17:59.545 --> 00:18:03.749
an app's scheduling
efficiency.

00:18:03.749 --> 00:18:06.585
Let's close on this section.

00:18:06.585 --> 00:18:08.287
By following those guidelines,

00:18:08.287 --> 00:18:10.556
you will make the most
out of the CPU

00:18:10.556 --> 00:18:14.894
and streamline what
the scheduler has to do.

00:18:14.894 --> 00:18:18.197
Compacting tiny, tiny jobs
into longer running ones

00:18:18.197 --> 00:18:21.567
increases the benefits of
microarchitectural features,

00:18:21.567 --> 00:18:25.905
like caches, prefetchers,
and predictors.

00:18:25.905 --> 00:18:29.475
Processing more jobs at once
means less interrupt latency

00:18:29.475 --> 00:18:31.978
and context switches.

00:18:31.978 --> 00:18:34.847
An appropriately scaled
thread pool makes it easier

00:18:34.847 --> 00:18:39.251
for the scheduler to rebalance
work between E and P cores.

00:18:39.251 --> 00:18:42.121
A key take-away for efficiency
and performance

00:18:42.121 --> 00:18:43.656
is to minimize the frequency

00:18:43.656 --> 00:18:49.161
at which your workload
is going wide and narrow.

00:18:49.161 --> 00:18:52.465
Let's now dive into which
API blocks you can leverage

00:18:52.465 --> 00:18:55.001
while applying
those guidelines.

00:18:55.001 --> 00:18:56.302
In this section,

00:18:56.302 --> 00:19:00.639
we'll cover prioritization
and scheduling policies,

00:19:00.639 --> 00:19:02.541
synchronization primitives,

00:19:02.541 --> 00:19:08.047
and memory considerations
when multithreading.

00:19:08.047 --> 00:19:13.819
But first, let's start by
having a sneak peek at GCD.

00:19:13.819 --> 00:19:15.788
If you don't have
a job manager,

00:19:15.788 --> 00:19:19.792
or if it doesn't reach the high
performance you are aiming for,

00:19:19.792 --> 00:19:23.062
GCD is a fantastic choice.

00:19:23.062 --> 00:19:28.367
It is a general-purpose
job manager using job stealing.

00:19:28.367 --> 00:19:32.004
It is available on all
Apple platforms and Linux,

00:19:32.004 --> 00:19:35.608
and it is open source.

00:19:35.608 --> 00:19:38.277
This API is highly optimized.

00:19:38.277 --> 00:19:42.681
First, it already follows
all the best practices for you.

00:19:42.681 --> 00:19:47.019
Second, it is integrated
in the XNU kernel.

00:19:47.019 --> 00:19:51.223
That means GCD may keep track
of internal details for you,

00:19:51.223 --> 00:19:54.960
like the heat dissipation
capacity of the current machine,

00:19:54.960 --> 00:19:58.964
its P/E core ratio,
the current thermal pressure

00:19:58.964 --> 00:20:01.067
and so on.

00:20:01.067 --> 00:20:05.771
Its interface relies on serial
and concurrent dispatch queues.

00:20:05.771 --> 00:20:11.110
You can enqueue jobs in them
with varying priorities.

00:20:11.110 --> 00:20:14.880
Internally, each dispatch queue
leverages a variable amount

00:20:14.880 --> 00:20:18.717
of threads from
a private thread pool.

00:20:18.717 --> 00:20:24.690
That number depends on the type
of queue and job properties.

00:20:24.690 --> 00:20:29.361
This internal thread pool is
shared for the entire process.

00:20:29.361 --> 00:20:31.597
That means in a given process,

00:20:31.597 --> 00:20:36.869
multiple libraries may use GCD
without recreating a new pool.

00:20:36.869 --> 00:20:39.105
GCD offers many features.

00:20:39.105 --> 00:20:42.174
Here we'll quickly review
just two functions

00:20:42.174 --> 00:20:44.477
from the concurrent
dispatch queues,

00:20:44.477 --> 00:20:47.346
just to get a sense
of how it works.

00:20:47.346 --> 00:20:50.182
The first one, dispatch_async,

00:20:50.182 --> 00:20:53.285
allows you to enqueue a job
made of a function pointer

00:20:53.285 --> 00:20:55.421
and a data pointer.

00:20:55.421 --> 00:20:57.790
When starting a job,
the concurrent queue

00:20:57.790 --> 00:21:01.727
may leverage an additional
thread if the next job in line

00:21:01.727 --> 00:21:04.130
is also ready to be processed.

00:21:04.130 --> 00:21:05.397
That is a great option

00:21:05.397 --> 00:21:09.702
for typical asynchronous
independent jobs.

00:21:09.702 --> 00:21:13.639
But not so much
for massively parallel problems.

00:21:13.639 --> 00:21:18.677
In that case,
there is dispatch_apply.

00:21:18.677 --> 00:21:22.047
That one will leverage many
threads right from the start,

00:21:22.047 --> 00:21:25.784
without overloading
GCD's thread manager.

00:21:25.784 --> 00:21:29.588
We have seen several pro apps
increasing their performance

00:21:29.588 --> 00:21:36.295
by migrating a parallel for
to using dispatch_apply.

00:21:36.295 --> 00:21:39.498
That was just
a quick overview of GCD.

00:21:39.498 --> 00:21:42.701
To learn more about it
and which patterns to avoid,

00:21:42.701 --> 00:21:47.139
refer to those
two WWDC sessions.

00:21:49.141 --> 00:21:53.345
Let's now switch
to custom job managers.

00:21:53.345 --> 00:21:55.548
We'll cover the most
important points

00:21:55.548 --> 00:21:57.883
when manipulating
threads directly

00:21:57.883 --> 00:22:00.286
and synchronizing them.

00:22:00.286 --> 00:22:03.822
Let's begin
with prioritization.

00:22:03.822 --> 00:22:05.291
In the previous section,

00:22:05.291 --> 00:22:08.127
we reviewed how to increase
the CPU efficiency

00:22:08.127 --> 00:22:10.963
when submitting jobs.

00:22:10.963 --> 00:22:16.402
But so far, we had not mentioned
that all jobs are not equal.

00:22:16.402 --> 00:22:18.437
Some are time-critical,

00:22:18.437 --> 00:22:21.407
their result is needed
as soon as possible.

00:22:21.407 --> 00:22:23.809
And some other
will only be required

00:22:23.809 --> 00:22:25.878
in the next frame or two.

00:22:25.878 --> 00:22:29.014
So it is necessary to convey
a sense of importance

00:22:29.014 --> 00:22:32.184
when processing your jobs
to give more resources

00:22:32.184 --> 00:22:36.222
to the more important ones.

00:22:36.222 --> 00:22:40.292
That can be done
by prioritizing your threads.

00:22:40.292 --> 00:22:43.662
Setting the right thread
priorities also informs

00:22:43.662 --> 00:22:45.998
the system your game
is more important

00:22:45.998 --> 00:22:48.834
than background activity.

00:22:48.834 --> 00:22:50.703
This can be achieved
by setting a thread

00:22:50.703 --> 00:22:57.176
with either a raw CPU
priority value or a QoS class.

00:22:57.176 --> 00:23:02.314
Both concepts are related,
yet slightly different.

00:23:02.314 --> 00:23:05.384
A raw CPU priority
is an integer value

00:23:05.384 --> 00:23:09.888
telling how important
computational throughput is.

00:23:09.888 --> 00:23:12.925
On Apple platforms,
contrary to Linux,

00:23:12.925 --> 00:23:17.997
this is an ascending value --
the higher, the more important.

00:23:17.997 --> 00:23:21.934
This CPU priority also hints --
among other factors --

00:23:21.934 --> 00:23:27.640
at whether a thread should run
on a P or E core.

00:23:27.640 --> 00:23:30.376
Now, this CPU priority
doesn't affect

00:23:30.376 --> 00:23:32.578
the rest of system resources

00:23:32.578 --> 00:23:34.713
since it doesn't give
any intention

00:23:34.713 --> 00:23:37.950
about what the
thread is doing.

00:23:37.950 --> 00:23:40.052
Threads can instead
be prioritized

00:23:40.052 --> 00:23:45.424
with Quality of Service --
QoS for short.

00:23:45.424 --> 00:23:49.528
QoS has been designed
to attach semantics to threads.

00:23:49.528 --> 00:23:52.331
This intention greatly
helps the scheduler

00:23:52.331 --> 00:23:56.969
to make intelligent decisions
about when to execute tasks,

00:23:56.969 --> 00:23:59.305
and makes the OS
more responsive.

00:23:59.305 --> 00:24:02.308
For example,
a lower importance task

00:24:02.308 --> 00:24:06.245
may be slightly deferred in
time in order to save energy.

00:24:06.245 --> 00:24:10.382
It also allows to prioritize
system resources access

00:24:10.382 --> 00:24:13.452
like network, disk access.

00:24:13.452 --> 00:24:16.822
It also provides thresholds
for timer coalescing --

00:24:16.822 --> 00:24:19.892
an energy-saving feature.

00:24:19.892 --> 00:24:25.497
QoS classes also include
a CPU priority.

00:24:25.497 --> 00:24:27.466
There are five QoS classes,

00:24:27.466 --> 00:24:30.235
going from
QOS_CLASS_BACKGROUND,

00:24:30.235 --> 00:24:34.640
the least important one,
to QOS_CLASS_USER_INTERACTIVE,

00:24:34.640 --> 00:24:36.575
the highest one.

00:24:36.575 --> 00:24:40.379
Each includes
a default CPU priority.

00:24:40.379 --> 00:24:43.582
Optionally, you may slightly
downgrade it

00:24:43.582 --> 00:24:46.518
within a limited range.

00:24:46.518 --> 00:24:49.288
This is useful if you want
to finely tweak the CPU priority

00:24:49.288 --> 00:24:54.793
for several threads opting
into the same QoS class.

00:24:54.793 --> 00:24:58.230
Note to be very careful
with the Background class --

00:24:58.230 --> 00:25:03.235
threads using it may not
run at all for a very long time.

00:25:03.235 --> 00:25:10.709
So overall, games use CPU
priorities ranging from 5 to 47.

00:25:10.709 --> 00:25:16.181
Let's see how that's done
in practice.

00:25:16.181 --> 00:25:18.717
First, you need to allocate
and initialize

00:25:18.717 --> 00:25:23.856
the pthread attributes
with default values.

00:25:23.856 --> 00:25:27.292
You then set
the required QoS class

00:25:27.292 --> 00:25:32.965
and then pass those attributes
to the pthread_create function.

00:25:32.965 --> 00:25:36.568
Finish by destroying
the attributes structure.

00:25:36.568 --> 00:25:40.873
You can also set a QoS class
to an already existing thread.

00:25:40.873 --> 00:25:45.544
As an example, that function
affects the calling thread.

00:25:45.544 --> 00:25:49.081
Note here,
we used an offset of -5,

00:25:49.081 --> 00:25:54.553
downgrading the class
CPU priority from 47 to 42.

00:25:54.553 --> 00:25:57.689
Note you can see the np suffix
in the function names.

00:25:57.689 --> 00:26:01.560
That stands for "nonportable";
it's a naming convention

00:26:01.560 --> 00:26:05.998
used for functions
exclusive to Apple platforms.

00:26:05.998 --> 00:26:10.469
Finally, beware that if instead
of using those functions,

00:26:10.469 --> 00:26:14.273
you directly set a raw CPU
priority value,

00:26:14.273 --> 00:26:17.810
you opt out of QoS
for that thread.

00:26:17.810 --> 00:26:21.213
That is permanent,
and you cannot opt back in QoS

00:26:21.213 --> 00:26:25.984
for that thread afterwards.

00:26:25.984 --> 00:26:29.388
iOS and macOS deal
with many processes,

00:26:29.388 --> 00:26:32.958
user-facing or running
in the background.

00:26:32.958 --> 00:26:36.962
In some cases, the system
may become overloaded.

00:26:36.962 --> 00:26:40.098
If that happens,
the kernel needs a way

00:26:40.098 --> 00:26:44.036
to ensure all threads get
a chance to run at some point.

00:26:44.036 --> 00:26:47.806
That is done
with priority decay.

00:26:47.806 --> 00:26:49.641
In this special case,

00:26:49.641 --> 00:26:54.046
the kernel slowly lowers
thread priorities over time;

00:26:54.046 --> 00:26:58.884
all threads then have
a chance to run.

00:26:58.884 --> 00:27:01.153
Priority decay
may be problematic

00:27:01.153 --> 00:27:03.288
in very special cases.

00:27:03.288 --> 00:27:06.859
Typically, games have a couple
of highly critical threads,

00:27:06.859 --> 00:27:09.928
like the main thread
and the render thread.

00:27:09.928 --> 00:27:12.197
If the render thread
gets preempted,

00:27:12.197 --> 00:27:14.366
you may miss
a presentation window,

00:27:14.366 --> 00:27:16.768
and the game will stutter.

00:27:16.768 --> 00:27:20.072
In those cases, you can
opt out of priority decay

00:27:20.072 --> 00:27:22.875
with scheduling policies.

00:27:22.875 --> 00:27:24.510
By default,

00:27:24.510 --> 00:27:29.014
threads a created with
the SCHED_OTHER policy.

00:27:29.014 --> 00:27:31.283
This is a time-sharing policy.

00:27:31.283 --> 00:27:36.054
Threads using it may be subject
to priority decay.

00:27:36.054 --> 00:27:38.824
It is also compatible
with QoS classes

00:27:38.824 --> 00:27:40.859
we presented before.

00:27:40.859 --> 00:27:46.131
On the other hand, we have
the optional SCHED_RR policy.

00:27:46.131 --> 00:27:49.201
RR stands for "round-robin".

00:27:49.201 --> 00:27:52.504
Threads opting into it
have a fixed priority

00:27:52.504 --> 00:27:55.374
unaffected by priority decay.

00:27:55.374 --> 00:28:00.913
It offers a better consistency
in execution latency.

00:28:00.913 --> 00:28:05.384
Note it is exclusively designed
for consistent, periodic,

00:28:05.384 --> 00:28:09.821
and high-priority work,
for example,

00:28:09.821 --> 00:28:14.693
a dedicated render thread
or per-frame worker threads.

00:28:14.693 --> 00:28:18.964
Threads opting into it must work
on a very specific time window,

00:28:18.964 --> 00:28:24.036
and not continuously busy
the CPU 100 percent of the time.

00:28:24.036 --> 00:28:26.972
Using this policy may also
lead to starvation

00:28:26.972 --> 00:28:29.074
in your other threads.

00:28:29.074 --> 00:28:32.978
Finally, this policy is
incompatible with QoS classes --

00:28:32.978 --> 00:28:38.250
threads will need to use
a raw CPU priority.

00:28:38.250 --> 00:28:41.987
Here is a recommended
layout for game threads.

00:28:41.987 --> 00:28:45.123
First, define within your game

00:28:45.123 --> 00:28:48.660
what is high-, medium-
and low-priority

00:28:48.660 --> 00:28:51.930
and what is critical
to the user experience.

00:28:51.930 --> 00:28:54.833
Splitting work by priority
lets the system know

00:28:54.833 --> 00:28:59.905
which parts of your application
are the most important.

00:28:59.905 --> 00:29:02.174
Use instruments
to profile your game,

00:29:02.174 --> 00:29:05.077
and only opt into SCHED_RR
for the threads

00:29:05.077 --> 00:29:07.379
which actually need it.

00:29:07.379 --> 00:29:11.583
Also, never use SCHED_RR
for a long duration work,

00:29:11.583 --> 00:29:13.652
extending multiple frames.

00:29:13.652 --> 00:29:16.822
Rely on QoS in those cases
to help the system

00:29:16.822 --> 00:29:20.993
balance performance
with other processes.

00:29:20.993 --> 00:29:23.662
Another reason to favor
opting into QoS

00:29:23.662 --> 00:29:27.599
is when a thread interacts
with Apple frameworks like GCD

00:29:27.599 --> 00:29:29.835
or NSOperationQueues.

00:29:29.835 --> 00:29:32.838
Those frameworks try to
propagate the QoS class

00:29:32.838 --> 00:29:36.074
from the job issuer
into the job itself.

00:29:36.074 --> 00:29:39.277
That is obviously ignored
if the issuing thread

00:29:39.277 --> 00:29:42.547
has abandoned QoS.

00:29:42.547 --> 00:29:46.985
Let's cover one last point
related to priorities:

00:29:46.985 --> 00:29:50.055
priority inversion.

00:29:50.055 --> 00:29:54.526
Priority inversion happens when
a high-priority thread stalls,

00:29:54.526 --> 00:29:59.064
being blocking by
a low-priority thread.

00:29:59.064 --> 00:30:02.601
This typically happens
with mutual exclusions.

00:30:02.601 --> 00:30:05.504
Two threads try to access
the same resource,

00:30:05.504 --> 00:30:07.973
fighting to get the same lock.

00:30:07.973 --> 00:30:12.044
In some cases, the system may be
able to resolve this inversion

00:30:12.044 --> 00:30:15.414
by boosting
the low-priority thread.

00:30:15.414 --> 00:30:17.883
Let's see how that works.

00:30:17.883 --> 00:30:19.785
Let's consider two threads --

00:30:19.785 --> 00:30:22.487
here are their
execution timeline.

00:30:22.487 --> 00:30:25.590
In this example,
the blue thread is low priority,

00:30:25.590 --> 00:30:29.327
the green one
is high priority.

00:30:29.327 --> 00:30:31.963
In the middle,
we have the lock timeline,

00:30:31.963 --> 00:30:37.669
showing which of the two
threads will own that lock.

00:30:37.669 --> 00:30:39.905
The blue thread
starts executing,

00:30:39.905 --> 00:30:41.740
and acquires the lock.

00:30:41.740 --> 00:30:44.509
The green thread also starts.

00:30:44.509 --> 00:30:48.847
At this point, the green thread
tries to acquire that lock,

00:30:48.847 --> 00:30:51.883
currently owned
by the blue thread.

00:30:51.883 --> 00:30:53.251
The green thread blocks

00:30:53.251 --> 00:30:57.589
and waits for that lock
to be available again.

00:30:57.589 --> 00:30:59.024
In this case,

00:30:59.024 --> 00:31:04.096
the runtime can tell
which thread owns that lock.

00:31:04.096 --> 00:31:07.332
Therefore, it can resolve
the priority inversion,

00:31:07.332 --> 00:31:11.336
by boosting the blue thread's
low priority.

00:31:11.336 --> 00:31:14.106
Which primitives have
the ability to resolve

00:31:14.106 --> 00:31:19.077
priority inversion
and which ones don't?

00:31:19.077 --> 00:31:23.582
Symmetric primitives with a
single known owner can do that,

00:31:23.582 --> 00:31:30.489
like pthread_mutex_t or the most
efficient, os_unfair_lock.

00:31:30.489 --> 00:31:34.726
Asymmetric primitives like
pthread conditional variables

00:31:34.726 --> 00:31:38.797
or dispatch_semaphore
don't have this ability,

00:31:38.797 --> 00:31:43.668
because the runtime doesn't know
which thread will signal it.

00:31:43.668 --> 00:31:45.036
Keep this feature in mind

00:31:45.036 --> 00:31:47.773
when choosing
a synchronization primitive,

00:31:47.773 --> 00:31:53.779
and favor symmetric primitives
for mutually exclusive access.

00:31:53.779 --> 00:31:55.480
To finish this section,

00:31:55.480 --> 00:32:00.685
let's discuss a couple of
recommendations about memory.

00:32:00.685 --> 00:32:03.655
When interacting with
Objective-C frameworks,

00:32:03.655 --> 00:32:07.425
some objects are created
as autorelease.

00:32:07.425 --> 00:32:10.128
That means they are added
to a list,

00:32:10.128 --> 00:32:16.067
so that their deallocation
only happens at a later time.

00:32:16.067 --> 00:32:18.603
Autorelease pool blocks
are scopes

00:32:18.603 --> 00:32:22.874
limiting how long such objects
can be kept around.

00:32:22.874 --> 00:32:26.011
They effectively help reducing
the peak memory footprint

00:32:26.011 --> 00:32:28.413
of your app.

00:32:28.413 --> 00:32:31.550
It is important to have
at least one autorelease pool,

00:32:31.550 --> 00:32:34.319
in every thread entry point.

00:32:34.319 --> 00:32:37.422
If any thread manipulates
autoreleased objects --

00:32:37.422 --> 00:32:40.725
for example, through Metal --
without one,

00:32:40.725 --> 00:32:44.462
that will lead
to memory leaks.

00:32:44.462 --> 00:32:46.832
Autorelease pool blocks
can be nested,

00:32:46.832 --> 00:32:50.435
to better control
when memory is recycled.

00:32:50.435 --> 00:32:54.105
The render thread should
ideally create a second one

00:32:54.105 --> 00:32:57.943
around the repeated frame
rendering routine.

00:32:57.943 --> 00:33:00.645
Worker threads should
have a second one

00:33:00.645 --> 00:33:04.716
starting on activation
and closed as the worker

00:33:04.716 --> 00:33:08.186
gets parked,
waiting for more work.

00:33:08.186 --> 00:33:10.689
Let's see an example.

00:33:10.689 --> 00:33:13.692
This is a worker thread
entry point.

00:33:13.692 --> 00:33:18.930
It starts right away with
an autorelease pool block.

00:33:18.930 --> 00:33:22.267
It then waits for jobs
to be available.

00:33:22.267 --> 00:33:24.870
When the worker
gets activated,

00:33:24.870 --> 00:33:27.305
we add a new
autorelease pool block,

00:33:27.305 --> 00:33:30.642
and keep it around
as we process jobs.

00:33:30.642 --> 00:33:33.778
When the thread is about
to wait and be parked,

00:33:33.778 --> 00:33:36.915
we exit the nested pool.

00:33:36.915 --> 00:33:40.552
To conclude,
one quick tip about memory.

00:33:40.552 --> 00:33:45.056
To improve performance,
avoid having multiple threads

00:33:45.056 --> 00:33:49.761
simultaneously write data
located in the same cache line.

00:33:49.761 --> 00:33:52.330
That is known
as "false sharing".

00:33:52.330 --> 00:33:55.767
Multiple reads from the
same data structure is fine,

00:33:55.767 --> 00:34:00.372
but such competing writes lead
to ping-ponging that cache line

00:34:00.372 --> 00:34:03.742
between different
hardware caches.

00:34:03.742 --> 00:34:08.847
On Apple silicon,
a cache line is 128 bytes long.

00:34:08.847 --> 00:34:11.783
One solution to this
is inserting padding

00:34:11.783 --> 00:34:16.454
within your data structure
to reduce memory conflicts.

00:34:16.454 --> 00:34:20.125
We are done
with this last section.

00:34:20.125 --> 00:34:21.526
Let's wrap up.

00:34:21.526 --> 00:34:25.196
We first got an overview
of the Apple CPU architecture,

00:34:25.196 --> 00:34:27.632
and how its
groundbreaking design

00:34:27.632 --> 00:34:31.603
makes it much more efficient.

00:34:31.603 --> 00:34:35.240
Then, we got into how to feed
the CPU efficiently

00:34:35.240 --> 00:34:37.142
and make it run smoothly

00:34:37.142 --> 00:34:41.446
while reducing the load
put on the OS scheduler.

00:34:41.446 --> 00:34:44.149
We finally reviewed
important API concepts,

00:34:44.149 --> 00:34:48.520
such as thread prioritization,
scheduling policies,

00:34:48.520 --> 00:34:50.388
priority inversion,

00:34:50.388 --> 00:34:53.525
finishing with tips
about memory.

00:34:53.525 --> 00:34:55.560
Don't forget to regularly
profile your game

00:34:55.560 --> 00:34:58.897
with instruments to keep
an eye on its workload,

00:34:58.897 --> 00:35:03.101
so you can spot
performance issues early.

00:35:03.101 --> 00:35:05.203
Thank you for your attention.