WEBVTT

00:00:00.870 --> 00:00:03.810
Welcome, my name is Jedd Haberstro,

00:00:03.810 --> 00:00:06.660
and I'm an Engineer in Apple's GPU, Graphics,

00:00:06.660 --> 00:00:08.223
and Displays Software Group.

00:00:09.540 --> 00:00:10.950
I'm excited to tell you

00:00:10.950 --> 00:00:14.250
about the new Apple family 9 GPU architecture

00:00:14.250 --> 00:00:17.730
in A17 Pro and the M3 family of chips,

00:00:17.730 --> 00:00:21.243
which are at the heart of iPhone 15 Pro and the new Macs.

00:00:23.340 --> 00:00:25.470
Across Apple's product line,

00:00:25.470 --> 00:00:28.860
the GPU powers many of the rich user experiences

00:00:28.860 --> 00:00:30.810
our customers love.

00:00:30.810 --> 00:00:34.713
Whether it be gaming on the go with the new iPhone 15 Pro,

00:00:36.450 --> 00:00:39.810
delivering silky smooth UI animations for your apps

00:00:39.810 --> 00:00:40.833
on the new iMac,

00:00:41.850 --> 00:00:43.470
or leveraging machine learning

00:00:43.470 --> 00:00:46.290
to perform advanced video and image processing

00:00:46.290 --> 00:00:48.003
on the new MacBook Pros,

00:00:49.260 --> 00:00:53.613
the GPU plays a critical role in enabling these apps.

00:00:55.830 --> 00:00:59.220
The Metal API is used to harness the computing capabilities

00:00:59.220 --> 00:01:00.660
of Apple's GPUs,

00:01:00.660 --> 00:01:03.630
and collectively, these apps run a diverse set

00:01:03.630 --> 00:01:06.360
of Metal Shading Language programs.

00:01:06.360 --> 00:01:10.113
These shader programs can range from small, simple shaders

00:01:10.113 --> 00:01:13.410
that execute just a handful of lines of code

00:01:13.410 --> 00:01:15.540
to large complex shaders

00:01:15.540 --> 00:01:18.420
that spend hundreds of thousands of lines of code,

00:01:18.420 --> 00:01:20.640
frameworks, and libraries.

00:01:20.640 --> 00:01:23.100
What all these shaders share in common

00:01:23.100 --> 00:01:26.220
is massive data parallelism,

00:01:26.220 --> 00:01:27.420
which is the opportunity

00:01:27.420 --> 00:01:29.610
to greatly improve the performance of an app

00:01:29.610 --> 00:01:31.680
by running it in parallel.

00:01:31.680 --> 00:01:33.510
This parallelism is achieved

00:01:33.510 --> 00:01:37.470
by running Metal shader programs many times over in parallel

00:01:37.470 --> 00:01:38.940
on different inputs

00:01:38.940 --> 00:01:42.330
such as each vertex in a 3D rendered scene

00:01:42.330 --> 00:01:44.493
or each pixel of the screen.

00:01:46.470 --> 00:01:48.870
It is the GPU, ultimately,

00:01:48.870 --> 00:01:51.270
which is responsible for executing these shaders

00:01:51.270 --> 00:01:52.233
in parallel.

00:01:54.150 --> 00:01:57.780
At the heart of every GPU are its shader cores.

00:01:57.780 --> 00:02:01.140
Each shader core can run thousands of threads in parallel.

00:02:01.140 --> 00:02:03.300
And to scale performance even further,

00:02:03.300 --> 00:02:05.520
a GPU will have many shader cores

00:02:05.520 --> 00:02:07.230
that can also run in parallel,

00:02:07.230 --> 00:02:09.420
giving an app tens of thousands

00:02:09.420 --> 00:02:11.373
of parallel threads of execution.

00:02:12.990 --> 00:02:16.920
Already, the GPUs in today's existing iPhones, iPads,

00:02:16.920 --> 00:02:19.383
and Macs, have incredible performance.

00:02:20.490 --> 00:02:23.190
As well as a powerful suite of developer tools

00:02:23.190 --> 00:02:26.793
to allow app developers to maximize the GPU's potential.

00:02:27.630 --> 00:02:30.450
But with the new Apple family 9 GPUs,

00:02:30.450 --> 00:02:33.660
we are increasing performance to unprecedented levels

00:02:33.660 --> 00:02:36.363
thanks to several new exciting advancements.

00:02:37.470 --> 00:02:41.040
First is a brand new shader core architecture

00:02:41.040 --> 00:02:43.800
that improves the performance and power efficiency

00:02:43.800 --> 00:02:45.150
of your existing apps,

00:02:45.150 --> 00:02:48.870
which, right away, benefits the experience you deliver

00:02:48.870 --> 00:02:51.180
while also meeting the demanding challenges

00:02:51.180 --> 00:02:54.693
of the next-generation of apps that you will build.

00:02:55.980 --> 00:03:00.240
Hardware-accelerated ray tracing transparently benefits apps

00:03:00.240 --> 00:03:03.390
that already use Metal's ray tracing APIs,

00:03:03.390 --> 00:03:06.750
as well as expands the opportunities to use ray tracing

00:03:06.750 --> 00:03:10.233
to achieve rich rendering effects with great performance.

00:03:11.460 --> 00:03:13.920
And with hardware-accelerated mesh shading,

00:03:13.920 --> 00:03:16.650
apps can build advanced geometry processing pipelines

00:03:16.650 --> 00:03:18.063
like never before.

00:03:19.470 --> 00:03:21.870
Before we discuss these in more detail,

00:03:21.870 --> 00:03:24.180
let's look at the incredible performance

00:03:24.180 --> 00:03:26.280
apps are already able to achieve

00:03:26.280 --> 00:03:30.033
with no changes on the new Apple family 9 GPUs.

00:03:30.870 --> 00:03:34.290
This is "Baldur's Gate 3" by Larian Studios,

00:03:34.290 --> 00:03:38.460
running on the new MacBook Pro with M3 Macs pictured on top,

00:03:38.460 --> 00:03:41.850
and a MacBook Pro with M2 Macs pictured on bottom.

00:03:41.850 --> 00:03:46.047
Each rendering with ultra video quality settings at 1800p.

00:03:46.047 --> 00:03:48.360
The M3 Macs is able to deliver

00:03:48.360 --> 00:03:50.430
significant performance improvements,

00:03:50.430 --> 00:03:53.160
thanks to the next-generation shader core's ability

00:03:53.160 --> 00:03:54.900
to run the game's Metal shaders

00:03:54.900 --> 00:03:56.823
with higher thread occupancy.

00:03:57.780 --> 00:04:01.050
Here is Blender rendering an image of a barbershop scene

00:04:01.050 --> 00:04:03.000
using the Cycles Path Tracer,

00:04:03.000 --> 00:04:06.600
which leverages Metal Ray Tracing on M3 Macs.

00:04:06.600 --> 00:04:09.720
Both Renders were started at the same exact time,

00:04:09.720 --> 00:04:12.330
but thanks to hardware-accelerated ray tracing

00:04:12.330 --> 00:04:14.640
and the next-generation shader core,

00:04:14.640 --> 00:04:18.603
the Render on the M3 Macs converges significantly faster.

00:04:20.940 --> 00:04:23.430
This is a real-time visualization

00:04:23.430 --> 00:04:26.400
of the "Toy Story 4" Antiques Mall USD

00:04:26.400 --> 00:04:29.070
rendered by Pixar's Hydra Storm.

00:04:29.070 --> 00:04:32.460
Hydra Storm uses Metal mesh shading on M3 Macs,

00:04:32.460 --> 00:04:36.150
which when combined with hardware-accelerated mesh shading

00:04:36.150 --> 00:04:38.193
runs faster than ever before.

00:04:39.210 --> 00:04:42.180
Let's now look at each of these features in more detail,

00:04:42.180 --> 00:04:45.573
starting with the next generation shader core.

00:04:46.980 --> 00:04:49.440
The Apple family 9 GPUs are composed

00:04:49.440 --> 00:04:51.420
of several building blocks

00:04:51.420 --> 00:04:54.120
such as compute and vertex command processors

00:04:54.120 --> 00:04:56.020
that parse your Metal command buffers,

00:04:56.910 --> 00:05:00.873
a rasterizer that dispatches fragment shaders for execution,

00:05:02.430 --> 00:05:04.200
and a hierarchy of caches,

00:05:04.200 --> 00:05:06.510
including the GPU last level cache

00:05:06.510 --> 00:05:08.763
that services all GPU memory traffic.

00:05:10.170 --> 00:05:14.040
But central to any GPU are its shader cores.

00:05:14.040 --> 00:05:15.390
These are the building blocks

00:05:15.390 --> 00:05:17.973
that execute your app's Metal shaders.

00:05:19.800 --> 00:05:22.650
The shader core is also paired with a texture unit

00:05:22.650 --> 00:05:25.320
that can sample and write your texture resources,

00:05:25.320 --> 00:05:27.690
as well as a brand new ray tracing unit

00:05:27.690 --> 00:05:30.393
that accelerates ray intersection requests.

00:05:33.000 --> 00:05:35.250
A shader core can be further subdivided

00:05:35.250 --> 00:05:36.873
into its constituent parts.

00:05:38.190 --> 00:05:41.580
Each shader core has an array of execution pipelines

00:05:41.580 --> 00:05:43.800
that execute different types of instructions,

00:05:43.800 --> 00:05:47.910
such as FP32, FP16, and Integer math,

00:05:47.910 --> 00:05:51.120
which correspond to operations on variables in your shaders

00:05:51.120 --> 00:05:54.390
with Metal data types such as float, half, and int,

00:05:54.390 --> 00:05:57.600
as well as memory pipelines for read and write operations

00:05:57.600 --> 00:05:59.283
to textures and buffers.

00:06:00.510 --> 00:06:02.940
Keeping all of these execution pipelines busy

00:06:02.940 --> 00:06:05.370
usually requires executing instructions

00:06:05.370 --> 00:06:07.230
from multiple SIMDgroups.

00:06:07.230 --> 00:06:10.020
So there's a pool that keeps track of the SIMDgroups

00:06:10.020 --> 00:06:12.180
that are running on a shader core,

00:06:12.180 --> 00:06:14.760
and a scheduler that chooses which SIMDgroup

00:06:14.760 --> 00:06:16.743
to execute instructions from next.

00:06:17.910 --> 00:06:20.880
Typically, there's also a handful of on-chip memories

00:06:20.880 --> 00:06:22.530
for storing the different types of data

00:06:22.530 --> 00:06:24.210
a shader program may use,

00:06:24.210 --> 00:06:27.243
such as registers for storing the values of variables,

00:06:28.080 --> 00:06:29.820
threadgroup and tile memory

00:06:29.820 --> 00:06:32.730
for storing data shared across a compute threadgroup

00:06:32.730 --> 00:06:35.313
or color attachment data shared across the tile,

00:06:37.200 --> 00:06:39.990
and a cache to improve the performance of accesses

00:06:39.990 --> 00:06:41.883
to the stack and to buffers.

00:06:43.260 --> 00:06:45.900
With this understanding of what a shader core does

00:06:45.900 --> 00:06:47.730
and its constituent pieces,

00:06:47.730 --> 00:06:50.970
I'd like to explain three new exciting advancements

00:06:50.970 --> 00:06:53.850
in the Apple family 9 GPU shader core.

00:06:53.850 --> 00:06:56.220
These advancements will increase the performance

00:06:56.220 --> 00:06:59.100
of your shaders with no changes to your app.

00:06:59.100 --> 00:07:02.910
But by better understanding how this new shader core works,

00:07:02.910 --> 00:07:06.363
you'll be able to benefit from it to an even greater degree.

00:07:08.820 --> 00:07:12.060
The first change is dynamic shader core memory,

00:07:12.060 --> 00:07:15.000
which allows an app to achieve better thread occupancy,

00:07:15.000 --> 00:07:17.493
and as a result, often better performance.

00:07:18.900 --> 00:07:22.710
The second change is the flexible on-chip memory.

00:07:22.710 --> 00:07:24.390
This will increase the efficiency

00:07:24.390 --> 00:07:27.540
by which your shaders access buffer, stack, threadgroup,

00:07:27.540 --> 00:07:28.713
and tile memory.

00:07:30.540 --> 00:07:32.610
The last change is the shader core's

00:07:32.610 --> 00:07:35.160
high-performance ALU pipelines,

00:07:35.160 --> 00:07:38.700
which have increased their ability to execute in parallel.

00:07:38.700 --> 00:07:40.620
This will improve the performance of apps

00:07:40.620 --> 00:07:42.780
that perform a combination of floating point

00:07:42.780 --> 00:07:43.893
or integer math.

00:07:45.420 --> 00:07:47.910
Before further exploring these new features,

00:07:47.910 --> 00:07:49.530
let's dive into more detail

00:07:49.530 --> 00:07:52.920
about how a shader core keeps its execution pipelines busy

00:07:52.920 --> 00:07:56.193
and the importance of thread occupancy in this endeavor.

00:07:57.990 --> 00:07:59.460
Suppose your Metal shader,

00:07:59.460 --> 00:08:01.410
after executing some math operations

00:08:01.410 --> 00:08:04.320
using the ALU pipelines, reads a buffer

00:08:04.320 --> 00:08:07.320
whose result will be used immediately after.

00:08:07.320 --> 00:08:08.730
Accessing the buffer

00:08:08.730 --> 00:08:11.730
may require going all the way to device memory,

00:08:11.730 --> 00:08:14.430
which is a long latency operation.

00:08:14.430 --> 00:08:15.660
During this time,

00:08:15.660 --> 00:08:18.690
the SIMDgroup can't execute other operations,

00:08:18.690 --> 00:08:21.723
which causes the ALU pipelines to go unused.

00:08:22.890 --> 00:08:25.890
To mitigate this, the shader core can execute instructions

00:08:25.890 --> 00:08:27.480
from a different SIMDgroup,

00:08:27.480 --> 00:08:29.943
which may have some ALU instructions of its own.

00:08:30.780 --> 00:08:33.780
This reduces the amount of time the ALUs go and used

00:08:33.780 --> 00:08:36.240
and allows the SIMDgroups to run in parallel,

00:08:36.240 --> 00:08:38.193
thus improving performance.

00:08:40.260 --> 00:08:42.180
If there are additional SIMDgroups

00:08:42.180 --> 00:08:43.350
running on the shader core,

00:08:43.350 --> 00:08:45.240
this can be done many times over

00:08:45.240 --> 00:08:48.000
until the ALUs and other execution pipelines

00:08:48.000 --> 00:08:50.493
are never starved of instructions to execute.

00:08:51.540 --> 00:08:52.800
The number of SIMDgroups

00:08:52.800 --> 00:08:55.170
that are concurrently running on a shader core

00:08:55.170 --> 00:08:57.453
is called its thread occupancy.

00:08:58.500 --> 00:08:59.940
But you may be asking yourself,

00:08:59.940 --> 00:09:01.980
what dictates how many SIMDgroups

00:09:01.980 --> 00:09:04.443
will be running concurrently on a shader core?

00:09:05.850 --> 00:09:09.150
To answer that question, let's look at an example.

00:09:09.150 --> 00:09:11.800
This is a prototypical ray tracing compute kernel

00:09:12.690 --> 00:09:15.513
that intersects a ray with an acceleration structure,

00:09:17.640 --> 00:09:19.623
inspects the intersection result,

00:09:21.660 --> 00:09:24.120
and then executes a different shading function

00:09:24.120 --> 00:09:26.850
based on the material of the primitive intersected.

00:09:26.850 --> 00:09:27.930
In this example,

00:09:27.930 --> 00:09:30.813
it supports shading both glass and leather materials.

00:09:33.300 --> 00:09:35.700
Each line of code will use some amount of registers

00:09:35.700 --> 00:09:37.740
to store the program's variables.

00:09:37.740 --> 00:09:39.300
At different points of the program,

00:09:39.300 --> 00:09:41.820
more or fewer registers will be used

00:09:41.820 --> 00:09:43.420
depending on what the code does.

00:09:44.400 --> 00:09:46.080
In this particular example,

00:09:46.080 --> 00:09:48.510
the implementation of the shadeGlass function

00:09:48.510 --> 00:09:51.393
uses many more registers than the rest of the program.

00:09:53.730 --> 00:09:56.083
Prior to the Apple family 9 GPU,

00:09:56.083 --> 00:09:59.250
a SIMDgroup could not begin execution on a shader core

00:09:59.250 --> 00:10:03.450
until it allocated registers from the on-chip register file.

00:10:03.450 --> 00:10:05.010
The amount allocated would be equal

00:10:05.010 --> 00:10:08.943
to the maximum register usage at any point in the program.

00:10:09.870 --> 00:10:12.600
The SIMDgroup would keep that many registers allocated

00:10:12.600 --> 00:10:15.030
for the entire duration of the SIMDgroup,

00:10:15.030 --> 00:10:18.030
even though most of those registers may go unused

00:10:18.030 --> 00:10:19.953
in large sections of the program.

00:10:20.850 --> 00:10:23.820
Thus, based on the maximum register usage,

00:10:23.820 --> 00:10:26.100
we may only be able to run, for example,

00:10:26.100 --> 00:10:28.440
four SIMDgroups at a time on a shader core

00:10:28.440 --> 00:10:29.850
because any more would require

00:10:29.850 --> 00:10:33.570
more on-chip register filed memory than exists.

00:10:33.570 --> 00:10:36.900
However, thanks to the Apple family 9 GPU's

00:10:36.900 --> 00:10:39.750
new dynamic shader core memory feature,

00:10:39.750 --> 00:10:42.960
the maximum register usage no longer dictates

00:10:42.960 --> 00:10:45.450
how many SIMDgroups can be run.

00:10:45.450 --> 00:10:48.660
On-chip register memory is now dynamically allocated

00:10:48.660 --> 00:10:51.810
and deallocated over the lifetime of the shader

00:10:51.810 --> 00:10:54.963
according to what each part of the program actually uses.

00:10:55.860 --> 00:10:59.250
This allows SIMDgroups to make much more efficient use

00:10:59.250 --> 00:11:02.370
of the on-chip register file, freeing up space

00:11:02.370 --> 00:11:05.040
that would not have been available otherwise.

00:11:05.040 --> 00:11:06.900
This can have a profound impact

00:11:06.900 --> 00:11:08.760
on your app's thread occupancy,

00:11:08.760 --> 00:11:11.220
and ultimately, its performance

00:11:11.220 --> 00:11:13.893
by allowing many more SIMDgroups to run concurrently.

00:11:15.840 --> 00:11:17.220
As I just mentioned,

00:11:17.220 --> 00:11:19.677
registers are now dynamically allocated and deallocated

00:11:19.677 --> 00:11:22.980
over the course of a SIMDgroup's lifetime.

00:11:22.980 --> 00:11:25.080
This is in part possible

00:11:25.080 --> 00:11:27.780
because the register file is now a cache

00:11:27.780 --> 00:11:30.480
instead of the permanent storage for the registers,

00:11:30.480 --> 00:11:32.520
meaning more registers can be used

00:11:32.520 --> 00:11:33.870
that can be stored on chip.

00:11:35.040 --> 00:11:38.610
The flexible on-chip memory feature extends this treatment

00:11:38.610 --> 00:11:40.980
to the rest of the shader core's memory types,

00:11:40.980 --> 00:11:43.530
such as threadgroup and tile memory,

00:11:43.530 --> 00:11:45.663
making that a cache too.

00:11:46.680 --> 00:11:51.060
And now that register, threadgroup, tile, stack,

00:11:51.060 --> 00:11:53.730
and buffer data are all cached on chip,

00:11:53.730 --> 00:11:56.970
this has allowed us to redesign the on-chip memories

00:11:56.970 --> 00:11:59.100
into fewer larger caches

00:11:59.100 --> 00:12:01.023
that service all these memory types.

00:12:01.920 --> 00:12:03.960
This flexibility will benefit shaders

00:12:03.960 --> 00:12:07.080
that don't make heavy use of each memory type.

00:12:07.080 --> 00:12:10.800
In the past, if a compute kernel didn't use, for example,

00:12:10.800 --> 00:12:13.920
threadgroup memory, its corresponding on-chip storage

00:12:13.920 --> 00:12:15.783
would go completely unused.

00:12:16.620 --> 00:12:19.680
Now, the on-chip storage will be dynamically assigned

00:12:19.680 --> 00:12:22.620
to the memory types that are used by your shaders,

00:12:22.620 --> 00:12:25.500
giving them more on-chip storage than they had in the past,

00:12:25.500 --> 00:12:27.753
and ultimately, better performance.

00:12:29.220 --> 00:12:32.430
For example, for shaders with heavy register usage,

00:12:32.430 --> 00:12:34.413
that may mean higher occupancy.

00:12:35.670 --> 00:12:39.150
For shaders that repeatedly access a large working set

00:12:39.150 --> 00:12:42.960
of buffer data, that will mean better cache hit rates,

00:12:42.960 --> 00:12:47.550
lower buffer access latency, and thus, better performance.

00:12:47.550 --> 00:12:50.760
And for apps that make heavy use of non-inline functions,

00:12:50.760 --> 00:12:54.000
such as function pointers, visible function tables,

00:12:54.000 --> 00:12:56.640
and dynamically linked shader libraries,

00:12:56.640 --> 00:12:58.650
this means more on-chip stack space

00:12:58.650 --> 00:13:00.360
to pass function parameters,

00:13:00.360 --> 00:13:02.553
and thus, faster function calls.

00:13:04.740 --> 00:13:07.680
But what happens if your app still uses more memory

00:13:07.680 --> 00:13:09.543
than there is on-chip storage for?

00:13:10.380 --> 00:13:13.950
Unmitigated, that data will spill to the next cache level

00:13:13.950 --> 00:13:16.320
or even to main memory.

00:13:16.320 --> 00:13:19.320
Fortunately, the shader core will dynamically monitor

00:13:19.320 --> 00:13:22.470
your shaker's behavior and adjust the occupancy level

00:13:22.470 --> 00:13:24.570
to prevent this from occurring.

00:13:24.570 --> 00:13:26.520
This keeps data on chip.

00:13:26.520 --> 00:13:29.943
And ultimately, the execution pipelines busy.

00:13:30.990 --> 00:13:32.640
This does mean, however,

00:13:32.640 --> 00:13:35.190
that your shader's occupancy will be impacted

00:13:35.190 --> 00:13:37.428
by how your shader's access threadgroup, tile,

00:13:37.428 --> 00:13:39.480
stack, and buffer memory

00:13:39.480 --> 00:13:42.153
in addition to its dynamic register usage.

00:13:43.650 --> 00:13:45.420
These new hardware capabilities

00:13:45.420 --> 00:13:47.880
improve the occupancy of many apps,

00:13:47.880 --> 00:13:51.210
meaning you, the developer, need to optimize occupancy

00:13:51.210 --> 00:13:53.760
a lot less often than in the past.

00:13:53.760 --> 00:13:57.270
But if you do need to optimize occupancy further

00:13:57.270 --> 00:13:59.310
on Apple family 9 GPUs,

00:13:59.310 --> 00:14:02.340
we have developed a suite of profiling tools to help you.

00:14:02.340 --> 00:14:06.480
To learn more about how to diagnose and optimize occupancy,

00:14:06.480 --> 00:14:08.313
please refer to these talks.

00:14:10.530 --> 00:14:13.680
The last feature of the Apple family 9 GPU shader core

00:14:13.680 --> 00:14:18.363
I'd like to discuss is its high-performance ALU pipelines.

00:14:19.530 --> 00:14:22.650
Apple GPU shader cores have separate ALU pipelines

00:14:22.650 --> 00:14:24.270
for different instruction types,

00:14:24.270 --> 00:14:26.913
including FP16 instructions.

00:14:27.840 --> 00:14:32.840
Apple GPUs are highly optimized to execute FP16 arithmetic.

00:14:32.850 --> 00:14:36.090
And we recommend that you'll use FP16 data types

00:14:36.090 --> 00:14:37.383
wherever possible.

00:14:38.910 --> 00:14:42.153
FP16 math instructions execute at peak throughput.

00:14:43.470 --> 00:14:48.120
They use fewer registers than their FP32 equivalents.

00:14:48.120 --> 00:14:49.590
They reduce memory bandwidth

00:14:49.590 --> 00:14:53.160
if your buffers store data natively in FP16.

00:14:53.160 --> 00:14:55.950
And for situations where the source

00:14:55.950 --> 00:14:58.500
or destination variable of a math operation

00:14:58.500 --> 00:15:00.780
is not FP16 already,

00:15:00.780 --> 00:15:03.843
it can be converted to and from at no cost.

00:15:04.980 --> 00:15:08.130
But if your app still performs other math operations,

00:15:08.130 --> 00:15:10.590
such as FP32 and integer,

00:15:10.590 --> 00:15:13.950
the Apple family 9 GPU shader core can execute instructions

00:15:13.950 --> 00:15:16.410
from all three data types in parallel

00:15:16.410 --> 00:15:18.603
to a greater degree than ever before.

00:15:19.590 --> 00:15:22.620
This can deliver up to 2x ALU performance

00:15:22.620 --> 00:15:24.993
compared to prior Apple GPUs.

00:15:25.980 --> 00:15:28.950
In order to take advantage of this extra parallelism,

00:15:28.950 --> 00:15:31.950
instructions must be executed from multiple SIMDgroups,

00:15:31.950 --> 00:15:34.260
which means increasing occupancy

00:15:34.260 --> 00:15:38.400
can improve the utilization of the ALU pipelines.

00:15:38.400 --> 00:15:40.650
Let's consider an example.

00:15:40.650 --> 00:15:43.710
Imagine there are two SIMDgroups running concurrently,

00:15:43.710 --> 00:15:46.950
both executing ALU instructions.

00:15:46.950 --> 00:15:49.800
In the past, these SIMDgroups may have had to run

00:15:49.800 --> 00:15:50.883
one after another.

00:15:52.350 --> 00:15:56.550
But if they have FP32 and FP16 instructions

00:15:56.550 --> 00:16:00.660
to execute at different points in time, as depicted here,

00:16:00.660 --> 00:16:03.120
then their executions can be overlapped,

00:16:03.120 --> 00:16:06.003
increasing parallelism and performance.

00:16:07.770 --> 00:16:12.000
To recap what's new in the next-generation shader core,

00:16:12.000 --> 00:16:15.180
it will dynamically allocate and deallocate registers

00:16:15.180 --> 00:16:16.740
over the lifetime of a shader,

00:16:16.740 --> 00:16:19.173
which improves its thread occupancy.

00:16:20.550 --> 00:16:22.470
It has a large on-chip cache

00:16:22.470 --> 00:16:26.550
that services registers, threadgroup, tile, stack,

00:16:26.550 --> 00:16:29.160
and buffer memory, which improves the performance

00:16:29.160 --> 00:16:30.963
of accessing those memory types.

00:16:33.150 --> 00:16:35.940
The shader core will dynamically adjust occupancy

00:16:35.940 --> 00:16:39.153
to keep data on chip and the execution pipelines busy.

00:16:40.200 --> 00:16:45.200
And finally, FP16, FP32 and integer operations

00:16:45.330 --> 00:16:47.490
can execute in parallel more than ever,

00:16:47.490 --> 00:16:49.473
increasing ALU performance.

00:16:52.560 --> 00:16:56.793
Next, let's take a look at hardware-accelerated ray tracing.

00:16:59.130 --> 00:17:00.780
With Metal ray tracing,

00:17:00.780 --> 00:17:04.350
apps can leverage the massive parallelism of Apple GPUs

00:17:04.350 --> 00:17:07.860
to intersect rays with their scene geometry.

00:17:07.860 --> 00:17:09.930
If you're not familiar with Metal ray tracing

00:17:09.930 --> 00:17:11.640
and would like to learn more,

00:17:11.640 --> 00:17:14.940
please watch Your guide to Metal ray tracing

00:17:14.940 --> 00:17:17.943
and Enhance your app with Metal ray tracing.

00:17:20.580 --> 00:17:23.250
At the heart of the Metal ray chasing API

00:17:23.250 --> 00:17:24.810
is the intersector object

00:17:24.810 --> 00:17:26.610
that is responsible for determining

00:17:26.610 --> 00:17:28.380
the intersection point of a ray

00:17:28.380 --> 00:17:31.473
with the primitives contained in an acceleration structure.

00:17:32.430 --> 00:17:34.890
It is often invoked many times over

00:17:34.890 --> 00:17:38.370
by ray tracing app's GPU functions, also known as shaders,

00:17:38.370 --> 00:17:41.223
and thus, is central to the app's performance.

00:17:42.930 --> 00:17:45.450
Earlier, I showed such AGPU function

00:17:45.450 --> 00:17:47.100
when I looked at the register usage

00:17:47.100 --> 00:17:48.723
of this raytracingKernel.

00:17:49.710 --> 00:17:52.380
It creates an intersector object

00:17:52.380 --> 00:17:54.210
and finds an intersection

00:17:54.210 --> 00:17:57.363
by calling the object's intersect method.

00:17:59.640 --> 00:18:01.620
To determine the intersection point,

00:18:01.620 --> 00:18:04.563
the intersector performs a few key stages.

00:18:05.400 --> 00:18:08.220
First, it traverses the acceleration structure

00:18:08.220 --> 00:18:09.903
to find a candidate primitive.

00:18:10.800 --> 00:18:13.230
It then invokes an intersection function,

00:18:13.230 --> 00:18:15.150
which may be provided by the app,

00:18:15.150 --> 00:18:17.650
to determine if the rate intersects the primitive.

00:18:19.140 --> 00:18:20.070
If it does,

00:18:20.070 --> 00:18:23.160
the intersection is compared to previous intersections

00:18:23.160 --> 00:18:26.373
and the process is repeated until the closest is found.

00:18:27.600 --> 00:18:29.520
The closest intersection is then returned

00:18:29.520 --> 00:18:31.080
to the calling GPU function

00:18:31.080 --> 00:18:33.273
for further app-specific processing.

00:18:35.340 --> 00:18:38.070
New in Apple family 9 GPUs,

00:18:38.070 --> 00:18:40.470
the implementation of the intersector object

00:18:40.470 --> 00:18:42.480
is hardware-accelerated,

00:18:42.480 --> 00:18:44.580
which greatly increases the performance

00:18:44.580 --> 00:18:46.383
of this critical operation.

00:18:48.180 --> 00:18:50.220
The hardware-accelerated intersection

00:18:50.220 --> 00:18:53.250
does not execute in line with the GPU function.

00:18:53.250 --> 00:18:56.160
Thus, to facilitate the communication of the ray

00:18:56.160 --> 00:18:58.560
and the ray payload between the two,

00:18:58.560 --> 00:19:01.200
data is read and written to on-chip memory,

00:19:01.200 --> 00:19:02.280
which you can observe

00:19:02.280 --> 00:19:05.643
using the RT scratch performance counters in the new Xcode.

00:19:06.810 --> 00:19:08.280
Now that I've discussed the role

00:19:08.280 --> 00:19:10.830
and responsibilities of the intersector,

00:19:10.830 --> 00:19:13.200
let's dissect the performance characteristics

00:19:13.200 --> 00:19:16.953
of onetime through this intersector loop using an example.

00:19:18.660 --> 00:19:21.360
Imagine our app is executing two SIMDgroups

00:19:21.360 --> 00:19:23.550
that each wish to intersect four rays

00:19:23.550 --> 00:19:25.100
with an acceleration structure.

00:19:27.390 --> 00:19:28.530
In this example,

00:19:28.530 --> 00:19:30.180
our acceleration structure contains

00:19:30.180 --> 00:19:32.100
the classic kernel boxing,

00:19:32.100 --> 00:19:34.743
with one box object and one sphere object.

00:19:36.120 --> 00:19:37.740
The rays are cast into the scene

00:19:37.740 --> 00:19:39.540
by calling the intersect method,

00:19:39.540 --> 00:19:42.480
passing it the ray, the acceleration structure,

00:19:42.480 --> 00:19:45.060
and the intersection function table.

00:19:45.060 --> 00:19:48.810
Each SIMDgroup has two rays that intersect the box

00:19:48.810 --> 00:19:51.510
and two the intersect the sphere.

00:19:51.510 --> 00:19:52.740
In this example,

00:19:52.740 --> 00:19:56.070
the box is defined as opaque triangle primitives

00:19:56.070 --> 00:19:57.920
by using the MTLAccelerationStructure

00:19:59.037 --> 00:20:01.140
TriangleGeometryDescriptor

00:20:01.140 --> 00:20:03.960
and setting its opaque property to yes,

00:20:03.960 --> 00:20:06.930
thus, the intersection can compute the intersections

00:20:06.930 --> 00:20:09.363
using Metal's built-in intersection function.

00:20:11.400 --> 00:20:13.920
However, the sphere is defined procedurally

00:20:13.920 --> 00:20:16.770
using a custom bounding box intersection function

00:20:16.770 --> 00:20:18.573
that the intersection must invoke.

00:20:20.130 --> 00:20:23.130
The custom BoundingBoxIntersection function is declared

00:20:23.130 --> 00:20:24.960
using the intersection attribute

00:20:24.960 --> 00:20:26.733
with the bounding_box parameter.

00:20:28.560 --> 00:20:30.030
As I mentioned before,

00:20:30.030 --> 00:20:32.610
the intersect method is called by each thread

00:20:32.610 --> 00:20:35.730
that is testing a ray against the acceleration structure.

00:20:35.730 --> 00:20:37.920
So with this example in mind,

00:20:37.920 --> 00:20:41.070
let's look at how each intersect calls traversal

00:20:41.070 --> 00:20:43.410
and intersection test are executed

00:20:43.410 --> 00:20:45.603
in a traditional implementation.

00:20:47.310 --> 00:20:48.480
In typical usage,

00:20:48.480 --> 00:20:50.790
not all traversals will take the same amount of time

00:20:50.790 --> 00:20:54.150
to locate a primitive to test the ray against.

00:20:54.150 --> 00:20:57.060
This creates what is called execution divergence,

00:20:57.060 --> 00:20:59.070
which causes each thread in a SIMDgroup

00:20:59.070 --> 00:21:01.860
to wait for the longest traversal from that SIMDgroup

00:21:01.860 --> 00:21:03.903
before proceeding to the next stage.

00:21:05.640 --> 00:21:06.780
And as it turns out,

00:21:06.780 --> 00:21:08.400
the same overhead compounds

00:21:08.400 --> 00:21:10.773
when executing the intersection functions too.

00:21:11.700 --> 00:21:14.010
Execution divergence causes each type

00:21:14.010 --> 00:21:16.860
of intersection function to run one after another,

00:21:16.860 --> 00:21:18.933
further reducing parallelism.

00:21:19.770 --> 00:21:22.350
An aggregate across both stages,

00:21:22.350 --> 00:21:25.800
each thread spends a large proportion of its runtime idle,

00:21:25.800 --> 00:21:28.800
waiting on the other threads in the SIMDgroup to complete,

00:21:28.800 --> 00:21:30.903
which is a major performance bottleneck.

00:21:33.000 --> 00:21:36.390
With that picture of a traditional implementation in mind,

00:21:36.390 --> 00:21:39.360
let's discuss how hardware-accelerated ray tracing

00:21:39.360 --> 00:21:41.493
optimizes those inefficiencies.

00:21:43.320 --> 00:21:44.880
The first major improvement

00:21:44.880 --> 00:21:46.440
is that the hardware intersector

00:21:46.440 --> 00:21:50.130
is able to run each traversal completely independently

00:21:50.130 --> 00:21:52.590
using fixed function hardware.

00:21:52.590 --> 00:21:54.480
This is possible in part

00:21:54.480 --> 00:21:56.730
because the arrays are sent to the hardware intersector

00:21:56.730 --> 00:21:57.840
for processing

00:21:57.840 --> 00:22:00.963
instead of executing in line with the GPU function.

00:22:01.920 --> 00:22:05.250
This greatly decreases the time spent traversing

00:22:05.250 --> 00:22:06.750
and also removes the overhead

00:22:06.750 --> 00:22:10.203
of the traditional traversal's execution divergence.

00:22:13.634 --> 00:22:15.660
On the other hand, the intersection functions

00:22:15.660 --> 00:22:17.490
are Metal shading language code,

00:22:17.490 --> 00:22:19.830
so they still must be grouped into SIMDgroups

00:22:19.830 --> 00:22:21.633
to be run on the shader core.

00:22:22.470 --> 00:22:24.930
However, because the hardware intersector

00:22:24.930 --> 00:22:27.480
executes each ray independently,

00:22:27.480 --> 00:22:30.600
it is free to group together the intersection function calls

00:22:30.600 --> 00:22:33.603
from rays that originated from separate SIMDgroups.

00:22:35.940 --> 00:22:38.640
This is the role of the reorder stage.

00:22:38.640 --> 00:22:42.690
When rays reach this stage within close proximity and time,

00:22:42.690 --> 00:22:44.070
the intersection function calls

00:22:44.070 --> 00:22:46.650
will be grouped into coherent SIMDgroups,

00:22:46.650 --> 00:22:49.200
such that the execution divergence overhead

00:22:49.200 --> 00:22:52.710
present in the traditional implementation is reduced

00:22:52.710 --> 00:22:54.873
or even completely eliminated.

00:22:56.760 --> 00:22:57.930
So now that I've shown you

00:22:57.930 --> 00:23:00.240
how hardware-accelerated ray tracing

00:23:00.240 --> 00:23:01.350
improves the performance

00:23:01.350 --> 00:23:03.570
of your app's ray intersector calls,

00:23:03.570 --> 00:23:05.220
let's review some best practices

00:23:05.220 --> 00:23:08.673
that your apps can implement to maximize its benefits.

00:23:10.500 --> 00:23:14.430
Our first suggestion is to use the intersector object API

00:23:14.430 --> 00:23:15.960
whenever possible.

00:23:15.960 --> 00:23:17.850
Metal also allows ray tracing

00:23:17.850 --> 00:23:21.630
to be performed using the intersection query API,

00:23:21.630 --> 00:23:23.040
but this API increases

00:23:23.040 --> 00:23:24.870
the amount of ray trace scratch memory

00:23:24.870 --> 00:23:26.730
that must be read and written,

00:23:26.730 --> 00:23:29.133
as well as disables the reorder stage.

00:23:30.360 --> 00:23:31.950
We also recommend

00:23:31.950 --> 00:23:34.740
when authoring custom intersection functions

00:23:34.740 --> 00:23:37.440
to avoid creating one uber function

00:23:37.440 --> 00:23:38.970
that is capable of executing

00:23:38.970 --> 00:23:42.270
many different logical intersection routines.

00:23:42.270 --> 00:23:45.810
Instead, create one Metal intersection function

00:23:45.810 --> 00:23:48.870
for each logical intersection routine.

00:23:48.870 --> 00:23:51.993
This increases the benefits of the reorder stage.

00:23:53.430 --> 00:23:55.350
It is also important to try to minimize

00:23:55.350 --> 00:23:58.380
the size of the ray payload structure that has passed to

00:23:58.380 --> 00:24:01.500
and return from the intersector object.

00:24:01.500 --> 00:24:03.810
This will decrease your shader's latency

00:24:03.810 --> 00:24:06.723
and potentially increase its thread occupancy.

00:24:08.880 --> 00:24:10.290
For more details and guidance

00:24:10.290 --> 00:24:12.840
about how to optimize your ray tracing apps,

00:24:12.840 --> 00:24:14.493
please watch these talks.

00:24:16.380 --> 00:24:19.680
To recap, the Apple family 9 GPUs

00:24:19.680 --> 00:24:22.050
greatly improved the performance of ray tracing

00:24:22.050 --> 00:24:24.090
through new hardware acceleration

00:24:24.090 --> 00:24:26.610
that features fixed function traversal blocks

00:24:26.610 --> 00:24:29.133
and an intersection function reorder stage.

00:24:30.540 --> 00:24:31.980
And although this new hardware

00:24:31.980 --> 00:24:35.760
will improve the performance of all Metal ray tracing apps,

00:24:35.760 --> 00:24:38.940
to maximize the benefits your app derives from it,

00:24:38.940 --> 00:24:41.460
it's best to use the intersection API

00:24:41.460 --> 00:24:45.003
instead of the intersection query API whenever possible.

00:24:46.860 --> 00:24:49.860
The last advancement in the Apple family 9 GPUs

00:24:49.860 --> 00:24:51.360
that I'd like to talk to you about

00:24:51.360 --> 00:24:53.853
is hardware-accelerated mesh shading.

00:24:55.650 --> 00:24:57.150
Mesh shading is a flexible,

00:24:57.150 --> 00:24:59.610
GPU-driven geometry processing stage

00:24:59.610 --> 00:25:01.170
in the rendering pipeline

00:25:01.170 --> 00:25:04.050
that replaces the traditional vertex shader stage

00:25:04.050 --> 00:25:06.063
with two compute-like shaders.

00:25:07.260 --> 00:25:09.570
Object shaders execute in the first stage

00:25:09.570 --> 00:25:12.060
and can be used to perform coarse grain processing

00:25:12.060 --> 00:25:16.740
of app-specific inputs such as entire mesh objects.

00:25:16.740 --> 00:25:19.860
Each object threadgroup can choose to spawn a mesh group

00:25:19.860 --> 00:25:23.190
to perform subsequent finer grain processing.

00:25:23.190 --> 00:25:26.040
Mesh shaders comprise the second stage.

00:25:26.040 --> 00:25:28.320
Typically, a mesh threadgroup will process

00:25:28.320 --> 00:25:30.900
a constituent piece of the parent object,

00:25:30.900 --> 00:25:33.333
often referred to as a meshlet.

00:25:34.890 --> 00:25:36.660
The output of the mesh threadgroup

00:25:36.660 --> 00:25:39.540
is a Metal mesh object that encapsulates

00:25:39.540 --> 00:25:41.400
a list of vertices and primitives

00:25:41.400 --> 00:25:43.080
to be processed by the remainder

00:25:43.080 --> 00:25:45.063
of the traditional graphics pipeline.

00:25:46.620 --> 00:25:49.320
Mesh shading has numerous applications,

00:25:49.320 --> 00:25:52.023
such as fine-grained geometry calling,

00:25:53.100 --> 00:25:55.980
procedural geometry generation,

00:25:55.980 --> 00:25:59.520
custom app-specific geometry representations,

00:25:59.520 --> 00:26:01.800
such as compressed formats.

00:26:01.800 --> 00:26:04.230
And for porting geometry and tessellation shaders

00:26:04.230 --> 00:26:06.123
from other graphics APIs.

00:26:07.170 --> 00:26:09.720
If you're unfamiliar with mesh shading in Metal,

00:26:09.720 --> 00:26:12.270
I recommend that you check out the two talks below.

00:26:14.100 --> 00:26:16.230
With hardware-accelerated mesh shading

00:26:16.230 --> 00:26:18.450
on Apple family 9 GPUs,

00:26:18.450 --> 00:26:20.760
the most notable improvement you'll observe

00:26:20.760 --> 00:26:22.410
is much improved performance

00:26:22.410 --> 00:26:24.237
of your existing mesh shading code..

00:26:25.380 --> 00:26:26.760
Apple family 9 GPUs

00:26:26.760 --> 00:26:29.340
are able to much more efficiently schedule object

00:26:29.340 --> 00:26:30.780
and mesh threadgroups

00:26:30.780 --> 00:26:33.510
to keep intermediate meshlet data on chip.

00:26:33.510 --> 00:26:35.943
Thus, reducing memory traffic.

00:26:36.960 --> 00:26:37.950
With the new hardware

00:26:37.950 --> 00:26:41.580
also comes several Metal API enhancements.

00:26:41.580 --> 00:26:44.820
The first is support for encoding draw mesh commands

00:26:44.820 --> 00:26:47.460
into indirect command buffers.

00:26:47.460 --> 00:26:49.950
This allows GPU-driven rendering pipelines

00:26:49.950 --> 00:26:51.630
to make use of mesh shading

00:26:51.630 --> 00:26:54.243
in addition to traditional vertex shaders.

00:26:55.620 --> 00:26:57.270
The second API enhancement

00:26:57.270 --> 00:27:00.600
expands the maximum number of threadgroups per mesh grid

00:27:00.600 --> 00:27:03.303
from 1,024 to over 1 million.

00:27:04.560 --> 00:27:07.320
Let's now review a couple of best practices

00:27:07.320 --> 00:27:09.843
to ensure optimal mesh shading performance.

00:27:12.450 --> 00:27:15.360
The metal::mesh object output by a mesh threadgroup

00:27:15.360 --> 00:27:16.830
has several template parameters

00:27:16.830 --> 00:27:20.103
whose size are important to keep as small as possible.

00:27:21.960 --> 00:27:24.840
For the mesh's vertex and primitive data types,

00:27:24.840 --> 00:27:29.010
this can be done, for example, by removing unused attributes

00:27:29.010 --> 00:27:31.320
that may be present due to sharing those data types

00:27:31.320 --> 00:27:34.950
with other unrelated vertex or mesh functions.

00:27:34.950 --> 00:27:36.870
The mesh type must also specify

00:27:36.870 --> 00:27:39.750
the maximum number of primitives and vertices

00:27:39.750 --> 00:27:41.880
that may be output.

00:27:41.880 --> 00:27:43.830
These should not be set any larger

00:27:43.830 --> 00:27:46.740
than what your app's geometry, pipeline, and assets

00:27:46.740 --> 00:27:48.600
actually need.

00:27:48.600 --> 00:27:52.080
Being mindful of these sizes will reduce memory traffic

00:27:52.080 --> 00:27:54.243
and may increase occupancy.

00:27:55.260 --> 00:27:58.350
If performing per primitive calling in a mesh shader,

00:27:58.350 --> 00:28:00.780
we don't recommend writing vertex positions

00:28:00.780 --> 00:28:02.670
to the mesh object

00:28:02.670 --> 00:28:06.690
just to be called by the hardware subsequent calling stage.

00:28:06.690 --> 00:28:09.540
Instead, it is best to completely omit

00:28:09.540 --> 00:28:10.860
writing such primitives

00:28:10.860 --> 00:28:13.230
as that can save substantial processing time

00:28:13.230 --> 00:28:14.340
in the remainder

00:28:14.340 --> 00:28:16.863
of the hardware's geometry processing stages.

00:28:19.290 --> 00:28:21.690
All right, let's recap what I covered

00:28:21.690 --> 00:28:23.943
about the Apple family 9 GPUs.

00:28:25.620 --> 00:28:27.750
The next-generation shader core

00:28:27.750 --> 00:28:30.060
increases on-chip memory utilization

00:28:30.060 --> 00:28:32.880
for better thread occupancy and performance

00:28:32.880 --> 00:28:35.430
by dynamically allocating register storage

00:28:35.430 --> 00:28:39.273
and sharing on-chip memory across many memory types.

00:28:40.410 --> 00:28:42.510
Hardware-accelerated ray tracing

00:28:42.510 --> 00:28:44.580
greatly improves the performance of apps

00:28:44.580 --> 00:28:46.860
using the Metal ray tracing APIs,

00:28:46.860 --> 00:28:49.683
enabling new high-fidelity visual effects.

00:28:50.580 --> 00:28:54.120
And finally, mesh shading performance is greatly improved

00:28:54.120 --> 00:28:56.100
thanks to hardware acceleration,

00:28:56.100 --> 00:28:58.470
enabling more apps to customize

00:28:58.470 --> 00:29:00.543
their geometry processing pipeline.

00:29:02.250 --> 00:29:03.543
Thank you for watching.