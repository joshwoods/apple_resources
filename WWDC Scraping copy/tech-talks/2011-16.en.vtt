WEBVTT

00:00:22.910 --> 00:00:25.790
Hello, I'm Allan Schaffehr,
the Game Technologies Evangelist

00:00:25.860 --> 00:00:26.770
at Apple.

00:00:26.970 --> 00:00:31.890
Core Image is a new framework in iOS 5
that provides an easy way for your apps

00:00:31.890 --> 00:00:34.920
to apply effects to photos and videos.

00:00:34.990 --> 00:00:37.670
And it does this using a
number of built-in filters,

00:00:37.670 --> 00:00:41.280
such as for color transformations
and compositing operations,

00:00:41.370 --> 00:00:45.320
and also includes some advanced features,
such as face detection,

00:00:45.410 --> 00:00:48.040
red-eye reduction, and auto-enhance.

00:00:48.140 --> 00:00:51.880
So, in this video, I'll introduce the key
concepts of Core Image,

00:00:51.970 --> 00:00:55.540
and then dive deeper into its
three main classes on iOS:

00:00:55.690 --> 00:00:59.050
the CI Image, CI Filter, and CI Context.

00:00:59.160 --> 00:01:01.560
And then I'll wrap up with
those advanced features.

00:01:01.660 --> 00:01:03.160
So, let's get started.

00:01:04.220 --> 00:01:08.740
So let me begin with an explanation
of the basic concept of Core Image.

00:01:08.920 --> 00:01:11.230
The idea is that you start
with an image or a photo,

00:01:11.230 --> 00:01:13.860
like the one on the left here,
with the picture of the

00:01:13.880 --> 00:01:15.220
sailboat in a harbor.

00:01:15.410 --> 00:01:19.140
And you take that image,
and various image processing operations

00:01:19.170 --> 00:01:21.140
can be applied to each pixel.

00:01:21.140 --> 00:01:24.660
So in this case,
we're applying a sepia filter,

00:01:24.660 --> 00:01:28.940
which transforms the colors to
various shades of this brown color,

00:01:28.940 --> 00:01:31.610
and we get this photo on the right.

00:01:32.700 --> 00:01:35.800
But what's interesting is that
filters can be chained together

00:01:35.800 --> 00:01:38.310
to create more complex effects.

00:01:38.450 --> 00:01:41.600
So here we're starting with
that same original image and

00:01:41.600 --> 00:01:44.680
applying the same sepia filter,
which results in this

00:01:44.680 --> 00:01:47.170
brown-colored monochromatic image.

00:01:47.310 --> 00:01:50.840
But then we're running that
through a hue adjustment filter,

00:01:50.900 --> 00:01:55.600
which conceptually will rotate the
colors around on a color wheel.

00:01:55.690 --> 00:01:59.430
Then that result is being run
through a contrast filter,

00:01:59.430 --> 00:02:04.080
which boosts the contrast of the image,
and we get this result over on the right.

00:02:04.120 --> 00:02:08.620
And we have a lot of different
filters that you can choose from.

00:02:09.080 --> 00:02:13.840
But we don't actually create
all those intermediate images.

00:02:13.980 --> 00:02:18.610
Really, the chain of filters is treated
more like a recipe of operations

00:02:18.690 --> 00:02:22.760
that will be applied to the
pixels in the original image.

00:02:22.890 --> 00:02:27.000
And then the entire chain of
filters is concatenated together.

00:02:27.120 --> 00:02:31.590
This eliminates any need for
those intermediate buffers.

00:02:32.000 --> 00:02:37.000
And further, when that recipe is created,
there's a compiler that's used to further

00:02:37.000 --> 00:02:39.240
optimize the work that'll be done.

00:02:39.350 --> 00:02:42.240
In this case, for example,
I've replaced two of the filters

00:02:42.240 --> 00:02:45.940
with Color Matrix filters,
which can be combined

00:02:46.030 --> 00:02:50.140
together by the compiler,
and this further improves performance.

00:02:50.280 --> 00:02:54.860
So, that's the basic idea of what
Core Image is doing when you're

00:02:55.040 --> 00:02:57.560
applying filters to images.

00:02:57.790 --> 00:02:59.960
Now, Core Image is very flexible.

00:03:00.140 --> 00:03:04.000
It can take photos and videos
from the user's photo library.

00:03:04.100 --> 00:03:07.780
It can take live video being
captured from the camera.

00:03:07.880 --> 00:03:11.240
You can process images in
memory or images from files in

00:03:11.390 --> 00:03:13.520
a variety of different formats.

00:03:13.850 --> 00:03:16.380
The outputs are really flexible as well.

00:03:16.700 --> 00:03:20.210
Once you've run an image through
this so-called filter chain,

00:03:20.210 --> 00:03:23.950
it can be output as a
Core Graphics CG image ref,

00:03:24.250 --> 00:03:28.020
which itself is really versatile,
and can then be used as a UI image

00:03:28.020 --> 00:03:32.570
in the UI kit or go back out to the
photo library with AL Asset Library.

00:03:32.590 --> 00:03:37.960
Or, you can render images into a
CA Eagle layer with OpenGL ES.

00:03:38.100 --> 00:03:43.390
Or, third, you can render to a
CV Pixel Buffer ref for live video.

00:03:43.860 --> 00:03:45.920
Which can then be used by AV Foundation.

00:03:46.170 --> 00:03:51.240
And last, you can also have the output go
into memory as just raw pixel data.

00:03:51.480 --> 00:03:56.160
So, those are the inputs and the outputs,
and they're really flexible.

00:03:56.260 --> 00:03:59.320
But what about this part in the middle,
the filters?

00:04:00.900 --> 00:04:05.530
Well, so here's a list of all the
filters supported in iOS 5.

00:04:05.690 --> 00:04:08.240
It's a big list, 48 filters.

00:04:08.350 --> 00:04:11.120
And it's greatly expanded
since the original introduction

00:04:11.250 --> 00:04:15.360
of Core Image at WWDC 2011.

00:04:15.490 --> 00:04:18.150
So, what I want to do is to
go quickly through some of

00:04:18.160 --> 00:04:21.620
these filters by category,
just to help you understand

00:04:21.740 --> 00:04:23.160
what's being provided.

00:04:23.240 --> 00:04:26.850
And the first category are
the color adjustment filters.

00:04:27.050 --> 00:04:30.880
These are primarily various adjustments
that you can make on photos,

00:04:31.030 --> 00:04:35.790
similar to what you see in the color
adjustments panel in the iPhoto app.

00:04:35.890 --> 00:04:39.590
So, it's things like adjusting
the exposure in gamma.

00:04:39.690 --> 00:04:42.150
Color controls lets
you adjust saturation,

00:04:42.240 --> 00:04:44.000
brightness, and contrast.

00:04:44.050 --> 00:04:47.550
And over on the right,
you see white point, color temperature,

00:04:47.700 --> 00:04:48.600
and so on.

00:04:50.260 --> 00:04:52.340
This next group are the color effects.

00:04:52.480 --> 00:04:55.610
You've already seen sepia tone,
but the one that's really

00:04:55.610 --> 00:04:58.070
interesting here is Color Cube.

00:04:58.140 --> 00:05:02.090
So this lets you define a
three-dimensional color table

00:05:02.270 --> 00:05:04.230
where you're using the red,
green,

00:05:04.230 --> 00:05:09.680
and blue values of pixels in the source
image to look up positions in the 3D

00:05:09.680 --> 00:05:12.100
space and get a replacement color.

00:05:12.260 --> 00:05:16.590
This is really powerful and can be
used for a lot of different effects.

00:05:18.570 --> 00:05:20.560
Next are the compositing operations.

00:05:20.670 --> 00:05:24.390
This is almost half of all the filters,
and it's a big group that's

00:05:24.390 --> 00:05:26.770
been added since WWDC.

00:05:26.940 --> 00:05:28.530
These divide into two categories.

00:05:28.620 --> 00:05:31.540
The first category are
various blending operations,

00:05:31.600 --> 00:05:34.550
which blend the colors
from two source images,

00:05:34.570 --> 00:05:37.320
depending on which filter you're using.

00:05:37.420 --> 00:05:41.000
And the second category are
various compositing operations,

00:05:41.100 --> 00:05:45.340
which choose to take each pixel
from either of the two source images

00:05:45.760 --> 00:05:50.070
based on its color values and the
compositing technique you've chosen.

00:05:50.160 --> 00:05:52.810
So, as an example,
you might use these if you

00:05:52.810 --> 00:05:56.700
wanted to overlay a custom frame
around the edges of a photo.

00:05:58.370 --> 00:05:59.700
There's a few generators.

00:05:59.870 --> 00:06:02.890
These are primarily for testing
or just to provide a simple

00:06:02.890 --> 00:06:04.600
input to some other filter.

00:06:04.760 --> 00:06:09.590
They can create a checkerboard or
stripes or a monochromatic image.

00:06:10.580 --> 00:06:13.500
The next are the geometry adjustments.

00:06:13.620 --> 00:06:17.170
Conceptually, these are very simple,
such as filters to crop an image,

00:06:17.320 --> 00:06:22.780
apply an affine transformation,
or to rotate or straighten an image.

00:06:23.680 --> 00:06:26.710
And last are the stylized
filters and gradients.

00:06:26.830 --> 00:06:29.700
Something you can do, for example,
is to run an image or a

00:06:29.700 --> 00:06:33.250
constant color into a gradient,
and then blend that with another

00:06:33.250 --> 00:06:37.610
image to get some really interesting
fall-off or vignette effects.

00:06:37.760 --> 00:06:39.660
Okay, so that's a big list.

00:06:39.730 --> 00:06:42.710
You can find more information
about each of these filters in the

00:06:42.710 --> 00:06:46.200
Core Image Filter Reference document,
which is available on

00:06:46.200 --> 00:06:47.870
our developer website.

00:06:48.380 --> 00:06:51.230
So now with that introduction,
let's dive deeper and look at

00:06:51.230 --> 00:06:53.470
how to use the Core Image API.

00:06:53.600 --> 00:06:57.980
There's three major classes that
you'll use all the time in Core Image.

00:06:58.080 --> 00:07:01.490
The first is the CI Filter,
and this represents a specific

00:07:01.580 --> 00:07:03.100
image processing operation.

00:07:03.100 --> 00:07:06.390
You just saw a list of
48 different filters.

00:07:06.500 --> 00:07:10.170
Now, almost all the filters
take an image as an input,

00:07:10.180 --> 00:07:13.000
and all of them produce
an image as output.

00:07:13.120 --> 00:07:18.370
And many have various settings as input
parameters for the effect being applied.

00:07:19.480 --> 00:07:21.360
Next is the CI Image.

00:07:21.430 --> 00:07:23.960
This represents the image
that you'll be processing,

00:07:24.030 --> 00:07:28.400
but really it represents the recipe
that's going to be applied to that image.

00:07:28.400 --> 00:07:31.400
And it can come from those four
sources I discussed earlier,

00:07:31.400 --> 00:07:34.690
or it can be the output of a filter.

00:07:35.830 --> 00:07:38.600
And the third class is the CI Context.

00:07:38.730 --> 00:07:41.150
This is the class that
actually does the work.

00:07:41.360 --> 00:07:44.950
It's what you'll use to run the
image through the filters that

00:07:44.950 --> 00:07:46.700
you've defined and render it.

00:07:46.710 --> 00:07:51.310
And that work can be done
either on the CPU or the GPU.

00:07:52.490 --> 00:07:55.440
So this is a good time to mention
some of the platform-specific

00:07:55.440 --> 00:07:57.020
information about Core Image.

00:07:57.020 --> 00:08:00.680
Core Image got its start on
the Mac with Mac OS X Tiger,

00:08:00.680 --> 00:08:03.540
and there's some differences
between what you have on the

00:08:03.540 --> 00:08:05.920
Mac with the implementation on iOS.

00:08:05.920 --> 00:08:08.110
The first difference are the filters.

00:08:08.110 --> 00:08:12.300
We currently have 48 filters on iOS,
the list I just showed you.

00:08:12.740 --> 00:08:16.740
Whereas on the Mac, there are many more,
about 130 built-in filters.

00:08:16.740 --> 00:08:19.790
Plus, on the Mac,
it's possible for third-party

00:08:19.790 --> 00:08:22.360
developers to write your own filters.

00:08:22.420 --> 00:08:25.640
whereas that capability
is not provided on iOS.

00:08:27.460 --> 00:08:29.030
Next is the API.

00:08:29.140 --> 00:08:31.650
iOS uses the three
classes I just mentioned,

00:08:31.810 --> 00:08:35.820
the Filter, the Image, and the Context,
whereas on the Mac there are others,

00:08:36.000 --> 00:08:38.390
primarily to support
developing custom filters.

00:08:38.400 --> 00:08:41.500
You would subclass CI Filter and
implement your own kernel,

00:08:41.560 --> 00:08:42.390
for example.

00:08:42.400 --> 00:08:46.240
Now, on both platforms,
they share the same underlying

00:08:46.240 --> 00:08:51.210
implementation that concatenates and
optimizes the filter graph that you set

00:08:51.210 --> 00:08:53.380
up to get the best possible performance.

00:08:53.470 --> 00:08:58.160
And instructions are then generated
that either take advantage of

00:08:58.160 --> 00:09:03.400
the CPU or the GPU using the
native libraries for each platform.

00:09:05.190 --> 00:09:09.770
Now, let me show you a quick example of
using the Core Image API to apply sepia

00:09:09.770 --> 00:09:12.850
tone to an image and render the result.

00:09:12.950 --> 00:09:16.590
So first, we'll start by creating
a CI image instance.

00:09:16.910 --> 00:09:20.780
In this case, we're loading from a URL in
our application bundle or

00:09:20.780 --> 00:09:22.950
from some other location.

00:09:23.040 --> 00:09:25.300
So that gives us an image.

00:09:25.360 --> 00:09:29.440
Next, we're going to create a filter
and configure its inputs.

00:09:29.520 --> 00:09:32.840
I'm creating a sepia tone
filter and setting its input

00:09:32.970 --> 00:09:38.760
to be the image we just loaded,
and setting its intensity to 0.8.

00:09:39.360 --> 00:09:43.700
Then I create the context which is
going to render the filtered image.

00:09:43.810 --> 00:09:45.640
Now here I render it.

00:09:45.720 --> 00:09:48.900
I'm asking the filter
for its output CI image,

00:09:48.990 --> 00:09:53.240
which at this point just represents
the recipe that will be applied

00:09:53.240 --> 00:09:55.250
to the image when it's rendered.

00:09:55.390 --> 00:10:00.040
And in this final line,
I ask the context to render that CI image

00:10:00.200 --> 00:10:03.680
into a Core Graphics CG image ref.

00:10:03.860 --> 00:10:07.090
But so those four steps,
creating the three objects

00:10:07.130 --> 00:10:09.920
and then rendering,
represent most of the work

00:10:09.920 --> 00:10:11.640
you'll do with Core Image.

00:10:11.730 --> 00:10:16.500
But next I want to go into a
little more detail about each step.

00:10:16.630 --> 00:10:21.200
So first, let's look at how you
can create a CI image.

00:10:22.550 --> 00:10:25.570
As I said earlier,
the CI image is really flexible.

00:10:25.740 --> 00:10:29.360
You can initialize it with the data
from a number of different sources,

00:10:29.680 --> 00:10:35.380
such as a URL to a photo or an image
file using image with contents of URL,

00:10:35.550 --> 00:10:41.370
or an NSData containing an image file
in any of a number of common formats.

00:10:41.800 --> 00:10:46.400
Next is the Core Graphics CG Image Ref,
which is very commonly

00:10:46.400 --> 00:10:50.450
used on iOS and the Mac,
and itself is very versatile.

00:10:50.650 --> 00:10:54.100
And in fact, what I'm showing here is how
to start from a UI image.

00:10:54.270 --> 00:10:59.710
We're loading foo.ping and asking
the UI image for its CG Image Ref,

00:10:59.710 --> 00:11:04.410
and that's what we use to
initialize our CI image.

00:11:04.940 --> 00:11:08.190
A third source could be from
a Core Video Pixel Buffer,

00:11:08.190 --> 00:11:09.870
which is a buffer that
enables you to take,

00:11:10.010 --> 00:11:14.310
for example,
live video from an AV Foundation capture

00:11:14.310 --> 00:11:19.290
session and use those frames
as images in Core Image.

00:11:19.780 --> 00:11:22.470
And then finally,
you can also initialize a CI image

00:11:22.650 --> 00:11:24.700
from raw pixel data in memory.

00:11:24.710 --> 00:11:28.870
So here, I'm creating an image using an
NSData containing the bitmap,

00:11:29.250 --> 00:11:32.700
and then specifying the bytes per row,
the size, the format,

00:11:32.700 --> 00:11:35.680
and an optional color space.

00:11:37.840 --> 00:11:39.640
Now,
while I'm on the subject of color space,

00:11:39.700 --> 00:11:43.300
let me talk a little bit about
color management in Core Image.

00:11:43.340 --> 00:11:47.840
On Mac OS X, a CI image can be tagged
with any color space.

00:11:47.860 --> 00:11:51.540
This is provided using the
Core Graphics Data Type CG color space.

00:11:51.810 --> 00:11:55.640
And if a CI image is tagged,
the pixels are converted

00:11:55.740 --> 00:11:59.760
to a linear working space
before filtering is applied.

00:11:59.780 --> 00:12:02.040
Now, on iOS, it's a little bit different.

00:12:02.060 --> 00:12:05.920
A CI image can be tagged with
the device RGB color space,

00:12:05.960 --> 00:12:09.360
and if it is tagged,
the pixels are gamma corrected to a

00:12:09.360 --> 00:12:12.290
linear working space before filtering.

00:12:12.630 --> 00:12:16.190
So,
you can use the KCI Image Color Space key

00:12:16.560 --> 00:12:20.500
to override the default color space
on the CI image you're creating.

00:12:20.670 --> 00:12:26.000
And one value you might set is
NULL to leave the image unmanaged.

00:12:26.210 --> 00:12:30.800
While in most cases you do want to
have the gamma correction turned on,

00:12:30.970 --> 00:12:33.890
there are cases where you want to
run your entire filtering pipeline,

00:12:33.890 --> 00:12:36.100
perhaps,
with color management turned off,

00:12:36.290 --> 00:12:40.400
such as to improve performance
or to get a particular effect.

00:12:41.380 --> 00:12:44.480
And the last thing to mention
about images is metadata.

00:12:44.540 --> 00:12:48.940
Modern cameras can associate a lot of
metadata with the photos they take,

00:12:49.000 --> 00:12:52.880
the make and model of the camera,
the date and time the photo was taken,

00:12:52.940 --> 00:12:57.170
the orientation of the image,
the exposure, shutter speed, and so on.

00:12:57.290 --> 00:13:00.580
And we provide a way for you to get at
that information through a dictionary

00:13:00.580 --> 00:13:03.540
called Properties on the CI image.

00:13:03.700 --> 00:13:06.560
There's a wide variety of
data that could be included,

00:13:06.630 --> 00:13:10.920
and it's image format dependent
with different metadata for GIF,

00:13:11.040 --> 00:13:13.250
TIFF, JPEG, etc.

00:13:13.340 --> 00:13:16.470
But one piece of metadata that
is typically very important

00:13:16.850 --> 00:13:19.170
is the image orientation.

00:13:19.250 --> 00:13:23.890
You'll typically want to use this so
you can maintain the orientation any

00:13:23.930 --> 00:13:27.050
time you display that image in your UI.

00:13:27.390 --> 00:13:30.030
Now,
the metadata gets automatically defined

00:13:30.030 --> 00:13:35.360
for you if you're loading an image from a
file or from an NSData containing a file.

00:13:35.420 --> 00:13:37.900
But if you're creating a file,
you're creating CI images

00:13:37.900 --> 00:13:41.560
from data in memory,
for example, or from core video,

00:13:41.620 --> 00:13:44.300
and want to specify
the metadata yourself,

00:13:44.300 --> 00:13:48.560
then there's a way to do that as
well using the Image Properties key.

00:13:50.390 --> 00:13:52.140
Okay, so that's CI Image.

00:13:52.210 --> 00:13:56.890
Next, let's go on to how do we
define and apply filters.

00:13:57.700 --> 00:14:00.250
So let's talk about the inputs
and outputs of filters first.

00:14:00.400 --> 00:14:02.530
Here's a sepia tone filter.

00:14:02.560 --> 00:14:06.030
And as I said, almost all filters
take an image as input,

00:14:06.230 --> 00:14:10.600
perform some operation,
and then provide an image as output.

00:14:10.710 --> 00:14:13.890
So here you see an input and
output image for this filter.

00:14:14.010 --> 00:14:18.150
And for sepia tone,
we need also an intensity value

00:14:18.410 --> 00:14:20.890
of how much sepia to apply.

00:14:21.680 --> 00:14:24.200
Here's another filter, Color Controls.

00:14:24.280 --> 00:14:26.800
This also has an input
image and output image,

00:14:26.970 --> 00:14:29.440
of course,
but this one has multiple other

00:14:29.440 --> 00:14:32.430
parameters to control the saturation,
brightness,

00:14:32.510 --> 00:14:35.800
and contrast of the resulting image.

00:14:36.270 --> 00:14:37.700
This next one is a little different.

00:14:37.830 --> 00:14:39.800
This is Hue Blend Mode.

00:14:39.920 --> 00:14:42.250
So this is going to blend
two images together.

00:14:42.430 --> 00:14:46.940
So it takes two images as input and
provides one image as the output.

00:14:47.020 --> 00:14:49.940
And the point of all of this is to
remember that we'll typically be

00:14:49.940 --> 00:14:54.310
chaining filters together with the
output of one filter feeding into the

00:14:54.430 --> 00:14:56.600
input of the next filter and so on.

00:14:56.700 --> 00:14:59.660
So here I have two CI images
that will be blended.

00:14:59.770 --> 00:15:02.640
Then that result will go
into the sepia tone filter.

00:15:02.740 --> 00:15:06.870
Then that result will go into more
filters or be drawn by the context.

00:15:07.040 --> 00:15:10.700
And everything is wired up here
except for the sepia intensity,

00:15:10.780 --> 00:15:14.330
which perhaps you would allow the
user to adjust with the slider,

00:15:14.330 --> 00:15:16.130
for example.

00:15:17.030 --> 00:15:19.530
So, let's look at how we accomplish this.

00:15:19.780 --> 00:15:22.460
Filters are instantiated by name.

00:15:22.690 --> 00:15:25.980
For example,
here I'm asking for a sepia tone filter.

00:15:26.240 --> 00:15:28.990
This is an NSString,
and you can either use the

00:15:28.990 --> 00:15:32.340
API to query the list of
filters supported by the device,

00:15:32.470 --> 00:15:35.970
or you can just look up their
names in our documentation.

00:15:36.240 --> 00:15:40.090
The inputs to the filter are
set using key value syntax.

00:15:40.220 --> 00:15:43.540
So here, for example,
I'm telling the filter to set this

00:15:43.540 --> 00:15:49.020
image for its key input image,
and I'm telling it to set the value

00:15:49.020 --> 00:15:52.260
0.8 for its key input intensity.

00:15:52.420 --> 00:15:55.770
And just like the names,
you can either use the API to query the

00:15:55.770 --> 00:15:58.640
names of these keys or look them up.

00:15:58.690 --> 00:16:01.180
So those are the inputs.

00:16:01.720 --> 00:16:05.530
Now, all filters only have one output,
the output image,

00:16:05.530 --> 00:16:09.160
and there's three different
ways that you can get it.

00:16:09.520 --> 00:16:12.740
You can use the value for
key method on the filter,

00:16:12.740 --> 00:16:17.830
or you can use the output image method,
or just use dot syntax for

00:16:17.900 --> 00:16:19.900
the property output image.

00:16:19.940 --> 00:16:22.030
The only difference here is the syntax.

00:16:22.090 --> 00:16:24.050
You get the same result
from all three techniques.

00:16:26.410 --> 00:16:29.240
So let's say we have two filters
and we want to chain them

00:16:29.240 --> 00:16:31.100
together and get the result.

00:16:31.190 --> 00:16:33.900
So it's the exact same
code to start with.

00:16:34.010 --> 00:16:39.640
We're creating a sepia tone filter,
setting its intensity to 0.8,

00:16:39.780 --> 00:16:43.070
and setting its input image to some
image that we've previously loaded.

00:16:43.740 --> 00:16:47.620
But now let's create another filter,
in this case a hue adjustment,

00:16:47.920 --> 00:16:51.180
which rotates the colors of the
image around the color wheel.

00:16:51.210 --> 00:16:58.210
So we set its input angle to 0.5 radians,
and the input image for this filter is

00:16:58.320 --> 00:17:01.940
the output image from the sepia filter.

00:17:02.550 --> 00:17:05.330
And then we get the output
of the Hue Adjustment Filter,

00:17:05.330 --> 00:17:08.200
which is what we'll use to
render with the context.

00:17:08.400 --> 00:17:12.400
And that takes us to rendering.

00:17:15.440 --> 00:17:21.480
The CI context is the object we use to
render a CI image into its destination,

00:17:21.530 --> 00:17:26.030
into Core Graphics, into OpenGL,
into Core Video and AV Foundation,

00:17:26.320 --> 00:17:28.300
or into an array of bytes and memory.

00:17:28.600 --> 00:17:33.600
So I want to describe how this works
by showing you four common use cases:

00:17:33.870 --> 00:17:36.770
processing the image and
displaying it in a view,

00:17:36.850 --> 00:17:40.140
or saving the processed
image into the photo library,

00:17:40.380 --> 00:17:44.910
or rendering it into an OpenGL surface,
or rendering it into a

00:17:44.910 --> 00:17:47.070
Core Video pixel buffer.

00:17:47.740 --> 00:17:50.170
So, here's the first case.

00:17:50.270 --> 00:17:54.840
Assuming we've already created a
CI image and set up our filters,

00:17:54.990 --> 00:17:59.370
here's the steps we'll take to
actually render that into a view.

00:17:59.690 --> 00:18:04.380
First, we create our CI context with
whatever options need to be set.

00:18:04.560 --> 00:18:10.530
Then, we'll get the output image from the
last filter in our filter chain.

00:18:10.640 --> 00:18:13.840
And at this point,
the output image is just the

00:18:13.840 --> 00:18:18.920
recipe that the context will use
when it goes to render the image.

00:18:19.140 --> 00:18:22.680
So then we tell the context to
render that output image into

00:18:22.720 --> 00:18:24.750
a Core Graphics CG image ref.

00:18:25.000 --> 00:18:29.150
Now, we really have an image
now with pixels rendered,

00:18:29.350 --> 00:18:33.830
and we want to put that up in a view,
so we'll wrap it up in a UI image and

00:18:33.830 --> 00:18:36.630
put the UI image in a UI image view.

00:18:38.690 --> 00:18:40.840
So let's take a look at that code.

00:18:40.920 --> 00:18:42.600
First, we create the context.

00:18:42.620 --> 00:18:46.920
Now, a quick thing to mention,
in all the examples I'm about to show,

00:18:46.920 --> 00:18:50.200
I include a line like this
that creates a context,

00:18:50.370 --> 00:18:52.480
and then we go on to the other steps.

00:18:52.510 --> 00:18:55.960
But actually, if you're going to be
doing this repeatedly,

00:18:56.000 --> 00:19:01.100
you should only create the context once,
and then just keep reusing

00:19:01.160 --> 00:19:03.400
that same context every time.

00:19:03.420 --> 00:19:07.550
You don't want to create a new
context every time you render.

00:19:07.650 --> 00:19:08.830
But OK, coming back.

00:19:09.020 --> 00:19:11.280
So we have our context now.

00:19:11.320 --> 00:19:13.670
Next,
we're going to get the output image of

00:19:13.670 --> 00:19:16.760
the last filter in our filter chain.

00:19:16.820 --> 00:19:19.680
And now we render that
into a CG image ref.

00:19:19.720 --> 00:19:22.520
We're telling the context,
create a CG image from

00:19:22.520 --> 00:19:26.590
the output of our filter,
and we specify its dimensions.

00:19:26.990 --> 00:19:30.050
But now we have real pixels,
and we need to put them into a view.

00:19:30.340 --> 00:19:34.500
So we'll create a UI image,
passing the CG image we just created,

00:19:34.640 --> 00:19:37.640
specifying its scale and orientation.

00:19:37.760 --> 00:19:42.240
And finally, we put that UI image
into a UI image view.

00:19:42.370 --> 00:19:45.340
So it's a lot of steps,
but it illustrates each

00:19:45.340 --> 00:19:46.720
step of the process.

00:19:46.860 --> 00:19:49.820
However,
I only did it this way for completeness.

00:19:49.820 --> 00:19:52.430
There's actually a shortcut.

00:19:53.080 --> 00:19:58.200
And it's that we've added a method to
UI Image to take a CI image directly.

00:19:58.290 --> 00:20:03.160
So all we need to do is create the
UI Image and pass the output of the

00:20:03.160 --> 00:20:05.980
last filter in our filter chain.

00:20:06.200 --> 00:20:09.090
Then we just put that
UI Image into a UI Image view,

00:20:09.240 --> 00:20:10.780
and that's it.

00:20:10.860 --> 00:20:15.000
Next is rendering the output and
saving it to the photo library.

00:20:15.280 --> 00:20:18.150
So this is all the same
as rendering into a view,

00:20:18.150 --> 00:20:22.160
but what I want to illustrate here
is a case where you might want to

00:20:22.160 --> 00:20:26.280
have the context do its rendering
on the CPU instead of the GPU.

00:20:26.410 --> 00:20:29.240
And there's two situations
where you'd want this.

00:20:29.350 --> 00:20:34.240
The first is if the images you're dealing
with are larger than the GPU can handle.

00:20:34.340 --> 00:20:38.620
And this is frequently the case with
large photos in the user's photo library.

00:20:38.710 --> 00:20:42.890
On the iPhone 4S and iPad 2,
the maximum size for images

00:20:42.890 --> 00:20:46.370
on the GPU is 4K by 4K.

00:20:46.590 --> 00:20:51.850
And on iPhone 3GS, iPhone 4, iPad 1,
and the third and fourth

00:20:51.850 --> 00:20:55.380
generation iPod Touch,
the maximum size is 2K by 2K.

00:20:55.420 --> 00:20:58.000
And now,
the second case is if you're doing

00:20:58.050 --> 00:21:01.910
a particularly long processing
operation and want to allow it

00:21:01.910 --> 00:21:03.880
to continue in the background.

00:21:03.880 --> 00:21:07.220
Well, only CPU context can allow that.

00:21:07.420 --> 00:21:10.850
So, we'll create our context on the CPU,
and then the rest is

00:21:10.850 --> 00:21:12.500
just like you saw before.

00:21:13.020 --> 00:21:16.600
We'll get the output of the
last filter in our filter chain,

00:21:16.600 --> 00:21:20.490
render that into a CG image ref,
and then put that CG image

00:21:20.650 --> 00:21:22.290
ref into the photo library.

00:21:24.500 --> 00:21:25.850
So let's look at the code.

00:21:26.020 --> 00:21:29.640
First, we're creating a CPU context,
and I do that by specifying

00:21:29.640 --> 00:21:33.200
an options dictionary,
sending a Boolean yes for the key

00:21:33.200 --> 00:21:35.470
context use software renderer.

00:21:35.760 --> 00:21:38.330
And then it's the same
as what we saw before.

00:21:38.460 --> 00:21:42.320
I get the output image from the
last filter in my filter chain,

00:21:42.600 --> 00:21:46.340
and I ask the context to render it
to a Core Graphics CG image ref.

00:21:46.460 --> 00:21:50.580
Now I have a real image,
and I can use the Assets Library API to

00:21:50.580 --> 00:21:52.670
add it to the Photo Library.

00:21:52.830 --> 00:21:56.440
I get a handle to the library,
then I call write image

00:21:56.610 --> 00:22:00.300
to saved photos album,
passing the CG image ref I just created

00:22:00.440 --> 00:22:03.180
and the metadata for the original image.

00:22:03.180 --> 00:22:06.640
This API takes a block
as a completion handler,

00:22:06.640 --> 00:22:09.180
and in that I release the CG image.

00:22:09.180 --> 00:22:10.180
So great.

00:22:10.180 --> 00:22:13.870
So that's usage case number two,
rendering to the Photo Library and

00:22:14.240 --> 00:22:17.420
illustrating the use
of a CPU based context.

00:22:19.260 --> 00:22:22.700
So here's the third case,
rendering into OpenGL ES.

00:22:22.850 --> 00:22:24.720
For those of you who
are OpenGL programmers,

00:22:24.720 --> 00:22:26.040
this is really simple.

00:22:26.040 --> 00:22:29.850
We're going to initialize the
Core Image context using the

00:22:29.860 --> 00:22:34.200
same Eagle context that you're
drawing into with OpenGL ES.

00:22:34.410 --> 00:22:37.930
Then each frame, we do the same things as
the previous examples.

00:22:38.120 --> 00:22:42.120
Get the output image from the
last filter in our filter chain,

00:22:42.120 --> 00:22:44.540
and tell the context to render it.

00:22:44.750 --> 00:22:46.440
So here's that example.

00:22:46.530 --> 00:22:50.640
At initialization time,
we create our OpenGL ES context,

00:22:50.780 --> 00:22:54.420
telling it to use the OpenGL ES 2.0 API.

00:22:54.660 --> 00:23:00.430
Then we use that OpenGL ES context
to create the Core Image context.

00:23:00.590 --> 00:23:03.840
Core Image is really using
OpenGL ES internally in this case,

00:23:03.840 --> 00:23:07.280
so it's able to use your
context very efficiently.

00:23:07.370 --> 00:23:10.960
Then each frame at render time,
it's the similar as before.

00:23:11.120 --> 00:23:14.640
We get the output image from the
last filter in the filter chain,

00:23:14.740 --> 00:23:16.880
and then tell the context to render it.

00:23:17.010 --> 00:23:21.640
And the syntax with OpenGL is just
draw image at point from rect.

00:23:21.750 --> 00:23:24.430
And now we have our processed
image rendered into the

00:23:24.430 --> 00:23:26.380
OpenGL ES render buffer.

00:23:26.490 --> 00:23:30.960
Then we can go on to make the rest of
our OpenGL ES calls for that frame.

00:23:31.030 --> 00:23:35.000
And when the frame is finished,
we bind the render buffer and present it.

00:23:35.080 --> 00:23:36.960
This is with plain OpenGL ES.

00:23:37.030 --> 00:23:40.240
If you were using GL Kit instead,
you would just put the

00:23:40.310 --> 00:23:45.600
Core Image calls into your
glkViewDrawInRect delegate method.

00:23:45.680 --> 00:23:48.960
So that's rendering to OpenGL ES,
usage case number three,

00:23:49.060 --> 00:23:51.350
and there's one more to go.

00:23:51.530 --> 00:23:54.100
And that's rendering into Core Video.

00:23:54.140 --> 00:23:59.000
So this is going to let you source frames
of live video from a capture session,

00:23:59.000 --> 00:24:02.700
for example,
process the frames through Core Image,

00:24:02.730 --> 00:24:06.070
and render them back out to
Core Video and AV Foundation,

00:24:06.420 --> 00:24:07.430
all in real time.

00:24:07.740 --> 00:24:11.070
So the steps here are identical
to what we've seen before.

00:24:11.470 --> 00:24:13.940
We create a context when
we start this process,

00:24:13.960 --> 00:24:17.100
and then for each frame,
get the output image from the

00:24:17.100 --> 00:24:20.640
last filter in our filter chain,
and then tell the context to

00:24:20.640 --> 00:24:22.790
render it into a CVPixelBuffer rev.

00:24:22.960 --> 00:24:27.340
We use context rendered in CVPixelBuffer,
and specify the bounds

00:24:27.340 --> 00:24:28.860
and the color space.

00:24:28.960 --> 00:24:29.960
And that's it.

00:24:30.080 --> 00:24:34.950
We're processing frames of live video
and writing them back out to the buffer.

00:24:35.450 --> 00:24:38.890
So now we've covered the three
main classes of Core Image,

00:24:38.890 --> 00:24:43.500
the CI Image, CI Filter, and CI Context,
and looked at how we can get

00:24:43.500 --> 00:24:47.550
images from various sources,
apply filters, and write the output to

00:24:47.550 --> 00:24:48.720
various destinations.

00:24:48.720 --> 00:24:52.780
Next, I want to change topics and
talk about two advanced features

00:24:52.830 --> 00:24:57.460
provided for image analysis,
and the first of these is face detection.

00:24:58.200 --> 00:25:02.840
The idea of face detection, quite simply,
is to analyze a photo or live

00:25:02.840 --> 00:25:07.400
video to detect faces and report
back their position in the image.

00:25:07.500 --> 00:25:12.750
So here's a family vacation photo,
and we're able to analyze this image and

00:25:12.750 --> 00:25:17.590
get the position of the faces and the
location of the features of each person,

00:25:17.590 --> 00:25:19.100
like their eyes and mouth.

00:25:20.850 --> 00:25:22.700
Now here's the classes involved.

00:25:22.900 --> 00:25:24.940
The CI Detector is the main class.

00:25:25.240 --> 00:25:27.300
You'll provide it with
an image to analyze,

00:25:27.600 --> 00:25:33.040
and if it finds any faces,
it'll return an array of CI features.

00:25:33.130 --> 00:25:38.720
A CI feature has a type and bounds,
and a subclass called CI Face Feature,

00:25:39.150 --> 00:25:42.640
which contains the positions
of the eyes and the mouth.

00:25:43.730 --> 00:25:45.370
So let's look at how we set it up.

00:25:45.470 --> 00:25:49.100
So we create a CI detector
object by specifying its type,

00:25:49.130 --> 00:25:51.280
a context, and some options.

00:25:51.310 --> 00:25:53.870
The only type currently
supported is a face.

00:25:54.060 --> 00:25:57.820
If we set the context to nil,
the detector will create one of its own,

00:25:57.820 --> 00:25:59.840
and then there's the options.

00:25:59.890 --> 00:26:04.410
This is used to specify whether the
detector should have a trade-off

00:26:04.410 --> 00:26:06.540
for speed or for accuracy.

00:26:06.800 --> 00:26:12.000
Low accuracy is generally appropriate
for detecting faces in live video.

00:26:12.050 --> 00:26:16.170
And high accuracy is more appropriate
if you're doing processing on a single

00:26:16.170 --> 00:26:19.420
photo and need the most accurate result.

00:26:20.980 --> 00:26:24.190
We have our detector set up,
and we can ask it to find

00:26:24.190 --> 00:26:25.700
any faces in the image.

00:26:26.010 --> 00:26:29.290
So we're calling Detector
Features an Image,

00:26:29.400 --> 00:26:32.470
passing the image,
and another set of options.

00:26:32.690 --> 00:26:36.760
This returns an NSArray of
CI Face feature objects,

00:26:36.760 --> 00:26:39.390
one for each face in the image.

00:26:39.540 --> 00:26:43.550
The options are to tell the
detector which direction is up

00:26:43.800 --> 00:26:45.600
in the image you're providing.

00:26:45.660 --> 00:26:48.410
The detector needs that in order
to calculate whether or not

00:26:48.470 --> 00:26:49.880
something in the image is a face.

00:26:50.320 --> 00:26:52.060
So that's what this code is doing.

00:26:52.100 --> 00:26:55.770
We're getting the orientation key
from the image and using that to set

00:26:55.910 --> 00:26:59.140
the orientation key for the detector.

00:26:59.820 --> 00:27:04.500
Great, so now we have an array of faces,
and we can process the results.

00:27:04.620 --> 00:27:07.300
So here I'm just looping
through the array and printing

00:27:07.300 --> 00:27:10.280
information to the console,
starting with the bounds

00:27:10.390 --> 00:27:13.010
of the face of the image,
and then if we have data for

00:27:13.010 --> 00:27:16.210
the position of the left eye,
right eye, and mouth,

00:27:16.210 --> 00:27:18.320
we'll output that as well.

00:27:18.910 --> 00:27:20.800
So, that's face detection.

00:27:20.920 --> 00:27:25.050
The other advanced feature I wanted
to tell you about is Auto-enhance.

00:27:25.230 --> 00:27:28.970
This is very similar to what
you see in the Photos app,

00:27:28.990 --> 00:27:32.040
and now we've made it available
for developers to include

00:27:32.040 --> 00:27:33.800
this in your apps as well.

00:27:34.120 --> 00:27:36.020
The idea is to analyze the image.

00:27:36.110 --> 00:27:40.440
We look at its histogram,
we look for faces, and other metadata

00:27:40.450 --> 00:27:42.340
associated with the image.

00:27:42.450 --> 00:27:47.900
We run an analysis and provide your
app with an array of filters with

00:27:48.010 --> 00:27:54.190
their parameters perfectly set up
to enhance this particular image.

00:27:54.780 --> 00:27:56.580
Now, here's the filters we use.

00:27:56.650 --> 00:27:58.700
The first is Red Eye Correction.

00:27:58.800 --> 00:28:03.440
If we find any faces in the image,
we can look for the eyes and repair

00:28:03.440 --> 00:28:07.440
the red eye effect that happens in
some photos due to the camera flash.

00:28:07.440 --> 00:28:09.230
The next is Face Balance.

00:28:09.340 --> 00:28:12.210
Again,
we look for faces and can make color

00:28:12.300 --> 00:28:15.230
adjustments to give better skin tones.

00:28:15.240 --> 00:28:17.420
Vibrance makes the image pop.

00:28:17.500 --> 00:28:22.580
It increases the color saturation
while maintaining proper skin tones.

00:28:23.530 --> 00:28:26.880
The next is Tone Curve,
which can improve the contrast.

00:28:26.900 --> 00:28:31.890
And last is Highlight Shadow Adjust,
which will pull down the highlights and

00:28:31.890 --> 00:28:35.800
pull up the shadows in an image while
maintaining the contrast and color.

00:28:38.020 --> 00:28:39.960
The API for this is really simple.

00:28:40.300 --> 00:28:44.500
On the CI image, there's a method,
Auto Adjustment Filters with Options.

00:28:44.550 --> 00:28:48.440
It returns an array of
filters specifically tuned

00:28:48.440 --> 00:28:50.840
for that particular image.

00:28:50.840 --> 00:28:52.770
And it takes some options.

00:28:52.820 --> 00:28:55.440
The primary option is the orientation.

00:28:55.440 --> 00:28:58.090
Since, again,
it uses the face detector to

00:28:58.100 --> 00:29:01.360
find faces and improve skin
tones and remove red eye,

00:29:01.400 --> 00:29:04.930
the detector needs to
know what direction is up.

00:29:05.560 --> 00:29:10.340
And we use the same options as I showed
you in the face detection section.

00:29:10.550 --> 00:29:13.800
Then,
you can also disable certain adjustments.

00:29:13.880 --> 00:29:16.870
For example,
if you only want red eye correction,

00:29:16.920 --> 00:29:19.850
or you'd like everything
except red eye correction,

00:29:20.130 --> 00:29:21.720
there are keys that you can set for that.

00:29:23.910 --> 00:29:26.300
Now here's the code in context.

00:29:26.430 --> 00:29:29.800
We start with an image and
call auto-adjustment filters

00:29:29.800 --> 00:29:31.790
with options on that image.

00:29:31.990 --> 00:29:35.140
This gives you back an array of filters.

00:29:35.200 --> 00:29:38.990
And it's now your responsibility
to chain them together,

00:29:39.150 --> 00:29:43.480
hooking the output of the first filter
to the input of the next filter,

00:29:43.500 --> 00:29:44.360
and so on.

00:29:44.480 --> 00:29:46.170
So that's what this loop is doing.

00:29:46.200 --> 00:29:49.250
We're running through the array
of filters and setting the input

00:29:49.250 --> 00:29:53.800
image of each filter to be the
output image of the previous filter.

00:29:53.990 --> 00:29:56.640
And then once we're done,
we're ready to render that image.

00:29:58.660 --> 00:30:00.680
So let's look at some examples.

00:30:00.710 --> 00:30:02.740
On the left,
we have a photo of this little girl.

00:30:02.840 --> 00:30:07.000
It has nice composition,
but her skin tones are a little green.

00:30:07.030 --> 00:30:09.980
With Auto-enhance,
we're able to fix the skin tone and

00:30:10.030 --> 00:30:12.540
make the other colors more vibrant.

00:30:13.490 --> 00:30:15.830
Next are these girls standing
on a pier by the ocean.

00:30:15.840 --> 00:30:20.900
Their faces are in shadow,
and we're able to improve the brightness.

00:30:21.040 --> 00:30:24.920
And finally, this boy here on the right,
if you look closely at his eyes,

00:30:25.080 --> 00:30:28.900
there's some red eye in this picture,
which we can detect and

00:30:29.000 --> 00:30:30.800
correct automatically.

00:30:31.040 --> 00:30:32.830
Great, so that's Core Image.

00:30:32.830 --> 00:30:37.510
I've covered the basics, the CI image,
CI filter, and CI context,

00:30:37.510 --> 00:30:41.270
and a number of common use cases
for apps working with photos,

00:30:41.410 --> 00:30:43.140
live video, and OpenGL ES.

00:30:43.160 --> 00:30:47.760
Then I went into the advanced features
for face detection and auto-enhance.

00:30:47.760 --> 00:30:50.250
If you have any questions
about Core Image,

00:30:50.250 --> 00:30:53.710
you're welcome to contact
me at aschaffe at apple.com.

00:30:53.720 --> 00:30:57.230
We also have some great
documentation available in both

00:30:57.350 --> 00:30:59.280
the iOS and Mac dev centers.

00:30:59.890 --> 00:31:02.810
And you can find me in the
developer forums as well.

00:31:02.950 --> 00:31:03.860
Thanks for watching.