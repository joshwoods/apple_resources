WEBVTT

00:00:03.880 --> 00:00:05.430
Good afternoon.

00:00:05.500 --> 00:00:06.520
It's the last session of the day.

00:00:06.520 --> 00:00:09.440
I hope we all still have a lot of energy,
because we have a tremendous

00:00:09.440 --> 00:00:12.700
amount of information to show you,
a great presentation.

00:00:12.700 --> 00:00:14.900
And without further ado,
I think we'll just roll right in.

00:00:15.080 --> 00:00:19.800
Robert Bowdidge,
our Performance Tools guru.

00:00:19.840 --> 00:00:21.590
Thanks, Godfrey.

00:00:23.750 --> 00:00:26.250
Hi, my name is Robert Bowdidge
and I'm a member of Apple's

00:00:26.340 --> 00:00:29.300
Developer Tools group where I'm
responsible for the performance tools.

00:00:29.380 --> 00:00:33.510
What I'd like to do today is tell you
something about the tools and give

00:00:33.610 --> 00:00:35.600
you a quick introduction to them.

00:00:36.120 --> 00:00:38.470
Now, why are we giving this talk?

00:00:38.560 --> 00:00:43.420
Now, I'm sure all of you have great apps
that you're ready to ship at Macworld

00:00:43.420 --> 00:00:45.190
New York in just about two months.

00:00:45.190 --> 00:00:48.410
And hopefully they're all
pretty much feature complete,

00:00:48.410 --> 00:00:48.990
right?

00:00:50.900 --> 00:00:54.060
Thank you.

00:00:54.060 --> 00:00:59.360
So that means that you've got
about two months to go and you

00:00:59.360 --> 00:01:02.400
should probably be concerned a
little about now about whether your

00:01:02.400 --> 00:01:06.600
applications actually perform as well
as you want them to be performing.

00:01:06.600 --> 00:01:09.870
And more importantly that they're
performing well enough so that your

00:01:09.870 --> 00:01:11.760
customers also think they're good.

00:01:11.890 --> 00:01:15.300
and therefore hopefully you're going
to start using the performance tools

00:01:15.300 --> 00:01:18.040
to actually track down performance
problems and make your apps as

00:01:18.140 --> 00:01:20.350
good as they possibly can be.

00:01:21.180 --> 00:01:24.310
What I'll do is I'll
show you those tools.

00:01:24.410 --> 00:01:28.160
So what you should hopefully
learn today is first of all,

00:01:28.160 --> 00:01:32.170
you should find out that we actually
do have some cool tools available.

00:01:32.500 --> 00:01:36.410
Secondly, you should get some,
you are going to see me go through

00:01:36.580 --> 00:01:40.640
a sample program and actually
find some performance bugs in it.

00:01:40.690 --> 00:01:43.590
So we're going to do
some real world examples.

00:01:43.900 --> 00:01:46.150
and hopefully you can take that
home and get so excited about these

00:01:46.220 --> 00:01:48.440
tools that you're actually going
to go in and dive through them.

00:01:48.440 --> 00:01:53.430
I'm not going to do a tutorial,
I'm not going to go through in detail of,

00:01:53.430 --> 00:01:55.940
well you pull down this menu
and you press that button

00:01:56.310 --> 00:01:57.980
and this is what it shows.

00:01:57.980 --> 00:02:01.650
I'm really going to show you what you can
gain out of these in hopes of exciting

00:02:01.650 --> 00:02:04.590
you enough that you're actually going
to go off and play with these yourself.

00:02:04.750 --> 00:02:08.110
Hopefully some of you will be
playing with them as I speak.

00:02:09.130 --> 00:02:11.630
If you've got questions
about the frameworks,

00:02:11.670 --> 00:02:13.410
about what you should be
doing in your Carbon programs

00:02:13.530 --> 00:02:15.780
to make them more efficient,
I am not going to answer

00:02:15.870 --> 00:02:17.300
those sorts of questions.

00:02:17.330 --> 00:02:19.550
I am not the right person to
answer those and I would probably

00:02:19.640 --> 00:02:21.560
be telling you the wrong stuff.

00:02:21.640 --> 00:02:23.180
In those cases,
you want to go to the framework

00:02:23.400 --> 00:02:25.980
talks or look at them on DVD.

00:02:26.240 --> 00:02:28.180
So for example,
the Carbon talk that was at

00:02:28.180 --> 00:02:31.620
1:00 was absolutely--or 2:00
was absolutely wonderful.

00:02:31.660 --> 00:02:33.540
Go take a look at that
if you write Carbon apps.

00:02:33.580 --> 00:02:37.450
The Java talk was really good
about Java performance as well.

00:02:40.110 --> 00:02:42.000
So, just a quick summary.

00:02:42.060 --> 00:02:45.540
So what are the possible
causes of poor performance?

00:02:45.540 --> 00:02:47.830
On a Mac OS X system,
one of the most obvious is

00:02:47.830 --> 00:02:49.280
excessive use of memory.

00:02:49.280 --> 00:02:53.960
That is that your application
requires a working set of memory

00:02:53.960 --> 00:02:56.960
that is more than it really needs.

00:02:56.960 --> 00:03:00.790
Now this can be because you have a
lot of code because of dead code or

00:03:00.880 --> 00:03:03.290
stuff that isn't really necessary.

00:03:03.390 --> 00:03:06.410
It could be because you're allocating
too much memory via malloc.

00:03:06.410 --> 00:03:09.050
It could be because you're
mapping in shared files,

00:03:09.150 --> 00:03:10.200
that sort of thing.

00:03:10.290 --> 00:03:13.200
There's many ways that you can
increase your memory footprint.

00:03:13.210 --> 00:03:15.340
A second way that you could be
affecting performance is if your

00:03:15.340 --> 00:03:18.500
application is causing too much,
is basically using the CPU too much,

00:03:18.500 --> 00:03:19.810
executing too much code.

00:03:21.910 --> 00:03:24.990
A third way is that you might
not actually be doing anything.

00:03:25.230 --> 00:03:27.230
You may be waiting for
something to happen,

00:03:27.230 --> 00:03:30.200
such as going off on the
network or going out to disk.

00:03:30.270 --> 00:03:33.060
So we have both the case of doing
too much and doing too little.

00:03:33.290 --> 00:03:36.240
And finally, there may be cases where in
graphical application you may

00:03:36.240 --> 00:03:38.880
be doing too much drawing,
and therefore you're doing

00:03:38.880 --> 00:03:40.280
too much computation,
you're using memory,

00:03:40.290 --> 00:03:43.980
you're talking with the window manager,
and everything slows down.

00:03:44.040 --> 00:03:48.130
Almost all of these problems eventually
turn into memory problems on Mac OS X,

00:03:48.130 --> 00:03:52.080
because all of them have some connection
with how much memory is in the system

00:03:52.080 --> 00:03:54.420
and what memory is being touched.

00:03:54.420 --> 00:03:57.530
And the problem is that as soon as the
memory footprint of your application and

00:03:57.670 --> 00:04:01.360
of the rest of the system becomes larger
than you actually have physical memory,

00:04:01.400 --> 00:04:03.960
we have this wonderful
thing called virtual memory.

00:04:03.970 --> 00:04:05.770
And it just sort of
gives you extra memory.

00:04:05.780 --> 00:04:07.820
And to get around that,
it basically takes some of the memory out

00:04:07.820 --> 00:04:10.000
of memory and it writes it out to disk.

00:04:10.000 --> 00:04:13.820
And then it takes some other and takes
it off of disk and puts it into memory.

00:04:13.910 --> 00:04:16.440
And the problem is as
soon as this happens,

00:04:16.540 --> 00:04:19.260
suddenly your application is going
to be judged by the speed of the disk

00:04:19.260 --> 00:04:21.340
and not by the speed of the processor.

00:04:21.340 --> 00:04:23.870
So,
anything you can do to cut memory use,

00:04:23.870 --> 00:04:27.300
to keep your working set
as small as possible,

00:04:27.310 --> 00:04:28.040
is a great thing.

00:04:30.500 --> 00:04:34.590
Okay, so here's a quick summary of the
tools that are available with

00:04:34.590 --> 00:04:37.090
three categories that may or
may not be the most meaningful,

00:04:37.090 --> 00:04:38.400
but they seem to work.

00:04:38.560 --> 00:04:40.690
The first one is execution behavior.

00:04:40.800 --> 00:04:44.310
We have tools that help you
understand what code is executing.

00:04:44.310 --> 00:04:47.420
This includes tools such
as Sampler and Sample.

00:04:47.630 --> 00:04:55.100
The second set of tools we have
are for understanding heap use,

00:04:55.100 --> 00:04:58.570
for understanding how much memory
you explicitly ask for during

00:04:58.570 --> 00:05:00.400
the running of your program.

00:05:00.430 --> 00:05:03.300
These include tools such as malloc-debug,
object-alloc,

00:05:03.300 --> 00:05:07.570
which is a tool for understanding
Objective C and core foundation use,

00:05:07.570 --> 00:05:11.950
and the command line tools heap, leaks,
and malloc-history.

00:05:12.060 --> 00:05:14.850
malloc-history is particularly cool
because you can actually tell it,

00:05:14.890 --> 00:05:18.180
"Hey, in this process I've got a buffer
that starts at this address.

00:05:18.370 --> 00:05:20.330
Where did I allocate that?"
And it'll actually tell you.

00:05:20.390 --> 00:05:22.940
It'll give you a call stack
saying where that came from.

00:05:22.940 --> 00:05:26.970
Finally, there's a set that I call system
state because I don't have a

00:05:26.970 --> 00:05:28.460
better way to describe them.

00:05:28.710 --> 00:05:30.660
These include tools for
helping you do drawings,

00:05:30.670 --> 00:05:31.760
such as quartz-debug.

00:05:31.870 --> 00:05:35.370
It includes tools for understanding
how you use system calls,

00:05:35.370 --> 00:05:38.840
such as SC usage or file system calls,
which is FS usage,

00:05:38.880 --> 00:05:42.590
as well as programs like top,
which gives you sort of an overall

00:05:42.890 --> 00:05:44.860
state of the program--of the machine.

00:05:44.860 --> 00:05:46.900
VM map,
which will tell you how virtual memory

00:05:46.900 --> 00:05:49.700
is laid out for a specific process.

00:05:51.280 --> 00:05:55.820
Okay, so how are we actually going to
look at the performance tools?

00:05:55.980 --> 00:05:59.360
What we're going to do is, well,
we're going to do public embarrassment.

00:05:59.480 --> 00:06:03.920
We are going to rip apart
one of my own programs,

00:06:03.950 --> 00:06:07.900
a little test app that I've been
working on called Thread Viewer,

00:06:07.900 --> 00:06:11.710
and we are going to see if we
can find any performance problems

00:06:11.710 --> 00:06:16.220
that either I put in or that
linger from code that I borrowed.

00:06:16.790 --> 00:06:17.540
Okay.

00:06:17.540 --> 00:06:20.200
Scott, do you want to come up here
and show Thread Viewer?

00:06:20.220 --> 00:06:22.920
Now, the important things to know
about Thread Viewer is that,

00:06:23.020 --> 00:06:25.400
first of all, it's a small Carbon app.

00:06:25.610 --> 00:06:29.230
or excuse me,
it's a small Cocoa application intended

00:06:29.230 --> 00:06:32.120
for helping us do performance analysis.

00:06:34.290 --> 00:06:37.450
Actually, we need to go to demo two.

00:06:37.560 --> 00:06:39.570
Oh, we are on demo two.

00:06:40.790 --> 00:06:43.590
and what we can do with thread
viewer is as Scott just did we can

00:06:43.670 --> 00:06:47.260
say hey I want to look at a specific
process in this case the doc and

00:06:47.260 --> 00:06:51.350
what it does is it shows us a running
timeline showing us what's happening

00:06:51.350 --> 00:06:53.800
with specific threads in that task.

00:06:53.840 --> 00:06:57.280
So for example we can see here
that doc uses two threads.

00:06:57.280 --> 00:07:02.170
The one on the bottom is the main thread,
the one above is a secondary thread

00:07:02.170 --> 00:07:04.460
that's helping do computations.

00:07:04.460 --> 00:07:08.050
Now the little blocks there
represent 50 millisecond intervals

00:07:08.130 --> 00:07:09.600
in the life of that process.

00:07:09.600 --> 00:07:13.140
The green represents that the,
that doc was running during that,

00:07:13.140 --> 00:07:14.390
when it was examined.

00:07:14.390 --> 00:07:18.090
The yellow indicates that it had been
running in the last 50 milliseconds

00:07:18.090 --> 00:07:19.670
but it wasn't currently running.

00:07:19.670 --> 00:07:23.210
The gray indicates that it's blocking,
it's waiting for something to happen.

00:07:23.320 --> 00:07:25.760
The light green and light red are,
are waits also.

00:07:25.810 --> 00:07:28.430
The green means that it's
waiting in the run loop,

00:07:28.830 --> 00:07:30.770
waiting for something
interesting to happen.

00:07:30.770 --> 00:07:34.740
The red indicates that the
program was waiting on a lock.

00:07:34.740 --> 00:07:35.700
Now exactly what thread viewer is this?

00:07:35.700 --> 00:07:39.640
What you does doesn't really matter
except to understand the pretty colors.

00:07:39.640 --> 00:07:42.730
The important thing to see is that this
is a somewhat realistic program and that

00:07:42.780 --> 00:07:44.560
you can use it to examine other programs.

00:07:48.830 --> 00:07:52.280
So, was there anything else
I wanted to say on that?

00:07:52.280 --> 00:07:53.380
Ah, yes.

00:07:53.500 --> 00:07:58.200
So, what's wrong with this?

00:07:58.200 --> 00:07:58.630
Any comments?

00:07:58.720 --> 00:08:02.520
Does anyone see anything that looks
obviously wrong with this program?

00:08:07.740 --> 00:08:08.600
There's a lot of green.

00:08:08.600 --> 00:08:09.600
Non-green.

00:08:09.600 --> 00:08:10.600
Oh, non-green.

00:08:10.600 --> 00:08:12.830
Actually, I was hoping on Thread Viewer,
but that's an interesting thing that

00:08:12.980 --> 00:08:15.240
the doc is actually doing a fair amount,
but that's mostly because

00:08:15.310 --> 00:08:16.660
Scott's playing with it.

00:08:16.660 --> 00:08:19.420
However, with Thread Viewer,
hopefully there's nothing that you see

00:08:19.420 --> 00:08:22.540
here that makes Thread Viewer look bad.

00:08:22.540 --> 00:08:25.100
And that's the first lesson
I want you to take home tonight,

00:08:25.100 --> 00:08:27.210
which is that usually if you
just go and look at a program,

00:08:27.340 --> 00:08:30.120
you're not going to be able to
tell that there's necessarily

00:08:30.290 --> 00:08:31.910
any performance problems.

00:08:32.430 --> 00:08:35.940
In order to understand performance,
you need to do measurement.

00:08:35.940 --> 00:08:37.370
You need to do software metrics.

00:08:37.370 --> 00:08:40.300
You actually need to look at the
program and measure things like how

00:08:40.400 --> 00:08:43.700
much memory it's using and measuring
how much time it takes to do things.

00:08:43.700 --> 00:08:45.120
You need this for two reasons.

00:08:45.300 --> 00:08:48.430
The first one is so that you can actually
look at it and say whether those numbers

00:08:48.430 --> 00:08:50.110
actually match your expectations.

00:08:50.270 --> 00:08:54.060
Did I really expect it to take
two seconds to load that web page?

00:08:54.060 --> 00:08:58.060
The second thing is that when that
system or as the system changes,

00:08:58.060 --> 00:08:59.620
you want to be able to note regressions.

00:08:59.620 --> 00:09:01.720
And unless you wrote down
how it behaved two weeks ago,

00:09:01.720 --> 00:09:04.220
you won't actually notice
that you've been losing 5%

00:09:04.240 --> 00:09:06.410
performance every single time.

00:09:07.050 --> 00:09:09.830
Thus, you need to write stuff down.

00:09:09.830 --> 00:09:11.500
And in fact,
for the projects that I work on,

00:09:11.500 --> 00:09:13.590
the ones that are
relatively time critical,

00:09:13.670 --> 00:09:16.880
I actually have a checklist of things
that I measure and I write them down

00:09:16.900 --> 00:09:18.400
and just stick them in a folder.

00:09:18.400 --> 00:09:21.160
It's low tech, but it works.

00:09:22.480 --> 00:09:26.470
So we don't know how the system
performs and we'd like to find that out.

00:09:26.470 --> 00:09:29.280
What we can do is we can start
by looking at overall system

00:09:29.480 --> 00:09:34.360
behavior and we'll start by
running a text utility called top.

00:09:35.600 --> 00:09:39.590
Now what you see on top here is
at the very top of the screen you

00:09:39.590 --> 00:09:41.520
see information about the system.

00:09:41.540 --> 00:09:44.940
The information below
represents individual lines

00:09:45.010 --> 00:09:47.850
referring to a single process.

00:09:49.020 --> 00:09:52.860
So some of the things that we want to
look at is-- or some of the things that

00:09:52.860 --> 00:09:56.870
I find very important or interesting
that you should probably look at is,

00:09:56.870 --> 00:10:00.290
first of all, the page in and page out
rates at the very top.

00:10:02.240 --> 00:10:06.080
So what the line that says page ins,
page outs is showing us is

00:10:06.080 --> 00:10:09.050
it's telling us something about
the virtual memory system.

00:10:09.210 --> 00:10:11.550
Specifically,
that line alone is telling us

00:10:11.880 --> 00:10:15.070
how many pages the virtual memory
system is writing out to disk

00:10:15.070 --> 00:10:16.710
and how many it's bringing in.

00:10:16.900 --> 00:10:19.960
The first number represents the
number of pages that have been

00:10:20.040 --> 00:10:21.980
moved since the system was rebooted.

00:10:22.250 --> 00:10:26.480
The number in parentheses represents
the number of pages that have been paged

00:10:26.480 --> 00:10:29.420
in or paged out in the last second.

00:10:29.420 --> 00:10:31.970
The important number is the
one in parentheses for me.

00:10:32.110 --> 00:10:34.060
What I tend to say is that
you should look at that,

00:10:34.180 --> 00:10:37.160
you should look at top every now
and then when your system's running.

00:10:37.210 --> 00:10:41.220
If you ever see those numbers go
above zero for any length of time,

00:10:41.260 --> 00:10:44.150
you know, if they stay at 20 or at 50,
that usually means that

00:10:44.250 --> 00:10:45.150
you're paging a lot.

00:10:45.220 --> 00:10:48.710
It usually means that the amount of
physical memory you have is not enough

00:10:48.710 --> 00:10:51.000
for all the things you're running,
which may indicate that your

00:10:51.120 --> 00:10:54.380
app is taking up too much memory
or that something else is.

00:10:55.000 --> 00:10:58.580
Now if you're hitting 50 or 100,
then basically your system is thrashing.

00:10:58.830 --> 00:11:01.110
It's spending all of its time
throwing pages out to disk

00:11:01.170 --> 00:11:02.770
and bringing them back in.

00:11:02.770 --> 00:11:07.300
And at some point around 50 or 100,
you're going to be hitting the limits of

00:11:07.340 --> 00:11:09.230
the disk that it can't page any faster.

00:11:09.500 --> 00:11:11.520
And so if you ever see
anything at that level,

00:11:11.520 --> 00:11:13.820
that means that your
system is really hosed.

00:11:14.520 --> 00:11:17.810
Oh, that's a technical term.

00:11:18.640 --> 00:11:20.560
The other thing that's
interesting is on the bottom part.

00:11:20.600 --> 00:11:22.720
You can see that in
the list of processes,

00:11:22.720 --> 00:11:26.400
there's a column that says percent CPU,
which says which tasks are

00:11:26.400 --> 00:11:28.350
actually spending the time running.

00:11:28.360 --> 00:11:32.360
And we can see here that ThreadViewer
is responsible for 3% or 4% of the CPU.

00:11:32.440 --> 00:11:35.060
So it's not taking up a lot of CPU time.

00:11:35.390 --> 00:11:38.720
We can also see a column far
to the right that says RPRVT.

00:11:38.720 --> 00:11:42.220
That's the other one that I think is
interesting that you should look at.

00:11:42.220 --> 00:11:46.620
RPRVT stands for Resident Private Memory.

00:11:46.690 --> 00:11:52.640
It's telling how much memory is in
physical memory on the computer.

00:11:53.120 --> 00:11:57.960
and it's telling you how much is actually
private to that application alone.

00:11:58.050 --> 00:12:01.440
And this number tends to be a good
measure of how much memory your

00:12:01.440 --> 00:12:04.700
application needs in order to run.

00:12:04.780 --> 00:12:07.540
So go look at that number when
you want to get a rough footprint.

00:12:07.610 --> 00:12:09.570
Now notice if you have a
lot of things in memory,

00:12:09.590 --> 00:12:11.910
there could be a lot of memory paged out,
sitting out on disk.

00:12:11.980 --> 00:12:14.940
And in that case,
it would not be resident.

00:12:15.040 --> 00:12:18.910
So on a heavily loaded system,
that number may not make as much sense.

00:12:19.720 --> 00:12:23.120
Now, one thing we should do is
let's show another mode of Top.

00:12:23.260 --> 00:12:25.100
Top has a huge number of modes.

00:12:25.150 --> 00:12:27.740
And in fact,
this is probably as good a time

00:12:27.740 --> 00:12:28.900
as any to try to sell books.

00:12:28.900 --> 00:12:32.780
There is a book called
Inside Mac OS X Performance.

00:12:32.780 --> 00:12:36.610
It's available, I believe,
on the CD that you got in your bags.

00:12:36.640 --> 00:12:38.400
It's available on the web.

00:12:38.430 --> 00:12:41.390
And it's available from Fatbrain,
nicely bound like this.

00:12:41.520 --> 00:12:44.000
And it describes a lot
of the performance tools.

00:12:44.000 --> 00:12:46.450
And it,
along with the various manuals for Top,

00:12:46.480 --> 00:12:48.660
will tell you what some of the flags are.

00:12:48.660 --> 00:12:51.370
What I'm going to do is I'm
going to show you-- or Scott's

00:12:51.370 --> 00:12:53.450
already going to show you--

00:12:54.080 --> 00:12:57.660
A option to top, the top-d option,
extremely clear and

00:12:57.700 --> 00:13:00.200
extremely user friendly,
which gives us a few other

00:13:00.230 --> 00:13:02.100
pieces of information.

00:13:02.160 --> 00:13:03.860
We can see the number of page faults.

00:13:04.250 --> 00:13:09.490
The two I'm going to point at is CSW,
which is the number of context switches,

00:13:09.490 --> 00:13:10.890
way to the right.

00:13:11.260 --> 00:13:13.390
and messages sent, messages received.

00:13:13.420 --> 00:13:16.410
That's how many Mach messages are
getting sent around in the system.

00:13:16.410 --> 00:13:18.180
How many have to be received.

00:13:18.240 --> 00:13:21.050
Now, there's a reason why
I'm telling you this.

00:13:21.160 --> 00:13:23.360
If Scott actually plays
around with Thread Viewer,

00:13:23.360 --> 00:13:26.440
I guess it is kind of,
what we see is that the

00:13:26.620 --> 00:13:29.200
number of context switches,
that is the number of times

00:13:29.200 --> 00:13:32.420
that something got paged out so
that something else could run,

00:13:32.490 --> 00:13:35.810
is pretty high for both the window
manager and for Thread Viewer.

00:13:35.820 --> 00:13:38.830
And the number of messages sent
and received is relatively high.

00:13:39.010 --> 00:13:41.470
Does Does anyone know
why that's happening?

00:13:44.030 --> 00:13:46.130
Yes, I heard it.

00:13:46.230 --> 00:13:49.030
What's happening is that
Thread Viewer is drawing,

00:13:49.030 --> 00:13:51.660
and the drawing is not just
done by the Thread Viewer.

00:13:51.660 --> 00:13:54.900
It's actually done by Thread
Viewer and Window Manager together.

00:13:55.050 --> 00:14:01.690
And so at times when the
Thread Viewer is doing drawing,

00:14:01.690 --> 00:14:01.690
it actually has to communicate
with the Window Manager.

00:14:01.890 --> 00:14:04.500
So this is the second
take home lesson today,

00:14:04.560 --> 00:14:08.360
which is that sometimes the program that
you're running can't be measured just

00:14:08.360 --> 00:14:10.860
by looking at that program in isolation.

00:14:11.100 --> 00:14:13.950
You actually have to look at
daemons and servers that might

00:14:14.030 --> 00:14:15.530
also be doing computation.

00:14:15.850 --> 00:14:18.620
In the case of drawing,
you want to take a look at the window

00:14:18.620 --> 00:14:20.520
manager and at your application.

00:14:20.550 --> 00:14:22.540
There have been times where I've
seen an application that looked

00:14:22.540 --> 00:14:23.540
like it was performing great.

00:14:23.620 --> 00:14:26.780
It's only taking 30 percent of
the CPU doing a lot of drawing.

00:14:26.790 --> 00:14:30.380
But the window manager was responsible
for 60 percent of the drawing.

00:14:30.380 --> 00:14:32.700
And so to understand overall problems,
you want to look at both.

00:14:34.300 --> 00:14:36.150
So, Scott,
I actually saw something kind of

00:14:36.150 --> 00:14:37.440
interesting in that last top display.

00:14:37.440 --> 00:14:39.510
Can you switch back to it?

00:14:41.170 --> 00:14:44.220
Okay,
so does anyone see anything interesting

00:14:44.220 --> 00:14:47.300
in this display for Thread Viewer?

00:14:49.390 --> 00:14:51.300
Well, we don't know that it's leaking.

00:14:51.300 --> 00:14:54.350
What we see is it looks like--

00:14:54.690 --> 00:14:57.530
Maybe there's a reason
why we're doing that.

00:14:57.530 --> 00:14:59.590
Maybe it's intentional,
but we're seeing the amount of

00:14:59.800 --> 00:15:01.310
resident private memory growing.

00:15:01.310 --> 00:15:04.360
At least I think it's growing,
but luckily top has this little

00:15:04.360 --> 00:15:06.330
plus there and it actually
says how much memory is going.

00:15:06.340 --> 00:15:11.900
So it seems to be going up and
it's using 37 megabytes of memory.

00:15:11.900 --> 00:15:14.300
Hmm, this looks suspicious.

00:15:14.300 --> 00:15:16.300
Let's see if we can
track down that problem.

00:15:16.300 --> 00:15:20.740
So one thing I can try to do is I can
try to prove that it's actually growing.

00:15:20.740 --> 00:15:24.180
Yes, I know that seems kind of silly,
but let's try it anyway.

00:15:24.600 --> 00:15:28.990
So what I want to do is I want
to see if I can actually

00:15:28.990 --> 00:15:31.940
prove that it's increasing.

00:15:31.960 --> 00:15:33.500
I want to identify a trend over time.

00:15:33.570 --> 00:15:36.010
Now one of the problems is I don't
really have a tool for this.

00:15:36.010 --> 00:15:37.770
Okay, I'm not doing my job.

00:15:37.810 --> 00:15:41.580
However, I can show you how you
can make a tool like this.

00:15:41.580 --> 00:15:45.400
What Scott can do is because
top is a command line tool,

00:15:45.400 --> 00:15:50.170
because it's a Unix non-Mac like tool,
that means that you can actually

00:15:50.350 --> 00:15:54.150
use the various features of
Unix to basically roll your own.

00:15:55.720 --> 00:15:59.990
So what Scott has done here is he
has typed in a little shell script.

00:15:59.990 --> 00:16:04.390
While one, call top with the -l option,
which is basically a one shot,

00:16:04.390 --> 00:16:07.470
give me the data, and grep,
which is a search for the

00:16:07.470 --> 00:16:10.860
line that starts with thread,
and print that out.

00:16:10.890 --> 00:16:12.760
Then sleep for a second and keep looping.

00:16:12.860 --> 00:16:16.070
And if Scott runs that,
what we start getting is we

00:16:16.170 --> 00:16:19.340
get the top data only grabbing
the line for thread viewer.

00:16:19.340 --> 00:16:25.640
Okay, so, and we can see here that the
amount of resident private memory,

00:16:25.640 --> 00:16:29.660
which is the last column visible on
where thread viewer is obscuring things,

00:16:29.660 --> 00:16:30.830
is actually growing.

00:16:31.040 --> 00:16:34.360
So, geez, I've got a memory leak.

00:16:34.360 --> 00:16:37.600
So what you should take out of
this is that command line tools,

00:16:37.600 --> 00:16:42.330
regardless of how Mac-like,
Mac-unlike they may be, still have uses.

00:16:42.380 --> 00:16:44.430
You can use them to roll your own tools.

00:16:44.560 --> 00:16:47.600
You can run them via telnet from
another machine if you want to

00:16:47.600 --> 00:16:49.600
inspect somebody else's machine.

00:16:49.600 --> 00:16:52.680
And you can run them even when the
window manager is not responding,

00:16:52.690 --> 00:16:56.630
or when you've got full screen
because you're debugging a game.

00:16:56.720 --> 00:16:59.320
So don't ignore the command line tools.

00:16:59.320 --> 00:17:00.520
They do have a place.

00:17:00.580 --> 00:17:09.520
Okay, so what are the possible
causes for the memory leak?

00:17:15.130 --> 00:17:17.160
One possibility is we're
not throwing away old data.

00:17:17.160 --> 00:17:18.200
Any other suggestions?

00:17:18.240 --> 00:17:19.240
History.

00:17:19.240 --> 00:17:20.240
History?

00:17:20.240 --> 00:17:23.500
Actually, that's a good one.

00:17:23.500 --> 00:17:26.730
Well, I'm not quite sure actually,
so let's see if we can track it down.

00:17:27.030 --> 00:17:29.930
One thing I'm going to do is I'm going
to try another command line tool.

00:17:30.090 --> 00:17:31.560
It's called Heap.

00:17:31.560 --> 00:17:34.920
Actually, Scott can try running it,
but the slide actually has the output.

00:17:35.030 --> 00:17:37.870
Heap is a tool that's intended
to give you a dump of how much

00:17:38.070 --> 00:17:40.710
memory you're using via malloc.

00:17:41.390 --> 00:17:44.410
and the important,
most of it actually is just saying,

00:17:44.410 --> 00:17:46.240
oh here are all the
buffers you're allocating.

00:17:46.280 --> 00:17:49.000
The important part is
the very top though.

00:17:49.050 --> 00:17:53.060
Because one possibility,
although a relatively unlikely one,

00:17:53.080 --> 00:17:55.300
is that we could have
fragmentation of our heap.

00:17:55.300 --> 00:17:56.600
You know,
we could allocate a million bytes

00:17:56.680 --> 00:17:59.820
and then free it and then allocate
ten bytes that fills that space and

00:17:59.820 --> 00:18:03.250
keep going so that the amount of heap
space keeps growing even though we're

00:18:03.370 --> 00:18:04.680
not actually using a lot of memory.

00:18:04.790 --> 00:18:07.030
What we can do with heap is we
can look and we can see that

00:18:07.030 --> 00:18:10.640
there's actually two heaps,
what we call zones,

00:18:10.670 --> 00:18:13.460
one for core graphics and one
for the default malloc zone,

00:18:13.490 --> 00:18:17.530
that represent about, say,
2.3 megabytes of memory.

00:18:17.620 --> 00:18:20.300
We can see that from the
overall allocation part.

00:18:20.320 --> 00:18:22.330
We also see that the total
number of nodes malloc,

00:18:22.430 --> 00:18:25.060
the amount of space we're actually using,
is about two megabytes.

00:18:25.130 --> 00:18:29.480
Okay, so 2.3 minus two megabytes is about
300,000 bytes of unallocated memory.

00:18:29.500 --> 00:18:33.300
This indicates that heap
fragmentation is not our problem.

00:18:33.680 --> 00:18:38.510
Okay, so this means it's memory
we're actually allocating.

00:18:38.860 --> 00:18:41.130
So,
another thing that we could do Thank you.

00:18:43.290 --> 00:18:46.260
is we have two megabytes
of memory that's allocated.

00:18:46.370 --> 00:18:50.950
Our virtual memory size is--our
resident private size was 38 megabytes.

00:18:51.000 --> 00:18:55.030
Okay, that leaves 36 megabytes we don't
know where it's coming from.

00:18:55.590 --> 00:18:57.260
So what we can do is we
can run another tool.

00:18:57.270 --> 00:18:59.260
This one's called VM Map.

00:18:59.300 --> 00:19:02.160
And what it does is it gives us
information on how our virtual

00:19:02.160 --> 00:19:04.360
memory system is laid out.

00:19:04.400 --> 00:19:06.050
Most of the time this isn't
something you really care about,

00:19:06.150 --> 00:19:08.350
but every now and then it
might actually be interesting.

00:19:08.400 --> 00:19:10.340
And if Scott runs it,

00:19:12.480 --> 00:19:13.510
Oh yes, you've got to run that.

00:19:13.670 --> 00:19:16.760
Basically what it'll do is it'll say,
okay, there's a page at address zero,

00:19:16.760 --> 00:19:19.130
it's 4,000 bytes and it's
basically unreadable.

00:19:19.280 --> 00:19:22.860
There's a page at 4,096 and
it has the executable and it

00:19:22.980 --> 00:19:25.400
runs for 14k or something.

00:19:25.600 --> 00:19:28.630
And there is the dump.

00:19:28.670 --> 00:19:30.760
Very friendly.

00:19:31.570 --> 00:19:33.340
However,
it's got some interesting information.

00:19:33.340 --> 00:19:36.320
And if you ever wondered about how the
application was laid out in memory,

00:19:36.320 --> 00:19:37.680
this is how you'd find it out.

00:19:37.740 --> 00:19:39.700
So that when you're looking
in the debugger and you find

00:19:39.700 --> 00:19:43.620
yourself at address 1016,
you can find out why you might be there.

00:19:43.620 --> 00:19:46.150
However,
the slide actually shows you a little

00:19:46.150 --> 00:19:48.560
more detail about what's actually there.

00:19:48.560 --> 00:19:50.490
In this case,
we see that there's a malloc

00:19:50.490 --> 00:19:54.670
buffer at address 139A000,
and there's about 16,000 bytes.

00:19:54.710 --> 00:19:56.300
So that's space for the heap.

00:19:56.330 --> 00:19:57.780
A couple others.

00:19:58.030 --> 00:20:01.650
And then below the 512,000
bytes allocated for one of the

00:20:01.650 --> 00:20:07.550
stacks for one of the threads,
we see address 14B6000, 4,000 bytes.

00:20:07.590 --> 00:20:09.420
146D, 4,000 bytes.

00:20:09.790 --> 00:20:12.390
146F000, 4,000 bytes.

00:20:12.460 --> 00:20:14.500
This may be the source of our leak.

00:20:18.910 --> 00:20:21.720
See where it says 4K on the slide here?

00:20:21.780 --> 00:20:25.900
Sorry, actually this is a good reason
why I shouldn't have done this.

00:20:25.900 --> 00:20:31.260
So over on the slide you can see
the 146d 4000 bytes read/write.

00:20:33.070 --> 00:20:35.750
So the interesting thing is we keep
allocating these 4,000 byte buffers.

00:20:35.900 --> 00:20:36.480
This is kind of odd.

00:20:36.500 --> 00:20:38.570
And that would have been
allocated via VM allocate.

00:20:38.730 --> 00:20:41.440
That was how you'd actually
create a virtual memory page.

00:20:41.570 --> 00:20:45.300
And I went in when I found this problem
and went into the debugger and said,

00:20:45.370 --> 00:20:47.520
what do I see there?

00:20:47.720 --> 00:20:50.400
And what I did, if you can,
can you bring up thread viewer?

00:20:50.530 --> 00:20:53.000
Is I found that basically
that page was zero,

00:20:53.060 --> 00:20:55.930
that virtual memory space was zero,
except that there were a couple

00:20:56.030 --> 00:20:59.240
numbers that looked oddly like those
thread IDs to the left hand side,

00:20:59.240 --> 00:21:02.190
the 5A035903.

00:21:02.910 --> 00:21:05.260
which made me think, oh,
this may have something to do with, like,

00:21:05.280 --> 00:21:06.200
you know, a system buffer or something.

00:21:06.200 --> 00:21:10.670
And it turns out that what's
actually happening here is that

00:21:10.780 --> 00:21:14.830
the Mach APIs sometimes will
return you basically a buffer.

00:21:14.850 --> 00:21:19.300
It'll return you a VM allocated
page that you actually have to free.

00:21:19.530 --> 00:21:22.080
And the code,
which is actually in one of the

00:21:22.150 --> 00:21:26.010
libraries that ThreadViewer depends on,
does something like, hey,

00:21:26.190 --> 00:21:28.450
tell me all the threads that
are associated with this task,

00:21:28.480 --> 00:21:30.360
and you get a buffer back
that contains it all.

00:21:30.360 --> 00:21:34.350
And you need to make sure to free it,
but that yellow line wasn't there.

00:21:34.650 --> 00:21:38.950
and by adding that line we
got rid of the memory leak.

00:21:38.950 --> 00:21:41.560
Now this is not a case
you will ever run into.

00:21:41.810 --> 00:21:46.570
None of you will ever, ever,
ever hopefully use the Mach calls

00:21:46.570 --> 00:21:48.490
to find out what threads exist.

00:21:48.540 --> 00:21:51.280
This is something I'm doing
as a performance tool.

00:21:51.350 --> 00:21:53.450
However,
what you should take out of this is

00:21:53.650 --> 00:21:56.960
first of all I had to use multiple
tools to track this down and I had

00:21:56.980 --> 00:22:00.480
to compare their output to actually
figure out what the problem is.

00:22:00.560 --> 00:22:02.480
This is one of the problems
with performance that usually

00:22:02.480 --> 00:22:05.110
the tools aren't just going to
drop an answer out in your hand.

00:22:05.300 --> 00:22:08.240
Plan on inspecting things and
thinking about them to actually

00:22:08.240 --> 00:22:09.700
find out what the causes are.

00:22:12.200 --> 00:22:17.900
[Transcript missing]

00:22:19.120 --> 00:22:24.190
Now, we saw that we were using about 2.3
megabytes of memory in the heap.

00:22:24.240 --> 00:22:25.920
That is,
in memory we allocated via malloc.

00:22:26.080 --> 00:22:28.500
And that seems a little suspicious too.

00:22:28.620 --> 00:22:33.010
So, what are some of the reasons why we
might actually allocate that memory?

00:22:33.340 --> 00:22:35.580
Well, one thing is we could just have
really big data structures.

00:22:35.580 --> 00:22:40.620
We could be allocating two megabytes of
data structures because we want them.

00:22:41.900 --> 00:22:43.740
A second thing is that we
could be caching things.

00:22:43.740 --> 00:22:45.700
Even though we don't need them,
we're actually saving them just

00:22:45.700 --> 00:22:47.080
in case we need them again.

00:22:47.110 --> 00:22:49.510
And a third case is that we could
actually be creating things and

00:22:49.510 --> 00:22:50.720
just forgetting to destroy them.

00:22:50.860 --> 00:22:53.610
And there's a huge
amount of other things.

00:22:53.940 --> 00:22:57.110
But the problem is
that with any of these,

00:22:57.110 --> 00:23:01.220
the more memory you allocate via malloc,
basically the system will just

00:23:01.220 --> 00:23:02.280
keep giving you the memory.

00:23:02.400 --> 00:23:04.630
And eventually what happens is the
system grows to the point where it

00:23:04.630 --> 00:23:06.080
won't fit in physical memory anymore.

00:23:06.130 --> 00:23:09.480
You start thrashing,
or you start swapping stuff out to disk,

00:23:09.480 --> 00:23:12.260
and as a result your system slows down.

00:23:12.980 --> 00:23:15.570
So there's two tools I'm going to
show you that will help you track

00:23:15.730 --> 00:23:18.190
down why memory is being allocated.

00:23:18.190 --> 00:23:21.440
The first one's called malloc_debug,
and it tries to show you via a call

00:23:21.440 --> 00:23:25.130
graph where memory is allocated so that
you can track down the big allocations.

00:23:25.180 --> 00:23:29.170
The second one called object_alloc
tries to refer to things according

00:23:29.240 --> 00:23:30.880
to how many objects exist.

00:23:32.510 --> 00:23:35.010
So we'll start off with malloc-debug.

00:23:35.010 --> 00:23:38.730
And Scott can bring that up.

00:23:38.810 --> 00:23:44.430
What we do with malloc-debug is we select
an application and we can launch it.

00:23:47.830 --> 00:23:52.640
and Scott can connect and say okay
I want to look at the dock and

00:23:52.640 --> 00:23:53.560
we get our little thread viewer

00:23:56.590 --> 00:24:00.900
And now Scott can go back to
malloc-debug and hit the update button.

00:24:00.900 --> 00:24:03.890
And we find out that to get
to that point in the code,

00:24:03.890 --> 00:24:06.670
we needed 2.6 megabytes of memory.

00:24:06.770 --> 00:24:10.510
Now, ThreadViewer isn't that complex,
so this seems a little suspicious to me.

00:24:10.770 --> 00:24:14.670
Now, like I said,
malloc-debug works off a call graph.

00:24:14.700 --> 00:24:18.420
So what it's doing is it's showing the
various functions that were called.

00:24:18.420 --> 00:24:20.360
So for example,
Scott can start clicking on start,

00:24:20.360 --> 00:24:22.980
which is a secret
function in the runtime,

00:24:22.980 --> 00:24:25.250
which you don't need to know about.

00:24:25.400 --> 00:24:27.110
But start at main.

00:24:27.310 --> 00:24:29.880
Okay, so main is the top of our program.

00:24:30.010 --> 00:24:33.400
And the number to the side of that,
the 1.3 megabytes,

00:24:33.400 --> 00:24:38.220
represents the amount of memory
allocated by main and everything below

00:24:38.460 --> 00:24:41.090
it by all the functions it calls.

00:24:41.160 --> 00:24:42.930
The rest of the memory is
allocated on a different thread

00:24:42.940 --> 00:24:44.200
that isn't started at main.

00:24:44.200 --> 00:24:46.280
That's why you don't see it.

00:24:46.710 --> 00:24:51.720
and what we can see is to the right
of main 1.3 megabytes of memory was

00:24:51.760 --> 00:24:56.380
allocated in NS application main
and 10,000 bytes was allocated in

00:24:56.380 --> 00:24:57.750
calls to allocate unneeded buffer.

00:24:57.810 --> 00:25:00.240
Both of these were called
directly by main and if you went

00:25:00.240 --> 00:25:01.950
to the source code you see that.

00:25:02.320 --> 00:25:03.560
Okay, NS Application Main.

00:25:03.560 --> 00:25:06.120
You don't need to worry about
that because that is the

00:25:06.120 --> 00:25:09.860
root of the Cocoa framework,
and that's how you get it started.

00:25:09.860 --> 00:25:11.600
So of course there's going to
be a lot of memory allocated.

00:25:11.600 --> 00:25:14.470
The Call to Allocate Unneeded Buffer,
which calls malloc,

00:25:14.470 --> 00:25:17.070
allocates a single 10,000 byte buffer.

00:25:17.200 --> 00:25:20.550
Scott can look at the listing
below where it shows the buffers,

00:25:20.550 --> 00:25:22.780
double click on it, and get a hex dump.

00:25:22.830 --> 00:25:26.080
And we can find out that that buffer,
we can look at the buffer and maybe

00:25:26.140 --> 00:25:27.780
we can detect what the cause is.

00:25:27.800 --> 00:25:31.860
And it has a string that says
this buffer isn't really needed.

00:25:34.140 --> 00:25:38.600
Okay, so that's obviously a
buffer we can get rid of.

00:25:38.630 --> 00:25:40.370
I think.

00:25:41.720 --> 00:25:43.760
But this shows you how you can
walk through the call graph and you

00:25:43.760 --> 00:25:46.530
can actually find where there are
allocations and you can say hey this

00:25:46.530 --> 00:25:49.050
looks suspicious and try to fix it.

00:25:49.330 --> 00:25:52.220
And if you understand how your
program is structured top down,

00:25:52.220 --> 00:25:53.320
this makes a lot of sense.

00:25:53.320 --> 00:25:55.650
And you can walk through the tree
looking for things that are interesting,

00:25:55.650 --> 00:25:59.160
because this is basically sort of a scrub
your nose on the screen until you see

00:25:59.160 --> 00:26:01.160
something interesting kind of problem.

00:26:02.030 --> 00:26:05.610
Now the other thing that we can do is
there's times where we don't really

00:26:05.770 --> 00:26:07.800
care how we got there from main,
but we're sort of curious

00:26:07.830 --> 00:26:11.660
about who was calling malloc
directly to try to assign blame.

00:26:11.750 --> 00:26:15.760
So Scott can switch the mode of
display from standard to inverted.

00:26:15.760 --> 00:26:18.740
And this means we're looking at
the call tree from the bottom up.

00:26:18.870 --> 00:26:23.010
So for example,
we can see here that there's 740,000

00:26:23.010 --> 00:26:25.320
bytes allocated in calls to malloc.

00:26:27.840 --> 00:26:34.450
and 489,000 bytes of that was
when NSZoneMalloc called malloc.

00:26:34.500 --> 00:26:36.920
That's the Objective-C version of malloc.

00:26:36.990 --> 00:26:40.020
And 290,000 of that was
when we called NSReadImage,

00:26:40.020 --> 00:26:43.120
which called NSZoneMalloc,
which called malloc.

00:26:43.470 --> 00:26:45.510
Now,
ThreadViewer is a pretty simple program.

00:26:45.520 --> 00:26:46.520
It doesn't have any icons.

00:26:46.520 --> 00:26:47.460
It doesn't have any pictures.

00:26:47.460 --> 00:26:51.870
And so the idea that NSReadImage
is responsible for 290,000

00:26:51.870 --> 00:26:53.600
bytes seems a little suspicious.

00:26:53.740 --> 00:26:57.540
And so we can start walking up
the call graph to see who called

00:26:57.740 --> 00:27:00.060
what to get to NSReadImage.

00:27:00.180 --> 00:27:02.720
Now,
if we go up past all the AppKit stuff

00:27:02.910 --> 00:27:06.470
and we can see--actually,
Scott, can you point at where the library

00:27:06.470 --> 00:27:08.030
and the application name are?

00:27:08.060 --> 00:27:10.710
So we can see that we end up in a
function called ImageForProcess,

00:27:10.720 --> 00:27:12.460
which is in the ThreadViewer library.

00:27:12.460 --> 00:27:14.970
That's what the thing
in parentheses means.

00:27:15.140 --> 00:27:17.250
So this is my code.

00:27:17.540 --> 00:27:20.430
And this is what's being
called to actually cause those

00:27:20.840 --> 00:27:23.150
NS read images to be called.

00:27:23.940 --> 00:27:27.800
Now, unfortunately I know this code
and I know what that function is.

00:27:27.950 --> 00:27:32.750
That's the function that handles the
attached dialog box for Thread Viewer,

00:27:32.780 --> 00:27:34.800
which Scott can bring up.

00:27:36.660 --> 00:27:39.320
And so even though that
dialog box had been closed,

00:27:39.320 --> 00:27:41.610
those icons were still around.

00:27:42.230 --> 00:27:47.090
and the reason why was that basically
Thread Viewer was trying to be clever.

00:27:47.090 --> 00:27:50.040
It decided it would be really smart
if it could cache those icons,

00:27:50.040 --> 00:27:53.050
if it could keep them in memory
after the dialog box closed,

00:27:53.300 --> 00:27:56.000
just on the off chance it
was going to need them again.

00:27:58.420 --> 00:27:59.600
Now the--and that's nice.

00:27:59.830 --> 00:28:02.530
I don't have to read them in off of disk,
they're sitting in memory.

00:28:02.630 --> 00:28:06.070
The problem is because of virtual memory,
as soon as the application

00:28:06.160 --> 00:28:08.710
got a little too large,
what would happen is we'd take those

00:28:08.850 --> 00:28:12.300
pages that hadn't really been touched in
a while and we'd write them out to disk.

00:28:12.310 --> 00:28:16.060
And so when we actually did bring
up that attached dialog box again,

00:28:16.060 --> 00:28:18.940
well, to avoid reading the disk,
we'd go to memory,

00:28:19.010 --> 00:28:21.400
which would require reading the disk.

00:28:22.530 --> 00:28:25.280
Once again, the idea of saving memory is
pretty important because it's

00:28:25.400 --> 00:28:28.540
probably going to be cheaper just
to get the stupid thing off disk.

00:28:28.920 --> 00:28:31.940
And so the idea of caching
those icons was really stupid.

00:28:32.080 --> 00:28:35.700
What would have been much better was
when the dialog box went away if we

00:28:35.700 --> 00:28:39.640
just got rid of those icons and then we
recreated them on the fly the next time

00:28:39.640 --> 00:28:42.820
we opened that dialog box because it
was a relatively infrequent occurrence.

00:28:45.680 --> 00:28:52.000
Oh, one thing I haven't mentioned here
that I should is that everything

00:28:52.000 --> 00:28:54.620
that you saw in malloc_debug refers
to currently allocated blocks.

00:28:54.780 --> 00:28:56.400
So if something was
allocated and then freed,

00:28:56.420 --> 00:28:57.720
you won't see it in malloc_debug.

00:28:57.720 --> 00:29:00.890
So that's something to
keep in mind as you use it.

00:29:01.850 --> 00:29:06.020
Now, malloc-debug has a number of
other features that are useful.

00:29:06.020 --> 00:29:07.850
For example,
one of the things it can do is it can

00:29:07.950 --> 00:29:12.170
help you identify leaks in your program,
places where you're allocating memory

00:29:12.170 --> 00:29:15.220
and then you forget to deallocate it.

00:29:15.260 --> 00:29:18.990
Now, these are really important to
track down because leaked memory,

00:29:18.990 --> 00:29:21.840
stuff that you aren't--you're
forgetting to free and never free,

00:29:22.250 --> 00:29:24.020
basically sit in memory,
they occupy space,

00:29:24.020 --> 00:29:27.580
they keep you from having the
stuff you're using close together.

00:29:27.810 --> 00:29:30.410
And if you have a long running
app that goes for days,

00:29:30.410 --> 00:29:32.310
like a server app,
you're memory just keeps

00:29:32.310 --> 00:29:34.760
growing and growing and growing
until 30 days into the thing,

00:29:35.000 --> 00:29:38.500
suddenly the app crashes and
you have absolutely no idea why.

00:29:38.600 --> 00:29:42.620
So, and even if you have a small,
short app, this is still going

00:29:42.640 --> 00:29:43.800
to affect performance.

00:29:45.520 --> 00:29:47.750
You know,
this is a particularly strong point for

00:29:47.750 --> 00:29:50.160
me because there's at least one game
that I've been trying to play lately

00:29:50.160 --> 00:29:52.940
that after about four hours of playing,
it has a memory leak and eventually

00:29:52.940 --> 00:29:54.050
it sort of runs out of memory.

00:29:54.050 --> 00:29:58.900
And so please, if you write games,
don't do this.

00:30:01.850 --> 00:30:03.300
So Malictabug can help us detect this.

00:30:03.300 --> 00:30:06.400
What we can do is we can
switch from show me all things,

00:30:06.400 --> 00:30:08.440
or show me the things that
have recently changed,

00:30:08.440 --> 00:30:09.180
which is new.

00:30:09.180 --> 00:30:10.620
And we can switch to
the definite leaks mode.

00:30:10.760 --> 00:30:13.510
Show me only the buffers that aren't
referenced anywhere in memory.

00:30:13.620 --> 00:30:15.610
The way it does that is
basically garbage collection.

00:30:15.620 --> 00:30:19.140
It goes scanning in memory,
looking for pointers to the buffer.

00:30:19.620 --> 00:30:22.900
and any buffer in that's been malic
that it can't find a pointer to it,

00:30:22.900 --> 00:30:24.420
it assumes it's unreferenced.

00:30:24.420 --> 00:30:26.940
Because if we don't have a pointer to it,
there's absolutely no way we can free it.

00:30:28.150 --> 00:30:31.880
And we see here we have
about 10,000 bytes leaked.

00:30:31.880 --> 00:30:36.740
And Scott can click on the malloc and see
that there's a case in image for process,

00:30:37.080 --> 00:30:39.600
interesting,
where we're allocating a couple buffers

00:30:39.680 --> 00:30:42.680
and we've lost the pointers to them,
so we can't free them.

00:30:42.720 --> 00:30:43.810
I actually looked at this code.

00:30:43.940 --> 00:30:48.080
What I'm doing here is,
when I go and read the icons, basically,

00:30:48.080 --> 00:30:51.040
I have to make a list of-- or I have to
grab the command line to find the icons.

00:30:51.140 --> 00:30:52.960
You don't want to know.

00:30:53.150 --> 00:30:57.580
And I keep a buffer around
for that list of arguments.

00:30:57.600 --> 00:30:59.720
And in most cases,
at the very end of the routine,

00:30:59.720 --> 00:31:01.080
I actually free it.

00:31:01.160 --> 00:31:04.920
However, in a few cases,
I would find that there was an

00:31:04.920 --> 00:31:08.080
extraordinary situation that I knew
I wasn't going to find an icon.

00:31:08.340 --> 00:31:12.330
And so I would basically
just exit that routine.

00:31:12.440 --> 00:31:15.380
But I forgot to release the buffer.

00:31:15.410 --> 00:31:17.570
No one else does this, right?

00:31:19.750 --> 00:31:21.820
So, yeah,
this really is just one of those things

00:31:21.870 --> 00:31:24.990
that I wanted because it helps me.

00:31:26.980 --> 00:31:28.860
So this is another case
where MalictiBug helped us.

00:31:28.860 --> 00:31:30.350
And in fact,
it's showing us another lesson

00:31:30.430 --> 00:31:32.520
of software engineering,
which is if you keep finding

00:31:32.520 --> 00:31:34.630
bugs in the same code,
maybe this means you need

00:31:34.630 --> 00:31:36.240
to rewrite this routine.

00:31:37.510 --> 00:31:40.060
Now there's a couple
little things to remember.

00:31:40.200 --> 00:31:43.040
One is that you may say, oh,
you're only leaking 10,000 bytes.

00:31:43.060 --> 00:31:43.870
Who really cares?

00:31:43.950 --> 00:31:45.560
What's 10,000 bytes?

00:31:45.630 --> 00:31:49.760
However, the way that malloc-debug's
leak detection algorithm works,

00:31:49.760 --> 00:31:52.210
as you remember,
was if you can't find a pointer

00:31:52.210 --> 00:31:55.540
to this buffer anywhere,
then it must be leaked.

00:31:55.540 --> 00:31:58.350
This first of all means it doesn't
do well with circularly linked lists,

00:31:58.640 --> 00:32:00.710
because everything will
have a pointer to it,

00:32:00.710 --> 00:32:02.160
and so it'll never be linked.

00:32:02.180 --> 00:32:05.110
Or, I mean, it'll never be leaked.

00:32:05.110 --> 00:32:09.520
Also, that means that if you
have a tree data structure,

00:32:09.870 --> 00:32:13.050
The root doesn't have any pointers to it,
but everything else is pointed to,

00:32:13.050 --> 00:32:14.170
and so it's never leaked.

00:32:14.180 --> 00:32:16.230
So every leak that you see in
malloc_debug may be important,

00:32:16.230 --> 00:32:18.560
and so you should go track it down.

00:32:22.360 --> 00:32:25.080
Finally,
one more thing about malloc-debug.

00:32:25.130 --> 00:32:27.030
This doesn't necessarily
impact performance,

00:32:27.120 --> 00:32:28.700
but it does impact correctness.

00:32:28.750 --> 00:32:33.110
Malloc-debug can also help you
track down pointer problems.

00:32:33.190 --> 00:32:36.070
and that's and what happens is that
uh... there's a number of bugs are really

00:32:36.120 --> 00:32:40.240
subtle intermittent examples include
uh... cases where you free a buffer

00:32:40.540 --> 00:32:43.820
but you continue to read and write from
it even though somebody else now has

00:32:43.820 --> 00:32:48.770
the buffer these are miserable suddenly
data is changing you have no idea why

00:32:50.530 --> 00:32:53.910
The second kind of bug that is really
nasty is when you have buffer overruns,

00:32:53.970 --> 00:32:56.880
where you have, let's say,
a string that's 40 bytes long,

00:32:56.880 --> 00:32:59.280
but you write 45 bytes into it.

00:32:59.370 --> 00:33:02.620
And so suddenly you trash
the next thing after it.

00:33:03.110 --> 00:33:06.620
Malik Debug can help you
track down both of these.

00:33:06.660 --> 00:33:09.230
Excuse me, Malik Debug can help
you find both of these.

00:33:09.370 --> 00:33:13.270
And the reason why I say help is
because what Malik Debug does is

00:33:13.270 --> 00:33:17.900
it tries to encourage your program
to crash if it does stupid things.

00:33:18.050 --> 00:33:22.710
What it does, and here Scott's brought
up the memory dump.

00:33:23.890 --> 00:33:26.540
See, do we have any?

00:33:26.660 --> 00:33:30.070
So one of the things that it
does is when you free memory,

00:33:30.070 --> 00:33:34.480
malloc-debug carefully goes and it
erases the contents of that buffer.

00:33:34.490 --> 00:33:37.810
And it replaces it with 55 hex.

00:33:38.180 --> 00:33:39.830
So we would have seen a
lot of cases-- actually,

00:33:39.830 --> 00:33:42.420
there we are-- of 55555555.

00:33:42.500 --> 00:33:45.680
So if you try to read data from
this buffer that you've freed,

00:33:45.690 --> 00:33:46.600
you'll get garbage.

00:33:46.630 --> 00:33:49.800
And hopefully,
you'll crash or you'll behave badly.

00:33:49.890 --> 00:33:54.730
If you try to treat those as pointers,
it's even better because 55, 55, 55,

00:33:54.820 --> 00:33:59.140
55 is almost always unallocated memory.

00:33:59.250 --> 00:34:02.530
And as soon as you touch it,
basically your app crashes.

00:34:03.300 --> 00:34:06.300
So if your application ever
crashes in malloc debug,

00:34:06.400 --> 00:34:08.640
it's trying to tell you something.

00:34:08.680 --> 00:34:09.890
Hook up with a debugger.

00:34:09.900 --> 00:34:11.780
There's instructions in malloc
debug to tell you how to

00:34:11.900 --> 00:34:13.100
track down these kind of bugs.

00:34:13.120 --> 00:34:15.340
And find the pointer problem,
because it'll save you

00:34:15.340 --> 00:34:17.340
a lot of grief later.

00:34:17.570 --> 00:34:20.210
The other thing that Malik
Debug does is it puts guard

00:34:20.210 --> 00:34:21.820
words on each end of a buffer.

00:34:21.920 --> 00:34:24.690
At the beginning of the buffer
it puts the string dead beef,

00:34:24.750 --> 00:34:26.030
the first part that's highlighted.

00:34:26.120 --> 00:34:28.440
So D-E-A-D-B-E-E-F.

00:34:28.440 --> 00:34:31.370
Then you have the buffer and then at
the end you have the thing beef dead.

00:34:31.490 --> 00:34:34.290
I did not make this up.

00:34:34.490 --> 00:34:37.400
It just came this way.

00:34:38.310 --> 00:34:43.450
And what will happen is that
if you overrun the buffer,

00:34:43.460 --> 00:34:47.840
Malictabug checks occasionally on
freeze to see whether anything happens.

00:34:47.970 --> 00:34:50.140
And if it ever finds that
those bytes have been changed,

00:34:50.260 --> 00:34:52.160
what it will do is it will
actually print out a warning.

00:34:52.270 --> 00:34:54.160
Unfortunately,
the warning goes out to the console,

00:34:54.160 --> 00:34:56.570
so please keep the console open
while you're running Malictabug.

00:34:56.800 --> 00:34:58.140
Yes, this is lame.

00:34:58.220 --> 00:34:58.960
We're working on it.

00:34:58.960 --> 00:35:01.300
We'll try to do something about it.

00:35:04.920 --> 00:35:07.060
Okay, thanks for that Scott.

00:35:07.060 --> 00:35:10.040
The second tool I'll show you is called
ObjectAlloc and it's intended for helping

00:35:10.040 --> 00:35:15.650
you understand how many objects you have
rather than where they were allocated.

00:35:16.090 --> 00:35:18.850
Specifically,
it's mostly useful if you're

00:35:18.850 --> 00:35:23.890
doing Objective C work or if
you're doing core foundation.

00:35:23.970 --> 00:35:28.130
What we can do is again
select an application.

00:35:28.220 --> 00:35:30.910
In this case it doesn't
understand bundles.

00:35:32.170 --> 00:35:34.450
and we can start the program
running and what happens is it

00:35:34.450 --> 00:35:35.690
gives us a little histogram.

00:35:35.700 --> 00:35:39.820
It shows us a bunch of numbers
grouped by the type of object and

00:35:39.820 --> 00:35:42.660
these little histograms for showing
how many objects we've created.

00:35:42.840 --> 00:35:45.900
The first number and the darkest
bar represent the current

00:35:45.900 --> 00:35:48.470
number of objects that exist.

00:35:48.550 --> 00:35:52.800
The second bar represents the peak
number of objects of that type

00:35:52.970 --> 00:35:54.580
that were created during the run.

00:35:54.580 --> 00:35:58.130
Okay, so the peak number that
ever existed at one time.

00:35:58.220 --> 00:36:00.880
The final bar, the lightest color,
represents the total number

00:36:00.880 --> 00:36:02.880
of objects of that type.

00:36:03.570 --> 00:36:07.520
Now, this way of organizing things is
really nice for certain types of tasks.

00:36:07.670 --> 00:36:09.510
For example,

00:36:10.750 --> 00:36:12.970
Well actually,
let me mention one other thing.

00:36:13.090 --> 00:36:15.340
One thing that it's good
for is identifying trends.

00:36:15.340 --> 00:36:17.440
So we can see motion here,
we can watch the numbers growing,

00:36:17.670 --> 00:36:21.210
and so it's very nice for seeing
that memory use is expanding,

00:36:21.210 --> 00:36:22.280
for example.

00:36:22.550 --> 00:36:25.030
A second thing is it can help us
when we're trying to prove various

00:36:25.030 --> 00:36:26.110
statements about our program.

00:36:26.120 --> 00:36:30.440
So, for example,
the thread viewer display basically

00:36:30.440 --> 00:36:33.590
keeps sampling the application,
finding out information about

00:36:33.590 --> 00:36:35.430
how it's currently running,
and throws those on the

00:36:35.430 --> 00:36:36.760
right-hand side of the display.

00:36:36.760 --> 00:36:40.700
The information that scrolls
off the left disappears.

00:36:40.700 --> 00:36:44.370
And the way that's done is with a
data structure called a thread data.

00:36:44.380 --> 00:36:47.310
And what happens is the new
thread data gets put on the

00:36:47.470 --> 00:36:51.070
right side of a big array,
and when the information becomes out

00:36:51.070 --> 00:36:53.240
of date because it's scrolled off,
they get thrown off

00:36:53.240 --> 00:36:55.660
on the left-hand side,
and hopefully the objects are destroyed.

00:36:55.660 --> 00:37:01.070
So one bug I could really imagine
doing is that I could be forgetting

00:37:01.170 --> 00:37:04.860
to delete them correctly,
resulting in the number of thread

00:37:05.010 --> 00:37:07.670
data is growing without bounds,
and eventually my system

00:37:07.750 --> 00:37:09.180
performance would degrade.

00:37:09.180 --> 00:37:12.240
So we can prove that,
and Scott has actually done this.

00:37:12.350 --> 00:37:15.420
We can actually click,
we can find thread data in

00:37:15.580 --> 00:37:18.860
Object Alex display and see
how many objects we have.

00:37:19.270 --> 00:37:24.540
And what we see here is that the current
number of thread data objects varies.

00:37:24.540 --> 00:37:26.460
It goes between about 45 and 55.

00:37:26.460 --> 00:37:29.980
The peak number was 62,
but we noticed that the total

00:37:29.980 --> 00:37:32.190
number is about 1,400 and growing.

00:37:32.200 --> 00:37:34.930
So this implies to me that
I actually did this correctly.

00:37:35.060 --> 00:37:35.960
So this isn't a bug.

00:37:36.020 --> 00:37:39.240
We're keeping the correct number of
objects in sort of our ring buffer,

00:37:39.240 --> 00:37:42.200
throwing new items on the right-hand
side and pulling objects off the left.

00:37:42.300 --> 00:37:44.980
So Object Alex has been
able to help us prove that.

00:37:47.020 --> 00:37:50.560
One other thing, how many of you are
Objective-C programmers or are interested

00:37:50.560 --> 00:37:53.100
in being Objective-C programmers?

00:37:54.450 --> 00:37:59.430
Okay, as you do test code,
make sure you use ObjectAlloc because one

00:37:59.430 --> 00:38:01.780
of the things that it's really great at,
that you may have seen when we

00:38:01.780 --> 00:38:06.010
actually brought up the attach panel,
is that you can tell it to keep track

00:38:06.080 --> 00:38:10.680
of every single time that you retain
or you release a data structure.

00:38:10.790 --> 00:38:14.050
Basically Objective-C has reference
counting and it only deletes the object

00:38:14.050 --> 00:38:16.720
when you haven't retained it anymore.

00:38:16.740 --> 00:38:19.740
And so if you have a program,
you can figure out whether you're

00:38:19.740 --> 00:38:22.330
actually destroying objects
correctly and if you're not,

00:38:22.330 --> 00:38:25.210
you can find out when you
retained it one too many times

00:38:25.230 --> 00:38:26.910
to keep it around in memory.

00:38:26.970 --> 00:38:30.490
So use ObjectAlloc,
especially on the example code.

00:38:31.960 --> 00:38:33.960
Thank you, Scott.

00:38:34.020 --> 00:38:36.430
OK, so that's memory.

00:38:36.490 --> 00:38:40.010
The third case of performance
we might try to track down

00:38:40.290 --> 00:38:42.990
is what code is executing.

00:38:43.880 --> 00:38:48.900
Now, there's a number of ways that
we could be using too much CPU.

00:38:48.980 --> 00:38:50.950
We could be executing code
that we don't need to,

00:38:50.950 --> 00:38:54.360
either that it's dead code or that it
somehow is not really doing a value

00:38:54.360 --> 00:38:57.000
that--calculating a value we care about.

00:38:57.000 --> 00:38:59.760
We could have an algorithm that's much
more expensive than we ever expected,

00:38:59.760 --> 00:39:03.480
something that's let's say a quadratic,
you know, an N squared algorithm

00:39:03.480 --> 00:39:04.980
rather than linear.

00:39:04.980 --> 00:39:07.180
We could have cases where there's
some operation that's much

00:39:07.290 --> 00:39:08.440
more expensive than we thought.

00:39:08.440 --> 00:39:11.870
One example that was pointed out in
the Carbon performance session was

00:39:11.870 --> 00:39:16.700
that now that we have home directories
that could be out on an NFS server,

00:39:16.700 --> 00:39:19.610
when you go and get your preferences,
you might be going across the network.

00:39:19.680 --> 00:39:22.440
And so something that may have been a
really quick grab that from the disk

00:39:22.440 --> 00:39:26.670
kind of operation suddenly may take
seconds to actually get a result back.

00:39:26.840 --> 00:39:28.840
and so you may not have expected
that certain operations would be

00:39:28.840 --> 00:39:30.710
as time consuming as they are.

00:39:30.710 --> 00:39:34.060
As you may have seen in Avi's keynote,
there's also the problem that you may

00:39:34.060 --> 00:39:37.240
be checking for events by polling,
by constantly checking and

00:39:37.240 --> 00:39:39.420
seeing where the mouse is,
for example,

00:39:39.450 --> 00:39:41.900
rather than waiting for something
to happen and having the system say,

00:39:41.900 --> 00:39:44.390
"Hey, by the way, something changed."

00:39:46.020 --> 00:39:51.910
In general, the normal way that you solve
this kind of problem is basically

00:39:51.910 --> 00:39:57.330
track down the biggest problem,
track down the most expensive routine,

00:39:57.330 --> 00:40:00.470
because if you can cut the
CPU cost of that biggest routine,

00:40:00.530 --> 00:40:02.150
if you can make it faster,
you're going to improve the

00:40:02.180 --> 00:40:03.960
performance of your system as a whole.

00:40:03.960 --> 00:40:06.510
So don't try doing the little things,
try doing the big things first.

00:40:08.240 --> 00:40:11.780
So what we'll do is if we want
to try to improve performance,

00:40:11.830 --> 00:40:13.820
what we want to do is
find the expensive calls,

00:40:13.840 --> 00:40:15.590
improve them.

00:40:15.730 --> 00:40:17.700
The tool for doing this,
or at least one of them,

00:40:17.700 --> 00:40:20.570
is called Sampler,
which Scott has just brought up.

00:40:22.520 --> 00:40:25.480
So with Sampler,
we can select an application or we can

00:40:25.480 --> 00:40:27.900
connect to something that already exists.

00:40:30.620 --> 00:40:35.940
And what we can do after running
it is start sampling that.

00:40:36.090 --> 00:40:38.870
And what sampling means is that
we stop the program occasionally

00:40:38.870 --> 00:40:40.080
and ask what's going on.

00:40:40.080 --> 00:40:42.380
So every 20 milliseconds we
stop the program and we say,

00:40:42.380 --> 00:40:45.210
"Where are you executing?" And we
get basically a stack backtrace.

00:40:45.240 --> 00:40:48.530
Then we let the program continue,
stop it again, get another backtrace,

00:40:48.530 --> 00:40:50.440
and keep going.

00:40:50.440 --> 00:40:53.670
And the advantage of this is
that for relatively little

00:40:53.670 --> 00:40:56.160
impact on the running system,
we can actually find out what

00:40:56.260 --> 00:40:57.590
code is most likely to be running.

00:40:57.730 --> 00:40:58.560
This is statistical.

00:40:58.560 --> 00:41:00.600
We don't know all the
things that ran in between,

00:41:00.770 --> 00:41:04.190
but we've got some good idea of
what we were actually seeing.

00:41:05.730 --> 00:41:10.980
So what Scott, I believe,
has done is he's actually done some

00:41:11.090 --> 00:41:14.200
sampling while ThreadViewer is drawing.

00:41:14.300 --> 00:41:18.920
So let's see if we can find out what
ThreadViewer is doing when it's running.

00:41:19.140 --> 00:41:22.240
Personally, I'm very concerned about how
much time it's taking to actually

00:41:22.240 --> 00:41:26.540
grab its samples to find out
what's running in ThreadViewer.

00:41:26.660 --> 00:41:30.230
And I want to find out how much time
ThreadViewer is spending drawing.

00:41:30.830 --> 00:41:33.080
Now, I know a little about this program.

00:41:33.080 --> 00:41:37.670
I know that thread 3 happens
to be where the data gathering

00:41:37.670 --> 00:41:39.200
goes on in Thread Viewer.

00:41:39.200 --> 00:41:43.970
And we see that thread 3 was
found executing 486 times.

00:41:44.100 --> 00:41:46.800
That's the number of
samples that were taken.

00:41:46.800 --> 00:41:50.490
All of those samples occurred
in the function pthreadbody,

00:41:50.510 --> 00:41:54.860
which was calling sample threads,
which is my code.

00:41:54.870 --> 00:41:57.650
And every time that we
stopped sample thread,

00:41:57.650 --> 00:41:59.260
we found one of two things.

00:41:59.260 --> 00:42:02.300
470 of the times we found
ourselves in uSleep,

00:42:02.300 --> 00:42:04.750
which is a way of basically
stopping for a few microseconds

00:42:04.750 --> 00:42:09.010
to wait for something to happen,
or actually wait for a fixed time.

00:42:09.010 --> 00:42:13.260
In 16 of the times we
stopped it out of 476 though,

00:42:13.280 --> 00:42:17.350
we found ourselves in this function
called Thread Viewer Controller.

00:42:19.040 --> 00:42:21.950
log, which happens to be the code that
actually does the logging that gets

00:42:22.030 --> 00:42:24.400
the information for Thread Viewer.

00:42:24.860 --> 00:42:29.130
and if we look at that we find on the
far right a sample stack and we find out

00:42:29.190 --> 00:42:32.190
that most of the time was actually doing
what's called sample once all threads

00:42:32.570 --> 00:42:36.530
which is getting the stack backtrace
which ThreadViewer can actually display.

00:42:36.890 --> 00:42:42.250
So what's happening here is that we
found that basically about 4% of the

00:42:42.250 --> 00:42:45.730
time that we actually looked at thread 3,
it was actually doing something.

00:42:45.740 --> 00:42:48.510
It was actually gathering data and the
rest of the time it was doing nothing.

00:42:48.660 --> 00:42:49.420
This seems pretty good.

00:42:49.500 --> 00:42:52.860
This means that the data gathering
is relatively cheap and that's

00:42:52.860 --> 00:42:54.910
really good for a performance tool.

00:42:55.320 --> 00:42:58.710
So Scott can actually just sort of
ignore all that because it doesn't look

00:42:58.780 --> 00:43:01.360
like there's a performance problem.

00:43:01.370 --> 00:43:04.760
We'll prune that out of the tree
so we don't have to look at it.

00:43:04.830 --> 00:43:07.160
And now we can look at thread zero,
which is the main thread

00:43:07.160 --> 00:43:09.270
where the drawing goes on.

00:43:09.740 --> 00:43:12.800
and there were about seven hundred and
twenty times that there was sampling or

00:43:12.800 --> 00:43:16.890
that we sampled the main thread and most
of the time it was in Maine which is not

00:43:16.890 --> 00:43:21.880
surprising and then it goes down into
this DPS next event and the block until

00:43:21.880 --> 00:43:28.060
next event is basically sitting in the
run loop it's not really doing very much

00:43:30.210 --> 00:43:36.330
So we can see here that 570 out of the
720 times we were sitting in CFRunLoop.

00:43:36.360 --> 00:43:37.590
OK, so this is kind of interesting.

00:43:37.600 --> 00:43:39.860
So 572 samples.

00:43:40.010 --> 00:43:44.200
552 of the times that we found
ourselves in CFRunLoop run,

00:43:44.310 --> 00:43:47.180
we were actually in a
function called Mach message.

00:43:48.400 --> 00:43:52.220
Geez, why are we spending so
much time in Mach message?

00:43:52.220 --> 00:43:53.000
I'll give you a hint.

00:43:53.000 --> 00:43:54.800
That actually happens
to be a kernel routine.

00:43:54.860 --> 00:43:58.960
So obviously there's something wrong with
the kernel because we're just sitting

00:43:58.960 --> 00:44:01.060
there in Mach message all the time.

00:44:01.080 --> 00:44:03.880
Actually,
does anybody know what's going on there?

00:44:09.860 --> 00:44:10.840
Thank you very much.

00:44:10.870 --> 00:44:13.000
We are waiting for a Mach message.

00:44:13.110 --> 00:44:16.180
Okay, so Mach message is basically
saying--sending off a message,

00:44:16.230 --> 00:44:19.000
probably sending it off to
like the Windows server or to

00:44:19.000 --> 00:44:21.310
whoever's giving us events saying,
"Hey,

00:44:21.310 --> 00:44:25.460
let me know when something actually--that
I actually care about happens,

00:44:25.460 --> 00:44:29.170
like the mouse moves or we need to
do a redraw." And so most of the

00:44:29.170 --> 00:44:31.820
time we're going to find ourselves
in Mach message overwrite trap.

00:44:31.820 --> 00:44:34.080
Please do not open a bug
against the kernel saying,

00:44:34.080 --> 00:44:37.350
"Hey, you guys, I keep finding my code
running in here." Okay,

00:44:37.510 --> 00:44:40.560
that's why you see
Mach message overwrite trap.

00:44:40.560 --> 00:44:42.860
The rest of the time, however,
we find ourselves in CF run loop and

00:44:42.860 --> 00:44:46.100
if we look up the sample stack and
look for where the numbering changes,

00:44:46.100 --> 00:44:49.790
where the tree diverges, so to speak,
we find ourselves eventually

00:44:49.790 --> 00:44:51.560
in thread view draw rect.

00:44:51.630 --> 00:44:55.140
And we find that only in 17
out of those 700 samples,

00:44:55.140 --> 00:44:57.970
okay, once again,
maybe 3 or 4% of the time,

00:44:57.970 --> 00:45:00.030
we found ourselves in
thread view draw rect,

00:45:00.080 --> 00:45:03.300
which is actually the thing
for doing the drawing.

00:45:03.480 --> 00:45:06.860
and ten of the times that we
sampled it and found it in DrawRect,

00:45:06.930 --> 00:45:11.110
we found it doing some NSString drawing,
and the rest of the time we

00:45:11.110 --> 00:45:12.670
found it drawing rectangles.

00:45:13.200 --> 00:45:16.360
So this implies that the drawing
code is pretty efficient too,

00:45:16.390 --> 00:45:18.020
though we weren't spending
very much time doing it.

00:45:18.080 --> 00:45:21.740
And this suggests that if I wanted to up
the redraw speed so that Thread Viewer

00:45:21.740 --> 00:45:25.280
wasn't just sort of flashing the screen
every second redrawing the display,

00:45:25.350 --> 00:45:28.420
I could probably make that
animation much better.

00:45:28.420 --> 00:45:30.500
So that's a good thing to know.

00:45:30.690 --> 00:45:32.410
Okay, so we didn't find a bug,
but we learned something about

00:45:32.500 --> 00:45:35.910
how we're actually going,
how we could improve this application.

00:45:36.440 --> 00:45:42.600
So one other thing, as I said,
this is sampling.

00:45:42.600 --> 00:45:44.460
There are a few other
tools for helping you out.

00:45:44.460 --> 00:45:47.670
There's a tool called Sample which
gives you similar data that's a command

00:45:47.670 --> 00:45:50.560
line tool that's really good for
finding out why the machine is hanging,

00:45:50.560 --> 00:45:53.750
for example,
or why an application is stuck in a loop.

00:45:53.830 --> 00:45:56.140
You can actually run Sample
and get a stack back trace.

00:45:56.380 --> 00:45:58.610
The other tool that you may
want to know about is GPROF.

00:45:58.610 --> 00:46:00.090
That's the standard Unix profiler.

00:46:00.230 --> 00:46:01.670
We actually have that on our system.

00:46:01.670 --> 00:46:04.760
It generates a text report saying
here's the code that's running.

00:46:04.760 --> 00:46:08.380
If you want slightly more accurate data,
GPROF is a better way to go.

00:46:08.390 --> 00:46:11.130
However, it requires you to
recompile your program.

00:46:11.130 --> 00:46:13.600
Sampler, malloc-debug,
and the others don't require

00:46:13.600 --> 00:46:16.240
you to do recompiles and so
they're much easier to use.

00:46:20.150 --> 00:46:22.380
Now another way that we could be
having performance problems is

00:46:22.380 --> 00:46:24.370
if we're using the disk badly.

00:46:24.370 --> 00:46:29.440
That is, if we are trying to read the
disk at the wrong time and so on.

00:46:29.660 --> 00:46:33.040
In the Carbon session they actually
went through how important it was

00:46:33.260 --> 00:46:35.380
not to try to do disk accesses,
for example,

00:46:35.380 --> 00:46:38.340
when you're doing drawing because
of the possibility of blocking

00:46:38.440 --> 00:46:39.860
and slowing down your drawing.

00:46:39.990 --> 00:46:42.370
And also to minimize the amount
of reads and writes you do during

00:46:42.370 --> 00:46:46.360
application launch to try to make the
application launch as fast as possible.

00:46:46.430 --> 00:46:49.280
So one thing we can imagine
is trying to understand how

00:46:49.330 --> 00:46:53.690
the application uses the disk,
what files it tries to access.

00:46:54.430 --> 00:46:58.290
Now, luckily there's a really cool tool
that will actually help us with this.

00:46:58.450 --> 00:47:03.060
It's also a command line
tool and it's called fsusage.

00:47:08.410 --> 00:47:12.800
and what FS Usage does is it basically
dumps out a text report for a given

00:47:12.800 --> 00:47:17.080
process and it actually tells us every
single file system call that we do,

00:47:17.080 --> 00:47:21.290
every open and close and read and
write and get DER entries and the like.

00:47:21.620 --> 00:47:24.300
Now it has to be run as root because
it's actually a security hole because

00:47:24.300 --> 00:47:28.300
you can in theory find out what
other people are doing with it.

00:47:28.400 --> 00:47:31.480
Remember this is the fun of
multi-user operating systems.

00:47:32.270 --> 00:47:36.700
and Scott did not expose his password
unlike me in a previous demo.

00:47:36.790 --> 00:47:42.120
And what we get is we can say fs
usage for thread viewer and we can

00:47:42.120 --> 00:47:45.030
see the reads and writes and in fact
we can find the name of the file,

00:47:45.040 --> 00:47:49.790
we can find in the far right
column the amount of time it took.

00:47:50.420 --> 00:47:54.290
Now, this is a relatively boring example,
but you can imagine if you ran this,

00:47:54.340 --> 00:47:55.700
for example,
on simple text-- and actually,

00:47:55.700 --> 00:47:58.870
that's a take home-- a bit
of homework for you all.

00:47:59.100 --> 00:48:03.170
Go home, try running FS usage on
simple text when it starts up,

00:48:03.390 --> 00:48:07.540
and watch what file system accesses it
does to go and get the list of fonts

00:48:07.540 --> 00:48:12.120
and get the resources and the like,
and you'll be surprised.

00:48:13.560 --> 00:48:17.860
So what we can see here though,
if we get back to my problem,

00:48:17.860 --> 00:48:23.990
is we find that Thread Viewer is
every second doing an open Fstat,

00:48:24.060 --> 00:48:26.320
a write, and a close.

00:48:26.320 --> 00:48:28.790
And all of these are taking
less than a millisecond.

00:48:28.790 --> 00:48:31.880
They're taking like two ten
thousandths of a second to do,

00:48:31.890 --> 00:48:34.040
according to the numbers
on the far right.

00:48:34.060 --> 00:48:37.630
But this seems a little wrong to me.

00:48:37.710 --> 00:48:41.010
And we can see the file that we're
doing is /temp/threadviewer-log.

00:48:41.010 --> 00:48:43.170
And we could actually go and look
at that file if we needed to to try

00:48:43.170 --> 00:48:44.840
to understand what was going on.

00:48:44.840 --> 00:48:46.820
Actually, don't worry about that.

00:48:46.960 --> 00:48:50.240
So, so,
Fs usage has shown us that Thread Viewer

00:48:50.240 --> 00:48:51.710
is doing something really brain dead.

00:48:53.930 --> 00:48:58.600
So the question is,
where is that brain deadness in my code?

00:48:58.680 --> 00:49:02.100
And fsusage doesn't tell us that.

00:49:02.210 --> 00:49:07.110
Luckily, Sampler has a mode that will
actually help us on this problem.

00:49:08.820 --> 00:49:12.210
Rather than just doing CPU sampling,
Sampler will let us do

00:49:12.280 --> 00:49:13.040
several other things.

00:49:13.040 --> 00:49:15.310
It has a mode that helps
us track down mallocs,

00:49:15.330 --> 00:49:16.960
which is very similar to malloc debug.

00:49:17.090 --> 00:49:20.560
It also has a mode called
Watch for File Actions,

00:49:20.630 --> 00:49:24.060
which will, instead of stopping the
program and getting a stack

00:49:24.060 --> 00:49:28.410
backtrace every 50 milliseconds,
it will do that every time you call

00:49:28.410 --> 00:49:30.580
one of the system file routines.

00:49:33.190 --> 00:49:35.700
or it will crash.

00:49:35.700 --> 00:49:36.690
Let's try that again.

00:49:36.700 --> 00:49:39.480
Did we kill the doc?

00:49:39.480 --> 00:49:42.690
Oh good now.

00:49:45.550 --> 00:49:48.630
So one of the problems with Thread Viewer
is because it's a performance tool,

00:49:48.630 --> 00:49:51.860
and because we're running it,
it has this tendency to stop applications

00:49:51.940 --> 00:49:55.530
when it's looking at them so that it
can snarf their memory and the like.

00:49:55.600 --> 00:49:58.680
And one of the problems with demoing it,
which--

00:49:58.740 --> 00:50:00.930
I'm not sure why I was
silly enough to do that,

00:50:01.000 --> 00:50:05.160
is that if ThreadViewer manages to
crash when it has stopped the program,

00:50:05.180 --> 00:50:07.980
there's this nasty habit that
suddenly the dock is hung,

00:50:07.990 --> 00:50:09.960
which makes for really good demos
because suddenly you're trying

00:50:09.960 --> 00:50:11.180
desperately to get the system back.

00:50:11.230 --> 00:50:13.700
Luckily this didn't happen.

00:50:13.700 --> 00:50:18.610
Okay, so Scott's got this up and can
basically run the program in Sampler

00:50:18.610 --> 00:50:21.730
for a while and then can hit update
and can get a list of all the

00:50:21.770 --> 00:50:23.690
places where allocations occurred.

00:50:23.700 --> 00:50:27.300
Here we get the normal call
tree starting at the root,

00:50:27.300 --> 00:50:28.650
starting at main.

00:50:28.700 --> 00:50:31.700
It's a little more interesting to go
with the invert call tree option here.

00:50:31.710 --> 00:50:35.320
And here we can see that there
were 380 places where we did read,

00:50:35.320 --> 00:50:38.690
372 where we did L-seq,
128 opens and the like.

00:50:38.700 --> 00:50:42.480
And so I think we were
doing opens and writes.

00:50:42.850 --> 00:50:46.700
So let's click on open and we
see that of those 128 opens,

00:50:46.700 --> 00:50:49.650
we see a number in
CF read byte and the like.

00:50:49.820 --> 00:50:52.700
The one that's probably interesting
is the F open call there.

00:50:52.700 --> 00:50:53.700
Okay.

00:50:54.770 --> 00:50:55.940
is that it?

00:50:55.940 --> 00:51:00.480
Yes, which happens to be in the
ThreadView controller get sample array.

00:51:00.480 --> 00:51:03.360
And what's happening here is
that in my code for gathering

00:51:03.360 --> 00:51:06.760
the information on the thread,
for some really silly reason,

00:51:06.760 --> 00:51:09.380
I put in basically a
little loop that said,

00:51:09.380 --> 00:51:12.540
I think I've got that,
something like this.

00:51:12.560 --> 00:51:15.400
Open this file,
write out the samples to the disk,

00:51:15.730 --> 00:51:17.380
close the file.

00:51:17.380 --> 00:51:19.360
Okay?

00:51:19.360 --> 00:51:22.830
And because I'm doing this every second,
that's relatively inefficient.

00:51:22.850 --> 00:51:24.550
It didn't affect ThreadViewer,
but you can imagine if

00:51:24.550 --> 00:51:26.760
you had this in your code,
you might want to know about the

00:51:26.760 --> 00:51:29.890
fact that you were opening and
closing the same file a lot of times.

00:51:29.950 --> 00:51:32.960
It might be more efficient if I'd
done something like just open the

00:51:32.960 --> 00:51:35.830
file once and then just kept writing
to it every time I needed data.

00:51:35.980 --> 00:51:38.550
Or I could just yank this code out,
because it's actually

00:51:38.550 --> 00:51:39.840
pointless in this program.

00:51:42.130 --> 00:51:44.240
Okay, so you've seen two ways
that you can use Sampler,

00:51:44.240 --> 00:51:47.460
both to stop the program occasionally
to find out what code's executing,

00:51:47.800 --> 00:51:51.100
and you've seen another of
the cool features of Sampler,

00:51:51.100 --> 00:51:52.980
which is to look at file system accesses.

00:51:55.780 --> 00:51:59.460
The final type of problem I'm
going to tell you about is drawing.

00:51:59.530 --> 00:52:02.560
Because all of our applications
are graphics based,

00:52:02.620 --> 00:52:05.810
all the good ones that we do,

00:52:06.130 --> 00:52:08.100
Most of the good ones we do.

00:52:08.270 --> 00:52:13.490
Drawing is very important because
it's how we communicate with the user.

00:52:13.610 --> 00:52:17.350
All the value of the Macintosh
is basically presenting things

00:52:17.420 --> 00:52:20.050
to users in graphical ways so
that they can understand things,

00:52:20.060 --> 00:52:23.600
so that they can do the creative work and
let the computer do all the boring stuff.

00:52:23.820 --> 00:52:29.460
And so drawing is a key issue,
and you want the drawing to

00:52:29.460 --> 00:52:29.930
be as efficient as possible
so your application runs well.

00:52:30.620 --> 00:52:32.900
The problem is that if
you do too much drawing,

00:52:32.910 --> 00:52:35.870
you're going to use CPU time,
you're going to use memory

00:52:35.950 --> 00:52:38.720
because you've got buffers,
you're going to be using

00:52:39.080 --> 00:52:41.580
Mach messages as we saw to
communicate with the window manager,

00:52:41.650 --> 00:52:42.730
which means we'll be blocking.

00:52:42.970 --> 00:52:45.790
And so too much drawing will
cause a lot of blocking.

00:52:45.940 --> 00:52:47.450
It will be too much work.

00:52:47.540 --> 00:52:49.790
And so we want to minimize that.

00:52:50.250 --> 00:52:55.160
So what we can do is there's a really
cool tool done by the Core Graphics team.

00:52:55.180 --> 00:52:57.520
It's called Quartz Debug.

00:53:00.920 --> 00:53:03.800
and Scott will first bring
up Thread Viewer again,

00:53:03.800 --> 00:53:09.580
our sacrificial victim.

00:53:09.580 --> 00:53:12.610
And Quartz Debug has
a number of features.

00:53:12.760 --> 00:53:15.350
The two I'll show you, first of all,
it has this option called

00:53:15.480 --> 00:53:16.790
Flash Screen Update.

00:53:16.790 --> 00:53:20.500
And what Flash Screen Update is,
anytime that it has to redraw

00:53:20.500 --> 00:53:25.050
any part of the screen,
Quartz Debug tells the Windows server

00:53:25.050 --> 00:53:27.440
to actually color that in yellow.

00:53:27.460 --> 00:53:29.080
And so that makes the
amount of drawing explicit.

00:53:29.150 --> 00:53:31.640
So you can actually see what
goes on in the You know,

00:53:31.640 --> 00:53:33.920
we actually do a lot of work there.

00:53:34.820 --> 00:53:36.700
So that's really cool.

00:53:36.800 --> 00:53:38.560
And so if you had some case where,
let's say,

00:53:38.560 --> 00:53:41.740
during the same drawing cycle you
were redrawing the same thing twice,

00:53:41.740 --> 00:53:43.270
this would be a way to tell.

00:53:43.340 --> 00:53:45.860
Another thing it points out is that
the way that I handle the drawing

00:53:46.000 --> 00:53:49.600
in Thread Viewer is that I just
erase the entire portion that I'm

00:53:49.600 --> 00:53:52.280
animating and redraw the entire thing.

00:53:52.320 --> 00:53:55.050
Maybe it would be more efficient
if I actually just redrew the

00:53:55.110 --> 00:53:59.970
parts that changed every time
that I got some new samples.

00:54:00.930 --> 00:54:03.660
Oh, oh, there's a really cool feature
here I forgot to show you.

00:54:03.740 --> 00:54:07.050
One of the things that you can do with
Thread Viewer is that if the program

00:54:07.160 --> 00:54:10.460
has something interesting there,
you don't want it scrolling off the left.

00:54:10.580 --> 00:54:13.120
And I don't have history because
I've forgotten to add that.

00:54:13.120 --> 00:54:15.050
So what you can do is you
can press that pause button.

00:54:15.180 --> 00:54:18.290
And the pause button stops the
application that I'm looking at,

00:54:18.290 --> 00:54:19.510
in this case the doc.

00:54:19.700 --> 00:54:23.830
And it also stops the sampling
because the program's not running

00:54:23.890 --> 00:54:26.560
so I don't need to gather any data.

00:54:26.560 --> 00:54:26.560
Uh-oh.

00:54:28.040 --> 00:54:32.860
However,
someone wasn't very bright and when

00:54:32.920 --> 00:54:37.690
he implemented the pause button,
although he pauses the sampling,

00:54:37.690 --> 00:54:39.580
or the thread data gathering,

00:54:40.280 --> 00:54:43.020
He didn't bother to stop the display.

00:54:43.020 --> 00:54:47.550
And so the display that was on a timer so
that every second it would cause a redraw

00:54:48.180 --> 00:54:50.910
gets redrawn every single second.

00:54:50.910 --> 00:54:54.500
And so this is a bug that
would be extremely hard to

00:54:54.640 --> 00:54:56.310
track down in any other way.

00:54:56.310 --> 00:54:59.800
It would be very hard to see that
when you hit the pause button,

00:54:59.860 --> 00:55:00.840
this happens.

00:55:00.950 --> 00:55:03.710
And if you're using a tool like Sampler,

00:55:03.700 --> 00:55:21.000
[Transcript missing]

00:55:21.560 --> 00:55:24.600
and so this alone is a wonderful feature.

00:55:24.630 --> 00:55:28.020
Now another feature in Quartz Debug
is what's called Show Window List.

00:55:28.060 --> 00:55:31.300
And this tells you what the
window manager thinks all the

00:55:31.300 --> 00:55:33.780
windows it knows about are.

00:55:34.360 --> 00:55:37.720
And as we can see,
there are actually about six windows

00:55:37.800 --> 00:55:43.410
that are part of Quartz Debug,
even though only once on the screen.

00:55:44.160 --> 00:55:46.340
So ThreadViewer actually
has six windows open.

00:55:46.350 --> 00:55:51.350
Some of them are off screen,
some are actually--one is appearing.

00:55:51.350 --> 00:55:53.990
And the problem is that every
single window we create,

00:55:54.140 --> 00:55:56.980
whether it's an off screen window,
whether in this case some of

00:55:56.980 --> 00:55:59.900
those are actually windows that
we created via interface builder

00:56:00.000 --> 00:56:02.720
that are just not appearing
until we actually bring them up,

00:56:02.940 --> 00:56:06.390
all those windows need to have
space in the window manager.

00:56:06.510 --> 00:56:09.970
And so they occupy memory and
thus contribute to our memory

00:56:10.120 --> 00:56:13.280
footprint and contribute to the
chance we might be swapping.

00:56:13.670 --> 00:56:15.340
Once again,
this is something you may not know.

00:56:15.340 --> 00:56:18.360
You might not realize how many
windows you actually create.

00:56:18.490 --> 00:56:20.240
And therefore,
Quartz Debug actually gives

00:56:20.330 --> 00:56:21.640
us a way to find that out.

00:56:21.790 --> 00:56:23.870
And it tells us exactly how
many windows we've created.

00:56:23.990 --> 00:56:27.190
And so now I could go in and
I could try finding out exactly

00:56:27.490 --> 00:56:29.500
which of each of those windows was.

00:56:29.830 --> 00:56:35.780
And in the case of dialog boxes,
make sure I only create them

00:56:35.780 --> 00:56:36.340
when I actually need them,
as opposed to keeping

00:56:36.340 --> 00:56:36.340
them up all the time.

00:56:40.000 --> 00:56:43.000
Okay,
I didn't go through all the tools today.

00:56:43.030 --> 00:56:46.280
There were a couple that you
may want to examine on your own.

00:56:46.340 --> 00:56:47.870
The first one is called SC Usage.

00:56:47.870 --> 00:56:50.540
It's somewhat like FS Usage,
only it looks at some

00:56:50.670 --> 00:56:53.770
of the system calls,
like get time of day or Mach message,

00:56:53.770 --> 00:56:56.060
and it'll tell you how many
calls you're making to that.

00:56:56.080 --> 00:56:57.880
And you may find some
interesting behavior that you

00:56:57.880 --> 00:56:59.920
didn't expect in your system.

00:57:00.020 --> 00:57:02.880
Secondly, there were a number of tools
I didn't mention about heap use.

00:57:02.880 --> 00:57:05.390
So, for example,
we saw a little about heap,

00:57:05.390 --> 00:57:08.610
but the idea of being able to get
basically a text output describing

00:57:08.680 --> 00:57:11.210
all the buffers you've allocated
may be interesting to you,

00:57:11.310 --> 00:57:12.550
and so that may be useful.

00:57:12.820 --> 00:57:14.560
There's also a command
line tool called Leaks,

00:57:14.560 --> 00:57:16.960
which is like the leak
detection in malloc_debug.

00:57:16.960 --> 00:57:20.740
Its leak detection algorithm is actually
a bit better than malloc_debug's.

00:57:20.870 --> 00:57:24.410
It'll actually find any buffers that
aren't referenced from things that

00:57:24.410 --> 00:57:27.270
are reachable from well-known spots,
so to speak.

00:57:27.470 --> 00:57:30.180
It'll only find things--it'll
actually find groups of data

00:57:30.260 --> 00:57:34.270
structures that are leaked,
and so it's actually more useful.

00:57:34.310 --> 00:57:35.660
There's also a tool
called malloc_history,

00:57:35.670 --> 00:57:37.370
which I mentioned earlier,
that will actually help you

00:57:37.490 --> 00:57:43.420
identify for a given allocation,
for a given address, like, you know,

00:57:43.420 --> 00:57:48.060
0xE1C04, who is responsible for that,
who actually allocated that block.

00:57:48.060 --> 00:57:50.690
So if you're in the debugger and
you find something interesting,

00:57:50.690 --> 00:57:52.280
you could actually look at that.

00:57:52.730 --> 00:57:55.150
There are also a couple
tools for understanding how

00:57:55.180 --> 00:57:56.770
your application is running.

00:57:56.890 --> 00:57:58.710
So we saw Sampler.

00:57:58.990 --> 00:58:01.290
Sampler is the command line equivalent.

00:58:01.290 --> 00:58:03.150
And we had a quick
introduction to VM Map,

00:58:03.240 --> 00:58:05.630
which is useful for understanding
how virtual memory is laid

00:58:05.630 --> 00:58:06.990
out in your application.

00:58:07.040 --> 00:58:14.670
If you're coming over from the 9 side,
everything is completely different and

00:58:14.670 --> 00:58:14.990
it may be interesting to actually look at
that and realize how memory is laid out.

00:58:17.390 --> 00:58:19.240
As I said before,
I was really trying to just tease

00:58:19.240 --> 00:58:21.260
you saying these are the cool tools.

00:58:21.370 --> 00:58:24.330
There are a number of hints
that I should stress again

00:58:24.420 --> 00:58:26.050
or stress for the first time.

00:58:26.190 --> 00:58:29.500
The first one is that all these
tools have a very nice property,

00:58:29.530 --> 00:58:31.710
which is that you don't
need to recompile your code,

00:58:31.750 --> 00:58:35.500
you don't need to instrument it,
you just run the tools and they work.

00:58:35.630 --> 00:58:37.160
This makes them much more available.

00:58:37.200 --> 00:58:39.650
It's very easy to just sort of
go in and look at your own app,

00:58:39.930 --> 00:58:41.600
look at other people's apps.

00:58:41.690 --> 00:58:42.960
You know,
if you're curious about how some of

00:58:42.970 --> 00:58:45.720
Apple's own applications do disk stuff,
you can actually use some of

00:58:45.750 --> 00:58:47.690
these tools to find out how
they're accessing the disk.

00:58:49.260 --> 00:58:50.460
So that's a big advantage.

00:58:50.550 --> 00:58:52.350
Take advantage of it.

00:58:52.430 --> 00:58:54.840
Second,
if you're coming--if you're working

00:58:54.840 --> 00:58:58.490
on Code Warrior and you're using
CFM binaries as your output so

00:58:58.490 --> 00:59:01.930
that you can work on 9 and 10,
you need to do a little work to

00:59:02.050 --> 00:59:05.780
actually get the performance tools
to find information in the program,

00:59:05.860 --> 00:59:07.520
to get the symbolic information.

00:59:07.560 --> 00:59:11.270
First of all, you need to make sure that
you compile your code with the

00:59:11.270 --> 00:59:13.440
inline traceback table option on.

00:59:13.440 --> 00:59:15.890
This is part of the code
generation settings.

00:59:15.890 --> 00:59:18.500
And basically this says put
the name of the function.

00:59:20.600 --> 00:59:23.600
Excuse me,
put the name of the function immediately

00:59:23.600 --> 00:59:26.290
after the code in the binary.

00:59:30.340 --> 00:59:35.180
Second, Code Warrior gives you the chance
to actually use its version

00:59:35.180 --> 00:59:39.500
of malloc instead of using the
system's version of malloc.

00:59:39.530 --> 00:59:44.860
If you use the Code Warrior version,
basically it asks malloc for

00:59:44.860 --> 00:59:48.170
a huge buffer and then it
subdivides it and hands it out.

00:59:48.250 --> 00:59:50.010
If you do that,
then tools like malloc debug, heap,

00:59:50.010 --> 00:59:52.830
leaks, and the like won't be able to
help you with memory analysis.

00:59:52.910 --> 00:59:54.590
So make sure that you
actually turn on that option.

00:59:54.670 --> 00:59:57.940
And unfortunately,
I'm not quite sure exactly where it is.

00:59:58.790 --> 01:00:03.490
Third, Object ALEC, although a nice tool,
does have the problem that it

01:00:03.490 --> 01:00:06.070
doesn't understand what CFM apps are,
which is probably not that big a

01:00:06.070 --> 01:00:10.520
deal because it really understands
core foundation and Objective C only.

01:00:10.610 --> 01:00:13.530
However, if you want to look at it and
at least see how many objects

01:00:13.530 --> 01:00:16.830
of malloc size 20 you have,
which it will tell you,

01:00:16.970 --> 01:00:20.910
you need to actually select the
Launch CFM app hidden in the system

01:00:20.960 --> 01:00:24.450
folder and then make sure that
you actually name the application

01:00:24.550 --> 01:00:27.450
you're running on the command line,
just as if you were trying to to run

01:00:27.450 --> 01:00:29.930
the application from the command line.

01:00:30.810 --> 01:00:33.590
Okay,
so that's my presentation for today.

01:00:33.700 --> 01:00:38.780
As I said, in two months you're about to
hopefully all ship your apps at

01:00:38.860 --> 01:00:41.220
Macworld New York and you want to
make sure that you give the best

01:00:41.300 --> 01:00:42.910
impression to your customers.

01:00:43.080 --> 01:00:44.660
So start tuning your code.

01:00:44.720 --> 01:00:47.610
The primary thing you want to do
is cut memory use in all ways.

01:00:47.610 --> 01:00:52.600
That's going to be the best way to
actually make your programs efficient.

01:00:52.600 --> 01:00:55.330
And so take a look at heap,
take a look at how you're using the heap,

01:00:55.410 --> 01:00:59.300
take a look at how you're using memory,
take a look at your private memory use.

01:00:59.990 --> 01:01:03.390
Also remember that just looking
at the program in isolation

01:01:03.460 --> 01:01:04.530
isn't going to be useful.

01:01:04.620 --> 01:01:06.400
Make sure to write down some metrics.

01:01:06.400 --> 01:01:08.350
Measure how much memory you're using.

01:01:08.350 --> 01:01:10.400
Measure how long common operations take.

01:01:10.590 --> 01:01:14.320
Decide whether those are appropriate
and then compare them across multiple

01:01:14.320 --> 01:01:16.360
builds so you can note regressions.

01:01:16.430 --> 01:01:19.350
Also remember that your application
is not just your binary,

01:01:19.350 --> 01:01:21.560
but it's also some of the
servers that you connect to.

01:01:21.860 --> 01:01:24.620
And so make sure in the case of
drawing to look at both the window

01:01:24.910 --> 01:01:26.700
manager and at your application.

01:01:26.810 --> 01:01:28.920
And go out there and please
create some great apps.

01:01:28.970 --> 01:01:32.590
And I'll be looking forward
to seeing them at Macworld.

01:01:32.700 --> 01:01:33.340
Thank you very much.

01:01:36.800 --> 01:01:41.930
Thank you Scott.

01:01:42.040 --> 01:01:43.880
Damn, I forgot I got a slide.

01:01:43.940 --> 01:01:46.420
If you want more information
about these tools,

01:01:46.470 --> 01:01:49.600
first of all they're all available
on the Developer Tools CD.

01:01:49.640 --> 01:01:52.180
You've all got a copy of it,
go off and play with them.

01:01:52.210 --> 01:01:55.830
If you want documentation,
all the graphical applications

01:01:55.830 --> 01:01:57.560
have documentation built in.

01:01:57.680 --> 01:02:02.800
All the command line tools have man pages
online as standard Unix tools should.

01:02:02.840 --> 01:02:06.360
There is also documentation
in the release notes section.

01:02:07.380 --> 01:02:10.930
As I mentioned before,
there are also books to help you.

01:02:11.070 --> 01:02:15.170
Inside Mac OS X Performance is a
really cool book that talks about

01:02:15.630 --> 01:02:16.800
how to tune your application.

01:02:16.800 --> 01:02:19.210
It gives you information from
the level of how the system

01:02:19.210 --> 01:02:20.900
works to documenting the tools.

01:02:20.900 --> 01:02:24.190
And all of us engineers actually
tried to contribute to this.

01:02:24.210 --> 01:02:26.760
Also,
there's a Mac OS X system overview book,

01:02:26.760 --> 01:02:28.600
and this is really good for
understanding just sort of the

01:02:28.600 --> 01:02:30.800
overall ideas behind Mac OS X.

01:02:30.800 --> 01:02:32.840
And we try to suggest that people
actually look at this so that they

01:02:32.840 --> 01:02:35.980
understand some of the terminology.

01:02:35.990 --> 01:02:40.640
And with that,
I will turn it over to Godfrey.

01:02:46.520 --> 01:02:50.060
Thank you very much Robert and Scott.

01:02:50.180 --> 01:02:51.640
So, last session of the day.

01:02:51.860 --> 01:02:55.630
Information resources we put up in
all of our other tool sessions and the

01:02:55.630 --> 01:02:58.430
information remains pretty much the same.

01:03:00.010 --> 01:03:00.570
So a roadmap.

01:03:00.690 --> 01:03:03.940
We wanted to point you
to sessions 121 and 122,

01:03:03.940 --> 01:03:07.330
even though they've already occurred,
so that when you go to the DVDs

01:03:07.380 --> 01:03:10.100
that you'll receive after the show,
you'll see some other areas where

01:03:10.100 --> 01:03:11.760
we talk about performance tuning.

01:03:11.760 --> 01:03:18.840
Tomorrow, our last sessions for the tools
track happen in Hall 2 at 9 a.m.

01:03:18.840 --> 01:03:22.100
That's the debugging of
Mac OS X and the feedback forum for

01:03:22.100 --> 01:03:24.880
Apple developer tools at 3.30 p.m.

01:03:24.880 --> 01:03:25.630
in J1.

01:03:25.680 --> 01:03:26.480
Please attend.

01:03:26.480 --> 01:03:29.510
We're very interested to hear your
feedback at the end of the day.

01:03:31.390 --> 01:03:34.800
If you have questions on tools,
you can contact me.

01:03:34.830 --> 01:03:37.500
I am the Technology Manager for
Development Tools.

01:03:37.500 --> 01:03:39.640
That's my information up above.

01:03:39.640 --> 01:03:45.560
And the Developer Tools Feedback at
macos10toolsfeedbackgroup.apple.com.