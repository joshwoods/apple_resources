WEBVTT

00:00:07.820 --> 00:00:10.260
I will not waste any time.

00:00:10.410 --> 00:00:14.250
So the goal of this session is to
get the best performance out of

00:00:14.250 --> 00:00:16.950
your Java application on Mac OS X.

00:00:16.980 --> 00:00:19.010
Whether you're writing a new
one or whether you already have

00:00:19.090 --> 00:00:23.480
one that you're porting from
Mac OS 9 or some other platform.

00:00:23.650 --> 00:00:27.500
So what we'll cover in this
session is try and give you an

00:00:27.500 --> 00:00:34.700
understanding of the characteristics
of the Mac OS X Java performance.

00:00:35.250 --> 00:00:38.160
Some techniques and patterns you
can use in your code to try and

00:00:38.200 --> 00:00:41.370
get optimal performance on 10.

00:00:41.790 --> 00:00:45.450
point out why measurement is
critically important whenever

00:00:45.450 --> 00:00:48.220
you're trying to do optimization.

00:00:48.430 --> 00:00:50.170
And then lastly,
we'll have a demonstration of

00:00:50.270 --> 00:00:54.060
some performance analysis to give
you an idea of what sort of tools

00:00:54.060 --> 00:00:56.680
will be available on Mac OS X.

00:00:57.810 --> 00:01:01.790
So without further remiss,
I'll introduce Ivan Posva

00:01:02.030 --> 00:01:03.590
from the Java VM team.

00:01:03.710 --> 00:01:04.770
He's our tech lead.

00:01:04.980 --> 00:01:06.530
Thank you, Ivan.

00:01:09.320 --> 00:01:12.170
Good afternoon.

00:01:12.490 --> 00:01:18.150
Well, I wanted to question first,
what are the Java performance factors?

00:01:18.450 --> 00:01:23.840
Well, first of all, it's your application
design and implementation.

00:01:23.840 --> 00:01:28.020
If you have algorithms that
don't scale to your problem set,

00:01:28.020 --> 00:01:31.880
there is no performance tuning
we can do in the VM at any rate

00:01:32.010 --> 00:01:34.720
that will improve your app.

00:01:34.720 --> 00:01:37.620
So you have to make sure that you
don't have N-square algorithms

00:01:37.620 --> 00:01:39.660
or anything like that in there.

00:01:39.660 --> 00:01:46.320
The second factor is the amount of
memory your application is consuming.

00:01:46.320 --> 00:01:49.780
The more memory you use,
the more likely you

00:01:49.890 --> 00:01:52.720
are to be swapping out,
to be paging,

00:01:52.720 --> 00:01:57.340
the more stress you're putting
on the VM memory subsystem,

00:01:57.340 --> 00:02:00.750
and the less likely you
are benefiting from caches,

00:02:00.750 --> 00:02:03.280
data cache and instruction cache.

00:02:03.280 --> 00:02:06.240
Then the next factor down
is bytecode execution.

00:02:06.240 --> 00:02:07.240
The second factor down is the speed
of the application performance.

00:02:07.240 --> 00:02:11.700
It is at what speed does the
Java VM execute your Java code?

00:02:11.750 --> 00:02:17.510
Jim will talk about tips and tricks,
do's and don'ts in the second part of

00:02:17.540 --> 00:02:21.640
his talk on what you can do in this area.

00:02:21.650 --> 00:02:25.010
I will concentrate more on
Java VM efficiency in the

00:02:25.010 --> 00:02:27.120
first part of this talk.

00:02:27.120 --> 00:02:30.420
There are two other factors
that influence Java performance.

00:02:30.420 --> 00:02:34.240
One is the speed of the underlying OS.

00:02:34.240 --> 00:02:37.840
We are not touching Java VM efficiency
in the first part of this talk.

00:02:38.090 --> 00:02:42.640
Then there is obviously the speed
of the hardware you're running on.

00:02:42.740 --> 00:02:46.920
Let's look at what
Java VM efficiency means.

00:02:47.620 --> 00:02:55.380
There are two most cited issues that
influence Java performance at runtime.

00:02:55.380 --> 00:02:58.460
One of them is memory management.

00:02:58.460 --> 00:03:02.200
It is the footprint of your application,
the footprint of your

00:03:02.200 --> 00:03:03.780
Java process running.

00:03:03.780 --> 00:03:05.910
That includes your Java heap once.

00:03:06.280 --> 00:03:10.500
That includes all the supporting
Java VM memory structures.

00:03:10.500 --> 00:03:13.380
We are actually using the
Java heap for that as well.

00:03:13.380 --> 00:03:21.920
It includes parts for the OS,
parts for the code, for the VM and so on.

00:03:21.920 --> 00:03:24.470
Then second is the speed of allocation.

00:03:24.470 --> 00:03:26.200
Java is very object heavy.

00:03:26.310 --> 00:03:30.560
You allocate a lot of temporary objects,
so you need to be able to allocate

00:03:30.560 --> 00:03:32.440
those objects very quickly.

00:03:32.440 --> 00:03:35.690
To keep your footprint down,
if you just allocate very

00:03:35.690 --> 00:03:38.560
quickly and never reclaim,
you would grow your

00:03:38.810 --> 00:03:40.150
footprint to infinity.

00:03:40.260 --> 00:03:42.620
That's why you have to reclaim
very efficiently as well.

00:03:42.630 --> 00:03:47.570
I will touch briefly on what we do
there or how you can actually help

00:03:47.570 --> 00:03:51.240
us as Java programmers in that area.

00:03:51.240 --> 00:03:54.540
The second part is synchronization.

00:03:54.540 --> 00:04:02.420
Java has built-in support for
multithreading in the language.

00:04:02.420 --> 00:04:05.210
To make that... To make that viable use,
we actually have to make sure that

00:04:05.210 --> 00:04:08.610
the synchronization primitives
are implemented in a very

00:04:08.610 --> 00:04:10.260
efficient manner inside the VM.

00:04:12.550 --> 00:04:15.320
Another part is startup performance.

00:04:15.320 --> 00:04:18.760
There must be hundreds of
Java benchmarks out there on the web,

00:04:18.760 --> 00:04:21.560
small and big, and some useful,
some less.

00:04:21.560 --> 00:04:25.500
But I've never seen one that actually
measures Java startup performance.

00:04:25.500 --> 00:04:30.480
You don't want your application to
start up for a minute or two minutes

00:04:30.480 --> 00:04:35.540
seeing the bouncing icon in the dock,
so I will touch some of the issues

00:04:35.540 --> 00:04:38.380
we have done to address that issue.

00:04:39.120 --> 00:04:42.200
Most notably, reducing class loading.

00:04:42.200 --> 00:04:46.840
Class loading is about 40%
of your Java VM startup.

00:04:46.840 --> 00:04:53.950
I will touch on what you can do
to prevent or actually to help

00:04:53.950 --> 00:04:55.340
us out with startup performance.

00:04:55.590 --> 00:04:56.740
Direct Performance.

00:04:56.780 --> 00:04:59.510
So let's go to memory management first.

00:04:59.720 --> 00:05:04.680
The Hotspot VM that we ship on
Mac OS X has an accurate compacting

00:05:04.680 --> 00:05:07.500
generational copying garbage collector.

00:05:07.500 --> 00:05:09.220
What do those buzzwords mean?

00:05:09.220 --> 00:05:13.840
Accurate means we know at all
times where in the VM you have

00:05:13.840 --> 00:05:16.340
references to your object.

00:05:16.340 --> 00:05:19.530
We can distinguish between
real references to objects and

00:05:19.580 --> 00:05:23.020
just memory location that look
like references to objects.

00:05:24.200 --> 00:05:28.650
Compacting is also means
that we compact the heap.

00:05:28.950 --> 00:05:33.380
We don't leave any holes in the heap,
meaning

00:05:33.940 --> 00:05:38.050
All the memory you use is
basically moved together,

00:05:38.050 --> 00:05:43.640
compacted, well, as it says,
and that improves both your

00:05:43.760 --> 00:05:46.000
footprint and locality of reference.

00:05:48.740 --> 00:05:55.490
Generational copying means
that we allocate objects for a

00:05:55.600 --> 00:05:59.350
first time in a new generation,
and only when they survive

00:05:59.460 --> 00:06:01.850
for a certain amount of time,
we actually copy them

00:06:01.850 --> 00:06:08.380
into an older generation,
which we handle much less frequently

00:06:08.380 --> 00:06:13.370
than the newly allocated objects.

00:06:14.440 --> 00:06:17.760
So we spend a lot of cycles
on newly allocated objects,

00:06:17.760 --> 00:06:20.340
collecting newly allocated objects.

00:06:20.340 --> 00:06:22.790
For older objects,
we don't spend that time.

00:06:22.920 --> 00:06:24.200
So there is one thing.

00:06:24.200 --> 00:06:26.590
Lifetime of your objects is important.

00:06:26.680 --> 00:06:28.910
If you have objects that
you're not going to use,

00:06:28.910 --> 00:06:30.760
you have to null out the references.

00:06:30.760 --> 00:06:33.820
If you have object hierarchies
that you're not using anymore,

00:06:33.820 --> 00:06:36.970
null out the reference to that
object hierarchy to actually give

00:06:36.970 --> 00:06:40.940
the garbage collector an opportunity
to remove all the space that is

00:06:40.940 --> 00:06:43.760
allocated for those object hierarchies.

00:06:44.440 --> 00:06:49.490
Then the second part is if you use

00:06:49.960 --> 00:06:57.560
If you use some caches or have objects
that can be recreated at a later time,

00:06:57.560 --> 00:07:02.490
you can use the weak reference class
or soft reference class to give the

00:07:02.520 --> 00:07:07.760
collector a hint or an opportunity
to remove objects out of your working

00:07:08.140 --> 00:07:10.190
set if the memory is getting tight.

00:07:11.390 --> 00:07:14.060
Avoid finalizers where you can.

00:07:14.430 --> 00:07:20.080
Objects that can be finalized need
to be handled specially in the VM.

00:07:20.440 --> 00:07:25.620
We need to actually call the runtime
from interpreted or compiled code

00:07:25.620 --> 00:07:29.260
to allocate objects with finalizers
because we have to keep track of them.

00:07:29.260 --> 00:07:32.590
That makes it very hard
to allocate quickly.

00:07:32.590 --> 00:07:37.870
All other allocation is done in line
in interpreted and compiled code.

00:07:37.920 --> 00:07:41.570
For finalizable objects,
we first have to... We have

00:07:41.570 --> 00:07:45.060
to keep track of them by
allocating through the runtime.

00:07:45.060 --> 00:07:47.540
Then we actually have...
When we are throwing them away,

00:07:47.540 --> 00:07:50.060
we have to make sure to call
the finalizers when you're done.

00:07:50.060 --> 00:07:54.980
If you can avoid finalizers, please do.

00:07:55.210 --> 00:07:58.970
To reduce your footprint,
it is helpful to do lazy

00:07:58.970 --> 00:08:01.860
initialization and allocation.

00:08:01.860 --> 00:08:07.040
That way you reduce your footprint
as well as reduce your startup time

00:08:07.220 --> 00:08:10.100
to increase the startup performance.

00:08:10.310 --> 00:08:16.100
What did we do to memory management
to improve some footprint issues?

00:08:16.100 --> 00:08:19.820
Well,
we introduced a shared generation that

00:08:19.820 --> 00:08:26.100
stores the often used classes and methods
including their bytecodes and so on.

00:08:26.100 --> 00:08:31.270
And this shared generation
is mapped in from a file,

00:08:31.270 --> 00:08:35.560
so we used the underlying
Mach virtual memory system to just

00:08:35.960 --> 00:08:39.060
bring that memory into the VM.

00:08:39.100 --> 00:08:43.410
We reduced the GC time because
we never tried to collect,

00:08:43.510 --> 00:08:45.100
reclaim that memory.

00:08:45.180 --> 00:08:46.100
It is basically free.

00:08:46.100 --> 00:08:49.100
Because we read it from a file,
we are never touching it,

00:08:49.100 --> 00:08:54.100
so we share it across multiple
running Java processes.

00:08:54.280 --> 00:08:57.100
The thing you can help us there is, well,
don't break it.

00:08:57.130 --> 00:08:59.080
Don't change your boot class path.

00:08:59.210 --> 00:09:03.100
Do not modify the system jar files
that are installed on the system.

00:09:03.100 --> 00:09:09.090
And that way we can always use the
shared generation when we start the VM.

00:09:09.760 --> 00:09:12.540
It has additional
benefit in startup time,

00:09:12.660 --> 00:09:14.620
which I'm going to touch now.

00:09:14.650 --> 00:09:19.700
Avoid eager class loading
and class initialization.

00:09:19.910 --> 00:09:24.840
It has both effect on startup time
as well as your memory footprint.

00:09:24.840 --> 00:09:28.810
If you don't load those classes
when you don't need them,

00:09:28.810 --> 00:09:33.380
you're not using the memory as well
as you're spending cycles to actually

00:09:33.380 --> 00:09:38.000
load them from the class file,
decode them, bring them into memory,

00:09:38.090 --> 00:09:39.870
initialize, and so on.

00:09:40.230 --> 00:09:44.970
So if you want to see which
classes are loaded at what time,

00:09:45.140 --> 00:09:50.720
you can use the -verbose:class flag
on the command line to see what is

00:09:50.720 --> 00:09:54.370
loaded at what time and in which order.

00:09:54.680 --> 00:09:58.120
So as I mentioned before,
the shared generation reduces class

00:09:58.120 --> 00:10:02.200
load time because most of the classes
you're going to use out of Java lang,

00:10:02.200 --> 00:10:06.700
Java net, swing, and so on,
have been preloaded for you in that

00:10:06.760 --> 00:10:10.720
shared generation and are mapped
from the file when you actually--

00:10:11.010 --> 00:10:12.970
When you start the VM.

00:10:13.000 --> 00:10:15.940
So when you run the above
command that I mentioned,

00:10:15.940 --> 00:10:20.070
java-verbose-class with version,
you don't see any class

00:10:20.070 --> 00:10:21.080
loading going on at all.

00:10:21.140 --> 00:10:24.550
So the other part was synchronization.

00:10:26.690 --> 00:10:33.600
The hotspot VM we are using has very fast
synchronization in the uncontended case.

00:10:33.600 --> 00:10:37.040
What does an uncontended case mean?

00:10:37.050 --> 00:10:41.310
When you synchronize on an object,
the object is synchronized.

00:10:41.710 --> 00:10:43.260
One thread at a time.

00:10:43.260 --> 00:10:45.440
That is most often the case.

00:10:45.440 --> 00:10:48.670
You don't have any
locking really going on,

00:10:48.740 --> 00:10:54.330
but you want to protect if some other
thread happened to be in that code.

00:10:54.330 --> 00:10:57.960
For this,
we have a constant time overhead.

00:10:58.020 --> 00:11:02.480
The inline implementation in the
compiler or in the interpreter

00:11:03.160 --> 00:11:06.590
takes about 8 to 10 instructions.

00:11:06.590 --> 00:11:09.390
It has very low memory overhead.

00:11:11.610 --> 00:11:15.710
We don't allocate any space for
these object locks on the heap.

00:11:15.810 --> 00:11:21.330
They're all stack allocated in the
stack frame that you're locking this

00:11:21.330 --> 00:11:24.900
object that you're synchronizing on.

00:11:24.900 --> 00:11:29.200
We don't use any of the underlying
OS resources because that is expensive.

00:11:29.200 --> 00:11:32.190
That ties down memory,
sometimes even in the kernel,

00:11:32.190 --> 00:11:33.590
which you want to void.

00:11:33.600 --> 00:11:38.060
The contended case is rare for that,
but since it happens,

00:11:38.220 --> 00:11:41.600
for that we use Mach primitives directly.

00:11:41.600 --> 00:11:45.540
To speed up,
to get as much performance as we can.

00:11:45.600 --> 00:11:49.600
Before I want to hand
over this talk to Jim,

00:11:49.600 --> 00:11:53.600
I wanted to tell you what's new in 2001.

00:11:53.600 --> 00:11:55.570
We shipped Mac OS X.

00:11:55.670 --> 00:11:57.600
It has a shared generation.

00:11:57.600 --> 00:12:03.270
That is one of the big improvements
we did to the Java virtual

00:12:03.270 --> 00:12:05.590
machine we got from Sun.

00:12:05.600 --> 00:12:10.660
In the 1.3.1 developer
preview we're working on,

00:12:10.660 --> 00:12:11.600
or that should be the first one,
we're working on the developer preview.

00:12:11.600 --> 00:12:15.600
We did inline interpreter allocation.

00:12:15.600 --> 00:12:17.600
This was not in Mac OS X.

00:12:17.700 --> 00:12:19.600
We're working on thread local allocation.

00:12:19.600 --> 00:12:24.980
We have a faster instance off and
faster array copy that is tuned

00:12:25.110 --> 00:12:28.320
to G4 with the velocity engine.

00:12:28.600 --> 00:12:33.820
We use the same code actually for,
the code we use for our array copy we

00:12:33.820 --> 00:12:39.600
are also using inside the GC to copy
the objects between the generations.

00:12:39.600 --> 00:12:41.590
We're making use of that.

00:12:41.600 --> 00:12:43.600
We're making use of that
code in multiple areas.

00:12:43.780 --> 00:12:47.600
So now I would want to
give this over to Jim.

00:12:47.600 --> 00:12:49.400
Thanks.

00:12:55.820 --> 00:13:01.780
So memory management and synchronization
is a crucial part of what affects

00:13:01.840 --> 00:13:03.110
the performance of your application.

00:13:03.180 --> 00:13:07.840
But I think fundamentally when
we're working with Java code,

00:13:07.840 --> 00:13:12.620
we think in terms of the speed of
the code execution as being the key

00:13:12.620 --> 00:13:19.450
factor in determining what is actually
slowing down our particular application.

00:13:20.790 --> 00:13:24.330
So what I'm going to try to
do in this talk is describe

00:13:24.590 --> 00:13:27.580
how things get interpreted,
how they get compiled,

00:13:27.580 --> 00:13:29.900
when they get interpreted,
when they get compiled,

00:13:29.900 --> 00:13:35.190
and give you some ideas on some
of the coding hints that you can

00:13:35.190 --> 00:13:37.330
use to speed up the performance
once you find out what kinds of

00:13:37.390 --> 00:13:38.800
weaknesses you have in your code.

00:13:38.800 --> 00:13:41.430
And I'm also going to run
through a few things that

00:13:41.580 --> 00:13:45.040
have changed in the last year,
since our talk last year.

00:13:46.430 --> 00:13:51.380
So first of all, I want to point out that
people often question,

00:13:51.380 --> 00:13:52.840
well, why don't we compile everything?

00:13:52.840 --> 00:13:56.370
Obviously,
if you turn everything into native code,

00:13:56.370 --> 00:14:00.050
it's going to run a lot faster
than if we interpret it.

00:14:00.080 --> 00:14:04.150
But because the Java environment
compiles code on the fly

00:14:04.330 --> 00:14:08.870
using just-in-time compilers,
there's a certain amount of cost,

00:14:08.960 --> 00:14:12.810
both in CPU and memory usage,
in order to get things compiled.

00:14:13.740 --> 00:14:16.740
And when you're doing some
analysis of the actual VM,

00:14:16.740 --> 00:14:20.140
you find out that it's actually
cheaper to interpret the code,

00:14:20.140 --> 00:14:23.340
because the interpreter is fairly fast,
cheaper to interpret the

00:14:23.340 --> 00:14:26.200
code than actually go off and
compile it and try to run it.

00:14:26.280 --> 00:14:29.880
So we have to get a balance
there on what actually gets

00:14:29.880 --> 00:14:32.680
interpreted and what gets compiled.

00:14:36.200 --> 00:17:12.600
[Transcript missing]

00:17:13.090 --> 00:17:17.540
So using the criteria of
the number of invocations,

00:17:17.610 --> 00:17:22.740
the number of times it loops,
we can actually sort of find out

00:17:22.740 --> 00:17:25.110
what is actually hot in your code.

00:17:25.360 --> 00:17:29.500
And we find out it's only about 5% of
the bytecodes of 5% of your applications

00:17:29.640 --> 00:17:34.400
or methods that need to be compiled
and hence are hot in your application.

00:17:34.450 --> 00:17:39.320
And Andy will be going over some of the
tools which will allow you to determine

00:17:39.330 --> 00:17:42.960
which of those methods are actually
getting compiled in native code.

00:17:42.960 --> 00:17:45.520
And we can,
once we've got that information,

00:17:45.590 --> 00:17:50.370
we can start tweaking those particular
methods because those are the methods

00:17:50.370 --> 00:17:52.900
that are going to be problematic.

00:17:52.900 --> 00:17:58.220
So I'm just going to go through and
discuss a few things that you can do

00:17:58.340 --> 00:18:04.290
to get the best performance out of your
application and types of things that

00:18:04.300 --> 00:18:08.940
you can concentrate on once you find
out what's hot in your application.

00:18:08.940 --> 00:18:13.200
The first,
the most important thing I said is that

00:18:13.200 --> 00:18:14.900
you can't just keep compiling your code.

00:18:14.900 --> 00:18:16.760
You can't just keep compiling your code.

00:18:16.760 --> 00:18:18.820
You can't just keep compiling your code.

00:18:19.000 --> 00:18:21.160
You can't just keep compiling your code.

00:18:21.240 --> 00:18:22.580
You can't just keep compiling your code.

00:18:22.680 --> 00:18:23.790
You can't just keep compiling your code.

00:18:23.810 --> 00:18:24.560
You can't just keep compiling your code.

00:18:24.610 --> 00:18:24.940
You can't just keep compiling your code.

00:18:24.940 --> 00:18:25.460
You can't just keep compiling your code.

00:18:25.460 --> 00:18:25.880
You can't just keep compiling your code.

00:18:25.910 --> 00:18:26.380
You can't just keep compiling your code.

00:18:26.380 --> 00:18:26.720
You can't just keep compiling your code.

00:18:26.720 --> 00:18:27.090
You can't just keep compiling your code.

00:18:27.090 --> 00:18:27.620
You can't just keep compiling your code.

00:18:27.620 --> 00:18:27.980
You can't just keep compiling your code.

00:18:27.980 --> 00:18:28.490
You can't just keep compiling your code.

00:18:28.490 --> 00:18:28.850
You can't just keep compiling your code.

00:18:28.860 --> 00:18:29.320
You can't just keep compiling your code.

00:18:29.610 --> 00:18:33.090
And if only a small portion
of your method is actually

00:18:33.100 --> 00:18:35.660
being used all the time,
maybe you have some exception code

00:18:35.660 --> 00:18:39.700
in there or some special case code,
it's doing a disservice to the

00:18:39.700 --> 00:18:44.420
compilation process by having
it embedded in your method.

00:18:44.420 --> 00:18:48.520
So you should try to break that
code out into separate methods and

00:18:48.730 --> 00:18:51.470
try to keep your method focused,
small and focused,

00:18:51.480 --> 00:18:54.860
so that it can compile quickly
and then go off and execute.

00:18:54.960 --> 00:18:58.350
And then you get good locality
also of the execution of the code.

00:18:59.230 --> 00:19:02.820
So as I say, separate rarely used code
out into separate methods.

00:19:02.860 --> 00:19:09.930
As far as saying to yourself, well,
if I put it into a separate method,

00:19:09.940 --> 00:19:15.190
then we're going to incur the
cost of calling that method,

00:19:15.200 --> 00:19:18.030
basically pushing parameters
and so on and so forth.

00:19:18.300 --> 00:19:22.580
But you find that in the VM,
we actually inline things that make

00:19:22.580 --> 00:19:27.120
sense to inline if we can make more
optimal use of the code by inlining

00:19:27.120 --> 00:19:28.440
it as opposed to having it separately.

00:19:28.940 --> 00:19:29.560
So don't worry about that.

00:19:29.800 --> 00:19:32.900
And in particular,
accessor methods are always inlined,

00:19:32.900 --> 00:19:35.690
so you don't have to
worry about the fact that,

00:19:35.740 --> 00:19:38.340
well, I've got a very tiny method and
all it does is extract the field.

00:19:38.340 --> 00:19:42.520
And with the 131 code,
we actually have a much tighter

00:19:42.520 --> 00:19:45.920
implementation of accessors,
so they are very fast.

00:19:48.020 --> 00:19:54.170
One of the things that I like to do
is actually try to find which methods

00:19:54.180 --> 00:19:58.280
that are used fairly frequently
in the class libraries and tweak

00:19:58.280 --> 00:20:02.630
the code specifically to handle
those because those are the routines

00:20:02.630 --> 00:20:04.100
that are used a lot by everybody.

00:20:04.100 --> 00:20:10.230
And we want to try to get good
performance for those particular methods.

00:20:10.860 --> 00:20:12.350
So trust the supplied classes.

00:20:12.350 --> 00:20:15.960
You may have the urge to go out
and rewrite the vector class

00:20:15.960 --> 00:20:20.830
because it synchronizes every time
you access it and does an object,

00:20:21.000 --> 00:20:26.400
you know, a check cast every time
you do extraction from it.

00:20:26.440 --> 00:20:29.150
You know,
these are the sorts of things that

00:20:29.240 --> 00:20:31.170
we notice that are used a lot.

00:20:31.340 --> 00:20:35.270
So we hand tool, or not hand tool,
but we provide special service for

00:20:35.570 --> 00:20:39.840
those particular types of methods
that are used fairly frequently.

00:20:40.550 --> 00:20:42.110
So instead of going off
and writing your own,

00:20:42.340 --> 00:20:44.040
trust the supplied classes.

00:20:44.040 --> 00:20:48.750
So classes such as string, string buffer,
vector, and the collection classes,

00:20:48.750 --> 00:20:51.100
you know,
use what's there because we're going to,

00:20:51.110 --> 00:20:53.340
you know, get the performance up for you.

00:20:53.340 --> 00:20:59.700
And we've added some more optimization or
more special cases for those in the 131.

00:20:59.700 --> 00:21:03.810
If you have a copy that you're
copying from one array to another,

00:21:03.880 --> 00:21:06.820
use array copy because,
as Ivan mentioned,

00:21:06.820 --> 00:21:09.340
that we're using G4 acceleration.

00:21:10.180 --> 00:21:12.050
So you can do that on the array copy,
and hence it's going to be

00:21:12.050 --> 00:21:13.160
the fastest way of doing it.

00:21:13.220 --> 00:21:15.900
So instead of having a
loop that iterates through,

00:21:15.900 --> 00:21:16.920
use array copy.

00:21:16.940 --> 00:21:19.400
And then, of course,
there's certain functions like sine,

00:21:19.400 --> 00:21:21.790
cosine, and tan,
which have native implementation.

00:21:21.800 --> 00:21:27.490
So it's best to use, you know,
what's supplied and not go off and write

00:21:27.490 --> 00:21:31.000
your own JNI routines to deal with that.

00:21:31.490 --> 00:21:34.300
Make the best use of
the native data types.

00:21:34.300 --> 00:21:42.960
Again, the G4 is not a 64-bit processor,
so whenever you have a long arithmetic,

00:21:43.050 --> 00:21:46.540
it has a certain amount of
cost associated with it.

00:21:46.580 --> 00:21:51.290
Some of the basic operations like add
and subtract or bit and or whatever,

00:21:51.290 --> 00:21:55.070
they're reasonably cheap,
but when you get into shifting

00:21:55.360 --> 00:21:59.080
or divide or that sort of thing,
it can be fairly costly.

00:21:59.510 --> 00:22:02.620
So if you don't need all that precision,
stick with ints for the time being.

00:22:02.620 --> 00:22:05.560
And then also consider using
floats instead of doubles.

00:22:05.560 --> 00:22:08.700
Not necessarily in your computation,
because sometimes you

00:22:08.700 --> 00:22:11.680
just need the precision,
but when you're dealing

00:22:11.680 --> 00:22:15.080
with arrays of values,
it's best to keep the size of

00:22:15.080 --> 00:22:17.500
your arrays down by using floats.

00:22:17.540 --> 00:22:22.150
And there are quite well-known techniques
for actually keeping precision,

00:22:22.170 --> 00:22:25.120
even though you're using a 32-bit value.

00:22:25.120 --> 00:22:29.100
And then new to the 131,
we've added Bedrock.

00:22:29.500 --> 00:22:31.730
It's a better register
allocation for long,

00:22:31.830 --> 00:22:33.040
floats, and doubles.

00:22:33.040 --> 00:22:36.400
So you'll find that some of the, say,
especially if you're doing

00:22:36.400 --> 00:22:39.170
a looping-type calculation,
that performance will

00:22:39.230 --> 00:22:40.490
be improved on that.

00:22:42.450 --> 00:22:47.150
Try to avoid using the generic data types
because there is a cost in assigning,

00:22:47.150 --> 00:22:50.110
say, a generic data type to
a specific data type.

00:22:50.190 --> 00:22:52.270
We have to go through a check cast.

00:22:52.280 --> 00:22:57.370
Ivan mentioned that we've done some
performance improvements in 1.3.1 to

00:22:57.370 --> 00:22:59.840
deal with instance of and check cast.

00:22:59.860 --> 00:23:01.760
But it's still a cost.

00:23:01.810 --> 00:23:04.340
Instead of a simple assignment,
we have to go off and do this

00:23:04.340 --> 00:23:06.680
to make sure that it's the
right class for doing that.

00:23:06.760 --> 00:23:10.690
So try to avoid using generic types
and use subtyping or subclassing

00:23:10.700 --> 00:23:14.110
in these circumstances because
then that way you can avoid making

00:23:14.540 --> 00:23:17.880
assignments that require these checks.

00:23:20.420 --> 00:23:21.950
Try to work with local copies.

00:23:21.950 --> 00:23:27.450
Now, one of the things about people have
been asking about was why is the code

00:23:27.450 --> 00:23:33.810
generated by a hotspot client slower,
let's say, than maybe the server version

00:23:33.830 --> 00:23:34.820
and so on and so forth.

00:23:34.820 --> 00:23:38.080
Well, some of the optimizations you
get in the server version of the

00:23:38.080 --> 00:23:40.840
compiler are very sophisticated,
and they're not there

00:23:40.840 --> 00:23:42.390
in the client because,
again,

00:23:42.390 --> 00:23:46.320
we want to try to compile things fairly
quickly and get them up and running.

00:23:46.320 --> 00:23:51.800
So, if you have array access and you're
working with that array value,

00:23:51.800 --> 00:23:55.850
it's best to make a copy of
that value and work with that

00:23:55.920 --> 00:23:58.460
copy and put it back in again.

00:23:58.460 --> 00:24:02.460
In this particular example,
you have three accesses to that array.

00:24:02.460 --> 00:24:05.860
That means we have to do three
bounds checks and three null

00:24:06.090 --> 00:24:09.600
checks to see on the table itself,
whereas if we make a copy of it,

00:24:09.600 --> 00:24:11.320
we only have to do two in this case.

00:24:11.320 --> 00:24:13.310
And then, plus,
you get the locality issues

00:24:13.310 --> 00:24:15.800
where if you're working
with the value in registers.

00:24:16.300 --> 00:24:18.410
In this case,
what would happen is value would be

00:24:18.420 --> 00:24:21.300
assigned to a register as opposed
to going back to the array that you

00:24:21.390 --> 00:24:23.140
get performance boost there as well.

00:24:25.140 --> 00:24:28.780
This is sort of a
de-optimization of your code.

00:24:28.780 --> 00:24:32.360
One of the things that people run into,
especially on MP machines,

00:24:32.360 --> 00:24:36.950
is with a lot of threading,
if you have access to global values,

00:24:36.950 --> 00:24:39.400
they're kind of wondering why the
values are sort of changing or

00:24:39.400 --> 00:24:40.960
not changing from underneath them.

00:24:40.980 --> 00:24:43.820
Make sure that if you
have a global value,

00:24:43.900 --> 00:24:48.770
a global static that's being accessed
from several different threads

00:24:48.770 --> 00:24:53.650
or written to by other threads,
that you use the keyword volatile.

00:24:55.000 --> 00:24:56.870
One of the optimizations
a compiler will do is say,

00:24:56.930 --> 00:24:59.050
well, this is a value that I've
already got a copy of.

00:24:59.080 --> 00:25:00.940
Why should I go back
and get the original?

00:25:00.940 --> 00:25:04.370
If you put the keyword volatile,
this will guarantee that

00:25:04.370 --> 00:25:08.110
things will get reloaded every
time you access the variable.

00:25:10.640 --> 00:25:14.050
Use static finals.

00:25:14.060 --> 00:25:18.450
This basically specifies
that the variable,

00:25:18.450 --> 00:25:22.400
say in this case buffer size,
is a constant.

00:25:22.400 --> 00:25:24.760
And the compiler can treat
that as a constant all the way

00:25:24.760 --> 00:25:27.320
through the code and optimize,
do constant folding.

00:25:27.320 --> 00:25:30.840
In this example,
we know that the character array that

00:25:30.850 --> 00:25:33.210
we're allocating is a fixed size.

00:25:33.590 --> 00:25:38.660
We know that the initialization of
the buffer in the loop is going to

00:25:38.660 --> 00:25:41.420
iterate a fixed number of times.

00:25:41.420 --> 00:25:46.060
So take advantage of that by making
sure that you declare your statics

00:25:46.060 --> 00:25:49.420
as final if they're going to be
constant throughout your execution.

00:25:52.290 --> 00:25:56.440
There's a certain cost involved
in invoking anything which is a

00:25:56.440 --> 00:25:59.340
virtual call or an interface call.

00:25:59.340 --> 00:26:03.170
In the case of virtual calls,
we have to do an index into a

00:26:03.170 --> 00:26:07.460
table to find the address of the
method that we want to dispatch to.

00:26:07.460 --> 00:26:10.800
In interfaces,
it's a little bit more complicated

00:26:10.900 --> 00:26:15.670
because we actually have to do a match
to make sure that we match the class

00:26:15.680 --> 00:26:18.240
of the method that we want to invoke.

00:26:18.980 --> 00:26:23.150
So virtuals are a little
bit cheaper than interfaces.

00:26:23.150 --> 00:26:26.450
So if you have a choice,
try to stick with subclassing as

00:26:26.450 --> 00:26:29.580
opposed to creating interfaces,
and you'll get better

00:26:29.580 --> 00:26:30.900
performance that way.

00:26:30.900 --> 00:26:34.610
In the Hotspot VM,
we actually cache the call so that

00:26:34.610 --> 00:26:38.340
from any particular call site,
we know which method

00:26:38.340 --> 00:26:40.360
worked for us last time.

00:26:40.360 --> 00:26:45.240
We try to reuse that so we don't
actually do a lookup each time.

00:26:45.240 --> 00:26:48.950
But there is still a cost
in that initial lookup.

00:26:49.040 --> 00:26:53.500
and whatnot,
so try to use virtual versus interfaces.

00:26:54.660 --> 00:26:58.580
One of the optimizations that
we've done in the Hotspot

00:26:58.580 --> 00:27:03.300
compiler is dealing with switches.

00:27:03.740 --> 00:27:09.600
You can create switch statements with
fairly sparse values in your cases.

00:27:09.600 --> 00:27:13.490
In traditional compilation,
what would happen in those

00:27:13.490 --> 00:27:16.660
situations is that they would
create a big if-then-else.

00:27:16.660 --> 00:27:20.000
We were using a technique
of double indexing,

00:27:20.010 --> 00:27:24.050
which will allow us to actually
just dispatch basically

00:27:24.050 --> 00:27:26.470
fairly quickly on any switch.

00:27:26.530 --> 00:27:28.510
It's not a nested-if combination.

00:27:28.520 --> 00:27:34.960
So if you're comparing a single
variable against an integer data type,

00:27:35.100 --> 00:27:38.790
utilize switches over if statements.

00:27:42.230 --> 00:27:44.700
More and more, you know,
as more and more people

00:27:44.790 --> 00:27:48.880
are learning to program,
some of the new people new to programming

00:27:48.880 --> 00:27:54.680
have a tendency to use exceptions
for control of their program flow.

00:27:54.680 --> 00:28:01.040
You really should try to use exceptions
for the exceptional cases and not

00:28:01.040 --> 00:28:04.180
for actual flow of your program.

00:28:04.180 --> 00:28:06.520
So,
because there is quite a bit of cost in

00:28:06.520 --> 00:28:09.120
the VM to actually handle that exception.

00:28:09.970 --> 00:28:13.310
So, if you're going to,
if there's a good likelihood

00:28:13.380 --> 00:28:16.870
that the routine you're calling
is going to produce an error,

00:28:16.880 --> 00:28:20.180
then you should probably use error codes
and test the result when you come back

00:28:20.180 --> 00:28:21.900
as opposed to throwing the exception.

00:28:21.900 --> 00:28:26.870
Because that would be faster than
actually throwing the exception

00:28:27.090 --> 00:28:29.450
and having the VM deal with it.

00:28:32.090 --> 00:28:37.240
And finally,
I think you should think pure Java.

00:28:38.180 --> 00:28:40.330
In the 1.3.1 code,
we've implemented something

00:28:40.340 --> 00:28:43.190
called compiled natives,
which allow you to call

00:28:43.190 --> 00:28:45.760
JNI code fairly efficiently.

00:28:45.760 --> 00:28:50.120
We don't have to go through
any kind of marshaling code,

00:28:50.120 --> 00:28:53.620
which marshals up the parameters and
then goes off and calls the routine.

00:28:53.620 --> 00:28:56.870
What happens with these
compiled natives is that we

00:28:56.870 --> 00:29:01.550
actually have a thunking layer,
which already knows what the parameters

00:29:01.550 --> 00:29:06.980
are going to look like and assembles the
parameters for a call to the JNI routine.

00:29:08.120 --> 00:29:11.180
So on one side,
your JNI calls are going to

00:29:11.310 --> 00:29:16.420
be a little bit more efficient
and faster on the 1.3.1.

00:29:16.420 --> 00:29:19.740
But on the other side,
there still is a cost

00:29:19.770 --> 00:29:24.140
in using JNI or JDirect,
which is built on top of JNI.

00:29:24.140 --> 00:29:28.580
There's this translation layer involved,
and it costs.

00:29:28.580 --> 00:29:31.690
And then also,
if you're dealing with callbacks,

00:29:31.690 --> 00:29:34.460
it's going to require
some kind of lookup.

00:29:34.500 --> 00:29:38.060
So you should try to use
Java wherever you possibly can.

00:29:38.100 --> 00:29:40.440
And try to avoid going off.

00:29:40.800 --> 00:29:42.960
And as time goes on,
we're going to get the

00:29:42.990 --> 00:29:44.790
compiler faster and faster.

00:29:44.880 --> 00:29:48.130
And you can forget about C and C++.

00:29:48.140 --> 00:29:53.550
Okay, with that, I'll pass it on to Andy.

00:29:58.600 --> 00:30:00.490
Hi.

00:30:00.490 --> 00:30:04.370
So I get to do my little bit now where
I talk about how important measuring is.

00:30:04.480 --> 00:30:07.330
So all of this,
all of the information that we've

00:30:07.340 --> 00:30:10.620
been giving you is kind of useless if
you then go and apply it willy-nilly

00:30:10.620 --> 00:30:12.750
to your entire 60 megabyte code base.

00:30:12.770 --> 00:30:14.800
It's really important.

00:30:14.850 --> 00:30:19.780
All the textbook advice that
says don't optimize prematurely.

00:30:19.900 --> 00:30:21.360
It's really true.

00:30:21.420 --> 00:30:24.320
What you should be doing is measuring,
finding the major bottlenecks,

00:30:24.430 --> 00:30:27.930
optimizing those bottlenecks,
making sure that it actually worked

00:30:28.080 --> 00:30:33.040
because we've seen optimizations that
have actually slowed things down.

00:30:33.080 --> 00:30:36.560
I'll also go through what
you should try and measure.

00:30:36.760 --> 00:30:41.070
How with the 131 Java VM we've improved
things and how you've actually got tools

00:30:41.650 --> 00:30:43.110
to enable you to measure those things.

00:30:43.140 --> 00:30:46.770
And then I'll just cover a few
little myths that are still

00:30:46.770 --> 00:30:51.740
around in all those textbooks that
are not quite true on Mac OS X.

00:30:53.740 --> 00:30:56.820
So the first obvious thing
that you always think of is

00:30:56.900 --> 00:30:58.370
how fast is my program going?

00:30:58.580 --> 00:31:02.640
You look at the CPU meter on
Mac OS X and it's pegged at 100%.

00:31:03.000 --> 00:31:07.640
So obviously you should be looking
at where the CPU time is going.

00:31:07.660 --> 00:31:12.470
Your program, whatever it's doing,
is CPU limited.

00:31:13.290 --> 00:31:15.450
The first thing that we do
for you in Hotspot is we

00:31:15.450 --> 00:31:16.890
compile all those hot methods.

00:31:17.020 --> 00:31:20.500
We're counting which one gets
used most and we're compiling

00:31:20.500 --> 00:31:23.540
the ones that get used,
get called most frequently.

00:31:23.750 --> 00:31:28.510
So, obviously look at the hot methods,
look at those ones that

00:31:28.620 --> 00:31:29.440
are being compiled.

00:31:29.500 --> 00:31:32.260
And I'll cover how using
Xprof will actually tell you

00:31:32.260 --> 00:31:34.770
which ones have been compiled.

00:31:34.870 --> 00:31:41.460
Now secondly, depending on where that
CPU meter is reading,

00:31:41.460 --> 00:31:45.340
you might be using system
CPU and not user CPU.

00:31:45.340 --> 00:31:47.200
In which case you might be paging.

00:31:47.200 --> 00:31:50.680
And the poor old OS is trying to just
read and write things from disk and

00:31:50.680 --> 00:31:52.860
shuffle things around in the VM system.

00:31:53.340 --> 00:31:55.340
Paging is really expensive.

00:31:55.420 --> 00:32:02.560
So if you're running on a 128 meg system
and you set your heap to 256 megs,

00:32:02.820 --> 00:32:06.600
well, we think you've got 256 megs so
we'll happily go and allocate

00:32:06.600 --> 00:32:09.840
and we won't do full GCs until
we think we've run out of heap.

00:32:09.860 --> 00:32:11.680
But in the meantime
you'll be paging madly.

00:32:11.680 --> 00:32:18.290
So think very much about controlling
your footprint and heap usage.

00:32:20.320 --> 00:32:24.030
Now other times you get into
situations where your CPU isn't pegged,

00:32:24.080 --> 00:32:28.280
and in fact, at first glance your program
seems to be doing nothing.

00:32:28.290 --> 00:32:29.360
And that's probably what it is doing.

00:32:29.360 --> 00:32:33.830
It's probably waiting for the
disk or the network to reply.

00:32:34.000 --> 00:32:37.540
So there are some tools on Mac OS X,
some of which are covered,

00:32:37.550 --> 00:32:39.200
I will mention here,
some of which are covered in the

00:32:39.270 --> 00:32:42.450
performance tools talk later,
that allow you to look at

00:32:42.580 --> 00:32:46.820
what your program is doing
I/O wise and network wise.

00:32:47.590 --> 00:32:49.800
And then lastly,
one of the things that we talked about,

00:32:49.940 --> 00:32:52.230
synchronization.

00:32:52.230 --> 00:32:54.690
Monitor contention can
get very expensive.

00:32:54.690 --> 00:32:57.750
And the reason for that is that,
especially on 10,

00:32:57.750 --> 00:33:02.210
if you're used to Mac OS 9,
switching threads and processes

00:33:02.210 --> 00:33:06.550
was relatively expensive because it
didn't have the memory protection

00:33:06.550 --> 00:33:08.770
and preemption behind it.

00:33:09.280 --> 00:33:13.680
Whereas on 10, when we switch a thread,
there's all the state in the processor

00:33:13.820 --> 00:33:15.960
that has to be saved out to memory.

00:33:15.960 --> 00:33:18.200
And when we switch processes, i.e.

00:33:18.200 --> 00:33:21.020
threads between processes,
you've got to save all

00:33:21.150 --> 00:33:22.580
of that context as well.

00:33:22.580 --> 00:33:27.750
So it's a lot more expensive than 9,
so that's one thing to bear in mind.

00:33:30.090 --> 00:33:33.340
So how do you go about measuring all
of the things that I've talked about?

00:33:33.370 --> 00:33:38.280
The best thing is, from your perspective,
is to use a commercial performance tool.

00:33:38.310 --> 00:33:42.160
One example of which is Optimizeit,
which Scott will be demoing

00:33:42.170 --> 00:33:44.590
just after I've talked.

00:33:44.930 --> 00:33:48.200
It provides CPU profiling
and/or sampling.

00:33:48.290 --> 00:33:52.900
So profiling is a way of tracking each
and every time methods get called.

00:33:53.040 --> 00:33:55.900
Sampling does a statistical analysis.

00:33:55.920 --> 00:33:56.920
There are pros and cons of each.

00:33:57.100 --> 00:33:59.340
Profiling gives you a very
precise measure of exactly

00:33:59.340 --> 00:34:00.760
how often things get called.

00:34:00.900 --> 00:34:02.900
Sampling is less invasive.

00:34:02.910 --> 00:34:04.900
Your program doesn't slow down so much.

00:34:04.900 --> 00:34:08.250
So depending on what you're doing,
one or the other is better.

00:34:08.530 --> 00:34:11.260
You can also look at object allocation,
which objects are getting allocated,

00:34:11.360 --> 00:34:13.000
where they get allocated, etc.

00:34:13.180 --> 00:34:16.190
Scott will cover a lot of
that in the demonstration,

00:34:16.190 --> 00:34:16.910
I think.

00:34:17.800 --> 00:34:22.670
The other thing you can do
with Hotspot that we provide

00:34:23.410 --> 00:34:28.300
in the 1.3.1 developer preview,
HProf is now functional.

00:34:28.370 --> 00:34:30.810
It wasn't in Cheetah.

00:34:32.200 --> 00:34:37.500
And that's--HPROF is implemented
as a library loaded at runtime that

00:34:37.500 --> 00:34:40.900
uses the JVMPI interface in Hotspot.

00:34:41.300 --> 00:34:44.370
Secondly, you can use XPROF,
which is a per thread

00:34:44.840 --> 00:34:46.320
kind of measurement.

00:34:46.330 --> 00:34:52.010
And there's minus XA-PROF issue
allocation information.

00:34:52.950 --> 00:34:57.160
So as I mentioned,
HProf comes with the developer preview

00:34:57.160 --> 00:35:00.260
one that's available on the website.

00:35:00.360 --> 00:35:02.500
It's a basic CPU and
monitor profiling tool.

00:35:02.500 --> 00:35:07.110
So it doesn't give you a lot of
it gives you a lot of nitty gritty

00:35:07.110 --> 00:35:08.800
detail and not a lot of analysis.

00:35:08.890 --> 00:35:13.290
There's a relatively simple
UI available from Javasoft's website

00:35:13.500 --> 00:35:17.020
that gives you a primitive GUI on top
and lets you drill down a little bit.

00:35:17.030 --> 00:35:21.150
And I've used that to a certain
extent and that's quite helpful.

00:35:21.490 --> 00:35:22.900
It's relatively simple to use.

00:35:22.960 --> 00:35:25.660
You just pass a couple of command
line parameters and you tell

00:35:25.660 --> 00:35:28.900
it whether you want to sample
or look at monitor contention,

00:35:28.900 --> 00:35:29.180
etc.

00:35:31.700 --> 00:35:37.510
And this is the,
it turns out that the perf-vanil

00:35:37.510 --> 00:35:40.390
tool only works with CPU sampling.

00:35:40.390 --> 00:35:43.750
It doesn't work if you use profiling.

00:35:44.160 --> 00:35:48.500
You should use the first example.

00:35:48.500 --> 00:35:51.340
The monitor contention will give you
a little bit of information about

00:35:51.440 --> 00:35:55.280
how much time each thread spends
waiting on a particular monitor.

00:35:55.290 --> 00:36:02.750
So if you're seeing an application that
you can't really see why it's slow,

00:36:02.900 --> 00:36:08.210
but there seems to be a lot going on,
probably one of the first things you

00:36:08.250 --> 00:36:10.640
should do is look at monitor contention.

00:36:10.640 --> 00:36:13.840
You can see dramatic performance
improvements there because

00:36:14.000 --> 00:36:17.520
when we get contention as
to the cases where we don't,

00:36:17.520 --> 00:36:22.160
as Ivan explained earlier,
it's a case of going through ten

00:36:22.330 --> 00:36:27.710
instructions in line in the interpreter
or the compiler versus several thousand

00:36:27.820 --> 00:36:33.180
cycles going into the kernel and
doing context switches and the like.

00:36:33.180 --> 00:36:33.180
So that's why it's expensive.

00:36:34.000 --> 00:36:38.290
MinusXAProf will give you a
simple allocation profile.

00:36:38.310 --> 00:36:40.600
So you run your program and
right at the end it'll just,

00:36:40.660 --> 00:36:42.550
when it exits,
it'll spit out this dump of all

00:36:42.550 --> 00:36:44.960
the objects that got allocated,
how much space they took up,

00:36:45.070 --> 00:36:47.650
the average instance size, etc.

00:36:47.650 --> 00:36:51.020
And you can, just from that information,
you can say, well,

00:36:51.160 --> 00:36:55.230
maybe I shouldn't be allocating so
many vectors or hash tables or etc.

00:36:56.850 --> 00:36:59.810
But it doesn't give you any information
about where they got allocated,

00:36:59.880 --> 00:37:01.880
which is why it's, you know,
optimize it or something like

00:37:01.880 --> 00:37:02.880
that is much more useful.

00:37:04.520 --> 00:37:09.140
MinusXProf is of somewhat
limited use because it gives

00:37:09.210 --> 00:37:11.470
you per-thread information.

00:37:11.560 --> 00:37:14.530
So if you have a program
that forks 400 threads,

00:37:14.530 --> 00:37:18.020
like VolanoMock or something
that I tend to run on and off,

00:37:18.210 --> 00:37:21.730
at the end of the program when it exits,
it spits out 400 copies

00:37:21.810 --> 00:37:23.840
of the information,
which is not very useful.

00:37:23.840 --> 00:37:29.520
But it is the only way that I know
of where we actually list out the

00:37:29.520 --> 00:37:33.580
methods that got compiled versus
the ones that get interpreted,

00:37:33.620 --> 00:37:36.170
and how much time we spend in
interpreted code versus native

00:37:36.290 --> 00:37:39.290
code versus compiled code,
and how much time we spend in GC, etc.

00:37:39.370 --> 00:37:42.640
So that can give you some
very useful insight on,

00:37:42.640 --> 00:37:45.700
first of all, which methods got compiled.

00:37:45.700 --> 00:37:48.680
And you might look at it and say,
hang on a minute,

00:37:48.680 --> 00:37:52.390
I expected method A to get compiled
because I was under the impression

00:37:52.390 --> 00:37:54.280
that this was my most expensive method.

00:37:54.280 --> 00:37:57.190
But it turns out that in fact
we didn't go anywhere near that,

00:37:57.190 --> 00:37:59.910
we didn't compile it,
or maybe we did and we couldn't compile

00:38:00.010 --> 00:38:03.900
it because it's got some funny assembler
or some construct or it's too big,

00:38:03.920 --> 00:38:04.460
etc.

00:38:04.470 --> 00:38:09.820
So that will pinpoint which
methods are getting compiled.

00:38:09.820 --> 00:38:12.950
You can sanity check that the
ones that are getting compiled

00:38:13.030 --> 00:38:14.260
are the ones you expect.

00:38:16.970 --> 00:38:18.560
Once you know the ones
that are getting compiled,

00:38:18.560 --> 00:38:21.660
you can then focus your
optimizations on those methods.

00:38:23.510 --> 00:38:25.740
And as a little example
use down at the bottom,

00:38:25.740 --> 00:38:27.500
it's very simple to use.

00:38:27.500 --> 00:38:29.420
But like I said,
don't try it with 400 threads.

00:38:29.420 --> 00:38:31.160
Just do it on something
with a minimal number.

00:38:33.760 --> 00:38:39.440
Now measuring memory is a little
harder because the Java VM has several

00:38:39.440 --> 00:38:42.420
different perspectives on what memory is.

00:38:42.510 --> 00:38:44.420
As far as you are concerned,
the only memory you can

00:38:44.420 --> 00:38:48.460
really have any control of is
the memory in the Java heap.

00:38:48.490 --> 00:38:52.680
The tips that Ivan explained
whereby you null out references,

00:38:52.680 --> 00:38:54.520
you try and avoid using finalizers, etc.

00:38:54.520 --> 00:38:57.310
That's the kind of thing you can control.

00:38:57.830 --> 00:39:03.370
Other than that,
so you can watch the heap as it grows

00:39:03.370 --> 00:39:06.720
and shrinks using the VeloC flag.

00:39:07.100 --> 00:39:09.470
As we mentioned,
verbose class will show you

00:39:09.470 --> 00:39:11.100
classes as they get loaded.

00:39:11.100 --> 00:39:15.580
You might see classes getting
loaded before you think

00:39:15.580 --> 00:39:17.620
you should be using them,
and that's an example where you

00:39:17.790 --> 00:39:20.100
should sort of go in and pinpoint
why they're getting pulled in,

00:39:20.100 --> 00:39:21.780
and maybe you can load them a bit later.

00:39:23.780 --> 00:39:29.010
There's a command called top which
will give you an overall memory

00:39:29.230 --> 00:39:30.570
View of the whole system.

00:39:30.770 --> 00:39:35.740
And that's good for splitting
out memory that's being shared.

00:39:35.740 --> 00:39:38.640
For example,
when you run multiple Java processes,

00:39:38.640 --> 00:39:41.560
some of the memory that we pull
in from the shared generation is

00:39:41.560 --> 00:39:43.370
shared between several processes.

00:39:43.450 --> 00:39:47.760
And you can tell the difference between
memory that's privately allocated,

00:39:47.760 --> 00:39:51.110
I used in the heap for you,
versus memory that's being shared

00:39:51.110 --> 00:39:54.980
in the shared generation or is being
shared because of dynamic libraries

00:39:55.250 --> 00:39:59.080
that are being pulled in by native code,
either your code or our.

00:39:59.100 --> 00:40:03.180
VM Map is another command line utility.

00:40:03.180 --> 00:40:06.900
It gives you a lot more specific
insight on the intricacies of

00:40:06.960 --> 00:40:08.690
the virtual memory being used.

00:40:08.780 --> 00:40:10.480
And it's relatively complicated.

00:40:10.690 --> 00:40:13.700
If you want to learn a bit more about it,
they might cover it in

00:40:13.700 --> 00:40:15.100
the performance tools.

00:40:15.280 --> 00:40:19.600
It turns out that Java VMs are
improving faster than the books about

00:40:19.600 --> 00:40:22.110
performance in Java can be written.

00:40:22.190 --> 00:40:25.840
So there are quite a few books out there,
most of which contain

00:40:25.840 --> 00:40:27.450
extremely good advice.

00:40:27.580 --> 00:40:31.180
But some of their tips just...
become outdated with time

00:40:31.180 --> 00:40:33.480
as the technology rolls on.

00:40:34.240 --> 00:40:41.410
Um, traditionally in 1.0 VMs, 1.1 VMs,
allocation was very slow.

00:40:41.590 --> 00:40:45.030
They had a malloc based
allocation scheme or something.

00:40:45.550 --> 00:40:48.100
Our allocation is now extremely cheap.

00:40:48.100 --> 00:40:51.550
Now the initialization
of an object may not be,

00:40:51.610 --> 00:40:55.090
but allocating it is few instructions.

00:40:55.880 --> 00:40:58.270
As a result of that,
and as a result of the scavenging,

00:40:58.410 --> 00:41:02.160
short-lived objects are very
cheap to GC because we essentially

00:41:02.160 --> 00:41:03.040
don't do anything with them.

00:41:03.040 --> 00:41:06.240
We just throw them away at
the end of their life cycle.

00:41:06.400 --> 00:41:09.940
Synchronized methods cost us small,
as Ivan mentioned.

00:41:11.220 --> 00:41:13.510
And the contended case
is still expensive.

00:41:13.810 --> 00:41:18.670
Now, lastly, as I hinted at before,
system calls which involve

00:41:19.070 --> 00:41:21.900
entry into the kernel,
just because of the whole context

00:41:21.900 --> 00:41:25.480
switching and a little bit more
weight involved than with Mac OS 9,

00:41:25.570 --> 00:41:26.720
they are expensive.

00:41:26.800 --> 00:41:30.280
There are certain things that we
do on your behalf as part of the

00:41:30.280 --> 00:41:34.630
Java APIs that involve system calls,
network operations, I/O operations,

00:41:34.630 --> 00:41:36.480
things like that, thread yield.

00:41:36.560 --> 00:41:37.950
All of those are system calls.

00:41:38.200 --> 00:41:40.630
So if you don't need
to do things like that,

00:41:40.630 --> 00:41:41.490
avoid them.

00:41:42.350 --> 00:41:45.390
So here's a quick graph
which you've seen before.

00:41:45.630 --> 00:41:49.070
This is the peak allocation performance
from various different technologies.

00:41:49.070 --> 00:41:51.060
I think Blaine showed it in his talk.

00:41:51.110 --> 00:41:55.270
And you can see that this includes
the garbage collection side of things,

00:41:55.370 --> 00:41:56.900
so it's not just allocation.

00:41:57.030 --> 00:41:59.880
And you see that Compile Java,
which is the tall one,

00:42:00.230 --> 00:42:03.190
is just way faster than
any other technology.

00:42:04.610 --> 00:42:08.480
Now, so here's an example that
I pulled from a performance book

00:42:08.630 --> 00:42:11.160
published a year or two ago.

00:42:11.280 --> 00:42:14.820
They gave us an example of one thing
you can do to improve performance:

00:42:14.820 --> 00:42:17.470
pooling objects.

00:42:17.690 --> 00:42:21.980
So that you avoid, by recycling them,
you avoid the cost of

00:42:22.060 --> 00:42:23.890
allocating and GCing them.

00:42:24.020 --> 00:42:30.160
So I wrote a little benchmark
and I ran it on my G4 PowerBook.

00:42:31.210 --> 00:42:32.820
And I got this sort of description.

00:42:32.910 --> 00:42:37.740
So as I increased the number of threads,
you can see for the single

00:42:37.740 --> 00:42:41.700
and two-threaded cases,
the pooling is just slightly faster.

00:42:41.700 --> 00:42:45.240
So I'm allocating 100,000 and then...

00:42:45.830 --> 00:42:47.040
Filling them up and
then throwing them away,

00:42:47.040 --> 00:42:47.250
etc.

00:42:47.300 --> 00:42:51.280
But when you get to a
larger number of threads,

00:42:51.300 --> 00:42:56.040
you can see that the time taken
to actually recycle these vectors

00:42:56.040 --> 00:42:59.480
is actually longer than it took
to create them and GC them.

00:43:01.740 --> 00:43:06.380
Now, in the dual processor case,
the moment you go to anything

00:43:06.520 --> 00:43:11.160
other than single threaded,
the simple allocate and throw

00:43:11.460 --> 00:43:13.600
away mechanism is faster.

00:43:13.600 --> 00:43:16.490
And the point being is that you
don't have to incorporate any

00:43:16.490 --> 00:43:19.510
complicated pooling code if you
just do the brain dead thing and

00:43:19.560 --> 00:43:21.500
just allocate and throw it away.

00:43:21.500 --> 00:43:26.540
So, this is one example where the
technology has just moved on and

00:43:26.540 --> 00:43:31.500
that little truth about pooling
things is not quite so true.

00:43:31.980 --> 00:43:34.170
Now,

00:43:34.430 --> 00:43:38.630
On the other hand, if you have an object,
in this particular example I'm

00:43:38.690 --> 00:43:42.110
talking about a Java thread,
that is extremely expensive

00:43:42.280 --> 00:43:44.700
to create and initialize.

00:43:44.790 --> 00:43:48.930
The expense of creating a thread
involves a couple of kernel trips to

00:43:49.060 --> 00:43:51.380
create the internal data structures.

00:43:51.700 --> 00:44:01.000
[Transcript missing]

00:44:01.980 --> 00:44:05.840
The corollary is that sometimes,
especially with something like a thread,

00:44:05.970 --> 00:44:09.350
which involves a kernel data structure,
it can actually be costly

00:44:09.350 --> 00:44:10.610
to keep them around as well.

00:44:10.620 --> 00:44:15.000
So you have this trade-off between
some things get more performant,

00:44:15.090 --> 00:44:17.960
but on the other hand,
you have to pay the penalty of

00:44:18.050 --> 00:44:21.930
keeping the kernel-wide memory around,
and the extra stack, etc.

00:44:21.980 --> 00:44:26.440
So this little graph just,
there's an example web

00:44:26.450 --> 00:44:35.170
server on Sun's site,
which, brain dead simple, it just,

00:44:35.170 --> 00:44:35.170
it's a web server.

00:44:37.100 --> 00:44:45.040
sits in a socket listening for a request,
gets a request, it hands it off to a

00:44:45.040 --> 00:44:47.770
thread to respond to it,
and sends back a response.

00:44:47.780 --> 00:44:52.350
So I took that example and
I produced three variants,

00:44:52.460 --> 00:44:55.590
one of which forks a new
thread for every request,

00:44:55.720 --> 00:44:59.990
the second one which uses a
pooled collection of threads,

00:44:59.990 --> 00:45:04.920
and then a third one where all the worker
threads themselves actually sit and

00:45:04.920 --> 00:45:08.750
accept and handle the request directly,
so there is no listen thread.

00:45:08.760 --> 00:45:13.900
And the purple, sorry, the line...

00:45:15.470 --> 00:45:18.720
Well, this is the response time
as seen by the client.

00:45:18.720 --> 00:45:23.420
So with the version that forks a
different thread for every request,

00:45:23.810 --> 00:45:25.940
it just doesn't scale as the
number of requests go up.

00:45:25.940 --> 00:45:31.030
You just see a response time
that degrades with n squared

00:45:31.120 --> 00:45:35.040
according to the number of clients.

00:45:35.560 --> 00:45:39.400
The others degrade as well,
but they degrade much more gracefully.

00:45:39.510 --> 00:45:41.730
Now interestingly,

00:45:43.780 --> 00:45:48.060
I had kind of expected when I did
this exercise that the version running

00:45:48.060 --> 00:45:53.700
multiple threads in Accept would scale
even better than the pooled version.

00:45:53.790 --> 00:45:57.260
And lo and behold,
it isn't actually that true.

00:45:57.410 --> 00:46:01.440
So that's actually a--I really
wanted to include this slide

00:46:01.440 --> 00:46:04.370
because it's an indication of
why you should be measuring.

00:46:05.910 --> 00:46:09.220
Because my expectations were dashed.

00:46:09.270 --> 00:46:13.090
Now on a dual processor it's
really interesting as well because

00:46:14.580 --> 00:46:20.220
The Perth Red one seems to
be doing almost as well as

00:46:20.220 --> 00:46:24.220
the the pooled versions.

00:46:24.830 --> 00:46:27.060
That was somewhat unexpected.

00:46:27.060 --> 00:46:29.330
But then I realized that what's
happening in the pooled versions

00:46:29.410 --> 00:46:32.460
is that I'm getting a lot more
contention because I'm on an MP system.

00:46:33.220 --> 00:46:37.370
So the version where I'm running
multiple workers in Accept is

00:46:37.410 --> 00:46:39.330
actually the best performing one.

00:46:42.430 --> 00:46:46.040
And the reason for that is
because all of the contention

00:46:46.220 --> 00:46:48.840
is handled right in the kernel,
right at the accept call,

00:46:49.360 --> 00:46:54.680
rather than everything coming
out and fighting over the socket.

00:46:58.210 --> 00:47:01.980
So our conclusion is very simple.

00:47:02.030 --> 00:47:03.860
Your application design is paramount.

00:47:03.860 --> 00:47:07.820
That's the most important part
of the performance of your app.

00:47:09.190 --> 00:47:14.240
There's a lot of new stuff in recent
VMs and in Hotspot and on Mac OS X that

00:47:14.240 --> 00:47:17.480
have improved some of those bottlenecks.

00:47:18.490 --> 00:47:22.740
If you follow our advice,
you'll get better compiled code,

00:47:22.830 --> 00:47:25.110
and so your app will run faster.

00:47:25.270 --> 00:47:27.900
And where you are seeing bottlenecks,
just keep measuring and improving

00:47:27.900 --> 00:47:29.700
those things and you'll see results.

00:47:34.000 --> 00:47:37.040
So what we're going to do now,
Scott's going to come up and give

00:47:37.040 --> 00:47:39.650
a demonstration of Optimizeit.

00:47:40.310 --> 00:47:42.210
So I'm going to show you Optimize It.

00:47:42.330 --> 00:47:47.280
I showed the memory portion of Optimize
It at the Java development tools session.

00:47:47.300 --> 00:47:51.180
So I'm going to concentrate
on the profiling portion.

00:47:51.180 --> 00:47:53.840
It's a tool written by a
company called VM Gear.

00:47:53.840 --> 00:47:55.860
They used to be called Intuitive.

00:47:55.860 --> 00:47:57.290
It's a pure Java tool.

00:47:57.290 --> 00:48:00.010
It uses just a tiny
little bit of native code,

00:48:00.010 --> 00:48:01.500
so it's mostly pure.

00:48:03.140 --> 00:48:06.020
The way it works is it just
sort of instantiates its own

00:48:06.020 --> 00:48:08.660
hooks between you and the VM,
and then you run your

00:48:08.730 --> 00:48:11.890
application on top of it,
and then you're able to just look

00:48:11.890 --> 00:48:15.490
at all the same kind of profiling
stuff that you see in like Xprof and

00:48:15.490 --> 00:48:17.910
memory profiling and all that stuff.

00:48:18.070 --> 00:48:18.910
It's really cool.

00:48:18.910 --> 00:48:19.850
We use it at Apple.

00:48:19.860 --> 00:48:22.050
We've been helping them
get it up and running,

00:48:22.050 --> 00:48:25.280
and we've been using it to actually
work on all of our AWT work and swing

00:48:25.280 --> 00:48:27.320
work to find all of our bottlenecks.

00:48:27.320 --> 00:48:30.990
I mean, it saved us tons of time,
and we're hoping that we

00:48:31.000 --> 00:48:33.080
can convince them to get it.

00:48:33.160 --> 00:48:35.210
We've got a developer
release out for you guys.

00:48:35.210 --> 00:48:37.320
They've committed to a
fourth quarter release,

00:48:37.320 --> 00:48:39.870
so that's good that we have
it coming out eventually.

00:48:39.880 --> 00:48:43.530
So let me just go right
to my demo machine.

00:48:43.890 --> 00:48:47.640
There's not that much
I have to say about it,

00:48:47.740 --> 00:48:51.460
but this is my CAN sorting table demo.

00:48:51.460 --> 00:48:53.190
This is right out of SwingSet.

00:48:53.190 --> 00:48:55.380
I just put in names
of people on our team,

00:48:55.380 --> 00:48:57.200
and I added some sorting to it.

00:48:57.300 --> 00:48:59.580
I didn't use any of
the collection classes.

00:48:59.580 --> 00:49:00.760
I wrote my own sort.

00:49:02.100 --> 00:49:04.200
So I wrote the worst sort possible.

00:49:04.200 --> 00:49:05.760
I do a little bubble sort here.

00:49:05.760 --> 00:49:08.900
So it's kind of slow.

00:49:08.900 --> 00:49:11.180
So I click here,
and I sort by first name,

00:49:11.180 --> 00:49:12.440
and that's sorted.

00:49:12.440 --> 00:49:15.800
And that's only 58 items
that I sorted by first name,

00:49:15.800 --> 00:49:17.290
and I sort by last name.

00:49:17.300 --> 00:49:19.840
So that's not really good,
and I actually pause all of

00:49:19.840 --> 00:49:21.220
the UI while I'm sorting.

00:49:21.220 --> 00:49:23.610
So I want to figure out what's going on.

00:49:23.620 --> 00:49:25.400
Why is this taking so long?

00:49:25.470 --> 00:49:28.010
So what I'm going to do is I'm
going to go over to Optimize It,

00:49:28.010 --> 00:49:30.980
which I've already launched,
and if I want to hook into this,

00:49:31.130 --> 00:49:33.390
I started this other app
using Optimize It stubs,

00:49:33.490 --> 00:49:35.960
and I'm going to do this
through remote debugging.

00:49:35.960 --> 00:49:38.860
You can launch it all through here,
but I kind of like doing the

00:49:38.950 --> 00:49:41.570
remote thing because it shows you
can do it on a separate machine.

00:49:41.580 --> 00:49:44.460
So I'm going to go to Remote Application.

00:49:44.460 --> 00:49:47.040
It's already been set up on
this machine on this port.

00:49:47.040 --> 00:49:49.780
I have my source path set up,
so I'll just attach to that.

00:49:51.310 --> 00:49:53.260
and it'll just take a
second for it to connect up.

00:49:53.260 --> 00:49:56.530
It gets, I had this all set up
for my demo this morning.

00:49:56.530 --> 00:50:00.580
So this is the memory,
the memory profile of everything,

00:50:00.580 --> 00:50:02.440
all the objects that have
been instantiated and there's

00:50:02.440 --> 00:50:03.630
a lot of cool stuff in here.

00:50:03.740 --> 00:50:06.260
But I'm going to go
into the CPU profiler.

00:50:06.300 --> 00:50:10.610
So the CPU profiler hasn't been
profiling until you tell it to.

00:50:10.610 --> 00:50:13.610
So that's one of the big
differences between this and like

00:50:13.610 --> 00:50:16.980
X-Prof is you get to profile just
a segment of the application that

00:50:17.060 --> 00:50:20.460
you want and you can turn it on,
do your work, turn it back off.

00:50:21.050 --> 00:50:23.240
So what I'm going to do is I'm
going to press on the button,

00:50:23.240 --> 00:50:29.370
go to my application and click
sort and click sort again.

00:50:29.530 --> 00:50:30.930
Let's do another one.

00:50:30.950 --> 00:50:33.180
Then I'll go back here and I'll stop.

00:50:34.830 --> 00:50:37.360
So here's one of the cool things.

00:50:37.600 --> 00:50:38.340
Here are all of our threads.

00:50:38.430 --> 00:50:42.530
Red is idle, green is active.

00:50:42.550 --> 00:50:45.380
And there are even groups of threads,
like main, system.

00:50:45.380 --> 00:50:49.530
So if I just start generally at the
main thread and I'm looking around

00:50:49.570 --> 00:50:51.570
trying to see what's going on.

00:50:51.660 --> 00:50:56.260
And let me flip this around
into the normal execution path.

00:50:56.270 --> 00:50:59.820
So what we have right now is 49% of
this happened through event dispatch,

00:50:59.820 --> 00:51:03.170
which makes sense because we clicked
on buttons to do most of our work.

00:51:03.740 --> 00:51:06.510
And then 34% of it was in thread run.

00:51:06.740 --> 00:51:09.730
Now, I know I wrote this application
and I have a separate thread that

00:51:09.910 --> 00:51:11.710
gets spawned off every time I sort.

00:51:11.770 --> 00:51:12.780
I actually created a new thread.

00:51:12.780 --> 00:51:14.480
I wrote this really badly.

00:51:14.480 --> 00:51:18.410
So I spawned a new
thread and I run my sort.

00:51:18.420 --> 00:51:22.600
So if I look through here and I can
see that I have my sort and it ends up

00:51:22.690 --> 00:51:28.250
calling greater than because I'm doing an
excellent single directional bubble sort.

00:51:28.250 --> 00:51:32.620
And I have a greater than and I have
some of my time in two string.

00:51:32.780 --> 00:51:34.700
So let's see what else is going
on inside of greater than.

00:51:34.700 --> 00:51:36.820
I've got a compare
inside of greater than.

00:51:36.820 --> 00:51:39.250
And there's something
called two lowercase.

00:51:39.260 --> 00:51:41.620
So that immediately is,
there's something going on here.

00:51:41.620 --> 00:51:44.870
And I can look, if I just click here and
I double click on this,

00:51:44.870 --> 00:51:47.030
it'll bring up my source code viewer.

00:51:47.280 --> 00:51:51.420
And I see there's some two lowercase
that's inside of the AWT code.

00:51:51.420 --> 00:51:53.010
And I don't care that much about it.

00:51:53.110 --> 00:51:55.860
But two lowercase is
taking up a lot of time.

00:51:55.980 --> 00:51:58.790
And that's inside of,
there's a whole bunch

00:51:58.790 --> 00:52:00.620
of things inside of AWT.

00:52:00.620 --> 00:52:01.820
But I want to see my stuff.

00:52:01.820 --> 00:52:01.820
So mine is, I want to see my stuff.

00:52:01.820 --> 00:52:04.620
So my sort data is my class.

00:52:04.620 --> 00:52:07.020
And let's see what's
going on in my compare.

00:52:07.040 --> 00:52:09.190
And I can see right here
that I get two strings.

00:52:09.190 --> 00:52:11.910
And to compare them,
I turn them into lowercase first because

00:52:11.910 --> 00:52:14.850
I want them to be case insensitive.

00:52:14.860 --> 00:52:17.320
And then I compare them
character by character.

00:52:17.320 --> 00:52:21.610
And, okay, so you know that's really bad.

00:52:21.850 --> 00:52:24.760
You know, actually,
I've had an engineer who's done this

00:52:24.780 --> 00:52:26.820
before at another company I was at.

00:52:26.960 --> 00:52:30.860
So there are a lot of
things that you could,

00:52:30.860 --> 00:52:30.860
this immediately drives you crazy.

00:52:30.860 --> 00:52:32.200
So I'm going to go ahead and do that.

00:52:32.200 --> 00:52:33.400
And this immediately drives you there.

00:52:33.400 --> 00:52:34.920
Now, I sort of dove around here
because I wanted you to

00:52:34.960 --> 00:52:35.810
see this allocation graph.

00:52:35.900 --> 00:52:36.800
It's pretty cool.

00:52:36.800 --> 00:52:39.690
It shows you, you know,
each allocation entry point

00:52:39.720 --> 00:52:41.200
and how much time is spent.

00:52:41.200 --> 00:52:43.270
You can even get sub percentages.

00:52:43.350 --> 00:52:46.560
So if I select here, I think,
if I mouse over here,

00:52:46.560 --> 00:52:51.620
it says that 99.25% of the time
is spent inside of this compare.

00:52:51.620 --> 00:52:52.750
And that's just my compare.

00:52:52.760 --> 00:52:59.900
That's not anything else going on.

00:52:59.900 --> 00:53:02.480
If I want to see right down here,
this would tell me immediately

00:53:02.860 --> 00:53:03.930
what the problems are.

00:53:03.930 --> 00:53:05.060
These are my hotspots.

00:53:05.170 --> 00:53:07.300
And this is just taking
the individual methods,

00:53:07.310 --> 00:53:09.600
no matter who called them,
whatever direction,

00:53:09.600 --> 00:53:13.150
and showing you what percentage of
your time is spent in those hotspots.

00:53:13.220 --> 00:53:16.290
And if you flip the graph back around,
you can start from the hotspots

00:53:16.290 --> 00:53:18.670
back down and you can see
who's calling each hotspot.

00:53:18.730 --> 00:53:20.750
And you can look and
see that two lowercase.

00:53:20.750 --> 00:53:22.070
Okay, who's calling that?

00:53:22.070 --> 00:53:23.760
And it really is only from one place.

00:53:23.760 --> 00:53:25.080
It's from my sorter.

00:53:25.080 --> 00:53:27.000
So that's kind of cool.

00:53:27.000 --> 00:53:29.660
And I found that pretty easily.

00:53:29.900 --> 00:53:32.280
here just on the main thread,
but you can also go into

00:53:32.280 --> 00:53:33.350
your individual threads.

00:53:33.360 --> 00:53:36.140
And I can say,
let's just look exactly at the sorter.

00:53:36.140 --> 00:53:40.560
I looked at the whole main thing and
included all my event loops and stuff.

00:53:40.560 --> 00:53:43.650
So if we look at my sorter,
my sorter's even worse.

00:53:43.720 --> 00:53:45.360
It's got 60% spent in there.

00:53:45.360 --> 00:53:48.320
So if I actually had a big list
I was trying to sort or something,

00:53:48.320 --> 00:53:50.060
I could do it a little better.

00:53:50.060 --> 00:53:54.130
So without, I'm going to reprofile again.

00:53:54.520 --> 00:53:55.300
Let me go back here.

00:53:55.300 --> 00:53:56.980
So I have this thing called Fixit.

00:53:56.980 --> 00:53:58.720
Now,
this really shouldn't be called Fixit.

00:53:58.720 --> 00:54:00.820
This should be like, don't be so stupid.

00:54:00.820 --> 00:54:05.040
Which is, what this is doing is it's
not doing a whole to lower.

00:54:05.040 --> 00:54:07.580
It's getting each character,
lower casing the characters,

00:54:07.590 --> 00:54:09.000
and testing against those.

00:54:09.000 --> 00:54:09.860
That's a little better.

00:54:09.870 --> 00:54:15.300
So I'll turn that on and I'll

00:54:15.420 --> 00:54:24.290
Reprofile again.

00:54:24.290 --> 00:54:24.290
Do a sort by first.

00:54:24.290 --> 00:54:24.290
Oh, I already did that.

00:54:24.290 --> 00:54:24.290
So I'll click these a
couple times back and forth.

00:54:29.620 --> 00:54:33.890
And we'll see that now we don't
even have anything about lowercase

00:54:33.980 --> 00:54:37.600
in our... Let me get up here,
actually.

00:54:37.600 --> 00:54:37.740
Sorry.

00:54:37.740 --> 00:54:41.300
We'll see some lowercase in here,
but it's not going to be as huge.

00:54:41.300 --> 00:54:43.950
I can't even find it right now.

00:54:43.980 --> 00:54:47.560
So that's pretty cool,
is that now our compare is no longer

00:54:47.560 --> 00:54:49.220
the huge portion of this whole thing.

00:54:49.220 --> 00:54:53.340
We see that there's something
about app context and graphics,

00:54:53.340 --> 00:54:55.580
which if you're using the
hardware accelerator would

00:54:55.670 --> 00:54:56.720
be a lot lower than this.

00:54:56.720 --> 00:54:59.770
So this is a really cool
way for you to find out what

00:54:59.770 --> 00:55:01.840
actually is your bottleneck.

00:55:02.380 --> 00:55:05.620
Obviously, you wouldn't be writing these
really bad sort routines,

00:55:05.620 --> 00:55:07.660
but who knows where things
might pop up like this.

00:55:07.660 --> 00:55:10.450
If you're using a library
from someone else,

00:55:10.470 --> 00:55:12.980
you'll actually see what
parts of their library,

00:55:12.980 --> 00:55:15.250
as long as it's not been
obfuscated or something like that,

00:55:15.310 --> 00:55:16.560
what parts of their library are slow.

00:55:16.560 --> 00:55:19.260
You can even see into our libraries
and see what's going on inside

00:55:19.270 --> 00:55:21.860
of graphics and things like that,
but you don't necessarily

00:55:21.860 --> 00:55:22.530
want to do that.

00:55:22.690 --> 00:55:24.660
But sometimes it's fun to do.

00:55:24.660 --> 00:55:26.640
And we've actually used this.

00:55:26.800 --> 00:55:29.740
A lot of our graphics
code is written in Java,

00:55:29.970 --> 00:55:31.790
so we use this all the time.

00:55:31.900 --> 00:55:34.500
I mean, I have engineers,
other coworkers of mine

00:55:34.500 --> 00:55:36.450
are coming in and saying,
you know,

00:55:36.450 --> 00:55:39.950
I made some changes over the past
week and everything just slowed down.

00:55:40.000 --> 00:55:41.630
Like JBuilder doesn't run very fast.

00:55:41.700 --> 00:55:42.270
What's going on?

00:55:42.300 --> 00:55:43.360
And we haven't changed anything.

00:55:43.360 --> 00:55:45.610
And we run it through
here and we find that,

00:55:45.620 --> 00:55:49.710
yeah, it was someone did a really bad
draw circle or something like that.

00:55:49.760 --> 00:55:52.850
And so we optimize that and we
get back our 10 times improvement.

00:55:52.920 --> 00:55:54.740
So it's pretty cool.

00:55:54.740 --> 00:55:56.670
Let me just show you another thing.

00:55:56.680 --> 00:55:57.620
Which is useful.

00:55:57.660 --> 00:56:00.810
This is the VM statistics.

00:56:00.820 --> 00:56:02.130
I've had this running the whole time.

00:56:02.140 --> 00:56:05.490
And it shows you things that
lead into what the other people

00:56:05.970 --> 00:56:08.900
before me all were talking about,
which is you don't want to load

00:56:08.900 --> 00:56:10.080
all your classes right away.

00:56:10.160 --> 00:56:13.150
So you can turn this on at startup
and you can see your classes

00:56:13.150 --> 00:56:15.940
being loaded and it'll show
you as you do different things.

00:56:15.940 --> 00:56:18.480
So if you actually have
dynamically loading classes,

00:56:18.480 --> 00:56:21.050
which is what you want,
you want to load them slowly as users

00:56:21.050 --> 00:56:23.700
get to different portions of your app,
you'll see your classes

00:56:23.700 --> 00:56:24.710
going up and up and up.

00:56:25.140 --> 00:56:26.600
Plus threads active.

00:56:26.600 --> 00:56:31.100
So if I were to go over here and
I actually say sort first name,

00:56:31.100 --> 00:56:33.180
oh wait.

00:56:33.200 --> 00:57:08.100
[Transcript missing]

00:57:08.340 --> 00:57:11.940
or something that's going to be really
slow here like resizing these things.

00:57:12.270 --> 00:57:14.560
And you can see what's
changed since you did that.

00:57:14.560 --> 00:57:17.720
And you can see that a whole bunch
of char arrays were allocated.

00:57:17.720 --> 00:57:20.860
Rectangles, obviously we use a lot of
rectangles in graphics.

00:57:20.860 --> 00:57:24.020
And they should mostly go away
when you run a garbage collector.

00:57:24.030 --> 00:57:26.830
So I hit the garbage collector
and we see that rectangle went

00:57:26.930 --> 00:57:28.490
down to none so we did a good job.

00:57:28.490 --> 00:57:31.900
There's something going on where
there's one string and one character

00:57:31.900 --> 00:57:34.800
and we might go hunt those down
for references or something.

00:57:34.960 --> 00:57:36.560
But that's basically what you have.

00:57:36.730 --> 00:57:42.380
There's a lot of different
things in this sampler.

00:57:42.380 --> 00:57:45.480
You actually can do, let's see, this is a

00:57:45.650 --> 00:57:46.600
There it is.

00:57:46.600 --> 00:57:48.540
So you have different types of profiling.

00:57:48.540 --> 00:57:52.900
I did all this profiling using sampling,
so every five milliseconds it tried

00:57:52.950 --> 00:57:54.640
to get what routine we were using.

00:57:54.640 --> 00:57:56.300
I could crank this down or up.

00:57:56.450 --> 00:57:59.100
I could also go to this
method called instrumentation,

00:57:59.100 --> 00:58:02.740
which is every single instruction,
every single call is being calculated

00:58:02.740 --> 00:58:05.850
so that you don't miss anything
because it only happened for half

00:58:05.850 --> 00:58:09.560
a millisecond and you happen to
always miss that half a millisecond.

00:58:09.560 --> 00:58:11.020
Sampling usually works pretty well.

00:58:11.020 --> 00:58:13.080
Instrumentation will slow
your app down even more.

00:58:14.580 --> 00:58:16.940
A couple things about running this.

00:58:16.960 --> 00:58:21.950
This requires the 131 interpreter
or hotspot version that's in DP1.

00:58:21.950 --> 00:58:26.170
I'm running this all on the
interpreter from a pre-DP1 release.

00:58:26.170 --> 00:58:29.530
That's why my app is even slower,
but it worked pretty

00:58:29.530 --> 00:58:31.420
well for this sort demo.

00:58:31.420 --> 00:58:35.440
The other thing is that I just
wanted to mention again,

00:58:35.440 --> 00:58:37.250
this UI was done in IFC.

00:58:37.250 --> 00:58:41.400
The guy who writes Optimize it wrote IFC,
so he loves it,

00:58:41.450 --> 00:58:44.410
but that's why it's not an Aqua look.

00:58:44.590 --> 00:58:45.080
I'm going to show you how
it looks like in a second.

00:58:45.080 --> 00:58:46.580
This is his own UI inside of there.

00:58:46.580 --> 00:58:52.400
If you're interested in finding out
when it's going to be available and how

00:58:52.400 --> 00:58:55.080
much it's going to cost and all that,
contact VMGear.

00:58:55.080 --> 00:58:57.080
It's VMGear.com.

00:58:57.080 --> 00:59:01.280
I'm sure they'd love to hear from all
you guys because they got this up and

00:59:01.280 --> 00:59:06.080
working and they're excited to have a
whole bunch of sales to Java programmers.

00:59:06.080 --> 00:59:08.000
That's about it.

00:59:12.650 --> 00:59:15.330
There's a little slide with a
roadmap of the relevant talks that

00:59:15.330 --> 00:59:17.740
are coming up following this one.

00:59:17.900 --> 00:59:23.500
There's a demonstration and talk about
JBuilder that you might want to go to.

00:59:23.690 --> 00:59:26.400
in the Civic Center just after this talk.

00:59:26.450 --> 00:59:30.360
And then some of the other ones,
quick time for Java.

00:59:30.360 --> 00:59:34.600
As I mentioned in my bit,
Apple Performance Tools,

00:59:34.670 --> 00:59:37.340
that'll give you more information
about the performance tools if you're

00:59:37.340 --> 00:59:38.920
specifically interested in that.

00:59:40.600 --> 00:59:44.040
So all we'll do now is we'll
have a quick Q&A session.

00:59:44.040 --> 00:59:46.180
I'll invite the rest of the...