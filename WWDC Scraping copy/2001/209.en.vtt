WEBVTT

00:00:00.720 --> 00:00:01.740
Thank you for coming.

00:00:01.780 --> 00:00:06.360
This is a session on Audio Processing
and Sequencing Services.

00:00:06.390 --> 00:00:10.040
And just before I bring Chris Rogers up,
who's going to be doing

00:00:10.040 --> 00:00:13.250
most of the conversation,
I just thought I'd welcome you.

00:00:13.460 --> 00:00:14.300
My name's Bill Stewart.

00:00:14.300 --> 00:00:17.680
I'm the Engineering Manager of
the Quario team.

00:00:17.710 --> 00:00:22.180
If you missed the talk yesterday,
we talked just very briefly

00:00:22.180 --> 00:00:28.040
about the audio hardware layer
that we have in Mac OS X.

00:00:28.040 --> 00:00:33.240
And all of this sort of fits in the
general frame of what we call Core Audio.

00:00:33.240 --> 00:00:39.220
And aside from the audio hardware stuff,
yesterday we talked about the

00:00:39.220 --> 00:00:45.880
default output unit as a way of very
easily having your application get

00:00:45.880 --> 00:00:52.300
sound out the current output devices
selected by the control panel.

00:00:52.300 --> 00:00:56.530
So what we're going to be doing
today is to talk about audio units,

00:00:56.530 --> 00:00:57.960
and Chris will go
through that in a moment.

00:00:57.960 --> 00:01:00.710
And the default output
unit is one of them.

00:01:00.710 --> 00:01:04.050
So if you were at the talk
yesterday and a little bit confused

00:01:04.050 --> 00:01:06.350
about where all that fits in,
you should be quite clear

00:01:06.350 --> 00:01:07.840
by the end of this talk,
we hope.

00:01:07.840 --> 00:01:10.220
So without any further ado,
I'll bring Chris Rogers up

00:01:10.220 --> 00:01:11.830
from the Quario team.

00:01:11.830 --> 00:01:12.790
Thank you.

00:01:17.490 --> 00:01:19.350
Good afternoon.

00:01:19.400 --> 00:01:22.160
I'm pleased to be here to speak
with you a little bit about some

00:01:22.160 --> 00:01:28.080
of the higher level services
available for audio on Mac OS X.

00:01:32.870 --> 00:01:36.120
Overview of the talk is,
first thing I'm going to talk

00:01:36.200 --> 00:01:39.340
about is Audio Unit Architecture.

00:01:39.340 --> 00:01:43.210
Audio Units are components for
processing audio or just dealing

00:01:43.210 --> 00:01:45.800
with audio in some general manner.

00:01:45.800 --> 00:01:49.070
Then I'm going to talk
about the Audio Toolbox,

00:01:49.070 --> 00:01:53.660
hooking Audio Units together
and processing graphs.

00:01:53.830 --> 00:01:57.190
Lastly,
I'll be speaking about some timing and

00:01:57.410 --> 00:02:03.800
scheduling services that are available
using the Music Sequencing Services.

00:02:03.800 --> 00:02:12.790
Let's take a look at where these higher
level services sit in our audio stack.

00:02:13.150 --> 00:02:20.390
Audio Units are one level above
the core Audio How APIs which

00:02:20.390 --> 00:02:24.490
Jeff spoke about yesterday.

00:02:25.320 --> 00:02:29.590
Then above that we have what are
known as the AU Graph Services,

00:02:29.620 --> 00:02:32.700
which are a set of APIs for
connecting these Audio Units together.

00:02:32.850 --> 00:02:37.630
And the Music Sequencing
Services we'll see can talk

00:02:37.640 --> 00:02:39.980
to AU Graphs and Audio Units.

00:02:40.880 --> 00:02:45.000
and Core MIDI is off to the side there.

00:02:45.000 --> 00:02:48.250
And later on,
Doug will be going over that in quite

00:02:48.340 --> 00:02:51.580
some detail and actually showing how
there are some relationships between

00:02:51.650 --> 00:02:58.070
the MIDI services and some of the
Audio Unit technology that we provide.

00:03:00.120 --> 00:03:04.630
So audio units are components,
components that are handled

00:03:04.830 --> 00:03:10.340
by the component manager that
QuickTime makes wide use of.

00:03:10.340 --> 00:03:13.760
Like all components,
individual components are

00:03:13.760 --> 00:03:18.940
identified by a four character type,
subtype, and identifier field,

00:03:18.940 --> 00:03:22.880
and the component manager can
query the list of available

00:03:22.880 --> 00:03:24.690
components of a given type.

00:03:24.710 --> 00:03:28.280
Audio units are the ones
we're interested in this case,

00:03:28.290 --> 00:03:33.820
so you can call find next component to go
through the list of available components.

00:03:33.820 --> 00:03:36.770
Then once you've gotten to the one
that you're interested in opening,

00:03:36.770 --> 00:03:39.710
you can call open a component,
and to close it,

00:03:39.710 --> 00:03:41.770
you call close component.

00:03:41.820 --> 00:03:44.430
The component manager has some
pretty good documentation,

00:03:44.450 --> 00:03:47.200
so you can refer to our
developer documentation about

00:03:47.320 --> 00:03:49.460
the component manager for that.

00:03:50.420 --> 00:03:54.910
The Audio Unit component, in general,
it generates, receives, processes,

00:03:54.910 --> 00:03:58.960
translates, or otherwise manipulates
streams of audio,

00:03:58.960 --> 00:04:00.530
as I have in my slide.

00:04:00.690 --> 00:04:03.040
And they're good
high-level building blocks.

00:04:03.070 --> 00:04:07.900
They can be connected together to
create networks of audio processing.

00:04:07.900 --> 00:04:11.290
And you can also use
them singly and just,

00:04:11.290 --> 00:04:15.040
for instance,
instantiate a software synth

00:04:15.120 --> 00:04:18.800
and wire it up so this output
goes directly to the hardware.

00:04:18.800 --> 00:04:24.190
We provide a set of Audio Units
that ships with OS X and we'll

00:04:24.190 --> 00:04:27.900
be adding more in the future,
in future releases.

00:04:27.900 --> 00:04:33.730
And developers are free to
create their own Audio Units.

00:04:33.730 --> 00:04:38.700
And we'll be providing an
SDK to make that a lot easier

00:04:38.700 --> 00:04:40.920
than it would be otherwise.

00:04:41.510 --> 00:04:44.400
So what kinds of Audio Units are there?

00:04:44.470 --> 00:04:47.360
There are Audio Units that
just provide audio data.

00:04:47.360 --> 00:04:48.390
So they're sources.

00:04:48.390 --> 00:04:49.800
They have no inputs to them.

00:04:49.800 --> 00:04:52.410
An example of that type of
Audio Unit would be what's

00:04:52.420 --> 00:04:54.310
known as a Music Device.

00:04:54.310 --> 00:04:56.740
It's a software synthesizer.

00:04:56.810 --> 00:05:07.990
And Apple ships a DLS2 sound font
compliant software synth in OS X.

00:05:08.010 --> 00:05:10.850
Also there are Audio Units which
interface with the hardware.

00:05:10.910 --> 00:05:15.310
They provide a higher level
abstraction of hardware I/O devices.

00:05:15.310 --> 00:05:17.980
So they talk to the audio HAL.

00:05:17.980 --> 00:05:21.170
And one of the most common
types of Audio Units are in

00:05:21.250 --> 00:05:23.990
the category of DSP processors.

00:05:23.990 --> 00:05:27.640
Reverbs, filters, mixers,
this type of thing.

00:05:27.660 --> 00:05:34.130
So these type of Audio Units typically,
they take input and process it

00:05:34.150 --> 00:05:36.840
somehow and output the audio stream.

00:05:36.840 --> 00:05:37.840
Another kind of Audio Units
is the Audio Processing Unit.

00:05:37.840 --> 00:05:37.840
This is a kind of Audio Processing
Unit that's used to process audio.

00:05:37.840 --> 00:05:37.840
It's a kind of audio processing
unit that's used to process audio.

00:05:37.840 --> 00:05:37.840
It's a kind of audio processing
unit that's used to process audio.

00:05:37.840 --> 00:05:37.850
It's a kind of audio processing
unit that's used to process audio.

00:05:37.850 --> 00:05:38.030
It's a kind of audio processing
unit that's used to process audio.

00:05:38.030 --> 00:05:38.690
It's a kind of audio processing
unit that's used to process audio.

00:05:38.730 --> 00:05:41.830
It's a kind of audio processing
unit that's used to process audio.

00:05:41.830 --> 00:05:45.090
And one of the main types of
Audio Unit is Format Converter.

00:05:45.090 --> 00:05:47.870
Converting between 16 bits,
floating point,

00:05:47.870 --> 00:05:51.040
interleaving stereo streams
into discrete mono streams,

00:05:51.040 --> 00:05:53.490
sample rate converters,
and there are quite a

00:05:53.590 --> 00:05:54.890
few other types as well.

00:05:54.890 --> 00:05:58.590
And then another class is what I call
an adapter class of Audio Unit.

00:05:58.590 --> 00:06:02.080
And this is a little bit strange
because it doesn't actually process

00:06:02.260 --> 00:06:03.900
the audio in any way using DSP.

00:06:03.980 --> 00:06:06.920
It also doesn't change the
format of the audio stream.

00:06:06.920 --> 00:06:11.180
But But instead,
it somehow does some other

00:06:11.180 --> 00:06:13.620
kind of impedance matching
between the input and output.

00:06:13.620 --> 00:06:16.010
For instance,
it may do some kind of internal buffering

00:06:16.010 --> 00:06:20.620
so that the input buffer size is
different from the output buffer size.

00:06:20.920 --> 00:06:28.120
Or it may lay on top of a thread
scheduling scheme so that everything

00:06:28.120 --> 00:06:32.640
before that Audio Unit in the chain
would get deferred to another thread.

00:06:32.670 --> 00:06:37.620
And this way you could potentially
take advantage of multiprocessing.

00:06:37.620 --> 00:06:39.350
on multiprocessor machines.

00:06:39.350 --> 00:06:44.020
It's a very interesting application.

00:06:44.400 --> 00:06:46.990
I guess we're going to try
running a little demo here.

00:06:46.990 --> 00:06:49.400
I should probably explain a
little bit what I've done.

00:06:49.400 --> 00:06:51.400
Oh, this is your demo.

00:06:51.400 --> 00:06:51.400
Okay.

00:06:51.400 --> 00:06:55.400
So if we can have the machine up.

00:06:55.510 --> 00:07:05.350
I can't see anything.

00:07:05.350 --> 00:07:05.350
Oh, neither can you.

00:07:05.350 --> 00:07:05.350
I can see this.

00:07:05.350 --> 00:07:05.350
Okay, let me just get this to run.

00:07:05.350 --> 00:07:05.350
So,

00:07:20.500 --> 00:07:25.640
So what I'm actually running
here is a Java program.

00:07:25.640 --> 00:07:30.570
The Java program is quite simple.

00:07:31.000 --> 00:07:35.560
This is very interesting,
trying to do it this way.

00:07:35.560 --> 00:07:40.200
This is just demonstrating how to
use the component description and

00:07:40.200 --> 00:07:43.020
the component services from Java.

00:07:43.100 --> 00:07:44.980
It's the same as you would do in C.

00:07:45.080 --> 00:07:47.480
As we've been talking
about in the sessions,

00:07:47.480 --> 00:07:49.860
you have both a Java and a C API to this.

00:07:49.860 --> 00:07:53.580
So I thought it might be interesting
just to show you some simple

00:07:53.580 --> 00:07:58.120
Java code that is quite clear about
the concept of what we're doing.

00:07:58.120 --> 00:07:59.680
We create a component description.

00:07:59.680 --> 00:08:04.100
This describes to the component manager
the type of components we're looking for.

00:08:04.100 --> 00:08:07.680
The type is a K audio unit type.

00:08:07.680 --> 00:08:11.210
And then what I'm going to do here
is I've just told it to look for

00:08:11.210 --> 00:08:14.360
the first component of this type.

00:08:14.360 --> 00:08:17.880
And I'll use the AU component,
which is actually in

00:08:17.880 --> 00:08:19.620
this package in Java.

00:08:19.620 --> 00:08:21.620
And this will find the first one.

00:08:21.620 --> 00:08:23.800
Then I'm going to print out
some information about it.

00:08:23.800 --> 00:08:26.670
And then I'm going to keep
finding the next component from

00:08:26.670 --> 00:08:28.120
the one that I've already found.

00:08:28.120 --> 00:08:32.940
And until I don't find any more,
and then my program finishes.

00:08:32.940 --> 00:08:36.650
So this will give you an idea of
the audio units that we ship with

00:08:36.750 --> 00:08:40.820
the current version of this system.

00:08:40.820 --> 00:08:48.740
So there's delay, some filters,
there's a matrix reverb, interleavers,

00:08:48.740 --> 00:08:51.940
deinterleavers, a mixer unit.

00:08:51.940 --> 00:08:53.780
There's the DLS music device.

00:08:53.780 --> 00:08:58.000
You can see it's type, subtype,
manufacturer.

00:08:58.000 --> 00:09:01.100
The audio device output,
the default output unit,

00:09:01.100 --> 00:09:05.940
which is actually an extension of the
basic unit that will talk to any device.

00:09:05.940 --> 00:09:11.520
The default output device will track a
default output that the user might set.

00:09:11.520 --> 00:09:12.510
And various others.

00:09:12.530 --> 00:09:13.520
Thanks.

00:09:13.520 --> 00:09:15.510
I'll go back to the slides.

00:09:17.580 --> 00:09:23.510
And of course I can't see my screen.

00:09:24.040 --> 00:09:27.540
Okay, just before the demo,
I went over some of the

00:09:27.540 --> 00:09:29.680
categories of Audio Units.

00:09:29.680 --> 00:09:33.890
This is just a little simple diagram
showing that an Audio Unit is

00:09:34.010 --> 00:09:38.340
basically a box that can have
an arbitrary number of inputs,

00:09:38.410 --> 00:09:41.990
arbitrary number of outputs.

00:09:42.000 --> 00:09:45.980
I think this diagram is showing that
DSPFX would have both inputs and outputs

00:09:45.980 --> 00:09:49.910
with some kind of processing going on.

00:09:50.560 --> 00:09:54.820
Software synth music device may
only have outputs because it's

00:09:54.820 --> 00:09:57.260
actually synthesizing sound.

00:09:57.260 --> 00:10:00.630
It's creating it from scratch.

00:10:00.850 --> 00:10:05.240
A default output unit,
which Doug talked about yesterday,

00:10:05.240 --> 00:10:11.290
it would just take inputs and it would
send that audio to a hardware device.

00:10:12.460 --> 00:10:15.220
Um,
we'll talk about audio unit properties.

00:10:15.220 --> 00:10:17.940
Properties are very,
very fundamental to the

00:10:17.940 --> 00:10:22.790
way that audio units work,
and a very general and extensible way to

00:10:22.790 --> 00:10:25.900
pass information to and from audio units.

00:10:25.920 --> 00:10:30.220
Audio units themselves,
they implement a very small set of, um,

00:10:30.460 --> 00:10:33.290
functions, which you,
which can be called to deal

00:10:33.320 --> 00:10:37.290
with getting audio in and out,
and setting parameters, and so on.

00:10:37.290 --> 00:10:41.940
But we created this property mechanism
to allow audio units to be extensible,

00:10:41.940 --> 00:10:46.210
so when we come up with some new feature
that audio units need to implement,

00:10:46.210 --> 00:10:50.110
we can always, um, use the, uh,
the property APIs and pass in

00:10:50.180 --> 00:10:52.750
data via void star pointer,
and, uh,

00:10:52.780 --> 00:10:56.900
the length of the data is indicated by a,
a byte size parameter.

00:10:56.970 --> 00:11:01.400
The, we define several
different properties today,

00:11:01.400 --> 00:11:05.180
and the type of property is identified
by an audio unit property ID.

00:11:05.230 --> 00:11:07.930
It's just a 32-bit integer.

00:11:08.030 --> 00:11:11.880
Uh, both the MIDI APIs and the
AudioHAL APIs also support

00:11:11.880 --> 00:11:13.540
this concept of properties.

00:11:13.540 --> 00:11:19.980
It's almost identical type of
mechanism in the API between the three.

00:11:22.100 --> 00:11:27.580
Secondary to the property identifier,
identifying which property,

00:11:27.580 --> 00:11:33.730
we have addressing within the
Audio Unit to specify which part of the

00:11:33.740 --> 00:11:36.990
Audio Unit is to deal with the property.

00:11:37.000 --> 00:11:41.000
So there's a scope, which can be global,
input, output, or group.

00:11:41.270 --> 00:11:44.990
Global means that the property
applies to the Audio Unit as a whole.

00:11:45.110 --> 00:11:49.000
Input scope means the property
applies to a particular input.

00:11:49.140 --> 00:11:52.000
Output is pretty clear.

00:11:52.000 --> 00:11:55.700
And group is a particular
type of scope that is relevant

00:11:55.700 --> 00:11:59.000
for software synthesizers,
music devices.

00:11:59.000 --> 00:12:03.910
The Audio Unit element is an index
specifying which particular input,

00:12:04.060 --> 00:12:06.990
output, or group we're identifying.

00:12:07.000 --> 00:12:10.350
So if we want to talk
to input number two,

00:12:10.640 --> 00:12:13.000
then this argument would be two.

00:12:13.060 --> 00:12:14.990
Or actually, it would probably be two.

00:12:15.140 --> 00:12:21.190
Depending on how we're numbering,
starting at one or zero.

00:12:23.300 --> 00:12:31.060
The IDs are defined in
AudioUnitProperties.h.

00:12:31.060 --> 00:12:33.800
It's important to understand what the
data formats of the properties are.

00:12:33.830 --> 00:12:38.300
We're just passing information as a void
star pointer and a length parameter.

00:12:38.300 --> 00:12:41.800
You have to understand
the structure of the data.

00:12:41.800 --> 00:12:46.300
Those data formats are defined
also in the header file.

00:12:46.300 --> 00:12:49.640
We have a couple properties
defined right now.

00:12:49.640 --> 00:12:52.640
Additional properties
will be added as needed.

00:12:52.640 --> 00:12:55.630
Developers are free to implement
their own private properties

00:12:55.630 --> 00:12:56.640
for their own AudioUnits.

00:12:56.640 --> 00:13:00.640
There's a range of reserved
property IDs for that.

00:13:00.940 --> 00:13:06.240
All AudioUnits should support
the general system properties.

00:13:06.940 --> 00:13:12.600
So yesterday at Jeff's talk,
somebody was interested in

00:13:12.640 --> 00:13:15.360
knowing about the performance
capabilities of our system.

00:13:15.360 --> 00:13:17.750
And somebody asked about, well,
how many audio units can

00:13:17.780 --> 00:13:18.790
you connect together?

00:13:18.790 --> 00:13:23.420
And is there going to be a problem?

00:13:23.820 --> 00:13:30.010
The KAudioUnitPropertyFastDispatch
property is a way of getting

00:13:30.010 --> 00:13:34.000
access to function pointers which
call directly into the Audio Unit.

00:13:34.000 --> 00:13:37.840
This bypasses the
ComponentManagerDispatchOverhead

00:13:37.860 --> 00:13:39.800
that is typically incurred.

00:13:39.800 --> 00:13:42.440
Basically, you're just dealing with
function call overhead when

00:13:42.440 --> 00:13:43.760
you're doing your processing.

00:13:43.760 --> 00:13:48.800
It's no different from any other kind of
DSP you might be doing in your own code.

00:13:48.800 --> 00:13:53.170
So really, the time,
the bandwidth limiting factor is

00:13:53.170 --> 00:13:55.800
the actual DSP that is occurring.

00:13:55.800 --> 00:14:00.210
So the answer to the question of how many
units can you string together is really

00:14:00.210 --> 00:14:02.780
a function of how much processing are
you doing among all these Audio Units.

00:14:02.800 --> 00:14:05.800
It's not a limitation of
the Audio Units themselves.

00:14:06.160 --> 00:14:11.250
This particular property can
be used as an optimization,

00:14:11.320 --> 00:14:14.800
but you're not required to use it.

00:14:16.110 --> 00:14:22.200
Here's the second example of a property.

00:14:22.200 --> 00:14:22.200
And that's the

00:14:22.830 --> 00:14:26.420
The K Music Device Property Sound Bank,
FSSpec.

00:14:26.420 --> 00:14:29.900
This is a way of telling the
software synthesizer which

00:14:29.990 --> 00:14:31.670
sample bank it should use.

00:14:31.720 --> 00:14:38.610
So you identify the sample bank
with an FSSpec pointer and you

00:14:38.670 --> 00:14:42.700
pass that information in by
calling AudioUnitSetProperty.

00:14:42.770 --> 00:14:45.690
The first argument is the
actual component instance.

00:14:45.750 --> 00:14:52.700
The second argument is the property ID,
the Sound Bank FSSpec ID.

00:14:52.700 --> 00:14:56.640
Then it's global scope because it
applies to the synth as a whole.

00:14:56.640 --> 00:14:59.700
The element doesn't really matter
in the case of global scope.

00:14:59.700 --> 00:15:04.990
Second to last argument is pointer
to the file spec and then the size of

00:15:05.000 --> 00:15:07.690
the data is the size of the FSSpec.

00:15:07.700 --> 00:15:12.540
So that shows you how you can
communicate property information to an

00:15:12.540 --> 00:15:15.700
AudioUnit with AudioUnitSetProperty.

00:15:15.700 --> 00:15:19.700
And all properties are
dealt with in a similar way.

00:15:19.700 --> 00:15:22.910
Of course the specifics of
what the property information

00:15:22.910 --> 00:15:24.700
is and what it does will vary.

00:15:26.750 --> 00:15:33.030
Parameters are less for configuring
attributes of Audio Units and more

00:15:33.030 --> 00:15:35.700
for changing real-time control points.

00:15:35.700 --> 00:15:40.320
And we have the Audio Unit Get parameter
and Audio Unit Set parameter

00:15:40.490 --> 00:15:41.700
functions for this.

00:15:41.710 --> 00:15:45.900
It allows real-time control
of processing algorithms.

00:15:45.900 --> 00:15:50.700
The parameters are identified with
Audio Unit Parameter ID and the value

00:15:50.700 --> 00:15:53.840
is a 32-bit floating point number.

00:15:53.940 --> 00:15:57.700
And these parameters are very similar
to MIDI continuous controllers,

00:15:57.700 --> 00:16:00.670
but they're much more resolute.

00:16:00.670 --> 00:16:05.790
They're 32-bit floating point numbers
as I pointed out and it's much,

00:16:05.790 --> 00:16:11.110
much higher resolution
than a 7-bit MIDI value.

00:16:11.300 --> 00:16:16.900
The Set Parameter function includes a
timestamp allowing the Audio Unit to

00:16:16.900 --> 00:16:21.200
schedule the parameter change for a very
specific time within a buffer of audio.

00:16:21.200 --> 00:16:24.950
So the timestamp is in units
of sample frames on the next

00:16:24.950 --> 00:16:27.200
buffer of audio to be processed.

00:16:27.200 --> 00:16:33.730
An Audio Unit supplies its list
of supported parameters with the

00:16:33.730 --> 00:16:36.200
kAudioUnitPropertyParameterList.

00:16:36.620 --> 00:16:43.200
Individual parameters can be queried
with the ParameterInfo property.

00:16:43.200 --> 00:16:46.090
Information such as the
name of the parameter,

00:16:46.450 --> 00:16:51.500
what units the parameter uses,
whether that be Hertz, Cents, Decibels,

00:16:51.500 --> 00:16:52.200
whatever.

00:16:52.220 --> 00:16:57.130
Min, Max,
and Default value for that parameter.

00:16:57.350 --> 00:16:59.200
And some flags as well.

00:16:59.200 --> 00:17:03.200
The information provided by this
property could be used by a client

00:17:03.200 --> 00:17:08.200
to put up some kind of user interface
representing this particular parameter.

00:17:08.250 --> 00:17:12.200
So if the parameter represents
volume in decibels or

00:17:12.200 --> 00:17:17.370
something like that from maybe,
I don't know, minus 150 decibels up

00:17:17.370 --> 00:17:23.200
to 0 dB or who knows,
the control is actually able to

00:17:23.200 --> 00:17:28.200
display the units and the minimum
and maximum value appropriately.

00:17:28.200 --> 00:17:36.110
Audio Units are in one of several
states and it's important to

00:17:36.110 --> 00:17:41.200
understand the state transitions
that Audio Units go through.

00:17:41.240 --> 00:17:45.360
When an Audio Unit is first
opened with Open a Component,

00:17:45.360 --> 00:17:48.200
it's considered to be in the open state.

00:17:48.200 --> 00:17:52.200
Once the component is closed,
then it doesn't even exist anymore.

00:17:52.330 --> 00:17:56.290
So that's kind of like the base
level constructor/destructor,

00:17:56.290 --> 00:17:58.200
if you're thinking in
object-oriented terms.

00:17:58.200 --> 00:18:00.100
But there's an additional
level of construction and

00:18:00.100 --> 00:18:01.200
destruction in Audio Units.

00:18:01.200 --> 00:18:03.200
That's the initialized
and uninitialized state.

00:18:03.200 --> 00:18:10.200
And we have the Audio Unit initialized
and Audio Unit uninitialized

00:18:10.210 --> 00:18:13.200
functions for that.

00:18:13.200 --> 00:18:17.200
Once the Audio Unit is initialized,
you can actually start it

00:18:17.200 --> 00:18:18.200
running with Audio Unit start.

00:18:18.450 --> 00:18:22.390
And that applies specifically to

00:18:22.880 --> 00:18:31.690
I/O Audio Units such as the Default
Output Unit to actually start the

00:18:31.690 --> 00:18:31.690
hardware through the Audio HAL.

00:18:33.470 --> 00:18:37.340
So, let's go into a little more detail
about what's the difference between

00:18:37.340 --> 00:18:39.600
just opening the component in the
first place and initializing it.

00:18:39.600 --> 00:18:41.240
It seems like it's the same thing.

00:18:41.380 --> 00:18:44.640
Well, when you open the component,
you have a component instance and

00:18:44.640 --> 00:18:47.820
you can make calls to the component,
but at this time,

00:18:47.850 --> 00:18:53.350
the component is not expected to allocate
any significant amount of resources,

00:18:53.360 --> 00:18:56.310
such as memory.

00:18:56.660 --> 00:18:59.990
But you are able to make property
calls to the Audio Unit to

00:18:59.990 --> 00:19:02.140
configure it in some particular way.

00:19:02.410 --> 00:19:03.480
For example,
you might want to configure its

00:19:03.530 --> 00:19:10.600
sample rate or bit depth or maybe
you're interested in setting up some

00:19:10.600 --> 00:19:14.780
other aspect of the Audio Unit before
you actually initialize it.

00:19:14.780 --> 00:19:18.970
Then once you call Audio Unit Initialize
and you've made your property calls

00:19:19.070 --> 00:19:24.430
to configure it just like you want,
then the appropriate allocations

00:19:24.430 --> 00:19:28.630
can occur optimizing for the
particular configuration.

00:19:28.810 --> 00:19:32.430
So if you're configuring a
reverb for a certain number of

00:19:32.430 --> 00:19:35.740
delays or something like that,
it's able to allocate exactly

00:19:35.750 --> 00:19:37.550
the amount of memory it needs.

00:19:40.300 --> 00:19:45.800
[Transcript missing]

00:19:46.060 --> 00:19:48.350
This is when audio is
actually being processed.

00:19:48.390 --> 00:19:51.510
It's actually going through
from input to output.

00:19:51.520 --> 00:19:56.900
Audio Unit or the hardware is running,
whatever the case may be.

00:19:57.970 --> 00:20:02.910
The Audio Output Unit implements
the Audio Unit Start and

00:20:02.910 --> 00:20:08.200
Audio Unit Stop functions.

00:20:08.380 --> 00:20:10.920
If you have a graph of
Audio Units connected together,

00:20:11.010 --> 00:20:13.610
this is where

00:20:13.850 --> 00:20:18.150
The Audio Actually Starts
Being Processed or Stops Being Processed.

00:20:18.150 --> 00:20:22.350
The Audio Output Unit generally
represents the terminal

00:20:22.350 --> 00:20:23.910
node in a connection.

00:20:24.420 --> 00:20:29.100
And as Doug talked about a bit yesterday,
the default Audio Unit,

00:20:29.180 --> 00:20:35.270
it starts the hardware device that
the Audio HAL is connected to.

00:20:36.330 --> 00:20:41.700
Here's another API to Audio Unit Reset.

00:20:42.280 --> 00:20:47.200
It basically is used when
an Audio Unit is initialized

00:20:47.300 --> 00:20:50.520
and it may even be running.

00:20:50.520 --> 00:20:54.830
But if you want to clear
the state of the Audio Unit,

00:20:54.940 --> 00:20:59.150
it's typically used with DSP Audio Units.

00:20:59.210 --> 00:21:02.120
So for delay effects,
you would clear the delay line.

00:21:02.210 --> 00:21:05.200
For reverbs,
you would clear out the reverb memory.

00:21:05.200 --> 00:21:07.080
For filters, you clear the filter state.

00:21:07.260 --> 00:21:09.160
And this is important.

00:21:09.160 --> 00:21:13.180
If the graph is actively
processing audio,

00:21:13.180 --> 00:21:17.720
but you perhaps reposition the playhead
to a different part in the audio stream,

00:21:17.720 --> 00:21:21.810
and you don't want reverb tail from

00:21:22.500 --> 00:21:25.930
is the Director of the
Java Processing Unit.

00:21:25.930 --> 00:21:29.550
He will be speaking at
the end of the session.

00:21:33.370 --> 00:21:41.180
Audio Unit I/O Management Audio Units
process audio or deal with

00:21:41.240 --> 00:21:44.800
audio using a pull I/O model.

00:21:44.800 --> 00:21:47.540
And what I mean by that is
that there's a function,

00:21:47.540 --> 00:21:51.460
a particular function you
call to read the audio output

00:21:51.460 --> 00:21:54.130
stream from the Audio Unit.

00:21:54.140 --> 00:21:57.040
And when you do that,
the Audio Unit has to go

00:21:57.040 --> 00:21:59.540
and fetch its audio input.

00:21:59.540 --> 00:22:03.270
So it typically does that by
calling the same function on the

00:22:03.270 --> 00:22:05.250
output of another Audio Unit.

00:22:05.260 --> 00:22:07.900
And then that Audio Unit has to
get its audio input from somewhere.

00:22:07.900 --> 00:22:11.400
So if you imagine a chain of
these Audio Units connected

00:22:11.400 --> 00:22:14.880
together one after the other,
you would pull on the very end one and

00:22:14.980 --> 00:22:17.010
in turn it would go down the chain.

00:22:17.010 --> 00:22:20.490
Each one would fetch its input
from the one before and you pull

00:22:20.490 --> 00:22:22.110
the audio through the chain.

00:22:22.170 --> 00:22:25.390
It's possible to connect
these Audio Units up in more

00:22:25.390 --> 00:22:29.430
complicated configurations
than just simple linear chains,

00:22:29.430 --> 00:22:31.050
but the idea is still the same.

00:22:33.120 --> 00:22:37.080
Audio Unit I/O Management Audio Units
process audio or deal with audio input.

00:22:37.600 --> 00:22:39.020
Audio Unit I/O Management Audio Units
process audio or deal with audio input

00:22:39.660 --> 00:22:45.340
Audio Units specify their number of
inputs and outputs through properties.

00:22:45.340 --> 00:22:52.360
The format of the audio data is
canonically n-channel interleaved audio.

00:22:52.360 --> 00:22:59.280
Or it could be sideband data
such as spectral analysis data.

00:22:59.280 --> 00:23:03.720
This is linear PCM 32-bit floating point.

00:23:08.340 --> 00:23:12.090
Okay, I've been talking about connecting
these Audio Units together and this

00:23:12.090 --> 00:23:13.540
is just a picture showing that.

00:23:15.820 --> 00:23:21.730
I'm going to talk about how to
get audio into an Audio Unit and

00:23:21.730 --> 00:23:24.700
then I'm going to talk about how
to get audio out of an Audio Unit.

00:23:24.700 --> 00:23:29.700
There are two ways to provide an
Audio Unit with its audio data.

00:23:29.700 --> 00:23:38.800
The first way is using a property
called kAudioUnitPropertyMakeConnection.

00:23:39.000 --> 00:23:47.140
With this property, you specify a source
Audio Unit and its output number.

00:23:47.210 --> 00:23:51.770
And then that connection is made and
the Audio Unit knows that it's to read

00:23:51.830 --> 00:23:57.580
its audio data for a particular input
from another Audio Unit's output.

00:23:58.450 --> 00:24:01.200
A second way of providing audio
data to an Audio Unit is by

00:24:01.200 --> 00:24:03.300
registering a Client Callback.

00:24:03.300 --> 00:24:08.360
This is the standard sort of callback
that we get in the Audio Howler,

00:24:08.360 --> 00:24:13.620
in the Sound Manager,
or in many other audio systems,

00:24:13.660 --> 00:24:15.600
such as ASIO.

00:24:15.600 --> 00:24:18.110
Using this method,
the Client just specifies

00:24:18.150 --> 00:24:21.710
a callback function,
and then every buffer slice cycle,

00:24:21.750 --> 00:24:25.880
this callback function is called,
and the Client is able to

00:24:25.880 --> 00:24:29.750
provide the audio or a particular
input of an audio unit.

00:24:31.050 --> 00:24:34.560
In the case where Audio Units
have multiple inputs,

00:24:34.840 --> 00:24:39.840
each input may be configured separately
using either of these two methods.

00:24:39.860 --> 00:24:44.000
So if an Audio Unit has two inputs,
the first may be connected to the

00:24:44.070 --> 00:24:50.260
output of another Audio Unit and
the second input may be

00:24:50.600 --> 00:25:06.700
[Transcript missing]

00:25:07.780 --> 00:25:12.120
So, I've just gone over two ways
that you provide audio input

00:25:12.290 --> 00:25:13.820
data to the Audio Unit.

00:25:13.820 --> 00:25:17.170
Now, it's important to understand
how do you actually get the

00:25:17.210 --> 00:25:19.440
rendered audio out of the output.

00:25:19.440 --> 00:25:23.730
And you have to use the
Audio Unit Render/Slice

00:25:23.740 --> 00:25:24.590
function call for this.

00:25:24.660 --> 00:25:28.450
It pulls the audio or reads audio
from a particular Audio Unit output.

00:25:28.560 --> 00:25:31.410
And it must be called once per output.

00:25:31.410 --> 00:25:35.800
So if there are three outputs,
then it would be called three times.

00:25:35.850 --> 00:25:39.850
Each audio output can be an n-channel
interleaved stream like I said before.

00:25:39.900 --> 00:25:45.530
So it's possible that an Audio Unit might
just have one output and provide

00:25:45.590 --> 00:25:51.020
actually eight discrete channels
internally in that one output.

00:25:54.200 --> 00:25:56.450
I'm going to go over some of
the important arguments to the

00:25:56.450 --> 00:25:59.000
Audio Unit Render Slice function.

00:25:59.060 --> 00:26:06.290
It's actually very similar to
the Audio HAL callback arguments.

00:26:08.630 --> 00:26:14.570
The Audio Timestamp is exactly the
same Audio Timestamp structure as in

00:26:14.570 --> 00:26:20.500
Audio HAL and it specifies the start time
of the buffer that is to be rendered.

00:26:20.500 --> 00:26:25.560
It provides information about the
current host time of the machine or

00:26:25.670 --> 00:26:28.500
the host time that corresponds to
the start of the buffer along with

00:26:28.500 --> 00:26:31.500
the sample time that corresponds to.

00:26:31.500 --> 00:26:35.340
It also provides information about the
current host time of the system and

00:26:35.340 --> 00:26:38.950
how to synchronize other system events
such as MIDI events which use this

00:26:38.960 --> 00:26:45.420
host time to the audio stream which
are synchronizing based on sample time.

00:26:45.630 --> 00:26:50.000
The Audio Buffer argument is where
you can pass a buffer that you

00:26:50.040 --> 00:26:52.170
expect the Audio Unit to render into.

00:26:52.370 --> 00:26:57.670
Or, optionally, you can pass a null for
this and it will pass back a

00:26:57.670 --> 00:27:02.010
returned buffer that it caches.

00:27:04.430 --> 00:27:12.400
The Audio Buffer is also a
structure used in the Audio HAL.

00:27:12.400 --> 00:27:16.980
We share the same data structures
between the high level and the low

00:27:16.980 --> 00:27:19.690
level parts of the audio stack.

00:27:22.490 --> 00:27:29.420
So with Audio Unit Render Slice,
it's optionally possible to schedule

00:27:29.480 --> 00:27:32.100
events before a given I/O cycle.

00:27:32.100 --> 00:27:36.160
So before Audio Unit Render
Slice is called,

00:27:36.160 --> 00:27:37.600
you may want to do some scheduling.

00:27:37.620 --> 00:27:40.240
Parameter changes with
Audio Unit Set Parameter,

00:27:40.240 --> 00:27:44.130
or in the case of Software Synthesis,
you might want to schedule some

00:27:44.240 --> 00:27:48.900
notes to start or stop playing
during that slice of audio.

00:27:48.950 --> 00:27:51.770
So there's this two-phase
schedule render model.

00:27:51.800 --> 00:27:56.100
In step one, you would schedule events,
and in step two,

00:27:56.100 --> 00:27:59.180
you call Audio Unit Render Slice.

00:28:03.200 --> 00:28:14.300
[Transcript missing]

00:28:14.320 --> 00:28:17.350
It's an optional callback
that a client can install on

00:28:17.350 --> 00:28:21.250
an output of an Audio Unit,
which gets called before rendering

00:28:21.250 --> 00:28:23.300
occurs and after rendering occurs.

00:28:23.450 --> 00:28:28.180
So every time that
Audio Unit render slice is called,

00:28:28.720 --> 00:28:32.720
The Audio Unit will call
this notification callback,

00:28:33.080 --> 00:28:37.200
perform its rendering,
and then call the callback again

00:28:37.200 --> 00:28:38.600
to say that it's done rendering.

00:28:38.690 --> 00:28:45.100
This allows the client who installs this
notification to perform arbitrary tasks.

00:28:45.100 --> 00:28:50.820
Some examples would be scheduling,
which you might do in

00:28:50.820 --> 00:28:54.390
exactly the same way as...

00:28:54.650 --> 00:29:00.280
as I described here
before every render slice.

00:29:01.430 --> 00:29:06.700
Another use that you might make of this
is to perform CPU load measurement.

00:29:06.700 --> 00:29:12.300
You can actually measure the
time that the Audio Unit takes

00:29:12.300 --> 00:29:12.300
to perform its rendering.

00:29:13.750 --> 00:29:15.430
How do you write one?

00:29:15.570 --> 00:29:19.800
Audio Units are components,
the same kind of components

00:29:19.840 --> 00:29:22.840
that have been used in
QuickTime for a very long time.

00:29:22.840 --> 00:29:26.130
Anybody who is familiar with writing
components in QuickTime should

00:29:26.140 --> 00:29:28.440
be able to get up and running
pretty quickly with Audio Units.

00:29:28.440 --> 00:29:32.050
For people who haven't
written components before,

00:29:32.200 --> 00:29:37.510
it can be a little bit difficult
unless you have some help.

00:29:37.600 --> 00:29:40.030
We have an SDK that
makes this very simple.

00:29:40.140 --> 00:29:43.560
All you really have to do is
override one or two basic functions.

00:29:43.560 --> 00:29:45.340
Um...

00:29:45.600 --> 00:29:49.230
and in fact, at the base level,
you really only have to override one,

00:29:49.280 --> 00:29:54.970
and that's the function which
does your actual DSP processing.

00:29:55.100 --> 00:30:01.400
[Transcript missing]

00:30:01.730 --> 00:30:05.790
This SDK will be placed
on our website soon,

00:30:05.850 --> 00:30:08.120
in the next few days we hope.

00:30:09.610 --> 00:30:12.630
So, I'm finished talking about
Audio Units at the lower level.

00:30:12.810 --> 00:30:14.400
I'm going to talk about AUGraph.

00:30:14.400 --> 00:30:16.310
It stands for Audio Unit Graph.

00:30:16.450 --> 00:30:22.600
And this is a set of APIs for
connecting these components together.

00:30:22.610 --> 00:30:26.510
represents a high level representation
of a set of audio units and

00:30:26.510 --> 00:30:28.200
the connections between them.

00:30:28.340 --> 00:30:31.840
It allows you to construct
arbitrary signal paths,

00:30:32.390 --> 00:30:36.590
fairly arbitrary anyway,
and represents a modular routing system.

00:30:36.590 --> 00:30:40.260
And it can deal with
large numbers of them.

00:30:41.750 --> 00:30:44.740
Why use AUGraph?

00:30:44.740 --> 00:30:48.400
One of the reasons is that connections
and disconnections between the audio

00:30:48.400 --> 00:30:52.700
units can be performed while audio is
actually being pulled through the chain.

00:30:52.700 --> 00:30:58.830
If you are talking to the
audio units at the lower level,

00:30:58.990 --> 00:31:02.800
you have to take great care
at changing connections,

00:31:02.800 --> 00:31:05.700
making connections and disconnections
while the audio is being pulled through.

00:31:05.700 --> 00:31:08.700
You can get all kinds
of bad things happening.

00:31:08.700 --> 00:31:10.700
There's a lot of
synchronization involved.

00:31:10.700 --> 00:31:13.700
AUGraph takes care of this for you.

00:31:13.700 --> 00:31:16.670
Another reason you might want to
use AUGraph is that it maintains

00:31:16.710 --> 00:31:19.700
the representation of the
graph even when the audio units

00:31:19.840 --> 00:31:20.700
themselves are not instantiated.

00:31:20.720 --> 00:31:27.600
It also takes care of and embodies
the state that the audio units are in,

00:31:27.600 --> 00:31:29.180
as we'll see.

00:31:29.560 --> 00:31:37.330
AU Node is the primary object
that's represented in an AU Graph.

00:31:37.420 --> 00:31:42.360
An AU node actually represents a
particular Audio Unit in the graph,

00:31:42.360 --> 00:31:44.370
whether that Audio Unit is
instantiated yet or not.

00:31:44.410 --> 00:31:48.400
It's a placeholder
representing that Audio Unit.

00:31:51.300 --> 00:31:56.570
To create a node,
you can call auGraphNewNode.

00:31:56.950 --> 00:31:59.920
The next argument to this
function is Component Description,

00:31:59.920 --> 00:32:04.580
which specifies the type, subtype,
and ID of the component.

00:32:04.840 --> 00:32:08.800
This information that you would
get from Find Next Component

00:32:08.800 --> 00:32:09.800
to Component Manager Call.

00:32:09.800 --> 00:32:15.940
It's also possible to pass
what we call Class Data to

00:32:15.940 --> 00:32:17.800
initialize an Audio Unit with.

00:32:17.850 --> 00:32:20.770
And we'll talk about that in a minute.

00:32:20.810 --> 00:32:24.630
AU Graph Remove node just
takes it out of the graph.

00:32:25.220 --> 00:32:27.600
To create connections
between these nodes,

00:32:27.600 --> 00:32:31.490
once you've added a bunch of nodes,
you've got Connect Node Input,

00:32:31.490 --> 00:32:34.510
Disconnect Node Input,
Clear Connections that clears

00:32:34.570 --> 00:32:38.420
all the connections in the graph,
and Update for synchronizing

00:32:38.420 --> 00:32:40.100
real-time changes.

00:32:40.100 --> 00:32:44.020
You can also walk through
the graph and look at all the

00:32:44.090 --> 00:32:48.100
nodes with Get Node Count,
Get Node, Get Node Info.

00:32:48.440 --> 00:32:55.100
These are in the AUGraph.h header
file in our Audio Toolbox framework.

00:32:55.100 --> 00:32:58.180
I think that the APIs are
fairly self-explanatory if

00:32:58.180 --> 00:33:01.460
you're looking at header files,
so we're not going to go into

00:33:01.530 --> 00:33:03.100
detail with all the arguments.

00:33:03.200 --> 00:33:10.080
Just like the Audio Units themselves,
the AUGraph mirrors the state

00:33:10.090 --> 00:33:13.100
of the Audio Units in them.

00:33:13.210 --> 00:33:18.100
This slide is just pretty much
going over what I talked about.

00:33:18.100 --> 00:33:20.070
Um...

00:33:24.730 --> 00:33:27.920
The important thing is that
the AUGraph state corresponds

00:33:27.920 --> 00:33:27.920
with the Audio Unit state.

00:33:29.420 --> 00:33:33.250
So the state APIs for
AUGraph are Graph Open,

00:33:33.300 --> 00:33:34.090
Close.

00:33:34.090 --> 00:33:39.300
So AUGraph Open, when you call that,
it actually opens all of the audio units

00:33:39.400 --> 00:33:40.960
represented by the nodes in the graph.

00:33:40.960 --> 00:33:42.680
And Close closes all the audio units.

00:33:42.680 --> 00:33:46.740
And correspondingly, Initialize,
Uninitialize,

00:33:46.790 --> 00:33:49.140
it calls the audio unit functions.

00:33:49.140 --> 00:33:51.940
And Start and Stop, same thing.

00:33:51.940 --> 00:33:58.710
In addition, there are inspection
or introspection APIs.

00:33:58.750 --> 00:34:01.240
AUGraph is Open, is Initialized,
is Running.

00:34:01.240 --> 00:34:04.750
So you can determine what
state the graph is in.

00:34:06.400 --> 00:34:07.550
This class data.

00:34:07.620 --> 00:34:13.630
When an AU node is added to the graph,
it's possible to pass arbitrary data in

00:34:13.790 --> 00:34:17.300
with a void star pointer and a length,
similar to the properties.

00:34:17.400 --> 00:34:23.530
And if you add this information,
this optional class info,

00:34:23.530 --> 00:34:28.730
when the Audio Unit is opened,
a property call will be made on

00:34:28.730 --> 00:34:31.190
the Audio Unit with this data.

00:34:31.920 --> 00:34:35.810
and the Audio Unit is able to make use
of this data in any way that it wants.

00:34:35.810 --> 00:34:39.220
There's no data format that's
defined currently for it.

00:34:39.700 --> 00:34:45.650
But it's a way to maintain persistence
because when the graph is closed,

00:34:45.650 --> 00:34:48.840
data can be gotten from the
Audio Unit and stored in the node.

00:34:48.990 --> 00:34:53.480
So it can be recreated in the
same configuration that it was.

00:34:55.400 --> 00:34:59.720
So, a little bit earlier I tried
to explain the notion of this

00:34:59.810 --> 00:35:01.000
head Audio Unit in a graph.

00:35:01.040 --> 00:35:05.360
In a very simple case,
you would have a chain of Audio Units.

00:35:05.360 --> 00:35:07.000
Say you might have three.

00:35:07.000 --> 00:35:10.940
First one connected to the second,
connected to the last one,

00:35:10.940 --> 00:35:14.140
the third Audio Unit,
which is maybe an output device.

00:35:14.140 --> 00:35:20.080
And this last one represents
the head of the graph.

00:35:20.080 --> 00:35:23.680
And we talked about the pole model.

00:35:24.140 --> 00:35:29.500
The Head Audio Unit is very important
because it controls the hardware.

00:35:29.500 --> 00:35:34.090
It controls whether processing is
being pulled through the graph or not.

00:35:34.100 --> 00:35:38.390
There are special Audio Unit APIs,
Audio Unit Start and Stop,

00:35:38.390 --> 00:35:40.600
which I talked about before.

00:35:40.760 --> 00:35:46.050
The AU Graph has corresponding calls,
AU Graph Start and Stop.

00:35:51.590 --> 00:35:55.340
Okay, done talking about
Audio Units and AUGraph.

00:35:55.340 --> 00:35:58.160
So finally I'm going to
talk about some Sequencing

00:35:58.160 --> 00:36:00.500
Services for performing scheduling.

00:36:00.500 --> 00:36:04.970
A few slides back I had a diagram
up showing the Schedule Render

00:36:04.980 --> 00:36:09.490
Model where in step one you schedule
events to Audio Units and secondly

00:36:09.500 --> 00:36:11.190
you would render a slice of audio.

00:36:11.320 --> 00:36:16.380
The Music Sequencing Services actually
would be one mechanism where

00:36:16.380 --> 00:36:18.760
the scheduling could occur.

00:36:19.420 --> 00:36:26.050
It's a simple API for creating and
editing multi-track MIDI sequences.

00:36:26.060 --> 00:36:30.150
Individual events in the tracks
can address Audio Units and

00:36:30.150 --> 00:36:33.310
Software Synthesizer Music Devices.

00:36:34.680 --> 00:36:39.260
It provides a runtime for the
playback of these sequences and has

00:36:39.260 --> 00:36:43.940
a bunch of APIs for doing live edits,
cut, copy, paste, merge,

00:36:43.960 --> 00:36:46.040
this type of thing.

00:36:46.100 --> 00:36:50.230
So while the sequence is playing,
you can be editing.

00:36:50.260 --> 00:36:53.740
It's important in a sequence or
application to be able to do that.

00:36:53.910 --> 00:36:58.440
For those developers who already have a
sequencing engine in their application,

00:36:58.520 --> 00:37:00.360
this set of APIs might
not be so interesting.

00:37:00.360 --> 00:37:04.910
But for somebody developing
a new application,

00:37:04.950 --> 00:37:09.790
because the APIs are there for
doing the basic edits and for

00:37:09.930 --> 00:37:14.190
actually providing the runtime system
for playing back the sequences.

00:37:14.230 --> 00:37:16.740
It's only necessary to write a UI.

00:37:17.900 --> 00:37:38.800
[Transcript missing]

00:37:40.100 --> 00:38:00.900
[Transcript missing]

00:38:01.530 --> 00:38:06.850
The Music Player is the highest level
object and it provides transport.

00:38:06.940 --> 00:38:16.070
So you can seek the sequence to
a particular time and start the

00:38:16.150 --> 00:38:18.610
sequence playing and stop the sequence
playing with the Music Player.

00:38:20.100 --> 00:38:26.390
provides the appropriate synchronization
between host-based processing and MIDI.

00:38:28.030 --> 00:38:30.400
Music Sequence is a next level object.

00:38:30.400 --> 00:38:35.490
A Music Sequence contains
multiple playback tracks,

00:38:35.490 --> 00:38:36.880
which are called Music Tracks.

00:38:36.890 --> 00:38:41.960
Each Music Track contains
a series of time-ordered,

00:38:41.960 --> 00:38:44.400
time-stamped events.

00:38:47.530 --> 00:38:54.880
Each Music Track with these events
addresses a particular AU node in the

00:38:54.880 --> 00:38:59.780
AU Graph that we spoke about before.

00:39:00.910 --> 00:39:05.340
So for instance, if you have a graph
with a software synth,

00:39:05.390 --> 00:39:07.580
there would be a node
representing the software synth,

00:39:07.580 --> 00:39:10.190
and one of these tracks
might be addressing its

00:39:10.230 --> 00:39:11.910
events to the software synth.

00:39:12.080 --> 00:39:14.800
There might be another
audio unit in the graph,

00:39:14.840 --> 00:39:16.760
which is a filter of some kind.

00:39:16.820 --> 00:39:19.930
There might be another track which is
addressing parameter changes to the

00:39:19.930 --> 00:39:22.620
filter to sweep the filter cutoff.

00:39:26.050 --> 00:39:29.880
There are also attributes to the track,
Mute and Solo, as I said before.

00:39:29.880 --> 00:39:32.970
You can set up the loop length,
number of repetitions of the loop,

00:39:33.110 --> 00:39:39.880
and also offset the start of the
track relative to the other tracks.

00:39:41.770 --> 00:39:45.740
There are several
different types of events.

00:39:45.740 --> 00:39:47.900
The most basic one is MIDI Note Message.

00:39:47.950 --> 00:39:52.490
It's MIDI Note Number with velocity,
MIDI Channel, and a duration.

00:39:52.590 --> 00:39:55.840
The Note Off is not a
music event as such.

00:39:55.850 --> 00:40:00.830
It's implied in this
MIDI Note Message using the duration.

00:40:00.910 --> 00:40:05.290
There's a MIDI Channel Message for
all the other channel messages.

00:40:05.370 --> 00:40:07.790
There's Raw Data for System Exclusive.

00:40:07.870 --> 00:40:10.550
There's a Tempo Event for
doing tempo changes.

00:40:10.620 --> 00:40:15.480
And there's this Extended Note API,
which is very much like

00:40:15.590 --> 00:40:22.260
the MIDI Note Message,
but it provides for more sophisticated

00:40:22.260 --> 00:40:24.890
control in esoteric applications.

00:40:25.010 --> 00:40:29.260
MIDI Note Event only has a
Note Number and a velocity and a

00:40:29.430 --> 00:40:32.950
MIDI Channel associated with it.

00:40:33.070 --> 00:40:35.690
This Extended Note Event allows
for an arbitrary number of

00:40:35.780 --> 00:40:37.700
floating point arguments to be
associated with the MIDI Note.

00:40:37.700 --> 00:40:37.700
There's a Tempo Event for
doing Tempo Changes and

00:40:37.700 --> 00:40:37.760
there's this Extended Note API,
which is very much like

00:40:37.760 --> 00:40:37.760
the MIDI Note Message,
but it provides for more sophisticated

00:40:37.760 --> 00:40:37.760
control in esoteric applications.

00:40:37.770 --> 00:40:41.470
instantiation of a note.

00:40:42.600 --> 00:40:45.860
Not everybody may be interested in that,
but it's there.

00:40:45.860 --> 00:40:53.100
The Extended Control represents a
32-bit floating point parameter change.

00:40:53.250 --> 00:41:01.130
And ultimately, it may end up calling
Audio Unit Set Parameter at exactly the

00:41:01.130 --> 00:41:03.960
right moment in the audio processing.

00:41:03.960 --> 00:41:06.900
For sweeping filter,
for changing volumes, for panning,

00:41:07.060 --> 00:41:11.290
positioning audio,
all different kinds of parameter changes.

00:41:12.440 --> 00:41:15.500
There's also a User Event where
arbitrary data can be placed

00:41:15.840 --> 00:41:19.520
and a Meta Event for MIDI.

00:41:21.090 --> 00:41:23.140
So here's one example.

00:41:23.170 --> 00:41:27.700
There's an API corresponding to
each one of these types of events.

00:41:27.700 --> 00:41:30.300
And all of them are very
similar to this one.

00:41:30.300 --> 00:41:32.120
So I'm just going to go over the one.

00:41:32.180 --> 00:41:35.640
This is how you add a MIDI note event.

00:41:35.690 --> 00:41:41.490
The first argument is the music
track you're adding the event to.

00:41:41.540 --> 00:41:45.230
The second argument,
I think they got the arguments

00:41:45.310 --> 00:41:46.230
lined up a little bit funny.

00:41:46.290 --> 00:41:48.920
But the second argument,
if you can make sense of that,

00:41:48.920 --> 00:41:49.980
is the timestamp.

00:41:49.990 --> 00:41:51.380
It's 32 bit.

00:41:51.380 --> 00:41:54.580
Actually, I think it's a 64 bit.

00:41:54.580 --> 00:41:58.640
It's a double precision
timestamp representing the beat.

00:41:58.750 --> 00:42:01.640
And then you have a
pointer to a structure,

00:42:01.640 --> 00:42:05.560
the MIDI note message structure,
which essentially just

00:42:05.690 --> 00:42:09.040
has the note number,
velocity, MIDI channel,

00:42:09.100 --> 00:42:11.680
and the duration of the note in it.

00:42:15.210 --> 00:42:19.350
So this is a diagram showing how
you can have a Music Sequence

00:42:19.410 --> 00:42:21.600
with multiple tracks,
three in this case,

00:42:21.920 --> 00:42:26.930
where each track is addressing a
separate Audio Unit in an AU Graph.

00:42:30.130 --> 00:42:32.280
This corresponds with
the previous diagram,

00:42:32.320 --> 00:42:36.910
the two-phase schedule render model,
if you remember.

00:42:37.650 --> 00:42:44.260
There are some editing APIs for copy,
cut, paste, this type of thing.

00:42:44.260 --> 00:42:47.760
And I think if you look in the
musicplayer.h header file in

00:42:47.760 --> 00:42:52.890
the Audio Toolbox framework,
it will be very clear how to use these.

00:42:54.160 --> 00:42:59.520
One of the lowest level objects
in Music Sequencing Services is

00:42:59.520 --> 00:43:01.540
the Music Event Iterator.

00:43:01.540 --> 00:43:03.640
It allows you to actually walk
through the events in a track,

00:43:03.710 --> 00:43:05.090
see what they are.

00:43:05.150 --> 00:43:10.900
It could be useful if you're
writing a sequencer application,

00:43:10.900 --> 00:43:16.110
you're writing the UI sitting
on top of this engine for just

00:43:16.110 --> 00:43:16.110
walking through the events,
seeing what they are,

00:43:16.110 --> 00:43:16.110
so you can display them.

00:43:16.370 --> 00:43:23.200
Also, if you want to create custom
editing operations on track,

00:43:23.350 --> 00:43:29.670
you could use this Music Event Iterator
to walk through all of the events

00:43:29.670 --> 00:43:29.670
in a particular time range.

00:43:32.420 --> 00:43:35.340
So there are a couple different
APIs for dealing with these iterators.

00:43:35.340 --> 00:43:38.500
The Music Event Iterator
is a first class object,

00:43:38.500 --> 00:43:42.210
so you create it with new and dispose.

00:43:42.210 --> 00:43:46.300
And you can seek the iterator to
a particular point in the track.

00:43:46.300 --> 00:43:51.300
It finds the first event at or after
the time that you're seeking to.

00:43:51.350 --> 00:43:56.100
You can move the iterator one event
forward in time with next event,

00:43:56.100 --> 00:43:59.300
one event previous with previous event.

00:43:59.300 --> 00:44:04.650
And you can actually ask the
iterator if it's at the end or

00:44:04.650 --> 00:44:09.090
beginning of a track with has
previous event and has next event.

00:44:11.500 --> 00:44:15.990
Then the Music Event Iterator
Get Event Info function can be

00:44:15.990 --> 00:44:20.500
used to find out what type of
events you have at that point.

00:44:20.500 --> 00:44:24.160
So you can find out
what the event type is,

00:44:24.440 --> 00:44:28.610
what time the event occurs at,
and you get a void star pointer which you

00:44:28.610 --> 00:44:32.900
cast to the appropriate event type to get
access to the particulars of that event.

00:44:32.900 --> 00:44:38.040
For instance, if it's a MIDI note event,
you can find out what its note number is,

00:44:38.040 --> 00:44:38.890
and so on.

00:44:39.470 --> 00:44:43.360
You can delete an event at the
point where the iterator is,

00:44:43.360 --> 00:44:45.790
or you can change the event's time.

00:44:47.920 --> 00:44:50.590
Okay, I'm going to play a demo.

00:44:50.790 --> 00:44:56.190
It's kind of a little bit vague
what I'm trying to illustrate here,

00:44:56.370 --> 00:45:02.670
but I think what I'm trying to show is
how all of these services work together.

00:45:02.800 --> 00:45:05.570
So, in the example, I've created a graph.

00:45:05.830 --> 00:45:09.290
I have a software synth,
a DLS music device that's loaded up

00:45:09.320 --> 00:45:14.050
with a 12-string guitar sample bank,
and it's playing through kind

00:45:14.050 --> 00:45:14.800
of an arpeggio really quickly.

00:45:14.850 --> 00:45:18.630
And then I have some additional
audio units in the graph.

00:45:18.760 --> 00:45:20.800
I think there's a digital delay in there.

00:45:20.800 --> 00:45:25.800
There's a look-ahead limiter node
to keep the volume pretty steady.

00:45:25.800 --> 00:45:27.790
And there's a reverb at the end.

00:45:27.790 --> 00:45:34.920
And I also have a bandpass filter in the
graph whose center frequency I'm sweeping

00:45:35.050 --> 00:45:38.800
using the Music Sequencing Services.

00:45:38.800 --> 00:45:41.940
So I've set up a music sequence,
and I have one of the tracks

00:45:41.940 --> 00:45:45.800
addressing the node in the graph,
which is the bandpass filter,

00:45:45.800 --> 00:45:46.800
and sweeping that around.

00:45:46.980 --> 00:45:52.800
And it basically just ends up sounding
like some kind of ethereal wash.

00:45:52.800 --> 00:45:56.800
But you should hear that there are these
kind of filter sweeps going in and out,

00:45:56.800 --> 00:46:00.820
and there's a reverb in there for sure.

00:46:00.900 --> 00:46:06.500
Let's see if I can wake this
machine up and get this going here.

00:46:06.500 --> 00:46:08.570
Okay, got my cursor.

00:46:10.220 --> 00:46:12.600
Unfortunately,
it's not nearly as impressive

00:46:12.600 --> 00:46:15.190
as Jeff Moore's multi-channel
demo the other day,

00:46:15.190 --> 00:46:18.030
but this will give you an idea.

00:46:19.380 --> 00:46:22.500
Okay, I'm only launching an
application actually.

00:46:22.500 --> 00:46:25.730
It's nothing too visual,
it's just to hear.

00:46:25.820 --> 00:46:28.100
But I'll show you.

00:46:28.150 --> 00:46:32.930
Double click,
and you should hear something.

00:47:33.260 --> 00:47:37.120
But it's showing all the parts of
the system that I talked about.

00:47:37.170 --> 00:47:38.540
And I think, Bill?

00:47:38.920 --> 00:47:43.860
I'd like Bill Stewart to come up and he's
going to show us some more demonstrations

00:47:43.860 --> 00:47:47.150
and talk about the Java interfaces.

00:47:52.560 --> 00:47:58.600
I thought I'd make mention that all
of that sound you heard was just some

00:47:58.600 --> 00:48:00.740
notes coming from the 12-string guitar.

00:48:00.740 --> 00:48:03.500
So it's pretty interesting, I think.

00:48:03.500 --> 00:48:12.330
Now, if I can have the demo machine back.

00:48:12.330 --> 00:48:12.330
I've got to get my mouse here somewhere.

00:48:16.200 --> 00:48:27.300
[Transcript missing]

00:48:32.400 --> 00:48:37.400
This is cool, I can just show you
what I want to show you.

00:48:37.400 --> 00:48:40.650
So this is my Java application.

00:48:41.470 --> 00:48:46.510
and do we have sound
coming out from this?

00:48:46.700 --> 00:48:52.590
I'll very quickly walk through
the code with you after this.

00:48:52.730 --> 00:48:55.450
Basically,
I'm using a sound font that we got,

00:48:55.460 --> 00:48:58.540
the same sound font that
Chris is using from Emu,

00:48:58.540 --> 00:49:02.500
which is a 12-string guitar.

00:49:03.000 --> 00:49:37.400
[Transcript missing]

00:49:37.710 --> 00:49:45.830
So I'm going to quit this guy and
let me just bring up the project.

00:49:45.860 --> 00:49:48.880
You have to just

00:49:50.700 --> 00:50:17.700
[Transcript missing]

00:50:29.090 --> 00:50:32.760
So there's a bunch of stuff here just
to sort of set the thing running.

00:50:32.760 --> 00:50:37.700
I'm loading the sound bank here which
I've just installed with Mac OS X.

00:50:37.700 --> 00:50:41.000
We also wanted to make sure that there
were directories that were available

00:50:41.000 --> 00:50:45.420
as standard places for you to put
standard resources that you may want to

00:50:45.420 --> 00:50:47.000
share between different applications.

00:50:47.000 --> 00:50:54.000
So in this case I'm putting the sound
bank in the /library/audio/sounds/banks

00:50:54.000 --> 00:50:57.300
and then this is the name
of the sound file here.

00:50:57.420 --> 00:51:01.330
And there's also a mirror
of this directory in the

00:51:01.330 --> 00:51:03.000
user's directory as well.

00:51:03.000 --> 00:51:06.790
So sound banks that were stored
here would be available to any

00:51:06.790 --> 00:51:08.990
user that came onto the machine.

00:51:08.990 --> 00:51:12.240
So you might imagine like a lab
where you could have a setup that

00:51:12.240 --> 00:51:14.000
you want available to every user.

00:51:14.000 --> 00:51:16.000
And then individual users can
have their own sound bank.

00:51:16.000 --> 00:51:16.570
So you might imagine like a lab
where you could have a setup that

00:51:16.570 --> 00:51:16.990
you want available to every user.

00:51:17.000 --> 00:51:20.520
sandbanks in their own
user version of this thing.

00:51:20.520 --> 00:51:25.460
And there are fine folder constants
that you can use for look for these

00:51:25.460 --> 00:51:29.150
in systems that have been localized.

00:51:30.000 --> 00:51:35.960
There are other folders for Audio Units
if you want to make them available

00:51:36.040 --> 00:51:38.000
for different types of sounds.

00:51:38.000 --> 00:51:42.000
There's a whole bunch of MIDI stuff
here that I'll go through later on.

00:51:42.000 --> 00:51:46.380
What I want to show you is
basically how the graph is set up.

00:51:46.500 --> 00:51:52.600
[Transcript missing]

00:51:52.830 --> 00:51:58.030
constructor here is creating the graph
and that just calls through to the

00:51:58.030 --> 00:52:02.870
C API to create the graph here and
then this is me creating to synth two

00:52:02.960 --> 00:52:07.470
nodes one for the DLS music device and
I give it a component description and

00:52:07.550 --> 00:52:10.900
I already know what I'm looking for here
I could use the fine component staff

00:52:10.940 --> 00:52:16.450
to actually hunt around for components
I'm just creating two nodes I'm creating

00:52:16.680 --> 00:52:23.220
a music device and I'm going to use
the DLS synth as the that one for that

00:52:23.220 --> 00:52:27.520
you know we're very hopeful to see other
music devices from you guys like you

00:52:27.570 --> 00:52:33.120
know some acoustic modeling synthesizer
devices and all that kind of stuff and

00:52:33.130 --> 00:52:38.210
if I get the mouse pad here and then the
second one is the default output unit

00:52:38.210 --> 00:52:42.090
I'm just going to create that because
I want the sound to actually come out of

00:52:42.090 --> 00:52:47.040
the computer and then to connect those
I just tell the graph to connect the node

00:52:47.040 --> 00:52:47.040
and the output unit to the DLS synth and
then I'm going to create the DLS synth

00:52:47.040 --> 00:52:47.040
and then I'm going to create the
DLS synth and then I'm going to create

00:52:47.040 --> 00:52:47.040
the DLS synth and then I'm going to
create the DLS synth and then I'm going

00:52:47.040 --> 00:52:47.220
to create the DLS synth and then I'm
going to create the DLS synth and then

00:52:47.220 --> 00:52:53.300
I'm going to create the output from the
synth into the input of the output node.

00:52:53.700 --> 00:52:59.060
And just to show you that there's
some stuff going on with the graph,

00:52:59.160 --> 00:53:03.360
after I create the graph,
I'm going to get the node count

00:53:03.360 --> 00:53:08.460
here and I get the index node
starting from zero to the number

00:53:08.460 --> 00:53:11.070
of nodes and I print that out.

00:53:11.120 --> 00:53:16.190
And I can show you what
that guy looks like.

00:53:20.300 --> 00:53:23.800
So you can see there that it's
telling me I've got two nodes and

00:53:23.800 --> 00:53:28.490
there's a component which is the music
node and the default output node.

00:53:28.500 --> 00:53:32.270
And I've lost my mouse again,
there it is.

00:53:32.330 --> 00:53:34.990
Okay, so after I do the
connection of the nodes,

00:53:34.990 --> 00:53:37.000
I'm going to open the graph.

00:53:37.040 --> 00:53:40.710
And this will open the audio
units and that will actually,

00:53:40.710 --> 00:53:42.100
it's really a constructor.

00:53:42.100 --> 00:53:45.890
Until you do open on the graph,
you don't have an audio unit,

00:53:45.900 --> 00:53:47.200
you just have the node itself.

00:53:47.300 --> 00:53:49.730
It's an abstract
representation of the graph.

00:53:49.890 --> 00:53:53.850
So when I open the graph,
I can ask the graph to give me

00:53:53.870 --> 00:53:57.560
the audio unit that's associated
with a particular node.

00:53:57.590 --> 00:54:02.110
And if we have a look back up here,
you'd see that I had

00:54:02.460 --> 00:54:03.750
kept this guy around.

00:54:03.760 --> 00:54:07.560
I could actually go and look in the
graph and find out the nodes here

00:54:07.560 --> 00:54:09.560
and I could ask it for information.

00:54:09.600 --> 00:54:12.730
But I've kept this node here,
so I know that what I want to

00:54:12.730 --> 00:54:17.610
do is get the music device,
which is a subclass of audio

00:54:17.650 --> 00:54:19.420
unit for that particular node.

00:54:19.420 --> 00:54:25.420
And what we do in the core audio
Java APIs is we actually give you

00:54:25.420 --> 00:54:30.420
the subclasses that correspond to the
extensions of the component selectors.

00:54:30.420 --> 00:54:34.240
So you could have a basic audio unit,
which is your parent class,

00:54:34.240 --> 00:54:38.120
and you've got an extension of
that with your output audio unit.

00:54:38.120 --> 00:54:40.050
Another extension is the music device.

00:54:40.090 --> 00:54:44.210
And that, in component terms,
just provides additional

00:54:44.260 --> 00:54:48.280
component selectors,
additional functionality.

00:54:48.290 --> 00:54:48.620
If I've specified a set of components,
I can just add them to

00:54:48.640 --> 00:54:48.980
the component selectors.

00:54:48.980 --> 00:54:51.750
And if I've specified a sound bank,
which I'd be kind of a

00:54:51.750 --> 00:54:55.470
bit lame if I didn't,
then I'm going to set the property

00:54:55.470 --> 00:54:57.280
of the sound bank on the music synth.

00:54:57.280 --> 00:54:59.780
And this is a convenience
routine in Java.

00:54:59.840 --> 00:55:03.280
It does the same thing as what
Chris showed in the C file.

00:55:03.280 --> 00:55:05.460
And it just takes the Java file object.

00:55:05.530 --> 00:55:08.130
It does all the necessary
conversions to the native

00:55:08.130 --> 00:55:10.820
structures and instantiates that.

00:55:10.820 --> 00:55:13.790
And you notice that I'm doing
this when I've opened the graph

00:55:13.900 --> 00:55:15.320
but before I initialize it.

00:55:15.360 --> 00:55:18.470
So this is one of these things
where this is a property that

00:55:18.470 --> 00:55:18.540
we want the synth to have.

00:55:18.540 --> 00:55:21.280
We want the synthesizer to
have when it's initialized.

00:55:21.400 --> 00:55:26.570
And until the synth is initialized,
it's not going to try to use a particular

00:55:26.630 --> 00:55:29.780
sound bank until you've initialized it.

00:55:29.800 --> 00:55:33.100
And then just to get the
synth sort of up and running,

00:55:33.100 --> 00:55:38.770
I'm going to set some default
volume and channel stuff.

00:55:38.790 --> 00:55:42.700
And there's some commented at code
there I'm not going to worry about.

00:55:42.730 --> 00:55:47.810
And because I want this to be responsive,

00:55:48.260 --> 00:55:50.200
I quit it, didn't I?

00:55:50.390 --> 00:55:55.170
Because I want this to be responsive,
what I'm actually doing at this point

00:55:55.170 --> 00:56:02.680
is to get the Audio Device Output Unit,
which is the default output

00:56:02.680 --> 00:56:05.200
unit that I'm sending out to.

00:56:05.200 --> 00:56:07.110
From that guy,
I'm going to get the device

00:56:07.160 --> 00:56:10.110
that it's attached to,
and I'm going to set its buffer size.

00:56:10.200 --> 00:56:17.200
That's going to basically take
the default callback buffer,

00:56:17.200 --> 00:56:22.270
which is about 11 milliseconds,
512 sample frames, and I'm going to set

00:56:22.270 --> 00:56:24.200
that to 64 sample frames.

00:56:24.200 --> 00:56:28.200
The responsiveness of the system
is about 1.5 milliseconds.

00:56:28.200 --> 00:56:33.200
I create a memory object that
I can pass that information in,

00:56:33.200 --> 00:56:37.200
and then I just set the output
property straight on the device.

00:56:37.200 --> 00:56:42.200
I get the device from the Output Unit,
and it's actually a device output unit,

00:56:42.200 --> 00:56:47.250
and I set the property to 64 frames,
which is 64 times 2,

00:56:47.260 --> 00:56:53.200
because it's a stereo device, times 4,
which is the size of a float.

00:56:53.200 --> 00:56:55.380
Of course, in Java,
you don't do size of float,

00:56:55.420 --> 00:56:59.170
because it's Java, not C, I suppose.

00:56:59.200 --> 00:57:02.290
Then I actually get
that property as well,

00:57:02.350 --> 00:57:08.200
and I print it out, and you can see,
if I can find my mouse,

00:57:08.200 --> 00:57:11.020
and you can see there that
it's telling me that the buffer

00:57:11.020 --> 00:57:17.030
size for the IOPROC is 512,
which is what we'd expect, 64 by 2 by 4.

00:57:17.200 --> 00:57:22.200
So that's all we do, and then after I've
set the buffer size up,

00:57:22.450 --> 00:57:28.200
and I've initialized the synthesizer,
I then just start the graph,

00:57:28.200 --> 00:57:29.190
and that will start the device.

00:57:29.200 --> 00:57:33.190
It'll start pulling through the chain,
and away we go.

00:57:33.200 --> 00:57:37.480
And then in the next session,
I'll go through how I -- I'll

00:57:37.480 --> 00:57:40.460
set up the MIDI parsing stuff,
so that I can actually send

00:57:40.520 --> 00:57:43.200
MIDI events to the music device.

00:57:43.200 --> 00:57:45.570
So can we go back to slides?

00:57:49.640 --> 00:57:52.350
Okay, so just some notes about the Java.

00:57:52.350 --> 00:57:57.920
There's also a Java doc
available on the website.

00:57:58.000 --> 00:58:03.150
We're also preparing a PDF of
just the Core Audio architecture

00:58:03.240 --> 00:58:08.120
itself and that will be available
at the website early next week.

00:58:08.230 --> 00:58:12.500
And as we stressed yesterday,
we sort of look at Core Audio as

00:58:12.510 --> 00:58:16.050
basically an architecture that you
should understand how it works.

00:58:16.130 --> 00:58:19.780
And that's really a
language neutral feature.

00:58:19.850 --> 00:58:24.200
The Java and the C APIs represent the
same API to the same functionality

00:58:24.200 --> 00:58:29.270
that's implemented underneath and
in a variety of different ways.

00:58:29.660 --> 00:58:32.670
So we also have a mailing list.

00:58:32.740 --> 00:58:34.300
It's at list.apple.com.

00:58:34.300 --> 00:58:36.930
It's called Core Audio API.

00:58:36.930 --> 00:58:41.430
And we welcome you to involve
yourself in that mailing list.

00:58:41.500 --> 00:58:44.690
And it's a good way to send
information to us and we'll

00:58:44.690 --> 00:58:47.500
answer questions on that list.

00:58:47.500 --> 00:58:49.300
And there's a website.

00:58:49.420 --> 00:58:52.340
I don't know if it's completely up yet.

00:58:52.340 --> 00:58:56.410
If it's not up today,
it will be over the next few days.

00:58:56.410 --> 00:58:56.690
developer.apple.com/audio.

00:58:56.810 --> 00:59:05.640
There are some related sessions for the
DVD viewers of this wonderful thing.

00:59:05.640 --> 00:59:10.880
The next session, which should be MIDI,
is at 5:00 today.

00:59:10.880 --> 00:59:13.870
Audio Services and Mac OS X was
yesterday and there's the

00:59:13.930 --> 00:59:16.210
Sound Networking Games before this one.

00:59:16.730 --> 00:59:21.810
If you have any questions
related to FireWire and USB,

00:59:21.910 --> 00:59:28.200
there's a lot of USB audio devices and
MIDI devices and stuff that's coming out,

00:59:28.430 --> 00:59:31.600
you can contact Craig Keithley,
Keithley@apple.com.

00:59:31.600 --> 00:59:34.780
If you're interested in seeding,
we're obviously constantly

00:59:34.780 --> 00:59:36.580
working on these technologies.

00:59:36.630 --> 00:59:40.840
If you're interested in getting access
to that before we release it publicly,

00:59:40.840 --> 00:59:44.940
you can contact us at
audio@apple.com or talk to your

00:59:44.940 --> 00:59:50.520
Developer Relations Manager and
inquire about seeding.