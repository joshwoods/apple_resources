WEBVTT

00:00:01.570 --> 00:00:05.180
Good afternoon and welcome to the
Audio Services in Mac OS X session.

00:00:05.480 --> 00:00:06.260
I'm Craig Keithley.

00:00:06.260 --> 00:00:10.490
I'm the USB and FireWire technology
manager in Worldwide Developer Relations.

00:00:10.540 --> 00:00:14.130
A lot of audio devices these days
are moving to FireWire and USB,

00:00:14.130 --> 00:00:16.500
and we have a great fondness of them.

00:00:16.500 --> 00:00:20.560
We're especially interested in seeing
great FireWire audio devices using

00:00:20.560 --> 00:00:22.390
the MLan standard in the future.

00:00:22.590 --> 00:00:27.480
We are working very diligently to add
that support in an upcoming OS release.

00:00:27.520 --> 00:00:29.500
I can't tell you when or where.

00:00:29.690 --> 00:00:33.290
The thing that I want to stress
as I get ready to introduce the

00:00:33.290 --> 00:00:37.500
engineering team is that Steve said in
his keynote that one of the developer

00:00:37.500 --> 00:00:39.500
feedback items we got was audio support.

00:00:39.530 --> 00:00:43.500
And I've been working with the audio team
and going through their presentations,

00:00:43.500 --> 00:00:46.500
and I think you'll be very
excited by what we've got.

00:00:46.500 --> 00:00:48.510
So with that,
I'd like to bring up Bill Stewart,

00:00:48.510 --> 00:00:50.490
the manager of
Audio Software Engineering.

00:00:50.490 --> 00:00:51.450
Thanks.

00:01:01.070 --> 00:01:07.370
Okay, so I'd like to just give you some
overview of the audio and MIDI services

00:01:07.480 --> 00:01:12.080
that we have shipping in Mac OS X today.

00:01:12.120 --> 00:01:15.000
It came out with the
first version of Mac OS X.

00:01:15.000 --> 00:01:19.100
And give you some idea of the sessions
that we have at the conference.

00:01:19.130 --> 00:01:22.960
This is the first session and we're
going to be mainly focusing on the

00:01:22.960 --> 00:01:27.120
lowest levels of the audio system,
how hardware is presented

00:01:27.120 --> 00:01:29.780
to applications,
how applications can

00:01:29.890 --> 00:01:31.380
talk to audio hardware.

00:01:31.410 --> 00:01:35.760
And there's two sessions
tomorrow starting at 3:30 on

00:01:35.760 --> 00:01:40.380
MIDI and MIDI services that
are provided in Mac OS X,

00:01:40.380 --> 00:01:44.760
as well as the latest session at
5:00 is talking about some high level

00:01:44.890 --> 00:01:50.380
services such as software synthesizers,
audio units and audio unit graphs.

00:01:50.380 --> 00:01:55.380
And in the MIDI session tomorrow,
we'll actually go through some of the

00:01:55.380 --> 00:01:55.380
services that we have in the software.

00:01:55.380 --> 00:01:58.670
So, through some detail about some
of the performance and latency

00:01:58.700 --> 00:02:03.380
characteristics of OS X,
there's been some good and bad,

00:02:03.380 --> 00:02:06.380
usually misinformed
information about that.

00:02:06.380 --> 00:02:11.400
And we'd like to sort of present to you
some information that we've got based

00:02:11.400 --> 00:02:13.380
on some of the tests that we've done.

00:02:13.390 --> 00:02:17.010
And you saw the keynote on Monday,
so there's some information

00:02:17.010 --> 00:02:18.380
we'd like to give to you.

00:02:18.380 --> 00:02:21.250
And that'll be mainly
in tomorrow's session.

00:02:21.510 --> 00:02:26.390
Before I bring up Jeff Moore,
who will talk about the

00:02:26.400 --> 00:02:29.330
bottom layers of the system,
I thought I should give you

00:02:29.490 --> 00:02:33.690
some overview of what is on 10,
some of the things that we had in mind

00:02:33.690 --> 00:02:37.660
when we were designing this system,
and some of the concerns that we sought

00:02:37.690 --> 00:02:40.600
to address by re-architecting the system.

00:02:40.600 --> 00:02:44.990
So, I think flexible audio format,
what does that mean?

00:02:45.000 --> 00:02:47.970
Well,
we've seen audio come a long way since

00:02:47.970 --> 00:02:53.250
two channels was the king of the day,
and we've now got multi-channel

00:02:53.690 --> 00:02:56.600
fairly commonly placed in
a lot of different areas,

00:02:56.910 --> 00:03:00.200
not just in the pro audio world,
but in consumer devices

00:03:00.200 --> 00:03:01.600
and games and so forth.

00:03:01.600 --> 00:03:07.580
We also realize that there's a
need to address the PCM format,

00:03:07.590 --> 00:03:10.600
which is a standard sort of audio format.

00:03:10.600 --> 00:03:15.490
We've got the raw sample data there,
and there's also a lot of devices coming

00:03:15.490 --> 00:03:19.610
out now that deal with non-PCM formats,
like devices that will

00:03:19.640 --> 00:03:24.600
take encoded format in,
say, AC3 as a DVD player or something.

00:03:24.600 --> 00:03:27.590
So, we needed to take that
into consideration.

00:03:27.590 --> 00:03:32.080
We also wanted to preserve as much of
the information as we could if we're

00:03:32.080 --> 00:03:34.600
going to be doing signal processing.

00:03:34.690 --> 00:03:40.600
A lot of the devices are not
just 16-bit devices anymore.

00:03:40.600 --> 00:03:44.600
You'll have resolutions on
the devices up to 24 bits,

00:03:44.700 --> 00:03:48.600
and when we were discussing this,
we thought that the best way to do this,

00:03:48.600 --> 00:03:51.440
not only from a DSP point of view,
but also in terms of

00:03:51.440 --> 00:03:55.140
representing the hardware,
is to present that in a format that

00:03:55.640 --> 00:04:02.520
is capable of preserving the integrity
and the resolution that you require.

00:04:02.600 --> 00:04:07.150
And also, as many of you are aware,
sample rates have gone up quite a

00:04:07.150 --> 00:04:10.550
long way since 44.1K was the standard.

00:04:10.600 --> 00:04:14.670
And so we needed to address that issue.

00:04:15.670 --> 00:04:20.450
Another thing that we wanted to
do in the professional world,

00:04:20.450 --> 00:04:24.600
you typically see a very strong
focus of audio being generated

00:04:24.600 --> 00:04:27.540
and controlled by one application.

00:04:27.540 --> 00:04:32.020
And in an operating system level,
that is not going to fly or it's

00:04:32.020 --> 00:04:37.010
not going to be particularly useful
for applications that may want to

00:04:37.010 --> 00:04:41.050
generate audio and they may not be
the focus application or they want a

00:04:41.050 --> 00:04:45.200
sys beep and they're in the background
or all kinds of things like that.

00:04:45.240 --> 00:04:48.180
So if you're going to have multiple
applications talking to the audio system,

00:04:48.180 --> 00:04:53.400
you've then also got to understand, well,
what might be an appropriate latency for

00:04:53.630 --> 00:04:58.050
one application is not an appropriate
latency for another application.

00:04:58.080 --> 00:05:02.390
So we wanted to design a system that
took care of those issues and let

00:05:02.390 --> 00:05:08.390
applications operate at a level of
detail that they were comfortable with.

00:05:08.760 --> 00:05:13.550
Another weakness that we saw with
the existing sound manager stack

00:05:13.610 --> 00:05:20.600
on Mac OS Classic systems was we
sometimes had no idea what time it was.

00:05:20.600 --> 00:05:23.600
If you look at the samples that are
coming in or samples that are going out,

00:05:23.600 --> 00:05:27.770
you can sort of guess at some
of the time information that

00:05:27.770 --> 00:05:30.820
should be around those samples,
but you may not exactly know what

00:05:31.170 --> 00:05:35.290
time it is and that has a lot of
ramifications for how you deal with

00:05:35.290 --> 00:05:38.960
synchronization with other media,
with hardware that may be

00:05:38.960 --> 00:05:40.600
doing both video and audio.

00:05:40.600 --> 00:05:43.600
So we wanted to address that.

00:05:43.600 --> 00:05:49.750
And if some of you know my history,
I have a particular fondness for Java,

00:05:49.750 --> 00:05:53.600
so I felt we should present these
services not just to C programmers,

00:05:53.610 --> 00:05:57.430
but also present an API for
the Java platform that would

00:05:57.430 --> 00:05:58.600
be comfortable with the sound.

00:05:58.600 --> 00:06:03.120
for people that are
working in that space.

00:06:03.290 --> 00:06:06.790
This is a diagram of the
architecture that we have.

00:06:06.810 --> 00:06:10.540
The white line in the diagram
is the difference between the

00:06:10.600 --> 00:06:13.740
kernel space and then above it
is the different user spaces.

00:06:13.740 --> 00:06:17.580
As you all know,
Mac OS X is a memory protected system.

00:06:17.580 --> 00:06:22.780
Each client has -- each user space
has its own memory address mapping.

00:06:22.780 --> 00:06:28.210
So below the white line you've
got the kernel and the I/O in

00:06:28.340 --> 00:06:31.040
Mac OS X is handled by I/O Kit.

00:06:32.070 --> 00:06:36.070
I/O Kit is a set of classes that deal
with different types of buses and

00:06:36.070 --> 00:06:41.060
different types of hardware like PCI,
et cetera, FireWire, USB.

00:06:41.060 --> 00:06:45.260
And the -- above the line,
the first layer that you see for

00:06:45.410 --> 00:06:47.020
audio is called the Audio HAL.

00:06:47.020 --> 00:06:48.880
HAL stands for Hardware
Abstraction Layer.

00:06:48.880 --> 00:06:54.460
And the main services that you
would expect at that point is

00:06:54.480 --> 00:06:58.440
that the Audio HAL is going to
cross the boundaries when it needs

00:06:58.440 --> 00:07:00.200
to in order to talk to hardware.

00:07:00.960 --> 00:07:06.370
And each user client of the system
is going to see different versions of

00:07:06.370 --> 00:07:09.830
the HAL but then there must be some
global state if you've got two apps

00:07:09.830 --> 00:07:11.800
talking to the same device for instance.

00:07:11.800 --> 00:07:15.510
And so we'll be going through
in today's talk about some of

00:07:15.510 --> 00:07:16.680
the stuff that goes on there.

00:07:16.680 --> 00:07:19.780
And then on top of that,
in the client process,

00:07:19.780 --> 00:07:23.660
you have services that in the
-- on the left hand side or no,

00:07:23.660 --> 00:07:26.630
your right hand side,
you've got classic and classic

00:07:26.630 --> 00:07:30.520
talks to the Audio HAL in order to
talk to hardware in the same way.

00:07:30.540 --> 00:07:34.060
In the second process, you might have the
Carbon Sound Manager and that talks

00:07:34.060 --> 00:07:35.860
to the HAL to talk to hardware.

00:07:35.860 --> 00:07:40.000
And then audio units are
some processing units.

00:07:40.000 --> 00:07:43.630
It's part of the audio toolbox
stuff that we'll be going into

00:07:43.670 --> 00:07:45.110
tomorrow's session at 5:00.

00:07:47.720 --> 00:07:53.060
So the Core Audio Framework,
we have frameworks on 10 and that's

00:07:53.060 --> 00:07:56.410
how libraries and their header
files are presented to users.

00:07:56.560 --> 00:07:59.360
The Core Audio Framework
has the audio hell,

00:07:59.360 --> 00:08:00.590
that's where you find that.

00:08:00.770 --> 00:08:04.080
There's an audio hardware.h file
in there that is where you find

00:08:04.210 --> 00:08:08.750
the APIs and type defs and so forth
for interfacing to the audio hell.

00:08:08.840 --> 00:08:11.680
It has basically the audio
device is the primary object,

00:08:11.720 --> 00:08:15.310
we'll be going through
this in some detail today.

00:08:16.920 --> 00:08:21.460
Audio Toolbox and Audio Unit Frameworks
are the high-level frameworks.

00:08:21.460 --> 00:08:23.880
I'll be covered in detail tomorrow.

00:08:23.910 --> 00:08:26.100
Audio Toolbox,
the aim there is to provide a

00:08:26.100 --> 00:08:30.570
set of tools that you can use
in your application to do audio

00:08:30.570 --> 00:08:35.330
tasks like sequencing a MIDI file,
dealing with different types of

00:08:35.330 --> 00:08:40.380
events that might translate to
some changes in audio behavior,

00:08:40.380 --> 00:08:45.770
like generating notes on a software
synthesizer or panning a sound from

00:08:45.770 --> 00:08:48.360
one channel to another over time.

00:08:48.410 --> 00:08:51.780
And if you've got a sequence of events,
you need some object to play it.

00:08:51.880 --> 00:08:53.800
That's what the music player does.

00:08:53.850 --> 00:08:56.430
Audio units,
I suppose the best way to think

00:08:56.430 --> 00:08:59.460
about this is like a plug-in.

00:08:59.690 --> 00:09:04.240
It's a way of taking audio,
of making a component

00:09:04.610 --> 00:09:08.320
that will take audio in,
do something to it and spit audio out.

00:09:08.530 --> 00:09:10.850
But some audio units
might just generate audio.

00:09:11.000 --> 00:09:12.780
They might not have any audio input.

00:09:12.780 --> 00:09:17.200
A good example of that would
be the software synthesizer.

00:09:17.200 --> 00:09:21.320
And some audio units may
just sort of end there.

00:09:21.320 --> 00:09:23.720
You don't see anything come out of them,
at least from the

00:09:23.720 --> 00:09:25.310
application point of view.

00:09:25.320 --> 00:09:29.900
You might get audio into
them and see nothing out.

00:09:29.900 --> 00:09:33.130
And later on in this talk
we'll be talking about a unit

00:09:33.130 --> 00:09:35.160
we call a default output unit.

00:09:35.160 --> 00:09:39.900
And I'll leave the details of
that until later this afternoon.

00:09:39.980 --> 00:09:43.220
Just to talk a little
bit about Sound Manager.

00:09:43.300 --> 00:09:43.980
That's in Carbon.

00:09:43.980 --> 00:09:46.220
Carbon Sound Manager.

00:09:46.220 --> 00:09:50.790
It's nearly equivalent
API between what's on Tenum,

00:09:50.880 --> 00:09:52.480
what's in the Classic.

00:09:52.480 --> 00:09:56.440
There's some deprecated
APIs that we removed.

00:09:56.440 --> 00:09:58.800
One that was very unpopular,
but we've been telling you for a long

00:09:58.800 --> 00:10:01.350
time we're going to remove it anyway,
so we thought we would,

00:10:01.400 --> 00:10:03.420
which is SoundPlay Double Buffer.

00:10:03.420 --> 00:10:07.300
There is some sample code up on the
website if you want to see how you

00:10:07.300 --> 00:10:09.640
can provide that capability yourself.

00:10:09.880 --> 00:10:13.880
I think an important thing
to understand about the two,

00:10:13.880 --> 00:10:14.840
the two platforms,
is that they're different roles

00:10:14.930 --> 00:10:18.380
the Sound Manager performs
on both of those platforms.

00:10:18.380 --> 00:10:22.160
On a Classic system,
the Sound Manager is a global state.

00:10:22.160 --> 00:10:26.490
You can really see the whole state of
Sound Manager from all the different

00:10:26.490 --> 00:10:31.160
applications and stuff by looking
at the Sound Manager in a nine.

00:10:31.160 --> 00:10:32.840
It also provides a hardware model.

00:10:32.840 --> 00:10:35.360
We have SDEV components,
we have input drivers,

00:10:35.360 --> 00:10:39.580
input components and things like that,
that really do interface directly

00:10:39.580 --> 00:10:43.540
to the hardware and for better
or worse this was the way that,

00:10:43.570 --> 00:10:47.530
if you wanted Sound Manager and clients
of Sound Manager like QuickTime to

00:10:47.530 --> 00:10:50.920
talk to your audio hardware,
you had to write components

00:10:50.920 --> 00:10:52.400
for the Sound Manager.

00:10:52.400 --> 00:10:55.390
On Mac OS X,
the Sound Manager is quite different.

00:10:55.440 --> 00:10:58.900
It's a client of the audio system
and the audio system we have on

00:10:58.900 --> 00:11:03.520
X is really called Core Audio,
which is what these talks are about.

00:11:03.520 --> 00:11:06.980
On Mac OS X,
the Sound Manager is per process.

00:11:06.980 --> 00:11:09.760
You don't have any idea what's
going on in another process

00:11:09.920 --> 00:11:11.760
through Sound Manager calls.

00:11:11.820 --> 00:11:13.440
It's only what's happening
in that particular process.

00:11:13.440 --> 00:11:16.850
It also does not provide
the hardware model.

00:11:16.940 --> 00:11:20.520
The lower levels of the
Sound Manager on X talk to the hell

00:11:20.600 --> 00:11:25.650
and the hardware model for audio
devices are provided by I/O Kit,

00:11:25.670 --> 00:11:31.530
specifically by the I/O audio family
classes and support capabilities.

00:11:31.620 --> 00:11:35.560
So that's an important
distinction to understand.

00:11:35.560 --> 00:11:40.090
Another important distinction
I think is the Sound Manager has a

00:11:40.090 --> 00:11:43.340
native format of 16 bit integers.

00:11:43.340 --> 00:11:46.190
Doesn't deal very well
with formats beyond that.

00:11:46.240 --> 00:11:51.560
Doesn't deal very well with stereo,
with anything beyond stereo.

00:11:51.560 --> 00:11:55.210
These are all limitations that we
hope we've completely gone beyond

00:11:55.320 --> 00:11:59.210
with Core Audio where you can have
N channels of data where you can

00:11:59.210 --> 00:12:04.670
deal with floating point 32 bit
as your primary generic format.

00:12:05.400 --> 00:13:27.800
[Transcript missing]

00:13:28.230 --> 00:13:28.830
Devices.

00:13:28.880 --> 00:13:29.980
What devices?

00:13:30.190 --> 00:13:36.900
Well, Apple currently supports
the USB audio class driver,

00:13:37.140 --> 00:13:39.250
or as I was reminded earlier,
USB audio and MIDI.

00:13:39.260 --> 00:13:41.100
I've got two slides for this.

00:13:41.270 --> 00:13:44.210
We support obviously built-in hardware.

00:13:44.210 --> 00:13:49.100
Most of Apple's built-in
hardware is 16-bit 44K stereo.

00:13:49.330 --> 00:13:54.040
USB provides a completely different
thing and we'll show you some of the

00:13:54.040 --> 00:14:00.100
extended work that we're doing to make
sure we do support USB audio as we should

00:14:00.100 --> 00:14:03.680
on the platform later in this session.

00:14:04.030 --> 00:14:07.110
Firewire Audio,
as Craig mentioned earlier, is a very,

00:14:07.270 --> 00:14:10.570
we consider this at Apple to
be a very important technology

00:14:10.570 --> 00:14:14.750
going forward for audio,
particularly for the pro audio space

00:14:14.870 --> 00:14:20.890
for the high performance and the kind
of stringent demands of pro audio wants.

00:14:21.070 --> 00:14:23.810
Firewire Audio is really
comprised of two things,

00:14:23.810 --> 00:14:27.450
the 61883-6 transport layer,
it took me months to

00:14:27.450 --> 00:14:30.900
remember that number,
and I can get it now,

00:14:30.900 --> 00:14:33.900
and the MLan connection management.

00:14:33.920 --> 00:14:37.440
The transport layer is just
how do you put audio and

00:14:37.440 --> 00:14:39.860
MIDI data across Firewire bus.

00:14:39.900 --> 00:14:44.240
MLan connection manager is intelligent
information that the devices can

00:14:44.240 --> 00:14:48.140
publish about their capabilities
to other Firewire devices and

00:14:48.390 --> 00:14:53.900
allows for configuration of a
network of audio or MIDI devices.

00:14:53.900 --> 00:14:57.790
We are at Apple a signatory of the
MLan connection management stuff

00:14:57.860 --> 00:15:01.880
and we will certainly be doing,
I think, some very interesting work

00:15:01.880 --> 00:15:03.900
in this sphere going forward.

00:15:03.900 --> 00:15:05.900
And it's a very similar story for MIDI.

00:15:05.900 --> 00:15:08.900
We actually do provide
a USB MIDI class driver.

00:15:08.900 --> 00:15:15.140
We also, in our developer example code,
provide a sample code for

00:15:15.140 --> 00:15:19.730
vendor specific USB drivers,
so it's very easy if you're publishing

00:15:20.260 --> 00:15:25.150
a vendor specific USB MIDI device
to just sort of take that code as

00:15:25.150 --> 00:15:28.760
a basis and then provide a driver,
which many people in this room will

00:15:28.760 --> 00:15:30.720
probably would like to use yesterday.

00:15:30.900 --> 00:15:32.830
And we also have Firewire.

00:15:32.950 --> 00:15:33.520
Okay.

00:15:33.900 --> 00:15:34.900
So, we've got a couple of questions.

00:15:34.900 --> 00:15:37.940
One is,
what are the plans for MIDI as part

00:15:37.940 --> 00:15:40.900
of the MLan connection management?

00:15:40.950 --> 00:15:42.760
So, you've had enough of me.

00:15:42.920 --> 00:15:44.900
I'm going to bring up Jeff Moore now.

00:15:44.900 --> 00:15:45.900
Jeff's the engineer.

00:15:45.900 --> 00:15:47.790
Been working on the audio
hardware abstraction layer,

00:15:47.790 --> 00:15:48.890
and please make him welcome.

00:15:48.910 --> 00:15:49.900
Thank you.

00:15:50.360 --> 00:15:50.900
Thank you.

00:15:50.900 --> 00:15:51.900
I like you.

00:15:54.420 --> 00:15:57.660
My job here is to kind of delve
into what the audio hardware

00:15:57.660 --> 00:15:58.730
abstraction layer is all about.

00:15:58.740 --> 00:16:03.360
So, as Bill said,
it provides the lowest user level

00:16:03.360 --> 00:16:06.720
access to audio devices on the system.

00:16:06.720 --> 00:16:10.580
Its role is to efficiently move
data to and from the device.

00:16:10.580 --> 00:16:14.800
It also allows you to manipulate
device attributes like the volume,

00:16:14.800 --> 00:16:17.520
the sample rate, mute, whatever.

00:16:17.960 --> 00:16:21.450
And it also provides very strong
synchronization primitives to

00:16:21.450 --> 00:16:24.800
allow you to know what's going on
at any given time in the hardware.

00:16:24.800 --> 00:16:31.400
The design goals that we had for the HAL,
primarily the first foremost,

00:16:31.460 --> 00:16:32.870
a clean low overhead signal path.

00:16:32.880 --> 00:16:36.560
We don't want to introduce any
distortion into your signal at all.

00:16:36.610 --> 00:16:38.680
And further,
we want to get out of your way.

00:16:38.680 --> 00:16:42.790
We want to make sure it's getting out
to the hardware as fast as it can.

00:16:43.080 --> 00:16:47.760
Another big ticket item is multiple
simultaneous client access.

00:16:48.060 --> 00:16:49.920
To single audio devices.

00:16:49.930 --> 00:16:54.430
That's been a hallmark feature
of the Macintosh forever.

00:16:54.560 --> 00:16:57.830
We needed to carry that forward.

00:16:57.920 --> 00:17:01.760
And then it's to break through some
of the bottlenecks we had on OS 9.

00:17:01.760 --> 00:17:05.350
You know, more channels,
higher bit depths, higher sample rates.

00:17:05.360 --> 00:17:08.000
And then again, you know,
synchronization.

00:17:08.000 --> 00:17:09.780
It's like, I can't say it enough.

00:17:09.870 --> 00:17:11.690
I mean,
it's like one of the most important

00:17:11.690 --> 00:17:14.040
things that we do now with audio on 10.

00:17:14.090 --> 00:17:17.920
And then finally,
we came up with a pretty small API.

00:17:17.920 --> 00:17:19.850
But it's fairly complete.

00:17:19.920 --> 00:17:23.920
We think that it should cover the
majority of the needs that people

00:17:23.920 --> 00:17:27.920
will have to do when trying to
manipulate their audio hardware.

00:17:27.920 --> 00:17:32.870
The API itself is object-oriented C code.

00:17:32.880 --> 00:17:34.830
That's not objective C code.

00:17:34.930 --> 00:17:37.700
But it has a Java interface as well.

00:17:38.240 --> 00:17:41.830
Since it's object-oriented,
it has two major objects.

00:17:41.910 --> 00:17:45.520
It has a device object,
which basically encapsulates

00:17:45.520 --> 00:17:47.880
all the information you need
to know about the device.

00:17:48.180 --> 00:17:50.920
And access to the actual device.

00:17:50.920 --> 00:17:53.920
And then there's a kind
of a more nebulous object,

00:17:53.920 --> 00:17:57.300
which is a global context,
which allows you access to

00:17:57.300 --> 00:18:00.730
information about the system,
such as, you know,

00:18:00.730 --> 00:18:03.100
the device list and whatnot.

00:18:03.680 --> 00:18:07.870
So every object in the
system has properties,

00:18:07.950 --> 00:18:12.570
and properties manipulate the
different aspects of the object.

00:18:12.630 --> 00:18:19.600
Things like the name, the buffer size,
volume of channel 2, whatever.

00:18:19.600 --> 00:18:24.600
You can pretty much describe anything
about a device in terms of properties.

00:18:24.600 --> 00:18:27.970
The actual property itself
is a unique identifier.

00:18:27.970 --> 00:18:32.600
It's a 32-bit int, and they can have an
arbitrary amount of data.

00:18:32.600 --> 00:18:36.100
And there is no self-description
mechanism like you get

00:18:36.100 --> 00:18:37.560
from an Apple event.

00:18:37.630 --> 00:18:41.180
The data types you get are agreed on,
and they're documented

00:18:41.180 --> 00:18:42.480
in the header files.

00:18:42.590 --> 00:18:48.600
And it's also important to know that some
of the properties are only read-only.

00:18:48.600 --> 00:18:51.250
You need to make sure you're
checking with the device before

00:18:51.260 --> 00:18:52.590
you try to set a property.

00:18:52.640 --> 00:18:53.900
Some devices may allow you to
check with the device before

00:18:53.900 --> 00:18:54.600
you try to set a property.

00:18:54.600 --> 00:18:55.600
Some may change the sample rate.

00:18:55.600 --> 00:18:56.600
Some may not.

00:18:56.610 --> 00:18:58.990
And then finally,
another big important thing about

00:18:58.990 --> 00:19:02.600
properties is that you can sign up for
notifications for when things change.

00:19:02.600 --> 00:19:07.470
So if someone changes the system volume,
you can find out about it.

00:19:08.350 --> 00:19:13.480
So the global properties are all
referenced using the routines in the

00:19:13.480 --> 00:19:17.310
API that use the prefix audio hardware.

00:19:17.400 --> 00:19:21.740
The constants and everything that
pertain to the global context,

00:19:21.740 --> 00:19:25.100
you'll see the words audio
hardware in their name somewhere.

00:19:25.100 --> 00:19:28.130
The important properties when
you're looking at the global

00:19:28.130 --> 00:19:32.520
space are the device list,
which is the default

00:19:32.580 --> 00:19:34.150
input and output device.

00:19:34.220 --> 00:19:36.610
And then there's a kind
of a fourth property,

00:19:36.610 --> 00:19:39.650
which is whether or not you
want the machine to idle sleep

00:19:39.650 --> 00:19:41.480
while you're playing sound.

00:19:41.550 --> 00:19:44.270
By default,
that's disabled so that idle sleeping

00:19:44.270 --> 00:19:48.360
won't happen while you're playing sound,
much like it happens on OS 9.

00:19:48.480 --> 00:19:55.570
So here's a code example of how you
would go about getting the device list.

00:19:55.950 --> 00:19:59.730
The first step is to call audio
hardware get property info because

00:19:59.730 --> 00:20:03.440
the device list is documented
to be a variable length array.

00:20:03.490 --> 00:20:04.160
So you need to find out
what the device list is.

00:20:04.160 --> 00:20:06.920
And then you need to find out how
much memory it's going to take up.

00:20:06.930 --> 00:20:10.870
So one of the parameters you get
back from audio hardware get property

00:20:10.870 --> 00:20:13.980
info is the actual size of the data.

00:20:13.980 --> 00:20:16.980
And you can see -- and then to
get the number of actual devices,

00:20:16.980 --> 00:20:19.300
you divide the size you get back,
which is in bytes,

00:20:19.300 --> 00:20:22.340
by the size of an audio device ID.

00:20:22.620 --> 00:20:26.400
Then you allocate the
memory to hold the list.

00:20:26.430 --> 00:20:29.640
And then you call audio
hardware get property.

00:20:29.660 --> 00:20:31.920
And you pass that memory
you just allocated in.

00:20:31.920 --> 00:20:36.200
And the HAL will go then and
fill out all the device IDs for

00:20:36.200 --> 00:20:38.810
all the devices on the system.

00:20:38.940 --> 00:20:41.480
And then you can peruse and
get information about those

00:20:41.480 --> 00:20:47.320
devices and use which devices you
want that best fit your needs.

00:20:47.410 --> 00:20:50.900
And then once you have a device,
all the routines, again,

00:20:50.900 --> 00:20:58.050
we use audio device to refer to
the things that manipulate devices.

00:20:58.300 --> 00:21:02.630
As you'll see, devices are addressed with
a very specific protocol.

00:21:02.700 --> 00:21:06.980
You need to make sure you're
asking about an input or an

00:21:06.980 --> 00:21:15.860
output property and exactly what
channel you are talking about.

00:21:16.070 --> 00:21:21.990
Simple, but it's also very particular
about doing things like that.

00:21:22.000 --> 00:21:25.120
The important properties that
you'll see with a device are,

00:21:25.120 --> 00:21:29.000
of course, the buffer size of the device
for doing the I/O transfers.

00:21:29.000 --> 00:21:33.380
An important thing to realize about
that is the buffer size is not

00:21:33.380 --> 00:21:36.000
just settable by the application.

00:21:36.200 --> 00:21:41.000
It's settable for each
application individually.

00:21:41.000 --> 00:21:45.590
One application can be communicating
with a device at a buffer size of,

00:21:45.600 --> 00:21:48.850
say, 64 frames,
and another process could be

00:21:49.040 --> 00:21:53.950
communicating with the same device
with a buffer size of 2,048 frames.

00:21:54.000 --> 00:21:57.960
It's what Bill was talking about when
he was talking about controllable

00:21:57.960 --> 00:22:00.000
latency on a per-process basis.

00:22:00.150 --> 00:22:04.000
We think this is one of the big
new features of audio on 10.

00:22:04.000 --> 00:22:09.000
Then you'll also want to know things
about the layout of the device,

00:22:09.000 --> 00:22:09.990
and you can use the stream
configuration property to do that.

00:22:10.030 --> 00:22:14.100
Then you'll also want to know
things about the volume and mute,

00:22:14.100 --> 00:22:17.000
and what data source you're talking to.

00:22:17.000 --> 00:22:21.970
They're all individual
properties for the device.

00:22:22.000 --> 00:22:26.170
Here's a code example of how
you would go about setting the

00:22:26.200 --> 00:22:28.800
volume of channel 2 on a device.

00:22:29.220 --> 00:22:31.420
First thing you need to find
out is if you actually can

00:22:31.420 --> 00:22:33.000
set the volume of channel 2.

00:22:33.000 --> 00:22:35.930
Again,
you use Audio Device Git Property Info.

00:22:36.000 --> 00:22:39.000
You can see how I'm being very
particular about specifying channel 2.

00:22:39.000 --> 00:22:44.110
I'm also making sure I'm finding
out ahead of time whether or not

00:22:44.110 --> 00:22:46.980
I actually can set the property.

00:22:47.070 --> 00:22:51.550
Then when I figure it out,
you just turn around and call

00:22:51.550 --> 00:22:53.000
Audio Device Set Property.

00:22:53.000 --> 00:22:55.320
In this case,
the volume scaler is a floating

00:22:55.330 --> 00:22:57.000
point number from 0 to 1.

00:22:57.000 --> 00:23:01.000
In this example,
we're setting the volume to full on.

00:23:02.700 --> 00:23:07.520
So now you kind of know your
way around the API a little bit.

00:23:07.890 --> 00:23:12.750
Now it's time to talk a little
bit about how you actually get

00:23:13.130 --> 00:23:16.130
audio data to the hardware.

00:23:16.410 --> 00:23:21.580
We rethought how audio I/O was
done to take advantage of

00:23:21.590 --> 00:23:23.930
what OS X really does well.

00:23:24.160 --> 00:23:26.460
In particular,
OS X is really good at handling

00:23:26.750 --> 00:23:34.700
shared memory and threading and things
that OS 9 really wasn't as good at.

00:23:34.700 --> 00:23:39.900
It's a little less good at things
like hardware that has high degrees,

00:23:39.900 --> 00:23:42.300
high rates of interrupts being fired.

00:23:42.300 --> 00:23:45.330
So we had to work around all these
different constraints and come up

00:23:45.330 --> 00:23:48.980
with a system that could deliver
the performance we were looking for

00:23:49.030 --> 00:23:55.380
without having to re-architect the
entire kernel from the inside out.

00:23:55.380 --> 00:24:00.200
So the model we came up with is
basically the hardware and the

00:24:00.200 --> 00:24:04.780
HAL that have a ring buffer that
is a pretty large ring buffer.

00:24:04.840 --> 00:24:09.000
It's about 3/4 of a second
in size for most drivers.

00:24:09.000 --> 00:24:13.090
And that one ring buffer is
shared all across the system.

00:24:13.940 --> 00:24:19.300
So, and the hardware tracks its progress
through the ring buffer by time stamping

00:24:19.590 --> 00:24:21.890
the point at which it wraps back around.

00:24:22.130 --> 00:24:26.270
So the DMA engine is cycling through the
buffer and when it jumps back around,

00:24:26.280 --> 00:24:29.800
it will generate some timing information.

00:24:29.820 --> 00:24:32.940
And that is also coincidentally
the only time when you'll

00:24:32.940 --> 00:24:34.600
see a hardware interrupt.

00:24:34.620 --> 00:24:38.280
So the hardware rate,
the interrupt rate of the system

00:24:38.290 --> 00:24:40.700
has gone way down compared to OS 9.

00:24:40.700 --> 00:24:42.700
And this is much less
taxing on the system.

00:24:42.700 --> 00:24:44.590
You get better throughput.

00:24:44.870 --> 00:24:47.190
The rest of the system is a lot happier.

00:24:47.270 --> 00:24:50.470
And as people are finding,
audio is becoming more and more

00:24:50.470 --> 00:24:54.000
integrated in what you're doing
in other parts of the system.

00:24:54.000 --> 00:24:57.290
So it's really important
to be well behaved.

00:24:57.830 --> 00:25:01.710
So to move data in and
out of the ring buffer,

00:25:01.710 --> 00:25:05.970
the HAL provides what's
called an I/O thread.

00:25:06.050 --> 00:25:10.380
And each device has a single
I/O thread in each process in

00:25:10.380 --> 00:25:15.960
order to call your function that
will provide or consume data.

00:25:15.960 --> 00:25:19.060
This thread is a Mach real-time thread.

00:25:19.060 --> 00:25:23.780
The reason why we do that is that
Mach real-time threads have certain

00:25:23.900 --> 00:25:30.540
guarantees about the level of accuracy
they will get for latency in scheduling.

00:25:30.540 --> 00:25:34.560
So when the HAL says this thread needs
to wake up at such and such a time,

00:25:34.560 --> 00:25:38.580
because it's a Mach real-time thread,
we're pretty guaranteed that

00:25:38.580 --> 00:25:42.420
it's going to wake up pretty
close to when we asked it to.

00:25:42.450 --> 00:25:46.650
And then the HAL will also use another
lower priority thread in order to

00:25:46.650 --> 00:25:50.210
handle all the property notifications.

00:25:50.220 --> 00:25:55.420
And the client can actually have some
degree of control over the priority.

00:25:55.420 --> 00:26:00.550
And timing characteristics of that
thread using the audio hardware

00:26:00.550 --> 00:26:05.400
property run loop to tell the
HAL about your own CF run loop that

00:26:05.720 --> 00:26:08.770
you want your notifications fired in.

00:26:09.690 --> 00:26:13.680
So I'm going to walk you through kind
of the algorithm of the I/O thread

00:26:13.680 --> 00:26:17.730
so you kind of have an idea of what's
going on kind of behind the scenes.

00:26:17.820 --> 00:26:22.330
So the first thing that happens is
when the HAL first enters the loop,

00:26:22.430 --> 00:26:25.260
the work loop of the I/O thread,
is it goes to sleep.

00:26:25.390 --> 00:26:29.830
The reason why is that it needs
to prepare for input data.

00:26:29.980 --> 00:26:32.710
So presumably the hardware
has just started up,

00:26:32.710 --> 00:26:36.620
and now it needs to wait for the first
full buffer's worth of input data.

00:26:36.730 --> 00:26:40.480
So the very first thing it does is
figure out when that's going to be,

00:26:40.480 --> 00:26:43.450
and then it puts the
thread to sleep until then.

00:26:44.130 --> 00:26:49.090
So then when it comes back and wakes up,
presumably all the input data,

00:26:49.090 --> 00:26:51.160
if it's there, is available now.

00:26:51.190 --> 00:26:57.060
So it calculates what the actual
timestamp is of that input data.

00:26:57.230 --> 00:27:01.240
And then it calls down into the
kernel to actually fill up all the

00:27:01.240 --> 00:27:03.840
buffers out of the ring buffer.

00:27:03.840 --> 00:27:08.780
The driver itself has complete control
over the transfer out of the ring

00:27:08.880 --> 00:27:11.870
buffer and into the client's buffer.

00:27:12.900 --> 00:27:17.170
And then the same is true for output,
but in this case we're about to call

00:27:17.170 --> 00:27:19.590
out to ask the client for output.

00:27:19.590 --> 00:27:22.110
So we have to figure out
when we want the client,

00:27:22.110 --> 00:27:25.790
what time it is that we're
asking the client for audio.

00:27:25.800 --> 00:27:30.800
And then we set up their audio
buffers by clearing them out.

00:27:30.800 --> 00:27:34.290
The one thing that the HAL does
is that it tries to isolate

00:27:34.290 --> 00:27:37.800
you as much as possible from
other clients on the system.

00:27:37.820 --> 00:27:42.470
So you don't have to worry about
necessarily accumulating to a buffer

00:27:42.470 --> 00:27:47.770
or worrying that someone else is going
to have already written data there.

00:27:47.880 --> 00:27:50.580
And then once we have the input
and output data marshalled,

00:27:50.580 --> 00:27:52.800
we then call out to all
the client I/O procs.

00:27:52.800 --> 00:27:57.020
And I'll talk a little bit more
about writing I/O procs in a minute.

00:27:57.710 --> 00:28:01.980
So once the I/O procs have all assembled,
presumably they've consumed the

00:28:02.020 --> 00:28:05.260
input data and they provided
the requested output data,

00:28:05.640 --> 00:28:10.600
the HAL will then have to call back
down into the driver to mix the

00:28:10.600 --> 00:28:14.120
provided data into the system's mix bus.

00:28:14.400 --> 00:28:17.410
This is all part of the shared
device management we have,

00:28:17.420 --> 00:28:21.210
and it's also the main reason
why we use Float32 as our lingua

00:28:21.210 --> 00:28:24.050
franca as far as sample formats go.

00:28:24.170 --> 00:28:29.730
We want to maintain the headroom of
the mix as far down and as close to the

00:28:29.730 --> 00:28:36.390
hardware as possible to minimize the
amount of distortion added to the signal.

00:28:36.530 --> 00:28:40.500
Then once we've handled the mixing,
the HAL will also then handle

00:28:40.500 --> 00:28:44.150
any reentrant actions that
might have occurred by calling

00:28:44.150 --> 00:28:46.130
out to the client I/O procs.

00:28:46.250 --> 00:28:48.800
For instance,
you might want to adjust the

00:28:48.800 --> 00:28:53.600
buffer size on the fly to
adjust your own performance.

00:28:53.600 --> 00:28:55.760
This is a very common
problem with I/O procs.

00:28:55.760 --> 00:29:04.160
The HAL has a mechanism for catching
those circumstances and handling it after

00:29:04.160 --> 00:29:08.760
that complete I/O cycle has completed.

00:29:15.500 --> 00:30:12.100
[Transcript missing]

00:30:13.140 --> 00:30:15.290
So next up,
you're going to need to have an

00:30:15.290 --> 00:30:17.640
I/O proc for the I/O thread to call.

00:30:17.690 --> 00:30:22.440
So you register your
I/O proc with the device,

00:30:22.450 --> 00:30:26.300
and then you have to start and stop them.

00:30:26.560 --> 00:30:30.300
You do individual transactions
on starting and stopping

00:30:30.630 --> 00:30:33.940
based on the I/O proc,
not on the entire device.

00:30:33.950 --> 00:30:37.800
And the reason why is that these guys
are shared across the whole system.

00:30:37.820 --> 00:30:43.610
When you register your I/O proc,
the hardware may already be in use by

00:30:43.610 --> 00:30:46.500
someone else in a different process.

00:30:46.500 --> 00:30:51.280
So we manage the state
relative to the I/O proc.

00:30:51.280 --> 00:30:55.200
So when you start your I/O proc,
the device will only start if

00:30:55.200 --> 00:30:59.130
this is the actual first I/O proc
on the entire system that is

00:30:59.130 --> 00:31:01.180
starting up on that device.

00:31:01.180 --> 00:31:05.320
And the converse is true
with stopping the device.

00:31:06.380 --> 00:31:11.120
So when your I/O proc is called,
you're going to receive all the input

00:31:11.500 --> 00:31:15.660
data and you're going to be provided
with a buffer in which to write all

00:31:15.710 --> 00:31:18.040
the output data for that time slice.

00:31:18.190 --> 00:31:23.750
And there are going to be timestamps
that tell you exactly what's going on,

00:31:23.840 --> 00:31:27.120
where those buffers
belong in the timeline.

00:31:27.370 --> 00:31:32.010
The input timestamp will tell
you when the first frame of

00:31:32.260 --> 00:31:34.180
the input data was acquired.

00:31:34.250 --> 00:31:38.440
And the output timestamp will tell
you when the first frame is going

00:31:38.440 --> 00:31:41.430
to be written out to the hardware.

00:31:44.440 --> 00:31:48.160
The final timestamp is
also the current time,

00:31:48.160 --> 00:31:54.260
which as you'll see from
the previous diagram,

00:31:54.480 --> 00:31:56.000
falls in between.

00:31:56.120 --> 00:31:59.900
That gives you some dead reckoning
on what's going on in the system.

00:31:59.900 --> 00:32:03.220
You might use that to sync
other subsystems to what's

00:32:03.220 --> 00:32:05.070
going on in the hardware.

00:32:06.160 --> 00:32:10.130
So when your I/O proc is called,
we pass everything in.

00:32:10.220 --> 00:32:12.550
The data is passed into
your I/O proc using an audio

00:32:12.590 --> 00:32:14.010
buffer list data structure.

00:32:14.130 --> 00:32:18.150
An audio buffer list is a collection
of individual audio buffers.

00:32:18.270 --> 00:32:22.660
And you'll get a buffer for each
stream that the audio device has.

00:32:22.800 --> 00:32:26.490
And you can think of a stream
in this case as being a single

00:32:26.560 --> 00:32:30.640
interleaved container for the data.

00:32:30.640 --> 00:32:36.570
So a device might have two streams
that have two channels in each stream.

00:32:36.730 --> 00:32:39.020
And the audio buffer
list will reflect that,

00:32:39.070 --> 00:32:40.220
that you'll receive.

00:32:40.260 --> 00:32:43.500
So what we're really trying to
do is kind of mimic what the

00:32:43.590 --> 00:32:48.700
hardware is presenting to us,
to you.

00:32:48.770 --> 00:32:53.920
And you can find out ahead of time what
that's actually going to look like by

00:32:53.920 --> 00:32:55.990
using the stream configuration property.

00:32:56.090 --> 00:33:00.300
So that if you want to set up-- if you
want to know what's going on so you

00:33:00.300 --> 00:33:03.830
can set up your blit loops or whatever,
the information's there.

00:33:05.580 --> 00:33:10.680
So this is kind of a code
example of an actual I/O proc.

00:33:10.710 --> 00:33:16.030
You can see this slide has
the prototype of an I/O proc.

00:33:16.160 --> 00:33:20.840
You see you get the device ID,
you get all the timestamps,

00:33:20.890 --> 00:33:22.880
you get the individual
audio buffer lists,

00:33:22.930 --> 00:33:29.730
and you also get a client data pointer,
AKA a Refcon from OS 9.

00:33:29.960 --> 00:33:33.590
And you can use the client data
pointer for anything you want.

00:33:34.640 --> 00:33:38.360
In this case, this I/O proc,
I'm using the client data pointer

00:33:38.360 --> 00:33:44.060
as a pointer to my C++ object
that has my audio engine in it.

00:33:45.970 --> 00:33:49.170
So basically, you have to make sure
you're-- in the I/O proc,

00:33:49.380 --> 00:33:52.930
you make sure you're checking for null.

00:33:53.100 --> 00:33:57.800
The device can change format on
you between calls to your I/O proc.

00:33:57.840 --> 00:34:03.340
It can change format on you between
your call to getting the format

00:34:03.340 --> 00:34:06.140
of the device and your I/O proc.

00:34:06.440 --> 00:34:08.980
The only way to be sure you know
what's going on is to sign up for

00:34:09.070 --> 00:34:12.000
notifications on the format properties.

00:34:12.070 --> 00:34:14.690
But in the I/O proc,
you should always make sure you're

00:34:14.700 --> 00:34:18.910
doing your sanity checks or you
will crash and die horribly.

00:34:21.490 --> 00:34:24.400
So, you can see in this case,
we're basically doing all that.

00:34:24.400 --> 00:34:28.690
I passed the input buffers and
the input timestamps to my engine.

00:34:29.250 --> 00:34:35.450
In this slide we see how you actually
register and unregister an I/O proc.

00:34:35.480 --> 00:34:37.160
It's pretty straightforward.

00:34:37.160 --> 00:34:39.750
Add I/O proc, remove I/O proc.

00:34:40.200 --> 00:34:42.940
And you can see how I'm passing
in the pointer to my engine

00:34:42.940 --> 00:34:45.790
object in this case as well.

00:34:46.560 --> 00:34:49.900
And then starting and
stopping is very much similar.

00:34:49.900 --> 00:34:53.050
There's individual
calls to start and stop.

00:34:53.760 --> 00:34:56.270
and next up,
I'm going to bring up Doug Wyatt.

00:34:56.330 --> 00:35:00.260
He's another engineer for Core Audio and
he's going to talk a little bit

00:35:00.260 --> 00:35:03.500
about the default output unit,
which is going to allow you to ignore

00:35:03.500 --> 00:35:05.980
everything I just told you about.

00:35:11.820 --> 00:35:15.390
Thanks, Jeff.

00:35:15.460 --> 00:35:18.380
You won't have to ignore everything
about it because there's some parts

00:35:18.380 --> 00:35:22.360
of the default output unit that build
on top of the HAL data structures,

00:35:22.380 --> 00:35:23.430
as we'll see.

00:35:23.540 --> 00:35:26.740
I also wanted to just correct
something Bill said earlier.

00:35:26.760 --> 00:35:30.500
The two sessions tomorrow,
the earlier one at 3:30 is the one

00:35:30.500 --> 00:35:37.060
on the audio toolbox and audio units,
and the later one at 5:00 is on MIDI.

00:35:37.070 --> 00:35:38.690
Yeah, the time's reversed.

00:35:38.710 --> 00:35:41.750
Okay, so the default output unit

00:35:41.740 --> 00:35:47.080
is a component that provides you with
a slightly simpler API to the HAL.

00:35:47.140 --> 00:35:51.840
You don't have to worry about
receiving quite as many notifications.

00:35:51.880 --> 00:35:55.690
And it builds in a sample
rate converter for you.

00:35:55.740 --> 00:35:59.010
So if you want to render data at 22K,
you don't have to care whether

00:35:59.010 --> 00:36:03.700
the hardware is at 44K or 48K.

00:36:03.700 --> 00:36:06.440
You can just render your 22K data,
say I want a sample

00:36:06.440 --> 00:36:10.560
rate converter in there,
and it'll do the conversion for you.

00:36:10.970 --> 00:36:16.400
So I'd like to just walk through a simple
example of using the default output unit.

00:36:16.400 --> 00:36:21.260
It doesn't take a whole lot to write
a program to generate sound this way.

00:36:21.440 --> 00:36:27.560
and David There's a call open default
output unit to open the component.

00:36:27.570 --> 00:36:30.670
Audio unit initialized to initialize it.

00:36:32.490 --> 00:36:37.350
Here we're going to
manipulate the HAL device.

00:36:37.470 --> 00:36:40.400
So first we want to find out what it is.

00:36:40.400 --> 00:36:46.150
We make the call to AudioUnitGetProperty,
passing the property constant,

00:36:46.220 --> 00:36:50.250
kAudioOutputUnitPropertyCurrentDevice.

00:36:50.460 --> 00:36:52.470
and David We passed the global scope.

00:36:52.480 --> 00:36:57.520
I'm not sure that's necessary,
but it's the best choice.

00:36:57.520 --> 00:37:02.170
And we'll get back the audio
device ID of the current device.

00:37:05.200 --> 00:37:09.810
Here in this example,
we're going to set our client's

00:37:09.810 --> 00:37:14.450
buffer size to 64 frames,
which is only 1.5 milliseconds.

00:37:14.560 --> 00:37:18.100
We're going to render some audio
in a very responsive manner.

00:37:18.100 --> 00:37:21.590
So we're calculating the
buffer size in bytes,

00:37:21.600 --> 00:37:28.220
64 frames times the number of channels,
which is probably two, but could be more,

00:37:28.360 --> 00:37:31.560
depending on how -- we would have
interrogated the stream format of

00:37:31.560 --> 00:37:33.090
the device to find out for sure.

00:37:33.100 --> 00:37:39.100
In any case, it's 64 times the number of
channels times the size of a float.

00:37:39.280 --> 00:37:43.510
And then we're making a call to
the how audio device set property

00:37:43.510 --> 00:37:48.000
to change the buffer size at
which this client will get called,

00:37:48.110 --> 00:37:53.350
or rather the interval at which this
client will get called to render data.

00:37:57.250 --> 00:38:00.220
Having done that,
the next thing we'll do is set

00:38:00.220 --> 00:38:02.190
up a sample rate conversion.

00:38:02.270 --> 00:38:08.760
In this example,
I want to render data at 22,050 hertz.

00:38:08.760 --> 00:38:12.820
I don't care what the
hardware sample rate is.

00:38:12.820 --> 00:38:17.600
So there's a property on the audio unit,
the default output unit,

00:38:17.600 --> 00:38:21.290
called kAudioUnit property sample rate.

00:38:21.290 --> 00:38:26.370
Now, here the scope is important
because this audio unit,

00:38:26.380 --> 00:38:29.520
if you ask it for its sample
rate in the global scope,

00:38:29.520 --> 00:38:32.970
it will give you back
the hardware sample rate.

00:38:33.090 --> 00:38:39.120
Whereas if you want to manipulate or
interrogate the input sample rate,

00:38:39.120 --> 00:38:44.230
as you would -- in this example,
you're saying I want to render at 22k.

00:38:44.250 --> 00:38:48.400
So we're telling the audio unit, yes,
my input is at 22k.

00:38:48.400 --> 00:38:53.450
And so we're passing a g sample rate.

00:38:56.790 --> 00:39:00.460
And the next step is to
provide a callback function

00:39:00.460 --> 00:39:03.890
to the audio unit to tell it,
here's a function in which

00:39:03.890 --> 00:39:05.310
I want to do my rendering.

00:39:05.420 --> 00:39:08.260
And this is kind of a simpler
version of the HALS I/O proc,

00:39:08.330 --> 00:39:10.340
as we'll see in a moment.

00:39:10.340 --> 00:39:17.290
We fill out an audio unit input callback
structure with a pointer to a function.

00:39:17.390 --> 00:39:21.190
The refcon here is null,
but it can be any piece of data that

00:39:21.200 --> 00:39:24.730
you want to access in your renderer.

00:39:24.880 --> 00:39:30.490
and David Then we make another call to
AudioUnit set property with the constant

00:39:30.540 --> 00:39:35.400
KAudioUnit property set input callback,
also in the global scope.

00:39:35.400 --> 00:39:38.320
And so that's telling the AudioUnit,
whenever you want to render data,

00:39:38.430 --> 00:39:40.910
call me back at this function.

00:39:43.450 --> 00:39:47.840
and here's an example of what a
rendering function looks like.

00:39:47.840 --> 00:39:50.310
Like I said a moment ago,
it's pretty similar to the

00:39:50.310 --> 00:39:53.240
Hal rendering function.

00:39:53.240 --> 00:39:59.330
The first parameter you
get back is your RefCon.

00:39:59.330 --> 00:39:59.330
You get some flags.

00:39:59.730 --> 00:40:02.070
and David Well,
we don't need to go into those.

00:40:02.140 --> 00:40:04.300
They're usually not used.

00:40:04.300 --> 00:40:07.790
You get the time stamp for output.

00:40:08.020 --> 00:40:11.340
You're getting a bus number,
which is relevant when there

00:40:11.340 --> 00:40:14.640
are multiple streams of audio.

00:40:14.770 --> 00:40:19.110
And you're getting an audio buffer,
which is one of the buffers that

00:40:19.110 --> 00:40:21.910
the HAL provided in its callback.

00:40:22.210 --> 00:40:26.600
So you can look at that
I/O data parameter,

00:40:26.600 --> 00:40:30.400
which is an audio buffer,
and find out how many channels are in it,

00:40:30.430 --> 00:40:34.150
what the buffer size is in bytes,
and from that we can compute

00:40:34.150 --> 00:40:35.600
the number of frames.

00:40:35.720 --> 00:40:40.100
We can set up a local pointer,
sample pointer, as a float32,

00:40:40.240 --> 00:40:44.790
and treat that as an array into
which we can fill our sample data.

00:40:45.310 --> 00:40:50.570
That's pretty much all there is to
rendering to the default output unit.

00:40:50.950 --> 00:40:54.830
After having gotten all that prepared,
we can just make a call to

00:40:54.830 --> 00:40:57.800
Audio Output Unit Start.

00:40:57.800 --> 00:40:59.000
The HAL will get fired up.

00:40:59.000 --> 00:41:00.200
It will start running.

00:41:00.200 --> 00:41:02.960
Our rendering proc will
start getting called back.

00:41:02.960 --> 00:41:08.640
We can synthesize sine waves or
horrible noises or whatever we want.

00:41:08.640 --> 00:41:10.940
And when we're done,
we can call Audio Unit Stop --

00:41:11.080 --> 00:41:13.960
Audio Output Unit Stop,
rather.

00:41:13.960 --> 00:41:17.480
And when we're completely done,
we can call Close Component

00:41:17.530 --> 00:41:18.610
to get rid of it.

00:41:18.900 --> 00:41:22.300
That's the default output unit.

00:41:22.300 --> 00:41:26.380
It's a pretty simple and
high-level interface to the HAL.

00:41:27.700 --> 00:41:31.220
So this demo, I believe, is Jeff's.

00:41:31.220 --> 00:41:32.420
I'll turn it back over to you.

00:41:32.420 --> 00:41:33.770
Thanks.

00:41:40.500 --> 00:41:48.260
So what we're going to do now is,
for the very first time I think,

00:41:48.270 --> 00:41:52.620
we're going to demonstrate on an
Apple operating system built-in system

00:41:52.620 --> 00:41:56.500
services for multi-channel sound output.

00:41:56.500 --> 00:42:00.490
So I'm just going to go ahead
and play it and then tell you

00:42:00.490 --> 00:42:02.960
a little bit more afterwards.

00:42:06.400 --> 00:42:26.500
[Transcript missing]

00:43:19.800 --> 00:43:22.240
The first thing I want to do is kind
of show you what we're using here.

00:43:22.240 --> 00:43:25.360
This is a stock
Wall Street PowerBook running

00:43:25.370 --> 00:43:29.360
OS X with a USB PC card attached to it.

00:43:29.360 --> 00:43:33.230
This is the eMagic EMI26.

00:43:36.080 --> 00:43:39.520
Thank you, eMagic.

00:43:39.850 --> 00:43:42.560
And this is on sale today.

00:43:42.650 --> 00:43:48.150
And it's using our stock
built-in USB audio class driver.

00:43:48.240 --> 00:43:49.740
This is all built into the system.

00:43:49.740 --> 00:43:51.930
It's great.

00:43:52.230 --> 00:43:55.170
A couple other things I want to show
you about this little test app here.

00:43:55.610 --> 00:44:01.880
This little test app is in the
/developer/examples/core-audio directory.

00:44:01.880 --> 00:44:04.980
It was written by one
of our other engineers,

00:44:05.070 --> 00:44:10.740
Bob Aaron, to kind of demonstrate what's
going on in the how and kind

00:44:10.780 --> 00:44:13.010
of where everything's at.

00:44:13.020 --> 00:44:17.360
It kind of gives you a user interface
for all the different little properties

00:44:17.400 --> 00:44:20.070
you'll see in the header files.

00:44:20.140 --> 00:44:22.260
In particular,
you'll see it shows the buffer

00:44:22.260 --> 00:44:25.120
size in bytes and in frames.

00:44:25.120 --> 00:44:28.960
It shows you all the format information
that you might have for the device.

00:44:28.960 --> 00:44:32.400
It allows you to change the format.

00:44:32.500 --> 00:44:37.930
And then it gives you individual
controls over the hardware for as long

00:44:37.930 --> 00:44:39.600
as the hardware has it implemented.

00:44:39.650 --> 00:44:43.920
You notice in this case that
this device only has a master

00:44:44.310 --> 00:44:46.440
output volume and a master mute.

00:44:46.600 --> 00:44:47.780
So you can see it now.

00:44:47.780 --> 00:44:54.120
I'll switch to the
built-in output device.

00:44:54.320 --> 00:44:58.640
And you can see it has
two left/right control,

00:44:58.640 --> 00:45:01.660
individual channel controls,
and no master.

00:45:01.680 --> 00:45:05.420
But it does still have a master mute.

00:45:05.440 --> 00:45:09.460
The how does not abstract
this away from you.

00:45:09.540 --> 00:45:12.870
If you're looking for a master control,
it may not be there.

00:45:12.930 --> 00:45:16.580
You need to be prepared for that.

00:45:17.900 --> 00:45:22.490
The how is not going to hide what's going
on in the hardware from you too much.

00:45:22.530 --> 00:45:24.440
Obviously,
it's going to abstract a little

00:45:24.450 --> 00:45:28.290
bit of stuff away from you,
obviously the sample

00:45:28.290 --> 00:45:30.720
format being one of them.

00:45:32.000 --> 00:45:35.980
You can see that there's
a whole lot in here.

00:45:35.980 --> 00:45:41.230
And I encourage you to go and build this
code example yourself and play with it.

00:45:41.290 --> 00:45:44.530
And those of you bringing up hardware,
this is a great test tool to figure

00:45:44.530 --> 00:45:47.450
out if you're doing everything right.

00:45:47.550 --> 00:45:50.860
So I think I'll play that again,
kind of give you-- the track

00:45:50.860 --> 00:45:54.300
I'm playing here is a 5.1 mix.

00:45:54.380 --> 00:46:00.820
It's a six-channel AIFF file
that I authored last week.

00:46:01.270 --> 00:46:04.320
So I think I'm just going to
go ahead and play it again.

00:46:04.390 --> 00:46:06.310
Turn the volume down.

00:46:17.100 --> 00:46:27.900
[Transcript missing]

00:47:37.700 --> 00:47:41.880
The app is called Daisy and
you have the source code.

00:47:41.880 --> 00:47:45.680
It's already installed in your system
if you install the developer tools.

00:47:45.700 --> 00:47:50.700
I think that's pretty much it.

00:47:50.700 --> 00:47:52.690
Can we go back to the slides please?

00:47:52.710 --> 00:48:01.530
Next, Bill is going to come up and
tell you more about Java.

00:48:08.630 --> 00:48:12.040
So we'll be doing some
Q&A and stuff in a minute,

00:48:12.040 --> 00:48:14.930
so don't all rush off unless you
want to get to the beer bust,

00:48:15.070 --> 00:48:17.200
but we won't keep you too much longer.

00:48:17.450 --> 00:48:20.310
I hate to keep a man from his beer.

00:48:20.600 --> 00:48:23.650
So I don't actually have any
examples today that I want to show

00:48:23.650 --> 00:48:25.460
you about the Cordia Java stuff.

00:48:25.560 --> 00:48:28.590
I do have some stuff tomorrow
that I'll go through.

00:48:29.400 --> 00:48:35.300
The audio hardware and the audio
device stuff is in core audio Java.

00:48:35.300 --> 00:48:39.000
It's in actually a
com.apple.audio.hardware package.

00:48:39.000 --> 00:48:42.880
You'll see the same kinds of
APIs that Jeff has talk about,

00:48:42.970 --> 00:48:43.650
start, stop.

00:48:43.760 --> 00:48:48.020
You've got audio device,
IOProc is a Java interface.

00:48:48.060 --> 00:48:54.000
You implement that interface and
then you instantiate that interface

00:48:54.140 --> 00:49:01.020
as your call back in order to render
data for the audio device IOProc.

00:49:01.240 --> 00:49:03.710
The default output audio
unit is there as well.

00:49:03.880 --> 00:49:06.480
One thing I'd like to add to
those comments is that that will,

00:49:06.480 --> 00:49:11.170
the reason the default output unit
is there is to track choices that

00:49:11.170 --> 00:49:15.130
the user might make in the system
control panel when the system control

00:49:15.130 --> 00:49:18.140
panel will let you make choices,
which it currently doesn't.

00:49:18.140 --> 00:49:22.750
So you might imagine that you could
have a user that might have two or

00:49:22.750 --> 00:49:26.520
three different audio devices on their
system and they would like to say,

00:49:26.520 --> 00:49:28.700
well, my, you know,
default output device,

00:49:28.710 --> 00:49:32.260
will be the USB guy,
it will be built in audio.

00:49:32.260 --> 00:49:36.490
And the default output unit will
listen for those things and it

00:49:36.500 --> 00:49:37.960
will switch automatically for you.

00:49:37.960 --> 00:49:40.910
So your application,
if you're just doing basic audio stuff,

00:49:40.920 --> 00:49:44.630
this becomes a very useful service
at the bottom and it's the same

00:49:44.630 --> 00:49:47.670
kind of service that the sound
manager itself sits on top of.

00:49:47.850 --> 00:49:51.720
The default output unit is
actually an extension of something

00:49:51.780 --> 00:49:53.120
we call a HAL output unit.

00:49:53.120 --> 00:49:56.930
So if you're writing audio
units or you're using audio

00:49:56.930 --> 00:50:00.340
units to do your application,
you can use a HAL output unit

00:50:00.340 --> 00:50:03.930
just to talk to a particular
device or you can use this guy if

00:50:03.930 --> 00:50:07.940
you want to track any changes the
user might make to their system.

00:50:07.940 --> 00:50:12.440
And as Doug said,
he showed you setting the sample rate,

00:50:12.440 --> 00:50:16.800
you can also not have it do the
sample rate conversion if you want

00:50:16.800 --> 00:50:18.300
to take care of that detail yourself.

00:50:18.300 --> 00:50:21.810
It's not a requirement
that it does that for you.

00:50:22.520 --> 00:50:26.580
With the Cordia Java stuff,
there's Java doc that's available.

00:50:26.620 --> 00:50:32.950
And I'll give you some URLs at the end
of this talk for that sign of stuff.

00:50:33.170 --> 00:50:36.990
Regardless of the language that you use,
we're really not sort of language

00:50:37.020 --> 00:50:41.430
centric in the Corio group,
because I'm there I suppose

00:50:41.430 --> 00:50:43.940
more than anything else,
otherwise it would all be C++.

00:50:44.000 --> 00:50:48.950
So we really kind of stress that you
should understand the architecture,

00:50:49.020 --> 00:50:52.870
whether you're going to approach
dealing with the API through

00:50:52.870 --> 00:50:56.000
C or C++ or through Java,
you really need to understand

00:50:56.000 --> 00:50:59.000
the architecture of how
the system is working.

00:50:59.000 --> 00:51:02.040
You need to understand the
underpinnings of the system and these

00:51:02.140 --> 00:51:05.240
are really not language constructs,
they're architectural

00:51:05.240 --> 00:51:07.000
constructs that you should do.

00:51:07.000 --> 00:51:10.240
So when we talk about an
API for the Java platform,

00:51:10.240 --> 00:51:14.000
we're not talking about
any additional capability.

00:51:14.000 --> 00:51:16.340
We haven't rewritten
this whole thing in Java,

00:51:16.340 --> 00:51:20.170
it's not going to be portable because
it's based on a native audio engine,

00:51:20.180 --> 00:51:24.320
but we're just providing
an access to it from Java.

00:51:25.130 --> 00:51:29.500
So some of the stuff that you see today,
if you go home on your

00:51:29.500 --> 00:51:32.700
Mac OS X system now and you run DAISY,
you'll see it looks a little

00:51:32.700 --> 00:51:34.580
bit different than the
version that we're running.

00:51:34.600 --> 00:51:41.950
We've done additional work since
Mac OS X was released in order to support

00:51:41.950 --> 00:51:44.500
the USB stuff that we were showing today.

00:51:44.520 --> 00:51:46.760
And we are very interested
in doing technology.

00:51:46.760 --> 00:51:49.340
Obviously we're going to release
this stuff to the broader

00:51:49.340 --> 00:51:51.100
community as soon as we can.

00:51:51.100 --> 00:51:53.810
I just don't want you to sort of
get a false expectation of going

00:51:53.810 --> 00:51:56.200
home and trying it doesn't work
and then you're going to abuse me.

00:51:56.200 --> 00:51:59.560
So you can contact us.

00:51:59.620 --> 00:52:03.160
I'll give you an email at the end
of the talk today about contacting

00:52:03.160 --> 00:52:05.600
us if you're interested in getting
technology seeding from us.

00:52:05.600 --> 00:52:09.700
And of course, you know,
we're close to releasing this

00:52:09.700 --> 00:52:11.160
to the general public as well.

00:52:11.160 --> 00:52:15.410
So another thing that we're
sort of announcing today is

00:52:15.450 --> 00:52:16.990
we have a mailing list up.

00:52:17.000 --> 00:52:21.050
Apple sponsors a website
called list.apple.com.

00:52:21.100 --> 00:52:24.540
These are not really Apple's,
Apple run lists or anything.

00:52:24.540 --> 00:52:27.250
They're really a service
to Apple developers and the

00:52:27.320 --> 00:52:29.860
Apple community that's basically there.

00:52:29.860 --> 00:52:31.290
You can get it from Apple.

00:52:31.300 --> 00:52:34.230
But these aren't sort of like,
I suppose they are in some way

00:52:34.340 --> 00:52:36.080
official Apple mailing lists.

00:52:36.080 --> 00:52:37.610
But they're not official
Apple mailing lists.

00:52:37.620 --> 00:52:38.620
They're your mailing lists.

00:52:38.710 --> 00:52:42.920
They're not censored or
monitored by people at Apple.

00:52:42.920 --> 00:52:46.490
They're really,
it's your resource that you can use.

00:52:46.500 --> 00:52:48.030
You can support each other with it.

00:52:48.040 --> 00:52:50.620
We obviously like to be
involved in these things.

00:52:51.100 --> 00:52:53.390
And we do now have a
core audio mailing list.

00:52:53.590 --> 00:52:55.780
And I'm pleased about it.

00:52:55.950 --> 00:53:00.860
And you can get there at list.apple.com
and you'll see the core audio list.

00:53:00.860 --> 00:53:03.400
And we'll field any
questions there on Java,

00:53:03.810 --> 00:53:07.980
core audio stuff, on the MIDI stuff,
on the things that we'll

00:53:07.980 --> 00:53:09.350
cover tomorrow in the talks.

00:53:09.380 --> 00:53:14.600
And also today we have a website
up that actually talks about

00:53:14.680 --> 00:53:16.420
Mac OS X audio for the first time.

00:53:16.490 --> 00:53:19.720
And it's developer.apple.com/audio.

00:53:19.780 --> 00:53:21.000
Okay.

00:53:21.310 --> 00:53:23.860
And we're also this
close to documentation.

00:53:23.860 --> 00:53:29.420
We realize it's been a very,
very frustrating experience for

00:53:29.420 --> 00:53:32.170
us and probably for you about,
well, this stuff is there but how do

00:53:32.170 --> 00:53:34.740
you know about it because we're
not telling anybody about it.

00:53:34.740 --> 00:53:36.680
It's sort of a best kept secret.

00:53:36.680 --> 00:53:40.660
But up on the audio website
there will be a PDF.

00:53:40.770 --> 00:53:44.500
It's a draft only of the core audio APIs.

00:53:44.500 --> 00:53:48.280
And we're doing more work and
we'll update that in the future.

00:53:48.280 --> 00:53:52.680
If that's not available by tomorrow,
then it will be available

00:53:52.700 --> 00:53:53.720
early next week.

00:53:53.770 --> 00:53:56.410
And it's a pretty good
sort of first pass.

00:53:56.500 --> 00:54:00.440
And we're also on the mailing list
so if you get the documentation,

00:54:00.440 --> 00:54:04.470
you've got SDK examples and
we're more than happy to answer

00:54:04.490 --> 00:54:08.740
questions and help you with
problems or listen to your feedback.

00:54:08.740 --> 00:54:12.200
So we really welcome your input.

00:54:13.800 --> 00:54:20.160
Ok, there's a sound networking
for games session tomorrow,

00:54:20.270 --> 00:54:22.720
and then they're the two
times of the sessions,

00:54:22.720 --> 00:54:25.480
and they're the right times,
not the wrong times.

00:54:25.690 --> 00:54:28.420
I'll come back to this slide probably.

00:54:28.450 --> 00:54:33.310
Ok, so if you've got any
questions on Firewire or USB,

00:54:33.310 --> 00:54:37.820
as Craig said earlier,
this is stuff that we're fairly

00:54:37.820 --> 00:54:41.230
involved in with the audio community.

00:54:41.230 --> 00:54:45.720
So, you can contact Craig at that
address and develop a seating,

00:54:45.720 --> 00:54:47.700
which I talked about earlier.

00:54:47.700 --> 00:54:51.880
You can send us an email and we will
certainly consider your request.