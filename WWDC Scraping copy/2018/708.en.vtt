WEBVTT

00:00:07.016 --> 00:00:15.500
[ Music ]

00:00:20.516 --> 00:00:26.786
[ Applause ]

00:00:27.286 --> 00:00:27.936
>> Good morning.

00:00:28.826 --> 00:00:31.276
Welcome. I'm Michael, and this

00:00:31.276 --> 00:00:33.316
session is about what's new in

00:00:33.316 --> 00:00:33.816
Core ML.

00:00:35.626 --> 00:00:37.786
So introduced a year ago, Core

00:00:37.846 --> 00:00:39.286
ML is all about making it

00:00:39.286 --> 00:00:41.346
unbelievably simple for you to

00:00:41.346 --> 00:00:42.476
integrate machine learning

00:00:42.476 --> 00:00:43.846
models into your app.

00:00:45.436 --> 00:00:46.696
It's been wonderful to see the

00:00:46.696 --> 00:00:48.286
adoption over the past year.

00:00:49.946 --> 00:00:51.276
We hope it has all of you

00:00:51.276 --> 00:00:53.316
thinking about what great new

00:00:53.316 --> 00:00:55.776
experiences you can enable if

00:00:55.776 --> 00:00:57.496
your app had the ability to do

00:00:57.496 --> 00:00:59.756
things like understand the

00:00:59.756 --> 00:01:03.826
content of images, or perhaps,

00:01:04.726 --> 00:01:05.876
analyze some text.

00:01:08.696 --> 00:01:11.266
What could you do if your app

00:01:11.266 --> 00:01:13.026
could reason about audio or

00:01:13.026 --> 00:01:16.536
music, or interpret your users'

00:01:16.536 --> 00:01:17.956
actions based on their motion

00:01:17.956 --> 00:01:21.506
activity, or even transform or

00:01:21.506 --> 00:01:22.976
generate new content for them?

00:01:24.416 --> 00:01:26.106
All of this, and much, much

00:01:26.106 --> 00:01:28.336
more, is easily within reach.

00:01:29.046 --> 00:01:30.356
And that's because this type of

00:01:30.356 --> 00:01:32.496
functionality can be encoded in

00:01:32.496 --> 00:01:33.356
a Core ML model.

00:01:35.146 --> 00:01:36.786
Now if we take a peek inside one

00:01:36.786 --> 00:01:39.126
of these, we may find a neural

00:01:39.126 --> 00:01:41.396
network, tree ensemble, or some

00:01:41.396 --> 00:01:42.426
other model architecture.

00:01:43.686 --> 00:01:44.876
They may have millions of

00:01:44.926 --> 00:01:46.876
parameters, the values of which

00:01:46.876 --> 00:01:47.976
have been learned from large

00:01:47.976 --> 00:01:48.666
amounts of data.

00:01:50.596 --> 00:01:52.686
But for you, you could focus on

00:01:52.686 --> 00:01:53.446
a single file.

00:01:54.166 --> 00:01:55.716
You can focus on the

00:01:55.716 --> 00:01:57.566
functionality it provides and

00:01:57.566 --> 00:01:59.816
the experience it enables rather

00:01:59.816 --> 00:02:00.816
than those implementation

00:02:00.816 --> 00:02:01.216
details.

00:02:04.806 --> 00:02:06.556
Adding a Core ML model to your

00:02:06.926 --> 00:02:08.106
app is simple as adding that

00:02:08.196 --> 00:02:09.666
file to your Xcode project.

00:02:11.226 --> 00:02:12.456
Xcode will give you a simple

00:02:12.456 --> 00:02:15.206
view, describe what it does in

00:02:15.236 --> 00:02:17.106
terms of the inputs it requires

00:02:17.416 --> 00:02:18.616
and the outputs it provides.

00:02:20.136 --> 00:02:21.476
Xcode will take this one step

00:02:21.476 --> 00:02:22.576
further and generate an

00:02:22.576 --> 00:02:25.186
interface for you, so that

00:02:25.186 --> 00:02:26.336
interacting with this model is

00:02:26.336 --> 00:02:29.206
just a few lines of code, one to

00:02:29.206 --> 00:02:31.916
load the model, one to make a

00:02:31.916 --> 00:02:34.526
prediction, and sometimes one to

00:02:34.526 --> 00:02:35.876
pull out the specific output you

00:02:35.876 --> 00:02:36.516
are interested in.

00:02:38.146 --> 00:02:39.206
Note that in some cases you

00:02:39.206 --> 00:02:40.116
don't even have to write back

00:02:40.146 --> 00:02:41.806
code because Core ML integrates

00:02:41.806 --> 00:02:43.296
with some of our higher-level

00:02:43.296 --> 00:02:44.966
APIs and allows you to customize

00:02:44.966 --> 00:02:46.416
their behavior if you give them

00:02:46.416 --> 00:02:47.286
a Core ML model.

00:02:47.736 --> 00:02:49.576
So with Vision, this is done

00:02:49.576 --> 00:02:51.106
through the VNCoreML Request

00:02:51.106 --> 00:02:51.496
object.

00:02:51.946 --> 00:02:53.106
And in the new Natural Language

00:02:53.106 --> 00:02:54.586
framework, you can instantiate

00:02:54.686 --> 00:02:56.536
an MLModel from a CoreML model.

00:02:57.246 --> 00:03:00.786
So that's Core ML in a nutshell.

00:03:01.606 --> 00:03:02.576
But we are here to talk about

00:03:02.576 --> 00:03:03.096
what's new.

00:03:04.326 --> 00:03:05.826
We took all the great feedback

00:03:06.026 --> 00:03:06.886
we received from you over the

00:03:06.886 --> 00:03:08.796
past year and focused on some

00:03:09.116 --> 00:03:11.086
key enhancements to CoreML 2.

00:03:12.586 --> 00:03:13.666
And we are going to talk about

00:03:13.666 --> 00:03:14.736
these in two sessions.

00:03:15.656 --> 00:03:16.836
In the first session, the one

00:03:16.836 --> 00:03:17.626
you are all sitting in right

00:03:17.626 --> 00:03:18.786
now, we are going to talk about

00:03:18.786 --> 00:03:20.176
what's new from the perspective

00:03:20.176 --> 00:03:21.646
of your app.

00:03:21.846 --> 00:03:23.146
In the second session, which

00:03:23.146 --> 00:03:24.716
starts immediately after this at

00:03:24.716 --> 00:03:26.736
10 a.m. after a short break, we

00:03:26.736 --> 00:03:29.046
are going to talk about tools

00:03:29.366 --> 00:03:30.786
and how you can update and

00:03:30.786 --> 00:03:32.186
convert models to take advantage

00:03:32.186 --> 00:03:33.456
of the new features in Core ML

00:03:33.496 --> 00:03:33.676
2.

00:03:36.936 --> 00:03:37.836
When it comes to your app, we

00:03:37.836 --> 00:03:39.426
are going to focus on three key

00:03:39.426 --> 00:03:39.896
areas.

00:03:40.736 --> 00:03:42.746
The first is how you can reduce

00:03:42.796 --> 00:03:44.656
the size and number of models

00:03:45.076 --> 00:03:46.246
using your app while still

00:03:46.246 --> 00:03:47.616
getting the same functionality.

00:03:48.886 --> 00:03:50.206
Then we'll look about how you

00:03:50.206 --> 00:03:51.846
can get more performance out of

00:03:51.846 --> 00:03:52.496
a single model.

00:03:52.996 --> 00:03:55.286
And then we'll conclude about

00:03:55.336 --> 00:03:57.086
how using Core ML will allow you

00:03:57.086 --> 00:03:58.196
to keep pace with the

00:03:58.196 --> 00:03:59.686
state-of-the-art and rapidly

00:03:59.686 --> 00:04:00.946
moving field of machine

00:04:00.946 --> 00:04:01.236
learning.

00:04:02.286 --> 00:04:03.466
So to kick it off, let's talk

00:04:03.466 --> 00:04:04.346
about model size.

00:04:04.486 --> 00:04:05.366
I'm going to hand it off to

00:04:05.366 --> 00:04:05.966
Francesco.

00:04:06.516 --> 00:04:09.996
[ Applause ]

00:04:10.496 --> 00:04:11.166
>> Thank you Michael.

00:04:12.876 --> 00:04:15.936
Hello. Every ways to reduce the

00:04:15.936 --> 00:04:17.476
size of your Core ML app is very

00:04:17.476 --> 00:04:18.005
important.

00:04:18.636 --> 00:04:19.935
My name is Francesco, and I am

00:04:19.935 --> 00:04:21.286
going to introduce quantization

00:04:21.375 --> 00:04:23.266
and flexible shapes, two new

00:04:23.266 --> 00:04:25.086
features in Core ML 2 that can

00:04:25.086 --> 00:04:28.306
help reduce your app size.

00:04:28.516 --> 00:04:30.326
So Core ML, [inaudible] why

00:04:30.326 --> 00:04:32.506
should you learn in models and

00:04:33.316 --> 00:04:33.486
device.

00:04:33.636 --> 00:04:34.786
This gives your app four key

00:04:34.786 --> 00:04:36.316
advantages compared to running

00:04:36.316 --> 00:04:36.966
them in the cloud.

00:04:37.766 --> 00:04:39.746
First of all, user privacy is

00:04:39.746 --> 00:04:40.476
fully respected.

00:04:40.476 --> 00:04:42.066
There are many machine-learning

00:04:42.066 --> 00:04:42.976
models on device.

00:04:43.876 --> 00:04:45.406
We guarantee that the data never

00:04:45.406 --> 00:04:46.836
leaves the device of the user.

00:04:48.016 --> 00:04:49.636
Second, it can help you achieve

00:04:49.636 --> 00:04:50.626
real-time performance.

00:04:52.046 --> 00:04:52.956
And for silicon [phonetic] and

00:04:52.956 --> 00:04:54.956
devices are super- efficient for

00:04:54.956 --> 00:04:56.136
machine-learning workloads.

00:04:57.106 --> 00:04:58.226
Furthermore, you don't have to

00:04:58.226 --> 00:04:59.796
maintain and pay for Internet

00:04:59.796 --> 00:05:00.256
servers.

00:05:01.346 --> 00:05:02.486
And Core ML inference is

00:05:02.486 --> 00:05:04.976
available anywhere at any time

00:05:05.546 --> 00:05:06.806
despite natural connectivity

00:05:06.806 --> 00:05:07.226
issues.

00:05:07.916 --> 00:05:09.256
All these great benefits come

00:05:09.256 --> 00:05:10.706
with the fact that you now need

00:05:10.706 --> 00:05:12.106
to store your machine-learning

00:05:12.106 --> 00:05:13.076
models on device.

00:05:14.046 --> 00:05:15.246
And if the machine-learning

00:05:15.246 --> 00:05:17.326
models are big, then you might

00:05:17.326 --> 00:05:18.506
be concerned about the size of

00:05:18.506 --> 00:05:18.866
your app.

00:05:19.796 --> 00:05:21.416
For example, you are -- you have

00:05:21.416 --> 00:05:22.846
your [inaudible] map, and it's

00:05:22.846 --> 00:05:23.856
full of cool features.

00:05:24.446 --> 00:05:25.846
And your users are very happy

00:05:25.846 --> 00:05:26.246
about it.

00:05:26.836 --> 00:05:28.156
And now you want to take

00:05:28.156 --> 00:05:28.996
advantage of the new

00:05:28.996 --> 00:05:31.096
opportunities offered by machine

00:05:31.096 --> 00:05:32.526
learning on device, and you want

00:05:32.526 --> 00:05:33.936
to add new amazing capabilities

00:05:33.936 --> 00:05:34.946
to your app.

00:05:34.946 --> 00:05:36.166
So what you do, you train some

00:05:36.166 --> 00:05:38.126
Core ML models and you add them

00:05:38.126 --> 00:05:39.446
to your app.

00:05:39.806 --> 00:05:41.046
What this means is that your app

00:05:41.046 --> 00:05:42.716
has become more awesome and your

00:05:42.716 --> 00:05:43.916
users are even happier.

00:05:45.176 --> 00:05:46.546
But some of them might notice

00:05:47.006 --> 00:05:48.456
that your app has increased in

00:05:48.456 --> 00:05:49.276
size a little bit.

00:05:49.966 --> 00:05:51.376
It's not uncommon to see that

00:05:51.376 --> 00:05:53.066
apps grow up, either to tens or

00:05:53.106 --> 00:05:54.366
hundreds of megabytes after

00:05:54.366 --> 00:05:55.216
adding machine-learning

00:05:55.216 --> 00:05:56.106
capabilities to them.

00:05:57.546 --> 00:05:58.926
And as you keep adding more and

00:05:58.926 --> 00:06:01.216
more features to your app, your

00:06:01.216 --> 00:06:02.806
app size might simply become out

00:06:02.806 --> 00:06:03.286
of control.

00:06:04.696 --> 00:06:05.756
So that's the first thing that

00:06:05.756 --> 00:06:06.666
you can do about it.

00:06:06.976 --> 00:06:08.736
And if these machine-learning

00:06:08.736 --> 00:06:10.076
models are supporting other

00:06:10.116 --> 00:06:11.946
features to your app, you can

00:06:11.946 --> 00:06:13.506
keep them outside your initial

00:06:13.506 --> 00:06:13.956
bundle.

00:06:13.956 --> 00:06:17.196
And then as the user uses the

00:06:17.196 --> 00:06:18.666
other features, you can download

00:06:18.666 --> 00:06:20.546
them on demand, compile them on

00:06:20.546 --> 00:06:21.036
device.

00:06:21.796 --> 00:06:23.036
So this -- in this case, this

00:06:23.036 --> 00:06:24.356
user is happy in the beginning

00:06:24.356 --> 00:06:26.166
because the installation size is

00:06:26.166 --> 00:06:26.746
unchanged.

00:06:27.246 --> 00:06:28.956
But since the user downloads and

00:06:28.956 --> 00:06:30.106
uses all the current Core ML

00:06:30.106 --> 00:06:32.156
functionality in your app, at

00:06:32.156 --> 00:06:33.406
the end of the day the size of

00:06:33.406 --> 00:06:34.786
the -- of your app is still

00:06:34.786 --> 00:06:35.046
large.

00:06:36.216 --> 00:06:38.096
So wouldn't it be better if,

00:06:38.096 --> 00:06:40.766
instead, we could tackle this

00:06:40.806 --> 00:06:43.646
problem by reducing the size of

00:06:43.646 --> 00:06:44.616
the models itself?

00:06:46.016 --> 00:06:47.496
This would give us a smaller

00:06:47.496 --> 00:06:48.886
bundle in case we ship the

00:06:48.886 --> 00:06:51.716
models inside the app, faster

00:06:51.716 --> 00:06:54.026
and smaller downloads if instead

00:06:54.026 --> 00:06:55.346
of shipping the models in the

00:06:55.346 --> 00:06:56.206
app we download them.

00:06:56.206 --> 00:06:58.936
And in any case, your app will

00:06:58.936 --> 00:07:00.406
enjoy a lower memory footprint.

00:07:00.906 --> 00:07:02.456
Using less memory is better for

00:07:02.456 --> 00:07:04.176
the performance of your app and

00:07:04.176 --> 00:07:05.446
great for the system in general.

00:07:06.286 --> 00:07:08.046
So let's see how we can

00:07:08.046 --> 00:07:09.616
decompose the size of a Core ML

00:07:09.616 --> 00:07:11.026
app into factors to better

00:07:11.026 --> 00:07:11.886
tackle this problem.

00:07:13.386 --> 00:07:14.276
First, there is the number of

00:07:14.276 --> 00:07:14.666
models.

00:07:14.906 --> 00:07:16.046
This depends on how many

00:07:16.716 --> 00:07:18.006
machine-learning functionalities

00:07:18.006 --> 00:07:18.536
your app has.

00:07:19.286 --> 00:07:20.186
Then there is the number of

00:07:20.266 --> 00:07:20.576
weights.

00:07:21.466 --> 00:07:23.156
The number of weights depends on

00:07:23.156 --> 00:07:24.166
the architecture that you have

00:07:24.166 --> 00:07:24.926
chosen to solve your

00:07:24.926 --> 00:07:25.816
machine-learning problem.

00:07:26.446 --> 00:07:29.026
As Michael was mentioning, the

00:07:29.026 --> 00:07:30.216
number of weight -- the weights

00:07:30.416 --> 00:07:31.336
are the place in which the

00:07:31.336 --> 00:07:33.236
machine-learning model stores

00:07:33.236 --> 00:07:34.796
the information that it has been

00:07:34.796 --> 00:07:35.666
learning during training.

00:07:36.336 --> 00:07:38.036
So it is -- if it has been

00:07:38.036 --> 00:07:39.826
trained to do a complex task,

00:07:39.966 --> 00:07:41.556
it's not uncommon to see a model

00:07:41.556 --> 00:07:43.366
requiring tens of millions of

00:07:43.416 --> 00:07:43.696
weights.

00:07:45.216 --> 00:07:46.736
Finally, there is the size of

00:07:46.776 --> 00:07:47.196
the weight.

00:07:47.386 --> 00:07:48.456
How are we storing these

00:07:48.886 --> 00:07:50.086
parameters that we are learning

00:07:50.086 --> 00:07:50.666
during training?

00:07:51.796 --> 00:07:52.846
Let's focus on this factor

00:07:52.846 --> 00:07:53.166
first.

00:07:54.466 --> 00:07:55.776
For neural networks, we have

00:07:55.776 --> 00:07:57.346
several options to represent and

00:07:57.346 --> 00:07:57.976
store the weights.

00:07:59.476 --> 00:08:00.966
And the first, really, is of

00:08:00.966 --> 00:08:02.606
Core ML in iOS 11.

00:08:03.486 --> 00:08:04.816
Neural networks were stored

00:08:04.816 --> 00:08:07.296
using floating-point 32-bit

00:08:07.506 --> 00:08:07.856
weights.

00:08:08.496 --> 00:08:12.466
In iOS 11.2, we heard your

00:08:12.466 --> 00:08:13.856
feedback and we introduced half

00:08:13.856 --> 00:08:16.006
precision floating-point 16

00:08:16.006 --> 00:08:16.246
weight.

00:08:16.676 --> 00:08:18.986
This gives your app half the

00:08:18.986 --> 00:08:21.126
storage required for the same

00:08:21.126 --> 00:08:21.646
accuracy.

00:08:22.236 --> 00:08:23.756
But this year we wanted to take

00:08:23.756 --> 00:08:25.616
several steps further, and we

00:08:25.616 --> 00:08:27.116
are introducing quantized

00:08:27.116 --> 00:08:27.336
weights.

00:08:28.576 --> 00:08:29.836
With quantized weights we are no

00:08:29.836 --> 00:08:31.526
longer restricted to use either

00:08:31.526 --> 00:08:33.756
Float 32 or Float 16 values.

00:08:34.155 --> 00:08:35.236
But neural networks can be

00:08:35.236 --> 00:08:37.956
encoded using 8 bits, 4 bits,

00:08:38.476 --> 00:08:40.515
any bits all the way down to 1

00:08:41.876 --> 00:08:41.976
bit.

00:08:42.186 --> 00:08:43.876
So let's now see what

00:08:43.876 --> 00:08:45.166
quantization here is.

00:08:46.336 --> 00:08:47.746
Here we are representing a

00:08:47.746 --> 00:08:49.176
subset of the weights of our

00:08:49.176 --> 00:08:49.986
neural networks.

00:08:50.546 --> 00:08:52.186
As we can see, these weights can

00:08:52.186 --> 00:08:54.166
take any value in a continuous

00:08:54.166 --> 00:08:54.486
range.

00:08:55.536 --> 00:08:57.426
This means that in theory, a

00:08:57.426 --> 00:08:58.726
single weight can take an

00:08:58.726 --> 00:09:00.086
infinite number of possible

00:09:00.086 --> 00:09:00.576
values.

00:09:01.096 --> 00:09:02.506
So in practice, in neural

00:09:02.506 --> 00:09:04.476
networks we store weights using

00:09:04.476 --> 00:09:07.186
floater 32 point -- Float 32

00:09:07.186 --> 00:09:07.586
numbers.

00:09:08.096 --> 00:09:09.256
This means that this weight can

00:09:09.256 --> 00:09:10.946
take billions of values to

00:09:10.946 --> 00:09:13.206
better represent the -- their

00:09:13.206 --> 00:09:14.036
continuous nature.

00:09:14.576 --> 00:09:16.146
But it turns out the neural

00:09:16.146 --> 00:09:18.006
networks also work with lower

00:09:18.006 --> 00:09:18.716
precision weights.

00:09:19.946 --> 00:09:21.776
Quantization is the process it

00:09:21.826 --> 00:09:23.536
takes to discontinue strings of

00:09:23.536 --> 00:09:25.566
values and constrains them to

00:09:25.566 --> 00:09:27.926
take a very small and discrete

00:09:28.016 --> 00:09:30.606
subset of possible values.

00:09:31.176 --> 00:09:32.956
For example, here quantization

00:09:32.956 --> 00:09:34.156
has turned this continuous

00:09:34.156 --> 00:09:37.156
spectrum of weights into only

00:09:37.156 --> 00:09:39.196
256 possible values.

00:09:39.536 --> 00:09:41.156
So before quantization, the

00:09:41.156 --> 00:09:42.556
weights would take any possible

00:09:42.556 --> 00:09:42.986
values.

00:09:43.316 --> 00:09:44.906
After quantization, they only

00:09:44.906 --> 00:09:46.926
have 256 options.

00:09:47.976 --> 00:09:50.326
Now since its weight can be

00:09:50.366 --> 00:09:52.796
taken from this small set, Core

00:09:52.796 --> 00:09:54.366
ML now needs only 8 bits of

00:09:54.366 --> 00:09:55.606
stored information of a weight.

00:09:56.876 --> 00:09:58.856
But nothing can stop us here.

00:09:59.296 --> 00:09:59.976
We can go further.

00:10:00.066 --> 00:10:01.966
And for example, we can

00:10:02.436 --> 00:10:04.276
constrain the network to take,

00:10:04.276 --> 00:10:06.156
instead of one of 56 different

00:10:06.156 --> 00:10:08.056
values, for example, just 8.

00:10:08.536 --> 00:10:11.246
And since now we now have only 8

00:10:11.246 --> 00:10:13.926
options, Core ML will need 3-bit

00:10:13.966 --> 00:10:15.966
values per weight to store your

00:10:15.966 --> 00:10:16.306
model.

00:10:17.556 --> 00:10:19.726
There are now some details about

00:10:19.726 --> 00:10:20.806
how we are going to choose these

00:10:20.806 --> 00:10:22.316
values to represent the weights.

00:10:22.786 --> 00:10:24.486
They can be uniformly

00:10:24.516 --> 00:10:26.256
distributed in this range, and

00:10:26.256 --> 00:10:27.576
in this case we have linear

00:10:27.576 --> 00:10:30.276
quantization instead in lookup

00:10:30.356 --> 00:10:33.856
table quantization, we can have

00:10:33.906 --> 00:10:35.356
these values scattered in this

00:10:35.356 --> 00:10:37.146
range in an arbitrary manner.

00:10:37.766 --> 00:10:39.246
So let's see practically how

00:10:39.246 --> 00:10:40.906
quantization can help us reduce

00:10:41.066 --> 00:10:41.826
the size of our model.

00:10:41.826 --> 00:10:43.256
In this example, you are

00:10:43.256 --> 00:10:45.506
focusing on Resnet50, which is a

00:10:45.506 --> 00:10:47.186
common architecture used by many

00:10:47.186 --> 00:10:48.526
applications for many different

00:10:48.526 --> 00:10:48.846
tasks.

00:10:50.056 --> 00:10:52.106
It includes 25 million trained

00:10:52.106 --> 00:10:54.426
parameters and this means that

00:10:54.426 --> 00:10:56.186
you have to use 32-bit floats to

00:10:56.186 --> 00:10:56.916
represent it.

00:10:57.946 --> 00:10:59.386
Then the total model size is

00:10:59.386 --> 00:11:00.616
more than 100 megabytes.

00:11:01.196 --> 00:11:03.856
If we quantize it to 8-bits,

00:11:04.396 --> 00:11:05.616
then the architecture hasn't

00:11:05.616 --> 00:11:07.476
changed; we still have 25

00:11:07.476 --> 00:11:08.886
million parameters.

00:11:09.616 --> 00:11:11.606
But we are now using only 1 byte

00:11:12.686 --> 00:11:14.236
to store a single weight, and

00:11:14.236 --> 00:11:15.396
this means that the model size

00:11:15.396 --> 00:11:16.856
is reduced by a factor of 4x.

00:11:17.326 --> 00:11:18.686
It's only -- it now only takes

00:11:18.686 --> 00:11:20.066
26 megabytes to store this

00:11:20.066 --> 00:11:20.366
model.

00:11:20.906 --> 00:11:22.046
And we can go further.

00:11:22.426 --> 00:11:23.386
We can use that quantized

00:11:23.386 --> 00:11:24.926
representation that only uses 4

00:11:24.926 --> 00:11:26.716
bits per weight in this model

00:11:27.286 --> 00:11:28.586
and end up with a model that is

00:11:28.586 --> 00:11:28.976
even smaller.

00:11:29.516 --> 00:11:36.016
[ Applause ]

00:11:36.516 --> 00:11:38.986
And again, Core ML supports all

00:11:38.986 --> 00:11:40.626
the quantization modes all the

00:11:40.626 --> 00:11:43.876
way down to 8 bits.

00:11:44.296 --> 00:11:47.096
Now quantization is a powerful

00:11:47.096 --> 00:11:48.886
technique to take an existing

00:11:48.886 --> 00:11:50.286
architecture and of a smaller

00:11:50.286 --> 00:11:50.896
version of it.

00:11:51.396 --> 00:11:52.616
But how can you obtain quantized

00:11:52.616 --> 00:11:52.896
model?

00:11:54.516 --> 00:11:56.486
If you have any neural

00:11:56.486 --> 00:11:58.306
networking in Core ML format,

00:11:58.306 --> 00:11:59.626
you can use Core ML Tools to

00:11:59.626 --> 00:12:00.586
obtain a quantized

00:12:00.586 --> 00:12:01.476
representation of it.

00:12:01.716 --> 00:12:03.496
So Core ML 2 should quantize for

00:12:03.496 --> 00:12:04.216
you automatically.

00:12:05.516 --> 00:12:07.386
Or you can train quantized

00:12:07.386 --> 00:12:07.756
models.

00:12:09.316 --> 00:12:10.596
You can either train quantized

00:12:10.596 --> 00:12:11.416
-- with a quantization

00:12:11.416 --> 00:12:12.866
constraint from scratch, or

00:12:12.866 --> 00:12:14.256
retrain existing models with

00:12:14.256 --> 00:12:15.406
quantization constraints.

00:12:16.376 --> 00:12:17.426
After you have obtained your

00:12:17.426 --> 00:12:18.336
quantized model with your

00:12:18.336 --> 00:12:19.556
training tools, you can then

00:12:19.556 --> 00:12:21.616
convert it to Core ML as usual.

00:12:22.156 --> 00:12:23.466
And nothing will change in the

00:12:23.466 --> 00:12:24.596
app in the way you use the

00:12:24.596 --> 00:12:24.976
model.

00:12:25.626 --> 00:12:27.466
Inside the model, the numbers

00:12:27.466 --> 00:12:28.716
are going to be stored in

00:12:28.716 --> 00:12:29.826
different precision, but the

00:12:29.826 --> 00:12:31.816
interface for using the model

00:12:32.176 --> 00:12:32.976
will not change at all.

00:12:35.216 --> 00:12:36.856
However, we always have to

00:12:36.856 --> 00:12:38.426
consider that quantized models

00:12:38.986 --> 00:12:40.016
have lower-precision

00:12:40.016 --> 00:12:41.586
approximations of the original

00:12:41.586 --> 00:12:43.446
reference floating-point models.

00:12:44.256 --> 00:12:45.686
And this means that quantized

00:12:45.686 --> 00:12:47.166
models come with an accuracy

00:12:47.166 --> 00:12:48.316
versus size-of-the-model

00:12:48.316 --> 00:12:48.836
tradeoff.

00:12:49.736 --> 00:12:51.436
This tradeoff is model dependent

00:12:51.546 --> 00:12:53.096
and use case dependent.

00:12:53.096 --> 00:12:55.166
And it's also a very active area

00:12:55.166 --> 00:12:55.706
of research.

00:12:56.456 --> 00:12:57.846
So it's always recommended to

00:12:57.936 --> 00:12:58.846
check the accuracy of the

00:12:58.846 --> 00:13:00.786
quantized model and compare it

00:13:00.786 --> 00:13:02.396
with the referenced

00:13:02.396 --> 00:13:04.186
floating-point version for

00:13:04.186 --> 00:13:05.586
relevant this data and form

00:13:05.586 --> 00:13:07.076
metrics that are valid for your

00:13:07.076 --> 00:13:07.876
app and use case.

00:13:08.966 --> 00:13:10.786
Now let's see a demo of how we

00:13:10.786 --> 00:13:13.586
can use -- adopt quantized

00:13:13.586 --> 00:13:14.796
models to reduce the size of an

00:13:14.796 --> 00:13:14.936
app.

00:13:15.516 --> 00:13:18.500
[ Applause ]

00:13:25.236 --> 00:13:26.556
I would like to show you a style

00:13:26.556 --> 00:13:27.566
transfer app.

00:13:28.116 --> 00:13:29.416
In style transfer, a neural

00:13:29.416 --> 00:13:31.356
network has been trained to

00:13:31.356 --> 00:13:33.456
render user images using styles

00:13:33.456 --> 00:13:34.326
that have been learned by

00:13:34.326 --> 00:13:35.646
watching paintings or other

00:13:35.646 --> 00:13:36.086
images.

00:13:36.696 --> 00:13:37.616
So let me load my app.

00:13:37.616 --> 00:13:41.216
As we can see, I am shipping

00:13:41.216 --> 00:13:43.226
this app with four styles; City,

00:13:43.226 --> 00:13:45.066
Glass, Oils and Waves.

00:13:45.656 --> 00:13:47.606
And then I can pick images from

00:13:47.606 --> 00:13:49.256
the photo library of the users

00:13:49.256 --> 00:13:51.096
and then process them blending

00:13:51.096 --> 00:13:52.736
them in different styles right

00:13:52.736 --> 00:13:53.226
on device.

00:13:53.976 --> 00:13:55.406
So this is the original image,

00:13:56.136 --> 00:13:57.076
and I am going to render the

00:13:57.076 --> 00:13:57.716
City style,

00:13:59.636 --> 00:13:59.976
Glass,

00:14:02.336 --> 00:14:02.706
Oils,

00:14:04.506 --> 00:14:04.956
and Waves.

00:14:07.076 --> 00:14:08.746
Let's see how this app has been

00:14:08.746 --> 00:14:09.516
built in Xcode.

00:14:10.966 --> 00:14:13.296
This app uses Core ML and Vision

00:14:13.396 --> 00:14:15.406
API to perform this stylization.

00:14:16.226 --> 00:14:17.786
And as we can see, we have four

00:14:17.786 --> 00:14:19.586
Core ML models here bundled in

00:14:19.586 --> 00:14:21.436
Xcode; City, Glass, Oils, and

00:14:21.436 --> 00:14:22.706
Waves, the same ones we are

00:14:22.706 --> 00:14:23.406
seeing in the app.

00:14:23.406 --> 00:14:25.796
And we can see -- we can inspect

00:14:25.796 --> 00:14:26.336
this model.

00:14:26.766 --> 00:14:27.976
These are seen as quantized

00:14:27.976 --> 00:14:29.166
model, so each one of these

00:14:29.166 --> 00:14:31.246
models is 6.7 megabytes of this

00:14:31.246 --> 00:14:31.876
space on disk.

00:14:33.096 --> 00:14:34.266
We see that the models take an

00:14:34.266 --> 00:14:35.226
input image of a certain

00:14:35.226 --> 00:14:37.706
resolution and produce an image

00:14:37.706 --> 00:14:39.176
called Stylized of the same

00:14:39.176 --> 00:14:39.716
resolution.

00:14:41.016 --> 00:14:43.106
Now we want to investigate how

00:14:43.106 --> 00:14:44.736
much storage space and memory

00:14:44.736 --> 00:14:46.366
space we can use by -- we can

00:14:46.366 --> 00:14:47.796
save by switching to quantized,

00:14:47.796 --> 00:14:48.096
models.

00:14:48.096 --> 00:14:49.716
So I have been playing with Core

00:14:49.716 --> 00:14:52.656
ML Tools and obtained quantizer

00:14:52.656 --> 00:14:55.236
presentation for these models.

00:14:56.216 --> 00:14:57.486
And for a tutorial about how to

00:14:57.486 --> 00:14:59.446
obtain these models, stay for

00:14:59.446 --> 00:15:01.056
Part 2 that is going to cover

00:15:01.056 --> 00:15:02.766
quantization with Core ML Tools

00:15:02.766 --> 00:15:02.976
in detail.

00:15:04.126 --> 00:15:05.396
So I want to focus first on the

00:15:05.396 --> 00:15:07.856
Glass style and see how the

00:15:07.856 --> 00:15:09.416
different quantization versions

00:15:09.696 --> 00:15:10.746
work for these styles.

00:15:11.836 --> 00:15:13.646
So all I have to do is drag

00:15:13.646 --> 00:15:15.076
these new models inside the

00:15:15.076 --> 00:15:17.756
Xcode project, and rerun the

00:15:17.876 --> 00:15:17.943
app.

00:15:18.046 --> 00:15:19.976
And then we are going to see how

00:15:19.976 --> 00:15:20.806
these models behave.

00:15:21.296 --> 00:15:23.986
First we can see that the size

00:15:23.986 --> 00:15:25.856
has been greatly reduced.

00:15:25.856 --> 00:15:27.906
For example, the 8-bit version

00:15:27.906 --> 00:15:29.906
already from 6 or 7 megabytes

00:15:29.906 --> 00:15:30.976
went down to just 1.7.

00:15:31.516 --> 00:15:36.576
[ Applause ]

00:15:37.076 --> 00:15:39.226
In 4-bit, we can save even more,

00:15:39.226 --> 00:15:40.876
and now the model is less than 1

00:15:40.876 --> 00:15:41.336
megabyte.

00:15:41.936 --> 00:15:43.286
In 3-bit, it is -- that's even

00:15:43.286 --> 00:15:44.956
smaller, at 49 kilobytes.

00:15:44.956 --> 00:15:45.946
And so on.

00:15:47.106 --> 00:15:49.326
Now let's go back to the app.

00:15:50.826 --> 00:15:52.616
Let's make this same image for

00:15:52.616 --> 00:15:54.486
reference and apply the Glass

00:15:54.536 --> 00:15:56.276
style in the original version.

00:15:57.136 --> 00:15:58.266
Still looks as before.

00:15:59.016 --> 00:16:00.676
Now we can compare it with the

00:16:00.676 --> 00:16:01.716
8-bit version.

00:16:02.036 --> 00:16:05.796
And you can see nothing has

00:16:05.866 --> 00:16:06.186
changed.

00:16:06.796 --> 00:16:07.806
This is because 8-bit

00:16:07.806 --> 00:16:09.156
quantization methods are very

00:16:09.156 --> 00:16:09.536
solid.

00:16:10.806 --> 00:16:13.176
We can also venture further and

00:16:13.176 --> 00:16:14.956
try the 4-bit version of this

00:16:14.956 --> 00:16:15.316
model.

00:16:15.866 --> 00:16:18.256
Wow. The results are still

00:16:18.256 --> 00:16:18.626
great.

00:16:19.896 --> 00:16:21.366
And now let's try the 3-bit

00:16:21.366 --> 00:16:21.836
version.

00:16:24.556 --> 00:16:26.386
We see that there are -- we see

00:16:26.386 --> 00:16:27.506
the first color shift.

00:16:27.506 --> 00:16:29.116
So it probably is good if we go

00:16:29.116 --> 00:16:30.426
and check with the designers if

00:16:30.426 --> 00:16:32.446
this effect is still acceptable.

00:16:33.196 --> 00:16:34.596
And now, as we see the 2-bit

00:16:34.596 --> 00:16:37.256
version, this is not really what

00:16:37.256 --> 00:16:37.926
we were looking for.

00:16:37.926 --> 00:16:39.056
Maybe we will save it for a

00:16:39.056 --> 00:16:40.376
horror app, but I am not going

00:16:40.376 --> 00:16:40.976
to show this to the designer.

00:16:41.516 --> 00:16:46.026
[ Applause ]

00:16:46.526 --> 00:16:47.526
Let's go back to the 4-bit

00:16:47.566 --> 00:16:48.626
version and hide this one.

00:16:49.196 --> 00:16:51.056
This was just a reminder that

00:16:51.056 --> 00:16:52.686
quantized models are

00:16:52.686 --> 00:16:54.156
approximation of the original

00:16:54.156 --> 00:16:54.356
models.

00:16:55.216 --> 00:16:56.366
So it's always recommended to

00:16:56.416 --> 00:16:57.866
check them and compare with the

00:16:57.866 --> 00:16:58.696
original versions.

00:16:59.106 --> 00:17:00.716
Now for every model and

00:17:00.716 --> 00:17:02.156
quantization technique, there is

00:17:02.156 --> 00:17:03.356
always a point in which things

00:17:03.356 --> 00:17:06.526
start to mismatch.

00:17:06.526 --> 00:17:08.376
Now we -- after some discussion

00:17:08.376 --> 00:17:09.526
with the designer, extensive

00:17:09.526 --> 00:17:10.846
evaluation of many images, we

00:17:10.846 --> 00:17:12.156
decided to ship the 4-bit

00:17:12.156 --> 00:17:13.415
version of this model, which is

00:17:13.715 --> 00:17:15.086
the smallest size for the best

00:17:15.086 --> 00:17:15.506
quality.

00:17:16.955 --> 00:17:17.886
So let's remove all the

00:17:17.886 --> 00:17:19.566
floating-point version of the

00:17:19.566 --> 00:17:21.445
models that were taking a lot of

00:17:21.445 --> 00:17:23.996
space in our app and replace

00:17:23.996 --> 00:17:25.976
them with the 4-bit version.

00:17:30.736 --> 00:17:32.516
And now let's run the app one

00:17:32.516 --> 00:17:32.956
last time.

00:17:40.226 --> 00:17:41.846
OK. Let's pick the same image

00:17:41.846 --> 00:17:48.886
again and show all the styles.

00:17:48.886 --> 00:17:54.186
This was the City, Glass, Oils,

00:17:54.926 --> 00:17:56.996
and big Wave.

00:17:58.306 --> 00:18:02.076
So in this demo we saw how we

00:18:02.076 --> 00:18:04.336
started with four models and

00:18:04.336 --> 00:18:06.606
they were huge, in 32-bit -- or

00:18:06.606 --> 00:18:08.716
total app size was 27 megabytes.

00:18:09.426 --> 00:18:10.966
Then we evaluated the quality

00:18:10.966 --> 00:18:12.466
and switched to 4-bit models,

00:18:13.026 --> 00:18:14.406
and the total size of our app

00:18:14.406 --> 00:18:16.526
went down to just 3.4 megabytes.

00:18:16.806 --> 00:18:16.873
Now --

00:18:17.516 --> 00:18:22.646
[ Applause ]

00:18:23.146 --> 00:18:24.646
This doesn't cost us anything in

00:18:24.646 --> 00:18:26.756
terms of quality because all

00:18:26.756 --> 00:18:27.646
these versions -- these

00:18:27.646 --> 00:18:29.456
quantized versions look the

00:18:29.456 --> 00:18:31.626
same, and the quality is still

00:18:31.626 --> 00:18:32.076
amazing.

00:18:32.616 --> 00:18:36.006
We showed how quantization can

00:18:36.006 --> 00:18:37.446
help us reduce the size of an

00:18:37.446 --> 00:18:39.166
app by reducing the size of the

00:18:39.166 --> 00:18:40.796
weight at the very microscopic

00:18:40.796 --> 00:18:41.046
level.

00:18:41.046 --> 00:18:44.356
Now let's see how we can reduce

00:18:44.356 --> 00:18:45.546
the number of models that your

00:18:45.546 --> 00:18:45.976
app needs.

00:18:46.666 --> 00:18:48.866
In the most straightforward

00:18:48.866 --> 00:18:51.446
case, if your app has three

00:18:51.446 --> 00:18:52.936
machine-learning functionalities

00:18:53.306 --> 00:18:54.496
then you need three different

00:18:54.496 --> 00:18:55.536
machine-learning models.

00:18:56.156 --> 00:18:58.206
But in some cases, it is

00:18:58.206 --> 00:19:01.206
possible to have the same model

00:19:01.426 --> 00:19:03.116
to support two different

00:19:03.116 --> 00:19:03.676
functions.

00:19:04.406 --> 00:19:06.006
For example, you can train a

00:19:06.006 --> 00:19:06.906
multi-task model.

00:19:06.906 --> 00:19:09.316
And multi-task models has been

00:19:09.316 --> 00:19:10.936
trained to perform multiple

00:19:10.936 --> 00:19:11.626
things at once.

00:19:12.446 --> 00:19:13.656
There is an example about style

00:19:13.656 --> 00:19:14.776
transferring, the Turi Create

00:19:14.836 --> 00:19:16.276
session about multi-task models.

00:19:16.936 --> 00:19:19.366
Or in some cases, you can use a

00:19:19.366 --> 00:19:20.856
yet new feature in Core ML

00:19:21.146 --> 00:19:22.596
called Flexible Shapes and

00:19:22.596 --> 00:19:23.076
Sizes.

00:19:24.386 --> 00:19:27.006
Let's go back to our Style

00:19:27.006 --> 00:19:27.726
Transfer demo.

00:19:28.166 --> 00:19:30.516
In Xcode we saw that the size of

00:19:30.516 --> 00:19:31.916
the input image and the output

00:19:31.916 --> 00:19:34.826
image was encoded in part of the

00:19:34.826 --> 00:19:35.966
definition of the model.

00:19:36.816 --> 00:19:38.066
But what if we want to run the

00:19:38.066 --> 00:19:39.666
same style on different image

00:19:39.666 --> 00:19:40.346
resolution?

00:19:41.056 --> 00:19:42.576
What if we want to run the same

00:19:42.576 --> 00:19:44.746
network on different image

00:19:44.746 --> 00:19:45.226
sizes?

00:19:46.796 --> 00:19:49.156
For example, the user might want

00:19:49.156 --> 00:19:50.726
to see a high-definition style

00:19:50.726 --> 00:19:51.186
transfer.

00:19:51.826 --> 00:19:53.906
So they use -- they give us a

00:19:53.906 --> 00:19:54.906
high-definition image.

00:19:55.646 --> 00:19:57.316
Now if I were Core ML model all

00:19:57.316 --> 00:19:58.546
it takes is a lower resolution

00:19:58.546 --> 00:20:00.766
as an input, all we can do as

00:20:00.766 --> 00:20:03.436
developers is size -- or resize

00:20:03.436 --> 00:20:06.236
the image down, process it, and

00:20:06.236 --> 00:20:06.976
then scale it back up.

00:20:07.786 --> 00:20:09.346
This is not really going to

00:20:09.346 --> 00:20:10.206
amaze the user.

00:20:10.206 --> 00:20:14.336
Even in the past, we could

00:20:14.336 --> 00:20:16.146
reship this model with Corel ML

00:20:16.146 --> 00:20:18.206
Tools and make it accept any

00:20:18.206 --> 00:20:20.396
resolution, in particular, a

00:20:20.396 --> 00:20:21.826
higher-resolution image.

00:20:23.246 --> 00:20:24.756
So even in the past we could do

00:20:24.756 --> 00:20:27.096
this feature and feed directly

00:20:27.096 --> 00:20:28.716
the high-resolution image to a

00:20:28.716 --> 00:20:30.446
Corel ML model, producing a

00:20:30.586 --> 00:20:31.576
high-definition result.

00:20:31.576 --> 00:20:34.366
This is because we wanted to

00:20:34.366 --> 00:20:36.436
introduce a finer detail in the

00:20:36.436 --> 00:20:38.706
stylization and the way finer

00:20:38.706 --> 00:20:40.966
strokes that are amazing when

00:20:40.966 --> 00:20:42.406
you zoom in, because they have

00:20:42.706 --> 00:20:43.826
-- they add a lot of work into

00:20:43.826 --> 00:20:44.516
the final image.

00:20:44.516 --> 00:20:47.646
So in the past we could do it,

00:20:47.646 --> 00:20:49.656
but we could do it by

00:20:49.656 --> 00:20:51.116
duplicating the model and

00:20:51.116 --> 00:20:52.606
creating two different versions:

00:20:53.096 --> 00:20:54.576
one for the standard definition

00:20:54.576 --> 00:20:55.376
and one for the high

00:20:55.376 --> 00:20:56.046
definitions.

00:20:56.516 --> 00:20:57.796
And this, of course, means that

00:20:57.796 --> 00:20:59.726
our app is twice as much the

00:20:59.726 --> 00:21:01.356
size -- besides the fact that

00:21:01.356 --> 00:21:02.516
the network has been trained to

00:21:02.516 --> 00:21:03.736
support any resolution.

00:21:04.526 --> 00:21:05.186
Not anymore.

00:21:05.586 --> 00:21:06.866
We are introducing flexible

00:21:06.866 --> 00:21:07.246
shapes.

00:21:07.816 --> 00:21:09.036
And with flexible shapes, you

00:21:09.036 --> 00:21:10.526
have -- if you have -- you can

00:21:10.526 --> 00:21:12.286
have the single model to process

00:21:12.286 --> 00:21:13.776
more resolutions and many more

00:21:13.776 --> 00:21:14.426
resolutions.

00:21:14.806 --> 00:21:15.926
So now in Xcode --

00:21:16.516 --> 00:21:20.886
[ Applause ]

00:21:21.386 --> 00:21:23.416
-- in Xcode you are going to see

00:21:24.366 --> 00:21:26.016
that this -- the input is still

00:21:26.016 --> 00:21:27.986
an image, but the size of the

00:21:27.986 --> 00:21:29.746
full resolution, the model also

00:21:29.746 --> 00:21:31.896
accepts flexible resolutions.

00:21:32.066 --> 00:21:33.806
In this simple example, SD and

00:21:34.106 --> 00:21:34.173
HD.

00:21:34.173 --> 00:21:37.566
This means that now you have to

00:21:37.566 --> 00:21:38.786
ship a single model.

00:21:38.786 --> 00:21:41.426
You don't have to have any

00:21:41.426 --> 00:21:42.266
redundant code.

00:21:43.316 --> 00:21:44.316
And if you need to switch

00:21:44.316 --> 00:21:45.726
between standard definition and

00:21:45.726 --> 00:21:46.906
high definition, you can do it

00:21:46.906 --> 00:21:48.386
much faster because we don't

00:21:48.386 --> 00:21:49.456
need to reload the model from

00:21:49.456 --> 00:21:50.876
scratch; we just need to resize

00:21:51.496 --> 00:21:51.566
it.

00:21:52.236 --> 00:21:54.186
You have two options to specify

00:21:54.186 --> 00:21:55.716
the flexibility of the model.

00:21:57.026 --> 00:21:58.556
You can define a range for its

00:21:58.556 --> 00:22:00.416
dimension, so you can define a

00:22:00.416 --> 00:22:02.036
minimal width and height and the

00:22:02.036 --> 00:22:03.096
maximum width and height.

00:22:03.646 --> 00:22:05.356
And then at inference pick any

00:22:05.356 --> 00:22:06.136
value in between.

00:22:06.766 --> 00:22:08.276
But there is also another way.

00:22:08.576 --> 00:22:10.486
You can enumerate all the shapes

00:22:10.486 --> 00:22:11.326
that you are going to use.

00:22:11.736 --> 00:22:12.936
For example, all different

00:22:12.936 --> 00:22:14.606
aspect ratios, all different

00:22:14.606 --> 00:22:16.536
resolutions, and this is better

00:22:16.536 --> 00:22:17.306
for performance.

00:22:17.586 --> 00:22:19.086
Core ML knows more about your

00:22:19.086 --> 00:22:21.006
use case earlier, so it can --

00:22:21.006 --> 00:22:22.236
it has the opportunities of

00:22:22.236 --> 00:22:24.106
performing more optimizations.

00:22:24.926 --> 00:22:26.496
And it also gives your app a

00:22:26.496 --> 00:22:27.776
smaller tested surface.

00:22:28.286 --> 00:22:30.956
Now which models are flexible?

00:22:30.956 --> 00:22:32.996
Which models can be trained to

00:22:32.996 --> 00:22:34.526
support multiple resolutions?

00:22:35.206 --> 00:22:37.426
Fully convolutional neural

00:22:37.426 --> 00:22:39.746
networks, commonly used for MS

00:22:39.746 --> 00:22:41.636
processing tasks such as style

00:22:41.636 --> 00:22:43.746
transfer, image enhancement,

00:22:43.916 --> 00:22:45.786
super resolution, and so on --

00:22:46.016 --> 00:22:47.716
and some of the architecture.

00:22:48.446 --> 00:22:50.956
Core ML Tools can check if a

00:22:50.956 --> 00:22:52.636
model has this capability for

00:22:52.636 --> 00:22:52.786
you.

00:22:54.236 --> 00:22:55.516
So we still have the number of

00:22:55.516 --> 00:22:57.126
models Core ML uses in flexible

00:22:57.126 --> 00:22:58.606
sizes, and the size of the

00:22:58.606 --> 00:22:59.516
weights can be reduced by

00:22:59.516 --> 00:23:00.246
quantization.

00:23:00.646 --> 00:23:01.606
But what about the number of

00:23:01.686 --> 00:23:01.956
weights?

00:23:03.176 --> 00:23:05.426
Core ML, given the fact that it

00:23:05.426 --> 00:23:07.006
supports many, many different

00:23:07.006 --> 00:23:08.356
architecture at any framework,

00:23:08.956 --> 00:23:10.886
has always helped you choose the

00:23:10.886 --> 00:23:12.556
right -- the model of the right

00:23:12.606 --> 00:23:13.706
size for your machine-learning

00:23:13.706 --> 00:23:14.086
problem.

00:23:14.636 --> 00:23:17.206
So Core ML can help you tackle

00:23:17.206 --> 00:23:19.376
the size of your app using this

00:23:20.036 --> 00:23:21.106
-- all these three factors.

00:23:21.476 --> 00:23:23.046
In any case, the inference is

00:23:23.046 --> 00:23:24.246
going to be super performant.

00:23:24.246 --> 00:23:26.706
And to introduce new features in

00:23:26.706 --> 00:23:28.066
performance and customization,

00:23:28.126 --> 00:23:29.196
let's welcome Bill March.

00:23:29.746 --> 00:23:29.976
Thank you.

00:23:30.516 --> 00:23:36.776
[ Applause ]

00:23:37.276 --> 00:23:38.346
>> Thank you.

00:23:39.826 --> 00:23:41.056
One of the fundamental design

00:23:41.056 --> 00:23:42.416
principles of Core ML from the

00:23:42.416 --> 00:23:43.886
very beginning has been that it

00:23:43.886 --> 00:23:45.166
should give your app the best

00:23:45.166 --> 00:23:46.206
possible performance.

00:23:46.806 --> 00:23:48.106
And in keeping with that goal,

00:23:48.176 --> 00:23:49.256
I'd like to highlight a new

00:23:49.256 --> 00:23:50.526
feature of Core ML to help

00:23:50.526 --> 00:23:52.176
ensure that your app will shine

00:23:52.176 --> 00:23:53.376
on any Apple device.

00:23:54.286 --> 00:23:55.986
Let's take a look at the style

00:23:55.986 --> 00:23:57.426
transfer example that Francesco

00:23:57.426 --> 00:23:57.966
showed us.

00:23:58.356 --> 00:23:59.316
From the perspective of your

00:23:59.316 --> 00:24:01.186
app, it takes an image of an

00:24:01.186 --> 00:24:02.776
input and simply returns the

00:24:02.776 --> 00:24:03.806
stylized image.

00:24:04.346 --> 00:24:05.736
And there are two key components

00:24:05.736 --> 00:24:06.976
that go into making this happen:

00:24:07.416 --> 00:24:09.896
first, the MLModel file, which

00:24:09.896 --> 00:24:11.366
stores the particular parameters

00:24:11.366 --> 00:24:13.686
needed to apply this style; and

00:24:13.686 --> 00:24:15.626
second, the inference engine,

00:24:15.766 --> 00:24:17.546
which takes in the MLModel and

00:24:17.546 --> 00:24:18.746
the image and performs the

00:24:18.746 --> 00:24:20.096
calculations necessary to

00:24:20.166 --> 00:24:20.886
produce the result.

00:24:21.886 --> 00:24:23.006
So let's peek under the hood of

00:24:23.006 --> 00:24:24.286
this inference engine and see

00:24:24.286 --> 00:24:25.416
how we leverage Apple's

00:24:25.416 --> 00:24:27.146
technology to perform this style

00:24:27.146 --> 00:24:28.416
transfer efficiently.

00:24:30.156 --> 00:24:31.606
This model is an example of a

00:24:31.606 --> 00:24:33.126
neural network, which consists

00:24:33.126 --> 00:24:34.466
of a series of mathematical

00:24:34.466 --> 00:24:35.866
operations called layers.

00:24:36.406 --> 00:24:37.566
Each layer applies some

00:24:37.566 --> 00:24:39.026
transformation to the image,

00:24:39.026 --> 00:24:40.376
finally resulting in the

00:24:40.376 --> 00:24:41.566
stylized output.

00:24:41.996 --> 00:24:43.996
The model stores weights for

00:24:43.996 --> 00:24:45.506
each layer which determine the

00:24:45.556 --> 00:24:47.086
particular transformation and

00:24:47.086 --> 00:24:48.106
the style that we are going to

00:24:48.106 --> 00:24:48.496
apply.

00:24:49.576 --> 00:24:50.696
The Core ML neural network

00:24:50.696 --> 00:24:52.366
inference engine has highly

00:24:52.366 --> 00:24:53.866
optimized implementations for

00:24:53.866 --> 00:24:54.776
each of these layers.

00:24:55.176 --> 00:24:57.236
On the GPU, we use MTL shaders.

00:24:57.466 --> 00:24:58.686
On the CPU we can use

00:24:58.686 --> 00:24:59.946
Accelerate, the proficient

00:24:59.946 --> 00:25:00.676
calculation.

00:25:01.296 --> 00:25:02.626
And we can dispatch different

00:25:02.626 --> 00:25:03.836
parts of the computation to

00:25:03.836 --> 00:25:04.976
different pieces of hardware

00:25:04.976 --> 00:25:06.526
dynamically depending on the

00:25:06.526 --> 00:25:08.256
model, the device state, and

00:25:08.256 --> 00:25:08.986
other factors.

00:25:10.156 --> 00:25:12.736
We can also find opportunities

00:25:12.736 --> 00:25:14.406
to fuse layers in the network,

00:25:14.406 --> 00:25:15.836
resulting in fewer overall

00:25:15.836 --> 00:25:17.156
computations being needed.

00:25:18.206 --> 00:25:20.336
We are able to optimize here

00:25:20.336 --> 00:25:21.916
because we know what's going on.

00:25:22.206 --> 00:25:23.196
We know the details of the

00:25:23.196 --> 00:25:24.736
model; they are contained in the

00:25:24.736 --> 00:25:26.216
MLModel file that you provided

00:25:26.216 --> 00:25:26.576
to us.

00:25:27.016 --> 00:25:28.126
And we know the details of the

00:25:28.126 --> 00:25:30.036
inference engine and the device

00:25:30.176 --> 00:25:32.316
because we designed them.

00:25:32.316 --> 00:25:33.716
We can take care of all of these

00:25:33.716 --> 00:25:35.856
optimizations for you, and you

00:25:35.856 --> 00:25:37.376
can focus on delivering the best

00:25:37.376 --> 00:25:38.656
user experience in your app.

00:25:39.086 --> 00:25:41.586
But what about your workload?

00:25:42.436 --> 00:25:44.136
What about, in particular, if

00:25:44.136 --> 00:25:45.286
you need to make multiple

00:25:45.286 --> 00:25:45.986
predictions?

00:25:47.196 --> 00:25:48.406
If Core ML doesn't know about

00:25:48.406 --> 00:25:50.186
it, then Core ML can't optimize

00:25:50.186 --> 00:25:50.486
for it.

00:25:51.486 --> 00:25:53.096
So in the past, if you had a

00:25:53.096 --> 00:25:55.886
workload like this, you needed

00:25:55.886 --> 00:25:57.356
to do something like this: a

00:25:57.356 --> 00:25:58.926
simple for loop wrapped around a

00:25:58.926 --> 00:26:00.436
call to the existing Core ML

00:26:00.436 --> 00:26:01.246
prediction API.

00:26:01.246 --> 00:26:03.416
So you'd loop over some array of

00:26:03.416 --> 00:26:04.766
inputs and produce an array of

00:26:04.766 --> 00:26:05.336
outputs.

00:26:05.716 --> 00:26:08.166
Let's take a closer look at what

00:26:08.166 --> 00:26:10.216
happens under the hood when this

00:26:10.216 --> 00:26:11.206
-- when we are doing this.

00:26:12.146 --> 00:26:13.636
For each image, we will need to

00:26:13.636 --> 00:26:15.096
do some kind of preprocessing

00:26:15.096 --> 00:26:15.366
work.

00:26:15.686 --> 00:26:16.876
If nothing else, we need to send

00:26:16.876 --> 00:26:18.216
the data down to the GPU.

00:26:19.066 --> 00:26:20.316
Once we have done that, we can

00:26:20.316 --> 00:26:21.976
do the calculation and produce

00:26:21.976 --> 00:26:22.816
the output image.

00:26:23.046 --> 00:26:23.526
But then there is a

00:26:23.526 --> 00:26:25.546
postprocessing step in which we

00:26:25.546 --> 00:26:26.736
need to retrieve the data from

00:26:26.736 --> 00:26:28.046
the GPU and return it to your

00:26:28.046 --> 00:26:28.296
app.

00:26:29.816 --> 00:26:31.176
The key to improving this

00:26:31.176 --> 00:26:32.716
picture is to eliminate the

00:26:32.716 --> 00:26:34.526
bubbles in the GPU pipeline.

00:26:35.896 --> 00:26:36.876
This results in greater

00:26:36.876 --> 00:26:38.136
performance for two major

00:26:38.136 --> 00:26:38.726
reasons.

00:26:38.926 --> 00:26:40.696
First, since there is no time

00:26:40.696 --> 00:26:42.306
when the GPU is idle the overall

00:26:42.306 --> 00:26:43.516
compute time is reduced.

00:26:44.066 --> 00:26:45.996
And second, because the GPU is

00:26:45.996 --> 00:26:47.856
kept working continuously, it's

00:26:47.856 --> 00:26:49.006
able to operate in a higher

00:26:49.006 --> 00:26:51.036
performance state and reduce the

00:26:51.036 --> 00:26:52.886
time necessary to compute each

00:26:52.886 --> 00:26:54.146
particular output.

00:26:55.616 --> 00:26:56.596
But so much of the appeal in

00:26:56.596 --> 00:26:57.806
Core ML is that you don't have

00:26:57.806 --> 00:26:59.236
to worry about any details like

00:26:59.236 --> 00:26:59.706
this at all.

00:27:00.106 --> 00:27:01.706
In fact, for your app all you

00:27:01.706 --> 00:27:02.666
are really concerned with for

00:27:02.666 --> 00:27:05.336
your users is going from a long

00:27:05.336 --> 00:27:06.636
time to get results to a short

00:27:06.676 --> 00:27:06.886
time.

00:27:07.846 --> 00:27:10.096
So this year we are introducing

00:27:10.096 --> 00:27:11.796
a new batch API that will allow

00:27:11.796 --> 00:27:13.026
you to do exactly this.

00:27:14.096 --> 00:27:15.336
Where before you needed to loop

00:27:15.336 --> 00:27:16.386
over your inputs and call

00:27:16.386 --> 00:27:18.056
separate predictions, the new

00:27:18.056 --> 00:27:19.156
API is very simple.

00:27:20.466 --> 00:27:22.516
One-line predictions, it

00:27:22.516 --> 00:27:24.366
consumes an input -- an array of

00:27:24.366 --> 00:27:25.946
inputs and produces an array of

00:27:25.946 --> 00:27:26.566
outputs.

00:27:26.936 --> 00:27:27.826
Core ML will take care of the

00:27:27.826 --> 00:27:27.976
rest.

00:27:28.516 --> 00:27:34.336
[ Applause ]

00:27:34.836 --> 00:27:35.716
So let's see it in action.

00:27:36.446 --> 00:27:38.696
So in keeping with our style

00:27:38.696 --> 00:27:40.386
transfer example, let's look at

00:27:40.386 --> 00:27:41.376
the case where we wanted to

00:27:41.376 --> 00:27:42.776
apply a style to our entire

00:27:42.776 --> 00:27:43.586
photo library.

00:27:43.956 --> 00:27:45.046
So here I have a simple app

00:27:45.116 --> 00:27:46.066
that's going to do just that.

00:27:46.066 --> 00:27:47.926
I am going to apply a style to

00:27:47.926 --> 00:27:48.946
200 images.

00:27:49.206 --> 00:27:51.396
On the left, as in your left,

00:27:51.976 --> 00:27:54.026
there is an implementation using

00:27:54.026 --> 00:27:55.526
last year's API in a for loop.

00:27:55.936 --> 00:27:57.636
And on the right we have the new

00:27:57.636 --> 00:27:58.276
batch API.

00:27:58.626 --> 00:27:59.346
So let's get started.

00:28:00.466 --> 00:28:01.156
We are off.

00:28:03.596 --> 00:28:04.606
And we can see the new is

00:28:04.606 --> 00:28:05.176
already done.

00:28:05.176 --> 00:28:06.756
We'll wait a moment for last

00:28:06.756 --> 00:28:08.916
year's technology, and there we

00:28:08.916 --> 00:28:08.983
go.

00:28:09.516 --> 00:28:14.796
[ Applause ]

00:28:15.296 --> 00:28:16.556
In this example we see a

00:28:16.556 --> 00:28:18.006
noticeable improvement with the

00:28:18.006 --> 00:28:18.756
new batch API.

00:28:19.136 --> 00:28:20.836
And in general, the improvement

00:28:20.836 --> 00:28:22.096
you'll see in your app depends

00:28:22.096 --> 00:28:23.906
on the model and the device and

00:28:23.906 --> 00:28:24.546
the workload.

00:28:25.016 --> 00:28:26.186
But if you have a large number

00:28:26.186 --> 00:28:27.746
of predictions to call, use the

00:28:27.746 --> 00:28:29.606
new API and give Core ML every

00:28:29.606 --> 00:28:30.606
opportunity to accelerate your

00:28:30.606 --> 00:28:30.976
computation.

00:28:35.666 --> 00:28:36.796
Of course, the most

00:28:36.796 --> 00:28:37.806
high-performance app in the

00:28:37.806 --> 00:28:39.746
world isn't terribly exciting if

00:28:39.746 --> 00:28:40.596
it's not delivering an

00:28:40.596 --> 00:28:41.846
experience that you want for

00:28:41.846 --> 00:28:42.526
your users.

00:28:43.666 --> 00:28:44.966
We want to ensure that no matter

00:28:44.966 --> 00:28:46.846
what that experience is, or what

00:28:46.846 --> 00:28:48.546
it could be in the future, Core

00:28:48.546 --> 00:28:50.066
ML will be just as performant

00:28:50.066 --> 00:28:51.336
and simple to use as ever.

00:28:52.486 --> 00:28:53.286
But the field of machine

00:28:53.286 --> 00:28:54.656
learning is growing rapidly.

00:28:55.186 --> 00:28:56.086
How will we keep up?

00:28:56.786 --> 00:28:57.926
And just how rapidly?

00:28:57.976 --> 00:28:59.406
Well let me tell you a little

00:28:59.406 --> 00:29:00.636
bit of a personal story about

00:29:00.666 --> 00:29:00.876
that.

00:29:02.246 --> 00:29:03.496
Let's take a look at a

00:29:03.496 --> 00:29:05.196
deceptively simple question that

00:29:05.196 --> 00:29:06.346
we can answer with machine

00:29:06.346 --> 00:29:06.626
learning.

00:29:07.686 --> 00:29:09.256
Given an image, what I want to

00:29:09.256 --> 00:29:11.536
know: Are there any horses in

00:29:11.536 --> 00:29:11.603
it?

00:29:12.576 --> 00:29:14.346
So I think I heard a chuckle or

00:29:14.346 --> 00:29:14.626
two.

00:29:14.626 --> 00:29:15.666
Maybe this seems like kind of a

00:29:15.666 --> 00:29:16.836
silly challenge problem.

00:29:16.836 --> 00:29:18.626
Small children love this, by the

00:29:18.626 --> 00:29:18.856
way.

00:29:19.086 --> 00:29:22.056
But -- so way, way back in the

00:29:22.106 --> 00:29:23.356
past, when I was first starting

00:29:23.356 --> 00:29:24.446
graduate school and I was

00:29:24.446 --> 00:29:25.746
thinking about this problem and

00:29:25.746 --> 00:29:26.736
first learning about machine

00:29:26.736 --> 00:29:28.476
learning, my insights on the top

00:29:28.476 --> 00:29:29.426
came down to something like

00:29:29.426 --> 00:29:32.306
this: I don't know -- seems

00:29:32.306 --> 00:29:32.546
hard.

00:29:33.226 --> 00:29:34.276
I don't really have any good

00:29:34.276 --> 00:29:34.776
idea for you.

00:29:35.896 --> 00:29:38.106
So a few years pass.

00:29:38.266 --> 00:29:39.716
I get older, hopefully a little

00:29:39.716 --> 00:29:40.316
bit wiser.

00:29:40.316 --> 00:29:41.466
But certainly the field is

00:29:41.466 --> 00:29:42.676
moving very, very quickly,

00:29:42.676 --> 00:29:43.836
because there started to be a

00:29:43.836 --> 00:29:45.176
lot of exciting new results

00:29:45.176 --> 00:29:46.686
using deep neural networks.

00:29:47.736 --> 00:29:48.926
And so then my view on this

00:29:48.926 --> 00:29:49.676
problem changed.

00:29:49.676 --> 00:29:50.716
And suddenly, wow, this

00:29:50.716 --> 00:29:52.036
cutting-edge research can really

00:29:52.036 --> 00:29:53.366
answer these kind of questions,

00:29:53.366 --> 00:29:54.756
and computers can catch up with

00:29:54.756 --> 00:29:55.556
small children and horse

00:29:55.556 --> 00:29:56.196
recognition technology.

00:29:56.196 --> 00:29:56.976
What an exciting development.

00:29:57.516 --> 00:29:59.866
[ Laughter ]

00:30:00.366 --> 00:30:02.366
So a few more years pass.

00:30:02.486 --> 00:30:03.856
Now I work at Apple, and my

00:30:03.856 --> 00:30:05.226
perspective on this problem has

00:30:05.226 --> 00:30:06.476
changed again.

00:30:06.476 --> 00:30:09.256
Now, just grab Create ML.

00:30:09.436 --> 00:30:10.656
The UI is lovely.

00:30:10.656 --> 00:30:11.696
You'll have a horse classifier

00:30:11.696 --> 00:30:12.526
in just a few minutes.

00:30:13.366 --> 00:30:15.046
So, you know, if you are a

00:30:15.046 --> 00:30:16.226
machine learning expert, maybe

00:30:16.226 --> 00:30:17.136
you are looking at this and you

00:30:17.136 --> 00:30:18.016
are thinking, "Oh, this guy

00:30:18.016 --> 00:30:18.926
doesn't know what he is talking

00:30:18.926 --> 00:30:19.196
about.

00:30:19.196 --> 00:30:20.506
You know, in 2007 I knew how to

00:30:20.506 --> 00:30:21.376
solve that problem.

00:30:21.376 --> 00:30:23.336
In 2012 I'd solved it a hundred

00:30:23.336 --> 00:30:23.646
times."

00:30:24.506 --> 00:30:25.076
Not my point.

00:30:25.756 --> 00:30:27.146
If you are someone who cares

00:30:27.146 --> 00:30:29.446
about long-lasting, high-quality

00:30:29.446 --> 00:30:30.976
software, this should make you

00:30:30.976 --> 00:30:32.796
nervous, because in 11 years we

00:30:32.796 --> 00:30:34.136
have seen the entire picture of

00:30:34.136 --> 00:30:35.126
this problem turn over.

00:30:36.246 --> 00:30:37.386
So let's take a look at a few

00:30:37.386 --> 00:30:38.676
more features in Core ML that

00:30:38.676 --> 00:30:40.146
can help set your mind at ease.

00:30:41.606 --> 00:30:43.346
To do that, let's open the hood

00:30:43.346 --> 00:30:44.586
once again and peek at one of

00:30:44.586 --> 00:30:46.176
these new horse finder models,

00:30:46.656 --> 00:30:48.406
which is, once again, a neural

00:30:48.406 --> 00:30:48.766
network.

00:30:49.526 --> 00:30:51.106
As we have illustrated before,

00:30:51.306 --> 00:30:52.696
the neural network consists of a

00:30:52.696 --> 00:30:54.266
series of highly optimized

00:30:54.266 --> 00:30:54.696
layers.

00:30:54.746 --> 00:30:56.266
It is a series of layers, and we

00:30:56.266 --> 00:30:57.176
have highly optimized

00:30:57.176 --> 00:30:58.586
implementations for each of them

00:30:58.626 --> 00:30:59.916
in our inference engine.

00:31:00.656 --> 00:31:02.396
Our list of supported operations

00:31:02.396 --> 00:31:03.806
is large and always growing,

00:31:04.306 --> 00:31:05.416
trying to keep up with new

00:31:05.416 --> 00:31:06.506
developments in the field.

00:31:07.436 --> 00:31:08.746
But what if there is a layer

00:31:08.746 --> 00:31:10.116
that just isn't supported in

00:31:10.116 --> 00:31:10.646
Core ML?

00:31:11.256 --> 00:31:14.476
In the past, you either needed

00:31:14.476 --> 00:31:15.926
to wait or you needed a

00:31:15.926 --> 00:31:16.516
different model.

00:31:16.516 --> 00:31:18.866
But what if this layer is the

00:31:19.126 --> 00:31:20.596
key horse-finding layer?

00:31:21.076 --> 00:31:22.166
This is the breakthrough that

00:31:22.166 --> 00:31:23.626
your horse app was waiting for.

00:31:23.966 --> 00:31:24.836
Can you afford to wait?

00:31:26.546 --> 00:31:27.816
Given the speed of machine

00:31:27.816 --> 00:31:28.566
learning, this could be a

00:31:28.566 --> 00:31:29.426
serious obstacle.

00:31:31.216 --> 00:31:33.376
So we introduced custom layers

00:31:33.376 --> 00:31:34.406
for neural network models.

00:31:35.076 --> 00:31:37.176
Now if a neural network layer is

00:31:37.176 --> 00:31:38.866
missing, you can provide an

00:31:38.866 --> 00:31:40.926
implementation with -- will mesh

00:31:41.116 --> 00:31:42.326
seamlessly with the rest of the

00:31:42.326 --> 00:31:43.186
Core ML model.

00:31:44.026 --> 00:31:45.606
Inside the model, the custom

00:31:45.606 --> 00:31:46.876
layer stores the name of an

00:31:46.876 --> 00:31:48.166
implementing class -- the

00:31:48.166 --> 00:31:49.726
AAPLCustomHorseLayer in this

00:31:49.726 --> 00:31:50.126
case.

00:31:50.906 --> 00:31:52.626
The implementation class fills

00:31:52.626 --> 00:31:53.656
the role of the missing

00:31:53.656 --> 00:31:54.976
implementation in the inference

00:31:54.976 --> 00:31:55.286
engine.

00:31:55.906 --> 00:31:56.936
Just like the layer is built

00:31:56.936 --> 00:31:59.406
into Core ML, the implementation

00:31:59.406 --> 00:32:00.856
provided here should be general

00:32:00.856 --> 00:32:02.336
and applicable to any instance

00:32:02.336 --> 00:32:02.916
of the new layer.

00:32:04.736 --> 00:32:05.986
It simply needs to be included

00:32:05.986 --> 00:32:07.206
in your app at runtime.

00:32:07.706 --> 00:32:08.996
Then the parameters for this

00:32:08.996 --> 00:32:10.196
particular layer are

00:32:10.196 --> 00:32:11.946
encapsulated in the ML model

00:32:12.106 --> 00:32:13.466
with the rest of the information

00:32:13.466 --> 00:32:14.136
about the model.

00:32:15.996 --> 00:32:17.196
Implementing a custom layer is

00:32:17.196 --> 00:32:17.476
simple.

00:32:18.136 --> 00:32:19.576
We expose an MLCustomLayer

00:32:19.576 --> 00:32:20.196
protocol.

00:32:20.556 --> 00:32:21.816
You simply provide methods to

00:32:21.816 --> 00:32:23.356
initialize the layer based on

00:32:23.356 --> 00:32:24.846
the data stored in the ML model.

00:32:26.096 --> 00:32:27.176
You'll need to provide a method

00:32:27.176 --> 00:32:28.516
that tells us how much space to

00:32:28.516 --> 00:32:29.826
allocate for the outputs of the

00:32:29.826 --> 00:32:31.846
layer, and then a method that

00:32:31.846 --> 00:32:32.846
does the computation.

00:32:34.676 --> 00:32:36.596
Plus, you can add this

00:32:36.596 --> 00:32:38.406
flexibility without sacrificing

00:32:38.406 --> 00:32:39.626
the performance of your model as

00:32:39.626 --> 00:32:39.946
a whole.

00:32:40.976 --> 00:32:41.996
The protocol includes an

00:32:41.996 --> 00:32:43.336
optional method, which allows

00:32:43.336 --> 00:32:44.506
you to provide us with a MTL

00:32:44.506 --> 00:32:45.946
shader implementation of your

00:32:45.946 --> 00:32:47.436
model -- of the layer, excuse

00:32:47.436 --> 00:32:47.556
me.

00:32:47.936 --> 00:32:50.056
If you give us this, then it can

00:32:50.056 --> 00:32:51.556
be encoded in the same command

00:32:51.556 --> 00:32:52.796
buffer as the rest of the Core

00:32:52.796 --> 00:32:53.866
ML computation.

00:32:53.866 --> 00:32:55.026
So there is no extra overhead

00:32:55.026 --> 00:32:56.196
from additional encodings or

00:32:56.196 --> 00:32:57.346
multiple trips to and from the

00:32:57.346 --> 00:32:57.786
GPU.

00:32:58.536 --> 00:32:59.826
If you don't provide this, then

00:32:59.826 --> 00:33:01.026
we'll simply evaluate the layer

00:33:01.026 --> 00:33:02.546
on the CPU with no other work on

00:33:02.546 --> 00:33:03.506
your part.

00:33:04.256 --> 00:33:06.016
So no matter how quickly

00:33:06.016 --> 00:33:07.186
advancements in neural network

00:33:07.186 --> 00:33:08.466
models may happen, you have a

00:33:08.466 --> 00:33:09.716
way to keep up with Core ML.

00:33:10.556 --> 00:33:11.936
But there are limitations.

00:33:12.836 --> 00:33:14.196
Custom layers only work for

00:33:14.196 --> 00:33:15.476
neural network models, and they

00:33:15.476 --> 00:33:16.816
only take inputs and outputs

00:33:16.816 --> 00:33:18.066
which are ML MultiArrays.

00:33:18.066 --> 00:33:19.986
This is a natural way to

00:33:19.986 --> 00:33:21.326
interact with neural networks.

00:33:21.686 --> 00:33:22.706
But the machine learning field

00:33:22.706 --> 00:33:24.406
is hardly restricted to only

00:33:24.406 --> 00:33:25.556
advancing in this area.

00:33:26.656 --> 00:33:27.666
In fact, when I was first

00:33:27.666 --> 00:33:28.406
learning about image

00:33:28.406 --> 00:33:29.996
recognition, almost no one was

00:33:29.996 --> 00:33:31.416
talking about neural networks as

00:33:31.416 --> 00:33:32.626
a solution to that problem.

00:33:32.986 --> 00:33:34.006
And you can see today it's the

00:33:34.006 --> 00:33:37.186
absolute state of the art.

00:33:37.186 --> 00:33:38.686
And it's not hard to imagine

00:33:38.686 --> 00:33:39.896
machine-learning-enabled app

00:33:39.896 --> 00:33:41.426
experiences where custom layers

00:33:41.426 --> 00:33:42.426
simply wouldn't fit.

00:33:42.906 --> 00:33:44.666
For instance, a machine-learning

00:33:44.666 --> 00:33:46.246
app might use a neural network

00:33:46.246 --> 00:33:47.446
to embed an image in some

00:33:47.446 --> 00:33:49.356
similarity space, then look up

00:33:49.356 --> 00:33:50.576
similar images using a

00:33:50.576 --> 00:33:51.776
nearest-neighbor method or

00:33:51.776 --> 00:33:53.456
locality-sensitive hashing -- or

00:33:53.456 --> 00:33:54.626
even some other approach.

00:33:56.896 --> 00:33:58.306
A model might combine audio and

00:33:58.306 --> 00:33:59.736
motion data to provide a bit of

00:33:59.736 --> 00:34:01.096
needed encouragement to someone

00:34:01.096 --> 00:34:02.126
who doesn't always close his

00:34:02.126 --> 00:34:02.506
rings.

00:34:04.826 --> 00:34:06.336
Or even a completely new model

00:34:06.336 --> 00:34:07.776
type we haven't even imagined

00:34:07.776 --> 00:34:08.886
yet that enables novel

00:34:08.886 --> 00:34:10.286
experiences for your users.

00:34:11.076 --> 00:34:12.266
In all these cases, it would be

00:34:12.266 --> 00:34:13.116
great if we could have the

00:34:13.116 --> 00:34:14.545
simplicity and portability of

00:34:14.545 --> 00:34:16.496
Core ML without having to

00:34:16.496 --> 00:34:18.106
sacrifice the flexibility to

00:34:18.106 --> 00:34:19.096
keep up with the field.

00:34:19.815 --> 00:34:22.005
So we are introducing custom

00:34:22.005 --> 00:34:22.505
models.

00:34:23.496 --> 00:34:25.056
A Core ML custom model allows

00:34:25.056 --> 00:34:26.045
you to encapsulate the

00:34:26.045 --> 00:34:27.315
implementation of a part of a

00:34:27.315 --> 00:34:28.576
computation that's missing

00:34:28.735 --> 00:34:29.686
inside Core ML.

00:34:30.436 --> 00:34:32.045
Just like for custom layers, the

00:34:32.045 --> 00:34:33.286
model stores the name of an

00:34:33.286 --> 00:34:34.456
implementation class.

00:34:34.956 --> 00:34:36.216
The class fills the role of the

00:34:36.216 --> 00:34:37.606
general inference engine for

00:34:37.606 --> 00:34:38.656
this type of model.

00:34:39.116 --> 00:34:40.906
Then the parameters are stored

00:34:40.906 --> 00:34:42.235
in the ML Model just like

00:34:42.266 --> 00:34:42.626
before.

00:34:43.386 --> 00:34:44.826
This allows the model to be

00:34:44.826 --> 00:34:46.366
updated as an asset in your app

00:34:46.366 --> 00:34:48.286
without having to touch code.

00:34:50.005 --> 00:34:51.806
And implementing a custom model

00:34:51.806 --> 00:34:52.556
is simple as well.

00:34:52.896 --> 00:34:54.016
We expose a protocol,

00:34:54.065 --> 00:34:55.216
MLCustomModel.

00:34:55.466 --> 00:34:56.485
You provide methods to

00:34:56.485 --> 00:34:57.876
initialize based on the data

00:34:57.876 --> 00:34:59.126
stored in the ML Model.

00:34:59.126 --> 00:35:01.076
And you provide a method to

00:35:01.076 --> 00:35:02.206
compute the prediction on an

00:35:02.206 --> 00:35:02.656
input.

00:35:02.976 --> 00:35:04.626
There is an optional method to

00:35:04.626 --> 00:35:06.146
provide a batch implementation

00:35:06.146 --> 00:35:07.276
if there are opportunities in

00:35:07.276 --> 00:35:08.776
this particular model type to

00:35:08.776 --> 00:35:10.006
have optimizations there.

00:35:10.286 --> 00:35:11.276
And if not, we'll call the

00:35:11.276 --> 00:35:13.126
single prediction in a for loop.

00:35:14.026 --> 00:35:16.046
And using a customized model in

00:35:16.046 --> 00:35:17.316
your app is largely the same

00:35:17.316 --> 00:35:18.696
workflow as any other Core ML

00:35:18.696 --> 00:35:19.066
model.

00:35:19.646 --> 00:35:21.156
In Xcode, a model with

00:35:21.156 --> 00:35:22.936
customized components will have

00:35:22.936 --> 00:35:24.986
a dependency section listing the

00:35:24.986 --> 00:35:26.286
names of the implementations

00:35:26.286 --> 00:35:27.366
needed along with a short

00:35:27.366 --> 00:35:27.956
description.

00:35:28.576 --> 00:35:29.816
Just include these in your app,

00:35:29.936 --> 00:35:30.836
and you are ready to go.

00:35:31.696 --> 00:35:33.616
The prediction API is unchanged,

00:35:33.616 --> 00:35:34.946
whether for single predictions

00:35:34.996 --> 00:35:35.646
or batch.

00:35:37.016 --> 00:35:39.476
So custom layers and custom

00:35:39.476 --> 00:35:41.086
models allow you to use the

00:35:41.086 --> 00:35:42.806
power and simplicity of Core ML

00:35:42.806 --> 00:35:43.896
without sacrificing the

00:35:43.896 --> 00:35:45.676
flexibility needed to keep up

00:35:45.676 --> 00:35:46.886
with the fast-paced area of

00:35:46.886 --> 00:35:47.496
machine learning.

00:35:48.566 --> 00:35:49.896
For new neural network layers,

00:35:50.096 --> 00:35:52.016
custom layers allow you to make

00:35:52.066 --> 00:35:53.566
use of the many optimizations

00:35:53.566 --> 00:35:54.716
already present in the neural

00:35:54.716 --> 00:35:56.176
network inference engine in Core

00:35:56.176 --> 00:35:56.446
ML.

00:35:57.096 --> 00:35:58.956
Custom models are more flexible

00:35:59.816 --> 00:36:01.686
for types and functionality, but

00:36:01.686 --> 00:36:02.656
they do require more

00:36:02.656 --> 00:36:04.006
implementation work on your

00:36:05.296 --> 00:36:05.436
part.

00:36:05.806 --> 00:36:07.316
Both forms of customization

00:36:07.316 --> 00:36:08.916
allow you to encapsulate model

00:36:08.916 --> 00:36:10.346
parameters in an ML model,

00:36:10.826 --> 00:36:12.176
making the model portable and

00:36:12.176 --> 00:36:13.156
your code simpler.

00:36:14.516 --> 00:36:16.816
And we've only been able to

00:36:16.816 --> 00:36:18.146
touch on a few of the great new

00:36:18.146 --> 00:36:19.506
features in Core ML 2.

00:36:20.146 --> 00:36:22.336
Please download the beta, try

00:36:22.336 --> 00:36:22.976
them out for yourself.

00:36:27.066 --> 00:36:28.556
Core ML has many great new

00:36:28.556 --> 00:36:29.926
features to reduce your app

00:36:29.926 --> 00:36:31.916
size, improve performance, and

00:36:31.916 --> 00:36:33.136
ensure flexibility and

00:36:33.136 --> 00:36:34.476
compatibility with the latest

00:36:34.476 --> 00:36:35.406
developments in machine

00:36:35.406 --> 00:36:35.686
learning.

00:36:36.496 --> 00:36:37.886
We showed you how quantization

00:36:37.886 --> 00:36:39.676
can reduce model size, how the

00:36:39.676 --> 00:36:41.336
new batch API can enable more

00:36:41.336 --> 00:36:43.156
efficient processing, and how

00:36:43.156 --> 00:36:44.936
custom layers and custom models

00:36:44.936 --> 00:36:46.226
can help you bring cutting-edge

00:36:46.226 --> 00:36:47.446
machine learning to your app.

00:36:48.416 --> 00:36:49.756
Combined with our great new tool

00:36:49.756 --> 00:36:51.236
for training models in Create

00:36:51.236 --> 00:36:52.746
ML, there are more ways than

00:36:52.746 --> 00:36:54.596
ever to add ML-enabled features

00:36:54.596 --> 00:36:56.176
to your app and support great

00:36:56.176 --> 00:36:57.736
new experiences for your users.

00:36:59.576 --> 00:37:01.376
After just a short break, we'll

00:37:01.376 --> 00:37:02.986
be back right here to take a

00:37:02.986 --> 00:37:04.066
deeper look at some of these

00:37:04.066 --> 00:37:04.626
features.

00:37:04.896 --> 00:37:06.286
In particular, we'll show you

00:37:06.286 --> 00:37:07.686
how to use our Core ML Tools

00:37:07.686 --> 00:37:09.576
software to start reducing model

00:37:09.576 --> 00:37:11.596
sizes and customizing your user

00:37:11.596 --> 00:37:13.096
experiences with Core ML today.

00:37:13.096 --> 00:37:13.976
Thank you.

00:37:14.516 --> 00:37:20.500
[ Applause ]