WEBVTT

00:00:07.016 --> 00:00:15.500
[ Music ]

00:00:21.516 --> 00:00:28.316
[ Applause ]

00:00:28.816 --> 00:00:30.896
>> Good afternoon, everyone.

00:00:31.826 --> 00:00:33.356
Welcome to our talk on Metal for

00:00:33.356 --> 00:00:34.666
Accelerating Machine Learning.

00:00:35.596 --> 00:00:37.366
My name is Anna Tikhonova, and

00:00:37.366 --> 00:00:38.606
I'm an Engineer on the GPU

00:00:38.606 --> 00:00:40.006
Software Team.

00:00:42.056 --> 00:00:44.126
The Metal Performance Shaders

00:00:44.126 --> 00:00:45.596
Framework is built on top of

00:00:45.596 --> 00:00:45.926
Metal.

00:00:46.576 --> 00:00:48.216
And it provides GPU accelerated

00:00:48.256 --> 00:00:49.886
primitives that are optimized

00:00:50.036 --> 00:00:51.646
for both iOS and macOS.

00:00:52.916 --> 00:00:54.516
We provide primitives for image

00:00:54.516 --> 00:00:56.846
processing, linear algebra, and

00:00:56.846 --> 00:00:57.566
machine learning.

00:00:58.636 --> 00:01:00.106
We talked extensively about

00:01:00.106 --> 00:01:01.956
inference in our past WWDC

00:01:01.956 --> 00:01:03.416
sessions, so I just want to

00:01:03.416 --> 00:01:05.316
highlight them here.

00:01:06.456 --> 00:01:08.466
And this year, we've also added

00:01:08.466 --> 00:01:10.326
support for training on both iOS

00:01:10.326 --> 00:01:10.896
and macOS.

00:01:12.516 --> 00:01:15.436
[ Applause ]

00:01:15.936 --> 00:01:16.376
Thank you.

00:01:18.876 --> 00:01:20.136
We've also added support for

00:01:20.136 --> 00:01:21.586
accelerated ray tracing on our

00:01:21.586 --> 00:01:23.946
platform, and we had an entire

00:01:23.946 --> 00:01:25.936
session on this topic earlier

00:01:25.936 --> 00:01:26.406
this week.

00:01:26.676 --> 00:01:28.026
So, it was titled Metal for Ray

00:01:28.026 --> 00:01:29.136
Tracing Acceleration.

00:01:29.886 --> 00:01:31.666
And the video for the session

00:01:31.666 --> 00:01:32.686
will be available online

00:01:32.686 --> 00:01:33.086
shortly.

00:01:34.406 --> 00:01:35.846
In this session, I will talk

00:01:35.846 --> 00:01:36.996
primarily about machine

00:01:36.996 --> 00:01:38.886
learning, specifically training.

00:01:39.506 --> 00:01:43.206
So, I mentioned training and

00:01:43.206 --> 00:01:43.586
inference.

00:01:44.706 --> 00:01:46.316
Deep learning algorithms consist

00:01:46.316 --> 00:01:47.256
of these two phases.

00:01:47.646 --> 00:01:49.086
The first phase is the training

00:01:49.086 --> 00:01:49.476
phase.

00:01:50.526 --> 00:01:52.096
So, let's use an example where

00:01:52.096 --> 00:01:53.866
we want to train a model, to

00:01:53.866 --> 00:01:55.756
categorize images into classes

00:01:56.236 --> 00:01:58.056
like cats, dogs, giraffes,

00:01:58.056 --> 00:01:58.536
etcetera.

00:02:00.036 --> 00:02:02.016
So, in order to train a model to

00:02:02.016 --> 00:02:03.896
recognize cats, we need to feed

00:02:03.896 --> 00:02:05.586
it a large number of labeled

00:02:05.586 --> 00:02:06.476
images of cats.

00:02:07.116 --> 00:02:09.156
And same for the rabbits and all

00:02:09.156 --> 00:02:10.446
of the other animals you want

00:02:10.446 --> 00:02:11.416
your model to be able to

00:02:11.416 --> 00:02:11.936
recognize.

00:02:12.436 --> 00:02:15.606
Training is a computationally

00:02:15.606 --> 00:02:17.396
expensive and time-consuming,

00:02:17.466 --> 00:02:18.406
iterative process.

00:02:19.456 --> 00:02:20.806
The result of training are

00:02:20.806 --> 00:02:21.766
trained parameters.

00:02:23.956 --> 00:02:25.786
The trained parameters are

00:02:25.786 --> 00:02:27.236
required for the next phase, the

00:02:27.236 --> 00:02:27.966
inference phase.

00:02:28.746 --> 00:02:30.396
This is when your model is

00:02:30.396 --> 00:02:31.836
presented with a new image, that

00:02:31.836 --> 00:02:33.286
is never seen before, and it

00:02:33.286 --> 00:02:34.846
needs to classify it based on

00:02:34.846 --> 00:02:35.746
the trained parameters.

00:02:36.266 --> 00:02:36.836
This is a cat.

00:02:38.336 --> 00:02:40.266
We now provide GPU acceleration

00:02:40.516 --> 00:02:41.976
for both the training and

00:02:41.976 --> 00:02:42.826
inference phases.

00:02:43.466 --> 00:02:46.896
But before I talk about

00:02:46.896 --> 00:02:48.336
training, let me tell you about

00:02:48.336 --> 00:02:49.886
some enhancements to CNN

00:02:49.886 --> 00:02:51.106
Inference we've added this year.

00:02:51.976 --> 00:02:53.566
We now have support for FP16

00:02:53.626 --> 00:02:56.116
accumulation for the convolution

00:02:56.116 --> 00:02:57.536
and convolution transpose

00:02:57.536 --> 00:02:57.966
primitives.

00:02:58.996 --> 00:03:00.556
This new feature is available on

00:03:00.556 --> 00:03:02.936
devices with an Apple A11 Bionic

00:03:02.936 --> 00:03:03.216
GPU.

00:03:04.446 --> 00:03:06.516
We find that using FP16

00:03:06.546 --> 00:03:07.856
accumulation for inference

00:03:07.856 --> 00:03:09.266
workloads is more than

00:03:09.266 --> 00:03:10.956
sufficient in terms of precision

00:03:11.996 --> 00:03:13.636
for many commonly used neural

00:03:13.636 --> 00:03:14.036
networks.

00:03:15.656 --> 00:03:17.866
FP16 accumulation offers better

00:03:17.866 --> 00:03:19.926
precision and significant power

00:03:19.926 --> 00:03:20.556
benefits.

00:03:20.736 --> 00:03:22.196
So, please take advantage of it

00:03:22.706 --> 00:03:23.926
in your inference workloads.

00:03:25.216 --> 00:03:26.996
And this is an example of how

00:03:26.996 --> 00:03:29.046
you can enable FP16 accumulation

00:03:29.306 --> 00:03:30.536
for a convolution primitive.

00:03:30.996 --> 00:03:31.806
You just need to set the

00:03:31.806 --> 00:03:33.446
accumulatorPrecisionOption

00:03:33.446 --> 00:03:33.906
property.

00:03:36.126 --> 00:03:38.536
And now, let's talk in depth

00:03:38.536 --> 00:03:40.296
about the main topic of this

00:03:40.296 --> 00:03:41.986
session, training neural

00:03:41.986 --> 00:03:42.536
networks.

00:03:43.136 --> 00:03:44.356
And let's start with training

00:03:44.356 --> 00:03:46.476
convolutional neural networks.

00:03:48.316 --> 00:03:50.296
So, here we have a simple,

00:03:50.296 --> 00:03:51.856
handwritten, digit recognition

00:03:51.856 --> 00:03:54.606
network that takes an image of a

00:03:54.606 --> 00:03:57.336
handwritten image as input, and

00:03:57.646 --> 00:03:59.256
assigns it to one of 10 classes,

00:03:59.506 --> 00:04:00.386
from 0 to 9.

00:04:00.386 --> 00:04:02.826
In this example, we are

00:04:02.826 --> 00:04:04.546
correctly classifying this image

00:04:04.826 --> 00:04:06.406
as an image of a digit 7.

00:04:07.086 --> 00:04:11.496
For inference, we initialize our

00:04:11.496 --> 00:04:12.886
network with trained parameters.

00:04:14.086 --> 00:04:15.436
So, in this example, the trained

00:04:15.436 --> 00:04:16.716
parameters add the weights for

00:04:16.716 --> 00:04:18.196
the convolution, and fully

00:04:18.196 --> 00:04:19.106
connected primitives.

00:04:20.426 --> 00:04:21.886
The goal of a training algorithm

00:04:21.926 --> 00:04:23.046
is to compute these trained

00:04:23.046 --> 00:04:24.586
parameters, so the network can

00:04:24.586 --> 00:04:26.706
use them to map inputs to

00:04:26.706 --> 00:04:27.896
correct outputs during

00:04:27.926 --> 00:04:28.416
inference.

00:04:29.076 --> 00:04:31.436
When we start the training

00:04:31.436 --> 00:04:32.796
process, we don't have any

00:04:32.796 --> 00:04:33.106
weights.

00:04:33.496 --> 00:04:34.536
We need to compute them.

00:04:34.786 --> 00:04:37.116
So, the first step is to

00:04:37.246 --> 00:04:38.446
initialize the weights with

00:04:38.446 --> 00:04:39.386
small, random numbers.

00:04:40.026 --> 00:04:41.106
And now we are ready to train

00:04:41.106 --> 00:04:41.676
this network.

00:04:41.946 --> 00:04:43.386
So, let's take a look at all the

00:04:43.436 --> 00:04:44.696
steps involved in a training

00:04:44.696 --> 00:04:45.236
process.

00:04:47.836 --> 00:04:49.146
Training is an iterative

00:04:49.216 --> 00:04:51.006
process, and each iteration of

00:04:51.006 --> 00:04:52.506
training can be divided into

00:04:52.506 --> 00:04:53.056
four steps.

00:04:53.296 --> 00:04:55.256
The first step is the forward

00:04:55.256 --> 00:04:55.376
pass.

00:04:56.026 --> 00:04:57.556
This is when we take an input

00:04:57.696 --> 00:04:59.366
and pass it to our network to

00:04:59.366 --> 00:05:00.296
produce an output.

00:05:01.106 --> 00:05:02.396
It's very similar to inference.

00:05:04.226 --> 00:05:05.416
And next, we need to compute

00:05:05.416 --> 00:05:05.766
loss.

00:05:06.346 --> 00:05:08.156
So, loss intuitively measures

00:05:08.196 --> 00:05:09.326
the difference between the

00:05:09.326 --> 00:05:10.906
network's output and the ground

00:05:10.906 --> 00:05:11.196
truth.

00:05:13.336 --> 00:05:14.726
The objective of a training

00:05:14.726 --> 00:05:16.216
algorithm is to minimize loss.

00:05:17.076 --> 00:05:20.596
Our next step is the gradient

00:05:21.246 --> 00:05:21.426
pass.

00:05:21.536 --> 00:05:22.926
This is when we propagate this

00:05:22.926 --> 00:05:24.136
difference when the network's

00:05:24.136 --> 00:05:26.166
output and the ground truth,

00:05:26.166 --> 00:05:28.506
back to the network to update

00:05:28.506 --> 00:05:28.756
weights.

00:05:29.966 --> 00:05:32.486
The idea is that as training

00:05:32.486 --> 00:05:34.036
continues, our network is

00:05:34.036 --> 00:05:35.656
becoming better trained, so it's

00:05:35.656 --> 00:05:37.886
better able to map inputs to

00:05:37.886 --> 00:05:40.486
correct outputs, which in turn

00:05:40.486 --> 00:05:41.426
helps to minimize loss.

00:05:42.346 --> 00:05:45.136
So, this is an overview and now

00:05:45.136 --> 00:05:46.466
let's take a look at each one of

00:05:46.466 --> 00:05:47.686
these steps in more detail.

00:05:47.986 --> 00:05:50.986
The forward pass involves

00:05:51.586 --> 00:05:53.226
propagation forward to the

00:05:53.226 --> 00:05:55.046
network, to compute an output.

00:05:56.116 --> 00:05:57.416
As you can see, during this

00:05:57.416 --> 00:05:59.246
first situation of training, our

00:05:59.246 --> 00:06:00.616
network is not doing so well.

00:06:01.256 --> 00:06:03.136
It's outputting a result that's

00:06:03.136 --> 00:06:03.806
clearly wrong.

00:06:03.806 --> 00:06:05.056
So, why is it doing so badly?

00:06:05.656 --> 00:06:06.896
Well, this is expected.

00:06:06.896 --> 00:06:08.756
We just initialized our weights

00:06:08.756 --> 00:06:09.746
with some random numbers.

00:06:09.836 --> 00:06:11.046
We haven't trained the network

00:06:11.046 --> 00:06:12.626
to do any better yet.

00:06:13.656 --> 00:06:15.636
So, now, we need some weight to

00:06:15.636 --> 00:06:18.296
quantify how well or how badly

00:06:18.376 --> 00:06:19.636
our network is currently doing.

00:06:20.086 --> 00:06:21.886
So, we can use this information

00:06:22.476 --> 00:06:24.206
to improve our weights, so that

00:06:24.206 --> 00:06:25.686
hopefully after more iterations

00:06:25.686 --> 00:06:27.096
of training, the network can

00:06:27.096 --> 00:06:28.106
produce a better result.

00:06:28.776 --> 00:06:31.696
But in order to know how well

00:06:31.696 --> 00:06:33.006
we're doing, we need to know

00:06:33.006 --> 00:06:34.146
what the right answers are.

00:06:34.666 --> 00:06:36.326
So, the ground truth, which I

00:06:36.326 --> 00:06:38.406
will call labels from now on, is

00:06:38.406 --> 00:06:40.276
an input to the network along

00:06:40.276 --> 00:06:41.256
with the input image.

00:06:42.356 --> 00:06:43.646
So, in this case, it's a vector

00:06:43.646 --> 00:06:45.276
of 10 values, where we have 1

00:06:45.276 --> 00:06:47.006
for the correct class, Class 7,

00:06:47.376 --> 00:06:48.756
and zeros for all the other

00:06:49.566 --> 00:06:49.786
classes.

00:06:51.376 --> 00:06:53.526
The output of the network is our

00:06:53.526 --> 00:06:54.496
10 probabilities.

00:06:54.926 --> 00:06:55.526
One per class.

00:06:56.156 --> 00:06:58.136
So, as you can see in this first

00:06:58.136 --> 00:06:59.236
situation of training, the

00:06:59.236 --> 00:07:01.116
network is producing a very low

00:07:01.116 --> 00:07:03.026
result for the correct Class 7,

00:07:03.656 --> 00:07:05.336
and it's assigning the highest

00:07:05.336 --> 00:07:07.686
probability to a Class 9, which

00:07:07.686 --> 00:07:08.976
is why the network is returning

00:07:08.976 --> 00:07:09.756
9 as the answer.

00:07:11.486 --> 00:07:12.566
So, now we take all of this

00:07:12.566 --> 00:07:14.576
information and we pass it to

00:07:14.776 --> 00:07:15.646
our loss primitive.

00:07:16.316 --> 00:07:20.386
And as I mentioned previously,

00:07:21.476 --> 00:07:23.266
loss measures the difference

00:07:23.476 --> 00:07:25.506
between the network's output and

00:07:25.666 --> 00:07:26.466
the ground truth.

00:07:27.266 --> 00:07:28.836
And the objective of a training

00:07:28.836 --> 00:07:30.216
algorithm is to minimize loss.

00:07:32.306 --> 00:07:33.756
And now, we also need the second

00:07:33.756 --> 00:07:36.106
half of the graph.

00:07:36.616 --> 00:07:37.646
The second half of the graph

00:07:37.876 --> 00:07:40.736
contains gradient primitives for

00:07:40.736 --> 00:07:41.876
each responding forward

00:07:41.876 --> 00:07:42.226
primitive.

00:07:43.286 --> 00:07:45.456
The gradient primitives compute

00:07:45.456 --> 00:07:47.166
gradients that are needed to

00:07:47.166 --> 00:07:48.306
update weights.

00:07:49.136 --> 00:07:52.366
So, the loss primitive computes

00:07:52.786 --> 00:07:54.856
the first gradient, which is a

00:07:54.856 --> 00:07:56.146
derivative of a chosen loss

00:07:56.146 --> 00:07:57.376
function with respect to its

00:07:57.376 --> 00:07:57.756
inputs.

00:07:58.286 --> 00:07:59.646
And then we take this gradient

00:08:00.006 --> 00:08:02.926
and back propagate it, backward

00:08:02.926 --> 00:08:04.596
through the network, backward

00:08:04.596 --> 00:08:07.586
through the first gradient

00:08:07.586 --> 00:08:08.966
primitive in the backward

00:08:08.966 --> 00:08:09.496
direction.

00:08:09.556 --> 00:08:11.196
In this case, it's the SoftMax

00:08:12.026 --> 00:08:12.836
gradient primitive.

00:08:14.036 --> 00:08:15.266
And we use the Chain Rule to do

00:08:15.266 --> 00:08:15.486
that.

00:08:15.576 --> 00:08:16.796
So, the Chain Rule allows us to

00:08:16.796 --> 00:08:18.116
back propagate gradients,

00:08:18.116 --> 00:08:19.106
backwards through the network.

00:08:20.486 --> 00:08:21.556
And we're computing these

00:08:21.556 --> 00:08:23.396
gradients so that we can update

00:08:23.396 --> 00:08:23.796
weights.

00:08:24.166 --> 00:08:25.736
So, we're computing very small

00:08:25.736 --> 00:08:27.376
deltas to apply to the weights,

00:08:28.016 --> 00:08:29.216
in each iteration of training.

00:08:29.766 --> 00:08:32.645
And then we can use these

00:08:32.645 --> 00:08:34.056
updated weights in the next

00:08:34.056 --> 00:08:36.395
iteration of training, to

00:08:36.395 --> 00:08:37.785
ideally produce a lower loss

00:08:37.785 --> 00:08:38.816
value, which is what we're

00:08:38.816 --> 00:08:39.626
trying to minimize.

00:08:43.606 --> 00:08:45.156
In practice, any situation of

00:08:45.156 --> 00:08:47.086
training, we're not going to

00:08:47.086 --> 00:08:48.506
operate on a single image.

00:08:49.116 --> 00:08:50.666
We're going to operate on a

00:08:50.666 --> 00:08:52.226
group or a batch of images.

00:08:52.226 --> 00:08:54.396
For example, a batch of size 32

00:08:54.396 --> 00:08:55.056
or 64.

00:08:55.566 --> 00:08:57.176
And we need a corresponding

00:08:57.176 --> 00:08:59.916
batch of labels, for loss

00:08:59.916 --> 00:09:00.576
computation.

00:09:00.736 --> 00:09:02.096
So, in this case, we have a

00:09:02.096 --> 00:09:04.046
batch of labels, was 1 for the

00:09:04.046 --> 00:09:05.426
correct class and zeroes

00:09:05.426 --> 00:09:05.976
everywhere else.

00:09:09.506 --> 00:09:10.596
And in each situation of

00:09:10.596 --> 00:09:12.176
training, we're going to use a

00:09:12.176 --> 00:09:14.366
different batch of images and a

00:09:14.416 --> 00:09:15.716
responding batch of labels.

00:09:16.166 --> 00:09:17.166
So, let's now run through

00:09:17.166 --> 00:09:18.596
several iterations of training.

00:09:21.066 --> 00:09:23.686
For the first batch of images,

00:09:23.686 --> 00:09:25.006
we're doing a forward pass,

00:09:25.086 --> 00:09:27.276
computing loss, and doing a

00:09:27.276 --> 00:09:28.846
gradient pass.

00:09:28.976 --> 00:09:29.926
And updating weights.

00:09:30.356 --> 00:09:31.906
So, what happens with the second

00:09:31.906 --> 00:09:32.276
batch?

00:09:32.536 --> 00:09:33.876
Exactly the same process.

00:09:34.306 --> 00:09:35.446
The forward pass, then we

00:09:35.446 --> 00:09:37.276
compute loss, to the gradient

00:09:37.276 --> 00:09:38.656
pass, and update weights.

00:09:40.156 --> 00:09:41.566
And as we go through iterations

00:09:41.566 --> 00:09:44.126
of training, we want the loss

00:09:44.616 --> 00:09:47.006
for our network to decrease and

00:09:47.066 --> 00:09:48.796
the accuracy of the network to

00:09:48.796 --> 00:09:49.186
increase.

00:09:49.576 --> 00:09:51.526
And we continue training until

00:09:51.526 --> 00:09:52.736
the loss falls below a

00:09:52.736 --> 00:09:54.756
particular threshold and the

00:09:54.756 --> 00:09:56.916
accuracy of our network reaches

00:09:56.916 --> 00:09:57.626
a desired level.

00:09:58.426 --> 00:09:59.666
And then we know that the

00:09:59.666 --> 00:10:01.206
network is fully trained and now

00:10:01.206 --> 00:10:02.496
we can use the computed trained

00:10:02.526 --> 00:10:03.976
parameters for inference.

00:10:04.826 --> 00:10:06.196
And now, let's take a look at

00:10:06.666 --> 00:10:08.286
the steps necessary to train a

00:10:08.286 --> 00:10:10.046
neural network using the Metal

00:10:10.046 --> 00:10:11.336
Performance Shaders Framework.

00:10:11.516 --> 00:10:13.116
Neural networks are often

00:10:13.116 --> 00:10:14.826
described using graph

00:10:14.826 --> 00:10:15.516
abstraction.

00:10:15.816 --> 00:10:17.726
So, in MPS, we enable you to

00:10:17.726 --> 00:10:18.936
describe your networks as a

00:10:18.936 --> 00:10:19.316
graph.

00:10:20.996 --> 00:10:22.326
So, the first step is to create

00:10:22.326 --> 00:10:22.916
a training graph.

00:10:24.526 --> 00:10:25.526
Then we need to prepare our

00:10:25.526 --> 00:10:26.216
inputs.

00:10:26.456 --> 00:10:27.876
We need to specify weights.

00:10:28.066 --> 00:10:29.306
And then we execute the graph.

00:10:29.476 --> 00:10:31.186
So, we run the forward paths,

00:10:31.346 --> 00:10:33.166
compute loss, do the gradient

00:10:33.226 --> 00:10:34.496
pass, and update weights.

00:10:35.266 --> 00:10:37.086
And training is an iterative

00:10:37.176 --> 00:10:37.646
process.

00:10:38.706 --> 00:10:40.486
It can take many iterations to

00:10:40.486 --> 00:10:41.246
train a network.

00:10:41.246 --> 00:10:42.936
So, we'll also need to know when

00:10:42.936 --> 00:10:43.806
we can stop training.

00:10:43.806 --> 00:10:45.306
So, let's now discuss each one

00:10:45.306 --> 00:10:47.046
of these topics in more detail.

00:10:47.576 --> 00:10:51.246
Let's start with creating a

00:10:51.936 --> 00:10:52.486
training graph.

00:10:52.806 --> 00:10:54.626
So, as I said, in MPS, we enable

00:10:54.626 --> 00:10:56.236
you to describe your networks as

00:10:56.236 --> 00:10:57.636
a graph using a neural network

00:10:57.636 --> 00:10:58.336
graph API.

00:10:58.886 --> 00:11:00.296
So, here we again have a

00:11:00.296 --> 00:11:02.106
visualization of our

00:11:02.106 --> 00:11:03.576
handwritten, digit recognition

00:11:03.576 --> 00:11:03.966
network.

00:11:04.656 --> 00:11:06.276
But in this visualization, you

00:11:06.276 --> 00:11:08.026
can also see the image notes.

00:11:08.026 --> 00:11:09.846
They're the small white notes.

00:11:10.576 --> 00:11:12.666
The image notes are for your

00:11:12.666 --> 00:11:12.936
data.

00:11:13.426 --> 00:11:15.026
For your input, your outputs,

00:11:15.026 --> 00:11:16.356
and all of the intermediate

00:11:17.136 --> 00:11:17.346
results.

00:11:18.256 --> 00:11:20.576
They describe how data moves

00:11:20.576 --> 00:11:21.866
between different operations.

00:11:22.766 --> 00:11:24.026
And then, operations on the

00:11:24.026 --> 00:11:25.786
data, like convolution and

00:11:25.816 --> 00:11:28.976
pooling, are described with your

00:11:29.276 --> 00:11:29.826
filter nodes.

00:11:30.626 --> 00:11:32.416
We support all of the nodes

00:11:32.416 --> 00:11:34.236
necessary to create commonly

00:11:34.236 --> 00:11:35.716
used neural networks.

00:11:36.306 --> 00:11:38.316
And now, let's take a look at

00:11:38.316 --> 00:11:39.716
how easy it is to use the neural

00:11:39.716 --> 00:11:40.696
network graph API.

00:11:41.346 --> 00:11:43.916
So, here's an example of how you

00:11:43.916 --> 00:11:47.006
can create an MPSImageNode using

00:11:47.006 --> 00:11:48.136
the neural network graph API.

00:11:48.786 --> 00:11:49.906
And this is how you would create

00:11:49.906 --> 00:11:51.656
a convolution node using the

00:11:51.656 --> 00:11:52.296
graph API.

00:11:53.416 --> 00:11:55.676
And now, for every forward node,

00:11:56.046 --> 00:11:58.016
we support a corresponding

00:11:58.066 --> 00:11:59.416
gradient node for training.

00:11:59.796 --> 00:12:01.356
It takes a single line of code

00:12:01.686 --> 00:12:03.176
to create a gradient node from

00:12:03.176 --> 00:12:03.856
the forward node.

00:12:04.226 --> 00:12:05.976
So, here is an example of how

00:12:05.976 --> 00:12:07.296
you can create a gradient

00:12:07.296 --> 00:12:08.746
convolution node from the

00:12:08.746 --> 00:12:10.266
convolution node.

00:12:12.656 --> 00:12:14.376
And now, let's build an entire

00:12:14.376 --> 00:12:14.796
graph.

00:12:16.206 --> 00:12:18.016
So, here we have a very small

00:12:18.016 --> 00:12:18.476
network.

00:12:18.476 --> 00:12:19.946
We have a convolution node

00:12:19.946 --> 00:12:21.226
followed by a pooling node,

00:12:21.356 --> 00:12:22.776
followed by another convolution

00:12:22.776 --> 00:12:22.976
node.

00:12:23.916 --> 00:12:25.116
So, how do we connect these

00:12:25.116 --> 00:12:27.356
nodes into a graph?

00:12:27.966 --> 00:12:28.886
So, that's easy.

00:12:29.636 --> 00:12:32.396
We take the result node of --

00:12:32.396 --> 00:12:34.796
the result image of one node and

00:12:34.836 --> 00:12:36.926
pass it as a source image to the

00:12:36.926 --> 00:12:37.776
subsequent node.

00:12:38.976 --> 00:12:40.306
And here we have an entire

00:12:40.306 --> 00:12:41.576
connected graph.

00:12:42.716 --> 00:12:44.416
And now, let's build a training

00:12:44.416 --> 00:12:44.806
graph.

00:12:45.286 --> 00:12:47.776
So first, we need to add a loss

00:12:47.776 --> 00:12:49.956
node to the graph.

00:12:50.056 --> 00:12:51.516
And now, let's add some gradient

00:12:51.516 --> 00:12:51.776
nodes.

00:12:52.136 --> 00:12:53.386
So, as I said, it takes a single

00:12:53.386 --> 00:12:54.866
line of code to create a

00:12:54.866 --> 00:12:56.766
gradient node from its

00:12:56.766 --> 00:12:58.466
corresponding forward node.

00:12:58.466 --> 00:12:59.526
And then we connect them as

00:12:59.526 --> 00:13:01.376
previously, and now you have a

00:13:01.376 --> 00:13:03.206
complete training graph.

00:13:05.156 --> 00:13:06.766
So, as you can see from this

00:13:06.766 --> 00:13:09.576
example, the graph API is very

00:13:09.576 --> 00:13:10.396
simple to use.

00:13:11.666 --> 00:13:12.586
The graph does a lot for you

00:13:12.586 --> 00:13:13.236
automatically.

00:13:13.346 --> 00:13:14.906
It manages all the intermediate

00:13:14.906 --> 00:13:17.786
results, and even the output

00:13:17.786 --> 00:13:18.156
image.

00:13:19.246 --> 00:13:21.476
It minimizes the memory

00:13:21.476 --> 00:13:23.326
footprint of your networks, by

00:13:23.656 --> 00:13:25.516
aliasing memory for all your

00:13:25.516 --> 00:13:27.456
intermediate images, using Metal

00:13:28.386 --> 00:13:28.926
heaps.

00:13:28.926 --> 00:13:30.386
It can also fuse graph nodes.

00:13:30.386 --> 00:13:32.386
For example, it can fuse batch

00:13:32.386 --> 00:13:34.186
normalization and neural nodes.

00:13:34.626 --> 00:13:36.306
And it can optimize away nodes.

00:13:36.736 --> 00:13:38.056
For example, it optimizes the

00:13:38.056 --> 00:13:39.186
way you can cut nation nodes.

00:13:40.416 --> 00:13:41.996
The graph also automatically

00:13:41.996 --> 00:13:44.046
handles padding and manages

00:13:44.046 --> 00:13:45.336
state objects for you, which we

00:13:45.336 --> 00:13:46.276
will discuss later in this

00:13:46.276 --> 00:13:46.656
session.

00:13:47.456 --> 00:13:49.186
So, please take advantage of the

00:13:49.186 --> 00:13:49.876
graph API.

00:13:54.196 --> 00:13:55.906
So, now that we know how to

00:13:55.906 --> 00:13:57.736
create a training graph, let's

00:13:57.736 --> 00:14:00.266
now take a look at the inputs we

00:14:00.266 --> 00:14:01.386
need to pass to the graph.

00:14:02.556 --> 00:14:03.966
And for this, let's take a look

00:14:03.966 --> 00:14:05.546
at the encode call we will use

00:14:05.816 --> 00:14:07.266
to encode the graph to the GPU.

00:14:08.296 --> 00:14:09.886
So, as I already mentioned,

00:14:09.886 --> 00:14:11.636
we're not going to send in one

00:14:11.676 --> 00:14:13.026
image at a time for training.

00:14:13.156 --> 00:14:14.926
We're going to operate on groups

00:14:14.926 --> 00:14:16.236
or batches of images.

00:14:16.576 --> 00:14:17.856
So, one of the inputs to the

00:14:17.856 --> 00:14:19.536
graph, is a batch of images.

00:14:20.926 --> 00:14:22.746
And as you recall, for every

00:14:22.746 --> 00:14:24.566
batch of images, we also need a

00:14:24.566 --> 00:14:26.346
corresponding batch of labels

00:14:26.346 --> 00:14:27.446
for loss computation.

00:14:28.696 --> 00:14:31.836
The labels for loss computation

00:14:31.916 --> 00:14:33.366
are passed to the graph as

00:14:33.446 --> 00:14:33.786
states.

00:14:34.226 --> 00:14:36.536
So, the code call also takes a

00:14:36.536 --> 00:14:37.786
batch of states as input.

00:14:38.756 --> 00:14:40.496
And now, let's talk about these

00:14:40.496 --> 00:14:41.406
batches and states.

00:14:41.936 --> 00:14:42.456
What are they?

00:14:42.596 --> 00:14:43.516
Let's start with batches.

00:14:44.226 --> 00:14:46.496
So, batches are just arrays of

00:14:46.756 --> 00:14:47.996
images or states.

00:14:48.286 --> 00:14:49.416
We've added them this year

00:14:49.416 --> 00:14:50.676
specifically to support

00:14:50.676 --> 00:14:51.086
training.

00:14:51.596 --> 00:14:53.416
There are two new MPS types for

00:14:53.416 --> 00:14:56.376
you to use: MPSImageBatch and

00:14:56.376 --> 00:14:57.696
MPSStateBatch.

00:14:58.266 --> 00:15:00.106
So, here's an example of how you

00:15:00.106 --> 00:15:01.966
can create a single image, using

00:15:01.966 --> 00:15:02.696
our API.

00:15:04.136 --> 00:15:05.536
So, here we're creating one from

00:15:05.536 --> 00:15:06.736
an existing Metal texture.

00:15:07.306 --> 00:15:09.646
And this is an example of how

00:15:09.646 --> 00:15:11.206
you can create a batch of

00:15:11.206 --> 00:15:13.676
images, using our API and append

00:15:13.676 --> 00:15:15.116
a new image to the batch, so you

00:15:15.116 --> 00:15:18.566
can pass it to the graph.

00:15:18.566 --> 00:15:20.316
And now, what are state objects?

00:15:20.426 --> 00:15:23.976
An MPS state is an opaque data

00:15:23.976 --> 00:15:24.446
container.

00:15:24.446 --> 00:15:27.356
In training, it's frequently

00:15:27.356 --> 00:15:29.366
used to capture a state of a

00:15:29.366 --> 00:15:32.366
forward node, when it's called.

00:15:32.916 --> 00:15:35.206
And so, it can later be used by

00:15:35.206 --> 00:15:35.996
the gradient node.

00:15:37.106 --> 00:15:39.036
So, the graph manages all of the

00:15:39.036 --> 00:15:39.776
state objects.

00:15:39.776 --> 00:15:41.056
So, as a developer, you

00:15:41.056 --> 00:15:42.286
generally don't need to worry

00:15:42.286 --> 00:15:42.996
about states.

00:15:43.306 --> 00:15:44.366
But it's nice to know how they

00:15:44.366 --> 00:15:44.716
work.

00:15:44.876 --> 00:15:46.196
So, let's use a specific

00:15:46.196 --> 00:15:46.686
example.

00:15:48.776 --> 00:15:50.116
So, let's go back to our

00:15:50.116 --> 00:15:51.476
handwritten digit recognition

00:15:51.476 --> 00:15:51.876
network.

00:15:52.886 --> 00:15:55.096
And take a look specifically at

00:15:55.096 --> 00:15:56.416
the drop-out and drop-out

00:15:56.416 --> 00:15:57.956
gradient nodes.

00:16:00.196 --> 00:16:02.926
The forward drop-out node drops,

00:16:02.996 --> 00:16:04.636
or it zeroes out, values in its

00:16:04.636 --> 00:16:05.676
input, with a certain

00:16:05.676 --> 00:16:06.366
probability.

00:16:06.996 --> 00:16:08.736
And then, the dropout state

00:16:08.736 --> 00:16:10.826
object captures information

00:16:10.826 --> 00:16:11.926
about the forward drop-out

00:16:11.926 --> 00:16:14.486
operation, so it can later be

00:16:14.486 --> 00:16:16.186
used by the drop-out gradient

00:16:16.186 --> 00:16:18.576
node because it used to zero out

00:16:19.886 --> 00:16:22.376
values in its input gradient at

00:16:22.376 --> 00:16:24.156
the exact same locations as was

00:16:24.156 --> 00:16:25.316
zeroed out by the forward

00:16:25.316 --> 00:16:25.756
operation.

00:16:26.176 --> 00:16:30.056
So, as I said, you don't

00:16:30.056 --> 00:16:31.126
generally need to worry about

00:16:31.126 --> 00:16:32.216
states, because the graph

00:16:32.216 --> 00:16:32.946
manages them.

00:16:33.416 --> 00:16:35.396
But because the labels for loss

00:16:35.396 --> 00:16:37.346
computation are passed as states

00:16:37.346 --> 00:16:39.786
to the graph, and because they

00:16:39.786 --> 00:16:40.756
require user input.

00:16:40.756 --> 00:16:42.396
So, that's your ground truth or

00:16:42.396 --> 00:16:43.516
correct results.

00:16:44.016 --> 00:16:45.536
You need to create a batch of

00:16:45.536 --> 00:16:47.436
labels for loss computation and

00:16:47.436 --> 00:16:49.036
pass this batch as input to the

00:16:49.776 --> 00:16:50.026
graph.

00:16:50.186 --> 00:16:51.436
So, this is an example of how

00:16:51.436 --> 00:16:53.106
you would create a single label

00:16:53.506 --> 00:16:54.926
for loss computation.

00:16:55.276 --> 00:16:56.676
You first need to create a loss

00:16:56.746 --> 00:16:58.856
data descriptor which describes

00:16:58.856 --> 00:17:00.326
how the label's data is laid out

00:17:00.326 --> 00:17:00.836
in memory.

00:17:01.246 --> 00:17:03.886
And then you need to create an

00:17:03.886 --> 00:17:05.665
MPSCNNLossLabel object, with

00:17:05.665 --> 00:17:06.415
this descriptor.

00:17:06.976 --> 00:17:09.445
And then you create a batch of

00:17:09.445 --> 00:17:11.586
these for training, and when the

00:17:11.586 --> 00:17:13.066
GPU's done running the graph,

00:17:13.506 --> 00:17:15.236
your batch of labels will

00:17:15.236 --> 00:17:16.675
contain the per image loss

00:17:16.675 --> 00:17:18.626
values for the batch.

00:17:18.955 --> 00:17:20.006
And you can inspect these

00:17:20.006 --> 00:17:21.496
values, or you can compute a

00:17:21.496 --> 00:17:22.955
single value across the batch

00:17:22.955 --> 00:17:23.976
and inspect that value.

00:17:27.356 --> 00:17:28.946
So, now that we have a training

00:17:28.946 --> 00:17:30.626
graph and we talked about how to

00:17:30.796 --> 00:17:32.706
provide inputs to our graph,

00:17:32.706 --> 00:17:34.186
let's talk about how to provide

00:17:34.186 --> 00:17:35.726
weights to the graph nodes that

00:17:35.726 --> 00:17:36.316
require weights.

00:17:38.336 --> 00:17:40.606
The only way to provide weights

00:17:40.606 --> 00:17:42.546
to convolution fully connected,

00:17:42.786 --> 00:17:45.456
batch normalization and instance

00:17:45.456 --> 00:17:47.356
normalization nodes, is through

00:17:47.356 --> 00:17:48.966
data source provider protocols.

00:17:50.146 --> 00:17:52.216
This is an example of how to

00:17:52.216 --> 00:17:54.116
create a convolution node, with

00:17:54.116 --> 00:17:55.166
a data source provider.

00:17:56.206 --> 00:17:58.636
You need to implement a class

00:17:58.636 --> 00:18:00.106
that conforms to the protocol.

00:18:00.416 --> 00:18:01.706
We call it MyWeights in this

00:18:01.706 --> 00:18:02.116
example.

00:18:02.596 --> 00:18:06.886
Data source providers are very

00:18:06.886 --> 00:18:08.066
useful in many ways.

00:18:08.646 --> 00:18:10.996
For example, if you have many

00:18:10.996 --> 00:18:12.136
convolution nodes in your

00:18:12.136 --> 00:18:14.486
network, the overall size of the

00:18:14.486 --> 00:18:15.526
weights for the network can be

00:18:15.526 --> 00:18:16.406
quite considerable.

00:18:17.006 --> 00:18:18.606
And we do not want the weights

00:18:18.606 --> 00:18:19.956
for all of your convolution

00:18:19.956 --> 00:18:21.486
nodes to be in memory all at the

00:18:21.486 --> 00:18:22.086
same time.

00:18:22.806 --> 00:18:24.206
We want to keep the memory

00:18:24.206 --> 00:18:25.806
footprints of your networks as

00:18:25.806 --> 00:18:26.566
low as possible.

00:18:27.466 --> 00:18:28.876
And data source providers come

00:18:28.876 --> 00:18:30.746
into play here because they

00:18:30.746 --> 00:18:33.306
provide just in time loading and

00:18:33.466 --> 00:18:34.556
purging of weights data.

00:18:35.726 --> 00:18:37.076
So, we load the weights for one

00:18:37.076 --> 00:18:39.186
convolution kernel, when we

00:18:39.186 --> 00:18:39.846
process it.

00:18:40.076 --> 00:18:41.626
And then we purge them before

00:18:41.626 --> 00:18:43.436
moving on the next convolution.

00:18:43.906 --> 00:18:47.726
So, here's an implementation of

00:18:47.726 --> 00:18:48.366
MyWeights.

00:18:49.516 --> 00:18:51.056
You need to provide an

00:18:51.246 --> 00:18:52.996
initialization method that is

00:18:52.996 --> 00:18:54.646
responsible for pulling in

00:18:54.646 --> 00:18:55.856
memory and making it ready.

00:18:56.226 --> 00:18:57.936
And then the graph will call the

00:18:57.936 --> 00:18:58.686
load function.

00:18:59.136 --> 00:19:00.946
And then when the purge method

00:19:00.946 --> 00:19:02.406
is called, you can release the

00:19:02.406 --> 00:19:02.656
weights.

00:19:03.706 --> 00:19:05.366
Data source providers are also

00:19:05.366 --> 00:19:06.866
essential for training, and we

00:19:06.866 --> 00:19:08.206
will discuss this later in this

00:19:08.206 --> 00:19:08.526
session.

00:19:11.746 --> 00:19:13.216
So, now that we have a training

00:19:13.216 --> 00:19:14.666
graph and we've prepared our

00:19:14.666 --> 00:19:16.146
inputs and specified weights,

00:19:16.546 --> 00:19:17.976
we're ready to execute the graph

00:19:17.976 --> 00:19:18.566
on the GPU.

00:19:20.486 --> 00:19:21.616
To change the [inaudible] graph

00:19:21.616 --> 00:19:23.396
on the GPU, we first need to do

00:19:23.656 --> 00:19:24.826
the usual Metal setup.

00:19:25.466 --> 00:19:26.846
We need to initialize our

00:19:26.846 --> 00:19:27.686
training graph.

00:19:27.956 --> 00:19:29.516
So, we have prepared our inputs.

00:19:29.766 --> 00:19:31.326
And now, let's train a network

00:19:31.326 --> 00:19:34.836
on the GPU.

00:19:35.206 --> 00:19:36.326
Training is an iterative

00:19:36.326 --> 00:19:36.696
process.

00:19:38.156 --> 00:19:39.466
So, we want to set up a training

00:19:39.466 --> 00:19:39.666
loop.

00:19:40.316 --> 00:19:42.266
And we usually want to execute

00:19:42.266 --> 00:19:43.536
our graph over a number of

00:19:43.536 --> 00:19:44.186
EPOCHS.

00:19:44.856 --> 00:19:46.336
The number of EPOCHS is the

00:19:46.566 --> 00:19:48.606
total number -- is the number of

00:19:48.606 --> 00:19:49.986
times we want to iterate over

00:19:49.986 --> 00:19:51.906
our entire data set.

00:19:51.906 --> 00:19:53.486
And we want there to be multiple

00:19:53.656 --> 00:19:54.886
iterations in each EPOCH.

00:19:55.006 --> 00:19:56.646
So, the number of iterations is

00:19:56.646 --> 00:19:57.966
the total number of images in

00:19:57.966 --> 00:19:59.546
your data set divided by batch

00:19:59.546 --> 00:20:01.216
size, like 32 or 64.

00:20:02.256 --> 00:20:03.426
And now, let's take a look at

00:20:03.426 --> 00:20:04.766
each training iteration.

00:20:06.296 --> 00:20:10.126
In each training iteration, we

00:20:10.126 --> 00:20:11.506
encode a batch of images for

00:20:11.506 --> 00:20:11.906
training.

00:20:13.146 --> 00:20:14.976
But we don't want the CPU to

00:20:14.976 --> 00:20:17.116
wait for the GPU to finish

00:20:17.116 --> 00:20:19.616
running one run of the graph,

00:20:20.036 --> 00:20:22.226
with one batch of images before

00:20:22.226 --> 00:20:24.296
the CPU can start encoding

00:20:24.296 --> 00:20:25.526
commands to the command buffer

00:20:25.526 --> 00:20:26.786
for the next run of the graph.

00:20:27.536 --> 00:20:30.056
We want the CPU and the GPU to

00:20:30.056 --> 00:20:30.976
work concurrently.

00:20:31.466 --> 00:20:32.566
And for this, we're going to use

00:20:32.666 --> 00:20:33.486
double buffering.

00:20:34.106 --> 00:20:36.336
So, in this setup, we're going

00:20:36.336 --> 00:20:38.936
to create a counting semaphore

00:20:39.056 --> 00:20:40.426
with an initial value of 2.

00:20:40.566 --> 00:20:42.266
It's because we want only two

00:20:42.266 --> 00:20:43.586
encodes to be in flight at the

00:20:43.586 --> 00:20:44.146
same time.

00:20:45.726 --> 00:20:46.916
And then when we enter the

00:20:46.916 --> 00:20:48.496
training iteration function,

00:20:48.796 --> 00:20:50.066
we're going to call weight on

00:20:50.066 --> 00:20:50.696
the semaphore.

00:20:50.846 --> 00:20:51.866
That's decrementing it.

00:20:52.656 --> 00:20:54.596
So, if the value of the count

00:20:54.596 --> 00:20:55.816
has already been decremented to

00:20:55.816 --> 00:20:57.016
zero, we wait.

00:20:57.016 --> 00:20:58.126
Otherwise, we continue.

00:20:59.176 --> 00:21:00.966
And then we encode our graph,

00:21:01.346 --> 00:21:02.946
and the encode call returns

00:21:02.976 --> 00:21:03.566
immediately.

00:21:04.196 --> 00:21:06.006
And a user specified callback is

00:21:06.006 --> 00:21:07.886
called, when the GPU is done

00:21:07.886 --> 00:21:08.426
running the graph.

00:21:09.086 --> 00:21:10.136
So, now we know.

00:21:10.236 --> 00:21:11.446
The GPU is done running the

00:21:11.446 --> 00:21:13.896
graph, and the CPU can continue

00:21:14.316 --> 00:21:16.566
encoding more work to the GPU,

00:21:17.746 --> 00:21:19.226
work that was previously waiting

00:21:19.226 --> 00:21:19.916
on the semaphore.

00:21:20.936 --> 00:21:22.386
So, why are we using double

00:21:22.386 --> 00:21:22.906
buffering?

00:21:22.906 --> 00:21:24.856
Why not encode more runs of the

00:21:24.856 --> 00:21:27.536
graph, to the GPU concurrently?

00:21:28.766 --> 00:21:30.336
Well, it takes a lot less time

00:21:30.396 --> 00:21:31.446
to encode commands to the

00:21:31.446 --> 00:21:33.066
command buffer, than to run the

00:21:33.066 --> 00:21:33.476
graph.

00:21:33.786 --> 00:21:35.146
So, we don't want to encode too

00:21:35.146 --> 00:21:36.446
many runs of the graph

00:21:36.446 --> 00:21:37.926
concurrently to minimize memory

00:21:37.926 --> 00:21:38.386
footprint.

00:21:38.656 --> 00:21:42.576
Okay, we've talked about

00:21:42.576 --> 00:21:43.676
executing the graph.

00:21:43.936 --> 00:21:45.396
When we execute the graph, we do

00:21:45.396 --> 00:21:47.146
the forward pass, we compute

00:21:47.146 --> 00:21:49.206
loss, we do the gradient pass,

00:21:49.356 --> 00:21:50.756
and the graph will also update

00:21:50.756 --> 00:21:51.196
weights.

00:21:51.676 --> 00:21:53.136
So now, let's talk about weight

00:21:53.176 --> 00:21:53.516
updates.

00:21:54.196 --> 00:21:57.286
As I mentioned, data source

00:21:57.286 --> 00:22:00.176
providers are essential for

00:22:00.176 --> 00:22:00.616
training.

00:22:01.456 --> 00:22:02.946
All of your weight updates need

00:22:02.946 --> 00:22:04.246
to happen through an optional

00:22:04.246 --> 00:22:05.926
update method on a data source

00:22:05.926 --> 00:22:06.346
provider.

00:22:07.806 --> 00:22:08.996
The graph will call the update

00:22:08.996 --> 00:22:10.526
method automatically.

00:22:11.116 --> 00:22:12.256
So, what does the weight update

00:22:12.256 --> 00:22:13.486
step actually involve?

00:22:13.706 --> 00:22:14.906
Let's take a look.

00:22:16.316 --> 00:22:18.296
So, recall that we're computing

00:22:18.296 --> 00:22:19.706
gradients during the gradient

00:22:19.926 --> 00:22:21.526
pass that we can apply small

00:22:21.566 --> 00:22:22.986
deltas to the weights, in each

00:22:22.986 --> 00:22:23.976
situation of training.

00:22:25.546 --> 00:22:26.926
How these deltas are applied to

00:22:26.926 --> 00:22:29.296
the weights, is described by an

00:22:29.296 --> 00:22:29.996
optimizer.

00:22:30.536 --> 00:22:32.436
It's just a function that takes

00:22:32.516 --> 00:22:34.186
the old weights, the computed

00:22:34.186 --> 00:22:36.836
gradients as input, and produces

00:22:37.696 --> 00:22:39.246
updated weights as outputs.

00:22:40.336 --> 00:22:43.096
You will use an optimizer in the

00:22:43.096 --> 00:22:44.286
update method of your data

00:22:44.286 --> 00:22:44.996
source provider.

00:22:45.976 --> 00:22:47.226
And we support a number of

00:22:47.226 --> 00:22:48.476
different variants of the weight

00:22:48.476 --> 00:22:50.356
update step on the GPU,

00:22:50.556 --> 00:22:52.116
including Adam, Stochastic

00:22:52.116 --> 00:22:53.786
Gradient Descent, and RMSProp.

00:22:54.896 --> 00:22:57.376
And you can even define your own

00:22:57.436 --> 00:22:58.946
custom update weight step if you

00:22:58.946 --> 00:22:59.326
prefer.

00:23:00.046 --> 00:23:01.276
So now, let's take a look at how

00:23:01.276 --> 00:23:04.336
to use an optimizer in MPS.

00:23:05.436 --> 00:23:07.526
So, recall that your data source

00:23:07.526 --> 00:23:09.276
provider has an init method.

00:23:09.676 --> 00:23:11.066
This is where you want to create

00:23:11.066 --> 00:23:12.726
your optimizer because you only

00:23:12.726 --> 00:23:13.746
want to create it once.

00:23:14.356 --> 00:23:16.626
And now, let's take a look at

00:23:16.626 --> 00:23:18.176
the implementation of our update

00:23:18.176 --> 00:23:18.496
method.

00:23:19.016 --> 00:23:21.536
The update method receives the

00:23:21.536 --> 00:23:23.446
source state and gradient state

00:23:23.446 --> 00:23:23.956
as inputs.

00:23:24.676 --> 00:23:27.726
So, the source state contains

00:23:27.726 --> 00:23:29.176
the old weights, the gradient

00:23:29.176 --> 00:23:30.546
state contains the computed

00:23:30.546 --> 00:23:32.706
gradients, and now we can encode

00:23:32.706 --> 00:23:34.736
our optimizer with this data,

00:23:34.736 --> 00:23:36.666
and the last step is to return

00:23:36.666 --> 00:23:38.276
the source state, which now has

00:23:38.326 --> 00:23:39.226
the update weights.

00:23:39.636 --> 00:23:40.466
So, pretty simple.

00:23:40.936 --> 00:23:44.936
And now we have just one more

00:23:44.936 --> 00:23:45.856
step to discuss.

00:23:46.116 --> 00:23:47.546
So, as I said, training is an

00:23:47.546 --> 00:23:48.526
iterative process.

00:23:48.526 --> 00:23:50.416
It can take many iterations to

00:23:50.416 --> 00:23:51.186
train a network.

00:23:52.396 --> 00:23:53.906
And you will need to know when

00:23:53.906 --> 00:23:54.676
to stop training.

00:23:55.316 --> 00:23:57.036
So, let's now discuss how can

00:23:57.036 --> 00:23:58.716
you make this decision in the

00:23:58.716 --> 00:24:00.586
context of your training loop?

00:24:02.716 --> 00:24:04.546
So, here we again have our

00:24:04.546 --> 00:24:05.526
training loop, where we're

00:24:05.526 --> 00:24:06.836
training a neural network for a

00:24:06.836 --> 00:24:07.696
number of EPOCHS.

00:24:09.276 --> 00:24:10.826
To check whether you can stop

00:24:10.826 --> 00:24:11.926
training, you need to have a

00:24:11.926 --> 00:24:13.076
test set of images.

00:24:13.426 --> 00:24:14.726
So, a test set of images

00:24:15.046 --> 00:24:17.056
contains images that are not

00:24:17.056 --> 00:24:17.866
used for training.

00:24:18.126 --> 00:24:19.976
They're only used to evaluate

00:24:19.976 --> 00:24:21.546
the accuracy of your network.

00:24:22.486 --> 00:24:24.336
So, after each EPOCH, you can

00:24:24.446 --> 00:24:26.426
optionally wait for the graph to

00:24:27.096 --> 00:24:28.686
-- for the GPU to stop running

00:24:28.686 --> 00:24:31.056
the graph, and then you can use

00:24:31.306 --> 00:24:33.066
the current trained parameters

00:24:33.846 --> 00:24:35.236
to initialize an inference

00:24:35.236 --> 00:24:35.646
network.

00:24:36.916 --> 00:24:38.216
And then you can run this

00:24:38.216 --> 00:24:39.636
inference network on your test

00:24:39.676 --> 00:24:41.476
set, and you can optionally stop

00:24:41.476 --> 00:24:43.356
training when the accuracy of

00:24:43.356 --> 00:24:44.906
your network on this test set,

00:24:45.216 --> 00:24:46.426
reaches a particular level.

00:24:49.116 --> 00:24:51.486
So, now that we've discussed all

00:24:51.486 --> 00:24:53.816
of the steps that are necessary

00:24:53.816 --> 00:24:54.996
to train a neural network in

00:24:55.036 --> 00:24:57.026
MPS, it's time for a demo.

00:24:58.416 --> 00:25:00.326
So, as was already mentioned in

00:25:00.326 --> 00:25:01.786
the platform State of the Union,

00:25:02.776 --> 00:25:03.826
the Metal Performance Shaders

00:25:03.826 --> 00:25:07.726
Framework powers Core ML, Create

00:25:07.726 --> 00:25:09.026
ML, and Turi Create.

00:25:09.926 --> 00:25:12.636
To Turi Create is an easy to

00:25:12.636 --> 00:25:15.746
use, flexible, high-performance

00:25:15.746 --> 00:25:17.236
tool set for creating Core ML

00:25:17.236 --> 00:25:20.426
models for tasks such as image

00:25:20.426 --> 00:25:22.206
classification, object

00:25:22.206 --> 00:25:24.586
detection, recommendations, and

00:25:24.586 --> 00:25:24.726
more.

00:25:25.276 --> 00:25:26.636
For more information on Turi

00:25:26.636 --> 00:25:28.756
Create, we want to refer you to

00:25:28.756 --> 00:25:29.996
the A Guide to Turi Create

00:25:29.996 --> 00:25:30.666
Session Video.

00:25:32.336 --> 00:25:34.196
We've prepared a demo where we

00:25:34.196 --> 00:25:36.516
will be using an -- we'll be

00:25:36.516 --> 00:25:38.056
training an object detection

00:25:38.056 --> 00:25:40.626
network in Turi Create powered

00:25:40.626 --> 00:25:41.566
by MPS.

00:25:41.566 --> 00:25:44.536
As was mentioned in the platform

00:25:44.536 --> 00:25:47.266
State of the Union, this is nine

00:25:47.266 --> 00:25:48.846
times faster than without MPS.

00:25:50.116 --> 00:25:51.636
An object detection network

00:25:52.326 --> 00:25:53.986
draws bounding boxes around

00:25:54.076 --> 00:25:55.166
recognized objects.

00:26:00.696 --> 00:26:05.266
So, in this demo, I will be

00:26:05.266 --> 00:26:06.856
using a MacBook Pro, with a

00:26:06.856 --> 00:26:08.196
connected external GPU.

00:26:08.986 --> 00:26:12.066
I will be running Turi Create on

00:26:12.066 --> 00:26:14.816
the MacBook Pro and I will use

00:26:14.816 --> 00:26:16.526
an external GPU to train the

00:26:16.526 --> 00:26:17.596
network with MPS.

00:26:18.606 --> 00:26:20.276
This is a great example of how

00:26:20.276 --> 00:26:22.196
you can use an external GPU to

00:26:22.196 --> 00:26:23.806
enhance the computational power

00:26:23.806 --> 00:26:24.526
of a MacBook Pro.

00:26:24.526 --> 00:26:26.916
The external GPU we're using is

00:26:26.916 --> 00:26:28.116
an AMD Vega GPU.

00:26:29.006 --> 00:26:30.876
So, in this demo setup, I've

00:26:30.876 --> 00:26:32.486
already imported Turi Create,

00:26:32.956 --> 00:26:34.956
and preloaded the object

00:26:34.956 --> 00:26:36.606
detection network, and a

00:26:36.606 --> 00:26:37.436
training data set.

00:26:37.996 --> 00:26:41.556
So, now let's train this network

00:26:41.556 --> 00:26:42.606
for 10 iterations.

00:26:43.246 --> 00:26:46.206
And now, the entire object

00:26:46.206 --> 00:26:47.456
detection network, all the

00:26:47.456 --> 00:26:49.386
primitives, the optimizer, the

00:26:49.386 --> 00:26:52.906
weight update step, everything

00:26:52.906 --> 00:26:54.706
is running on the external GPU.

00:26:58.206 --> 00:26:59.256
Okay, so we're already done with

00:26:59.296 --> 00:27:01.476
10 iterations of training, it

00:27:01.476 --> 00:27:02.366
would take more than 10

00:27:02.366 --> 00:27:03.496
iterations to train this

00:27:03.496 --> 00:27:04.606
network, and we're not going to

00:27:04.606 --> 00:27:05.576
do this on stage.

00:27:06.116 --> 00:27:07.366
But what I'm going to do right

00:27:07.366 --> 00:27:09.546
now, is to load a network that

00:27:09.546 --> 00:27:11.796
we pretrained in advance, run it

00:27:11.796 --> 00:27:13.246
on a test set of images and

00:27:13.246 --> 00:27:14.696
visualize some of the results.

00:27:14.696 --> 00:27:15.586
So, let's take a look.

00:27:16.216 --> 00:27:20.306
Okay, so here we have a banana,

00:27:20.346 --> 00:27:21.776
that's correctly classified as a

00:27:21.776 --> 00:27:22.286
banana.

00:27:22.656 --> 00:27:24.706
And we have a bounding box, and

00:27:25.036 --> 00:27:26.866
now we have a perfect breakfast

00:27:26.866 --> 00:27:27.976
of a cup of coffee and a

00:27:27.976 --> 00:27:30.796
croissant, and very,

00:27:30.796 --> 00:27:31.496
mean-looking egg.

00:27:32.136 --> 00:27:35.776
Okay, so that's it for the Turi

00:27:35.776 --> 00:27:36.396
Create demo.

00:27:38.516 --> 00:27:43.046
[ Applause ]

00:27:43.546 --> 00:27:47.016
Thank you very much.

00:27:47.556 --> 00:27:49.036
And now, let's switch gears and

00:27:49.036 --> 00:27:50.526
talk about training recurrent

00:27:50.526 --> 00:27:51.266
neural networks.

00:27:51.996 --> 00:27:53.876
But first, let's do a recap of

00:27:53.876 --> 00:27:54.986
what are the recurrent neural

00:27:54.986 --> 00:27:55.396
networks?

00:27:57.296 --> 00:27:58.716
One of the disadvantages of

00:27:58.716 --> 00:28:00.516
convolutional neural networks is

00:28:00.516 --> 00:28:02.906
their inability to remember

00:28:02.906 --> 00:28:03.816
anything that happened

00:28:03.816 --> 00:28:04.366
previously.

00:28:05.336 --> 00:28:07.076
They can take one input, such as

00:28:07.076 --> 00:28:09.776
an image, and generate a single

00:28:09.776 --> 00:28:11.306
output, such as a set of

00:28:11.366 --> 00:28:12.466
probabilities of what is

00:28:12.466 --> 00:28:13.136
depicted in the image.

00:28:13.136 --> 00:28:19.296
RNNs on the other hand, have

00:28:19.296 --> 00:28:19.756
memory.

00:28:19.936 --> 00:28:21.636
And they're good at operating on

00:28:21.636 --> 00:28:23.676
sequences of inputs and outputs.

00:28:24.676 --> 00:28:26.146
For example, they can take one

00:28:26.146 --> 00:28:27.546
set of probabilities, so what is

00:28:27.546 --> 00:28:29.556
depicted in an image, which is

00:28:29.556 --> 00:28:32.026
an output of a CNN, and generate

00:28:32.026 --> 00:28:33.886
a sequence of outputs, which is

00:28:33.886 --> 00:28:35.436
a sequence of words that make up

00:28:35.436 --> 00:28:36.656
a caption for this image.

00:28:37.196 --> 00:28:40.136
They can also take a sequence of

00:28:40.136 --> 00:28:42.336
inputs, such as a sequence of

00:28:42.516 --> 00:28:44.306
words that make up a sentence

00:28:44.536 --> 00:28:45.666
and generate a sequence of

00:28:45.666 --> 00:28:48.996
outputs which is a same sentence

00:28:49.296 --> 00:28:50.586
but translated to a different

00:28:50.586 --> 00:28:51.096
language.

00:28:51.166 --> 00:28:52.166
For example, to ration or

00:28:52.166 --> 00:28:52.596
finish.

00:28:53.186 --> 00:28:56.146
With support, a number of

00:28:56.146 --> 00:28:57.416
different variance of RNNs.

00:28:58.426 --> 00:29:00.306
The most commonly used one is

00:29:00.396 --> 00:29:02.456
the Long Short-Term Memory RNN,

00:29:02.486 --> 00:29:03.606
or LSTM for short.

00:29:04.636 --> 00:29:06.706
In our last year's WWDC Session,

00:29:06.946 --> 00:29:08.596
we talked extensively about the

00:29:08.596 --> 00:29:10.896
gates inside LSTM and walked

00:29:11.006 --> 00:29:12.686
through a LSTM inference

00:29:12.686 --> 00:29:13.116
example.

00:29:13.816 --> 00:29:15.536
So, please refer to that session

00:29:15.596 --> 00:29:17.526
for more information on LSTM

00:29:17.526 --> 00:29:17.996
inference.

00:29:19.296 --> 00:29:21.526
This year, we've added support

00:29:21.526 --> 00:29:23.176
for training, for all of these

00:29:23.176 --> 00:29:24.176
variants of RNNs.

00:29:25.256 --> 00:29:27.106
And in this session, I'm going

00:29:27.106 --> 00:29:28.826
to talk about training LSTMs.

00:29:28.826 --> 00:29:33.606
So, let's use a specific

00:29:33.606 --> 00:29:34.086
example.

00:29:34.306 --> 00:29:36.026
So, here we have an activity

00:29:36.026 --> 00:29:38.586
classifier network which takes

00:29:38.656 --> 00:29:40.476
motion sensory data as input.

00:29:40.476 --> 00:29:42.206
For example, reading some

00:29:42.206 --> 00:29:44.146
sensors like an accelerometer or

00:29:44.146 --> 00:29:44.826
a gyroscope.

00:29:45.306 --> 00:29:46.936
And then the network uses this

00:29:46.936 --> 00:29:49.096
data to identify a physical

00:29:49.096 --> 00:29:50.706
activity performed by the user.

00:29:51.226 --> 00:29:52.716
So, for example, we want to know

00:29:52.716 --> 00:29:55.296
if a user is cycling, skiing, or

00:29:55.296 --> 00:29:55.886
walking.

00:29:56.476 --> 00:30:00.676
As you can see, this network is

00:30:00.676 --> 00:30:03.416
set up in an interesting way.

00:30:03.636 --> 00:30:07.496
So, it contains a series of CNN

00:30:07.496 --> 00:30:09.556
primitives, followed by LSTM

00:30:09.556 --> 00:30:11.656
primitive, followed by more CNN

00:30:11.656 --> 00:30:12.096
primitives.

00:30:12.666 --> 00:30:13.986
So, why is it set up this way?

00:30:13.986 --> 00:30:15.326
Let's take a look.

00:30:16.296 --> 00:30:18.716
So, even though our input is

00:30:18.716 --> 00:30:21.786
sensor data, it's represented by

00:30:21.786 --> 00:30:23.696
a batch of 1D images with six

00:30:23.696 --> 00:30:24.336
feature channels.

00:30:24.336 --> 00:30:26.466
So, one feature channel for

00:30:26.576 --> 00:30:28.466
access in the accelerometer and

00:30:28.466 --> 00:30:29.386
gyroscope readings.

00:30:30.716 --> 00:30:33.296
And each 1D image has 2,000

00:30:33.296 --> 00:30:33.676
pixels.

00:30:33.936 --> 00:30:35.516
And you can think of them as

00:30:36.206 --> 00:30:38.176
samples in time because the

00:30:38.386 --> 00:30:39.706
activity we're trying to

00:30:39.706 --> 00:30:41.546
identify, occurs over time.

00:30:43.346 --> 00:30:46.656
And then we pass these images

00:30:46.656 --> 00:30:47.976
through a 1D convolution

00:30:47.976 --> 00:30:51.026
primitive which compresses these

00:30:51.086 --> 00:30:53.186
2,000 samples, to just 20

00:30:53.186 --> 00:30:53.626
samples.

00:30:54.346 --> 00:30:57.236
But it expends a number of

00:30:57.236 --> 00:30:59.316
feature channels, because -- so,

00:30:59.366 --> 00:31:01.256
we're not losing any features in

00:31:01.836 --> 00:31:02.706
the data.

00:31:03.276 --> 00:31:04.446
And then, this new

00:31:04.446 --> 00:31:06.426
representation of the data, is

00:31:06.426 --> 00:31:08.116
passed to LSTM primitive as a

00:31:08.116 --> 00:31:09.306
sequence of lengths 20.

00:31:10.146 --> 00:31:11.826
And we ran LSTM for 20

00:31:11.826 --> 00:31:12.376
iterations.

00:31:12.936 --> 00:31:14.506
So, our LSTM is operating on a

00:31:14.506 --> 00:31:16.106
sequence of lengths 20 instead

00:31:16.106 --> 00:31:18.246
of 2,000, so it's operating on a

00:31:18.246 --> 00:31:19.126
higher-level feature

00:31:19.126 --> 00:31:20.416
representation of the data.

00:31:21.036 --> 00:31:24.056
And then we have additional CNN

00:31:24.056 --> 00:31:26.796
primitives that we find

00:31:26.906 --> 00:31:28.326
high-level features in the data.

00:31:29.366 --> 00:31:30.626
And the last primitive in this

00:31:30.626 --> 00:31:32.846
network is the SoftMax primitive

00:31:33.236 --> 00:31:34.906
which generates probabilities

00:31:34.906 --> 00:31:35.896
for the different activity

00:31:35.896 --> 00:31:37.306
classes, which is the output of

00:31:37.306 --> 00:31:37.756
the network.

00:31:38.666 --> 00:31:39.746
And now, let's take a look at

00:31:39.746 --> 00:31:40.896
how to train this network.

00:31:42.346 --> 00:31:44.166
So, we again need a loss

00:31:44.166 --> 00:31:45.626
primitive, which takes the

00:31:45.626 --> 00:31:46.886
output of the network and the

00:31:46.886 --> 00:31:47.906
labels as input.

00:31:48.386 --> 00:31:49.936
And then we need the second half

00:31:49.936 --> 00:31:50.296
of the graph.

00:31:50.926 --> 00:31:52.146
So, in the second half of the

00:31:52.146 --> 00:31:53.956
graph, we again have gradient

00:31:53.956 --> 00:31:55.496
primitives for the corresponding

00:31:55.496 --> 00:31:57.086
forward primitives, including

00:31:57.146 --> 00:31:58.176
the LSTM primitive.

00:31:59.036 --> 00:32:02.216
And now, for training, we do the

00:32:02.216 --> 00:32:03.696
forward pass through the

00:32:03.696 --> 00:32:06.296
network, then we compute loss,

00:32:07.646 --> 00:32:09.756
and we do the gradient pass to

00:32:09.756 --> 00:32:11.096
compute gradients that will be

00:32:11.096 --> 00:32:12.126
used to update weights.

00:32:12.686 --> 00:32:14.386
So, this is a very similar setup

00:32:15.316 --> 00:32:16.866
that we have for a CNN training.

00:32:17.156 --> 00:32:18.406
And the last step is of course

00:32:18.406 --> 00:32:20.466
to update the weights and as you

00:32:20.466 --> 00:32:22.236
know, the LSTM also has weights,

00:32:22.236 --> 00:32:24.366
so they need to be updated as

00:32:25.476 --> 00:32:25.606
well.

00:32:26.076 --> 00:32:27.286
And now, let's take a look at

00:32:27.286 --> 00:32:29.856
how to train this network in

00:32:29.926 --> 00:32:30.116
MPS.

00:32:30.116 --> 00:32:31.826
But first, let's take a look at

00:32:31.826 --> 00:32:33.756
how we can create LSTM layer for

00:32:33.756 --> 00:32:35.436
training using our framework.

00:32:36.306 --> 00:32:38.036
So, first, you need to create

00:32:38.036 --> 00:32:39.396
LSTM layer descriptor.

00:32:40.646 --> 00:32:42.536
And we initialize the descriptor

00:32:42.536 --> 00:32:44.106
with initial training parameters

00:32:44.776 --> 00:32:45.996
using data source providers.

00:32:46.526 --> 00:32:47.556
So, these initial training

00:32:47.586 --> 00:32:49.066
parameters are, use smaller

00:32:49.066 --> 00:32:50.746
random number or some checkpoint

00:32:50.746 --> 00:32:51.196
values.

00:32:52.466 --> 00:32:53.576
The descriptor setup for

00:32:53.576 --> 00:32:55.826
training, is exactly the same as

00:32:55.826 --> 00:32:56.776
it is for inference.

00:32:58.646 --> 00:33:00.656
And we discussed the layer

00:33:00.656 --> 00:33:02.986
descriptor setup in our last

00:33:02.986 --> 00:33:04.676
year WWDC session in a lot more

00:33:04.676 --> 00:33:05.146
detail.

00:33:05.146 --> 00:33:06.766
So, I want to refer you to the

00:33:06.766 --> 00:33:08.616
session for more information on

00:33:08.616 --> 00:33:10.376
LSTM layer descriptor setup.

00:33:11.196 --> 00:33:12.586
Once you have the descriptor,

00:33:13.936 --> 00:33:15.936
the next step is to create LSTM

00:33:15.936 --> 00:33:17.046
training layer with this

00:33:17.076 --> 00:33:17.596
descriptor.

00:33:19.836 --> 00:33:22.156
MPS will populate training

00:33:22.156 --> 00:33:24.146
weights using the data sources

00:33:24.146 --> 00:33:25.576
specified in the descriptor.

00:33:25.906 --> 00:33:27.506
And we also need to have some

00:33:27.506 --> 00:33:29.116
matrices to hold the computed

00:33:29.116 --> 00:33:29.626
gradients.

00:33:30.816 --> 00:33:31.496
You will use the

00:33:31.496 --> 00:33:34.376
createWeightGradientMatrices API

00:33:34.376 --> 00:33:36.346
on the training layer to create

00:33:36.346 --> 00:33:37.036
these matrices.

00:33:37.446 --> 00:33:39.456
And then, the training weights

00:33:39.886 --> 00:33:41.426
will be used in a forward and

00:33:41.426 --> 00:33:43.526
gradient passes and will be

00:33:43.526 --> 00:33:45.026
passed to an optimizer along

00:33:45.026 --> 00:33:46.246
with the computed gradients,

00:33:46.336 --> 00:33:47.106
job, to date, weights.

00:33:47.636 --> 00:33:50.726
And now we need to prepare some

00:33:50.726 --> 00:33:52.736
inputs and outputs for training

00:33:52.736 --> 00:33:53.516
our LSTM.

00:33:53.996 --> 00:33:55.836
So, here's an example of how you

00:33:55.836 --> 00:33:58.266
can create the matrices to hold

00:33:58.266 --> 00:33:59.776
the input and output sequences

00:33:59.776 --> 00:34:01.286
for both the forward and

00:34:01.286 --> 00:34:02.146
gradient passes.

00:34:02.446 --> 00:34:03.926
You will need 20 matrices for

00:34:03.926 --> 00:34:05.906
each one of those.

00:34:05.906 --> 00:34:06.896
And here is how you would

00:34:06.896 --> 00:34:08.496
initialize these matrices with

00:34:08.536 --> 00:34:08.746
data.

00:34:09.366 --> 00:34:13.346
And now, we are ready to train

00:34:13.346 --> 00:34:14.906
our activity classifier network

00:34:15.346 --> 00:34:15.795
in MPS.

00:34:16.166 --> 00:34:17.516
So, in this code example, I will

00:34:17.516 --> 00:34:19.335
be highlighting only the LSTM

00:34:19.335 --> 00:34:23.246
filter in the interest of time.

00:34:23.406 --> 00:34:24.976
So, in the forward pass, we ran

00:34:24.976 --> 00:34:26.585
a sequence of 20 matrices

00:34:26.585 --> 00:34:28.196
forward through the LSTM

00:34:28.196 --> 00:34:28.806
training layer.

00:34:29.726 --> 00:34:30.735
And then in the backward pass,

00:34:30.735 --> 00:34:33.166
we ran a sequence of 20 matrices

00:34:33.275 --> 00:34:34.936
though the LSTM layer to compute

00:34:34.936 --> 00:34:35.545
gradients.

00:34:36.676 --> 00:34:38.706
And now, you have the training

00:34:38.706 --> 00:34:39.755
weights, and you have the

00:34:39.755 --> 00:34:41.406
computed gradients, and you can

00:34:41.406 --> 00:34:43.186
pass them to an optimizer to

00:34:43.186 --> 00:34:43.926
update weights.

00:34:44.646 --> 00:34:46.896
So, there's just one more thing

00:34:46.896 --> 00:34:47.786
I'd like to mention.

00:34:49.466 --> 00:34:50.735
[Inaudible] neural networks

00:34:50.735 --> 00:34:53.676
operate on images and LSTMs

00:34:53.676 --> 00:34:54.886
operate on matrices.

00:34:55.946 --> 00:34:57.316
And we'll provide convenience

00:34:57.316 --> 00:34:59.616
kernels in the framework to make

00:34:59.616 --> 00:35:00.966
it easy to convert between

00:35:00.966 --> 00:35:02.006
images and matrices.

00:35:02.836 --> 00:35:06.166
So, in order to copy an image to

00:35:06.166 --> 00:35:07.406
a matrix, you need to use the

00:35:07.406 --> 00:35:09.546
MPI's Image Copy to Matrix

00:35:09.746 --> 00:35:10.056
Kernel.

00:35:10.256 --> 00:35:11.456
So, this is how you can create

00:35:11.456 --> 00:35:13.206
one, and this is how you can

00:35:13.206 --> 00:35:15.746
encode one on a batch of images.

00:35:17.186 --> 00:35:19.506
Here, each row in a destination

00:35:19.506 --> 00:35:21.586
matrix, will contain one source

00:35:21.586 --> 00:35:21.926
image.

00:35:23.026 --> 00:35:25.036
And to copy from a matrix to an

00:35:25.036 --> 00:35:27.066
image, you need to use the MPS

00:35:27.486 --> 00:35:29.216
Matrix Copy to Image Kernel.

00:35:29.306 --> 00:35:30.676
This is how you can create one

00:35:30.996 --> 00:35:32.476
and this is how you encode one

00:35:32.476 --> 00:35:33.166
to the GPU.

00:35:34.456 --> 00:35:37.586
So, we just showed you how to

00:35:37.586 --> 00:35:40.376
train CNNs and RNNs using MPS.

00:35:41.886 --> 00:35:43.656
We also showed you a demo of

00:35:43.716 --> 00:35:45.226
Turi Create which is now powered

00:35:45.226 --> 00:35:45.646
by MPS.

00:35:45.646 --> 00:35:47.526
And now it's time for one more

00:35:47.526 --> 00:35:47.806
demo.

00:35:49.576 --> 00:35:51.276
We have been working with Google

00:35:51.276 --> 00:35:52.376
to add support to the Metal

00:35:52.376 --> 00:35:54.236
Performance Shaders Framework to

00:35:54.236 --> 00:35:58.086
TensorFlow to accelerate machine

00:35:58.086 --> 00:35:59.716
learning on macOS, and we would

00:35:59.716 --> 00:36:00.966
love to show you a demo of that

00:36:00.966 --> 00:36:01.476
in action.

00:36:01.646 --> 00:36:03.276
Specifically, we want to show

00:36:03.276 --> 00:36:04.706
you a demo of training the

00:36:04.706 --> 00:36:06.006
InceptionV3 Object

00:36:06.006 --> 00:36:08.566
Classification Network, using

00:36:08.566 --> 00:36:11.736
TensorFlow, powered by MPS.

00:36:11.806 --> 00:36:15.396
So, for this demo, I will again

00:36:15.396 --> 00:36:17.216
be using a MacBook Pro with an

00:36:17.216 --> 00:36:18.456
attached external GPU.

00:36:19.006 --> 00:36:20.856
So, I will be running TensorFlow

00:36:21.216 --> 00:36:24.336
on this MacBook Pro, and I will

00:36:24.336 --> 00:36:25.876
use an external GPU to train a

00:36:25.876 --> 00:36:27.446
network using MPS.

00:36:27.836 --> 00:36:29.806
So, in this demo setup, I've

00:36:29.806 --> 00:36:32.116
already imported TensorFlow and

00:36:32.496 --> 00:36:34.276
preloaded the InceptionV3

00:36:34.276 --> 00:36:35.986
Network and a training data set.

00:36:36.236 --> 00:36:37.226
So, now, let's train this

00:36:37.286 --> 00:36:39.376
network for 30 iterations.

00:36:39.376 --> 00:36:42.516
So, you can see how fast this is

00:36:42.516 --> 00:36:42.896
going.

00:36:43.476 --> 00:36:45.076
Again, the entire network, all

00:36:45.076 --> 00:36:46.066
of the primitives, the

00:36:46.066 --> 00:36:47.506
optimizer, and the weight update

00:36:47.506 --> 00:36:49.316
step, everything is running on

00:36:49.316 --> 00:36:50.266
the external GPU.

00:36:50.726 --> 00:36:51.816
And we're already done.

00:36:52.516 --> 00:36:54.266
And as you can see, the training

00:36:54.266 --> 00:36:56.576
rate is approximately 100 images

00:36:56.606 --> 00:36:57.086
per second.

00:36:57.546 --> 00:36:58.906
So, as was stated in the

00:36:59.336 --> 00:37:00.736
platform State of the Union,

00:37:01.296 --> 00:37:03.056
training the InceptionV3

00:37:03.056 --> 00:37:05.386
Network, in TensorFlow powered

00:37:05.386 --> 00:37:09.316
by MPS, is up to 20 times faster

00:37:09.316 --> 00:37:10.166
than without MPS.

00:37:10.336 --> 00:37:12.056
So, this is it for the

00:37:12.056 --> 00:37:12.996
TensorFlow demo.

00:37:13.536 --> 00:37:20.486
Thank you very much.

00:37:21.836 --> 00:37:23.256
And now, let's summarize this

00:37:23.256 --> 00:37:23.636
session.

00:37:24.926 --> 00:37:26.596
This year, we've added a FP16

00:37:26.686 --> 00:37:29.176
accumulation for the convolution

00:37:29.176 --> 00:37:30.486
and convolution transpose

00:37:30.486 --> 00:37:33.056
primitives to improve the

00:37:33.056 --> 00:37:34.596
performance of CNN inference.

00:37:35.246 --> 00:37:36.966
We've also added GPU accelerate

00:37:37.106 --> 00:37:38.786
primitives for training neural

00:37:38.786 --> 00:37:39.296
networks.

00:37:39.536 --> 00:37:41.016
These primitives are optimized

00:37:41.086 --> 00:37:43.036
for both iOS and macOS.

00:37:44.806 --> 00:37:45.866
We've also added the neural

00:37:45.866 --> 00:37:47.526
network graph API for training.

00:37:48.476 --> 00:37:50.186
It makes it very easy to train

00:37:50.186 --> 00:37:52.016
neural networks on the GPU and

00:37:52.016 --> 00:37:54.346
enables us to provide the best

00:37:54.346 --> 00:37:55.996
performance across different

00:37:56.706 --> 00:37:56.866
GPUs.

00:37:58.066 --> 00:38:00.046
For more information on this

00:38:00.046 --> 00:38:01.406
session and links to related

00:38:01.406 --> 00:38:02.876
resources, please go to our

00:38:02.876 --> 00:38:03.866
developer website.

00:38:05.616 --> 00:38:07.016
We have the Metal for Machine

00:38:07.016 --> 00:38:08.726
Learning Lab tomorrow at 9 a.m.

00:38:09.156 --> 00:38:10.356
So, we would love to talk to

00:38:10.356 --> 00:38:10.576
you.

00:38:10.576 --> 00:38:12.026
So, please come talk to us.

00:38:12.316 --> 00:38:16.226
And thank you for coming and

00:38:16.226 --> 00:38:17.766
have a great WWDC.