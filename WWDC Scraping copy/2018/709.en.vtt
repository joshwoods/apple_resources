WEBVTT

00:00:06.516 --> 00:00:15.500
[ Music ]

00:00:21.516 --> 00:00:24.396
[ Applause ]

00:00:24.896 --> 00:00:25.196
>> Hello!

00:00:26.016 --> 00:00:28.000
[ Applause ]

00:00:31.236 --> 00:00:33.126
So welcome to the second session

00:00:33.286 --> 00:00:34.696
of Core ML.

00:00:34.696 --> 00:00:36.246
My name is Aseem, and I'm an

00:00:36.246 --> 00:00:39.806
engineer in the Core ML team.

00:00:40.656 --> 00:00:42.526
As you all know, Core ML is

00:00:42.526 --> 00:00:44.486
Apple's machine learning

00:00:44.486 --> 00:00:45.976
framework for on-device

00:00:45.976 --> 00:00:46.406
inference.

00:00:48.406 --> 00:00:49.906
And the one thing I really like

00:00:49.906 --> 00:00:51.416
about Core ML is that it's

00:00:51.546 --> 00:00:53.846
optimized on all Apple hardware.

00:00:55.176 --> 00:00:58.116
Over the last year, we have seen

00:00:58.406 --> 00:01:01.516
lots of amazing apps across all

00:01:01.516 --> 00:01:02.426
Apple platforms.

00:01:03.096 --> 00:01:04.366
So that's really exciting.

00:01:05.286 --> 00:01:07.026
And we are even more excited

00:01:07.586 --> 00:01:08.986
with the new features that we

00:01:08.986 --> 00:01:09.736
have this year.

00:01:10.656 --> 00:01:13.716
Now you can reduce the size of

00:01:13.796 --> 00:01:14.936
your app by a lot.

00:01:16.606 --> 00:01:18.376
You can make your app much

00:01:18.376 --> 00:01:20.246
faster by using the new

00:01:20.246 --> 00:01:21.286
batch-predict API.

00:01:23.066 --> 00:01:25.356
And you can really easily

00:01:25.356 --> 00:01:27.596
include cutting-edge research

00:01:27.796 --> 00:01:29.246
right in your app using

00:01:29.246 --> 00:01:29.966
customization.

00:01:31.376 --> 00:01:33.546
So that was a recap of the first

00:01:33.546 --> 00:01:33.926
session.

00:01:34.426 --> 00:01:36.146
And in case you missed it, I

00:01:36.146 --> 00:01:37.466
would highly encourage you to go

00:01:37.466 --> 00:01:38.936
back and check the slides.

00:01:39.716 --> 00:01:43.436
In this session, we are going to

00:01:43.436 --> 00:01:46.296
see how to actually make use of

00:01:46.296 --> 00:01:47.046
these features.

00:01:47.996 --> 00:01:50.046
More specifically, we'll walk

00:01:50.046 --> 00:01:52.166
through a few examples and show

00:01:52.166 --> 00:01:54.606
you that how in a few simple

00:01:54.606 --> 00:01:57.466
steps using Core ML Tools.

00:01:58.046 --> 00:01:59.486
You can reduce the size of the

00:01:59.486 --> 00:02:01.426
model, and you can include a

00:02:01.426 --> 00:02:02.846
custom feature in your model.

00:02:04.586 --> 00:02:05.566
Here's the agenda of the

00:02:05.566 --> 00:02:05.966
session.

00:02:07.076 --> 00:02:08.556
We'll start by a really quick

00:02:08.556 --> 00:02:10.166
update on the Core ML Tools

00:02:10.226 --> 00:02:10.816
ecosystem.

00:02:11.736 --> 00:02:13.586
And then we'll dive into a demo

00:02:13.876 --> 00:02:16.866
of our quantization and custom

00:02:16.866 --> 00:02:17.346
conversion.

00:02:18.026 --> 00:02:18.846
So let me start with the

00:02:18.846 --> 00:02:19.356
ecosystem.

00:02:23.276 --> 00:02:25.306
So how do you get an ML model?

00:02:25.746 --> 00:02:27.846
Well, the best thing is that if

00:02:27.846 --> 00:02:32.986
you, if you can, if you find it

00:02:32.986 --> 00:02:35.036
online, you just download it,

00:02:36.086 --> 00:02:36.406
right?

00:02:36.406 --> 00:02:38.616
Very good place to download your

00:02:38.676 --> 00:02:41.176
ML models is the Apple Machine

00:02:41.176 --> 00:02:42.366
Learning landing page.

00:02:42.726 --> 00:02:44.206
We have a few models there.

00:02:45.126 --> 00:02:46.836
Now let's say you want to train

00:02:46.836 --> 00:02:48.456
a model on your data set.

00:02:49.046 --> 00:02:51.746
In that case, you can use Create

00:02:52.476 --> 00:02:52.546
ML.

00:02:52.806 --> 00:02:55.336
This is a new framework that we

00:02:55.336 --> 00:02:56.976
have just launched this year,

00:02:57.436 --> 00:02:59.206
and you do not have to be a

00:02:59.206 --> 00:03:00.646
machine learning expert to use

00:03:00.646 --> 00:03:00.816
it.

00:03:01.326 --> 00:03:02.496
It's really easy to use.

00:03:02.556 --> 00:03:03.786
It's right there in Xcode.

00:03:04.446 --> 00:03:06.936
So go and give it a try.

00:03:08.476 --> 00:03:10.316
Now some of you are already

00:03:10.316 --> 00:03:12.326
familiar with the amazing

00:03:12.326 --> 00:03:13.416
machine learning tools that we

00:03:13.416 --> 00:03:15.516
have outside in the community.

00:03:16.026 --> 00:03:19.036
And for that, last year we had

00:03:19.106 --> 00:03:21.406
released Core ML Tools, a Python

00:03:21.406 --> 00:03:21.956
package.

00:03:22.546 --> 00:03:24.306
And along with that, we had

00:03:24.306 --> 00:03:26.786
released a few converters.

00:03:28.366 --> 00:03:30.036
Now there has been a lot of

00:03:30.036 --> 00:03:31.606
activity in this area over the

00:03:31.606 --> 00:03:32.236
last year.

00:03:32.956 --> 00:03:34.436
And this is how the picture

00:03:34.436 --> 00:03:34.976
looks now.

00:03:35.686 --> 00:03:38.936
So as you can see, there are

00:03:38.936 --> 00:03:42.416
many more converters out there.

00:03:42.906 --> 00:03:44.526
And you really do have a lot of

00:03:44.706 --> 00:03:46.306
choice to choose your training

00:03:46.306 --> 00:03:47.016
framework now.

00:03:48.336 --> 00:03:50.346
And all of these converters are

00:03:50.346 --> 00:03:52.546
built on top of Core ML Tools.

00:03:52.856 --> 00:03:57.326
Now, I do want to highlight a

00:03:57.326 --> 00:03:58.816
couple of different converters

00:03:58.816 --> 00:03:59.056
here.

00:04:01.186 --> 00:04:03.566
Last year, we collaborated with

00:04:03.566 --> 00:04:05.286
Google and released the

00:04:05.286 --> 00:04:06.096
TensorFlow converter.

00:04:06.096 --> 00:04:08.686
So that was exciting.

00:04:10.396 --> 00:04:12.416
As you know, TensorFlow is quite

00:04:12.416 --> 00:04:14.216
popular with researchers who try

00:04:14.216 --> 00:04:16.586
out new layers so we recently

00:04:16.586 --> 00:04:18.336
added support for custom layers

00:04:18.555 --> 00:04:19.396
into the converter.

00:04:20.776 --> 00:04:22.976
And TensorFlow recently released

00:04:22.976 --> 00:04:25.236
support for quantization during

00:04:25.376 --> 00:04:27.506
training and that's Core ML 2

00:04:27.506 --> 00:04:28.676
supports quantization.

00:04:29.086 --> 00:04:30.586
This feature will be added soon

00:04:30.586 --> 00:04:31.196
to the converter.

00:04:33.186 --> 00:04:34.616
Another exciting partnership we

00:04:34.616 --> 00:04:37.286
had was with Facebook and

00:04:37.326 --> 00:04:37.836
Prisma.

00:04:38.336 --> 00:04:40.406
And this resulted in the ONNX

00:04:40.406 --> 00:04:40.866
converter.

00:04:42.216 --> 00:04:43.646
The nice thing about ONNX is

00:04:43.646 --> 00:04:48.056
that now you have access to a

00:04:48.056 --> 00:04:49.376
bunch of different training

00:04:49.376 --> 00:04:50.946
libraries that can all be

00:04:50.946 --> 00:04:53.106
converted to Core ML using the

00:04:53.106 --> 00:04:54.136
new ONNX converter.

00:04:54.136 --> 00:04:58.956
So that was a quick wrap-up of

00:04:58.956 --> 00:05:00.446
Core ML Tools ecosystem.

00:05:00.866 --> 00:05:02.496
Now to talk about quantization,

00:05:02.656 --> 00:05:04.446
I would like to invite my friend

00:05:04.446 --> 00:05:05.476
Sohaib on stage.

00:05:06.516 --> 00:05:13.596
[ Applause ]

00:05:14.096 --> 00:05:14.876
>> Good morning, everyone.

00:05:15.186 --> 00:05:15.976
My name is Sohaib.

00:05:16.206 --> 00:05:17.426
I'm an engineer in the Core ML

00:05:17.426 --> 00:05:17.716
team.

00:05:18.106 --> 00:05:19.156
And today we're going to be

00:05:19.156 --> 00:05:20.256
taking a look at new

00:05:20.256 --> 00:05:21.766
quantization utilities in Core

00:05:21.766 --> 00:05:23.026
ML Tools 2.0.

00:05:28.136 --> 00:05:29.826
Core ML Tools 2.0 has support

00:05:29.826 --> 00:05:31.206
for the latest Core ML model

00:05:31.206 --> 00:05:32.366
format specification.

00:05:32.796 --> 00:05:34.306
It also has utilities which make

00:05:34.306 --> 00:05:36.166
it really easy for you to add

00:05:36.226 --> 00:05:38.016
flexible shapes and quantize in

00:05:38.016 --> 00:05:38.876
your own network machine

00:05:38.876 --> 00:05:39.486
learning models.

00:05:40.296 --> 00:05:41.416
Using these great new features

00:05:41.416 --> 00:05:43.046
in Core ML, you can not only

00:05:43.046 --> 00:05:44.266
reduce the size of your models.

00:05:44.576 --> 00:05:46.366
But also reduce the number of

00:05:46.366 --> 00:05:48.126
models in your app, reducing the

00:05:48.126 --> 00:05:49.536
footprint of your app.

00:05:50.356 --> 00:05:51.896
Now let's start off by taking a

00:05:51.896 --> 00:05:53.316
look at quantization.

00:05:54.846 --> 00:05:55.876
Core ML Tools supports

00:05:55.906 --> 00:05:57.166
post-training quantization.

00:05:57.666 --> 00:05:59.186
We start off with a Core ML

00:05:59.286 --> 00:06:00.976
neural network model which has

00:06:01.106 --> 00:06:03.336
32-bit float weight parameters.

00:06:03.766 --> 00:06:05.276
And we use Core ML Tools to

00:06:05.276 --> 00:06:07.136
quantize the weights for this

00:06:07.136 --> 00:06:07.466
model.

00:06:08.126 --> 00:06:09.556
The resulting model is smaller

00:06:09.556 --> 00:06:10.026
in size.

00:06:10.836 --> 00:06:12.346
Now size reduction of the model

00:06:12.346 --> 00:06:13.606
is directly dependent on the

00:06:13.606 --> 00:06:15.296
number of bits we quantize our

00:06:15.296 --> 00:06:15.886
model to.

00:06:17.836 --> 00:06:19.316
Now, many of us may be wondering

00:06:19.466 --> 00:06:21.046
what exactly is quantization?

00:06:21.336 --> 00:06:22.616
And how can it reduce the size

00:06:22.616 --> 00:06:23.116
of my models?

00:06:24.036 --> 00:06:25.786
Let's step back and take a peek

00:06:25.786 --> 00:06:26.246
under the hood.

00:06:30.386 --> 00:06:31.986
Neural networks are composed of

00:06:31.986 --> 00:06:32.336
layers.

00:06:32.786 --> 00:06:33.906
And these layers can be thought

00:06:33.906 --> 00:06:35.336
of as mathematical functions.

00:06:35.776 --> 00:06:37.356
And these mathematical functions

00:06:37.466 --> 00:06:38.876
have parameters called weights.

00:06:39.076 --> 00:06:41.116
And these weights are usually

00:06:41.116 --> 00:06:42.716
stored as 32-bit floats.

00:06:43.426 --> 00:06:45.886
Now in our previous session, we

00:06:45.886 --> 00:06:47.206
took a look at ResNet50.

00:06:47.636 --> 00:06:49.036
A popular machine-learning model

00:06:49.036 --> 00:06:50.116
which is used for image

00:06:50.116 --> 00:06:51.536
classification amongst other

00:06:51.536 --> 00:06:51.816
things.

00:06:52.476 --> 00:06:53.576
Now this particular model has

00:06:53.576 --> 00:06:55.696
over 25 million weight

00:06:55.696 --> 00:06:56.286
parameters.

00:06:56.636 --> 00:06:58.286
So you can imagine, if you could

00:06:58.286 --> 00:07:00.206
somehow represent these param --

00:07:00.516 --> 00:07:02.026
these parameters using a fewer

00:07:02.026 --> 00:07:03.456
number of bits, we can

00:07:03.456 --> 00:07:04.676
drastically reduce the size of

00:07:04.676 --> 00:07:05.096
this model.

00:07:05.776 --> 00:07:08.626
In fact, this process is called

00:07:08.906 --> 00:07:09.676
quantization.

00:07:10.476 --> 00:07:12.116
In quantization, we take the

00:07:12.116 --> 00:07:13.846
weights for our layers which

00:07:13.846 --> 00:07:15.086
[inaudible] to minimum and to

00:07:15.086 --> 00:07:18.816
maximum value and we map them to

00:07:18.816 --> 00:07:19.736
unsigned integers.

00:07:20.936 --> 00:07:22.706
Now for APIC quantization, we

00:07:22.706 --> 00:07:24.526
map these values from a range of

00:07:24.586 --> 00:07:26.686
0 to 55.

00:07:27.106 --> 00:07:28.876
For 7-bit quantization, we map

00:07:28.926 --> 00:07:31.826
them from 0 to 127, all the way

00:07:31.826 --> 00:07:32.556
down to 1 bit.

00:07:32.786 --> 00:07:34.076
Where we map these weights as

00:07:34.076 --> 00:07:35.486
either zeros or ones.

00:07:36.106 --> 00:07:37.606
Since we're using fewer bits to

00:07:37.606 --> 00:07:38.856
represent the same information,

00:07:39.176 --> 00:07:40.476
we reduce the size of our model.

00:07:42.406 --> 00:07:44.786
Great. Now many of you may have

00:07:44.786 --> 00:07:46.276
noticed that we're mapping

00:07:46.276 --> 00:07:47.246
floats to integers.

00:07:47.646 --> 00:07:48.926
And you may have come to the

00:07:48.926 --> 00:07:50.046
conclusion that maybe there's

00:07:50.046 --> 00:07:52.366
some accuracy loss in this

00:07:52.366 --> 00:07:52.786
mapping.

00:07:53.326 --> 00:07:53.836
That's true.

00:07:54.736 --> 00:07:56.316
The rule of thumb is the lower

00:07:56.316 --> 00:07:57.416
the number of bits you quantize

00:07:57.416 --> 00:07:59.226
your model to, the more of a hit

00:07:59.226 --> 00:08:00.346
our model takes in terms of

00:08:00.346 --> 00:08:00.786
accuracy.

00:08:00.786 --> 00:08:02.206
And we'll get back to that in a

00:08:02.206 --> 00:08:02.436
bit.

00:08:03.556 --> 00:08:04.716
So that's an overview of

00:08:04.716 --> 00:08:05.326
quantization.

00:08:06.016 --> 00:08:06.936
But the question remains.

00:08:07.246 --> 00:08:08.716
How do we obtain this mapping?

00:08:09.506 --> 00:08:10.796
Well, there are many popular

00:08:10.796 --> 00:08:12.386
algorithms and techniques out

00:08:12.386 --> 00:08:13.346
there which help you to do this.

00:08:13.716 --> 00:08:15.536
And Core ML supports two of the

00:08:15.536 --> 00:08:17.456
most popular ones: linear

00:08:17.456 --> 00:08:19.026
quantization and lookup table

00:08:19.026 --> 00:08:19.706
quantization.

00:08:20.626 --> 00:08:21.986
Let's have a brief overview.

00:08:26.276 --> 00:08:27.566
Linear quantization is an

00:08:27.566 --> 00:08:29.116
algorithm in which you map these

00:08:29.156 --> 00:08:31.596
full parameters equally.

00:08:32.946 --> 00:08:34.736
The quantization is parametrized

00:08:34.736 --> 00:08:37.106
by a scale and by values.

00:08:37.106 --> 00:08:38.775
And these values are calculated

00:08:38.885 --> 00:08:40.416
based on the parameters of the

00:08:40.416 --> 00:08:42.405
layers that we're quantizing.

00:08:43.086 --> 00:08:44.866
Now and a really intuitive way

00:08:44.926 --> 00:08:46.946
to see how this mapping works is

00:08:46.946 --> 00:08:47.896
if we take a step back.

00:08:47.896 --> 00:08:49.356
And see how we would go back

00:08:49.386 --> 00:08:50.756
from our quantized weights which

00:08:50.756 --> 00:08:52.236
are at the bottom back to our

00:08:52.236 --> 00:08:53.106
original float weights.

00:08:53.876 --> 00:08:55.416
In linear quantization, we would

00:08:55.416 --> 00:08:57.276
simply multiply our quantized

00:08:57.276 --> 00:08:58.846
weights with the scale parameter

00:08:59.196 --> 00:08:59.886
and add the bias.

00:09:02.186 --> 00:09:03.346
The second quantization

00:09:03.346 --> 00:09:04.726
technique that Core ML supports

00:09:05.046 --> 00:09:06.356
is lookup table quantization.

00:09:07.226 --> 00:09:08.456
And this technique is exactly

00:09:08.456 --> 00:09:09.166
what it sounds like.

00:09:09.716 --> 00:09:10.816
We construct a lookup table.

00:09:11.986 --> 00:09:13.396
Now again it's helpful if we

00:09:13.396 --> 00:09:14.596
imagine how we would go back

00:09:14.596 --> 00:09:15.946
from our quantized weights back

00:09:15.946 --> 00:09:16.676
to our original weights.

00:09:17.016 --> 00:09:19.056
And in this case, the quantized

00:09:19.056 --> 00:09:21.006
weights are simply indices back

00:09:21.006 --> 00:09:22.746
into our lookup table.

00:09:24.036 --> 00:09:25.326
Now, if you notice, unlike

00:09:25.326 --> 00:09:26.546
linear quantization, we have the

00:09:26.546 --> 00:09:28.986
ability to move our quantized

00:09:28.986 --> 00:09:29.536
weights around.

00:09:29.696 --> 00:09:30.956
They don't have to be spaced out

00:09:30.956 --> 00:09:31.866
in a linear fashion.

00:09:33.906 --> 00:09:35.806
So to recap, Core ML Tools

00:09:35.806 --> 00:09:38.746
supports linear quantization and

00:09:38.746 --> 00:09:40.126
lookup table quantization where

00:09:40.126 --> 00:09:41.266
we start off with a full

00:09:41.266 --> 00:09:42.576
precision neural network model.

00:09:42.936 --> 00:09:43.906
And quantize the weights for

00:09:43.906 --> 00:09:45.396
that model using the utilities.

00:09:46.226 --> 00:09:48.276
Now you may be wondering well

00:09:48.276 --> 00:09:49.646
great, I can reduce the size of

00:09:49.646 --> 00:09:50.106
my model.

00:09:50.836 --> 00:09:52.326
But how do I figure out the

00:09:52.466 --> 00:09:53.956
parameters for my quantization?

00:09:54.436 --> 00:09:55.296
If I'm doing linear

00:09:55.296 --> 00:09:56.706
quantization, how do I figure

00:09:56.706 --> 00:09:57.676
out my scale and bias?

00:09:58.366 --> 00:09:59.336
If I'm doing lookup table

00:09:59.336 --> 00:10:01.326
quantization, how do I construct

00:10:01.326 --> 00:10:02.036
my lookup table?

00:10:03.076 --> 00:10:04.426
I'm here to tell you that you

00:10:04.426 --> 00:10:05.946
don't have to worry about any of

00:10:05.986 --> 00:10:06.236
that.

00:10:06.986 --> 00:10:08.536
All you do is decide on the

00:10:08.536 --> 00:10:09.546
number of bits you want to

00:10:09.546 --> 00:10:10.596
quantize your model to.

00:10:10.876 --> 00:10:11.926
And decide on the algorithm you

00:10:11.926 --> 00:10:13.706
want to use, and let Core ML

00:10:13.706 --> 00:10:15.166
Tools do the rest.

00:10:16.106 --> 00:10:16.576
In fact --

00:10:17.516 --> 00:10:22.216
[ Applause ]

00:10:22.716 --> 00:10:24.756
In fact, it's so simple to take

00:10:24.756 --> 00:10:26.186
a Core ML neural network model.

00:10:26.286 --> 00:10:27.036
And quantize it.

00:10:27.446 --> 00:10:29.116
Then we can do it in a few lines

00:10:29.116 --> 00:10:29.806
of Python code.

00:10:29.996 --> 00:10:31.286
But why stand here and talk

00:10:31.286 --> 00:10:32.396
about it when we can show you a

00:10:32.396 --> 00:10:32.676
demo?

00:10:40.406 --> 00:10:42.296
So for the purposes of this

00:10:42.296 --> 00:10:44.346
demo, I'm going to need a neural

00:10:44.346 --> 00:10:45.596
network in the Core ML model

00:10:45.596 --> 00:10:46.026
format.

00:10:46.706 --> 00:10:47.846
Now, as my colleague Aseem

00:10:47.846 --> 00:10:49.376
mentioned, a great place to find

00:10:49.376 --> 00:10:50.686
these models is on the Core ML

00:10:50.686 --> 00:10:51.716
machine learning home page.

00:10:52.056 --> 00:10:52.896
And I've gone ahead and

00:10:52.896 --> 00:10:54.016
downloaded one of the models

00:10:54.106 --> 00:10:54.846
from that page.

00:10:55.306 --> 00:10:56.706
So this model's called

00:10:56.706 --> 00:10:57.176
SqueezeNet.

00:10:57.176 --> 00:10:58.146
And let's go ahead and open it

00:10:58.146 --> 00:10:58.296
up.

00:10:59.736 --> 00:11:02.356
As we can see, this model is 5

00:11:02.356 --> 00:11:03.226
megabytes in size.

00:11:03.616 --> 00:11:05.986
It has a input which is an image

00:11:06.036 --> 00:11:08.456
of 227 by 227 pixels.

00:11:08.676 --> 00:11:09.796
And it has two outputs.

00:11:10.476 --> 00:11:11.776
One of the outputs is the class

00:11:11.906 --> 00:11:13.696
label which is a string, and

00:11:13.696 --> 00:11:15.996
this is the most likely label

00:11:15.996 --> 00:11:17.416
for the, for the input image.

00:11:17.736 --> 00:11:19.386
And the second output is a

00:11:19.386 --> 00:11:20.526
mapping of strings to

00:11:20.526 --> 00:11:23.386
probabilities given that if we

00:11:23.386 --> 00:11:24.756
pass an image, it's going to be

00:11:24.936 --> 00:11:25.976
a list of probabilities of what

00:11:25.976 --> 00:11:26.666
that image may be.

00:11:28.826 --> 00:11:30.126
Now let's start quantizing this

00:11:30.126 --> 00:11:30.486
model.

00:11:30.736 --> 00:11:32.696
So the first thing I want to do

00:11:32.696 --> 00:11:34.066
is I want to get into a Python

00:11:34.066 --> 00:11:34.556
environment.

00:11:34.986 --> 00:11:36.406
Now a Jupyter Notebook is one

00:11:36.406 --> 00:11:37.206
such environment that I'm

00:11:37.206 --> 00:11:37.776
comfortable with.

00:11:38.156 --> 00:11:39.086
So I'm going to go ahead and

00:11:39.086 --> 00:11:39.526
open that up.

00:11:46.696 --> 00:11:49.346
Let's open up a new notebook and

00:11:49.346 --> 00:11:50.496
zoom in on that.

00:11:51.346 --> 00:11:53.476
Alright. So let's start off by

00:11:53.476 --> 00:11:54.926
importing Core ML Tools.

00:11:58.476 --> 00:11:59.536
Let's run that.

00:11:59.536 --> 00:12:01.186
Now the second thing I want to

00:12:01.186 --> 00:12:02.576
do is I want to import all the

00:12:02.576 --> 00:12:03.906
new quantization utilities that

00:12:03.906 --> 00:12:04.966
we have in Core ML Tools.

00:12:05.026 --> 00:12:06.976
And we do that by running this.

00:12:16.206 --> 00:12:17.516
And now we need to load up the

00:12:17.516 --> 00:12:18.866
model which we want to quantize.

00:12:19.186 --> 00:12:20.376
And we just saw the SqueezeNet

00:12:20.376 --> 00:12:21.126
model a minute okay.

00:12:21.126 --> 00:12:22.106
We're going to go ahead and get

00:12:22.106 --> 00:12:22.976
an instance of that model.

00:12:32.056 --> 00:12:37.716
Send this to my desktop.

00:12:38.086 --> 00:12:40.416
Great. Now to quantize this

00:12:40.416 --> 00:12:41.996
model, we just need to make one

00:12:41.996 --> 00:12:43.056
simple API call.

00:12:43.516 --> 00:12:44.666
And let's try a linear,

00:12:44.666 --> 00:12:46.166
quantizing this model using

00:12:46.166 --> 00:12:47.256
linear quantization.

00:12:51.176 --> 00:12:53.166
And its API is simply called

00:12:53.456 --> 00:12:54.346
quantize weights.

00:12:54.816 --> 00:12:56.216
And the first parameter we pass

00:12:56.216 --> 00:12:58.236
in is the original model which

00:12:58.236 --> 00:12:59.396
you just loaded up.

00:12:59.576 --> 00:13:00.996
The number of bits we want to

00:13:00.996 --> 00:13:02.066
quantize our model to.

00:13:02.226 --> 00:13:03.536
In this case, it's 8 bits.

00:13:04.716 --> 00:13:05.996
And the quantization algorithm

00:13:05.996 --> 00:13:06.666
we want to use.

00:13:07.066 --> 00:13:08.366
Let's try linear quantization.

00:13:09.866 --> 00:13:11.436
Now what's happening is that the

00:13:11.436 --> 00:13:12.996
utility is iterating over all of

00:13:12.996 --> 00:13:14.586
the layers of the linear

00:13:14.586 --> 00:13:15.126
networks.

00:13:15.126 --> 00:13:16.876
And is quantizing all the

00:13:16.876 --> 00:13:17.726
weights in those layers.

00:13:18.186 --> 00:13:18.756
And we're finished.

00:13:20.496 --> 00:13:23.246
Now, if you recall a few moments

00:13:23.246 --> 00:13:24.806
ago I mentioned that quantizing

00:13:24.806 --> 00:13:26.676
our model had an associated loss

00:13:26.676 --> 00:13:27.276
in accuracy.

00:13:27.586 --> 00:13:28.966
So we want to know how our

00:13:28.966 --> 00:13:30.746
quantized model stacks up to the

00:13:30.746 --> 00:13:31.286
original model.

00:13:31.476 --> 00:13:33.236
And the easiest way of doing

00:13:33.236 --> 00:13:34.706
this is taking some data,

00:13:35.236 --> 00:13:37.936
passing and getting inference on

00:13:37.936 --> 00:13:39.556
that data using our original

00:13:39.556 --> 00:13:39.916
model.

00:13:40.596 --> 00:13:42.296
And doing the same inference on

00:13:42.296 --> 00:13:43.406
the same data using our

00:13:43.406 --> 00:13:45.206
quantized model and comparing

00:13:45.206 --> 00:13:46.416
the predictions from that model.

00:13:47.146 --> 00:13:48.436
And seeing how well they agree.

00:13:49.066 --> 00:13:50.296
Core ML Tools has utilities

00:13:50.296 --> 00:13:51.226
which help you to do that.

00:13:51.486 --> 00:13:53.096
And we can do that by making

00:13:53.096 --> 00:13:54.176
this call which is called

00:13:54.176 --> 00:13:55.596
compare models.

00:13:56.236 --> 00:13:57.616
We pass in our full precision

00:13:57.616 --> 00:13:59.796
model, and we pass in our model

00:13:59.796 --> 00:14:01.136
which we had just quantized.

00:14:01.956 --> 00:14:03.126
And because this model is a

00:14:03.126 --> 00:14:05.976
simple image classifier which it

00:14:05.976 --> 00:14:07.386
only has one image inputs.

00:14:08.136 --> 00:14:09.026
We, we have a convenience

00:14:09.026 --> 00:14:09.426
utility.

00:14:09.426 --> 00:14:11.296
So we can just pass in a folder

00:14:11.296 --> 00:14:12.766
containing sample data images.

00:14:13.146 --> 00:14:14.746
Now on my desktop here, I have a

00:14:14.746 --> 00:14:16.216
folder with a set of images

00:14:16.216 --> 00:14:17.156
which are relevant for my

00:14:17.156 --> 00:14:17.816
application.

00:14:18.146 --> 00:14:18.966
So I'm going to go ahead and

00:14:18.966 --> 00:14:22.186
pass a path to this folder as my

00:14:22.186 --> 00:14:22.816
[inaudible] parameter.

00:14:28.066 --> 00:14:29.776
Great. So now we see we're

00:14:29.776 --> 00:14:31.306
analyzing all the images in that

00:14:31.306 --> 00:14:31.736
folder.

00:14:31.766 --> 00:14:33.796
We're running inference on the,

00:14:33.796 --> 00:14:35.546
we're using full prediction or

00:14:35.546 --> 00:14:36.486
full precision model.

00:14:36.576 --> 00:14:37.676
And we're running inference on

00:14:37.676 --> 00:14:38.606
our quantized model.

00:14:38.636 --> 00:14:39.616
And we're comparing our two

00:14:39.616 --> 00:14:40.176
predictions.

00:14:41.436 --> 00:14:42.376
So we seem to have finished

00:14:42.376 --> 00:14:42.616
that.

00:14:42.616 --> 00:14:45.066
And you can see our Top 1

00:14:45.066 --> 00:14:46.946
Agreement is 94.8%.

00:14:47.766 --> 00:14:50.186
Not bad. Now what does this Top

00:14:50.186 --> 00:14:50.996
1 Agreement mean?

00:14:51.586 --> 00:14:52.996
This means that when I pass in

00:14:52.996 --> 00:14:55.896
my original model, that image of

00:14:55.896 --> 00:14:57.546
a dog for example, and it

00:14:57.546 --> 00:14:58.576
predicted that this image was a

00:14:58.576 --> 00:14:58.916
dog.

00:14:59.226 --> 00:15:00.766
My quantized model did the same.

00:15:00.766 --> 00:15:03.476
And that happened over 98, 94.8%

00:15:03.476 --> 00:15:04.126
of the data set.

00:15:05.846 --> 00:15:07.576
So I can go ahead and use this

00:15:07.576 --> 00:15:08.276
model in my app.

00:15:08.746 --> 00:15:10.416
But I want to see if other

00:15:10.416 --> 00:15:11.576
quantization techniques work

00:15:11.576 --> 00:15:12.546
better on this model.

00:15:13.526 --> 00:15:15.146
As I mentioned, Core ML supports

00:15:15.176 --> 00:15:16.496
two types of quantization

00:15:16.496 --> 00:15:16.936
techniques.

00:15:17.096 --> 00:15:18.686
Linear quantization and lookup

00:15:18.686 --> 00:15:19.546
table quantization.

00:15:19.956 --> 00:15:21.636
So let's go ahead and try and

00:15:21.636 --> 00:15:23.326
quantize this model using lookup

00:15:23.326 --> 00:15:24.136
table quantization.

00:15:30.936 --> 00:15:31.996
Again, we pass in an original

00:15:31.996 --> 00:15:33.496
model, the number of bits we

00:15:33.496 --> 00:15:34.726
want to quantize our model to.

00:15:35.666 --> 00:15:37.306
And our quantization techniques.

00:15:37.896 --> 00:15:39.976
Oops, made a typo there.

00:15:48.156 --> 00:15:49.816
Let's go ahead and run this.

00:15:50.656 --> 00:15:52.746
Now, k-means is a simple

00:15:52.746 --> 00:15:54.136
clustering algorithm which

00:15:54.196 --> 00:15:55.856
approximates the distribution of

00:15:55.856 --> 00:15:56.346
our weights.

00:15:56.606 --> 00:15:58.566
And using this distribution, we

00:15:58.566 --> 00:16:00.606
can construct the lookup table

00:16:00.806 --> 00:16:02.046
for our weights.

00:16:02.546 --> 00:16:04.116
And what we're doing over here

00:16:04.116 --> 00:16:05.866
is that we're iterating over all

00:16:05.866 --> 00:16:06.556
the layers in the neural

00:16:06.556 --> 00:16:06.916
network.

00:16:07.306 --> 00:16:08.496
And we're quantizing and we're

00:16:08.496 --> 00:16:09.586
figuring out the lookup table

00:16:09.936 --> 00:16:11.426
for that particular layer.

00:16:12.226 --> 00:16:14.626
Now, if you're an expert and you

00:16:14.626 --> 00:16:15.836
know that your model, you know

00:16:15.836 --> 00:16:17.586
your model architecture and you

00:16:17.586 --> 00:16:18.826
know that k-means is not the

00:16:18.826 --> 00:16:20.426
algorithm for you, you have the

00:16:20.426 --> 00:16:22.456
flexibility of passing in your

00:16:22.456 --> 00:16:24.996
own custom function instead of

00:16:24.996 --> 00:16:26.456
this algorithm and the utility

00:16:26.456 --> 00:16:28.076
will use your custom function to

00:16:28.076 --> 00:16:29.126
actually construct the lookup

00:16:29.126 --> 00:16:29.436
table.

00:16:30.866 --> 00:16:32.686
So we finished quantizing this

00:16:32.686 --> 00:16:34.036
model again using the lookup

00:16:34.036 --> 00:16:34.646
table approach.

00:16:35.066 --> 00:16:36.596
And now let's see how well this

00:16:36.596 --> 00:16:38.006
model compares with our original

00:16:38.006 --> 00:16:38.306
model.

00:16:38.626 --> 00:16:40.016
So once again we call our

00:16:40.016 --> 00:16:41.286
compare model's API.

00:16:42.276 --> 00:16:43.766
We pass in our original model

00:16:43.766 --> 00:16:46.036
and we pass in our lookup table

00:16:46.036 --> 00:16:46.366
model.

00:16:47.036 --> 00:16:51.036
And again we pass in our sample

00:16:51.036 --> 00:16:51.656
data folder.

00:16:52.266 --> 00:16:56.406
Again, we run inference over all

00:16:56.406 --> 00:16:58.616
the images using both the

00:16:59.596 --> 00:17:00.866
original model and the quantized

00:17:00.866 --> 00:17:01.166
model.

00:17:01.686 --> 00:17:02.936
And we see this time we're

00:17:02.936 --> 00:17:04.286
getting a much better, little

00:17:04.286 --> 00:17:05.726
bit better Top 1 Agreement.

00:17:06.256 --> 00:17:08.425
Now for this model, we see that

00:17:08.425 --> 00:17:09.526
lookup table was the right way

00:17:09.526 --> 00:17:09.856
to go.

00:17:10.386 --> 00:17:11.016
But again, this is

00:17:11.016 --> 00:17:12.226
model-dependent and for other

00:17:12.226 --> 00:17:13.886
models, linear may be the way.

00:17:14.136 --> 00:17:15.705
So now that we're happy with

00:17:15.705 --> 00:17:17.536
this and we see that this is

00:17:17.685 --> 00:17:18.566
good enough for at least my

00:17:18.566 --> 00:17:19.836
application, let's go ahead and

00:17:19.836 --> 00:17:21.406
save this model out.

00:17:22.695 --> 00:17:24.925
We do that by causing or calling

00:17:24.925 --> 00:17:25.276
save.

00:17:31.076 --> 00:17:31.616
I'm going to give it the

00:17:31.616 --> 00:17:33.576
creative name of Quantized

00:17:33.656 --> 00:17:34.186
SqueezeNet.

00:17:41.076 --> 00:17:41.746
And there we go.

00:17:42.236 --> 00:17:43.256
We have a quantized model.

00:17:43.816 --> 00:17:45.616
So this was an original model.

00:17:45.616 --> 00:17:46.806
And we saw that it was 5

00:17:46.806 --> 00:17:47.626
megabytes in size.

00:17:48.226 --> 00:17:49.726
Let's open up our quantized

00:17:49.726 --> 00:17:50.036
model.

00:17:50.036 --> 00:17:53.646
And the first thing we notice

00:17:53.646 --> 00:17:54.826
right off the bat is that this

00:17:54.826 --> 00:17:57.206
model is only 1.3 megabytes in

00:17:57.206 --> 00:17:57.556
size.

00:17:58.516 --> 00:18:03.636
[ Applause ]

00:18:04.136 --> 00:18:06.306
So if you notice, all the

00:18:06.306 --> 00:18:07.696
details about, about our

00:18:07.696 --> 00:18:10.196
quantized model are the same as

00:18:10.196 --> 00:18:10.866
the original model.

00:18:11.226 --> 00:18:12.306
It still takes in an image

00:18:12.306 --> 00:18:14.066
input, and it still has two

00:18:14.066 --> 00:18:14.566
outputs.

00:18:15.216 --> 00:18:17.046
Now, if I had an app using this

00:18:17.046 --> 00:18:18.616
model, what I could do as we saw

00:18:18.616 --> 00:18:19.476
in the previous demo.

00:18:20.056 --> 00:18:21.256
Is we could just drag this

00:18:21.256 --> 00:18:22.766
quantized model into our app and

00:18:22.766 --> 00:18:23.876
start using that instead.

00:18:23.876 --> 00:18:25.456
And just like that, we reduce

00:18:25.456 --> 00:18:25.956
the size of our app.

00:18:32.386 --> 00:18:34.036
So that was quantization using

00:18:34.036 --> 00:18:34.906
Core ML Tools.

00:18:38.696 --> 00:18:40.996
To recap, we saw how easy it was

00:18:41.096 --> 00:18:43.246
to use Core ML Tools to quantize

00:18:43.246 --> 00:18:43.676
our model.

00:18:44.356 --> 00:18:46.466
Using a simple API, we provided

00:18:47.406 --> 00:18:49.176
our original model, the number

00:18:49.176 --> 00:18:50.446
of bits we wanted to quantize

00:18:50.446 --> 00:18:51.746
our model to, and the

00:18:51.746 --> 00:18:53.016
quantization algorithm we wanted

00:18:53.016 --> 00:18:53.396
to use.

00:18:54.236 --> 00:18:55.736
We also saw that Core ML Tools

00:18:55.736 --> 00:18:57.736
has utilities which help us to

00:18:57.736 --> 00:18:59.476
compare our quantized model to

00:18:59.476 --> 00:19:01.126
see how it performs against our

00:19:01.126 --> 00:19:01.696
original model.

00:19:03.516 --> 00:19:05.986
Now as we saw in the demo, there

00:19:05.986 --> 00:19:07.766
is a loss of accuracy associated

00:19:07.766 --> 00:19:08.876
with quantizing our model.

00:19:09.846 --> 00:19:12.056
And this loss of accuracy is

00:19:12.056 --> 00:19:13.756
highly model and data dependent.

00:19:14.636 --> 00:19:17.236
Some models work well or perform

00:19:17.236 --> 00:19:18.076
better than others after

00:19:18.076 --> 00:19:18.746
quantization.

00:19:19.006 --> 00:19:20.446
As a general rule of thumb

00:19:20.566 --> 00:19:22.006
again, the lower the number of

00:19:22.006 --> 00:19:23.696
bits we quantize our model to

00:19:24.376 --> 00:19:25.426
the more of a precision hit we

00:19:25.426 --> 00:19:25.676
take.

00:19:26.596 --> 00:19:28.226
Now in the demo we saw that we

00:19:28.436 --> 00:19:30.256
were able to use Core ML Tools

00:19:30.446 --> 00:19:31.806
to compare our quantized model

00:19:31.806 --> 00:19:33.326
and the original model using our

00:19:33.326 --> 00:19:34.586
Top 1 Agreement metric.

00:19:35.466 --> 00:19:37.596
But you have to figure out what

00:19:37.596 --> 00:19:38.756
the relevant metric for your

00:19:38.756 --> 00:19:40.386
model and your use case is and

00:19:40.386 --> 00:19:41.796
validate that your quantize

00:19:41.796 --> 00:19:42.866
model is acceptable.

00:19:44.026 --> 00:19:45.796
Now in a previous session, we

00:19:45.796 --> 00:19:47.136
took a look at a style transfer

00:19:47.136 --> 00:19:47.426
demo.

00:19:48.226 --> 00:19:50.096
And this network took in an

00:19:50.126 --> 00:19:52.566
input image, and the output for

00:19:52.566 --> 00:19:54.106
this network was a stylized

00:19:54.106 --> 00:19:54.496
image.

00:19:55.506 --> 00:19:56.566
Let's take a look at how this

00:19:56.736 --> 00:19:57.826
model performs at different

00:19:57.826 --> 00:19:58.886
levels of quantization.

00:20:00.126 --> 00:20:03.686
So on the top, top left here,

00:20:03.836 --> 00:20:04.276
your left.

00:20:04.766 --> 00:20:06.536
We see that original model is 30

00:20:06.686 --> 00:20:09.306
-- is 32 bits and it's 6.7

00:20:09.306 --> 00:20:10.186
megabytes in size.

00:20:10.326 --> 00:20:12.106
And our 8-bit linearly quantized

00:20:12.106 --> 00:20:14.146
model is only 1.7 megabits in

00:20:14.146 --> 00:20:14.446
size.

00:20:14.776 --> 00:20:16.016
And we see that the performance

00:20:16.486 --> 00:20:18.256
by visual inspection it's good

00:20:18.256 --> 00:20:19.406
enough for my style transfer

00:20:19.406 --> 00:20:19.746
demo.

00:20:20.756 --> 00:20:23.176
Now we can see that even down to

00:20:23.176 --> 00:20:25.016
4 bits, we don't lose out much

00:20:25.016 --> 00:20:25.896
in the way of performance.

00:20:26.636 --> 00:20:27.736
I would even argue that for my

00:20:27.736 --> 00:20:29.256
app at least, the 3 bit will

00:20:29.256 --> 00:20:30.066
work fine as well.

00:20:30.066 --> 00:20:32.946
And we see at 2 bit, we start to

00:20:32.946 --> 00:20:34.886
see a lot of artifacts and this

00:20:34.886 --> 00:20:36.406
may not be the right model for

00:20:37.236 --> 00:20:37.303
us.

00:20:38.906 --> 00:20:40.406
And that was quantization using

00:20:40.406 --> 00:20:41.186
Core ML Tools.

00:20:42.146 --> 00:20:43.056
Now I'm going to hand it back to

00:20:43.056 --> 00:20:44.836
Aseem who's going to talk about

00:20:44.836 --> 00:20:45.596
custom conversion.

00:20:45.596 --> 00:20:45.936
Thank you.

00:20:46.516 --> 00:20:51.846
[ Applause ]

00:20:52.346 --> 00:20:52.976
>> Thank you, Sohaib.

00:20:53.646 --> 00:20:56.136
So I want to talk about a

00:20:56.266 --> 00:20:58.396
feature that is essential to

00:20:58.396 --> 00:20:59.706
keep pace with the machine

00:20:59.706 --> 00:21:00.546
learning research that's

00:21:00.646 --> 00:21:02.616
happening around us.

00:21:02.616 --> 00:21:04.746
As you all know, the field of

00:21:04.746 --> 00:21:06.336
machine learning is expanding

00:21:06.336 --> 00:21:07.146
very rapidly.

00:21:07.766 --> 00:21:09.576
So it's very critical for us at

00:21:09.696 --> 00:21:11.836
Core ML to provide you with the

00:21:11.836 --> 00:21:14.046
necessary software tools to help

00:21:14.046 --> 00:21:15.616
with that.

00:21:15.886 --> 00:21:16.866
Now let's take an example.

00:21:17.836 --> 00:21:19.986
Let's say you are experimenting

00:21:19.986 --> 00:21:21.036
with a new model that that is

00:21:21.036 --> 00:21:22.096
not supported on Core ML.

00:21:22.426 --> 00:21:24.536
Or let's say you have a neural

00:21:24.536 --> 00:21:27.636
network that runs on Core ML but

00:21:27.636 --> 00:21:29.056
maybe there's a layer or two

00:21:29.436 --> 00:21:31.606
that Core ML does not have yet.

00:21:32.886 --> 00:21:34.606
In that case, you should still

00:21:34.606 --> 00:21:37.466
be able to use the power of Core

00:21:37.466 --> 00:21:38.446
ML, right?

00:21:38.736 --> 00:21:40.386
And the answer to that question

00:21:40.386 --> 00:21:40.766
is yes.

00:21:41.666 --> 00:21:43.726
And the feature of customization

00:21:43.986 --> 00:21:44.786
will help you there.

00:21:45.756 --> 00:21:47.906
In the next few minutes, I want

00:21:47.906 --> 00:21:50.346
to really focus on the specific

00:21:50.346 --> 00:21:52.866
use case of having a new neural

00:21:52.866 --> 00:21:53.506
network layer.

00:21:53.916 --> 00:21:55.546
And show you how you would

00:21:55.616 --> 00:21:57.496
convert it to Core ML and then

00:21:57.666 --> 00:21:58.796
how you would implement it in

00:21:58.826 --> 00:22:00.596
your app.

00:22:00.866 --> 00:22:02.496
So let's take a look at model

00:22:02.496 --> 00:22:02.956
conversion.

00:22:03.756 --> 00:22:05.396
So if you have used one of our

00:22:05.396 --> 00:22:06.916
converters, or even if you have

00:22:06.916 --> 00:22:08.996
not, it's a really simple API.

00:22:08.996 --> 00:22:10.276
It's just a call to one

00:22:10.276 --> 00:22:10.706
function.

00:22:11.146 --> 00:22:14.096
This is how it looks for the

00:22:14.166 --> 00:22:15.076
Keras converter.

00:22:15.466 --> 00:22:17.536
And it's very similar for say

00:22:17.656 --> 00:22:19.396
the ONNX converter or the

00:22:19.396 --> 00:22:21.416
TensorFlow converter.

00:22:21.776 --> 00:22:22.976
Now when you call this function,

00:22:23.406 --> 00:22:25.626
mostly everything goes right.

00:22:26.006 --> 00:22:27.656
But sometimes you might get an

00:22:27.776 --> 00:22:29.156
error message like this.

00:22:30.496 --> 00:22:32.826
It might say, "Hey, unsupported

00:22:32.826 --> 00:22:34.146
operation of such-and-such

00:22:34.146 --> 00:22:34.366
kind."

00:22:35.086 --> 00:22:37.546
Now if that happens to you, you

00:22:37.586 --> 00:22:39.286
only need to do a little bit

00:22:39.286 --> 00:22:40.886
more to get past this error.

00:22:41.666 --> 00:22:43.576
More specifically, such an error

00:22:43.576 --> 00:22:44.896
message is an indication that

00:22:44.896 --> 00:22:47.166
you should be using a custom

00:22:47.166 --> 00:22:47.346
layer.

00:22:48.976 --> 00:22:50.556
And before I show you what is

00:22:50.556 --> 00:22:51.946
the little bit of extra effort

00:22:51.946 --> 00:22:53.016
that you need to do to convert,

00:22:53.016 --> 00:22:55.816
let's look at a few examples

00:22:55.816 --> 00:22:57.156
where you would need to use a

00:22:57.156 --> 00:22:57.656
custom layer.

00:23:01.426 --> 00:23:02.776
So let's say you have an image

00:23:02.816 --> 00:23:03.426
classifier.

00:23:03.786 --> 00:23:05.846
This is how it looks in Xcode.

00:23:06.246 --> 00:23:07.096
So it will be high-level

00:23:07.096 --> 00:23:08.316
description of the model.

00:23:08.886 --> 00:23:11.136
If you look inside, it's very

00:23:11.136 --> 00:23:12.166
likely that it's a neural

00:23:12.166 --> 00:23:12.606
network.

00:23:13.046 --> 00:23:14.456
And it's very likely that it's a

00:23:14.456 --> 00:23:16.116
convolutional neural network.

00:23:16.116 --> 00:23:17.416
So it has a lot of layers,

00:23:17.656 --> 00:23:19.396
convolution, activation.

00:23:20.346 --> 00:23:22.386
Now it might happen that there's

00:23:22.386 --> 00:23:23.526
a new activation layer that

00:23:23.556 --> 00:23:24.856
comes up that Core ML does not

00:23:24.856 --> 00:23:25.206
support.

00:23:25.846 --> 00:23:29.066
And it's like at every machine

00:23:29.066 --> 00:23:30.916
learning conference, researchers

00:23:30.916 --> 00:23:32.046
are coming up with new layers

00:23:32.046 --> 00:23:32.516
all the time.

00:23:32.596 --> 00:23:33.476
So this is a very common

00:23:33.476 --> 00:23:33.906
scenario.

00:23:33.906 --> 00:23:37.356
Now if this happens, you only

00:23:37.356 --> 00:23:39.306
need to use a custom

00:23:39.486 --> 00:23:40.566
implementation of this new

00:23:40.566 --> 00:23:40.886
layer.

00:23:41.276 --> 00:23:42.256
And then you are good to go.

00:23:42.716 --> 00:23:43.796
So this is how the model will

00:23:43.796 --> 00:23:44.246
look like.

00:23:44.246 --> 00:23:46.096
The only difference is this

00:23:46.096 --> 00:23:48.016
dependency section at the

00:23:48.636 --> 00:23:49.446
bottom.

00:23:49.446 --> 00:23:52.416
Which would say that this model

00:23:52.416 --> 00:23:53.976
contains a description of this

00:23:54.026 --> 00:23:54.476
custom layer.

00:23:54.856 --> 00:23:56.246
Let's take a look at another

00:23:56.246 --> 00:23:56.746
example.

00:23:57.056 --> 00:23:58.576
Let's say we have a very simple

00:23:58.906 --> 00:23:59.946
digit classifier.

00:24:01.196 --> 00:24:03.526
Now I came across this research

00:24:03.526 --> 00:24:04.946
paper recently.

00:24:05.056 --> 00:24:06.486
It's called Spatial Transformer

00:24:06.486 --> 00:24:06.976
Network.

00:24:07.576 --> 00:24:09.236
And what it does is this.

00:24:09.886 --> 00:24:12.746
So it inserts a neural network

00:24:12.946 --> 00:24:15.036
after the digit that tries to

00:24:15.036 --> 00:24:16.516
localize the digit.

00:24:17.326 --> 00:24:18.856
And then it feeds it through a

00:24:18.856 --> 00:24:21.156
grid sampler layer which renders

00:24:21.156 --> 00:24:22.786
the digit again, but this time

00:24:23.066 --> 00:24:25.026
it has already focused on the

00:24:25.026 --> 00:24:25.276
digit.

00:24:25.596 --> 00:24:26.686
And then you pass it through

00:24:26.686 --> 00:24:29.516
your old classify method.

00:24:29.516 --> 00:24:31.626
Now we don't need to worry about

00:24:31.626 --> 00:24:32.416
the details here.

00:24:32.416 --> 00:24:33.716
But the point to note is that

00:24:33.756 --> 00:24:35.946
the portion in green is what

00:24:35.946 --> 00:24:36.986
Core ML supports.

00:24:37.136 --> 00:24:38.906
And the portion in red, which is

00:24:38.906 --> 00:24:41.456
this new grid sampler layer, is

00:24:41.456 --> 00:24:42.846
this new experimental layer that

00:24:42.846 --> 00:24:43.746
Core ML does not support.

00:24:44.136 --> 00:24:45.596
So I want to take an example of

00:24:45.716 --> 00:24:47.826
this particular model and show

00:24:47.826 --> 00:24:49.896
you how you would convert it

00:24:50.116 --> 00:24:51.316
using Core ML Tools.

00:24:51.566 --> 00:24:53.336
So let's go to demo.

00:25:00.076 --> 00:25:01.566
I hope it works on the first

00:25:01.566 --> 00:25:01.826
try.

00:25:03.236 --> 00:25:04.836
Back, oh yes.

00:25:05.996 --> 00:25:09.476
Okay. So let me close off these

00:25:09.476 --> 00:25:09.966
windows.

00:25:14.636 --> 00:25:16.766
Let me get, clear this.

00:25:16.926 --> 00:25:17.596
Clear the ML.

00:25:18.106 --> 00:25:20.246
Okay, so I'm also going to use

00:25:20.446 --> 00:25:22.526
Jupyter Notebook to show the

00:25:22.526 --> 00:25:22.846
demo.

00:25:30.046 --> 00:25:31.696
So I just navigate to the folder

00:25:31.696 --> 00:25:32.956
where I have my pre-trained

00:25:33.026 --> 00:25:33.356
network.

00:25:34.346 --> 00:25:35.976
So what you see here is that I

00:25:35.976 --> 00:25:38.386
have this spatial transformer

00:25:38.386 --> 00:25:39.056
dot [inaudible] file.

00:25:39.156 --> 00:25:41.136
This is a pre-trained Keras

00:25:41.136 --> 00:25:41.436
model.

00:25:42.346 --> 00:25:44.056
And if you are wondering if I

00:25:44.056 --> 00:25:45.526
did something special to get

00:25:45.526 --> 00:25:46.756
this model.

00:25:47.956 --> 00:25:50.536
Basically what I did was I could

00:25:50.646 --> 00:25:52.076
easily find an open source

00:25:52.076 --> 00:25:53.126
implementation of spatial

00:25:53.126 --> 00:25:53.676
transformer.

00:25:54.086 --> 00:25:55.616
I just exhibited that script in

00:25:55.616 --> 00:25:57.046
Keras, and I got this model.

00:25:57.646 --> 00:25:59.196
And along with this model, I

00:25:59.196 --> 00:26:01.476
also got this grid sampler layer

00:26:01.646 --> 00:26:02.436
Python script.

00:26:03.206 --> 00:26:04.936
Now this grid sampler layer that

00:26:04.936 --> 00:26:07.086
I'm talking about, it's also not

00:26:07.086 --> 00:26:08.676
supported on Keras natively.

00:26:09.326 --> 00:26:11.376
So the implementation that I got

00:26:11.376 --> 00:26:13.666
online used that Keras custom

00:26:13.666 --> 00:26:15.666
layer to implement the layer.

00:26:16.376 --> 00:26:18.496
So as you can see, the concept

00:26:18.496 --> 00:26:20.046
of customization is not unique

00:26:20.046 --> 00:26:20.606
to Core ML.

00:26:20.806 --> 00:26:22.986
In fact, it's very common in

00:26:22.986 --> 00:26:23.816
most machine learning

00:26:23.816 --> 00:26:24.416
frameworks.

00:26:24.756 --> 00:26:26.556
This is how people experiment in

00:26:26.556 --> 00:26:27.086
new layers.

00:26:27.966 --> 00:26:29.956
Okay, so so far, I just have a

00:26:30.006 --> 00:26:30.636
Keras model.

00:26:30.826 --> 00:26:32.306
And now I want to focus on how

00:26:32.306 --> 00:26:33.666
can I get a Core ML model?

00:26:34.406 --> 00:26:40.716
So I'll open -- there, let me

00:26:40.716 --> 00:26:42.506
launch a new Python notebook.

00:26:43.766 --> 00:26:46.176
So I'll start by importing this

00:26:46.226 --> 00:26:47.526
Keras model into my Python

00:26:47.526 --> 00:26:48.086
environment.

00:26:49.396 --> 00:26:53.576
Okay? So I import Keras, I

00:26:53.576 --> 00:26:56.676
import the, the custom layer

00:26:56.676 --> 00:26:57.976
that we have in Keras.

00:26:58.316 --> 00:27:00.366
And now I will load the model in

00:27:00.406 --> 00:27:00.926
Keras.

00:27:02.676 --> 00:27:04.656
Okay? So this is how you load

00:27:05.086 --> 00:27:06.196
model, Keras models.

00:27:06.196 --> 00:27:07.506
You give the part to the model

00:27:07.506 --> 00:27:08.536
and if there's a custom layer,

00:27:08.536 --> 00:27:09.796
you give a part to that.

00:27:11.016 --> 00:27:12.886
Okay. So we have the model now.

00:27:13.066 --> 00:27:14.276
Now let's convert this to Core

00:27:14.276 --> 00:27:14.466
ML.

00:27:15.656 --> 00:27:16.986
So I'm going to import Core ML

00:27:16.986 --> 00:27:17.626
Tools.

00:27:18.356 --> 00:27:19.076
Execute that.

00:27:19.896 --> 00:27:22.106
And now as I, as I showed you

00:27:22.106 --> 00:27:24.156
before that this is just a call

00:27:24.156 --> 00:27:25.626
to one function to convert it.

00:27:26.086 --> 00:27:26.966
So let me do that.

00:27:28.476 --> 00:27:29.196
That's my call.

00:27:29.776 --> 00:27:32.086
And I get an error as expected.

00:27:32.626 --> 00:27:35.096
Python likes to throw these huge

00:27:35.096 --> 00:27:35.696
error messages.

00:27:36.136 --> 00:27:37.946
But really what we're focused on

00:27:38.666 --> 00:27:40.076
is this last line.

00:27:40.336 --> 00:27:40.896
Let me --

00:27:46.626 --> 00:27:47.906
So as we can see in this last

00:27:47.906 --> 00:27:49.676
line it says that hey, the layer

00:27:49.676 --> 00:27:51.326
or sampler is not supported.

00:27:51.836 --> 00:27:53.146
So now let's see what we need to

00:27:53.146 --> 00:27:55.126
do to get rid of that.

00:27:55.756 --> 00:27:57.066
Maybe I clear this all so that

00:27:57.066 --> 00:27:57.466
you can see.

00:27:57.786 --> 00:27:59.616
Okay. So now I change my

00:27:59.616 --> 00:28:00.956
converter call just a little bit

00:28:01.176 --> 00:28:03.046
so I have my Core ML model.

00:28:03.646 --> 00:28:08.096
And now I'm going to pass one

00:28:08.096 --> 00:28:08.946
additional argument.

00:28:09.176 --> 00:28:14.176
It's called custom conversion

00:28:14.576 --> 00:28:14.976
functions.

00:28:19.396 --> 00:28:20.676
And this will be a dictionary

00:28:21.396 --> 00:28:23.796
from the name of the layer to a

00:28:23.796 --> 00:28:25.496
function that I will define in a

00:28:25.496 --> 00:28:25.826
minute.

00:28:26.186 --> 00:28:27.066
And that I'm calling a good

00:28:27.066 --> 00:28:27.386
sampler.

00:28:27.926 --> 00:28:29.006
So let me take a step back and

00:28:29.006 --> 00:28:30.026
explain what is happening here.

00:28:30.576 --> 00:28:31.796
So as we know the way converter

00:28:31.796 --> 00:28:34.126
works is that it goes through

00:28:34.346 --> 00:28:35.666
each and every Keras layer.

00:28:36.016 --> 00:28:38.386
It will, if you look at the

00:28:38.476 --> 00:28:38.986
first layer.

00:28:38.986 --> 00:28:40.046
Then [inaudible] its parameters

00:28:40.046 --> 00:28:40.666
to Core ML.

00:28:40.666 --> 00:28:41.816
If you go to the second layer,

00:28:41.816 --> 00:28:43.146
then translate its parameters

00:28:43.146 --> 00:28:44.306
and so on.

00:28:44.676 --> 00:28:45.936
Now when it hits this custom

00:28:45.936 --> 00:28:46.886
layer, it doesn't know what to

00:28:46.886 --> 00:28:47.146
do.

00:28:47.756 --> 00:28:49.446
So this function that I'm

00:28:49.446 --> 00:28:50.886
passing here that convert this

00:28:50.936 --> 00:28:53.246
sampler is going to help my

00:28:53.246 --> 00:28:54.256
converter in doing that.

00:28:54.876 --> 00:28:56.316
And let me show you what this

00:28:56.316 --> 00:28:56.976
function looks like.

00:29:01.046 --> 00:29:01.806
So this is a function.

00:29:01.806 --> 00:29:03.976
There are a few lines of code,

00:29:03.976 --> 00:29:05.866
but all that it's doing is three

00:29:05.866 --> 00:29:06.276
things.

00:29:07.226 --> 00:29:10.966
First, it's giving a name of a

00:29:11.026 --> 00:29:11.306
class.

00:29:11.806 --> 00:29:13.566
So as we might have noticed, the

00:29:13.616 --> 00:29:15.126
implementation of the layer is

00:29:15.126 --> 00:29:15.546
not here.

00:29:15.906 --> 00:29:17.096
The implementation will come

00:29:17.096 --> 00:29:19.506
later in the app and it will be

00:29:19.506 --> 00:29:20.866
encapsulated in a class.

00:29:21.286 --> 00:29:22.136
And this is the name of the

00:29:22.136 --> 00:29:23.836
class that we'll later

00:29:23.836 --> 00:29:24.286
implement.

00:29:24.666 --> 00:29:26.436
So during conversion, we just

00:29:26.436 --> 00:29:27.756
need to specify this class name.

00:29:28.156 --> 00:29:28.456
That's it.

00:29:29.156 --> 00:29:30.436
And then there's the description

00:29:30.816 --> 00:29:32.206
which is a, which you should

00:29:32.206 --> 00:29:34.536
provide so that if anybody is,

00:29:34.746 --> 00:29:37.086
if somebody is looking at your

00:29:37.086 --> 00:29:38.266
model, they know what it has.

00:29:39.276 --> 00:29:40.926
And the third thing is basically

00:29:40.996 --> 00:29:42.766
translating any parameters that

00:29:42.766 --> 00:29:44.466
the Keras layer had to Core ML.

00:29:45.046 --> 00:29:46.206
For this particular layer, it

00:29:46.206 --> 00:29:47.146
has two parameters.

00:29:47.276 --> 00:29:48.756
The output height, and output

00:29:48.816 --> 00:29:48.946
weight.

00:29:49.336 --> 00:29:50.716
And I'm just translating it to

00:29:50.716 --> 00:29:51.066
Core ML.

00:29:51.866 --> 00:29:52.956
If your custom layer that does

00:29:52.956 --> 00:29:54.356
not have any parameters, then

00:29:54.356 --> 00:29:56.626
you load, then you do not need

00:29:56.626 --> 00:29:57.616
to do, do this.

00:29:58.356 --> 00:29:59.516
If your layer has lots of

00:29:59.516 --> 00:30:01.116
parameters, they can all go

00:30:01.116 --> 00:30:02.466
here, and they will all be

00:30:02.466 --> 00:30:04.736
encapsulated inside the Core ML

00:30:04.736 --> 00:30:05.066
model.

00:30:05.996 --> 00:30:07.486
So as you might have noticed

00:30:07.486 --> 00:30:09.096
that all I did here was very

00:30:09.096 --> 00:30:10.576
similar to how you would define

00:30:10.576 --> 00:30:11.466
a class, right?

00:30:11.786 --> 00:30:12.656
You give a class name.

00:30:12.656 --> 00:30:14.106
Maybe a description, maybe some

00:30:14.106 --> 00:30:14.746
parameters.

00:30:15.546 --> 00:30:16.646
So now let me execute this.

00:30:20.066 --> 00:30:20.916
And now we see that the

00:30:20.916 --> 00:30:23.606
converter went, conversion went

00:30:25.196 --> 00:30:25.336
fine.

00:30:25.596 --> 00:30:30.216
So let me this is behaving very

00:30:30.216 --> 00:30:31.636
weirdly for some reason.

00:30:35.296 --> 00:30:36.836
If you don't mind, I'm just

00:30:36.886 --> 00:30:38.656
going to delete this all.

00:30:40.556 --> 00:30:42.396
So let me visualize this model,

00:30:42.676 --> 00:30:44.186
and you can do that very simply

00:30:44.356 --> 00:30:47.136
using function in Core ML Tools.

00:30:47.636 --> 00:30:48.596
That's called visualize spec.

00:30:50.766 --> 00:30:53.256
And here you can see a

00:30:53.256 --> 00:30:54.256
visualization of the model.

00:30:54.586 --> 00:30:56.166
So as we can see, we have the

00:30:56.216 --> 00:30:56.996
[inaudible] and some layers

00:30:56.996 --> 00:30:57.416
there.

00:30:57.876 --> 00:31:00.336
And this is our custom layer.

00:31:00.336 --> 00:31:01.936
And if I click on this, I see

00:31:01.936 --> 00:31:03.176
the parameters that it has.

00:31:03.176 --> 00:31:04.806
So this is the name of the class

00:31:05.086 --> 00:31:05.706
that I gave.

00:31:06.256 --> 00:31:07.816
And this, and these are the

00:31:07.816 --> 00:31:08.746
parameters that I set.

00:31:10.026 --> 00:31:10.886
It's always a good idea to

00:31:10.886 --> 00:31:13.146
visualize your Core ML model

00:31:13.146 --> 00:31:14.176
before you drag and drop just to

00:31:14.176 --> 00:31:15.906
see if everything looks fine.

00:31:17.346 --> 00:31:20.546
Okay. This is the wrong

00:31:20.546 --> 00:31:20.946
notebook.

00:31:21.256 --> 00:31:23.036
Okay. And now I'll save out this

00:31:23.036 --> 00:31:23.406
model.

00:31:30.256 --> 00:31:32.106
And now let's take a look at

00:31:32.106 --> 00:31:32.606
this model.

00:31:33.846 --> 00:31:35.816
So let me close this.

00:31:37.756 --> 00:31:37.926
Okay.

00:31:42.086 --> 00:31:44.696
Let me actually let me navigate

00:31:44.816 --> 00:31:50.476
to the directory that I have.

00:31:51.976 --> 00:31:52.736
And here's my model.

00:31:52.736 --> 00:31:53.946
So if I click on it and see it

00:31:53.946 --> 00:31:55.336
in Xcode just to see how it

00:31:55.336 --> 00:31:55.666
looks.

00:31:55.666 --> 00:31:58.756
We can see that it has the

00:31:58.756 --> 00:32:00.266
custom description here.

00:32:01.546 --> 00:32:03.186
Okay. Let me go back to slides.

00:32:04.516 --> 00:32:09.896
[ Applause ]

00:32:10.396 --> 00:32:12.466
So what we just saw was with a

00:32:12.466 --> 00:32:15.126
few simple lines, we could

00:32:15.156 --> 00:32:16.386
exhibit a convert a function to

00:32:16.546 --> 00:32:17.246
Core ML.

00:32:17.246 --> 00:32:19.496
And the process is pretty much

00:32:19.526 --> 00:32:21.266
the same if you are using the

00:32:21.266 --> 00:32:23.566
TensorFlow converter or the ONNX

00:32:23.566 --> 00:32:23.956
converter.

00:32:25.266 --> 00:32:26.596
So we have our model here on the

00:32:26.596 --> 00:32:27.176
left-hand side.

00:32:27.616 --> 00:32:29.286
The custom layer model with the

00:32:29.286 --> 00:32:29.856
parameters.

00:32:30.496 --> 00:32:31.646
Now when you drag and drop this

00:32:31.646 --> 00:32:33.966
model into Xcode, you will need

00:32:33.966 --> 00:32:35.336
to provide the implementation of

00:32:35.426 --> 00:32:36.456
the class.

00:32:36.456 --> 00:32:38.186
In a file say, for example,

00:32:38.186 --> 00:32:38.476
[inaudible].

00:32:38.476 --> 00:32:39.536
And this is how it would look

00:32:39.536 --> 00:32:39.726
like.

00:32:40.206 --> 00:32:41.646
So you have your class, so

00:32:42.006 --> 00:32:44.986
you'll have the initializer

00:32:45.246 --> 00:32:45.706
function.

00:32:45.996 --> 00:32:47.456
So this would be just

00:32:47.456 --> 00:32:49.106
initializing any parameters that

00:32:49.106 --> 00:32:50.116
we had in the model.

00:32:50.486 --> 00:32:52.636
And then the main function in

00:32:52.636 --> 00:32:55.906
this class would be evaluate.

00:32:56.846 --> 00:32:58.996
This is where the actual

00:32:59.266 --> 00:33:00.706
implementation of whatever

00:33:00.706 --> 00:33:01.876
mathematical function the layer

00:33:01.876 --> 00:33:03.396
is supposed to perform will go

00:33:03.596 --> 00:33:04.376
here, in here.

00:33:04.806 --> 00:33:05.976
And then there's one more

00:33:05.976 --> 00:33:07.266
function called output shape or

00:33:07.266 --> 00:33:07.946
input shapes.

00:33:08.136 --> 00:33:10.036
This just specifies the size of

00:33:10.036 --> 00:33:11.496
the output area that the layer

00:33:11.496 --> 00:33:12.106
produces.

00:33:12.556 --> 00:33:14.906
This helps Core ML in allocating

00:33:14.906 --> 00:33:17.116
the buffer size at load time so

00:33:17.116 --> 00:33:18.726
that your app is more efficient

00:33:19.076 --> 00:33:19.646
at runtime.

00:33:21.966 --> 00:33:24.256
So we just saw how you would

00:33:24.326 --> 00:33:26.136
tackle a new layer in a neural

00:33:26.136 --> 00:33:26.516
network.

00:33:26.826 --> 00:33:29.446
There's a very similar concept

00:33:29.506 --> 00:33:30.556
to a custom layer, and it's

00:33:30.556 --> 00:33:32.336
called custom model.

00:33:32.336 --> 00:33:35.426
It has the same idea, but it's

00:33:35.426 --> 00:33:36.766
sort of more generic.

00:33:37.226 --> 00:33:39.966
So with a custom model, you can

00:33:39.966 --> 00:33:41.546
deal with any sort of network.

00:33:41.806 --> 00:33:43.696
It need not be a neural -- it

00:33:43.726 --> 00:33:44.856
need not be a neural network.

00:33:45.306 --> 00:33:46.876
And basically gives you just

00:33:46.876 --> 00:33:48.646
more flexibility overall.

00:33:50.156 --> 00:33:51.796
So let me summarize the session.

00:33:52.356 --> 00:33:57.026
We saw how much more rich is

00:33:57.076 --> 00:33:58.656
this ecosystem around Core ML

00:33:58.656 --> 00:34:00.116
Tools and that's great for you

00:34:00.116 --> 00:34:00.396
guys.

00:34:00.396 --> 00:34:01.346
Because now you have lot of

00:34:01.436 --> 00:34:03.416
choice to get Core ML models

00:34:03.416 --> 00:34:03.726
from.

00:34:04.476 --> 00:34:06.526
We saw how easy it was to

00:34:06.846 --> 00:34:09.666
quantize this, quantize Core ML

00:34:09.666 --> 00:34:10.235
model.

00:34:10.716 --> 00:34:12.485
And we saw that with a few lines

00:34:13.156 --> 00:34:16.565
of code, we could easily

00:34:16.565 --> 00:34:18.295
integrate a new custom layer in

00:34:19.356 --> 00:34:20.456
the model.

00:34:20.806 --> 00:34:22.136
You can find more information at

00:34:22.136 --> 00:34:23.206
our documentation page.

00:34:23.596 --> 00:34:25.476
And come to the labs and talk to

00:34:25.476 --> 00:34:25.626
us.

00:34:26.176 --> 00:34:26.966
Okay, thank you.

00:34:27.507 --> 00:34:29.507
[ Applause ]