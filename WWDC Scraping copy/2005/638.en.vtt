WEBVTT

00:00:09.500 --> 00:02:43.100
[Transcript missing]

00:02:43.510 --> 00:02:43.940
Great.

00:02:44.250 --> 00:02:45.220
That's fantastic.

00:02:45.390 --> 00:02:47.450
How many of you,
and I want to get a little

00:02:47.460 --> 00:02:51.070
bit about where you're from,
how many of you are from education?

00:02:51.180 --> 00:02:51.400
Wow.

00:02:51.400 --> 00:02:52.370
Let's switch hands now.

00:02:52.640 --> 00:02:55.360
How many from the commercial market?

00:02:56.210 --> 00:02:58.480
Government?

00:02:58.480 --> 00:02:59.200
Mars?

00:02:59.200 --> 00:03:01.190
A few of you, okay.

00:03:01.450 --> 00:03:03.070
Well, we are at Apple, so why not, right?

00:03:03.630 --> 00:03:04.640
Okay, fantastic.

00:03:04.640 --> 00:03:07.580
So, with no further ado,
let me introduce Harry Ain,

00:03:07.580 --> 00:03:10.980
the President and CEO of Sand Solutions.

00:03:17.940 --> 00:03:24.660
Thank you very much, Alex,
for that great introduction.

00:03:24.660 --> 00:03:28.360
So I'm Harry Ain from Sand Solutions.

00:03:28.360 --> 00:03:29.740
Today we're going to cover a few things.

00:03:29.740 --> 00:03:33.140
I'll go through what, you know,
Sand Solutions' background is.

00:03:33.140 --> 00:03:41.060
We're going to talk about sand
planning and the deployment process.

00:03:41.060 --> 00:03:41.060
We're going to talk about

00:03:41.460 --> 00:03:46.210
What a large-scale deployment is
versus a small-scale deployment.

00:03:46.220 --> 00:03:49.760
What we consider a large-scale
versus a small-scale.

00:03:49.760 --> 00:03:54.240
We're going to talk about, in that part,
we're going to talk about

00:03:54.240 --> 00:03:57.080
how do you get from,
how do you build out a

00:03:57.080 --> 00:03:59.220
large-scale deployment.

00:03:59.750 --> 00:04:03.730
Well, we start with a small-scale
as our building block.

00:04:03.790 --> 00:04:07.230
So we're going to talk about,
we're going to talk through a

00:04:07.420 --> 00:04:10.880
small reference design and we're
going to run tests on that.

00:04:10.940 --> 00:04:16.720
So you're going to see some real live
data going through a reference design.

00:04:16.720 --> 00:04:23.600
Then we're going to scale that reference
design up to a large-scale design.

00:04:23.600 --> 00:04:26.980
Okay, and then we're going to show
some reference implementations,

00:04:26.980 --> 00:04:28.650
okay, for the large-scale.

00:04:29.420 --> 00:04:34.960
So we're going to have a question
and answer period and I have

00:04:35.070 --> 00:04:39.340
a few guests with me today,
okay, that we're going to bring up

00:04:39.430 --> 00:04:43.980
and hopefully we'll have time
to answer questions and if not,

00:04:43.980 --> 00:04:47.090
come up to the podium here.

00:04:49.150 --> 00:04:56.640
Sand Solutions was founded
in 1998 as a integrator,

00:04:56.640 --> 00:05:01.160
or a top-end integrator
of storage area networks.

00:05:01.160 --> 00:05:06.590
And in 1998, just about nothing worked.

00:05:07.440 --> 00:05:12.040
Today we have products that work,
but back in those days, it took about,

00:05:12.050 --> 00:05:15.780
well, it took us,
when we did the Sony Pictures DVD Center,

00:05:15.780 --> 00:05:20.950
it took over 300 traces for Amulex to
get their host adapter drivers right.

00:05:20.960 --> 00:05:25.400
So we've come a long way
since those early days.

00:05:27.100 --> 00:05:31.880
Prior life was at Mountain Gate,
which eventually got

00:05:31.880 --> 00:05:37.720
acquired by Ada Corporation,
which you guys are probably pretty

00:05:37.780 --> 00:05:39.990
familiar with the XSAN file system.

00:05:40.720 --> 00:05:47.370
We do largely SAN and
NAS integration work.

00:05:47.370 --> 00:05:49.460
We specialize in rich media.

00:05:49.460 --> 00:05:54.700
How many in the audience really deploy
rich media type solutions versus,

00:05:54.700 --> 00:05:57.400
okay,
so you guys are going to be probably

00:05:57.460 --> 00:06:02.360
more entertained by this presentation
than some of the enterprise guys,

00:06:02.360 --> 00:06:05.700
but all the things that we're
going to go through today,

00:06:05.700 --> 00:06:10.320
the same disciplines,
the same process applies to

00:06:10.760 --> 00:06:17.190
databases and IT applications as
it does to rich media applications,

00:06:17.190 --> 00:06:22.940
except for rich media applications tend
to stress things out a lot more than

00:06:22.940 --> 00:06:27.290
some of the database applications do.

00:06:29.100 --> 00:06:32.560
Anyway, so we have high-performance
infrastructures out there today.

00:06:32.560 --> 00:06:35.500
We have high-availability
infrastructures in both.

00:06:35.560 --> 00:06:39.540
Some of the customers we have
are Sony Pictures Entertainment.

00:06:39.540 --> 00:06:42.140
We did a lot of work for Microsoft.

00:06:42.140 --> 00:06:46.920
We put in the
Johnson Space Flight Center in

00:06:46.920 --> 00:06:53.210
Houston that do all the
simulation for the satellites,

00:06:53.270 --> 00:06:56.300
I mean for the astronauts,
for the shuttle missions.

00:06:57.220 --> 00:06:58.470
We put in that infrastructure.

00:06:58.470 --> 00:07:01.180
We've done a lot of work
for Skywalker Sound,

00:07:01.180 --> 00:07:05.720
putting in the whole entire
audio facility there.

00:07:05.720 --> 00:07:08.760
We do the Tonight Show at NBC.

00:07:08.760 --> 00:07:15.020
We do the majority of all
Warner Brothers television today.

00:07:18.550 --> 00:07:22.360
Most of our client,
I mean most of our CN installs

00:07:22.650 --> 00:07:27.340
are 10 clients and above,
or 10 host computers and above.

00:07:27.470 --> 00:07:34.920
Okay,
10 terabytes to 100 terabytes today.

00:07:35.320 --> 00:07:38.780
We're really good at tuning
real-time applications.

00:07:39.200 --> 00:07:44.090
In these SAN environments,
most of our infrastructure,

00:07:44.090 --> 00:07:47.410
as a matter of fact,
all of our infrastructure,

00:07:47.410 --> 00:07:52.380
have some Apple component in it,
both from the Power Macs, workstations,

00:07:52.470 --> 00:07:54.340
and Xserves in there.

00:07:56.340 --> 00:08:03.880
And we're highly dependent, I mean,
all our customers are really

00:08:03.880 --> 00:08:11.050
highly dependent on uptime,
okay, which these generally are

00:08:11.050 --> 00:08:15.000
not engineering deployments.

00:08:15.000 --> 00:08:15.000
They're deployments that actually, okay,
produce our customers' money.

00:08:15.230 --> 00:08:17.540
So when you've got the
cash register going,

00:08:17.670 --> 00:08:21.750
okay, they can't take any downtime.

00:08:23.090 --> 00:08:25.000
Okay,
the first part we're going to go through,

00:08:25.100 --> 00:08:28.310
okay, we're going to go through it pretty
quick because the more interesting stuff

00:08:28.310 --> 00:08:32.000
is towards the end of the presentation
when we actually get into test results.

00:08:32.000 --> 00:08:35.990
Okay, but for completeness sake, okay,
we're going to talk about

00:08:36.070 --> 00:08:37.680
the planning process.

00:08:37.680 --> 00:08:40.670
And that involves the
requirements assessment,

00:08:40.670 --> 00:08:41.700
okay, first.

00:08:41.720 --> 00:08:45.210
And then how do you design a SAN?

00:08:45.220 --> 00:08:46.130
How do you implement it?

00:08:46.150 --> 00:08:47.060
How do you document it?

00:08:47.080 --> 00:08:50.860
Okay, what services do you need to be
able to offer your end customers

00:08:50.860 --> 00:08:52.790
as far as training and support?

00:08:53.000 --> 00:08:54.100
Thank you.

00:08:57.790 --> 00:09:02.230
Okay, when we go out to a site,
we really start with what

00:09:02.300 --> 00:09:04.400
are the key business issues.

00:09:04.400 --> 00:09:09.800
Okay, and this ranges, okay,
depending on who the customer is

00:09:09.800 --> 00:09:11.420
or what their problems are today.

00:09:11.420 --> 00:09:14.290
But they may say that, okay,
we can't find anything.

00:09:14.300 --> 00:09:18.630
Everybody's got media all over the place,
but it's, you know,

00:09:18.640 --> 00:09:23.180
on their desktop or on a
removable drive or whatever.

00:09:23.180 --> 00:09:25.470
Okay, and we need to get this
thing consolidated.

00:09:26.260 --> 00:09:30.000
Okay,
but usually within the first hour or so,

00:09:30.000 --> 00:09:32.120
you have a pretty good,
clear understanding of why they

00:09:32.120 --> 00:09:36.060
think they need a SAN or why they
need a new look at how they put

00:09:36.060 --> 00:09:38.820
in their storage infrastructure.

00:09:38.820 --> 00:09:45.600
Okay, we also have to assess what
the risk adversity level is.

00:09:45.600 --> 00:09:48.710
Okay, is this a customer that
wants proven things,

00:09:48.790 --> 00:09:51.710
okay,
that have a lot of track record on them?

00:09:51.760 --> 00:09:55.250
Or is this a guy that wants
the latest and greatest thing,

00:09:55.260 --> 00:10:01.490
okay, that pushes his technology
a little bit to the edge?

00:10:01.520 --> 00:10:04.100
Okay, and we have a mix of customers.

00:10:04.100 --> 00:10:06.610
Some of them,
they won't touch anything that hasn't

00:10:06.620 --> 00:10:08.340
been out there for three years.

00:10:08.340 --> 00:10:10.340
And some that...

00:10:11.980 --> 00:10:19.190
I just want the latest and greatest thing
to make them as efficient as possible.

00:10:20.000 --> 00:10:21.800
Okay,
we look at the implementation scope,

00:10:21.800 --> 00:10:27.100
and this is -- is this a -- is this
today a department that's going to

00:10:27.100 --> 00:10:31.250
have to scale out to the full facility?

00:10:31.670 --> 00:10:35.040
Is it going to have remote
sites that need to get tied in?

00:10:35.350 --> 00:10:38.840
So we have to define what the scope
of the project is going to be.

00:10:38.990 --> 00:10:41.900
We've got to assess really
what the customer is,

00:10:42.000 --> 00:10:44.830
okay,
in terms of how much support he has.

00:10:44.830 --> 00:10:50.550
Because some things may get too
complicated for one class of customer or,

00:10:50.790 --> 00:10:54.890
you know, another type of customer,
he has a staff and he has the

00:10:54.890 --> 00:10:59.240
people that can be trained,
okay, to accomplish or to manage

00:10:59.390 --> 00:11:01.350
this thing once it's in.

00:11:04.420 --> 00:11:08.050
Okay, we have to look at the
workflow because as we change

00:11:08.140 --> 00:11:11.830
their storage infrastructure,
we change the way in

00:11:11.830 --> 00:11:13.100
which the people work.

00:11:13.240 --> 00:11:16.290
Okay,
a lot of our sites are shared media,

00:11:16.290 --> 00:11:23.660
and that's a different workflow,
or they can change the workflow, okay,

00:11:23.660 --> 00:11:28.260
versus a site that has been
using removable disk drives,

00:11:28.260 --> 00:11:29.700
for instance.

00:11:30.100 --> 00:11:33.440
Okay, we have to look at what the
technical requirements are,

00:11:33.540 --> 00:11:37.140
what applications have to
run on this infrastructure.

00:11:37.140 --> 00:11:39.640
Is there anything strange
about those applications?

00:11:39.640 --> 00:11:46.060
Because in a lot of cases, applications,
especially in the rich media space,

00:11:46.540 --> 00:11:52.960
The applications have been written for a
system that has a 12-inch cable between

00:11:52.960 --> 00:11:56.320
their disk and the computer motherboard.

00:11:56.330 --> 00:12:01.340
And when you put those things or
those applications in a network,

00:12:01.340 --> 00:12:04.330
you run into a whole new set of issues.

00:12:04.330 --> 00:12:08.680
And they usually generally
fall into the latency area.

00:12:08.680 --> 00:12:13.380
So we have to take a serious look
at what application is going to

00:12:13.410 --> 00:12:14.970
run on this new infrastructure.

00:12:16.140 --> 00:12:20.700
Okay, then this follows up on how
we're going to support this site.

00:12:20.740 --> 00:12:24.380
Okay, not for now and in the future.

00:12:24.380 --> 00:12:28.490
And how much downtime is allowed.

00:12:28.490 --> 00:12:32.400
Okay, some customers,
they work from 8 to 5.

00:12:32.400 --> 00:12:36.760
Some sites are 7 by 24
with no downtime ever.

00:12:36.760 --> 00:12:40.930
So you have to have these things
in mind when you lay out the

00:12:40.930 --> 00:12:43.710
design of these infrastructures.

00:12:44.920 --> 00:12:47.590
Okay,
your data migration requirements today.

00:12:47.590 --> 00:12:51.590
Okay, how are they going to get from what
they're using today onto the current

00:12:51.590 --> 00:12:53.560
system without taking them down?

00:12:53.560 --> 00:12:56.040
Okay,
so those things have to be explored.

00:12:57.490 --> 00:13:00.700
And then we have to look at, okay,
what is the phasing,

00:13:00.700 --> 00:13:04.170
what is the timeline,
when does a customer have any

00:13:04.170 --> 00:13:08.530
kind of window to switch over,
and then how are we going

00:13:08.530 --> 00:13:10.390
to meet their budgets?

00:13:10.630 --> 00:13:15.680
So that basically outlines what we
look for on the assessment level.

00:13:16.000 --> 00:14:22.900
[Transcript missing]

00:14:23.200 --> 00:14:27.130
But this new customer wants
Final Cut Pro across 10 machines,

00:14:27.130 --> 00:14:30.590
and then he wants to have
real-time replication.

00:14:30.610 --> 00:14:33.410
Okay,
so that real-time replication becomes

00:14:33.410 --> 00:14:40.250
incremental over a reference design
that we've already created and tested.

00:14:42.140 --> 00:14:46.500
Okay, so it's important at this stage
that we document physically

00:14:46.900 --> 00:14:51.190
the physical layout of the
architecture and the logical layout.

00:14:51.190 --> 00:14:53.690
And logical things are
how is the thing zoned?

00:14:53.820 --> 00:14:55.530
Okay, what are the volumes?

00:14:55.630 --> 00:14:59.360
Okay, how does failover occur?

00:14:59.380 --> 00:15:05.390
Okay, those kinds of things go into
the documentation package.

00:15:06.940 --> 00:15:12.920
Okay, then we talk about,
we got to look at what the future

00:15:13.000 --> 00:15:14.860
requirements of this facility are.

00:15:14.860 --> 00:15:19.390
And a lot of things are phased, okay,
that they first want to get, you know,

00:15:19.390 --> 00:15:21.460
the graphics department up, okay.

00:15:21.460 --> 00:15:25.140
And then next year
they're going to look at,

00:15:25.320 --> 00:15:29.880
they'll have budget for, say, editorial,
okay.

00:15:29.880 --> 00:15:34.850
And then they may want
to look at distribution,

00:15:34.860 --> 00:15:35.780
okay.

00:15:35.780 --> 00:15:39.410
So we have to look at it from a
broad spectrum early on because

00:15:39.410 --> 00:15:43.390
we don't want to put something in
that we're going to have to replace

00:15:43.620 --> 00:15:45.880
later if we can avoid it going in.

00:15:48.630 --> 00:15:52.740
The last thing we look at
is a lot of customers today,

00:15:52.740 --> 00:16:01.110
especially in the traditional IT area,
is they don't want to have downtime and

00:16:01.110 --> 00:16:04.100
they want proactive management tools.

00:16:04.100 --> 00:16:09.300
And what a proactive management tool
can do for you is can monitor the site,

00:16:09.300 --> 00:16:13.980
monitor your I.O., and based on policy,
alert you of certain

00:16:14.600 --> 00:16:16.350
things that are occurring.

00:16:17.200 --> 00:16:21.230
Okay, and those types of things are,
well, you just added five more users.

00:16:21.240 --> 00:16:25.190
How did that affect, okay,
the usage or latency or

00:16:25.320 --> 00:16:28.140
bandwidth of the infrastructure?

00:16:29.750 --> 00:16:35.990
And when those utilizations get
to a certain level on average,

00:16:35.990 --> 00:16:42.450
then we want to,
before we start losing real time

00:16:43.130 --> 00:16:47.040
of the current customers or,
you know, or

00:16:47.480 --> 00:16:51.540
Before we start losing bandwidth
today to our current customers,

00:16:51.570 --> 00:16:54.660
we want to upgrade to
handle the additional load.

00:16:54.700 --> 00:17:00.340
So there's a class of monitoring tools
that we're going to go through today

00:17:00.350 --> 00:17:05.620
that have that capability that we
deploy in the infrastructure early on

00:17:05.630 --> 00:17:07.960
so we can monitor for these things.

00:17:07.960 --> 00:17:12.060
And they're also great for
fault isolation and predicting

00:17:12.060 --> 00:17:13.920
failures of the optics.

00:17:14.140 --> 00:17:19.010
for instance,
if you have SFPs going out or your GBICs.

00:17:19.270 --> 00:17:22.340
or you have a particular
storage system giving you

00:17:22.340 --> 00:17:25.400
problems but you're unaware of.

00:17:27.170 --> 00:17:32.990
Okay, in the implementation phase, okay,
we normally put together a

00:17:33.000 --> 00:17:34.370
full implementation plan.

00:17:34.370 --> 00:17:39.260
Okay, and that kind of outlines
what the responsibility,

00:17:39.260 --> 00:17:43.080
our responsibilities are and what
the customer's responsibilities are.

00:17:43.320 --> 00:17:48.260
Because they kind of have to get
a step through the process or they

00:17:48.260 --> 00:17:52.360
don't really understand what's
involved in what we're going to do.

00:17:52.730 --> 00:17:55.230
Okay,
so usually there's third parties involved

00:17:55.230 --> 00:17:57.140
that have to get on the same page.

00:17:57.140 --> 00:18:00.430
We have to get involved on the
same page and our customer has

00:18:00.430 --> 00:18:02.380
to be involved in the same page.

00:18:05.200 --> 00:18:11.670
Okay, we have a list of
deliverables that we give,

00:18:11.670 --> 00:18:15.990
okay, and the customer has a
list of their deliverables.

00:18:16.090 --> 00:18:21.510
And then we try to create a
project schedule and also what the

00:18:21.510 --> 00:18:24.450
final test completion criteria is.

00:18:24.500 --> 00:18:28.800
It's important kind of to nail down
how we're going to test this thing.

00:18:28.800 --> 00:18:33.000
How do we know when it works?

00:18:33.000 --> 00:18:33.000
Okay.

00:18:33.490 --> 00:18:36.200
As you go through the process,
you find that, you know,

00:18:36.200 --> 00:18:38.740
you might be into a
project for a month or so,

00:18:38.740 --> 00:18:41.430
and then all of a sudden,
new applications just all

00:18:41.430 --> 00:18:42.840
of a sudden popped up.

00:18:42.930 --> 00:18:47.250
Okay, so you might have to have a
change control process in place.

00:18:47.390 --> 00:18:53.120
Okay, what do you do if they want
to change what you got off,

00:18:53.250 --> 00:18:56.500
deviate from the plan
that you started with?

00:18:57.400 --> 00:19:02.790
Okay, documentation is also a key
component of an installation.

00:19:02.920 --> 00:19:08.800
And here you got to kind of meet
the standards of that facility

00:19:08.800 --> 00:19:10.790
of what they expect to see.

00:19:10.790 --> 00:19:14.840
Okay, and a lot of things,
a lot of requirements actually

00:19:14.840 --> 00:19:17.200
come from procedural manuals.

00:19:17.390 --> 00:19:20.900
And these are, this is an area where,
okay, how do I fill over a server,

00:19:20.900 --> 00:19:21.780
for instance?

00:19:21.780 --> 00:19:26.190
Okay, or how do I shut down this
thing if I need to shut it down?

00:19:26.210 --> 00:19:32.810
Okay, so these kinds of procedures
need to be tested,

00:19:32.810 --> 00:19:34.480
okay, and documented.

00:19:36.100 --> 00:19:39.020
Okay, and then finally the training
and support side of it.

00:19:39.470 --> 00:19:42.380
Okay,
we specialize in training fiber channel,

00:19:42.800 --> 00:19:45.890
okay,
on the basics of the infrastructure.

00:19:45.900 --> 00:19:48.450
Okay,
how do you manage this infrastructure

00:19:48.840 --> 00:19:56.600
from the storage management side and
also the operational management side?

00:19:56.600 --> 00:19:56.600
Okay, we also...

00:19:56.710 --> 00:19:59.600
On the support side,
we've got to be able to handle break/fix.

00:19:59.650 --> 00:20:04.850
We've also got to be able to handle what
happens if a year from now they want to

00:20:04.850 --> 00:20:08.980
integrate a new application into them,
and how does that change

00:20:08.980 --> 00:20:10.990
the original design.

00:20:12.390 --> 00:20:14.650
Okay,
so now we're going to talk briefly on

00:20:15.270 --> 00:20:20.870
what we consider a large-scale deployment
versus a small-scale deployment.

00:20:21.390 --> 00:20:23.640
Okay, a large-scale deployment,
it's really,

00:20:23.640 --> 00:20:29.980
this classification is really not
a function of how many or how much

00:20:29.980 --> 00:20:32.670
capacity or how many servers you have.

00:20:32.700 --> 00:20:35.670
It's really what function,
what functions are in the

00:20:35.670 --> 00:20:39.310
infrastructure and what kind of
data sets are you dealing with.

00:20:39.410 --> 00:20:41.920
Okay,
are you dealing with real-time video?

00:20:41.920 --> 00:20:44.050
Are you dealing with real-time audio?

00:20:44.050 --> 00:20:47.050
Are you dealing with both
real-time audio and video?

00:20:47.050 --> 00:20:49.970
Okay, do you have databases?

00:20:49.970 --> 00:20:51.280
Okay, that you're dealing with?

00:20:51.300 --> 00:20:55.370
Okay,
or do you have a bunch of shared media?

00:20:55.370 --> 00:21:00.940
Or is it all allocating storage
to individual workstations?

00:21:01.010 --> 00:21:05.710
Okay, generally,
real-time characteristics

00:21:05.710 --> 00:21:11.490
are much more difficult to
deal with than non-real-time.

00:21:11.490 --> 00:21:15.000
And we're going to see that
a little bit later on here.

00:21:15.000 --> 00:21:18.500
Okay, how much redundancy do you
need for high availability?

00:21:18.500 --> 00:21:24.140
Okay, most of the large-scales need
a lot of redundancy built in.

00:21:24.560 --> 00:21:26.870
Okay, is it a homogeneous client?

00:21:26.870 --> 00:21:28.400
Is everything Mac?

00:21:28.400 --> 00:21:31.550
Or do you have Macs and Windows clients?

00:21:31.550 --> 00:21:35.390
Kind of all have to live together
in the same infrastructure.

00:21:35.490 --> 00:21:38.240
Okay, well in a large,
you're going to have both.

00:21:38.390 --> 00:21:41.740
Okay,
you have your Suns or other Unix flavors.

00:21:42.400 --> 00:23:13.500
[Transcript missing]

00:23:14.620 --> 00:23:18.120
So we start small and
we start scaling up.

00:23:18.120 --> 00:23:20.980
And that's sort of the process
that we've always gone through to

00:23:21.150 --> 00:23:23.940
stabilize a large SAN environment.

00:23:23.940 --> 00:23:29.440
Some of the things that you've got
to know about when you do this is

00:23:29.440 --> 00:23:36.250
that every disk system has a number
of logins that it will accept.

00:23:37.800 --> 00:23:43.870
So when we were at the supercomputer
center down in San Diego and they

00:23:43.870 --> 00:23:51.060
bought 800 terabytes of storage from
one of those three-letter companies,

00:23:51.060 --> 00:23:57.800
and they had 384 servers that
wanted to log in to one array,

00:23:57.800 --> 00:24:02.970
and they couldn't do that,
they have to understand

00:24:04.350 --> 00:24:09.540
A storage device typically
will allow 64 logins.

00:24:09.540 --> 00:24:14.240
So a lot of integrators out there
or people that are in the storage

00:24:14.240 --> 00:24:18.940
business don't necessarily know what
their limitations of their arrays are.

00:24:18.940 --> 00:24:23.560
But that's one area in large scale
that you've got to be aware of.

00:24:24.920 --> 00:24:30.390
Also, as you get more devices out there,
you add latency because

00:24:30.390 --> 00:24:34.360
as you have more logins,
you need more resources both on the

00:24:34.360 --> 00:24:39.540
storage controller and on the host
adapter to handle that connections.

00:24:39.540 --> 00:24:44.310
So as you start fanning out,
you get a lot more latency

00:24:44.320 --> 00:24:47.710
involved in your infrastructure.

00:24:49.350 --> 00:24:53.100
The other thing that we
look at is the queue depth.

00:24:53.180 --> 00:24:57.840
Okay, and this is the outstanding
I/Os that a host adapter can do.

00:24:57.920 --> 00:25:01.920
And when we tune a system, we'll minimize

00:25:03.030 --> 00:25:05.550
They allow QDEP to handle an application.

00:25:05.580 --> 00:25:09.120
Because if we don't,
then what happens is you can get

00:25:09.120 --> 00:25:16.310
into a hogging mode where one client
decides to do a copy and consumes all

00:25:16.310 --> 00:25:20.130
the potential bandwidth off an array,
for instance,

00:25:20.130 --> 00:25:22.870
and all your real-time streams die.

00:25:22.880 --> 00:25:26.100
So we have ways to throttle back I.O.

00:25:26.100 --> 00:25:29.660
so we can balance it across the facility.

00:25:30.740 --> 00:25:34.730
You also have to be aware of
fabric design to make sure that

00:25:34.730 --> 00:25:41.460
you have redundant fabrics or
you have isolation in your fabric

00:25:42.040 --> 00:25:44.970
so that if you lose a switch,
it doesn't bring down the whole facility.

00:25:44.980 --> 00:25:49.240
Or if you need to do
firmware upgrades later on,

00:25:49.240 --> 00:25:54.850
when the thing's in production,
how's that going to affect the

00:25:55.120 --> 00:25:57.800
users that are on the system?

00:25:58.350 --> 00:26:03.340
Okay, and the third item on
the slide is really,

00:26:03.370 --> 00:26:04.570
is in the operational.

00:26:04.580 --> 00:26:09.300
We have storage provisioning.

00:26:09.700 --> 00:26:13.770
We have some customers that
they provision storage out

00:26:13.770 --> 00:26:16.240
to their clients once a year.

00:26:16.240 --> 00:26:20.640
Okay,
and they may revise that provisioning,

00:26:20.640 --> 00:26:26.740
okay, you know, minor,
make minor adjustments after a year.

00:26:26.740 --> 00:26:28.180
They may add more storage or something.

00:26:28.180 --> 00:26:33.600
And then we got clients that
have to provision or reprovision

00:26:34.000 --> 00:26:36.610
storage 30 times a day.

00:26:37.620 --> 00:26:40.280
Well, you got to have the
management tools in place,

00:26:40.390 --> 00:26:44.280
okay, that allow you to do those
types of provisionings.

00:26:48.140 --> 00:26:51.640
Okay, backup, recovery, and archive.

00:26:51.750 --> 00:26:58.270
Okay, this is a great topic today, okay,
because this paradigm of going disk

00:26:58.270 --> 00:27:01.540
to tape is kind of fading away.

00:27:01.620 --> 00:27:03.540
Everybody wants to be -- if
they're going to back up,

00:27:03.540 --> 00:27:07.730
they might as well minimize
the time it takes to restore.

00:27:07.890 --> 00:27:13.370
So we're seeing a lot of requests out
there today for continuous backup.

00:27:13.550 --> 00:27:25.120
This is an infrastructure where every
I/O is mirrored in real time and

00:27:25.360 --> 00:27:28.760
at any point in time they can say,
"Okay, I want to roll back to

00:27:28.850 --> 00:27:30.460
what this was 10 hours ago.

00:27:30.820 --> 00:27:35.130
Give me a new volume with what it
looked like 10 hours ago." So we're

00:27:35.130 --> 00:27:39.160
getting involved in a lot of
those types of integrations today.

00:27:39.360 --> 00:27:45.180
Snapshots are where they want a
point-in-time image every couple

00:27:45.180 --> 00:27:50.490
hours of what the data looked like
and be able to manage those things.

00:27:51.480 --> 00:27:52.110
Okay.

00:27:52.440 --> 00:27:57.170
And as it used to be, an IT facility,
if you lost your data,

00:27:57.170 --> 00:28:02.260
you go to the IT manager and he says,
could you roll back that old backup tape,

00:28:02.490 --> 00:28:05.710
okay, if you can read it, okay,
and bring it back online

00:28:05.710 --> 00:28:06.890
so I can get that file off.

00:28:06.950 --> 00:28:07.820
I just deleted it.

00:28:07.820 --> 00:28:08.380
Okay.

00:28:08.380 --> 00:28:11.580
Well, in today's world,
that stuff's getting

00:28:11.580 --> 00:28:16.590
pushed more to the user,
okay, so the user can restore his own

00:28:16.590 --> 00:28:22.880
files instead of relying on a
central IT department to do that.

00:28:23.210 --> 00:28:27.670
Okay,
and finally we have the management tools.

00:28:27.750 --> 00:28:31.850
Okay, and these again is where,
this is where we start

00:28:31.850 --> 00:28:36.640
introducing proactive management
versus reactive management.

00:28:36.750 --> 00:28:41.440
And proactive can help predict
when we're going to start seeing

00:28:41.540 --> 00:28:46.290
problems out there instead of
reacting when a problem does come up,

00:28:46.290 --> 00:28:49.210
which usually leads to downtime.

00:28:50.890 --> 00:28:55.980
And those tools also help
eliminate the finger pointing.

00:28:57.700 --> 00:29:02.500
Okay, next we're going to start with
a small reference design and run

00:29:02.500 --> 00:29:08.740
some real live data through it,
okay, for -- on the Xserve RAID.

00:29:08.740 --> 00:29:13.350
So we get an idea of what does an
Xserve RAID -- how does that perform

00:29:13.480 --> 00:29:16.540
and how is that going to scale?

00:29:18.370 --> 00:29:24.060
Okay, so we're going to start with,
this is going to be,

00:29:24.060 --> 00:29:27.200
we're going to run some high-def
data through this thing.

00:29:27.200 --> 00:29:31.720
So we're going to start with a
thing called a test generator.

00:29:31.720 --> 00:29:32.860
And I have one here.

00:29:32.860 --> 00:29:37.900
It's a small little package
that basically you can

00:29:37.900 --> 00:29:41.760
program in any type of video,
okay,

00:29:41.760 --> 00:29:44.100
that you want to test your SAN with.

00:29:44.840 --> 00:29:51.640
And it goes from standard def all
the way up to DOLINK film res video.

00:29:51.640 --> 00:29:58.350
It saves us a ton of time out there
because without that thing we have to get

00:29:58.350 --> 00:30:01.720
a tape and a tape deck and a video deck.

00:30:01.740 --> 00:30:05.340
And we've got to get a tape with the
right format and things like that.

00:30:05.340 --> 00:30:07.560
Well,
this thing can be programmed basically

00:30:07.580 --> 00:30:09.610
to handle any format that we might see.

00:30:13.030 --> 00:30:16.900
Okay, the other things that may be new to
you is we're going to put taps in our

00:30:16.900 --> 00:30:20.820
small design reference implementation.

00:30:20.820 --> 00:30:27.320
And what a tap is,
is an optical device that allows us

00:30:27.320 --> 00:30:33.480
to plug an analyzer in without getting
in the direct way of the traffic.

00:30:34.000 --> 00:30:41.740
Okay, so what it does is it splits the
light from your GBIX basically,

00:30:41.740 --> 00:30:45.600
or your SFPs, okay,
and allows us to monitor, okay,

00:30:45.600 --> 00:30:47.520
that connection.

00:30:47.900 --> 00:30:52.230
Without getting in the
path of the connection.

00:30:52.310 --> 00:30:59.890
And FinnoSER has an extensive
line of these taps that we use.

00:31:00.120 --> 00:31:03.740
Okay, then we have a Finisterre
Net Wisdom probe.

00:31:03.740 --> 00:31:06.390
And what this probe does
is it collects traffic.

00:31:06.520 --> 00:31:10.420
It collects all the exchanges that go on.

00:31:10.420 --> 00:31:13.200
An exchange is equivalent
to a SCSI command.

00:31:13.200 --> 00:31:16.660
Okay,
and it collects basically all of the

00:31:16.910 --> 00:31:19.990
traffic that goes on that connection.

00:31:20.190 --> 00:31:23.740
And then there's a thing called a portal.

00:31:24.630 --> 00:31:29.720
And the portal can consolidate
a lot of probes and centralize

00:31:29.720 --> 00:31:32.750
all that data to help present it.

00:31:33.250 --> 00:31:38.910
Okay, we also have the capability if the
data would indicate something or

00:31:38.910 --> 00:31:41.240
we're getting errors someplace,
it has the ability,

00:31:41.240 --> 00:31:43.140
that portal has the ability to email us.

00:31:43.140 --> 00:31:46.700
Okay,
so not only for this test infrastructure,

00:31:46.700 --> 00:31:50.160
but it's also used for
proactive management.

00:31:50.160 --> 00:31:54.010
Okay,
and this is what we see from the views,

00:31:54.010 --> 00:31:56.140
okay, of the portal.

00:31:56.140 --> 00:32:01.790
Views is an application that
basically shows us we can

00:32:01.790 --> 00:32:05.000
select megabytes a second.

00:32:05.000 --> 00:32:06.170
We can select the latency.

00:32:06.230 --> 00:32:08.780
We can select, okay, any kind of errors.

00:32:08.840 --> 00:32:10.450
Okay, they go on.

00:32:10.460 --> 00:32:13.420
We can look at the queue
depth of those conversations.

00:32:13.420 --> 00:32:19.860
Okay, just about anything that you want
to know about the SCSI traffic.

00:32:20.330 --> 00:32:24.540
We can find out from the probe
and the management utility.

00:32:24.540 --> 00:32:25.890
Okay.

00:32:28.080 --> 00:32:35.530
Okay, so in the small reference design,
we have essentially two Power Macs,

00:32:35.530 --> 00:32:37.130
two Xserves.

00:32:37.130 --> 00:32:39.000
We're going to run XAN on this thing.

00:32:39.000 --> 00:32:43.390
We got two Xserve RAIDs,
or three Xserve RAIDs, I'm sorry,

00:32:43.390 --> 00:32:47.240
a Cisco switch,
and it looks kind of like this.

00:32:47.860 --> 00:32:54.030
So we have our test generator producing
data that goes through the Power Mac,

00:32:54.030 --> 00:32:57.720
okay, that has an AGA video card in it,
okay,

00:32:57.720 --> 00:33:03.760
that goes down to the brocade switch,
and we have two arrays dedicated

00:33:03.870 --> 00:33:08.800
for storage for XAN data,
and then we have another Xserve

00:33:08.800 --> 00:33:11.380
RAID as our metadata device.

00:33:11.380 --> 00:33:17.730
And then we got the two Xserve as our
metadata and a backup metadata server.

00:33:17.860 --> 00:33:19.000
Okay.

00:33:19.800 --> 00:33:24.820
Okay, and the first test we
run is really just I/O,

00:33:24.960 --> 00:33:28.740
direct I/O on the disk, okay,
because we want to understand what

00:33:28.740 --> 00:33:31.120
the disk performance looks like.

00:33:31.200 --> 00:33:36.740
Okay, and it's incredibly important at
this stage to understand what it

00:33:36.900 --> 00:33:39.700
looks like with known patterns.

00:33:39.700 --> 00:33:43.890
So when we get to scaling,
we can help predict, okay,

00:33:44.010 --> 00:33:48.690
this data here will help
predict how it's going to scale.

00:33:48.690 --> 00:33:53.700
So our first three tests
are sequential RAIDs.

00:33:53.840 --> 00:33:58.430
Okay, the red, okay,
the pattern over on the left-hand side,

00:33:58.430 --> 00:34:01.690
okay, is, reflects the data pattern.

00:34:01.880 --> 00:34:03.690
Okay, so let's take the first one,
for instance.

00:34:03.700 --> 00:34:07.030
Well,
we have four threads that are slightly

00:34:07.030 --> 00:34:09.700
offset at the beginning of the disk.

00:34:09.780 --> 00:34:10.950
Okay, so we have three threads that are
slightly offset at the beginning

00:34:10.950 --> 00:34:11.700
of the disk that are doing reads.

00:34:11.840 --> 00:34:15.500
Okay, the prefetch was set at 64 K bytes.

00:34:15.750 --> 00:34:19.650
That's the smallest that
the XSERVE RAID allows.

00:34:19.710 --> 00:34:24.700
Okay, and in this mode,
we were achieving 110 megabytes a second.

00:34:24.700 --> 00:34:28.290
Okay,
and then we also have some parameters

00:34:28.290 --> 00:34:33.720
listed here that reflect what the
exchange times were or the amount of

00:34:33.720 --> 00:34:37.660
latency it was to complete those things.

00:34:37.740 --> 00:34:38.160
Okay,
and then we also have the data that we're

00:34:38.170 --> 00:34:38.700
going to be using to do the scaling.

00:34:38.730 --> 00:34:38.960
Okay,
and then we also have some parameters

00:34:38.960 --> 00:34:39.350
listed here that reflect what the
exchange times were or the amount of

00:34:39.370 --> 00:34:39.700
latency it was to complete those things.

00:34:39.700 --> 00:34:42.440
what size we're writing,
so we're documenting what

00:34:42.600 --> 00:34:47.700
size of IO we're doing,
because we have, we can vary that.

00:34:47.970 --> 00:34:48.640
Okay?

00:34:48.710 --> 00:34:53.700
And the XSERV rate is really
optimized for transactions

00:34:53.700 --> 00:34:56.700
that happen at one megabyte,
okay?

00:34:56.700 --> 00:35:01.190
It has a megabyte of cash in it, okay,
so that's the size of

00:35:01.190 --> 00:35:03.700
IO that we selected at this.

00:35:03.700 --> 00:35:08.150
And then we show the pending exchanges,
and that's how many outstanding IOs,

00:35:08.200 --> 00:35:12.700
if we have four threads, one thread each,
that's how we get to the four.

00:35:12.850 --> 00:35:15.470
So, stepping down to test.

00:35:16.030 --> 00:35:19.900
You see that at 64K we got 110.

00:35:19.900 --> 00:35:25.440
At 512, which is the next level up,
we're getting 165.

00:35:25.440 --> 00:35:31.410
And at 8 meg read-ahead, or prefetch,
we're getting 191.

00:35:31.530 --> 00:35:36.460
Those are outstanding numbers
for a 6 plus 1 configuration.

00:35:36.460 --> 00:35:39.960
You got 6 drives and a parity drive.

00:35:40.880 --> 00:35:45.200
So those are outstanding
numbers in the industry,

00:35:45.200 --> 00:35:50.080
really, for read performance across
a 6 plus 1 configuration.

00:35:50.080 --> 00:35:56.350
The next thing we did is look at, okay,
those are the fast tracks.

00:35:56.350 --> 00:35:59.000
How does it perform on the slow tracks?

00:35:59.000 --> 00:36:02.570
Because when we design a large
scale and have to scale it up,

00:36:02.700 --> 00:36:06.020
we kind of have to take into
consideration that we might

00:36:06.020 --> 00:36:07.270
have data way out there.

00:36:07.280 --> 00:36:10.810
And how is that going to
affect the overall performance?

00:36:10.890 --> 00:36:15.930
So we look at -- you'll see
four tests in that area,

00:36:15.930 --> 00:36:16.880
okay?

00:36:16.980 --> 00:36:22.070
And the difference between those
tests are that there's now a new

00:36:22.300 --> 00:36:28.730
2 terabyte LUN level -- LUN limit,
okay?

00:36:29.050 --> 00:36:35.050
Tiger came out and XAN 1.1 came out.

00:36:35.510 --> 00:36:39.600
Okay, it broke through the
2 terabyte LUN limit.

00:36:39.600 --> 00:36:44.000
Okay, and to do that,
there's a logical block address

00:36:44.000 --> 00:36:47.000
that went from 4 bytes to 8 bytes.

00:36:47.160 --> 00:36:50.560
Okay,
so this 4 bytes to 8 bytes logical block

00:36:50.620 --> 00:36:53.570
address is kind of new to the industry.

00:36:53.800 --> 00:36:59.600
So we wanted to see, okay,
how the array hung up, I mean handled,

00:36:59.610 --> 00:37:01.460
being out there in those extended tracks.

00:37:01.460 --> 00:37:03.170
So that's what those tests were.

00:37:03.170 --> 00:37:07.190
Okay, and as you can see,
the performance dropped off slightly,

00:37:07.190 --> 00:37:10.440
okay, when we were dealing
with tracks way out in,

00:37:10.440 --> 00:37:12.650
beyond the two terabyte limit.

00:37:13.600 --> 00:37:19.460
Okay, they dropped down from 165
and 191 down to about 110.

00:37:19.580 --> 00:37:23.700
Okay, we got to know that when we look at
how we're going to scale this thing.

00:37:23.700 --> 00:37:26.990
Okay, the next set,
the next two is that we're

00:37:26.990 --> 00:37:29.580
going to go from sequential I.O.

00:37:29.620 --> 00:37:33.420
to random I.O., okay, because random I.O.

00:37:33.420 --> 00:37:37.700
is always going to be slower
than sequential I.O., okay?

00:37:37.840 --> 00:37:39.710
Okay, it doesn't matter whose
disk system it is,

00:37:39.710 --> 00:37:42.970
when you add in the seek times,
your performance goes down.

00:37:42.970 --> 00:37:45.920
So we had two threads at
the beginning of the disk,

00:37:45.920 --> 00:37:48.460
two threads at the end of the disk, okay?

00:37:48.460 --> 00:37:54.290
And in fact, at 64K, by transactions,
we went from 110 down to 44, okay,

00:37:54.290 --> 00:37:57.250
which could be, I mean, is explainable.

00:37:58.110 --> 00:38:02.410
And then we looked,
and that was at the 64K prefetch.

00:38:02.410 --> 00:38:07.480
And at 8-meg prefetch,
we went from 190 all the way down to 135.

00:38:07.480 --> 00:38:10.750
I should comment here that
these are half the array.

00:38:10.750 --> 00:38:15.400
This is a 6 plus 1 just portion,
not the full array.

00:38:15.400 --> 00:38:19.400
So for the full performance of the array,
you have to double it.

00:38:19.400 --> 00:38:23.600
But these are excellent numbers, okay,
generally speaking.

00:38:25.800 --> 00:38:32.820
Okay, then we went into write tests and
what its performance on writes were.

00:38:32.940 --> 00:38:37.380
Okay, and what we found at 64K,
it didn't matter what the prefix size

00:38:37.380 --> 00:38:43.250
was because that's more or less in reads,
okay, but what does matter is

00:38:43.370 --> 00:38:45.160
the write cache enable,
okay.

00:38:45.160 --> 00:38:49.120
But it didn't matter if it
was random or sequential,

00:38:49.120 --> 00:38:53.590
okay, because of the write pack cache,
we were getting the same performance

00:38:53.590 --> 00:38:57.500
and it would all range between 87
megs a second and 90 megs a second,

00:38:57.500 --> 00:38:58.400
okay.

00:38:58.400 --> 00:39:02.050
And that's the total performance
for writes on that segment,

00:39:02.060 --> 00:39:02.620
okay.

00:39:02.620 --> 00:39:05.250
So it doesn't matter how
many threads you have,

00:39:05.310 --> 00:39:07.600
okay,
the total performance is going to be

00:39:07.680 --> 00:39:10.440
roughly 90 megs a second on writes,
okay.

00:39:10.440 --> 00:39:14.580
So the fact that this is predictable,
okay.

00:39:15.160 --> 00:39:20.290
And believe me,
not all arrays are predictable.

00:39:20.340 --> 00:39:25.400
The fact that this is predictable
allows us to scale this thing,

00:39:25.550 --> 00:39:25.910
okay.

00:39:26.140 --> 00:39:29.440
If we were getting numbers
all over the place,

00:39:29.440 --> 00:39:34.050
okay, we would be lost as to how many
arrays it takes to achieve,

00:39:34.050 --> 00:39:36.400
you know, a number of users.

00:39:37.100 --> 00:39:40.280
Okay, so anyway, the next test,
series of tests,

00:39:40.280 --> 00:39:43.330
was what happens when you
have a reader and a writer,

00:39:43.330 --> 00:39:46.040
okay, in the infrastructure.

00:39:46.040 --> 00:39:50.180
Because in an XAN environment, okay,
or any disk environment,

00:39:50.370 --> 00:39:53.840
you're going to have both readers
and writers simultaneously.

00:39:53.840 --> 00:39:55.360
How does that affect things?

00:39:55.520 --> 00:39:58.110
Okay, you also want to...

00:39:58.310 --> 00:40:02.630
In captures,
you want to make sure that your

00:40:02.630 --> 00:40:05.810
writes have precedence over reads.

00:40:06.240 --> 00:40:09.910
Because the last thing you want
to do is lose data on acquisition.

00:40:09.940 --> 00:40:16.260
Okay, if you're out there, okay,
in the oil and gas industry and you're

00:40:16.270 --> 00:40:20.840
recording data that can't be re-recorded,
okay, to your disk subsystem,

00:40:20.840 --> 00:40:24.850
you don't want to lose it because a read
came in and slowed up your disk so you

00:40:24.850 --> 00:40:27.240
didn't have enough bandwidth to write.

00:40:27.260 --> 00:40:30.970
So we wanted to look at how well
it's going to behave in that area.

00:40:31.760 --> 00:40:36.930
So in read-write performance,
we found out that with

00:40:36.930 --> 00:40:42.440
one writer and one reader,
okay, at 64K, okay,

00:40:42.440 --> 00:40:44.090
your read performance went way down.

00:40:44.100 --> 00:40:47.550
So writes totally took
over at the small IO side.

00:40:47.560 --> 00:40:53.200
Okay, in the mid-level prefetch area,
okay, it was modest.

00:40:53.200 --> 00:40:55.870
And at the high, okay,
the reads and the writes

00:40:55.870 --> 00:40:57.690
started to balance out some.

00:40:57.700 --> 00:41:00.900
And that's because it was
reading ahead all the time,

00:41:00.900 --> 00:41:01.480
okay?

00:41:01.500 --> 00:41:04.870
And when it reads ahead all the time,
you're likely to get more readback,

00:41:04.870 --> 00:41:05.810
read bandwidth.

00:41:05.820 --> 00:41:08.880
So anyway,
that's what we determined there.

00:41:08.900 --> 00:41:12.960
And then what happens when you
add multiple readers and writers?

00:41:12.960 --> 00:41:13.560
What happens?

00:41:13.560 --> 00:41:21.510
Okay, and this area,
what we found was if you absolutely

00:41:21.510 --> 00:41:26.660
positively have to have your writes,
okay, then go with the lower,

00:41:26.680 --> 00:41:29.160
go with the 64K, okay?

00:41:29.190 --> 00:41:31.240
If you want to balance
your writes and your reads,

00:41:31.240 --> 00:41:35.950
then go either to the 512 or the 8 megs,
okay?

00:41:36.250 --> 00:41:42.420
But the 8 megs can drop your reads down
to levels that may not be acceptable.

00:41:42.460 --> 00:41:46.060
So we found that 512 was a good medium,
okay, in that.

00:41:46.160 --> 00:41:49.400
And then the last test was what
happens when you turn write cache off,

00:41:49.400 --> 00:41:51.660
and you never want to
turn write cache off,

00:41:51.660 --> 00:41:52.060
okay?

00:41:52.200 --> 00:41:55.160
Because it dropped not only
the writes performance,

00:41:55.300 --> 00:41:58.160
but it also dropped the read performance.

00:41:59.790 --> 00:42:04.830
Okay, so if we look at the
finiture traces of these,

00:42:05.000 --> 00:42:11.200
we're going to pick test number 16, okay,
which is the 8-meg transfer size,

00:42:11.200 --> 00:42:15.270
and we want to see what's actually
going on as we add these threads.

00:42:15.300 --> 00:42:20.210
Okay, so this graph, okay,
is showing the I-O size,

00:42:20.210 --> 00:42:23.590
which was set up for 1 megabyte.

00:42:24.710 --> 00:42:31.840
Okay, the yellow at the beginning shows
that 1 megabyte reader out there,

00:42:31.840 --> 00:42:34.870
okay,
and then when the red part comes in,

00:42:34.880 --> 00:42:37.990
that's where the second
reader came online,

00:42:38.000 --> 00:42:40.280
and then later on there's
another write-in reader,

00:42:40.280 --> 00:42:43.690
but they're all the same I-O size,
so basically that's why it's covered.

00:42:43.700 --> 00:42:45.660
Then we're going to look
at pinning exchanges.

00:42:45.660 --> 00:42:51.740
Okay, well, pinning exchange is basically
showing us how many outstanding

00:42:51.740 --> 00:42:54.370
I-Os are out at any given time.

00:42:54.680 --> 00:42:59.190
Okay, and it steps up from 1 to 4 over
time as we added those users.

00:42:59.200 --> 00:43:02.740
Okay,
so this is documenting what the test was,

00:43:02.830 --> 00:43:03.640
really.

00:43:03.640 --> 00:43:05.880
Then we're going to look
at megabytes a second.

00:43:05.900 --> 00:43:07.890
Now, this is the interesting part.

00:43:09.790 --> 00:43:13.050
Okay, at the beginning of the
test when we had one reader,

00:43:13.130 --> 00:43:16.170
okay, we had 125 megs a second
coming off this thing.

00:43:16.170 --> 00:43:18.970
When we added the writer,
the writer took precedence

00:43:18.970 --> 00:43:20.420
over the reader,
okay?

00:43:20.420 --> 00:43:25.760
And it's running roughly at 70,
and the reads are running

00:43:25.760 --> 00:43:28.760
roughly at 35 or so,
okay?

00:43:28.760 --> 00:43:32.240
And that was fine, okay,
because that's your precedent.

00:43:32.240 --> 00:43:35.280
The writes have
precedence over the reads.

00:43:35.600 --> 00:43:40.190
Okay, then what happens when we write,
when we put on the second reader,

00:43:40.190 --> 00:43:41.950
which was the third thread?

00:43:41.950 --> 00:43:46.240
Okay, in those cases,
because of the read-ahead was so large,

00:43:46.500 --> 00:43:49.560
the reads started out
overwhelming the writes,

00:43:49.560 --> 00:43:52.280
and the write performance went down.

00:43:52.300 --> 00:43:55.420
Okay, so we have to understand this,
okay,

00:43:55.420 --> 00:43:58.180
for how this thing is going to scale.

00:43:58.200 --> 00:44:01.580
Okay, then we add the fourth one,
and it stayed constant.

00:44:01.600 --> 00:44:05.250
Okay, so we can start predicting that.

00:44:05.520 --> 00:44:09.660
In that eight-meg prefetch, okay,
our writes are going to

00:44:09.660 --> 00:44:11.930
settle down at a level,
and our reads will

00:44:11.930 --> 00:44:13.400
settle down at a level.

00:44:13.420 --> 00:44:17.920
Okay, this is incredibly important to
understand how these things scale.

00:44:19.710 --> 00:44:24.830
Okay, this is another thing that you
normally don't see is latency.

00:44:24.920 --> 00:44:28.320
Okay,
just because you're getting the same

00:44:28.320 --> 00:44:31.670
bandwidth doesn't mean that the data
doesn't take a little longer to get in,

00:44:31.830 --> 00:44:32.180
come in.

00:44:32.200 --> 00:44:38.860
So this graph here shows that
the read latency for your,

00:44:38.860 --> 00:44:43.140
the read latency,
this measures read and write latency.

00:44:43.380 --> 00:44:50.700
The red portion is your reads and
the purple portion is your writes.

00:44:50.700 --> 00:44:54.890
So even though the bandwidth kept
constant for those last two threads,

00:44:54.980 --> 00:44:58.570
what really happened was the
write latency actually went up.

00:44:58.700 --> 00:44:59.990
Okay.

00:45:00.200 --> 00:45:02.200
And that means it's just a
little harder to get the data in.

00:45:02.280 --> 00:45:04.390
that right data back.

00:45:04.940 --> 00:45:07.470
But the RAID stayed constant.

00:45:07.580 --> 00:45:11.340
Okay, and this is more evident
when we look at not the max,

00:45:11.380 --> 00:45:13.140
but the average.

00:45:13.220 --> 00:45:15.130
Okay, latency.

00:45:15.400 --> 00:45:19.380
And this is taking consideration
of the MIMS and the MACs

00:45:19.460 --> 00:45:21.690
during those time periods.

00:45:23.960 --> 00:45:28.070
Okay, the other thing we found when
we did this testing was how we

00:45:28.070 --> 00:45:30.930
determined how XSAN stripes.

00:45:31.810 --> 00:45:34.460
And this is going to be
important on how we scale.

00:45:34.560 --> 00:45:40.700
And there's two parameters that you
specify when you specify a file system.

00:45:40.830 --> 00:45:44.480
Okay, one is the stripe breadth and the
other one is the block allocation.

00:45:44.480 --> 00:45:48.970
Well, the block allocation is how much
they read or write at a time.

00:45:48.970 --> 00:45:52.030
Okay, and the stripe breadth is how many
am I going to do it to one target

00:45:52.110 --> 00:45:53.640
before I advance to the next.

00:45:53.640 --> 00:45:57.690
Okay, well, typically they advise,
you know,

00:45:57.700 --> 00:46:01.300
something that multiplies out to one meg.

00:46:01.340 --> 00:46:03.200
So we took an example of four.

00:46:03.770 --> 00:46:09.140
Four stripe breadth and a
256 K byte block allocation.

00:46:09.200 --> 00:46:13.670
Okay,
so this is how the transfers take place.

00:46:13.670 --> 00:46:17.230
Okay, you have four to the first device,
then followed by four

00:46:17.320 --> 00:46:18.720
to the second device.

00:46:18.720 --> 00:46:22.780
Okay, and then the third and the fourth
before it comes back to the first.

00:46:22.780 --> 00:46:29.080
Well, when you're reading, okay,
a 64 K block prefetch isn't

00:46:29.130 --> 00:46:30.980
going to do you much good.

00:46:30.980 --> 00:46:37.980
much good, okay,
when your transactions are at 256K, okay.

00:46:37.980 --> 00:46:41.070
An 8 meg isn't going to do you
too good because that's kind of

00:46:41.070 --> 00:46:46.190
overwhelming the reads on the readback,
okay, and take more time, okay.

00:46:46.190 --> 00:46:50.910
So a 512 is just about
right on the prefetch size,

00:46:50.960 --> 00:46:53.990
okay, for this file system.

00:46:54.150 --> 00:47:01.810
And what happens is basically by the time
the IOs go through from 1 to 2 to 3 to 4,

00:47:01.810 --> 00:47:06.920
by the time it gets back to 1,
it's already prefetched the next

00:47:06.920 --> 00:47:10.220
two IOs when you set that to 512.

00:47:10.220 --> 00:47:17.610
So we found that the 512K
byte prefetch is ideal for at

00:47:17.610 --> 00:47:21.900
least this XAMPP file system

00:47:22.650 --> 00:47:27.080
Okay, the next part of what we did was we
started running high-def data through it.

00:47:27.080 --> 00:47:28.420
Okay.

00:47:28.540 --> 00:47:31.720
Because high-def or any kind of video
source that's a constant data stream,

00:47:31.720 --> 00:47:37.830
okay, shows up all kinds of array issues
because most arrays out there

00:47:37.830 --> 00:47:40.900
were never designed for video,
okay.

00:47:41.530 --> 00:47:46.990
Fortunately, the Xserve RAID, okay,
is a perfect device designed for video.

00:47:46.990 --> 00:47:52.020
It's designed for everything else,
but it handles video well,

00:47:52.020 --> 00:47:54.080
video extremely well.

00:47:54.450 --> 00:48:00.530
Okay, so we changed our test reference
implementation a little bit,

00:48:00.630 --> 00:48:05.600
and we moved the probe
up to the host side,

00:48:05.670 --> 00:48:09.530
because we're going to run
high def through one channel.

00:48:09.960 --> 00:48:14.860
Okay,
and we're going to stripe across four of

00:48:14.860 --> 00:48:19.640
the 6 plus 1 RAID groups in two arrays.

00:48:19.670 --> 00:48:21.620
And then we have a metadata server.

00:48:21.620 --> 00:48:25.690
So this is running Tiger, okay,
and it's running the

00:48:25.690 --> 00:48:28.120
1.1 XAMP file system.

00:48:29.070 --> 00:48:34.020
Okay, and the first thing I want to show
you is a slide of what we should

00:48:34.080 --> 00:48:36.580
be looking for and what is bad.

00:48:36.620 --> 00:48:46.290
Okay, and this is a slide or a trace
of a popular array out there,

00:48:46.290 --> 00:48:51.570
okay, that competes with Xserve RAID,
okay, probably a lot more

00:48:51.570 --> 00:48:54.430
expensive than Xserve RAID,
and how it handled the

00:48:54.430 --> 00:48:55.860
standard def stream.

00:48:55.880 --> 00:49:00.530
Okay, and what you see is three bands.

00:49:00.790 --> 00:49:06.040
Okay, the top band is total latency,
the middle band is bandwidth,

00:49:06.040 --> 00:49:13.010
and the bottom band is its
maximum write exchange.

00:49:13.130 --> 00:49:15.600
Okay,
and that's the time during that second,

00:49:15.600 --> 00:49:17.200
what's the longest time it took.

00:49:17.200 --> 00:49:22.350
Okay, and what you see is it's constant
for a little while on that latency,

00:49:22.350 --> 00:49:24.200
and then you get a little blivet.

00:49:24.200 --> 00:49:28.100
And then a little later
on you get a big blivet.

00:49:28.730 --> 00:49:32.760
Okay, well that big blivet is really
bad because if you get too many

00:49:32.760 --> 00:49:38.780
of those big blivets that stack up
together or just become too big,

00:49:38.780 --> 00:49:43.470
okay, your stream will fall off
in terms of acquisition.

00:49:43.480 --> 00:49:49.470
Okay, this is common throughout the
industry for this subsystems.

00:49:49.470 --> 00:49:55.830
Okay,
so that's why we're an Apple reseller.

00:49:56.640 --> 00:49:59.880
Okay, that's why we use Xserve RAIDs.

00:49:59.880 --> 00:50:02.840
Okay,
it's because of this kind of problems,

00:50:02.870 --> 00:50:04.980
okay, on competitive products out there.

00:50:04.980 --> 00:50:08.210
You have to have stability
or you cannot scale.

00:50:08.210 --> 00:50:11.390
You have to have predictability
or you can't scale.

00:50:11.420 --> 00:50:14.920
This kind of latency problem, okay,
because of house cleaning.

00:50:14.920 --> 00:50:20.870
And house cleaning is the functions
that that controller does in its

00:50:21.040 --> 00:50:26.100
spare time to clean up its cache
or whatever it's going to do,

00:50:26.640 --> 00:50:29.460
to manage itself.

00:50:29.730 --> 00:50:34.300
But if you look at, I mean,
if you're doing a lot of video stuff,

00:50:34.330 --> 00:50:39.460
this is the thing you need to look
at on how well that array performs

00:50:39.460 --> 00:50:41.390
with constant data going through it.

00:50:41.400 --> 00:50:44.320
And the way you test it
is pretty straightforward.

00:50:46.110 --> 00:50:47.890
So anyway, this is bad.

00:50:48.060 --> 00:50:54.240
Okay, this is the capture that
we had for high def,

00:50:54.720 --> 00:50:57.200
okay, on the Xserve RAID.

00:50:57.250 --> 00:50:58.200
Okay.

00:50:58.340 --> 00:51:03.020
The top line measures 158 megs a second.

00:51:03.340 --> 00:51:05.730
That's the rate for high-def data.

00:51:05.880 --> 00:51:08.220
Okay, that line is constant.

00:51:08.390 --> 00:51:11.980
The next one down,
the next two down are latency curves.

00:51:12.130 --> 00:51:15.300
Okay, those are completely flat.

00:51:15.410 --> 00:51:21.270
Okay, that little blivet you
see is a zoomed up,

00:51:21.310 --> 00:51:26.590
okay, event, okay,
that happens once a minute.

00:51:26.630 --> 00:51:31.060
Okay, in Tiger and XAN 1.1.

00:51:31.100 --> 00:51:33.280
Okay, at least what we found.

00:51:33.300 --> 00:51:37.040
And it's zoomed up here,
and we're going to see how much of an

00:51:37.070 --> 00:51:39.180
effect it has a little bit later on.

00:51:39.180 --> 00:51:43.040
Okay, but basically that's a 4K read
that happens once a minute.

00:51:43.060 --> 00:51:50.050
Okay, but the point of this slide is
that the latency curves are flat.

00:51:50.090 --> 00:51:53.970
Okay, very well behaved with
streaming media down the thing.

00:51:53.980 --> 00:51:55.130
Okay.

00:51:55.660 --> 00:51:57.160
Okay, so we're going to look
a little bit closer.

00:51:57.160 --> 00:52:03.600
Again, I/O size,
when the AGA card produces

00:52:03.600 --> 00:52:07.840
video and the file system takes
over and moves it to disk,

00:52:07.940 --> 00:52:12.890
it runs at a one meg rate when
you go through one channel.

00:52:13.030 --> 00:52:15.340
Okay, these are pending exchanges.

00:52:15.570 --> 00:52:20.860
They vary up to about four.

00:52:20.860 --> 00:52:20.870
Okay.

00:52:21.070 --> 00:52:24.450
The megabytes per second is constant,
okay?

00:52:24.580 --> 00:52:26.340
Very clean.

00:52:26.430 --> 00:52:29.780
Okay, here's the latency curve, okay?

00:52:29.900 --> 00:52:32.040
There's no spikes in this curve, okay?

00:52:32.040 --> 00:52:33.640
We're talking about the top one.

00:52:33.770 --> 00:52:37.290
Okay, that's the right exchange latency.

00:52:37.650 --> 00:52:39.160
There's no spikes in that.

00:52:39.280 --> 00:52:43.160
You can go for hours and hours
and hours and not get spikes.

00:52:43.290 --> 00:52:46.150
Okay, that's predictability.

00:52:48.800 --> 00:54:42.700
[Transcript missing]

00:54:43.360 --> 00:54:50.280
And as you can see at the bottom left,
okay, for two, three streams,

00:54:50.340 --> 00:54:53.090
you can pretty much predict how
many arrays you're going to need.

00:54:53.180 --> 00:54:55.900
You're going to need
two or -- two arrays,

00:54:55.900 --> 00:54:57.380
maybe three arrays.

00:54:57.520 --> 00:55:02.540
Okay, generally, the array -- the Xserve
RAID actually works closer to

00:55:02.540 --> 00:55:07.160
that middle line than it does,
okay, the high variant line,

00:55:07.310 --> 00:55:10.860
the green line, okay, because it's fast.

00:55:11.000 --> 00:55:15.800
But given the wrong AIO pattern, okay,
you could be out there in the green area.

00:55:15.940 --> 00:55:22.910
Okay, so all we know is that it's fairly
predictable in the low count of streams,

00:55:23.090 --> 00:55:28.090
but can become less predictable
at the high count of streams.

00:55:28.460 --> 00:55:34.510
Incidentally, the black line is a one
array per stream line.

00:55:34.510 --> 00:55:36.740
Okay, so eight streams, eight lines.

00:55:36.740 --> 00:55:42.070
Well, that just so happens to be,
comes out to that 87 megabytes

00:55:42.070 --> 00:55:44.580
a second average bandwidth.

00:55:44.580 --> 00:55:48.770
Okay, so I don't know how they did it,
but if you look at that top, you know,

00:55:48.770 --> 00:55:52.050
remember that right performance
that we saw of 87 megs a

00:55:52.090 --> 00:55:54.000
second no matter what we did?

00:55:54.000 --> 00:55:57.810
Okay, well,
that translates into that black line.

00:55:58.400 --> 00:56:03.030
And a very linear curve as to how
many arrays we're going to need to

00:56:03.030 --> 00:56:07.100
maintain writes in that bandwidth.

00:56:07.100 --> 00:56:11.810
Okay, so if you need eight writers
and they're not doing any reads,

00:56:11.920 --> 00:56:14.860
okay, you can do that on eight arrays.

00:56:15.240 --> 00:56:16.790
At 158 megs a second.

00:56:16.790 --> 00:56:19.640
So, but no environment that
I know is just writes.

00:56:19.640 --> 00:56:22.290
They got to read and they
want to interact between

00:56:22.290 --> 00:56:23.750
the writes and the reads.

00:56:23.760 --> 00:56:25.940
So that's what gives us this variance.

00:56:25.960 --> 00:56:29.000
But what we, again,
what we found from this is that it's

00:56:29.000 --> 00:56:32.720
much more deterministic down at the
lower number of streams than it is

00:56:32.800 --> 00:56:34.620
at the higher number of streams.

00:56:34.620 --> 00:56:37.690
And we're going to use that
for a benefit a little later.

00:56:37.690 --> 00:56:40.270
Okay,
and that's where we started getting into

00:56:40.270 --> 00:56:45.010
how do we take this knowledge and turn
it into a large-scale reference design.

00:56:45.510 --> 00:56:49.140
Well,
we're going to introduce some new toys.

00:56:49.360 --> 00:56:54.840
And this is a new class of
product called a storage network,

00:56:54.840 --> 00:56:58.070
a network storage controller.

00:56:58.180 --> 00:56:59.120
It looks like a switch.

00:56:59.120 --> 00:57:00.050
It smells like a switch.

00:57:00.080 --> 00:57:01.640
But it's not quite a switch.

00:57:01.910 --> 00:57:08.990
It scales from 16 ports all
the way to 128 ports today.

00:57:08.990 --> 00:57:13.110
Okay, these are 2 gig versions.

00:57:13.110 --> 00:57:13.110
Okay.

00:57:13.350 --> 00:57:20.200
And it adds some functionality that helps
us work with media and also enterprise.

00:57:20.200 --> 00:57:22.800
First, it has storage virtualization.

00:57:22.940 --> 00:57:27.480
Okay, it has this ability to do
provisioning on the fly,

00:57:27.480 --> 00:57:30.960
so we can stack a bunch of arrays,
set them up in one way,

00:57:30.960 --> 00:57:34.980
and do the carving up and allocating
from one point of management.

00:57:34.980 --> 00:57:39.700
Okay, it does mirroring,
it does replication, it does snapshot,

00:57:39.730 --> 00:57:42.520
and of course,
that single point of management.

00:57:42.520 --> 00:57:47.360
Okay, in an enterprise,
for you enterprise customers out there,

00:57:47.360 --> 00:57:53.920
it allows you to take tier one storage
and do your replication to XSERVE RAIDs.

00:57:54.040 --> 00:57:56.790
Okay, or do your mirroring
to those XSERVE RAIDs.

00:57:56.840 --> 00:57:59.740
The XSERVE RAIDs are probably
faster than your tier one storage.

00:57:59.740 --> 00:58:02.520
Okay, so it can probably keep up.

00:58:02.540 --> 00:58:04.540
Okay,
it also allows for remote replication.

00:58:04.980 --> 00:58:06.980
So you can go off site with your data.

00:58:06.980 --> 00:58:11.530
Okay, it allows you to do data
mining applications because

00:58:11.530 --> 00:58:15.420
you can snap them off and send
them to another server to do,

00:58:15.420 --> 00:58:21.800
you know, look for particular data, etc.

00:58:23.070 --> 00:58:25.350
Okay, so how are we going to use this?

00:58:25.360 --> 00:58:28.040
How do we benefit from
this in rich media,

00:58:28.050 --> 00:58:28.640
okay?

00:58:28.640 --> 00:58:32.480
Some of these things in enterprise
we don't need for rich media.

00:58:32.480 --> 00:58:34.910
Okay, well,
we're going to show you how to use

00:58:34.910 --> 00:58:36.360
this for increasing performance.

00:58:36.400 --> 00:58:39.220
We're going to show you how
to do workgroup isolation.

00:58:39.280 --> 00:58:44.100
Okay, we're going to show how we can
improve redundancy in the system,

00:58:44.100 --> 00:58:47.830
and again,
we have that single management point.

00:58:47.840 --> 00:58:51.310
Okay, it also has a function
that's not listed here,

00:58:51.790 --> 00:58:57.080
and it has the ability to
allow an administrator to

00:58:57.080 --> 00:58:58.470
go out and zero out a LUN.

00:58:58.500 --> 00:59:05.960
Okay, so for those posthouses
that need to reprovision,

00:59:05.960 --> 00:59:09.180
okay, when a new customer comes
in and zero out the data,

00:59:09.180 --> 00:59:13.580
it can be done on the switch level or
on the network storage control level,

00:59:13.580 --> 00:59:15.830
and you don't have to
dedicate a host to it.

00:59:15.840 --> 00:59:19.390
It has a one-click versus dealing
with a workstation to do that.

00:59:21.170 --> 00:59:25.680
Okay, so this is the design we have.

00:59:26.120 --> 00:59:29.600
Okay, basically it replaced the
brocade in the environment.

00:59:29.600 --> 00:59:31.810
Okay, and we got four arrays.

00:59:31.970 --> 00:59:35.640
We're going to talk about
a new mirroring method,

00:59:35.810 --> 00:59:38.370
okay, and the nomenclature for it.

00:59:38.730 --> 00:59:45.200
is, we call them two by three sets
or one by two sets in this case.

00:59:46.800 --> 00:59:50.820
First number represents how many volumes,
okay,

00:59:50.930 --> 00:59:56.390
and then the second number represents the
number of mirror members that you have.

00:59:56.480 --> 01:00:01.090
Okay, so for the orange set, okay,
we have basically two volumes

01:00:01.190 --> 01:00:03.780
and they're mirrored three ways.

01:00:03.880 --> 01:00:07.020
Okay, for the metadata,
we have one volume

01:00:07.140 --> 01:00:09.350
that's mirrored two ways.

01:00:09.680 --> 01:00:13.250
Okay, from the host perspective,
when we provision those

01:00:13.340 --> 01:00:19.150
out to the clients,
they see two volumes at 2.2 terabytes,

01:00:19.500 --> 01:00:26.300
one volume at 2.2 terabytes for metadata,
or we can shrink that with the switch.

01:00:26.300 --> 01:00:29.110
We can set that size.

01:00:29.620 --> 01:00:33.250
But basically,
it works just like the mirroring

01:00:33.250 --> 01:00:36.160
goes on behind the scenes on
the other side of the switch.

01:00:36.160 --> 01:00:40.510
Okay, when a write goes out, okay,
this is how we're going

01:00:40.510 --> 01:00:41.600
to increase performance.

01:00:41.600 --> 01:00:46.150
When a write goes out,
the switch takes that write and does a

01:00:46.150 --> 01:00:49.600
multicast to all three disk subsystems.

01:00:49.600 --> 01:00:54.190
Okay, so basically,
you're getting a three times multiple

01:00:54.190 --> 01:00:58.320
of that write performance from
the client to your disk subsystem.

01:00:59.080 --> 01:01:01.000
Okay, same applies to the metadata.

01:01:01.000 --> 01:01:02.570
You're getting your mirrored on the fly.

01:01:02.580 --> 01:01:08.680
Okay, on readback, okay,
it's going to take the performance

01:01:08.680 --> 01:01:12.100
from each array and assign it
back to one of its own clients.

01:01:12.220 --> 01:01:15.830
So edit one reads from the top array,
edit two reads from the middle array,

01:01:15.830 --> 01:01:17.910
edit three reads from the third array.

01:01:17.940 --> 01:01:19.220
Okay, well, what's this going to do?

01:01:19.220 --> 01:01:23.960
What this does is it eliminates,
when you're dealing with sequential data,

01:01:23.960 --> 01:01:28.320
for instance, okay,
it eliminates what happens when you

01:01:28.320 --> 01:01:30.800
put multiple streams on the same array.

01:01:30.820 --> 01:01:34.110
Okay, when you put multiple
streams on the same array,

01:01:34.110 --> 01:01:35.660
you get random access.

01:01:35.660 --> 01:01:39.840
Okay, but in this case,
because they're dedicated

01:01:39.840 --> 01:01:44.060
to each edit station,
okay, they can maintain their

01:01:44.600 --> 01:01:48.160
sequential data patterns.

01:01:48.160 --> 01:01:51.580
Okay,
and we all know what that meant from our

01:01:51.790 --> 01:01:54.680
test results in the first part of this.

01:01:54.700 --> 01:01:56.950
Okay, our performance goes way up.

01:01:57.000 --> 01:01:58.240
So not only are we getting
the multiple streams,

01:01:58.240 --> 01:01:58.240
but we're also getting the random access.

01:01:58.240 --> 01:02:02.740
the multiplication,
but are data patterns more predictable?

01:02:04.030 --> 01:02:06.890
Okay, so next we're going to
test this thing with HiDef.

01:02:06.900 --> 01:02:10.540
Okay, so we moved the probes
around a little bit.

01:02:10.580 --> 01:02:14.460
Okay, now we've got two probes up there,
and we're going to look at one.

01:02:14.460 --> 01:02:16.390
We're going to open up,
instead of piping the

01:02:16.460 --> 01:02:18.820
HiDef down one channel,
we're going to pipe it down both

01:02:18.890 --> 01:02:22.320
channels of the host adapter,
but we're only going to monitor one side.

01:02:22.320 --> 01:02:26.900
Okay,
and we're going to write to two arrays.

01:02:26.900 --> 01:02:30.460
One is going to be a
two-by-two configuration,

01:02:30.460 --> 01:02:34.910
two mirror members and
two volumes presented up,

01:02:35.290 --> 01:02:40.690
and XAM file system is going
to strike between the two.

01:02:41.120 --> 01:02:44.520
and the metadata is
the same configuration,

01:02:44.520 --> 01:02:45.060
okay?

01:02:45.360 --> 01:02:48.800
On reads, we're gonna set it up so that,
okay,

01:02:48.800 --> 01:02:53.500
the array 2 reads to that editorial 2,
so this should be no problem.

01:02:53.500 --> 01:02:57.090
We got--we've used Finisar,
we've optimized what the

01:02:57.320 --> 01:03:00.960
file system should look like,
okay, in terms of what its stripe

01:03:00.960 --> 01:03:04.590
breadth is and block allocation,
and we got hi-def running

01:03:04.590 --> 01:03:08.680
reliably on one array,
so this should be fine, okay?

01:03:09.030 --> 01:03:13.600
Next thing we're gonna do is we're
gonna over-commit that array,

01:03:13.890 --> 01:03:16.130
okay, with the second reader, okay?

01:03:16.210 --> 01:03:20.980
And here we should see the thing fail,
and then we're gonna put it into this

01:03:21.400 --> 01:03:26.600
preferred read mode where the data's
coming off of two streams in parallel,

01:03:26.600 --> 01:03:28.600
two arrays in parallel.

01:03:28.870 --> 01:03:30.910
Okay, so...

01:03:31.670 --> 01:03:36.500
From the I/O perspective,
what we see is the first thing

01:03:36.500 --> 01:03:38.360
is your write performance.

01:03:38.400 --> 01:03:41.960
Well, your write performance
is at that 158 level,

01:03:41.960 --> 01:03:45.620
and actually your latency is lower
than what we had on a one-stream.

01:03:45.680 --> 01:03:46.010
Okay?

01:03:46.320 --> 01:03:49.140
The reason why it's lower is because
we're going through two channels.

01:03:49.160 --> 01:03:53.690
When you go through two channels,
it's more stable on XAN or on

01:03:54.160 --> 01:03:59.970
the Apple driver because it
breaks the exchange out in half.

01:04:00.260 --> 01:04:00.800
Okay?

01:04:00.800 --> 01:04:04.720
So it's moving less data but
can respond back quicker.

01:04:04.750 --> 01:04:05.240
Okay.

01:04:05.240 --> 01:04:09.020
So we actually have a lower
latency at this point on writes.

01:04:09.030 --> 01:04:09.340
Okay?

01:04:09.340 --> 01:04:12.930
The second part is where you see that
little bit of it and then you see a

01:04:12.930 --> 01:04:15.110
bunch of green -- I mean a red flat line.

01:04:15.110 --> 01:04:15.540
Okay?

01:04:15.560 --> 01:04:19.030
That is the readback from the
first -- from that second array

01:04:19.080 --> 01:04:20.920
to the second workstation.

01:04:21.040 --> 01:04:21.430
Okay?

01:04:21.430 --> 01:04:22.510
Very constant.

01:04:22.510 --> 01:04:28.070
Again, you don't see any -- strange
things going on with latency.

01:04:28.110 --> 01:04:30.120
Okay?

01:04:30.120 --> 01:04:31.120
Then it hits the second
reader on that disk.

01:04:31.120 --> 01:04:31.990
Okay?

01:04:31.990 --> 01:04:38.900
And this is where the exchange
times go out of whack and everything

01:04:38.920 --> 01:04:43.110
starts falling apart because you've
overcommitted the storage system.

01:04:43.110 --> 01:04:43.880
Okay.

01:04:43.880 --> 01:04:47.460
Well, we can look at FinnoServe
traces and determine what events

01:04:47.520 --> 01:04:50.050
cause the streams to fall off.

01:04:50.050 --> 01:04:50.770
Okay?

01:04:50.770 --> 01:04:53.120
By doing these exercise.

01:04:53.120 --> 01:04:54.120
Okay?

01:04:54.120 --> 01:04:54.120
But in this case,
we overcommitted the storage system.

01:04:54.120 --> 01:04:54.120
Okay?

01:04:54.120 --> 01:04:57.880
But in this case, oversaturated,
and your red one falls off.

01:04:58.040 --> 01:05:00.790
Your purple one came up
and it started playing,

01:05:00.800 --> 01:05:03.340
okay, but eventually it fell off.

01:05:03.500 --> 01:05:09.250
Okay, then on this following slide,
what you see is we reset the storage

01:05:10.140 --> 01:05:13.040
controller into its preferred read mode.

01:05:13.040 --> 01:05:16.490
If you see those little
red diamonds out there,

01:05:16.770 --> 01:05:18.830
Those are where we physically
had to do some stuff,

01:05:18.940 --> 01:05:20.900
so you got a check condition.

01:05:21.020 --> 01:05:25.000
Okay, so Finisaur was really
good about reporting that.

01:05:25.130 --> 01:05:28.130
Okay,
and then we brought up the first reader,

01:05:28.130 --> 01:05:29.800
and then we brought up the second reader.

01:05:29.930 --> 01:05:36.290
Both are now reading in parallel
at high-def information that

01:05:36.290 --> 01:05:40.910
was written once and mirrored
by the storage controller,

01:05:40.910 --> 01:05:40.910
and we're maintaining two streams.

01:05:42.120 --> 01:05:44.740
Okay, we're going to look at this
again a little bit closer

01:05:44.740 --> 01:05:46.580
with the FinnoSort traces.

01:05:46.610 --> 01:05:50.430
Okay,
and what you see here is the same thing,

01:05:50.470 --> 01:05:54.760
except for we're looking
at what is the I/O sizes.

01:05:54.780 --> 01:05:57.780
Okay, and you see your one
megabytes across the board.

01:05:57.930 --> 01:06:02.380
Okay, you got to get your binoculars
out now because you might

01:06:02.380 --> 01:06:04.400
see those little blivets.

01:06:04.430 --> 01:06:08.350
And those little blivets are
the little blivets that we saw

01:06:08.420 --> 01:06:10.520
in that first high def trace.

01:06:10.740 --> 01:06:14.020
Okay,
where every minute there's a 4K read.

01:06:14.220 --> 01:06:16.700
Okay, it's down at the very bottom.

01:06:16.900 --> 01:06:22.110
Okay, you're starting to see
how much performance,

01:06:22.110 --> 01:06:26.760
okay, the XAMP metadata server or
the XAMP file system generally,

01:06:26.780 --> 01:06:30.190
okay, interferes with bandwidth.

01:06:30.190 --> 01:06:30.190
Okay.

01:06:31.080 --> 01:06:33.950
Here's the megabytes per second.

01:06:34.060 --> 01:06:38.940
Okay, we don't even see the blivets,
okay, they're so small in this graph.

01:06:39.370 --> 01:06:42.990
Okay, again, you see,
in this case you see 80 on this

01:06:42.990 --> 01:06:45.860
graph because that's one hand,
because we're only

01:06:45.860 --> 01:06:46.880
looking at one channel.

01:06:46.920 --> 01:06:50.640
But then you see the read, okay,
come back very consistent, okay,

01:06:50.640 --> 01:06:53.730
and then you see the place
where it falls apart,

01:06:53.730 --> 01:06:55.760
and then you see two reads.

01:06:56.600 --> 01:06:59.860
Okay, here,
this graph is the exchange times.

01:06:59.860 --> 01:07:03.700
And what we're going to look,
what we're most interested

01:07:03.700 --> 01:07:06.980
in is did the switch,
or the Morandi box, okay,

01:07:06.980 --> 01:07:09.300
contribute to any more latency?

01:07:09.300 --> 01:07:09.810
Okay.

01:07:09.850 --> 01:07:11.380
And the answer is no.

01:07:11.380 --> 01:07:15.860
Okay, so those reads are coming back
as if they were native reads,

01:07:15.860 --> 01:07:17.870
okay, for their own disk.

01:07:17.930 --> 01:07:18.610
Okay.

01:07:18.610 --> 01:07:24.070
I don't know if you can see it, but,
I mean, we noticed that in the

01:07:24.120 --> 01:07:29.900
megabytes per second,
that those metadata interactions

01:07:29.900 --> 01:07:32.370
were not even visible.

01:07:32.430 --> 01:07:37.740
Okay, we might be able to see it in the
average in the total read exchange.

01:07:37.740 --> 01:07:42.210
But if anybody tells you that
the metadata servers are adding,

01:07:42.590 --> 01:07:46.500
okay, to scalability problems or
any of that kind of stuff,

01:07:46.690 --> 01:07:50.670
okay, they can forget it because you
just don't see it in the traces.

01:07:52.970 --> 01:07:56.400
Okay, here is the maximum read exchange.

01:07:56.400 --> 01:08:01.970
Again, we're maintaining full bandwidth
at the same latency levels,

01:08:01.970 --> 01:08:08.290
okay, as what it was going to one
readback from one machine.

01:08:08.550 --> 01:08:12.860
Okay, so how do you scale high-def
streams using Mirani?

01:08:13.450 --> 01:08:19.470
Okay, in this case,
what we did was if you want four streams,

01:08:19.480 --> 01:08:23.340
then you have sort of four banks, okay,
of two arrays each.

01:08:23.420 --> 01:08:27.760
We've been able to optimize the
file system so that we can reliably

01:08:27.760 --> 01:08:32.700
get two streams read back and even
three streams off of two arrays,

01:08:32.700 --> 01:08:34.420
okay, of high def.

01:08:35.250 --> 01:08:40.800
And so if you do a, it scales linearly,
okay,

01:08:40.800 --> 01:08:46.730
as you put those two arrays together
and add them into a new mirror set.

01:08:46.920 --> 01:08:51.680
Okay, so if you need 12 streams
of one high def channel,

01:08:52.080 --> 01:08:55.060
okay, that would be the first column.

01:08:55.060 --> 01:08:57.210
If you need to have
two high def channels,

01:08:57.250 --> 01:08:59.960
okay, that would be the second column,
okay.

01:08:59.960 --> 01:09:02.750
If you need three and four, okay,
it just keeps scaling.

01:09:02.760 --> 01:09:05.050
Okay, we can scale further than that.

01:09:05.460 --> 01:09:14.000
If we take our two-drive pair and create
a three-drive pair or a four-drive pair.

01:09:14.330 --> 01:09:19.640
So, no matter what the requirements are,
okay, we have a solution set on how

01:09:19.640 --> 01:09:22.490
to scale high def streams.

01:09:22.580 --> 01:09:24.040
Okay, here's a reference implementation.

01:09:24.040 --> 01:09:31.540
Here's a group that they have
editors and they have graphic

01:09:31.580 --> 01:09:34.080
artists and they want to share media,
but they don't want to have

01:09:34.080 --> 01:09:35.290
interaction between the groups.

01:09:35.560 --> 01:09:38.690
Well, you create a, in this case,
we have a four by three.

01:09:38.850 --> 01:09:42.430
One of the mirror groups is dedicated
to the graphic artist department

01:09:42.430 --> 01:09:44.320
and the other two are to video.

01:09:44.320 --> 01:09:46.840
Okay,
we split the metadata so that we have

01:09:46.840 --> 01:09:49.100
a redundant mirror of that data out.

01:09:49.140 --> 01:09:55.070
So, and then the bottom two arrays that
are mirrored allow for the video

01:09:55.070 --> 01:09:58.330
side to see the graphic artist side.

01:10:00.830 --> 01:10:09.280
Okay, so in conclusion, okay,
to scale large-scale SANS, okay,

01:10:09.280 --> 01:10:13.350
you're going to have a real,
or to create a large-scale to begin with,

01:10:13.360 --> 01:10:16.710
you've got to have the customer
needs well-established.

01:10:16.720 --> 01:10:19.980
Okay, what are they trying to
accomplish with the project?

01:10:20.050 --> 01:10:22.760
Okay,
you've got to leverage reference designs.

01:10:24.010 --> 01:10:29.370
Okay, Xserve RAID is a great building
block because it's extremely

01:10:29.370 --> 01:10:31.220
predictable in its performance.

01:10:31.220 --> 01:10:37.440
Much more predictable than your
competitors' disk subsystems out there.

01:10:39.760 --> 01:10:41.840
You've got to know your
products inside and out.

01:10:42.020 --> 01:10:45.730
We do that using FinnoSERC gear.

01:10:46.040 --> 01:10:49.070
By the way,
this is the same gear that banks

01:10:49.070 --> 01:10:54.880
monitor their infrastructures with,
or investment bankers, etc.

01:10:54.940 --> 01:11:01.020
We build a small system to optimize I.O.

01:11:01.020 --> 01:11:04.580
Then we can add the
concurrent users to it.

01:11:04.620 --> 01:11:09.310
Then we scale out to the full deployment,
and then we test it.

01:11:09.600 --> 01:11:14.880
Then we add in the product
proactive management,

01:11:14.880 --> 01:11:19.980
and that way the thing is
supportable for future.

01:11:21.480 --> 01:11:25.140
That's basically the
end of the talk today.

01:11:25.140 --> 01:11:29.300
Okay, this is, we have,
if you're interested in

01:11:29.500 --> 01:11:35.240
getting full Finisar reports,
we have a bundle that has all of

01:11:35.240 --> 01:11:38.820
the traces that we've done and
bundled them up all together.

01:11:38.820 --> 01:11:43.370
And we can email that back to
you if you send us a request.

01:11:43.370 --> 01:11:48.450
And you need to just send an
email to wwdc at sansolutions.com.

01:11:49.340 --> 01:11:52.040
And thank you for coming out today.