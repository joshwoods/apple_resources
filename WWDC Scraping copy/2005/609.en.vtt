WEBVTT

00:00:05.900 --> 00:00:16.300
[Transcript missing]

00:00:17.170 --> 00:00:19.380
We've got a lot to cover.

00:00:19.500 --> 00:00:21.100
It's been an exciting year in HPC.

00:00:21.100 --> 00:00:28.550
Today we're going to cover and touch
on some of the HPC trends and showcase

00:00:28.790 --> 00:00:33.750
the major deployments of some of
Apple's larger HPC deployments.

00:00:34.080 --> 00:00:37.930
We're then going to survey the
HPC technologies on the platform.

00:00:37.930 --> 00:00:40.520
There's a very wide range to choose from.

00:00:40.520 --> 00:00:43.250
We don't have time to talk about each,
but I should point out there

00:00:43.290 --> 00:00:45.300
are representatives from
many of these projects,

00:00:45.300 --> 00:00:46.920
many of these companies
in the room today,

00:00:47.020 --> 00:00:50.120
and thank you for coming and thank
you for your support on the platform.

00:00:51.650 --> 00:00:55.890
We are next going to cover the two
major high performance interconnects

00:00:55.920 --> 00:01:00.260
available on the platform,
MirrorNet and Infiniband.

00:01:00.360 --> 00:01:02.340
And next,
we're going to take a survey of the

00:01:02.340 --> 00:01:06.870
tools available on the platform and get
a very important update from Absoft,

00:01:06.890 --> 00:01:10.800
who are going to let us know about
everything in their portfolio and

00:01:10.800 --> 00:01:13.560
what they're doing on Mac OS X.

00:01:13.750 --> 00:01:15.870
Finally,
we're going to give you a glimpse at

00:01:15.870 --> 00:01:19.280
a very interesting HPC technology,
and then we'll just go ahead

00:01:19.380 --> 00:01:20.930
and wrap up after that.

00:01:24.110 --> 00:01:26.600
So it's pretty clear to
almost anyone that is building

00:01:26.600 --> 00:01:30.310
high performance clusters,
rather high performance computers,

00:01:30.420 --> 00:01:34.970
that clusters are rapidly becoming
the predominant way that we

00:01:34.970 --> 00:01:38.000
build and put together high
performance compute environments.

00:01:38.000 --> 00:01:41.200
This chart represented,
this chart put together at the last

00:01:41.260 --> 00:01:44.410
Supercomputing Conference tracks
the major supercomputer

00:01:44.410 --> 00:01:47.000
architectures going back to 1998.

00:01:47.000 --> 00:01:52.950
So we've got 500 systems
going back from 1998.

00:01:53.120 --> 00:01:54.850
In orange at the bottom are clusters.

00:01:55.010 --> 00:01:59.230
We can see that as they gradually rise,
they push, they have displaced other

00:01:59.330 --> 00:02:00.900
architectures on the platform.

00:02:01.020 --> 00:02:06.630
Clusters, just about everyone knows,
are the predominant way,

00:02:06.630 --> 00:02:11.800
and this on our platform is
personified in the XSERV G5.

00:02:13.910 --> 00:02:18.700
The XSERP G5 is the
foundation for clusters.

00:02:18.700 --> 00:02:21.420
There have been a tremendous
amount of HPC deployments,

00:02:21.440 --> 00:02:25.860
tackling a very broad range of
challenges in high performance

00:02:25.860 --> 00:02:27.850
computing and the sciences.

00:02:28.240 --> 00:02:30.660
Let's check in with some
of the largest ones.

00:02:30.720 --> 00:02:33.270
Probably the most famous one
and well familiar to all of

00:02:33.640 --> 00:02:36.200
you is Virginia Tech System 10.

00:02:36.340 --> 00:02:40.960
System 10 this year converted from
dual 2 GHz Power Mac workstations

00:02:41.340 --> 00:02:44.200
to 2.3 GHz XServe G5s.

00:02:44.200 --> 00:02:49.470
This allowed them to reduce their
footprint by two-thirds and pulled

00:02:49.470 --> 00:02:54.200
a 425 kW load down to 310 kW.

00:02:54.200 --> 00:03:02.470
Major savings in power and
the concomitant heat that has

00:03:02.590 --> 00:03:05.300
to be managed out of that,
again, that problem becomes

00:03:05.300 --> 00:03:06.200
even more manageable.

00:03:06.200 --> 00:03:11.370
This year they generated a 12.25
Teraflop run and qualified as number 7

00:03:11.370 --> 00:03:14.200
on the top 500 list of supercomputers.

00:03:14.200 --> 00:03:17.190
And they remain the
largest academic cluster.

00:03:17.210 --> 00:03:19.590
We think they're going to get
some competition this coming year,

00:03:19.590 --> 00:03:24.200
but are still definitely the largest
and most powerful academic cluster.

00:03:24.470 --> 00:03:26.770
They entered production
this year and they,

00:03:26.770 --> 00:03:29.910
what's very exciting,
as not only an HPC cluster

00:03:30.050 --> 00:03:34.200
for researchers,
are also a technology test bed.

00:03:34.200 --> 00:03:37.140
And in fact they welcome new
technologies to the platform.

00:03:37.190 --> 00:03:41.110
If, say, someone wants to do a
scalability test and see if the

00:03:41.110 --> 00:03:44.560
new technology can come over,
if you contact them,

00:03:44.670 --> 00:03:48.130
they are very willing to
entertain such requests.

00:03:48.190 --> 00:03:50.600
So we're very pleased,
very excited at the work

00:03:50.600 --> 00:03:52.200
that Virginia Tech's doing.

00:03:53.170 --> 00:03:58.080
Another very exciting cluster this year,
the UCLA Dawson Cluster.

00:03:58.080 --> 00:04:00.290
The Dawson Cluster
investigates plasma physics.

00:04:00.290 --> 00:04:06.650
It is a mix of two 128-node systems,
one composed of 2 GHz XServe G5s

00:04:06.650 --> 00:04:10.070
and the other of 2.3 GHz XServe G5s.

00:04:10.100 --> 00:04:12.950
Runs over GigE, leverages LAMMPI.

00:04:13.090 --> 00:04:15.030
This system entered
production very quickly,

00:04:15.120 --> 00:04:20.100
and they were able to generate just
on one of those 128-node sides,

00:04:20.100 --> 00:04:24.030
just shy of a teraflop run,
which actually qualified them last

00:04:24.160 --> 00:04:27.050
year as number 444 in the top 500 list.

00:04:27.100 --> 00:04:31.920
Since then, using Dagger Research Pooch
Pro and MacMPI,

00:04:31.920 --> 00:04:35.100
they generated an even higher run.

00:04:35.140 --> 00:04:38.680
I think this shows, though,
that 128 nodes is within the

00:04:38.680 --> 00:04:40.060
reach of a lot of your devices.

00:04:40.100 --> 00:04:44.180
It's one of the top 500 universities,
and frankly, that can qualify as one of

00:04:44.180 --> 00:04:46.090
the top 500 supercomputers.

00:04:47.580 --> 00:04:51.940
Of course, we have to mention one
of the largest systems,

00:04:52.100 --> 00:04:52.600
COLSA.

00:04:52.910 --> 00:04:56.270
The Mach 5 system in Huntsville,
Alabama investigates

00:04:56.420 --> 00:04:57.800
hypersonic missile research.

00:04:57.950 --> 00:05:01.650
These are incredibly dense
CFD codes that are running on this

00:05:01.650 --> 00:05:03.730
very large collection of systems.

00:05:03.740 --> 00:05:08.160
This system has seen a lot
of development this year,

00:05:08.190 --> 00:05:12.900
and again,
is crunching those very large CFD codes.

00:05:12.900 --> 00:05:15.600
So, fantastic system.

00:05:15.600 --> 00:05:18.960
At University of Illinois,
Urbana-Champaign,

00:05:18.960 --> 00:05:22.000
they stood up the Turing
Cluster this year.

00:05:22.000 --> 00:05:25.600
This is used by their
CSE department and the Center for

00:05:25.600 --> 00:05:28.130
Simulation of Advanced Rockets.

00:05:28.130 --> 00:05:32.870
So, imagine the very complex CFD analysis
that has to be done as you're burning

00:05:32.870 --> 00:05:35.020
away solid motor fuel in a rocket.

00:05:35.020 --> 00:05:38.170
And that's exactly what
they're simulating there,

00:05:38.170 --> 00:05:39.650
among other codes.

00:05:39.660 --> 00:05:43.700
We think this is going to be a very,
very high performing system.

00:05:43.700 --> 00:05:43.700
It's 640 nodes.

00:05:43.700 --> 00:05:43.700
It's a very high performing system.

00:05:43.700 --> 00:05:43.700
It's 640 nodes.

00:05:43.700 --> 00:05:45.830
It's a very high performing system.

00:05:45.830 --> 00:05:47.220
It's 640 nodes.

00:05:47.220 --> 00:05:49.850
It's leveraging the
mirror net interconnect.

00:05:49.980 --> 00:05:53.870
The system was budgeted around 3 million,
and we think it's going to

00:05:53.870 --> 00:05:57.610
place very well on the very
soon to forth come top 500 list.

00:05:57.610 --> 00:06:00.890
So, outstanding work there
at Urbana-Champaign.

00:06:01.550 --> 00:06:05.350
Another fantastic cluster that you
may have heard of that did generate

00:06:05.350 --> 00:06:09.500
some press is the XSEED cluster at
Bowie State University in Maryland.

00:06:09.500 --> 00:06:15.660
So this system again is 224 nodes
leveraging the mirror net interconnect,

00:06:15.660 --> 00:06:19.500
and they generated a 2.1
teraflop benchmark run.

00:06:19.660 --> 00:06:25.340
And so naturally we think they'll make
this next list that comes out shortly.

00:06:25.500 --> 00:06:30.230
It was really fun to watch the progress
of this system because this system

00:06:30.240 --> 00:06:32.500
was student and faculty planned.

00:06:32.500 --> 00:06:36.260
It was an incredibly useful teaching aid,
incredibly useful learning

00:06:36.260 --> 00:06:42.400
aid to help teach HPC and
CSE material for the campus.

00:06:42.510 --> 00:06:45.590
So well done.

00:06:46.770 --> 00:06:50.830
Now for every large system that makes
the list and makes a lot of headlines,

00:06:50.830 --> 00:06:54.700
there are dozens and
dozens of smaller systems.

00:06:54.700 --> 00:06:58.670
And this is probably best personified
in the Apple Workgroup cluster.

00:06:58.700 --> 00:07:03.700
The concept here is that we take it
about as far as you can possibly get

00:07:03.700 --> 00:07:06.620
to true turnkey cluster computing.

00:07:06.820 --> 00:07:10.670
Very well integrated set of applications,
the hardware is integrated,

00:07:10.700 --> 00:07:15.700
all the technology pieces that comprise a
cluster are all very tightly integrated.

00:07:15.700 --> 00:07:20.980
Inexpensive enough and with enough
performance that these types of machines

00:07:20.980 --> 00:07:26.790
are within the reach now for workgroups,
desk side, departments,

00:07:26.900 --> 00:07:29.130
discretionary budgets,
and we've even seen people

00:07:29.130 --> 00:07:32.340
enter it into grants and then
be awarded funds for a grant,

00:07:32.340 --> 00:07:34.700
purchase the cluster and away they go.

00:07:34.700 --> 00:07:37.700
So it really can scale to fit
a very wide variety of needs.

00:07:37.700 --> 00:07:42.700
I should point out it makes a very
nice dedicated x-grid cluster as well.

00:07:47.480 --> 00:07:53.120
So it takes a lot of technology,
a lot of technology to make a cluster.

00:07:53.120 --> 00:07:55.630
All of the technology,
and we've represented them from the

00:07:55.630 --> 00:08:00.400
hardware at the bottom up through the OS,
hardware interconnects, the middleware,

00:08:00.400 --> 00:08:03.210
all the way up to your application,
which is what we want to get at.

00:08:04.110 --> 00:08:06.890
So there's all this technology,
there's a very rich array of technology

00:08:07.020 --> 00:08:10.030
on the platform for you to choose from
when you're building your cluster.

00:08:10.030 --> 00:08:14.910
I should point out that we don't
have time to talk about each of them,

00:08:15.080 --> 00:08:18.290
but we'll call out unique developments
that you should be aware of

00:08:18.290 --> 00:08:19.890
that have happened this year.

00:08:23.050 --> 00:08:25.730
At the hardware layer,
now we do know that it's

00:08:25.730 --> 00:08:27.600
primarily for visualization.

00:08:27.890 --> 00:08:30.400
Many people are using the
Power Mac G5 because it can

00:08:30.400 --> 00:08:32.900
host very fast graphics cards.

00:08:32.900 --> 00:08:37.250
So we do know that the Power Mac G5
remains probably the leading choice

00:08:37.310 --> 00:08:39.700
of people doing visualization in HPC.

00:08:39.900 --> 00:08:43.460
And we do know that people are trying
to build very large collections

00:08:43.460 --> 00:08:45.540
of Mac minis for X-Grid research.

00:08:45.880 --> 00:08:47.530
However,

00:08:47.820 --> 00:08:49.760
That would be quite fun to play with.

00:08:49.950 --> 00:08:54.700
But the XServe G5 remains the primary
building block of building clusters.

00:08:54.700 --> 00:09:01.930
The 64-bit computing,
the massive amount of memory available,

00:09:01.930 --> 00:09:05.810
amazing amount of performance power,
all told,

00:09:05.910 --> 00:09:10.700
make it an extremely well-balanced,
basic building block of a cluster,

00:09:10.700 --> 00:09:14.990
with fantastic I/O throughput,
very good environmentals,

00:09:15.080 --> 00:09:17.700
and excellent processing power.

00:09:17.700 --> 00:09:22.750
Major developments here are that we now
support up to 2GB DIMMs on the XServe G5,

00:09:22.800 --> 00:09:28.700
and if your wallet can stand it,
and you can afford to max out 2GB chips,

00:09:28.700 --> 00:09:33.900
you can fit up to 16GB of
ECC memory on the Power Mac G5.

00:09:37.310 --> 00:09:40.000
So at the OS level,
Tiger landed a tremendous amount

00:09:40.000 --> 00:09:44.400
of benefit of immediate interest
and use for HPC customers.

00:09:44.400 --> 00:09:49.700
Of course, 64-bit memory address space,
being able to run and build 64-bit

00:09:49.700 --> 00:09:57.040
applications using Xcode 2 and GCC4 with
the GCC4 underpinnings is of immediate

00:09:57.040 --> 00:09:59.560
use and benefit to our HPC customers.

00:10:00.900 --> 00:10:07.300
For those of you that have had many trips
to SysControl to tune kernel parameters,

00:10:07.300 --> 00:10:11.560
we want to guide you to
have a look at LaunchD,

00:10:11.560 --> 00:10:16.900
which is a way to set global
system-wide kernel parameters.

00:10:16.960 --> 00:10:20.390
ManLaunchD gives you a
wealth of information,

00:10:20.670 --> 00:10:26.010
and I'd like to point out that at the
large system administration session,

00:10:26.010 --> 00:10:29.250
Josh Durham of Virginia Tech is
going to take you through

00:10:29.320 --> 00:10:30.880
exactly how he leverages LaunchD.

00:10:30.880 --> 00:10:36.710
ManLaunchD and Tiger, to do exactly that,
tune his systems for HPC environments.

00:10:37.010 --> 00:10:39.460
One of the things you may have
heard at Virginia Tech was

00:10:39.550 --> 00:10:43.900
that for very large systems,
if you want to eke out the very last

00:10:44.450 --> 00:10:50.070
mile of performance on your HPC codes
across a very large number of systems,

00:10:50.070 --> 00:10:54.890
you would like to have physically
contiguous memory to be available

00:10:54.970 --> 00:10:57.200
to those HPC applications.

00:10:57.200 --> 00:11:02.830
I am very pleased to tell you that
Apple's DTS department has written code

00:11:02.830 --> 00:11:06.050
on Tiger that does exactly that for you.

00:11:06.180 --> 00:11:11.700
So if you are an HPC customer and
you know exactly what you're doing,

00:11:11.700 --> 00:11:14.360
this code I think will be
very instrumental in helping

00:11:14.370 --> 00:11:18.340
show you how to get even more
performance out of your HPC codes.

00:11:18.340 --> 00:11:21.530
And if you would like to see that,
I recommend you have a look at

00:11:21.580 --> 00:11:24.700
Friday's session on debugging
parallel applications where

00:11:24.700 --> 00:11:26.280
that will be introduced.

00:11:27.070 --> 00:11:29.820
that would be introduced in that session.

00:11:34.530 --> 00:11:39.490
So moving up through the
hardware interconnects,

00:11:39.490 --> 00:11:42.320
we have a broad range of
choices in the gigabit Ethernet.

00:11:42.320 --> 00:11:47.680
A company by the name of Smalltree has
actually taken a very early lead in

00:11:47.680 --> 00:11:53.290
writing Mac OS X drivers for gigabit
Ethernet cards from a number of vendors.

00:11:53.300 --> 00:11:57.210
So they in fact offer one, two,
four and even six port

00:11:57.310 --> 00:12:03.340
copper gig E cards,
and then that combined with 802.3ad runs

00:12:03.340 --> 00:12:06.060
very nicely in a server environment.

00:12:06.060 --> 00:12:10.560
They also offer one and two port
optical gigabit Ethernet cards.

00:12:10.560 --> 00:12:15.370
So if you already have an
optical infrastructure and

00:12:15.370 --> 00:12:18.300
want to do very long runs,
this is an excellent choice.

00:12:18.390 --> 00:12:20.800
Surprisingly,
another reason that some customers have

00:12:20.800 --> 00:12:26.390
reported wanting these cards is because
in very highly secure environments,

00:12:26.390 --> 00:12:31.400
they know that you can't basically
listen in on an optical line.

00:12:31.400 --> 00:12:34.250
So movie studios, for example,
they're not going might be of interest,

00:12:34.250 --> 00:12:39.740
certain three-letter agencies that don't
want people knowing what's passing down

00:12:39.740 --> 00:12:42.240
those wires might be a good choice.

00:12:42.240 --> 00:12:45.880
You might like it just for the
simple fact that you already

00:12:45.990 --> 00:12:48.880
have fiber optic plumbing.

00:12:48.900 --> 00:12:51.700
Also available,
SmallTree wrote a driver for

00:12:51.780 --> 00:12:55.690
Intel's 10 gigabit Ethernet card,
so if you need 10 gigabit Ethernet

00:12:55.730 --> 00:12:59.720
and you want to move very large files,
possibly from one work group to another,

00:12:59.720 --> 00:13:04.510
this again has been a very popular
of choice with our customers.

00:13:05.400 --> 00:13:08.300
So moving on to high
performance interconnects.

00:13:08.370 --> 00:13:12.300
Miranet, you know, have been a long time
developer on the platform.

00:13:12.370 --> 00:13:19.050
This is an example of their connectors
and the types of switches they have.

00:13:19.270 --> 00:13:23.200
So news from them is that they
expect two new XSERV-based clusters

00:13:23.200 --> 00:13:28.090
will appear on the June 2005 list
of the top 500 supercomputers.

00:13:28.110 --> 00:13:31.060
So stay tuned for news and
more product news from them.

00:13:31.600 --> 00:13:39.930
They've had a lot of success with
their MX message passing framework.

00:13:39.990 --> 00:13:45.440
The MX2G has been released for
the PCIX network interface cards.

00:13:45.440 --> 00:13:52.170
This is an even lower latency than the
previous generation of cards from them.

00:13:52.180 --> 00:13:55.940
They are getting on the order
of 3.6 microseconds signal

00:13:56.270 --> 00:14:01.150
latencies over raw hardware,
and only 3.8 microseconds using MPitch.

00:14:01.400 --> 00:14:04.360
The MX2G is a great example of
a message-passing framework.

00:14:04.360 --> 00:14:07.040
One of the really neat things
about these cards is that you

00:14:07.050 --> 00:14:09.600
install them in your system,
and it shows up just like a

00:14:09.600 --> 00:14:11.770
configurable network interface card.

00:14:11.780 --> 00:14:14.900
You're able to pass your
TCP/IP traffic over that.

00:14:15.190 --> 00:14:17.890
They do offer, again,
their own middleware,

00:14:17.980 --> 00:14:22.620
and are working on the next generation
MPitch2 MX to be coming soon.

00:14:23.220 --> 00:14:26.200
So surprisingly,
they report that Tiger allowed them,

00:14:26.200 --> 00:14:30.600
not surprisingly, but very welcome,
they report that Tiger allowed them

00:14:30.600 --> 00:14:31.980
to get even better performance.

00:14:32.540 --> 00:14:37.100
MX uses a unique approach
to memory management.

00:14:37.100 --> 00:14:39.460
So for messages smaller
than 32 kilobytes,

00:14:39.460 --> 00:14:41.850
they simply copy it
across from node to node.

00:14:41.850 --> 00:14:46.480
Larger messages than that get
pinned and unpinned on demand,

00:14:46.480 --> 00:14:53.080
and they say that Tiger's
code made them more efficient.

00:14:53.110 --> 00:14:56.710
And that practical on Mac OS X and
allowed them to deliver as much as four

00:14:56.830 --> 00:15:01.110
times the memory to the application.

00:15:01.240 --> 00:15:05.200
and Kevin They also announced
experimental support for

00:15:05.200 --> 00:15:08.090
64-bit binaries on Tiger.

00:15:08.100 --> 00:15:13.310
And again, we will stay tuned for news
from them at the Top 500 at the

00:15:13.320 --> 00:15:16.100
Supercomputing Conference next month.

00:15:17.780 --> 00:15:19.970
So the other high
performance interconnect on

00:15:19.970 --> 00:15:23.230
the platform is InfiniBand,
and we are very pleased to welcome

00:15:23.300 --> 00:15:28.300
to the platform the combination
of SilverStorm Technologies and

00:15:28.300 --> 00:15:32.550
SmallTree Communications,
who have partnered to deliver a

00:15:32.900 --> 00:15:36.700
tremendous amount of InfiniBand
options on our platform.

00:15:36.700 --> 00:15:39.370
So, who are SilverStorm?

00:15:39.760 --> 00:15:42.910
Well, the first thing to tell you about
them is that you probably already

00:15:43.030 --> 00:15:44.700
know them as Infinicon Systems.

00:15:44.760 --> 00:15:48.250
They were founded in June of 2000,
and again,

00:15:48.250 --> 00:15:50.700
originally as Infinicon Systems.

00:15:50.700 --> 00:15:54.540
They have sales and support teams
to answer technical questions or to

00:15:54.540 --> 00:15:57.560
field customer requests in the US,
Europe and Asia.

00:15:57.700 --> 00:16:02.500
They have a very strong emphasis in
high performance technical computing,

00:16:02.540 --> 00:16:07.700
or commercial computing field,
so areas like oil and gas.

00:16:07.700 --> 00:16:11.350
They offer a complete
solution set for both HPC and

00:16:11.350 --> 00:16:14.700
enterprise compute customers.

00:16:14.700 --> 00:16:20.700
And we'll explain in a little bit exactly
what that could mean to your data center.

00:16:21.300 --> 00:16:25.470
One of the things that could mean
is that through their solutions you

00:16:25.520 --> 00:16:30.340
can pull together fiber channel,
Ethernet, and InfiniBand architectures.

00:16:30.340 --> 00:16:34.250
And again, we'll show you what that
means in a few minutes.

00:16:36.060 --> 00:16:37.290
So who are Smalltree?

00:16:37.380 --> 00:16:40.920
So we've already mentioned them once
for their gigabit Ethernet cards.

00:16:40.920 --> 00:16:43.920
Smalltree are a hardware and
software development company.

00:16:43.920 --> 00:16:46.890
The principles of the company
come from companies like

00:16:46.890 --> 00:16:48.500
Cray and SGI and Sun and HP.

00:16:48.500 --> 00:16:52.130
They have technology
partnerships in place with Intel,

00:16:52.490 --> 00:16:56.470
Silicon and Silverstorm, obviously,
and also have experience

00:16:56.470 --> 00:16:58.270
as system integrators.

00:16:58.270 --> 00:17:04.190
They have a wide range of offerings that
we've seen from GIGI up to InfiniBand.

00:17:04.220 --> 00:17:08.490
And probably the best news here is
that they are focused exclusively

00:17:08.490 --> 00:17:10.220
on the Mac OS X platform.

00:17:10.220 --> 00:17:19.960
So they've taken a very nice lead in
building network performance on Mac OS X.

00:17:20.390 --> 00:17:23.210
So we may have heard a lot about
InfiniBand and exactly what it means.

00:17:23.310 --> 00:17:27.500
I can tell you InfiniBand is a very
dense spec with a lot of protocols.

00:17:27.680 --> 00:17:32.570
But the vision for InfiniBand
was really building an enterprise

00:17:32.570 --> 00:17:37.380
class infrastructure that really
started with a clean sheet of paper.

00:17:37.380 --> 00:17:42.180
What if we didn't have to sort of pull
along other networking technologies?

00:17:42.240 --> 00:17:48.450
What if we could start with a clean slate
and try to bridge together fiber channel?

00:17:48.460 --> 00:17:53.190
This is all made possible
through a very fast,

00:17:53.510 --> 00:17:55.500
high bandwidth, low latency interconnect.

00:17:55.500 --> 00:17:59.690
And this was originally what
Infinicon was founded on.

00:17:59.690 --> 00:18:04.180
So in the center in orange would be
one of their switches that is able

00:18:04.180 --> 00:18:06.660
to tie together gigabit Ethernet.

00:18:07.380 --> 00:18:11.350
So this is a very simple way to bridge
off to fiber channel storage and bridge

00:18:11.400 --> 00:18:15.160
off to high performance applications,
be it for commercial computing

00:18:15.160 --> 00:18:16.810
or technical computing.

00:18:20.270 --> 00:18:22.620
So they do offer a complete
range of solutions,

00:18:22.680 --> 00:18:26.700
starting of course at the very bottom
with the host channel adapters.

00:18:26.710 --> 00:18:31.020
This is an example of their
PCIX host channel adapter.

00:18:31.050 --> 00:18:35.440
Switches like this that are able
to bridge the I/O infrastructure.

00:18:35.450 --> 00:18:37.890
It might be a bit hard
to see in the picture,

00:18:37.890 --> 00:18:41.250
but on the left is a modular
chassis that allows you to plug

00:18:41.250 --> 00:18:45.880
in gigabit Ethernet in the center,
up to 12 ports of InfiniBand,

00:18:45.880 --> 00:18:49.310
and on the right,
two ports of fiber channel.

00:18:51.310 --> 00:18:55.470
They of course have a range of
switches ranging from the 1U switches

00:18:55.480 --> 00:18:59.470
that make possible very flexible
and very efficient topologies for

00:18:59.480 --> 00:19:04.460
designing supercomputer layouts,
and also great big monsters that

00:19:04.460 --> 00:19:07.000
will go up to 288 ports as well.

00:19:07.830 --> 00:19:11.790
And then to tie all of that together,
they have arranged, so in combination of,

00:19:12.160 --> 00:19:15.580
by working with Smalltree
and Silverstorm,

00:19:15.580 --> 00:19:19.420
they've pulled together a fantastic
range of software from what

00:19:19.420 --> 00:19:22.780
sits directly on top of the HCA,
all the way up to how

00:19:22.780 --> 00:19:25.830
you manage the fabric,
and how you enforce things

00:19:25.930 --> 00:19:27.810
like quality of service.

00:19:27.810 --> 00:19:32.660
So, a complete end-to-end solution.

00:19:35.350 --> 00:19:38.200
So what's here now inside
that InfiniBand stack?

00:19:38.200 --> 00:19:41.230
Today, as you would expect, MPI is here.

00:19:41.370 --> 00:19:45.300
MPI codes run on SilverStorm InfiniBand.

00:19:45.390 --> 00:19:51.300
They have also already landed
and fully support VirtualNIC,

00:19:51.300 --> 00:19:57.480
so iNIC, and IP over IB,
meaning that you can pass TCP/IP,

00:19:57.480 --> 00:20:02.300
any TCP/IP application can
simply run over InfiniBand.

00:20:02.560 --> 00:20:06.300
They have also landed SRP,
Storage Resource Protocol,

00:20:06.300 --> 00:20:10.670
that allows things like Fiber
Channel to simply plug in using one

00:20:10.670 --> 00:20:14.490
of the bridging switches we saw,
and allows InfiniBand clients to reach

00:20:14.780 --> 00:20:17.260
out and simply see Fiber Channel storage.

00:20:17.330 --> 00:20:22.300
They're faithfully passing through
the Fiber Channel and SCSI protocols.

00:20:22.300 --> 00:20:26.390
So one of the things having that
capability unlocks is that we can

00:20:26.470 --> 00:20:31.460
have applications like Oracle 10G Rack
sitting up in user land that is

00:20:31.670 --> 00:20:32.300
actually running on the Mac OS X.

00:20:32.300 --> 00:20:32.300
So that's a great opportunity to have.

00:20:32.300 --> 00:20:35.000
So we're going to be reaching
out and seeing storage,

00:20:35.000 --> 00:20:38.480
Fiber Channel storage,
but it's all happening over InfiniBand,

00:20:38.520 --> 00:20:42.300
and TCP/IP also, same wire,
going over InfiniBand.

00:20:42.300 --> 00:20:48.550
So we think it's going to unlock some
very interesting ways for enterprise

00:20:48.580 --> 00:20:52.290
and HPC centers to leverage InfiniBand.

00:20:55.230 --> 00:20:59.350
So we're actually demonstrating
exactly what I described down

00:20:59.350 --> 00:21:02.300
in the data center downstairs.

00:21:02.300 --> 00:21:04.920
In one of the racks in the
data center on the first floor,

00:21:04.920 --> 00:21:12.820
you will find four XServe cluster nodes
that have Oracle 10G Rack installed.

00:21:12.820 --> 00:21:16.240
Oracle 10G Rack is leveraging
storage on an XServe RAID,

00:21:16.370 --> 00:21:19.510
and again,
simply by plugging into one of the

00:21:19.510 --> 00:21:22.080
InfiniO switches in the bottom.

00:21:22.080 --> 00:21:26.450
So while we could plug in fiber channel
off to larger fiber channel switches,

00:21:26.540 --> 00:21:29.920
in this case we're simply
plugging directly the two ports

00:21:29.920 --> 00:21:33.940
of fiber channel from the XServe
RAID into the InfiniO switch.

00:21:34.310 --> 00:21:43.220
That is then simply presented and
available to InfiniBand clients.

00:21:43.530 --> 00:21:45.570
And then of course we had
extra ports on the switch,

00:21:45.570 --> 00:21:50.170
so we added on up to six
more ports to run HPC codes.

00:21:50.170 --> 00:21:53.400
So, not too bad for one one-use switch.

00:21:53.580 --> 00:21:55.680
Well done.

00:21:57.450 --> 00:22:02.850
What's also exciting and what SilverStorm
bring to the table in working with

00:22:02.930 --> 00:22:07.400
Apple is that because of a lot of their
existing emphasis on commercial codes,

00:22:07.400 --> 00:22:12.130
they have partnerships in place
with a fantastic portfolio

00:22:12.260 --> 00:22:17.390
of companies to work with,
including Fluent, Ansys, and Abacus,

00:22:17.390 --> 00:22:21.040
and they have committed to help
drive these applications to Mac OS X,

00:22:21.040 --> 00:22:25.210
and naturally we'll be looking for
customers that are ready to do that.

00:22:25.250 --> 00:22:27.100
We're ready to help them.

00:22:27.300 --> 00:22:29.490
Um...

00:22:32.490 --> 00:22:36.600
In the IPC middleware arena,
moving slightly up the stack,

00:22:36.600 --> 00:22:40.640
we have a very broad range,
very large toolkit to choose from.

00:22:40.640 --> 00:22:44.810
Each of the hardware vendors, of course,
have a version of MPI that is

00:22:44.810 --> 00:22:47.440
tightly integrated to their hardware.

00:22:47.440 --> 00:22:53.440
But other major news is OpenMPI,
this is the next generation MPI from

00:22:53.440 --> 00:22:57.010
the people who brought you LAMMPI,
has announced it is on the

00:22:57.010 --> 00:23:01.440
platform and have even said that
they're working on XGrid support.

00:23:01.440 --> 00:23:03.380
So thank you for that.

00:23:03.440 --> 00:23:10.640
We have mVapage, the mVapage team at
Ohio State have conducted a very,

00:23:10.640 --> 00:23:14.440
very fast run, in fact,
might even be the fastest

00:23:14.440 --> 00:23:19.510
networking speed recorded on a Mac,
in excess of 900 megabytes per second

00:23:19.510 --> 00:23:23.430
and latencies under six microseconds.

00:23:23.600 --> 00:23:25.440
So a lot of fantastic work.

00:23:25.440 --> 00:23:28.390
We have a lot of great work
going on at the mVapage program,

00:23:28.480 --> 00:23:28.890
Dr.

00:23:28.890 --> 00:23:31.400
DK Panda and his team at Ohio State.

00:23:36.960 --> 00:23:40.320
In the area of code libraries,
probably the major news here

00:23:40.630 --> 00:23:47.100
is that if you do anything,
if you use BLAS levels 1, 2 or 3, LAPAC,

00:23:47.150 --> 00:23:49.420
if you do any type of
digital signal processing,

00:23:49.420 --> 00:23:52.250
if you do fast Fourier transforms,
you should really

00:23:52.250 --> 00:23:53.810
leverage Dash Accelerate.

00:23:53.860 --> 00:24:09.660
So the Accelerate framework delivered
in Tiger is absolutely your friend.

00:24:09.660 --> 00:24:09.660
A little bit later,
Rodney Mock of Absoft will

00:24:09.660 --> 00:24:09.660
take you through and talk about
some of the IMSL math libraries

00:24:09.660 --> 00:24:09.660
that Absoft will deliver.

00:24:13.730 --> 00:24:17.790
So in the area of file systems,
the state of the art

00:24:17.860 --> 00:24:24.810
kernel in Tiger has landed,
has yielded even better performance

00:24:24.810 --> 00:24:27.670
for NFS and AFP on the platform.

00:24:27.740 --> 00:24:30.740
XSAN has been a very compelling choice.

00:24:30.860 --> 00:24:33.950
It is being tested and used in
a number of HPC installations.

00:24:34.120 --> 00:24:38.400
A common way to do this would be to
make each head node on top of a rack of

00:24:38.400 --> 00:24:43.450
other compute nodes the client on XSAN,
and now they can all now see and

00:24:43.450 --> 00:24:46.400
pass along and share shared storage.

00:24:46.400 --> 00:24:52.520
Lustre, on the heels of announcing that
they had worked on Panther,

00:24:52.520 --> 00:24:57.710
have done the same,
and are now in beta stage on Tiger,

00:24:57.710 --> 00:25:03.970
and are, if large scale file systems
like Lustre are of interest,

00:25:03.970 --> 00:25:06.650
I encourage you to contact them.

00:25:06.660 --> 00:25:10.490
They are looking forward
to working on Tiger.

00:25:10.880 --> 00:25:12.300
Lastly, I'll mention Blaze.

00:25:12.450 --> 00:25:18.080
Blaze is a network file system
offering from the guys at Smalltree.

00:25:18.080 --> 00:25:23.730
It is their goal to make network
file transfers very fast,

00:25:23.750 --> 00:25:24.980
and I encourage you to check that out.

00:25:24.980 --> 00:25:27.750
That was just announced this Monday.

00:25:31.390 --> 00:25:34.840
In the area of resource management,
I have to absolutely encourage

00:25:34.840 --> 00:25:36.150
you to check out Xgrid.

00:25:36.220 --> 00:25:39.300
People are using Xgrid in
some very interesting ways.

00:25:39.300 --> 00:25:41.240
It already comes from Apple.

00:25:41.420 --> 00:25:44.510
It's built in a Mac OS X server,
so leverage that if you can.

00:25:44.520 --> 00:25:49.070
Moab, the commercial,
it can be grossly oversimplified to think

00:25:49.070 --> 00:25:51.920
of it as the commercial version of Maui.

00:25:51.920 --> 00:25:54.040
Moab was released this year.

00:25:54.040 --> 00:25:58.970
They substantially re-architected
what happens under the hood.

00:26:01.000 --> 00:26:06.940
Very recently, the team at Oscar HA at
Louisiana State have said that they,

00:26:06.940 --> 00:26:12.120
in conjunction with the OpenMPI team,
are now going to tackle

00:26:12.230 --> 00:26:14.260
Oscar HA on Mac OS X.

00:26:14.260 --> 00:26:16.530
So expect news on that.

00:26:20.760 --> 00:26:22.380
Okay.

00:26:22.430 --> 00:26:27.310
In the tool space,
obviously Tiger delivered a fantastic

00:26:27.310 --> 00:26:30.740
version of Xcode with GCC and GDB 4.0.

00:26:30.740 --> 00:26:38.860
Some very important news is this year,
Etnis have released their Totalview

00:26:38.860 --> 00:26:41.740
parallel debugger on the platform.

00:26:41.740 --> 00:26:44.380
And in fact,
they accompanied us on a road show

00:26:44.470 --> 00:26:48.420
where we went to several major cities,
and people got a chance to

00:26:48.420 --> 00:26:50.730
see Etnis Totalview in action.

00:26:50.740 --> 00:26:56.110
Absoft have a very nice
portfolio of tools,

00:26:56.520 --> 00:27:02.740
and what I would like to do is
go ahead and invite Rodney Mock,

00:27:02.870 --> 00:27:07.200
who is the Absoft HPC Technical Director,
to come up and tell us more about

00:27:07.200 --> 00:27:09.550
what they have coming up next.

00:27:09.940 --> 00:27:10.740
Rodney.

00:27:10.740 --> 00:27:14.900
Rodney Mock: Thank you.

00:27:14.900 --> 00:27:14.900
Nice, John.

00:27:17.400 --> 00:27:18.400
Thanks Skip.

00:27:18.400 --> 00:27:21.740
I'm Rod Mock,
I'm Absoft HPC Technical Director.

00:27:21.740 --> 00:27:26.380
So if you haven't heard about Absoft,
we've had continuous tool support

00:27:26.500 --> 00:27:29.400
on the Apple platform since 1984.

00:27:29.420 --> 00:27:31.700
We've been there through
the various transitions,

00:27:31.700 --> 00:27:33.680
including the transition to PowerPC.

00:27:33.760 --> 00:27:36.570
We're going to be there for
the transition on Intel,

00:27:36.570 --> 00:27:43.390
so you'll be seeing tools from Absoft
for the Mac on Intel coming forth.

00:27:43.530 --> 00:27:45.880
And we've also done the
first commercial Fortran,

00:27:45.880 --> 00:27:47.380
Fortran 95 compilers.

00:27:47.400 --> 00:27:51.240
For the Mac platform,
we'll have a full Fortran 2003 compiler

00:27:51.740 --> 00:27:55.600
also available on the Mac platform
and intend to be the first there.

00:27:55.600 --> 00:27:59.420
And we are going to be continuing
development of cutting edge developer and

00:27:59.420 --> 00:28:01.600
high performance tools for OS X Tiger.

00:28:04.170 --> 00:28:07.570
Some of the Absoft solutions that
we've done on the platform are

00:28:07.670 --> 00:28:10.840
our High Performance Computing
Software Development Kit for

00:28:10.900 --> 00:28:12.500
XServe clusters.

00:28:12.500 --> 00:28:16.240
This is a kit that will bundle
together the best of breed solutions

00:28:16.240 --> 00:28:20.110
available for the platform,
make it all easy to

00:28:20.110 --> 00:28:22.340
install in less than five,
six seconds.

00:28:22.340 --> 00:28:26.700
You'll be able to install this kit,
have all the compilers, debuggers,

00:28:26.700 --> 00:28:32.300
math libraries and other tools that you
need to do to optimize on the platform.

00:28:32.300 --> 00:28:36.100
And we'll talk about each of these
in detail here in a few minutes.

00:28:36.100 --> 00:28:40.990
Another thing that we've announced at
this conference is our FXP MPI Parallel

00:28:40.990 --> 00:28:44.460
Debugger for OS X Tiger and Panther.

00:28:44.460 --> 00:28:47.590
You'll be able to debug basic
MPI codes with this debugger.

00:28:47.710 --> 00:28:52.260
It's a low-cost entry-level
debugger from Absoft.

00:28:52.260 --> 00:28:54.840
You can also get Total View,
which Skip talked about.

00:28:54.840 --> 00:28:59.060
If you need to move up to the next level,
do hybrid debugging,

00:28:59.060 --> 00:29:03.040
more advanced type codes,
that solution is also

00:29:03.040 --> 00:29:04.960
available from Absoft.

00:29:05.350 --> 00:29:08.590
64-bit and 32-bit Pro Fortran
compiled for Tiger.

00:29:08.600 --> 00:29:12.720
Right on the heels of Tiger,
we released our 64-bit Fortran compiler,

00:29:12.720 --> 00:29:14.280
as well as our other tools.

00:29:14.290 --> 00:29:16.800
This is available right now.

00:29:16.800 --> 00:29:19.700
You can download it if you need
to do 64-bit development on Tiger.

00:29:19.760 --> 00:29:23.300
You can use our tools and
immediately take advantage

00:29:23.300 --> 00:29:25.800
of the 64-bit capabilities.

00:29:26.100 --> 00:29:30.490
We're also announcing the 64-bit and
32-bit IMSL Fortran numerical libraries,

00:29:30.490 --> 00:29:34.980
version 5.0,
with serial and MPI support for XServe.

00:29:35.050 --> 00:29:39.110
If you're currently using
IMSL on the Mac platform,

00:29:39.110 --> 00:29:45.000
want to take advantage of your cluster,
it makes it very easy to use with MPI.

00:29:45.000 --> 00:29:49.600
It hides a lot of the MPI complexity
away from the user and lets you take

00:29:49.700 --> 00:29:52.810
advantage of the resources that you have.

00:29:55.000 --> 00:31:05.500
[Transcript missing]

00:31:06.200 --> 00:31:08.270
Some other best of breed
components that are included,

00:31:08.270 --> 00:31:11.530
we have MPish2, LAMMPI,
we'll also be supporting

00:31:11.530 --> 00:31:14.410
OpenMPI and other various
MPIs as they come out in the kit.

00:31:14.560 --> 00:31:17.710
We have the 64-bit and
32-bit IMSL libraries,

00:31:17.710 --> 00:31:20.940
as well as Absoft Pro4chan
we discussed earlier.

00:31:20.940 --> 00:31:23.470
We also have available VAST.

00:31:23.540 --> 00:31:25.420
VAST has two technologies.

00:31:25.420 --> 00:31:28.060
One is VAST AlteVec,
which is bundled on the CD.

00:31:28.060 --> 00:31:30.720
This lets you do
auto-vectorization for your codes.

00:31:31.380 --> 00:31:33.280
If you have a code,
you want to be able to take

00:31:33.280 --> 00:31:35.750
advantage of the AlteVec unit,
but you're not familiar

00:31:35.750 --> 00:31:38.570
with how to do it,
you can just run it through VAST.

00:31:38.730 --> 00:31:41.720
It'll automatically
auto-vectorize your code for you.

00:31:41.870 --> 00:31:45.070
It gives you good diagnostics
to tell you why certain parts of

00:31:45.190 --> 00:31:47.410
your code couldn't be vectorized.

00:31:47.470 --> 00:31:51.030
For example, if you have possible pointer
early seeing or other reasons,

00:31:51.030 --> 00:31:53.870
and helps you kind of work your
way through to get even higher

00:31:53.870 --> 00:31:55.510
performance out of your code.

00:31:55.520 --> 00:32:00.380
It also has another tool that's automatic
parallelization and OpenMP support.

00:32:00.820 --> 00:32:03.760
It'll go through your code,
automatically parallelizes

00:32:03.760 --> 00:32:06.720
the parts that it feels that
you can gain performance on.

00:32:06.720 --> 00:32:09.750
If you want to use
handwritten OpenMP directives,

00:32:09.750 --> 00:32:13.490
you can also compile your code
using VAST to support OpenMP.

00:32:16.210 --> 00:32:21.050
The next product that we announced
is our FXP MPI Parallel Debugger.

00:32:21.120 --> 00:32:24.580
It's built on our FX2 serial
debugger that supports all the major

00:32:24.660 --> 00:32:26.300
compiler vendors on all platforms.

00:32:26.300 --> 00:32:29.550
It also supports all the
major MPI implementations all

00:32:29.550 --> 00:32:31.250
in one easy to use package.

00:32:31.340 --> 00:32:34.280
It is very easy to use,
it's very sharp looking

00:32:34.280 --> 00:32:36.240
based on the Aqua interface.

00:32:36.380 --> 00:32:38.410
If you want to see a sneak
preview of this debugger,

00:32:38.810 --> 00:32:41.800
please come on Friday to the
parallel debugging session.

00:32:41.800 --> 00:32:45.110
We'll be able to show it to you,
and you can enter in our beta program

00:32:45.180 --> 00:32:47.770
to help us beta this this month.

00:32:48.190 --> 00:32:51.140
If you need a higher level debugger,
like I mentioned before,

00:32:51.210 --> 00:32:53.900
TotalView is available,
and we can make that,

00:32:53.940 --> 00:32:56.060
that's available through
AppSoft and supported as well.

00:32:58.220 --> 00:33:00.100
It has some unique features.

00:33:00.100 --> 00:33:02.500
One is automatically
attached to MPI processes.

00:33:02.500 --> 00:33:05.470
This is very popular,
so you don't have to figure out what

00:33:05.470 --> 00:33:07.550
your PIDs are on different nodes.

00:33:07.550 --> 00:33:11.310
It will automatically attach to
whatever MPI processes are out there,

00:33:11.310 --> 00:33:14.540
present it to one easy-to-use interface,
so you'll be able to

00:33:14.540 --> 00:33:17.190
step through your code,
run on various, you know,

00:33:17.380 --> 00:33:20.910
view the registers, variables,
stack on all the different nodes,

00:33:20.910 --> 00:33:22.980
all through the same interface.

00:33:23.300 --> 00:33:27.740
It also has a visual element that lets
you see what the state of your nodes are.

00:33:27.740 --> 00:33:31.280
It has like a color-coded scheme,
so you can see which nodes are

00:33:31.280 --> 00:33:33.240
dead or possibly deadlocked.

00:33:33.430 --> 00:33:36.780
Other various easy-to-use visual
elements that help you quickly

00:33:36.780 --> 00:33:38.760
identify problems with your code.

00:33:40.000 --> 00:33:55.500
[Transcript missing]

00:33:56.380 --> 00:33:59.760
Absoft Pro Fortran,
this also was released

00:33:59.760 --> 00:33:59.900
on the heels of Tiger.

00:33:59.900 --> 00:34:03.480
Like I said, it has 64-bit and 32-bit
support for Fortran 95.

00:34:03.480 --> 00:34:06.300
It includes free technical
support from Absoft,

00:34:06.300 --> 00:34:10.000
which is one of our, you know,
we have very good tech support.

00:34:10.000 --> 00:34:13.020
We support all the products,
including on these various HPC SDKs,

00:34:13.020 --> 00:34:15.810
Total View, all are supported
centrally through Absoft.

00:34:15.880 --> 00:34:18.620
It includes C, C++ compilers.

00:34:18.640 --> 00:34:21.040
It has an IDE that's
cross-platform on Windows,

00:34:21.100 --> 00:34:24.260
Linux, and other platforms,
including our FX2 graphical

00:34:24.260 --> 00:34:26.280
debugger is included at no charge.

00:34:26.300 --> 00:34:31.590
It has Blast libraries, Atlas, HDF,
other libraries that make it

00:34:31.640 --> 00:34:37.420
easy to use on the platform,
and it is fully link compatible with GNU.

00:34:39.900 --> 00:34:42.400
Finally,
the IMSO Fortran Numerical Library.

00:34:42.460 --> 00:34:45.950
This has a complete set of
scientific algorithms and routines.

00:34:46.050 --> 00:34:49.200
Instead of having to hand code your own,
they have a vast array of these

00:34:49.200 --> 00:34:50.800
routines that you can use.

00:34:50.890 --> 00:34:52.860
They're already pre-validated
for the platform,

00:34:52.960 --> 00:34:54.770
so you know you can
get the correct answer.

00:34:54.870 --> 00:34:56.610
They have excellent documentation.

00:34:56.800 --> 00:34:59.670
If you want to then scale up,
if you get a cluster with a few

00:34:59.800 --> 00:35:03.890
simple modifications to your code,
you can then have MPI support,

00:35:03.920 --> 00:35:05.800
and it hides much of
this complexity from you.

00:35:05.800 --> 00:35:08.090
It handles bundling up,
packaging the arrays,

00:35:08.180 --> 00:35:09.800
setting them out to the nodes.

00:35:09.800 --> 00:35:12.790
All you have to know is,
"I want to solve this problem.

00:35:12.790 --> 00:35:17.230
Here's the algorithm that I want to
use." You leave the rest to IMSL.

00:35:19.380 --> 00:35:20.890
So roadmap going forward.

00:35:20.890 --> 00:35:24.300
Of course, we're going to be
supporting the Mac on Intel.

00:35:24.300 --> 00:35:26.640
That's going to be a
very big push for us.

00:35:26.660 --> 00:35:30.020
We're going to have full Fortran
2003 support on the platform.

00:35:30.020 --> 00:35:33.180
We're going to be increasingly
improving our speed of our compilers,

00:35:33.260 --> 00:35:35.880
tools and utilities, of course,
on the platform.

00:35:35.930 --> 00:35:40.690
Continued emphasis on our technical
support and introduction of some new

00:35:40.840 --> 00:35:44.320
HPC tools that you can look forward
to seeing this time next year.

00:35:44.420 --> 00:35:46.170
So thank you.

00:35:53.000 --> 00:35:53.880
Thank you.

00:35:53.880 --> 00:35:55.160
Thank you, Rod.

00:35:55.160 --> 00:35:55.720
Okay.

00:35:55.720 --> 00:35:59.380
So we did a quick run through
of the technology stacks,

00:35:59.390 --> 00:36:03.560
but what we're really trying
to get at is your applications.

00:36:03.560 --> 00:36:08.800
We're trying to get at what exactly,
what challenge it is, what question it is

00:36:08.910 --> 00:36:10.500
you're trying to answer.

00:36:10.690 --> 00:36:13.960
There are a tremendous
amount of technologies,

00:36:13.960 --> 00:36:18.030
tremendous amount of applications
that hit the platform.

00:36:18.120 --> 00:36:21.530
Some of them commercial,
some of them open source.

00:36:21.640 --> 00:36:27.700
But what we want to do here today was
introduce you to technology that looks

00:36:27.700 --> 00:36:30.920
to be very promising in the HPC arena.

00:36:31.100 --> 00:36:34.150
And to show that to you,
I'd like to ask Kevin Howard from

00:36:34.150 --> 00:36:37.830
Massively Parallel to join
us and give us a look.

00:36:38.000 --> 00:36:38.600
Kevin?

00:36:43.430 --> 00:36:43.430
Thank you.

00:36:45.680 --> 00:36:49.500
Okay, I'm Kevin Howard from Astle
Loop Parallel Technologies.

00:36:49.590 --> 00:36:52.940
We're going to show a demo,
but first I have to tell you,

00:36:52.940 --> 00:36:57.750
since this is an Apple audience,
that it's going to be ugly, really ugly,

00:36:57.750 --> 00:37:00.680
because it doesn't have flashy
graphics or anything like that.

00:37:00.680 --> 00:37:02.820
It's just going to show
one impossible thing.

00:37:02.900 --> 00:37:07.630
So hopefully that one impossible thing
will be compelling enough so that you,

00:37:07.630 --> 00:37:12.660
or at least Apple,
invites me here again sometime.

00:37:13.540 --> 00:37:16.590
Okay, why massively parallel technology?

00:37:16.600 --> 00:37:17.160
What do we do?

00:37:17.160 --> 00:37:18.030
Where do we come from?

00:37:18.040 --> 00:37:19.190
Nobody's ever heard of us.

00:37:19.600 --> 00:37:24.910
Well, our technology really is
based upon years of research.

00:37:25.000 --> 00:37:27.770
We've been around for, I think,
six years now.

00:37:27.780 --> 00:37:32.620
Primary funding came from the
Defense Advanced Projects Agency,

00:37:32.620 --> 00:37:33.360
DARPA.

00:37:34.540 --> 00:37:37.910
What we did is we derived a
new version of Amdahl's Law.

00:37:37.910 --> 00:37:41.440
What this version does,
the standard version of Amdahl's

00:37:41.440 --> 00:37:43.200
Law is kind of pathetic.

00:37:43.200 --> 00:37:47.190
Everybody in here who does high
performance computing knows that if

00:37:47.190 --> 00:37:51.110
you open up a parallelization book,
in chapter one, page three,

00:37:51.110 --> 00:37:54.380
it will say Amdahl's Law,
and you never see it again.

00:37:54.380 --> 00:37:55.960
Why is that?

00:37:55.980 --> 00:37:59.530
That's because Amdahl's
Law does not fundamentally

00:37:59.530 --> 00:38:04.450
talk about number of channels,
speed of channels, latency issues.

00:38:04.480 --> 00:38:09.320
How physical topologies are connected up,
or any of the other real issues

00:38:09.320 --> 00:38:11.820
that face practitioners of the art.

00:38:11.820 --> 00:38:15.090
And because it really only
talks about two items,

00:38:15.090 --> 00:38:18.990
it talks about Amdahl's fraction,
which is the percentage

00:38:18.990 --> 00:38:23.530
of parallel activity,
and it talks about the number of nodes.

00:38:23.640 --> 00:38:29.210
That's kind of useless
for doing real work.

00:38:29.300 --> 00:38:32.780
What we do is we've taken this new model.

00:38:34.540 --> 00:38:36.540
This derivative.

00:38:36.840 --> 00:38:41.030
Found out how it works in
terms of complete overlap,

00:38:41.200 --> 00:38:43.580
overlapping communication
and computation,

00:38:43.580 --> 00:38:47.270
and overlaid that with
different communication models.

00:38:47.280 --> 00:38:50.250
We'll show you some of those models,
not all of them, but just a few of them

00:38:50.250 --> 00:38:51.130
to give you a flavor.

00:38:51.760 --> 00:38:55.830
These are, of course, proprietary,
but so what?

00:38:55.890 --> 00:39:02.880
What we're going to show here are
a number of applications with true,

00:39:02.880 --> 00:39:06.290
what we call true super-linear,
capability.

00:39:06.640 --> 00:39:08.540
What do I mean by true
super-linear capability?

00:39:08.540 --> 00:39:12.500
What I mean is it is not a
function of cache effects.

00:39:12.540 --> 00:39:16.640
Super-linearity that comes
right from Amdahl's Law,

00:39:16.670 --> 00:39:20.410
and it comes right from the algorithms
themselves if done properly.

00:39:22.670 --> 00:39:29.490
Okay, basically our technology is kind
of instantiated in a number of

00:39:29.490 --> 00:39:33.080
libraries because like everyone else,
we want to hide how hard

00:39:33.080 --> 00:39:35.650
it is to actually do this
stuff because it is hard.

00:39:35.660 --> 00:39:42.510
What we want to do is show that you
can get much higher performing systems

00:39:42.890 --> 00:39:45.960
using some of these techniques.

00:39:46.580 --> 00:39:50.420
In fact, what we're going to show
is something kind of silly.

00:39:50.420 --> 00:39:55.190
In the world of high bandwidth,
low latency systems,

00:39:55.310 --> 00:39:58.680
we have a machine over there
where we just are using 100BaseT.

00:39:58.680 --> 00:40:02.310
And we're going to show at
least one application with

00:40:02.480 --> 00:40:04.460
super linear performance.

00:40:04.460 --> 00:40:09.520
We're going to show some pictures in a
few minutes of some performance graphs on

00:40:09.760 --> 00:40:12.900
real machines also using just 100BaseT.

00:40:12.900 --> 00:40:16.460
We're talking 80 nodes,
that sort of thing, so a reasonable size.

00:40:16.530 --> 00:40:22.690
A number of applications with
super linear performance.

00:40:23.710 --> 00:40:25.600
Okay, well here's the first one.

00:40:25.750 --> 00:40:28.500
Doesn't,
in fact we're going to show E to the X.

00:40:28.700 --> 00:40:30.360
Why do we choose E to the X?

00:40:30.440 --> 00:40:34.230
A friend of mine, John Gustafson,
who's been in the high

00:40:34.310 --> 00:40:38.100
performance computing community
for pretty much forever,

00:40:38.100 --> 00:40:43.820
and he works for a competing company,
so I won't mention it.

00:40:44.360 --> 00:40:48.550
He ran E to the X on his
competing companies machine,

00:40:48.640 --> 00:40:53.420
and what they found is that it
doesn't really scale very well.

00:40:53.420 --> 00:40:53.820
Why?

00:40:53.840 --> 00:40:57.770
Because unlike Pi and other things like
that where Ramanujan was good enough

00:40:57.890 --> 00:41:01.230
to give us solutions that you collect,
compute the last digit,

00:41:01.240 --> 00:41:03.500
and so you can spread that
over multiple machines,

00:41:03.500 --> 00:41:07.660
unfortunately E to the X is a
true series expansion problem,

00:41:07.660 --> 00:41:11.740
so you have these huge series expansions,
and you have to do the

00:41:11.880 --> 00:41:13.820
cross-summing all the way through.

00:41:13.820 --> 00:41:16.780
And so it doesn't really scale greatly.

00:41:16.780 --> 00:41:18.860
It scales okay,
but it doesn't scale greatly.

00:41:18.860 --> 00:41:22.600
What we were able to do,
this is with two channels,

00:41:22.600 --> 00:41:24.590
and we'll explain why.

00:41:24.600 --> 00:41:28.900
200 base T channels on fast machines,
80 of them,

00:41:28.900 --> 00:41:36.860
and the effect of that machine was to
have performance that was near 140 times,

00:41:36.860 --> 00:41:41.590
140 machines is what the equivalent was.

00:41:41.590 --> 00:41:43.340
So we had 80 machines that
were acting like this,

00:41:43.340 --> 00:41:43.340
and we had to do a lot of
work to get it to work.

00:41:43.340 --> 00:41:47.650
And so we had 100 machines that
were acting like 140 machines.

00:41:47.650 --> 00:41:48.740
How can that be?

00:41:49.700 --> 00:48:41.900
[Transcript missing]

00:48:42.300 --> 00:48:47.590
Okay, here's our first little,
our simple cascading model.

00:48:47.750 --> 00:48:48.780
Why are we showing this?

00:48:48.830 --> 00:48:50.200
It doesn't look like much.

00:48:50.200 --> 00:48:52.140
It looks kind of like
just a tree architecture,

00:48:52.200 --> 00:48:52.600
right?

00:48:52.780 --> 00:48:55.250
Except that if you look
at a standard B-tree,

00:48:55.260 --> 00:49:01.000
binary tree architecture,
the expansion rate is 1, 2, 4, 8, etc.

00:49:01.000 --> 00:49:02.350
You can go play over there.

00:49:02.550 --> 00:49:03.220
This is Glenn.

00:49:03.550 --> 00:49:07.470
He'll be playing the Mac for you today.

00:49:08.320 --> 00:49:12.150
This model here,
the growth rate of how many nodes

00:49:12.210 --> 00:49:15.460
are in communication per unit time,
oh and by the way that thing that says

00:49:15.600 --> 00:49:22.340
home node is really our version of a
head node or something analogous to that.

00:49:22.520 --> 00:49:25.190
It's not really part
of the communication.

00:49:25.330 --> 00:49:28.690
But its growth rate for
a single channel is 1,

00:49:28.700 --> 00:49:33.830
3, 7, 15, 31,
etc., etc., which is basically 1

00:49:34.270 --> 00:49:36.850
minus 2 times faster than a B-tree.

00:49:37.300 --> 00:49:39.160
What the heck does that mean?

00:49:39.490 --> 00:49:45.960
Since what it means on a single
channel is that prior to our discovery,

00:49:45.960 --> 00:49:52.170
our company's discovery of this pattern,
the binary tree was considered for

00:49:52.170 --> 00:49:55.200
a single channel to be the best
expansion rate you're going to.

00:49:55.410 --> 00:49:58.720
So we're going to be able to have this.

00:49:58.720 --> 00:50:03.500
This is showing the effects of
just adding a single additional

00:50:04.600 --> 00:53:15.400
[Transcript missing]

00:53:15.620 --> 00:53:19.060
The number of channels that
are involved in a communication

00:53:19.060 --> 00:53:25.480
model have a non-linear effect
on the speed of your system.

00:53:25.670 --> 00:53:26.860
A nonlinear effect.

00:53:26.960 --> 00:53:31.940
In fact, the nonlinear effect is--
we're going to do the demo,

00:53:31.940 --> 00:53:35.950
so you might as well start.

00:53:36.720 --> 00:53:40.580
Can you switch over to
demo one so that he can,

00:53:40.640 --> 00:53:41.760
oh, you've already done that.

00:53:41.810 --> 00:53:44.520
The reason I'm just showing you,
I told you it was going to be ugly, guys.

00:53:44.520 --> 00:53:47.960
He just ran a quickie little
version of E to the X.

00:53:48.120 --> 00:53:51.470
We're only showing 300
digits because it goes nuts,

00:53:51.700 --> 00:53:55.380
but this has 65,000 digits of precision.

00:53:55.380 --> 00:54:01.420
And it took, on a single node,
it took 160 seconds.

00:54:02.110 --> 00:54:04.120
What we're going to do
is now run it on three,

00:54:04.120 --> 00:54:06.800
run it on seven nodes,
that machine right there.

00:54:06.800 --> 00:54:10.960
We're running it on seven nodes.

00:54:12.100 --> 00:54:13.740
Let's see.

00:54:13.740 --> 00:54:15.300
You know,
it takes a long time when you're standing

00:54:15.300 --> 00:54:19.620
up here and waiting for a machine to
write little green numbers on the screen,

00:54:19.800 --> 00:54:21.340
but there you go.

00:54:21.880 --> 00:54:24.300
Okay, it's done.

00:54:24.620 --> 00:54:28.800
Now, it took 17 seconds for seven nodes,
so if we do the math,

00:54:28.800 --> 00:54:30.600
we have a little calculator here.

00:54:30.630 --> 00:54:32.770
If you do the math...

00:54:35.650 --> 00:54:39.600
You can see that our seven
machines acts as if we had nine.

00:54:39.600 --> 00:54:43.110
But that to me is pretty cool.

00:54:43.240 --> 00:54:45.580
I don't know about you, but.

00:54:50.130 --> 00:54:52.700
So where are we going with this?

00:54:52.890 --> 00:54:56.210
How are we going to provide this
kind of capability to the community?

00:54:56.220 --> 00:55:01.320
What kind of other things
can we do with this?

00:55:01.390 --> 00:55:03.840
Of course, we're doing bioinformatics.

00:55:03.880 --> 00:55:07.240
We're building right now a very
large bioinformatics website

00:55:07.440 --> 00:55:09.700
application to try before you buy.

00:55:11.080 --> 00:55:14.130
It'll be made of, of course,
Apple clusters.

00:55:14.540 --> 00:55:19.310
Molecular modeling,
it turns out that our method of,

00:55:19.320 --> 00:55:22.500
we call it beta phase,
overlapped I/O and communication

00:55:22.500 --> 00:55:26.560
and computation model,
using some of these cheap tricks,

00:55:26.710 --> 00:55:30.520
allows us to do molecular
modeling that scales,

00:55:30.520 --> 00:55:35.240
in most cases, super linearly,
but there are some cases where it will

00:55:35.240 --> 00:55:38.110
scale just a little under sub-linearly.

00:55:38.240 --> 00:55:40.020
Oh well, can't have it all.

00:55:40.290 --> 00:55:41.040
Not everything's scalable.

00:55:41.040 --> 00:55:42.840
It doesn't scale super linearly,
I will say that.

00:55:42.840 --> 00:55:48.400
It depends on the underlying
nature of the algorithm.

00:55:48.400 --> 00:55:52.160
We do a lot of image processing today.

00:55:52.160 --> 00:55:54.480
Naturally doing things with DARPA,
what they want us to do is

00:55:54.510 --> 00:55:59.000
find stuff in bizarre places.

00:55:59.000 --> 00:56:01.350
And I can't tell anybody,
including my wife,

00:56:01.360 --> 00:56:03.080
what I'm actually looking at.

00:56:03.080 --> 00:56:03.980
What did you see today?

00:56:04.070 --> 00:56:05.960
Nothing.

00:56:05.960 --> 00:56:09.610
So neural networks,
the reason why neural networks are

00:56:09.680 --> 00:56:13.840
particularly nice for us is because
they're an order n squared problem.

00:56:13.900 --> 00:56:17.180
And it turns out that the
mathematical order of the problem,

00:56:17.180 --> 00:56:20.860
depending on certain aspects,
but the mathematical order of

00:56:20.860 --> 00:56:24.800
the problem is what's generating
the super linear effect.

00:56:24.800 --> 00:56:30.750
It's the order of the problem because
what's happening is that acts as a

00:56:30.760 --> 00:56:36.150
nonlinear decreaser of the amount
of work that's being calculated

00:56:36.150 --> 00:56:37.770
when you break the problem up.

00:56:37.840 --> 00:56:40.960
Seismic processing,
we've already heard about that.

00:56:40.960 --> 00:56:44.960
We've already shown very,
very good effects with Kirchhoff

00:56:44.960 --> 00:56:47.700
time migration codes and other codes.

00:56:47.770 --> 00:56:52.640
I know that those are -- that that's
an embarrassingly parallel mechanism,

00:56:52.730 --> 00:56:56.860
but using some of our I/O models,
we were able to show a 10x

00:56:56.900 --> 00:57:02.350
increase on the same cluster over
existing -- several existing codes.

00:57:02.560 --> 00:57:07.610
And computational fluid dynamics models,
these are very, very tough codes,

00:57:07.710 --> 00:57:12.280
mainly because of the very,
very small number of bytes that are

00:57:12.280 --> 00:57:17.480
being transmitted between pieces
and parts during the computation.

00:57:17.480 --> 00:57:19.240
You just don't move a lot of data.

00:57:19.240 --> 00:57:21.480
And of course,
if you don't move a lot of data,

00:57:21.480 --> 00:57:26.640
then scaling becomes an issue
because all your latency builds up.

00:57:26.640 --> 00:57:30.440
In our model,
we found some relatively unique ways

00:57:30.440 --> 00:57:37.210
of being able to continue to mask
the communication time completely,

00:57:37.440 --> 00:57:40.880
which means that even though we're
moving small amounts of data,

00:57:40.880 --> 00:57:43.320
including around the system, it's okay.

00:57:43.320 --> 00:57:46.700
So we are --

00:57:46.880 --> 00:57:49.280
As I said,
we are just now coming to market.

00:57:49.550 --> 00:57:52.640
That's why we don't have
a lot of flashy stuff.

00:57:52.820 --> 00:57:56.800
We've been around again
for a very long time.

00:57:56.800 --> 00:57:59.870
We've been primarily working
on mathematical models of

00:57:59.870 --> 00:58:07.110
computation and how those models
interact with numbers of nodes.

00:58:07.120 --> 00:58:10.930
We've found several effects
that we think are novel.

00:58:11.280 --> 00:58:14.860
The effects really revolve
around this notion that if

00:58:14.860 --> 00:58:18.620
you give me two slow channels,
I can make them work much

00:58:18.740 --> 00:58:21.110
faster than one fast channel.

00:58:21.120 --> 00:58:25.500
If you give me two fast channels,
I can make wonders occur because

00:58:25.900 --> 00:58:29.870
the effect occurs regardless
of the speed of the channel.

00:58:29.880 --> 00:58:32.870
So I know I'm not
consuming all of my time,

00:58:32.870 --> 00:58:35.440
but you know, I'm done talking.

00:58:45.860 --> 00:58:47.400
Thank you, Kevin.

00:58:47.480 --> 00:58:48.750
So I hope you enjoyed that sneak peek.

00:58:48.800 --> 00:58:53.730
It's not often you get to see what
DARPA actually gets to work on.

00:58:53.800 --> 01:00:57.100
[Transcript missing]

01:00:58.100 --> 01:01:01.430
There are some great resources where we
can exchange this type of information.

01:01:01.540 --> 01:01:05.490
The Apple SciTech list is really sort
of the granddaddy of HPC at Apple.

01:01:05.490 --> 01:01:08.310
It's where a tremendous
amount of HPC-oriented and

01:01:08.310 --> 01:01:12.350
obviously science-oriented
communication takes place.

01:01:12.350 --> 01:01:16.660
Since HPC did receive so much activity,
this year we actually

01:01:16.660 --> 01:01:18.400
created an Apple HPC list.

01:01:18.400 --> 01:01:21.570
So these can both be found at
lists.apple.com and receive a

01:01:21.570 --> 01:01:24.680
tremendous amount of traffic,
users helping users,

01:01:25.130 --> 01:01:28.480
tremendous amount of Apple engineers
sign on and actually provide

01:01:28.480 --> 01:01:30.500
expertise on those lists as well.

01:01:30.500 --> 01:01:35.300
So that should really be
something you monitor continually.

01:01:35.300 --> 01:01:40.630
The HPC page at
developer.apple.com is found at

01:01:40.630 --> 01:01:44.780
developer.apple.com/hardware/HPC.

01:01:44.920 --> 01:01:47.310
Success stories, how-tos,
that sort of thing are the types

01:01:47.310 --> 01:01:49.180
of material you'll find there.

01:01:49.180 --> 01:01:51.340
And of course the
Apple and Science website,

01:01:51.340 --> 01:01:54.990
apple.com/science,
is another fantastic resource.

01:01:55.000 --> 01:01:57.810
So you can find all of that there.

01:01:57.810 --> 01:02:02.000
And of course,
we also have a lot of resources that

01:02:02.000 --> 01:02:07.330
you can use to keep up with the data
and what's happening on the platform.

01:02:07.330 --> 01:02:11.220
We mentioned a couple
of sessions earlier.

01:02:11.220 --> 01:02:16.750
I would recommend that you
check out the large installation

01:02:16.750 --> 01:02:19.420
and HPC systems admin session.

01:02:19.420 --> 01:02:24.490
We have a small InfiniBand cluster
in the Enterprise IT lab for you to

01:02:24.510 --> 01:02:27.500
use and ask questions of that team.

01:02:27.500 --> 01:02:33.500
The Xgrid session looks fantastic.

01:02:33.500 --> 01:02:36.770
Using Xgrid, create and deploy
distributed computations.

01:02:36.770 --> 01:02:41.000
Using Xgrid session on Friday, again,
another excellent session.

01:02:41.000 --> 01:02:46.810
And if you would like a peek
at FXP that Rodney showed you

01:02:46.810 --> 01:02:50.650
earlier and Etnis TotalView,
I highly recommend you check out the

01:02:50.660 --> 01:02:52.930
debugging parallel application session.

01:02:53.040 --> 01:02:56.120
That's also the session where we
will be giving details of how you can

01:02:56.130 --> 01:03:01.000
get your hands on the memory manager
code that was discussed earlier.

01:03:03.890 --> 01:03:06.540
and if all else fails,
I strongly encourage you

01:03:06.540 --> 01:03:08.940
to contact skip@apple.com.

01:03:08.970 --> 01:03:10.940
Incredibly easy email to remember.

01:03:10.980 --> 01:03:16.040
I look forward to helping you
engage the resources at Apple and

01:03:16.230 --> 01:03:20.780
being of help wherever I can.

01:03:20.890 --> 01:03:24.910
I want to thank all of the
Apple HPC developer partners,

01:03:24.910 --> 01:03:26.800
customers that are here.