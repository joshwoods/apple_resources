WEBVTT

00:00:10.310 --> 00:00:11.490
Good morning.

00:00:11.590 --> 00:00:18.070
Welcome to the
Java Virtual Machine Exposed.

00:00:18.180 --> 00:00:19.700
Don't panic.

00:00:19.820 --> 00:00:22.480
We didn't bring trench coats.

00:00:23.910 --> 00:00:28.340
For those of you who did come here
out of voyeuristic tendencies,

00:00:28.340 --> 00:00:29.640
you can leave now.

00:00:29.770 --> 00:00:35.640
What this talk is really for are for
developers who want to look under the

00:00:35.800 --> 00:00:42.530
hood at the Java VM engine on Tiger,
and who want to kick the tires of the

00:00:42.530 --> 00:00:45.750
Java VM on the new Intel-based Macs.

00:00:46.620 --> 00:00:51.610
So, what you're going to learn
about Mac OS X on Java,

00:00:51.610 --> 00:00:56.560
Java on Mac OS X in this talk,
is the here, the now,

00:00:56.560 --> 00:00:59.630
and a preliminary look at the race ahead.

00:00:59.640 --> 00:01:04.740
The here is Tiger,
what we delivered six weeks ago,

00:01:04.980 --> 00:01:07.600
namely J2SE 5.0.

00:01:07.600 --> 00:01:13.000
That's Java 1.5, for those of you on
the old naming scheme.

00:01:13.540 --> 00:01:16.540
With the C1 compiler,
that's the same compiler

00:01:16.540 --> 00:01:22.530
we've shipped in the past,
with earlier versions of Mac OS X.

00:01:22.680 --> 00:01:26.250
And then, let's see,
Mikey McDougall's gonna come

00:01:26.250 --> 00:01:27.790
up and fill you in on that.

00:01:27.950 --> 00:01:33.780
And then Victor Hernandez is
going to come up and talk about

00:01:33.910 --> 00:01:38.530
what we just did right now,
namely the Tiger on Intel Macs,

00:01:39.180 --> 00:01:50.490
how we have older versions of Java 1.4.2,
and then the C1 compiler on Java 1.5,

00:01:50.490 --> 00:01:54.500
and also he's gonna introduce
the new C2 compiler that we're

00:01:54.500 --> 00:02:02.100
shipping with the developer kit
on the new Intel-based Macs.

00:02:02.110 --> 00:02:05.980
Actually, it's not on the developer DVD,
but it's gonna be a separate download,

00:02:05.980 --> 00:02:08.500
just like 1.5 was with Tiger.

00:02:08.500 --> 00:02:13.410
And then I'm gonna come back,
my name's Roger Hoover, after that,

00:02:13.570 --> 00:02:17.700
and give you some preliminary numbers
on where we are on the development

00:02:17.830 --> 00:02:19.480
platform with the Intel box.

00:02:19.480 --> 00:02:21.080
So, Mikey?

00:02:26.700 --> 00:02:29.020
Thanks, Roger.

00:02:29.090 --> 00:02:32.580
So the Java Virtual Machine in Tiger

00:02:32.790 --> 00:02:34.840
It's really sort of cool to
be able to talk about this

00:02:34.840 --> 00:02:37.170
because it's a really good VM.

00:02:37.200 --> 00:02:40.810
Tiger comes,
and this is sort of probably the

00:02:40.900 --> 00:02:44.110
third time you've heard this,
but it comes with 1.4.2 and

00:02:44.380 --> 00:02:46.280
1.3.1 sort of installed.

00:02:46.280 --> 00:02:47.200
Every Mac's got them.

00:02:47.220 --> 00:02:51.030
And now some of your
customers have got J2SE 5.0.

00:02:51.040 --> 00:02:54.320
This came out six weeks ago,
same time that Tiger came out.

00:02:54.410 --> 00:02:58.120
And it's available as
a software download.

00:02:58.120 --> 00:03:01.690
It's so, I mean,
it turned out really well.

00:03:01.940 --> 00:03:03.400
It's going to be a software update soon.

00:03:08.130 --> 00:03:12.860
So, one of the things that people
ask a lot about is like,

00:03:12.860 --> 00:03:15.840
where are we going with Java?

00:03:15.870 --> 00:03:19.440
So, what's our support story?

00:03:19.490 --> 00:03:21.700
We're moving forward, really are.

00:03:21.820 --> 00:03:27.040
The J2SE 5.0 is really the
focus of our efforts now.

00:03:27.660 --> 00:03:30.770
and the good news there is that
it's a really good release,

00:03:30.770 --> 00:03:35.980
and that all your 1.4 stuff
still runs in J2SE 5.0.

00:03:36.150 --> 00:03:39.210
If you have any backwards compatibility
bugs that you're encountering,

00:03:39.220 --> 00:03:39.930
let us know.

00:03:39.960 --> 00:03:43.900
Let us know right now,
because this is where we're going.

00:03:43.920 --> 00:03:48.830
And really, really,
Java 1.3.1 is going away.

00:03:49.180 --> 00:03:55.690
So this is some stuff that Sun said
about their release of J2SE 5.0.

00:03:55.760 --> 00:03:57.850
Your application, it already works.

00:03:57.920 --> 00:03:59.760
The platform was subjected
to a lot of testing,

00:03:59.760 --> 00:04:02.420
and Sun was really
proud of their release,

00:04:02.420 --> 00:04:03.030
and rightly so.

00:04:03.040 --> 00:04:04.600
They did a great job.

00:04:04.630 --> 00:04:08.900
And I'm here to tell you that
it's a really good release.

00:04:08.930 --> 00:04:12.330
If you're not on J2SE 5.0,
you should download it.

00:04:12.340 --> 00:04:16.430
You should give it a try,
because it's really solid.

00:04:16.760 --> 00:04:18.220
So what's new?

00:04:18.420 --> 00:04:25.910
This really is sort of the biggest delta
for a Java release that has happened in

00:04:25.910 --> 00:04:28.420
the history of Delta's Java releases.

00:04:28.560 --> 00:04:30.550
There's language changes that
I'm gonna go into briefly.

00:04:30.690 --> 00:04:32.920
There's library and runtime changes.

00:04:33.180 --> 00:04:35.640
How many folks are running J2SE 5.0?

00:04:35.780 --> 00:04:38.420
I mean,
how much of this is review material for?

00:04:38.440 --> 00:04:39.260
Okay, good.

00:04:39.260 --> 00:04:42.240
So I won't burn through this,
but I'll go through

00:04:42.240 --> 00:04:43.700
this relatively quickly.

00:04:44.660 --> 00:04:46.820
So language features.

00:04:46.860 --> 00:04:48.040
So now we get auto boxing.

00:04:48.040 --> 00:04:52.150
So as a programmer,
you can go back and forth between

00:04:52.150 --> 00:04:58.030
the base type int and the object
integer sort of interchangeably.

00:04:58.220 --> 00:04:59.780
We have support for generics.

00:04:59.800 --> 00:05:03.000
So this really lets you
provide more object-oriented,

00:05:03.000 --> 00:05:05.830
more portable code and libraries.

00:05:05.860 --> 00:05:07.920
Static import, very handy.

00:05:08.030 --> 00:05:11.380
You get the enhanced for loop for,
just makes iterators a

00:05:11.380 --> 00:05:13.170
little bit cleaner looking.

00:05:13.180 --> 00:05:14.970
You get var args, which are very nice.

00:05:15.000 --> 00:05:17.270
And metadata,
metadata is really powerful.

00:05:17.280 --> 00:05:20.120
I mean, originally when I saw
the whole Java doc thing,

00:05:20.120 --> 00:05:21.680
I'm like, huh?

00:05:21.850 --> 00:05:26.890
But it's turned out to be just huge,
and I think it's gonna get bigger.

00:05:27.670 --> 00:05:30.330
So one thing you should know about
all these language enhancements

00:05:30.420 --> 00:05:33.360
are it's all syntactic.

00:05:33.360 --> 00:05:34.840
It's all up in Java C.

00:05:34.840 --> 00:05:37.330
It doesn't actually affect the
bytecodes that get spit out.

00:05:37.400 --> 00:05:44.460
And they all really do things that make
it so that you can write good code,

00:05:44.460 --> 00:05:46.250
portable code.

00:05:46.280 --> 00:05:49.880
And by and large,
none of these changes sort of affect

00:05:49.880 --> 00:05:52.460
how your application will perform.

00:05:52.460 --> 00:05:55.360
With one exception that I'm
going to talk about briefly.

00:05:55.360 --> 00:05:56.550
This is autoboxing.

00:05:57.240 --> 00:06:01.610
Autoboxing is a little odd in that
autoboxing does create objects for you.

00:06:01.920 --> 00:06:05.270
And so while this is generally
good because you're going to have

00:06:05.270 --> 00:06:07.560
to create the objects anyway,
you can get yourself into

00:06:07.560 --> 00:06:08.610
a little bit of trouble.

00:06:08.760 --> 00:06:13.960
So here's an example for those
of you who took calculus in

00:06:14.000 --> 00:06:15.800
maybe high school or college.

00:06:15.800 --> 00:06:18.660
If you remember Taylor series
and things like that,

00:06:18.660 --> 00:06:21.580
this is something that's going
to calculate a quarter of pi

00:06:21.580 --> 00:06:26.240
by successive approximations,
a little loop.

00:06:26.880 --> 00:06:30.120
And it runs through and builds it
up by running through this loop

00:06:30.120 --> 00:06:32.230
again and again and again and again.

00:06:32.360 --> 00:06:34.960
And, you know,
if you're straining your brain

00:06:35.260 --> 00:06:41.190
to remember Taylor polynomials,
you might make...

00:06:41.780 --> 00:06:43.720
A slightly different version.

00:06:43.910 --> 00:06:49.690
Now, the only difference
between these two versions,

00:06:49.690 --> 00:06:49.690
I don't know if you can see it,

00:06:51.500 --> 00:06:55.400
The first example is
using a double capital D,

00:06:55.400 --> 00:06:58.400
and the second example is
using a double lowercase d.

00:06:58.410 --> 00:07:03.660
If you push this through into what that
means once auto-boxing is turned on,

00:07:03.810 --> 00:07:12.190
you can see that you've got a
fair amount of extra cruft there.

00:07:12.450 --> 00:07:14.400
So, just be aware that this can happen.

00:07:14.420 --> 00:07:17.900
So there's sort of a good news,
bad news kind of thing.

00:07:17.900 --> 00:07:20.920
The bad news is it's like,
takes two and a half,

00:07:20.980 --> 00:07:22.260
2.7 times longer to run.

00:07:22.260 --> 00:07:25.270
The good news is,
that only took two and a half,

00:07:25.270 --> 00:07:27.180
2.7 times longer to run.

00:07:27.180 --> 00:07:34.090
So, the good news is,
the compilers are really good.

00:07:34.650 --> 00:07:36.590
So, just be aware that this can happen.

00:07:36.600 --> 00:07:40.100
So there's sort of a good news,
bad news kind of thing.

00:07:40.100 --> 00:07:43.180
The bad news is it's like,
takes two and a half,

00:07:43.180 --> 00:07:44.460
2.7 times longer to run.

00:07:44.460 --> 00:07:47.460
The good news is,
that only took two and a half,

00:07:47.470 --> 00:07:49.370
2.7 times longer to run.

00:07:49.430 --> 00:07:56.190
So, the good news is,
the compilers are really good.

00:08:04.600 --> 00:08:17.470
So, the good news is,
the compilers are really good.

00:08:18.850 --> 00:08:21.690
We got a real high precision
time available now.

00:08:21.990 --> 00:08:26.540
We got a super fast string builder.

00:08:26.540 --> 00:08:29.210
It's not thread safe,
but it allows us to do sort

00:08:29.210 --> 00:08:32.520
of what would be older,
slower string operations much faster.

00:08:32.520 --> 00:08:35.880
The string plus operator
now goes much faster.

00:08:35.920 --> 00:08:38.140
And if you were in the
session before this,

00:08:38.160 --> 00:08:41.320
you probably heard about some of the
graphics improvements that are available.

00:08:41.320 --> 00:08:46.510
So you also get some
graphics pipelining stuff.

00:08:47.930 --> 00:08:50.030
So I want to talk about
the virtual machine that is

00:08:50.110 --> 00:08:53.010
shipping right now in Tiger.

00:08:53.230 --> 00:08:55.800
This is the C1 compiler,
something that's called

00:08:55.800 --> 00:08:57.180
the client compiler.

00:08:57.200 --> 00:09:00.430
It's a just-in-time compiler,
so you're running along, doing bytecodes,

00:09:00.500 --> 00:09:03.900
bytecodes, bytecodes, and it says, "Oh,
this is a hot method.

00:09:04.020 --> 00:09:07.360
"I'm gonna take it down and crank it
right onto the machine." There's a couple

00:09:07.360 --> 00:09:08.950
things that you need to know about this.

00:09:08.960 --> 00:09:11.810
One is that it will go to
whatever machine it's on.

00:09:11.860 --> 00:09:13.420
If it's on a G5, it'll go to a G5.

00:09:13.480 --> 00:09:15.230
If it's on a G4, it'll go to a G4.

00:09:15.430 --> 00:09:19.720
So it really uses the best
instructions that it has available

00:09:19.770 --> 00:09:22.090
to it to do any particular job.

00:09:22.790 --> 00:09:29.500
The internal representation of
classes are basically static.

00:09:29.500 --> 00:09:30.180
They don't change.

00:09:30.180 --> 00:09:34.530
And so one of the other things we've
managed to do to boost startup time

00:09:34.530 --> 00:09:38.250
is to sort of freeze dry classes,
and then share those between

00:09:38.250 --> 00:09:39.540
various instances of JVMs.

00:09:39.540 --> 00:09:42.630
And what you may not know is
you have a variety of garbage

00:09:42.630 --> 00:09:45.020
collector options available to you.

00:09:45.020 --> 00:09:47.320
So let's talk about the
compiler itself first.

00:09:48.150 --> 00:09:53.860
So the whole idea behind a compiler
is that you want to run fast.

00:09:54.320 --> 00:09:57.880
And you also don't want to spend
a whole lot of time compiling.

00:09:57.920 --> 00:10:00.770
So we look at the hot methods,
and those are the methods

00:10:00.800 --> 00:10:02.900
that we recompile.

00:10:03.160 --> 00:10:06.500
Now, generally, just having a JIT,
a Just-In-Time Compiler,

00:10:06.500 --> 00:10:08.890
will make your stuff run 10 times faster.

00:10:09.120 --> 00:10:12.720
But there's a sort of balance that
needs to be struck between compilation

00:10:12.720 --> 00:10:17.170
time and time that you actually
spend doing work with the processors.

00:10:18.590 --> 00:10:23.590
So, we'll talk later about sort
of more classic compilers,

00:10:23.590 --> 00:10:25.160
stuff you'd see in C and C++.

00:10:25.200 --> 00:10:30.180
These you have to crank pretty hard
to get absolutely optimal code out of,

00:10:30.190 --> 00:10:32.820
and optimal register allocation.

00:10:32.850 --> 00:10:36.130
So the C1 compiler actually provides
a good balance between that.

00:10:36.200 --> 00:10:39.260
It compiles quickly and still
produces quite good code.

00:10:39.310 --> 00:10:42.370
The C1 compiler is the default compiler.

00:10:43.090 --> 00:10:45.700
It's got a couple things in it that
are sort of interesting to know about.

00:10:45.880 --> 00:10:47.890
The first thing took me a
while to understand this

00:10:47.930 --> 00:10:49.020
on-stack replacement thing.

00:10:49.020 --> 00:10:54.050
So, you can imagine that you have
a method that has a loop in it,

00:10:54.090 --> 00:10:55.320
and you're going to
loop around that loop,

00:10:55.320 --> 00:10:56.540
you know, 50,000 times.

00:10:56.560 --> 00:11:01.850
Now, it'd be kind of a drag if you had
to be interpreted 50,000 times

00:11:01.860 --> 00:11:04.590
before you got up out of that loop,
and then you could compile it.

00:11:04.720 --> 00:11:08.860
So, the C1 compiler actually does,
it compiles it in place.

00:11:08.860 --> 00:11:12.380
It says, oh,
you've been in this function a long time.

00:11:12.990 --> 00:11:14.360
Maybe I should compile it.

00:11:14.580 --> 00:11:18.020
And then so, it compiles it,
and it sort of inserts you

00:11:18.040 --> 00:11:21.950
into the function on the stack,
you know, sort of live.

00:11:22.080 --> 00:11:24.130
And that's really quite powerful.

00:11:24.140 --> 00:11:31.010
The other thing that it does is the
VM knows sort of instantaneously what

00:11:31.560 --> 00:11:33.760
classes have and haven't been loaded.

00:11:33.760 --> 00:11:36.580
So,
even if you have two classes with virtual

00:11:36.580 --> 00:11:40.460
methods that might have the same name,
if only one of them has ever shown up,

00:11:40.480 --> 00:11:42.240
it's able to know.

00:11:42.480 --> 00:11:44.360
Oh, I can treat that as final.

00:11:44.360 --> 00:11:45.350
I can inline that.

00:11:45.370 --> 00:11:50.010
I can push that back in and up and
optimize out all the call cost.

00:11:51.690 --> 00:11:54.790
The other kind of cool thing
that the C1 compiler does is,

00:11:54.790 --> 00:11:58.180
along with all this
compilation and optimization,

00:11:58.180 --> 00:11:59.370
there's deoptimization.

00:11:59.380 --> 00:12:00.940
So, for instance,
if you're in this function

00:12:00.940 --> 00:12:02.710
I was talking about before,
you're cranking around

00:12:02.720 --> 00:12:06.290
the loop 50,000 times,
and you put a breakpoint on it,

00:12:06.290 --> 00:12:07.160
for instance.

00:12:07.160 --> 00:12:12.300
The hotspot VM realizes, oh,
he wants to debug.

00:12:12.300 --> 00:12:15.040
And what it'll do is,
it kind of unfolds all

00:12:15.050 --> 00:12:18.040
of that compiled code,
takes you back into the

00:12:18.070 --> 00:12:20.250
bytecodes at the right spot.

00:12:20.750 --> 00:12:24.720
You know, sticks your, you know,
lets the debugger know, oh,

00:12:24.720 --> 00:12:26.220
this is the bytecode that you're on.

00:12:26.240 --> 00:12:28.610
And then you just kind of
keep going in real time.

00:12:28.620 --> 00:12:30.680
So that, I was really,
sort of blew my mind the

00:12:30.680 --> 00:12:31.820
first time I saw that.

00:12:31.950 --> 00:12:32.620
It's really good.

00:12:32.660 --> 00:12:36.850
So, also in the C2 compiler,
they've got some other support for, just,

00:12:36.970 --> 00:12:39.960
basically if you do
object-oriented programming,

00:12:39.990 --> 00:12:41.580
you subclass things.

00:12:41.580 --> 00:12:45.450
These are optimizations to make
it so that you can just write

00:12:45.450 --> 00:12:49.530
good object-oriented code and
still expect it to go fast.

00:12:49.840 --> 00:12:55.260
And lastly, the code generation in the C1
compiler is optimized for PowerPC.

00:12:55.260 --> 00:13:00.090
So, you get Altevec instructions,
you get 64-bit instructions for longs,

00:13:00.090 --> 00:13:01.420
floats, doubles.

00:13:03.070 --> 00:13:04.380
Let's go to this whole faster math thing.

00:13:04.380 --> 00:13:05.700
You may have seen this
slide earlier today.

00:13:05.700 --> 00:13:10.640
On a G5,
this thing turns into just a pair of

00:13:10.640 --> 00:13:14.240
loads off the stack into registers.

00:13:14.240 --> 00:13:17.520
On a G4, of course,
you haven't got 64-bit registers,

00:13:17.520 --> 00:13:20.520
so dealing with longs takes
a few more operations.

00:13:20.520 --> 00:13:25.400
In the context of this slide,
this is a really nice,

00:13:25.450 --> 00:13:27.790
clean Java function.

00:13:27.800 --> 00:13:30.330
This guy is almost certainly
going to get inlined.

00:13:30.980 --> 00:13:35.130
So on the G5,
you're not even going to pay the

00:13:35.130 --> 00:13:37.510
branch through the link register,
generally speaking.

00:13:40.340 --> 00:13:44.200
Class data showing,
so we're kind of proud of this,

00:13:45.320 --> 00:13:45.350
'cause it's cool.

00:13:46.390 --> 00:13:49.290
What you do is, as I said before,
you sort of freeze dry that

00:13:49.300 --> 00:13:52.020
internal representation of a class,
store it off on disk,

00:13:52.070 --> 00:13:57.100
and then this lets you very
quickly invoke that class.

00:13:57.100 --> 00:14:01.010
There's ways to control this if you
want to sort of tightly control and

00:14:01.010 --> 00:14:03.640
profile your code and understand things.

00:14:03.640 --> 00:14:04.610
They're listed up here.

00:14:04.620 --> 00:14:08.830
We originally came up with it at Apple,
and we gave that back to Sun,

00:14:08.900 --> 00:14:11.820
and Sun's like, wow, this is a cool idea.

00:14:11.820 --> 00:14:14.600
And they've pushed it out
through all the VMs now.

00:14:14.600 --> 00:14:18.380
So you'll be seeing this
in J2SE 5.0 everywhere.

00:14:20.810 --> 00:14:21.990
Garbage collection.

00:14:22.090 --> 00:14:25.020
So one of the great things about
Java is you haven't got to do

00:14:25.020 --> 00:14:27.130
your own memory management.

00:14:27.180 --> 00:14:33.170
There are three garbage collectors that
you have available to you right now.

00:14:33.710 --> 00:14:36.500
and they meet different needs.

00:14:36.500 --> 00:14:39.330
The default garbage collector is the
one that works with class data sharing.

00:14:39.340 --> 00:14:45.680
It's got a good mix of sort of time
spent paused to do garbage collection,

00:14:45.680 --> 00:14:50.270
versus over the length of your program,
the amount of time spent

00:14:50.270 --> 00:14:52.540
garbage collecting.

00:14:53.200 --> 00:14:56.320
There's another one available,
Concurrent Mark and Sweep.

00:14:56.370 --> 00:14:59.920
This is sort of optimized so that
over the life of your program,

00:14:59.920 --> 00:15:02.790
a long running program,
you will spend as little time as possible

00:15:03.030 --> 00:15:04.970
actually doing the garbage collection.

00:15:05.070 --> 00:15:09.270
But you might have slightly longer pauses
while it does that very efficiently.

00:15:09.330 --> 00:15:11.390
And then lastly,
we've got another one that

00:15:11.410 --> 00:15:15.380
has sort of briefer pauses,
so it's sort of more real time.

00:15:15.530 --> 00:15:20.500
But over the life of your program,
you're going to see longer,

00:15:20.560 --> 00:15:23.600
more time actually spent
doing the garbage collection.

00:15:23.650 --> 00:15:27.190
The take home point from this
whole slide is that you have

00:15:27.190 --> 00:15:28.370
choices available to you.

00:15:28.450 --> 00:15:31.390
Garbage collection is kind of
really tricky to get in your

00:15:31.390 --> 00:15:32.940
head exactly how it performs.

00:15:33.130 --> 00:15:35.540
And we don't even suggest that you try.

00:15:35.580 --> 00:15:37.880
What we really suggest that
you try is if you have sort

00:15:37.880 --> 00:15:41.960
of performance critical stuff,
try each of these garbage collectors,

00:15:42.010 --> 00:15:45.580
see which one matches
your needs the best,

00:15:45.580 --> 00:15:48.390
and then use that one.

00:15:49.900 --> 00:15:54.560
And you shouldn't go to any
Java sessions this week and not

00:15:54.560 --> 00:15:56.700
have us tell you about Shark,
because Shark is so damn cool.

00:15:56.700 --> 00:16:01.380
It's a really good performance tool.

00:16:01.380 --> 00:16:03.380
You get it for free with
your development package.

00:16:03.380 --> 00:16:07.840
I've just seen people get huge
wins just doing some really

00:16:07.950 --> 00:16:10.000
straightforward optimizations.

00:16:10.000 --> 00:16:17.360
So, get Shark, run Shark,
identify your execution bottlenecks.

00:16:18.040 --> 00:16:21.500
I mean, it's really great.

00:16:21.520 --> 00:16:25.040
So, knowing that there's a compiler
under there actually has

00:16:25.040 --> 00:16:27.300
implications for your code.

00:16:27.560 --> 00:16:30.800
I mean, it's really great.

00:16:30.990 --> 00:16:34.310
So knowing that there's a
compiler under there actually

00:16:34.310 --> 00:16:36.760
has implications for your code.

00:16:57.500 --> 00:17:01.010
two different things,
but we're unable to hoist a method up

00:17:01.090 --> 00:17:06.900
and inline it if it's synchronized,
if it's very very heavy, if it's large,

00:17:07.010 --> 00:17:09.520
and also if you have
an exception handler,

00:17:09.740 --> 00:17:12.810
that also will not be inlined.

00:17:13.350 --> 00:17:17.430
and you know, it's the 21st century,
so here's some sort of general

00:17:17.500 --> 00:17:20.540
programming tips knowing that you're
working with the hotspot compiler.

00:17:20.550 --> 00:17:21.620
Exceptions.

00:17:21.640 --> 00:17:23.860
Exceptions are objects,
and they're not like little

00:17:23.950 --> 00:17:25.760
featherweight objects,
they're heavy objects.

00:17:25.760 --> 00:17:27.660
They got a whole stack frame in them.

00:17:27.680 --> 00:17:29.740
So use them for error handling.

00:17:29.840 --> 00:17:31.510
Don't use them for control flow.

00:17:31.660 --> 00:17:34.700
Don't use them to like terminate the
end of your loop or something like that.

00:17:34.700 --> 00:17:36.210
It's just...

00:17:36.830 --> 00:17:38.620
Very odd to see that.

00:17:38.960 --> 00:17:42.900
The other thing,
and this is sort of more important,

00:17:42.950 --> 00:17:44.620
is don't use object pools.

00:17:45.010 --> 00:17:50.300
The whole power of Java in many
ways is that you don't have to

00:17:50.300 --> 00:17:53.880
worry about object management.

00:17:53.950 --> 00:17:57.610
Object creation, object deletion,
it's very different from Cocoa that way.

00:17:57.960 --> 00:18:03.340
And we keep making the handling of
objects faster and faster and faster.

00:18:03.380 --> 00:18:04.380
So all of these things are going on.

00:18:04.570 --> 00:18:07.800
Garbage collection these days is very,
very fast.

00:18:07.820 --> 00:18:10.180
New has been in line,
new gets in lined along

00:18:10.180 --> 00:18:11.020
with everything else.

00:18:11.020 --> 00:18:12.500
And so it's really fast.

00:18:12.790 --> 00:18:15.340
We get lock-free
allocation on each thread.

00:18:15.340 --> 00:18:17.880
So stay away from these object pools.

00:18:18.100 --> 00:18:23.990
So Java, Tiger, and you, J2SE,

00:18:24.170 --> 00:18:26.440
5.0, it's ready, it's there.

00:18:26.470 --> 00:18:30.110
And generally speaking,
when you're tuning for Java,

00:18:30.200 --> 00:18:31.880
Don't optimize first.

00:18:32.430 --> 00:18:35.400
Write your code as clean as you can.

00:18:35.440 --> 00:18:37.440
Let us do the heavy
lifting with the compiler,

00:18:37.450 --> 00:18:39.400
and then only afterwards
take it to something like

00:18:39.400 --> 00:18:41.660
Shark and do your optimization.

00:18:41.660 --> 00:18:45.140
And with that,
I'd like to bring up Victor to

00:18:45.140 --> 00:18:47.390
talk about what's new on Intel.

00:18:51.720 --> 00:18:53.380
Don't optimize first.

00:18:53.380 --> 00:18:56.900
Write your code as clean as you can.

00:18:56.900 --> 00:18:58.940
Let us do the heavy
lifting with the compiler,

00:18:58.940 --> 00:19:00.890
and then only afterwards
take it to something like

00:19:00.910 --> 00:19:03.160
Shark and do your optimization.

00:19:03.160 --> 00:19:06.640
And with that,
I'd like to bring up Victor to

00:19:06.640 --> 00:19:08.900
talk about what's new on Intel.

00:19:13.960 --> 00:19:18.050
So, this is what the picture looks
like for JVM support in Tiger.

00:19:18.060 --> 00:19:21.100
We support 1.3.1, 1.4.2, and 5.0.

00:19:21.350 --> 00:19:23.530
Basically, all of the support that
Mikey just described.

00:19:23.540 --> 00:19:26.660
How does this change on the
Intel-based Macintosh computers?

00:19:26.660 --> 00:19:28.890
Well, there's two main changes.

00:19:28.890 --> 00:19:32.700
The first thing you'll see
is that 1.3.1 is gone away.

00:19:32.720 --> 00:19:35.360
We've told you for a while
that it was going away.

00:19:35.360 --> 00:19:36.230
Now it's gone.

00:19:36.230 --> 00:19:39.590
But we are actually providing
something new in replacement,

00:19:39.640 --> 00:19:41.980
which is something you've
been asking for for a while,

00:19:41.980 --> 00:19:43.580
and that's the server compiler.

00:19:44.000 --> 00:19:49.280
It's available to you under Java 1.5
on the Intel-based Macintosh computers.

00:19:52.120 --> 00:19:55.010
When you sit down at one
of these Intel-based Macs,

00:19:55.010 --> 00:19:57.540
what kind of Java support can you expect?

00:19:57.560 --> 00:20:02.870
You can expect basically
equivalent support for 1.4 and 5.0,

00:20:03.020 --> 00:20:04.560
as you found right now in Tiger.

00:20:04.560 --> 00:20:07.050
It is really robust,
and we're really proud of the

00:20:07.370 --> 00:20:09.300
implementation that we have going.

00:20:09.300 --> 00:20:13.020
I've been working with developers all
week on basically trying out their

00:20:13.020 --> 00:20:16.710
application on the Intel-based Mac,
and it's been really, really successful.

00:20:17.610 --> 00:20:20.620
Not only are they running
on Intel for the first time,

00:20:20.620 --> 00:20:24.920
but they're also sometimes going
from 1.4 to 1.5 for the first time,

00:20:24.940 --> 00:20:26.690
and it's going really, really smoothly.

00:20:26.700 --> 00:20:30.420
As Mikey mentioned before,
5.0 is really the way to go,

00:20:30.420 --> 00:20:33.510
and it also applies on
the Intel-based machines.

00:20:33.540 --> 00:20:36.590
Your Java application
should continue to work.

00:20:36.880 --> 00:20:40.960
There are some issues with having
to upgrade to universal binaries

00:20:40.960 --> 00:20:44.210
for your native libraries,
but one of the key things also

00:20:44.210 --> 00:20:47.420
is that the developer tools like
Xcode will continue to work there.

00:20:47.520 --> 00:20:52.620
Also, a lot of the open-source things
like Eclipse are basically one

00:20:52.620 --> 00:20:56.490
recompiler away from working,
and will be available for

00:20:56.490 --> 00:20:58.800
download for the systems very,
very soon.

00:21:01.640 --> 00:21:05.620
Okay, so I'm here to mainly talk about
the one thing that we're adding,

00:21:05.620 --> 00:21:08.480
which is the C2 compiler.

00:21:08.550 --> 00:21:12.420
Something that we've heard a lot from
developers that they're interested

00:21:12.420 --> 00:21:14.520
in using when deploying on Mac OS X.

00:21:15.360 --> 00:21:20.320
The main difference between C1 and C2
is the fact that C2 uses more advanced

00:21:20.320 --> 00:21:24.080
compilation techniques to generate
faster code for your Java methods.

00:21:24.080 --> 00:21:28.760
It plugs into Hotspot just like C1,
and uses a lot of the same

00:21:28.760 --> 00:21:32.370
dynamic features as C1,
including the ones mentioned

00:21:32.400 --> 00:21:33.910
here that Mikey described.

00:21:34.020 --> 00:21:36.940
Things like on-stack replacement,
and the ability to dynamically

00:21:36.940 --> 00:21:39.980
de-optimize a compiled
method back into interpreter,

00:21:39.980 --> 00:21:43.700
either for implementation
of deep inlining,

00:21:43.800 --> 00:21:46.330
aggressive inlining,
or for debugging purposes.

00:21:46.340 --> 00:21:51.390
All you have to do to get access
to the C2 compiler is to invoke

00:21:51.390 --> 00:21:54.350
Java with the minus server flag.

00:21:54.400 --> 00:21:57.610
I'm actually kind of curious,
how many people here on Mac OS X,

00:21:57.610 --> 00:21:59.740
or on any Java platform
they did deploy on,

00:21:59.810 --> 00:22:01.340
use the minus server flag?

00:22:02.960 --> 00:22:05.900
So you wanted to change your application,
and it will just work like that.

00:22:05.950 --> 00:22:08.790
We've always supported the
minus server flag back in

00:22:08.790 --> 00:22:10.900
previous versions of Mac OS X.

00:22:10.910 --> 00:22:13.220
It doesn't get you a different
just-in-time compiler.

00:22:13.220 --> 00:22:15.200
It basically gets you a
few different parameters.

00:22:15.200 --> 00:22:18.110
But now,
on the Intel-based Macintosh computers,

00:22:18.110 --> 00:22:21.250
it's actually going to get
you a different JIT compiler,

00:22:21.270 --> 00:22:25.100
a completely different set of
performance characteristics.

00:22:25.500 --> 00:22:29.230
We do, in the Apple Developer Kit,
we don't currently support

00:22:29.390 --> 00:22:32.840
passing the minus server flag in
application bundles or in applets,

00:22:32.840 --> 00:22:34.820
but that will be supported very soon.

00:22:40.360 --> 00:22:43.390
All right,
so let's talk about the C2 compiler.

00:22:43.530 --> 00:22:46.760
It does generate better
code than the C1 compiler.

00:22:46.760 --> 00:22:50.720
One of the things that you've got to
keep in mind with the C2 compiler,

00:22:50.720 --> 00:22:53.670
and your understanding of it,
is that all of the things that

00:22:53.740 --> 00:22:58.000
Mikey described earlier about
how C1 generates optimized code,

00:22:58.000 --> 00:23:01.900
think of that as a
subset of what C2 does.

00:23:01.900 --> 00:23:03.740
It simply does a better job.

00:23:03.760 --> 00:23:06.660
To do a better job at compiling
your hot Java methods,

00:23:06.660 --> 00:23:09.030
it unfortunately does take longer to do.

00:23:09.290 --> 00:23:13.050
The general rule of thumb is that the
C2 compiler takes 10 times as long

00:23:13.060 --> 00:23:16.050
to compile your Java method as C1.

00:23:16.080 --> 00:23:20.960
This can actually cause a visible
pause in your application,

00:23:20.960 --> 00:23:24.820
similar to a visible pause you
might see from garbage collection.

00:23:24.820 --> 00:23:29.460
So when is it actually useful
for you to use a C2 compiler?

00:23:29.460 --> 00:23:33.560
It's mainly when you have a long-running
application that can withstand those

00:23:33.560 --> 00:23:35.380
possibly longer compilation pauses.

00:23:35.380 --> 00:23:39.230
And also, if you have some hot methods,
you can use some methods that

00:23:39.230 --> 00:23:42.350
are very compute-intensive,
so that the amount of time spent in them,

00:23:42.450 --> 00:23:44.850
algorithmically,
is important enough to get the

00:23:44.950 --> 00:23:46.690
best code you can possibly.

00:23:48.050 --> 00:23:51.060
The other thing to keep in mind
is that the compiler scales very

00:23:51.060 --> 00:23:54.280
well on multi-processor systems,
meaning that the pause is

00:23:54.280 --> 00:23:57.330
not so visible when you're
running on various processors.

00:23:59.920 --> 00:24:03.580
So let's actually go into detail
on what we're talking about in

00:24:03.580 --> 00:24:06.290
this compilation pause time.

00:24:07.000 --> 00:24:10.000
and how that can affect your
application's performance.

00:24:10.000 --> 00:24:13.300
So let's say this application,
basically the horizontal axis,

00:24:13.300 --> 00:24:15.730
is the amount of time it
takes to run this app.

00:24:15.810 --> 00:24:18.520
If you run it with the interpreter,
it takes the green amount of time.

00:24:18.580 --> 00:24:22.510
As you can see with C1 and C2,
they both take a less amount of time.

00:24:22.630 --> 00:24:26.240
Why does C1 take more time
than C2 in this example?

00:24:26.360 --> 00:24:29.170
Well, as you can see,
even though C1 takes less time

00:24:29.210 --> 00:24:33.010
to compile the three methods,
the generated code is

00:24:33.010 --> 00:24:36.810
substantially slower than C2,
and it actually takes a lot longer

00:24:36.870 --> 00:24:38.680
to actually execute those methods.

00:24:38.730 --> 00:24:42.230
So in this case, C2 is faster than C1.

00:24:42.280 --> 00:24:45.270
Remember, that is not always true.

00:24:45.860 --> 00:24:48.590
Here's a similar application
with three methods.

00:24:48.670 --> 00:24:51.500
And in this case,
C2 is taking so long to compile

00:24:51.500 --> 00:24:55.370
these methods that it actually
takes longer than C1 would

00:24:55.370 --> 00:24:56.770
take to even just execute them.

00:24:56.800 --> 00:25:01.800
And the amount of time taken overall
turns out to be faster for C1.

00:25:01.960 --> 00:25:04.780
This can be seen in a lot
of GUI applications when

00:25:04.780 --> 00:25:06.800
running with the C2 compiler.

00:25:06.800 --> 00:25:09.970
And you really just have to
measure your application to find

00:25:10.240 --> 00:25:14.080
out which of these two scenarios
your application falls under.

00:25:15.640 --> 00:25:19.380
All right, so how does C2 generate
better code than C1?

00:25:19.700 --> 00:25:23.060
Well, one of the things that it's mainly
doing is it's applying a lot more

00:25:23.300 --> 00:25:27.160
classic compiler optimizations
to your Java method than C1 is.

00:25:27.160 --> 00:25:30.520
That's where it's spending all
of the extra time and possibly

00:25:30.520 --> 00:25:32.180
using more memory to do so.

00:25:32.530 --> 00:25:35.060
Here's a whole list of
optimizations that C2 does,

00:25:35.060 --> 00:25:38.900
some of which are done by C1,
but mainly it's things that only C2 does.

00:25:38.900 --> 00:25:41.860
Instead of describing them here
with just a few bullet points,

00:25:41.970 --> 00:25:45.080
it's much better to illustrate them,
so let's actually go through

00:25:45.080 --> 00:25:46.680
each of these in succession.

00:25:48.960 --> 00:25:50.760
First of all,
we've got strength reduction.

00:25:50.840 --> 00:25:53.430
The thing to keep in mind is
that an optimizing compiler

00:25:53.430 --> 00:25:55.150
optimizes your code in two ways.

00:25:55.170 --> 00:25:58.970
One, it either reduces the amount
of operations being done,

00:25:58.970 --> 00:26:03.300
or chooses an operation
that's less costly.

00:26:03.540 --> 00:26:07.280
Or, it moves your code around in such
a way that it makes some other

00:26:07.280 --> 00:26:11.730
optimization able to reduce the number
of operations that are required.

00:26:11.760 --> 00:26:15.540
In this case, dividing by two is more
expensive than shifting by one,

00:26:15.540 --> 00:26:20.780
and C2 is able to identify
such a situation and replace

00:26:20.780 --> 00:26:23.770
it with a cheaper operation.

00:26:24.410 --> 00:26:25.040
Here's another one.

00:26:25.260 --> 00:26:29.170
Here's one where it actually just
moves your code around to provide

00:26:29.230 --> 00:26:30.830
another optimization to do its job.

00:26:31.230 --> 00:26:33.610
Basically,
you can move values and variables

00:26:33.610 --> 00:26:39.420
around and simplify the expression
for something like constant folding.

00:26:39.420 --> 00:26:42.400
Constant folding is powerful
because you got rid of two,

00:26:42.400 --> 00:26:46.260
you reduced from one add
operation to just simply,

00:26:46.260 --> 00:26:49.710
from two add operations to just one,
because the compiler knows what

00:26:49.710 --> 00:26:51.940
the value of the constants are
since it moved them around.

00:26:52.320 --> 00:26:54.740
And even though your code might not have
looked like this in the first place,

00:26:54.800 --> 00:26:57.830
it's able to do this for you.

00:27:00.070 --> 00:27:02.730
Another optimization is
constant propagation.

00:27:02.800 --> 00:27:05.660
Here, if you know the value of
a particular variable,

00:27:05.710 --> 00:27:09.510
you can cascade that value
down into later operations and

00:27:09.510 --> 00:27:12.600
simplify expressions later on.

00:27:14.780 --> 00:27:17.840
There's also common
sub-expression elimination.

00:27:17.880 --> 00:27:20.620
It's not just a value,
but it's actually an expression that

00:27:20.620 --> 00:27:22.570
you can propagate down and simplify it.

00:27:22.750 --> 00:27:25.930
Here, the key thing is that you're doing
the plus operation only once,

00:27:26.010 --> 00:27:27.370
and reusing the value.

00:27:29.540 --> 00:27:30.220
Loop unrolling.

00:27:30.430 --> 00:27:32.900
This is a pretty interesting one.

00:27:32.920 --> 00:27:36.950
Loops can be basically,
if you know that this loop, for example,

00:27:36.950 --> 00:27:39.280
is only being executed
four times exactly,

00:27:39.280 --> 00:27:41.990
you may as well just
execute four times in a row.

00:27:42.010 --> 00:27:43.920
It looks like this is doing
the same amount of work,

00:27:43.920 --> 00:27:45.950
but there's two things
that are being gained here.

00:27:45.970 --> 00:27:48.800
One is the fact that if
you still do the for loop,

00:27:49.090 --> 00:27:52.470
there is actually a minor overhead
to checking to see if we've

00:27:52.470 --> 00:27:56.020
gotten to the end of the loop,
and the code on the right-hand

00:27:56.020 --> 00:27:58.390
side doesn't have that problem.

00:27:58.920 --> 00:28:02.480
The other thing also is that once you
move all of this code and flatten it out,

00:28:02.480 --> 00:28:05.330
it actually enables a lot of
other optimizations to happen,

00:28:05.330 --> 00:28:07.910
especially in the case where
you're able to inline foo,

00:28:07.960 --> 00:28:09.710
which we'll get to in a second.

00:28:11.720 --> 00:28:13.300
There's also loop invariant code motion.

00:28:13.300 --> 00:28:14.660
This one is very, very powerful.

00:28:14.660 --> 00:28:16.500
C1 is not able to do this.

00:28:16.500 --> 00:28:20.230
If an expression is always
computed every single time

00:28:20.510 --> 00:28:23.060
during the iteration of a loop,
you may as well just do

00:28:23.060 --> 00:28:25.000
it once before the loop,
and then reuse the

00:28:25.000 --> 00:28:26.390
value every single time.

00:28:26.400 --> 00:28:29.890
In this case, X divided by Y,
an expensive operation,

00:28:29.930 --> 00:28:33.750
divide is expensive,
although we do do the best job we can.

00:28:33.790 --> 00:28:36.920
It's always the same every
time you go around the loop,

00:28:36.920 --> 00:28:40.640
and you may as well just do it
once before you go into the loop.

00:28:42.210 --> 00:28:43.360
There's also dead code elimination.

00:28:43.360 --> 00:28:46.400
Clearly,
this expression can be simplified

00:28:46.400 --> 00:28:52.340
to just the body of the true
branch in the if statement.

00:28:52.360 --> 00:28:56.220
You might say, well,
my code never looks like this.

00:28:56.220 --> 00:28:57.740
Why would you be doing this optimization?

00:28:57.740 --> 00:29:01.600
Well, because of other optimizations,
like, for example,

00:29:01.600 --> 00:29:05.610
knowing constants and expression values,
you can basically get into a

00:29:05.610 --> 00:29:08.430
scenario where your code actually
does end up looking like this,

00:29:08.450 --> 00:29:11.660
as far as the compiler is concerned,
and you didn't have to refactor your

00:29:11.660 --> 00:29:13.900
code specifically for this one scenario.

00:29:13.900 --> 00:29:15.900
Really, really powerful optimizations.

00:29:17.860 --> 00:29:18.890
Class Iric Analysis.

00:29:19.010 --> 00:29:20.460
Mikey mentioned this one earlier.

00:29:20.620 --> 00:29:23.260
This is actually an optimization
that C1 is able to do as well.

00:29:23.290 --> 00:29:25.980
And the key thing here is
that even though the answer

00:29:25.980 --> 00:29:28.760
is declared to be virtual,
because you wanted your

00:29:28.840 --> 00:29:32.550
code to be as easy to reuse
later on for other purposes,

00:29:32.550 --> 00:29:35.420
and override that method,
if you actually haven't

00:29:35.420 --> 00:29:38.640
overridden your method,
we can just treat the answer

00:29:38.640 --> 00:29:41.270
as being a declared final,
and we know we can

00:29:41.270 --> 00:29:43.130
dispatch directly there.

00:29:43.530 --> 00:29:46.260
That gives us the opportunity
to reduce the overhead of

00:29:46.260 --> 00:29:49.970
the virtual method lookup,
and it also lets us do the following.

00:29:50.020 --> 00:29:56.040
If we know that that method is the
only destination for the virtual call,

00:29:56.100 --> 00:29:58.690
we can inline it.

00:29:59.100 --> 00:30:30.600
[Transcript missing]

00:30:31.330 --> 00:30:34.380
All right, so you can say,
all of the specific optimizations,

00:30:34.390 --> 00:30:36.070
my code never looks like that.

00:30:36.330 --> 00:30:39.590
Well, the point I'm trying to make is
that your code does end up looking

00:30:39.860 --> 00:30:42.950
like that once the compiler is
able to move all your code around.

00:30:42.960 --> 00:30:44.390
Here's an example.

00:30:44.400 --> 00:30:47.380
Basically, all we're doing is we're
calling foo with two values.

00:30:47.380 --> 00:30:51.630
And you can reuse the foo
method in many different places

00:30:51.770 --> 00:30:54.260
with many different values.

00:30:54.610 --> 00:30:57.010
But in this specific case,
what's really powerful is that

00:30:57.010 --> 00:31:00.070
once you know the value of n and m,
you can basically optimize

00:31:00.070 --> 00:31:01.550
this thing like crazy.

00:31:01.560 --> 00:31:04.000
Once you know n and m,
you know what x is.

00:31:04.000 --> 00:31:06.450
Basically,
you can collapse the whole switch

00:31:06.450 --> 00:31:08.580
statement because you know what x is.

00:31:08.580 --> 00:31:11.260
At that point, then,
you also know what y is,

00:31:11.270 --> 00:31:14.740
and you can do dead code
elimination on the if statement.

00:31:14.740 --> 00:31:18.620
And at that point, then,
all you have is basically

00:31:18.620 --> 00:31:21.970
that return value,
which obviously you can

00:31:22.200 --> 00:31:24.500
simplify down to something.

00:31:24.540 --> 00:31:26.540
So, this is something as simple as 42.

00:31:26.540 --> 00:31:27.290
It's printing out 42.

00:31:27.300 --> 00:31:35.300
This is a real-world example that C2
does genuinely simplify down to print 42.

00:31:35.300 --> 00:31:37.090
C1 is not able to do this.

00:31:37.100 --> 00:31:40.690
This is really, really powerful,
and this does extend to just

00:31:40.810 --> 00:31:45.150
about any kind of algorithmic,
compute-intensive code you

00:31:45.150 --> 00:31:48.200
might put into your hot methods.

00:31:48.200 --> 00:31:52.300
So, as you can see,
when you're doing a lot

00:31:52.300 --> 00:31:56.330
of things like this,
C2 will be able to generate

00:31:56.330 --> 00:31:57.880
better code than C1.

00:31:57.940 --> 00:32:03.650
What happened there?

00:32:07.500 --> 00:32:09.730
Sorry, there's a delay here.

00:32:09.740 --> 00:32:14.790
All right, well, now I'm having trouble.

00:32:24.440 --> 00:32:25.240
Wow.

00:32:25.240 --> 00:32:30.320
Yeah, it's compiling.

00:32:30.320 --> 00:32:33.600
This is, you know,
one of those visual pauses.

00:32:33.650 --> 00:32:35.490
Maybe I'm compiling.

00:32:35.530 --> 00:32:39.960
Well, escape, and then press play.

00:32:39.960 --> 00:32:41.480
Let's try that.

00:32:43.580 --> 00:32:48.200
What else do you need to know about C2?

00:32:48.200 --> 00:32:51.620
There is actually some more things you
need to know about C2 than just simply

00:32:51.650 --> 00:32:53.440
how it's able to optimize your code.

00:32:53.540 --> 00:32:55.640
One of the cool things
is that it has a really,

00:32:55.640 --> 00:32:57.470
really great register allocator.

00:32:57.490 --> 00:32:59.650
This is the sort of register
allocator that's common in

00:32:59.650 --> 00:33:01.490
a lot of static compilers.

00:33:01.530 --> 00:33:04.570
A full graph coloring allocator
where it's able to use as many

00:33:04.570 --> 00:33:08.500
registers as it possibly can and limit
the amount of spills that happen.

00:33:08.500 --> 00:33:12.810
It also uses a static single assignment
intermediate representation that's

00:33:12.810 --> 00:33:14.450
pretty heavy duty compiler jargon.

00:33:14.600 --> 00:33:18.280
What you need to know about
that intermediate representation

00:33:18.580 --> 00:33:21.860
is the fact that it's able to,
it makes a lot of the optimizations

00:33:21.860 --> 00:33:25.480
that we described earlier capable
of being done very easily.

00:33:25.500 --> 00:33:29.390
You should also know that GCC4
is also using this style of

00:33:29.390 --> 00:33:34.360
intermediate representation,
and that's also one of the ways that

00:33:34.490 --> 00:33:37.830
it's able to do better optimizations.

00:33:38.040 --> 00:33:42.630
Also, there's null check elimination
that C2 is able to do.

00:33:42.660 --> 00:33:45.330
You might know that null
checks can be implemented very

00:33:45.330 --> 00:33:48.540
easily using hardware support,
and so there might not be a

00:33:48.540 --> 00:33:51.300
reason to get rid of null checks.

00:33:51.400 --> 00:33:53.800
Well, in fact,
since a null check is able to do,

00:33:53.800 --> 00:33:57.290
have as a side effect the
throwing of a Java exception,

00:33:57.340 --> 00:34:01.240
it can actually minimize the ability
of other optimizations to happen,

00:34:01.240 --> 00:34:05.120
because there's a lot of kind of control
flow that gets simplified once you

00:34:05.120 --> 00:34:06.980
get rid of that possible side effect.

00:34:06.980 --> 00:34:09.060
And so this is really powerful.

00:34:09.060 --> 00:34:11.620
Just like C1,
it's able to dynamically determine

00:34:11.890 --> 00:34:15.170
what the best instructions are on
the given process you're running.

00:34:15.290 --> 00:34:19.560
For example,
choosing to use SSC2 instructions.

00:34:19.620 --> 00:34:24.500
And it also has the ability to
knowledge of the Java memory model.

00:34:24.580 --> 00:34:28.210
So if you're using a volatile variable,
it knows how to inline code

00:34:28.210 --> 00:34:30.470
that does specifically that.

00:34:33.490 --> 00:34:38.090
So, now that you know a lot about C2,
and that you know that C2 is basically a

00:34:38.090 --> 00:34:41.650
superset of the optimizations done by C1,
the thing to keep in

00:34:41.650 --> 00:34:44.810
mind is that therefore,
all the things that Mikey described

00:34:44.920 --> 00:34:49.380
earlier as good practices for optimizing
your code in C1 also apply to C2.

00:34:49.380 --> 00:34:53.590
Just that C2 simply does a better
job at optimizing your code.

00:34:53.600 --> 00:34:57.610
So, just write your code as you would,
just object-oriented.

00:34:57.620 --> 00:35:01.200
Let us do the optimizations,
and just keep your code clean.

00:35:01.200 --> 00:35:05.090
But I want to reiterate, once again,
the tips for the 21st

00:35:05.090 --> 00:35:07.950
century that Mikey described,
which are that exceptions

00:35:07.950 --> 00:35:09.140
should be exceptional.

00:35:09.140 --> 00:35:14.770
There really is no need to optimize ahead
of time by making your methods final.

00:35:14.780 --> 00:35:17.470
And also,
that object pools are only really

00:35:17.610 --> 00:35:20.000
needed in very specific scenarios.

00:35:22.510 --> 00:35:24.540
And with that,
I want to invite Roger Hoover on

00:35:24.540 --> 00:35:27.790
stage to tell you about how,
what kind of performance changes

00:35:27.840 --> 00:35:31.680
we've seen in applications that
we've been running that will

00:35:31.680 --> 00:35:35.190
also apply to your application.

00:35:38.680 --> 00:35:42.510
Thank you, Victor.

00:35:42.860 --> 00:35:47.840
Okay, how does this performance
change with the new JVM?

00:35:47.840 --> 00:35:55.060
I'm gonna talk about how C1
and C2 compilers compare on

00:35:55.060 --> 00:35:57.460
the new Intel-based Macs.

00:35:57.480 --> 00:36:03.770
And I'm gonna talk about a couple,
three measurements, namely startup time,

00:36:03.830 --> 00:36:07.000
how does that look,
and the memory footprint,

00:36:07.010 --> 00:36:10.530
and then I'll run through
some benchmark numbers.

00:36:10.720 --> 00:36:15.060
But first let me say,
these are really preliminary results.

00:36:15.080 --> 00:36:19.650
We're running on a machine hardware
that's just a development system.

00:36:19.700 --> 00:36:21.760
The real machines are gonna be different.

00:36:21.760 --> 00:36:25.120
We just got this working like last week.

00:36:25.200 --> 00:36:29.390
So we really haven't spent
any time performance tuning

00:36:29.750 --> 00:36:31.020
on this implementation.

00:36:31.120 --> 00:36:33.590
And in specific,
there are known things that we know

00:36:33.690 --> 00:36:35.640
we have to fix performance-wise.

00:36:35.710 --> 00:36:38.560
So even though it seems to
be working really stable,

00:36:38.560 --> 00:36:43.140
and we're really happy about that,
things should improve and

00:36:43.430 --> 00:36:45.460
the numbers will change.

00:36:45.490 --> 00:36:48.280
And now I'm having this, there we go.

00:36:48.340 --> 00:36:53.280
So C1 versus C2, just startup time.

00:36:53.280 --> 00:36:57.800
Well, we kind of told you what the
goals were in the design of

00:36:57.800 --> 00:37:00.670
these two different compilers,
and that really shows

00:37:00.750 --> 00:37:02.260
up on this slide here.

00:37:02.300 --> 00:37:05.650
You can see that for a simple
little program that doesn't

00:37:05.650 --> 00:37:12.300
take much time at all,
C1 is faster than C2 on HelloAWT.

00:37:12.300 --> 00:37:17.660
But a longer and more
involved GUI-type example,

00:37:17.670 --> 00:37:22.660
C2 takes a lot longer,
62% slower at startup time.

00:37:22.760 --> 00:37:27.720
So if your application
looks like Java 2D,

00:37:27.720 --> 00:37:31.270
you're probably gonna
wanna just stay with C1.

00:37:31.990 --> 00:37:36.390
Memory footprint,
there's a little bit of difference here.

00:37:36.390 --> 00:37:39.400
And you can see that on
both of these examples,

00:37:39.400 --> 00:37:42.710
C2 takes slightly more memory.

00:37:45.100 --> 00:37:50.640
Now, on some benchmarks,
Simark is a benchmark that does

00:37:50.640 --> 00:37:55.270
a bunch of math computation,
and you'd perhaps expect a more

00:37:55.410 --> 00:37:58.140
crunching compiler might do better,
and this shows you.

00:37:58.240 --> 00:38:03.780
Here we have the scores that Simark has,
so the taller the score, the better.

00:38:03.920 --> 00:38:07.020
And you can see that
in all of these cases,

00:38:07.020 --> 00:38:13.860
C2 is outperforming C1,
and if you look at the composite score,

00:38:13.870 --> 00:38:18.440
it's 2.3 times faster, so big win by C2.

00:38:22.160 --> 00:38:30.760
And on a well-known compiler benchmark,
we're just measuring the time here.

00:38:30.760 --> 00:38:33.700
So the shorter bar is faster.

00:38:33.700 --> 00:38:35.500
The C2 numbers are there.

00:38:35.500 --> 00:38:38.360
C1 numbers are here.

00:38:38.360 --> 00:38:40.840
And you'll notice that
C2 doesn't always win.

00:38:40.860 --> 00:38:45.230
So some of this benchmark suite, C2 wins.

00:38:45.300 --> 00:38:48.360
Some of the benchmark suite, C1 wins.

00:38:48.360 --> 00:38:51.020
What does this say
about your application?

00:38:51.520 --> 00:38:52.910
Well,

00:38:53.440 --> 00:38:56.030
Just like we tried these benchmarks,
you're going to have to try your

00:38:56.110 --> 00:38:59.920
applications and see just how
well they perform under each,

00:38:59.920 --> 00:39:02.480
and pick the best solution
for your deployment.

00:39:03.990 --> 00:39:08.900
So another benchmark,
business logic benchmark.

00:39:09.020 --> 00:39:14.390
C2, a little bit faster than C1.

00:39:15.870 --> 00:39:16.700
and Vellano Mark.

00:39:16.700 --> 00:39:20.580
This is a chat room with
a server and a client,

00:39:20.580 --> 00:39:22.690
lots of message sends.

00:39:22.790 --> 00:39:26.480
C1, fairly big win for C1.

00:39:26.570 --> 00:39:30.140
So again, measure your own applications.

00:39:30.160 --> 00:39:34.000
Don't just assume that one compiler
is going to win over the other.

00:39:34.590 --> 00:39:37.900
So, pretty exciting, we think.

00:39:37.930 --> 00:39:43.060
We get faster performance
with C2 on many applications.

00:39:43.110 --> 00:39:47.100
I know people who are in the server
space have been clamoring for this,

00:39:47.100 --> 00:39:50.490
and we expect it to work well.

00:39:51.700 --> 00:39:56.600
We want your feedback about all
of this stuff and other things.

00:39:56.600 --> 00:40:01.600
So later this afternoon, 3:30,
please come to the feedback forum.

00:40:01.970 --> 00:40:05.600
Tell us what you want,
what you think about Java.

00:40:05.680 --> 00:40:13.420
If you want to give us praise,
please make sure you're there.

00:40:13.920 --> 00:40:17.020
To contact, we have Alan Samuel,
who will be up here on stage

00:40:17.020 --> 00:40:19.220
in a little bit for some Q&A.

00:40:19.430 --> 00:40:24.290
And Francois Jouel,
who is the manager of the group,

00:40:24.520 --> 00:40:26.500
he'll be here as well.