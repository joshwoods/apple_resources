WEBVTT

00:00:08.290 --> 00:00:11.490
My name's David Lopatta,
and I'm here with Josh Durham.

00:00:11.750 --> 00:00:15.400
I work at Apple in the,
I'm an area consulting

00:00:15.400 --> 00:00:18.700
engineer based out of Atlanta,
and I focus on high performance computing

00:00:18.700 --> 00:00:20.320
and server and infrastructure stuff.

00:00:20.340 --> 00:00:24.400
And then Josh Durham is from
Virginia Tech and is the root user

00:00:24.400 --> 00:00:26.820
of the Terrascale cluster there.

00:00:26.820 --> 00:00:30.020
So he manages the
Virginia Tech Terrascale cluster.

00:00:30.920 --> 00:00:34.280
And today we're going to talk about,
the title I should say is

00:00:34.280 --> 00:00:35.800
a little bit incorrect.

00:00:35.800 --> 00:00:37.210
There's a couple of typos, so to speak.

00:00:37.210 --> 00:00:40.150
The word administration really should
be installation and deployment,

00:00:40.150 --> 00:00:43.130
and large installation
should probably be gone.

00:00:43.140 --> 00:00:46.920
So we're really going to focus
on high performance computing.

00:00:46.920 --> 00:00:51.070
A lot of what we talk about will apply
to large installations in general,

00:00:51.420 --> 00:00:55.180
particularly the first half where
I focus on the physical layouts,

00:00:55.180 --> 00:00:59.680
but we really are focusing on the
high performance computing clusters.

00:01:00.320 --> 00:01:02.930
So in terms of an agenda,
we're going to first sort

00:01:03.030 --> 00:01:06.460
of go over a computing 101,
cluster computing 101,

00:01:06.460 --> 00:01:10.550
and then talk about the things you
might do to architect an appropriate

00:01:10.620 --> 00:01:12.920
cluster for your specific problem.

00:01:12.920 --> 00:01:16.420
We'll go over some of the physical
deployment considerations,

00:01:16.570 --> 00:01:19.540
and then I will switch over the
balance of the time to Josh,

00:01:19.610 --> 00:01:23.380
who will talk about building a basic
cluster framework and getting the

00:01:23.380 --> 00:01:25.600
LINPACK and why that's important.

00:01:25.600 --> 00:01:29.190
We'll also talk about testing the
various cluster components to make sure

00:01:29.190 --> 00:01:32.720
that... the system is up and stable and
performing the way you would like it.

00:01:32.720 --> 00:01:36.870
We'll go into job scheduling
actually at that point,

00:01:36.980 --> 00:01:41.020
and then jump back to tweaking OS X for
performance and trying to eke the

00:01:41.020 --> 00:01:43.550
most performance out of the system.

00:01:43.960 --> 00:01:46.260
Again, I'll be covering the first
half specifically sort of

00:01:46.270 --> 00:01:47.720
the physical considerations.

00:01:47.720 --> 00:01:50.510
When we go through these things,
hopefully none of these things

00:01:50.510 --> 00:01:54.710
are earth shattering and things
you didn't really consider.

00:01:54.720 --> 00:01:58.720
I'm really hoping this is more
like enhanced common sense.

00:01:58.720 --> 00:01:59.120
Whereas you look at the other side,
you're going to see that the system

00:01:59.120 --> 00:01:59.120
is up and stable and performing
the way you would like it.

00:01:59.120 --> 00:02:00.480
You look at these things, you say,
"Oh yeah,

00:02:00.480 --> 00:02:03.710
I may not have thought about that,
but I know I should have." And then

00:02:03.710 --> 00:02:08.190
we'll provide that sort of as
a basis for creating a cluster.

00:02:08.290 --> 00:02:11.740
So to step back a little bit and go
over what cluster computing really

00:02:11.740 --> 00:02:15.640
means to us in this specific session,
we want to talk about, first off,

00:02:15.700 --> 00:02:18.110
what we're not going to cover,
and that's the available or scalable

00:02:18.110 --> 00:02:21.560
clusters that you might use in
an IT infrastructure environment.

00:02:21.560 --> 00:02:24.960
These will not be covered because
the specific things that we're

00:02:24.960 --> 00:02:29.080
bringing to the table here in terms
of techniques don't generally apply.

00:02:29.080 --> 00:02:33.830
And also, even in a large-scale,
high-availability or scalable cluster,

00:02:33.960 --> 00:02:37.850
the number of nodes we're talking
about aren't as large as you

00:02:37.850 --> 00:02:40.190
might see in an HPC environment.

00:02:40.480 --> 00:02:41.600
So again, high availability.

00:02:41.600 --> 00:02:44.240
We're not going to discuss this,
but just so you know what this is.

00:02:44.240 --> 00:02:47.170
This is basically where you
have two systems providing a

00:02:47.170 --> 00:02:54.440
redundant front end for a service,
whether it's an Oracle database or

00:02:54.440 --> 00:02:57.400
some other service that is business
critical to your environment.

00:02:57.400 --> 00:03:00.110
And the idea here is that we
eliminate single points of failure

00:03:00.110 --> 00:03:03.410
by introducing redundant systems
and networks with redundant paths

00:03:03.460 --> 00:03:04.900
and heartbeats to the environment.

00:03:04.900 --> 00:03:06.640
So pretty straightforward system.

00:03:06.800 --> 00:03:09.050
Tiger, as an OS,
has a lot of features built into it

00:03:09.050 --> 00:03:10.960
to allow for this kind of environment.

00:03:11.000 --> 00:03:13.810
So there are really exciting
things you can do with an HA kind

00:03:13.810 --> 00:03:15.520
of environment with Tiger.

00:03:15.520 --> 00:03:20.260
But again, this is not specifically
something we'll discuss today.

00:03:20.300 --> 00:03:22.520
And then the idea of scalable clusters.

00:03:22.520 --> 00:03:26.230
Again, things like Oracle Rack and also
web services that might have load

00:03:26.230 --> 00:03:28.500
balancers in the front where,
essentially,

00:03:28.500 --> 00:03:32.760
most of your data is static and can
be handled by a bunch of stateless

00:03:32.760 --> 00:03:35.520
devices or tied to a shared storage
environment in the back end.

00:03:35.670 --> 00:03:39.340
So some of the things that
we're going to talk about today,

00:03:39.340 --> 00:03:43.160
the features or the procedures we'll
be talking about today for deploying

00:03:43.160 --> 00:03:45.370
HPC clusters does apply here.

00:03:45.380 --> 00:03:47.600
But again, it's sort of a one-off.

00:03:47.610 --> 00:03:48.860
At the end, we will have Q&A.

00:03:48.990 --> 00:03:52.130
So while these are sort of off
limits from the slides perspective,

00:03:52.140 --> 00:03:53.940
if you have specific
questions about this,

00:03:53.940 --> 00:03:55.360
we'll be happy to entertain them.

00:03:55.360 --> 00:03:59.070
And we will answer every question,
provided you accept I don't

00:03:59.080 --> 00:04:00.880
know as a possible answer.

00:04:00.880 --> 00:04:02.950
And then we'll just move from there.

00:04:03.410 --> 00:04:04.910
So computational clusters.

00:04:05.010 --> 00:04:08.110
Basically, we break this down into two
types of computational clusters.

00:04:08.140 --> 00:04:12.260
The first is a traditional high
performance computing cluster.

00:04:12.420 --> 00:04:15.400
In this environment, really,
the computer is the solution.

00:04:15.400 --> 00:04:19.580
So folks that are involved in this
space are developing their own code.

00:04:19.650 --> 00:04:22.490
They're selecting an architecture
that is interesting or

00:04:22.490 --> 00:04:25.220
appropriate for that type of code.

00:04:25.220 --> 00:04:28.710
And then they're modeling and building
and designing their software around that.

00:04:28.800 --> 00:04:31.330
So they pick it based on a
hardware architecture or a

00:04:31.330 --> 00:04:33.700
performance characteristic
of a hardware architecture,

00:04:33.700 --> 00:04:35.860
and then build up from there.

00:04:35.900 --> 00:04:38.020
The second kind is the
solutions-based architecture.

00:04:38.020 --> 00:04:41.060
And we have a great story here
with the Apple Workgroup Cluster.

00:04:41.250 --> 00:04:45.360
And here, the software really defines
what we're trying to deliver.

00:04:45.360 --> 00:04:48.740
And it's really about time to
results and making sure that the

00:04:48.830 --> 00:04:52.210
software is robust and flexible
on the platform that you're using.

00:04:52.330 --> 00:04:53.960
So the software drives the solution.

00:04:53.960 --> 00:04:55.890
And from there,
you build out the appropriate

00:04:55.890 --> 00:05:00.490
cluster infrastructure that provides
the best experience for the user.

00:05:02.100 --> 00:05:04.630
So if we look again at sort of
the generalized needs of the

00:05:04.630 --> 00:05:07.690
high performance environment,
essentially good double precision

00:05:07.690 --> 00:05:08.990
floating point is critical.

00:05:09.080 --> 00:05:12.000
Traditionally here we also see
the high speed interconnect,

00:05:12.000 --> 00:05:15.000
so Miranet or InfiniBand become
very interesting in this space.

00:05:15.000 --> 00:05:19.390
And of course we need strong compilers,
because again the researchers

00:05:19.640 --> 00:05:22.000
here are actually developing
and designing their own code.

00:05:22.010 --> 00:05:26.250
So a strong compiler environment to
be able to bring good performance

00:05:26.250 --> 00:05:30.000
to the algorithms that are
being used here is essential.

00:05:30.000 --> 00:05:35.000
And this is what a typical layout might
look like for an HPC kind of environment.

00:05:35.000 --> 00:05:38.980
In the solutions-based environment,
essentially the cluster is a

00:05:39.090 --> 00:05:40.990
black box for submitting jobs.

00:05:41.000 --> 00:05:45.910
We have a good story here again with
the workroom cluster for bioinformatics.

00:05:46.210 --> 00:05:49.220
The idea here is that this is
designed to be a product and

00:05:49.220 --> 00:05:51.000
not necessarily a project.

00:05:51.000 --> 00:05:52.960
The back-end infrastructure
of the cluster becomes less

00:05:52.960 --> 00:05:55.890
important to the researcher,
and it's really about time to results,

00:05:55.890 --> 00:05:58.000
being able to plug
something into the wall,

00:05:58.020 --> 00:06:01.000
get your research done
as quickly as possible,

00:06:01.000 --> 00:06:04.000
and get the research done and submitted.

00:06:04.000 --> 00:06:07.360
And however we get that accomplished
on the back-end is really not of

00:06:07.360 --> 00:06:11.000
interest to the specific researcher.

00:06:12.400 --> 00:08:26.500
[Transcript missing]

00:08:31.970 --> 00:08:35.980
So, those are the types of
architectures that you might have.

00:08:35.980 --> 00:08:39.920
Then the key here is figuring out exactly
how you go about designing and building

00:08:39.920 --> 00:08:42.200
the cluster that's appropriate for you.

00:08:42.200 --> 00:08:46.680
So the key thing is to figure out
exactly what your goal is here.

00:08:46.680 --> 00:08:48.900
It may be a specific application,
for example.

00:08:48.900 --> 00:08:52.010
You may be building it
for a solution like Blast,

00:08:52.070 --> 00:08:55.620
or you may be trying to target a
specific performance benchmark.

00:08:55.620 --> 00:08:58.010
In this case,
we're going to discuss the HP LINPACK.

00:08:58.070 --> 00:09:00.000
Once you've got that,
you need to figure out exactly

00:09:00.000 --> 00:09:02.160
what kind of performance
you're attempting to achieve.

00:09:02.160 --> 00:09:05.630
So, it might be that you're
trying to hit 2.0 teraflops,

00:09:05.630 --> 00:09:08.050
again, using the LINPACK experience.

00:09:08.060 --> 00:09:11.250
Once that's done, you really need to pay
attention to exactly what you're

00:09:11.250 --> 00:09:12.540
going to do after the fact.

00:09:12.600 --> 00:09:16.230
There's a lot of times where the
cluster is and always is going

00:09:16.230 --> 00:09:19.850
to be a cluster environment,
but there's also times where you may

00:09:19.860 --> 00:09:21.680
need to do redeployment over time.

00:09:21.680 --> 00:09:24.580
So, it may be that a research project,
at the end of its life cycle,

00:09:24.580 --> 00:09:28.180
the systems may be... need to
be redeployed for other uses,

00:09:28.180 --> 00:09:32.590
whether that's infrastructure services or
maybe redeployed into a lab environment,

00:09:32.590 --> 00:09:32.960
etc.

00:09:33.020 --> 00:09:35.640
So, trying to figure out what you're
going to do tomorrow helps you decide

00:09:35.640 --> 00:09:38.940
what architecture that you're going
to use to build the cluster today.

00:09:38.940 --> 00:09:41.050
Once that's done,
that really gives you the

00:09:41.050 --> 00:09:43.920
opportunity to select what specific
platform that you would want.

00:09:43.920 --> 00:09:46.900
So, for example,
an XSERV G5 is an excellent system if

00:09:46.960 --> 00:09:48.380
you're going to have a dedicated cluster.

00:09:48.380 --> 00:09:52.320
If it's something that you're doing
a short-term research project that

00:09:52.320 --> 00:09:55.500
you're going to turn over later on,
you might also consider PowerMaker.

00:09:55.500 --> 00:09:59.350
PowerMaker is a system that's going to be
deployed into a lab environment later on

00:09:59.350 --> 00:10:01.470
if you want to be able to repurpose them.

00:10:01.500 --> 00:10:04.840
From there,
we have to figure out what storage needs

00:10:05.140 --> 00:10:07.500
are going to be used by the system.

00:10:07.500 --> 00:10:11.100
This generally happens by discussing
with your researchers exactly the kind

00:10:11.100 --> 00:10:14.060
of storage needs they're going to have,
whether it can be dedicated

00:10:14.060 --> 00:10:18.200
storage on a single shared system,
whether or not they need

00:10:18.340 --> 00:10:20.370
high-speed access using something
like XSAN to communicate

00:10:20.370 --> 00:10:22.510
to a bunch of high-speed nodes,
or whether they need a more

00:10:22.510 --> 00:10:24.750
distributed environment with
lots of local scratch space,

00:10:24.750 --> 00:10:25.380
for example.

00:10:25.380 --> 00:10:26.550
Thank you.

00:10:26.670 --> 00:10:29.220
The next thing is to figure out, well,
what are we going to actually use to

00:10:29.220 --> 00:10:30.590
tie this thing together as an image?

00:10:30.730 --> 00:10:32.400
So we may be using MPI.

00:10:32.400 --> 00:10:35.860
There's been a lot of discussions
about OpenMPI here at the show,

00:10:35.860 --> 00:10:39.240
and there's a variety of MPIs that
are available for Mac OS X.

00:10:39.240 --> 00:10:42.880
We may be creating a PVM environment,
or we may be using something like

00:10:42.930 --> 00:10:46.800
XGrid's BEEP protocol to exchange
information across the network.

00:10:46.850 --> 00:10:50.620
Once we've figured that out,
the next key thing is to figure out,

00:10:50.620 --> 00:10:51.700
well, what the interconnect is.

00:10:51.700 --> 00:10:54.070
And this ends up being
extremely important,

00:10:54.070 --> 00:10:57.150
because the interconnect can
be a very large percentage of

00:10:57.330 --> 00:10:59.320
the actual cost of the system.

00:10:59.410 --> 00:11:03.600
So we want to take care to make
sure that we select this wisely.

00:11:03.640 --> 00:11:06.570
When we look at the interconnect itself,
there are a few deciding factors.

00:11:06.630 --> 00:11:10.200
Basically, it's the old price performance
decision that we have to figure out.

00:11:10.240 --> 00:11:12.860
Gigabit Ethernet is by
far the cheapest solution.

00:11:12.860 --> 00:11:17.350
The XServe G5 ships with two
gigabit ports on the motherboard,

00:11:17.460 --> 00:11:19.690
so it's a very convenient
way to create two networks,

00:11:19.800 --> 00:11:22.160
one dedicated for the
communications network,

00:11:22.180 --> 00:11:24.620
and then a second network
for back-end services.

00:11:24.720 --> 00:11:26.680
So you don't need any
additional HBAs for that,

00:11:26.690 --> 00:11:28.280
and it's just there out of the box.

00:11:28.480 --> 00:11:32.080
In terms of performance,
when you're looking at needing

00:11:32.080 --> 00:11:35.250
a low-latency interconnect,
both MirrorNet and InfiniBand provide

00:11:35.250 --> 00:11:37.930
very low latencies versus Ethernet.

00:11:38.030 --> 00:11:40.880
So those are the two candidates on
OS X that you would generally be looking

00:11:40.880 --> 00:11:42.570
for for a low-latency interconnect.

00:11:42.580 --> 00:11:45.910
In terms of throughput,
MirrorNet advertises about 900 megabytes

00:11:46.030 --> 00:11:49.560
per second with dual-ported HBAs,
where InfiniBand can scale up

00:11:49.620 --> 00:11:53.780
to 10 gigabits per second per
port on a dual-ported HBA.

00:11:53.780 --> 00:11:57.240
And I mentioned there, of course,
that exceeds the PCI-X specification

00:11:57.620 --> 00:12:00.750
that we ship on the XServe,
but it does create a nice

00:12:01.120 --> 00:12:02.540
future-proof interface.

00:12:02.580 --> 00:12:04.930
In terms of what's available on OS X,
both are available and

00:12:04.930 --> 00:12:07.940
very robust on OS X,
and we have a growing number

00:12:07.940 --> 00:12:09.440
of deployments in each.

00:12:09.590 --> 00:12:12.310
Currently,
MirrorNet tends to be the leader in

00:12:12.310 --> 00:12:15.840
terms of the numbers of systems that
we've currently shipped and are deployed,

00:12:15.910 --> 00:12:18.840
and is probably the most popular
high-speed interconnect on that

00:12:18.840 --> 00:12:22.690
top 500 list as it exists today.

00:12:23.560 --> 00:12:26.140
So again,
what we're going into now is the

00:12:26.270 --> 00:12:28.960
physical deployment considerations,
and hopefully this ends up being

00:12:28.960 --> 00:12:30.870
essentially enhanced common sense.

00:12:30.880 --> 00:12:33.280
Hopefully there's nothing
really earth-shattering here,

00:12:33.280 --> 00:12:34.850
but we want to cover
these kinds of things,

00:12:34.850 --> 00:12:39.960
because when we go through with customers
and walk through the deployment cycle,

00:12:39.960 --> 00:12:43.140
this is where the biggest
pains come from our customers.

00:12:43.140 --> 00:12:47.150
The physical considerations that tend to
either get ignored or overlooked during

00:12:47.150 --> 00:12:51.320
the purchasing cycle cause the biggest
headaches for our customers long-term.

00:12:52.810 --> 00:12:55.380
So in terms of racks,
the XSERV can support either

00:12:55.380 --> 00:12:57.570
a two or four post rack.

00:12:57.710 --> 00:12:59.770
We strongly recommend
a four post cabinet,

00:12:59.770 --> 00:13:00.320
of course.

00:13:00.540 --> 00:13:04.830
The XSERV RAID is only
available-- you can only configure

00:13:04.830 --> 00:13:06.040
that into a four post rack.

00:13:06.100 --> 00:13:08.510
You'll notice that the
sizes for the various racks,

00:13:08.660 --> 00:13:11.550
the depths,
actually do not exactly overlap

00:13:11.550 --> 00:13:14.100
for your XSERV RAID and your XSERV.

00:13:14.210 --> 00:13:17.410
So you want to make sure that if
you're building out an infrastructure,

00:13:17.490 --> 00:13:20.070
you're getting the appropriately
sized racks for your

00:13:20.070 --> 00:13:22.520
XSERV RAID as well as your XSERV.

00:13:23.640 --> 00:13:27.700
In terms of mount order,
if you're using a fairly small cluster,

00:13:27.700 --> 00:13:30.370
so you've got maybe one or two racks,
you want to make sure your UPSs

00:13:30.380 --> 00:13:33.160
are physically on the bottom
of the rack just because of the

00:13:33.160 --> 00:13:36.040
physical weight considerations,
and then you'd put the

00:13:36.040 --> 00:13:37.350
XRV raids on top of that.

00:13:37.470 --> 00:13:41.070
That alone will save you a lot
of pain and suffering in terms of

00:13:41.070 --> 00:13:43.440
back injuries and things like that.

00:13:43.500 --> 00:13:48.800
Then, basically, on the top of the rack,
you'll basically start adding your

00:13:48.800 --> 00:13:53.360
cluster nodes and your head node
from the top down to the middle.

00:13:53.500 --> 00:13:56.220
All your infrastructure hardware,
your gigabit Ethernet,

00:13:56.220 --> 00:13:58.860
etc., would go essentially
in the center of the rack.

00:13:58.900 --> 00:14:02.080
For larger deployments,
we would recommend considering

00:14:02.080 --> 00:14:05.740
a center rack that has basically
your infrastructure services,

00:14:05.740 --> 00:14:09.460
your gigabit ports,
your gigabit switches,

00:14:09.460 --> 00:14:10.940
and your marionette switches,
and things like that,

00:14:10.980 --> 00:14:13.690
and then fan out the cluster nodes
on the sides of that because as

00:14:13.690 --> 00:14:16.440
you're building this infrastructure,
particularly with a

00:14:16.530 --> 00:14:19.180
high-speed interconnect like
marionette or InfiniBand,

00:14:19.180 --> 00:14:22.280
the cost of the cables are directly
related to the length of the cable.

00:14:22.420 --> 00:14:26.670
So having a central point where these
feed into reduces the cost by lowering

00:14:26.830 --> 00:14:29.080
the physical length of the cables.

00:14:29.120 --> 00:14:33.730
In terms of tools and parts,
you want a lot of batteries

00:14:33.730 --> 00:14:36.790
for your screwdrivers.

00:14:36.800 --> 00:14:38.500
You're putting in a lot of systems.

00:14:38.500 --> 00:14:41.030
Generally,
you do need two people installing this,

00:14:41.030 --> 00:14:43.420
one on the back of the system,
one on the front.

00:14:43.420 --> 00:14:46.380
You'll find you can do an XRV raid
in just a couple minutes if

00:14:46.380 --> 00:14:49.720
you've got two people doing
it and you get into a good rhythm.

00:14:49.720 --> 00:14:52.290
It's a pretty straightforward process.

00:14:52.440 --> 00:14:56.060
But it's fairly something
you've just got to get used to.

00:14:56.060 --> 00:14:59.150
It becomes more of an art than
a science in many respects.

00:14:59.200 --> 00:15:03.730
One thing that we run into a lot
is in terms of which screws to use.

00:15:03.760 --> 00:15:06.100
I know this sounds really simplistic,
but you've got a bunch

00:15:06.140 --> 00:15:07.260
of bags of screws around.

00:15:07.260 --> 00:15:08.380
Some of them are metric.

00:15:08.380 --> 00:15:10.500
Some of them are English measure.

00:15:10.500 --> 00:15:12.160
Make sure you're using the right screws.

00:15:12.160 --> 00:15:15.730
So if you're attaching anything
physically to the XRV or the XRV raid,

00:15:15.730 --> 00:15:17.560
use the screws that we provide.

00:15:17.560 --> 00:15:19.910
If you're attaching anything
to the actual racks,

00:15:19.910 --> 00:15:22.300
make sure you're using
the rack manufacturers.

00:15:22.420 --> 00:15:25.320
And that will save you
a lot of headaches.

00:15:25.360 --> 00:15:27.800
And again,
it's sort of enhanced common sense,

00:15:27.890 --> 00:15:31.250
but we have a lot of folks who sort
of overlook that and have a lot

00:15:31.260 --> 00:15:33.730
of headaches associated with that.

00:15:34.680 --> 00:15:38.480
In terms of power and environmentals,
this is again probably the number one

00:15:38.480 --> 00:15:39.880
issue we have in deploying a cluster.

00:15:39.880 --> 00:15:43.320
Folks will buy a large,
reasonably sized cluster and then

00:15:43.320 --> 00:15:47.420
attempt to deploy it in a closet with
no heating or cooling consideration.

00:15:47.440 --> 00:15:49.160
These are the basic numbers.

00:15:49.160 --> 00:15:54.240
The K bases at the top are kept up to
date as we provide various releases,

00:15:54.240 --> 00:15:55.520
the XSERV and the XSERV RAID.

00:15:55.520 --> 00:15:58.790
And I won't go over the numbers directly,
but you can see sort

00:15:58.790 --> 00:16:00.140
of how they progress.

00:16:00.250 --> 00:16:03.970
And the XSERV RAID obviously pulls a lot
of power and generates a lot of BTUs.

00:16:04.600 --> 00:16:07.260
If you look at that in some
typical cluster configurations,

00:16:07.260 --> 00:16:08.870
these are estimates.

00:16:08.880 --> 00:16:13.090
If you're planning on deploying
a 32 or 64 node cluster,

00:16:13.090 --> 00:16:15.480
please do not use these as gospel.

00:16:15.480 --> 00:16:16.800
Make sure you measure it yourself.

00:16:16.800 --> 00:16:21.890
But this is the kind of power pull
and BTUs that we're looking at.

00:16:21.940 --> 00:16:25.820
The 1110 node cluster here is, of course,
the Virginia Tech TerraScale cluster.

00:16:25.820 --> 00:16:33.810
So 310 kilowatts and just under
2 million BTUs in terms of heat.

00:16:34.600 --> 00:16:37.410
So again, you want to keep this in mind
because as you add nodes,

00:16:37.410 --> 00:16:39.600
this gets to be a pretty
significant number.

00:16:42.260 --> 00:16:45.520
And with that, I'm very, very early.

00:16:45.640 --> 00:16:49.300
I'm going to be turning this over
to Josh from Virginia Tech to

00:16:49.450 --> 00:16:52.030
cover the software aspects.

00:16:52.160 --> 00:16:52.470
Great.

00:16:52.700 --> 00:16:54.380
Thanks, Dave.

00:16:57.320 --> 00:17:00.490
So I'm going to go over some of the
software side of things on what's

00:17:00.560 --> 00:17:01.660
involved in setting up a cluster.

00:17:01.660 --> 00:17:04.820
And we're going to go
over some different terms.

00:17:04.820 --> 00:17:08.080
There's different kind of roles that
we have of systems in a cluster.

00:17:08.080 --> 00:17:10.960
And we're going to start out with
the service and head node setup.

00:17:11.700 --> 00:17:16.300
So a service node in a cluster is
basically providing infrastructure.

00:17:16.300 --> 00:17:21.470
It provides different services like DHCP,
DNS, LDAP, the common storage.

00:17:21.500 --> 00:17:24.180
Basically all the things that
the compute nodes are going

00:17:24.180 --> 00:17:27.380
to need to be able to run,
the service nodes are going to provide.

00:17:27.380 --> 00:17:30.970
This can be one system,
or if you have a larger cluster,

00:17:30.970 --> 00:17:34.560
maybe larger than 32,
you might want to think about

00:17:34.560 --> 00:17:37.160
putting this onto multiple systems.

00:17:39.820 --> 00:17:43.000
So the Service Node Setup,
the first thing we have to do

00:17:43.080 --> 00:17:44.890
obviously is install the OS.

00:17:45.080 --> 00:17:48.650
I recommend installing basically
the full OS with everything added.

00:17:48.780 --> 00:17:51.680
That way if later you find
out you need something,

00:17:51.770 --> 00:17:54.670
it's already there,
so nothing's going to be missing.

00:17:54.710 --> 00:17:58.490
So install the base OS,
install all the developer tools,

00:17:58.490 --> 00:18:02.700
and basically just put everything
on that you get in the box.

00:18:02.700 --> 00:18:04.700
There's different ways of doing this.

00:18:04.700 --> 00:18:08.450
One of the problems in clusters is
that often the systems won't have a

00:18:08.450 --> 00:18:10.700
video display or a keyboard or mouse.

00:18:10.700 --> 00:18:13.700
So if you are lucky enough to have that,
you can do your standard install.

00:18:13.700 --> 00:18:17.970
You boot from the CD, stick it in,
just go through the standard

00:18:17.970 --> 00:18:19.690
install like normal.

00:18:19.690 --> 00:18:22.690
If you can't do that,
you have a couple of other options.

00:18:22.780 --> 00:18:27.250
What you can do is you can stick the
XServe in a target FireWire mode,

00:18:27.330 --> 00:18:30.040
get a laptop,
and basically use your laptop

00:18:30.240 --> 00:18:32.690
to install onto that hard drive.

00:18:32.690 --> 00:18:33.700
And that works out pretty well.

00:18:33.700 --> 00:18:41.690
A third thing you can do is do remotely
with the CD and server assistant.

00:18:41.700 --> 00:18:44.790
So what server assistant will let
you do is basically over the network,

00:18:44.820 --> 00:18:47.700
act like you're doing a GUI install,
but you're actually installing

00:18:47.700 --> 00:18:50.680
a system across the network.

00:18:50.750 --> 00:18:54.460
And finally you do a network install,
which is basically set up a system,

00:18:54.460 --> 00:18:57.100
have all the services on it,
and then you deploy that

00:18:57.140 --> 00:18:58.700
image across the network.

00:18:58.700 --> 00:19:01.230
And we'll go into that in a little bit.

00:19:02.660 --> 00:19:05.590
So we're going to go over some of
the services that I think are very

00:19:05.590 --> 00:19:10.390
important for installing a cluster
and for these infrastructure servers.

00:19:10.400 --> 00:19:12.250
The first one is DHCP.

00:19:12.380 --> 00:19:16.520
So DHCP, most of you know,
is a way of dynamically allocating

00:19:16.520 --> 00:19:18.370
IP addresses to systems.

00:19:18.380 --> 00:19:22.300
And we're going to use this to allocate
addresses to the cluster nodes.

00:19:22.340 --> 00:19:24.760
Well,
we don't want it to be quite dynamic,

00:19:24.760 --> 00:19:28.220
because the problem we're going to
have is that if you don't know which

00:19:28.230 --> 00:19:31.060
cluster node has which IP address,
and you have a problem

00:19:31.060 --> 00:19:33.410
with that cluster node,
it's going to be a lot harder

00:19:33.410 --> 00:19:34.970
to find and diagnose an issue.

00:19:35.020 --> 00:19:37.970
So if you have a system that
might be acting unusual or weird,

00:19:37.970 --> 00:19:41.040
you want to make sure that the
third system down has the third

00:19:41.040 --> 00:19:42.460
IP address that you've assigned.

00:19:42.460 --> 00:19:45.820
Otherwise, it could be the fifth,
and you'll spend some time just

00:19:45.820 --> 00:19:49.200
trying to figure out which system
really is causing the problem.

00:19:49.200 --> 00:19:52.300
So obviously,
this is set up through server admin,

00:19:52.300 --> 00:19:54.720
which is part of the OS X server tools.

00:19:56.720 --> 00:20:00.600
The second service
that we provide is DNS.

00:20:00.600 --> 00:20:04.300
And DNS is -- DNS and DHCP I kind
of put in the same basket,

00:20:04.300 --> 00:20:07.800
because DHCP will give
out the IP addresses,

00:20:07.800 --> 00:20:11.770
and then DNS lets the
infrastructure nodes,

00:20:11.880 --> 00:20:19.310
the service nodes,
and the head nodes basically find

00:20:19.310 --> 00:20:19.310
each other and work together.

00:20:19.500 --> 00:20:22.640
So through our experiences,
we found with the original System 10,

00:20:22.640 --> 00:20:26.380
when we did the PowerMax,
we had this node name number

00:20:26.450 --> 00:20:29.280
of N0001 through N1100.

00:20:29.280 --> 00:20:31.570
And it turned out to be
a scripting nightmare,

00:20:31.580 --> 00:20:35.580
because we'd have to
pad out the node name,

00:20:35.580 --> 00:20:38.440
and regular expressions
just wouldn't work.

00:20:38.480 --> 00:20:42.480
So when we did the second
System 10 with the XSERVs,

00:20:42.480 --> 00:20:47.320
we switched this naming scheme of
basically node and then just the number.

00:20:47.580 --> 00:20:52.310
So don't try to pad it
with the zeros there.

00:20:52.320 --> 00:20:55.500
And we always get into this debate,
the computer scientists at

00:20:55.500 --> 00:20:58.660
Virginia Tech want to start
with the node number being zero.

00:20:58.660 --> 00:21:02.300
And the system administrators
wanted to start out with one.

00:21:02.300 --> 00:21:05.890
And so I finally won out on that one,
because I was the guy who got to do it.

00:21:07.420 --> 00:21:12.030
Again, this is set up using Server Admin.

00:21:12.250 --> 00:21:14.400
Pretty straightforward.

00:21:14.400 --> 00:21:17.390
The only thing that you need to make
sure of is that the IP addresses

00:21:17.390 --> 00:21:21.670
and the names here correspond
to what you also did in DHCP.

00:21:22.970 --> 00:21:26.800
So shared storage,
this has kind of been an issue

00:21:26.800 --> 00:21:32.420
of debate on the cluster stuff,
with how much it can scale in

00:21:32.500 --> 00:21:34.930
OS X and which one you should use.

00:21:34.960 --> 00:21:38.530
So at Virginia Tech, we use NFS,
and we actually do

00:21:38.540 --> 00:21:41.240
have all 1,100 systems.

00:21:41.240 --> 00:21:47.440
Actually, each NFS server has all
1,100 systems attached to it.

00:21:47.460 --> 00:21:50.100
So definitely no problem scaling there.

00:21:51.280 --> 00:21:53.050
And I think we've been
doing that since 10.3.6,

00:21:53.050 --> 00:21:55.760
so we haven't even looked at
how to do that with Tiger yet.

00:21:57.240 --> 00:22:00.190
But with NFS,
it's going to basically provide this

00:22:00.200 --> 00:22:03.040
common store across the entire cluster.

00:22:03.060 --> 00:22:07.240
And what that will let us do is that,
instead of like before,

00:22:07.240 --> 00:22:10.560
if you saw the XGrid demonstration,
when you submit something to XGrid,

00:22:10.560 --> 00:22:13.130
XGrid handles taking those jobs,
submitting it to all the nodes,

00:22:13.130 --> 00:22:17.380
and moving the files to those nodes,
the nodes work with them,

00:22:17.380 --> 00:22:21.420
things get moved back,
and then downloaded to the agent.

00:22:21.860 --> 00:22:25.260
Well, we don't need to do that,
because we're going to do a shared NFS.

00:22:25.260 --> 00:22:28.230
So instead of having to transfer
these files back and forth,

00:22:28.300 --> 00:22:31.380
the files are already directly
accessible from each node.

00:22:33.250 --> 00:22:37.360
So before I said that on larger systems
you want to kind of break out the

00:22:37.450 --> 00:22:41.960
roles that each service node does,
an NFS is probably the first one you'll

00:22:41.960 --> 00:22:46.610
want to break out because it's probably
the most demanding of all the roles.

00:22:46.730 --> 00:22:51.310
Again, this is sort of using the
OS X server tool work group manager.

00:22:51.420 --> 00:22:56.900
Basically just set up an NFS export
and then the clients can pick that up.

00:22:58.090 --> 00:23:03.190
Finally, the last service I'm going to
talk about is Open Directory.

00:23:03.240 --> 00:23:08.560
Open Directory allows basically a common
user view across the entire cluster.

00:23:08.740 --> 00:23:13.380
And this is important because
you want each compute node

00:23:14.050 --> 00:23:18.320
to-- so when you submit a job,
and you want all the compute nodes

00:23:18.400 --> 00:23:22.860
to have the same user ID and have
the same access control stuff that

00:23:22.860 --> 00:23:26.300
is common with the local interface.

00:23:26.390 --> 00:23:28.070
So.

00:23:28.300 --> 00:23:29.440
We use Open Directory.

00:23:29.440 --> 00:23:32.340
What I love about Open Directory,
and we do use this at Virginia Tech,

00:23:32.400 --> 00:23:37.700
is it really does take about 10 or
15 seconds to create a new user.

00:23:37.700 --> 00:23:40.140
So it's very easy, very quick.

00:23:40.240 --> 00:23:44.200
You do it in one place,
and then you get the users, basically,

00:23:44.200 --> 00:23:48.740
users can access all the
systems once you've done that.

00:23:49.720 --> 00:23:53.420
This is done using Workgroup Manager.

00:23:53.510 --> 00:23:55.830
You do set up using Server Admin,
and then you do the user

00:23:55.830 --> 00:23:58.140
maintenance in Workgroup Manager.

00:24:01.770 --> 00:24:05.200
Another thing that you can do
is do the system monitoring,

00:24:05.200 --> 00:24:07.930
also from a service node.

00:24:08.070 --> 00:24:12.700
So I think a lot of people have probably
seen the XServe system monitoring.

00:24:12.700 --> 00:24:15.930
One thing I have to note is that you
really need to have an XServe or an

00:24:15.930 --> 00:24:17.420
XServe ray to do system monitoring.

00:24:17.420 --> 00:24:21.510
The hardware is just designed to
provide a lot more information.

00:24:21.520 --> 00:24:26.900
It provides all the thermal information,
all the CPU power usage, for example.

00:24:26.900 --> 00:24:30.000
I can't even remember how many
different metrics there are.

00:24:30.000 --> 00:24:30.770
I think it was over 22.

00:24:31.710 --> 00:24:34.590
Obviously, you can't get that if you're
trying to install a cluster

00:24:34.590 --> 00:24:35.730
on a Mac mini or something.

00:24:35.850 --> 00:24:39.520
I don't think it provides
all that information.

00:24:40.030 --> 00:24:44.090
So what's great about the server monitor
is that you can set it up to email out

00:24:44.100 --> 00:24:47.030
if there's something on certain events.

00:24:47.030 --> 00:24:52.660
So you can say, if a CPU temperature gets
out of a certain boundary,

00:24:52.660 --> 00:24:54.100
go in and email someone.

00:24:54.100 --> 00:24:57.600
So you can have a system
administrator or have someone get

00:24:57.600 --> 00:24:59.920
paged or an SMS type thing happen.

00:24:59.920 --> 00:25:05.890
And that's really helped us
integrate into our call center.

00:25:05.910 --> 00:25:09.890
We have a 24-hour call center at
Virginia Tech separate from the system.

00:25:09.890 --> 00:25:16.090
And that's really helped us be able to
basically monitor the system after hours.

00:25:17.880 --> 00:25:20.320
So that was the service nodes.

00:25:20.400 --> 00:25:22.490
What we're going to talk
about next is the head node.

00:25:22.600 --> 00:25:25.780
Head node's a little bit different
because it's basically what the

00:25:25.830 --> 00:25:27.300
users have access to directly.

00:25:27.300 --> 00:25:31.420
It's probably the only systems they
should be having access to directly.

00:25:31.520 --> 00:25:35.160
The service nodes are kind of more of
the system administration type stuff,

00:25:35.160 --> 00:25:37.830
and you don't usually have
users logging into that.

00:25:37.900 --> 00:25:40.050
But the head nodes basically
provide this bridge,

00:25:40.060 --> 00:25:43.830
or this portal,
that allows people from the external

00:25:43.830 --> 00:25:48.350
network to get in and use the
resources of the internal network.

00:25:49.010 --> 00:25:54.000
So it really should be the only
system that is on both networks.

00:25:54.000 --> 00:25:58.000
Most of the times in the
common cluster setup,

00:25:58.000 --> 00:26:00.000
people will be using SSH to log in.

00:26:00.000 --> 00:26:04.990
They can use SCP to move files
back and forth from the cluster.

00:26:05.240 --> 00:26:10.080
So on the head node,
users will often compile their

00:26:10.170 --> 00:26:13.600
programs and they will run their
codes directly from the head node.

00:26:13.600 --> 00:26:19.200
So the head node's in charge of
getting the jobs out so people

00:26:19.270 --> 00:26:22.350
can run on the entire cluster.

00:26:22.760 --> 00:26:26.630
And again, on small clusters,
maybe smaller than 32,

00:26:26.660 --> 00:26:31.490
it might not make sense to break
this out into a separate system,

00:26:31.490 --> 00:26:35.720
just because it's kind of a waste
of a system on a small cluster to

00:26:35.720 --> 00:26:39.240
have head nodes and service nodes,
and then all your computational nodes.

00:26:43.650 --> 00:26:47.150
With the compute nodes,
I'm going to talk a little bit about

00:26:47.210 --> 00:26:52.190
how to deploy that to your cluster,
how to deploy a common

00:26:52.190 --> 00:26:54.790
image across your cluster.

00:26:55.400 --> 00:26:58.790
Basically, a compute node is the
workhorse of your cluster,

00:26:58.790 --> 00:27:03.480
so you shouldn't be doing anything
but running user codes on this.

00:27:03.610 --> 00:27:06.570
It shouldn't be doing any
other services or tasks,

00:27:06.680 --> 00:27:10.450
so all the services that you
don't need should be turned off.

00:27:10.640 --> 00:27:13.980
So basically,
we start with a basic image.

00:27:15.340 --> 00:27:20.300
And we may install the developer
tools on that basic image.

00:27:20.370 --> 00:27:24.030
And we configure it with
DHCP on the first interface,

00:27:24.370 --> 00:27:26.100
because when you want the
cluster nodes to come up,

00:27:26.100 --> 00:27:30.000
you obviously want them to use the
infrastructure that we just talked about.

00:27:30.100 --> 00:27:32.970
And again,
I should mention that the easiest way

00:27:32.970 --> 00:27:37.100
to probably do this is to kind of set up
one system exactly the way you want it.

00:27:37.150 --> 00:27:39.100
So again, with basic image and DHCP.

00:27:39.100 --> 00:27:41.100
And then we're going
to install an SSH key,

00:27:41.100 --> 00:27:45.010
which is going to allow us
to do passwordless login.

00:27:45.100 --> 00:27:48.500
And that's going to be really important,
because when you want to run

00:27:48.570 --> 00:27:49.970
something across the entire cluster,
you don't want to be

00:27:49.970 --> 00:27:54.100
prompted for a password 16,
15, or 1100 times.

00:27:54.400 --> 00:27:57.240
That would take a lot of time.

00:27:57.500 --> 00:28:01.230
And then you want to make sure that
the cluster nodes are going to be using

00:28:01.460 --> 00:28:03.610
all the services we had just set up.

00:28:03.640 --> 00:28:07.850
And finally, something I like to do,
because I don't like to mess

00:28:07.940 --> 00:28:10.350
with the compute nodes too
much after I've set them up,

00:28:10.410 --> 00:28:13.590
I don't like to log in and
change scripts and modify things,

00:28:13.660 --> 00:28:17.650
is we create a compute node
startup script that's kind of

00:28:17.670 --> 00:28:22.700
going to refer to a centralized
directory and a centralized script.

00:28:22.750 --> 00:28:26.100
And I have an example of one
coming up here in a slide or two.

00:28:29.230 --> 00:28:35.680
So how do we get that
image onto the cluster?

00:28:36.140 --> 00:28:37.920
Sorry, skipped ahead.

00:28:38.060 --> 00:28:41.480
Basically, we need to, on the image,
this is kind of what we

00:28:41.560 --> 00:28:44.100
were talking about before,
we need to install the Server OS,

00:28:44.230 --> 00:28:45.010
developer tools.

00:28:45.040 --> 00:28:48.140
Don't enable any services
except for maybe ARD.

00:28:48.140 --> 00:28:51.420
If you're uncomfortable with
doing SSH to manage your systems,

00:28:51.420 --> 00:28:53.710
ARD is a very good way to do it.

00:28:53.720 --> 00:28:56.340
And I think there's an
ARD session tomorrow if you want

00:28:56.360 --> 00:28:58.000
more information about that.

00:28:58.040 --> 00:29:02.470
You want to make sure the image
has all the updates that are

00:29:02.470 --> 00:29:04.150
relevant for your situation.

00:29:04.930 --> 00:29:07.800
And you want to make sure that the image
has any software that needs to be local.

00:29:07.800 --> 00:29:10.990
So any libraries such as

00:29:11.330 --> 00:29:16.340
As MPI, or maybe your drivers like
Marinette GM or InfiniBand,

00:29:16.630 --> 00:29:18.140
they all need to be local.

00:29:18.160 --> 00:29:23.700
Also, I know in the past,
like if you installed IBM's XLF,

00:29:23.700 --> 00:29:26.360
you had to install certain
libraries on each system as well.

00:29:26.360 --> 00:29:30.130
I think it was their runtime
library or something like that.

00:29:30.150 --> 00:29:34.970
So basically, you set up the image
exactly the way you want it.

00:29:36.740 --> 00:29:40.600
This is the startup script
that I was referring to before.

00:29:40.640 --> 00:29:42.380
We basically want to
achieve two things here.

00:29:42.380 --> 00:29:45.930
We want to mount the NFS server,
and then we want to run the

00:29:46.270 --> 00:29:49.300
centralized script that I talked about.

00:29:49.430 --> 00:29:55.700
So the centralized script is sitting on
the NFS server under ETSI nodes.start.

00:29:55.700 --> 00:30:00.530
And so the reason we do that is,
let's say we want to install or start up

00:30:00.540 --> 00:30:03.040
InfiniBand or across the entire cluster.

00:30:03.040 --> 00:30:05.280
Well,
instead of going in and logging into each

00:30:05.280 --> 00:30:08.660
system and modifying the local start,
that's important later because

00:30:08.660 --> 00:30:11.300
let's say you wanted to try
a new InfiniBand driver out,

00:30:11.300 --> 00:30:13.370
but you didn't want to
toss the old one out.

00:30:13.370 --> 00:30:16.150
All you've got to do is put this
again in a central location,

00:30:16.220 --> 00:30:19.300
update this one script,
and reboot your entire cluster.

00:30:21.900 --> 00:30:24.460
Alright,
so now we're going to talk about imaging.

00:30:24.500 --> 00:30:25.760
There's a few different ways to do it.

00:30:25.910 --> 00:30:28.670
So we have the golden image,
and the golden image is set

00:30:28.710 --> 00:30:31.180
up exactly how we want it,
and it looks exactly how a

00:30:31.180 --> 00:30:32.830
cluster node is going to be.

00:30:32.840 --> 00:30:36.230
We want to first make a
disk image out of that,

00:30:36.260 --> 00:30:37.400
and I use ASR.

00:30:37.400 --> 00:30:42.420
You can also use the disk utility
that comes with every Apple system

00:30:42.420 --> 00:30:44.980
to basically make a disk image.

00:30:46.020 --> 00:30:48.590
Easiest way to do that is
basically to stick the golden

00:30:48.590 --> 00:30:51.940
client into the target FireWire,
and from another system

00:30:51.940 --> 00:30:56.170
like a laptop or a desktop,
you can then just create

00:30:56.170 --> 00:30:58.180
a disk image from that.

00:30:58.650 --> 00:31:01.920
An application we like to use,
and we use a lot with the

00:31:02.040 --> 00:31:06.940
first cluster especially,
is NetRestore, which Mike Baumbach wrote.

00:31:06.940 --> 00:31:10.440
And what's great about
NetRestore is it lets you drop a

00:31:10.480 --> 00:31:16.550
cluster image over the network,
and it actually has full automation

00:31:16.550 --> 00:31:22.230
support so that you can just say,
do a net boot and walk away,

00:31:22.230 --> 00:31:23.870
and it should drop the image on
the server and reboot the server.

00:31:24.100 --> 00:31:26.600
So then the only real step there is
that you have to make sure that the

00:31:26.600 --> 00:31:30.300
server boots up and does a net boot,
which is you can modify the lights on

00:31:30.360 --> 00:31:31.820
the front when you're first doing it.

00:31:31.820 --> 00:31:34.900
I'm not sure if you've done that,
but the XSurf has a way of basically,

00:31:34.900 --> 00:31:37.370
right as you turn it on, you can say,
I don't want to boot

00:31:37.370 --> 00:31:39.610
from the local drive,
it's going to boot over the network.

00:31:39.610 --> 00:31:43.220
It's similar to like holding down
the N key on a PowerBook or a

00:31:43.220 --> 00:31:45.470
PowerMac or something like that.

00:31:45.520 --> 00:31:48.340
So you can do that completely
on the front without having to

00:31:48.340 --> 00:31:50.180
have a keyboard or a video thing.

00:31:50.180 --> 00:31:53.350
And that works out really well.

00:31:54.000 --> 00:31:56.890
And that's how we did
a lot of deployments.

00:31:57.270 --> 00:31:59.700
Finally,
you can do the sneak-it-up method.

00:31:59.710 --> 00:32:04.550
So what I love about the XSERV is
it's got the nice pop-out hard drives.

00:32:04.560 --> 00:32:08.400
So you can just pop out this hard drive
and walk over to an imaging system,

00:32:08.400 --> 00:32:12.140
stick the hard drive in,
and just image that drive.

00:32:12.140 --> 00:32:16.070
Another great thing about the XSERV is
that the drives are hot-swappable,

00:32:16.090 --> 00:32:18.590
so you don't have to power
down the imaging XSERV,

00:32:18.670 --> 00:32:22.020
stick the drive in, power it up, image,
and then recycle again.

00:32:22.020 --> 00:32:25.190
You can just go in, pop it in,
it automatically mounts,

00:32:25.190 --> 00:32:26.640
and then you can ASR it.

00:32:30.800 --> 00:32:34.950
Alright, so after imaging,
we have the golden image

00:32:35.000 --> 00:32:37.460
on all the cluster nodes.

00:32:37.460 --> 00:32:41.200
We have all of our
infrastructure services set up,

00:32:41.310 --> 00:32:46.890
so it should have the common storage,
common authentication set up.

00:32:46.900 --> 00:32:50.700
Should have the portal node or head
node set up and the service nodes.

00:32:50.700 --> 00:32:53.600
Cluster nodes should
have their DNS set up,

00:32:53.600 --> 00:32:58.740
and everything should be
resolving with DNS just fine.

00:32:59.480 --> 00:33:04.380
We do the common file script,
which makes doing driver deployment

00:33:04.390 --> 00:33:06.600
and upgrades much easier.

00:33:06.600 --> 00:33:09.740
And then we also have
that passwordless key,

00:33:09.740 --> 00:33:13.900
which I'll go into why you want
to do that in just a minute.

00:33:16.950 --> 00:33:23.040
So there's a couple ways of
making changes or modifying

00:33:23.040 --> 00:33:25.400
how your compute nodes work.

00:33:25.400 --> 00:33:28.340
The first one is to edit
that central script,

00:33:28.450 --> 00:33:31.290
and you can kind of drop some
things that might make a change

00:33:31.290 --> 00:33:33.120
or a fix on the next reboot.

00:33:33.140 --> 00:33:38.620
Like, for example,
what I was saying before,

00:33:38.620 --> 00:33:38.620
where you might want to load a
different driver on a reboot.

00:33:38.750 --> 00:33:42.110
Another command that
we use a lot is PDSH,

00:33:42.110 --> 00:33:44.700
and that stands for
Parallel Distributed Shell.

00:33:44.820 --> 00:33:49.460
And what that lets you do is it
lets you run a command in parallel

00:33:49.500 --> 00:33:51.350
across your entire cluster.

00:33:51.560 --> 00:33:54.570
So in the example above, we have

00:33:54.740 --> 00:33:58.700
We're running command on nodes 1 through
999 with a timeout of 30 seconds,

00:33:58.700 --> 00:34:00.200
and we're just telling it to reboot.

00:34:00.200 --> 00:34:01.000
But you could do anything.

00:34:01.000 --> 00:34:03.400
You could have it copy files.

00:34:03.470 --> 00:34:06.260
You can change system settings.

00:34:06.260 --> 00:34:08.180
Anything that you can
do by the command line,

00:34:08.320 --> 00:34:10.540
you can do with this command.

00:34:10.580 --> 00:34:13.750
And finally,
we should be able to run basic MPI jobs,

00:34:13.750 --> 00:34:16.540
since now all the
systems have MPI on them,

00:34:16.540 --> 00:34:20.170
and all the service is required for that.

00:34:21.530 --> 00:34:24.900
So one of the problems that,
so you have your cluster up,

00:34:24.900 --> 00:34:28.500
is you really need to make sure that
everything's working as expected.

00:34:28.500 --> 00:34:35.150
Averaging and tech,
we had been rushing so much to get...

00:34:35.840 --> 00:34:37.800
We had to get to do the benchmarks.

00:34:37.800 --> 00:34:40.660
We were on such a time frame that
for the first three nights we

00:34:40.660 --> 00:34:43.280
had just turned everything on and
then we kept trying to run these

00:34:43.290 --> 00:34:44.700
benchmarks across all the systems.

00:34:44.700 --> 00:34:48.880
And it would continuously
fail because one system will

00:34:48.890 --> 00:34:51.700
basically mess up your whole run.

00:34:51.700 --> 00:34:56.240
So everything has to be stable and
reliable for you to be able to do that.

00:34:56.980 --> 00:35:00.410
So the first thing we need to do
is we want to test each system out.

00:35:00.410 --> 00:35:05.500
And what we did is we used LINPACK,
or the HPL benchmark,

00:35:05.560 --> 00:35:09.440
to kind of stress test each system.

00:35:09.510 --> 00:35:13.440
I kind of chose LINPACK because
it pegs the CPU,

00:35:13.480 --> 00:35:17.140
it can use a whole lot of RAM,
and you can have it run for a

00:35:17.140 --> 00:35:18.870
really long time unattended.

00:35:18.980 --> 00:35:23.010
So those met kind of some of the
requirements that we needed to kind

00:35:23.010 --> 00:35:25.440
of stress test the systems locally.

00:35:25.930 --> 00:35:31.030
So we run a two CPU job,
or two CPU tests per each XServe,

00:35:31.150 --> 00:35:34.140
so P being one XServe
and Q being two CPUs.

00:35:34.160 --> 00:35:39.200
And we pick a matrix size,
about 80% physical RAM.

00:35:39.200 --> 00:35:42.100
If you don't know what HPL is
or LIMPAC benchmark is,

00:35:42.100 --> 00:35:45.120
it's basically a linear
algebra problem solver.

00:35:45.120 --> 00:35:49.220
And so we want to be able to create a
matrix size that uses most of the CPU.

00:35:49.220 --> 00:35:53.220
And we want to do that because we want
to stress all of the RAM as we can,

00:35:53.220 --> 00:35:54.940
so we can see if there are
any errors with the RAM.

00:35:56.550 --> 00:35:59.400
And we want to run multiple tests.

00:35:59.400 --> 00:36:01.570
The reason we want to do that is
because we want to make sure things

00:36:01.570 --> 00:36:05.310
get nice and warm inside the XServe,
because some of the problems don't

00:36:05.310 --> 00:36:09.400
develop until they've been running at
kind of a warmer temperature for a while.

00:36:09.400 --> 00:36:12.360
So for example,
I think on a 4 gig XServe,

00:36:12.360 --> 00:36:16.370
the most you can have a single
run be is about 8 minutes,

00:36:16.370 --> 00:36:20.190
so adding multiple sets,
like in this case 4,

00:36:20.190 --> 00:36:23.210
you can test it out for much longer.

00:36:24.140 --> 00:36:29.000
The way you really find out if a compute
node is acting strangely is just to kind

00:36:29.000 --> 00:36:31.990
of compare it to the rest of the systems.

00:36:32.000 --> 00:36:36.150
So let's say you're
testing across 32 systems,

00:36:36.250 --> 00:36:40.120
and 31 of them may be
finished in 400 seconds,

00:36:40.120 --> 00:36:42.990
but that last one may be finished
in just 600 or 700 seconds.

00:36:43.000 --> 00:36:46.990
You have a pretty good idea that there's
something not quite right with that one.

00:36:47.110 --> 00:36:49.890
And there's a few things to try.

00:36:49.890 --> 00:36:51.840
Obviously,
one of the things you might want to

00:36:51.840 --> 00:36:55.000
do is just try to reboot and see if
that kind of clears up the error.

00:36:55.070 --> 00:36:58.930
Sometimes a service might
get stuck in a weird state,

00:36:58.940 --> 00:37:01.310
or may not come up all the way.

00:37:02.050 --> 00:37:04.590
If the rebooting doesn't fix it,
then you might want to

00:37:04.590 --> 00:37:06.000
look at hardware issues.

00:37:06.400 --> 00:37:14.000
One of the more common ones that we see
is memory errors or memory failures.

00:37:14.150 --> 00:37:17.650
When the XR boots and it detects
a bad DIMM or something like that,

00:37:17.650 --> 00:37:19.990
it will actually disable that DIMM.

00:37:20.130 --> 00:37:24.340
So the nice thing about having a test
that uses almost all of what we think is

00:37:24.340 --> 00:37:29.000
all the RAM is that we can really stress
and make sure all the RAM is being used.

00:37:29.000 --> 00:37:32.390
So if an XR came up and let's
say it had 4 gigs and found a

00:37:32.400 --> 00:37:37.000
bad DIMM and so it came up with
really 3.5 gigs of available RAM,

00:37:37.000 --> 00:37:40.000
it's going to start to swap
a lot when you run this test,

00:37:40.000 --> 00:37:45.000
and the swapping is going to create a
huge time increase in how long it takes.

00:37:45.000 --> 00:37:49.840
So again, you can check for ECC errors
by looking at the system logs.

00:37:49.990 --> 00:37:54.060
It's in VAR log, system log,
and it's also in the hardware

00:37:54.060 --> 00:37:56.000
monitor D log on the XRs.

00:37:56.000 --> 00:37:58.000
But it might not just be memory.

00:37:58.000 --> 00:37:59.000
It might be memory.

00:37:59.000 --> 00:38:04.000
We've seen one or two bad CPUs
that the system will work fine,

00:38:04.000 --> 00:38:06.970
but the CPUs will perform inconsistently.

00:38:07.000 --> 00:38:09.890
And then finally it
could be logic boards.

00:38:10.000 --> 00:38:13.770
Usually a logic board error is
a little bit different because

00:38:13.770 --> 00:38:16.000
the system is just not reliable.

00:38:16.000 --> 00:38:19.380
Usually it will crash,
and on the XRs when the system crashes,

00:38:19.460 --> 00:38:22.000
it can actually reboot
within five minutes.

00:38:22.000 --> 00:38:26.030
That's called a watchdog on OS X.

00:38:26.100 --> 00:38:29.620
So if a system crashes during your test,
then you might want to be able to look at

00:38:29.620 --> 00:38:32.810
the logic board or the CPU on that one.

00:38:33.250 --> 00:38:36.950
So between tests,
you want to replace the DIMMs that

00:38:37.000 --> 00:38:38.920
have the high ECC errors.

00:38:39.000 --> 00:38:44.620
And really, when I say high,
ECC errors are going to happen.

00:38:44.620 --> 00:38:47.500
I mean, at Virginia Tech,
we have 8,800 DIMMs.

00:38:47.500 --> 00:38:49.880
We're going to have
ECC errors all the time.

00:38:50.700 --> 00:38:53.620
What you want to look out for
is the stuff that's persistent,

00:38:53.620 --> 00:38:56.660
especially the stuff that's
persistent across reboots.

00:38:56.680 --> 00:39:00.810
So if I do a bunch of tests,
and I see one system that has ECC errors,

00:39:00.810 --> 00:39:02.920
what I'm going to do is I'm
going to reboot all the systems

00:39:02.980 --> 00:39:04.100
and run the tests again.

00:39:04.100 --> 00:39:06.650
If that same system
is getting ECC errors,

00:39:06.650 --> 00:39:10.090
then I know I definitely need
to replace that one DIMM.

00:39:10.350 --> 00:39:13.620
And I can't stress this long enough,
but you really should keep very good

00:39:13.620 --> 00:39:18.300
records of problems you've encountered.

00:39:18.340 --> 00:39:22.020
An example I can give you is that you
can maybe perhaps replace a CPU on a

00:39:22.060 --> 00:39:25.360
system when you thought the CPU was bad,
and pop it back in and

00:39:25.540 --> 00:39:27.400
everything works fine.

00:39:27.400 --> 00:39:32.030
Later down the road,
the system starts to develop errors.

00:39:32.060 --> 00:39:35.080
So unless you kind of kept a
log of what the problem was,

00:39:35.120 --> 00:39:38.890
you might think, hey, it's a CPU,
and replace the CPU again.

00:39:39.540 --> 00:39:41.630
So keeping a good log is good,
especially when you start

00:39:41.630 --> 00:39:42.730
getting larger systems.

00:39:43.030 --> 00:39:45.540
It becomes very critical to do that.

00:39:49.530 --> 00:39:54.290
So after that,
we should have all the nodes

00:39:54.370 --> 00:39:56.950
working well individually.

00:39:57.070 --> 00:39:58.900
So now we need to start
stressing the fabric out.

00:39:58.900 --> 00:40:01.480
And when I say fabric,
I mean either the Ethernet network

00:40:01.480 --> 00:40:04.400
or MirrorNet or InfiniBand,
the communications fabric

00:40:04.520 --> 00:40:06.340
that you're going to be using.

00:40:07.280 --> 00:40:08.730
So we kind of step it up a notch.

00:40:08.840 --> 00:40:11.370
Instead of running on one XServe,
we're going to run one

00:40:11.420 --> 00:40:12.880
test across two XServes.

00:40:12.890 --> 00:40:16.710
And so what we're going to do is
we're going to set up a four CPU job.

00:40:16.710 --> 00:40:20.460
So two CPUs on two XServes.

00:40:20.500 --> 00:40:22.340
Let's say you get the four CPUs.

00:40:22.400 --> 00:40:24.860
Same thing as before,
we're going to use 80%

00:40:24.860 --> 00:40:26.060
of the physical RAM.

00:40:26.060 --> 00:40:28.470
But you've got to remember
that this is the available

00:40:28.470 --> 00:40:30.240
physical RAM on all your systems.

00:40:30.240 --> 00:40:33.130
So before, we may have been running
on one 4GB XServe.

00:40:33.180 --> 00:40:35.280
Now we're running on two 4GB XServes.

00:40:35.280 --> 00:40:37.210
So that's actually 8GB of 3GB.

00:40:37.340 --> 00:40:39.510
of available RAM.

00:40:39.740 --> 00:40:43.790
And again, like the single pairs,
we want to run multiple tests just

00:40:43.790 --> 00:40:47.220
to kind of stress things out and
make sure that when things start

00:40:47.570 --> 00:40:50.440
running at a higher heat load,
it doesn't matter.

00:40:50.440 --> 00:40:53.910
I've seen things where the heat
will actually start to loosen

00:40:53.910 --> 00:40:56.780
the connector a little bit,
because obviously the connectors

00:40:56.780 --> 00:41:00.580
are getting much warmer in the back,
and the connector wasn't seated properly.

00:41:00.580 --> 00:41:04.940
So the heat can actually affect how
good the contact is of your connectors.

00:41:05.760 --> 00:41:08.650
So running at a high heat is
actually kind of important for

00:41:08.770 --> 00:41:10.720
making sure things are working.

00:41:12.660 --> 00:41:19.040
So slow performance here is often
going to be a network issue,

00:41:19.050 --> 00:41:22.080
but not always,
because you could see problems before

00:41:22.080 --> 00:41:24.770
that just didn't develop earlier.

00:41:24.780 --> 00:41:29.280
And with Ethernet, obviously,
you can look at the error counters,

00:41:29.280 --> 00:41:31.690
and you can also do the same
thing with Mirrodin and FinnaBand.

00:41:31.700 --> 00:41:34.170
Just check the error counters
and see if they're counting up.

00:41:34.240 --> 00:41:37.100
If you're getting any
error counters whatsoever,

00:41:37.170 --> 00:41:41.000
I think you should be looking at the
equipment and maybe replacing the cable,

00:41:41.000 --> 00:41:42.040
receiving the cables.

00:41:43.580 --> 00:41:45.610
So one of the problems that
we had with this is that,

00:41:45.610 --> 00:41:50.260
so now we're kind of trying to
figure out network problems.

00:41:50.310 --> 00:41:52.380
One of the first things we did at
Virginia Tech was we kind of did

00:41:52.380 --> 00:41:55.140
a binary search for bad nodes.

00:41:55.140 --> 00:42:00.050
So we would take maybe a cluster of
like 32 and run two 16-node tests.

00:42:00.050 --> 00:42:02.670
And then the first one would fail,
and we'd go, okay,

00:42:02.670 --> 00:42:05.840
so we got it down to 16,
and then we'd run two more tests of 8,

00:42:05.840 --> 00:42:08.030
and okay,
now we know which one of the 8 is doing

00:42:08.170 --> 00:42:10.240
it until you finally got down to it.

00:42:10.280 --> 00:42:12.490
And that took a lot of time.

00:42:13.500 --> 00:42:14.500
So we came up with this different way.

00:42:14.500 --> 00:42:18.500
Basically,
you have the pairs like I mentioned.

00:42:18.500 --> 00:42:23.500
So you pair like maybe node 1, node 2 up,
and node 3 and node 4 up, and so on,

00:42:23.630 --> 00:42:24.500
and run those tests.

00:42:24.500 --> 00:42:27.620
So let's say that node
1 and 2 tests failed.

00:42:27.620 --> 00:42:30.500
Well, now you've narrowed it down to 2.

00:42:30.630 --> 00:42:33.500
And so what we do on our second iteration
is we'll just kind of shift it by 1.

00:42:33.500 --> 00:42:38.740
We'll match up nodes 2 and 3 together,
and maybe nodes 4 and 1 together,

00:42:38.740 --> 00:42:40.630
and then test that.

00:42:40.640 --> 00:42:45.500
So if the tests 1 and 2 failed,
and the tests 2 and 3 failed,

00:42:45.500 --> 00:42:50.180
then chances are it's node 2
that's causing those problems.

00:42:52.800 --> 00:42:56.880
And so, obviously,
you can look more closely at Node 2's

00:42:56.880 --> 00:43:00.480
errors and see what is going on with it.

00:43:02.200 --> 00:43:04.680
So that's basically some of
the testing methodologies.

00:43:04.680 --> 00:43:09.170
There's a lot more than that,
but that really got us to a very

00:43:09.800 --> 00:43:13.080
good point where we were kind of
happy with the state of the system.

00:43:13.080 --> 00:43:17.180
I'm going to briefly go over some of the
job scheduling and resource management

00:43:17.220 --> 00:43:20.780
stuff that's involved in clusters.

00:43:20.780 --> 00:43:26.960
I always tell people that you don't
want to do this if you don't need to,

00:43:26.960 --> 00:43:30.100
because it really does increase
the complexity of your system.

00:43:30.860 --> 00:43:34.360
The only reason you really need to do
job scheduling and resource management

00:43:34.360 --> 00:43:37.860
is if you have multiple users,
and it becomes really more

00:43:37.860 --> 00:43:41.340
important if they all want
the system at the same time.

00:43:41.340 --> 00:43:44.380
So, for example,
if you have a shared resource,

00:43:44.380 --> 00:43:47.890
a cluster of 16 nodes,
and you have maybe two

00:43:47.890 --> 00:43:51.860
faculty professors that,
you know, faculty,

00:43:51.860 --> 00:43:53.640
they want to run on the entire system,
right?

00:43:53.670 --> 00:44:00.580
So you can basically set it up so that
if both of them submit a 16-node job,

00:44:00.860 --> 00:44:06.570
then it can split it,
or it can schedule the two and do that.

00:44:11.070 --> 00:44:12.660
So we're going to break this
down into kind of resource

00:44:12.730 --> 00:44:13.820
management job scheduling.

00:44:13.820 --> 00:44:18.040
Resource management is in
charge of running the actual

00:44:18.040 --> 00:44:22.640
codes on the compute nodes,
and

00:44:22.880 --> 00:44:25.700
That the nodes are operating and they're
doing what they're supposed to be.

00:44:25.700 --> 00:44:29.900
So make sure they're running the jobs,
make sure they're up,

00:44:29.900 --> 00:44:32.970
and everything's staying fine.

00:44:33.160 --> 00:44:37.160
So this basically just
handles the queue of jobs,

00:44:37.160 --> 00:44:40.060
making sure the jobs run,
and making sure they

00:44:40.060 --> 00:44:41.160
don't exceed resources.

00:44:41.160 --> 00:44:45.810
Often when someone submits a job,
they have to estimate the resources.

00:44:45.840 --> 00:44:49.960
So they'll say,
I want two wall hours of time,

00:44:49.960 --> 00:44:53.420
which means they want their
code run for two hours.

00:44:53.420 --> 00:44:56.390
And if their two hours comes
up and they're still running,

00:44:56.400 --> 00:45:03.050
then the resource manager is responsible
for killing it and cleaning that up.

00:45:03.460 --> 00:45:07.740
A resource management solution
I recommend is called GridEngine.

00:45:07.740 --> 00:45:11.180
It was originally called Sun GridEngine,
or SGE.

00:45:11.490 --> 00:45:15.240
It is free and it is open sourced,
and it is distributed by Sun,

00:45:15.240 --> 00:45:18.350
and is probably one of the
more actively developed,

00:45:18.350 --> 00:45:20.860
freely available resource managers.

00:45:21.610 --> 00:45:25.490
There are a lot of others
that are commercial and free,

00:45:25.490 --> 00:45:27.980
but I really like GridEngine.

00:45:27.980 --> 00:45:28.950
I've been pretty happy with it.

00:45:29.010 --> 00:45:32.880
They support OS X,
and I think they've been

00:45:33.050 --> 00:45:35.970
doing it for a few years now.

00:45:37.700 --> 00:45:43.900
So the job schedule will work
with the resource manager.

00:45:43.900 --> 00:45:47.680
And its goal is to basically
make sure that your resources

00:45:47.680 --> 00:45:49.820
are being used effectively.

00:45:50.340 --> 00:45:53.890
And basically,
this is where you get into the

00:45:54.010 --> 00:45:56.180
political issues that I was
kind of talking about before.

00:45:56.180 --> 00:46:01.280
Another issue that you might
run into is that you might,

00:46:01.370 --> 00:46:04.690
let's say you build a
cluster and you have,

00:46:05.040 --> 00:46:07.500
Some people donate some
money to build that cluster.

00:46:07.510 --> 00:46:11.830
Well, maybe the person who donated twice
as much as everyone else might want

00:46:11.830 --> 00:46:13.730
to have a higher priority access.

00:46:13.900 --> 00:46:17.730
A job schedule will let you kind of
give it so that when he submits a job,

00:46:17.730 --> 00:46:20.350
his jobs get a higher
priority in the queue.

00:46:20.740 --> 00:46:23.920
Most resource managers, though,
do come with a basic job scheduler,

00:46:23.920 --> 00:46:26.590
and it's very similar to
what the X-Grid one is,

00:46:26.670 --> 00:46:28.560
for example, which is basically first in,
first out.

00:46:28.560 --> 00:46:32.350
If you submit a job,
you're going to be the first one to run,

00:46:32.350 --> 00:46:33.380
and so forth.

00:46:33.420 --> 00:46:36.500
But again,
if you need the fair share queuing,

00:46:36.500 --> 00:46:40.380
if you need to perhaps have only
certain users run at certain

00:46:40.380 --> 00:46:43.780
times on certain systems,
then you need to look at a

00:46:43.800 --> 00:46:45.740
more complex job scheduler.

00:46:46.060 --> 00:46:47.540
And one of those is called MAUI.

00:46:47.540 --> 00:46:52.750
And MAUI is kind of the de facto
standard in HPC clusters for

00:46:52.750 --> 00:46:55.000
doing complex job scheduling.

00:46:55.000 --> 00:46:58.960
But it is complex,
because there's a lot of different

00:46:58.960 --> 00:47:01.730
ways of prioritizing your users.

00:47:01.740 --> 00:47:06.050
And MAUI has all the,
what we call different knobs and dials

00:47:06.250 --> 00:47:11.660
where you can tune and tweak it so that
people can run at different levels.

00:47:13.080 --> 00:47:14.000
Maui is free.

00:47:14.000 --> 00:47:17.670
There is a commercial aspect of it.

00:47:17.840 --> 00:47:18.870
It's called Moab.

00:47:18.960 --> 00:47:21.620
So if you need commercial
support for Maui,

00:47:21.620 --> 00:47:25.000
then you can look at Moab as well.

00:47:25.110 --> 00:47:26.380
And it works pretty well.

00:47:26.680 --> 00:47:30.590
Job scheduling is a really hairy thing.

00:47:30.700 --> 00:47:34.560
So for example, at Virginia Tech,
we have someone who's probably

00:47:34.880 --> 00:47:39.450
80% of their time is devoted to
just administering job scheduling.

00:47:39.580 --> 00:47:42.680
And honestly, it can be quite hairy.

00:47:42.700 --> 00:47:46.000
And he's responsible for
making sure everybody is happy,

00:47:46.000 --> 00:47:49.090
which is a really difficult thing to do.

00:47:50.480 --> 00:47:53.330
I'm also going to talk
about tweaking the OS.

00:47:53.530 --> 00:47:57.370
This is actually something
I presented about last year.

00:47:57.500 --> 00:48:01.020
And one of the things we're going to
talk about is how to reduce the number

00:48:01.030 --> 00:48:07.100
of services that's running on a system,
on one of your compute nodes.

00:48:08.230 --> 00:48:11.820
So what is our goal?

00:48:11.830 --> 00:48:14.680
Everyone's probably seen,
either in TOP or in the

00:48:15.130 --> 00:48:18.560
activity monitor on OS X,
all the things that are running.

00:48:18.560 --> 00:48:22.530
And there's a lot of things in
there that you're not sure about.

00:48:22.530 --> 00:48:28.520
Well, a lot of those are kind of the
core infrastructure for OS X.

00:48:28.850 --> 00:48:31.400
But a lot of them aren't required
in the cluster environment.

00:48:32.340 --> 00:48:36.790
So what we're going to do is we're
kind of going to wean these out and

00:48:36.790 --> 00:48:41.370
get it down to kind of a core set
that we need to run on the system.

00:48:42.780 --> 00:48:44.070
So why would we do this?

00:48:44.220 --> 00:48:47.590
Because we want to free up
more cycles for the user codes.

00:48:47.590 --> 00:48:51.150
And not only that,
but we want to make sure that these

00:48:51.150 --> 00:48:55.970
things running in the background
don't ask time for the processor.

00:48:56.010 --> 00:48:58.510
That generates what's
called a context switch.

00:48:58.530 --> 00:49:01.280
The processor has to go do
something else and then come back.

00:49:01.320 --> 00:49:05.260
And it just generally increases the
amount of time things take to run.

00:49:06.260 --> 00:49:10.420
Also by reducing the number of services,
you generally reduce the system

00:49:10.460 --> 00:49:14.860
complexity and potentially be able
to reduce things that go wrong.

00:49:14.860 --> 00:49:19.080
So if you turn off a
service that you don't need,

00:49:19.090 --> 00:49:24.350
then that's one less service that
might do something that's not

00:49:24.350 --> 00:49:31.650
right and interfere with your task.

00:49:31.770 --> 00:49:37.300
This can make diagnosing certain
things more of a problem.

00:49:37.400 --> 00:49:40.170
For example, one thing that I often
do is I'll turn off,

00:49:40.170 --> 00:49:43.410
and this is hard to say in
front of an OS X audience,

00:49:43.510 --> 00:49:46.010
but I'll turn off the user interface,
because we don't need

00:49:46.010 --> 00:49:47.030
it on the compute nodes.

00:49:47.040 --> 00:49:52.420
You're not logging into each compute
node and running applications on it.

00:49:52.480 --> 00:49:55.110
Most of the time you're running
it over the network through SSH.

00:49:56.430 --> 00:49:59.210
But obviously this can make
diagnosing a little bit harder,

00:49:59.280 --> 00:50:02.810
because you can't just sit down, log in,
and then go through the logs

00:50:02.810 --> 00:50:04.390
or look at certain things.

00:50:04.420 --> 00:50:09.400
So because it makes certain
things like diagnosing harder,

00:50:09.400 --> 00:50:11.540
we only want to do this
on the compute nodes.

00:50:11.540 --> 00:50:14.320
We don't want to do it on
the service and head nodes.

00:50:17.840 --> 00:50:24.790
So Mike Bombeck, when he was working on
the UIUC Turing cluster,

00:50:24.900 --> 00:50:29.320
created a script that would go
through a system and kind of disable

00:50:29.390 --> 00:50:32.660
a recommended set of services.

00:50:32.740 --> 00:50:38.980
And so he did that,
and I actually ran this on System 10.

00:50:38.980 --> 00:50:39.800
I thought it was great.

00:50:39.800 --> 00:50:43.060
I had done some of this work before,
but it was much more complete

00:50:43.060 --> 00:50:44.880
than the stuff I had been doing.

00:50:46.130 --> 00:50:49.320
So what we can do is we can,
based on each cluster node,

00:50:49.320 --> 00:50:52.140
run HPC Tune dash dash recommended.

00:50:52.140 --> 00:50:56.210
It's going to turn off a
recommended set of services,

00:50:56.310 --> 00:51:00.440
and it's going to turn things
off like the Windows server,

00:51:00.440 --> 00:51:04.570
and it's going to turn off things
like the Bonjour DNS stuff,

00:51:04.580 --> 00:51:07.960
which if you're doing things like XGrid,
you don't want to turn Bonjour off

00:51:07.960 --> 00:51:09.200
because it's going to break that.

00:51:09.200 --> 00:51:14.420
It's also going to break XSAN as well
because XSAN uses Bonjour to communicate

00:51:14.510 --> 00:51:18.880
and find the metadata controllers.

00:51:19.070 --> 00:51:24.150
So there are certain times where you want
to make sure a service is turned back on.

00:51:24.220 --> 00:51:29.040
And so you can actually use HPC Tune
and kind of adjust that recommended set.

00:51:29.040 --> 00:51:31.780
So one thing it does leave
on is the auto-mounter.

00:51:31.780 --> 00:51:34.880
And in certain situations,
you might not want the auto-mounter.

00:51:34.880 --> 00:51:38.520
So there's an example of how
to apply the recommended set,

00:51:38.520 --> 00:51:41.220
but instead of leaving
the auto-mounter on,

00:51:41.220 --> 00:51:42.800
we're going to turn it off.

00:51:43.230 --> 00:51:45.860
It is being updated for 10.4 right now.

00:51:46.060 --> 00:51:48.500
10.4 did change a few things
about how services started.

00:51:48.500 --> 00:51:51.600
It introduced LaunchD,
which is kind of completely

00:51:51.600 --> 00:51:55.230
changing some things that happened.

00:51:55.240 --> 00:52:01.140
It will be available on my .Mac account,
which is right there,

00:52:01.140 --> 00:52:04.940
and it'll probably be available soon.

00:52:04.940 --> 00:52:08.390
I would probably say at the
end of next week for 10.4,

00:52:08.390 --> 00:52:11.520
and there's a 10.3 version there already.

00:52:13.100 --> 00:52:18.100
And with that,
I'm going to hand it over to Jason.

00:52:27.000 --> 00:52:28.140
Thanks a lot.

00:52:28.150 --> 00:52:29.840
So I'm Jason Anthony Guy.

00:52:29.840 --> 00:52:33.420
I manage a developer
technical support team.

00:52:33.420 --> 00:52:40.030
So more information,
developer.apple.com/wwdc2005.

00:52:40.040 --> 00:52:43.770
We've got a bunch of fantastic sessions
that either have passed and you'll be

00:52:43.770 --> 00:52:47.760
able to watch on a streaming video in
the future or are coming up shortly.

00:52:47.760 --> 00:52:51.900
One I'd like to definitely highlight
is the monitoring your system

00:52:51.900 --> 00:52:55.550
with ARD and open source tools,
which is Thursday at 2:00.

00:52:55.610 --> 00:53:00.130
So if you want to see how to monitor
those systems you're deploying remotely,

00:53:00.130 --> 00:53:02.300
this is a great session to attend.

00:53:02.300 --> 00:53:04.670
You should also join us
in the Enterprise IT Lab,

00:53:04.670 --> 00:53:09.140
and a bunch of these guys and a bunch
of other systems administrators and so

00:53:09.140 --> 00:53:11.520
on will be there to take your questions.

00:53:11.640 --> 00:53:16.180
Chris Bledsoe is the person you want to
email if you have questions or comments.