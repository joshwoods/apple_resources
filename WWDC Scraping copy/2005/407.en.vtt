WEBVTT

00:00:05.620 --> 00:00:07.500
Good afternoon.

00:00:07.500 --> 00:00:09.740
Let's see.

00:00:09.820 --> 00:00:10.850
So I'm Nathan Slingerland.

00:00:10.860 --> 00:00:13.020
I'm with the Architecture and
Performance Group,

00:00:13.040 --> 00:00:15.090
and welcome to Swimming with Shark.

00:00:15.100 --> 00:00:19.960
Hopefully, during the sessions this week,
you've seen Shark in use,

00:00:20.060 --> 00:00:24.660
but today we want to go over the
basics first and kind of tell you

00:00:24.660 --> 00:00:26.620
how to use Shark on your code.

00:00:26.640 --> 00:00:31.810
But we also have some new features that
we're excited to introduce to you today,

00:00:31.830 --> 00:00:33.780
and so let's get started.

00:00:35.860 --> 00:00:39.100
So a few of you may still be wondering,
what is Shark?

00:00:39.140 --> 00:00:44.400
Well, it's a really simple and fast
profiling tool for Mac OS X.

00:00:44.520 --> 00:00:48.100
It's going to help you find
your performance bottlenecks.

00:00:48.130 --> 00:00:50.260
You know,
works with the language of your choice,

00:00:50.340 --> 00:00:55.760
Java, Objective-C, C, whatever,
whatever compiler you're on.

00:00:55.980 --> 00:00:58.910
And there are two flavors,
GUI and command line.

00:00:59.230 --> 00:01:03.130
So command line version you can
use to script for your-- look

00:01:03.230 --> 00:01:07.990
for performance regressions,
automatically profile your code.

00:01:08.380 --> 00:01:11.610
And today we have CHUD 4.2 preview.

00:01:11.800 --> 00:01:14.110
So hopefully you've had
a chance to download it.

00:01:14.180 --> 00:01:15.550
It's available on the website here.

00:01:15.550 --> 00:01:19.090
And of course, it's free, as always.

00:01:21.290 --> 00:01:27.040
So Shark has a lot of powerful workflows,
the most basic being time profiling.

00:01:27.070 --> 00:01:28.910
There's also malloc tracing.

00:01:28.920 --> 00:01:32.470
New in Shark 4.2, a counter spreadsheet,
which we'll talk about all these.

00:01:32.510 --> 00:01:35.430
You can also create your
own custom configurations.

00:01:35.440 --> 00:01:38.780
And of course, any of the configurations
that we support,

00:01:38.780 --> 00:01:40.930
you can use with network profiling.

00:01:40.930 --> 00:01:44.300
So you can look at the
performance of network machines.

00:01:46.090 --> 00:01:47.860
So time profiling.

00:01:47.960 --> 00:01:52.280
This is just the most common workflow
we use in performance analysis.

00:01:52.300 --> 00:01:54.270
You know, because you want to spend the
time optimizing the code that

00:01:54.380 --> 00:01:55.370
affects performance the most.

00:01:55.390 --> 00:01:58.020
So that's the code that takes
the most time on your system.

00:01:58.020 --> 00:01:59.620
So how does that work?

00:01:59.750 --> 00:02:02.230
Well,
we interrupt the system periodically.

00:02:02.230 --> 00:02:04.800
By default,
that's once every millisecond.

00:02:04.800 --> 00:02:08.610
So, you know, a thousand times a second
we record what was running,

00:02:08.610 --> 00:02:13.160
what process, what thread, you know,
all that good stuff, and a backtrace.

00:02:13.890 --> 00:02:16.610
And it's all very low overhead because
we have a kernel extension that

00:02:16.610 --> 00:02:18.360
we're using to collect all this data.

00:02:18.360 --> 00:02:21.230
And that also lets us see everything.

00:02:21.230 --> 00:02:24.440
So no matter what you're working on,
if it's a driver or an application,

00:02:24.440 --> 00:02:28.470
command line, you know, daemon, whatever,
we're going to pick that up and be able

00:02:28.470 --> 00:02:30.670
to show you something about it in Shark.

00:02:33.300 --> 00:02:37.460
So another important workflow besides
the time profile is malloc tracing.

00:02:37.460 --> 00:02:42.090
Of course,
many large applications have memory

00:02:42.090 --> 00:02:44.620
usage as a real performance limiter.

00:02:44.620 --> 00:02:48.330
So this is going to help you
track that by recording a sample

00:02:48.330 --> 00:02:50.560
on every malloc and every free.

00:02:50.560 --> 00:02:50.560
So this is going to help you
track that by recording a sample

00:02:50.560 --> 00:02:50.620
on every malloc and every free.

00:02:50.620 --> 00:02:54.290
And so you can understand, you know,
in simplest terms,

00:02:54.290 --> 00:02:57.200
you can understand where
you're using memory.

00:02:57.320 --> 00:03:00.540
But it's also useful for
visualizing program behavior

00:03:00.940 --> 00:03:04.950
because most sophisticated software
is going to allocate memory

00:03:04.950 --> 00:03:07.600
whenever it performs an operation.

00:03:07.600 --> 00:03:10.880
So you can look for this
pattern of allocation.

00:03:10.880 --> 00:03:14.800
And if you see repeated patterns,
you can hopefully try to

00:03:14.800 --> 00:03:18.960
remove potentially redundant
operations from your code.

00:03:18.960 --> 00:03:18.960
So this is a good way to understand
where you're using memory.

00:03:18.960 --> 00:03:18.960
And so you can understand, you know,
in simplest terms,

00:03:18.960 --> 00:03:18.960
you can understand where
you're using memory.

00:03:18.960 --> 00:03:18.960
But it's also useful for
visualizing program behavior

00:03:18.960 --> 00:03:18.960
because most sophisticated software
is going to allocate memory

00:03:18.960 --> 00:03:18.960
whenever it performs an operation.

00:03:18.960 --> 00:03:18.960
So you can look for this
pattern of allocation.

00:03:18.960 --> 00:03:18.960
And if you see repeated patterns,
you can hopefully try and

00:03:18.960 --> 00:03:18.960
remove potentially redundant
operations from your code.

00:03:20.070 --> 00:03:23.270
So this is a good way to get a kind of
a high-level overview of what's going

00:03:23.270 --> 00:03:25.960
on in the behavior in your program.

00:03:27.120 --> 00:03:30.640
So new in Shark 4.2 is
a counter spreadsheet.

00:03:30.730 --> 00:03:35.640
And if you've ever used the
Monster application that lets you--

00:03:35.640 --> 00:03:37.740
this is a similar functionality.

00:03:37.740 --> 00:03:40.590
It's going to let you look at the
actual performance counter values.

00:03:40.840 --> 00:03:44.700
So we have, of course,
in our hardware processors and

00:03:44.700 --> 00:03:49.560
our PowerPC memory controllers,
we have performance monitoring counters.

00:03:49.780 --> 00:03:52.660
And using these values,
you can use this counter

00:03:52.670 --> 00:03:54.620
spreadsheet to compute metrics.

00:03:54.710 --> 00:03:58.020
For example, memory bandwidth or CPI.

00:03:58.100 --> 00:04:00.380
And the memory bandwidth
config comes built in.

00:04:00.480 --> 00:04:05.060
And that's useful if you want to see how
your memory bandwidth usage compares to

00:04:05.190 --> 00:04:08.160
the actual hardware limits on the system.

00:04:08.250 --> 00:04:09.500
It's a good check.

00:04:09.500 --> 00:04:12.480
You can also use it to design
your own metrics and do your own

00:04:12.480 --> 00:04:15.190
customized performance studies.

00:04:16.790 --> 00:04:19.080
So, of course, you know,
those are just a few basic ones.

00:04:19.160 --> 00:04:22.790
We have more built-ins,
but you can also create your own,

00:04:22.870 --> 00:04:23.990
your own configurations.

00:04:24.020 --> 00:04:27.360
You know,
for the performance monitoring counters,

00:04:27.360 --> 00:04:29.380
you can search by event name.

00:04:29.380 --> 00:04:33.000
So this is really useful because,
you know, each bit of hardware,

00:04:33.000 --> 00:04:36.420
each processor is a little different
and has different performance events.

00:04:36.440 --> 00:04:38.570
So this will let you just quickly
do a text search and find the

00:04:38.590 --> 00:04:40.750
events that you're interested
in without worrying about,

00:04:40.750 --> 00:04:42.950
you know,
which counter is it on and so on.

00:04:43.860 --> 00:04:46.800
And, of course, if you want to,
you can go to advanced mode here

00:04:46.800 --> 00:04:51.040
and you can control absolutely every
aspect of profiling in Shark and,

00:04:51.040 --> 00:04:53.040
you know, manage every little detail.

00:04:56.240 --> 00:04:59.560
So in conjunction with any
of these configurations,

00:04:59.580 --> 00:05:01.830
you can use network profiling.

00:05:01.830 --> 00:05:07.210
So this is using either
Bonjour over TCP/IP.

00:05:07.220 --> 00:05:13.200
You can control and collect
profile sessions over the network.

00:05:13.200 --> 00:05:17.650
So this is useful if you want to
monitor your distributed network

00:05:17.730 --> 00:05:21.160
computing or your cluster,
your headless machine that you

00:05:21.160 --> 00:05:25.480
have someplace or a machine you
don't have immediate access to.

00:05:25.540 --> 00:05:28.090
This is useful for doing that.

00:05:30.470 --> 00:05:32.900
So we've talked about kind
of the basic workflows,

00:05:32.910 --> 00:05:37.050
but it's probably a pretty good
idea to just review the basics,

00:05:37.050 --> 00:05:37.640
right?

00:05:37.640 --> 00:05:39.480
I mean, what are we doing with Shark?

00:05:39.480 --> 00:05:42.870
And what are we doing when we
do performance optimization?

00:05:43.050 --> 00:05:45.440
Well, the most basic thing is
to establish a baseline,

00:05:45.440 --> 00:05:45.600
right?

00:05:45.730 --> 00:05:48.850
So no matter what study you do and
no matter what you're looking at,

00:05:48.910 --> 00:05:52.370
you need to have something that's
repeatable and something that,

00:05:52.410 --> 00:05:54.590
you know, you're going to be able to
measure the performance of easily.

00:05:54.600 --> 00:05:56.510
So, you know, for example,
the time that an

00:05:56.510 --> 00:05:57.890
important operation takes.

00:05:57.890 --> 00:05:59.810
So you can instrument
your code with a timer,

00:05:59.810 --> 00:06:01.580
for example, just to have a baseline.

00:06:01.580 --> 00:06:04.060
Or you can even use, you know,
a stopwatch to do that.

00:06:05.320 --> 00:06:07.890
And step two,
you want to profile optimize your

00:06:08.110 --> 00:06:11.970
deployment code since that's what
your end users are going to have.

00:06:12.020 --> 00:06:14.270
And so, you know,
and that's where it's going to

00:06:14.270 --> 00:06:16.250
include the compiler optimizations.

00:06:16.260 --> 00:06:21.440
So you don't want to waste time,
you know, optimizing development code.

00:06:21.470 --> 00:06:25.100
But you do want to have debug symbols
on because that's how Shark is

00:06:25.210 --> 00:06:27.200
going to tell you about your code.

00:06:27.220 --> 00:06:30.630
So, and finally, step three, of course,
Shark it, right?

00:06:30.630 --> 00:06:32.920
Use Shark and see where
the time is going.

00:06:32.920 --> 00:06:34.680
Make sure that makes sense.

00:06:35.320 --> 00:06:39.600
So, you know,
you can start profiling with

00:06:39.600 --> 00:06:45.600
the start button and you're on
your way to begin profiling.

00:06:45.600 --> 00:06:48.450
There's also a global hotkey,
so Shark doesn't have to be in the

00:06:48.540 --> 00:06:50.600
foreground to start and stop profiling.

00:06:50.600 --> 00:06:56.590
Option escape is the default hotkey.

00:06:56.820 --> 00:07:01.070
And there are a bunch of
built-in sampling configurations,

00:07:01.160 --> 00:07:04.010
the most basic, of course, time profile,
the most commonly used.

00:07:04.060 --> 00:07:08.020
There are a bunch of these other
handy ones that we've also included.

00:07:09.260 --> 00:07:15.150
And by default,
we like to profile everything usually,

00:07:15.150 --> 00:07:17.850
so that we don't miss
any important behavior.

00:07:17.860 --> 00:07:22.160
But sometimes for certain
types of workflows,

00:07:22.190 --> 00:07:25.680
it may be useful to select just a
single process or a single file.

00:07:25.830 --> 00:07:28.650
And that's what this process pop-up does.

00:07:30.530 --> 00:07:32.710
So Shark,
once you take a profiling session,

00:07:32.710 --> 00:07:35.020
it's going to give you
three ways to look at it.

00:07:35.020 --> 00:07:36.540
And the first is heavy view.

00:07:36.540 --> 00:07:39.340
And that's really the most
basic and most important.

00:07:39.340 --> 00:07:43.810
You just want to first look at which
functions were sampled most in your code.

00:07:44.080 --> 00:07:46.840
So it's going to list them from
most sampled to least sampled.

00:07:46.840 --> 00:07:49.740
And the most sampled would
be taking the most time.

00:07:49.790 --> 00:07:55.340
So if you use this pop-up in
the bottom right-hand corner,

00:07:55.340 --> 00:07:56.840
you can switch to a tree view.

00:07:57.020 --> 00:07:59.460
So rather than seeing hot
spots in the heavy view,

00:07:59.460 --> 00:08:01.760
you can see hot paths.

00:08:01.760 --> 00:08:05.250
Or you can see them both at once.

00:08:05.660 --> 00:08:06.100
So that's great.

00:08:06.100 --> 00:08:09.640
That's going to show you the
breakdown over time for all

00:08:09.640 --> 00:08:11.920
the functions in your code.

00:08:11.930 --> 00:08:14.100
But if you double click
on any of these functions,

00:08:14.100 --> 00:08:15.840
you're going to get a code browser.

00:08:15.860 --> 00:08:19.550
And this is great because this shows
you your source code highlighted

00:08:19.550 --> 00:08:21.500
according to where the samples landed.

00:08:21.560 --> 00:08:24.700
So lines that took more
time are sampled more often,

00:08:24.720 --> 00:08:25.840
and they're brighter yellow.

00:08:25.840 --> 00:08:28.740
In this example,
you can see there are some bright yellow

00:08:28.830 --> 00:08:30.980
lines and some dimmer yellow lines.

00:08:31.050 --> 00:08:34.280
So the brighter yellow,
the more time was spent there.

00:08:34.360 --> 00:08:36.510
So you want to focus on those.

00:08:36.760 --> 00:08:40.050
And we have a custom scroll bar
that's going to help you quickly

00:08:40.050 --> 00:08:41.750
jump to hot spots in your code.

00:08:42.000 --> 00:08:43.660
So you can use that.

00:08:43.660 --> 00:08:45.700
You can see that here in this example.

00:08:45.750 --> 00:08:49.890
And of course, the full cycle is to--
you profile your code,

00:08:49.890 --> 00:08:53.250
and then you want to go back to Xcode,
edit it, make a change,

00:08:53.250 --> 00:08:54.610
and then re-profile.

00:08:54.730 --> 00:09:01.120
So you click the Edit button,
and you're going to go back to Xcode,

00:09:01.120 --> 00:09:01.270
and you can make whatever
change it is that you want to.

00:09:03.370 --> 00:09:06.390
So, okay, those are the basic workflows.

00:09:06.400 --> 00:09:10.440
Shark also provides several tools for
helping to better understand your code,

00:09:10.440 --> 00:09:14.150
better understand what's
going on in your code.

00:09:14.240 --> 00:09:17.790
So many of you probably have
complex code trees with some,

00:09:17.790 --> 00:09:21.460
you know, lots and lots of branches and,
you know, lots of things going on.

00:09:21.460 --> 00:09:25.170
If you look at this example,
there's -- the symbol "B" is

00:09:25.170 --> 00:09:28.230
showing up twice in this call tree,
and it's called from

00:09:28.230 --> 00:09:29.260
two different points.

00:09:29.260 --> 00:09:33.360
And, you know, it's calling some other
sub-functions to do some work for it.

00:09:33.400 --> 00:09:35.800
And really what we'd like to know is,
you know,

00:09:35.810 --> 00:09:40.660
not just how much time one little
-- one branch of "B" is taking,

00:09:40.660 --> 00:09:45.240
but how much all the little usages
of "B," how much time they take.

00:09:45.300 --> 00:09:47.910
So if you click this "Show
all branches" -- and this is

00:09:47.970 --> 00:09:49.200
the advanced settings drawer.

00:09:49.220 --> 00:09:51.280
You can show this in Shark.

00:09:51.340 --> 00:09:55.530
It's going to collapse -- it's going to
give you a first top-level entry for each

00:09:55.530 --> 00:09:58.360
symbol that occurred in the call tree.

00:09:58.480 --> 00:10:01.490
So this is just -- it's pretty useful
for finding out how much time you spent

00:10:01.510 --> 00:10:05.440
in a symbol and everything it calls.

00:10:05.480 --> 00:10:06.720
So that's one way to look at it.

00:10:06.740 --> 00:10:10.010
We'll have a few other things to look at,
too.

00:10:10.160 --> 00:10:12.380
Also, color by library.

00:10:12.550 --> 00:10:15.910
So this is pretty useful for large
applications with a lot of modules.

00:10:16.010 --> 00:10:20.300
It'll let you quickly,
visually distinguish between where all

00:10:20.300 --> 00:10:23.870
the time is being spent based on module,
so based on, you know,

00:10:23.870 --> 00:10:25.660
which framework you were in.

00:10:27.800 --> 00:10:30.170
And of course,
Shark gives you data mining.

00:10:30.260 --> 00:10:33.770
So what this allows you to do is
customize the display of the profile

00:10:33.770 --> 00:10:35.680
so that you can understand it better.

00:10:35.680 --> 00:10:40.360
You know, by default,
we're showing the profile just based on

00:10:40.460 --> 00:10:44.300
the symbols that are in the application,
and that may not necessarily be

00:10:44.300 --> 00:10:45.870
the easiest way to understand it.

00:10:45.940 --> 00:10:48.690
So there are a bunch of operators here.

00:10:48.700 --> 00:10:51.560
First, you know,
charge symbol or library.

00:10:51.560 --> 00:10:54.070
What that's going to do is it's
going to take a symbol that you

00:10:54.070 --> 00:10:57.060
select and you either right-click or,
you know,

00:10:57.060 --> 00:11:01.060
if you have a right mouse button,
or you control-click in the profile

00:11:01.060 --> 00:11:04.440
on a particular symbol or library.

00:11:04.440 --> 00:11:07.790
And you can hide any samples that
landed in that symbol or library.

00:11:07.800 --> 00:11:10.320
So we're not actually
removing these samples.

00:11:10.360 --> 00:11:12.790
They're actually just being
assigned to the caller.

00:11:12.800 --> 00:11:17.420
So the profile is
overall staying constant.

00:11:18.010 --> 00:11:18.700
Focus Symbol.

00:11:19.020 --> 00:11:21.940
So you can show only paths
starting from a symbol,

00:11:22.060 --> 00:11:23.820
foo in this example.

00:11:23.830 --> 00:11:27.070
So this is useful when you have
really deep call stacks and you just

00:11:27.360 --> 00:11:31.400
want to really focus your analysis
on this part that you understand.

00:11:32.030 --> 00:11:36.040
And FlattenLibrary is another very
useful operator because a lot of us,

00:11:36.040 --> 00:11:40.150
if you call a framework function,
you don't so much care about

00:11:40.150 --> 00:11:43.770
the interior symbols that
that function may be calling.

00:11:43.790 --> 00:11:46.250
I mean, we don't have any control
over those things.

00:11:46.460 --> 00:11:49.150
So this is going to hide all those
things except for the entry point,

00:11:49.410 --> 00:11:52.120
which is what you'd be familiar with.

00:11:52.290 --> 00:11:55.760
So data mining is going to help
you in cases where there isn't a

00:11:55.810 --> 00:11:59.220
function that pops right out in
the heavy view or really where

00:11:59.220 --> 00:12:01.120
you're spending a lot of time.

00:12:01.140 --> 00:12:02.700
And it's going to help you
with high-level analysis.

00:12:02.700 --> 00:12:06.610
If you spend a lot of time in frameworks,
you can't really change

00:12:06.610 --> 00:12:09.750
the framework code,
but you could possibly change

00:12:09.750 --> 00:12:11.780
how you call that framework.

00:12:14.030 --> 00:12:18.090
So in the previous data
mining example we showed you,

00:12:18.090 --> 00:12:22.630
our previous data mining technique,
you pick a specific similar library.

00:12:22.630 --> 00:12:26.700
Well, there are also some useful
macro-level data mining operations

00:12:26.700 --> 00:12:31.150
where it'll just automatically do,
apply an operation to the whole profile.

00:12:31.160 --> 00:12:38.080
And one of the most useful is charge
code without debug symbols to callers.

00:12:38.150 --> 00:12:42.390
So usually as a developer,
the code that's in your system that has

00:12:42.470 --> 00:12:44.490
debug information is only your code,
right?

00:12:44.580 --> 00:12:46.100
System code doesn't have any of that.

00:12:46.100 --> 00:12:48.530
So if you do that,
it's just going to automatically

00:12:48.590 --> 00:12:51.450
do that to everything in the
profile and just show you the

00:12:51.460 --> 00:12:55.170
code that you have control of,
you know, your source code.

00:12:56.940 --> 00:12:58.920
So that's high-level analysis,
some of the high-level

00:12:58.920 --> 00:12:59.980
analysis tools in Shark.

00:13:00.010 --> 00:13:04.040
Of course, Shark also provides tools
for tuning your code.

00:13:04.040 --> 00:13:08.820
In the code browser,
there's automated code analysis.

00:13:08.820 --> 00:13:13.450
So Shark will show this little
exclamation point button when you have,

00:13:13.450 --> 00:13:16.420
and it has an optimization tip available.

00:13:16.420 --> 00:13:19.080
If you click that,
it's going to put some help.

00:13:19.360 --> 00:13:20.600
Maybe you could do this.

00:13:20.600 --> 00:13:22.140
Maybe you could try this.

00:13:22.190 --> 00:13:27.910
So this is all based on problems we've
run across or different performance

00:13:28.020 --> 00:13:33.440
optimizations we've run across that we've
tried to automate and put into Shark.

00:13:35.250 --> 00:13:38.780
Shark also, in the code browser,
you can see source code

00:13:38.780 --> 00:13:40.820
and assembly side by side.

00:13:40.940 --> 00:13:43.200
So this is really useful for seeing,
you know,

00:13:43.370 --> 00:13:46.920
what the compiler generated when you,
you know, with your source line.

00:13:46.920 --> 00:13:48.960
So if you, it's all integrated.

00:13:48.960 --> 00:13:52.320
If you click a line of source,
it will highlight corresponding

00:13:52.350 --> 00:13:54.120
instructions and vice versa.

00:13:54.120 --> 00:13:56.750
So it's all integrated
between the two views.

00:13:58.740 --> 00:14:01.270
And of course, although you may not be
an expert at assembly,

00:14:01.270 --> 00:14:05.290
there's an online assembly reference
guide that's integrated with this.

00:14:05.350 --> 00:14:07.920
So if you click on an instruction,
it's going to bring up the

00:14:08.020 --> 00:14:11.290
definition of that instruction,
in this case, FMAD.

00:14:11.360 --> 00:14:15.640
So you don't have to memorize every
single instruction definition and little

00:14:15.640 --> 00:14:18.020
variation between the instructions.

00:14:19.810 --> 00:14:21.710
So those are the basics
of how to use Shark.

00:14:21.880 --> 00:14:27.000
So just to take Shark for a test drive,
at this time,

00:14:27.000 --> 00:14:30.100
let's bring up Sanjay Patel,
one of my colleagues from the

00:14:30.100 --> 00:14:31.300
Architecture and Performance group.

00:14:34.560 --> 00:14:37.590
Thank you.

00:14:37.620 --> 00:14:40.240
Can we get the demo machine one?

00:14:45.700 --> 00:14:47.900
So this has become kind of
an annual tradition for us.

00:14:48.030 --> 00:14:52.530
We go out and try to find an application
that's running on Mac that we think

00:14:52.580 --> 00:14:53.760
we can help the performance of.

00:14:53.820 --> 00:14:56.620
In this case, we found this application.

00:14:56.620 --> 00:14:58.490
It's actually a screensaver
in its base form.

00:14:58.490 --> 00:14:59.860
It's called Einstein at Home.

00:14:59.860 --> 00:15:01.800
And if you want to launch that.

00:15:02.060 --> 00:15:02.240
Sure.

00:15:03.700 --> 00:15:07.400
What it's doing is it's collecting
data from different observatories,

00:15:07.400 --> 00:15:09.700
both in the United States and in Germany.

00:15:09.700 --> 00:15:12.560
And it's trying to detect
gravitational waves.

00:15:12.660 --> 00:15:16.340
These were predicted by
Albert Einstein in 1915.

00:15:16.340 --> 00:15:19.630
So it's collecting data from
satellites and so on and

00:15:19.630 --> 00:15:21.940
processing them as a screensaver.

00:15:21.940 --> 00:15:26.270
So it's similar to SETI at Home,
which you might be familiar with.

00:15:26.350 --> 00:15:29.360
In this case, what we're looking at
is the celestial sphere.

00:15:29.360 --> 00:15:33.680
And what we see are the purple dots.

00:15:33.700 --> 00:15:35.260
The purple dots represent pulsars.

00:15:35.260 --> 00:15:36.140
You'll see red dots.

00:15:36.360 --> 00:15:38.790
Those are the remnants of supernovas.

00:15:38.790 --> 00:15:40.610
You can drag around.

00:15:40.610 --> 00:15:45.220
If you guys have this on
your laptops as a download,

00:15:45.220 --> 00:15:49.210
you can also download -- launch
that from the disk image.

00:15:50.660 --> 00:15:54.760
And you'll actually see the green L,
blue L,

00:15:54.760 --> 00:15:56.320
and there should be a red L somewhere.

00:15:56.320 --> 00:15:58.940
Those are the actual points
of the observatories on

00:15:58.940 --> 00:16:00.540
Earth that are collecting data.

00:16:00.540 --> 00:16:02.800
If you Control-click,
you can actually zoom in

00:16:02.800 --> 00:16:04.320
and out of this application.

00:16:06.300 --> 00:16:08.730
And so we got the screensaver.

00:16:08.820 --> 00:16:10.720
We actually got the source from Dr.

00:16:10.720 --> 00:16:14.550
Khanna to see what we could do with it.

00:16:14.590 --> 00:16:17.270
And so the first thing,
as Nathan mentioned,

00:16:17.290 --> 00:16:21.100
when you want to do a performance
optimization is set a baseline.

00:16:21.100 --> 00:16:23.860
And you can see down here
we added a little timer.

00:16:23.860 --> 00:16:27.960
And we're going to iterate over a fixed
set of data and process that data.

00:16:27.960 --> 00:16:30.500
And if you hit the space bar,
you'll actually see a

00:16:30.820 --> 00:16:31.790
little orange target.

00:16:31.790 --> 00:16:34.360
It's on the other side
of the sphere right now.

00:16:34.360 --> 00:16:39.730
It's actually processing data at
that point on the celestial sphere.

00:16:40.200 --> 00:17:11.700
[Transcript missing]

00:17:12.440 --> 00:17:14.230
And hit the space bar.

00:17:14.340 --> 00:17:16.200
That'll get it going.

00:17:16.230 --> 00:17:19.650
And then you can use option
escape to get Shark going.

00:17:19.750 --> 00:17:22.120
So right now, Shark is actually sampling
at one millisecond.

00:17:22.120 --> 00:17:25.840
So this is our default
configuration for time profiling.

00:17:25.880 --> 00:17:26.920
And it stopped in.

00:17:26.920 --> 00:17:30.760
We're going to process the samples that
we just took over that period of time.

00:17:31.510 --> 00:17:35.700
And here you see the default view,
the heavy view.

00:17:35.700 --> 00:17:38.910
And what you're looking at is a
list of the functions that actually

00:17:38.910 --> 00:17:42.450
ran during that sampling period
in order of number of samples that

00:17:42.450 --> 00:17:44.260
landed in each particular function.

00:17:44.260 --> 00:17:48.900
And you'll notice there's one function
that's clearly the most important here,

00:17:49.080 --> 00:17:50.280
this LALD mod.

00:17:50.280 --> 00:17:53.770
But before we even get to that,
we should always look at the

00:17:54.270 --> 00:17:57.580
process pop-up in this case is
showing us that only a little more

00:17:57.580 --> 00:18:02.400
than half of the CPU cycles on our
system are going into the process.

00:18:02.400 --> 00:18:04.050
So we're going to go ahead
and look at the process name

00:18:04.160 --> 00:18:08.210
of this Einstein at Home app.

00:18:09.450 --> 00:18:09.470
And if we look at the thread pop-up,

00:18:10.490 --> 00:18:11.990
You'll notice there are two threads.

00:18:11.990 --> 00:18:15.260
One of them is eating up
basically one whole CPU,

00:18:15.260 --> 00:18:16.670
and that's the one that's
doing the calculations.

00:18:16.670 --> 00:18:18.990
And the other one is just
taking up a little time,

00:18:18.990 --> 00:18:22.960
and you'll notice that it's pretty much
all in graphics drivers and OpenGL code.

00:18:22.960 --> 00:18:27.200
So that's the graphics thread that's
actually drawing stuff on screen.

00:18:28.820 --> 00:18:31.030
Well, we looked at that and we said,
okay, we've got all these empty cycles.

00:18:31.030 --> 00:18:34.750
We should see if we can thread this app,
because clearly that's the direction

00:18:34.920 --> 00:18:38.040
all future hardware is going to
take is being more multi-threaded,

00:18:38.040 --> 00:18:40.620
more multi-core, more multi-processor.

00:18:40.620 --> 00:18:43.950
So the place to look, we figured,
was let's go to the top function

00:18:43.950 --> 00:18:47.900
and let's see what it's doing,
and let's see if that code is threadable.

00:18:47.900 --> 00:18:51.770
So if you double-click on that function,
you get what we call the code browser.

00:18:51.780 --> 00:18:54.390
And again,
this is showing you your code annotated

00:18:54.390 --> 00:18:56.480
and highlighted in terms of hotspots.

00:18:57.510 --> 00:18:59.830
And we have this little edit
button down at the bottom.

00:18:59.860 --> 00:19:02.810
If you click that,

00:19:03.730 --> 00:19:08.200
It'll bring up your source code in
Xcode where you can go and edit.

00:19:08.200 --> 00:19:12.110
It should jump right to the
highlighted line that you had in Shark.

00:19:12.420 --> 00:19:16.280
And what we noticed in the case of
Einstein here is that all the time

00:19:16.280 --> 00:19:19.300
is going into this single for loop,
and it's got a fixed boundary,

00:19:19.300 --> 00:19:22.240
k equals 0 to the k limit.

00:19:22.260 --> 00:19:24.960
And we said, well,
what if we just split that

00:19:25.000 --> 00:19:26.430
for loop n ways for n CPUs?

00:19:26.690 --> 00:19:30.530
In this particular case, of course,
it's going to be two CPUs.

00:19:30.720 --> 00:19:34.550
And if you look a little bit
further down in the source code,

00:19:34.560 --> 00:19:37.560
you'll see how we implemented
that using Pthreads.

00:19:37.690 --> 00:19:41.030
So there's a Pthread create,
which is highlighted there.

00:19:41.150 --> 00:19:42.960
So that's actually spinning off threads.

00:19:43.030 --> 00:19:45.580
And then there's a Pthread
join shortly thereafter.

00:19:45.580 --> 00:19:47.700
That's so when threads
are done with their work,

00:19:47.750 --> 00:19:50.100
they actually get destroyed there.

00:19:50.310 --> 00:19:52.650
And Pthread create takes
a function pointer.

00:19:52.930 --> 00:19:56.270
So this is this LALDmodCore.

00:19:56.330 --> 00:19:59.010
And if we jump over to that function,

00:20:00.640 --> 00:20:03.060
You'll notice that, well,
it's just that loop,

00:20:03.090 --> 00:20:06.780
except that we've changed the iteration
bounds to take parameters for the

00:20:06.780 --> 00:20:10.000
beginning and ending points of the array.

00:20:10.120 --> 00:20:11.990
So we wrote this up with pthreads.

00:20:12.000 --> 00:20:14.000
This didn't take very long.

00:20:14.150 --> 00:20:17.050
And we implemented that back in Einstein.

00:20:17.250 --> 00:20:19.850
And we called that optimization level 1.

00:20:20.020 --> 00:20:21.690
You can hit the number 1.

00:20:21.950 --> 00:20:24.940
That'll switch it over to the
newly threaded code if you

00:20:25.090 --> 00:20:26.990
hit the spacebar after that.

00:20:27.280 --> 00:20:32.430
Well, you'll see it is running,
but time isn't ticking,

00:20:32.520 --> 00:20:33.840
or at least not very fast right now.

00:20:33.840 --> 00:20:39.680
And what happened is-- well,
let's see if it even goes anywhere.

00:20:39.760 --> 00:20:41.720
We know we have a problem.

00:20:51.400 --> 00:21:12.100
[Transcript missing]

00:21:12.260 --> 00:21:15.060
Our optimization wasn't really
a very good optimization,

00:21:15.060 --> 00:21:15.300
was it?

00:21:15.300 --> 00:21:17.720
We should take Shark again, right?

00:21:18.030 --> 00:21:19.970
Performance optimization
is an iterative process.

00:21:20.400 --> 00:21:21.880
Sometimes you hit, sometimes you miss.

00:21:21.880 --> 00:21:23.400
Looks like we missed pretty bad.

00:21:25.400 --> 00:21:30.140
Well, we took another time profile and,
well, things are definitely different.

00:21:30.160 --> 00:21:34.540
And what we see here is that a lot of the
functions are this maroon or red color,

00:21:34.540 --> 00:21:38.180
and those signify
supervisor space samples.

00:21:38.200 --> 00:21:40.940
So these are things inside the kernel.

00:21:40.940 --> 00:21:44.340
And we have a problem
with the time profile.

00:21:44.340 --> 00:21:48.230
We can't actually link a call stack
from user space into the kernel and

00:21:48.280 --> 00:21:52.330
through the kernel because there's a
breakpoint there when you enter the

00:21:52.390 --> 00:21:54.660
kernel with a normal time profile.

00:21:55.300 --> 00:21:57.930
So we want to figure out--

00:21:58.370 --> 00:22:00.870
What's going on here, right?

00:22:00.980 --> 00:22:03.260
How do we figure out how we're
getting into the kernel so

00:22:03.260 --> 00:22:04.740
often and what's taking time?

00:22:04.740 --> 00:22:07.990
Well,
that leads us to a new feature in Shark,

00:22:08.130 --> 00:22:09.810
which I'll let Nathan explain to you.

00:22:10.670 --> 00:22:11.620
Thank you, Sanjay.

00:22:11.680 --> 00:22:14.160
Can we have the slides again, please?

00:22:21.220 --> 00:22:25.690
So with Shark 4.2,
we're introducing a new way to use Shark,

00:22:25.770 --> 00:22:29.500
a new configuration,
and that is system trace.

00:22:29.590 --> 00:22:33.660
And so what this does,
this is going to give you a

00:22:33.660 --> 00:22:37.060
way to understand how your code
is interacting with Mac OS X,

00:22:37.130 --> 00:22:39.500
to look at multi-threaded behavior.

00:22:39.560 --> 00:22:46.350
And so the goal here is just to get
an eye into these things that we

00:22:46.350 --> 00:22:49.000
can't normally see with time profile.

00:22:49.500 --> 00:22:52.930
So the methodology is an exact
trace of boundary crossing.

00:22:53.000 --> 00:22:54.840
So that's a lot different
than time profile,

00:22:54.840 --> 00:22:56.750
which is a statistical, right?

00:22:56.760 --> 00:22:59.480
It's taking--it's, you know,
every millisecond it's

00:22:59.480 --> 00:23:00.400
recording a sample.

00:23:00.580 --> 00:23:03.010
This is going to record a
call stack every time we enter

00:23:03.010 --> 00:23:04.390
the kernel from user space.

00:23:04.400 --> 00:23:07.400
And it doesn't require
any instrumentation.

00:23:07.400 --> 00:23:09.400
It's all going to just
happen automatically.

00:23:09.400 --> 00:23:12.310
It's system-wide.

00:23:12.520 --> 00:23:14.540
And it provides three new views.

00:23:14.540 --> 00:23:17.050
The first is a summary view,
and we'll go over each

00:23:17.080 --> 00:23:20.040
of these in more detail,
but the basic view is a summary view

00:23:20.040 --> 00:23:22.300
that's going to show you overall
what's going on in the system.

00:23:22.300 --> 00:23:26.210
Trace view, which is going to give you a
list of all the trace records,

00:23:26.250 --> 00:23:29.080
so things we're looking
at here are system calls,

00:23:29.080 --> 00:23:32.060
VM faults,
when your threads were scheduled.

00:23:33.030 --> 00:23:35.590
And timeline view,
and this is a visualization

00:23:35.590 --> 00:23:38.480
of the threads in your system,
so you can see the

00:23:38.530 --> 00:23:42.420
threads running over time,
and we'll talk about each of these.

00:23:42.420 --> 00:23:44.810
So the summary view,
this is all about the

00:23:45.090 --> 00:23:47.060
biggest piece of the pie,
right?

00:23:47.060 --> 00:23:50.000
So here we have a pie chart, you know,
where were we spending time?

00:23:50.000 --> 00:23:51.520
Was it in user space?

00:23:51.520 --> 00:23:56.480
Was it calling into the system calls,
handling VM faults, interrupts,

00:23:56.480 --> 00:23:57.290
and so on?

00:23:57.320 --> 00:24:01.610
And underneath the pie chart,
we have a summary view.

00:24:02.520 --> 00:24:04.610
So this is summarizing
each aspect of the system.

00:24:04.620 --> 00:24:06.030
So first is scheduler.

00:24:06.080 --> 00:24:09.730
So, you know,
a lot of times it'd be nice to know

00:24:09.890 --> 00:24:11.510
how your threads are getting scheduled.

00:24:11.570 --> 00:24:13.800
You know, how often are they,
how long are they running for,

00:24:13.800 --> 00:24:17.390
at what priority, you know,
how much time do they get to actually run

00:24:17.390 --> 00:24:20.170
on the CPU before they're suspended and,
you know, and something else is

00:24:20.180 --> 00:24:20.980
run by the scheduler.

00:24:21.000 --> 00:24:23.430
Well,
that's what summary view is going to do.

00:24:23.440 --> 00:24:27.640
And so you can use this as
kind of a good check to say,

00:24:27.640 --> 00:24:31.300
well, if I'm expecting a CPU bound thread
and it's only getting scheduled for,

00:24:31.300 --> 00:24:35.230
you know, a few microseconds, and I know,
you know, on Mac OS X,

00:24:35.230 --> 00:24:38.410
and I'll tell you that it's, you know,
you should expect in the range, you know,

00:24:38.410 --> 00:24:41.710
of tens of milliseconds if
you're actually CPU bound.

00:24:41.720 --> 00:24:44.560
You should actually be on the
CPU before you get kicked off.

00:24:44.660 --> 00:24:47.840
And we'll talk more about how that works.

00:24:47.840 --> 00:24:49.910
System calls.

00:24:50.090 --> 00:24:55.480
So this is going to provide you a profile
of the system calls in the system.

00:24:55.480 --> 00:24:59.720
So it's going to give you, you know,
it gives you the ability to

00:24:59.720 --> 00:25:01.640
attribute system call time.

00:25:01.640 --> 00:25:02.240
So it's going to give you the
ability to attribute system

00:25:02.240 --> 00:25:02.720
call time to user space code.

00:25:02.720 --> 00:25:05.990
So it means it records the
user space call stack for,

00:25:05.990 --> 00:25:08.750
and it links it to a
specific system call.

00:25:08.870 --> 00:25:10.850
So if you spend a lot of
time in a system call,

00:25:10.850 --> 00:25:12.990
you can see, oh,
this piece of code actually

00:25:13.020 --> 00:25:14.370
called that in my code.

00:25:16.160 --> 00:25:23.700
Similarly, for VM faults,
when you're taking virtual memory faults,

00:25:23.700 --> 00:25:24.980
these are otherwise transparent.

00:25:24.980 --> 00:25:26.540
They're handled by the kernel.

00:25:26.560 --> 00:25:30.020
And this will allow you to say, oh,
this piece of code was actually

00:25:30.020 --> 00:25:34.850
triggering all those VM faults that maybe
you see in VM stat or some other tool.

00:25:34.860 --> 00:25:38.240
You can actually pin these
back to your source code.

00:25:39.890 --> 00:25:40.750
And of course, TraceView.

00:25:40.930 --> 00:25:43.200
This is just the complete
list of all the trace records.

00:25:43.200 --> 00:25:47.300
You can look at all the call stacks,
the arguments to the system calls.

00:25:47.340 --> 00:25:52.300
But best of all,
this supports hierarchical sorting,

00:25:52.300 --> 00:25:56.810
so that's a useful technique for
finding perhaps maybe the system

00:25:56.880 --> 00:26:00.300
calls or a specific type of system
call that took the most time.

00:26:00.300 --> 00:26:02.870
You can double-click on one of those,
and you can go to that

00:26:02.870 --> 00:26:04.300
event in the Timeline View.

00:26:05.040 --> 00:26:09.840
And this view gives you a
way to navigate this trace.

00:26:09.840 --> 00:26:12.240
So you can interactively zoom and scroll.

00:26:12.240 --> 00:26:13.540
You can click on an event.

00:26:13.540 --> 00:26:15.680
In this case, this is a BSD system call.

00:26:15.680 --> 00:26:18.970
And it's going to give you
an inspector to look at that,

00:26:19.010 --> 00:26:21.680
you know, all the details about that
particular system call.

00:26:21.680 --> 00:26:24.260
You can filter it based
on a variety of things,

00:26:24.260 --> 00:26:28.860
you know, system process thread,
a lot like the normal

00:26:28.860 --> 00:26:34.030
time profile per CPU,
per event type.

00:26:34.890 --> 00:26:38.740
You can also, you know,
color these threads based on, you know,

00:26:38.740 --> 00:26:44.040
various parameters, the CPU they ran on,
priority, you know,

00:26:44.120 --> 00:26:45.740
the reason that they were
switched off the CPU.

00:26:45.740 --> 00:26:50.110
Just so you can get a good overview
of why things are happening

00:26:50.110 --> 00:26:52.500
the way they are in the system.

00:26:53.620 --> 00:26:56.360
So, you know,
what are all these little pictures?

00:26:56.450 --> 00:26:59.280
Well, these rectangles,
these are the thread run intervals.

00:26:59.480 --> 00:27:03.730
So this is the duration of
time that a thread ran for.

00:27:04.660 --> 00:27:08.220
And also we can see within the thread,
there are system calls,

00:27:08.220 --> 00:27:10.150
these little telephone icons.

00:27:10.150 --> 00:27:13.400
And there's a black line
underneath the system call,

00:27:13.400 --> 00:27:16.280
and that indicates the time that the
system call took within that thread.

00:27:16.280 --> 00:27:20.540
Similarly for VM faults,
these page icons,

00:27:20.540 --> 00:27:24.400
this is the time that it took to
handle this virtual memory fault.

00:27:24.420 --> 00:27:29.900
So that's what these, you know,
the system events that we're looking at.

00:27:30.520 --> 00:27:33.120
So why, you know,
a lot of you may wonder, well, you know,

00:27:33.120 --> 00:27:35.930
why would my thread not run
for the full amount of time?

00:27:35.940 --> 00:27:38.240
You know, well, you know,
if it's a CPU-bound thread,

00:27:38.240 --> 00:27:40.640
truly CPU-bound, well,
it will probably run for the full

00:27:41.140 --> 00:27:44.410
allotted time slice or quantum
before the kernel will kick

00:27:44.410 --> 00:27:47.840
it off and run something else,
assuming it has something else to run.

00:27:49.470 --> 00:27:52.220
Another possibility is that
it's waiting on a resource,

00:27:52.220 --> 00:27:57.280
so disk or some other thing
that isn't available right away.

00:27:57.280 --> 00:28:01.400
The kernel is going to pick another
thread to run while this one is waiting,

00:28:01.430 --> 00:28:02.630
so that's blocked.

00:28:02.700 --> 00:28:07.120
It's also possible that while you
were running a higher priority thread,

00:28:07.120 --> 00:28:10.660
like a driver's,
an interrupt handler thread,

00:28:10.660 --> 00:28:12.350
was made runnable.

00:28:12.360 --> 00:28:14.540
And so you're going to get this,
the currently running

00:28:14.540 --> 00:28:16.190
thread will be preempted,
and another,

00:28:16.190 --> 00:28:18.120
this higher priority thread will be run.

00:28:21.740 --> 00:28:24.580
So a little more about system calls.

00:28:24.580 --> 00:28:28.060
This is work done by the kernel
on behalf of the calling process.

00:28:28.060 --> 00:28:31.190
And it's really just a safe way
for user space programs to call

00:28:31.610 --> 00:28:35.460
into these various kernel APIs,
BSD, Mach, MIG messages.

00:28:35.460 --> 00:28:39.370
The distinction between these
isn't all terribly important,

00:28:39.370 --> 00:28:43.370
but there's a variety of APIs here,
just to keep that in mind.

00:28:44.920 --> 00:28:46.890
The first one is virtual memory fault.

00:28:46.960 --> 00:28:49.580
So Mac OS X obviously has a
powerful virtual memory system.

00:28:49.580 --> 00:28:52.610
And what that's doing,
it's dividing the overall

00:28:52.610 --> 00:28:56.600
address space into pages,
so into equal size chunks.

00:28:56.680 --> 00:29:00.260
And these are brought into
physical memory on demand,

00:29:00.260 --> 00:29:02.740
of course,
because we want to be able to have more

00:29:02.740 --> 00:29:06.530
applications running than we actually
have physical memory to contain them all,

00:29:06.540 --> 00:29:10.210
potentially, or for the data that an
application is operating on.

00:29:10.220 --> 00:29:13.120
And there are a wide
variety of these faults.

00:29:13.700 --> 00:29:17.520
The simplest, page in and page out,
just shuffling things around

00:29:17.520 --> 00:29:19.450
for the currently used pages.

00:29:19.520 --> 00:29:22.370
Zero fill is,
these occur when you malloc or

00:29:22.370 --> 00:29:24.800
allocate a large chunk of memory.

00:29:24.940 --> 00:29:27.670
The kernel is going to mark those
as to be zero filled on demand.

00:29:27.720 --> 00:29:30.640
And so the first time
you touch those pages,

00:29:30.690 --> 00:29:33.680
they're going to be, the kernel will,
you'll interrupt,

00:29:33.680 --> 00:29:35.270
it'll go into the kernel,
and the kernel will zero

00:29:35.270 --> 00:29:36.390
that page out automatically.

00:29:36.420 --> 00:29:39.650
Copy on write happens when
you're sharing a page,

00:29:39.650 --> 00:29:41.480
you modify a shared page.

00:29:41.580 --> 00:29:43.640
So if you're sharing a
page between two processes,

00:29:43.730 --> 00:29:46.050
and you modify it,
it's going to make you your

00:29:46.050 --> 00:29:48.670
own personal copy of that page.

00:29:49.240 --> 00:29:50.030
And page cache hit.

00:29:50.140 --> 00:29:53.410
The page is resonant in the buffer cache,
but it's not yet mapped

00:29:53.410 --> 00:29:56.360
into your application,
so it's going to take a fault.

00:29:56.460 --> 00:29:58.030
A fault, you know,
it takes some time to get

00:29:58.030 --> 00:30:01.890
in and out of the kernel,
so it's good to be aware of these.

00:30:02.200 --> 00:30:05.390
So this is going to give you a
way to actually see these things.

00:30:05.470 --> 00:30:08.640
So for a demo of that,
let's bring Sanjay up again,

00:30:08.640 --> 00:30:11.500
and we'll see how this works.

00:30:15.440 --> 00:30:18.260
Okay,
so when we last left off with Einstein,

00:30:18.300 --> 00:30:23.640
we had this whole time profile full
of system calls and supervisor space

00:30:23.900 --> 00:30:25.190
time that we can't account for.

00:30:25.200 --> 00:30:27.340
So what we want to do is,
and you can see that

00:30:27.610 --> 00:30:29.030
Einstein is still running.

00:30:29.030 --> 00:30:30.390
It's horribly slow.

00:30:30.400 --> 00:30:37.150
If we go to the config menu in Shark,
we can just select system trace and

00:30:37.150 --> 00:30:41.330
then either hit start or option escape
and that will start taking a trace.

00:30:42.680 --> 00:30:44.390
Okay, so it's going,
and you can see it actually

00:30:44.440 --> 00:30:45.260
stopped rather quickly.

00:30:45.260 --> 00:30:47.660
That's because a system
trace is truly a trace.

00:30:47.750 --> 00:30:49.260
It's not statistical sampling.

00:30:49.260 --> 00:30:51.260
So by default,
we're going to collect up to

00:30:51.260 --> 00:30:54.410
a million different events,
which is probably all you

00:30:54.480 --> 00:30:56.620
can handle at any given time.

00:30:56.640 --> 00:30:58.730
But if you're some kind
of super programmer,

00:30:58.830 --> 00:31:01.800
you can actually up that limit
inside the config editor.

00:31:01.800 --> 00:31:05.420
In this case,
so what we're seeing is the summary view,

00:31:05.420 --> 00:31:09.580
which Nathan explained,
and it gives you a pie chart.

00:31:10.320 --> 00:31:13.520
And it's breaking down time
in terms of user space versus

00:31:13.520 --> 00:31:15.280
system space versus idle.

00:31:15.280 --> 00:31:18.870
And it's further breaking down
supervisor space into faults,

00:31:18.870 --> 00:31:20.490
kernel, and interrupts.

00:31:20.580 --> 00:31:24.770
And in our case with Einstein here,
you can see we're getting about

00:31:24.770 --> 00:31:28.860
45% of the time in user space,
so that's executing Einstein code.

00:31:28.860 --> 00:31:31.470
But at this point,
we're getting almost a quarter of

00:31:31.570 --> 00:31:33.390
the time inside of system calls.

00:31:33.440 --> 00:31:36.180
And that's this orange
section of the pie.

00:31:36.180 --> 00:31:40.940
And down here in the scheduler,
you can see... These are the

00:31:40.940 --> 00:31:42.950
Einstein threads underneath CFS Boink.

00:31:43.020 --> 00:31:47.220
You can see that the average thread
is running for a few microseconds.

00:31:47.220 --> 00:31:50.340
In this case, about 60, 63 microseconds.

00:31:50.850 --> 00:31:55.810
And if we look at system calls...

00:31:57.100 --> 00:31:59.140
So remember,
this is an exact trace of all the system

00:31:59.140 --> 00:32:01.960
calls made during our sampling period.

00:32:01.960 --> 00:32:03.930
You'll notice there are
a lot of VM deallocates,

00:32:04.090 --> 00:32:06.160
so that's virtual memory deallocating.

00:32:06.160 --> 00:32:09.040
There's virtual memory mapping
and thread create running.

00:32:09.040 --> 00:32:12.060
You can option click on one
of these disclosure triangles,

00:32:12.060 --> 00:32:16.180
and it'll pop open the back trace to tell
you how you got into that system call.

00:32:16.180 --> 00:32:19.350
So in this case, you can see all the way
from pthreadbody up,

00:32:19.350 --> 00:32:22.380
we end up in that LALDmod function,
which remember when we

00:32:22.400 --> 00:32:24.760
added our threading,
that was the name of the

00:32:24.760 --> 00:32:26.760
function that we were calling.

00:32:27.000 --> 00:32:30.760
And it's calling pthreadcreate,
which we showed you in the source,

00:32:30.760 --> 00:32:33.620
and then ending up, of course,
inside of a system call.

00:32:33.620 --> 00:32:37.240
Now let's take a look at this
inside the timeline view as well.

00:32:37.240 --> 00:32:40.680
So by default,
what you get is on the horizontal axis,

00:32:40.680 --> 00:32:43.720
the x-axis is time,
and you have a full list of all

00:32:43.830 --> 00:32:47.780
the threads along the y-axis
that are going on in the system.

00:32:47.780 --> 00:32:51.990
And at this level, well,
it's just a bunch of yellow rectangles.

00:32:52.250 --> 00:32:56.980
You can zoom in either by rubber
banding or by using the zoom slider.

00:32:57.000 --> 00:33:01.820
And if you zoom in far enough,
about two-thirds of the way through,

00:33:02.380 --> 00:33:04.800
probably, the icons will start to appear.

00:33:05.060 --> 00:33:07.920
And in this case, we see a whole bunch
of little telephones.

00:33:07.960 --> 00:33:13.280
Those are all calls, system calls.

00:33:13.280 --> 00:33:13.280
If you click on a phone,

00:33:13.600 --> 00:34:10.900
[Transcript missing]

00:34:13.700 --> 00:34:56.500
[Transcript missing]

00:34:56.910 --> 00:35:00.520
or sorry, down to test LALD mod.

00:35:00.520 --> 00:35:04.190
You'll see how we implemented
this inside of the Einstein code.

00:35:04.710 --> 00:35:09.420
So again, there's a pthread create
call and pthread join call.

00:35:09.460 --> 00:35:12.870
But in this case,
we're calling a different function,

00:35:12.870 --> 00:35:14.360
LALDmodOptimized.

00:35:14.360 --> 00:35:17.070
And if you jump over to that function,

00:35:18.200 --> 00:35:41.000
[Transcript missing]

00:35:41.100 --> 00:35:49.700
[Transcript missing]

00:35:49.800 --> 00:36:11.800
[Transcript missing]

00:36:17.830 --> 00:36:20.810
But we're not done, of course.

00:36:20.860 --> 00:36:22.200
You know, we're performance guys.

00:36:22.200 --> 00:36:24.280
We can't stop now.

00:36:24.650 --> 00:36:26.670
What we want to do is
take another time profile.

00:36:26.770 --> 00:36:27.780
We've threaded this thing.

00:36:27.780 --> 00:36:29.890
Let's see how it looks now,
and let's see if we can

00:36:29.890 --> 00:36:31.520
attack another problem.

00:36:32.900 --> 00:36:35.260
So again,
if you hit spacebar in Einstein,

00:36:35.260 --> 00:36:37.260
it will take a time profile.

00:36:37.260 --> 00:36:40.410
And then you start up
Shark with a hotkey.

00:36:40.900 --> 00:38:00.200
[Transcript missing]

00:38:00.700 --> 00:38:36.200
[Transcript missing]

00:38:36.400 --> 00:38:50.300
[Transcript missing]

00:38:51.200 --> 00:39:37.900
[Transcript missing]

00:39:38.040 --> 00:39:40.770
You can see we've just stamped
out code in terms of four to

00:39:40.770 --> 00:39:42.510
help fill the pipeline slots.

00:39:42.510 --> 00:39:46.390
And rather than using division,
we're using this intrinsic FRES to

00:39:46.390 --> 00:39:49.340
generate a reciprocal estimate.

00:39:49.380 --> 00:39:52.270
And if we go back to Einstein--

00:39:53.700 --> 00:40:05.700
[Transcript missing]

00:40:06.700 --> 00:40:20.700
[Transcript missing]

00:40:25.290 --> 00:40:28.680
So as expected,
our top function is still our hot loop.

00:40:28.880 --> 00:40:32.890
And what we really want to do,
if we double click on that again,

00:40:32.960 --> 00:40:34.640
see its hot spots.

00:40:34.680 --> 00:40:37.550
OK, so now you'll see our new
code is being highlighted.

00:40:37.580 --> 00:40:39.300
It's unrolled.

00:40:39.460 --> 00:40:42.540
Shark still has some advice there.

00:40:43.330 --> 00:40:44.940
Well, you're doing some single precision.

00:40:45.130 --> 00:40:46.310
Why don't you try using Altevec?

00:40:46.380 --> 00:40:51.410
Now, the interesting thing about this
loop and about the Einstein app

00:40:51.490 --> 00:40:54.560
is that it's using some doubles
and some singles within the loop.

00:40:54.620 --> 00:40:58.860
And we wanted to preserve correctness,
so we decided to attack this loop in

00:40:58.860 --> 00:41:02.970
a couple of different steps because
we don't want to detect gravitational

00:41:03.170 --> 00:41:04.940
waves where there are none.

00:41:08.030 --> 00:41:11.190
So we looked at this and we said, okay,
what we need to do is break out all the

00:41:11.350 --> 00:41:15.300
single precision computation that we
can so we can use vector instructions.

00:41:15.300 --> 00:41:18.720
Now, some of you may be saying that's
a little crazy at this point.

00:41:20.730 --> 00:41:24.330
No, it really isn't because we're
still shipping these Macs.

00:41:24.370 --> 00:41:27.060
These Macs are going to
ship for a long time.

00:41:27.060 --> 00:41:30.740
And even after we stop shipping
G5s or G4s with Altevec,

00:41:30.740 --> 00:41:34.060
those machines are going
to be out there for years.

00:41:35.160 --> 00:41:36.660
And beyond that,
if you attended the Altevec event,

00:41:36.660 --> 00:41:36.660
you're going to see a
lot of different things.

00:41:37.720 --> 00:41:38.150
And beyond that,
if you attended the Altevec

00:41:38.170 --> 00:41:40.800
to SSE session earlier today,
you'll know that once you make

00:41:40.900 --> 00:41:44.920
the step to vectorization,
the porting from one instruction set

00:41:45.070 --> 00:41:47.080
to the other is not really that hard.

00:41:48.080 --> 00:41:49.460
The hard work is getting
there in the first place,

00:41:49.530 --> 00:41:51.150
making sure your data is in
the right order and so on.

00:41:53.170 --> 00:41:56.350
But what we had to do in order
to write any vector code,

00:41:56.530 --> 00:41:59.140
first of all, was get it down to single
precision in a block.

00:41:59.140 --> 00:42:04.350
And if we look at our
source back in Xcode,

00:42:05.000 --> 00:42:26.900
[Transcript missing]

00:42:28.300 --> 00:42:34.180
And if you hit the space bar,
we hit another big oops.

00:42:34.500 --> 00:42:38.610
Again, time is ticking off and not
updating very quickly because

00:42:38.670 --> 00:42:39.590
we're not doing a lot of work.

00:42:39.670 --> 00:42:41.500
So again, something went wrong.

00:42:41.500 --> 00:42:46.810
And what we should do is
take a Shark time profile.

00:42:49.310 --> 00:42:54.040
And once again,
we have a whole window full of red stuff.

00:42:54.040 --> 00:42:55.700
What do you do about that?

00:42:55.810 --> 00:42:59.760
Well, time profile isn't too
informative in this case because,

00:42:59.880 --> 00:43:03.600
again, you can't track all the way
through from user to kernel space.

00:43:03.600 --> 00:43:06.200
What you want to do is
system trace in this case.

00:43:06.200 --> 00:43:09.530
So Nathan switched over to system trace.

00:43:09.640 --> 00:43:12.110
We can take another profile.

00:43:17.500 --> 00:43:39.900
[Transcript missing]

00:43:40.570 --> 00:43:46.190
Again, we'll zoom in until we can
see our whole bunch of icons.

00:43:46.700 --> 00:44:21.600
[Transcript missing]

00:44:21.930 --> 00:44:27.590
If we go back to the source, however,
we notice that

00:44:27.910 --> 00:44:31.980
What we could do is again move that
malloc and free operation outside

00:44:31.980 --> 00:44:36.440
of the loops to basically where the
thread is created and destroyed,

00:44:36.500 --> 00:44:41.360
essentially creating a static
buffer for this operation rather

00:44:41.360 --> 00:44:45.640
than having a very dynamic,
too dynamic of a buffer.

00:44:45.730 --> 00:44:48.080
So we made that optimization.

00:44:48.680 --> 00:44:54.440
And if we go back to Einstein,
call that level 5, you'll see that,

00:44:54.440 --> 00:44:58.260
again, things have picked up pretty well.

00:45:00.970 --> 00:45:04.890
If we run it again,
we'll get a feel for how much

00:45:04.970 --> 00:45:06.660
things have changed from before.

00:45:06.720 --> 00:45:10.130
Now, we're not expecting a lot
from optimization level 3.

00:45:10.140 --> 00:45:13.950
This is really just scaffolding
in place so we can use vector

00:45:13.950 --> 00:45:16.480
instructions at the next step.

00:45:16.630 --> 00:45:19.970
Now, if we profile once
more with time profile,

00:45:27.940 --> 00:45:29.800
You'll notice again,
we're back in user space.

00:45:29.800 --> 00:45:32.270
So we've eliminated all that
problem of making system

00:45:32.330 --> 00:45:34.500
calls and faulting in pages.

00:45:34.630 --> 00:45:37.910
And we can once again
click on our top function.

00:45:38.100 --> 00:45:40.240
You'll notice now we're
in this new function.

00:45:40.250 --> 00:45:43.180
We still have the AlteVic
advice from Shark.

00:45:44.860 --> 00:45:47.940
and we decided, yeah,
we could write Altevec for this loop.

00:45:48.000 --> 00:45:48.660
It's pretty small.

00:45:48.660 --> 00:45:52.290
It's a good investment in time for
what we're going to achieve here

00:45:52.300 --> 00:45:55.950
because we expect Einstein to be
running on a few million computers as

00:45:55.950 --> 00:45:58.150
screensavers for several man years.

00:45:58.150 --> 00:46:03.320
We could save the world a few gigawatts
by writing the most efficient code.

00:46:04.100 --> 00:46:26.600
[Transcript missing]

00:46:27.300 --> 00:46:50.900
[Transcript missing]

00:46:54.390 --> 00:46:56.460
You see we ended up down
at about five seconds.

00:46:56.570 --> 00:47:01.700
So remember, we started with our scalar
code was about nine seconds.

00:47:01.750 --> 00:47:06.520
So we got it to be about 80%
faster using vector instructions.

00:47:13.640 --> 00:47:21.210
And if we go back to our slides,
let's summarize what we

00:47:21.210 --> 00:47:22.950
experienced here with Einstein.

00:47:23.040 --> 00:47:27.430
So this, I'm not going to lie about this,
this porting or this optimization

00:47:27.430 --> 00:47:31.110
process went on for a few days,
probably a few extra days than

00:47:31.130 --> 00:47:33.120
my management knows about.

00:47:35.380 --> 00:47:48.500
But it wasn't too bad.

00:47:48.500 --> 00:47:48.540
You can see here at optimization level
one was where we screwed things up.

00:47:48.540 --> 00:47:48.540
We had our threads that were just
operating on a very small chunk of

00:47:48.540 --> 00:47:48.540
data and we were paying a heavy,
heavy price.

00:47:48.700 --> 00:47:59.600
[Transcript missing]

00:48:00.240 --> 00:48:03.730
We unrolled and took out some division,
picked up another couple of,

00:48:03.740 --> 00:48:06.980
or about 20% more than step two.

00:48:07.290 --> 00:48:12.480
At optimization level 4, we again,
we blew it.

00:48:12.520 --> 00:48:16.310
We added that malloc and we ended
up spending tons of time inside

00:48:16.410 --> 00:48:18.480
of VM faults and system calls.

00:48:18.540 --> 00:48:20.340
We fixed that again.

00:48:20.410 --> 00:48:22.020
We got to vectors.

00:48:22.090 --> 00:48:25.120
And then we did even
more vectorization work.

00:48:25.170 --> 00:48:28.530
And we ended up about 3 and 1/2
times faster than the original code,

00:48:28.740 --> 00:48:30.560
which is not to say that
you can't do any more.

00:48:30.740 --> 00:48:36.120
We just kind of came up
against the deadline for WWDC.

00:48:37.410 --> 00:48:39.680
But there is one more thing.

00:48:39.680 --> 00:48:43.120
I'll do my best SJ impersonation.

00:48:43.670 --> 00:48:47.380
So we made a little
announcement on Monday.

00:48:47.380 --> 00:48:51.100
And you may be wondering,
what's the state of Shark running

00:48:51.100 --> 00:48:54.560
on the developer transition
boxes and on future boxes?

00:48:54.560 --> 00:48:57.360
Well,
you'll be happy to know it is running.

00:48:57.360 --> 00:48:59.560
If you've been down to the labs,
you may have used it already.

00:48:59.560 --> 00:49:03.000
We have basic time profiling,
because that's obviously the most

00:49:03.160 --> 00:49:05.160
common workflow for most of you.

00:49:05.160 --> 00:49:07.850
We also have performance event profiling.

00:49:07.850 --> 00:49:12.140
So that means we can look at performance
monitor counters on the chip.

00:49:12.680 --> 00:49:16.060
So special hardware
registers inside the Pentium,

00:49:16.060 --> 00:49:19.890
you can configure those and use
those for generating hardware

00:49:19.890 --> 00:49:21.610
events like cache misses and so on.

00:49:23.680 --> 00:49:24.370
Cross-platform.

00:49:24.540 --> 00:49:27.350
So you can take a session on
one of those developer boxes,

00:49:27.480 --> 00:49:30.550
save it,
and then open it back on a non-developer

00:49:30.550 --> 00:49:33.450
transition system or vice versa.

00:49:34.780 --> 00:49:41.220
We do have code analysis and optimization
tips for x86 or Intel 32 architecture.

00:49:42.420 --> 00:49:44.950
And probably most important for those
of you who are getting down to the

00:49:44.950 --> 00:49:50.170
low level and coming from power PCs,
we have a disassembler

00:49:50.310 --> 00:49:54.630
which chooses your syntax,
either Intel syntax or AT&T syntax.

00:49:54.690 --> 00:49:58.950
A little explanation there,
AT&T is the syntax that GCC uses.

00:49:59.020 --> 00:50:01.510
Intel syntax is the
official Intel syntax.

00:50:01.510 --> 00:50:02.620
Don't ask me why.

00:50:02.620 --> 00:50:04.550
They're so different, but they are.

00:50:04.630 --> 00:50:06.860
But you can choose
whichever one you like.

00:50:09.040 --> 00:50:11.620
And perhaps best of all,
we have an integrated

00:50:11.620 --> 00:50:13.170
instruction reference guide.

00:50:13.170 --> 00:50:17.620
So we recently got approval from
Intel Legal to get this done.

00:50:17.620 --> 00:50:24.750
And probably the best way to show
this off is to go to Demo Machine 3,

00:50:24.750 --> 00:50:26.210
which is our--

00:50:26.500 --> 00:50:43.420
and the developer transition system.

00:50:43.420 --> 00:50:43.420
And you can see Shark is
already launched.

00:50:43.420 --> 00:50:43.420
We have Xcode here.

00:50:43.420 --> 00:50:43.420
If any of you saw our show back in 2002,
you know the work we did with Flurry.

00:50:43.420 --> 00:50:43.420
That's a screen saver.

00:50:43.420 --> 00:50:43.420
We'll launch that.

00:50:47.270 --> 00:50:51.360
We've added a little timer again
and put it into its own window so

00:50:51.380 --> 00:50:53.680
we could get repeatable results.

00:50:53.840 --> 00:50:56.850
You can see that's the flurry
you know and love and have

00:50:56.850 --> 00:50:58.710
seen over and over too much.

00:50:59.100 --> 00:51:23.600
[Transcript missing]

00:51:25.590 --> 00:51:29.740
It's just like a Mac, isn't it?

00:51:29.780 --> 00:51:31.810
You can then double-click
on the top function,

00:51:31.810 --> 00:51:34.010
in this case, Update Smoke.

00:51:34.170 --> 00:51:35.910
There's your source code.

00:51:36.120 --> 00:51:38.340
There's some advice here.

00:51:38.370 --> 00:51:40.900
In this case, integer division.

00:51:40.900 --> 00:51:43.600
In another case, floating point division.

00:51:43.650 --> 00:51:48.110
You can look at assembly or both.

00:51:49.200 --> 00:51:53.000
If you have a wide enough monitor,
this makes more sense.

00:51:53.000 --> 00:51:56.700
But there you see Intel instructions
running on Mac OS.

00:51:56.700 --> 00:51:58.940
Everything's highlighted.

00:51:59.000 --> 00:52:00.660
If you don't understand
Intel instructions,

00:52:00.660 --> 00:52:02.890
you click on ASIM help.

00:52:03.650 --> 00:52:07.980
And if you highlight a
particular line of your ASIM,

00:52:07.980 --> 00:52:10.840
the documentation updates with you.

00:52:10.970 --> 00:52:13.710
So hopefully that'll help you.

00:52:13.700 --> 00:52:17.100
Get up to speed on speaking x86 if
you're tuning down at this level.

00:52:17.100 --> 00:52:18.940
It was certainly a big help for us.

00:52:29.540 --> 00:52:32.480
So it wouldn't be enough
just to take a profile.

00:52:32.480 --> 00:52:37.480
We actually wrote SSE2
code to optimize Flurry.

00:52:37.480 --> 00:52:40.730
And again, remember,
we had written AlteVec code.

00:52:40.730 --> 00:52:41.700
And you can get this.

00:52:41.700 --> 00:52:42.880
This is a fat binary.

00:52:42.880 --> 00:52:44.960
It comes with our package,
which you download.

00:52:44.960 --> 00:52:47.670
It's got both the AlteVec
code and the SSE2 code.

00:52:47.670 --> 00:52:49.810
So you can see exactly how we did this.

00:52:49.810 --> 00:52:54.630
I can tell you that the porting process
from AlteVec to SSE2 was probably about

00:52:54.750 --> 00:52:59.180
one-tenth the time that it took us to
write the initial vector implementation.

00:52:59.400 --> 00:53:06.260
But if we go back to Flurry and run
with vector optimizations in place,

00:53:08.390 --> 00:53:11.300
Things go a bit quicker.

00:53:11.400 --> 00:53:12.480
So this is SSE2.

00:53:12.480 --> 00:53:17.050
It can do four single precision
floats just like AlteVec.

00:53:17.110 --> 00:53:18.540
It's a 128-bit vector.

00:53:18.720 --> 00:53:24.700
So we got about a 4x speed up here
from 10 seconds down to about 2.6.

00:53:24.800 --> 00:53:27.370
So it can be done.

00:53:27.680 --> 00:53:29.560
And believe me,
I've written a lot of AlteVec,

00:53:29.670 --> 00:53:31.790
so this was a big change for me.

00:53:31.940 --> 00:53:34.790
But it wasn't that bad.

00:53:41.300 --> 00:53:42.850
We can go back to slides now.

00:53:42.900 --> 00:53:49.490
So let's summarize what
we've talked about here.

00:53:50.670 --> 00:53:55.540
Hopefully you can see that Shark is
not too hard to get used to,

00:53:55.690 --> 00:53:56.280
to get running.

00:53:56.280 --> 00:53:59.140
You hit the start button
and for most of you,

00:53:59.140 --> 00:54:02.680
that's probably all you'll ever have
to do with it except fix your own code.

00:54:02.680 --> 00:54:07.280
We have a lot of different workflows
which Nathan walked through a

00:54:07.440 --> 00:54:09.380
lot of those different paths.

00:54:09.460 --> 00:54:12.710
Basically time profile and system
trace we think are going to be

00:54:12.740 --> 00:54:16.720
your major weapons of choice
to solve performance problems.

00:54:16.720 --> 00:54:19.770
But we have a whole host of other
configs and you can obviously

00:54:19.880 --> 00:54:21.640
create your own custom configs.

00:54:21.640 --> 00:54:27.270
We go down to the metal,
whatever metal that might be.

00:54:28.860 --> 00:54:32.590
And again,
Shud 4.2 preview is available right now

00:54:32.800 --> 00:54:36.020
off the developer.apple.com website.

00:54:36.020 --> 00:54:40.020
We just put out a package during WWDC,
so please download that, give it a whirl.

00:54:40.020 --> 00:54:43.560
Particularly if you're on
the developer transition box,

00:54:43.560 --> 00:54:46.090
we'd love to hear what your feedback is.

00:54:46.090 --> 00:54:49.780
And of course,
this doesn't cost you anything.

00:54:52.820 --> 00:54:59.380
For more information, of course,
the WWDC web page will give

00:54:59.400 --> 00:55:00.830
you links to our sample code.

00:55:00.830 --> 00:55:02.340
Everything we've shown
you here is available.

00:55:04.800 --> 00:55:08.940
And some other sessions coming up
right after this one is performance

00:55:09.040 --> 00:55:10.340
analysis of your memory code.

00:55:10.340 --> 00:55:15.000
There's a DevTools feedback forum
at 5:00 and debugging parallel

00:55:15.070 --> 00:55:20.860
applications for those of you
writing large networked applications.

00:55:20.960 --> 00:55:23.890
That should be a fun
session also at 5:00.

00:55:25.380 --> 00:55:28.840
If you need to contact us,
the best thing you can do

00:55:28.840 --> 00:55:30.260
is use PerfTools feedback.

00:55:30.260 --> 00:55:33.660
That goes to Nathan, myself,
and several other team members.

00:55:33.680 --> 00:55:39.370
Or Xavier Legro is our
evangelist and chief marketeer.

00:55:39.370 --> 00:55:41.600
Bug him for features.