WEBVTT

00:00:03.110 --> 00:00:05.180
Good morning everyone.

00:00:05.180 --> 00:00:07.390
Hope you're enjoying
the conference so far.

00:00:07.730 --> 00:00:09.300
My name is Sean Gies.

00:00:09.340 --> 00:00:12.450
I'm one of the video engineers in
the QuickTime team and I'd like

00:00:12.450 --> 00:00:17.390
to welcome you to our session on
High Performance Video in Mac OS X Tiger.

00:00:17.860 --> 00:00:20.400
So I see we've got some
pretty good turnout here.

00:00:20.400 --> 00:00:22.670
You'll be glad to know we're going
to be talking about a lot of the same

00:00:22.710 --> 00:00:25.300
things we covered last year at WWDC.

00:00:25.430 --> 00:00:27.270
So not going to miss anything.

00:00:27.300 --> 00:00:29.500
We're also going to try to answer
a lot of the questions that we've

00:00:29.500 --> 00:00:31.330
been receiving over the past year.

00:00:31.510 --> 00:00:34.240
So hope you enjoy.

00:00:34.790 --> 00:00:38.060
So before we get started
in the new video pipeline,

00:00:38.060 --> 00:00:41.360
let's talk about where
video was before Tiger.

00:00:41.430 --> 00:00:43.740
So this is our old rendering model.

00:00:43.740 --> 00:00:46.940
You see we have Quicktime,
the Image Compression Manager,

00:00:46.940 --> 00:00:48.020
and Codecs.

00:00:48.020 --> 00:00:50.020
They were all talking through Quickdraw.

00:00:50.020 --> 00:00:52.080
And in the APIs,
that would be the Graphport

00:00:52.080 --> 00:00:54.360
and the G-World and so forth.

00:00:54.360 --> 00:00:56.020
That worked pretty well.

00:00:56.030 --> 00:00:57.000
But it wasn't quite optimal.

00:00:57.200 --> 00:00:59.680
So this transfer codec was introduced.

00:00:59.760 --> 00:01:02.220
Now, this is a sneaky codec.

00:01:02.220 --> 00:01:05.580
It could see that G-World and go,
I can do better.

00:01:05.580 --> 00:01:08.620
I'll talk directly to the
hardware and get that video to

00:01:08.620 --> 00:01:12.760
the screen a little bit faster,
maybe support more native YCVCR pixel

00:01:12.760 --> 00:01:16.820
formats than you would have had
through the G-World normally.

00:01:16.830 --> 00:01:19.620
And this gave you a
little more performance.

00:01:19.620 --> 00:01:22.760
Unfortunately, it's not too flexible.

00:01:23.020 --> 00:01:25.900
The video would go straight to the
graphics hardware and there wasn't really

00:01:25.900 --> 00:01:30.680
a way for your application to get in
there and do interesting things with it.

00:01:30.830 --> 00:01:33.600
So we realized we had
to redesign this a bit.

00:01:33.690 --> 00:01:36.500
And when we did so, we had some goals.

00:01:36.560 --> 00:01:40.200
One of which was we needed to have
very good integration with OpenGL.

00:01:40.230 --> 00:01:43.200
Now OpenGL is our connection
to the graphics hardware.

00:01:43.240 --> 00:01:45.890
And it's how we can take advantage
of those new graphics processors

00:01:45.900 --> 00:01:50.900
and all the programmability and
just raw horsepower available.

00:01:50.960 --> 00:01:52.780
So that was one thing.

00:01:52.890 --> 00:01:57.060
We also wanted to have it to
be layered on top of Quartz.

00:01:57.060 --> 00:02:01.970
And this was sort of a way to
not marry QuickTime to OpenGL,

00:02:02.230 --> 00:02:04.780
you know, kind of how the way it's
been married to QuickDraw.

00:02:04.780 --> 00:02:08.800
And it gives us another
layer that you can hook into.

00:02:09.280 --> 00:02:15.510
This is where things like Core Video and
Core Image technologies are.

00:02:15.510 --> 00:02:15.510
It allows QuickTime to
leverage those as well.

00:02:16.130 --> 00:02:18.660
Now we also wanted support
for multiple buffers.

00:02:18.770 --> 00:02:25.350
This was another important problem
with the old model where a codec would

00:02:25.350 --> 00:02:30.420
be decompressing into this buffer and
before it could start the next frame,

00:02:30.420 --> 00:02:33.880
it had to wait for the graphics
hardware to go and download all

00:02:34.140 --> 00:02:36.000
those pixels onto the graphics card.

00:02:36.000 --> 00:02:39.120
And there's this dead time in
between where the codec could not

00:02:39.120 --> 00:02:41.000
begin doing work on the next frame.

00:02:41.000 --> 00:02:44.220
So that was the thing we had to clear up.

00:02:44.730 --> 00:02:48.660
And architecturally,
we want to separate the decoding of

00:02:48.660 --> 00:02:50.600
video from the presentation of video.

00:02:50.600 --> 00:02:54.720
Now this is really where we allow
your applications to hook in,

00:02:54.720 --> 00:02:55.600
in between there.

00:02:55.600 --> 00:02:58.960
So in the old model,
when the codecs were finished

00:02:58.960 --> 00:03:01.500
decompressing the data,
well those pixels were

00:03:01.590 --> 00:03:02.600
already on the screen.

00:03:02.600 --> 00:03:05.580
I mean there was nowhere for you
to get in there and interrupt.

00:03:06.730 --> 00:03:11.220
So here's sort of a logical
view of this new pipeline.

00:03:11.320 --> 00:03:13.710
So we have QuickTime on
top sitting on Quartz,

00:03:13.900 --> 00:03:17.120
which is core graphics,
core image and core video,

00:03:17.120 --> 00:03:20.600
all of which talk through OpenGL to
get to the graphics hardware.

00:03:20.730 --> 00:03:24.010
So how about a little more
dynamic view of this pipeline,

00:03:24.010 --> 00:03:25.600
a little more useful?

00:03:25.600 --> 00:03:28.600
So you got the movie,
which has your video frames,

00:03:28.600 --> 00:03:31.600
and you have some graphics
hardware you need to get that to.

00:03:31.600 --> 00:03:36.860
So QuickTime provides the video
and we use OpenGL to render

00:03:36.860 --> 00:03:39.600
to the graphics hardware.

00:03:39.700 --> 00:03:41.600
So how do we get the
frames out of that movie?

00:03:41.680 --> 00:03:44.580
Well, we've introduced a new construct
called a Visual Context.

00:03:44.580 --> 00:03:47.860
Now this is like a tap into
the movie through which you

00:03:47.950 --> 00:03:49.790
can extract video frames.

00:03:50.100 --> 00:03:52.840
And you'll notice here there's this gap.

00:03:52.880 --> 00:03:57.000
We need to bridge the QuickTime APIs,
which are rather high level,

00:03:57.060 --> 00:03:58.530
over to the OpenGL APIs.

00:03:58.700 --> 00:04:00.910
They're completely different
styles of programming.

00:04:01.000 --> 00:04:03.920
So this is where we've
introduced Core Video.

00:04:03.980 --> 00:04:08.000
It connects these two
very different worlds.

00:04:08.190 --> 00:04:12.490
And it provides timing services so you
know when to draw and buffering services

00:04:12.880 --> 00:04:15.530
which contain what you're going to draw.

00:04:19.010 --> 00:04:23.020
And in this gap you can
continue to fill in things.

00:04:23.230 --> 00:04:25.760
So we have a place where
your application can hook in.

00:04:26.110 --> 00:04:29.170
You can do additional
OpenGL transformations or you

00:04:29.170 --> 00:04:31.840
can pull in Core Image and do,
you know, crazy effects.

00:04:31.900 --> 00:04:33.600
Now these things are optional.

00:04:33.770 --> 00:04:37.210
This is where you can do whatever
you want and it doesn't need to be

00:04:37.210 --> 00:04:41.490
there if your application is just,
you know, playing back video simply.

00:04:42.150 --> 00:04:45.060
So what does this give us?

00:04:45.130 --> 00:04:48.540
Well, there's a couple facets to
the performance enhancements.

00:04:48.570 --> 00:04:49.500
You have more pipelining.

00:04:49.500 --> 00:04:52.390
This is where the CPU has been freed
up so the codec can be working at

00:04:52.390 --> 00:04:55.520
the same time that the graphics
hardware is downloading pixels.

00:04:55.520 --> 00:04:59.190
So this becomes relative,
or actually very important for

00:04:59.190 --> 00:05:02.150
high definition video where
you have a lot more pixels.

00:05:02.160 --> 00:05:06.580
And the compression technology
takes a lot more time to compute.

00:05:07.640 --> 00:05:11.520
And because it's high definition
and there's so much more video,

00:05:11.590 --> 00:05:15.980
well that dead time while it's uploading
that video becomes even more important.

00:05:17.100 --> 00:05:21.310
The other facet has to do with
taking advantage of that GPU.

00:05:21.310 --> 00:05:25.000
So now that we have
OpenGL under the hood,

00:05:25.030 --> 00:05:28.460
under QuickTime,
it becomes very easy to leverage the

00:05:28.570 --> 00:05:31.760
GPU and do things like color sync,
do color matching in real time,

00:05:31.760 --> 00:05:35.280
adding filters and effects to the video,
or just fun new things that

00:05:35.510 --> 00:05:37.390
weren't really possible before.

00:05:39.290 --> 00:05:43.330
So, I realize there's a class of
applications who merely want

00:05:43.460 --> 00:05:44.800
to play back video efficiently.

00:05:44.800 --> 00:05:47.870
They want to play back that high
definition H.264 clip without

00:05:47.870 --> 00:05:49.200
dropping frames on the floor.

00:05:49.200 --> 00:05:52.380
And for that, it's very easy.

00:05:52.990 --> 00:05:57.490
Cocoa developers, if you adopt QtKit,
you can use the QtMovieView object.

00:05:57.640 --> 00:06:01.640
Carbon developers,
if you've adopted the high level toolbox,

00:06:01.640 --> 00:06:02.960
you can use H.I.MovieView.

00:06:03.120 --> 00:06:06.640
Now both of these views are implemented
to use the new video pipeline.

00:06:06.750 --> 00:06:09.390
So it's going to take advantage
of OpenGL and the GPU and

00:06:09.390 --> 00:06:10.580
all that kind of stuff.

00:06:10.580 --> 00:06:13.000
And it's pretty much free.

00:06:13.020 --> 00:06:14.560
With Cocoa,
you can do this stuff in Interface

00:06:14.680 --> 00:06:16.650
Builder and have it playing
back without writing any code.

00:06:18.450 --> 00:06:21.640
So, back to our diagram.

00:06:21.700 --> 00:06:24.200
The view basically takes
care of everything here.

00:06:24.240 --> 00:06:25.870
You give it a movie,
it does all the work of

00:06:26.000 --> 00:06:28.100
pulling out the frames,
dealing with the timing,

00:06:28.140 --> 00:06:31.930
managing buffers and then drawing
them to the screen for you.

00:06:32.130 --> 00:06:34.800
So with that,
I'd like to have a little demo showing

00:06:34.800 --> 00:06:37.000
you how QuickTime Player is doing this.

00:06:37.410 --> 00:06:39.100
Demo 1 please.

00:06:41.410 --> 00:06:45.020
All right,
so I'm going to open a movie here.

00:06:45.040 --> 00:06:47.700
This is an H.264 clip.

00:06:47.830 --> 00:06:52.170
It is 960 by 540.

00:06:56.250 --> 00:06:59.490
So QuickTime Player is a
Cocoa application now and it

00:06:59.490 --> 00:07:03.100
is merely a client of the Qt
MovieView object that I just mentioned.

00:07:03.100 --> 00:07:06.250
So QuickTime Player has no idea how
that video is getting to the screen,

00:07:06.250 --> 00:07:09.100
it's just relying on the
MovieView to do it for it.

00:07:09.100 --> 00:07:13.100
And we have this great
new live resizing feature.

00:07:13.100 --> 00:07:18.240
Now, the interesting thing here,
compared to before,

00:07:18.450 --> 00:07:22.100
the codec has no idea that I'm
changing the size of this window.

00:07:22.100 --> 00:07:26.100
It continues to decode frames at its
native resolution very efficiently.

00:07:26.210 --> 00:07:28.200
Whereas in the old model,
every time you needed

00:07:28.200 --> 00:07:32.100
to resize the video,
you tore down the codec chain,

00:07:32.100 --> 00:07:37.100
reinitialized it to decompress at the
new size and that was a lot of overhead.

00:07:37.100 --> 00:07:40.390
Now, only OpenGL knows that we
need to change the size of

00:07:40.390 --> 00:07:42.100
this to surface on the screen.

00:07:42.100 --> 00:07:46.590
And the codec just carries on
doing everything in real time.

00:07:47.640 --> 00:07:51.340
So we also have Color Sync
being used here.

00:07:51.400 --> 00:07:56.370
Now this video is tagged with the
information that describes the high

00:07:56.460 --> 00:08:01.430
definition TV color profile and our
display has been calibrated for the

00:08:01.540 --> 00:08:05.380
characteristics of this display and we're
getting Color Sync for free and it's

00:08:05.380 --> 00:08:07.970
all being color matched in real time.

00:08:08.740 --> 00:08:13.190
Now, since we're using the GPU,
we can do a few other things.

00:08:13.350 --> 00:08:16.820
Here's a little script that
makes this easier for me,

00:08:16.820 --> 00:08:21.970
which takes that movie and
creates a new one from it.

00:08:23.210 --> 00:08:25.570
This movie is the same
but it has two tracks now.

00:08:25.610 --> 00:08:30.100
It's that same clip
but below and flipped.

00:08:30.100 --> 00:08:34.100
So this work of compositing the
video is happening on the GPU.

00:08:34.100 --> 00:08:39.100
It has two streams of video frames coming
in and we have an off-screen buffer

00:08:39.100 --> 00:08:42.100
on the graphics card into which we're
rendering these two streams of video.

00:08:42.100 --> 00:08:45.980
And it's all happening in real
time and again the view doesn't

00:08:45.980 --> 00:08:48.750
really know it's happening,
it's just you get these larger

00:08:48.750 --> 00:08:50.100
textures out of the movie.

00:08:50.100 --> 00:08:55.740
And as well, on top of that,
we're using the GPU to do other

00:08:55.950 --> 00:08:59.910
operations like these video controls
where we're changing the color,

00:09:00.010 --> 00:09:03.100
saturation, contrast, tint, all that.

00:09:03.320 --> 00:09:07.590
And this is not affecting our CPU at
all because all the work is being

00:09:07.590 --> 00:09:10.100
done on that graphics processor.

00:09:10.100 --> 00:09:13.140
It's basically get that for free.

00:09:14.080 --> 00:09:18.230
And any application that you use QtKit,
you're going to get this behavior,

00:09:18.230 --> 00:09:20.200
you know, out of the box.

00:09:20.200 --> 00:09:23.690
These APIs are public,
you can tell the movie, you know,

00:09:23.690 --> 00:09:25.070
be twice as bright.

00:09:26.770 --> 00:09:30.700
All right, so that was a demo.

00:09:30.750 --> 00:09:33.340
QuickTime Player using QtKit.

00:09:33.450 --> 00:09:35.330
Back to the slides, please.

00:09:38.500 --> 00:09:41.150
All right,
so not all applications want to merely

00:09:41.250 --> 00:09:44.560
play video in a rectangle on the screen.

00:09:44.760 --> 00:09:47.320
Maybe you want to do some
additional video processing.

00:09:47.320 --> 00:09:49.140
You want to apply some
effects to that video,

00:09:49.140 --> 00:09:50.500
make it look cool.

00:09:50.500 --> 00:09:54.040
Or maybe you want to draw additional
elements on top of the video.

00:09:54.040 --> 00:09:55.500
Or maybe it's the inverse of that.

00:09:55.500 --> 00:09:58.880
Maybe you have other drawing elements,
and you want the video to be something

00:09:58.880 --> 00:10:00.820
embedded in that larger scene.

00:10:00.820 --> 00:10:03.280
Well, you can't use the view to do this.

00:10:03.280 --> 00:10:05.620
You're going to have to
implement some of its behavior.

00:10:05.620 --> 00:10:08.920
You're going to have to learn
OpenGL or Core Image and

00:10:08.920 --> 00:10:10.720
do that rendering yourself.

00:10:10.720 --> 00:10:13.830
And on hardware that can't support this,
you're going to have to fall

00:10:13.830 --> 00:10:14.710
back to the G World case

00:10:16.520 --> 00:10:20.320
So, let me show you a few
examples of this working.

00:10:20.350 --> 00:10:22.280
Back to demo one please.

00:10:28.390 --> 00:10:32.100
So this is an application
I showed last year.

00:10:32.110 --> 00:10:37.180
And the point of this is to
show that we're using OpenGL.

00:10:37.200 --> 00:10:38.110
Got that same movie.

00:10:38.120 --> 00:10:41.540
It's continuing to be
composited in real time.

00:10:41.550 --> 00:10:44.360
And these OpenGL transformations
are trivial.

00:10:44.580 --> 00:10:46.980
You're just rotating this thing around.

00:10:46.980 --> 00:10:52.060
And to really showcase the
horsepower of these GPUs,

00:10:52.150 --> 00:10:54.310
showing how much video we
can render at the same time.

00:10:54.410 --> 00:10:58.180
So you can have quite a few frames there.

00:11:02.600 --> 00:11:10.810
So instead of just rendering a frame and
then releasing it to be recycled later,

00:11:10.810 --> 00:11:13.730
this application holds on to a
few frames and continues to render

00:11:13.730 --> 00:11:15.100
them over and over and over.

00:11:15.140 --> 00:11:18.150
And you get an interesting effect.

00:11:18.150 --> 00:11:18.150
I mean, you can see.

00:11:18.500 --> 00:11:22.670
You can,
I think I have about 30 frames here.

00:11:22.670 --> 00:11:24.070
Playing back, no problem.

00:11:24.190 --> 00:11:27.900
So these graphics processors
have a whole lot of horsepower.

00:11:30.490 --> 00:11:32.940
The other application you may
have seen in the previous session

00:11:32.940 --> 00:11:36.410
this week is this jigsaw puzzle.

00:11:36.830 --> 00:11:38.900
So I have a high definition clip here.

00:11:38.900 --> 00:11:45.210
I believe this is a
1280 by 720 progressive.

00:11:47.300 --> 00:11:54.620
So, it's a little jigsaw puzzle,
and this was relatively easy to write.

00:11:54.620 --> 00:11:57.170
The most difficult part was, you know,
making those shapes,

00:11:57.170 --> 00:11:59.520
but I'd like to explain
what's going on here.

00:11:59.570 --> 00:12:02.330
So,
we have the video playing to a texture,

00:12:02.340 --> 00:12:06.100
or really a series of textures,
and every time I get a new texture,

00:12:06.100 --> 00:12:08.100
I have to draw this whole
puzzle from scratch.

00:12:08.100 --> 00:12:12.100
So, for each piece, I have a mask.

00:12:12.100 --> 00:12:16.130
It's a little grayscale image that
is the shape of the puzzle piece.

00:12:16.230 --> 00:12:19.490
And first, I render that using core image
to get that shadow effect.

00:12:19.560 --> 00:12:23.880
It's being run through a Gaussian blur
filter of a radius of maybe two or so.

00:12:23.880 --> 00:12:27.400
And then,
I take that same mask and I draw the

00:12:27.400 --> 00:12:32.180
video and the mask simultaneously to
cut out a piece of the video and draw

00:12:32.180 --> 00:12:34.830
it on top of where that shadow was,
and just repeat that

00:12:34.830 --> 00:12:36.780
process for every piece.

00:12:36.780 --> 00:12:39.700
And you know, it's quite, you know,
it's an interesting effect.

00:12:39.700 --> 00:12:44.700
So the gratuitous little cute demo.

00:12:44.700 --> 00:12:45.700
Okay.

00:12:45.700 --> 00:12:52.380
And if this is going to work,
I hope it does.

00:13:00.300 --> 00:13:10.600
So we got here some live video
coming off this i80c camera here.

00:13:11.720 --> 00:13:17.110
So instead of using a movie as a source,
I'm using a sequence grabber to get

00:13:17.260 --> 00:13:20.910
this video off of the camera and run
it through the exact same code that

00:13:20.920 --> 00:13:22.600
was doing all that rendering before.

00:13:22.600 --> 00:13:27.560
So, live video, movies,
all working through the

00:13:27.570 --> 00:13:29.600
same video pipeline.

00:13:29.940 --> 00:13:32.600
Alright.

00:13:32.600 --> 00:13:36.080
Back to slides please.

00:13:38.670 --> 00:13:40.140
Slides please.

00:13:40.140 --> 00:13:41.710
All right,
so that just shows some of the fun new

00:13:41.800 --> 00:13:46.140
things that really weren't possible
before with the old rendering pipeline.

00:13:46.190 --> 00:13:47.420
So how does this work?

00:13:47.430 --> 00:13:48.920
Well, the view's not going to help you.

00:13:48.930 --> 00:13:50.620
That's right out.

00:13:52.170 --> 00:13:54.470
So you have to get underneath this
thing and figure out what is it doing?

00:13:54.480 --> 00:13:56.360
How is it getting those
frames to the screen?

00:13:56.360 --> 00:13:58.880
And how can you do that?

00:13:59.080 --> 00:14:01.360
So first, you need a movie.

00:14:01.360 --> 00:14:05.580
Now you may wonder why
this is about rendering.

00:14:05.590 --> 00:14:07.640
We're opening a movie, OK.

00:14:07.660 --> 00:14:11.900
There's an interesting new semantic
that we've introduced that is

00:14:11.900 --> 00:14:14.820
important to video rendering when
it comes to opening your movie.

00:14:14.820 --> 00:14:17.420
We've added an API called
newMovieFromProperties.

00:14:17.430 --> 00:14:20.880
And it's a replacement for all
the other new movie calls that

00:14:20.880 --> 00:14:21.860
are available in QuickTime.

00:14:21.860 --> 00:14:24.700
I don't know, all 10, 12,
how many there are.

00:14:24.720 --> 00:14:27.860
And you use this API by specifying
a list of properties that

00:14:28.180 --> 00:14:29.530
describe how to open the movie.

00:14:29.670 --> 00:14:32.910
Maybe you're telling it open from a file,
or open from a URL,

00:14:32.910 --> 00:14:35.880
or open from a handle, or data reference.

00:14:35.910 --> 00:14:38.210
And you can also supply options,
like I want this movie

00:14:38.320 --> 00:14:42.130
to be active immediately,
or allow it to download progressively

00:14:42.580 --> 00:14:45.390
and continue asynchronously.

00:14:45.620 --> 00:14:49.510
But the important thing here is
that the movie will not necessarily

00:14:49.510 --> 00:14:54.320
inherit the current G world as is
the case with all the other calls.

00:14:54.450 --> 00:14:58.950
That was a very subtle side effect of
all the other ones is that upon creation

00:14:59.150 --> 00:15:02.000
the movie would be ready to start drawing
to that G world that would happen to be

00:15:02.000 --> 00:15:04.300
active at the time you created the movie.

00:15:04.450 --> 00:15:08.530
So with this new call you
can specify a visual context

00:15:09.760 --> 00:15:13.600
at creation time and the movie will
go there instead of to the G world.

00:15:14.960 --> 00:15:18.100
Now if you're using QtKit,
the QtMovie class will be using

00:15:18.100 --> 00:15:21.100
this under the hood so you don't
need to worry about it there.

00:15:21.550 --> 00:15:22.900
So you got the movie.

00:15:22.950 --> 00:15:26.440
Now you need a visual context
so you can get those frames out.

00:15:26.510 --> 00:15:28.640
First, what's a visual context?

00:15:28.670 --> 00:15:32.760
Well, this is our abstract rendering
destination for QuickTime movies.

00:15:32.790 --> 00:15:35.540
And it's our replacement for the G world.

00:15:35.700 --> 00:15:40.080
One of the biggest differences between
Visual Context and the G World as far

00:15:40.080 --> 00:15:45.440
as writing your code is concerned is
that your application is now responsible

00:15:45.530 --> 00:15:46.940
for getting those pixels to the screen.

00:15:46.940 --> 00:15:49.740
When you use the G World,
you just told the movie,

00:15:49.740 --> 00:15:52.550
render to this G World and it
did all the work of getting

00:15:52.600 --> 00:15:54.190
those pixels to the screen.

00:15:54.200 --> 00:15:57.870
When you use a Visual Context,
you merely get a frame of video out and

00:15:57.870 --> 00:16:00.070
it's up to you to get it to the screen.

00:16:00.500 --> 00:16:04.190
Now this is more responsibility but
it lets you do all those interesting

00:16:04.190 --> 00:16:09.070
things and it gives us the advantages
of the GPU and so you can take advantage

00:16:09.070 --> 00:16:10.960
of all that horsepower down there.

00:16:10.960 --> 00:16:13.490
It enables us to have multiple
buffers in flight so the codec

00:16:13.530 --> 00:16:16.310
is no longer constrained,
you know, waiting for the upload before

00:16:16.410 --> 00:16:17.900
it can start the next decode.

00:16:17.900 --> 00:16:21.690
And it's not restricted to
certain types of movies for

00:16:21.780 --> 00:16:24.250
playback to an OpenGL texture.

00:16:24.260 --> 00:16:28.040
We've talked about some previous
techniques for doing this and they were

00:16:28.040 --> 00:16:30.260
typically limited to single track video.

00:16:30.500 --> 00:16:33.830
So you can have a bunch of video
media pieces of content but this

00:16:33.920 --> 00:16:35.280
is not supposed to be restricted.

00:16:35.300 --> 00:16:38.880
This will work with, you know,
multi-track movies as we've

00:16:38.890 --> 00:16:40.380
seen here in the demo.

00:16:40.520 --> 00:16:42.800
It'll work with different types
of media so you don't need

00:16:42.890 --> 00:16:44.170
to worry about that anymore.

00:16:44.180 --> 00:16:47.670
It gives you more asynchronous
processing so this is,

00:16:47.790 --> 00:16:51.370
you know, I talked about before,
the codec can continue working while

00:16:51.370 --> 00:16:53.870
you're rendering with the processor,
the graphics processor.

00:16:53.880 --> 00:16:57.320
So how do you create one?

00:16:57.690 --> 00:17:01.620
We have this API,
QT OpenGL Texture Context Create,

00:17:01.620 --> 00:17:09.630
and it will create you a type of visual
context through which you can pull out

00:17:09.630 --> 00:17:09.630
OpenGL textures for the video frames.

00:17:09.890 --> 00:17:12.960
Now obviously you're going
to need OpenGL to use this.

00:17:12.960 --> 00:17:16.920
So you're going to need the
CGL context and the CGL pixel format.

00:17:17.100 --> 00:17:21.200
Now these are the core OpenGL API types.

00:17:21.200 --> 00:17:25.500
And unless you're doing only full screen
or only off screen OpenGL rendering,

00:17:25.500 --> 00:17:29.160
you're probably going to be talking
to OpenGL through Cocoa or Carbon.

00:17:29.370 --> 00:17:34.180
Now Cocoa uses the NSOpenGL objects
and Carbon you'll be using AGL.

00:17:34.230 --> 00:17:37.780
Both of these have ways to get the
underlying core OpenGL objects,

00:17:37.790 --> 00:17:39.800
which you will give to QuickTime.

00:17:45.710 --> 00:17:50.150
Now it's important to note that this
call can fail if one of your displays

00:17:50.150 --> 00:17:52.240
does not support Quartz Extreme.

00:17:52.360 --> 00:17:55.150
So you've got to be prepared for
this and possibly fall back to

00:17:55.260 --> 00:17:59.080
using G-Worlds if you need to
continue to support that hardware.

00:17:59.200 --> 00:18:02.240
So you have the Visual Context,
you have the movie,

00:18:02.370 --> 00:18:04.150
you need to connect them.

00:18:04.760 --> 00:18:08.380
So that's what
setMovieVisualContext is for.

00:18:08.450 --> 00:18:11.240
It's the replacement for setMovieGWorld.

00:18:11.290 --> 00:18:14.910
And quite simply, it points that movie
at the visual context.

00:18:15.210 --> 00:18:19.770
Now, a visual context can only have
one movie connected to it.

00:18:19.910 --> 00:18:20.910
So you've got to be careful of that.

00:18:21.030 --> 00:18:25.700
It'll fail if a different movie is
already connected to that visual context.

00:18:25.800 --> 00:18:28.780
So it's important that
you can disconnect them.

00:18:28.840 --> 00:18:31.580
And I want to point this out.

00:18:31.660 --> 00:18:36.240
Calling setMovieVisualContext with a null
pointer as the visual context parameter,

00:18:36.240 --> 00:18:39.270
it's an important semantic
for disconnecting a movie

00:18:39.270 --> 00:18:41.000
from a visual context.

00:18:41.000 --> 00:18:47.350
And also, similar to the new movie calls,
there is a difference here

00:18:47.380 --> 00:18:50.160
compared to setMovieGWorld.

00:18:50.260 --> 00:18:53.040
setMovieGWorld,
when you passed it null for the GWorld,

00:18:53.040 --> 00:18:55.360
it didn't mean don't render anywhere.

00:18:55.360 --> 00:18:59.460
It meant go take the current GWorld and
start rendering there instead.

00:18:59.540 --> 00:19:03.280
So the visual context call is different,
where passing no will cause

00:19:03.320 --> 00:19:05.390
the movie to stop rendering.

00:19:06.080 --> 00:19:09.190
Now it's important that you use
this when you maybe have to switch

00:19:09.330 --> 00:19:11.300
the movie to start using a G World.

00:19:11.360 --> 00:19:14.980
Just calling setMovieGWorld won't
necessarily disconnect the movie from the

00:19:14.980 --> 00:19:16.840
Visual Context for compatibility reasons.

00:19:16.950 --> 00:19:19.800
So it's important that if you
do that and you want to make

00:19:19.890 --> 00:19:25.300
sure that movie is disconnected,
you pass null to setMovieVisualContext.

00:19:25.350 --> 00:19:26.800
Okay, so they're connected.

00:19:26.870 --> 00:19:29.160
Now you need to get those images out.

00:19:30.080 --> 00:19:32.660
These are where these three APIs come in.

00:19:32.740 --> 00:19:35.500
So the visual context
is kind of a pull model.

00:19:35.550 --> 00:19:38.420
You ask it,
is there a new frame available for

00:19:38.420 --> 00:19:40.840
this time using that first API there?

00:19:40.890 --> 00:19:42.860
Is new image available?

00:19:42.920 --> 00:19:46.150
And then if that returns true,
you'll move on to the next one,

00:19:46.150 --> 00:19:47.760
copy image for time.

00:19:47.790 --> 00:19:51.660
And you'll probably pass it the same time
stamp as you asked in the first call.

00:19:51.680 --> 00:19:55.150
And then you get the image out
and you can render as you will.

00:19:55.240 --> 00:19:57.990
And when you're done, after rendering,
you need to call these

00:19:57.990 --> 00:20:00.380
visual context task call.

00:20:00.430 --> 00:20:03.520
Now this allows the visual context
to do some housekeeping work,

00:20:03.520 --> 00:20:07.100
recycle resources, reclaim buffers,
all that kind of thing.

00:20:07.280 --> 00:20:09.530
And due to threading issues,
it's important that you call this

00:20:09.530 --> 00:20:13.300
while you have OpenGL locked and
protected from other threads.

00:20:14.530 --> 00:20:17.520
So, you might wonder,
where do I get the timestamps and when

00:20:17.520 --> 00:20:20.030
do I call those functions and what do
these buffers look like that I get out?

00:20:20.100 --> 00:20:22.230
This is where Core Video comes into play.

00:20:22.230 --> 00:20:24.100
So let's talk about
Core Video for a little bit.

00:20:24.100 --> 00:20:30.010
So it provides buffer
services and timing services.

00:20:30.340 --> 00:20:33.340
So this is how we can
bridge the gap between a

00:20:33.350 --> 00:20:37.210
QuickTime Visual Context and OpenGL,
where you have buffer objects

00:20:37.330 --> 00:20:38.600
that speak both languages.

00:20:38.670 --> 00:20:41.610
And it allows us to
connect those two things.

00:20:41.920 --> 00:20:45.760
And it deals with all the ugly details
under the hood of managing buffers and

00:20:45.760 --> 00:20:49.260
managing their connections to OpenGL and
basically all the things you have to do

00:20:49.260 --> 00:20:51.830
to make the video playback efficient.

00:20:51.930 --> 00:20:53.990
And also, it deals with timing.

00:20:54.120 --> 00:20:56.950
So it's going to be telling
you when you need to draw.

00:20:56.960 --> 00:20:59.130
And it's kind of analogous
to the Core Audio model,

00:20:59.130 --> 00:21:01.800
where the audio hardware is
telling you when it needs samples.

00:21:01.800 --> 00:21:05.680
Core Video will tell you when
you need to draw video frames.

00:21:06.500 --> 00:21:07.820
So about the buffers.

00:21:08.100 --> 00:21:11.040
Core Video buffers are core
foundation based objects.

00:21:11.070 --> 00:21:15.160
So they have the same retain and release
semantics of the rest of core foundation.

00:21:15.170 --> 00:21:17.910
And there's sort of a hierarchy
of classes for the buffers.

00:21:18.020 --> 00:21:21.200
At the top there's CVBuffer,
just kind of an abstract class.

00:21:21.210 --> 00:21:24.960
And with this you can have
attachments on the buffers.

00:21:24.970 --> 00:21:26.350
Now attachments can be anything you want.

00:21:26.470 --> 00:21:27.800
They're just a CF type.

00:21:27.840 --> 00:21:31.960
Think of it as like a dictionary
of stuff that's on the buffer.

00:21:32.280 --> 00:21:37.900
Examples might be the timestamp of the
buffer or the color space of the buffer,

00:21:38.030 --> 00:21:38.460
things like that.

00:21:38.580 --> 00:21:40.530
And you can put anything
you want on there yourself,

00:21:40.560 --> 00:21:41.100
custom data.

00:21:41.190 --> 00:21:42.740
That's at the CvBuffer level.

00:21:42.740 --> 00:21:46.690
The next thing in the
hierarchy is CvImageBuffer.

00:21:46.690 --> 00:21:51.070
Now this is specific to images
and you'll find things like the

00:21:51.070 --> 00:21:52.300
dimensions of the image here.

00:21:52.300 --> 00:21:56.260
And all of the Visual Context
APIs that deal with Core Video buffers,

00:21:56.410 --> 00:21:59.120
they'll be using the CvImageBuffer type.

00:21:59.160 --> 00:22:03.340
Now in reality there are
multiple types of image buffers.

00:22:03.340 --> 00:22:07.080
Like here we have the Pixel Buffer,
the OpenGL Texture and the OpenGL Buffer.

00:22:07.160 --> 00:22:09.150
For now let's talk about
the OpenGL Texture.

00:22:09.150 --> 00:22:14.160
This is a simple, I shouldn't say simple,
it's a wrapper around an OpenGL Texture.

00:22:14.160 --> 00:22:17.740
And it does all the dirty work
of managing the memory mappings

00:22:17.740 --> 00:22:21.010
and recycling textures and
memory and all that stuff.

00:22:21.160 --> 00:22:24.260
And if you speak OpenGL,
you know about texture

00:22:24.470 --> 00:22:26.160
targets and texture names.

00:22:26.160 --> 00:22:29.160
And if you can get the
texture target names,

00:22:29.160 --> 00:22:29.160
you can get the texture
targets and texture names.

00:22:29.160 --> 00:22:31.730
You can get the name out of this thing,
you can render that texture

00:22:31.910 --> 00:22:33.160
using OpenGL directly.

00:22:33.160 --> 00:22:36.560
And to do that,
you're going to need texture coordinates.

00:22:36.560 --> 00:22:41.160
And there's an API we recommend you use,
OpenGL Texture Get Clean Text Cords.

00:22:41.300 --> 00:22:45.020
Now we recommend that over just getting
the size of the image and going from

00:22:45.110 --> 00:22:47.160
zero to whatever width and height.

00:22:49.970 --> 00:22:52.770
So timing,
this is where the Core Video display

00:22:52.780 --> 00:22:54.280
link comes into play.

00:22:54.520 --> 00:22:58.970
The display link is an object that helps
you drive that visual context pull model,

00:22:58.970 --> 00:23:01.770
where it's giving you
the callbacks to say,

00:23:01.770 --> 00:23:05.010
if you start drawing right
now and you hurry up,

00:23:05.260 --> 00:23:09.140
your pixels will get on the
screen in exactly 25 milliseconds.

00:23:09.200 --> 00:23:11.480
So it's going to give you that
time stamp that you pass to the

00:23:11.710 --> 00:23:15.780
visual context to query if there
are new textures available,

00:23:15.780 --> 00:23:18.120
and then to get that texture out.

00:23:18.150 --> 00:23:21.890
And it does this on a
dedicated I/O thread.

00:23:23.190 --> 00:23:26.200
And because a computer can
support multiple displays,

00:23:26.200 --> 00:23:29.560
you'll need to keep that
display link synchronized with--

00:23:30.950 --> 00:23:35.940
with the current display that
the video is being played on.

00:23:36.180 --> 00:23:38.220
So there's a call to do that.

00:23:38.840 --> 00:23:42.300
So once you have these buffers,
you need to render them.

00:23:42.360 --> 00:23:44.140
Well,
there's two obvious choices off the bat.

00:23:44.180 --> 00:23:47.080
You have Core Image and you have OpenGL.

00:23:47.390 --> 00:23:50.600
With Core Image,
it might not be accelerated.

00:23:50.640 --> 00:23:52.290
I think not be available
is the wrong word.

00:23:52.300 --> 00:23:54.110
Might not be accelerated.

00:23:54.180 --> 00:23:58.370
And since we're talking about live video,
maybe that's the same thing.

00:23:58.760 --> 00:24:02.250
and with Core Image you get
built-in color matching.

00:24:02.260 --> 00:24:04.880
So all that color sync work,
that's basically free if

00:24:04.880 --> 00:24:06.700
you render with Core Image.

00:24:06.700 --> 00:24:08.650
And effects are downright trivial.

00:24:08.680 --> 00:24:09.700
It's easy.

00:24:09.700 --> 00:24:11.350
You can do it in one line of code.

00:24:11.740 --> 00:24:14.230
Long line,
but one line of code can take that

00:24:14.290 --> 00:24:16.700
image and apply a crazy filter to it.

00:24:17.590 --> 00:24:20.300
and it supports all the usual
video transformations and

00:24:20.300 --> 00:24:22.260
compositions and things like that.

00:24:22.260 --> 00:24:24.400
So that's the easy way.

00:24:24.420 --> 00:24:25.900
You can also use OpenGL.

00:24:25.970 --> 00:24:29.800
Now if you go this route,
you can have broader hardware support.

00:24:29.860 --> 00:24:33.620
You don't need to worry whether the
graphics has fragment programmability.

00:24:33.810 --> 00:24:34.970
You can render without that.

00:24:35.070 --> 00:24:39.320
And it could be more efficient if
you know exactly what you need to

00:24:39.650 --> 00:24:41.680
render and how you want to render it.

00:24:42.040 --> 00:24:44.310
Unfortunately,
it's going to be a bit more code

00:24:44.310 --> 00:24:44.310
to write than using Core Image.

00:24:45.970 --> 00:24:48.580
So color management.

00:24:48.600 --> 00:24:50.920
Many of you are probably
familiar with gamma.

00:24:50.930 --> 00:24:55.770
And dealing with gamma in video
has been an issue for a long time.

00:24:55.840 --> 00:24:59.340
And that's just a small piece
of the color management puzzle.

00:24:59.360 --> 00:25:03.320
There are more aspects to images
and video than just the gamma curve.

00:25:03.330 --> 00:25:05.930
There's like the chromacities of red,
green, and blue.

00:25:05.980 --> 00:25:07.970
The red on your display might
be different from the red

00:25:07.980 --> 00:25:09.680
that the camera recorded.

00:25:09.740 --> 00:25:13.260
So color management is
how we solve this issue.

00:25:13.290 --> 00:25:20.960
And applications have been describing
in the movie file the characteristics

00:25:21.020 --> 00:25:22.700
of this video for a while now.

00:25:22.720 --> 00:25:24.780
And there's a document we have
that describes how you can put

00:25:24.820 --> 00:25:29.670
this information in your video
if you're capturing video.

00:25:29.890 --> 00:25:33.090
When you get the frames
out of the Visual Context,

00:25:33.120 --> 00:25:35.670
they're going to have a color
profile attached to them.

00:25:35.670 --> 00:25:38.790
And this color profile
describes that source video,

00:25:38.850 --> 00:25:40.650
its color characteristics.

00:25:40.920 --> 00:25:44.140
And when you render that
video using Core Image,

00:25:44.230 --> 00:25:47.120
it'll have some color profiles
that describe the destination

00:25:47.590 --> 00:25:48.630
and a working color space.

00:25:48.710 --> 00:25:52.640
So the color is going to
be transformed for you.

00:25:53.500 --> 00:25:58.060
The Visual Context is a couple
APIs for describing how you

00:25:58.060 --> 00:26:00.400
want these colors to be managed.

00:26:00.400 --> 00:26:02.820
It's very analogous to the
Core Image model where you have a

00:26:02.820 --> 00:26:06.140
working color space in which the
pixels are manipulated and an output

00:26:06.140 --> 00:26:10.400
color space in which everything is
eventually matched to for the display.

00:26:10.410 --> 00:26:14.050
That's typically going to
be the calibrated display

00:26:14.390 --> 00:26:16.630
profile for your monitor.

00:26:16.830 --> 00:26:18.580
And again,
very little code is required for this.

00:26:18.720 --> 00:26:22.200
You just need to get the display profile,
give it the core image,

00:26:22.200 --> 00:26:25.720
and then just draw the buffers
and they're color managed for you.

00:26:26.520 --> 00:26:28.940
So I'm going to be showing
you some sample code.

00:26:28.940 --> 00:26:33.530
Now this is really sort of a best
practices example of how to render

00:26:33.650 --> 00:26:35.640
using the new video pipeline.

00:26:35.720 --> 00:26:38.430
And it uses QtKit, but not for rendering.

00:26:38.590 --> 00:26:42.330
It uses the QtMovie class
to manage that movie,

00:26:42.330 --> 00:26:46.730
to call movies tasks when necessary,
and deal with opening the movie

00:26:46.730 --> 00:26:48.280
and all that kind of stuff.

00:26:48.480 --> 00:26:51.170
And it does the rendering
in one of three ways.

00:26:51.270 --> 00:26:54.550
It uses OpenGL directly and
if Core Image is accelerated,

00:26:54.620 --> 00:26:57.600
it'll use that to get
the color management.

00:26:57.690 --> 00:27:00.640
And if the hardware does
not support Quartz Extreme,

00:27:00.700 --> 00:27:02.640
we're going to fall
back to using GWorlds.

00:27:02.690 --> 00:27:06.070
So let's walk through the
code for a little bit.

00:27:06.420 --> 00:27:08.170
Demo 1 please.

00:27:11.560 --> 00:27:16.950
So this example has been recently
posted to the ADC or WWDC website,

00:27:16.960 --> 00:27:20.040
so you can follow along if you have it.

00:27:20.090 --> 00:27:22.110
So I'm going to walk through the
different stages of the pipeline

00:27:22.120 --> 00:27:24.100
that I talked about earlier.

00:27:24.580 --> 00:27:27.500
The first one being movie creation.

00:27:27.740 --> 00:27:30.260
Now I'm using QtKit so this is trivial.

00:27:30.290 --> 00:27:34.740
I just call the initWithFileRoutine
and QtKit creates that using that

00:27:34.740 --> 00:27:39.240
new movie from Properties Call under
the hood and it's all easy.

00:27:39.410 --> 00:27:41.500
Then here I tell the
movie to start looping.

00:27:42.220 --> 00:27:45.170
Okay, so that's movie creation.

00:27:45.400 --> 00:27:48.050
Now, creating the Visual Context.

00:27:48.120 --> 00:27:50.000
There's basically three steps here.

00:27:50.060 --> 00:27:52.840
The first is to create OpenGL objects.

00:27:52.980 --> 00:27:57.800
So we use the NSOpenGLPixel format
and the NSOpenGLContext to do that.

00:27:57.900 --> 00:28:00.590
And from those,
I get the underlying core OpenGL objects.

00:28:00.890 --> 00:28:03.600
See the CGLContext and
the CGLPixel format.

00:28:03.700 --> 00:28:07.180
I stash those away because I use
them quite frequently in the code.

00:28:07.840 --> 00:28:10.940
Then I create the display
link and the visual context.

00:28:11.070 --> 00:28:15.000
You see here I pass the CGL context
and CGL pixel format to that

00:28:15.160 --> 00:28:17.400
visual context creation call.

00:28:17.490 --> 00:28:19.820
So now I have the display link for
the timing and I have the visual

00:28:19.830 --> 00:28:22.120
context so I can pull buffers out.

00:28:22.240 --> 00:28:26.060
Well, what does that callback look like?

00:28:26.680 --> 00:28:30.330
Sorry, before that,
we need to connect it to the movie.

00:28:30.620 --> 00:28:36.200
So my view class will provide the
visual context that it created

00:28:36.200 --> 00:28:38.810
to the controller object here.

00:28:38.810 --> 00:28:42.530
And the controller gets the movie
from the document and simply

00:28:42.530 --> 00:28:44.520
calls setMovieVisualContext.

00:28:44.790 --> 00:28:45.730
And that's it.

00:28:46.390 --> 00:28:49.270
You see all the other boilerplate code
here for calling back to G-Worlds.

00:28:49.290 --> 00:28:52.290
You should look at that if you're
interested in how to write a

00:28:52.360 --> 00:28:54.870
robust video rendering app here.

00:28:55.470 --> 00:28:59.820
Okay, so at some point you need to kick
off that display link to get it

00:28:59.930 --> 00:29:06.400
started so it's going to start calling
you every display refresh time.

00:29:06.800 --> 00:29:09.040
The lock focus call is a good
place to hook in to do that.

00:29:09.140 --> 00:29:13.150
This is called right before the
view is about to start rendering.

00:29:13.300 --> 00:29:16.290
So we'll make sure if we
haven't been connected yet,

00:29:16.300 --> 00:29:16.920
let's do so.

00:29:17.190 --> 00:29:21.700
So we connect the OpenGL context,
get the display link set up,

00:29:21.880 --> 00:29:26.030
provide it our callback function,
get it updated to the current display

00:29:26.030 --> 00:29:28.740
that our view is rendering to,
and then call start.

00:29:28.820 --> 00:29:31.260
And once we call start,
it's going to begin giving

00:29:31.260 --> 00:29:33.760
us those callbacks every,
well on this machine,

00:29:33.820 --> 00:29:36.650
60 times a second because
I have this LCD here.

00:29:37.540 --> 00:29:39.280
All right.

00:29:39.420 --> 00:29:43.320
Now in that callback is where
we do the meat of our rendering.

00:29:43.430 --> 00:29:48.340
And here's the little algorithm that
we use to drive the visual context.

00:29:48.460 --> 00:29:50.420
The first thing we do is we
ask the visual context if

00:29:50.480 --> 00:29:52.080
a new image is available.

00:29:52.220 --> 00:29:54.960
And here we just pass it the
time stamp that we got from

00:29:54.960 --> 00:29:57.160
that display link callback.

00:29:57.290 --> 00:30:01.740
And when that returns true, we move on,
release any old buffers that we had

00:30:02.090 --> 00:30:04.780
pulled out of the visual context,
and then get a new one out of

00:30:04.910 --> 00:30:09.260
it with copy image for time,
passing it that same output time stamp.

00:30:09.340 --> 00:30:13.510
And that gives us a
CV OpenGL texture object.

00:30:13.590 --> 00:30:17.590
And I'll stash that away in my class
and then move on to the rendering step.

00:30:19.110 --> 00:30:21.530
So, rendering.

00:30:21.540 --> 00:30:24.970
I'm not going to get into details
here because Frank later will have

00:30:25.150 --> 00:30:28.600
some more tutorials that will show
you different ways of rendering.

00:30:28.600 --> 00:30:31.790
So I want to call out a few details here.

00:30:32.640 --> 00:30:34.430
This is the render function.

00:30:34.580 --> 00:30:38.860
And you'll notice here that I check
to see that our texture is not null.

00:30:38.860 --> 00:30:42.720
Now, this is subtle,
but a Visual Context may return a success

00:30:42.850 --> 00:30:47.480
code when you call copy image for time,
but it'll give you a null pointer.

00:30:47.480 --> 00:30:51.720
This is completely valid and you had
better expect it because that just

00:30:51.980 --> 00:30:54.330
means there is no video for that time.

00:30:54.330 --> 00:30:58.170
Maybe there was video and then
to get you to stop rendering it,

00:30:58.170 --> 00:31:02.160
it has to tell you, okay,
now start rendering nothing.

00:31:02.500 --> 00:31:04.190
So you have to be prepared for that.

00:31:04.300 --> 00:31:09.740
And in this case, when we encounter that,
I just clear the buffer using OpenGL.

00:31:10.490 --> 00:31:13.670
At the end, when we're done drawing,
we call the flush buffer method

00:31:13.820 --> 00:31:16.400
to get that data onto the screen.

00:31:16.530 --> 00:31:18.900
And then I call the task routine.

00:31:19.030 --> 00:31:22.570
And I do it here because we still
have our OpenGL locks taken,

00:31:22.630 --> 00:31:25.400
so no other threads are going to be
messing with OpenGL at this point.

00:31:25.460 --> 00:31:28.510
And it's a good time to
do this housekeeping work.

00:31:29.000 --> 00:31:30.050
So that's pretty much it.

00:31:30.100 --> 00:31:32.130
Those are the important
points of this application.

00:31:32.160 --> 00:31:35.870
There's a lot more code in here dealing
with resizing and all that fun stuff.

00:31:36.090 --> 00:31:37.740
But it's a very simple application.

00:31:37.890 --> 00:31:40.830
Just renders the video.

00:31:40.830 --> 00:31:43.240
And that's it.

00:31:43.330 --> 00:31:43.760
Back to slides.

00:31:51.770 --> 00:31:57.310
So there's a few more uses
of visual context that I'd

00:31:57.310 --> 00:31:59.480
like to go over real quick.

00:31:59.480 --> 00:32:03.300
The Image Compression Manager can
drive visual context also.

00:32:03.310 --> 00:32:07.250
And you don't have to necessarily
connect the visual context to a movie.

00:32:07.380 --> 00:32:11.460
You can connect it to an
ICM decompression session.

00:32:11.460 --> 00:32:14.680
Now this is what I did in the jigsaw
puzzle where I had the live video going.

00:32:14.690 --> 00:32:15.600
I had a sequence grabber.

00:32:15.600 --> 00:32:16.590
I was pulling frames down.

00:32:16.600 --> 00:32:19.100
And I was feeding them through
the decompression session,

00:32:19.110 --> 00:32:21.860
which itself was connected
to that visual context.

00:32:21.930 --> 00:32:24.180
So the rendering code had no idea
where the video was coming from.

00:32:24.180 --> 00:32:26.220
It was just doing the
visual context work.

00:32:26.240 --> 00:32:31.180
And I connected it through this
API to that decompression session.

00:32:35.150 --> 00:32:38.210
Oh, and there's a session after this,
you know, 2 o'clock,

00:32:38.210 --> 00:32:42.710
on using those new APIs and
decompression sessions and so forth.

00:32:43.280 --> 00:32:45.940
So, CVPixelBuffer.

00:32:46.030 --> 00:32:49.100
This is another core video buffer
that I didn't talk about earlier.

00:32:49.100 --> 00:32:50.980
I'll just focus on the
OpenGL texture for a moment.

00:32:51.080 --> 00:32:54.590
But this is where you have
memory-based pixel data.

00:32:54.600 --> 00:32:59.100
So this is equivalent to having like a
pixmap in the previous rendering model.

00:32:59.100 --> 00:33:02.030
So now it's a reference-counted
buffer object.

00:33:02.030 --> 00:33:05.400
And it's used heavily internally
by QuickTime for all of the

00:33:05.480 --> 00:33:07.100
decompression session work.

00:33:07.100 --> 00:33:10.430
And it's really the foundation
for our multi-buffer

00:33:10.540 --> 00:33:13.520
ACM APIs and allows us to have,
you know,

00:33:13.520 --> 00:33:18.030
one pixel buffer being mapped to the
hardware while we're decompressing

00:33:18.120 --> 00:33:19.890
into another pixel buffer elsewhere.

00:33:20.720 --> 00:33:24.180
Sometimes you may want to get these out
of the movie instead of OpenGL Textures.

00:33:24.360 --> 00:33:27.470
Maybe you just want to do, you know,
CPU processing on those buffers.

00:33:27.930 --> 00:33:31.560
Well,
you can do QTPixelBufferContextCreate and

00:33:31.560 --> 00:33:33.580
it will give you also a Visual Context.

00:33:33.600 --> 00:33:35.590
But this one will not
give you OpenGL Textures.

00:33:35.620 --> 00:33:37.600
It will give you CVPixelBufferObjects.

00:33:37.760 --> 00:33:40.030
And from there you can get
the base address and the pixel

00:33:40.030 --> 00:33:41.550
format and do whatever you want.

00:33:41.620 --> 00:33:44.600
And it's useful for things that
don't need the GPU to work.

00:33:44.840 --> 00:33:47.600
Maybe you're doing offline rendering
or you're just extracting video.

00:33:47.600 --> 00:33:51.710
Now you can--to this API you
can pass options to say,

00:33:51.720 --> 00:33:54.660
"I want specific pixel formats,"
or "I want a specific dimension for

00:33:54.760 --> 00:33:56.590
this pixel buffer," and it'll work.

00:33:56.740 --> 00:34:00.580
If you don't pass any options here,
you'll get whatever the native

00:34:00.970 --> 00:34:02.510
size is of the movie video.

00:34:03.080 --> 00:34:05.930
And with that, I want to hand it over to
Frank who's going to talk more about

00:34:05.930 --> 00:34:08.500
rendering and using Core Image.

00:34:12.660 --> 00:34:15.100
Thank you, Sean.

00:34:15.100 --> 00:34:19.380
Welcome to sunny California.

00:34:19.380 --> 00:34:22.600
That worked better in the rehearsal.

00:34:22.600 --> 00:34:24.840
Okay, we are talking a little
bit now about Core Image.

00:34:24.840 --> 00:34:29.030
My name is Frank Doepke and
I'm actually showing you some

00:34:29.030 --> 00:34:31.920
of the rendering effects that
Sean already introduced a little bit.

00:34:32.630 --> 00:34:36.310
You've seen the pipeline,
same slide as we had already last year,

00:34:36.370 --> 00:34:38.040
and we're going on to
some of the more details.

00:34:38.040 --> 00:34:41.330
So let's start first with
the core image part of it.

00:34:42.220 --> 00:34:46.740
So what does Core Image bring us as
an addition to our playback scenario?

00:34:46.740 --> 00:34:50.970
So we get over a hundred effects
right out of the box from Tiger and

00:34:51.570 --> 00:34:54.370
already developers are writing
additional image units which even

00:34:54.370 --> 00:34:56.580
provides us even with more filter.

00:34:56.600 --> 00:35:00.210
We get very fast processing of
these filters right on the GPU.

00:35:00.490 --> 00:35:05.010
So you can do fancy effects
and it doesn't tax your CPU.

00:35:05.770 --> 00:35:08.840
What you need to learn is basically
how do we get from the CV image buffers

00:35:08.850 --> 00:35:12.140
that we get from QuickTime through
Core Video and put them into CI image.

00:35:12.140 --> 00:35:13.700
So I will show you that part.

00:35:13.750 --> 00:35:17.860
And the same is true like how do
we get the CI context which is our

00:35:17.860 --> 00:35:20.290
rendering target for Core Image.

00:35:20.950 --> 00:35:23.320
One thing that is important
to know here is that you can

00:35:23.320 --> 00:35:24.900
chain these filters together.

00:35:24.900 --> 00:35:27.000
So right now we've just seen, okay,
we use one filter for

00:35:27.000 --> 00:35:29.670
the color correction,
but I can do multiple effects and then

00:35:29.670 --> 00:35:31.840
put them together into one pipeline.

00:35:31.840 --> 00:35:34.670
And you want to chain them together
because these chains can fold

00:35:34.670 --> 00:35:36.680
together in one pixel program.

00:35:36.680 --> 00:35:38.780
So if you haven't been at the
Core Image session yesterday,

00:35:38.940 --> 00:35:41.720
you will actually see that there is
an advantage in Core Image built in

00:35:41.720 --> 00:35:45.460
that it lazily evaluates these filters
and therefore computes a very small

00:35:45.460 --> 00:35:49.950
program out of a longer filter chain
to make the rendering more efficient.

00:35:51.200 --> 00:35:55.440
So the sample code that we want
to look at is the CIVDU Demo GL.

00:35:55.440 --> 00:35:58.160
I call it episode two because
it's actually a slightly

00:35:58.190 --> 00:36:02.230
different version that you've
seen already on your Tiger disk.

00:36:02.290 --> 00:36:05.500
So we show the integration
Core Image going through

00:36:05.500 --> 00:36:08.780
Core Video and OpenGL and QuickTime.

00:36:09.130 --> 00:36:14.190
We actually look at how we create the
CI context from our NSOpenGL context.

00:36:14.190 --> 00:36:18.990
And we use the CI image from
CvImageBuffer API to really get our

00:36:18.990 --> 00:36:21.780
frames from QuickTime into Core Image.

00:36:21.780 --> 00:36:24.380
Last year, for those who have been here,
we showed mostly to

00:36:24.380 --> 00:36:25.420
use image with texture.

00:36:25.420 --> 00:36:30.490
This is still a valid API call,
but I would like you not to use it for

00:36:30.490 --> 00:36:32.430
this case as it has some drawbacks.

00:36:32.460 --> 00:36:36.290
It can go into technical details
in the lab if you want to stop by.

00:36:37.010 --> 00:36:39.770
So also show you how to create a
filter pipeline as already mentioned.

00:36:39.780 --> 00:36:44.530
We'll use multiple filters on that video
and then we also use some compositing

00:36:44.530 --> 00:36:48.530
effects to draw on top of our video and
I will show you how to do that as well.

00:36:48.540 --> 00:36:51.700
So with this I would like
to go to the demo machine.

00:37:02.840 --> 00:37:04.790
So here we have our CI Video Demo GL.

00:37:04.910 --> 00:37:07.090
And before I actually
show too much of the code,

00:37:07.090 --> 00:37:10.120
I will actually show you what
this application really can do.

00:37:11.800 --> 00:37:15.310
I'll run it in just a second.

00:37:15.390 --> 00:37:16.700
Okay.

00:37:16.700 --> 00:37:18.320
So here I have some video.

00:37:18.390 --> 00:37:20.290
It's not running in this moment.

00:37:20.660 --> 00:37:21.640
I can scrub through it.

00:37:21.920 --> 00:37:23.320
It's a very short clip.

00:37:23.320 --> 00:37:26.640
You will see that clip quite a
bit more in the afternoon as well.

00:37:26.850 --> 00:37:30.820
And I have a zoom blur effect
which I can move around.

00:37:30.940 --> 00:37:35.860
As you can see right here and I can
set how much of a zoom blur I want

00:37:35.860 --> 00:37:37.710
and I can even take it completely out.

00:37:37.710 --> 00:37:37.710
So this is kind of my zoom blur.

00:37:38.110 --> 00:37:39.730
And I can do this while
the clip is playing.

00:37:39.740 --> 00:37:40.580
You can see this.

00:37:40.580 --> 00:37:42.920
And I change the zoom blur.

00:37:42.990 --> 00:37:45.180
This is all running live.

00:37:45.230 --> 00:37:49.200
Same as I can do with-- we showed that
we can do some color correction as well.

00:37:49.200 --> 00:37:52.400
So if I say, well, I want this a little
bit brighter or darker,

00:37:52.460 --> 00:37:53.660
saturation nicely in.

00:37:53.660 --> 00:37:59.050
And now we have a very, well,
psychedelic scene here.

00:37:59.990 --> 00:38:02.280
So this is, in short,
what this application can do.

00:38:02.280 --> 00:38:05.780
Now let's have a look at the code,
actually, how we did this.

00:38:08.830 --> 00:38:12.360
First part, as I said, okay,
we need to create a CI context.

00:38:12.360 --> 00:38:15.760
And I've do this in the
prepare for OpenGL of my view.

00:38:15.760 --> 00:38:21.750
So when you look at it,
we again set up our display link.

00:38:22.310 --> 00:38:26.840
And we need to look at basically
the amount of displays that we have,

00:38:26.890 --> 00:38:32.400
set our display link up, and I have some,
I set up my output callback.

00:38:32.490 --> 00:38:34.730
I'm ready for rendering
on this kind of part now.

00:38:37.750 --> 00:38:40.870
The second part that I'm going to
need to look at is... Actually,

00:38:40.870 --> 00:38:43.840
sorry, I need to go a little
bit more on top here.

00:38:46.330 --> 00:38:50.110
How do I set up the context
for the CI drawing itself?

00:38:50.250 --> 00:38:54.900
So I have my CGL context and
I create a CI context from it.

00:38:55.350 --> 00:38:58.090
I set up the color space which is
important for the rendering part as

00:38:58.200 --> 00:39:02.120
we mentioned already that CI will
do the color correction for you.

00:39:02.180 --> 00:39:04.180
And then I set up a bunch of filters.

00:39:04.280 --> 00:39:07.480
So I used the color control filter
which is the one that does all the hue,

00:39:07.520 --> 00:39:09.860
saturation,
brightness control and I used the

00:39:09.860 --> 00:39:14.050
CI Zoom Blur that was the fancy
effect that I put on top of it.

00:39:14.310 --> 00:39:17.070
So I don't set all the values
on the Zoomer right here.

00:39:17.180 --> 00:39:20.960
So what I'm doing actually is I use
set defaults to make sure that all the

00:39:20.960 --> 00:39:25.390
parameters of my filter are at least
initialized to something sensible.

00:39:26.170 --> 00:39:27.480
So that is now everything set up.

00:39:27.550 --> 00:39:29.820
How do we do the rendering
part of the filter?

00:39:29.850 --> 00:39:31.980
Well, that is very simple.

00:39:33.220 --> 00:39:35.940
The callback that actually like
Sean already showed that actually now

00:39:35.940 --> 00:39:42.120
will render these frames coming from
QuickTime will simply go in... Oops,

00:39:42.120 --> 00:39:43.410
this is actually in the wrong space.

00:39:46.970 --> 00:39:50.740
Provide me with a CV image buffer and
I create a CI image from it by simply

00:39:50.740 --> 00:39:53.680
using Image with CV Image Buffer.

00:39:53.910 --> 00:39:56.890
So now from this buffer,
I get my rectangle.

00:39:57.010 --> 00:40:01.860
So this is really the frame size coming
out of QuickTime that I wanted to use.

00:40:02.310 --> 00:40:05.200
Then I run my color correction on it.

00:40:05.250 --> 00:40:08.000
And now the output of this
color correction I use as the

00:40:08.000 --> 00:40:11.790
input to my effect filter,
which was the zoom blur.

00:40:12.140 --> 00:40:17.000
And then I draw, last but not least,
my image to the screen.

00:40:17.130 --> 00:40:20.100
And you can see that I'm using
here the image rectangle that I got

00:40:20.100 --> 00:40:22.000
out before I applied the filter.

00:40:22.000 --> 00:40:23.890
So like the zoom draw,
you can imagine this actually

00:40:23.890 --> 00:40:27.000
makes the image much bigger
because it zooms outside.

00:40:27.060 --> 00:40:30.000
But I really only want to render
like the original frame size.

00:40:30.030 --> 00:40:33.490
So that's why I had to get the
actual extent from the original

00:40:33.490 --> 00:40:38.620
frame and then render really just
using this as my source rectangle.

00:40:39.370 --> 00:40:45.050
The other part that I can show you
here is how do I actually use my

00:40:45.740 --> 00:40:52.040
"So I have simply an event handler
and while I'm playing back,

00:40:52.070 --> 00:40:55.620
again I have to make sure that I'm
actually locking my OpenGL context here,

00:40:55.940 --> 00:40:58.870
I can change the parameter of my filter.

00:40:58.950 --> 00:40:59.940
So this is the effect filter.

00:40:59.940 --> 00:41:03.600
This is how I'm doing the zoom part
and moving it around with the mouse.

00:41:03.600 --> 00:41:09.020
But it's simply changing the
input center as that is my target.

00:41:11.090 --> 00:41:13.530
The next part for that,
actually I want to go quickly back

00:41:13.540 --> 00:41:15.670
into like running the application,
is like actually how

00:41:15.680 --> 00:41:16.600
do I get my stuff back?

00:41:16.720 --> 00:41:20.340
So we've seen how we render and then
can bring everything back to the screen.

00:41:20.480 --> 00:41:26.450
But we often get the question, OK,
screen is not the only target.

00:41:26.450 --> 00:41:30.320
I want to get stuff
back from my graphics.

00:41:30.320 --> 00:41:34.520
So for this,
let's go quickly back to slides and

00:41:34.570 --> 00:41:35.760
I'll talk a little bit about that.

00:41:35.760 --> 00:41:35.760
Can we go to slides, please?

00:41:40.060 --> 00:41:43.260
So how do I get the
stuff back from the GPU?

00:41:43.310 --> 00:41:44.940
There are two ways that
I want to talk about.

00:41:44.950 --> 00:41:47.500
The first part is when
I'm using Core Image,

00:41:47.520 --> 00:41:51.520
I can ask the CI context,
the Core Image context,

00:41:51.570 --> 00:41:52.860
to bring back a CG image.

00:41:52.860 --> 00:41:56.380
And this is good so I can write
the CG image out to disk or just

00:41:56.380 --> 00:42:01.100
render it as a thumbnail and some
views that I need in my application.

00:42:01.100 --> 00:42:05.510
So this is very easy and it's good
if you just want to use CG image.

00:42:05.600 --> 00:42:08.030
If you want to read
back what's on the GPU,

00:42:08.030 --> 00:42:12.080
now let's say I do actually some stuff
on top of my Core Image rendering

00:42:12.080 --> 00:42:14.080
and I'm using OpenGL on top of it.

00:42:14.080 --> 00:42:17.150
So there's a way how I can read
back these pixels from the graphics

00:42:17.470 --> 00:42:19.340
card and do something with it.

00:42:19.340 --> 00:42:22.900
So that's what GL Read Pixels is doing.

00:42:23.110 --> 00:42:24.940
This is not also,
now for those who are a little

00:42:25.050 --> 00:42:27.770
bit more familiar with OpenGL,
they say, oh, GL ReadPixel,

00:42:27.860 --> 00:42:28.790
that's old stuff.

00:42:28.870 --> 00:42:30.800
There's a more advanced way of doing it.

00:42:30.800 --> 00:42:32.250
It's just a little bit more complicated.

00:42:32.300 --> 00:42:38.170
And for those who are interested,
please go to the Maximizing

00:42:38.170 --> 00:42:44.380
OpenGL Performance session that
is on Thursday at 2 o'clock.

00:42:44.380 --> 00:42:44.750
And there you can see a little
bit more fancier ways how to

00:42:44.750 --> 00:42:44.750
read back pixels from the GPU.

00:42:46.290 --> 00:42:50.350
So we'll go back into our CI Video Demo
GL and we have a look at how we can use

00:42:50.610 --> 00:42:54.640
Core Image together with Image I/O to
write the stuff out to a file.

00:42:54.710 --> 00:42:57.830
And when we just look
at the OpenGL Context,

00:42:57.830 --> 00:42:59.800
we'll actually use the
GL Read Pixels part,

00:42:59.820 --> 00:43:03.390
read the pixels from the graphics card,
and then we use movie export from

00:43:03.390 --> 00:43:08.230
procedures to export what we've just
rendered into a QuickTime movie.

00:43:08.330 --> 00:43:11.220
And with this, I would like to go back
to the demo machine.

00:43:16.480 --> 00:43:19.300
So let me pick a little
bit more nicer frame here.

00:43:19.300 --> 00:43:21.140
OK.

00:43:21.140 --> 00:43:22.840
Move my zoomer.

00:43:22.840 --> 00:43:24.790
And now what I can do is
simply save this frame.

00:43:24.840 --> 00:43:29.710
And let me put this on the desktop.

00:43:41.000 --> 00:43:41.990
"The main point here is my frame.

00:43:41.990 --> 00:43:43.660
I can open it up in preview.

00:43:43.680 --> 00:43:47.000
Now you see this is actually the full
size of the frame and I will show you in

00:43:47.000 --> 00:43:49.060
code why this looks so much different.

00:43:49.440 --> 00:43:51.460
But before I do that,
I'll also show you that we can

00:43:51.460 --> 00:43:52.560
actually export this movie.

00:43:52.560 --> 00:43:54.300
I have this little export button here.

00:43:54.300 --> 00:43:56.400
Now I'm creating a movie.

00:43:57.610 --> 00:44:00.580
I want to prepare for
internet and instead of H264,

00:44:00.580 --> 00:44:06.300
let me just sort of time
constrain use actually DV.

00:44:10.290 --> 00:44:14.210
So what I'm doing here is actually I'm
rendering the movie on the screen and

00:44:14.270 --> 00:44:16.080
output it out into a QuickTime movie.

00:44:16.080 --> 00:44:16.840
And there it's open.

00:44:16.840 --> 00:44:18.860
This is now in the QuickTime player.

00:44:18.890 --> 00:44:20.000
And I can play back my movie.

00:44:20.000 --> 00:44:23.870
It has even some audio with it.

00:44:24.000 --> 00:44:29.540
Okay, let's have a look back
into the code of this.

00:44:29.540 --> 00:44:30.670
Okay.

00:44:32.860 --> 00:44:35.380
So as I promised first,
it's very easy to just get

00:44:35.610 --> 00:44:38.870
the CG image out of it.

00:44:39.740 --> 00:44:41.860
I have my save panel,
where I'm simply asking the user, OK,

00:44:41.860 --> 00:44:43.490
where do you want to store your image?

00:44:43.560 --> 00:44:44.400
And then I have a URL.

00:44:44.420 --> 00:44:47.640
And now what I can create
is a CG image destination.

00:44:47.730 --> 00:44:49.760
And with the CG image
destination from URL,

00:44:49.760 --> 00:44:52.310
I'm telling it I want to
use actually a JPEG here.

00:44:52.600 --> 00:44:56.440
That is my target for file format.

00:44:56.450 --> 00:44:58.560
When I have this destination,
all I need to do is,

00:44:58.640 --> 00:45:04.230
on the CI context that I have, I ask it,
give me a CG image from my

00:45:04.330 --> 00:45:07.850
effects builder pipeline,
which I have right here.

00:45:07.910 --> 00:45:13.090
And I have this image,
which I simply add now to my destination.

00:45:13.130 --> 00:45:14.060
I need to finalize it.

00:45:14.080 --> 00:45:15.250
And that writes it out as a file.

00:45:15.340 --> 00:45:16.370
So it's very simple.

00:45:16.600 --> 00:45:19.480
I have now a JPEG file on disk.

00:45:19.510 --> 00:45:21.280
The part that is slightly
different here is,

00:45:21.420 --> 00:45:25.340
you can see that I'm really using
the extent of the final image.

00:45:25.340 --> 00:45:27.820
In comparison to before,
where I actually used the extent of my

00:45:27.820 --> 00:45:31.340
buffer that I'm getting from QuickTime,
I'm now actually using the

00:45:31.340 --> 00:45:32.420
extent of the final rendering.

00:45:32.420 --> 00:45:34.370
That's why you saw how
we have this zoom blur,

00:45:34.370 --> 00:45:37.360
the really big image coming out of it.

00:45:39.660 --> 00:45:43.040
The next part is, okay,
how did we do the movie export?

00:45:43.120 --> 00:45:46.230
So for this,
we need to set up our export first.

00:45:46.380 --> 00:45:50.990
So I create a QuickTime export component
and there will be more in other sessions.

00:45:51.040 --> 00:45:54.190
I'm going a little bit fast on this now.

00:45:55.450 --> 00:46:00.400
We set up the component and the crucial
part here is actually that we set up

00:46:00.480 --> 00:46:07.300
two callbacks for the video track that
allows me to do the rendering part.

00:46:09.790 --> 00:46:15.280
For the audio that we have in here,
I simply pass through the audio.

00:46:15.350 --> 00:46:17.990
So I'm simply telling QuickTime, OK,
what is your standard behavior

00:46:17.990 --> 00:46:19.350
of passing through the audio?

00:46:19.350 --> 00:46:23.630
And that is what I'm doing right here,
since I'm just talking about

00:46:23.630 --> 00:46:25.570
image parts and not audio.

00:46:26.770 --> 00:46:28.520
Let the user change the settings.

00:46:28.640 --> 00:46:32.390
That picks the compressor.

00:46:32.390 --> 00:46:35.660
And now, I already mentioned that
I'm rendering on screen.

00:46:35.690 --> 00:46:38.050
This is probably not a good
practice in your applications.

00:46:38.050 --> 00:46:39.950
First of all,
you normally don't want to really

00:46:40.150 --> 00:46:43.340
show what you're compressing on
screen and it has a slight downside.

00:46:44.520 --> 00:46:48.120
The coordinate system that we use in
OpenGL is exactly flip-flop to the

00:46:48.120 --> 00:46:49.800
one that we actually use in QuickTime.

00:46:49.810 --> 00:46:53.330
So what I have to do is I have
to read back the pixels and

00:46:53.330 --> 00:46:57.030
then flip them so that I'm not
rendering my movie upside down.

00:46:57.460 --> 00:47:00.170
So I need two buffers.

00:47:00.850 --> 00:47:05.440
I have hard coded here to 720 by 480
just because I'm a little bit lazy.

00:47:05.440 --> 00:47:10.560
And then I have my first context in which
I'm actually reading back and then I'm

00:47:10.560 --> 00:47:16.920
flipping my pixels so that I actually
have them in the QuickTime format.

00:47:18.070 --> 00:47:20.730
Then I need to set up my
image description so that

00:47:20.730 --> 00:47:24.140
actually QuickTime knows what
I'm really rendering here.

00:47:26.980 --> 00:47:29.360
And then I call MovieExpress
from Procedures.

00:47:29.360 --> 00:47:32.870
So it will now call in a tight loop for
every frame that it wants to render.

00:47:32.900 --> 00:47:36.690
So let's look actually how
this frame callback looks like.

00:47:44.000 --> 00:47:47.720
So it starts always by calling
me for each single frame.

00:47:47.720 --> 00:47:49.720
And so this is running in a tight loop.

00:47:49.940 --> 00:47:52.360
The part that's important is
we're dealing with core images.

00:47:52.360 --> 00:47:53.890
It's using Objective-C code.

00:47:53.900 --> 00:47:58.610
And so we definitely have a need
for creating an auto-release pool.

00:47:58.930 --> 00:48:02.410
As we are doing this in a tight loop,
normally you would just release

00:48:02.410 --> 00:48:04.260
your auto-release pool at
the end of an event loop,

00:48:04.260 --> 00:48:07.960
which is not efficient enough
here because you do this over a

00:48:08.000 --> 00:48:10.650
long period of time for multiple
frames and for each frame you will

00:48:10.880 --> 00:48:12.580
accumulate some auto-released objects.

00:48:12.580 --> 00:48:15.280
So you will mash up your memory
and you don't want to do that.

00:48:15.400 --> 00:48:18.290
So it's important that you create an
auto-release pool around your rendering

00:48:18.290 --> 00:48:19.770
that you do here in Core Image.

00:48:19.840 --> 00:48:22.540
So that's why I set up one and
at the end of this call I simply

00:48:22.540 --> 00:48:24.380
release this auto-release pool.

00:48:28.550 --> 00:48:32.920
I'm now using the time that
gets passed into me and set

00:48:32.920 --> 00:48:35.670
the movie point to this time.

00:48:35.670 --> 00:48:39.120
I make sure that it really
renders and then I really

00:48:39.120 --> 00:48:41.380
render my frame onto the screen.

00:48:41.380 --> 00:48:43.490
As I said,
that's not probably the best behavior,

00:48:43.500 --> 00:48:46.260
but this makes it a little bit
easier in this demo to show.

00:48:46.260 --> 00:48:50.190
I render everything and now all I need
to do is I need to read it back.

00:48:50.260 --> 00:48:52.200
I'll look into the read
back part in just a moment.

00:48:53.190 --> 00:48:56.370
Now I have my pixels coming back from
the graphics card and as I mentioned,

00:48:56.370 --> 00:48:59.310
I have to flip them and that's what
I'm just doing in this section here.

00:48:59.410 --> 00:49:01.900
It's just a very simple flipping code.

00:49:02.210 --> 00:49:06.640
And I can return my rendered pixels
into the parameters and that's all

00:49:06.640 --> 00:49:11.100
I need to do actually in my movie
callback to do the rendering part.

00:49:11.260 --> 00:49:13.870
So let's have a look
at the readback part.

00:49:14.810 --> 00:49:16.190
Very scary OpenGL code.

00:49:16.200 --> 00:49:17.740
It's just a few lines.

00:49:17.770 --> 00:49:20.380
Make sure that we keep our state before.

00:49:20.380 --> 00:49:25.000
Then we set up our memory layout and
all we need to call is GLReadPixels.

00:49:25.060 --> 00:49:28.380
This reads it back from the
graphics card synchronously.

00:49:28.410 --> 00:49:29.940
And then we clean up and we are done.

00:49:29.970 --> 00:49:36.580
So we have our frame coming
back from the graphics card.

00:49:36.580 --> 00:49:36.580
And that simply exports the whole movie.

00:49:36.940 --> 00:49:38.750
Now there's one part that
I actually want to do a little bit

00:49:39.030 --> 00:49:42.080
here to show you how easy it is
actually to deal with Core Image.

00:49:42.100 --> 00:49:45.150
And I have my little cheat sheet here.

00:49:46.230 --> 00:49:50.090
The version that you already have on your
disks shows actually a little bit more.

00:49:50.090 --> 00:49:52.800
It shows already a time
code on top of your video.

00:49:52.800 --> 00:49:56.090
So let me show you actually
how I simply can add this.

00:49:56.230 --> 00:49:59.600
So all that I need to do is
I need a composite filter.

00:49:59.660 --> 00:50:01.090
This is my very first step.

00:50:01.090 --> 00:50:04.590
And let me add this to my video view.

00:50:16.100 --> 00:50:29.300
[Transcript missing]

00:50:30.400 --> 00:50:46.800
[Transcript missing]

00:50:52.400 --> 00:50:53.500
And release my filter.

00:50:53.500 --> 00:50:54.480
Let me do this right here.

00:50:54.500 --> 00:51:01.140
And the last part is now
in the rendering part,

00:51:01.140 --> 00:51:04.720
I simply need to chain this
into my current rendering part.

00:51:04.800 --> 00:51:09.030
So I'm doing this here,
and I go back to my filter rendering.

00:51:15.740 --> 00:51:19.580
And at the very end of it,
before I actually draw,

00:51:19.590 --> 00:51:20.580
I will add my filter.

00:51:20.580 --> 00:51:22.880
So I'm using the composite filter.

00:51:22.880 --> 00:51:27.870
The output of the effect filter
is now my real background image.

00:51:27.870 --> 00:51:30.870
And then on top of it,
I put a timecode image.

00:51:30.910 --> 00:51:33.120
And I have already prepared
that a little bit up front.

00:51:33.120 --> 00:51:36.740
So we have this timecode overlay from
which I'm getting another CI image.

00:51:36.740 --> 00:51:38.760
And so this is simply
put on top of it now.

00:51:39.460 --> 00:51:42.820
Now, instead of using the
output of my effect filter,

00:51:42.820 --> 00:51:46.150
I'm using my composite filter
here and draw that image.

00:51:46.150 --> 00:51:50.760
Now, let's hope that everything
works fine and let's run this.

00:51:50.760 --> 00:51:54.890
It doesn't want to save.

00:51:54.930 --> 00:51:54.930
Okay.

00:51:58.100 --> 00:52:02.980
And there you can see I have now a
timecode track right on top of my screen.

00:52:02.980 --> 00:52:06.160
And this is how simple I can render

00:52:06.820 --> 00:52:09.700
with Core Image.

00:52:09.700 --> 00:52:12.340
OK, let's go back to slides, please.

00:52:12.340 --> 00:52:13.800
Thank you.

00:52:18.580 --> 00:52:24.450
Now we talk about OpenGL as the other
way of rendering VCD onto the screen.

00:52:25.460 --> 00:52:28.320
The first part is, okay,
where we are in our pipeline,

00:52:28.350 --> 00:52:31.430
we've seen the Core Image part
and now we talk about our OpenGL.

00:52:31.430 --> 00:52:34.610
So we can do transformation
and also the rendering part.

00:52:35.340 --> 00:52:37.800
First thing when we talk
about OpenGL is shop safety.

00:52:37.800 --> 00:52:39.290
Wear your threat safety glasses.

00:52:39.300 --> 00:52:42.900
OpenGL contexts are not reentrant safe,
so you have to deal

00:52:42.900 --> 00:52:44.190
with threat safety here.

00:52:44.200 --> 00:52:48.200
Now I'm just giving you three examples
of like how to actually do this.

00:52:48.200 --> 00:52:51.180
You can use the Pthread logs,
which I'm doing in some of

00:52:51.180 --> 00:52:52.200
the sample code already.

00:52:52.200 --> 00:52:56.090
And you can also see that we can use
a shared OpenGL context to get around

00:52:56.100 --> 00:52:58.200
some of the threat safety issues.

00:52:58.200 --> 00:53:02.050
And as you saw already in Sean's demo,
there's in Tiger a new API which

00:53:02.160 --> 00:53:04.200
I really recommend to use also.

00:53:04.200 --> 00:53:07.200
It's the CGL log context
and CGL unlock context.

00:53:07.230 --> 00:53:11.610
Those are some of the calls that you can
use to make your context threat safe.

00:53:13.100 --> 00:53:15.930
You also need to do this whenever
you talk to the Visual Context,

00:53:16.040 --> 00:53:19.160
the Qt OpenGL Texture Context.

00:53:19.160 --> 00:53:21.950
Although you can actually use
IsNewMovieTextureAvailable

00:53:21.990 --> 00:53:24.520
outside of this lock because
that one is actually thread safe.

00:53:24.640 --> 00:53:26.430
So you can check outside of
your locking if you actually

00:53:26.430 --> 00:53:27.540
have a new frame available.

00:53:28.360 --> 00:53:31.670
And if you use AppKit and
you use the NSOpenGL view,

00:53:31.700 --> 00:53:33.760
there's one thing that
you need to keep in mind.

00:53:33.760 --> 00:53:38.150
There's an update call which you need to
subclass and actually make sure that you

00:53:38.210 --> 00:53:42.560
put locks before and after you call super
update as there's some OpenGL code being

00:53:42.660 --> 00:53:45.050
executed within AppKit on this part.

00:53:47.110 --> 00:53:50.240
Now even more about threat safety,
there's a very nice tool called

00:53:50.240 --> 00:53:54.560
the OpenGL Profiler and with
the WWDC disk that you got,

00:53:54.560 --> 00:53:58.000
there's a new version of it with the
developer tools which really works great

00:53:58.000 --> 00:54:01.600
and it helps you a lot because you can
set breakpoints on threat safety issues

00:54:01.710 --> 00:54:04.980
which allows you to catch the threat
safety before your machine goes downhill.

00:54:04.980 --> 00:54:11.830
And you can see more about this
also again in the Maximizing

00:54:11.830 --> 00:54:11.830
OpenGL Performance session.

00:54:12.910 --> 00:54:14.850
So we look again now at
our live video mixer.

00:54:14.920 --> 00:54:16.630
Again,
the same stuff as we've seen last year,

00:54:16.640 --> 00:54:17.730
but slightly refined.

00:54:17.800 --> 00:54:20.020
First part that we did,
so we updated everything to

00:54:20.040 --> 00:54:22.800
the latest APIs that we are
really shipping in Tiger.

00:54:22.800 --> 00:54:25.320
And we've also improved a
little bit our timings that

00:54:25.320 --> 00:54:26.800
the movies really run in sync.

00:54:27.020 --> 00:54:29.930
Because we really play three
movies into one surface,

00:54:29.930 --> 00:54:33.800
and the movies you will see is like one
video shot from different camera angles.

00:54:33.800 --> 00:54:36.800
And that's what we will render in OpenGL.

00:54:36.940 --> 00:54:39.800
And we simply use OpenGL here,
no Core Image,

00:54:39.800 --> 00:54:43.130
just to do all the rendering
effects and the compositing.

00:54:43.690 --> 00:54:46.530
There's something new in the
sample code that I will show you,

00:54:46.530 --> 00:54:49.140
which is not available to you yet,
but will be available soon.

00:54:49.140 --> 00:54:53.420
We use the AVC video services
to read back the frames and

00:54:53.600 --> 00:54:54.830
put them out on FireWire.

00:54:54.840 --> 00:55:00.110
So we again use GLReadPixels and then
we compress them using QuickTime and

00:55:00.240 --> 00:55:01.800
use the DVFireWire output.

00:55:01.800 --> 00:55:05.360
And this was worked with any kind
of even like the Core Image part.

00:55:05.360 --> 00:55:08.000
So this session for this
was actually yesterday.

00:55:08.340 --> 00:55:10.720
So you wish you knew
that already earlier,

00:55:10.720 --> 00:55:14.070
but you can still go back on the
slides and get the information when

00:55:14.070 --> 00:55:16.890
you look up for session 504 and
get the information what happens

00:55:16.890 --> 00:55:18.470
there with the new FireWire SDK.

00:55:18.520 --> 00:55:21.750
And with this, I would like to go back
to the demo machine.

00:55:30.200 --> 00:55:32.420
Okay, let me show you the
live video mixer first,

00:55:32.550 --> 00:55:33.090
what it does.

00:55:33.100 --> 00:55:39.670
So I'm opening up three movies.

00:55:51.200 --> 00:56:03.990
You can play them back.

00:56:03.990 --> 00:56:03.990
I'd like to thank Ralph here
in the audience for the great

00:56:03.990 --> 00:56:03.990
pool play that we have here.

00:56:03.990 --> 00:56:03.990
So we can now mix this on top of
each other with semi-transparency,

00:56:03.990 --> 00:56:03.990
this channel, I can use this channel.

00:56:05.240 --> 00:56:07.170
And we can use some funny shapes.

00:56:07.220 --> 00:56:11.850
I can position this movie
up in this corner over here.

00:56:12.730 --> 00:56:17.130
This one I'm going to run into a bubble
and position this into the other corner.

00:56:17.130 --> 00:56:21.130
Let's say the background I'm just
running through some fancy brush strokes.

00:56:21.180 --> 00:56:25.490
So this is simply now running with
OpenGL and QuickTime and the new

00:56:25.490 --> 00:56:30.290
part for it and therefore we have
this elaborate two camera setup here.

00:56:31.470 --> 00:56:34.690
I'm now simply having one
FireWire camera here which I'm

00:56:34.690 --> 00:56:36.140
actually running my output to.

00:56:36.140 --> 00:56:38.060
And since this screen
is a little bit small,

00:56:38.060 --> 00:56:44.660
I'm trying now to show you this
in my QuickTime movie recorder.

00:56:44.700 --> 00:56:46.160
Hopefully it should show anything.

00:56:46.160 --> 00:56:51.020
It does not want to do this now.

00:56:51.280 --> 00:56:56.080
Bear with me.

00:56:56.080 --> 00:56:58.020
We'll try this once more.

00:57:07.600 --> 00:57:14.600
[Transcript missing]

00:57:18.930 --> 00:57:21.800
It's now the small
display here of my camera.

00:57:21.800 --> 00:57:25.370
You can see that's where
actually what I'm pointing at.

00:57:25.370 --> 00:57:27.500
Make this a little bit better.

00:57:31.000 --> 00:57:35.750
And now I can really move my video
around and change this back like,

00:57:35.760 --> 00:57:37.980
okay, I don't want this brush
stroke in the background.

00:57:38.170 --> 00:57:40.100
This is running on the full frame part.

00:57:40.260 --> 00:57:42.560
I can change this to be a star.

00:57:42.560 --> 00:57:45.440
And you can see up here in my camera
display that I'm actually really

00:57:45.440 --> 00:57:47.340
playing this back out on the FireWire.

00:57:56.420 --> 00:57:59.410
So let's have a look actually
how we did this in code.

00:58:14.200 --> 00:58:17.610
So the key part that I just want
to point out here is so much how do

00:58:17.610 --> 00:58:19.900
I really do the rendering in OpenGL.

00:58:19.900 --> 00:58:23.280
And this actually reflects a little bit
back also to what we saw earlier with

00:58:23.360 --> 00:58:28.450
the jigsaw puzzle where we use something
which is called multi-texturing,

00:58:28.450 --> 00:58:33.230
where I actually use one texture as
a mask to really render my video.

00:58:39.360 --> 00:58:42.030
So you saw that we actually have
the capability of running videos

00:58:42.150 --> 00:58:46.790
through shapes or just as a major
part I can also do it without a shape,

00:58:46.850 --> 00:58:48.940
just running it in plain.

00:58:49.000 --> 00:58:53.220
So what I'm using here is actually
I'm setting up my blending of OpenGL.

00:58:53.240 --> 00:58:55.490
I'm getting my textures.

00:58:55.990 --> 00:58:58.450
And then I'm,
all what I'm using is actually I use the

00:58:58.540 --> 00:59:00.840
color and you can see the opacity part.

00:59:00.840 --> 00:59:03.720
That is giving me the capability
of actually running with a

00:59:03.720 --> 00:59:05.600
different opacity of my video.

00:59:05.600 --> 00:59:10.090
And all I have to do is now
render this texture on a quad,

00:59:10.190 --> 00:59:12.600
my rectangle that I'm
actually rendering out.

00:59:12.600 --> 00:59:16.500
When I want to use any of the shapes,

00:59:17.720 --> 00:59:19.070
I need to set up the multi-texturing.

00:59:19.150 --> 00:59:22.240
So I'm loading one texture
as the first texture target,

00:59:22.260 --> 00:59:24.960
and the second texture as
the second texture target.

00:59:24.990 --> 00:59:29.100
The first texture is actually my shape,
which is just a grayscale image

00:59:29.100 --> 00:59:31.140
that I'm using as an alpha mask.

00:59:31.190 --> 00:59:36.640
And those two together get rendered,
then you can see slightly more code.

00:59:36.800 --> 00:59:52.100
[Transcript missing]

00:59:53.390 --> 00:59:56.730
Now the next part that I needed
to do for the speech bubble was,

00:59:56.730 --> 00:59:57.980
OK, I need something on top of it.

00:59:57.980 --> 00:59:59.130
This was the white outline.

00:59:59.130 --> 01:00:00.900
This is the shape here.

01:00:00.980 --> 01:00:03.570
So I'm again using this,
the texture for that,

01:00:03.610 --> 01:00:06.400
which is just a regular RGB texture,
and render this on one

01:00:06.530 --> 01:00:08.000
quad on top of each other.

01:00:08.040 --> 01:00:12.680
And OpenGL is compositing all these
steps on top of each other when I'm

01:00:12.680 --> 01:00:14.570
rendering it out to the graphics card.

01:00:14.630 --> 01:00:17.500
And that is actually how
I do the rendering in OpenGL.

01:00:17.560 --> 01:00:20.170
So I will go back to slides, please.

01:00:24.390 --> 01:00:25.080
So what did we use?

01:00:25.210 --> 01:00:30.400
We used GL Blend together with
Alpha to get the opacity effect

01:00:30.480 --> 01:00:32.140
that we had in the video.

01:00:32.160 --> 01:00:35.280
Then we used the masking
with multi-textures.

01:00:35.300 --> 01:00:39.590
That is actually giving us the
capability of stamping out video.

01:00:39.770 --> 01:00:41.420
And then we look at the FireWire part.

01:00:41.420 --> 01:00:43.570
I haven't showed you much in
code yet because my sample code

01:00:43.580 --> 01:00:44.970
is not quite ready for you yet.

01:00:45.020 --> 01:00:48.460
But we use again GLReadPixels to,
same way as I've done

01:00:48.570 --> 01:00:51.120
in the movie export,
to read back from the graphics card.

01:00:51.120 --> 01:00:54.400
I have to make sure that my coordinate
system is the right way around.

01:00:54.400 --> 01:00:57.330
Then I compress it into
DV by using QuickTime,

01:00:57.330 --> 01:00:59.490
just as a simple codec for this.

01:00:59.500 --> 01:01:03.400
And then I can simply use the
AVC video services to render

01:01:03.400 --> 01:01:05.540
this out through FireWire.

01:01:05.560 --> 01:01:08.700
And the sample code will hopefully
be available soon for you so you can

01:01:08.700 --> 01:01:10.590
all see which kind of tricks we used.

01:01:13.110 --> 01:01:16.340
and now comes the part where hopefully
you don't get any X thrown at me.

01:01:16.390 --> 01:01:19.940
There are problems and I need
to talk about those as well.

01:01:20.970 --> 01:01:24.090
There are certain hardware limitations
that we have to deal with when we

01:01:24.090 --> 01:01:26.900
talk about OpenGL and Core Image.

01:01:26.930 --> 01:01:31.630
First of all, when you talk about OpenGL,
we have to have a Quartz Extreme capable

01:01:32.250 --> 01:01:33.900
graphics hardware on all displays.

01:01:33.900 --> 01:01:38.920
So if you found in your attic an old
H128 PCI card that takes the Quartz

01:01:39.080 --> 01:01:43.590
Extreme capability available for
QuickTime in May simply from you,

01:01:43.600 --> 01:01:46.000
you can't use those two together.

01:01:46.170 --> 01:01:49.020
This is a limitation that
we have to deal with.

01:01:49.060 --> 01:01:52.920
The other part is different graphics
cards can support different sizes

01:01:52.920 --> 01:01:56.100
of actually the drawable surface
and even like the texture size.

01:01:56.100 --> 01:01:58.020
So if you have some special
movies that are really,

01:01:58.020 --> 01:02:00.690
really big,
you might actually have to resize those

01:02:00.690 --> 01:02:04.730
down into the limitations that you
have to deal with the graphics card.

01:02:04.760 --> 01:02:08.520
This will not hit most of you because
actually the graphics hardware is pretty

01:02:08.520 --> 01:02:10.860
good that we have in our machines,
so you will not

01:02:10.860 --> 01:02:13.210
necessarily run into this,
but it can be.

01:02:13.220 --> 01:02:16.060
And when we talk about Core Image,
Core Image is always

01:02:16.060 --> 01:02:17.700
available for you on Tiger.

01:02:17.700 --> 01:02:19.840
Actually,
the live video mix that we saw will

01:02:19.840 --> 01:02:22.360
run on Pantra as well with QuickTime 7.

01:02:22.370 --> 01:02:24.870
But Core Image,
always available on Tiger.

01:02:25.130 --> 01:02:29.760
But depending on the graphics hardware,
will run on the GPU or on the CPU.

01:02:29.800 --> 01:02:32.980
And for a lot of the filter effects,
actually on the CPU,

01:02:32.980 --> 01:02:36.440
will not get the real-time
performance as we've seen here.

01:02:36.470 --> 01:02:38.980
And again,
I would like to point out threat safety.

01:02:39.000 --> 01:02:42.660
With OpenGL, it's really an important
issue that you look out.

01:02:42.670 --> 01:02:45.970
Make sure that all your access
to OpenGL is locked when you

01:02:46.300 --> 01:02:49.020
render from multiple threats,
as we do here.

01:02:49.020 --> 01:02:50.490
here.

01:02:51.930 --> 01:02:55.870
So we've covered quite a bit today
and so let's just recapture a little

01:02:55.870 --> 01:03:00.030
bit more what we really have as
an important message to you here.

01:03:00.150 --> 01:03:02.210
So the first part,
we have our new rendering

01:03:02.210 --> 01:03:05.110
architecture where QuickTime sits
on top of our Quartz layers,

01:03:05.330 --> 01:03:09.160
Core Graphics, Core Image,
Core Video and then we render through

01:03:09.160 --> 01:03:12.400
OpenGL into the graphics hardware.

01:03:13.180 --> 01:03:19.100
We have the new video pipeline where
you can simply use the Qt Movie View or

01:03:19.100 --> 01:03:23.370
the HI Movie View as the easiest
step when you just want to render

01:03:23.370 --> 01:03:26.100
out video to the graphics hardware.

01:03:26.100 --> 01:03:29.670
Or if you want to do your
own pipeline effects,

01:03:29.900 --> 01:03:34.100
you simply can use the
Core Image and also the OpenGL part

01:03:34.100 --> 01:03:37.290
to customize your frame pipeline.

01:03:38.130 --> 01:03:40.630
More information,
you definitely find more on our website.

01:03:40.630 --> 01:03:43.620
There's normally a lot
of documentation there,

01:03:43.990 --> 01:03:46.200
sample code and all the other
resources like the iSchool

01:03:46.200 --> 01:03:47.740
that was already pointed out.

01:03:49.700 --> 01:03:50.460
Sessions.

01:03:50.470 --> 01:03:52.120
Well, unfortunately we don't
have a time machine.

01:03:52.120 --> 01:03:55.640
The FireWire session was yesterday,
but you still find the information.

01:03:55.660 --> 01:03:59.000
Then I would like to point out
today afternoon Sam Bushel's

01:03:59.040 --> 01:04:01.630
advanced video formats,
always very entertaining,

01:04:01.630 --> 01:04:03.240
also very informative.

01:04:03.250 --> 01:04:07.000
Where you see more about the export
and also the sequence grabber part.

01:04:07.030 --> 01:04:08.860
Then we look into the color imaging part.

01:04:09.080 --> 01:04:13.460
The color sync is also on
Wednesday afternoon and the

01:04:13.460 --> 01:04:18.200
maximizing OpenGL performance part
is a lab also and is a session.

01:04:19.000 --> 01:04:20.490
Feedback on Friday.

01:04:20.690 --> 01:04:24.210
You can talk to us and we will
hopefully also listen to you.

01:04:26.260 --> 01:04:28.080
And you can find us in the lab.

01:04:28.110 --> 01:04:32.260
We have a graphics and media lab and
you can just go from here down the hall,

01:04:32.310 --> 01:04:33.400
take a turn and there we are.

01:04:35.650 --> 01:04:39.100
Sample code you've seen already
that we introduced the video viewer,

01:04:39.100 --> 01:04:40.430
which was Sean's sample today.

01:04:40.440 --> 01:04:42.400
We have the CIVideo Demo GL.

01:04:42.400 --> 01:04:46.600
Watch out that this is a new version
that you have on the WWDC distribution.

01:04:46.670 --> 01:04:48.600
Same goes for the live video mixer.

01:04:48.600 --> 01:04:52.910
Then we have a new Visual Context sample
also from Sean that shows a little bit

01:04:52.910 --> 01:04:55.330
more about how to use the Visual Context.

01:04:55.330 --> 01:04:59.280
And we have two one-on-one sessions
actually that just get you going on

01:04:59.290 --> 01:05:01.720
Core Video and the Core Image part.