WEBVTT

00:00:05.010 --> 00:00:08.020
Welcome to session 124,
threading on Mac OS X.

00:00:08.200 --> 00:00:09.830
If that's not where
you're supposed to be,

00:00:09.840 --> 00:00:13.380
now's your chance to make a run for it.

00:00:14.720 --> 00:00:16.810
I'm not Mark Tozer,
I'm not an evangelist,

00:00:16.870 --> 00:00:19.340
I don't play one on TV.

00:00:19.390 --> 00:00:24.860
So, George Warner, here I am,
the schizophrenic optimization scientist.

00:00:24.910 --> 00:00:26.660
They cut my email,
if you ever need to get hold of me,

00:00:26.660 --> 00:00:29.280
it's geowar@apple.com.

00:00:30.980 --> 00:00:34.040
So what we're going to talk about today,
threading on Mac OS X,

00:00:34.080 --> 00:00:36.280
our agenda is basically we're going
to talk about some terminology,

00:00:36.280 --> 00:00:38.940
so you'll understand
what I'm talking about,

00:00:39.030 --> 00:00:39.800
the terms.

00:00:39.800 --> 00:00:41.920
We'll talk about the why, the why not.

00:00:41.920 --> 00:00:45.010
Talk about threading architectures.

00:00:45.020 --> 00:00:47.830
We'll go into some do's and some don'ts.

00:00:47.880 --> 00:00:51.410
This is pretty much stocked
from presentations I've

00:00:51.410 --> 00:00:53.320
made any number of times.

00:00:53.540 --> 00:00:56.000
This next section,
how to write thread-safe code,

00:00:56.000 --> 00:01:00.290
I really was kind of tasked with
coming up with something really

00:01:00.370 --> 00:01:04.640
hands-on that you could take away
that would be useful when you sit

00:01:04.640 --> 00:01:07.740
down in front of your computer and
actually get ready to write some code.

00:01:07.740 --> 00:01:09.680
And so I've got a whole
new section there.

00:01:09.680 --> 00:01:14.300
Then I have a quick little demo,
and then we'll break for some Q&As.

00:01:22.170 --> 00:01:25.080
There we go, threading terminology.

00:01:25.230 --> 00:01:27.900
So thread versus processes.

00:01:27.990 --> 00:01:31.210
First off, a thread is an independent
execution path,

00:01:31.220 --> 00:01:34.320
preemptively or cooperatively scheduled.

00:01:34.350 --> 00:01:55.090
And a process is a collection of threads
plus the resources necessary to run them.

00:01:55.090 --> 00:01:55.090
Now the reason I make this distinction
is we have this thing called a task.

00:01:55.090 --> 00:01:55.090
In Unix land, a task is a process,
but in Carbon land, a task is a thread.

00:01:55.090 --> 00:01:55.090
So I kind of avoid using the word task.

00:01:55.090 --> 00:01:55.090
I'm going to say thread
when I mean thread,

00:01:55.090 --> 00:01:55.090
and I'll say process when I mean process.

00:01:56.750 --> 00:02:00.050
Multiprocessing is a special
case of multitasking.

00:02:00.220 --> 00:02:03.160
I'll probably slip up and say
multiprocessing occasionally.

00:02:03.160 --> 00:02:04.690
I usually mean multithreading.

00:02:04.690 --> 00:02:09.740
The main reason I explain this one is,
I remember back when 8.6 was released,

00:02:09.740 --> 00:02:12.160
when we first released
the dual processor G4s,

00:02:12.160 --> 00:02:16.060
a developer would come in with a problem,
and I would suggest MP as a solution.

00:02:16.060 --> 00:02:19.780
And he'd go,
but my customers don't have MP boxes.

00:02:19.780 --> 00:02:23.910
Well,
MP multiprocessors and MP multiprocessing

00:02:23.910 --> 00:02:26.300
are two different things.

00:02:26.300 --> 00:02:30.430
So I'll typically try to use
multitasking or multithreading.

00:02:32.780 --> 00:02:35.220
Reentrant is a term that
gets banned about a lot,

00:02:35.270 --> 00:02:36.320
gets thrown out.

00:02:36.450 --> 00:02:40.690
Basically what it means is
concurrently executing the same code,

00:02:40.690 --> 00:02:45.060
either time-sliced or actually
parallel on two different

00:02:45.060 --> 00:02:47.090
processors at the same time.

00:02:49.290 --> 00:02:51.480
So parallelism is a
subset of concurrency.

00:02:51.480 --> 00:02:53.930
That's when you actually have
the same code running on two

00:02:53.940 --> 00:02:56.300
processors at the same time.

00:02:59.600 --> 00:03:02.040
So thread state.

00:03:02.040 --> 00:03:04.400
There's five thread states
that I typically use.

00:03:04.400 --> 00:03:05.530
I'll explain what these are.

00:03:05.540 --> 00:03:07.240
I kind of use them fast and loose.

00:03:07.240 --> 00:03:09.140
I'm used to using the terms,
but I want to make sure you

00:03:09.140 --> 00:03:10.430
understand what I'm talking about.

00:03:10.430 --> 00:03:16.060
Just to make it easier,
I drew a cute little picture,

00:03:16.230 --> 00:03:18.150
if I can get to it.

00:03:22.500 --> 00:03:25.150
Here we go.

00:03:25.160 --> 00:03:26.300
Oh, they stripped.

00:03:26.300 --> 00:03:31.650
They didn't get the graphics cleaned up.

00:03:32.300 --> 00:03:37.880
When you create a thread,
it's in a suspended state.

00:03:37.900 --> 00:03:40.060
It's waiting for a CPU.

00:03:40.060 --> 00:03:41.300
That's the orange box.

00:03:41.310 --> 00:03:42.130
It's ready.

00:03:42.310 --> 00:03:44.800
As soon as the scheduler runs
and it's of the highest priority

00:03:44.800 --> 00:03:46.820
and there's a CPU available,
it'll actually get

00:03:46.820 --> 00:03:49.500
scheduled and start running,
which is the green box.

00:03:49.500 --> 00:03:52.690
After its quorum,
which is typically about 10 microseconds,

00:03:52.690 --> 00:03:55.370
so 100 times a second,
it'll get preempted and put

00:03:55.370 --> 00:03:58.040
back in the ready state,
and someone else will get a turn.

00:04:00.110 --> 00:04:03.740
From the running state, it can also,
if it calls a blocking I/O routine,

00:04:03.860 --> 00:04:08.060
it'll go into a wait mode where it's
waiting for the I/O to complete.

00:04:08.060 --> 00:04:10.360
It's basically a form of suspension.

00:04:10.360 --> 00:04:13.220
And when the I/O completes,
when it unblocks and the

00:04:13.280 --> 00:04:17.110
computer continues running,
it'll be woken up and go back into a

00:04:17.110 --> 00:04:19.850
ready state where it's waiting for a CPU.

00:04:19.860 --> 00:04:22.640
The top gray box is suspended.

00:04:23.030 --> 00:04:26.370
If while it's running or
while it's ready to run,

00:04:26.390 --> 00:04:31.640
an external thread calls a suspend API,
it'll be taken out of the run queue

00:04:31.830 --> 00:04:34.880
and put in the suspended queue
where it's basically being held.

00:04:34.880 --> 00:04:37.410
It won't get any CPU time
and it's sitting there.

00:04:37.510 --> 00:04:41.360
From either the running mode
or from the suspended mode,

00:04:41.530 --> 00:04:44.520
it can be terminated,
in which case it's basically a zombie

00:04:44.520 --> 00:04:47.260
until this operating system cleans it up.

00:04:47.380 --> 00:04:52.490
And that's the last gray circle
that the text got wiped out of.

00:04:53.480 --> 00:04:56.150
So why use threads?

00:04:56.320 --> 00:05:01.680
There's customer expectation that your
application is going to be responsive.

00:05:01.700 --> 00:05:06.440
And if he clicks on a button and
your application starts running,

00:05:06.460 --> 00:05:09.770
and he goes to, like,
move a window or access a

00:05:09.770 --> 00:05:13.470
menu or something like this,
and he can't, basically,

00:05:13.630 --> 00:05:15.650
he's got a nonresponsive application.

00:05:15.650 --> 00:05:17.740
He's less than happy
with the performance.

00:05:17.760 --> 00:05:22.400
It's an expectation that user
interactions can happen at any time.

00:05:22.400 --> 00:05:26.830
So by using threads and putting
synchronous calls and stuff like

00:05:26.830 --> 00:05:29.420
this over on their own threads,
long execution,

00:05:29.420 --> 00:05:33.050
long computation cycles and stuff
like this all on their own threads,

00:05:33.090 --> 00:05:37.510
you keep the main event loop responsive,
you keep the user experience optimized.

00:05:43.260 --> 00:05:47.140
No spinning beach ball.

00:05:47.190 --> 00:05:48.130
So scalability.

00:05:48.140 --> 00:05:50.400
We mentioned earlier the multiprocessors.

00:05:50.440 --> 00:05:53.750
If a guy goes out and spends the money
to get one of the dual processor boxes,

00:05:53.860 --> 00:05:57.210
he kind of expects to see
both processors getting used.

00:05:57.400 --> 00:06:00.500
And if he runs your application,
and one processor is maxed out,

00:06:00.500 --> 00:06:02.780
and the other processor
is completely idle,

00:06:02.820 --> 00:06:05.220
he's pretty much aware of the
fact that he's not getting

00:06:05.220 --> 00:06:06.670
everything that he paid for.

00:06:06.820 --> 00:06:08.970
And so scalability is a good thing.

00:06:09.210 --> 00:06:12.410
MP hardware equals MP performance.

00:06:13.490 --> 00:06:16.780
Speaking of scalability,
this is a chart I drag out every year.

00:06:16.890 --> 00:06:22.020
So I think the first time I ran
these numbers was in 1988,

00:06:22.080 --> 00:06:27.060
'89-- excuse me, '98 and '99,
when we were working on 8.6,

00:06:27.060 --> 00:06:28.400
the MP implementation.

00:06:28.510 --> 00:06:33.600
We took a bunch of the common
Photoshop filters and multithreaded them.

00:06:33.620 --> 00:06:37.260
And for most things, motion blur,
unsharp mask, Gaussian blur,

00:06:37.360 --> 00:06:43.960
we got an incremental amount of
about 30%, 80%-- can't remember,

00:06:43.960 --> 00:06:46.520
can't read it-- and 90% increase.

00:06:46.590 --> 00:06:49.100
The one that's always
interesting about this slide is,

00:06:49.100 --> 00:06:53.900
like, maximize here was 2.3 times
faster than the single scaler.

00:06:53.900 --> 00:06:57.280
Well, the first time I saw this number,
I thought, there must be something

00:06:57.280 --> 00:06:58.560
wrong with my testing.

00:06:58.610 --> 00:07:00.620
There's only twice as much horsepower.

00:07:00.680 --> 00:07:03.380
How could it actually be
more than twice as fast?

00:07:03.380 --> 00:07:07.570
And so I went back and re-ran my data,
re-ran my test, and checked my timings

00:07:07.570 --> 00:07:08.700
and everything else.

00:07:08.700 --> 00:07:12.500
And after a bit of head scratching,
finally come to realize I didn't

00:07:12.500 --> 00:07:16.580
only have twice the horsepower,
I also had twice the cache.

00:07:16.650 --> 00:07:21.080
And my data set,
what I was doing the test data on,

00:07:21.160 --> 00:07:24.590
didn't fit so well in a single cache,
but it fit really extremely

00:07:24.590 --> 00:07:25.910
well in two caches.

00:07:26.020 --> 00:07:29.590
And so by dividing the job up between
the two processors and the two caches,

00:07:29.590 --> 00:07:33.660
it actually ran better than it did
on a single-- than just a times two.

00:07:33.920 --> 00:07:38.280
And that's what we typically
refer to as super scaler.

00:07:38.370 --> 00:07:42.160
So why use threads?

00:07:42.960 --> 00:07:45.170
So preemption.

00:07:45.200 --> 00:07:47.040
More threads equals more CPU time.

00:07:47.040 --> 00:07:50.030
This is kind of a sneaky
way to get more CPU time.

00:07:50.390 --> 00:07:53.460
If there's two processors running and
they're both in the same priority,

00:07:53.530 --> 00:07:56.160
you're both getting about half the CPU.

00:07:56.200 --> 00:07:59.800
But if you're running two threads
and he's only running one,

00:07:59.820 --> 00:08:02.050
then you'll get two-thirds
of the CPU time,

00:08:02.050 --> 00:08:03.540
and he'll only get one-third.

00:08:03.890 --> 00:08:07.410
So it's a sneaky way
to grab more CPU time.

00:08:07.840 --> 00:08:11.130
I call it the store shelf policy.

00:08:11.270 --> 00:08:13.600
If you ever go to the store and
go to the laundry detergent,

00:08:13.600 --> 00:08:18.140
you see All and Cheer and Biz and
Fizz and all these 14 different

00:08:18.140 --> 00:08:19.990
thousand brands of laundry detergent.

00:08:20.100 --> 00:08:23.930
And you wonder why there's so many
brands until you realize that half of

00:08:23.930 --> 00:08:26.620
them are manufactured by one company.

00:08:26.660 --> 00:08:29.480
And it's basically their way
of getting more shelf space.

00:08:29.760 --> 00:08:32.240
So this is kind of the same thing.

00:08:33.390 --> 00:08:35.790
Synchronous request.

00:08:36.070 --> 00:08:38.320
I mentioned earlier
about a responsive UI.

00:08:38.470 --> 00:08:44.280
If you've got a synchronous
operation that blocks and prevents

00:08:44.340 --> 00:08:49.200
your UI from being responsive,
put it in its own thread.

00:08:49.300 --> 00:08:51.200
If it blocks a separate thread,
it doesn't matter.

00:08:51.200 --> 00:08:54.840
If it blocks the main thread,
you're going to affect

00:08:54.840 --> 00:08:56.620
the user experience.

00:08:57.940 --> 00:09:00.050
So polling is bad.

00:09:00.120 --> 00:09:02.260
So this is, to me,
is the equivalent-- if you've ever

00:09:02.260 --> 00:09:05.090
gone on vacation with a nine-year-old,
this is the equivalent of,

00:09:05.140 --> 00:09:06.080
are you there yet?

00:09:06.090 --> 00:09:06.890
Are we there yet?

00:09:07.110 --> 00:09:08.580
Are we there yet?

00:09:08.600 --> 00:09:11.520
And for me, especially,
about as annoying.

00:09:11.700 --> 00:09:15.120
So block instead.

00:09:15.800 --> 00:09:17.700
Tell them to take a nap
until we get to grandma's.

00:09:17.700 --> 00:09:18.860
I'll wake you up.

00:09:18.880 --> 00:09:21.060
So I wasn't talking to you.

00:09:21.060 --> 00:09:23.160
Not now.

00:09:27.500 --> 00:09:29.290
So why not?

00:09:29.320 --> 00:09:36.280
Well, a little fun with the clicker here.

00:09:36.330 --> 00:09:38.070
Added overhead.

00:09:38.150 --> 00:09:41.810
So it takes a finite amount of
time to actually spawn a thread,

00:09:41.810 --> 00:09:48.000
to create it, and set it up,
get it ready to run, get it scheduled.

00:09:53.100 --> 00:09:54.860
We'll get there eventually.

00:09:54.860 --> 00:09:56.160
Here we go.

00:09:56.230 --> 00:09:59.320
So on a 2.7 gigahertz,
our fastest G5 right now,

00:09:59.340 --> 00:10:01.420
it takes about 126 microseconds.

00:10:01.630 --> 00:10:04.320
So sure,
you can do about 4,000 of those a second,

00:10:04.320 --> 00:10:09.850
but you'd probably rather be doing
some computation in that time.

00:10:09.910 --> 00:10:14.020
So my kind of rule of thumb is anything
that takes less than that amount of time

00:10:14.040 --> 00:10:18.870
to compute is kind of a no-win to go
throw it in a thread when it's going to

00:10:18.870 --> 00:10:21.060
take it that long to create the thread.

00:10:22.010 --> 00:10:26.000
So the preemption time,
so every hundredth of a second when a

00:10:26.070 --> 00:10:30.200
task gets preempted and it gets swapped
out and another context gets loaded in,

00:10:31.250 --> 00:10:32.960
it's 30 microseconds.

00:10:33.040 --> 00:10:37.980
Now we've improved this on a 2.0 system,
this was about 40 microseconds,

00:10:38.040 --> 00:10:40.110
and if you do the math,
we did a little bit

00:10:40.110 --> 00:10:41.440
better than just linear.

00:10:41.620 --> 00:10:44.360
We've gotten that time down a little bit.

00:10:45.600 --> 00:10:47.340
The time quorum,
which I've already mentioned,

00:10:47.340 --> 00:10:50.110
that's about 10 microseconds,
about 100 times a second

00:10:50.110 --> 00:10:51.240
that we do that switch.

00:10:51.240 --> 00:10:56.050
So if you've got thousands of threads,
you're going to probably spend more time

00:10:56.050 --> 00:10:57.600
switching than actually doing any work.

00:10:57.640 --> 00:11:00.240
So you want to really avoid
doing too many threads.

00:11:00.240 --> 00:11:03.800
I'll talk a little bit
more about that later.

00:11:04.110 --> 00:11:05.790
So why not memory?

00:11:06.090 --> 00:11:09.460
So for every thread,
every pthread that spawned,

00:11:09.520 --> 00:11:14.260
we locked down 2K of system
memory down in the kernel.

00:11:14.480 --> 00:11:16.360
And that's physical memory.

00:11:16.360 --> 00:11:19.060
So you definitely don't want to
waste too many kernel resources.

00:11:19.060 --> 00:11:23.270
So thousands of threads,
probably not a good idea.

00:11:23.410 --> 00:11:26.440
Part of what that is
that we're locking down,

00:11:26.440 --> 00:11:29.780
the hardware context,
32 general purpose registers,

00:11:29.920 --> 00:11:33.960
32 floating point registers,
floating point status control register,

00:11:33.960 --> 00:11:37.740
transfer link count registers,
memory management registers, et cetera,

00:11:37.740 --> 00:11:40.360
velocity engine registers,
pretty much the entire hardware

00:11:40.370 --> 00:11:43.060
context of that running thread.

00:11:43.280 --> 00:11:46.790
The 32 bit-- I mean,
the 32 general purpose registers on a G5,

00:11:46.790 --> 00:11:48.310
obviously, are 64 bit.

00:11:48.370 --> 00:11:52.120
So that's a pretty big chunk of memory.

00:11:52.540 --> 00:11:55.950
And the user land resources,
this would be the frameworks that threads

00:11:55.950 --> 00:12:02.460
are implemented in at a higher level,
like MP for Carbon, NSTask for Cocoa,

00:12:02.460 --> 00:12:03.750
et cetera.

00:12:03.760 --> 00:12:05.500
There's some user land resources.

00:12:05.500 --> 00:12:08.970
At the very least,
there's a 512K byte virtual

00:12:08.970 --> 00:12:10.700
stack for each thread.

00:12:10.700 --> 00:12:12.660
Obviously, you have a lot of threads.

00:12:12.660 --> 00:12:14.700
You can eat up a lot of
your virtual address space.

00:12:14.700 --> 00:12:17.140
Not a too bad idea.

00:12:17.140 --> 00:12:19.340
So added complexity.

00:12:20.790 --> 00:12:25.840
If you have data structures
that you have to protect,

00:12:25.880 --> 00:12:27.500
then you've got to write locks codes.

00:12:27.500 --> 00:12:30.290
If you've ever written
deadlock prevention code,

00:12:30.290 --> 00:12:32.040
it can get very complicated.

00:12:32.040 --> 00:12:34.450
If you're doing multiple
things in multiple threads,

00:12:34.450 --> 00:12:36.640
you have to think of all
those things at the same time.

00:12:36.640 --> 00:12:38.710
That's where I picked
up the schizophrenia,

00:12:38.710 --> 00:12:39.230
I think.

00:12:39.240 --> 00:12:43.110
So the shared data may require locks.

00:12:43.120 --> 00:12:46.060
And the last reason,
non-thread safe APIs.

00:12:46.060 --> 00:12:49.780
If an API is non-thread safe,
you can't call it from another thread.

00:12:50.680 --> 00:12:52.480
Then you have to call
it from the main thread.

00:12:52.480 --> 00:12:54.020
And so that's why not to use.

00:12:54.020 --> 00:12:57.270
So 100 threads,
I've already mentioned all of

00:12:57.270 --> 00:13:00.360
the reasons why 100 threads,
a lot of threads.

00:13:00.400 --> 00:13:02.280
The more threads you are,
the more memory you used, et cetera.

00:13:02.280 --> 00:13:04.240
So other options.

00:13:04.240 --> 00:13:06.200
Cooperative threads.

00:13:06.200 --> 00:13:09.140
For the non-thread safe APIs,
if you need thread-type behavior,

00:13:09.140 --> 00:13:13.800
you can use the old 68K from
however many years ago now.

00:13:13.800 --> 00:13:15.370
Over 15 years, I think.

00:13:15.400 --> 00:13:17.120
Thread manager.

00:13:17.120 --> 00:13:18.610
Yield to any thread.

00:13:18.620 --> 00:13:19.460
Yield to thread.

00:13:19.460 --> 00:13:20.460
And use thread.

00:13:20.680 --> 00:13:22.880
Use cooperative threads
and call thread safe APIs.

00:13:22.880 --> 00:13:27.660
If you have a repetitive task
that repeats at a fixed interval,

00:13:27.660 --> 00:13:29.960
timers might be a better idea.

00:13:32.990 --> 00:13:36.310
So we'll talk about some
threading architectures.

00:13:36.420 --> 00:13:38.450
So parallel threads with
parallel I/O buffers.

00:13:38.500 --> 00:13:41.660
Now this is where each one of
these threads and their data is

00:13:41.720 --> 00:13:44.290
completely independent of each other.

00:13:44.460 --> 00:13:47.200
You may be doing,
like in a word processor,

00:13:47.200 --> 00:13:49.700
maybe grammar checking,
maybe spell checking.

00:13:49.740 --> 00:13:51.140
You may be kerning.

00:13:51.180 --> 00:13:53.060
You may be doing 1,000 different things.

00:13:53.110 --> 00:13:56.000
But they're basically
independent of each other.

00:13:57.700 --> 00:14:00.700
In this case, we have parallel threads
with shared I/O buffer.

00:14:00.700 --> 00:14:03.110
This is what I call
the divide and conquer.

00:14:03.230 --> 00:14:05.830
This is where you take
something like a huge image,

00:14:05.920 --> 00:14:08.240
breaking it up into little bitty chunks.

00:14:08.310 --> 00:14:10.330
Each one of those little
chunks is fed off to another,

00:14:10.470 --> 00:14:13.970
to a different thread,
and each one of those threads crunches

00:14:13.980 --> 00:14:16.580
that chunk in pretty much the same way,
and then it's reassembled

00:14:16.580 --> 00:14:22.170
on the output to reassemble
the picture with the effect.

00:14:23.310 --> 00:14:25.510
So sequential threads
with multiple I/O buffers.

00:14:25.550 --> 00:14:29.790
I always like to call it the pizza oven,
which might be obvious why I think that.

00:14:29.870 --> 00:14:32.200
Some people call it the assembly line.

00:14:32.200 --> 00:14:37.190
I think the CS101 computer
term is provider-consumer,

00:14:37.200 --> 00:14:38.690
or something to that effect.

00:14:38.790 --> 00:14:42.960
But basically you have multiple
tasks where each task is linked to

00:14:42.960 --> 00:14:46.930
the output of the previous task,
and its output goes to the

00:14:46.930 --> 00:14:48.700
input of the next task.

00:14:48.730 --> 00:14:52.960
And it doesn't look like it's
immediately parallel until you

00:14:52.960 --> 00:14:58.380
realize that thread number three can
be computing on the third paragraph,

00:14:58.380 --> 00:15:02.140
while thread two can be working
on the second paragraph,

00:15:02.430 --> 00:15:04.680
while thread one can be
working on the first paragraph.

00:15:04.850 --> 00:15:06.700
Other way around.

00:15:06.740 --> 00:15:08.960
Thread one can be working
on the third paragraph,

00:15:08.980 --> 00:15:12.170
thread two on the second,
and thread three on the first paragraph.

00:15:12.320 --> 00:15:15.720
So as the first thread finishes the
work and passes it to the next one,

00:15:15.760 --> 00:15:18.010
he can start working
on the next paragraph.

00:15:18.240 --> 00:15:21.200
So that's an example
of sequential threads.

00:15:21.200 --> 00:15:26.200
Now most applications have both parallel
and independent execution paths.

00:15:26.350 --> 00:15:29.850
And some examples are driving
simulator where you've got an AI,

00:15:29.850 --> 00:15:32.030
physics engine, rendering, etc.

00:15:32.270 --> 00:15:34.740
An image processor where
you've got color correction,

00:15:34.850 --> 00:15:37.310
you've got filters,
you've got special effects,

00:15:37.340 --> 00:15:39.200
word processor, grammaring,
spell checking, kerning.

00:15:39.200 --> 00:15:42.710
All these things can happen
in parallel and sequence,

00:15:42.870 --> 00:15:43.180
etc.

00:15:44.620 --> 00:15:47.730
So this is about the only slide
I have on implementations.

00:15:47.730 --> 00:15:51.710
I'm not going to nail down
into any specific APIs.

00:15:51.930 --> 00:15:55.260
Carbon has the MP APIs,
Cocoa has NS threads,

00:15:55.370 --> 00:15:56.690
Java has Java threads.

00:15:56.870 --> 00:16:00.010
All of these are implemented
on top of pthreads.

00:16:00.140 --> 00:16:03.120
That's underneath it all.

00:16:04.430 --> 00:16:05.640
So threading implementations.

00:16:05.640 --> 00:16:08.340
Now all these implementations
have some things in common.

00:16:08.380 --> 00:16:12.910
Thread management, creating threads,
deleting threads, terminating threads,

00:16:12.910 --> 00:16:16.880
suspending threads,
setting thread priorities, et cetera.

00:16:16.880 --> 00:16:22.420
You can change thread behavior
from round robin to FIFO,

00:16:22.440 --> 00:16:24.900
real time threads, et cetera.

00:16:24.910 --> 00:16:28.660
Synchronization primitives
to critical sections,

00:16:28.660 --> 00:16:32.810
mutexes, semaphores, spin locks,
et cetera.

00:16:33.310 --> 00:16:34.420
and thread-safe services.

00:16:34.450 --> 00:16:37.960
And this is basically all
the toolbox things that these

00:16:37.960 --> 00:16:43.960
have that are thread-safe,
like malloc and free and etc.

00:16:44.090 --> 00:16:46.700
So we have some dos and some don'ts.

00:16:49.490 --> 00:16:51.360
Do avoid the creation
destruction overhead.

00:16:51.360 --> 00:16:55.820
I mentioned it's about 126
microseconds to spawn a thread.

00:16:55.820 --> 00:16:59.570
What you can do to work around
that is you can pull threads.

00:16:59.660 --> 00:17:02.040
You can preallocate them,
block them against the job queue,

00:17:02.040 --> 00:17:05.120
and then they're not
taking up any CPU time.

00:17:05.480 --> 00:17:07.210
When you're ready to
actually do some work,

00:17:07.270 --> 00:17:08.560
you stick it in the queue.

00:17:08.560 --> 00:17:11.520
One of the thread wakes up,
pulls the job out of the queue,

00:17:11.520 --> 00:17:13.130
executes the job.

00:17:13.130 --> 00:17:15.800
When it's done, it goes back around and
blocks on the queue again.

00:17:15.800 --> 00:17:17.800
But you've avoided the creation time.

00:17:17.800 --> 00:17:21.240
The next time you do a job,
signal the queue, same thing happens.

00:17:21.420 --> 00:17:24.790
You've avoided that overhead
of creating the job queue.

00:17:24.790 --> 00:17:27.790
I mean, creating the threads.

00:17:28.470 --> 00:17:29.290
Be data-driven.

00:17:29.300 --> 00:17:33.200
I'm kind of surprised by the number
of times that I've run into really,

00:17:33.200 --> 00:17:37.230
really complicated thread code
that kind of gets away from the

00:17:37.300 --> 00:17:40.040
basic way that we all pretty
much learned how to write code,

00:17:40.040 --> 00:17:44.100
where we have a prologue where we open,
allocate, retain our objects.

00:17:44.150 --> 00:17:47.220
We have a crunch loop where we read,
modify, write our data.

00:17:47.220 --> 00:17:49.260
And then we have an
epilogue where we close,

00:17:49.260 --> 00:17:50.920
dispose, release, et cetera.

00:17:50.920 --> 00:17:52.550
This is a good paradigm.

00:17:52.550 --> 00:17:53.300
It works.

00:17:53.300 --> 00:17:56.360
Do the same thing with your threads,
as long as all of these

00:17:56.360 --> 00:17:57.780
calls are thread-safe.

00:17:58.470 --> 00:18:02.300
In 8.6, when we first did this,
so many of these things like the open,

00:18:02.300 --> 00:18:05.410
allocate, new pointer wasn't thread-safe,
and the close,

00:18:05.430 --> 00:18:08.060
dispose wasn't thread-safe,
but the file I.O.

00:18:08.060 --> 00:18:09.110
was thread-safe.

00:18:09.110 --> 00:18:11.930
So what we did is you
could write the prologue,

00:18:12.160 --> 00:18:16.030
spawn a thread to do the crunch,
and then when the crunch was finished,

00:18:16.120 --> 00:18:19.810
the epilogue would happen back
on your main thread again.

00:18:19.810 --> 00:18:23.050
Nowadays, with more and more and more of
the toolbox being thread-safe,

00:18:23.060 --> 00:18:25.140
this is becoming less and less necessary.

00:18:25.140 --> 00:18:28.990
You can do all this in the,
right there in the in your

00:18:29.180 --> 00:18:31.750
thread in sequential code.

00:18:32.060 --> 00:18:34.360
I've got my only "don't" on this page.

00:18:34.500 --> 00:18:35.700
Let me see.

00:18:35.800 --> 00:18:37.440
And I put it here for a reason.

00:18:37.530 --> 00:18:39.370
Don't suspend, resume, or terminate.

00:18:39.530 --> 00:18:45.080
Another operating system
whose name I won't mention,

00:18:45.220 --> 00:18:47.220
for some reason thinks
this is a good model.

00:18:47.310 --> 00:18:50.770
And it really drives me crazy when
I have developers from that platform

00:18:50.770 --> 00:18:53.000
come over here and do it over here.

00:18:53.190 --> 00:18:56.270
The problem with the suspend, resume,
and terminate is,

00:18:56.400 --> 00:18:59.920
if you look at the data-driven,
that prolog, crunch, epilog,

00:18:59.920 --> 00:19:05.000
if you suspend,
what state is that thread in?

00:19:05.000 --> 00:19:06.000
Was it in the prolog?

00:19:06.000 --> 00:19:08.000
Is it halfway through the crunch?

00:19:08.000 --> 00:19:09.980
Is it in the epilog?

00:19:10.060 --> 00:19:11.020
Has it started closing files?

00:19:11.020 --> 00:19:11.990
You have no way of knowing.

00:19:12.000 --> 00:19:15.840
So if you go and terminate that thread,
chances are you've leaked memory,

00:19:15.840 --> 00:19:19.070
you've leaked ports,
your retains are off, it's basically,

00:19:19.070 --> 00:19:22.000
your application now is
in an undetermined state.

00:19:22.000 --> 00:19:27.360
So we very seriously discourage
developers from using suspend,

00:19:27.360 --> 00:19:30.000
resume, terminate models.

00:19:33.210 --> 00:19:34.800
So what can you do instead?

00:19:34.900 --> 00:19:37.270
Use synchronization primitives.

00:19:37.450 --> 00:19:42.870
You can have a cue that, or even a mutex,
that you signal that says abort.

00:19:42.870 --> 00:19:47.670
And so in your middle crunch loop,
however often that's appropriate,

00:19:47.720 --> 00:19:50.370
you check and see if
anything's in the cue.

00:19:50.380 --> 00:19:52.090
Nothing is in the cue,
continue crunching.

00:19:52.110 --> 00:19:54.770
Do it often enough that
you have a responsive UI,

00:19:54.810 --> 00:19:57.070
which is like two or
three times a second,

00:19:57.230 --> 00:20:00.850
but you don't do it like on every,
if you were doing like a big image,

00:20:00.900 --> 00:20:03.070
you wouldn't do it on every pixel.

00:20:03.100 --> 00:20:04.450
That's way too much overhead.

00:20:04.450 --> 00:20:06.860
Then you're back into the are
we there yet kind of thing.

00:20:06.860 --> 00:20:09.730
So you just want to do it
often enough so that the user

00:20:10.180 --> 00:20:14.640
experience is when he hits cancel,
within a reasonable amount of time,

00:20:14.640 --> 00:20:19.630
your loop notices, aborts your thread,
cleans up, disposes memory,

00:20:19.630 --> 00:20:23.170
falls out the bottom,
and aborts the operation.

00:20:25.900 --> 00:20:27.170
don't over- or under-lock.

00:20:27.260 --> 00:20:30.520
So I'll get in later into how
to write thread-safe code,

00:20:30.520 --> 00:20:32.570
and I'll talk a little
more about locking.

00:20:32.580 --> 00:20:36.520
But basically what you want to do,
let's say you've got a tree structure.

00:20:36.520 --> 00:20:40.060
If you lock every single access point,
you'll probably spend more time

00:20:40.060 --> 00:20:43.380
administrating all those locks,
locking and unlocking those locks,

00:20:43.380 --> 00:20:46.680
than you would if you actually acted
the time it takes to access the data.

00:20:46.720 --> 00:20:49.530
That would be over-locking.

00:20:49.560 --> 00:20:53.500
Under-locking would be having one
lock for the entire data structure.

00:20:53.500 --> 00:20:57.500
So what would happen then is everybody
that wanted to access would be blocked,

00:20:57.650 --> 00:20:59.240
waiting for whoever's got that one lock.

00:20:59.340 --> 00:21:01.080
That's definitely under-locking.

00:21:01.080 --> 00:21:05.270
So the answer there is to find a balance,
and you know your data set's the best.

00:21:05.370 --> 00:21:06.360
You have to make the decision.

00:21:06.360 --> 00:21:07.810
You have to understand the trade-offs.

00:21:07.820 --> 00:21:13.100
You should have metrics and
measure and meet your criteria

00:21:13.100 --> 00:21:14.280
for what you think is appropriate.

00:21:15.910 --> 00:21:17.030
Don't spin wait.

00:21:17.110 --> 00:21:21.030
We've already mentioned the
are we there yet scenario.

00:21:21.030 --> 00:21:23.580
In this particular case,
what I'm talking about,

00:21:23.580 --> 00:21:27.550
if you're waiting on more than one thing,
and you're waiting on multiple things,

00:21:27.550 --> 00:21:32.150
one thing I see people do is they'll go,
check that one, then check that one,

00:21:32.150 --> 00:21:34.370
then check that one,
then delay for a little bit,

00:21:34.370 --> 00:21:37.220
then check that one, then check that one,
then check that one,

00:21:37.220 --> 00:21:38.740
then delay for a little bit.

00:21:38.760 --> 00:21:40.580
And basically, they're spin locking.

00:21:40.580 --> 00:21:42.730
They're still spin waiting.

00:21:42.730 --> 00:21:45.310
And it wastes CPU cycles.

00:21:45.800 --> 00:21:48.250
If you have multiple things
that you can wait on like that,

00:21:48.250 --> 00:21:51.380
you could actually spawn three threads,
have each one of those three

00:21:51.380 --> 00:21:54.230
threads wait on one of those things,
and when any one of

00:21:54.340 --> 00:21:58.270
those threads wakes up,
it could signal the fourth thread to say,

00:21:58.490 --> 00:21:59.450
this event has happened.

00:21:59.460 --> 00:22:03.080
And the fourth thread that's waiting
on that event from any one of those

00:22:03.320 --> 00:22:07.400
other threads would then wake up,
check that one, check that one, check,

00:22:07.400 --> 00:22:10.830
oh, this one's it, process that one,
and then go back and wait

00:22:11.010 --> 00:22:12.500
for one of the other two.

00:22:13.610 --> 00:22:16.160
used separate threads to merge signals.

00:22:16.160 --> 00:22:20.950
There's also a nice Unix API signal
or something like that that allows

00:22:20.960 --> 00:22:26.420
you to do the same type of thing,
wait on multiple events at the same time.

00:22:27.310 --> 00:22:28.190
So don't GUI.

00:22:28.200 --> 00:22:34.860
Now, these sets of slides have been in my
presentation for going on six years now,

00:22:34.860 --> 00:22:38.630
and I'm really looking forward to the
day that I can remove that bullet point.

00:22:38.630 --> 00:22:41.730
It's becoming less and
less true every day.

00:22:41.730 --> 00:22:44.860
We're working digitally to
get it the way we want it,

00:22:44.860 --> 00:22:47.500
where we can just do anything anywhere.

00:22:47.500 --> 00:22:51.880
But right now, you can't do user input or
click your mouse clicks and

00:22:51.920 --> 00:22:54.130
stuff like that from your GUI.

00:22:54.130 --> 00:22:56.180
But you can call courts.

00:22:56.180 --> 00:22:57.180
You can do your drawing.

00:22:57.200 --> 00:23:00.150
you can do OpenGL.

00:23:00.800 --> 00:23:03.800
One of the tricks in this one,
and Cocoa has the same type of thing,

00:23:03.800 --> 00:23:06.420
but I'll pick on Carbon this time,
is postevent2q.

00:23:06.420 --> 00:23:10.060
So if you've got a thread that
computes a nice little graphical image,

00:23:10.060 --> 00:23:11.640
and you're ready to
draw it onto the screen,

00:23:11.640 --> 00:23:14.850
you can use postevent2q to
tell the main event loop,

00:23:15.440 --> 00:23:16.650
update the screen.

00:23:17.680 --> 00:23:19.700
And one of the nice things in
the Carbon event system is,

00:23:19.710 --> 00:23:22.690
especially for update events,
is what we call event coalescing.

00:23:22.690 --> 00:23:27.630
So if you're updating something on
the screen five or six times a second,

00:23:27.630 --> 00:23:31.170
but the main event loop is busy,
and he hasn't updated the screen yet,

00:23:31.190 --> 00:23:33.860
when you call postevent2q,
it'll look in the queue and go, oh,

00:23:33.860 --> 00:23:35.100
there's already an event in there.

00:23:35.100 --> 00:23:36.700
It won't put another one in the queue.

00:23:36.700 --> 00:23:39.200
So you don't have to worry
about overrunning the queue.

00:23:42.530 --> 00:23:45.490
So this is our new section
on writing thread-safe code.

00:23:45.680 --> 00:23:49.400
This is the part you'll hopefully
take home and find most useful.

00:23:49.400 --> 00:23:52.880
When I was preparing this,
I started working on this about

00:23:52.880 --> 00:23:56.530
a little over a month ago,
I went on the web and I was

00:23:56.530 --> 00:23:59.400
looking for a good definition
of what thread-safe meant.

00:23:59.400 --> 00:24:03.300
And unfortunately,
there's a lot of misconceptions out

00:24:03.300 --> 00:24:06.230
there of exactly what thread-safe means.

00:24:06.590 --> 00:24:10.400
But I pulled up a couple of them here,
and I'll go through them.

00:24:10.400 --> 00:24:15.990
Thread-safe code may be safely invoked
concurrently by multiple threads.

00:24:16.090 --> 00:24:17.160
Okay.

00:24:17.550 --> 00:24:20.400
Well, what does "safely" mean?

00:24:20.400 --> 00:24:23.330
Well, you know, I've been working on
computers for 30 years.

00:24:23.450 --> 00:24:26.360
I've never ever seen one blow up
and blow sparks all over the place.

00:24:26.440 --> 00:24:28.890
Regardless of what
Hollywood have you believe,

00:24:28.890 --> 00:24:30.380
I've never seen it happen.

00:24:30.510 --> 00:24:35.400
So, "safely" here, I don't know,
might be an OSHA rating or something,

00:24:35.400 --> 00:24:38.280
but it's not a very helpful definition.

00:24:38.400 --> 00:24:41.350
So, thread-safe code can be
called from multiple threads

00:24:41.460 --> 00:24:43.400
without unwanted interaction.

00:24:43.400 --> 00:24:45.330
Well, this is a little more useful.

00:24:45.400 --> 00:24:49.230
Maybe a little vague about
what unwanted interaction is,

00:24:49.460 --> 00:24:51.830
but a little bit better definition.

00:24:52.530 --> 00:24:53.930
So here's one found.

00:24:54.190 --> 00:24:57.480
Thread safe code is reentrant,
are protected from multiple

00:24:57.540 --> 00:25:02.380
simultaneous execution by
some form of mutual exclusion.

00:25:03.150 --> 00:25:05.940
I don't know how you, you know,
I know computer guys tend to

00:25:05.940 --> 00:25:09.990
be less English literate than
maybe the rest of the populace.

00:25:10.020 --> 00:25:11.060
I know I certainly was.

00:25:11.100 --> 00:25:13.540
But I was always irritated
when my English teacher,

00:25:13.540 --> 00:25:15.950
you know,
I'd ask her about a word and she'd say,

00:25:16.100 --> 00:25:19.000
"Go look it up." I'd go look in
the dictionary and I'd find five

00:25:19.000 --> 00:25:22.030
other words in the definition that
I don't know what they mean either.

00:25:22.100 --> 00:25:26.480
So, I will say that I do know
what these words mean,

00:25:26.480 --> 00:25:30.810
but I'll also admit I had
to look some of them up.

00:25:31.290 --> 00:25:31.830
Here's another one.

00:25:31.900 --> 00:25:35.280
Thread safe code is guaranteed to
compute the same result regardless of

00:25:35.280 --> 00:25:37.150
whether it is run on one thread or many.

00:25:37.160 --> 00:25:41.400
This is probably the best definition,
a little verbose,

00:25:41.400 --> 00:25:46.660
but once you understand,
once you kind of get thread safety,

00:25:46.660 --> 00:25:51.200
this is probably the definition that
kind of makes the most sense compared to

00:25:51.200 --> 00:25:52.900
what you've learned about thread safety.

00:25:52.900 --> 00:25:58.580
So I kind of mooged all these together,
and this is what I came up with.

00:25:59.380 --> 00:26:02.530
Thread safe code must behave correctly
in a single-threaded environment.

00:26:02.540 --> 00:26:05.840
If you've got code that
crashes in the main thread,

00:26:05.840 --> 00:26:09.450
it's not thread safe,
just by the fact it's not

00:26:09.450 --> 00:26:11.100
even main thread safe.

00:26:11.160 --> 00:26:14.410
So it must behave correctly in
a single-thread environment.

00:26:14.420 --> 00:26:16.090
And here's the trick.

00:26:16.180 --> 00:26:20.340
It must behave the same in
a multithreaded environment.

00:26:20.340 --> 00:26:23.540
So the way it works in a
single-thread environment,

00:26:23.540 --> 00:26:26.340
that's the way you expect it to
work in a multithreaded environment.

00:26:27.360 --> 00:26:30.610
If it doesn't, more times than not,
it's because you've got

00:26:30.770 --> 00:26:32.220
some thread safe issue.

00:26:32.220 --> 00:26:33.760
So...

00:26:35.220 --> 00:26:36.960
The problem with all
these definitions is,

00:26:36.960 --> 00:26:40.730
I don't know if any of them really
help you write thread-safe code.

00:26:40.880 --> 00:26:46.170
They just give you a definition.

00:26:46.860 --> 00:26:50.780
In the course of researching this,
Joshua Bloom wrote a

00:26:50.780 --> 00:26:52.560
book on Java threading.

00:26:52.560 --> 00:26:53.800
I don't remember the
title right off the bat.

00:26:53.800 --> 00:26:56.430
I think it's Excellent Java Apps or
something like this.

00:26:56.440 --> 00:26:58.520
In that book,
he actually had a section where

00:26:58.520 --> 00:27:01.420
he got a little more detailed
about what thread-safe means,

00:27:01.420 --> 00:27:06.020
and he took the definition
down for thread safety.

00:27:06.020 --> 00:27:10.090
So thread-safe, I've already mentioned.

00:27:14.210 --> 00:27:17.460
Okay, I'm going to use that one.

00:27:17.560 --> 00:27:20.790
So, immutable.

00:27:20.790 --> 00:27:20.790
So,

00:27:22.300 --> 00:27:25.400
Immutable data and immutable
is always thread safe.

00:27:25.690 --> 00:27:28.250
If you don't have to worry
about someone writing to it,

00:27:28.400 --> 00:27:31.900
you don't have to worry about
it changing unexpectedly.

00:27:32.070 --> 00:27:35.360
So things like maybe the processor type.

00:27:35.900 --> 00:27:41.430
Unless you've got some
hybrid mixed Intel PC board,

00:27:41.730 --> 00:27:42.250
OK.

00:27:42.740 --> 00:27:44.970
Chances are your processor's
not going to swap out from under

00:27:44.970 --> 00:27:46.040
you in the middle of your code.

00:27:46.080 --> 00:27:48.620
So if you've got a routine that
returns the processor type,

00:27:48.830 --> 00:27:51.290
chances are it's thread safe.

00:27:52.440 --> 00:27:56.490
: In mutable never requires
any type of synchronization.

00:27:56.490 --> 00:28:00.380
If it's not going to change,
you don't have to worry about

00:28:00.380 --> 00:28:02.440
somebody changing it on you.

00:28:02.470 --> 00:28:04.230
So no synchronization.

00:28:04.230 --> 00:28:05.600
So thread aware.

00:28:05.600 --> 00:28:09.440
This is code that can detect
when it's being executed in a

00:28:09.440 --> 00:28:12.170
thread and behave differently.

00:28:13.420 --> 00:28:14.580
I was trying to think
of an example of that,

00:28:14.580 --> 00:28:17.400
and I forgot my example
right off the top of my head.

00:28:17.540 --> 00:28:19.160
Cocoa.

00:28:19.160 --> 00:28:21.400
Cocoa runtime is a
perfectly good example.

00:28:21.660 --> 00:28:26.370
If your code never instantiates
an NSThread object,

00:28:26.370 --> 00:28:29.790
Cocoa is smart enough that it
doesn't instantiate all the locks

00:28:29.890 --> 00:28:33.320
and all the mechanisms that make
the Cocoa runtime thread safe.

00:28:33.400 --> 00:28:36.200
But the moment that you
instantiate an NSThread,

00:28:36.200 --> 00:28:40.070
it goes at the same time and goes, "Oh,
let's make sure that all these

00:28:40.070 --> 00:28:43.250
memory structures are protected."
It instantiates all these

00:28:43.250 --> 00:28:44.390
locks and does the right thing.

00:28:44.400 --> 00:28:46.290
So this is code that's thread aware.

00:28:46.290 --> 00:28:50.560
It behaves--it detects when
it's executing in a thread

00:28:51.040 --> 00:28:53.200
and does the right thing.

00:28:57.530 --> 00:29:01.450
So conditionally thread safe.

00:29:01.530 --> 00:29:04.510
So specific usages are
known not to be thread safe.

00:29:04.580 --> 00:29:09.420
If you've got an API that you've got
one parameter that if you specify it,

00:29:09.420 --> 00:29:13.240
if you pass null as that parameter,
then it picks up some system default.

00:29:13.240 --> 00:29:16.050
That system default
may not be thread safe.

00:29:16.050 --> 00:29:18.870
It could be stored in a
global that everybody's using.

00:29:18.870 --> 00:29:23.120
So maybe the condition on this
particular API would be is as long as

00:29:23.120 --> 00:29:27.380
you don't pass null for this parameter,
then it's not thread safe.

00:29:27.410 --> 00:29:29.220
And it's thread safe.

00:29:29.220 --> 00:29:32.240
So it's conditionally thread safe.

00:29:32.490 --> 00:29:36.630
A good example of this one also is

00:29:37.050 --> 00:29:39.800
Get Main Event Loop, the Carbon API.

00:29:39.850 --> 00:29:43.080
It's only thread safe if it's
executed once to completion

00:29:43.080 --> 00:29:45.490
before another thread calls it.

00:29:45.530 --> 00:29:49.490
And the reason why is it has
to instantiate the run loop.

00:29:49.560 --> 00:29:52.030
If you've never called run
application event loop,

00:29:52.030 --> 00:29:55.620
you don't have a run loop instantiated,
then the first time you call this,

00:29:55.620 --> 00:29:58.380
it instantiates the run
loop for the application.

00:29:58.380 --> 00:30:02.650
And instantiating the run
loop is not thread safe.

00:30:02.650 --> 00:30:06.980
So if you've called it one time to
completion from the main thread,

00:30:07.000 --> 00:30:10.000
it's instantiated the run loop,
then every time after that,

00:30:10.000 --> 00:30:12.960
it's going to return the run
loop that's already instantiated,

00:30:13.000 --> 00:30:14.000
and that's always thread safe.

00:30:14.000 --> 00:30:16.000
That's kind of like the immutable.

00:30:16.000 --> 00:30:17.960
Like the first time you run it,
it instantiates it,

00:30:18.010 --> 00:30:19.950
but now it's kind of an immutable object.

00:30:20.060 --> 00:30:21.970
It's not going to go away
until your application quits.

00:30:22.000 --> 00:30:28.000
So the second or sequential access is,
are threads safe?

00:30:29.420 --> 00:30:32.940
So thread compatible or thread friendly.

00:30:33.060 --> 00:30:38.060
These are routines that inherently
in and of themselves are thread safe.

00:30:38.190 --> 00:30:41.800
However, the data that you pass to
them may not be thread safe.

00:30:42.070 --> 00:30:46.420
But if you've properly protected,
if you can guarantee mutual access to

00:30:46.720 --> 00:30:52.210
the data that you're passing to this API,
then this API is thread safe.

00:30:53.260 --> 00:30:56.730
And then pretty much everything else,
which is just thread hostile.

00:30:56.830 --> 00:30:59.520
And if you look at all
these other ones up here,

00:30:59.550 --> 00:31:05.440
I find very few toolbox routines
that are thread hostile.

00:31:05.510 --> 00:31:09.170
They may not be marked as thread safe,
but they're typically

00:31:09.170 --> 00:31:12.250
one of these other ones,
either conditionally thread

00:31:12.670 --> 00:31:14.190
safe or thread compatible.

00:31:16.360 --> 00:31:20.780
So we've talked about APIs and functions,
routines that are thread safe.

00:31:20.780 --> 00:31:22.390
We'll talk about things
that are thread safe,

00:31:22.500 --> 00:31:23.720
data that's thread safe.

00:31:23.720 --> 00:31:25.300
We already talked about immutable.

00:31:25.300 --> 00:31:28.180
Immutable routines are
thread safe because the data

00:31:28.180 --> 00:31:29.760
they return are immutable.

00:31:29.760 --> 00:31:33.770
So immutable variables are,
by definition, thread safe.

00:31:33.770 --> 00:31:36.550
Any constants, et cetera,
are thread safe.

00:31:36.550 --> 00:31:40.100
And just one of the things to
help the compiler help you,

00:31:40.100 --> 00:31:43.300
if you've got read-only data,
declare a const.

00:31:43.300 --> 00:31:45.520
You can save yourself a lot of headaches.

00:31:46.300 --> 00:31:49.770
Because you can prevent the
compiler from generating code

00:31:49.770 --> 00:31:52.770
that would write on constant data.

00:31:53.750 --> 00:31:55.070
So non-shared data.

00:31:55.310 --> 00:31:57.600
And by non-shared data,
I don't mean just globals.

00:31:57.600 --> 00:31:59.220
What I'm talking about
is local variables,

00:31:59.220 --> 00:32:01.850
anything that you've
declared on the stack.

00:32:04.430 --> 00:32:06.540
method parameters,
anything that's passed to your

00:32:06.540 --> 00:32:09.580
routine that's passed in registers,
the contents of those

00:32:09.580 --> 00:32:11.120
registers are thread safe.

00:32:11.140 --> 00:32:13.950
Now, the gotcha here is you
might be past a pointer,

00:32:13.950 --> 00:32:15.800
and that pointer's thread safe.

00:32:15.840 --> 00:32:21.320
But the data that pointer points
to is not necessarily thread safe.

00:32:21.460 --> 00:32:26.220
So we're talking about strictly
the parameters that are passed,

00:32:26.220 --> 00:32:29.610
not necessarily the data that
those parameters point to.

00:32:29.710 --> 00:32:31.740
method parameters,
anything that's passed to your

00:32:31.740 --> 00:32:34.780
routine that's passed in registers,
the contents of those

00:32:34.780 --> 00:32:35.100
registers are thread safe.

00:32:35.360 --> 00:32:37.770
Any locally allocated
memory is thread safe.

00:32:37.840 --> 00:32:42.070
You call malloc,
or any one of the variants of malloc,

00:32:42.120 --> 00:32:43.810
that memory is thread safe.

00:32:43.900 --> 00:32:46.720
And you can pass it to any
routine that's thread safe,

00:32:46.730 --> 00:32:50.950
or any routine that's thread friendly
that can take thread safe data,

00:32:50.950 --> 00:32:53.410
and you're still thread safe.

00:32:57.280 --> 00:33:01.380
So thread safe code may call
other thread safe functions.

00:33:01.510 --> 00:33:02.440
Kind of makes sense.

00:33:02.500 --> 00:33:04.190
If you call something
that's not thread safe,

00:33:04.360 --> 00:33:06.730
then you're not thread safe.

00:33:07.680 --> 00:33:11.840
So thread safe code may call
other thread safe functions.

00:33:11.840 --> 00:33:12.840
Kind of makes sense.

00:33:12.840 --> 00:33:14.720
If you call something
that's not thread safe,

00:33:14.720 --> 00:33:17.130
then you're not thread safe.

00:33:23.860 --> 00:33:28.040
So thread safe code may call
other thread safe functions.

00:33:28.040 --> 00:33:29.040
Kind of makes sense.

00:33:29.040 --> 00:33:30.920
If you call something
that's not thread safe,

00:33:30.920 --> 00:33:33.340
then you're not thread safe.

00:33:42.280 --> 00:33:46.440
So thread safe code may call
other thread safe functions.

00:33:46.440 --> 00:33:47.440
Kind of makes sense.

00:33:47.440 --> 00:33:49.320
If you call something
that's not thread safe,

00:33:49.320 --> 00:33:51.740
then you're not thread safe.

00:34:12.200 --> 00:34:16.000
If it's thread safe,
then you have to kind

00:34:16.060 --> 00:34:18.300
of assume that it isn't,
and you have to do things

00:34:18.300 --> 00:34:20.010
to make sure that it is.

00:34:20.020 --> 00:34:22.750
And the most common solution is locks.

00:34:22.810 --> 00:34:25.890
It's kind of falling out of favor
because of all of the negative aspects

00:34:25.890 --> 00:34:31.020
of deadlocking and race conditions,
et cetera, that the locks cause.

00:34:31.020 --> 00:34:32.820
They're hard to administrate, et cetera.

00:34:32.820 --> 00:34:36.190
There are some new
methodologies coming around.

00:34:36.260 --> 00:34:40.310
I wish I had time to include them,
where we talk about where

00:34:40.580 --> 00:34:42.180
different methods are used.

00:34:42.180 --> 00:34:46.390
other than locks that don't have
some of the problems that locks do.

00:34:47.530 --> 00:34:51.500
So,
may simultaneously access distinct data.

00:34:51.500 --> 00:34:54.000
Now, this is kind of like
record locking in a file.

00:34:54.000 --> 00:34:58.540
If I've got a huge block of memory,
I can have one thread

00:34:58.540 --> 00:35:01.020
working on this part,
and one thread working on this part,

00:35:01.020 --> 00:35:03.060
and one thread working
on this part over here.

00:35:03.060 --> 00:35:05.020
This is distinct pieces of data.

00:35:05.020 --> 00:35:08.390
All it requires is I do some
kind of housekeeping to make sure

00:35:08.390 --> 00:35:12.180
that these threads don't step on
each other when they're accessing

00:35:12.180 --> 00:35:14.220
this and modifying the data.

00:35:15.130 --> 00:35:18.600
So that's what I mean by distinct data.

00:35:22.080 --> 00:35:25.830
So thread safe code
has no race conditions.

00:35:25.880 --> 00:35:28.890
A race condition is something
like a clock where you've got

00:35:29.020 --> 00:35:32.520
your thread waits for a second,
increments seconds.

00:35:32.580 --> 00:35:35.990
If the seconds hit 60,
then it resets seconds back to zero,

00:35:35.990 --> 00:35:37.260
then increments minutes.

00:35:37.400 --> 00:35:41.900
If after incrementing the minutes,
the minutes hit 60,

00:35:41.970 --> 00:35:46.020
it resets minutes back to zero,
and then increments the hours.

00:35:46.110 --> 00:35:46.700
That's OK.

00:35:46.700 --> 00:35:47.860
That's all well and good.

00:35:47.950 --> 00:35:51.040
But let's say you've got
another routine that's supposed

00:35:51.040 --> 00:35:52.990
to be able to read the time.

00:35:53.220 --> 00:35:57.450
What happens if my one thread
that's keeping my clock running,

00:35:57.710 --> 00:36:00.960
he goes to increment the seconds,
the seconds hit 60,

00:36:01.040 --> 00:36:05.020
but before he can reset
the seconds back to zero,

00:36:05.110 --> 00:36:06.010
he gets interrupted.

00:36:06.210 --> 00:36:08.840
He gets preempted by another thread.

00:36:08.900 --> 00:36:11.220
This other thread says,
I want to know what time it is.

00:36:11.290 --> 00:36:12.600
And he goes out and looks at it.

00:36:12.600 --> 00:36:15.690
Well, it's 1, 30, and 60 seconds.

00:36:15.830 --> 00:36:17.280
That's not a valid time.

00:36:17.630 --> 00:36:20.340
So that's called a race condition,
where you've got multiple threads.

00:36:20.340 --> 00:36:22.000
You've got multiple
threads accessing the data.

00:36:22.000 --> 00:36:25.070
And at any point in time,
the data can be in an invalid

00:36:25.070 --> 00:36:30.070
state because one thread hasn't
completed manipulating the

00:36:30.070 --> 00:36:32.220
data back into a valid state.

00:36:32.220 --> 00:36:34.250
You have a race condition.

00:36:34.720 --> 00:36:36.440
So, does not deadlock.

00:36:36.490 --> 00:36:39.270
Now this is a case where you've got,
let's say we've got two threads.

00:36:39.360 --> 00:36:42.160
The first thread locks on one variable,
the second thread locks

00:36:42.160 --> 00:36:44.300
on a second variable,
and now they both need the

00:36:44.380 --> 00:36:45.640
other thread's variable.

00:36:45.740 --> 00:36:48.600
So you've got two threads
locked on each other,

00:36:48.600 --> 00:36:50.810
holding the variable
that the other one needs,

00:36:50.810 --> 00:36:53.560
and waiting on the variable
that the other one has locked.

00:36:53.670 --> 00:36:56.670
That's a deadlock condition.

00:36:57.370 --> 00:36:58.740
So it has no priority failures.

00:36:58.740 --> 00:37:02.330
So let's say you've got a thread that
runs that constantly updates the screen

00:37:02.330 --> 00:37:06.890
showing how much percent done a task is.

00:37:07.310 --> 00:37:09.160
Unfortunately,
the task that it's measuring

00:37:09.160 --> 00:37:11.750
is a low priority task,
and the fact that we're updating

00:37:11.750 --> 00:37:15.830
the screen all the time to show
what percent done is affecting

00:37:16.230 --> 00:37:18.260
how much time the task gets.

00:37:18.380 --> 00:37:21.300
And so it's the Heisenberg
uncertainty principle.

00:37:21.300 --> 00:37:23.090
Observing it slows it down.

00:37:23.210 --> 00:37:27.100
So that's called a priority failure.

00:37:27.230 --> 00:37:30.330
Now the extreme case of this
is where the secondary thread

00:37:30.410 --> 00:37:34.130
can't get any time at all,
and that's called a full starvation.

00:37:34.230 --> 00:37:36.630
So things to watch out for.

00:37:39.000 --> 00:37:42.200
Static or global read/write data,
what can you do about it?

00:37:42.200 --> 00:37:44.110
Well, you can eliminate it.

00:37:44.690 --> 00:37:50.770
: You can convert it to
thread-specific data.

00:37:50.770 --> 00:37:56.470
All of the different
APIs that I mentioned,

00:37:56.830 --> 00:38:01.110
all the implementations,
have thread-specific variables,

00:38:01.110 --> 00:38:01.110
where you can have a
global variable that says,

00:38:01.110 --> 00:38:01.110
"This is the variable ID for

00:38:01.270 --> 00:38:05.260
: I'm going to show you some piece
of data that all of my threads need.

00:38:05.260 --> 00:38:08.470
And each one of those threads
will use that variable ID to

00:38:08.550 --> 00:38:10.900
access a thread specific variable.

00:38:13.760 --> 00:38:16.130
So you can protect it via
synchronization methods.

00:38:16.150 --> 00:38:19.460
Use an entry and exit kind of thing.

00:38:19.460 --> 00:38:21.400
You can use critical sections, et cetera.

00:38:21.430 --> 00:38:23.910
They basically said anybody
that wants to access this is

00:38:23.910 --> 00:38:26.060
going to have to take a lock.

00:38:26.060 --> 00:38:29.320
And if someone has the lock when
someone else want to access it,

00:38:29.320 --> 00:38:31.500
they're going to have
to wait for the lock.

00:38:31.510 --> 00:38:34.540
And then when you're done accessing it,
modifying it, whatever,

00:38:34.540 --> 00:38:38.070
you release the lock so that
anyone that's waiting for it can

00:38:38.070 --> 00:38:40.450
get the lock and then access it.

00:38:46.070 --> 00:38:48.400
I think I'm on the last battery here.

00:38:48.410 --> 00:38:50.380
Oh, here we go.

00:38:50.440 --> 00:38:52.540
So writing thread-safe code.

00:38:52.570 --> 00:38:56.490
Here's a little example I threw
together just to give you a taste.

00:38:56.830 --> 00:38:58.890
It's non-reentrant.

00:38:59.190 --> 00:39:01.860
That's because we have
a static local there.

00:39:01.920 --> 00:39:06.160
And the problem with this code-- all
this code does is we pass it a string.

00:39:06.410 --> 00:39:09.010
It's got a local buffer that
it copies the string to while

00:39:09.120 --> 00:39:10.600
it's converting it to uppercase.

00:39:10.650 --> 00:39:13.590
Null terminates it and returns
the address of the buffer.

00:39:13.790 --> 00:39:15.870
That's all well and good.

00:39:17.100 --> 00:39:19.490
unless you do this.

00:39:19.590 --> 00:39:21.760
So, let's say you're doing
a password compare.

00:39:21.840 --> 00:39:25.740
You pass it in the user-- the
password that was typed in,

00:39:25.810 --> 00:39:29.100
and the second parameter is the
password that was in a password file,

00:39:29.100 --> 00:39:29.910
let's say.

00:39:30.090 --> 00:39:34.090
What happens is the order of
those two calls inside the string

00:39:34.090 --> 00:39:35.940
compare are non-determinant in C.

00:39:35.940 --> 00:39:37.500
They could happen in either order.

00:39:37.580 --> 00:39:41.160
But let's say that the first one,
the leftmost one happens first,

00:39:41.250 --> 00:39:45.200
and he stores the value
passed in into the buffer.

00:39:45.310 --> 00:39:47.200
And then when he finishes,
the second one executes,

00:39:47.200 --> 00:39:51.250
and he stores uppercase,
the value that was in the password file,

00:39:51.480 --> 00:39:52.800
into the same buffer.

00:39:52.800 --> 00:39:57.040
So when string compare gets called,
what's being returned is the address

00:39:57.040 --> 00:39:59.850
of that static character buffer,
and it's the same address.

00:40:00.040 --> 00:40:03.190
So it's always gonna match,
and I don't think you'll

00:40:03.200 --> 00:40:05.250
have a very secure system.

00:40:06.630 --> 00:40:07.700
So what can you do?

00:40:07.700 --> 00:40:09.720
So one thing you can do
is you can malloc memory,

00:40:09.780 --> 00:40:12.480
because malloc memory
is always thread safe.

00:40:12.530 --> 00:40:13.690
And use that instead.

00:40:13.890 --> 00:40:17.690
You copy-- you do the same kind
of string copy into the malloc.

00:40:17.790 --> 00:40:19.350
That's okay.

00:40:19.350 --> 00:40:21.320
It's a better function
because it is reentrant,

00:40:21.320 --> 00:40:24.100
but it's got a problem.

00:40:24.100 --> 00:40:26.000
It requires error checking
and external free.

00:40:26.000 --> 00:40:30.040
The person that calls this routine is
going to have to know that it may fail.

00:40:30.040 --> 00:40:31.980
I may not be able to
allocate that memory.

00:40:31.980 --> 00:40:34.970
I'm going to have to check that pointer
that's returned and see if it's null.

00:40:34.970 --> 00:40:37.580
And it's also going to have
to know that if it isn't null,

00:40:37.660 --> 00:40:39.110
I'm going to have to free it.

00:40:39.110 --> 00:40:41.860
And that's kind of a bad
programming paradigm.

00:40:41.860 --> 00:40:45.560
A better solution is to let the
external guy malloc his memory

00:40:45.560 --> 00:40:47.680
and worry about freeing it.

00:40:48.790 --> 00:40:53.860
And in this case,
we're just going to do -- the first

00:40:53.860 --> 00:40:57.300
parameter would be the string in.

00:40:57.300 --> 00:40:59.700
The second parameter
would be the string out.

00:40:59.700 --> 00:41:01.700
And we just copy the input to the output.

00:41:01.700 --> 00:41:04.760
And it's really up to the guy
that calls this routine to

00:41:04.760 --> 00:41:08.190
make sure that the buffers are
allocated and freed and et cetera.

00:41:09.200 --> 00:41:13.360
So if -- that also means that
he's got to be responsible for

00:41:13.360 --> 00:41:17.200
making sure that what's being
passed to this routine is mutable.

00:41:17.700 --> 00:41:18.700
And then the third
parameter is the string in.

00:41:18.700 --> 00:41:20.960
And this is the string that's going
to be used to make sure that the

00:41:20.960 --> 00:41:22.700
data is being used in the right way.

00:41:23.700 --> 00:41:26.670
And then the third parameter is
the string that's going to be

00:41:26.750 --> 00:41:30.010
used to make sure that the data
is being used in the right way.

00:41:31.470 --> 00:41:35.900
So at the last minute,
I had one of the QuickTime engineers

00:41:35.900 --> 00:41:39.740
that knew I was doing my threading
session drop me two QuickTime slides.

00:41:39.770 --> 00:41:42.860
And so these don't quite fit in
with the rest of the presentation,

00:41:42.860 --> 00:41:46.060
but I promised him that
I would plug it anyway.

00:41:46.060 --> 00:41:48.820
He did a lot of work on
QuickTime to make it thread safe,

00:41:48.900 --> 00:41:50.280
as thread safe as possible.

00:41:50.510 --> 00:41:56.550
And at the very least,
I can share that with you.

00:41:58.120 --> 00:42:03.040
So anybody who uses QuickTime knows that
it basically uses plug-in components.

00:42:03.080 --> 00:42:05.000
Some of those components
are provided by Apple,

00:42:05.000 --> 00:42:06.170
some of them are not.

00:42:06.200 --> 00:42:08.830
Most of them provided by
Apple are thread-safe,

00:42:08.890 --> 00:42:12.730
and if they're not, they will be soon,
because this particular engineer is very

00:42:12.730 --> 00:42:14.510
religious about making them thread-safe.

00:42:16.580 --> 00:42:18.900
But you can't always predict which
components you'll need at runtime.

00:42:19.110 --> 00:42:21.170
Some of these components may
be third-party components

00:42:21.170 --> 00:42:22.360
that may not be thread-safe.

00:42:22.470 --> 00:42:23.980
And so how do you tell?

00:42:24.100 --> 00:42:28.000
Well, fortunately for you,
we let QuickTime do the job for you.

00:42:28.060 --> 00:42:30.340
If you've written a
thread-safe component,

00:42:30.340 --> 00:42:33.580
you've done all the right work,
there's actually an attribute of

00:42:33.670 --> 00:42:36.880
the component that you set that
basically tells the QuickTime,

00:42:36.960 --> 00:42:40.440
"I am a thread-safe component." Now,
you could always lie,

00:42:40.600 --> 00:42:44.710
but you'll probably find out
pretty quick if you were wrong.

00:42:45.020 --> 00:42:48.680
So I already mentioned opening
a movie may invoke many

00:42:48.950 --> 00:42:51.290
components under the hood.

00:42:52.350 --> 00:42:54.480
Got a little ahead of
my bullet points here.

00:42:54.530 --> 00:42:56.280
And some components
written by third parties,

00:42:56.280 --> 00:42:57.490
some legacy.

00:42:57.660 --> 00:43:00.680
How do you know whether the
components are thread safe?

00:43:02.460 --> 00:43:08.540
Non-thread-safe components
cause hard-to-reproduce crashes.

00:43:14.900 --> 00:43:20.300
So how do you use QuickTime from
other than the main thread?

00:43:20.300 --> 00:43:22.890
Well, this API, innerMovieOnThread.

00:43:22.960 --> 00:43:25.560
If you do this,
you're telling QuickTime that

00:43:25.560 --> 00:43:31.760
you're not on the main thread,
and you want to do threaded

00:43:32.330 --> 00:43:34.700
component movie stuff.

00:43:36.180 --> 00:43:39.380
It protects you from opening
non-thread-safe components.

00:43:39.480 --> 00:43:41.500
So you might just open a movie.

00:43:41.570 --> 00:43:44.290
But then underneath,
as QuickTime goes out and starts

00:43:44.350 --> 00:43:47.100
loading other components and et cetera,
it's going to check each and every one

00:43:47.100 --> 00:43:49.610
of those to make sure it's thread-safe,
make sure it's got the

00:43:49.690 --> 00:43:50.980
thread-safe attribute.

00:43:51.090 --> 00:43:55.710
And if any of the components that that
movie requires aren't thread-safe,

00:43:55.730 --> 00:43:58.650
he's going to return back to
your thread from new movie or

00:43:58.650 --> 00:44:04.190
whatever you called this error,
component non-thread-safe error.

00:44:05.070 --> 00:44:08.340
So, and this is an indication to you,
well for this particular movie,

00:44:08.340 --> 00:44:11.270
you're going to have to
migrate it to the main thread.

00:44:11.560 --> 00:44:13.560
Well,
instead of having to throw away all the

00:44:13.560 --> 00:44:16.810
hard work you've done and start over,
we actually have some

00:44:16.830 --> 00:44:18.980
APIs that make this easier.

00:44:20.400 --> 00:44:25.700
"Detach movie from current thread and
attach movie to current thread." So from

00:44:25.700 --> 00:44:28.960
the thread where you got the error,
you can detach it.

00:44:29.030 --> 00:44:31.940
You can send a message to
the main event queue saying,

00:44:31.960 --> 00:44:34.220
look, you're going to have to play
this movie because I can't.

00:44:34.350 --> 00:44:38.660
The main event,
the main thread calls attach

00:44:38.660 --> 00:44:41.370
movie to current thread,
and it takes that movie and it's in

00:44:41.370 --> 00:44:45.490
whatever state the thread left it in,
and is able to play it on and

00:44:45.490 --> 00:44:47.490
use it on the main thread.

00:44:47.990 --> 00:44:51.100
So it's important to check for
errors returned by these APIs.

00:44:51.140 --> 00:44:54.510
They may fail if the movie uses
non-thread-safe components.

00:44:54.630 --> 00:44:59.770
So we do have a new tech note out, 2125,
that goes into the details of

00:44:59.770 --> 00:45:03.380
the QuickTime threading stuff
for anyone that's interested.

00:45:11.100 --> 00:45:21.800
[Transcript missing]

00:45:34.850 --> 00:45:37.640
is a-- currently it's not threaded.

00:45:37.700 --> 00:45:41.090
By clicking the window,
they spend for about 10 seconds, well,

00:45:41.090 --> 00:45:42.860
maybe about five seconds.

00:45:42.890 --> 00:45:44.140
I can't drag the window.

00:45:44.140 --> 00:45:45.920
I can't open any menus.

00:45:45.980 --> 00:45:47.840
I got it post-haste here.

00:45:47.870 --> 00:45:51.370
My thread-- my application
is basically bricked.

00:45:51.710 --> 00:45:53.260
Eventually,
if it was timed out long enough,

00:45:53.260 --> 00:45:56.170
I'd get to spinning beach ball,
but we're not getting it here.

00:45:56.200 --> 00:45:59.740
If you look down here, I'm running it in
thread viewer down here,

00:45:59.740 --> 00:46:03.620
you can see I've got one thread,
and when it's running,

00:46:03.620 --> 00:46:08.540
the green down here is
basically that thread,

00:46:08.540 --> 00:46:12.860
or the main thread,
drawing this quartz code and

00:46:12.860 --> 00:46:15.800
taking up that one thread.

00:46:16.900 --> 00:46:23.360
If I switch it into threaded mode,
now I can drag the window around,

00:46:24.180 --> 00:46:26.220
slows down a little bit,

00:46:26.250 --> 00:46:29.870
I can look at my windows, my menus,
et cetera.

00:46:30.180 --> 00:46:32.580
And you notice down
in thread viewer here,

00:46:32.630 --> 00:46:35.790
I can actually see work being
done on more than one thread.

00:46:35.940 --> 00:46:40.840
So this is an example of the difference
in user expectations as far as

00:46:40.840 --> 00:46:49.440
interactivity and why you should use
threaded code to free up your event loop,

00:46:49.470 --> 00:46:51.090
et cetera.

00:46:56.260 --> 00:46:59.600
Back to the slides.

00:46:59.690 --> 00:47:02.380
There we go.

00:47:02.670 --> 00:47:06.140
Okay, this is pretty much the canned
URL for all of our sample

00:47:06.140 --> 00:47:07.510
codes and everything else.

00:47:07.710 --> 00:47:11.940
I can provide the source to
this one if you want to see it.

00:47:11.940 --> 00:47:14.870
It's basically, it's Ovaltine,
and I just added like six lines of

00:47:14.960 --> 00:47:17.640
code to thread the spinning part.

00:47:17.680 --> 00:47:18.840
But it's pretty simple.

00:47:18.840 --> 00:47:22.200
If anyone wants it, you can either email
me at my email address,

00:47:22.330 --> 00:47:23.340
or I can post it.

00:47:23.480 --> 00:47:28.900
So, and there's my email address.

00:47:29.440 --> 00:47:32.100
I did want to point out the last one,
and it's really hard to read there.

00:47:32.100 --> 00:47:34.900
We do have an MP, SMP mailing list.

00:47:34.900 --> 00:47:36.630
It doesn't get a whole lot of traffic.

00:47:36.630 --> 00:47:39.760
We have both,
I know three or four of the internal

00:47:39.760 --> 00:47:43.050
engineers that lurk on that list,
and we have a pretty good community

00:47:43.050 --> 00:47:46.070
of developers that do MP programming
that lurk on that list also.

00:47:46.070 --> 00:47:48.420
When someone typically
posts to that list,

00:47:48.420 --> 00:47:51.770
sometimes I'll see five responses
to the list even before I even

00:47:51.770 --> 00:47:53.500
see the original question.

00:47:53.500 --> 00:47:57.870
So it is actively being monitored
regardless of the low traffic.

00:48:01.330 --> 00:48:06.720
My email address is
g-e-o-w-a-r at apple.com.