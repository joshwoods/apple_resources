WEBVTT

00:00:05.900 --> 00:00:14.200
[Transcript missing]

00:00:15.450 --> 00:00:19.640
In this session we're going to talk about
supporting the latest and greatest video

00:00:19.640 --> 00:00:22.180
compression formats in your application.

00:00:23.510 --> 00:00:27.330
We're going to start with a brief
bit of technology background,

00:00:27.370 --> 00:00:30.250
and then we're going to talk about
how to access this stuff in your

00:00:30.320 --> 00:00:33.390
applications using our most modern APIs.

00:00:33.480 --> 00:00:35.320
We're going to support this
with a lot of sample code,

00:00:35.660 --> 00:00:38.400
which is already available
for you to download.

00:00:38.410 --> 00:00:42.330
If you go to the page for this session,
you can log in and download all

00:00:42.330 --> 00:00:45.730
that stuff right now and look at
it while we're talking about it.

00:00:50.200 --> 00:00:53.940
We have a great new codec
in QuickTime 7 and Tiger.

00:00:54.010 --> 00:00:56.300
It's called H.264.

00:00:56.470 --> 00:01:01.780
It's a standards-based codec that's the
result of a joint effort by two major

00:01:02.070 --> 00:01:06.400
standards bodies with a bunch of experts,
international collaboration.

00:01:06.680 --> 00:01:10.400
Collaboration in particular
between the experts from ISO,

00:01:10.400 --> 00:01:13.530
the International Organization for
Standardization,

00:01:13.530 --> 00:01:16.010
which brought us the MPEG codecs,
and the ITU,

00:01:16.010 --> 00:01:18.400
the International Telecommunications
Union,

00:01:18.410 --> 00:01:23.400
who developed the H.261 and
H.263 video conferencing codecs.

00:01:23.400 --> 00:01:28.590
It's been chosen for
a number of standards.

00:01:28.590 --> 00:01:28.590
It's been supported by a number of

00:01:28.600 --> 00:01:45.600
[Transcript missing]

00:01:46.270 --> 00:01:47.640
It's got a lot of names.

00:01:47.850 --> 00:01:52.720
You'll see people calling it AVC,
which stands for Advanced Video Coding.

00:01:52.950 --> 00:01:56.760
You'll see people calling it JVT,
which stands for Joint Video Team,

00:01:56.860 --> 00:01:59.950
which refers to its joint heritage.

00:02:00.430 --> 00:02:04.080
You'll see it called MPEG-4 Part 10
because that's the name of the standard.

00:02:04.120 --> 00:02:07.560
Apple uses the name H.264 which
is another of the choices.

00:02:07.720 --> 00:02:10.180
All of these are referring
to the same thing.

00:02:10.290 --> 00:02:12.500
So this is our new video
codec in QuickTime 7.

00:02:12.500 --> 00:02:14.990
We put a lot of energy
into making it work.

00:02:15.000 --> 00:02:20.500
I'd like to give you a brief demo of it
over on this wonderful demo machine here.

00:02:32.000 --> 00:02:37.240
In 20 days and 20 nights,
the Emperor Penguin will march to a place

00:02:37.240 --> 00:02:41.910
so extreme it supports no other life.

00:02:57.110 --> 00:03:01.100
Excitement, adventure,
and really wild things.

00:03:01.100 --> 00:03:03.880
So this is one of the
videos from our site.

00:03:04.870 --> 00:03:11.680
If you have QuickTime Player Pro you
can export to H.264 yourself.

00:03:12.850 --> 00:03:16.730
It's under Movie to QuickTime Movie and
if you click Options there's another

00:03:16.750 --> 00:03:18.180
dialog that has a list of settings.

00:03:18.180 --> 00:03:21.550
The video settings,
H.264 is one of the choices

00:03:21.600 --> 00:03:23.180
in the compressor list.

00:03:23.180 --> 00:03:27.200
And down here you'll see that we've,
if you've seen this dialog before,

00:03:27.200 --> 00:03:30.260
you've seen that we've rearranged it a
bit in order to support our new features.

00:03:30.280 --> 00:03:36.420
A major new feature for H.264 encoding
is a feature called multi-pass encoding,

00:03:36.520 --> 00:03:38.990
intelligent multi-pass encoding.

00:03:39.700 --> 00:03:45.290
This is what we use to get the
best quality in our encodes.

00:03:50.690 --> 00:03:57.160
Let's come back to slides.

00:03:57.280 --> 00:03:58.900
Since this is the year
of high definition,

00:03:58.910 --> 00:04:01.900
I wanted to point out that Apple actually
has been delivering quite a number

00:04:01.900 --> 00:04:03.860
of codecs which are high def ready.

00:04:03.960 --> 00:04:07.840
As well as H.264,
which is our premier delivery codec.

00:04:08.120 --> 00:04:12.700
This year in Final Cut Pro 5,
Final Cut Pro Studio,

00:04:12.700 --> 00:04:16.750
we delivered support for,
native support for HDV.

00:04:16.860 --> 00:04:23.460
HDV is an MPEG-2 19 megabit
IPB based code format.

00:04:23.510 --> 00:04:25.620
It's constant bit rate.

00:04:25.680 --> 00:04:27.880
It's supported by a new
breed of FireWire cameras,

00:04:28.080 --> 00:04:31.190
of HDV cameras from some manufacturers.

00:04:31.290 --> 00:04:34.880
And we have native support
for this in Final Cut Pro 5.

00:04:35.300 --> 00:04:41.420
iMovie HD and
Final Cut Express HD support HDV by

00:04:41.420 --> 00:04:46.840
converting it to an intermediate format
called the Apple Intermediate Codec.

00:04:47.240 --> 00:04:49.510
But these aren't the only ones.

00:04:49.690 --> 00:04:54.870
We've also supported a
format called DVC Pro HD,

00:04:54.870 --> 00:05:00.410
which is a 100 megabit DV-style,
DCT-based high-def format since

00:05:00.410 --> 00:05:03.630
Final Cut Pro 4.5 last year.

00:05:03.640 --> 00:05:10.150
In Panther we introduced Pixlet,
and for some time we've supported

00:05:10.440 --> 00:05:14.580
uncompressed HD playback if you have
a machine that has sufficient I.O.

00:05:14.680 --> 00:05:15.280
bandwidth.

00:05:15.940 --> 00:05:18.550
These movies have
exceptionally high data rates,

00:05:18.600 --> 00:05:20.880
up in the hundreds of
megabytes per second.

00:05:20.940 --> 00:05:25.370
Now, the last four of these
codecs are iframe only.

00:05:25.430 --> 00:05:30.850
H.264 and HDV use B-frames,
and some of you may be saying,

00:05:30.930 --> 00:05:34.580
"What's that again?" So,
let's have a little bit of technology

00:05:34.630 --> 00:05:38.600
background on what B-frames are
and why you might need to worry

00:05:38.670 --> 00:05:41.440
about them and why they're important.

00:05:42.510 --> 00:05:48.890
Video codecs present you with a
trade-off between bit rate and quality.

00:05:48.990 --> 00:05:51.400
If you want a better quality,
you generally have to use more bits.

00:05:51.410 --> 00:05:53.580
If you want to reduce the bit rate,
you have to sacrifice the

00:05:53.580 --> 00:05:54.900
quality to some extent.

00:05:55.010 --> 00:05:57.190
So there's this curve,
and we're constantly trying to move

00:05:57.240 --> 00:06:01.210
that curve better and better towards
better quality at lower bit rates.

00:06:01.600 --> 00:06:04.400
We do this by adding more tricks.

00:06:04.690 --> 00:06:07.400
Smart people who write codecs
come up with more tricks.

00:06:07.410 --> 00:06:10.340
Some of those tricks are
entirely inside the codec,

00:06:10.510 --> 00:06:12.920
but some of them involve...

00:06:13.920 --> 00:06:19.220
aspects that need to be taken care of
by parts that are outside of the codec,

00:06:19.360 --> 00:06:23.710
by other things,
other parts of the system that

00:06:23.710 --> 00:06:23.710
deal with compressed frames.

00:06:24.680 --> 00:06:26.650
Suppose you want to compress some video.

00:06:26.650 --> 00:06:29.500
Here's a clip of me parking my car.

00:06:29.570 --> 00:06:31.080
Prosaic.

00:06:31.320 --> 00:06:35.200
Well, you can compress each
frame independently.

00:06:35.370 --> 00:06:40.790
This is called iframes or
keyframes or sync samples.

00:06:40.890 --> 00:06:43.140
In this case,
every frame is self-contained,

00:06:43.360 --> 00:06:47.390
so random access is fast,
but the bitrate isn't too great.

00:06:48.100 --> 00:07:01.900
[Transcript missing]

00:07:06.900 --> 00:07:11.560
So you can improve compression
performance substantially by using one

00:07:11.560 --> 00:07:14.240
frame as a base for encoding another.

00:07:14.260 --> 00:07:18.210
In the example I'm illustrating here,
we're describing frame 5 in

00:07:18.320 --> 00:07:20.760
terms of frame 4 as a base.

00:07:20.920 --> 00:07:24.540
First, we describe the areas that
are similar to frame 4.

00:07:24.560 --> 00:07:29.760
Now the orange area in this diagram
is the piece that's the same pixels

00:07:29.800 --> 00:07:31.860
from the same place in frame 4.

00:07:31.880 --> 00:07:35.370
The green bit in the middle is
where we have the same pixels,

00:07:35.460 --> 00:07:39.300
but they kind of moved over
from where they were in frame 4.

00:07:39.390 --> 00:07:42.630
This is called motion compensation.

00:07:42.860 --> 00:07:45.830
Now, that doesn't completely
describe the scene.

00:07:45.830 --> 00:07:49.400
There's still a fix-up that has
to be made because not everything

00:07:49.470 --> 00:07:51.310
is completely stationary.

00:07:51.310 --> 00:07:55.250
The reflection on the car
doesn't move with the car.

00:07:55.250 --> 00:07:57.320
It's got to be adjusted.

00:07:57.320 --> 00:07:59.800
The tire is moving,
so you have to adjust the

00:07:59.800 --> 00:08:01.290
image for that as well.

00:08:01.300 --> 00:08:04.200
This is called the residue,
this fix-up image.

00:08:04.240 --> 00:08:07.800
Now there's a whole strip of
the image that wasn't in frame 4

00:08:07.800 --> 00:08:09.910
because it was out of the picture.

00:08:09.910 --> 00:08:11.230
It's just moved in.

00:08:11.300 --> 00:08:16.460
This strip has to be coded
from scratch by the residue.

00:08:17.310 --> 00:08:21.570
So this is what it looks like if
we encode the last five frames of

00:08:21.730 --> 00:08:27.500
those six using difference frames,
or that is using motion compensation.

00:08:27.650 --> 00:08:32.590
Difference frames use information
from a previous frame.

00:08:33.520 --> 00:08:35.460
They're also known as
P-Frames for predicted.

00:08:35.780 --> 00:08:38.510
This gives you better compression because

00:08:39.200 --> 00:08:43.360
Motion compensation is extremely
compact relative to describing

00:08:43.430 --> 00:08:45.110
something from scratch.

00:08:45.730 --> 00:08:49.140
Another way of looking at it is
to say that each of these frames

00:08:49.140 --> 00:08:53.330
depends on the previous one because
each frame is described in terms

00:08:53.470 --> 00:08:55.700
of the previous one as a base.

00:08:55.710 --> 00:08:58.660
That means that if you
want to display frame 6,

00:08:58.660 --> 00:09:02.970
you have to start with frame 1 and
then decode all of the ones in between.

00:09:05.140 --> 00:09:08.980
So we call this IP when you
have iframes and pframes.

00:09:09.080 --> 00:09:11.710
It gives you better
compression than iframes only,

00:09:11.710 --> 00:09:13.100
but random access can be slow.

00:09:13.100 --> 00:09:16.100
For example, to get to that last frame
you had to decode all six.

00:09:16.100 --> 00:09:18.100
It could take some time.

00:09:18.100 --> 00:09:22.220
Also, another thing to note is that
images that appear gradually have

00:09:22.220 --> 00:09:23.100
to be constructed incrementally.

00:09:23.100 --> 00:09:27.100
Like the last image in that sequence,
when you see it,

00:09:27.100 --> 00:09:30.960
it's been constructed from
strips in five different frames.

00:09:31.350 --> 00:09:35.100
That might not always be the
most efficient way to do things.

00:09:35.210 --> 00:09:38.220
So suppose for a moment that
we encoded the first frame in

00:09:38.220 --> 00:09:41.100
that sequence as an iframe,
self-contained,

00:09:41.360 --> 00:09:45.370
and then went and encoded the
last frame in the sequence as a

00:09:45.370 --> 00:09:48.100
pframe directly based on frame one.

00:09:48.330 --> 00:09:51.460
Then we could pick a frame in the middle
and we could encode it using motion

00:09:51.460 --> 00:09:57.380
compensation partly from the frame before
it and partly from the frame after it.

00:09:58.500 --> 00:09:59.820
Well,
then you can see we can encode it almost

00:09:59.940 --> 00:10:02.540
completely using motion compensation.

00:10:02.540 --> 00:10:04.630
And in fact,
we can do that for a whole bunch

00:10:04.680 --> 00:10:08.140
of frames in between those,
and then mostly encode it using motion

00:10:08.180 --> 00:10:10.810
compensation so the bit rate is improved.

00:10:11.640 --> 00:10:13.200
These are called B-frames.

00:10:13.280 --> 00:10:16.080
B stands for bidirectional prediction.

00:10:16.080 --> 00:10:21.600
B-frames refer to information from frames
that will be displayed in the future.

00:10:22.830 --> 00:10:27.300
They may also use information
from a previous ILP frame.

00:10:27.370 --> 00:10:30.370
As you can see, the bitrate has improved.

00:10:32.540 --> 00:10:37.200
Using B-frames as well as IMP frames
gives you better compression

00:10:37.200 --> 00:10:38.240
when objects appear gradually.

00:10:38.240 --> 00:10:41.240
It also gives you somewhat
faster random access.

00:10:41.240 --> 00:10:46.240
To decode and display any of the frames
in the middle from a starting standpoint,

00:10:46.240 --> 00:10:48.240
you only have to decode
the frames at the ends,

00:10:48.240 --> 00:10:50.240
not any of the others.

00:10:50.240 --> 00:10:55.590
Also, other frames can be skipped when,
for example, you don't have enough

00:10:55.630 --> 00:10:59.240
CPU to play at full rate,
or you're playing fast forward.

00:10:59.270 --> 00:11:03.020
The jargon for this is
temporal scalability.

00:11:04.970 --> 00:11:08.260
One weird thing that happens
when you have B frames though,

00:11:08.260 --> 00:11:12.240
is that the order the frames are
decoded in is not the same as the

00:11:12.240 --> 00:11:13.900
order the frames are displayed in.

00:11:13.960 --> 00:11:16.500
Because the codec isn't able to
predict what's happening in the future,

00:11:16.500 --> 00:11:20.450
it has to use information
that is already decoded.

00:11:21.010 --> 00:11:25.210
This reordering is why other parts of
the system need to understand about

00:11:25.300 --> 00:11:27.740
B-frames in order to support them.

00:11:28.940 --> 00:11:32.340
So, some of you are saying that you all
knew all of that already because

00:11:32.340 --> 00:11:35.970
you've been doing some work with
MPEG and it's all familiar to you.

00:11:36.140 --> 00:11:38.200
Well, there is a twist.

00:11:38.890 --> 00:11:41.970
You guys may know that you can
implement this kind of semantic using

00:11:41.970 --> 00:11:43.300
what's called a one-frame delay.

00:11:43.300 --> 00:11:46.140
And you build a small finite
state machine and you do certain

00:11:46.150 --> 00:11:50.300
transitions when you see that
the frame type is I or P or B.

00:11:50.330 --> 00:11:54.300
And that completely defines
the dependency structure.

00:11:54.550 --> 00:11:58.870
Now this is true for MPEG-2's
IPB pattern because only one future

00:11:59.020 --> 00:12:01.300
frame can ever be held at one time.

00:12:01.460 --> 00:12:05.300
This is not true for the
general case of H.264.

00:12:05.300 --> 00:12:08.560
In fact,
the H.264 encoder has allowed a lot more

00:12:08.560 --> 00:12:11.300
flexibility than previous frame patterns.

00:12:11.370 --> 00:12:14.300
P frames can use more than
one prior frame for decode.

00:12:14.380 --> 00:12:18.070
B frames can use more than
two prior or future frames.

00:12:18.300 --> 00:12:20.710
Not all I frames reset the decoder.

00:12:20.880 --> 00:12:21.790
We have a new name for these.

00:12:21.800 --> 00:12:25.300
These are IDR frames
in H.264 terminology.

00:12:25.300 --> 00:12:28.710
It stands for Instantaneous
Decoder Reset.

00:12:29.190 --> 00:12:32.700
P and B frames can depend
on other B frames and some

00:12:32.700 --> 00:12:34.740
I and P frames may be dropped.

00:12:35.220 --> 00:12:39.720
So where you had a pattern that
if you knew what to look for was

00:12:39.720 --> 00:12:46.770
fairly regular for MPEG-2 IPB,
the general case for H.264 could

00:12:46.770 --> 00:12:49.830
be a whole lot more complicated and
unpredictable and you couldn't just

00:12:49.830 --> 00:12:49.830
derive it from the IP and B letters.

00:12:50.350 --> 00:12:55.890
So instead of trying to store the IP and
B letters and using that as our basis,

00:12:56.380 --> 00:12:58.190
We identified that it was
important to record the

00:12:58.190 --> 00:13:01.300
following information per frame.

00:13:01.460 --> 00:13:04.300
Number one, is it a sync sample?

00:13:04.600 --> 00:13:09.260
Not every iframe is a sync sample now,
because decoding an iframe,

00:13:09.300 --> 00:13:13.300
although you could do it alone
based on no other information,

00:13:13.300 --> 00:13:18.300
the following P and B frames may need
information that's from previous frames.

00:13:18.460 --> 00:13:20.970
Number two, is it droppable?

00:13:21.300 --> 00:13:24.410
Not all B frames are
going to be droppable now,

00:13:24.490 --> 00:13:27.200
and some I and P frames
might be droppable.

00:13:27.450 --> 00:13:31.110
Number 3:
In what order are frames to be decoded?

00:13:31.210 --> 00:13:35.400
And possibly you might have some hints
about what timing they should be decoded.

00:13:35.670 --> 00:13:40.190
And number 4: At what time should
each frame be displayed?

00:13:41.910 --> 00:13:46.080
So, the moral of this little background
bit is that the dependencies between

00:13:46.200 --> 00:13:48.840
frames are becoming weirder and weirder,
but it's all in the cause of

00:13:48.840 --> 00:13:51.520
improved compression performance.

00:13:51.710 --> 00:13:55.690
IPB means that frames are stored
and decoded in a funny order,

00:13:55.830 --> 00:13:57.600
and some parts of the system
need to be aware of that.

00:13:57.600 --> 00:14:01.600
And convenient rules that
were true for MPEG-2,

00:14:01.600 --> 00:14:04.600
like the one frame delay,
don't work in H.264.

00:14:04.600 --> 00:14:07.980
They're not actually sufficient
to handle the general case.

00:14:09.700 --> 00:14:15.740
So let's talk about what this means for
QuickTime and for developers like you.

00:14:15.950 --> 00:14:17.840
Let's take a look at what
a video track looks like so

00:14:17.840 --> 00:14:20.110
that we can describe the API.

00:14:20.250 --> 00:14:25.140
At the basic level,
a video track contains a list of frames.

00:14:25.200 --> 00:14:27.360
Now in some of our APIs you'll
see we say samples.

00:14:27.390 --> 00:14:30.040
In some of our APIs you'll
see we say frames.

00:14:30.090 --> 00:14:33.410
We use sample in APIs that need to
be more generic because they might

00:14:33.770 --> 00:14:37.460
refer to things that other than
video they might refer to sound.

00:14:37.890 --> 00:14:40.620
But in our video APIs we
tend to use the word frame.

00:14:40.680 --> 00:14:43.280
And in fact when we're talking
about video we're using frame

00:14:43.330 --> 00:14:45.620
and sample interchangeably.

00:14:45.750 --> 00:14:49.070
Each sample has some timing
and dependency information,

00:14:49.070 --> 00:14:51.500
and they're stored in decode order.

00:14:51.550 --> 00:14:54.130
You'll notice that these frames
are kind of in a different order

00:14:54.130 --> 00:14:57.430
from how you'd expect them to,
because the car is jerking in and

00:14:57.430 --> 00:14:59.770
out of the scene as it moves along.

00:15:00.530 --> 00:15:05.790
In the cases where the decode order
is different from the display order,

00:15:05.860 --> 00:15:09.920
we also introduce information
called display offsets,

00:15:09.920 --> 00:15:13.640
which are how we describe
that reordering for the codec.

00:15:13.800 --> 00:15:16.940
The display offset is simply
the difference between the

00:15:16.980 --> 00:15:19.240
display time and the decode time.

00:15:19.420 --> 00:15:23.630
Specifically,
it's display time minus decode time.

00:15:23.910 --> 00:15:28.240
So by interpreting the decode times
and adding the display offsets,

00:15:28.250 --> 00:15:30.800
we know when each frame
should be displayed.

00:15:30.970 --> 00:15:35.170
And now we have the frames from that
portion of compressed video that

00:15:35.210 --> 00:15:40.500
can be decoded and displayed in the
order that makes sense to a user.

00:15:41.680 --> 00:15:44.720
Now in general,
those display offsets don't

00:15:44.720 --> 00:15:46.590
need to be exposed to the user.

00:15:46.600 --> 00:15:48.600
They don't need to
know about them at all.

00:15:48.660 --> 00:15:51.160
When the user wants to
rearrange portions of media,

00:15:51.160 --> 00:15:52.600
they use a different mechanism.

00:15:52.910 --> 00:15:55.600
They use the edit mechanism in QuickTime.

00:15:55.600 --> 00:16:00.660
Edits describe the segments of
video or other kinds of media that

00:16:01.100 --> 00:16:04.600
have been brought into the movie's
presentation at various times.

00:16:04.600 --> 00:16:08.000
So for example, in this case,
if the user decided to trim down to just

00:16:08.000 --> 00:16:12.650
include the end part of that parking
sequence and then follow it with a

00:16:12.830 --> 00:16:15.600
piece where the car's door is open,
the next scene, whatever that is,

00:16:15.600 --> 00:16:20.360
then we would have an edit that
described insertion of that

00:16:20.360 --> 00:16:26.600
piece of video into the track,
into the movie's presentation.

00:16:31.780 --> 00:16:37.360
Broadly, our APIs are divided into
high-level APIs and low-level APIs.

00:16:37.460 --> 00:16:42.600
The high-level APIs work
on tracks and movies.

00:16:42.600 --> 00:16:46.890
The low-level APIs work
on media and samples.

00:16:47.700 --> 00:16:50.990
So, we have high-level
APIs that implement cut,

00:16:51.230 --> 00:16:53.600
copy, and paste using the clipboard.

00:16:53.610 --> 00:16:57.100
We also have high-level APIs called
track segment editing APIs,

00:16:57.190 --> 00:17:01.600
which lets you insert pieces--segments
of media from movie to movie

00:17:01.600 --> 00:17:05.100
or track to track directly,
to delete, to rescale,

00:17:05.100 --> 00:17:07.510
and otherwise manipulate
bits of--bits of movies.

00:17:07.600 --> 00:17:09.600
These work without using the clipboard.

00:17:09.600 --> 00:17:13.370
The clipboard belongs to the user,
and they get antsy when it

00:17:13.370 --> 00:17:15.400
gets trashed unexpectedly.

00:17:15.710 --> 00:17:18.240
So, if you're trying to
programmatically manipulate

00:17:18.240 --> 00:17:20.870
media with the high-level APIs,
you should use the track

00:17:20.870 --> 00:17:23.600
segment APIs rather than cut,
copy, and paste.

00:17:24.920 --> 00:17:28.760
Beneath that we have the media
manipulation APIs which let

00:17:28.760 --> 00:17:32.220
you access individual samples,
and we have the image compression

00:17:32.320 --> 00:17:37.240
and decompression APIs which let you
compress and decode video frames.

00:17:37.370 --> 00:17:41.110
Now most of the plumbing changes we've
needed to make in order to support

00:17:41.110 --> 00:17:46.740
H.264 have been in these low-level APIs.

00:17:48.200 --> 00:17:50.360
Specifically,
the changes we've made in order to

00:17:50.480 --> 00:17:55.580
support H.264 have been to support
frame reordering by adding display

00:17:55.580 --> 00:17:58.650
offsets of various levels of API.

00:17:58.860 --> 00:18:02.000
We've also started to store
new dependency information.

00:18:02.030 --> 00:18:05.020
We have new sample flags,
such as one that says that

00:18:05.030 --> 00:18:06.990
a given frame is droppable.

00:18:10.010 --> 00:18:16.460
Applications that access individual video
frames must migrate to our new APIs if

00:18:16.460 --> 00:18:22.890
they want to access this information
in H.264 or other codecs with B-frames.

00:18:23.280 --> 00:18:26.740
We made this a deliberate design
decision because we wanted to avoid a

00:18:26.740 --> 00:18:31.200
situation where a user's content with
one of these new codecs would be damaged

00:18:31.200 --> 00:18:36.510
and messed up by an application that
failed to preserve the display offsets.

00:18:38.680 --> 00:18:42.250
These API changes are part of an
evolution that you'll see happening

00:18:42.540 --> 00:18:44.400
across the QuickTime landscape.

00:18:44.520 --> 00:18:47.310
We're moving away from handles.

00:18:47.530 --> 00:18:51.270
We're moving towards core foundation
style objects because retain counting

00:18:51.270 --> 00:18:53.400
helps you manage object lifetimes.

00:18:53.490 --> 00:18:54.900
We're moving away from QuickDraw.

00:18:54.960 --> 00:18:58.240
In place of Pixmaps as a
place to store image data,

00:18:58.320 --> 00:19:00.940
you'll see we're using
core video pixel buffers.

00:19:01.020 --> 00:19:05.060
And as part of this plumbing we're able
to do things in a multi-buffer manner.

00:19:05.160 --> 00:19:08.040
This allows us to do more
pipelining and have more

00:19:08.120 --> 00:19:10.600
asynchrony between various stages.

00:19:10.640 --> 00:19:13.700
In place of GWolds and GraphPorts
as a way to describe where

00:19:13.700 --> 00:19:16.590
a movie should be played,
we're moving towards a thing

00:19:16.640 --> 00:19:19.190
called the visual context,
which you would have seen a bunch

00:19:19.190 --> 00:19:24.100
of detail on in this morning's 207
session on a high performance video.

00:19:25.700 --> 00:19:29.440
The visual contexts give you
advantages like being able to do

00:19:29.440 --> 00:19:33.180
convenient hookups with OpenGL.

00:19:33.390 --> 00:19:36.410
If you've seen any of the audio sessions,
then you know we're moving away from

00:19:36.530 --> 00:19:39.740
Sound Manager towards Core Audio,
which gives us benefits

00:19:39.740 --> 00:19:44.940
like high resolution,
high fidelity, high definition audio.

00:19:45.180 --> 00:19:46.490
We're moving away from FS specs.

00:19:46.630 --> 00:19:50.070
We have a bunch of APIs to replace
those which all use data references.

00:19:50.380 --> 00:19:53.100
This gives us access to
Unicode and long file names.

00:19:53.100 --> 00:19:56.670
And there are two other
transformations that we're going

00:19:56.670 --> 00:19:58.940
through which I wanted to touch on.

00:19:59.100 --> 00:20:05.100
We're moving towards 64-bit time values
and towards 64-bit sample numbers.

00:20:05.100 --> 00:20:11.100
These enable us to have very high
time scales and very long movies.

00:20:11.390 --> 00:20:15.040
Now,
those transitions are not complete yet.

00:20:15.130 --> 00:20:16.100
Not all the plumbing is completed.

00:20:16.170 --> 00:20:20.580
But you'll notice that we use 64-bit
integers in the new APIs because all of

00:20:20.580 --> 00:20:22.100
the compilers support them natively now.

00:20:22.100 --> 00:20:24.390
There's no reason for us not to.

00:20:26.310 --> 00:20:29.330
There is a tech note,
particularly on the FS spec to

00:20:29.450 --> 00:20:32.440
data reference transition that you
should make in your applications.

00:20:32.440 --> 00:20:35.090
It's tech note number 2140.

00:20:37.110 --> 00:20:40.900
So that's now it for
theory in this session.

00:20:40.920 --> 00:20:43.200
The rest of it is going to be practice.

00:20:43.240 --> 00:20:47.400
We're going to show you how to do
things using our favorite modern APIs,

00:20:47.400 --> 00:20:51.870
a bunch of high-level demonstrations,
a bunch of low-level demonstrations,

00:20:51.980 --> 00:20:55.800
and then I'm going to finish it out
by showing you how to write your

00:20:55.930 --> 00:20:58.190
own codec with the new codec APIs.

00:20:58.200 --> 00:21:02.620
Now as I said, these samples are already
available for download.

00:21:02.620 --> 00:21:04.740
You can follow along with us as you like.

00:21:04.800 --> 00:21:06.560
We won't tell you when to turn the page.

00:21:08.410 --> 00:21:11.500
To start off the high level demos
I'd like to introduce for the

00:21:11.500 --> 00:21:15.650
first time on stage speaking,
David Eldred.

00:21:17.700 --> 00:21:20.450
All right, thanks a lot, Sam.

00:21:20.540 --> 00:21:25.420
So as Sam has described,
we have all of these high level

00:21:25.510 --> 00:21:30.940
APIs that are available to you that will
transparently take care of all of the

00:21:30.940 --> 00:21:34.020
complexities he's been talking about.

00:21:34.020 --> 00:21:37.340
These APIs, as Sam has said,

00:21:40.240 --> 00:21:43.680
include using the clipboard to do cut,
copy, and paste.

00:21:43.840 --> 00:21:46.790
This is user level operation.

00:21:46.900 --> 00:21:50.690
The Insert Segment APIs,
there's both movie and track

00:21:50.910 --> 00:21:52.800
versions of these APIs.

00:21:52.800 --> 00:21:58.890
They're available in the C and
the Cocoa QuickTime interfaces.

00:21:59.790 --> 00:22:01.090
Saving movies.

00:22:01.200 --> 00:22:03.960
You don't have to worry about all
of the complexities of B-frames

00:22:04.360 --> 00:22:06.060
when you're dealing with saving.

00:22:06.370 --> 00:22:10.830
This includes saving as reference movies,
saving as self-contained movies,

00:22:10.990 --> 00:22:12.560
or flattening movies.

00:22:13.240 --> 00:22:19.040
and exporting movies to whatever,
to various formats that we support,

00:22:19.090 --> 00:22:21.700
all high level APIs.

00:22:21.700 --> 00:22:26.050
So let's go to Demo Machine 1
and we'll take a look at a

00:22:26.380 --> 00:22:29.770
high level editing sample.

00:22:37.630 --> 00:22:40.170
So here I've got three clips.

00:22:40.560 --> 00:22:44.870
These are all HD size H.264 clips.

00:22:44.980 --> 00:22:48.490
Let's actually open them
in QuickTime Player.

00:22:48.500 --> 00:22:51.500
I have a question.

00:22:51.590 --> 00:22:53.290
Jim, he looks happy.

00:22:54.280 --> 00:22:56.490
And these are all clips
of QuickTime engineers

00:22:56.610 --> 00:23:00.210
doing what they do best,
standing around talking.

00:23:01.480 --> 00:23:04.740
But I'd like to edit them
into a coherent composition.

00:23:04.740 --> 00:23:06.820
So I've got a little
application here that does that.

00:23:06.920 --> 00:23:07.710
Very simple.

00:23:07.950 --> 00:23:09.900
Uses QtKit.

00:23:09.960 --> 00:23:14.260
And it's so simple I'm just going
to step through it with you.

00:23:15.640 --> 00:23:18.510
So first thing we're going to do is
we're going to create Qt movie objects

00:23:18.690 --> 00:23:21.550
to represent the three source movies.

00:23:22.950 --> 00:23:27.800
And we're going to create a QtMovie
object to represent our target movie.

00:23:28.170 --> 00:23:31.600
And we set an attribute on
that to make it editable.

00:23:31.670 --> 00:23:36.590
And using the insert
segment of movie API,

00:23:36.590 --> 00:23:42.310
we're going to insert a segment defined
by a QT time range from our source

00:23:42.310 --> 00:23:45.100
movie into our destination movie.

00:23:46.560 --> 00:23:51.860
And finally,
we write out the destination movie.

00:23:51.860 --> 00:23:55.380
So this example is so trivial
where I don't believe the

00:23:55.380 --> 00:23:56.740
sample code is available to you.

00:23:56.740 --> 00:23:58.490
If you're really interested
in getting your hands on it,

00:23:58.490 --> 00:24:02.500
we can give it to you in the lab.

00:24:02.500 --> 00:24:09.530
And let's see what we've made.

00:24:13.310 --> 00:24:15.200
There we are.

00:24:15.240 --> 00:24:19.290
All right, back to slides.

00:24:22.700 --> 00:25:07.000
[Transcript missing]

00:25:07.170 --> 00:25:09.200
And we could have added
together anything there.

00:25:09.200 --> 00:25:13.580
That could be DV content,
that could be H.264, MPEG-4,

00:25:13.630 --> 00:25:16.620
whatever codecs QuickTime supports,
or even other track types that

00:25:16.650 --> 00:25:19.340
QuickTime supports for editing.

00:25:19.420 --> 00:25:23.800
So there's your first simple
example of high level.

00:25:24.310 --> 00:25:25.200
Editing.

00:25:25.200 --> 00:25:28.720
And now we're going to look at an
example where we're doing multi-pass

00:25:28.760 --> 00:25:32.000
export using a custom data source.

00:25:33.410 --> 00:25:35.460
So,
Multipass Export is new in QuickTime 7,

00:25:35.460 --> 00:25:36.200
as Sam pointed out.

00:25:36.230 --> 00:25:41.300
It gives you the highest quality
H.264 compression possible.

00:25:41.360 --> 00:25:43.690
And in this example,
we're going to use the high-level

00:25:43.700 --> 00:25:48.910
APIs to allow QuickTime to
present its compression UI.

00:25:49.400 --> 00:25:52.640
Using the custom sources,
we're just going to provide the

00:25:52.670 --> 00:25:57.530
source data frames and QuickTime will
handle everything else for us.

00:25:57.610 --> 00:26:02.400
It'll handle the multi-pass
configuration of the codec,

00:26:02.520 --> 00:26:05.200
it'll handle the
compressor configuration,

00:26:05.200 --> 00:26:10.060
it'll write out the file for us,
flattening, hinting, everything else.

00:26:10.720 --> 00:26:13.300
So how does this example work?

00:26:13.380 --> 00:26:15.480
First we're going to create
a movie export component.

00:26:15.700 --> 00:26:19.580
Movie export component has video source
procedures and audio source procedures.

00:26:19.600 --> 00:26:23.600
We just have to hook our movie up to it.

00:26:23.610 --> 00:26:26.600
In this case we're going to have a
movie playing into a visual context.

00:26:26.600 --> 00:26:31.760
And that visual context is going to
be using core image to do some video,

00:26:31.780 --> 00:26:34.600
some messing around with the video.

00:26:34.600 --> 00:26:40.600
And that's going to be the source
for our video source procedure.

00:26:41.500 --> 00:26:44.000
Since we're video people,
we don't care about audio.

00:26:44.040 --> 00:26:47.090
We're just gonna plug the
audio directly through.

00:26:49.200 --> 00:26:50.960
So, off to this demo.

00:26:51.080 --> 00:26:53.860
Demo machine one, please.

00:26:53.940 --> 00:26:58.180
So, this, how many of you were in
session 207 earlier today?

00:27:05.590 --> 00:27:05.590
A few of you.

00:27:05.590 --> 00:27:05.590
So this is going to look familiar to you.

00:27:05.590 --> 00:27:05.590
The

00:27:06.700 --> 00:27:12.200
So I won't go into great detail about how
the visual context is configured and how

00:27:12.200 --> 00:27:14.200
we're hooking up the core image filter.

00:27:14.260 --> 00:27:17.460
If you have more questions about that,
come and see us in the lab.

00:27:17.500 --> 00:27:21.800
But first a quick demo of
what this application does.

00:27:22.730 --> 00:27:26.740
So we've got our favorite
car driving along.

00:27:26.750 --> 00:27:31.600
We've got a core image filter
we can change the settings of.

00:27:31.600 --> 00:27:38.720
And I like it a little darker.

00:27:40.550 --> 00:27:45.400
Go over here, make it,
that's about right.

00:27:45.530 --> 00:27:47.790
Change the color some.

00:27:54.570 --> 00:27:54.570
So this is how I like it.

00:27:56.600 --> 00:28:02.390
So when I click the export button,
we're first prompted for where

00:28:02.400 --> 00:28:04.380
we want to save the movie.

00:28:04.380 --> 00:28:06.840
We'll just replace that one.

00:28:06.840 --> 00:28:09.990
And in the settings dialog-- so
we're bringing up the standard

00:28:10.010 --> 00:28:13.790
QuickTime compression settings
dialog from our application.

00:28:13.810 --> 00:28:18.650
And here we're going to choose
H.264 and multi-pass encoding.

00:28:18.680 --> 00:28:22.680
And we're going to allow frame reordering
because we're using high-level APIs.

00:28:22.680 --> 00:28:24.970
We don't care if there's B frames.

00:28:35.300 --> 00:28:35.300
And we go ahead with these settings.

00:28:35.300 --> 00:28:35.300
And as you see, it's doing several
passes through the video.

00:28:35.300 --> 00:28:35.300
And

00:28:35.610 --> 00:28:39.800
So for each pass through the video,
our data proc is being

00:28:39.800 --> 00:28:42.060
called once for each frame.

00:28:42.060 --> 00:28:46.620
So if this had 100 frames of
video and it did five passes,

00:28:46.620 --> 00:28:48.640
we'd be called 500 times.

00:28:48.640 --> 00:28:51.740
So it opens up our resulting
movie in QuickTime player.

00:28:51.740 --> 00:28:53.080
Vroom.

00:28:53.710 --> 00:28:56.880
And there it is.

00:28:56.880 --> 00:28:59.300
So let's take a look at the code.

00:29:04.400 --> 00:29:07.860
So we're gonna look at this
pretty quickly since we've seen

00:29:07.860 --> 00:29:11.300
it before and of course come to
the lab if you want more details.

00:29:11.400 --> 00:29:15.730
But we're gonna focus on what happens
when that export button is clicked.

00:29:15.860 --> 00:29:18.120
So as you can see,

00:29:18.230 --> 00:29:23.660
First thing that happens is we prompt you
for the file you want to save this as.

00:29:25.450 --> 00:29:29.300
And we open a movie export component.

00:29:29.300 --> 00:29:32.400
That's as easy as describing the
component and opening an instance.

00:29:32.400 --> 00:29:35.380
Two, two procs that we have
inside our application,

00:29:35.520 --> 00:29:38.900
the video track property proc
and the video track data proc.

00:29:38.900 --> 00:29:42.140
We pass,
we create UPPs from these functions

00:29:42.370 --> 00:29:47.990
and we pass these in to the the
exporter component we just opened.

00:29:49.250 --> 00:29:54.930
And since we're video people,
we're just going to use the standard

00:29:55.160 --> 00:30:02.760
audio source procs from the source movie
and add those to the movie exporter.

00:30:05.620 --> 00:30:08.000
We invoke the standard
compression dialog.

00:30:08.110 --> 00:30:09.490
Nothing too complicated there.

00:30:09.660 --> 00:30:11.910
We ask the movie exporter
that we open to do it.

00:30:12.070 --> 00:30:15.350
Movie export do user dialog.

00:30:15.750 --> 00:30:21.690
Assuming that they don't cancel,
we are going to start our export with the

00:30:21.690 --> 00:30:23.490
movie Export from Procedures to DataRef.

00:30:23.820 --> 00:30:28.520
At this point,
we begin having data pulled from us.

00:30:28.620 --> 00:30:32.230
Our data proc that we provide
is going to be called.

00:30:33.060 --> 00:30:36.090
- And we'll take a quick
look at this Dataproc.

00:30:36.240 --> 00:30:39.440
So this is the procedure that we defined.

00:30:39.500 --> 00:30:42.430
It simply calls our
export frame procedure.

00:30:42.510 --> 00:30:45.090
And inside that export frame procedure,

00:30:46.120 --> 00:30:51.000
The parameters that we're called
with include a current time.

00:30:51.000 --> 00:30:53.880
That's the time for the frame
that it wants the data for.

00:30:54.000 --> 00:30:58.930
And in this application we just
set that time on our movie,

00:30:58.990 --> 00:31:03.020
task the movie to get it to draw.

00:31:03.300 --> 00:31:06.000
Do a little bit of application
specific stuff to get it to render

00:31:06.000 --> 00:31:08.200
that current time onto the screen.

00:31:08.440 --> 00:31:12.200
And then it's going to do a
readback from that screen buffer.

00:31:12.200 --> 00:31:17.320
Since we're doing this core image filter
and we want you to have visual feedback,

00:31:17.320 --> 00:31:19.390
we're rendering this to the screen.

00:31:20.650 --> 00:31:25.740
And once we read back that data,
we just fill in the parameters

00:31:25.740 --> 00:31:33.230
that are provided as part of
this call and return that data.

00:31:33.690 --> 00:31:34.790
That's it.

00:31:35.270 --> 00:31:38.600
That's how easy it is to
use movie export procedures.

00:31:38.710 --> 00:31:43.580
So, yeah, let's go back to slides.

00:31:46.310 --> 00:31:48.230
So there's a few things to note here.

00:31:48.340 --> 00:31:50.920
When you're using custom source
procedures like this and you're

00:31:50.920 --> 00:31:55.060
doing multi-pass compression,
you must return the same data for a given

00:31:55.180 --> 00:31:57.200
frame on each pass through the movie.

00:31:57.370 --> 00:32:01.360
That's very critical,
otherwise you'll throw things off.

00:32:02.500 --> 00:32:08.130
And if you've got very slow rendering,
you may want to consider exporting

00:32:08.650 --> 00:32:12.300
to an intermediate movie first,
an uncompressed intermediate movie.

00:32:12.370 --> 00:32:15.370
That will make the entire process
a little quicker since it has to do

00:32:15.380 --> 00:32:17.500
several passes through the movie.

00:32:17.550 --> 00:32:20.900
And if you have very slow processing,
that can take a while.

00:32:20.940 --> 00:32:24.400
That can also be handy if
your manipulations that you're

00:32:24.400 --> 00:32:27.480
doing on the video are for
some reason non-deterministic,

00:32:27.480 --> 00:32:29.860
you can't guarantee you're
going to return the same thing.

00:32:29.980 --> 00:32:32.660
render to an intermediate movie first.

00:32:33.150 --> 00:32:36.690
So with that,
I'm going to bring Sam back in

00:32:36.750 --> 00:32:38.540
to dive into the low levels.

00:32:38.560 --> 00:32:39.940
Where has he gone?

00:32:39.950 --> 00:32:41.390
There he is.

00:32:41.420 --> 00:32:42.600
SAM BAKER: Thanks a lot.

00:32:42.600 --> 00:32:48.260
I need a drink.

00:32:53.320 --> 00:32:55.540
Thank heavens.

00:32:55.580 --> 00:33:02.070
So, some apps need to dip below those
high-level APIs and access things

00:33:02.410 --> 00:33:03.890
directly at the frame level.

00:33:04.030 --> 00:33:05.800
For example,
if you want to do more complicated,

00:33:05.800 --> 00:33:08.060
more advanced editing,
if you want to use effects,

00:33:08.060 --> 00:33:10.860
if you need fine-grained control,
all sorts of reasons,

00:33:10.860 --> 00:33:12.970
you may need to go down
to those low-level APIs.

00:33:13.080 --> 00:33:14.730
Let's talk about them.

00:33:16.190 --> 00:33:20.790
To access one frame at a time,
we should call the new

00:33:20.790 --> 00:33:23.100
API GetMediaSample2.

00:33:23.150 --> 00:33:27.490
This is a replacement for an old API,
GetMediaSample.

00:33:28.430 --> 00:33:30.250
To get information about
multiple frames at once,

00:33:30.250 --> 00:33:33.900
you should call
CopyMediaMutableSampleTable.

00:33:33.900 --> 00:33:38.250
This is a replacement for a
suite of APIs with names like

00:33:38.440 --> 00:33:42.770
Get Media Sample Reference,
Get Media Sample References,

00:33:42.770 --> 00:33:46.040
and Get Media Sample References 64.

00:33:47.000 --> 00:33:53.540
The old APIs, the old-- that suite of old
APIs used an array of C structs

00:33:53.540 --> 00:33:55.870
that you would allocate.

00:33:56.160 --> 00:34:00.900
Each time we changed the API,
we had to introduce a whole new struct,

00:34:00.900 --> 00:34:03.570
and you had to migrate all of
this code over to the new struct,

00:34:03.570 --> 00:34:04.790
and it was a bit of a mess.

00:34:05.180 --> 00:34:09.890
The new API gives you an opaque
object called a QT sample table,

00:34:09.970 --> 00:34:14.900
which you call-- which you get the
information out of through accesses.

00:34:14.900 --> 00:34:17.780
That means that we'll be able to
introduce more information without

00:34:17.780 --> 00:34:21.250
having to completely reinvent the world.

00:34:22.070 --> 00:34:25.910
The corresponding APIs for adding
samples to media are AddMediaSample2,

00:34:25.910 --> 00:34:28.130
which is a replacement
for AddMediaSample,

00:34:28.230 --> 00:34:31.850
and AddSampleTableToMedia,
which is a replacement for

00:34:31.850 --> 00:34:35.630
AddMediaSampleReference,
AddMediaSampleReferences,

00:34:35.630 --> 00:34:37.750
and AddMediaSampleReferences64.

00:34:38.000 --> 00:34:42.980
Those older APIs will return errors
if you call on movies with B frames.

00:34:43.040 --> 00:34:45.420
Again,
it's deliberate in order to protect

00:34:46.000 --> 00:34:50.310
users from having their movies
damaged by failure to copy it across

00:34:50.310 --> 00:34:52.000
the frame reordering infrastructure.

00:34:52.000 --> 00:34:53.480
information.

00:34:55.270 --> 00:34:59.740
In QuickTime 7 we have a brand
new decompression API called

00:34:59.740 --> 00:35:02.520
the ICM Decompression Session.

00:35:03.240 --> 00:35:06.900
To use it, to create one,
you provide a dictionary that

00:35:06.930 --> 00:35:10.020
describes the pixel buffers
that you want to get out of it.

00:35:10.170 --> 00:35:15.970
Then you push in the compressed frames
and your callback function is called with

00:35:16.370 --> 00:35:21.100
buffers that contain those new compressed
frames and with other status messages.

00:35:21.100 --> 00:35:25.810
So, let's go and take a look at
that on the demo machine.

00:35:36.500 --> 00:35:40.310
Let's take a look at that
movie that we looked at before.

00:35:40.450 --> 00:35:43.830
The traditional way to show the
internal structure of movies was to

00:35:43.830 --> 00:35:45.400
use an application called Dumpster.

00:35:45.400 --> 00:35:48.390
Dumpster's been around since 1991.

00:35:48.420 --> 00:35:53.040
It's quite venerable and you
can still download it from

00:35:53.070 --> 00:35:55.270
our developer tools page.

00:35:55.420 --> 00:35:58.850
We've updated it so that it is
aware of the new information

00:35:59.240 --> 00:36:01.560
in QuickTime 7's movie files.

00:36:01.630 --> 00:36:02.960
And we can take a look at that here.

00:36:02.960 --> 00:36:05.590
For example,
this is the information about the

00:36:05.590 --> 00:36:07.400
video track and the video media.

00:36:07.550 --> 00:36:09.230
And if I click on this,
it'll open up the information

00:36:09.270 --> 00:36:10.230
about the sample descriptions.

00:36:10.270 --> 00:36:11.700
You probably can't read this.

00:36:11.750 --> 00:36:13.700
It says H.264 here.

00:36:13.950 --> 00:36:16.960
And if I open up this,
you probably can't read this either.

00:36:17.040 --> 00:36:19.240
But it says that all of the
samples have the same duration,

00:36:19.240 --> 00:36:21.060
125.

00:36:21.110 --> 00:36:23.820
And even if you can read this,
it's really hard to visualize.

00:36:23.840 --> 00:36:28.180
This is the list of display offsets
or composition offsets for the movie.

00:36:28.240 --> 00:36:31.940
And they're plus or minus 125.

00:36:32.260 --> 00:36:34.580
Not very visual,
so I thought it might be kind of neat

00:36:35.450 --> 00:36:40.900
to have a sample application which
shows this in a more visual way.

00:36:41.720 --> 00:36:44.600
It's called Movie Video Chart.

00:36:44.710 --> 00:36:46.600
It's already available for download.

00:36:46.600 --> 00:36:48.590
You could be doing this yourself.

00:36:48.730 --> 00:36:54.710
Let's take a look at that March of the
Penguins movie with this application.

00:36:55.110 --> 00:36:58.440
You can see thumbnails for
individual frames here.

00:36:58.440 --> 00:36:59.660
And you can see them in three columns.

00:36:59.740 --> 00:37:02.740
Let's move past that green thing
we always have at the beginning

00:37:02.740 --> 00:37:05.440
of the movie so we can find some
interesting pieces of video.

00:37:05.440 --> 00:37:09.430
Where are we here?

00:37:09.430 --> 00:37:09.430
I like this bit here.

00:37:11.380 --> 00:37:12.260
Here we go.

00:37:12.400 --> 00:37:16.280
So, thumbnails of all of these
high definition frames.

00:37:16.320 --> 00:37:20.080
This application is decoding
these high def frames so they can

00:37:20.080 --> 00:37:22.060
display little thumbnails of them.

00:37:22.120 --> 00:37:24.730
And this is the same as the diagram
I just showed you a little while ago.

00:37:25.030 --> 00:37:28.960
At the bottom we have
frames in decode order.

00:37:29.030 --> 00:37:32.250
And you can see that they're in decode
order because the penguin is going

00:37:32.370 --> 00:37:34.140
forward and back and forward and back.

00:37:34.690 --> 00:37:38.300
And then above that you can see
how the frames are reordered by the

00:37:38.440 --> 00:37:41.300
display offsets into display order.

00:37:41.400 --> 00:37:45.040
And here the penguin is jumping up,
up and away.

00:37:45.160 --> 00:37:50.040
Above that is a place where we have
information about the track edits that

00:37:50.180 --> 00:37:55.460
a user might have used in order to
construct the movie as they wanted.

00:37:55.790 --> 00:38:00.340
In this case the edit is rather dull,
it's just the whole movie,

00:38:00.410 --> 00:38:03.530
it's all there, but the application
displays it all the same.

00:38:03.710 --> 00:38:07.380
At the bottom you can see more
information about the samples.

00:38:07.450 --> 00:38:09.440
You can see the data
size of these frames,

00:38:09.440 --> 00:38:13.080
and I've also marked out which ones are
droppable and which ones are keyframes.

00:38:13.100 --> 00:38:17.470
It actually says "sync" here
when we find a keyframe.

00:38:19.220 --> 00:38:20.660
Keyframes are rare here.

00:38:20.710 --> 00:38:25.360
Well, let's take a look at another movie.

00:38:25.660 --> 00:38:27.990
Here's the movie that

00:38:28.350 --> 00:38:33.300
David just showed us that he edited
together from some high definition clips.

00:38:33.740 --> 00:38:41.160
Here's Tim and here's Jim and here's me.

00:38:42.380 --> 00:38:45.150
Let's zoom out so we can
have another look at this,

00:38:45.220 --> 00:38:48.100
because the edits are much
longer than the display offsets.

00:38:48.300 --> 00:38:51.300
You can see this piece at the top here,
these three different colors.

00:38:51.300 --> 00:38:54.300
I've chosen a different
color for each of the edits.

00:38:54.420 --> 00:38:58.300
And you can see that we've brought
together three clips of video.

00:38:58.300 --> 00:39:00.290
You can also see a
couple of other things.

00:39:00.450 --> 00:39:02.300
The keyframes are actually marked in red.

00:39:02.300 --> 00:39:06.300
And none of those clips
began with a keyframe.

00:39:06.590 --> 00:39:10.300
This frame was about four or
five frames after the keyframe.

00:39:10.300 --> 00:39:14.210
This edit was a few frames after as well.

00:39:14.340 --> 00:39:18.300
So there's some extra media information
that we've needed to include in

00:39:18.300 --> 00:39:18.300
this movie that we constructed
by bringing together these clips.

00:39:18.300 --> 00:39:19.230
So there's some extra media information
that we've needed to include in

00:39:19.270 --> 00:39:20.300
this movie that we constructed
by bringing together these clips.

00:39:21.400 --> 00:39:24.610
In order to prime the
decompressor for displaying the

00:39:24.730 --> 00:39:27.310
frames that you do want to see,
we had to bring back the key

00:39:27.430 --> 00:39:29.250
frame and the intermediate frames.

00:39:29.380 --> 00:39:32.670
But by using the track edits,
those frames are removed from

00:39:32.670 --> 00:39:36.910
the composition that has been
created at the user level.

00:39:38.290 --> 00:39:42.200
Now, if you have a very long
number of keyframes-- sorry,

00:39:42.200 --> 00:39:44.150
a very long number of difference
frames between the keyframe

00:39:44.230 --> 00:39:49.800
and the beginning of the edit,
then playing across these cuts might

00:39:49.800 --> 00:39:57.390
be hard and might take a lot of CPU,
and maybe you don't have enough CPU.

00:39:57.390 --> 00:39:57.390
But in this case,
we were able to play across

00:39:57.390 --> 00:39:57.390
this because we have a nice,
fast machine.

00:40:01.070 --> 00:40:05.240
So this application is
a piece of sample code.

00:40:05.310 --> 00:40:07.070
Let's take a quick look at it.

00:40:07.300 --> 00:40:10.400
It is an HIV-based application.

00:40:10.400 --> 00:40:13.100
It's a composited HIV.

00:40:13.180 --> 00:40:15.690
It has a single main view
that draws all of that art,

00:40:15.800 --> 00:40:18.590
including all of the thumbnails.

00:40:19.900 --> 00:40:23.900
I'll show a couple of
important points about it.

00:40:24.040 --> 00:40:29.660
We get information about
individual samples in groups.

00:40:29.730 --> 00:40:34.740
We use copy media mutable sample
table on a range and it gives

00:40:34.740 --> 00:40:37.050
us back a sample table object.

00:40:37.460 --> 00:40:40.570
Then we walk along the sample table
object and call these accesses to get

00:40:40.570 --> 00:40:45.400
the information out that we're going
to display in the user interface one,

00:40:45.460 --> 00:40:47.060
in the chart.

00:40:50.740 --> 00:40:54.450
In order to display the thumbnails,
we have to decode those frames.

00:40:54.450 --> 00:40:57.940
To decode the frames,
we create one of these new

00:40:58.220 --> 00:41:00.600
decompression session objects.

00:41:00.660 --> 00:41:03.320
To create a decompression session,
you build this dictionary

00:41:03.320 --> 00:41:04.600
that says the width,
the height,

00:41:04.600 --> 00:41:06.600
and the pixel format that you want.

00:41:06.850 --> 00:41:10.600
And there's other information
you might put in as well.

00:41:10.600 --> 00:41:14.600
You also provide the callback
function that you want to have

00:41:14.600 --> 00:41:17.600
called when frames are emitted
and other information happens.

00:41:17.600 --> 00:41:20.990
And you also provide
the image description.

00:41:24.360 --> 00:41:26.520
Once we've created the
decompression session,

00:41:26.520 --> 00:41:28.140
we can decode frames with it.

00:41:28.230 --> 00:41:31.230
To load those frames,
we call getMediaSample twice.

00:41:31.790 --> 00:41:37.300
Once to find out the size of the sample,
then we allocate some memory,

00:41:37.300 --> 00:41:40.300
and then we read it in into that buffer.

00:41:40.720 --> 00:41:46.300
After that, we call the decompression
session to decode the frame.

00:41:46.330 --> 00:41:49.300
Now, note one important thing here.

00:41:49.300 --> 00:41:52.300
Because we're working at the low level,
it's our responsibility to make sure

00:41:52.300 --> 00:41:56.340
that the decoder is always primed with
the right state for doing the decode

00:41:56.340 --> 00:41:58.300
of the frame we're going to give it.

00:41:58.300 --> 00:42:01.480
So, if we find that we're not...

00:42:01.700 --> 00:42:23.900
[Transcript missing]

00:42:25.830 --> 00:42:28.600
When the frames are decoded,
it calls our callback function.

00:42:28.600 --> 00:42:30.800
It's called a tracking callback.

00:42:30.870 --> 00:42:34.600
It calls us with a flag
that says emitting frame

00:42:34.600 --> 00:42:37.150
when it's emitting a frame,
and when that happens,

00:42:37.150 --> 00:42:46.490
we wrap that frame with--as a CG image,
and then we use an HIView

00:42:46.490 --> 00:42:48.770
utility to draw that CG image
with the correct orientation.

00:42:49.050 --> 00:42:53.020
When we get the message that the
Image Compression Manager no longer

00:42:53.020 --> 00:42:57.000
needs that source data to be held around,
we can free the buffer.

00:42:57.000 --> 00:43:00.990
So that's it for this demo.

00:43:01.050 --> 00:43:03.390
Let's go back to slides.

00:43:09.420 --> 00:43:12.370
Like I said,
if you use the low-level APIs,

00:43:12.390 --> 00:43:16.390
it's your responsibility to ensure that
frames are decoded in the right order,

00:43:16.390 --> 00:43:18.230
and that means that if
you're jumping around,

00:43:18.230 --> 00:43:21.440
you may need to go back to
the keyframe and decode ahead.

00:43:21.550 --> 00:43:23.430
We call this catch-up.

00:43:24.100 --> 00:43:26.660
Also,
you may need to handle frame reordering.

00:43:26.840 --> 00:43:29.570
Now, in this application,
we are deliberately showing the

00:43:29.660 --> 00:43:33.020
frames in decode order because we're
trying to show what the internal

00:43:33.020 --> 00:43:35.000
structure of the movie is like.

00:43:35.000 --> 00:43:36.990
Not all applications
are going to do that.

00:43:36.990 --> 00:43:39.900
Normally you want to extract the frames
and pull them out in display order.

00:43:40.000 --> 00:43:44.980
If all you want is pixels,
then you should be using the high-level

00:43:44.980 --> 00:43:47.000
API to access the decompressed frames.

00:43:47.000 --> 00:43:50.000
And those APIs are visual context APIs.

00:43:50.000 --> 00:43:52.890
If you're trying to go
towards OpenGL processing,

00:43:53.010 --> 00:43:56.990
then what you want is the
OpenGL texture context.

00:43:57.000 --> 00:44:01.000
If you're going to do more
processing on the CPU,

00:44:01.000 --> 00:44:04.920
then you want pixel buffers,
and so you want to use the Qt pixel

00:44:04.920 --> 00:44:06.730
buffer visual context instead.

00:44:10.300 --> 00:44:15.960
It's time to talk about the
low-level image compression APIs.

00:44:16.150 --> 00:44:18.760
Once again,
sometimes the high-level APIs for

00:44:18.930 --> 00:44:21.040
movie export aren't applicable to you.

00:44:21.250 --> 00:44:24.540
For example,
the movie export components always create

00:44:24.540 --> 00:44:27.530
a new file to write out their content,
a new movie file,

00:44:27.530 --> 00:44:28.930
a new MP4 file or whatever.

00:44:29.230 --> 00:44:32.540
Well, if what you're trying to
generate isn't a file,

00:44:32.770 --> 00:44:34.960
then that might not be what you want.

00:44:35.080 --> 00:44:39.060
So you may have to go down and use the
low-level compression APIs yourself.

00:44:39.760 --> 00:44:44.600
There's a new API in QuickTime 7
called the compression session.

00:44:44.650 --> 00:44:47.970
And when you create this,
you pass in the width, the height,

00:44:48.100 --> 00:44:51.790
the codec type,
and a session options object.

00:44:52.050 --> 00:44:57.030
Then you push in pixel buffers with
display times in display order.

00:44:57.030 --> 00:45:01.900
Your callback is called with the
encoded frames in decode order,

00:45:02.140 --> 00:45:04.900
so frames may be reordered
during compression.

00:45:04.900 --> 00:45:07.830
For that reason,
you're not going to get the compressed

00:45:07.830 --> 00:45:11.380
frames returned to you immediately,
they might be returned

00:45:11.380 --> 00:45:12.900
to you after some delay.

00:45:13.250 --> 00:45:22.290
This delay, this queue,
is called a look ahead window.

00:45:22.290 --> 00:45:22.290
It also gives the codec an opportunity
for further optimized compression.

00:45:24.980 --> 00:45:28.870
So I'm about to show a
demonstration of compressing live

00:45:28.880 --> 00:45:33.330
video from a camera to H.264,
similar to what you get in the

00:45:33.330 --> 00:45:38.080
QuickTime Player new movie recording
feature in QuickTime Player Pro.

00:45:38.460 --> 00:45:42.300
What we're doing is we're going
to have RGB frames stored in pixel

00:45:42.300 --> 00:45:46.090
buffers and then we'll pass those
to a compression session which will

00:45:46.090 --> 00:45:49.630
return us H.264 frames that we're
going to drop into a new movie.

00:45:49.640 --> 00:45:51.790
How do we get those frames?

00:45:51.790 --> 00:45:55.800
Well, I took advantage of some sample
code that was already there.

00:45:55.800 --> 00:46:00.210
I took the, I think it was the Son of
Mungrab sample code,

00:46:00.290 --> 00:46:03.060
which shows you how to use a
sequence grabber to get frames in.

00:46:03.060 --> 00:46:04.110
But I adjusted it.

00:46:04.320 --> 00:46:08.870
I made it use decompression sessions to
decode the frames from whatever format

00:46:08.870 --> 00:46:11.320
you get them from the camera into RGB.

00:46:12.950 --> 00:46:16.300
While they're in RGB format,
we can also do other things.

00:46:16.410 --> 00:46:20.300
In this sample code I'm showing how
to draw on top of these frames with

00:46:20.310 --> 00:46:23.030
a Core Graphics bitmap context.

00:46:23.570 --> 00:46:27.180
And for some variety,
we're using-- we're also going

00:46:27.180 --> 00:46:29.990
to wrap them as a CG image in
order to display a preview.

00:46:30.100 --> 00:46:33.560
Now this isn't the only way to
display previews during Capture.

00:46:33.950 --> 00:46:39.500
In fact, it's probably more efficient
to use OpenGL to display them.

00:46:39.620 --> 00:46:43.410
There's also some sample code around
that was demonstrated yesterday in

00:46:43.410 --> 00:46:45.260
the audio session about Capture.

00:46:45.260 --> 00:46:47.940
That shows how to use
OpenGL to display a preview.

00:46:48.100 --> 00:46:52.160
So this is an alternative for variety.

00:46:52.500 --> 00:46:54.510
So, to help me with this
demo on the demo machine,

00:46:54.510 --> 00:46:56.710
I'd like to bring up the QuickTime Mr.

00:46:56.710 --> 00:46:58.680
Canada, Tim Cherna.

00:46:59.020 --> 00:47:00.730
Big hand please for Mr.

00:47:00.730 --> 00:47:01.290
Cherna.

00:47:06.200 --> 00:47:20.600
[Transcript missing]

00:47:21.600 --> 00:47:28.200
[Transcript missing]

00:47:33.400 --> 00:47:48.400
[Transcript missing]

00:47:49.340 --> 00:47:55.510
Okay, let's open this movie back in
movie video chart to prove that

00:47:55.840 --> 00:47:58.240
the frames are being reordered,
which you can only get if you

00:47:58.240 --> 00:47:59.790
use the new compression APIs.

00:47:59.930 --> 00:48:02.300
Here's a whole lot of Tims.

00:48:02.300 --> 00:48:03.300
And here's a flag.

00:48:03.600 --> 00:48:06.780
So you can see that the frames are
being displayed in a different order

00:48:06.910 --> 00:48:08.400
from the order they were decoded.

00:48:08.490 --> 00:48:10.890
Great.

00:48:10.990 --> 00:48:12.680
Let's have a look at the code.

00:48:25.420 --> 00:48:28.390
Once again,
we create a decompression session

00:48:28.650 --> 00:48:31.020
by constructing a dictionary
that has the width and the

00:48:31.020 --> 00:48:32.300
height and the pixel format.

00:48:32.310 --> 00:48:36.410
We also indicate that we want this
pixel buffer to be compatible with

00:48:36.410 --> 00:48:38.300
Big Map contexts and CG images.

00:48:38.570 --> 00:48:43.610
And we provide the image description
which came to us from the sequence

00:48:43.610 --> 00:48:47.050
grabber and our tracking callback.

00:48:47.300 --> 00:48:52.380
Now we construct the compression
session by first creating

00:48:52.700 --> 00:48:54.300
the session options object.

00:48:54.310 --> 00:48:57.360
Now if you were looking at this
on your PowerBook at the moment,

00:48:57.360 --> 00:48:58.300
you'd probably see a bit more text.

00:48:58.300 --> 00:49:03.050
I've removed some of the error
checking just so it fits on the slide.

00:49:04.470 --> 00:49:07.740
There's a bunch of options that
it's very important that we set.

00:49:07.810 --> 00:49:10.730
We must set the flags to
say that we allow temporal

00:49:10.730 --> 00:49:15.150
compression and frame reordering,
or we won't get B-frames.

00:49:15.850 --> 00:49:20.640
We set the keyframe interval because
it's nice to know what it can be,

00:49:20.880 --> 00:49:24.080
but we're not required to set that.

00:49:24.080 --> 00:49:26.220
Because we want to store
the frames in a movie,

00:49:26.220 --> 00:49:30.020
we need to know their decode durations,
and so we set the flag that

00:49:30.020 --> 00:49:31.850
says durations are needed.

00:49:32.140 --> 00:49:35.380
Now we're only going to give
the ICM display timestamps,

00:49:35.380 --> 00:49:40.560
but it's going to derive out of
that all of the durations as well.

00:49:41.250 --> 00:49:45.040
We tell the ICM it's okay to
drop frames by setting the

00:49:45.040 --> 00:49:50.440
allow frame time changes flag,
and we also set the data rate.

00:49:50.500 --> 00:49:52.960
Once again,
just like with the decompression session,

00:49:53.040 --> 00:49:58.350
we give it a callback that's going
to be called with our encoded frames.

00:50:01.300 --> 00:50:06.410
So, the sequence grabber works by calling
the dataproc function that we provide it,

00:50:06.530 --> 00:50:08.160
and it calls us with each frame.

00:50:08.200 --> 00:50:11.180
Once we've made sure that we
have a compression session

00:50:11.190 --> 00:50:15.200
and a decompression session,
we decode these frames.

00:50:15.350 --> 00:50:19.290
In this case I'm using an API called
non-scheduled display time,

00:50:19.290 --> 00:50:22.200
which is what you'd use if
the frames had been reordered.

00:50:22.440 --> 00:50:26.060
Here it's just the way I'm triggering
those frames to be actually displayed,

00:50:26.300 --> 00:50:29.200
well, not displayed,
output to our callback function.

00:50:29.200 --> 00:50:32.180
So here's the callback function
for the decompression session.

00:50:32.180 --> 00:50:35.820
Once again it gives us different flags.

00:50:36.580 --> 00:50:40.410
When it tells us it's emitting a frame,
then we're going to

00:50:40.720 --> 00:50:42.860
take that pixel buffer,
we're going to draw on top

00:50:42.860 --> 00:50:47.340
of it using a bitmap context,
and then we wrap it in a CG image

00:50:47.670 --> 00:50:51.490
and place it in an HI image
view inside the preview window.

00:50:51.500 --> 00:50:57.550
We also then feed the frame
to the compression session.

00:51:00.650 --> 00:51:03.150
The compression session
has another callback.

00:51:03.260 --> 00:51:06.400
Here's our callback from that function,
from that session.

00:51:06.450 --> 00:51:09.840
Now when we get the first frame,
we have to create a new

00:51:09.840 --> 00:51:12.400
media to store the frames.

00:51:12.470 --> 00:51:15.620
And it's convenient to do that once
we get the first frame because that's

00:51:15.620 --> 00:51:17.800
when we get the image description,
and we can use the information

00:51:17.810 --> 00:51:21.270
in the image description to
work out how large what the

00:51:21.350 --> 00:51:23.310
dimensions of the media should be.

00:51:23.530 --> 00:51:27.410
And then we call
addMediaSampleTo to add samples.

00:51:27.490 --> 00:51:32.020
There's actually a utility here I'm
pointing out which adds media samples

00:51:32.130 --> 00:51:35.330
directly from the encoded frame object.

00:51:38.410 --> 00:51:40.690
When we're told to stop,
when I click the close

00:51:40.690 --> 00:51:44.970
button in the window,
or quit, we tell the compression

00:51:44.970 --> 00:51:48.300
session to complete any frames
that are still in its queue.

00:51:48.300 --> 00:51:52.440
And then we release the object.

00:51:55.810 --> 00:51:58.650
So once we've done that,
we have a movie that has a

00:51:58.650 --> 00:52:02.860
media that's full of frames,
but the movie's still empty.

00:52:02.950 --> 00:52:03.420
Why is that?

00:52:03.610 --> 00:52:05.080
Because the edit list
hasn't been touched.

00:52:05.200 --> 00:52:06.490
The edit list is still empty.

00:52:06.620 --> 00:52:10.950
So it's important to insert all of that
media into the track so that we have an

00:52:11.300 --> 00:52:14.190
edit that includes all of that media.

00:52:14.290 --> 00:52:18.200
It's easy to forget this, but if you do,
you'll have an empty movie

00:52:18.200 --> 00:52:19.190
and you'll know what to do.

00:52:19.190 --> 00:52:22.720
Then the only thing left to
do is to write out the movie

00:52:22.720 --> 00:52:24.200
header and to close the file.

00:52:24.500 --> 00:52:27.200
This sample code uses
the movie storage APIs.

00:52:27.200 --> 00:52:30.160
These are a placement
for FS spec based APIs.

00:52:30.200 --> 00:52:35.580
We have a--the movie storage APIs use
data references and we have a bunch

00:52:35.580 --> 00:52:40.200
of useful utilities to construct
data references from FS refs,

00:52:40.200 --> 00:52:46.200
from CFURLs, and from CFStrings,
from other kinds of things as well.

00:52:49.860 --> 00:52:52.630
So that's it for that demo.

00:52:52.680 --> 00:52:53.960
Let's go back to slides.

00:52:54.240 --> 00:52:59.490
And for me, another drink of water.

00:53:08.330 --> 00:53:10.030
Okay.

00:53:10.330 --> 00:53:14.430
Once again,
in order to get the advantages of H.264,

00:53:14.520 --> 00:53:17.910
you must use the new low-level
compression APIs if you're doing

00:53:17.960 --> 00:53:19.300
compression at the low level.

00:53:19.390 --> 00:53:23.300
The high-level API, the movie export,
already does this for you,

00:53:23.300 --> 00:53:26.120
but if you're at the low level,
you've got to use the new APIs.

00:53:26.300 --> 00:53:29.590
The good news is that the
new APIs support all codecs,

00:53:29.590 --> 00:53:30.690
new and old.

00:53:32.330 --> 00:53:35.020
One more point on this.

00:53:35.020 --> 00:53:38.470
While we're doing that
real-time compression there,

00:53:38.500 --> 00:53:43.520
there's a whole lot of pixel buffers
that are being created and destroyed.

00:53:44.060 --> 00:53:47.430
Mapping and unmapping large
pieces of virtual memory involves

00:53:47.430 --> 00:53:49.530
some per-page kernel overhead.

00:53:49.530 --> 00:53:52.890
And that can be quite a lot when
all those frames are high def and

00:53:53.240 --> 00:53:55.830
quite a lot when you're doing 30
of them or 60 of them per second.

00:53:56.000 --> 00:53:59.160
So it's important to have an
efficient recycling mechanism so

00:53:59.160 --> 00:54:02.810
that you're not constantly mapping
and unmapping memory over and over.

00:54:03.000 --> 00:54:06.660
Core Video provides a mechanism
called the Pixel Buffer

00:54:06.660 --> 00:54:10.230
Pool which helps reallocate,
helps reuse and recycle

00:54:10.230 --> 00:54:11.760
buffers efficiently.

00:54:12.000 --> 00:54:17.430
It's recommended that you use it so that,
like I said, you're not always unmapping

00:54:17.520 --> 00:54:19.000
and paying kernel overheads.

00:54:19.000 --> 00:54:22.450
And this sample code, if you look at it,
you won't actually find the

00:54:22.620 --> 00:54:24.000
word "pool" in it at all.

00:54:24.000 --> 00:54:27.630
But we are taking advantage of
Pixel Buffer Pools in this sample

00:54:27.850 --> 00:54:32.220
code because decompression sessions
use Pixel Buffer Pools to create the

00:54:32.220 --> 00:54:35.640
buffers that they return us back.

00:54:36.320 --> 00:54:38.830
If you take that code and you
change it and you just take the

00:54:38.930 --> 00:54:41.540
compression session stuff with you,
then you should consider using

00:54:41.540 --> 00:54:44.200
a pixel buffer pool to create
your source pixel buffers.

00:54:44.200 --> 00:54:47.180
There's actually utility as
part of compression sessions

00:54:47.190 --> 00:54:49.200
that helps you do that.

00:54:49.360 --> 00:54:50.910
So,

00:54:51.540 --> 00:54:57.380
One more thing, one more piece,
we're going to talk about writing codecs.

00:54:58.210 --> 00:55:00.310
If you write a codec,
generally that means that you

00:55:00.310 --> 00:55:03.110
need to write two components,
an image compressor and

00:55:03.480 --> 00:55:05.710
an image decompressor.

00:55:06.340 --> 00:55:11.420
Now we have a brand new,
bold new image compressor API for

00:55:11.420 --> 00:55:13.200
components in QuickTime 7.

00:55:13.200 --> 00:55:20.200
We have made more modest changes to
the decompressor API in QuickTime 7.

00:55:20.200 --> 00:55:24.230
And I'm going to show you
on the demo machine a brief

00:55:24.230 --> 00:55:28.210
survey of the example codec.

00:55:30.760 --> 00:55:34.080
Now this is a well
commented piece of code.

00:55:34.100 --> 00:55:37.700
It should show you how to
use the interface well.

00:55:37.700 --> 00:55:38.700
But it's not a very good codec.

00:55:38.700 --> 00:55:40.480
A better spin?

00:55:40.840 --> 00:55:42.790
You can do better.

00:55:43.080 --> 00:55:45.230
It's a very simplistic
encoding algorithm.

00:55:45.240 --> 00:55:49.160
It just encodes the high bits first,
and when it runs out of bits, it stops.

00:55:49.240 --> 00:55:51.700
Well, one benefit of this is it's
very easy to see the loss.

00:55:51.960 --> 00:55:54.460
Loss means you're rounding
down by some amount,

00:55:54.540 --> 00:55:58.800
and that means that images get darker,
or because we're doing it in YUV,

00:55:58.800 --> 00:56:00.270
greener.

00:56:00.930 --> 00:56:03.230
So let me show you how to use this.

00:56:03.450 --> 00:56:09.800
I've already built this and so I'm going
to install it in library/quicktime.

00:56:10.120 --> 00:56:12.900
Library/quicktime is the
place where you'll normally

00:56:12.900 --> 00:56:14.910
install your codec components.

00:56:14.910 --> 00:56:19.800
They can also be installed in the
home directory/library/quicktime.

00:56:19.800 --> 00:56:26.700
So now that I've installed this,
I can open up one of those

00:56:26.700 --> 00:56:28.630
clips that we saw before.

00:56:29.420 --> 00:56:31.190
Here's Jim.

00:56:34.200 --> 00:56:35.160
This time.

00:56:35.470 --> 00:56:37.180
Okay.

00:56:37.270 --> 00:56:40.040
I like it better when he's smiling.

00:56:40.110 --> 00:56:42.580
So now that I've added that codec,

00:56:44.170 --> 00:56:48.010
Assuming that I quit the player,
let's quit the player.

00:56:48.010 --> 00:56:51.740
You have to quit the player
and now when I relaunch it,

00:56:51.760 --> 00:56:53.090
it'll see the new codec.

00:56:53.290 --> 00:56:57.100
If you don't see your new codec,
try quitting.

00:56:57.100 --> 00:57:02.140
Let's export again.

00:57:05.760 --> 00:57:07.920
There it is, example IPB.

00:57:08.120 --> 00:57:13.600
So, at top quality we don't see any
loss as we drag things down.

00:57:13.600 --> 00:57:19.590
Loss means dark and green and
brooding and kind of messed up.

00:57:19.720 --> 00:57:21.600
Well, you can do better, like I said.

00:57:21.920 --> 00:57:24.600
Well, I've already compressed this one.

00:57:24.600 --> 00:57:27.600
I cooked one before the show.

00:57:27.620 --> 00:57:29.590
And here it is.

00:57:36.590 --> 00:57:37.960
You saw a flicker there.

00:57:38.020 --> 00:57:40.610
The better encoded
frames are the B frames.

00:57:40.720 --> 00:57:43.200
The ones that look poorer
are the I frames because they

00:57:43.320 --> 00:57:44.260
had to start from scratch.

00:57:44.330 --> 00:57:48.940
The B frames could use the other
frames as a better starting point.

00:57:48.940 --> 00:57:52.430
We could take a look at
this in movie video chart.

00:57:52.740 --> 00:57:55.570
If I quit it,
you can take a look at this movie

00:57:55.570 --> 00:57:58.550
video chart and you can do all
of this yourself and confirm to

00:57:58.580 --> 00:58:00.100
yourself that it's using B-frames.

00:58:00.100 --> 00:58:02.600
You can see the frames
are reordered there.

00:58:02.600 --> 00:58:07.570
Okay,
let's have a quick go with the source.

00:58:09.950 --> 00:58:13.700
Now, I've separated out the
naive encoding logic,

00:58:13.750 --> 00:58:15.400
which is in these naive files here.

00:58:15.430 --> 00:58:18.210
You'll replace those with
your very smart algorithms.

00:58:18.310 --> 00:58:20.710
I've separated that from the interface.

00:58:20.950 --> 00:58:25.380
Let's look at the important functions
first in the new compressor API.

00:58:28.700 --> 00:58:30.850
There are three main functions.

00:58:30.850 --> 00:58:36.560
In Prepare to Compress Frames you
return another dictionary that

00:58:36.560 --> 00:58:40.600
describes the pixel buffers that
you want to get as your source.

00:58:40.620 --> 00:58:43.600
For example, you want to say what the
width and height you want are,

00:58:43.600 --> 00:58:47.440
and you can ask for extra pixels
to be allocated for scratch

00:58:47.440 --> 00:58:49.600
memory on the right and bottom.

00:58:49.750 --> 00:58:54.600
In this case we allocate memory to
round up to 16 by 16 macro blocks.

00:58:54.600 --> 00:58:59.720
We can actually now allocate
extra memory on the top and left,

00:58:59.720 --> 00:59:03.600
which may help if you're doing something
called unrestricted motion compensation.

00:59:03.600 --> 00:59:06.200
Which means you can have
motion vectors from outside.

00:59:06.200 --> 00:59:10.190
It's one of those geeky codec things.

00:59:11.600 --> 00:59:18.410
You can also ask for the bytes per row
to be a multiple of a particular number.

00:59:18.630 --> 00:59:22.280
If you're using AlteVec or SSE code,
it's likely that you want to

00:59:22.280 --> 00:59:26.150
ask for at least a multiple
of 16 for the bytes per row.

00:59:28.450 --> 00:59:32.120
Another thing you can ask for
is a list of input pixel formats

00:59:32.300 --> 00:59:33.340
that you want to support.

00:59:33.580 --> 00:59:38.440
And this codec asks for our
standard YCBCR 422 codec,

00:59:38.440 --> 00:59:44.290
which is pixel format type,
which is called 2VUI.

00:59:45.870 --> 00:59:47.840
This is a new feature
for the new Codec API.

00:59:48.000 --> 00:59:50.430
Previously you had to
implement RGB input.

00:59:50.760 --> 00:59:53.640
We were sick of everyone having to
write their own RGB to YUV converters,

00:59:53.640 --> 00:59:58.590
so now we provide some standard ones.

00:59:59.290 --> 01:00:01.250
Another element that goes
in this dictionary here,

01:00:01.340 --> 01:00:02.500
we're saying what the gamma level is.

01:00:02.500 --> 01:00:05.130
We ask for the video gamma level.

01:00:05.250 --> 01:00:10.880
Also, the color profiles,
in particular the YCBCR to

01:00:11.270 --> 01:00:14.900
RGB matrix used for standard def
and high def video are different.

01:00:15.000 --> 01:00:16.960
So we specify which of
those we're going to use.

01:00:17.100 --> 01:00:20.700
We're using the standard def
ones in this example codec.

01:00:23.060 --> 01:00:29.020
Apart from creating that dictionary,
prepare to compress frames is also

01:00:29.020 --> 01:00:32.330
a place where it's appropriate
to set any extra information

01:00:32.340 --> 01:00:33.240
on the image description.

01:00:33.240 --> 01:00:36.000
Here we're setting the
gamma level to 2.2.

01:00:39.670 --> 01:00:45.290
The encode frame function is
called for each source frame.

01:00:47.330 --> 01:00:49.040
You don't have to encode it immediately.

01:00:49.200 --> 01:00:50.890
You can build up a
little queue of frames.

01:00:51.410 --> 01:00:56.540
In this case, we're using a C array
to store that queue,

01:00:56.540 --> 01:00:58.540
but you can store it however you want.

01:00:58.540 --> 01:01:02.540
You can use core foundation arrays
because these objects are core

01:01:02.540 --> 01:01:04.400
foundation retain counted objects.

01:01:04.750 --> 01:01:06.410
You can use STL if you like.

01:01:06.540 --> 01:01:07.610
I don't care.

01:01:09.670 --> 01:01:12.590
But you probably want to have some
threshold beyond which you decide that,

01:01:12.600 --> 01:01:14.800
okay, I've got enough frames,
I'll start encoding,

01:01:14.810 --> 01:01:17.810
or else you could end up starting
to swap the frames out because

01:01:17.950 --> 01:01:19.900
you'll have so many sitting there.

01:01:21.510 --> 01:01:25.100
So encodeFrame is called for each
source frame in display order.

01:01:25.290 --> 01:01:30.130
CompleteFrame is called to say
it's time to decode a given frame.

01:01:30.590 --> 01:01:36.400
You don't have to return that one first,
but you must emit it and encode it,

01:01:36.600 --> 01:01:40.020
or drop it, before this function returns.

01:01:40.580 --> 01:01:44.640
So there's a function here that
decides which frame to encode next.

01:01:44.730 --> 01:01:46.250
I'm not going to go into that in detail.

01:01:46.400 --> 01:01:49.300
I'll just say that it does
implement an MPEG-2 IPB pattern.

01:01:49.450 --> 01:01:53.180
You could modify it to support
other patterns if you want

01:01:53.440 --> 01:01:55.630
that kind of flexibility.

01:01:55.990 --> 01:01:58.700
When you decide it is
time to encode a frame,

01:01:58.700 --> 01:02:03.090
you create a buffer with the ICM encoded
frame create mutable function,

01:02:03.100 --> 01:02:07.390
and you pass it the worst case,
the largest data size you might need.

01:02:07.400 --> 01:02:10.230
Then you fill in as much as you need.

01:02:10.500 --> 01:02:29.400
[Transcript missing]

01:02:29.720 --> 01:02:31.990
The other choice is that
you could drop a frame.

01:02:32.000 --> 01:02:34.860
This codec doesn't actually drop frames,
but if you look in the header

01:02:34.950 --> 01:02:37.400
file you'll see how to do that.

01:02:38.000 --> 01:02:41.780
Let's have a look at the important
functions in the decompressor API.

01:02:41.860 --> 01:02:44.200
There are five functions
that are important.

01:02:44.270 --> 01:02:46.660
In initialize,

01:02:46.800 --> 01:02:50.700
You set some basic flags to describe
the basic characteristics of your codec.

01:02:50.700 --> 01:02:53.020
In this case we say that
we support B frames.

01:02:53.350 --> 01:02:55.070
This is out of order display times.

01:02:55.220 --> 01:02:57.760
We also say that we
are multi-buffer aware.

01:02:57.760 --> 01:03:02.280
We get high performance playback
if we are able to decode each

01:03:02.310 --> 01:03:04.460
frame to a separate buffer.

01:03:04.540 --> 01:03:08.210
If you don't,
you'll lose some performance because

01:03:08.410 --> 01:03:14.280
we may need to make extra copies of the
same buffer to upload them to OpenGL,

01:03:14.380 --> 01:03:15.880
for example.

01:03:16.330 --> 01:03:20.300
You set some basic flags to describe
the basic characteristics of your codec.

01:03:20.300 --> 01:03:22.800
In this case we say that
we support B frames.

01:03:22.860 --> 01:03:24.700
This is out of order display times.

01:03:25.890 --> 01:03:31.260
You can also ask for extra scratch space
at the bottom and right of the buffer.

01:03:31.300 --> 01:03:33.800
We rounded up to a multiple of 16 by 16.

01:03:34.030 --> 01:03:37.700
This is also a convenient time
to allocate internal buffers.

01:03:38.340 --> 01:03:41.320
In the beginBand function,
for most codecs,

01:03:41.320 --> 01:03:45.360
band is equivalent to frame,
so you could think of this as beginFrame.

01:03:45.360 --> 01:03:48.940
In the beginBand function,
it's time to classify the frame.

01:03:48.940 --> 01:03:52.340
You need to read as much of your
frame header as is necessary to

01:03:52.340 --> 01:03:57.010
classify the frame as a keyframe,
a difference frame, or a droppable frame.

01:03:57.460 --> 01:04:02.060
This is how the Image Compression
Manager is able to know which frames

01:04:02.060 --> 01:04:04.010
to drop when we are low on CPU.

01:04:08.140 --> 01:04:11.920
The DecodeBand function is new for
QuickTime 7 in order to support

01:04:12.680 --> 01:04:14.800
B-frames and frame reordering.

01:04:15.000 --> 01:04:19.790
It's called to decode a frame that
isn't the next frame to display.

01:04:20.000 --> 01:04:21.000
It's up to you.

01:04:21.000 --> 01:04:26.000
You can also choose to have it
called for all frames if you want.

01:04:27.380 --> 01:04:31.840
Finally, the drawband function is called
when it's time for you to output

01:04:31.970 --> 01:04:34.970
a frame to an output buffer.

01:04:36.980 --> 01:04:39.600
So that's that sample code.

01:04:39.600 --> 01:04:43.170
We'll return to slides and recap this.

01:04:45.670 --> 01:04:49.000
Three important functions
for the compressor:

01:04:49.090 --> 01:04:51.280
Prepare to compress frames,
which is where you describe

01:04:51.280 --> 01:04:52.500
the source pixel buffers.

01:04:52.500 --> 01:04:54.660
Encode frame,
which is called with each source

01:04:54.810 --> 01:04:56.500
frame so you can put it in your queue.

01:04:57.060 --> 01:05:00.340
Complete frame,
time to put up or shut up.

01:05:00.960 --> 01:05:03.240
Five important functions
for the decompressor:

01:05:03.310 --> 01:05:07.600
Initialize, report basic characteristics
about your codec and what it does.

01:05:07.700 --> 01:05:11.300
Preflight, where you negotiate the
output pixel format.

01:05:11.350 --> 01:05:13.900
BeginBand, where you classify the
frame as a keyframe,

01:05:13.900 --> 01:05:15.460
a droppable, whatever.

01:05:15.600 --> 01:05:18.940
DecodeBand, where you decode a frame
to an internal buffer.

01:05:18.990 --> 01:05:24.000
And DrawBand, where you write a frame to
an external pixel buffer.

01:05:25.230 --> 01:05:27.830
So it's now time for the
Thread Safety Pledge.

01:05:27.950 --> 01:05:30.160
If you would all please
raise your right hand.

01:05:32.320 --> 01:05:34.540
Repeat after me:
I promise that every new component

01:05:34.570 --> 01:05:36.930
I write shall be thread safe.

01:05:39.280 --> 01:05:40.200
It's the 21st century.

01:05:40.200 --> 01:05:42.200
It's time that we were all
writing thread-safe code.

01:05:42.760 --> 01:05:47.060
Avoid using unprotected global variables.

01:05:47.200 --> 01:05:51.200
Use the instance storage
to store your state.

01:05:51.200 --> 01:05:56.200
Avoid thread-unsafe APIs like
the old resource manager calls.

01:05:56.200 --> 01:05:58.920
Test your codec to make sure that
it's thread-safe and then ship it

01:05:59.100 --> 01:06:01.080
with the component thread-safe flag.

01:06:01.650 --> 01:06:03.200
This excludes UI.

01:06:03.200 --> 01:06:05.140
None of our UI frameworks
are actually thread-safe.

01:06:05.200 --> 01:06:09.480
We're referring--the thread-safety
flag refers to the core processing

01:06:09.480 --> 01:06:10.180
that your component does.

01:06:10.200 --> 01:06:13.440
There's a good tech note on thread-safe
programming in QuickTime that

01:06:13.440 --> 01:06:17.200
I recommend all of you have a look at,
especially if you're component authors.

01:06:17.480 --> 01:06:23.200
And another topic about components,
you are a child of the universe.

01:06:23.200 --> 01:06:26.300
And your component should be too.

01:06:27.580 --> 01:06:33.220
Now Rosetta does not include a mixed mode
the way we have with 68K and PowerPC.

01:06:33.220 --> 01:06:37.420
If you're inside a PowerPC application
you will only be able to

01:06:37.420 --> 01:06:39.180
use PowerPC components.

01:06:39.180 --> 01:06:42.000
Intel based applications will only be
able to see Intel based components.

01:06:42.000 --> 01:06:45.460
So it's important that your
component is universal and

01:06:45.460 --> 01:06:47.940
contains both kinds of code.

01:06:47.940 --> 01:06:54.040
There are some instructions
in the universal binary page.

01:06:54.040 --> 01:06:56.440
They give you a fairly
aggressive approach,

01:06:56.540 --> 01:07:01.880
a very clever way of doing things so that
you will change the resource template

01:07:01.880 --> 01:07:06.670
dynamically based on what platforms,
what architectures you are compiling

01:07:06.780 --> 01:07:08.340
for with your Xcode project.

01:07:08.360 --> 01:07:11.670
It isn't necessary to do that,
all of that macro cleverness,

01:07:11.670 --> 01:07:16.340
if you just want to always ship,
always build universal components.

01:07:16.340 --> 01:07:21.100
In that case all you have to do is put
both platform codes in the platform list.

01:07:21.120 --> 01:07:26.690
If you remember back with 68K and
PowerPC components it's just like that.

01:07:27.130 --> 01:07:30.360
So, in summary, wake up.

01:07:30.420 --> 01:07:33.000
We've looked at a whole
bunch of sample code.

01:07:33.050 --> 01:07:36.840
We've looked at the high-level
APIs for movie editing and for export.

01:07:37.000 --> 01:07:40.780
We've looked at the low-level
APIs which give you access to level

01:07:40.780 --> 01:07:44.000
sample information for decompressing
frames and compressing them.

01:07:44.040 --> 01:07:48.000
And we've shown you the new APIs and the
new example for writing your own codecs.

01:07:48.000 --> 01:07:50.840
If you use the high-level APIs,
all of that B-frame trickiness

01:07:50.890 --> 01:07:52.000
will be hidden from you.

01:07:52.150 --> 01:07:55.600
Or you can choose to use the
low-level APIs and QuickTime will

01:07:55.760 --> 01:07:57.720
expose it to you in all its glory.

01:08:00.740 --> 01:08:04.410
We have a lab where there are going
to be lots of QuickTime engineers who

01:08:04.410 --> 01:08:06.600
are waiting to solve your problems.

01:08:06.600 --> 01:08:08.940
Not just QuickTime,
but also a bunch of other

01:08:08.970 --> 01:08:10.600
graphics and media engineers.

01:08:11.030 --> 01:08:14.730
You are here, and if you go out there
and go around the corner,

01:08:14.730 --> 01:08:18.140
around the back,
you'll find the lab and it's

01:08:18.190 --> 01:08:22.090
open until some hour each day.