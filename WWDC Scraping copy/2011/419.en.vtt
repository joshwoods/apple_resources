WEBVTT

00:00:10.400 --> 00:00:16.310
Good afternoon and welcome to
Capturing from... Thank you.

00:00:18.020 --> 00:00:19.890
Yes, I can push a button.

00:00:20.140 --> 00:00:23.930
Capturing from the camera
using AV Foundation on iOS 5.

00:00:23.980 --> 00:00:27.810
What you're about to hear is not a
repeat of last year's presentation.

00:00:27.970 --> 00:00:31.910
This is all new stuff,
so it's worthwhile being here.

00:00:34.040 --> 00:00:37.040
What are you going to learn about?

00:00:37.120 --> 00:00:40.300
Which iOS 5 capture
APIs to use in your app?

00:00:40.390 --> 00:00:42.940
There are several choices.

00:00:43.000 --> 00:00:45.260
The AV Foundation capture
programming model.

00:00:45.330 --> 00:00:47.690
We'll just discuss it briefly.

00:00:48.200 --> 00:00:52.010
And we'll delve deeply into
iOS 5 performance improvements

00:00:52.040 --> 00:00:54.870
we've made in AV Foundation,
which I think you're really going

00:00:54.910 --> 00:00:58.910
to like because they're going to
equal more performance for your apps.

00:00:59.370 --> 00:01:02.660
Finally, we'll talk about some
API enhancements that are not

00:01:02.660 --> 00:01:04.610
strictly performance related.

00:01:06.140 --> 00:01:09.560
For this session,
we have four pieces of sample code.

00:01:09.610 --> 00:01:12.770
And before the session,
I went and looked and I think two,

00:01:13.080 --> 00:01:14.960
maybe three of them were
already published and there

00:01:14.960 --> 00:01:15.950
was a holdup on one of them.

00:01:15.950 --> 00:01:19.750
But if it's not up there by now,
check tomorrow and all four of

00:01:19.750 --> 00:01:21.670
those should be ready to go.

00:01:23.550 --> 00:01:26.790
First,
let's take a look at the overview of our

00:01:26.790 --> 00:01:29.530
technology framework and where we sit.

00:01:29.810 --> 00:01:32.480
You'll notice the green ones,
these refer to the frameworks

00:01:32.670 --> 00:01:35.280
that are below the UI level.

00:01:35.340 --> 00:01:38.170
AV Foundation,
the focus of our talk today,

00:01:38.450 --> 00:01:40.220
sits below that line.

00:01:40.270 --> 00:01:42.500
But at the top,
you have some other choices,

00:01:42.500 --> 00:01:46.360
namely UI kits,
UI image picker controller.

00:01:46.410 --> 00:01:50.400
Apple's Camera app is built on
that technology and the UI image

00:01:50.520 --> 00:01:54.530
picker controller itself is
built on top of AV Foundation.

00:01:57.030 --> 00:02:02.880
The UI Image Picker Controller offers
simple programmatic access to the camera.

00:02:03.640 --> 00:02:07.940
API for high, medium,
or low quality recording.

00:02:08.080 --> 00:02:09.740
Hideable Camera Controls UI.

00:02:09.740 --> 00:02:12.650
So if you're familiar with
the camera look and feel with

00:02:12.740 --> 00:02:15.450
the control bar at the bottom,
that's what you'll be getting

00:02:15.450 --> 00:02:16.280
when you use this view.

00:02:16.280 --> 00:02:19.220
And if all you need is a
simple view controller,

00:02:19.220 --> 00:02:23.830
this is the API for you to use because
you get a lot of bang for your buck.

00:02:23.990 --> 00:02:27.740
You can also hide that Camera Controls
UI if you want to build your own

00:02:27.740 --> 00:02:32.880
buttons and do simple things like
take a picture or start video capture,

00:02:32.880 --> 00:02:35.790
which you get programmatic access to.

00:02:36.030 --> 00:02:39.500
Or if you want to set the camera device,
that is switch cameras

00:02:39.500 --> 00:02:42.700
between back and front,
or control whether the

00:02:42.700 --> 00:02:44.410
flash should fire or not.

00:02:44.840 --> 00:02:50.420
You can also touch on the view that's
managed by the UI image picker controller

00:02:50.990 --> 00:02:54.600
so that you get the touch to focus and
touch to expose behavior that Camera app

00:02:54.700 --> 00:02:56.640
does and you get all of that for free.

00:02:56.790 --> 00:03:00.000
Also, new in iOS 5,
if you touch and hold on that

00:03:00.000 --> 00:03:06.010
UI image picker controller view,
it will lock and hold AE or AF.

00:03:08.150 --> 00:03:11.630
So given that there's all of that
power available for very little code,

00:03:11.640 --> 00:03:14.740
why might you want to use
AV Foundation for capture?

00:03:14.790 --> 00:03:17.440
Again, the lower level framework.

00:03:17.920 --> 00:03:22.520
If you need full access to the camera,
you'll want to use AV Foundation because

00:03:22.540 --> 00:03:25.610
of the additional features
that it provides you.

00:03:26.060 --> 00:03:31.440
You can independently control focus,
exposure, and white balance controls,

00:03:31.520 --> 00:03:36.010
independently lock any of those things,
and select independent points of

00:03:36.010 --> 00:03:37.910
interest for focus or exposure.

00:03:38.040 --> 00:03:41.610
So for instance,
if you want to expose on one section

00:03:41.610 --> 00:03:45.560
of the frame while focusing on another,
you have the power to do that,

00:03:45.560 --> 00:03:48.140
whereas in the UI image
picker controller,

00:03:48.140 --> 00:03:51.980
focus and exposure are always
tied to the same point.

00:03:52.510 --> 00:03:56.240
You also get access to video
frame data in your process.

00:03:56.370 --> 00:04:00.500
These are video frames with accurate
timestamps telling you exactly

00:04:00.630 --> 00:04:03.150
when the frames came in on the bus.

00:04:03.640 --> 00:04:06.570
uses the Perframe metadata
to tell you things about the

00:04:06.650 --> 00:04:11.790
images that are captured,
like their exposure levels, for instance.

00:04:12.050 --> 00:04:15.900
And you can configure the output
format so that instead of getting

00:04:15.900 --> 00:04:20.890
just the native pixel format,
you can get a different pixel format

00:04:20.970 --> 00:04:24.630
that works better for your app,
such as BGRA.

00:04:25.060 --> 00:04:27.240
You also can configure the
frame rate of the camera.

00:04:27.240 --> 00:04:31.430
So if you don't need full frame rate,
you have some control over how

00:04:31.430 --> 00:04:35.030
many frames per second you expect
to get in your application.

00:04:35.120 --> 00:04:39.040
And finally, when getting access to video
frame data in your app,

00:04:39.120 --> 00:04:43.660
you can configure the resolution so
that if you don't need full 720p,

00:04:43.660 --> 00:04:47.660
you can get a lower frame,
a lower resolution that works

00:04:47.770 --> 00:04:49.700
well for your processing.

00:04:50.390 --> 00:04:55.890
Also, AV Foundation provides
some flexible output.

00:04:56.000 --> 00:05:00.150
When capturing still images,
you don't have to capture to JPEG.

00:05:00.450 --> 00:05:05.410
You can capture to uncompressed
formats like YUV and RGB and

00:05:05.410 --> 00:05:10.520
insert your own EXIF metadata
before writing to the camera roll.

00:05:10.820 --> 00:05:14.070
When doing QuickTime movie recording,
you have some additional powerful

00:05:14.070 --> 00:05:18.620
features like inserting metadata into
the movie or locking the orientation.

00:05:18.620 --> 00:05:21.140
So for instance,
if you want your app to be

00:05:21.240 --> 00:05:24.780
landscape only and enforce that
you only take landscape movies,

00:05:24.780 --> 00:05:27.310
you can set that orientation lock.

00:05:27.360 --> 00:05:30.450
Also,
you get a layer-based video preview,

00:05:30.650 --> 00:05:35.400
which you can insert into a core
animation layer tree and get all of the

00:05:35.410 --> 00:05:40.660
benefits that core animation provides
in its implicit and explicit animation.

00:05:40.700 --> 00:05:45.260
Also, you get some control with that
layer about how it displays

00:05:45.260 --> 00:05:49.460
within its layer bounds,
whether it fits, fills, or stretches.

00:05:51.340 --> 00:05:55.800
Finally, some ancillary properties
like audio level metering.

00:05:55.830 --> 00:05:59.490
And of course,
access to audio sample data.

00:06:00.170 --> 00:06:04.600
So let's take a high look at the
AV Foundation capture programming model.

00:06:04.640 --> 00:06:07.900
We view the world as inputs and outputs.

00:06:08.360 --> 00:06:13.230
If you've got a phone,
it's got a camera on it, maybe two,

00:06:13.230 --> 00:06:16.550
and from those cameras you might want to
do several things like see a real-time

00:06:16.550 --> 00:06:22.120
preview of what the camera is seeing
and have it delivered to you as a core

00:06:22.120 --> 00:06:28.130
animation layer or get high-quality
still images from the camera.

00:06:28.450 --> 00:06:32.250
Or as we were just talking
about getting video data,

00:06:32.320 --> 00:06:35.660
per frame video data
into your application.

00:06:35.850 --> 00:06:39.170
Or perhaps simply write
to a QuickTime movie.

00:06:39.530 --> 00:06:42.520
Likewise,
all of our iOS 5 supported devices

00:06:43.010 --> 00:06:47.520
have built-in microphones from
which you might want to process the

00:06:47.520 --> 00:06:51.560
audio data in real time or write
the output to a QuickTime movie

00:06:51.920 --> 00:06:54.500
along with the video or separately.

00:06:55.950 --> 00:07:00.280
This is what the world
looks like to AV Capture.

00:07:00.310 --> 00:07:03.780
You have AV Capture
inputs at the top level.

00:07:03.840 --> 00:07:06.300
They're the ones that provide the data.

00:07:06.340 --> 00:07:08.640
You have an AV Capture
session in the middle.

00:07:08.840 --> 00:07:15.760
The session is the guy that gets inputs
added to it and outputs added to it.

00:07:15.810 --> 00:07:19.300
And it controls the flow of data
such that even if you've added

00:07:19.300 --> 00:07:23.240
inputs and outputs to the session,
nobody delivers data or consumes data

00:07:23.290 --> 00:07:25.700
until you tell it to start running.

00:07:25.730 --> 00:07:27.890
AV capture session is also
the place that tells you if

00:07:27.980 --> 00:07:30.390
there were any runtime errors.

00:07:31.300 --> 00:07:34.580
On the output side,
we have some concrete subclasses of

00:07:34.580 --> 00:07:37.440
the AV Capture Output superclass.

00:07:37.620 --> 00:07:44.240
Still image output, video data output,
audio data output, and movie file output.

00:07:44.350 --> 00:07:46.640
The video preview is a
little bit different.

00:07:46.710 --> 00:07:51.280
I drew it in orange there to highlight
that it does not descend from AV Capture

00:07:51.280 --> 00:07:53.300
Output like the rest of the classes do.

00:07:53.480 --> 00:07:56.990
It is a subclass of CA Layer,
so it can be inserted into

00:07:57.090 --> 00:07:58.960
a Core Animation Layer tree.

00:07:59.060 --> 00:08:03.920
Also, the ownership model is such that
the session owns its outputs,

00:08:04.110 --> 00:08:05.700
but it does not own its layer.

00:08:05.940 --> 00:08:07.360
The layer owns it.

00:08:07.360 --> 00:08:11.590
So if you want to have a layer that
you insert into a view hierarchy,

00:08:11.660 --> 00:08:14.400
you can attach a session
to it and forget about it.

00:08:14.550 --> 00:08:17.820
And when the layer tree
disposes of itself,

00:08:17.990 --> 00:08:21.050
it will clean up the session as well.

00:08:22.050 --> 00:08:26.740
We're going to cover four
capture use cases today.

00:08:26.800 --> 00:08:28.590
And we'll take these one by one.

00:08:28.910 --> 00:08:31.930
The first of which is processing
video frames from the camera

00:08:32.030 --> 00:08:33.760
and rendering with OpenGL.

00:08:33.890 --> 00:08:36.150
I hope you like augmented
reality apps because we're

00:08:36.150 --> 00:08:38.510
going to do a lot of that today.

00:08:39.140 --> 00:08:44.560
So let me call up Sylvain Neuse who's
going to help me with a chroma key demo.

00:08:56.740 --> 00:09:02.700
And Sylvain is going to
find me on the screen.

00:09:02.770 --> 00:09:09.420
And he's going to touch on the
interface to sample a color.

00:09:09.490 --> 00:09:13.580
And when he samples that color,
he's also going to expose and do

00:09:13.670 --> 00:09:16.660
a white balance and then lock.

00:09:16.800 --> 00:09:19.580
So that he's now locking--
there's no focus on the iPad 2,

00:09:19.580 --> 00:09:21.800
but if there were focus,
it would lock focus as well.

00:09:21.960 --> 00:09:24.840
Locking exposure, white balance,
and focus.

00:09:24.890 --> 00:09:28.600
And then he goes and selects a
background image and uses what

00:09:28.600 --> 00:09:33.260
he just found as a background
substitution to do a chroma key.

00:09:33.500 --> 00:09:36.920
And using this, he's able to substitute
a background image.

00:09:36.920 --> 00:09:38.540
And you'll see the buttons at the bottom.

00:09:38.540 --> 00:09:42.130
He can snap a picture or record a movie.

00:09:42.320 --> 00:09:45.460
But let's also take a look
at the options on the left.

00:09:45.540 --> 00:09:46.890
He's got a fast button.

00:09:47.180 --> 00:09:49.440
Don't you wish your
app had a fast button?

00:09:49.490 --> 00:09:51.930
Let's turn fast off.

00:09:52.960 --> 00:09:54.870
Okay, so when Fast is off,
let's look at the frame

00:09:54.870 --> 00:09:56.130
rate up at the top.

00:09:56.260 --> 00:09:59.900
His FIPS is hovering around 20 right now.

00:10:00.020 --> 00:10:03.740
He's doing 720p and he's doing
a lot of heavy work in OpenGL.

00:10:03.740 --> 00:10:07.290
He has a shader running that's
doing the background substitution.

00:10:07.290 --> 00:10:11.130
Okay, let's turn the Fast button back on.

00:10:12.010 --> 00:10:13.900
And now we go back up to 25.

00:10:13.900 --> 00:10:15.500
We're in 25 because we're in low light.

00:10:15.500 --> 00:10:18.460
It actually goes up to 30 if we
had a little more light since we're

00:10:18.460 --> 00:10:23.140
letting the device throttle down to a
reasonable frame rate for low light.

00:10:23.310 --> 00:10:28.210
So he's able to get 30 frames per
second capture with no dropped

00:10:28.210 --> 00:10:33.380
frames doing some pretty heavy
OpenGL work due to the fast button.

00:10:33.380 --> 00:10:38.140
And the fast button is all about
improvements that we've made in iOS 5.

00:10:38.370 --> 00:10:41.010
Thank you, Sylvain.

00:10:41.100 --> 00:10:45.300
All right, so let's talk about
what we just did there.

00:10:45.370 --> 00:10:49.140
We started with an iPad
and used the back camera.

00:10:50.440 --> 00:10:55.880
and captured video frames using that
video data output because we wanted

00:10:55.880 --> 00:10:57.740
the frames into the client process.

00:10:57.740 --> 00:11:03.780
And then everything after that point
happened in custom client code,

00:11:03.940 --> 00:11:07.610
processing and rendering with OpenGL,
including the preview that you saw.

00:11:07.620 --> 00:11:11.710
AV Foundation was not
handling that preview at all.

00:11:12.030 --> 00:11:15.170
So our involvement as far as
AV Foundation is concerned,

00:11:15.170 --> 00:11:18.590
we had a device and an input
and a session and a video

00:11:18.590 --> 00:11:21.500
data output delivering sample
buffers and that was it.

00:11:21.540 --> 00:11:23.550
Then we got out of the way.

00:11:24.680 --> 00:11:28.660
To create an AV capture session,
you simply allocate an edit and

00:11:28.660 --> 00:11:33.210
set a session preset to determine
the baseline quality that will

00:11:33.220 --> 00:11:34.870
be delivered to your outputs.

00:11:35.040 --> 00:11:38.020
For this demo,
Sylvain chose 1280 by 720 so we could

00:11:38.080 --> 00:11:41.760
highlight the kinds of performance that
we're getting with these improvements

00:11:41.760 --> 00:11:45.200
in iOS 5 at a very high resolution.

00:11:45.520 --> 00:11:48.000
He then found a suitable
AV capture device.

00:11:48.120 --> 00:11:49.150
He chose the default.

00:11:49.320 --> 00:11:54.090
The default device on all iOS
devices is the back camera.

00:11:54.930 --> 00:11:58.100
He then created and
added the device input.

00:11:58.220 --> 00:12:04.200
He associated a device input with the
device and then called session add input.

00:12:05.160 --> 00:12:06.910
Then he was done with the input side.

00:12:07.170 --> 00:12:10.900
For the video output side,
he alloc'd and annited a

00:12:10.900 --> 00:12:15.520
AV Capture video data output,
added it to the session.

00:12:15.750 --> 00:12:17.730
and then performed some
minimal configuration

00:12:18.270 --> 00:12:20.040
before starting the session.

00:12:20.220 --> 00:12:25.880
He set the output format to BGRA since
that's a format that OpenGL likes.

00:12:26.030 --> 00:12:29.340
And then set a delegate
on the video data output.

00:12:29.500 --> 00:12:33.080
Our AV Capture data outputs
have fancy delegates.

00:12:33.160 --> 00:12:36.940
So it's not just a delegate callback,
it's a delegate with a queue.

00:12:37.210 --> 00:12:40.200
So you call
setSampleBufferDelegate queue.

00:12:40.360 --> 00:12:44.420
And with that queue, you're telling it,
when you call me back with video frames,

00:12:44.570 --> 00:12:47.640
I want them to be delivered
on this dispatch queue.

00:12:47.760 --> 00:12:51.740
So you have some control over where
and how these frames come to you

00:12:51.740 --> 00:12:56.040
by specifying the queue on which
they are delivered to your process.

00:12:56.170 --> 00:13:01.030
Then he called sessionStartRunning
and he was up and going.

00:13:01.600 --> 00:13:05.070
Inside the delegate callback,
for each video frame,

00:13:05.070 --> 00:13:08.620
you're delivered a capture output,
did output sample buffer from

00:13:09.010 --> 00:13:12.700
connection call in which you're free
to do whatever processing you need.

00:13:14.370 --> 00:13:17.410
I also mentioned that when
he touched on the screen,

00:13:17.410 --> 00:13:20.340
he was sampling a pixel
and before he did that,

00:13:20.340 --> 00:13:25.010
he was performing an auto
exposure and then locking.

00:13:25.180 --> 00:13:28.980
To lock auto exposure,
you first need to lock the device

00:13:28.980 --> 00:13:32.980
for configuration and then you
can set any of the properties on

00:13:32.990 --> 00:13:37.300
the device such as focus mode,
exposure mode, white balance mode.

00:13:37.450 --> 00:13:40.170
And when you're done,
don't forget to unlock the

00:13:40.170 --> 00:13:42.010
device for configuration.

00:13:42.940 --> 00:13:46.930
Now to talk about the Fast Button and
what exactly was going on there and

00:13:46.930 --> 00:13:51.540
the great improvements we've made with
bridging between Core Video and OpenGL,

00:13:51.540 --> 00:13:54.890
I'd like to invite up
Brandon Corey to talk about that.

00:13:58.300 --> 00:14:00.030
So I'm here to talk
about the fast button.

00:14:00.060 --> 00:14:07.400
To make things faster,
to allow people to interact with

00:14:07.400 --> 00:14:12.010
the GPU better for capture and
for all of our AV Foundation APIs,

00:14:12.210 --> 00:14:17.110
we added a new API called
CVOpenGLES Texture Cache.

00:14:17.870 --> 00:14:23.810
Now, the intent of this was to provide
a CF type that allows clients to

00:14:23.810 --> 00:14:28.350
bridge Core Video Pixel Buffers
to OpenGL ES textures.

00:14:28.620 --> 00:14:32.420
And the idea is to avoid
copies to and from the GPU.

00:14:32.420 --> 00:14:36.310
So for something like that 720p video,
if you were doing BGRA and you wanted

00:14:36.310 --> 00:14:38.740
to get that data in and out of the GPU,
you know,

00:14:38.900 --> 00:14:43.180
you're talking in the neighborhood of 220
megabytes a second of copying and not to

00:14:43.230 --> 00:14:45.260
mention twiddling data back and forth.

00:14:45.300 --> 00:14:47.580
And, you know,
it's a significant overhead that we

00:14:47.590 --> 00:14:49.220
like to avoid if at all possible.

00:14:49.770 --> 00:14:53.560
The other thing we like to do is
allow you to recycle textures so

00:14:53.560 --> 00:14:58.030
that GL doesn't have to spend so
much time maintaining its state every

00:14:58.070 --> 00:15:00.750
time you create and use new textures.

00:15:01.160 --> 00:15:05.620
I wanted to point out that this is only
supported in OpenGL ES 2.0 and later,

00:15:05.680 --> 00:15:07.570
so you can use it with
all your fun shaders,

00:15:07.600 --> 00:15:11.850
but you will have to use
2.0 and not the older ES,

00:15:11.860 --> 00:15:13.580
which is kind of a compatibility mode.

00:15:13.580 --> 00:15:17.870
And all the API is contained
in the header above there,

00:15:17.870 --> 00:15:21.440
the Core Video CV OpenGL ES Texture
Cache.

00:15:24.080 --> 00:15:27.900
So this is what the
API itself looks like.

00:15:27.900 --> 00:15:33.070
This is the main API that you'll
call once you've created the texture

00:15:33.070 --> 00:15:37.900
cache to bind an actual pixel
buffer to an OpenGL ES texture.

00:15:37.900 --> 00:15:42.750
So to kind of go through this,
we have an OpenGL ES texture cache ref,

00:15:42.810 --> 00:15:46.230
which is one you'll
have previously created.

00:15:46.600 --> 00:15:49.530
Then we have here a pixel buffer,
which is the one you would be

00:15:49.540 --> 00:15:55.770
getting from your output delegate
callback in the capture case.

00:15:56.300 --> 00:17:24.900
[Transcript missing]

00:17:25.770 --> 00:17:28.590
So kind of an overview here,
the standard texture binding

00:17:28.650 --> 00:17:31.940
that you would use with OpenGL.

00:17:31.990 --> 00:17:34.840
You have a CV pixel buffer here.

00:17:35.520 --> 00:17:40.980
And OpenGL contains these textures,
texture objects which are backed

00:17:41.160 --> 00:17:46.500
by texture images that you would
get if you use GL Gen Textures.

00:17:46.770 --> 00:17:50.940
So normally to upload data to a texture,
you'd create your texture.

00:17:50.950 --> 00:17:53.380
You would get the base address
from your pixel buffer.

00:17:53.400 --> 00:17:55.350
And notice that's a void star there.

00:17:55.460 --> 00:17:57.740
It's essentially raw
data that you're getting.

00:17:57.750 --> 00:18:01.930
And you're calling the GL text image
2D with your appropriate parameters.

00:18:02.070 --> 00:18:05.560
And at the end there,
the actual pixel data itself.

00:18:05.560 --> 00:18:09.170
And what happens here is we do
the equivalent of a mem copy with

00:18:09.170 --> 00:18:12.950
some twiddling and such to get
this data into the texture image,

00:18:12.950 --> 00:18:16.460
which OpenGL ES was sourced from.

00:18:16.480 --> 00:18:20.290
Now for the CVOpenGL ES texture binding,

00:18:21.190 --> 00:18:26.090
The idea here is you have the
equivalent extremely long function name,

00:18:26.210 --> 00:18:29.070
CBOpenGLESTextureCacheCreateTextureFromIm
age,

00:18:29.070 --> 00:18:31.740
with all the very
similar parameters there.

00:18:31.740 --> 00:18:35.780
But you'll notice here that we actually
pass in the pixel buffer itself and

00:18:35.920 --> 00:18:37.760
not the pointer to the raw data.

00:18:37.760 --> 00:18:43.250
And we're kind of moving this model so
when you call in with the pixel buffer,

00:18:43.260 --> 00:18:48.400
you'll end up getting back a texture
that you would use in PointOpenGLES at.

00:18:49.160 --> 00:18:53.320
And what this does is it allows us to
avoid this copy by binding that CV pixel

00:18:53.320 --> 00:18:55.510
buffer directly into the texture object.

00:18:59.180 --> 00:19:01.700
So to get,
that's all fine if you just want to get

00:19:01.750 --> 00:19:05.610
your data to the screen and you just
want to do some processing before there,

00:19:05.610 --> 00:19:07.170
but what if you want to
get your data back out?

00:19:07.180 --> 00:19:11.260
Now normally with OpenGL,
for those who aren't familiar with this,

00:19:11.260 --> 00:19:15.170
we use what's called a frame
buffer object to encapsulate

00:19:15.170 --> 00:19:16.940
what OpenGL draws into.

00:19:17.040 --> 00:19:19.630
Now normally your frame buffer
object can be your screen,

00:19:19.630 --> 00:19:23.010
but if you want to get the data back out,
you can do this in one of two ways.

00:19:23.200 --> 00:19:25.590
You can create and
attach a render buffer,

00:19:25.590 --> 00:19:29.950
which is essentially a data-backed
buffer that GL can write into,

00:19:29.950 --> 00:19:33.410
or you can create and attach a
texture to that frame buffer object.

00:19:33.470 --> 00:19:36.930
And that's usually used for something
like if you wanted to feed that back

00:19:36.990 --> 00:19:38.850
in for another pass of rendering.

00:19:38.860 --> 00:19:42.050
And to get the data back out,
you would then use GL read pixels

00:19:42.050 --> 00:19:46.580
in the case of a render buffer or
GL text image 2D if you wanted to move

00:19:46.630 --> 00:19:49.080
the data from one texture to another.

00:19:51.590 --> 00:19:54.250
And these all are attached with
this GL color attachment as

00:19:54.310 --> 00:19:55.490
part of the frame buffer object.

00:19:55.500 --> 00:19:59.400
Note, the diagram there shows multiple
color attachments and our

00:19:59.400 --> 00:20:02.680
implementation supports the first,
but you really only

00:20:02.740 --> 00:20:04.290
need one in this case.

00:20:05.070 --> 00:20:08.840
So normally here you have the
OpenGL ES render and you'd

00:20:08.840 --> 00:20:12.480
have the equivalent of a
texture render buffer object.

00:20:13.430 --> 00:20:18.310
And what you would do is you would render
into this texture and then you would

00:20:18.310 --> 00:20:21.970
use GL read pixels to pull the data back
out of this texture into your buffer.

00:20:22.090 --> 00:20:24.550
But again,
that's going to incur the equivalent

00:20:24.550 --> 00:20:27.980
or worse than a mem copy because
the data will have to be pulled

00:20:27.980 --> 00:20:29.610
back out and also twizzled.

00:20:29.710 --> 00:20:32.230
And in this case,
you can have OpenGL rendered

00:20:32.270 --> 00:20:36.380
directly into your iOS
Surface-backed pixel buffer.

00:20:36.780 --> 00:20:40.100
And you can completely avoid
that GL read pixels call.

00:20:40.140 --> 00:20:43.900
And this is a similar binding to attach
a texture to a frame buffer object where

00:20:43.900 --> 00:20:48.700
you would create the image in the very
same way you would for the input case.

00:20:48.790 --> 00:20:51.630
But when you get that texture back,
you can bind it to the frame

00:20:51.680 --> 00:20:55.000
buffer and attach it to the
actual frame buffer object here.

00:20:55.090 --> 00:20:58.320
And it would be the same or very
similar to if you had GL render

00:20:58.320 --> 00:21:00.180
into a texture that it created.

00:21:00.310 --> 00:21:04.470
But in this case,
it's rendering directly into your buffer.

00:21:06.290 --> 00:21:09.400
So a couple usage notes here.

00:21:09.450 --> 00:21:12.830
We kind of have a special
buffer type to handle this.

00:21:13.090 --> 00:21:16.380
And all the buffers from AV Capture
and AV Asset Reader are created

00:21:16.380 --> 00:21:17.640
with the appropriate formatting.

00:21:17.670 --> 00:21:20.770
So if you use either of those,
you don't really need to do anything.

00:21:20.900 --> 00:21:25.120
If you create your own pools, however,
you must pass this

00:21:25.120 --> 00:21:30.120
KCVPixelBufferIOSurfaceProperties key
as a pixel buffer attribute when you

00:21:30.200 --> 00:21:31.750
create your buffer or your buffer pool.

00:21:31.760 --> 00:21:35.240
And that'll make sure that everything's
formatted correctly so we can apply

00:21:35.280 --> 00:21:41.080
an efficient attachment and get
that fast speed that you'd like.

00:21:41.080 --> 00:21:44.840
And if you use AV Asset Reader,
be sure to use the pixel

00:21:44.840 --> 00:21:46.930
buffer pool it provides.

00:21:47.040 --> 00:21:51.620
Not only will it give you this particular
backing that you need in this case,

00:21:51.620 --> 00:21:54.400
but on top of which,
there could be other alignment issues

00:21:54.400 --> 00:21:56.020
and such that the encoder might need.

00:21:56.060 --> 00:21:59.760
So it's always a good idea to
use that whenever possible.

00:22:00.860 --> 00:22:06.080
As I mentioned earlier, BGRA, 420V,
and 420F are supported.

00:22:06.080 --> 00:22:10.570
And OpenGLS also now
supports GLRED and GLRG,

00:22:10.570 --> 00:22:12.210
which are the single-channel
render targets.

00:22:12.220 --> 00:22:18.290
So instead of rendering into BGRA,
you can render using GLRED and GLRG.

00:22:18.300 --> 00:22:21.010
And I just wanted to mention that
those are only available on the iPad,

00:22:21.010 --> 00:22:23.280
too,
but they're definitely cool to play with.

00:22:23.300 --> 00:22:25.450
And with that,
I'd like to turn it back over to Brad.

00:22:29.480 --> 00:22:31.310
Thank you.

00:22:34.770 --> 00:22:38.640
So if you want automatic
10 to 15 FIPS more in your

00:22:38.640 --> 00:22:43.340
process and you're using GPU,
you might want to check that API out.

00:22:43.830 --> 00:22:50.090
AV Capture Video Data Output has some
peculiarities that I'd like to address.

00:22:50.180 --> 00:22:53.780
That sample buffer delegate queue
that I talked about earlier,

00:22:53.850 --> 00:22:55.800
it needs to be a serial dispatch queue.

00:22:55.800 --> 00:22:57.500
If you're familiar with GCD,
you know that there are

00:22:57.500 --> 00:22:58.460
two kinds of queues.

00:22:58.610 --> 00:23:01.660
There's concurrent
queues and serial queues.

00:23:01.950 --> 00:23:05.660
Well, video frames are coming in
in a particular order and you

00:23:05.660 --> 00:23:08.450
want them delivered to your
callback in that same order.

00:23:08.510 --> 00:23:11.050
If you use a concurrent queue,
they might be delivered to you out

00:23:11.060 --> 00:23:13.800
of order and that would be hilarious.

00:23:14.000 --> 00:23:18.510
So make sure that you use
a serial dispatch queue

00:23:18.670 --> 00:23:23.150
and do not ever pass dispatch get current
queue because you may think you're on

00:23:23.150 --> 00:23:25.510
a serial queue but you don't really
know if you use get current queue.

00:23:25.520 --> 00:23:29.000
So always use a known
queue by making your own,

00:23:29.000 --> 00:23:32.880
which is of the serial type,
or use dispatch get main queue,

00:23:32.880 --> 00:23:34.120
which is always a serial queue.

00:23:34.120 --> 00:23:37.030
By default,
we're going to give the buffers to

00:23:37.040 --> 00:23:42.220
you in the camera's most efficient
format because we want you to get the

00:23:42.220 --> 00:23:48.160
most efficient pipeline possible and
use the least amount of CPU possible.

00:23:49.880 --> 00:23:54.290
But if you need it in a different format,
such as BGRA, which is not the default,

00:23:54.400 --> 00:23:59.600
you can use the video settings
property to override that behavior.

00:23:59.660 --> 00:24:04.580
Both Core Graphics and
OpenGL work very well with BGRA.

00:24:05.510 --> 00:24:07.310
So here are some performance tips.

00:24:07.430 --> 00:24:10.810
You can also set the min frame
duration property if you want

00:24:10.930 --> 00:24:12.200
to cap the max frame rate.

00:24:12.470 --> 00:24:14.200
Probably scratching your heads,
that sounds backwards.

00:24:14.270 --> 00:24:18.180
But to get a frame rate,
you need to talk about

00:24:18.180 --> 00:24:19.920
the one over the duration.

00:24:19.990 --> 00:24:23.480
And in our APIs,
we refer to things in terms of durations,

00:24:23.490 --> 00:24:24.800
not rates.

00:24:24.970 --> 00:24:28.280
So if you want a max frame
rate of 15 frames a second,

00:24:28.320 --> 00:24:33.110
you need to tell us to set the
min frame duration to one over 15.

00:24:33.740 --> 00:24:37.130
You configure the session to output
the lowest practical resolution.

00:24:37.150 --> 00:24:41.600
So if you only need 320
by 240 or something small,

00:24:41.600 --> 00:24:46.410
don't set it to session preset high
because you're going to be getting

00:24:46.410 --> 00:24:47.880
way more pixels than you can process.

00:24:47.880 --> 00:24:52.330
Also, there's a property called always
discards late video frames.

00:24:52.330 --> 00:24:56.570
Set this to yes, which is the default,
if you want us to efficiently drop

00:24:56.640 --> 00:25:00.780
late video frames before they're
even delivered to your process.

00:25:01.020 --> 00:25:04.910
So if all you're doing is say preview,
you definitely want to set this to

00:25:04.940 --> 00:25:09.000
yes because we won't bother trying
to message frames to you into your

00:25:09.000 --> 00:25:11.480
process that you're too late for anyway.

00:25:11.480 --> 00:25:15.390
You might want to set it to no, however,
if you're trying to write a movie

00:25:15.400 --> 00:25:17.260
file and late frames are okay.

00:25:19.200 --> 00:25:21.870
Your sample buffer delegate
callback must be fast.

00:25:21.900 --> 00:25:24.550
That's the main thing.

00:25:24.750 --> 00:25:29.360
Be as efficient as you can in your
callback and don't waste time.

00:25:29.520 --> 00:25:33.430
Let's talk about the supported
pixel formats for video data output.

00:25:33.670 --> 00:25:36.360
The default is 420V.

00:25:36.360 --> 00:25:38.600
If you're not familiar with this format,
it is a planar,

00:25:38.650 --> 00:25:41.540
a bi-planar YUV 420 video format.

00:25:41.540 --> 00:25:44.500
So that means the luma and the
chroma are in separate planes,

00:25:44.600 --> 00:25:47.320
separate sections of the buffer.

00:25:47.400 --> 00:25:52.470
And the chroma is subsampled in both
the horizontal and vertical direction.

00:25:52.930 --> 00:25:56.530
The V is for video range,
which means the samples that you get

00:25:56.610 --> 00:26:00.010
are clamped to the range of 16 to 235.

00:26:00.140 --> 00:26:05.150
This is common in video processing
so that you have some leeway in

00:26:05.150 --> 00:26:09.980
the 0 to 15 area for super black,
super white.

00:26:10.950 --> 00:26:15.800
And this is the default format
on all iOS 5 supported cameras.

00:26:15.900 --> 00:26:19.800
We also have a variant
of 4.2.0 called 4.2.0f,

00:26:19.950 --> 00:26:23.560
which is just like 4.2.0v
except it has an f.

00:26:23.730 --> 00:26:26.000
And the f is for full range color.

00:26:26.240 --> 00:26:29.400
That is,
the full range of 0 to 255 is used.

00:26:29.520 --> 00:26:33.470
And this is our default when you're
using the photo preset because when

00:26:33.470 --> 00:26:38.190
you're doing still image capture,
you want full range and not video range.

00:26:38.920 --> 00:26:44.780
We also support BGRA, which is a blue,
green, red alpha format,

00:26:44.780 --> 00:26:48.300
and it's 8 bits per pixel,
which is more than twice the

00:26:48.300 --> 00:26:50.900
bandwidth of the YUV flavors.

00:26:51.070 --> 00:26:52.490
So it does come at a cost.

00:26:52.630 --> 00:26:57.200
If you can avoid using
BGRA and do your work in YUV,

00:26:57.450 --> 00:27:01.060
it's more efficient from
a bandwidth standpoint.

00:27:01.480 --> 00:27:05.670
Now let's talk about some specific iOS
5 enhancements on top of the core video

00:27:05.670 --> 00:27:08.570
bridging that Brandon already mentioned.

00:27:08.770 --> 00:27:13.540
In iOS 5,
we support rotation for CVPixel buffers.

00:27:13.710 --> 00:27:17.010
Rotation is hardware accelerated.

00:27:17.470 --> 00:27:21.940
And you can use AV Capture Connection's
set video orientation property.

00:27:21.940 --> 00:27:25.290
So for instance, by default,
you're going to get

00:27:25.460 --> 00:27:28.410
landscape-oriented video frames,
but if you need to pass them

00:27:28.520 --> 00:27:31.220
off to a library that needs
them portrait-oriented,

00:27:31.370 --> 00:27:35.090
we can do that work for
you using the hardware.

00:27:35.260 --> 00:27:37.960
All four video capture
orientations are supported:

00:27:37.980 --> 00:27:41.920
Portrait, Upside Down, Landscape Right,
and Landscape Left,

00:27:41.920 --> 00:27:46.200
but we do not support arbitrary
rotation to weird angles.

00:27:46.530 --> 00:27:48.900
The default is still non-rotated buffers.

00:27:48.960 --> 00:27:52.580
So for the front camera, by default,
they're gonna be landscape left,

00:27:52.600 --> 00:27:55.450
and for the back camera, landscape right.

00:27:56.820 --> 00:28:00.960
Also new in iOS 5 is support for
pinning of minimum frame rate.

00:28:01.230 --> 00:28:05.700
Up to now, we've only let you specify
the max to throttle.

00:28:05.760 --> 00:28:09.460
But by giving you access to
minimum frame rate pinning,

00:28:09.540 --> 00:28:13.970
you can now create a fixed frame
rate capture if you need to.

00:28:14.400 --> 00:28:16.690
You can do this using
AV Capture Connection's

00:28:17.020 --> 00:28:19.060
setMaxFrameDuration property.

00:28:19.340 --> 00:28:22.620
Again, we express it using duration,
not frame rate.

00:28:22.810 --> 00:28:26.780
So to cap the min frame rate,
you set the max frame duration

00:28:26.780 --> 00:28:29.270
to one over what you want.

00:28:29.890 --> 00:28:32.830
But here's a caveat:
fixed frame rate captures can result

00:28:32.920 --> 00:28:35.300
in reduced image quality in low light.

00:28:35.380 --> 00:28:37.570
By default,
we like to throttle down the camera

00:28:37.570 --> 00:28:41.040
in low light to get longer exposures
and better looking pictures.

00:28:41.190 --> 00:28:44.200
If you pin the frame rate
to a high frame rate,

00:28:44.250 --> 00:28:47.390
you might get worse looking
pictures in low light.

00:28:48.590 --> 00:28:54.600
Also, you can use the AV Capture Sessions
session preset property to affect the

00:28:54.600 --> 00:28:57.700
resolution of the video data output.

00:28:58.290 --> 00:29:02.600
Here are the currently
supported session presets.

00:29:02.660 --> 00:29:07.870
All but the top three should be familiar
to you if you've used this API before.

00:29:07.980 --> 00:29:12.080
High, medium, low, and then some named
presets and photo mode,

00:29:12.160 --> 00:29:13.780
all which have their purposes.

00:29:13.780 --> 00:29:18.280
And then those three at the
bottom are new in iOS 5.

00:29:18.330 --> 00:29:22.300
Now we allow on all of
our devices 352 by 288.

00:29:22.390 --> 00:29:24.840
So if you have a streaming
application and you only need

00:29:24.870 --> 00:29:29.320
SIF quality video frames,
we can deliver them in that resolution.

00:29:29.500 --> 00:29:32.400
And also I'll be talking a
little bit more about iframe,

00:29:32.410 --> 00:29:37.290
which is an important format for
Apple and an interesting format for

00:29:37.290 --> 00:29:41.880
you if you plan to be doing a lot of
editing with your captured movies.

00:29:43.530 --> 00:29:46.500
Here is the grid of
supported resolutions.

00:29:46.510 --> 00:29:50.370
You'll note that high or the high
quality preset means different

00:29:50.370 --> 00:29:51.500
things on different devices.

00:29:51.500 --> 00:29:55.590
If you have an iPhone 3GS,
it only supports up to SD video,

00:29:55.590 --> 00:29:57.950
so the highest quality
it gives you is VGA.

00:29:58.540 --> 00:30:04.500
Whereas the back cameras on iPhone 4 and
iPad 2 and iPod Touch can give you 720p.

00:30:04.810 --> 00:30:08.500
Medium and low mean the
same thing on all platforms.

00:30:08.500 --> 00:30:13.180
And then I'd like to call out a
peculiarity here at the bottom for photo.

00:30:13.570 --> 00:30:18.750
Now when you use the photo preset
and you capture still images from it,

00:30:18.880 --> 00:30:21.500
you get the full resolution.

00:30:21.500 --> 00:30:25.460
5 megapixels or 2 megapixels
or whatever that happens to be.

00:30:25.600 --> 00:30:28.150
But if you're using the photo
preset with video data output,

00:30:28.310 --> 00:30:29.500
it's a little different.

00:30:29.520 --> 00:30:35.140
We have some special considerations here
because these were added in iOS 4.3.

00:30:36.130 --> 00:30:40.680
The preset for photo delivers full
res out the still image output,

00:30:40.730 --> 00:30:43.600
but only preview-sized buffers
out the video data output.

00:30:43.600 --> 00:30:47.480
We can't give you full res
stills for every video frame.

00:30:47.480 --> 00:30:49.970
The bandwidth would be too intolerable.

00:30:49.980 --> 00:30:53.310
But you get preview-sized buffers
out the video data output,

00:30:53.310 --> 00:30:56.020
which are sized to about
the size of the screen,

00:30:56.020 --> 00:30:59.430
but with exactly the same aspect
ratio as the full-sized buffers.

00:30:59.970 --> 00:31:05.480
So if you have any processing to do,
lining up or choosing where to set focus,

00:31:05.480 --> 00:31:08.720
you can do those on the preview-sized
buffers in your video data output

00:31:08.840 --> 00:31:11.280
callback and then still snap
pictures using the still image

00:31:11.280 --> 00:31:12.620
output whenever you want to.

00:31:12.620 --> 00:31:16.200
And as I mentioned,
the aspect ratio is unchanged.

00:31:17.860 --> 00:31:21.340
Continuing along with our
supported resolutions grid,

00:31:21.340 --> 00:31:24.060
you'll note that every single
device and every camera supports

00:31:24.170 --> 00:31:28.890
the SD or lower resolutions,
but only the back cameras

00:31:28.960 --> 00:31:31.680
support the HD modes.

00:31:32.880 --> 00:31:34.800
All right,
now let's delve into that iframe thing.

00:31:34.880 --> 00:31:36.360
What is iframe?

00:31:36.470 --> 00:31:40.800
Well, to explain what iframe is,
I need to tell you what iframes are.

00:31:40.930 --> 00:31:42.860
Little i,
big F is a little different than

00:31:42.960 --> 00:31:44.790
what iframes are in general.

00:31:45.610 --> 00:31:51.800
This is a term used in video
compression to talk about dependencies

00:31:51.800 --> 00:31:53.790
between these video frames.

00:31:53.800 --> 00:31:58.310
Iframes or intraframes have no
dependencies on any other frames.

00:31:58.580 --> 00:32:04.530
They can be decoded individually and they
don't rely on any frames before or after.

00:32:04.710 --> 00:32:07.300
P-frames are predictive.

00:32:07.450 --> 00:32:10.680
That means in order to be decoded,
they need to search back to

00:32:10.700 --> 00:32:15.630
their previous iframe in order to
fully reconstitute the picture.

00:32:15.770 --> 00:32:16.990
B-frames are even worse.

00:32:17.040 --> 00:32:21.020
They can predict in either direction
and they're stored out of order.

00:32:21.020 --> 00:32:26.400
These are added levels of
complexity to get smaller files.

00:32:27.200 --> 00:32:28.660
So what is iFrame?

00:32:28.720 --> 00:32:34.960
Well, iFrame is an Apple term for
Apple ecosystem-friendly video.

00:32:35.070 --> 00:32:42.180
It's a format that we've given out to
third-party camera and camcorder vendors.

00:32:42.250 --> 00:32:45.280
There are already 30-plus
camcorders and cameras on the

00:32:45.280 --> 00:32:47.770
market that support iFrame.

00:32:48.030 --> 00:32:54.110
It means H.264 iframe-only video
plus AAC or PCM at a constant

00:32:54.220 --> 00:33:02.170
frame rate of 2997 or 25 on the
iOS products we support 2997.

00:33:02.540 --> 00:33:04.580
And the data rate is quite high.

00:33:04.760 --> 00:33:11.420
It's 30 megabits for quarter
HD or 40 megabits for 720p.

00:33:11.500 --> 00:33:40.600
[Transcript missing]

00:33:40.930 --> 00:33:45.640
It is supported on all iOS
devices with HD cameras.

00:33:45.750 --> 00:33:48.400
Let's move on to the second capture case.

00:33:48.490 --> 00:33:51.240
And to show that,
I'm going to call up Matthew Calhoun

00:33:51.240 --> 00:33:53.910
to show us Rosie Writer.

00:33:59.100 --> 00:34:03.800
Rosie Ryder is yet another
augmented reality application.

00:34:03.910 --> 00:34:07.950
Something you should know about Matthew,
he's a very cheerful guy.

00:34:08.180 --> 00:34:09.480
He's a very friendly guy.

00:34:09.580 --> 00:34:12.340
I like to say that he views the
world through rose-colored glasses.

00:34:12.340 --> 00:34:16.420
So he wrote an app that really does view
the world through rose-colored glasses.

00:34:16.420 --> 00:34:19.680
It's taking the video
from the back camera.

00:34:19.680 --> 00:34:26.660
It's processing every pixel of every
buffer and applying a tint to them so

00:34:26.740 --> 00:34:28.180
that they come out kind of rosy-colored.

00:34:28.180 --> 00:34:30.240
And then he's recording.

00:34:30.240 --> 00:34:32.760
So he's actually already
started the recording there.

00:34:32.760 --> 00:34:35.140
And you can see he's getting
25 frames per second.

00:34:35.140 --> 00:34:36.890
That's, again,
only because we're in low light.

00:34:36.900 --> 00:34:39.720
It will actually do a full 30
fips without dropping any frames.

00:34:39.720 --> 00:34:46.580
So applying processing to every pixel,
doing a real-time preview using OpenGL,

00:34:46.580 --> 00:34:50.250
and writing a quick-time
movie at 30 frames a second,

00:34:50.480 --> 00:34:54.170
720p, and we can view that live.

00:34:55.800 --> 00:34:56.800
That video.

00:34:56.800 --> 00:35:03.610
It's processing every pixel of every
-- So it was doing audio as well,

00:35:03.610 --> 00:35:05.010
obviously.

00:35:09.430 --> 00:35:10.540
Thank you, Matthew.

00:35:10.650 --> 00:35:11.710
Let's talk about how we did that.

00:35:11.740 --> 00:35:21.130
Okay, Rosie Writer again used the
back camera from the iPad.

00:35:21.500 --> 00:35:26.710
to get video data output into
his process and also audio data.

00:35:27.540 --> 00:35:30.590
and Write to a QuickTime Movie,
but he wasn't using the movie file

00:35:30.590 --> 00:35:35.290
output because that doesn't let you
get at the buffers to process them.

00:35:35.540 --> 00:35:36.610
So what does this look like to us?

00:35:36.750 --> 00:35:39.100
Well,
you still have a session in the middle.

00:35:39.120 --> 00:35:45.540
You still have device inputs for
the camera and for the microphone.

00:35:45.570 --> 00:35:47.420
You have data outputs.

00:35:47.460 --> 00:35:51.370
But then there are some new classes that
are not strictly part of AV Capture,

00:35:51.370 --> 00:35:55.580
but they are part of AV Foundation and
it's called AV Asset Writer.

00:35:55.610 --> 00:35:59.880
AV Asset Writer has an
input for video and audio.

00:36:00.790 --> 00:36:02.740
To talk about AV Asset Writer,
I first need to talk

00:36:02.790 --> 00:36:05.280
about what an AV Asset is.

00:36:05.460 --> 00:36:12.690
AV Asset, which is defined in AV Asset.h,
is how we abstract a media

00:36:12.690 --> 00:36:14.440
asset on our platform.

00:36:14.440 --> 00:36:16.720
So it can be URL based or stream based.

00:36:16.720 --> 00:36:20.790
It can be inspected for properties.

00:36:21.290 --> 00:36:24.090
If you want to play one,
you would use an AV player.

00:36:24.340 --> 00:36:29.800
If you want to read
one in an offline mode,

00:36:29.910 --> 00:36:32.070
you would use AV Asset Reader.

00:36:32.240 --> 00:36:35.420
If you want to do a full file
export of one of these assets,

00:36:35.510 --> 00:36:38.220
you would use AV Asset Export Session.

00:36:38.330 --> 00:36:44.170
And for our use, you can write them using
an AV Asset Writer.

00:36:44.790 --> 00:36:48.500
To create one of them,
you alloc and init one with the

00:36:48.610 --> 00:36:50.640
file type that you want to write to.

00:36:51.040 --> 00:36:54.170
In Matthew's app,
we were writing a QuickTime movie.

00:36:55.060 --> 00:36:58.990
And then there's a little bit of setup
to create the inputs for each kind

00:36:58.990 --> 00:37:00.590
of input data you'll be feeding it.

00:37:00.640 --> 00:37:04.240
In this slide,
I put the video input setup.

00:37:04.240 --> 00:37:07.540
So you tell it you're going to
be providing it video input,

00:37:07.540 --> 00:37:10.680
and you give it some output
settings to tell it what kind

00:37:10.680 --> 00:37:14.610
of output it should produce,
what format it should produce.

00:37:14.680 --> 00:37:20.790
You set the expect media data in real
time flag to yes so that it knows that

00:37:20.790 --> 00:37:24.100
you're going to be in the real-time
capture scenario and so it doesn't do

00:37:24.110 --> 00:37:25.940
additional buffering on your behalf.

00:37:26.000 --> 00:37:30.360
And then you add the input
to your AV asset writer,

00:37:30.360 --> 00:37:33.540
start up your delegate
as you normally do,

00:37:33.540 --> 00:37:34.960
and start the session running.

00:37:36.690 --> 00:37:41.210
Inside your delegate callback,
after you do your processing,

00:37:41.210 --> 00:37:44.680
you would call video
input a pen sample buffer.

00:37:44.980 --> 00:37:45.900
That's it.

00:37:45.900 --> 00:37:48.570
You're now writing the movie
one frame at a time from

00:37:48.570 --> 00:37:51.180
within the delegate callbacks.

00:37:52.180 --> 00:37:54.190
So now let's talk about
which one you would use.

00:37:54.300 --> 00:37:57.800
We have two classes that sound
like they do the same thing,

00:37:57.830 --> 00:38:00.740
Writer and Movie File Output.

00:38:00.770 --> 00:38:05.340
Well, Movie File Output has some
features all of its own.

00:38:05.360 --> 00:38:07.380
It requires no setup.

00:38:07.380 --> 00:38:10.870
You can't specify an output
settings on it because it inherits

00:38:10.870 --> 00:38:13.260
them from the session preset.

00:38:13.770 --> 00:38:16.840
It's a flexible object in that you
can record multiple movies from it.

00:38:16.840 --> 00:38:20.140
You can start one, then stop,
and then reuse that same movie file

00:38:20.140 --> 00:38:23.040
output to do multiple recordings.

00:38:23.160 --> 00:38:27.080
It also supports some limiting like
file size limiting or duration,

00:38:27.080 --> 00:38:29.040
and it will automatically
stop the recording when any

00:38:29.040 --> 00:38:31.360
of those limits are tripped.

00:38:31.670 --> 00:38:34.960
But it does not allow for
client access to the buffers

00:38:35.070 --> 00:38:36.840
before writing them to disk.

00:38:36.990 --> 00:38:41.060
So if you use a movie file output,
you get what the camera sees.

00:38:41.130 --> 00:38:43.600
You don't get to process the frames.

00:38:43.600 --> 00:38:48.210
AV Asset Writer, on the other hand,
is a general purpose writing utility

00:38:48.210 --> 00:38:52.580
that can be used for the non-real-time
case or the real-time case.

00:38:52.610 --> 00:38:55.760
So it does require setup of
output settings because it doesn't

00:38:55.760 --> 00:38:57.600
know what to produce by default.

00:38:57.720 --> 00:38:58.920
It's a one-shot writer.

00:38:58.920 --> 00:39:01.600
You can't use this to
record multiple movie files.

00:39:01.600 --> 00:39:05.130
Once you've finished a movie
file with an asset writer,

00:39:05.130 --> 00:39:08.010
you need to throw it
away and make a new one.

00:39:08.900 --> 00:39:12.650
But it does allow for client access
to the video buffers because they

00:39:12.710 --> 00:39:16.440
come into your process in the video
data output and audio data output,

00:39:16.620 --> 00:39:19.240
and then you send them along
their way to the asset writer

00:39:19.240 --> 00:39:20.540
in your delegate callback.

00:39:20.600 --> 00:39:23.560
Be aware, though,
that asset writer does incur some

00:39:23.560 --> 00:39:25.880
more overhead than movie file output.

00:39:25.880 --> 00:39:28.330
So if you don't intend
to do any processing,

00:39:28.330 --> 00:39:30.880
you will want to use
the movie file output.

00:39:32.780 --> 00:39:35.410
Here's what the sample video
settings might look like that you

00:39:35.410 --> 00:39:37.100
would feed to the asset writer.

00:39:37.240 --> 00:39:40.400
And these are defined
in AVVideoSettings.h.

00:39:40.490 --> 00:39:42.800
You can tell it what codec you want.

00:39:42.910 --> 00:39:45.500
I chose H.264.

00:39:45.570 --> 00:39:48.340
You can specify a width and a height.

00:39:48.610 --> 00:39:52.970
You can also give it a dictionary
of compressor-specific properties.

00:39:53.180 --> 00:39:55.780
For H.264, you can specify a bit rate.

00:39:55.870 --> 00:39:59.300
I chose 10.5 megabits per second.

00:39:59.440 --> 00:40:05.620
You can also specify a max keyframe rate
interval so that it will force a keyframe

00:40:05.620 --> 00:40:08.740
or an iframe at least once a second.

00:40:08.920 --> 00:40:11.850
You can also specify profile level.

00:40:12.970 --> 00:40:15.660
And now let's look at
the equivalent for audio.

00:40:15.750 --> 00:40:19.100
The audio settings look like this.

00:40:19.160 --> 00:40:24.520
You can specify a layout,
so is it stereo, is it mono,

00:40:24.570 --> 00:40:29.350
the format that you expect it to produce,
AAC for instance, the bit rate,

00:40:29.590 --> 00:40:34.270
number of channels, sample rate,
and the layout.

00:40:35.710 --> 00:40:39.800
So lastly with AV Asset Writer,
let's talk about some do's and don'ts.

00:40:39.830 --> 00:40:45.760
Do set that expects media data in
real time to yes when you're using

00:40:45.760 --> 00:40:48.640
AV Asset Writer input with capture.

00:40:48.680 --> 00:40:52.000
Otherwise, it will do some buffering
and try to do some optimal

00:40:52.000 --> 00:40:56.590
interleaving of video and audio,
and you will drop more frames.

00:40:57.590 --> 00:41:00.930
Also,
set AV Asset Writer's movie fragment

00:41:01.840 --> 00:41:06.000
interval to a non-zero value if you
want to preserve your recordings

00:41:06.000 --> 00:41:07.280
in the event of an interruption.

00:41:07.410 --> 00:41:09.590
I'll talk more about that in a minute.

00:41:10.420 --> 00:41:12.880
Also,
that always discards late video frames.

00:41:13.200 --> 00:41:15.020
Set it to no.

00:41:15.250 --> 00:41:16.690
Earlier I said set it to yes.

00:41:16.800 --> 00:41:19.010
But for this case,
I'm saying set it to no because

00:41:19.020 --> 00:41:20.600
you want to capture a movie file.

00:41:20.730 --> 00:41:24.480
So even if the frames are a little bit
late and there's a possibility that

00:41:24.480 --> 00:41:27.960
you can still write them to the movie,
you would want them.

00:41:28.690 --> 00:41:33.550
Don't hold on to those sample
buffers outside the callback.

00:41:33.640 --> 00:41:34.800
So don't do your own buffering.

00:41:34.800 --> 00:41:38.780
Append them to the asset
writer within the callback.

00:41:38.780 --> 00:41:41.720
And again,
don't take too long inside your callback.

00:41:41.800 --> 00:41:43.730
I just mentioned movie fragments.

00:41:43.830 --> 00:41:48.380
This is a neat technology that helps in
the event of crashes or interruptions.

00:41:48.380 --> 00:41:50.000
Interruptions are unpredictable.

00:41:50.000 --> 00:41:52.500
You don't know,
this is a phone that you're dealing with.

00:41:52.500 --> 00:41:54.540
You don't know when you're
going to get a call.

00:41:55.140 --> 00:41:57.120
So if you're in the
middle of a movie file,

00:41:57.280 --> 00:41:59.580
you really want that movie
file to succeed even if it

00:41:59.580 --> 00:42:01.340
gets interrupted or crashed.

00:42:01.340 --> 00:42:05.090
Here's what a movie file looks like
that you would get off the web.

00:42:05.090 --> 00:42:10.290
It has a movie header at the top
which tells where all of the data is.

00:42:10.290 --> 00:42:15.510
It has sample offsets that say find
sample N at this offset in the file

00:42:15.510 --> 00:42:18.450
and the actual data is afterwards.

00:42:18.460 --> 00:42:21.220
It's called a fast
start quick time movie.

00:42:21.680 --> 00:42:23.690
When we're capturing,
we can't write fast start

00:42:23.820 --> 00:42:26.560
movies because we don't know how
long the movie is going to be.

00:42:26.560 --> 00:42:28.000
We don't know where all
the samples are yet.

00:42:28.000 --> 00:42:32.130
So we have to put the movie data up
front and only when they end the capture

00:42:32.130 --> 00:42:34.360
do we append the header to the end.

00:42:34.360 --> 00:42:37.160
You see the problem with this strategy.

00:42:37.160 --> 00:42:40.330
If you crash or you are interrupted
in the middle of your recording

00:42:40.330 --> 00:42:43.580
and you haven't had a chance to
lay down that movie header yet,

00:42:43.580 --> 00:42:46.990
you've got a big file that's unreadable.

00:42:47.730 --> 00:42:50.440
Our solution for this is movie fragments.

00:42:50.560 --> 00:42:55.360
We write on iOS devices,
QuickTime movies with movie fragments.

00:42:55.480 --> 00:42:59.790
So if you've specified a movie fragment
interval to the AV asset writer or

00:42:59.810 --> 00:43:04.140
to the AV capture movie file output,
it will lay down a small header

00:43:04.140 --> 00:43:07.210
at the beginning that accounts
for some number of seconds.

00:43:07.250 --> 00:43:08.380
By default, it's 10.

00:43:08.380 --> 00:43:11.150
And then for every 10
seconds of your movie,

00:43:11.150 --> 00:43:13.270
it will lay down a movie fragment.

00:43:13.270 --> 00:43:16.730
And if you crash at any point
or are interrupted at any point,

00:43:17.770 --> 00:43:20.730
your movie will be safe up to
the point of the last fragment

00:43:20.740 --> 00:43:22.340
that was recorded in the movie.

00:43:22.500 --> 00:43:25.000
So the most that you
would lose is 10 seconds.

00:43:25.030 --> 00:43:27.220
All right,
let's move on to our third capture case,

00:43:27.240 --> 00:43:30.330
which is scanning video
frames for patterns using the

00:43:30.330 --> 00:43:32.520
flash and video data output.

00:43:32.620 --> 00:43:36.620
And to help me with that,
I'd like to call up Valentin Bonnet.

00:43:44.670 --> 00:43:48.700
and could we dim the front lights a bit?

00:43:48.790 --> 00:43:52.180
Valentin has helped us out a lot
with his summer intern project.

00:43:52.290 --> 00:43:56.590
He's written a really cool application
about an emerging technology.

00:43:56.830 --> 00:44:03.250
This technology I like to call Lost,
L-O-S-T, or line of sight texting.

00:44:04.050 --> 00:44:08.310
Have you ever been in a scenario
where you have no cell coverage but

00:44:08.400 --> 00:44:12.890
you'd really like to text a person,
you can see them, but you just,

00:44:13.010 --> 00:44:14.240
you can't message them.

00:44:14.240 --> 00:44:17.310
There's no Wi-Fi, there's no cell.

00:44:17.320 --> 00:44:20.210
Well, wouldn't it be cool if
you could use your iPhone?

00:44:20.350 --> 00:44:22.190
You ready for me?

00:44:37.940 --> 00:44:46.780
So this line of sight texting business
is using input from a sender phone,

00:44:46.940 --> 00:44:52.100
translating it into a new
technology I call Morse code.

00:44:52.940 --> 00:44:56.890
and translating it into dots and dashes
that it then uses the LED flash to

00:44:57.070 --> 00:45:02.040
reproduce on the receiver side where the
receiver interprets the pulses of light,

00:45:02.320 --> 00:45:05.980
turns them back into text and
makes it look like a text message.

00:45:06.190 --> 00:45:09.080
Let's go back to slides to
talk about how we did that.

00:45:09.270 --> 00:45:15.900
And we can bring up house lights now
unless we want a late afternoon ambiance.

00:45:17.250 --> 00:45:19.340
Cool app, huh?

00:45:19.360 --> 00:45:24.380
The MorseMe application has
a sender and a receiver side.

00:45:24.400 --> 00:45:27.720
And the sender side is
as simple as can be.

00:45:27.760 --> 00:45:30.570
It just uses the LED torch.

00:45:31.340 --> 00:45:36.200
So all it looks like to
AV Foundation is an AV capture device.

00:45:36.260 --> 00:45:38.940
That's it, nothing else.

00:45:39.430 --> 00:45:45.120
On the receiver side,
he's using the iPad's back camera.

00:45:45.350 --> 00:45:48.760
along with the video data
output to process those frames,

00:45:48.810 --> 00:45:52.000
find the light pulses and
turn them back into text.

00:45:52.170 --> 00:45:55.090
So that looks like session in the middle.

00:45:55.430 --> 00:46:00.480
Device input on top, video data output,
and live video preview.

00:46:01.130 --> 00:46:03.640
So let's talk about torch
support in general in iOS 5.

00:46:03.720 --> 00:46:07.650
We have three torch modes, off, on,
and auto.

00:46:07.820 --> 00:46:09.450
For this app, we were just using on.

00:46:09.480 --> 00:46:13.220
He was setting it to on when he
wanted it on and off when he didn't.

00:46:13.670 --> 00:46:19.510
You can also use the hasTorch property to
determine if the capture device has one.

00:46:19.890 --> 00:46:24.290
Obviously it was going to be a one-sided
conversation because he was using

00:46:24.390 --> 00:46:27.610
an iPad and it doesn't have a torch.

00:46:27.830 --> 00:46:32.400
You call lock for configuration before
attempting to set the torch mode.

00:46:32.940 --> 00:46:36.140
And flashlight apps,
I don't know if I should ask

00:46:36.140 --> 00:46:36.990
anyone to raise their hand.

00:46:37.000 --> 00:46:38.600
Has anyone written a flashlight app here?

00:46:38.600 --> 00:46:41.220
Because you know who you are.

00:46:41.220 --> 00:46:44.040
And you know that last year
I stood up here and I told you

00:46:44.040 --> 00:46:47.260
not to write flashlight apps
with our LED torch API because it

00:46:47.260 --> 00:46:49.300
was only for video illumination.

00:46:49.300 --> 00:46:52.380
But you went out and you wrote
your flashlight apps anyway.

00:46:53.470 --> 00:46:56.350
And the problem with that
is doing it the old way,

00:46:56.350 --> 00:46:59.140
the AV capture session had to be running.

00:46:59.140 --> 00:47:03.130
So you were in effect running
a full video pipeline just

00:47:03.130 --> 00:47:05.290
to turn the flashlight on.

00:47:05.290 --> 00:47:09.190
So you were burning battery
at an alarmingly fast rate.

00:47:09.200 --> 00:47:12.590
Well, in iOS 5,
we've altered this so that to use

00:47:12.740 --> 00:47:16.160
the LED torch for a flashlight,
you don't need to run

00:47:16.160 --> 00:47:17.760
the session anymore.

00:47:17.760 --> 00:47:22.350
You can just find the capture device
and turn the LED torch on or off.

00:47:23.000 --> 00:47:26.670
We will not run the capture
session in the background.

00:47:27.090 --> 00:47:28.760
There's no allocation of buffers.

00:47:28.860 --> 00:47:30.300
There's no additional CPU.

00:47:30.300 --> 00:47:33.290
So you're only using the power
that would be used by the

00:47:33.290 --> 00:47:35.060
LED torch turning on and off.

00:47:35.100 --> 00:47:36.250
So that's the problem with that.

00:47:36.750 --> 00:47:39.460
So the code shrinks down
to just three lines,

00:47:39.500 --> 00:47:40.330
basically.

00:47:40.510 --> 00:47:44.340
You lock the device for configuration,
set the torch mode to on,

00:47:44.370 --> 00:47:45.240
unlock the device.

00:47:45.380 --> 00:47:48.930
That's pretty much all that the sender
side of the MorseMe application did.

00:47:51.500 --> 00:47:54.900
We have a couple torch
enhancements in iOS 5 as well,

00:47:55.100 --> 00:47:57.640
some availability accessors.

00:47:58.080 --> 00:48:03.680
The device housing the torch is a phone.

00:48:03.680 --> 00:48:05.600
It has a lot of components
in it that can heat up.

00:48:05.600 --> 00:48:09.150
And the torch may become unavailable
as the phone gets too hot.

00:48:09.200 --> 00:48:13.480
So you can now key value observe
this isTorchAvailable property

00:48:13.480 --> 00:48:17.500
to know if it suddenly becomes so
hot that it needs to cool down.

00:48:17.500 --> 00:48:20.120
And so it will tell you when
it's available again as well.

00:48:20.520 --> 00:48:22.860
Also,
you can key value observe the TorchLevel

00:48:22.860 --> 00:48:27.020
property of the AV Capture device
to know when it's throttling the

00:48:27.020 --> 00:48:30.900
actual illumination level of the torch
down because it's getting too hot.

00:48:33.400 --> 00:48:35.710
All right, with that,
let's move on to the fourth demo,

00:48:35.710 --> 00:48:40.590
which is processing captured
still images using Core Image.

00:48:41.190 --> 00:48:43.600
And to demo that,
I'm going to invite some

00:48:43.600 --> 00:48:46.150
of my friends up on stage.

00:48:47.430 --> 00:48:50.060
Okay, there's StashCam.

00:48:50.110 --> 00:48:54.600
Okay, StashCam is a still image
capture application.

00:48:54.710 --> 00:48:55.600
Pretty basic.

00:48:55.600 --> 00:48:57.650
It does the kinds of things
you would expect it to do.

00:48:57.890 --> 00:49:01.280
You can switch between the
front and back cameras.

00:49:03.920 --> 00:49:05.400
So there are my friends.

00:49:05.400 --> 00:49:06.590
There's me.

00:49:06.730 --> 00:49:10.700
You can also take pictures,
as you might expect.

00:49:10.770 --> 00:49:14.340
It's doing a little animation when
the capture is supposed to happen.

00:49:14.610 --> 00:49:19.230
You can zoom in just like the Camera app,
so we now support digital zoom or

00:49:19.230 --> 00:49:24.760
scale and crop and take pictures
of digitally zoomed pictures.

00:49:24.840 --> 00:49:29.210
But the killer feature of the
app is that it can detect faces.

00:49:29.390 --> 00:49:33.400
So I'm going to turn the
Detect Faces button on.

00:49:36.940 --> 00:49:41.140
Okay, so go ahead, Matthew,
turn sideways so that you, so, oh,

00:49:41.140 --> 00:49:42.090
it needs two eyes.

00:49:42.090 --> 00:49:43.010
Oh, there it comes back.

00:49:43.010 --> 00:49:46.610
And it works equally well
on the front camera as well.

00:49:46.770 --> 00:49:49.970
My kids have been beta testing
this app for the last week

00:49:50.140 --> 00:49:51.900
and it's a hit with them.

00:49:51.900 --> 00:49:54.160
Okay, thanks, guys.

00:50:01.060 --> 00:50:03.650
All right, how did we do that?

00:50:03.800 --> 00:50:07.010
Well,
we used the iPad and the back camera

00:50:07.250 --> 00:50:10.440
I was showing you a video
preview while I was superimposing

00:50:10.440 --> 00:50:12.700
the funny glasses on top.

00:50:13.490 --> 00:50:16.560
and Capturing Still Images.

00:50:16.660 --> 00:50:21.090
But everything else that was done,
learning where the faces are,

00:50:21.090 --> 00:50:22.840
was done with Core Image.

00:50:23.000 --> 00:50:27.110
Core Image is making its iOS debut
in iOS 5 and they have a new class

00:50:27.140 --> 00:50:31.140
called CI Face Detector and I'll
talk about that a little bit more.

00:50:31.250 --> 00:50:35.820
So from the session standpoint,
it just looks like session, device input,

00:50:35.820 --> 00:50:39.510
still image output, video preview layer.

00:50:41.760 --> 00:50:47.540
Let's talk about the enhancements for
still images within AV Capture first.

00:50:47.600 --> 00:50:49.950
You notice that when
I took a still image,

00:50:50.090 --> 00:50:51.530
I flashed the screen.

00:50:51.830 --> 00:50:54.680
I could have done a fancier sort of
iris animation or something like that.

00:50:54.800 --> 00:50:57.950
But in order to do that and to
have it actually line up with

00:50:57.950 --> 00:51:01.700
the picture that's being taken,
you need to know exactly when

00:51:01.800 --> 00:51:03.720
the camera is taking the photo.

00:51:03.800 --> 00:51:06.900
And that's not instantaneous,
especially on like an iPhone

00:51:06.970 --> 00:51:09.760
if it's using the flash,
it might take a minute

00:51:09.760 --> 00:51:11.340
for the flash to warm up.

00:51:11.600 --> 00:51:15.180
So AV Capture still image
output now has a new property

00:51:15.240 --> 00:51:17.500
called isCapturingStillImage.

00:51:17.680 --> 00:51:20.460
You can key value observe this
property to know exactly when

00:51:20.460 --> 00:51:23.700
the still image is being taken
and you can key your animation,

00:51:23.700 --> 00:51:29.180
your iris shutting or your shutter off
of that key value observed property.

00:51:29.560 --> 00:51:34.680
There's also another new property called
Subject Area Change Monitoring Enabled,

00:51:34.740 --> 00:51:35.960
which is a mouthful.

00:51:36.250 --> 00:51:41.860
But what it means is if you
have previously locked focus,

00:51:41.860 --> 00:51:47.140
it might be nice to know when the scene
changes enough that you would like to

00:51:47.140 --> 00:51:49.350
go back to a continuous focus mode.

00:51:49.510 --> 00:51:53.330
So it's great that we let you
continuously focus or lock,

00:51:53.330 --> 00:51:56.720
but now we're giving you the ability
to know when things have changed enough

00:51:56.890 --> 00:51:59.500
that you might want to focus again.

00:51:59.680 --> 00:52:04.340
So once you lock a device,
focus or exposure,

00:52:04.430 --> 00:52:07.820
you can start observing this
property and set Subject

00:52:07.890 --> 00:52:10.620
Area Change Monitoring Enabled to true.

00:52:10.800 --> 00:52:14.970
And once you do that,
you'll be notified when we think

00:52:14.970 --> 00:52:20.370
that the picture has changed
drastically or the user has moved.

00:52:23.330 --> 00:52:27.900
And then on the receiving side,
we have Core Image and CI filters.

00:52:27.940 --> 00:52:31.100
This is their iOS debut, iOS 5,
and there are some great

00:52:31.590 --> 00:52:33.900
CI filters available.

00:52:33.940 --> 00:52:36.460
There's one for Red Eye Reduction.

00:52:36.500 --> 00:52:40.490
There's one for Auto-Enhance,
which is just like the iPhoto

00:52:40.510 --> 00:52:43.060
One Touch cleanup button.

00:52:43.090 --> 00:52:48.030
It applies a number of good
cleanups to the still image.

00:52:48.480 --> 00:52:49.460
There are others as well.

00:52:49.460 --> 00:52:50.330
I just called those out.

00:52:50.340 --> 00:52:52.580
You can, of course,
do sepia tone and color cube

00:52:52.580 --> 00:52:54.380
and other things like that.

00:52:54.460 --> 00:52:59.640
Also new is a CI detector class,
which doesn't process the image

00:52:59.730 --> 00:53:04.370
to deliver an output image,
but it processes the image to

00:53:04.370 --> 00:53:07.750
deliver data about the image.

00:53:07.890 --> 00:53:11.120
In our case,
the CI detector is finding faces

00:53:11.120 --> 00:53:13.460
and features on those faces.

00:53:13.780 --> 00:53:16.740
So the CI image interfaces are great.

00:53:17.020 --> 00:53:20.280
They line up well with AV Capture
because they allow you to make a

00:53:20.290 --> 00:53:22.830
CI image out of a CV pixel buffer.

00:53:22.980 --> 00:53:26.380
So if you get a still image
or a video data output buffer,

00:53:26.430 --> 00:53:31.320
you can easily create a core
image CI image object from it.

00:53:31.810 --> 00:53:35.800
You do need to specify BGRA output
to be compatible with CI image.

00:53:35.800 --> 00:53:38.800
It doesn't work with the
other formats right now.

00:53:39.470 --> 00:53:40.100
Here's some code.

00:53:40.100 --> 00:53:45.540
This is the main code in
StashCam that finds the faces.

00:53:45.580 --> 00:53:50.980
Inside the still image capture button,

00:53:51.260 --> 00:53:55.960
I get the sample buffer's image buffer,
so now I have my CVPixel buffer pointer,

00:53:56.080 --> 00:54:00.490
and I create a CI image out of it.

00:54:00.620 --> 00:54:04.740
And then I create one of
these face detector objects,

00:54:04.890 --> 00:54:06.440
giving it some options.

00:54:06.550 --> 00:54:12.230
Here I told it CI detector accuracy low,
so it's because I'm a real-time app,

00:54:12.260 --> 00:54:15.860
I don't want it to take
forever to find faces.

00:54:15.980 --> 00:54:19.190
And then I ask it to
find features in image,

00:54:19.190 --> 00:54:22.710
so calling face detector
features in image,

00:54:22.950 --> 00:54:25.600
it produces an array of features.

00:54:25.700 --> 00:54:30.340
And from those CI face feature objects,
I can find the bounding rectangle,

00:54:30.430 --> 00:54:35.110
left eye, right eye, mouth,
and do some interesting things with

00:54:35.220 --> 00:54:37.790
how the face is tilted as well.

00:54:38.790 --> 00:54:41.250
This is all available
for third-party use.

00:54:41.340 --> 00:54:42.940
And I highly encourage you,
if you're interested in

00:54:42.940 --> 00:54:46.640
this core image stuff,
to take a look at their session tomorrow,

00:54:46.920 --> 00:54:50.210
which is 2:00 PM in Mission.

00:54:51.550 --> 00:54:53.460
Let me summarize.

00:54:53.530 --> 00:54:58.290
In iOS 5, AV Foundation Capture
gives you more CPU cycles.

00:54:58.550 --> 00:55:02.480
In the dramatic OpenGL case,
you saw us getting an

00:55:02.480 --> 00:55:04.980
immediate 10 to 15 frames back.

00:55:05.030 --> 00:55:06.730
That's huge performance.

00:55:06.960 --> 00:55:09.260
There's also a lot of other
performance work we've done under

00:55:09.260 --> 00:55:11.420
the hood that require no changes.

00:55:11.470 --> 00:55:15.660
And hopefully you'll feel the
performance benefits even if you

00:55:15.660 --> 00:55:18.690
don't opt into the GL improvements.

00:55:19.210 --> 00:55:23.740
Bridging Core Video and
OpenGL is a key win in iOS 5.

00:55:23.860 --> 00:55:27.180
We've exposed more resolutions,
so you can get smaller resolutions

00:55:27.180 --> 00:55:29.380
for SIF streaming apps.

00:55:29.380 --> 00:55:33.460
Also, if you're doing editing apps,
you now have the option

00:55:33.690 --> 00:55:35.410
of iFrame recording.

00:55:35.800 --> 00:55:38.800
We've added more flexibility
to still image capture for

00:55:38.880 --> 00:55:42.990
doing things like digital zoom,
crop and scale.

00:55:43.780 --> 00:55:47.340
And we hope this is going to enable you
to deliver even cooler apps now that you

00:55:47.340 --> 00:55:49.940
have more cycles to do those cool things.

00:55:50.210 --> 00:55:55.380
So for more information,
please contact Eric Verschen,

00:55:55.460 --> 00:55:58.640
our Media Technologies Evangelist
for AV Foundation,

00:55:58.750 --> 00:56:01.730
or check out the documentation online.

00:56:01.860 --> 00:56:06.610
Also, the developer forums are a great
place to ask questions and have them

00:56:06.660 --> 00:56:12.990
answered either by fellow users or
sometimes even by Apple engineers.

00:56:13.540 --> 00:56:16.790
Our related sessions have all passed,
but please give them a look

00:56:17.090 --> 00:56:19.920
in your iTunes after the show.

00:56:19.920 --> 00:56:22.360
Thank you for attending today.

00:56:22.360 --> 00:56:23.680
I hope you've had a great session.

00:56:23.680 --> 00:56:24.420
Have a great show.