WEBVTT

00:00:10.450 --> 00:00:12.400
So good morning everyone.

00:00:12.400 --> 00:00:13.640
My name is Steve Canon.

00:00:13.640 --> 00:00:16.300
I'm a senior engineer in
the Vector Numerics group.

00:00:16.300 --> 00:00:21.810
And today I'm going to be talking to you
about the Accelerate Framework for iOS.

00:00:22.810 --> 00:00:24.930
So just a little bit of introduction.

00:00:25.220 --> 00:00:27.700
What is the Accelerate Framework?

00:00:27.700 --> 00:00:32.100
We like to think of it as being
sort of one stop shopping for high

00:00:32.110 --> 00:00:34.450
performance computational libraries.

00:00:34.490 --> 00:00:36.390
What kind of libraries?

00:00:37.060 --> 00:00:43.580
We introduced it on iOS in iOS 4 and
we introduced three libraries in iOS 4.

00:00:43.650 --> 00:00:47.190
The first was VDSP,
which is a digital signal

00:00:47.190 --> 00:00:48.450
processing library.

00:00:48.550 --> 00:00:52.750
It can do everything from sort
of basic operations on vectors,

00:00:52.760 --> 00:00:58.720
adding them and multiplying
them and things like that,

00:00:58.780 --> 00:01:00.290
on up to some one dimensional
convolutions and fast Fourier transforms.

00:01:00.640 --> 00:01:04.640
The second library that we
introduced in iOS 4 is called BLAS,

00:01:04.780 --> 00:01:10.260
which is an industry standard library
of basic linear algebra routines.

00:01:10.380 --> 00:01:14.860
And that includes things like
matrix-matrix operations and products

00:01:14.920 --> 00:01:18.200
between matrices and vectors,
all that kind of stuff.

00:01:18.330 --> 00:01:21.650
And the last one was LAPAC,
which stands for linear algebra package.

00:01:21.860 --> 00:01:24.750
That's also an industry standard library.

00:01:25.310 --> 00:01:30.680
and LAPAC provides sort of high
level linear algebra operations.

00:01:30.680 --> 00:01:33.830
If you want to solve a system of
equations or that kind of stuff,

00:01:33.980 --> 00:01:37.800
then LAPAC is what you want to use.

00:01:38.330 --> 00:01:42.640
On the Mac, where we've had Accelerate
for a very long time,

00:01:42.640 --> 00:01:44.530
we also have a few other things.

00:01:44.530 --> 00:01:49.050
We have Vimage,
which is a image processing framework,

00:01:49.050 --> 00:01:52.040
and we have Vforce,
which is a vector math library.

00:01:52.040 --> 00:01:57.100
It provides the same kind of operations
that you find in the system math library,

00:01:57.100 --> 00:02:00.540
but on vectors of data
instead of on scalar data.

00:02:00.670 --> 00:02:03.780
And today, we are bringing

00:02:05.260 --> 00:02:07.740
all of those same libraries to iOS 5.

00:02:07.830 --> 00:02:11.430
So we're adding vimage
and we're adding vforce.

00:02:15.710 --> 00:02:20.540
So my goals for this session are sort
of to introduce you to the new to iOS

00:02:20.660 --> 00:02:22.560
components of the Accelerate Framework.

00:02:22.610 --> 00:02:25.600
Those of you who are primarily iOS
developers who don't have a lot of

00:02:25.600 --> 00:02:29.030
experience on the Mac may be asking,
you know, what's in these new libraries?

00:02:29.060 --> 00:02:33.900
So we're just going to give you
a broad overview of what's there.

00:02:33.920 --> 00:02:36.740
And the other thing we want to do is
spend a little bit of time just talking

00:02:36.740 --> 00:02:39.900
about the improvements we've made
to the stuff that was already there,

00:02:39.960 --> 00:02:42.530
the components that we had in iOS 4.

00:02:42.720 --> 00:02:45.990
and what I really want you to do,
or what I really want to do,

00:02:45.990 --> 00:02:49.340
is to help you identify places in
your code that you might be able

00:02:49.340 --> 00:02:53.770
to use the components we provide
to you in the Accelerate Framework.

00:02:54.380 --> 00:02:57.290
So let's dive right in.

00:02:57.440 --> 00:02:59.460
We're going to start with vImage,
which as I mentioned,

00:02:59.460 --> 00:03:01.200
this is new in iOS 5.

00:03:01.270 --> 00:03:06.350
And vImage stands for the Vectorized
Image Processing Framework.

00:03:06.560 --> 00:03:10.520
We introduced it on the Mac in 10.3.

00:03:10.700 --> 00:03:12.770
And it's proved to be
pretty popular on the Mac.

00:03:13.000 --> 00:03:16.230
In fact,
it's used in six out of seven of the

00:03:16.300 --> 00:03:18.390
top grossing apps on the Mac App Store.

00:03:18.540 --> 00:03:20.820
So it's been very popular on the Mac.

00:03:20.900 --> 00:03:22.930
And we've had a lot of
requests to bring it to iOS,

00:03:22.930 --> 00:03:26.290
so we're really excited
that we have it there now.

00:03:27.080 --> 00:03:29.680
So just to start out,
for those of you who

00:03:29.800 --> 00:03:34.130
aren't familiar with it,
I'm going to give you a small example.

00:03:34.340 --> 00:03:36.530
Let's start with by
looking at convolution,

00:03:36.710 --> 00:03:40.020
which is one of the most
important and also one of the most

00:03:40.020 --> 00:03:42.980
complicated operations in V-image.

00:03:43.040 --> 00:03:46.920
And it forms the core of a lot
of common image processing tasks.

00:03:47.000 --> 00:03:51.080
Convolution is basically the
weighted average of nearby pixels.

00:03:51.170 --> 00:03:54.850
For each pixel in an image,
you're going to compute a new image

00:03:55.310 --> 00:03:59.570
based on the pixels that surround
the pixel you're interested in.

00:04:00.010 --> 00:04:05.090
and it gets used to do blur, sharpen,
edge detection, emboss filters,

00:04:05.250 --> 00:04:08.020
lots and lots of image
processing algorithms are

00:04:08.020 --> 00:04:10.110
fundamentally convolutions.

00:04:10.200 --> 00:04:11.990
So just to explain a
little more what I mean,

00:04:11.990 --> 00:04:15.190
if we look at sort of what we do,
if we were going to evaluate

00:04:15.190 --> 00:04:18.220
the convolution on that
center pixel in the image,

00:04:18.220 --> 00:04:20.600
what we do is we have
a matrix of weights,

00:04:20.600 --> 00:04:25.450
it's called a kernel and we multiply
that by the window around that pixel.

00:04:25.460 --> 00:04:28.370
So here we have a three by three kernel,
so we take a three by three window

00:04:28.610 --> 00:04:33.250
around the pixel that we're interested
in and we multiply each weight by

00:04:33.250 --> 00:04:38.460
the corresponding pixel and then
we sum those all up and that gives

00:04:38.460 --> 00:04:40.660
us a new color in the output image.

00:04:40.760 --> 00:04:43.880
Now here, there were some white pixels
and some darker purple pixels,

00:04:43.880 --> 00:04:46.650
so when we mix them together,
we get kind of a lighter purple

00:04:46.710 --> 00:04:51.320
color and then we just do this
process across the whole image.

00:04:51.360 --> 00:04:52.870
So you get something like this.

00:04:52.870 --> 00:04:55.600
This is--this is a small blur kernel,
so you can see it's smoothed out a

00:04:55.860 --> 00:05:01.120
hard edge from sort of dark purple
to white to be a smoother transition.

00:05:01.250 --> 00:05:03.800
That's very basic convolution.

00:05:03.920 --> 00:05:06.060
Now you could write a
convolution yourself.

00:05:06.090 --> 00:05:07.440
It's not terribly hard.

00:05:07.440 --> 00:05:10.920
This is sort of the simplest
kernel of a convolution that

00:05:10.930 --> 00:05:12.980
you could--you could write.

00:05:13.430 --> 00:05:15.530
But you really shouldn't do this.

00:05:15.640 --> 00:05:18.180
So why not?

00:05:18.290 --> 00:05:21.630
Well, that simple example I showed you
there has a lot of problems with it.

00:05:21.810 --> 00:05:24.510
It doesn't handle edging properly.

00:05:24.610 --> 00:05:27.440
When you look at a pixel that's
close to the edge of an image,

00:05:27.550 --> 00:05:30.790
that kernel is actually going to
hang off the side of the image.

00:05:30.930 --> 00:05:32.180
And you need to handle that properly.

00:05:32.180 --> 00:05:34.120
It results in a lot of edging code.

00:05:34.210 --> 00:05:36.390
It's kind of a pain.

00:05:36.440 --> 00:05:37.760
It doesn't handle integer overflow.

00:05:37.860 --> 00:05:39.140
This is just a basic thing.

00:05:39.140 --> 00:05:41.360
But it's something you have to deal with.

00:05:41.590 --> 00:05:45.040
And more to the point,
and what I care about, it's really slow.

00:05:45.170 --> 00:05:47.790
So instead of writing your own--

00:05:47.900 --> 00:06:46.600
[Transcript missing]

00:06:46.790 --> 00:06:51.560
and it uses about a tenth the
energy to use the vimage convolve

00:06:51.700 --> 00:06:55.360
as it does to use a good high
quality scalar implementation.

00:06:55.470 --> 00:06:56.500
So this is really nice.

00:06:56.630 --> 00:06:58.990
It means your app is
going to last longer and,

00:06:59.120 --> 00:07:01.010
you know,
your users can use your app longer,

00:07:01.010 --> 00:07:01.860
which is great.

00:07:01.860 --> 00:07:02.500
That's good for you.

00:07:02.500 --> 00:07:05.510
That means they like your app
and they get to play with it.

00:07:06.300 --> 00:07:11.690
So that's sort of a brief introduction
into why you should use vImage.

00:07:11.720 --> 00:07:12.590
I think three good points.

00:07:12.630 --> 00:07:16.050
It's simple, it's fast,
and it uses less energy.

00:07:16.190 --> 00:07:18.670
So what else do we provide?

00:07:19.240 --> 00:07:23.070
We provide a big variety of operations
and we're just going to kind of

00:07:23.430 --> 00:07:27.290
walk through each of the categories
and see roughly what's there.

00:07:27.370 --> 00:07:30.500
We just talked about convolution.

00:07:30.670 --> 00:07:32.910
Just to give you a little
bit more about that.

00:07:33.000 --> 00:07:36.600
As I mentioned, it's the weighted
average of nearby pixels.

00:07:36.720 --> 00:07:40.100
We also let you to supply a
bias that can be applied to it.

00:07:40.200 --> 00:07:43.400
That's often useful for certain kernels.

00:07:43.490 --> 00:07:46.600
And we allow you to use a different
set of weights for each color channel.

00:07:46.720 --> 00:07:49.600
And we give--I mentioned the
problem of edging before.

00:07:49.600 --> 00:07:52.930
We give you a bunch of
different ways to handle edging.

00:07:53.280 --> 00:07:55.600
You can do--use the background color.

00:07:55.600 --> 00:07:58.720
You can do edge extend where you
basically treat the edge pixels as

00:07:58.720 --> 00:08:00.600
though they went off to infinity.

00:08:00.720 --> 00:08:04.920
We also have a mode called truncate
and a sort of do nothing mode where

00:08:04.920 --> 00:08:06.830
you just assume the data's there.

00:08:08.110 --> 00:08:12.110
And then we also have
geometry operations.

00:08:12.290 --> 00:08:14.900
These are pretty self-explanatory,
but we'll have some

00:08:14.900 --> 00:08:16.880
pictures here to help out.

00:08:16.880 --> 00:08:19.670
We have rotation operations.

00:08:19.760 --> 00:08:23.350
We have shearing operations.

00:08:23.510 --> 00:08:26.110
can reduce and enlarge your image.

00:08:26.110 --> 00:08:29.180
I should mention that a lot
of these geometry operations

00:08:29.180 --> 00:08:30.550
need to do resampling.

00:08:30.720 --> 00:08:34.140
We use Lantos sampling,
so you're going to get really good

00:08:34.140 --> 00:08:36.340
high quality output images from this.

00:08:36.480 --> 00:08:41.200
Not only is it fast,
but you get really nice image quality.

00:08:41.200 --> 00:08:43.850
We also provide Affine Warp.

00:08:44.340 --> 00:08:48.100
which is just a general affine
transformation applied to the image.

00:08:48.100 --> 00:08:52.100
And we have Reflect which
is very straightforward.

00:08:52.900 --> 00:08:54.350
We have transformations.

00:08:54.480 --> 00:08:57.290
These operate on the
color space of the image.

00:08:57.380 --> 00:08:59.000
So they operate pixel by pixel.

00:08:59.000 --> 00:09:02.270
They do some transform to the colors.

00:09:02.470 --> 00:09:05.010
We have matrix multiplications,
which you can use for

00:09:05.010 --> 00:09:08.450
color space conversions,
like if you want to move

00:09:08.450 --> 00:09:10.440
from RGB to YUV or HSV.

00:09:10.550 --> 00:09:14.150
And you can also use them just to
directly manipulate the hue or the

00:09:14.150 --> 00:09:18.260
saturation of an image or to just
sort of twist the color space.

00:09:18.370 --> 00:09:21.910
We also have gamma correction
operations and we have fast

00:09:21.910 --> 00:09:24.600
polynomial and rational evaluators.

00:09:24.720 --> 00:09:27.940
Since you can approximate any
function well with polynomial

00:09:27.940 --> 00:09:30.680
or rational approximation,
this sort of lets you do just

00:09:30.680 --> 00:09:33.150
about anything you want to
the color space of the image.

00:09:33.250 --> 00:09:36.160
So these are just a few examples
of the sorts of things you can

00:09:36.160 --> 00:09:41.310
do with the transform operations,
the basically color space operations.

00:09:41.690 --> 00:09:43.210
We have morphology operations.

00:09:43.400 --> 00:09:46.010
These are kind of cool.

00:09:46.080 --> 00:09:49.950
Morphology operations basically give
you a way to sort of grow either the

00:09:49.960 --> 00:09:52.340
highlights or the shadows of an image.

00:09:52.380 --> 00:09:56.400
Min and max grow them sort
of in rectangular ways.

00:09:56.410 --> 00:10:00.180
Erode and dilate apply
a probe that you supply,

00:10:00.180 --> 00:10:03.000
which can be an arbitrary shape.

00:10:03.100 --> 00:10:05.600
And you can do some really
cool tricks like this.

00:10:05.600 --> 00:10:08.820
Like here we're going to use a star
shaped probe to sort of grow the

00:10:09.320 --> 00:10:12.080
highlights into stars in this image.

00:10:12.220 --> 00:10:15.100
You can do things that are a little
less gaudy than this with it too,

00:10:15.100 --> 00:10:18.010
but this is kind of a cool example.

00:10:18.540 --> 00:10:20.100
Another cool thing is histograms.

00:10:20.260 --> 00:10:22.760
We have a really nice set
of histogram functions,

00:10:22.800 --> 00:10:24.490
which you can use to
calculate histograms,

00:10:24.490 --> 00:10:27.530
but you can also use to perform
a lot of operations on the image.

00:10:27.700 --> 00:10:32.160
For example, you can take one image and
sort of infuse it with the

00:10:32.160 --> 00:10:34.220
color palette of another image.

00:10:34.490 --> 00:10:38.650
That's kind of a cool trick,
which I like a lot.

00:10:39.300 --> 00:10:43.130
And the last category of sort
of image operations we have

00:10:43.370 --> 00:10:46.870
is called alpha operations,
which are basically, in other words,

00:10:46.870 --> 00:10:48.100
for transparency.

00:10:48.150 --> 00:10:51.070
We give you a lot of ways to work
with the alpha channels on images

00:10:51.160 --> 00:10:52.600
and do those transformations.

00:10:52.720 --> 00:10:56.090
Sort of all the standard ways
that you might want to have.

00:10:56.450 --> 00:10:58.590
So what data types do we support?

00:10:58.650 --> 00:11:01.440
There are two main core formats.

00:11:01.440 --> 00:11:05.500
We support 8-bit unsigned integer,
four channels per pixel,

00:11:05.730 --> 00:11:10.150
and also four channel per pixel,
32-bit floating point formats.

00:11:11.270 --> 00:11:14.310
Within those data types,
we have a couple data

00:11:14.310 --> 00:11:15.600
layouts that we support.

00:11:15.600 --> 00:11:20.950
We have interleaved data layouts where
you have the first pixel alpha channel,

00:11:21.000 --> 00:11:23.840
red channel, green channel,
blue channel stored in some order,

00:11:23.840 --> 00:11:27.510
then the second pixels channels,
then the third pixels channels.

00:11:27.620 --> 00:11:32.490
We also have planar data layouts where
you have all the red channel image data,

00:11:32.490 --> 00:11:36.950
all the green channel image data,
all the blue channel image data.

00:11:37.110 --> 00:11:40.370
Sometimes planar formats are
really nice to work with,

00:11:40.370 --> 00:11:42.340
so these are an important thing.

00:11:42.420 --> 00:11:46.460
And we provide conversions
between all of these core formats.

00:11:46.550 --> 00:11:48.660
So we have conversions
from planar to interleaved.

00:11:48.940 --> 00:11:51.500
We have conversions between
8-bit and floating point.

00:11:51.580 --> 00:11:55.130
And we have conversion operations that
just let you swap around channel orders.

00:11:55.140 --> 00:11:59.110
So if you have an RGBA image and
you need a BGRA image or whatever,

00:11:59.270 --> 00:12:01.310
we give you ways to do that.

00:12:02.300 --> 00:12:06.850
Now we also support a lot of other
data formats as storage formats.

00:12:06.980 --> 00:12:12.410
So we have conversions that take
most common image formats and

00:12:12.410 --> 00:12:16.450
convert from those into one of the
core formats and then also from the

00:12:16.450 --> 00:12:19.320
core formats back to the storage
formats that you're going to want.

00:12:19.420 --> 00:12:22.270
So when you're working with image
data that's not in a core format,

00:12:22.270 --> 00:12:25.730
what your workflow usually looks
like is you have a conversion to a

00:12:25.730 --> 00:12:29.130
core format and then you do a bunch
of processing in that core format.

00:12:29.430 --> 00:12:31.770
And then if you need to get back to
one of the other formats at the end,

00:12:31.770 --> 00:12:33.860
you convert back after you
do all your processing.

00:12:33.860 --> 00:12:36.840
You generally don't want to be sort
of going back and forth all the time.

00:12:36.950 --> 00:12:43.540
We have very lenient data requirements
as far as alignment and things like that.

00:12:43.920 --> 00:12:46.820
8-bit data only needs
single byte alignment,

00:12:46.920 --> 00:12:50.060
so you don't really need
to worry about that at all.

00:12:50.180 --> 00:12:52.800
The floating point data types,
we require 4 byte alignment,

00:12:52.800 --> 00:12:54.560
which is just native float alignment.

00:12:54.640 --> 00:12:58.680
So there's no heavyweight vector
alignment you have to satisfy.

00:12:58.910 --> 00:13:00.900
And we don't need containerized data.

00:13:00.900 --> 00:13:05.450
We're not passing around objects,
none of this stuff.

00:13:05.560 --> 00:13:09.260
We basically want a
pointer to your image data.

00:13:09.570 --> 00:13:12.200
We want to know how tall the image is,
how wide the image is,

00:13:12.480 --> 00:13:16.380
and what the offset between
consecutive rows of the image is.

00:13:16.460 --> 00:13:19.470
With these two features,
what this means is that we can usually

00:13:19.470 --> 00:13:21.670
operate on your image data in place.

00:13:21.900 --> 00:13:26.000
You don't need to copy it into some
special structure for us to work on it.

00:13:26.310 --> 00:13:28.190
You just give us a pointer to your image.

00:13:28.340 --> 00:13:30.980
Call the function, and you're fine.

00:13:31.090 --> 00:13:33.960
So this reduces our overhead a lot.

00:13:33.990 --> 00:13:40.340
And as I mentioned, vImage is vectorized,
so we give you really good performance.

00:13:40.370 --> 00:13:44.400
And each function has some dispatching,
so it will use the best

00:13:44.480 --> 00:13:48.280
implementation available for the
hardware that it's running on.

00:13:48.450 --> 00:13:51.170
On the Mac,
this means that we take advantage

00:13:51.170 --> 00:13:56.500
of SSE3 or supplemental SSE3 and
SSE4.1 when they're available.

00:13:56.820 --> 00:14:02.260
and on iOS we take advantage of Neon,
which is the vector engine on ARM.

00:14:02.680 --> 00:14:06.410
If you're running on the
A5 processor in the iPad 2,

00:14:06.410 --> 00:14:09.250
then we'll also take advantage of
some other stuff like hardware,

00:14:09.400 --> 00:14:11.970
half precision to floating
point precision conversions and

00:14:11.970 --> 00:14:13.100
a few other things like that.

00:14:13.200 --> 00:14:15.550
If you're not running on the A5,
you'll fall back to a

00:14:15.550 --> 00:14:19.060
very fast software path,
so you still get good performance.

00:14:19.110 --> 00:14:22.740
So the thing I want to sort of
drum in here is that we take care

00:14:22.740 --> 00:14:25.760
of all these hardware specific
details behind the scenes so you

00:14:25.760 --> 00:14:26.740
don't have to worry about them.

00:14:26.750 --> 00:14:30.240
You write one code path and
you can get fast performance

00:14:30.240 --> 00:14:32.420
across the supported devices.

00:14:33.540 --> 00:14:37.140
and David Shull.

00:14:37.140 --> 00:14:45.840
We have a lot of things
to talk about today.

00:14:45.840 --> 00:14:52.150
We have a lot of things
to talk about today.

00:14:52.150 --> 00:14:52.150
We have a lot of things
to talk about today.

00:14:52.840 --> 00:14:57.160
And you can provide your own scratch
space to avoid getting hidden

00:14:57.160 --> 00:14:59.390
malloc calls or anything like that.

00:14:59.390 --> 00:15:01.200
If you're going to provide
your own scratch space,

00:15:01.200 --> 00:15:05.270
you need to first query the function
that you're going to use to find

00:15:05.270 --> 00:15:07.160
out how much space is needed.

00:15:07.600 --> 00:15:12.310
You can use the KV image get
temp buffer size flag to do that.

00:15:12.310 --> 00:15:15.790
So the way you use this flag is
that you call the function exactly

00:15:15.790 --> 00:15:20.160
the same way that you're going to
call it with all the same arguments,

00:15:20.160 --> 00:15:25.400
except the final argument to most
V image functions is a flags argument.

00:15:25.570 --> 00:15:30.720
And here you just add in the KV image get
temp buffer size flag and it returns the

00:15:30.860 --> 00:15:33.880
amount of scratch space that it requires.

00:15:34.000 --> 00:15:38.120
So typically the way you use this
is sort of what's illustrated here,

00:15:38.200 --> 00:15:40.630
where you get the amount of
scratch space that's needed,

00:15:40.630 --> 00:15:43.310
you allocate that much space,
maybe you have your own custom

00:15:43.310 --> 00:15:46.770
allocation or something like that,
you grab the space that's needed

00:15:46.770 --> 00:15:49.870
and then you can use that to
apply the filter numerous times

00:15:50.280 --> 00:15:52.150
using that same scratch space.

00:15:52.250 --> 00:15:55.330
The image is threaded using GCD.

00:15:55.430 --> 00:15:57.930
So on devices that have
multiple processors,

00:15:58.010 --> 00:15:59.620
we take advantage of that.

00:15:59.690 --> 00:16:03.560
And again, this just happens without
any involvement from you.

00:16:03.560 --> 00:16:03.860
You don't need to do that.

00:16:03.860 --> 00:16:05.800
to worry about it.

00:16:07.040 --> 00:16:11.400
So as I said,
just you write your code once.

00:16:11.400 --> 00:16:13.630
And if there's more than one processor,
you're going to get

00:16:13.630 --> 00:16:14.780
multi-processor performance.

00:16:14.920 --> 00:16:16.780
If there's only one processor,
that's fine too.

00:16:16.780 --> 00:16:19.120
You get good performance either way.

00:16:19.360 --> 00:16:22.800
and you can disable threading if you
already have your own threading engine

00:16:22.800 --> 00:16:27.540
or your own tiling engine and you
don't want us to conflict with that.

00:16:27.670 --> 00:16:31.890
You pass the kvimage do not tile
flag to the functions and then

00:16:31.890 --> 00:16:35.300
we just turn off tiling and you
can take care of all of that.

00:16:35.300 --> 00:16:38.500
So if you've already written some of
your own image processing algorithms,

00:16:38.560 --> 00:16:40.440
you already have your
own threading model,

00:16:40.560 --> 00:16:42.000
that's fine.

00:16:42.000 --> 00:16:43.790
You can still work with vimage.

00:16:43.900 --> 00:16:47.340
You can just swap us in for whatever
routines you're already using internally.

00:16:47.430 --> 00:16:53.280
And our APIs also provide ways
to handle edging that will

00:16:53.400 --> 00:16:55.170
support your tiling model.

00:16:55.290 --> 00:16:57.350
So if you look here,
suppose you had already written

00:16:57.360 --> 00:17:00.380
your own tiling model and you had
broken this image up into tiles.

00:17:00.460 --> 00:17:04.940
If you look at, say, tile six,
on the right hand edge of that tile,

00:17:05.000 --> 00:17:05.590
there's an edge.

00:17:05.770 --> 00:17:08.200
But on the top,
there's no edging and on the left,

00:17:08.200 --> 00:17:09.040
there's no edge.

00:17:09.120 --> 00:17:12.490
But maybe the bottom has a couple of
pixels that are influenced by the edge.

00:17:12.690 --> 00:17:16.900
We give you ways to handle this
even in the face of tiling.

00:17:17.010 --> 00:17:21.660
So you can specify that you only
need the edging to occur this way.

00:17:21.860 --> 00:17:24.150
So that's really nice.

00:17:24.560 --> 00:17:28.240
And so that's the quick
introduction to V-force.

00:17:28.240 --> 00:17:30.520
And at this point,
I'd like to bring up my colleague,

00:17:30.540 --> 00:17:34.610
Luke Chang,
who will tell you about V-force.

00:17:39.030 --> 00:17:41.310
Thank you, Steve.

00:17:41.310 --> 00:17:42.900
Hi, everybody.

00:17:43.070 --> 00:17:43.990
My name is Luke Chang.

00:17:43.990 --> 00:17:47.000
I'm an engineer in Vector Numerics Group.

00:17:47.000 --> 00:17:50.000
Today I'm going to tell you about VForce.

00:17:50.030 --> 00:17:51.450
In the following about
eight to ten minutes,

00:17:51.580 --> 00:17:55.480
I'm going to tell you what is VForce,
how to use VForce and why

00:17:55.480 --> 00:17:57.240
you want to use VForce.

00:17:57.240 --> 00:17:58.170
So let's get started.

00:17:59.370 --> 00:18:00.920
What is VForce?

00:18:00.920 --> 00:18:03.590
We made VForce to satisfy
your computational need.

00:18:03.600 --> 00:18:05.960
We know smartphones are
not just a phone anymore.

00:18:05.960 --> 00:18:07.660
There are tons of apps out there.

00:18:07.660 --> 00:18:09.960
Many of them process a lot of data.

00:18:09.960 --> 00:18:11.760
Don't even mention the iPad.

00:18:11.760 --> 00:18:13.160
Doctors are using it.

00:18:13.160 --> 00:18:14.490
Pilots are using it.

00:18:14.500 --> 00:18:17.670
So how do we process
those data efficiently?

00:18:17.670 --> 00:18:20.420
This is where VForce comes into play.

00:18:20.510 --> 00:18:23.760
VForce has elementary
math functions for arrays,

00:18:23.760 --> 00:18:26.480
so it handles array data efficiently.

00:18:26.480 --> 00:18:29.940
If you're a Mac developer,
you probably are

00:18:29.940 --> 00:18:31.820
familiar with it already.

00:18:31.970 --> 00:18:34.000
We introduced it in Mac OS X Tiger.

00:18:34.000 --> 00:18:36.310
Now we bring it over to iOS 5.

00:18:36.360 --> 00:18:39.510
Let's say you want to write
an app to generate sine wave,

00:18:39.510 --> 00:18:40.810
a signal generator.

00:18:40.810 --> 00:18:43.000
Here's one way you can do it in plain C.

00:18:43.010 --> 00:18:45.580
It's very simple, straightforward.

00:18:45.600 --> 00:18:49.620
You have output buffer and you
have input buffer indices because

00:18:49.620 --> 00:18:53.980
you probably want to generate a
frequency modulated sine wave instead

00:18:54.160 --> 00:18:56.550
of just a fixed frequency sine wave.

00:18:56.580 --> 00:18:58.650
And then you write a for loop.

00:18:58.920 --> 00:19:02.920
to calculate the sign value
of each index and store all

00:19:02.920 --> 00:19:05.420
the results to OutputBuffer.

00:19:05.430 --> 00:19:06.410
This is very simple.

00:19:06.420 --> 00:19:08.920
Nothing can be more simpler than that.

00:19:08.920 --> 00:19:10.920
But how can we use v-force for this?

00:19:10.920 --> 00:19:12.790
How can we improve this code?

00:19:12.960 --> 00:19:14.920
Here's how.

00:19:15.300 --> 00:19:17.720
First,
you have to include AccelerateHeader

00:19:17.930 --> 00:19:20.390
to have access to AccelerateFramework.

00:19:20.420 --> 00:19:24.230
And then you just have to
replace your for loop by a

00:19:24.380 --> 00:19:27.420
single function call to vvsignf.

00:19:27.420 --> 00:19:32.420
Vv is a function prefix to v-force
functions followed by its functionality.

00:19:32.420 --> 00:19:35.920
In this case, it's signf,
single precision sign.

00:19:35.920 --> 00:19:37.410
Takes three arguments.

00:19:37.420 --> 00:19:39.920
The first argument is the OutputBuffer.

00:19:40.150 --> 00:19:42.420
Second argument is the indices.

00:19:42.620 --> 00:19:47.420
And the third argument is the
pointer to the array length.

00:19:47.600 --> 00:19:50.920
So vvsignf will do the
same thing as a for loop,

00:19:51.320 --> 00:19:54.160
calculating the sign value of
each index and store all the

00:19:54.230 --> 00:19:56.720
results to the OutputBuffer.

00:19:56.850 --> 00:19:58.230
I said it's doing the same thing.

00:19:58.340 --> 00:20:00.220
Why did I say it's better?

00:20:00.280 --> 00:20:03.020
Well,
let's look at the performance comparison.

00:20:03.080 --> 00:20:09.140
This chart shows how many signs you
can calculate in one microsecond.

00:20:09.170 --> 00:20:13.910
As you can see, vForce is more than twice
faster than using a for loop.

00:20:14.150 --> 00:20:16.820
This means your app
will be more responsive.

00:20:16.890 --> 00:20:20.830
The user of your app don't have to
wait long enough to get the results.

00:20:20.900 --> 00:20:21.980
That's nice.

00:20:22.050 --> 00:20:24.510
But this is not the only
thing vForce has to offer.

00:20:24.720 --> 00:20:27.740
Let's also look at the
energy consumption.

00:20:27.810 --> 00:20:30.840
This is the energy comparison.

00:20:30.940 --> 00:20:35.390
The chart shows how much energy
is consumed per one signed result.

00:20:35.560 --> 00:20:41.300
Again, v4 is more than twice more energy
efficient than using a for loop.

00:20:41.370 --> 00:20:44.300
This means your app will run longer.

00:20:44.410 --> 00:20:48.170
If you're writing apps
for doctor or pilots,

00:20:48.170 --> 00:20:52.360
you really want your app to run longer,
right?

00:20:52.360 --> 00:20:52.360
So,

00:20:52.800 --> 00:20:55.600
We've seen that VVSciX is pretty good.

00:20:55.660 --> 00:20:58.900
What else is also available in VForce?

00:20:58.960 --> 00:21:04.280
We have transcendental functions, power,
sine, cosine.

00:21:04.600 --> 00:22:07.600
[Transcript missing]

00:22:07.990 --> 00:22:13.400
V4 supports both single and double
precision folding point numbers.

00:22:13.600 --> 00:23:59.800
[Transcript missing]

00:24:04.340 --> 00:24:05.540
Thanks, Luke.

00:24:05.570 --> 00:24:08.240
I just want to say we really
put a lot of effort into making

00:24:08.240 --> 00:24:10.240
a great math library on iOS.

00:24:10.300 --> 00:24:14.250
So the fact that VForce is able
to go even faster is actually

00:24:14.250 --> 00:24:15.390
really quite impressive.

00:24:15.590 --> 00:24:21.860
So now, that was the quick overview of
what we just introduced in iOS 5.

00:24:21.910 --> 00:24:24.810
I want to spend a little bit of
time talking about my favorite

00:24:24.810 --> 00:24:28.640
thing in the Accelerate Framework,
which is LAPack and BLAS.

00:24:28.700 --> 00:24:32.140
And we've improved these
tremendously for iOS 5.

00:24:32.140 --> 00:24:33.010
They're really nice.

00:24:33.100 --> 00:24:34.680
So I mentioned we improved them.

00:24:34.750 --> 00:24:35.490
How did we improve them?

00:24:35.490 --> 00:24:39.490
We improved the performance a lot.

00:24:39.930 --> 00:24:45.510
So, LAPAC and BLAS now,
the A5 processor and the

00:24:45.510 --> 00:24:49.030
iPad 2 has a great double
precision floating point unit.

00:24:49.120 --> 00:24:50.380
It's really fast.

00:24:50.520 --> 00:24:55.110
And we want to take advantage of
that in LAPAC and BLAS to give

00:24:55.140 --> 00:24:59.630
much better performance than
we've been able to give so far.

00:25:00.250 --> 00:25:03.460
How do you measure performance
of these libraries?

00:25:03.460 --> 00:25:08.190
One of the most commonly used
benchmarks is something called LINPACK.

00:25:08.500 --> 00:25:11.000
and this is a very old benchmark.

00:25:11.060 --> 00:25:13.980
It actually derives from an
old linear algebra library

00:25:13.990 --> 00:25:17.260
called the LINPACK library.

00:25:18.550 --> 00:25:24.770
and it basically measures how fast can
you solve a system of linear equations.

00:25:25.520 --> 00:25:27.240
When people talk about
the LINPACK benchmark,

00:25:27.320 --> 00:25:30.920
there's a lot of confusion because
it's not actually just one benchmark.

00:25:31.040 --> 00:25:35.180
It's three separate benchmarks
that all go by that name.

00:25:35.330 --> 00:25:38.210
The first one,
you measure how fast can you solve

00:25:38.290 --> 00:25:44.710
a system of 100 equations using a
reference implementation of the solver.

00:25:44.830 --> 00:25:47.540
Now,
that reference implementation comes from,

00:25:47.720 --> 00:25:49.970
as I mentioned, the LINPACK library.

00:25:49.970 --> 00:25:51.830
And it's not tuned to the hardware.

00:25:51.950 --> 00:25:53.380
It's nothing like that.

00:25:53.550 --> 00:25:57.060
The benchmark formally is in Fortran,
so it would be Fortran reference code.

00:25:57.190 --> 00:26:00.980
People have ported it to C and
Java and a lot of other languages.

00:26:01.060 --> 00:26:04.540
Most of the time you see people talk
about LINPACK benchmark numbers,

00:26:04.780 --> 00:26:09.830
this is the benchmark they're
talking about in the mobile space.

00:26:09.920 --> 00:26:12.340
When you see someone talk about
a new device's LINPACK score

00:26:12.340 --> 00:26:13.910
or something like that,
this is the kind of thing

00:26:13.910 --> 00:26:14.540
they're talking about.

00:26:14.540 --> 00:26:19.490
They're talking about reference code
running for a fairly small size matrix.

00:26:19.610 --> 00:26:23.080
The second benchmark is
a thousand equations,

00:26:23.160 --> 00:26:25.780
where you get to use your
tuned implementation.

00:26:25.870 --> 00:26:30.030
You get to go ahead and write the
fastest solver you can write and solve

00:26:30.030 --> 00:26:31.930
a system of a thousand equations.

00:26:32.040 --> 00:26:34.190
And the third benchmark
that goes by this name,

00:26:34.200 --> 00:26:36.740
this is the benchmark people
are usually talking about in the

00:26:36.760 --> 00:26:39.330
high performance computing world
when they talk about LINPACK.

00:26:39.400 --> 00:26:42.210
If you hear about some new
supercomputer that has,

00:26:42.210 --> 00:26:44.680
you know,
a hundred petaflops or something,

00:26:44.680 --> 00:26:46.870
that's this benchmark.

00:26:47.020 --> 00:26:50.120
This is sort of the no-holds-barred
division to LINPACK.

00:26:50.180 --> 00:26:52.710
You get to choose your problem size.

00:26:52.730 --> 00:26:58.620
If where your computer runs fastest is
on a 1.2 million by 1.2 million matrix,

00:26:58.620 --> 00:26:59.820
so be it.

00:26:59.820 --> 00:27:02.460
You get to use the fastest
solver you can come up with.

00:27:02.460 --> 00:27:03.820
It's the no-holds-barred division.

00:27:03.820 --> 00:27:06.220
So that's LINPACK.

00:27:06.220 --> 00:27:10.380
And when you see people talk
about LINPACK on mobile devices,

00:27:10.380 --> 00:27:14.620
you often see numbers like, you know,
35 megaflops or 50 megaflops.

00:27:14.640 --> 00:27:18.080
That kind of number gets
tossed around a lot.

00:27:18.080 --> 00:27:21.820
iPad 2, as I mentioned,
it's got a nice processor and the

00:27:21.820 --> 00:27:22.360
compiler does a pretty good job.

00:27:22.360 --> 00:27:25.330
So with the reference code,
we do a little bit better

00:27:25.330 --> 00:27:26.230
than that typically.

00:27:26.270 --> 00:27:30.550
We get about 90 megaflops on LINPACK.

00:27:31.270 --> 00:27:34.870
I think this is basically criminal
because you've got this beautiful

00:27:34.930 --> 00:27:39.180
hardware and you're going to run this
reference code on it that doesn't

00:27:39.340 --> 00:27:41.750
take advantage of it at all really.

00:27:42.430 --> 00:27:45.970
And so we want to see, you know,
what can we get on

00:27:45.970 --> 00:27:50.560
LINPACK using Accelerate,
using a tuned implementation, you know,

00:27:50.780 --> 00:27:54.010
actually take advantage of the hardware,
make it as good as we can.

00:27:55.120 --> 00:28:01.200
So on the iPad 2 using Accelerate,

00:28:02.100 --> 00:28:03.700
Forget mega flops.

00:28:03.700 --> 00:28:09.090
We give you 1.5 giga flops on Linpac.

00:28:09.110 --> 00:28:11.540
That's more like it.

00:28:11.540 --> 00:28:14.540
I should note that not all
of this work is in the seed.

00:28:14.540 --> 00:28:17.180
In the seed,
you'll get about 1.3 giga flops.

00:28:17.190 --> 00:28:21.340
When iOS 5 ships, you get the full 1.5.

00:28:21.340 --> 00:28:22.740
Even 1.3 is pretty good, though.

00:28:22.740 --> 00:28:27.530
That's 12 times faster than the
reference code or something like that.

00:28:27.820 --> 00:28:28.580
So I think that's great.

00:28:28.580 --> 00:28:31.170
And in fact,
the iPad 2 would have been one

00:28:31.170 --> 00:28:34.610
of the 500 fastest supercomputers
in the world in 1994.

00:28:34.780 --> 00:28:37.460
How cool is that?

00:28:37.460 --> 00:28:40.390
Yeah, that's awesome, right?

00:28:44.400 --> 00:29:48.900
[Transcript missing]

00:29:49.180 --> 00:29:53.160
and all the way up to a Mac Pro where
you get 80 something gigaflops

00:29:53.160 --> 00:29:55.500
of double precision performance.

00:29:55.580 --> 00:30:00.700
So, you know, three lines of code,
it's portable across all these devices,

00:30:00.750 --> 00:30:03.200
we give you really nice
performance on all of them.

00:30:03.250 --> 00:30:06.900
And that's really,
that's what we want from Accelerate.

00:30:06.950 --> 00:30:09.450
We want to give you something
that's easier to use than it

00:30:09.450 --> 00:30:11.100
would be to write your own code.

00:30:11.140 --> 00:30:13.530
We want to give you something
that gives you great performance

00:30:13.600 --> 00:30:16.100
across all the hardware we provide.

00:30:16.190 --> 00:30:19.790
And we want to give you something
that lets you use less energy,

00:30:20.080 --> 00:30:22.900
save the battery,
make your app last longer.

00:30:22.980 --> 00:30:25.100
If you want more information
about any of this,

00:30:25.180 --> 00:30:26.890
we have a couple people you can contact.

00:30:27.000 --> 00:30:28.980
There's documentation online.

00:30:29.150 --> 00:30:31.500
The V-Image programming
guide is really nice.

00:30:31.540 --> 00:30:35.000
And you can also, if you have questions,
post them to the Apple Developer Forums

00:30:35.100 --> 00:30:38.040
or just file a bug if something doesn't
work the way you think it should.