---
Title:  Working with iOS Accessories
Year:   2011
Web:    https://nonstrict.eu/wwdcindex/wwdc2011/201/

[!] This transcript was generated using Whisper, it has known transcription errors.
---

Good morning and welcome to our session, Working with iOS Accessories. I'm Emily Schubert and I'm responsible for our interface to accessories. iOS provides more opportunities than ever for applications to interact with accessories and external devices. In this session, we will talk about some of those opportunities and we will highlight for you what is new with iOS 5.

Last year in this session, we gave you an overview of all of the available interfaces. We also then went into some detail on Bluetooth and told you how best to make use of the iOS Bluetooth interfaces from an app and accessory perspective. Then we talked to you about the External Accessory framework, which was an exciting new feature. We talked about it in some depth.

Now today we have a number of new topics for you, and then we'll give you updates on some of the other topics. First, we will talk about AirPlay. This is a Wi-Fi feature and we will talk to you about what AirPlay means to you as both an app developer and an accessory developer.

We will then give you an update on Bluetooth and talk to you about what's new in Bluetooth for iOS 5, as well as our recommendations for how to make the best Bluetooth accessories or apps that really take advantage of our interfaces. We will wrap up with some updates on specific accessory features that are more general and include updates on EA in that section. Now I would like to hand it over to Peter Langenfeld to talk about AirPlay.

Thanks, Emily. Morning. I'm Peter Langenfeld, and I'm the feature manager here at Apple, and I'm here to talk to you guys about AirPlay today. So there's three basic things I want to cover. First, what is AirPlay? Why are we here at Apple so excited about it? Why do we think it's a great technology, and why do we think all of you should be really excited about it as well?

Next, for you iOS developers, how do you best take advantage of AirPlay in your own apps to make sure that your customers can get the kind of the premier experience, if you will, with AirPlay accessories and AirPlay technology? And finally, for you accessory manufacturers, I want to talk to you about kind of a couple of things that you should be thinking about when designing an AirPlay accessory, and a couple of things that are a little different than your normal accessory development and application. So let's go ahead and get started.

So what is AirPlay? AirPlay is a collection of wireless streaming services and technologies that's really meant to take the experience from the iOS device in the palm of your hand and to put it up on the big screen television, on your home theater, on pretty much any other accessory that isn't actually in the palm of your hand. It consists of audio, video, photo, and as we just introduced yesterday, now, mirroring.

So basically, AirPlay is designed to work around the Wi-Fi standard. That means you don't have to buy any special equipment. It just works with the router that's already in your home. You don't have to buy any packages. Basically, you buy an AirPlay accessory, you bring it home, get it on your network, and it just works. If you want a second one, you can buy a second one, but there's no need to buy any sort of system or integrated technology beyond your iOS device and the single accessory you just purchased in the store.

[Transcript missing]

On to photos. So photo streaming is once again to the Apple TV. And the idea is basically if you've created this great slide show on your iOS device, on your Mac, et cetera, and you spent all this time synchronizing your audio and designing your transitions perfectly, with AirPlay photos you can just stream it directly to your Apple TV and enjoy it with your friends and family a little easier than just trying to show them the small device in your hand.

If you decide to create a slide show kind of on the fly and you decide not to put all that time and effort into the transitions, you can also take advantage of the transitions on the Apple TV itself. So you'll look like a rock star even though you may not have spent the time to really become one.

Finally, the announcement of yesterday announced AirPlay mirroring. The idea between AirPlay mirroring is really that it's the same as the digital AV cable that we introduced the iPad to, just without the wires. So it's not the same as the digital AV cable that we introduced the iPad to, just without the wires.

It's the same mirroring function, mirroring service, you don't have to worry about having to walk over your TV, having an adapter, hooking it up, it goes straight to the Apple TV just from a single route selection. You can utilize games, web, apps, pretty much everything on your iOS device and put it up on the screen without having to worry about it. You get the same rotation capabilities you have with the digital AV adapter. When I rotate the iPad, I see it in landscape or portrait depending on the orientation. So what does that mean?

That means I can have a different display on my iPad than I'm seeing on the big screen television. So I can actually tailor my app to give a really great experience if I want to basically see a map or see some sort of status on my iPad but really focus on the action on the big screen or alternatively also the audio on the big home theater system.

So moving on, AirPlay Accessories. We have this great ecosystem of AirPlay Accessories that already exist. We're going to see an even bigger ecosystem of AirPlay Accessories this holiday season. And I want to get you guys excited a little bit about the concept that you can create accessories that really fit into any corner of the lifestyle of the users.

As you can see, we have units that have displays, units without displays, small sort of home theater systems. We have the Apple TV, the Airport Express. Pretty much anything you can imagine can be made into an AirPlay accessory. And I think there's a lot of space here. There's a lot of excitement. There's a lot of interesting things we can do for the consumer in terms of making sure they can enjoy their audio and their video kind of in every corner of their home.

So that's it. That's AirPlay. Hopefully you guys are a little more excited than when you started. You understand kind of generally what it is we're talking about and maybe some possibilities of what we can do. Let's talk through a few more specifics of actually using the individual AirPlay experiences. So on the left of the screen, you'll notice the iPod app. On the right, you'll notice the movie player. This is the standard view you get, standard transport control, standard volume controller.

And in this particular example, I'm looking at a network that has no AirPlay devices resident. Now I've gone on to a network where I do have AirPlay devices, and you'll notice the AirPlay icon has popped up automatically with no kind of effort or action from the user other than actually getting on the network itself.

Now, once I click on that icon, I get the route picker, and there's something I want to point out here in terms of a difference between the iPod application and the movie player application. You'll notice for the iPod application, all I have are audio endpoints. Obviously that indicates I can route the audio. It's the only thing you can route from the iPod, so I guess it's kind of obvious. But on the right, I have the movie player application. And you'll notice that I have both video endpoints, indicated by the movie screen, and audio endpoints.

So the difference there is that a video endpoint will route both the video and the audio to that endpoint, whereas the audio route will continue to show the video on the iOS device in your hand, and will actually take the audio and route it up to the home theater system, speaker system, et cetera. Okay.

So I'm going to select each of these. You'll notice that the focus point changes. And now that I'm actually streaming the AirPlay stream to the endpoint, the AirPlay icon turns blue. And in the case of the movie player, I also get a nice message that indicates where that video is going.

There's one thing I want to point out, just because I mentioned it earlier, and that's with the memory of volume for each of the AirPlay accessories, you'll notice that the system volume on the left has now shifted up to the right and has remembered what that AirPlay endpoint was set to as soon as I connect to it, which is great.

Okay, so how do you actually use this in your application? For audio, basically you want to use the Media Player framework, and you just want to use the standard volume view. That's the one thing you have to remember, just the standard volume view for your application, and you'll get all the features I just showed you.

For video, you can use either MP Movie Player Controller or AV Foundation, which will give you a little more control over what you want to do with the video. But it's really just as simple as audio. All you have to do is use the standard system level services that we offer to make sure you can get both of those features integrated to your own application. Everything else is taken care of for you.

Basically, we trigger the route button automatically. The user can access all the volume levels, the routes, et cetera, without you having to worry about anything. You don't even have to worry about when it's routing to AirPlay. It's all kind of taken care of for you. It's kind of a blind transition, and there's no differences in terms of the way you control things.

There are a couple more details that I want to point out just because they're very useful to make sure that your user can have the best experience. The first, if you use the MP Volume View, make sure you don't hide the Route button. You have to do this actively.

The default is definitely that the Route button is visible, but if you hide it, it makes the user have to go to the multi-touch -- excuse me -- the The switcher, basically, from the home screen. So you have to double-click the home button, and you have to go to the multitasking UI to select your AirPlay route, which is kind of inconvenient. So just don't say no to this, and you'll be fine. Finally, on the video side, we ask that you explicitly allow AirPlay streaming from your different media assets. In iOS 5, this is done for you by default, but in iOS 4.3, these are default set to no.

So if you want to be backwards compatible, it's best to go through and actively set these to allow, and yes. Obviously, this is just kind of the one or two high-level things you guys should do. It's fairly simple. If you want more information, we have lots of sessions on core audio, core video. I encourage you to go to those, ask questions at the labs, make sure you know exactly what it is you want to do. But from a high level, these are the points you really need to memorize and know about. Okay.

Let's move on to using mirroring. So the way you use mirroring is you actually go again to the multitasking UI. You pull it up with the double click and the slide to the left. And obviously when you get on a network where there's an AirPlay device, you get the AirPlay icon. I select it.

I have routes just like before. But there's a slight difference now. When I select a route that actually has the ability to mirror, I get this nice little switch, mirroring, on or off. If I leave it off, basically I get the same functionality I had when I was using AirPlay. And I showed you the movie player. So I'll just stream all of my media assets directly to the screen. But when I turn it on, I now get mirroring functionality.

It's just that simple for the user. Obviously when I'm actually routing in the multitasking UI, I have the glowing blue AirPlay icon. And once I actually go back to the home screen, I want to point out the status bar. It now turns blue. And in the upper right, there's actually a little AirPlay icon to remind the user that they're mirroring everything on their screen to the big screen television.

So this is a situation of just standard mirroring. Basically, I see the exact same thing on the iPad that I see on the television. It's a great user experience. But I can enhance that even, if I so choose for my app, to basically give the UI screen view or a different view between the iPad screen and the television screen. This can really be used to enhance your game, to enhance your app, to really give a targeted experience for AirPlay. But it's something you have to do a little more actively.

So how do you take advantage of it inside your app? If all you want is mirroring, it's blank. You do nothing. You just leave the system to whatever it wants to do and we'll take care of it for you. But if you're looking to create that kind of tailored view for the actual AirPlay experience, what you'll want to do is use UI screen.

There's a session specifically on AirPlay mirroring and UI screen here at WWDC. I encourage you to go and get all the details there. Basically, the overall function is that you get informed when an external screen arrives, you get informed of the resolution of that screen, and you can decide as an application what it is you want to do with that screen in terms of frame rate resolutions, etc.

There's a lot of detail there. I highly encourage you to go to that session, go to those labs to really ask questions about this and figure out how to really take advantage of the mirroring functionality if you really want your app to have a tailored mirroring experience. Everything else is handled by the system automatically. So just like all the other AirPlay technologies, we handle the routes, we handle the volume, etc.

So you don't need to worry about it. Okay. So we're going to move on a little bit from AirPlay and talk about some technologies that, although really useful to be leveraged by AirPlay, are also applicable to... all of our other transports in terms of accessories. First is using metadata.

We're excited to announce that in iOS 5, we now allow metadata to come from third-party apps and be used on the system and distributed to accessories to inform the user what it is you're playing back in terms of what your media asset is right now. So this is the lock screen.

And basically, as you can see, we have audio on the left, we have video on the right. I can see the album title, I can see the artist name, I can see the track title. And again, if I now go to... to the multitasking UI, you can see the track title there. All these are standard things that you've seen before.

What we also get, though, is for AirPlay accessories, there's the ability to take that metadata and distribute it outwards so that you can see everything that's playing. And so you have a television system on the left, you have a small speaker dock on the right. Great example of ways to use metadata really thoroughly and inform your user, not only from the palm of their hand, but from the device they're actually enjoying the music experience on as to exactly what it is they're listening to.

And so we have a lot of different ways to use the metadata, including the album art. This goes beyond just AirPlay accessories, though. And we also have automotive systems, standard wired MFI accessories, wireless MFI accessories. Everything can use this metadata information from a third-party app. So it's really interesting and important, I think, that your app take advantage of this in iOS 5.0.

So how do you actually provide metadata from your app? Well, the first thing I want to touch on is to make sure you guys realize you should provide all the metadata that you can from your app. There's no reason to gate your user's experience by you just deciding, oh, well, all they can see is the album title right now, so that's all I'm gonna provide. The more information you give the system, the better experience the user can have at the end of the day, and the more expandability the system has into the future.

So I really encourage you to give all of the metadata you have accessible to yourselves and make sure that the system is fully informed so that we can make sure that all of the accessories are fully informed. The system will take care of formatting, consistency, et cetera, so you don't have to worry about trying to do anything special. Just give us the complete data and we'll handle the rest for you. It should really be that easy.

How do you actually do this in code? Well, first you want to create a dictionary for the actual metadata information about what it is that's now playing. Essentially, in this case, we have the property of the artist and of the track title. Obviously, we just create a nice variable. If you want to blank the metadata or you don't want to show any metadata at all, you'd set that variable to nil. And then we simply assign the now playing info center information to that variable. And that's it. You're done.

This is the extensive list of everything you can provide to the app, and once again, I want to stress, and you'll hear this over and over in this presentation, please provide as much of this information as possible and we'll try to utilize it to the best of our ability.

And I think it's a really great experience if users know exactly what it is that's going on in the system at the time, instead of having that blind experience of maybe seeing one piece of metadata and hearing something else. So please go through in that, start using metadata in iOS 5.

The next thing I want to talk about is using playback controls. This is something we introduced last year and talked a little bit about, but I think it's more important than ever now that we have AirPlay involved. Basically, you can do play, pause, stop, next, previous, all sorts of media controls directly from your device, directly from your accessory.

So not forcing the user to go back to the iOS device and worrying about having to change tracks, et cetera. They can do it right from the remote control of their device, right from the front panel of their device. It's a great experience. It applies to AirPlay, Bluetooth, MFI accessories, even the headphone remote. So all you have to do is handle this once and we take care of distributing it out to all the accessories that may be used at one time.

How do you do this in code? Well, most importantly, you have to turn it on. So make sure you start receiving remote control events first and foremost. And then create a handler. Now, when you create the handler, only handle those events that mean something to your app. Events only get sent to the Now Playing app.

So it's not like somebody else is going to be able to handle it. So you can't just send it to the Now Playing app. It's not like somebody else is going to be able to handle it. It's not like somebody else is going to be able to handle it. So you can't just send it to the Now Playing app.

It's not like somebody else is going to be able to handle it. It's not like somebody else is going to be able to handle it. So it's not like somebody else is going to get the events if you don't handle them. And in this case, we're showing just how to handle a Play event. But it's really just a simple case switch. Go through and handle Play Pause next if you don't handle previous. Don't handle previous. It'll drop on the floor. No one will be the wiser. It'll really be a great experience and it'll match the UI of your application.

This is the extent of the remote playback controls available to you. Again, use everything that makes sense. Not all accessories may have all of these, but you'll receive all the events that come from the specific remote controls of that accessory. So again, don't worry about trying to predict where the user will be using it. Just support everything that you can from the API.

A little bit of summary. So I think the biggest thing to stress is just make sure you're using standard audio and video controls in your apps. It really helps the user have a consistent experience. It really makes sure that you don't have to worry about going out to the multitasking UI and trying to fiddle around and make sure everything works just the way you want. If you just use the standard APIs that we provide, you'll get the standard experience that we've designed.

Make sure you don't hide the route button. I think that's an important one for convenience. Make sure you do allow AirPlay video to make sure that your media assets are backwards compatible to iOS 4.3. And then the other two that apply not just to AirPlay but to all the other accessories, make sure you provide as much metadata as possible to the system and start handling remote playback controls to make sure that the user doesn't have to go back to the iOS device to figure out what's going on or how to get to the next track or how to skip through the functionality that you're using. that you've really designed and given a great experience for already.

Let's move on to accessories a little bit. This is the AirPlay technology badge. This is the icon that you and users will see on packaging throughout the world now and this coming holiday season to indicate that a device or an accessory is fully compliant with the AirPlay standard.

So it should be, the idea is that you walk into a store, you see this icon, you can pick it up and have confidence and know I take this device home and I install it onto my network and I have the same experience that I would have with an Apple product and of the same quality.

So how do you get started in this program? Well, just like most of our other accessory technologies, this is the standard MFI program. You go to developer.apple.com/mfi and you can get all the documentation, all the details of what you need to do to become and create an AirPlay accessory.

And it's a great place to go if you're interested in accessories in general because you not only can get all the information about AirPlay, you can also get the information for all sorts of accessories and how to support them. So it's really a great place to get started and a great source for information.

best practices. Obviously, the full documentation is available through the MFI program, but I think there's a few things you should be thinking about if you're conceiving of an Airplay accessory. First, make sure you take full advantage of the information you've registered for. What does that mean? Just as I told the iOS developers that they should be taking advantage of the full metadata and full controls that are available to them, if you have a product and you're conceiving of a product either with the screen or with a remote control, make sure you don't limit what they can do with it. Make sure it makes sense for your product, of course. We're not telling you you should or shouldn't put a screen on it, but if you put a screen on, don't limit the user to just using album title or just seeing track title.

Give them the opportunity to see all the rich information that's provided. It's really a better experience. It really gives them a richer experience. There's no reason to restrict it. So we really encourage you to take full advantage of the information that's available to you. Next up, make sure you're available to the user whenever possible.

The most frustrating experience is when you really want to listen to something on your home theater and you're looking at it in the palm of your hand, but you have to pause it, you have to get up, you have to walk over, you have to turn on the device, then you have to go back to your iOS device, you have to select the AirPlay route, and then start everything.

It's really a better experience if you can just select the AirPlay route as soon as it occurs to you from the palm of your hand. We understand there's power concerns, we understand there's sleep/wake issues, there's all sorts of regulations surrounding this. We're not telling you to violate those, but please keep this in mind and make sure that you're taking full advantage of the power regulations that you have and make sure that you're available as often as possible so that you can really have that kind of continuous user experience where I'm listening to it on my iOS device, I realize I want to listen to it on a bigger, richer system, and I just press the button and go.

Finally, I want to encourage you all to think about how to get on the network. Getting on the network is key in terms of a first experience with your particular AirPlay accessory. Obviously, standard Wi-Fi is not particularly complicated, but especially if you don't have a display and you need to start entering SSIDs and you need to start entering security information, you want to give this some serious thought to make sure the user doesn't come home, is super excited about their AirPlay accessory, and then all of a sudden has to go through this really complex process to get it set up on their network. You don't want them to start off frustrated. You don't want them to have a negative opinion about your product before they even start. So really put some serious thought into this and figure out how to do it right, how to give a great experience to your users.

Okay, so that's pretty much it on AirPlay. I hope you guys are a little more excited, a little more interested. Obviously, we'll be available in the lab session to answer any questions you might have. And I hope you start using AirPlay as a technology in both your apps and your accessories. And kind of now staying on the wireless topic, I want to bring up Brian Tucker to talk through Bluetooth and iOS so that you can understand how to use that technology as well. Thank you.

Good morning. My name is Brian Tucker and I'm the Senior Software Engineering Manager for Bluetooth Technologies. Pretty much across our entire iOS platform.

[Transcript missing]

So with hands-free up till now, we've been limited to 8 kilohertz 16-bit audio. And it works, but it doesn't sound really great. So the standards body has implemented an update to this to support wideband speech. So we're supporting this in iOS 5. We're using the standard modified SBC codec for this. It's a 16-kilo audio sample, but it does not increase bandwidth consumption.

So we're still looking at 64k streams up and down. So the big win here, obviously, is a lot better audio quality. And so the advantage that you're going to see as a consumer and as a developer is much better processing capabilities. For example, voice recognition is going to work a lot better. Voice recording is going to sound a lot better.

Products like FaceTime or a VoIP app is going to sound a lot better. And ultimately the goal here is to try to create a seamless connection or a transparent, just as you would perceive if you're using the microphone on the phone or if you're using the microphone on a headset.

So with this, we also give apps the ability to use audio from the headset and to send audio to the headset. So the voice capabilities of WBS can now apply to third-party apps. So you can be tunneling your audio from a VoIP app, for example, through a hands-free device. So a user could theoretically make a VoIP call mobily in a car.

And then finally, we're allowing now multiple MFI accessories to be connected simultaneously via Bluetooth. With wired, this wasn't nearly as much of an issue. But with Bluetooth, you can have multiple accessories, and we're allowing that to happen with Bluetooth products or with the Bluetooth transport. So state of iOS Bluetooth, this is kind of your menu of features that are available to you. HFP 1.6 is new. We covered that. AVRCP 1.4 is new. All of the other things are available. So we're allowing that to happen with Bluetooth. And then, of course, we still support the iPod Accessory Protocol. Okay.

Guidelines. So last year, we didn't have this available yet. But now, this is available to you from the iOS Dev Center. You'll see it right on the website, plain as day. And this is a living document. So every time that we update our software or we update Bluetooth on our platform, we update this document. And this talks about things like what we support.

What best practices we encourage you to develop for in terms of audio codecs, compression ratios, stiff modes, everything that you should run into as a Bluetooth accessory manufacturer or an application developer you should need. And if you don't see it in there, let us know, and we'll keep updating it.

So speaking of best practices, there are some key things that I think everybody should strive for. One of them is use the latest Bluetooth standards. Bluetooth 2.1 plus EDR or Bluetooth 3.0 which has EDR built in. Implement these in your products. Your users are going to love you for this.

Some key things off of that, use device ID profile. In our products, in every iOS platform that we develop, we provide information in the device ID profile. So if you're an accessory and you connect to iOS, you know exactly what you've connected to. What hardware, what version of software, etc. What version of controller, all of that information is developed to you in device ID. And we like the same information from your accessory, hopefully.

[Transcript missing]

Topology Management. So one of the big things we're running into right now is coexistence. We have more and more accessories trying to connect to iOS at one time, and we have to try to keep all those devices working together. So we have to manage antenna time.

And when we have Wi-Fi connected and we have a keyboard connected and we have a headset connected and we're tethering with somebody else's phone and my, you know, on and on and on, and we try not to limit you from doing all this, well, in order for us to get that to work, we have to be the master. But unfortunately, we're not always able to do that because accessories don't allow us.

Some key points here, the master always has to be controlling timing and the client always has to adhere to the master's timing. So let's look at a typical use case. So let's say we have our iOS device here and you have an accessory and the accessory connects to the iOS device. So in that case, the accessory is master. The iOS device is slave. So let's say we have another accessory.

So the first accessory, let's say it's a headset and you're streaming HTTP audio. And the second device is a keyboard and it connects. Well, in that case, it's master as well. But let's say the accessory, the first device doesn't allow for a topology switch. So if we include Wi-Fi in this whole thing and we're connected to a base station, the entire environment, the whole ecosystem is severely, severely limited. So if we have a device that's connected to a base station, the entire environment, the entire ecosystem is severely limited because we can't control the timing for Wi-Fi nor can we control the timing for the accessories.

So the way that we fix this is if we have this accessory, we bring the other accessory into the picture, the first accessory can be master. We don't have a problem with that if it's one device. But as soon as the second accessory connects, the iOS device is going to try to do a role switch. And the idea here is it wants to stay master of the ecosystem.

So that when Wi-Fi comes along and it connects, the overall experience is much better. We can now allow for time for Wi-Fi to have maximum bandwidth, and we can allow A2DP to get all of its packets across, because we're controlling those windows of time. Time spaces are, unfortunately, we have not conquered physics yet, so we have to adhere to time space.

So key things to look at here, allow iOS to do topology role switching. This is being covered in the Bluetooth standards body right now, and it's being heavily debated. But in cases where you have a device such as an iOS device that has to manage multiple accessories, allow for role switching.

And then secondary, which I know is somewhat contradictory to what I'm saying, but allow the iOS device to politely refuse a role switch. Because if it's running a scenario where it has multiple Bluetooth accessories, and Wi-Fi, it may say, "No, sorry, I can't let you be master right now. I've got to deal with all these products." Okay, and then one other issue.

It's probably not surprising to you that audio is really, really important for us. We want the best possible experience for the consumer, whether they're on a phone call, whether they're listening to music, whether they're watching a movie. We want it to sound really, really good. So there's some key things that we think we can do to improve that. First, use wideband speech. We talked about that already, but if you're going to be making accessories in the future and it involves SCO audio or mono audio, use wideband speech.

Secondly, with HTTP, we realized that SBCs are a requirement when you're implementing an HTTP profile. So if you're going to do that, use standards or use parameters that generate the best possible audio quality. And the key point here is the bit pool, primarily. You should shoot for a 53-bit pool, which is going to generate about a 370 kbps stream. And you should be able to manage that in modern headset controllers.

But beyond that, if you can, look at AAC. We implemented this last year, and we're starting to see devices on the market that are using AAC today. But implement this. Big win for two reasons. One, it uses about 40% of the data bandwidth that a high-quality SBC stream uses. So rather than 350 or 360 kbps streams, you're looking at 128 kbps stream with an AAC stream. And then secondly, it just sounds better. I mean, both subjectively and objectively measured, AAC truly sounds better than SBC.

Okay, so in summary, two key points. Go to the design guidelines document. Look that up. You'll find an iOS dev center. Like I said, we keep this up to date as much as we possibly can. Definitely review that. And then secondly, if you have questions, you can ask our Dev Relations team, but you can also get involved with us by joining our mailing list. And we all monitor this mailing list.

So if you have a question or if you're working on a particular implementation, send your question on that mail list and we will respond to it, usually within a day or even immediately, depending upon whether we're at our computer or not. So that's Bluetooth. So I'd like to bring Emily back up to cover accessories and we'll go from there.

Now I have some more general accessory related updates for you. Some of you might not be familiar with the term IAP. We use it every once in a while. IAP stands for iPod Accessory Protocol, and it allows an accessory to communicate with and control iOS device, an iOS device. And if you want any more information on IAP or if you want access to the hundreds and hundreds of pages of specifications, just get in touch with the MFI program.

First, I would like to talk to you about the External Accessory framework, which we sometimes call the EA framework. EA allows an app to connect to a compatible accessory and then exchange arbitrary data with it over a protocol of the developer's design. This has been exciting for a lot of developers.

They've had an opportunity to make accessories in all different categories that really weren't available before. I'd like to give you some examples of what we've seen come to market. This has been available since version 3 of the OS. I'd like to give you a few examples out of the hundreds and hundreds of accessories that are out there, just to show you the breadth of opportunities that the developers have found. Here's the iBike Dash cycling computer. And the iGrill grilling and cooking thermometer. Very useful.

We also have the iHealth Blood Pressure Dock. So these devices exist in every vertical from home health to automotive and have really made for some wonderful user experiences. We're very excited about what's out there already and about what's coming down the pike for our developers and for our customers. Really a lot of great products that drive a lot of excitement about the iOS platform. All the things that our users can do with this one device that they bought.

They can buy all of these other accessories and really make it the center of their lives. So what's new for the EA framework? What have we added for iOS 5? Well, first, we've added the ability for apps that can already multitask to continue talking to their connected accessory while they're in the background.

Yeah, long awaited. And this, I think, is key for a number of use cases. So, for example, an audio app or someone who is using core location, when they go into the background, they can keep that EA session open and continue talking to their app. I mean, really for a lot of navigation apps in particular, this is just critical. So, very excited about this. We've also added the capability for an accessory to trigger a notification to the user that will then allow the user to launch an app.

[Transcript missing]

So another update that I have for you today concerns accessibility. Apple is dedicated to accessibility. On both Mac OS and iOS we have a number of features that are aimed specifically at accessibility and our support is really legendary. It's something that users rave about. Last year we talked about voiceover, which gave users who can't see the screen the ability to remotely with an assistive input accessory navigate the OS and listen to where they are on the device.

Now we're adding a new feature with iOS 5 called Assistive Touch. Assistive Touch allows someone who can see the screen but might not be able to manipulate it with their hands, manipulate it remotely. And so they'll use an accessibility input accessory to navigate around the screen and they'll get a cursor like the one you see circled here on the left.

They also have access using other special features to multi-touch inputs. So they really get to do just about anything that I can do on this device, they can do as well, which is really liberating and I think quite a wonderful feature for iOS 5. There's going to be a session on accessibility and they will show a demo in that session. And so that's something that is accessory related. And that we hope accessory manufacturers will take advantage of, much like they have voiceover with iOS 5.

[Transcript missing]
