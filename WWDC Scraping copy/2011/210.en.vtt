WEBVTT

00:00:09.600 --> 00:00:10.670
Good morning.

00:00:10.850 --> 00:00:13.800
Welcome to Mastering
Grand Central Dispatch.

00:00:13.800 --> 00:00:14.890
My name is Daniel Steffen.

00:00:15.100 --> 00:00:19.480
I'm the engineer responsible
for GCD on the CoreOS team.

00:00:19.800 --> 00:00:24.720
And as you know,
we've introduced Grand Central Dispatch

00:00:24.930 --> 00:00:28.900
in Mac OS X Snow Leopard and
were pleased to bring it to iOS

00:00:28.900 --> 00:00:30.740
4 last year here at the conference.

00:00:30.740 --> 00:00:37.480
It has been very widely adopted in both
your apps and in Apple's frameworks.

00:00:37.820 --> 00:00:41.860
And indeed, as you may have seen in
other sessions this week,

00:00:42.050 --> 00:00:46.250
we are introducing a large number
of new APIs in line and iOS 5 that

00:00:46.490 --> 00:00:49.680
coordinate or relate to GCD in some way.

00:00:49.680 --> 00:00:53.390
So it's becoming an ever more important
core technology for you to understand

00:00:53.390 --> 00:00:57.350
if you do any kind of asynchronous or
concurrent programming on our platform.

00:00:57.360 --> 00:01:00.830
It's worth pointing out that
the GCD API is identical

00:01:00.900 --> 00:01:04.960
across our platforms,
so you can use the same functionality

00:01:04.960 --> 00:01:06.700
on both Mac OS X and iOS.

00:01:07.720 --> 00:01:12.150
And it will,
GCD will scale across our hardware

00:01:12.150 --> 00:01:17.180
from a single core iPhone all
the way up to a 24 core Mac Pro.

00:01:17.180 --> 00:01:21.600
So today we're going to give a
very brief introduction to GCD.

00:01:21.600 --> 00:01:25.210
There was a more in-depth introductory
session yesterday that I hope you managed

00:01:25.330 --> 00:01:27.650
to catch if you're new to the technology.

00:01:27.700 --> 00:01:32.380
And we'll look at what is new in
GCD on Mac OS X line and iOS 5 and

00:01:32.380 --> 00:01:35.380
give some advanced usage of GCD API.

00:01:35.380 --> 00:01:36.960
So what is GCD?

00:01:37.620 --> 00:01:40.150
So the very core of
GCD are blocks and queues.

00:01:40.320 --> 00:01:42.070
Why blocks?

00:01:42.120 --> 00:01:47.410
These are very useful in asynchronous
programming to encapsulate units of work.

00:01:48.150 --> 00:01:52.200
Along with the associated state,
so in this example here,

00:01:52.220 --> 00:01:57.410
we create a block and it encapsulates
some piece of work that we want

00:01:57.470 --> 00:02:01.170
to do asynchronously and the
block captures the state that

00:02:01.260 --> 00:02:04.480
it needs from the surrounding stack.

00:02:04.540 --> 00:02:04.690
Along with the associated state,
so in this example here,

00:02:04.690 --> 00:02:04.840
we create a block and it encapsulates
some piece of work that we want

00:02:04.840 --> 00:02:05.010
to do asynchronously and the
block captures the state that

00:02:05.010 --> 00:02:05.500
it needs from the surrounding stack.

00:02:05.720 --> 00:02:08.630
For instance, here,
it will retain the object and

00:02:08.630 --> 00:02:10.460
copy the integer argument.

00:02:10.470 --> 00:02:13.940
So once that is done,
we pass that block to a function that

00:02:13.940 --> 00:02:18.770
will execute the block asynchronously
and now we can safely change the integer

00:02:18.850 --> 00:02:23.470
or even release the Objective-C object
without affecting the block.

00:02:23.560 --> 00:02:23.560
Along with the associated state,
so in this example here,

00:02:23.560 --> 00:02:23.560
we create a block and it encapsulates
some piece of work that we want

00:02:23.560 --> 00:02:23.560
to do asynchronously and the
block captures the state that

00:02:23.560 --> 00:02:23.560
it needs from the surrounding stack.

00:02:23.560 --> 00:02:23.560
For instance, here,
it will retain the object and

00:02:23.560 --> 00:02:23.560
copy the integer argument.

00:02:23.560 --> 00:02:23.560
So once that is done,
we pass that block to a function that

00:02:23.560 --> 00:02:23.560
will execute the block asynchronously
and now we can safely change the integer

00:02:23.560 --> 00:02:23.560
or even release the Objective-C object
without affecting the block.

00:02:23.560 --> 00:02:26.560
Queues are the central concept in GCD.

00:02:26.560 --> 00:02:29.220
They are just a very
lightweight list of blocks,

00:02:29.220 --> 00:02:31.850
really, and it is very cheap to create
queues so it's fine to have

00:02:31.930 --> 00:02:33.560
many queues in your application.

00:02:33.560 --> 00:02:37.550
The enqueue and dequeue
operations of our queues are FIFO.

00:02:37.550 --> 00:02:41.480
So, blocks get dequeued and executed in
the order that they were enqueued.

00:02:41.560 --> 00:02:45.560
And importantly,
the enqueue operation itself is atomic.

00:02:45.560 --> 00:02:48.640
So, it is safe for multiple
threads to enqueue onto a

00:02:48.710 --> 00:02:50.560
single queue at the same time.

00:02:50.560 --> 00:02:52.770
These properties,
along with the fact that serial

00:02:52.960 --> 00:02:57.660
queues execute blocks one at a time,
make these queues useful for

00:02:57.760 --> 00:03:00.520
synchronization and serialization
purposes in your application.

00:03:00.590 --> 00:03:02.560
We also provide concurrent queues.

00:03:02.560 --> 00:03:05.560
These execute multiple
blocks at the same time.

00:03:05.560 --> 00:03:09.010
There's still dequeue blocks in order,
but they will,

00:03:09.010 --> 00:03:12.540
because they can execute concurrently,
and these blocks may then

00:03:12.540 --> 00:03:13.560
complete out of order.

00:03:13.770 --> 00:03:17.500
Another way to get concurrency with
GCD is to use multiple serial queues.

00:03:17.560 --> 00:03:21.360
That works because all of our
queues execute concurrently

00:03:21.510 --> 00:03:23.560
with respect to other queues.

00:03:23.560 --> 00:03:25.560
So, this is an example of that in action.

00:03:25.560 --> 00:03:28.560
Here we have two serial queues
with some blocks enqueued.

00:03:28.560 --> 00:03:31.720
And we have a worker thread on
one CPU that starts to dequeue

00:03:31.720 --> 00:03:33.550
some blocks and execute them.

00:03:33.560 --> 00:03:38.560
If a second worker thread can come along,
it can dequeue from the other queue,

00:03:38.560 --> 00:03:40.710
and as you can see,
we have some concurrency where

00:03:40.710 --> 00:03:41.560
there was overlap between blocks.

00:03:41.580 --> 00:03:45.780
Similarly, with a concurrent queue,
here we have two worker threads that

00:03:45.970 --> 00:03:50.560
dequeue items from the one concurrent
queue and execute them concurrently.

00:03:50.560 --> 00:03:53.550
Very briefly,
the API to submit blocks to queues

00:03:53.740 --> 00:03:56.560
is DispatchAsync and DispatchSync.

00:03:56.560 --> 00:04:00.660
DispatchAsync just enqueues
a block and goes along,

00:04:00.930 --> 00:04:04.480
whereas DispatchSync will enqueue
the block and wait until that block

00:04:04.480 --> 00:04:06.560
is actually executed on the queue.

00:04:06.560 --> 00:04:10.560
You can also delay the
submission of blocks to a queue.

00:04:10.560 --> 00:04:12.630
You can also delay the submission
of blocks to a queue by

00:04:12.720 --> 00:04:16.880
calling the DispatchAfter API,
or concurrently submit and execute

00:04:16.880 --> 00:04:21.560
a single block many times to a queue
by calling the DispatchApply API.

00:04:23.030 --> 00:04:28.500
We also allow you to suspend and resume
execution of a dispatch queue by calling

00:04:28.500 --> 00:04:31.730
the Dispatch Suspend and Resume APIs.

00:04:32.170 --> 00:04:34.480
This is an asynchronous
operation suspension.

00:04:34.480 --> 00:04:38.380
It will take effect at the earliest
possible opportunity on a queue,

00:04:38.400 --> 00:04:42.240
meaning it won't interrupt any ongoing
blocks that are already executing,

00:04:42.240 --> 00:04:44.990
but as soon as the can't
block is finished executing,

00:04:45.080 --> 00:04:46.830
the queue will stop execution.

00:04:46.840 --> 00:04:49.270
If you want to make this
process deterministic,

00:04:49.350 --> 00:04:52.950
the way to do that is to call dispatch
suspend from a block that is actually

00:04:53.040 --> 00:04:54.840
executing on the queue itself.

00:04:54.920 --> 00:04:58.070
This way you know that you can't block
is the last one that will execute

00:04:58.070 --> 00:04:59.700
before suspension takes effect.

00:04:59.800 --> 00:05:03.810
And our queues are reference counted
and you maintain that reference count

00:05:03.900 --> 00:05:05.360
with dispatch retain and release.

00:05:05.380 --> 00:05:09.250
A couple of pitfalls to go
over with dispatch queues.

00:05:09.320 --> 00:05:12.400
They are really intended for flow
control in your application and not

00:05:12.400 --> 00:05:14.230
as a general purpose data structure.

00:05:14.260 --> 00:05:17.320
If you have a lot of work that
you may need to execute in your

00:05:17.430 --> 00:05:21.360
application asynchronously,
it is better to keep that work in an

00:05:21.520 --> 00:05:25.700
actual data structure until you're sure
that you really have to execute the

00:05:25.720 --> 00:05:27.800
work and only then submit it to a queue.

00:05:27.800 --> 00:05:29.760
In particular, this is the case.

00:05:29.760 --> 00:05:31.480
Because once you submit
a block to a queue,

00:05:31.480 --> 00:05:32.900
there is no way to cancel that.

00:05:32.900 --> 00:05:35.390
The block, we guarantee that that
block will execute.

00:05:35.570 --> 00:05:38.280
Also,
as with any multithreaded programming,

00:05:38.280 --> 00:05:41.160
be very careful when you
use any synchronous API.

00:05:41.400 --> 00:05:44.240
GCD does not protect you from
the possibility of deadlock.

00:05:44.400 --> 00:05:47.560
For instance,
if you have used our dispatch sync API,

00:05:47.630 --> 00:05:51.310
it is very possible that you can
deadlock yourself by waiting for

00:05:51.320 --> 00:05:54.900
a queue that is waiting on the
queue that you're on in turn.

00:05:54.900 --> 00:05:58.870
Also, if you use our recommended
pattern to apply,

00:05:58.900 --> 00:06:04.250
or use a serial queue for each subsystem
in your application or each resource

00:06:04.370 --> 00:06:08.480
that you need to -- each shared
resource that you need to protect,

00:06:08.480 --> 00:06:13.400
if you block the queue that protects
that resource or that subsystem,

00:06:13.440 --> 00:06:15.850
you will block the whole
application's access to that.

00:06:15.960 --> 00:06:19.730
So any kind of synchronous
API on those queues would be bad.

00:06:19.900 --> 00:06:22.550
We have a couple of queue types.

00:06:22.990 --> 00:06:27.190
The global queues that you get with
the dispatch get global queue API,

00:06:27.430 --> 00:06:29.300
is passing in a priority.

00:06:29.300 --> 00:06:30.590
These are concurrent queues.

00:06:30.640 --> 00:06:32.710
They execute multiple
blocks at the same time.

00:06:32.750 --> 00:06:35.780
And they are global across
your whole application.

00:06:35.940 --> 00:06:38.210
The main queue is another global object.

00:06:38.330 --> 00:06:41.650
This is a serial queue that
cooperates with the main run

00:06:41.650 --> 00:06:43.400
loop of your application.

00:06:43.400 --> 00:06:46.690
This is typically where you
would execute blocks that

00:06:46.690 --> 00:06:48.990
interact with the UI subsystem.

00:06:49.500 --> 00:06:52.710
This is one of the queues where
it is very important to not run

00:06:53.000 --> 00:06:55.110
any blocking synchronous API,
of course,

00:06:55.240 --> 00:06:59.200
because you would prevent event
processing in the app if you do that.

00:06:59.360 --> 00:07:03.220
Serial queues you can create yourself
by calling the Dispatch Queue

00:07:03.220 --> 00:07:05.590
Create API and passing in a label.

00:07:05.910 --> 00:07:11.990
New in Lion and iOS 5
are concurrent queues.

00:07:12.400 --> 00:07:15.950
Like the serial queues,
you create these with the Dispatch

00:07:16.060 --> 00:07:20.990
Queue Create API and you pass in the
Dispatch Queue concurrent parameter.

00:07:21.270 --> 00:07:24.660
These, like the global concurrent queues,
will execute multiple

00:07:24.660 --> 00:07:27.750
blocks at the same time,
but they can be suspended

00:07:27.840 --> 00:07:29.340
like a serial queue.

00:07:29.440 --> 00:07:32.700
And a new concept,
they support barrier blocks.

00:07:32.760 --> 00:07:34.550
What are barrier blocks?

00:07:34.740 --> 00:07:38.050
These are blocks that represent
synchronization points in an

00:07:38.060 --> 00:07:39.700
otherwise concurrent queue.

00:07:39.820 --> 00:07:43.200
So they will not run until all the
blocks that were submitted before

00:07:43.200 --> 00:07:45.460
the barrier block have completed.

00:07:45.530 --> 00:07:48.420
And while they're executing,
nothing else can run on the queue.

00:07:48.420 --> 00:07:52.110
Any blocks that were submitted
later will wait until that

00:07:52.110 --> 00:07:54.140
barrier block has executed.

00:07:54.190 --> 00:07:57.630
You submit barrier blocks by
calling the Dispatch Barrier Async

00:07:57.800 --> 00:07:59.860
or Dispatch Barrier Sync APIs.

00:07:59.990 --> 00:08:02.680
These are exactly like Dispatch
Async and Dispatch Sync,

00:08:02.820 --> 00:08:06.140
except that they mark the
block as a barrier block.

00:08:06.180 --> 00:08:10.680
Note that these barrier blocks don't
do anything special on the global

00:08:10.740 --> 00:08:13.700
concurrent queues because the global
concurrent queues are a shared

00:08:13.700 --> 00:08:15.180
resource across the whole application.

00:08:15.180 --> 00:08:19.420
We don't want anybody to be
able to monopolize those queues.

00:08:19.500 --> 00:08:21.620
So here we have the same
animation as before,

00:08:21.750 --> 00:08:24.660
except that we have one
of these concurrent,

00:08:24.660 --> 00:08:27.500
one of these barrier blocks in
the middle of one of these new

00:08:27.500 --> 00:08:31.980
concurrent queues submitted by
the Dispatch Barrier Async API.

00:08:32.020 --> 00:08:34.600
And once threads start
dequeuing blocks here,

00:08:34.830 --> 00:08:38.460
the yellow barrier block will not
start executing until all the existing

00:08:38.460 --> 00:08:39.540
work on the queue has finished.

00:08:39.540 --> 00:08:42.780
Then while the block,
the barrier block is executing,

00:08:42.780 --> 00:08:44.100
nothing else can run.

00:08:44.120 --> 00:08:47.340
And once it has finished,
normal concurrent execution can continue.

00:08:47.360 --> 00:08:51.170
One important use case for these new
concurrent queues are to implement

00:08:51.260 --> 00:08:54.180
efficient read or write schemes with GCD.

00:08:54.400 --> 00:11:12.600
[Transcript missing]

00:11:13.360 --> 00:11:16.200
So we currently have
three priority levels,

00:11:16.200 --> 00:11:17.500
low, default, and high.

00:11:17.500 --> 00:11:20.900
And if you create a serial queue,
by default,

00:11:21.000 --> 00:11:24.860
it will come with the default priority
global queue set as its target queue.

00:11:24.860 --> 00:11:27.800
Then you can submit some back
to that queue and later on

00:11:27.800 --> 00:11:31.960
potentially change its target queue,
say, to the low priority queue.

00:11:32.220 --> 00:11:34.240
Of course, you can also,
when you create a queue,

00:11:34.240 --> 00:11:36.180
immediately change the target queue here.

00:11:36.220 --> 00:11:46.210
You set it to be the high priority queue.

00:11:46.210 --> 00:11:46.220
So items that you submit to the serial
queue on the right will now execute on

00:11:46.220 --> 00:11:46.220
a thread that runs at high priority.

00:11:46.800 --> 00:11:49.600
New in iOS 5 and line,
we're adding a fourth priority level,

00:11:49.600 --> 00:11:51.070
the background priority.

00:11:51.200 --> 00:11:53.140
And of course,
with the new concurrent queues,

00:11:53.210 --> 00:11:54.640
you can also set the target queue.

00:11:54.640 --> 00:11:58.020
So here we've set it to be this new
background priority global queue.

00:12:00.300 --> 00:12:01.440
More on that.

00:12:01.480 --> 00:12:05.000
The background priority global queue
you get by passing the dispatch

00:12:05.050 --> 00:12:08.110
queue priority background constant
to dispatch get global queue.

00:12:08.140 --> 00:12:13.130
Blocks that you submit to this
queue will run on threads that run

00:12:13.140 --> 00:12:17.580
at the lowest possible scheduling
priority in your process.

00:12:17.660 --> 00:12:18.570
And any I.O.

00:12:18.650 --> 00:12:21.860
that you do from that thread
will be throttled I.O.

00:12:21.860 --> 00:12:24.560
system-wide that will take a
backseat to any normal I.O.

00:12:24.560 --> 00:12:25.400
that's ongoing.

00:12:25.910 --> 00:12:30.510
So this queue is very useful if you
have any kind of idle time work or

00:12:30.510 --> 00:12:33.850
scanning of the disk or indexing,
anything like that that should

00:12:33.870 --> 00:12:37.730
not interfere with other more
higher priority work on the system.

00:12:37.740 --> 00:12:41.050
As mentioned,
target queues can form a whole hierarchy.

00:12:41.180 --> 00:12:45.720
And the way to set that up is by
calling dispatch set target queue.

00:12:45.720 --> 00:12:49.560
This in itself is an asynchronous
barrier operation on the queue.

00:12:49.560 --> 00:12:52.220
So any work that is already
enqueued on the queue and

00:12:52.220 --> 00:12:55.840
executing will not be affected
by this changing of target queue.

00:12:55.840 --> 00:12:58.970
It's only once all the work that
was submitted ahead of the call

00:12:58.970 --> 00:13:02.380
to dispatch set target queue has
finished that the target queue will

00:13:02.500 --> 00:13:04.630
change and affect any future blocks.

00:13:04.640 --> 00:13:09.480
And we support arbitrary deep
hierarchies if that is useful to you,

00:13:09.590 --> 00:13:10.740
but not loops.

00:13:10.740 --> 00:13:15.690
There's a couple of useful somewhat
non-trivial idioms with target queues

00:13:15.860 --> 00:13:18.270
that we'd like to go through now.

00:13:18.280 --> 00:13:23.230
One is that you can synchronize with a
serial queue by setting the target queue

00:13:23.230 --> 00:13:25.810
of your queue to be that serial queue.

00:13:25.860 --> 00:13:29.460
Synchronize meaning that there
will be a block executing either on

00:13:29.460 --> 00:13:32.890
your queue or on the serial queue
but not on both at the same time.

00:13:33.340 --> 00:13:37.380
However, setting that up does not
implicitly order -- impose any

00:13:37.960 --> 00:13:40.100
ordering between the two queues.

00:13:40.100 --> 00:13:42.970
So if you submit blocks to
both queues in a certain order,

00:13:42.970 --> 00:13:46.180
that doesn't mean that they
will be dequeued in that order.

00:13:48.150 --> 00:13:53.700
Here we have two zero queues that come in
the default state with the target queue

00:13:53.700 --> 00:13:56.100
set to the default priority global queue.

00:13:56.100 --> 00:14:00.100
You can synchronize them by
changing the target queue of

00:14:00.100 --> 00:14:02.500
one to be the other zero queue.

00:14:02.520 --> 00:14:10.350
This means there will be a block
executing on one or the other

00:14:10.370 --> 00:14:10.370
of these two zero queues but
not on both at the same time.

00:14:11.410 --> 00:14:15.200
Of course, you can do the same with one
of our new concurrent queues,

00:14:15.200 --> 00:14:19.020
and what that does is that it actually
makes the concurrent queue serial.

00:14:19.150 --> 00:14:21.780
The reason for that is that,
remember I said,

00:14:21.780 --> 00:14:25.700
target queues are where the dequeue
operation for a queue is executed.

00:14:25.850 --> 00:14:30.090
And because here the target queue
of a concurrent queue is serial,

00:14:30.220 --> 00:14:32.840
the dequeue operation becomes serialized,
so we cannot run more than

00:14:32.840 --> 00:14:38.500
one block at the same time,
even though the queue was concurrent.

00:14:39.020 --> 00:14:42.680
One use case for that is if you
can't use a read or write lock,

00:14:42.690 --> 00:14:46.700
you can promote it to an
exclusive lock by doing this.

00:14:49.600 --> 00:17:55.200
[Transcript missing]

00:17:55.820 --> 00:17:59.990
As soon as that red high
priority item has finished,

00:17:59.990 --> 00:18:05.280
we resume the low priority queue and
the DQ operation can continue as normal.

00:18:05.290 --> 00:18:10.550
So now we've managed to
jump ahead of the queue.

00:18:10.900 --> 00:18:14.000
Our next topic is queue-specific data.

00:18:14.000 --> 00:18:18.220
This is a new feature in iOS 5 and Lion.

00:18:18.220 --> 00:18:22.180
This allows you to associate arbitrary
key-value storage to a queue.

00:18:22.180 --> 00:18:24.730
We already had DispatchSetContext
and GetContext,

00:18:24.810 --> 00:18:28.090
but that was a single pointer and
wasn't really sufficient if you were

00:18:28.090 --> 00:18:31.850
sharing your queues among multiple
subsystems in your application.

00:18:31.900 --> 00:18:34.650
So here we allow you
with the DispatchQueue,

00:18:34.650 --> 00:18:38.740
GetSpecific and DispatchQueueSetSpecific
APIs to set key-value

00:18:38.740 --> 00:18:40.460
storage for any queue.

00:18:40.900 --> 00:18:44.060
And we use keys that we
just compare as pointers.

00:18:44.210 --> 00:18:46.210
So typically,
we recommend that you use the

00:18:46.210 --> 00:18:49.080
address of a static variable in
your subsystem that will be a

00:18:49.200 --> 00:18:50.810
unique pointer to your subsystem.

00:18:51.040 --> 00:18:56.110
Don't just pass in a string because
that may not be a unique pointer.

00:18:57.130 --> 00:18:59.200
And for the setters,
you pass in a value and

00:18:59.200 --> 00:19:00.520
a destructor function.

00:19:00.520 --> 00:19:04.000
That destructor function is called
when the value is unset by another

00:19:04.000 --> 00:19:07.480
call to dispatch queue set specific
or when the queue is destroyed.

00:19:07.480 --> 00:19:11.140
And a new capability we make available
for you with this is that you can get

00:19:11.140 --> 00:19:14.740
the current value for a given key when
you're running in a block on a queue

00:19:14.740 --> 00:19:16.660
with the dispatch get specific API.

00:19:16.660 --> 00:19:19.820
You just pass in the key and this
will return the value that is set

00:19:19.960 --> 00:19:22.450
on the currently executing queue or,
importantly,

00:19:22.580 --> 00:19:26.160
this is the value that is set on
the currently executing queue.

00:19:27.200 --> 00:19:28.570
Daniel Steffen: And this is a way of the
target queue hierarchy.

00:19:28.700 --> 00:19:30.950
If this current queue
doesn't have a value set,

00:19:31.060 --> 00:19:34.020
it will go and look in the target queue,
does this have the

00:19:34.160 --> 00:19:36.510
value set for this key,
and so on until it reaches

00:19:36.600 --> 00:19:38.490
the bottom of the hierarchy.

00:19:39.650 --> 00:19:43.550
So to illustrate this here,
we have three queues set up in

00:19:43.650 --> 00:19:48.080
a target queue relationship,
and we set a value for a key,

00:19:48.080 --> 00:19:51.800
value one set on the
QA and value two set on QB,

00:19:51.800 --> 00:19:53.920
but no value for this key set on QC.

00:19:53.920 --> 00:19:58.400
If you now run a block on QB and
call dispatch get specific there,

00:19:58.510 --> 00:20:00.280
you will see value two.

00:20:00.280 --> 00:20:03.500
But if you run it on QC,
because it doesn't have its own value,

00:20:03.500 --> 00:20:05.810
it will see the value
from the target queue,

00:20:05.810 --> 00:20:06.640
value one.

00:20:07.550 --> 00:20:10.730
Another new API set that
we are introducing in iOS 5

00:20:10.730 --> 00:20:12.540
and line is dispatch data.

00:20:12.540 --> 00:20:16.900
This is a facility that provides
container objects to you for multiple

00:20:16.900 --> 00:20:18.320
discontinuous memory buffers.

00:20:18.320 --> 00:20:22.360
And importantly, these container objects,
as well as the represented buffers,

00:20:22.360 --> 00:20:23.990
must be immutable.

00:20:24.000 --> 00:20:26.240
So the container objects,
once you've created them,

00:20:26.240 --> 00:20:29.570
you can no longer change them,
and any buffers that you pass to us to

00:20:29.570 --> 00:20:34.650
be represented by dispatch data must not
be changed once you've given them to us.

00:20:35.480 --> 00:20:38.780
And the central goal of the
whole API set is to avoid copying

00:20:38.780 --> 00:20:40.920
buffers as much as possible.

00:20:43.120 --> 00:20:46.070
The way you create one of
these Dispatch Data objects is

00:20:46.130 --> 00:20:48.680
to call Dispatch Data Create.

00:20:48.900 --> 00:20:52.740
You pass in a buffer and a size
and it will return you a Dispatch

00:20:52.740 --> 00:20:57.530
Data T object that you manage
with Dispatch Retain Release like

00:20:57.770 --> 00:21:01.230
all the other Dispatch objects.

00:21:01.230 --> 00:21:01.230
So we represent that
graphically here with a

00:21:01.460 --> 00:21:05.380
Blue memory buffer represented
by a red Dispatch data object.

00:21:05.420 --> 00:21:08.240
And because the buffers are
required to be immutable,

00:21:08.240 --> 00:21:12.600
we can now have multiple Dispatch data
objects that reference the same buffer.

00:21:12.600 --> 00:21:16.070
And of course,
only once all the data objects go

00:21:16.100 --> 00:21:18.550
away will the buffer be deallocated.

00:21:18.900 --> 00:21:22.680
The way that deallocation happens
is via this destructor block

00:21:22.680 --> 00:21:26.680
that you give us as the last
parameter in Dispatch Data Create.

00:21:26.680 --> 00:21:30.060
And there's a couple
possibilities for that.

00:21:30.090 --> 00:21:34.520
You can pass the Dispatch
Data Destructor default constant.

00:21:34.520 --> 00:21:37.680
This will tell us you should
make a snapshot of this data.

00:21:37.680 --> 00:21:41.200
I can't really guarantee that
this buffer will not change for

00:21:41.290 --> 00:21:43.280
the lifetime of the data object.

00:21:43.280 --> 00:21:48.320
So we make a copy and then manage
the lifetime of that copy ourselves.

00:21:48.900 --> 00:21:51.620
So of course, if you can avoid it,
this is not the preferred choice

00:21:51.700 --> 00:21:53.290
because it involves making a copy.

00:21:53.290 --> 00:21:55.730
If you just have a buffer
allocated in malloc,

00:21:55.730 --> 00:21:59.660
you pass Dispatch Data Destructor free,
which is essentially equivalent

00:21:59.660 --> 00:22:02.510
to giving us a block that
we'll call free on the buffer,

00:22:02.510 --> 00:22:04.020
but is more efficient.

00:22:04.020 --> 00:22:08.420
And if you have some custom buffer that
is maybe wrapped into another object,

00:22:08.420 --> 00:22:13.220
it just passes a block that then does
whatever deallocation break is required.

00:22:13.220 --> 00:22:17.000
So in this example, for instance,
we've passed in the buffer underlying

00:22:17.000 --> 00:22:18.880
a CFData object to the creation.

00:22:18.910 --> 00:22:22.860
And for the destructor,
we just release the CFData.

00:22:22.860 --> 00:22:25.590
You can also create Dispatch
Data objects from other objects,

00:22:25.690 --> 00:22:27.020
from other data objects.

00:22:27.020 --> 00:22:31.180
For instance, via concatenation with the
Dispatch Data Create concat API.

00:22:31.180 --> 00:22:35.550
This takes two existing data objects
and makes a new object from them

00:22:35.550 --> 00:22:40.220
that now represents the two memory
buffers concatenated together.

00:22:40.220 --> 00:22:43.980
And of course, again,
because of counting of references,

00:22:43.980 --> 00:22:46.480
when these original
source objects go away,

00:22:46.480 --> 00:22:48.870
the memory buffers will stick
around until they're gone.

00:22:48.900 --> 00:22:51.560
So the concatenated
object also goes away.

00:22:51.600 --> 00:22:54.800
You can also make a subrange
of an existing data object if

00:22:54.920 --> 00:22:57.670
you're only interested in a
part of the data by calling the

00:22:57.900 --> 00:23:00.180
Dispatch Data Create subrange API.

00:23:00.180 --> 00:23:03.520
So here, as an example,
we have a data object that represents

00:23:03.580 --> 00:23:05.640
four discontinuous memory buffers.

00:23:05.640 --> 00:23:10.960
And we're really only interested in the
piece between the two light blue arrows.

00:23:10.960 --> 00:23:14.220
So we call the Dispatch
Data Create subrange

00:23:14.350 --> 00:23:17.330
API with this range here,
which makes a new object

00:23:17.330 --> 00:23:20.900
that only represents... which
represents those two pieces.

00:23:21.100 --> 00:23:24.050
And if you now get rid of
the source data object,

00:23:24.240 --> 00:23:28.130
you can deallocate the two buffers
that are no longer required.

00:23:28.260 --> 00:23:31.660
So if you do any kind of incremental
processing of data in your application,

00:23:31.660 --> 00:23:35.500
this can be a very easy way to get
rid of data on an ongoing basis.

00:23:35.700 --> 00:23:37.520
And again, of course,
when that goes away,

00:23:37.830 --> 00:23:40.560
the remaining buffers get deallocated.

00:23:40.600 --> 00:25:03.100
[Transcript missing]

00:25:03.100 --> 00:26:34.000
[Transcript missing]

00:26:34.710 --> 00:26:37.090
Let's look at an example
of this in action.

00:26:37.270 --> 00:26:41.510
Here we acquire a dispatch data object
from somewhere and what we're interested

00:26:41.890 --> 00:26:45.440
in is to try and find the header
on a character by character basis.

00:26:45.440 --> 00:26:49.380
We want to look through the
data and find the terminator

00:26:49.470 --> 00:26:53.340
character of a header here,
which we picked control Z as an

00:26:53.340 --> 00:26:55.920
example to be the terminator.

00:26:55.920 --> 00:27:02.180
We need to fill in this position under
under block variable with the position

00:27:02.180 --> 00:27:03.820
of that character in the buffer.

00:27:03.900 --> 00:27:07.820
So we call dispatch data apply
and our iterator block will

00:27:07.990 --> 00:27:13.040
get past four parameters,
a region object, which we don't actually

00:27:13.040 --> 00:27:15.210
use in this example,
but that just is an object that

00:27:15.250 --> 00:27:18.770
represents the current memory
area that's being traversed,

00:27:18.770 --> 00:27:21.450
and an offset,
which indicates where that area

00:27:21.600 --> 00:27:24.730
starts in the overall data object,
and a buffer and the size

00:27:24.830 --> 00:27:25.900
that gives you direct data.

00:27:25.940 --> 00:27:28.020
So we can then create
access to the storage.

00:27:28.090 --> 00:27:31.420
So here we just look for the position
of that terminator character.

00:27:31.430 --> 00:27:33.780
If we find it,
we calculate the overall position

00:27:33.780 --> 00:27:36.650
in the buffer and we return a
Boolean that indicates whether

00:27:36.650 --> 00:27:38.330
the iteration should continue.

00:27:38.330 --> 00:27:40.830
So for instance here,
once we have found the header,

00:27:40.970 --> 00:27:44.380
there's no need to go through all the
rest of the buffers in the object so we

00:27:44.380 --> 00:27:47.020
can return false and stop the iteration.

00:27:48.900 --> 00:30:33.600
[Transcript missing]

00:30:33.840 --> 00:30:36.430
and also allow us to track
file descriptor ownership with

00:30:36.520 --> 00:30:38.900
a reference countered object.

00:30:38.910 --> 00:30:41.980
And these dispatch IO channels
have two different types,

00:30:42.110 --> 00:30:46.920
stream or random that you have to
specify a channel creation time

00:30:46.920 --> 00:30:48.280
with the following two constants.

00:30:48.310 --> 00:30:51.860
The stream access channels
perform IO operations at the

00:30:51.860 --> 00:30:55.100
file pointer position and advance
the file pointer position.

00:30:55.100 --> 00:30:58.800
And if there's multiple asynchronous
IO operations in flight,

00:30:58.810 --> 00:31:02.490
these will be performed serially
one after the other because they

00:31:02.570 --> 00:31:04.300
depend on the file pointer position.

00:31:04.310 --> 00:31:06.550
Of course,
if you give us a file descriptor

00:31:06.560 --> 00:31:10.180
like a socket where reads can be
performed concurrently with writes

00:31:10.180 --> 00:31:12.220
without interfering with each other,
we will do that.

00:31:12.240 --> 00:31:17.260
The random access channels on the other
hand will start at a specified offset.

00:31:17.330 --> 00:31:20.510
The IO operations on those channels
will start at a specified offset to

00:31:20.510 --> 00:31:23.660
the initial file pointer position
when you created the channel.

00:31:23.670 --> 00:31:27.500
And these operations then change
the file pointer position.

00:31:27.540 --> 00:31:29.680
So this is like period versus read,
right?

00:31:30.550 --> 00:31:34.080
And in this case,
any asynchronous IO operations

00:31:34.080 --> 00:31:36.570
that you submit may be performed
concurrently if it makes sense

00:31:36.800 --> 00:31:39.140
for the device in question.

00:31:39.640 --> 00:31:42.370
And of course,
any file descriptors that you give us for

00:31:42.370 --> 00:31:45.380
these kinds of channels must be seekable.

00:31:45.730 --> 00:31:49.080
So three ways to create
Dispatch.io channels.

00:31:49.080 --> 00:31:52.600
The Dispatch.io create API where
you pass in a file descriptor.

00:31:52.600 --> 00:31:57.170
The Dispatch.io create with pass
API where you pass in a path on disk.

00:31:57.200 --> 00:32:00.950
And the Dispatch.io create
with IO API where you pass

00:32:00.950 --> 00:32:02.570
in an existing IO channel.

00:32:02.570 --> 00:32:05.230
This is useful if you
need to change the type,

00:32:05.330 --> 00:32:08.320
for instance,
of a channel or change configuration

00:32:08.340 --> 00:32:11.860
parameters on a channel
without affecting the original.

00:32:11.860 --> 00:32:14.980
And all of these take a cleanup
block as the last parameter.

00:32:14.980 --> 00:32:17.600
So let's look at that.

00:32:17.600 --> 00:32:21.640
This cleanup block is what
allows us to do that wrapping

00:32:21.910 --> 00:32:25.360
and control of file descriptors.

00:32:25.370 --> 00:32:28.560
When you give us a file descriptor
with the Dispatch.io create API,

00:32:28.560 --> 00:32:30.900
it becomes under system control.

00:32:30.900 --> 00:32:34.370
And you must not change
it during that time.

00:32:34.500 --> 00:32:38.040
So you must not do any direct
IO to it or change the file pointer

00:32:38.060 --> 00:32:41.720
position or close it or change
any configuration parameters on it.

00:32:42.340 --> 00:32:47.370
And you must not do these things
until that cleanup handler is called.

00:32:47.490 --> 00:32:50.800
That cleanup handler gets called
when the IO operations that you have

00:32:50.900 --> 00:32:54.000
submitted to the channel have all
completed and either the channel has been

00:32:54.000 --> 00:32:58.730
released or it has been closed with an
API that we'll see in a couple of slides.

00:32:59.550 --> 00:33:03.380
The syntax of these cleanup
handles is very simple.

00:33:03.380 --> 00:33:04.700
We just pass in an error.

00:33:04.880 --> 00:33:07.580
These errors are only creation
errors for the channel.

00:33:07.580 --> 00:33:11.000
So we only pass you errors that occur
when we try to create a channel,

00:33:11.000 --> 00:33:13.300
for instance,
if you pass a bad file descriptor

00:33:13.450 --> 00:33:16.250
or one of the wrong type or a
pass to a to a volume that doesn't

00:33:16.250 --> 00:33:17.700
exist or anything like that.

00:33:17.720 --> 00:33:20.840
You won't get any I/O errors
on the channel that occur later

00:33:20.840 --> 00:33:22.700
on when you submit operations.

00:33:22.710 --> 00:33:25.260
And the typical thing that you
will do in this cleanup handler

00:33:25.300 --> 00:33:26.940
is close the file descriptor.

00:33:26.940 --> 00:33:29.840
Again, you must not do that before
this point because it would

00:33:29.840 --> 00:33:31.060
rip it out from under us.

00:33:31.100 --> 00:33:34.870
The I/O operations that we support,
asynchronous reads and

00:33:34.870 --> 00:33:35.960
asynchronous writes.

00:33:36.140 --> 00:33:40.400
You submit an asynchronous read
request at the file pointer or

00:33:40.510 --> 00:33:45.120
at the specified object offsets,
depending on the channel type

00:33:45.290 --> 00:33:47.280
with the dispatch I/O read API.

00:33:47.280 --> 00:33:49.850
And here you just give us the
lengths that you would like

00:33:49.980 --> 00:33:51.680
to read from that channel.

00:33:51.680 --> 00:33:54.760
And asynchronous write, you pass in,
again,

00:33:54.760 --> 00:33:58.740
occurs at the file pointer position
or at the specified offset,

00:33:58.740 --> 00:34:01.310
and you just pass in the dispatch
data object that you would

00:34:01.310 --> 00:34:02.760
like to write to the channel.

00:34:02.790 --> 00:34:07.160
Both of these take a handler block,
which is an I/O handler.

00:34:07.180 --> 00:34:09.700
This is what allows you to
do incremental processing.

00:34:09.700 --> 00:34:12.790
We will call this block multiple times,
potentially, during the I/C.

00:34:12.880 --> 00:34:15.970
This is an asynchronous I/O operation
and give you incremental

00:34:16.040 --> 00:34:17.570
pieces of data as they come in.

00:34:17.580 --> 00:34:23.000
So importantly, that data that comes in,
we will get rid of it as

00:34:23.000 --> 00:34:24.520
soon as the handler returns.

00:34:24.590 --> 00:34:27.150
So if you can do any ongoing processing,
that's great.

00:34:27.160 --> 00:34:29.520
Otherwise,
you must retain that data if you need

00:34:29.960 --> 00:34:32.580
it after the handler has returned.

00:34:32.640 --> 00:34:36.960
So just call dispatch retain on it or
concatenate it with another object,

00:34:36.960 --> 00:34:38.620
a dispatch data object, et cetera.

00:34:38.620 --> 00:34:42.870
And the read operations are
passed the data in one of these

00:34:42.930 --> 00:34:46.900
handlers that was read since the
last time the handler was called,

00:34:46.900 --> 00:34:50.350
whereas the write operations actually
pass the data that we couldn't write yet.

00:34:50.510 --> 00:34:54.490
So this is the end piece of the data,
which will get smaller

00:34:54.490 --> 00:34:55.700
as the write proceeds.

00:34:55.700 --> 00:34:58.620
And this is particularly
useful if the write fails,

00:34:58.640 --> 00:35:01.230
then you can get at the data
that we couldn't write and

00:35:01.230 --> 00:35:04.560
maybe write it somewhere else,
a retry or something like that.

00:35:04.710 --> 00:35:09.390
The syntax of these handlers is,
we pass you three parameters, boolean,

00:35:09.400 --> 00:35:13.430
done flag, a dispatch data object,
and an error, parameter.

00:35:13.450 --> 00:35:17.830
And so you should treat all of
these three parameters independently

00:35:17.830 --> 00:35:19.940
and check for their presence.

00:35:20.070 --> 00:35:22.220
If there is a data object,
that means maybe there

00:35:22.340 --> 00:35:24.960
was some data read,
you process it and handle

00:35:24.960 --> 00:35:26.640
any error that occurred.

00:35:26.650 --> 00:35:30.100
And once you get the done flag,
that indicates to you that this is the

00:35:30.100 --> 00:35:31.760
last time it will call this handler.

00:35:31.830 --> 00:35:35.520
So complete any processing of
incremental data or any state that

00:35:35.520 --> 00:35:37.320
you've accumulated during that time.

00:35:37.340 --> 00:35:41.340
The other kind of operation we support
on dispatch area channels are barriers.

00:35:41.370 --> 00:35:44.500
These are very similar to the,
barrier blocks that we discussed

00:35:44.500 --> 00:35:46.460
earlier for concurrent queues.

00:35:46.460 --> 00:35:50.110
They only execute once any pending
I/O operations that you already

00:35:50.110 --> 00:35:51.970
submitted to the channel have completed.

00:35:52.020 --> 00:35:57.270
So this is a way to allow you to
synchronize I/O operations on the

00:35:57.270 --> 00:35:59.060
channel and know when they have finished.

00:35:59.070 --> 00:36:01.800
And while one of these
barrier blocks is ongoing,

00:36:01.800 --> 00:36:05.670
it has exclusive access to the file
descriptor underlying the channel.

00:36:05.800 --> 00:36:09.370
So we do actually allow
non-destructive modifications of the

00:36:09.370 --> 00:36:11.460
file descriptor during this time.

00:36:11.630 --> 00:36:16.700
So here we submit one of these barrier
blocks with the dispatch I/O barrier API.

00:36:16.810 --> 00:36:19.260
And inside this block,
now we are sure when we're

00:36:19.260 --> 00:36:22.160
executing that we have exclusive
access to the file descriptor.

00:36:22.160 --> 00:36:26.490
So if you don't know it, for instance,
if you open the channel from the path,

00:36:26.660 --> 00:36:30.040
from a path, we will open the file
descriptor on your behalf.

00:36:30.040 --> 00:36:32.440
So you don't have it,
you call the dispatch I/O get

00:36:32.550 --> 00:36:34.140
descriptor API to get it.

00:36:34.250 --> 00:36:36.900
And in this example, we just call fsync,
for instance.

00:36:36.990 --> 00:36:39.460
So if you have a bunch of asynchronous
writes that you have submitted,

00:36:39.460 --> 00:36:41.000
you want to make sure
that they're synced.

00:36:41.000 --> 00:36:44.180
So you put in a barrier block
that will make sure they have

00:36:44.180 --> 00:36:45.620
all completed and now sync.

00:36:45.620 --> 00:36:47.700
I mentioned closing channels before.

00:36:47.720 --> 00:36:50.680
You do that with the
dispatch I/O close API.

00:36:50.710 --> 00:36:55.830
This will close the channel from
accepting any new operations,

00:36:55.910 --> 00:36:59.680
but it will let any existing already
submitted operations complete.

00:36:59.700 --> 00:37:02.530
Once they have all completed,
it will invoke that cleanup handler

00:37:02.890 --> 00:37:05.120
where you can close the file descriptor.

00:37:05.120 --> 00:37:08.580
If you need to interrupt any already
submitted existing operations,

00:37:08.580 --> 00:37:11.700
you can pass in the dispatch
I/O stop flag and we will attempt

00:37:11.700 --> 00:37:15.420
to interrupt any ongoing I/O at
the earliest possible opportunity.

00:37:15.420 --> 00:37:17.320
But of course, this is asynchronous.

00:37:17.570 --> 00:37:22.570
So again,
you must wait for the cleanup handler

00:37:22.580 --> 00:37:26.200
to be called to be sure that all the
operations have actually completed.

00:37:27.120 --> 00:37:32.660
Any ongoing I/O handlers that are
interrupted get passed the eCancel

00:37:32.660 --> 00:37:36.850
error flag set that they know that
they were interrupted by a stop.

00:37:37.650 --> 00:37:42.140
To go through an example
of all this tied together,

00:37:42.140 --> 00:37:45.380
we are implementing a
transliteration of maybe a very,

00:37:45.380 --> 00:37:49.360
very large file that we pass
in via a file descriptor,

00:37:49.360 --> 00:37:53.860
and we want to transliterate that
file on a character-by-character

00:37:53.970 --> 00:37:57.220
basis and write it out to a path,
and want to do it all asynchronously.

00:37:59.140 --> 00:38:00.830
So we create an input channel.

00:38:00.830 --> 00:38:03.180
In this example,
we make it a random channel

00:38:03.180 --> 00:38:05.690
from this file descriptor,
and as you can see,

00:38:05.690 --> 00:38:10.030
the cleanup handler here just handles any
creation errors and then closes the file

00:38:10.090 --> 00:38:14.440
descriptor behind us when the operations
are done on this input channel.

00:38:15.350 --> 00:38:19.220
The output channel we create from a path,
and importantly for this example,

00:38:19.220 --> 00:38:22.300
we make that a stream type,
and we put in the parameters that

00:38:22.300 --> 00:38:26.210
we would have otherwise passed to
open to create the file for writing.

00:38:27.600 --> 00:38:29.940
And the cleanup handler here
just handles any errors.

00:38:29.940 --> 00:38:32.720
It doesn't need to close any file
descriptors because we do that on your

00:38:32.770 --> 00:38:34.260
behalf when we open from the path.

00:38:35.270 --> 00:38:39.050
Once you have that all set up,
you can put in an asynchronous read

00:38:39.060 --> 00:38:42.500
operation to the input channel,
and here we pass in offset

00:38:42.510 --> 00:38:45.530
zero and length size max,
which will actually read

00:38:45.530 --> 00:38:47.200
the whole file on disk.

00:38:47.780 --> 00:38:50.040
So it could be 200 megabyte
file or something like that.

00:38:50.420 --> 00:38:55.600
And we have the handler block here,
which gets passed those done

00:38:55.600 --> 00:38:56.920
data and error parameters.

00:38:57.940 --> 00:39:00.390
And remember, this gets called
potentially multiple times.

00:39:00.480 --> 00:39:03.040
So each time we check,
do we have a data object?

00:39:03.300 --> 00:39:06.410
If there is one,
we call this translate function

00:39:06.410 --> 00:39:09.550
that I haven't shown here,
which will go through and create a new

00:39:09.550 --> 00:39:12.810
data object with the result of this
transformation that we want to do.

00:39:13.880 --> 00:39:17.370
And create a new data object
that returns to us that we now

00:39:17.370 --> 00:39:19.550
pass to an asynchronous write.

00:39:19.700 --> 00:39:22.150
So this is where it becomes
important that the output channel

00:39:22.150 --> 00:39:25.000
was a stream channel because,
of course, we could have multiple of

00:39:25.140 --> 00:39:26.320
these asynchronous writes.

00:39:27.850 --> 00:39:35.890
And we just pass the output data piece at
offset zero to be written to the channel.

00:39:36.050 --> 00:39:40.470
And we, in this instance,
in the IO handler,

00:39:40.480 --> 00:39:43.620
we don't do anything special.

00:39:43.620 --> 00:39:46.270
We just handle any errors.

00:39:46.470 --> 00:39:52.030
We don't try to write any data
if it couldn't retry writing

00:39:52.040 --> 00:39:55.630
any data or anything like that.

00:39:56.620 --> 00:39:59.950
And once we have passed the output
data object to dispatcher write,

00:40:00.050 --> 00:40:03.310
the system will retain that object so
we can get rid of it here immediately.

00:40:04.200 --> 00:40:06.350
And the same with the input data object.

00:40:06.350 --> 00:40:09.660
We didn't, as remember, as I mentioned,
this data object gets

00:40:09.740 --> 00:40:13.040
released immediately as soon
as the read handler returns.

00:40:13.780 --> 00:40:15.810
And so here,
if you have a very large file,

00:40:15.810 --> 00:40:18.240
we will get pieces of that
file one after the other,

00:40:18.240 --> 00:40:21.100
and we will actually never
have all the file in memory.

00:40:21.100 --> 00:40:23.430
As soon as any processing and
writing of that piece is done,

00:40:23.430 --> 00:40:25.060
we can get rid of it here immediately.

00:40:26.060 --> 00:40:30.250
And so once the read handler is done,
the memory will be released,

00:40:30.470 --> 00:40:34.260
and you will have very good
memory efficiency with this API.

00:40:34.570 --> 00:40:37.360
When you pass the done
flag in the read handler,

00:40:37.370 --> 00:40:40.170
you know that the read
request has completed,

00:40:40.170 --> 00:40:43.620
so we know that we never need
the output channel anymore,

00:40:43.620 --> 00:40:45.610
so now we can just release it.

00:40:45.740 --> 00:40:49.700
And the same with the input channel,
once we have submitted the read request,

00:40:49.740 --> 00:40:53.540
the system will retain the input channel
for the lifetime of that request,

00:40:53.540 --> 00:40:56.220
so we can immediately
get rid of that channel.

00:40:57.880 --> 00:41:00.360
So now at the end here,
we have released all our

00:41:00.360 --> 00:41:04.810
references to all the objects,
and as soon as the asynchronous

00:41:04.870 --> 00:41:07.830
IO completes all the references,
all the objects will take care

00:41:07.930 --> 00:41:09.080
of themselves and release.

00:41:09.350 --> 00:41:15.440
So to summarize, we went over a couple of
advanced examples of usage of

00:41:15.440 --> 00:41:18.140
the target queue API in GCD,
then looked at

00:41:18.640 --> 00:41:22.160
The new APIs that we are
introducing in iOS 5 and Lion,

00:41:22.160 --> 00:41:26.110
concurrent queues and barrier blocks,
which are useful in particular for

00:41:26.200 --> 00:41:29.040
implementing read-or-writer schemes,
queue-specific data,

00:41:29.040 --> 00:41:34.300
which allows you to associate arbitrary
key-value storage to a dispatch queue,

00:41:34.300 --> 00:41:38.250
dispatch data API for managing
this contiguous memory buffers,

00:41:38.250 --> 00:41:41.180
and the dispatch
IO API for asynchronous IO.

00:41:42.180 --> 00:41:44.410
For more information,
you can contact Michael Churowicz,

00:41:44.420 --> 00:41:46.080
our DevTools and Performance Evangelist.

00:41:46.080 --> 00:41:51.150
And we have a concurrency programming
guide on the DevTools website,

00:41:51.600 --> 00:41:56.340
which is very informative and
covers many of these APIs.

00:41:57.340 --> 00:42:00.550
And of course,
LibDispatch itself is open source.

00:42:00.800 --> 00:42:05.310
We will very soon merge in the source
changes for the new APIs that we are

00:42:05.310 --> 00:42:07.880
introducing into that repository.

00:42:07.940 --> 00:42:09.820
So if you want to go and have
a look at the implementation,

00:42:09.870 --> 00:42:10.460
you can do that.

00:42:12.190 --> 00:42:13.980
And of course,
there's a special forum for

00:42:13.980 --> 00:42:16.320
GCD questions on the developer forum,
so we encourage you to

00:42:16.360 --> 00:42:17.480
ask any questions there.

00:42:19.260 --> 00:42:21.760
Related sessions happened
all yesterday already,

00:42:21.760 --> 00:42:24.680
unfortunately, so if you haven't had a
chance to go to those,

00:42:24.840 --> 00:42:26.620
please watch them on video.

00:42:26.620 --> 00:42:30.720
And I've also put the references
to the GCD sessions from last year

00:42:30.720 --> 00:42:35.160
on here that's focused specifically
on iOS programming with GCD.

00:42:35.160 --> 00:42:36.220
Thank you.