WEBVTT

00:00:10.570 --> 00:00:12.060
My name is Steve Lewallen.

00:00:12.060 --> 00:00:14.800
I'm the Engineering Manager for
Performance Tools.

00:00:14.850 --> 00:00:16.660
So what do we have for you today?

00:00:16.740 --> 00:00:18.870
Well, first up,
we're going to cover new workflow

00:00:19.040 --> 00:00:22.070
improvements we've made to
Instruments to make your life using

00:00:22.070 --> 00:00:23.870
the tool much more productive.

00:00:24.710 --> 00:00:29.590
Secondly, we're going to cover a new
concept called strategies.

00:00:30.350 --> 00:00:33.340
Third,
we're going to look at new profiling

00:00:33.340 --> 00:00:38.390
API that we've added to allow you to
programmatically profile your own apps.

00:00:38.580 --> 00:00:42.550
Then we're going to cover System Trace,
what's new, and do a deep dive on

00:00:42.550 --> 00:00:44.700
how to use it and why.

00:00:45.800 --> 00:00:48.400
And then we will cover our
network analysis instrumentation

00:00:48.460 --> 00:00:50.640
that we've added for iOS.

00:00:50.740 --> 00:00:54.100
And finally, we'll conclude with our
Arc support in Instruments.

00:00:54.240 --> 00:00:55.930
So let's get started.

00:00:58.740 --> 00:01:01.240
First of all,
we've added a new set of options

00:01:01.450 --> 00:01:05.420
in a sheet on Instruments
to allow you to change,

00:01:05.510 --> 00:01:07.600
for example,
the start time between the time

00:01:07.600 --> 00:01:11.020
you hit the recording button and
when the recording actually starts.

00:01:11.170 --> 00:01:14.550
This allows you to do any manual
setup you might have to do.

00:01:14.820 --> 00:01:19.430
Then we've added the ability to
set a time limit on the recording,

00:01:19.620 --> 00:01:23.080
say a maximum recording time,
at which point the recording

00:01:23.080 --> 00:01:23.080
will automatically stop.

00:01:23.380 --> 00:01:25.620
And finally,
we've added the ability to set deferred

00:01:25.620 --> 00:01:28.520
mode on the sheet for the trace document.

00:01:28.640 --> 00:01:31.790
Now, as I hope you know by now,
we have two major modes of

00:01:32.110 --> 00:01:33.700
taking traces in Instruments.

00:01:33.820 --> 00:01:34.690
One is a Median mode.

00:01:34.700 --> 00:01:37.800
Now, a Median mode is when we
collect the data and we show

00:01:37.800 --> 00:01:39.360
it to you on screen right away.

00:01:39.430 --> 00:01:42.960
So you're profiling your app and you
do something and you see a spike in

00:01:42.960 --> 00:01:46.460
allocations Instrument or time profiling,
you can associate that,

00:01:46.550 --> 00:01:48.820
make a mental note, "Oh,
that's about the explosion

00:01:48.940 --> 00:01:50.250
that just happened in my game.

00:01:50.250 --> 00:01:54.400
I'm going to go back and look at
that." But we also have deferred mode.

00:01:54.420 --> 00:01:58.020
Deferred mode is all about
reducing that observer effect.

00:01:58.150 --> 00:02:00.440
Instruments is running on
the same system your app is.

00:02:00.520 --> 00:02:03.080
In deferred mode,
you lose the immediate display,

00:02:03.240 --> 00:02:06.060
but we take Instruments off
the CPU to a very great extent,

00:02:06.060 --> 00:02:08.460
thus giving over more
resources to your app.

00:02:09.980 --> 00:02:12.170
Now if you want to run in
deferred mode all the time,

00:02:12.170 --> 00:02:12.900
you can do that too.

00:02:12.900 --> 00:02:15.010
We have a new preference
in the Preferences pane.

00:02:15.280 --> 00:02:18.080
Just check that and every new trace
document and every new recording

00:02:18.080 --> 00:02:20.390
you take will be in deferred mode.

00:02:20.750 --> 00:02:23.540
So we also have a new way to
trigger recording in instruments.

00:02:23.540 --> 00:02:26.690
So let's say, regardless of whether you
have instruments in the

00:02:26.690 --> 00:02:28.770
foreground or background,
you want to trigger a trace.

00:02:28.780 --> 00:02:32.410
All you have to do is hit Control-Excape,
and you can modify that key

00:02:32.410 --> 00:02:33.830
combo with this preference.

00:02:33.840 --> 00:02:37.550
In the frontmost document,
in instruments,

00:02:37.560 --> 00:02:41.100
we'll start recording or stop recording,
depending on what it was doing before.

00:02:41.100 --> 00:02:42.150
It'll just toggle that setting.

00:02:42.160 --> 00:02:45.480
So I like to do this, for example,
when I'm doing a system trace.

00:02:45.480 --> 00:02:49.600
I like this because I have
instruments in the background,

00:02:49.700 --> 00:02:52.100
and I'm bringing it to the foreground,
hitting the record button,

00:02:52.100 --> 00:02:53.270
and then taking it back.

00:02:53.340 --> 00:02:55.550
That does a lot of different
things to the system.

00:02:55.560 --> 00:02:57.340
It keeps the Windows server busy,
et cetera.

00:02:57.340 --> 00:02:59.820
So with Control-Excape,
with it already in the background,

00:02:59.820 --> 00:03:01.710
we don't affect the system.

00:03:03.710 --> 00:03:09.040
We've also added the ability to change
how the cores on your Mac behave.

00:03:09.140 --> 00:03:11.330
So first of all,
we have the ability to turn on

00:03:11.330 --> 00:03:13.500
and off hardware multi-threading.

00:03:13.600 --> 00:03:18.520
We also have the ability to reduce the
number of active cores on your system.

00:03:18.550 --> 00:03:21.000
This is a way to get
a rough approximation,

00:03:21.000 --> 00:03:24.500
but approximation nevertheless of
how your app would perform on a

00:03:24.500 --> 00:03:27.590
lesser system with fewer cores,
for example.

00:03:29.610 --> 00:03:32.400
Now let's talk about Track Gestures.

00:03:32.520 --> 00:03:35.640
So when we introduced System Trace
and a few other instruments,

00:03:35.690 --> 00:03:40.070
we also introduced a much higher
resolution and time in the Track View.

00:03:40.180 --> 00:03:43.340
Now we show data right down
to the nanosecond level.

00:03:43.450 --> 00:03:46.080
And because of that,
that drove us to create new gestures,

00:03:46.080 --> 00:03:49.390
which we think are so important we remind
you of what they are in the bottom right

00:03:49.390 --> 00:03:51.510
hand corner of every Trace document.

00:03:52.400 --> 00:03:56.340
So this allows you to directly
manipulate time in the track view.

00:03:56.340 --> 00:03:59.640
So for example, if you want to zoom in,
you just hold the shift key down,

00:03:59.640 --> 00:04:03.000
do a mouse drag, you'll get immediate
feedback about the scale,

00:04:03.000 --> 00:04:05.970
that is time per pixel,
and how much the duration of time

00:04:05.970 --> 00:04:10.640
you selected and release and then you
zoomed in precisely where you selected.

00:04:10.740 --> 00:04:13.330
If you've zoomed in,
you'll probably want to zoom out,

00:04:13.330 --> 00:04:15.990
so you can use control
and mouse drag to do that.

00:04:16.050 --> 00:04:16.970
Simple.

00:04:17.070 --> 00:04:21.420
We've also added a really cool new
feature called Snap Track to Fit.

00:04:21.490 --> 00:04:24.930
Let's say that you have made a
recording that extends beyond the

00:04:24.930 --> 00:04:28.710
visual range of the window or maybe it
doesn't fill up much of it and you just

00:04:28.780 --> 00:04:32.410
want to get that trace just fitting
perfectly in your visible window.

00:04:32.570 --> 00:04:37.410
Well, now you can do that with control
command Z and the track snaps to fit.

00:04:37.840 --> 00:04:39.430
Great little feature,
I do it all the time,

00:04:39.520 --> 00:04:42.000
and in deferred mode,
we do it for you automatically

00:04:42.000 --> 00:04:42.820
at the end of the trace.

00:04:44.250 --> 00:04:45.580
And finally, of course,
we've always had the

00:04:45.590 --> 00:04:48.380
ability to filter in time,
and now you can use Option and

00:04:48.380 --> 00:04:51.490
Mouse Drag to do that time filtering.

00:04:51.900 --> 00:04:53.680
So those are track gestures.

00:04:53.820 --> 00:04:57.300
Now let's move on to
Call Tree Data Mining.

00:04:57.570 --> 00:05:00.390
So we've always had a powerful
set of options to do data

00:05:00.390 --> 00:05:04.080
mining in the call tree view,
but now we've added

00:05:04.080 --> 00:05:06.460
several more on focus.

00:05:06.520 --> 00:05:09.070
So for example, you can focus on a
subtree of the call tree.

00:05:09.300 --> 00:05:11.030
That's just to eliminate noise.

00:05:11.170 --> 00:05:13.710
But even neater,
you can focus on all the calls,

00:05:13.710 --> 00:05:16.400
for example, made by the selected symbol.

00:05:16.410 --> 00:05:19.110
Or the inverse, who all calls the symbol?

00:05:19.120 --> 00:05:23.360
Or even further, who all calls this
library or this framework?

00:05:23.360 --> 00:05:26.880
So that's very handy when when you
want to see how your framework,

00:05:26.880 --> 00:05:28.470
your library that you're
providing is doing.

00:05:31.180 --> 00:05:34.800
So we've always had the ability
to filter in detail views.

00:05:34.810 --> 00:05:35.740
This is a great feature.

00:05:35.740 --> 00:05:40.750
It brings all the information you're
filtering to down to a concise list.

00:05:40.780 --> 00:05:43.840
But you lose one important thing,
the context around what

00:05:43.840 --> 00:05:44.890
you're filtering to.

00:05:44.890 --> 00:05:46.920
And that can be as equally as important.

00:05:46.920 --> 00:05:48.060
So now we have find.

00:05:48.210 --> 00:05:49.870
It works just as you'd expect.

00:05:49.870 --> 00:05:54.160
Hit Command-F and this little find
bar appears above your detail view.

00:05:54.240 --> 00:05:58.440
You can search forward and backward
in outline views and table views.

00:05:58.630 --> 00:06:02.010
And we'll even search inside of
unexpanded outline view nodes

00:06:02.010 --> 00:06:04.380
and associated back traces.

00:06:04.460 --> 00:06:06.060
That's very handy.

00:06:06.140 --> 00:06:10.080
Now, incredibly minor but also
very useful is discontinuous

00:06:10.080 --> 00:06:12.550
selection in all the detail views.

00:06:12.640 --> 00:06:17.300
And now you can select things
and do shallow or deep copies.

00:06:17.560 --> 00:06:20.730
and then paste it somewhere and
we will copy the column headers,

00:06:20.730 --> 00:06:23.420
the contents of all the columns,
and the indentation level

00:06:23.480 --> 00:06:24.520
of the outline view.

00:06:24.760 --> 00:06:28.120
This is really helpful if, for example,
you want to share some performance

00:06:28.130 --> 00:06:30.910
information with a colleague,
file a bug report,

00:06:30.910 --> 00:06:33.330
or jot down a note to yourself.

00:06:33.430 --> 00:06:35.480
So that is copying.

00:06:36.770 --> 00:06:40.420
Now, we've had for a while now this great
ability to see performance data

00:06:40.420 --> 00:06:43.060
right in line with your own source.

00:06:43.070 --> 00:06:45.380
This is awesome because it
can say this line of code is

00:06:45.510 --> 00:06:47.430
taking up 27% of your time.

00:06:47.540 --> 00:06:51.160
You allocated this much amount
of memory at this point.

00:06:51.290 --> 00:06:54.240
But often, especially if you're
looking at disassembly,

00:06:54.330 --> 00:06:56.970
which we can also show,
you can have huge number of hits.

00:06:57.130 --> 00:06:59.020
We're telling you about a lot of stuff.

00:06:59.150 --> 00:07:01.930
And it's been tedious and painful
to navigate up and down the source

00:07:02.070 --> 00:07:05.190
view or the disassembly view to
find what's the hottest frame.

00:07:05.190 --> 00:07:06.780
Now, what's the second hottest?

00:07:06.780 --> 00:07:08.600
Where was that hottest frame again?

00:07:08.700 --> 00:07:11.240
So now, if you have the extended detail
view open and you're looking

00:07:11.240 --> 00:07:13.130
at source or disassembly,
we will list all the

00:07:13.130 --> 00:07:15.850
performance annotations,
descending or ascending,

00:07:15.850 --> 00:07:16.820
it's your choice.

00:07:16.820 --> 00:07:17.900
You can sort them.

00:07:17.900 --> 00:07:20.950
And then you can use your
arrow keys or your mouse to

00:07:20.950 --> 00:07:22.860
navigate amongst all of these.

00:07:22.860 --> 00:07:25.090
The source views will scroll up
and down and will highlight the

00:07:25.090 --> 00:07:26.450
selection that you're looking at.

00:07:26.550 --> 00:07:29.590
This makes it really easy to
navigate the source and all

00:07:29.590 --> 00:07:31.670
those performance annotations.

00:07:32.380 --> 00:07:35.660
So that's the workflow improvements
we've added to Instruments.

00:07:35.770 --> 00:07:39.150
So now let's talk about a brand
new concept that we've added to

00:07:39.180 --> 00:07:41.570
the application called Strategies.

00:07:41.670 --> 00:07:42.840
So what's a strategy?

00:07:42.920 --> 00:07:46.520
A strategy is a way to take the data
that Instruments has gathered and

00:07:46.520 --> 00:07:48.960
rotate it onto a different axis.

00:07:49.010 --> 00:07:52.360
Now we've actually already been using
strategies no matter how long we've

00:07:52.360 --> 00:07:56.770
been using Instruments because now we're
formally we're making the first default

00:07:56.770 --> 00:07:58.800
view you see the Instrument Strategy.

00:07:58.930 --> 00:08:01.540
So let's take a look at how this works.

00:08:01.990 --> 00:08:04.250
Of course, the most common thing
in instruments is time,

00:08:04.260 --> 00:08:05.400
and that's along the x-axis.

00:08:05.500 --> 00:08:09.600
And in the instrument strategy,
we have instruments along the y-axis.

00:08:09.660 --> 00:08:13.060
But we collect data on
other very important,

00:08:13.060 --> 00:08:14.720
common types of things.

00:08:14.720 --> 00:08:16.890
For example, cores.

00:08:16.900 --> 00:08:21.450
So now we have a CPU strategy where
we list the CPUs on the y-axis.

00:08:21.500 --> 00:08:25.440
Now, any instrument that knows anything
about a particular core has a place

00:08:25.580 --> 00:08:29.880
in the track view to annotate it
with its information about that core.

00:08:30.880 --> 00:08:33.470
And of course,
we also have a thread strategy now.

00:08:33.480 --> 00:08:36.770
So we have threads listed on the y-axis,
all the threads all the

00:08:36.820 --> 00:08:39.900
instruments have encountered,
and it gives an opportunity for any

00:08:39.900 --> 00:08:42.970
instrument that knows anything about
a particular thread to annotate

00:08:42.970 --> 00:08:44.440
that space in the track view.

00:08:46.820 --> 00:08:51.340
So, what are the elements of a
strategy that you'll use?

00:08:51.490 --> 00:08:52.790
Well, first of all,
if we're going to give

00:08:52.790 --> 00:08:54.730
you multiple strategies,
we have to give you a

00:08:54.730 --> 00:08:55.900
way to choose a strategy.

00:08:56.160 --> 00:09:00.540
So we have a strategy chooser just below
the record button in the upper left.

00:09:00.620 --> 00:09:04.080
To the right of that,
we have a series of highlight controls.

00:09:04.180 --> 00:09:08.520
This allows you to highlight particular
data you might be interested in

00:09:08.600 --> 00:09:11.000
and sort of gray out other data.

00:09:11.120 --> 00:09:13.720
For example, show me everything about
a particular process,

00:09:13.720 --> 00:09:16.470
a particular thread or set of threads.

00:09:16.810 --> 00:09:19.410
Of course, the track view is a major
part of any strategy.

00:09:19.410 --> 00:09:21.640
That's where most of the action happens.

00:09:21.800 --> 00:09:25.300
And then we have this little legend
button over on the right hand side.

00:09:25.430 --> 00:09:28.700
So various strategies will use
color to mean different things.

00:09:28.790 --> 00:09:30.460
So if you want to find
out what a color means,

00:09:30.470 --> 00:09:33.590
just click on the legend button
and you'll get a description.

00:09:34.720 --> 00:09:39.240
So now let's look at the
CPU strategy in depth.

00:09:39.370 --> 00:09:42.300
Why would I want to use a CPU strategy?

00:09:42.410 --> 00:09:45.100
Well,
I might want to know how frequently,

00:09:45.210 --> 00:09:48.990
for how long,
and on how many CPUs my code is running.

00:09:49.500 --> 00:09:52.360
Now in doing this,
the CPU strategy can give you

00:09:52.360 --> 00:09:54.360
a quick visual indication,
for example,

00:09:54.360 --> 00:09:59.190
of whether you've achieved concurrency
and to what degree in your app.

00:09:59.510 --> 00:10:02.100
It can also show you how you're
sharing the system and the

00:10:02.100 --> 00:10:04.900
CPU resources with other processes.

00:10:05.490 --> 00:10:09.230
So this is a screenshot of
the CPU strategy in action.

00:10:09.230 --> 00:10:12.760
We zoomed in all the cores and stuff
so you could see them nicely here.

00:10:12.900 --> 00:10:16.650
We took a simple single threaded app,
we threw it on multiple threads,

00:10:16.650 --> 00:10:19.800
and we added a big lock
to guard all the data.

00:10:19.930 --> 00:10:22.290
This actually is not
an uncommon approach.

00:10:22.480 --> 00:10:26.990
Sometimes we see when apps are
taken to run multi-threading.

00:10:27.390 --> 00:10:30.850
The problem is that we
see our samples in blue.

00:10:30.870 --> 00:10:34.200
Blue, in this case,
indicates user stack frames.

00:10:34.320 --> 00:10:38.630
We never see the same sample or blue
on multiple CPUs at the same time.

00:10:38.780 --> 00:10:41.670
So you visually scan vertically
up and down all the cores,

00:10:41.680 --> 00:10:44.850
and you want to see more blue aligned,
but we don't.

00:10:44.990 --> 00:10:48.500
Instead, we see these troubling gaps.

00:10:48.610 --> 00:10:52.510
Now, this is a time profile
across the entire system.

00:10:53.040 --> 00:10:56.020
And so absolutely nothing
is happening in these gaps.

00:10:56.200 --> 00:10:57.940
The CPUs aren't overburdened.

00:10:58.070 --> 00:11:02.370
Your app simply--or this app
simply hasn't achieved concurrency.

00:11:02.790 --> 00:11:04.780
What we want to see is
the ideal concurrency.

00:11:04.790 --> 00:11:08.130
We see blue samples on
all cores all the time.

00:11:08.140 --> 00:11:09.520
That would be the perfect world.

00:11:09.520 --> 00:11:15.430
Now, our processes run on a system that
shares CPUs with other processes.

00:11:15.520 --> 00:11:18.480
So even if you've achieved
and created the most elegant

00:11:18.480 --> 00:11:22.180
concurrency model ever devised,
what you really see often

00:11:22.200 --> 00:11:24.200
is something more like this.

00:11:24.200 --> 00:11:27.380
The gray samples are samples
from other processes on a CPU.

00:11:27.480 --> 00:11:30.490
And then you can see where
the CPUs have shifted over

00:11:30.520 --> 00:11:33.000
and are processing your code.

00:11:33.000 --> 00:11:35.040
And that's your samples.

00:11:35.080 --> 00:11:36.540
And are leaved.

00:11:36.540 --> 00:11:39.550
So fret not,
in this case we do have concurrency,

00:11:39.550 --> 00:11:42.540
but we're sharing the
system with other processes.

00:11:42.540 --> 00:11:45.000
So that is the CPU strategy.

00:11:45.130 --> 00:11:49.320
And I'd like to invite Daniel Delwood
on stage to give us a demo of using

00:11:49.410 --> 00:11:53.510
Time Profiler and the CPU strategy.

00:11:53.510 --> 00:11:53.510
Daniel?

00:11:56.670 --> 00:11:57.760
Thank you, Steve.

00:11:57.840 --> 00:12:01.440
So I'd like to start out on the iPad,
just to show you an example

00:12:01.630 --> 00:12:03.830
app that I'm working on.

00:12:05.200 --> 00:12:06.100
All right.

00:12:06.230 --> 00:12:07.370
So, oh, there it is.

00:12:07.500 --> 00:12:09.640
Excellent.

00:12:09.790 --> 00:12:12.940
So anyway,
this app-- application is showing me, uh,

00:12:12.940 --> 00:12:13.900
loans.

00:12:14.030 --> 00:12:16.900
So perhaps I'm a loan holder,
and I want to see what the effects of

00:12:16.900 --> 00:12:22.160
the interest rate or the payment schedule
are on the amount I pay over 30 years.

00:12:22.290 --> 00:12:25.000
And so I can go ahead
and drag this slider.

00:12:25.340 --> 00:12:28.670
As I do so,
you'll notice it updates really slowly.

00:12:28.860 --> 00:12:30.500
That's not a great experience.

00:12:30.580 --> 00:12:33.020
And we want to actually improve that.

00:12:33.080 --> 00:12:34.810
Now notice also there's
a concurrency slider.

00:12:35.020 --> 00:12:39.040
Probably should turn that on now,
but it'll come back in the demo later.

00:12:40.620 --> 00:12:42.040
So I'll switch back to the demo.

00:12:42.150 --> 00:12:44.460
And I want to show some of
those workflow improvements

00:12:44.520 --> 00:12:46.700
that Steve was talking about.

00:12:47.210 --> 00:12:50.920
So here I am in Xcode,
and I've got my project open.

00:12:51.080 --> 00:12:54.220
So how do I run it with Instruments?

00:12:54.290 --> 00:12:57.360
Well, we've got a lot better
integration with Xcode 4,

00:12:57.560 --> 00:13:00.540
and that's through the
use of Xcode 4 schemes.

00:13:00.580 --> 00:13:02.850
Now, schemes are a combination of
both what you want to build

00:13:02.850 --> 00:13:05.580
and then what you want to do,
what action.

00:13:05.670 --> 00:13:08.610
So if I go up to the toolbar
and select Edit Scheme,

00:13:08.650 --> 00:13:14.190
I get a sheet that pops down and
shows me what that scheme consists of.

00:13:14.190 --> 00:13:17.460
So on the left side,
we've got a bunch of actions, run, test.

00:13:17.460 --> 00:13:19.760
What we're interested in is profile.

00:13:21.220 --> 00:13:23.510
Something interesting to note here
is that the build configuration

00:13:23.610 --> 00:13:26.540
for profile is release by default.

00:13:26.710 --> 00:13:29.340
Now this makes a lot of sense
because what you want to profile

00:13:29.340 --> 00:13:35.200
is the application as close to
your release version to customers.

00:13:35.340 --> 00:13:38.700
And so building with release
will usually compile with high

00:13:38.700 --> 00:13:44.200
optimization and all that final,
to replicate the final bits

00:13:44.200 --> 00:13:46.160
you'll be passing to customers.

00:13:46.310 --> 00:13:48.460
The other thing you'll notice
is that the Instruments

00:13:48.460 --> 00:13:49.800
template can be selected here.

00:13:49.800 --> 00:13:52.410
And so by default it'll ask on launch,
but if you know you always

00:13:52.410 --> 00:13:55.330
want to use the same template,
or perhaps you want to create a scheme

00:13:55.340 --> 00:13:58.900
just for using a certain template,
you can do that here.

00:13:59.060 --> 00:14:01.940
Now one thing I want to quickly
note about that build configuration

00:14:01.940 --> 00:14:06.900
being release is that we need
debug information in Instruments.

00:14:07.030 --> 00:14:10.080
And for all the default projects,
that's not an issue.

00:14:10.450 --> 00:14:14.560
But if you have a legacy project or you
know you've changed your debug format,

00:14:14.690 --> 00:14:17.840
you may want to make sure that
you set your debug information

00:14:17.840 --> 00:14:19.780
format back to DWARF with ESIM.

00:14:20.760 --> 00:14:21.700
It's all right.

00:14:21.700 --> 00:14:22.700
I have my scheme.

00:14:22.880 --> 00:14:23.700
How do I run it?

00:14:23.740 --> 00:14:29.180
Well, from the Run menu, just pull down,
select Profile, and Xcode will go ahead

00:14:29.180 --> 00:14:31.450
and build my application.

00:14:31.810 --> 00:14:35.800
Launch Instruments, it asks me on launch,
and I'm going to use the

00:14:35.800 --> 00:14:37.970
Time Profiler for this demo.

00:14:39.090 --> 00:14:42.000
So my app starts in the background,
and I'm actually just going to move

00:14:42.000 --> 00:14:46.100
that slider to generate some activity.

00:14:46.690 --> 00:14:50.390
And you'll notice the track view
immediately starts updating with data.

00:14:50.500 --> 00:14:52.870
Alright, that's probably enough.

00:14:54.620 --> 00:14:57.000
And I want to tell you sort
of what you're seeing here and

00:14:57.000 --> 00:14:58.600
how to work with this data.

00:14:58.690 --> 00:15:02.340
So Time Profiler is all
about CPU utilization.

00:15:02.440 --> 00:15:07.240
And every one millisecond by default,
it will take a back trace of

00:15:07.240 --> 00:15:11.830
whatever code's running on the
cores of the target machine.

00:15:11.840 --> 00:15:15.970
In this case, I've got an iPad 2,
and I've got two cores.

00:15:16.030 --> 00:15:18.580
And so if I want to
scrub in the track view,

00:15:18.680 --> 00:15:21.580
it's going to show me, by default,
the Instrument Strategy with

00:15:21.770 --> 00:15:24.650
whatever the instrument finds
the most useful statistic.

00:15:24.710 --> 00:15:28.740
So my application was
using about 96% of my CPU.

00:15:28.740 --> 00:15:29.740
Excellent.

00:15:29.740 --> 00:15:31.670
So how do I optimize?

00:15:31.740 --> 00:15:34.400
Well, that's what the Detail View is for,
giving you a lot more

00:15:34.400 --> 00:15:38.240
detailed information on what
the instrument collected.

00:15:38.240 --> 00:15:43.270
And this is a call tree just aggregating
all of those back traces together.

00:15:43.280 --> 00:15:46.040
So the top frame, by default,
is showing me that I made

00:15:46.040 --> 00:15:47.380
a lot of message sends.

00:15:47.380 --> 00:15:48.350
Excellent.

00:15:48.380 --> 00:15:50.440
That doesn't tell me very much.

00:15:50.440 --> 00:15:54.970
But if I want to use some of the
call tree options on the left here,

00:15:54.990 --> 00:15:56.480
I can dive in more.

00:15:56.550 --> 00:16:00.360
So I'm going to choose Uninvert to
see from a sort of a top-down view.

00:16:00.360 --> 00:16:04.200
And you'll notice I have a
worker thread and a main thread.

00:16:04.200 --> 00:16:06.590
And if I actually just want
to start turning these down,

00:16:06.740 --> 00:16:08.290
you'll see that I'm using Dispatch.

00:16:08.320 --> 00:16:09.220
OK.

00:16:09.320 --> 00:16:10.320
Hm.

00:16:10.320 --> 00:16:12.030
I wonder if I'm getting
good concurrency there.

00:16:12.040 --> 00:16:15.520
Well,
there's a bunch of Dispatch frames here.

00:16:15.740 --> 00:16:17.760
And I'm not really interested in them.

00:16:17.880 --> 00:16:23.730
So I can just use one of those new call
tree options of control clicking and

00:16:23.850 --> 00:16:27.560
selecting charge lib dispatch to callers.

00:16:27.700 --> 00:16:29.100
And it no longer appears.

00:16:29.230 --> 00:16:31.940
And now I can continue
with my investigation.

00:16:32.070 --> 00:16:33.270
So here we go.

00:16:33.510 --> 00:16:34.180
Oh, there we go.

00:16:34.390 --> 00:16:35.460
Loan calculate data.

00:16:35.700 --> 00:16:37.230
That's something I'm interested in.

00:16:37.370 --> 00:16:39.800
And in fact,
I don't want to see the rest of the tree.

00:16:39.800 --> 00:16:43.110
So I'm just going to go ahead
and select the focus arrow.

00:16:43.640 --> 00:16:45.260
Okay, so now I'm looking at my code.

00:16:45.260 --> 00:16:48.980
The code I even suspect is going
to be the one taking a lot of time,

00:16:48.980 --> 00:16:50.140
and it indeed is.

00:16:50.220 --> 00:16:54.780
And you'll notice that right above
this detail view is a jump bar,

00:16:54.780 --> 00:16:56.580
much like Xcode Force.

00:16:56.660 --> 00:16:58.680
And so if you want to get
back to where you came from,

00:16:58.680 --> 00:17:01.960
you can just click back in the
jump bar and get right back there.

00:17:03.330 --> 00:17:04.310
All right, so what do I see?

00:17:04.470 --> 00:17:09.070
Well, I see a lot of NSNumber, NSDate,
NSDecimal number frames.

00:17:09.190 --> 00:17:10.710
And I'm interested in that.

00:17:10.970 --> 00:17:14.540
And if I'm going to optimize,
maybe I could use less wrappers.

00:17:14.540 --> 00:17:15.780
I'm not sure.

00:17:15.900 --> 00:17:18.480
But I'm also interested in how
I'm using the auto-release tool.

00:17:18.480 --> 00:17:22.180
Perhaps I'm using that poorly.

00:17:22.360 --> 00:17:24.580
So I'll hit Command-F.

00:17:24.700 --> 00:17:27.800
And that find bar that Steve was
talking about jumps down.

00:17:28.060 --> 00:17:30.690
So I'll type in auto-release.

00:17:30.930 --> 00:17:33.590
And you'll see I've got
a pop auto-release pool.

00:17:33.650 --> 00:17:36.830
But if I even want to jump
into sub-parts of this tree,

00:17:36.860 --> 00:17:40.240
I can just select auto-expand
and start searching for it.

00:17:40.440 --> 00:17:43.500
So here we are in a subject auto-release.

00:17:43.630 --> 00:17:45.860
How many different places
in my code am I using this?

00:17:45.970 --> 00:17:48.950
Or at least,
is it showing a pot in the time profile?

00:17:49.100 --> 00:17:52.780
Well, I'll control-click on it,
and I can select focus on

00:17:52.780 --> 00:17:56.010
all callers of auto-release.

00:17:56.310 --> 00:17:57.290
Excellent.

00:17:57.290 --> 00:18:01.400
And so you'll see that I have used
about 89 milliseconds in my application,

00:18:01.440 --> 00:18:02.540
just calling auto-release.

00:18:02.630 --> 00:18:06.760
So perhaps I need to do something
different with my object wrappers.

00:18:06.800 --> 00:18:11.210
Now, that's a way to get a high level
view of sort of what my app's doing.

00:18:11.250 --> 00:18:14.180
But how do I actually improve this?

00:18:14.310 --> 00:18:16.130
Well, there's three different
things I could do.

00:18:16.390 --> 00:18:18.760
First of all, I could do less work.

00:18:18.780 --> 00:18:22.300
And that's probably the thing
I really want to look into.

00:18:22.630 --> 00:18:25.760
Perhaps I need to calculate
the loans differently.

00:18:25.760 --> 00:18:28.080
Maybe I need to calculate them less.

00:18:28.260 --> 00:18:32.910
Second of all,
I could do my work more efficiently.

00:18:33.340 --> 00:18:36.340
Maybe I should use a different algorithm.

00:18:36.480 --> 00:18:38.920
And I can really look into that.

00:18:38.980 --> 00:18:41.590
But third,
and what I want to show you today,

00:18:41.590 --> 00:18:43.990
is doing that work concurrently.

00:18:44.170 --> 00:18:47.300
So that's where the strategies come in.

00:18:48.210 --> 00:18:50.720
And if I use the strategy
control on the top left,

00:18:50.750 --> 00:18:53.600
I'm going to select CPU Strategy.

00:18:53.920 --> 00:18:58.440
So the track view reconfigures to
show me both cores on my iPad 2,

00:18:58.440 --> 00:19:01.940
as well as some data that looks
different from the previous view.

00:19:02.050 --> 00:19:06.250
Now, this data is now showing me
each individual backtrace at

00:19:06.250 --> 00:19:08.170
a one millisecond interval.

00:19:08.180 --> 00:19:09.580
That's hard to see.

00:19:09.580 --> 00:19:13.120
So I'm going to go ahead and
use the shift key and zoom in.

00:19:15.820 --> 00:19:19.380
As I zoom in even further,
you'll notice that pattern

00:19:19.380 --> 00:19:22.240
that Steve was talking about
of ping-ponging between cores.

00:19:22.240 --> 00:19:25.660
So from core zero to core
one and back and forth,

00:19:25.750 --> 00:19:29.350
I'm doing a lot of work,
but I'm not doing it at the same time.

00:19:29.610 --> 00:19:33.660
So there really is an opportunity
for optimization here.

00:19:33.810 --> 00:19:37.320
And we can see that very
quickly with the core strategy.

00:19:37.460 --> 00:19:40.030
All right, so I know I actually have
that concurrency slider.

00:19:40.040 --> 00:19:42.190
What happens when I use it?

00:19:43.750 --> 00:19:49.100
So go ahead and turn it on and
start moving the percentage rate.

00:19:51.300 --> 00:19:53.300
That's probably enough.

00:19:53.300 --> 00:19:57.400
And you'll notice that the graph
looks a little bit different,

00:19:57.400 --> 00:20:01.990
and if I just check the utilization,
it's about 160%. But let's go ahead

00:20:01.990 --> 00:20:03.910
and take a look in the CPU strategy.

00:20:03.920 --> 00:20:07.040
So I'll zoom in again.

00:20:09.100 --> 00:20:12.860
And what we'll notice here is that
we actually are doing a lot more work

00:20:13.020 --> 00:20:14.720
concurrently than we were before.

00:20:14.720 --> 00:20:18.980
So we have a lot of work
on both cores 1 and 2,

00:20:18.980 --> 00:20:21.700
and then there's some time that we're
only spending on a certain core.

00:20:21.700 --> 00:20:26.210
Now if I'm curious what stacks are these,
I can go ahead and click on them,

00:20:26.220 --> 00:20:29.660
or even double click to
bring up the sample list,

00:20:29.800 --> 00:20:34.090
and it shows me a list of all
the samples with the call stack.

00:20:34.140 --> 00:20:36.350
In this case,
I was recalculating my loans.

00:20:37.900 --> 00:20:39.750
So, OK.

00:20:40.200 --> 00:20:42.200
Excellent.

00:20:42.200 --> 00:20:45.200
When I select one of these stacks,
you'll notice it turns yellow.

00:20:45.200 --> 00:20:50.120
And if you're ever curious about what
something in the strategies mean,

00:20:50.120 --> 00:20:52.410
you can always use the legend.

00:20:52.410 --> 00:20:55.100
And the legend will show you that
red in this case means kernel,

00:20:55.100 --> 00:20:57.260
yellow means selected,
and blue means user.

00:20:58.450 --> 00:20:59.220
All right.

00:20:59.330 --> 00:21:01.160
So I've achieved some
sort of concurrency,

00:21:01.530 --> 00:21:04.260
but what was that bit about
that wasn't concurrent?

00:21:04.260 --> 00:21:05.940
Well,
I can scroll through here a little bit,

00:21:06.010 --> 00:21:11.060
and I see a lot of stacks that it
looks like were part of the chart view,

00:21:11.060 --> 00:21:12.520
which are drawing in the main thread.

00:21:12.520 --> 00:21:16.040
And if I'd really like to verify this,
I can go ahead and go to the pop-up

00:21:16.120 --> 00:21:20.100
and select main thread to highlight
everything that's on the main thread.

00:21:20.100 --> 00:21:23.910
And you'll notice in the detail view,
we have desaturated everything that

00:21:23.910 --> 00:21:25.780
isn't on the main thread for you.

00:21:26.740 --> 00:21:27.230
Now, this is great.

00:21:27.260 --> 00:21:30.320
Because as you zoom out a little
bit using the control modifier,

00:21:30.320 --> 00:21:34.160
you'll notice that lots of my other
threads were doing work concurrently,

00:21:34.160 --> 00:21:36.950
and my main thread is where
I should optimize next.

00:21:37.020 --> 00:21:38.840
So thank you very much.

00:21:38.940 --> 00:21:42.160
That's a demo of using the CPU strategy,
and I'll pass it back to Steve.

00:21:48.980 --> 00:21:50.050
Thank you, Daniel.

00:21:50.120 --> 00:21:51.400
It was a great demo.

00:21:51.480 --> 00:21:53.000
This is the CPU strategy.

00:21:53.100 --> 00:21:56.700
And now let's move on to the
new profiling API that we're

00:21:56.700 --> 00:22:02.440
introducing for Mac OS X programmatic
profiling for your use.

00:22:02.560 --> 00:22:06.110
So this is called the
DT Performance Session Framework.

00:22:06.830 --> 00:22:10.040
And it allows you to
programmatically target yourself,

00:22:10.040 --> 00:22:14.700
that is the app actually calling the API,
or other processes in the entire system.

00:22:14.810 --> 00:22:18.930
We've provided all of our major sets
of instrumentation for this framework:

00:22:19.010 --> 00:22:23.300
Time Profiling, Allocations,
including allocations with Sambi support,

00:22:23.310 --> 00:22:28.010
System Trace, Activity Monitor, Leaks.

00:22:28.310 --> 00:22:31.740
And you can even have an API--
you even have an API that'll

00:22:31.740 --> 00:22:35.300
allow you to programmatically
post flags to the timeline.

00:22:35.800 --> 00:22:39.810
So when you're done using this framework,
you will have generated a DTPS,

00:22:39.890 --> 00:22:42.700
DT Performance Session file.

00:22:42.820 --> 00:22:45.490
And you open that file in Instruments.

00:22:46.180 --> 00:22:49.010
But Instruments needs
the DSIMs that you have,

00:22:49.010 --> 00:22:51.740
along with your binary,
and that file to fully make

00:22:51.780 --> 00:22:54.160
sense of all the data collected.

00:22:54.900 --> 00:22:58.160
So we ask that you don't rebuild
your binaries in between the time

00:22:58.160 --> 00:23:01.180
you take a DTPerformance session
and you open it in Instruments.

00:23:01.200 --> 00:23:03.030
Otherwise,
the data isn't going to be nearly

00:23:03.030 --> 00:23:05.100
as good as it could have been.

00:23:05.120 --> 00:23:07.040
Now,
this framework is located in Library,

00:23:07.040 --> 00:23:10.030
Developer 4.0, Instruments Frameworks.

00:23:10.030 --> 00:23:13.350
Pointing out the obvious,
this will not be on user systems,

00:23:13.350 --> 00:23:17.390
so don't forget to not link
to this in the release build.

00:23:18.780 --> 00:23:22.000
Okay, so why would you want
to use this framework?

00:23:22.130 --> 00:23:25.800
Well, one of the things you can do is
programmatically profile something

00:23:25.800 --> 00:23:27.370
only under extreme conditions.

00:23:28.170 --> 00:23:30.490
Some instance where
something has gone awry,

00:23:30.490 --> 00:23:33.530
you can just kick in profiling like that,
and then you'll have that data.

00:23:33.530 --> 00:23:37.600
Or you might want to indicate
when extreme events have happened.

00:23:37.680 --> 00:23:39.660
Oh, I lost network connectivity here.

00:23:39.670 --> 00:23:42.710
And therefore,
when you look at the performance data,

00:23:42.710 --> 00:23:46.050
you can interpret that data
along with keeping that in mind.

00:23:46.050 --> 00:23:50.490
Or you might want to write, you know,
performance regression tests

00:23:50.490 --> 00:23:54.310
around certain API and make sure
that from release to release,

00:23:54.310 --> 00:23:55.550
you're not regressing a performance.

00:23:57.380 --> 00:23:58.060
So how do you use it?

00:23:58.060 --> 00:23:59.300
It's really simple.

00:23:59.450 --> 00:24:02.040
So you create a session and
you add Instruments to it.

00:24:02.040 --> 00:24:05.240
In this case, we're adding the
Time Profiler instrument.

00:24:05.370 --> 00:24:07.140
And then you want to start
and stop the Profiler.

00:24:07.140 --> 00:24:08.440
Now, you could do this just once.

00:24:08.440 --> 00:24:10.530
You could start at the beginning,
you could stop it at the end.

00:24:10.540 --> 00:24:12.140
That's perfectly fine.

00:24:12.300 --> 00:24:15.640
But you can also start and
stop it repeatedly in certain

00:24:15.690 --> 00:24:17.360
situations in your app.

00:24:17.520 --> 00:24:18.990
It's up to you.

00:24:19.500 --> 00:24:22.200
While doing so,
you can post signal flags.

00:24:22.270 --> 00:24:24.440
Again,
you can note extraordinary events like,

00:24:24.600 --> 00:24:27.020
oh,
network activity is lost or something.

00:24:27.020 --> 00:24:30.130
Or you can say, hey,
here's the beginning of something and

00:24:30.210 --> 00:24:32.050
here's the end of that same something.

00:24:32.050 --> 00:24:34.250
And then when it's
visualized in Instruments,

00:24:34.350 --> 00:24:37.930
we'll actually use that data to allow
you to quickly and easily time filter,

00:24:37.950 --> 00:24:40.000
for example, along those boundaries.

00:24:40.060 --> 00:24:44.000
It just helps you dive into
the data you collected.

00:24:44.620 --> 00:24:46.520
When you're done,
you save the session out,

00:24:46.520 --> 00:24:48.880
note where you saved it,
and then you open it in Instruments,

00:24:48.940 --> 00:24:52.590
and it's as if you took the time profile,
for example, right from the app itself.

00:24:54.230 --> 00:24:57.140
Now along with this framework,
we're shipping a new command line

00:24:57.140 --> 00:24:58.740
called the iProfiler command line.

00:24:58.740 --> 00:25:01.700
It's built entirely on the
DT Performance Session Framework.

00:25:01.800 --> 00:25:03.800
It can do exactly the same things.

00:25:03.910 --> 00:25:07.200
So if you'd rather work this into
your workflow from the command line,

00:25:07.300 --> 00:25:10.090
in scripts, for example,
feel free to do so.

00:25:10.240 --> 00:25:11.890
You can do either.

00:25:12.610 --> 00:25:16.980
So that is the new API for profiling.

00:25:17.080 --> 00:25:19.200
Now let's move on to System Trace.

00:25:19.280 --> 00:25:23.120
Now we introduced System Trace last year,
but a lot has changed and improved.

00:25:23.120 --> 00:25:26.410
The biggest news is that
it's now available for iOS 5.

00:25:26.410 --> 00:25:29.290
This is pretty significant
to have this type of

00:25:29.390 --> 00:25:31.960
instrumentation on an embedded OS.

00:25:31.960 --> 00:25:37.680
And it's particularly timely now
that we have these dual core iPad 2s.

00:25:37.680 --> 00:25:40.330
So when those games, for example,
are really pushing the system,

00:25:40.330 --> 00:25:44.240
they can use System Trace to
squeeze out just a little bit more.

00:25:44.400 --> 00:25:46.660
So System Trace is all about
providing a comprehensive

00:25:46.660 --> 00:25:48.540
analysis of the entire system.

00:25:48.720 --> 00:25:51.460
What are the threads doing
and why are they doing that?

00:25:51.580 --> 00:25:52.800
What system calls am I making?

00:25:52.800 --> 00:25:54.700
How long are they taking?

00:25:54.830 --> 00:25:58.480
What VM operations are going on that
the system has to perform that might

00:25:58.480 --> 00:26:00.420
affect my performance in my app?

00:26:01.810 --> 00:26:04.700
So, of course, System Trace has a default
instrument strategy.

00:26:04.700 --> 00:26:05.680
It has three instruments.

00:26:05.810 --> 00:26:06.700
We have scheduling.

00:26:06.720 --> 00:26:08.700
This is all of your thread information.

00:26:08.940 --> 00:26:10.700
Then we have system calls.

00:26:10.700 --> 00:26:12.670
And, of course, we have VM operations.

00:26:12.700 --> 00:26:16.700
But things get very interesting when
you switch to the thread strategy view,

00:26:16.700 --> 00:26:19.700
where we have threads in the vertical.

00:26:19.700 --> 00:26:24.290
Now you can see a lot more
detail about what's going on.

00:26:25.110 --> 00:26:26.270
When your clickers work.

00:26:26.480 --> 00:26:27.040
There you go.

00:26:27.180 --> 00:26:28.400
Okay.

00:26:28.500 --> 00:26:31.050
So first of all,
we can show you thread context switches.

00:26:31.120 --> 00:26:33.710
Now the arrows are colored
to indicate a particular CPU.

00:26:33.760 --> 00:26:36.240
So how does a context switch work?

00:26:36.570 --> 00:26:42.580
Well, you have a thread and eventually
it's context switched onto a CPU.

00:26:42.580 --> 00:26:43.870
It's actually

00:26:44.640 --> 00:26:46.500
The length of its run,
for that period of time,

00:26:46.500 --> 00:26:47.300
it's called its tenure.

00:26:47.300 --> 00:26:50.200
And then it will be switched off
because the CPU has to move on

00:26:50.200 --> 00:26:51.210
to service some other thread.

00:26:51.260 --> 00:26:52.640
And back and forth you go.

00:26:52.680 --> 00:26:54.760
So those are context switches.

00:26:54.760 --> 00:26:58.640
It's quite interesting to follow
the CPU around and see what it's

00:26:58.700 --> 00:27:00.420
servicing at what point in time.

00:27:00.460 --> 00:27:04.000
You can often see ping-ponging
instances where one thread is

00:27:04.000 --> 00:27:06.640
switching context with another,
again and again and again.

00:27:06.640 --> 00:27:08.090
That's inefficient,
so you might want to look

00:27:08.090 --> 00:27:08.970
at why that's happening.

00:27:11.340 --> 00:27:14.000
So we also have virtual memory events.

00:27:14.240 --> 00:27:16.200
We cover a whole lot of them.

00:27:16.350 --> 00:27:20.180
And we should stop here and sort
of describe what the virtual memory

00:27:20.180 --> 00:27:22.070
system is and what it's doing.

00:27:22.080 --> 00:27:25.610
So the system manages memory
in segments called pages.

00:27:25.610 --> 00:27:27.860
And these become real with faults.

00:27:27.860 --> 00:27:30.490
They're faulting in a page.

00:27:30.970 --> 00:27:36.270
Now, these pages can actually be
shared amongst processes.

00:27:36.850 --> 00:27:39.340
Okay, everything's good.

00:27:39.440 --> 00:27:42.030
But then someone wants
to write to a page,

00:27:42.080 --> 00:27:42.730
a shared page.

00:27:42.860 --> 00:27:45.000
So what happens in that case?

00:27:45.130 --> 00:27:47.580
Well, the system has to do some work.

00:27:47.680 --> 00:27:51.800
It has to make a copy of the shared
page and assign it to the writer.

00:27:51.950 --> 00:27:54.390
This is what we call a
copy on write page fault,

00:27:54.390 --> 00:27:56.030
and it takes a little time.

00:27:56.050 --> 00:27:59.910
So if you see a lot of these,
you think to yourself, am I causing this?

00:27:59.910 --> 00:28:02.010
Is there something else I could do?

00:28:03.020 --> 00:28:06.780
Now, in addition,
when pages are no longer used,

00:28:06.780 --> 00:28:08.650
they're returned to the system.

00:28:09.370 --> 00:28:13.680
And at some point in time,
somebody needs a new one.

00:28:13.830 --> 00:28:15.980
So we reuse that page.

00:28:16.050 --> 00:28:18.750
But it has the data from the previous
app in it or whoever wrote into it last,

00:28:18.750 --> 00:28:20.820
so we don't want to share that.

00:28:20.820 --> 00:28:24.030
So the system has to take
a zero fill page fault.

00:28:24.120 --> 00:28:26.280
Again,
spending a little time writing zeros

00:28:26.420 --> 00:28:28.900
in that page before it hands it back.

00:28:28.900 --> 00:28:34.570
So page faults can take a little time,
minuscule really, but still can build up.

00:28:34.700 --> 00:28:37.260
When the system is servicing
the needs of your app,

00:28:37.330 --> 00:28:40.260
so you should take that
into consideration.

00:28:41.830 --> 00:28:43.350
So we also show system calls.

00:28:43.670 --> 00:28:49.370
Now system calls are calls from
user space into the kernel.

00:28:49.980 --> 00:28:51.870
and they can take some time as well.

00:28:51.950 --> 00:28:55.770
Kernel is very efficient at its job,
but some calls can take

00:28:55.770 --> 00:28:56.990
longer than others.

00:28:56.990 --> 00:29:01.140
You might want to think to yourself,
do I want to really take this time now?

00:29:01.400 --> 00:29:03.430
Can I put it off,
do it at a different point,

00:29:03.430 --> 00:29:06.030
a different juncture of
execution of my program?

00:29:06.140 --> 00:29:08.710
So it's useful to see that information.

00:29:10.120 --> 00:29:13.660
And finally,
System Trace will show you thread states.

00:29:13.690 --> 00:29:17.340
A thread can be in many different
states for many different reasons.

00:29:17.340 --> 00:29:20.430
It's not always running, for example.

00:29:21.340 --> 00:29:23.840
So we show these states with colors.

00:29:23.910 --> 00:29:26.220
Now a thread can be in a runnable state.

00:29:26.300 --> 00:29:30.380
It's ready to go,
but there's no CPU there to handle it,

00:29:30.470 --> 00:29:31.300
to service it.

00:29:31.430 --> 00:29:33.440
Eventually a CPU comes along
and says I'm available,

00:29:33.440 --> 00:29:35.380
the thread is context
switched on on that CPU,

00:29:35.380 --> 00:29:39.630
runs for a while, and eventually it'll be
context switched off.

00:29:40.400 --> 00:29:42.680
So this is the life of a thread,
and threads are always moving

00:29:42.680 --> 00:29:45.080
in between these states,
and you should be aware of

00:29:45.080 --> 00:29:46.250
when that happens and why.

00:29:47.900 --> 00:29:50.330
So now I'd like to invite
Daniel back on stage for a demo of

00:29:50.330 --> 00:29:53.300
using System Trace on the iPad 2.

00:29:54.150 --> 00:29:56.000
Thank you very much, Steve.

00:29:56.120 --> 00:29:59.880
All right, so I am going to start
on the iPad 2 here.

00:30:01.180 --> 00:30:05.140
And I would like to show you
an app that is a lot of fun.

00:30:05.300 --> 00:30:08.140
It is a Mandelbrot fractal drawing app.

00:30:08.140 --> 00:30:10.970
And it's actually
pretty highly optimized.

00:30:10.970 --> 00:30:14.020
I mean, it's done in software,
so we probably should do this on the GPU.

00:30:14.020 --> 00:30:17.910
But for the purposes of this example,
we've optimized it using CPU,

00:30:17.910 --> 00:30:21.460
or using the Time Profiler and
the CPU strategy to make sure it's

00:30:21.680 --> 00:30:24.190
achieving concurrency and is very quick.

00:30:25.340 --> 00:30:26.760
So, I'll calculate it.

00:30:26.760 --> 00:30:29.800
And you'll notice it's
calculating from left to right,

00:30:29.800 --> 00:30:32.560
doing scan lines, top to bottom.

00:30:32.560 --> 00:30:35.520
And each thread is just picking
up a different scan line.

00:30:35.520 --> 00:30:38.820
And when it completes,
it passes it to the main thread,

00:30:38.820 --> 00:30:41.670
displays the image,
and that's what we see.

00:30:42.000 --> 00:30:46.200
So, all right, how can I use System Trace
to analyze this?

00:30:46.200 --> 00:30:47.720
I'll go back to the Mac.

00:30:52.080 --> 00:30:53.740
And here we are in my project.

00:30:53.750 --> 00:30:57.760
I'm just going to use that same
profile action from Xcode 4.

00:30:57.880 --> 00:31:00.500
Compiles my application.

00:31:00.620 --> 00:31:02.680
Launches Instruments.

00:31:03.950 --> 00:31:07.640
And I'm going to select
the System Trace template.

00:31:07.770 --> 00:31:10.550
Now, when I hit Profile,
you notice that Instruments really

00:31:10.570 --> 00:31:12.480
doesn't do too terribly much.

00:31:12.610 --> 00:31:17.230
So it's drawing in the background,
and I'll tell it to recalculate there.

00:31:17.700 --> 00:31:20.640
And when I'm done,
I can just go ahead and say stop.

00:31:20.770 --> 00:31:22.540
Now this is where
Instruments starts analyzing,

00:31:22.540 --> 00:31:27.540
because the System Trace template,
by default, works in deferred mode.

00:31:27.660 --> 00:31:31.500
And so that means that it tries to
stay off the CPU as much as it can,

00:31:31.500 --> 00:31:35.740
do as little work as possible,
and keep out of your application's

00:31:35.830 --> 00:31:39.870
way so that you get the best
performance metrics you can.

00:31:40.280 --> 00:31:42.760
So what are we seeing here,
the first screen?

00:31:42.760 --> 00:31:46.890
Well, this is a trace highlight showing
some high-level ideas of system usage,

00:31:46.980 --> 00:31:48.150
context switches, and the like.

00:31:48.220 --> 00:31:50.930
But if we really want more data,
we can start clicking on these

00:31:50.930 --> 00:31:54.120
individual instruments and seeing
that the scheduling instrument shows

00:31:54.360 --> 00:31:58.360
us time division ratios and how
much time the thread spent running

00:31:58.360 --> 00:32:02.090
versus blocked versus interrupted,
that sort of thing.

00:32:02.100 --> 00:32:06.450
System calls, much like its name,
will show us a count and duration,

00:32:06.450 --> 00:32:08.590
and same with the VM operations.

00:32:09.360 --> 00:32:11.470
But again,
all of this data isn't terribly

00:32:11.470 --> 00:32:14.780
useful until you really combine
it with the thread strategy.

00:32:14.780 --> 00:32:18.280
So in the top left,
I'll select the thread strategy.

00:32:20.110 --> 00:32:23.940
And you'll notice that
we have a lot of data.

00:32:24.020 --> 00:32:26.160
Now, how do you use System Trace?

00:32:26.280 --> 00:32:30.990
Well, System Trace really requires some
thought on the part of the developer.

00:32:31.020 --> 00:32:34.360
You need to come up with a mental
model of what's going on in your

00:32:34.360 --> 00:32:37.730
application and use System Trace
to attempt to verify that.

00:32:37.730 --> 00:32:41.220
Now, when you notice discrepancies,
that's when you have really good

00:32:41.220 --> 00:32:43.660
opportunities for improvement.

00:32:43.810 --> 00:32:48.470
So with our fractal drawer,
we really expect that we've got as

00:32:48.470 --> 00:32:52.860
many threads as Dispatch will create,
namely two for a iPad 2.

00:32:53.160 --> 00:32:58.160
And we'd expect that they're both
running as fast as they possibly can

00:32:58.160 --> 00:33:01.470
until the completion of the drawing.

00:33:02.180 --> 00:33:05.930
Well, looking at the thread strategy,
we'll immediately notice

00:33:06.400 --> 00:33:09.230
that there's a lot more than
two Dispatch worker threads.

00:33:09.310 --> 00:33:12.740
In fact, there's one, two, three, four,
five,

00:33:12.850 --> 00:33:16.740
at least five Dispatch worker threads
just to draw it the first time.

00:33:16.850 --> 00:33:17.450
Now, why is that?

00:33:17.540 --> 00:33:19.240
Dispatch really is a great API.

00:33:19.240 --> 00:33:23.280
Why is it creating more threads
than our number of CPUs?

00:33:23.420 --> 00:33:26.670
Well, that's where we can zoom in here
and find out more information

00:33:26.750 --> 00:33:29.150
about what all these events are.

00:33:30.110 --> 00:33:32.280
So you'll notice that
there's a bunch of colors,

00:33:32.370 --> 00:33:36.310
and these are all the thread states of
our different dispatch worker threads.

00:33:36.460 --> 00:33:39.540
And if I show the legend,
this is a great reminder

00:33:39.540 --> 00:33:43.440
for what they are,
unknown, running, supervisor, et cetera.

00:33:43.440 --> 00:33:46.130
Now you'll also notice
a lot of events here,

00:33:46.130 --> 00:33:48.400
and some of these are system calls.

00:33:48.400 --> 00:33:51.790
So you'll notice by the phone,
it's a Mach system call,

00:33:51.870 --> 00:33:54.510
and it has a stack trace
associated with it.

00:33:54.660 --> 00:33:58.430
And here is a page cache
hit from the VM Instrument.

00:33:58.460 --> 00:34:02.050
And so all of the Instruments are
contributing to this one view.

00:34:02.420 --> 00:34:04.760
Now, for our application,
you'll notice all these

00:34:04.760 --> 00:34:06.240
copy on write faults.

00:34:06.440 --> 00:34:07.330
Copy on write.

00:34:07.340 --> 00:34:09.210
That's why the cow is there.

00:34:09.330 --> 00:34:15.210
But you'll notice here that we've got our
Mandelbrot renderer in the stack trace

00:34:15.210 --> 00:34:18.910
on pretty much every single one of these.

00:34:19.080 --> 00:34:20.200
Okay, so what could be going on?

00:34:20.390 --> 00:34:24.640
Well, we're probably using
memory in a suboptimal way.

00:34:24.640 --> 00:34:27.540
And we need to go back to the
drawing board of our mental model of

00:34:27.550 --> 00:34:29.870
what's going on in our application.

00:34:29.900 --> 00:34:33.640
So dispatch is stalling because we're
making all these copy on write faults.

00:34:33.700 --> 00:34:37.270
And all we're doing is passing
that image back from each completed

00:34:37.270 --> 00:34:39.420
scan line to the main thread.

00:34:39.560 --> 00:34:42.590
Well, hmm, when we pass the image
back to the main thread,

00:34:42.600 --> 00:34:44.750
it doesn't actually do the copy then.

00:34:44.750 --> 00:34:47.100
It does the copy whenever
we touch the pages,

00:34:47.100 --> 00:34:50.020
which would be when the the
next scan line continues on

00:34:50.020 --> 00:34:52.210
its vertical progression.

00:34:52.370 --> 00:34:53.740
Okay, that gives us an idea then.

00:34:53.740 --> 00:34:56.300
Like, what happens if we did
horizontal scan lines?

00:34:56.370 --> 00:34:59.500
Because the pages go horizontally,
and maybe that'll actually reduce

00:34:59.600 --> 00:35:00.800
our number of copy on write faults.

00:35:00.840 --> 00:35:03.300
So, let's go ahead and try that.

00:35:03.340 --> 00:35:05.850
You'll notice that the
application actually had a left

00:35:05.850 --> 00:35:07.240
to right and top to bottom.

00:35:07.240 --> 00:35:09.450
And so it'll render
left to right to start.

00:35:09.460 --> 00:35:11.360
I'm just going to switch
it to top to bottom.

00:35:13.320 --> 00:35:16.640
And you'll have to trust me here,
but the millisecond timer at the

00:35:16.640 --> 00:35:21.360
bottom showed around six seconds
for the first render and about

00:35:21.360 --> 00:35:24.240
4.4 seconds for the second render.

00:35:24.360 --> 00:35:26.940
So we actually did get a
really good speedup there,

00:35:26.960 --> 00:35:30.960
and let's go ahead and verify
what we thought in System Trace.

00:35:30.970 --> 00:35:33.740
So going back to the thread strategy.

00:35:33.940 --> 00:35:37.490
We'll see again that we've got all
of these dispatch worker threads,

00:35:37.680 --> 00:35:42.000
lots of cows, and lots of events,
lots of different thread states.

00:35:42.130 --> 00:35:46.500
But once we get to that idle portion,
on the right,

00:35:46.740 --> 00:35:50.260
There's only really two,
maybe three threads working very, very,

00:35:50.260 --> 00:35:50.750
very hard.

00:35:50.750 --> 00:35:54.250
So we've really actually
improved the situation.

00:35:54.310 --> 00:35:56.340
It's closer to our model of
what should be happening,

00:35:56.340 --> 00:35:59.630
and we have two threads running
as fast as they possibly can.

00:35:59.640 --> 00:36:03.780
Now looking at these events,
we have a lot of zero fill faults here,

00:36:03.890 --> 00:36:08.590
and this is something to be expected
when working with large pieces of memory,

00:36:08.590 --> 00:36:12.820
because as Steve described,
when the VM system gives you a new page,

00:36:12.820 --> 00:36:14.950
it has to fill it with zeros.

00:36:16.160 --> 00:36:19.110
So either way,
we've both verified what we

00:36:19.120 --> 00:36:22.720
thought was going to be happening,
as well as gained a

00:36:22.720 --> 00:36:25.950
really good improvement,
over 20% in our application.

00:36:25.980 --> 00:36:28.470
So thank you very much,
that's System Trace.

00:36:28.480 --> 00:36:30.370
I hope you guys use it and like it.

00:36:37.390 --> 00:36:39.300
Alrighty, that was pretty awesome.

00:36:39.300 --> 00:36:42.990
Okay, so...

00:36:43.480 --> 00:36:48.390
Let's move on to our network
instrumentation for iOS.

00:36:48.500 --> 00:36:50.140
So we have two new Instruments for you.

00:36:50.280 --> 00:36:52.440
One is called the
Network Connections Instrument.

00:36:52.640 --> 00:36:57.300
This is all about looking at data volume
coming in and out over TCP and UDP.

00:36:57.380 --> 00:37:03.310
You can use this to debug latency issues,
issues with drop packets, et cetera.

00:37:03.470 --> 00:37:04.880
Here it is in all its glory.

00:37:04.900 --> 00:37:09.050
It's tracking a spike in
incoming and outgoing traffic.

00:37:09.160 --> 00:37:11.410
But we also have another
interesting Instrument called

00:37:11.410 --> 00:37:13.350
the Network Activity Instrument.

00:37:13.470 --> 00:37:16.740
We put this Instrument in the
Energy Diagnostics Template.

00:37:16.860 --> 00:37:18.600
Now why would we do that?

00:37:18.730 --> 00:37:22.320
Well,
that's because using radios takes power.

00:37:22.430 --> 00:37:26.520
And when you use the network,
whether it's incoming or outgoing,

00:37:26.650 --> 00:37:29.060
you want to use it as
efficiently as possible.

00:37:29.180 --> 00:37:32.020
Because whenever you dribble
in or dribble out data,

00:37:32.150 --> 00:37:34.360
the power's out-- if the
radios are already up,

00:37:34.490 --> 00:37:38.500
cellular and Wi-Fi, for example,
you have to go into a higher power state,

00:37:38.610 --> 00:37:41.380
then they handle the traffic,
and then eventually they go down.

00:37:41.520 --> 00:37:43.320
They can't do this on a dime.

00:37:43.320 --> 00:37:44.920
So this burns up extra power.

00:37:44.920 --> 00:37:47.290
So if you can collect a
lot more networking data

00:37:47.710 --> 00:37:51.880
coming in or out together,
then the radios can power down sooner,

00:37:51.880 --> 00:37:54.720
not waste as much power,
the batteries will last longer,

00:37:54.720 --> 00:37:57.980
and your customers will be happier,
and your customers are our customers,

00:37:57.980 --> 00:37:59.480
so everybody's happy.

00:37:59.480 --> 00:38:01.580
So that's the
Network Activity Instrument.

00:38:01.580 --> 00:38:03.780
I encourage you to take a look at it.

00:38:03.780 --> 00:38:05.700
It's very interesting.

00:38:06.160 --> 00:38:08.860
Now finally,
we'd like to conclude with our

00:38:08.880 --> 00:38:12.490
Arc Instrumentation support.

00:38:12.490 --> 00:38:12.490
So,

00:38:12.770 --> 00:38:15.840
We've introduced Arc at
the conference this week,

00:38:15.860 --> 00:38:17.990
and the support that we'll be
talking about in Instruments

00:38:18.120 --> 00:38:20.180
will be in the next seat.

00:38:20.780 --> 00:38:21.700
So what is Arc again?

00:38:21.700 --> 00:38:23.920
It's automatic reference counting.

00:38:24.010 --> 00:38:26.560
Essentially,
this is the compiler taking the burden of

00:38:26.560 --> 00:38:29.600
retains and releases and auto releases,
all that bookkeeping,

00:38:29.600 --> 00:38:33.940
off your shoulders and doing it itself,
more precisely.

00:38:33.970 --> 00:38:37.740
This allows you to focus on the
relationships between the objects

00:38:37.800 --> 00:38:44.180
in your app as you think of them
for the purposes of your app,

00:38:44.180 --> 00:38:44.180
not for the purpose of managing memory.

00:38:44.560 --> 00:38:48.230
Now, as with anything,
it's not a panacea.

00:38:48.440 --> 00:38:52.020
You still can have leaks in some spots,
and you can leak graphs of

00:38:52.020 --> 00:38:53.600
objects or cycles of objects.

00:38:53.600 --> 00:38:57.700
But this is where Instruments and the
new support we've introduced comes in.

00:38:57.700 --> 00:39:00.330
Now, Instruments,
as I'm sure you're all aware,

00:39:00.330 --> 00:39:02.410
has an instrument called
a leaks instrument.

00:39:02.410 --> 00:39:06.820
This will find memory you've allocated
for which you no longer have a reference.

00:39:07.020 --> 00:39:09.780
And for Arc support,
we're introducing cycle

00:39:09.860 --> 00:39:11.130
detection as well.

00:39:11.160 --> 00:39:13.120
So what's all this about?

00:39:13.120 --> 00:39:16.300
Well, here you have your and you have
your object graph laid out and

00:39:16.300 --> 00:39:18.670
you have references to everything.

00:39:18.860 --> 00:39:23.360
But some point along the way,
a critical reference is released.

00:39:23.410 --> 00:39:27.800
And perhaps that will cause an
entire cycle of objects to be leaked.

00:39:27.820 --> 00:39:28.620
They're lost.

00:39:28.820 --> 00:39:32.120
They're out there taking up who knows
how much memory away from your app

00:39:32.130 --> 00:39:34.300
and all the other apps in the system.

00:39:34.360 --> 00:39:36.670
In your main memory,
all your other references

00:39:36.740 --> 00:39:38.120
have nothing back to them.

00:39:38.140 --> 00:39:40.870
So they're going to stay
there until your app exits.

00:39:42.070 --> 00:39:45.240
So when you're using the Lix
Instrument and when you're

00:39:45.240 --> 00:39:49.230
designing your apps and using Arc,
what you need to remember is

00:39:49.230 --> 00:39:53.290
probably you really have Arc objects,
automatic reference counting objects,

00:39:53.290 --> 00:39:56.070
and you might still have some
manually reference counted objects.

00:39:56.110 --> 00:39:58.970
So you have a legacy old school
framework that you've been

00:39:59.060 --> 00:40:03.160
using as part of your app and it
hasn't been converted over yet.

00:40:03.160 --> 00:40:07.910
And amongst all those objects,
Arc and MRC will be references.

00:40:08.240 --> 00:40:13.120
And along the way you can leak graphs,
complex graphs of objects.

00:40:13.330 --> 00:40:17.050
Well, Instruments can look at those and
detect the fundamental cycles.

00:40:17.150 --> 00:40:19.880
It'll find the graph itself,
all the leaks,

00:40:19.940 --> 00:40:22.440
and isolate the fundamental cycles.

00:40:22.480 --> 00:40:27.070
And it will define for you all the
I of Rs that point to which objects.

00:40:27.070 --> 00:40:30.760
So A points to B with a certain I of R,
and B points to C with a certain I of R,

00:40:30.760 --> 00:40:32.280
and so forth.

00:40:32.280 --> 00:40:35.470
This provides you the information
that you need to surgically determine

00:40:35.840 --> 00:40:40.700
in place perhaps a zeroing weak
reference to eliminate that cycle.

00:40:40.790 --> 00:40:41.540
That cycle goes away.

00:40:41.540 --> 00:40:45.620
And you proceed down your list
of cycles in Instruments to get

00:40:45.620 --> 00:40:47.400
rid of all the other cycles.

00:40:47.450 --> 00:40:49.370
Until finally, again,
if you were interacting

00:40:49.370 --> 00:40:51.340
with some MRC code,
you have some stragglers,

00:40:51.340 --> 00:40:54.930
you use your normal leaks workflow,
and you identify where you

00:40:54.930 --> 00:40:57.400
need to put a release in,
you do it,

00:40:57.400 --> 00:41:00.170
and then your app is leak free.

00:41:00.180 --> 00:41:01.820
So to demonstrate this
awesome technology,

00:41:01.820 --> 00:41:04.640
I'd like to invite Daniel back on stage.

00:41:04.640 --> 00:41:04.640
Daniel Delwood,

00:41:10.440 --> 00:41:13.800
Thank you very much, Steve.

00:41:13.930 --> 00:41:17.100
So for this demo,
we're actually just going to be using

00:41:17.100 --> 00:41:19.620
a very simple application on Mac OS X.

00:41:19.910 --> 00:41:27.340
And it's just actually working
on-- I've written a custom

00:41:27.340 --> 00:41:28.540
tree node implementation.

00:41:28.540 --> 00:41:33.030
And so it's working on showing whether
that actually works right in an outline

00:41:33.030 --> 00:41:35.070
view and just how the tree node works.

00:41:35.220 --> 00:41:36.770
And so I can create one.

00:41:36.780 --> 00:41:37.500
I can replace it.

00:41:37.550 --> 00:41:39.400
And it's just randomly
creating different trees.

00:41:39.400 --> 00:41:43.080
And hopefully, these will all be released
properly when I replace the tree.

00:41:43.170 --> 00:41:46.900
But as you can see, the outline view is
working pretty much right.

00:41:47.070 --> 00:41:48.690
So I'm running Arc.

00:41:48.940 --> 00:41:50.200
This is excellent.

00:41:50.390 --> 00:41:52.530
I didn't have to think
about retain and release.

00:41:52.680 --> 00:41:54.610
But can I still leak?

00:41:54.730 --> 00:41:57.050
As Steve was talking about, yeah.

00:41:57.690 --> 00:42:03.100
So I'll use the profile action
and select the leaks template.

00:42:03.320 --> 00:42:08.700
And Instruments pops up showing me
both two instruments in the template.

00:42:08.860 --> 00:42:13.200
Allocations,
which will get me the malloc-free retain,

00:42:13.200 --> 00:42:16.810
release, and auto-release events,
but the leaks instrument as well.

00:42:17.010 --> 00:42:21.370
And so as I actually create and
replace some of these trees,

00:42:21.610 --> 00:42:24.560
some bigger and some smaller,
you notice that leaks will

00:42:24.690 --> 00:42:30.070
eventually kick in in the background
and notice if I have any leaks.

00:42:30.440 --> 00:42:33.340
And so there it is, a lot of leaks.

00:42:34.000 --> 00:42:34.690
I'll go ahead and stop.

00:42:34.720 --> 00:42:37.220
So I select the leaks instrument.

00:42:37.220 --> 00:42:40.230
And by default,
the leaks instrument shows

00:42:40.230 --> 00:42:41.520
us leaks by backtrace.

00:42:41.730 --> 00:42:45.490
And this is for a world
of manual retain counting.

00:42:45.500 --> 00:42:50.700
You have a bunch of different leaks,
and some of them happen at

00:42:50.730 --> 00:42:53.010
the same place multiple times.

00:42:53.060 --> 00:42:58.140
This is because you have probably
a missing release or some flaw in

00:42:58.270 --> 00:43:01.050
your code that's going to do the
same thing over and over again.

00:43:01.060 --> 00:43:04.690
So we try to help you out by
aggregating those into the

00:43:04.750 --> 00:43:06.500
backtrace where they're created.

00:43:06.500 --> 00:43:09.500
But this really doesn't tell
you much about the leak.

00:43:09.580 --> 00:43:12.160
Does this array reference a tree node?

00:43:12.160 --> 00:43:13.620
Does a tree node reference
another tree node?

00:43:13.620 --> 00:43:15.960
That's what the cycle detection's for.

00:43:15.960 --> 00:43:18.530
So from the jump bar,
I'm just going to pull

00:43:18.530 --> 00:43:19.830
down and select cycles.

00:43:19.940 --> 00:43:21.830
And here we go.

00:43:21.880 --> 00:43:26.660
So it's finding the cycles
inside of our leaked graph.

00:43:26.740 --> 00:43:32.060
Now, you notice that I had some bigger
trees and some smaller trees.

00:43:32.260 --> 00:43:34.640
And so actually,
we had a really complex cycle

00:43:34.640 --> 00:43:37.140
there because one node was
holding onto another node.

00:43:37.140 --> 00:43:41.210
And the idea is that we would
like to make this as usable

00:43:41.210 --> 00:43:43.030
as possible for developers.

00:43:43.040 --> 00:43:48.490
And we identify the simple
cycles within the complex cycle.

00:43:48.500 --> 00:43:51.510
And so if we turn these open,
you'll notice that what's

00:43:51.710 --> 00:43:54.170
shown on the left is exactly
the graph shown on the right.

00:43:54.220 --> 00:43:59.200
We have a tree node, which has a children
mutable array property.

00:43:59.200 --> 00:44:03.740
And that mutable array has a
reference to its list of objects,

00:44:03.780 --> 00:44:07.120
which has some malloc bytes,
which also point to

00:44:07.250 --> 00:44:09.200
eventually a tree node.

00:44:09.200 --> 00:44:13.110
And that children,
that child node of that

00:44:13.110 --> 00:44:15.250
original tree node,
actually has a parent pointer,

00:44:15.260 --> 00:44:16.780
which isn't surprising.

00:44:16.780 --> 00:44:20.400
But the important part to
notice here is that it's red.

00:44:20.440 --> 00:44:22.960
And that's because it's
a strong reference.

00:44:23.020 --> 00:44:26.260
Now,
you'll notice we have red and blue here.

00:44:26.260 --> 00:44:30.860
And that's because arc works with
strong references and weak references.

00:44:31.980 --> 00:44:36.150
And you're going to be living in a world
that has both manual retain counting

00:44:36.160 --> 00:44:38.190
as well as automatic retain counting.

00:44:38.200 --> 00:44:42.040
And so those manual retain
counting references,

00:44:42.060 --> 00:44:44.730
we can't tell for sure
that they're strong.

00:44:44.820 --> 00:44:47.240
And so we'll display
those to you in blue.

00:44:47.240 --> 00:44:50.000
They're very likely strong references.

00:44:50.000 --> 00:44:54.240
But as your project gets
converted over to arc and more

00:44:54.240 --> 00:44:56.710
and more of your frameworks do,
you'll notice that more and more

00:44:56.710 --> 00:45:00.860
of these become red and the cycle
analysis becomes more accurate.

00:45:00.880 --> 00:45:05.300
Now, the great part of this is that if
you're running manually retain counting,

00:45:05.300 --> 00:45:07.700
the cycle detector should
work for you as well.

00:45:09.520 --> 00:45:11.290
So, how do we go ahead and fix this?

00:45:11.370 --> 00:45:14.630
Well, we could break any one of these,
but it probably makes the most sense

00:45:14.640 --> 00:45:16.400
to break this parent reference.

00:45:16.400 --> 00:45:19.890
So I'm just going to go ahead and
double click on the reference.

00:45:22.460 --> 00:45:25.240
And Xcode popped to the front.

00:45:25.240 --> 00:45:28.200
And you'll notice here that
it's taken me to the declaration

00:45:28.200 --> 00:45:29.800
of my tree node interface.

00:45:29.940 --> 00:45:32.630
And I've got my children property and,
oh, there's the parent.

00:45:32.790 --> 00:45:35.220
But I didn't declare it week.

00:45:35.360 --> 00:45:38.360
Now, if I had a property,
I could just declare week there,

00:45:38.360 --> 00:45:41.400
but I'm actually just setting
this in the initializer.

00:45:41.530 --> 00:45:47.950
So, in the Ivar declaration,
I just put in the magic keyword week.

00:45:47.950 --> 00:45:49.620
And I'll hit profile.

00:45:50.710 --> 00:45:54.180
And this time, those up references will
be zeroing weak pointers,

00:45:54.180 --> 00:45:57.780
even safer than the just
unretained pointers before.

00:45:57.780 --> 00:46:00.680
And so as I create my tree nodes,
you'll notice that the

00:46:00.840 --> 00:46:04.510
outline view updates properly,
still works right,

00:46:04.570 --> 00:46:08.710
and we'll notice in the background
that Instruments detected no leaks.

00:46:08.820 --> 00:46:11.760
So we solved all of those just
by solving that one cycle,

00:46:11.760 --> 00:46:13.060
which was repeated.

00:46:13.060 --> 00:46:15.070
So, thank you very much.

00:46:15.260 --> 00:46:18.500
Hope you guys really
enjoy the cycle detector.

00:46:24.630 --> 00:46:25.480
Thank you, Daniel.

00:46:25.570 --> 00:46:27.560
That was pretty awesome, too.

00:46:27.690 --> 00:46:30.160
Well, we're already towards
the end of the session.

00:46:30.160 --> 00:46:34.400
In conclusion, we're thrilled with the
instrumentation that we've been

00:46:34.400 --> 00:46:35.680
able to provide you in this release.

00:46:35.710 --> 00:46:39.830
It's going to allow you to
create far more concurrent code,

00:46:39.830 --> 00:46:42.760
to use System Trace to make
your code run more efficiently.

00:46:42.760 --> 00:46:45.660
It provides you this
programmatically API that you can

00:46:45.770 --> 00:46:47.760
look for performance regressions.

00:46:47.760 --> 00:46:51.160
And now we have this excellent
ARC support to detect leaked cycles

00:46:51.160 --> 00:46:53.170
and support your ARC development.

00:46:53.980 --> 00:46:57.050
So, for further information,
you can turn to Michael Jurowicz,

00:46:57.050 --> 00:46:59.270
the Developer Tools and
Performance Evangelist.

00:46:59.330 --> 00:47:02.890
You can also turn to our documentation,
which we are actively working on.

00:47:02.890 --> 00:47:06.290
And you can go to devforums.apple.com
and ask questions,

00:47:06.440 --> 00:47:09.940
where you'll get answers from
your peers here in the audience,

00:47:09.940 --> 00:47:12.690
in your industry,
the engineers here on stage,

00:47:12.690 --> 00:47:13.890
and back at Apple.

00:47:15.410 --> 00:47:19.300
So there's some interesting new sessions
for the rest of the week that you might

00:47:19.300 --> 00:47:21.700
want to attend focused on performance.

00:47:21.750 --> 00:47:23.600
They'll be using Instruments as well.

00:47:23.650 --> 00:47:25.570
Thank you very much for
coming and have a good week.