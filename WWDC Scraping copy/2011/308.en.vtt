WEBVTT

00:00:10.520 --> 00:00:11.730
Welcome, everybody.

00:00:11.740 --> 00:00:15.360
This is the Blocks and
Grand Central Dispatch in Practice talk.

00:00:15.430 --> 00:00:20.350
I'm Dave Zarzycki and I'm
from Developer Technologies.

00:00:22.210 --> 00:00:24.590
So where are we?

00:00:24.680 --> 00:00:27.450
We're talking about
Grand Central Dispatch and blocks,

00:00:27.470 --> 00:00:31.500
and these are very low-level
technologies that are available in

00:00:31.500 --> 00:00:37.060
Mac OS X and iOS for implementing
a wide variety of services.

00:00:37.690 --> 00:00:42.530
Everything on the system uses it,
all the way from lib system,

00:00:42.540 --> 00:00:50.590
from very traditional Unix APIs up to
UIKit and applications and foundation.

00:00:51.640 --> 00:00:53.840
So what are we going to talk about?

00:00:54.030 --> 00:00:56.440
Well, there's two major or
three major sections.

00:00:56.580 --> 00:00:59.420
First, we're going to do an introduction,
talk about blocks,

00:00:59.500 --> 00:01:02.740
Grand Central Dispatch,
and then the latter half of, well,

00:01:02.740 --> 00:01:07.120
once you start using these technologies,
what are the subtle things you

00:01:07.280 --> 00:01:11.460
need to be aware of that can impact
your design or maybe surprise you

00:01:11.460 --> 00:01:13.630
or surprise you in great ways?

00:01:13.630 --> 00:01:18.740
So that will involve what blocks automate
and what blocks don't automate as far

00:01:18.830 --> 00:01:21.190
as memory management is concerned.

00:01:22.830 --> 00:01:26.500
So blocks, what are blocks?

00:01:26.600 --> 00:01:29.400
Blocks simplify function callbacks.

00:01:29.510 --> 00:01:32.170
They allow you to...

00:01:32.520 --> 00:01:38.400
and I will talk about the
functions of blocks and GCD.

00:01:38.400 --> 00:01:50.920
We have a function, a body of code.

00:01:50.930 --> 00:01:54.400
We have our curlies,
we have some statements inside.

00:01:54.400 --> 00:01:56.540
Well, guess what?

00:01:56.540 --> 00:01:57.740
We have blocks.

00:01:57.740 --> 00:01:58.650
They have bodies of code.

00:01:58.940 --> 00:02:00.000
They have the same curlies.

00:02:00.000 --> 00:02:02.380
They have the same
statements you already know.

00:02:05.760 --> 00:02:08.360
Now here's where things start to diverge.

00:02:08.430 --> 00:02:11.210
You obviously know pointers in C.

00:02:11.350 --> 00:02:14.320
Well, with blocks,
we have a special kind of pointer,

00:02:14.320 --> 00:02:16.060
and it uses the caret.

00:02:16.380 --> 00:02:20.070
And as you can see,
we can declare a pointer to

00:02:20.070 --> 00:02:22.460
a function and a pointer to a
block with these characters.

00:02:22.460 --> 00:02:26.860
In fact, this is the only difference
between these two pointers.

00:02:26.870 --> 00:02:30.120
One uses a star, one uses the caret.

00:02:30.230 --> 00:02:32.420
Now, of course,
you all are good programmers and you

00:02:32.420 --> 00:02:36.460
would never just type that out in random
places in your code and use typedefs.

00:02:36.460 --> 00:02:39.640
The typedefs look the same.

00:02:39.640 --> 00:02:41.820
They're the same syntax, same style.

00:02:41.820 --> 00:02:46.280
And in fact, after that point,
functions and blocks look the same.

00:02:46.300 --> 00:02:48.300
They have a type.

00:02:48.300 --> 00:02:49.960
You can assign to them.

00:02:49.960 --> 00:02:52.770
And in fact, you can call them the same.

00:02:55.930 --> 00:02:59.300
So let's go back to these blocks of code.

00:02:59.330 --> 00:03:01.800
Let's make some room.

00:03:01.800 --> 00:03:03.790
We're going to add some arguments.

00:03:04.140 --> 00:03:06.050
And in fact, the arguments are the same.

00:03:06.220 --> 00:03:08.600
You declare them the same,
they use parentheses,

00:03:08.610 --> 00:03:14.210
you have a comma separated list, int A,
int B.

00:03:14.450 --> 00:03:16.540
Well, let's see where they
start to diverge now.

00:03:16.540 --> 00:03:18.260
Well, with a function,
we need to name it.

00:03:18.300 --> 00:03:19.940
So here's my comparison function.

00:03:19.940 --> 00:03:23.490
And it needs a return value,
so we're going to say end.

00:03:23.530 --> 00:03:25.320
What are we going to do for a block?

00:03:25.390 --> 00:03:27.640
Do you remember that special
character we talked about,

00:03:27.720 --> 00:03:28.280
the carrot?

00:03:28.280 --> 00:03:29.930
There, that's it.

00:03:29.980 --> 00:03:31.420
One character.

00:03:31.420 --> 00:03:34.190
And if you missed that,
let's highlight it.

00:03:34.190 --> 00:03:34.970
That's it.

00:03:34.970 --> 00:03:36.760
Just one little character.

00:03:36.760 --> 00:03:38.720
There it is, hiding there.

00:03:38.720 --> 00:03:41.900
But it's a huge character,
and it makes a big difference,

00:03:41.900 --> 00:03:43.300
as we're about to see.

00:03:45.380 --> 00:03:47.510
So let's talk about implementing
a sort function with the little

00:03:47.570 --> 00:03:50.160
comparison routine we just implemented.

00:03:50.250 --> 00:03:54.460
With a function, we would, of course,
take some kind of pointer

00:03:54.460 --> 00:03:58.220
to an array of integers,
maybe a size of the integer array,

00:03:58.220 --> 00:04:02.150
and then we'll take our function
callback to compare them and let

00:04:02.150 --> 00:04:04.240
the sort routine do its magic.

00:04:04.640 --> 00:04:06.850
With blocks, it's the same.

00:04:06.850 --> 00:04:10.520
We're going to have our pointer
to the array of integers,

00:04:10.520 --> 00:04:12.940
maybe the size of it, and our block.

00:04:12.940 --> 00:04:13.640
Okay.

00:04:13.840 --> 00:04:15.460
Everything's similar.

00:04:15.460 --> 00:04:16.470
We know this.

00:04:16.580 --> 00:04:18.610
Well, how are we going to use it?

00:04:18.610 --> 00:04:21.910
Well, with a function,
we're going to take our array,

00:04:21.910 --> 00:04:25.060
pass up the size of it,
and then we're going to use that

00:04:25.060 --> 00:04:27.660
name we declared for function,
my compare,

00:04:27.660 --> 00:04:29.820
and pass it to the sort array.

00:04:29.820 --> 00:04:31.840
Well, what do we do for blocks?

00:04:31.840 --> 00:04:34.040
Here's where the power kicks in.

00:04:34.640 --> 00:04:37.360
We're going to actually just
take that body of code and put

00:04:37.480 --> 00:04:38.960
it right in our sort function.

00:04:38.960 --> 00:04:43.470
This keeps the usage close
to the definition and usage

00:04:43.470 --> 00:04:45.660
right in the same spot.

00:04:45.680 --> 00:04:47.520
There's no ambiguity.

00:04:47.520 --> 00:04:49.140
It's very clear.

00:04:49.140 --> 00:04:51.380
It's right when you're
thinking about it anyway.

00:04:51.380 --> 00:04:55.800
And in fact, that body of code for
the block disappears.

00:04:55.800 --> 00:04:57.100
They all stay together.

00:04:57.100 --> 00:04:58.220
Okay.

00:04:58.740 --> 00:05:00.980
So this is a basic block.

00:05:00.980 --> 00:05:02.240
All right, that's cool.

00:05:02.240 --> 00:05:05.900
We understand they're the same,
but one might not see this as

00:05:05.900 --> 00:05:08.160
really saving a lot of time.

00:05:08.220 --> 00:05:11.280
So let's build on this and
talk about trivial blocks

00:05:11.280 --> 00:05:16.530
versus non-trivial functions,
or let's implement a configurable sort.

00:05:16.600 --> 00:05:19.960
So starting again with our
function and block comparison,

00:05:19.960 --> 00:05:25.270
we have our sort as we just implemented,
taking a block, and we can add maybe some

00:05:25.590 --> 00:05:27.360
kind of reverse argument.

00:05:27.360 --> 00:05:29.670
We want to do a reverse sort.

00:05:30.030 --> 00:05:31.300
Well, here we go.

00:05:31.530 --> 00:05:32.460
That's it.

00:05:32.460 --> 00:05:36.660
The reverse argument is in scope
in our function when we are calling

00:05:36.680 --> 00:05:41.250
sort and the block is in scope when
we declare it and pass it to sort.

00:05:41.530 --> 00:05:44.640
And because of that,
we can just use the reverse variable or

00:05:44.640 --> 00:05:49.730
any other variable we want inside of that
block because everything is in scope.

00:05:50.740 --> 00:05:54.700
: Well, let's see what we'd have
to do for functions now.

00:05:54.840 --> 00:05:55.130
All right.

00:05:55.140 --> 00:05:58.700
So here we're starting
from a very similar point.

00:05:58.780 --> 00:06:02.900
We have our reverse argument,
we have our sort, but we have a problem.

00:06:03.050 --> 00:06:05.740
That comparison function
is defined somewhere else.

00:06:05.970 --> 00:06:10.710
We need to get that data to it somehow.

00:06:10.900 --> 00:06:12.230
But how?

00:06:12.230 --> 00:06:14.500
Well, we need to pack that reverse
argument in a data structure.

00:06:14.700 --> 00:06:17.350
And then we're going to have
to change our sort routine.

00:06:17.410 --> 00:06:18.900
All of a sudden, we have to change APIs.

00:06:18.980 --> 00:06:22.090
We have to go then deal with the
ripple effect of everybody that

00:06:22.090 --> 00:06:25.250
uses sort and now pass in this data.

00:06:25.330 --> 00:06:29.060
And in fact, we now take the address of
that data and pass it in.

00:06:29.480 --> 00:06:34.040
So you can see the address of D there,
and the complexity is just beginning.

00:06:34.070 --> 00:06:36.540
Now we need to move the code down.

00:06:36.690 --> 00:06:39.820
Well, not literally,
but somewhere else in our code.

00:06:39.870 --> 00:06:43.790
We need to actually go back and
declare that data structure.

00:06:43.850 --> 00:06:48.470
We need to add whatever arguments we
want to pass in to the sort routine.

00:06:48.680 --> 00:06:51.690
We then need to go to
our cert routine again,

00:06:51.800 --> 00:06:54.350
make a lot of room.

00:06:54.840 --> 00:06:57.700
Add a context parameter,
and because this is C, of course,

00:06:57.700 --> 00:06:58.560
it's a void star.

00:06:58.560 --> 00:07:00.040
It's generic.

00:07:00.040 --> 00:07:04.240
We now need to cast it
back to that data type.

00:07:04.240 --> 00:07:07.210
And then we finally use it,
but it's abstracted away

00:07:07.220 --> 00:07:10.860
through pointer indirection,
and we have a lot more code.

00:07:11.120 --> 00:07:16.840
And worst of all, as the slash, slash,
slash shows, that code is far away from

00:07:17.030 --> 00:07:19.280
the actual implementation.

00:07:19.280 --> 00:07:22.850
We've had to deal with the mental
overhead of code over here,

00:07:22.850 --> 00:07:23.960
code over there.

00:07:23.970 --> 00:07:26.080
It's just starting to get messy.

00:07:28.130 --> 00:07:29.260
So that's it.

00:07:29.320 --> 00:07:32.750
These are basic blocks,
basic non-trivial blocks,

00:07:32.750 --> 00:07:35.260
but you can see that we're already
starting to save a lot of code and

00:07:35.260 --> 00:07:40.040
a lot of mental overhead of basic
boilerplate of C data structures

00:07:40.190 --> 00:07:43.230
and pointer and direction.

00:07:43.910 --> 00:07:44.390
Let's move on.

00:07:44.390 --> 00:07:45.160
Let's even build on this.

00:07:45.260 --> 00:07:50.350
Let's build an even more non-trivial
block and let's extract results from it.

00:07:50.350 --> 00:07:53.420
Well, here we are with the code
that we just left off with.

00:07:53.420 --> 00:07:57.830
We're going to make a little bit
of room in the block and just to

00:07:58.200 --> 00:08:01.590
declare another variable in the stack,
a count.

00:08:01.630 --> 00:08:05.300
Let's say we're going to count the number
of comparisons we made because we're

00:08:05.300 --> 00:08:07.430
trying to do some performance analysis.

00:08:07.430 --> 00:08:09.070
Well, here we are.

00:08:09.130 --> 00:08:13.310
We just declare int count,
initialize it to zero on the stack.

00:08:14.830 --> 00:08:27.050
and David Koehn.

00:08:27.490 --> 00:08:27.490
We are going to start
with the first step,

00:08:27.490 --> 00:08:27.490
which is to say count++ in our code
and then at the end we can log it and

00:08:27.490 --> 00:08:27.490
see how our sort routine is performing.

00:08:27.490 --> 00:08:27.490
That's almost true and that's why we
left a little bit of space in front

00:08:27.490 --> 00:08:27.490
of that declaration of the count.

00:08:27.800 --> 00:08:31.150
We need one more thing,
under under block.

00:08:31.220 --> 00:08:34.720
This keyword is the only
keyword you need to know to

00:08:34.720 --> 00:08:36.760
get information out of a block.

00:08:36.800 --> 00:08:40.350
And we'll talk more about why this is
the case at the end of the talk when we

00:08:40.360 --> 00:08:42.660
talk about memory management and blocks.

00:08:44.170 --> 00:08:45.040
But that's it.

00:08:45.140 --> 00:08:48.820
Three simple lines,
one of which is just logging.

00:08:48.820 --> 00:08:50.620
And we've gotten data out of a block.

00:08:50.740 --> 00:08:52.270
That simple.

00:08:53.190 --> 00:08:56.230
Let's look at what we'd
have to do with functions.

00:08:56.370 --> 00:08:59.840
You're not actually meant to read this,
because what ends up happening

00:08:59.840 --> 00:09:02.720
is we're adding yet more code,
and yet more code in lots of

00:09:02.870 --> 00:09:04.620
different places in our file.

00:09:04.690 --> 00:09:08.900
We're gonna have to, and in fact,
this is what that code looks like.

00:09:09.290 --> 00:09:12.550
Again, I don't expect you to read this,
but I'll just tell you what's going on.

00:09:12.550 --> 00:09:14.450
We had to go back to that data structure.

00:09:14.510 --> 00:09:17.730
We had to declare that, you know,
our out parameter.

00:09:17.730 --> 00:09:22.070
We had to then go to where we needed
the sort and then assign the result

00:09:22.070 --> 00:09:26.600
into our temporary stack variable
that we're going to pass to sort.

00:09:27.060 --> 00:09:30.870
And then finally, in our sort routine,
we now need to deal with the pointer

00:09:30.870 --> 00:09:34.710
indirection and making sure that we
actually save off the result and the data

00:09:34.740 --> 00:09:38.710
structure so that way after sort returns,
we can get at the result and it wasn't

00:09:38.750 --> 00:09:41.340
left behind in the sort routine function.

00:09:43.030 --> 00:09:45.400
So yeah, this gets really complicated.

00:09:45.400 --> 00:09:49.070
And as you can see,
it's a lot simpler with blocks.

00:09:49.210 --> 00:09:51.000
And in fact,
it can get even more complicated

00:09:51.120 --> 00:09:53.550
with functions when we start
dealing with asynchronous code,

00:09:53.550 --> 00:09:57.750
but we'll deal with that when we talk
about GCD and at the end of the talk

00:09:57.750 --> 00:10:00.260
when we deal with memory management.

00:10:00.500 --> 00:10:04.600
So in comparison, the mental overhead of
blocks versus functions,

00:10:04.720 --> 00:10:06.770
we've highlighted on the right.

00:10:06.830 --> 00:10:09.840
You just need to know that caret,
it may even be hard to see,

00:10:09.840 --> 00:10:13.360
but it's there, that one character,
and then under under block.

00:10:13.360 --> 00:10:15.900
That's all the mental overhead you need.

00:10:15.900 --> 00:10:17.900
But for functions, we have a lot.

00:10:17.900 --> 00:10:21.810
We have data structure definitions,
we have pointer indirection,

00:10:21.810 --> 00:10:23.760
we have code far away from usage.

00:10:23.760 --> 00:10:25.000
It's a big mess.

00:10:28.150 --> 00:10:29.860
So let's talk about blocks at Apple.

00:10:29.860 --> 00:10:31.440
We love them.

00:10:31.440 --> 00:10:34.020
And a lot of APIs use them.

00:10:34.020 --> 00:10:38.160
Some common patterns you'll see
are things like enumeration.

00:10:38.160 --> 00:10:42.680
And I'd like to use these two
examples enumerating a dictionary

00:10:42.680 --> 00:10:44.820
and an array of why we like them.

00:10:44.820 --> 00:10:49.380
With the Objective-C for each syntax,
you can only do one value at a time,

00:10:49.400 --> 00:10:50.160
one thing.

00:10:50.160 --> 00:10:53.990
But in the case of a dictionary,
we can iterate both the key and

00:10:53.990 --> 00:10:57.650
the value at the same time because
that's how we can define our block.

00:10:58.100 --> 00:11:01.880
And in fact, that's what the enumeration
of a dictionary does here.

00:11:02.550 --> 00:11:09.360
and David We can do that
for any containing object.

00:11:09.360 --> 00:11:11.780
Much more is available in the system.

00:11:11.780 --> 00:11:14.320
We have GCD, which we'll talk about.

00:11:14.320 --> 00:11:16.480
We have lots of callbacks.

00:11:16.480 --> 00:11:19.730
And in fact, callbacks are one of the
simplest and most common

00:11:19.730 --> 00:11:21.640
patterns and they look like this.

00:11:22.030 --> 00:11:23.630
Well, almost.

00:11:23.730 --> 00:11:27.570
That wouldn't be great if we're not going
to pass any arguments with the block.

00:11:27.570 --> 00:11:30.360
We could just delete them.

00:11:30.360 --> 00:11:33.760
In fact, that caret becomes the only
character you need to define a block.

00:11:33.760 --> 00:11:37.540
You're going to have to use the curlies
anyway with functions or blocks,

00:11:37.540 --> 00:11:41.820
but the caret is the only overhead
you need to do a callback.

00:11:42.340 --> 00:11:46.630
And with that,
I would like to transition to Kevin to

00:11:46.690 --> 00:11:49.810
talk about Grand Central Dispatch
and its use of these blocks and

00:11:49.810 --> 00:11:51.540
doing really powerful patterns.

00:11:51.660 --> 00:11:52.030
Good morning.

00:11:52.040 --> 00:11:53.420
My name is Kevin Van Vechten.

00:11:53.520 --> 00:11:56.590
I'm with the CoreOS team at Apple.

00:11:57.200 --> 00:11:59.740
So today we're going to talk
about Grand Central Dispatch,

00:11:59.740 --> 00:12:02.450
which builds upon the idea of blocks.

00:12:02.450 --> 00:12:05.750
And as Dave just outlined,
you saw how blocks are really great in

00:12:05.760 --> 00:12:11.420
terms of C syntax and eliminating a lot
of the conceptual overhead for defining

00:12:11.420 --> 00:12:18.330
blocks of code that can be called by the
system as an iterator or as a callback.

00:12:18.560 --> 00:12:22.940
Well, we built on that further and we
created Grand Central Dispatch,

00:12:22.940 --> 00:12:26.340
which really treats blocks
as fundamental objects.

00:12:26.340 --> 00:12:30.560
And like any objects, you can put them in
interesting data structures.

00:12:30.560 --> 00:12:34.060
The fundamental data structure
of Dispatch is a queue,

00:12:34.230 --> 00:12:38.270
and we'll talk about how we use
these queues to efficiently provide

00:12:38.270 --> 00:12:42.310
you with tools for serialization
in your multi-threaded code,

00:12:42.310 --> 00:12:46.060
for concurrency,
and for asynchronous execution.

00:12:48.020 --> 00:12:51.130
So you're probably familiar
with the concept of queues from,

00:12:51.130 --> 00:12:54.490
you know,
common computer science literature.

00:12:54.600 --> 00:12:58.080
They're pretty straightforward things,
simple linked list.

00:12:58.090 --> 00:13:00.760
In this case,
it's a linked list of blocks.

00:13:01.240 --> 00:13:03.740
Now,
the queues that we implement in GCD have

00:13:03.740 --> 00:13:06.060
some very important characteristics.

00:13:06.060 --> 00:13:09.220
The first of those is that
the queues are strictly FIFO.

00:13:09.220 --> 00:13:12.380
So anytime you enqueue a block,
that's always going to go

00:13:12.400 --> 00:13:15.740
on to the tail of the queue,
and anytime a block is dequeued,

00:13:15.740 --> 00:13:18.860
that's going to come off
of the head of the queue.

00:13:20.410 --> 00:13:24.100
And we've taken it a step further
and we've provided synchronization

00:13:24.100 --> 00:13:28.090
primitives that guarantee that
the end queue is always atomic.

00:13:28.200 --> 00:13:31.260
So you can have any number of threads
in your application and all of those

00:13:31.260 --> 00:13:34.940
threads can be enqueuing blocks onto
the same queue and will automatically

00:13:34.940 --> 00:13:39.030
sort it all out and make sure that
the blocks get enqueued in FIFO order

00:13:39.080 --> 00:13:41.510
and ultimately get run in FIFO order.

00:13:41.720 --> 00:13:44.920
So because it's safe to use these
queues for multiple threads,

00:13:44.920 --> 00:13:48.360
the queues themselves become
a synchronization primitive.

00:13:48.680 --> 00:13:52.930
And finally, we take these queues and we
actually manage the dequeuing of

00:13:53.010 --> 00:13:57.440
blocks and the execution of them
automatically by the runtime.

00:13:57.440 --> 00:14:01.830
As you saw with the block that
Dave showed where there were no arguments

00:14:01.830 --> 00:14:04.980
and no return result from the block,
it was just a very simple callback.

00:14:05.060 --> 00:14:09.060
That means that all of the blocks
conform to the same signature,

00:14:09.180 --> 00:14:11.560
and we know exactly how to call them.

00:14:11.560 --> 00:14:12.490
They just need to be invoked.

00:14:12.570 --> 00:14:14.020
They don't need to be
passed any arguments.

00:14:14.020 --> 00:14:15.660
They don't have any return results.

00:14:15.800 --> 00:14:18.800
All of that is managed by what
variables were captured in scope

00:14:18.800 --> 00:14:20.440
when the block was declared.

00:14:20.440 --> 00:14:23.980
And so we can automatically dequeue
these things in the background on

00:14:23.980 --> 00:14:26.270
threads that the system manages for you.

00:14:29.060 --> 00:14:34.110
So some interesting characteristics of
this approach become apparent when we

00:14:34.170 --> 00:14:40.220
compare the queues to traditional locks
like spin locks or Pthread mutexes.

00:14:40.220 --> 00:14:43.160
So in real rough terms,
a mutex or a spin lock is most

00:14:43.160 --> 00:14:45.340
efficient when there's no contention.

00:14:45.450 --> 00:14:47.010
In other words,
if there's only one thread

00:14:47.010 --> 00:14:50.250
running in your application
and it acquires the lock,

00:14:50.440 --> 00:14:52.840
then it can start executing
the code right away.

00:14:52.870 --> 00:14:56.100
Any other threads that try to
acquire the lock at the same time

00:14:56.240 --> 00:14:59.110
are going to be stalled and they
aren't going to be able to make any

00:14:59.110 --> 00:15:01.570
progress until that lock is released.

00:15:01.590 --> 00:15:05.760
So really the efficiency in terms
of throughput of your multithreaded

00:15:05.760 --> 00:15:09.700
code is going to decrease linearly
about with the number of threads you

00:15:09.790 --> 00:15:13.260
have and the amount of contention.

00:15:13.260 --> 00:15:17.360
On the other hand, with queues,
because we can do a very quick atomic

00:15:17.360 --> 00:15:21.200
operation to enqueue a block on a
queue and then move on in your code,

00:15:21.280 --> 00:15:24.160
we actually have a much different curve.

00:15:24.160 --> 00:15:27.540
And that is that peak efficiency
in terms of throughput is actually

00:15:27.540 --> 00:15:30.440
achieved when the system is the busiest.

00:15:30.440 --> 00:15:33.120
The more queues you have,
the more blocks you're

00:15:33.120 --> 00:15:36.170
submitting to those queues,
the faster they return,

00:15:36.170 --> 00:15:38.880
the more throughput you're going to get.

00:15:39.750 --> 00:15:44.390
And this is a really interesting
thing to consider when you're

00:15:44.390 --> 00:15:48.710
designing applications that run
across a very wide range of hardware.

00:15:48.890 --> 00:15:54.230
So you might be designing iOS apps
that run on iPhones and iPod Touches,

00:15:54.230 --> 00:15:56.900
and those only have a single core.

00:15:57.010 --> 00:15:59.080
and David Koehn.

00:15:59.080 --> 00:16:01.420
The characteristics were pretty good,
but if we take that same

00:16:01.420 --> 00:16:03.890
code and run it on an iPad 2,
which has two cores,

00:16:03.890 --> 00:16:06.810
you're going to start seeing
some benefits from the

00:16:06.810 --> 00:16:09.000
approach of using queues.

00:16:09.000 --> 00:16:10.900
And of course,
all of these interfaces are

00:16:10.900 --> 00:16:13.460
available on the Mac as well,
and so you might have many

00:16:13.460 --> 00:16:16.030
cores on your Macintosh,
and the same coding style,

00:16:16.200 --> 00:16:19.890
the same approach,
literally the same binary can run

00:16:19.890 --> 00:16:23.760
from one core to many cores very well.

00:16:23.760 --> 00:16:29.040
When the system automatically dequeues,
what it does is it's guaranteeing

00:16:29.040 --> 00:16:31.980
that strict FIFO ordering of
the blocks being dequeued,

00:16:32.110 --> 00:16:34.930
but if there are multiple
queues that are independent,

00:16:35.000 --> 00:16:39.000
that actually presents the system
with an opportunity for concurrency.

00:16:39.000 --> 00:16:41.480
So in this example,
we have a couple of queues.

00:16:41.480 --> 00:16:42.480
One has three blocks.

00:16:42.480 --> 00:16:43.900
The other has two blocks.

00:16:43.900 --> 00:16:46.660
The system might begin
dequeuing some of these blocks

00:16:46.660 --> 00:16:48.370
and executing them on one CPU.

00:16:48.400 --> 00:16:51.770
If another CPU becomes
available to execute the blocks,

00:16:51.770 --> 00:16:54.880
then that second queue can
start and you can see there's

00:16:54.880 --> 00:16:56.490
some overlap in the timeline.

00:16:56.540 --> 00:17:00.150
That's concurrency that was
available to your application.

00:17:02.370 --> 00:17:04.340
So in order to use Dispatch,
we're going to talk about

00:17:04.540 --> 00:17:06.440
four fundamental concepts.

00:17:06.480 --> 00:17:11.880
The queue, which is a data structure,
and then there are three functions, sync,

00:17:11.900 --> 00:17:13.120
apply, and async.

00:17:13.120 --> 00:17:15.670
And we'll go into each
of these in detail.

00:17:16.990 --> 00:17:21.550
So starting off with queues,
they're a pretty standard data structure.

00:17:21.640 --> 00:17:24.060
You're probably familiar with
the semantics from a lot of

00:17:24.060 --> 00:17:25.520
other APIs on the system.

00:17:25.520 --> 00:17:29.860
When you call dispatch queue create,
it returns to you a new queue object.

00:17:29.930 --> 00:17:32.000
These objects are retained and released.

00:17:32.000 --> 00:17:34.040
You should retain and release in pairs.

00:17:34.040 --> 00:17:37.300
The last release balances
out with the original create,

00:17:37.300 --> 00:17:39.980
and that actually deallocates the object.

00:17:39.980 --> 00:17:43.560
You can see in the create
function here that there's a name.

00:17:43.560 --> 00:17:46.740
We recommend giving a
reverse DNS style name.

00:17:47.220 --> 00:17:50.660
These names actually show up
in things like crash reports,

00:17:50.660 --> 00:17:53.400
so it can be pretty useful
as a diagnostic tool to give

00:17:53.400 --> 00:17:55.430
descriptive names to your queues.

00:17:55.430 --> 00:17:57.740
So you can see what was executing
at the time something went

00:17:57.750 --> 00:17:59.000
wrong in your application.

00:17:59.000 --> 00:18:02.460
The second parameter,
which is null in this example,

00:18:02.460 --> 00:18:06.040
has advanced attributes that
you can assign to queues.

00:18:06.040 --> 00:18:08.730
We won't talk about any of those today.

00:18:11.590 --> 00:18:13.480
So moving on to Dispatch Sync.

00:18:13.480 --> 00:18:20.520
Dispatch Sync is probably
the simplest API call in GCD.

00:18:20.770 --> 00:18:24.470
And basically all it does
is it takes two arguments,

00:18:24.640 --> 00:18:28.770
the first being a queue and
the second being a block.

00:18:28.770 --> 00:18:33.740
And DispatchSync synchronously enqueues
the block onto the queue and that's

00:18:33.740 --> 00:18:37.700
done in an atomic nature so it's safe
to call DispatchSync on the same queue

00:18:37.700 --> 00:18:39.460
from multiple threads at the same time.

00:18:40.280 --> 00:18:44.200
The blocks will be serialized,
they'll be executed by the

00:18:44.200 --> 00:18:48.040
system and then after the
block has finished executing,

00:18:48.040 --> 00:18:52.500
DispatchSync will return to the
call site in your application.

00:18:52.940 --> 00:18:55.990
So this is a very useful thing for
implementing critical sections.

00:18:55.990 --> 00:18:59.580
Maybe you have a lot of threads,
you're implementing some

00:18:59.620 --> 00:19:01.920
shared data structure,
you can use a queue to

00:19:01.920 --> 00:19:06.240
protect that data structure,
you can use Dispatch Sync to serialize

00:19:06.450 --> 00:19:08.580
access to that data structure.

00:19:08.580 --> 00:19:12.090
And so in this code example,
we're doing something, you know,

00:19:12.090 --> 00:19:16.010
classic textbook example of having
some sort of account balance that's

00:19:16.010 --> 00:19:19.260
updated with the transaction,
and we want to make sure that

00:19:19.260 --> 00:19:21.500
there's atomicity in this update.

00:19:22.420 --> 00:19:24.280
So pretty straightforward stuff.

00:19:24.310 --> 00:19:27.760
A slightly more advanced pattern
building on Dispatch Sync is using

00:19:27.760 --> 00:19:32.280
the under under block keyword to
extract results back out of the

00:19:32.280 --> 00:19:35.100
block that's synchronously executed.

00:19:35.310 --> 00:19:37.400
So we've expanded this
example a little bit.

00:19:37.590 --> 00:19:38.760
There's a return value.

00:19:38.760 --> 00:19:41.960
We're just going to have a Boolean
that says whether or not the update

00:19:41.960 --> 00:19:45.660
happened successfully because maybe
we want to check to make sure the

00:19:45.660 --> 00:19:49.440
balance in the account is actually
sufficient that we can adjust it

00:19:49.640 --> 00:19:51.680
for the amount of the transaction.

00:19:51.680 --> 00:19:55.710
Now, one of the key things that differs
here between Dispatch Sync and a

00:19:55.710 --> 00:19:59.750
traditional locking approach is
highlighted by the return statement

00:19:59.750 --> 00:20:02.400
that's about halfway through the code.

00:20:03.280 --> 00:20:07.510
If you've ever written a very
complicated function with a lot

00:20:07.510 --> 00:20:12.260
of locking that checks various
error conditions or checks input,

00:20:12.260 --> 00:20:14.830
you know you have to be very
careful to unlock any of the

00:20:14.830 --> 00:20:18.300
locks that you've acquired before
returning from your function.

00:20:18.300 --> 00:20:21.380
If you ever leave a lock locked,
then a lot of times somewhere

00:20:21.380 --> 00:20:24.160
much later in your code,
something deadlocks against

00:20:24.160 --> 00:20:27.380
that because the logic was
left in an inconsistent state.

00:20:27.410 --> 00:20:30.950
One of the really powerful things about
blocks in Dispatch Sync is that the

00:20:30.950 --> 00:20:33.220
runtime manages all the serialization.

00:20:33.280 --> 00:20:35.280
And it does that on block boundaries.

00:20:35.280 --> 00:20:38.640
It's safe for you to return from
your block at any point because it's

00:20:38.640 --> 00:20:42.250
the returning from the block that
signals that that block is finished

00:20:42.280 --> 00:20:43.280
and the next block can execute.

00:20:43.280 --> 00:20:47.120
So it really lets you do some
concise things for checking for

00:20:47.120 --> 00:20:49.720
error conditions and whatnot,
simply returning from

00:20:49.720 --> 00:20:50.280
the block in the middle.

00:20:50.280 --> 00:20:53.130
You don't have to worry about
unlocking a lot of locks before

00:20:53.140 --> 00:20:55.230
continuing execution of your program.

00:20:55.280 --> 00:21:00.750
And then, of course, in this example,
assuming the transaction was

00:21:00.750 --> 00:21:03.070
able to return from the block,
you can do that.

00:21:03.340 --> 00:21:05.590
to be interpreted,
we didn't return early,

00:21:05.590 --> 00:21:09.120
then we just assigned true to the result,
and that'll be visible by the caller

00:21:09.120 --> 00:21:12.190
when we return from DispatchSync,
and that can be returned

00:21:12.590 --> 00:21:14.750
from the outside function.

00:21:16.490 --> 00:21:19.500
So there's one thing you probably
should be aware of with Dispatch Sync,

00:21:19.500 --> 00:21:21.300
and that is it does what it says.

00:21:21.300 --> 00:21:23.480
It waits until the block has finished.

00:21:23.480 --> 00:21:27.150
So if you were to do something like
this and call Dispatch Sync on the

00:21:27.150 --> 00:21:31.440
same queue from within a block that's
also Dispatch Synced to that queue,

00:21:31.440 --> 00:21:33.040
you're going to deadlock.

00:21:33.100 --> 00:21:36.680
And the reason for that is the
outer block is still executing.

00:21:36.680 --> 00:21:38.100
It hasn't returned.

00:21:38.100 --> 00:21:41.550
The runtime enforces very strict
FIFO serialized nature on these queues,

00:21:41.550 --> 00:21:42.980
like I mentioned earlier.

00:21:43.200 --> 00:21:46.710
So that inner block is really never going
to get an opportunity to execute because

00:21:46.850 --> 00:21:49.450
it's still waiting for the outer block,
which of course is waiting

00:21:49.450 --> 00:21:50.260
for the inner block.

00:21:50.320 --> 00:21:52.800
Now,
this example is pretty straightforward.

00:21:52.800 --> 00:21:54.780
If you saw this in code,
you'd probably say, well, yeah,

00:21:54.820 --> 00:21:56.120
of course that's going to happen.

00:21:56.120 --> 00:21:58.780
Though if there's a lot
of layers of indirection,

00:21:58.840 --> 00:22:02.540
then sometimes it's less obvious
that this might be occurring.

00:22:02.540 --> 00:22:06.940
So the takeaway point here is if you're
debugging and you see things that are

00:22:06.970 --> 00:22:11.130
kind of hanging up and you look at your
backtrace and there are multiple calls

00:22:11.240 --> 00:22:14.660
to Dispatch Sync in that backtrace,
you probably want to go and look and see,

00:22:14.660 --> 00:22:16.340
oh, are these in fact the same queue?

00:22:16.340 --> 00:22:19.540
Might this be a pretty classic
deadlock pattern there?

00:22:22.230 --> 00:22:23.660
So that's Dispatch Sync.

00:22:23.800 --> 00:22:27.480
Now we'll move on to Dispatch Apply,
which really takes the exact same

00:22:27.480 --> 00:22:31.720
approach that Dispatch Sync does,
but provides concurrency

00:22:31.770 --> 00:22:33.490
to your application.

00:22:33.790 --> 00:22:37.230
And this is very useful
for data level parallelism.

00:22:37.420 --> 00:22:42.360
The runtime is able to scale the
blocks that run concurrently to the

00:22:42.360 --> 00:22:43.580
number of cores that are available.

00:22:43.580 --> 00:22:47.900
And it really takes a pattern
very similar to a traditional for

00:22:47.900 --> 00:22:50.340
loop that you're used to using.

00:22:50.340 --> 00:22:52.160
You give it an overall count.

00:22:52.360 --> 00:22:55.940
It's going to provide an
index parameter to the block.

00:22:56.020 --> 00:22:58.490
It's going to be incrementing
that index as it runs.

00:22:58.490 --> 00:23:01.020
You can use that index
to index into an array,

00:23:01.080 --> 00:23:02.970
something along those lines.

00:23:03.700 --> 00:23:09.160
Now, Dispatch Apply is also beneficial
in that it has knowledge of

00:23:09.160 --> 00:23:11.780
what various different parts
of your application are doing.

00:23:11.780 --> 00:23:14.770
So you might imagine calling
into one framework that

00:23:14.770 --> 00:23:16.980
has made the decision that,
oh, well,

00:23:16.980 --> 00:23:18.500
we have eight cores on this machine.

00:23:18.500 --> 00:23:22.690
We're going to divide this image
up into pieces and execute eight

00:23:22.690 --> 00:23:25.910
threads to process that image.

00:23:26.320 --> 00:23:28.460
And meanwhile,
you might be playing a sound or something

00:23:28.460 --> 00:23:31.380
and the different framework has said,
oh, we have eight cores on the machine.

00:23:31.380 --> 00:23:33.540
We can split the sound up and process it.

00:23:33.620 --> 00:23:33.620
And this is a very useful
tool for data analysis.

00:23:33.620 --> 00:23:34.820
With eight concurrent threads.

00:23:34.820 --> 00:23:36.500
And pretty soon you're
running 16 threads,

00:23:36.500 --> 00:23:39.620
but you only have eight cores and
you aren't really gaining anything.

00:23:39.620 --> 00:23:44.070
Dispatch Apply is able to balance
the competing demands from different

00:23:44.070 --> 00:23:47.910
areas of your application because
it actually has visibility into what

00:23:47.920 --> 00:23:50.430
these different subsystems are doing.

00:23:50.570 --> 00:23:54.340
The subsystems themselves can
partition things in a logical manner

00:23:54.340 --> 00:23:57.540
and not really have to worry about
how busy the system is at that moment.

00:23:57.540 --> 00:24:01.880
So as you see in this simple example,
we're just using the index

00:24:01.880 --> 00:24:03.540
to index into an input array.

00:24:03.540 --> 00:24:05.810
We perform some computation.

00:24:05.810 --> 00:24:07.460
We write into some output array.

00:24:07.460 --> 00:24:09.460
And we're done.

00:24:10.690 --> 00:24:16.380
And what this looks like in practice is
we might have a block and we have a CPU.

00:24:16.720 --> 00:24:18.840
We start executing some
blocks on that CPU.

00:24:18.840 --> 00:24:20.270
Another one becomes available.

00:24:20.400 --> 00:24:23.960
The system just automatically fans
out the blocks to the multiple CPUs.

00:24:23.980 --> 00:24:31.010
That can grow and contract over time
depending on what the system looks like.

00:24:32.930 --> 00:24:36.590
So one thing that we get a
lot of questions about is why

00:24:36.590 --> 00:24:40.540
isn't Dispatch Apply working
as fast as I think it should?

00:24:40.540 --> 00:24:43.300
You know, I have an eight-core machine,
I did a Dispatch Apply,

00:24:43.300 --> 00:24:47.370
and I'm maybe seeing a one-and-a-half
times speedup instead of close

00:24:47.420 --> 00:24:48.640
to an eight-times speedup.

00:24:48.720 --> 00:24:50.590
Well, there's a few reasons for that.

00:24:50.740 --> 00:24:53.220
One of the most common
is actually hidden locks.

00:24:53.220 --> 00:24:56.640
You might have a fairly complicated
body of code in this block that

00:24:56.640 --> 00:25:00.470
you're using with Dispatch Apply,
and on the other side of some

00:25:00.470 --> 00:25:04.300
function call that you make,
perhaps there's a lock.

00:25:04.300 --> 00:25:06.580
In this example,
we're just calling printf

00:25:06.630 --> 00:25:08.190
and printing out the index.

00:25:08.290 --> 00:25:11.350
And if you were to try this example,
you would see that it actually

00:25:11.350 --> 00:25:15.790
executes at about one-time speed,
even on a multi-core machine.

00:25:15.800 --> 00:25:20.250
Well, the reason for that is because
the standard C library actually

00:25:20.250 --> 00:25:24.010
has a lock inside printf,
and so it doesn't really matter how many

00:25:24.010 --> 00:25:26.180
threads are running these apply blocks.

00:25:26.180 --> 00:25:30.530
They're all going to be executing
one after another because it's

00:25:30.580 --> 00:25:32.620
all contending for that lock.

00:25:32.780 --> 00:25:34.620
outside printf.

00:25:34.810 --> 00:25:37.200
So when you're profiling your code,
this is definitely

00:25:37.200 --> 00:25:38.080
something to be aware of.

00:25:38.080 --> 00:25:40.880
Look for those bottlenecks if you
aren't seeing the performance you

00:25:40.880 --> 00:25:42.200
would expect from Dispatch Apply.

00:25:45.360 --> 00:25:50.630
Another somewhat related issue is
perhaps the block you're providing

00:25:50.630 --> 00:25:54.740
to Dispatch Apply is too small,
and so the cost of the overhead

00:25:54.740 --> 00:25:58.730
of actually bringing up these
threads and fanning out the work

00:25:58.730 --> 00:26:02.370
is dominating the execution,
and so you don't really see much

00:26:02.380 --> 00:26:04.380
benefit from doing a concurrent apply.

00:26:04.860 --> 00:26:08.720
Or, another possibility is perhaps
you're accessing memory that's

00:26:08.790 --> 00:26:12.400
too close together and you're
constantly invalidating the cache

00:26:12.400 --> 00:26:16.080
line between cores because they're
both contending at a hardware

00:26:16.080 --> 00:26:18.410
level for the same memory accesses.

00:26:18.680 --> 00:26:20.560
Well,
striding is an approach that you can

00:26:20.560 --> 00:26:22.280
use to mitigate both of these issues.

00:26:22.280 --> 00:26:26.380
Basically,
the concept of striding is take

00:26:26.400 --> 00:26:31.260
the input and divide it up into
chunks and then perform the apply

00:26:31.260 --> 00:26:34.860
block on just each individual chunk
instead of each individual element.

00:26:35.640 --> 00:26:38.750
Sometimes it's easy to divide
things into chunks logically,

00:26:38.840 --> 00:26:41.370
like you might have an
image and you might decide,

00:26:41.370 --> 00:26:43.860
okay, well,
we're actually going to apply over

00:26:43.860 --> 00:26:46.990
all of the rows of the image and
then iterate linearly through each

00:26:47.220 --> 00:26:49.330
of the columns within those rows.

00:26:49.340 --> 00:26:52.860
Sometimes it's a bit more arbitrary
and it really is just chunking

00:26:52.860 --> 00:26:54.530
an array in a specific size.

00:26:54.540 --> 00:26:58.950
There's not really a good rule of
thumb for what the right sizes are,

00:26:58.950 --> 00:27:02.830
but typically it's pretty easy
to come up with a pattern like

00:27:02.830 --> 00:27:08.230
the one on the slide where,
you know, you're jumping to a major index

00:27:08.230 --> 00:27:11.820
in an array and then iterating
over a subindex and you can use

00:27:12.080 --> 00:27:15.280
performance tools to tune your code,
look for something that looks

00:27:15.280 --> 00:27:20.280
fairly optimal as kind of a stride,
you know, a chunk size.

00:27:20.280 --> 00:27:23.640
And one of the things you can also
do is make sure that the chunk

00:27:23.640 --> 00:27:27.520
sizes are big enough that if you are
dealing with an array that you would

00:27:27.690 --> 00:27:33.060
actually be operating on cache line
boundaries so that you don't give,

00:27:33.090 --> 00:27:34.560
you know,
something that's within the same

00:27:34.560 --> 00:27:35.620
cache line to the same array.

00:27:35.640 --> 00:27:38.450
So you can't have the same
block on multiple CPUs,

00:27:38.460 --> 00:27:41.710
but they can operate on disjoint
sections of data that will

00:27:41.760 --> 00:27:43.630
keep the contention lower.

00:27:43.650 --> 00:27:47.600
Cache line size is something
that you can look up.

00:27:47.600 --> 00:27:50.820
There's sysctl calls and whatnot
for obtaining the cache line size

00:27:50.820 --> 00:27:52.600
value for the current machine.

00:27:52.600 --> 00:27:55.410
So it's really just
an iterative approach,

00:27:55.410 --> 00:27:58.680
measure and tune,
but this can make a huge difference

00:27:58.960 --> 00:28:02.590
in the performance that you
get out of dispatch apply.

00:28:04.750 --> 00:28:10.670
So the last major fundamental building
block of GCD is Dispatch Async.

00:28:10.690 --> 00:28:14.040
And Dispatch Async is a bit
unlike the others in that it

00:28:14.220 --> 00:28:16.610
executes blocks asynchronously.

00:28:16.810 --> 00:28:20.530
It doesn't wait for them to
complete before returning.

00:28:20.710 --> 00:28:24.470
So this is also useful for implementing
critical sections because remember

00:28:24.700 --> 00:28:29.380
the queues and the dequeue of the
queues is what maintains the strict

00:28:29.450 --> 00:28:32.840
FIFO ordering and mutual exclusion.

00:28:32.900 --> 00:28:35.590
But in this case,
because we're calling dispatch

00:28:35.590 --> 00:28:38.590
async instead of dispatch sync,
we're saying we're willing

00:28:38.590 --> 00:28:41.820
to just enqueue the block,
fire and forget, return immediately.

00:28:41.820 --> 00:28:45.750
We know the system will execute
that at some later time.

00:28:46.140 --> 00:28:50.000
So sometimes you know you need
to perform some operation,

00:28:50.000 --> 00:28:53.840
you know it needs to be consistent
with respect to whatever data

00:28:53.840 --> 00:28:57.750
is also protected by that queue,
but you're not actually interested

00:28:57.750 --> 00:28:59.420
in the result of that operation.

00:28:59.420 --> 00:29:02.950
And Dispatch Async is a great
tool to use in those cases.

00:29:03.990 --> 00:29:08.160
So here we've expanded on the example
that we were using previously,

00:29:08.160 --> 00:29:12.120
and we can use Dispatch Async to
perform some maintenance task,

00:29:12.120 --> 00:29:16.380
like maybe we calculate some sort of
interest that accumulates in the account.

00:29:16.380 --> 00:29:18.540
We don't really need to
know what that was here,

00:29:18.540 --> 00:29:21.440
so we can just enqueue the block,
fire and forget, move on.

00:29:21.440 --> 00:29:26.090
This is even more powerful when you
apply it to the overall architecture

00:29:26.110 --> 00:29:30.070
of your application and move
work off of the main thread.

00:29:30.080 --> 00:29:33.620
So the main thread on
both Mac OS and iOS,

00:29:33.820 --> 00:29:36.320
is what's responsible
for handling events.

00:29:36.320 --> 00:29:40.890
It is running the main event loop,
events are processed there.

00:29:40.920 --> 00:29:44.710
Any code that's executing for a long
period of time on the main thread

00:29:44.710 --> 00:29:48.700
prevents your application from receiving
and processing additional events.

00:29:48.700 --> 00:29:51.190
And so on an iOS app,
this makes it look like the app's

00:29:51.190 --> 00:29:53.000
not responding to touch events.

00:29:53.000 --> 00:29:57.640
On a Mac OS app,
you actually see the cursor change

00:29:57.640 --> 00:30:00.810
into this spinning beach ball if
it's in the state long enough.

00:30:00.820 --> 00:30:04.130
So anytime you have a large task,
you might want to consider using

00:30:04.130 --> 00:30:09.300
Dispatch Async to defer the execution
of that task off of the main thread.

00:30:09.320 --> 00:30:12.990
And of course,
anytime you defer something on a queue,

00:30:13.140 --> 00:30:17.050
and you have multiple queues,
that gives the system the opportunity

00:30:17.090 --> 00:30:19.300
to execute those blocks concurrently.

00:30:19.300 --> 00:30:23.880
So you actually implicitly get multi-core
benefits on machines that support that.

00:30:23.900 --> 00:30:26.700
So building on this
pattern of Dispatch Async,

00:30:27.020 --> 00:30:33.340
one of the most useful features is
embedding multiple layers of Dispatch

00:30:33.340 --> 00:30:33.780
Async within one single block.

00:30:33.780 --> 00:30:34.070
So you can actually do
that by using the command

00:30:34.070 --> 00:30:34.770
"dispatch_async_dispatch_async" to get
the result back to the main thread.

00:30:35.020 --> 00:30:38.650
And using that to first take a
block and defer its execution

00:30:38.760 --> 00:30:40.260
off of the main thread.

00:30:40.560 --> 00:30:43.120
And then once you have a
result from that block,

00:30:43.130 --> 00:30:45.800
using Dispatch Async to get the
result back to the main thread,

00:30:45.820 --> 00:30:50.250
so that you can actually use that data
to perform some sort of UI update.

00:30:50.550 --> 00:30:54.930
So here we have an example where, again,
we probably want to execute

00:30:55.160 --> 00:30:58.680
with consistency of the
account information we had,

00:30:58.950 --> 00:31:01.460
but we're going to call some function
that renders an account statement.

00:31:01.460 --> 00:31:03.760
Maybe it returns an image that
can be drawn to the screen.

00:31:03.760 --> 00:31:05.760
That might take a while.

00:31:05.760 --> 00:31:07.760
We don't want to do
that on the main thread.

00:31:07.760 --> 00:31:09.720
But once we have the
information available,

00:31:09.810 --> 00:31:11.740
we can do a subsequent Dispatch Async.

00:31:11.760 --> 00:31:14.760
We can call a special function
called Dispatch Get Main Queue,

00:31:14.910 --> 00:31:17.760
and that's tied in with the main
event loop of your application.

00:31:17.760 --> 00:31:22.230
We'll make sure that blocks submitted to
the main Dispatch Queue get executed as

00:31:22.230 --> 00:31:24.760
you would expect on the main event loop.

00:31:24.760 --> 00:31:29.090
And then you can perform the draw
operation of the image on the

00:31:29.350 --> 00:31:32.670
main thread where it belongs,
but you didn't have to wait

00:31:32.670 --> 00:31:35.740
for a long time in order to
gather the raw data before that.

00:31:35.900 --> 00:31:40.240
So if you're going to be doing this
type of nested Dispatch Call Something,

00:31:40.250 --> 00:31:42.630
Have It Call Back,
a lot of times it makes sense to

00:31:42.630 --> 00:31:47.720
actually define a function that
encapsulates that pattern in the API.

00:31:47.740 --> 00:31:51.740
And so what we really recommend,
as you see in the top line of code here,

00:31:51.740 --> 00:31:55.210
is providing the queue and
the callback block as the last

00:31:55.210 --> 00:31:56.740
two arguments of the function.

00:31:56.740 --> 00:32:01.740
The queue provides the execution context
for where that block should be invoked,

00:32:01.740 --> 00:32:03.720
and the callback block is a
function that's going to be

00:32:03.720 --> 00:32:03.720
used to call back the function.

00:32:03.720 --> 00:32:05.710
And then the callback block is,
of course,

00:32:05.740 --> 00:32:08.720
whatever you're expecting to run after
the asynchronous operation has completed.

00:32:10.090 --> 00:32:11.900
So those are the last two parameters.

00:32:12.020 --> 00:32:15.600
The reason for putting the
block at the end is that,

00:32:15.620 --> 00:32:18.950
as you saw in the earlier examples,
it's very convenient to define a

00:32:18.950 --> 00:32:21.340
block in line with the function call.

00:32:21.410 --> 00:32:24.040
Keeping it at the end is
a syntactic convenience.

00:32:24.040 --> 00:32:26.740
It means that you don't have to go
looking for extra parameters that

00:32:26.740 --> 00:32:32.270
might be dangling past a very large
block that's there in the declaration.

00:32:32.740 --> 00:32:38.140
So we get a queue, we get a block,
we call this new asynchronous

00:32:38.140 --> 00:32:41.700
function we've defined,
and we pass in the queue.

00:32:41.700 --> 00:32:44.530
In this case, like the previous example,
we want it to be executed

00:32:44.540 --> 00:32:45.510
on the main queue.

00:32:45.760 --> 00:32:50.390
And then now we just define a simple
image draw block that executes there.

00:32:52.310 --> 00:32:56.700
So one thing you do need to be
aware of is that dispatch queues

00:32:56.700 --> 00:33:01.180
must be retained when you're
using this nested block approach.

00:33:01.180 --> 00:33:05.020
So this example is really what
the implementation of that

00:33:05.020 --> 00:33:08.200
asynchronous function we defined on
the previous slide might look like.

00:33:08.400 --> 00:33:12.260
And because we're being passed a
queue that we don't have ownership of,

00:33:12.260 --> 00:33:16.540
and because all we're really doing in
this function is calling dispatch async,

00:33:16.590 --> 00:33:20.970
which returns immediately,
the caller is free to release that

00:33:20.970 --> 00:33:25.370
queue as soon as it's passed it to us,
which means that queue might be

00:33:25.370 --> 00:33:28.370
invalid by the time the block
actually gets around to running.

00:33:28.380 --> 00:33:31.080
Because remember,
dispatch async is fire and forget.

00:33:31.220 --> 00:33:32.460
It's deferred execution.

00:33:32.460 --> 00:33:35.080
Some other thread will run
that at some later time.

00:33:35.080 --> 00:33:38.240
So we need to make sure the
lifetime of the queue is extended.

00:33:38.400 --> 00:33:43.080
Such that we know it's valid so that
it can receive the last callback block.

00:33:43.140 --> 00:33:48.260
So here we have a queue input parameter.

00:33:48.260 --> 00:33:53.120
Before we do the dispatch async,
we need to do a dispatch retain.

00:33:53.120 --> 00:33:56.910
And the dispatch retain means
that we're asserting some degree

00:33:56.910 --> 00:33:58.800
of ownership of this queue.

00:33:58.800 --> 00:34:05.150
Then inside our background task block,
we can deliver the callback block

00:34:05.150 --> 00:34:08.290
to that queue via dispatch async.

00:34:08.400 --> 00:34:11.380
And once we've done that,
we're done with the queue.

00:34:11.440 --> 00:34:14.010
The system will take it from
there so we can actually release

00:34:14.010 --> 00:34:15.350
our ownership of the queue.

00:34:15.380 --> 00:34:20.330
So this retain release pattern,
where you retain things before

00:34:20.640 --> 00:34:23.610
an async block and then release
them in the async block,

00:34:23.620 --> 00:34:26.060
is necessary when you have a
couple layers of indirection

00:34:26.060 --> 00:34:30.920
like this and you're working with
types that are basic C types,

00:34:30.960 --> 00:34:34.060
you know, C malloced objects,
you need malloc and free,

00:34:34.060 --> 00:34:36.420
duplicate strings,
retain or release queues,

00:34:36.660 --> 00:34:37.400
that type of thing.

00:34:38.520 --> 00:34:41.090
And so to talk a little bit
more about some of these memory

00:34:41.090 --> 00:34:44.560
management rules surrounding
blocks and asynchronous execution,

00:34:44.560 --> 00:34:46.320
I'd like to invite
Dave back up to the stage.

00:34:46.320 --> 00:34:47.060
All right.

00:34:47.130 --> 00:34:47.560
Thanks, Kevin.

00:34:47.560 --> 00:34:50.680
So now we're going to
really start to talk about,

00:34:50.680 --> 00:34:55.340
we're going to talk about the
practical details of blocks and async.

00:34:55.360 --> 00:34:57.180
This is where the memory
management kicks in.

00:34:57.180 --> 00:35:01.000
This is where a lot of your kind of
daily details about how do I make

00:35:01.000 --> 00:35:03.760
everything work right and work reliably.

00:35:03.760 --> 00:35:08.360
So building on what Kevin talked about,
let's take dispatch async.

00:35:08.400 --> 00:35:09.500
And implement it.

00:35:09.580 --> 00:35:11.150
We're going to implement it right here,
right now.

00:35:11.160 --> 00:35:12.780
Well, we have that block parameter.

00:35:12.780 --> 00:35:14.230
What are we going to do?

00:35:14.240 --> 00:35:17.920
Well, in fact,
GCD provides an alternative

00:35:17.920 --> 00:35:20.540
function-based interface.

00:35:20.540 --> 00:35:23.390
They're the same names with
an underbar F at the end.

00:35:23.420 --> 00:35:28.810
It takes a context parameter and a
classic C style and it takes a function.

00:35:28.820 --> 00:35:31.150
And that function takes
that context parameter.

00:35:31.160 --> 00:35:32.700
All right.

00:35:32.700 --> 00:35:35.000
Well, here's the body of dispatch async.

00:35:35.020 --> 00:35:38.380
It just calls the underbar
F version of the same.

00:35:38.400 --> 00:35:39.900
It passes the same API.

00:35:39.920 --> 00:35:41.400
Passes the same queue.

00:35:41.400 --> 00:35:43.370
And then it does two interesting things.

00:35:43.410 --> 00:35:46.320
It calls block copy,
which is a block API.

00:35:46.400 --> 00:35:49.400
And what that does is that
copies the block to the heap.

00:35:49.460 --> 00:35:51.360
And we'll see why in a second.

00:35:51.400 --> 00:35:54.400
And then it calls this
little static helper routine.

00:35:54.460 --> 00:35:55.390
What is that?

00:35:55.400 --> 00:35:57.220
Well, that's just this.

00:35:57.410 --> 00:36:00.900
It takes that context parameter,
casts it back to a block,

00:36:01.180 --> 00:36:03.400
calls the block, and then releases it.

00:36:03.400 --> 00:36:04.250
That's it.

00:36:04.600 --> 00:36:07.390
That's how you -- and the important
thing to take away with this

00:36:07.390 --> 00:36:07.400
is that it's not just a block.

00:36:07.400 --> 00:36:07.900
It's a block.

00:36:07.900 --> 00:36:08.360
It's a block.

00:36:08.410 --> 00:36:13.120
that any async API should
block copy and block release.

00:36:13.130 --> 00:36:16.990
This ensures that a lot of automatic
memory management kicks in and it saves

00:36:16.990 --> 00:36:21.030
you a lot of boilerplate in your code of
taking that data structure that we talked

00:36:21.030 --> 00:36:23.760
about at the beginning of the talk,
copying it to the heap,

00:36:23.910 --> 00:36:25.700
figuring out when to free it.

00:36:25.720 --> 00:36:29.400
This is what blocks really is all about.

00:36:30.290 --> 00:36:33.680
So the really important takeaway, too,
is let's say there's an API in

00:36:33.680 --> 00:36:35.940
the system or an API in your code.

00:36:35.960 --> 00:36:36.990
Well, guess what?

00:36:37.050 --> 00:36:38.600
You can add a block wrapper to it.

00:36:38.680 --> 00:36:39.990
It's that simple.

00:36:40.100 --> 00:36:41.840
And in fact, you don't even need to
wait for Apple to do it.

00:36:41.840 --> 00:36:45.840
You can just write your own little
block wrapper around any function

00:36:45.840 --> 00:36:47.980
and context pointer-based API.

00:36:48.810 --> 00:36:50.050
So what is this block copy?

00:36:50.180 --> 00:36:51.900
What does block copy do?

00:36:51.900 --> 00:36:53.210
It does a lot of great things.

00:36:53.340 --> 00:36:58.940
It automatically copies values, integers,
floats, pointers, et cetera,

00:36:59.090 --> 00:37:01.980
and that under under block
variable forces sharing.

00:37:01.980 --> 00:37:07.890
This is what allows the caller
and the block to share data.

00:37:08.490 --> 00:37:11.580
But otherwise, that copy,
that automatic copy of values

00:37:11.590 --> 00:37:14.680
is what allows things like
Dispatch Async to just work.

00:37:14.740 --> 00:37:18.250
Any values that are captured
are automatically brought

00:37:18.250 --> 00:37:20.110
to the heap with the block.

00:37:21.140 --> 00:37:25.800
Block copy also automatically
copies and releases other blocks.

00:37:25.800 --> 00:37:29.800
So if you just happen to use a block
parameter within another block,

00:37:29.800 --> 00:37:32.550
you don't need to worry
about memory managing the

00:37:32.550 --> 00:37:34.490
relationship between the two.

00:37:34.870 --> 00:37:37.780
and David Koeppel.

00:37:37.780 --> 00:37:42.020
It automatically releases
Objective C objects.

00:37:42.020 --> 00:37:42.910
If you may have noticed this week,
this is a big theme of the conference,

00:37:43.330 --> 00:37:46.800
where we're automatically retaining
and releasing Objective C objects.

00:37:46.800 --> 00:37:51.500
And blocks are one example of how this
pattern has been increasingly apparent

00:37:51.500 --> 00:37:53.790
and obvious in hindsight at Apple.

00:37:53.790 --> 00:37:58.600
We need to automatically retain
and release objects because

00:37:58.600 --> 00:37:58.600
it's just the right thing to do.

00:37:59.240 --> 00:38:01.400
and David Koeppel.

00:38:01.400 --> 00:38:05.560
Block copy also automatically calls
C++ constructors and destructors.

00:38:05.560 --> 00:38:08.450
Having said that,
strongly recommend that you

00:38:08.450 --> 00:38:13.230
use our latest compiler,
the Apple LLVM 3.0 compiler to get

00:38:13.230 --> 00:38:16.950
the best C++ and block relationship.

00:38:18.560 --> 00:38:21.780
All right, let's talk about what
block copy doesn't do.

00:38:21.820 --> 00:38:26.050
And this is really why
certainly some of you are here.

00:38:26.780 --> 00:38:29.280
Block copy doesn't read your mind.

00:38:29.390 --> 00:38:31.380
What do I mean by that?

00:38:31.430 --> 00:38:33.560
Let's say you're going
to use dispatch async,

00:38:33.560 --> 00:38:36.730
and you're using it from within one
of the methods in one of your classes.

00:38:37.030 --> 00:38:39.420
And you pass an instance
variable in that block,

00:38:39.420 --> 00:38:41.220
and you tell it to do something.

00:38:41.220 --> 00:38:43.910
Well, what's actually happening here?

00:38:43.940 --> 00:38:48.090
Well, implicitly, this is the way the
compiler thinks about it,

00:38:48.110 --> 00:38:51.710
is that's actually taking the
self pointer and dereferencing

00:38:51.710 --> 00:38:53.480
it and getting at the Ivar.

00:38:53.500 --> 00:38:56.290
And therefore,
from the compiler's perspective,

00:38:56.290 --> 00:38:59.490
self is actually the variable
that's captured in the block,

00:38:59.560 --> 00:39:01.780
and self is automatically retained.

00:39:01.780 --> 00:39:03.860
This might surprise you.

00:39:03.900 --> 00:39:06.340
Why is my object living this long?

00:39:06.380 --> 00:39:09.990
Well, it's because the object that
you thought was being captured

00:39:09.990 --> 00:39:11.660
isn't the one you thought.

00:39:11.660 --> 00:39:12.800
There's a mismatch.

00:39:13.940 --> 00:39:14.780
So what can you do?

00:39:14.780 --> 00:39:16.710
Real simple workaround.

00:39:16.710 --> 00:39:17.590
Just be explicit.

00:39:17.600 --> 00:39:21.210
Create a temporary variable,
assign the instance variable,

00:39:21.210 --> 00:39:23.310
capture the temporary variable.

00:39:23.340 --> 00:39:24.890
Now you're explicit.

00:39:25.150 --> 00:39:28.520
Only the Ivar is captured,
and only the Ivar lives long

00:39:28.540 --> 00:39:30.700
enough to do the right thing.

00:39:30.700 --> 00:39:33.510
Another side effect of this,
of capturing self,

00:39:33.510 --> 00:39:37.920
is if the Ivar changes between the
dispatch async and when the block starts,

00:39:37.920 --> 00:39:41.140
you may get a different value
for that Ivar being used,

00:39:41.140 --> 00:39:43.000
and that might surprise you.

00:39:43.000 --> 00:39:43.910
So this is another good reason.

00:39:43.920 --> 00:39:46.680
To be really explicit,
say I want this one right here,

00:39:46.680 --> 00:39:49.140
and the same one when
the block actually runs.

00:39:51.240 --> 00:39:55.340
Let's talk about another example
about what block copy doesn't do.

00:39:55.340 --> 00:39:57.190
It doesn't automatically
fix retain cycles.

00:39:57.190 --> 00:39:59.420
Now, luckily,
a lot of our frameworks have

00:39:59.540 --> 00:40:02.360
been designed over the years
to avoid retain cycles,

00:40:02.360 --> 00:40:07.540
but it's still always possible with new
code to introduce new retain cycles.

00:40:07.910 --> 00:40:10.660
Here's a really simple example.

00:40:10.660 --> 00:40:14.560
It's also really obvious in
isolation that what is going on here.

00:40:14.570 --> 00:40:18.070
We're going to take an object,
we're going to set some kind of handler,

00:40:18.260 --> 00:40:19.220
and guess what?

00:40:19.400 --> 00:40:22.170
We want to use that object in the block.

00:40:22.170 --> 00:40:24.250
Well, what's going on here?

00:40:24.250 --> 00:40:28.140
The object is going to
block copy the block,

00:40:28.140 --> 00:40:32.710
and that block is automatically
going to retain the captured objects.

00:40:32.780 --> 00:40:35.370
And here we go, a simple cycle.

00:40:36.800 --> 00:40:39.000
Well, what can we do about this?

00:40:39.110 --> 00:40:43.880
Well, a simple workaround is that
in manual memory management,

00:40:43.880 --> 00:40:47.760
under-under block variables are
left to the programmer to decide

00:40:47.760 --> 00:40:49.970
the policies about how they work.

00:40:49.970 --> 00:40:55.200
And under-under block variables are not
implicitly retained at block copy time.

00:40:55.200 --> 00:41:03.200
The value is, but the object itself is
not implicitly retained.

00:41:03.220 --> 00:41:06.680
So you can take advantage of
this fact to assign the object

00:41:06.780 --> 00:41:11.220
to a temporary block variable to
prevent automatic retaining of it.

00:41:11.400 --> 00:41:12.210
And guess what?

00:41:12.210 --> 00:41:13.820
The object now just retains the block.

00:41:13.900 --> 00:41:18.170
The block just holds its hands off
the under-under block variable,

00:41:18.170 --> 00:41:20.210
and the right thing happens.

00:41:20.730 --> 00:41:23.430
and Kevin's work on
dispatch sync deadlocking.

00:41:23.650 --> 00:41:27.670
As Kevin was illustrating earlier
with dispatch sync deadlocking,

00:41:27.670 --> 00:41:31.200
it may be obvious in isolation,
but with a nontrivial code,

00:41:31.310 --> 00:41:40.670
this pattern can end up in
practice in nonobvious ways.

00:41:40.690 --> 00:41:40.690
Retaining nonobjects.

00:41:40.690 --> 00:41:40.690
This is what Kevin was
talking about with queues.

00:41:40.730 --> 00:41:44.020
: GCD queues are an
example of a non-object,

00:41:44.020 --> 00:41:46.860
and so are CF objects.

00:41:46.860 --> 00:41:47.900
So what do we do?

00:41:48.190 --> 00:41:52.990
Well, we just need to retain, use it,
release it after we're done

00:41:52.990 --> 00:41:54.690
with it asynchronously.

00:41:54.770 --> 00:41:55.880
Pretty simple.

00:41:56.330 --> 00:41:58.360
Here we are with the CF example.

00:41:58.360 --> 00:42:02.270
Make sure to CF retain,
then we maybe do something

00:42:02.440 --> 00:42:07.340
awesome with that foo thing,
and then we can CF release it when

00:42:07.340 --> 00:42:07.510
we're done asynchronously using it.

00:42:08.730 --> 00:42:13.120
Well, what else does block copy not
do that you need to be aware of?

00:42:13.120 --> 00:42:17.480
It's not implicitly called by non-blocks.

00:42:17.480 --> 00:42:19.150
Remember how I said that
blocks can capture blocks

00:42:19.300 --> 00:42:21.370
and the right thing happens?

00:42:21.370 --> 00:42:25.900
Well, let's say you're building an array
on the stack and you're trying to do

00:42:25.930 --> 00:42:29.880
something clever by creating custom
blocks and putting them into an array.

00:42:29.880 --> 00:42:33.270
We actually saw examples of this
when we first started designing

00:42:33.270 --> 00:42:36.850
blocks and people were trying to
experiment with design patterns.

00:42:36.850 --> 00:42:37.970
Well, guess what?

00:42:38.060 --> 00:42:43.340
This block is only valid for the
scope of the enclosing for loop.

00:42:43.410 --> 00:42:48.600
Once that for loop exits,
that block is invalid anymore.

00:42:48.700 --> 00:42:51.020
So that entire array becomes invalid.

00:42:51.050 --> 00:42:55.310
The workaround for this is to use
block copy inside the for loop.

00:42:55.980 --> 00:42:58.460
Similarly,
functions themselves are not blocks.

00:42:58.550 --> 00:43:00.920
They don't automatically
manage the memory of blocks.

00:43:00.920 --> 00:43:04.390
Therefore,
if you're trying to do something clever,

00:43:04.470 --> 00:43:05.390
and actually, it's not

00:43:05.770 --> 00:43:08.760
fairly reasonable and
just return a block,

00:43:08.760 --> 00:43:09.780
that won't work.

00:43:09.780 --> 00:43:13.910
You need to call block copy in
manual memory management mode.

00:43:14.090 --> 00:43:17.180
And then, of course, if you block copy,
your caller needs to be aware

00:43:17.250 --> 00:43:19.500
in this last case that it
needs to block release it.

00:43:19.520 --> 00:43:21.800
So keep that in mind.

00:43:23.180 --> 00:43:24.670
So let's talk about better blocks.

00:43:24.690 --> 00:43:26.600
Let's talk about automatic
reference counting,

00:43:26.600 --> 00:43:29.570
which is one of the big themes
of this week's conference.

00:43:29.570 --> 00:43:35.090
Many of the challenges outlined about
block copy and what it does not do are

00:43:35.090 --> 00:43:36.940
fixed with automatic reference counting.

00:43:36.940 --> 00:43:39.470
I strongly hope you can
check out this feature,

00:43:39.560 --> 00:43:41.840
but we won't be talking about it here.

00:43:43.180 --> 00:43:44.980
Also, some of the things are not fixed.

00:43:45.070 --> 00:43:48.200
You still need to be aware of these
facts that you've learned today.

00:43:48.200 --> 00:43:51.040
Non-objects are not automatically
retained and released.

00:43:51.040 --> 00:43:54.800
You still need to do dispatch retain
and release or CF retain and release.

00:43:54.800 --> 00:43:57.180
See the ARC talks for more info.

00:43:59.350 --> 00:44:02.660
So to conclude,
blocks and Grand Central Dispatch are

00:44:02.670 --> 00:44:07.300
a very important part of the design
patterns we use at Apple for both

00:44:07.300 --> 00:44:13.330
encapsulating enumeration and concurrent
for loops to asynchronous callbacks.

00:44:13.500 --> 00:44:14.640
They're simpler.

00:44:14.640 --> 00:44:15.800
They're safer.

00:44:15.800 --> 00:44:18.560
They help you avoid boilerplate code.

00:44:18.560 --> 00:44:21.360
There are already patterns
that you use today.

00:44:21.360 --> 00:44:24.090
For more information,
I'd like to suggest that you

00:44:24.280 --> 00:44:26.950
contact Michael Jurowicz,
our developer tools and

00:44:26.950 --> 00:44:28.540
performance evangelist.

00:44:29.300 --> 00:44:32.780
Paul Danbold, our Core OS evangelist.

00:44:32.840 --> 00:44:34.210
We have documentation.

00:44:34.250 --> 00:44:36.220
GCD is open source if
you want to check it out.

00:44:36.240 --> 00:44:37.900
Here are the related sessions.

00:44:37.900 --> 00:44:43.250
We have the developer tools kickoff,
the introducing ARC session,

00:44:43.250 --> 00:44:48.100
mastering GCD,
Objective-C advanced on Friday,

00:44:48.100 --> 00:44:51.560
and mastering GCDs tomorrow.

00:44:51.560 --> 00:44:52.660
Thanks for coming.