WEBVTT

00:00:10.310 --> 00:00:11.140
Good morning.

00:00:11.280 --> 00:00:13.740
My name's Abe Stephens,
and I'm a member of the

00:00:13.810 --> 00:00:15.440
OpenCL team at Apple.

00:00:15.440 --> 00:00:19.130
In this session,
I'm going to tell you about what's

00:00:19.130 --> 00:00:22.670
new in the OpenCL framework in Lion.

00:00:23.800 --> 00:00:29.040
I'm going to start the session with
a short overview of what OpenCL is.

00:00:29.460 --> 00:00:32.630
and then talk about a new Lion feature,
which is the integration of

00:00:32.700 --> 00:00:36.110
OpenCL with Grand Central Dispatch.

00:00:36.320 --> 00:00:38.780
After that,
I'll talk about some new options in

00:00:38.780 --> 00:00:43.450
Lion for compiling OpenCL source files,
both using Xcode and using

00:00:43.450 --> 00:00:45.220
a new command line tool.

00:00:45.940 --> 00:00:50.010
So let me dive right in
and tell you about OpenCL.

00:00:50.050 --> 00:00:54.990
OpenCL is a framework that you can
use to take advantage of both CPU and

00:00:54.990 --> 00:00:59.650
GPU parallel processors in your
system to accelerate computationally

00:01:00.220 --> 00:01:02.580
intensive aspects of your application.

00:01:03.200 --> 00:01:07.790
OpenCL was designed to allow
you to use both CPUs and GPUs,

00:01:07.790 --> 00:01:11.400
and it's designed for parts
of your application that are

00:01:11.490 --> 00:01:13.050
computationally intensive.

00:01:13.210 --> 00:01:18.770
But the OpenCL system
consists of a runtime API,

00:01:18.860 --> 00:01:22.890
a compiler and a programming
language that you use to implement

00:01:22.890 --> 00:01:25.110
these pieces of your application.

00:01:25.500 --> 00:01:28.710
OpenCL's programming language
is portable between devices,

00:01:28.790 --> 00:01:32.360
so you can write a single
program and run it on a GPU,

00:01:32.360 --> 00:01:35.280
and then also run that part
of your program on the CPU or

00:01:35.280 --> 00:01:36.710
a different type of GPU.

00:01:36.960 --> 00:01:42.780
Now, OpenCL is able to be this portable
because the programs that you write for

00:01:42.980 --> 00:01:47.860
it use a data parallel programming model,
and we can map that data parallel

00:01:47.860 --> 00:01:51.700
programming model to a variety
of different kinds of processors.

00:01:51.910 --> 00:01:54.700
So data parallel is the
name of the game in OpenCL,

00:01:54.700 --> 00:01:59.440
and I'd like to give a very simple
example of a data parallel problem,

00:01:59.440 --> 00:02:04.340
and then show you how you would
implement this problem in OpenCL.

00:02:04.610 --> 00:02:06.500
So let's take a very simple example.

00:02:06.730 --> 00:02:12.580
We're just going to add the variables a,
b, and c together, or to get c.

00:02:12.670 --> 00:02:15.970
And to make this a parallel problem,
we'll say that a, b,

00:02:15.970 --> 00:02:19.360
and c are arrays of numbers
or vectors of numbers.

00:02:19.430 --> 00:02:25.020
And we'd like to compute the
element-wise sum of a and b and get c.

00:02:25.020 --> 00:02:27.820
So if we wanted to do
this in a serial program,

00:02:27.820 --> 00:02:30.560
say with just one
thread or one processor,

00:02:30.630 --> 00:02:33.150
we could use a for loop,
and our for loop would just iterate

00:02:33.240 --> 00:02:36.880
over the elements of the array.

00:02:37.220 --> 00:02:39.250
So if we take a closer
look at this for loop,

00:02:39.250 --> 00:02:43.490
and this is the for loop that
we're going to try to make

00:02:43.490 --> 00:02:43.490
a data parallel version of,

00:02:44.000 --> 00:05:38.300
[Transcript missing]

00:05:38.720 --> 00:05:41.230
Now,
it's also possible for some problems to

00:05:41.230 --> 00:05:47.560
have instances where the individual work
items aren't completely independent.

00:05:47.560 --> 00:05:49.140
Maybe they need to communicate.

00:05:49.160 --> 00:05:52.300
Maybe instead of computing the
sum of two different arrays,

00:05:52.340 --> 00:05:55.060
we're computing the sum across
all the elements in an array.

00:05:55.060 --> 00:05:59.360
And in order to schedule work
items in such a way that they

00:05:59.740 --> 00:06:04.050
can communicate and share data,
we have to put those work items

00:06:04.610 --> 00:06:06.930
in what's called a work group.

00:06:07.340 --> 00:06:12.030
So the second type of dimension
that a problem has in OpenCL is

00:06:12.030 --> 00:06:16.200
the size of a work group,
which is the local size.

00:06:16.490 --> 00:06:19.530
And of course,
the problems that we solve with

00:06:20.030 --> 00:06:23.560
OpenCL can be one-dimensional,
for example, that addition thing

00:06:23.570 --> 00:06:25.180
that we just looked at,
or two-dimensional.

00:06:25.180 --> 00:06:29.060
Maybe I'm processing an image,
and the image has a width and a height.

00:06:29.060 --> 00:06:32.000
And I can also work with
three-dimensional data,

00:06:32.010 --> 00:06:36.040
and so I can define a global and a
local size that divides my work in one,

00:06:36.330 --> 00:06:37.950
two, or three dimensions.

00:06:40.360 --> 00:06:45.140
Now I'd like to talk a little bit about
the memory model that OpenCL uses.

00:06:45.160 --> 00:06:47.890
And this is essentially where the
information that your kernel is

00:06:47.950 --> 00:06:51.350
going to process on comes from.

00:06:51.400 --> 00:06:55.700
Now, the data starts off in
the host application,

00:06:55.740 --> 00:06:59.460
and the application thread will
allocate data in host memory.

00:06:59.460 --> 00:07:02.800
This is the same memory that your
application is generally running in.

00:07:02.960 --> 00:07:05.570
And then when you send a
command to an OpenCL device,

00:07:05.570 --> 00:07:08.890
or when you send a kernel
to the OpenCL device,

00:07:08.980 --> 00:07:13.060
the runtime will move the memory
objects that you've created into

00:07:13.230 --> 00:07:18.300
global memory on that device,
or the global address space.

00:07:18.320 --> 00:07:22.970
And using the CPU device,
the address spaces are

00:07:22.970 --> 00:07:23.760
actually all the same.

00:07:23.760 --> 00:07:27.650
They're all physically in
main memory on the machine.

00:07:27.860 --> 00:07:33.140
With GPU devices,
or at least with discrete GPU devices,

00:07:33.150 --> 00:07:35.800
the global memory might
actually be a separate piece

00:07:35.800 --> 00:07:38.160
of memory on the discrete GPU.

00:07:38.200 --> 00:07:38.560
Anyway.

00:07:38.660 --> 00:07:42.450
When your kernel is executing,
it can load data into

00:07:42.450 --> 00:07:45.260
another address space,
another piece of memory,

00:07:45.260 --> 00:07:47.760
which we call local memory.

00:07:47.870 --> 00:07:51.270
And local memory is that
communication memory that is

00:07:51.410 --> 00:07:53.440
exclusive to the work group.

00:07:53.490 --> 00:07:57.080
So all of the work items executing
in the same work group can access

00:07:57.550 --> 00:07:59.970
this piece of local memory.

00:08:00.170 --> 00:08:03.950
Now, each work item also has an
exclusive piece of memory that's

00:08:04.510 --> 00:08:09.720
specific to just the work item,
and this is where your program

00:08:09.780 --> 00:08:13.240
can store intermediate values
or other types of variables,

00:08:13.240 --> 00:08:14.680
and that's called private memory.

00:08:14.680 --> 00:08:17.270
And so, as a programmer,
when you write an OpenCL kernel,

00:08:17.280 --> 00:08:20.620
you can write a very simple kernel,
like the one that I showed earlier,

00:08:20.620 --> 00:08:24.520
or you can write a kernel that will
move data between these address spaces.

00:08:25.080 --> 00:08:29.130
And when your program is
mapped onto a GPU device,

00:08:29.130 --> 00:08:34.540
or if it's, you know, run on the CPU,
the address space and the type of

00:08:34.590 --> 00:08:37.850
memory that's used for different
variables will change depending on

00:08:37.850 --> 00:08:41.180
what the underlying hardware supports.

00:08:41.490 --> 00:08:45.460
Okay, so after we've allocated memory
and we've seen sort of how to

00:08:45.460 --> 00:08:47.790
write a data parallel program,
let's look at the steps

00:08:47.930 --> 00:08:51.200
that are involved in getting
started with an OpenCL program.

00:08:51.270 --> 00:08:53.300
Well, the first step,

00:08:53.660 --> 00:08:57.920
is to decide or to select a
compute device from your system,

00:08:57.920 --> 00:09:02.340
and there's an API that allows you
to query the system and discover

00:09:02.340 --> 00:09:04.840
which devices support OpenCL.

00:09:04.840 --> 00:09:09.360
After you've selected a device,
in this case, I'm selecting the CPU,

00:09:09.360 --> 00:09:14.180
you create a CL command queue,
which is the object that you use

00:09:14.180 --> 00:09:18.980
to communicate with that compute
device that you can send commands to.

00:09:18.980 --> 00:09:22.180
After that,
you also have to create memory objects.

00:09:22.730 --> 00:09:27.100
In the example that we looked at,
I had three buffer objects, A, B, and C,

00:09:27.100 --> 00:09:28.140
so we can create those.

00:09:28.200 --> 00:09:33.520
And then the next step is to
create a program object and then a

00:09:33.520 --> 00:09:36.320
kernel instance from that object.

00:09:36.320 --> 00:09:38.320
And actually,
in the next part of the talk,

00:09:38.560 --> 00:09:43.610
we'll talk about some new ways of
handling this step that are new for Lion.

00:09:43.620 --> 00:09:45.760
Anyway,
once I've created my program object,

00:09:45.760 --> 00:09:50.320
I can send an execute
kernel command to my device,

00:09:50.380 --> 00:09:54.420
and the runtime takes care of moving
the data objects and the program

00:09:54.420 --> 00:09:56.840
to the device that'll be executed.

00:09:56.840 --> 00:10:01.030
And after that,
my device will go off and compute

00:10:01.030 --> 00:10:05.250
the work and perform the work,
and eventually I can move the

00:10:05.410 --> 00:10:10.380
results back to the application
and continue with my computation.

00:10:12.380 --> 00:10:16.550
Okay, that's a very, I guess,
brief summary of how OpenCL is

00:10:16.650 --> 00:10:21.230
used to solve data parallel
problems and how you might or what

00:10:21.470 --> 00:10:25.380
you might need to do in order to
integrate it with your application.

00:10:25.490 --> 00:10:28.180
Now I'd like to switch gears and
talk about a new Lion feature,

00:10:28.180 --> 00:10:32.970
which is integration between
Grand Central Dispatch

00:10:32.970 --> 00:10:35.590
and the OpenCL framework.

00:10:35.950 --> 00:10:40.010
So as we just discussed,
OpenCL uses a queuing model to

00:10:40.010 --> 00:10:43.560
send commands to the device,
to the compute device.

00:10:43.750 --> 00:10:46.800
And there are other
queuing systems in Lion.

00:10:46.940 --> 00:10:49.710
A very common queuing system
is Grand Central Dispatch.

00:10:49.800 --> 00:10:53.610
And in Grand Central Dispatch,
you create a dispatch queue or

00:10:53.620 --> 00:10:57.120
you get a global dispatch queue,
and you send blocks

00:10:57.120 --> 00:10:58.580
of work to that queue.

00:10:58.910 --> 00:11:02.850
And so these blocks are just
essentially arbitrary pieces of

00:11:02.990 --> 00:11:08.040
code that are queued in a certain
order and executed by the CPU.

00:11:08.040 --> 00:11:11.160
And there are different ways and
different types of queues that you can

00:11:11.160 --> 00:11:12.940
create using Grand Central Dispatch.

00:11:13.230 --> 00:11:16.290
Now,
OpenCL also has this command queue model,

00:11:16.380 --> 00:11:19.500
but one difference is that instead
of sending blocks to the device,

00:11:19.520 --> 00:11:19.780
you can send blocks to the device.

00:11:19.780 --> 00:11:22.780
So instead of sending blocks of
work to a OpenCL command queue,

00:11:22.780 --> 00:11:24.900
you have to send specific commands.

00:11:25.050 --> 00:11:29.270
So there's a specific command or
specific function API to enqueue a

00:11:29.270 --> 00:11:35.220
read buffer operation or to enqueue
a kernel execution and specify the

00:11:35.550 --> 00:11:39.310
local and global sizes that the
kernel should be executed with.

00:11:39.530 --> 00:11:43.910
So in Lion,
we've added the ability to combine

00:11:43.970 --> 00:11:49.010
these two types of queuing systems.

00:11:50.630 --> 00:11:55.380
It's possible to create a dispatch queue,
a Grand Central Dispatch dispatch queue,

00:11:55.500 --> 00:11:57.540
and then send blocks to it.

00:11:57.670 --> 00:12:02.420
And those blocks can contain either
functions that execute kernels,

00:12:02.470 --> 00:12:06.660
that execute OpenCL kernels,
or other types of OpenCL commands.

00:12:06.800 --> 00:12:11.130
And so in this case,
the dispatch queue will support the other

00:12:11.130 --> 00:12:16.870
parts of the Grand Central Dispatch API,
but it can send OpenCL commands

00:12:16.980 --> 00:12:19.870
to a CPU or a GPU compute device.

00:12:20.740 --> 00:12:25.170
Okay, so let's take a look at what this
would actually look like in your code.

00:12:25.460 --> 00:12:28.540
Now, I have a block here,
and I'm going to invoke

00:12:28.600 --> 00:12:30.940
the add arrays kernel.

00:12:31.000 --> 00:12:34.740
And this is a version of the same
kernel that we looked at earlier.

00:12:34.840 --> 00:12:39.420
And I do this by calling dispatch async,
passing a queue,

00:12:39.480 --> 00:12:42.500
and then a block that contains
a call to the add arrays

00:12:42.760 --> 00:12:45.160
underscore kernel function.

00:12:45.230 --> 00:12:48.960
And then there's a special structure
that we'll talk about in a second.

00:12:49.020 --> 00:12:52.210
and then the arguments of that kernel, A,
B, and C.

00:12:53.650 --> 00:12:55.690
Okay, so if you remember,
this is the kernel that we

00:12:55.690 --> 00:12:57.760
looked at just a few minutes ago.

00:12:57.820 --> 00:12:58.840
It's the @arrays kernel.

00:12:58.840 --> 00:13:03.240
It takes three arguments
that are arrays of integers.

00:13:03.310 --> 00:13:06.360
And what I'm going to do is I'm
going to take this kernel and feed

00:13:06.360 --> 00:13:09.990
it through the OpenCLC compiler,
and the output of that compiler will be

00:13:10.140 --> 00:13:16.640
some bit code that's loaded at runtime
and also a declaration of a kernel block.

00:13:16.700 --> 00:13:20.720
Now, the kernel block has the same
name as the kernel with the word

00:13:20.730 --> 00:13:23.060
underscore kernel appended to it.

00:13:23.230 --> 00:13:28.950
It also takes this nd_range structure,
which simply defines the global

00:13:28.950 --> 00:13:34.120
and local size of the kernel that
should be launched on a device.

00:13:34.170 --> 00:13:40.890
And so what's happened here is
I've changed the process of...

00:13:41.070 --> 00:13:43.710
of creating a program,
compiling a kernel,

00:13:43.710 --> 00:13:53.640
and then launching the kernel such
that all I have to do is to define my

00:13:53.640 --> 00:13:53.790
add_arrays kernel in a .CL source file,

00:13:53.950 --> 00:13:58.130
And then at runtime in
my application program,

00:13:58.130 --> 00:14:01.220
I simply call the add arrays
underscore kernel function,

00:14:01.220 --> 00:14:05.630
pass the end array structure,
and then the arguments to the kernel.

00:14:05.760 --> 00:14:09.320
And so this process is a
little bit more streamlined.

00:14:11.150 --> 00:14:14.000
Okay, let me tell you where
these queues come from.

00:14:14.150 --> 00:14:17.600
So the dispatch queue that you
are sending this work to is

00:14:17.630 --> 00:14:21.270
an ordinary dispatch queue,
except that it's associated

00:14:21.640 --> 00:14:22.630
with a compute device.

00:14:22.640 --> 00:14:26.320
And you associate the queue with
a compute device by calling the

00:14:26.420 --> 00:14:28.750
GCL create dispatch queue function.

00:14:28.900 --> 00:14:32.330
And you can associate or find a
device for your dispatch queue

00:14:32.330 --> 00:14:34.100
in several different ways.

00:14:34.300 --> 00:14:39.100
One way is to specify device type,
in this case the GPU.

00:14:39.170 --> 00:14:44.040
It's also possible to specify other
attributes like the queue priority.

00:14:44.170 --> 00:14:47.540
And if your application has
some way of allowing the user

00:14:47.540 --> 00:14:51.590
to specify a compute device,
or you have, you'd like to use a specific

00:14:52.020 --> 00:14:55.990
OpenCL compute device,
you can always specify the

00:14:55.990 --> 00:15:00.190
CL device type use device ID flag,
and the command queue will be,

00:15:00.250 --> 00:15:04.400
or the dispatch queue will be
created with that specific device.

00:15:05.420 --> 00:15:08.790
Now, the data that you pass to your
kernel block functions has to

00:15:08.790 --> 00:15:10.900
be allocated in a special way.

00:15:11.030 --> 00:15:16.490
And we can support either data
that's been allocated as a buffer,

00:15:16.710 --> 00:15:20.670
so in this case, using GCL malloc,
or data that's allocated as an image.

00:15:20.670 --> 00:15:25.700
And of course, we can also support
passing immediate data,

00:15:25.700 --> 00:15:29.320
so just a floating point or
an integer value to a kernel.

00:15:30.300 --> 00:15:34.150
So, in this case,
I'm allocating memory that is safe to

00:15:34.150 --> 00:15:36.880
pass to a kernel block using GCL malloc.

00:15:37.330 --> 00:15:41.600
I can also use GCL malloc to
wrap an existing allocation,

00:15:41.690 --> 00:15:45.870
in this case,
using the flag CLmem use host pointer.

00:15:46.320 --> 00:15:51.210
And to create an image,
I use the function GCL create image

00:15:51.460 --> 00:15:56.440
and pass a format that describes
the channel order and channel

00:15:56.880 --> 00:15:59.380
type of the image to be created.

00:16:00.590 --> 00:16:03.200
There are other commands that
I can send to dispatch queues.

00:16:03.260 --> 00:16:06.640
There's a memcopy command that
operates very much like memcopy.

00:16:06.640 --> 00:16:11.730
The order of the operands indicate
the type of command that's executed.

00:16:11.760 --> 00:16:14.740
So for example, in this case,
my destination is A,

00:16:14.740 --> 00:16:18.760
which is that buffer that
I allocated using GCL malloc.

00:16:18.800 --> 00:16:20.910
And my source is the host data.

00:16:21.110 --> 00:16:29.610
And so I'm copying data from
the host to the CL resource.

00:16:30.340 --> 00:16:35.100
I can also use map,
and I can map and manipulate data

00:16:35.100 --> 00:16:39.340
that was used by OpenCL on the host.

00:16:39.470 --> 00:16:43.120
Let me talk a little bit more about
how MAP is used in conjunction

00:16:43.120 --> 00:16:47.520
with the memory consistency model
that you have to follow when you're

00:16:47.520 --> 00:16:49.670
using this Dispatch Queue API.

00:16:49.750 --> 00:16:53.120
So in this case, what I've done is I've
allocated my three buffers,

00:16:53.120 --> 00:16:54.010
A, B, and C.

00:16:54.090 --> 00:16:56.680
And as you can see,
they've been allocated in

00:16:56.760 --> 00:16:58.740
host memory by the runtime.

00:16:58.900 --> 00:17:04.570
Now, when I enqueue and dispatch
the add arrays kernel,

00:17:04.570 --> 00:17:08.590
and it's actually executed by the device,
the runtime will move those

00:17:08.600 --> 00:17:11.330
buffer objects into global memory.

00:17:11.420 --> 00:17:14.340
And now at this point,
my application might not be

00:17:14.340 --> 00:17:18.840
able to safely manipulate A,
B, and C, because it's possible that the

00:17:18.840 --> 00:17:23.420
runtime is -- or that the device
is using the data asynchronously.

00:17:23.470 --> 00:17:26.280
And since I've dispatched
the block of work already,

00:17:26.340 --> 00:17:29.440
I don't really know if it's been
completed or if it's pending

00:17:29.440 --> 00:17:30.800
or what its current state is.

00:17:30.850 --> 00:17:32.750
Maybe it's running concurrently.

00:17:32.850 --> 00:17:35.720
And so in order for the application
thread to be certain that it

00:17:35.830 --> 00:17:41.930
can safely access the data in A,
B, or C, we have to either map the

00:17:41.930 --> 00:17:43.660
data or make a copy of it.

00:17:43.690 --> 00:17:46.800
And those commands are both
enqueued into the command queue.

00:17:47.060 --> 00:17:49.900
And so if I wanted to map the data,
I would do something like this.

00:17:49.900 --> 00:17:53.490
I would call dispatch sync
because I want the results of my

00:17:53.490 --> 00:17:57.240
command to have completed before
the dispatch function returns.

00:17:57.500 --> 00:17:59.060
And call GCL map pointer.

00:17:59.240 --> 00:18:01.240
Process the results.

00:18:01.240 --> 00:18:04.700
And then when I'm done,
I have to call again with dispatch sync.

00:18:05.240 --> 00:18:06.660
Actually, in this case, it's a call.

00:18:06.660 --> 00:18:08.900
In this case,
either dispatch sync or async would work

00:18:08.910 --> 00:18:11.600
since I don't care when this finishes.

00:18:11.700 --> 00:18:14.000
GCL unmap,
which tells the runtime that it's

00:18:14.170 --> 00:18:18.880
possible or that it's able to move that
data to a different device if it needs

00:18:18.900 --> 00:18:21.790
to in the future for some other command.

00:18:22.540 --> 00:18:27.630
Now, in this API,
we can synchronize between the work

00:18:27.720 --> 00:18:31.460
that's running on an OpenCL device
and the application thread,

00:18:31.510 --> 00:18:34.270
and we do that using
Grand Central Dispatch

00:18:34.520 --> 00:18:36.020
synchronization primitives.

00:18:36.100 --> 00:18:37.630
So I'd like to show two examples.

00:18:37.910 --> 00:18:43.490
The first one is a case where we enqueue
work to multiple GPUs and then wait for

00:18:43.490 --> 00:18:45.960
that work to complete in the application.

00:18:46.070 --> 00:18:49.880
The second example will be having
work executed on one GPU and then

00:18:49.890 --> 00:18:54.810
having another kernel on a second
GPU wait for the first kernel to finish.

00:18:54.820 --> 00:18:57.540
So let's look at the first example.

00:18:57.550 --> 00:19:01.930
In this case, I have a dispatch group
that I've created,

00:19:02.320 --> 00:19:07.480
and I enqueue work to the first queue,
then enqueue work to the second

00:19:07.590 --> 00:19:10.180
queue using dispatch group async.

00:19:10.190 --> 00:19:13.220
After that, I perform more work on
the application thread.

00:19:13.240 --> 00:19:15.940
And then at some point in the future,
I call dispatch group 1,

00:19:15.950 --> 00:19:18.020
and I call dispatch group 2,
and I call dispatch group 3,

00:19:18.220 --> 00:19:20.330
and I call dispatch group 4,
and I call dispatch group 5,

00:19:20.340 --> 00:19:22.580
and I call dispatch group 6,
and I call dispatch group 7,

00:19:22.590 --> 00:19:24.820
and I call dispatch group 8,
and I call dispatch group 9,

00:19:24.820 --> 00:19:27.060
and I call dispatch group 10,
and I call dispatch group 11,

00:19:27.170 --> 00:19:29.220
and I call dispatch group 12,
and I call dispatch group 13,

00:19:29.220 --> 00:19:31.220
and I call dispatch group 14,
and I call dispatch group 15,

00:19:31.220 --> 00:19:33.300
and I call dispatch group 16,
and I call dispatch group 17,

00:19:33.300 --> 00:19:35.300
and I call dispatch group 18,
and I call dispatch group 19,

00:19:35.400 --> 00:19:37.140
and I call dispatch group 20,
and I call dispatch group 21,

00:19:37.140 --> 00:19:38.820
and I call dispatch group 22,
and I call dispatch group 23,

00:19:38.820 --> 00:19:40.260
and I call dispatch group 24,
and I call dispatch group 25,

00:19:40.260 --> 00:19:44.260
and I call dispatch group 26,
and I call dispatch group 27,

00:19:44.260 --> 00:19:48.260
and I call dispatch group 28,
and I call dispatch group 29,

00:19:48.260 --> 00:19:52.260
and I call dispatch group 30,
and I call dispatch group 31,

00:19:52.260 --> 00:19:56.190
and I call dispatch group 32,
and I call dispatch group 33,

00:19:56.460 --> 00:20:00.260
and I call dispatch group 34,
and I call dispatch group 35,

00:20:01.580 --> 00:20:05.900
There are many other functions
you can send to Dispatch Queues.

00:20:05.900 --> 00:20:09.980
There are a number of other
functions for synchronization

00:20:10.290 --> 00:20:12.190
for callbacks and semaphores.

00:20:12.720 --> 00:20:19.260
There's a memory object finalizer that's
similar to the existing CL 1.1 API.

00:20:19.350 --> 00:20:25.120
And timing functions and then also
the ability to interoperate between

00:20:25.120 --> 00:20:27.860
Dispatch Queues and the command
queues that are part of OpenCL 1.1.

00:20:28.110 --> 00:20:33.240
Now, the documentation for this and the
APIs are all in the header file,

00:20:33.240 --> 00:20:36.280
in the gcl.h header file
in the OpenCL framework.

00:20:36.300 --> 00:20:40.240
And you can access this functionality
by just including the framework header,

00:20:40.250 --> 00:20:42.490
opencl.h.

00:20:42.880 --> 00:20:47.870
Now I'd like to switch gears and talk
a little bit about offline compilation,

00:20:47.880 --> 00:20:51.690
which is another new feature in Lion.

00:20:52.760 --> 00:20:56.960
Now, in Lion, you can use either an
online compilation routine,

00:20:56.980 --> 00:20:58.940
which is what you would
have used in Snow Leopard.

00:20:58.940 --> 00:21:03.720
In this case, your kernel source is in
a string in your program,

00:21:03.720 --> 00:21:07.690
and you pass this string to
cl create program with source,

00:21:07.700 --> 00:21:11.760
and then you call cl build program,
and you end up with a device binary

00:21:11.760 --> 00:21:14.240
that is executed on the compute device.

00:21:14.240 --> 00:21:19.440
And what this means is that your
program source code has to be in

00:21:19.440 --> 00:21:24.070
a really human-readable format,
in that kernel source.

00:21:24.510 --> 00:21:29.860
Now, in Lion,
we've added the capability to compile

00:21:29.860 --> 00:21:36.190
offline and then load a binary,
and that binary is independent of the

00:21:36.190 --> 00:21:38.440
specific device that's in your system.

00:21:38.440 --> 00:21:41.060
It's actually specific
to a class of devices.

00:21:41.060 --> 00:21:46.040
And so in that case,
you would put your source in a kernel,

00:21:46.040 --> 00:21:50.320
in this case, kernel file.cl,
a cl source file.

00:21:50.600 --> 00:21:53.780
Then, while you're building your project,
you run that source file

00:21:53.780 --> 00:21:55.700
through the OpenCL C compiler.

00:21:55.780 --> 00:21:58.340
That produces a bitcode file.

00:21:58.480 --> 00:22:03.390
And then from there, at runtime,
you can call cl create program with

00:22:03.470 --> 00:22:07.470
binary to load that bitcode file,
then build your program and

00:22:07.530 --> 00:22:09.690
end up with a device binary.

00:22:10.140 --> 00:22:14.780
Okay, so to create your bitcode file,
you use the OpenCL C compiler,

00:22:14.780 --> 00:22:20.020
and there are three different
triple architecture arguments.

00:22:20.020 --> 00:22:23.490
You can create a bitcode
file for 32-bit GPUs,

00:22:23.830 --> 00:22:27.050
for 32-bit CPUs, or for 64-bit CPUs.

00:22:27.080 --> 00:22:30.850
And now this bitcode file is
specific to the class of device,

00:22:30.930 --> 00:22:35.520
not the individual device,
so that 32-bit GPU bitcode file will work

00:22:35.520 --> 00:22:38.420
with any of the GPUs that support OpenCL.

00:22:39.280 --> 00:22:45.640
Now, you'll notice in this example,
the bitcode file is named, in this case,

00:22:45.720 --> 00:22:47.160
kernel-gpu32.bc.

00:22:47.160 --> 00:22:50.360
The important part of that
name is the .bc at the end.

00:22:50.360 --> 00:22:54.900
When you pass this file name to
cl-create-program-with-binary,

00:22:56.430 --> 00:23:01.330
The runtime makes sure that you have
passed a bitcode file by checking to

00:23:01.390 --> 00:23:04.490
see that the extension ends in .bc.

00:23:05.360 --> 00:23:10.180
Okay, so it's possible to create
this bitcode file and automate

00:23:10.180 --> 00:23:12.520
this process in Xcode 4.

00:23:12.640 --> 00:23:17.700
To do this, you would add the OpenCL.cl
file to your target,

00:23:17.760 --> 00:23:20.400
and then when you build your project,

00:23:21.070 --> 00:23:25.110
Xcode build will produce the--
or will invoke the CL compiler,

00:23:25.210 --> 00:23:27.640
then produce that bitcode file,
and then at runtime,

00:23:27.640 --> 00:23:29.240
you can load the bitcode file.

00:23:29.240 --> 00:23:32.650
You can either use CL create
program with binary,

00:23:32.680 --> 00:23:34.630
like we just discussed,

00:23:34.810 --> 00:23:40.880
or you can use the Dispatch Queue API and
call the kernel block that was created

00:23:40.880 --> 00:23:43.530
by the compiler for your function.

00:23:43.800 --> 00:23:48.910
And now I'd like to show you an
example of how that works in Xcode.

00:23:51.340 --> 00:23:55.890
Okay, so I'm launching Xcode here.

00:23:56.290 --> 00:23:59.230
And what I have here is
a very simple example.

00:23:59.430 --> 00:24:03.260
It's not quite as simple as the
kernel we looked at just a second ago,

00:24:03.260 --> 00:24:05.330
but it's very straightforward.

00:24:05.530 --> 00:24:08.390
And so what I'm going to do is -- I'll
look at the code in just a second,

00:24:08.400 --> 00:24:10.930
but what I'm going to do first
is show you how to add a .CL

00:24:10.930 --> 00:24:12.780
file to your Xcode project.

00:24:12.840 --> 00:24:18.130
So the first thing I'm doing is going
up to "Add files to my project," and

00:24:18.130 --> 00:24:22.300
now I'm going to select the .CL file
that I created before the session.

00:24:22.300 --> 00:24:26.310
And you can see that it contains a
kernel called "countstuff." I'll go

00:24:26.310 --> 00:24:29.400
down here and make sure that I've
added the .CL file to the target.

00:24:29.650 --> 00:24:31.560
In this case,
the target's called "example0,"

00:24:31.560 --> 00:24:37.180
and click "Add." And there's
my .CL file right in Xcode.

00:24:37.640 --> 00:24:40.700
Okay, so what else do I have to do in
order to use this integration?

00:24:40.700 --> 00:24:43.910
Well, I have to include the header
file that the compiler generated.

00:24:44.120 --> 00:24:46.810
In this case,
the header file is the name of the

00:24:46.810 --> 00:24:49.490
.CL file with a .h appended to it.

00:24:49.520 --> 00:24:54.380
And then after I've initialized data
and I'm ready to execute the kernel,

00:24:54.440 --> 00:24:56.220
I can just call my kernel.

00:24:56.220 --> 00:24:58.280
In this case, it's the countstuff kernel.

00:24:58.280 --> 00:25:01.370
So the name of the kernel in
the .CL file was countstuff,

00:25:01.370 --> 00:25:05.740
and I've appended underscore kernel to
it and created an ndrange structure.

00:25:06.260 --> 00:25:08.340
Okay, two more things I have to check.

00:25:08.340 --> 00:25:14.420
If I go to my build phases settings,
so this is under the project settings.

00:25:15.070 --> 00:25:19.620
I want to make sure that I've
added the source to the .CL

00:25:19.790 --> 00:25:23.230
to my compile sources list,
and that I'm linking the project

00:25:23.230 --> 00:25:24.840
with the OpenCL framework.

00:25:24.970 --> 00:25:27.090
So now if I build...

00:25:28.220 --> 00:25:31.600
and I'm able to successfully
build the project.

00:25:31.600 --> 00:25:35.190
In this case,
the program just counts some numbers

00:25:35.320 --> 00:25:39.340
and outputs the statistics of the count.

00:25:39.460 --> 00:25:42.630
What's more interesting is that
if I look at the build settings

00:25:42.630 --> 00:25:48.670
in Xcode and click on all,
and then type in OpenCL,

00:25:49.280 --> 00:25:52.040
We can see that there are a number
of OpenCL build settings that

00:25:52.070 --> 00:25:54.700
I can set within the Xcode GUI.

00:25:54.800 --> 00:25:57.500
There are settings for an
auto-vectorizer that we'll talk

00:25:57.500 --> 00:26:00.500
about later in the session,
as well as some other optimizations

00:26:00.580 --> 00:26:02.240
and flags that can be set.

00:26:02.290 --> 00:26:04.720
You'll also notice the
OpenCL architectures option,

00:26:04.780 --> 00:26:09.070
and this is setting the different types
of bitcode files that I should produce.

00:26:09.140 --> 00:26:11.200
In this case,
I'm producing bitcode files for each

00:26:11.330 --> 00:26:13.160
of the three types of architectures.

00:26:13.360 --> 00:26:18.320
Okay, now we've seen how to use
OpenCL with Xcode integration,

00:26:18.360 --> 00:26:22.140
and how to use OpenCL with
Grand Central Dispatch to make launching

00:26:22.140 --> 00:26:25.150
kernels and executing programs easier.

00:26:25.240 --> 00:26:29.090
Now I'd like to invite my colleague,
Jim Sheare on the stage to talk a

00:26:29.090 --> 00:26:33.800
little bit more about OpenCL and
how it can be used with OpenGL.

00:26:35.400 --> 00:26:37.510
Thanks, Abe.

00:26:37.660 --> 00:26:40.440
So a lot of you are
probably using OpenGL,

00:26:40.470 --> 00:26:44.130
and you might be wondering how you
can add OpenCL to your application.

00:26:44.370 --> 00:26:45.710
So I'm going to be
talking about that topic,

00:26:45.730 --> 00:26:49.370
sharing with OpenGL and a few others
for about the next 20 minutes.

00:26:50.230 --> 00:26:52.100
So what do we mean by sharing?

00:26:52.100 --> 00:26:54.100
Well, if you have an OpenGL program,
it probably has some data.

00:26:54.100 --> 00:26:57.220
In this case, some geometry data,
like a vertex buffer object,

00:26:57.220 --> 00:26:59.090
which you want to render in OpenGL.

00:26:59.100 --> 00:27:03.170
But then maybe you want to, say,
do some compute on that same geometry,

00:27:03.170 --> 00:27:06.100
or maybe you wanted to generate
that geometry using OpenCL.

00:27:06.100 --> 00:27:07.100
And that's the same data.

00:27:07.100 --> 00:27:09.090
So that's one type of sharing,
geometry data.

00:27:09.100 --> 00:27:11.180
But if you're rendering in OpenGL,
you're going to render

00:27:11.180 --> 00:27:13.100
some kind of picture,
right?

00:27:13.100 --> 00:27:15.100
Maybe through some frame
buffer object to a texture.

00:27:15.100 --> 00:27:19.100
And then maybe you want to post-process
that same piece of data in OpenCL.

00:27:19.100 --> 00:27:22.090
So again,
you're working on the same data, sharing.

00:27:22.100 --> 00:27:24.550
The other type of sharing you might
not be as familiar with is sharing

00:27:24.550 --> 00:27:25.990
of synchronization primitives.

00:27:26.160 --> 00:27:28.090
And I'll come back to
this a little bit later.

00:27:29.830 --> 00:27:33.450
So let's step back for a second and take
a look at how CL and GL view your system.

00:27:33.520 --> 00:27:37.100
So imagine you have this Mac that
has a GPU and a CPU in it.

00:27:37.120 --> 00:27:41.740
OpenCL sees these as devices,
devices for doing computation,

00:27:41.760 --> 00:27:46.550
whereas OpenGL sees them as renderers,
renderers for making a picture.

00:27:46.930 --> 00:27:50.130
You get access to the devices
in CL by asking for device

00:27:50.130 --> 00:27:52.030
types with CL get device IDs.

00:27:52.230 --> 00:27:55.420
So you can ask for the CPU,
or you can ask for the GPU,

00:27:55.420 --> 00:27:57.460
or if you're computationally
greedy or whatever,

00:27:57.460 --> 00:28:00.100
you can ask for all of them
and we'll give them to you.

00:28:00.220 --> 00:28:03.330
In OpenGL, you can restrict the devices
you ask for similarly,

00:28:03.350 --> 00:28:05.850
except you use pixel formats,
and you set up your pixel

00:28:05.850 --> 00:28:07.600
format with certain attributes.

00:28:07.670 --> 00:28:09.830
So here we have a pixel format
that has some attributes about

00:28:09.940 --> 00:28:12.380
the drawable destination,
like how many bits are we going

00:28:12.450 --> 00:28:13.650
to use for color and for alpha.

00:28:13.950 --> 00:28:16.950
But notice there's also an attribute
for what type of device we want.

00:28:17.110 --> 00:28:20.130
We want accelerated devices in this case.

00:28:20.190 --> 00:28:22.500
So we'll give you just those devices.

00:28:22.620 --> 00:28:25.850
So then you're going to create a
context in CL and GL with these devices.

00:28:25.880 --> 00:28:28.260
Now, even though the CL and GL context
look the same on the slide,

00:28:28.300 --> 00:28:29.690
that's a little bit misleading.

00:28:29.700 --> 00:28:30.900
They're not the same type of object.

00:28:30.990 --> 00:28:37.370
But from your perspective
as a programmer,

00:28:37.370 --> 00:28:37.370
they're the gateway to your devices.

00:28:37.370 --> 00:28:37.370
So that's why I've drawn them the same.

00:28:37.980 --> 00:28:39.540
So how do you actually
do work on these devices?

00:28:39.580 --> 00:28:42.340
Well, in CL, like Abe said,
you create command queues, right?

00:28:42.340 --> 00:28:44.660
You can have a GPU command
queue or a CPU command queue.

00:28:44.790 --> 00:28:48.390
Just depends on what device is better
for the kind of work you want to do.

00:28:49.370 --> 00:28:50.670
In GL, it's a little bit different.

00:28:50.860 --> 00:28:53.590
The device that's doing the work
is represented by a virtual screen,

00:28:53.600 --> 00:28:53.940
okay?

00:28:54.120 --> 00:28:57.480
A virtual screen is a combination
of physical hardware in your

00:28:57.480 --> 00:28:59.270
box plus the attached display.

00:28:59.370 --> 00:29:01.420
In this case,
I only have drawn one virtual screen,

00:29:01.460 --> 00:29:03.890
but that's what's doing the work in GL.

00:29:04.110 --> 00:29:05.600
So here's the situation we want.

00:29:05.670 --> 00:29:09.240
On one side, we have GL on the GPU,
which is going to do some rendering.

00:29:09.240 --> 00:29:12.240
And on the other side, we have CL,
which is going to do some compute.

00:29:12.240 --> 00:29:14.760
And they're going to do it on
the shared geometry between them.

00:29:14.910 --> 00:29:17.060
But they want to take different
views of the geometry,

00:29:17.060 --> 00:29:17.340
okay?

00:29:17.570 --> 00:29:20.390
GL is going to see this as a VBO,
whereas CL just wants to

00:29:20.390 --> 00:29:23.110
see it as a buffer of data,
as a CLM object.

00:29:23.320 --> 00:29:25.790
Well, they can do that because
these are managed by what's

00:29:25.790 --> 00:29:27.120
called a CGL share group.

00:29:27.210 --> 00:29:30.550
It's an object we haven't
talked about until now.

00:29:30.720 --> 00:29:32.200
And actually,
the CGL share group does more

00:29:32.200 --> 00:29:33.360
than just manage the data.

00:29:33.370 --> 00:29:35.560
It also manages these shared devices.

00:29:35.580 --> 00:29:37.120
So the real picture is this.

00:29:37.140 --> 00:29:40.960
The CL context and the GL context
are both associated with a

00:29:41.040 --> 00:29:43.300
particular CGL share group.

00:29:44.150 --> 00:29:46.240
So, how do you use share groups?

00:29:46.330 --> 00:29:48.500
Well,
your first step is to get a share group.

00:29:48.710 --> 00:29:50.510
And the good news is,
if you have an OpenGL context

00:29:50.510 --> 00:29:52.340
in your program,
which you probably do,

00:29:52.420 --> 00:29:53.380
you already have a share group.

00:29:53.470 --> 00:29:56.730
You just need to ask your
GL context for that share group.

00:29:56.970 --> 00:30:02.400
So step two is you create your CL context
in a special way using the share group.

00:30:02.750 --> 00:30:05.030
Step three is to create any
of these objects that you

00:30:05.030 --> 00:30:07.810
want to share between APIs,
create them in OpenGL first.

00:30:07.900 --> 00:30:11.260
And you do that in the exact normal way
you would create any OpenGL objects.

00:30:11.260 --> 00:30:13.250
You don't do anything different.

00:30:14.090 --> 00:30:17.230
But then on the CL side,
you create your CL objects from

00:30:17.230 --> 00:30:21.610
the GL objects using special
API that we provide in the system.

00:30:22.880 --> 00:30:24.470
So let's take a look
at how the code looks.

00:30:24.480 --> 00:30:29.360
OK, so here we have maybe a custom
NSOpenGL view in your application.

00:30:29.390 --> 00:30:31.560
So you can interrogate
that OpenGL view and say,

00:30:31.560 --> 00:30:33.440
oh, give me the context, please.

00:30:33.850 --> 00:30:37.240
And then the next step is you then
interrogate that context and ask for the

00:30:37.240 --> 00:30:39.760
share group using cgl get share group.

00:30:39.780 --> 00:30:42.030
And then you package that share
group that you have into an

00:30:42.030 --> 00:30:44.480
array of CL context properties.

00:30:44.480 --> 00:30:46.080
So you make this array.

00:30:46.110 --> 00:30:48.250
And then you pass this
to CL create context.

00:30:48.380 --> 00:30:50.510
And when you do that,
we're going to give you back a

00:30:50.510 --> 00:30:53.770
CL context that has the devices
from the share group in it.

00:30:54.500 --> 00:30:55.400
But there's one tip.

00:30:55.400 --> 00:30:58.160
If you want to have the CL CPU device
in that context as well,

00:30:58.160 --> 00:30:59.160
you have to do one extra step.

00:30:59.330 --> 00:31:00.390
So the setup is the same.

00:31:00.560 --> 00:31:02.840
Setup hasn't changed from before.

00:31:02.950 --> 00:31:06.860
But now the extra step is you have to
get the device ID for the CPU device by

00:31:06.860 --> 00:31:09.300
calling CL get device IDs for the CPU.

00:31:09.390 --> 00:31:11.790
And then in addition to
passing this properties array,

00:31:11.860 --> 00:31:14.300
you also feed us this device ID.

00:31:15.640 --> 00:31:17.820
So another thing you might want
to do is you might want to use

00:31:17.870 --> 00:31:20.660
the CL device that corresponds
to the current virtual screen.

00:31:20.740 --> 00:31:23.300
Now again, to remind you,
that's the currently

00:31:23.480 --> 00:31:24.830
in use OpenGL renderer.

00:31:25.010 --> 00:31:26.420
So why would you want to do that?

00:31:26.460 --> 00:31:28.830
Well, you would want to do that
for performance reasons.

00:31:28.920 --> 00:31:31.230
So let's say that you have a
problem that's memory bound,

00:31:31.230 --> 00:31:33.260
and you want to do compute and
render on some shared data.

00:31:33.370 --> 00:31:35.730
So again,
the time to shuffle this data across the

00:31:35.730 --> 00:31:38.040
PCIe bus is what's bottlenecking you.

00:31:38.140 --> 00:31:40.650
So you'd want to make sure you're doing
your compute and your render on the same

00:31:40.660 --> 00:31:43.180
device so we don't have to move the data.

00:31:43.290 --> 00:31:43.940
So you can do that.

00:31:44.030 --> 00:31:45.200
You just ask CL.

00:31:45.260 --> 00:31:48.750
You call CL get GL context info apple
and ask for the current virtual screen.

00:31:48.840 --> 00:31:51.450
We'll give you back the
CL device that corresponds to

00:31:51.450 --> 00:31:53.510
the current virtual screen,
and you can use the command

00:31:53.530 --> 00:31:56.110
queue associated with that
device to do your work.

00:31:57.680 --> 00:32:00.750
So how do you create these OpenCL memory
objects that we talked about?

00:32:00.810 --> 00:32:03.620
Now remember we said you're going
to create them in OpenGL first,

00:32:03.690 --> 00:32:05.190
and you don't have to
do anything special,

00:32:05.200 --> 00:32:06.730
normal OpenGL creation.

00:32:06.830 --> 00:32:08.910
And then you call these
special entry points.

00:32:08.990 --> 00:32:12.900
So here you see CL create from GL buffer,
and in this case we're passing in

00:32:12.900 --> 00:32:14.480
a VBO that we've created in GL.

00:32:14.550 --> 00:32:17.170
But there are entry points
for creating CL objects,

00:32:17.170 --> 00:32:19.730
image memory objects, from textures,
you know,

00:32:19.830 --> 00:32:23.510
2 and 3D from render buffers that you
might have attached to a frame buffer.

00:32:24.000 --> 00:32:26.590
So that brings up the question, okay,
let's say I create an

00:32:26.590 --> 00:32:27.600
image from a texture.

00:32:27.600 --> 00:32:28.480
What is my format?

00:32:28.580 --> 00:32:29.040
What do I get?

00:32:29.230 --> 00:32:31.500
It just depends on the
internal format of the texture.

00:32:31.600 --> 00:32:32.990
So there are some examples here.

00:32:33.130 --> 00:32:36.000
For example,
if your internal format is GLRGBA8,

00:32:36.100 --> 00:32:38.560
when you create a CL image from that,
you're going to get, you know,

00:32:38.580 --> 00:32:41.100
CLRGBAUnormInt8.

00:32:41.210 --> 00:32:43.300
It just depends on the internal format.

00:32:43.450 --> 00:32:45.780
And there's a mapping for each one.

00:32:46.760 --> 00:32:48.860
Okay,
so now we know how to create the objects.

00:32:49.110 --> 00:32:50.340
How do we actually use them in CL?

00:32:50.430 --> 00:32:51.530
What do we have to do special?

00:32:51.530 --> 00:32:52.990
Any work that we have to do?

00:32:53.100 --> 00:32:53.640
Well, not really.

00:32:53.640 --> 00:32:56.530
You just use them as you
normally would any CL objects.

00:32:56.530 --> 00:32:58.760
If you're already a CL programmer,
you're going to be very comfortable here.

00:32:58.760 --> 00:33:00.780
Okay, but there's one thing
you should remember,

00:33:00.790 --> 00:33:03.620
and that's this notion of flush, acquire,
compute, release.

00:33:03.620 --> 00:33:06.760
So if you guys have been here in
previous OpenGL sessions at WWDC,

00:33:06.760 --> 00:33:08.970
you've probably heard of flush and bind.

00:33:08.970 --> 00:33:10.220
We talked about that a lot.

00:33:10.380 --> 00:33:11.380
This is the same idea.

00:33:11.380 --> 00:33:12.340
So what does this mean?

00:33:12.370 --> 00:33:17.980
Okay, let's say we're going to do some
work in GL on this shared object.

00:33:17.990 --> 00:33:17.990
We want to call GL flush.

00:33:18.200 --> 00:33:20.900
Next step is that we acquire
these objects in OpenCL,

00:33:20.900 --> 00:33:21.440
okay?

00:33:21.440 --> 00:33:23.540
CLNQ acquired GL objects.

00:33:23.750 --> 00:33:26.170
Special function we provide,
and you pass it an array of the

00:33:26.170 --> 00:33:27.540
objects you're going to work on.

00:33:27.650 --> 00:33:29.140
And that's your way of telling us, "Hey,
OpenCL,

00:33:29.140 --> 00:33:32.300
I'm about to do something to these,
so watch out." And then you

00:33:32.300 --> 00:33:35.690
just do whatever it is that
you want to do in OpenCL.

00:33:35.690 --> 00:33:35.690
Whale away.

00:33:36.000 --> 00:33:38.750
Okay, when you're done, though,
make sure you call CL_NQ_RELEASE,

00:33:38.970 --> 00:33:39.560
GL objects.

00:33:39.560 --> 00:33:42.490
And again, that's just your way of
telling us that you're done.

00:33:42.560 --> 00:33:44.500
As long as you do that, you'll be fine.

00:33:44.580 --> 00:33:48.200
Now note, this GL flush is only required
if you're NQing CL and

00:33:48.200 --> 00:33:50.230
GL work on different threads.

00:33:50.270 --> 00:33:52.350
If they're on the same thread, hey,
it's even easier.

00:33:52.490 --> 00:33:55.400
You can remove this flush,
maybe get some better performance, right?

00:33:55.520 --> 00:33:57.360
And then you can -- this is
what the picture looks like.

00:33:57.410 --> 00:33:59.960
It's just acquire compute release.

00:33:59.960 --> 00:34:01.110
Even easier.

00:34:01.890 --> 00:34:02.990
So that's on the CL side.

00:34:03.210 --> 00:34:05.560
What about using the
objects on the GL side?

00:34:05.610 --> 00:34:06.590
Well, really, you have to do nothing.

00:34:06.670 --> 00:34:07.720
That's the good news.

00:34:07.770 --> 00:34:11.000
When you call CLNQRelease GL objects,
we call all the flushing

00:34:11.000 --> 00:34:12.550
that's necessary on our side.

00:34:12.620 --> 00:34:15.200
So you simply bind and use
them in OpenGL as you would.

00:34:15.240 --> 00:34:16.950
Nothing special.

00:34:18.010 --> 00:34:19.660
So that was sharing data.

00:34:19.670 --> 00:34:22.130
And remember I said there's a second
half that's maybe new to some of you,

00:34:22.140 --> 00:34:23.870
which is sharing synchronization.

00:34:23.980 --> 00:34:25.740
So what am I talking about?

00:34:25.790 --> 00:34:29.420
Well,
both CL and GL have these primitives.

00:34:29.420 --> 00:34:30.770
In CL, they're called CL events.

00:34:30.780 --> 00:34:32.630
In GL,
they're called GL sync objects that allow

00:34:32.630 --> 00:34:36.080
you to wait on some work to complete,
okay, but not all of the work.

00:34:36.120 --> 00:34:37.850
Basically,
the point of these is that you want to

00:34:37.880 --> 00:34:39.670
avoid completely flushing your pipe.

00:34:39.730 --> 00:34:41.770
If there's a piece of work at the
beginning of the pipe that you need

00:34:41.770 --> 00:34:43.420
to wait on for some other work,
you don't want to wait

00:34:43.420 --> 00:34:44.100
until everything's done.

00:34:44.100 --> 00:34:46.630
You just want to wait on that one piece.

00:34:47.470 --> 00:34:50.930
And because these two things do the exact
same thing in their respective APIs,

00:34:50.930 --> 00:34:54.560
when we're using the APIs together,
it makes sense to use them together.

00:34:54.560 --> 00:34:58.210
There's this natural correspondence,
so that's what you can do.

00:35:00.030 --> 00:35:01.890
So what's the picture look like?

00:35:02.000 --> 00:35:05.200
Okay, so on one side again we have GL,
which is doing some

00:35:05.200 --> 00:35:08.500
rendering on a shared object,
and then on the other side we have CL.

00:35:08.610 --> 00:35:11.040
And what you want to do is you want
to call render on the GL side and

00:35:11.040 --> 00:35:12.730
then acquire compute release on CL.

00:35:12.790 --> 00:35:14.840
And remember we said we
need this flush here.

00:35:14.920 --> 00:35:15.840
So what are we going to get?

00:35:15.840 --> 00:35:16.800
We're going to get something like this.

00:35:16.920 --> 00:35:19.640
The flush happens, and then the acquire
compute release happens,

00:35:19.640 --> 00:35:23.500
and our post-processing
works out fine in CL.

00:35:23.590 --> 00:35:26.150
But note that if you're doing
this over and over again,

00:35:26.160 --> 00:35:28.880
if you're doing render flush,
render flush, maybe for multiple frames,

00:35:29.020 --> 00:35:31.620
you're going to be doing a little
bit of GL work in between each flush.

00:35:31.720 --> 00:35:34.000
And if you're following our
best practices for OpenGL,

00:35:34.000 --> 00:35:35.300
you know that really that's not great.

00:35:35.300 --> 00:35:37.560
You should queue up a lot of render work,
right?

00:35:37.600 --> 00:35:39.490
Now these other renders
that have just dropped in,

00:35:39.580 --> 00:35:41.260
these aren't touching
that shared object notice.

00:35:41.330 --> 00:35:44.470
They're other GL work
unrelated to the shared object.

00:35:44.720 --> 00:35:47.220
So what you want is you still
want this one render to happen

00:35:47.220 --> 00:35:50.900
before your CL compute to happen,
but that's not what is going to happen.

00:35:51.050 --> 00:35:53.580
What's going to happen is everything
up to the flush is going to,

00:35:53.710 --> 00:35:57.470
you know, complete before they acquire
the compute and the release.

00:35:57.570 --> 00:35:58.920
So that's bad.

00:35:58.920 --> 00:36:00.980
You're going to get the correct result,
right?

00:36:01.000 --> 00:36:02.460
But the problem here is performance.

00:36:02.560 --> 00:36:04.960
The CL side is now
waiting on GL to finish.

00:36:04.990 --> 00:36:08.270
And so you're not taking full
advantage of your CL queue.

00:36:08.370 --> 00:36:11.100
So what you can do is you can
insert a GL sync object into your

00:36:11.100 --> 00:36:12.850
command stream on the GL side.

00:36:13.010 --> 00:36:18.760
So then what happens is you use CL API to
create a CL event from that sync object,

00:36:18.800 --> 00:36:22.390
and then you wait on the acquire
for that event to complete.

00:36:23.550 --> 00:36:26.810
So what happens is that sync
gets processed on the GL side,

00:36:26.810 --> 00:36:29.430
and this event is triggered,
and your acquire and compute release

00:36:29.470 --> 00:36:32.060
can proceed right from there before
waiting on all that other work.

00:36:32.100 --> 00:36:34.370
So that's exactly what we want.

00:36:35.100 --> 00:36:36.890
So what does this look like in code?

00:36:36.990 --> 00:36:38.550
Okay, again, here's our situation.

00:36:38.600 --> 00:36:42.100
We have OpenGL that wants to render
the geometry and make a picture,

00:36:42.150 --> 00:36:44.820
and then we want to
post-process the picture.

00:36:44.880 --> 00:36:46.410
So the code is pretty straightforward.

00:36:46.520 --> 00:36:49.710
We just have some function which draws
all of our stuff to this shared FBO.

00:36:49.880 --> 00:36:50.880
And then this is what we need to do.

00:36:50.880 --> 00:36:53.920
We need to create a GLSync
object using GLFenceSync.

00:36:54.040 --> 00:36:56.470
Okay, and that inserts this fence
into our command stream.

00:36:56.540 --> 00:37:00.800
We do all this unrelated work,
and then we eventually call it GLFlush.

00:37:00.860 --> 00:37:03.500
So the trick on the CL side
is use the special function

00:37:03.500 --> 00:37:07.580
CLCreateEvent from GLSyncKHR,
kind of a mouthful.

00:37:07.830 --> 00:37:09.940
And you give it the sync object,
it gives you back an event.

00:37:10.090 --> 00:37:13.160
And then you can pass that event
in the wait list of any of the

00:37:13.160 --> 00:37:14.570
CL functions that take a wait list.

00:37:14.590 --> 00:37:17.090
In this case,
it's the NQ acquired GL objects.

00:37:17.180 --> 00:37:20.500
And that makes that CL call
block until that event is done.

00:37:22.360 --> 00:37:24.730
So I'm happy to say that you heard
Abe talk about in the first half

00:37:24.740 --> 00:37:27.300
of the session how we've added
integration with Grand Central Dispatch.

00:37:27.300 --> 00:37:30.130
Well, the CL/GL stuff,
it plays very nicely with that.

00:37:30.130 --> 00:37:30.630
It's fine.

00:37:30.740 --> 00:37:34.340
But one thing you have to remember to do
is before you create your dispatch queue,

00:37:34.380 --> 00:37:35.650
you have to remember
to set the share group,

00:37:35.770 --> 00:37:37.050
first thing you need to do.

00:37:37.100 --> 00:37:39.620
So you get your share group
in the same way as before,

00:37:39.710 --> 00:37:41.770
okay, and then you pass it to
the special function,

00:37:41.770 --> 00:37:44.420
GCL/GL, set share group.

00:37:44.610 --> 00:37:46.800
As long as you do that,
everything else will work fine.

00:37:46.830 --> 00:37:52.250
And then we provide special entry points
for creating CL objects from GL objects.

00:37:52.910 --> 00:37:54.500
And in addition,
if you use OpenCL in this way,

00:37:54.540 --> 00:37:56.840
you don't have to worry about
the acquire and the release.

00:37:57.000 --> 00:37:59.360
We're going to take care of that for you.

00:38:00.190 --> 00:38:01.910
So I have a demo for you.

00:38:02.090 --> 00:38:06.430
So those of you who are old
hats at WWDC have probably

00:38:06.430 --> 00:38:07.670
seen a Blue Pony demo before.

00:38:07.930 --> 00:38:10.850
And you know that
Blue Pony is this lonely,

00:38:10.850 --> 00:38:11.810
solitary guy.

00:38:12.300 --> 00:38:14.390
He was always walking around by himself,
you know.

00:38:14.460 --> 00:38:16.480
So we thought this year we would
do a couple things for him.

00:38:16.480 --> 00:38:19.600
So the first thing we did is we gave
him some nice terrain to play around in.

00:38:19.600 --> 00:38:23.250
So what we're doing is we're
generating this terrain in OpenCL into

00:38:23.260 --> 00:38:27.070
a VBO directly on the card,
and then we're rendering

00:38:27.360 --> 00:38:28.780
that VBO in OpenGL.

00:38:28.990 --> 00:38:30.590
So then we also thought, well,
that's great,

00:38:30.590 --> 00:38:33.420
but Blue Pony is still extremely lonely
and just walking around by himself.

00:38:33.510 --> 00:38:35.540
We decided to give him some friends.

00:38:35.640 --> 00:38:38.060
So now Blue Pony is no longer lonely.

00:38:38.110 --> 00:38:41.730
Okay, so one other thing we decided to do
is add some fun into their lives.

00:38:41.740 --> 00:38:44.140
So we're going to add this
nice depth of field effect.

00:38:44.240 --> 00:38:48.640
So what we're doing in GL is we're
rendering to a texture through an FBO.

00:38:48.690 --> 00:38:50.500
And so we're going to do
some post-processing on

00:38:50.500 --> 00:38:51.640
that texture in OpenCL.

00:38:51.640 --> 00:38:54.170
So I'm going to command
the ponies to line up here.

00:38:54.250 --> 00:38:56.720
So our ponies are going to obey us,
as they should.

00:38:56.770 --> 00:38:58.560
Okay, so now the ponies are
all going to line up,

00:38:58.670 --> 00:39:00.080
and we're going to turn on
our depth of field effect.

00:39:00.170 --> 00:39:02.230
So you can see that this
pony in the foreground,

00:39:02.320 --> 00:39:05.720
the gray pony, he's in focus where these
guys in the back are blurred.

00:39:05.750 --> 00:39:07.960
And we can move the depth planes.

00:39:07.960 --> 00:39:11.450
So now I've moved it so the purple
pony and the yellow pony are in focus.

00:39:11.530 --> 00:39:13.090
And now there's that
beige guy in the center.

00:39:13.100 --> 00:39:15.840
And then our buddy Blue Pony,
he's in focus now.

00:39:15.940 --> 00:39:17.590
And then this guy in the back.

00:39:17.670 --> 00:39:19.150
Forward, back, forward, back.

00:39:19.210 --> 00:39:22.530
So we can really mess with the ponies.

00:39:22.730 --> 00:39:25.340
Anyway, we've shown how it's really
easy just to sort of share

00:39:25.340 --> 00:39:27.460
data between these two APIs,
and we can improve

00:39:27.460 --> 00:39:28.940
ponies' lives by doing so.

00:39:28.940 --> 00:39:31.270
So that's a good thing.

00:39:32.380 --> 00:39:34.110
So I want to talk about another
type of sharing that you

00:39:34.110 --> 00:39:36.750
might not be familiar with,
which is sharing iOS

00:39:36.750 --> 00:39:38.690
services with OpenCL.

00:39:39.800 --> 00:39:43.460
So IOSurface, that's a technology we
introduced in Snow Leopard,

00:39:43.460 --> 00:39:43.960
right?

00:39:44.080 --> 00:39:47.700
And it's an abstraction
for shared image data.

00:39:47.720 --> 00:39:49.440
Okay, so great, but what does that mean?

00:39:49.440 --> 00:39:51.540
Well,
I talked about two APIs in the system,

00:39:51.540 --> 00:39:53.940
OpenCL and OpenGL,
which use image-like data.

00:39:53.940 --> 00:39:56.920
But we have other APIs in the system,
like Core Video, for example,

00:39:56.920 --> 00:39:58.380
which uses image-type data.

00:39:58.840 --> 00:40:02.030
So wouldn't it be nice if there was
one way of packaging up images so that

00:40:02.030 --> 00:40:05.290
we can pass them around between APIs,
keeping in mind that these APIs might

00:40:05.290 --> 00:40:09.080
use different architectures like the GPU,
which have their own memory spaces?

00:40:09.080 --> 00:40:10.940
And that's exactly what IOSurface is.

00:40:10.940 --> 00:40:13.950
It's a way of packaging your image
data so it can transcend APIs,

00:40:13.950 --> 00:40:16.810
architectures, address spaces,
and even processes.

00:40:18.400 --> 00:40:19.650
So what's an example use case?

00:40:19.820 --> 00:40:22.700
Okay, you're writing a video
processing application.

00:40:22.730 --> 00:40:24.880
Okay,
you're gonna process some video frames.

00:40:24.930 --> 00:40:27.190
And you're writing your effects
using certain processing modules

00:40:27.200 --> 00:40:29.420
that you might write some now,
you might write some later,

00:40:29.450 --> 00:40:32.110
maybe using GL, CL, GPU, you don't know.

00:40:32.120 --> 00:40:33.670
You don't wanna have to worry about it.

00:40:33.770 --> 00:40:36.690
So instead of passing your image
data around in some special way,

00:40:36.790 --> 00:40:38.280
you can use an I/O surface.

00:40:38.360 --> 00:40:41.020
You just pass an I/O surface to
one of these processing modules.

00:40:41.060 --> 00:40:43.180
It does what it needs to do,
and then the change to that

00:40:43.180 --> 00:40:46.100
I/O surface is reflected in your
application's point of view.

00:40:46.240 --> 00:40:48.200
And you don't have to set
up any special context,

00:40:48.200 --> 00:40:49.200
anything like that.

00:40:49.200 --> 00:40:49.360
It just works.

00:40:49.360 --> 00:40:51.330
Just wrap it up in an iOS
Surface and you can do that.

00:40:53.080 --> 00:40:54.830
So another use case
is multiple processes.

00:40:54.860 --> 00:40:58.540
Again, you have some application which
is going to process video frames.

00:40:58.580 --> 00:41:01.570
And maybe out on your box you
have some nice render server that

00:41:01.580 --> 00:41:02.500
does all the effects for you.

00:41:02.500 --> 00:41:04.420
And again,
it's using a variety of different things.

00:41:04.490 --> 00:41:08.870
Maybe it's running in 64-bit mode,
and your application's in 32-bit mode.

00:41:09.580 --> 00:41:14.240
You can open a connection to this
process using a Mach port and just

00:41:14.310 --> 00:41:17.830
chuck your iOS surface right over
that Mach port to that other process.

00:41:18.090 --> 00:41:20.220
It does what it needs to do,
and then again,

00:41:20.220 --> 00:41:23.640
that change is reflected in
your process's point of view.

00:41:24.250 --> 00:41:25.160
Very nice.

00:41:25.260 --> 00:41:27.200
So the simplicity is one
reason to use IOSurface.

00:41:27.200 --> 00:41:29.500
It's just easy to get your data
around between different APIs,

00:41:29.570 --> 00:41:30.760
but it's also efficient.

00:41:30.940 --> 00:41:33.050
So in this case,
if the application and the render

00:41:33.050 --> 00:41:37.840
server are using the same device,
as you bounce back and forth

00:41:37.900 --> 00:41:40.660
between these two processes
doing work to this image,

00:41:40.730 --> 00:41:43.660
it doesn't need to be copied,
so of course we're not going to copy it.

00:41:43.700 --> 00:41:45.890
IOSurface just knows.

00:41:47.060 --> 00:41:48.670
So how do you integrate this with CL?

00:41:48.780 --> 00:41:51.690
Well, you create CL images that
are backed by I/O surfaces,

00:41:51.810 --> 00:41:53.640
and you use a special API to do that.

00:41:53.700 --> 00:41:55.760
So here we have an I/O surface
that somebody's given us

00:41:55.800 --> 00:41:57.530
or we've created ourselves.

00:41:57.590 --> 00:42:00.240
We can interrogate that to
get the width and the height,

00:42:00.240 --> 00:42:04.250
and then we just call a special function,
CLCreateImage from I/O Surface 2D.

00:42:04.320 --> 00:42:06.600
All right, and we pass it the width,
the height, the surface,

00:42:06.790 --> 00:42:08.930
and this image format parameter.

00:42:09.090 --> 00:42:11.060
Okay, so what about that image format?

00:42:11.110 --> 00:42:13.520
Well, I/O surfaces don't have
a native image format.

00:42:13.570 --> 00:42:14.920
They're just a bucket of bits.

00:42:14.950 --> 00:42:18.340
So you can interpret them however
is most useful for your application,

00:42:18.450 --> 00:42:19.640
however you need to.

00:42:19.800 --> 00:42:21.960
Okay, so whatever image
format you want to pass,

00:42:22.000 --> 00:42:23.600
basically we're going to believe you.

00:42:23.770 --> 00:42:27.020
We're ready to believe you as long
as your width and height matches up,

00:42:27.250 --> 00:42:28.860
all right, and times the pixel width.

00:42:28.860 --> 00:42:30.600
So as long as the size matches up.

00:42:30.620 --> 00:42:33.120
You can interpret this I/O surface
in multiple ways even in the

00:42:33.150 --> 00:42:35.430
same CL program if you want to.

00:42:36.950 --> 00:42:38.260
So what do you actually
have to do to use these?

00:42:38.260 --> 00:42:41.750
Well, let's say you want to modify this
IOSurface in your application on the

00:42:41.800 --> 00:42:44.050
CPU before you do some work in CL.

00:42:44.060 --> 00:42:45.180
What do you need to do?

00:42:45.180 --> 00:42:47.740
Well, you need to lock it first
and then do something.

00:42:47.740 --> 00:42:49.410
In this case,
we're going to throw some dog

00:42:49.410 --> 00:42:52.490
pixels into the image and then
unlock it when you're done.

00:42:52.500 --> 00:42:54.480
And then you're free to go
ahead and use that in OpenCL,

00:42:54.570 --> 00:42:56.740
perhaps modify it in OpenCL.

00:42:56.740 --> 00:42:59.370
So that brings up the question, OK, wait,
I'm going to modify in CL.

00:42:59.400 --> 00:43:01.660
Do I need to do a lock and unlock?

00:43:01.660 --> 00:43:05.120
No, you do not need to do that
because OpenCL is asynchronous.

00:43:05.380 --> 00:43:07.150
We're going to do this lock
at the time and the place

00:43:07.220 --> 00:43:08.740
where it's most appropriate.

00:43:08.740 --> 00:43:10.170
OK, so you don't need to
worry about doing that.

00:43:10.180 --> 00:43:11.970
We'll take care of that.

00:43:13.770 --> 00:43:16.670
So what do you need to do if
you're going to modify this in CL?

00:43:16.760 --> 00:43:18.380
What is your responsibility?

00:43:18.430 --> 00:43:19.060
Not much.

00:43:19.070 --> 00:43:22.300
Again, you just kind of treat it as any
image memory object in OpenCL.

00:43:22.370 --> 00:43:23.800
Do whatever you need to do, kernels,
whatnot.

00:43:23.800 --> 00:43:25.390
Okay, then you call CL flush.

00:43:25.390 --> 00:43:27.160
Just make sure you call CL flush.

00:43:27.160 --> 00:43:30.870
And then back on the application side,
before you access that data,

00:43:30.870 --> 00:43:33.400
even if you're only reading,
make sure you call IOSurface

00:43:33.400 --> 00:43:35.100
lock and unlock when you're done.

00:43:35.100 --> 00:43:38.290
And you can see from the second
parameter here to IOSurface lock,

00:43:38.350 --> 00:43:40.100
we're doing a read-only lock.

00:43:40.100 --> 00:43:41.030
You still need to do that.

00:43:41.100 --> 00:43:43.980
That's IOSurface's chance to say, okay,
what else in the system has

00:43:43.980 --> 00:43:45.100
touched this shared memory?

00:43:45.100 --> 00:43:47.590
If I need to move it around or
copy or make sure it's clean,

00:43:47.590 --> 00:43:49.090
that's my chance to do that.

00:43:49.090 --> 00:43:52.090
So just make sure you do your
lock and you're all right.

00:43:52.350 --> 00:43:54.650
One other feature that's new in
Lion that we've added is support

00:43:54.650 --> 00:43:56.840
for YUV I/O surfaces in OpenCL.

00:43:56.840 --> 00:43:59.430
So if you guys are video
processing people out there,

00:43:59.510 --> 00:44:02.460
and you're using YUV format images,
you're gonna be pretty happy with this.

00:44:02.530 --> 00:44:06.450
So you can create an OpenCL image
from a YUV I/O surface just using

00:44:06.450 --> 00:44:08.420
the mechanism I just described.

00:44:08.450 --> 00:44:09.850
And we have this new
image format for you.

00:44:09.860 --> 00:44:12.820
There's two different channel orders
depending on how your data's packed.

00:44:12.990 --> 00:44:14.620
And then we support three data types.

00:44:14.850 --> 00:44:17.210
And then within your CL kernel,
you use these as you would any

00:44:17.210 --> 00:44:20.510
other image object using the
OpenCL built-ins to access your data.

00:44:20.630 --> 00:44:22.680
You have read image, write image,
you're going to get back or

00:44:22.680 --> 00:44:25.830
write the YUV data for one pixel.

00:44:27.480 --> 00:44:29.820
So that's I/O Surfaces in OpenCL.

00:44:29.900 --> 00:44:32.000
So that brings us to our
last topic of the session,

00:44:32.050 --> 00:44:35.970
which is another new feature in Lion,
which is the auto-vectorizer.

00:44:36.530 --> 00:44:39.120
Okay,
so to sort of set the stage for this,

00:44:39.200 --> 00:44:42.620
let's talk about parallelism
in CPUs versus GPUs.

00:44:42.810 --> 00:44:43.690
Okay?

00:44:44.510 --> 00:44:48.570
OpenCL sees all of these devices as
having a number of compute cores with

00:44:48.570 --> 00:44:51.020
in them a number of processing elements.

00:44:51.240 --> 00:44:55.040
So what I've drawn on the slide
is we have a CPU with four cores,

00:44:55.070 --> 00:44:57.700
and that CPU has one processing
element in each core.

00:44:57.800 --> 00:45:00.160
Okay, but this is a big,
beefy processing element.

00:45:00.160 --> 00:45:03.550
It's got a nice vector unit on it,
out-of-order execution, super scalar,

00:45:03.550 --> 00:45:03.960
right?

00:45:03.960 --> 00:45:05.950
But on the other side, I've drawn a GPU.

00:45:05.960 --> 00:45:09.310
It has eight cores, and it has 16 scalar
processors in each core.

00:45:09.400 --> 00:45:11.360
So these are, like, you know, in a sense,
wimpier,

00:45:11.360 --> 00:45:14.520
not as tough as this nice CPU core,
but there's a lot more of them.

00:45:14.520 --> 00:45:16.910
So what does that mean
for us as a programmer?

00:45:16.910 --> 00:45:18.380
All right,
let's say you write this kernel,

00:45:18.380 --> 00:45:20.190
and this is the same kernel
that Abe showed earlier,

00:45:20.190 --> 00:45:22.180
this, you know, add arrays kernel.

00:45:22.200 --> 00:45:25.270
Now, notice we're passing into this
kernel floating point arrays,

00:45:25.460 --> 00:45:26.460
floats, single floats.

00:45:26.460 --> 00:45:29.980
So when we do this addition,
it's a scalar addition operating

00:45:29.980 --> 00:45:31.720
on one data element at a time.

00:45:31.720 --> 00:45:32.620
Okay, well, great.

00:45:32.620 --> 00:45:33.500
What does that mean?

00:45:33.520 --> 00:45:35.510
All right,
you send it to the CPU and the GPU.

00:45:35.510 --> 00:45:38.140
Now, notice the GPU lights up completely.

00:45:38.160 --> 00:45:40.420
All of these little scalar processors
are going to be really busy.

00:45:40.520 --> 00:45:43.880
But the CPU, all the cores are busy,
but that processing element,

00:45:43.880 --> 00:45:46.400
we're only using one
quarter of the vector width.

00:45:46.520 --> 00:45:49.370
If you were to look at Activity Monitor,
he would tell you, "Oh, yeah,

00:45:49.370 --> 00:45:50.520
I'm completely busy.

00:45:50.520 --> 00:45:52.300
All the cores are running."
It's not really true,

00:45:52.350 --> 00:45:54.520
because you're only using a
quarter of your vector width.

00:45:54.520 --> 00:45:55.520
So what can you do?

00:45:55.520 --> 00:45:58.520
Well, you can write a specialized kernel,
all right?

00:45:58.600 --> 00:46:00.520
So what's different about
the specialized kernel?

00:46:00.550 --> 00:46:02.520
Well, now I'm passing in float fours.

00:46:02.520 --> 00:46:03.040
I've vectorized my kernel.

00:46:03.100 --> 00:46:06.080
And that means that addition in the
kernel is going to be a vector addition.

00:46:06.080 --> 00:46:09.930
Okay, so I send that to CPU only,
specialized for that device.

00:46:10.080 --> 00:46:12.080
Oh, now, look,
my vector unit's all lit up,

00:46:12.080 --> 00:46:15.530
and I've extracted all the
firepower from my machine.

00:46:15.710 --> 00:46:18.780
Okay, so now I know how to get all the
performance out of both devices,

00:46:18.880 --> 00:46:20.760
but that leads us to
this terrible conclusion,

00:46:20.760 --> 00:46:22.260
which is that we have to
write multiple kernels.

00:46:22.280 --> 00:46:23.410
This is no good.

00:46:23.430 --> 00:46:26.040
You would tell me,
I don't want to write multiple kernels.

00:46:26.040 --> 00:46:28.100
I should write one CL kernel
and get good performance.

00:46:28.100 --> 00:46:29.800
That's what OpenCL is
supposed to do for me.

00:46:29.800 --> 00:46:31.260
Give me good performance on all devices.

00:46:31.260 --> 00:46:34.170
And you would rightfully say,
it's easier to write a scalar kernel.

00:46:34.170 --> 00:46:36.970
Any of you guys who have done vector
programming know that there are

00:46:36.970 --> 00:46:38.520
weird edge cases to watch out for.

00:46:38.520 --> 00:46:40.690
It gets just a lot harder
than writing scalar code.

00:46:41.700 --> 00:46:42.530
And this should just work.

00:46:42.530 --> 00:46:43.860
You should just take care of this for us.

00:46:43.860 --> 00:46:47.170
So we agree with you,
and to tell you about how he did that,

00:46:47.190 --> 00:46:49.630
I'd like to invite to the stage
Sion Berkowits from Intel to

00:46:49.630 --> 00:46:51.210
talk about the auto-vectorizer.

00:46:52.750 --> 00:46:57.710
So I'll start with, as Jim said,
a little bit about what it really takes

00:46:57.940 --> 00:47:01.300
to write kernels for the CPU these days.

00:47:01.440 --> 00:47:05.840
So you start with a kernel,
which is a straightforward implementation

00:47:05.840 --> 00:47:09.590
of your algorithm or your problem domain.

00:47:09.680 --> 00:47:12.760
And then when you want
to run it on the CPU,

00:47:12.830 --> 00:47:14.810
you see that it's underperforming.

00:47:14.990 --> 00:47:17.560
So you need to add some
optimizations to it.

00:47:17.560 --> 00:47:22.020
And you really start that by adding
a loop over the kernel code to

00:47:22.020 --> 00:47:25.610
execute several work items together.

00:47:25.670 --> 00:47:30.640
And this gives you some performance,
but again, this still underutilizes

00:47:30.640 --> 00:47:33.010
the vector or SIMD units.

00:47:33.090 --> 00:47:38.090
So you're going to optimize again
and add execution of multiple

00:47:38.170 --> 00:47:45.530
parallel work items to utilize
the different lanes in the vector.

00:47:45.590 --> 00:47:50.070
So you get code,
which is somewhat far from

00:47:50.070 --> 00:47:53.700
your original algorithm,
is harder to implement and maintain,

00:47:53.700 --> 00:47:57.180
and is less likely to be
scalable among devices.

00:47:57.260 --> 00:48:01.180
So what you'd really like is for all of
this to be done automatically for you.

00:48:01.370 --> 00:48:04.460
And this is where the
Intel AutoVectorizer comes to help.

00:48:04.460 --> 00:48:07.920
The AutoVectorizer is a
compiler optimization,

00:48:07.920 --> 00:48:12.180
which is run by default whenever you
compile OpenCL kernels for the CPU.

00:48:12.400 --> 00:48:20.260
What it does is it packs together several
work items into the vector instructions,

00:48:20.280 --> 00:48:23.660
and it also generates a loop
over the entire work group,

00:48:23.680 --> 00:48:27.300
which allows using more
complex optimizations,

00:48:27.300 --> 00:48:31.000
such as the invariant code
motion and strength reduction,

00:48:31.030 --> 00:48:35.260
and also gives you the added
benefit that the work group size,

00:48:35.260 --> 00:48:40.720
the maximum work group size running
on the CPU grows from one to 128.

00:48:40.750 --> 00:48:43.590
All of this allows you to achieve
a speed up of up to 4x without

00:48:43.590 --> 00:48:45.270
any additional developer effort.

00:48:45.960 --> 00:48:48.810
So I'll show you a short,
simple example of how

00:48:48.810 --> 00:48:50.340
the vectorizer works.

00:48:50.450 --> 00:48:54.300
You have a kernel code,
which represents a single work item,

00:48:54.390 --> 00:48:59.620
and you have many such work items going
to run in parallel in your application.

00:48:59.730 --> 00:49:01.770
So let's visualize this a little better.

00:49:01.880 --> 00:49:06.350
So basically, the kernel code is a
bunch of instructions,

00:49:06.430 --> 00:49:09.520
some of them scalar,
some of them maybe vector instructions.

00:49:09.610 --> 00:49:13.110
So the first thing that the
vectorizer does is it scalarizes

00:49:13.110 --> 00:49:15.160
all of the vector instruction.

00:49:15.970 --> 00:49:19.630
Now, once this is done,
the vectorizer continues by

00:49:19.750 --> 00:49:23.270
packing several of these work
items into the vector data types,

00:49:23.310 --> 00:49:27.090
each lane representing
a different work item.

00:49:27.810 --> 00:49:30.830
As an added benefit of this,
the amount of invocations that

00:49:30.830 --> 00:49:32.420
actually happens is reduced.

00:49:32.500 --> 00:49:37.190
And once this is done,
the vectorizer adds a loop over the

00:49:37.380 --> 00:49:40.750
kernel to execute the entire workgroup.

00:49:41.670 --> 00:49:46.860
Now, a little bit about the do's and
don'ts of writing optimal code

00:49:46.860 --> 00:49:48.400
for the CPU with this vectorizer.

00:49:48.400 --> 00:49:51.010
So what you should do is not much.

00:49:51.180 --> 00:49:52.000
Keep it simple.

00:49:52.070 --> 00:49:55.200
Write your code as best fits the
algorithm of your problem's domain.

00:49:56.520 --> 00:50:01.460
Don't try to add optimizations
that are device or target

00:50:01.460 --> 00:50:03.460
specific into your kernel.

00:50:03.500 --> 00:50:06.480
Also, avoid as much as possible
using control flow,

00:50:06.550 --> 00:50:12.700
which is work item ID dependent,
as this is kind of a vectorizer killer.

00:50:12.770 --> 00:50:16.260
and let the auto-vectorizer do
the optimization work for you,

00:50:16.260 --> 00:50:17.880
basically.

00:50:18.540 --> 00:50:21.220
So I'll show a demo of
the auto-vectorizer.

00:50:21.370 --> 00:50:28.840
So this is an example of a popular
scenario where you use OpenCL filters,

00:50:28.910 --> 00:50:38.020
filters written in OpenCL to
process like a movie.

00:50:38.660 --> 00:50:43.390
Now, since the processor is

00:50:44.080 --> 00:50:49.000
- Okay, okay,
since the process is relatively strong,

00:50:49.310 --> 00:50:52.370
we're going to do many
of them in parallel.

00:50:52.530 --> 00:50:54.640
So I'm going to start them.

00:50:54.660 --> 00:51:00.140
And as you can see on the top right,
the frames per second goes down,

00:51:00.160 --> 00:51:04.890
the more filters I process
at the same time.

00:51:05.140 --> 00:51:09.990
And right now the movie has slowed
down to practically a crawling pace.

00:51:11.820 --> 00:51:16.720
So all I have to do is I flip
on the vectorizer switch,

00:51:16.720 --> 00:51:21.590
and here I get a 4x speedup more or less,
and the movie is again

00:51:21.590 --> 00:51:23.210
running at a very good speed.

00:51:23.280 --> 00:51:31.420
So I'll turn back to James.

00:51:37.680 --> 00:51:40.380
So, let's wrap up the session.

00:51:40.590 --> 00:51:42.840
What did we talk about today?

00:51:43.300 --> 00:51:45.530
Okay,
so Abe talked to you about how using

00:51:45.530 --> 00:51:48.540
OpenCL online is really easier than ever.

00:51:48.620 --> 00:51:51.670
This new integration with Xcode that
we offer and the ability to send

00:51:51.750 --> 00:51:55.340
kernels to Grand Central Dispatch
dispatch queues is going to get you

00:51:55.340 --> 00:51:58.300
up and running with CL faster than
ever before if you're new to CL.

00:51:58.310 --> 00:52:01.400
And it's just an easier
way to use OpenCL online.

00:52:01.430 --> 00:52:03.100
Sion just told you about
the auto-vectorizer,

00:52:03.100 --> 00:52:06.070
which is great news that you can now
just write scalar code and get the

00:52:06.070 --> 00:52:08.300
best possible performance on your CPU.

00:52:08.300 --> 00:52:11.670
And he showed you video processing,
which is typically an application

00:52:11.690 --> 00:52:14.060
which runs in the domain of GPUs,
but we get really good

00:52:14.080 --> 00:52:15.300
performance on the CPU as well.

00:52:15.300 --> 00:52:17.300
So that's great.

00:52:17.330 --> 00:52:19.300
Abe also talked to you
about the offline compiler,

00:52:19.300 --> 00:52:22.240
so if you don't want to ship your human
readable source with your program now,

00:52:22.320 --> 00:52:23.300
you don't have to.

00:52:23.300 --> 00:52:25.600
So for those of you who
are asking for that,

00:52:25.800 --> 00:52:27.390
you know, you're welcome.

00:52:27.540 --> 00:52:30.420
I talked to you about easy and
efficient sharing that you can

00:52:30.420 --> 00:52:34.410
do between OpenCL and OpenGL,
and also about how to share data

00:52:34.410 --> 00:52:36.500
using IOSurface across APIs.

00:52:36.500 --> 00:52:39.780
So we really encourage you to check
that out if you haven't before.

00:52:40.830 --> 00:52:42.630
So for more information
about anything you saw,

00:52:42.630 --> 00:52:43.800
you should contact Alan Schafer.

00:52:43.800 --> 00:52:46.400
He's our graphics and game
technologies evangelist,

00:52:46.400 --> 00:52:48.440
and his email's up there on the slide.

00:52:48.500 --> 00:52:51.290
Or you can go to the
Apple Developer Forums.

00:52:52.700 --> 00:52:55.490
So if you found this session interesting,
there's another GL session

00:52:55.490 --> 00:52:56.930
which you might like a lot.

00:52:56.940 --> 00:52:59.580
Okay, that's going to be Thursday,
10:15 in Mission,

00:52:59.630 --> 00:53:01.400
which is the room right behind us.

00:53:01.490 --> 00:53:03.770
And that's called "Advances
in OpenGL for MacOSX Lion."

00:53:03.860 --> 00:53:06.710
So I encourage you to check that out.

00:53:06.810 --> 00:53:09.620
And with that,
I'd like to thank you for coming today.