WEBVTT

00:00:08.900 --> 00:00:09.970
Good morning.

00:00:10.350 --> 00:00:11.600
Thank you all for coming.

00:00:11.600 --> 00:00:12.630
I'm Michael Hopkins.

00:00:12.690 --> 00:00:14.560
I'm with the Core Audio Engineering team.

00:00:14.560 --> 00:00:21.210
And today we'll be talking about
music on both iOS and Mac OS X.

00:00:22.070 --> 00:00:26.210
We'll be covering four main topics,
the first of which is how to use

00:00:26.210 --> 00:00:29.160
audio units to do audio processing.

00:00:29.560 --> 00:00:34.400
We'll talk about a specific audio unit,
the audio unit sampler.

00:00:34.450 --> 00:00:40.190
We'll be talking about how to
do MIDI on your iOS device.

00:00:40.680 --> 00:00:44.780
And finally,
we'll be playing back music sequences.

00:00:46.210 --> 00:00:50.120
Mac OS X as well as iOS provide
a wide range of facilities for

00:00:50.120 --> 00:00:52.200
developers making music applications.

00:00:52.200 --> 00:00:57.260
These can range from the simple
soundboard application to a more complex

00:00:57.420 --> 00:01:00.200
application that edits musical content.

00:01:00.260 --> 00:01:04.230
This hypothetical example that
you see here on the screen is one

00:01:04.270 --> 00:01:07.160
that's geared more at live playback.

00:01:07.160 --> 00:01:10.230
We have on your left-hand
side a file player,

00:01:10.360 --> 00:01:13.600
which is playing back
content stored on disk.

00:01:14.000 --> 00:01:18.360
We have two live inputs,
a guitar and a microphone.

00:01:18.510 --> 00:01:22.780
These inputs are each going
through an audio processing effect,

00:01:22.780 --> 00:01:26.110
kind of like a guitar
stomp box that you'd have.

00:01:26.120 --> 00:01:30.810
We also have a music device
that is playing a MIDI,

00:01:30.970 --> 00:01:36.220
triggering samples on an
electric piano sampler.

00:01:36.220 --> 00:01:41.900
Each of these inputs in turn
is going through a mixer,

00:01:41.900 --> 00:01:41.900
which is changing the relative
volumes of each track,

00:01:41.900 --> 00:01:41.900
doing something different.

00:01:41.900 --> 00:01:41.900
Mac OS X as well as iOS
and Mac OS X feature a

00:01:41.900 --> 00:01:41.900
state-of-the-art audio engine,
enabling a wide range of facilities for

00:01:41.900 --> 00:01:41.900
developers making music applications.

00:01:41.900 --> 00:01:41.900
These can range from the simple
soundboard application to a more

00:01:41.900 --> 00:01:41.900
complex application that edits
musical content stored on disk.

00:01:41.900 --> 00:01:41.900
Mac OS X as well as iOS provide
a wide range of facilities for

00:01:41.900 --> 00:01:41.900
developers making music applications.

00:01:41.900 --> 00:01:41.900
These can range from the
simple soundboard application

00:01:41.900 --> 00:01:41.900
to a more complex application,
doing something different.

00:01:41.900 --> 00:01:44.000
changing the relative
volumes of each track,

00:01:44.100 --> 00:01:46.440
doing some panning,
combining all that audio,

00:01:46.440 --> 00:01:48.880
and outputting it to the speaker.

00:01:50.500 --> 00:01:53.700
So now let's talk about how
to make such an application.

00:01:53.800 --> 00:01:57.740
It seems very daunting and complex,
but actually one of the fundamental

00:01:57.740 --> 00:02:03.980
building blocks for doing such processing
is available for you on Mac OS X and iOS,

00:02:04.020 --> 00:02:06.040
and that's the audio unit.

00:02:06.070 --> 00:02:10.420
This is a plug-in architecture
for processing audio.

00:02:10.740 --> 00:02:14.880
It is very low latency and
designed for real-time input,

00:02:15.020 --> 00:02:18.400
output, or simultaneous input and output.

00:02:18.710 --> 00:02:23.340
These audio units can be
organized in an audio unit graph,

00:02:23.340 --> 00:02:27.180
and they can be controlled
via properties and parameters,

00:02:27.180 --> 00:02:30.600
which do things such as set
the sample rate of the unit.

00:02:30.800 --> 00:02:35.310
And on the desktop, Mac OS X,
they can be controlled by the

00:02:35.320 --> 00:02:39.200
user with a user interface,
and we'll see that in a bit.

00:02:42.250 --> 00:02:48.140
There are several types of audio units
that ship as part of Mac OS X and iOS.

00:02:48.140 --> 00:02:52.500
These include effects units,
which do things such as apply

00:02:52.500 --> 00:02:53.860
a delay effect to audio.

00:02:53.860 --> 00:02:58.610
We have music effects, which are similar,
except that they also allow the

00:02:58.730 --> 00:03:02.640
input of MIDI data to control
how that audio is rendered.

00:03:03.240 --> 00:03:05.740
We have instruments,
such as the new sampler unit,

00:03:05.740 --> 00:03:07.740
which we'll be showing you a bit later.

00:03:07.740 --> 00:03:10.070
We have generators, which take no input.

00:03:10.170 --> 00:03:14.260
They simply generate an output signal,
such as a sine wave generator.

00:03:14.260 --> 00:03:18.400
We have panner units,
such as our 3D panners,

00:03:18.420 --> 00:03:22.940
which spatialize audio
into a 3D environment.

00:03:22.940 --> 00:03:26.270
We have converter units for
taking audio from one format

00:03:26.390 --> 00:03:28.440
and converting it to another.

00:03:28.440 --> 00:03:30.380
Mixers.

00:03:30.380 --> 00:03:33.220
Offline effects, which do play a role in
the sound of the audio.

00:03:33.220 --> 00:03:36.030
We have processing that can't
be accomplished in real time,

00:03:36.190 --> 00:03:40.730
such as an audio unit that takes
a file and reverses its contents,

00:03:40.860 --> 00:03:43.690
playing it backwards,
revealing satanic messages

00:03:43.750 --> 00:03:45.340
in our favorite '70s rock.

00:03:45.360 --> 00:03:49.040
We have output units.

00:03:50.170 --> 00:03:53.540
And now that we've talked about
these different types of units,

00:03:53.600 --> 00:03:57.040
I'd like to discuss
organizing them in a graph.

00:03:57.100 --> 00:04:02.400
We have something called an AU graph,
and these audio units can be added

00:04:02.400 --> 00:04:05.140
and organized in this structure.

00:04:06.400 --> 00:04:10.060
Each item, as shown in my example,
is an AU node,

00:04:10.060 --> 00:04:15.140
and in this example we have a
microphone input going to a reverb unit,

00:04:15.140 --> 00:04:19.340
and then the output from that
reverb is going to the output.

00:04:19.670 --> 00:04:23.830
This graph is defining the topology
and the connections between

00:04:23.830 --> 00:04:26.060
each audio unit in the graph.

00:04:27.300 --> 00:04:30.490
Finally,
I'd like to point out that the graph

00:04:30.600 --> 00:04:35.450
must end with a single output unit,
as shown here.

00:04:35.880 --> 00:04:38.670
These graphs can be
made much more complex.

00:04:38.980 --> 00:04:42.160
I've now modified the example
to show that we can additionally

00:04:42.160 --> 00:04:47.940
have a MIDI device that
is going to an instrument,

00:04:47.940 --> 00:04:52.330
and then the audio is passing through
two separate effects units before

00:04:52.330 --> 00:04:57.550
being combined by the mixer unit
and then output to the output unit.

00:04:59.890 --> 00:05:04.940
Now when we go back to our diagram of
our hypothetical audio application,

00:05:04.940 --> 00:05:10.550
we can see that this very complex
behavior is actually being managed by

00:05:10.650 --> 00:05:17.080
the host application by using an AU graph
with several specific audio units.

00:05:17.080 --> 00:05:20.870
For example, the file player unit is
handling file playback.

00:05:20.870 --> 00:05:26.620
A distortion unit is taking the input
from our guitar and modifying it.

00:05:26.620 --> 00:05:30.160
The reverb unit is being
connected to the microphone input.

00:05:30.190 --> 00:05:33.790
A sampler unit is what's playing
those MIDI-triggered samples.

00:05:33.790 --> 00:05:39.090
And all these are going through a
mixer unit before going to the output.

00:05:40.810 --> 00:05:43.200
Now that I've given a bit
of background on this,

00:05:43.200 --> 00:05:47.720
it's important to understand how we
can interact programmatically with

00:05:47.820 --> 00:05:49.600
these audio units on the system.

00:05:49.700 --> 00:05:51.550
There are two main ways.

00:05:51.690 --> 00:05:55.700
The first is properties,
and these are key value pairs.

00:05:55.700 --> 00:06:00.700
The properties allow a host
to configure specific states,

00:06:00.700 --> 00:06:04.690
such as sample rate or stream format.

00:06:05.150 --> 00:06:25.820
and Michael Kahn.

00:06:26.890 --> 00:06:26.980
These properties are set on the
audio unit and take effect when

00:06:26.980 --> 00:06:26.980
the audio unit is initialized.

00:06:26.980 --> 00:06:26.980
And then finally,
parameters are designed to change

00:06:26.980 --> 00:06:26.980
the way that that real-time
processing is accomplished.

00:06:26.980 --> 00:06:26.980
For example,
in the case of a delay effect,

00:06:26.980 --> 00:06:26.980
we may want to change in real-time
our delay time or feedback amount.

00:06:27.340 --> 00:06:31.000
Typically, parameters are set
through a user interface,

00:06:31.030 --> 00:06:34.040
as I'll show in my demo shortly.

00:06:35.590 --> 00:06:39.290
I'd now like to take a look at AU Lab,
which is our reference host

00:06:39.400 --> 00:06:42.600
application running on Mac OS X.

00:06:42.720 --> 00:06:46.940
And we're going to take a
look at an example document,

00:06:46.940 --> 00:06:51.330
which contains a graph that is
configured in the manner that

00:06:51.330 --> 00:06:54.650
we saw in the slides previously.

00:06:55.020 --> 00:06:57.200
We have a microphone track.

00:06:57.200 --> 00:06:59.800
That's input coming from the microphone.

00:06:59.820 --> 00:07:05.200
And we have some meters that allow us
to monitor the input coming through.

00:07:05.240 --> 00:07:08.070
Exactly like our diagram,
that's going through a

00:07:08.070 --> 00:07:10.240
matrix reverb effect.

00:07:10.270 --> 00:07:16.470
And then that track has a
panner and a volume control.

00:07:16.480 --> 00:07:18.240
This is actually in the mixer unit.

00:07:18.240 --> 00:07:21.880
And when we change the UI here,
we're actually changing how the

00:07:21.950 --> 00:07:26.130
mixer is panning and setting
the volume of that track.

00:07:26.560 --> 00:07:30.330
We have our guitar track
with a distortion effect,

00:07:30.340 --> 00:07:33.340
a sampler, and then the file player.

00:07:33.460 --> 00:07:40.080
The file player is
configured to play a file.

00:07:40.080 --> 00:07:43.320
And I'd like to audition
that for you now.

00:07:43.320 --> 00:07:44.940
Whoa.

00:07:44.940 --> 00:07:47.970
Wake all of you up, I hope.

00:07:52.570 --> 00:07:55.360
Okay, so that's the file that we're
playing back in our file player.

00:07:55.360 --> 00:07:58.690
Now I'm going to add some
effects to that track.

00:07:58.760 --> 00:08:01.790
And when I'm doing this,
I'm just adding additional

00:08:01.850 --> 00:08:05.440
nodes in that graph that this
host application is running.

00:08:05.440 --> 00:08:09.360
So I'm going to go ahead and I'm
going to choose a distortion effect.

00:08:09.420 --> 00:08:12.990
As you can see in this pop-up menu,
this allows me to pick

00:08:12.990 --> 00:08:14.500
both a type of effect.

00:08:14.500 --> 00:08:17.940
In this case,
we can have converters or panners.

00:08:17.940 --> 00:08:20.020
I'm going to choose an effect.

00:08:20.100 --> 00:08:22.480
These effects are
grouped by manufacturer.

00:08:22.560 --> 00:08:25.560
So that all of Apple,
for example's effects are

00:08:25.570 --> 00:08:28.540
grouped in the same location.

00:08:28.760 --> 00:08:29.580
I'm going to pick an effect.

00:08:29.580 --> 00:08:33.580
This is the distortion unit.

00:08:33.910 --> 00:08:37.520
And what you see here is a generic,
excuse me, is a custom view,

00:08:37.520 --> 00:08:43.440
which is a very elaborate user
interface that allows the end user

00:08:43.440 --> 00:08:45.250
to interact with that audio unit.

00:08:45.260 --> 00:08:51.320
And when I go ahead and change the delay,
for example, of the distortion unit,

00:08:51.910 --> 00:08:57.010
dragging this control changes the
parameters of that audio unit.

00:08:57.020 --> 00:09:00.610
So let me go ahead and pull up a
generic view of that same audio unit

00:09:00.610 --> 00:09:02.760
so you can see what's going on here.

00:09:07.330 --> 00:09:10.540
This AU distortion unit has
several groups of parameters.

00:09:10.540 --> 00:09:13.610
It has a delay group,
a ring modulation group,

00:09:13.700 --> 00:09:15.160
a decimation group, etc.

00:09:15.160 --> 00:09:18.840
And each of these groups
has related parameters,

00:09:18.840 --> 00:09:23.100
which are specified by
the name of the parameter.

00:09:23.540 --> 00:09:27.750
They have a minimum and a maximum value,
a current value,

00:09:27.750 --> 00:09:30.440
which is editable by this slider.

00:09:30.440 --> 00:09:34.640
You'll also notice that as I'm
changing that parameter value,

00:09:34.690 --> 00:09:38.630
that the custom view,
which is looking at the same audio unit,

00:09:38.640 --> 00:09:41.120
is showing that change in real time.

00:09:41.120 --> 00:09:44.720
And that also happens
the other way as well.

00:09:44.720 --> 00:09:48.670
You'll notice that with a custom view,
I'm able to control two different

00:09:48.790 --> 00:09:53.520
parameters simultaneously,
because this view allows me to do it.

00:09:53.540 --> 00:09:58.200
X and Y and not just operate
on a single parameter.

00:09:59.280 --> 00:10:03.700
So let's go ahead and hear the
effect of this audio unit now.

00:10:03.700 --> 00:10:05.900
Ooh, that sounds horrible.

00:10:05.900 --> 00:10:07.900
Bypass.

00:10:07.900 --> 00:10:09.480
And now applying the effect.

00:10:09.480 --> 00:10:12.760
You can really hear that
ring modulator working.

00:10:17.740 --> 00:10:22.540
Now, at any time,
I can add more audio units to this chain

00:10:22.540 --> 00:10:24.690
just by selecting another audio unit.

00:10:24.690 --> 00:10:27.610
And in this case,
I'm going to choose a filter.

00:10:28.150 --> 00:10:32.940
Now the audio is going from the
distortion effect through the filter,

00:10:32.970 --> 00:10:36.890
and using the custom view,
I can change the

00:10:37.600 --> 00:10:42.510
The audio, now I have a really,
I've really cut out the high

00:10:42.520 --> 00:10:45.400
frequencies of that audio file.

00:10:49.150 --> 00:10:53.660
So although these custom
views are available in AU Lab,

00:10:53.660 --> 00:10:56.430
we can use them in any
other host application,

00:10:56.510 --> 00:10:59.200
such as Logic or GarageBand.

00:10:59.260 --> 00:11:01.960
So that's an important distinction.

00:11:01.960 --> 00:11:03.220
All right.

00:11:03.220 --> 00:11:04.220
Go back to slides.

00:11:04.220 --> 00:11:09.350
That was AU Lab.

00:11:16.780 --> 00:11:20.370
With iOS version 5,
we've now taken a lot of these

00:11:20.370 --> 00:11:22.990
audio units that you've just seen
running on the desktop and made

00:11:23.310 --> 00:11:25.000
them available to developers.

00:11:25.000 --> 00:11:29.490
These include several new effects,
such as some filters,

00:11:29.490 --> 00:11:31.710
the high-pass and low-pass.

00:11:31.820 --> 00:11:33.740
We have a peak limiter.

00:11:33.740 --> 00:11:38.580
We have a dynamics processor that
allows you to do some compression.

00:11:38.620 --> 00:11:42.780
And a new reverb unit,
which is very high quality,

00:11:42.830 --> 00:11:46.410
so it's great to use in your application.

00:11:48.120 --> 00:11:51.560
We have additionally
some time pitch effects.

00:11:51.560 --> 00:11:55.810
The VeraSpeed, which does only pitch,
and then SimpleTime and

00:11:55.810 --> 00:11:58.830
NotQuiteSoSimpleTime,
which allow you to change

00:11:58.830 --> 00:12:00.640
time and pitch independently.

00:12:00.680 --> 00:12:07.420
The difference being that the
NotQuiteSoSimpleTime version

00:12:07.850 --> 00:12:08.600
is higher quality at the
expense of higher CPU usage.

00:12:08.780 --> 00:12:11.250
We have two new generators,
a file player,

00:12:11.250 --> 00:12:14.260
as well as a scheduled slice player,
which allows you to play

00:12:14.260 --> 00:12:17.280
back snippets of audio,
schedule them from any

00:12:17.280 --> 00:12:19.740
thread for rendering later.

00:12:19.810 --> 00:12:24.550
And then finally, we have an AU sampler,
which we'll talk about in

00:12:24.550 --> 00:12:26.420
greater detail in a bit.

00:12:29.020 --> 00:12:30.730
So that's how audio units are used.

00:12:30.920 --> 00:12:35.000
But how can you as a developer take
advantage of them in your application?

00:12:35.100 --> 00:12:37.180
Well, first of all,
you need to know how to find

00:12:37.180 --> 00:12:39.610
and load those audio units.

00:12:39.900 --> 00:12:43.490
When I talk about an audio unit,
I'm talking specifically about

00:12:43.580 --> 00:12:45.400
an instance of an audio unit.

00:12:45.430 --> 00:12:48.860
And that's created from something
called an audio component.

00:12:48.900 --> 00:12:52.610
The audio component is a factory,
which knows how to create

00:12:52.610 --> 00:12:54.880
those audio unit instances.

00:12:55.260 --> 00:12:59.660
Each component is uniquely identified
by an audio component description,

00:12:59.710 --> 00:13:04.940
which has three key fields: a type,
a subtype, and a manufacturer.

00:13:05.170 --> 00:13:07.040
The type is the type of effect.

00:13:07.190 --> 00:13:09.940
In this case, AUFX stands for effect.

00:13:10.120 --> 00:13:13.090
And then the subtype and
manufacturer are provided by

00:13:13.090 --> 00:13:15.760
the creator of that audio unit.

00:13:15.840 --> 00:13:19.350
There's also a name,
which the host maybe will use to display,

00:13:19.540 --> 00:13:21.400
and a version.

00:13:22.210 --> 00:13:26.730
So there's an audio component API,
which can be used to find

00:13:26.730 --> 00:13:30.380
the load and do management on
the state of that audio unit.

00:13:30.410 --> 00:13:33.920
It's available on both iOS and Mac OS X.

00:13:33.970 --> 00:13:37.080
And on the desktop,
it replaces the component manager,

00:13:37.080 --> 00:13:41.490
while maintaining backwards
compatibility with older components.

00:13:45.550 --> 00:13:47.660
Let's look at how that
works in an application.

00:13:47.910 --> 00:13:52.480
This is identical on both the desktop
as well as our mobile platforms.

00:13:52.740 --> 00:13:58.000
An application first needs to
find a specific audio unit,

00:13:58.000 --> 00:14:01.630
and it does that by creating
an audio component description.

00:14:01.890 --> 00:14:05.780
It passes that component description
to the audio component system by

00:14:05.780 --> 00:14:08.620
calling audio component find next.

00:14:08.620 --> 00:14:11.880
The audio component system then
looks through its list of audio

00:14:11.880 --> 00:14:15.860
components and returns the matching
component back to the application.

00:14:15.860 --> 00:14:20.260
The application can then create
an audio unit instance by calling

00:14:20.260 --> 00:14:23.480
audio component instance new,
passing that component

00:14:23.480 --> 00:14:26.000
along to the system,
which will then return

00:14:26.000 --> 00:14:28.180
back an audio unit.

00:14:29.390 --> 00:14:33.640
Let's look at more detail about
the audio component Find Next Call.

00:14:33.640 --> 00:14:39.180
And there are a couple of
additional pieces of information

00:14:39.210 --> 00:14:40.280
I'd like to provide here.

00:14:40.280 --> 00:14:45.220
The first is that the
first argument can be null,

00:14:45.230 --> 00:14:49.260
to specify that we want to start
looking in that list at the beginning.

00:14:49.260 --> 00:14:52.510
Or if we want to iterate
through several components,

00:14:52.510 --> 00:14:56.810
we can provide a specific component
after which search will begin.

00:14:56.810 --> 00:15:00.910
We have the facility
for wildcard searches,

00:15:00.910 --> 00:15:06.920
which would allow you to say,
find all instances of effects.

00:15:06.920 --> 00:15:10.890
In order to do that,
we would zero out all fields

00:15:10.890 --> 00:15:17.740
in our component descriptor,
except for the component type.

00:15:20.810 --> 00:15:24.160
Audio components are registered
on the system in two main ways,

00:15:24.160 --> 00:15:25.840
the first of which is statically.

00:15:25.840 --> 00:15:30.390
The system at boot time will look
through all the bundles in these

00:15:30.390 --> 00:15:34.930
three specific directory locations,
four bundles with a specific extension,

00:15:34.930 --> 00:15:37.560
and then those audio
units will be loaded.

00:15:37.560 --> 00:15:41.700
Components can also be
registered at runtime.

00:15:41.700 --> 00:15:46.330
When you use audio
component register to do so,

00:15:46.340 --> 00:15:49.870
that component becomes private
to your application process.

00:15:50.580 --> 00:15:52.240
So it's not a system-wide component.

00:15:52.240 --> 00:15:56.350
In order to make that call,
you provide a description for the

00:15:56.580 --> 00:16:00.290
component that you're registering,
a name, a version,

00:16:00.290 --> 00:16:02.320
and a factory function.

00:16:04.040 --> 00:16:15.350
and Michael Hopkins, Doug Scott,
and Michael Hopkins, Doug Scott,

00:16:15.350 --> 00:16:15.350
and Michael Hopkins,

00:16:16.030 --> 00:16:19.400
Looking into a bit more detail
about how the audio component

00:16:19.400 --> 00:16:22.640
system works under the hood,
as I showed earlier,

00:16:22.640 --> 00:16:25.880
all audio components have an
audio component factory function,

00:16:25.880 --> 00:16:28.780
and this is used to create
instances of that component.

00:16:28.780 --> 00:16:32.980
It returns a pointer to an audio
component plug-in interface structure.

00:16:32.980 --> 00:16:36.320
This structure has
three function pointers.

00:16:36.320 --> 00:16:41.040
The first two are open and close,
which are used to open and close,

00:16:41.370 --> 00:16:43.880
surprisingly, that component.

00:16:43.880 --> 00:16:47.880
And then we have a lookup function
pointer that takes a selector.

00:16:47.880 --> 00:16:51.860
We'll examine this in a
bit more detail shortly.

00:16:53.970 --> 00:16:58.130
Now when our application is calling
audio component instance new,

00:16:58.340 --> 00:17:01.250
what's actually happening is
that the system is getting that

00:17:01.390 --> 00:17:05.980
component factory function,
and then getting the audio component

00:17:06.100 --> 00:17:08.640
plug-in interface by calling that.

00:17:08.680 --> 00:17:11.780
That audio component plug-in
interface contains a function

00:17:11.780 --> 00:17:14.760
pointer to the open method,
which is then called,

00:17:14.880 --> 00:17:19.480
and that open method returns
an instance of an audio unit,

00:17:19.480 --> 00:17:22.570
which goes back to the application.

00:17:24.150 --> 00:17:26.740
Now let's say, for example,
that application has

00:17:26.910 --> 00:17:30.880
taken that audio unit,
added it into a graph, initialized it,

00:17:30.950 --> 00:17:33.560
and then is ready to render.

00:17:33.560 --> 00:17:38.170
The application calls audio unit render.

00:17:40.040 --> 00:17:45.800
The audio component
system gets the component,

00:17:45.800 --> 00:17:50.730
looks up the factory function,
gets the plug-in interface,

00:17:50.930 --> 00:17:54.170
gets a function pointer
to the lookup method,

00:17:54.290 --> 00:17:59.990
which then it provides the selector
K audio unit render select.

00:18:04.170 --> 00:18:08.100
And then the function pointer for
render is returned by the lookup method,

00:18:08.100 --> 00:18:10.880
which is then called by the system.

00:18:12.810 --> 00:18:14.640
That's all I have for audio units.

00:18:14.790 --> 00:18:17.530
I'd now like to call up Doug Scott,
who will be speaking about

00:18:17.580 --> 00:18:19.300
the sampler audio unit.

00:18:19.830 --> 00:18:23.320
Doug?

00:18:24.710 --> 00:18:26.100
Good morning.

00:18:26.150 --> 00:18:26.930
I'm Doug Scott.

00:18:26.930 --> 00:18:30.700
I am also a member of the
Core Audio Engineering team.

00:18:30.770 --> 00:18:34.720
And I'd like to talk to you in some
detail about the new sampler audio unit

00:18:34.810 --> 00:18:37.200
that Michael mentioned a minute ago.

00:18:38.790 --> 00:18:41.740
If you remember the diagram
that Michael showed,

00:18:41.740 --> 00:18:45.410
this is the location of the
sample unit in that diagram.

00:18:47.360 --> 00:18:53.770
What makes this sampler an instrument
type is that it sits at the top of any

00:18:54.230 --> 00:18:59.440
branch of an audiograph and generates
audio in response to commands that are

00:18:59.440 --> 00:19:03.560
sent to it via the music device API,
such as note-ons, note-offs,

00:19:03.650 --> 00:19:07.030
and anything that might change
the quality of that sound,

00:19:07.100 --> 00:19:10.330
like adjusting the volume
and things of that sort.

00:19:11.920 --> 00:19:16.130
What makes this a sampler is
that it organizes a set of audio

00:19:16.130 --> 00:19:21.450
files that you provide it by,
it organizes them and shapes them

00:19:21.670 --> 00:19:26.500
and organizes them into a coherent,
playable instrument.

00:19:26.500 --> 00:19:30.950
Examples of this might be a drum kit,
where you've loaded a set of sampled

00:19:30.960 --> 00:19:34.600
percussion sounds to various places
on the keyboard and then play them

00:19:34.690 --> 00:19:36.700
back either together or individually.

00:19:36.700 --> 00:19:41.220
Or perhaps you're creating a very fancy
acoustic piano where you've loaded a

00:19:41.220 --> 00:19:46.200
complete range of samples from end to
end on the keyboard and want to play

00:19:46.200 --> 00:19:47.700
that back as if it were a real piano.

00:19:47.700 --> 00:19:53.270
Or you could just have an arbitrary set
of sound effects that you want to be able

00:19:53.340 --> 00:19:56.400
to trigger in a game or an application.

00:19:58.580 --> 00:20:02.590
Some of the features of the new sampler
are it accepts samples in any of the

00:20:02.720 --> 00:20:06.020
formats that our audio library supports,
which includes compressed formats.

00:20:06.070 --> 00:20:11.110
It shares any sample resources across
all instances of the sampler that

00:20:11.240 --> 00:20:13.830
are present in your application.

00:20:13.840 --> 00:20:17.860
And files that exceed a certain size
are automatically streamed for you.

00:20:17.860 --> 00:20:20.800
So what this means is that you
can have a fairly large body of

00:20:20.800 --> 00:20:24.580
sample content within the sampler,
but have a fairly low memory footprint

00:20:24.580 --> 00:20:28.380
because you're not spending a lot of
time duplicating or storing information.

00:20:28.400 --> 00:20:30.140
in memory.

00:20:30.440 --> 00:20:32.990
In addition,
the sampler has a very lightweight

00:20:33.330 --> 00:20:37.180
preset format stored as a text file,
a plist file actually,

00:20:37.310 --> 00:20:43.010
which makes it very easy to
transfer that from your desktop

00:20:43.100 --> 00:20:47.140
where you can audition and create,
as we will see in a few minutes,

00:20:47.240 --> 00:20:52.310
to other desktop machines or into
your iOS application or game.

00:20:53.040 --> 00:20:58.100
The structure of the sampler is
such that it provides an extremely

00:20:58.100 --> 00:21:01.600
flexible instrument design,
which allows you to create anything

00:21:01.600 --> 00:21:05.660
from the most bare-bones instrument that
plays a single sound on a single key,

00:21:05.940 --> 00:21:09.720
all the way to incredibly rich
and complex instrumental timbres,

00:21:09.720 --> 00:21:13.190
all depending upon the needs
of your particular application.

00:21:13.880 --> 00:21:17.230
And in addition,
this sampler has the capability of

00:21:17.230 --> 00:21:22.380
translating both DLS and SoundFont 2
preset format into its own native format,

00:21:22.380 --> 00:21:28.320
which allows you to work with the huge
existing body of designed instruments

00:21:28.350 --> 00:21:33.890
as a starting place before you begin
working on your own custom designs.

00:21:35.170 --> 00:21:38.390
Let's look a little bit about how
the sampler patch is organized.

00:21:38.490 --> 00:21:43.400
What we have is a hierarchy of zones and
layers where each level of the system

00:21:43.590 --> 00:21:46.420
is dependent upon the layer above it.

00:21:47.130 --> 00:21:48.880
At the bottom, we have zones.

00:21:48.880 --> 00:21:53.960
Zones represent the way in which any
individual audio file in the system

00:21:53.960 --> 00:21:57.340
is going to be played back within
the context of the entire instrument.

00:21:57.340 --> 00:21:59.420
That includes information
about its root key,

00:21:59.420 --> 00:22:02.160
in other words,
where it plays without any transposition,

00:22:02.160 --> 00:22:04.840
the range of keys over
which that sample will play,

00:22:04.840 --> 00:22:07.020
the range of velocities
over which it will play,

00:22:07.600 --> 00:22:10.980
plus information specific to how that
sample is going to be played back,

00:22:11.220 --> 00:22:14.080
involving looping,
perhaps if you wanted to do some

00:22:14.230 --> 00:22:17.500
fixed gain or detuning to make
it match your other samples,

00:22:17.500 --> 00:22:19.310
that's all available to
be set there as well.

00:22:19.320 --> 00:22:22.170
And there's many other properties
that I can't even get into

00:22:22.170 --> 00:22:23.910
here that are available there.

00:22:26.190 --> 00:22:29.680
The layer, besides being a collection
of the zones below it,

00:22:29.740 --> 00:22:34.540
is also the place where all of what we
often call articulation data is provided,

00:22:34.540 --> 00:22:38.160
which means anything that
modifies the quality of the sound,

00:22:38.160 --> 00:22:41.050
such as filters,
LFOs that might be controlling

00:22:41.160 --> 00:22:44.230
frequency or amplitude,
envelopes, of course,

00:22:44.230 --> 00:22:47.480
which can control various
other things in the system.

00:22:47.500 --> 00:22:49.480
All of this is defined
at the layer level.

00:22:49.480 --> 00:22:52.780
Like I said,
all the modulation connections.

00:22:53.760 --> 00:22:57.380
Additionally, there are other parameters
here specific to the layer,

00:22:57.410 --> 00:23:00.210
such as the ability to select --
to determine how your zones are

00:23:00.210 --> 00:23:05.130
being selected when they're picked,
and an offset which allows you to affect

00:23:05.130 --> 00:23:07.210
all the zones which are present below it.

00:23:07.420 --> 00:23:11.210
And there's several other things -- many
other things you can do here as well.

00:23:14.020 --> 00:23:15.780
The instrument is the top level.

00:23:15.890 --> 00:23:17.360
It's a collection of the layers.

00:23:17.500 --> 00:23:20.240
It doesn't expose any parameters
or properties at this time,

00:23:20.240 --> 00:23:24.040
but it is also the place where
all of the samples that are

00:23:24.300 --> 00:23:27.010
referenced by your patch are kept.

00:23:28.780 --> 00:23:31.160
Here's an example of a simple patch.

00:23:31.240 --> 00:23:34.040
We have a single layer
with multiple zones in it.

00:23:34.220 --> 00:23:36.900
The layer extends the entire
length of the keyboard,

00:23:36.900 --> 00:23:43.350
and the zones basically divide up that
keyboard space into four equal parts.

00:23:45.730 --> 00:23:50.430
Here's the behavior of the sampler if a
note on is received on a particular key.

00:23:50.540 --> 00:23:53.770
Sorry.

00:23:56.260 --> 00:24:01.570
The zone whose key range includes
that key number will be activated.

00:24:02.090 --> 00:24:04.650
And that zone will generate
a single active voice,

00:24:04.650 --> 00:24:08.200
which is the actual audio
producing engine of the system.

00:24:08.880 --> 00:24:12.460
In contrast, let's look at a slightly
more complicated patch,

00:24:12.460 --> 00:24:13.930
which is layered.

00:24:13.940 --> 00:24:15.010
It's layered layers.

00:24:15.030 --> 00:24:17.240
In this case, we have two,
both of which extend the

00:24:17.240 --> 00:24:19.210
entire range of the keyboard.

00:24:19.340 --> 00:24:23.210
But each one has its own
individual set of zones,

00:24:23.250 --> 00:24:29.520
so you're going to have different samples
in each associated with each layer.

00:24:29.700 --> 00:24:33.500
When a MIDI note-on is
received by this instrument,

00:24:35.230 --> 00:24:38.670
Because we have two zones
whose key ranges intercept

00:24:38.950 --> 00:24:43.480
that particular key number,
we have two zones become activated.

00:24:43.910 --> 00:24:45.810
and you generate two voices.

00:24:45.810 --> 00:24:48.980
So the idea is that a complex
instrument may very well

00:24:48.980 --> 00:24:54.470
generate a set of active voices,
which will combine to produce the

00:24:54.520 --> 00:24:57.170
complex timbre of that instrument.

00:24:58.150 --> 00:25:00.230
Here's another example
of a complex patch.

00:25:00.360 --> 00:25:02.680
This one,
instead of having overlapping layers,

00:25:02.800 --> 00:25:06.750
has two complete layers which
divide the keyboard approximately

00:25:06.750 --> 00:25:08.780
into one-third and two-third.

00:25:08.780 --> 00:25:12.840
The bottom one is dedicated to
producing the timbre of a double bass,

00:25:12.840 --> 00:25:15.780
which means its zones are
going to contain samples

00:25:15.780 --> 00:25:17.760
that are double bass samples.

00:25:17.760 --> 00:25:20.330
And the upper half of the
keyboard is a vibraphone preset,

00:25:20.350 --> 00:25:23.620
which has its own zones representing
the samples for the vibraphone.

00:25:23.660 --> 00:25:27.660
So this is a traditional
split keyboard patch.

00:25:30.900 --> 00:25:33.660
Okay, what I'd like to do now
is have Michael back up,

00:25:33.660 --> 00:25:37.660
and we're going to see how we can
audition and edit one of these,

00:25:37.660 --> 00:25:43.310
a patch that is loaded into the
sampler using its custom view.

00:25:45.340 --> 00:25:48.780
What we have on the screen right
now is the custom view for the

00:25:48.840 --> 00:25:51.040
AU Sampler in its compact form.

00:25:51.190 --> 00:25:53.720
What's displaying by
default is the keyboard,

00:25:53.720 --> 00:25:55.690
which allows you to audition the preset.

00:25:56.370 --> 00:26:04.450
What we have loaded here first
is an example of a simple preset,

00:26:04.460 --> 00:26:07.140
which has,
like the simple example we gave,

00:26:07.140 --> 00:26:10.140
a single layer and
several zones under it.

00:26:10.250 --> 00:26:13.930
If we open up the editor
portion of the window,

00:26:14.140 --> 00:26:17.600
We can see on the left a list of
whatever layers are present in the file,

00:26:17.600 --> 00:26:19.590
plus the zones underneath them.

00:26:19.710 --> 00:26:22.870
When the layer is selected in that list,
you can see on the right we have

00:26:23.010 --> 00:26:25.780
an editor for the settings that
are available for the layer,

00:26:25.780 --> 00:26:29.990
and when we select each of the zones,
we get an editor showing the

00:26:29.990 --> 00:26:32.480
characteristics of that zone.

00:26:32.620 --> 00:26:35.750
The large rectangles at the
bottom are simply a display of

00:26:35.750 --> 00:26:39.070
the currently selected zone and
its location on the keyboard.

00:26:39.070 --> 00:26:43.430
Horizontally, we're displaying the key
numbers that it's active over,

00:26:43.530 --> 00:26:46.890
and vertically we're
displaying its velocity range.

00:26:47.880 --> 00:26:53.000
If we go back to the layer on the left,
and then bring up the parameters button

00:26:53.000 --> 00:26:55.690
on the right-hand side of the screen,
here's where we have access

00:26:55.690 --> 00:26:58.160
to all of what I was talking
about as the articulation data,

00:26:58.160 --> 00:27:01.560
which includes envelope settings,
filter settings,

00:27:01.560 --> 00:27:04.130
any low-frequency oscillators
that might be available.

00:27:04.420 --> 00:27:05.300
All these are settable here.

00:27:05.300 --> 00:27:09.620
And as I said,
each layer that you have in your

00:27:09.710 --> 00:27:12.690
instrument will have its own set of these
that can be configured independently.

00:27:13.900 --> 00:27:16.560
Okay, let's bring up a second preset,
which is an example of a

00:27:16.560 --> 00:27:18.660
slightly more complicated one.

00:27:18.660 --> 00:27:23.580
In this case, we have a vibraphone,
which has two layers,

00:27:23.580 --> 00:27:26.480
one of which has been dedicated to
the attack portion of the vibraphone,

00:27:26.480 --> 00:27:28.220
the second to the sustain portion.

00:27:28.220 --> 00:27:31.950
Let's just audition that for a
minute so they can hear that.

00:27:34.630 --> 00:27:38.330
To let you hear the way this is
divided up as a complex instrument,

00:27:38.340 --> 00:27:42.480
would you please mute the one
that's for the sustain portion?

00:27:42.510 --> 00:27:47.010
And now if we play this back-- Oh,
we got the wrong one.

00:27:47.010 --> 00:27:49.240
Let's try the other one.

00:27:49.240 --> 00:27:51.360
It's a little more obvious when we--

00:27:53.130 --> 00:27:54.080
There we go.

00:27:54.230 --> 00:27:57.880
So now all you're hearing is
the layer which represents the

00:27:57.880 --> 00:27:59.330
attack portion of the sound.

00:27:59.330 --> 00:28:03.220
So this is an example of how the layers
combine to produce a complex timbre.

00:28:03.230 --> 00:28:09.380
Each note on when this instrument is
completely active produces two voices.

00:28:09.380 --> 00:28:11.360
And lastly,
what I'd like to do is show you

00:28:11.360 --> 00:28:15.060
that the presets which are being
loaded and auditioned here are simply

00:28:15.060 --> 00:28:18.870
small files which are available
and accessible directly through

00:28:18.880 --> 00:28:21.250
the file system as .au preset files.

00:28:21.250 --> 00:28:25.040
We're seeing the path,
the default path to these here.

00:28:25.040 --> 00:28:30.000
And these are the files that can be
moved from machine to machine or to

00:28:30.060 --> 00:28:34.230
an iOS application in order to be
able to play these files in a game

00:28:34.230 --> 00:28:37.040
or in an application on your phone.

00:28:37.070 --> 00:28:40.760
So what I'm going to -- I'll be
talking about that again in a minute.

00:28:40.760 --> 00:28:42.760
Thank you very much, Michael.

00:28:50.000 --> 00:28:52.600
Let's talk a little bit about how
you can configure the sampler.

00:28:52.670 --> 00:28:56.150
We have three methods for loading
what I call a patch or a preset.

00:28:56.260 --> 00:28:58.500
The first is using the
files that I just described,

00:28:58.500 --> 00:29:01.200
which are AU preset files,
text files that can be loaded.

00:29:01.200 --> 00:29:05.810
The second is that you can build up a
custom patch from a set of individual

00:29:05.810 --> 00:29:08.000
audio files that you provide.

00:29:08.000 --> 00:29:13.240
Or third, as I mentioned earlier,
you can load an instrument definition

00:29:13.320 --> 00:29:16.100
from a DLS bank or a SoundFont 2 file.

00:29:16.100 --> 00:29:17.960
Let's look at each of those
in a little more detail.

00:29:18.380 --> 00:29:22.350
Firstly,
loading a patch from an AU preset file.

00:29:22.400 --> 00:29:25.190
The first thing that's most important
if you're under iOS is to make

00:29:25.260 --> 00:29:27.640
sure that all of the resources
that are used by that patch,

00:29:27.690 --> 00:29:29.630
which means any of the
samples associated with it,

00:29:29.950 --> 00:29:32.470
plus the patch itself,
are part of your application

00:29:32.470 --> 00:29:35.550
bundle's resource directory
so that they're accessible to

00:29:35.740 --> 00:29:37.490
the sampler when it's loading.

00:29:37.890 --> 00:29:40.090
The second thing you do,
and we'll be providing some sample

00:29:40.140 --> 00:29:43.800
code for this in a future seed,
is to convert that preset file

00:29:43.800 --> 00:29:46.590
into a property list using a
couple of standard API calls.

00:29:46.640 --> 00:29:47.980
I won't go into those now.

00:29:47.980 --> 00:29:51.310
And then thirdly,
you take that property list that

00:29:51.310 --> 00:29:55.840
you have and load it into the
sampler using the set property call.

00:29:55.840 --> 00:30:00.540
This is a property of the type
that Michael talked about earlier,

00:30:00.540 --> 00:30:04.050
using this 1K audio unit
property class info,

00:30:04.050 --> 00:30:05.570
and that will load that
preset into the sampler.

00:30:06.890 --> 00:30:10.260
Secondly, creating a patch from
a set of audio files.

00:30:10.270 --> 00:30:12.880
Again,
all of your files must be available to

00:30:12.880 --> 00:30:18.690
the sampler by being in your applications
bundle if you are running iOS.

00:30:18.700 --> 00:30:22.690
And if you want these individual files
to be loaded and produce a zone which

00:30:22.690 --> 00:30:26.840
has information such as we talked about,
which is like key range and

00:30:26.840 --> 00:30:30.630
looping and things of that sort,
you can provide that information

00:30:30.630 --> 00:30:33.450
to the sampler by editing
the file's instrument chunk.

00:30:33.530 --> 00:30:37.000
This is something which is
available in core audio file format,

00:30:37.120 --> 00:30:38.380
the CAF file format.

00:30:38.380 --> 00:30:40.010
If you do that,
when these files are loaded,

00:30:40.010 --> 00:30:42.700
that information will be transferred
directly into the zone so that you

00:30:42.750 --> 00:30:45.900
can actually have a pre-configured
set of files that when they're loaded,

00:30:46.100 --> 00:30:49.390
they'll, for example,
map to your keyboard in an orderly

00:30:49.390 --> 00:30:52.360
fashion that you can control in advance.

00:30:53.490 --> 00:30:58.240
And lastly, we can load patches directly
from DLS sound banks.

00:30:58.280 --> 00:31:01.040
As I said, once again,
make sure that all of your file content

00:31:01.040 --> 00:31:04.180
is available to the sampler if you're
under iOS by putting it in your bundle.

00:31:04.180 --> 00:31:08.220
And we have a property that you
can pass to the sampler where

00:31:08.220 --> 00:31:12.800
you pass the URL to your bank,
plus a bank ID and an instrument ID,

00:31:12.800 --> 00:31:15.910
which is the way in which
presets are identified in

00:31:16.010 --> 00:31:18.080
both DLS and sound font banks.

00:31:18.630 --> 00:31:21.120
And that will load that
instrument definition in.

00:31:21.120 --> 00:31:23.890
All of the properties that are
present in the DLS or sound font

00:31:23.890 --> 00:31:27.440
bank will be translated into a form
that the sampler understands itself.

00:31:27.460 --> 00:31:31.480
What I'd like to show
you now is a quick demo.

00:31:33.950 --> 00:31:40.420
having the two presets that we saw
earlier auditioned on a desktop

00:31:40.420 --> 00:31:43.690
machine loaded directly into an app.

00:31:47.340 --> 00:31:49.330
Okay, there's not much to look at.

00:31:49.460 --> 00:31:53.380
I am an audio guy.

00:31:53.620 --> 00:31:56.960
All I have here is two buttons
representing trombone and vibraphone.

00:31:56.960 --> 00:31:58.760
If you remember,
those are the two presets that we

00:31:58.760 --> 00:32:00.620
auditioned on the desktop using AU Lab.

00:32:00.660 --> 00:32:06.880
If I select the trombone
and have my audition keys...

00:32:08.900 --> 00:32:13.250
We have the exact same timbre
brought over to an iOS app simply by

00:32:13.300 --> 00:32:17.040
bringing that AU preset file plus its
associated samples over and loading

00:32:17.040 --> 00:32:20.080
them into my bundle and writing
an app which made those function

00:32:20.080 --> 00:32:21.650
calls that I showed you a minute ago.

00:32:21.670 --> 00:32:22.900
The vibraphone.

00:32:23.200 --> 00:32:25.540
Ooh, a little loud.

00:32:25.760 --> 00:32:27.650
Oh, well.

00:32:45.330 --> 00:32:49.440
Now I'd like to bring Michael back up to
talk about the introduction of Core MIDI,

00:32:49.450 --> 00:32:52.300
which is now available in iOS 5.

00:32:58.940 --> 00:33:02.870
Now we're going to talk about this
portion in the upper right-hand corner,

00:33:02.870 --> 00:33:07.820
which is getting MIDI to your iOS device.

00:33:08.780 --> 00:33:13.800
Under iOS 4.3 or later,
you can use Core MIDI,

00:33:13.800 --> 00:33:17.280
which is a set of services
where applications can

00:33:17.390 --> 00:33:21.970
communicate with MIDI devices,
and it provides abstractions for

00:33:21.970 --> 00:33:25.010
interacting with a complete network.

00:33:25.380 --> 00:33:28.760
As you can see in the diagram, we have,
for example,

00:33:28.760 --> 00:33:34.640
a MIDI keyboard that's connected to
an iPad via a camera connection kit.

00:33:34.660 --> 00:33:38.700
Any MIDI that occurs on that
device will be sent to the

00:33:38.700 --> 00:33:43.120
MIDI server in its own process,
and then it will communicate with your

00:33:43.120 --> 00:33:45.840
application via the core MIDI APIs.

00:33:48.240 --> 00:33:51.560
A bit of terminology about MIDI devices.

00:33:51.840 --> 00:33:54.950
When we use the term device,
we're referring specifically

00:33:55.100 --> 00:33:57.140
to a piece of hardware.

00:33:57.170 --> 00:33:59.450
This piece of hardware can contain

00:34:02.050 --> 00:34:06.540
Endpoints, and these can be either source
or destination endpoints,

00:34:06.680 --> 00:34:12.390
depending on whether they
receive or send MIDI data.

00:34:15.010 --> 00:34:19.400
When an application running on
an iPad wants to get information

00:34:19.400 --> 00:34:22.990
about devices that are connected
and receive notifications,

00:34:23.170 --> 00:34:29.000
for example, when something's plugged in,
it first needs to create a client object,

00:34:29.000 --> 00:34:33.330
at which point the MIDI server will
then call that application's MIDI notify

00:34:33.440 --> 00:34:39.290
proc and tell it when a device changes,
connected or disconnected, for example,

00:34:39.470 --> 00:34:43.470
or if any properties on
that specific device change.

00:34:46.300 --> 00:34:47.430
So now we have a keyboard.

00:34:47.440 --> 00:34:49.020
We're plugging it in.

00:34:49.050 --> 00:34:53.040
The MIDI server process gets that
information and communicates to the

00:34:53.040 --> 00:34:56.240
application via MIDI notify proc.

00:34:56.970 --> 00:35:01.150
Looking at those types of properties
that we can get information about,

00:35:01.290 --> 00:35:06.490
those include name of, say, a device,
or even a particular

00:35:06.490 --> 00:35:09.420
endpoint if it's named,
manufacturer information,

00:35:09.420 --> 00:35:14.240
whether that device is offline or not,
hardware settings,

00:35:14.240 --> 00:35:17.800
such as whether that piece
of hardware supports general

00:35:17.800 --> 00:35:19.660
MIDI or can transmit clock.

00:35:19.730 --> 00:35:25.500
We can also get information
about what patch it has loaded,

00:35:25.550 --> 00:35:26.650
et cetera.

00:35:27.750 --> 00:35:29.900
Let's look at that in code.

00:35:29.920 --> 00:35:33.200
As I mentioned,
we used a MIDI notify proc to

00:35:33.200 --> 00:35:37.100
register a handler with core
MIDI that it will then call back.

00:35:37.210 --> 00:35:41.370
As you can see here in this code,
we're specifically handling the

00:35:41.370 --> 00:35:44.120
KMIDIMESSAGE property changed.

00:35:44.250 --> 00:35:46.570
And depending on what
that means to our app,

00:35:46.620 --> 00:35:49.420
for example,
if we wanted to display a picture

00:35:49.420 --> 00:35:53.320
of a keyboard when it's connected,
we can then do that here.

00:35:55.730 --> 00:36:00.200
We then create a MIDI client,
passing a string for

00:36:00.200 --> 00:36:03.320
the name of our client,
our notification handler that we're

00:36:03.320 --> 00:36:07.480
registering with the MIDI server,
and then we get back a MIDI client,

00:36:07.480 --> 00:36:10.570
which we'll use in further calls.

00:36:11.190 --> 00:36:16.040
So now the more interesting bit,
getting MIDI data to our application.

00:36:16.390 --> 00:36:19.580
First, we create a MIDI input port.

00:36:19.600 --> 00:36:22.440
Then the MIDI server will
call our MIDI read proc when

00:36:22.440 --> 00:36:24.220
MIDI messages are received.

00:36:24.260 --> 00:36:29.710
And the information that we get is a
packet list containing those MIDI events.

00:36:29.910 --> 00:36:34.220
So for example,
when the user plays E on our keyboard,

00:36:34.260 --> 00:36:37.770
the MIDI server gets that note on,
and then calls the MIDI read

00:36:37.880 --> 00:36:43.160
proc in our application,
passing the packet list of data.

00:36:44.760 --> 00:36:48.640
Let's look at that in more detail.

00:36:48.640 --> 00:36:51.380
MIDI packets are variable length.

00:36:51.380 --> 00:36:55.140
So that means that when we're
getting this packet list,

00:36:55.140 --> 00:36:58.680
in addition to the timestamp,
which is the time, an absolute time that

00:36:58.680 --> 00:37:01.440
that event occurred,
we have a length that tells us

00:37:01.560 --> 00:37:03.820
how much data is in that packet.

00:37:03.820 --> 00:37:08.630
We can get the first packet by
getting element zero in that array,

00:37:08.630 --> 00:37:13.880
but we need to use MIDI packet next
to retrieve successive packets.

00:37:14.700 --> 00:37:17.990
You'll see that in my
programming example.

00:37:18.090 --> 00:37:21.700
We're registering our MIDI read proc.

00:37:21.710 --> 00:37:25.500
Here, we're processing packets by
calling MIDI packet next.

00:37:25.560 --> 00:37:30.000
And those packets that we receive,
their data is just standard MIDI,

00:37:30.000 --> 00:37:36.410
and it's up to your application to use
the MIDI spec to decode those packets.

00:37:36.940 --> 00:37:41.370
We call MIDI input port create,
getting our client that

00:37:41.370 --> 00:37:45.070
we previously created,
specifying the name for our port,

00:37:45.070 --> 00:37:48.260
the callback proc,
and then we get back our MIDI port,

00:37:48.360 --> 00:37:52.290
which we can then use to
connect to destinations,

00:37:52.290 --> 00:37:53.660
for example.

00:37:55.730 --> 00:37:58.700
Finally, if our application
wants to send MIDI data,

00:37:58.850 --> 00:38:04.400
we use MIDI send with a packet list
of the events that we're sending.

00:38:04.400 --> 00:38:09.160
We first need to create a MIDI output
port by calling MIDI output port create.

00:38:09.160 --> 00:38:12.600
And then we specify where
we want that data to go.

00:38:12.600 --> 00:38:14.460
And off it goes.

00:38:14.460 --> 00:38:16.580
It's that simple.

00:38:18.690 --> 00:38:22.560
Now, we also have the capability of not
only sending and receiving MIDI events

00:38:22.620 --> 00:38:26.590
from locally connected devices,
we can also get devices

00:38:26.600 --> 00:38:28.330
that exist over a network.

00:38:28.360 --> 00:38:34.350
In this case, we have an API called
MIDI Network Session that uses

00:38:34.420 --> 00:38:37.240
Bonjour to register peers.

00:38:37.240 --> 00:38:42.070
So we can use the MIDI Session
default session object and

00:38:42.090 --> 00:38:48.440
set its enabled property,
and that registers our peer with Bonjour.

00:38:48.600 --> 00:38:51.080
Saying, "Hey, I'm here.

00:38:51.100 --> 00:38:53.320
Anybody interested in
communicating with me,

00:38:53.320 --> 00:38:56.530
go ahead." And we can also
specify a connection policy

00:38:56.650 --> 00:38:58.760
limiting who can connect to us.

00:38:58.760 --> 00:39:01.660
So each peer that wants to
communicate in this manner

00:39:01.870 --> 00:39:06.280
creates a MIDI Network Session
object and enables a connection.

00:39:06.280 --> 00:39:10.880
Now, on the other hand,
when we want to make that connection,

00:39:11.080 --> 00:39:16.030
We use the MIDI network host
API to specify which of those

00:39:16.040 --> 00:39:18.040
Bonjour peers we're connecting to.

00:39:18.040 --> 00:39:21.770
And the name, address, and port,
we just use using the

00:39:21.770 --> 00:39:23.620
NS Network Session API.

00:39:23.620 --> 00:39:28.980
And there's a number of other
facilities and network hosts,

00:39:28.980 --> 00:39:31.420
if you have other types of
information other than the name,

00:39:31.420 --> 00:39:34.140
address, or port that you can use.

00:39:34.220 --> 00:39:36.200
Please see the header for more details.

00:39:36.600 --> 00:39:39.280
Once you've created
that MIDI network host,

00:39:39.400 --> 00:39:42.940
you simply call connect with host,
and then you're done.

00:39:42.940 --> 00:39:46.430
At that point, your MIDI notification
proc will get called,

00:39:46.430 --> 00:39:48.940
telling you what devices are available.

00:39:48.940 --> 00:39:52.420
You'll get your MIDI read
proc called with MIDI data,

00:39:52.420 --> 00:39:53.580
and that's it.

00:39:53.580 --> 00:39:56.660
So that includes my talk on Core MIDI.

00:39:56.660 --> 00:39:58.710
I'd now like to return
the presentation to Doug,

00:39:58.710 --> 00:40:00.720
who will be talking
about the MIDI sequencer.

00:40:05.910 --> 00:40:08.840
Thank you, Michael.

00:40:08.880 --> 00:40:15.760
I'd like to start this part of the
presentation by doing another demo.

00:40:15.810 --> 00:40:18.910
Once again, my minimalist application.

00:40:20.300 --> 00:40:24.530
But what you'll need to do is
imagine that you are creating a game.

00:40:24.630 --> 00:40:28.360
And in this game,
you want the player to be driving around

00:40:28.360 --> 00:40:32.930
the countryside in an antique automobile.

00:40:36.200 --> 00:40:39.250
After a while you realize
this is sort of boring,

00:40:39.250 --> 00:40:42.000
so you decide you want to make
sure that when the automobile is

00:40:42.100 --> 00:40:45.200
traveling up and down the hills,
the speed of your music changes.

00:40:45.200 --> 00:40:50.400
As it goes up a hill...

00:40:53.500 --> 00:40:56.740
down the other side very quickly.

00:40:56.740 --> 00:40:59.040
Now you might be able to figure
out how to do that using a set

00:40:59.040 --> 00:41:02.960
of pre-created audio files,
but it might be quite complicated.

00:41:02.960 --> 00:41:08.010
What about if your automobile were
to travel up to the very top of

00:41:08.010 --> 00:41:14.230
a hill and crash into a church?

00:41:18.920 --> 00:41:23.610
That kind of a musical transition would
likely be quite difficult to do with

00:41:23.610 --> 00:41:25.940
a set of pre-existing musical files.

00:41:26.140 --> 00:41:30.800
But using the music sequencing API,
that operation is extremely simple.

00:41:30.920 --> 00:41:33.900
So let's talk about how that all works.

00:41:40.820 --> 00:41:44.280
So the music sequencing
API allows you to record,

00:41:44.280 --> 00:41:48.660
edit, save,
and playback music events that are

00:41:48.710 --> 00:41:51.920
organized in a time-based fashion.

00:41:51.930 --> 00:41:55.510
By music events here,
we're not just talking about MIDI.

00:41:55.680 --> 00:41:59.630
Well, we're talking about MIDI,
but we're talking about any event which

00:41:59.630 --> 00:42:04.780
is synchronized in time with other events
to control your musical performance.

00:42:04.780 --> 00:42:07.810
So that includes -- that
does include all MIDI events,

00:42:07.810 --> 00:42:10.850
but we'll talk about some
other things as well.

00:42:10.960 --> 00:42:14.470
The complete set of API is
declared in the header that

00:42:14.550 --> 00:42:16.200
you see on the screen now.

00:42:16.330 --> 00:42:20.890
Let's talk about the sort of parts
that go into the sequencing API.

00:42:21.200 --> 00:42:23.100
First, we have the music sequence.

00:42:23.370 --> 00:42:29.210
This data type is the actual storage
place for all of the information that

00:42:29.440 --> 00:42:31.540
is part of your musical performance.

00:42:31.640 --> 00:42:34.980
And that information is
organized into tracks.

00:42:35.310 --> 00:42:38.770
One track will always be a tempo track,
which allows you to store

00:42:38.770 --> 00:42:41.320
information about the rate at
which the music plays back,

00:42:41.420 --> 00:42:43.720
which is usually defined
in beats per minute.

00:42:43.840 --> 00:42:46.180
All the other tracks in the
file will be event tracks,

00:42:46.180 --> 00:42:48.750
which contain information, like we said,
about music events,

00:42:48.760 --> 00:42:51.510
which we'll be talking
about here in a second.

00:42:52.560 --> 00:42:54.600
Next, we have the music track itself.

00:42:54.630 --> 00:42:57.480
These are contained within
the music sequencer.

00:42:57.650 --> 00:43:00.180
In other words,
they belong to a particular sequencer.

00:43:00.210 --> 00:43:04.490
And they are a time-ordered
collection of events.

00:43:05.600 --> 00:43:07.470
These events can be of two types.

00:43:07.470 --> 00:43:10.550
MIDI events, like I said,
which includes note on, note off,

00:43:10.870 --> 00:43:14.380
controller events,
meta events relating to, you know,

00:43:14.380 --> 00:43:17.100
you can add tags and things of that sort,
anything that is defined

00:43:17.190 --> 00:43:18.440
by the MIDI specification.

00:43:18.440 --> 00:43:21.400
And secondly,
it allows you to store audio

00:43:21.550 --> 00:43:24.120
unit parameter automation data.

00:43:24.300 --> 00:43:27.300
Now, this is something which
allows you to take parameters,

00:43:27.300 --> 00:43:30.730
such as Mike described earlier,
and give them time points so that

00:43:30.730 --> 00:43:34.280
over the course of a performance,
the parameter values change

00:43:34.280 --> 00:43:37.640
in a way which is synchronized
with your other events.

00:43:37.640 --> 00:43:41.160
This is very powerful when you're
working with a fairly complex audio

00:43:41.160 --> 00:43:44.990
unit graph because it allows you to
manipulate various parameters and make

00:43:44.990 --> 00:43:49.290
sure that everything happens precisely
synchronized the way you would want it.

00:43:49.360 --> 00:43:52.030
And in addition,
we're able to add arbitrary user

00:43:52.210 --> 00:43:54.280
data events into the audio unit.

00:43:54.280 --> 00:43:55.850
So, this is a very powerful tool.

00:43:56.070 --> 00:43:57.620
It's very useful for you to use.

00:43:57.640 --> 00:43:58.950
And finally,
it allows you to use the music

00:43:58.950 --> 00:44:00.930
track so that during playback,
you can trigger things to happen in

00:44:00.960 --> 00:44:03.640
your application that are not directly
related to the playback of the sequence.

00:44:06.380 --> 00:44:09.900
And lastly,
we have the music player itself, which,

00:44:09.940 --> 00:44:12.770
when it's associated with
a given music sequence,

00:44:12.770 --> 00:44:16.460
allows you to convert that
sequence into a performance.

00:44:16.460 --> 00:44:20.370
Now let's look a little more about what
each of these components does for you.

00:44:20.770 --> 00:44:22.840
First, the music sequence.

00:44:22.870 --> 00:44:26.330
This is the place where you add, remove,
and merge the music tracks.

00:44:26.330 --> 00:44:29.380
In addition,
this is the place where you can

00:44:29.380 --> 00:44:31.810
read and write MIDI files to disk.

00:44:31.810 --> 00:44:35.020
And in addition,
it has a series of utilities that let

00:44:35.150 --> 00:44:37.460
you convert between beats and time.

00:44:37.460 --> 00:44:43.770
This is useful if you're trying to
display your events in a user interface,

00:44:43.770 --> 00:44:48.470
where you want to maybe provide the
user with an alternate way of displaying

00:44:48.470 --> 00:44:49.800
where the events are occurring.

00:44:52.910 --> 00:44:56.410
The music track, as I said,
part of the music sequence,

00:44:56.410 --> 00:44:58.850
allows you to add, move,
and clear music events.

00:44:58.850 --> 00:45:02.230
It has properties on it which
allow you to mute a track,

00:45:02.400 --> 00:45:05.490
solo it, control whether the track
loops during performance,

00:45:05.520 --> 00:45:07.670
and several other things
that you can do with it.

00:45:08.720 --> 00:45:13.680
A very important aspect of a track is
that it is responsible for associating

00:45:13.680 --> 00:45:17.440
the events that are contained within
it with a particular destination.

00:45:17.440 --> 00:45:21.600
The destination in this case is
the particular portion of the

00:45:21.610 --> 00:45:26.380
system that is going to respond to
the events that are in that track.

00:45:26.480 --> 00:45:30.610
The two primary places where you
might send things are to audio

00:45:30.770 --> 00:45:32.840
units that are part of a graph.

00:45:32.840 --> 00:45:34.880
For example,
an instrument like the sampler.

00:45:34.880 --> 00:45:37.460
Note-on events would be sent
to an instrument like that.

00:45:38.280 --> 00:45:43.710
You might have parameters which you
want to have control a sound effect,

00:45:43.780 --> 00:45:46.980
perhaps in the same
way as Michael showed,

00:45:47.280 --> 00:45:49.220
where you might want to be
adjusting your delay time,

00:45:49.460 --> 00:45:52.750
and have this all part of this
synchronized performance that is

00:45:52.750 --> 00:45:54.540
being controlled by the sequence.

00:45:54.560 --> 00:46:00.140
MIDI events can also be sent out to
external MIDI devices using the core

00:46:00.140 --> 00:46:03.360
MIDI system that Michael described.

00:46:03.360 --> 00:46:06.720
This would allow you to take
a sequence and not only play

00:46:06.720 --> 00:46:07.920
back music on the device,
but also play it back on the device.

00:46:07.920 --> 00:46:09.610
You could also use the track as
a way to control the playback.

00:46:09.620 --> 00:46:11.100
You could control the
playback on the device itself,

00:46:11.100 --> 00:46:12.770
or your desktop or your device,
but it could actually control

00:46:12.800 --> 00:46:14.910
an external device synchronously
with the playback internally.

00:46:15.020 --> 00:46:18.920
So you could create an entire ensemble
of internal and external devices.

00:46:18.920 --> 00:46:21.170
And in addition,
the track supports the ability

00:46:21.170 --> 00:46:24.480
to iterate through its events,
which is something you would need to use

00:46:24.480 --> 00:46:28.170
if you were going to build an interactive
program which displayed all the events

00:46:28.270 --> 00:46:29.890
that were in all of your tracks.

00:46:29.930 --> 00:46:36.350
And there's a thing called a music event
iterator which allows you to do that.

00:46:37.170 --> 00:46:41.370
And the music player, as I said,
the part that actually creates,

00:46:41.440 --> 00:46:44.760
makes it become a performance,
supports all the playback controls,

00:46:44.760 --> 00:46:46.700
starting, stopping,
adjusting the position,

00:46:46.700 --> 00:46:48.670
and also adjusting the playback rate.

00:46:48.680 --> 00:46:54.010
In the app that I demonstrated for you,
that was the function that I used

00:46:54.010 --> 00:46:57.620
to basically slow us down and speed
us up as we traveled the mountains.

00:46:57.920 --> 00:47:01.280
And we have additional utilities
here for converting between host

00:47:01.370 --> 00:47:04.160
time to beats and vice versa,
which is very important when

00:47:04.160 --> 00:47:06.590
you are reading in MIDI events
from the outside world,

00:47:06.590 --> 00:47:09.500
which are tagged with host times,
and you want to turn those into a

00:47:09.500 --> 00:47:12.640
format that the sequence understands,
which is typically in beats.

00:47:12.640 --> 00:47:14.560
We'll talk about that more in a minute.

00:47:14.560 --> 00:47:18.900
So let's walk through what happens when
you load a MIDI file into a sequence.

00:47:18.900 --> 00:47:22.590
This is useful because it shows the
relationship between all the components

00:47:22.660 --> 00:47:26.120
that go into actually producing
a performance with that sequence.

00:47:26.990 --> 00:47:29.770
So first thing you do is we assume
we have a sequence that we've

00:47:29.770 --> 00:47:33.280
created using our creation API that
I don't actually describe here.

00:47:33.280 --> 00:47:35.560
We have a function music
sequence file load,

00:47:35.560 --> 00:47:37.720
where you hand it your
empty new sequence,

00:47:37.720 --> 00:47:39.820
the path to the MIDI file
you want to load,

00:47:39.820 --> 00:47:41.390
and some other properties.

00:47:42.930 --> 00:47:45.840
First thing that happens when you
do that is the tempo track is loaded

00:47:45.840 --> 00:47:47.640
in from the file if it's present.

00:47:47.790 --> 00:47:52.200
If it isn't, the sequence will create a
default tempo track for you.

00:47:52.990 --> 00:47:57.560
Next, any tracks contained within
that MIDI file will be pulled

00:47:57.560 --> 00:48:01.900
in with their associated events,
and the structure of that file

00:48:01.900 --> 00:48:04.700
in terms of the layout of the
tracks and the organization of the

00:48:04.710 --> 00:48:08.250
events will be mirrored exactly
in the sequence that's created.

00:48:09.090 --> 00:48:12.400
Next,
the sequence creates a default AU graph,

00:48:12.520 --> 00:48:15.050
which always contains a
single instrument at the top,

00:48:15.100 --> 00:48:17.830
plus a limiter to just
help control the volume,

00:48:17.830 --> 00:48:21.920
and of course,
the necessary output audio unit.

00:48:22.580 --> 00:48:27.960
Lastly, all the events in all the tracks
that are in that file have their

00:48:27.960 --> 00:48:32.630
destination set so that the events
will control the instrument that

00:48:32.630 --> 00:48:33.750
is at the top of that graph.

00:48:33.840 --> 00:48:36.660
Any node-ons, any controller events,
anything of that sort will

00:48:36.800 --> 00:48:38.280
be sent to that instrument.

00:48:38.280 --> 00:48:41.220
That's exactly what was
happening in my demo.

00:48:41.220 --> 00:48:45.140
In that case, there's just a single track
with the piano music in it,

00:48:45.200 --> 00:48:47.880
and the node-ons and node-offs
from that piano music were being

00:48:47.910 --> 00:48:51.580
sent to an instance of a sampler
that was built automatically

00:48:51.580 --> 00:48:53.150
by the load of the SMF file.

00:48:56.980 --> 00:49:01.210
To play the sequence,
we create a music player and then

00:49:01.330 --> 00:49:05.540
call music player set sequence
to associate the two together.

00:49:05.620 --> 00:49:07.770
And music players start.

00:49:08.070 --> 00:49:12.480
We have, and of course we can stop,
we can pause, and we can set position.

00:49:12.530 --> 00:49:15.600
And the function there, the last one,
Music Player Set Play Rate Scaler,

00:49:15.600 --> 00:49:18.810
is what I use to adjust
the speed of the playback.

00:49:22.200 --> 00:49:26.310
There'll be a lot of instances, though,
where loading a sequence from a

00:49:26.430 --> 00:49:27.540
file is not what you want to do.

00:49:27.540 --> 00:49:29.820
You want to have a lot more control,
and perhaps you want to

00:49:29.820 --> 00:49:31.070
create something from scratch.

00:49:31.140 --> 00:49:34.820
So you might want to create a
MIDI recorder sequencer application,

00:49:34.940 --> 00:49:39.200
for example, a drum machine where you can
actually interactively add note-on

00:49:39.290 --> 00:49:42.030
and note-off events on the screen
and then hear them played back.

00:49:42.040 --> 00:49:44.670
Or you might want to play a
MIDI sequence that you know

00:49:44.680 --> 00:49:47.770
has multiple instruments in it,
and so you need to have a graph

00:49:47.810 --> 00:49:51.400
which is going to have more
instruments than just the default one.

00:49:52.100 --> 00:49:56.270
Or you might want to create a sequence
which has a large number of tracks

00:49:56.270 --> 00:50:00.290
which contain this automated parameter
information if you wanted to create

00:50:00.300 --> 00:50:03.660
some very complex system where things
are being controlled in real time.

00:50:03.660 --> 00:50:06.950
Way more information is being
controlled than you could

00:50:06.960 --> 00:50:09.160
control having a user interface.

00:50:12.260 --> 00:50:15.190
To create a custom sequence,
we pretty much follow the same set

00:50:15.190 --> 00:50:19.940
of steps as you saw automated in
the process of loading an SMF file.

00:50:19.940 --> 00:50:22.220
We create an empty sequence.

00:50:22.220 --> 00:50:26.010
We create our own custom graph
that meets our particular needs.

00:50:26.010 --> 00:50:29.760
We add any number of tracks to
that sequence that we need and

00:50:29.770 --> 00:50:31.520
add events to those tracks.

00:50:31.670 --> 00:50:34.800
Usually,
tracks are designed to group events which

00:50:34.800 --> 00:50:39.440
are targeted at a particular audio unit,
just for clarity,

00:50:39.520 --> 00:50:42.340
but it's up to you how you organize.

00:50:42.340 --> 00:50:48.220
And then the last thing you do is make
sure that each of your tracks is targeted

00:50:48.220 --> 00:50:52.090
to some destination so that those
events will be recognized and played

00:50:52.090 --> 00:50:55.330
back when the sequence is performed.

00:50:56.820 --> 00:50:58.930
Here's an example of a custom sequence.

00:50:59.020 --> 00:51:01.560
In this case,
the sequence itself looks very

00:51:01.560 --> 00:51:02.800
much like the one we saw before.

00:51:02.800 --> 00:51:05.170
We have a tempo track and
a couple of event tracks.

00:51:05.290 --> 00:51:09.900
But note that we have created a custom
AU graph with two instruments in it,

00:51:09.990 --> 00:51:15.450
and each of the tracks has been set to
play back through a different instrument.

00:51:19.790 --> 00:51:24.680
Lastly, you might want to actually
turn this into a system which

00:51:24.690 --> 00:51:27.550
can record a live performance,
which would be coming in for

00:51:27.550 --> 00:51:28.960
MIDI events sent from the outside.

00:51:28.960 --> 00:51:34.350
And you can synchronize the recording
of the live events with what's playing

00:51:34.360 --> 00:51:40.300
back by simply listening for MIDI events
on a MIDI input using the system that

00:51:40.300 --> 00:51:42.100
Michael described just a few minutes ago.

00:51:42.900 --> 00:51:44.890
The events that is,
as the events come in,

00:51:44.960 --> 00:51:49.130
they are tagged with a system time,
which can then be converted by the

00:51:49.130 --> 00:51:54.020
utility and the music player into
the exact beat and measure that is

00:51:54.020 --> 00:51:57.260
currently active in the playback.

00:51:57.260 --> 00:52:03.010
And this allows those events to be placed
precisely in the track at a place where

00:52:03.060 --> 00:52:08.860
you get a time-synchronized recording
of your live data combined with whatever

00:52:08.860 --> 00:52:12.020
is playing back through the sequence.

00:52:16.240 --> 00:52:17.490
All right.

00:52:17.590 --> 00:52:21.620
So in summary, what we've seen today as
technologies for music.

00:52:21.750 --> 00:52:24.580
First of all,
we saw how audio units can be

00:52:24.580 --> 00:52:30.340
used as building blocks in music
processing and synthesis applications.

00:52:30.870 --> 00:52:33.330
We saw the AU Sampler,
which allows you to

00:52:33.350 --> 00:52:35.410
create simple or very,
very rich,

00:52:35.490 --> 00:52:40.190
complex instrumental patches for use for
musical and sound effects purposes in

00:52:40.580 --> 00:52:45.120
iOS or Mac OS X games and applications.

00:52:45.120 --> 00:52:49.350
We saw how Core MIDI allows you to
send and receive audio -- sorry,

00:52:49.350 --> 00:52:53.810
send and receive MIDI under
both iOS and Mac OS X,

00:52:54.080 --> 00:52:58.510
either to a locally connected
MIDI device or via the network.

00:52:58.510 --> 00:53:01.460
And lastly,
we saw how the music sequencing

00:53:01.460 --> 00:53:06.780
API allows you to add -- to record,
add, create, edit, save,

00:53:07.110 --> 00:53:11.780
and playback music events to create
a complex musical performance.

00:53:15.360 --> 00:53:18.050
I'd like to invite you to a
related session which follows

00:53:18.050 --> 00:53:20.840
upon this one right away,
which is audio session

00:53:20.840 --> 00:53:21.960
management for iOS.

00:53:22.170 --> 00:53:25.810
This will be of great interest to
anybody who is working with anything

00:53:25.810 --> 00:53:29.690
that uses audio seriously under iOS.

00:53:29.810 --> 00:53:33.260
We're very excited to provide you
with these new technologies and are

00:53:33.260 --> 00:53:35.690
very much looking forward to see
what you're able to do with them.

00:53:35.700 --> 00:53:40.100
Lastly, if you have questions and we're
looking for more information,

00:53:40.100 --> 00:53:42.500
here's a list of
resources for all of you.

00:53:42.550 --> 00:53:48.640
Our media technology evangelist,
plus where to look for documentation.

00:53:48.640 --> 00:53:53.040
Of course, there's a WWDC website
which has information.

00:53:53.040 --> 00:53:55.860
And, of course,
our wonderful Apple Developer Forums,

00:53:55.860 --> 00:53:57.530
where you'll find us lurking.

00:53:58.930 --> 00:54:01.260
Thank you very much for coming,
and I really hope that you enjoy

00:54:01.260 --> 00:54:02.480
the rest of your stay here.