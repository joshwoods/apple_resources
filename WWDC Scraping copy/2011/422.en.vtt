WEBVTT

00:00:10.360 --> 00:00:11.260
Good afternoon, everyone.

00:00:11.260 --> 00:00:13.890
Thank you so much for coming to
today's session on Core Image.

00:00:13.970 --> 00:00:17.650
My name is David Hayward,
and the mission of Core Image is

00:00:17.730 --> 00:00:21.300
to provide a powerful yet simple
image processing framework for your

00:00:21.300 --> 00:00:23.480
applications to take advantage of.

00:00:23.800 --> 00:00:27.290
We'll be talking today about
some additions we provided

00:00:27.290 --> 00:00:30.370
to Core Image on Mac OS Lion,
but even more so,

00:00:30.550 --> 00:00:35.130
it's my true pleasure today to talk
about how you can take advantage of

00:00:35.360 --> 00:00:40.050
Core Image in the exciting platform
of iOS devices and applications.

00:00:40.300 --> 00:00:42.520
So, on with the slides.

00:00:42.540 --> 00:00:44.810
So what will we be talking about today?

00:00:44.880 --> 00:00:48.540
I'll be introducing you
today to Core Image on iOS 5.

00:00:48.540 --> 00:00:51.410
We'll be talking about
some of the key concepts,

00:00:51.410 --> 00:00:54.680
the basic architecture,
the basic classes that you'll

00:00:54.700 --> 00:00:57.990
be using in Core Image,
as well as some platform-specific

00:00:58.050 --> 00:01:00.130
details of Core Image on iOS.

00:01:00.140 --> 00:01:03.420
After that, we'll go into deeper detail
on how you can really use

00:01:03.420 --> 00:01:08.220
Core Image in your application,
how to initialize Core Image data types,

00:01:08.550 --> 00:01:11.100
how to filter images,
and how to render them.

00:01:11.260 --> 00:01:13.100
There's a lot of good detail there.

00:01:13.190 --> 00:01:17.020
And lastly, we'll be talking about some
new additions to Core Image,

00:01:17.020 --> 00:01:19.990
which we're calling image
analysis operations.

00:01:21.130 --> 00:01:25.200
So, first off,
introducing Core Image in iOS 5.

00:01:25.560 --> 00:01:27.980
Here's the basic concept of
Core Image for those of you in the room

00:01:27.980 --> 00:01:30.150
who are maybe new to this technology.

00:01:30.280 --> 00:01:35.010
The basic premise is that you can
use Core Image to filter an image

00:01:35.030 --> 00:01:37.910
to perform per pixel operations.

00:01:38.010 --> 00:01:40.460
In a very simple example,
like the one I have presented here,

00:01:40.540 --> 00:01:43.840
we have an original image,
and we're going to apply a filter to it,

00:01:43.990 --> 00:01:48.870
in this case the sepia tone filter,
and produce a new result image.

00:01:49.300 --> 00:01:51.600
So that's a very simple example,
but as it turns out,

00:01:51.720 --> 00:01:54.640
you can use Core Image to chain
together multiple filters.

00:01:54.740 --> 00:01:58.590
And this will allow you to produce much
more complex and interesting effects.

00:01:58.650 --> 00:02:00.920
As an example here,
we start with the image,

00:02:01.050 --> 00:02:04.390
we apply sepia tone to it,
we then apply a hue

00:02:04.500 --> 00:02:08.550
adjustment filter to it,
which turns it from a brownish

00:02:08.550 --> 00:02:10.700
tone image to a blue tone image.

00:02:10.770 --> 00:02:15.040
And then we apply another filter,
which will increase the contrast of the

00:02:15.040 --> 00:02:18.160
image to produce a more artistic effect.

00:02:18.230 --> 00:02:21.440
You can conceptually think of an
intermediate image existing in

00:02:21.440 --> 00:02:23.220
between each of these filters.

00:02:23.300 --> 00:02:27.010
But in order to improve performance,
what Core Image does is to

00:02:27.010 --> 00:02:29.270
concatenate these filters.

00:02:29.410 --> 00:02:34.430
Again, the key concept here is that we
implicitly eliminate intermediate

00:02:34.540 --> 00:02:37.100
buffers to improve performance.

00:02:37.170 --> 00:02:39.970
There are several other image
processing type operations,

00:02:40.040 --> 00:02:43.430
optimizations that we
do on our filter chain.

00:02:43.560 --> 00:02:47.120
For example, both the hue adjustment and
the contrast adjustment,

00:02:47.120 --> 00:02:53.050
can be equally well represented
as a color matrix in RGB space.

00:02:53.180 --> 00:02:55.760
So what one of the optimizations
Core Image can do is

00:02:55.760 --> 00:02:58.590
convert those to matrices,
and since two matrices

00:02:58.600 --> 00:03:02.410
can be concatenated,
we can concatenate those into one matrix.

00:03:02.500 --> 00:03:04.430
And again,
this further improves performance and

00:03:04.430 --> 00:03:08.680
actually improves precision as well,
reducing the amount of intermediate math.

00:03:10.560 --> 00:03:13.180
So now let me go in a little bit of
detail about the basic architecture

00:03:13.180 --> 00:03:16.740
of where Core Image fits into
the rest of the operating system.

00:03:16.870 --> 00:03:20.950
The basic architecture is that
we have applications at the top,

00:03:20.950 --> 00:03:24.840
and those applications are using
image data types on the system,

00:03:24.980 --> 00:03:29.050
such as Core Graphics data structures,
Core Video structures,

00:03:29.150 --> 00:03:33.540
or Image.io images that are returned
from the Image.io framework.

00:03:33.680 --> 00:03:37.620
You can then input these into
the Core Image framework.

00:03:37.760 --> 00:03:41.870
Core Image has a set of built-in filters,
which you can then combine

00:03:41.870 --> 00:03:45.030
in new and interesting ways,
and those are optimized

00:03:45.100 --> 00:03:46.910
by the Core Image runtime.

00:03:47.030 --> 00:03:49.910
The Core Image runtime has
two basic ways of rendering.

00:03:50.120 --> 00:03:54.200
One is a GPU rendering path,
and the other is a CPU rendering path.

00:03:54.260 --> 00:03:59.840
The GPU rendering path is
based on OpenGL ES 2.0 on iOS,

00:03:59.920 --> 00:04:05.580
and our CPU rendering path is
based on optimized CPU code,

00:04:05.630 --> 00:04:10.590
which will run using libdispatch to
take advantage of multi-core devices.

00:04:12.800 --> 00:04:14.950
So now that you have a
basic understanding of the

00:04:14.950 --> 00:04:17.170
architecture of Core Image,
let me talk about what the

00:04:17.180 --> 00:04:19.700
classes that are available
to your applications to use.

00:04:19.760 --> 00:04:22.680
There are three main classes that
you can use in your application.

00:04:22.780 --> 00:04:25.330
The first is the CI Filter class.

00:04:25.430 --> 00:04:28.400
The CI Filter is a mutable
object that represents an

00:04:28.400 --> 00:04:30.640
effect that you want to apply.

00:04:30.680 --> 00:04:34.640
The CI Filter has multiple
one or more input parameters,

00:04:34.670 --> 00:04:39.570
and those input parameters can be either
images or other numerical parameters.

00:04:39.830 --> 00:04:45.120
And the output of a filter is a new
image that's produced based on the

00:04:45.120 --> 00:04:50.250
current state of the filter at the
time that the output is requested.

00:04:51.370 --> 00:04:53.800
The second key data type is a CI image.

00:04:53.800 --> 00:04:58.840
A CI image is an immutable object that
represents the recipe for an image,

00:04:58.920 --> 00:05:00.600
for how to produce that image.

00:05:00.760 --> 00:05:03.860
And it can represent either a file
that's come just directly from disk,

00:05:03.870 --> 00:05:07.130
or the output of a filter
or chain of filters.

00:05:07.790 --> 00:05:11.340
And the third key data
type is a CI context.

00:05:11.440 --> 00:05:14.380
The CI context is the objects
through which you will render

00:05:14.750 --> 00:05:18.500
your images to your output.

00:05:18.550 --> 00:05:21.860
And as I alluded to earlier,
the CI context can either

00:05:21.860 --> 00:05:23.540
be CPU-based or GPU-based.

00:05:23.580 --> 00:05:26.320
And we'll talk in some detail about that.

00:05:27.680 --> 00:05:33.600
One thing to keep in mind is both GPU and
CPU context have their advantages.

00:05:33.600 --> 00:05:35.900
Both have their uses in your application.

00:05:35.900 --> 00:05:40.890
And there's, for example,
CPUs generally have the improved fidelity

00:05:40.890 --> 00:05:43.390
because they're fully IEEE compliant.

00:05:43.500 --> 00:05:48.900
The GPUs-based context have the advantage
of performance in most situations.

00:05:49.070 --> 00:05:53.390
CPU has the advantage that when you're
using Core Image to render using the CPU,

00:05:53.540 --> 00:05:56.660
you can do other operations on
the foreground task while you do

00:05:56.660 --> 00:05:58.890
rendering on the background thread.

00:05:58.950 --> 00:06:03.490
The GPU has the counter advantage
that you can offload the effort

00:06:03.620 --> 00:06:06.120
from the GPU to do other tasks.

00:06:06.590 --> 00:06:08.840
We'll give some examples later
on in the presentation of when

00:06:08.840 --> 00:06:13.600
you would want to use CPU and
GPU in actual use case scenarios.

00:06:15.240 --> 00:06:18.460
So as I mentioned when
I first came up on stage,

00:06:18.480 --> 00:06:21.790
it's very, very easy to do powerful image
processing in Core Image.

00:06:21.800 --> 00:06:25.870
And just to show you how easy it is,
I can show you in just four bullet items

00:06:26.080 --> 00:06:28.900
how to do rendering using Core Image.

00:06:29.020 --> 00:06:32.000
The first step is we want to
create a Core Image object.

00:06:32.110 --> 00:06:36.090
We call CI image,
image with URL or contents with URL,

00:06:36.090 --> 00:06:38.870
and that instantiates a CI image object.

00:06:39.070 --> 00:06:41.380
The next step,
we want to create a filter object.

00:06:41.480 --> 00:06:45.200
We call CI filter, filter with name,
and in this example,

00:06:45.200 --> 00:06:47.470
we'll be creating a sepia tone filter.

00:06:47.570 --> 00:06:51.000
The next thing we do on that filter
is set some default parameters.

00:06:51.000 --> 00:06:52.810
We're going to set two
parameters on this filter.

00:06:53.090 --> 00:06:56.520
The first one is the input image,
and the second one is the amount of

00:06:56.520 --> 00:06:59.380
a sepia tone effect we want to apply.

00:06:59.460 --> 00:07:02.190
The third thing we want to
do is to create a CI context.

00:07:02.210 --> 00:07:06.400
We call context with options nil to
get the default context behavior.

00:07:06.470 --> 00:07:11.280
And fourth, we want to render the output
image through that context.

00:07:11.300 --> 00:07:15.010
First thing we do here is to get
the output image of the filter,

00:07:15.010 --> 00:07:18.670
and in this very simple example,
we're going to be creating a new CG image

00:07:18.670 --> 00:07:23.890
from the results of that filter by
asking the context to create a CG image.

00:07:24.400 --> 00:07:25.640
So that's how simple it is.

00:07:25.820 --> 00:07:29.860
We'll have some more interesting
examples later on in the presentation.

00:07:31.220 --> 00:07:33.780
As I mentioned before,
Core Image has a set of

00:07:34.120 --> 00:07:35.700
built-in filters on iOS.

00:07:35.770 --> 00:07:37.780
In the seed we have today,
these are the set of filters

00:07:37.780 --> 00:07:39.300
that we have available.

00:07:39.370 --> 00:07:41.730
The filters fall into
several different categories.

00:07:41.840 --> 00:07:44.920
We have filters for adjusting
color values in images,

00:07:44.930 --> 00:07:47.000
such as hue adjust and sepia tone.

00:07:47.130 --> 00:07:50.700
We also have filters for adjusting
the geometry of an image,

00:07:50.700 --> 00:07:54.980
such as applying a fine transform
to an image or cropping an

00:07:54.980 --> 00:07:57.200
image or straightening an image.

00:07:57.250 --> 00:07:59.800
And also we have a filter for
doing composite operations,

00:07:59.870 --> 00:08:03.990
which allows you to combine
two images into a third image.

00:08:08.160 --> 00:08:09.850
Next thing I'd like to talk
about are some of the key

00:08:09.920 --> 00:08:12.860
differences between iOS and Mac OS.

00:08:12.870 --> 00:08:18.340
So in both cases, on iOS and Mac OS,
we have a set of built-in filters.

00:08:18.340 --> 00:08:22.460
On iOS, in our current release,
we have 16 built-in filters,

00:08:22.460 --> 00:08:27.580
and those are chosen to give you good
performance and also good results

00:08:27.670 --> 00:08:29.760
for photo adjustment operations.

00:08:29.760 --> 00:08:33.270
On Mac OS,
we actually have 130 built-in filters,

00:08:33.270 --> 00:08:38.110
plus the ability to add your own
developer-provided Thank you.

00:08:39.210 --> 00:08:43.340
The Core API on iOS and
Mac OS is very much the same.

00:08:43.340 --> 00:08:46.100
The three key data types
that I mentioned earlier,

00:08:46.240 --> 00:08:51.140
CI Filter, CI Image, and CI Context,
are the same on iOS and Mac OS.

00:08:51.260 --> 00:08:53.040
Mac OS does have some
additional data types,

00:08:53.040 --> 00:08:56.410
such as CI Kernel and CI Filter Shape,
which are useful if you're

00:08:56.440 --> 00:08:59.000
creating your own custom filters.

00:09:01.000 --> 00:09:03.280
In terms of performance,
there's also much in common

00:09:03.280 --> 00:09:05.500
between iOS and Mac OS.

00:09:05.500 --> 00:09:08.240
In both cases,
we do render time optimizations

00:09:08.330 --> 00:09:11.710
of your filter graph in order to
produce the best possible performance.

00:09:14.250 --> 00:09:18.240
And lastly,
there's on both iOS and Mac OS,

00:09:18.370 --> 00:09:21.200
Core Image supports both
CPU and GPU based rendering.

00:09:21.330 --> 00:09:23.740
There is a subtle
difference in that on iOS,

00:09:23.900 --> 00:09:27.940
Core Image will use OpenGL ES 2.0,
whereas on Mac OS,

00:09:28.040 --> 00:09:30.610
Core Image will use OpenGL.

00:09:32.400 --> 00:09:35.040
So now that we've given
a basic introduction,

00:09:35.050 --> 00:09:37.260
I'd like to pass the
stage over to Chendi,

00:09:37.260 --> 00:09:40.550
who will be giving a demonstration
of how to use Core Image.

00:09:44.450 --> 00:09:47.440
So this is an extremely simple demo app.

00:09:47.700 --> 00:09:50.240
Basically,
it consists of a single UI image view.

00:09:50.270 --> 00:09:52.740
And if I tap on the screen,
I have another view controller

00:09:52.740 --> 00:09:54.920
that comes up as a UI popover.

00:09:55.200 --> 00:09:58.140
And what happens is every time
I adjust one of these sliders,

00:09:58.160 --> 00:10:01.160
they're attached to the
input of a particular filter.

00:10:01.310 --> 00:10:06.240
So if I adjust brightness up and down,
the filter will render a new CG image,

00:10:06.270 --> 00:10:09.550
wrap that in a UI image,
and set that as the contents

00:10:09.550 --> 00:10:11.150
of your UI image view.

00:10:11.430 --> 00:10:15.660
And so this happens all in real time,
and it concatenates the filters.

00:10:15.660 --> 00:10:17.690
It runs pretty quickly.

00:10:17.870 --> 00:10:21.920
So I can also adjust it
to be some setting I like.

00:10:21.990 --> 00:10:23.180
Say like this.

00:10:23.180 --> 00:10:24.580
Hit Save.

00:10:24.580 --> 00:10:27.270
Then I can exit to the Photos app.

00:10:27.850 --> 00:10:30.160
and my photo is saved just like I saw it.

00:10:30.180 --> 00:10:31.580
And that's about it.

00:10:31.810 --> 00:10:33.390
Back to you, David.

00:10:39.720 --> 00:10:43.050
So now that we have a tease,
let's talk a little bit more detail

00:10:43.300 --> 00:10:47.030
about how you can really take advantage
of Core Image in your application.

00:10:47.200 --> 00:10:51.640
So I'll be talking in three key areas:
how to initialize a CI image,

00:10:51.850 --> 00:10:54.900
how to filter a CI image,
and how to render an image.

00:10:55.000 --> 00:10:58.990
So, first step we'll be talking
about is initializing a CI image.

00:10:59.510 --> 00:11:02.900
There are three basic ways
to initialize a CI image.

00:11:03.050 --> 00:11:06.070
The first is to initialize an
image based on any of the image

00:11:06.120 --> 00:11:08.500
I/O supported image file formats.

00:11:08.610 --> 00:11:14.320
You can do that by either
calling the CI image class init

00:11:14.320 --> 00:11:17.510
with URL or image with data.

00:11:17.670 --> 00:11:19.970
In both cases,
you're passing either a URL or some

00:11:19.970 --> 00:11:25.890
data that contains either JPEG or
PNG or other types of image content.

00:11:26.300 --> 00:11:32.960
The second way to initialize a CI image
is with other key image data formats that

00:11:32.960 --> 00:11:34.900
are available in our operating system.

00:11:35.020 --> 00:11:38.600
For example, you can initialize an
image with a CG image.

00:11:38.710 --> 00:11:41.910
You can also, on iOS,
you can initialize an image

00:11:41.910 --> 00:11:46.220
with a CV pixel buffer,
and this allows you to work using

00:11:46.220 --> 00:11:50.640
Core Image in conjunction with
a Core Video-based pipeline.

00:11:51.220 --> 00:11:53.730
On Mac OS,
there's a subtle difference in that the

00:11:54.200 --> 00:11:59.090
way to work with Core Video data types
is to call Image with CV Image Buffer.

00:11:59.270 --> 00:12:02.420
They're conceptually very similar,
it's just slightly different data types.

00:12:02.600 --> 00:12:08.690
The other option that's available on
Mac OS is calling Image with iOS Surface.

00:12:09.630 --> 00:12:12.170
The end result is a CI image
in any of these cases that

00:12:12.170 --> 00:12:14.330
you can then apply filters to.

00:12:14.480 --> 00:12:19.040
The third and last way that you can
create a CI image is with raw pixel data.

00:12:19.170 --> 00:12:21.800
And this is useful in many situations.

00:12:21.920 --> 00:12:25.920
You can call image with bitmap data,
you pass in NSData that

00:12:26.000 --> 00:12:29.070
represents your pixel data,
and you specify at that time

00:12:29.180 --> 00:12:31.920
what the bytes per row are,
and what the pixel format

00:12:32.080 --> 00:12:34.390
and what the color space is.

00:12:34.590 --> 00:12:37.320
So on the subject of color space,
let me talk a little bit about

00:12:37.460 --> 00:12:39.870
color management in Core Image.

00:12:40.440 --> 00:12:44.540
So on Mac OS, a CI image can be tagged
with any color space.

00:12:44.720 --> 00:12:50.210
This is provided using the
Core Graphics Data Type CG color space.

00:12:50.500 --> 00:12:53.550
If a CI image is tagged
with a color space,

00:12:53.550 --> 00:12:57.480
then all the pixels in that image
are converted to a linear working

00:12:57.480 --> 00:13:00.190
space before filters are applied.

00:13:01.080 --> 00:13:03.570
On iOS it is slightly different.

00:13:03.880 --> 00:13:07.240
A CI image can be tagged
with device RGB color space.

00:13:07.240 --> 00:13:10.370
And if it is such tagged,
then the pixels and images

00:13:10.480 --> 00:13:15.840
are gamma corrected to the
linear space before filtering.

00:13:16.350 --> 00:13:19.900
One important option that we had,
if you saw in the previous slide,

00:13:19.900 --> 00:13:22.900
the initializing methods
have an options parameter,

00:13:22.940 --> 00:13:27.340
is an option called
KCI Image Color Space.

00:13:27.460 --> 00:13:32.550
And you can use this option to override
the default color space on a CI image.

00:13:32.950 --> 00:13:36.790
One particular value that you may
be interested in is you can set the

00:13:36.790 --> 00:13:41.700
value for this key to be NSNULL if
you want to turn off color management.

00:13:41.850 --> 00:13:44.660
While in most situations you want
to have color management turned on,

00:13:44.660 --> 00:13:49.700
or in the case of iOS having
the gamma correction turned on,

00:13:49.860 --> 00:13:53.410
there are situations where you
want to work your entire rendering

00:13:53.410 --> 00:13:57.250
pipeline without color management,
perhaps to improve performance

00:13:57.320 --> 00:13:59.030
or to give a different effect.

00:14:01.770 --> 00:14:06.420
Another thing that's very interesting
about CI images on iOS is that we now

00:14:06.420 --> 00:14:09.400
have support for CI image metadata.

00:14:09.530 --> 00:14:13.640
There's a new method on CI image
that allows you to get a metadata

00:14:13.640 --> 00:14:16.600
properties dictionary from a CI image.

00:14:16.720 --> 00:14:18.350
The method is very obvious.

00:14:18.370 --> 00:14:22.600
It's just properties,
and it returns an NSDictionary.

00:14:22.750 --> 00:14:25.900
The contents of this dictionary is
the same contents as you would get if

00:14:26.030 --> 00:14:31.400
you were to call the image IO function
CG image source copy properties at index.

00:14:31.560 --> 00:14:35.600
So inside that dictionary will be
sub dictionaries for TIFF metadata,

00:14:35.600 --> 00:14:42.550
EXIF metadata, and other properties that
can come from an image file.

00:14:42.910 --> 00:14:45.590
There's one piece of metadata
that is particularly important

00:14:45.590 --> 00:14:49.030
that you should be aware of,
and that is the property that's

00:14:49.100 --> 00:14:54.640
in this dictionary under the key
KCG image property orientation.

00:14:54.750 --> 00:14:58.190
All modern cameras support
pictures being taken in any of

00:14:58.190 --> 00:15:02.240
the four possible orientations,
and when they produce an image,

00:15:02.250 --> 00:15:06.570
embedded in the metadata of that image
is a numerical value that tells you

00:15:06.790 --> 00:15:09.740
which orientation the camera was in.

00:15:10.020 --> 00:15:14.320
In the vast majority of situations,
your application will want to present

00:15:14.320 --> 00:15:17.500
the image in the desired direction of up.

00:15:17.610 --> 00:15:21.740
So you can use this orientation
property to correctly display

00:15:21.740 --> 00:15:23.970
the image to the user.

00:15:24.290 --> 00:15:26.950
There's some other important aspects
of the orientation which we'll

00:15:26.950 --> 00:15:30.590
discuss later on the presentation,
so keep this in mind.

00:15:31.210 --> 00:15:34.220
Another great feature of this new
metadata properties is it's all fully

00:15:34.220 --> 00:15:41.460
automatic if you use the image with
URL or image with data initializers.

00:15:41.680 --> 00:15:45.510
If you are initializing your
images using other data types,

00:15:45.510 --> 00:15:48.800
then you can specify the
properties on your own by using

00:15:48.800 --> 00:15:51.900
the KCI image properties option.

00:15:55.030 --> 00:15:58.280
So now we know quite a bit about
instantiating an image object.

00:15:58.420 --> 00:16:01.950
The next important step is
applying filters to that image.

00:16:02.090 --> 00:16:05.200
So let's talk about that in
a little bit more detail.

00:16:06.120 --> 00:16:09.140
So as I mentioned a few slides back,
there's a different set of

00:16:09.140 --> 00:16:12.000
built-in filters on Mac OS and iOS.

00:16:12.220 --> 00:16:14.700
And in both cases,
our set of filters that are

00:16:14.810 --> 00:16:17.570
supported will grow in the future.

00:16:18.070 --> 00:16:21.520
Your application may want to query
at runtime the set of available

00:16:21.520 --> 00:16:24.400
filters that are available on that OS.

00:16:24.580 --> 00:16:29.770
This API is a class method on CIFilter,
and it's called CIFilter

00:16:30.050 --> 00:16:32.700
Filters in Name with Category.

00:16:32.710 --> 00:16:35.490
In most cases, if you want all the
filters that are built in,

00:16:35.580 --> 00:16:39.640
you would pass in the
category KCI filter -- sorry,

00:16:39.640 --> 00:16:41.900
KCI category built in.

00:16:41.980 --> 00:16:45.490
There's other categories if you
just want to get the set of geometry

00:16:45.490 --> 00:16:49.700
filters or other subclasses of filters.

00:16:50.500 --> 00:16:53.750
The result of this call
is an array of NSStrings,

00:16:53.780 --> 00:16:56.100
the name of all the
filters that are built in.

00:16:56.200 --> 00:16:59.620
Once you have the name of a filter,
then you can use that name to

00:16:59.740 --> 00:17:01.640
instantiate a filter object.

00:17:01.740 --> 00:17:05.860
As I showed a few slides back,
you can call filter with name

00:17:05.860 --> 00:17:08.310
CI Sepia tone as an example.

00:17:08.730 --> 00:17:11.730
Once you have a filter instance,
you may want to know what are

00:17:11.730 --> 00:17:14.600
the possible parameters that
you can set on this filter.

00:17:14.740 --> 00:17:17.600
Well, you can go and look that up
on our online documentation,

00:17:17.730 --> 00:17:20.960
but there's also some runtime
documentation of each filter's

00:17:20.960 --> 00:17:23.600
parameters that is available to you.

00:17:23.860 --> 00:17:26.660
There's a call you can make
called Filter Attributes,

00:17:26.660 --> 00:17:29.590
and this returns a data structure that
gives you information about each of

00:17:29.640 --> 00:17:31.600
the input parameters on the filter.

00:17:31.640 --> 00:17:33.590
For example,
it gives you the name of each input,

00:17:33.590 --> 00:17:35.600
it gives you the expected
type of each input.

00:17:35.600 --> 00:17:39.080
For example,
if the expected parameter is an

00:17:39.080 --> 00:17:42.600
image or a vector or a number.

00:17:42.600 --> 00:17:47.070
And it also gives you, wherever possible,
the common values that are

00:17:47.200 --> 00:17:49.530
important for that input value.

00:17:49.600 --> 00:17:52.740
For example, it'll give you the default
value for that input,

00:17:52.950 --> 00:17:56.590
or the identity value,
or the minimum or the maximum value.

00:17:56.710 --> 00:18:01.600
When Chendi gave the demo a minute ago,
he was showing you some sliders

00:18:01.600 --> 00:18:04.020
that were adjusting a filter.

00:18:04.110 --> 00:18:07.810
What you can do is you can use this
API to determine what the minimum

00:18:07.810 --> 00:18:10.690
maximum values for that slider should be.

00:18:14.200 --> 00:18:16.250
So now that we know a
little bit about filters,

00:18:16.300 --> 00:18:18.180
let's talk about setting
parameters on them.

00:18:18.240 --> 00:18:20.170
This is the next step.

00:18:20.370 --> 00:18:25.400
Input parameters on filters are specified
using standard key value conventions.

00:18:25.400 --> 00:18:28.440
So we can call set value for key.

00:18:28.740 --> 00:18:31.770
Most filters take an input
image as one of their keys,

00:18:31.910 --> 00:18:36.740
so we have a common key for
that called KCI input image key.

00:18:36.890 --> 00:18:40.160
For other keys, you call set value and
let's say a number.

00:18:40.180 --> 00:18:42.710
And in the case of sepia tone,
we want to set the amount

00:18:43.050 --> 00:18:45.630
or intensity of sepia tone.

00:18:46.280 --> 00:18:49.670
Once you've set all the input parameters,
you can ask the filter

00:18:49.670 --> 00:18:51.440
for its output image.

00:18:51.810 --> 00:18:55.640
The way this is traditionally done
on Mac OS is you call cifilter

00:18:55.640 --> 00:18:58.450
valueForKey outputImageKey.

00:18:58.590 --> 00:19:00.840
And this works on both platforms.

00:19:00.940 --> 00:19:03.340
On iOS, however,
we've actually implemented the

00:19:03.340 --> 00:19:05.300
output image as a property.

00:19:05.390 --> 00:19:08.540
And that gives you two other ways
that you can get the output image.

00:19:08.630 --> 00:19:12.980
You can either call filterOutputImage
or filter.outputImage.

00:19:13.090 --> 00:19:14.640
These are all functionally equivalent.

00:19:14.640 --> 00:19:19.020
It's just a question of what
your preferred coding style is.

00:19:20.210 --> 00:19:23.480
Now, these three previous
things that I've mentioned,

00:19:23.540 --> 00:19:26.080
instantiating a filter,
setting the inputs,

00:19:26.090 --> 00:19:28.740
and getting the output,
there's a shortcut which allows

00:19:28.740 --> 00:19:30.280
you to do all of those on one line.

00:19:30.280 --> 00:19:34.350
And I'm using that shortcut convention
in these slides because it gives us a

00:19:34.350 --> 00:19:36.880
little bit more room on the presentation.

00:19:36.880 --> 00:19:40.460
The idea behind this shortcut
is you just call ciFilter,

00:19:40.460 --> 00:19:43.090
filter with name,
and then right after that

00:19:43.280 --> 00:19:45.200
you specify keys and values.

00:19:45.300 --> 00:19:50.010
And then you can specify a nil
terminated array of keys and values.

00:19:50.200 --> 00:19:53.140
And lastly, at the end of the,
after the closing bracket,

00:19:53.140 --> 00:19:55.590
we call output image to
get the output image.

00:19:55.600 --> 00:20:01.000
So that's a nice compact representation
for applying a filter to an image.

00:20:04.010 --> 00:20:07.050
So again, as I mentioned,
this is the one line of code to

00:20:07.050 --> 00:20:10.040
apply a single filter to an image.

00:20:10.290 --> 00:20:12.040
As I mentioned in the introduction,
however,

00:20:12.040 --> 00:20:13.700
you can chain together multiple filters.

00:20:13.700 --> 00:20:14.830
So how do we do that?

00:20:14.980 --> 00:20:16.420
Well, it's actually the same idea.

00:20:16.460 --> 00:20:17.650
It's very, very simple.

00:20:17.790 --> 00:20:20.440
The idea is we just
chain in another filter,

00:20:20.570 --> 00:20:24.660
but now the input to the second filter
is the output from the first filter.

00:20:24.770 --> 00:20:26.390
And that's it.

00:20:26.670 --> 00:20:29.740
One important thing to keep in
mind is that no pixel processing

00:20:29.740 --> 00:20:33.400
is actually occurring while
you're building this filter chain.

00:20:33.530 --> 00:20:38.430
All the real work of rendering the images
is deferred until a render is requested.

00:20:38.530 --> 00:20:41.700
And that brings us up to the third
section of this introduction,

00:20:41.700 --> 00:20:44.100
which is more detail about how to render.

00:20:44.150 --> 00:20:48.800
And to discuss that, I'd like to bring
Chendi back up on stage.

00:20:53.710 --> 00:20:55.200
Thanks, David.

00:20:55.200 --> 00:20:57.410
All right, at this point,
you know how to create a

00:20:57.430 --> 00:21:00.460
filter chain and get an output
image from your last filter.

00:21:00.480 --> 00:21:02.200
So what can you do with this?

00:21:02.200 --> 00:21:04.700
As we saw in the earlier demo,
an easy way to get it on screen

00:21:04.700 --> 00:21:08.500
is to use a UI image view and
set the image as the contents.

00:21:08.500 --> 00:21:10.920
You can also save the
result to the photo library.

00:21:10.920 --> 00:21:12.520
And if you're writing an
image manipulation app,

00:21:12.520 --> 00:21:15.940
this will be a pretty common
operation for your users.

00:21:15.940 --> 00:21:19.960
Another method of drawing on screen
is using a CA Eagle layer-backed view.

00:21:19.960 --> 00:21:23.460
This is actually more performant
than using a UI image view.

00:21:23.460 --> 00:21:27.700
And it's similar to how most OpenGL apps
on the App Store render their content.

00:21:27.710 --> 00:21:31.800
And lastly, we're going to discuss how to
run it to a CV pixel buffer.

00:21:31.830 --> 00:21:36.730
You can use this rendering method
to use Core Image to process video

00:21:36.830 --> 00:21:41.250
frames in a larger AV foundation
or a Core Video-based pipeline.

00:21:43.100 --> 00:21:48.320
So if you want to set a UI image view's
contents to your rendered CI image,

00:21:48.490 --> 00:21:51.640
you need a context and you need
the image you want to render.

00:21:51.670 --> 00:21:54.410
With one line,
you can just call context createCgImage

00:21:54.510 --> 00:21:58.650
from rect with the area of interest
cgRect that you want to render.

00:21:58.770 --> 00:22:01.170
This will give you a new CG image ref.

00:22:01.200 --> 00:22:03.150
Now,
you can wrap this easily in a UI image

00:22:03.370 --> 00:22:05.500
for display in your image view.

00:22:05.510 --> 00:22:09.940
Also note that since you have a CG image,
you can also use normal Core graphics

00:22:09.940 --> 00:22:13.090
function calls and also set the
contents of a base CA layer,

00:22:13.190 --> 00:22:14.940
for instance,
anything that you would normally

00:22:14.940 --> 00:22:16.100
use the CG image ref for.

00:22:19.450 --> 00:22:22.420
One thing to note is that there
is an orientation flag that you

00:22:22.420 --> 00:22:23.890
might want to set for UI image.

00:22:24.150 --> 00:22:27.260
When you take a photo with
your iPhone or iPad 2,

00:22:27.260 --> 00:22:31.250
and if it's in landscape or upside down,
there's going to be a metadata

00:22:31.250 --> 00:22:35.520
flag or metadata key that
says what the orientation is.

00:22:35.580 --> 00:22:39.960
Most SLRs and point-and-shoots also
have information similar to this.

00:22:40.020 --> 00:22:42.870
Because of this,
you're going to want to map the

00:22:42.870 --> 00:22:46.720
CG orientation to the equivalent
UI image orientation enum so

00:22:46.740 --> 00:22:50.750
the UI image view orients the
image properly in the image view.

00:22:53.400 --> 00:23:53.000
[Transcript missing]

00:23:55.460 --> 00:23:59.520
So now that you know that,
how do you save to the photo library?

00:23:59.520 --> 00:24:02.660
Well, the code for that is identical
with the CPU context as it

00:24:02.660 --> 00:24:04.100
is with the GPU context.

00:24:04.120 --> 00:24:07.130
You call the same method,
createCgImage from rect

00:24:07.130 --> 00:24:08.700
to get the CG image ref.

00:24:08.710 --> 00:24:12.720
Then you can use assets library framework
to call writeImageToSavePhotosAlbum

00:24:12.750 --> 00:24:15.300
to save your image to the photo album.

00:24:15.300 --> 00:24:19.770
And those two or three lines are all you
need to do to save to the photo roll.

00:24:22.570 --> 00:24:26.670
The other method of rendering I discussed
is CA Eagle Layer Backed Views.

00:24:26.800 --> 00:24:30.260
So it turns out if you use a
UI image view to display your image,

00:24:30.300 --> 00:24:33.460
there's going to be implicit
call to GL read pixels every

00:24:33.550 --> 00:24:35.420
time you create the CG image.

00:24:35.470 --> 00:24:37.700
And after you set the
contents of the UI image view,

00:24:37.870 --> 00:24:40.540
Core Animation has to do
another GPU upload to get the

00:24:40.540 --> 00:24:42.020
contents into its own memory.

00:24:42.340 --> 00:24:45.450
So this turns out to be
slower than you want.

00:24:45.530 --> 00:24:47.850
In our example app,
since we weren't doing that

00:24:47.890 --> 00:24:50.620
complex of a filter chain,
it was still pretty performant.

00:24:50.650 --> 00:24:54.380
But if you've got any sort of
complex filter chain or large images,

00:24:54.410 --> 00:24:55.760
it might be too slow.

00:24:55.800 --> 00:24:58.330
When you have stuff like that,
you're going to want to use a

00:24:58.350 --> 00:25:00.230
CA Eagle Layer to display the images.

00:25:00.280 --> 00:25:04.420
And again, this is how most OpenGL games
display the content.

00:25:04.780 --> 00:25:06.430
So what you do is you
create a CA Eagle Layer,

00:25:06.460 --> 00:25:09.400
which has a corresponding
Eagle context that contains the

00:25:09.400 --> 00:25:12.180
frame buffer and the render buffer.

00:25:12.210 --> 00:25:15.550
One thing to note is you must
create your CI context with

00:25:15.830 --> 00:25:19.510
the exact same Eagle context
that backs the CA Eagle Layer.

00:25:22.090 --> 00:25:24.680
Normally in an open GL game,
what happens is you'll

00:25:24.790 --> 00:25:27.940
issue a bunch of GL code,
a bunch of GL calls to draw stuff,

00:25:27.990 --> 00:25:31.090
and then when you're ready to display,
you call glBindRenderBuffer and

00:25:31.140 --> 00:25:33.830
then egoContextPresentRenderBuffer.

00:25:33.950 --> 00:25:35.740
For Core Image, it's very similar.

00:25:35.820 --> 00:25:38.350
Instead of the GL calls,
you set up your filter chain

00:25:38.540 --> 00:25:42.860
and finally call the new method
contextDrawImageAtPointFromRect.

00:25:42.930 --> 00:25:47.620
This will draw your CI image onto the
render buffer for that ego context.

00:25:47.700 --> 00:25:49.960
You're not done at this point
because you still have to display it,

00:25:50.030 --> 00:25:53.230
so you still have to call the
GLBindRenderBuffer and present render

00:25:53.230 --> 00:25:57.300
buffer like you would in a normal game.

00:25:57.350 --> 00:25:59.950
And to demonstrate that,
I have a demo app.

00:26:01.900 --> 00:26:04.300
So in this app,
I'm using AV Foundation to

00:26:04.380 --> 00:26:08.080
stream live video from the front
camera as CV pixel buffers.

00:26:08.090 --> 00:26:12.170
I then wrap the CV pixel buffers in
a CI image and use the code from the

00:26:12.260 --> 00:26:14.420
previous slide to display on screen.

00:26:14.420 --> 00:26:16.700
And just like the first demo,
I can use the UI popover to bring

00:26:16.700 --> 00:26:20.310
up a little bunch of sliders
and adjust stuff in real time,

00:26:20.390 --> 00:26:25.140
like contrast, saturation, temperature,
and so on.

00:26:25.140 --> 00:26:28.960
And this all happens in real time,
and that's about it.

00:26:28.960 --> 00:26:32.070
So, yeah.

00:26:36.090 --> 00:26:40.100
And the last method I'm going to discuss
today is rendering to a CVPixel buffer.

00:26:40.150 --> 00:26:41.790
The API for this is pretty simple.

00:26:41.830 --> 00:26:45.370
It's similar to the createCG method,
except you have to have a CVPixel

00:26:45.370 --> 00:26:47.460
buffer to already render into.

00:26:47.630 --> 00:26:49.400
Once you have your
context and your image,

00:26:49.500 --> 00:26:53.870
call renderToCVPixelBuffer with the
appropriate bounds and color space.

00:26:54.200 --> 00:26:57.130
And once you have this,
it allows you to use Core Image to

00:26:57.190 --> 00:27:02.360
process individual frames within a
larger Core Video AV Foundation pipeline.

00:27:02.390 --> 00:27:05.880
And to demonstrate this,
I have a slightly modified demo

00:27:06.400 --> 00:27:08.060
to show you.

00:27:08.170 --> 00:27:10.020
So this is identical
to the previous demo,

00:27:10.040 --> 00:27:12.820
except I have an additional
record button here.

00:27:13.000 --> 00:27:14.730
So when I hit record,
instead of rendering

00:27:14.730 --> 00:27:17.230
directly to the screen,
my app first renders

00:27:17.270 --> 00:27:21.580
to a CV pixel buffer,
passes that on to an H.264 encoder,

00:27:21.630 --> 00:27:25.720
and also uses that same
image to display on screen.

00:27:25.800 --> 00:27:30.470
So I can hit record,
adjust stuff like hue, tint, temperature,

00:27:30.470 --> 00:27:33.050
saturation, just like before.

00:27:33.940 --> 00:27:35.570
Stop recording.

00:27:35.600 --> 00:27:39.180
If I go to my Photos library, yeah,
so you can see all the live edits

00:27:39.180 --> 00:27:42.370
I made show up in the encoded video.

00:27:42.470 --> 00:27:44.390
So yeah, that's it.

00:27:48.880 --> 00:27:52.640
So I'll start with the previous
demo that didn't do the recording.

00:27:52.690 --> 00:27:56.130
The AV Foundation callback
looks like this.

00:27:56.430 --> 00:27:59.600
Basically, we grab the CV Pixel Buffer
from the sample buffer,

00:27:59.690 --> 00:28:04.940
create a CI image around it,
and we turn off color matching.

00:28:05.020 --> 00:28:07.540
So we pass in NSNull for the color space.

00:28:07.580 --> 00:28:10.440
Now, CV Pixel Buffers come in
rotated by 90 degrees,

00:28:10.530 --> 00:28:13.250
so we have to rotate it for
display on screen by applying a

00:28:13.250 --> 00:28:16.510
CG affine transform of 90 degrees.

00:28:16.710 --> 00:28:18.920
We passed the image
through our set of filters.

00:28:19.000 --> 00:28:20.910
In this case,
the six sliders corresponded to

00:28:21.290 --> 00:28:23.630
inputs on three separate filters.

00:28:23.750 --> 00:28:26.350
Once we have our result image,
we can draw directly to the

00:28:26.460 --> 00:28:30.240
GLES context with the draw
image in Rect from Rect Call,

00:28:30.320 --> 00:28:33.300
and then present it to the screen.

00:28:33.430 --> 00:28:35.600
And for the second demo,

00:28:39.270 --> 00:28:40.900
It's slightly more complex.

00:28:40.950 --> 00:28:43.740
There is a bunch of AV Foundation code
I'm not going to include.

00:28:43.850 --> 00:28:46.180
It's mostly boilerplate code
to set up your encoder and

00:28:46.480 --> 00:28:49.430
append sample buffers to that.

00:28:49.520 --> 00:28:51.090
But basically what you
have now is the same stuff.

00:28:51.170 --> 00:28:52.940
You have the pixel buffer.

00:28:52.990 --> 00:28:55.440
In this case,
since we're rendering to a pixel buffer,

00:28:55.480 --> 00:28:59.580
we set up a CVPixelBufferPool to
handle the buffers more efficiently.

00:28:59.620 --> 00:29:02.900
And you can read more about
that in the AV Foundation docs.

00:29:02.980 --> 00:29:05.900
Create the CG image again
with the null color space.

00:29:06.050 --> 00:29:07.360
Filter the image.

00:29:07.410 --> 00:29:09.080
And since we're not
rendering to the screen yet,

00:29:09.080 --> 00:29:12.250
we don't have to rotate the
image until we finally render.

00:29:12.370 --> 00:29:15.990
So we initially render
to a CVPixelBuffer here.

00:29:17.720 --> 00:29:21.060
We wrap the CVE fix buffer
in another CI image,

00:29:21.140 --> 00:29:25.340
rotate it,
and we draw the rotated one onto screen,

00:29:25.450 --> 00:29:30.290
and then we append the non-rotated
one to our AV asset writer.

00:29:30.400 --> 00:29:35.140
And that's how you can encode video and
display it to screen at the same time.

00:29:35.170 --> 00:29:37.180
Back to slides.

00:29:40.200 --> 00:29:44.060
So there's a couple of key performance
best practices to keep in mind.

00:29:44.060 --> 00:29:47.540
Firstly is that CI images and
CI filters are auto-released.

00:29:47.650 --> 00:29:50.980
So if you're doing a complex or long
chain of filtering and rendering,

00:29:51.030 --> 00:29:54.340
you're going to want to wrap your stuff,
your code, in auto-release pools.

00:29:54.410 --> 00:29:56.590
That way you can reduce memory
pressure in case a CI image is

00:29:56.680 --> 00:29:59.740
holding onto a large image in
the background or something.

00:30:00.300 --> 00:30:02.230
When you're creating
your CVPixel buffers,

00:30:02.380 --> 00:30:06.630
try to include the key
KCVPixelBufferIOSurfaceProperties key.

00:30:06.740 --> 00:30:10.100
This creates the CVPixelBuffer
with the iOSurface backing,

00:30:10.190 --> 00:30:12.600
and that allows us to do some
secret sauce stuff in the background

00:30:12.600 --> 00:30:15.300
to make the rendering faster.

00:30:15.670 --> 00:30:16.900
The next point is very key.

00:30:16.900 --> 00:30:20.240
Do not create a CI context
every time you render.

00:30:20.310 --> 00:30:23.430
That's equivalent to writing
an OpenGL app and creating a

00:30:23.470 --> 00:30:25.540
GL context for every frame.

00:30:25.600 --> 00:30:28.360
And CI context generally
include a Eagle context,

00:30:28.440 --> 00:30:31.790
so this is going to really slow
down your app if you do so.

00:30:32.080 --> 00:30:34.940
Another point is that
because Core Animation and

00:30:35.060 --> 00:30:37.850
Core Image both use the GPU,
they could contend for

00:30:37.850 --> 00:30:38.740
the same resources.

00:30:38.870 --> 00:30:42.710
So you should try avoiding
scheduling CA animations while you're

00:30:42.710 --> 00:30:45.150
performing CI rendering on the GPU.

00:30:45.330 --> 00:30:48.270
If you've written enough
Core Animation code,

00:30:48.430 --> 00:30:51.740
you know that once you have a certain
number of layers or animations,

00:30:51.810 --> 00:30:54.490
it starts to get a bit stuttery
when things get too complex.

00:30:54.690 --> 00:30:57.350
And this will only get worse if you
try to render with Core Image at

00:30:57.350 --> 00:30:59.150
the same time as these animations.

00:30:59.280 --> 00:31:03.260
So either use a CPU context to
render when you have animations,

00:31:03.320 --> 00:31:07.020
or just schedule a very simple animation
or no animation at all on screen while

00:31:07.090 --> 00:31:08.920
you're performing your rendering.

00:31:10.770 --> 00:31:14.480
Note again that both contexts
have maximum image sizes.

00:31:14.620 --> 00:31:18.200
You can query the sizes with
the input image maximum size

00:31:18.200 --> 00:31:22.410
and output image maximum size
calls on the context for iOS.

00:31:22.510 --> 00:31:26.400
Again,
this is 2K by 2K on older iOS hardware,

00:31:26.590 --> 00:31:30.340
4K by 4K on the iPad 2,
and the CPU context

00:31:30.410 --> 00:31:32.520
supports up to 8K by 8K.

00:31:32.930 --> 00:31:34.400
And the last point is
pretty important too.

00:31:34.590 --> 00:31:37.650
Performance generally scales
linearly with the number of output

00:31:37.650 --> 00:31:39.940
destination pixels you have to render.

00:31:40.040 --> 00:31:43.880
So if you have an iPad 2,
which is a 1024 by 768 screen,

00:31:43.900 --> 00:31:46.860
there's no point rendering a 4
megapixel image and then displaying

00:31:46.860 --> 00:31:48.500
that on screen scaled down.

00:31:48.520 --> 00:31:50.970
What you should do is try to
render a screen size image

00:31:51.200 --> 00:31:52.860
for optimal performance.

00:31:52.880 --> 00:31:56.890
And when you're performing your final
save to disk or to photo library,

00:31:56.960 --> 00:32:02.200
you can use the CPU context to render
the large image in the background thread.

00:32:02.210 --> 00:32:05.230
And to get smaller images,
you can either use Core Graphics

00:32:05.230 --> 00:32:08.830
or Image IO that have APIs to give
you either thumbnails or cropped

00:32:09.010 --> 00:32:11.600
sub-reqs of your current image.

00:32:11.620 --> 00:32:14.910
And with that,
I'm going to give the mic back to David.

00:32:19.300 --> 00:32:20.800
Thank you very much, Chendi.

00:32:20.930 --> 00:32:24.040
So the third section of today's
discussion I'd like to talk about is

00:32:24.050 --> 00:32:26.440
something we're calling image analysis.

00:32:26.580 --> 00:32:28.630
Previously we've been talking
about image filtering,

00:32:28.720 --> 00:32:33.440
and filtering has the properties that
it's very cheap to set up a filter chain

00:32:33.710 --> 00:32:36.860
and all the work is done at render time.

00:32:37.000 --> 00:32:41.560
Image analysis, however,
goes beyond those normal constraints.

00:32:41.670 --> 00:32:45.030
The idea is that for a
large class of operations,

00:32:45.030 --> 00:32:48.560
you may need to read the entire
image in order to figure out

00:32:48.590 --> 00:32:50.260
what you want to do to the image.

00:32:50.420 --> 00:32:55.660
So we have two types of image
analysis tools that are now

00:32:55.730 --> 00:33:00.860
available in Core Image that
we're introducing to you on iOS.

00:33:00.990 --> 00:33:04.180
One is face detection,
and the second is auto enhance,

00:33:04.180 --> 00:33:06.760
and we'll talk about them in that order.

00:33:06.900 --> 00:33:10.330
So first, let's talk about
Core Image face detection.

00:33:10.920 --> 00:33:14.090
So we all have images in our
photo library with faces in them,

00:33:14.110 --> 00:33:18.090
and one very nice thing to do is to
know where those faces are in the image.

00:33:18.200 --> 00:33:20.480
Note that face detection is
a different problem than face

00:33:20.570 --> 00:33:22.250
identification or face recognition.

00:33:22.380 --> 00:33:24.630
Today we're talking about face detection.

00:33:24.750 --> 00:33:28.180
And there's a lot of stuff you can do if
you know where the faces are in an image.

00:33:28.200 --> 00:33:31.840
There's lots of interesting image
optimizations you can do if you know

00:33:31.840 --> 00:33:36.590
how to crop or improve the quality of an
image if you know where the faces are.

00:33:37.360 --> 00:33:38.960
So, how can we do that?

00:33:39.090 --> 00:33:42.700
Well, we have a new Core Image based
face detection API.

00:33:42.840 --> 00:33:48.700
And this API is available and identical
on both iOS 5 and Mac OS Lion.

00:33:48.700 --> 00:33:51.140
It's a very simple to use API.

00:33:51.210 --> 00:33:52.500
It has two classes.

00:33:53.100 --> 00:33:56.170
First class is a CI detector,
and the second class is

00:33:56.170 --> 00:33:59.230
the result of a detection,
which is a CI feature,

00:33:59.410 --> 00:34:01.700
specifically CI face feature.

00:34:01.700 --> 00:34:05.700
We've designed this API to
be feature upgradable,

00:34:05.760 --> 00:34:07.700
while today we just have face features.

00:34:07.700 --> 00:34:10.430
You can imagine that that could
grow in the future to different

00:34:10.430 --> 00:34:12.140
types of detected objects.

00:34:13.430 --> 00:34:14.990
So, how do we use this API?

00:34:15.000 --> 00:34:16.220
As I said, it's very simple.

00:34:16.370 --> 00:34:18.440
First thing we do is create a detector.

00:34:18.620 --> 00:34:23.200
We call a CI detector, detector of type,
and this time we specify that

00:34:23.240 --> 00:34:26.560
we want the one and only type,
which is detector type face.

00:34:26.560 --> 00:34:29.560
And at this time,
we can also specify some options.

00:34:29.560 --> 00:34:31.640
So, what options are important?

00:34:31.640 --> 00:34:35.750
Well, there's a very important option,
which allows you to tell the detector

00:34:35.870 --> 00:34:37.730
whether to be fast or thorough.

00:34:38.520 --> 00:34:41.000
Face detection,
like many problems in life,

00:34:41.040 --> 00:34:44.890
you can choose between having the best
answer in a long amount of time or a

00:34:44.970 --> 00:34:47.400
good answer in a shorter amount of time.

00:34:47.400 --> 00:34:51.410
And while in many cases the low accuracy,
high performance is suitable,

00:34:51.410 --> 00:34:54.550
there may be certain applications
that want to take longer

00:34:54.550 --> 00:34:58.540
and produce a better answer,
for example, finding smaller faces.

00:35:00.400 --> 00:35:04.510
So you have that choice in your
application to direct the detector

00:35:04.760 --> 00:35:06.950
whether to be fast or thorough.

00:35:09.080 --> 00:35:12.250
Second step is to search for
the features in an image.

00:35:12.470 --> 00:35:15.000
We pass in the detector
object we created,

00:35:15.060 --> 00:35:18.040
and we pass in the image we
want to look for features in.

00:35:18.190 --> 00:35:21.310
And we can specify options,
and the result is an array

00:35:21.310 --> 00:35:23.460
of detected features.

00:35:23.920 --> 00:35:26.920
In this case,
the options are also important.

00:35:27.030 --> 00:35:31.690
In the case of faces,
the detector needs to know what

00:35:31.690 --> 00:35:35.700
direction is up in the image in
order to look for upright faces.

00:35:35.870 --> 00:35:38.730
So this is a very important property.

00:35:38.860 --> 00:35:45.790
The way you pass it in is via the key
value KCI detector image orientation.

00:35:45.910 --> 00:35:49.260
In the vast majority of situations,
the value you're going to pass in

00:35:49.440 --> 00:35:53.300
for this key is the orientation
metadata that came from the image.

00:35:53.390 --> 00:35:55.100
So in this case, we're doing two things.

00:35:55.100 --> 00:35:59.000
We're getting the image properties and
getting the orientation key from that,

00:35:59.030 --> 00:36:04.690
and then passing that in as the value for
the key CI detector image orientation.

00:36:05.030 --> 00:36:09.320
And again, the purpose of this is to tell
the detector what direction up is.

00:36:09.420 --> 00:36:12.840
Once you call the detector and
it returns an array of features,

00:36:12.930 --> 00:36:16.300
you can loop over those features
to do whatever you want.

00:36:16.300 --> 00:36:19.090
In this very simple code example,
we're going to do a few things.

00:36:19.220 --> 00:36:21.750
First thing we're going to do is
ask the -- we're going to loop

00:36:21.750 --> 00:36:24.400
over the features F in the array.

00:36:24.480 --> 00:36:27.630
We're going to ask the
feature F for its bounds.

00:36:27.720 --> 00:36:32.100
All features have bounds,
and those bounds are a CG rectangle

00:36:32.350 --> 00:36:36.190
in the Core Image coordinate space,
which is lower left-hand based.

00:36:36.310 --> 00:36:38.060
So in this example,
we're just going to print that

00:36:38.060 --> 00:36:40.800
to the console using NSLog.

00:36:41.210 --> 00:36:43.470
The other thing we can do on
a face feature is query it

00:36:43.650 --> 00:36:47.500
for interesting sub-features,
such as the left eye location,

00:36:47.500 --> 00:36:49.490
the right eye location,
and the mouth location.

00:36:49.500 --> 00:36:51.360
And again, this is very simple.

00:36:51.360 --> 00:36:57.040
The CI face feature class has properties,
such as has left eye position,

00:36:57.040 --> 00:36:58.780
and that returns a Boolean.

00:36:58.780 --> 00:37:01.540
And if that is true,
then you can query for the

00:37:01.690 --> 00:37:04.950
property left eye position,
and that returns a point

00:37:04.950 --> 00:37:06.650
with an X and a Y value.

00:37:06.660 --> 00:37:09.980
So again, very,
very simple in your application.

00:37:11.720 --> 00:37:14.210
So to show this in practice,
and we'll be showing this

00:37:14.280 --> 00:37:18.690
on both Mac OS and iOS,
I'm going to welcome up to the

00:37:18.690 --> 00:37:23.780
stage Piotr Maj to talk about how
to use our face detection API.

00:37:23.850 --> 00:37:25.610
David.

00:37:28.580 --> 00:37:35.200
So I would like to show you today how to
take advantage of new CI detector API,

00:37:35.310 --> 00:37:38.540
first on Mac OS, then on iOS.

00:37:39.120 --> 00:37:44.330
So the best way to experiment
with graphical filters on

00:37:44.330 --> 00:37:46.440
Mac OS is Quartz Composer.

00:37:46.540 --> 00:37:52.530
So let me show you how to wire up
simple Quartz Composer composition,

00:37:52.530 --> 00:37:55.310
which performs face detection.

00:37:55.880 --> 00:38:00.660
As you see on the output image,
it consists of the background

00:38:00.790 --> 00:38:05.660
image and face frames,
one frame on top of each

00:38:05.660 --> 00:38:08.560
face detected in this image.

00:38:08.680 --> 00:38:14.290
So, how does this composition look like?

00:38:14.570 --> 00:38:16.700
First, let's do the background.

00:38:16.710 --> 00:38:17.800
The background is simple.

00:38:17.980 --> 00:38:23.930
We just start with input image and
pass it directly to a billboard patch,

00:38:23.930 --> 00:38:29.740
which is responsible for
displaying the image on screen.

00:38:29.740 --> 00:38:31.730
That was quick and easy.

00:38:31.820 --> 00:38:38.190
Now, how to draw the faces,
the face frames on top of faces.

00:38:38.270 --> 00:38:43.170
To do this, we need to use a new patch
in Quartz Composer on Lion,

00:38:43.230 --> 00:38:44.830
which is called Detector.

00:38:44.840 --> 00:38:46.750
It's located here.

00:38:46.850 --> 00:38:52.000
And it takes image as an input,
entry times,

00:38:52.300 --> 00:38:56.040
An array of face features.

00:38:56.170 --> 00:39:01.000
We can look at it by pointing
mouse over these features.

00:39:01.000 --> 00:39:06.900
As we see, we have four faces,
four elements of the structure.

00:39:06.990 --> 00:39:10.630
The rest of the job is done in the loop.

00:39:11.110 --> 00:39:13.270
represented by iterator patch.

00:39:13.460 --> 00:39:18.480
I will now jump into this iterator
patch by double-clicking on it.

00:39:23.330 --> 00:39:25.200
Let's make it bigger.

00:39:25.280 --> 00:39:28.770
It may look scary,
but actually it's very simple.

00:39:28.850 --> 00:39:33.120
All it comes down to is we
extract from each face feature

00:39:33.460 --> 00:39:35.440
its individual components.

00:39:35.600 --> 00:39:43.550
So here we extract X position, Y, width,
and height.

00:39:44.410 --> 00:39:47.600
We can look at these values.

00:39:47.650 --> 00:39:51.170
I don't know if you see it,
but this value is pretty low.

00:39:51.180 --> 00:39:54.470
It's 0.26 for X value.

00:39:54.470 --> 00:39:56.680
Why is it so low?

00:39:56.680 --> 00:40:02.020
It is so low because detector
patch uses Quartz Composer's

00:40:02.020 --> 00:40:05.700
normalized coordinate system,
which is from minus one to one.

00:40:05.700 --> 00:40:13.640
So in order to use it on real image and
to see actually the result on the screen,

00:40:13.640 --> 00:40:18.720
we need to convert it to
CG image coordinate system,

00:40:18.720 --> 00:40:21.640
which is pixel based.

00:40:21.670 --> 00:40:30.630
This job is done in a Core Image filter,
which uses some simple kernel.

00:40:31.360 --> 00:40:37.610
To first do the math,
here is the part which

00:40:37.670 --> 00:40:42.660
converts coordinates to pixels.

00:40:42.660 --> 00:40:50.840
And it's also responsible for
drawing the square on top of face.

00:40:50.840 --> 00:40:55.710
And the output of this filter is sent
directly to the billboard patch and the

00:40:55.710 --> 00:40:59.130
face frame is drawn on the second frame.

00:41:00.570 --> 00:41:03.630
We repeat this step for every
face we found in the image,

00:41:03.630 --> 00:41:05.870
and this produces...

00:41:06.700 --> 00:41:08.900
The output image.

00:41:08.950 --> 00:41:14.980
So this works on still images,
but there is absolutely no reason why

00:41:14.980 --> 00:41:20.650
we could not attach video input here.

00:41:26.400 --> 00:41:28.300
So it's tracking me in real time.

00:41:28.300 --> 00:41:36.090
That's the Mac OS part.

00:41:36.410 --> 00:41:43.230
Now for next demo,
I need to switch to my iPad.

00:41:43.270 --> 00:41:45.490
OK, here it is.

00:41:47.850 --> 00:41:53.840
This is a simple app which
allows me to pick an image.

00:41:56.010 --> 00:42:01.400
And we have faces detected in
just a fraction of a second.

00:42:03.630 --> 00:42:07.400
Two faces or even five.

00:42:07.480 --> 00:42:11.340
So this simple app.

00:42:11.340 --> 00:42:14.990
And how does the code look like?

00:42:20.130 --> 00:42:24.500
The interesting part starts
in ImagePicker controller.

00:42:24.500 --> 00:42:28.280
Picker did finish picking
media with info method.

00:42:28.370 --> 00:42:32.410
This is a callback from UI ImagePicker.

00:42:32.620 --> 00:42:38.400
And we get reference to image
user chosen from the library,

00:42:38.500 --> 00:42:43.000
and perform the detection in detect
faces and draw face boxes method.

00:42:43.110 --> 00:42:46.800
Please note that I sent
this job to the background.

00:42:46.850 --> 00:42:50.900
It's good practice to offload
this job to the background,

00:42:50.920 --> 00:42:56.440
not giving the chance for UI to freeze.

00:42:56.610 --> 00:43:01.810
So let's jump to this method
and see what's happening here.

00:43:03.360 --> 00:43:08.180
First,
we create CI image from our UI image.

00:43:08.180 --> 00:43:14.170
Then we instantiate CI detector
with accuracy level of our choice.

00:43:14.900 --> 00:43:20.880
Then we perform the actual detection
by invoking features in image method.

00:43:20.890 --> 00:43:24.300
This method returns an array of faces.

00:43:25.240 --> 00:43:28.530
Over which we can iterate
to compose our final image.

00:43:28.640 --> 00:43:34.100
Our final image will be done
using only Core Image filters.

00:43:34.100 --> 00:43:40.960
So, for each face,
I create face box or face frame for image

00:43:41.150 --> 00:43:44.100
in this face box image for face method.

00:43:44.100 --> 00:43:49.660
And using source over compositing filter,
I attach this reddish square

00:43:49.710 --> 00:43:52.100
on top of background image.

00:43:52.100 --> 00:43:55.100
I repeat this step for every face.

00:43:55.180 --> 00:44:00.100
So, finally, every face finds its
way to background image.

00:44:00.100 --> 00:44:04.140
If you're curious how I create
this reddish square using

00:44:04.140 --> 00:44:08.100
only Core Image filters,
here it is.

00:44:08.100 --> 00:44:15.100
I use CI constant color generator
filter which generates constant color,

00:44:15.100 --> 00:44:17.100
like its name says.

00:44:17.350 --> 00:44:20.090
This color is on infinite dimensions.

00:44:20.100 --> 00:44:24.100
So, infinity is probably too
much for our purpose.

00:44:24.100 --> 00:44:32.070
So, we'll use another filter, CI crop,
to crop this infinite color

00:44:32.080 --> 00:44:35.080
to reflect the face bounce.

00:44:35.100 --> 00:44:38.100
Here, input rectangle, face bounce.

00:44:38.100 --> 00:44:47.100
And we just use output image property
of CI filter to get this box.

00:44:47.100 --> 00:44:55.100
Now, having this image ready, we can use,
as David showed in presentation,

00:44:55.100 --> 00:44:59.100
CI context, default CI context to
render this to CG image.

00:44:59.100 --> 00:45:06.090
And now it's trivial to just put it as
an input for UI image and UI image view.

00:45:06.100 --> 00:45:08.090
What's important?

00:45:08.120 --> 00:45:11.700
This code, the detection part,
you could copy paste to

00:45:11.700 --> 00:45:16.100
Mac OS application and it
will look exactly the same.

00:45:16.100 --> 00:45:19.210
It's pretty easy.

00:45:19.880 --> 00:45:22.770
So that's how it looks on Mac OS.

00:45:22.790 --> 00:45:26.760
I have one bonus demo to show.

00:45:26.800 --> 00:45:35.030
So here I am.

00:45:35.030 --> 00:45:36.690
But not only me.

00:45:44.940 --> 00:45:45.570
Thank you very much.

00:45:45.610 --> 00:45:48.140
David?

00:45:48.200 --> 00:45:51.080
I want to give thanks to
the AV Foundation folks who

00:45:51.080 --> 00:45:55.080
did the fun mustache demo,
and we happily stole it for

00:45:55.080 --> 00:45:57.280
our presentation as well.

00:45:57.430 --> 00:45:58.820
So that's face detection.

00:45:59.020 --> 00:46:02.400
And as I mentioned a little earlier,
if you know where faces are in an image,

00:46:02.400 --> 00:46:04.240
there's lots of other
interesting things you can do,

00:46:04.240 --> 00:46:08.560
including help improve the image based
on whether faces are present or not.

00:46:08.690 --> 00:46:11.560
And that brings us to the next
subject of our discussion today,

00:46:11.620 --> 00:46:14.510
which is automatically
enhancing an image.

00:46:14.780 --> 00:46:17.770
This was one of the things that
we demonstrated on Monday where we

00:46:17.770 --> 00:46:22.230
were talking about some of the new
features in iOS 5 where we have

00:46:22.240 --> 00:46:28.090
one-touch improvement to your images
to produce a better image than you

00:46:28.090 --> 00:46:30.330
would necessarily get by default.

00:46:30.840 --> 00:46:33.350
Great thing is we've actually
provided this as an API for

00:46:33.350 --> 00:46:35.550
all of you guys to use as well.

00:46:35.710 --> 00:46:37.720
So how does auto enhance work?

00:46:37.940 --> 00:46:43.430
Well, like other problems of this class,
we need to analyze the

00:46:43.430 --> 00:46:44.900
entire image first.

00:46:44.990 --> 00:46:48.090
And the idea is we analyze the
entire image for its histogram,

00:46:48.230 --> 00:46:51.550
for its face region contents,
and also for other bits of

00:46:51.550 --> 00:46:53.290
metadata that are present.

00:46:53.660 --> 00:46:56.080
Once we have analyzed
for that information,

00:46:56.110 --> 00:47:00.590
the Auto Enhancement API will
return to you an array of CI filters

00:47:01.040 --> 00:47:05.590
that are custom designed to be
applied to the image in question.

00:47:05.960 --> 00:47:07.310
Let me just repeat this one more time.

00:47:07.400 --> 00:47:11.880
So the array of filters that we return
have had all the input parameters set up

00:47:11.980 --> 00:47:16.900
and customized for that specific image
to produce the best possible output.

00:47:18.330 --> 00:47:20.200
So, how does this API work?

00:47:20.250 --> 00:47:24.300
Well, again, as I said before,
we return an array of filters.

00:47:24.300 --> 00:47:26.900
These are some of the filters
that we apply today when we

00:47:26.930 --> 00:47:28.480
are doing auto enhancement.

00:47:28.480 --> 00:47:32.620
One of the first filters that
we apply based on our image

00:47:33.080 --> 00:47:35.240
analysis is CI red eye correction.

00:47:35.240 --> 00:47:38.460
And this in itself is a very
complex problem to be able to

00:47:38.460 --> 00:47:42.380
both detect and repair red eye
artifacts that are sometimes red,

00:47:42.380 --> 00:47:46.560
sometimes amber, sometimes white,
that result from camera flashes.

00:47:46.560 --> 00:47:50.430
This is a very, very interesting problem,
and we've provided that as part of

00:47:50.440 --> 00:47:54.500
the auto enhancement capabilities.

00:47:54.500 --> 00:47:58.420
Some of the other filters that we apply
is a filter called CI face balance,

00:47:58.420 --> 00:48:03.000
and this filter allows the auto
enhancement to adjust the color of an

00:48:03.150 --> 00:48:05.810
image to produce pleasing skin tones.

00:48:07.010 --> 00:48:10.440
Next, CI Vibrance,
which increases the saturation

00:48:10.440 --> 00:48:13.220
of an image when appropriate
without distorting skin tones,

00:48:13.320 --> 00:48:15.340
which is also important.

00:48:15.470 --> 00:48:18.980
It's worth mentioning at this point that
some of these things like vibrance may

00:48:18.980 --> 00:48:21.740
be familiar to users of like Aperture.

00:48:21.870 --> 00:48:25.670
These are algorithms that we worked
in conjunction with those teams to

00:48:25.830 --> 00:48:30.480
use the best imaging technology we
have at Apple to improve images,

00:48:30.520 --> 00:48:34.270
and we've brought them to
you as part of this API.

00:48:36.410 --> 00:48:39.960
Next filter that we can
apply is CI Tone Curve,

00:48:39.960 --> 00:48:44.290
and this is a filter that allows
you to adjust the image contrast.

00:48:44.990 --> 00:48:49.280
And then also CI highlights and shadows,
and we use that to bring up the shadow

00:48:49.280 --> 00:48:51.870
detail of images when appropriate.

00:48:53.300 --> 00:48:58.350
So what are some examples of how
this looks in actual practice?

00:48:58.450 --> 00:49:00.870
So we have three images,
each of which have some degree of issues

00:49:00.870 --> 00:49:03.190
with them in different types of areas.

00:49:03.340 --> 00:49:07.030
This first image is a
great photo of a girl,

00:49:07.030 --> 00:49:09.940
but if you look at the skin tones,
it looks a little greenish.

00:49:10.020 --> 00:49:12.850
So we can actually improve that by
using the face balance to improve that

00:49:12.930 --> 00:49:15.230
to make the skin tones more natural.

00:49:15.410 --> 00:49:18.910
The next photo, we had nice content,
but the shadows were a little dark,

00:49:18.910 --> 00:49:23.030
and we can make the image more
lively by bringing up the shadows.

00:49:23.100 --> 00:49:25.340
And in this last photo,
I'm hoping you guys can see,

00:49:25.340 --> 00:49:29.070
we've got a case of red eye,
which is very,

00:49:29.070 --> 00:49:33.120
very hard to repair usually,
but with our red eye filter,

00:49:33.120 --> 00:49:35.750
we can make that problem go away.

00:49:36.100 --> 00:49:37.470
So that's how it works in practice.

00:49:37.720 --> 00:49:42.480
Let me talk to you in some detail about
how you can use this in your application.

00:49:42.610 --> 00:49:45.290
So it's a very simple API,
first and foremost.

00:49:45.400 --> 00:49:49.460
The API is one call called
Auto Adjustment Filters with Options.

00:49:49.560 --> 00:49:51.680
And you pass in an options dictionary.

00:49:51.680 --> 00:49:54.870
And some of these options are
actually pretty important.

00:49:55.140 --> 00:49:58.350
You may, for example,
not want to just do all the adjustments,

00:49:58.450 --> 00:49:59.940
which is what we return by default.

00:50:00.030 --> 00:50:03.580
But you may want to just do Red Eye or
just do the other auto enhancements,

00:50:03.680 --> 00:50:06.640
depending on what your
application desires.

00:50:06.780 --> 00:50:11.860
So you can pass in either
KCI Auto Adjust Enhance

00:50:12.120 --> 00:50:17.000
to either true or false,
or KCI Auto Red Eye to false,

00:50:17.060 --> 00:50:20.540
if you just want to perform
the adjustment filters.

00:50:21.220 --> 00:50:26.820
Another very important option is to
tell the auto-adjustment API what

00:50:26.910 --> 00:50:29.140
direction up is in your image.

00:50:29.260 --> 00:50:33.110
You do that, as I showed before,
in the face detection example by

00:50:33.110 --> 00:50:37.180
asking the image for its properties,
getting the properties KCG image

00:50:37.180 --> 00:50:42.530
property orientation key,
and then passing that in

00:50:42.610 --> 00:50:46.120
as an option with the key
CI detector image orientation.

00:50:46.490 --> 00:50:49.100
You need to do this if you want
to get the best possible results

00:50:49.190 --> 00:50:50.960
out of the auto-enhancement.

00:50:51.020 --> 00:50:55.110
If you do not pass it in,
then the detector may not find the face,

00:50:55.110 --> 00:50:56.960
and we will be able to
do some adjustments,

00:50:57.030 --> 00:51:02.710
but we won't be able to necessarily
find red eyes or do skin balance.

00:51:05.870 --> 00:51:09.880
The next thing I'd like to
talk about this API is the

00:51:10.050 --> 00:51:12.200
result of this API is an array.

00:51:12.200 --> 00:51:14.860
It returns an array
of adjustment filters.

00:51:14.860 --> 00:51:16.410
Once you have that array,
there's several options

00:51:16.410 --> 00:51:17.890
that are available to you.

00:51:17.950 --> 00:51:21.270
The most likely scenario is that you're
going to want to chain those filters

00:51:21.270 --> 00:51:23.040
together and get an output image.

00:51:23.040 --> 00:51:25.440
And you do that using the
following short line of code.

00:51:25.520 --> 00:51:28.980
You basically loop over each
filter in the adjustments.

00:51:28.980 --> 00:51:33.120
You then set the input image to
be the input image preceding it.

00:51:33.120 --> 00:51:36.510
And then you set the image
variable to the output of that.

00:51:36.510 --> 00:51:37.680
And then you just loop.

00:51:37.680 --> 00:51:40.150
And the idea behind this loop is
that you're chaining the output

00:51:40.150 --> 00:51:44.070
to the input to the output to the
input for that array of filters.

00:51:44.620 --> 00:51:47.340
You can, however,
use this opportunity to do other things.

00:51:47.440 --> 00:51:51.100
You can query each of those filters
and see what the parameter values are.

00:51:51.210 --> 00:51:54.060
You could store the names of the
filters and their parameters off

00:51:54.060 --> 00:51:57.620
so that if you want to apply the
adjustments at a later time without

00:51:57.710 --> 00:52:02.220
incurring the cost of the analysis phase,
you can do so.

00:52:02.740 --> 00:52:04.080
Typically, however,
you're just going to chain

00:52:04.080 --> 00:52:06.740
the filters together,
and you'll get an output image that's

00:52:06.790 --> 00:52:11.080
ready to render using any of the
techniques we mentioned earlier.

00:52:12.630 --> 00:52:15.840
So now to give a demonstration,
this is a very short demo of

00:52:15.840 --> 00:52:19.930
auto enhancement in progress,
then I'll pass the microphone

00:52:19.930 --> 00:52:21.240
over to Chendi again.

00:52:21.380 --> 00:52:22.860
Thanks.

00:52:24.170 --> 00:52:25.780
So this is a pretty simple demo.

00:52:25.780 --> 00:52:27.720
You open up the app,
and there's a bunch of

00:52:27.730 --> 00:52:31.060
different images taken with,
in this case, SLR.

00:52:31.090 --> 00:52:33.500
And if we just tap on the image--

00:52:33.990 --> 00:52:36.600
CI will perform an auto-enhance,
and you can see from the result

00:52:36.650 --> 00:52:40.930
that it's increased the contrast,
improved the colors and the saturation.

00:52:49.840 --> 00:52:51.100
I think I'll do two more.

00:52:51.100 --> 00:52:55.590
This one and that one.

00:52:55.640 --> 00:52:57.090
And that's it.

00:52:59.840 --> 00:53:03.020
So that's the end of the bulk
of our conversation for today.

00:53:03.020 --> 00:53:04.820
I want to thank all of
you for coming today,

00:53:04.820 --> 00:53:07.820
and I look forward to seeing all
the great applications that you

00:53:07.820 --> 00:53:10.730
build using Core Image on iOS.

00:53:10.780 --> 00:53:14.320
If you have any questions,
please contact Alan Schaffer.

00:53:14.380 --> 00:53:20.140
Also, if you didn't already see it,
there is a session on capturing from

00:53:20.200 --> 00:53:24.870
the camera using AV Foundation on iOS 5.

00:53:24.900 --> 00:53:26.890
Thank you all very, very much.