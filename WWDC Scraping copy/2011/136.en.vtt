WEBVTT

00:00:10.800 --> 00:00:12.660
Good morning, everybody.

00:00:12.840 --> 00:00:16.230
It's nice to see,
flattering for us to see that you

00:00:16.330 --> 00:00:20.800
sacrificed your beauty sleep to come
up here and catch our presentation.

00:00:20.920 --> 00:00:22.580
So, what do we have for you today?

00:00:22.620 --> 00:00:25.040
We're going to be talking
about Latent Semantic Mapping,

00:00:25.040 --> 00:00:27.000
also known as LSM.

00:00:27.050 --> 00:00:30.860
First,
we're going to explain what LSM is,

00:00:30.860 --> 00:00:31.980
how it works.

00:00:32.350 --> 00:00:34.060
how you can use the APIs.

00:00:34.210 --> 00:00:38.400
We're going to present some
case studies on how we used LSM.

00:00:38.410 --> 00:00:42.300
And then more interestingly for you,
we're going to talk about how

00:00:42.430 --> 00:00:45.380
you can use LSM for yourself.

00:00:45.400 --> 00:00:47.630
There is going to be some
time for a Q&A at the end,

00:00:47.640 --> 00:00:48.810
of course.

00:00:49.220 --> 00:00:52.420
So what is latent semantic mapping?

00:00:52.430 --> 00:00:56.800
Latent semantic mapping is a
technology to analyze documents,

00:00:56.980 --> 00:01:00.810
especially text documents,
according to their meaning

00:01:00.810 --> 00:01:02.710
and classify them by topic.

00:01:02.810 --> 00:01:05.350
Now that's a little bit vague.

00:01:05.360 --> 00:01:11.530
So allow me to give you a brief
demonstration on how this is working.

00:01:14.420 --> 00:01:19.500
So, first thing you're going to see
here is no C code was written

00:01:19.500 --> 00:01:20.840
for this demo whatsoever.

00:01:20.940 --> 00:01:24.490
We're going to be using
just a shell tool,

00:01:24.580 --> 00:01:27.610
user/bin/lsm.

00:01:27.650 --> 00:01:33.390
And what we're going to do is build a
classifier to distinguish Perl files

00:01:33.390 --> 00:01:37.000
from Python files from Ruby files.

00:01:37.520 --> 00:01:40.930
We have a whole bunch
of these on the system.

00:01:41.020 --> 00:01:44.380
You can see we have in
the library folders,

00:01:44.380 --> 00:01:49.120
we have something like 14 megabytes
of text documents or of text

00:01:49.120 --> 00:01:54.100
files for these three languages,
roughly equally split.

00:01:54.460 --> 00:02:01.860
What we are going to do now is train a
latent semantic mapping map from this.

00:02:01.860 --> 00:02:07.020
So you see here the command
line tool LSM create.

00:02:07.020 --> 00:02:10.860
We have a name for the map that
you can choose arbitrarily.

00:02:10.860 --> 00:02:13.910
We have our Perl files,
we have our Python files,

00:02:14.020 --> 00:02:16.330
and we have our Ruby training files.

00:02:16.390 --> 00:02:19.840
Each of this is a group of
files separated by a semicolon.

00:02:19.840 --> 00:02:27.500
So we say our categories are grouped,
split by semicolons.

00:02:27.500 --> 00:02:30.780
And we're now going to build
this map when I hit return.

00:02:30.780 --> 00:02:33.740
These 14 megabytes of data
are going to be read in.

00:02:33.950 --> 00:02:37.280
The map is going to be built using
a very sophisticated process.

00:02:37.280 --> 00:02:42.860
So this takes some time,
roughly two seconds.

00:02:42.860 --> 00:02:45.400
And we have our map.

00:02:47.650 --> 00:02:50.810
We compiled this into a
map of about one megabyte.

00:02:51.130 --> 00:02:56.110
This map is supposed to classify Perl,
Python, and Ruby files.

00:02:56.230 --> 00:03:00.840
So let's pick some files we haven't
seen in the training material.

00:03:00.880 --> 00:03:06.730
There are a bunch of scripts in /usr/bin,
so we're going to...

00:03:07.680 --> 00:03:13.180
See what this map can do on acLocal,
on pyDoc,

00:03:13.360 --> 00:03:17.960
and on Generate Bridge Metadata.

00:03:19.900 --> 00:03:22.870
Now remember, we have three categories.

00:03:23.000 --> 00:03:26.900
Number one is Perl, number two is Python,

00:03:27.290 --> 00:03:29.540
Number 3 is Ruby.

00:03:29.590 --> 00:03:35.880
And our classifier tells us
that aclocal is most likely,

00:03:36.430 --> 00:03:39.250
belongs to category 1, that is Perl.

00:03:39.500 --> 00:03:44.970
pydoc most likely belongs to category 2,
that is Ruby, Python.

00:03:45.100 --> 00:03:50.360
And generate-bridge metadata
is most likely category 3,

00:03:50.360 --> 00:03:50.360
Ruby.

00:03:52.630 --> 00:03:55.130
For those of you who have
looked at the file names,

00:03:55.130 --> 00:04:00.070
which the tool didn't do,
it's likely that pydoc

00:04:00.220 --> 00:04:02.220
was a correct guess.

00:04:02.220 --> 00:04:02.220
So let's see.

00:04:02.360 --> 00:04:04.800
"What languages these
scripts actually are.

00:04:04.940 --> 00:04:08.180
And you see that indeed,
aclocal is a Perl script,

00:04:08.340 --> 00:04:11.710
pydoc is a Python script,
and generate-bridge-metadata

00:04:11.940 --> 00:04:13.740
is a Ruby script.

00:04:13.830 --> 00:04:22.330
So you see that with little effort
you can train up a recognizer to

00:04:22.340 --> 00:04:25.850
distinguish programming languages
that may have different keywords.

00:04:26.490 --> 00:04:28.160
Now, what other things can you do?

00:04:28.230 --> 00:04:33.360
You probably are not going to tell apart
Perl files from Python files all day.

00:04:33.510 --> 00:04:38.620
One of our first successful applications
was the Junk Mail Filter in Mail.app,

00:04:38.620 --> 00:04:41.970
which assesses whether an
incoming mail message is a

00:04:42.200 --> 00:04:44.190
legitimate mail or junk mail.

00:04:44.540 --> 00:04:48.530
The same principle applies
to Parental Controls,

00:04:48.530 --> 00:04:52.110
which do the same
classification for web pages.

00:04:53.310 --> 00:04:58.690
We also use this for the Japanese users
might be familiar with the Kanatukanji

00:04:58.690 --> 00:05:04.100
conversion to convert Japanese text
from a syllabic representation to

00:05:04.180 --> 00:05:10.600
an ideographic Kanji representation,
which is a context-sensitive operation.

00:05:10.880 --> 00:05:16.320
Internally, we are also using LSM with
some successful localization,

00:05:16.540 --> 00:05:21.440
where again you have a word that can map
to different words in another language,

00:05:21.550 --> 00:05:29.160
and you need some context to disambiguate
what the word possibly could mean.

00:05:29.990 --> 00:05:37.500
So now you've seen what it looks like
and what you could possibly use it for,

00:05:37.510 --> 00:05:39.480
but how does it work under the hood?

00:05:39.490 --> 00:05:41.520
To answer this question,
I would like to bring up my

00:05:41.520 --> 00:05:47.250
colleague Jerome Bellegarda,
who actually developed the algorithms

00:05:47.250 --> 00:05:50.500
underlying Latent Semantic Mapping.

00:05:55.500 --> 00:05:56.500
Thank you, Matthias.

00:05:56.500 --> 00:05:59.100
Welcome.

00:05:59.110 --> 00:06:03.090
So actually explaining latent
semantic mapping is rather simple

00:06:03.200 --> 00:06:05.330
because it's all in the name.

00:06:05.330 --> 00:06:10.010
Let's unravel those three words,
latent semantic and mapping.

00:06:10.020 --> 00:06:12.920
First of all,
we're talking about mapping,

00:06:12.920 --> 00:06:17.300
specifically between words and
documents and points in some

00:06:17.300 --> 00:06:20.650
complex multidimensional space,
which we'll talk about.

00:06:20.680 --> 00:06:25.840
And the important thing about
this mapping is that it is a way

00:06:25.840 --> 00:06:30.480
to go from discrete entities,
words and documents in

00:06:30.480 --> 00:06:35.060
this particular case,
to a continuous vector space,

00:06:35.140 --> 00:06:37.330
which has very nice
mathematical properties.

00:06:37.340 --> 00:06:40.000
The second point is that
this mapping is very simple.

00:06:40.000 --> 00:06:43.970
The third point is that
this mapping is semantic,

00:06:43.970 --> 00:06:48.770
meaning that we are using the
meaning behind the words and the

00:06:48.770 --> 00:06:53.970
documents in order to unravel the
entire fabric of the language,

00:06:53.980 --> 00:06:55.240
so to speak.

00:06:55.320 --> 00:07:00.720
And finally, this mapping is latent,
which in this particular case means

00:07:00.720 --> 00:07:05.940
that the meaning that we're talking
about here is inferred directly from

00:07:06.060 --> 00:07:09.040
the data and not obtained explicitly.

00:07:09.980 --> 00:07:11.100
For example, from a dictionary.

00:07:11.100 --> 00:07:15.590
And that inference is based
on word co-occurrences.

00:07:15.600 --> 00:07:19.010
Let's drill down a little
bit on those three aspects,

00:07:19.010 --> 00:07:22.210
starting with this word
co-occurrence thing.

00:07:22.220 --> 00:07:25.280
In particular,
what I would like to stress

00:07:25.280 --> 00:07:29.600
is that two words can co-occur
either in the same document,

00:07:29.620 --> 00:07:35.560
but also, and that's a more subtle way of
getting to the co-occurrence,

00:07:35.560 --> 00:07:38.350
they could co-occur with
a common set of words,

00:07:38.440 --> 00:07:39.960
but never to each other.

00:07:39.960 --> 00:07:44.830
In both cases, those words will have a
representation in the LSM space,

00:07:44.830 --> 00:07:47.760
which are in the same
region of the space.

00:07:47.760 --> 00:07:51.270
So this is great for things
like discovering synonyms,

00:07:51.270 --> 00:07:55.960
for example, car and automobile,
which can be used interchangeably,

00:07:55.960 --> 00:08:02.040
but also to get to multiple,
to disambiguate between multiple

00:08:02.040 --> 00:08:04.670
senses of a certain word.

00:08:04.780 --> 00:08:06.270
For example, the word bank.

00:08:06.380 --> 00:08:11.670
If bank and rate happens to be close,
in the LSM space,

00:08:11.670 --> 00:08:15.440
then we can pretty much be
certain that the general region

00:08:15.440 --> 00:08:17.580
of the space represents finance.

00:08:17.600 --> 00:08:22.620
Whereas if bank and river are close
in the same region of the space,

00:08:22.620 --> 00:08:24.910
then that region is more
likely about phishing.

00:08:24.920 --> 00:08:30.200
So now let's talk about the
semantic part of things.

00:08:30.200 --> 00:08:33.750
And here I'd like to take an example,
which is parental control,

00:08:33.750 --> 00:08:35.920
which Mathias was just mentioning.

00:08:35.920 --> 00:08:41.990
In parental control, the aim,
the aim of this feature is

00:08:42.010 --> 00:08:45.400
to assess whether a web page
contains any explicit words.

00:08:45.430 --> 00:08:49.080
And so a naive approach might be to say,
well,

00:08:49.080 --> 00:08:51.410
we're going to look for words like sex.

00:08:51.790 --> 00:08:54.740
However,
it's not that simple because there

00:08:54.740 --> 00:08:58.210
are many legitimate documents
that talk about sex education,

00:08:58.210 --> 00:08:58.400
for example.

00:08:58.420 --> 00:09:02.420
So we really have to go to the
meaning behind the documents.

00:09:02.470 --> 00:09:06.420
And we're going to leverage this
aspect that I was just discussing,

00:09:06.420 --> 00:09:08.920
the closeness in the LSM space.

00:09:09.450 --> 00:09:13.710
Again,
if sex occurs close to toys in the,

00:09:13.710 --> 00:09:17.290
a word like toys, in the LSM space,
then probably that web

00:09:17.350 --> 00:09:18.870
page is objectionable.

00:09:18.930 --> 00:09:22.890
Whereas if sex occurs next
to education in that space,

00:09:22.990 --> 00:09:24.900
then that page is probably okay.

00:09:25.020 --> 00:09:30.850
In the LSM implementation, we use,
in that case, two categories.

00:09:30.920 --> 00:09:34.920
One for the legitimate material,
one for the explicit material.

00:09:34.920 --> 00:09:37.920
And we define special
points in the LSM space.

00:09:37.920 --> 00:09:40.920
Which are called semantic anchors.

00:09:41.030 --> 00:09:43.920
And then all we have to do is,
for a new web page,

00:09:43.920 --> 00:09:47.830
evaluate whether the web page,
the representation of the web page

00:09:47.950 --> 00:09:52.920
in the LSM space is close to the
legitimate anchor or the explicit anchor.

00:09:53.020 --> 00:09:56.920
Let me just illustrate
on a very simple 2D map.

00:09:56.920 --> 00:10:01.080
So let's assume we have an
anchor which represents,

00:10:01.080 --> 00:10:06.920
in a sense, it's the centroid of all the
legitimate pages that we've seen.

00:10:06.920 --> 00:10:09.920
And then we have somewhere
else the explicit anchor,

00:10:10.010 --> 00:10:13.920
which would be the centroid
of all the explicit documents,

00:10:13.920 --> 00:10:15.920
web pages that we've seen.

00:10:15.920 --> 00:10:19.920
Then what we do is we map an
incoming web page into that space.

00:10:19.920 --> 00:10:23.570
And then simply compute the distance
between that representation of

00:10:23.570 --> 00:10:25.920
the web page and the two anchors.

00:10:25.920 --> 00:10:28.850
And in this particular case,
we see that the web page is closer to

00:10:28.850 --> 00:10:30.920
the anchor of the explicit material.

00:10:30.920 --> 00:10:34.920
So we would conclude that
this is an explicit page.

00:10:34.920 --> 00:10:35.920
Now this was a very simple illustration.

00:10:35.920 --> 00:10:37.920
This is a very simple example.

00:10:37.920 --> 00:10:40.810
More generally, of course,
we have more than two categories.

00:10:40.980 --> 00:10:43.780
We might have, for example,
a document about finance, phishing,

00:10:43.880 --> 00:10:44.920
and so on and so forth.

00:10:44.920 --> 00:10:46.910
But the mapping,
the principle is the same.

00:10:46.960 --> 00:10:49.920
We're going to map this into
a multidimensional space.

00:10:49.920 --> 00:10:52.920
In this particular case,
I used three dimensions.

00:10:53.030 --> 00:10:54.820
But in general, it's much more than that.

00:10:54.930 --> 00:10:56.920
It could be up to a couple of hundreds.

00:10:56.920 --> 00:11:00.920
For example, we found that for a natural
language like English,

00:11:00.920 --> 00:11:04.920
dimensionality of about
200 to 300 is appropriate.

00:11:04.920 --> 00:11:10.820
And so what we do is in a training phase,
we're going to map all of those

00:11:10.820 --> 00:11:14.920
documents that we have to that space.

00:11:14.920 --> 00:11:19.920
So this results in all those
little red triangles there.

00:11:19.920 --> 00:11:24.340
And what you see is that those red
triangles collectively define some

00:11:24.340 --> 00:11:28.850
regions in the LSM space which represents
the categories we had to begin with.

00:11:28.920 --> 00:11:31.920
And then now when a
new document comes in,

00:11:31.920 --> 00:11:34.780
we have a phase-course
classification where we map

00:11:34.780 --> 00:11:36.920
that document into the space.

00:11:36.940 --> 00:11:39.920
And then based on where
it falls into that space,

00:11:40.070 --> 00:11:43.070
we conclude that, for example,
in this case,

00:11:43.200 --> 00:11:45.910
it's a document about computer science.

00:11:47.040 --> 00:11:51.650
Now, of course,
this feels a little bit like magic,

00:11:51.650 --> 00:11:52.950
but it's not magic.

00:11:52.960 --> 00:11:57.150
The basic information that we
use is the difference between how

00:11:57.150 --> 00:12:01.920
often each word appears in each
document and compared to how often

00:12:01.920 --> 00:12:04.230
it appears in the entire collection.

00:12:04.230 --> 00:12:09.890
And what this allows us to do is build
up a matrix of words and documents.

00:12:09.890 --> 00:12:14.820
So for example, in this case,
let's assume we have M words

00:12:15.010 --> 00:12:19.340
in our vocabulary and
N documents in our collections.

00:12:19.340 --> 00:12:23.290
We build up a matrix like this and,
of course, the first thing to note is

00:12:23.290 --> 00:12:26.660
that this matrix is going
to be very sparse because,

00:12:26.660 --> 00:12:30.790
after all,
most words don't occur in most documents.

00:12:30.840 --> 00:12:35.740
However,
each -- what this allows us to do is

00:12:35.800 --> 00:12:40.720
that each word now can be represented
by a vector of dimension N and each

00:12:40.720 --> 00:12:42.840
document by a vector of dimension M.

00:12:43.080 --> 00:12:43.880
The problem, as I said,
is that each word can be represented

00:12:43.880 --> 00:12:44.760
by a vector of dimension M.

00:12:44.880 --> 00:12:46.410
So what we can say is that those
vectors tend to be sparse and,

00:12:46.410 --> 00:12:50.010
of course, those two representations
for words and documents don't

00:12:50.010 --> 00:12:51.940
really -- are not comparable.

00:12:51.940 --> 00:12:55.420
So in order to address this,
we use a mathematical process called

00:12:55.420 --> 00:12:59.370
the singular value decomposition,
which takes this matrix and

00:12:59.530 --> 00:13:05.760
decomposes it into various -- into
those three matrices shown here.

00:13:05.760 --> 00:13:10.330
What this allows us to do is
to transform that original,

00:13:10.330 --> 00:13:15.150
let's say, the document,
which was in yellow,

00:13:15.220 --> 00:13:21.750
to map it into a vector,
VJ on the other side, the red vector,

00:13:21.750 --> 00:13:25.810
which now is no longer sparse.

00:13:25.880 --> 00:13:26.970
It's now dense.

00:13:26.970 --> 00:13:31.580
It has a dimension R,
which is much smaller than either M or N.

00:13:31.690 --> 00:13:35.530
So this kind of mathematical trick
allows us to have a much more

00:13:35.530 --> 00:13:39.780
manageable representation to deal with.

00:13:39.780 --> 00:13:43.720
And so this leads to the kind
of LSM space that we saw before.

00:13:43.720 --> 00:13:47.880
And this is a very interesting example
where we can now map those new documents.

00:13:47.880 --> 00:13:50.100
One thing that's
interesting to note is that,

00:13:50.330 --> 00:13:53.610
well, those documents have words,
of course, and those words are also

00:13:53.610 --> 00:13:54.930
mapped in that space.

00:13:54.980 --> 00:14:00.680
In fact,
there are those green -- I'm sorry.

00:14:00.680 --> 00:14:04.490
There are those green vectors.

00:14:05.110 --> 00:14:08.090
here, those green squares.

00:14:08.420 --> 00:14:10.920
And so what we can do is two things.

00:14:10.970 --> 00:14:14.700
We can, for a blue triangle,
which is a new document in the

00:14:14.700 --> 00:14:18.230
space-- we can find out the
red triangle that's closest,

00:14:18.350 --> 00:14:21.710
that's essentially the document
that's most similar to that document.

00:14:21.850 --> 00:14:26.510
We can also find the word that's most
aligned with that document that would,

00:14:26.510 --> 00:14:29.540
for example, function as a good
title for that document.

00:14:29.600 --> 00:14:32.610
So as you can see,
there is a range of applications that

00:14:32.740 --> 00:14:36.480
could benefit from such a representation.

00:14:36.740 --> 00:14:41.710
Having said that, of course,
there are some caveats to the method.

00:14:41.720 --> 00:14:44.650
One is that we've been
talking about semantic.

00:14:44.650 --> 00:14:46.840
However,
it's a fairly shallow sense of semantic

00:14:46.840 --> 00:14:49.650
since it's only based on co-occurrences.

00:14:49.650 --> 00:14:53.640
There is no actual deep
parsing or anything like that.

00:14:53.640 --> 00:14:57.030
Second caveat is that the
word order is ignored.

00:14:57.030 --> 00:14:59.330
So there is no local constraints.

00:14:59.340 --> 00:15:02.730
This actually can be added
explicitly to the framework.

00:15:02.750 --> 00:15:06.160
And we'll talk about that
later on in the presentation.

00:15:06.160 --> 00:15:09.660
But that's just something
to be cognizant about.

00:15:09.700 --> 00:15:12.850
Also, this is a statistical method,
which means that like

00:15:12.850 --> 00:15:16.050
all statistical methods,
it lives and dies by the

00:15:16.060 --> 00:15:18.460
training data that's fed to it.

00:15:18.460 --> 00:15:20.860
And so, for example,
let's assume we had a document

00:15:20.860 --> 00:15:25.100
which had both river bank
and bank of Cupertino in it.

00:15:25.100 --> 00:15:29.570
Well, how could we possibly resolve
the two senses of bank?

00:15:29.570 --> 00:15:32.090
In that particular case, we couldn't.

00:15:32.200 --> 00:15:34.880
So it's important to have
training data that is useful.

00:15:34.890 --> 00:15:34.920
Thank you.

00:15:34.950 --> 00:15:37.630
So we're going to have to do a
little bit more of a research.

00:15:37.670 --> 00:15:39.990
So let's start with the first caveat.

00:15:40.040 --> 00:15:43.810
What is the use of the
single value decomposition?

00:15:43.960 --> 00:15:47.360
Well, the single value decomposition
is a very simple thing.

00:15:47.460 --> 00:15:48.550
It's a very simple thing.

00:15:48.550 --> 00:15:53.040
You can use a single-value decomposition
to find the single-value decomposition.

00:15:53.100 --> 00:15:55.880
And the single-value
decomposition is a very,

00:15:55.880 --> 00:15:57.640
very simple thing.

00:15:57.820 --> 00:16:04.880
You can use a single-value decomposition
to find the single-value decomposition.

00:16:04.880 --> 00:16:05.540
So let's start with the
single value decomposition.

00:16:05.540 --> 00:16:05.880
So let's start with the
single-value decomposition.

00:16:05.880 --> 00:16:09.750
The final thing I wanted to
touch on before moving on

00:16:09.750 --> 00:16:11.870
to the API is clustering.

00:16:11.870 --> 00:16:16.370
I've talked about how this LSM space
had nice mathematical properties,

00:16:16.470 --> 00:16:22.540
and that's an illustration of what we
can do with this nice continuous space.

00:16:22.590 --> 00:16:28.800
And I'd like to take the example of the
Kana to Kanji conversion to illustrate.

00:16:28.880 --> 00:16:31.690
In Kana to Kanji conversion,
we use the topic information

00:16:31.700 --> 00:16:33.320
to disambiguate between
the various characters.

00:16:33.320 --> 00:16:35.680
So we have a very simple example
of the Kana to Kanji conversion.

00:16:35.810 --> 00:16:39.100
So it's for those of us who are
not native Japanese speakers,

00:16:39.100 --> 00:16:43.080
this is a little bit like analogous
to resolving tail in the tail of a

00:16:43.080 --> 00:16:45.420
princess versus the tail of a peacock.

00:16:45.650 --> 00:16:48.940
We use the context to sort of get
to the orthography in that case.

00:16:48.940 --> 00:16:52.200
So a little bit similar in
the Kana to Kanji conversion.

00:16:52.200 --> 00:16:55.950
Now, of course,
the Japanese corpus is a large corpus.

00:16:56.060 --> 00:17:01.590
In this particular case,
it contains over 300,000 documents.

00:17:01.590 --> 00:17:04.520
So how to best extract
that topic information?

00:17:04.520 --> 00:17:07.190
Well, the first thing you have to do
is to sort of go through each

00:17:07.700 --> 00:17:12.750
document and assign it a category.

00:17:12.750 --> 00:17:16.400
Not really feasible.

00:17:16.400 --> 00:17:19.340
Or you can take advantage of
the fact that we have this nice

00:17:19.340 --> 00:17:23.210
continuous space to do clustering
right there in the LSM space.

00:17:23.340 --> 00:17:26.720
In that case,
the procedure is very simple.

00:17:26.720 --> 00:17:30.590
We map all the document into a space,
treating each document

00:17:30.640 --> 00:17:32.000
as a separate category.

00:17:32.040 --> 00:17:34.660
And then we do some
data-driven computation.

00:17:34.660 --> 00:17:35.710
So we can do that in the space,
right in the space to reduce

00:17:35.710 --> 00:17:37.010
the number of categories.

00:17:37.010 --> 00:17:40.480
And then optionally,
we can iterate on that process.

00:17:40.480 --> 00:17:44.260
And as Mathias will say a little later,
we have two implementations

00:17:44.260 --> 00:17:44.980
of clustering.

00:17:45.100 --> 00:17:49.060
There are many -- I should say there are
many clustering applications out there,

00:17:49.240 --> 00:17:51.040
many clustering implementations.

00:17:51.040 --> 00:17:53.750
We've implemented two as
part of the framework.

00:17:53.750 --> 00:17:57.110
One is called k-means clustering,
where you start with

00:17:57.170 --> 00:18:01.930
essentially k cluster centers
randomly assigned usually,

00:18:01.930 --> 00:18:05.940
compute distances to
those cluster centers,

00:18:05.940 --> 00:18:10.660
and then iterate as you
adjust the center rates.

00:18:10.660 --> 00:18:12.610
This is fairly straightforward.

00:18:12.820 --> 00:18:13.810
Works quite well.

00:18:13.810 --> 00:18:18.830
The caveat is that it's sensitive to,
well, those initial cluster assignments.

00:18:19.050 --> 00:18:21.810
If you start with another
set of k clusters,

00:18:21.950 --> 00:18:25.200
you're likely to get
slightly different clusters.

00:18:25.200 --> 00:18:28.890
And, of course, the problem is to find
k in the first place.

00:18:28.980 --> 00:18:32.330
The second implementation,
which is available in the LSM API,

00:18:32.330 --> 00:18:33.980
is agglomerative clustering.

00:18:34.290 --> 00:18:37.440
Where here you compute all
pairwise distances between

00:18:37.440 --> 00:18:39.530
all the points in your system.

00:18:39.530 --> 00:18:43.530
And you merge the closest
pair and then iterate again,

00:18:43.590 --> 00:18:46.420
adjusting the distances appropriately.

00:18:46.420 --> 00:18:49.360
Now this solves the problem of
initial cluster assignments,

00:18:49.480 --> 00:18:49.940
of course.

00:18:50.050 --> 00:18:52.760
The caveat here is that if you
have a large data set with,

00:18:52.820 --> 00:18:55.280
let's say,
thousands and thousands of points,

00:18:55.350 --> 00:18:57.790
then the cost might be prohibitive.

00:18:57.810 --> 00:19:03.800
So anyway, that's just a brief overview
of what we can do with LSM.

00:19:03.800 --> 00:19:09.040
And to -- To continue with the LSM API,
I'm going to bring Mathias back on stage.

00:19:11.120 --> 00:19:14.560
Thank you, Jérôme.

00:19:14.740 --> 00:19:19.920
So, let's discuss how you can
actually use the LSM API.

00:19:20.280 --> 00:19:24.870
As I said before, if at all possible,
especially if your data

00:19:24.870 --> 00:19:28.230
is primarily text-based,
try to prototype with the command

00:19:28.230 --> 00:19:32.040
line tool as long as possible,
because it's probably going to be easier

00:19:32.040 --> 00:19:34.640
for you to design your experiments.

00:19:34.640 --> 00:19:39.930
Once you then do move to the API,
our API is Core Foundation based.

00:19:39.950 --> 00:19:46.480
All our data types can be used
interchangeably with CF-based objects.

00:19:46.480 --> 00:19:51.280
You can put them into collections,
retain, release them, and so on.

00:19:51.300 --> 00:19:54.300
We have fundamentally three data types.

00:19:54.310 --> 00:20:00.000
Our fundamental type is the LSM map,
which stores all our data.

00:20:00.000 --> 00:20:03.440
An LSM map can be in three states.

00:20:03.440 --> 00:20:07.690
In training state,
it's ready to accept categories,

00:20:07.970 --> 00:20:09.240
ready to accept data.

00:20:09.240 --> 00:20:13.740
In this-- in these categories,
once you have all the data

00:20:13.740 --> 00:20:18.360
you want for your training,
the map goes into evaluation state,

00:20:18.430 --> 00:20:24.540
at which point it's ready to
classify data you ask it to.

00:20:24.730 --> 00:20:28.790
And finally, a map can be stored to disk
and read back from disk without

00:20:28.790 --> 00:20:32.340
having to be recompiled,
and that's so you can

00:20:32.340 --> 00:20:34.550
archive it and unarchive it.

00:20:34.870 --> 00:20:37.560
The second important
data type is an LSM text,

00:20:37.630 --> 00:20:39.860
which represents a document.

00:20:41.020 --> 00:20:44.140
The first step is to
create a list of tokens.

00:20:44.140 --> 00:20:48.360
As we will see,
it doesn't have to be words necessarily.

00:20:48.360 --> 00:20:55.970
And finally, when you do an evaluation,
what you get back is an LSM result

00:20:56.040 --> 00:21:01.280
from which you can extract
various pieces of information

00:21:01.280 --> 00:21:01.280
that you feel important to know.

00:21:01.660 --> 00:21:06.740
So to look at this in detail,
you create a map with lsm_map_create,

00:21:06.740 --> 00:21:11.510
so this is very much terminology
you should be familiar with from

00:21:11.510 --> 00:21:14.200
all this core foundation APIs.

00:21:14.520 --> 00:21:20.330
You then add categories and data
to those categories with LSM map

00:21:20.510 --> 00:21:24.960
add category and LSM map add text,
respectively.

00:21:24.960 --> 00:21:27.980
Once you have all your
training data together,

00:21:28.150 --> 00:21:32.560
you call LSM map compile,
which puts the map into evaluation state.

00:21:33.340 --> 00:21:36.880
If at some point you want to go
back and train with some more data,

00:21:36.880 --> 00:21:40.560
let's say in chunk mail filtering,
every time the user

00:21:40.560 --> 00:21:44.480
hits the chunk button,
what happens is the map gets

00:21:44.570 --> 00:21:46.830
put back into training state.

00:21:46.940 --> 00:21:51.150
The new mail gets added to the
chunk or non-chunk category,

00:21:51.280 --> 00:21:54.300
and then the map gets compiled again.

00:21:54.350 --> 00:21:58.360
So you call LSM map start
training to go back to training.

00:21:58.360 --> 00:22:02.500
You add your data,
LSM map compile to go back.

00:22:04.270 --> 00:22:10.980
To evaluate a document you call "LSM
Result Create," which is a somewhat,

00:22:10.980 --> 00:22:10.980
uh,

00:22:11.360 --> 00:22:14.690
Interesting name because it doesn't
have evaluate anywhere in the name,

00:22:14.690 --> 00:22:17.290
but what you get back from
this is an LSM result.

00:22:17.300 --> 00:22:20.590
So that's, again, consistent terminology.

00:22:20.830 --> 00:22:24.120
And to archive a map,
you call lsm_map_write to URL,

00:22:24.260 --> 00:22:27.540
lsm_map_create from
URL will load it back.

00:22:27.640 --> 00:22:31.210
And finally,
to compute clusters on a map,

00:22:31.210 --> 00:22:35.210
you call lsm_map_create_clusters,
and if you want to reorganize

00:22:35.210 --> 00:22:41.120
the map based on those clusters,
you call lsm_map_apply_clusters.

00:22:41.230 --> 00:22:43.440
So that's it for maps.

00:22:43.590 --> 00:22:46.620
Here we have assumed that you
get your texts from somewhere,

00:22:46.670 --> 00:22:50.340
and this is,
we're going to discuss this now

00:22:50.340 --> 00:22:51.460
in a little bit more detail.

00:22:51.460 --> 00:22:55.460
There are three ways of
putting data into a text.

00:22:55.740 --> 00:23:01.000
The simplest of them is you trust
our built-in parser and call

00:23:01.050 --> 00:23:06.480
lsm_text_add_words with just
a CFString of arbitrary length.

00:23:06.670 --> 00:23:09.460
We're going to chop up
this string for you,

00:23:09.460 --> 00:23:10.940
extract the words.

00:23:11.260 --> 00:23:14.380
And add those words to the text.

00:23:14.540 --> 00:23:17.530
Now, this is often sufficient.

00:23:17.760 --> 00:23:20.990
This is all the command line tool can do,
and as we've seen,

00:23:21.190 --> 00:23:25.440
command line tool can do
a lot of experimenting,

00:23:25.550 --> 00:23:28.360
prototyping,
and even actual applications.

00:23:28.450 --> 00:23:30.280
However,
there are some limitations there.

00:23:30.380 --> 00:23:37.290
First of all, we only,
you're at the mercy of our rules

00:23:37.290 --> 00:23:40.760
for what consists of a word.

00:23:40.760 --> 00:23:41.960
Secondly,
we throw away all words that consist

00:23:42.390 --> 00:23:45.840
purely of digits because those,
in many applications,

00:23:45.900 --> 00:23:48.500
those don't really add a lot
of information and clutter up

00:23:48.610 --> 00:23:53.270
the space because you have all
those numbers among the words.

00:23:53.440 --> 00:23:56.150
But in some applications,
if you have zip codes

00:23:56.290 --> 00:24:00.480
or something like that,
then the numbers become very important.

00:24:00.630 --> 00:24:05.880
In this case, and also in some languages,
our parser may not do the right thing.

00:24:05.880 --> 00:24:09.260
In this case,
you're welcome to write your own parser

00:24:09.330 --> 00:24:10.580
and to add the words individually.

00:24:10.590 --> 00:24:16.080
With LSM text, add word in the singular,
which also works.

00:24:16.220 --> 00:24:20.590
Finally, it doesn't,
the words don't have to be text.

00:24:20.720 --> 00:24:26.180
As Jerome hinted at, we don't,
LSM doesn't care what your words are.

00:24:26.340 --> 00:24:27.880
It doesn't look inside the words.

00:24:27.910 --> 00:24:32.210
All it uses the words
for is distinguishing,

00:24:32.220 --> 00:24:36.160
saying this is different from this one,
this is equal to this one.

00:24:36.320 --> 00:24:40.400
So you can have arbitrary binary
tokens instead of the words.

00:24:40.400 --> 00:24:43.950
So you just add a bunch
of bytes in a CFData.

00:24:44.120 --> 00:24:46.140
You call LSM text add token.

00:24:46.260 --> 00:24:50.530
It will be as good as a word
as far as LSM is concerned.

00:24:51.750 --> 00:24:56.220
Finally, when you evaluate such a text,
you get back an LSM result

00:24:56.220 --> 00:24:59.330
with lsm_result_create,
as we've said.

00:24:59.580 --> 00:25:05.320
Now, the most frequent result you want
from that is lsm_result_get_category,

00:25:05.330 --> 00:25:09.400
which is, is this a good guy,
is it a bad guy?

00:25:09.400 --> 00:25:14.780
Is it a junk mail, is it legitimate mail?

00:25:14.890 --> 00:25:19.690
Sometimes you also want to know the
scores associated with those categories.

00:25:19.760 --> 00:25:22.850
Our scores are normalized
between 0 and 1,

00:25:22.940 --> 00:25:25.860
so for each category,
you get back a score,

00:25:25.980 --> 00:25:28.370
and if you add them up
across all categories,

00:25:28.370 --> 00:25:29.090
you get 1.

00:25:31.330 --> 00:25:35.390
In some applications,
you don't actually want to know what

00:25:35.390 --> 00:25:40.290
the best category is for a document,
but you want to know what words

00:25:40.450 --> 00:25:44.100
best represent a document,
in which case you would

00:25:44.100 --> 00:25:48.350
call lsm_result_copy word,
or in the binary case,

00:25:48.450 --> 00:25:50.950
lsm_result_copy token.

00:25:51.070 --> 00:25:54.730
So that's more or less
all there is to the API.

00:25:54.810 --> 00:25:58.790
You will see more documentation in Xcode.

00:25:58.790 --> 00:25:59.850
You can read our headers.

00:25:59.930 --> 00:26:04.190
It should be relatively
simple to figure out.

00:26:04.380 --> 00:26:09.030
So what have we been using
LSM internally at Apple?

00:26:09.060 --> 00:26:12.940
As I said, one of our first applications
was junk mail filtering.

00:26:12.940 --> 00:26:16.300
In many ways,
this is a relatively simple application.

00:26:16.340 --> 00:26:24.070
We have two categories: legitimate mail,
junk mail, and we have

00:26:24.630 --> 00:26:29.540
In a typical map,
you will find tens of thousands of words,

00:26:29.740 --> 00:26:30.700
if not more.

00:26:31.060 --> 00:26:35.330
Now, there are some refinements that
we did for junk mail filtering.

00:26:35.590 --> 00:26:40.080
First of all, we did not just call
lsm_result_get_category.

00:26:40.310 --> 00:26:48.000
We don't treat the risk of throwing away
a legitimate mail just as bad as the

00:26:48.000 --> 00:26:50.160
risk of letting through a junk mail.

00:26:50.220 --> 00:26:54.010
Most people would rather see an
occasional junk mail than have

00:26:54.260 --> 00:26:56.270
a legitimate mail discarded.

00:26:56.580 --> 00:26:58.980
So, we introduce a slight bias.

00:26:59.210 --> 00:27:03.280
We want to err on the side of
caution and rather let through

00:27:03.440 --> 00:27:05.860
the extra mail every now and then.

00:27:05.920 --> 00:27:10.410
The way we do this is that we call
not just lsm_result_get_category and

00:27:10.430 --> 00:27:12.960
compare that to our junk mail category.

00:27:13.010 --> 00:27:16.060
We also make sure that the
score associated with this

00:27:16.060 --> 00:27:20.160
category is not just greater than
50%, which it always will be,

00:27:20.160 --> 00:27:26.160
if it's the first category,
but greater than maybe 55 or 60.

00:27:26.160 --> 00:27:30.800
So, we want a threshold that
is higher than just 50/50.

00:27:31.130 --> 00:27:35.790
The second complication associated with
junk mail is that the bad guys know

00:27:35.790 --> 00:27:37.720
that we're doing this kind of filtering.

00:27:37.720 --> 00:27:43.970
So, what they did was they started
spelling their keywords in funny

00:27:43.990 --> 00:27:47.730
ways so they wouldn't be caught
so easily by a simple filter.

00:27:47.740 --> 00:27:52.440
They added periods between the
letters or they started using accents

00:27:52.450 --> 00:27:55.140
where no accents are really needed.

00:27:55.140 --> 00:27:57.860
We call these heavy metal umlauts.

00:28:00.000 --> 00:28:05.300
And so, we added a couple of
heuristics to our parser.

00:28:05.430 --> 00:28:07.500
Obviously,
we're not going to tell in great

00:28:07.630 --> 00:28:12.290
detail what those heuristics are,
but the basic idea is that

00:28:12.320 --> 00:28:18.510
we recognize such attempts
of disguising your keywords,

00:28:18.510 --> 00:28:23.700
and we pluck out those keywords,
and these attempts, in fact,

00:28:23.700 --> 00:28:26.940
make the keywords stand out even more,
like sore thumbs.

00:28:27.090 --> 00:28:31.930
So, we're using the bad
guys' methods to help us,

00:28:31.970 --> 00:28:37.380
because their words are going to
be even more obvious afterwards.

00:28:37.530 --> 00:28:42.020
Which we do with this flag,
lsmtextappliesbyheuristics.

00:28:42.110 --> 00:28:45.750
The second problem is that,
the third problem is that the map will

00:28:45.750 --> 00:28:52.210
end up with all sorts of offensive words,
and we don't want third graders to look

00:28:52.320 --> 00:28:56.700
at the map in a text editor if they,
if your average third grader

00:28:56.700 --> 00:28:59.860
knows how to fire up Emacs
and look at the junk mail map.

00:29:00.080 --> 00:29:03.510
We don't want them to
point at the words and say,

00:29:03.510 --> 00:29:07.610
"Haha, I found a naughty word." So,
what we do here is add another flag,

00:29:07.670 --> 00:29:09.580
lsmmap#text.

00:29:09.640 --> 00:29:12.000
This is not cryptographic
security or anything,

00:29:12.000 --> 00:29:17.100
it's just the map will no longer
be readable in plain text.

00:29:18.310 --> 00:29:22.980
Another thing to emphasize is that we're
employing LSM as a last line of defense.

00:29:22.980 --> 00:29:27.470
This is not our ultimate
or it is our final judge of

00:29:27.580 --> 00:29:32.280
junk mail versus good mail,
but we're using a lot of rules before.

00:29:32.280 --> 00:29:36.470
First of all, the mails are already going
to be filtered by your

00:29:36.470 --> 00:29:37.890
Internet service provider.

00:29:37.960 --> 00:29:41.160
Most of them throw away a
lot of their incoming mail,

00:29:41.160 --> 00:29:43.580
and some of them are pretty good at this.

00:29:43.580 --> 00:29:46.420
So a lot of the junk mail is
going to be filtered before

00:29:46.420 --> 00:29:48.050
it even hits your machine.

00:29:48.060 --> 00:29:53.570
Once it hits your machine,
first we apply the rules you have

00:29:53.570 --> 00:29:59.490
added explicitly in the mail rules,
and anything that gets covered by one of

00:29:59.520 --> 00:30:02.200
these rules gets filed away as good mail.

00:30:02.200 --> 00:30:06.270
Another thing we do is we compare
it against your address book.

00:30:06.280 --> 00:30:11.690
Of course, the bad guys can forge your
senders so they look legitimate,

00:30:11.700 --> 00:30:14.640
but we want to err on
the side of caution.

00:30:14.660 --> 00:30:18.550
Anything that comes from somebody you
know and is in your address book gets

00:30:18.550 --> 00:30:20.300
considered to be legitimate mail.

00:30:20.300 --> 00:30:27.190
And only once none of these rules
fire do we use latent semantic mapping

00:30:27.290 --> 00:30:31.660
to decide as a last line of defense,
is this junk mail, is it good mail?

00:30:31.660 --> 00:30:37.660
So this was one of our first applications
of LSM that we employed in the OS.

00:30:37.660 --> 00:30:44.570
For one of the latest applications
of latent semantic mapping in the OS,

00:30:44.880 --> 00:30:49.170
I would like to bring up my
colleague Kim Silverman to talk

00:30:49.320 --> 00:30:52.520
about the application of LSM to Help.

00:30:52.630 --> 00:30:55.610
Thanks Matthias.

00:30:55.610 --> 00:30:55.610
Thank you.

00:30:57.700 --> 00:30:59.920
So this is new,
and we wanted to tell you about

00:30:59.920 --> 00:31:03.280
this because this is a different
way that you might think about

00:31:03.280 --> 00:31:06.750
applying Latent Semantic Mapping
in your own applications.

00:31:06.940 --> 00:31:12.370
OS X contains about 10,000
text documents to help users.

00:31:12.430 --> 00:31:15.040
You get these through the
Help menu that's in the menu

00:31:15.040 --> 00:31:17.800
bar of all the applications,
you type a query,

00:31:17.940 --> 00:31:21.280
and we bring back the best document.

00:31:21.380 --> 00:31:24.740
Well,
we try to bring back the best document.

00:31:24.890 --> 00:31:31.500
The problem is finding which
document is the most relevant.

00:31:31.500 --> 00:31:31.500
Let me show you an example.

00:31:31.640 --> 00:31:36.630
One of the most common search queries
that people send to help in OS X is

00:31:36.630 --> 00:31:40.360
the word "save", believe it or not.

00:31:40.700 --> 00:31:45.360
And if you type that into Snow Leopard,
these are the documents

00:31:45.360 --> 00:31:47.000
that are returned.

00:31:47.000 --> 00:31:47.950
Let's take a look at them.

00:31:48.060 --> 00:31:53.140
Top is installing and
uninstalling screensavers.

00:31:53.180 --> 00:31:56.640
Probably not very relevant
to saving documents,

00:31:56.680 --> 00:31:58.520
which is what I bet this query is about.

00:31:58.620 --> 00:32:00.380
So, meh.

00:32:00.420 --> 00:32:02.060
What about the next one?

00:32:02.690 --> 00:32:05.190
Turning off the screensaver in front row.

00:32:05.240 --> 00:32:06.580
Meh.

00:32:06.580 --> 00:32:08.030
What about the next one?

00:32:08.760 --> 00:32:11.420
Converting grab screenshots
to other formats.

00:32:11.420 --> 00:32:12.440
Meh.

00:32:12.440 --> 00:32:15.420
And so on it goes.

00:32:17.480 --> 00:32:18.900
Oh, wait a minute,
here's one that might be

00:32:18.900 --> 00:32:19.860
a little bit relevant.

00:32:19.960 --> 00:32:22.990
Saving a document in PostScript format.

00:32:23.030 --> 00:32:25.350
Marginally relevant.

00:32:25.460 --> 00:32:28.400
How many of you save documents
in PostScript formats?

00:32:28.560 --> 00:32:33.170
Yeah, see, okay,
so we're getting about a 5% hit rate.

00:32:33.360 --> 00:32:37.780
Retrieving the right
document is not trivial.

00:32:37.790 --> 00:32:41.100
And Snow Leopard does not
take a trivial approach.

00:32:41.290 --> 00:32:46.790
At its core, it uses Search Kit to find
help documents that contain the

00:32:46.820 --> 00:32:49.240
words that are in the query.

00:32:49.380 --> 00:32:51.570
The more query words
that are in a document,

00:32:51.670 --> 00:32:53.290
the more relevant it might be.

00:32:53.290 --> 00:32:57.600
In addition,
experts have added by hand synonyms

00:32:57.670 --> 00:33:03.300
to common typos and known words
that are related to each other.

00:33:05.610 --> 00:33:11.890
The problem is that... well,
there are two problems.

00:33:12.000 --> 00:33:17.530
One is, when people type a query,
the words in that query might also

00:33:17.530 --> 00:33:21.590
occur in documents that are not
really relevant to that query.

00:33:21.840 --> 00:33:27.000
The second problem is that the words
in the query might have synonyms

00:33:27.000 --> 00:33:29.870
that are used in the actual document.

00:33:29.870 --> 00:33:32.470
The person might have
not used the right words.

00:33:32.710 --> 00:33:36.570
So both of these suggest that maybe
latent semantic mapping could help

00:33:36.570 --> 00:33:38.650
things along and be fruitful here.

00:33:38.700 --> 00:33:43.110
So we take all of these 10,000
documents and map them into a

00:33:43.260 --> 00:33:46.270
multidimensional semantic space.

00:33:46.490 --> 00:33:49.850
As Jerome has mentioned,
words exist in the same

00:33:49.850 --> 00:33:51.970
space as the documents.

00:33:52.220 --> 00:33:56.440
When a user types in a query,
we map that query into the same space,

00:33:56.440 --> 00:34:00.310
find the nearest documents,
and show those to the user.

00:34:00.680 --> 00:34:02.780
How well does it work?

00:34:02.990 --> 00:34:07.650
Well, to evaluate,
we took a hundred of the most common

00:34:07.650 --> 00:34:11.090
queries that people type to help.

00:34:11.590 --> 00:34:15.860
For each query,
we retrieved the top documents using

00:34:15.860 --> 00:34:21.700
the keyword-based method and the top
documents using latent semantic mapping.

00:34:21.790 --> 00:34:27.940
About 20 people rated how relevant
each document was to its query.

00:34:28.020 --> 00:34:30.670
And for each query, therefore,
we ended up with two numbers.

00:34:30.940 --> 00:34:36.420
One was the average relevance of
results returned by keyword search,

00:34:36.490 --> 00:34:41.570
and the average relevance of results
returned by latent semantic mapping.

00:34:41.870 --> 00:34:46.050
The horizontal axis here
shows the relevance of results

00:34:46.050 --> 00:34:48.740
returned by the keyword search.

00:34:48.920 --> 00:34:53.300
Queries to the left
produce irrelevant results,

00:34:53.300 --> 00:34:58.300
the right-hand end means
highly relevant results.

00:34:58.300 --> 00:35:01.500
The vertical axis is the
relevance of results returned

00:35:01.590 --> 00:35:03.520
by Latent Semantic Mapping.

00:35:03.520 --> 00:35:05.500
Higher is better.

00:35:05.500 --> 00:35:09.700
Every point in this space is a query.

00:35:09.700 --> 00:35:14.140
Queries that land along this
diagonal would mean that Latent

00:35:14.230 --> 00:35:18.500
Semantic Mapping performed about
as well as a keyword search.

00:35:18.500 --> 00:35:22.660
Any queries that came out near
the bottom right would have been

00:35:22.660 --> 00:35:27.500
answered better by a keyword search
than by Latent Semantic Mapping.

00:35:27.500 --> 00:35:31.170
Queries that come out at the top
left would have been answered

00:35:31.220 --> 00:35:33.380
better by Latent Semantic Mapping.

00:35:33.920 --> 00:35:39.300
Here are the queries that were answered
better by Latent Semantic Mapping.

00:35:39.620 --> 00:35:43.550
Here are those where both
methods worked about the same.

00:35:43.640 --> 00:35:48.450
And here are the few where keyword
search actually performed better,

00:35:48.450 --> 00:35:50.140
but not much better.

00:35:51.670 --> 00:35:54.260
This region is particularly important.

00:35:54.410 --> 00:35:59.600
These are the queries for which a
keyword-based search totally failed,

00:35:59.670 --> 00:36:03.960
and LSM made the results
significantly better.

00:36:04.070 --> 00:36:07.790
One advantage of doing an analysis like
this is that you can identify those

00:36:07.790 --> 00:36:11.420
cases where LSM is not working so well.

00:36:11.660 --> 00:36:14.510
For example, this guy down here.

00:36:14.830 --> 00:36:18.020
Once you identify those cases,
you can analyze them,

00:36:18.190 --> 00:36:22.820
figure out what went wrong,
and do something to address the problem.

00:36:23.220 --> 00:36:27.700
Which brings me to what can you do
to improve how well Latent Semantic

00:36:27.810 --> 00:36:31.090
Mapping works in your own application?

00:36:31.340 --> 00:36:33.870
We send documents into the
Latent Semantic Mapping Framework

00:36:33.900 --> 00:36:35.700
and we get out a result.

00:36:35.730 --> 00:36:39.190
You can improve the performance
if you insert a pre-processor

00:36:39.580 --> 00:36:43.460
before you send the text from
your documents into the engine,

00:36:43.460 --> 00:36:47.250
and improve it even more
if you post-process the

00:36:47.250 --> 00:36:50.120
results on their way out.

00:36:50.120 --> 00:36:55.660
One way to pre-process those
results is to use engrams.

00:36:55.670 --> 00:36:59.320
Jerome foreshadowed this when he
mentioned syntactic relationships

00:36:59.360 --> 00:37:01.870
between adjacent words.

00:37:01.940 --> 00:37:06.640
Engrams are word pairs and word triplets,
and they can capture different

00:37:07.040 --> 00:37:09.560
senses of meanings of words.

00:37:09.560 --> 00:37:16.480
For example, click in double click
refers to a user action.

00:37:16.650 --> 00:37:21.550
Click in Key Click refers
to a system sound.

00:37:21.550 --> 00:37:25.220
Ngrams can capture these
kinds of differences.

00:37:25.220 --> 00:37:29.300
You can ask the Latent Semantic
Mapping engine itself to

00:37:29.300 --> 00:37:32.340
calculate all bigrams and,
if you wish,

00:37:32.340 --> 00:37:37.420
trigrams in your training data and
use these in the classification.

00:37:37.500 --> 00:37:42.710
Or you can preprocess the text and
explicitly add tokens into your documents

00:37:42.710 --> 00:37:43.790
to represent Ngrams that you care about.

00:37:44.030 --> 00:37:48.910
Another thing you can do, and should do,
is remove from your documents

00:37:49.260 --> 00:37:53.140
text that is not systematically
related to the content,

00:37:53.300 --> 00:37:54.880
to the meaning of those documents.

00:37:54.880 --> 00:37:59.920
For example, HTML tags,
or text that occurs in every document,

00:38:00.150 --> 00:38:02.970
such as "return to the contents".

00:38:03.840 --> 00:38:06.640
Another thing you can do is stemming.

00:38:06.760 --> 00:38:09.330
For example,
the words "save" and "saving"

00:38:09.330 --> 00:38:13.700
probably should be collapsed
together for purposes of semantics,

00:38:13.970 --> 00:38:16.700
but the word "saver" should not.

00:38:16.840 --> 00:38:22.800
The point is:
help latent semantic mapping,

00:38:22.800 --> 00:38:25.510
a statistical technique,
by capitalizing on your

00:38:25.950 --> 00:38:30.370
application domain-specific
knowledge and encoding that,

00:38:30.400 --> 00:38:35.120
implementing that as heuristics
in a text preprocessor.

00:38:35.670 --> 00:38:39.890
You can further improve the results
by implementing a post-processor.

00:38:39.960 --> 00:38:42.890
Let me walk you through
an example that we did.

00:38:43.160 --> 00:38:46.990
When you type a query to help in Lion,
we suggest some

00:38:47.100 --> 00:38:49.720
completions for your query.

00:38:49.720 --> 00:38:54.680
When we return the results,
we also return some related search terms.

00:38:54.920 --> 00:38:58.400
All of these are engrams
generated by the way we use the

00:38:58.400 --> 00:39:00.750
Latent Semantic Mapping Engine.

00:39:01.200 --> 00:39:03.460
And generally, they're quite helpful.

00:39:03.650 --> 00:39:04.920
But not always.

00:39:05.440 --> 00:39:08.960
For example,
there are documents that tell you

00:39:08.960 --> 00:39:10.730
how to clean up your workspace.

00:39:11.400 --> 00:39:14.640
There are other documents that tell
you how to set up your workspace.

00:39:14.990 --> 00:39:22.030
The most common substring that these
contain is "up your workspace".

00:39:22.520 --> 00:39:32.160
So, we filter the output, the engrams,
to exclude any engrams that

00:39:32.160 --> 00:39:32.160
begin or end with a preposition.

00:39:33.720 --> 00:39:35.660
So how can you make latent
semantic mapping work better

00:39:35.780 --> 00:39:36.960
for you in your own application?

00:39:36.960 --> 00:39:40.080
First of all,
think about are there aspects of my

00:39:40.080 --> 00:39:42.430
application where we could use it?

00:39:42.540 --> 00:39:45.970
We've shown you a few examples,
here are some more to get you thinking.

00:39:45.980 --> 00:39:46.930
Bookmarks.

00:39:47.040 --> 00:39:50.140
If your users have bookmarks,
you could classify them by analysing the

00:39:50.140 --> 00:39:52.480
documents that those bookmarks point to.

00:39:52.520 --> 00:39:56.930
Similarly,
if your users have RSS feeds coming in,

00:39:56.960 --> 00:40:00.530
you could analyse their content and
perhaps categorise them together or

00:40:00.910 --> 00:40:06.510
use this query approach that we've just
described to allow users to retrieve

00:40:06.510 --> 00:40:09.920
the RSS feeds or the documents that
they think are relevant by some query.

00:40:09.920 --> 00:40:13.670
If your users have media,
you could go out on the web,

00:40:13.760 --> 00:40:16.350
retrieve reviews and categorise
them by those reviews.

00:40:16.360 --> 00:40:20.100
Similarly,
if you've got wines and cheeses,

00:40:20.100 --> 00:40:23.110
go out and find the reviews and the
descriptions about those and categorise

00:40:23.110 --> 00:40:24.500
them by what people say about them.

00:40:24.580 --> 00:40:30.280
People have used latent semantic
mapping to analyse DNA sequences.

00:40:30.460 --> 00:40:32.560
In fact,
it's a pretty common technique for those.

00:40:35.330 --> 00:40:37.730
Some guidelines.

00:40:37.730 --> 00:40:40.850
In trying to think about can
latent semantic mapping help

00:40:40.960 --> 00:40:44.170
me in a task in my application,
you should ask yourself,

00:40:44.270 --> 00:40:48.800
is the problem that I'm going to
address syntactic or semantic in nature?

00:40:48.890 --> 00:40:54.120
If you're looking for syntactic patterns
such as dates or email addresses,

00:40:54.120 --> 00:40:58.560
latent semantic mapping is not
the appropriate technology.

00:40:58.560 --> 00:41:03.620
But if you want to sort things by
topic or retrieve documents by queries,

00:41:03.620 --> 00:41:04.220
then it will help.

00:41:04.680 --> 00:41:09.380
If you are training it to
distinguish between categories,

00:41:09.510 --> 00:41:13.060
then you have to make sure that those
categories are sufficiently distinct.

00:41:13.060 --> 00:41:20.590
It will be difficult for Latent
Semantic Mapping to learn the difference

00:41:20.650 --> 00:41:24.180
between economics and business,
but it will be much easier for it

00:41:24.180 --> 00:41:24.180
to learn the difference between
economics and entertainment.

00:41:25.660 --> 00:41:31.060
If you are training it,
you may have lots of documents already

00:41:31.300 --> 00:41:33.450
that you've already categorized by topic.

00:41:33.710 --> 00:41:35.800
For example,
Matthias showed you an example where

00:41:36.070 --> 00:41:39.150
we had lots of different text files of,
we knew which programming

00:41:39.150 --> 00:41:40.290
language they were in.

00:41:40.790 --> 00:41:42.650
If you're doing that,

00:41:42.910 --> 00:41:46.770
Partition your training set
randomly into 10 random chunks.

00:41:47.010 --> 00:41:50.750
Train on the first 9,
hold out the last chunk for testing.

00:41:50.930 --> 00:41:54.730
And do that for all
possible 90%/10% splits.

00:41:54.870 --> 00:42:01.800
That'll give you the best way
to predict how will this work on

00:42:01.800 --> 00:42:01.800
documents that I haven't seen yet.

00:42:03.630 --> 00:42:06.840
If, while you're testing you find that
some of the results look strange,

00:42:06.850 --> 00:42:09.480
then there are some things that
you can do to improve that.

00:42:09.490 --> 00:42:14.010
Some of them I've already told you about,
pre-processing and post-processing.

00:42:14.160 --> 00:42:17.960
In addition to another way to help the
pre-processing is to use stop words.

00:42:18.000 --> 00:42:22.520
The engine allows you to give it a list
of words that you want it to ignore.

00:42:22.590 --> 00:42:27.590
For example, words like "the" and "of"
will occur in all documents

00:42:27.650 --> 00:42:29.440
regardless of their meaning.

00:42:29.450 --> 00:42:33.680
And if your training set is large enough,
the engine can automatically learn these,

00:42:33.770 --> 00:42:35.680
but you might need an
infinite training set.

00:42:35.680 --> 00:42:38.800
You could probably help
it along by saying,

00:42:38.870 --> 00:42:42.950
"I know that the following words are
not going to help for my topics."

00:42:43.230 --> 00:42:45.740
Also,
experiment with the number of dimensions.

00:42:45.750 --> 00:42:48.000
By default,
we use the number of dimensions

00:42:48.000 --> 00:42:51.080
to match the number of
categories you're training it on.

00:42:51.200 --> 00:42:53.860
But if you're just
sending in all documents,

00:42:53.940 --> 00:42:57.480
like we did in the Help case,
then choose somewhere between

00:42:57.530 --> 00:43:00.120
100 and 300 dimensions.

00:43:00.170 --> 00:43:02.870
We used 300 for Help.

00:43:05.350 --> 00:43:08.440
The training data should,
as much as possible,

00:43:08.510 --> 00:43:12.400
represent the full breadth of the domain,
the kind of variability that you

00:43:12.400 --> 00:43:15.860
expect to see in the next document
that your user brings along.

00:43:15.990 --> 00:43:22.000
And if you have multiple categories,
try to have roughly even numbers

00:43:22.000 --> 00:43:22.000
of documents in each category.

00:43:22.280 --> 00:43:25.870
People ask, "So how much training
data should I have?" Well,

00:43:25.920 --> 00:43:27.140
it's hard to say.

00:43:27.360 --> 00:43:32.000
One rule of thumb is large
enough to cover the variability

00:43:32.000 --> 00:43:33.820
in your topic domain.

00:43:34.100 --> 00:43:40.090
A rule of thumb is try to have
at least 30,000 unique words.

00:43:40.100 --> 00:43:43.090
The more categories you
need to distinguish between,

00:43:43.120 --> 00:43:48.100
the more instances of each category
you need to let the engine see.

00:43:48.220 --> 00:43:51.270
And if the data is going
to change over time,

00:43:51.270 --> 00:43:55.440
such as news,
then you need to train on even more data.

00:43:56.510 --> 00:44:02.610
Finally, we suggest, recommend in fact,
that you integrate Latent Semantic

00:44:02.680 --> 00:44:04.870
Mapping with other sources of knowledge.

00:44:05.000 --> 00:44:07.630
It's not a replacement
for other techniques,

00:44:07.900 --> 00:44:09.500
it's a complement to them.

00:44:09.500 --> 00:44:13.500
It can often improve the
robustness of an overall system.

00:44:13.510 --> 00:44:16.680
Matthias already worked you
through in some detail what we

00:44:16.680 --> 00:44:18.300
did with the Junk Mail Filter.

00:44:18.500 --> 00:44:22.440
In the same way, when we were doing the
Kana to Kanji conversion,

00:44:22.490 --> 00:44:25.200
we did not replace everything
with the Latent Semantic Mapping,

00:44:25.200 --> 00:44:28.570
but rather used it as just an
additional source of information that

00:44:28.960 --> 00:44:32.080
was exploited in the final decisions.

00:44:32.530 --> 00:44:34.870
So, now over to all of you.

00:44:35.220 --> 00:44:37.490
Go forth and map some text.

00:44:37.490 --> 00:44:39.500
Thank you.

00:44:45.380 --> 00:44:48.180
We're supposed to show
you some more information.

00:44:48.180 --> 00:44:50.620
There's some documentation available.

00:44:50.670 --> 00:45:00.000
There's a mailing list at lists.apple.com
and that's a good place to talk about

00:45:00.000 --> 00:45:02.830
with people what worked for you,
what didn't work for you,

00:45:02.830 --> 00:45:02.830
and get hints and best practices.