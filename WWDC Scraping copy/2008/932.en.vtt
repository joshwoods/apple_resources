WEBVTT

00:00:20.930 --> 00:00:23.200
Welcome to session 932,
Performance Tuning

00:00:23.200 --> 00:00:24.880
Your Application with Shark.

00:00:24.990 --> 00:00:26.310
My name is Wade Tregaskiis.

00:00:26.380 --> 00:00:29.750
I work in the Architecture and
Performance group here at Apple.

00:00:30.500 --> 00:00:32.240
In this session,
we're going to show you how you

00:00:32.240 --> 00:00:35.240
can use Shark to improve the
performance of your applications.

00:00:35.380 --> 00:00:37.450
But before we get right
down into how to use Shark,

00:00:37.520 --> 00:00:41.040
I'd like to lay the groundwork a bit,
starting with why you're here.

00:00:41.170 --> 00:00:42.720
Hopefully,
you're all here for the same reason.

00:00:42.780 --> 00:00:43.800
You're about performance.

00:00:43.950 --> 00:00:47.080
You want your applications to
be faster and more responsive.

00:00:47.180 --> 00:00:50.580
This makes end users happier, of course,
and makes them more productive.

00:00:50.670 --> 00:00:53.870
It also gives you room to add
additional features to your

00:00:53.870 --> 00:00:55.930
application and enhancements.

00:00:56.170 --> 00:01:00.460
Another way of looking at performance,
though, is in terms of power and

00:01:00.490 --> 00:01:02.620
economy and efficiency.

00:01:03.100 --> 00:01:05.980
Typically,
a program that's more optimized,

00:01:05.980 --> 00:01:09.010
that runs faster, uses less power to do
the same amount of work.

00:01:09.080 --> 00:01:11.510
This is, of course,
really important today with so

00:01:11.570 --> 00:01:14.440
many of our users on portable
devices running off batteries,

00:01:14.440 --> 00:01:17.800
where they want to get the most life
out of those batteries for each charge.

00:01:17.800 --> 00:01:21.300
In a similar vein,
but at the other end of the spectrum,

00:01:21.300 --> 00:01:25.490
you also have your server people who
care about power much the same because

00:01:25.490 --> 00:01:29.040
it goes right to their bottom line in
terms of cooling and electricity costs.

00:01:29.040 --> 00:01:32.510
So these are the reasons
we care about performance.

00:01:34.100 --> 00:01:37.340
So let's look at when you should
start optimizing your application.

00:01:37.480 --> 00:01:40.710
If we just consider your
development workflow to start with,

00:01:41.030 --> 00:01:42.960
you start out by writing your code,
of course.

00:01:43.010 --> 00:01:45.990
Now, for this, you probably use some
high-level tools like Xcode,

00:01:45.990 --> 00:01:47.940
Interface Builder, so forth.

00:01:48.110 --> 00:01:50.240
And these allow you to write your code,
get it compiling,

00:01:50.300 --> 00:01:52.180
and get an application built.

00:01:52.510 --> 00:01:56.120
But this isn't an application you'd
probably want to release to end users,

00:01:56.120 --> 00:01:56.120
right?

00:01:56.160 --> 00:01:59.950
Because it's possibly going to
have some bugs still left in it.

00:02:00.030 --> 00:02:00.790
So, of course, you don't.

00:02:00.840 --> 00:02:03.350
You first test it and you debug it.

00:02:03.470 --> 00:02:06.580
Now,
debugging you can do using just Printf,

00:02:06.580 --> 00:02:07.040
right?

00:02:07.230 --> 00:02:10.430
But hopefully no one here
is just using Printf.

00:02:10.540 --> 00:02:13.250
Hopefully you're using the high-level
tools that we provide that allow

00:02:13.250 --> 00:02:15.160
you to do this much more easily.

00:02:15.310 --> 00:02:17.640
Things like Xcode with
its GDB integration,

00:02:17.640 --> 00:02:21.640
Instruments, malloc-debug, leaks,
and so forth.

00:02:21.640 --> 00:02:24.400
What these tools are
about is telling you,

00:02:24.400 --> 00:02:28.150
"Okay, not just that a problem occurred,
but where it occurred,

00:02:28.180 --> 00:02:30.100
what code was executing,
what your code was

00:02:30.110 --> 00:02:32.610
doing when it occurred,
what was leading up to that point."

00:02:32.800 --> 00:02:36.360
They basically let you go in and
see what your program was up to

00:02:36.360 --> 00:02:38.890
and figure out what's going on.

00:02:39.000 --> 00:03:49.300
[Transcript missing]

00:03:49.600 --> 00:03:52.130
And they, much the same,
let you get in there and see what

00:03:52.130 --> 00:03:55.380
your program is actually doing
and understand what's happening.

00:03:55.580 --> 00:03:58.000
And having used these tools,
they give your program

00:03:58.000 --> 00:04:01.720
that real final polish,
that real gloss and finish

00:04:02.330 --> 00:04:04.410
that users are going to love.

00:04:06.220 --> 00:04:08.210
All right, so let's get down to it.

00:04:08.290 --> 00:04:11.020
What do you need to do in order
to use Shark with your program?

00:04:11.020 --> 00:04:12.780
Technically nothing.

00:04:12.780 --> 00:04:15.420
I mean, you can fire up Shark whenever
you feel like it and start

00:04:15.430 --> 00:04:18.180
using it on your program,
but there's some things that you should

00:04:18.180 --> 00:04:19.710
do in order to get the most out of it.

00:04:19.760 --> 00:04:21.500
First up, debug your program.

00:04:21.500 --> 00:04:24.410
It's hard enough at times
improving your performance,

00:04:24.450 --> 00:04:27.660
let alone if it's not even
working properly to start with.

00:04:27.720 --> 00:04:31.300
So worry about getting it working,
and then worry about the speed.

00:04:31.940 --> 00:04:35.030
When you do get to the stage where
you want to improve the performance,

00:04:35.030 --> 00:04:37.610
you want to be profiling what
end users are going to see,

00:04:37.620 --> 00:04:39.190
because that's ultimately
what you care about.

00:04:39.230 --> 00:04:41.690
So you want to be profiling
your release build.

00:04:41.690 --> 00:04:45.130
Turn on all your optimizations,
turn on any flags, do any extra work that

00:04:45.240 --> 00:04:46.430
you would normally do.

00:04:46.440 --> 00:04:50.010
With one exception, you want to keep your
symbol information in,

00:04:50.100 --> 00:04:51.420
so don't strip those.

00:04:51.420 --> 00:04:53.620
Simply so that when you
see your results in Shark,

00:04:53.620 --> 00:04:56.210
you can still see all your
symbol names and your source code

00:04:56.230 --> 00:04:57.720
and that sort of information.

00:04:59.350 --> 00:05:01.640
Now, when you're looking at
a performance problem,

00:05:01.780 --> 00:05:03.390
you want to start out
by picking a metric,

00:05:03.460 --> 00:05:07.400
some way to quantify its performance
so that after every code change,

00:05:07.400 --> 00:05:10.840
you can measure it and see if you
actually did get a performance benefit.

00:05:10.840 --> 00:05:13.830
What this metric is will, of course,
depend on what problem you're

00:05:13.830 --> 00:05:16.800
looking at and what your application
does and various factors,

00:05:16.800 --> 00:05:20.550
but typical things are, of course,
response time, throughput, frame rate.

00:05:20.580 --> 00:05:23.070
Having picked one of these,
you then want to go and

00:05:23.160 --> 00:05:24.780
take a baseline measurement.

00:05:25.680 --> 00:05:28.320
Now, this step's really about
coming up with a benchmark,

00:05:28.320 --> 00:05:31.770
some kind of sequence of steps
or data that you use with your

00:05:31.810 --> 00:05:35.180
program that you can go and run
again in a repeatable manner so

00:05:35.260 --> 00:05:38.610
that every time you make a change,
you can rerun this little benchmark

00:05:38.610 --> 00:05:40.450
and see how the performance varies.

00:05:40.460 --> 00:05:42.750
I want to point out here,
although we're not going to

00:05:42.750 --> 00:05:45.420
really cover it in this session,
that Shark can be automated

00:05:45.420 --> 00:05:46.840
in a lot of different ways.

00:05:46.870 --> 00:05:50.370
So you can integrate it with your scripts
and with your programs and do all sorts

00:05:50.370 --> 00:05:52.250
of automated testing and profiling.

00:05:52.260 --> 00:05:55.480
If you want to know more about that,
check out the Shark manual.

00:05:55.600 --> 00:05:56.460
There's a whole section on it.

00:05:56.560 --> 00:06:00.350
Also, we talked about this in more detail
in the advanced session on Tuesday.

00:06:00.430 --> 00:06:03.420
Of course, if you missed that,
you can check it out on ADC on

00:06:03.420 --> 00:06:05.120
iTunes after the conference.

00:06:05.120 --> 00:06:08.030
So these are the things
that you should do first.

00:06:08.040 --> 00:06:10.630
There's also some things that you're
going to need to know in order

00:06:10.630 --> 00:06:12.200
to really use Shark effectively.

00:06:12.200 --> 00:06:16.090
Now, it's true that Shark is
actually really easy to use.

00:06:16.140 --> 00:06:18.140
We go to a lot of effort
to make this true.

00:06:18.140 --> 00:06:20.610
But it does produce a lot of data.

00:06:20.620 --> 00:06:25.400
So knowing how to refine it to focus
it in on just your particular issue.

00:06:25.540 --> 00:06:26.470
Is really important.

00:06:26.480 --> 00:06:28.240
And this really boils
down to three things.

00:06:28.240 --> 00:06:31.600
First up is knowing which configuration
to use for a given problem.

00:06:31.600 --> 00:06:34.810
Second is knowing how to focus
Shark at just the part of the

00:06:35.070 --> 00:06:36.770
system that's of interest.

00:06:36.780 --> 00:06:41.680
And the third thing is having gotten your
results using Shark's data mining tools.

00:06:41.680 --> 00:06:44.350
To focus in on just the parts of
your program and the execution

00:06:44.350 --> 00:06:46.830
that are actually relevant
and that you care about.

00:06:47.000 --> 00:06:49.140
Now, we're going to cover all
of these in this session.

00:06:49.140 --> 00:06:50.800
So you'll have this down pat by the end.

00:06:50.840 --> 00:06:53.830
But there's one thing that you
also need to know that we can't

00:06:53.830 --> 00:06:55.460
really fit into one session.

00:06:55.460 --> 00:06:57.560
Which is how to interpret your results.

00:06:57.560 --> 00:06:59.540
This basically boils down to experience.

00:06:59.540 --> 00:07:02.180
It's like debugging or
programming in general.

00:07:02.180 --> 00:07:05.210
It's just a matter of learning
and having dealt with similar

00:07:05.230 --> 00:07:07.100
problems and building on those.

00:07:07.100 --> 00:07:09.820
So the best way, of course,
to do this is to experiment.

00:07:09.820 --> 00:07:10.890
You know, go into Shark.

00:07:10.960 --> 00:07:12.020
Try different configurations.

00:07:12.020 --> 00:07:13.650
Try different programs.

00:07:13.660 --> 00:07:14.600
Look at different problems.

00:07:14.600 --> 00:07:16.060
Change your code.

00:07:16.060 --> 00:07:17.370
See how it affects your performance.

00:07:17.380 --> 00:07:18.420
You know, play around.

00:07:18.420 --> 00:07:21.040
You can also talk to other people.

00:07:21.040 --> 00:07:24.900
And we're going to do some demos
later in this session to effectively.

00:07:25.530 --> 00:07:27.980
show you some common problems and
how you're going to fix them and

00:07:27.980 --> 00:07:30.110
give you a bit of a jump start.

00:07:30.600 --> 00:07:34.370
All right, so that's what you should do
first and what you need to know.

00:07:34.470 --> 00:07:37.380
So let's go in and I'll
introduce you to Shark.

00:07:37.510 --> 00:07:39.840
So we'll just switch to the demo machine.

00:07:41.000 --> 00:07:43.970
So, the other demo machine.

00:07:44.000 --> 00:07:47.370
Thank you, Ryan.

00:07:48.970 --> 00:07:51.300
So, fire up Shark.

00:07:51.320 --> 00:07:52.300
I'll just hide that for the moment.

00:07:52.300 --> 00:07:54.880
So, when you launch Shark,
this is the main window.

00:07:54.880 --> 00:07:55.760
This is what you see first.

00:07:55.820 --> 00:07:58.710
Now, this is where you choose what
profiling technique you're going

00:07:58.710 --> 00:08:00.760
to use and the scope of it,
and also, of course,

00:08:00.760 --> 00:08:03.380
control profiling with the
start and stop button here.

00:08:03.380 --> 00:08:05.290
The first menu is the configuration.

00:08:05.290 --> 00:08:09.330
This is the profiling technique and
analysis techniques you're going to use.

00:08:09.330 --> 00:08:11.480
You can see here that there's
a whole bunch of these.

00:08:11.480 --> 00:08:13.400
We'll just go with time profile for now.

00:08:14.240 --> 00:08:15.480
The second menu is the scope.

00:08:15.540 --> 00:08:17.260
You can see there's three options here.

00:08:17.260 --> 00:08:20.030
You can choose to profile everything,
which is the whole system,

00:08:20.030 --> 00:08:22.950
so you can see everything running at,
you know, maybe the expense of a

00:08:22.950 --> 00:08:24.100
little more overhead.

00:08:24.100 --> 00:08:26.040
You can choose to profile
a specific process,

00:08:26.070 --> 00:08:27.100
which is really good.

00:08:27.100 --> 00:08:30.110
It's lower overhead, so if you're just
interested in your program,

00:08:30.110 --> 00:08:31.230
then you can do that.

00:08:31.430 --> 00:08:34.810
And for some configurations,
you can even profile a binary file on

00:08:34.810 --> 00:08:36.820
disk without even having to run it.

00:08:36.880 --> 00:08:39.620
Now, if you choose to profile
a particular process,

00:08:39.660 --> 00:08:42.920
the window expands and you get
a list of processes to target.

00:08:42.920 --> 00:08:44.220
I'd like to point out that
this is a very simple process.

00:08:44.220 --> 00:08:44.220
You can choose to profile a binary file
on disk without even having to run it.

00:08:44.220 --> 00:08:46.330
And I'd like to point out just
quickly that this is how you would

00:08:46.330 --> 00:08:47.760
go about launching a process as well.

00:08:47.830 --> 00:08:50.030
This will then,
when you click the start button,

00:08:50.040 --> 00:08:52.310
having chosen launch,
it'll put up a dialog and you

00:08:52.310 --> 00:08:54.690
can set your executable path,
your arguments and all

00:08:54.690 --> 00:08:55.840
that sort of stuff.

00:08:55.840 --> 00:08:59.040
Now, I've got a demo application
here that I want to use,

00:08:59.040 --> 00:09:02.310
and we'll get to this a little
bit in detail a little later,

00:09:02.320 --> 00:09:04.840
but first up, I'll just show it to you.

00:09:04.980 --> 00:09:09.110
So what it is is an MPEG-2 decoder that
we just downloaded off the Internet.

00:09:11.710 --> 00:09:14.180
Thank you.

00:09:14.250 --> 00:09:16.250
And I'll just build it and
run it so you can see it.

00:09:16.520 --> 00:09:19.700
And we just slapped a real
simple Cocoa GUI on it so

00:09:19.700 --> 00:09:22.090
it would display movies.

00:09:22.470 --> 00:09:26.280
And we want to use this to
display HD MPEG-2 movies.

00:09:26.410 --> 00:09:27.900
So I have a test movie here.

00:09:27.920 --> 00:09:31.020
I'll just pull that up and we'll
play that so you can see it.

00:09:31.110 --> 00:09:33.400
So this is one of the astronauts up
on the International Space Station.

00:09:33.400 --> 00:09:34.350
He's cruising around.

00:09:34.360 --> 00:09:35.800
He's having fun.

00:09:35.870 --> 00:09:37.900
Unfortunately, he's cruising a little
slower than I'd like,

00:09:37.950 --> 00:09:40.100
because this is a
30-frame-per-second video.

00:09:40.140 --> 00:09:43.560
And you can see here,
we're only getting 22 frames a second.

00:09:43.620 --> 00:09:45.360
So that's obviously our
performance problem,

00:09:45.360 --> 00:09:47.090
and that's also our
metric we're going to use.

00:09:47.150 --> 00:09:50.490
We want to get up to 30
frames a second or beyond.

00:09:50.650 --> 00:09:52.570
Now, this is where we fire up Shark.

00:09:52.630 --> 00:09:55.460
We choose our process,
our MPEG-2 decoder,

00:09:55.460 --> 00:09:57.040
and we start profiling.

00:09:57.060 --> 00:09:57.850
So we click Start.

00:09:58.160 --> 00:09:59.440
Shark's doing its sampling.

00:09:59.440 --> 00:10:01.500
We give it a few seconds
to collect some samples.

00:10:01.500 --> 00:10:02.460
That's plenty.

00:10:02.460 --> 00:10:05.100
So we'll stop it,
and we can stop our movie now.

00:10:05.740 --> 00:10:07.190
And now it's doing its analysis.

00:10:07.190 --> 00:10:09.440
This is where it goes through
and crunches the numbers

00:10:09.440 --> 00:10:11.740
and looks at the data,
and it pulls symbol information

00:10:11.740 --> 00:10:14.340
out of your binaries,
so it can display those to you later.

00:10:14.340 --> 00:10:17.140
And when it's done this,
it'll produce a session window,

00:10:17.140 --> 00:10:20.250
which is where you spend your time
looking at what your program's

00:10:20.250 --> 00:10:22.400
doing and making your optimizations.

00:10:22.400 --> 00:10:25.420
So when it does that, in a few seconds,
please.

00:10:25.430 --> 00:10:26.270
Thank you.

00:10:26.430 --> 00:10:28.280
So here we have a session window.

00:10:28.280 --> 00:10:31.980
Now, before I go in and we start
optimizing this program,

00:10:31.980 --> 00:10:35.640
I want to jump back to slides and talk
about what a time profile actually is.

00:10:35.740 --> 00:10:38.660
how it works, and how you can use it.

00:10:40.380 --> 00:10:43.600
So, I mentioned briefly, I mean,
you saw in the configuration menu,

00:10:43.600 --> 00:10:45.580
there's a whole bunch of
them that you can use.

00:10:45.580 --> 00:10:48.200
I'm only going to cover, well,
we're only going to cover

00:10:48.270 --> 00:10:50.910
the main three today,
the three most common ones that

00:10:50.910 --> 00:10:52.740
will get most of what you need done.

00:10:52.740 --> 00:10:56.310
We're also going to do a little
call out to some of our Java tracing

00:10:56.310 --> 00:10:57.970
options towards the end.

00:10:57.980 --> 00:11:03.140
But if you want to know more, as I said,
play around with Shark or read the manual

00:11:03.140 --> 00:11:07.960
or check out the advanced session where
we covered some additional configurations

00:11:07.960 --> 00:11:09.690
and system trace in more detail.

00:11:11.500 --> 00:13:59.200
[Transcript missing]

00:13:59.870 --> 00:14:02.470
All right, so that's time profile
and how you use it.

00:14:02.550 --> 00:14:05.860
What are the advantages and why
would you want to use time profile as

00:14:05.950 --> 00:14:08.220
compared to other profiling techniques?

00:14:08.220 --> 00:14:11.650
Well, in its favor,
time profile is really easy.

00:14:11.650 --> 00:14:14.970
It requires no work on your part,
no code changes, no recompilation,

00:14:14.970 --> 00:14:15.850
nothing at all.

00:14:15.860 --> 00:14:18.480
In fact, you don't even need the
developer tools installed,

00:14:18.480 --> 00:14:20.850
just Shark,
so even end users can use time profile.

00:14:20.880 --> 00:14:23.010
So that's one good thing,
it's really easy.

00:14:23.010 --> 00:14:25.900
The second thing is those call
stacks we were talking about,

00:14:26.010 --> 00:14:28.460
they're accurate right down
to the instruction level.

00:14:28.960 --> 00:14:31.820
So you can see exactly where you were
at that particular moment in time.

00:14:31.840 --> 00:14:35.460
And the third good thing,
which is really the most important,

00:14:35.540 --> 00:14:37.280
is that it's really low overhead.

00:14:37.340 --> 00:14:41.320
On a modern machine,
less than a percent of CPU,

00:14:41.460 --> 00:14:43.930
a few megabytes of RAM, really nothing.

00:14:43.940 --> 00:14:46.260
And this has the important
consequence that it doesn't

00:14:46.260 --> 00:14:47.860
generally affect your profiling.

00:14:49.350 --> 00:14:51.050
And this is a really important point.

00:14:51.060 --> 00:14:52.620
I can't overstress this enough.

00:14:52.620 --> 00:14:56.030
You really want to be profiling,
as I said before,

00:14:56.030 --> 00:14:57.440
what the end user is going to see.

00:14:57.440 --> 00:15:01.910
Now, if your profiling technique that
you're using has really high overhead,

00:15:01.910 --> 00:15:04.180
that's going to impact your program.

00:15:04.180 --> 00:15:07.000
And so you're no longer really
seeing what the end user is seeing.

00:15:07.000 --> 00:15:08.740
You're kind of seeing something else.

00:15:08.740 --> 00:15:11.890
And you can go in and make
changes then that look good,

00:15:11.890 --> 00:15:15.420
but then aren't actually what
you want when you go and run them

00:15:15.570 --> 00:15:17.550
on a standalone end user system.

00:15:18.390 --> 00:15:22.190
So low overhead is a really,
really good thing.

00:15:22.450 --> 00:15:24.400
Of course, you know,
all these things don't

00:15:24.400 --> 00:15:25.730
come without a trade-off.

00:15:25.850 --> 00:15:26.460
There is a con.

00:15:26.460 --> 00:15:30.080
And that brings us to the main
disadvantage of time profiling,

00:15:30.080 --> 00:15:31.550
which is that it's not exact.

00:15:31.650 --> 00:15:34.000
It's not telling you every
single function call,

00:15:34.000 --> 00:15:35.920
every single line of code executed.

00:15:35.920 --> 00:15:37.480
It's just a statistical method.

00:15:37.480 --> 00:15:38.860
So let's take an example.

00:15:38.860 --> 00:15:42.160
Here we have a program running,
and the white lines here are where

00:15:42.160 --> 00:15:44.200
Shark has come in and taken a sample.

00:15:44.200 --> 00:15:47.210
So from Shark's point of view,
what the program was

00:15:47.210 --> 00:15:48.880
doing was actually this.

00:15:49.670 --> 00:15:50.830
Now,
you can see that this is quite different.

00:15:50.830 --> 00:15:52.560
There's two real problems here.

00:15:52.560 --> 00:15:56.320
First, in the first half,
Shark has come in and sampled on

00:15:56.370 --> 00:15:58.630
either side of these functions.

00:15:58.640 --> 00:15:59.690
So it didn't see them at all.

00:15:59.700 --> 00:16:00.680
It doesn't think they happened.

00:16:00.700 --> 00:16:03.700
On the other hand, these functions happen
to fall on a sample,

00:16:03.780 --> 00:16:06.590
but they may not have actually
been running for very long.

00:16:06.590 --> 00:16:09.330
But Shark doesn't know that,
so it just assumes they were

00:16:09.330 --> 00:16:11.240
running for the whole interval.

00:16:11.240 --> 00:16:13.460
And you can see that the
result is kind of like,

00:16:13.460 --> 00:16:15.620
well, that's not what my program's doing.

00:16:15.690 --> 00:16:18.350
I mean, how is this going to be helpful,
right?

00:16:19.320 --> 00:16:22.150
Luckily, because these are opposites,
when you get enough samples,

00:16:22.150 --> 00:16:24.140
they average out,
and time profile still gives

00:16:24.140 --> 00:16:26.790
you an accurate picture of what
your program is actually doing.

00:16:26.820 --> 00:16:30.490
Now, the default sampling rate for
time profile is 1 kilohertz,

00:16:30.570 --> 00:16:32.350
1,000 times a second.

00:16:32.380 --> 00:16:34.560
Now, that's actually pretty high.

00:16:34.570 --> 00:16:36.350
And for most programs,
nearly all of them,

00:16:36.350 --> 00:16:39.090
that's going to give you an
accurate picture of what's going on.

00:16:39.100 --> 00:16:41.170
If you find yourself
sort of cranking that up,

00:16:41.180 --> 00:16:43.510
you know,
thinking that you need to make it higher,

00:16:43.510 --> 00:16:45.930
and especially if you're
going right up to the limit,

00:16:45.930 --> 00:16:48.720
you probably don't actually
want to use time profile.

00:16:49.130 --> 00:16:50.890
You probably want to try
different techniques,

00:16:50.890 --> 00:16:52.600
such as function tracing or system trace.

00:16:52.600 --> 00:16:55.380
And we'll get to system trace
in the second half of this talk.

00:16:55.440 --> 00:16:57.820
But let's focus on time profile.

00:16:57.830 --> 00:17:01.910
So you saw before at the end of
our little brief demo that this is

00:17:01.930 --> 00:17:04.440
what a Shark session looks like,
and this is a time profile.

00:17:04.440 --> 00:17:08.020
So you have these tabs at the top,
and we're looking at profile by default.

00:17:08.020 --> 00:17:09.890
And this shows you your call stacks.

00:17:09.940 --> 00:17:12.740
Now, when it first opens,
it's in heavy mode,

00:17:12.740 --> 00:17:15.840
and this is showing you the
tops of your call stacks,

00:17:15.840 --> 00:17:17.710
the leafs of the call trees.

00:17:18.800 --> 00:17:20.200
And this is a really
great place to start,

00:17:20.200 --> 00:17:22.030
because straightaway,
it's answering the question,

00:17:22.140 --> 00:17:23.100
what code is executing?

00:17:23.100 --> 00:17:25.340
What is my program actually doing?

00:17:25.880 --> 00:17:29.060
Furthermore, since it's sorted,
you can see that the functions that are

00:17:29.060 --> 00:17:30.930
executing most go straight to the top.

00:17:31.250 --> 00:17:33.160
And this is where you play
a game of whack-a-mole.

00:17:33.180 --> 00:17:35.820
You focus in on the functions
at the top and just ignore,

00:17:35.820 --> 00:17:37.950
for the moment at least,
everything below,

00:17:37.950 --> 00:17:39.420
because they don't matter.

00:17:39.990 --> 00:17:41.800
This is a great way of
looking from the bottom up.

00:17:41.920 --> 00:17:44.670
You're going straight to where
you're actually executing,

00:17:44.760 --> 00:17:46.540
and by clicking on the
disclosure triangles,

00:17:46.580 --> 00:17:49.560
you can trace back through the
callers to see how you got there,

00:17:49.560 --> 00:17:52.500
you know,
all the way back from your main function.

00:17:52.550 --> 00:17:55.060
However, sometimes you kind of want
to look at it the other way.

00:17:55.060 --> 00:17:58.650
You want to start from your main function
and sort of work down from there,

00:17:58.650 --> 00:18:03.180
you know, see what main called,
where its time was divided,

00:18:03.240 --> 00:18:05.980
and then look down to the next level,
where the time went there,

00:18:06.000 --> 00:18:09.210
and sort of work your
way down instead of up.

00:18:09.430 --> 00:18:12.800
This is the standard TreeView,
which we just call TreeView.

00:18:12.930 --> 00:18:15.600
This is a really good way of looking
at things for a couple of reasons,

00:18:15.600 --> 00:18:18.290
but mainly because it
shows you different ways,

00:18:18.300 --> 00:18:21.390
different code paths you can
use to reach a given function.

00:18:21.410 --> 00:18:23.800
For example,
the square root function here.

00:18:23.880 --> 00:18:26.690
You can see it's called in two
completely different code paths.

00:18:26.810 --> 00:18:31.010
This is important from an optimization
perspective because how you optimize

00:18:31.150 --> 00:18:34.620
that function may depend or may
be completely different for one

00:18:34.620 --> 00:18:37.390
code path as opposed to another.

00:18:37.790 --> 00:18:39.730
In Shark, of course,
the tree view looks like this,

00:18:39.730 --> 00:18:41.700
just the outline view, like a heavy view.

00:18:41.820 --> 00:18:43.640
Now,
we got here by going to the View menu

00:18:43.640 --> 00:18:45.380
down here and choosing Tree.

00:18:45.480 --> 00:18:47.440
You can also, from there,
choose to view both tree

00:18:47.440 --> 00:18:48.700
and heavy view together.

00:18:48.740 --> 00:18:52.480
If you want to look at both,
look at it from both angles.

00:18:53.420 --> 00:18:55.290
Now,
this is all good and well for a simple

00:18:55.290 --> 00:18:57.200
program with all of three functions,
right?

00:18:57.200 --> 00:19:00.260
But in a real program,
these views get really busy.

00:19:00.260 --> 00:19:02.860
You have a lot of different functions,
your call tree is very large,

00:19:02.860 --> 00:19:05.920
there's all different code paths,
and it can quickly get to a

00:19:05.920 --> 00:19:09.440
point where the parts that are
interesting are obscured by

00:19:09.470 --> 00:19:11.670
parts that really don't matter.

00:19:12.160 --> 00:19:14.260
This is where Shark's
data mining comes in.

00:19:14.260 --> 00:19:16.320
For example,
we had this call tree before,

00:19:16.440 --> 00:19:19.360
and we've got these trig functions here,
cos and square root,

00:19:19.360 --> 00:19:21.600
which are from the system's math library.

00:19:21.600 --> 00:19:23.600
This is a library that
comes with the system,

00:19:23.610 --> 00:19:27.100
so it's something that you as third-party
developers can't actually change,

00:19:27.100 --> 00:19:29.250
and it's already pretty
well optimized anyway,

00:19:29.320 --> 00:19:29.900
right?

00:19:29.920 --> 00:19:31.610
So you really don't care about that.

00:19:31.610 --> 00:19:34.500
What you're interested in is
how you're using that library.

00:19:34.500 --> 00:19:38.200
So you'd rather get rid of these
functions and just take their

00:19:38.340 --> 00:19:41.850
samples and push them up into
their callers in your code.

00:19:42.160 --> 00:19:44.410
And then you have a
much simpler call tree,

00:19:44.600 --> 00:19:46.850
and you can still see
where the time was going,

00:19:46.850 --> 00:19:49.540
but you don't have to
deal with all the details.

00:19:49.540 --> 00:19:51.770
Now,
that's just an example of one data mining

00:19:51.770 --> 00:19:53.880
technique called charging to callers.

00:19:53.930 --> 00:19:56.500
Shark has a whole host of
other ones you can use,

00:19:56.620 --> 00:19:59.140
but we don't have time, unfortunately,
in this session to go through them.

00:19:59.140 --> 00:20:01.550
They were covered in the
advanced session on Tuesday,

00:20:01.550 --> 00:20:03.120
and they're also in the manual.

00:20:03.820 --> 00:20:07.550
So that's one good way of looking at your
time profile results using call stacks.

00:20:07.560 --> 00:20:10.000
Another way is using the chart view.

00:20:10.000 --> 00:20:14.030
This simply shows your program's
execution over time where the

00:20:14.030 --> 00:20:16.030
y-axis is the call stack depth.

00:20:16.140 --> 00:20:19.600
And this actually turns out to be
a really good way of seeing how

00:20:19.600 --> 00:20:22.440
your execution changes over time,
the different phases of execution

00:20:22.490 --> 00:20:23.620
your program goes through.

00:20:23.620 --> 00:20:24.950
For example,
you can see here on the left,

00:20:25.040 --> 00:20:28.360
the first 400 samples,
your program's doing something.

00:20:28.360 --> 00:20:29.660
I mean, it has some sort of pattern.

00:20:29.660 --> 00:20:31.990
But over here,
it's doing something else entirely.

00:20:32.000 --> 00:20:33.240
It's got a completely different pattern.

00:20:33.770 --> 00:20:35.620
And in between,
there's these spires where

00:20:35.620 --> 00:20:37.450
something else crazy is going on.

00:20:37.630 --> 00:20:39.840
And the red here actually
denotes kernel time.

00:20:39.840 --> 00:20:41.620
So this is probably a
system call we made,

00:20:41.620 --> 00:20:42.970
and it's doing some work on our behalf.

00:20:42.980 --> 00:20:46.720
Of course, you can zoom in on these,
so you can get a closer look and a

00:20:46.910 --> 00:20:50.250
better feel for what's going on using
either the zoom slider or simply by

00:20:50.310 --> 00:20:52.320
drag selecting the area of interest.

00:20:52.320 --> 00:20:56.960
And that can take you right down into
the individual samples themselves.

00:20:56.960 --> 00:21:00.240
If you click on one of those,
it highlights, as you can see.

00:21:00.240 --> 00:21:02.270
And over here on the right,
you can see the call stack

00:21:02.380 --> 00:21:03.160
for that particular sample.

00:21:03.320 --> 00:21:07.190
This is a great way to see
what your program is doing at

00:21:07.190 --> 00:21:09.480
that particular moment in time.

00:21:10.150 --> 00:21:14.060
Now, so far we've shown that you can get
down to the individual functions.

00:21:14.060 --> 00:21:16.460
Now, that's really good for finding
out where your time is going.

00:21:16.470 --> 00:21:19.960
But knowing which function is
executing may not be enough to help

00:21:19.960 --> 00:21:22.250
you figure out why it's taking so long.

00:21:22.260 --> 00:21:24.670
For that,
what you can do is just double click

00:21:24.860 --> 00:21:29.120
on a function anywhere you see it in a
profile view or a call stack somewhere,

00:21:29.120 --> 00:21:33.280
and that will open a tab with a source
code for that function and go straight

00:21:33.300 --> 00:21:37.800
to the lines in that function where
you're spending most of your time.

00:21:38.780 --> 00:21:41.750
You can see on the left here that
we have the breakdown by line of how

00:21:41.870 --> 00:21:45.320
much time is being spent on each one,
and there's also the color coding.

00:21:45.320 --> 00:21:47.670
So straight away you can see
what the hottest code is,

00:21:47.700 --> 00:21:49.560
where you're spending all your time.

00:21:49.560 --> 00:21:52.450
This color coding also extends to
the scroll bar over on the right,

00:21:52.450 --> 00:21:55.130
so you can navigate around large
functions or large files with

00:21:55.130 --> 00:21:56.650
lots of different hotspots.

00:21:58.340 --> 00:22:00.010
At this point,
you may have figured out why your

00:22:00.100 --> 00:22:02.140
function is running so slow and
you want to go in and fix it.

00:22:02.190 --> 00:22:05.410
You can simply click the
Edit button to open it in Xcode or

00:22:05.410 --> 00:22:09.300
whatever your favorite editor is,
and then go and make your changes.

00:22:09.300 --> 00:22:12.480
Alternatively,
you may not still know at this

00:22:12.480 --> 00:22:16.580
point why this is so slow,
so you know now what line is slow,

00:22:16.580 --> 00:22:20.320
but you might still think, well, okay,
say here we have some floating point.

00:22:20.320 --> 00:22:22.240
Well, that looks sort of okay.

00:22:22.240 --> 00:22:23.430
I mean, what's slow about it, right?

00:22:24.140 --> 00:22:26.750
To get a closer look,
you can click on the Assembly button

00:22:26.750 --> 00:22:29.350
to toggle to the Assembly view,
and now you can see down

00:22:29.350 --> 00:22:31.800
to the instruction level
where your time is going.

00:22:31.800 --> 00:22:34.320
Again, color-coded with the
percentages broken down.

00:22:34.320 --> 00:22:36.700
And even if you're not
an Assembly person,

00:22:36.700 --> 00:22:39.610
this can still be a great
way to figure out exactly why

00:22:39.610 --> 00:22:41.320
a particular line is slow.

00:22:42.960 --> 00:22:46.490
And I pointed out also,
because you may find yourself

00:22:46.580 --> 00:22:48.680
double-clicking on a function,
and it goes straight to this.

00:22:48.680 --> 00:22:50.120
You don't see the source code at all.

00:22:50.120 --> 00:22:52.670
This typically happens if
you're looking at something you

00:22:52.740 --> 00:22:54.250
don't have the source code for.

00:22:54.260 --> 00:22:56.340
You know, a kernel function,
or a system library,

00:22:56.340 --> 00:22:58.350
or you're an end user,
and you simply don't have the

00:22:58.350 --> 00:23:00.020
source code for the application.

00:23:00.020 --> 00:23:02.410
Now, if that's the case,
or in any other case,

00:23:02.410 --> 00:23:04.740
and you find yourself
in this assembly view,

00:23:04.740 --> 00:23:08.340
and you're not really sure, you know,
what it all means, that's cool,

00:23:08.420 --> 00:23:10.290
because Shark's going
to try and help you.

00:23:10.780 --> 00:23:12.970
You can see here that there's a
column there with an exclamation

00:23:12.970 --> 00:23:14.270
mark and this little button in it.

00:23:14.340 --> 00:23:17.800
This is Shark saying, hey,
I've noticed that here you're

00:23:17.800 --> 00:23:20.180
doing something which could
be a performance problem.

00:23:20.180 --> 00:23:23.170
If you click on it,
it'll tell you what it's seeing,

00:23:23.200 --> 00:23:25.210
and it'll suggest a fix for you.

00:23:25.220 --> 00:23:28.050
In this case, we've got a loop doing
some floating point code,

00:23:28.270 --> 00:23:30.140
but using scalars, just the normal FPU.

00:23:30.140 --> 00:23:33.560
So this is maybe something where we
could go and look at vectorizing it.

00:23:35.320 --> 00:23:38.650
Now, the second thing that Shark does
that's helpful in this view,

00:23:38.710 --> 00:23:40.730
if you're not familiar with it,
is if you see an instruction

00:23:41.280 --> 00:23:43.740
that seems to be important,
but you're not sure what it does,

00:23:43.780 --> 00:23:44.960
just select it.

00:23:44.980 --> 00:23:47.320
And then click on the Help button
down the bottom right there.

00:23:47.320 --> 00:23:49.660
And that'll take you straight to
the part of the instruction set

00:23:49.670 --> 00:23:51.980
reference describing that function.

00:23:51.980 --> 00:23:53.980
So you can read up on it.

00:23:54.460 --> 00:23:56.760
All right,
so that's time profile from the very

00:23:56.890 --> 00:23:58.180
top right down to the very bottom.

00:23:58.200 --> 00:24:02.130
So let's go and use all this
in a real example to see how we

00:24:02.130 --> 00:24:03.930
can optimize our MPEG-2 decoder.

00:24:03.940 --> 00:24:06.350
So let's jump over to the demo machine.

00:24:08.630 --> 00:24:10.360
Okay, so this is where we left off.

00:24:10.450 --> 00:24:12.880
And we've got our little
time profile here.

00:24:12.880 --> 00:24:14.440
And now we know how to use it.

00:24:14.510 --> 00:24:17.870
So straightaway we can see that we want
to focus in on this reference IDCT.

00:24:17.930 --> 00:24:19.680
That's simply where we're
spending most of our time.

00:24:19.700 --> 00:24:21.910
But while we're here,
I'd just like to point out

00:24:22.030 --> 00:24:23.510
that floor is also showing up.

00:24:23.590 --> 00:24:26.220
This is the standard
floor in the math library.

00:24:26.220 --> 00:24:29.320
And it's kind of odd that it
would be showing up so much.

00:24:29.320 --> 00:24:31.500
And Shark kind of agrees
with this intuition.

00:24:31.500 --> 00:24:32.920
It's offering us a tip here.

00:24:32.920 --> 00:24:34.920
If we click on this,
it's saying that because

00:24:34.970 --> 00:24:37.260
we're using floor so much,
maybe we're interested

00:24:37.260 --> 00:24:38.460
in this alternative.

00:24:38.460 --> 00:24:39.820
So we're going to use
the alternative version,

00:24:39.830 --> 00:24:42.220
which isn't quite as accurate,
but it's a little bit faster.

00:24:42.220 --> 00:24:46.070
So we thought, well, all right,
we'll give that a shot, see if it works.

00:24:46.070 --> 00:24:49.690
So we copied that into our program,
took nearly 30 seconds to do.

00:24:49.690 --> 00:24:52.280
And that gave us this
version of our decoder.

00:24:52.280 --> 00:24:53.540
Well, that's not bad.

00:24:53.600 --> 00:24:58.160
I mean, you can see now we're getting 25,
26 frames a second up from, what was it,

00:24:58.160 --> 00:24:59.000
22 before.

00:24:59.180 --> 00:25:00.320
So that's pretty good.

00:25:00.320 --> 00:25:03.980
That's a 15, 20% improvement from a
30-second code change.

00:25:03.980 --> 00:25:05.790
I was pretty happy with that.

00:25:06.160 --> 00:25:09.240
But it's still not playing
We want to play in real-time.

00:25:09.370 --> 00:25:10.420
We want 30 frames a second.

00:25:10.420 --> 00:25:12.380
So we're going to have
to do some more work.

00:25:12.480 --> 00:25:15.920
Let's go back to our profile and go
into this Reference IDCT function

00:25:15.920 --> 00:25:17.160
and see what's going on.

00:25:17.160 --> 00:25:18.820
So I just double-clicked on it.

00:25:18.820 --> 00:25:20.080
And oops.

00:25:20.080 --> 00:25:21.660
We'll go to the source view.

00:25:21.660 --> 00:25:24.460
So now you can see --
hopefully you can see.

00:25:24.460 --> 00:25:26.620
I'll just increase the font size for you.

00:25:26.810 --> 00:25:29.120
Now you can see what
this function is doing.

00:25:29.120 --> 00:25:32.120
It's just spending all this time
inside these loops doing all

00:25:32.330 --> 00:25:34.180
this floating-point arithmetic.

00:25:34.300 --> 00:25:38.260
We looked at this for a minute or two
and we came to -- we realized two things.

00:25:38.260 --> 00:25:42.770
The first thing is it's taking as
its input and its output -- it's the

00:25:42.770 --> 00:25:45.900
same thing -- an array of shorts,
integers.

00:25:46.010 --> 00:25:49.390
But it's converting these to doubles,
doing its arithmetic using

00:25:49.390 --> 00:25:51.670
the floating-point unit,
and then converting it

00:25:51.690 --> 00:25:53.330
back to integers as output.

00:25:53.450 --> 00:25:55.250
Now, this is slow in a lot of ways.

00:25:55.300 --> 00:25:56.360
The conversion is slow.

00:25:56.360 --> 00:26:00.560
The floating-point is slower than
the equivalent integer arithmetic.

00:26:00.710 --> 00:26:03.150
And when we actually look at this,

00:26:03.880 --> 00:26:05.790
There's no actual floating
point values here.

00:26:05.800 --> 00:26:07.140
These are all integers, whole values.

00:26:07.140 --> 00:26:11.410
So we could get exactly the same
result using integer arithmetic.

00:26:11.590 --> 00:26:13.560
We figured we'd give that a
shot and see if it actually

00:26:13.560 --> 00:26:15.340
made a performance difference.

00:26:15.460 --> 00:26:17.830
And that gave us this version.

00:26:18.460 --> 00:26:20.740
Well, now you can see that that did
make a big performance difference.

00:26:20.740 --> 00:26:22.460
We finally got more
than 30 frames a second.

00:26:22.540 --> 00:26:24.380
In fact, we got 35 frames a second, 34.

00:26:24.380 --> 00:26:25.930
So that's really cool.

00:26:25.950 --> 00:26:28.380
Now our movie's playing back
in better than real time,

00:26:28.380 --> 00:26:30.440
and we can watch it
all nice and smoothly.

00:26:30.440 --> 00:26:32.250
Unfortunately, I cheated a bit.

00:26:32.280 --> 00:26:35.550
I thought we were going to have a
smaller screen on our projector,

00:26:35.550 --> 00:26:39.040
so I actually shrank the movie
down so it's not even a 720p.

00:26:39.040 --> 00:26:41.700
It turns out,
if we go and play it in 720p,

00:26:41.700 --> 00:26:44.440
it's still not running
quite smoothly enough.

00:26:44.440 --> 00:26:47.140
So let's go back and see what
other changes we can make.

00:26:47.780 --> 00:26:49.590
I said there were two things before.

00:26:49.620 --> 00:26:54.120
The second thing we noticed,
and Shark is even pointing it out for us,

00:26:54.130 --> 00:26:57.310
is that here we have some
loops doing some math.

00:26:57.310 --> 00:26:59.590
This is a classical sort of
case of where you want to

00:26:59.660 --> 00:27:01.290
look at vectorizing something.

00:27:01.290 --> 00:27:03.050
We had one of our engineers go in.

00:27:03.050 --> 00:27:04.430
He spent about a day on it.

00:27:04.430 --> 00:27:05.450
He looked into it.

00:27:05.450 --> 00:27:07.370
He figured out how to vectorize it.

00:27:08.780 --> 00:27:09.970
and we thought, well,
this is going to be great.

00:27:09.990 --> 00:27:11.500
You know, vectorization is awesome.

00:27:11.500 --> 00:27:14.540
Everyone tells you, you vectorize things,
it'll make it four times faster.

00:27:14.540 --> 00:27:16.320
It'll be fantastic.

00:27:16.390 --> 00:27:20.280
So we went in and we did that,
and that gave us this version.

00:27:21.060 --> 00:27:22.440
Huh.

00:27:22.480 --> 00:27:24.180
40 frames a second, I mean, it's better.

00:27:24.180 --> 00:27:24.860
It's faster.

00:27:24.860 --> 00:27:26.720
It's not four times faster.

00:27:26.720 --> 00:27:30.180
So that didn't really
work the way we expected.

00:27:30.180 --> 00:27:35.080
And this is the point where
our expectations fell apart.

00:27:35.090 --> 00:27:39.280
We made a change we thought was going
to have some really good benefit,

00:27:39.280 --> 00:27:40.350
and it didn't.

00:27:40.500 --> 00:27:42.730
So we need to go in and
reorientate ourselves,

00:27:42.730 --> 00:27:45.440
get our bearings again,
and try and figure out what's

00:27:45.700 --> 00:27:48.400
going on and why that change
didn't work as expected.

00:27:48.400 --> 00:27:50.280
So you take another time profile.

00:27:51.090 --> 00:27:52.730
So I'll play a movie.

00:27:52.860 --> 00:27:56.160
I'll jump over to Shark again,
and we'll take another time profile.

00:27:56.160 --> 00:27:59.510
Again, just a few seconds is plenty
for something like this.

00:27:59.510 --> 00:28:00.410
So we'll stop that.

00:28:00.490 --> 00:28:02.660
Shark will do its analysis again.

00:28:02.660 --> 00:28:04.540
It'll be a bit faster this
time because we've already got

00:28:04.540 --> 00:28:05.740
our symbol information handy.

00:28:05.740 --> 00:28:08.760
And when that pops up,
hopefully we'll be able to figure out why

00:28:08.770 --> 00:28:11.140
that optimization apparently didn't work.

00:28:12.990 --> 00:28:14.260
When it pops up, please.

00:28:14.260 --> 00:28:18.900
Any moment now.

00:28:18.900 --> 00:28:22.450
We do Shark Shark as well,
so it works on itself.

00:28:22.610 --> 00:28:23.160
There we go.

00:28:23.390 --> 00:28:23.660
All right.

00:28:23.840 --> 00:28:27.460
I'll just crank up the
volume -- font size again.

00:28:27.940 --> 00:28:31.130
So this isn't obvious because
we renamed the function,

00:28:31.130 --> 00:28:34.290
but what used to be reference IDCT,
our integer vectorized

00:28:34.290 --> 00:28:36.900
version is this iweight IDCT.

00:28:36.900 --> 00:28:41.900
So you can see that this is actually now
taking up 1.8% from 20-something before.

00:28:41.900 --> 00:28:43.900
So it's actually an order
of magnitude faster.

00:28:44.050 --> 00:28:46.900
So it turns out our
vectorization did work.

00:28:46.900 --> 00:28:50.690
It worked really, really well,
except it worked so well that now this

00:28:50.690 --> 00:28:55.900
function is not taking up any time,
so the benefit is comparatively tiny.

00:28:55.900 --> 00:28:59.450
What we need to do instead is
refocus on these functions up here,

00:28:59.460 --> 00:29:01.900
which are now taking up most of our time.

00:29:01.900 --> 00:29:04.520
Now, these first two in
particular are very similar,

00:29:04.580 --> 00:29:06.900
and if I just double-click
on one and go into it,

00:29:06.900 --> 00:29:09.900
you can see that,
like our previous function,

00:29:09.900 --> 00:29:10.900
this is just some math in a loop.

00:29:10.900 --> 00:29:14.890
And again, this is something that could
probably be vectorized really well.

00:29:14.910 --> 00:29:17.430
It turns out in this particular decoder,
there are a lot of different

00:29:17.430 --> 00:29:18.900
places where you can vectorize it.

00:29:18.900 --> 00:29:20.900
So our vectorization was working.

00:29:20.900 --> 00:29:23.800
We just didn't do enough of it.

00:29:24.000 --> 00:29:26.240
So we had an engineer, he sat down,
he spent another three

00:29:26.240 --> 00:29:26.940
or four days on it.

00:29:26.940 --> 00:29:29.840
He went through and he vectorized
functions one after another,

00:29:29.930 --> 00:29:32.400
you know, starting with the slowest ones.

00:29:32.500 --> 00:29:36.440
And long story short,
that gave us this version.

00:29:37.860 --> 00:29:39.200
Now that's more like it, right?

00:29:39.230 --> 00:29:41.140
We're getting over 100 frames a second.

00:29:41.160 --> 00:29:44.560
This is the kind of benefit that
everyone promises with vectorization.

00:29:44.640 --> 00:29:45.400
So that's really cool.

00:29:45.400 --> 00:29:46.550
We were really happy with that.

00:29:46.560 --> 00:29:49.560
We can now play our huge
1080p movies really smoothly.

00:29:49.570 --> 00:29:51.300
That was great.

00:29:51.640 --> 00:29:54.800
All right, so that's how we optimize
that particular program.

00:29:54.850 --> 00:29:58.090
What I'd like to do while I'm also
here is show you how you can do the

00:29:58.090 --> 00:29:59.740
same sort of thing on an iPhone.

00:29:59.780 --> 00:30:02.640
Now, we don't have time for a full demo,
but I just want to show you the important

00:30:02.640 --> 00:30:05.560
bits in order to profile on an iPhone.

00:30:05.640 --> 00:30:08.430
So first up,
in order to actually get to the phone,

00:30:08.460 --> 00:30:12.740
you go up to the sampling menu and
choose Network/iPhone Profiling.

00:30:12.780 --> 00:30:16.070
This adds to Shark's main window,
this little list here you can see,

00:30:16.070 --> 00:30:19.540
which will show up any connected
iPhones or iPod Touches,

00:30:19.560 --> 00:30:23.920
and also any Macs you have on
the network in remote mode.

00:30:24.290 --> 00:30:26.980
So to use the phone for profiling,
you click on Use.

00:30:27.000 --> 00:30:30.160
This will go to the phone, launch Shark,
get it ready for profiling,

00:30:30.210 --> 00:30:32.590
get it configured,
and then you can choose which

00:30:32.590 --> 00:30:35.440
configuration you want to use,
which profiling technique,

00:30:35.460 --> 00:30:38.630
and also the scope of the profiling,
much like these pop-up menus from before.

00:30:38.850 --> 00:30:42.880
And you can control profiling on the
phone using the Start and Stop button.

00:30:42.890 --> 00:30:44.980
Now, I mentioned we didn't have
time to go through this,

00:30:45.000 --> 00:30:47.880
so I just had a session
that I prepared earlier,

00:30:47.910 --> 00:30:49.140
so we can just open it straight away.

00:30:49.260 --> 00:30:53.540
I took a profile of a particular
application I was working on,

00:30:53.580 --> 00:30:55.310
which is just drawing a sine wave.

00:30:55.420 --> 00:30:57.120
So I can actually show it to you here.

00:30:57.120 --> 00:30:59.820
I'll launch it in the simulator.

00:31:00.170 --> 00:31:00.890
It's a really simple app.

00:31:00.960 --> 00:31:04.740
I was just interested in getting an
idea on OpenGL performance on the phone.

00:31:04.740 --> 00:31:06.530
So we'll build that.

00:31:11.510 --> 00:31:14.660
When it runs, there it is.

00:31:14.960 --> 00:31:17.200
So very, very simple.

00:31:17.220 --> 00:31:19.540
But on the phone,
it was running a little slower

00:31:19.540 --> 00:31:20.500
than I thought it should be.

00:31:20.560 --> 00:31:23.730
So I took a time profile
to see why it was,

00:31:23.730 --> 00:31:25.260
you know, what it was doing,
why it was so slow.

00:31:25.260 --> 00:31:26.460
And that gave me this.

00:31:26.460 --> 00:31:28.840
Now, you can see this is okay.

00:31:28.840 --> 00:31:32.760
We've got some symbols
for our system libraries.

00:31:32.760 --> 00:31:35.450
But for our application, we've got this.

00:31:35.450 --> 00:31:38.270
No symbols, just instruction addresses.

00:31:38.280 --> 00:31:39.640
That's not very helpful.

00:31:40.220 --> 00:31:43.300
This is because when Xcode installs
your application on the phone,

00:31:43.300 --> 00:31:45.280
it removes all the symbol information.

00:31:45.280 --> 00:31:48.740
It's trying to save space because
the phone doesn't have much space.

00:31:48.740 --> 00:31:50.520
That's not really convenient for us.

00:31:50.640 --> 00:31:53.320
But luckily,
you can always add it in afterwards.

00:31:53.320 --> 00:31:55.860
So you go up to the file
menu and choose symbolicate.

00:31:55.860 --> 00:31:59.240
That brings up a dialog where you can
select the original binary on your Mac,

00:31:59.240 --> 00:32:01.160
the one that still has
symbols info in it.

00:32:01.160 --> 00:32:02.270
And you just open that.

00:32:02.270 --> 00:32:04.630
And Shark can pull the symbol
information out of that.

00:32:04.630 --> 00:32:07.220
And you can see now that we
can see what's actually going

00:32:07.220 --> 00:32:08.870
on inside our application.

00:32:08.950 --> 00:32:11.780
what's actually going
on inside our program.

00:32:12.100 --> 00:34:37.700
[Transcript missing]

00:34:38.390 --> 00:34:41.260
And I'll just search for it here.

00:34:41.370 --> 00:34:42.990
Compile for Thumb.

00:34:43.070 --> 00:34:46.980
So this is telling Xcode that it's
allowed to generate thumb code.

00:34:47.070 --> 00:34:49.060
By turning this off,
and I rebuilt it and put

00:34:49.100 --> 00:34:51.650
it back on the phone,
I went from about 30 frames a

00:34:51.650 --> 00:34:53.780
second to 40 frames per second.

00:34:53.800 --> 00:34:55.450
I was pretty happy with that.

00:34:55.620 --> 00:34:57.780
33% for unticking a checkbox.

00:34:57.780 --> 00:34:59.510
I like these kind of optimizations.

00:34:59.520 --> 00:35:02.810
Now,

00:35:03.090 --> 00:35:04.420
There's a lot more to it than that.

00:35:04.530 --> 00:35:07.920
There's a lot more optimizations
that I made to this program.

00:35:08.000 --> 00:35:09.960
Unfortunately, I mentioned we don't have
time to go into them now.

00:35:09.960 --> 00:35:11.140
This is a demo for another time.

00:35:11.290 --> 00:35:16.370
So let's just jump back to our slides
and wrap up what we've done so far.

00:35:17.640 --> 00:35:21.270
So in the first half of that demo,
I showed you how we used Time Profile

00:35:21.450 --> 00:35:22.900
to optimize our MPEG-2 decoder.

00:35:22.900 --> 00:35:25.720
And just by following
that iterative process,

00:35:25.740 --> 00:35:28.690
focusing in on one function at a
time and optimizing them in turn,

00:35:28.700 --> 00:35:32.580
we ended up running, what was it,
nearly five times faster.

00:35:32.580 --> 00:35:34.400
So that was pretty cool.

00:35:34.400 --> 00:35:35.480
We were very happy with that.

00:35:35.520 --> 00:35:39.570
In the second half of the demo,
you saw how I used Shark to

00:35:39.740 --> 00:35:41.740
Time Profile on an iPhone.

00:35:41.740 --> 00:35:45.200
Now, these are the takeaway
points from that demo.

00:35:45.600 --> 00:35:47.580
Some of them I explained in the demo.

00:35:47.580 --> 00:35:50.500
You know, you need to add search paths so
Shark can see your symbol information.

00:35:50.500 --> 00:35:53.240
There's also a couple I just
want to talk to briefly.

00:35:53.240 --> 00:35:55.340
The second point is kind of important.

00:35:55.340 --> 00:35:58.980
On a Mac, Shark's overhead is tiny,
like I mentioned.

00:35:58.980 --> 00:36:01.780
But on a phone,
it's a much less powerful system.

00:36:01.780 --> 00:36:04.140
But Shark still needs to
do the same amount of work,

00:36:04.210 --> 00:36:07.320
so its overhead is significantly higher.

00:36:07.320 --> 00:36:10.810
This is just important
to remember in terms of,

00:36:10.810 --> 00:36:13.520
you know,
try to profile just your application.

00:36:13.700 --> 00:36:15.640
Don't profile the whole
system if you can avoid it.

00:36:15.700 --> 00:36:18.660
Don't crank up the sample
rate to crazy levels,

00:36:18.660 --> 00:36:19.950
things like that.

00:36:20.190 --> 00:36:22.160
The third point is also important.

00:36:22.160 --> 00:36:25.070
I mentioned before that you should always
be profiling your release build so you're

00:36:25.070 --> 00:36:26.590
seeing what the end user is seeing.

00:36:26.600 --> 00:36:30.750
On the phone with the ARM processor,
this is particularly important because

00:36:30.750 --> 00:36:36.240
GCC's code generation between unoptimized
and optimized varies very widely,

00:36:36.300 --> 00:36:37.240
to put it politely.

00:36:37.240 --> 00:36:39.820
So you really want to be
using your optimized version.

00:36:39.840 --> 00:36:41.460
Otherwise,
you're probably not going to be seeing

00:36:41.460 --> 00:36:42.920
what's actually going to be happening.

00:36:43.740 --> 00:36:46.410
The fourth point,
I mentioned the ARM versus Thumb thing.

00:36:46.490 --> 00:36:50.610
I just unchecked that box,
which changed my entire program

00:36:50.610 --> 00:36:52.050
from using Thumb code to using ARM.

00:36:52.100 --> 00:36:57.770
You can also go in using these compiler
flags and change this on a per file basis

00:36:57.840 --> 00:36:59.700
using the info inspector for a file.

00:36:59.800 --> 00:37:04.390
So that's a great way to selectively
choose which instruction set you use

00:37:04.450 --> 00:37:05.510
for different parts of your program.

00:37:05.600 --> 00:37:11.280
The last point is simply, as you saw,
the phone is kind of a new device.

00:37:11.300 --> 00:37:12.260
It's a new architecture.

00:37:12.260 --> 00:37:13.560
It's a new type of system.

00:37:13.650 --> 00:37:16.750
The performance problems you have on
it are going to be quite different to

00:37:16.750 --> 00:37:18.410
what you may be familiar with on a Mac.

00:37:18.600 --> 00:37:21.690
And they're probably going to
require some original solutions.

00:37:21.700 --> 00:37:25.450
So this is just an acknowledgment of
the fact that even if you're familiar

00:37:25.450 --> 00:37:29.820
with performance work on a Mac,
this might be like going back to school

00:37:29.820 --> 00:37:32.730
a little bit once you move to the iPhone.

00:37:33.010 --> 00:37:35.500
Now, the other thing we saw in the demo,
just as a last point,

00:37:35.500 --> 00:37:37.580
is you saw me open a saved session file.

00:37:37.580 --> 00:37:41.420
So, yes, Shark sessions can be saved,
and they can be copied

00:37:41.420 --> 00:37:45.010
between any machine,
Intel to PowerPC to ARM, whatever.

00:37:45.020 --> 00:37:46.640
There's no end in this
issues or anything like that.

00:37:46.710 --> 00:37:47.330
They just work.

00:37:47.390 --> 00:37:50.800
Now, they include, of course,
the sample information you took,

00:37:50.800 --> 00:37:51.520
obviously.

00:37:51.520 --> 00:37:54.400
They also include any data
mining settings you had active

00:37:54.400 --> 00:37:55.900
when you saved the session.

00:37:55.900 --> 00:37:56.650
All right.

00:37:56.870 --> 00:38:00.620
But they can also include your symbol
information and your source code.

00:38:01.640 --> 00:38:04.860
Now, this is really cool because it means
the session file is self-contained.

00:38:04.860 --> 00:38:07.330
You can delete your source code,
rebuild your application,

00:38:07.410 --> 00:38:09.260
move to another machine, whatever.

00:38:09.260 --> 00:38:11.480
When you open the session,
it's still going to look exactly

00:38:11.480 --> 00:38:12.590
the same as when you took it.

00:38:12.590 --> 00:38:14.240
It'll still have all the information.

00:38:14.240 --> 00:38:16.450
Of course,
if you're going to send the session

00:38:16.450 --> 00:38:19.580
file to someone and you don't want
them to have this information,

00:38:19.580 --> 00:38:21.690
you need to not include it to begin with.

00:38:21.720 --> 00:38:23.580
Luckily, this is easy to remember.

00:38:23.580 --> 00:38:26.480
When you go to save a session,
Shark will pop down a sheet,

00:38:26.520 --> 00:38:29.520
which will give you the choice
of whether you want to include

00:38:29.520 --> 00:38:31.010
that information or not.

00:38:31.200 --> 00:38:32.690
So if you're going to
send this to someone,

00:38:32.690 --> 00:38:35.050
you don't want them having the info,
remember to click strip.

00:38:35.140 --> 00:38:39.030
As we saw also in the iPhone demo,
even if you get a session file

00:38:39.030 --> 00:38:44.020
that doesn't have that information,
you can always add it in again later.

00:38:44.020 --> 00:38:46.540
So that's not really an issue.

00:38:47.960 --> 00:38:52.540
Now, obviously you can use Shark session
files just so you can save them

00:38:52.540 --> 00:38:54.980
and you can come back later
and continue working on them.

00:38:55.120 --> 00:38:57.130
But there's another
two good uses of this.

00:38:57.140 --> 00:39:01.610
The first up is that you can take
these as an end user or as a developer

00:39:01.610 --> 00:39:05.210
if you're sending them to someone
else and send them in alongside your

00:39:05.210 --> 00:39:10.040
performance problem reports so that
even if the person who gets your bug

00:39:10.080 --> 00:39:14.720
report can't reproduce the problem,
it might be an intermittent issue or

00:39:14.720 --> 00:39:18.720
requires some special configuration,
they'll still be able

00:39:18.720 --> 00:39:20.860
to open the session,
see what the program is doing

00:39:20.860 --> 00:39:23.170
when the problem does occur,
and that may be enough for

00:39:23.260 --> 00:39:26.810
them to figure out what's
going on and how to improve it.

00:39:27.050 --> 00:39:28.850
In fact,
that's what we at Apple recommend if

00:39:28.850 --> 00:39:30.570
you're sending performance bugs into us.

00:39:30.810 --> 00:39:33.910
Grab a Shark session if you can,
because that helps us immensely when

00:39:33.910 --> 00:39:35.060
trying to figure out what's going on.

00:39:35.080 --> 00:39:39.310
And the last point is going
back to that whole methodical

00:39:39.310 --> 00:39:40.580
approach to performance analysis.

00:39:40.580 --> 00:39:44.090
Use your Shark session files
to document your progress.

00:39:44.180 --> 00:39:47.790
Every time you make a change,
take a new time profile or whatever

00:39:47.790 --> 00:39:50.320
you're using and record that.

00:39:50.320 --> 00:39:52.700
So you can go back and
see each step of the way,

00:39:52.700 --> 00:39:56.470
okay, we made this change,
this is what the performance change was,

00:39:56.760 --> 00:39:58.990
and so on and so forth.

00:39:59.370 --> 00:40:00.620
All right.

00:40:00.690 --> 00:40:02.740
That's time profile,
and that's how to use Shark.

00:40:02.800 --> 00:40:06.170
But that's, while very useful,
certainly not the only

00:40:06.210 --> 00:40:07.440
thing that Shark can do.

00:40:07.440 --> 00:40:12.140
I mentioned one other thing before,
system trace, that is also very useful.

00:40:12.140 --> 00:40:14.270
And to talk about that,
I'd like to welcome

00:40:14.320 --> 00:40:16.300
on stage my colleague,
Ryan Du Bois.

00:40:23.530 --> 00:40:24.520
Thank you, Wade.

00:40:24.640 --> 00:40:28.340
As he said, my name is Ryan Du Bois,
and I'm here to talk about system trace.

00:40:28.560 --> 00:40:31.080
So by now,
you've gained a general familiarity

00:40:31.080 --> 00:40:34.460
with time profile and how it can be
useful in tracking down your CPU cycles.

00:40:34.560 --> 00:40:36.070
Where are they going?

00:40:36.110 --> 00:40:37.920
Oftentimes, however,
that's not the complete

00:40:38.000 --> 00:40:39.050
picture you're looking for.

00:40:39.070 --> 00:40:40.530
That tells you one part of the piece.

00:40:40.600 --> 00:40:43.400
But what if, for example,
you just added multithreading

00:40:43.410 --> 00:40:44.420
to your application?

00:40:44.600 --> 00:40:48.580
What kind of tools do we have
that allow you to look into that?

00:40:48.800 --> 00:40:50.210
That's where System Trace comes in.

00:40:50.580 --> 00:40:56.010
System Trace allows you to get a new
understanding of the multi-threaded

00:40:56.010 --> 00:40:57.380
behavior of your application.

00:40:57.380 --> 00:41:01.160
It gives you visibility into how
your application interacts with OS X.

00:41:01.160 --> 00:41:05.680
It even lets you view things like virtual
memory behavior in a very visual way,

00:41:05.680 --> 00:41:07.180
so you can begin to understand that.

00:41:07.180 --> 00:41:10.830
So let's take a quick look at how
it's different from Time Profile.

00:41:10.840 --> 00:41:14.400
And a Time Profile, as Wade said,
will come in periodically

00:41:14.460 --> 00:41:16.510
and ask your program,
hey, what are you doing?

00:41:17.560 --> 00:41:19.870
Now in this example here,
you see that that red chunk

00:41:19.870 --> 00:41:22.340
of system code is completely
missed by Time Profile.

00:41:22.380 --> 00:41:25.350
That's that sampling error
that Wade was talking about.

00:41:25.360 --> 00:41:29.530
System Trace, on the other hand,
is an exact trace of every single

00:41:29.530 --> 00:41:32.970
operating system event that happens
during the course of profiling.

00:41:32.980 --> 00:41:34.520
So we don't miss a thing.

00:41:34.540 --> 00:41:38.970
And this happens for every single thread
running in the system all at once.

00:41:38.980 --> 00:41:42.280
The really beautiful part
about this is you don't need to

00:41:42.280 --> 00:41:44.330
alter your programs in any way.

00:41:44.340 --> 00:41:46.240
You don't have to change
any instrumentation,

00:41:46.240 --> 00:41:47.220
do any of that stuff.

00:41:47.380 --> 00:41:49.240
It just works.

00:41:49.780 --> 00:41:53.210
So when you take a system trace,
it'll be presented with three main views.

00:41:53.470 --> 00:41:56.160
The first one here is the summary view.

00:41:56.740 --> 00:42:00.580
We also have the Trace
View and the Timeline View.

00:42:00.710 --> 00:42:04.000
So let's talk about these
a little more in depth.

00:42:04.040 --> 00:42:06.780
The Summary View, as the name implies,
is just kind of an overall

00:42:06.880 --> 00:42:10.230
summary of what was going on
during that profiling session.

00:42:10.290 --> 00:42:12.180
In the upper right here,
you see kind of a general

00:42:12.180 --> 00:42:15.580
breakdown of your CPU time,
how much was spent in user code,

00:42:15.580 --> 00:42:18.660
how much was spent in system code,
how much was doing actual work,

00:42:18.710 --> 00:42:20.550
and how much was idle.

00:42:20.640 --> 00:42:22.700
In the upper left here,
we break it down even further

00:42:22.700 --> 00:42:27.490
with a nifty little pie chart,
breaking it into system calls, VM faults,

00:42:27.750 --> 00:42:30.870
driver time interrupts, and idle time.

00:42:31.310 --> 00:42:33.950
Down below, you'll see three tabs.

00:42:33.960 --> 00:42:37.230
These are the more detailed
views of the summary.

00:42:37.270 --> 00:42:40.440
The first one here is the scheduler tab.

00:42:40.480 --> 00:42:44.260
This gives you kind of summary statistics
for all the thread scheduling decisions

00:42:44.260 --> 00:42:47.400
made during the course of profiling.

00:42:47.450 --> 00:42:51.930
So as you see here, we can organize these
statistics by busy time,

00:42:52.070 --> 00:42:54.940
user time, system time,
and even dynamic priority of

00:42:54.940 --> 00:42:57.310
your threads as they're running.

00:42:57.840 --> 00:43:00.940
The next tab over is
the system call profile.

00:43:01.010 --> 00:43:04.090
This looks very much like a timed
profile with the exception that it's

00:43:04.090 --> 00:43:09.080
a profile of every single system call
made during the course of profiling.

00:43:09.340 --> 00:43:12.040
Now, just like a time profile,
you can switch between heavy and

00:43:12.050 --> 00:43:13.680
tree view using a little pop-up here.

00:43:13.680 --> 00:43:16.980
And the real power here is using
these disclosure triangles,

00:43:16.980 --> 00:43:19.920
you can walk back from each and
every system call back to the

00:43:19.920 --> 00:43:22.880
line of code in your application
where that system call came from.

00:43:22.880 --> 00:43:25.270
So, for example,
you make a couple OpenGL calls,

00:43:25.270 --> 00:43:28.220
you're not sure if they're going
to result in a system call.

00:43:28.220 --> 00:43:31.760
System trace now gives you that
visibility into your application.

00:43:33.680 --> 00:43:35.660
And the third tab is the VM fault tab.

00:43:35.660 --> 00:43:37.540
This is just like the system call tab.

00:43:37.540 --> 00:43:41.000
It's an exact profile of
every VM fault that happened.

00:43:41.060 --> 00:43:43.160
And again,
using the little disclosure triangles,

00:43:43.160 --> 00:43:46.640
you can walk back into your
function and your line of code,

00:43:46.640 --> 00:43:49.210
which causes VM faults.

00:43:50.060 --> 00:43:53.000
The next main view I want to
talk about is the trace view.

00:43:53.030 --> 00:43:55.320
As the name implies,
this is a complete list of all

00:43:55.350 --> 00:43:58.520
the traced events that happen
during the course of profiling.

00:43:58.520 --> 00:44:01.140
Now, if you're looking at system calls,
for example,

00:44:01.140 --> 00:44:04.420
we give you all the arguments to
every system call made right here.

00:44:04.420 --> 00:44:08.050
You poke this little button there,
and the little pane will slide open,

00:44:08.200 --> 00:44:11.380
and you can see the full call stack
for each and every system call as

00:44:11.380 --> 00:44:13.440
you're walking through the trace.

00:44:13.440 --> 00:44:16.150
Now, that's really cool,
and that gives you a lot of detail,

00:44:16.160 --> 00:44:18.460
but it doesn't really
give you a visual picture.

00:44:19.400 --> 00:44:22.140
So for that, if you double-click on any
one of these system calls,

00:44:22.140 --> 00:44:24.120
it'll take you right to
that in the timeline view,

00:44:24.120 --> 00:44:25.210
which looks like this.

00:44:25.280 --> 00:44:29.480
As the name implies,
it's a graph of all your threads

00:44:29.480 --> 00:44:32.430
over time and what they're doing.

00:44:33.820 --> 00:44:37.630
Now you can navigate this view using
the interactive zoomer up here,

00:44:37.730 --> 00:44:41.960
which will zoom in on the timeline view,
and the scroll bar below.

00:44:41.980 --> 00:44:44.880
And as I said before,
we record trace events for every

00:44:44.890 --> 00:44:46.640
single thread in the system at once.

00:44:46.640 --> 00:44:49.810
So down at the bottom here,
we give you some options for filtering

00:44:49.810 --> 00:44:52.100
that down to just your process.

00:44:53.000 --> 00:44:56.020
So I'd like to take a brief interlude
here and talk about some advanced

00:44:56.020 --> 00:44:58.300
settings when using System Trace.

00:44:58.310 --> 00:45:01.740
I find it very helpful when
doing a System Trace to pop open

00:45:01.740 --> 00:45:03.400
the Advanced Settings drawer.

00:45:03.430 --> 00:45:07.570
That gives you a couple extra
options for your visualizations.

00:45:07.690 --> 00:45:10.010
First one I'd like to talk
about is the thread coloring

00:45:10.030 --> 00:45:12.500
options up at the top here.

00:45:12.500 --> 00:45:15.500
Before I move on, however,
you can get to this by going to Window,

00:45:15.500 --> 00:45:18.810
Show Advanced Settings, just so you know.

00:45:19.000 --> 00:45:20.940
So here's that timeline view.

00:45:20.940 --> 00:45:24.760
Each one of these little colored
rectangles represents the time that

00:45:24.850 --> 00:45:27.580
any given thread was running on a CPU.

00:45:27.580 --> 00:45:30.680
By default, as you can see here,
they're all colored the same yellow.

00:45:30.680 --> 00:45:34.000
And that doesn't really tell
us what was running where.

00:45:34.000 --> 00:45:38.240
We have three different ways to add
colors to these thread run intervals to

00:45:38.240 --> 00:45:41.530
make them more visually understandable.

00:45:41.580 --> 00:45:45.160
The first one I'd like to
talk about is color by CPU.

00:45:45.200 --> 00:45:47.460
Now this is the one that
I find myself using the most.

00:45:47.460 --> 00:45:50.780
So it's the default
setting on my machine.

00:45:50.800 --> 00:45:54.340
Let's see how this can help
us visualize multi-threaded

00:45:54.350 --> 00:45:55.940
behavior of our application.

00:45:56.020 --> 00:45:59.240
So if we focus in on these
four Noble Ape threads here,

00:45:59.280 --> 00:46:02.110
these are worker threads for an
application that's supposed to be

00:46:02.110 --> 00:46:04.360
doing some CPU intensive calculations.

00:46:04.600 --> 00:46:07.310
And we went and added multi-threading
and wanted to make sure that

00:46:07.310 --> 00:46:10.310
they were sharing all four cores,
just kind of do a sanity check.

00:46:10.370 --> 00:46:12.780
And that's what this system trace is.

00:46:13.090 --> 00:46:15.760
Now, if we look right here,
using these coloring options,

00:46:15.760 --> 00:46:19.620
we can see at a glance that all four
of these worker threads are sharing all

00:46:19.620 --> 00:46:21.930
four cores in this machine pretty well.

00:46:21.960 --> 00:46:23.360
That's good news.

00:46:23.360 --> 00:46:24.180
That's what we expected.

00:46:24.180 --> 00:46:26.320
However,
if we look a little bit to the right,

00:46:26.380 --> 00:46:29.360
we notice the color green
disappears from those threads.

00:46:29.360 --> 00:46:31.250
And furthermore,
we notice two of them are

00:46:31.250 --> 00:46:32.620
fighting for the same core.

00:46:32.640 --> 00:46:35.200
And that's maybe going to
mess up some cache locality,

00:46:35.200 --> 00:46:37.030
maybe cause a big performance hit.

00:46:37.040 --> 00:46:40.470
And just by using that color down here,
we can identify which thread

00:46:40.510 --> 00:46:42.210
came in and started running.

00:46:43.800 --> 00:46:47.160
The next option I'd like to
talk about is color by priority.

00:46:47.280 --> 00:46:49.700
In case you weren't aware,
the priority of each thread in your

00:46:49.700 --> 00:46:53.810
system gets adjusted dynamically over
time according to what it's doing.

00:46:53.970 --> 00:46:56.080
Using the timeline view
and color by priority,

00:46:56.080 --> 00:47:00.090
you can see how those adjustments were
made for each one of your threads.

00:47:00.190 --> 00:47:03.300
So for example, here, these bottom three
threads are high priority,

00:47:03.300 --> 00:47:05.900
and that's denoted by kind
of a pinkish-red color.

00:47:06.010 --> 00:47:09.040
And just above it,
you can see three hourglass threads

00:47:09.040 --> 00:47:11.730
here running at low priority,
blue color.

00:47:12.440 --> 00:47:15.840
The final option I'd like to talk about
is color by context switch reason.

00:47:15.840 --> 00:47:19.480
So every thread, when it stops running,
stops running for one

00:47:19.480 --> 00:47:20.730
of these five reasons.

00:47:20.760 --> 00:47:23.640
And just at a glance,
you can kind of get a handle on,

00:47:23.640 --> 00:47:25.840
are these threads making a
lot of blocking system calls?

00:47:25.840 --> 00:47:27.300
Are they getting preempted?

00:47:27.300 --> 00:47:28.480
Are they just running out of time?

00:47:28.520 --> 00:47:31.040
Just at a glance with the timeline view.

00:47:33.650 --> 00:47:37.340
The next set of options I want to talk
about is the event selection here.

00:47:37.350 --> 00:47:40.820
So here we have a list of the icons
that will show in the timeline view,

00:47:40.820 --> 00:47:42.750
and you can turn each
one of these on and off.

00:47:44.400 --> 00:47:46.460
So let's go back to that
timeline view and let's see

00:47:46.460 --> 00:47:48.270
what else we can get out of it.

00:47:48.420 --> 00:47:50.130
Remember each one of those run intervals.

00:47:50.220 --> 00:47:53.390
If you click on one of those,
you'll get a little inspector that'll

00:47:53.390 --> 00:47:55.370
give you a lot more information about it.

00:47:55.530 --> 00:47:57.060
How much time was spent in user mode?

00:47:57.060 --> 00:47:58.680
How much in system mode?

00:47:58.920 --> 00:48:00.010
Why did it stop running?

00:48:00.020 --> 00:48:03.030
What was its priority
when it stopped running?

00:48:03.150 --> 00:48:05.720
Now,
if you turn on some of those event icons,

00:48:05.780 --> 00:48:08.000
they will show up in these
run intervals telling you

00:48:08.000 --> 00:48:10.150
what those threads were doing.

00:48:10.280 --> 00:48:12.400
Here we see a normal system call.

00:48:12.470 --> 00:48:16.800
The little underbar represents how
long that it was running on the CPU.

00:48:17.080 --> 00:48:19.470
Just above you can see
a blocking system call,

00:48:19.470 --> 00:48:22.210
and I think you'll notice the underbar
extends beyond the end of one run

00:48:22.210 --> 00:48:24.800
interval on into the next one.

00:48:24.920 --> 00:48:26.440
Now,
if you were to click on any one of these,

00:48:26.440 --> 00:48:27.480
you'll get another inspector.

00:48:27.480 --> 00:48:29.080
It looks slightly different.

00:48:29.100 --> 00:48:32.980
This gives you the full call stack
of where that system call came from.

00:48:32.980 --> 00:48:34.760
Down below there,
you can see all the arguments

00:48:34.910 --> 00:48:36.380
to it and its return value even.

00:48:36.410 --> 00:48:39.010
So if you have an error case,
you can click on it in the timeline

00:48:39.010 --> 00:48:40.460
view and see the return value.

00:48:40.460 --> 00:48:44.620
Also, in the bottom left there,
you can see how much time was spent

00:48:44.620 --> 00:48:49.340
blocked waiting on whatever resource
that system call was called for.

00:48:49.910 --> 00:48:53.160
Now we have three main types of
system calls we display icons for.

00:48:53.160 --> 00:48:56.480
You can see them here, BSD, Mach,
and MIG messages.

00:48:56.480 --> 00:48:59.230
One thing I'd like to
call out is the lock icon.

00:48:59.420 --> 00:49:02.990
Now this happens any time your
Pthread mutex operations result

00:49:03.080 --> 00:49:04.500
in a system call to the kernel.

00:49:04.700 --> 00:49:08.400
So that's not going to be every
single Pthread mutex lock or unlock,

00:49:08.460 --> 00:49:11.890
but just the ones that happen
as a result of lock contention.

00:49:11.980 --> 00:49:14.140
So if you see that show
up in your timeline view,

00:49:14.180 --> 00:49:18.340
you know you've got some lock contention
going on that you need to track down.

00:49:20.200 --> 00:49:22.300
So again, here's those page faults.

00:49:22.300 --> 00:49:24.520
Here you see a couple of zero fills.

00:49:24.590 --> 00:49:25.800
Again, more information there.

00:49:25.800 --> 00:49:27.560
If you click on them,
you'll get a little inspector

00:49:27.560 --> 00:49:30.950
that'll show you the full call stack,
in this case very short.

00:49:31.090 --> 00:49:33.370
It'll show you the address
and the size of the page.

00:49:33.550 --> 00:49:36.660
And it'll even tell you if it was
a code fault by telling you which

00:49:36.670 --> 00:49:38.970
library was faulting a page in there.

00:49:40.110 --> 00:49:42.160
As you can see,
we display icons for a number

00:49:42.160 --> 00:49:43.890
of different VM fault types.

00:49:44.020 --> 00:49:48.890
My personal favorite is copy on right,
little cow skin icon.

00:49:50.530 --> 00:49:53.360
We also show icons for interrupts
that happen on the processors.

00:49:53.360 --> 00:49:54.980
These happen for a number of reasons.

00:49:54.980 --> 00:49:58.890
I/O operations, DMAs,
the timer interrupts used

00:49:58.900 --> 00:50:00.660
to enforce a time quantum.

00:50:00.700 --> 00:50:02.410
As I said before,
you don't need to add any

00:50:02.410 --> 00:50:05.790
instrumentation to your application,
but you can if you're interested.

00:50:05.990 --> 00:50:07.460
We have two ways to do that.

00:50:07.520 --> 00:50:08.920
They're both signposts.

00:50:09.090 --> 00:50:11.700
The first one is called a point signpost.

00:50:11.740 --> 00:50:14.220
You can think of this as
kind of like a print F.

00:50:14.360 --> 00:50:15.950
Your program kind of
raises its hand and says,

00:50:16.010 --> 00:50:20.200
"Hey, this is what I'm doing right now."
The other kind is an interval signpost,

00:50:20.200 --> 00:50:23.250
which again shows up with that
little underbar that tells you

00:50:23.260 --> 00:50:25.030
how long any given operation took.

00:50:25.230 --> 00:50:28.540
And with each one of these,
you can record four auxiliary values,

00:50:28.540 --> 00:50:31.320
whatever you like, and you can put these
in your code anywhere,

00:50:31.430 --> 00:50:34.310
in your user application,
in your driver even.

00:50:34.450 --> 00:50:36.920
To see how to do that,
check out the Shark manual.

00:50:36.920 --> 00:50:38.690
There's a couple of really
good examples in there.

00:50:38.700 --> 00:50:41.540
Or after the conference,
check out the video of the advanced

00:50:41.540 --> 00:50:44.190
session that happened on Tuesday.

00:50:45.280 --> 00:50:49.520
So with that, let's go into a little
quick demo of System Trace.

00:50:49.620 --> 00:50:53.850
See how we can use it to
visualize the multi-threaded

00:50:53.910 --> 00:50:56.270
behavior of an application.

00:50:57.080 --> 00:50:58.160
So here's Shark window here.

00:50:58.160 --> 00:51:00.780
You can find system trace
about the fourth position over

00:51:00.780 --> 00:51:04.380
here in the config pop-up.

00:51:04.400 --> 00:51:08.390
Let me go ahead and get
my demo set up here.

00:51:10.650 --> 00:51:14.090
All right, so I have this video camera
that unfortunately records

00:51:14.160 --> 00:51:15.480
direct-to-DVD format.

00:51:15.480 --> 00:51:18.360
It's kind of a pain for me because I want
to watch these on my home computer,

00:51:18.360 --> 00:51:19.840
maybe even take them on my iPod.

00:51:19.880 --> 00:51:22.490
So I'm using an application
here some of you may be

00:51:22.590 --> 00:51:24.020
familiar with called Handbrake.

00:51:24.120 --> 00:51:28.040
And what it does is it transcodes
video from one format to another.

00:51:28.040 --> 00:51:33.720
In this case, I'm going to be looking
at the X264 encoder,

00:51:33.720 --> 00:51:36.780
which if you go read
documentation online,

00:51:36.780 --> 00:51:37.980
says that it's multi-threaded.

00:51:38.020 --> 00:51:40.290
So on these eight-core machines,
it should go pretty fast.

00:51:40.600 --> 00:51:44.380
So what I'm going to do is just start
to encode this video that I've taken.

00:51:46.240 --> 00:51:48.840
And again,
using that whole performance workflow,

00:51:48.840 --> 00:51:51.050
I'm going to write down the metric,
which is going to be the frames

00:51:51.050 --> 00:51:52.280
per second right over here.

00:51:52.280 --> 00:51:55.640
And it looks like it reaches
steady state right around 126,

00:51:55.640 --> 00:51:56.400
127.

00:51:56.400 --> 00:51:59.760
Now I'm going to go ahead and take a
system trace using the hotkey here,

00:51:59.760 --> 00:52:00.760
Option Escape.

00:52:02.310 --> 00:52:04.190
And that starts,
and you'll notice that starts,

00:52:04.260 --> 00:52:07.310
the icon down there changes to red.

00:52:08.140 --> 00:52:11.940
I'm going to go ahead and pause
this while Shark is processing.

00:52:12.000 --> 00:52:14.170
In fact, I'll even quit it.

00:52:16.610 --> 00:52:18.840
Shark gives you a little progress bar,
tell you how far along

00:52:18.840 --> 00:52:20.020
it is in its processing.

00:52:20.020 --> 00:52:22.070
All right, so here's our system trace.

00:52:22.080 --> 00:52:25.510
This is the summary tab
I was talking about.

00:52:25.520 --> 00:52:28.890
And immediately,
something should jump out at you here.

00:52:28.910 --> 00:52:32.620
You see that this is supposed to
be a CPU-intensive application.

00:52:32.630 --> 00:52:35.460
It's supposed to be using as many
cycles as possible to transcode this

00:52:35.560 --> 00:52:36.860
video from one format to another.

00:52:36.860 --> 00:52:42.170
Problem I see here is
we have 45% idle time.

00:52:42.180 --> 00:52:44.790
That's a lot of time sitting
around doing nothing when we

00:52:44.790 --> 00:52:46.440
could be working on this video.

00:52:46.500 --> 00:52:50.680
So let's go see if we can kind of
get a handle on what's going on.

00:52:52.070 --> 00:52:54.110
If we click that little
disclosure triangle,

00:52:54.120 --> 00:52:56.540
it'll show us all the
threads in this application.

00:52:56.540 --> 00:52:58.400
You notice there's quite a few.

00:52:58.400 --> 00:53:00.920
Nothing too exciting there.

00:53:00.920 --> 00:53:03.900
Let's go over and check
out the system calls.

00:53:04.430 --> 00:53:06.300
All right, the first one here,
these are kind of interesting.

00:53:06.300 --> 00:53:09.220
They don't really have a name,
but if you open up their call stack,

00:53:09.230 --> 00:53:12.260
you see that they're
coming from Thread Start.

00:53:12.260 --> 00:53:14.760
Another way to see that is
by clicking this button here.

00:53:14.760 --> 00:53:17.290
It'll pull out the full call stack.

00:53:17.610 --> 00:53:23.000
So what this is telling me here
is our x264 encoder is kicking off

00:53:23.150 --> 00:53:26.680
threads and maybe joining them later,
and it's doing this really fast,

00:53:26.680 --> 00:53:29.050
probably once for each frame.

00:53:29.060 --> 00:53:32.920
So the problem here on OS X,
it takes about 90 microseconds

00:53:32.920 --> 00:53:34.840
of time to make a new thread.

00:53:34.880 --> 00:53:38.490
So we're bleeding all that time
for every single frame of encoding.

00:53:38.500 --> 00:53:42.950
So realizing this,
I went in and tried to optimize that.

00:53:42.960 --> 00:53:45.590
Now I'm just going to go ahead
and give you a quick tour of this

00:53:45.590 --> 00:53:47.240
over here in the trace view too.

00:53:47.500 --> 00:53:49.720
Switch over to the system calls here.

00:53:49.720 --> 00:53:53.880
There's one of those without a name,
BSD system call.

00:53:53.880 --> 00:53:56.870
Again, you can poke this button
to see the full call stack.

00:53:56.920 --> 00:54:00.780
Now what I'm going to do is double click
on this and go to the timeline view.

00:54:00.780 --> 00:54:02.960
Now again,
you'll notice that's really busy,

00:54:02.960 --> 00:54:04.400
and it's all the same color.

00:54:04.400 --> 00:54:06.020
It's not telling us a
whole lot of information.

00:54:06.020 --> 00:54:08.920
So let's go to window,
show advanced settings.

00:54:10.270 --> 00:54:13.000
- Let's scroll all the way up here,
and we'll enable thread coloring.

00:54:13.190 --> 00:54:14.000
There we go.

00:54:14.000 --> 00:54:16.330
So now at a glance,
we can see which threads were

00:54:16.330 --> 00:54:18.520
running on which CPUs at which time.

00:54:18.710 --> 00:54:20.560
It was very useful.

00:54:20.960 --> 00:54:23.020
Now again, I said it would take
me right to that event,

00:54:23.020 --> 00:54:24.430
but I'm not exactly sure where it is.

00:54:24.540 --> 00:54:25.680
So I'm going to press Enter.

00:54:25.890 --> 00:54:26.520
There it is.

00:54:26.620 --> 00:54:29.680
And it gives me the pop-up,
that little inspector for the

00:54:29.680 --> 00:54:31.400
event that is highlighted.

00:54:31.440 --> 00:54:34.050
That pops right up to the front there.

00:54:34.210 --> 00:54:36.790
So let's go ahead and zoom in on that.

00:54:37.130 --> 00:54:40.840
And let's use these filtering options
here to just look at Handbrake.

00:54:40.930 --> 00:54:42.710
And let's see if we can
find this new thread.

00:54:42.730 --> 00:54:44.880
So here's our call-- oh, this is an exit.

00:54:44.900 --> 00:54:46.870
All right, let's find the other one.

00:54:50.720 --> 00:54:53.550
Here it is.

00:54:53.670 --> 00:54:55.940
So there's the thread
creation system call.

00:54:55.960 --> 00:54:59.800
Let's see if we can find where that
thread comes in and how long it took.

00:54:59.930 --> 00:55:01.670
So there's the whole call stack.

00:55:01.710 --> 00:55:05.640
Here's how much CPU time it took,
77 microseconds there.

00:55:05.660 --> 00:55:08.200
And there it looks like the first thread.

00:55:08.260 --> 00:55:11.780
So you can see the big delay chunk
here between where we asked for

00:55:11.780 --> 00:55:15.320
a new thread to be created and
where it actually starts running.

00:55:15.330 --> 00:55:17.300
And with System Trace,
we didn't have to do anything

00:55:17.300 --> 00:55:20.260
to our application to
measure this amount of time.

00:55:21.490 --> 00:55:22.600
All right.

00:55:22.680 --> 00:55:24.740
So that's all well and good.

00:55:24.780 --> 00:55:27.600
I spent about a week working
with the source code here,

00:55:27.600 --> 00:55:29.700
mainly because I'd never seen it before.

00:55:29.750 --> 00:55:31.960
I wasn't too familiar
with how it was working.

00:55:32.040 --> 00:55:35.010
And I made it so that these
worker threads that it's

00:55:35.040 --> 00:55:38.120
using to share all the CPUs,
they stick around when

00:55:38.170 --> 00:55:41.040
they're done with the work,
and they wait for the main thread to say,

00:55:41.040 --> 00:55:44.200
"Hey, here's another frame to
work on." So I was thinking,

00:55:44.200 --> 00:55:47.160
if I can get rid of this 80-ish
microseconds of overhead,

00:55:47.160 --> 00:55:50.000
maybe I can get some
more frames per second.

00:55:50.540 --> 00:55:53.960
So let's go ahead and do
another encoding here.

00:55:54.080 --> 00:55:55.720
So I'm going to start it.

00:55:55.770 --> 00:55:56.530
Let's overwrite that.

00:55:56.580 --> 00:56:00.320
And again,
I'm going to start it with that hot key.

00:56:00.320 --> 00:56:02.230
There it goes.

00:56:03.790 --> 00:56:09.580
Let's go ahead and quit this application,
give Shark all of the CPU power it needs.

00:56:09.580 --> 00:56:12.330
We'll wait for that session to come up.

00:56:14.100 --> 00:56:14.860
Oh, yeah.

00:56:14.860 --> 00:56:15.730
Forgot to show you this.

00:56:15.880 --> 00:56:19.280
One main thing I wanted
to show you here was--

00:56:20.220 --> 00:56:22.300
There we go.

00:56:22.370 --> 00:56:24.880
After spending all that
time working with it,

00:56:24.920 --> 00:56:28.280
what did I get out of it
with the frames per second?

00:56:28.300 --> 00:56:29.330
Not a whole lot.

00:56:29.420 --> 00:56:31.570
It's still, in fact,
it even looks about a

00:56:31.590 --> 00:56:33.120
frame per second worse.

00:56:33.240 --> 00:56:36.390
So let's see if we can
figure out why that is.

00:56:37.600 --> 00:56:40.520
So I'm just going to go ahead and
jump right to the timeline here.

00:56:40.580 --> 00:56:43.440
Again, you can see that idle
time is still pretty high.

00:56:43.480 --> 00:56:45.530
The thing that jumps out at
me at the timeline here was,

00:56:45.550 --> 00:56:47.100
whoa, look at that.

00:56:47.150 --> 00:56:49.660
Over here on the left,
we have all of these worker threads

00:56:49.690 --> 00:56:51.820
sharing the CPUs reasonably well.

00:56:52.050 --> 00:56:54.770
But over on the right there,
we've got that one thread just

00:56:54.770 --> 00:56:57.580
doing whatever it's doing for
such a long amount of time.

00:56:57.580 --> 00:57:00.330
It's blocking all the other threads,
preventing them from

00:57:00.330 --> 00:57:02.010
doing their parallel work.

00:57:02.190 --> 00:57:05.320
So just at a glance here,
we can get an idea of where

00:57:05.320 --> 00:57:07.200
our performance issue is.

00:57:07.400 --> 00:57:10.260
It's over in this thread that's
doing all this serial execution.

00:57:10.320 --> 00:57:14.650
So if we can figure out why that is,
then we can go in and address that issue.

00:57:14.730 --> 00:57:16.240
Unfortunately,
I didn't have time to do that,

00:57:16.300 --> 00:57:20.210
so I'm gonna show you
another demo as well.

00:57:23.090 --> 00:57:26.350
So here's an application that some
of you who've been here before are

00:57:26.430 --> 00:57:27.960
probably familiar with called Noble Ape.

00:57:28.040 --> 00:57:32.300
And what it does is just simulates
these apes running around the island.

00:57:32.300 --> 00:57:34.690
It does a bunch of calculations
and simulates how many

00:57:34.690 --> 00:57:36.060
thoughts a second it gets.

00:57:36.060 --> 00:57:37.700
We originally got it.

00:57:37.700 --> 00:57:40.520
We get about 2,000 ape thoughts a second.

00:57:40.520 --> 00:57:44.600
And we spent a lot of time with time
profile to go ahead and optimize that.

00:57:44.620 --> 00:57:51.840
And we got it up to about 14,000,
almost 15,000 ape thoughts per second.

00:57:52.620 --> 00:57:54.540
We also went in and multithreaded it.

00:57:54.580 --> 00:57:58.200
And we're getting up to,
what are we at there?

00:57:58.200 --> 00:58:01.410
Topping out about 60,000
ape thoughts per second.

00:58:01.420 --> 00:58:02.210
That's pretty good.

00:58:02.220 --> 00:58:06.580
Now my theory here is we've done kind
of a first pass multithreading here.

00:58:06.580 --> 00:58:08.980
So we're probably some performance
we can still pull out of it.

00:58:09.030 --> 00:58:11.390
So let's take a quick system
trace and see what's going on.

00:58:13.090 --> 00:58:16.340
Go, and I'm just going to quit that.

00:58:16.340 --> 00:58:17.520
Let Shark do its processing.

00:58:19.250 --> 00:58:19.710
Here we go.

00:58:19.790 --> 00:58:22.560
Again,
you see that high idle time number.

00:58:22.560 --> 00:58:23.800
That's probably not very good.

00:58:23.800 --> 00:58:26.030
Let's go focus in on Noble Ape.

00:58:26.120 --> 00:58:27.170
Now, we click this up.

00:58:27.230 --> 00:58:29.560
We can see all of the
threads in the application.

00:58:29.570 --> 00:58:33.050
These here are the worker threads
that are supposed to be sharing all

00:58:33.050 --> 00:58:35.000
eight of the cores in the machine.

00:58:35.000 --> 00:58:37.320
And one thing that
jumps out at me here is,

00:58:37.320 --> 00:58:40.120
see this number here,
the average run interval,

00:58:40.130 --> 00:58:44.180
the average amount of busy time
each one of these threads is taking?

00:58:44.180 --> 00:58:45.680
That's really low.

00:58:45.680 --> 00:58:48.620
It's on the order of 500 microseconds.

00:58:49.200 --> 00:58:53.640
Now, the time quantum for any thread in
OS X is topped out at 10 milliseconds.

00:58:53.690 --> 00:58:58.750
So these could be using a whole
lot more CPU time before blocking.

00:59:00.100 --> 00:59:03.590
So my theory is we can probably
improve the performance by

00:59:03.590 --> 00:59:05.340
giving them more work to do.

00:59:05.340 --> 00:59:08.450
Now, if we go look at the timeline view,
we see kind of a regular pattern.

00:59:08.540 --> 00:59:11.640
We see this main thread,
which doing the UI update here,

00:59:11.660 --> 00:59:14.940
the worker threads doing
some work and synchronizing.

00:59:15.030 --> 00:59:17.350
So if we go zoom in,
we can see a couple of

00:59:17.400 --> 00:59:18.400
things jump out at us.

00:59:18.590 --> 00:59:22.120
First of all,
we've got all this lock contention here.

00:59:22.170 --> 00:59:24.120
Second of all,

00:59:24.400 --> 00:59:29.900
[Transcript missing]

00:59:30.060 --> 00:59:31.950
Wade on cue, coming from Noble Abe.

00:59:32.120 --> 00:59:35.720
So that tells me they're using
the kind of work cue model,

00:59:35.720 --> 00:59:37.370
which is pretty good in my opinion.

00:59:37.480 --> 00:59:39.440
You have a work cue,
you have a thread attached to it,

00:59:39.440 --> 00:59:41.400
you give it some work,
it wakes up and does it,

00:59:41.430 --> 00:59:43.150
and then tells the cue that it's done.

00:59:43.220 --> 00:59:44.170
That's great.

00:59:44.400 --> 00:59:46.920
Problem is it looks like they're
not getting enough work to do

00:59:46.920 --> 00:59:52.070
because they just run for very short
here and then go back to sleep.

00:59:52.610 --> 00:59:55.340
So I'm going to close this session.

00:59:55.340 --> 00:59:57.950
Long story short,
I spent some time trying to increase

00:59:57.950 --> 01:00:00.180
the amount of work these are doing.

01:00:00.180 --> 01:00:03.920
And I actually had to add an extra digit
here to the eight thoughts per second.

01:00:03.920 --> 01:00:07.900
So I've got it up to 102,000
eight thoughts per second.

01:00:07.900 --> 01:00:12.440
Now let's go take a system trace and
see what that looks like real quick.

01:00:12.450 --> 01:00:14.220
Looks like I pressed the wrong button.

01:00:14.240 --> 01:00:16.920
There we go.

01:00:16.920 --> 01:00:18.930
All right, quit that.

01:00:21.730 --> 01:00:24.800
All right, so remember,
the idle time previously

01:00:24.800 --> 01:00:26.270
was up at 40-something.

01:00:26.320 --> 01:00:28.870
It's now down to 20,
so we've improved that quite a bit.

01:00:28.880 --> 01:00:32.450
If we look at Noble Ape here and
we look at its worker threads,

01:00:32.450 --> 01:00:36.280
we notice their average run
interval is up to 5 milliseconds,

01:00:36.280 --> 01:00:39.360
that order of magnitude,
and that's a really good improvement.

01:00:39.360 --> 01:00:43.980
All right,
so let's go back to the slides.

01:00:44.040 --> 01:00:45.270
All right.

01:00:50.790 --> 01:00:53.560
So just a quick recap,
what did we see there?

01:00:53.570 --> 01:00:56.000
When we were working with Handbrake,
we saw the worker threads were

01:00:56.000 --> 01:00:58.460
being created and destroyed
for every single frame,

01:00:58.460 --> 01:01:00.600
and that's kind of a lot of overhead.

01:01:00.670 --> 01:01:02.340
So we went ahead and
removed all that overhead,

01:01:02.340 --> 01:01:04.900
but we really didn't get
much boost out of it.

01:01:04.940 --> 01:01:07.420
But using System Trace,
we could immediately identify why

01:01:07.420 --> 01:01:09.760
we didn't get much boost out of it,
and that's because there was that

01:01:09.760 --> 01:01:14.190
large chunk of serial execution
masking any performance gain that

01:01:14.190 --> 01:01:16.490
we got out of the worker threads.

01:01:17.270 --> 01:01:22.730
Now with Noble Ape, however,
we were able to get 1.8 speed up there,

01:01:22.740 --> 01:01:25.130
1.8x in roughly 45 minutes of work.

01:01:25.160 --> 01:01:30.560
And that's not bad, not having a strong
familiarity with the code.

01:01:30.560 --> 01:01:34.460
So by now you've seen time profile
and you've seen system trace.

01:01:34.460 --> 01:01:37.930
You've seen a couple of very powerful
ways to optimize your application.

01:01:37.960 --> 01:01:42.360
But what happens if you're not working
on the CPU and you're not strictly

01:01:42.460 --> 01:01:43.920
having multi-threading issues?

01:01:44.060 --> 01:01:47.440
Maybe your performance issue is
more centered around memory usage.

01:01:47.440 --> 01:01:49.960
Well, for that we offer Malik Trace.

01:01:51.390 --> 01:01:54.810
Now this is easily found in
the configuration pop-up.

01:01:54.850 --> 01:01:57.270
It's very cleverly disguised,
named Malik Trace.

01:01:57.280 --> 01:02:02.460
What this does is it takes an exact trace
of every allocation and every destruction

01:02:02.460 --> 01:02:04.420
of memory in your application.

01:02:04.420 --> 01:02:06.730
So any kind of time
you call Malik or new,

01:02:06.730 --> 01:02:09.420
free or delete, it'll show up in this.

01:02:09.500 --> 01:02:12.380
That looks exactly like a time profile.

01:02:12.380 --> 01:02:16.360
All the data mining options
work in much the same way.

01:02:16.360 --> 01:02:18.320
A couple of things to notice here.

01:02:18.320 --> 01:02:22.420
We've got the total column,
just like you do in a time profile.

01:02:22.420 --> 01:02:25.800
So this allows you to find excessive
numbers of small allocations,

01:02:25.870 --> 01:02:26.540
perhaps.

01:02:26.540 --> 01:02:29.460
We also have the Alex size column here.

01:02:29.460 --> 01:02:31.430
This is the new column
we added for Malik Trace,

01:02:31.540 --> 01:02:35.190
which will allow you to find maybe small
numbers of really large allocations.

01:02:37.650 --> 01:02:40.760
Now if you flip over to the chart view,
we've got this extra chart here,

01:02:40.770 --> 01:02:43.430
which represents the change
in heap size over time.

01:02:43.440 --> 01:02:47.140
So any bar above the midline
represents an allocation.

01:02:47.140 --> 01:02:49.340
Any bar below represents a free.

01:02:49.340 --> 01:02:53.610
So just by looking at this chart here,
we can get two pieces of really

01:02:53.610 --> 01:02:55.420
key information out of it.

01:02:55.500 --> 01:02:58.390
First, it does not look like
we're leaking any memory,

01:02:58.410 --> 01:03:02.190
because for every bar above the midline,
we have an equally sized one below later.

01:03:02.660 --> 01:03:05.120
The second thing is,
we see this repeating pattern

01:03:05.120 --> 01:03:07.800
of allocate a bunch of stuff,
free a bunch of stuff.

01:03:07.940 --> 01:03:08.980
Allocate, free.

01:03:08.980 --> 01:03:12.340
And that makes me think,
maybe we can be reusing these buffers

01:03:12.340 --> 01:03:15.230
instead of releasing them back to the
system and then getting them again.

01:03:15.240 --> 01:03:17.410
If we do that,
we'd probably see a performance

01:03:17.470 --> 01:03:19.080
speedup in this application.

01:03:21.610 --> 01:03:25.200
The other real powerful thing
here is just like with the

01:03:25.250 --> 01:03:29.030
system trace system call profile,
sometimes you don't know that you're

01:03:29.030 --> 01:03:30.250
going to be making allocations.

01:03:30.260 --> 01:03:32.500
Maybe you call a library
method and you're not sure.

01:03:32.500 --> 01:03:34.220
Well, those will show up here.

01:03:34.220 --> 01:03:37.810
And using the disclosure triangles,
you can walk back into the

01:03:37.920 --> 01:03:41.090
function in your application
that called that library,

01:03:41.100 --> 01:03:43.420
which in turn made the allocation.

01:03:44.080 --> 01:03:46.640
Now, if that isn't quite
enough level of detail,

01:03:46.640 --> 01:03:49.790
you can double click on that and
it'll take you right to the line

01:03:49.790 --> 01:03:53.530
of source code in your application
where that allocation was made.

01:03:55.660 --> 01:03:57.440
All right, so you've seen time profile.

01:03:57.440 --> 01:03:58.380
You've seen system trace.

01:03:58.400 --> 01:03:59.840
You've seen malloc trace.

01:03:59.870 --> 01:04:01.430
All the Java guys in
the audience are going,

01:04:01.430 --> 01:04:02.140
what about Java?

01:04:02.180 --> 01:04:04.470
Well, let's talk about Java real quick.

01:04:04.470 --> 01:04:06.820
As you're probably
beginning to understand,

01:04:06.820 --> 01:04:09.930
Shark is a native machine profiler,
so it tells you right down

01:04:09.960 --> 01:04:12.770
to the hardware what the
hardware is doing over time.

01:04:12.770 --> 01:04:15.220
Now, if you were to do this
for a Java application,

01:04:15.220 --> 01:04:18.240
that would kind of give you an
idea of what the Java VM was doing,

01:04:18.240 --> 01:04:21.060
which isn't very useful
for your Java application.

01:04:21.060 --> 01:04:25.580
Shark's Java profiling lifts us one
level up into the Java application.

01:04:25.600 --> 01:04:29.920
So now we can give you meaningful
feedback into your Java classes

01:04:29.920 --> 01:04:32.080
and what are they doing over time.

01:04:34.190 --> 01:04:37.380
Now we've got three main analysis
techniques that we support for Java.

01:04:37.380 --> 01:04:40.050
We have the time profile,
which looks and behaves

01:04:40.060 --> 01:04:41.810
exactly like a time profile.

01:04:41.960 --> 01:04:45.490
We've got the call trace,
and this is an exact function trace of

01:04:45.490 --> 01:04:48.400
every method called in your application.

01:04:48.460 --> 01:04:50.410
And we also have the allocation trace.

01:04:50.560 --> 01:04:53.160
And again,
you'll notice the Alexize column

01:04:53.160 --> 01:04:55.020
looks just like Malik trace.

01:04:55.410 --> 01:04:57.760
So let's wrap things up here.

01:04:57.840 --> 01:05:00.150
What have we learned
today in this session?

01:05:00.270 --> 01:05:02.910
First of all,
Shark is a really powerful tool for

01:05:02.910 --> 01:05:08.090
giving you performance profiling feedback
right down to the instruction level.

01:05:08.420 --> 01:05:11.640
You've seen three main
types of profiling,

01:05:11.640 --> 01:05:14.510
and you've seen their analogs
for what we have in the

01:05:14.510 --> 01:05:17.770
Java VM and how it can be useful.

01:05:18.000 --> 01:05:20.310
If you're interested in more,
check out the video of the

01:05:20.320 --> 01:05:23.260
advanced session on ADC and
iTunes after the conference.

01:05:23.260 --> 01:05:24.780
That's session 911.

01:05:24.780 --> 01:05:27.400
Or feel free to send us an email.

01:05:27.400 --> 01:05:29.020
We always love hearing
from our developers.

01:05:29.020 --> 01:05:34.600
For more information,
contact the developer tools

01:05:34.600 --> 01:05:36.840
or performance evangelist,
Michael Jurowicz.

01:05:36.840 --> 01:05:39.880
His email is jurowicz at apple.com.

01:05:39.880 --> 01:05:42.820
Also,
feel free to send us feedback directly.

01:05:42.820 --> 01:05:47.400
You can send that to
perftools-feedback at group.apple.com.

01:05:48.020 --> 01:05:49.520
Our whole team is on that,
and we always love

01:05:49.520 --> 01:05:51.740
hearing from developers,
whether it be good or bad.

01:05:51.740 --> 01:05:55.340
Again, check out sample codes
associated with these sessions,

01:05:55.340 --> 01:05:56.940
and check out the attendee website.

01:05:58.390 --> 01:06:02.210
For any more advanced questions
that we don't get to in Q&A,

01:06:02.280 --> 01:06:05.960
feel free to come hit us up in
the optimization labs tomorrow.

01:06:05.960 --> 01:06:07.670
We'll be there all day.