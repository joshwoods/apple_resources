WEBVTT

00:00:20.630 --> 00:00:23.530
Welcome to Cocoa Performance Techniques.

00:00:23.540 --> 00:00:27.510
My name is Ali Ozer.

00:00:27.510 --> 00:00:27.510
I'm the manager of the
Cocoa Frameworks team.

00:00:33.760 --> 00:00:36.380
What are we going to discuss today?

00:00:36.480 --> 00:00:38.580
About half the talk is going
to be about concurrency.

00:00:38.580 --> 00:00:41.680
As you heard during Bertrand's
talk and during other talks,

00:00:41.690 --> 00:00:46.020
concurrency, multi-core,
is an important aspect of Snow Leopard,

00:00:46.020 --> 00:00:49.580
and I do want to cover it in some detail
during the first half of this talk.

00:00:50.020 --> 00:00:52.160
Beyond concurrency,
we're also going to talk about

00:00:52.280 --> 00:00:55.620
computational complexity,
memory, and drawing, and also point out a

00:00:55.620 --> 00:00:56.920
number of other topics.

00:00:56.920 --> 00:00:59.540
Now, we were not going to cover all
of the performance techniques

00:00:59.540 --> 00:01:02.040
and topics that we usually cover,
but I am going to point at some

00:01:02.040 --> 00:01:04.650
documentation and make sure I sort
of outline them at the end to make

00:01:04.750 --> 00:01:06.580
sure everybody's on the same page.

00:01:08.560 --> 00:01:12.500
Okay,
so the first topic is multi-threading.

00:01:12.530 --> 00:01:17.780
Multi-threading is use of multiple
threads of execution in one process.

00:01:17.820 --> 00:01:21.420
And this is in one address space,
which of course has benefits

00:01:21.500 --> 00:01:23.480
but also has some downsides.

00:01:23.480 --> 00:01:24.980
We're going to see some of these.

00:01:24.980 --> 00:01:27.850
And these days, of course,
thanks to multiple cores,

00:01:27.850 --> 00:01:30.700
multiple threads are really
executed concurrently.

00:01:30.810 --> 00:01:33.480
Two threads are executing
at the same exact instant,

00:01:33.480 --> 00:01:36.000
touching the same exact memory locations.

00:01:36.760 --> 00:01:39.550
Now,
multi-threading can have many surprises,

00:01:39.550 --> 00:01:41.770
and I'm going to show a demo for this.

00:01:42.560 --> 00:01:47.950
In the demo, we have a model object
with an integer property,

00:01:47.950 --> 00:01:51.690
and we have various activity
objects which are running threads,

00:01:51.690 --> 00:01:55.500
and these threads are updating
this integer as fast as possible.

00:01:55.500 --> 00:01:58.100
So they're just setting it
with incrementing values.

00:01:58.240 --> 00:02:00.020
And we can run one thread
or multiple threads,

00:02:00.070 --> 00:02:01.840
and we're going to see how this behaves.

00:02:01.840 --> 00:02:05.790
So can we switch to the demo machine,
please?

00:02:09.410 --> 00:02:12.000
I'm not going to show you
the source code very much,

00:02:12.110 --> 00:02:13.400
just this piece here.

00:02:13.400 --> 00:02:15.840
This is basically the one step
of our little thread loop.

00:02:15.990 --> 00:02:19.400
It's basically incrementing a
value and setting it on the model.

00:02:19.490 --> 00:02:20.400
That's all it's doing.

00:02:20.400 --> 00:02:22.830
So let's go ahead and run this.

00:02:23.030 --> 00:02:25.400
So, first -- Oh,
let me just show you this.

00:02:25.400 --> 00:02:28.050
In the number of tests we run today,
number of demos we run today,

00:02:28.050 --> 00:02:31.510
you're going to see this pattern where
we have a button up here that lets us

00:02:31.510 --> 00:02:33.680
switch between various tests we're doing.

00:02:33.680 --> 00:02:36.650
And then we can specify the
number of seconds to run the test,

00:02:36.760 --> 00:02:38.790
in this case, three,
and number of threads.

00:02:39.120 --> 00:02:41.640
So,
let's go ahead and set a single property

00:02:41.640 --> 00:02:44.380
from one thread for three seconds.

00:02:44.380 --> 00:02:47.710
And as you can see,
we're getting about 300 million updates.

00:02:47.840 --> 00:02:48.240
Okay.

00:02:48.240 --> 00:02:48.980
That's fine.

00:02:48.980 --> 00:02:51.420
We're running on a pretty nice,
fast machine here.

00:02:51.470 --> 00:02:54.320
So, one question is,
what happens if we go up to two threads?

00:02:54.540 --> 00:02:58.050
Now, the optimists among you might be
thinking we're going to get 2X,

00:02:58.050 --> 00:03:01.350
and some of the realists among you are
thinking we're not going to get 2X.

00:03:01.350 --> 00:03:03.300
We're going to get slightly less than 2X.

00:03:03.300 --> 00:03:05.680
So, let's go ahead and run
and see what happens.

00:03:05.830 --> 00:03:09.020
Now, remember,
we got 300 million with one thread.

00:03:09.020 --> 00:03:12.540
And with two threads, we get 108 million.

00:03:12.570 --> 00:03:15.350
Now, consider, we're running on a
machine with eight cores.

00:03:15.420 --> 00:03:16.320
We have two threads.

00:03:16.320 --> 00:03:18.390
Presumably each one is
running on their own core.

00:03:18.520 --> 00:03:20.740
And we're getting a third of the
throughput that we're getting.

00:03:20.740 --> 00:03:22.640
That we're getting with just one thread.

00:03:22.640 --> 00:03:24.130
So, that's already pretty surprising.

00:03:24.370 --> 00:03:26.680
Now, you might be thinking, you know,
you didn't show us the whole program.

00:03:26.680 --> 00:03:28.320
There's something shifty going on.

00:03:28.390 --> 00:03:31.240
Maybe the OBJC runtime
is slowing you down.

00:03:31.260 --> 00:03:34.520
Let's go look at that program a bit.

00:03:34.520 --> 00:03:36.240
I'm going to show another sample.

00:03:36.240 --> 00:03:39.230
I'm just going to show a sample
where instead of setting a value,

00:03:39.230 --> 00:03:41.600
all we're going to be doing
is reading the value back.

00:03:41.620 --> 00:03:43.130
We're not going to
bother setting the value.

00:03:43.180 --> 00:03:44.740
But, again, we're doing a message send.

00:03:44.740 --> 00:03:48.160
I'm using the exact same
infrastructure for the application.

00:03:48.160 --> 00:03:49.750
Let's go ahead and run this.

00:03:49.870 --> 00:03:52.300
So, reading a message send.

00:03:52.300 --> 00:03:56.780
And we're going to be
reading the value back.

00:03:56.840 --> 00:03:59.730
And we're going to be reading

00:04:00.250 --> 00:04:02.320
We do get twice the throughput.

00:04:02.320 --> 00:04:05.990
So the problem here is clearly
in the setting of that value,

00:04:05.990 --> 00:04:08.480
and it has nothing to do
with the rest of the program.

00:04:08.480 --> 00:04:11.390
So if we can switch back to the slides,
please.

00:04:14.270 --> 00:04:16.580
Okay,
so multiple threads aren't always better.

00:04:16.720 --> 00:04:19.700
Even with N threads
running on N processors,

00:04:19.720 --> 00:04:21.320
and without any sort
of locking or anything,

00:04:21.320 --> 00:04:23.260
we're not, you know,
we weren't doing any locking in there.

00:04:23.260 --> 00:04:24.430
There's nothing to stall us.

00:04:24.430 --> 00:04:25.900
We're seeing a slowdown.

00:04:26.240 --> 00:04:28.790
So the problem in this case
is what's called contention.

00:04:28.800 --> 00:04:31.400
Multiple threads are writing
into the same memory location,

00:04:31.400 --> 00:04:34.540
and the hardware is trying very hard
to make sure that memory location,

00:04:34.540 --> 00:04:37.580
and it turns out other memory
locations on the same cache line,

00:04:37.580 --> 00:04:39.270
are visible to all processors.

00:04:39.270 --> 00:04:42.660
And that update ends up
stalling all these cores.

00:04:42.660 --> 00:04:44.470
And that's part of the problem here.

00:04:44.470 --> 00:04:46.870
And it's not really obvious, you know,
when you look at the program,

00:04:46.870 --> 00:04:47.610
what's going on.

00:04:47.640 --> 00:04:50.360
Now,
what are possible solutions in this case?

00:04:50.360 --> 00:04:52.950
Well,
one is to not use the same shared state.

00:04:53.020 --> 00:04:55.620
For instance,
you might decide to do the work locally,

00:04:55.730 --> 00:04:58.470
increment your value all locally,
and only once in a while

00:04:58.490 --> 00:04:59.730
update shared state.

00:04:59.760 --> 00:05:02.320
In some cases, though,
you might have to deal with conflicts.

00:05:02.320 --> 00:05:06.080
And we're going to talk about some
of these topics in a few seconds.

00:05:07.800 --> 00:05:12.300
One other thing to note, in our example,
we didn't care about correctness at all.

00:05:12.300 --> 00:05:14.300
Our threads were just
blasting that value in there,

00:05:14.300 --> 00:05:15.950
and we didn't care what
happened to the value,

00:05:15.950 --> 00:05:18.420
whether it survived or whether
it made sense and so on.

00:05:18.460 --> 00:05:22.410
And we're going to talk
about that right now.

00:05:24.000 --> 00:05:25.060
So, atomic properties.

00:05:25.060 --> 00:05:29.500
When I say atomic properties here,
I'm referring to OBJC 2.0 properties.

00:05:29.500 --> 00:05:32.560
And you may know, or you may not,
but by default, they're atomic.

00:05:32.560 --> 00:05:36.080
And you can make them non-atomic by
putting this non-atomic keyword in there.

00:05:36.860 --> 00:05:40.300
Now, what atomic guarantees is that in
the presence of multiple threads,

00:05:40.300 --> 00:05:45.390
some are setting, some are getting,
the value you're trying to set

00:05:45.390 --> 00:05:47.880
or get is set or retrieved fully.

00:05:47.880 --> 00:05:51.210
So,
it doesn't come back to you as garbage.

00:05:51.240 --> 00:05:53.920
So, you either get the previous
value or the after value,

00:05:53.920 --> 00:05:56.040
but you never get some
intermediate state.

00:05:56.140 --> 00:05:58.540
Let's see what this means.

00:05:58.540 --> 00:06:00.020
Can we switch to the demo machine?

00:06:00.020 --> 00:06:01.330
Okay.

00:06:07.830 --> 00:06:10.170
In this example,
what I'm going to do is I have

00:06:10.170 --> 00:06:13.700
a bunch of model objects and
I have a bounds property here.

00:06:13.700 --> 00:06:15.790
In one case, it's declared atomic.

00:06:15.890 --> 00:06:18.660
In another case,
it's declared non-atomic.

00:06:18.720 --> 00:06:21.940
We're going to see the effects
of atomic versus non-atomic.

00:06:21.940 --> 00:06:25.980
If you run this test case,
first let's do the atomic

00:06:25.980 --> 00:06:28.300
struct as we saw in NSRect.

00:06:29.040 --> 00:06:32.350
So you can see that with one thread,
we're getting some updates.

00:06:32.570 --> 00:06:36.020
Rather than number of updates,
this failure rate is what we're

00:06:36.020 --> 00:06:40.900
looking at right now because
we're tracking correctness.

00:06:40.900 --> 00:06:40.900
With two threads,

00:06:41.070 --> 00:06:42.100
Again, no failures.

00:06:42.100 --> 00:06:42.700
That's great.

00:06:42.700 --> 00:06:46.020
And our rate has slowed down a bit,
again, because of the contention problem.

00:06:46.170 --> 00:06:51.170
Now let's go ahead and use a non-atomic
struct property with two threads.

00:06:51.410 --> 00:06:53.540
In this case,
we are seeing a failure rate.

00:06:53.540 --> 00:06:56.590
The way we're detecting a failure rate
is we're setting a rectangle where the X,

00:06:56.590 --> 00:06:58.200
Y with height are all the same.

00:06:58.500 --> 00:07:00.720
When we read it back,
we see if they're all the same.

00:07:00.740 --> 00:07:02.300
Once in a while, they are not.

00:07:02.330 --> 00:07:04.580
Clearly,
we read back an intermediate state.

00:07:04.730 --> 00:07:07.810
Let's see what happens when we
do the same thing with NSStrings,

00:07:07.810 --> 00:07:09.690
which is, of course, an object.

00:07:09.840 --> 00:07:12.600
With an atomic NSString, no failure rate.

00:07:12.650 --> 00:07:14.580
Again, atomic is good.

00:07:14.770 --> 00:07:18.170
Let's see what happens
with non-atomic NSStrings.

00:07:18.990 --> 00:07:19.780
Okay, crash.

00:07:19.800 --> 00:07:22.140
And don't panic, that was all expected.

00:07:22.140 --> 00:07:23.890
In fact,
that's the purpose of the demo here.

00:07:23.990 --> 00:07:27.240
With non-atomic, in some cases,
you may crash.

00:07:27.380 --> 00:07:30.160
Okay, so let's go back to the slides.

00:07:30.160 --> 00:07:34.130
Slides, please.

00:07:38.520 --> 00:07:40.360
Let's look at the
struct value case first.

00:07:40.590 --> 00:07:43.960
By default, multi-word structs are
not copied atomically.

00:07:43.960 --> 00:07:46.270
Here's what our -- I'm using
size here instead of rect,

00:07:46.280 --> 00:07:49.370
just because it's simpler,
but as you know, a size and then a size

00:07:49.380 --> 00:07:50.800
has a width and a height.

00:07:51.090 --> 00:07:54.970
Now, when you see size equals new size,
that looks atomic enough for you,

00:07:54.970 --> 00:07:57.380
but it's really assigning two values.

00:07:57.380 --> 00:07:59.130
So if we were to look
at what happens here,

00:07:59.130 --> 00:08:00.470
let's say we have two threads.

00:08:00.630 --> 00:08:01.910
One is doing a set size.

00:08:02.210 --> 00:08:03.710
One is doing size.

00:08:03.710 --> 00:08:07.200
Thread one sets the
new width on the size.

00:08:07.240 --> 00:08:10.120
At the same time,
thread two is trying to read the size.

00:08:10.120 --> 00:08:11.420
It happens to fetch the width.

00:08:11.490 --> 00:08:13.000
It happens to fetch the height.

00:08:13.000 --> 00:08:14.900
And thread one happens to set the height.

00:08:15.110 --> 00:08:17.600
You know,
these can happen non-deterministically.

00:08:17.600 --> 00:08:19.300
And as a result,
what you got back is the new

00:08:19.300 --> 00:08:22.220
width and the old height,
and that's the failure case we're seeing.

00:08:22.350 --> 00:08:24.340
And of course, not good.

00:08:24.340 --> 00:08:25.000
Fail.

00:08:25.000 --> 00:08:26.470
Let's look at the object value case.

00:08:26.590 --> 00:08:29.340
Now, you might be thinking,
aren't objects atomic?

00:08:29.340 --> 00:08:30.120
They're pointers.

00:08:30.120 --> 00:08:30.970
They must be atomic.

00:08:30.980 --> 00:08:35.070
Well, here's what set name and
name methods look like,

00:08:35.070 --> 00:08:37.220
the non-atomic variants.

00:08:37.220 --> 00:08:41.530
We copied the assigned name
and we released the first one.

00:08:41.620 --> 00:08:43.800
So again, looking at the two threads,

00:08:44.070 --> 00:08:48.700
Thread 1 fetches the name into
old name in the first line,

00:08:48.700 --> 00:08:50.760
and at the same time,
before or after this,

00:08:50.760 --> 00:08:53.620
it doesn't really matter,
thread 2 fetches name with

00:08:53.710 --> 00:08:54.930
the intent of returning

00:08:56.400 --> 00:08:59.140
Thread 1, in the meantime,
ends up making a copy of the new

00:08:59.140 --> 00:09:00.530
name and releases the old name.

00:09:00.570 --> 00:09:04.300
And notice that what it's releasing
is what Thread 2 is about to return.

00:09:04.330 --> 00:09:07.620
So in Thread 2, the name is returned,
the client tries to use it,

00:09:07.670 --> 00:09:09.500
and they got the crash we just saw.

00:09:09.500 --> 00:09:12.060
So that was -- you know,
that doesn't always crash.

00:09:12.070 --> 00:09:13.280
Sometimes it doesn't.

00:09:13.280 --> 00:09:15.880
You know, if you're lucky,
if you're a bug like this,

00:09:15.880 --> 00:09:19.330
it'll crash for you during development
and not when your users use it.

00:09:19.420 --> 00:09:20.060
So it's good.

00:09:20.060 --> 00:09:23.290
Anyway, so this is the reason why
object values properties also,

00:09:23.290 --> 00:09:25.410
when they're non-atomic,
they're dangerous.

00:09:25.640 --> 00:09:29.590
So atomic properties,
the whole notion of atomicity is

00:09:29.590 --> 00:09:31.350
not really overall thread safety.

00:09:31.450 --> 00:09:34.400
Just by slapping atomic
or not making non-atomic,

00:09:34.400 --> 00:09:35.900
you're not saying you're thread safe.

00:09:35.900 --> 00:09:40.150
But it does provide a low level of
thread safety for a single property.

00:09:40.150 --> 00:09:42.990
So it does not provide
consistency between properties,

00:09:42.990 --> 00:09:45.700
and we're going to talk about
this in the next section.

00:09:46.540 --> 00:09:48.920
It's often a good idea to
leave properties atomic.

00:09:48.920 --> 00:09:52.710
As we introduce properties in Cocoa,
our intention is to make them atomic.

00:09:52.710 --> 00:09:55.380
When would you consider not
making properties atomic?

00:09:55.380 --> 00:09:57.810
Well, performance-critical usages.

00:09:57.820 --> 00:09:58.860
It does have overhead.

00:09:58.860 --> 00:10:02.880
If you ever see OBJC get property
or OBJC set property in your

00:10:02.880 --> 00:10:06.760
samples or your shark analysis,
it's time to see whether

00:10:06.760 --> 00:10:08.820
atomicity is the reason.

00:10:09.140 --> 00:10:11.010
Now, of course,
if you're calling properties

00:10:11.010 --> 00:10:12.920
millions of times,
you are going to see these.

00:10:12.940 --> 00:10:14.830
Just see if the atomic
makes a difference.

00:10:14.840 --> 00:10:16.000
And if it does, it's critical.

00:10:16.140 --> 00:10:18.590
you can consider eliminating it.

00:10:18.970 --> 00:10:23.770
Another case where it might be
reasonable to eliminate atomicity,

00:10:23.960 --> 00:10:27.520
especially if it's performance critical,
is in cases where you have some

00:10:27.530 --> 00:10:29.420
higher level of synchronization.

00:10:29.420 --> 00:10:33.410
For instance, collections, NSArray,
NSDictionary, etc.

00:10:33.410 --> 00:10:35.410
do not do atomic behaviors.

00:10:35.890 --> 00:10:38.780
That's because we expect you to
have some other external locking

00:10:38.780 --> 00:10:42.350
if you're adding items to an array
and extracting items to an array.

00:10:42.350 --> 00:10:43.660
It's a mutable collection.

00:10:43.660 --> 00:10:45.990
And as long as you're
mutating it and reading it,

00:10:45.990 --> 00:10:49.000
you really should have some
external synchronization.

00:10:49.030 --> 00:10:52.500
Another example is NS managed
object context out of core data.

00:10:52.540 --> 00:10:54.860
It, of course, conforms to NS locking.

00:10:54.890 --> 00:10:57.380
And if you want to use
it for multiple threads,

00:10:57.380 --> 00:11:01.690
it's a good idea to lock and unlock the
whole object graph when mutating it.

00:11:01.710 --> 00:11:04.590
And that will provide
that synchronization.

00:11:04.890 --> 00:11:07.890
Speaking of synchronization,
earlier I talked about

00:11:07.890 --> 00:11:10.120
consistency between properties.

00:11:10.120 --> 00:11:14.780
The problem is to provide consistent
view between multiple related properties.

00:11:14.810 --> 00:11:17.040
Examples are first name, last name.

00:11:17.170 --> 00:11:20.670
Even if we made two properties,
first name and last name atomic,

00:11:20.670 --> 00:11:24.110
if one thread set first name and
set last name and then some other

00:11:24.110 --> 00:11:28.340
thread read first name and last name,
they might get an inconsistent view.

00:11:28.340 --> 00:11:32.470
They might get somebody else's last
name with somebody else's first name.

00:11:32.470 --> 00:11:33.720
Same with day, month, and year.

00:11:34.530 --> 00:11:38.000
One solution to this is to use
properties that combine related state.

00:11:38.000 --> 00:11:41.400
You may have a person object that
combines first and last name.

00:11:41.400 --> 00:11:44.640
You would set your first and last name
there and assign the person object.

00:11:44.790 --> 00:11:47.920
Instead of day, month, year,
use an NSState.

00:11:48.040 --> 00:11:49.270
Earlier we used NSRect.

00:11:49.330 --> 00:11:52.560
That combines four properties, X, Y,
width, height.

00:11:52.690 --> 00:11:55.630
By using an NSRect and
making the whole rect atomic,

00:11:55.790 --> 00:11:57.090
we avoid the problem.

00:11:57.170 --> 00:11:58.400
You can use RGB.

00:11:58.630 --> 00:12:01.920
Instead of individual RGB,
you can use NSColor.

00:12:02.080 --> 00:12:05.470
Using these abstractions is better
anyway because you're getting

00:12:05.470 --> 00:12:07.300
a higher level of abstraction.

00:12:07.360 --> 00:12:11.520
By using NSColor,
you're not only using RGB,

00:12:11.520 --> 00:12:15.420
you can now use CMYK, pattern colors,
et cetera.

00:12:15.520 --> 00:12:18.520
But this is just a solution
that sometimes applies.

00:12:18.520 --> 00:12:23.050
Sometimes you do need locking or
some other form of synchronization.

00:12:24.150 --> 00:12:26.100
Okay, let's go into a case study.

00:12:26.360 --> 00:12:28.180
We're going to look at a linked list.

00:12:28.180 --> 00:12:32.160
And in case you skipped out
on your college classes or

00:12:32.160 --> 00:12:34.820
you just don't remember,
a linked list is a simple

00:12:34.820 --> 00:12:36.260
data structure like this.

00:12:36.300 --> 00:12:39.020
There's next and previous pointers.

00:12:39.160 --> 00:12:41.310
Don't dwell too much on this.

00:12:42.060 --> 00:12:45.380
Here's what the API looks like.

00:12:45.450 --> 00:12:48.110
So let's say the scenario is your
boss wants you to create a linked

00:12:48.110 --> 00:12:50.610
list class and you work really
hard and write the linked list

00:12:50.610 --> 00:12:52.000
and here's the API you create.

00:12:52.000 --> 00:12:54.800
As you can see,
it's simple and efficient.

00:12:54.800 --> 00:12:59.700
By efficient, I mean the whole API here,
first node, node after node, insert node,

00:12:59.700 --> 00:13:02.250
remove node,
these are all off-one behaviors.

00:13:02.260 --> 00:13:03.240
They're constant time.

00:13:03.280 --> 00:13:07.900
No matter how big the linked list,
these operations will execute very fast.

00:13:07.900 --> 00:13:10.810
And of course, you have a node object
which just has a value.

00:13:10.830 --> 00:13:13.820
It carries the value that's
inserted at a certain location.

00:13:13.920 --> 00:13:15.850
Pretty straightforward.

00:13:16.540 --> 00:13:16.920
Okay.

00:13:17.170 --> 00:13:21.180
So, let's say you write this,
give it to your boss and

00:13:21.180 --> 00:13:25.210
your boss says great,
but I also want indexed access.

00:13:25.210 --> 00:13:28.060
I would love to have a
node at index method.

00:13:28.060 --> 00:13:31.180
Now, of course,
you realize this is an O of N method

00:13:31.180 --> 00:13:35.140
meaning it will -- its execution time
depends on the length of the linked list.

00:13:35.440 --> 00:13:37.190
Here's what the
implementation looks like.

00:13:37.340 --> 00:13:41.280
You start from the first node and
you iterate with that while loop

00:13:41.280 --> 00:13:43.710
until you get to the node you want.

00:13:43.860 --> 00:13:45.380
So, you know, your heart sinks.

00:13:45.400 --> 00:13:46.800
It's like this is a crappy method.

00:13:46.910 --> 00:13:48.550
The rest of the API was so good.

00:13:48.570 --> 00:13:49.860
What can you do?

00:13:49.860 --> 00:13:53.360
And let's go take a look
at this behavior here.

00:13:53.390 --> 00:13:55.470
A demo machine,

00:14:00.610 --> 00:14:04.260
Okay, so what we're doing
here is we're basically,

00:14:04.410 --> 00:14:07.000
our analysis in this case is
to run through the linked list

00:14:07.340 --> 00:14:10.440
starting at some location and
through incrementing locations.

00:14:10.440 --> 00:14:12.370
So our threads are going to
be running through the linked

00:14:12.370 --> 00:14:13.520
list just querying for values.

00:14:13.520 --> 00:14:16.820
And at every location we put in
an S number with the same value

00:14:16.820 --> 00:14:20.020
so we can also detect failures
if there happen to be any.

00:14:20.440 --> 00:14:22.050
So let's go ahead and run this.

00:14:22.050 --> 00:14:25.650
We have our linked list
test with 1,000 objects.

00:14:26.020 --> 00:14:28.400
If we run it,
you can see that we're going

00:14:28.400 --> 00:14:30.970
to get about a million,
1.1 million updates.

00:14:31.040 --> 00:14:35.240
But if we go up to 100,000 objects,
you see that the number of updates

00:14:35.240 --> 00:14:38.540
we can do goes down drastically
because it is all end behavior.

00:14:38.540 --> 00:14:41.440
So we're only getting 33,000
updates and that's really bad.

00:14:41.440 --> 00:14:43.600
You may get fired for this.

00:14:43.600 --> 00:14:47.890
So if we can switch back to the slides,
please.

00:14:48.110 --> 00:14:49.720
So you get an optimization idea.

00:14:49.840 --> 00:14:53.600
What if you saved away the most recent
access and reuse it if possible?

00:14:53.750 --> 00:14:55.170
This is a common usage pattern.

00:14:55.280 --> 00:15:01.430
Often, you know, things are accessed
sequentially or in clumps.

00:15:01.430 --> 00:15:01.430
So this might actually help.

00:15:01.970 --> 00:15:03.300
So what do I mean here?

00:15:03.300 --> 00:15:05.540
Here's what node-index looks like.

00:15:05.540 --> 00:15:08.340
You sort of make some room,
and then you go ahead

00:15:08.340 --> 00:15:09.720
and insert some code.

00:15:09.720 --> 00:15:13.910
You have this get cached index method,
which I'll show you in a second.

00:15:14.040 --> 00:15:16.780
It gets you the last node in node-index.

00:15:16.780 --> 00:15:19.530
And then if the node is nil or
the index is not applicable,

00:15:19.630 --> 00:15:22.460
you go ahead and fall back to
the slow way of doing things,

00:15:22.530 --> 00:15:24.510
meaning looking from the beginning.

00:15:24.830 --> 00:15:27.190
Otherwise,
you can look from this interim location,

00:15:27.190 --> 00:15:28.630
which is closer to your goal.

00:15:28.710 --> 00:15:30.560
And then you run through
the while loop again.

00:15:30.560 --> 00:15:33.700
If the optimization matches,
the while loop will be much faster.

00:15:33.700 --> 00:15:36.710
And then if you did find a result,
before you return it,

00:15:36.710 --> 00:15:40.340
you'll go ahead and set cached index,
meaning you set that as the new

00:15:40.340 --> 00:15:42.340
result for the next time around.

00:15:45.110 --> 00:15:48.360
Here's what set cache index
and get cache index look like.

00:15:48.360 --> 00:15:52.570
You add two new instance variables,
cache index and cache node,

00:15:52.570 --> 00:15:56.000
and these methods set and
retrieve those values.

00:15:56.000 --> 00:15:59.610
We'll see why we just added one
method here instead of two distinct

00:15:59.640 --> 00:16:01.960
properties like we traditionally do.

00:16:02.120 --> 00:16:05.920
Let's see the behavior
with this new optimization.

00:16:08.480 --> 00:16:10.560
So running the linked list with cache,
and I'm just going to run

00:16:10.560 --> 00:16:12.900
the 100,000 element case.

00:16:12.930 --> 00:16:15.490
Remember, we're getting 33,000.

00:16:15.590 --> 00:16:16.000
There you go.

00:16:16.000 --> 00:16:17.240
Now we're getting 30 million.

00:16:17.240 --> 00:16:20.100
We speed it up by, what,
1,000x right there.

00:16:20.170 --> 00:16:20.640
So that's great.

00:16:20.670 --> 00:16:23.780
And of course, you're thinking,
if I run it two threads,

00:16:23.780 --> 00:16:25.160
it'll be even better.

00:16:26.280 --> 00:16:29.860
It is running pretty fast,
but notice the failure rate.

00:16:30.040 --> 00:16:31.350
There is a failure rate here.

00:16:31.350 --> 00:16:33.790
And again,
given the topic we've been talking about,

00:16:33.800 --> 00:16:40.470
it's due to lack of synchronization
on those two properties we're setting.

00:16:40.470 --> 00:16:40.470
So if we go back to the slides.

00:16:44.590 --> 00:16:46.160
So there's a lack of synchronization.

00:16:46.210 --> 00:16:48.620
While one thread is setting,
now by the way,

00:16:48.620 --> 00:16:51.190
I didn't describe the scenario here.

00:16:51.200 --> 00:16:53.580
But we mutate the linked list up front.

00:16:53.720 --> 00:16:55.030
And we don't even mutate it anymore.

00:16:55.050 --> 00:16:56.590
We're just fetching values out of it.

00:16:56.610 --> 00:17:00.630
So just the act of fetching the values
is of course setting the cache index.

00:17:00.790 --> 00:17:03.920
And as one thread is setting
the cache index and node,

00:17:03.920 --> 00:17:06.960
and another thread is reading it,
they're getting inconsistent results.

00:17:07.020 --> 00:17:09.550
They happen to catch it halfway.

00:17:09.630 --> 00:17:11.020
So we need synchronization.

00:17:11.050 --> 00:17:15.230
Note that having the combined API set
in get prepares us for adding that

00:17:15.240 --> 00:17:16.940
synchronization fairly easily.

00:17:16.980 --> 00:17:18.740
Let's see how that works.

00:17:18.810 --> 00:17:21.280
First we're going to just do the
traditional locking solutions.

00:17:21.340 --> 00:17:23.260
One is to use addSynchronized,
which is an

00:17:23.260 --> 00:17:26.000
Objective-C synchronization mechanism.

00:17:26.040 --> 00:17:27.090
It's fairly simple.

00:17:27.260 --> 00:17:29.520
You go into your set,
you go into your get,

00:17:29.650 --> 00:17:33.150
and you add this addSynchronized
around the critical section.

00:17:33.260 --> 00:17:36.200
So by doing this,
you've guaranteed that the index and

00:17:36.200 --> 00:17:39.850
node will be set or retrieved together.

00:17:40.260 --> 00:17:41.560
No other initialization is needed.

00:17:41.660 --> 00:17:43.860
This is all you need to do.

00:17:43.930 --> 00:17:48.000
The other solution,
another solution is to use an NSLock.

00:17:48.740 --> 00:17:50.440
And again, the code looks very similar.

00:17:50.440 --> 00:17:54.030
You lock and unlock around
your critical section.

00:17:54.490 --> 00:17:56.320
In this case,
you have to declare cacheLock

00:17:56.330 --> 00:17:57.640
as an instance variable.

00:17:57.640 --> 00:17:59.940
You also have to initialize the
lock and you have to release the

00:17:59.940 --> 00:18:03.240
lock when you're done with it,
when your object is going away.

00:18:03.240 --> 00:18:04.940
There's a little more
initialization here.

00:18:05.040 --> 00:18:07.460
The third solution is to use a spin lock.

00:18:07.460 --> 00:18:10.920
We have spin locks at the POSIX layer.

00:18:10.960 --> 00:18:12.480
It's called OS spin lock.

00:18:12.690 --> 00:18:14.580
Again, the code looks very similar.

00:18:14.580 --> 00:18:17.100
Lock and unlock around
your critical region.

00:18:17.100 --> 00:18:19.100
In the case of spin lock,
you still have to declare it

00:18:19.100 --> 00:18:21.380
as an instance variable and
you have to initialize it,

00:18:21.540 --> 00:18:23.430
but it's not an object so
it actually does not need

00:18:23.510 --> 00:18:25.280
to be allocated or released.

00:18:25.280 --> 00:18:27.720
So, there's still initialization,
but a little less.

00:18:27.720 --> 00:18:30.680
So, if we were to go back to our demo.

00:18:32.650 --> 00:18:34.220
Let's see what performance we get.

00:18:34.380 --> 00:18:37.930
First, running with @synchronized.

00:18:41.210 --> 00:18:45.120
So we're getting about six
million of these a second.

00:18:45.370 --> 00:18:53.700
With NSLock, 10 million a second.

00:18:56.100 --> 00:19:13.300
[Transcript missing]

00:19:14.610 --> 00:19:18.070
So just to recap those three,
at synchronized is a recursive and

00:19:18.110 --> 00:19:20.000
exception safe mutual exclusion.

00:19:20.010 --> 00:19:21.860
It's very convenient as we saw.

00:19:21.860 --> 00:19:24.860
NSLock is your basic
Cocoa mutual exclusion lock.

00:19:24.860 --> 00:19:27.100
It's fairly efficient.

00:19:27.100 --> 00:19:32.200
What I mean by that is
when the lock is locked,

00:19:32.430 --> 00:19:35.120
the thread that's locked is
put into a sleeping state.

00:19:35.180 --> 00:19:39.000
On the other hand, with Spinlock,
it is also very efficient and fast,

00:19:39.000 --> 00:19:42.200
but when it's in a locked state,
it's actually looping,

00:19:42.220 --> 00:19:45.530
and hence why it's called Spinlock.

00:19:46.090 --> 00:19:48.790
So although it's the fastest,
it's also got sort of

00:19:48.790 --> 00:19:50.670
the most caveat as well.

00:19:50.700 --> 00:19:53.100
If I were to do an analogy,

00:19:54.540 --> 00:20:00.180
Imagine a traffic intersection and
there's red lights and a green light.

00:20:00.180 --> 00:20:02.220
And the cars at the red
light with OS spin lock,

00:20:02.220 --> 00:20:04.350
with NS lock,
the cars at the red light are just

00:20:04.350 --> 00:20:06.590
standing there with their engines idling.

00:20:06.590 --> 00:20:09.700
But the cars with spin lock are
actually sitting there with their

00:20:09.700 --> 00:20:11.660
engines revving as high as possible.

00:20:11.660 --> 00:20:13.500
And you can imagine that all
these cars are just stopped

00:20:13.500 --> 00:20:14.710
there with their engines running.

00:20:14.760 --> 00:20:17.040
It's really not good for your CPU,
not good for your computer.

00:20:18.340 --> 00:20:21.330
So you might be wondering, okay,
wise guy, what's the AdSynchronize

00:20:21.340 --> 00:20:22.790
like in this world of cars?

00:20:23.020 --> 00:20:25.700
Well,
the AdSynchronize is not a car you own.

00:20:25.720 --> 00:20:28.140
It's basically like a taxi or a
limo you hire once in a while.

00:20:28.140 --> 00:20:30.220
It's very convenient,
but you pay a lot for it.

00:20:30.220 --> 00:20:30.910
So there you go.

00:20:31.130 --> 00:20:32.100
Hopefully you'll notice.

00:20:32.100 --> 00:20:36.500
So the lesson here is
AdSynchronize is convenient,

00:20:36.500 --> 00:20:41.600
and if you must use spin lock,
make sure you only use the small

00:20:41.600 --> 00:20:47.290
bits of code where there's very
small likelihood of contention.

00:20:47.750 --> 00:20:50.460
I'm going to look at
two lockless techniques.

00:20:50.460 --> 00:20:52.340
The locking ones most
people have heard about.

00:20:52.340 --> 00:20:54.980
There are also some lockless
techniques we can apply.

00:20:54.980 --> 00:20:58.650
The two techniques I'm going to talk
about are just a truly lockless technique

00:20:58.650 --> 00:21:01.260
and one which is a caller managed cache.

00:21:01.730 --> 00:21:06.850
The lockless one is one where we
access and update the cache locklessly,

00:21:06.850 --> 00:21:09.370
but we detect and react
to inconsistencies.

00:21:09.570 --> 00:21:12.590
This is a tough one,
so I'm going to use some graphics.

00:21:12.600 --> 00:21:18.940
You might want to take a sip from your
coffee or lemonade or whatever you have.

00:21:18.940 --> 00:21:20.660
Let's see what happens.

00:21:20.700 --> 00:21:26.650
You have two variables,
your node and your index that

00:21:26.650 --> 00:21:26.650
you want to update atomically.

00:21:26.850 --> 00:21:29.020
So we put four additional values.

00:21:29.180 --> 00:21:30.300
Let's put these in the stack.

00:21:30.410 --> 00:21:31.640
It doesn't matter.

00:21:31.640 --> 00:21:32.440
These are not IVARs.

00:21:32.440 --> 00:21:33.960
These are just on the stack.

00:21:33.960 --> 00:21:38.170
And then we take the high word of
your node and we put it into the high

00:21:38.310 --> 00:21:40.740
word of that first word we created.

00:21:40.740 --> 00:21:43.090
Then we take the low word of
your node and we put it into

00:21:43.090 --> 00:21:44.620
the high word of the second.

00:21:44.700 --> 00:21:46.630
And then we do the same with your index.

00:21:46.630 --> 00:21:50.480
We put it into the high words
of the remaining two words.

00:21:50.480 --> 00:21:54.050
Now we have another element,
which is a generation count.

00:21:54.050 --> 00:21:56.260
And this is really a half word size.

00:21:56.340 --> 00:21:59.760
This generation count is the
only thing that's incremented

00:21:59.760 --> 00:22:01.720
atomically in this scenario.

00:22:01.720 --> 00:22:05.420
Whenever a thread wants to use it,
it does OS atomic increment 32

00:22:05.420 --> 00:22:07.500
and it gets back a unique value.

00:22:07.500 --> 00:22:10.950
So each thread gets back
a unique value for this.

00:22:11.350 --> 00:22:16.700
And then the gen count is placed
into the low word of all the words.

00:22:17.400 --> 00:22:20.240
So now we have these words.

00:22:20.240 --> 00:22:22.250
We copy these into the cache.

00:22:22.250 --> 00:22:25.700
And by cache here,
I mean the instance variable.

00:22:25.700 --> 00:22:28.960
These four words are now
in your instance variables.

00:22:29.140 --> 00:22:32.040
So these four words are
shared by all the threads.

00:22:32.040 --> 00:22:34.350
So note that when we copied
those four words into here,

00:22:34.350 --> 00:22:37.260
we didn't worry about synchronization,
locking, anything.

00:22:37.260 --> 00:22:39.810
We just copied them in there,
which means some other thread might

00:22:39.810 --> 00:22:42.920
have been copying at the same time,
and these values might be all messed up.

00:22:44.010 --> 00:22:46.490
However,
when time comes to read these values,

00:22:46.490 --> 00:22:48.560
we copy them all to local storage.

00:22:48.560 --> 00:22:52.260
And then the thing we do is we check
to see if the gen counts are the same.

00:22:52.260 --> 00:22:55.590
If the gen counts are all the same,
that means that this value,

00:22:55.590 --> 00:22:58.210
this cache values,
were not corrupted as a result

00:22:58.220 --> 00:23:00.020
of all that copying and such.

00:23:00.020 --> 00:23:03.410
So if the values are the same,
the cache is good, we can reassemble

00:23:03.410 --> 00:23:05.050
Node.NET Index and profit.

00:23:05.050 --> 00:23:05.990
That's great.

00:23:06.040 --> 00:23:08.260
And note that if the
gen counts don't match,

00:23:08.260 --> 00:23:09.520
we just dump the cache.

00:23:09.520 --> 00:23:10.420
We don't use it.

00:23:10.420 --> 00:23:12.040
We go back and do the slow way.

00:23:12.040 --> 00:23:13.600
So the way out here is
actually pretty easy.

00:23:13.620 --> 00:23:15.090
simple.

00:23:15.200 --> 00:23:18.480
The other lockless approach we're going
to do is call or manage the cache.

00:23:18.480 --> 00:23:20.680
This is one,
actually this is fairly simple.

00:23:20.710 --> 00:23:22.830
The cache is owned and
provided by the caller.

00:23:22.830 --> 00:23:25.890
In this case,
instead of know that index as our API,

00:23:25.890 --> 00:23:30.080
we're going to go ahead and use an
API like know that index using cache.

00:23:30.080 --> 00:23:33.990
And note that the using cache is an
argument which is a struct pointer

00:23:33.990 --> 00:23:38.120
and the caller is expected to provide
a cache where they provide us with

00:23:38.270 --> 00:23:40.460
the results from the previous call.

00:23:40.460 --> 00:23:43.190
So this is,
this means that we're just letting

00:23:43.190 --> 00:23:45.600
the caller manage the cache.

00:23:45.600 --> 00:23:46.700
Okay.

00:23:46.700 --> 00:23:49.850
In this case,
no locks or synchronization are needed.

00:23:49.860 --> 00:23:52.610
So if we can switch to the demo machine,
please, and let's see what these

00:23:52.680 --> 00:23:54.600
last two techniques do.

00:23:54.600 --> 00:23:54.810
Okay.

00:23:54.910 --> 00:23:58.860
So remember with the spin lock
we're getting about 20 million.

00:23:58.870 --> 00:24:00.210
Our lockless solution

00:24:02.400 --> 00:24:04.060
Okay, that's getting us 25 million.

00:24:04.060 --> 00:24:05.140
We're doing a little better.

00:24:05.310 --> 00:24:09.140
And let's go ahead and look
at the caller supplied cache.

00:24:09.980 --> 00:24:11.400
This one is actually the big winner.

00:24:11.400 --> 00:24:14.740
This one is getting us 93 million
because it totally avoids all

00:24:14.740 --> 00:24:16.800
locks and synchronization.

00:24:17.190 --> 00:24:18.730
There's one more advantage.

00:24:18.730 --> 00:24:22.160
Now note that all the other solutions are
writing into the same memory locations.

00:24:22.160 --> 00:24:24.170
This one has much less of that.

00:24:24.220 --> 00:24:28.000
This one has none of that because
all the caches are per caller.

00:24:28.000 --> 00:24:31.360
So with this solution,
if I were to switch to eight cores,

00:24:31.690 --> 00:24:34.960
instead of 93 million,
we're actually going to probably

00:24:34.960 --> 00:24:37.090
see close to 400 million.

00:24:37.100 --> 00:24:39.870
So we are actually
getting a good increment.

00:24:39.900 --> 00:24:44.680
So again, eliminating contention,
working as locally as possible

00:24:44.680 --> 00:24:47.880
are the good lessons out of this.

00:24:47.880 --> 00:24:53.440
So if we can switch back to the slides,
there are some caveats, of course.

00:24:53.440 --> 00:24:55.550
The lockless solutions are hard.

00:24:55.550 --> 00:24:57.980
That first solution I showed
was hard to explain.

00:24:57.980 --> 00:24:59.380
We needed a lot of graphics.

00:24:59.380 --> 00:25:00.440
We needed coffee.

00:25:00.440 --> 00:25:02.360
And the example we used was simple.

00:25:02.360 --> 00:25:05.330
We didn't even care about consistency
because if it's inconsistent,

00:25:05.330 --> 00:25:06.460
we just didn't touch it.

00:25:06.580 --> 00:25:08.690
But if you really cared about
those values being consistent,

00:25:08.690 --> 00:25:09.850
you have to do a lot more work.

00:25:09.900 --> 00:25:13.930
And most lockless approaches require
use of what are called memory barriers.

00:25:13.930 --> 00:25:15.300
We didn't even show that.

00:25:15.310 --> 00:25:18.310
Typically when you use locks,
you have these things called

00:25:18.310 --> 00:25:21.670
barriers that guarantee a certain
ordering to the reads and writes

00:25:21.670 --> 00:25:23.460
that the processor is doing.

00:25:23.460 --> 00:25:26.170
When you don't use locks,
you don't have your barriers

00:25:26.170 --> 00:25:28.950
and you might have to use
these barriers explicitly.

00:25:29.000 --> 00:25:30.970
So most lockless approaches use that.

00:25:31.100 --> 00:25:33.710
If you want to experiment or
implement a lockless approach,

00:25:33.790 --> 00:25:36.010
please be sure to read
about memory barriers.

00:25:36.010 --> 00:25:39.810
There's a function called OS memory
barrier on OS 10 that does this.

00:25:39.920 --> 00:25:43.530
The caller managed cache issue is
that it's inconvenient for the caller.

00:25:43.530 --> 00:25:46.240
We're putting the burden on the caller.

00:25:46.360 --> 00:25:49.540
And on mutation,
all caches need to be invalidated.

00:25:49.610 --> 00:25:51.490
In the first approach,
if there's a mutation,

00:25:51.490 --> 00:25:53.020
we can just invalidate the cache.

00:25:53.020 --> 00:25:57.230
But in the second approach,
all these callers have their own

00:25:57.230 --> 00:26:02.210
caches which somehow have to be
invalidated and that could be tough.

00:26:02.210 --> 00:26:02.210
But

00:26:02.650 --> 00:26:04.810
The cache is also not
shared between callers,

00:26:04.810 --> 00:26:06.970
which could be a good
thing and a bad thing.

00:26:07.040 --> 00:26:09.210
It's actually maybe a good thing,
like in our example,

00:26:09.210 --> 00:26:11.700
because if each caller has
their own access pattern,

00:26:11.920 --> 00:26:15.060
they will benefit from their
cache being just their own state.

00:26:15.070 --> 00:26:18.000
So the last one in this case
turns out to be the best,

00:26:18.000 --> 00:26:18.720
really.

00:26:19.430 --> 00:26:21.290
Let's talk about
multi-threading alternatives.

00:26:21.340 --> 00:26:23.770
So far we've just been
using NS threads explicitly,

00:26:23.860 --> 00:26:25.700
but there are some alternatives.

00:26:25.860 --> 00:26:29.280
One set of alternatives are just
not to use multi-threading at all,

00:26:29.280 --> 00:26:31.020
like use background processing.

00:26:31.020 --> 00:26:34.380
For instance, you can use NS notification
center with postman idle,

00:26:34.380 --> 00:26:37.580
which means in the main event loop,
whenever there are no events,

00:26:37.580 --> 00:26:39.160
you'll get notifications.

00:26:39.160 --> 00:26:41.670
You can use run loop observers,
you can use timers,

00:26:41.670 --> 00:26:44.780
perform select with object
after delay is also a favorite.

00:26:44.780 --> 00:26:48.380
These all allow you to do background
processing on the main thread.

00:26:49.500 --> 00:26:51.350
Of course,
these are not true concurrency.

00:26:51.590 --> 00:26:55.220
Your machine might have 64 cores in it
and 63 of them will be sitting idle,

00:26:55.220 --> 00:26:56.180
so it's not very good.

00:26:56.260 --> 00:26:59.490
But it is useful nonetheless,
and it does give you responsiveness.

00:26:59.510 --> 00:27:01.960
For instance,
the Cocoa text system uses background

00:27:01.960 --> 00:27:03.400
processing to lay out text.

00:27:03.400 --> 00:27:06.310
So when you open a large document,
although the document

00:27:06.310 --> 00:27:08.770
is still being laid out,
the user is still able to

00:27:08.770 --> 00:27:10.240
type and scroll and so on.

00:27:10.240 --> 00:27:13.600
And it can do this without using any sort
of locking because it knows everything

00:27:13.600 --> 00:27:15.120
is happening on the main thread.

00:27:16.040 --> 00:27:18.700
Now, for true concurrency,
without using threads,

00:27:18.860 --> 00:27:21.280
another solution is operation
and operation queue.

00:27:21.280 --> 00:27:24.670
We talked about these last year
and previous year because we

00:27:24.670 --> 00:27:26.620
introduced these in Leopard.

00:27:26.860 --> 00:27:30.920
These allow you to achieve concurrency
without explicit use of threads.

00:27:31.030 --> 00:27:35.130
And the system basically decides how
many threads to throw at the problem.

00:27:35.280 --> 00:27:39.150
And if the system is very busy,
you might get less threads, so on.

00:27:40.110 --> 00:27:42.760
NSOperation is an object
that encapsulates work units.

00:27:42.760 --> 00:27:45.080
You can specify priorities, dependencies,
etc.

00:27:45.110 --> 00:27:47.530
And Operation Queue manages operations.

00:27:47.530 --> 00:27:51.040
And you can enable specifying the
amount of desired concurrency.

00:27:51.040 --> 00:27:52.970
By default, you would not do this.

00:27:52.970 --> 00:27:57.500
And NSOperation Queue will use as many
threads to execute as many simultaneous

00:27:57.500 --> 00:28:00.180
operations as it decides is appropriate.

00:28:00.180 --> 00:28:02.430
But if you do specify
desired concurrency,

00:28:02.430 --> 00:28:06.180
like don't run more than four at once,
then it will use that as a limit.

00:28:07.200 --> 00:28:10.490
Now,
let's look at a special case of this.

00:28:10.760 --> 00:28:15.040
Which is using operation
queues for synchronization.

00:28:15.050 --> 00:28:17.940
And by this, I mean serial queues.

00:28:17.940 --> 00:28:19.520
And what's a serial queue?

00:28:19.560 --> 00:28:23.770
That's a queue which has a max
concurrent operation count of one.

00:28:23.950 --> 00:28:27.140
Meaning it will only run one
operation at any given time.

00:28:27.240 --> 00:28:32.450
And at first glance,
such a thing does not look very useful.

00:28:33.810 --> 00:28:36.580
What you might use this
for is something like this,

00:28:36.580 --> 00:28:38.300
where you have a model.

00:28:38.310 --> 00:28:41.010
It's either a model object
or it's an object graph.

00:28:41.010 --> 00:28:43.840
And then you slap on
it an operation queue.

00:28:43.840 --> 00:28:48.220
And then basically all accesses to this
model go through this operation queue.

00:28:48.220 --> 00:28:52.000
So all the changes you want to do are
basically operations in this model.

00:28:52.140 --> 00:28:56.550
And if this operation queue has a max
concurrent operation count of one,

00:28:56.720 --> 00:29:00.960
this means that the model does not
need any locking or synchronization,

00:29:00.970 --> 00:29:04.670
because it's only getting one
request at any given time.

00:29:05.190 --> 00:29:06.950
For instance,
one of these operations might say,

00:29:06.950 --> 00:29:08.680
set the first name and last name.

00:29:08.760 --> 00:29:11.090
And that will happen automatically,
and the model doesn't have

00:29:11.090 --> 00:29:11.920
to lock itself at all.

00:29:11.980 --> 00:29:14.970
Now, this looks interesting,
but you're thinking, you know,

00:29:14.970 --> 00:29:17.000
but we've eliminated concurrency here.

00:29:17.000 --> 00:29:18.790
We're not using multiple cores.

00:29:18.800 --> 00:29:22.650
Well, one thing that happens here is
that all the operations will

00:29:22.650 --> 00:29:24.700
be performed one after another.

00:29:24.700 --> 00:29:27.240
They may still be performed
on different threads.

00:29:27.240 --> 00:29:28.420
So that's important.

00:29:28.690 --> 00:29:30.160
That by itself is not a big win.

00:29:30.160 --> 00:29:33.720
But if you have many such
models in your application,

00:29:33.720 --> 00:29:35.600
then they all have their own queues.

00:29:35.600 --> 00:29:37.940
Suddenly,
you introduce concurrency in your

00:29:37.940 --> 00:29:41.570
application by having every model
be processed on a separate thread.

00:29:41.580 --> 00:29:44.600
So this is actually a
pretty powerful paradigm.

00:29:44.600 --> 00:29:47.240
It's not one we use in
Cocoa very much yet.

00:29:47.260 --> 00:29:51.000
If you go to session 419,
which is Doug Davidson's talk

00:29:51.000 --> 00:29:54.680
at 3 o'clock in Russian Hill,
he will actually show using this

00:29:54.680 --> 00:29:57.220
technique to optimize a program
that he will be demonstrating.

00:29:57.240 --> 00:29:58.660
stream.

00:30:00.680 --> 00:30:02.990
Okay,
now we're going to switch gears a bit

00:30:03.070 --> 00:30:05.710
and talk about computational complexity.

00:30:06.170 --> 00:30:09.620
And here, what I'm referring to is
the computational complexity

00:30:09.690 --> 00:30:11.620
of some foundation objects.

00:30:11.620 --> 00:30:16.480
It's good to know performance
characteristics of your APIs.

00:30:16.480 --> 00:30:18.550
For instance,
the linked list class from earlier,

00:30:18.800 --> 00:30:20.900
the original API was all of one.

00:30:20.900 --> 00:30:24.670
Everything was very fast,
and then you were forced

00:30:24.670 --> 00:30:27.540
to go add the slow method,
which is OFN,

00:30:27.540 --> 00:30:34.040
but then adding that cache provided all
of one behavior for some usage scenarios.

00:30:34.060 --> 00:30:38.940
Namely, the usage scenario of
incrementing through the items,

00:30:38.940 --> 00:30:41.870
perhaps, or accessing items in bulk.

00:30:43.660 --> 00:30:47.040
So before we talk about computational
complexity of foundation objects,

00:30:47.110 --> 00:30:49.560
let's talk about primitive APIs.

00:30:49.750 --> 00:30:53.700
What we refer to as primitive
APIs are the funnel points.

00:30:53.790 --> 00:30:57.130
These are usually the minimal
APIs to implement a new subclass.

00:30:57.230 --> 00:30:59.940
And this is not something
we do for all Cocoa objects,

00:30:59.950 --> 00:31:04.340
but the most widely used ones,
such as the collection, string data,

00:31:04.340 --> 00:31:07.990
et cetera, all applies to those objects.

00:31:08.330 --> 00:31:10.660
So these primitive APIs,
the interesting thing about them

00:31:10.680 --> 00:31:14.250
is they indicate to you the overall
computational complexity of that object

00:31:14.450 --> 00:31:19.880
and what the expected behaviors are.

00:31:20.300 --> 00:31:22.340
These are often listed in the base class.

00:31:22.430 --> 00:31:26.100
So if you were to open NSDictionary.h
and look at it carefully,

00:31:26.150 --> 00:31:29.150
you'll see that there is an interface
declaration for NSDictionary which

00:31:29.400 --> 00:31:30.540
only has three methods.

00:31:30.610 --> 00:31:34.200
So this looks like NSDictionary is
a class with just three methods.

00:31:34.200 --> 00:31:36.900
However, if you scroll down further,
you'll see the rest of the

00:31:36.900 --> 00:31:38.470
methods declared in categories.

00:31:39.080 --> 00:31:42.380
The interesting thing here is that
the first three methods are the

00:31:42.380 --> 00:31:45.440
ones you'd have to override in
a subclass if you want to create

00:31:45.440 --> 00:31:47.040
your own subclass on a dictionary.

00:31:47.040 --> 00:31:50.280
And all these other methods
are implemented in terms

00:31:50.590 --> 00:31:52.030
of these first three.

00:31:52.030 --> 00:31:55.400
And that's how it will just work.

00:31:56.630 --> 00:31:58.790
And these first three also
give you a hint of what the

00:31:58.790 --> 00:32:01.350
performance characteristics is.

00:32:01.360 --> 00:32:04.080
So count and object for
key and key numerator.

00:32:04.080 --> 00:32:07.110
Now, if you look at mutable dictionary,
you'll see that there are

00:32:07.110 --> 00:32:09.140
only two primitive methods,
remove object for key

00:32:09.250 --> 00:32:11.540
and set object for key,
pretty straightforward.

00:32:11.540 --> 00:32:15.530
And if you scroll further down,
you'll see many other methods

00:32:15.540 --> 00:32:18.370
that are extension methods,
meaning implemented in

00:32:18.390 --> 00:32:19.250
terms of the first two.

00:32:19.260 --> 00:32:22.580
So talking about the performance
characteristics of NSDictionary,

00:32:23.380 --> 00:32:27.080
those methods we saw, object for key,
set object for key,

00:32:27.080 --> 00:32:31.040
remove object for key,
these are all constant time and fast.

00:32:31.040 --> 00:32:34.850
And by constant time here,
I'm not saying O of 1 because big

00:32:34.850 --> 00:32:39.000
O purists will get mad at this,
but what I'm saying is it's fast.

00:32:39.000 --> 00:32:43.420
NSDictionary is a hashing collection,
and when the hash function is good,

00:32:43.440 --> 00:32:46.260
it's really a very fast collection,
of course.

00:32:46.260 --> 00:32:49.180
If you have an NSDictionary with
a bad hash value,

00:32:49.180 --> 00:32:53.000
the objects have bad hash values,
the performance really takes center.

00:32:53.160 --> 00:32:56.820
The axis becomes OFN.

00:32:56.880 --> 00:32:59.620
and edits are often squared.

00:32:59.620 --> 00:33:02.790
And what are bad hash functions?

00:33:02.920 --> 00:33:04.600
Well, a constant value

00:33:04.810 --> 00:33:09.560
No matter how interesting of a value,
a constant value is a bad idea.

00:33:09.620 --> 00:33:13.640
A random value is also a bad idea because
your NSDictionary will take the object,

00:33:13.640 --> 00:33:15.530
put it somewhere,
and when you ask for it,

00:33:15.530 --> 00:33:16.700
it won't be there anymore.

00:33:16.700 --> 00:33:18.700
So a random value is
actually it'll be there,

00:33:18.860 --> 00:33:20.960
but the dictionary will
look somewhere else.

00:33:20.960 --> 00:33:22.090
So that's not a good idea either.

00:33:22.160 --> 00:33:25.610
And here's one that looks better,
but it's also bad because

00:33:25.610 --> 00:33:29.050
we're taking a self-name,
presumably the object has some name,

00:33:29.050 --> 00:33:31.900
which is perhaps an NSString,
where somehow, for some reason,

00:33:32.270 --> 00:33:34.980
extracting just one character out of it,
and then we're multiplying

00:33:34.980 --> 00:33:37.060
by 10,000 for some reason.

00:33:37.060 --> 00:33:39.230
And again,
we could have used the whole name as

00:33:39.230 --> 00:33:42.590
the hash key or something based on that,
but instead we decide to just go

00:33:42.590 --> 00:33:46.070
use one character out of it and
sort of multiply it by 10,000,

00:33:46.070 --> 00:33:48.210
which gives us a limited set of values.

00:33:48.210 --> 00:33:49.560
That's not good either.

00:33:49.560 --> 00:33:52.140
So if you do implement
your own hash values,

00:33:52.160 --> 00:33:57.470
be sure to return dispersed hash values,
use as much of your state as possible.

00:33:57.780 --> 00:34:02.240
One other thing to remember is
that if two objects are isEqual,

00:34:02.250 --> 00:34:04.700
their hash must also be equal.

00:34:04.700 --> 00:34:09.490
So a piece of advice here is if you
ever implement isEqual on any object,

00:34:09.610 --> 00:34:12.160
go ahead and implement hash as well.

00:34:13.740 --> 00:34:16.790
NSRA is computational complexity.

00:34:16.920 --> 00:34:18.700
It's like a C array.

00:34:19.190 --> 00:34:23.300
Access is O of 1,
appending and removing at end are O of 1,

00:34:23.300 --> 00:34:26.010
replacing an object,
taking an object out and putting

00:34:26.070 --> 00:34:29.890
another one in its place is O of 1,
but inserting or deleting in the middle

00:34:29.890 --> 00:34:33.700
where some objects move out of there
and the rest have to slide is O of N.

00:34:33.850 --> 00:34:37.690
That's just like a C array.

00:34:37.690 --> 00:34:37.690
However,

00:34:38.090 --> 00:34:41.100
At very large sizes,
NSArray switches to a

00:34:41.100 --> 00:34:42.700
tree implementation.

00:34:43.030 --> 00:34:45.250
And the reason for this
is we find that very,

00:34:45.250 --> 00:34:48.420
very large, big,
monolithic arrays are quite unwieldy

00:34:48.420 --> 00:34:52.080
when you start having to reallocate
tens and hundreds of megabytes.

00:34:52.130 --> 00:34:53.880
So we switch to a tree implementation.

00:34:53.880 --> 00:34:56.580
And this is really truly
for very large sizes.

00:34:56.580 --> 00:34:59.190
At that point,
all operations become O log N,

00:34:59.190 --> 00:35:01.850
which is what a balanced
tree will give you.

00:35:02.900 --> 00:35:06.460
However, using the trick we
learned from linked list,

00:35:06.460 --> 00:35:08.570
we have a last access cache.

00:35:08.570 --> 00:35:12.520
And so for iterative,
sequential access and edits,

00:35:12.750 --> 00:35:16.580
operations are still
constant time on an NSArray.

00:35:17.960 --> 00:35:20.490
NSString is much like an NSArray.

00:35:20.540 --> 00:35:24.900
Access is O , appending and
removing are O , but edits

00:35:24.900 --> 00:35:29.540
in the middle are O . Again,
very similar.

00:35:29.540 --> 00:35:29.540
However,

00:35:30.100 --> 00:35:32.580
The string you get back
from the Cocoa text system,

00:35:32.580 --> 00:35:34.940
as you might know,
the Cocoa text system uses an

00:35:34.940 --> 00:35:36.580
NSString as its backing store.

00:35:36.580 --> 00:35:41.060
It does use a tree implementation, again,
for the same reasons we do an array,

00:35:41.110 --> 00:35:43.260
but in this case it uses it all the time.

00:35:43.260 --> 00:35:46.160
And again, there is a last access cached.

00:35:46.160 --> 00:35:49.760
And this actually fits very well
with the way a text system is used.

00:35:49.950 --> 00:35:52.770
Typically, users tend to click somewhere,
they type, type, type, type,

00:35:52.770 --> 00:35:55.200
then they click somewhere else,
they type, type, type.

00:35:55.220 --> 00:35:59.970
So the first click might be off log in,
but the rest of the accesses are fast.

00:36:00.000 --> 00:36:03.000
because they're all very localized
and take advantage of that cache.

00:36:05.060 --> 00:36:07.240
Let's talk about bulk operations.

00:36:07.300 --> 00:36:11.920
One way to speed up access to these
objects is to use bulk operations.

00:36:12.190 --> 00:36:14.990
Note that this doesn't fundamentally
change the performance behavior.

00:36:14.990 --> 00:36:17.000
If it's O of N, it's still O of N.

00:36:17.140 --> 00:36:20.170
But bulk operations
reduce the cost of access.

00:36:20.360 --> 00:36:26.900
It's like going to Costco and
getting a huge bag of rice

00:36:26.900 --> 00:36:26.950
rather than getting teeny little
bags of rice at your grocery.

00:36:27.490 --> 00:36:30.400
So one way to do this is to
call the highest level API.

00:36:30.420 --> 00:36:33.900
For instance, in NSString,
do not call character at index

00:36:33.900 --> 00:36:35.760
repeatedly to do something.

00:36:35.820 --> 00:36:39.420
Call method like getCharactersRange,
which allows you to return a

00:36:39.420 --> 00:36:42.100
number of characters with one call.

00:36:42.140 --> 00:36:43.640
Or you can do the higher level things.

00:36:43.640 --> 00:36:44.340
These are even better.

00:36:44.490 --> 00:36:47.960
hasPrefix or rangeOfString
to see if a string occurs

00:36:47.960 --> 00:36:50.560
anywhere in the other string,
or the freshly added

00:36:50.560 --> 00:36:52.250
enumerates lines using block.

00:36:52.260 --> 00:36:55.010
With one line,
you're doing this high-level call.

00:36:55.020 --> 00:36:58.720
You don't have to worry about characters,
line endings, et cetera.

00:36:58.720 --> 00:37:01.140
And with array,
instead of object at index,

00:37:01.140 --> 00:37:03.260
go ahead and call these
high-level methods,

00:37:03.260 --> 00:37:07.680
arrayByAddingObjects, objectsAtIndexes,
so on.

00:37:07.680 --> 00:37:11.380
And in Leopard, of course,
we added the new for in fast enumeration,

00:37:11.430 --> 00:37:15.180
which is very fast, very efficient,
and it's a great way to enumerate through

00:37:15.180 --> 00:37:17.890
arrays and some other collections.

00:37:19.500 --> 00:37:22.840
While we're on the topic of collections,
we're going to talk about concurrency.

00:37:22.840 --> 00:37:26.940
You've heard in other talks that
we've introduced block-based

00:37:26.940 --> 00:37:30.400
APIs for enumeration,
searching, and sorting.

00:37:30.400 --> 00:37:35.710
One thing these APIs allow you to
do is specify a concurrent flag.

00:37:36.430 --> 00:37:39.010
When you do that,
the operation can be done concurrently,

00:37:39.090 --> 00:37:41.290
meaning it can be done
on multiple processors.

00:37:41.460 --> 00:37:43.540
However,
the overall call is still synchronous.

00:37:43.870 --> 00:37:45.400
So I call sort.

00:37:45.610 --> 00:37:47.610
The sort happens,
and by the time it returns,

00:37:47.610 --> 00:37:48.470
the sort is done.

00:37:48.730 --> 00:37:51.970
But while it's happening,
an SRA might choose to use more

00:37:52.160 --> 00:37:53.840
than one core to do the job.

00:37:53.870 --> 00:37:58.450
Of course, what this requires is that
the block you're providing,

00:37:58.450 --> 00:38:02.050
like the compare function
or whatever you're doing,

00:38:02.050 --> 00:38:02.920
is thread safe.

00:38:02.920 --> 00:38:02.920
And that's really the only requirement.

00:38:04.160 --> 00:38:07.000
Here is a sorting API,
just to take one example.

00:38:07.160 --> 00:38:10.860
Sorted array with
options using comparator.

00:38:10.860 --> 00:38:15.700
One of the options you can pass is
that highlighted sort concurrent flag.

00:38:15.700 --> 00:38:17.530
This is how you use

00:38:19.650 --> 00:38:23.000
Let me show you what
gains we're seeing here.

00:38:23.110 --> 00:38:28.080
One warning,
our implementation is pretty preliminary,

00:38:28.170 --> 00:38:30.080
it's very fresh,
and the measurements are also

00:38:30.130 --> 00:38:31.310
just very quick measurements.

00:38:31.380 --> 00:38:34.600
I just want to give you an idea
of what sort of wins we see.

00:38:34.600 --> 00:38:37.780
In the bottom you see
the number of cores,

00:38:37.780 --> 00:38:40.910
and along the side we're seeing speed up.

00:38:41.530 --> 00:38:46.000
With 4,000 objects,
an array that has 4,000 objects,

00:38:46.110 --> 00:38:49.510
you'll note that with two cores,
we're actually seeing

00:38:49.540 --> 00:38:51.030
2x this performance.

00:38:51.100 --> 00:38:52.300
So that's pretty good.

00:38:52.300 --> 00:38:59.090
But with eight cores,
we're seeing only 2.4x the performance.

00:38:59.090 --> 00:39:03.090
And that's because with eight cores,
the overhead of divvying up

00:39:03.090 --> 00:39:03.090
the thing into eight cores sort
of overwhelms the wins we see.

00:39:03.970 --> 00:39:06.840
With 128,000 objects,
you see that the curve

00:39:06.870 --> 00:39:07.780
is a little better.

00:39:08.030 --> 00:39:11.210
At 4 processors, we're seeing 3x.

00:39:11.520 --> 00:39:14.760
With 8 processors,
we're seeing 4.x speedup.

00:39:15.050 --> 00:39:18.060
With 4 million objects,
the curve is slightly lower.

00:39:18.060 --> 00:39:22.310
However, we are still seeing a
3.5x win for 8 processors.

00:39:22.990 --> 00:39:24.500
Again, very preliminary numbers.

00:39:24.500 --> 00:39:26.360
We're going to be fine-tuning these.

00:39:26.360 --> 00:39:29.300
And NSRA will actually not do
the job concurrently if it thinks

00:39:29.300 --> 00:39:30.980
it's not going to be a win.

00:39:30.980 --> 00:39:34.050
However, this is a great way to
sort of get up to 4,

00:39:34.050 --> 00:39:37.940
5, maybe even more speedup pretty easily,
assuming your sort

00:39:38.030 --> 00:39:39.760
function is concurrent.

00:39:42.420 --> 00:39:45.660
Okay, switching gears again,
we're now going to talk about memory.

00:39:45.780 --> 00:39:49.700
You can have whole
WWDC sessions about memory.

00:39:49.700 --> 00:39:52.540
In fact, in the past, we have given whole
sessions about memory.

00:39:52.720 --> 00:39:56.260
Often, the thing we say,
the conventional wisdom,

00:39:56.260 --> 00:39:59.660
and that's still very applicable,
is don't use too much memory.

00:39:59.660 --> 00:40:04.060
Memory is really the number one
performance problem in most applications.

00:40:04.160 --> 00:40:05.970
That is true.

00:40:06.340 --> 00:40:10.260
However,
we have this new thing called caching,

00:40:10.260 --> 00:40:14.130
which you've heard
about in other sessions.

00:40:16.710 --> 00:40:19.640
So the thing is if the
user has a lot of memory,

00:40:19.640 --> 00:40:20.600
why not use it?

00:40:20.910 --> 00:40:24.270
Just give it back quickly
if the system needs it.

00:40:24.360 --> 00:40:28.440
So that's sort of another
approach to using memory.

00:40:28.440 --> 00:40:30.270
And most apps already do this.

00:40:30.270 --> 00:40:33.760
One observation is that if you
run an app for a long time,

00:40:33.760 --> 00:40:38.150
and then you go look at the usage,
memory usage of the application,

00:40:38.230 --> 00:40:40.750
you might see that
maybe the app is using,

00:40:40.750 --> 00:40:42.650
say, 10 megabytes memory.

00:40:42.890 --> 00:40:46.210
You can quit the app, restart it,
get it back to the same state,

00:40:46.270 --> 00:40:48.760
and if you look at the amount
of memory the machine is using,

00:40:48.930 --> 00:40:52.870
you might often see that it's only using,
say, two, three megabytes of memory.

00:40:52.950 --> 00:40:56.590
So clearly, the machine -- the app
can be in the same state.

00:40:56.880 --> 00:40:58.870
However, in one case,
it had accumulated a lot of

00:40:58.900 --> 00:41:00.290
stuff throughout the runtime.

00:41:00.290 --> 00:41:03.390
And if you restart it,
it somehow gives that back up.

00:41:03.500 --> 00:41:06.360
So many apps already do this caching
as a way to speed up their operations.

00:41:06.570 --> 00:41:10.300
But one thing they don't know
is when to give up these caches.

00:41:10.400 --> 00:41:13.710
So caching is good because clearly if
the user is using your application and,

00:41:13.860 --> 00:41:17.100
you know, the machine has four,
eight gigabytes of memory,

00:41:17.160 --> 00:41:20.550
and the user is not doing anything else,
your app should be able to

00:41:20.550 --> 00:41:23.120
use a little extra memory as
a way to get some speed gain.

00:41:23.210 --> 00:41:24.900
However,
the trick is to give it back to the

00:41:24.900 --> 00:41:29.000
system if the user switches to another
app and starts doing other things.

00:41:29.000 --> 00:41:32.750
So to that end,
we've added this new class, NSCache.

00:41:33.470 --> 00:41:36.810
It's key value-based access to objects,
sort of like a NEST dictionary.

00:41:36.960 --> 00:41:39.160
It has a strong reference to the objects.

00:41:39.260 --> 00:41:46.230
And what happens is if there's memory
pressure or some other settings match up,

00:41:46.410 --> 00:41:49.640
the cache will evict the objects,
which means it will release them,

00:41:49.640 --> 00:41:52.000
and they are then evicted from the cache.

00:41:52.110 --> 00:41:54.710
And let me just show you the API now.

00:41:55.920 --> 00:42:01.160
There's an object for key,
so the objects are fetched using keys.

00:42:01.160 --> 00:42:04.130
You can set an object for a key,
of course, just like a dictionary.

00:42:04.130 --> 00:42:06.520
And you can set an object
for a key using a cost.

00:42:06.530 --> 00:42:08.790
And this is the cost of existence.

00:42:08.870 --> 00:42:11.960
So, higher the cost,
higher the object will be

00:42:11.960 --> 00:42:13.280
evicted from the cache.

00:42:13.280 --> 00:42:16.320
If you use the first method,
the cost is assumed to be zero.

00:42:16.550 --> 00:42:18.880
And this cost is, you know,
you choose your units,

00:42:18.880 --> 00:42:20.080
whatever they might be.

00:42:20.080 --> 00:42:22.330
And there's, of course,
ways to remove it.

00:42:22.420 --> 00:42:25.540
And we have two other sets of methods,
two other properties,

00:42:25.540 --> 00:42:27.730
cost limit and count limit.

00:42:27.740 --> 00:42:29.760
By setting these,
you're setting approximate limits

00:42:29.760 --> 00:42:32.650
that you'd like the cache to follow.

00:42:32.960 --> 00:42:34.120
Now, these are very approximate.

00:42:34.120 --> 00:42:38.120
Just because you set a cost limit of 50,
it doesn't mean the cache will wait until

00:42:38.240 --> 00:42:40.640
50 before it starts evicting things.

00:42:40.640 --> 00:42:44.830
Another thing it will mean -- another
thing it doesn't mean is the cache

00:42:44.930 --> 00:42:47.080
will not always maintain a cost of 50.

00:42:47.240 --> 00:42:49.650
You know, for instance,
there might be 50 objects

00:42:49.650 --> 00:42:52.200
in there and you go back,
you go away, come back,

00:42:52.200 --> 00:42:55.270
it might turn out the objects
have been evicted in the meantime.

00:42:55.280 --> 00:42:57.430
So, this is not setting a limit,
an exact limit.

00:42:57.430 --> 00:42:59.920
This is just setting a
boundary and it's pretty rough.

00:42:59.920 --> 00:43:02.600
So, let's just see a quick demo of this.

00:43:02.600 --> 00:43:05.870
We can switch back to demo machine.

00:43:10.320 --> 00:43:14.330
I have a simple application
here which shows images and it

00:43:14.330 --> 00:43:19.100
fetches the images from a folder,
a file stem location.

00:43:19.210 --> 00:43:24.000
This method here is the one,
let's just take a look at it.

00:43:24.010 --> 00:43:28.040
This method returns the image
associated with every file.

00:43:28.040 --> 00:43:33.070
This image is fetched from a cache,
an NS cache image path.

00:43:33.240 --> 00:43:35.490
If it is found, it's returned, great.

00:43:35.510 --> 00:43:38.050
If not,
we'll go ahead and create the NS image

00:43:38.050 --> 00:43:40.120
and we insert it into the cache.

00:43:40.300 --> 00:43:43.760
The fact that we are going into
our cache to fetch it every time,

00:43:43.890 --> 00:43:46.350
of course,
informs the cache that there was

00:43:46.350 --> 00:43:49.920
interest in this image and it
allows the cache to update its most

00:43:49.920 --> 00:43:52.200
recently used statistics and so on.

00:43:52.300 --> 00:43:54.700
So anyway,
that gives the idea of the cache which

00:43:54.710 --> 00:43:56.920
images are more popular than the others.

00:43:56.920 --> 00:44:00.410
So let's go ahead and
run this application.

00:44:02.490 --> 00:44:05.040
So one thing you'll notice here
is the background is turning red.

00:44:05.160 --> 00:44:09.630
Not very good UI,
but it's good for a demo here.

00:44:09.770 --> 00:44:12.070
The idea is that as
these images are fetched,

00:44:12.070 --> 00:44:14.410
if they're not in the cache,
we highlight the background to

00:44:14.410 --> 00:44:16.000
show that there was a cache miss.

00:44:16.000 --> 00:44:18.830
So if I scroll,
every time a new image is revealed,

00:44:18.830 --> 00:44:20.630
the background flashes red.

00:44:20.650 --> 00:44:24.880
So you can see that it's also a bit pokey
here because it's loading these images.

00:44:24.880 --> 00:44:28.000
But if I were to scroll up,
you can see it's much faster until we

00:44:28.000 --> 00:44:32.100
get to the top because those images at
the top have been evicted from the cache.

00:44:32.420 --> 00:44:36.210
Again, you go all the way to the bottom,
we're seeing a lot of red, and then red,

00:44:36.210 --> 00:44:36.820
red, red.

00:44:36.830 --> 00:44:38.920
But if we were to go back
to the region we're in,

00:44:38.920 --> 00:44:42.670
again, it turns out these were evicted,
but those aren't, and so on.

00:44:42.680 --> 00:44:44.280
So it's all automatically being handled.

00:44:44.280 --> 00:44:47.080
I think in this demo,
we're setting a cache count limit of 40,

00:44:47.080 --> 00:44:49.860
and if you don't set the limit,
then the system will decide it.

00:44:49.940 --> 00:44:52.800
And also note that even if all
these images might be cached,

00:44:52.800 --> 00:44:55.320
and as I said,
you go away and come back an hour later,

00:44:55.390 --> 00:44:58.110
they might all be evicted in
the meantime since the system

00:44:58.110 --> 00:45:00.080
was using memory in other ways.

00:45:01.970 --> 00:45:04.670
Okay, if we can switch back to slides.

00:45:10.470 --> 00:45:17.820
Another memory topic is this new
protocol called NSDiscardableContent.

00:45:18.100 --> 00:45:21.180
This is a new protocol,
and it allows arbitrary

00:45:21.210 --> 00:45:26.890
objects to automatically free
their memory when not at use.

00:45:27.390 --> 00:45:30.620
When an object responses protocol,
to access its contents,

00:45:30.690 --> 00:45:32.670
whatever it may be,
it'll be different things

00:45:32.710 --> 00:45:36.500
for different objects,
you call this method beginContentAccess.

00:45:36.520 --> 00:45:41.310
And beginContentAccess pins the contents
down and it's sort of like a count.

00:45:41.350 --> 00:45:44.190
You're using it.

00:45:44.230 --> 00:45:46.960
If this returns no,
that means the contents had gone

00:45:46.960 --> 00:45:49.060
away and it could not be pinned down.

00:45:49.060 --> 00:45:51.000
So the return value is important.

00:45:51.000 --> 00:45:55.580
And when you're done using that memory,
you just call end content access.

00:45:55.580 --> 00:45:58.130
And these are used in a paired fashion,
and there might, of course,

00:45:58.200 --> 00:46:01.510
be multiple of these in existence,
sort of like retain release.

00:46:01.520 --> 00:46:05.890
There might be multiple people interested
in the contents of a given object.

00:46:06.630 --> 00:46:08.720
If an object's contents
are not being accessed,

00:46:08.730 --> 00:46:14.340
it's free to go ahead
and free it at any time.

00:46:14.830 --> 00:46:17.130
We added a new class
called NSPurgableData,

00:46:17.230 --> 00:46:21.040
and this is a subclass,
a concrete subclass of mutable data,

00:46:21.220 --> 00:46:25.000
and it implements the
discardable content protocol.

00:46:25.500 --> 00:46:29.190
The interesting thing about
NS Purgeable data is that it manages

00:46:29.190 --> 00:46:32.400
this thing called purgeable memory.

00:46:32.470 --> 00:46:37.010
Purgeable memory is a special variant
of malloc memory which the kernel can

00:46:37.010 --> 00:46:39.500
simply take back if it's not in use.

00:46:39.500 --> 00:46:41.800
The kernel is very sneaky about it.

00:46:41.800 --> 00:46:43.300
It'll just take it back.

00:46:43.300 --> 00:46:44.300
It won't even tell you.

00:46:44.300 --> 00:46:45.640
You won't hear about it.

00:46:45.780 --> 00:46:49.080
So it's like a good pickpocket trick,
very stealthy.

00:46:49.140 --> 00:46:52.390
You wake up in the morning, oh,
where are my keys?

00:46:52.390 --> 00:46:53.440
They're gone.

00:46:53.440 --> 00:46:55.220
Well, the kernel took it.

00:46:55.570 --> 00:46:58.350
And this is, of course,
very nice because the last thing

00:46:58.410 --> 00:47:01.520
you need when your system is
running low on memory is for all the

00:47:01.520 --> 00:47:04.410
process to be woken up to be told,
free all your memory.

00:47:04.410 --> 00:47:07.430
And all those processes will
be stepping all over each other

00:47:07.430 --> 00:47:08.990
trying to free their memory.

00:47:08.990 --> 00:47:12.370
You just want to take the memory
away from them without even telling

00:47:12.370 --> 00:47:15.910
them and hope that when they wake up,
they notice the memory is gone.

00:47:15.910 --> 00:47:17.650
That's their problem.

00:47:17.650 --> 00:47:20.900
But anyway,
so that's what purgeable data does.

00:47:20.900 --> 00:47:25.480
It's basically to access the contents,
you would use the protocol I showed you.

00:47:25.500 --> 00:47:27.940
In fact,
let me just show you a usage pattern.

00:47:27.990 --> 00:47:30.550
Now, one interesting thing about
purgeable data is that it starts

00:47:30.550 --> 00:47:32.730
off with the content access enabled.

00:47:32.950 --> 00:47:35.930
So when you first create it,
it's like as if you've

00:47:35.930 --> 00:47:37.820
called begin content access.

00:47:38.130 --> 00:47:40.990
So here's a common usage
or maybe not common,

00:47:40.990 --> 00:47:42.060
but we'll see.

00:47:42.060 --> 00:47:44.290
But anyway, one usage pattern.

00:47:44.290 --> 00:47:45.680
You have your data.

00:47:45.680 --> 00:47:47.140
It starts off nil.

00:47:47.850 --> 00:47:50.140
Let's say you haven't allocated it,
my data is nil,

00:47:50.160 --> 00:47:53.700
or else you have allocated it previously,
but you want to access its contents,

00:47:53.700 --> 00:47:56.270
so you call beginContentAccess,
and it returns no,

00:47:56.270 --> 00:47:59.360
meaning it's been purged,
the contents have been discarded.

00:47:59.360 --> 00:48:02.620
In this case,
I go ahead and free the data,

00:48:02.620 --> 00:48:06.170
if I had it,
and I go ahead and recreate it.

00:48:06.170 --> 00:48:10.200
And let's say we created it from
a file with contents of URL.

00:48:10.800 --> 00:48:13.570
Now, if it happens to fail,
something really went wrong,

00:48:13.570 --> 00:48:15.820
the data is no longer
around on the file system,

00:48:15.820 --> 00:48:17.740
so you return no or whatever you do.

00:48:17.740 --> 00:48:20.550
But if this succeeds,
then that means you now have the

00:48:20.580 --> 00:48:22.740
data and its contents are accessed.

00:48:22.740 --> 00:48:26.740
Now, in the case of NSData, of course,
the contents is the bytes.

00:48:26.740 --> 00:48:29.740
You go ahead and use the
bytes to your heart's content,

00:48:29.740 --> 00:48:31.740
whatever you want to do,
and when you're done,

00:48:31.740 --> 00:48:34.730
you call endContentAccess.

00:48:35.480 --> 00:48:37.620
Now,
next time you come back in this loop,

00:48:37.640 --> 00:48:40.840
you already have My Data,
so you call beginContentAccess,

00:48:40.840 --> 00:48:42.620
and if it returns yes,
you're in good shape.

00:48:42.720 --> 00:48:43.400
You just go use it.

00:48:43.400 --> 00:48:46.360
If it returns no, again,
you discard the data and

00:48:46.360 --> 00:48:48.190
you create it from scratch.

00:48:49.540 --> 00:48:55.580
NS Purgeable Data serves your primitive
Cocoa-level object for implementing

00:48:55.580 --> 00:49:00.210
the discardable content protocol,
if you wish.

00:49:00.210 --> 00:49:00.210
Of course,
you can do it any way you want.

00:49:01.610 --> 00:49:05.260
So let's talk about the connection
between cache and this discarding stuff.

00:49:05.260 --> 00:49:10.710
NSCache is aware of this protocol
and its discardable content.

00:49:11.530 --> 00:49:16.490
If you enable this property,
EvictsObjectsWithDiscardedContent,

00:49:16.630 --> 00:49:21.640
if NSCache notices that an object
inside it has discarded content,

00:49:21.640 --> 00:49:25.090
it will evict it from the
cache and return nil to you

00:49:25.110 --> 00:49:27.230
when you ask for the object.

00:49:27.300 --> 00:49:30.730
So what this gives you is,
typically when you're using

00:49:30.730 --> 00:49:34.270
discardable content protocol,
the object's contents go away,

00:49:34.270 --> 00:49:35.670
but the object is still alive.

00:49:35.700 --> 00:49:39.880
By putting such objects in the cache,
you're allowing the objects to go

00:49:39.930 --> 00:49:41.700
away when their contents go away.

00:49:41.700 --> 00:49:45.830
So it sort of automates the
process even a little further.

00:49:49.490 --> 00:49:52.240
Our last major topic is drawing.

00:49:52.240 --> 00:49:54.150
We have talked about drawing before.

00:49:54.200 --> 00:49:55.440
It's a fascinating topic.

00:49:55.440 --> 00:50:00.240
It's another major reason
why programs tend to be slow.

00:50:01.170 --> 00:50:04.240
We did say we were going to
talk about a lot of new stuff,

00:50:04.290 --> 00:50:07.190
but I just can't help
talk about old stuff.

00:50:07.280 --> 00:50:09.100
This is from 2001.

00:50:09.300 --> 00:50:16.650
It's a demo we showed
back in 2001 at WWDC.

00:50:16.650 --> 00:50:16.650
How many of you were here?

00:50:16.890 --> 00:50:18.770
Oh, that's about 10%. Great.

00:50:19.120 --> 00:50:22.170
Well, it's good to see you back.

00:50:22.700 --> 00:50:25.270
And all the new people, welcome.

00:50:25.540 --> 00:50:28.050
Okay, so let's go ahead and run Worm.

00:50:28.320 --> 00:50:30.200
So this is, yeah,
this is a fairly old app and has

00:50:30.410 --> 00:50:33.240
some good lessons about drawing
and we're going to show a few more

00:50:33.240 --> 00:50:34.700
lessons about drawing in conjunction.

00:50:34.700 --> 00:50:38.660
So let me go run this.

00:50:38.920 --> 00:50:41.100
And by the way, we ship this as an
example on your systems.

00:50:41.100 --> 00:50:42.130
We've been shipping it for seven years.

00:50:42.220 --> 00:50:43.700
It hasn't really changed very much.

00:50:43.760 --> 00:50:48.400
The worm game is, you know,
this game where you're a worm.

00:50:48.400 --> 00:50:50.890
You go around trying to eat this dot,
and every time you eat it,

00:50:50.890 --> 00:50:54.270
you get bigger, and the bigger you are,
the harder it is for you to move around.

00:50:54.270 --> 00:50:58.960
You might have seen this in movies
such as Tron or the Snake app on Unix.

00:50:58.960 --> 00:51:01.320
Anyway, it's pretty lame, really.

00:51:03.290 --> 00:51:05.600
One thing, though,
we do use this for is to

00:51:05.600 --> 00:51:06.880
show animation speeds.

00:51:07.200 --> 00:51:11.900
And worm has a bunch of modes here to
show you different animation techniques.

00:51:11.900 --> 00:51:14.950
And it's really showing drawing,
not animation, because really for true,

00:51:15.180 --> 00:51:18.030
fast animation these days, we have,
you know, core animation and so on.

00:51:18.050 --> 00:51:20.480
But let's go ahead and
run the slowest mode.

00:51:20.480 --> 00:51:23.370
And when you run it,
you'll notice that it's going ahead

00:51:23.370 --> 00:51:25.300
at a zippy 60 frames a second.

00:51:25.310 --> 00:51:30.460
And back in the day, it was running,
I think, 12, 15 frames a second.

00:51:30.460 --> 00:51:32.870
So we're already seeing a big win here,
this app.

00:51:32.920 --> 00:51:34.910
So let's go see what good does.

00:51:34.910 --> 00:51:38.060
It's also going along
at 60 frames a second.

00:51:38.060 --> 00:51:38.730
Better.

00:51:38.730 --> 00:51:42.030
That's also going along
at 60 frames a second.

00:51:42.030 --> 00:51:45.980
And it begins to dawn on you
there's something fishy here.

00:51:45.980 --> 00:51:48.550
And many of you will know
that this is because of what

00:51:48.560 --> 00:51:50.320
we call beam synchronization.

00:51:50.320 --> 00:51:55.300
And beam synchronization is the
system's attempt to make sure people

00:51:55.430 --> 00:51:58.380
don't draw faster than what's needed.

00:51:58.380 --> 00:52:02.380
And so that apps do not hog the display,
the screen, basically, which is,

00:52:02.380 --> 00:52:04.280
after all, a shared resource.

00:52:04.280 --> 00:52:06.620
Now, we can disable beam synchronization.

00:52:06.620 --> 00:52:09.610
This is a good thing to do just
for testing debugging purposes.

00:52:09.620 --> 00:52:13.130
And when we do that,
and if we go switch okay and run it,

00:52:13.130 --> 00:52:18.320
we see that worm is actually capable
of running at 490 frames a second.

00:52:18.320 --> 00:52:20.690
So, yeah, try playing that.

00:52:20.700 --> 00:52:24.140
I'm just going to enable the good mode.

00:52:24.140 --> 00:52:27.850
And in the good mode,
we're seeing 500 frames a second.

00:52:27.860 --> 00:52:33.840
The better mode, we're seeing 12, 1,000,
14.

00:52:33.840 --> 00:52:34.760
It's up there.

00:52:34.760 --> 00:52:36.330
And let's look at the best mode.

00:52:36.330 --> 00:52:40.120
And the best mode is giving us 13, 11,
et cetera.

00:52:40.120 --> 00:52:41.980
It's actually very similar
to the better mode.

00:52:42.000 --> 00:52:43.900
Now,
I won't show you the new mode right now,

00:52:43.900 --> 00:52:45.590
but let's talk about what we just saw.

00:52:45.610 --> 00:52:47.560
Go back to the slides, please.

00:52:54.650 --> 00:52:56.590
So what are the drawing tips from Wurm?

00:52:56.630 --> 00:52:58.960
What do those settings do?

00:52:58.960 --> 00:53:01.700
Well, the first setting is our classic.

00:53:01.770 --> 00:53:03.740
We say it once at every WWDC.

00:53:03.740 --> 00:53:06.160
Override is opaque to return yes.

00:53:06.160 --> 00:53:08.300
If your view draws
everything in its bounds,

00:53:08.320 --> 00:53:11.940
overriding is opaque assures that
views behind it are not drawn.

00:53:12.190 --> 00:53:13.770
So this is a big win.

00:53:13.840 --> 00:53:15.800
This is a good thing to do.

00:53:15.800 --> 00:53:18.850
The better mode basically gets us
to redraw as little as possible.

00:53:18.980 --> 00:53:20.780
And we'll see this in action soon.

00:53:20.780 --> 00:53:24.680
But instead of using set needs display,
which dirties your whole rec,

00:53:24.930 --> 00:53:28.760
just compute the smaller rec that
needs to be displayed and dirty that.

00:53:28.760 --> 00:53:30.350
So you call set needs display in rect.

00:53:30.410 --> 00:53:32.930
And then in your draw rect,
pay attention to the rect

00:53:32.930 --> 00:53:34.620
argument you've been given.

00:53:34.640 --> 00:53:36.380
This is again a classic
and it again helps.

00:53:36.380 --> 00:53:41.000
As you saw, this is what gets us more
than 1,000 frames a second.

00:53:41.000 --> 00:53:44.210
The best mode is not really
applicable these days.

00:53:44.260 --> 00:53:47.100
It used to be that instead
of using string drawing,

00:53:47.110 --> 00:53:49.810
methods such as draw string at point.

00:53:49.820 --> 00:53:57.320
If you use the text system,
the Cocoa text system directly

00:53:57.320 --> 00:54:01.280
and cache the way the components,
you could get huge wins.

00:54:01.280 --> 00:54:04.120
These days string drawing has
been speeded up so greatly that

00:54:04.120 --> 00:54:05.960
you often do not have to do this.

00:54:05.960 --> 00:54:08.120
So you use a text system
when you need a text system,

00:54:08.120 --> 00:54:12.440
but otherwise just use string drawing
and it will do the job most of the time.

00:54:12.440 --> 00:54:14.980
So it's not something you have to
go the extra mile for these days.

00:54:15.120 --> 00:54:17.520
But again,
if your string drawing seems a bit slow,

00:54:17.520 --> 00:54:19.760
this is something to consider.

00:54:19.760 --> 00:54:21.200
Now,
one other tip that we saw is that the

00:54:21.200 --> 00:54:24.180
system limits you to 60 frames a second.

00:54:24.340 --> 00:54:28.660
You should not ask to be drawn at
higher than 60 frames a second.

00:54:28.660 --> 00:54:32.250
In fact, if you do,
your program might actually stall waiting

00:54:32.250 --> 00:54:33.910
for the display to become available.

00:54:33.980 --> 00:54:38.350
So it might actually even have an adverse
effect on your program's execution.

00:54:38.420 --> 00:54:41.150
Now, let's switch back to the demo.

00:54:42.400 --> 00:54:45.560
And I want to show you the last mode.

00:54:45.560 --> 00:54:48.390
And the last mode is
running on this machine.

00:54:48.400 --> 00:54:51.400
It's again running 13,
consistently giving us 13.

00:54:51.400 --> 00:54:53.360
It's slightly faster than
some of the previous modes.

00:54:53.430 --> 00:54:54.740
It's actually consistently faster.

00:54:54.770 --> 00:54:57.790
The other one was jumping around 1,000,
1,200, so on.

00:54:57.900 --> 00:55:00.840
So, okay,
let me explain what this one is.

00:55:00.840 --> 00:55:05.030
Let me actually show you
what the okay mode does.

00:55:05.120 --> 00:55:08.250
This app, Quartz Debug,
with which we turned off Beam Sync,

00:55:08.310 --> 00:55:12.290
also has flash screen updates,
which shows every part of the

00:55:12.290 --> 00:55:15.080
screen that's been updated
by drawing operations.

00:55:15.200 --> 00:55:17.850
So in the okay mode,
you'll see that Worm is

00:55:18.240 --> 00:55:19.560
drawing the whole view.

00:55:19.560 --> 00:55:23.300
That yellow is a flash every
time we update that view.

00:55:23.340 --> 00:55:26.070
In the better mode,
you see that we actually take

00:55:26.220 --> 00:55:27.800
time to compute one rectangle.

00:55:27.800 --> 00:55:32.820
And in this new mode we added,
we actually update just the

00:55:32.820 --> 00:55:35.790
rectangles that are updating.

00:55:36.810 --> 00:55:39.880
So, you know, if I start playing here,
you can see that it's really

00:55:39.950 --> 00:55:41.300
doing a very nice job.

00:55:41.300 --> 00:55:44.140
Anyway,
so you can see that it's doing a very

00:55:44.140 --> 00:55:46.940
nice job of computing the update area.

00:55:47.000 --> 00:55:47.720
Okay.

00:55:47.840 --> 00:55:50.520
If you switch back to the slides.

00:55:51.280 --> 00:55:54.960
So to minimize drawing even further,
in drawRect, instead of using the single

00:55:54.960 --> 00:55:57.040
union rect you were given,
call these methods.

00:55:57.120 --> 00:55:59.100
GetRectsBeingDrawn, count.

00:55:59.240 --> 00:56:02.430
It returns to the currently
list of rects that are dirty.

00:56:02.570 --> 00:56:05.940
Or if you have a rect of yourself,
your own rect,

00:56:05.940 --> 00:56:10.900
you can call needsToDrawRect and
see if it's in the dirty region.

00:56:10.930 --> 00:56:13.400
And this gives you this nice
behavior of instead of that,

00:56:13.440 --> 00:56:15.090
it gives you that little region.

00:56:15.190 --> 00:56:17.500
So in this case, it helps.

00:56:17.500 --> 00:56:19.610
In other cases, it might help even more.

00:56:20.560 --> 00:56:23.570
Now,
more drawing tips that are good to know.

00:56:23.840 --> 00:56:24.960
Flashing screen updates.

00:56:24.960 --> 00:56:25.640
You just saw that.

00:56:25.640 --> 00:56:28.400
That's a very good way to
see if you're overdrawing.

00:56:28.580 --> 00:56:30.700
You can optimize live resize speeds.

00:56:30.700 --> 00:56:35.330
If your drawing speed seems fine,
but when the window is resized,

00:56:35.330 --> 00:56:39.840
it's a bit slow, you can check in live
resize and do some rougher,

00:56:39.840 --> 00:56:43.690
quicker drawing as a way
to make live resize faster.

00:56:43.690 --> 00:56:43.690
Ali Ozer

00:56:44.130 --> 00:56:46.500
You should eliminate
usages of NSCachedImageRep.

00:56:46.740 --> 00:56:51.460
This is now a mostly deprecated class.

00:56:51.460 --> 00:56:53.750
One thing we're doing is
we're eliminating window

00:56:53.750 --> 00:56:57.580
backing stores in NSImages,
and that is something that's

00:56:57.590 --> 00:56:58.600
happening automatically.

00:56:58.600 --> 00:57:01.290
But if you use
NSCachedImageRep for compatibility,

00:57:01.290 --> 00:57:04.490
we usually have no choice but
to use window backing stores.

00:57:04.590 --> 00:57:09.650
So it's a good idea to just get off
this and get off window backing stores,

00:57:09.650 --> 00:57:11.930
which gives NSImage huge
performance boost in many cases.

00:57:12.800 --> 00:58:23.100
[Transcript missing]

00:58:24.050 --> 00:58:26.490
There are other facilities
available for drawing.

00:58:26.830 --> 00:58:30.360
Quartz is what Cocoa uses
as the graphic substrate.

00:58:30.360 --> 00:58:34.450
It's used heavily by Cocoa objects,
but you can use it yourself

00:58:34.450 --> 00:58:36.280
directly if you need to.

00:58:36.480 --> 00:58:40.700
Core animation is also indirectly
used in NSView's animator,

00:58:40.870 --> 00:58:43.260
but you can use
standalone CA layer trees,

00:58:43.260 --> 00:58:44.750
assign them to views.

00:58:44.760 --> 00:58:47.560
Inside a view, you can have many,
many CA layers if you wish

00:58:47.590 --> 00:58:49.270
and manipulate them directly.

00:58:49.280 --> 00:58:52.320
Core image gives you
fast image processing,

00:58:52.320 --> 00:58:53.730
filtering, so on.

00:58:53.770 --> 00:58:57.060
Quartz Composer is great
for creating animations.

00:58:57.060 --> 00:59:00.750
QtKit, of course,
is Cocoa-level access to QuickTime,

00:59:00.750 --> 00:59:02.220
media files, etc.

00:59:02.270 --> 00:59:07.580
And OpenGL, of course,
gives you a very fast 3D engine.

00:59:07.580 --> 00:59:11.850
Note that most of these also do
their job in separate threads,

00:59:11.850 --> 00:59:15.390
so they introduce
behind-the-scenes concurrency

00:59:15.390 --> 00:59:17.840
into your app in various cases.

00:59:17.840 --> 00:59:18.840
So that's, again, interesting.

00:59:19.280 --> 00:59:21.460
Another benefit of using some of these.

00:59:21.480 --> 00:59:23.880
Of course, don't go ahead and use
them as your first thing,

00:59:23.880 --> 00:59:27.090
but when the situation is right,
when you need that extra boost

00:59:27.090 --> 00:59:30.040
or when you need that subsystem,
these are very, very appropriate.

00:59:31.810 --> 00:59:34.960
One other thing we added in Snow Leopard
is this concurrency in drawing.

00:59:34.960 --> 00:59:38.770
You heard about this perhaps
earlier or perhaps yesterday.

00:59:38.940 --> 00:59:41.980
We now allow views to be
drawn concurrently just

00:59:41.990 --> 00:59:45.580
by setting this property,
can draw concurrently.

00:59:46.610 --> 00:59:49.330
Now, this doesn't mean the view will be
drawn concurrently all the time.

00:59:49.330 --> 00:59:51.280
It gives AppKit permission to do so.

00:59:51.310 --> 00:59:54.090
And if the situation is right,
the AppKit will draw the view

00:59:54.120 --> 00:59:56.070
concurrently along with its siblings.

00:59:56.100 --> 00:59:59.380
What happens is the
display mechanism kicks in,

00:59:59.380 --> 01:00:03.970
and as your window is being redrawn,
if NSView notices that a certain

01:00:04.040 --> 01:00:07.860
number of children have this flag,
those children can be

01:00:07.860 --> 01:00:09.790
drawn in separate cores.

01:00:10.300 --> 01:00:12.970
However,
by the time the display method returns,

01:00:13.030 --> 01:00:14.480
all drawing has been done.

01:00:14.780 --> 01:00:18.430
This is not equivalent to views
being drawn randomly here and

01:00:18.430 --> 01:00:22.040
there in separate threads,
totally concurrent with your app.

01:00:22.070 --> 01:00:24.110
This is still being done in
a very controlled fashion.

01:00:24.160 --> 01:00:29.370
Let's see a quick demo of this.

01:00:33.310 --> 01:00:39.020
Let's go back to normal state here.

01:00:41.700 --> 01:00:46.580
Again, we just called
SetCanDrawConcurrentlyYes as

01:00:46.640 --> 01:00:48.670
a part of this demo.

01:00:48.670 --> 01:00:48.670
Let me run it.

01:00:48.740 --> 01:00:53.080
This is a simple little demo which has
three table views with a bunch of data.

01:00:53.080 --> 01:00:55.680
As you can see, it's already fast enough,
nothing to worry about.

01:00:55.690 --> 01:00:59.620
We're going to give it a
big kick by slowing it down.

01:00:59.620 --> 01:01:01.860
Basically,
every fetch of data source is going

01:01:01.860 --> 01:01:04.360
to introduce a 0.01 second delay.

01:01:04.360 --> 01:01:10.840
This is going to give us
slightly pokier behavior here.

01:01:11.240 --> 01:01:16.420
Let's go ahead and bring up this
frame meter that Quartz debug has.

01:01:16.420 --> 01:01:18.420
Let's put it over here.

01:01:18.420 --> 01:01:21.320
And then let's go ahead and resize
our window in an automated fashion.

01:01:21.340 --> 01:01:24.300
So as you can see,
the window is resizing.

01:01:24.300 --> 01:01:25.570
It's not great.

01:01:25.640 --> 01:01:28.320
It's getting 20, 22, 24 frames a second.

01:01:28.320 --> 01:01:32.620
But if we enable concurrent drawing,
you can see that it's a bit smoother

01:01:32.620 --> 01:01:36.280
and we're going up to about 40,
42 frames a second.

01:01:36.280 --> 01:01:37.500
Let's go ahead and do that again.

01:01:40.140 --> 01:01:44.670
So again, concurrent drawing really
helps in this case.

01:01:44.710 --> 01:01:46.730
We've enabled concurrent
drawing for this view,

01:01:46.730 --> 01:01:49.080
this view, this view,
basically those three views,

01:01:49.080 --> 01:01:50.480
and they're all table views.

01:01:50.610 --> 01:01:53.220
And concurrent drawing is really
good for these cases where

01:01:53.290 --> 01:01:54.560
we have big sibling views.

01:01:54.560 --> 01:01:58.920
You know, mail is this way,
Xcode is this way, iCal is this way.

01:01:58.920 --> 01:02:01.500
And many applications tend to have
these big views that are sibling,

01:02:01.500 --> 01:02:03.140
so they can benefit from this behavior.

01:02:03.160 --> 01:02:06.490
And one thing to note here is that

01:02:07.060 --> 01:02:12.180
Since these views aren't being drawn
haphazardly in background threads,

01:02:12.350 --> 01:02:16.170
the threading requirements of your
model remain pretty much the same.

01:02:16.170 --> 01:02:19.130
It's just that instead of your model
being called from the main thread,

01:02:19.240 --> 01:02:21.290
it is going to be called
from a background thread.

01:02:21.320 --> 01:02:24.470
But it's not going to be called while,
for instance,

01:02:24.470 --> 01:02:28.100
the user is doing some action,
because it's only being called

01:02:28.180 --> 01:02:32.660
in this limited window of a draw,
the whole display machinery kicking in.

01:02:32.690 --> 01:02:36.460
So this might be fairly straightforward
to turn on in most situations.

01:02:36.980 --> 01:02:40.310
So you should go ahead and
experiment to see if it's a win.

01:02:40.560 --> 01:02:42.960
And of course,
you can even turn it on in your Leopard

01:02:43.100 --> 01:02:46.980
or Tiger-based app just by checking to
see if this method exists and calling it,

01:02:47.080 --> 01:02:49.520
because it's a dynamic runtime.

01:02:49.520 --> 01:02:55.340
You can do these things dynamically.

01:02:57.390 --> 01:03:01.140
Okay, so I just want to now enumerate
a bunch of other topics because

01:03:01.140 --> 01:03:03.830
this is a Cocoa Performance talk.

01:03:03.870 --> 01:03:06.800
I do want to make sure, this is the first
Cocoa Performance talk you've come to,

01:03:06.800 --> 01:03:09.890
I want to make sure you realize
some of the topics that you

01:03:09.890 --> 01:03:11.570
should really be aware of.

01:03:11.630 --> 01:03:13.110
Responsiveness.

01:03:13.110 --> 01:03:18.900
This includes topics like app launch,
how fast the app resizes, scrolls,

01:03:18.900 --> 01:03:20.050
responds to user events.

01:03:20.130 --> 01:03:22.040
This is there are many
things you can do here.

01:03:22.040 --> 01:03:26.730
There are many techniques for speeding
up app launch and reacting quickly.

01:03:26.730 --> 01:03:29.470
Be sure to pay attention to these.

01:03:29.470 --> 01:03:31.620
Fast shutdown is a part of this.

01:03:31.620 --> 01:03:34.070
Mark Petrelli will talk about
this in session 425 this

01:03:34.210 --> 01:03:36.460
afternoon at 5:00 in Russian Hill.

01:03:36.600 --> 01:03:41.340
This is basically how to get your app to
quit as fast as possible so the logout

01:03:41.370 --> 01:03:44.300
or shutdown experience can be quick.

01:03:44.300 --> 01:03:45.870
Memory usage is a huge topic.

01:03:45.870 --> 01:03:49.420
Auto release and how much
memory do objects use,

01:03:49.440 --> 01:03:50.380
et cetera.

01:03:50.380 --> 01:03:52.480
There's a lot written about this.

01:03:52.580 --> 01:03:52.580
Especially when you're
using a lot of memory,

01:03:52.580 --> 01:03:52.580
you're not going to be able
to get a lot of memory.

01:03:52.580 --> 01:03:52.580
So, I'm going to talk about memory
usage and how to use it.

01:03:52.580 --> 01:03:52.580
Memory usage is a huge topic.

01:03:52.580 --> 01:03:52.580
Auto release and how much
memory do objects use,

01:03:52.580 --> 01:03:52.620
et cetera.

01:03:52.620 --> 01:03:52.620
There's a lot written about this,
especially when you're

01:03:52.620 --> 01:03:52.620
using a lot of memory.

01:03:52.620 --> 01:03:55.460
Especially with garbage collection
versus explicit memory management,

01:03:55.460 --> 01:03:56.460
things to be aware of.

01:03:56.540 --> 01:04:00.250
64-bit,
you've seen good CPU benefits to 64-bit.

01:04:00.350 --> 01:04:02.930
Turns out, of course,
64-bit does have a memory impact.

01:04:02.930 --> 01:04:04.080
Pointers are bigger.

01:04:04.080 --> 01:04:06.440
The integers that we
use in Cocoa are bigger.

01:04:06.440 --> 01:04:08.940
So, you need to be aware of trade-offs.

01:04:08.940 --> 01:04:11.050
Avoiding impedance mismatches.

01:04:11.050 --> 01:04:14.430
Impedance mismatch is a term
we use whenever you're calling

01:04:14.430 --> 01:04:17.640
through APIs and you have to
convert one data type to another.

01:04:17.640 --> 01:04:21.820
Let's say you have an NSString that
you use for a file reference.

01:04:21.820 --> 01:04:24.420
You have to convert it
to a Fesref and so on.

01:04:24.420 --> 01:04:27.020
Every time you do this, of course,
it's sort of wasted energy.

01:04:27.020 --> 01:04:30.340
If you're doing this a lot,
it does show up in program samples.

01:04:30.340 --> 01:04:34.140
It's good to avoid impedance
mismatches by keeping data in your

01:04:34.140 --> 01:04:36.920
program that's of consistent types.

01:04:36.920 --> 01:04:40.780
Don't convert from char stars to
NSStrings or RGBs to NS colors

01:04:40.840 --> 01:04:42.690
back and forth all the time.

01:04:42.690 --> 01:04:46.380
Try to stick to high-level
abstractions whenever possible.

01:04:46.480 --> 01:04:48.750
And finally, of course,
I didn't show you many of

01:04:48.750 --> 01:04:50.020
them except Quartz Debug.

01:04:50.070 --> 01:04:52.220
We have a wonderful suite
of performance tools.

01:04:52.220 --> 01:04:54.130
There are other sessions about them.

01:04:54.130 --> 01:04:58.170
Instruments, Shark, Malloc Debug,
Quartz Debug, and a bunch of others.

01:04:58.170 --> 01:05:02.860
Be sure to use these and check for
hot spots in your applications.

01:05:03.490 --> 01:05:06.150
Our documentation,
there's a whole subsection

01:05:06.150 --> 01:05:07.900
dedicated to performance.

01:05:07.900 --> 01:05:09.310
You should go ahead and look at that.

01:05:09.390 --> 01:05:13.030
And also last year's session,
session 141, Boosting Responsiveness and

01:05:13.030 --> 01:05:15.870
Performance in Cocoa Application,
talks about a good number of

01:05:15.870 --> 01:05:17.400
the responsiveness issues.

01:05:17.400 --> 01:05:21.110
You would find this in, I believe,
iTunes, ADC, iTunes.

01:05:21.110 --> 01:05:24.190
There's a number of other sessions
you can get to from last year's talk.

01:05:24.190 --> 01:05:27.280
And you should go try to grab
those before they might go away.

01:05:27.310 --> 01:05:30.790
I don't know if they will, but, you know,
sometimes they do.

01:05:31.720 --> 01:05:33.870
So, in a summary, it's multi-core world.

01:05:33.870 --> 01:05:36.380
Multi-threading is hard to get right,
to get fast.

01:05:36.430 --> 01:05:39.390
But if you do the right things,
avoid contention, you know,

01:05:39.390 --> 01:05:41.000
there are a lot of benefits.

01:05:41.080 --> 01:05:43.270
You might want to consider
using NSOperation,

01:05:43.340 --> 01:05:45.760
Operation Queue, and, of course,
the GCD stuff,

01:05:45.760 --> 01:05:48.860
which has been added in Snow Leopard,
is also very powerful.

01:05:48.860 --> 01:05:51.310
We are looking at integrating
that under Operation Queue

01:05:51.360 --> 01:05:52.800
and under Operation as well.

01:05:52.800 --> 01:05:56.190
But using these facilities instead of
threads gives you more of an abstraction,

01:05:56.220 --> 01:05:57.060
more leverage.

01:05:57.060 --> 01:05:59.420
Know the performance
characteristics of APIs.

01:06:00.150 --> 01:06:03.360
And, again, you know,
use memory if it's going to help you,

01:06:03.470 --> 01:06:04.820
but know when to let go.

01:06:04.920 --> 01:06:08.020
And as far as drawing goes,
if there's one thing we can say,

01:06:08.150 --> 01:06:11.210
that's to minimize it, you know,
just minimize drawing and be

01:06:11.320 --> 01:06:13.920
aware of what you're drawing.

01:06:14.160 --> 01:06:19.040
We have two sessions left in the Cocoa,
by the Cocoa today for Cocoa.

01:06:19.040 --> 01:06:22.100
And both of these sessions
have performance topics.

01:06:22.100 --> 01:06:24.860
They're both in Russian Hill,
419 and 425.

01:06:24.860 --> 01:06:27.190
They're definitely worth seeing.