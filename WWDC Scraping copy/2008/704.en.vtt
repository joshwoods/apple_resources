WEBVTT

00:00:20.710 --> 00:00:24.350
So I'm Jeff Moore and I'm
going to be talking about the

00:00:24.350 --> 00:00:25.870
architecture of Core Audio.

00:00:25.880 --> 00:00:28.770
The talk is going to be
divided into two parts.

00:00:28.770 --> 00:00:31.300
In the first part,
I kind of want to give you kind of

00:00:31.310 --> 00:00:35.130
a taste of some of the generalities
behind the Core Audio architecture.

00:00:35.130 --> 00:00:39.140
I'm going to talk a little bit about
what generally holds the pieces together.

00:00:39.300 --> 00:00:42.640
And then I'm going to talk a little
bit about two specific concepts

00:00:42.700 --> 00:00:44.710
that are used throughout the APIs.

00:00:44.710 --> 00:00:48.760
First, the property mechanism,
which is an API construct,

00:00:48.760 --> 00:00:51.910
and then format agnosticism,
which is kind of an

00:00:51.910 --> 00:00:54.100
API concept and philosophy.

00:00:56.300 --> 00:00:58.700
After that,
I'm going to introduce you to some

00:00:58.700 --> 00:01:03.820
of the specific Core Audio APIs using
four general usage cases,

00:01:03.860 --> 00:01:06.380
such as converting audio data
from one format to another,

00:01:06.380 --> 00:01:08.980
reading and writing
audio data from a file,

00:01:08.980 --> 00:01:12.840
as well as adding an effect to some data,
and playing audio data

00:01:12.850 --> 00:01:14.250
out to the speaker.

00:01:17.230 --> 00:01:20.980
When you look at the Core Audio API set,
you see a couple of hundred functions

00:01:20.980 --> 00:01:27.930
across a couple of dozen header files
and a couple of different frameworks.

00:01:27.940 --> 00:01:29.950
It's kind of a lot to
get your head around.

00:01:29.960 --> 00:01:32.600
But, you know,
there is some method to that madness.

00:01:32.600 --> 00:01:36.790
And the first thing to know about
them is that even though there

00:01:36.790 --> 00:01:40.130
seem like there are a lot of APIs,
each API is kind of focused

00:01:40.200 --> 00:01:41.840
on one specific job.

00:01:41.900 --> 00:01:45.320
And it tends not to, they don't like the
color outside the line.

00:01:45.480 --> 00:01:47.880
So if you're dealing
with an audio file API,

00:01:47.880 --> 00:01:51.270
you're not going to have to worry
about it doing other things besides

00:01:51.290 --> 00:01:52.780
just dealing with audio files.

00:01:52.820 --> 00:01:59.050
Now, even though the API set is kind of,
each API is focused in on its own world,

00:01:59.080 --> 00:02:03.040
we built the API such that they
are cooperative and that they can

00:02:03.040 --> 00:02:06.220
be strung together in order to
perform more complicated tasks.

00:02:07.680 --> 00:02:12.060
We've also organized the
API into basically three layers.

00:02:12.120 --> 00:02:15.360
At the lowest layer,
you have mostly APIs that

00:02:15.460 --> 00:02:17.840
deal with audio hardware,
that deal with talking to the hardware,

00:02:17.840 --> 00:02:21.420
such as the IO Audio driver family,
the audio HAL that

00:02:21.420 --> 00:02:24.520
talks to those drivers,
Core MIDI, et cetera.

00:02:25.520 --> 00:02:29.200
Now, at the middle tier of APIs,
that's where you're going to find kind

00:02:29.200 --> 00:02:31.320
of the meat of the Core Audio API set.

00:02:31.640 --> 00:02:35.050
That's where you're going to
find the APIs you use the most,

00:02:35.050 --> 00:02:39.520
such as the audio converter,
the audio file API, the audio unit APIs,

00:02:39.520 --> 00:02:41.820
AU Graph, and a bunch of others.

00:02:41.880 --> 00:02:44.900
And then we have a few high-level APIs.

00:02:44.900 --> 00:02:49.300
Now, when we talk about high-level
APIs in the Core Audio world,

00:02:49.300 --> 00:02:52.500
what we really mean is an
API that combines two or

00:02:52.500 --> 00:02:54.480
more of the lower-level APIs.

00:02:55.520 --> 00:02:58.510
And then we have a few high-level
APIs that combine them into

00:02:58.580 --> 00:03:00.080
one easier-to-use package.

00:03:00.080 --> 00:03:03.560
And examples of our high-level
APIs are the Extended Audio File API,

00:03:03.560 --> 00:03:06.380
OpenAL, the Audio Queue Services,
and a few others.

00:03:09.130 --> 00:03:14.650
So another part of the APIs that
kind of makes everything kind of hang

00:03:14.650 --> 00:03:16.320
together is this notion of properties.

00:03:16.340 --> 00:03:22.880
A property represents some attribute or
some piece of state of an API object.

00:03:22.960 --> 00:03:27.950
And you use a key value pair to
describe this little aspect of the audio

00:03:27.950 --> 00:03:30.180
object and to manipulate it as well.

00:03:30.220 --> 00:03:33.610
Now,
a given object in the API might have,

00:03:33.610 --> 00:03:36.650
you know, lots of properties on it.

00:03:36.810 --> 00:03:40.640
So in order to be able to tell
what property you're talking about,

00:03:40.640 --> 00:03:42.060
each property has an address.

00:03:42.180 --> 00:03:45.730
Now, for most of the Core Audio APIs,
the address is just a

00:03:45.840 --> 00:03:47.910
32-bit integer selector.

00:03:47.920 --> 00:03:52.820
However, several APIs use further qualify
the address using a scope

00:03:52.900 --> 00:03:56.890
and element selector to allow
you to talk about different

00:03:56.890 --> 00:03:59.210
sub-pieces of a given API object.

00:04:01.130 --> 00:04:05.860
Now, the value of a property can be
pretty much whatever the API needs.

00:04:05.860 --> 00:04:08.940
If it's a CF object,
it'll be a CF object.

00:04:08.940 --> 00:04:11.060
If it's a structure,
it'll be a structure.

00:04:11.060 --> 00:04:16.060
Basically, it's organized as a blob,
a pointer to a blob of memory and a size

00:04:16.380 --> 00:04:19.300
that says how big that blob of memory is.

00:04:20.840 --> 00:04:24.760
Now, some APIs provide what we
call property listeners.

00:04:24.760 --> 00:04:29.540
A property listener is a callback
that you can install on the object in

00:04:29.640 --> 00:04:35.120
the API that will get called whenever
the value of that property changes.

00:04:35.770 --> 00:04:39.620
And this is a way you keep up with
some of the busyness of some of the

00:04:39.620 --> 00:04:42.080
APIs as they go about doing their work.

00:04:43.020 --> 00:04:45.500
Now, as I said,
the properties are kind of a

00:04:45.500 --> 00:04:48.180
humanifying mechanism with the API set.

00:04:48.560 --> 00:04:49.770
And as such, you're going to find

00:05:37.100 --> 00:05:40.310
Now, in addition to formats,
to properties,

00:05:40.310 --> 00:05:44.120
which is kind of an API mechanics
thing that kind of makes

00:05:44.120 --> 00:05:47.800
the API set hang together,
we also have this concept

00:05:47.960 --> 00:05:51.580
of format agnosticism that
runs throughout the API set.

00:05:51.580 --> 00:05:55.770
Now, what we mean by format agnosticism
is kind of a mindset more than

00:05:55.770 --> 00:05:57.740
it is anything mechanical.

00:05:57.740 --> 00:06:01.600
It's about organizing your coding
so that you are approaching it

00:06:01.600 --> 00:06:05.860
without any preconditions on the
data formats you're dealing with.

00:06:06.780 --> 00:06:10.410
This allows you to use whatever
data that comes your way,

00:06:10.450 --> 00:06:13.500
no matter what it is,
as long as it can be probably

00:06:13.500 --> 00:06:15.880
APIs we'll talk about in a minute.

00:06:15.900 --> 00:06:19.360
As I said,
pretty much every Core Audio API uses

00:06:19.360 --> 00:06:23.200
format agnosticism in its API,
so by following along

00:06:23.230 --> 00:06:26.700
with what the APIs do,
you will naturally build

00:06:26.700 --> 00:06:28.550
it into your program.

00:06:31.740 --> 00:06:35.310
Now, even though we talk about not
really trying to focus on the

00:06:35.310 --> 00:06:39.870
individual details of a format,
the Core Audio API set does have what

00:06:40.010 --> 00:06:42.440
we call canonical sample formats.

00:06:42.500 --> 00:06:46.500
And the job of the canonical sample
format is first and foremost to

00:06:46.500 --> 00:06:50.790
be the default format you get when
you don't specify anything else.

00:06:50.800 --> 00:06:55.830
The canonical format is also chosen
to be rich enough that it can be

00:06:55.830 --> 00:07:01.010
used as an intermediate format
in various format conversions.

00:07:01.850 --> 00:07:04.740
And finally,
the other big job in picking the

00:07:04.740 --> 00:07:09.780
canonical format is it has to be an
efficient format for the platform you're

00:07:09.780 --> 00:07:11.350
on and in the situation you're in.

00:07:13.590 --> 00:07:19.550
Now, on Mac OS X and on the iPhone,
we have basically two kinds

00:07:19.570 --> 00:07:21.830
of canonical sample formats.

00:07:21.830 --> 00:07:27.960
And we represent them both using a
typedef as well as a set of flags that...

00:07:28.380 --> 00:07:31.650
The first one I want to talk
about is the audio sample type and

00:07:31.720 --> 00:07:33.840
K-audio format flags canonical.

00:07:33.980 --> 00:07:40.080
This represents the canonical format
that is used in I/O situations.

00:07:40.360 --> 00:07:45.750
On the desktop, this is a 32-bit native
Indian floating point format.

00:07:45.880 --> 00:07:47.950
On the iPhone, however, it's different.

00:07:48.280 --> 00:07:51.720
It's a 16-bit integer format.

00:07:52.250 --> 00:07:56.480
The other canonical format is the one
that's used for audio units and also

00:07:56.480 --> 00:07:59.040
in other signal processing situations.

00:07:59.040 --> 00:08:03.730
And it's represented by the audio
unit sample type and K-Audio

00:08:03.730 --> 00:08:06.170
format flags audio unit canonical.

00:08:06.180 --> 00:08:09.040
Now, this format differs from the I.O.

00:08:09.040 --> 00:08:09.600
format.

00:08:09.790 --> 00:08:13.670
As I said, it comes up in places where
you're going to be doing

00:08:13.670 --> 00:08:16.300
processing and you need headroom.

00:08:17.100 --> 00:08:21.390
As such, on the desktop,
the format is going to be a 32-bit float.

00:08:21.530 --> 00:08:23.100
It's exactly the same as the I.O.

00:08:23.100 --> 00:08:23.660
format.

00:08:23.660 --> 00:08:28.090
However, on the phone,
you're going to find that it's an

00:08:28.090 --> 00:08:33.340
8.24 fixed-point sample format,
which is much more efficient on a

00:08:33.340 --> 00:08:37.380
processor where you don't really
have a good floating-point processor.

00:08:39.080 --> 00:08:42.440
Now,
even though you're being format agnostic,

00:08:42.440 --> 00:08:46.880
you still have to know a certain
amount of base-level information

00:08:46.880 --> 00:08:49.270
about the format you're dealing with.

00:08:49.270 --> 00:08:54.000
And we've kind of boiled that information
down into this structure here,

00:08:54.020 --> 00:08:56.610
the AudioStream Basic Description.

00:08:56.620 --> 00:09:01.280
Let's talk, and you'll see written in our
documentation in other places,

00:09:01.340 --> 00:09:06.050
we often refer to the AudioStream
Basic Description by its abbreviation,

00:09:06.060 --> 00:09:06.740
ASBD.

00:09:07.880 --> 00:09:10.920
So, as you can see,
this structure has several fields,

00:09:11.060 --> 00:09:13.720
some of which might jump
out at you as familiar,

00:09:13.720 --> 00:09:16.750
such as the sample rate field
and the bits per channel and

00:09:16.800 --> 00:09:18.420
channels per frame field.

00:09:18.420 --> 00:09:20.760
And those mean pretty much
what you think they do.

00:09:20.760 --> 00:09:24.100
They represent the sample
rate of the data stream,

00:09:24.100 --> 00:09:27.030
as well as how many bits
deep the samples are,

00:09:27.030 --> 00:09:30.650
as well as how many channels
are in the data stream.

00:09:30.660 --> 00:09:34.780
The other fields are just as important,
starting with the format

00:09:34.780 --> 00:09:36.730
ID and format flags fields.

00:09:37.310 --> 00:09:41.520
Now, these two fields together allow
you to identify the general

00:09:41.520 --> 00:09:46.160
category of the audio data,
whether it be linear PCM or AAC or MP3,

00:09:46.160 --> 00:09:48.630
Apple lossless, what have you.

00:09:48.660 --> 00:09:51.280
Now, the other fields,
the bytes per frame,

00:09:51.390 --> 00:09:54.750
the frames per packet,
and the bytes per packet fields,

00:09:54.750 --> 00:09:58.390
are there to describe different
ways of breaking down the data

00:09:58.400 --> 00:10:00.560
stream into component pieces.

00:10:00.560 --> 00:10:03.640
And we'll talk a little bit more about
where that's important in a few minutes.

00:10:03.720 --> 00:10:04.830
Now,
before we go on and look at a few areas,

00:10:04.830 --> 00:10:06.400
we're going to talk a little
bit more about the data stream.

00:10:06.480 --> 00:10:07.920
So,
let's go ahead and look at a few ASPDs.

00:10:07.960 --> 00:10:11.560
Just wanted to talk a little bit
about how the structure works.

00:10:11.560 --> 00:10:15.940
Now, for any given format,
not necessarily all the fields

00:10:15.940 --> 00:10:18.490
in the ASPD are going to be used.

00:10:18.540 --> 00:10:21.250
For some formats,
it just doesn't make any sense,

00:10:21.320 --> 00:10:24.660
or you just can't know that
information about the format.

00:10:24.660 --> 00:10:28.080
In other cases,
it just doesn't make any sense.

00:10:28.140 --> 00:10:31.520
Now,
in those cases where you have a field in

00:10:31.520 --> 00:10:36.290
an ASPD that's not needed for the format,
you have to set that field.

00:10:36.480 --> 00:10:36.860
You have to set that field to zero.

00:10:36.860 --> 00:10:40.220
You have to be sure you do that,
or otherwise you're going to get the

00:10:40.220 --> 00:10:42.110
ASPD rejected by various API calls.

00:10:42.180 --> 00:10:45.730
Now, I want to talk a little bit
about some terminology we

00:10:45.770 --> 00:10:49.820
use in the core audio world,
particularly these three words here,

00:10:50.160 --> 00:10:51.760
sample, frame, and packet.

00:10:51.760 --> 00:10:56.170
Now, these terms are very overloaded in
the computer science world as well

00:10:56.170 --> 00:10:58.390
as in the signal processing world.

00:10:58.420 --> 00:11:01.020
But when we use them within
the core audio context,

00:11:01.020 --> 00:11:02.940
we mean something very specific.

00:11:02.940 --> 00:11:06.310
A sample is a single data
point for a given change.

00:11:06.480 --> 00:11:09.010
It's a single channel in an audio stream.

00:11:09.060 --> 00:11:13.840
A frame is a collection of
samples that are time coincident.

00:11:13.840 --> 00:11:20.480
You can think of it as kind of the left
and right stereo pair of a stereo signal.

00:11:20.480 --> 00:11:22.350
And then you have packets.

00:11:22.680 --> 00:11:27.410
Now, packets are important here,
but for now, it's sufficient to define a

00:11:27.410 --> 00:11:31.720
packet as a collection of frames
that go together for some person.

00:11:31.800 --> 00:11:36.240
It's also important to know that the
frames in a packet are all contiguous.

00:11:38.340 --> 00:11:40.530
Here we see an
Audio Stream Basic description

00:11:40.680 --> 00:11:45.050
filled out for two channels of
data in the I/O canonical sample

00:11:45.170 --> 00:11:47.200
format at a 44:1 sample rate.

00:11:47.200 --> 00:11:49.200
And, you know,
as you walk through the fields here,

00:11:49.200 --> 00:11:56.200
you can see that we filled out each
field appropriately for this format.

00:11:56.200 --> 00:12:00.200
Starting with the format
ID and format flags fields,

00:12:00.210 --> 00:12:03.200
you can see that we've
set them to linear PCM,

00:12:03.260 --> 00:12:09.080
and we're using the audio format
flags canonical flag to set the flags.

00:12:09.200 --> 00:12:14.250
We've also set the sample rate,
and we've set the bits per channel by

00:12:14.630 --> 00:12:17.100
using the audio sample type typedef.

00:12:17.200 --> 00:12:20.390
You can see we've taken the size of
that and multiplied by eight to get

00:12:20.490 --> 00:12:24.160
the number of bits for each channel.

00:12:24.200 --> 00:12:27.200
And then for filling
out the other fields,

00:12:27.200 --> 00:12:32.200
we just multiply that size by the number
of channels to get the bytes per frame,

00:12:32.200 --> 00:12:36.200
which indicates that we
get two times that size,

00:12:36.200 --> 00:12:38.180
and the bytes per packet also.

00:12:38.200 --> 00:12:42.630
Now, it's interesting to note with
linear PCM is that the number of

00:12:42.670 --> 00:12:46.200
frames in each packet of linear
PCM is always going to be one.

00:12:46.200 --> 00:12:49.250
And when we talk about packets
a little bit more later,

00:12:49.250 --> 00:12:51.200
you'll understand why that is.

00:12:51.200 --> 00:12:56.590
Now, another interesting thing to
note about this structure is that

00:12:56.590 --> 00:12:59.080
it is also platform agnostic.

00:12:59.200 --> 00:13:01.190
This structure is filled out
correctly for each channel.

00:13:01.330 --> 00:13:05.180
It's filled out correctly whether
you're on the desktop or the iPhone.

00:13:05.200 --> 00:13:09.790
Now, the way we did that is making sure
you're using the audio sample type data

00:13:09.790 --> 00:13:13.200
type and the canonical format flags.

00:13:13.210 --> 00:13:17.190
Those data types will change
in size depending on what

00:13:17.610 --> 00:13:20.200
platform you're compiling for.

00:13:21.770 --> 00:13:25.390
Now here we see the
Audio Unit Canonical sample format,

00:13:25.470 --> 00:13:28.850
similarly with a two-channel 44:1 stream.

00:13:29.010 --> 00:13:32.740
Now going through this,
the main differences to notice here is

00:13:32.820 --> 00:13:38.380
that we're using the Audio Unit Canonical
flags here to describe the format flags.

00:13:39.190 --> 00:13:42.830
Now, and we've also using the
Audio Unit sample type to

00:13:42.830 --> 00:13:45.200
describe the size of the sample.

00:13:45.370 --> 00:13:49.650
Now, one thing to note about the
Audio Unit Canonical type is

00:13:49.650 --> 00:13:53.430
that this format is what we
call a non-interleaved format.

00:13:54.110 --> 00:13:56.730
And so what this means
is that each sample,

00:13:56.760 --> 00:14:02.180
each channel is represented by its
own buffer in the processing chain.

00:14:02.270 --> 00:14:06.190
As such, you have to be sure you fill
out the ASBD to reflect this.

00:14:06.820 --> 00:14:08.260
So you can see here,
we filled out the ASBD.

00:14:08.510 --> 00:14:10.820
And so we filled out the bytes
per channel as one times the

00:14:10.820 --> 00:14:14.500
size of Audio Unit sample type,
whereas in the previous ASBD,

00:14:14.500 --> 00:14:15.940
we did two times.

00:14:15.950 --> 00:14:18.710
And that's because of the
difference between interleaved

00:14:18.780 --> 00:14:21.210
versus non-interleaved formats.

00:14:22.920 --> 00:14:26.490
Now here we see a slightly
more complicated ASBD.

00:14:26.490 --> 00:14:32.760
It represents a two-channel format that
has 24-bit samples that have been aligned

00:14:32.760 --> 00:14:36.880
high in 32 bits of a big-endian sample.

00:14:36.900 --> 00:14:41.960
That sounds like a mouthful,
and it's kind of a complicated format,

00:14:41.960 --> 00:14:45.620
but you can see here as you go through
the different fields that we can

00:14:45.620 --> 00:14:49.740
represent the sample type pretty much
exactly the way you would expect.

00:14:50.100 --> 00:14:54.080
Now the key thing to notice here is
the difference between how we filled

00:14:54.080 --> 00:14:58.610
out the channels per frame field,
or I'm sorry, the bytes per frame field.

00:14:58.620 --> 00:15:05.790
Here we use the value of 8, which is 2,
the number of channels, times 4 bytes,

00:15:05.820 --> 00:15:11.850
which is each sample is actually 4
bytes wide because it's 24 actual

00:15:12.140 --> 00:15:15.240
bits carried in a 32-bit word.

00:15:17.280 --> 00:15:23.600
And finally, we see an ASBD here that
describes the AAC format.

00:15:23.600 --> 00:15:28.240
Now, the key thing to note here is
that the AAC format does not

00:15:28.240 --> 00:15:32.050
actually allow you to define,
to fill out a lot of these flags because

00:15:32.050 --> 00:15:36.630
they're not really relevant or even
knowable about the AAC data stream.

00:15:37.360 --> 00:15:39.040
For example, the bits per channel.

00:15:39.050 --> 00:15:42.140
You can't really talk about the bits
per channel of an encoded format,

00:15:42.260 --> 00:15:44.100
so in this case, we've set it to zero.

00:15:44.100 --> 00:15:49.710
And the fields that otherwise
depend on that also are set to zero,

00:15:49.870 --> 00:15:53.330
such as the bytes per frame and others.

00:15:54.880 --> 00:15:57.370
One key field that you
will always see filled out,

00:15:57.380 --> 00:16:01.470
however,
for AAC is the bytes per packet field,

00:16:01.470 --> 00:16:03.880
or the frames per packet field.

00:16:03.880 --> 00:16:07.270
So that's so you know how many
frames are in each packet.

00:16:11.880 --> 00:16:16.400
Now, in addition to an ASPD,
some formats require an extra

00:16:16.400 --> 00:16:21.810
blob of data to fully describe
the content of a data stream.

00:16:22.060 --> 00:16:25.300
In Core Audio,
we refer to this blob as a magic cookie.

00:16:25.300 --> 00:16:30.040
And magic cookies have to accompany
the data stream that they go with,

00:16:30.040 --> 00:16:32.630
and they can't really be separated.

00:16:33.810 --> 00:16:38.060
You get the magic cookie when you first
create the stream using an encoder,

00:16:38.060 --> 00:16:41.940
and you absolutely have to be sure
you pass it along to the decoder when

00:16:41.960 --> 00:16:44.120
it comes time to decode the data.

00:16:44.950 --> 00:16:48.190
Now, as you might imagine,
the magic cookie has some fairly

00:16:48.190 --> 00:16:51.550
interesting information in it,
and it's tempting to parse

00:16:51.550 --> 00:16:52.900
inside the black box.

00:16:52.900 --> 00:16:56.440
But you just have to be sure you
remember that it is a black box.

00:16:56.440 --> 00:16:58.500
You really shouldn't do that.

00:16:59.080 --> 00:17:01.730
Instead, you should use the
various Core Audio APIs,

00:17:01.730 --> 00:17:03.140
such as the AudioFormat.

00:17:03.770 --> 00:17:07.690
And you can use the AudioFormat
API to get access to the information

00:17:07.690 --> 00:17:09.710
contained in the magic cookie.

00:17:11.780 --> 00:17:16.410
We talked a little bit earlier about
packets and before we defined a packet

00:17:16.510 --> 00:17:22.700
as a collection of sample frames
that go together for some purpose.

00:17:22.700 --> 00:17:26.130
Now, going forward,
I'd want to strengthen that definition

00:17:26.130 --> 00:17:31.440
a little bit and say that a packet
is actually the smallest indivisible

00:17:31.830 --> 00:17:34.280
unit of a given data format.

00:17:34.280 --> 00:17:37.670
Now,
you might remember I mentioned earlier

00:17:38.070 --> 00:17:42.940
that the number of frames in each
packet of linear PCM is always one.

00:17:43.080 --> 00:17:47.400
Now, when you think of that in terms of
this new definition of a packet,

00:17:47.400 --> 00:17:51.810
it kind of makes some sense because the
smallest logical block you can break

00:17:51.950 --> 00:17:56.620
down a linear PCM stream to is a frame,
to make sense.

00:17:57.050 --> 00:18:03.790
And that's why a packet of linear
PCM is exactly one frame of data.

00:18:04.010 --> 00:18:09.080
Now, there are three general kinds of
packetization of various data formats

00:18:09.160 --> 00:18:10.100
that you're going to run into.

00:18:10.100 --> 00:18:13.770
The first is the simplest,
constant bitrate.

00:18:13.770 --> 00:18:18.450
What this means is that each packet of
the format has the same size in terms

00:18:18.450 --> 00:18:23.280
of both the number of bytes in the
packet and the number of sample frames.

00:18:25.160 --> 00:18:29.690
Examples of a constant bitrate
format are linear PCM and IMA.

00:18:29.690 --> 00:18:33.150
The great thing about constant
bitrate formats is you always

00:18:33.150 --> 00:18:36.830
know how to break the stream apart
into pieces and manipulate it.

00:18:36.920 --> 00:18:40.700
You don't need to have any
external framing information

00:18:40.700 --> 00:18:42.340
to know how to do that.

00:18:44.010 --> 00:18:48.480
The next form of packetization you get
is the variable bitrate packetization.

00:18:49.090 --> 00:18:53.880
Variable bitrate or VBR,
as you'll hear me say,

00:18:53.880 --> 00:18:58.860
those packets have the same
number of frames in them,

00:18:58.900 --> 00:19:04.210
but each packet might vary in size in
terms of bytes from packet to packet.

00:19:05.560 --> 00:19:08.920
Now, that gives you a lot of
flexibility in terms of managing

00:19:08.920 --> 00:19:11.060
the bitrate of an encoded format.

00:19:11.060 --> 00:19:14.990
And because of that,
you're going to find that pretty

00:19:14.990 --> 00:19:19.220
much most of the encoded formats
you run into these days are

00:19:19.370 --> 00:19:25.320
of a variable bitrate nature,
such as AAC, Apple lossless, MP3, etc.

00:19:26.640 --> 00:19:30.060
Now, the final packetization I want
to mention to you is the variable

00:19:30.070 --> 00:19:31.610
frame rate packetization.

00:19:32.460 --> 00:19:34.310
Now, this is kind of the most
general packetization.

00:19:35.210 --> 00:19:38.640
Now, this is kind of the most general
packetization type that you can get,

00:19:38.640 --> 00:19:41.040
because each packet can have
both a different size in

00:19:41.040 --> 00:19:43.170
terms of the frames in it,
as well as the number

00:19:43.170 --> 00:19:44.440
of bytes in the packet.

00:19:45.300 --> 00:19:49.590
Now, the final packetization I want
to mention to you is the

00:19:49.590 --> 00:19:52.440
variable bitrate packetization.

00:19:52.440 --> 00:19:52.440
Variable bitrate packetization.

00:19:53.680 --> 00:19:57.160
In the Core Audio world,
we represent a packet using the

00:19:57.160 --> 00:20:00.440
AudioStream packet description structure.

00:20:00.540 --> 00:20:04.650
Now, one thing to note is that you're
really unlikely to ever run into

00:20:05.120 --> 00:20:06.790
one AudioStream packet description.

00:20:06.990 --> 00:20:10.050
You're most likely to see
packet descriptions ganged

00:20:10.050 --> 00:20:13.520
up into arrays of them,
and they are used to describe

00:20:13.520 --> 00:20:17.150
another buffer of memory
that contains audio data.

00:20:19.100 --> 00:20:22.220
Packet descriptions are
absolutely required when you're

00:20:22.260 --> 00:20:25.540
dealing with variable bitrate
and variable framerate formats.

00:20:25.640 --> 00:20:30.740
Every time you run into an API and
you're dealing with that kind of format,

00:20:30.940 --> 00:20:35.990
you're going to need to be filling out
and passing along packet descriptions.

00:20:36.160 --> 00:20:39.140
Now, at the bottom of the slide,
you can see how a packet

00:20:39.140 --> 00:20:40.700
description is declared.

00:20:40.700 --> 00:20:45.770
It has a field that represents an
offset into the buffer of audio

00:20:46.160 --> 00:20:47.980
data where the packet starts.

00:20:48.040 --> 00:20:53.720
It has a field that represents how
long the packet is in terms of bytes.

00:20:53.810 --> 00:20:56.350
Now,
that middle field is a little tricky.

00:20:56.350 --> 00:21:00.720
In a variable bitrate format,
the number of frames in

00:21:00.720 --> 00:21:02.930
each packet is the same.

00:21:02.950 --> 00:21:06.080
So you can get that information
from the ASPD format.

00:21:06.190 --> 00:21:09.380
As such,
you're going to find that this field,

00:21:09.380 --> 00:21:13.300
the variable frames in packet field,
is always going to be

00:21:13.300 --> 00:21:15.420
set to zero for VBR data.

00:21:15.420 --> 00:21:18.660
However,
in the case of variable frame rate data,

00:21:18.770 --> 00:21:22.790
you're going to find that this
field is going to contain the number

00:21:22.790 --> 00:21:27.140
of frames in that specific packet
in that part of the data stream.

00:21:30.260 --> 00:21:33.540
Now, as you might imagine,
ASPDs are kind of important.

00:21:33.540 --> 00:21:36.930
They're also kind of
complicated to fill out.

00:21:37.000 --> 00:21:41.280
The good news is that there are
lots of ways to fill them out.

00:21:41.350 --> 00:21:45.080
Probably the easiest way to get
an ASPD filled out is to just

00:21:45.360 --> 00:21:47.490
let the Core Audio APIs do it.

00:21:48.870 --> 00:21:52.020
Every Core Audio API uses an
ASPD to represent and describe

00:21:52.340 --> 00:21:53.560
the format of its data.

00:21:53.560 --> 00:21:56.570
So, consequently,
you're going to find that you're

00:21:56.570 --> 00:22:00.290
going to be getting ASPDs from it
and giving them back all the time,

00:22:00.300 --> 00:22:04.000
so you might as well just use the
ones the API hands out to you.

00:22:06.030 --> 00:22:10.740
We also provide the Audio Format API to
help you out with filling in ASBD.

00:22:10.740 --> 00:22:15.060
The Audio Format API is
a property-based API.

00:22:15.060 --> 00:22:18.640
In fact,
all the API calls in the audio format

00:22:18.640 --> 00:22:22.030
are related to property manipulation.

00:22:40.460 --> 00:22:45.430
One of the interesting things
the Audio Format API can do

00:22:45.430 --> 00:22:50.050
for you is that given just the
format ID and the magic cookie,

00:22:50.050 --> 00:22:52.400
you can have the Audio Format
API fill out in ASBD.

00:22:53.160 --> 00:22:54.980
SBD for you.

00:22:55.270 --> 00:23:00.100
Now the final mechanism I want to talk
about in terms of filling out ASBDs

00:23:00.140 --> 00:23:02.500
is to just plug the Core Audio SDK.

00:23:02.590 --> 00:23:07.690
You'll find a class in that SDK in
our public utility section called

00:23:07.700 --> 00:23:09.360
CA Stream Basic Description.

00:23:10.970 --> 00:23:15.470
It is a big,
giant raft of information about

00:23:15.480 --> 00:23:18.400
formats and filling out ASBDs.

00:23:18.630 --> 00:23:23.650
Even though the code is C++,
and a lot of you Objective-C programmers

00:23:23.650 --> 00:23:27.060
may be wary of using it,
you should still take a look and

00:23:27.060 --> 00:23:31.330
understand what this code is doing so
that you can at least get the knowledge

00:23:31.590 --> 00:23:35.530
transferred out of that code and
into your head so that you can use it.

00:23:43.590 --> 00:23:48.430
Now that we've kind of looked at
some of the general attributes

00:23:48.430 --> 00:23:51.930
of the Core Audio API,
we looked a little bit about ASBDs

00:23:51.940 --> 00:23:53.620
because they're really important.

00:23:53.680 --> 00:23:58.170
I kind of want to talk a little bit
about some of the usage cases and

00:23:58.180 --> 00:24:02.790
use those as a stepping stone to
introduce you to lots of the actual

00:24:02.790 --> 00:24:06.700
Core Audio APIs you're going to use in
order to accomplish the usage cases.

00:24:09.180 --> 00:24:13.710
So the first case I want to
start with is converting audio

00:24:13.710 --> 00:24:16.100
data from one format to another.

00:24:16.330 --> 00:24:20.100
Now one thing to be aware of,
the audio converter is only

00:24:20.100 --> 00:24:21.700
supported on the desktop.

00:24:21.750 --> 00:24:23.740
It is not supported on the phone.

00:24:23.870 --> 00:24:27.280
So everything I'm going to talk
about in terms of audio converter

00:24:27.280 --> 00:24:29.450
for now does not apply to the iPhone.

00:24:29.880 --> 00:24:34.310
Now, in order to use a converter,
you have to actually make one,

00:24:34.370 --> 00:24:37.250
and that's what the Audio Converter
New function is for.

00:24:37.260 --> 00:24:42.560
In order to call Audio Converter New,
you need to have the properly filled

00:24:42.680 --> 00:24:47.250
out ASPDs for your input format and
the output format to the conversion.

00:24:47.260 --> 00:24:52.710
Now, one thing to note is that one
or both of the input or output

00:24:52.710 --> 00:24:55.500
formats has to be linear PCM.

00:24:56.150 --> 00:24:59.020
In other words,
you can go from linear PCM to

00:24:59.020 --> 00:25:03.220
another linear PCM format,
or you can go from linear

00:25:03.220 --> 00:25:07.080
PCM to an encoded format,
or you can go from an encoded

00:25:07.080 --> 00:25:09.780
format back to a linear PCM format.

00:25:09.780 --> 00:25:15.720
The Audio Converter does not do
transcoding between encoded formats.

00:25:17.660 --> 00:25:20.250
Now,
once you've created your audio converter,

00:25:20.250 --> 00:25:23.700
there are usually a bunch of other
properties you're going to want to

00:25:23.700 --> 00:25:25.160
set on it to control the conversion.

00:25:25.160 --> 00:25:30.040
This is also the time and place where
you will tell the audio converter about

00:25:30.040 --> 00:25:32.840
the magic cookie for your input stream.

00:25:32.840 --> 00:25:35.940
And you will also use this
as an opportunity to set up

00:25:35.960 --> 00:25:40.190
various settings on the encoder,
such as the quality of the

00:25:40.190 --> 00:25:43.480
encoding or the bit rate to use,
etc.

00:25:47.300 --> 00:25:51.230
After you have an audio converter,
you have to be able to move

00:25:51.270 --> 00:25:54.180
the data through it in order
to do the transformation.

00:25:54.210 --> 00:25:58.720
So the audio converter provides two
basic functions for you to do this.

00:25:58.890 --> 00:26:04.780
The first is a very specialized function,
audio converter convert buffer.

00:26:04.780 --> 00:26:10.420
This function is there strictly for
converting between linear PCM formats.

00:26:10.420 --> 00:26:15.010
And only then if you do not have
a sample rate conversion involved.

00:26:15.220 --> 00:26:20.020
Now, we also provide the audio converter
fill complex buffer format.

00:26:20.020 --> 00:26:25.100
And this function can do linear
PCM to linear PCM conversions.

00:26:25.210 --> 00:26:27.030
It can also do it with rate conversion.

00:26:27.100 --> 00:26:29.630
In fact,
this is the function you're going

00:26:29.720 --> 00:26:34.100
to call to pretty much convert any
kind of format from one to the other.

00:26:34.100 --> 00:26:34.100
Jeff Moore

00:26:34.270 --> 00:26:37.600
Now, we were talking a little bit
earlier about format agnosticism.

00:26:37.600 --> 00:26:41.240
And if you're following those practices,
you probably won't even use audio

00:26:41.240 --> 00:26:44.700
converter or convert buffer and
will just concentrate your code on

00:26:44.700 --> 00:26:48.790
audio converter fill complex buffer,
given that it's the most flexible way

00:26:48.870 --> 00:26:51.100
to move the data through the converter.

00:26:53.520 --> 00:26:58.400
So you can pull data out of the
converter using the appropriate call.

00:26:58.400 --> 00:27:01.510
So now you need to be able to
get the data into the converter

00:27:01.510 --> 00:27:02.860
in order for it to be converted.

00:27:02.860 --> 00:27:06.860
So you do this by implementing
an input data callback.

00:27:06.860 --> 00:27:12.350
And you pass the input data callback to
the converter when you make your call

00:27:12.350 --> 00:27:15.540
to audio converter fill complex buffer.

00:27:15.540 --> 00:27:20.720
Now, what the converter does with
this function is a couple things.

00:27:21.380 --> 00:27:23.380
First, this is the function that
gets called to the converter.

00:27:23.400 --> 00:27:26.580
It gets called whenever the
converter needs input data

00:27:26.630 --> 00:27:28.390
for it to do the conversion.

00:27:28.390 --> 00:27:33.200
Now, in the callback,
what you provide back to the converter

00:27:33.200 --> 00:27:35.680
is just pointers to your data.

00:27:35.680 --> 00:27:37.920
You don't copy your data.

00:27:37.920 --> 00:27:41.120
In fact,
the converter has been specifically

00:27:41.250 --> 00:27:45.070
plumbed to eliminate as many
extra copies of data as it

00:27:45.070 --> 00:27:47.430
can for performance reasons.

00:27:47.510 --> 00:27:51.890
Now, the key thing about these
pointers that you return is that

00:27:51.890 --> 00:27:53.950
they have to remain very valid.

00:27:53.950 --> 00:27:57.790
In other words,
they have to continue to point at

00:27:57.790 --> 00:28:02.220
good memory until your input callback
is called again by the converter.

00:28:02.220 --> 00:28:06.230
Now, the other big job you have
to do in your input callback,

00:28:06.230 --> 00:28:09.160
as I said earlier,
is if you're dealing with a variable

00:28:09.210 --> 00:28:13.130
bit rate or variable frame rate format,
you have to be sure you provide the

00:28:13.190 --> 00:28:17.700
packet descriptions of the buffers that
you're providing into the converter.

00:28:20.300 --> 00:28:23.880
So you can get data out,
you can push data in.

00:28:23.940 --> 00:28:25.200
What do you do,
how do you know when you're done,

00:28:25.200 --> 00:28:26.490
or what do you even do?

00:28:26.990 --> 00:28:30.920
Basically,
there are two end-of-stream modes

00:28:31.010 --> 00:28:32.860
that you need to know about.

00:28:32.900 --> 00:28:37.860
The first is the one where you just
have no more data and you're really

00:28:37.860 --> 00:28:41.660
at the end of the stream and you're
never gonna have any more input data.

00:28:41.670 --> 00:28:45.790
And the way you signal that to the
converter is that you return zero

00:28:45.790 --> 00:28:51.920
packets of data in your input proc as
well as no error from your input proc.

00:28:51.920 --> 00:28:55.550
So your input proc will return
an error code and you should

00:28:55.630 --> 00:28:56.960
return zero in this case.

00:28:56.990 --> 00:28:59.360
And this will tell the
converter that you're done,

00:28:59.490 --> 00:29:01.500
you don't have any more
input data to give it.

00:29:01.510 --> 00:29:04.660
And so the converter will know
not to call your input data,

00:29:04.810 --> 00:29:07.080
your input data proc anymore.

00:29:07.100 --> 00:29:11.180
Now, the other situation is kinda
like the first situation,

00:29:11.180 --> 00:29:17.740
only it's kind of a momentary situation
where let's say you're decoding data

00:29:17.860 --> 00:29:21.230
coming over the network and you're
in the middle of downloading and the

00:29:21.280 --> 00:29:26.840
packets you need for the next part of the
conversion haven't been downloaded yet.

00:29:26.860 --> 00:29:30.400
You can signal this
situation to the converter,

00:29:30.400 --> 00:29:34.800
again,
by first by returning no packets of data,

00:29:34.820 --> 00:29:37.220
but also you return an error code.

00:29:37.240 --> 00:29:39.700
Now, it doesn't really matter
what the error code is,

00:29:39.740 --> 00:29:44.510
but you should keep track of it because
what's gonna happen is you're gonna

00:29:44.510 --> 00:29:48.290
return that error code to the converter
and then the converter's gonna note

00:29:48.290 --> 00:29:52.840
that and see that you mean for this
to be the end of the stream for now.

00:29:52.840 --> 00:30:00.100
And then return that error to back to
you through the return value to the call

00:30:00.190 --> 00:30:03.110
to audio converter fill complex data.

00:30:04.190 --> 00:30:07.140
So then after you're all
done and you're about,

00:30:07.140 --> 00:30:10.160
and so there are a couple
other things you need to know.

00:30:10.160 --> 00:30:13.590
If you're going to use the
converter to do another conversion,

00:30:13.650 --> 00:30:16.880
let's say, for example,
you're playing through a data

00:30:16.880 --> 00:30:20.190
stream and you're seeking
around randomly in the stream,

00:30:20.190 --> 00:30:23.550
as you finish each segment,
you're going to need to call the

00:30:23.590 --> 00:30:27.930
audio converter reset function
before you start on the next segment.

00:30:28.590 --> 00:30:32.720
Now, what the audio converter reset
function does is tell the converter

00:30:33.130 --> 00:30:37.080
that you're done with a given segment
and to return the converter to

00:30:37.080 --> 00:30:41.550
its ground state and clear out any
cache data or any other leftovers.

00:30:46.380 --> 00:30:52.210
So the audio converter has a
plug-in API called Audio Codecs.

00:30:52.720 --> 00:30:58.420
And audio codecs are also used to
plug into the audio format API because

00:30:58.510 --> 00:31:04.930
codecs are expected to be the experts
on the data format that they represent.

00:31:07.220 --> 00:31:14.620
So audio codecs are expected to
be the experts on the data format

00:31:14.820 --> 00:31:22.420
API because codecs are expected to
be the experts on the data format

00:31:22.570 --> 00:31:31.480
API because codecs are expected to be
the experts on the data format API.

00:31:33.330 --> 00:31:36.540
So, in your application,
there are a lot of factors

00:31:36.630 --> 00:31:41.260
that go into deciding on what
kind of audio data to use.

00:31:41.260 --> 00:31:45.170
The first ones are the obvious ones,
bit rate and quality.

00:31:45.170 --> 00:31:49.170
For your application,
you may or may not have bandwidth

00:31:49.340 --> 00:31:54.050
limitations on how much space
you have to fit your data into.

00:31:54.050 --> 00:31:58.850
And if you do, you're going to be very
concerned about the bit rate,

00:31:58.860 --> 00:32:01.760
how much space the encoding will take.

00:32:01.840 --> 00:32:06.180
And, you know, most encoders will have
control throttles on bit rate.

00:32:06.180 --> 00:32:10.430
Now, the other things you're going to
want to be concerned about are

00:32:10.670 --> 00:32:14.000
how much does it cost to encode
in terms of CPU and decode,

00:32:14.030 --> 00:32:15.520
for that matter.

00:32:15.520 --> 00:32:19.300
And that's going to need to play
into the logic in your application.

00:32:19.300 --> 00:32:21.990
For example,
you might be very concerned about

00:32:21.990 --> 00:32:25.870
the performance of the decoder if
you're trying to decode MP3 and do

00:32:25.870 --> 00:32:29.550
a bunch of 3D graphics or something
like that at a high frame rate

00:32:29.550 --> 00:32:31.750
in a game or something like that.

00:32:32.170 --> 00:32:36.880
Now, another factor that goes into codec
choice is something that doesn't come up

00:32:36.940 --> 00:32:40.860
in a lot of people's thinking too often,
and that's the data latency.

00:32:40.860 --> 00:32:44.650
Now, what we mean by data
latency is the number,

00:32:44.650 --> 00:32:50.700
is the amount of delay between putting
in the first samples into the encoder

00:32:50.700 --> 00:32:55.360
and how many zeros it takes the encoder
to spit out before it actually gets

00:32:55.450 --> 00:32:58.240
to the beginning of your encoded data.

00:32:58.700 --> 00:33:02.640
And this can vary widely
from codec to codec.

00:33:02.710 --> 00:33:07.930
And if you're in a real-time situation,
such as like iChat,

00:33:08.050 --> 00:33:12.680
you might look at using
something like AAC low delay,

00:33:12.680 --> 00:33:17.760
which is a codec that's been specifically
optimized to reduce data latency.

00:33:17.760 --> 00:33:21.870
The final factor you're probably
going to want to consider is,

00:33:21.870 --> 00:33:26.090
you know, what platform are you on and
what encoders and decoders

00:33:26.160 --> 00:33:28.090
does that platform have?

00:33:30.380 --> 00:33:33.560
The iPhone has some peculiar limitations,
including the fact that it has a

00:33:33.570 --> 00:33:36.850
limited set of decoders available on it.

00:33:36.920 --> 00:33:42.350
And the other interesting aspect
of it is the iPhone only allows you

00:33:42.370 --> 00:33:48.590
to decode a single instance of AAC,
MP3, or Apple lossless at a time.

00:33:48.620 --> 00:33:51.040
That's not one of each at a time.

00:33:51.040 --> 00:33:54.050
That's one of any of the three at a time.

00:33:54.100 --> 00:33:56.870
So you have to keep that in
mind when you're developing

00:33:56.870 --> 00:33:58.430
your iPhone application.

00:33:58.680 --> 00:33:59.720
Thank you.

00:34:00.370 --> 00:34:03.590
Now here's a little chart I put
up that I put together that kind

00:34:03.600 --> 00:34:10.210
of compares and contrasts several
common formats according to the

00:34:10.380 --> 00:34:13.020
The topics I just talked about.

00:34:13.050 --> 00:34:16.690
I threw 16-bit linear PCM up
there just to kind of give you a

00:34:16.810 --> 00:34:18.740
baseline to compare it against.

00:34:18.790 --> 00:34:23.940
Now, just a word about the encoding
cost and decoding cost per channel.

00:34:23.940 --> 00:34:29.420
Those are desktop numbers
on a fairly hefty CPU.

00:34:29.420 --> 00:34:33.980
So you want to take those numbers as kind
of a general way to compare and contrast.

00:34:33.980 --> 00:34:38.540
Don't take them as written in stone for
the CPU that you happen to be working on.

00:34:40.350 --> 00:34:43.850
One interesting format I want
to call out here is ILBC.

00:34:43.850 --> 00:34:49.010
This is a new codec that we've
just added with QuickTime 7.5,

00:34:49.010 --> 00:34:51.160
which just shipped yesterday.

00:34:51.160 --> 00:34:55.700
ILBC is a speech codec, and as such,
it's geared mostly toward

00:34:55.700 --> 00:34:58.280
very low bandwidth situations.

00:34:58.300 --> 00:35:03.230
Among the things that ILBC is
really good at is for dealing

00:35:03.230 --> 00:35:07.900
with lossy packet situations,
such as in a network environment,

00:35:08.090 --> 00:35:12.930
and reconstructing the
data from what you have.

00:35:16.110 --> 00:35:21.070
Now I want to talk a little bit about
reading and writing audio data to a file.

00:35:21.070 --> 00:35:26.620
The aptly named Audio File API provides
the abstraction that you're going to

00:35:26.620 --> 00:35:28.290
use for reading and writing files.

00:35:28.340 --> 00:35:33.800
The Audio File API provides a set of
global properties that describe the

00:35:34.280 --> 00:35:40.400
capabilities of the system that are much
in common with the Audio Format API,

00:35:40.400 --> 00:35:44.900
except that they deal specifically
with the file types that are supported.

00:35:45.380 --> 00:35:50.970
The Audio File API provides a set
of global properties that describe

00:35:50.970 --> 00:35:55.960
the capabilities of the system
that are much in common with the

00:35:56.050 --> 00:35:59.000
file types that are supported.

00:36:08.690 --> 00:36:13.790
In the Audio File API,
files are specified using a CFURL that

00:36:13.890 --> 00:36:15.480
points into the file system.

00:36:15.590 --> 00:36:17.510
They have to be file system URLs.

00:36:17.540 --> 00:36:20.280
You can't pass an HTTP URL or
anything like that.

00:36:20.280 --> 00:36:26.680
And when you're creating a new file,
you need to have the audio file type

00:36:26.680 --> 00:36:29.140
ID of the file you want to create.

00:36:29.140 --> 00:36:34.030
And this is to tell the
Audio File API specifically whether

00:36:34.030 --> 00:36:39.840
you want a WAV file or an MPEG-4 file
or an AIFC file or what have you.

00:36:39.960 --> 00:36:44.120
You also need to have the
ASBD of the format of the data

00:36:44.120 --> 00:36:46.160
you're going to put in the file.

00:36:46.160 --> 00:36:49.720
And then beyond that,
other information about the file

00:36:49.720 --> 00:36:54.150
are going to be set on the file
after you've created it using

00:36:54.150 --> 00:36:56.910
the Audio File Property API.

00:36:57.990 --> 00:37:00.340
Existing files can be opened.

00:37:00.340 --> 00:37:04.950
Just like every other API,
the Audio File API has

00:37:05.150 --> 00:37:08.470
properties in order to access
the information about that file.

00:37:08.470 --> 00:37:12.320
That's how you're going to find the
ASBD and Magic Cookie for the file,

00:37:12.320 --> 00:37:16.280
how you're going to find the
channel layout of the file,

00:37:16.280 --> 00:37:19.420
as well as the packet descriptions, etc.

00:37:19.420 --> 00:37:22.610
And then there's also
properties for metadata,

00:37:22.610 --> 00:37:27.740
such as markers, regions, lyrics,
album covers, pretty much whatever you

00:37:27.740 --> 00:37:29.480
can stick in the file.

00:37:33.620 --> 00:37:35.300
One word of caution.

00:37:35.360 --> 00:37:38.360
When you're using some
kinds of audio files,

00:37:38.370 --> 00:37:42.860
you have to be careful about what
sort of information you query of

00:37:42.860 --> 00:37:48.420
the file in order to avoid taking
some rather large performance hits.

00:38:06.590 --> 00:38:06.760
For example,
files like MPEG-1 files or ADTS or

00:38:06.760 --> 00:38:06.860
AC3 files are self-packetized files.

00:38:06.860 --> 00:38:07.020
In other words, the packetization is
embedded in the file.

00:38:07.020 --> 00:38:07.460
There's no external framing
information in those files.

00:38:08.180 --> 00:38:11.540
So in order to discover all
the information about all

00:38:11.610 --> 00:38:15.260
the packets in the files,
you have to go through and pretty

00:38:15.420 --> 00:38:18.080
much parse every packet in the file.

00:38:18.620 --> 00:38:22.990
And as you might imagine,
If you're looking for longer files,

00:38:22.990 --> 00:38:27.260
that can take a significant amount of
time when all you want to do is open

00:38:27.260 --> 00:38:29.380
the file up and ask how long it is.

00:38:29.380 --> 00:38:33.530
So what we provide in the audio file
API are some less time-consuming

00:38:33.530 --> 00:38:38.070
alternatives that give you some
estimations as to some of these important

00:38:38.070 --> 00:38:40.810
quantities you might want to know.

00:38:40.810 --> 00:38:44.240
For example,
there's a property to get the estimated

00:38:44.340 --> 00:38:48.060
upper bound on the packet size,
which is how you might want

00:38:48.060 --> 00:38:50.040
to govern the packet size.

00:38:50.920 --> 00:38:52.350
So in order to do that,
you have to go through and pretty

00:38:52.360 --> 00:38:53.410
much parse every packet in the file.

00:38:53.410 --> 00:38:53.420
There's no external framing
information in those files.

00:38:53.610 --> 00:38:58.560
And then we also have a property
for returning an estimated duration,

00:38:58.560 --> 00:39:03.350
which will not need to parse
the whole file to get to it.

00:39:04.260 --> 00:39:09.830
So, reading and writing in the audio file
API are pretty much mirror images of

00:39:09.840 --> 00:39:13.040
each other in terms of the operation.

00:39:13.440 --> 00:39:17.870
Both calls will block until they're
complete and both calls can either go

00:39:18.010 --> 00:39:23.460
through the file system cache or not,
depending on what you want.

00:39:24.050 --> 00:39:28.800
Now, there are two styles of reads
and writes in the Audio File API,

00:39:28.800 --> 00:39:32.360
one that goes by bytes and
one that goes by packets.

00:39:32.440 --> 00:39:38.330
You can use the one that uses bytes,
but in keeping with the

00:39:38.330 --> 00:39:42.820
format agnosticism theme,
you really want to do your I.O.

00:39:42.820 --> 00:39:44.400
in terms of whole packets.

00:39:45.890 --> 00:39:49.640
For instance,
it's really the only way to reasonably

00:39:49.640 --> 00:39:53.480
handle variable bitrate data,
and it also allows it much

00:39:53.580 --> 00:39:58.330
easier to manipulate time because
packets represent a unit of time,

00:39:58.330 --> 00:40:01.610
as well as being an
organizational concept.

00:40:03.170 --> 00:40:06.710
Now, another operation the
Audio File API provides is a

00:40:06.720 --> 00:40:08.560
means to optimize the file.

00:40:08.990 --> 00:40:13.230
And so when you optimize the--tell
the Audio File API to optimize a file,

00:40:13.560 --> 00:40:19.930
it'll go through and relay the file
out on disk to optimize the access

00:40:19.930 --> 00:40:24.150
performance so that the data is
offset into the file at an appropriate

00:40:24.280 --> 00:40:27.840
place so that you don't take cache
hits or cache misses in order to

00:40:27.840 --> 00:40:30.190
read them and stuff like that.

00:40:31.850 --> 00:40:38.620
So we also provide a high-level
API called the Extended Audio File API.

00:40:38.620 --> 00:40:42.400
Now, this API,
unlike the regular audio file API,

00:40:42.400 --> 00:40:43.940
is only available on the desktop.

00:40:43.940 --> 00:40:50.740
And what the Extended Audio File does
is it unites an audio file

00:40:50.740 --> 00:40:52.180
with an audio converter.

00:40:52.180 --> 00:40:57.480
And what this does is it allows
you to think about the operations

00:40:57.480 --> 00:41:01.440
on the file as if you were
dealing purely with linear PCM.

00:41:01.860 --> 00:41:07.500
Even if the file was encoded in something
like AAC or lossless or what have you.

00:41:07.540 --> 00:41:13.500
The Extended Audio File has analogous
routines to the regular audio file

00:41:13.500 --> 00:41:18.210
API for reading and writing to both
existing files and creating new files.

00:41:20.830 --> 00:41:25.040
And then, just like the audio once you've
created or opened a file with

00:41:25.040 --> 00:41:28.370
the Extended Audio File API,
you're going to use properties in order

00:41:28.670 --> 00:41:30.590
to get more information about that.

00:41:30.630 --> 00:41:34.260
And in particular,
there's one property you need to set

00:41:34.260 --> 00:41:38.890
in order to tell the audio file what
format of data you want to give it,

00:41:38.960 --> 00:41:41.540
or in the case of reading,
what format of data you

00:41:41.540 --> 00:41:42.740
want it to give you.

00:41:42.780 --> 00:41:46.050
And that property is really important
or else you're going to miss,

00:41:46.060 --> 00:41:49.360
or else the things aren't going
to work quite the way you expect.

00:41:50.680 --> 00:41:53.670
Now, as I said,
the point of the Extended Audio File is

00:41:53.790 --> 00:41:57.500
to make it so that you are conceptually
thinking about the data in the

00:41:57.950 --> 00:41:59.450
file as if it was linear PCM.

00:41:59.500 --> 00:42:03.500
Now, as such,
all the operations in the API are

00:42:03.500 --> 00:42:06.520
handled in terms of sample frames.

00:42:06.620 --> 00:42:10.850
One thing about the reads
and writes are also very

00:42:10.980 --> 00:42:13.940
analogous to the Audio File API.

00:42:13.940 --> 00:42:18.870
The difference is that
while reads block always,

00:42:20.460 --> 00:42:23.460
the writes in the Extended
Audio File come in two basic flavors.

00:42:23.540 --> 00:42:26.470
You have the blocking flavor,
which is pretty much the same as

00:42:26.480 --> 00:42:30.790
what you get with the Audio File API,
but you also have a non-blocking form.

00:42:30.820 --> 00:42:35.220
Now, the interesting thing about the
non-blocking form is that it will offload

00:42:35.220 --> 00:42:39.720
the work of writing to the audio file to
a separate worker thread on your behalf.

00:42:39.820 --> 00:42:45.330
Now, the cool thing there is that this is
safe to use in an I/O proc context.

00:42:45.340 --> 00:42:48.980
And an I/O proc context,
which we'll talk a little bit

00:42:48.980 --> 00:42:51.720
more about in a few minutes,
is a place where you're up

00:42:51.720 --> 00:42:56.330
against a real-time deadline,
and doing things like blocking

00:42:56.330 --> 00:42:58.930
is going to get in the way.

00:43:00.200 --> 00:43:05.220
So, now I want to talk a little bit about
applying effects to some audio data.

00:43:05.240 --> 00:43:07.900
Now, to do that, you're going to be
talking about audio units.

00:43:07.900 --> 00:43:11.790
Audio units are our plug-in
API that encapsulates signal

00:43:11.790 --> 00:43:13.650
processing operations.

00:43:13.650 --> 00:43:15.570
Audio units are great.

00:43:15.640 --> 00:43:20.530
They can be hooked together either
manually or using an AU graph to

00:43:20.530 --> 00:43:23.720
control the connections between them.

00:43:24.980 --> 00:43:28.920
Now, as I mentioned before,
this is a processing context.

00:43:28.920 --> 00:43:33.140
So, the canonical format that's used
in this case is going to be the

00:43:33.230 --> 00:43:35.360
audio unit canonical format.

00:43:35.360 --> 00:43:41.150
Now, there's some exceptions to this,
and those are the converter

00:43:41.360 --> 00:43:43.860
units and the output units.

00:43:43.860 --> 00:43:47.720
Now, the interesting things about
those two categories of audio

00:43:47.720 --> 00:43:51.420
units is that they also encompass
an audio converter in them,

00:43:51.490 --> 00:43:54.370
so that they can handle
other formats as well.

00:43:54.980 --> 00:43:57.690
well as just the canonical format.

00:43:57.730 --> 00:44:03.110
Now, on the desktop,
audio units also provide a GUI component

00:44:03.310 --> 00:44:08.330
that your app can use to display to
the user to allow the user to interact

00:44:08.330 --> 00:44:10.190
with the parameters of the audio unit.

00:44:13.530 --> 00:44:15.760
Parameters are exactly what you think.

00:44:15.840 --> 00:44:19.300
Those are the,
they're much like properties except

00:44:19.450 --> 00:44:24.360
that they represent the portion,
the controllable portions of

00:44:24.370 --> 00:44:26.710
the signal processing algorithm.

00:44:26.710 --> 00:44:30.170
For example, if it was,
if you're talking about an

00:44:30.170 --> 00:44:34.360
audio unit that is using an EQ,
the parameters are things like

00:44:34.360 --> 00:44:38.640
the cutoff frequency and the
amplitude and the resonance.

00:44:40.740 --> 00:44:45.060
Parameters, unlike properties,
are always the same value and that value

00:44:45.060 --> 00:44:48.000
is a single 32-bit floating point number.

00:44:48.000 --> 00:44:52.050
However, the range and meaning of this
value is going to be different

00:44:52.280 --> 00:44:54.120
from parameter to parameter.

00:44:54.120 --> 00:44:57.860
And the audio unit can fill
out an audio unit parameter

00:44:57.860 --> 00:45:02.660
infrastructure for you in order to
describe what the valid range is,

00:45:02.720 --> 00:45:06.070
what the units of the parameter are,
etc., etc.

00:45:06.260 --> 00:45:11.870
Audio units also allow you to
change parameters by scheduling

00:45:11.940 --> 00:45:16.920
them in the future as well as being
able to ramp them from one value

00:45:16.920 --> 00:45:18.920
to another value continuously.

00:45:19.020 --> 00:45:21.870
And these two operations
are really important for

00:45:22.180 --> 00:45:25.910
applications that are trying to
automate their signal processing,

00:45:26.120 --> 00:45:29.680
you know, such as in a digital audio
workstation environment.

00:45:32.090 --> 00:45:36.580
Audio units organize all
their I/O into buses.

00:45:36.690 --> 00:45:42.220
Each audio unit will always have
some number of input buses and

00:45:42.270 --> 00:45:44.000
some number of output buses.

00:45:44.280 --> 00:45:50.380
Each individual bus has some number of
channels in its stream and may or may

00:45:50.440 --> 00:45:56.320
not have an audio channel layout that
describes the usage of the channels

00:45:56.320 --> 00:45:59.390
in that bus in surround contexts.

00:46:01.190 --> 00:46:05.470
Now, when you're dealing with the
property API and you're talking

00:46:05.880 --> 00:46:11.090
about a bus-related property,
you have to make sure that you address

00:46:11.090 --> 00:46:14.470
the bus on each bus on its own element.

00:46:16.560 --> 00:46:19.810
So, in order to use an audio unit,
the first thing you have to

00:46:19.810 --> 00:46:22.740
do is locate it and find the
one that you're interested in.

00:46:22.740 --> 00:46:27.330
Now, one thing we'll say up front
is the component manager has

00:46:27.330 --> 00:46:31.450
been deprecated on the desktop,
and it's not available

00:46:31.450 --> 00:46:33.350
at all on the iPhone.

00:46:34.810 --> 00:46:38.930
So, in order to use the various
audio components on the system,

00:46:38.930 --> 00:46:43.360
such as audio units and audio codecs,
you need to use the new API in the audio

00:46:43.830 --> 00:46:47.070
unit framework called audio-component.h.

00:46:47.100 --> 00:46:51.490
The audio component API is
basically a one-for-one replacement

00:46:51.500 --> 00:46:53.650
for the component manager.

00:46:53.650 --> 00:46:56.390
For example,
you're going to use audio component

00:46:56.390 --> 00:46:58.880
findNext instead of findNext component.

00:46:58.880 --> 00:47:03.110
You're going to use audio component
instance new instead of open component.

00:47:03.670 --> 00:47:08.320
Now, you have to be sure that as a
host that you don't try to mix

00:47:08.320 --> 00:47:13.280
and match audio components with
component manager components.

00:47:13.380 --> 00:47:16.420
They are not interchangeable
at the host level,

00:47:16.470 --> 00:47:19.990
even though underneath they
may still be implemented as

00:47:19.990 --> 00:47:22.190
the same underlying API object.

00:47:22.280 --> 00:47:24.780
Now, don't worry.

00:47:24.780 --> 00:47:27.460
Even though the component
manager is deprecated,

00:47:27.500 --> 00:47:31.360
your existing component manager code
is still going to continue to work,

00:47:31.360 --> 00:47:33.120
but we would really advise
you to start moving on.

00:47:33.130 --> 00:47:35.820
You can use the component
manager to start moving away

00:47:35.820 --> 00:47:38.990
from using the component manager
for audio unit discovery.

00:47:40.720 --> 00:47:45.490
So once you've found your audio unit,
you configure the audio unit by,

00:47:45.490 --> 00:47:46.920
guess what, using properties.

00:47:46.940 --> 00:47:51.860
And this is the time where you're
going to set the sample rate of

00:47:51.860 --> 00:47:57.260
the audio unit and as well as the
maximum number of frames that you're

00:47:57.260 --> 00:48:00.960
going to ever call the audio unit
to render for in one single call.

00:48:00.980 --> 00:48:03.920
Now, both properties are important,
you know,

00:48:03.920 --> 00:48:07.870
the sample rate for the obvious reason,
but you have to be sure you set

00:48:07.970 --> 00:48:10.070
the max frames on the audio unit.

00:48:10.600 --> 00:48:13.890
Because if you don't,
you might get an unexpected error

00:48:13.890 --> 00:48:17.940
on down the road when you're on the
real-time thread trying to render

00:48:17.940 --> 00:48:22.000
because you're asking for more data than
the audio unit is prepared to render for.

00:48:22.040 --> 00:48:26.490
Now, other properties for the audio
unit you can then set up such as,

00:48:26.750 --> 00:48:30.090
you know,
algorithmic specific attributes such as,

00:48:30.130 --> 00:48:34.320
you know, reverb time or EQ cutoff
frequency or whatever.

00:48:34.320 --> 00:48:38.090
And this is also the time at
which you're going to set up

00:48:38.160 --> 00:48:40.560
the audio unit's connections.

00:48:40.600 --> 00:48:45.940
And audio units support several different
flavors of callback and you're going

00:48:46.010 --> 00:48:50.940
to want to install your callbacks
at the initialization time as well.

00:48:53.330 --> 00:48:56.610
So once you finish
configuring your audio unit,

00:48:56.610 --> 00:49:00.360
you then call Audio Unit Initialize
to tell the audio unit that you're

00:49:00.360 --> 00:49:03.950
done configuring it and you want it to
get ready to process some audio data.

00:49:04.810 --> 00:49:08.450
This is the signal to the
audio unit that it can go ahead

00:49:08.450 --> 00:49:16.710
and allocate large tables,
delay lines, load impulse response files,

00:49:16.800 --> 00:49:21.850
do other things that might take up a
lot of space or otherwise might take

00:49:21.850 --> 00:49:24.600
some time in order to accomplish.

00:49:25.010 --> 00:49:27.640
And you need to do this,
things that need to be done

00:49:27.640 --> 00:49:29.470
before you start rendering.

00:49:31.040 --> 00:49:33.980
And then in order to tell
the audio unit to render,

00:49:34.030 --> 00:49:36.260
you just call audio unit render.

00:49:36.260 --> 00:49:40.750
And when you call it,
you have to pass in the timestamp

00:49:41.180 --> 00:49:44.810
that indicates the relative
stream position that you want

00:49:44.810 --> 00:49:46.480
the audio unit to render for.

00:49:46.480 --> 00:49:50.720
You also have to pass in the number of
frames you want the audio unit to render.

00:49:50.720 --> 00:49:54.100
And again, to be careful,
you have to make sure you don't

00:49:54.100 --> 00:49:57.710
pass a number of frames larger
than the max frames that you set

00:49:57.710 --> 00:50:00.060
in the initialization sequence.

00:50:01.900 --> 00:50:04.770
And then you also have to pass
an audio buffer list in in order

00:50:04.910 --> 00:50:08.600
to receive the buffers from the
rendered buffers from the audio unit.

00:50:08.670 --> 00:50:13.620
Now, the buffers that you pass in the
audio buffer list can come two ways.

00:50:13.620 --> 00:50:17.210
You can either pass pointers
to actual blocks of memory,

00:50:17.210 --> 00:50:20.560
in which case you're telling
the audio unit that you want

00:50:20.670 --> 00:50:24.410
it to render its output into
the memory that you provide it.

00:50:24.510 --> 00:50:26.710
Or you can pass null pointers.

00:50:26.710 --> 00:50:30.880
And this is telling the audio unit
that you want it to render its output

00:50:30.880 --> 00:50:30.880
into the memory that you provide it.

00:50:30.920 --> 00:50:35.420
into its own internal buffers and to
provide you with pointers into them.

00:50:35.420 --> 00:50:43.040
Now, this is useful for optimizing the
data flow and memory copies in your

00:50:43.040 --> 00:50:46.580
various overall chain of audio units.

00:50:46.720 --> 00:50:51.890
Now, then after you're done rendering,
you, much like the audio converter,

00:50:51.890 --> 00:50:55.500
there's an audio unit reset
call that will return the

00:50:55.500 --> 00:50:57.740
audio unit to its ground state.

00:50:57.750 --> 00:51:01.570
And this will also kill
things like reverb tails,

00:51:01.570 --> 00:51:07.040
stop delays, clear out FFT buffers,
what have you.

00:51:09.600 --> 00:51:11.870
So that's how you get the data out.

00:51:12.160 --> 00:51:15.710
To get data into the audio unit,
you basically have two choices.

00:51:15.710 --> 00:51:20.160
You can either get the data
from another audio unit,

00:51:20.180 --> 00:51:24.860
in which case you can make a manual
connection in the setup phase of

00:51:24.860 --> 00:51:30.110
your audio unit using kAudioUnit
property make connection to directly

00:51:30.110 --> 00:51:35.280
connect the output bus from one audio
unit to the input bus of another.

00:51:36.360 --> 00:51:38.780
Or you could use the AUGraph API.

00:51:38.780 --> 00:51:45.030
The AUGraph API's whole reason for
being is to provide for the management

00:51:45.030 --> 00:51:51.060
of the connections in what can be a
very complicated graph of connections.

00:51:52.970 --> 00:51:55.930
Now,
the other way you can get audio into your

00:51:55.940 --> 00:51:58.140
audio unit is to use a render callback.

00:51:58.220 --> 00:52:03.370
Now, you install render callback using
KAudioUnit property set render callback.

00:52:03.430 --> 00:52:07.520
And much like the input data
callback for the audio converter

00:52:07.520 --> 00:52:11.520
that we talked about earlier,
this is the function that the

00:52:11.520 --> 00:52:15.740
audio unit will then call whenever
it needs data for that bus.

00:52:16.220 --> 00:52:18.150
Now, you have to be careful.

00:52:18.260 --> 00:52:21.390
These two methods,
whether you use a connection to

00:52:21.390 --> 00:52:25.820
another audio unit or a callback,
those are mutually exclusive.

00:52:25.820 --> 00:52:30.210
You can only do one or the other
on each bus of the audio unit.

00:52:30.240 --> 00:52:33.550
But you can mix and
match on a per bus basis.

00:52:36.090 --> 00:52:38.640
So we ship a lot of audio
units in the system that have

00:52:38.640 --> 00:52:44.160
a lot of different kinds,
starting with effects and music effects,

00:52:44.160 --> 00:52:49.260
such as these are your delays, EQs,
high-pass, low-pass filters,

00:52:49.260 --> 00:52:51.680
dynamics processing, what have you.

00:52:52.230 --> 00:52:55.250
The difference between a music
effect and an effect is that music

00:52:55.250 --> 00:52:57.250
effects can be controlled using MIDI.

00:52:57.250 --> 00:53:00.770
And we have panner units.

00:53:00.820 --> 00:53:06.270
Panner units are there for applying
spatialization and other kinds of sound

00:53:06.890 --> 00:53:09.320
positioning algorithms to a sound.

00:53:10.000 --> 00:53:13.630
We have format converters,
which I mentioned earlier.

00:53:13.690 --> 00:53:17.130
Format converters,
in addition to being able to accept

00:53:17.410 --> 00:53:23.100
very non-canonical audio formats,
format converters can also be flexible in

00:53:23.160 --> 00:53:28.530
the way they pull for their input data,
and as such are also used where

00:53:28.530 --> 00:53:31.820
you find audio units that do
things like pitch shifting,

00:53:32.130 --> 00:53:36.660
time compression expansion,
and other kinds of algorithms

00:53:36.660 --> 00:53:39.160
that manipulate time.

00:53:39.350 --> 00:53:42.150
We also have generators
and music devices.

00:53:42.280 --> 00:53:46.180
These are sources of audio,
of new audio in an audio graph,

00:53:46.310 --> 00:53:51.160
and music devices in particular represent
things like software synthesizers.

00:53:51.290 --> 00:53:56.640
Generators can also represent other
things like file players or the

00:53:56.640 --> 00:54:02.110
scheduled slice playing audio unit,
which we ship on the desktop.

00:54:02.110 --> 00:54:02.110
Jeff

00:54:02.260 --> 00:54:06.180
And then we also have mixer units,
and you're going to find that we

00:54:06.180 --> 00:54:09.900
ship several different kind of
mixers on each of the platforms,

00:54:09.900 --> 00:54:14.080
and that's always going to at least
be a stereo mixer and a 3D mixer.

00:54:14.080 --> 00:54:17.300
On the desktop,
we also ship a few other mixers,

00:54:17.310 --> 00:54:21.130
including the Matrix mixer,
which is really good for doing

00:54:21.490 --> 00:54:24.080
routing and other complex mix setups.

00:54:25.520 --> 00:54:27.100
And then finally, we have output units.

00:54:27.100 --> 00:54:30.430
Output units are the sinks in a graph.

00:54:30.430 --> 00:54:33.870
That's where the data goes in
order to go out to the hardware or

00:54:33.870 --> 00:54:37.660
to be pulled out of the graph and
written to a file or what have you.

00:54:40.000 --> 00:54:43.800
So now I want to finish up by kind of
going over a little bit about how you

00:54:43.800 --> 00:54:45.550
get your audio data out to the speaker.

00:54:45.550 --> 00:54:51.500
Now, there are probably more APIs in
Core Audio for playing audio than just

00:54:51.500 --> 00:54:54.190
about doing any other single task.

00:54:55.980 --> 00:54:59.940
So the first method I want to
talk about is using the audio HAL.

00:55:00.000 --> 00:55:05.260
Now, the HAL is among the lowest-level
pieces of software we have in the stack.

00:55:05.260 --> 00:55:08.310
And as such,
the HAL's job is really to be focused

00:55:08.450 --> 00:55:13.410
on providing access to the audio
device on the audio device's terms.

00:55:14.180 --> 00:55:18.060
As such, you're not going to find that
the HAL provides much in the

00:55:18.070 --> 00:55:20.080
way of convenience functions.

00:55:20.140 --> 00:55:23.300
It doesn't provide much
in the way of utilities.

00:55:24.900 --> 00:55:25.070
And it's not going to provide much
in the way of the software itself.

00:55:25.460 --> 00:55:26.350
But it's also very chatty.

00:55:26.360 --> 00:55:31.200
The HAL provides a lot of notifications,
and applications that are talking

00:55:31.270 --> 00:55:34.790
directly to the HAL are going to be
expected to sign up for and handle

00:55:34.970 --> 00:55:37.310
all these notifications appropriately.

00:55:39.130 --> 00:55:41.690
Now, when you're dealing with mixing
for dealing with the HAL,

00:55:41.750 --> 00:55:44.050
you're pretty much,
as with anything else in the HAL,

00:55:44.050 --> 00:55:44.730
on your own.

00:55:44.730 --> 00:55:48.040
In your IOPROC,
which is what the HAL calls

00:55:48.040 --> 00:55:53.490
in order to get data from you,
you also have to be very, very careful.

00:55:55.440 --> 00:55:59.980
This PROC is called on a real-time
thread and is up against a hard deadline.

00:55:59.990 --> 00:56:03.700
So you can't do things on that
thread that are going to cause it

00:56:03.700 --> 00:56:07.940
to potentially miss that deadline,
like block while you're

00:56:07.960 --> 00:56:11.200
trying to lock a lock,
spin on a spin lock,

00:56:11.350 --> 00:56:14.680
or do a file I.O., or what have you.

00:56:15.310 --> 00:56:18.950
Now, given all the general complexity
of using the HAL directly,

00:56:18.950 --> 00:56:22.190
we really don't recommend
that many applications do it.

00:56:23.280 --> 00:56:26.620
We provide higher-level abstractions
that make it a bit easier.

00:56:26.620 --> 00:56:30.710
The other thing is that the HAL is
not available to you on the iPhone,

00:56:30.710 --> 00:56:33.250
so you can't even go there on the phone.

00:56:35.290 --> 00:56:40.640
So to step up from the how,
we provide output audio units.

00:56:40.640 --> 00:56:44.010
Now, the only difference between an
output audio unit and a regular

00:56:44.010 --> 00:56:47.960
audio unit is the addition of
these two transport methods.

00:56:47.960 --> 00:56:51.400
Audio output unit start
and audio output unit stop.

00:56:51.400 --> 00:56:53.520
And they do what their names imply.

00:56:53.550 --> 00:56:56.250
They start and stop IO.

00:56:56.220 --> 00:57:01.440
There are two basic flavors
of output unit that you're

00:57:01.500 --> 00:57:02.440
going to want to deal with.

00:57:02.660 --> 00:57:06.570
AU-HAL,
which is only available on the desktop.

00:57:06.570 --> 00:57:10.640
And this audio unit is what does
all the really hard and complicated

00:57:10.640 --> 00:57:16.960
work I was talking about earlier
with being a proper HAL client.

00:57:16.960 --> 00:57:16.960
And it does all this
so you don't have to.

00:57:16.960 --> 00:57:16.960
So for most of you,
it's going to be a little

00:57:16.960 --> 00:57:16.960
bit more complicated.