WEBVTT

00:00:20.740 --> 00:00:22.220
Hello and welcome.

00:00:22.350 --> 00:00:25.060
This is the advanced OpenCL course
where we're going to be taking you

00:00:25.140 --> 00:00:29.600
into the nuts and bolts of how to
use OpenCL within your applications.

00:00:29.690 --> 00:00:32.980
My name is Derek Gerstmann,
and let's get started.

00:00:33.130 --> 00:00:36.440
So I think with the introduction
of multi-core processors and

00:00:36.520 --> 00:00:39.530
especially programmable GPUs,
there's sort of been this looming

00:00:39.640 --> 00:00:43.310
question that I think a lot of
us have been asking ourselves.

00:00:43.430 --> 00:00:45.900
What if we could use all the
compute devices in our Macs

00:00:45.940 --> 00:00:48.390
to make our applications run
faster and do more stuff?

00:00:48.980 --> 00:00:51.340
Wouldn't that be great?

00:00:51.340 --> 00:00:54.050
And ultimately,
I think OpenCL is one giant step

00:00:54.160 --> 00:00:59.400
forward in providing a solution and an
answer to this overwhelming question.

00:00:59.400 --> 00:01:01.730
So in this session,
what we're going to be getting

00:01:01.730 --> 00:01:05.610
into is how to develop kernels and
applications with OpenCL and how

00:01:05.620 --> 00:01:10.080
to combine OpenCL with OpenGL for
advanced visual processing.

00:01:10.950 --> 00:01:14.930
So OpenCL is provided as a
framework that fits within the

00:01:14.930 --> 00:01:17.330
graphics and media layer for OS X.

00:01:17.400 --> 00:01:21.560
It's a first class citizen and sits
alongside all the other frameworks that

00:01:21.560 --> 00:01:23.440
you're probably already familiar with.

00:01:23.550 --> 00:01:26.690
In addition,
OpenCL has been designed to work

00:01:26.690 --> 00:01:32.550
efficiently with OpenGL so you can share
data for advanced visual processing.

00:01:33.500 --> 00:02:32.000
[Transcript missing]

00:02:33.430 --> 00:02:36.930
Some basic OpenCL concepts that I'm going
to be using throughout my slides here

00:02:37.340 --> 00:02:42.260
are going to get into address spaces
as well as threads and thread blocks.

00:02:42.260 --> 00:02:47.020
In terms of address spaces,
OpenCL describes a memory hierarchy

00:02:47.020 --> 00:02:54.910
that maps onto current generation
compute devices where threads have

00:02:55.820 --> 00:02:59.640
varying scopes in terms of how
they can access and modify memory.

00:02:59.640 --> 00:03:02.610
At the highest level,
we've got global memory,

00:03:02.610 --> 00:03:05.460
which is read/write for
all threads in a kernel.

00:03:05.820 --> 00:03:08.740
We also have local memory,
which is read/write for all

00:03:08.890 --> 00:03:10.790
threads running in a thread block.

00:03:11.120 --> 00:03:14.100
And then finally, we have private memory,
which is read/write only for a

00:03:14.100 --> 00:03:16.400
single thread in its local scope.

00:03:16.720 --> 00:03:19.730
And the hierarchy here that we're
describing in terms of threads

00:03:20.190 --> 00:03:23.380
is indicated by this diagram
that we've got here on the right,

00:03:23.380 --> 00:03:34.180
where a thread is a single execution...
a single work unit executing on a device,

00:03:34.190 --> 00:03:37.740
which is included within
a group of threads,

00:03:37.860 --> 00:03:43.080
which is running on a compute device,
which can be composed of within a

00:03:43.080 --> 00:03:45.360
logical grouping called a device group.

00:03:45.500 --> 00:03:49.220
So a device group consists
of multiple compute devices,

00:03:49.280 --> 00:03:55.060
which have multiple thread groups,
which contain multiple threads.

00:03:55.200 --> 00:03:59.560
And ultimately,
this is how OpenCL exposes this.

00:03:59.560 --> 00:04:02.250
these device resources.

00:04:02.610 --> 00:04:04.060
So some basic OpenCL usage.

00:04:04.310 --> 00:04:06.910
First thing that we need to do is
connect to one or more devices,

00:04:07.030 --> 00:04:10.100
create a device group,
create a context which provides

00:04:10.100 --> 00:04:13.440
us with a command stream for
submitting to a specific device,

00:04:13.460 --> 00:04:17.540
allocate some device memory for
storing the results of a computation,

00:04:17.580 --> 00:04:21.530
load and compile a compute kernel,
set our kernel arguments,

00:04:21.540 --> 00:04:23.640
and then execute the
kernel and use the results.

00:04:23.640 --> 00:04:27.050
And then we can just repeat as
much as necessary to get the

00:04:27.050 --> 00:04:28.990
results of our computation.

00:04:29.010 --> 00:04:30.600
And then finally,
we just need to shut down

00:04:30.660 --> 00:04:32.130
and release system resources.

00:04:32.500 --> 00:04:33.660
Thank you.

00:04:33.920 --> 00:04:37.700
So to start, let's just go through a very
basic example for biasing

00:04:37.700 --> 00:04:38.870
and scaling some coordinates.

00:04:38.890 --> 00:04:42.480
You can think of this as generating
UV coordinates for texture maps or

00:04:42.480 --> 00:04:46.640
just a uniform two-dimensional mapping.

00:04:46.760 --> 00:04:51.040
So here we have a very simple kernel,
which is going to be executing

00:04:51.040 --> 00:04:53.450
across a two-dimensional domain.

00:04:53.640 --> 00:04:59.000
Here we've got three parameters, bias,
scale, and result.

00:04:59.140 --> 00:05:02.180
Bias and scale are going to be two
constant values that we're going

00:05:02.180 --> 00:05:04.110
to use to modify our coordinates.

00:05:04.330 --> 00:05:11.190
Result is an array of
two-dimensional float values.

00:05:11.410 --> 00:05:18.300
You can notice here that we have a
global specifier for this parameter,

00:05:18.300 --> 00:05:23.880
which is indicating the memory
address scope for this value.

00:05:23.950 --> 00:05:29.120
Global means that it's going to be
allocated from the memory address scope.

00:05:29.150 --> 00:05:34.060
It's from the application side,
so that all threads are going

00:05:34.060 --> 00:05:35.620
to have read/write access.

00:05:35.660 --> 00:05:39.060
And then finally,
you notice the kernel intrinsic here,

00:05:39.080 --> 00:05:44.470
which is dictating that this
method will be exported and can be

00:05:44.530 --> 00:05:46.980
called from the application side.

00:05:47.110 --> 00:05:50.270
And as far as what this
kernel is actually doing,

00:05:50.490 --> 00:05:55.880
we're using the global thread ID in
two dimensions and scaling that

00:05:55.950 --> 00:05:58.310
based on our global domain size.

00:05:58.440 --> 00:05:58.920
So we're going to use this global
thread ID in two dimensions,

00:05:58.920 --> 00:05:58.980
and scaling that based on
our global domain size,

00:05:58.980 --> 00:05:59.120
and scaling that based on
our global domain size,

00:05:59.150 --> 00:06:04.420
to normalize the coordinates,
which are then biased and scaled,

00:06:04.440 --> 00:06:07.680
and then stored within the result array.

00:06:09.850 --> 00:06:12.120
So now we've got our compute kernel.

00:06:12.120 --> 00:06:14.000
Let's use it.

00:06:14.000 --> 00:06:18.340
So first we need to connect
to our compute device,

00:06:18.340 --> 00:06:23.220
which in this case,
we're going to request a single GPU,

00:06:23.400 --> 00:06:28.100
create a context for our command stream,
and then allocate some device memory.

00:06:28.160 --> 00:06:37.390
Here we're going to create a 128
by 128 two-dimensional array,

00:06:37.400 --> 00:06:45.670
which we're going to ask for as
read/write usage in terms of being able

00:06:45.670 --> 00:06:50.630
to both read and modify the array values.

00:06:50.640 --> 00:06:52.780
And then finally,
we're going to create our program

00:06:52.960 --> 00:06:57.540
from a source string that's just
loaded as a standard C string,

00:06:57.550 --> 00:07:01.880
currently in host memory,
build and compile that executable,

00:07:01.880 --> 00:07:05.300
and then ask for the entry
point for our kernel method

00:07:05.300 --> 00:07:07.290
that we declared previously,
which was that bias_c.

00:07:07.400 --> 00:07:10.140
and scale core 2D method.

00:07:13.110 --> 00:07:15.380
Next,
we just need to set our kernel arguments.

00:07:15.380 --> 00:07:20.410
So these are application-dependent
values that we would normally call

00:07:20.410 --> 00:07:22.810
within just a normal function.

00:07:22.980 --> 00:07:25.480
We just need to pass these on to OpenCL.

00:07:25.480 --> 00:07:29.760
So for our bias and scale values,
we're going to use minus 1

00:07:29.760 --> 00:07:35.060
and minus 1 for our bias,
and scale is going to be 2.0 and 2.0.

00:07:35.060 --> 00:07:38.770
So this will modify the origin,
set it back,

00:07:38.840 --> 00:07:44.210
and create values ranging from
negative 1 to positive 1 across

00:07:44.210 --> 00:07:46.180
a two-dimensional domain.

00:07:46.430 --> 00:07:53.920
So we just pack our arg values
by passing in the memory location

00:07:54.250 --> 00:07:58.640
for each one of these variables,
and then specify the total size

00:07:58.810 --> 00:08:01.590
in bytes for each one of these.

00:08:07.500 --> 00:08:09.260
So now that we've
passed in our arguments,

00:08:09.330 --> 00:08:11.640
we just need to specify
our global domain size,

00:08:11.640 --> 00:08:15.210
the number of threads that we wish
to issue across each dimension.

00:08:15.280 --> 00:08:19.130
This is handled by the global
dimensions and local dimensional

00:08:19.140 --> 00:08:20.400
arrays that I specify here.

00:08:20.400 --> 00:08:23.900
So we're just going to keep things
simple and issue 128 by 128.

00:08:23.900 --> 00:08:29.480
And notice the number two there,
which is being used to indicate that we

00:08:29.480 --> 00:08:33.040
are executing a two-dimensional kernel.

00:08:33.770 --> 00:08:38.270
And then once this finishes,
we can read the data back into host

00:08:38.690 --> 00:08:43.200
memory and use the results for whatever
we wish to within our application.

00:08:45.030 --> 00:08:50.230
And if we examine this two-dimensional
array as a bitmap or an image,

00:08:50.250 --> 00:08:54.330
we can see here that we've generated
a gradient which has values ranging

00:08:54.340 --> 00:08:59.290
from minus one to minus one,
to positive one and positive one.

00:09:00.990 --> 00:09:04.620
So let's take this further and
actually see if we can use this

00:09:04.700 --> 00:09:07.170
to generate some Perlin noise.

00:09:07.500 --> 00:09:11.490
So here we've got a compute kernel,
and this is going to be implementing

00:09:11.690 --> 00:09:13.500
Perlin's improved noise function.

00:09:13.600 --> 00:09:18.730
And we're going to start off by declaring
some arrays of constant data that

00:09:18.730 --> 00:09:23.500
we're going to reference as basically
a lookup table within our compute kernel.

00:09:23.640 --> 00:09:27.120
So the first array here is p,
which is just Perlin's

00:09:27.120 --> 00:09:32.660
recommended permutation table,
which contains pseudo-random

00:09:32.660 --> 00:09:37.500
numbers of a size of 256.

00:09:37.720 --> 00:09:41.650
Next we're going to create an
array of gradient directions,

00:09:41.840 --> 00:09:47.240
which indicate the direction from
the center of a cube to its edges.

00:09:47.500 --> 00:09:52.810
And we're going to extend it to 16
so we can avoid a division by 12.

00:09:53.320 --> 00:09:58.490
Next we have our smoothing kernel,
which is just a cubic spline smoothing

00:09:58.490 --> 00:10:04.300
function to combine values that
we calculate across lattice cells.

00:10:04.300 --> 00:10:10.800
Next we're going to have a helper
routine for computing this value at each

00:10:10.800 --> 00:10:17.300
gradient location by passing in a lattice
coordinate and its fractional offset

00:10:17.300 --> 00:10:23.300
to use that to index into our permutation
array and calculate a contribution value.

00:10:23.300 --> 00:10:23.800
Thank you.

00:10:26.930 --> 00:10:30.380
To actually evaluate the noise function,
we're going to have

00:10:30.380 --> 00:10:35.120
another helper routine,
which basically computes our fractional

00:10:35.290 --> 00:10:39.840
offset and our lattice coordinate,
indexes into all of the surrounding

00:10:40.370 --> 00:10:44.780
neighboring lattice cells,
and computes a contribution

00:10:44.780 --> 00:10:48.620
value for each one of those
based on the gradient directions.

00:10:48.910 --> 00:10:52.640
And then combines those using the
smoothing function that we specified

00:10:52.640 --> 00:10:58.590
previously to form our final contribution
value at that location in space.

00:11:01.170 --> 00:11:02.720
So those were helper routines.

00:11:02.820 --> 00:11:05.980
The actual entry point is going
to be our compute kernel here,

00:11:06.110 --> 00:11:08.300
which we call Perlin Noise 2D.

00:11:08.300 --> 00:11:12.170
We're passing in the same parameters
that I illustrated in my last example,

00:11:12.170 --> 00:11:14.580
where we've got a bias
and a scale factor.

00:11:14.580 --> 00:11:18.830
And in this example,
we're going to store the results in an

00:11:18.830 --> 00:11:21.820
image to make it easier to visualize.

00:11:21.860 --> 00:11:24.700
So you'll notice that
everything's basically the same,

00:11:24.700 --> 00:11:29.220
except we're going to now use our
coordinate as an input into our

00:11:29.220 --> 00:11:32.320
improved Perlin Noise function
that we declared previously.

00:11:32.320 --> 00:11:38.150
And then we're going to use the resulting
value to write and update our array.

00:11:38.230 --> 00:11:41.900
Or sorry, image in this case.

00:11:41.950 --> 00:11:44.930
So I'm going to go ahead and demo that.

00:11:53.300 --> 00:11:57.320
So here we have a Perlin noise
function that we're evaluating

00:11:57.710 --> 00:11:59.560
across two-dimensional domain.

00:11:59.560 --> 00:12:04.560
This is 1024 by 1024,
and on a single GPU,

00:12:04.560 --> 00:12:08.560
performance is upwards
of 450 frames a second.

00:12:08.610 --> 00:12:11.140
And ultimately,
this is because we aren't doing

00:12:11.230 --> 00:12:12.880
any memory reads whatsoever.

00:12:12.880 --> 00:12:15.510
Everything is just a lookup table,
and we're actually just

00:12:15.510 --> 00:12:17.390
doing a pure data generation.

00:12:17.780 --> 00:12:21.700
So, scaling and biasing.

00:12:21.700 --> 00:12:26.060
We can zoom in and out and
modify our coordinates.

00:12:26.060 --> 00:12:29.010
And one of the examples that I'm
going to be showing in just a

00:12:29.180 --> 00:12:32.700
second is going to be using this
as a basis for doing displacement.

00:12:32.700 --> 00:12:35.710
So, if we sum up multiple
octaves of Perlin noise,

00:12:35.710 --> 00:12:39.600
we get some higher level detail,
and we can use this as an

00:12:39.600 --> 00:12:42.690
interesting means for textures.

00:12:42.690 --> 00:12:47.700
And this is what I'm going to
be using for my displacement

00:12:47.700 --> 00:12:48.690
demo in just a minute.

00:12:48.700 --> 00:12:52.510
So, we can switch back to the slides.

00:12:59.010 --> 00:13:02.230
So now that we have some basic
understanding of how to execute kernels

00:13:02.230 --> 00:13:07.100
and what's needed to connect to devices,
let's look at using OpenCL with OpenGL.

00:13:07.100 --> 00:13:09.580
And this really provides
us the best of both.

00:13:09.580 --> 00:13:13.480
We can use OpenGL for rendering for
all of our existing geometric data,

00:13:13.480 --> 00:13:19.310
and then OpenCL for doing simulations
and more advanced rendering techniques.

00:13:20.190 --> 00:13:25.740
One of the great things about OpenCL is
that it provides direct access to memory

00:13:25.740 --> 00:13:28.420
that's allocated and managed by OpenGL.

00:13:28.570 --> 00:13:32.220
This means that we can reference
data on the device without

00:13:32.290 --> 00:13:35.070
copying it back across the host.

00:13:36.130 --> 00:13:39.250
Our general usage here is all we
have to do is create and allocate

00:13:39.250 --> 00:13:45.090
the object or memory in OpenGL and
then just reference it in OpenCL.

00:13:45.250 --> 00:13:49.930
Then we can use attachments as
a means for allowing OpenCL to

00:13:50.310 --> 00:13:55.500
modify the data and then detach
so that OpenGL can then use it.

00:13:57.520 --> 00:14:00.420
So in this example,
we're going to create a vertex

00:14:00.560 --> 00:14:07.830
buffer object using a standard
gen buffer and bind buffer,

00:14:07.830 --> 00:14:12.640
which you should all be familiar
with in terms of OpenGL programming.

00:14:12.650 --> 00:14:15.990
The main thing that I want to point
out here is that we're issuing

00:14:16.010 --> 00:14:20.340
the GL static draw usage flag,
which is very important.

00:14:20.340 --> 00:14:23.730
This is indicating that as
far as OpenGL is concerned,

00:14:23.750 --> 00:14:27.040
this data is not going to
be updated from OpenGL.

00:14:27.090 --> 00:14:30.580
From CL, we're actually going to be
modifying the values ourselves,

00:14:30.600 --> 00:14:34.570
but OpenGL, there's not going to be
any more data submission.

00:14:34.700 --> 00:14:37.500
This ensures that there's not
going to be an additional copy,

00:14:37.500 --> 00:14:39.950
and we're just referencing the data.

00:14:40.690 --> 00:14:43.640
Then in OpenCL,
we just need to create an array passing

00:14:43.640 --> 00:14:47.700
in the memalloc reference usage flag,
which simply states the device

00:14:47.810 --> 00:14:51.690
is already out -- the memory has
already been allocated on the device.

00:14:51.900 --> 00:14:54.600
There's no need to
allocate any more memory.

00:14:54.600 --> 00:15:00.910
Simply use the memory address
for this particular buffer.

00:15:01.010 --> 00:15:07.270
And we can then attach the existing
VBO using a CL attach GL buffer method,

00:15:07.270 --> 00:15:10.470
execute our kernel,
and then detach to give

00:15:10.680 --> 00:15:12.300
control back over to OpenGL.

00:15:12.570 --> 00:15:16.650
And then here we just bind our buffer
and then issue a GL draw elements call,

00:15:16.830 --> 00:15:22.000
and the data's already been updated,
and we get the results.

00:15:23.200 --> 00:15:27.570
The other usage pattern that we
can use with sharing data between

00:15:27.690 --> 00:15:32.410
OpenCL and OpenGL is to allocate
and manage the data in OpenCL.

00:15:32.580 --> 00:15:36.680
This might be useful if you're doing,
you know,

00:15:36.770 --> 00:15:40.260
if you're generating data in OpenCL,
or if you're dealing

00:15:40.420 --> 00:15:43.130
with unusual formats,
or if you simply want to use

00:15:43.470 --> 00:15:47.300
OpenCL on one device and OpenGL on
another device and actually do need

00:15:47.300 --> 00:15:49.490
to copy the data between devices.

00:15:49.500 --> 00:15:53.430
So our general use of share is we
just create an allocated OpenCL,

00:15:53.610 --> 00:15:59.700
read and write via the OpenCL kernel,
and then copy the results back to OpenGL.

00:16:00.470 --> 00:16:02.880
So very similar,
except this time we're going

00:16:02.880 --> 00:16:07.680
to be creating and allocating
the memory in OpenCL.

00:16:07.680 --> 00:16:09.580
We're going to use the
mem read write usage flag,

00:16:09.580 --> 00:16:12.060
which I was using previously.

00:16:12.060 --> 00:16:16.570
Execute our kernel,
and then read the data back into

00:16:16.570 --> 00:16:19.780
host memory that's already been
allocated in our application.

00:16:19.780 --> 00:16:21.910
That's the data array.

00:16:21.920 --> 00:16:24.860
And then submit this to OpenGL.

00:16:24.870 --> 00:16:28.140
So here we are copying from the
device back into our application,

00:16:28.140 --> 00:16:30.500
submitting to OpenGL,
which is going to be slower,

00:16:30.530 --> 00:16:33.990
but it gives us more flexibility
in terms of what we can do.

00:16:34.960 --> 00:16:38.900
So, as an example of sharing data
between OpenGL and OpenCL,

00:16:38.950 --> 00:16:41.800
I'm going to show you this
geometric displacement demo

00:16:41.800 --> 00:16:42.670
that I was talking about.

00:16:42.800 --> 00:16:45.800
Here we're going to create
a GL vertex buffer object,

00:16:45.960 --> 00:16:50.800
fill it with some initial positions,
attach, and then use within OpenCL.

00:16:50.860 --> 00:16:52.800
We're going to evaluate several
octaves of Perlin noise,

00:16:52.800 --> 00:16:56.870
move the vertex positions in
the direction of the normal,

00:16:56.870 --> 00:16:59.720
which is the normal outward direction,
and then detach and

00:16:59.720 --> 00:17:02.790
then render with OpenGL.

00:17:02.880 --> 00:17:05.320
So let's switch over to the demo.

00:17:15.010 --> 00:17:18.460
So, like I was saying,
we're initially submitting just a sphere,

00:17:18.460 --> 00:17:21.530
which is a whole bunch of vertices,
and then using that as our

00:17:21.530 --> 00:17:24.610
inputs to our OpenCL kernel,
we evaluate that Perlin noise

00:17:24.740 --> 00:17:29.650
function at that location in space,
and then modify the vertex locations

00:17:29.750 --> 00:17:31.960
by pushing out in the normal direction.

00:17:32.160 --> 00:17:35.710
Then we also take three more samples
using finite differences to compute a

00:17:35.710 --> 00:17:39.300
new normal at that location in space,
and then all the rendering

00:17:39.300 --> 00:17:40.880
is being done by OpenGL.

00:17:40.880 --> 00:17:46.580
So here we've got several different
shaders to give us Fong illumination,

00:17:46.580 --> 00:17:51.550
and then we're also doing
perspective-corrected shadow maps so

00:17:51.550 --> 00:17:58.450
that we get a highly detailed shadow,
and we're also jittering the sample

00:17:58.590 --> 00:18:02.130
at the edges of the shadow map.

00:18:02.160 --> 00:18:05.130
to give us a nice soft blend.

00:18:05.160 --> 00:18:09.240
We can also adjust some parameters
of the noise evaluation,

00:18:09.260 --> 00:18:11.300
so we can increase and
decrease the frequency,

00:18:11.300 --> 00:18:20.110
and modify the number of
octaves that we're calculating.

00:18:33.400 --> 00:18:50.860
Increase the amplitude.

00:18:50.870 --> 00:18:53.120
And then just as another
demonstration of what we can do

00:18:53.120 --> 00:18:56.090
on the rendering side of things,
we can also use this for

00:18:56.090 --> 00:18:57.960
reflection and refraction.

00:18:58.060 --> 00:19:00.650
And we're also doing some
chromatic dispersion.

00:19:00.770 --> 00:19:04.840
So we're using the Fresnel
calculations for doing the reflection

00:19:04.840 --> 00:19:06.490
and the refraction coefficients.

00:19:06.500 --> 00:19:09.500
And then we're also simulating the
chromatic dispersion by assuming

00:19:09.500 --> 00:19:15.080
that each channel of color is a
different wavelength of light,

00:19:15.250 --> 00:19:18.310
which is going to refract differently.

00:19:18.370 --> 00:19:23.050
That's how you get the highlights
to have drastically different

00:19:23.100 --> 00:19:27.410
colors as the light gets refracted.

00:19:32.830 --> 00:19:39.800
Okay, back to slides.

00:19:39.800 --> 00:19:44.030
Okay, and one other demo that I want to
show you here is a smooth particle

00:19:44.030 --> 00:19:46.080
hydrodynamics fluid simulation.

00:19:46.100 --> 00:19:49.600
This was actually shown in John's
State of the Union for the graphics

00:19:49.610 --> 00:19:52.920
and media session that you might
have seen earlier this week.

00:19:52.920 --> 00:19:56.590
So SPH is a simulation
method for free surface flow.

00:19:56.670 --> 00:20:01.770
It uses the Navier-Stokes equations
for fluid motion by calculating a

00:20:01.770 --> 00:20:04.340
weighted sum of physical values.

00:20:04.410 --> 00:20:08.040
So you can think of this as
a 3D convolution of forces.

00:20:09.200 --> 00:20:15.440
And the algorithm that I've implemented
here is for every single frame,

00:20:15.440 --> 00:20:18.880
we're going to compute particle
positions based on the forces.

00:20:18.970 --> 00:20:24.250
And then we're going to dynamically
fill a uniform grid with particles

00:20:24.560 --> 00:20:26.040
in grid cell coordinates.

00:20:26.090 --> 00:20:29.960
We're going to use that as an
acceleration structure to identify

00:20:29.960 --> 00:20:33.420
nearest neighbors so that we
don't have to compare ourselves

00:20:33.420 --> 00:20:35.500
against every single particle.

00:20:35.500 --> 00:20:42.820
So we only look at our
neighboring grid cells based on

00:20:42.830 --> 00:20:46.900
our current particle location.

00:20:47.030 --> 00:20:50.250
From this, we're going to create
a nearest neighbor map.

00:20:50.510 --> 00:20:52.600
from the uniform grid
that I just mentioned.

00:20:52.640 --> 00:20:56.620
And then for every particle,
we're going to evaluate density,

00:20:56.810 --> 00:21:00.120
pressure, curvature,
and viscosity contributions

00:21:00.120 --> 00:21:02.860
based on the particle attributes.

00:21:03.000 --> 00:21:06.920
From this, we can define forces,
which we can update and

00:21:06.920 --> 00:21:08.660
integrate per time step.

00:21:08.820 --> 00:21:11.220
And then finally,
create a distance field based

00:21:11.220 --> 00:21:14.160
on the particle locations,
which I'm gonna store

00:21:14.160 --> 00:21:15.720
as just a 3D texture.

00:21:15.740 --> 00:21:21.510
And then ultimately use that for a
volume ray casting for the rendering.

00:21:21.700 --> 00:21:26.810
So let me switch back
over to the demo machine.

00:21:37.080 --> 00:21:43.900
So here we have the Stanford bunny
sitting within this pedestal.

00:21:43.900 --> 00:21:46.240
And as I mentioned,
this is a 3D texture which we're

00:21:46.240 --> 00:21:52.600
using as an input to a raycast
volume shader so that we can do the

00:21:52.600 --> 00:21:56.620
rendering and then also calculating
reflection and refraction,

00:21:56.670 --> 00:22:00.990
similar to the previous
geometric displacement demo.

00:22:03.290 --> 00:22:06.300
So he melts in a fluid-like fashion.

00:22:06.330 --> 00:22:10.110
And we've got upwards of
34,000 particles here.

00:22:10.250 --> 00:22:14.100
And for this particular demo,
we're actually doing the rendering on

00:22:14.100 --> 00:22:20.040
one GPU and the simulation on another,
and then moving the data between

00:22:20.080 --> 00:22:24.860
devices to do the rendering
and updates for every frame.

00:22:24.860 --> 00:22:31.920
And you can also interact with this,
so you can splash things around.

00:22:40.240 --> 00:22:46.630
And with about 35,000 particles and
doing all of the force calculations

00:22:47.150 --> 00:22:50.440
and the texture generation,
we're getting on the order

00:22:50.440 --> 00:22:54.460
of 40 to 50 frames a second,
depending on how tightly packed

00:22:54.580 --> 00:22:58.960
each particle is in terms
of its location in space.

00:22:58.960 --> 00:23:03.050
We can actually get this up
to about 100,000 particles

00:23:03.060 --> 00:23:05.350
and still be interactive.

00:23:09.620 --> 00:23:20.420
So, as a segue into the next presenters'
sections within this course,

00:23:20.420 --> 00:23:23.650
I just wanted to give you some
brief optimization strategies

00:23:23.650 --> 00:23:25.600
for developing OpenCL kernels.

00:23:25.870 --> 00:23:28.500
Overall,
you really want to maximize computation.

00:23:28.500 --> 00:23:31.680
You need to keep the
device as busy as possible.

00:23:31.680 --> 00:23:35.870
And the best way to do that is to
minimize the amount of memory that you're

00:23:36.240 --> 00:23:38.740
using and use that memory effectively.

00:23:38.740 --> 00:23:44.810
So make sure that it's as close to
the current thread that's running

00:23:45.330 --> 00:23:48.700
and utilize your caches efficiently.

00:23:48.700 --> 00:23:53.780
And I tried to illustrate this with
these two best and worst case scenarios.

00:23:53.780 --> 00:23:57.990
So the best case is to have uniform
memory access where every thread is

00:23:58.170 --> 00:24:04.520
accessing memory from a unique location,
doing a whole bunch of work,

00:24:04.520 --> 00:24:09.320
using a linear execution,
so not too much branching,

00:24:09.320 --> 00:24:12.920
not too many if statements,
and then writing to unique

00:24:13.410 --> 00:24:16.260
destinations within your output buffer.

00:24:16.310 --> 00:24:19.840
This will give you ideally
the best performance.

00:24:19.840 --> 00:24:24.920
The worst case is to read from random
locations a whole bunch of data,

00:24:24.920 --> 00:24:28.880
do a whole bunch of branching
and iteration in arbitrary

00:24:28.880 --> 00:24:32.370
ways within your kernel,
and then scatter and write

00:24:32.470 --> 00:24:34.600
to random output locations.

00:24:34.600 --> 00:24:35.270
By doing this,
you're going to force the device

00:24:35.270 --> 00:24:36.080
to be able to run the kernel in a
way that is not too much branching,

00:24:36.080 --> 00:24:36.580
not too many if statements,
and then writing to unique

00:24:36.580 --> 00:24:37.840
destinations within your output buffer.

00:24:37.840 --> 00:24:44.010
So you can basically blow out all
of its caches and force data to get

00:24:44.510 --> 00:24:50.360
transferred way too frequently and
also decrease performance drastically.

00:24:50.360 --> 00:24:53.840
So you can sort of judge this and
see that for most applications you're

00:24:53.840 --> 00:24:58.110
going to fit somewhere in between,
but you can use this as a

00:24:58.110 --> 00:25:00.760
guideline for best practices.

00:25:00.890 --> 00:25:05.620
Also on the OpenGL side of things,
you should really focus on referencing

00:25:06.040 --> 00:25:14.100
data that's been managed in OpenGL and
just use those as your memory outputs.

00:25:14.240 --> 00:25:16.190
So don't allocate memory through OpenCL.

00:25:16.200 --> 00:25:18.640
Simply reference and use
OpenGL managed memory.

00:25:18.890 --> 00:25:20.550
Then you don't incur
the copy across the bus.

00:25:20.680 --> 00:25:23.890
And then also something else that
I didn't have time to talk about

00:25:24.040 --> 00:25:27.880
is you can actually separate out
your rendering from your computation

00:25:27.880 --> 00:25:29.950
using asynchronous execution.

00:25:30.070 --> 00:25:33.420
So OpenCL allows you to execute
a kernel asynchronously.

00:25:33.440 --> 00:25:37.890
And you can just let it sit there and
run and then get an event indicating that

00:25:37.890 --> 00:25:43.030
the kernel is finished and use that as a
means for updating your rendering thread.

00:25:43.750 --> 00:25:47.790
So in summary,
OpenCL provides a very straightforward

00:25:47.800 --> 00:25:51.100
programming model for performing data
parallel computation that's incredibly

00:25:51.100 --> 00:25:55.160
flexible and incredibly powerful,
and also integrates seamlessly

00:25:55.160 --> 00:25:59.200
with OpenGL to enable you as a
developer to perform some really

00:25:59.200 --> 00:26:03.660
advanced visual processing and use
cutting edge rendering techniques.

00:26:03.850 --> 00:26:07.790
And with that, I'd like to hand it over
to Ian Buck from NVIDIA.

00:26:17.240 --> 00:26:18.140
Okay, hi.

00:26:18.250 --> 00:26:19.040
So I'm Ian Buck.

00:26:19.040 --> 00:26:24.400
I actually did my doctorate research
in GPU computing back in the day,

00:26:24.400 --> 00:26:27.000
and I'm excited that now all
this is becoming a reality.

00:26:27.000 --> 00:26:28.440
It's been a pleasure working with Apple.

00:26:28.440 --> 00:26:31.480
I drive all of the computing
software at NVIDIA,

00:26:31.480 --> 00:26:35.880
and we've been working tightly with them,
obviously, on OpenCL.

00:26:36.130 --> 00:26:39.060
So the purpose of my talk is
basically to give you an idea,

00:26:39.060 --> 00:26:45.200
a conceptual model of some of the
differences between the GPU and a CPU.

00:26:45.200 --> 00:26:49.680
And I want to emphasize that a GPU is
not just a CPU with more cores.

00:26:49.680 --> 00:26:53.340
It really is a fundamentally
different kind of processor.

00:26:53.340 --> 00:26:57.340
And it had to be in order to deal
with the level of parallelism

00:26:57.340 --> 00:26:59.160
that we want to exploit.

00:26:59.170 --> 00:27:01.820
We have hundreds of
cores in our processors.

00:27:01.830 --> 00:27:06.860
Each core is capable of maintaining
thousands of threads at the same time.

00:27:06.890 --> 00:27:09.230
And as a result,
it's much less dependent on caches.

00:27:09.310 --> 00:27:13.960
We can spend more of our diary
on ALU and on computation than

00:27:13.980 --> 00:27:17.440
traditional caching architectures.

00:27:17.550 --> 00:27:21.540
So the outline for this talk,
I'm going to highlight three key things

00:27:21.540 --> 00:27:24.120
that emphasize how GPUs are different.

00:27:24.180 --> 00:27:26.320
First is the importance
of filling up GPUs.

00:27:26.320 --> 00:27:28.700
There was a question earlier
about how many threads do

00:27:28.700 --> 00:27:31.210
you really want to kick off,
and I'll talk about that.

00:27:31.280 --> 00:27:33.680
Using the local memory,
that's a big advantage

00:27:33.680 --> 00:27:35.250
that we have on the GPU.

00:27:35.260 --> 00:27:39.790
And I'll dig into that in terms of
performance and give some examples.

00:27:39.940 --> 00:27:42.040
Secondly,
optimizing for that memory performance.

00:27:42.110 --> 00:27:47.080
The previous speaker talked about
scatter and gather and how making

00:27:47.080 --> 00:27:52.580
a linear kind of problem can really
present the best kind of memory access

00:27:52.640 --> 00:27:55.580
pattern to the memory subsystems.

00:27:56.510 --> 00:27:58.360
So first, filling up the GPU.

00:27:58.390 --> 00:28:02.440
There was a question earlier, you know,
if I have a 10 million pixel image,

00:28:02.440 --> 00:28:04.740
do I really want to kick
off 10 million threads?

00:28:04.750 --> 00:28:06.160
And the answer is yes.

00:28:06.160 --> 00:28:08.930
I mean, GPUs just love threading.

00:28:08.930 --> 00:28:10.600
You know, bring it.

00:28:10.630 --> 00:28:13.600
So that's how we keep the GPU busy.

00:28:13.600 --> 00:28:17.970
Today's GPUs have 16 processors,
each with 8 cores.

00:28:18.190 --> 00:28:19.430
Henry talked about that this morning.

00:28:22.830 --> 00:28:27.000
8800GT has a total of 128 cores.

00:28:27.090 --> 00:28:28.520
Threads are really cheap
in our architecture.

00:28:28.520 --> 00:28:33.180
We've built our architecture
to absorb that many threads.

00:28:33.260 --> 00:28:36.150
We actually have dedicated
hardware support for context

00:28:36.150 --> 00:28:37.340
switching between those threads.

00:28:37.400 --> 00:28:41.150
And we'll actually switch on every
instruction to another thread

00:28:41.520 --> 00:28:45.180
in order to keep the GPU busy,
whether it be the ALU functional

00:28:45.180 --> 00:28:47.330
units or the memory subsystem.

00:28:47.400 --> 00:28:49.900
So more threads, the better.

00:28:51.090 --> 00:28:54.740
The thread groups as part of
the OpenCL model are distributed

00:28:54.860 --> 00:28:56.710
across the different processors.

00:28:56.710 --> 00:28:58.560
So we're actually going to be
processing the threads and the

00:28:58.560 --> 00:29:00.800
thread groups concurrently.

00:29:01.310 --> 00:29:03.670
There's a first question,
how many threads do I really

00:29:03.670 --> 00:29:05.240
need in order to get it working?

00:29:05.240 --> 00:29:08.070
Obviously you want to kick
off more than one thread,

00:29:08.070 --> 00:29:10.460
but typically you want to kick
off at least the number of thread

00:29:10.460 --> 00:29:11.840
groups as you have processors.

00:29:12.050 --> 00:29:18.270
On the 8800GT,
that may be 16 of these processors.

00:29:18.510 --> 00:29:22.130
But you can certainly go more,
and the GPU can queue

00:29:22.130 --> 00:29:26.190
up millions of threads,
and that's not a problem.

00:29:26.610 --> 00:29:28.610
It also means as GPUs
scale and get bigger,

00:29:28.620 --> 00:29:30.740
and we add more
processors to NVIDIA GPUs,

00:29:30.890 --> 00:29:33.340
your applications will scale
seamlessly because we can

00:29:33.340 --> 00:29:35.960
execute more things in parallel.

00:29:38.580 --> 00:29:40.290
So I want to talk about
filling up each processor.

00:29:40.300 --> 00:29:43.620
We have that thread group,
which is a collection of threads.

00:29:43.660 --> 00:29:45.740
How big should my thread groups be?

00:29:45.740 --> 00:29:48.850
On current architectures,
we can support up to 768

00:29:49.270 --> 00:29:51.680
threads per processor.

00:29:52.370 --> 00:29:54.290
The more threads per group, the better.

00:29:54.470 --> 00:29:56.030
Obviously, we'll use those threads.

00:29:56.180 --> 00:29:58.150
The hardware will switch
between one thread to another

00:29:58.270 --> 00:30:01.940
thread to keep the ALUs,
the cores, busy on the GPU.

00:30:01.940 --> 00:30:05.310
The recommended minimum
is around 64 threads,

00:30:05.310 --> 00:30:09.630
but certainly 256 threads is
a better choice that hides any

00:30:09.740 --> 00:30:14.240
latencies that may be in the
pipelines within the architecture.

00:30:14.350 --> 00:30:16.340
But certainly, you can grow further.

00:30:16.340 --> 00:30:19.660
This is often algorithm dependent.

00:30:19.660 --> 00:30:22.040
So you want to experiment,
just try different sizes.

00:30:22.390 --> 00:30:23.460
See what makes sense for you.

00:30:23.830 --> 00:30:27.150
Often the size of a thread group
is determined by the kinds of

00:30:27.150 --> 00:30:28.790
memory blocking you're doing.

00:30:29.000 --> 00:30:32.040
Things you may have used to do on
the cache for blocking for caches.

00:30:32.170 --> 00:30:38.310
That thread group defines
typically your blocking size.

00:30:38.550 --> 00:30:44.350
OpenCL also provides a function
called get kernel thread group info,

00:30:44.350 --> 00:30:49.340
which can give you an idea of
based on how your code compiled,

00:30:49.350 --> 00:30:53.270
number of registers, amount of resources,
what's a good optimal size for that

00:30:53.390 --> 00:30:55.000
thread group to help you decide.

00:30:55.020 --> 00:30:56.990
But certainly experiment.

00:31:01.790 --> 00:31:03.840
So using the GPU's local memory.

00:31:03.990 --> 00:31:07.750
The local memory is a dedicated
on-die memory that sits

00:31:07.870 --> 00:31:09.800
right next to those ALUs.

00:31:09.840 --> 00:31:12.660
It's as fast as registers to access.

00:31:12.770 --> 00:31:15.920
You can think of it as an
indexable register file.

00:31:15.920 --> 00:31:19.140
So it's orders of magnitude
faster than global memory.

00:31:19.140 --> 00:31:23.430
We computed on the 8800GT,
you're pushing over 760

00:31:23.450 --> 00:31:27.890
gigabytes of second of aggregate
bandwidth to that local memory.

00:31:27.890 --> 00:31:30.660
So if you can move your
data from the global,

00:31:30.660 --> 00:31:35.840
which is a around 70 to
80 gigabytes a second,

00:31:35.890 --> 00:31:38.720
copy it into the local memory,
spin on it there,

00:31:38.730 --> 00:31:41.340
you can actually get some
huge performance improvements.

00:31:41.630 --> 00:31:44.550
So as an optimization tip,
first get your algorithm working,

00:31:44.670 --> 00:31:47.250
then see if you can optimize
for that local memory.

00:31:48.950 --> 00:31:50.660
A bit of detail about that memory.

00:31:50.660 --> 00:31:51.680
It's banked.

00:31:51.750 --> 00:31:53.790
It contains 16 banks.

00:31:54.040 --> 00:31:59.390
Every word will service a different
bank based on the MSBs of the address

00:31:59.510 --> 00:32:00.790
you're accessing in the local memory.

00:32:00.900 --> 00:32:04.900
So you want to think about that if
you're doing some further perf-turning.

00:32:04.900 --> 00:32:07.900
Once you've taken your
code into the local memory,

00:32:07.980 --> 00:32:10.900
think about how your access patterns are
happening across the different threads.

00:32:10.910 --> 00:32:15.900
Because that can further
give you more performance.

00:32:16.690 --> 00:32:22.360
Today's GPUs, the local memory is 16
kilobytes per processor,

00:32:22.360 --> 00:32:24.600
per thread group,
and the threads within that thread group

00:32:24.600 --> 00:32:26.960
will be able to share that local memory.

00:32:31.800 --> 00:32:33.600
Further optimizing for the global memory.

00:32:33.600 --> 00:32:35.860
The previous speaker talked
a little bit about that.

00:32:36.040 --> 00:32:38.550
Today it's about 80 gigabytes
of peak performance.

00:32:38.550 --> 00:32:43.460
You want to think about how the
GPU is executing the threads.

00:32:43.500 --> 00:32:46.740
On a CPU, you'll switch to one thread,
and that thread will

00:32:46.970 --> 00:32:48.240
dominate that processor.

00:32:48.290 --> 00:32:52.540
And you often want to optimize for
locality across different accesses.

00:32:52.540 --> 00:32:54.820
So the first access will
read the first word,

00:32:54.820 --> 00:32:58.620
the next one will read the next word,
the next one will read the next word.

00:32:58.620 --> 00:33:02.560
And you know in your head that the
cache is trying to coalesce those

00:33:02.560 --> 00:33:05.300
together and leverage the caching.

00:33:05.390 --> 00:33:07.460
On a GPU,
we're running your threads in parallel,

00:33:07.490 --> 00:33:08.730
all at the same time.

00:33:08.730 --> 00:33:12.860
On current architectures,
we gang them together in groups of 32.

00:33:12.860 --> 00:33:17.010
So instead of thinking about how threads,
how an individual thread is going to

00:33:17.030 --> 00:33:20.560
access memory in its access pattern,
you want to think about

00:33:20.690 --> 00:33:24.490
how the threads across,
across threads are accessing memory.

00:33:24.490 --> 00:33:26.940
If the first thread's
reading the first word,

00:33:27.050 --> 00:33:28.600
the second thread's
reading the next word,

00:33:28.600 --> 00:33:31.710
the third thread's reading the next word,
that's going to look like one wide

00:33:31.710 --> 00:33:33.740
memory to the memory subsystem.

00:33:33.950 --> 00:33:36.690
And the GPUs are going to
optimize for that and get you

00:33:36.690 --> 00:33:38.220
that 80 gigabytes a second.

00:33:38.220 --> 00:33:41.300
So you need to turn your
thinking around a little bit.

00:33:41.340 --> 00:33:43.910
The good news is often,
as I'll show later in an example,

00:33:43.910 --> 00:33:48.690
this is how naturally you want to program
-- your algorithms naturally want to

00:33:48.970 --> 00:33:53.040
access data when you have 10 million
threads flying through the system.

00:33:53.270 --> 00:33:55.970
Often the threads are working on
neighboring parts of the problem,

00:33:55.970 --> 00:33:57.760
neighboring parts of the data.

00:33:57.770 --> 00:34:00.960
They'll naturally read neighboring
values and get that good performance.

00:34:00.980 --> 00:34:02.700
performance.

00:34:04.000 --> 00:34:06.780
So as a result,
brute force algorithms often

00:34:06.780 --> 00:34:08.160
result in the best performance.

00:34:08.300 --> 00:34:13.340
The complex stuff you may have used to
doing your traditional C and CPU code,

00:34:13.350 --> 00:34:17.040
where you're trying to minimize
the number of memory accesses,

00:34:17.090 --> 00:34:20.590
typically the best performance
is just the brute force one.

00:34:20.740 --> 00:34:25.480
Create a thread per every
data element in your problem,

00:34:25.480 --> 00:34:30.390
solve the problem for that thread,
and the memory subsystem likes

00:34:30.390 --> 00:34:30.390
that kind of memory access.

00:34:32.640 --> 00:34:34.500
So here's a simple example.

00:34:34.500 --> 00:34:40.970
Typically in fluid simulations,
we're updating our

00:34:41.100 --> 00:34:42.180
forces within the fluid.

00:34:42.320 --> 00:34:44.080
It's a little different kind
of fluid than we showed before.

00:34:44.080 --> 00:34:49.160
But let's say I have a simulation grid,
and I want to update my fluid motion.

00:34:49.160 --> 00:34:52.300
I want to keep the pressures
normalized in the system.

00:34:52.300 --> 00:34:54.800
Water is an incompressible solution.

00:34:55.730 --> 00:34:57.460
So usually what we do is
we perform our updates,

00:34:57.460 --> 00:34:59.280
and then we have to
normalize the pressure.

00:34:59.280 --> 00:35:03.170
Normalizing the pressure is simply
doing a local pressure calculation

00:35:03.590 --> 00:35:07.870
for every grid cell in my simulation,
and seeing what the differences are

00:35:07.870 --> 00:35:11.170
and trying to normalize the velocities
of all the fluid so that we keep the

00:35:11.390 --> 00:35:13.300
pressure constant across the fluid.

00:35:13.310 --> 00:35:16.110
This is simply a neighborhood
kind of calculation.

00:35:16.190 --> 00:35:18.920
In my grid cell,
I look at my neighboring pressure

00:35:18.920 --> 00:35:21.040
and my pressure and try to normalize.

00:35:21.040 --> 00:35:22.470
Simple add operation.

00:35:22.480 --> 00:35:25.620
But I have to do this
across all the cells.

00:35:25.640 --> 00:35:27.780
in the simulation.

00:35:28.630 --> 00:35:29.180
There's the diagram.

00:35:29.260 --> 00:35:32.620
So I'll have one thread be
working on the green cell,

00:35:32.620 --> 00:35:34.580
looking at my neighbors,
influencing my pressure.

00:35:34.580 --> 00:35:37.220
I'll obviously have a
thread for every cell,

00:35:37.480 --> 00:35:40.330
looking at my neighbors,
updating my pressures.

00:35:42.300 --> 00:35:45.250
So there's two kinds of
ways we do this on the GPU.

00:35:45.540 --> 00:35:47.920
The first way,
and the way I'd recommend you start,

00:35:47.920 --> 00:35:50.260
is just do a simple
brute force algorithm.

00:35:50.300 --> 00:35:52.240
Create a thread for every grid cell.

00:35:52.260 --> 00:35:55.900
We get lots of threads, bring it,
the GPU loves it,

00:35:55.930 --> 00:35:58.060
and then we'll get great
streaming performance.

00:35:58.090 --> 00:36:00.820
Because if we look across
the different threads,

00:36:00.830 --> 00:36:02.670
everyone's going to
read the guy above them.

00:36:02.770 --> 00:36:06.280
Well, that's one big wide memory access,
as seen by the GPU.

00:36:06.300 --> 00:36:07.770
Then we'll read everyone to the right.

00:36:07.860 --> 00:36:11.520
That's again, one big wide memory read,
as seen by the GPU, and below.

00:36:11.580 --> 00:36:14.420
So typically,
that brute force algorithm works great.

00:36:14.420 --> 00:36:16.620
And this just doesn't apply
to the fluid simulation.

00:36:16.620 --> 00:36:18.720
We've seen this in image processing.

00:36:18.720 --> 00:36:22.400
We see this in more complex
stuff like ray tracing.

00:36:22.410 --> 00:36:25.440
Often,
you start with a brute force algorithm.

00:36:25.470 --> 00:36:28.070
It's the simplest thing to code up,
and often gives you the great

00:36:28.070 --> 00:36:30.510
performance right from the starting gate.

00:36:32.370 --> 00:36:35.300
There's another big
possibility for improvement is,

00:36:35.320 --> 00:36:37.850
like I mentioned before,
use the local memory.

00:36:37.980 --> 00:36:42.660
So what I can do is take the fluid,
the grid simulation, all those pressures,

00:36:42.730 --> 00:36:45.610
copy some region of it,
the region that my threads care about,

00:36:45.700 --> 00:36:48.140
the threads in my thread
group might care about,

00:36:48.140 --> 00:36:49.900
into the local memory.

00:36:49.960 --> 00:36:53.210
Now I have much better,
the threads can work together

00:36:53.290 --> 00:36:55.230
to not overfetch their data.

00:36:55.280 --> 00:36:59.040
Every thread we can spin
on the local memory,

00:36:59.080 --> 00:37:03.260
perhaps do multiple iterations of the
pressure solve to try to normalize

00:37:03.260 --> 00:37:05.190
all the pressures in the system.

00:37:05.490 --> 00:37:08.870
But using the local memory,
which is 700 gigabytes a

00:37:08.870 --> 00:37:10.900
second of memory bandwidth.

00:37:14.660 --> 00:37:17.550
So that was a really quick introduction
of some of the top three things that

00:37:17.550 --> 00:37:19.600
you want to do to keep the GPU busy.

00:37:19.620 --> 00:37:21.870
It is a massively data
parallel processor.

00:37:21.960 --> 00:37:25.450
Don't be afraid to kick
off millions of threads.

00:37:25.520 --> 00:37:26.290
We can handle it.

00:37:26.330 --> 00:37:26.960
We love it.

00:37:27.020 --> 00:37:28.360
It keeps the GPU busy.

00:37:28.410 --> 00:37:30.010
Threads are really cheap.

00:37:30.240 --> 00:37:34.060
The GPUs have dedicated hardware to
keep all those threads busy and active.

00:37:34.100 --> 00:37:39.870
And we need that in order to
keep all the pipelines full.

00:37:41.200 --> 00:37:43.400
The other big point is
use the local memory.

00:37:43.510 --> 00:37:46.980
Huge speedups have been seen by
taking stuff out of the memory,

00:37:47.030 --> 00:37:50.890
putting it in that local-- think
of it as a software managed cache.

00:37:50.990 --> 00:37:54.720
You get to decide what goes in the cache,
not some hardware prediction

00:37:54.870 --> 00:37:56.250
scheme or caching scheme.

00:37:56.310 --> 00:37:58.120
It's under your software control.

00:37:58.130 --> 00:38:00.300
You simply declare
variables that are local.

00:38:00.330 --> 00:38:03.620
They get allocated to that memory,
and you can control

00:38:03.760 --> 00:38:06.630
reads and writes into it.

00:38:07.560 --> 00:38:10.330
The other big point is you want
to think about memory access.

00:38:10.380 --> 00:38:11.750
The threads are all working together.

00:38:11.980 --> 00:38:15.220
They're going to see similar
memory-- they're going to perform

00:38:15.220 --> 00:38:16.780
their memory accesses together.

00:38:16.830 --> 00:38:18.980
Don't waste time
optimizing for one thread.

00:38:18.980 --> 00:38:23.980
Don't start with the most complex
CPU data structure you know

00:38:23.980 --> 00:38:25.750
about to minimize memory access.

00:38:25.830 --> 00:38:26.860
Just start with the brute force one.

00:38:26.860 --> 00:38:27.580
You'll be surprised.

00:38:27.680 --> 00:38:32.390
It's simple, and you'll get great
performance right off the bat.

00:38:34.070 --> 00:38:35.000
So that was a brief overview.

00:38:35.000 --> 00:38:38.320
I'm going to introduce now the next Ian,
and he's going to talk about

00:38:38.320 --> 00:38:40.100
optimizing compute kernels.

00:38:40.120 --> 00:38:41.480
Thanks.

00:38:47.040 --> 00:38:47.340
Thank you.

00:38:47.400 --> 00:38:51.280
So it was something of a question
to me what to say after five people

00:38:51.280 --> 00:38:52.700
in a row have talked about OpenCL.

00:38:52.700 --> 00:38:55.580
So I think the best thing I can do
right now is to sort of bring it

00:38:55.580 --> 00:38:58.650
all together and give an example of
sort of bringing it down to earth

00:38:58.650 --> 00:39:01.580
in the context of how you'd be
programming in your own application.

00:39:04.680 --> 00:39:07.280
So the first question you
have to ask yourself is,

00:39:07.280 --> 00:39:08.970
like, when do I use OpenCL?

00:39:09.000 --> 00:39:10.760
There are a lot of other
technologies out there,

00:39:10.830 --> 00:39:12.500
so I'll try to give you some context.

00:39:12.580 --> 00:39:15.360
And then after that,
I'm going to work through a version

00:39:15.360 --> 00:39:20.180
of the NBody simulation that you've
been seeing to sort of show how our

00:39:20.190 --> 00:39:24.360
developer tools work in conjunction
with this and some survival

00:39:24.480 --> 00:39:26.700
techniques for working with this.

00:39:27.490 --> 00:39:29.230
So when to use OpenCL?

00:39:29.320 --> 00:39:31.560
OpenCL, as you've learned,
is great for large,

00:39:31.650 --> 00:39:33.180
data-intensive problems.

00:39:33.220 --> 00:39:36.480
It's really good for
expensive computations,

00:39:36.480 --> 00:39:39.020
anything that might touch
data more than once,

00:39:39.020 --> 00:39:41.580
like an NLogN implementation in op.

00:39:41.690 --> 00:39:45.410
And it's great for highly
parallelizable problems.

00:39:45.540 --> 00:39:48.800
If your problem is really tiny,
then perhaps if you'd just written

00:39:48.820 --> 00:39:52.710
a little piece of code to do that,
it might have finished before

00:39:52.780 --> 00:39:54.650
OpenCL even got started.

00:39:54.730 --> 00:39:57.370
If you're just limited by the
rate at which you can copy data

00:39:57.380 --> 00:40:00.410
around by the bus bandwidth,
then you don't need a lot of

00:40:00.410 --> 00:40:02.030
software to help you out on this.

00:40:02.030 --> 00:40:06.520
You're basically bottlenecked by the bus,
and the software isn't

00:40:06.830 --> 00:40:09.370
going to change that.

00:40:09.720 --> 00:40:11.890
Think twice before putting
inherently sequential problems

00:40:11.890 --> 00:40:13.600
that simply can't be parallelized.

00:40:13.600 --> 00:40:16.600
OpenCL is a data parallel
programming environment.

00:40:16.600 --> 00:40:20.590
If you have no parallelism,
it's not going to help you much.

00:40:20.690 --> 00:40:23.740
And similarly, you could have a highly
parallel algorithm,

00:40:23.740 --> 00:40:27.600
but it might be constantly synchronizing
back and forth and tripping over itself.

00:40:27.670 --> 00:40:29.430
So those things might
not work so well either.

00:40:29.840 --> 00:40:32.880
And finally,
the OpenCL kernels right now don't

00:40:32.970 --> 00:40:35.600
have object-oriented code in them.

00:40:35.600 --> 00:40:39.930
and of course call OpenCL from
object-oriented code.

00:40:41.310 --> 00:40:43.630
So here's examples of some
other high-performance

00:40:43.680 --> 00:40:44.960
technologies you might use.

00:40:44.960 --> 00:40:48.210
There's Accelerate Framework,
which is Apple's proprietary

00:40:48.210 --> 00:40:54.060
framework consisting of APIs for
common computational algorithms like

00:40:54.100 --> 00:40:58.260
convolutions or FFTs or linear algebra,
big number computation,

00:40:58.350 --> 00:40:59.420
that kind of thing.

00:41:00.440 --> 00:41:03.490
And they're all hand-tuned by engineers
who sit there and stare at one piece of

00:41:03.490 --> 00:41:06.490
code all day and eventually check it in.

00:41:06.490 --> 00:41:11.030
And it's really great if it does
the thing that you need to do.

00:41:11.030 --> 00:41:13.400
And I used to work in that
group up until recently.

00:41:13.400 --> 00:41:15.510
And that was always
sort of my frustration,

00:41:15.510 --> 00:41:17.900
is like I would have to sort
of predict ahead of time

00:41:17.900 --> 00:41:19.520
what everybody needed to do.

00:41:19.520 --> 00:41:21.880
And sometimes I get it right,
sometimes I get it wrong.

00:41:21.880 --> 00:41:24.860
And quite often,
there's something in your application

00:41:24.860 --> 00:41:26.500
which is its heart and soul.

00:41:26.500 --> 00:41:29.940
It's a specialized computation
that you need to do that

00:41:29.950 --> 00:41:30.420
makes your application work.

00:41:30.440 --> 00:41:31.800
And if you don't know what
application what it is,

00:41:31.800 --> 00:41:33.020
that's probably not going to be there.

00:41:33.020 --> 00:41:35.680
So Accelerate Framework is
something you could plug in.

00:41:35.680 --> 00:41:38.390
It'll help in a few places,
but often you need to go

00:41:38.390 --> 00:41:40.760
ahead and write your own code.

00:41:40.770 --> 00:41:43.910
And that's really where OpenCL comes in,
because we let you write your own

00:41:43.910 --> 00:41:46.420
kernel and have it executed in
a highly parallel environment,

00:41:46.450 --> 00:41:48.320
hopefully with good performance.

00:41:48.660 --> 00:41:50.850
There's also OpenGL.

00:41:50.850 --> 00:41:54.670
If you have a purely graphical task,
it's an excellent thing.

00:41:55.250 --> 00:41:58.490
Previously in GPU areas,
people might have used

00:41:58.490 --> 00:41:58.770
OpenCL for a lot of things.

00:41:58.860 --> 00:41:58.860
But now, it's a little bit different.

00:41:58.860 --> 00:41:58.860
So if you're using
OpenCL for a lot of things,

00:41:58.860 --> 00:41:58.900
you might want to use
OpenCL for a lot of things.

00:41:58.900 --> 00:41:58.940
But now, it's a little bit different.

00:41:58.940 --> 00:41:58.940
So if you're using
OpenCL for a lot of things,

00:41:58.940 --> 00:41:58.960
you might want to use
OpenCL for a lot of things.

00:41:58.960 --> 00:42:03.120
People might have used OpenGL shaders
to do general computation,

00:42:03.120 --> 00:42:05.840
but you've got to couch your
algorithm in terms of pixels

00:42:05.840 --> 00:42:07.300
and vertices and whatever else.

00:42:07.300 --> 00:42:08.820
And so it kind of gets in the way.

00:42:08.820 --> 00:42:13.950
So that's great for graphics,
but OpenCL is really designed to open

00:42:13.950 --> 00:42:19.240
up the wide space of algorithms that
you might want to write and give you

00:42:19.260 --> 00:42:22.120
the language and ability to write those.

00:42:24.160 --> 00:42:28.790
So there's also the question of,
should I be using the GPU?

00:42:29.060 --> 00:42:31.840
And GPU has great bandwidth
to its own internal memory,

00:42:31.840 --> 00:42:34.610
and even better bandwidth to
the local memory as described.

00:42:34.650 --> 00:42:36.860
But you do have to get the
data over there right now

00:42:36.860 --> 00:42:38.030
with current architectures.

00:42:38.040 --> 00:42:42.210
And it's sitting on the
other side of a PCIe bus.

00:42:42.330 --> 00:42:46.940
The throughput over the PCIe bus is
about three to four gigabytes a second.

00:42:47.020 --> 00:42:49.460
And if you're thinking
in terms of CPU terms,

00:42:49.580 --> 00:42:55.390
well, that's maybe around one
byte for every CPU cycle.

00:42:55.490 --> 00:42:58.440
So what could I have done
on the CPU during that time?

00:42:58.440 --> 00:43:01.340
Well, I could do four single precision
multiplies in the vector unit.

00:43:01.340 --> 00:43:03.620
I could do four single precision adds.

00:43:03.620 --> 00:43:05.920
I could copy four floats
somewhere out of the register.

00:43:05.920 --> 00:43:08.700
I could have done four
loads as a vector load.

00:43:08.700 --> 00:43:10.420
I could have done four
stores as a vector load.

00:43:10.420 --> 00:43:12.460
And in fact,
I could have done all of those things

00:43:12.460 --> 00:43:14.780
in that one cycle concurrently.

00:43:14.790 --> 00:43:19.770
So you could get quite a
lot done in that CPU cycle.

00:43:20.280 --> 00:43:24.040
Usually when you're writing code,
you aren't operating on one byte.

00:43:24.070 --> 00:43:25.970
I might want to multiply
two floats together,

00:43:26.090 --> 00:43:27.310
and that's eight bytes of data.

00:43:27.410 --> 00:43:30.010
And during that time,
I could have done this much computation.

00:43:30.220 --> 00:43:32.860
And of course,
we don't have just one CPU.

00:43:32.990 --> 00:43:35.480
We might have eight CPUs.

00:43:35.620 --> 00:43:39.070
So I could have done that much work.

00:43:39.450 --> 00:43:43.220
And of course,
I might have to get the data back.

00:43:43.270 --> 00:43:45.500
So that might mean
copying eight bytes back.

00:43:45.520 --> 00:43:47.940
So I could have gotten that
much work done on the CPU.

00:43:48.010 --> 00:43:53.570
So you really want to focus on
the GPUs to move problems that

00:43:54.610 --> 00:44:00.220
Move your data over there and
keep it there as long as you can.

00:44:00.220 --> 00:44:02.710
You might have to copy it back
in order to do something with it,

00:44:02.730 --> 00:44:06.390
but you really want to just get it there
and take advantage of the bandwidth on

00:44:06.390 --> 00:44:11.060
the device and use that as best you can.

00:44:11.060 --> 00:44:16.900
And if that process is too expensive
for your overall computation,

00:44:16.900 --> 00:44:20.540
then maybe you're better
off just stay on the CPU.

00:44:20.540 --> 00:44:20.540
Don't bounce beta back and forth.

00:44:21.400 --> 00:44:26.560
and so I'm going to move
forward onto the simulation.

00:44:26.560 --> 00:44:31.560
This is the same simulation or a similar
one as the one you've seen before.

00:44:31.560 --> 00:44:33.590
We have a system of n particles.

00:44:33.600 --> 00:44:37.240
They all have a gravity
interaction between them all,

00:44:37.240 --> 00:44:39.400
and we calculate them all.

00:44:39.480 --> 00:44:41.500
A little bit of a disclosure here.

00:44:41.500 --> 00:44:42.860
We're using a substandard algorithm.

00:44:42.860 --> 00:44:45.390
We're really actually calculating
the direct interaction between every

00:44:45.390 --> 00:44:48.420
particle and every other particle,
so there's an n squared cost here.

00:44:48.420 --> 00:44:52.700
There are algorithms published out
which are significantly cheaper.

00:44:52.700 --> 00:44:54.830
So in our context,
we're doing this because

00:44:54.830 --> 00:44:56.980
it's great for demos,
but it's not going to

00:44:56.980 --> 00:44:58.030
be good for your users.

00:44:58.070 --> 00:45:00.970
So if you really want to sell a product,
make sure you're using

00:45:00.980 --> 00:45:02.060
the right algorithm.

00:45:02.060 --> 00:45:04.390
If you're a scientist,
obviously you want to make sure

00:45:04.390 --> 00:45:05.640
you get in the right journal.

00:45:05.640 --> 00:45:08.680
You don't want to stand up here in
front of a thousand people and then

00:45:08.680 --> 00:45:12.110
have a second year grad student say,
I used a different algorithm,

00:45:12.120 --> 00:45:15.520
and I can get better performance
with my scientific calculator.

00:45:15.520 --> 00:45:20.390
So -- It's always disheartening when
you hear that after months of work.

00:45:23.160 --> 00:45:26.840
Make sure you have the
right algorithm first.

00:45:26.840 --> 00:45:28.280
This is not just OpenCL.

00:45:28.280 --> 00:45:30.580
This is any bit of optimization.

00:45:30.580 --> 00:45:33.520
So you don't want to pour your
heart and soul into something

00:45:33.520 --> 00:45:37.840
and realize that you're just
completely on the wrong track.

00:45:38.000 --> 00:45:41.110
So I'd like to switch over to the demo.

00:45:49.400 --> 00:47:13.400
[Transcript missing]

00:47:13.730 --> 00:47:15.660
So we can run that.

00:47:15.660 --> 00:47:19.720
And I haven't really done any
tweaking on the graphics on this,

00:47:19.760 --> 00:47:22.480
so it's just going to show
the particles moving around.

00:47:22.480 --> 00:47:26.860
And we're getting somewhere
around 2 gigaflops.

00:47:26.860 --> 00:47:29.620
But as you notice,
this is probably not something

00:47:29.620 --> 00:47:33.800
the end user is going to want,
because this is barely moving.

00:47:33.800 --> 00:47:38.100
And it's flipping the
frames every second or two.

00:47:38.170 --> 00:47:40.580
So well, OK, what do I need to do?

00:47:40.580 --> 00:47:42.220
Well, let's haul out Shark.

00:47:42.220 --> 00:47:46.770
And I can quickly take a sample on it
and see what my application is doing.

00:47:46.980 --> 00:47:48.440
Am I spending too much time drawing?

00:47:48.440 --> 00:47:51.500
Am I spending too much
time doing something else?

00:47:51.500 --> 00:47:52.300
No, indeed.

00:47:52.300 --> 00:47:55.020
My update particles,
which is this function I just showed

00:47:55.020 --> 00:48:00.010
you that iterates over the particles,
is actually consuming 98.6% of the time.

00:48:00.050 --> 00:48:04.010
And so obviously,
I need to focus my efforts there.

00:48:04.180 --> 00:48:06.300
Well, I've heard about OpenCL.

00:48:06.300 --> 00:48:09.140
And those guys from Apple,
they told me that, oh,

00:48:09.140 --> 00:48:10.980
I could use that to run on multiple CPUs.

00:48:10.980 --> 00:48:12.220
Obviously, I'm not using it.

00:48:12.250 --> 00:48:13.880
I've got the full potential of my device.

00:48:13.880 --> 00:48:15.790
I've only got one CPU busy with this.

00:48:15.970 --> 00:48:18.220
So let's try that.

00:48:18.220 --> 00:48:23.390
I'll put this away and switch
over to the OpenCL project.

00:48:24.390 --> 00:48:26.800
So what have I done here?

00:48:26.960 --> 00:48:31.480
Well, I have the same create particle
set that initializes my positions.

00:48:31.480 --> 00:48:34.540
And at the end, instead of just putting
it in a regular array,

00:48:34.630 --> 00:48:37.300
I've created a CL array using that data.

00:48:37.300 --> 00:48:41.300
And this is a device that just tells CL,
okay, here's my data.

00:48:41.300 --> 00:48:43.070
And CL needs this because it
needs to be able to copy your

00:48:43.070 --> 00:48:47.530
data over onto the compute device,
whatever that may be.

00:48:48.190 --> 00:48:53.480
I've also taken my
original update kernel,

00:48:53.510 --> 00:48:59.700
update particles function,
and rewritten it as a kernel.

00:48:59.760 --> 00:49:02.090
Here I've got the kernel written
as a series of C strings.

00:49:02.140 --> 00:49:04.020
You can also put it off in
a separate file if you like.

00:49:04.110 --> 00:49:09.080
I have it this way because I wanted
to have a C preprocessor thing

00:49:09.080 --> 00:49:11.720
embedded in there for my own use.

00:49:11.920 --> 00:49:20.610
And you can see that these look pretty
similar with one interesting thing.

00:49:20.850 --> 00:49:24.090
You'll notice the inner loop actually
is quite a bit smaller in CL,

00:49:24.090 --> 00:49:28.110
and that's because CL has native vector
types and native vector arithmetic.

00:49:28.110 --> 00:49:31.590
So whereas before, I had to write out in
scalar code the x component,

00:49:31.660 --> 00:49:34.550
the y component,
and the z component for calculating the

00:49:34.550 --> 00:49:37.580
distance from one object to another,
here I can just do it in

00:49:37.580 --> 00:49:39.260
one line of vector code.

00:49:39.280 --> 00:49:42.140
And down here, again,
I've got the velocity component

00:49:42.140 --> 00:49:44.750
calculated component-wise,
whereas I can do it in

00:49:44.750 --> 00:49:46.220
one line of vector code.

00:49:46.220 --> 00:49:48.020
So in some ways,
this may actually clean up your

00:49:48.020 --> 00:49:50.680
code and give you a little bit
more of an idea of what's going on.

00:49:50.700 --> 00:49:53.790
So I've added a little bit of
vector acceleration for free,

00:49:53.790 --> 00:49:55.420
pretty much.

00:49:55.420 --> 00:49:59.340
So we can run this.

00:49:59.450 --> 00:50:00.350
Stop the other task.

00:50:00.350 --> 00:50:01.160
Run the new one.

00:50:01.160 --> 00:50:01.990
Here's my thing.

00:50:02.090 --> 00:50:02.410
Oops.

00:50:02.410 --> 00:50:03.020
Oh, dear.

00:50:03.020 --> 00:50:04.320
There went all my data.

00:50:04.320 --> 00:50:05.660
Well, what did I do wrong?

00:50:05.660 --> 00:50:11.400
So just to reassure you, this is a bug,
but I found this bug two weeks ago.

00:50:11.400 --> 00:50:15.600
So I'll just walk through the
process I went through to find

00:50:15.690 --> 00:50:17.740
out what was causing this.

00:50:18.620 --> 00:50:20.680
So I've added a little printout.

00:50:20.700 --> 00:50:23.470
I've added a little pnf in here
to read the output from the array,

00:50:23.480 --> 00:50:27.000
just to get some hint
about what happened.

00:50:27.150 --> 00:50:29.410
So I can run the debug version.

00:50:29.570 --> 00:50:33.520
It should spew coordinates from my stuff.

00:50:33.520 --> 00:50:36.980
And eventually,
if this was working correctly,

00:50:36.980 --> 00:50:39.930
we would actually see NANDs appear.

00:50:40.110 --> 00:50:42.100
It looks like it's freezing.

00:50:42.100 --> 00:50:45.100
So anyway, what we saw was a bunch of
NANDs appear in the data.

00:50:45.100 --> 00:50:46.590
You don't have to bear with me.

00:50:46.730 --> 00:50:51.480
So like, well,
where were these coming from?

00:51:01.730 --> 00:51:06.910
So what I can do is when
OpenCL compiles your kernel,

00:51:06.910 --> 00:51:08.800
it's compiling it at runtime.

00:51:08.830 --> 00:51:12.690
So the usual set of symbols that would
normally be expected to accompany

00:51:12.760 --> 00:51:14.960
your binary in this case aren't there.

00:51:14.960 --> 00:51:18.120
So there's no symbol to stay in GBA,
okay,

00:51:18.120 --> 00:51:20.920
I want you to break on my filter here,
or kernel here,

00:51:20.920 --> 00:51:24.000
because it doesn't know what
that is because there's no

00:51:24.000 --> 00:51:27.020
symbol in your binary because the
function never was in your binary.

00:51:27.020 --> 00:51:28.900
It's just sitting here as a C string.

00:51:28.900 --> 00:51:30.220
So the question is how do I stop there?

00:51:30.320 --> 00:51:33.480
So what I've done is I've taken
my kernel and I've instrumented

00:51:33.480 --> 00:51:35.580
it with some new lines of code.

00:51:35.580 --> 00:51:37.320
Here I'm just looking at all of my data.

00:51:37.540 --> 00:51:42.020
I have one for the position and one
for a distance and one for a velocity.

00:51:42.050 --> 00:51:45.460
And I'm checking to see if any is NANDs
-- any NANDs appeared in the data.

00:51:45.460 --> 00:51:47.940
And if so,
I call this function called breakpoint.

00:51:47.940 --> 00:51:50.140
Is breakpoint some
magical thing in OpenCL?

00:51:50.140 --> 00:51:50.960
Absolutely not.

00:51:51.120 --> 00:51:52.900
It's just a little function
I wrote right here,

00:51:52.900 --> 00:51:55.770
and I'm going to put a breakpoint there.

00:51:59.400 --> 00:52:02.900
So I can run this in the debugger.

00:52:02.900 --> 00:52:07.280
And once it comes up,
I noticed I landed in my breakpoint.

00:52:07.280 --> 00:52:13.050
One of the things I did
when I created this--

00:52:15.110 --> 00:52:17.460
The thing is I passed a different
number to each breakpoint,

00:52:17.460 --> 00:52:19.600
so I actually know which one it was.

00:52:19.700 --> 00:52:24.170
So this time I landed in breakpoint 3,
so I can go look and say, oh,

00:52:24.180 --> 00:52:25.680
breakpoint 3, okay, that's this one.

00:52:25.680 --> 00:52:28.170
So it looks like one of my input
velocities had a NAN in it.

00:52:28.290 --> 00:52:29.680
Well, how did that happen?

00:52:29.680 --> 00:52:31.110
Well, I have a hint here.

00:52:31.110 --> 00:52:33.750
I also put in a breakpoint
for the output velocity,

00:52:33.820 --> 00:52:36.470
so it seems like if my kernel
ran and produced a NAN,

00:52:36.470 --> 00:52:39.580
and then I read it in later,
I would have caught this earlier

00:52:39.580 --> 00:52:41.440
in a different breakpoint.

00:52:41.440 --> 00:52:44.440
So apparently my input
velocities are busted.

00:52:45.120 --> 00:52:47.820
In the very first frame, well,
how do I manage that?

00:52:47.820 --> 00:52:50.620
Well, we can go up to my
create particles thing,

00:52:50.850 --> 00:52:55.900
and I realize, well, okay,
these are actually a vector full of XYZW.

00:52:55.900 --> 00:53:01.940
I forgot to initialize the W,
so I have just random data in there.

00:53:01.940 --> 00:53:03.440
So let me fix this.

00:53:03.440 --> 00:53:07.100
And that's where the NAN came from,
or so I would assert.

00:53:07.100 --> 00:53:11.980
And I'll fix that up,
and we'll rerun it again.

00:53:11.980 --> 00:53:12.800
And there we go.

00:53:12.800 --> 00:53:14.400
Now the simulation is running.

00:53:15.000 --> 00:53:18.000
So -- and you can see we're
using all the CPUs up here,

00:53:18.000 --> 00:53:20.160
and I'm running much
faster than 2 gigaflops,

00:53:20.210 --> 00:53:22.600
and now somewhat closer to 10.

00:53:22.600 --> 00:53:23.960
So that's great.

00:53:24.090 --> 00:53:28.390
But we think, well, gee, wait a minute,
I am running on 8 CPUs here,

00:53:28.390 --> 00:53:30.840
but only going maybe 5 times faster.

00:53:30.850 --> 00:53:31.950
Well, what happened there?

00:53:32.330 --> 00:53:37.870
Well,
we can take a look in Shark once again.

00:53:41.000 --> 00:53:44.330
And so here is-- because we have no
symbol information for the kernel,

00:53:44.330 --> 00:53:48.120
it's just showing up as unknown library.

00:53:48.120 --> 00:53:51.280
So but this is, actually,
my update particles thing.

00:53:51.340 --> 00:53:53.510
And then we have a called a square root.

00:53:53.510 --> 00:53:54.470
Wait a minute.

00:53:54.490 --> 00:53:55.970
Was that in my last one?

00:53:56.030 --> 00:53:57.170
My original code?

00:53:57.190 --> 00:53:59.630
No, there's no called a square root.

00:53:59.630 --> 00:54:01.860
Well, my kernel does do a square root.

00:54:02.080 --> 00:54:03.840
Maybe that came from there.

00:54:03.840 --> 00:54:07.410
We can click down and see, oh,
there's a square root here.

00:54:07.470 --> 00:54:08.800
So how did that happen?

00:54:08.800 --> 00:54:12.330
Well, we can go and look at what the
GCC was doing in my original version.

00:54:12.330 --> 00:54:14.310
Yeah,
it's got the square root instruction.

00:54:14.480 --> 00:54:17.340
For some reason, OpenCL decided to call
the square root function.

00:54:17.510 --> 00:54:19.780
Well, that's not really entirely optimal.

00:54:19.890 --> 00:54:27.680
You'll have to believe me when I told
you I've already filed this bug.

00:54:29.230 --> 00:54:32.580
In this particular case,
since we're just writing a demo,

00:54:32.580 --> 00:54:35.630
we have to kind of ask ourselves, well,
did I really need a square

00:54:35.780 --> 00:54:37.000
root in the first place?

00:54:37.080 --> 00:54:40.070
And it turns out that in addition
to providing a full math library,

00:54:40.070 --> 00:54:43.060
OpenCL also has a full half precision
math library which it does all

00:54:43.060 --> 00:54:45.650
its work in single precision,
but it doesn't guarantee results

00:54:45.650 --> 00:54:47.570
for anything better than what you
would need for half precision,

00:54:47.570 --> 00:54:50.630
which is a 16 bit floating point type.

00:54:50.740 --> 00:54:54.570
And since this is just a demo,
we can go in here and we can

00:54:54.570 --> 00:54:57.200
change this to use that function.

00:54:57.200 --> 00:55:02.900
And the advantage of throwing away some
precision is that it can run much faster.

00:55:02.900 --> 00:55:04.350
So once again,
if you're going to go present

00:55:04.430 --> 00:55:07.600
in a scientific conference,
I don't recommend doing this,

00:55:07.600 --> 00:55:10.600
but since we're just at WWDC.

00:55:11.560 --> 00:55:13.380
Just go for it.

00:55:13.480 --> 00:55:16.870
So we'll rerun it.

00:55:17.630 --> 00:55:33.490
And I have a little problem
with things getting stuck.

00:55:33.490 --> 00:55:33.490
Let's see if I can get it to run.

00:55:33.490 --> 00:55:33.490
OK, well, there it goes.

00:55:33.490 --> 00:55:33.490
So we're getting about 22 gigaflops,
which is a lot better than 10.

00:55:34.200 --> 00:55:35.000
Performing much better.

00:55:35.000 --> 00:55:38.470
So we've gone through multi-threading
through the CPU and fixing one

00:55:38.470 --> 00:55:39.740
little thing with the divide.

00:55:39.740 --> 00:55:43.050
We've gone up a factor
of 11 in performance.

00:55:43.270 --> 00:55:46.700
Well, we're not entirely done here.

00:55:46.850 --> 00:55:47.880
Let me run this again.

00:55:47.880 --> 00:55:51.880
And then I can sample it in Shark.

00:55:51.880 --> 00:55:56.760
Let's jiggle this so it works.

00:55:57.250 --> 00:56:00.320
If we go actually look at
what the kernel was doing,

00:56:00.340 --> 00:56:03.400
we notice that there's actually
quite a bit of scalar code in there.

00:56:03.400 --> 00:56:05.600
And that's because I haven't
really fully vectorized my stuff.

00:56:05.600 --> 00:56:08.670
I have vector code here
and vector code here,

00:56:09.130 --> 00:56:12.090
but this is all more
or less scalar stuff.

00:56:12.130 --> 00:56:15.550
And we can see that if you
like looking at assembly,

00:56:15.660 --> 00:56:19.930
we can in here, and we see all of these
scalar instructions right

00:56:19.980 --> 00:56:21.780
in the middle of the loop.

00:56:21.780 --> 00:56:23.790
So what can we do about that?

00:56:23.790 --> 00:56:25.700
So can I switch back
to slides for a moment?

00:56:32.580 --> 00:56:35.630
So the current data layout,
as I explained earlier,

00:56:35.660 --> 00:56:38.130
just has packed x, y, z, w in it.

00:56:38.430 --> 00:56:40.550
And for this calculation,
as we've already learned,

00:56:40.670 --> 00:56:42.680
w doesn't really do anything.

00:56:42.680 --> 00:56:45.400
And in fact, it causes trouble if there
happens to be a nan in that value.

00:56:45.400 --> 00:56:48.670
So this is sort of bad
in a number of respects.

00:56:48.690 --> 00:56:50.830
It wastes space on touching
more data than I need to.

00:56:50.830 --> 00:56:54.850
And it forces us to write scalar
code in order to do much of the work.

00:56:54.910 --> 00:56:57.410
And that's because,
as part of my calculation,

00:56:57.410 --> 00:57:00.830
I need the z component to talk to
the x component in vector code.

00:57:00.830 --> 00:57:02.390
They're in the wrong place in the vector.

00:57:02.510 --> 00:57:05.980
So you have to move the z over by
the x and then do the operation.

00:57:05.980 --> 00:57:09.260
And that sort of, in fact,
has the effect of turning what should

00:57:09.280 --> 00:57:12.100
be vector code into something which
is executing more like scalar code.

00:57:12.230 --> 00:57:14.170
Except it's worse than that,
because I have the extra

00:57:14.170 --> 00:57:16.650
operations in there required to
move the z over to where the x is.

00:57:16.730 --> 00:57:19.870
So instead, what I'm going to do
is reorganize my data.

00:57:19.870 --> 00:57:23.230
So I'm walking through it in a
little more straight line fashion

00:57:23.300 --> 00:57:25.000
for what the CPU would like.

00:57:25.060 --> 00:57:27.710
I've thrown out the w component,
and I put the x, y,

00:57:27.710 --> 00:57:28.960
and z in their own array.

00:57:29.030 --> 00:57:31.130
So now when I load a
vector of four things,

00:57:31.130 --> 00:57:32.430
it has four x's in it.

00:57:32.500 --> 00:57:34.380
So I can do that for
four different particles.

00:57:34.400 --> 00:57:36.690
And that uses 25% less space.

00:57:36.770 --> 00:57:40.600
All the operations are vertical now,
and I have no more scalar code.

00:57:40.600 --> 00:57:45.840
So I can show that code.

00:57:50.500 --> 00:57:51.840
And so there it is running.

00:57:51.880 --> 00:57:58.340
So that's a pretty good speedup from
220 to 84 gigaflops for vector code.

00:57:58.340 --> 00:58:01.700
And we can actually go look at what
code I had to write to get that,

00:58:01.700 --> 00:58:02.960
and it's pretty similar code.

00:58:02.960 --> 00:58:06.240
Here it is.

00:58:06.240 --> 00:58:07.320
It's the same thing over and over again.

00:58:07.320 --> 00:58:10.520
The big difference is that X, Y,
and Z are now float 4s.

00:58:10.520 --> 00:58:13.310
And so this is the vector code,
and you'll notice there

00:58:13.310 --> 00:58:16.360
isn't a vec underbar or an
mm underbar anywhere in this.

00:58:16.360 --> 00:58:18.840
It's all vectorized in OpenCL.

00:58:19.180 --> 00:58:23.060
And so it's a lot cleaner and
hopefully more portable this way.

00:58:23.060 --> 00:58:27.510
So you can do it like that.

00:58:27.990 --> 00:58:30.280
So the next question is, well, great,
I've done that.

00:58:30.280 --> 00:58:34.440
And now I know that some of
my users will be using GPUs.

00:58:34.440 --> 00:58:36.690
Not all of them, of course,
but some of them will.

00:58:36.700 --> 00:58:39.600
So I want to make sure
this works OK on the GPU.

00:58:39.610 --> 00:58:45.420
So I can change one
character here to GPU.

00:58:45.420 --> 00:58:47.520
And we'll rebuild it.

00:58:47.520 --> 00:58:50.610
And we'll run it.

00:58:51.500 --> 00:58:52.340
There we go.

00:58:52.560 --> 00:58:53.580
Starting to move up.

00:58:53.740 --> 00:58:56.820
And you'll notice that we aren't
really getting the hundreds of

00:58:56.820 --> 00:58:59.650
gigaflops that we were hoping for.

00:59:00.870 --> 00:59:04.950
Optimization has its
sort of ups and downs.

00:59:05.370 --> 00:59:06.700
And we have to accept what we get.

00:59:06.700 --> 00:59:10.830
And the reason is that I really
haven't done anything yet

00:59:11.220 --> 00:59:12.100
to optimize for the GPU.

00:59:12.100 --> 00:59:14.320
And it turns out that all
this stuff that Ian Buck was

00:59:14.370 --> 00:59:16.560
talking about is very important.

00:59:16.660 --> 00:59:19.560
So I'll cut the slides.

00:59:19.570 --> 00:59:22.390
And I actually tried quite a few things.

00:59:22.460 --> 00:59:26.200
I wrote several kernels
here and several down there.

00:59:26.200 --> 00:59:28.510
So I'll cut the slides and show
you what actually worked for me.

00:59:33.460 --> 00:59:37.450
So the first thing I tried was to
look at how many threads I was using.

00:59:37.560 --> 00:59:44.020
In my CPU thing,
I defined 512 threads just because

00:59:44.020 --> 00:59:45.460
that was the number I picked.

00:59:45.480 --> 00:59:47.160
And it turned out that wasn't enough.

00:59:47.320 --> 00:59:50.390
And if I actually define
thousands and thousands,

00:59:50.390 --> 00:59:53.690
or in this case, 16,000 threads,
it ran twice as fast.

00:59:53.740 --> 00:59:56.960
So that was kind of educational.

00:59:57.060 --> 01:00:00.130
The other thing is that because I'm
running over the particle list over and

01:00:00.130 --> 01:00:03.290
over and over again in every thread,
it turns out that for

01:00:03.290 --> 01:00:06.350
every particle I do,
I end up touching all the other ones,

01:00:06.350 --> 01:00:09.180
which means 16,000 loads for
every particle I calculate.

01:00:09.420 --> 01:00:13.880
And so this is a case where copying
data to local memory is very important.

01:00:14.020 --> 01:00:18.030
And I got a six-fold speed
increase on top of the two-fold

01:00:18.030 --> 01:00:20.620
I already got by doing that.

01:00:20.770 --> 01:00:23.030
Now,
it's not true that in every algorithm,

01:00:23.070 --> 01:00:24.500
local is going to help you.

01:00:24.500 --> 01:00:26.920
There are going to be some things
where you really actually end

01:00:26.980 --> 01:00:28.340
up only touching the data once.

01:00:28.340 --> 01:00:30.020
So copying from global to
local and reading it there

01:00:30.080 --> 01:00:30.680
isn't really going to help you.

01:00:30.680 --> 01:00:32.800
It's not really going to
be any much better than

01:00:33.110 --> 01:00:34.700
directly accessing the global.

01:00:34.710 --> 01:00:39.860
But you can define your problem
set to use the image type,

01:00:40.190 --> 01:00:41.710
which is cached.

01:00:41.730 --> 01:00:44.900
And you can take advantage of
the speed improvement there,

01:00:44.910 --> 01:00:48.150
which I did,
but I don't have enough time to show you.

01:00:48.220 --> 01:00:49.640
I also did a few other things.

01:00:49.640 --> 01:00:51.740
I unrolled the inner loop four times.

01:00:51.740 --> 01:00:54.130
This is the same sort of
trick you might use on a CPU.

01:00:54.320 --> 01:00:58.320
It cuts down the loop overhead
and gives the compiler better

01:00:58.340 --> 01:01:00.560
scheduling opportunities.

01:01:00.660 --> 01:01:00.660
And then I did a couple of things.

01:01:00.660 --> 01:01:01.660
I unrolled the inner loop four times.

01:01:01.660 --> 01:01:02.840
I did a little more
optimization where I said,

01:01:03.090 --> 01:01:05.670
well, I have a loop here to
copy from global to local,

01:01:05.760 --> 01:01:10.060
but this is doing some strange things
because some of the threads are looping

01:01:10.060 --> 01:01:12.800
through this twice and some of them
are looping through it three times.

01:01:12.990 --> 01:01:16.150
And that means that there's
some divergence in what

01:01:16.150 --> 01:01:18.020
the processors are doing.

01:01:18.020 --> 01:01:22.530
So I carefully controlled the number
of thread groups I had according to the

01:01:22.530 --> 01:01:26.200
local memory size that I had allocated
so that all the threads iterate through

01:01:26.200 --> 01:01:27.700
the loop the same number of times.

01:01:27.700 --> 01:01:30.640
And that gave me another 40
percent over the top of that.

01:01:31.530 --> 01:01:37.380
So the overall improvement from my rather
disappointing result at first is 22x.

01:01:37.510 --> 01:01:41.380
So I'll cut back to the slides,
and we can see that.

01:01:41.400 --> 01:01:49.060
So there we have the
demo running on the GPU.

01:01:53.600 --> 01:01:57.880
For comparison,
I've got the original ticking along.

01:01:57.880 --> 01:02:00.940
It is moving, I promise.

01:02:00.950 --> 01:02:02.030
So there you go.

01:02:02.100 --> 01:02:09.720
So can we cut back to slides, please?

01:02:13.080 --> 01:02:17.840
So I'd like to invite Jeff Stollup
to take any questions and to

01:02:17.900 --> 01:02:22.460
talk about downloading materials
for working on these things.

01:02:31.680 --> 01:02:33.040
Thanks very much Ian.

01:02:33.040 --> 01:02:37.440
So, again,
Alan Schaffer is a Schaffer at Apple.com,

01:02:37.470 --> 01:02:43.100
a good point of contact for questions you
may have in general about technologies,

01:02:43.110 --> 01:02:44.440
and he can route those things to us.

01:02:44.460 --> 01:02:46.800
The OpenCL developer resources,
as I said,

01:02:46.800 --> 01:02:50.640
are on the attendee website for this
session and the previous session.

01:02:51.080 --> 01:02:54.240
Definitely a must download for
you guys who want to use OpenCL.

01:02:54.240 --> 01:02:57.320
It has the specification,
has release notes,

01:02:57.320 --> 01:03:00.550
has an example project,
and also has an update for

01:03:00.550 --> 01:03:02.480
the OpenCL on your C disk.

01:03:02.510 --> 01:03:05.800
And finally,
we have a lab in the Mac Lab B this

01:03:05.810 --> 01:03:13.860
afternoon at 2:00 p.m., and then the
OpenGL lab should be in the graphics

01:03:13.940 --> 01:03:15.580
and media lab same time at 2:00 p.m.

01:03:15.580 --> 01:03:17.440
if you have OpenGL related questions.