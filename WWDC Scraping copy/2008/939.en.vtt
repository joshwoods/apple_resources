WEBVTT

00:00:20.140 --> 00:00:20.560
Good morning.

00:00:20.560 --> 00:00:26.640
I'm Phil Kerly, and I'm from Intel's
Software and Solutions Group.

00:00:26.640 --> 00:00:30.170
And today I'm going to be talking
about performance optimization

00:00:30.170 --> 00:00:32.240
techniques using Intel Libraries.

00:00:32.300 --> 00:00:37.210
This is actually going to delve
into the topic previously by James.

00:00:37.240 --> 00:00:42.360
I hope that you had a chance to
listen to that topic as well,

00:00:42.360 --> 00:00:44.620
because this will definitely
fit right into his one,

00:00:44.620 --> 00:00:50.050
two, and three recommendations,
focused on using libraries that are

00:00:50.340 --> 00:00:56.590
already optimized for threading,
using OpenMP, and then building on top of

00:00:56.690 --> 00:01:01.000
that with things like the
Intel Threading Building Blocks.

00:01:02.940 --> 00:01:07.060
So I had an opportunity,
this is actually my fourth WWDC,

00:01:07.060 --> 00:01:10.330
the third one that I've had
an opportunity to talk at.

00:01:10.360 --> 00:01:15.880
Last year, I talked in quite a bit
of detail on P-Threads,

00:01:15.880 --> 00:01:19.940
OpenMP,
and the Intel Threading Building Blocks.

00:01:19.940 --> 00:01:24.330
And we actually had a demo last year
that was a little bit far-fetched.

00:01:24.390 --> 00:01:27.010
It was more just the demo
of the different techniques.

00:01:27.060 --> 00:01:30.040
And this year, I thought, you know,
we ought to look at something that's a

00:01:30.040 --> 00:01:31.660
little bit more closer to real world.

00:01:32.340 --> 00:01:35.550
Something that, you know,
kind of addresses what

00:01:35.550 --> 00:01:38.270
James talked about in terms of,
you know,

00:01:38.280 --> 00:01:44.100
looking at increasing complexity,
functionality, and quality of software.

00:01:46.750 --> 00:01:51.410
So we're going to talk about
a face tracker application,

00:01:51.460 --> 00:01:55.420
which is using the open
computer vision library,

00:01:55.420 --> 00:01:57.540
which Intel has open sourced.

00:01:57.570 --> 00:02:00.280
We're going to look at the
existing threading model.

00:02:00.280 --> 00:02:05.160
The interesting thing about the
existing threading model is I figured,

00:02:05.160 --> 00:02:08.560
you know,
Intel released OpenCV as an open

00:02:08.560 --> 00:02:13.880
source application or a library
to support computer vision.

00:02:14.480 --> 00:02:19.910
It released the face tracker application,
and I thought, wow,

00:02:19.910 --> 00:02:23.720
this will be a great opportunity to look
at its threading model and see how it

00:02:23.720 --> 00:02:27.520
scales on the Mac Pro up to eight cores.

00:02:29.590 --> 00:02:32.150
And then we're going to look at how
we can actually improve that using

00:02:32.150 --> 00:02:34.340
the Intel Threading Building Blocks.

00:02:34.340 --> 00:02:38.360
So the existing application
doesn't use TBB,

00:02:38.360 --> 00:02:41.570
and so we're going to look at
how we can actually improve that,

00:02:41.580 --> 00:02:44.280
and then we'll wrap
that up with a summary.

00:02:46.400 --> 00:02:50.500
So first of all, OpenCV is an open-source
computer vision library.

00:02:50.500 --> 00:02:54.600
It's a collection of algorithms
and sample applications.

00:02:54.600 --> 00:02:56.790
Space Tracker just
happens to be one of them.

00:02:56.800 --> 00:03:00.100
You can actually
download the source code.

00:03:00.100 --> 00:03:05.100
It's available--prebuilt libraries
available for the Mac OS X,

00:03:05.100 --> 00:03:08.300
Linux, as well as Microsoft Windows.

00:03:08.300 --> 00:03:12.600
So it's cross-platform supported.

00:03:13.360 --> 00:03:15.850
It is released under the
Intel License Agreement for

00:03:15.850 --> 00:03:17.560
Open Source Computer Vision Library.

00:03:17.560 --> 00:03:21.660
You can go to the website and you can
find out exactly what that license is,

00:03:21.660 --> 00:03:25.660
but it's pretty much almost
a free use for both private

00:03:25.660 --> 00:03:27.700
as well as commercial use.

00:03:29.370 --> 00:03:32.740
And here are the references
for the open source cv.org.

00:03:32.780 --> 00:03:37.420
Again, you can go and check out the
actual details of OpenCV.

00:03:38.990 --> 00:03:44.440
But what OpenCV actually targets
is the computer vision domain.

00:03:44.520 --> 00:03:48.690
And so at the base of it is
the object identification.

00:03:49.450 --> 00:03:51.400
Being able to recognize
different objects.

00:03:51.400 --> 00:03:54.340
That can be extended
to facial recognition.

00:03:54.340 --> 00:03:59.020
If you can do facial recognition
and you can do motion tracking,

00:03:59.020 --> 00:04:02.110
then you can start looking at
gesture recognition and can improve

00:04:02.590 --> 00:04:04.670
on the human-computer interaction.

00:04:06.000 --> 00:04:11.280
So this is an area where I think there's
a lot of opportunity for parallelism,

00:04:11.280 --> 00:04:15.920
and there's a lot of opportunity
to take advantage of the processing

00:04:15.920 --> 00:04:20.340
power that is available on the
Intel multi-core platforms.

00:04:24.840 --> 00:04:27.140
First of all,
before we get too far into it,

00:04:27.140 --> 00:04:30.560
we might as well just give you a little
demo that actually shows you what the

00:04:30.660 --> 00:04:34.080
Face Tracker application looks like,
and you can begin to see

00:04:34.080 --> 00:04:36.180
what we're talking about.

00:04:36.180 --> 00:04:40.640
So Palavi's joining me on stage,
and she'll take over for the demo.

00:04:40.640 --> 00:04:42.680
Well, hello, everyone.

00:04:42.680 --> 00:04:46.920
I'm also an engineer with the
Software Solutions Group back at Intel,

00:04:46.920 --> 00:04:51.660
and I'm going to demo the open-source
version of the Face Tracker application,

00:04:52.210 --> 00:04:55.240
which you can download
from the opencv.org site.

00:04:55.240 --> 00:04:57.660
So back to the demo machine.

00:05:04.120 --> 00:05:07.090
So this demo,
which I'm going to show you,

00:05:07.090 --> 00:05:09.610
is going to take input from a camera,
which we have here.

00:05:10.110 --> 00:05:13.590
And let me just run it here.

00:05:19.400 --> 00:05:21.570
So as you can see,
it's taking input from the camera.

00:05:21.680 --> 00:05:23.400
It's me, who you see on screen.

00:05:23.400 --> 00:05:28.040
And it's tracking my face as
I'm moving across the screen.

00:05:28.040 --> 00:05:29.920
It tracks my face.

00:05:29.920 --> 00:05:32.480
So this is the original version.

00:05:32.480 --> 00:05:34.110
And look at the chart.

00:05:34.570 --> 00:05:37.920
Look at the CPU utilization
on the right side.

00:05:37.920 --> 00:05:40.440
And as you can see,
it's about 100%. This is

00:05:40.460 --> 00:05:45.160
an 8-core machine,
3.2 gigahertz processors.

00:05:45.160 --> 00:05:48.500
And back to Phil.

00:06:01.120 --> 00:06:06.740
We're going to switch back to the slides.

00:06:06.830 --> 00:06:10.440
So this application is
fairly straightforward.

00:06:10.500 --> 00:06:14.340
If you download the executable
and look at the source code,

00:06:14.420 --> 00:06:17.260
there's basically a
face tracker executable.

00:06:17.340 --> 00:06:21.660
It includes or takes advantage
of the QuickTime framework,

00:06:21.730 --> 00:06:25.860
and it's using the OpenCV library
for object detection.

00:06:26.510 --> 00:06:29.180
Again,
the application itself is not threaded,

00:06:29.370 --> 00:06:29.780
per se.

00:06:29.780 --> 00:06:35.050
It's strictly a sequential
loop or a continuing loop,

00:06:35.130 --> 00:06:37.480
which basically does image acquisition.

00:06:37.480 --> 00:06:42.550
We actually call the face or object
detection where we pass a heuristic,

00:06:42.550 --> 00:06:47.540
which is looking for a particular
pattern to identify faces.

00:06:47.540 --> 00:06:49.330
And then we actually display it.

00:06:49.330 --> 00:06:50.750
We actually filter that.

00:06:50.760 --> 00:06:52.660
We identify where it's located.

00:06:53.480 --> 00:06:56.890
The return from the object detection
library will actually give us a

00:06:57.250 --> 00:06:59.250
rectangle or coordinates around the face.

00:06:59.260 --> 00:07:02.540
Returns that back to the
face detection routine.

00:07:02.540 --> 00:07:03.750
Returns that back to us.

00:07:03.780 --> 00:07:06.140
We put a little circle around it,
display it.

00:07:06.180 --> 00:07:09.680
There's actually a short delay if you
go download the actual source code.

00:07:09.680 --> 00:07:12.480
There's a short delay before
it actually loops around.

00:07:12.480 --> 00:07:18.150
There's no software in that tries to
do any kind of real-time performance,

00:07:18.220 --> 00:07:23.180
so it's not trying to schedule
these at 30 frames a second.

00:07:23.650 --> 00:07:29.300
But as you can see,
the CPU utilization was very low.

00:07:29.300 --> 00:07:33.250
And in fact, if you actually noticed,
it really wasn't all that

00:07:33.250 --> 00:07:35.440
great in terms of performance.

00:07:35.440 --> 00:07:38.080
There tended to be a little
bit of hesitation and drag on

00:07:38.080 --> 00:07:39.860
this particular application.

00:07:43.250 --> 00:07:49.010
Now, because Intel released OpenCV,
it was designed to actually work with

00:07:49.010 --> 00:07:51.260
the Intel Performance Primitives Library.

00:07:51.260 --> 00:07:54.890
And this library supports
a number of domains,

00:07:54.890 --> 00:08:00.590
cryptography, small matrices,
typed algorithms, image processing,

00:08:00.590 --> 00:08:02.420
signal processing.

00:08:02.420 --> 00:08:05.510
And the way this library
was actually designed,

00:08:05.640 --> 00:08:09.810
it was designed to work with
the Intel Performance Libraries.

00:08:09.820 --> 00:08:14.880
And that's why, you know,
the OpenCV library was actually released,

00:08:14.960 --> 00:08:18.980
was to actually show the benefit of
actually using the libraries instead

00:08:18.980 --> 00:08:22.310
of writing or rolling your own
version of these particular routines.

00:08:22.320 --> 00:08:24.720
So how's it actually implemented?

00:08:24.780 --> 00:08:29.280
Within the OpenCV library,
there's a master array,

00:08:29.280 --> 00:08:33.540
which is a function
pointers back to functions.

00:08:33.540 --> 00:08:37.310
And so the way it was actually
implemented is that all of the

00:08:37.310 --> 00:08:39.800
routines that are available or needed.

00:08:39.820 --> 00:08:43.060
To do the object detection are
actually within the CB library.

00:08:43.060 --> 00:08:46.130
So you can actually get
the source code to that.

00:08:46.310 --> 00:08:48.210
If it doesn't detect
the Intel performance

00:08:48.210 --> 00:08:52.200
libraries on your system,
which would be in the default location

00:08:52.200 --> 00:08:58.840
for frameworks within the Mac OS X or 10,
it'll actually just point all the

00:08:59.000 --> 00:09:03.660
pointers directly into the original
functions that were written.

00:09:03.660 --> 00:09:08.560
But if you have the libraries installed,
it'll actually go and load those modules,

00:09:08.560 --> 00:09:12.480
it'll do a search on the names of
the functions that are available,

00:09:12.910 --> 00:09:14.860
and it finds the ones
that it's interested in,

00:09:14.860 --> 00:09:18.050
and it actually just changes the
function pointer to point into

00:09:18.180 --> 00:09:20.070
the Intel performance libraries.

00:09:20.080 --> 00:09:22.310
Now, this is a little bit of a...

00:09:23.820 --> 00:09:31.700
and David Kahn, and David Kahn,
and David Kahn, and David Kahn,

00:09:31.700 --> 00:09:42.670
and David Kahn, and David Kahn,
and David Kahn, and David

00:09:44.160 --> 00:09:47.150
This is what it would actually
look like when you're all done,

00:09:47.280 --> 00:09:51.290
if you have the
Intel Performance Primitives installed,

00:09:51.290 --> 00:09:54.100
and we'll go ahead and access it.

00:09:54.100 --> 00:09:57.830
One of the things that we noticed when
you actually ran the routine is that

00:09:57.860 --> 00:10:00.090
it's actually using the EyeSight camera.

00:10:00.200 --> 00:10:03.430
And so one of the problems with doing
performance optimization is kind of

00:10:03.430 --> 00:10:06.450
having a workload that's repeatable
so that you know that when you make

00:10:06.450 --> 00:10:11.530
an improvement to the application
that you've actually improved it and

00:10:11.530 --> 00:10:14.740
that something in your data hasn't
actually changed that gives you

00:10:14.740 --> 00:10:16.690
false positives or false negatives.

00:10:16.700 --> 00:10:20.510
So one of the things that we do is
we actually replace the code that

00:10:20.510 --> 00:10:25.100
used the iSight camera with actually
pulling in a QuickTime video file.

00:10:25.630 --> 00:10:28.170
And it's actually very easy
using the QuickTime framework,

00:10:28.170 --> 00:10:31.240
so it was just a couple lines
of code different in order

00:10:31.240 --> 00:10:33.280
to be able to implement that.

00:10:35.160 --> 00:10:37.970
The other thing we wanted to do is
that the code doesn't really tell you

00:10:38.050 --> 00:10:39.740
exactly how fast anything is running.

00:10:39.890 --> 00:10:44.310
It's got this timing loop that's in there
that basically just has a fixed delay.

00:10:44.480 --> 00:10:49.640
And so what we wanted to do was actually
have a way that we can actually measure

00:10:49.640 --> 00:10:52.940
the performance improvement from
the optimizations that are being

00:10:52.940 --> 00:10:55.160
actually made to the application.

00:10:55.160 --> 00:10:59.800
And so we added a little bit of
timing code at the very bottom of it.

00:11:01.200 --> 00:11:02.740
And we wanted it to run full throttle.

00:11:02.740 --> 00:11:05.200
So right now,
there's like a 10 millisecond

00:11:05.200 --> 00:11:06.960
delay at the end of the routine.

00:11:06.960 --> 00:11:11.290
We really wanted to look at this from
a full throttle performance standpoint

00:11:11.290 --> 00:11:13.640
to maximize the actual performance.

00:11:13.640 --> 00:11:19.080
And then, when we're done,
we can go back and implement a real-time

00:11:19.080 --> 00:11:25.570
synchronization routine to make it so
that it runs at 30 frames a second.

00:11:26.810 --> 00:11:31.960
So this is kind of the changes that
we made from a very high level.

00:11:32.030 --> 00:11:36.420
Again, we just changed the image
acquisition to a file decode.

00:11:36.420 --> 00:11:39.510
We eliminated the delay and we
added some timing at the end.

00:11:39.510 --> 00:11:40.760
And so we're using the QuickTime.

00:11:40.760 --> 00:11:44.520
So we have a little funny video that
Palavi is going to show us in the demo.

00:11:44.520 --> 00:11:48.710
And we'll see where that takes us.

00:12:01.860 --> 00:12:07.840
Okay, so Phil talked about having a way
to measure repeatable performance,

00:12:07.980 --> 00:12:11.340
having some kind of metric to
see how fast we are running,

00:12:11.350 --> 00:12:15.180
and thirdly,
eliminate any delays that are there.

00:12:15.300 --> 00:12:18.410
So I'm going to show you the
version which I showed you before,

00:12:18.410 --> 00:12:20.080
but it's a little different.

00:12:20.080 --> 00:12:22.900
It now takes input from a
video stream instead of taking

00:12:22.900 --> 00:12:26.330
from the EyeSight camera,
and so that we can establish a baseline

00:12:26.330 --> 00:12:29.060
version of how fast we are running.

00:12:38.500 --> 00:12:41.300
We have this funny video running.

00:12:41.300 --> 00:12:44.890
It's tracking the phase.

00:12:45.230 --> 00:12:47.540
It's going to complete pretty soon.

00:12:47.560 --> 00:12:49.790
Look at the CPU utilization on the right.

00:12:49.790 --> 00:12:54.320
It's again about close to just 100%.
We are not fully utilizing the majority

00:12:54.320 --> 00:12:56.660
of the eight cores that we have on hand.

00:12:56.660 --> 00:13:00.200
And it's going to be done soon.

00:13:01.850 --> 00:13:05.500
Okay, so there were about 309
frames in the stream,

00:13:05.500 --> 00:13:10.200
and it took about just 24
seconds to complete the video.

00:13:10.620 --> 00:13:12.000
I'm sure we can do better.

00:13:12.040 --> 00:13:15.020
Phil talked about using
performance libraries and such,

00:13:15.130 --> 00:13:18.090
but before we do that,
why don't we see how does

00:13:18.090 --> 00:13:20.420
a shark profile look like?

00:13:29.000 --> 00:13:37.600
a load very soon.

00:13:37.600 --> 00:13:41.490
So looking at a shark profile,
the very first hot spot that we have

00:13:41.850 --> 00:13:46.390
is coming from the OpenCV libraries
that we are utilizing here,

00:13:46.390 --> 00:13:49.280
and I'm sure we can
do something about it.

00:13:49.280 --> 00:13:54.130
Phil talked about that OpenCV supports
Intel performance libraries,

00:13:54.180 --> 00:13:58.020
so the next thing I'm going to show
you a version where we actually

00:13:58.020 --> 00:14:01.950
are using the Intel performance
libraries and see what kind of

00:14:01.950 --> 00:14:05.460
performance gains we can get with that.

00:14:05.460 --> 00:14:10.520
So keep that in mind that we ran
309 frames in about 24 seconds.

00:14:10.520 --> 00:14:15.450
We had all about 100% CPU utilization
using probably just about

00:14:15.450 --> 00:14:20.620
one core out of the eight,
and we are spending about 76% of

00:14:20.620 --> 00:14:23.740
the time in the OpenCV libraries.

00:14:23.740 --> 00:14:25.620
So I'm going to quit out of here.

00:14:35.860 --> 00:14:41.810
So this is the IPP version where now
the OpenCV calls are being replaced

00:14:41.890 --> 00:14:44.030
by the Intel Performance Libraries.

00:14:44.130 --> 00:14:45.880
Look at the CPU utilization on the right.

00:14:45.880 --> 00:14:49.880
We are all about using close to all
the eight cores that we have on hand.

00:14:49.900 --> 00:14:51.200
And let's see how much time it takes.

00:14:51.350 --> 00:14:55.220
Oh, it took only about 15 seconds
to complete the same workload.

00:14:55.250 --> 00:14:59.220
Oh, we are about 50% faster,
about 1.5x speedup.

00:14:59.250 --> 00:15:02.350
Let's see how does the profile look like.

00:15:10.150 --> 00:15:13.650
As you can see,
we don't see that 76% time being

00:15:13.650 --> 00:15:15.940
spent in the OpenCV libraries.

00:15:16.490 --> 00:15:22.890
Instead,
we are calling some of the IPP libraries,

00:15:22.890 --> 00:15:22.890
if I expand this.

00:15:25.200 --> 00:15:28.200
Plus, you notice we see something else.

00:15:28.200 --> 00:15:31.730
We see some of the KMP calls here,
which are coming from

00:15:31.730 --> 00:15:36.370
something called OpenMP,
since Intel Performance Libraries uses

00:15:36.570 --> 00:15:38.550
OpenMP to implement the threading.

00:15:38.560 --> 00:15:41.980
And that's it for this demo,
and Phil is going to come back

00:15:41.980 --> 00:15:45.350
and talk more about OpenMP,
and then we're going to have

00:15:45.350 --> 00:15:47.400
some more demos following it.

00:15:50.680 --> 00:15:54.840
So I think it's great that the
Intel Performance Libraries produced

00:15:54.850 --> 00:15:58.780
about a 50% improvement in performance.

00:15:58.820 --> 00:16:02.000
But I was really shocked when
I saw that we had an eight-core

00:16:02.130 --> 00:16:05.950
machine and we're running almost
100% across all eight cores.

00:16:06.070 --> 00:16:13.220
So we got from 15 seconds to about 24,
25 seconds, but it took us eight times

00:16:13.340 --> 00:16:15.680
the CPU processing power.

00:16:16.600 --> 00:16:20.100
So, the reality is that the
Intel Performance Libraries

00:16:20.100 --> 00:16:21.380
also are using OpenMP.

00:16:21.490 --> 00:16:27.060
And we'll talk a little bit about
what those additional functions

00:16:27.060 --> 00:16:28.320
were actually doing and why.

00:16:28.320 --> 00:16:32.040
But OpenMP is a specification
which was targeted at symmetric

00:16:32.040 --> 00:16:35.210
multi-threaded processing systems.

00:16:35.260 --> 00:16:38.120
So,
it's really targeted for these types of

00:16:38.120 --> 00:16:41.140
processors where you have shared memory.

00:16:41.920 --> 00:16:46.700
And it really is focused on encouraging
you to write threaded code incrementally.

00:16:46.700 --> 00:16:51.050
So, one of the things that James Reinders
talked about was the fact

00:16:51.050 --> 00:16:54.120
that whenever you write code,
that in addition to having

00:16:54.280 --> 00:16:57.120
parallel code or threaded code,
you would want to be able to go

00:16:57.120 --> 00:16:58.680
back to the single-threaded code.

00:16:58.680 --> 00:17:01.260
Well,
OpenMP actually supports that paradigm.

00:17:01.260 --> 00:17:07.040
You write it initially in
a straight serial fashion.

00:17:07.040 --> 00:17:10.390
And then you describe to it exactly
how you want to do the threading.

00:17:10.500 --> 00:17:11.840
So, you can actually go to the OpenMP.

00:17:11.920 --> 00:17:15.910
And look up OpenMP and
the specifics on it.

00:17:15.980 --> 00:17:22.430
But the interesting thing is that both
Apple's GCC 4.2 compiler and Intel's

00:17:22.430 --> 00:17:25.150
C and Fortran compiler support OpenMP.

00:17:25.240 --> 00:17:27.080
So, there's a lot of advantage.

00:17:27.120 --> 00:17:31.700
And the Intel compilers are
supported across the Mac OS as

00:17:31.700 --> 00:17:34.080
well as Linux and Windows.

00:17:36.900 --> 00:17:37.790
So, what is OpenMP?

00:17:37.880 --> 00:17:40.030
It's really a set of
programming directives.

00:17:40.030 --> 00:17:43.440
So you describe how you
want your single-threaded

00:17:43.440 --> 00:17:45.220
implementation to run in parallel.

00:17:45.220 --> 00:17:50.930
There are some support routines that
allow you to get additional information,

00:17:51.010 --> 00:17:56.300
so you can query things like how many
CPUs are actually in your system.

00:17:56.300 --> 00:17:59.010
You can set how many CPUs you
would like to be in your system,

00:17:59.010 --> 00:18:02.690
how many threads you actually want to
use if you don't want to use the default,

00:18:02.690 --> 00:18:04.930
which would be the number
of cores that are available.

00:18:05.670 --> 00:18:09.470
There are also environment variables,
so when your application starts up,

00:18:09.470 --> 00:18:11.830
it will look at the
environment variables.

00:18:11.890 --> 00:18:14.270
Again,
you can control the number of threads,

00:18:14.280 --> 00:18:17.180
you can control the scheduling
paradigm that is used,

00:18:17.230 --> 00:18:20.380
and the specifications
allow vendor extensions.

00:18:22.800 --> 00:18:26.780
So let's take a real quick look
at what the code looks like in

00:18:26.780 --> 00:18:29.560
OpenCV for the object detection.

00:18:29.560 --> 00:18:32.310
It's basically a simple for loop.

00:18:32.560 --> 00:18:39.880
It breaks up the image into a number of
strips across the image and basically

00:18:39.880 --> 00:18:44.730
applies a heuristic for object
detection across each of those strips.

00:18:44.820 --> 00:18:49.830
If you had a single core,
the number of strips would be one.

00:18:49.840 --> 00:18:51.790
You would just actually
do the full image.

00:18:52.170 --> 00:18:54.540
But if you had multiple cores,
you could actually break up the

00:18:54.620 --> 00:18:57.760
strips and have one per core
or even more than one per core,

00:18:57.760 --> 00:19:00.570
depending on how you wanted
to implement your code.

00:19:00.580 --> 00:19:05.180
So a simple way to do this in OpenMP is
you could just add the pragma,

00:19:05.420 --> 00:19:07.020
OpenMP parallel 4.

00:19:07.020 --> 00:19:11.650
And this code would actually then,
if you compiled it with a compiler

00:19:11.650 --> 00:19:17.280
that supported the OpenMP pragmas,
this code would actually become threaded.

00:19:17.280 --> 00:19:19.840
In fact, that's what you get when
you look at the code.

00:19:19.840 --> 00:19:22.020
So we went from
single-threaded performance.

00:19:22.060 --> 00:19:28.100
To eight threads actually
running 100% on the CPU.

00:19:28.420 --> 00:19:31.470
Now,
you can extend these pragmas they support

00:19:31.650 --> 00:19:34.600
so that you can specify the number of
threads that are actually supported.

00:19:34.600 --> 00:19:38.150
There are some cases where you
don't necessarily want your

00:19:38.310 --> 00:19:43.920
application to necessarily use
all of the cores on your system,

00:19:43.920 --> 00:19:47.440
or there are times when you actually
want to have more threads running than

00:19:47.440 --> 00:19:49.330
you have physical cores available.

00:19:49.340 --> 00:19:53.110
So if you have a thread that's going
to go out and do some synchronization

00:19:53.460 --> 00:19:55.760
across the network and there's
going to be a fair amount of delay,

00:19:56.250 --> 00:19:58.300
you might actually run
100 threads of these,

00:19:58.300 --> 00:20:00.550
and you can actually do that
with OpenMP if you like.

00:20:00.560 --> 00:20:03.750
The default, if you don't specify this,
is it's going to launch the same

00:20:03.750 --> 00:20:07.160
number of threads as you have
cores available on your platform.

00:20:09.730 --> 00:20:14.840
You can go further in terms of
scheduling how you want your threads to

00:20:14.840 --> 00:20:20.900
be able to interact with the workload
in terms of dividing up the workload.

00:20:20.900 --> 00:20:27.360
I'm going to talk a little bit more about
how you can influence the scheduling

00:20:27.360 --> 00:20:30.390
and why you would want to do that.

00:20:32.020 --> 00:20:36.770
So, if you didn't define anything
at all in that pragma,

00:20:36.780 --> 00:20:38.990
and you just left it blank,
and you had the first statement,

00:20:39.000 --> 00:20:44.020
which was pragma OpenMP parallel 4,
you would basically

00:20:44.100 --> 00:20:45.780
get static scheduling.

00:20:46.530 --> 00:20:49.840
Now, depending on how many strips
you actually had in your image,

00:20:49.960 --> 00:20:52.740
how you wanted to define that,
you would essentially,

00:20:52.740 --> 00:20:56.440
this is what you would have,
you would have,

00:20:57.330 --> 00:21:02.770
Each of these rows between the serial
code would be operation on a frame,

00:21:03.010 --> 00:21:04.590
so it's frame-based.

00:21:04.680 --> 00:21:08.440
And you would divide up the number of
stripes that you're actually going to

00:21:08.440 --> 00:21:11.440
operate on and assign those to threads.

00:21:11.440 --> 00:21:12.780
So that's the static scheduling.

00:21:12.780 --> 00:21:17.610
It says if I have 16 stripes,

00:21:17.860 --> 00:21:19.360
I only have four threads.

00:21:19.360 --> 00:21:21.290
I'm just going to divide each of them up.

00:21:21.320 --> 00:21:23.760
The first thread is
going to get 0 through 3,

00:21:23.760 --> 00:21:26.290
and the next one 4 through
whatever and continue.

00:21:26.630 --> 00:21:28.000
And that would be static scheduling.

00:21:28.000 --> 00:21:30.420
So it's already fixed by the framework.

00:21:30.530 --> 00:21:34.080
The OpenMP framework has already
decided exactly how the threads

00:21:34.080 --> 00:21:38.980
are going to operate on those
particular strips of the image.

00:21:39.330 --> 00:21:42.930
And that's great, because there's really
no synchronization.

00:21:43.060 --> 00:21:44.870
The synchronization
only happens at the end,

00:21:44.880 --> 00:21:46.540
when all of the threads are done.

00:21:46.600 --> 00:21:49.620
There's no coordination between
any of the threads on which stripe

00:21:49.620 --> 00:21:50.750
they're actually gonna work on.

00:21:50.920 --> 00:21:54.040
It's predetermined
before they even start.

00:21:54.410 --> 00:21:56.780
The problem is,
what happens if that workload

00:21:56.850 --> 00:21:58.460
is not exactly balanced?

00:21:58.460 --> 00:22:02.690
So, in this particular case,
maybe a slight exaggeration,

00:22:02.690 --> 00:22:06.350
but suppose that the image was
such that you had a very white

00:22:06.350 --> 00:22:10.120
background that was very uniform
at the very top of the image,

00:22:10.120 --> 00:22:14.540
but a lot more detail and a group
shot of people actually in an image.

00:22:14.540 --> 00:22:17.750
There would be a lot more processing that
would need to take place on the lower

00:22:17.750 --> 00:22:19.520
part of the image to detect the face.

00:22:19.960 --> 00:22:22.960
So, you could actually have
this broken up such that,

00:22:22.960 --> 00:22:25.940
for a given image,
one thread takes a lot longer than

00:22:25.940 --> 00:22:28.440
the rest of the image actually takes.

00:22:28.440 --> 00:22:33.170
And so, you would not want a static
scheduling in this case.

00:22:33.200 --> 00:22:35.260
You would want more of a dynamic.

00:22:35.260 --> 00:22:39.310
Now, you can actually change it a little
bit and still have static scheduling

00:22:39.580 --> 00:22:41.600
by basically doing round robin.

00:22:41.640 --> 00:22:43.800
So, you can change the chunk size.

00:22:43.800 --> 00:22:47.140
So, I told you that if we had 16
stripes in the image and we

00:22:47.150 --> 00:22:51.050
broke that up by four threads,
the first thread would

00:22:51.050 --> 00:22:52.400
get zero through four.

00:22:52.400 --> 00:22:54.590
Well,
maybe those are the first four stripes

00:22:54.600 --> 00:22:58.400
on the bottom that have a lot of detail,
and the top part of the image,

00:22:58.400 --> 00:23:01.320
which is all white,
would be the last four.

00:23:01.320 --> 00:23:05.260
Well, they would still go really fast,
but if you round robined that such

00:23:05.690 --> 00:23:11.600
that the first thread got zero and
the second one got stripe number one,

00:23:11.600 --> 00:23:14.690
then you kind of even out
the workload a little bit.

00:23:14.720 --> 00:23:19.810
The problem is that you can still
end up with an imbalanced workload,

00:23:19.840 --> 00:23:21.820
but not nearly as severe.

00:23:21.820 --> 00:23:23.820
But again, we can improve on that.

00:23:23.820 --> 00:23:26.820
So, what we can do is we can
actually say dynamic.

00:23:26.820 --> 00:23:31.050
It says that each thread
works on one stripe,

00:23:31.050 --> 00:23:35.800
and only when it's complete does it
go back and actually get the next one.

00:23:35.880 --> 00:23:40.240
Now, the problem is with this is that you
have a little bit more overhead because

00:23:40.240 --> 00:23:43.420
now every time I finish a thread,
I have to do some synchronization

00:23:43.440 --> 00:23:46.770
with the framework to say,
okay, what is the next stripe

00:23:46.770 --> 00:23:47.820
that I have to work on?

00:23:47.820 --> 00:23:49.800
Is it, you know, is it one or is it two?

00:23:49.800 --> 00:23:51.800
Is it number 16?

00:23:51.800 --> 00:23:53.800
But you get a more balanced view.

00:23:53.950 --> 00:23:58.300
So, as long as the workload per
stripe is fairly large versus

00:23:58.300 --> 00:24:02.640
the synchronization of overhead,
then dynamic actually works pretty well.

00:24:02.810 --> 00:24:05.920
And that's actually what's
actually been implemented in

00:24:06.070 --> 00:24:07.800
the OpenCV framework library.

00:24:10.710 --> 00:24:15.240
So, I told you that
Intel Performance Libraries

00:24:15.240 --> 00:24:18.940
support OpenMP as well,
because we saw that

00:24:18.940 --> 00:24:21.290
in the Shark profiles.

00:24:21.300 --> 00:24:25.040
So, the problem that we have is that
we have our main application,

00:24:25.040 --> 00:24:26.670
which has no threading.

00:24:26.680 --> 00:24:31.240
We're using the QuickTime framework,
which has its own implementation,

00:24:31.240 --> 00:24:34.360
whatever that may be,
that Apple has provided.

00:24:34.360 --> 00:24:38.610
Then we have the Object Detection
OpenCV Library framework,

00:24:38.610 --> 00:24:40.310
which is using OpenMP.

00:24:40.600 --> 00:24:40.660
Thank you.

00:24:42.200 --> 00:24:44.860
So every time you crawl that library,
it's going to basically,

00:24:44.860 --> 00:24:48.920
if you don't give it any parameters,
it's going to invoke as many threads

00:24:49.330 --> 00:24:51.420
as there are cores on the system.

00:24:51.420 --> 00:24:54.000
But for every one of the
libraries that we load in the

00:24:54.000 --> 00:24:58.200
Intel Performance Libraries,
it's also, those are independent modules.

00:24:58.200 --> 00:25:02.310
When they get initialized,
they are also using OpenMP,

00:25:02.400 --> 00:25:06.550
and so they're also creating as many
threads as are in the libraries.

00:25:10.360 --> 00:25:12.680
When we actually looked
at the Shark profile,

00:25:12.680 --> 00:25:18.740
we saw that KMP fork was like 20%
or 25% of the actual workload.

00:25:18.740 --> 00:25:24.270
Turns out that one of the things that
OpenMP does is it likes to not have

00:25:24.500 --> 00:25:31.260
threads be context switched out in case
its synchronization objects are available

00:25:31.260 --> 00:25:34.560
immediately or soon after it needs it.

00:25:34.620 --> 00:25:38.040
So we actually enter spin weights,
and that's what you're actually

00:25:38.040 --> 00:25:39.360
seeing is spin weights.

00:25:39.420 --> 00:25:42.600
But the problem with spin weights is that
as long as you're holding on to the CPU,

00:25:42.600 --> 00:25:45.390
nothing else can actually get scheduled.

00:25:46.100 --> 00:25:50.460
So one of the things is,
Intel has a vendor extension

00:25:51.260 --> 00:25:53.550
environment variable,
which allows us to change

00:25:53.550 --> 00:25:54.980
the spin weight block time.

00:25:54.980 --> 00:25:57.720
And so in this case,
we wanted to see what the real

00:25:57.720 --> 00:26:02.060
performance improvement was,
or what the real CPU utilization was,

00:26:02.090 --> 00:26:03.600
and we can actually set it to zero.

00:26:03.600 --> 00:26:07.510
And we'll eliminate the
OpenMP spin weights.

00:26:08.060 --> 00:26:13.370
So we'll go ahead and switch to
the demo again and show you what

00:26:13.370 --> 00:26:17.970
the performance looks like with
OpenMP and setting it to zero.

00:26:21.420 --> 00:26:25.570
Okay, so in the previous demo,
we utilized the Intel's

00:26:25.570 --> 00:26:29.490
performance libraries,
which also uses OpenMP,

00:26:29.620 --> 00:26:31.960
and we showed you the Shark profile.

00:26:31.960 --> 00:26:34.920
Now, in this demo,
what I'm going to do is,

00:26:35.080 --> 00:26:39.140
as Phil mentioned,
the OpenCV libraries also use OpenMP.

00:26:39.140 --> 00:26:43.540
So we're going to run a version with
that and also show you the Shark profile,

00:26:43.540 --> 00:26:48.580
and we're going to see same kind
of calls showing up in the profile.

00:26:48.580 --> 00:26:52.740
And then I'll follow it with another
version of the demo where we'll

00:26:52.740 --> 00:26:56.580
set the KMP block time to zero
and see how it changes our profile

00:26:56.580 --> 00:26:58.560
as well as our CPU utilization.

00:26:58.560 --> 00:27:01.720
So let's run the first version.

00:27:01.720 --> 00:27:05.860
This is where the OpenCV is
running with OpenMP enabled.

00:27:15.650 --> 00:27:17.600
So keep an eye on the CPU utilization.

00:27:17.600 --> 00:27:21.540
It's the same funny video,
the bobblehead video.

00:27:21.570 --> 00:27:25.380
Look, we are utilizing almost
close to all the cores and,

00:27:25.380 --> 00:27:28.420
yeah, we are tracking the face and etc.

00:27:28.510 --> 00:27:30.540
But I'm sure we can do better.

00:27:30.640 --> 00:27:33.250
And in order to do better,
let's first eliminate all the

00:27:33.360 --> 00:27:36.340
KMP wait calls that we were seeing.

00:27:36.600 --> 00:27:39.080
But before that,
let's see how the profile

00:27:39.080 --> 00:27:40.680
looks for this version.

00:27:44.660 --> 00:27:49.250
You see the CV calls are there,
which we had seen in the original demo,

00:27:49.250 --> 00:27:51.090
but we also see the KMP calls.

00:27:51.160 --> 00:27:53.760
And all these are actually
coming from the OpenMP weights,

00:27:53.810 --> 00:27:55.100
which Phil just talked about.

00:27:55.100 --> 00:27:58.870
So, now let's set next the
KMP block time to zero,

00:27:58.970 --> 00:28:02.580
like Phil mentioned,
and see how it changes.

00:28:02.580 --> 00:28:07.890
So I'm going to keep
this on the background.

00:28:09.340 --> 00:28:12.860
And also notice we are
running about 10 milliseconds,

00:28:12.860 --> 00:28:13.370
which is good.

00:28:13.440 --> 00:28:16.240
I mean,
we started with about 24 milliseconds.

00:28:16.270 --> 00:28:19.210
And we now, instead of using Intel's
performance libraries,

00:28:19.220 --> 00:28:24.110
we are utilizing the
OpenMP feature on the OpenCV.

00:28:24.330 --> 00:28:26.380
So we did good.

00:28:36.700 --> 00:28:41.020
So now we are,
what this version is where the

00:28:41.020 --> 00:28:43.580
KMP block time has been set to zero.

00:28:43.580 --> 00:28:47.200
And it's the same thing
which I ran just previously.

00:28:47.200 --> 00:28:48.480
Look at the CPU utilization.

00:28:48.550 --> 00:28:51.800
It's only about like,
I'll run it one more time,

00:28:51.800 --> 00:28:54.050
about like just 400 or 500 percent.

00:28:54.070 --> 00:28:59.150
Not the 700 or close to 800 percent
which we were seeing before.

00:28:59.680 --> 00:29:02.610
You can see a lot of the random circles,
and I'm going to talk about

00:29:02.610 --> 00:29:06.470
that and how we actually utilize
the multi-core feature of the

00:29:06.470 --> 00:29:14.540
Intel platform and solve that with
other threading implementations.

00:29:14.540 --> 00:29:18.440
Before I turn over to Phil,
let me show you the

00:29:18.750 --> 00:29:21.160
profile for this version.

00:29:23.860 --> 00:29:27.820
So as you can see,
if you compare the previous profile,

00:29:27.820 --> 00:29:33.000
all the KMP, fork, barrier, KMP pause,
KMP yield, all those calls are actually

00:29:33.010 --> 00:29:37.300
gone from the top four,
five, or 10, or 15 hotspots.

00:29:37.320 --> 00:29:41.500
And that's due to the KMP block
time setting equal to zero.

00:29:41.520 --> 00:29:44.800
So like I said,
as far as the random circles we see,

00:29:44.800 --> 00:29:46.560
I'm going to talk about it later.

00:29:46.560 --> 00:29:47.900
Back to slides and Phil.

00:29:47.900 --> 00:29:49.390
Thanks.

00:29:55.490 --> 00:29:57.730
So one of the things that
we saw in that demo was,

00:29:57.750 --> 00:30:01.240
first of all,
when we set the spin weights to zero,

00:30:01.240 --> 00:30:05.020
I don't know if you noticed,
we actually went from like 10.7

00:30:05.020 --> 00:30:07.220
seconds to over 11 seconds.

00:30:07.220 --> 00:30:09.700
So there is benefit
from those spin weights,

00:30:09.700 --> 00:30:12.110
right, in terms of actual performance.

00:30:12.170 --> 00:30:15.220
But the problem with seeing those
spin weights when you're trying

00:30:15.220 --> 00:30:18.590
to do performance analysis is the
fact that it hides the potential.

00:30:18.600 --> 00:30:20.900
You don't know exactly where
you're spending your time.

00:30:21.330 --> 00:30:25.860
If you just look at the CPU utilization,
you would have been up at 800%.

00:30:25.860 --> 00:30:29.540
When you actually look at the final
result with the KMP turned off,

00:30:29.580 --> 00:30:30.780
it was much lower.

00:30:30.780 --> 00:30:36.190
It was down probably around,
I don't know, 300% or 400%. So...

00:30:38.340 --> 00:30:40.000
The fact that we have,
that we got rid of all

00:30:40.000 --> 00:30:42.400
that extra spin weight,
we can clearly see that there's a

00:30:42.400 --> 00:30:47.540
lot more performance power that's
available to take advantage of and

00:30:47.540 --> 00:30:48.740
actually improve their performance.

00:30:48.740 --> 00:30:52.730
The video obviously goes a lot faster,
but we also are introducing a

00:30:52.730 --> 00:30:56.070
little bit of false positives,
and we'll talk about that

00:30:56.150 --> 00:30:57.540
a little bit further.

00:30:57.540 --> 00:31:01.680
But what we decided to do was actually
implement a pipeline approach.

00:31:02.290 --> 00:31:05.100
This is one of the things that
James talked about earlier,

00:31:05.100 --> 00:31:09.540
was changing the kind of the paradigm
on how you look at the actual problem.

00:31:09.540 --> 00:31:13.760
The original code was very serial,
and we relied on the library to

00:31:13.760 --> 00:31:16.170
provide us with the parallelism.

00:31:16.200 --> 00:31:20.970
But the reality is that if you
really look at this serial code,

00:31:21.090 --> 00:31:24.280
you can break it up
into a file decode step,

00:31:24.320 --> 00:31:28.570
or task, as James called it,
the face detection.

00:31:28.580 --> 00:31:30.880
So once you actually have the image,
you can do the face

00:31:30.960 --> 00:31:32.200
detection on the image.

00:31:32.240 --> 00:31:35.340
You don't have to worry about
file decode at that point.

00:31:35.340 --> 00:31:37.310
And then the display task.

00:31:37.340 --> 00:31:40.940
So that's why we decided to come
up with the pipeline approach,

00:31:41.060 --> 00:31:44.260
where each of those steps
are actually separate tasks,

00:31:44.260 --> 00:31:47.530
and they're all operating
or delivering a frame.

00:31:49.720 --> 00:31:53.130
And that's instead of actually
dividing up the frame into stripes.

00:31:53.210 --> 00:31:56.130
So the other thing that we
decided to do was instead of

00:31:56.130 --> 00:31:59.410
operating on strips of the image,
so that we can get rid

00:31:59.410 --> 00:32:03.430
of a lot of the overhead,
was to actually then say, hey,

00:32:03.690 --> 00:32:06.220
once we have the image
from the file decode,

00:32:06.220 --> 00:32:10.210
then we can start the face
detection immediately.

00:32:10.220 --> 00:32:13.850
And these blocks are fairly
representative of the time that it

00:32:13.850 --> 00:32:15.470
actually takes to do each of these steps.

00:32:15.540 --> 00:32:18.680
So if you looked at
file decode and display,

00:32:19.740 --> 00:32:23.010
that combined is about a
third of the total processing

00:32:23.010 --> 00:32:27.330
time for the file decode,
face detection, and display time.

00:32:28.880 --> 00:32:32.730
If we can actually eliminate the face
detection or get that reduced down,

00:32:32.730 --> 00:32:39.610
the file detection or decode and the
display are basically serial operations.

00:32:39.620 --> 00:32:42.910
There's not much that we can do
from our application standpoint.

00:32:42.920 --> 00:32:47.000
Now, QuickTime may actually be threaded
and provide some additional benefit.

00:32:47.000 --> 00:32:49.280
But from our perspective,
out of the QuickTime,

00:32:49.280 --> 00:32:51.360
we're basically getting
a frame at a time.

00:32:51.360 --> 00:32:53.520
We can't get a second frame
until we get the first frame.

00:32:53.520 --> 00:32:56.950
We can't display the second frame
until we've displayed the first frame.

00:32:57.350 --> 00:32:58.770
So those are serial operations.

00:32:58.780 --> 00:33:02.920
But clearly, the face detection itself
can be a parallel operation.

00:33:02.920 --> 00:33:05.900
And the fact,
instead of doing this by stripes,

00:33:06.010 --> 00:33:09.300
in other words, making the workload
smaller for each thread,

00:33:09.300 --> 00:33:14.090
we're basically giving each thread the
full set of workload on an image and

00:33:14.330 --> 00:33:15.920
allowing that to be very localized.

00:33:15.920 --> 00:33:17.990
And there's a lot of
benefits to that as well.

00:33:18.000 --> 00:33:21.380
Now,
each frame that's having face detection,

00:33:21.400 --> 00:33:26.240
all of that data that's associated with
that thread can run on that same CPU,

00:33:26.890 --> 00:33:30.010
can take advantage of all the
cache that's available for that.

00:33:30.020 --> 00:33:34.500
And there's not a whole lot of data
migration from one core to another

00:33:34.580 --> 00:33:36.740
or from one cache to another.

00:33:39.660 --> 00:33:41.430
So, how did we actually implement this?

00:33:41.520 --> 00:33:43.920
We used the Intel Threading
Building Blocks.

00:33:43.920 --> 00:33:49.710
It really targets more complex threading
models than what you can get with,

00:33:49.860 --> 00:33:52.700
certainly with P-threads,
which really doesn't provide you

00:33:52.700 --> 00:33:56.200
much support other than the ability
to implement your own model.

00:33:56.700 --> 00:34:01.280
But certainly over OpenMP,
which was much more data

00:34:01.420 --> 00:34:05.840
decomposition-oriented
with the parallel four,

00:34:05.840 --> 00:34:08.500
does allow for some
functional decomposition,

00:34:08.500 --> 00:34:11.830
but it doesn't give you the
higher-level threading paradigm

00:34:11.830 --> 00:34:14.140
that you would really like to do.

00:34:14.140 --> 00:34:17.020
And in this case,
we actually used the linear pipelining

00:34:17.020 --> 00:34:19.090
with filter stages to implement that.

00:34:20.750 --> 00:34:22.620
You can also do things
like parallel scan,

00:34:22.750 --> 00:34:24.220
parallel sort.

00:34:24.280 --> 00:34:28.300
You can do the parallel for,
which is what's available on OpenMP,

00:34:28.330 --> 00:34:33.700
as well as reduction and
parallel while implementations.

00:34:33.720 --> 00:34:38.160
Now, the Threading Building Blocks
are optimized libraries for

00:34:38.160 --> 00:34:42.910
the Intel-based platforms,
again, that are available on the Mac,

00:34:42.910 --> 00:34:44.790
Linux, and Windows.

00:34:45.190 --> 00:34:47.840
They're not limited to the
Intel compiler because it's

00:34:47.840 --> 00:34:49.860
actually available in source.

00:34:49.920 --> 00:34:53.900
They are C++, so if you're using Fortran,
you don't have TBB available

00:34:53.900 --> 00:34:56.700
because we're actually using
templates to implement it.

00:34:56.760 --> 00:35:00.300
But it is cross-platform,
and the best benefit is the fact that,

00:35:00.350 --> 00:35:04.090
unlike last year,
TBB is now open-sourced.

00:35:04.170 --> 00:35:07.190
So you can actually download it,
compile it yourself, modify it,

00:35:07.190 --> 00:35:14.280
implement it, and use it as you see fit
under the GPL license.

00:35:15.240 --> 00:35:19.280
So, we saw those extra rings
that were showing up.

00:35:19.370 --> 00:35:22.080
And so the idea was that, hey,
we're only using, you know,

00:35:22.080 --> 00:35:25.320
three or four cores out
of the eight-core system.

00:35:25.320 --> 00:35:28.310
Why not add more value to the end user?

00:35:28.320 --> 00:35:30.890
And the end user, you know,
is going to see those

00:35:30.890 --> 00:35:32.700
additional false positives.

00:35:32.700 --> 00:35:36.460
So what we decided to do was,
let's take advantage of that and

00:35:36.460 --> 00:35:39.300
add a frame detection history.

00:35:40.130 --> 00:35:42.260
So, in other words,
as we get these frames

00:35:42.260 --> 00:35:45.120
or these images in,
and we've detected these regions

00:35:45.120 --> 00:35:50.000
that we think they are faces,
we'll actually keep a history of that.

00:35:50.000 --> 00:35:54.870
And if we see from frame to frame that
we have a fairly consistent view that

00:35:55.350 --> 00:36:00.360
that image or that face has shown up
multiple times through that history,

00:36:00.360 --> 00:36:01.800
then we keep the circle.

00:36:01.800 --> 00:36:04.780
But if it's a flash,
if it only shows up one time

00:36:04.780 --> 00:36:08.770
or in just a couple of frames
and isn't there consistently,

00:36:08.780 --> 00:36:10.080
then we actually add a
frame detection history.

00:36:10.100 --> 00:36:12.740
And so we can actually remove that.

00:36:12.740 --> 00:36:16.060
And so we actually implemented, again,
some more stages,

00:36:16.060 --> 00:36:18.020
which could be done in parallel.

00:36:18.020 --> 00:36:20.480
So as soon as we went to
the frame detection history,

00:36:20.620 --> 00:36:22.450
we determined what that history was.

00:36:22.480 --> 00:36:25.070
Then on each frame,
we can decide whether we actually want to

00:36:25.140 --> 00:36:27.040
keep the circle or not keep the circle.

00:36:27.040 --> 00:36:29.970
And then display it.

00:36:30.850 --> 00:36:32.100
But we can actually go further.

00:36:32.100 --> 00:36:36.460
If you actually look at what
Palavi showed before when she

00:36:36.460 --> 00:36:39.490
was using the video camera,
was that when she turned sideways,

00:36:39.490 --> 00:36:40.800
the circle went away.

00:36:40.820 --> 00:36:44.500
So what we were really doing
is only using the heuristic

00:36:44.500 --> 00:36:46.940
for frontal facial recognition.

00:36:46.940 --> 00:36:50.280
So only if you were basically
looking straight ahead would

00:36:50.370 --> 00:36:52.150
we actually detect the face.

00:36:52.180 --> 00:36:55.150
But there's also a
heuristic in the OpenCV,

00:36:55.160 --> 00:36:58.750
which allows you to have
profile face detection.

00:36:58.960 --> 00:37:02.110
So if you turn sideways,
it'll actually show up as well.

00:37:02.120 --> 00:37:05.500
And so we decided that, hey,
that's an end-user benefit.

00:37:05.500 --> 00:37:09.050
Why not add both frontal as
well as profile detection,

00:37:09.050 --> 00:37:12.070
add some quality filtering
to the capability,

00:37:12.070 --> 00:37:13.680
and then display that?

00:37:15.880 --> 00:37:21.170
So that's what we've done,
and we'll let Palavi show you the result.

00:37:25.300 --> 00:37:30.060
Okay, so this version which I'm going to
show you is where we implemented

00:37:30.060 --> 00:37:33.180
a pipeline approach using the
Threading Building Blocks.

00:37:33.180 --> 00:37:37.140
And I'm going to show you a part
of that code and how it looks.

00:37:37.140 --> 00:37:40.520
Phil talked about the various
filters in the various stages.

00:37:40.520 --> 00:37:44.320
And in the previous talk,
James talked about how you can increase.

00:37:44.320 --> 00:37:47.750
One of the ways to increase
scaling with parallelism is by

00:37:47.750 --> 00:37:51.440
increasing your workload or making
it more complex or value-added,

00:37:51.440 --> 00:37:52.560
as Phil mentioned.

00:37:52.920 --> 00:37:57.910
So not only are we
increasing the scalability,

00:37:57.910 --> 00:38:00.360
we are doing the parallelism
using the pipeline approach,

00:38:00.370 --> 00:38:04.670
where we would end up probably hiding
some of the latencies of the more serial

00:38:04.670 --> 00:38:08.690
or the slower stages of the pipeline,
but we are going to also add more

00:38:09.010 --> 00:38:13.200
and more stages for improving
quality or just increasing features,

00:38:13.200 --> 00:38:18.290
like adding side-face detection in
addition to the front face as an

00:38:18.290 --> 00:38:21.240
additional stage to the pipeline.

00:38:21.240 --> 00:38:22.900
So I'm going to show you first.

00:38:22.920 --> 00:38:27.120
some of that code and then I'm
gonna demo you how it looks like.

00:38:34.450 --> 00:38:38.900
So this is just the,
and as mentioned earlier, I mean,

00:38:38.900 --> 00:38:43.330
we are working at a very higher,
at an abstract layer rather than going

00:38:43.710 --> 00:38:51.280
into the nits and bits of the details of
writing and managing your native threads.

00:38:51.460 --> 00:38:55.640
This is a TBB code and
see how clean it looks.

00:38:55.640 --> 00:38:59.420
I mean, we start with the, let's say,

00:39:00.710 --> 00:39:04.280
initializing the task
scheduler for the TBB.

00:39:04.280 --> 00:39:07.940
And we follow it by just adding
the various stages of the pipeline,

00:39:07.940 --> 00:39:11.230
starting with the input filter,
where we are reading from the video file,

00:39:11.790 --> 00:39:17.060
followed by the face detection filter,
where we are detecting the front face,

00:39:17.060 --> 00:39:22.940
followed by the side face detection,
and then adding some more quality

00:39:22.940 --> 00:39:26.130
filters for maybe removing
some of those random circles,

00:39:26.140 --> 00:39:28.560
which we were seeing as false positives.

00:39:29.000 --> 00:39:32.830
And then ending up with the drawing
stage and then the output stage.

00:39:32.830 --> 00:39:36.650
And finally, we have this run pipeline,
where we are running the,

00:39:36.650 --> 00:39:39.480
where we actually get
the TBB pipeline going.

00:39:39.480 --> 00:39:45.190
So as you can see, it looks pretty neat,
pretty clear, pretty well encapsulated,

00:39:45.260 --> 00:39:49.320
the various stages can be
just implemented very neatly.

00:39:56.970 --> 00:40:00.580
So I'm going to now demo
the TBB pipeline version.

00:40:00.590 --> 00:40:05.780
And this will include the additional
feature of detecting the side faces,

00:40:05.780 --> 00:40:09.100
as well as a quality feature
where it's removing some of the

00:40:09.120 --> 00:40:10.970
random circles which we saw.

00:40:16.910 --> 00:40:21.300
And again,
we are reading from a video file.

00:40:21.320 --> 00:40:22.690
Look at the CPU utilization.

00:40:22.730 --> 00:40:27.460
We have many more features added
for quality and functionality.

00:40:27.460 --> 00:40:33.800
And we actually were down to
like 300% or 400% utilization.

00:40:33.800 --> 00:40:36.520
And now we are back to
using most of the cores,

00:40:36.520 --> 00:40:39.220
but this time with
additional functionality.

00:40:39.220 --> 00:40:41.660
I'm going to run it one more time.

00:40:48.170 --> 00:40:52.480
The red circles are for the side faces,
and the front ones are for the green.

00:40:52.530 --> 00:40:55.340
You also saw some of the blue ones,
and those are where we detected

00:40:55.340 --> 00:40:59.080
a side face and a front face,
and just to show you that how

00:40:59.080 --> 00:41:00.640
you can add more functionality.

00:41:00.700 --> 00:41:05.550
Also look at the time it took to
complete some of those 408 frames.

00:41:05.890 --> 00:41:09.460
It's only about like seven seconds,
so you can see a vast improvement

00:41:09.520 --> 00:41:14.590
in your speedup for the video.

00:41:15.620 --> 00:41:21.360
So we started this whole presentation by
demoing you the original version where

00:41:21.360 --> 00:41:23.560
we were taking input from the camera.

00:41:23.560 --> 00:41:27.980
And I just showed you,
then we kind of switched back to,

00:41:27.980 --> 00:41:31.750
switched to like reading from
a video file so that we can see

00:41:31.750 --> 00:41:35.400
more repeatable performance and we
can get some performance metrics.

00:41:35.400 --> 00:41:37.620
So we were trying to run
it like full throttle,

00:41:37.620 --> 00:41:39.260
you know, as fast as we could.

00:41:39.260 --> 00:41:42.980
And now I'm going to show you a version
where we go back to where we started.

00:41:43.140 --> 00:41:46.580
Now we are again going to take
input from the EyeSight camera.

00:41:46.580 --> 00:41:49.200
We are going to run
real-time and see how we do.

00:41:49.200 --> 00:41:51.520
And again,
this is the TB pipeline version,

00:41:51.520 --> 00:41:54.130
but now this time taking
input from the camera.

00:42:08.180 --> 00:42:10.600
So I'm like sideways,
probably it's reading,

00:42:10.600 --> 00:42:13.530
it's trying to see,
maybe detected a front

00:42:13.670 --> 00:42:16.140
face and a side face,
so it's looking blue.

00:42:16.170 --> 00:42:18.210
If I look forward...

00:42:18.880 --> 00:42:21.930
It kind of switches to
green in the middle,

00:42:21.930 --> 00:42:26.400
but I think it's also detecting it as a
side face in addition to a front face.

00:42:26.410 --> 00:42:28.710
That's why it's looking blue.

00:42:31.330 --> 00:42:34.290
It's running pretty fast.

00:42:34.310 --> 00:42:36.480
It's running real-time.

00:42:37.470 --> 00:42:39.020
Look at the CPU utilization.

00:42:39.020 --> 00:42:42.900
We are probably about, what, 500, 400%.

00:42:45.260 --> 00:42:48.750
So as you can see,
we can utilize Intel's Threading

00:42:48.750 --> 00:42:55.340
Building Blocks to probably get the same
kind of performance speedups in terms

00:42:55.340 --> 00:42:59.810
of CPU utilization and overall time.

00:43:01.100 --> 00:43:03.480
Okay, I'm out of the camera,
so I'm going to do it again.

00:43:03.480 --> 00:43:06.300
That's pretty much it for this video.

00:43:06.300 --> 00:43:08.000
Phil?

00:43:08.000 --> 00:43:10.490
Thank you.

00:43:17.610 --> 00:43:20.650
So I don't know if you noticed,
there was actually green circles,

00:43:20.860 --> 00:43:23.160
there was blue circles,
and there were red circles.

00:43:23.160 --> 00:43:27.050
What that was really trying to highlight
is the fact that the green circles

00:43:27.050 --> 00:43:29.020
were the front facial detection.

00:43:29.020 --> 00:43:34.720
Now, the profile also could pick
up some frontal views as well.

00:43:34.720 --> 00:43:38.220
And so what we decided to do was,
in addition to drawing the green circles,

00:43:38.220 --> 00:43:40.970
is that if both heuristics
detected the face,

00:43:41.060 --> 00:43:42.900
then we would make it blue.

00:43:43.720 --> 00:43:48.050
But if you actually had a side profile,
and it was only the side profile,

00:43:48.110 --> 00:43:49.750
then it would turn red.

00:43:49.760 --> 00:43:51.780
So at the very end,
Palavi was actually looking

00:43:51.850 --> 00:43:54.520
a little bit more sideways,
and actually you could see the

00:43:54.520 --> 00:43:56.080
red circles showing up as well.

00:43:57.860 --> 00:43:59.800
So,
one of the things that was interesting

00:43:59.800 --> 00:44:06.810
is we also got rid of the spin weights,
which is, other than improving task or

00:44:06.810 --> 00:44:09.800
context switching between threads,
we actually,

00:44:09.800 --> 00:44:13.960
by getting rid of that and improving
the quality and adding features,

00:44:13.960 --> 00:44:18.190
we were down to less than
somewhere around seven seconds

00:44:18.190 --> 00:44:20.320
in terms of performance.

00:44:20.320 --> 00:44:22.570
So, it was actually the best performance.

00:44:24.150 --> 00:44:29.180
So, in summary, by adding IPP,
we definitely got an improvement,

00:44:29.180 --> 00:44:32.840
about a 50% overall
performance improvement.

00:44:32.850 --> 00:44:36.360
And again, that's from a
single-threaded perspective.

00:44:36.410 --> 00:44:40.990
Keep it in mind, the fact that the video
decode and the display were

00:44:40.990 --> 00:44:43.110
not threaded at all from IPP.

00:44:43.170 --> 00:44:46.920
So, we already started with less
of the total performance.

00:44:46.940 --> 00:44:50.400
We were only focused on
the face detection part.

00:44:52.600 --> 00:44:56.620
With OpenMP, we saw that we got a
scaling from 1p to 8p,

00:44:56.620 --> 00:45:00.440
and that was about a 2x
improvement with the OpenCV and the

00:45:00.440 --> 00:45:03.570
Intel Performance Libraries using OpenMP.

00:45:03.710 --> 00:45:10.240
Now, the problem is,
is not that OpenMP was bad, right?

00:45:10.240 --> 00:45:11.440
That's the wrong message.

00:45:11.440 --> 00:45:14.200
If you look at it from
a library perspective,

00:45:14.200 --> 00:45:17.590
the only thing the library
has is an image to work on.

00:45:18.300 --> 00:45:23.040
So their point of view was the fact that
they actually would take the image and

00:45:23.040 --> 00:45:27.090
subdivide the image in terms of how they
were going to actually do parallelism.

00:45:27.100 --> 00:45:28.600
And they did see performance.

00:45:28.600 --> 00:45:33.180
The other thing is that if you
looked at this on a dual-core

00:45:33.180 --> 00:45:36.960
or a quad-core system,
you would never notice all of

00:45:36.970 --> 00:45:40.720
the performance improvement
that you can get on an 8-core,

00:45:40.720 --> 00:45:45.220
because you're already doing fairly
well with OpenMP and IPP already.

00:45:45.220 --> 00:45:51.690
But 2.5 is actually... be fairly
disappointing just to an OpenMP.

00:45:53.290 --> 00:45:59.400
But by introducing TBB and up-leveling
the problem and actually focusing on,

00:45:59.400 --> 00:46:03.160
instead of data
decomposition of an image,

00:46:03.160 --> 00:46:05.700
we actually up-leveled
to the image level,

00:46:05.700 --> 00:46:08.830
we were able to actually improve
the real-time performance

00:46:08.950 --> 00:46:10.460
on the facial detection.

00:46:10.460 --> 00:46:15.700
We could add filtering and profiling
to get rid of the false positives.

00:46:15.700 --> 00:46:20.100
You could add additional profile
detection and additional features.

00:46:20.220 --> 00:46:21.320
And that's not the end of it.

00:46:21.410 --> 00:46:25.780
I mean, we were probably up around
5x in terms of performance.

00:46:25.780 --> 00:46:30.780
We were already at about
600% CPU utilization.

00:46:30.780 --> 00:46:33.700
But clearly,
you can't do that on a quad-core system.

00:46:33.700 --> 00:46:37.050
And there's still room for
improvement in terms of capability.

00:46:37.060 --> 00:46:40.380
And the nice thing about the pipeline,
it was very easy to add stages.

00:46:40.380 --> 00:46:44.830
Each of those stages, actually,
in terms of implementation,

00:46:45.190 --> 00:46:49.120
was probably done in a half
a day from the original code.

00:46:49.820 --> 00:46:52.840
Because we were able to basically
cut and paste the original code

00:46:52.840 --> 00:46:56.580
into the filter stages and then just
added those lines to the pipeline.

00:46:56.580 --> 00:46:59.610
And what you really want to
do is focus on the fact that,

00:46:59.710 --> 00:47:03.540
you know, if you're targeting an
eight-way quad-core system,

00:47:03.540 --> 00:47:09.140
that you're adding value to the people
that have eight-way quad-core systems.

00:47:09.140 --> 00:47:11.620
But at some point,
you're going to be also marketing

00:47:11.620 --> 00:47:14.340
to people who only have a
dual-core or a single-core.

00:47:14.340 --> 00:47:17.860
So you want to be able to dial back
the features and functionality.

00:47:17.860 --> 00:47:22.160
And one way to do that is just... to
simply not have those stages added or

00:47:22.160 --> 00:47:24.940
removed to the actual implementation.

00:47:24.940 --> 00:47:28.680
So in summary, first of all,
develop for the high-end

00:47:28.680 --> 00:47:30.490
quad-core platforms.

00:47:30.500 --> 00:47:34.070
And then test and reduce the amount
of functionality that you're actually

00:47:34.070 --> 00:47:35.890
going to provide to the lower end.

00:47:35.900 --> 00:47:37.440
You want to be ahead of the curve.

00:47:37.440 --> 00:47:39.550
You have platforms already
out there for eight-way.

00:47:39.560 --> 00:47:43.440
James already talked about, you know,
the vision of Intel adding

00:47:43.870 --> 00:47:45.300
more and more quad-cores.

00:47:45.300 --> 00:47:47.380
Or adding more cores in general.

00:47:49.250 --> 00:47:51.700
Eight cores is here today,
and most applications don't

00:47:51.700 --> 00:47:53.000
take care of that already.

00:47:53.000 --> 00:47:56.750
16, 32, 40, 100.

00:47:56.760 --> 00:47:59.160
The vision's already there within Intel.

00:48:00.960 --> 00:48:06.320
And if you're not developing on high-end,
you can't actually see or think about

00:48:06.380 --> 00:48:11.970
or get the benefit of being able to
innovate on those higher-end platforms.

00:48:13.020 --> 00:48:15.740
It's not just about data decomposition.

00:48:15.740 --> 00:48:21.100
It's not just about taking your
problem today and making it faster.

00:48:21.100 --> 00:48:23.940
It's really about
increasing the complexity.

00:48:23.980 --> 00:48:28.640
So in terms of images,
making the images larger.

00:48:28.640 --> 00:48:32.760
In terms of video,
it's taking simple video codecs

00:48:32.760 --> 00:48:36.160
into the high-definition range.

00:48:36.160 --> 00:48:39.820
Adding functionality,
things like the filtering capability

00:48:39.820 --> 00:48:42.600
or side profile detection.

00:48:42.600 --> 00:48:44.740
I mean, at some point,
you could even envision that on

00:48:44.740 --> 00:48:48.240
an 8 or a 16 way that you would
move to facial recognition,

00:48:48.240 --> 00:48:51.460
that you would actually be able to
look up in a database and extract

00:48:51.520 --> 00:48:56.910
the image and do a compare to see,
you know, if that image matches

00:48:56.930 --> 00:48:58.580
somebody in your database.

00:48:58.590 --> 00:49:00.390
And of course, the improving quality.

00:49:00.580 --> 00:49:04.400
So high-definition video,
the filtering that we did to

00:49:04.400 --> 00:49:06.270
remove those false positives.

00:49:06.450 --> 00:49:10.250
So use the available compute resources
that you have to really innovate

00:49:10.260 --> 00:49:12.190
and improve the end-user experience.

00:49:12.200 --> 00:49:13.320
Thank you.

00:49:16.750 --> 00:49:18.700
There's a lot of new
techniques that are out there.

00:49:18.700 --> 00:49:20.310
Some of them are newer than others.

00:49:20.310 --> 00:49:23.460
Certainly,
OpenMP has been around for quite a while,

00:49:23.460 --> 00:49:27.030
but TBB is also available,
and that's actually open source.

00:49:27.040 --> 00:49:30.610
So that's something that you
can take and utilize in however

00:49:30.610 --> 00:49:32.420
you see fit going forward.

00:49:32.420 --> 00:49:35.580
Certainly,
Apple's innovating in the same region,

00:49:35.580 --> 00:49:39.370
moving that threading paradigm
higher and higher so that you're

00:49:39.370 --> 00:49:43.260
focused on the task and not at the
implementation or the details of

00:49:43.260 --> 00:49:45.730
managing your individual threads.

00:49:45.740 --> 00:49:48.240
You have to take advantage
of those capabilities.

00:49:48.240 --> 00:49:52.220
The benefit of doing that is as
processors get better and better

00:49:52.420 --> 00:49:58.040
and hardware innovation improves and
various curveballs get thrown at you

00:49:58.040 --> 00:50:02.520
from a NUMA architecture perspective,
those threading libraries are going to

00:50:02.520 --> 00:50:04.470
be evolving and taking advantage of that.

00:50:04.520 --> 00:50:07.140
They're going to be optimized
for those particular changes in

00:50:07.150 --> 00:50:08.730
the architecture going forward.

00:50:08.730 --> 00:50:11.670
If you're using those libraries,
you're going to get the benefit

00:50:11.670 --> 00:50:13.520
when those things become available.

00:50:15.960 --> 00:50:19.160
And for me, you know,
here was a prime example

00:50:19.160 --> 00:50:22.940
of an application that
Intel released as open source,

00:50:22.940 --> 00:50:27.070
built on OpenCV using the
Intel Performance Primitives.

00:50:27.080 --> 00:50:32.680
While it was very much focused on image
processing and added value at that level,

00:50:32.680 --> 00:50:36.060
the reality is that as more
threading becomes implemented,

00:50:36.060 --> 00:50:39.200
not only in your own applications,
but in other libraries

00:50:39.210 --> 00:50:43.280
that you might be using,
there's going to be conflict, right?

00:50:43.280 --> 00:50:46.820
So libraries need to be able to allow
you to control how threading happens,

00:50:46.820 --> 00:50:48.280
even at the library level.

00:50:48.280 --> 00:50:51.940
And as you move into those
areas of multi-threading,

00:50:52.000 --> 00:50:54.760
you know, think about extracting.

00:50:54.760 --> 00:50:58.350
Just don't take the benefit that you
get from the libraries for threading,

00:50:58.350 --> 00:51:01.280
but start thinking about your
own applications and how you can

00:51:01.330 --> 00:51:03.290
abstract that higher into your own.