WEBVTT

00:00:20.260 --> 00:00:24.510
The original doCallback function
is the change from a star to a

00:00:24.600 --> 00:00:28.280
caret in the parameter prototype.

00:00:28.280 --> 00:00:30.310
The invocation of the
block looks the same,

00:00:30.310 --> 00:00:33.160
but the big difference is that
when we invoke the callback,

00:00:33.160 --> 00:00:36.760
we don't actually have to have a
separately defined function elsewhere.

00:00:36.760 --> 00:00:42.910
We can just put the block of code
directly in line in our doCallback call.

00:00:44.660 --> 00:00:51.480
So queues are a concept in
Grand Central Dispatch that are

00:00:51.480 --> 00:00:52.720
basically a very fundamental

00:00:53.100 --> 00:01:13.400
[Transcript missing]

00:01:13.870 --> 00:01:16.200
Once the work has been
submitted to a queue,

00:01:16.250 --> 00:01:20.290
the work is run one at a time
for that queue in FIFO order.

00:01:20.300 --> 00:01:23.750
So the queue is basically a serialization
mechanism all of its own for

00:01:23.860 --> 00:01:26.060
serializing work that's submitted to it.

00:01:27.980 --> 00:01:31.440
Here's an example where we
have a couple of work blocks,

00:01:31.440 --> 00:01:34.700
and they might be on two threads
running at the same time.

00:01:34.700 --> 00:01:36.980
Both get submitted to the same queue,
and the queue will take the

00:01:37.100 --> 00:01:39.390
first one that's submitted to it,
execute that work block.

00:01:39.450 --> 00:01:42.810
When it's done,
it'll execute the second work block.

00:01:44.270 --> 00:01:47.400
To speak of how this
can help with MultiCore,

00:01:47.400 --> 00:01:49.410
well,
if you have multiple queues that are

00:01:49.530 --> 00:01:52.530
each working on work independently,
those queues can have their work

00:01:52.530 --> 00:01:54.290
scheduled on different CPUs.

00:01:54.300 --> 00:01:57.990
So here we have an example of a queue
that might be working on a work block,

00:01:58.050 --> 00:02:00.660
and if work gets submitted
to the second queue,

00:02:00.660 --> 00:02:04.550
we can start working on that as well
on a different CPU on the machine.

00:02:07.940 --> 00:02:09.910
Why would you want to use
queues instead of Pthreads?

00:02:10.020 --> 00:02:13.330
Well, aside from the challenge of knowing
how many Pthreads to create,

00:02:13.330 --> 00:02:16.790
it should also be noted that queues
are much more efficient than Pthreads.

00:02:16.830 --> 00:02:21.800
They're a simple data structure
on the order of about 256 bytes,

00:02:21.960 --> 00:02:25.320
whereas a Pthread with its stack
and all of its kernel resources,

00:02:25.450 --> 00:02:29.620
you know, might be well over half a
megabyte of information.

00:02:30.060 --> 00:02:34.640
and Grand Central Dispatch manages
a pool of threads that execute the

00:02:34.640 --> 00:02:37.170
work that are submitted to queues.

00:02:37.340 --> 00:02:40.220
The queues themselves are not
bound to any specific thread.

00:02:40.290 --> 00:02:43.010
They will be executing on
any thread that's available

00:02:43.110 --> 00:02:45.380
to this pool that's managed.

00:02:45.450 --> 00:02:48.920
However, again,
a queue will only execute one work

00:02:49.210 --> 00:02:52.070
block at a time for that queue.

00:02:52.190 --> 00:02:55.060
So you'll never switch threads
in the middle of a work block,

00:02:55.110 --> 00:02:57.360
but between work blocks that
are executed on a queue,

00:02:57.360 --> 00:03:00.140
the underlying thread might change.

00:03:01.410 --> 00:03:03.810
In this way,
we can scale the threads dynamically with

00:03:03.960 --> 00:03:07.970
the number of CPUs that are available
based on queues from the kernel and

00:03:07.980 --> 00:03:12.030
the kernel scheduler and the activity
that's happening both in the hardware and

00:03:12.030 --> 00:03:14.800
with other applications on the system.

00:03:17.190 --> 00:03:20.080
So as a design pattern,
what we'd recommend is that

00:03:20.180 --> 00:03:22.960
queues are used for distinct
subsystems of your code.

00:03:23.020 --> 00:03:27.300
And this allows the different
subsystems in your application to

00:03:27.300 --> 00:03:31.930
potentially execute concurrently,
but the access to the data inside

00:03:31.930 --> 00:03:35.370
of a subsystem you don't need
to take locks on because of the

00:03:35.390 --> 00:03:37.140
serialization nature of the queue.

00:03:37.140 --> 00:03:42.090
Any updates to the state of that
subsystem are implicitly serialized,

00:03:42.160 --> 00:03:44.780
so you don't need to incur
the expense of locks.

00:03:45.710 --> 00:03:48.840
And one way to look at this is
each queue becomes an island of

00:03:48.840 --> 00:03:52.290
serialization in a sea of concurrency.

00:03:53.750 --> 00:03:57.480
So here's some sample
code of creating a queue.

00:03:57.480 --> 00:04:00.630
Up at the top here,
we're creating some context data that

00:04:00.630 --> 00:04:02.020
we're going to associate with our queue.

00:04:02.020 --> 00:04:07.930
This is the context data that the
queue is going to serialize access to.

00:04:08.350 --> 00:04:11.370
You can see the first argument
to dispatch queue new is a label.

00:04:11.380 --> 00:04:14.410
We've given you the ability
to attach labels to queues.

00:04:14.480 --> 00:04:18.120
This helps in debugging to see what
queue is supposed to serve what purpose.

00:04:18.160 --> 00:04:22.140
And we're informally recommending
the reverse DNS style naming for

00:04:22.140 --> 00:04:25.150
queues so that as you create queues
and frameworks create queues,

00:04:25.160 --> 00:04:27.880
we can keep them all apart.

00:04:28.730 --> 00:04:32.740
There's a block as the third
argument to the new queue function,

00:04:32.740 --> 00:04:34.420
which is a deletion callback.

00:04:34.510 --> 00:04:36.960
So when the queue is deleted,
this gets invoked,

00:04:36.960 --> 00:04:39.630
and this basically lets us
free our context data that's

00:04:39.690 --> 00:04:40.780
associated with the queue.

00:04:40.780 --> 00:04:42.990
And then we pass in the
context data itself.

00:04:44.760 --> 00:04:49.070
I should note that blocks
do have all of the outer

00:04:49.390 --> 00:04:51.300
variables in scope in the block.

00:04:51.440 --> 00:04:54.370
So yes, it's okay for that block
to free context data,

00:04:54.380 --> 00:04:58.390
and it really does see that pointer
that's from the calling function.

00:05:00.010 --> 00:05:03.510
Further down on the slide,
you can see we have a dispatch call wait.

00:05:03.580 --> 00:05:05.390
That's one of our very primitive calls.

00:05:05.500 --> 00:05:09.540
It simply enqueues a block
for execution on that queue.

00:05:09.560 --> 00:05:12.950
And so you could imagine that there
might be multiple threads that all

00:05:12.950 --> 00:05:14.830
want to print this context data.

00:05:15.130 --> 00:05:18.330
Perhaps there's, you know, some,

00:05:18.520 --> 00:05:20.160
computation that needs to be done.

00:05:20.270 --> 00:05:23.120
Things need to stay synchronized
in the context data to generate

00:05:23.120 --> 00:05:24.810
the representation to be printed.

00:05:24.950 --> 00:05:26.480
So this would keep things serialized.

00:05:26.580 --> 00:05:30.550
It would make sure that no two
threads are trying to print the

00:05:30.560 --> 00:05:32.790
context data at the same time.

00:05:33.030 --> 00:05:35.080
And then finally,
we have an example of deleting

00:05:35.080 --> 00:05:37.480
a queue via DispatchQueueDelete.

00:05:37.500 --> 00:05:41.050
And once the queue is deleted,
it will fire that block

00:05:41.240 --> 00:05:43.680
that will free our context.

00:05:46.050 --> 00:05:49.370
So a higher-level concept
than DispatchCallWait is

00:05:49.370 --> 00:05:52.680
something called DispatchCall,
and that operates using items.

00:05:52.800 --> 00:05:56.410
And items are essentially handles
to a block that's outstanding.

00:05:56.420 --> 00:05:57.780
But it's more than that.

00:05:57.810 --> 00:05:59.920
It's not just the block
that needs to be run.

00:05:59.920 --> 00:06:04.550
It's also a completion block that
will be run when the first work

00:06:04.550 --> 00:06:08.520
block has completed to let you
know that the work has finished.

00:06:08.520 --> 00:06:11.950
And then context data can also
be associated with an item.

00:06:13.100 --> 00:06:16.760
And the use of item allows for
synchronization between multiple queues.

00:06:16.820 --> 00:06:20.560
The same item handle that's created
when you make a DispatchCall will

00:06:20.560 --> 00:06:23.600
be passed to the work function,
and again it will be passed to

00:06:23.600 --> 00:06:26.300
the completion callback so you
can correlate the two events.

00:06:29.300 --> 00:06:32.130
Another interesting aspect
of items is that in our API,

00:06:32.130 --> 00:06:34.860
there's a uniform mechanism
for canceling them.

00:06:34.980 --> 00:06:38.540
The cancellations advisory,
which means that the work blocks and

00:06:38.540 --> 00:06:42.410
the completion blocks need to test if
it's been canceled and voluntarily exit

00:06:42.500 --> 00:06:45.020
early if the item has been canceled.

00:06:45.070 --> 00:06:47.500
And we do this for memory
management reasons,

00:06:47.500 --> 00:06:49.870
which I will discuss in more depth later.

00:06:50.100 --> 00:06:54.090
But the work block and the
completion block are always called,

00:06:54.090 --> 00:06:59.040
and this gives your application the
ability to clean up any context data

00:06:59.040 --> 00:07:00.410
that are associated with the work.

00:07:02.830 --> 00:07:08.150
So here's a little animation of a
queue using DispatchCall to submit

00:07:08.720 --> 00:07:13.920
some work to another queue and
then receiving a completion block.

00:07:14.100 --> 00:07:19.550
So here on the left we have a queue
which is working on a work block.

00:07:19.590 --> 00:07:24.550
That in turn is making a DispatchCall to
call out to a second queue.

00:07:25.930 --> 00:07:28.720
The second queue will
execute its work block,

00:07:28.720 --> 00:07:31.730
and when that finishes,
a completion block will be

00:07:31.790 --> 00:07:35.000
submitted back to the first
queue that initiated the work,

00:07:35.020 --> 00:07:38.250
letting it know that its
work is now complete.

00:07:40.060 --> 00:07:43.730
And here's some sample code
for using DispatchCall.

00:07:43.800 --> 00:07:47.240
DispatchCall is called with the
queue as the first argument.

00:07:47.310 --> 00:07:50.920
We have a work block,
which this is going to iterate

00:07:50.920 --> 00:07:54.050
over an array and perform some
potentially expensive operation.

00:07:54.060 --> 00:07:55.770
So maybe we want to do
this in the background.

00:07:55.820 --> 00:07:57.600
We don't want to do
it on the main thread,

00:07:57.680 --> 00:08:00.190
because that'll block the
UI from processing events.

00:08:00.340 --> 00:08:03.340
We can submit it to
this background queue.

00:08:03.580 --> 00:08:05.700
And then once the array
has been processed,

00:08:05.750 --> 00:08:07.460
we'll get the completion callback.

00:08:07.460 --> 00:08:11.420
We can make a note in our structures
that the array has been processed,

00:08:11.420 --> 00:08:14.240
and then the UI can update appropriately.

00:08:14.250 --> 00:08:17.420
And the last argument in the
call you can see is the item.

00:08:17.430 --> 00:08:21.990
That is the item handle that refers
to this asynchronous operation.

00:08:22.130 --> 00:08:24.760
It's the same item that's passed
into the work block and is

00:08:24.760 --> 00:08:27.130
passed into the completion block.

00:08:28.620 --> 00:08:33.040
This is an example of the same code which
is testing for cancellation as it should.

00:08:33.040 --> 00:08:36.300
So there's an API,
DispatchItemTestCancel,

00:08:36.300 --> 00:08:40.030
which we will be testing for in
each iteration of our expensive

00:08:40.030 --> 00:08:41.640
operation on the array.

00:08:41.640 --> 00:08:44.310
And if at any time we see that
the work has been canceled,

00:08:44.310 --> 00:08:46.170
we simply break out of the for loop.

00:08:46.260 --> 00:08:48.440
We don't need to continue processing it.

00:08:48.620 --> 00:08:50.620
You know,
the user has hit the cancel button

00:08:50.620 --> 00:08:53.720
or done something else to indicate
that they don't care about the

00:08:53.720 --> 00:08:55.740
results of this operation anymore.

00:08:56.520 --> 00:09:00.020
And then in the completion callback,
we again test for cancellation to

00:09:00.020 --> 00:09:04.890
see if we really should honor the
results of this computation or not.

00:09:08.710 --> 00:09:12.160
But you might ask yourself,
what if each of these indexes on the

00:09:12.160 --> 00:09:14.030
array can be processed independently?

00:09:14.110 --> 00:09:18.480
Maybe there's no
correlation between the two,

00:09:18.480 --> 00:09:20.390
and we have a lot of CPUs.

00:09:20.400 --> 00:09:22.960
Is there anything we can do to
get more multi-core efficiency?

00:09:22.960 --> 00:09:27.460
Well, we have a concept called subtasks
that allow your applications to break

00:09:27.460 --> 00:09:29.360
the work up into multiple subtasks.

00:09:29.360 --> 00:09:33.340
And the subtasks may run
concurrently on available CPUs.

00:09:33.340 --> 00:09:35.760
Of course, they're not guaranteed
to run concurrently.

00:09:35.760 --> 00:09:37.350
They just may run concurrently.

00:09:37.420 --> 00:09:39.990
And that, of course,
depends on how many CPUs are actually

00:09:39.990 --> 00:09:41.450
available to perform the work.

00:09:41.560 --> 00:09:45.160
And that might change during the
course of the computation itself.

00:09:47.460 --> 00:09:50.980
So subtasks can be thought
of as a group of work blocks.

00:09:51.010 --> 00:09:55.800
We actually represent all of the
subtasks by a single item handle,

00:09:55.850 --> 00:09:58.640
and cancellation applies
to the group as a whole.

00:09:58.660 --> 00:10:02.820
So you can kick off a bunch of subtasks,
have them all run,

00:10:02.900 --> 00:10:04.480
cancel them all as a group.

00:10:04.550 --> 00:10:08.160
Completion is also done as a group.

00:10:08.160 --> 00:10:11.320
So basically the completion
block will fire once all of

00:10:11.380 --> 00:10:13.950
the subtasks have completed.

00:10:15.910 --> 00:10:18.640
We have two APIs for
working with subtasks.

00:10:18.720 --> 00:10:23.060
DispatchSubtasks takes an array of
blocks and it will run all of the

00:10:23.060 --> 00:10:27.800
blocks in the array as a subtask
concurrently with the current work block

00:10:27.800 --> 00:10:29.480
and anything else on the current queue.

00:10:29.480 --> 00:10:34.560
And you will get the completion callback
when all of those blocks have terminated.

00:10:34.560 --> 00:10:38.950
And DispatchApply is a similar
API which only takes one work block,

00:10:38.950 --> 00:10:42.500
but it takes a range,
a numeric range that should be applied.

00:10:43.080 --> 00:10:46.040
And so this is where you might have
the same operation that you want to

00:10:46.170 --> 00:10:48.100
apply to multiple things in an array.

00:10:51.250 --> 00:10:55.340
So to illustrate how subtasks
allow concurrent execution,

00:10:55.450 --> 00:10:59.730
here we have a queue
that's running on one core.

00:10:59.750 --> 00:11:01.830
It's executing a work block.

00:11:02.160 --> 00:11:04.780
That work block might
kick off four subtasks.

00:11:04.780 --> 00:11:09.070
So the first two start
executing on a couple of CPUs.

00:11:09.240 --> 00:11:12.980
The third and the fourth start
executing once the first two are done.

00:11:13.050 --> 00:11:16.310
And then when all have completed,
the completion block gets enqueued

00:11:16.380 --> 00:11:19.000
back on the original queue.

00:11:21.380 --> 00:11:26.760
Here's that same example of performing
an expensive operation on an array,

00:11:26.830 --> 00:11:29.790
but this time it's using
DispatchApply instead of using

00:11:29.790 --> 00:11:31.910
a for loop in the work block.

00:11:31.980 --> 00:11:35.350
It looks very similar,
but instead of a for loop,

00:11:35.420 --> 00:11:39.510
we are getting our index from an
API called DispatchItemGetIndex,

00:11:39.520 --> 00:11:42.520
and this tells us which index
of the apply operation the work

00:11:42.520 --> 00:11:44.350
block is currently operating on.

00:11:44.360 --> 00:11:48.240
Again, we test for cancellation to see
if the operation's been canceled.

00:11:48.240 --> 00:11:50.270
We perform the expensive operation.

00:11:51.460 --> 00:11:53.510
There's a new argument in
the middle here called count,

00:11:53.550 --> 00:11:57.430
and this tells us how many times we
need to iterate over the work block.

00:11:57.480 --> 00:12:00.450
And then, of course,
once all iterations are complete,

00:12:00.630 --> 00:12:03.530
the completion callback is returned,
and that looks the same as it

00:12:03.540 --> 00:12:04.720
did in the previous example.

00:12:07.970 --> 00:12:11.440
Here's an alternate example
using the Dispatch subtasks,

00:12:11.440 --> 00:12:15.360
where maybe you don't have the same work
that you want to perform multiple times,

00:12:15.360 --> 00:12:18.720
but you actually have discrete
units of work that need to

00:12:18.720 --> 00:12:21.880
be performed in parallel,
or may be performed in parallel.

00:12:21.880 --> 00:12:25.380
So here we have an example
of a Sudoku puzzle,

00:12:25.380 --> 00:12:29.720
where we are validating whether
a number is a legitimate

00:12:29.720 --> 00:12:32.500
choice in a particular square.

00:12:33.100 --> 00:12:35.860
And of course,
the constraints on Sudoku are that the

00:12:35.860 --> 00:12:41.810
number may not reappear in the same row,
the same column, or the same 3x3 grid.

00:12:41.820 --> 00:12:45.700
And so we can test each of
these constraints independently,

00:12:45.700 --> 00:12:49.280
and each block is colored
corresponding to the region of

00:12:49.280 --> 00:12:51.540
the puzzle which it's testing.

00:12:51.540 --> 00:12:55.180
And the API call we use
is DispatchSubtasksWait,

00:12:55.230 --> 00:13:00.440
so it runs all three of these
subtasks potentially at the same time,

00:13:00.440 --> 00:13:02.800
and then continues running 1x1.

00:13:03.020 --> 00:13:05.390
once all three have completed.

00:13:07.650 --> 00:13:11.630
The final concept of
Grand Central Dispatch is that of events.

00:13:11.700 --> 00:13:18.400
We've added a lot of support for tying
in core kernel types into an event loop

00:13:18.570 --> 00:13:20.500
that can be managed by your queues.

00:13:20.530 --> 00:13:23.870
And so we have sources of work for
queues that can be automatically

00:13:23.870 --> 00:13:27.980
generated based on timers,
signals, file descriptors, pipes,

00:13:28.140 --> 00:13:33.610
processes, Mach messages,
all sorts of core kernel types.

00:13:34.750 --> 00:13:38.480
And these sources are
bound to a specific queue,

00:13:38.530 --> 00:13:43.840
so every time an event fires
on that core kernel type,

00:13:43.880 --> 00:13:48.130
a work block will automatically
be submitted to your queue.

00:13:49.450 --> 00:13:50.450
And here's a sample code.

00:13:50.460 --> 00:13:55.140
We have a timer which is going to print
ping to standard output every second.

00:13:55.320 --> 00:14:00.170
So we're creating a new timer,
running it once a second on an interval,

00:14:00.300 --> 00:14:02.990
and it's just simply printing ping.

00:14:04.780 --> 00:14:08.740
We have some more sample code here,
which is a simple implementation

00:14:08.740 --> 00:14:13.240
of the cat command in Unix,
and that simply echoes standard

00:14:13.240 --> 00:14:15.890
input back out to standard output.

00:14:16.120 --> 00:14:18.830
So you can see we're creating
a new event source on the

00:14:18.890 --> 00:14:21.310
standard input file descriptor.

00:14:21.550 --> 00:14:23.680
We're reading data from
that file descriptor when we

00:14:23.680 --> 00:14:27.320
get this work block called,
and it indicates data as readable.

00:14:27.650 --> 00:14:30.780
If we got data from the file descriptor,
we'll go ahead and write it back

00:14:30.780 --> 00:14:32.310
out again to standard output.

00:14:32.500 --> 00:14:34.360
Otherwise,
we'll test for an error condition

00:14:34.360 --> 00:14:37.900
or test for the end of the
file and exit appropriately.

00:14:38.380 --> 00:14:42.220
And so we hope that providing some
of these event delivery mechanisms

00:14:42.320 --> 00:14:47.040
will help tie Grand Central Dispatch
into other aspects of your

00:14:47.040 --> 00:14:48.400
application more naturally.

00:14:50.880 --> 00:14:55.630
Now on to some of the advanced
topics of Grand Central Dispatch.

00:14:55.750 --> 00:15:00.500
And these are suspending and
resuming queues and event sources,

00:15:00.870 --> 00:15:04.270
how Grand Central Dispatch
deals with preemption,

00:15:04.340 --> 00:15:08.750
some techniques for memory management,
how deletion is handled and just

00:15:08.820 --> 00:15:12.490
resource deallocation in general,
ways to use queues for

00:15:12.700 --> 00:15:17.190
synchronization in your applications,
how Grand Central Dispatch is integrated

00:15:17.190 --> 00:15:22.880
with the frameworks on the system,
and a final note on how to properly

00:15:22.940 --> 00:15:26.630
abstract the use of dispatch queues.

00:15:27.390 --> 00:15:29.230
So suspend and resume.

00:15:29.400 --> 00:15:32.980
Queues and event sources
may be suspended or resumed.

00:15:33.000 --> 00:15:35.390
The suspend and resume
is reference counted,

00:15:35.390 --> 00:15:38.980
and basically any time this
reference count is greater than zero,

00:15:38.990 --> 00:15:41.570
it's considered to be suspended.

00:15:42.030 --> 00:15:46.100
Suspension, like cancellation,
does not interrupt anything

00:15:46.100 --> 00:15:47.010
that's currently running.

00:15:47.020 --> 00:15:50.840
It's just some internal state
that's tracked in our objects.

00:15:51.230 --> 00:15:54.340
However, once the current work block
has finished executing,

00:15:54.340 --> 00:15:58.860
no new work blocks will be run
from that queue if it's suspended,

00:15:58.860 --> 00:16:01.700
and no further work blocks
will be delivered from an

00:16:01.700 --> 00:16:02.980
event source if it's suspended.

00:16:02.980 --> 00:16:08.550
But when either one of them is resumed,
anything that had been pent

00:16:08.550 --> 00:16:10.830
up will be sent through.

00:16:12.730 --> 00:16:14.980
For event sources,
it's also important to note that

00:16:14.980 --> 00:16:16.760
some of the events may be coalesced.

00:16:16.760 --> 00:16:20.860
So if a file descriptor is
readable and you suspend it,

00:16:20.970 --> 00:16:23.320
you're not going to get
a flood of callbacks.

00:16:23.320 --> 00:16:26.630
When it's resumed,
you'll probably only get one callback

00:16:26.710 --> 00:16:30.320
indicating that there is data to
be read on the file descriptor.

00:16:33.350 --> 00:16:37.560
Grand Central Dispatch is designed
to work well with preemption.

00:16:37.720 --> 00:16:39.460
Queues use a weight-free algorithm.

00:16:39.460 --> 00:16:42.350
Submitting work to queues
does not need locking.

00:16:42.350 --> 00:16:44.420
And so they are preemption-friendly.

00:16:44.420 --> 00:16:48.720
The kernel can preempt threads that
are submitting work to a queue.

00:16:50.300 --> 00:16:53.170
However, the queues themselves never
preempt the work that's running.

00:16:53.220 --> 00:16:55.920
The work is always run one
at a time in FIFO order.

00:16:55.920 --> 00:16:59.410
The work blocks are always executed,
and the completion blocks

00:16:59.470 --> 00:17:00.900
are always executed.

00:17:00.920 --> 00:17:04.480
So you do have the guarantee that
you will see that block fire.

00:17:04.480 --> 00:17:06.920
You will be able to manage your memory.

00:17:08.460 --> 00:17:10.630
and queues are not reentrant in any way.

00:17:10.640 --> 00:17:14.920
Again, that's just consistent with
the one-at-a-time philosophy.

00:17:16.000 --> 00:17:19.340
So memory management is
a very important subject.

00:17:19.410 --> 00:17:23.500
Dispatch call is asynchronous,
and a lot of other of the API calls in

00:17:23.500 --> 00:17:25.770
Grand Central Dispatch are asynchronous.

00:17:25.850 --> 00:17:30.200
And so it's important not to reference
the local stack in the blocks.

00:17:30.240 --> 00:17:33.950
If you're using integers or
floats or other small instance

00:17:34.120 --> 00:17:39.880
variables or stack variables,
those will get copied into the block.

00:17:39.940 --> 00:17:42.990
Dispatch call will retain
the blocks as necessary and

00:17:42.990 --> 00:17:45.040
release them when it's done.

00:17:45.120 --> 00:17:49.450
But you don't want to do something
like refer to a large structure that's

00:17:49.450 --> 00:17:54.410
on the local stack that might get
freed before the dispatched block

00:17:54.460 --> 00:17:56.730
actually gets around to executing.

00:17:57.720 --> 00:18:03.500
So we advise the use of Dispatch Call in
a way where the context data that goes

00:18:03.500 --> 00:18:07.780
along with the Dispatch Call is copied
specifically for that Dispatch Call,

00:18:07.780 --> 00:18:12.120
or the ownership is just completely
handed over to that Dispatch Call.

00:18:12.350 --> 00:18:16.870
And then the copy is freed,
or the ownership means

00:18:16.870 --> 00:18:21.440
the original is freed,
whenever the completion callback fires.

00:18:21.440 --> 00:18:27.460
And that is why we always run the
work blocks and the completion blocks.

00:18:28.940 --> 00:18:32.030
Deletion is also fairly interesting
in Grand Central Dispatch

00:18:32.140 --> 00:18:34.230
because it's very asynchronous.

00:18:34.440 --> 00:18:36.650
When you delete a queue,
it doesn't actually free

00:18:36.650 --> 00:18:37.870
the memory right there.

00:18:38.160 --> 00:18:41.760
It waits for any outstanding
callbacks that may need to come back

00:18:41.760 --> 00:18:43.900
to that queue to have completed.

00:18:44.070 --> 00:18:47.510
and the deletion block will only
run after all of the outstanding

00:18:47.510 --> 00:18:50.470
work that's currently enqueued on
the queue and all of the outstanding

00:18:50.470 --> 00:18:53.980
completion callbacks have returned.

00:18:54.470 --> 00:18:57.040
At that point,
the deletion block is a good opportunity

00:18:57.040 --> 00:19:01.830
to free the context data because nothing
else is going to be accessing it.

00:19:01.960 --> 00:19:06.900
And the handle to the dispatch
queue will be invalid at the point

00:19:06.900 --> 00:19:08.300
that the deletion block returns.

00:19:08.300 --> 00:19:11.950
So the deletion block is your
application's notice that

00:19:12.040 --> 00:19:13.400
that queue is going away.

00:19:13.400 --> 00:19:17.120
It should not refer to that
same queue at any point after

00:19:17.120 --> 00:19:19.420
the deletion block has fired.

00:19:20.020 --> 00:19:23.830
Event sources will automatically be
deleted whenever a queue is deleted.

00:19:23.940 --> 00:19:27.440
And the reason for this is that the event
sources are bound to a specific queue.

00:19:27.440 --> 00:19:29.920
That's the only place
their work blocks will go.

00:19:29.920 --> 00:19:33.610
So if the queue gets deleted,
there's really no point trying

00:19:33.610 --> 00:19:37.880
to generate any work blocks
for those events anymore.

00:19:38.290 --> 00:19:42.440
And there's also an optional flag
you can pass to DispatchQueueDelete,

00:19:42.490 --> 00:19:45.420
which will implicitly set
the cancel bit on any work

00:19:45.480 --> 00:19:47.580
that's remaining on the queue.

00:19:47.580 --> 00:19:50.960
So there might be a bunch of work
that's already waiting on the queue.

00:19:50.960 --> 00:19:52.920
You decide you don't
want to do any of it.

00:19:52.960 --> 00:19:54.700
You want to delete the queue.

00:19:54.700 --> 00:19:58.080
Passing this flag means that as
each of those work blocks are run,

00:19:58.080 --> 00:20:00.670
whether or not the individual
items are canceled,

00:20:00.790 --> 00:20:04.690
the queue itself will just mark that
cancellation bit on each of the items.

00:20:04.690 --> 00:20:06.940
So the work blocks will test for that.

00:20:07.180 --> 00:20:09.440
They'll exit early,
and it lets you delete the queue

00:20:09.440 --> 00:20:12.400
as quickly as possible while
still fulfilling the guarantee

00:20:12.510 --> 00:20:13.660
that everything will be run.

00:20:17.060 --> 00:20:20.250
A note on synchronization
is that we recommend the use

00:20:20.330 --> 00:20:22.110
of queues instead of locks.

00:20:22.110 --> 00:20:28.040
You can run blocks using DispatchCall to
implement critical sections in your code.

00:20:28.060 --> 00:20:32.570
And this guarantees mutually
exclusive access to the resources

00:20:32.840 --> 00:20:34.640
that you want to protect.

00:20:34.640 --> 00:20:38.480
So there's really no need to
use mutexes or spinlocks when

00:20:38.480 --> 00:20:40.750
you can use the DispatchQueues.

00:20:40.820 --> 00:20:43.160
And this has a few advantages.

00:20:43.530 --> 00:20:48.220
One is the weight-free algorithm
of DispatchQueues does scale

00:20:48.220 --> 00:20:50.380
better with multiple CPUs.

00:20:50.380 --> 00:20:55.070
You are wasting fewer resources
trying to acquire a lock.

00:20:55.490 --> 00:20:59.040
And as programs evolve,
we've typically noticed that critical

00:20:59.070 --> 00:21:00.800
sections tend to get bigger and bigger.

00:21:00.800 --> 00:21:04.250
So even if it seemed really cheap
to acquire a lock and there wasn't

00:21:04.300 --> 00:21:07.020
much contention to begin with,
usually more code gets added,

00:21:07.110 --> 00:21:10.490
different functions and different
subsystems start doing more work,

00:21:10.640 --> 00:21:13.610
those locks get held for
increasing periods of time,

00:21:13.610 --> 00:21:17.620
the chance of contention increases,
and the performance goes down

00:21:17.680 --> 00:21:19.600
as the number of CPUs scale.

00:21:21.570 --> 00:21:25.210
So our weight-free queuing
algorithm really helps address that.

00:21:25.430 --> 00:21:29.290
And it's also important to note
that the proper use of a dispatch

00:21:29.290 --> 00:21:33.180
queue and that proper use is to
always specify a completion block.

00:21:33.310 --> 00:21:36.780
Don't use the weight variance,
but actually use the

00:21:36.900 --> 00:21:38.770
fully asynchronous APIs.

00:21:38.880 --> 00:21:41.140
That will avoid deadlock
because you're scheduling all

00:21:41.140 --> 00:21:42.660
of your work at that point.

00:21:42.710 --> 00:21:46.400
The system will always be able to
make forward progress executing work

00:21:46.470 --> 00:21:49.380
that has been submitted to the queues,
and you've eliminated the

00:21:49.380 --> 00:21:53.430
potential for deadlock,
which is one of the more common obstacles

00:21:53.480 --> 00:21:56.540
of using locks with a bunch of threads.

00:21:59.570 --> 00:22:04.650
So here's a very simple example of some
sample code where we're using dispatch

00:22:04.750 --> 00:22:06.900
call to implement a critical section.

00:22:06.900 --> 00:22:10.980
This is an example you may have
seen in a CS class a long time ago,

00:22:10.980 --> 00:22:13.020
where you have an account
balance at a bank,

00:22:13.170 --> 00:22:16.190
you make a deposit,
and somebody else might be

00:22:16.190 --> 00:22:18.980
trying to make a withdrawal,
and you don't want to end up in

00:22:18.980 --> 00:22:20.380
some weird intermediate state.

00:22:20.430 --> 00:22:24.080
You want each of the transactions
to operate in completion

00:22:24.080 --> 00:22:27.040
and one before the other,
hopefully the deposit

00:22:27.040 --> 00:22:28.490
before the withdrawal.

00:22:29.420 --> 00:22:34.000
And so we can see here that we're making
a dispatch call at the top of the slide.

00:22:34.190 --> 00:22:35.100
There is a work block.

00:22:35.190 --> 00:22:39.360
It's implementing the critical section
by being submitted to the queue,

00:22:39.360 --> 00:22:43.820
and it adds a deposit to the
balance of the bank account.

00:22:44.070 --> 00:22:47.940
The bottom half of the slide
is an example of a withdrawal.

00:22:47.960 --> 00:22:50.840
We are subtracting the
amount from the balance,

00:22:50.840 --> 00:22:54.840
determining whether there were
sufficient funds in the account.

00:22:54.840 --> 00:22:58.400
If not, we're using a trick of actually
canceling the item itself

00:22:58.430 --> 00:22:59.950
from within its work block.

00:23:00.020 --> 00:23:04.600
And this is a fairly effective way to
see whether an item succeeded or not.

00:23:04.790 --> 00:23:07.360
It's also very useful for subtasks.

00:23:07.360 --> 00:23:12.600
If maybe one branch of many subtasks
decide the operation wasn't worthwhile,

00:23:12.600 --> 00:23:13.980
it can cancel itself.

00:23:14.080 --> 00:23:17.360
And that actually takes out all of
the other subtasks at the same time.

00:23:17.360 --> 00:23:20.450
Otherwise,
if there were sufficient funds,

00:23:20.580 --> 00:23:23.990
we subtract the amount
from the account balance.

00:23:25.760 --> 00:23:28.540
So you may be curious about how
Grand Central Dispatch is integrated

00:23:28.540 --> 00:23:31.390
with the other frameworks on the system.

00:23:31.670 --> 00:23:37.420
We do plan to have integration
with the CFRunLoop and NSRunLoop.

00:23:37.540 --> 00:23:41.910
I'll speak to that,
the current state of the integration,

00:23:41.910 --> 00:23:42.540
in a minute.

00:23:42.540 --> 00:23:45.320
But the ultimate plan is
that they will be integrated,

00:23:45.320 --> 00:23:49.650
so all the completion callbacks that
come back to the main thread will

00:23:49.650 --> 00:23:52.150
be delivered to the current RunLoop.

00:23:52.200 --> 00:23:55.260
So in other words,
if you're running a thread in

00:23:55.370 --> 00:23:59.130
a CFRunLoop or an NSRunLoop
and you use a dispatch call,

00:23:59.300 --> 00:24:01.680
the completion callback
will automatically be

00:24:01.740 --> 00:24:03.400
scheduled with your RunLoop.

00:24:04.060 --> 00:24:08.980
NSOperation is being worked
on to provide a higher-level

00:24:08.980 --> 00:24:11.720
abstraction for Dispatch Queues,
but it will be using Dispatch

00:24:11.780 --> 00:24:16.600
Queues as its underlying mechanism,
so they'll be fully consistent there.

00:24:16.780 --> 00:24:19.230
There are some important
considerations which are fully

00:24:19.230 --> 00:24:20.860
documented in the headers.

00:24:20.860 --> 00:24:22.700
I'll point to the headers
a little bit later.

00:24:22.700 --> 00:24:25.700
About POSIX threads compatibility.

00:24:25.700 --> 00:24:30.700
As I mentioned earlier in the talk,
a queue is free to run work on any thread

00:24:30.700 --> 00:24:35.200
that's managed by Grand Central Dispatch,
and that thread may change between the

00:24:35.200 --> 00:24:36.930
invocations of work blocks on a queue.

00:24:36.940 --> 00:24:40.690
So there are a lot of things that
continue to work as they always have,

00:24:40.880 --> 00:24:43.530
but there are certain things
you should be aware of.

00:24:43.650 --> 00:24:46.400
In particular,
some of the Pthread get and

00:24:46.400 --> 00:24:48.440
set specific data won't work.

00:24:48.550 --> 00:24:50.650
If you've changed which
thread you're working on,

00:24:50.650 --> 00:24:53.220
you might not find certain
values that you've cached away.

00:24:53.220 --> 00:24:58.120
So you really shouldn't be manipulating
threads that you didn't create.

00:24:58.200 --> 00:25:02.340
Calling Pthread exit, for example,
would be a particularly rude thing

00:25:02.340 --> 00:25:04.270
to do from within a work block.

00:25:04.340 --> 00:25:06.920
And so one good rule of thumb
is to not use Pthread exit.

00:25:06.920 --> 00:25:10.160
The other one is to take nothing
but pictures and leave nothing

00:25:10.240 --> 00:25:14.550
but footprints when working
in work blocks with Dispatch.

00:25:16.220 --> 00:25:19.070
And finally,
it should be noted that if a dispatch

00:25:19.150 --> 00:25:24.100
call is made from a context that's
completely outside of any dispatch queue,

00:25:24.100 --> 00:25:26.640
the completion callback will get invoked.

00:25:26.800 --> 00:25:30.110
We'll route that back to
the main thread's RunLoop.

00:25:31.460 --> 00:25:34.510
And so if you're using a
RunLoop in your application,

00:25:34.650 --> 00:25:36.940
you can handle all of
these completion blocks.

00:25:37.250 --> 00:25:39.700
If you don't use a RunLoop
in your application,

00:25:39.740 --> 00:25:45.990
there is a DispatchMain API call that
can be used as a low-level POSIX RunLoop.

00:25:47.830 --> 00:25:51.700
So a final note on abstraction and
queues as an implementation detail.

00:25:51.700 --> 00:25:55.110
That's exactly what we
view these queues as,

00:25:55.200 --> 00:25:58.170
an implementation detail of your
applications and our frameworks.

00:25:58.180 --> 00:26:01.540
We're going to try not to expose
the queues directly as arguments

00:26:01.540 --> 00:26:05.710
in our higher-level API,
but they'll be used internally for

00:26:06.050 --> 00:26:11.750
thread-safe classes to protect the access
to instance variables in the objects,

00:26:11.790 --> 00:26:15.930
for example,
or to coalesce work that might be

00:26:15.930 --> 00:26:19.830
coming in for multiple threads but
needs to be performed on a single

00:26:19.830 --> 00:26:21.540
thread by a particular subsystem.

00:26:21.540 --> 00:26:24.700
And so in your own frameworks
and your own classes,

00:26:24.700 --> 00:26:27.230
we recommend you do the same,
to use the queues as an

00:26:27.240 --> 00:26:32.120
internal implementation detail
to achieve thread safety,

00:26:32.120 --> 00:26:34.980
but not necessarily be
passing queues everywhere,

00:26:34.980 --> 00:26:38.940
because once you start passing
queues between different subsystems,

00:26:38.990 --> 00:26:42.060
they have to start having knowledge
about what the context data that's

00:26:42.060 --> 00:26:44.820
associated with the queue is,
what blocks are safe to

00:26:44.820 --> 00:26:47.330
execute on that queue,
and it starts getting more

00:26:47.330 --> 00:26:48.420
difficult to maintain.

00:26:51.850 --> 00:26:54.930
So a final note is that
Grand Central Dispatch

00:26:54.950 --> 00:26:57.390
is a work in progress.

00:26:57.750 --> 00:27:03.050
There are certain things that you may
have seen in the presentations this week

00:27:03.190 --> 00:27:04.880
that aren't on the developer seat yet.

00:27:05.050 --> 00:27:09.960
We're working on things like a D-trace
static probe provider and a dispatch

00:27:09.960 --> 00:27:14.620
instrument to help analyze code
that's using Grand Central Dispatch

00:27:14.620 --> 00:27:15.800
and aid with debugging.

00:27:15.800 --> 00:27:18.250
These aren't in the seat yet.

00:27:18.680 --> 00:27:22.430
There was a demonstration of
the technology earlier today in

00:27:22.570 --> 00:27:25.240
getting started with instruments.

00:27:27.820 --> 00:27:30.260
There's also a known issue,
and that is the integration

00:27:30.260 --> 00:27:34.160
with CFRunLoop and NSRunLoop is
not in the Snow Leopard seed.

00:27:34.160 --> 00:27:37.670
However,
that hopefully will be coming soon.

00:27:40.280 --> 00:27:44.350
So now on to some demonstrations
of the technology.

00:27:44.460 --> 00:27:47.560
The first demonstration
I have is a Sudoku solver.

00:27:47.580 --> 00:27:52.060
And we're using a brute
force algorithm that-- oops,

00:27:52.060 --> 00:27:55.110
can we go back to the slide for a second?

00:27:57.710 --> 00:28:02.990
We're using a brute force algorithm,
and this is essentially iterating

00:28:03.060 --> 00:28:07.110
through the puzzle linearly,
trying possible values for each of

00:28:07.120 --> 00:28:10.650
the squares and backtracking if it
ever gets itself into a situation

00:28:11.000 --> 00:28:13.570
that it can't solve the puzzle from.

00:28:13.660 --> 00:28:19.780
We'll use the subtasks API to break
the puzzle up into multiple copies,

00:28:19.880 --> 00:28:24.330
pin another initial variable down so
we have a smaller puzzle to solve,

00:28:24.380 --> 00:28:27.320
run all of these smaller
puzzles in parallel,

00:28:27.320 --> 00:28:29.600
and if any one of them finds
the solution that's valid,

00:28:29.600 --> 00:28:32.770
it will cancel its own work item,
which stops the computation

00:28:32.770 --> 00:28:34.220
of all the other threads.

00:28:34.220 --> 00:28:38.130
And that means that once
the subtasks have returned,

00:28:38.130 --> 00:28:42.920
either a solution has been found
or no solution has been found.

00:28:43.600 --> 00:28:49.880
So now, if we go to the demo machine,

00:28:57.160 --> 00:28:59.820
Is that large enough?

00:28:59.890 --> 00:29:02.700
So we have a worst-case puzzle here.

00:29:02.740 --> 00:29:06.300
The one on the left-hand side
is going to be solved serially

00:29:06.320 --> 00:29:07.630
using the brute force algorithm.

00:29:07.650 --> 00:29:10.270
The one on the right-hand side
is going to be solved in parallel

00:29:10.310 --> 00:29:14.340
using the same algorithm,
but with a smaller search space.

00:29:14.470 --> 00:29:17.210
And one of the reasons this is
a worst-case puzzle is because

00:29:17.270 --> 00:29:19.560
the top row is all blank,
which means it's going to need

00:29:19.560 --> 00:29:22.080
to do a lot of backtracking
to fill in the values there.

00:29:22.280 --> 00:29:25.060
And we actually know the
solution is at the top row,

00:29:25.060 --> 00:29:27.410
987654321.

00:29:27.450 --> 00:29:30.480
So it really has to backtrack
a lot to get that first square

00:29:30.480 --> 00:29:31.990
up to the value of nine.

00:29:32.000 --> 00:29:36.200
And so we will start.

00:29:38.010 --> 00:29:43.180
And you can see the parallel version
found the answer in about a second.

00:29:46.700 --> 00:30:11.300
[Transcript missing]

00:30:14.840 --> 00:30:18.040
So our second demonstration
is a simple web server.

00:30:18.040 --> 00:30:21.090
This is actually available as
sample code that's associated

00:30:21.100 --> 00:30:23.780
with the session on the website.

00:30:24.180 --> 00:30:30.960
The architecture of this server is to use
a dispatch queue for each HTTP connection

00:30:31.130 --> 00:30:32.640
that's initiated on the server.

00:30:32.800 --> 00:30:37.040
HTTP connection really is a conversation
between the client and the server that

00:30:37.080 --> 00:30:39.290
needs to happen in a specific order.

00:30:39.410 --> 00:30:41.780
There's not much opportunity
for concurrency there,

00:30:41.950 --> 00:30:45.490
but obviously if there are multiple
connections at the same time,

00:30:45.490 --> 00:30:48.480
there's a lot of
opportunity for concurrency.

00:30:48.810 --> 00:30:51.980
The web server is also a great
example of using the events API.

00:30:52.100 --> 00:30:56.530
All of the I/O in the web server
is non-blocking and asynchronous,

00:30:56.600 --> 00:31:00.150
including reading the files
off of the local disk.

00:31:00.550 --> 00:31:03.500
And another great example
of the web server is that it

00:31:03.870 --> 00:31:06.240
uses queues for serialization.

00:31:06.280 --> 00:31:09.230
So we don't need to lock
when logging to our log file.

00:31:09.460 --> 00:31:13.940
We can just use dispatch call to
send a message to our logging queue,

00:31:13.940 --> 00:31:19.020
and that will append all the log
messages to the file in the right order.

00:31:25.210 --> 00:31:27.080
So looking at the demo machine.

00:31:27.100 --> 00:31:37.310
I already have the web server running,
but I'll pull up Safari,

00:31:37.310 --> 00:31:41.360
and you can see it loaded a
bunch of images from the Sierras.

00:31:41.650 --> 00:31:44.660
And each of these thumbnails
is actually the full big image.

00:31:44.750 --> 00:31:48.960
And Safari is actually
what's scaling it down.

00:31:49.170 --> 00:31:52.810
So we can bring up Top.

00:32:07.780 --> 00:32:12.300
And this process right here
is the Dispatch web server.

00:32:12.300 --> 00:32:14.970
You can see this is
an eight-core Mac Pro.

00:32:15.000 --> 00:32:17.100
It's actually scaled up
to using nine threads.

00:32:17.100 --> 00:32:21.770
It has the main thread and
the eight Dispatch threads.

00:32:22.110 --> 00:32:26.470
And it's not a great example
of using a lot of CPU because

00:32:26.480 --> 00:32:29.060
it's actually all non-blocking.

00:32:29.060 --> 00:32:33.670
So as we refresh this,
the CPU actually stays relatively low.

00:32:33.940 --> 00:32:36.200
But it is a good example of
design patterns that you want

00:32:36.200 --> 00:32:39.080
to use with dispatch queues.

00:32:44.780 --> 00:32:51.660
And if we can go back to the slides,
please.

00:32:51.660 --> 00:32:56.840
Our final example is an interesting
example of a variation of

00:32:56.930 --> 00:33:00.960
Conway's Game of Life that
is completely asynchronous.

00:33:00.960 --> 00:33:02.820
If you're familiar with
Conway's Game of Life,

00:33:02.820 --> 00:33:06.440
you'll recall that it's a grid of cells,
cellular automata,

00:33:06.440 --> 00:33:09.840
and the cells become alive
if they have two or three

00:33:09.840 --> 00:33:13.090
neighboring cells that are alive,
and the cells die if they

00:33:13.090 --> 00:33:15.930
have less than two neighboring
cells because they're lonely,

00:33:15.930 --> 00:33:18.990
or if they have greater than
three neighboring cells because

00:33:18.990 --> 00:33:21.810
there's population overcrowding.

00:33:22.510 --> 00:33:25.730
However, we have a quite unconventional
implementation of this.

00:33:25.850 --> 00:33:30.800
We're actually creating one
queue per cell in the simulation.

00:33:30.930 --> 00:33:35.890
And each cell will use dispatch call
to contact neighboring cells to ask

00:33:36.040 --> 00:33:38.080
them whether they're alive or not.

00:33:38.180 --> 00:33:42.870
And as the completion callback will
indicate whether or not it's alive.

00:33:43.340 --> 00:33:48.580
So all of the queries are
performed asynchronously.

00:33:48.730 --> 00:33:51.300
There's no locking that needs
to be done to look at the

00:33:51.300 --> 00:33:53.660
state of the neighboring cells.

00:33:53.710 --> 00:33:55.650
Once all the results
have been accumulated,

00:33:55.680 --> 00:33:57.740
the cell decides whether
it needs to change its own

00:33:57.740 --> 00:33:59.300
state according to the rules.

00:33:59.300 --> 00:34:02.250
And if it does have a state change,
it'll broadcast yet another

00:34:02.250 --> 00:34:06.280
message out to all the neighboring
cells to indicate that the state

00:34:06.280 --> 00:34:08.900
needs to change in those cells,
or potentially needs to

00:34:08.910 --> 00:34:10.510
change in those cells,
and then they'll start

00:34:10.510 --> 00:34:12.760
up their own queries,
and the whole process

00:34:12.760 --> 00:34:14.460
just kind of cascades.

00:34:14.540 --> 00:34:15.530
So here's a little animation.

00:34:15.530 --> 00:34:17.990
I have a simple 3x3 grid.

00:34:18.000 --> 00:34:21.190
There are three dark squares
which are considered to be living.

00:34:21.200 --> 00:34:24.600
There are other white squares
which are considered to not

00:34:24.600 --> 00:34:26.660
have anything in them at all.

00:34:26.800 --> 00:34:30.360
The center square will be
checking out its surroundings,

00:34:30.360 --> 00:34:33.440
so it sends a dispatch call
to the queue that represents

00:34:33.450 --> 00:34:35.670
each of the neighboring cells.

00:34:35.800 --> 00:34:38.650
Those, in turn,
will all reply with either a 1 or a 0,

00:34:38.650 --> 00:34:40.830
indicating that they're alive or not.

00:34:41.200 --> 00:34:44.000
In this case,
there are three living neighboring cells,

00:34:44.070 --> 00:34:48.490
and so there will be a state change
for this middle cell to be alive.

00:34:53.780 --> 00:34:55.700
So if we can go to the demo machine,
please.

00:34:55.700 --> 00:35:05.420
So here we have each of these squares
is represented by a dispatch queue.

00:35:05.420 --> 00:35:08.990
There's over 12,000
dispatch queues right here.

00:35:09.670 --> 00:35:15.800
We can pull up the processor palette
to see how busy this Mac Pro is.

00:35:17.270 --> 00:35:19.250
Pretty busy.

00:35:19.450 --> 00:35:23.880
Because Grand Central Dispatch adjusts
to the number of available CPUs,

00:35:24.010 --> 00:35:27.880
we can actually start turning off CPUs.

00:35:31.210 --> 00:35:33.990
And you'll see the simulation
start to slow down.

00:35:34.050 --> 00:35:36.240
You'll also see more gray
appear on the screen.

00:35:36.310 --> 00:35:39.740
Gray is all of the cells that
are in an intermediate state.

00:35:39.750 --> 00:35:42.780
They've received a notification
that they should change,

00:35:42.800 --> 00:35:45.520
but they haven't heard back
from all of their neighbors yet.

00:35:45.530 --> 00:35:47.920
And you can see the work is building up.

00:35:47.930 --> 00:35:50.090
We have a backlog of work to do.

00:35:50.160 --> 00:35:53.360
There's a lot of cells
that need processing.

00:35:53.420 --> 00:35:56.540
And then as we re-enable the CPUs,

00:35:57.520 --> 00:35:59.900
will start to catch up
with the simulation more.

00:35:59.950 --> 00:36:01.990
A lot of those gray regions
will start to vanish.

00:36:02.120 --> 00:36:06.020
So this is a great example of how
dynamic Grand Central Dispatch is in

00:36:06.030 --> 00:36:09.170
dispatching work to available CPUs.

00:36:19.400 --> 00:36:25.640
So back to the slides, please.

00:36:25.740 --> 00:36:28.040
So to get more information
about Grand Central Dispatch,

00:36:28.120 --> 00:36:33.140
there are header files in
the Snow Leopard preview.

00:36:33.210 --> 00:36:37.560
They're at userinclude-dispatch.h
and userinclude-dispatch-events.h.

00:36:37.630 --> 00:36:40.410
Dispatch.h is what we're
calling the core API.

00:36:40.600 --> 00:36:43.060
It has queues and dispatch
call and subtasks,

00:36:43.060 --> 00:36:45.840
and then events are where you
can monitor for activity on any

00:36:45.840 --> 00:36:48.640
of the core kernel event types.

00:36:48.820 --> 00:36:53.570
There's also HeaderDoc HTML documentation
that's available on the WWDC site

00:36:53.640 --> 00:36:55.610
associated with this session.

00:36:55.620 --> 00:36:59.500
You do need to log in to the attendee
site to get that information.

00:36:59.500 --> 00:37:04.010
And we've also set up a mailing list,
or actually not a mailing list,

00:37:04.030 --> 00:37:05.680
but just a dropbox for mail.

00:37:05.680 --> 00:37:10.480
You can send feedback to
gcd-feedback at group.apple.com.

00:37:10.480 --> 00:37:11.720
It's not a mailing list.

00:37:11.720 --> 00:37:15.070
You can't subscribe to it because
this is still pre-release technology.

00:37:15.080 --> 00:37:16.960
We're not going to discuss
it in a public forum.

00:37:17.510 --> 00:37:19.880
But if you have questions
or comments or feedback,

00:37:19.880 --> 00:37:22.860
feel free to send an email there,
and we will read it.

00:37:22.960 --> 00:37:24.240
Value of nine.

00:37:24.240 --> 00:37:27.500
And so we will start.

00:37:27.540 --> 00:37:28.690
Thank you.

00:37:30.540 --> 00:37:35.680
And you can see the parallel version
found the answer in about a second.

00:37:39.120 --> 00:37:42.000
Whereas it took the serial
version 5.5 seconds.

00:37:42.040 --> 00:37:45.610
The great thing about this is if
you reduce the number of CPUs,

00:37:45.710 --> 00:37:49.920
it actually runs in about the same amount
of time as the serial version because

00:37:49.940 --> 00:37:52.960
it's doing the same amount of work
that it would need to have done anyway.

00:37:53.030 --> 00:37:57.370
It's just that by executing
them more than one at a time,

00:37:57.460 --> 00:38:00.920
you can find the solution faster.

00:38:01.250 --> 00:38:03.830
All right, go back to the slides, please.

00:38:07.300 --> 00:38:10.540
So our second demonstration
is a simple web server.

00:38:10.570 --> 00:38:13.590
This is actually available as
sample code that's associated

00:38:13.600 --> 00:38:16.220
with the session on the website.

00:38:16.670 --> 00:38:23.460
The architecture of this server is to use
a dispatch queue for each HTTP connection

00:38:23.610 --> 00:38:25.130
that's initiated on the server.

00:38:25.280 --> 00:38:29.520
HTTP connection really is a conversation
between the client and the server that

00:38:29.550 --> 00:38:31.650
needs to happen in a specific order.

00:38:31.900 --> 00:38:34.160
There's not much opportunity
for concurrency there,

00:38:34.260 --> 00:38:37.860
but obviously if there are multiple
connections at the same time,

00:38:37.980 --> 00:38:41.010
there's a lot of
opportunity for concurrency.

00:38:41.240 --> 00:38:44.410
The web server is also a great
example of using the events API.

00:38:44.530 --> 00:38:48.930
All of the I/O in the web server
is non-blocking and asynchronous,

00:38:49.020 --> 00:38:52.550
including reading the files
off of the local disk.

00:38:53.010 --> 00:38:56.430
And another great example
of the web server is that it

00:38:56.430 --> 00:38:58.710
uses queues for serialization.

00:38:58.740 --> 00:39:01.690
So we don't need to lock
when logging to our log file.

00:39:01.920 --> 00:39:06.400
We can just use dispatch call to
send a message to our logging queue,

00:39:06.400 --> 00:39:11.520
and that will append all the log
messages to the file in the right order.

00:39:17.670 --> 00:39:19.600
So looking at the demo machine.

00:39:19.600 --> 00:39:29.810
I already have the web server running,
but I'll pull up Safari,

00:39:29.810 --> 00:39:34.140
and you can see it loaded a
bunch of images from the Sierras.

00:39:34.140 --> 00:39:37.240
And each of these thumbnails
is actually the full big image.

00:39:37.240 --> 00:39:41.640
And Safari is actually
what's scaling it down.

00:39:41.640 --> 00:39:45.310
So we can bring up Top.

00:40:00.250 --> 00:40:04.780
And this process right here
is the Dispatch web server.

00:40:04.780 --> 00:40:07.430
You can see this is
an eight-core Mac Pro.

00:40:07.480 --> 00:40:09.560
It's actually scaled up
to using nine threads.

00:40:09.560 --> 00:40:14.150
It has the main thread and
the eight Dispatch threads.

00:40:14.570 --> 00:40:18.930
And it's not a great example
of using a lot of CPU because

00:40:19.030 --> 00:40:21.320
it's actually all non-blocking.

00:40:21.500 --> 00:40:26.130
So as we refresh this,
the CPU actually stays relatively low.

00:40:26.400 --> 00:40:28.660
But it is a good example of
design patterns that you want

00:40:28.660 --> 00:40:31.580
to use with dispatch queues.

00:40:37.230 --> 00:40:50.800
Our final example is a variation
of Conway's Game of Life that

00:40:50.800 --> 00:40:53.440
is completely asynchronous.

00:40:53.460 --> 00:40:55.300
If you're familiar with
Conway's Game of Life,

00:40:55.300 --> 00:40:58.910
you'll recall that it's a grid of cells,
cellular automata,

00:40:58.910 --> 00:41:01.660
and the cells become alive
if they have two or three

00:41:01.660 --> 00:41:04.910
neighboring cells that are alive,
and the cells die if they

00:41:04.910 --> 00:41:08.300
have less than two neighboring
cells because they're lonely,

00:41:08.300 --> 00:41:11.580
or if they have greater than
three neighboring cells because

00:41:11.590 --> 00:41:13.200
there's population overcrowding.

00:41:14.970 --> 00:41:18.280
However, we have a quite unconventional
implementation of this.

00:41:18.330 --> 00:41:23.300
We're actually creating one
queue per cell in the simulation.

00:41:23.390 --> 00:41:28.350
And each cell will use dispatch call
to contact neighboring cells to ask

00:41:28.500 --> 00:41:30.580
them whether they're alive or not.

00:41:30.640 --> 00:41:35.310
And as the completion callback will
indicate whether or not it's alive.

00:41:35.800 --> 00:41:41.190
So all of the queries are
performed asynchronously.

00:41:41.200 --> 00:41:43.800
There's no locking that needs
to be done to look at the

00:41:43.800 --> 00:41:46.160
state of the neighboring cells.

00:41:46.170 --> 00:41:48.150
Once all the results
have been accumulated,

00:41:48.170 --> 00:41:50.230
the cell decides whether
it needs to change its own

00:41:50.240 --> 00:41:51.800
state according to the rules.

00:41:51.800 --> 00:41:54.760
And if it does have a state change,
it'll broadcast yet another

00:41:54.760 --> 00:41:58.740
message out to all the neighboring
cells to indicate that the state

00:41:58.740 --> 00:42:01.310
needs to change in those cells,
or potentially needs to

00:42:01.310 --> 00:42:02.170
change in those cells.

00:42:02.200 --> 00:42:04.490
And then they'll start
up their own queries,

00:42:04.490 --> 00:42:06.920
and the whole process
just kind of cascades.

00:42:07.010 --> 00:42:08.040
So here's a little animation.

00:42:08.040 --> 00:42:10.460
I have a simple three-by-three grid.

00:42:10.500 --> 00:42:13.650
There are three dark squares,
which are considered to be living.

00:42:13.660 --> 00:42:17.120
There are other white squares,
which are considered to not

00:42:17.120 --> 00:42:19.120
have anything in them at all.

00:42:19.260 --> 00:42:22.770
The center square will be
checking out its surroundings.

00:42:22.800 --> 00:42:26.040
So it sends a dispatch call
to the queue that represents

00:42:26.040 --> 00:42:28.110
each of the neighboring cells.

00:42:28.260 --> 00:42:31.140
Those, in turn,
will all reply with either a 1 or a 0,

00:42:31.140 --> 00:42:33.640
indicating that they're alive or not.

00:42:33.660 --> 00:42:36.500
In this case,
there are three living neighboring cells,

00:42:36.540 --> 00:42:40.990
and so there will be a state change
for this middle cell to be alive.

00:42:46.300 --> 00:42:48.460
So if we can go to the demo machine,
please.

00:42:48.460 --> 00:42:57.920
So here we have each of these squares
is represented by a dispatch queue.

00:42:57.920 --> 00:43:01.400
There's over 12,000
dispatch queues right here.

00:43:02.150 --> 00:43:08.330
We can pull up the processor palette
to see how busy this Mac Pro is.

00:43:09.730 --> 00:43:11.900
Pretty busy.

00:43:11.910 --> 00:43:16.340
Because Grand Central Dispatch adjusts
to the number of available CPUs,

00:43:16.470 --> 00:43:20.380
we can actually start turning off CPUs.

00:43:23.630 --> 00:43:26.420
And you'll see the simulation
start to slow down.

00:43:26.470 --> 00:43:28.670
You'll also see more gray
appear on the screen.

00:43:28.730 --> 00:43:32.140
Gray is all of the cells that
are in an intermediate state.

00:43:32.180 --> 00:43:35.180
They've received a notification
that they should change,

00:43:35.220 --> 00:43:37.940
but they haven't heard back
from all of their neighbors yet.

00:43:37.960 --> 00:43:40.360
And you can see the work is building up.

00:43:40.360 --> 00:43:42.520
We have a backlog of work to do.

00:43:42.580 --> 00:43:45.780
There's a lot of cells
that need processing.

00:43:45.840 --> 00:43:49.000
And then as we re-enable the CPUs,

00:43:49.980 --> 00:43:52.400
will start to catch up
with the simulation more.

00:43:52.430 --> 00:43:54.420
A lot of those gray regions
will start to vanish.

00:43:54.590 --> 00:43:58.530
So this is a great example of how
dynamic Grand Central Dispatch is in

00:43:58.530 --> 00:44:01.670
dispatching work to available CPUs.

00:44:11.870 --> 00:44:20.400
So to get more information
about Grand Central Dispatch,

00:44:20.610 --> 00:44:24.660
there are header files in
the Snow Leopard preview.

00:44:24.660 --> 00:44:29.840
User include dispatch.h and
user include dispatch events.h.

00:44:29.840 --> 00:44:32.360
Dispatch.h is what we're
calling the core API.

00:44:32.360 --> 00:44:34.760
It has queues and dispatch
call and subtasks,

00:44:34.830 --> 00:44:37.980
and then events are where you
can monitor for activity on any

00:44:37.980 --> 00:44:39.960
of the core kernel event types.

00:44:40.810 --> 00:44:45.770
There's also HeaderDoc HTML documentation
that's available on the WWDC site

00:44:45.770 --> 00:44:48.040
associated with this session.

00:44:48.040 --> 00:44:51.940
You do need to log into the attendee
site to get that information.

00:44:51.940 --> 00:44:56.410
And we've also set up a mailing list,
or actually not a mailing list,

00:44:56.410 --> 00:44:58.240
but just a dropbox for mail.

00:44:58.330 --> 00:45:02.900
You can send feedback to
gcd-feedback at group.apple.com.

00:45:02.900 --> 00:45:04.010
It's not a mailing list.

00:45:04.010 --> 00:45:07.120
You can't subscribe to it because
this is still pre-release technology.

00:45:07.210 --> 00:45:09.390
We're not going to discuss
it in a public forum.

00:45:09.900 --> 00:45:12.360
But if you have questions
or comments or feedback,

00:45:12.370 --> 00:45:15.270
feel free to send an email there,
and we will read it.