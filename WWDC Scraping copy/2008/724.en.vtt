WEBVTT

00:00:29.410 --> 00:00:30.350
Hello, everyone.

00:00:30.440 --> 00:00:34.410
My name is Aaftaib Munshi,
and I'm going to talk about OpenCL and

00:00:34.410 --> 00:00:39.090
using OpenCL to do data parallel
computing on the GPU and CPU.

00:00:39.600 --> 00:01:52.600
[Transcript missing]

00:01:54.260 --> 00:01:58.320
So the OpenCL specification has been
developed in collaboration with AMD,

00:01:58.320 --> 00:02:01.000
Imagination Technologies, Intel,
and NVIDIA.

00:02:01.090 --> 00:02:04.310
And so this is something that
we work very closely with them,

00:02:04.310 --> 00:02:06.680
with these vendors,
to define something that's really robust.

00:02:06.740 --> 00:02:10.670
Robust in its feature set and
robust in its ease of use.

00:02:10.680 --> 00:02:14.670
It has to be really easy for people
to use for this to be successful.

00:02:15.470 --> 00:02:19.000
and in addition,
Synchronos is an industry-wide consortium

00:02:19.180 --> 00:02:23.200
which includes members that I had
talked about in the previous slide.

00:02:23.670 --> 00:02:28.600
and other developers have developed
standards such as OpenGL and OpenGL ES.

00:02:28.600 --> 00:02:32.040
Apple has released a specification
to Khronos and a new compute working

00:02:32.040 --> 00:02:37.420
group has been started with the goal
of taking this and ratifying it to be

00:02:37.420 --> 00:02:42.600
an open standard that will run across
multiple devices and multiple platforms.

00:02:44.100 --> 00:05:31.500
[Transcript missing]

00:05:31.660 --> 00:05:37.080
What are the goals or the requirements
that OpenCL had to answer or solve?

00:05:37.430 --> 00:05:41.720
Well, the first thing is, you know,
today we use CPUs for doing

00:05:41.920 --> 00:05:44.300
certain things and we use
GPUs for doing certain things.

00:05:44.300 --> 00:05:47.400
We want to actually leverage all the
computational resources in the system.

00:05:47.400 --> 00:05:49.210
So we want to view
CPUs and GPUs as peers.

00:05:49.350 --> 00:05:54.880
Now I talked about, you know,
the 8800GT has 504 gigaflops and the

00:05:55.010 --> 00:05:57.840
Mac Pro CPU 8 core has 96 gigaflops.

00:05:58.000 --> 00:06:00.240
Well,
why not take advantage of both of them?

00:06:00.240 --> 00:06:01.910
You get 600 gigaflops.

00:06:01.940 --> 00:06:04.380
And there are certain tasks
a CPU is really good for and

00:06:04.380 --> 00:06:06.020
certain tasks a GPU is good for.

00:06:06.020 --> 00:06:07.600
You may actually want to
design the algorithms that

00:06:07.610 --> 00:06:08.710
take advantage of both of them.

00:06:08.870 --> 00:06:11.090
And we want to be able to do that.

00:06:11.090 --> 00:06:13.770
And so OpenCL allows you to do that.

00:06:14.170 --> 00:06:17.600
The parallel programming model
has to be really easy to use.

00:06:17.680 --> 00:06:20.300
And I mean,
one of the reasons why a GPU is really

00:06:20.300 --> 00:06:23.840
a good example of a successful parallel
computing model is that if you just

00:06:24.190 --> 00:06:27.230
think of it in terms of 3D graphics,
I have a vertex shader,

00:06:27.230 --> 00:06:29.710
a fragment shader,
I really write one shader and then

00:06:29.710 --> 00:06:34.100
the data gets executed with a bunch
of vertices or a bunch of pixels.

00:06:34.180 --> 00:06:37.860
And I don't really have to worry
about how parallelism happens.

00:06:38.050 --> 00:06:40.340
It just has taken care of for me.

00:06:40.430 --> 00:06:45.390
And we want to do something very similar,
but apply it to functions.

00:06:45.880 --> 00:06:50.010
So it's great I have the GPU doing
really amazing computational power,

00:06:50.150 --> 00:06:54.640
but I'm trying to move my algorithm
that used to run on the CPU to the GPU.

00:06:54.660 --> 00:06:57.440
I want to see consistent
numerical behavior.

00:06:57.460 --> 00:07:02.970
And so we need to be able to specify the
accuracy of floating-point computations.

00:07:03.140 --> 00:07:06.300
So when I do an addition, subtract,
or multiplication of the GPU,

00:07:06.300 --> 00:07:09.980
I expect to see the same
result as I would on the CPU.

00:07:10.270 --> 00:07:13.900
And we actually need to
take it a little further.

00:07:13.900 --> 00:07:18.600
We have a bunch of math functions
that are defined in math.h today.

00:07:18.600 --> 00:07:22.690
We want to be able to support
that on any computer device,

00:07:23.180 --> 00:07:25.280
but also define what is
the minimum error bound.

00:07:25.300 --> 00:07:29.090
Because if I am using the
GPU for computations and

00:07:29.100 --> 00:07:33.980
the CPU for computations,
I need a library that clearly defines

00:07:34.150 --> 00:07:40.240
what the minimum error bound is so
I can design my algorithm accordingly.

00:07:40.300 --> 00:07:41.020
So that's really important.

00:07:41.020 --> 00:07:42.160
Numerical accuracy is really important.

00:07:42.180 --> 00:07:47.340
And then we can use OpenCL as the
vehicle to drive future hardware

00:07:47.400 --> 00:07:52.100
requirements for compute as future
revisions of OpenCL come into play.

00:07:52.140 --> 00:07:56.130
So where can I use OpenCL?

00:07:57.780 --> 00:07:59.140
Any data parallel algorithm.

00:07:59.140 --> 00:08:01.680
Okay, next slide, I'll talk about what
I mean by data parallel.

00:08:01.880 --> 00:08:04.170
Any data parallel algorithm,
that's a performance critical path.

00:08:04.400 --> 00:08:06.900
So some of the domains where you
would encounter data parallel

00:08:06.900 --> 00:08:08.940
algorithms would be physics simulation.

00:08:08.940 --> 00:08:12.700
Let's say you're doing rigid body
collision or fluid simulation.

00:08:12.880 --> 00:08:15.260
Image processing is a natural, you know,
false writing.

00:08:15.260 --> 00:08:16.180
I don't need to explain that.

00:08:16.300 --> 00:08:21.100
Signal processing, FFTs, data parallel,
video audio encoding, math libraries,

00:08:21.100 --> 00:08:25.000
you know, BLAS, LAPAC could totally run
on the GPU and the CPU.

00:08:25.000 --> 00:08:28.910
Non-traditional graphics,
restoration techniques like ray tracing,

00:08:28.980 --> 00:08:32.440
you know, financial modeling,
medical imaging, the list goes on and on.

00:08:32.440 --> 00:08:36.480
This is just a small sub-sample of...

00:08:36.810 --> 00:08:37.700
and many more.

00:08:37.700 --> 00:08:42.660
So, what is data parallel computing?

00:08:42.660 --> 00:08:46.350
In general, parallel computing can be
classified into two subcategories:

00:08:46.350 --> 00:08:48.400
task parallelism and data parallelism.

00:08:48.610 --> 00:08:50.700
What do I mean by task parallelism?

00:08:50.700 --> 00:08:54.620
Well,
let's say I have a library that operates

00:08:54.620 --> 00:09:00.220
on numbers and I have an algorithm
that produces the sum of the numbers.

00:09:00.270 --> 00:09:03.050
I have an algorithm that computes
the min and max and another

00:09:03.140 --> 00:09:05.600
algorithm that produces the median.

00:09:05.760 --> 00:09:08.780
So, these are independent tasks
or functions and they can

00:09:08.780 --> 00:09:10.600
actually operate in parallel.

00:09:10.600 --> 00:09:13.630
And in the example I just gave,
there is no dependencies between

00:09:13.630 --> 00:09:16.430
them and they can actually
be executed in parallel.

00:09:16.700 --> 00:09:20.150
But as a programmer,
I have to really think about how do

00:09:20.150 --> 00:09:25.120
I break these things into multiple
tasks which can execute in parallel.

00:09:25.120 --> 00:09:26.960
There may be cases where
there are dependencies.

00:09:26.960 --> 00:09:29.160
For example,
let's say I want to generate a histogram

00:09:29.160 --> 00:09:33.000
and then I want to take the output of
the histogram to do some tone mapping.

00:09:33.110 --> 00:09:34.820
Well, I cannot execute the tone mapping
in parallel to the histogram.

00:09:34.820 --> 00:09:38.250
There is a dependency that the
histogram task has to complete

00:09:38.310 --> 00:09:40.460
first before I do tone mapping.

00:09:40.460 --> 00:09:43.220
So I need to basically -- when
I write something that's going

00:09:43.220 --> 00:09:46.570
to be task parallel in nature,
I need to carefully think,

00:09:46.680 --> 00:09:51.340
break up these tasks and provide
the -- define the dependencies and

00:09:51.340 --> 00:09:53.640
then create the parallel model.

00:09:53.730 --> 00:09:54.690
That's what I mean by task parallelism.

00:09:54.700 --> 00:09:57.350
In OpenCL, you can actually do that.

00:09:57.410 --> 00:10:00.120
And each task can actually
be data parallel in nature.

00:10:00.120 --> 00:10:03.620
So in the example I just gave
you about summing the integers.

00:10:03.620 --> 00:10:04.620
We're going to write a task parallel.

00:10:04.630 --> 00:10:07.440
We're going to talk about how that would
be described as a data parallel example.

00:10:07.440 --> 00:10:09.730
So they can coexist together.

00:10:09.740 --> 00:10:11.080
But what is data parallel computing?

00:10:11.080 --> 00:10:13.360
Well,
if I have a function that is actually

00:10:13.440 --> 00:10:17.370
operating on multiple data elements
and the operation on each of the data

00:10:17.370 --> 00:10:20.340
elements can be executed in parallel,
that's what I mean by

00:10:20.340 --> 00:10:21.530
data parallel computing.

00:10:21.580 --> 00:10:26.320
And that's basically been the most
successful model of parallel computing.

00:10:26.320 --> 00:10:29.840
Traditionally most
supercomputers use that model.

00:10:29.840 --> 00:10:32.410
So let me describe an example.

00:10:33.030 --> 00:10:35.300
Let's look at the scalar code here.

00:10:35.330 --> 00:10:39.790
I want to multiply a number in A,
an array A, which has n entries,

00:10:39.790 --> 00:10:43.150
with another number in B,
corresponding index in B,

00:10:43.150 --> 00:10:46.000
and the product I want
to store in result.

00:10:46.130 --> 00:10:49.960
Well, if I were to write scalar code,
it would just be a for loop.

00:10:50.030 --> 00:10:51.910
If I were to write a
data parallel version,

00:10:52.300 --> 00:10:56.560
well, if you look at the code,
each multiplication for any entry in

00:10:56.560 --> 00:10:58.880
A and B can actually execute in parallel.

00:10:59.230 --> 00:11:05.000
So if I could create n
independent units of execution,

00:11:05.000 --> 00:11:08.620
where n is the size of the array here,
then I can execute all

00:11:08.620 --> 00:11:10.370
of them in parallel.

00:11:10.570 --> 00:11:12.930
And so that's what I mean
by data parallel computing.

00:11:13.110 --> 00:11:16.410
Well, let's take another example,
a little more complex.

00:11:16.630 --> 00:11:19.330
Well, let's take the sum of all integers.

00:11:19.510 --> 00:11:23.010
So if I wanted to sum all integers,
then it's basically for loop,

00:11:23.190 --> 00:11:25.520
which sums and produces a result.

00:11:25.570 --> 00:11:29.460
Let's look at how you would
describe that as a data parallel.

00:11:29.460 --> 00:11:31.430
And this is just one example,
one way you would do it.

00:11:31.520 --> 00:11:33.900
There are many ways of doing this.

00:11:33.950 --> 00:11:40.320
Well, let's say I have-- so
the array of integers,

00:11:40.590 --> 00:11:43.720
and I have m threads, in this case,
eight threads.

00:11:43.910 --> 00:11:45.240
Oops, there's nothing there.

00:11:45.320 --> 00:11:46.500
Sorry.

00:11:46.550 --> 00:11:49.090
In this case, eight threads.

00:11:49.200 --> 00:11:51.550
Then each thread generates a partial sum.

00:11:51.730 --> 00:11:55.350
So in this example,
thread 0 takes the sum of minus

00:11:55.520 --> 00:11:58.540
2 and 1 and produces minus 1.

00:11:58.600 --> 00:12:03.280
Thread 1 takes the sum of 10
and 5 and produces 15 and so on.

00:12:03.410 --> 00:12:07.320
So at the end of the first
instance of running this code,

00:12:07.320 --> 00:12:09.850
I have eight partial sums.

00:12:09.880 --> 00:12:10.720
OK.

00:12:10.770 --> 00:12:16.520
Then I can actually loop together and
now take four of those eight threads.

00:12:16.660 --> 00:12:19.060
And produce four partial sums.

00:12:19.080 --> 00:12:23.300
And then take two of those threads
and produce two partial sums.

00:12:23.450 --> 00:12:28.690
So finally, I'll end up with something
that has two partial sums.

00:12:28.700 --> 00:12:31.800
And then I can use the last thread,
the first thread,

00:12:31.890 --> 00:12:32.960
to generate the final result.

00:12:33.170 --> 00:12:35.640
So this is one example of how
you would do data parallel.

00:12:35.700 --> 00:12:40.470
So you have the first main
loop and then a log n loop.

00:12:44.140 --> 00:12:48.420
So what is the data parallel
compute model in OpenCL?

00:12:48.440 --> 00:12:53.290
So define your computation domain,
i.e., how many units of execution

00:12:53.800 --> 00:12:58.710
can operate in parallel on the
data that your function is using.

00:12:58.720 --> 00:13:01.230
We call them threads,
but these are basically

00:13:01.260 --> 00:13:05.830
just elements of execution,
the fine-grained in nature.

00:13:05.980 --> 00:13:07.760
They're not like P-threads.

00:13:07.790 --> 00:13:10.530
So that computation domain in
OpenCL is called global thread.

00:13:10.560 --> 00:13:13.570
So in the example we talked about where
we just multiplied one array with another

00:13:14.030 --> 00:13:17.790
and the array was n elements in size,
I would spawn n threads of

00:13:18.310 --> 00:13:23.390
computation because they can
all be executed in parallel.

00:13:23.780 --> 00:13:25.920
If there is no communication
between these threads,

00:13:26.070 --> 00:13:29.800
which was in the case when
I multiplied one number with another,

00:13:29.910 --> 00:13:31.240
I don't care.

00:13:31.430 --> 00:13:35.900
I shouldn't have to care how the compute
device goes and parallelizes that.

00:13:35.960 --> 00:13:39.530
And OpenCL refers to
this as implicit mapping.

00:13:39.650 --> 00:13:42.040
And so the mapping,
you just specify how many

00:13:42.040 --> 00:13:46.140
independent threads of execution
you want and OpenCL will go figure

00:13:46.140 --> 00:13:50.350
it out how to map that and execute
that on the compute device.

00:13:50.470 --> 00:13:53.400
In the example where I gave
the sum of all integers,

00:13:53.440 --> 00:13:56.530
I needed to know how many threads
were working because they were

00:13:56.660 --> 00:13:59.400
sharing information as they
were generating partial sums.

00:13:59.550 --> 00:14:02.180
That is an example of what
I call explicit mapping.

00:14:02.350 --> 00:14:04.960
So in there,
you really need to know how many

00:14:04.960 --> 00:14:06.840
threads are working together.

00:14:07.140 --> 00:14:09.100
And we call that a thread group.

00:14:09.200 --> 00:14:12.610
So thread group has a special property
that the threads that are in the

00:14:12.610 --> 00:14:16.380
thread group that are operating on the
data can communicate with each other,

00:14:16.380 --> 00:14:19.400
which they needed to in the
case of the sum of all integers.

00:14:19.480 --> 00:14:22.900
Because you needed to share
the partial sums together.

00:14:23.010 --> 00:14:25.280
So they can synchronize,
they can coordinate and

00:14:25.290 --> 00:14:26.900
communicate with each other.

00:14:26.920 --> 00:14:30.000
And these multiple thread
groups can execute in parallel.

00:14:30.000 --> 00:14:33.820
And they can execute in parallel
on the same compute unit in a

00:14:33.820 --> 00:14:38.300
compute device or across multiple
compute units in a compute device.

00:14:38.350 --> 00:14:41.570
So this is a high level
architecture overview.

00:14:41.630 --> 00:14:44.140
Well, how do I use OpenCL?

00:14:47.200 --> 00:16:00.600
[Transcript missing]

00:16:02.730 --> 00:16:03.590
Next thing, memory objects.

00:16:03.670 --> 00:16:05.060
Well,
there are two kinds of memory objects.

00:16:05.190 --> 00:16:05.940
The first is array.

00:16:05.960 --> 00:16:10.360
It's basically very similar to something
that you do when you do a malloc.

00:16:10.440 --> 00:16:11.070
You get a pointer.

00:16:11.290 --> 00:16:12.600
So in an array, you get a pointer.

00:16:12.600 --> 00:16:14.220
It's a linear collection of elements.

00:16:14.360 --> 00:16:16.600
You can reference each
element using a pointer.

00:16:16.600 --> 00:16:21.400
The thing to note is, you know, on a CPU,
the reads and writes will be cached,

00:16:21.400 --> 00:16:24.580
but on a GPU, typically,
it won't be cached.

00:16:24.680 --> 00:16:27.110
And so there are performance
implications of that which you need to

00:16:27.190 --> 00:16:32.600
remember or recognize when you write
your compute kernels or functions.

00:16:33.510 --> 00:16:37.700
and then 2D images, sorry, images are,
there are two types, 2D and 3D.

00:16:37.800 --> 00:16:41.620
And there are, you can,
the difference between an image and an

00:16:41.670 --> 00:16:43.860
array can be classified as the following.

00:16:44.070 --> 00:16:47.330
In the image, the data is stored in
a non-linear format,

00:16:47.490 --> 00:16:50.500
so you do not get a pointer and
cannot directly access the elements.

00:16:50.580 --> 00:16:54.530
So there are built-in functions that
the OpenCL language will allow you

00:16:54.530 --> 00:16:57.610
to specify what you want to read,
where you want to read and

00:16:57.610 --> 00:16:59.200
where you want to write.

00:16:59.310 --> 00:17:04.270
In addition to that,
you also get what we call a sampler,

00:17:04.350 --> 00:17:08.290
so controls that dictate or determine
how you want to read from an image.

00:17:08.350 --> 00:17:12.400
So given a 2D coordinate for
a 2D image in an XY or a 3D

00:17:12.400 --> 00:17:16.380
coordinate for a 3D image XYZ,
you can also specify how to convert

00:17:16.500 --> 00:17:20.300
that to the actual position in the image
using what we call addressing mode,

00:17:20.380 --> 00:17:23.030
which is very similar to what GL,
OpenGL has,

00:17:23.060 --> 00:17:26.710
and the filter mode if you want
to do a bilinear filter or not.

00:17:27.260 --> 00:17:31.240
And the last object, well, the function,
the data parallel function that we

00:17:31.240 --> 00:17:32.430
have is called a compute kernel.

00:17:32.710 --> 00:17:33.890
So here's an example.

00:17:34.100 --> 00:17:38.320
So ignore some of the tokens
like _kernel and _global for now,

00:17:38.320 --> 00:17:39.940
and I will talk about that.

00:17:40.060 --> 00:17:42.580
So it just looks like C code.

00:17:42.730 --> 00:17:46.100
And a program would be
analogous to a dynamic library.

00:17:46.190 --> 00:17:47.250
It has a bunch of functions.

00:17:47.250 --> 00:17:48.350
Some of them are exported.

00:17:48.550 --> 00:17:52.010
So the functions that are declared with
the kernel qualifier are your exported

00:17:52.010 --> 00:17:55.450
functions that you can actually call.

00:17:55.750 --> 00:17:58.240
So how does one write compute kernels?

00:17:58.280 --> 00:17:59.430
Well,
we want to make sure there's something

00:17:59.550 --> 00:18:00.600
people are very familiar with.

00:18:00.660 --> 00:18:05.740
And so the OpenCL C is
basically derived from C99,

00:18:05.740 --> 00:18:06.230
okay?

00:18:06.230 --> 00:18:09.190
And has additions to it,
and some of the additions

00:18:09.260 --> 00:18:10.600
are vector data types.

00:18:10.690 --> 00:18:12.960
So you can add -- and vector
data types can be two component,

00:18:12.960 --> 00:18:16.570
four component, eight component,
up to 16 components for, you know,

00:18:16.600 --> 00:18:18.590
integer and float types.

00:18:18.600 --> 00:18:22.630
And you can do all your basic
operations you would do,

00:18:22.720 --> 00:18:26.750
like if you say A plus B,
it works for a vector type as

00:18:26.750 --> 00:18:28.410
it would for a scalar type.

00:18:28.600 --> 00:18:32.100
Then there are image types,
so you can actually use image data

00:18:32.100 --> 00:18:36.600
types and reference them and call the
built-in functions to read and write.

00:18:36.720 --> 00:18:37.570
There are address and
function qualifiers.

00:18:37.610 --> 00:18:40.650
The function qualifier that only
exists today is the kernel qualifier,

00:18:40.660 --> 00:18:42.600
which is basically something that
tells us that it's a compute kernel.

00:18:42.600 --> 00:18:45.600
And address qualifiers,
I will talk about that in a few slides.

00:18:45.600 --> 00:18:47.600
And there's a rich suite
of built-in functions.

00:18:47.600 --> 00:18:51.540
So all the math functions that you
would see today in C99 are supported.

00:18:51.610 --> 00:18:54.030
There is functions for doing
synchronization that you would

00:18:54.030 --> 00:18:55.570
need for threads in a thread group.

00:18:55.660 --> 00:18:59.250
There are atomic functions,
there are conversion functions to

00:18:59.400 --> 00:19:03.600
convert from one data type to another,
like into a float and so on, okay?

00:19:03.830 --> 00:19:06.600
So let's take an example.

00:19:06.800 --> 00:19:08.600
Remember the sum of all integers?

00:19:08.870 --> 00:19:12.260
So this was the first loop.

00:19:12.500 --> 00:19:13.900
So what would that code look like?

00:19:14.000 --> 00:19:19.400
So there's the underscore underscore
kernel qualifier that says dpsum and

00:19:19.400 --> 00:19:27.320
there was the input array which is called
a and the temp sum is basically the array

00:19:27.320 --> 00:19:29.400
that will hold the partial sums together.

00:19:29.400 --> 00:19:32.220
And finally I'll store
the result in result.

00:19:32.400 --> 00:19:35.400
So ignore the global and
local qualifiers for now.

00:19:35.400 --> 00:19:42.340
So each thread is identified by a
unique ID and it has a global ID.

00:19:42.430 --> 00:19:45.490
So if you had n global
threads you had spawned,

00:19:45.520 --> 00:19:47.360
each gets a unique ID which
can be one dimensional,

00:19:47.420 --> 00:19:48.400
two dimensional, and three dimensional.

00:19:48.400 --> 00:19:52.400
And that has an advantage and we'll talk
about that in the next example I give.

00:19:52.400 --> 00:19:54.970
And each thread in a thread
group which is the explicit

00:19:55.120 --> 00:20:00.390
working group together that works
together has a unique ID as well.

00:20:00.520 --> 00:20:02.400
So we generate the partial sums here.

00:20:02.400 --> 00:20:04.240
The first partial sum.

00:20:04.480 --> 00:20:05.270
So we'll have eight of them.

00:20:05.400 --> 00:20:08.740
And like I said in the example
I gave then you loop over until you

00:20:08.740 --> 00:20:13.400
finally get two partial sums and then
thread zero computes the final sum.

00:20:13.400 --> 00:20:16.390
So here is what the code looks like.

00:20:18.140 --> 00:20:21.530
So the first loop was getting
a partial sum across n threads,

00:20:21.530 --> 00:20:25.080
then we summed the partial sums,
and then we generated the final sum.

00:20:26.260 --> 00:20:28.100
Let's take another example.

00:20:28.330 --> 00:20:31.620
Let's say I want to do an image
filter where I want to just do

00:20:31.620 --> 00:20:33.600
a reflection across the Y-axis.

00:20:33.890 --> 00:20:36.600
And in this case I have an image.

00:20:36.600 --> 00:20:39.230
So each element actually
in this particular filter,

00:20:39.230 --> 00:20:42.600
since I'm doing a reflect,
can operate independently.

00:20:42.600 --> 00:20:46.810
And in this, so each element or pixel in
the image can actually be an

00:20:46.810 --> 00:20:49.100
independent unit of execution.

00:20:49.100 --> 00:20:55.070
So the threads I want to spawn here would
be the width times height of the image.

00:20:55.190 --> 00:20:56.920
Well,
I think it would be better to describe

00:20:56.990 --> 00:20:59.600
this not as a one-dimensional problem
but as a two-dimensional problem.

00:20:59.600 --> 00:21:02.520
Because I can actually,
if I have a two-dimensional identifier,

00:21:02.600 --> 00:21:06.600
I can actually use that identifier as
my coordinate to get into the image.

00:21:06.780 --> 00:21:09.100
And that's, you know, you can do that.

00:21:09.230 --> 00:21:11.100
And so you call read image to read.

00:21:11.100 --> 00:21:13.660
And then, you know,
the right reflected to the

00:21:13.990 --> 00:21:15.600
reflection and then the right.

00:21:15.780 --> 00:21:22.100
Well, with image, what happens is,
you know, the underlying data format

00:21:22.210 --> 00:21:25.090
can be different from,
but when you read or write, you know,

00:21:25.090 --> 00:21:29.100
the output of the read
is always four floats.

00:21:29.240 --> 00:21:31.000
And the input to the right
is always four floats.

00:21:31.100 --> 00:21:33.100
And automatic conversion happens.

00:21:33.100 --> 00:21:34.720
Well,
what if I already know what the format

00:21:34.830 --> 00:21:36.100
is and I don't want to use images?

00:21:36.100 --> 00:21:40.100
So let's say it's RGBA 8888,
8 bits per pixel.

00:21:40.100 --> 00:21:41.980
You know, can I use something else?

00:21:41.980 --> 00:21:43.030
Yeah, you can.

00:21:43.100 --> 00:21:46.730
In this case,
you can use the Clarison array.

00:21:47.160 --> 00:21:57.230
and many more.

00:21:57.230 --> 00:21:57.230
You have many choices available.

00:21:57.230 --> 00:21:57.230
These are not optimized examples,
so don't go and write this

00:21:57.230 --> 00:21:57.230
and expect full performance.

00:21:58.300 --> 00:22:00.050
So let's talk about,
remember in some of the

00:22:00.050 --> 00:22:02.790
slides we were talking about
global and underscore local.

00:22:02.900 --> 00:22:04.080
So what are these things?

00:22:04.110 --> 00:22:07.080
Well,
in OpenCL we have memory address spaces.

00:22:07.120 --> 00:22:13.060
And this describes a different
memory subsystems or hierarchy that

00:22:13.060 --> 00:22:15.240
may exist in the computer device.

00:22:15.290 --> 00:22:18.280
On some computer devices they may be
collapsed and I'll talk about that.

00:22:18.500 --> 00:22:22.670
So first thing is remember each thread
is an independent unit of execution

00:22:23.200 --> 00:22:27.250
and has what is called its own memory,
what we call private memory.

00:22:27.430 --> 00:22:33.290
Then the threads that are in the thread
group that want to be able to share data,

00:22:33.550 --> 00:22:38.440
well they need some buffers to
be able to share data with and

00:22:38.440 --> 00:22:39.880
that's called local memory.

00:22:39.880 --> 00:22:46.560
And local memory, so in the case on GPUs,
the local memory would be actually

00:22:46.560 --> 00:22:50.520
a user managed cache where your
computer is going to manage it.

00:22:50.520 --> 00:22:56.530
Whereas on the CPU it's just basically
cached memory that maps to your DRAM.

00:22:56.600 --> 00:22:58.420
And then there's global memory,
which is where your input and output

00:22:58.530 --> 00:23:00.680
data finally get read from or stored.

00:23:00.690 --> 00:23:04.120
And there's a property of global memory.

00:23:04.120 --> 00:23:05.480
Let's take an example.

00:23:05.480 --> 00:23:09.070
Let's say I want to do
a convolution filter.

00:23:09.290 --> 00:23:11.950
The filter weight can be stored
in global memory but it's

00:23:11.950 --> 00:23:12.920
basically just being read.

00:23:12.920 --> 00:23:14.280
I'm not modifying it.

00:23:14.280 --> 00:23:19.480
And it has a unique property that
each thread as it's operating,

00:23:19.480 --> 00:23:22.520
applying the filter,
it wants to read the same filter weight.

00:23:22.720 --> 00:23:25.800
So wouldn't it be nice if there
was a way to be able to do that?

00:23:25.800 --> 00:23:29.020
was a way to be able to cache that
and be able to access that really,

00:23:29.090 --> 00:23:29.860
really fast.

00:23:29.860 --> 00:23:32.870
And that's what the constant
qualifier is telling,

00:23:33.320 --> 00:23:33.760
indicating.

00:23:33.890 --> 00:23:37.360
So that tells the OpenCL compiler
that there is a unique

00:23:37.360 --> 00:23:39.640
property behind this constant.

00:23:39.760 --> 00:23:42.550
And these memory address spaces
can only be specified to pointers.

00:23:42.550 --> 00:23:45.200
So anything you declare as a pointer,
you need to tell us what

00:23:45.390 --> 00:23:47.200
the memory address space is.

00:23:47.200 --> 00:23:49.500
OK, so we looked at the high-level
architecture overview.

00:23:49.630 --> 00:23:51.370
We looked at what the objects are.

00:23:51.500 --> 00:23:54.160
We looked at how to
write computer kernels.

00:23:54.160 --> 00:23:55.380
So I wrote a computer kernel.

00:23:55.380 --> 00:23:57.120
I think it's going to
perform really well.

00:23:57.140 --> 00:24:01.500
Well, how do I get the computer
device to execute it?

00:24:01.610 --> 00:24:05.940
What code do I need to
write on the host to execute

00:24:06.000 --> 00:25:33.900
[Transcript missing]

00:25:35.080 --> 00:25:37.340
Now, I want to load my program.

00:25:37.350 --> 00:25:41.000
Well, so I have my source, I load it,
I build the executable,

00:25:41.000 --> 00:25:41.920
and I then create the kernel.

00:25:42.000 --> 00:25:46.040
CreateKernel is basically very
similar to if you do a load library,

00:25:46.070 --> 00:25:49.660
then you call get proc address to
get your address of the function.

00:25:49.760 --> 00:25:51.840
That's effectively what it's doing.

00:25:51.910 --> 00:25:54.460
Now, before I call my function,
so when I make a function call,

00:25:54.460 --> 00:25:56.750
I call the function,
I specify the arguments.

00:25:56.820 --> 00:25:59.940
I specify the arguments
in OpenCL using CL,

00:25:59.970 --> 00:26:02.960
something called CL set kernel args.

00:26:03.080 --> 00:26:05.980
So in this case,
the input arguments are the three memory

00:26:05.980 --> 00:26:09.940
objects I created and their sizes are
the size of the memory object type.

00:26:10.140 --> 00:26:14.710
So all memory objects that are created
on the host side are opaque handles and

00:26:14.710 --> 00:26:20.000
they get mapped to the right pointers by
the OpenCL framework when actually the

00:26:20.000 --> 00:26:21.850
function gets executed on the device.

00:26:21.970 --> 00:26:25.680
So on the host side,
you never have to deal with pointers.

00:26:26.200 --> 00:29:08.200
[Transcript missing]

00:29:10.220 --> 00:29:11.150
All right, great.

00:29:11.290 --> 00:29:12.140
I executed my kernel.

00:29:12.140 --> 00:29:13.100
I got data.

00:29:13.100 --> 00:29:14.900
I want to visualize that data now.

00:29:14.980 --> 00:29:15.690
So how do I do that?

00:29:15.890 --> 00:29:17.500
I'm going to use OpenGL.

00:29:17.620 --> 00:29:20.100
Well,
so how does OpenCL work with OpenGL?

00:29:20.280 --> 00:29:22.340
Well,
we want to make sure that any GL objects,

00:29:22.460 --> 00:29:25.000
such as a texture or a buffer object,
vertex or pixel,

00:29:25.550 --> 00:29:32.100
can actually be used as a
CL array or a text image,

00:29:32.100 --> 00:29:33.030
which you can.

00:29:33.210 --> 00:29:35.550
Well, not only that,
we want to make sure that they

00:29:35.550 --> 00:29:37.100
both point to the same bits.

00:29:37.100 --> 00:29:38.540
We don't want to be doing any copying,
so sharing has to be really,

00:29:38.540 --> 00:29:39.040
really efficient.

00:29:39.110 --> 00:29:44.040
So OpenCL can generate some data which
can actually be directly used as the

00:29:44.200 --> 00:29:50.100
input to GL without requiring any copy,
and that's what it's implemented.

00:29:50.100 --> 00:29:52.100
So how do you share these resources?

00:29:52.100 --> 00:29:55.910
Well, the first thing you need to
make sure that the GL context is

00:29:55.910 --> 00:29:58.100
created using a CGL share group.

00:29:58.100 --> 00:30:02.060
You want to make sure that the compute
device group that you are creating

00:30:02.060 --> 00:30:05.850
with CL points to that same thing,
which is basically saying I'm

00:30:05.860 --> 00:30:08.100
going to point to the same devices.

00:30:08.100 --> 00:30:10.930
And when I allocate my arrays or images,
I use this flag called

00:30:11.050 --> 00:30:13.250
memalloc_reference,
which basically says that

00:30:13.250 --> 00:30:17.100
I'm not going to allocate any
data or memory behind this,

00:30:17.100 --> 00:30:21.100
but I'm going to use whatever
memory has been allocated by GL.

00:30:21.100 --> 00:30:22.100
And then sharing is very easy.

00:30:22.100 --> 00:30:26.100
I attach, so I can attach to a GL image
or a GL buffer object,

00:30:26.100 --> 00:30:29.100
and then use that as an argument
in my CL set kernel args.

00:30:29.100 --> 00:30:31.100
And I can execute multiple kernels.

00:30:31.100 --> 00:30:32.980
When I'm done with it, I detach.

00:30:33.100 --> 00:30:37.090
And that's basically telling GL that,
okay, CL is done with it.

00:30:37.100 --> 00:30:37.100
Now you can use it.

00:30:37.100 --> 00:30:38.350
it.

00:30:39.610 --> 00:30:41.760
So in summary,
we have a new compute framework

00:30:42.280 --> 00:30:43.680
that works across GPUs and CPUs.

00:30:43.930 --> 00:30:46.840
It uses something we're
familiar with extensions.

00:30:47.160 --> 00:30:49.870
We define numerical position
requirements so that there's a

00:30:49.910 --> 00:30:53.500
consistent minimum error bound that's
guaranteed across compute devices.

00:30:53.510 --> 00:30:58.570
And this will be the open standard
for compute on desktop platforms.

00:30:59.560 --> 00:31:01.440
Well, let me introduce
Henry Moreton from NVIDIA.

00:31:01.580 --> 00:31:04.500
He's a distinguished engineer in
the architecture group at NVIDIA.

00:31:04.500 --> 00:31:08.500
And he's going to talk about GPUs and
OpenCL and how they're such a close fit.

00:31:08.670 --> 00:31:12.490
Thank you.

00:31:16.680 --> 00:31:18.380
Good morning.

00:31:18.530 --> 00:31:21.320
So, I'm Henry Moreton, obviously.

00:31:21.320 --> 00:31:24.540
And I have a few things I'd
like to talk with you about.

00:31:24.820 --> 00:31:28.710
First of all,
I would like to give you some idea why

00:31:28.710 --> 00:31:31.630
you should be interested in OpenCL.

00:31:31.740 --> 00:31:35.200
And then we'll drop into a kind
of a brief discussion of CPUs

00:31:35.200 --> 00:31:39.840
versus GPUs and some of their sort
of respective characteristics.

00:31:40.010 --> 00:31:44.280
And then I'll spend the majority
of my time talking about the

00:31:44.370 --> 00:31:46.950
GPU as an OpenCL processor.

00:31:47.200 --> 00:31:52.090
The image at the bottom of the screen
is kind of a schematic view of a series

00:31:52.090 --> 00:31:57.460
of processing clusters that are kind of
typical in the GPUs that we build today.

00:31:58.810 --> 00:32:00.530
So, you know, why is this interesting?

00:32:00.730 --> 00:32:03.800
You know, even at the outset, I mean,
this is the genesis of it.

00:32:03.800 --> 00:32:06.840
This is the beginning of
OpenCL as a computing platform.

00:32:06.840 --> 00:32:15.920
But it happens that OpenCL is
supported on all shipping NVIDIA GPUs.

00:32:15.980 --> 00:32:21.140
Okay, that means that any GPU that ships
today is capable of supporting OpenCL.

00:32:21.140 --> 00:32:23.230
Okay, so that's nice.

00:32:23.230 --> 00:32:26.940
But in fact,
this has been the case for a while now.

00:32:27.040 --> 00:32:31.990
There are 70 million OpenCL capable
GPUs already out there in the

00:32:31.990 --> 00:32:34.440
marketplace in people's hands.

00:32:34.440 --> 00:32:37.130
And that number is going up
at about 2 million a week.

00:32:37.200 --> 00:32:42.970
So there's a non-trivial,
possibly a non-trivial user base.

00:32:46.000 --> 00:32:49.580
Now as far as the Mac platform
or the Apple platform,

00:32:49.580 --> 00:32:57.860
these are the current products that
have OpenCL capable GPUs in them.

00:32:58.060 --> 00:33:03.380
from relatively high-end,
very high-end GPUs down to kind

00:33:03.380 --> 00:33:06.890
of entry-level integrated systems.

00:33:08.550 --> 00:33:13.580
Okay, so what about GPUs and CPUs?

00:33:14.580 --> 00:33:21.620
The 8800GT I think is actually
what's shipping in the Mac Pro.

00:33:21.630 --> 00:33:26.840
It has 112 cores and supports about
half a gigaflop and has some pretty

00:33:26.850 --> 00:33:28.500
significant memory bandwidth.

00:33:28.810 --> 00:33:36.440
If you compare that with kind
of a contemporary desktop CPU,

00:33:36.580 --> 00:33:40.880
it has two cores,
it has a little bit less than a

00:33:40.880 --> 00:33:44.240
tenth the floating point horsepower,
and rounding it has about a

00:33:44.240 --> 00:33:46.360
tenth of the memory bandwidth.

00:33:46.420 --> 00:33:53.460
So for very data parallel,
data intensive, bandwidth intensive,

00:33:53.550 --> 00:34:00.490
floating point intensive workloads,
it's not as capable a platform.

00:34:00.490 --> 00:34:04.600
Both of these support IEEE floating
point and I'll stress that,

00:34:04.700 --> 00:34:07.490
and Afi actually was
fairly clear about this,

00:34:07.490 --> 00:34:10.500
when I say IEEE floating point,
I'm referring to rounding modes.

00:34:10.500 --> 00:34:13.150
The GPUs don't yet support
floating point exceptions,

00:34:13.250 --> 00:34:16.430
those sorts of things,
so it's not the full IEEE standard.

00:34:16.500 --> 00:34:22.360
It's important to understand that
they're both programmable using OpenCL,

00:34:22.710 --> 00:34:25.340
which is actually great,
because one of the things that the

00:34:25.340 --> 00:34:33.980
CPU enables is the use of OpenCL across
all Macs that are shipping today.

00:34:35.580 --> 00:34:41.510
Okay, so what about the GPU as
an OpenCL processor?

00:34:41.900 --> 00:34:51.700
[Transcript missing]

00:34:51.980 --> 00:34:56.710
and I are going to talk about the TPCs.

00:34:56.710 --> 00:35:03.560
The original name stems
from its graphics heritage.

00:35:03.560 --> 00:35:10.420
Each TPC has a series of texture
filtering blocks and a texture L1 cache.

00:35:10.420 --> 00:35:13.610
You can see it on the diagram.

00:35:13.610 --> 00:35:20.340
And then they're further broken
down into multiple I called them

00:35:20.340 --> 00:35:23.030
thread group processors internally.

00:35:23.030 --> 00:35:26.660
In video we call them SMs which
stands for streaming multi-processor.

00:35:26.660 --> 00:35:31.410
And a thread processing cluster
can have varying numbers of

00:35:31.410 --> 00:35:33.560
these thread group processors.

00:35:34.200 --> 00:35:37.990
On the right you see a
TPC composed of two of them.

00:35:37.990 --> 00:35:41.300
On the left in sort of
smaller scale there are three.

00:35:41.300 --> 00:35:46.140
And depending on the market
segment that you're going after,

00:35:46.140 --> 00:35:50.330
one will be more
appropriate than the other.

00:35:50.800 --> 00:35:56.010
The,
now each of these SMs or thread group

00:35:56.010 --> 00:36:02.020
processors is made up of a collection
of scalar processors and there are

00:36:02.020 --> 00:36:08.100
eight per thread processing group.

00:36:08.430 --> 00:36:11.120
and David Let's see,
what else do we have here?

00:36:11.170 --> 00:36:14.060
You can see the local memory
that Afi was talking about.

00:36:14.060 --> 00:36:16.300
It's explicitly embodied
in the architecture.

00:36:16.300 --> 00:36:22.820
It's a program managed cache.

00:36:22.820 --> 00:36:22.820
We also, I think

00:36:23.010 --> 00:36:42.190
and others.

00:36:42.210 --> 00:36:42.220
I called them address qualifiers here.

00:36:42.220 --> 00:36:42.220
Basically the hardware implements
both the private address space,

00:36:42.220 --> 00:36:42.220
the local and global as well as
optimizing constant accesses.

00:36:42.450 --> 00:36:46.460
Now, when I say it implements
the private address space,

00:36:46.460 --> 00:36:51.450
there are instructions in the instruction
set that actually reference memory,

00:36:51.450 --> 00:36:54.260
and then the hardware takes care of
mapping that to memory in an efficient

00:36:54.360 --> 00:36:58.440
fashion in terms of interleaving the
memory references from all of the threads

00:36:58.440 --> 00:37:02.470
that are executing in parallel so that
the memory accesses are efficient.

00:37:02.670 --> 00:37:06.950
The same is true of
local and global memory,

00:37:06.950 --> 00:37:09.790
and I'll talk a little bit
more about that in a moment.

00:37:12.470 --> 00:37:18.620
So if you take and put
together a whole group of TPCs,

00:37:18.630 --> 00:37:19.610
you can build up a chip.

00:37:19.660 --> 00:37:25.960
And what we see here at the higher
level is a set of four TPCs.

00:37:26.340 --> 00:37:33.630
which are then set on top of a
memory subsystem that's quite wide,

00:37:33.630 --> 00:37:37.540
which is how we achieve the remarkable
memory bandwidth the system supports.

00:37:37.600 --> 00:37:43.600
And you can also see on the left side in
gray this little block that says atomic.

00:37:43.600 --> 00:37:48.590
So the language supports
atomic operations on memory.

00:37:48.600 --> 00:37:51.800
And we implement those in what
we call fixed function hardware,

00:37:51.920 --> 00:37:54.000
which sits very,
very close to the memory.

00:37:54.000 --> 00:37:56.340
So that the read-modify-write
of the atomic operation

00:37:56.410 --> 00:37:59.350
can be extremely efficient.

00:38:00.780 --> 00:38:04.700
It will also note that
all of the bulk of the,

00:38:04.700 --> 00:38:06.940
let's see, how do I describe this?

00:38:06.960 --> 00:38:11.520
The host CPU is connected
to the GPU via PCIe,

00:38:11.810 --> 00:38:15.500
but everything else in
this diagram is on die,

00:38:15.540 --> 00:38:19.830
with the exception of perhaps
multiple gigabytes of memory.

00:38:21.720 --> 00:38:27.880
So given that,
we can build a variety of different

00:38:27.880 --> 00:38:33.190
platforms by incorporating different
numbers of TPCs and different

00:38:33.190 --> 00:38:34.600
numbers of memory partitions.

00:38:34.600 --> 00:38:36.680
So you can scale the
bandwidth of a product,

00:38:36.710 --> 00:38:39.980
you can scale the compute capabilities
of the product depending on

00:38:39.990 --> 00:38:42.710
your point in the marketplace.

00:38:42.760 --> 00:38:48.450
So going back to the Mac configurations
that we were talking about earlier,

00:38:49.000 --> 00:38:55.670
We have ranging from
130 gigaflops up to 504.

00:38:55.740 --> 00:39:00.440
And you can see the varying numbers
of SPs in these different products.

00:39:00.610 --> 00:39:05.920
These are all fairly powerful machines,
but there's still a wide range of

00:39:06.050 --> 00:39:08.480
performance from the bottom to the top.

00:39:10.800 --> 00:39:13.120
Okay, so what's, you know,
I talked a little bit about

00:39:13.210 --> 00:39:16.220
the kinds of things that we've
implemented in the GPU in terms of

00:39:16.280 --> 00:39:18.240
providing direct support for OpenCL.

00:39:18.240 --> 00:39:21.940
What other sort of attributes are there?

00:39:21.940 --> 00:39:23.040
What's interesting?

00:39:23.600 --> 00:39:28.050
Well, one of the really critical and
extremely valuable attributes

00:39:28.120 --> 00:39:32.840
of OpenCL and our architecture,
frankly, is that there's a transparent

00:39:32.950 --> 00:39:36.920
scaling in terms of performance
across the product lines.

00:39:37.000 --> 00:39:41.700
You can take the same piece of
code and run it efficiently on

00:39:41.800 --> 00:39:44.350
a single TPC low-end system.

00:39:44.350 --> 00:39:49.190
That's what this 8SPM GPU,
we call the motherboard GPUs,

00:39:49.190 --> 00:39:51.980
a relatively low-end machine.

00:39:51.980 --> 00:39:53.580
You can take that same piece of code.

00:39:53.600 --> 00:40:00.200
And run it on even conceivably a dual
GPU system that has 256 processors and

00:40:00.200 --> 00:40:03.990
supports about a teraflop of performance.

00:40:04.000 --> 00:40:06.280
And the code will just run faster.

00:40:06.280 --> 00:40:08.700
I mean, it isn't, you know,
perfectly magical.

00:40:08.700 --> 00:40:11.820
I mean,
there are some limitations to scaling.

00:40:11.820 --> 00:40:15.530
But one of the wonderful things
about this is when we ship a new

00:40:15.530 --> 00:40:21.480
product that has higher performance,
has more cores, your application,

00:40:22.140 --> 00:40:24.480
assuming that at least some of
your application developers,

00:40:24.480 --> 00:40:27.050
just runs faster without
any effort on your part.

00:40:28.400 --> 00:40:34.260
Another attribute,
actually flipping back to this for

00:40:34.260 --> 00:40:40.080
just a second to clarify or kind of
illustrate why this comes about is,

00:40:40.230 --> 00:40:43.710
Afi had mentioned the thread group.

00:40:43.950 --> 00:40:47.100
So the thread group is the unit
of execution that's guaranteed

00:40:47.250 --> 00:40:49.130
to execute at any given instant.

00:40:49.130 --> 00:40:53.290
And those thread groups run on
these thread group processors.

00:40:53.480 --> 00:40:56.270
And you might have a machine with
a single thread group processor

00:40:56.470 --> 00:41:01.400
or you might have a machine with,
say, 32 or 64 thread group processors.

00:41:01.400 --> 00:41:03.400
Because there's no dependency
among thread groups,

00:41:03.400 --> 00:41:07.490
we can run them serially or we can
run them all in parallel and that's

00:41:07.560 --> 00:41:09.400
where this scaling comes from.

00:41:09.400 --> 00:41:10.400
Okay.

00:41:10.400 --> 00:41:16.400
Another attribute on the GPU side is
that we support thousands of threads.

00:41:16.400 --> 00:41:19.400
And I mean it very literally,
thousands of threads.

00:41:19.700 --> 00:41:24.400
Now, what is that, you know,
what is that bias?

00:41:24.430 --> 00:41:30.710
Well, you can use a very,
very sort of deep workload to hide

00:41:30.710 --> 00:41:32.400
the latency of compute operations.

00:41:32.430 --> 00:41:40.400
For example, if you perform some costly,
say you perform a sign function

00:41:40.400 --> 00:41:46.000
or a multiply add that has maybe
10 or 20 clocks of latency.

00:41:46.520 --> 00:41:47.400
and others.

00:41:47.400 --> 00:41:49.210
While that is grinding
through the machine,

00:41:49.320 --> 00:41:53.890
I can run ten other threads to hide
the latency of that operation so that

00:41:53.890 --> 00:41:59.840
we never have trouble with dependent
operations on the GPU or rarely do.

00:41:59.840 --> 00:42:02.260
The same holds for memory accesses.

00:42:02.260 --> 00:42:06.900
As Afi mentioned,
in current shipping GPUs,

00:42:06.900 --> 00:42:08.840
there are few memory caches.

00:42:08.840 --> 00:42:13.940
There is a memory cache that sits behind
the constant accesses and I'll talk a

00:42:13.940 --> 00:42:15.580
little bit more about that in a moment.

00:42:16.670 --> 00:42:22.340
But accesses to global memory
go all the way out to the DRAMs.

00:42:22.360 --> 00:42:27.650
And it is by having hundreds and
literally thousands of threads that we

00:42:27.650 --> 00:42:31.130
can hide the latency of memory access.

00:42:31.310 --> 00:42:35.990
So, okay, thousands of threads,
that's great.

00:42:36.800 --> 00:42:40.700
One of the things that we've done
in the design of our processors

00:42:40.700 --> 00:42:44.100
is to make sure that the threads
are extremely lightweight.

00:42:44.100 --> 00:42:46.830
You can imagine that if you
have a thousand threads or

00:42:46.850 --> 00:42:50.700
tens of thousands of threads,
the threads have to be very lightweight.

00:42:50.700 --> 00:42:52.700
The threads are all managed in hardware.

00:42:52.700 --> 00:42:56.270
They're created, scheduled,
and even the synchronization

00:42:56.270 --> 00:42:59.650
operations among threads
are all managed in hardware.

00:42:59.750 --> 00:43:04.310
So there's no visible overhead
to the application for these

00:43:04.360 --> 00:43:07.610
very large numbers of threads.

00:43:08.870 --> 00:43:10.650
Okay.

00:43:10.810 --> 00:43:16.040
Now the hardware also, you know,
in terms of the way we design hardware,

00:43:16.040 --> 00:43:19.830
we look at the kind of typical operations
that are being performed by applications

00:43:20.710 --> 00:43:25.360
and optimize the hardware to make
those operations as fast as possible.

00:43:25.490 --> 00:43:32.410
So one of the things that
you often see in a program,

00:43:32.580 --> 00:43:33.430
a kernel, as Afi was describing, is that

00:43:33.590 --> 00:43:36.200
All of the threads that are
executing along are often,

00:43:36.210 --> 00:43:39.040
in fact, the vast majority of the time,
they're at the same program counter.

00:43:39.040 --> 00:43:41.900
They're executing the same instruction.

00:43:42.070 --> 00:43:45.400
So we optimize that.

00:43:45.400 --> 00:43:45.400
If a

00:43:46.290 --> 00:43:50.920
If a wavefront of SPs all reads
from the same memory location,

00:43:50.920 --> 00:43:53.190
all reads the same instruction,
it does it in a single clock.

00:43:53.200 --> 00:43:57.340
If, on the other hand,
some of the threads within that

00:43:57.340 --> 00:44:01.420
wavefront take the if path and some
of the threads take the else path,

00:44:01.420 --> 00:44:03.200
it takes two clocks.

00:44:03.200 --> 00:44:07.130
Or it takes two
instruction memory fetches.

00:44:07.200 --> 00:44:13.060
So we make the common case go
extremely fast and we make the

00:44:13.060 --> 00:44:15.200
other cases go as fast as possible.

00:44:15.200 --> 00:44:18.440
We're sort of providing for
the right trade-off in terms of

00:44:18.440 --> 00:44:20.200
the instruction cache design.

00:44:20.550 --> 00:44:23.650
Because ports into an instruction
cache are expensive and the

00:44:23.710 --> 00:44:27.990
caches tend to grow roughly
linearly with the number of ports.

00:44:28.120 --> 00:44:35.760
We also optimize the accesses
to the local memory as well as

00:44:35.760 --> 00:44:38.850
global memory in the sense that

00:44:39.800 --> 00:44:44.800
Typically,
threads will access a series of

00:44:44.800 --> 00:44:46.670
consecutive locations in memory.

00:44:46.670 --> 00:44:49.730
So what the hardware does is it
coalesces all of those requests

00:44:49.730 --> 00:44:51.720
into sort of one cache line fetch.

00:44:51.800 --> 00:44:56.670
And if, because of your algorithm,
it has slightly less structure,

00:44:56.670 --> 00:45:00.670
if they access multiple cache lines,
we take exactly as many clocks as

00:45:00.740 --> 00:45:02.800
there are cache lines referenced.

00:45:02.800 --> 00:45:09.800
And the hardware takes care
of managing all of that.

00:45:09.800 --> 00:45:11.060
So there's no need for the
programmer to deal with creating

00:45:11.060 --> 00:45:13.970
masks to perform a vector load but
mask off the elements that they

00:45:13.970 --> 00:45:14.800
don't want to actually be loaded.

00:45:14.830 --> 00:45:20.730
So none of that overhead in terms of
lane management falls to the programmer.

00:45:20.740 --> 00:45:24.690
So not only do you not
have to program it,

00:45:24.840 --> 00:45:27.800
but it's also dramatically
more efficient.

00:45:27.800 --> 00:45:31.800
Now, as far as the constant accesses.

00:45:31.800 --> 00:45:36.720
Afi had mentioned that it's very
beneficial to identify regions

00:45:36.720 --> 00:45:41.050
of memory that are read-only and
also regions of memory that are

00:45:41.050 --> 00:45:47.790
dominated by a large number of threads
accessing the same memory location.

00:45:47.800 --> 00:45:53.900
It happens that in graphics hardware,
the heritage of the constant

00:45:53.900 --> 00:45:59.810
cache comes from the transform
and lighting engines of like eight

00:45:59.810 --> 00:46:03.710
years ago because in those engines,
there was this chunk of memory

00:46:03.710 --> 00:46:07.380
which held matrices for transforming
vertices and every thread that

00:46:07.390 --> 00:46:10.670
was executing was going to access
the same coefficient every clock.

00:46:10.800 --> 00:46:15.410
It happens that that kind of
characteristic memory access pattern

00:46:15.410 --> 00:46:20.780
shows up in memory image processing
operations but in actually a very,

00:46:20.810 --> 00:46:22.800
very wide variety of compute loads.

00:46:22.800 --> 00:46:24.800
So what we do is optimize for that.

00:46:24.800 --> 00:46:29.800
However, if there's some renegade
thread within a wave front,

00:46:29.800 --> 00:46:33.800
a group of threads that are
executing all along at the same time,

00:46:33.800 --> 00:46:37.950
if it goes off and reads
from some other location,

00:46:37.950 --> 00:46:40.800
the hardware manages that gracefully.

00:46:40.860 --> 00:46:46.730
It simply takes another clock, typically,
to read this other memory location.

00:46:46.800 --> 00:46:51.800
But again, it's all managed in hardware
on behalf of the application,

00:46:51.800 --> 00:46:58.800
so there's no additional compiler
or programming necessary to

00:46:58.800 --> 00:46:58.800
get the programming running.

00:46:58.800 --> 00:47:01.620
to get the performance there.

00:47:01.960 --> 00:47:07.370
And then finally, obviously we have huge
amounts of bandwidth.

00:47:07.370 --> 00:47:14.500
And we can stream data from the
DRAM onto the chip into local memories,

00:47:14.500 --> 00:47:19.010
into registers,
and operate on it at a sustained

00:47:19.010 --> 00:47:27.170
rate such that you can keep all of
the compute units fully occupied

00:47:27.170 --> 00:47:27.750
during the execution of a kernel.

00:47:29.400 --> 00:47:34.700
[Transcript missing]

00:47:35.080 --> 00:47:40.290
You should understand that our GPUs
in OpenCL were designed together.

00:47:40.310 --> 00:47:48.760
I've spent a great deal of time at Apple,
literally a couple of hours a week.

00:47:48.860 --> 00:47:51.430
Fortunately,
they're not too far from campus.

00:47:51.450 --> 00:47:54.470
Our campus, that is.

00:47:54.940 --> 00:47:57.240
I think it should be apparent,
if you haven't figured this out already,

00:47:57.240 --> 00:48:01.550
that the GPUs are a tremendous
platform for running OpenCL.

00:48:01.560 --> 00:48:03.920
They're extremely efficient.

00:48:03.920 --> 00:48:08.210
If the workload is appropriate,
you get great performance.

00:48:08.480 --> 00:48:11.080
and you get,
in terms of absolute performance,

00:48:11.080 --> 00:48:14.270
as you move up the product line,
you get sort of rewarded for

00:48:14.370 --> 00:48:17.900
moving up the product line
with even better performance.

00:48:18.070 --> 00:48:20.900
Also, in terms of efficiency,
and this is,

00:48:21.110 --> 00:48:23.400
depending on what environments
you're working in,

00:48:23.400 --> 00:48:25.080
a really critical issue.

00:48:25.170 --> 00:48:28.730
They have, they're extremely efficient
from the perspective of

00:48:28.730 --> 00:48:30.310
what we call perf per watt.

00:48:30.440 --> 00:48:35.320
One of our design criteria when
building GPUs is to optimize the

00:48:35.620 --> 00:48:42.700
performance delivered against
the power consumed because,

00:48:42.700 --> 00:48:42.700
you know, if you,

00:48:42.740 --> 00:48:45.290
If you have a laptop,
you don't want it to get too hot.

00:48:45.380 --> 00:48:47.480
You need to be able to cool
the devices that are inside it.

00:48:47.760 --> 00:48:52.600
And it turns out that power consumption,
battery life, thermal dissipation,

00:48:52.600 --> 00:48:54.340
all of these things kind of
determine what you can get

00:48:54.360 --> 00:48:56.800
away with in a given platform.

00:48:56.800 --> 00:49:04.800
And then finally, to revisit the open
standard aspect of OpenCL,

00:49:04.800 --> 00:49:13.350
it will be supportable
across all of our products.

00:49:13.360 --> 00:49:17.420
So the pictures across the bottom of
the slide kind of are exemplary of

00:49:17.420 --> 00:49:19.900
the various product lines from NVIDIA.

00:49:19.900 --> 00:49:24.350
And we plan to continue to support,
to continue to ship

00:49:24.350 --> 00:49:26.800
OpenCL capable machines.

00:49:26.800 --> 00:49:28.800
And I, that's it.

00:49:28.800 --> 00:49:32.800
Our, that's all I had today.

00:49:32.800 --> 00:49:34.790
Our next speaker is Kevin.

00:49:34.800 --> 00:49:39.970
Kevin Quehnesson: Thank you.

00:49:39.970 --> 00:49:39.970
Thanks.

00:49:43.340 --> 00:49:44.570
So hi and thanks.

00:49:44.750 --> 00:49:47.900
So you learn about OpenCL,
it's a very cool and

00:49:47.900 --> 00:49:50.000
very exciting technology.

00:49:50.000 --> 00:49:52.610
So you might want to do your
OpenCL development in your usual

00:49:52.610 --> 00:49:57.540
development environment such as
Xcode or whatever you would prefer.

00:49:57.540 --> 00:49:59.340
But there's this great tool on the U.S.

00:49:59.340 --> 00:50:02.620
called Quartz Composer that can help you.

00:50:02.620 --> 00:50:07.400
So Quartz Composer basically can be
seen as a visual programming environment

00:50:07.400 --> 00:50:12.000
that -- so it's pretty much a graph
that organizes a processing flow

00:50:12.000 --> 00:50:16.500
where each of the nodes of this graph
is a patch that takes some inputs,

00:50:16.500 --> 00:50:19.770
do some processing and
returns some outputs.

00:50:19.980 --> 00:50:23.820
and so Quartz Composer is used for
doing graphics animation or image

00:50:23.820 --> 00:50:29.490
processing or compositing pipelines
and also data visualization.

00:50:29.730 --> 00:50:32.790
But the great thing about Quark's
Composer is that most of the time these

00:50:32.790 --> 00:50:35.450
things are done with no code at all.

00:50:35.820 --> 00:50:41.300
and so for those of you
OpenCL developer who wants to know,

00:50:41.300 --> 00:50:43.760
learn about OpenCL,
learn about a language and discover

00:50:43.760 --> 00:50:48.100
that or go further or explore
what OpenCL can do for you,

00:50:48.440 --> 00:50:52.940
CrossComposer is a great place
to look at because it will handle

00:50:53.340 --> 00:50:55.700
all what you don't want to handle.

00:50:55.700 --> 00:50:57.700
For instance,
OpenGL attachments for drawing

00:50:57.700 --> 00:51:03.200
or vertex buffer object texture,
texture caching for OpenGL context

00:51:03.300 --> 00:51:08.290
and also the OpenCL program setups
and device setups and so on.

00:51:08.440 --> 00:51:11.500
So you just go inside CrossComposer
and you have an application

00:51:11.500 --> 00:51:16.000
where you can directly,
simply go and type your kernel

00:51:16.000 --> 00:51:22.540
code and start playing and
interfacing with other patches.

00:51:23.840 --> 00:51:27.480
So another great thing
about Cross Composer is

00:51:27.480 --> 00:51:31.170
that it integrates all the,
most of the OS technologies.

00:51:31.350 --> 00:51:37.260
So for instance, QuickTime, CoreImage,
CoreMedia, CoreAudio and so on.

00:51:37.340 --> 00:51:41.590
So you can use and leverage
these OS technologies in

00:51:41.810 --> 00:51:44.530
your OpenCL development,
for instance,

00:51:44.590 --> 00:51:50.300
as inputs to your kernels or
as environments to visualize or

00:51:50.300 --> 00:51:56.110
to better explore and present
the result of a simulation.

00:51:56.500 --> 00:51:59.830
So Quast Composer adds OpenCL support.

00:51:59.830 --> 00:52:03.060
And so what it means is we
have an OpenCL kernel patch

00:52:03.170 --> 00:52:06.390
that's simple as just one patch,
so one of these processing nodes

00:52:06.390 --> 00:52:12.530
that's... And in this patch you can
write OpenCL kernel and Quast Composer

00:52:12.530 --> 00:52:17.330
will automatically parse this kernel,
find its argument and understand

00:52:17.660 --> 00:52:19.810
what are the input ports,
the output ports,

00:52:19.920 --> 00:52:24.370
and automatically populates the
patch so that you can have some

00:52:24.450 --> 00:52:29.850
abstract processing units that you
can insert in some processing flows

00:52:29.940 --> 00:52:33.580
and some Quast Composer composition.

00:52:33.780 --> 00:52:34.700
and so that's very easy.

00:52:34.700 --> 00:52:37.050
You open Cross Composer,
you create an OpenCL kernel

00:52:37.050 --> 00:52:40.220
patch and right there,
without having to set up anything else,

00:52:40.300 --> 00:52:45.680
you can start doing some OpenCL and start
leveraging Cross Composer to visualize

00:52:45.680 --> 00:52:49.290
the results to eventually go pretty far.

00:52:49.300 --> 00:52:53.210
So we'll show that in a demo in a second.

00:52:53.440 --> 00:52:56.660
So one other great advantage
of Cross Composer is that

00:52:56.660 --> 00:52:58.300
everything is at runtime.

00:52:58.410 --> 00:53:01.580
So you don't need to build and
run whenever you make a change in

00:53:01.670 --> 00:53:03.490
the kernel or change somewhere.

00:53:03.890 --> 00:53:08.720
You can simply go in the setting
of the badge and inspect the

00:53:08.720 --> 00:53:12.700
kernel code and as you change,
as you type in the kernel,

00:53:12.810 --> 00:53:16.600
Cross Composer will automatically call
CL to recompile the program in the

00:53:16.600 --> 00:53:20.620
background so that your simulation or
whatever processing in OpenCL you're

00:53:20.620 --> 00:53:24.540
doing will pick up the change at the
simulation or where the processing goes.

00:53:24.540 --> 00:53:30.540
So I'm going to demonstrate that
right now on the demo machine.

00:53:36.460 --> 00:53:39.400
So here I'm going to open
in-course composer here.

00:53:39.400 --> 00:53:46.860
So this N-Body demo that you've
been seeing previously in the show.

00:53:46.910 --> 00:53:51.300
So you have 16,000 particles, 16,384.

00:53:51.300 --> 00:53:54.250
And here are the composition look.

00:53:54.890 --> 00:53:57.320
So you have the antibody kernel
which is here and simply the

00:53:57.320 --> 00:54:01.800
result which is the position,
the output of that kernel is

00:54:01.800 --> 00:54:06.600
passed into a renderer that
will simply render that thing.

00:54:06.650 --> 00:54:10.590
Render the result of
the OpenCL simulation.

00:54:10.600 --> 00:54:13.560
And that's as simple as it can be,
as simple as it is.

00:54:13.610 --> 00:54:16.650
And Quartz Composer also provides
you with patch to interact with

00:54:16.830 --> 00:54:20.700
the visualization so you can very
easily do some rotation like that.

00:54:20.770 --> 00:54:25.320
And so let me show you now the
settings of that OpenCL kernel.

00:54:27.520 --> 00:54:32.190
So here I have all the code of the
kernel that is a copy and paste of the

00:54:32.190 --> 00:54:34.400
OpenCL kernel from all the other demos.

00:54:34.470 --> 00:54:35.800
So there's no translation to do.

00:54:36.000 --> 00:54:38.400
Just paste that stuff in and it works.

00:54:38.660 --> 00:54:42.350
And here I have a line that I commented
out that simply takes the time step

00:54:42.440 --> 00:54:45.400
and multiplies it by a constant,
which is five.

00:54:45.400 --> 00:54:50.180
So I can just uncomment that line
and immediately the simulation

00:54:50.180 --> 00:54:52.400
at the current time picks that
up and goes five times faster.

00:54:52.450 --> 00:54:57.400
So if I can comment that line up and
so we're back to the previous time.

00:54:57.400 --> 00:55:02.400
So it's very easy as you go to change the
kernel as it goes and to see the results.

00:55:02.400 --> 00:55:06.010
It's a very nice way without having
to build and run to iterate really

00:55:06.100 --> 00:55:11.400
fast and to go to where you want to
go and probably further much faster.

00:55:12.130 --> 00:55:15.200
So let me start the simulation again.

00:55:15.200 --> 00:55:19.300
And show you other power of
CrossComposer is also that you have

00:55:19.300 --> 00:55:22.240
access to all the CrossComposer
pipeline and other possibilities.

00:55:22.660 --> 00:55:25.420
For instance, you can create very easily
this little UI that allows

00:55:25.530 --> 00:55:28.390
you to change some parameters,
but you can also leverage

00:55:28.460 --> 00:55:30.840
the CrossComposer pipeline to
create some interesting effects.

00:55:30.850 --> 00:55:37.340
And so for instance here,
I'm going to create a trail of all this.

00:55:37.810 --> 00:55:40.800
of all these particles over time.

00:55:40.870 --> 00:55:45.890
So here you can see in
three dimension over 200,

00:55:46.400 --> 00:55:50.300
so, n-body simulations stacked over
time and composited together.

00:55:50.300 --> 00:55:52.320
So, Quast Composer is going
to handle everything,

00:55:52.320 --> 00:55:54.810
the vertex buffer object
caching and the display,

00:55:54.810 --> 00:55:58.680
so you don't have to
worry about anything.

00:55:58.680 --> 00:56:04.480
So, you know, it's -- so let me put that
maybe later so you can -- oh.

00:56:04.700 --> 00:56:08.750
So it's a very nice way to
leverage Quartz Composer to

00:56:08.760 --> 00:56:10.600
go in some other direction.

00:56:10.600 --> 00:56:13.900
So how it works is to
do that motion blur,

00:56:13.900 --> 00:56:17.890
it's a simple, here I go,
and it's a simple queue patch that will

00:56:18.060 --> 00:56:22.600
stack up 200 vertex buffer objects,
so 200 results of that simulation,

00:56:22.600 --> 00:56:25.090
and then an iterator,
which is a patch that will

00:56:25.490 --> 00:56:29.600
render what's contained within
that queue for all its elements.

00:56:29.600 --> 00:56:30.910
So all the elements
contained in that queue,

00:56:30.910 --> 00:56:31.570
so 200 times.

00:56:31.650 --> 00:56:35.600
So here you have, so, or 90 times.

00:56:35.600 --> 00:56:39.060
So here, for instance,
you have 200 vertex buffer objects

00:56:39.640 --> 00:56:45.600
of 16,000 particles drawn in front of
you by Quartz Composer in real time.

00:56:45.600 --> 00:56:52.720
So let me go back to the slide, please.

00:57:01.000 --> 00:57:02.900
Can I go back to slides, please?

00:57:02.900 --> 00:57:02.900
Thanks.

00:57:02.900 --> 00:57:06.900
So more details on the
OpenCL kernel patch.

00:57:06.900 --> 00:57:10.900
So by default,
the OpenCL kernel patch is going

00:57:10.900 --> 00:57:11.900
to handle everything automatically.

00:57:11.910 --> 00:57:15.900
So it's going to find out the outputs and
input ports from the kernel prototype.

00:57:15.900 --> 00:57:18.900
It's going to figure out the thread
dimensions and the output dimensions,

00:57:18.900 --> 00:57:21.900
so the dimensions of
the output arguments.

00:57:21.990 --> 00:57:23.900
But you have the option
to override everything.

00:57:23.900 --> 00:57:27.000
So you have the option in the settings
to override the local thread dimension,

00:57:27.000 --> 00:57:27.750
all those things.

00:57:27.900 --> 00:57:32.640
So by default, it's very simple,
but you can give all the

00:57:32.640 --> 00:57:34.900
customization that you want.

00:57:34.900 --> 00:57:37.890
So yet very powerful.

00:57:37.950 --> 00:57:41.610
So we really want Chorus
Composer to be a very,

00:57:41.780 --> 00:57:44.900
very good OpenCL development tool.

00:57:44.940 --> 00:57:48.900
And so we added to the Chorus Composer
editor itself new functionalities

00:57:48.900 --> 00:57:51.900
to make that support really optimal.

00:57:51.900 --> 00:57:55.370
So in particular,
we added this ability to define a

00:57:55.370 --> 00:57:57.860
global OpenCL kernel-- OpenCL kernel.

00:57:57.950 --> 00:57:59.880
OpenCL program, sorry.

00:57:59.930 --> 00:58:02.690
So what it means is that--so if you
have a kernel patch and you write an

00:58:02.690 --> 00:58:07.170
OpenCL kernel within that kernel patch,
this code and this kernel will

00:58:07.250 --> 00:58:09.900
be accessed by the patch itself.

00:58:09.900 --> 00:58:12.900
But using this global code view,
you can write an OpenCL program,

00:58:12.900 --> 00:58:16.450
so maybe multiple kernels,
and these kernels will be accessible

00:58:16.550 --> 00:58:18.900
by every patch in the composition,
so will be shared.

00:58:19.010 --> 00:58:20.900
So if you change the
kernel in the program,

00:58:20.900 --> 00:58:24.900
it will be reflected
automatically to all the patches.

00:58:24.900 --> 00:58:27.760
So that's a very nice way to have
a centralized-- a centralized

00:58:27.830 --> 00:58:29.900
location for OpenCL development.

00:58:30.110 --> 00:58:37.200
And so it allows you also to bring
in any sort of OpenCL code within

00:58:37.200 --> 00:58:40.780
Chorus Composer and start using it
and leveraging in the composition.

00:58:42.580 --> 00:58:47.800
So another thing we added
to Cross Composer is a new

00:58:47.800 --> 00:58:52.730
patch specifically targeted
at OpenCL simulations.

00:58:53.030 --> 00:58:58.390
So a simulation pretty much means
that I have a kernel that iterates

00:58:58.390 --> 00:58:59.950
over time over the same objects.

00:58:59.960 --> 00:59:01.500
For instance, position and velocity.

00:59:01.510 --> 00:59:03.940
So in the NBody case, that's what we do.

00:59:03.940 --> 00:59:05.100
We have position and velocity.

00:59:05.300 --> 00:59:10.410
Each time we use the NBody kernels
to update that position and velocity.

00:59:10.520 --> 00:59:11.500
So each time step.

00:59:11.720 --> 00:59:15.450
But CrossComposer only have access
to a slice of that time step,

00:59:15.560 --> 00:59:17.410
to a patch at a given time.

00:59:17.510 --> 00:59:22.100
And so what we need is that so that
this patch can have at the next time

00:59:22.100 --> 00:59:26.500
step in its inputs the result of this
same patch at the previous time step.

00:59:26.500 --> 00:59:29.500
So at the previous execution
of the composition.

00:59:29.720 --> 00:59:34.510
So we added a very powerful
feedback patch that allows you

00:59:34.510 --> 00:59:36.480
to do that sort of feedback.

00:59:36.500 --> 00:59:40.500
And so the feedback patch at
t=0 is simply a pass-through.

00:59:40.500 --> 00:59:42.700
So we'll pass the arguments
coming on the left,

00:59:42.700 --> 00:59:47.500
so on the input ports to the output ports
and pass it to the current output port.

00:59:47.500 --> 00:59:48.430
So we have a kernel.

00:59:48.630 --> 00:59:52.670
But at t different from 0,
so at t bigger than 0,

00:59:52.670 --> 00:59:57.500
it will feedback the previous
outputs on the inputs of the patch.

00:59:57.620 --> 01:00:00.500
So that allows you to do this
sort of evolution over time.

01:00:00.500 --> 01:00:02.500
And it's very powerful.

01:00:02.500 --> 01:00:07.530
It can also be used in iterators if you
want to iterate the same simulation or

01:00:07.660 --> 01:00:10.400
the same processing at a given time step.

01:00:10.500 --> 01:00:13.500
And so that allows you
to do a lot of things.

01:00:13.500 --> 01:00:15.990
And so I'm going to show you
an example of the sort of

01:00:16.030 --> 01:00:17.500
thing that we can do with that.

01:00:17.600 --> 01:00:19.490
feedback badge.

01:00:34.300 --> 01:00:37.620
So here I have a cloth.

01:00:37.710 --> 01:00:38.950
So that's as simple as it is.

01:00:38.960 --> 01:00:41.700
We use the feedback patch to
do some cloth simulation using

01:00:41.700 --> 01:00:44.200
leveraging OpenCL in Quartz Composer.

01:00:44.200 --> 01:00:46.250
And so you can play
with the cloth around,

01:00:46.250 --> 01:00:47.040
you can move it.

01:00:47.190 --> 01:00:51.200
It's kind of fun, it's kind of elastic,
but it's really nice.

01:00:51.200 --> 01:00:55.030
So you can do all that stuff.

01:00:55.200 --> 01:00:59.220
So the feedback is there used to
propagate the result over time,

01:00:59.340 --> 01:01:03.200
but also at a given time step to iterate
the simulation over all the springs.

01:01:03.200 --> 01:01:06.960
So here you have a set of points,
so there are springs.

01:01:07.200 --> 01:01:10.400
And so to enforce these springs,
we use the feedback patch to

01:01:10.400 --> 01:01:12.200
propagate over all the cloth.

01:01:12.200 --> 01:01:14.190
So you can have fun.

01:01:14.200 --> 01:01:17.310
So to import that image,
it's as simple as an image source

01:01:17.310 --> 01:01:19.200
patch that will bring in the image.

01:01:19.200 --> 01:01:22.180
And so you can simply put that,
so here's the composition,

01:01:22.210 --> 01:01:27.030
as an image port on the
mesh and it's automatically

01:01:27.230 --> 01:01:28.200
textured by Quartz Composer.

01:01:28.290 --> 01:01:30.200
So in that composition,
just like in the N-body,

01:01:30.200 --> 01:01:32.200
there's no code beyond the OpenCL.

01:01:32.200 --> 01:01:34.790
the OpenCL code.

01:01:35.690 --> 01:01:38.930
So here I can play because Quartz
Composer is integrated within the OS.

01:01:38.980 --> 01:01:41.990
I can, for instance,
apply a core image effect on that

01:01:42.090 --> 01:01:46.580
texture and it's composited and
everything very easily and you see

01:01:46.800 --> 01:01:50.190
the alpha blending on the side.

01:01:57.800 --> 01:02:02.110
Okay, and so to sum up,
so Quartz Composer is great to help

01:02:02.110 --> 01:02:06.910
you visualize the result of an OpenCL,
OpenCL simulation, OpenCL kernel,

01:02:06.910 --> 01:02:07.700
anything.

01:02:07.700 --> 01:02:09.700
It's great to help you interact
with that visualization.

01:02:09.720 --> 01:02:14.640
And as I showed you in the composition,
it's also great to understand

01:02:15.040 --> 01:02:18.260
the different parts,
the different processing

01:02:18.260 --> 01:02:19.700
parts of what's going on.

01:02:19.700 --> 01:02:23.330
So, for instance,
you have the optimization step,

01:02:23.330 --> 01:02:27.700
you have the flow, you have the--here the
position are updated and so on.

01:02:27.700 --> 01:02:31.630
So you can understand what comes next
because of that graphical representation.

01:02:31.710 --> 01:02:33.700
So the things are organized.

01:02:33.700 --> 01:02:37.700
It's also great to help you communicate
your processing to other people.

01:02:37.700 --> 01:02:40.130
You can also write notes in
the graph that allows you

01:02:40.130 --> 01:02:41.700
to document what's going on.

01:02:41.700 --> 01:02:46.700
So Quartz Composer is great to
leverage OS technology with no code.

01:02:46.730 --> 01:02:49.560
So, as I demonstrate to you.

01:02:49.700 --> 01:02:52.660
And so Quartz Composer
integrates technologies,

01:02:52.710 --> 01:02:54.690
but it's also integrated in the OS.

01:02:54.870 --> 01:02:57.700
It's integrated in Cocoa using QC view.

01:02:57.700 --> 01:03:01.320
So you can take that composition,
put it in Xcode and have an

01:03:01.320 --> 01:03:04.700
application out of it with,
again, with no code.

01:03:04.700 --> 01:03:08.160
It's also integrated
within Core Animation using

01:03:08.160 --> 01:03:09.700
QC composition layer.

01:03:09.980 --> 01:03:14.690
And it can also be used as a pure
renderer in your application.

01:03:14.700 --> 01:03:19.700
So without even any OpenGL,
pure OpenCL processing pipeline.

01:03:19.750 --> 01:03:23.700
So you can use it in applications to
take some data and to return some data,

01:03:23.700 --> 01:03:27.690
leveraging OpenCL while using
Quartz Composer to create

01:03:27.780 --> 01:03:30.690
this graphical workflow.

01:03:30.950 --> 01:03:36.500
And so the message is really that
Cross Composer can be this bridge that

01:03:36.500 --> 01:03:43.500
takes OpenCL and brings it to new limits
and also that integrates it in new areas,

01:03:43.590 --> 01:03:46.700
for instance,
applications or for instance some

01:03:46.700 --> 01:03:50.800
very new possibilities there.

01:03:50.920 --> 01:03:57.800
So the Cross Composer is
in the C so you can try it.

01:03:57.800 --> 01:03:59.150
You can try the NBody example.

01:03:59.150 --> 01:04:04.970
And we are having a session this
afternoon that will show you some

01:04:05.080 --> 01:04:08.800
very nice things of the new things we
added in Cross Composer for the first

01:04:08.800 --> 01:04:10.800
time in particular interactivity.

01:04:10.800 --> 01:04:13.800
So in a very short amount of
time we're going to create some

01:04:13.800 --> 01:04:15.800
interactive OpenCL for the browser.

01:04:16.000 --> 01:04:19.800
And I'm going to show you an
example of what the kind of

01:04:19.800 --> 01:04:20.800
synergy is between the two.

01:04:20.800 --> 01:04:23.620
So this is a little bit of a demo
of what the cross-composer can

01:04:23.620 --> 01:04:27.100
bring while bringing OpenCL into
other area in particular with this

01:04:27.100 --> 01:04:31.720
ability to integrate by going to
a last demo to the demo machine.

01:04:44.980 --> 01:04:50.680
So here I simply took this demo we're
going to bring this afternoon and added

01:04:50.980 --> 01:04:56.290
the closed simulation to simulate...

01:04:59.680 --> 01:05:01.840
to simulate some paper
or some paper claw thing.

01:05:01.840 --> 01:05:05.040
So we just took this co-simulation,
wrapped it as a patch,

01:05:05.140 --> 01:05:08.040
and then we don't have to worry
about what that patch contains,

01:05:08.130 --> 01:05:10.940
and put that within that simulation.

01:05:10.940 --> 01:05:12.840
And here's an example of what it can do.

01:05:12.900 --> 01:05:17.030
So you can now, you know, give some,
you know, so it's a little elastic,

01:05:17.030 --> 01:05:19.460
so it's not real paper, but, you know,
you can have some fun,

01:05:19.530 --> 01:05:22.940
and so you can bring these things here,
and you know, take the other people,

01:05:22.940 --> 01:05:24.000
and they grow automatically.

01:05:24.000 --> 01:05:27.740
So all these things are
physically simulated using OpenCL.

01:05:27.740 --> 01:05:30.950
So it takes OpenCL,
which is not only for scientists,

01:05:31.020 --> 01:05:31.980
it's for everybody.

01:05:31.980 --> 01:05:35.340
It's for everybody who wants to bring
new possibilities to their applications,

01:05:35.340 --> 01:05:38.480
to whatever they want to,
they're working on, speed in sorting,

01:05:38.480 --> 01:05:42.070
speed in visualization,
or new possibilities in, you know,

01:05:42.180 --> 01:05:44.900
in things that you thought were fixed.

01:05:44.970 --> 01:05:47.480
So for instance,
an image don't have to be on a quad.

01:05:47.500 --> 01:05:48.500
It doesn't have to be on a quad.

01:05:48.500 --> 01:05:50.400
It can be on this, on this claw thing.

01:05:50.400 --> 01:05:56.590
And for instance, so, and here,
so you can simulate some real page curls.

01:05:56.700 --> 01:05:57.700
So here we have some.

01:05:57.750 --> 01:05:58.900
You know, a page curl going on.

01:05:58.900 --> 01:06:01.800
So you don't have to fake the page
curl to try to find the function.

01:06:01.800 --> 01:06:06.040
You can just take that image and say,
"Okay, let's do a page curl to

01:06:06.040 --> 01:06:08.190
reveal what's beneath." Well,
that's all the things.

01:06:08.230 --> 01:06:10.700
And then, oh, she's gonna,
and if I release,

01:06:10.700 --> 01:06:12.200
the key is gonna drop down.

01:06:12.280 --> 01:06:15.560
So, and, and, so to do that page curl,
literally what we just do,

01:06:15.560 --> 01:06:17.910
we just take the two
corners and put them up,

01:06:17.910 --> 01:06:19.040
and that's all what you need.

01:06:19.100 --> 01:06:22.400
And so that's also what bring,
what the simulation brings to you,

01:06:22.400 --> 01:06:25.480
the fact that you don't have to worry
about the functions and all the things,

01:06:25.570 --> 01:06:27.660
because you have the simulation,
because now it's abstracted.

01:06:27.660 --> 01:06:31.090
Now, with that patch,
you can leverage it to do lots of,

01:06:31.090 --> 01:06:34.810
lots of very powerful things,
but very easily,

01:06:34.810 --> 01:06:37.900
because you have all the things which is,
which is hidden.

01:06:37.910 --> 01:06:42.900
So, and, and that's it.

01:06:42.900 --> 01:06:45.960
So can I, can we go back to slides,
please?

01:06:54.310 --> 01:06:56.300
So that was this little extra picture.

01:06:56.300 --> 01:06:58.540
So we're having a session this afternoon.

01:06:58.540 --> 01:07:03.670
Please come if you want to learn
more about the technologies and

01:07:03.860 --> 01:07:07.300
about the new features in Quark's
Composer to do these sort of things.

01:07:07.580 --> 01:07:11.740
And now I'm going to bring
back Jeff on stage for Q&A.