WEBVTT

00:00:20.600 --> 00:00:25.500
So welcome to this session on
audio development for iPhone.

00:00:25.520 --> 00:00:29.080
My name's William Stewart and
I work in the Core Audio group.

00:00:29.080 --> 00:00:31.900
And in the conversations
today in this session,

00:00:31.900 --> 00:00:35.580
we're going to be going through
all of the various services

00:00:35.580 --> 00:00:37.460
for doing audio on iPhone.

00:00:37.460 --> 00:00:42.070
And I thought what we would do to do this
is to go through a set of tasks that we

00:00:42.070 --> 00:00:46.770
think are representative of the kinds
of things that you would want to do.

00:00:46.860 --> 00:00:49.580
So we're going to look
at doing system sounds,

00:00:49.580 --> 00:00:52.680
we're going to look at playing
back and recording files,

00:00:52.680 --> 00:00:56.770
we're going to look at audio in games,
and we're going to look at doing

00:00:56.770 --> 00:01:01.380
audio units if you want to do
some more low-level processing.

00:01:01.420 --> 00:01:05.810
And we're also going to look at some
of the general behavior of audio.

00:01:05.810 --> 00:01:09.840
There was a question in the last
session in games about managing

00:01:09.840 --> 00:01:12.230
audio as calls come in and so forth.

00:01:12.230 --> 00:01:14.490
So we're going to have a look at that.

00:01:14.570 --> 00:01:19.780
And I'd just like to say a personal
note of it's very interesting to see

00:01:19.780 --> 00:01:23.730
all of the games that are coming out,
seeing the use of the audio system

00:01:23.730 --> 00:01:31.970
and the way it's being used with
MooCam Music and the games yesterday.

00:01:31.970 --> 00:01:44.150
So it's very exciting and I'm really
glad that you're here and we're

00:01:44.230 --> 00:01:50.410
looking for some great things.

00:01:50.460 --> 00:01:50.460
So the APIs are really what we want
to cover here today to look at what

00:01:50.460 --> 00:01:50.460
you're going to learn in the session
and what APIs to use for what purposes.

00:01:50.460 --> 00:01:50.460
And also to have a better understanding
of the types of experiences that

00:01:50.460 --> 00:01:50.460
your user is going to be having.

00:01:50.510 --> 00:01:52.640
So the audio system is going to
have with your game in the larger

00:01:52.640 --> 00:02:20.440
context of the phone and the
audio system that it is using.

00:02:20.440 --> 00:02:20.440
So the technology frameworks that
you have to look at are primarily

00:02:20.440 --> 00:02:20.440
the audio toolbox framework.

00:02:20.440 --> 00:02:20.440
And this is the primary APIs for
most of the functionality.

00:02:20.440 --> 00:02:20.440
The audio unit framework provides
some headers that you use for if

00:02:20.440 --> 00:02:20.440
you're doing the lower-level stuff.

00:02:20.440 --> 00:02:20.440
And then there's also
the OpenAL framework,

00:02:20.440 --> 00:02:20.440
of course,
which we'll get into in a moment.

00:02:20.440 --> 00:02:50.410
So the first step is system sounds.

00:02:50.420 --> 00:02:50.420
System sounds are basically small sounds.

00:02:50.420 --> 00:02:50.420
UI sounds, mail send, keyboard taps,
SMS alerts, these kinds of things.

00:02:50.420 --> 00:02:50.420
If you've got a very simple game,
this could be all that you need if

00:02:50.420 --> 00:02:50.420
you're just making kind of tick sounds,
stuff like that.

00:02:50.420 --> 00:02:50.420
It's a very lightweight API,
very lightweight use on you.

00:02:50.420 --> 00:02:52.110
There's no individual control
for volumes or for panning.

00:02:52.500 --> 00:02:57.290
So it's just very much a
play and forget sort of API.

00:02:57.290 --> 00:03:00.730
And let's have a look
at what that looks like.

00:03:00.900 --> 00:03:05.290
It's in the audio toolbox framework,
audioservices.h.

00:03:05.800 --> 00:03:10.540
And the way the API works is
that you provide a CFURL file and

00:03:10.880 --> 00:03:14.860
then the sounds must be short,
so they should be less than 30 seconds.

00:03:14.860 --> 00:03:20.420
Even if you're over 5 or 10 seconds,
you might think about compressing

00:03:20.860 --> 00:03:24.560
the sound with IMA or something
similar to that because it will

00:03:24.560 --> 00:03:29.140
make a smaller memory footprint
when we load the sound into memory.

00:03:29.930 --> 00:03:33.220
And then when you're
finished playing the sound,

00:03:33.220 --> 00:03:34.920
if you're only going to
play it sporadically,

00:03:34.920 --> 00:03:37.200
you could dispose the sounds as you go.

00:03:37.200 --> 00:03:39.300
If you're going to play
the sound multiple times,

00:03:39.300 --> 00:03:40.420
you just keep it around.

00:03:42.030 --> 00:03:46.350
So playing the sound then is just
a simple matter of calling one of

00:03:46.350 --> 00:03:48.690
the two APIs that you see here.

00:03:48.700 --> 00:03:51.640
There's play system sound,
and this plays the

00:03:51.680 --> 00:03:53.440
sound with normal usage.

00:03:53.440 --> 00:03:56.270
It'll obey the ringer
switch on the phone.

00:03:56.270 --> 00:04:00.180
It will play back at the sort of
the volume that the phone is set

00:04:00.180 --> 00:04:02.550
at at any particular point in time.

00:04:02.550 --> 00:04:05.540
And it's commonly the
API that you'll use.

00:04:21.890 --> 00:04:21.890
If you need to do some kind of alert,
like let's say you have a notification,

00:04:21.890 --> 00:04:21.890
you want to do some kind of an alarm,
or you want to attract the

00:04:21.890 --> 00:04:21.890
user's attention in some way,
then you can use the play alert

00:04:21.890 --> 00:04:21.890
sound variant of this API call.

00:04:22.210 --> 00:04:28.230
This will vibrate as well on the phone
if the user has got vibrate set on.

00:04:28.370 --> 00:04:31.950
Of course, there's no vibration
capability on iPhone Touch,

00:04:32.140 --> 00:04:35.960
so it will make a small buzzing
noise from the iPod Touch speaker.

00:04:35.960 --> 00:04:42.420
Because it has this kind of
extra actions associated with it,

00:04:42.420 --> 00:04:45.710
you don't really want to be
bothering the user all the time,

00:04:45.710 --> 00:04:45.710
so use the API appropriately.

00:04:47.530 --> 00:04:54.320
And then with the API,
because it is a play and forget API,

00:04:54.320 --> 00:04:59.000
if you want to know when we're
finished playing the sound for you,

00:04:59.000 --> 00:05:00.620
then you can add a completion proc.

00:05:00.700 --> 00:05:04.620
And this completion callback will fire
when the sound is finished playing.

00:05:04.620 --> 00:05:08.040
You can decide to dispose the
sound at that point or you

00:05:08.380 --> 00:05:10.340
could loop it if you wanted to.

00:05:10.340 --> 00:05:14.430
And then if you want to
explicitly vibrate the phone,

00:05:14.440 --> 00:05:17.380
there's a sound ID called vibrate.

00:05:17.400 --> 00:05:20.740
And then you just play this
ID as if it were a sound file

00:05:20.970 --> 00:05:23.220
and that will vibrate the phone.

00:05:25.030 --> 00:05:27.190
Okay, so that's system sounds.

00:05:27.190 --> 00:05:32.400
And what I want to do now is
to go through the general sets

00:05:32.400 --> 00:05:35.460
of APIs that you can use for
playing and recording audio files.

00:05:35.470 --> 00:05:39.780
And there's really two
things to look at here.

00:05:39.780 --> 00:05:43.980
There's the API that is going to
read or write audio files and then

00:05:43.980 --> 00:05:47.970
there's the API that's going to
be used to render those files or

00:05:48.090 --> 00:05:50.480
to get the audio for recording.

00:05:51.140 --> 00:05:55.620
So let's just look very briefly at
the file services that we provide.

00:05:55.620 --> 00:05:57.730
There's two different file objects.

00:05:57.840 --> 00:06:01.490
There's an audio file object
and an audio file stream object.

00:06:01.530 --> 00:06:06.220
Audio files are used for
reading and writing audio data.

00:06:06.220 --> 00:06:10.220
Audio file stream is just a data
object and it can only be used

00:06:10.350 --> 00:06:15.120
for reading and I'll go into some
more details about that shortly.

00:06:16.980 --> 00:06:19.900
Now both of the APIs are
capable of dealing with several

00:06:19.900 --> 00:06:21.720
different types of file formats.

00:06:21.730 --> 00:06:25.030
And in Cordio,
we talk very differently about

00:06:25.030 --> 00:06:27.460
file formats and data formats.

00:06:27.590 --> 00:06:33.080
So a file format is a description of
the container of a file of itself,

00:06:33.080 --> 00:06:35.040
or it could be a network
stream or whatever,

00:06:35.040 --> 00:06:37.540
but it's a description of a container.

00:06:37.540 --> 00:06:40.460
A data format is the
data that's in that file,

00:06:40.460 --> 00:06:43.170
the specific audio data,
and the two are not

00:06:43.200 --> 00:06:44.450
the same in most cases.

00:06:45.460 --> 00:06:48.040
So when we talk about file formats,
we're talking about

00:06:48.040 --> 00:06:49.540
specifications for files.

00:06:49.580 --> 00:06:53.320
MPEG-4 is a file format,
and you'll see .m4a,

00:06:53.320 --> 00:06:55.940
.mp4 as common extensions.

00:06:55.940 --> 00:07:01.880
A .m4a file can contain
AAC data or Apple lossless data.

00:07:01.880 --> 00:07:07.680
An .mp3 is both a file format,
commonly called a .mp3,

00:07:07.680 --> 00:07:10.960
which is actually an MPEG-1-03,
and then there's a data

00:07:10.980 --> 00:07:12.670
format that can go into that.

00:07:13.820 --> 00:07:18.160
And then ADTS is a bitstream
that's part of MPEG-2,

00:07:18.160 --> 00:07:19.940
where AAC was first specified.

00:07:19.940 --> 00:07:24.280
And this is a similar type
of bitstream to .mp3 in that

00:07:24.280 --> 00:07:26.500
it can be used on a network.

00:07:26.780 --> 00:07:33.840
And these are really a case where
the file and the data that's in it

00:07:33.840 --> 00:07:37.290
are kind of the same type of thing,
.mp3 or .aac.

00:07:37.380 --> 00:07:42.550
A CAF file, Cordio format file,
was a file format we introduced in Tiger.

00:07:42.560 --> 00:07:43.800
And this is a file format
that we introduced in Tiger.

00:07:43.800 --> 00:07:47.760
And this is a file format
that can contain any data,

00:07:47.850 --> 00:07:52.950
can contain Apple lossless, AAC, .mp3,
linear PCM, IMA,

00:07:52.950 --> 00:07:55.780
any sort of audio data you like.

00:07:55.880 --> 00:07:57.780
And then you've got AAF and WAV files.

00:07:57.780 --> 00:08:03.750
So these files can be read
or written on the iPhone.

00:08:05.550 --> 00:08:09.400
So with Audio File API, of course,
you've got to pair the right

00:08:09.430 --> 00:08:11.280
data to the right file format.

00:08:11.280 --> 00:08:14.290
And one of the things about
Audio File API is that it's

00:08:14.430 --> 00:08:18.020
used with the file data that
the file is completely there.

00:08:18.020 --> 00:08:21.660
It's not appropriate for a
network download or for a

00:08:21.660 --> 00:08:24.300
chatcast-style network stream.

00:08:24.300 --> 00:08:28.900
But because the file is completely there,
you can do arbitrary seeking.

00:08:28.900 --> 00:08:33.610
You can pull arbitrary data to or
write arbitrary data to the file.

00:08:33.680 --> 00:08:39.990
So that can be things like overviews
or regions or marker chunks,

00:08:40.070 --> 00:08:40.980
that kind of thing.

00:08:40.980 --> 00:08:47.060
And then Audio File can read or
write data to or from the file.

00:08:47.470 --> 00:08:50.190
It has a fairly
straightforward way of usage.

00:08:50.210 --> 00:08:53.220
There's two calls that
create an audio file object.

00:08:53.340 --> 00:08:57.140
One where you're going to create
a file on the disk or you're going

00:08:57.140 --> 00:08:59.030
to override an existing file.

00:08:59.030 --> 00:09:02.640
And to do the create call,
you just provide the data format and

00:09:02.660 --> 00:09:04.970
the file format for the new file.

00:09:05.540 --> 00:09:07.330
And then if you're
going to open the file,

00:09:07.430 --> 00:09:09.320
typically you'll do this for reading.

00:09:09.320 --> 00:09:14.030
Then you do audio file,
open URL and you provide, if possible,

00:09:14.030 --> 00:09:17.800
a hint to the call to tell us
what type of file it is that

00:09:18.110 --> 00:09:20.240
you're wanting us to open.

00:09:20.240 --> 00:09:24.000
Sometimes the extension
doesn't match the file type.

00:09:24.000 --> 00:09:28.720
Some of the files are very difficult
to determine because they could be

00:09:28.720 --> 00:09:31.490
one of two or three different things.

00:09:31.500 --> 00:09:34.160
So if you know,
giving us a hint is very useful.

00:09:35.180 --> 00:09:35.330
And then once you've
created the audio file,

00:09:35.330 --> 00:09:35.500
you can go ahead and open the file.

00:09:35.600 --> 00:09:38.030
And then once you have
an audio file object,

00:09:38.160 --> 00:09:41.860
as with other Cordio APIs,
we have a property semantic where you

00:09:41.920 --> 00:09:44.300
can get and set properties of files.

00:09:44.340 --> 00:09:48.450
And then to get the data from
the file or to write the data,

00:09:48.450 --> 00:09:53.420
it's just a straightforward audio
file read packets to read from this

00:09:53.420 --> 00:09:59.620
particular audio packet for this many
packets into the buffer you've provided.

00:09:59.620 --> 00:10:02.100
And writing is the inverse of that,
of course.

00:10:03.800 --> 00:10:06.300
And then that's fairly straightforward.

00:10:06.300 --> 00:10:09.380
Now,
Audio File Stream is a different type

00:10:09.470 --> 00:10:13.920
of API in the sense that it's not
really dealing with the file itself.

00:10:13.920 --> 00:10:15.100
It's dealing with bytes.

00:10:15.190 --> 00:10:18.940
And it's bytes that you're pushing
into the file stream object.

00:10:18.940 --> 00:10:20.820
So the file stream is a parser.

00:10:20.830 --> 00:10:24.600
It's parsing those bytes and
it's telling you what's in them.

00:10:26.200 --> 00:10:28.410
So because of this,
the data doesn't have to

00:10:28.420 --> 00:10:29.740
be present completely.

00:10:29.740 --> 00:10:32.450
It's just a buffer that
you're going to push into it.

00:10:32.510 --> 00:10:36.500
So you can use this with the file,
in which case you'll open the

00:10:36.500 --> 00:10:38.810
file and read the bytes from it.

00:10:39.770 --> 00:10:42.220
Or you could do an HTTP download.

00:10:42.220 --> 00:10:45.350
So while the file is
actually downloading,

00:10:45.350 --> 00:10:51.030
you could be pushing the bytes through
and getting the audio metadata from it.

00:10:51.620 --> 00:10:55.120
Or it could even be a
shoutcast style network stream.

00:10:55.510 --> 00:10:56.190
So it's a very simple way to do it.

00:10:56.200 --> 00:10:57.190
It's not really a file.

00:10:57.190 --> 00:11:00.200
It's just a stream of bytes and you're
kind of jumping in at one point.

00:11:00.950 --> 00:11:05.960
And then the way the API works is
that it uses a notification mechanism

00:11:06.330 --> 00:11:10.200
to tell you information about the
bytes that you're pushing into it.

00:11:11.000 --> 00:11:15.870
And we also try to be efficient with
minimal copying and to really kind of

00:11:15.870 --> 00:11:21.050
not have an overhead in us doing the
work to tell you information about the

00:11:21.050 --> 00:11:23.130
stream that you're pushing into it.

00:11:23.360 --> 00:11:24.200
And so because it's not a file stream,
it's not a data stream.

00:11:24.200 --> 00:11:24.200
So it's not a data stream.

00:11:24.510 --> 00:11:25.200
And so because it's not a data stream,
it's not a data stream.

00:11:25.310 --> 00:11:25.640
And so because it's not a data stream.

00:11:25.640 --> 00:11:25.870
So it's not a data stream.

00:11:25.870 --> 00:11:26.190
So it's not a data stream.

00:11:26.200 --> 00:11:26.200
So it's not a data stream.

00:11:26.200 --> 00:11:26.200
So it's not a data stream.

00:11:26.830 --> 00:11:30.000
And so because it's not really
dealing with a file but with bytes,

00:11:30.000 --> 00:11:33.400
the API to create the audio file
stream object doesn't provide a file,

00:11:33.400 --> 00:11:35.200
it just provides callbacks.

00:11:36.110 --> 00:11:39.650
And they're the callbacks
that we're going to give you,

00:11:39.650 --> 00:11:44.290
to call you with when you've pushed
data in and we're going to call you back

00:11:44.350 --> 00:11:46.840
and tell you something about that data.

00:11:47.930 --> 00:11:51.140
And as with audio file,
if you know the format of the file,

00:11:51.150 --> 00:11:54.150
it's very helpful to provide
a hint to tell us that.

00:11:54.200 --> 00:11:55.310
Thank you.

00:11:55.800 --> 00:14:16.000
[Transcript missing]

00:14:16.680 --> 00:14:19.900
And so,
AudioQ can play back any supported file

00:14:19.900 --> 00:14:24.910
format through the use of audio file,
of course, and any of the data formats,

00:14:25.050 --> 00:14:30.590
but there are some qualifications on
the data formats that you can use.

00:14:30.660 --> 00:14:36.450
And this is because the iPhone itself
has limitations and some of the formats

00:14:36.450 --> 00:14:39.920
are going to be decoded through hardware.

00:14:39.920 --> 00:14:45.680
And so, that makes us,
puts a limitation on what we can do.

00:14:46.400 --> 00:14:50.620
So, these are the list of the formats
that we can decode that we ship with.

00:14:50.620 --> 00:14:56.920
And the top three, AAC, MP3,
Apple Lossless, are all modern codecs,

00:14:57.070 --> 00:14:59.480
are all fairly complex
computational objects.

00:14:59.540 --> 00:15:04.440
And because of this,
their actual work is done in hardware.

00:15:04.440 --> 00:15:07.960
And you can only have one of
these operating at a time.

00:15:07.960 --> 00:15:11.200
And this isn't just one of
each of the three of them,

00:15:11.200 --> 00:15:12.680
but one of any of them.

00:15:12.680 --> 00:15:16.380
So, only one AAC,
and then you couldn't have MP3 or MP4,

00:15:16.400 --> 00:15:19.300
or MP5, or MP6, or MP7, or MP8, or MP9,
or MP10, or MP11, or MP12, or MP13,

00:15:19.300 --> 00:15:21.160
or MP14, or MP15, or MP16, or MP17.

00:15:21.160 --> 00:15:22.040
And so,
that's the kind of things that we can do.

00:15:22.040 --> 00:15:23.160
And so, the format that we can
use is pretty simple.

00:15:23.160 --> 00:15:26.170
It's very simple.

00:15:26.170 --> 00:15:30.690
It's a pretty good fidelity.

00:15:30.800 --> 00:15:36.910
And it's a 4:1 compression ratio.

00:15:37.160 --> 00:15:40.800
ILBC is Internet Low Bit Rate Codec
and AMR.

00:15:40.800 --> 00:15:42.450
These are both speech codecs.

00:15:42.590 --> 00:15:47.670
AMR is a speech codec that's in 3GPP,
which is a standard that's

00:15:47.670 --> 00:15:49.380
kind of based off MPEG-4.

00:15:49.380 --> 00:15:54.290
And the speech codecs are very small,
usually 8 kilohertz,

00:15:54.290 --> 00:15:57.280
so it's what's called narrowband.

00:15:57.280 --> 00:16:01.740
And they're very small data streams
and ILBC is particularly good for

00:16:01.740 --> 00:16:03.900
network where you may lose packets.

00:16:04.140 --> 00:16:10.180
ILBC can interpolate with lost packets
and provide a fairly good audio signal.

00:16:10.180 --> 00:16:12.700
And of course,
if you're doing just sort of speech

00:16:12.880 --> 00:16:16.800
content and you're concerned about size,
these codecs are both very good.

00:16:18.930 --> 00:16:22.820
And so with AudioCue,
you create an AudioCue object and you

00:16:22.820 --> 00:16:28.010
provide a description to the AudioCue
to say what kind of data you're

00:16:28.030 --> 00:16:29.660
going to play back with this cue.

00:16:29.660 --> 00:16:33.560
And then you give it a callback and that
callback is going to be called by the cue

00:16:33.560 --> 00:16:35.920
when it's finished processing your data.

00:16:35.920 --> 00:16:38.770
And you provide a thread
context on which the cue is

00:16:39.000 --> 00:16:40.840
going to call that callback on.

00:16:42.290 --> 00:16:46.280
And based on the type of latency or
the type of the size of the buffers

00:16:46.280 --> 00:16:49.800
that you're using with the AudioCue,
the thread context can be very

00:16:49.990 --> 00:16:54.060
important to make sure that you get
your callbacks in a timely manner.

00:16:56.640 --> 00:17:00.320
And the queue owns the buffers
that you're using on it.

00:17:00.340 --> 00:17:05.210
It's not something that you just
kind of can give us arbitrary memory.

00:17:05.210 --> 00:17:11.420
So you allocate buffers on the queue that
you're using and you just provide a size.

00:17:11.580 --> 00:17:16.320
And a good optimization that we've
added since Leopard is the allocate

00:17:16.320 --> 00:17:19.060
buffers with packet descriptions.

00:17:19.060 --> 00:17:23.020
And this means that the packet
descriptions are in the same

00:17:23.020 --> 00:17:26.480
memory buffer that we're
using internally as the queue.

00:17:26.610 --> 00:17:32.030
And it means we've got one less copy
and that's a good efficiency for

00:17:32.130 --> 00:17:35.230
the queue when it's playing back.

00:17:35.300 --> 00:17:37.840
And then once you've
got the queue buffer,

00:17:37.840 --> 00:17:41.240
you fill it up with audio data,
typically from audio file

00:17:41.240 --> 00:17:42.820
or audio file stream.

00:17:42.820 --> 00:17:49.010
And then you just enqueue that buffer
to the queue and it'll get played.

00:17:49.010 --> 00:17:50.890
And of course, how do you play it?

00:17:50.890 --> 00:17:52.030
Well, you start it.

00:17:52.030 --> 00:17:53.820
You just start the audio queue.

00:17:53.820 --> 00:17:55.880
When you want to stop, you stop it.

00:18:26.480 --> 00:18:26.480
If you've got intermittent data,
let's say you've got just bits of

00:18:26.480 --> 00:18:26.480
audio here and then there's like some
indeterminate silence and a bit there,

00:18:26.480 --> 00:18:26.480
you can keep the queue underneath sort of
running by just pausing the queue itself.

00:18:26.480 --> 00:18:26.480
But you sort of keep some
of the engine underneath the

00:18:26.480 --> 00:18:26.480
audio queue API kind of primed.

00:18:26.480 --> 00:18:26.480
So you can pause the queue at that point.

00:18:26.480 --> 00:18:26.480
You can prime the queue if you want to
get it ready so that when you hit start,

00:18:26.480 --> 00:18:26.480
it's going to really
get going straight away.

00:18:26.480 --> 00:18:26.480
And then if you're doing
transport controls,

00:18:26.480 --> 00:18:26.480
you've got audio queue readouts.

00:18:26.930 --> 00:18:29.010
If you want to reset and
deal with compressed formats,

00:18:29.070 --> 00:18:32.350
you can flush the queue at the
end to make sure that we play

00:18:32.350 --> 00:18:34.490
out all of the data that's there.

00:18:36.120 --> 00:18:42.640
So that's a very simple overview of the
cue for playback and cue in recording

00:18:42.640 --> 00:18:44.590
is actually not that different.

00:18:44.700 --> 00:18:51.930
So in this case you'd use audio
cue to record and then we'd use

00:18:51.930 --> 00:18:54.870
audio file to write the audio
file as we're getting the data.

00:18:55.940 --> 00:19:01.800
So, AudioQ and AudioFile together can,
as you would hope,

00:19:01.890 --> 00:19:05.690
record into any supported file
format that we can write and

00:19:05.690 --> 00:19:08.150
any supported data format.

00:19:08.540 --> 00:19:13.290
And so what are the data formats that
the AudioQ can be constructed to use?

00:19:13.630 --> 00:19:16.240
And linear PCM, uncompressed.

00:19:16.240 --> 00:19:18.620
We do have an Apple lossless encoder.

00:19:18.620 --> 00:19:22.420
And so this will compress
your audio losslessly.

00:19:22.420 --> 00:19:25.420
It'll be the same when you play it
back as it was when you recorded.

00:19:25.420 --> 00:19:28.760
And that's about a 55, 50, 60 percent.

00:19:28.870 --> 00:19:31.660
It varies a little bit,
compression ratio.

00:19:31.660 --> 00:19:36.740
You've got MU-LOR and ALOR again,
which are both 8-bit formats.

00:19:36.820 --> 00:19:38.620
IMA4 is a 4-bit format.

00:19:38.620 --> 00:19:42.510
And for speech content,
we just have the ILBC codec

00:19:42.510 --> 00:19:44.000
available for speech.

00:19:44.030 --> 00:19:48.120
And when you create the Q,
you just provide the data format

00:19:48.120 --> 00:19:50.660
that you want the Q to record into.

00:19:50.660 --> 00:19:55.810
And it will create all of the codecs
that it's needing to get from the device,

00:19:55.810 --> 00:19:59.690
from whatever the device has got,
including sample rate

00:19:59.690 --> 00:20:03.170
conversion and so forth,
to the format that you've

00:20:03.170 --> 00:20:05.790
specified when you create the Q.

00:20:07.740 --> 00:20:11.210
and you give it a buffer callback
and the buffer callback is how

00:20:11.210 --> 00:20:14.960
the cue delivers the data to
you and as with the output case,

00:20:14.960 --> 00:20:22.190
you provide through the thread
context and it has much of the same

00:20:22.190 --> 00:20:22.190
semantics as it does previously.

00:20:22.610 --> 00:20:28.010
And so as with output,
you allocate a buffer for the queue to

00:20:28.010 --> 00:20:29.710
use and then you enqueue the buffer.

00:20:29.800 --> 00:20:33.220
Now of course you don't have anything
in that buffer yet because the

00:20:33.220 --> 00:20:35.020
queue is filling the buffer for you.

00:20:35.020 --> 00:20:37.720
So you just enqueue the
buffer for the queue to use.

00:20:37.720 --> 00:20:42.480
And then the input queue when it's run,
it's going to put data into that

00:20:42.540 --> 00:20:47.390
buffer until it's filled those buffers
and then it's going to deliver them

00:20:47.390 --> 00:20:51.680
to you through the callback that
you provided in the construction.

00:20:52.560 --> 00:20:55.640
And so it has the same
APIs for playback control.

00:20:55.640 --> 00:21:00.530
You can start the cue, you can stop it,
you can pause it,

00:21:00.650 --> 00:21:04.880
and you can reset it to kind of get
rid of all of the data if you're not

00:21:04.950 --> 00:21:07.540
interested in it and that kind of thing.

00:21:07.730 --> 00:21:13.510
So we're actually going to go to a demo
now and I'll show you a simple example

00:21:13.510 --> 00:21:19.390
that we've written to demonstrate
playback and record called AQ Touch.

00:21:19.390 --> 00:21:20.620
So if we can go to the projector.

00:21:25.410 --> 00:21:28.660
Okay, there's some sound, that's good.

00:21:28.660 --> 00:21:36.490
So, this application,
I'm going to just select

00:21:36.510 --> 00:21:38.410
the file here and play it.

00:21:47.340 --> 00:21:51.120
We believe in very high fidelity
audio in the Core Audio Group,

00:21:51.190 --> 00:21:52.310
as you can see.

00:21:53.110 --> 00:21:56.840
So the file we're playing
here is an M4A file,

00:21:56.840 --> 00:21:58.810
the same type of file
you'd have in iTunes.

00:21:58.920 --> 00:22:04.550
It's got AAC data in it and we
were metering two channels and

00:22:04.640 --> 00:22:07.290
we were metering that because
the file had two channels in it.

00:22:07.420 --> 00:22:10.120
I'm actually going to record
a file now so I'm just going

00:22:10.120 --> 00:22:11.540
to hit the record button.

00:22:11.540 --> 00:22:17.700
Hello, welcome to WWDC 2008.

00:22:19.300 --> 00:22:26.200
[Transcript missing]

00:22:26.390 --> 00:22:28.860
And as you can hear,
high fidelity recording.

00:22:28.860 --> 00:22:31.420
And that's coming in from the microphone.

00:22:31.420 --> 00:22:34.060
And of course, you know,
it's not really great in such

00:22:34.060 --> 00:22:35.810
a reverberant room as this.

00:22:35.810 --> 00:22:41.210
But basically what we're also seeing
there is that we're seeing one meter

00:22:41.210 --> 00:22:47.600
coming in on the input side because the
microphone is an 8 kilohertz mono input.

00:22:47.600 --> 00:22:50.990
And so we're reflecting the
level that's coming in from

00:22:51.070 --> 00:22:53.100
the microphone in the meters.

00:22:53.100 --> 00:22:55.300
If we can go back to slides, please.

00:23:03.600 --> 00:23:09.020
Okay, so how do we know that
it's 8 kilohertz and mono?

00:23:09.020 --> 00:23:12.920
How do we know that we're actually
getting audio in from the microphone?

00:23:12.920 --> 00:23:18.470
I had a dock connector plugged in there,
why wasn't I getting audio through that?

00:23:18.470 --> 00:23:22.340
Well, these are all important questions,
of course,

00:23:22.400 --> 00:23:26.430
and it's a good point to step back
and just think about how complex

00:23:26.910 --> 00:23:32.020
the iPhone is as an audio device,
because it's actually very complex.

00:23:32.020 --> 00:23:35.870
It has a built-in microphone,
it has a built-in speaker,

00:23:35.990 --> 00:23:41.120
it actually has another built-in speaker,
and we call that a receiver to

00:23:41.130 --> 00:23:43.890
distinguish between that and the speaker.

00:23:43.900 --> 00:23:47.880
They're both speakers,
but you understand the difference.

00:23:48.550 --> 00:23:53.920
It has the ability to plug in headphones
and you can also plug in headphones

00:23:54.090 --> 00:23:58.200
that we call headsets because they
have a microphone in them as well.

00:23:58.200 --> 00:24:02.600
So you can plug in either headphones
or headphones with microphones.

00:24:02.680 --> 00:24:04.860
You can use Bluetooth for calls.

00:24:04.930 --> 00:24:09.680
You can get Bluetooth which
will have both input and output.

00:24:10.180 --> 00:24:15.470
You can plug your iPhone into a dock
and you can get line out from an iPhone

00:24:15.920 --> 00:24:17.840
or an iPod touch for that matter.

00:24:17.960 --> 00:24:23.860
Or you can do the dock connector
like I'm using up on stage there.

00:24:23.860 --> 00:24:28.370
You can also use,
get USB output directly from an

00:24:28.370 --> 00:24:33.960
iPhone and go into a car unit and
most of the car units now are using

00:24:33.960 --> 00:24:37.420
USB as their main transport mechanism.

00:24:37.640 --> 00:24:40.580
precisely to avoid the noise problems
we had while we were setting up,

00:24:40.580 --> 00:24:41.060
actually.

00:24:41.060 --> 00:24:43.780
And then there's also controls.

00:24:43.780 --> 00:24:47.340
There's a ringer switch control
which silences the phone or not,

00:24:47.340 --> 00:24:49.610
and then there's volume keys.

00:24:49.610 --> 00:24:52.080
So that's quite a complex device.

00:24:52.150 --> 00:24:56.040
If you look at your desktop computer,
it doesn't have this

00:24:56.040 --> 00:24:57.610
complexity for audio.

00:24:57.830 --> 00:25:01.600
And so we wanted to really
understand how this should behave

00:25:01.600 --> 00:25:06.600
and what do users expect from their
phone when they're doing things.

00:25:06.760 --> 00:25:11.130
And we wanted the behaviour
for the users to be consistent.

00:25:11.230 --> 00:25:15.400
The user shouldn't be surprised,
and the user shouldn't have to go and

00:25:15.400 --> 00:25:17.740
do configuration types of activities.

00:25:17.740 --> 00:25:23.240
And so this meant that we had to take
a lot of the control of the audio

00:25:23.240 --> 00:25:27.610
system from individual applications
and do it at a system level so

00:25:27.610 --> 00:25:29.970
we could ensure that consistency.

00:25:29.980 --> 00:25:35.110
And a guiding principle that we had is
that we wanted to do what the user means.

00:25:36.240 --> 00:25:39.340
When they plug in headphones,
what do they mean?

00:25:39.340 --> 00:25:41.030
What does that gesture mean for them?

00:25:41.100 --> 00:25:45.140
When they're hitting the volume keys,
are they changing the iPod volume?

00:25:45.140 --> 00:25:46.970
Are they changing the ringer volume?

00:25:46.970 --> 00:25:49.560
Are they changing the
volume of your application?

00:25:51.260 --> 00:25:54.300
If I hit the ringer switch,
should that always be silent?

00:25:54.370 --> 00:25:57.790
What if you hit the ringer switch
and you have a clock alarm?

00:25:58.260 --> 00:26:03.150
So these are all kinds of semantics
layers on top of just the basic

00:26:03.240 --> 00:26:08.510
mechanics of getting it to work
that we wanted to take account of.

00:26:08.610 --> 00:26:11.540
And we wanted to respond
to interruptions too.

00:26:11.540 --> 00:26:14.590
You've got alarms playing,
you've got calls being

00:26:14.590 --> 00:26:17.800
rejected and so forth,
and accepted of course.

00:26:17.800 --> 00:26:22.250
And so we needed sometimes to be
able to silence the audio that's

00:26:22.370 --> 00:26:27.010
playing on the phone because you
don't want to miss your alarm because

00:26:27.010 --> 00:26:29.260
you're listening to loud music.

00:26:30.900 --> 00:26:35.390
So the way we're expressing this
behavior to your application is

00:26:35.500 --> 00:26:37.180
through an API called Audio Session.

00:26:37.180 --> 00:26:39.420
And this is a new API.

00:26:39.420 --> 00:26:42.630
All of the other APIs that I'm
talking about are in Leopard and

00:26:42.630 --> 00:26:46.330
they're available on the desktop
and they behave in the same way.

00:26:46.330 --> 00:26:51.630
This API is just for iPhone because
we're dealing with behavior on iPhone

00:26:51.640 --> 00:26:54.860
and behavior in the iPhone audio system.

00:26:54.860 --> 00:27:00.720
And there's two primary features about
Audio Session that is important to us.

00:27:00.800 --> 00:27:02.330
And that is categories.

00:27:02.360 --> 00:27:06.790
We talk about different applications
being in different categories.

00:27:06.790 --> 00:27:12.380
So MooCow Music and their sequencer
is a different type of application

00:27:12.380 --> 00:27:16.910
than if I were to bring up notes
and just tap on the keyboard

00:27:16.910 --> 00:27:19.270
or if I were to play an iPod.

00:27:20.920 --> 00:27:24.040
And some of the behaviors that are
associated with these different

00:27:24.040 --> 00:27:29.180
categories are things like whether you
mix or not with other applications.

00:27:29.290 --> 00:27:32.360
Would you allow,
is it sensible for your application to

00:27:32.360 --> 00:27:34.640
have iPod playing in the background?

00:27:34.770 --> 00:27:36.770
And it's also got to do with routing.

00:27:36.770 --> 00:27:38.280
Where is your audio going?

00:27:38.740 --> 00:27:46.200
Where a ringtone or an alarm is going
to go is going to be different than

00:27:46.200 --> 00:27:47.420
when iPod is playing back audio.

00:27:47.700 --> 00:27:49.520
There's also the question of volume.

00:27:49.520 --> 00:27:51.600
When you're changing volume,
what are you changing?

00:27:51.600 --> 00:27:53.400
And ringer switch behavior.

00:27:53.400 --> 00:27:58.540
And then the other concept with session
is whether your session is active or not.

00:27:58.670 --> 00:28:00.400
And when do you make your session active?

00:28:00.400 --> 00:28:05.180
When do you assert your role and
your desires into the system?

00:28:07.400 --> 00:28:11.150
So the other thing to think about
as a developer and something we have

00:28:11.150 --> 00:28:15.770
to think about as well is different
models because different models

00:28:15.770 --> 00:28:16.940
will have different behaviors.

00:28:16.940 --> 00:28:21.190
We use the same code on iPod Touch and
iPhone but from an audio standpoint

00:28:21.190 --> 00:28:23.520
they're quite different devices.

00:28:23.520 --> 00:28:25.400
iPhone is very complex.

00:28:25.470 --> 00:28:31.990
iPod Touch really just has
headphones and line out and USB out.

00:28:32.000 --> 00:28:34.140
There's nothing terribly
complex about that.

00:28:34.140 --> 00:28:36.700
It's much more similar to
a desktop type environment.

00:28:38.410 --> 00:28:40.300
And then who knows what
we'll do in the future.

00:28:40.300 --> 00:28:44.700
We could have a whole bunch of different
types of audio possibilities and so

00:28:44.700 --> 00:28:49.200
these types of things are fluid and
they're flexible and they're dynamic.

00:28:49.200 --> 00:28:53.620
And so the API is based around
the fact that we want you

00:28:53.660 --> 00:28:58.900
to express your meanings,
your intentions and we'll take

00:28:59.060 --> 00:29:01.720
care of the behavior for you.

00:29:01.720 --> 00:29:05.110
Now we may not take care of it
completely right all the time and

00:29:05.110 --> 00:29:06.820
you can tell us when we're not.

00:29:06.980 --> 00:29:07.280
But that's the idea.

00:29:07.280 --> 00:29:07.780
That's the intention.

00:29:07.890 --> 00:29:11.820
And so part of the reason I'm talking
to you today about this is to have

00:29:11.870 --> 00:29:15.840
you to understand our thinking here so
that you can code your apps to behave

00:29:16.240 --> 00:29:21.030
and to fit in with the way that we're
actually doing our applications as well.

00:29:23.440 --> 00:29:27.640
So, to buy into Audio Session,
you initialize the session.

00:29:27.720 --> 00:29:29.900
It's not actually an object.

00:29:29.900 --> 00:29:33.240
A session is associated
globally with your application,

00:29:33.240 --> 00:29:36.140
so we don't actually give
you a session object.

00:29:36.260 --> 00:29:44.380
You just initialize the session
and you provide a callback which

00:29:44.380 --> 00:29:46.530
we're going to use to tell you when
your audio has been interrupted.

00:29:47.040 --> 00:29:50.020
And you can get interrupted
because you get a call or

00:29:50.020 --> 00:29:52.020
because the clock alarm goes off.

00:29:52.020 --> 00:29:53.200
And they're just two examples.

00:29:53.200 --> 00:29:57.200
Who knows what may else
interrupt you in the future.

00:29:57.470 --> 00:30:02.020
And then the interruption has
two states associated with it.

00:30:02.120 --> 00:30:05.860
There's a begin interruption,
so when the alarm goes to ring,

00:30:05.860 --> 00:30:07.650
we're going to tell you, hey,
you've been interrupted.

00:30:07.790 --> 00:30:10.100
You are no longer playing
audio at this point.

00:30:10.250 --> 00:30:11.180
We've stopped you.

00:30:11.180 --> 00:30:14.000
And here's the notification
to tell you that we have.

00:30:14.180 --> 00:30:16.610
Now, you may not get an end interruption.

00:30:16.610 --> 00:30:19.840
As you know, if you answer a call,
your application is

00:30:20.010 --> 00:30:21.380
going to be terminated.

00:30:21.380 --> 00:30:23.690
So you won't get an end interruption.

00:30:23.690 --> 00:30:25.160
There's no one to tell.

00:30:25.260 --> 00:30:26.260
You're gone.

00:30:27.300 --> 00:30:30.050
And if you want to do
something at that point,

00:30:30.050 --> 00:30:33.600
you can use the Cocoa UIs to
interface to that fact that your

00:30:33.600 --> 00:30:36.040
application is being terminated.

00:30:36.040 --> 00:30:39.540
But if it's just a clock alarm
and the user dismisses it or the

00:30:39.540 --> 00:30:42.860
user doesn't answer the call,
then we'll tell you that the

00:30:43.060 --> 00:30:47.840
interruption has finished and that
you're actually free to make audio again.

00:30:50.840 --> 00:30:55.580
So now we mentioned set active
and when a session becomes

00:30:55.580 --> 00:30:59.860
active it asserts its behaviors,
its requests,

00:30:59.960 --> 00:31:03.240
its characteristics on the system.

00:31:03.300 --> 00:31:08.620
So when you become active you
may stop other people playing.

00:31:08.620 --> 00:31:14.620
And when you become deactive the
other people may play again or not.

00:31:14.620 --> 00:31:18.560
It depends on what's
going on on the system.

00:31:18.560 --> 00:31:23.740
So the set active call is really just
saying right I want the device now,

00:31:23.740 --> 00:31:26.780
I want to make audio or
I want to record audio.

00:31:29.120 --> 00:31:32.200
So Audio Session, just as with all the
other Core Audio APIs,

00:31:32.200 --> 00:31:36.280
uses a property mechanism to get data,
to set data.

00:31:36.280 --> 00:31:43.390
You can instantiate listeners in order
to listen to changes in the audio data.

00:31:44.610 --> 00:31:46.000
And these are the categories.

00:31:46.000 --> 00:31:48.410
And the categories are
implemented as a property and

00:31:48.410 --> 00:31:50.260
the property is audio category.

00:31:50.260 --> 00:31:53.620
So there's six categories
defined at the moment.

00:31:53.620 --> 00:31:57.540
Two of those categories allow
your application to mix with

00:31:57.540 --> 00:32:00.030
other applications on the system.

00:32:01.420 --> 00:32:03.720
Now, the sound effect one is
kind of like a default.

00:32:03.810 --> 00:32:06.570
If you think of the Notes app,
it would be sort of, you know,

00:32:06.570 --> 00:32:08.090
user interface sound effects.

00:32:08.090 --> 00:32:09.520
You're not really doing much.

00:32:09.590 --> 00:32:12.340
You're happy for anything
else to happen on the system.

00:32:12.340 --> 00:32:14.350
Ambient sound is sort of similar to that.

00:32:14.360 --> 00:32:18.840
And if you're a game and you want to,
or any application and you want to allow

00:32:18.840 --> 00:32:23.100
the iPod to play music in the background
while your game is in the front,

00:32:23.320 --> 00:32:25.540
this would be the category to set.

00:32:25.560 --> 00:32:27.380
And it's not the default.

00:32:27.380 --> 00:32:30.740
So if you know nothing else about this
session from audio session and you

00:32:30.740 --> 00:32:30.740
want to play music in the background,
you can do that.

00:32:30.760 --> 00:32:34.030
and you want to do this,
you have to know this.

00:32:34.200 --> 00:32:39.380
Now, if your application is
much more audio centered,

00:32:39.380 --> 00:32:44.440
then you need to be one of
these four remaining categories.

00:32:44.540 --> 00:32:49.240
And these categories are going to stop
other applications from making audio.

00:32:49.240 --> 00:32:53.180
You will now own the audio on the system.

00:32:55.030 --> 00:32:57.320
Except for one which
we'll go into in a moment.

00:32:57.500 --> 00:32:59.590
There's always got to be one exception.

00:32:59.640 --> 00:33:05.270
So the four categories of media playback,
which you could imagine the

00:33:05.290 --> 00:33:08.040
iPod would be in that category.

00:33:08.040 --> 00:33:12.160
Live audio, which could be something
like the sequences,

00:33:12.160 --> 00:33:16.490
the instruments, the iPhone band stuff,
all of these would be

00:33:16.590 --> 00:33:18.660
kind of live audio things.

00:33:18.660 --> 00:33:21.270
Recording,
if you're just doing recording,

00:33:21.270 --> 00:33:24.880
I'm sure there's at least 20 voice
notes apps being written to you.

00:33:24.900 --> 00:33:27.080
I'm looking for one.

00:33:27.080 --> 00:33:31.990
And then there's a play and record
and this allows you to do both

00:33:31.990 --> 00:33:34.750
input and output at the same time.

00:33:36.600 --> 00:33:43.540
And these categories are going to
keep things about them like volume.

00:33:43.540 --> 00:33:49.320
So when the user sets volume,
that's a gesture that the user does.

00:33:49.370 --> 00:33:53.200
It's not actually a gesture
your application can do.

00:33:53.260 --> 00:33:56.660
It's from what we
consider a user's action.

00:33:56.660 --> 00:34:00.790
And when they set the volume,
they're going to set it on a category.

00:34:00.940 --> 00:34:04.280
So let's say you've got
three applications that do

00:34:04.280 --> 00:34:06.110
different types of band things.

00:34:06.270 --> 00:34:10.270
You might have a guitar app and a
piano app or something and they all

00:34:10.270 --> 00:34:14.620
come up as the live audio category,
then they're going to share

00:34:14.620 --> 00:34:17.060
volume for the different routes.

00:34:17.060 --> 00:34:21.040
So if they've got headphones plugged in,
that's going to be a volume for

00:34:21.040 --> 00:34:23.030
that category on that route.

00:34:23.030 --> 00:34:27.160
If they've got just playing to speaker,
that's a different audio

00:34:27.160 --> 00:34:29.260
route but the same category.

00:34:29.260 --> 00:34:33.860
So those two applications would
share volume to that destination.

00:34:33.860 --> 00:34:38.140
And so this is--this covers
across the whole thing.

00:34:38.140 --> 00:34:42.190
And because we're really trying
to do volume as much as possible

00:34:42.200 --> 00:34:45.230
in hardware because it gives
us a much better fidelity and

00:34:45.230 --> 00:34:48.640
a much better user experience,
this is why we're really

00:34:48.640 --> 00:34:50.720
actually trying to control it.

00:34:50.730 --> 00:34:53.500
So how can you present
volume to the user?

00:34:53.500 --> 00:34:57.280
Well, on the iPhone,
the user has volume controls

00:34:57.280 --> 00:34:58.730
that they can hit directly.

00:34:58.730 --> 00:35:00.280
On iPod Touch, they don't.

00:35:00.280 --> 00:35:03.000
So there's two ways you can
bring up a volume slider.

00:35:03.000 --> 00:35:07.150
Bill Stewart You can just do
one which brings up a HUD,

00:35:07.160 --> 00:35:10.380
which is the MP Volume Settings Alert.

00:35:10.380 --> 00:35:13.870
Or you can get a slider,
which is a Cocoa UI View slider

00:35:13.870 --> 00:35:15.410
and you can,
you know,

00:35:15.410 --> 00:35:20.450
put that in the place that you would
like to put it best for your application.

00:35:20.450 --> 00:35:23.340
And the slider that you
see in the iPod app,

00:35:23.340 --> 00:35:26.000
for example, is this type of slider.

00:35:27.470 --> 00:35:31.980
Now there's some audio hardware
settings that you may be interested

00:35:31.980 --> 00:35:35.670
in and this for various reasons.

00:35:35.670 --> 00:35:41.720
The most common hardware setting that
people would like will be sample rate.

00:35:41.810 --> 00:35:48.000
But this is not something you can
set because the current route where

00:35:48.000 --> 00:35:53.770
audio is coming from or going to may
not be able to do that sample rate.

00:35:53.790 --> 00:35:57.380
As an example,
you might want to get 44kHz audio.

00:35:58.370 --> 00:36:03.170
So you're not going to get your
preferred hardware sample rate.

00:36:03.170 --> 00:36:07.650
So you have to be prepared for the
fact that you may not get this,

00:36:07.700 --> 00:36:10.180
but you can still request it.

00:36:11.110 --> 00:36:11.570
I.O.

00:36:11.710 --> 00:36:15.880
buffer size you'll mostly normally get,
but you might not get it

00:36:15.880 --> 00:36:17.670
for one reason or another.

00:36:17.810 --> 00:36:18.710
So the I.O.

00:36:18.780 --> 00:36:22.340
buffer size is talking about
the size of the buffers that

00:36:22.340 --> 00:36:24.390
we're going to use to do I.O.

00:36:24.390 --> 00:36:25.770
to the device.

00:36:26.160 --> 00:36:27.380
And this affects the
latency of the audio.

00:36:27.380 --> 00:36:31.390
of the audio coming in or
going out of the device.

00:36:32.480 --> 00:36:35.550
And so when you do your
preferred hardware settings,

00:36:35.580 --> 00:36:40.000
if you want to get down to that level,
they're going to be made active.

00:36:40.000 --> 00:36:42.700
When you become active,
they're going to be asserted

00:36:42.700 --> 00:36:44.620
onto the device if it's possible.

00:36:44.620 --> 00:36:47.700
And then you can make these
calls which gets now the

00:36:47.730 --> 00:36:49.820
current state of the hardware.

00:36:49.820 --> 00:36:51.270
What is the sample rate?

00:36:51.270 --> 00:36:55.180
Did I get the preferred one or am
I stuck with some other sample rate?

00:36:55.380 --> 00:36:57.050
How many input channels do I have?

00:36:57.050 --> 00:36:58.980
How many output channels do I have?

00:36:59.960 --> 00:37:02.980
And this is a very useful thing,
of course, to do before you're recording.

00:37:02.980 --> 00:37:08.540
It's no good recording 44k kilohertz
stereo if you've got mono 8 kilohertz.

00:37:08.740 --> 00:37:11.910
So this is a useful thing to know.

00:37:11.920 --> 00:37:14.630
And your session must be active.

00:37:14.660 --> 00:37:17.720
If you just make these calls
randomly and you're not active,

00:37:17.720 --> 00:37:19.640
then you're getting
whatever the device is now.

00:37:22.330 --> 00:37:26.480
So we've talked about routes and
routes are things like headphone,

00:37:26.480 --> 00:37:30.490
line out speaker and you can get the
current route at any time by calling

00:37:30.680 --> 00:37:34.160
the get property call with audio route.

00:37:34.880 --> 00:37:39.940
An interesting thing to know
is when the route changes,

00:37:39.940 --> 00:37:43.620
when the user plugs in headphones
or pulls out headphones.

00:37:43.620 --> 00:37:49.230
And this is informed to you through a
property notification and the property

00:37:49.230 --> 00:37:51.760
ID on this is AudioRouteChange.

00:37:51.760 --> 00:37:55.460
You cannot get AudioRouteChange
or set AudioRouteChange,

00:37:55.460 --> 00:37:59.310
you can just get notified that
the audio route has changed.

00:38:00.060 --> 00:38:04.760
And the property notification
includes the data for that property.

00:38:04.760 --> 00:38:07.890
And in the AudioRouteChange property,
it's going to tell you

00:38:07.890 --> 00:38:09.380
two bits of information.

00:38:09.380 --> 00:38:12.340
It tells you why the route has changed.

00:38:12.380 --> 00:38:14.780
Let's say the route has been
removed because the user has

00:38:14.780 --> 00:38:15.920
unplugged their headphones.

00:38:15.920 --> 00:38:21.090
And also it will tell you what
it was before the route changed.

00:38:21.240 --> 00:38:24.760
So if you're going, say,
from speaker to headphones,

00:38:24.760 --> 00:38:28.160
this would tell you speaker
and the reason would be,

00:38:28.160 --> 00:38:29.960
well, you know,
there's a new route available.

00:38:30.060 --> 00:38:33.790
And then you can get the current route,
which would now be headphones because

00:38:33.790 --> 00:38:35.680
the user plugged in headphones.

00:38:35.680 --> 00:38:38.250
And of course,
the reverse if you remove them.

00:38:42.780 --> 00:38:45.350
And so what if you
don't use audio session?

00:38:45.350 --> 00:38:48.060
So this is the exception
I was talking about earlier.

00:38:48.060 --> 00:38:51.560
If you're just playing system sounds,
you don't need to know

00:38:51.560 --> 00:38:53.480
anything about audio sessions.

00:38:53.480 --> 00:38:54.780
So half of you could have left.

00:38:54.930 --> 00:38:56.430
You're wasting your time here.

00:38:56.480 --> 00:38:58.790
Good to see you're still awake.

00:38:59.100 --> 00:39:00.680
That's very reassuring.

00:39:00.680 --> 00:39:03.420
I thought I might be
putting you all to sleep.

00:39:05.160 --> 00:39:08.520
If you're just using system sounds,
you do not need to know about audio

00:39:08.520 --> 00:39:11.900
session because system sounds,
the audio services, play system,

00:39:11.940 --> 00:39:15.230
sound play, alert sound,
they're just going to mix in.

00:39:15.230 --> 00:39:18.860
They're just going to mix in with
whatever else is going on in the system.

00:39:18.860 --> 00:39:22.350
But if you're doing anything else,
you probably do want to know

00:39:22.350 --> 00:39:25.700
about audio session because if
you just open a cue and start to

00:39:25.700 --> 00:39:28.850
play it back or start to record,
then you're going to make

00:39:28.850 --> 00:39:30.680
everything else be quiet.

00:39:30.680 --> 00:39:33.230
And if you really don't want to do that,
then you need to know

00:39:33.350 --> 00:39:34.520
about audio session.

00:39:35.540 --> 00:39:38.360
If your application is running
happily playing back a file

00:39:38.360 --> 00:39:41.230
and the user gets a call,
you're not going to know we

00:39:41.240 --> 00:39:42.930
stopped your application.

00:39:42.990 --> 00:39:45.850
And you also won't know when
you can start again because,

00:39:45.930 --> 00:39:48.230
well,
you don't even know you were stopped.

00:39:48.330 --> 00:39:51.640
So all the rest of the
APIs are great and it's really,

00:39:51.640 --> 00:39:56.500
you know, fine for bring up and for doing
your testing and development.

00:39:56.500 --> 00:40:00.180
But you really probably don't want
to ship your application this way.

00:40:00.180 --> 00:40:03.650
You really want to tie in and use
audio session to really make sure

00:40:03.650 --> 00:40:04.820
that you're not getting a call.

00:40:04.840 --> 00:40:08.460
So all the rest of the
APIs are great and it's really,

00:40:08.620 --> 00:40:13.820
you know, fine for bring up and for doing
your testing and development.

00:40:13.820 --> 00:40:19.180
But you really probably don't want
to ship your application this way.

00:40:19.180 --> 00:40:19.180
You really provide a good
integration experience with the

00:40:19.180 --> 00:40:19.180
rest of the stuff going on and with
how users are using their phone.

00:40:19.180 --> 00:40:20.480
So now I'm going to demonstrate
some of this in action.

00:40:20.480 --> 00:40:20.480
So if we can go to the projector.

00:40:36.140 --> 00:40:40.100
I'm going to start playing
something in the iPod app.

00:40:40.120 --> 00:40:41.340
This is only a test.

00:40:41.350 --> 00:40:44.590
And then I'm going to go
to the Notes application.

00:40:44.640 --> 00:40:46.080
This is a test.

00:40:46.800 --> 00:40:51.110
And I'm just going to
type some text in here.

00:40:51.110 --> 00:40:56.250
And you can hear the clicks coming
in from the keyboard as we're typing.

00:40:56.310 --> 00:40:57.790
The iPod is still playing.

00:40:57.800 --> 00:41:00.830
So this is system sounds
just playing back,

00:41:00.830 --> 00:41:01.800
mixing in.

00:41:01.860 --> 00:41:06.760
The Notes application knows
nothing about audio sessions.

00:41:07.570 --> 00:41:11.770
So now I'm going to go to AQ Touch,
the example we had before,

00:41:11.800 --> 00:41:13.300
and I'm going to start playing this file.

00:41:13.300 --> 00:41:19.020
Now this is an AAC MP4 file and this
is using the media playback category.

00:41:19.020 --> 00:41:24.890
So it's interrupted the iPod,
the iPod has stopped playing and

00:41:24.890 --> 00:41:27.970
now I'm starting to play my sound.

00:41:28.350 --> 00:41:29.500
Now I'm going to stop it.

00:41:29.600 --> 00:41:32.940
The iPod isn't resuming,
that's the behavior of the iPod.

00:41:32.940 --> 00:41:36.060
It's not going to resume
after that interruption.

00:41:36.130 --> 00:41:40.460
Now I'm going to kill the volume here
because I want to do a couple of things.

00:41:40.550 --> 00:41:42.640
It's like having a patient
on an operating table here.

00:41:42.640 --> 00:41:48.010
So now I'm going to start playing
and I'm just using the speaker.

00:41:49.640 --> 00:41:51.540
You're going to have to trust me.

00:41:51.540 --> 00:41:52.850
You can see the metering, right?

00:41:52.850 --> 00:41:55.420
So now I'm going to
plug in the headphones.

00:41:55.420 --> 00:41:57.190
Let me kill that for a bit.

00:41:57.220 --> 00:41:58.000
Oops.

00:41:58.100 --> 00:41:59.600
Caught that.

00:41:59.900 --> 00:42:01.800
So I'm plugging in the headphones.

00:42:01.800 --> 00:42:02.550
Oh, hang on.

00:42:02.620 --> 00:42:03.170
Let me try.

00:42:03.350 --> 00:42:04.290
Sorry about that.

00:42:04.380 --> 00:42:05.920
I'm going to play this again.

00:42:05.920 --> 00:42:07.140
So I'm on speaker.

00:42:09.770 --> 00:42:12.810
plugging in the headphones and
now the sound is still kept

00:42:12.810 --> 00:42:17.570
playing and it's playing through
the headphone speaker now.

00:42:18.040 --> 00:42:20.050
Now what happens when
I pull the headphones out?

00:42:20.060 --> 00:42:21.680
I'll kill the audio there.

00:42:21.680 --> 00:42:23.480
And it stops.

00:42:23.480 --> 00:42:30.100
And the reason it stops is because
the application has set up a listener

00:42:30.100 --> 00:42:31.600
for the route change notification.

00:42:31.600 --> 00:42:35.470
And it's seen that the route
change has come in when we

00:42:35.570 --> 00:42:37.030
pulled the headphones out.

00:42:37.270 --> 00:42:41.320
The reason for the route change
is that the old route was removed.

00:42:41.380 --> 00:42:43.530
So it's using that as a cue to stop.

00:42:44.180 --> 00:42:47.210
When we plug the headphones in,
the reason for the route

00:42:47.210 --> 00:42:50.340
change wasn't removed,
so we didn't actually do anything.

00:42:50.340 --> 00:42:53.570
And this is actually
how the iPod behaves.

00:42:53.580 --> 00:42:56.020
If the user plugs in headphones,
it keeps playing.

00:42:56.020 --> 00:42:58.800
If the user removes headphones,
it stops playing.

00:42:58.800 --> 00:43:03.780
And this is how this behavior
is implemented in the iPhone,

00:43:03.780 --> 00:43:05.560
in the iPod app.

00:43:05.710 --> 00:43:10.600
So now what I'm going to do is to show
you how to deal with interruptions.

00:43:10.690 --> 00:43:13.640
So I'm going to first
go to a clock alarm.

00:43:13.960 --> 00:43:15.520
Here.

00:43:15.520 --> 00:43:18.400
And let's see if we can make that.

00:43:18.440 --> 00:43:22.160
Okay.

00:43:22.160 --> 00:43:23.590
Okay.

00:43:28.890 --> 00:43:36.480
So I'm playing happily away here and very
shortly an alarm is going to fire and my

00:43:36.480 --> 00:43:38.800
application is going to be interrupted.

00:43:38.800 --> 00:43:40.800
Now have a listen to the sound.

00:43:40.800 --> 00:43:44.800
You'll notice we've got a
red stop button up and oh,

00:43:44.800 --> 00:43:46.800
looks like I might have been too late.

00:43:46.800 --> 00:43:49.790
Let me go and reset that alarm again.

00:43:50.670 --> 00:43:53.590
I knew I was going to be
cutting that one fine.

00:43:53.590 --> 00:43:56.310
Let me see how much time I've got.

00:43:56.400 --> 00:43:56.530
Okay.

00:43:56.620 --> 00:44:03.600
So I'm going to add an alarm here and
it's at 4:19 which should be no good.

00:44:03.600 --> 00:44:05.360
I'm going to edit that one.

00:44:05.360 --> 00:44:06.590
I'm going to add another one.

00:44:06.590 --> 00:44:08.270
That's what I've got to do.

00:44:08.580 --> 00:44:11.140
Otherwise you're going
to be here all day.

00:44:12.100 --> 00:44:13.640
"I really don't want that.

00:44:13.670 --> 00:44:17.790
Okay, so let's try that again.

00:44:17.790 --> 00:44:17.790
Set that.

00:44:17.970 --> 00:44:21.580
Here's our car, we're all revving up,
ready to go.

00:44:21.580 --> 00:44:25.330
So at the moment you
can see we're metering,

00:44:25.330 --> 00:44:28.930
we've got the red stop button
up because that would be the

00:44:28.940 --> 00:44:31.010
gesture we want the user to do.

00:44:31.560 --> 00:44:34.740
When the alarm comes in,
it's going to interrupt our

00:44:34.740 --> 00:44:38.600
application and what the
application is going to do...

00:44:40.140 --> 00:44:40.480
Wake up.

00:44:40.660 --> 00:44:45.440
So what the application did,
as you can see,

00:44:45.440 --> 00:44:47.790
is that the button changed
because it got told,

00:44:47.790 --> 00:44:48.960
oh, you've been interrupted.

00:44:49.080 --> 00:44:51.250
You're no longer playing any audio.

00:44:51.250 --> 00:44:58.270
So it changed the button to its
play state so that it can stop.

00:44:58.320 --> 00:45:02.780
Now, if I dismiss the alarm,
we're going to get an end interruption

00:45:02.780 --> 00:45:05.100
and we go back to playing again.

00:45:05.100 --> 00:45:08.830
And I'm going to stop that so
I don't have to talk over it.

00:45:09.520 --> 00:45:14.130
And this behavior has just been
programmed into the app and this

00:45:14.360 --> 00:45:18.230
is how we decided that we would
respond to the interruption.

00:45:18.390 --> 00:45:21.400
So we can go back to slides, please.

00:45:21.400 --> 00:45:25.560
Okay.

00:45:33.500 --> 00:47:12.900
[Transcript missing]

00:47:13.300 --> 00:47:17.200
Open AL works in a fairly
straightforward way.

00:47:17.350 --> 00:47:21.370
You create a device which is where
you're going to render the audio on.

00:47:21.860 --> 00:47:25.310
For Open AL on the phone,
there's just one device,

00:47:25.430 --> 00:47:27.040
which is the system audio device.

00:47:27.040 --> 00:47:30.820
Then you create an Open AL context,
and this is really the mixer.

00:47:30.970 --> 00:47:33.200
This is where all of the work is done.

00:47:33.200 --> 00:47:35.020
It's the rendering engine.

00:47:35.020 --> 00:47:39.170
And the listener is
implicit to the context.

00:47:39.940 --> 00:47:43.150
And then the way you work is that
you add sources to the context.

00:47:43.280 --> 00:47:46.100
And the sources, of course,
are sources of sound, of audio.

00:47:46.100 --> 00:47:50.650
So you generate the sources,
and then you generate the buffers.

00:47:50.650 --> 00:47:54.040
So you create buffers to
put the audio data in.

00:47:54.040 --> 00:47:55.680
You put the audio data in them.

00:47:55.680 --> 00:47:58.670
And then you cue the
buffers up to the sources,

00:47:58.670 --> 00:48:00.350
and then you play them.

00:48:00.390 --> 00:48:01.620
So it's pretty straightforward.

00:48:01.620 --> 00:48:04.080
It's somewhat similar
to AudioCue as well.

00:48:04.080 --> 00:48:07.520
And many of the sound playing
APIs have got this notion

00:48:07.640 --> 00:48:09.560
of sort of cues and buffers.

00:48:09.940 --> 00:48:14.080
With Open AL, of course,
then you can position the audio to

00:48:14.240 --> 00:48:18.240
different locations at any time,
so you can move sounds around.

00:48:18.440 --> 00:48:21.800
And you use a 3D coordinate
system to do that.

00:48:22.040 --> 00:48:27.530
Now there's two extensions
that we provide with Open AL,

00:48:27.530 --> 00:48:29.780
and this is actually true
on the desktop as well,

00:48:29.780 --> 00:48:32.970
because both of them are
useful in both situations.

00:48:33.080 --> 00:48:35.300
There's a static buffer extension.

00:48:35.370 --> 00:48:39.340
The static buffer extension allows
you to get access to the audio data.

00:48:39.340 --> 00:48:43.560
And then you have the buffer
pointer memory that's used in the

00:48:43.560 --> 00:48:46.140
actual buffered objects in Open AL.

00:48:46.340 --> 00:48:49.840
So instead of writing into a buffer
that is then copied into those,

00:48:49.840 --> 00:48:52.920
you get access to the
buffer data directly.

00:48:53.020 --> 00:48:57.310
So this is a very good efficiency,
and we strongly recommend that you use

00:48:57.490 --> 00:48:59.740
this version of the AL buffer data core.

00:48:59.940 --> 00:49:02.940
And then if you've got a
lot of sources in your game,

00:49:02.940 --> 00:49:06.930
and you're quite content to mix
them at a different sample rate

00:49:07.040 --> 00:49:11.070
than the device might be at,
then we would also recommend

00:49:11.250 --> 00:49:16.650
that you use the mixer output
sample rate extension for Open AL.

00:49:17.000 --> 00:49:20.940
And an example of this would be,
let's say you had sounds that there's not

00:49:20.940 --> 00:49:25.740
really high frequency content in them,
so 22 kilohertz is perfectly fine.

00:49:25.740 --> 00:49:29.810
But the device might be running
at 44K because you want to play

00:49:29.810 --> 00:49:31.740
music as well with the iPod.

00:49:31.780 --> 00:49:33.790
So in this case, you could have,
you know,

00:49:33.790 --> 00:49:35.650
all your sources running at 22K.

00:49:35.760 --> 00:49:38.140
You could be mixing them at 22K.

00:49:38.140 --> 00:49:41.000
And then you would just do
one sample rate conversion

00:49:41.000 --> 00:49:43.130
to get to 44K for playback.

00:49:43.140 --> 00:49:49.740
So this is a very useful extension
to understand and to use.

00:49:49.790 --> 00:49:53.270
So I'm going to go to a demo,
the slides will click,

00:49:53.270 --> 00:49:58.650
and go over to OpenAO so we can
have the projector up again.

00:50:08.640 --> 00:50:13.570
Okay, I'll just start this playing here.

00:50:13.600 --> 00:50:21.310
So this is a very simple example of
OpenAL and the reason we wanted to

00:50:21.370 --> 00:50:25.420
provide this is to give you a very
simple code base where you can just

00:50:25.430 --> 00:50:31.120
look at what it takes to play one sound
and just get a real flavor for the API.

00:50:31.120 --> 00:50:36.180
Now I can just use this sound
to move it from left to right.

00:50:36.200 --> 00:50:38.440
I presume that's panning.

00:50:38.600 --> 00:50:43.800
I can put the sound closer or
I can move it further away,

00:50:43.800 --> 00:50:50.600
which is a notion of a 3D space
that the sound is occurring within.

00:50:50.600 --> 00:50:59.020
Now we talked in the section just before
about the ambient category and the

00:50:59.210 --> 00:51:02.490
ambient category is good if you want
to have music playing in your game.

00:51:02.500 --> 00:51:06.890
So where the music is coming from,
say the user's iPod,

00:51:06.890 --> 00:51:08.500
they may have a bunch of applications.

00:51:08.680 --> 00:51:10.490
They may have a playlist that
they want to use or whatever.

00:51:10.500 --> 00:51:15.090
So in this case,
as compared to AQ Touch where

00:51:15.090 --> 00:51:18.980
the music stopped,
when we make sound with this application,

00:51:18.980 --> 00:51:22.840
it's going to let the iPod
keep playing in the background.

00:51:22.840 --> 00:51:27.460
I get the full panning control of OpenAL.

00:51:27.460 --> 00:51:31.820
I can go from left to right
and in and out and so forth.

00:51:31.820 --> 00:51:35.300
And the user can also control the iPod.

00:51:35.300 --> 00:51:37.380
They can bring up the
HUD by the double click.

00:51:37.500 --> 00:51:42.340
They can stop the... They can stop the
iPod and just go back to their game,

00:51:42.340 --> 00:51:43.380
which I seem to have lost.

00:51:43.500 --> 00:51:45.220
Oh well, it doesn't matter.

00:51:45.220 --> 00:51:50.330
You can... But you'll see how you can
really just keep the iPod going and

00:51:50.400 --> 00:51:55.720
the game will still go and the user can
bring the iPod up and play sound or not.

00:51:58.120 --> 00:52:03.940
We go back to slides,
which we're already at.

00:52:04.200 --> 00:52:04.200
Back to slides.

00:52:04.200 --> 00:52:04.200
Thank you.

00:52:05.050 --> 00:52:09.270
So the last section that I want
to get into is audio units.

00:52:09.560 --> 00:52:19.270
Audio units are really the foundations
for iPhone development and these

00:52:19.270 --> 00:52:25.690
represent a kind of a lower level of
the Core Audio services for you to use.

00:52:29.610 --> 00:52:32.420
It gives you a lot more control
over the rendering behavior.

00:52:32.420 --> 00:52:36.610
The rendering is done in the audio
units case in your application.

00:52:36.690 --> 00:52:40.220
And it will probably give
you more code to write,

00:52:40.230 --> 00:52:43.420
but it gives you much greater control.

00:52:43.420 --> 00:52:47.440
If you want to do lower latency
input to output or output processing,

00:52:47.560 --> 00:52:50.940
then this is probably a
service you need to know about.

00:52:50.940 --> 00:52:54.940
If you want to do mixing and
to do open AL types of things,

00:52:55.030 --> 00:52:59.480
then audio units are actually the
implementation that we're using.

00:52:59.500 --> 00:53:02.010
doing OpenAL in.

00:53:03.100 --> 00:53:06.950
So in order to use audio units,
you need to understand something

00:53:06.950 --> 00:53:09.540
about the data format that
we're using on the phone.

00:53:09.540 --> 00:53:13.160
And linear PCM, of course,
is the uncompressed audio format.

00:53:13.160 --> 00:53:17.740
And there's actually two canonical
PCM formats that are used on the phone.

00:53:17.740 --> 00:53:22.130
There's a 16-bit format,
which is the device format,

00:53:22.130 --> 00:53:25.360
the IO format that's used on the device.

00:53:25.380 --> 00:53:29.020
And then audio units are
using an 8.24 fixed point.

00:53:29.020 --> 00:53:32.980
The 8 bits are the integer part,
the 24 bits are the fractional part.

00:53:33.000 --> 00:53:36.060
And the reason that we're using
fixed point is that you've got

00:53:36.060 --> 00:53:39.650
some headroom for mixing and doing
processing and that kind of thing.

00:53:39.850 --> 00:53:44.280
Now, there's no floating point
sample support on the phone.

00:53:44.280 --> 00:53:48.450
There's a considerable difference
in the CPU on the phone between

00:53:48.560 --> 00:53:52.800
doing floating point calculation
and integer calculations.

00:53:52.800 --> 00:53:57.250
And the penalty for doing floating
point is really too much for the

00:53:57.250 --> 00:53:59.800
types of audio that you want to do.

00:53:59.800 --> 00:54:05.720
So this is the reason why we've
decided to look at a fixed point

00:54:05.720 --> 00:54:09.870
and integer-based format for audio.

00:54:10.040 --> 00:54:14.130
Formats are generally described using
an AudioStream basic description,

00:54:14.220 --> 00:54:16.720
which is declared in
the Cordio types header,

00:54:16.720 --> 00:54:20.760
and this is the header that's used
for all of the basic types in Cordio.

00:54:24.160 --> 00:54:27.970
So audio units are published
in the Audio Unit Framework

00:54:28.120 --> 00:54:31.460
and they're pluggable,
they're bits of code.

00:54:31.540 --> 00:54:36.000
They're basically discrete
modules that you can load and

00:54:36.000 --> 00:54:37.510
use for different activities.

00:54:37.520 --> 00:54:43.000
So we have audio units available to
you to use for doing I.O., for doing

00:54:43.500 --> 00:54:46.500
mixing and there's an EQ effect as well.

00:54:49.310 --> 00:54:53.960
On the desktop environment,
audio units are used to also allow

00:54:54.010 --> 00:54:58.780
third-party plug-ins to run in host
applications like Logic and GarageBand.

00:54:58.780 --> 00:55:02.580
We're not supporting this at this
point in time for the iPhone,

00:55:02.580 --> 00:55:06.600
so there's no third-party audio units
that will be found and discovered.

00:55:06.600 --> 00:55:09.830
You, of course,
could implement code using audio units

00:55:09.830 --> 00:55:14.330
and just load that code yourself,
but we're not providing any discovery

00:55:14.400 --> 00:55:17.080
process for third-party audio units.

00:55:17.680 --> 00:55:20.380
Because of that,
there's no need for us to

00:55:20.380 --> 00:55:24.000
also do custom UI support,
which is a common feature on the

00:55:24.000 --> 00:55:25.900
desktop version of audio units.

00:55:25.980 --> 00:55:29.970
So the audio unit is really there
as an API for you to use for

00:55:30.360 --> 00:55:32.720
discrete activities and tasks.

00:55:32.770 --> 00:55:38.660
And probably the one you will need to
know about most is the AU Remote I.O.

00:55:38.660 --> 00:55:43.460
This is how you get audio in and
out of the system at this level.

00:55:43.460 --> 00:55:45.720
Now,
if you've used the desktop environment,

00:55:45.720 --> 00:55:47.360
it's very similar to AU Health.

00:55:47.580 --> 00:55:51.580
It's a very simple interface,
and it works in the very same way.

00:55:51.580 --> 00:55:56.630
It has an input bus and an output
bus in terms of the device.

00:55:56.900 --> 00:56:00.700
And it works in a just-in-time notion.

00:56:00.700 --> 00:56:04.590
And what I mean by that is that the
work that you do with this audio

00:56:04.590 --> 00:56:07.840
unit is done on a real-time thread,
on a time-constrained thread.

00:56:08.210 --> 00:56:11.500
That thread has a deadline to meet,
and that deadline is

00:56:11.500 --> 00:56:13.080
not half a second away.

00:56:13.180 --> 00:56:16.440
It's normally some small
number of milliseconds away.

00:56:16.560 --> 00:56:17.550
So this puts me in a
position where I can do that.

00:56:17.650 --> 00:56:20.500
It puts very strong constraints
on the kinds of things you can do.

00:56:20.580 --> 00:56:24.110
For instance, you cannot block,
you cannot read or write from the

00:56:24.110 --> 00:56:26.700
thread that the AU Remote I.O.

00:56:26.700 --> 00:56:27.830
is going to call you on.

00:56:27.900 --> 00:56:31.730
And there's a whole host of
things you have to be careful of.

00:56:31.940 --> 00:56:35.650
Allocating memory is another
activity you can't do on this thread.

00:56:35.900 --> 00:56:37.500
Now, AU Remote I.O.

00:56:37.500 --> 00:56:40.930
also will do sample rate
conversion from the format that

00:56:40.940 --> 00:56:44.610
you're using as a client of it,
and the format that the

00:56:44.610 --> 00:56:46.900
device is currently set to.

00:56:46.900 --> 00:56:49.900
And then we provide mixer audio units.

00:56:50.000 --> 00:56:53.140
There's a stereo mixer,
and it will take either 16-bit

00:56:53.140 --> 00:56:55.830
or 8.24 fixed-point inputs.

00:56:55.900 --> 00:56:58.900
Those inputs can have mono or stereo.

00:56:58.900 --> 00:57:02.580
And then the parameters you
have for this mixer is a volume

00:57:02.580 --> 00:57:04.680
control and enable on each input.

00:57:04.900 --> 00:57:06.900
The enable can be seen as like a muting.

00:57:06.900 --> 00:57:08.900
It still leaves that input active.

00:57:08.900 --> 00:57:10.870
It's just not being called for data.

00:57:11.040 --> 00:57:14.320
And you can enable it and disable it,
and we take care of ramping it

00:57:14.430 --> 00:57:15.900
up and down so you don't glitch.

00:57:16.900 --> 00:57:21.900
And then it has a single 8.24
fixed-point output in stereo,

00:57:21.900 --> 00:57:26.900
and then you can connect that up as you
can with audio units to the remote I.O.,

00:57:26.900 --> 00:57:30.900
and then that gives you an ability to do
mixing of multiple sources to one output.

00:57:30.900 --> 00:57:35.900
Now, OpenAL is implemented on top
of the 3D embedded mixer,

00:57:35.900 --> 00:57:39.060
and this is also an audio unit that
you can just use on your own if

00:57:39.060 --> 00:57:43.890
you want to do something different
than what you can do with OpenAL.

00:57:43.900 --> 00:57:45.900
So the 3D embedded mixer
gives you a lot of options.

00:57:45.950 --> 00:57:49.900
It gives you inputs
of either 8 or 16 bit,

00:57:49.900 --> 00:57:51.780
mono or stereo.

00:57:52.010 --> 00:57:55.900
It has a lot more parameters
available for each input.

00:57:55.900 --> 00:57:57.890
It has a volume control.

00:57:57.930 --> 00:57:59.900
You can enable inputs.

00:57:59.900 --> 00:58:02.900
You can also pan using
3D coordinate space.

00:58:02.900 --> 00:58:04.900
It has a distance attenuation.

00:58:04.920 --> 00:58:10.180
That was the sound getting quieter
and louder as the sound was

00:58:10.180 --> 00:58:12.900
further away from the listener.

00:58:12.900 --> 00:58:14.900
You can also do rate control.

00:58:14.900 --> 00:58:17.900
Rate control can be used
to simulate Doppler,

00:58:17.900 --> 00:58:23.400
so you have a pitch change as the
sound comes to you and goes away.

00:58:23.520 --> 00:58:26.910
You can do rate conversion on inputs,
but really the rate conversion

00:58:26.910 --> 00:58:31.060
is a side effect of the fact that
we have a rate control parameter.

00:58:31.060 --> 00:58:34.430
And we really don't recommend that you do
a lot of rate conversion on the inputs.

00:58:34.520 --> 00:58:35.500
It's expensive.

00:58:35.500 --> 00:58:39.260
It's better to get all of your data
at the same sample rate if you can.

00:58:39.260 --> 00:58:41.830
And then it gives you
a fixed point output,

00:58:41.980 --> 00:58:45.800
8.24 stereo output,
just like the other mixer.

00:58:45.800 --> 00:58:47.050
Bill Stewart

00:58:47.500 --> 00:58:49.100
There's the iPod EQ.

00:58:49.140 --> 00:58:54.440
The iPod EQ is going to
do 8.24 input and output.

00:58:54.440 --> 00:58:58.250
It's the same EQ that's
used with the iPod settings.

00:58:58.250 --> 00:59:01.310
So if you go into settings,
there's EQ settings there.

00:59:01.320 --> 00:59:05.190
And the iPod EQ just gives you
a collection of presets and

00:59:05.340 --> 00:59:09.480
this will match what the user
can select for iPod playback.

00:59:11.390 --> 00:59:14.030
So there's two ways that you
can interact with audio units.

00:59:14.030 --> 00:59:18.730
There's using the AU Graph API,
and the AU Graph API is a little

00:59:18.730 --> 00:59:23.480
bit more abstract than directly
using Audio Unit API directly.

00:59:23.480 --> 00:59:27.930
And the way you do work with a
graph is that you create a graph,

00:59:27.930 --> 00:59:32.390
you create a collection of nodes,
and then you start and stop the graph.

00:59:32.440 --> 00:59:35.460
And the collection of the nodes,
you connect up to each other

00:59:35.540 --> 00:59:39.220
to form your signal chain,
your processing chain, processing graph.

00:59:40.190 --> 00:59:43.590
Now, one of the features that the graph
does that's very useful is that it

00:59:43.740 --> 00:59:47.030
provides you the ability to interact
with the graph while it's running.

00:59:47.270 --> 00:59:49.350
It takes care of the threading issues.

00:59:49.490 --> 00:59:53.480
So when you add and remove nodes
or add and remove callbacks for

00:59:53.480 --> 00:59:57.640
inputs and things like this,
you're able to do that

00:59:57.640 --> 01:00:01.260
while the graph is running,
and you're not going to get

01:00:01.340 --> 01:00:04.350
breakups in your audio or other
problems because of the different

01:00:04.360 --> 01:00:06.440
threads that are involved in this.

01:00:06.500 --> 01:00:08.980
If you want to deal with
the audio units directly,

01:00:08.980 --> 01:00:13.220
you can use the Audio Unit API as it is.

01:00:13.310 --> 01:00:16.260
And because the component
manager is not available,

01:00:16.260 --> 01:00:21.800
we have a new mechanism for publishing
the components that represent the

01:00:21.800 --> 01:00:24.250
audio units that you're going to use.

01:00:24.370 --> 01:00:26.020
So this is the audio component.

01:00:26.020 --> 01:00:27.020
It's a new API.

01:00:27.020 --> 01:00:30.260
It's available both in
Snow Leopard and on the iPhone.

01:00:30.320 --> 01:00:32.200
And it's how you find the components.

01:00:32.230 --> 01:00:34.190
And the components are sort of factories.

01:00:34.210 --> 01:00:36.600
They're class objects in a way.

01:00:36.900 --> 01:00:37.820
And then once you've
found the components,

01:00:37.820 --> 01:00:41.820
which you find by describing it.

01:00:41.910 --> 01:00:44.060
So you say, well,
I want to find the component

01:00:44.060 --> 01:00:49.180
that represents the mixer or
the iPod EQ or the AU Remote IO.

01:00:49.240 --> 01:00:52.840
Once I've found that component,
I then want to create an

01:00:52.840 --> 01:00:54.940
instance of that component.

01:00:54.940 --> 01:00:57.190
I want that factory to
make me an instance.

01:00:57.230 --> 01:01:00.730
And this is the audio unit itself.

01:01:01.730 --> 01:01:05.750
So the example that I want to
show you now is a duplex I.O.

01:01:05.750 --> 01:01:06.600
example.

01:01:06.600 --> 01:01:11.480
And what this does is it takes
input and it copies it to the output

01:01:11.700 --> 01:01:14.360
and it just does a simple I.O.

01:01:14.360 --> 01:01:16.360
and it uses AU Remote I.O.

01:01:16.360 --> 01:01:17.280
to do this.

01:01:18.200 --> 01:01:22.830
The way that it works very briefly is
that the callback fires to say that

01:01:22.970 --> 01:01:25.420
there's data there for the input.

01:01:25.420 --> 01:01:29.220
You call Audio Unit Render
on Render Bus 1,

01:01:29.220 --> 01:01:31.530
which is the input side.

01:01:31.540 --> 01:01:34.480
And then you pass that
data to the callback,

01:01:34.480 --> 01:01:37.060
which is going to be used for output.

01:01:37.060 --> 01:01:41.320
And this example is provided as a
starting point for you just to get

01:01:41.470 --> 01:01:44.140
some idea of how do you use Audio Unit.

01:01:44.140 --> 01:01:47.170
So if we can go to slides
and we'll go to the demo.

01:02:00.690 --> 01:02:03.930
So this is AURIO Touch.

01:02:03.930 --> 01:02:07.160
And what I'm displaying here
is an oscilloscope that's

01:02:07.230 --> 01:02:09.760
representing the audio coming in.

01:02:09.790 --> 01:02:16.650
I can zoom it out so that the
time granularity is a lot longer.

01:02:16.650 --> 01:02:21.480
I can zoom it in so it's covering,
you know,

01:02:21.480 --> 01:02:24.760
very small and live piece of data.

01:02:24.760 --> 01:02:28.750
Now, if we could cut my microphone.

01:02:34.970 --> 01:02:43.090
Taking audio in and pushing it
through this cable into the output.

01:02:43.980 --> 01:02:44.900
Thank you.

01:02:44.900 --> 01:02:45.890
I think I'm back online.

01:02:46.010 --> 01:02:49.460
So I've got a mute button here
and as I hit the mute button it,

01:02:49.510 --> 01:02:51.580
you know, played a little sound.

01:02:51.580 --> 01:02:55.730
That's using the audio services
play system sound call.

01:02:55.890 --> 01:02:58.240
We thought we'd have a little
bit of fun with this app,

01:02:58.410 --> 01:03:02.840
so what you see here now is actually a
spectrogram of the input that's coming

01:03:02.840 --> 01:03:07.190
in and we're doing an FFT of the data.

01:03:09.810 --> 01:03:12.500
And you can sort of see
the yellow lines there.

01:03:12.790 --> 01:03:15.340
That's my whistle tone.

01:03:15.340 --> 01:03:18.700
It's a pretty noisy environment up here,
so it's picking up a lot of data.

01:03:18.700 --> 01:03:19.920
Maybe if I sort of move it.

01:03:21.200 --> 01:03:43.100
[Transcript missing]

01:03:47.390 --> 01:03:48.300
And thank you.

01:03:48.300 --> 01:03:50.840
And as I said,
all of this code is available

01:03:50.970 --> 01:03:53.990
for you to download,
so you can try this app yourself.

01:03:53.990 --> 01:03:58.760
You'll notice a difference when you
plug in the headphones and the headset.

01:03:58.800 --> 01:04:01.910
You'll get a much cleaner signal than
you can from the mic and so forth.

01:04:01.960 --> 01:04:04.460
So it's really quite a
lot of fun to play with.

01:04:06.290 --> 01:04:09.970
Thank you very much.

01:04:10.070 --> 01:04:10.980
I'm basically done.

01:04:10.980 --> 01:04:13.460
Sorry we've run a little bit over.

01:04:13.460 --> 01:04:18.010
To just summarize what we've been
doing is looking at audio toolbox.

01:04:18.190 --> 01:04:22.480
We've looked at AudioCue as a
primary source for input and output.

01:04:22.480 --> 01:04:24.760
We used AudioFile with this.

01:04:24.870 --> 01:04:29.800
We had a look at audio services,
both the session and system sound APIs,

01:04:29.900 --> 01:04:34.640
OpenAL for games, AudioUnit if you want
to do low-level audio.

01:04:36.050 --> 01:04:37.520
There's a couple of related sessions.

01:04:37.590 --> 01:04:39.940
There was the introduction
to game development,

01:04:39.940 --> 01:04:43.560
which puts audio in the broader
context of doing your game.

01:04:43.560 --> 01:04:48.390
Now I've skimmed over a lot of details
about the Core Audio API set in general,

01:04:48.520 --> 01:04:52.410
and the understanding
Core Audio architecture of session is

01:04:52.510 --> 01:04:56.960
very useful in giving you much more
detail about how Core Audio works,

01:04:56.960 --> 01:05:01.580
how you can use the APIs and get a
sense of how it's all put together.

01:05:01.580 --> 01:05:03.380
We have a lab tomorrow afternoon.

01:05:03.380 --> 01:05:07.360
We'll all be there, bring your code,
bring your questions.

01:05:07.360 --> 01:05:10.040
I'm not going to read
the resources slide.

01:05:10.040 --> 01:05:12.360
I'll just leave it up
there while we do Q&A.

01:05:12.360 --> 01:05:14.740
Thank you very much.