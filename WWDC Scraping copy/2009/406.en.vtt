WEBVTT

00:00:12.720 --> 00:00:19.580
>> Good afternoon, welcome to the Programming
With Blocks in Grand Central Dispatch talk.

00:00:19.579 --> 00:00:21.239
My name is Blaine Garst.

00:00:21.239 --> 00:00:27.809
I'll be talking to you about blocks and for the most part
what they've been used for in other languages as well

00:00:27.809 --> 00:00:31.519
in Snow Leopard, in sort of traditional ways of programming.

00:00:31.519 --> 00:00:36.390
But something new for us, new for
you, is Grand Central Dispatch.

00:00:36.390 --> 00:00:39.890
My colleague Dave Zarzycki will
be up here later to tell you all

00:00:39.890 --> 00:00:43.850
about how you can use blocks to
deal with concurrency issues.

00:00:43.850 --> 00:00:52.090
But first let's talk about blocks-- start there.

00:00:52.090 --> 00:00:59.010
If you've ever programmed in a language
that required you to write this...

00:00:59.009 --> 00:01:02.640
[ Applause ]

00:01:02.640 --> 00:01:08.019
...you would have been writing in
Scheme or what turned into LISP.

00:01:08.019 --> 00:01:12.619
Perhaps later on in your career you saw something like this.

00:01:12.620 --> 00:01:17.150
Times repeat, you know, this funny thing in square brackets.

00:01:17.150 --> 00:01:19.700
Square brackets, what a syntax actually.

00:01:19.700 --> 00:01:22.329
And that would have been SmallTalk of course.

00:01:22.329 --> 00:01:31.890
And more recently if you kind of ride the
rails of Ruby, you also see some closures.

00:01:31.890 --> 00:01:35.519
So the funny thing about all of these languages though,

00:01:35.519 --> 00:01:41.459
is that they not only have garbage collection all
the time, but they're also mostly interpreted.

00:01:41.459 --> 00:01:48.849
And so that's not kind of something
that we're kind of used to.

00:01:48.849 --> 00:01:57.439
When we program in C or one of its derived languages,
Objective-C or C++, or for those of you who want to do both,

00:01:57.439 --> 00:02:04.469
Objective-C++, we have not had that kind of an option.

00:02:04.469 --> 00:02:06.170
Until now.

00:02:06.170 --> 00:02:16.039
In Snow Leopard we offer you blocks, which are our name
for closures, and what they let you do is write this kind

00:02:16.039 --> 00:02:25.030
of function expression and pass it to things, and
you'll see lots of things that we get to pass it to.

00:02:25.030 --> 00:02:32.909
And the neat thing about it is that it carries a
long local context, in this case the variable D.

00:02:32.909 --> 00:02:34.240
So where did D come from?

00:02:34.240 --> 00:02:41.650
Well that's just a local variable in your function or
something, and you pass this function expression along

00:02:41.650 --> 00:02:47.099
and it carries along that value of D so you can
just write your code the way you want to write it,

00:02:47.099 --> 00:02:53.280
and it gets passed off, and it gets
used as you wish it to be used.

00:02:53.280 --> 00:02:57.439
So let's take a look a little bit about this in more detail.

00:02:57.439 --> 00:03:00.879
So how would we implement the repeat function?

00:03:00.879 --> 00:03:07.359
So first of all, one of those blocks
looks kind of like a function pointer,

00:03:07.360 --> 00:03:10.930
except we use the carrot symbol instead of the star.

00:03:10.930 --> 00:03:17.969
But otherwise this is kind of like a function pointer
declaration, and so it ought to be somewhat familiar to you.

00:03:17.969 --> 00:03:25.000
We think that functions, the return values and take
parameters, are kind of complex to writing again and again

00:03:25.000 --> 00:03:28.919
and again in your APIs; and so we
quite often put a typedef around it.

00:03:28.919 --> 00:03:35.280
And so if we have this typedef for a block that
takes no arguments void and returns nothing,

00:03:35.280 --> 00:03:38.310
returns void, we get this work block thing.

00:03:38.310 --> 00:03:42.560
That's a very interesting concept,
it's just this chunk of work,

00:03:42.560 --> 00:03:45.810
and we'll see this used in a number of places later on.

00:03:45.810 --> 00:03:50.659
But anyway to complete the story, the
implementation of repeat is very simple.

00:03:50.659 --> 00:03:56.870
It takes one of those blocks as a parameter, and
then it just calls it like it was a function.

00:03:56.870 --> 00:04:01.430
And whatever you passed in happens, and so it's very simple.

00:04:01.430 --> 00:04:06.849
Both simple to create, simple to simply use.

00:04:06.849 --> 00:04:15.900
So closures came about in the 1970s
actually, pso that was 40 years ago.

00:04:15.900 --> 00:04:20.519
And so there's been kind of like 40
years worth of experience with closures.

00:04:20.519 --> 00:04:28.620
And so we now, in the C language, in a compiled environment,
get to take advantage of these kinds of programming patterns

00:04:28.620 --> 00:04:32.310
that have been around in other contexts for a long time.

00:04:32.310 --> 00:04:38.750
So one of them for example, is kind of what Ruby
specializes in, is writing little wrapper kinds of things.

00:04:38.750 --> 00:04:43.810
So you write this function, all lines in a
file, you pass it in the carrot star file name.

00:04:43.810 --> 00:04:49.610
Of course this works for Objective-C, but let's
do carrot star for now, and you pass in a block.

00:04:49.610 --> 00:04:54.930
And what this function does is, does
all the boiler plate, it opens the file.

00:04:54.930 --> 00:04:59.310
For every line in the file hands the line off to the block.

00:04:59.310 --> 00:05:01.009
This is a neat way to write.

00:05:01.009 --> 00:05:08.509
And then of course after the function is done, it closes
it, and so what you get to do is just write your code.

00:05:08.509 --> 00:05:13.360
Now you think, think, think, think, in the compute
section you use some of your thoughts earlier and stuff,

00:05:13.360 --> 00:05:20.850
and you just kind of write it, and it's a nice, neat way to
concisely express the idea that you can process each line

00:05:20.850 --> 00:05:24.900
of a file with this code that's just right there.

00:05:24.899 --> 00:05:30.069
Now you don't have to write this, I mean we know
how to write these kinds of things in C today.

00:05:30.069 --> 00:05:33.019
They look something like this.

00:05:33.019 --> 00:05:41.879
You've got your computation interspersed with all this
boiler plate, and it can be done, it's just, I think,

00:05:41.879 --> 00:05:50.620
more concise, easier to read, more utilitarian,
reusable, it's just better off this way with a block.

00:05:50.620 --> 00:06:01.389
So another very common, classic, lambda,
and whatever concept is the idea of a map.

00:06:01.389 --> 00:06:11.120
So here's a category on the NSArray class that does map,
and so map takes sort of a transform block, function block.

00:06:11.120 --> 00:06:21.000
And what it does is for every element in the array it
applies the transform block to that object, to that element,

00:06:21.000 --> 00:06:26.389
and accumulates the result into a result thing;
and then when it's done returns a result.

00:06:26.389 --> 00:06:29.300
And so that's your transform function.

00:06:29.300 --> 00:06:31.879
It's very simple to use, it's very cool.

00:06:31.879 --> 00:06:39.050
Another one is sort of the collect, whereas you pass in a
predicate which says for every element in the collection,

00:06:39.050 --> 00:06:43.910
test this predicate with the element, and
if it's true collect it, return this back.

00:06:43.910 --> 00:06:46.939
So this is sort of an array reducing kind of thing.

00:06:46.939 --> 00:06:53.329
So in use for example, you just
pass in return [item length] > 20;

00:06:53.329 --> 00:06:58.069
and you get all strings whose length is greater than 20.

00:06:58.069 --> 00:07:00.790
Now the syntax for this one is a little bit different.

00:07:00.790 --> 00:07:02.560
Notice the BOOL there.

00:07:02.560 --> 00:07:06.139
So we won't talk about the syntax in great detail,

00:07:06.139 --> 00:07:11.479
but BOOL in this case is the return
value coming back from that block.

00:07:11.480 --> 00:07:19.980
We don't use it very often, it's kind of a
corner case, but it's there if you need it.

00:07:19.980 --> 00:07:24.600
Now what I like about blocks in Snow
Leopard is some of the new uses.

00:07:24.600 --> 00:07:30.010
But let's go back, if you have to
do something as simple as sorting.

00:07:30.009 --> 00:07:35.930
Now let's think about sorting, I mean how many years
have computer scientists been refining the sort function?

00:07:35.930 --> 00:07:38.230
Like decades, right?

00:07:38.230 --> 00:07:41.710
None of us really like to write the sort function, right?

00:07:41.709 --> 00:07:43.959
So we want to use qsort of course.

00:07:43.959 --> 00:07:48.759
There's other variations, bsort and
other things, but let's talk about qsort.

00:07:48.759 --> 00:07:57.699
If you have to sort things according to different
criteria, like in this case what I'm going to do--

00:07:57.699 --> 00:08:02.789
well if you have to use different criteria, what
they require you to do to be fully parameterized,

00:08:02.790 --> 00:08:08.740
is to pass in this callback argument which you
set up and then every time you compare 2 elements,

00:08:08.740 --> 00:08:12.019
you're also passed back the thing you passed in.

00:08:12.019 --> 00:08:19.899
So you can put your context in this callback argument and
vary your sort based on runtime parameters of some kind.

00:08:19.899 --> 00:08:23.000
So let's take a look at what that looks like in practice.

00:08:23.000 --> 00:08:26.970
So the first thing you do is you declare a custom structure.

00:08:26.970 --> 00:08:29.150
OK, you put your things in here.

00:08:29.149 --> 00:08:35.319
In this case we're going to say sort some kind of a person
field, or object, or whatever, and maybe age first is one

00:08:35.320 --> 00:08:41.730
of the options and maybe first or last or capitalized or
not is another; but in any case, let's focus on age first.

00:08:41.730 --> 00:08:50.120
So you set up this custom data structure somewhere,
you write a custom compare function somewhere

00:08:50.120 --> 00:08:57.500
that takes this void star context coming back in, you put
lots of castes in and you ask, "Well if it's age first then

00:08:57.500 --> 00:08:59.990
of course I'm going to compare the age fields."

00:08:59.990 --> 00:09:02.100
You know, you do this.

00:09:02.100 --> 00:09:09.220
The third thing you do is somewhere near where you're
going to call qsort, you actually initialize the structure,

00:09:09.220 --> 00:09:13.980
you declare an instance of the structure,
initialize it with the right parameters,

00:09:13.980 --> 00:09:18.759
and then finally, finally, you get to call qsort_r.

00:09:18.759 --> 00:09:22.399
Remembering which order the context versus
the call, but you know-- you call it.

00:09:22.399 --> 00:09:24.100
So what do you have here?

00:09:24.100 --> 00:09:27.040
You've got your code in 4 different places.

00:09:27.039 --> 00:09:34.019
Now if you have to change your criteria, if you're actually
developing a program and you know, you change your mind

00:09:34.019 --> 00:09:39.679
or some new option comes up, you've got
to go change code in 3 different places.

00:09:39.679 --> 00:09:45.279
It can be done, it's tedious, might be in
separate files, they might-- it's just a pain.

00:09:45.279 --> 00:09:50.959
So what we like instead is a qsort_b
function, which you will find on Snow Leopard,

00:09:50.960 --> 00:09:54.780
and it simply takes a block as the 4th parameter.

00:09:54.779 --> 00:10:00.199
Now notice that these are the elements;
these are the things that matter to qsort.

00:10:00.200 --> 00:10:06.990
It's the array, it's how many elements in the array, it's
the width of each element in the array, and the sort block.

00:10:06.990 --> 00:10:13.690
There's no extraneous thing to confuse, what
does qsort want with this funny callback thing?

00:10:13.690 --> 00:10:15.080
You don't have that, you've got the block.

00:10:15.080 --> 00:10:16.720
It's very simple to understand this API.

00:10:16.720 --> 00:10:23.259
And when you use it, step 1 is, you set up your options,

00:10:23.259 --> 00:10:30.009
you use your variables as you wish in
your block, and there is no step 2.

00:10:30.009 --> 00:10:32.809
[ Laughter ]

00:10:32.809 --> 00:10:39.429
Right? I mean, if you need to change parameters up in
the options area, or down in the block, you just do it.

00:10:39.429 --> 00:10:43.209
And it's much cleaner, it's much nicer.

00:10:43.210 --> 00:10:46.540
Now this is a very simple callback scenario.

00:10:46.539 --> 00:10:53.379
We have more complexities, say with the timer or with
some networking APIs where you need to set in some kind

00:10:53.379 --> 00:10:57.939
of a function, and it needs to stick
around for a long time and get called back.

00:10:57.940 --> 00:11:04.890
In those cases the memory you use to set up that
context pointer is often really hard to reclaim.

00:11:04.889 --> 00:11:07.569
It's hard to know when you're done with it.

00:11:07.570 --> 00:11:09.610
But you've got to figure that out,
or else you're going to leak,

00:11:09.610 --> 00:11:15.950
and so that's also a problem that
is very neatly solved with blocks.

00:11:15.950 --> 00:11:18.980
So the thesis here is that blocks
are far better callback API.

00:11:18.980 --> 00:11:29.289
In the old style we saw qsort_r, but it turns out
that we have lots of kinds of callbacks in our system.

00:11:29.289 --> 00:11:34.889
The performSelector:withObject: in
the Objective-C world is exactly that.

00:11:34.889 --> 00:11:41.970
You want to send it some kind of code to do it, and
you often write a custom selector, and you go do that.

00:11:41.970 --> 00:11:44.550
We have this performSelector:withObject:--

00:11:44.549 --> 00:11:49.159
with object because at the generic level we don't
know how many parameters you really want to pass it,

00:11:49.159 --> 00:11:56.899
and often they're integers or void stars; they're not really
objects, and you're violating the typing system and stuff,

00:11:56.899 --> 00:12:01.980
and we don't have a general language way to
introduce all that stuff, or at least we didn't.

00:12:01.980 --> 00:12:03.769
Now we have blocks.

00:12:03.769 --> 00:12:10.960
Another case is the whole idea of the set
target-- the target action sequence inside Cocoa.

00:12:10.960 --> 00:12:14.660
That is also perhaps better expressed as a block.

00:12:14.659 --> 00:12:20.990
But these are the mechanisms by which we have done sort
of this callback, and we see it in many, many places.

00:12:20.990 --> 00:12:24.090
There are all kinds of notification-based
APIs in our system.

00:12:24.090 --> 00:12:32.000
The NSNotification for example, you can carry
back arbitrary context in a user info dictionary.

00:12:32.000 --> 00:12:38.169
Those are so much fun to set up, and to pull
your values out, and think about, you know?

00:12:38.169 --> 00:12:42.549
So the new style of course is much simpler.

00:12:42.549 --> 00:12:49.439
You just set the callback, I mean every subsystem
provides its own set callback API, but it takes a block

00:12:49.440 --> 00:12:54.110
and you put the code in there that
you want it to do, and you're done.

00:12:54.110 --> 00:12:56.000
It's nice, it's good.

00:12:56.000 --> 00:13:03.389
A key thing about this is that blocks are a C
language extension, and so Objective-C gets to do it,

00:13:03.389 --> 00:13:08.009
C++ gets to do it, Objective-C++ gets to do it.

00:13:08.009 --> 00:13:10.649
[ Applause ]

00:13:10.649 --> 00:13:15.970
So if you are programming Cocoa GUI from C++ you just do it.

00:13:15.970 --> 00:13:21.980
If you are providing a C API like qsort, and
you need some Objective-C goodness going on,

00:13:21.980 --> 00:13:25.779
then you just do your goodness in
Objective-C and hand it to the C API.

00:13:25.779 --> 00:13:32.799
So this interoperability is actually very
cool also, we're really excited about that.

00:13:32.799 --> 00:13:36.609
Let's talk about Lifetime.

00:13:36.610 --> 00:13:43.169
So blocks start out on the stack, and these
interpreted languages, sometimes they're stack frames

00:13:43.169 --> 00:13:45.689
that actually can be allocated off the heap.

00:13:45.690 --> 00:13:49.260
And if the Lifetime persisted,
they'd just use garbage collection.

00:13:49.259 --> 00:13:51.899
I mean that's way inefficient, we don't do that.

00:13:51.899 --> 00:13:54.299
Blocks start out on the stack.

00:13:54.299 --> 00:14:02.789
They can be copied to the heap, but you or the subsystem
has to take an explicit action to make that happen.

00:14:02.789 --> 00:14:08.029
Blocks carry a const version of most variables that it uses.

00:14:08.029 --> 00:14:15.189
So in the previous case, that variable D, we actually made
a copy of D, made it a const version inside that block

00:14:15.190 --> 00:14:20.100
so that it goes along with the data,
so that whereever that executes,

00:14:20.100 --> 00:14:23.360
it's got a local copy of that local stack variable.

00:14:23.360 --> 00:14:25.730
So we don't have to worry about that stack variable.

00:14:25.730 --> 00:14:31.950
So being a const is good for 90 percent of
your uses, but every now and then you need

00:14:31.950 --> 00:14:36.230
to mutate something and hand it back to the caller.

00:14:36.230 --> 00:14:46.670
And in that case, we have a new storage class called
__block, and these variables are shared both with the stack

00:14:46.669 --> 00:14:52.689
that uses it and any blocks that reference that variable.

00:14:52.690 --> 00:14:57.930
So here's how __block works in practice.

00:14:57.929 --> 00:15:03.079
You just stick it up there, it's sort of at
the level of register and auto and static and--

00:15:03.080 --> 00:15:10.120
maybe there's another one, and you just
update your field when you need to.

00:15:10.120 --> 00:15:14.320
This is actually a real example,
innumerate keys in objects using block.

00:15:14.320 --> 00:15:16.480
This is a new API in Snow Leopard.

00:15:16.480 --> 00:15:21.980
If you have to innumerate a dictionary
and examine both key and value,

00:15:21.980 --> 00:15:25.730
with the current scheme of things, it's an order login.

00:15:25.730 --> 00:15:35.009
It's N login, because the N is just getting the keys back,
but then you have to do, in theory at least, a login lookup.

00:15:35.009 --> 00:15:39.740
In practice it's much faster than
that, but if it's a deeper table.

00:15:39.740 --> 00:15:44.060
This turns out to be, when you're
handed both keys and values,

00:15:44.059 --> 00:15:48.659
to be the most efficient way to go through a dictionary.

00:15:48.659 --> 00:15:55.939
In this case we're pulling back the key that happens
to match a particular value that's passed in.

00:15:55.940 --> 00:16:02.630
So blocks give us the ability for these collection
types to offer many different ways to go through there.

00:16:02.629 --> 00:16:08.120
So there are also APIs to reverse--
to examine an array in reverse order.

00:16:08.120 --> 00:16:13.370
And that's also the most efficient
way we have for doing that.

00:16:13.370 --> 00:16:16.769
Let me talk about Lifetime.

00:16:16.769 --> 00:16:22.879
So imagine-- we're about ready to
call a routine that uses blocks.

00:16:22.879 --> 00:16:26.590
So we have stack and we've got the heap.

00:16:26.590 --> 00:16:32.070
Once we enter the stack we're going to have an int local,
we're going to have one of these __ block variables,

00:16:32.070 --> 00:16:35.720
and we're going to have a block that uses both of them.

00:16:35.720 --> 00:16:44.399
So once the block is constructed, it makes that constant
local copy of int local, and it creates a reference

00:16:44.399 --> 00:16:49.569
to the stack based shared __block variable.

00:16:49.570 --> 00:17:01.490
If the block is copied to the heap, what happens is
we actually move the __block storage also to the heap.

00:17:01.490 --> 00:17:05.900
So when the function, so we've got 2
cases that are kind of interesting.

00:17:05.900 --> 00:17:10.480
When the function returns, well we let go
of our reference to that shared variable

00:17:10.480 --> 00:17:18.430
and we return letting the heap version of
that block keep the __block thing alive.

00:17:18.430 --> 00:17:25.000
In case two it can happen that the heap object perishes
first, in which case it lets go of its reference

00:17:25.000 --> 00:17:31.009
to the __share variable, and the heap object
goes away but the stack is still in good shape

00:17:31.009 --> 00:17:35.720
because it has a real hard reference
to that shared variable.

00:17:35.720 --> 00:17:42.110
And so in this case when the thread is ready, when that
function is ready to return, we let go of the block,

00:17:42.109 --> 00:17:49.909
the shared variable-- it goes away, in this case
because it's the last reference, and then the stack,

00:17:49.910 --> 00:17:55.480
the function returns and we end up
at the initial state in both cases.

00:17:55.480 --> 00:18:02.160
So we do the memory management for you in a non-garbage
collected way so that you can just use blocks,

00:18:02.160 --> 00:18:08.480
as I said they're cheap because they start out on a stack,
get a little expensive if they have to move to the heap--

00:18:08.480 --> 00:18:11.210
but they're very versatile when they do that.

00:18:11.210 --> 00:18:15.309
And so we deal with the memory management stuff for you.

00:18:15.309 --> 00:18:23.109
If you're an Objective-C programmer, I hope many
of you are, we have a few extra specializations.

00:18:23.109 --> 00:18:28.009
First of all, all blocks are always Objective-C objects.

00:18:28.009 --> 00:18:32.160
[ Applause ]

00:18:32.160 --> 00:18:38.269
Even if you're just sitting in a plain
old .C program, they're always objects.

00:18:38.269 --> 00:18:46.400
And so they don't do much as objects though, but they do
respond to the copy and release messages in auto release.

00:18:46.400 --> 00:18:54.710
What's cool about that is that as a first class
citizen they can participate in our @property syntax,

00:18:54.710 --> 00:18:58.910
using the copy attribute, and they just work fine that way.

00:18:58.910 --> 00:19:06.300
So there's more about syntax, there's more about
Lifetime, there's some kind of interesting gotchas

00:19:06.299 --> 00:19:11.339
and fun surprises we've encountered over the
time, and if you want to learn more about that,

00:19:11.339 --> 00:19:16.709
please come to the 5 o'clock talk today in Russian Hill.

00:19:16.710 --> 00:19:20.210
So what are blocks good for?

00:19:20.210 --> 00:19:24.660
There's over 100 uses of them in Snow Leopard.

00:19:24.660 --> 00:19:27.700
We have found a lot of things to do with them.

00:19:27.700 --> 00:19:32.940
There are sort of the traditional things that I've
talked about, which are for the most part blocking,

00:19:32.940 --> 00:19:41.330
they're synchronous, they're primarily single-threaded
uses that collect in the iteration kinds of stuff.

00:19:41.329 --> 00:19:44.549
We have, as I said, iterations with and without options.

00:19:44.549 --> 00:19:52.619
We don't quite have the map, but map's so easy to write,
you guys can write it, but we got reduce and collect kinds

00:19:52.619 --> 00:19:56.539
of cases; and we have wrappers as I've illustrated.

00:19:56.539 --> 00:20:02.750
A new thing for us is-- sorry, callbacks.

00:20:02.750 --> 00:20:08.700
The new thing for us is using these in an
asynchronous, non-blocking, multithreaded way.

00:20:08.700 --> 00:20:11.360
And that's what Grand Central Dispatch offers you.

00:20:11.359 --> 00:20:16.419
So this lets you think about concurrent
computation on independent data,

00:20:16.420 --> 00:20:23.620
it lets you think about serialized computation
on shared data, and all commas of the above.

00:20:23.619 --> 00:20:27.009
And before I steal anymore of Dave's
thunder, I'll turn it over to him.

00:20:27.009 --> 00:20:36.000
[ Applause ]

00:20:36.000 --> 00:20:37.779
>> Mic? All right excellent.

00:20:37.779 --> 00:20:38.960
Thank you Blaine.

00:20:38.960 --> 00:20:40.950
We really appreciate blocks.

00:20:40.950 --> 00:20:44.370
So Grand Central Dispatch, what is it?

00:20:44.369 --> 00:20:50.319
You may have been reading all sorts of things online
wondering what it is, well now you get to know.

00:20:50.319 --> 00:20:52.159
Well why GCD?

00:20:52.160 --> 00:20:53.380
Where are we coming from?

00:20:53.380 --> 00:20:58.240
Well GCD is all about asynchronous
blocks, as led off from Blaine,

00:20:58.240 --> 00:21:00.309
and we'd like to give a little
teaser for what that looks like.

00:21:00.309 --> 00:21:06.589
This is a basic block, no parameters,
no return value, print something out.

00:21:06.589 --> 00:21:08.799
We're going to provide a very simple API.

00:21:08.799 --> 00:21:11.589
Dispatch async and it takes a queue.

00:21:11.589 --> 00:21:15.709
That's how you can get a basic block
running in the background doing work.

00:21:15.710 --> 00:21:17.190
That's it.

00:21:17.190 --> 00:21:19.110
So why are we doing this?

00:21:19.109 --> 00:21:26.639
Well on the top end of Apple's platforms
we have the Mac Pro with 16 virtual cores.

00:21:26.640 --> 00:21:30.600
We have at the very low end our
Mac Mini, that many customers like.

00:21:30.599 --> 00:21:33.329
It has 2 cores.

00:21:33.329 --> 00:21:39.289
Apple on the Mac platform is 100
percent multicore, top to bottom.

00:21:39.289 --> 00:21:46.049
We also have one more problem we'd
like to fix-- user experience.

00:21:46.049 --> 00:21:50.700
Every time you'd see a spinning beach ball, that's an
opportunity for concurrency, to get something running

00:21:50.700 --> 00:21:53.840
in the background and keep a snappy user interface.

00:21:53.839 --> 00:21:57.409
So that's why we're doing GCD.

00:21:57.410 --> 00:22:00.710
Well what is GCD?

00:22:00.710 --> 00:22:02.690
There are 3 reasons why we're doing it.

00:22:02.690 --> 00:22:05.360
It's fast, it's easy, it's fun.

00:22:05.359 --> 00:22:08.769
So I want to talk first about being fast.

00:22:08.769 --> 00:22:10.879
How is it that GCD's fast?

00:22:10.880 --> 00:22:19.730
Well GCD sits at the lowest layer of your process, sits
right on top of the hardware, right on top of the kernel.

00:22:19.730 --> 00:22:25.950
And what we can do with some of our APIs is
bypass the kernel to provide a nice speed boost,

00:22:25.950 --> 00:22:34.029
in this case the API isn't particularly important, but
what happens is, your application can call into GCD

00:22:34.029 --> 00:22:41.349
and on the fast path we can potentially just return right
away to your application in a few number of instructions.

00:22:41.349 --> 00:22:44.980
However if we need to do some more complex work,

00:22:44.980 --> 00:22:52.849
we can then drop down to the actual kernel
equivalent API, and do the work and then come back up.

00:22:52.849 --> 00:23:00.419
All right, so we have a nice fast path and for this
particular API, I'd like to point out that it can be

00:23:00.420 --> 00:23:06.009
up to 200 times faster on a Mac Pro with 16 virtual cores.

00:23:06.009 --> 00:23:12.670
So that's a nice advantage of bypassing
the kernel, give a little turbo boost.

00:23:12.670 --> 00:23:20.019
So moving onto easy-- as I was talking
about, we have a great metaphor.

00:23:20.019 --> 00:23:21.279
We have blocks and queues.

00:23:21.279 --> 00:23:28.000
They're very easy to think about, very easy to
use, and again something you'll be using a lot of.

00:23:28.000 --> 00:23:31.559
A basic block, you're going to be
sliding in a dispatch_async and a queue,

00:23:31.559 --> 00:23:36.179
and you're going to be running a lot
of code concurrently really easily.

00:23:36.180 --> 00:23:43.019
So then moving onto fun; I want to talk about
some stories around campus that I've heard,

00:23:43.019 --> 00:23:48.329
from engineers we run into in the halls,
and just as we chat up like normal.

00:23:48.329 --> 00:23:52.720
As you've heard during some of the Monday
talks, we had some external developers talk

00:23:52.720 --> 00:23:57.420
about either how fast it made their
code, or how easy it was.

00:23:57.420 --> 00:24:00.140
And around campus we've heard the same story.

00:24:00.140 --> 00:24:05.430
Wow! My code runs a lot faster
and that's really cool, thanks.

00:24:05.430 --> 00:24:10.180
Another thing we've heard, honestly,
Why hasn't it always been this way?

00:24:10.180 --> 00:24:14.539
Developers around the company,
after playing with GCD, love it.

00:24:14.539 --> 00:24:19.210
They just start running around, they add it
here and there, it's just really easy to use

00:24:19.210 --> 00:24:22.250
and they find new opportunities
that they never thought were there.

00:24:22.250 --> 00:24:29.549
And a quote I really like from someone in our developer
tools team, "Dispatch is like writing poetry".

00:24:29.549 --> 00:24:34.609
They enjoyed it that much, so dispatch is fun.

00:24:34.609 --> 00:24:40.119
So technology overview; what are
we going to be talking about?

00:24:40.119 --> 00:24:45.409
GCD is a part of libSystem; you don't
need to do a single thing to get it.

00:24:45.410 --> 00:24:52.470
It's in every process, it comes right along with basic
Malloc and free and other fundamental library API.

00:24:52.470 --> 00:25:01.730
All you need to do is include one header and you get
access to all our API - Dispatch/Dispatch.h, that's it.

00:25:01.730 --> 00:25:09.589
We have a basic object-oriented architecture, so here's
some common routines shared among just Dispatch objects.

00:25:09.589 --> 00:25:19.289
We have queues, we have sources for monitoring external
events, we have groups for tracking sets of blocks,

00:25:19.289 --> 00:25:24.409
and we have semaphores for more
intricate synchronization problems.

00:25:24.410 --> 00:25:30.560
And finally we have some basic not object
types, we have a concept of time in GCD,

00:25:30.559 --> 00:25:37.369
and we have a concept of running a block once
and only once for the lifetime of the process.

00:25:37.369 --> 00:25:47.929
However in this talk, we are only talking about a few object
API, a few queue API, a little bit of groups, and time.

00:25:47.930 --> 00:25:52.870
The rest of this will be covered
in the in-depth talk tomorrow.

00:25:55.220 --> 00:26:01.740
And those sessions-- so Understanding Grand Central
Dispatch In Depth, is tomorrow morning at 9 a.m.,

00:26:01.740 --> 00:26:06.910
and in the early afternoon we have Migrating
Your Application to Grand Central Dispatch

00:26:06.910 --> 00:26:12.140
where we'll really focus on the comparison
between existing technologies and what's provided

00:26:12.140 --> 00:26:17.320
in dispatch, and how to compare the two and adopt.

00:26:17.319 --> 00:26:20.129
We also have some more related sessions.

00:26:20.130 --> 00:26:25.430
Yesterday there was a Designing your Cocoa Application For
Concurrency, where lots of edge cases and nuances

00:26:25.430 --> 00:26:27.660
that you might run into are addressed and talked about.

00:26:27.660 --> 00:26:30.759
I hope that you might be able to catch that on video.

00:26:30.759 --> 00:26:36.079
There's a What's New in Instruments,
it'll be right after this downstairs.

00:26:36.079 --> 00:26:43.419
And there's Blaine's second talk Objective-C and Garbage
Collection Advancements, which will also cover blocks.

00:26:43.420 --> 00:26:47.000
And finally there's some Advanced Debugging
and Performance Analysis you might want to look

00:26:47.000 --> 00:26:53.869
into for understanding whether your particular
project will find concurrency to be worth it.

00:26:53.869 --> 00:26:56.179
All right, so GCD objects.

00:26:56.180 --> 00:26:58.340
Let's talk about some shared context.

00:26:58.339 --> 00:27:05.240
Each GCD object has some-- we have some
basic polymorphic API among our types.

00:27:05.240 --> 00:27:08.490
For example, Dispatch objects are reference counted.

00:27:08.490 --> 00:27:11.039
We have Dispatch retain, Dispatch release.

00:27:11.039 --> 00:27:18.089
It can retain or release objects just like you
would with Cocoa, and GCD does the right thing

00:27:18.089 --> 00:27:19.990
with the parameters that are passed to it.

00:27:19.990 --> 00:27:27.460
So if a queue is passed to dispatch_async, the queue
will be retained throughout the usage of that block.

00:27:27.460 --> 00:27:33.039
I'd like to point out that Dispatch objects do not
participate in Garbage Collection in this point in time.

00:27:33.039 --> 00:27:38.569
And there are a few other shared
functions, which I'll talk about tomorrow.

00:27:38.569 --> 00:27:42.409
Moving onto queues, our first example of a dispatch_object.

00:27:42.410 --> 00:27:48.980
We'd like to talk about the fundamentals of a queue first,
and then we'll talk about specific instances of queues.

00:27:48.980 --> 00:27:53.279
A queue in Dispatch is a lightweight list of blocks.

00:27:53.279 --> 00:27:55.649
It's really lightweight.

00:27:55.650 --> 00:27:58.009
It provides-- sorry...

00:27:58.009 --> 00:28:07.039
[ Silence ]

00:28:07.039 --> 00:28:14.349
It's a lightweight list of blocks, and it
provides asynchronous execution of those blocks.

00:28:14.349 --> 00:28:16.549
Enqueue/Dequeue is FIFO.

00:28:16.549 --> 00:28:18.490
This is shared among all queues.

00:28:18.490 --> 00:28:22.799
And we'd like to show an animation of what this looks like.

00:28:22.799 --> 00:28:26.240
So you can have your thread, and
your allocated dispatch_queue.

00:28:26.240 --> 00:28:30.380
You can then create a block and then
dispatch_async to it like we've shown earlier.

00:28:30.380 --> 00:28:36.440
It puts it on the queue, and in fact you can
allocate multiple blocks, and what GCD will do is,

00:28:36.440 --> 00:28:40.450
it will notice this queue has become
available with work, and it will start--

00:28:40.450 --> 00:28:45.009
it will bind a thread to that queue
and start running blocks.

00:28:45.009 --> 00:28:48.849
In fact it can reuse threads to run a few blocks in a row,

00:28:48.849 --> 00:28:53.139
thus getting a nice speed boost for
recycling an available resource.

00:28:53.140 --> 00:28:56.140
So that's the basics of a Dispatch queue.

00:28:56.140 --> 00:29:02.020
This is what that code would have looked like for
that animation we just saw: async, async, async.

00:29:02.019 --> 00:29:05.079
We had 3 blocks A, B and C.

00:29:05.079 --> 00:29:06.589
That's it, it's that simple.

00:29:06.589 --> 00:29:11.579
All right so global queues.

00:29:11.579 --> 00:29:15.769
This is our first instance of an
actual queue that your code will use.

00:29:15.769 --> 00:29:18.690
The unique property about it is while it--

00:29:18.690 --> 00:29:25.009
concurrent execution, however it shares all the
properties of queues that we've described so far.

00:29:25.009 --> 00:29:30.390
It's lightweight, provides asynchronous
execution, and Enqueue/Dequeue is FIFO.

00:29:30.390 --> 00:29:33.820
It's just that completion part that's not FIFO.

00:29:33.819 --> 00:29:42.079
So a diagram of what this looks like in practice, again
you have your thread, it starts allocating blocks,

00:29:42.079 --> 00:29:47.639
they get put on the Global Dispatch queue that's
available, and GCD will notice that queue has work

00:29:47.640 --> 00:29:51.600
and start binding threads to it, and
then run the blocks to completion

00:29:51.599 --> 00:29:57.539
and then return the threads to
the pool for other queues to use.

00:29:57.539 --> 00:29:59.940
This is the code of what we just saw.

00:29:59.940 --> 00:30:04.590
In this case we're getting one of the well known
Global queues, there's some optional parameters.

00:30:04.589 --> 00:30:09.939
We're just going to pass 0 for now, we're going
to take that queue and dispatch_async to it;

00:30:09.940 --> 00:30:13.700
just put the blocks on the queue, let
the queue have the policy for what to do

00:30:13.700 --> 00:30:17.240
with the blocks; in this case, run them concurrently.

00:30:17.240 --> 00:30:20.019
So that's pretty easy.

00:30:20.019 --> 00:30:24.559
Let's talk about a different example and
a different application, the Global queue.

00:30:24.559 --> 00:30:32.819
Suppose, suppose, suppose you wanted to
characters, words, and paragraphs in a document.

00:30:32.819 --> 00:30:36.319
And let's say your document is large, very large.

00:30:36.319 --> 00:30:40.009
Well here's your basic UI.

00:30:40.009 --> 00:30:50.589
[ Silence ]

00:30:50.589 --> 00:30:52.759
All right, we got some results.

00:30:52.759 --> 00:30:57.369
Yay! All right well that's a lot of
characters, that's a lot of words.

00:30:57.369 --> 00:31:00.179
That's a lot of work the computer's doing.

00:31:00.180 --> 00:31:02.769
Well how can we use GCD to make this faster?

00:31:02.769 --> 00:31:07.019
All right well let's say the code
is doing something like this.

00:31:07.019 --> 00:31:13.339
We have our work, we're going to run a for loop
over it, scanning the text for the words,

00:31:13.339 --> 00:31:16.740
being able to deal with some of
the divisions around the document.

00:31:16.740 --> 00:31:21.559
We can aggregate the results into
a set, in this case basic C array.

00:31:21.559 --> 00:31:28.450
And then after the for loop completes we can then
summarize the results and eventually draw them.

00:31:28.450 --> 00:31:30.259
Well with GCD it's real simple.

00:31:30.259 --> 00:31:36.450
We drop that C construct and we
replace it with a Dispatch for loop.

00:31:36.450 --> 00:31:41.490
All we need to do is take the Global concurrent
queue that we've talked about already,

00:31:41.490 --> 00:31:47.690
and take a new function called dispatch_apply that
acts just like a basic for loop that you write everyday;

00:31:47.690 --> 00:31:53.190
you pass in the count, you pass in
the queue, and you pass in a block.

00:31:53.190 --> 00:31:58.440
This particular block takes 1 parameter
back, which is the index to do the work on.

00:31:58.440 --> 00:32:05.299
So what will happen is this block is farmed out among
the CPUs, each CPU gets a different index assigned to it

00:32:05.299 --> 00:32:12.869
until we've exhausted the count, from 0 to the count,
not including the count, just like a regular for loop.

00:32:12.869 --> 00:32:17.919
To give you an example of the actual
problem we just talked about--

00:32:17.920 --> 00:32:20.430
here's some speed improvements
you can get out of dispatch_apply.

00:32:20.430 --> 00:32:26.650
We took the basic scanning of a very
large document, which took 17 1/4 seconds.

00:32:26.650 --> 00:32:33.380
We tried running on 2 CPUs and it got almost
super perfect linear scaling on Mac Pro.

00:32:33.380 --> 00:32:39.230
We then broke it up into 4 chunks and 8 chunks,
and again still super near linear scaling.

00:32:39.230 --> 00:32:42.009
It's really efficient.

00:32:42.009 --> 00:32:49.250
[ Applause ]

00:32:49.250 --> 00:32:50.150
Thank you.

00:32:50.150 --> 00:32:52.100
However we still have a problem.

00:32:52.099 --> 00:32:57.049
That's still a very large document and sometimes it doesn't
complete in time and we still have a spinning beach ball,

00:32:57.049 --> 00:32:59.430
despite reducing it from 17 seconds to a few.

00:32:59.430 --> 00:33:02.720
All right so what can we do about that?

00:33:02.720 --> 00:33:05.220
Let's talk about asynchronous design.

00:33:05.220 --> 00:33:09.059
Let's talk about that little teaser
API we've been talking about.

00:33:09.059 --> 00:33:13.589
So imagine this is your basic Cocoa
callback for when that button's pressed.

00:33:13.589 --> 00:33:17.389
We're going to take your document,
call the summarize method,

00:33:17.390 --> 00:33:22.810
and that summarize method would probably use
dispatch_reply to do the work that we've just talked about,

00:33:22.809 --> 00:33:27.440
and it's going to take the resulting
dictionary, hand it off to the model,

00:33:27.440 --> 00:33:30.390
and then update the view and release the dictionary.

00:33:30.390 --> 00:33:31.009
What could we do?

00:33:31.009 --> 00:33:37.279
All right let's make a little bit of
wiggle room, and let's slide in some code.

00:33:37.279 --> 00:33:42.109
Does dispatch_async to a Global queue to get
the intermediate code running in the background,

00:33:42.109 --> 00:33:44.849
and that's it, so now it's running in the background.

00:33:44.849 --> 00:33:48.969
OK great. We need to do one more thing.

00:33:48.970 --> 00:33:55.740
We need to dispatch_async back to the main
thread, and get the model and the view updated

00:33:55.740 --> 00:33:59.059
in a thread-safe way and coordinate it with the GUI.

00:33:59.059 --> 00:34:06.349
That's it-- 2 lines of code, 2 lines of basic boiler
plate, and now we've taken an existing function,

00:34:06.349 --> 00:34:13.009
gotten code running in the background, and we've gotten
the results running back where we need them; that's it.

00:34:13.010 --> 00:34:20.920
[ Applause ]

00:34:20.920 --> 00:34:21.490
Thank you.

00:34:21.489 --> 00:34:27.909
All right, so let's talk a little bit-- so we've
taken this problem and now we've replaced it

00:34:27.909 --> 00:34:31.099
with this, and your users are a lot happier.

00:34:32.719 --> 00:34:36.639
So let's talk about that main queue that we just hinted at.

00:34:36.639 --> 00:34:40.569
It executes blocks serially, unlike the Global queues.

00:34:40.570 --> 00:34:47.400
It cooperates with the Cocoa main run loop, which our
application is probably calling through NSApplicationMain.

00:34:47.400 --> 00:34:50.340
It's also usable by pure GCD programs.

00:34:50.340 --> 00:34:56.329
Instead of NSApplicationMain, our program
can call dispatch_main, that's it.

00:34:56.329 --> 00:35:04.730
The main queue, here's an example usage, you might
get the main queue, dispatch_async Hello World to it.

00:35:04.730 --> 00:35:07.909
Maybe after that, oh sorry here's main.

00:35:07.909 --> 00:35:12.219
Here you're going to get the main queue, maybe
you're going to put a little Hello World.

00:35:12.219 --> 00:35:17.809
It's going to wait in the queue until we actually drain
the main queue, and then your application's probably going

00:35:17.809 --> 00:35:23.230
to initialize some libraries or itself;
and they can also use the main queue.

00:35:23.230 --> 00:35:29.889
And then finally your program will probably call
NSApplicationMain and the main queue will start draining,

00:35:29.889 --> 00:35:34.849
that block will probably run first, and
then any callbacks that have been installed

00:35:34.849 --> 00:35:37.889
by libraries that the application will run.

00:35:37.889 --> 00:35:43.199
To reiterate, your program in a pure GCD
world could also just call dispatch_main.

00:35:45.849 --> 00:35:48.829
Let's talk a little bit about serial queues.

00:35:48.829 --> 00:35:52.139
The main queue's one, how can you allocate one?

00:35:52.139 --> 00:35:55.509
Well this is the second type of queue.

00:35:55.510 --> 00:35:58.480
Only 2 types that we have in GCD.

00:35:58.480 --> 00:36:05.170
We have the Global queues and the Serial queues, same
common properties are shared, it's really lightweight,

00:36:05.170 --> 00:36:10.570
it supports asynchronous execution, Enqueue/Dequeue is FIFO,

00:36:10.570 --> 00:36:16.410
but unlike the Global queues the
completion of blocks is also FIFO.

00:36:16.409 --> 00:36:23.039
To again show an animation, we're going to have
2 threads now trying to enqueue at the same time.

00:36:23.039 --> 00:36:26.719
One of them won the race, its block got inserted first.

00:36:26.719 --> 00:36:32.509
GCD assigns a thread, the queue starts
draining the blocks, they finish serially.

00:36:32.510 --> 00:36:35.300
So that's a Serial queue.

00:36:35.300 --> 00:36:38.340
Here's an example in code of what you just saw.

00:36:38.340 --> 00:36:41.800
Thread 1 and thread 2 exist already on the system.

00:36:41.800 --> 00:36:48.330
One of them executes a dispatch_async and
queues the first block, then our race happens

00:36:48.329 --> 00:36:52.829
and 1 of these threads wins the
race as we saw in the animation.

00:36:55.130 --> 00:36:58.329
Here's some more code to show what that's like.

00:36:58.329 --> 00:37:01.690
Here's how you create a queue, it couldn't be easier.

00:37:01.690 --> 00:37:08.659
You call dispatch_queue_create, you pass in a label, this
label can be whatever you want, although we recommend

00:37:08.659 --> 00:37:14.739
and strongly encourage some kind of reverse style DNS,
because there's a lot of queues running around the system

00:37:14.739 --> 00:37:17.309
and it's nice for them to be distinguishable.

00:37:17.309 --> 00:37:23.059
And then the second parameter is future
proofing, just pass null; so that's it.

00:37:23.059 --> 00:37:26.769
That label you'll also see in future
talks, like in the Instruments talk,

00:37:26.769 --> 00:37:30.719
or in your crash supports, or samples,
or other debugging tools.

00:37:30.719 --> 00:37:34.569
And that's how you can help aid
your own debugging and development.

00:37:34.570 --> 00:37:40.690
You can also get the label that we've just assigned
to it, we have this so again you're debugging,

00:37:40.690 --> 00:37:44.860
you might want to print out programmatically
what label it is.

00:37:44.860 --> 00:37:48.750
And that's how you print it out, pointer and label.

00:37:49.889 --> 00:37:56.400
So building on the topics that we've talked about
so far, I want to talk about multiple Serial queues.

00:37:56.400 --> 00:37:58.150
How do they behave?

00:37:58.150 --> 00:38:01.039
Well queues are lightweight.

00:38:01.039 --> 00:38:05.059
GCD imposes no limits other than available memory.

00:38:05.059 --> 00:38:10.269
So you could allocate 100, 1,000, 10,000.

00:38:10.269 --> 00:38:15.320
Doesn't really matter, you can decide
what's appropriate for your application.

00:38:15.320 --> 00:38:19.820
Because of that fact it's very
natural to assign 1 queue per task.

00:38:19.820 --> 00:38:26.070
So if you have a lot of tasks, well assign a queue
per task and then let them roam around independently.

00:38:26.070 --> 00:38:30.240
And this is how GCD achieves yet more concurrency.

00:38:30.239 --> 00:38:33.789
Each queue can run around independently
and we have an animation to show that.

00:38:33.789 --> 00:38:38.809
So you have your thread, it allocates
a queue and then maybe another queue.

00:38:38.809 --> 00:38:44.570
It starts assigning blocks to queues, and GCD
will intelligently assign threads to queues

00:38:44.570 --> 00:38:47.789
and allow the blocks to execute concurrently.

00:38:47.789 --> 00:38:52.259
However each Serial queue is still
only processing blocks serially.

00:38:52.260 --> 00:38:55.710
That's how you can get some task level concurrency.

00:38:57.559 --> 00:39:05.420
In code of what we just saw, there were 2 queues that were
created, queue 1 and queue 2, in blue and gold respectively.

00:39:05.420 --> 00:39:10.070
There were dispatch_asyncs A through
E in a row, 2 alternative queues,

00:39:10.070 --> 00:39:13.030
queue 1, queue 2, queue 1, queue 2, queue 1.

00:39:13.030 --> 00:39:15.240
And that was the animation we just saw.

00:39:15.239 --> 00:39:25.539
So again, each queue operates independently and serially,
but we can achieve concurrency at the same time.

00:39:25.539 --> 00:39:30.469
So that's been queues and blocks, now let's talk
about grouping blocks and start tracking work

00:39:30.469 --> 00:39:34.649
and build a more interesting and fun program.

00:39:34.650 --> 00:39:36.950
So let's group some blocks.

00:39:36.949 --> 00:39:43.379
Well we have an API for that, it allows you to
track a set of blocks and then get a notification

00:39:43.380 --> 00:39:46.820
when those blocks complete, the whole set.

00:39:46.820 --> 00:39:51.630
The blocks may run on different queues--
the blocks that are part of the group,

00:39:51.630 --> 00:39:57.769
and each individual block can add more blocks to the group.

00:39:57.769 --> 00:40:00.340
And you might call this recursive decomposition.

00:40:00.340 --> 00:40:03.280
So what does that look like?

00:40:03.280 --> 00:40:08.730
We're going to take some queues that are
running around the system and allocate a group.

00:40:08.730 --> 00:40:13.480
We're then going to start assigning blocks
to that group and get them running on queues.

00:40:13.480 --> 00:40:19.070
All along the Dispatch group is
counting how many blocks are outstanding.

00:40:22.679 --> 00:40:28.559
So when that 0's hit, we can now get a notification
callback, say, Ah-ha, all the blocks are done,

00:40:28.559 --> 00:40:33.329
we can now move onto the next stage of our project.

00:40:33.329 --> 00:40:39.650
This is what, more or less what we
saw-- a Dispatch group was created,

00:40:39.650 --> 00:40:44.300
we then used whatever iteration
API we want, in this case a for loop.

00:40:44.300 --> 00:40:51.260
Your code could use an NSDictionary, or
some more complex object graph to evaluate.

00:40:51.260 --> 00:40:54.310
Whatever the case may be, it's up to you.

00:40:54.309 --> 00:41:00.420
We then, instead of a dispatch_async, do a
slight modification and call dispatch_group_async

00:41:00.420 --> 00:41:06.500
with the group parameter passed first, and the rest
of the parameters are just like dispatch_async.

00:41:06.500 --> 00:41:14.460
Inside of our block we can do something with our
task, and that's how you get blocks into a group.

00:41:14.460 --> 00:41:21.800
Finally once we're done populating the group, we
can call dispatch_group_notify, pass in the group,

00:41:21.800 --> 00:41:30.420
pass in a queue to run the block on, and then pass
in a block that will get run when the group empties.

00:41:30.420 --> 00:41:38.490
And then finally we can call dispatch_release.

00:41:38.489 --> 00:41:41.739
It's worth noting that GCD will
retain the group while it's in flight,

00:41:41.739 --> 00:41:45.009
so you don't need to worry about
waiting for the group to complete.

00:41:45.010 --> 00:41:49.550
[ Silence ]

00:41:49.550 --> 00:41:54.130
So completion, groups are a generic
completion notification system.

00:41:54.130 --> 00:41:59.970
However we'd like to remind you of something we just
saw earlier in the talk, which are called nested blocks.

00:41:59.969 --> 00:42:03.079
It can be fewer lines of code, it can be more efficient.

00:42:03.079 --> 00:42:07.279
So what was it that we saw earlier in the talk?

00:42:07.280 --> 00:42:15.750
We saw a dispatch_group_async to a Global queue, we then saw
some work done inside the block, and then as the last part

00:42:15.750 --> 00:42:22.199
of our block, the outer block, we then ran
another dispatch_async, we picked a queue,

00:42:22.199 --> 00:42:25.609
in this case the main queue, being a very popular one.

00:42:25.610 --> 00:42:33.650
And then finally we took the results of our
background operation, and did something with them.

00:42:33.650 --> 00:42:37.320
The last part is the boiler plate to end our blocks.

00:42:37.320 --> 00:42:39.110
That's it.

00:42:39.110 --> 00:42:42.030
Two lines of code just to get some
code running in the background,

00:42:42.030 --> 00:42:45.670
and the results running back where you want them.

00:42:45.670 --> 00:42:53.320
However sometimes asynchronous code
isn't convenient, and code needs to wait.

00:42:53.320 --> 00:42:55.450
Let's talk about waiting.

00:42:55.449 --> 00:42:58.159
Why do you need to wait?

00:42:58.159 --> 00:43:00.230
Why does your code need to wait?

00:43:00.230 --> 00:43:07.650
Well when you're bridging synchronous code and
asynchronous code, that's where you need to wait.

00:43:07.650 --> 00:43:14.970
You can wait for a set of blocks to complete with the
Group API, which is sometimes called a Fork Join model,

00:43:14.969 --> 00:43:21.829
or you can wait with 1 block with an API we call
dispatch_sync, because it's synchronous, it waits.

00:43:21.829 --> 00:43:24.860
However groups are more a more generic mechanism.

00:43:24.860 --> 00:43:32.539
So here's an example of dispatch_sync, it's
like dispatch_async, but without 1 letter.

00:43:32.539 --> 00:43:34.570
[ Laughter ]

00:43:34.570 --> 00:43:35.930
So it's real simple.

00:43:35.929 --> 00:43:42.059
Here's an example using the syntax that was outlined
earlier in the talk with the __block syntax.

00:43:42.059 --> 00:43:50.409
We're going to set up an RVal to get the result of a
function, we can then Enqueue the block on the target queue

00:43:50.409 --> 00:43:54.069
and let it run in the proper order
of all the blocks Enqueue'd.

00:43:54.070 --> 00:43:59.360
And then finally once the block runs,
it can do its work, generate the result,

00:43:59.360 --> 00:44:06.380
and once that block completes dispatch_sync will
return, and now we can use the resulting value.

00:44:06.380 --> 00:44:10.579
That's it, that's how you can wait; just drop a letter.

00:44:10.579 --> 00:44:14.920
Here's the more generic example
as we talked about with groups,

00:44:14.920 --> 00:44:18.480
but now we're going to talk about
more the advance block syntax.

00:44:18.480 --> 00:44:25.510
So we can create a group and an array of
resulting values, using the __block syntax.

00:44:25.510 --> 00:44:32.770
Again we can pick our favorite iteration technique.

00:44:32.769 --> 00:44:35.570
In this case the for loop.

00:44:35.570 --> 00:44:42.130
We can do the group async, and now instead
of a group notify we can do a group wait.

00:44:42.130 --> 00:44:47.160
Pass the group; tell it we want to wait forever,
and when that returns the group is done.

00:44:47.159 --> 00:44:52.849
We can release the group, and then
finally use the resulting values.

00:44:52.849 --> 00:44:56.179
So that's it, that's how you can go
create a bunch of asynchronous work,

00:44:56.179 --> 00:45:01.009
and then wait for it all to complete with 1 API.

00:45:01.010 --> 00:45:04.960
All right, so what was that dispatch time forever all about?

00:45:07.130 --> 00:45:10.510
In GCD we have a concept of time.

00:45:10.510 --> 00:45:19.490
It's an opaque type, it's not safe for arithmetic, it's
not safe for comparisons, it's just a bucket of bits.

00:45:19.489 --> 00:45:24.339
There are 2 well known constants that GCD
uses, and your code will probably use too.

00:45:24.340 --> 00:45:29.750
There's DISPATCH_TIME_NOW for just checking immediately
and having it return with a result of either a success

00:45:29.750 --> 00:45:36.380
or timeout, and there's DISPATCH_TIME_FOREVER,
which tells GCD to wait forever.

00:45:36.380 --> 00:45:40.000
In that case it will always succeed.

00:45:40.000 --> 00:45:49.869
So using the group API that we already talked about,
group-- wait with DISPATCH_TIME_FOREVER will wait forever,

00:45:49.869 --> 00:45:53.289
and this always returns success, which is 0.

00:45:53.289 --> 00:45:56.610
We also just pull the group by saying an immediate timeout,

00:45:56.610 --> 00:46:01.800
and the group wait API will return
non-zero for the timeout case.

00:46:01.800 --> 00:46:07.140
So this will print out group is not
empty if the group is not empty.

00:46:07.139 --> 00:46:09.719
So that's Dispatch Time.

00:46:09.719 --> 00:46:16.819
We also support arbitrary timeouts, and it's also
possible to defer a block just like with dispatch_async,

00:46:16.820 --> 00:46:22.150
except with an API called dispatch_after,
which will run the block after a dispatch time.

00:46:22.150 --> 00:46:25.170
However we're not going to talk about it now.

00:46:25.170 --> 00:46:30.630
We're going to talk about that tomorrow morning at
our Understanding Grand Central Dispatch in Depth talk

00:46:30.630 --> 00:46:38.340
at 9 a.m. Finally I'd like to talk about Instruments.

00:46:38.340 --> 00:46:44.910
They've done a great job at making
blocks easy to track and introspect.

00:46:44.909 --> 00:46:49.009
They've got some great behavioral analysis
for understanding the latency of blocks form

00:46:49.010 --> 00:46:51.970
when they're enqueued to when they actually start executing.

00:46:51.969 --> 00:46:59.049
They also can help you track when
they were enqueued, by which queues,

00:46:59.050 --> 00:47:01.470
and essentially just all these blocks in flight.

00:47:01.469 --> 00:47:03.549
I hope you understand that.

00:47:03.550 --> 00:47:09.800
You can also track the latency of the block
execution itself, and it can also help you track

00:47:09.800 --> 00:47:15.940
which blocks are run synchronously;
which is important to understand as far

00:47:15.940 --> 00:47:22.019
as accidentally creating too many stall points in
your program, where a thread did a dispatch_sync,

00:47:22.019 --> 00:47:26.150
waited for a block to run, which
completed, started much later.

00:47:26.150 --> 00:47:30.380
And it's an opportunity for switching
to a callback base design.

00:47:30.380 --> 00:47:34.390
Which leads into optimization, which are
what are those longest running blocks

00:47:34.389 --> 00:47:36.449
and what are the most executed and how can we tune?

00:47:36.449 --> 00:47:43.599
These are going to be some teaser
diagrams for what Instruments can provide.

00:47:43.599 --> 00:47:51.949
Here's a call tree view that's-- here's a queue centric
view that's keeping some accumulated statistics.

00:47:51.949 --> 00:47:58.730
Here's a block centric view of tracking how blocks
are flying around, and then a block focus view.

00:47:58.730 --> 00:48:03.400
And what I'd like to point out here, which I think is
really cool and we're really excited about, is on the right,

00:48:03.400 --> 00:48:06.940
the very right is this stack of backtrace.

00:48:06.940 --> 00:48:14.010
But there's a division in the middle which is the difference
between the enqueue point and the execution point.

00:48:14.010 --> 00:48:18.890
[ Applause ]

00:48:18.889 --> 00:48:23.980
So as the clapping hands have already figured out, it
makes completely decoupled code look like a straight

00:48:23.980 --> 00:48:27.349
up backtrace from where it started to where it ran.

00:48:27.349 --> 00:48:30.429
So we really think that's cool.

00:48:30.429 --> 00:48:38.559
Instruments has 2 talks, there's one right after this one.

00:48:38.559 --> 00:48:40.500
And there's an advanced one tomorrow morning.

00:48:40.500 --> 00:48:44.570
So that's GCD in a nutshell.

00:48:44.570 --> 00:48:47.550
We think it's really fast.

00:48:47.550 --> 00:48:54.010
We think it's easy, and we think it's
really fun, and we hope you think so too.

00:48:54.010 --> 00:49:04.110
[ Applause ]

00:49:04.110 --> 00:49:06.000
All right cool, thank you.

00:49:06.000 --> 00:49:10.699
So to wrap things up, Michael Jurewitz will
be getting on stage soon to handle Q & A.

00:49:10.699 --> 00:49:15.029
He's our Developer Tools and Performance
Evangelist, and he'd be happy to hear from you.

00:49:15.030 --> 00:49:18.780
We have lots of great documentation showing up.

00:49:18.780 --> 00:49:24.640
There's a section on the dev's forum for communication,
and I'd like to point out something that's not here

00:49:24.639 --> 00:49:30.710
on the slides that we forgot is, we have
header doc which is fairly extensive,

00:49:30.710 --> 00:49:36.289
and more than anything that I think we feel proud of is a
huge section of man pages that our engineers have written

00:49:36.289 --> 00:49:39.009
that document a lot of this stuff in depth.

00:49:39.010 --> 00:49:45.010
[ Applause ]