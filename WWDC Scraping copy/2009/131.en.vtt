WEBVTT

00:00:13.279 --> 00:00:14.899
>> So welcome everyone.

00:00:14.900 --> 00:00:21.890
First of all, a big thanks for staying for one
of the last sessions at this WWDC, so thank you

00:00:21.890 --> 00:00:29.670
My name is Pallavi Mehrotra and I'm part of the Software and
Services Group at Intel and I'm a software engineer there.

00:00:29.670 --> 00:00:36.510
Today myself and my colleague, Richard Hubbard also from
Software and Services group, will be presenting a talk

00:00:36.509 --> 00:00:43.239
on Scaling Performance with the Grand
Central Dispatch and Intel's Libraries.

00:00:43.240 --> 00:00:53.570
So before we dive into the talk, I want to give a
special thanks for Phil Kerly and Justin Landon both

00:00:53.570 --> 00:00:57.200
from the Software and Services Group at Intel.

00:00:57.200 --> 00:00:59.859
They guided us throughout our project.

00:00:59.859 --> 00:01:08.469
And lastly but not the least, the developers
from Grand Central Dispatch Group at Apple.

00:01:08.469 --> 00:01:09.659
Thank you.

00:01:09.659 --> 00:01:13.069
So what are our objectives today?

00:01:13.069 --> 00:01:19.589
We have 2 main objectives today: Demonstrate
performance on the latest Mac Pro platform

00:01:19.590 --> 00:01:27.409
which uses the latest Core i7 Architecture from
Intel using the Snow Leopard Operating System.

00:01:27.409 --> 00:01:36.229
And secondly, we'll demonstrate how we can implement an
application called Face Tracker using Grand Central Dispatch

00:01:36.230 --> 00:01:41.760
threading methodology and we'll
show you some results for that.

00:01:43.849 --> 00:01:50.799
So our agenda is the first thing is we'll
kind of revisit the Face Tracker Application

00:01:50.799 --> 00:01:54.329
but using Intel's Threading Building Blocks technique.

00:01:54.329 --> 00:01:57.890
So we'll visit that design a little bit.

00:01:57.890 --> 00:02:03.469
Secondly, we'll dive into how did we go
about implementing the Grand Central Dispatch

00:02:03.469 --> 00:02:07.950
and we'll show you some performance result
in processor scaling that we achieve

00:02:07.950 --> 00:02:12.849
at both these methodologies and
then we'll wrap up with a summary.

00:02:14.889 --> 00:02:22.289
So before showing all these, I want to give you a
brief overview, a background of what we did last year.

00:02:22.289 --> 00:02:29.189
We were here last year's WWDC and we
demonstrated an application called Face Tracker

00:02:29.189 --> 00:02:38.650
and this application basically simply tracks a human
face in a video clip and it tracks front and side faces

00:02:38.650 --> 00:02:41.370
and we'll show you a little bit of that a little bit later.

00:02:41.370 --> 00:02:47.120
And-- but the main thing is we demonstrated
how we can use some of the tools from Intel

00:02:47.120 --> 00:02:52.020
like Intel's Performance Primitives,
Intel's Threading Building Blocks,

00:02:52.020 --> 00:02:56.200
and we also utilized Intel's optimized Compiler,

00:02:56.199 --> 00:03:02.759
and at the same time we demonstrated 3
different threading technologies, pthreads,

00:03:02.759 --> 00:03:06.629
OpenMP and TBB or Threading Building Blocks.

00:03:06.629 --> 00:03:08.960
So we demonstrated that last year.

00:03:08.960 --> 00:03:16.490
And in addition to the design and code, we showed you
how we can scale performance using these methodologies

00:03:16.490 --> 00:03:21.650
on the previous generation Mac Pro
which ran Leopard Operating System

00:03:21.650 --> 00:03:25.400
and was running the core architecture code name Penryn.

00:03:25.400 --> 00:03:30.210
So that was last year.

00:03:30.210 --> 00:03:37.010
So before we dive too much into it, let me show
you a brief demo of what this Face Tracker app is.

00:03:37.009 --> 00:03:51.549
[ Pause ]

00:03:51.550 --> 00:03:54.570
>> So as you can see, it's just this video clip.

00:03:54.569 --> 00:04:05.180
It's a 720p clip and it's tracking a face on
the video clip using a circle around the face.

00:04:05.180 --> 00:04:09.890
Sometimes you'll see it's in red, sometimes
in green while other times in blue.

00:04:09.889 --> 00:04:13.709
And what that indicates is it's
using like various mechanisms.

00:04:13.710 --> 00:04:18.050
Sometimes it's detecting a front
face and marking it with a green.

00:04:18.050 --> 00:04:25.819
When it will detect a side face, it marks it with a
red and if it detects both, it marks it with a blue.

00:04:25.819 --> 00:04:28.560
So just to note those things.

00:04:28.560 --> 00:04:31.980
Another thing to see it's running pretty slow.

00:04:31.980 --> 00:04:39.250
It's really using majority of only one CPU at this time.

00:04:39.250 --> 00:04:41.430
It's pretty compute-intensive.

00:04:41.430 --> 00:04:43.689
It has various-- doing various things.

00:04:43.689 --> 00:04:48.000
It's detecting various kinds of
faces, side, front, et cetera.

00:04:48.000 --> 00:04:51.939
At the same time, it's doing additional
things like a quality filter.

00:04:51.939 --> 00:04:55.319
It's eliminating some of the false
positives it might be finding.

00:04:55.319 --> 00:04:59.990
Maybe there's something in the background
which is not really a face and it's marking it.

00:04:59.990 --> 00:05:04.460
And we are also at the same time
eliminating those kinds of false positives.

00:05:04.459 --> 00:05:10.810
So it's running pretty slow and
that's the simple Face Tracker App.

00:05:10.810 --> 00:05:18.430
Going back to the slides.

00:05:18.430 --> 00:05:24.980
So let's show you a little bit of
what this Face Tracker design is

00:05:24.980 --> 00:05:28.930
and how it was implemented using
the Threading Building Blocks.

00:05:28.930 --> 00:05:33.120
So as you can see the various blocks
in this diagram basically correspond

00:05:33.120 --> 00:05:36.120
to various functional units in the application.

00:05:36.120 --> 00:05:41.530
The File Decode just parses the video
clip extracting one frame data at a time

00:05:41.529 --> 00:05:48.539
It takes that data and passes onto the next stage which is
the Frontal Face Detection and that just detects front faces

00:05:48.540 --> 00:05:52.920
in the clip and marks it with the
circle over there where you want.

00:05:52.920 --> 00:05:59.580
The second state is the Profile Face Detection and
which is just detecting side faces in the clip.

00:05:59.579 --> 00:06:05.189
Then that data moves on to something
called a Quality Filter where it--

00:06:05.189 --> 00:06:11.629
one of the stages just collects some heuristics data from
the previous four or five frames and then in the next stage,

00:06:11.629 --> 00:06:17.439
it uses that information to eliminate any false
positives that might be occurring in that frame.

00:06:17.439 --> 00:06:19.750
And finally, it renders to the screen.

00:06:19.750 --> 00:06:26.139
So as you can see, this design lends
pretty well to a pipeline design.

00:06:26.139 --> 00:06:31.259
And Intel's Threading Building
Block provides such an abstraction.

00:06:31.259 --> 00:06:37.480
It provides a very good abstraction, high-level
abstraction for a pipeline-based design where each

00:06:37.480 --> 00:06:46.620
of these functional units very nicely correspond to specific
stages of the pipeline, a token in the File Decode is parsed

00:06:46.620 --> 00:06:51.870
from the file, that token is passed on to
the next stage of the pipeline and so on.

00:06:53.269 --> 00:06:59.419
You will also notice that some of these stages are
marked parallel while others are marked serial.

00:06:59.420 --> 00:07:04.199
So depending on what you're doing, if you're
parsing the file or rendering to the screen

00:07:04.199 --> 00:07:08.490
or if you're collecting some heuristics
data, those might be serial functions

00:07:08.490 --> 00:07:11.629
and those are marked as serial stages of the pipeline.

00:07:11.629 --> 00:07:19.740
While others there are detecting faces up in a given
frame or if you're eliminating any false positives,

00:07:19.740 --> 00:07:25.319
those kinds of functions can be easily done
concurrently, so those are marked as parallel.

00:07:27.470 --> 00:07:32.080
So let's dive a little bit into the
code of the Threading Building Blocks.

00:07:32.079 --> 00:07:39.240
So as you see, TBB provides a very nice
abstraction, but there are few things you need to do,

00:07:39.240 --> 00:07:44.030
you have to set up your code before
you can use this abstraction.

00:07:44.029 --> 00:07:49.709
One of the first things you have to do is you
have to create C++ classes basically one class

00:07:49.709 --> 00:07:54.229
for each stage of the pipeline of your app.

00:07:54.230 --> 00:08:02.390
Also note that you have to inherit your C++ class from
the TBB filter class so that you see in this code,

00:08:02.389 --> 00:08:08.439
then you just provide your constructor and destructor
and whatever initialization you need to do there.

00:08:08.439 --> 00:08:14.279
Also note it defines an operator function and
we'll go into detail a little bit later on that.

00:08:14.279 --> 00:08:19.509
And-- but another thing to note here
when you're defining your constructor,

00:08:19.509 --> 00:08:26.459
you need to identify if this particular
stage of your pipeline is serial or parallel.

00:08:26.459 --> 00:08:32.970
And fortunately, TBB provides a very
simple mechanism, basically a Boolean flag

00:08:32.970 --> 00:08:39.970
to save whether this stage would be serial or parallel
as you can see in this code diversity end of the slide.

00:08:39.970 --> 00:08:42.840
So that's your class definition.

00:08:42.840 --> 00:08:49.190
You would be doing this for each stage
of your TBB or application pipeline.

00:08:49.190 --> 00:08:56.410
So once you've defined your classes and your constructor,
et cetera, the next thing you have to do is the work.

00:08:56.409 --> 00:09:01.079
So for each of the classes, you
have to define your work function.

00:09:01.080 --> 00:09:03.850
And that's where this operator function comes into play.

00:09:03.850 --> 00:09:10.570
This is where you would be processing your token
or in this particular application, the frame data.

00:09:10.570 --> 00:09:15.900
Another thing to notice when you're doing this,
there's something you return out of this function

00:09:15.899 --> 00:09:23.539
and that's basically your frame data or token data which has
been modified by this stage in whichever manner you desire

00:09:23.539 --> 00:09:26.980
and then passed on to the next stage of the pipeline.

00:09:26.980 --> 00:09:31.680
If you're done with all the items to
be processed, you just return NULL.

00:09:31.679 --> 00:09:39.059
So that's how you would be implementing your work
function where your meat of the work would be happening.

00:09:39.059 --> 00:09:45.359
And now that you've set up all your classes, you have
written your work function, now you need to be able

00:09:45.360 --> 00:09:49.029
to initialize, create, and run your pipeline.

00:09:49.029 --> 00:09:51.089
So how would you initialize the pipeline?

00:09:51.090 --> 00:09:56.670
One of the first things you have to
do is initialize your TBB scheduler.

00:09:56.669 --> 00:09:59.379
Now one thing to know, you'll notice CORE_NUM.

00:09:59.379 --> 00:10:07.539
This is where you are defining how many threads you
desire and then you instantiate your pipeline object.

00:10:07.539 --> 00:10:11.309
So those are the 2 things you have to do when initializing.

00:10:14.009 --> 00:10:17.539
>> So now you've done your-- you've created your classes,

00:10:17.539 --> 00:10:22.549
you've defined your work function,
you've initialized your TBB pipeline.

00:10:22.549 --> 00:10:28.069
Now, you have to basically add your
various stages of filters of the pipeline.

00:10:28.070 --> 00:10:39.120
As you can see here, we are adding 2 filters or 2 stages
of the pipeline, the input filter and the detection filter.

00:10:39.120 --> 00:10:40.889
That's pretty easy to do as well in TBB.

00:10:40.889 --> 00:10:49.529
You just instantiate your filter stage object, in this case,
for example the input filter, invoking your constructor,

00:10:49.529 --> 00:10:55.480
initializing whatever data you want to, and
just calling the add_filter function to add it

00:10:55.480 --> 00:11:00.820
to your pipeline, so it's as simple as that.

00:11:00.820 --> 00:11:05.879
So you would do that for each stage of your TBB pipeline.

00:11:05.879 --> 00:11:10.820
So once you've added it, the only thing
remaining is running this pipeline.

00:11:10.820 --> 00:11:19.200
So that's easy to do as well, just invoke your run
function and pass in a parameter called your tbbNumPipeline

00:11:19.200 --> 00:11:25.020
which basically is setting your level of parallelism.

00:11:25.019 --> 00:11:29.519
Once you're done, you obviously
want to clear up your pipeline.

00:11:29.519 --> 00:11:37.110
So that's the sort of a gist or an overview of how
the TBB code look like, pretty straightforward.

00:11:40.409 --> 00:11:49.889
So what were some of the features or key things
about TBB that helped us when doing this application?

00:11:49.889 --> 00:11:54.029
Well, the first thing is TBB provides
number of abstractions.

00:11:54.029 --> 00:11:58.350
I mean pipeline is just one of them, so keep that in mind.

00:11:58.350 --> 00:12:04.899
Also, pipeline abstraction inherently
provides us 2 really important things.

00:12:04.899 --> 00:12:08.899
One is flow-control and the other is reordering.

00:12:08.899 --> 00:12:10.720
So why flow-control?

00:12:10.720 --> 00:12:17.320
When you have a pipeline-based design, you could have some
stages which are obviously slower while others are faster.

00:12:17.320 --> 00:12:23.100
You don't want to flood your pipeline with
tokens or frames and congest the pipeline.

00:12:23.100 --> 00:12:27.980
Maybe there are some bottlenecks down the
pipeline and you don't want to just flood it.

00:12:27.980 --> 00:12:31.050
So you need some kind of mechanism to control that.

00:12:31.049 --> 00:12:33.409
So that's where the flow-control comes in.

00:12:33.409 --> 00:12:34.689
Now, reordering.

00:12:34.690 --> 00:12:40.690
So in a typical pipeline, you would have
some serial stages and parallel stages.

00:12:40.690 --> 00:12:48.540
So if you have a parallel stage followed by a serial
stage, now what will happen is your tokens or iTunes

00:12:48.539 --> 00:12:53.559
or whatever would become out of
order in the parallel stages,

00:12:53.559 --> 00:12:58.349
but your serial stage which is following
the parallel stage is expecting these items

00:12:58.350 --> 00:13:00.800
to be in order, in the correct order.

00:13:00.799 --> 00:13:03.199
So you need some kind of reordering mechanism.

00:13:03.200 --> 00:13:05.460
And again, TBB helps us here.

00:13:05.460 --> 00:13:07.790
It gives us both these mechanisms for free.

00:13:07.789 --> 00:13:11.829
We don't need to write any user
code to achieve these 2 things.

00:13:11.830 --> 00:13:12.690
So that helped us.

00:13:12.690 --> 00:13:14.900
And then some other things.

00:13:14.899 --> 00:13:18.019
TBB is portable across platforms.

00:13:18.019 --> 00:13:23.659
It's optimized for the Intel architecture
and it's open source.

00:13:23.659 --> 00:13:27.679
There is a commercial version supported as well.

00:13:27.679 --> 00:13:33.339
So those are some of the key features
of Threading Building Blocks.

00:13:33.340 --> 00:13:35.420
So moving on.

00:13:35.419 --> 00:13:40.079
So what were our key messages from last year's WWDC?

00:13:40.080 --> 00:13:46.389
We showed you the Face Tracker design, how complex it was.

00:13:46.389 --> 00:13:52.090
I mean you can add any more amount of complexity
you want and why were we able to do this,

00:13:52.090 --> 00:13:56.080
because of all the multiple cores available to us.

00:13:56.080 --> 00:13:58.180
So that's one of the reasons.

00:13:58.179 --> 00:13:59.389
You can add more complexity.

00:13:59.389 --> 00:14:03.980
You can add more features to your
application and utilize all the resources

00:14:03.980 --> 00:14:07.450
and the cores available with the multiple cores.

00:14:07.450 --> 00:14:15.210
Secondly, we showed that there was a
potential to run even higher resolution clips.

00:14:15.210 --> 00:14:21.300
So last year we ran a 480p clip and this year we're
going to show you a much higher resolution clip.

00:14:21.299 --> 00:14:26.709
And so there was definitely with multiple
cores more resources available at hand,

00:14:26.710 --> 00:14:29.629
you can run even higher resolution videos.

00:14:29.629 --> 00:14:35.779
And lastly but not the least, by utilizing
TBB, we demonstrated that it's good

00:14:35.779 --> 00:14:42.860
to use high level threading technologies
and we'll show that with GCD again later on.

00:14:42.860 --> 00:14:47.090
So those were some of the key messages
from last year's WWDC.

00:14:47.090 --> 00:14:49.730
And these still apply.

00:14:49.730 --> 00:14:58.769
All these recommendations, all these key messages are still
valid even today and even more so as we'll demonstrate.

00:14:58.769 --> 00:15:05.049
So this year what do we have at our disposal?

00:15:05.049 --> 00:15:09.699
We have Intel's new Core i7 Architecture,
and what does that give us?

00:15:09.700 --> 00:15:12.759
It gives us a lot of enhancements.

00:15:12.759 --> 00:15:21.220
If I were to name a few, you have the enhanced memory
system, your integrated memory controller instead of your--

00:15:21.220 --> 00:15:25.600
and you have your front-- instead of the
front side bus you have the Nehalem system

00:15:25.600 --> 00:15:32.700
where you can leverage the lower latency of
accessing the local socket versus the remote.

00:15:32.700 --> 00:15:38.030
You have much larger caches and
you have more levels of caches.

00:15:38.029 --> 00:15:42.110
So you have your first level, midlevel, last level cache.

00:15:42.110 --> 00:15:48.149
More importantly, the last level cache is inclusive,
meaning you have better coherency or easier coherency.

00:15:48.149 --> 00:15:57.399
A lot of the resources like your load and store
buffers, their-- the numbers are just larger on Nehalem.

00:15:57.399 --> 00:16:00.769
Like your reorder buffer, you have much larger number.

00:16:00.769 --> 00:16:02.480
What does that mean?

00:16:02.480 --> 00:16:07.340
It means you can have a higher number of
micro apps in flight at any given time.

00:16:07.340 --> 00:16:12.389
That means more parallelism, more
resources at your disposal.

00:16:12.389 --> 00:16:13.639
So what else?

00:16:13.639 --> 00:16:15.889
I mean if I were to name, I could name more.

00:16:15.889 --> 00:16:21.590
I mean you have more cases where you
would forward your stores to loads.

00:16:21.590 --> 00:16:23.610
You have many more cases of macro-fusion.

00:16:23.610 --> 00:16:29.700
You have many more combinations of compared
jump and testing jump which are macro-fused

00:16:29.700 --> 00:16:32.950
within Nehalem architecture and so on and so forth.

00:16:32.950 --> 00:16:39.780
You have a lot of power efficiency features
like Loop Stream Detector which was mentioned

00:16:39.779 --> 00:16:43.899
by Phil in the earlier presentation and so on.

00:16:43.899 --> 00:16:48.449
But lastly and not the least, we have Hyperthreading
and we are going to go into a little bit

00:16:48.450 --> 00:16:51.610
of details, give you an overview of what that is.

00:16:51.610 --> 00:16:57.360
So with all of this, the new architecture
and of course we have the Apple's new

00:16:57.360 --> 00:17:01.919
and latest Operating System, Snow Leopard, what can we do?

00:17:01.919 --> 00:17:10.349
So we'll illustrate even higher better performance,
better processor scaling with the Face Tracker App.

00:17:10.349 --> 00:17:16.839
We're going to show you what Hyperthreading gives you
and then we're going to take this app and convert it

00:17:16.839 --> 00:17:23.279
into the Grand Central Dispatch methodology and
see what kind of performance we get with that.

00:17:23.279 --> 00:17:26.389
So that's what we're going to do this year.

00:17:26.390 --> 00:17:32.870
So this is a graph showing performance
of the Face Tracker Application.

00:17:32.869 --> 00:17:40.369
Running a 720p clip on Nehalem,
the numbers are frames per second.

00:17:40.369 --> 00:17:46.789
The blue bars or the gray bars are the ones
which were obtained from the Penryn platform

00:17:46.789 --> 00:17:49.409
which was the previous generation of the Mac Pro

00:17:49.410 --> 00:17:54.810
and the red bars are the Nehalem platform
which is the latest Mac Pro generation.

00:17:54.809 --> 00:17:59.470
And it shows the data for various cores.

00:17:59.470 --> 00:18:05.519
And the Penryn system was running on about 3.2 gigahertz,

00:18:05.519 --> 00:18:09.809
the Nehalem is running on a lower
frequency, about 2.93 gigahertz.

00:18:09.809 --> 00:18:14.700
Additionally, we got a boost by compiling
our application with the Intel Compiler

00:18:14.700 --> 00:18:18.569
which has obviously been optimized
for the Nehalem architecture.

00:18:18.569 --> 00:18:23.200
So obviously, longer bars are better and what did we see?

00:18:23.200 --> 00:18:29.980
We found about 15 to 18 percent boost
in performance with no code changes.

00:18:29.980 --> 00:18:35.269
We just took the app, just out of the box,
we run it on Nehalem and we got about 15

00:18:35.269 --> 00:18:39.500
to 18 percent pretty consistently on all the cores.

00:18:39.500 --> 00:18:47.480
Another thing to notice with the new architecture and
the platform, we can run the 720 clip in real time.

00:18:47.480 --> 00:18:49.559
This was not possible last year.

00:18:49.559 --> 00:18:53.139
This is possible now, with more
resources, yadda-yadda-yadda,

00:18:53.140 --> 00:18:55.470
everything I've mentioned about the architecture.

00:18:55.470 --> 00:18:59.019
You have more resources available.

00:18:59.019 --> 00:19:06.930
So definitely great, so Nehalem is great.

00:19:06.930 --> 00:19:11.539
OK. So like I promised to you, I'll
talk a little bit about Hyperthreading.

00:19:11.539 --> 00:19:13.230
So what is Hyperthreading?

00:19:13.230 --> 00:19:18.029
So just a brief overview, no lot of details or anything.

00:19:18.029 --> 00:19:25.889
It's as basic as running 2 hardware threads,
2 logical threads on the same processor core.

00:19:25.890 --> 00:19:29.600
And what's happening is the front-end of
the processor, the allocate, the retire,

00:19:29.599 --> 00:19:39.869
all those subsystems of the processor basically alternate
between the two logical threads running in each cycle.

00:19:39.869 --> 00:19:44.799
So in each cycle, each of the threads
are given an equal chance to run.

00:19:44.799 --> 00:19:49.889
And if that thread doesn't need that cycle, the other--
it's fair game that the other thread can take it.

00:19:49.890 --> 00:19:54.420
So it does ensure to some degree a
fairness between the 2 logical threads.

00:19:54.420 --> 00:19:59.650
So on the right we have these 2 diagrams.

00:19:59.650 --> 00:20:04.519
The one on the left is when the
Hyperthreading is not turned on.

00:20:04.519 --> 00:20:07.920
On the right it's when the Hyperthreading is turned on.

00:20:07.920 --> 00:20:16.580
The various blocks or boxes in this are basically the
blue ones are Thread 1 and the green ones are Thread 2.

00:20:16.579 --> 00:20:20.250
>> The white ones are the Idle times.

00:20:20.250 --> 00:20:27.279
The vertical axis is time and the
horizontal one is basically execution units.

00:20:27.279 --> 00:20:30.319
So as you can see when Hyperthreading is turned on,

00:20:30.319 --> 00:20:36.319
your application is finishing up
in nowhere all less amount of time.

00:20:36.319 --> 00:20:41.059
When Hyperthreading is not turned on, the
2 threads are running basically serially.

00:20:41.059 --> 00:20:45.379
So you have the concurrency with Hyperthreading.

00:20:45.380 --> 00:20:52.580
Now talking a little bit about the various resources,
the various processor resources, that type of threading.

00:20:52.579 --> 00:20:59.539
So a lot of these buffers and queues which I had talked
about earlier are basically shared or partitioned.

00:20:59.539 --> 00:21:06.490
There are some which are statically shared like your
buffers, like your load buffers and store buffers,

00:21:06.490 --> 00:21:10.839
your reservation station, your
reorder buffers, your instruction TLB,

00:21:10.839 --> 00:21:14.839
all those are statically partitioned between the 2 threads.

00:21:14.839 --> 00:21:23.119
But then there are other resources which are competitively
shared and those are your caches, your first level cache,

00:21:23.119 --> 00:21:26.739
your midlevel, your last level cache, your data TLB,

00:21:26.740 --> 00:21:32.460
all those are competitively shared
between the 2 logical threads.

00:21:32.460 --> 00:21:35.710
So what else does the architecture give you?

00:21:35.710 --> 00:21:39.920
The architecture provides you with the four-wide decode.

00:21:39.920 --> 00:21:43.170
You could decode up to four instructions every cycle.

00:21:43.170 --> 00:21:47.800
You could retire up to 4 and you
have up to 7 execution ports,

00:21:47.799 --> 00:21:54.389
3 for your memory applications and
4 for your computation operations.

00:21:54.390 --> 00:21:59.420
So with all this, I mean what's the best way to utilize it?

00:21:59.420 --> 00:22:03.690
Feed the execution engine and one
of the ways to do this is thread.

00:22:03.690 --> 00:22:07.490
So Hyperthreading helps you certainly with that.

00:22:07.490 --> 00:22:14.440
Also when you have more number of threads, it's pretty
obvious that you can hide latency of a single thread

00:22:14.440 --> 00:22:19.340
or even your slower threads so you get that.

00:22:19.339 --> 00:22:22.179
Plus, Hyperthreading is a power efficient feature.

00:22:22.180 --> 00:22:29.150
I mean it's much cheaper to add a logical
thread than to add a new core completely.

00:22:29.150 --> 00:22:32.550
So it's definitely a low die cost.

00:22:32.549 --> 00:22:38.680
And depending on the design of your
application, it can boost your performance.

00:22:38.680 --> 00:22:41.539
So that's Hyperthreading in a nutshell.

00:22:41.539 --> 00:22:49.769
So how can you turn this on or off in Snow Leopard?

00:22:49.769 --> 00:22:54.619
This is just a snapshot of the
Processor Palette in Snow Leopard.

00:22:54.619 --> 00:23:02.109
As you can see there's a check box which is
circled in red and you can turn that on and off.

00:23:02.109 --> 00:23:05.209
I think by default it comes turned on.

00:23:05.210 --> 00:23:10.420
And on the right you see the usage of the various cores.

00:23:10.420 --> 00:23:16.620
Per core you can see about 2 lines, one basically
corresponding each to the logical threads

00:23:16.619 --> 00:23:22.719
of the logical CPUs, so showing you how
to enable or disable Hyperthreading.

00:23:22.720 --> 00:23:32.190
And now, I'm going to show you a demo with all
cores turned on, with Hyperthreading turned on,

00:23:32.190 --> 00:23:35.910
and going back to the same application,
the Face Tracker Application and see--

00:23:35.910 --> 00:23:43.009
I mean if you remember when I ran it the first time it
was pretty much crawling and let's see how does it do now.

00:23:43.009 --> 00:23:49.549
[ Pause ]

00:23:49.549 --> 00:23:52.129
>> I mean it's running much, much faster.

00:23:52.130 --> 00:23:54.480
It's doing exactly the same things.

00:23:54.480 --> 00:23:56.380
[Applause] It's running a 720p.

00:23:56.380 --> 00:24:00.320
You can look at the CPU usage, I mean
it's using-- utilizing all the cores.

00:24:00.319 --> 00:24:06.889
Let me just run it one more time, run pretty quickly.

00:24:06.890 --> 00:24:10.340
Still detecting all the side and the front faces, et cetera.

00:24:10.339 --> 00:24:15.529
You don't see false positives, that's because
of the quality filter functionality we added.

00:24:15.529 --> 00:24:18.639
And there it is.

00:24:18.640 --> 00:24:20.840
So Nehalem is good.

00:24:22.289 --> 00:24:25.879
Go back to the slides.

00:24:25.880 --> 00:24:36.420
So that was the demo on the 16 logical core or
8-core machine with Hyperthreading turned on.

00:24:36.420 --> 00:24:39.279
So what does, I mean, Hyperthreading give you?

00:24:39.279 --> 00:24:45.789
We talked about how great it is and all that but we
go back to the frame, to the Face Tracker Application.

00:24:45.789 --> 00:24:50.359
Again, we collect the frames per second numbers
and here we are running on the same platform,

00:24:50.359 --> 00:24:59.199
the same Macro platform except in one case, we have turned
off the Hyperthreading and in the other case we turn it on.

00:24:59.200 --> 00:25:04.240
Obviously, again, the longer is better
and you can see Hyperthreading gives

00:25:04.240 --> 00:25:08.370
about 8 to 25 percent boost in your performance.

00:25:08.369 --> 00:25:11.500
Again, I've made no code changes with the Face Tracker App.

00:25:11.500 --> 00:25:16.059
It's the same code just Hyperthreading turned on.

00:25:16.059 --> 00:25:22.970
It's running on the same 2.93 gigahertz Macro
platform using the new Nehalem Architecture,

00:25:22.970 --> 00:25:27.970
Snow Leopard OS and again, leveraging Intel's Compiler.

00:25:27.970 --> 00:25:33.170
So there you have it, HT giving you
a boost of about 8 to 25 percent.

00:25:33.170 --> 00:25:36.940
It's again running the same 720p clip.

00:25:38.289 --> 00:25:41.889
So to summarize, what did we see?

00:25:41.890 --> 00:25:52.300
Going from the previous generation Mac Pro using the
Leopard OS Operating System, and the Penryn Architecture,

00:25:52.299 --> 00:25:58.789
going from there, and then moving on to the
latest generation Mac Pro utilizing Snow Leopard

00:25:58.789 --> 00:26:06.069
and Core i7 Nehalem Architecture using the Intel
Compiler, we saw about 29 percent boost in performance

00:26:06.069 --> 00:26:11.079
when running a 720p clip without any code changes.

00:26:11.079 --> 00:26:17.819
So we demonstrated that not only the new
architecture, the OS, the tools are good for you.

00:26:17.819 --> 00:26:21.919
We also showed you that we can now
support much higher resolution videos.

00:26:21.920 --> 00:26:23.830
Last year we ran 420.

00:26:23.829 --> 00:26:31.109
This year we're running-- I think that should
be 480-- and this year we are running 720p clip.

00:26:31.109 --> 00:26:41.149
We also demonstrated that we can support many more thread
style, 16 versus 8 before and that's the TBB design for you.

00:26:41.150 --> 00:26:46.620
And now we're going to dive into how did
we go about porting this TBB based design

00:26:46.619 --> 00:26:52.909
over to the Grand Central Dispatch based design and to
talk about that I'll invite my colleague Richard Hubbard.

00:26:52.910 --> 00:26:53.250
Thank you.

00:26:53.250 --> 00:26:53.730
[ Applause ]

00:26:53.730 --> 00:26:58.190
>> Thank you.

00:27:01.039 --> 00:27:10.099
Hello! So now you've seen the background on Face
Tracker with TBB and now as Pallavi mentioned I'll talk

00:27:10.099 --> 00:27:13.809
about how we ported the application
to Grand Central Dispatch.

00:27:13.809 --> 00:27:16.259
So it's exciting always to work with a new technology.

00:27:16.259 --> 00:27:25.819
Apple presents this new technology to take advantage of a
multicore platform and our goals with doing this port was

00:27:25.819 --> 00:27:30.609
to learn about the technology, how
do we apply it to Face Tracker,

00:27:30.609 --> 00:27:38.799
how does it compare to previous threading
frameworks and we also wanted to either maintain

00:27:38.799 --> 00:27:44.169
or see if we could even improve performance by using GCD.

00:27:44.170 --> 00:27:49.269
Some of the challenges we faced, and some of
these were really specific to our application.

00:27:49.269 --> 00:27:57.000
We had a C++ based application and the APIs that we used

00:27:57.000 --> 00:28:01.029
with Grand Central Dispatch just
take-- they took the function pointers.

00:28:01.029 --> 00:28:10.369
We used the _f version of Dispatch so we had to figure out,
you know, will we invoke the C++ method and we also had

00:28:10.369 --> 00:28:17.829
to provide some of the infrastructure that TBB
gave us, so the flow-control, the pipelining,

00:28:17.829 --> 00:28:21.059
and the reordering, we had to handle that.

00:28:21.059 --> 00:28:29.950
And of course this was a-- you know when you think
about threading, multi-threaded apps here we are working

00:28:29.950 --> 00:28:33.299
with a new technology where you
probably think of the common things.

00:28:33.299 --> 00:28:37.329
How am I going to start threads,
how am I going to synchronize access

00:28:37.329 --> 00:28:42.299
to various data items and is this a headache or not.

00:28:42.299 --> 00:28:47.930
But the cool thing about GCD really is
that it manages so much of that for you.

00:28:47.930 --> 00:28:51.170
It makes it very simple.

00:28:51.170 --> 00:29:00.289
So I just wanted to walk through some of the things
that we had to provide that GCD wouldn't give us.

00:29:00.289 --> 00:29:06.829
So we had to, first of all, we had to
pipeline the application and the pipeline was

00:29:06.829 --> 00:29:09.309
as Pallavi showed is built very simply in TBB.

00:29:09.309 --> 00:29:18.609
You just add objects to the pipeline and with GCD
we had to just define a different type of class.

00:29:18.609 --> 00:29:28.159
We called it a stage worker class and the class just
had links to the queue of the next stage in the pipeline

00:29:28.160 --> 00:29:33.300
and also a reference to the next object in the pipeline.

00:29:33.299 --> 00:29:42.049
So that's how we connected the pipeline
together and I just wanted to show you that.

00:29:42.049 --> 00:29:49.899
When we instantiated the pipeline in this approach we
worked backwards, so we instantiated the output stage first

00:29:49.900 --> 00:30:00.430
and then the last profile stage and as you see, when
you instantiate the object you have to create the link

00:30:00.430 --> 00:30:08.190
and that's why we worked backwards, and looking down at the
bottom of the slide instantiating, you know, the stage N-2

00:30:08.190 --> 00:30:11.049
and then connecting that up to
the next stage in the pipeline.

00:30:11.049 --> 00:30:13.549
So that really wasn't that hard to do.

00:30:13.549 --> 00:30:19.200
>> So that's how the stages in the
pipeline were connected together.

00:30:19.200 --> 00:30:23.910
Another thing we had to implement
was moving through the pipeline.

00:30:23.910 --> 00:30:34.900
So again, this was pretty transparent by TBB and in
GCD all we really had to do was there was a simple,

00:30:34.900 --> 00:30:40.240
very small structural context information
that was passed through the pipeline.

00:30:40.240 --> 00:30:45.789
This is different from the application context
that you may have seen in the GCD deep dive.

00:30:45.789 --> 00:30:53.259
This was just a structure with a pointer to
a frame structure and a pointer to an object

00:30:53.259 --> 00:30:58.559
that was going to handle the work for a given stage.

00:30:58.559 --> 00:31:07.730
So when a stage is done with the work, all it had to do
was using the next pointer, it would just get the queue

00:31:07.730 --> 00:31:14.069
for the next stage and put work on to that queue.

00:31:14.069 --> 00:31:16.669
Here's an example of that.

00:31:16.670 --> 00:31:23.330
There at the top is just a thin layer
where we just cached this void pointer.

00:31:23.329 --> 00:31:33.289
With GCD, when you invoke the dispatch_async you're
passing in a queue, you pass in a context pointer

00:31:33.289 --> 00:31:37.559
and you pass in the work that you want to do.

00:31:37.559 --> 00:31:46.859
So from that we had to get the pointer to the context and
then just invoke the right method which we called stage work

00:31:46.859 --> 00:31:54.990
and then at the end after a stage is done with its work
it's very simple, it just acquires the next stage's queue

00:31:54.990 --> 00:32:00.980
and invokes dispatch_async with that queue

00:32:00.980 --> 00:32:12.220
and we continually would use even though use object
stage was always used but because it was object oriented,

00:32:12.220 --> 00:32:15.920
each object would provide its own
implementation of stage work.

00:32:15.920 --> 00:32:24.440
So this is how we managed to just use one simple
thin layer and keep moving work through the pipeline.

00:32:24.440 --> 00:32:31.750
Another difference between TBB and GCD is how to specify
whether you're dealing with a parallel or a serial stage.

00:32:31.750 --> 00:32:40.049
So Pallavi showed how you do that when you invoke the base
class constructor in TBB, and in GCD the way you do it is

00:32:40.049 --> 00:32:42.759
with the type of queue that you instantiate.

00:32:42.759 --> 00:32:48.279
So there's a global concurrent queue if you
want to work, if you have a parallel stage

00:32:48.279 --> 00:32:55.779
so you would simply call dispatch_get_global_queue
to get that global concurrent queue.

00:32:55.779 --> 00:32:59.859
There's an alternative way to do
this which we experimented with.

00:32:59.859 --> 00:33:06.719
So if you have a parallel stage you could
have an array of queues for that stage.

00:33:06.720 --> 00:33:16.440
Now this creates some problems or issues which I'll go into
in a minute but that's, if you didn't want to use this,

00:33:16.440 --> 00:33:20.000
the global concurrent queue, you
could do it this way as well.

00:33:20.000 --> 00:33:28.700
And the performance was very similar, but there's an
advantage to using the global queue and if you're using,

00:33:28.700 --> 00:33:38.630
if you know your stage is serial, just, you can instantiate
just a single FIFO queue and that will handle all your work.

00:33:39.650 --> 00:33:43.330
So, here's an example of that.

00:33:43.329 --> 00:33:47.159
Here we see, I think I went.

00:33:47.160 --> 00:33:48.890
Oh, no. I'm sorry.

00:33:48.890 --> 00:33:51.280
I just want to go over the complexity.

00:33:51.279 --> 00:33:57.529
If you do choose to go with the parallel
queues, now you have some overhead to handle.

00:33:57.529 --> 00:34:00.389
This shows kind of a very generic approach

00:34:00.390 --> 00:34:07.600
where a given stage doesn't know a priori
whether the next stage is serial or parallel.

00:34:07.599 --> 00:34:14.089
So it looks at the queue count of the next
stage and if it's 1 then it knows it's serial

00:34:14.090 --> 00:34:17.780
and that's pretty straightforward then
you just queue it up on that queue.

00:34:17.780 --> 00:34:28.110
But if the next stage was parallel you have to somehow
spread the work out among all this array of serial queues.

00:34:28.110 --> 00:34:31.309
So this is just a simple round robin approach

00:34:31.309 --> 00:34:35.469
but when you use the global concurrent queue
you don't have to worry about any of this.

00:34:35.469 --> 00:34:37.839
GCD just handles it for you.

00:34:37.840 --> 00:34:42.000
So from that point of view, you
know one of our key learnings was

00:34:42.000 --> 00:34:47.860
to use the global concurrent queue
when you need to work in a parallel.

00:34:47.860 --> 00:34:52.269
So let GCD do that work for you.

00:34:52.269 --> 00:34:56.039
Another difference was detecting
whether or not the work was done.

00:34:56.039 --> 00:35:02.400
So with TBB the first stage returns a
null to the framework and then TBB knows

00:35:02.400 --> 00:35:06.960
that that's the signal that the work is done.

00:35:06.960 --> 00:35:09.909
In GCD the pipeline is the same.

00:35:09.909 --> 00:35:17.460
As I mentioned earlier we didn't want to try-- we
didn't want to change much of this application.

00:35:17.460 --> 00:35:24.590
We wanted to port to GCD with minimal
changes so, you know we, of course,

00:35:24.590 --> 00:35:30.390
so the first stage we saw it's reading a movie,
it knows the frame count and it can easily find

00:35:30.389 --> 00:35:36.960
out when the last frame is there and what we did with
GCD was we kind of circumvented the pipeline when we saw

00:35:36.960 --> 00:35:44.579
that it was the last frame and we sent a special
frame with a flag marked that it was the last frame.

00:35:44.579 --> 00:35:52.489
And rather than push that through the entire pipeline which
would push a lot of non-necessary work we just sent it right

00:35:52.489 --> 00:35:58.859
to the output stage, and from there then the
main would then just wait for a semaphore.

00:35:58.860 --> 00:36:04.110
So that was another mechanism that
GCD provides is the semaphores.

00:36:04.110 --> 00:36:07.440
Here's an example of this.

00:36:07.440 --> 00:36:09.409
This is the input stage.

00:36:09.409 --> 00:36:17.519
It's just watching the frame count and when it gets
down to zero it knows that it's at the last frame.

00:36:17.519 --> 00:36:28.849
It instantiates a separate async context and marks it
such that it wants to send this right to the output stage.

00:36:28.849 --> 00:36:33.969
It marks the frame number and this is
important because of the parallel stages

00:36:33.969 --> 00:36:37.459
and the out of ordering that could happen.

00:36:37.460 --> 00:36:44.320
You don't want to send this frame down to the
output stage directly then it could show well,

00:36:44.320 --> 00:36:50.190
without marking it as the last frame because you might
miss some of the frames that are meant to be displayed.

00:36:50.190 --> 00:36:58.519
And then you simply put it on the queue for the output stage
and drop down and wait for that semaphore at the bottom,

00:36:58.519 --> 00:37:04.119
and there the main just goes to sleep
using the dispatch time forever.

00:37:04.119 --> 00:37:12.349
So here's a little walkthrough of how
we-- from a kind of a queue point of view,

00:37:12.349 --> 00:37:18.469
on the bottom you see the global concurrent
queue, three other FIFO queues that we used.

00:37:18.469 --> 00:37:21.119
One was used for reordering.

00:37:21.119 --> 00:37:28.750
The heuristics data stage that Pallavi
mentioned earlier is also a FIFO queue.

00:37:28.750 --> 00:37:33.320
In the output stage, of course, we
need to present the frames in order.

00:37:33.320 --> 00:37:41.590
So it begins by main putting work on the global
concurrent queue for the frontal detect stage

00:37:41.590 --> 00:37:48.289
and the frontal detect stage does
its work and it queues up more work

00:37:48.289 --> 00:37:50.789
on the exact same queue but it's for different work.

00:37:50.789 --> 00:37:57.110
So you can use the same queue and
just put whatever work you like on it.

00:37:57.110 --> 00:38:00.099
Well at least it worked here just fine.

00:38:00.099 --> 00:38:09.279
So when the profile detect was finished it knows that
the next stage is connected up to the reorder stage,

00:38:09.280 --> 00:38:16.320
so gets its queue out of the object
and puts the work on the reorder stage.

00:38:16.320 --> 00:38:23.769
Now, here you also see some of the flexibility
that you have when you work with GCD.

00:38:23.769 --> 00:38:30.429
The output stage also requires work to be in order
but we didn't throw that on a reorder stage first.

00:38:30.429 --> 00:38:37.199
So you might have some reason where you want a separate
reorder stage or you could just embed that work

00:38:37.199 --> 00:38:39.989
into the stage that needs the reordering.

00:38:39.989 --> 00:38:44.089
So it's very flexible in that manner.

00:38:44.090 --> 00:38:51.090
Once the data heuristics stage is done, it queues
work up for some more parallel stages so it goes back

00:38:51.090 --> 00:39:00.680
on to the global concurrent queue and the two drawing stages
complete and finally after the draw a profile stage is done,

00:39:00.679 --> 00:39:04.889
it queues work on the output stage's queue.

00:39:04.889 --> 00:39:11.839
We also experimented with even more
queues and when we first began this work,

00:39:11.840 --> 00:39:20.280
we separated out into separate objects how
do we move work from one stage to another.

00:39:20.280 --> 00:39:28.460
So we had what we called a pipe connector and its only
job was to take work from stage A and move it to stage B.

00:39:28.460 --> 00:39:30.440
So we had more queues.

00:39:30.440 --> 00:39:35.750
We probably had at least-- well, we didn't
have dozens but we had at least twice this many

00:39:35.750 --> 00:39:41.590
and it presented no real performance
impact so the number of queues didn't seem

00:39:41.590 --> 00:39:45.460
to be a problem at all with the Face Tracker demo.

00:39:45.460 --> 00:39:53.010
Then one of the last pieces of the infrastructure
we needed was a way to reorder frames.

00:39:53.010 --> 00:39:56.910
So they're processed in order as reading the film clip.

00:39:56.909 --> 00:40:03.049
At some point in the pipeline they pass through parallel
stages to achieve the concurrency and get to speed up

00:40:03.050 --> 00:40:06.320
and they have to get processed in order again.

00:40:06.320 --> 00:40:10.539
So it's a rather simple reorder algorithm.

00:40:10.539 --> 00:40:19.449
>> We just cache the pointers to context that are out of
order and then when the expected one comes in we'll walk

00:40:19.449 --> 00:40:24.649
through that cache and just play
out everything that's in that cache.

00:40:24.650 --> 00:40:31.079
So here's a diagram, a little picture
of that, so this is the first stage.

00:40:31.079 --> 00:40:37.860
Again, the file decode, everything comes in order and
after the frontal face and the profile detection stages,

00:40:37.860 --> 00:40:43.840
you just really have no idea what the
order could be, and so then you reorder.

00:40:43.840 --> 00:40:49.550
Again, there's another parallel stage and
back to in order at the display output.

00:40:49.550 --> 00:40:59.370
Just to show you the algorithm we used for this,
we first looked to see if it's the expected frame

00:40:59.369 --> 00:41:05.480
so we watched the frame count as it comes in and
if it's not the frame we expect then we cache it

00:41:05.480 --> 00:41:14.860
and we form an index based on a simple module of
the frame number and we increment the cache count

00:41:14.860 --> 00:41:19.530
so we don't want to-- later on we didn't want
to walk through that cache unless we knew

00:41:19.530 --> 00:41:21.160
that there were some out of order frames in there.

00:41:21.159 --> 00:41:25.789
So that's just a simple flag to help us with that.

00:41:25.789 --> 00:41:32.559
And as other frames come in and we see they are the
frame we want, then we simply display that frame,

00:41:32.559 --> 00:41:35.739
check that flag to see if there's
something in the cache and if it is,

00:41:35.739 --> 00:41:38.309
we'll play out everything in the cache that's in order.

00:41:38.309 --> 00:41:44.489
So that was pretty straightforward,
worked really nicely for us.

00:41:44.489 --> 00:41:52.059
And there was-- another thing was the flow-control
and you get the smoothest presentation by that,

00:41:52.059 --> 00:41:59.250
the video just frankly looks the best when you limit
the number of frames in flight and the number works best

00:41:59.250 --> 00:42:02.590
as equal to the number of CPUs in your system.

00:42:02.590 --> 00:42:09.990
So you could either detect the number of
cores and adjust your parallel or limit your--

00:42:09.989 --> 00:42:17.359
the number of frames and flight at anytime to that number
and again, here's-- the GCD gives us a mechanism to do this.

00:42:17.360 --> 00:42:21.880
The semaphores could either be used as
counting semaphores where you initialize

00:42:21.880 --> 00:42:26.320
into the maximum number of things that you want to manage.

00:42:26.320 --> 00:42:32.460
The input stage decrements as it
attempts to test the semaphore

00:42:32.460 --> 00:42:38.289
and as long it doesn't go below zero you won't
sleep and then you add work to the pipeline,

00:42:38.289 --> 00:42:43.449
and then as the output stage processes
the work, it increments that semaphore.

00:42:43.449 --> 00:42:49.939
So it was very easy to use, no problem at all and we
used another semaphore as well at the end of the movie.

00:42:49.940 --> 00:42:55.559
So after the main had queued up the last
frame, it waits for the movie to complete.

00:42:55.559 --> 00:43:02.840
So this shows you the feedback, the flow control
between the input and the output stages so you have

00:43:02.840 --> 00:43:06.980
to use dispatch_semaphore_create
to instantiate the semaphore.

00:43:06.980 --> 00:43:18.230
Semaphore wait on the input side and dispatch semaphore
signal when the output stage is done with the work.

00:43:18.230 --> 00:43:25.030
So here's just the code to do that, so another
type, dispatch_semaphore_t for the semaphore.

00:43:25.030 --> 00:43:33.930
We initialize it with the number of CPUs,
so if we were on either 2 or 8 or 16

00:43:33.929 --> 00:43:42.789
and then the input stage calls dispatch_semaphore_wait at
the top of the loop, does its work and the output stage

00:43:42.789 --> 00:43:45.779
in the bottom simply signals the semaphore.

00:43:45.780 --> 00:43:49.930
So again, very straightforward, nice, easy to use.

00:43:49.929 --> 00:43:53.009
So now I'd like to show you the demo using GCD

00:43:53.010 --> 00:44:07.390
[ Pause ]

00:44:07.389 --> 00:44:09.009
>> OK. Helps if I hit Enter.

00:44:09.010 --> 00:44:14.410
[ Pause ]

00:44:14.409 --> 00:44:18.480
>> So there you see the works being
distributed across the cores.

00:44:18.480 --> 00:44:25.010
It's moving pretty quickly and in case
you looked away, we'll run it once more.

00:44:25.010 --> 00:44:34.620
[ Pause ]

00:44:34.619 --> 00:44:41.779
>> So, it's pretty hard to tell the difference when you
look by the eye between the TBB and GCD implementations.

00:44:41.780 --> 00:44:47.540
But now let's look at some numbers
and compare GCD and TBB performance.

00:44:47.539 --> 00:44:57.070
So these are on the same Nehalem system, the dual-quad-core
system and these numbers are all with Hyperthreading enabled

00:44:57.070 --> 00:45:03.390
and as you can see for the 1 core,
2, and 4, the numbers are very close.

00:45:03.389 --> 00:45:11.190
So this shows how you get a lot of choices here to
get good performance but note on the 8-core system

00:45:11.190 --> 00:45:19.740
that GCD actually performed about little more than 5
percent better than TBB and both of the frameworks were able

00:45:19.739 --> 00:45:24.009
to achieve real-time performance on the 8-core system.

00:45:24.010 --> 00:45:32.140
So we're really thrilled with that to see, to play with
something new and see that the performance was really nice.

00:45:32.139 --> 00:45:35.159
Now how did GCD scale across processors?

00:45:35.159 --> 00:45:42.309
So this slide shows the same system and starting
on the lowest that's the baseline number.

00:45:42.309 --> 00:45:48.049
It's the 1-core without Hyperthreading and then we enabled
Hyperthreading and then 2-core is with and without.

00:45:48.050 --> 00:45:52.590
And as you can see it scales up
pretty nicely and you get some--

00:45:52.590 --> 00:46:02.900
between 20 and 30 some percent0 performance with this
application with Hyperthreading and scales up to 8.3X

00:46:02.900 --> 00:46:05.440
when all 8 cores with Hyperthreading are enabled.

00:46:05.440 --> 00:46:11.559
So this scaled very nicely as well.

00:46:11.559 --> 00:46:16.820
So, just to go through some of the
takeaways from our experience using GCD,

00:46:16.820 --> 00:46:20.960
it's a very good framework for
centralized thread management.

00:46:20.960 --> 00:46:29.119
It allows you to construct pipelines in the way you
might need to do it and so TBB gives you a nice framework

00:46:29.119 --> 00:46:36.549
for linear pipelines but if your pipeline is
different then you can do whatever you really need

00:46:36.550 --> 00:46:41.220
to do with GCD-- let GCD do the work for you.

00:46:41.219 --> 00:46:50.189
We tried to work with parallel or rather an
array of queues and it wasn't that hard to manage

00:46:50.190 --> 00:46:56.400
but it was an additional piece of work you had
to put into your application, so let GCD do that.

00:46:56.400 --> 00:47:00.030
The global concurrent queue takes away that burden.

00:47:00.030 --> 00:47:06.840
One thing to point out, the Intel Compiler currently
does not support block objects but there is--

00:47:06.840 --> 00:47:14.410
the function pointer versions of GCD work just fine
and there's no performance penalty that we could tell.

00:47:14.409 --> 00:47:18.779
Some other points to bring out,
standard debugging techniques

00:47:18.780 --> 00:47:23.140
that you know from multi-threading all apply here.

00:47:23.139 --> 00:47:27.739
Our application evolved overtime.

00:47:27.739 --> 00:47:32.689
When we first started we mocked
up a very simple application just

00:47:32.690 --> 00:47:37.909
to learn the APIs and we started with a synchronous model.

00:47:37.909 --> 00:47:45.469
We used the sync APIs first that worked just
fine then switched over to the async versions

00:47:45.469 --> 00:47:52.639
and we avoided the parallel queues at first and that
just delayed the issue of dealing with reordering.

00:47:52.639 --> 00:47:59.849
If everything is serial, you can debug the rest of the work
of your pipeline and not worry about reordering and things.

00:47:59.849 --> 00:48:07.579
So you could consider putting that piece off 'til the
end and just work with all serial queues at first.

00:48:07.579 --> 00:48:12.489
So those are just some of the things
we learned working with GCD.

00:48:12.489 --> 00:48:19.289
So to summarize, Snow Leopard and Nehalem
Architecture are really a winning combination.

00:48:19.289 --> 00:48:29.940
You get 15 to 18 percent performance gain from the
Hyperthreading without any changes to your application.

00:48:29.940 --> 00:48:35.389
Hyperthreading provides an additional gain, 8 to 25 percent.

00:48:35.389 --> 00:48:39.819
Here we have another great choice for threading framework.

00:48:39.820 --> 00:48:45.460
We've got TBB and GCD and the key messages that
we presented last year are still true today.

00:48:45.460 --> 00:48:52.230
As you continue to add more and more features and complexity
to your application, you want to be able to take advantage

00:48:52.230 --> 00:48:59.740
of the multicore systems and use technologies like Threading
Building Blocks and Grand Central Dispatch to do that.