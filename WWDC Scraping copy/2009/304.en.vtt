WEBVTT

00:00:12.550 --> 00:00:13.710
>> Good morning, everybody.

00:00:13.710 --> 00:00:16.530
And welcome to WWDC 2009.

00:00:16.530 --> 00:00:20.900
My name is Allan Schaffer, I'm Apple's Technology
Graphics Evangelist, and I'm going to be talking

00:00:20.899 --> 00:00:24.399
to you this morning about game development for the iPhone.

00:00:24.399 --> 00:00:29.329
So welcome to the center of the universe.

00:00:29.329 --> 00:00:29.539
[ Laughter ]

00:00:29.539 --> 00:00:33.240
>> You know, last year I got up here and I
had the privilege of making the prediction

00:00:33.240 --> 00:00:37.820
that we were all witnessing the
birth of a new mobile games platform.

00:00:37.820 --> 00:00:44.980
We talked last year about the iPhone 3G and the iPod touch
as being such fantastic platforms for handheld games.

00:00:44.979 --> 00:00:51.129
Last year went through a lot of technologies that are
available on the iPhone, iPhone -- and iPod touch.

00:00:51.130 --> 00:00:56.370
Things like touch input, to be able to directly
manipulate a character within your game,

00:00:56.369 --> 00:00:59.899
and the kind of game play experiences that that enables.

00:00:59.899 --> 00:01:02.179
Also talked about the accelerometer.

00:01:02.179 --> 00:01:08.519
So just the idea of being able to use the
orientation of the device as a game controller itself.

00:01:08.519 --> 00:01:10.589
Also, the iPhone is always connected.

00:01:10.590 --> 00:01:17.439
And so the ability to have multiplayer
game experiences was created for you then.

00:01:17.439 --> 00:01:20.679
Last year we also went through a lot
of the graphics and media technologies.

00:01:20.680 --> 00:01:25.440
We started with a rich graphic
stack use OpenGL ES on the iPhone.

00:01:25.439 --> 00:01:31.890
Went into the capabilities for audio playback
and recording, and also for video playback.

00:01:31.890 --> 00:01:36.239
So you guys took all of that stuff
and it's just been an amazing year.

00:01:36.239 --> 00:01:39.569
I think you saw something like
this yesterday and downstairs.

00:01:39.569 --> 00:01:45.159
These are a few of the icons of games that are on the
App Store today, and I just have a question for you guys,

00:01:45.159 --> 00:01:48.859
the audience, raise your hand if you
have an app published on the App Store.

00:01:48.859 --> 00:01:50.629
I'm curious to see.

00:01:50.629 --> 00:01:53.409
Wow, okay, so that's very cool.

00:01:53.409 --> 00:01:56.590
All right, and you know games have been huge.

00:01:56.590 --> 00:02:02.430
The gaming category on the App Store
is the largest category on that store.

00:02:02.430 --> 00:02:07.600
And you guys deserve a huge round of applause for
what you've accomplished in the previous year.

00:02:07.599 --> 00:02:09.009
So thank you guys.

00:02:09.009 --> 00:02:13.780
[ Applause ]

00:02:13.780 --> 00:02:16.890
>> So this year of course we're talking about iPhone OS 3.0,

00:02:16.889 --> 00:02:19.699
and the technologies that have been
added there for game development.

00:02:19.699 --> 00:02:27.329
On the left, the game kit API, giving you the
ability to have network gaming over Bluetooth,

00:02:27.330 --> 00:02:31.200
and head-to-head communication
between two different instances

00:02:31.199 --> 00:02:35.649
of your application on a device iPod library access.

00:02:35.650 --> 00:02:42.530
So letting you take the user's iPod library, and the play
list, and the songs that they have synced onto their device,

00:02:42.530 --> 00:02:46.729
and have those play as the background
sound track within your game.

00:02:46.729 --> 00:02:48.869
OpenGL ES 2.0.

00:02:48.870 --> 00:02:55.800
So now for game developers, the shader pipeline
that you need to develop rich and powerful graphics

00:02:55.800 --> 00:03:00.900
on the new iPhone 3G S is enabled for you through that API.

00:03:00.900 --> 00:03:05.680
And something that's very interesting, and I'm
sure a lot of game developers have been thinking

00:03:05.680 --> 00:03:13.980
about is how they can take in that purchase and use that to
unlock levels in their game or download additional content.

00:03:13.979 --> 00:03:21.289
And needless to say also, the iPhone 3G S, being able
to take this new platform with higher performance

00:03:21.289 --> 00:03:27.479
and incredible graphics compatibilities
and load and use that for game play.

00:03:27.479 --> 00:03:32.799
So to show off some of these capabilities and
just the things that we've been talking about,

00:03:32.800 --> 00:03:37.930
I want to bring back up on stage Graeme Devine, who you saw
yesterday during the Graphics and Media State of the Union.

00:03:37.930 --> 00:03:41.060
He's going to tell you a little bit more about shock.

00:03:41.060 --> 00:03:43.349
Graeme, take it away.

00:03:43.349 --> 00:03:44.229
>> Good morning.

00:03:44.229 --> 00:03:46.569
Yesterday we showed you a little bit about shock.

00:03:46.569 --> 00:03:51.449
Today I'm going tell you a little
bit about how we actually made it.

00:03:51.449 --> 00:03:55.089
So five days ago we were asked to make a demo for WWDC.

00:03:55.090 --> 00:03:59.370
And you know, that's not much time to make a demo.

00:03:59.370 --> 00:04:02.620
But we were given a bunch of engineers, and a phone.

00:04:02.620 --> 00:04:04.469
A lot of people hadn't seen the phone before.

00:04:04.469 --> 00:04:07.250
It's the very first time they've seen the iPhone 3G S.

00:04:07.250 --> 00:04:13.490
And the one thing that struck us was
it has a CPU, which is quite powerful,

00:04:13.490 --> 00:04:15.840
but it has a GPU, which is extremely powerful.

00:04:15.840 --> 00:04:22.649
So what would be interesting to us was to write a
sandbox application, something that was really simple,

00:04:22.649 --> 00:04:29.509
and yet could show off the power of the GPU which is
where we wanted to spend the majority of our time working.

00:04:29.509 --> 00:04:36.569
So we came up with the idea of this highly scoped, this
really simple game that was just really a sandbox game.

00:04:36.569 --> 00:04:39.399
And we had this game up and running in one day.

00:04:39.399 --> 00:04:41.120
Or maybe even on the first hour.

00:04:41.120 --> 00:04:44.300
And it was really easy to get up and running.

00:04:44.300 --> 00:04:47.960
And then we started to add shaders to the
background, and shaders to the demonstration.

00:04:47.959 --> 00:04:52.319
So the first stage was really very, very simple.

00:04:52.319 --> 00:04:54.420
It was three lines of GLSL code.

00:04:54.420 --> 00:04:56.890
And a couple things struck me about that.

00:04:56.889 --> 00:04:59.949
Last time I wrote a shader was in assembly code.

00:04:59.949 --> 00:05:03.099
And this is all in C code.

00:05:03.100 --> 00:05:06.740
And it's way easier than it ever
was before to write a shader.

00:05:06.740 --> 00:05:12.449
This is a fragment shader up and running in
just a few lines of code, doing stuff per pixel.

00:05:12.449 --> 00:05:21.209
So the smoke simulation that you see running is actually six
or seven shaders, and it took us about three days to write.

00:05:21.209 --> 00:05:26.239
It inserts density and velocity with the ball, and
you can see that the bat; see if I move the bat up

00:05:26.240 --> 00:05:33.160
and down (you know, I hate to lose the game) as
we effect the smoke simulation in the background.

00:05:33.160 --> 00:05:38.860
But the key thing is the simulation
is completely solved on the GPU.

00:05:38.860 --> 00:05:41.490
The CPU isn't doing any of the work.

00:05:41.490 --> 00:05:50.090
Leaving your CPU free to do things like
AI, networking, voice chat, or play pong.

00:05:51.160 --> 00:05:55.820
One thing that struck us too, was once
we had the simple game up and running

00:05:55.819 --> 00:05:59.199
and the shader running, we could change the pallets.

00:05:59.199 --> 00:06:01.439
Lots of ways to be able to change pallets.

00:06:01.439 --> 00:06:06.509
One of the easiest ways is to be able to
just insert a few more lines of GLSL code.

00:06:06.509 --> 00:06:11.969
But because we're working on a team
with artists, we actually made a texture

00:06:11.970 --> 00:06:18.310
that would let the artist adjust the pallet for
us, and that generally gets much better results,

00:06:18.310 --> 00:06:22.850
because you get things like this fire,
actually, then, that looks pretty darn cool.

00:06:22.850 --> 00:06:29.120
So if you're listening to the sound,
we added sound into the game as well.

00:06:29.120 --> 00:06:36.579
And you can hear if we stop for a second-or if I stop
for a second-that the sound is actually working 3D,

00:06:36.579 --> 00:06:38.819
and bouncing from one side of the room to the other.

00:06:38.819 --> 00:06:43.939
I think this is the loudest game of tennis ever.

00:06:43.939 --> 00:06:47.819
The sound was actually written by an
Apple engineer in the CoreOS group.

00:06:47.819 --> 00:06:53.000
We've a great team working on this project.

00:06:53.000 --> 00:06:58.850
Then lastly, we made one more shader.

00:06:58.850 --> 00:07:01.350
Which is a bump map shader.

00:07:01.350 --> 00:07:03.900
This is nine lines of GLSL.

00:07:03.899 --> 00:07:10.279
And you might think, well, I know how to write nine
lines of code that just casts a light around a ball.

00:07:10.279 --> 00:07:17.759
But actually it's taking a normal map and it's attenuating
the light based upon the normal, in the normal map,

00:07:17.759 --> 00:07:26.469
and doing the correct falloff for that light, and working
out how the actual light would work against that normal map,

00:07:26.470 --> 00:07:30.200
and presenting that-so you can see around
the ball here, that it's actually falling off

00:07:30.199 --> 00:07:33.399
around the actual textures in the background.

00:07:33.399 --> 00:07:36.159
And all of that is nine lines of GLSL.

00:07:36.160 --> 00:07:42.730
Now I don't know about you, but last time I wrote a
bump map shader, it was a few more lines than nine.

00:07:42.730 --> 00:07:45.430
The power of the GPU is absolutely incredible,

00:07:45.430 --> 00:07:49.709
and we were able to make this application
in a very short amount of time.

00:07:49.709 --> 00:07:57.549
Using -- it was really a good little crash course in a
small disciplined team showing off the power of the GPU.

00:07:57.550 --> 00:08:00.810
But it was a lot of fun to work on, and
we're still using this today at Apple

00:08:00.810 --> 00:08:02.399
to be able to play around with more shaders.

00:08:02.399 --> 00:08:08.139
And so we wanted to just give you a quick little
background on some of the work that went into making this.

00:08:08.139 --> 00:08:09.529
Thank you very much.

00:08:09.529 --> 00:08:10.309
>> So thank you, Graeme.

00:08:10.310 --> 00:08:16.629
And it's a little cruel of me, I think, to make you have
to play the game while you were talking about it as well.

00:08:16.629 --> 00:08:20.060
That had to have been kind of tough.

00:08:20.060 --> 00:08:23.810
So for folks -- today, this is a two-part session.

00:08:23.810 --> 00:08:30.189
Here in part one I'm going to spend some time talking about
graphics and audio, and then after lunch we'll come back

00:08:30.189 --> 00:08:34.009
to talk about game kit, the accelerometer, and store kit.

00:08:34.009 --> 00:08:39.429
So let's just dive straight in with graphics,
and we're going to focus on OpenGL ES.

00:08:39.429 --> 00:08:45.259
So OpenGL ES is the high performance
3D graphics API that you use for many

00:08:45.259 --> 00:08:47.830
of the games that you see on the App Store today.

00:08:47.830 --> 00:08:50.700
It's designed for hardware accelerated 3D rendering.

00:08:50.700 --> 00:08:56.070
And OpenGL ES is the mobile variant of
the OpenGL standard from the desktop.

00:08:56.070 --> 00:09:00.700
It's defined by an industry consortium called
the Khronos Group, of which Apple is a member.

00:09:00.700 --> 00:09:10.040
And really, OpenGL ES represents a distillation of the API
that you find on the desktop, brought down, -simplified,

00:09:10.039 --> 00:09:15.079
and made more efficient and appropriate
for the capabilities of mobile devices,

00:09:15.080 --> 00:09:17.560
which is as you can see are very, very capable.

00:09:17.559 --> 00:09:27.039
And really, what OpenGL ES is, is the front leading edge
of the capabilities that you find on OpenGL on the desktop.

00:09:27.039 --> 00:09:33.449
There's a lot of the older functionality that have
been built up over the years in the desktop API

00:09:33.450 --> 00:09:36.629
that simply didn't need to be brought
over on to a mobile device.

00:09:36.629 --> 00:09:41.309
There's more efficient ways of submitting
geometry now and doing other thingso only

00:09:41.309 --> 00:09:46.089
that functionality has been brought into OpenGL ES.

00:09:46.090 --> 00:09:52.300
So there's a subtle point that we need
to make regarding OpenGL ES versions

00:09:52.299 --> 00:09:55.469
that are now supported on iPhone OS devices.

00:09:55.470 --> 00:10:05.190
We have some devices that support OpenGL ES version 1.1,
and other devices such as the iPhone 3G S that support both,

00:10:05.190 --> 00:10:11.300
OpenGL ES 1.1, and 2.0, which is
a more capable graphics pipeline.

00:10:11.299 --> 00:10:17.419
So a lot of times when you hear us talking about OpenGL,
you're hear us discussing this graphics pipeline.

00:10:17.419 --> 00:10:20.159
The parts that are in purple up here are intended

00:10:20.159 --> 00:10:24.120
to be essentially the elements that
are under direct control by you.

00:10:24.120 --> 00:10:31.539
So the OpenGL ES API, the definition of your 3D geometry
and textures, and the creation of frame buffer surface.

00:10:31.539 --> 00:10:36.189
And the parts in blue are intended to
represent the elements of the pipeline

00:10:36.190 --> 00:10:39.230
that are implemented in the GPU hardware itself.

00:10:39.230 --> 00:10:43.200
And so ES 1.1 implements a fixed function pipeline.

00:10:43.200 --> 00:10:48.330
And so that means that each one of those blue
stages is essentially burned into the silicon.

00:10:48.330 --> 00:10:53.879
And there are elements that you can
manipulate for the different stages.

00:10:53.879 --> 00:11:01.990
For example, you can change texture modes, or adjust,
enable stenciling, change the blending modes, and so on,

00:11:01.990 --> 00:11:04.980
enable lighting up on the vertex processing stage.

00:11:04.980 --> 00:11:10.060
But fundamentally, that API is fixed,
and its functionality is fixed.

00:11:10.059 --> 00:11:18.209
So the ES 1.1 API is targeted to
hardware that supports that pipeline.

00:11:18.210 --> 00:11:21.680
Now all of the rendering that is
done in OpenGL ES kind of --

00:11:21.679 --> 00:11:27.739
or all of the APIs calls kind of boil down into these
three categories where you're either creating objects,

00:11:27.740 --> 00:11:33.899
things like textures or buffers to render into,
you're setting graphics state, that right-hand column,

00:11:33.899 --> 00:11:40.299
for changing basically knobs and dials along the
graphics pipeline to control how the next element

00:11:40.299 --> 00:11:42.709
to come through the pipeline will be rendered.

00:11:42.710 --> 00:11:45.120
And then you're submitting 3D geometry.

00:11:45.120 --> 00:11:49.000
You know, things like your vertices,
normals, colors, and texture coordinates.

00:11:49.000 --> 00:11:57.110
So now let's draw the contrast between that and the
OpenGL ES 2.0 pipeline that you find on the iPhone 3G S.

00:11:57.110 --> 00:11:59.080
So this is a programmable pipeline.

00:11:59.080 --> 00:12:04.020
And many of the stages in the hardware
are now programmable by you.

00:12:04.019 --> 00:12:10.240
You write shader, so you write these little
C-like programs using the GL shading language.

00:12:10.240 --> 00:12:13.930
That is compiled by the system and
loaded directly into the hardware.

00:12:13.929 --> 00:12:21.299
And then the geometry and the different graphics, 3D
data, that is submitted to that hardware then runs

00:12:21.299 --> 00:12:26.490
through the pipeline, but using the program that
you wrote to actually handle the processing.

00:12:26.490 --> 00:12:31.090
So the objects now, you have another
type of object, which are shaders.

00:12:31.090 --> 00:12:35.149
Vertex shaders and fragment shaders that can
be loaded into different parts of the pipeline.

00:12:35.149 --> 00:12:40.829
A lot of things about setting graphics state
now actually just becomes about writing

00:12:40.830 --> 00:12:44.050
that shader and loading it into the GPU itself.

00:12:44.049 --> 00:12:51.099
And of course there's a new API now or a different API
for submitting 3D geometry using uniforms and attributes,

00:12:51.100 --> 00:12:57.090
and then sampler is kind of a search case
for texture data coming into the vertex unit.

00:12:57.090 --> 00:13:02.399
So we're actually not going to really
teach you guys OpenGL at this conference,

00:13:02.399 --> 00:13:06.100
and certainly not in the 20 minutes
that I'm spending on it here.

00:13:06.100 --> 00:13:11.899
So to learn the basics of OpenGL the places to
start are the OpenGL ES 2.0 Programming Guide.

00:13:11.899 --> 00:13:14.789
This will go into the shader pipeline quite a bit.

00:13:14.789 --> 00:13:19.019
They call it the Gold Book-it's kind of gold
and purple, and that's what you're looking for.

00:13:19.019 --> 00:13:25.470
If you want to get more information about using the GL
shading language, there's the so-called Orange Book,

00:13:25.470 --> 00:13:29.129
OpenGL Shading Language, now in a second edition.

00:13:29.129 --> 00:13:35.539
Apple has published a lot of information about
our implementation of OpenGL on the iPhone,

00:13:35.539 --> 00:13:39.370
so that's The OpenGL Es Programming Guide for iPhone.

00:13:39.370 --> 00:13:47.009
Then for those of you who are maybe thinking about also
using OpenGL on the desktop, this is a good place to start.

00:13:47.009 --> 00:13:52.649
The so-called Red Book is the OpenGL programming guide
really for a lot of us who have been using OpenGL

00:13:52.649 --> 00:13:55.409
for a long time-this is probably where we got started.

00:13:55.409 --> 00:14:00.079
And then there's a tremendous amount
of resources up on the web.

00:14:00.080 --> 00:14:02.930
And so OpenGL.org is a great place to go.

00:14:02.929 --> 00:14:04.620
Now I wanted to pay special attention.

00:14:04.620 --> 00:14:10.679
This is where you can find the spec. And I would say that
those books I showed you can kind of take a beginning

00:14:10.679 --> 00:14:16.179
up to being, you know, an intermediate
to slightly advanced user.

00:14:16.179 --> 00:14:22.849
But to really become an advanced user of OpenGL you have
to read the spec, and really learn about how the internals

00:14:22.850 --> 00:14:27.779
of that API really work, rather than
just how to use the API from above.

00:14:27.779 --> 00:14:29.659
So check that out.

00:14:29.659 --> 00:14:36.269
Now those of you who are knowledgeable about
OpenGL ES already and just want to be --

00:14:36.269 --> 00:14:39.909
maybe you're not really this familiar
with iPhone programming and you want

00:14:39.909 --> 00:14:45.699
to know how can I drop some OpenGL ES code onto
the iPhone and kind of get it to work with all

00:14:45.700 --> 00:14:50.759
of these other frameworks, like
Core Animation and UIKit, and so on.

00:14:50.759 --> 00:14:58.029
We provide a template in Xcode that lets you just
drop some code into the DrawView routine there,

00:14:58.029 --> 00:15:01.679
and you can really get up and running
really, really quickly.

00:15:01.679 --> 00:15:04.609
So check that out.

00:15:04.610 --> 00:15:06.259
But all right, so for this.

00:15:06.259 --> 00:15:13.389
For today where I want to actually spend the most time is
to talk about our implementation of OpenGL ES on the iPhone.

00:15:13.389 --> 00:15:17.509
And the first place to start is to
remind you about these platforms.

00:15:17.509 --> 00:15:24.939
So here we have a list of the different devices on the left,
and the different variants of API in the other two columns.

00:15:24.940 --> 00:15:29.140
Notice that OpenGL ES 1.1 is supported
across the product line.

00:15:29.139 --> 00:15:36.740
And so this is how applications that are in the App Store
today all written using OpenGL ES are going to be able

00:15:36.740 --> 00:15:42.570
to run on the iPhone 3G S, and of
course are already running there.

00:15:42.570 --> 00:15:47.080
Now for the iPhone 3G S we also support OpenGL ES 2.0.

00:15:47.080 --> 00:15:54.900
And so that's the API that's going to enable the shader
pipeline to be used- that's available on that hardware.

00:15:54.899 --> 00:16:01.139
So it's important to notice, also, that that
means that the iPhone 3G S really has kind

00:16:01.139 --> 00:16:05.419
of two different paths that it can take for applications.

00:16:05.419 --> 00:16:09.059
Both versions of OpenGL can co-exist on the same device,

00:16:09.059 --> 00:16:14.629
and we'll be showing you a few best
practices about that in just a minute.

00:16:14.629 --> 00:16:21.210
So you need to now decide which version of
OpenGL are you going to use for your application,

00:16:21.210 --> 00:16:26.430
and the decision should fundamentally come from
the requirements of what you need to be rendering.

00:16:26.429 --> 00:16:31.979
So for a lot of the very hardcore game developers,
they've been asking us, we need that shader pipeline

00:16:31.980 --> 00:16:35.379
and we need a device that can implement that in hardware.

00:16:35.379 --> 00:16:40.740
So for those folks, obviously, it
would be a choice of OpenGL ES 2.0.

00:16:40.740 --> 00:16:47.250
But also, well, you may also want to have a product
with the widest reach, and that can be addressed,

00:16:47.250 --> 00:16:50.610
can load on all of the products in the product line.

00:16:50.610 --> 00:16:55.779
So for that choice you would choose OpenGL ES 1.1.

00:16:55.779 --> 00:17:00.000
And so how you reconcile these two things.

00:17:00.000 --> 00:17:01.389
Well, this is the way.

00:17:01.389 --> 00:17:09.369
So what you should probably do is if your application
needs the shader pipeline of OpenGL ES 2.0,

00:17:09.369 --> 00:17:16.049
you are capable of implementing it to use that pipeline,
but then also implement code in your application

00:17:16.049 --> 00:17:22.589
to provide an ES 1.1 fall-back
path for all of the other devices.

00:17:22.589 --> 00:17:23.049
All right?

00:17:23.049 --> 00:17:30.940
So that's something we would really recommend that people
look into, to get the best reach between platform capability

00:17:30.940 --> 00:17:34.840
on one hand, and capability of the hardware on the other.

00:17:34.839 --> 00:17:44.079
So to do this in code is -- this is
configured by the EAGL context object.

00:17:44.079 --> 00:17:50.899
And so when you initialize the EAGL content you
actually define which API you want to be using.

00:17:50.900 --> 00:17:54.720
You make a choice between ES 2 or ES 1.

00:17:54.720 --> 00:17:59.569
And it's important to note you can make
both of these calls in one application.

00:17:59.569 --> 00:18:06.700
If you request the ES 2 context on, say, an iPod touch,
you don't have hardware there that would be capable

00:18:06.700 --> 00:18:09.090
of supporting that, so that call will fail.

00:18:09.089 --> 00:18:10.939
But then, it will return nil.

00:18:10.940 --> 00:18:16.490
But you can then go and try to initialize
the API with ES 1, and that will succeed.

00:18:16.490 --> 00:18:24.049
Now I want to switch gears a little bit and talk about
some of the extensions to OpenGL that are of interest

00:18:24.049 --> 00:18:27.879
on this platform, and maybe just to back up a little bit.

00:18:27.880 --> 00:18:33.530
So every version of OpenGL defines some
core functionality for that version.

00:18:33.529 --> 00:18:38.240
And then in addition to that, there are a
number of extensions specific to that version

00:18:38.240 --> 00:18:44.009
which have been either added by the vendors
themselves, some have been added by Apple,

00:18:44.009 --> 00:18:49.450
some have been sort of standardized
by the Khronos Group and so on.

00:18:49.450 --> 00:18:56.190
And there's core functionality and then there's this
sort of cloud of extensions around each version.

00:18:56.190 --> 00:19:04.250
And as the versions progress, the trend is for a lot of
the more popular extensions sometimes to make their way

00:19:04.250 --> 00:19:07.579
into the core of the next version of OpenGL.

00:19:07.579 --> 00:19:12.210
And so that's something that indeed happened
with the frame buffer object extension.

00:19:12.210 --> 00:19:16.529
So on the iPhone, we use the frame
buffer object extension extensively.

00:19:16.529 --> 00:19:23.180
It is the destination for all of the
rendering that's done in OpenGL on the iPhone.

00:19:23.180 --> 00:19:31.730
And so every time that you set up your surfaces in
OpenGL ES you're creating a frame buffer object.

00:19:31.730 --> 00:19:34.440
And then all of the rendering will end up there.

00:19:34.440 --> 00:19:39.279
So this was an extension in OpenGL
ES 1.1 in our implementation there,

00:19:39.279 --> 00:19:43.970
and it's now become a core feature of OpenGL ES 2.0.

00:19:43.970 --> 00:19:48.140
The second extension here is PVRTC.

00:19:48.140 --> 00:19:51.310
So this is Powered VR Texture Compression.

00:19:51.309 --> 00:19:58.319
And it's because it's important to be using, because
memory is such a precious commodity on mobile hardware.

00:19:58.319 --> 00:20:04.279
So this is a way that lets you take a
compressed representation of texture images

00:20:04.279 --> 00:20:09.430
and load those compressed representations into
memory and keep them compressed when they're there.

00:20:09.430 --> 00:20:15.920
And then the hardware is able to decode that in
realtime as those textures are being referenced.

00:20:15.920 --> 00:20:18.240
And the third one here is a really interesting one.

00:20:18.240 --> 00:20:23.390
It's an extension that we've added called
the limited non-power-of-2 extension.

00:20:23.390 --> 00:20:32.300
So a lot of times you might want to define a texture that
is something other than a power of two in dimensions, know,

00:20:32.299 --> 00:20:37.039
something where it has different dimensions,

00:20:37.039 --> 00:20:44.519
and in particular one that would be interesting
on the iPhone hardware is 480 by 320.

00:20:44.519 --> 00:20:49.000
Because that corresponds to the size of the display.

00:20:49.000 --> 00:20:56.500
So the support for non-power-of-2
textures is core in OpenGL ES 2.0.

00:20:56.500 --> 00:21:00.460
So in OpenGL ES 2.0, you just get it.

00:21:00.460 --> 00:21:06.799
And the way that the textures are referenced
there is through pixel coordinates.

00:21:06.799 --> 00:21:12.289
It's through pixel coordinates you
can use bitmapping- there's a lot

00:21:12.289 --> 00:21:16.529
of functionality that's associated
with that core feature set.

00:21:16.529 --> 00:21:25.099
We decided that since the hardware on the iPhone 3G S can
actually support these non-power-of-2 dimension textures,

00:21:25.099 --> 00:21:33.049
that we would port that functionality back to the
OpenGL ES 1.1 implementation on the iPhone 3G S.

00:21:33.049 --> 00:21:37.509
And so it's a limited availability
extension, and it's available there.

00:21:37.509 --> 00:21:40.440
And so it's called the texture 2D limited npot-.

00:21:40.440 --> 00:21:45.559
So now let me spend a little more time on PVRTC,

00:21:45.559 --> 00:21:49.389
because this is something we really
want to drive home with developers.

00:21:49.390 --> 00:21:55.810
This allows you to take an original
image, say a 32-bit PNG or2 TIF image,

00:21:55.809 --> 00:22:01.089
and compress its depth down to 2 or 4 bits per pixel.

00:22:01.089 --> 00:22:06.449
There's various modes that you can set as you're doing
the compressions, sort of one for linear weighting

00:22:06.450 --> 00:22:11.259
for how the compression is down, and one
more tuned for the human visual perception.

00:22:11.259 --> 00:22:16.379
But the main benefit of doing this is the memory savings.

00:22:16.380 --> 00:22:22.700
Taking an image from 32 bits down to 4, I mean,
that's an incredible amount of memory savings.

00:22:22.700 --> 00:22:28.710
And usually a game using OpenGL ES
is going to have a lot of textures.

00:22:28.710 --> 00:22:32.880
So there's also a performance benefit from doing so.

00:22:32.880 --> 00:22:35.240
Reducing the amount of memory that has to be moved

00:22:35.240 --> 00:22:42.500
around as objects are being rendered can reduce the
memory bandwidth pressure as you're drawing your scene,

00:22:42.500 --> 00:22:46.140
and give you performance enhancement because of that.

00:22:46.140 --> 00:22:51.600
So we provide a tool with the SDK,
and this is the path to it here.

00:22:51.599 --> 00:22:55.529
Now I wanted to do a quick comparison
of different PVRTC results.

00:22:55.529 --> 00:23:00.639
So the image on the left here is
the uncompressed original PNG image.

00:23:00.640 --> 00:23:04.509
And this is an explosion sequence that
we took from the Touch Fighter game

00:23:04.509 --> 00:23:07.670
that we showed everybody at last year's WWDC.

00:23:07.670 --> 00:23:15.360
You see, it's a texture atlas defining sort
of an animation sequence for that explosion.

00:23:15.359 --> 00:23:18.019
Now on the left, original 32-bit image.

00:23:18.019 --> 00:23:22.410
In the middle there is the compressed
down to 4 bits per pixel using PVRTC.

00:23:22.410 --> 00:23:27.910
I know that we're projecting this, and
it might be a little hard to detect.

00:23:27.910 --> 00:23:30.769
There are only some slight differences in that image.

00:23:30.769 --> 00:23:36.500
Now if you go down to 2 bits per pixel, in that
one you can start to notice some artifacting,

00:23:36.500 --> 00:23:41.509
and it depends on how detailed
you need your imagery to remain.

00:23:41.509 --> 00:23:47.769
So PVRTC is great for sort of natural,
real-world looking textures.

00:23:47.769 --> 00:23:52.869
It's less well suited for things like
line art or vector type graphics.

00:23:52.869 --> 00:23:55.459
But for this sort of thing it's really great.

00:23:55.460 --> 00:24:00.259
And of course the memory savings, 8X
memory savings, 16X memory savings.

00:24:00.259 --> 00:24:09.099
So just a couple of best practices to bear in mind as
we're talking about textures on the iPhone OS hardware.

00:24:09.099 --> 00:24:11.269
So I'll drive this home again.

00:24:11.269 --> 00:24:13.460
Minimize your texture storage requirements.

00:24:13.460 --> 00:24:17.720
You always want to be treating memory
on the device as a precious commodity.

00:24:17.720 --> 00:24:23.350
And I saw how many of you raised your hands about loading
your apps, getting apps published on the App Store.

00:24:23.349 --> 00:24:26.199
You know how precious that memory is.

00:24:26.200 --> 00:24:33.559
So the thing to do is, you know, the
first stop is to try and use PVRTC.

00:24:33.559 --> 00:24:37.359
Probably try the 4 bit depth for that.

00:24:37.359 --> 00:24:43.659
And sort of a pro tip that some people will do, you
know, we'll hear from developers, well, you know,

00:24:43.660 --> 00:24:49.540
the compression quality of PVRTC isn't quite what
I want, so I'm sticking with my original image.

00:24:49.539 --> 00:24:56.700
What you could actually do is sample from a larger
original image than what you would be using in your game.

00:24:56.700 --> 00:25:03.370
So say you had a 512 by 512 32-bit depth image.

00:25:03.369 --> 00:25:06.019
Well, what would happen if you increased the --

00:25:06.019 --> 00:25:11.539
if you had art work that originally
existed at 1 K by 1 K, 32-bit depth.

00:25:11.539 --> 00:25:15.309
You could depress that into a 4 built PVRTC.

00:25:15.309 --> 00:25:23.740
And if you work out the math it will actually save you
half of the memory compared to that 512 x 512 32-bit image.

00:25:23.740 --> 00:25:27.200
It's a great way -- and the quality is very comparable.

00:25:27.200 --> 00:25:29.100
So it's a great way to use PVRTC.

00:25:29.099 --> 00:25:35.490
The second thing, obviously, if PVRTC isn't going
to work for you then maybe you want to switch

00:25:35.490 --> 00:25:40.009
over to use a 16-bit depth, just raw
depth on your images instead of 32.

00:25:40.009 --> 00:25:48.869
And we provide a number of different formats for
you to use, 4444, 5551 Alpha, and 565 with no Alpha.

00:25:48.869 --> 00:25:56.869
And just a reminder, OpenGL ES makes a copy
of your texture data when you define it.

00:25:56.869 --> 00:25:59.009
It keeps its own separate copy.

00:25:59.009 --> 00:26:06.109
So you can deallocate a copy that you've perhaps loaded
from a file if you're not going to be needing it any more.

00:26:06.109 --> 00:26:12.259
And then one more thing is we -- OpenGL programmers
are always talking minimizing stay changes.

00:26:12.259 --> 00:26:16.240
And so that previous example, where I
had all the different explosions burned

00:26:16.240 --> 00:26:19.539
into one texture image, that was a texture atlas.

00:26:19.539 --> 00:26:23.539
And you're able to achieve some
efficiency games because of that.

00:26:23.539 --> 00:26:29.569
Essentially, you can load that one texture and
reference all 16 of the explosions within it,

00:26:29.569 --> 00:26:37.309
rather than having to load 16 different textures and
switch between them with state changes at runtime.

00:26:37.309 --> 00:26:47.059
All right, so I've talked about all of the platforms, and
there's actually one more platform that I want to address.

00:26:47.059 --> 00:26:48.869
So let's go back to our chart.

00:26:48.869 --> 00:26:54.839
We have the iPhone 3G S, 3G, iPhone,
iPod touch 2nd Gen, and the iPod touch.

00:26:54.839 --> 00:26:59.429
And there is one more that game
developers really need to spend a lot

00:26:59.430 --> 00:27:05.269
of time focusing on, and it's the Simulator, okay?

00:27:05.269 --> 00:27:16.109
So the Simulator supports now in the 3.0 SDK supports both
an ES 1.1 implementation and an ES 2.0 implementation.

00:27:16.109 --> 00:27:19.109
Which -- that is fantastic.

00:27:19.109 --> 00:27:25.959
For developers to be able to have all -- either one of those
choices available as they do their programming on their Mac

00:27:25.960 --> 00:27:30.150
with the Simulator is going to
really speed up your workflow.

00:27:30.150 --> 00:27:31.850
And just think about the other tools.

00:27:31.849 --> 00:27:38.029
If you're familiar with working with shaders, you know,
you can use things like Quartz Composer to make live edits

00:27:38.029 --> 00:27:42.980
on a GLSL shader on the Mac, and
then very easily take that shader

00:27:42.980 --> 00:27:47.549
with minor modifications and bring it over into OpenGL ES.

00:27:47.549 --> 00:27:52.399
So it's a great environment for you to
work in, to be able to use the Simulator.

00:27:52.400 --> 00:27:55.910
Now there's some fine print, of course.

00:27:55.910 --> 00:28:03.210
So when you're using the Simulator for game development you
need to always keep in mind that the functionality is there.

00:28:03.210 --> 00:28:08.750
The ES 2.0 implementation and the
ES 1.1 implementation are there.

00:28:08.750 --> 00:28:13.529
But the performance is going to be very, very
different from what you see on the device.

00:28:13.529 --> 00:28:17.589
And it's because the performance is depending
on what kind of Mac you're running on

00:28:17.589 --> 00:28:21.199
and a number of different things like that.

00:28:21.200 --> 00:28:26.160
So use the Simulator basically just
for your coding and development time.

00:28:26.160 --> 00:28:33.190
It's to give you faster turn around on minor changes in your
builds, but not so much where you would necessarily look

00:28:33.190 --> 00:28:38.420
at the Simulator to figure out if you're
doing the right thing for performance.

00:28:38.420 --> 00:28:42.330
When you're testing for performance, and
you should be always testing performance

00:28:42.329 --> 00:28:45.250
as well, you need to be doing that on a device.

00:28:45.250 --> 00:28:49.640
So be sure to be hopping back and
forth between the two of them.

00:28:49.640 --> 00:28:52.490
Now a couple of fine print about the Simulator.

00:28:52.490 --> 00:28:56.940
Number one, it doesn't enforce all of
the same OpenGL ES error generation

00:28:56.940 --> 00:29:00.620
that you might actually find when
you're on the hardware itself.

00:29:00.619 --> 00:29:05.919
There's limits in the hardware that are
not necessarily enforced on the Simulator.

00:29:05.920 --> 00:29:07.380
Let me give you an example.

00:29:07.380 --> 00:29:16.590
On the original iPhone, iPod touch, iPhone 3G, and iPod
touch second generation you have a 24 megabyte limit

00:29:16.589 --> 00:29:22.359
for the textures and surfaces that you can load into OpenGL.

00:29:22.359 --> 00:29:26.179
Now that limit no longer exists on the iPhone 3G S.

00:29:26.180 --> 00:29:31.410
There's no architectural limitation, you just are
limited by however much memory you have available.

00:29:31.410 --> 00:29:36.370
So the Simulator, though, does not enforce that
limit no matter how you're running the application.

00:29:36.369 --> 00:29:39.239
It doesn't know which hardware you would be running that on.

00:29:39.240 --> 00:29:42.210
And so just bear that in mind.

00:29:42.210 --> 00:29:44.380
Likewise, it's a different architecture.

00:29:44.380 --> 00:29:52.860
You know, on the power on all of the iPhone devices
use a tile-based deferred renderer within the GPU.

00:29:52.859 --> 00:29:55.049
And that's the architecture of the GPU.

00:29:55.049 --> 00:30:00.529
And it's just-it would be a different
representation in the Simulator.

00:30:00.529 --> 00:30:07.519
So use the Simulator to test for code correctness in
your code paths, use the device to test for performance.

00:30:07.519 --> 00:30:13.629
But all right, so folks, that's going
to end now the part of the section

00:30:13.630 --> 00:30:17.680
on graphics, and I would like to move on to audio.

00:30:17.680 --> 00:30:25.970
So we're going to start with audio
just to -- sort of a high level view.

00:30:25.970 --> 00:30:32.490
The Audio Toolbox framework that's available on the
iPhone provides a broad set of APIs for audio playback

00:30:32.490 --> 00:30:37.370
and audio recording for a variety
of different application types.

00:30:37.369 --> 00:30:45.929
But for today, I want to focus on specifically the APIs
that you might use for games, and I have a specific scenario

00:30:45.930 --> 00:30:54.789
in mind that you might be using the OpenAL API, which
is a 3D spatial audio API to play your game sounds,

00:30:54.789 --> 00:31:00.259
and then you might be taking music out of the
IpodLibrary to play the background sound track.

00:31:00.259 --> 00:31:06.339
And so I'd like to talk to you about how you
would accomplish that thing together today.

00:31:06.339 --> 00:31:10.250
So let's start with OpenAL.

00:31:10.250 --> 00:31:17.779
So OpenAL is an API for 3D audio mixing, and we
see it in use in a lot of the sound effects in --

00:31:17.779 --> 00:31:22.099
we see it in use in a lot of the applications
in games that are up on the App Store today,

00:31:22.099 --> 00:31:25.559
just like we see OpenGL in such heavy use as well.

00:31:25.559 --> 00:31:33.000
And OpenGL is modeling audio in a 3D space as its being
heard by a single listener who is moving around in

00:31:33.000 --> 00:31:38.819
that 3D space, and may be changing his location,
changing his orientation to hear things,

00:31:38.819 --> 00:31:42.909
and hear things in different ears
as he moves around, and so on.

00:31:42.910 --> 00:31:51.420
Now OpenAL mimics a lot of the conventions that you
may be familiar with from the OpenGL side of the world.

00:31:51.420 --> 00:31:56.810
So things like the coronet space can be made
in common between these two different APIs.

00:31:56.809 --> 00:32:02.419
A lot of the actual programming style is similar
as well, where you're setting up a context,

00:32:02.420 --> 00:32:08.509
and then all of the things that you're doing with the
API after that point are relative to that context.

00:32:08.509 --> 00:32:15.730
Now on the iPhone, our implementation is OpenAL
1.1 without the audio capture functionality.

00:32:15.730 --> 00:32:17.799
So it's simply a playback API.

00:32:17.799 --> 00:32:22.169
And our implementation there is built
on top of the remote I/O audio unit.

00:32:22.170 --> 00:32:27.850
So that provides you with very low
latency playback of your sounds in OpenAL.

00:32:27.849 --> 00:32:34.579
And OpenAL is an open standard, you can actually find
out a lot more about it on the OpenAL.org web site.

00:32:34.579 --> 00:32:40.689
But now I'm going to go into a bit more detail and
walk through some of the fundamentals of OpenAL.

00:32:40.690 --> 00:32:44.990
So there's three fundamental concepts with OpenAL.

00:32:44.990 --> 00:32:54.569
You have sources, and sources are a 3D point in space, or
excuse me, a point in the 3D space, that is emitting audio.

00:32:54.569 --> 00:33:02.179
So in a game perhaps that would be an opponent
or a game piece, or some character crawling

00:33:02.180 --> 00:33:08.130
across your tower defense environment,
emitting audio as it goes around.

00:33:08.130 --> 00:33:12.370
You have buffers, which are the
data containers for the audio.

00:33:12.369 --> 00:33:17.000
And then the listener is you, corresponding
to, for example, if you're using OpenGL,

00:33:17.000 --> 00:33:20.720
corresponding to the I point that you set in OpenGL.

00:33:20.720 --> 00:33:27.600
And it's the position in the 3D space from
where you're hearing those sources being played.

00:33:27.599 --> 00:33:31.230
So I'm going to unwind this now,
go back through these things.

00:33:31.230 --> 00:33:39.099
So in OpenAL the listener defines the context
for the audio playback that's going to occur.

00:33:39.099 --> 00:33:44.500
There's one listener moving around,
being positioned in the virtual space.

00:33:44.500 --> 00:33:54.089
And its position orientation is defined -- excuse me --
the orientation of the listener is defined by two vectors.

00:33:54.089 --> 00:33:59.959
It's a vector of which direction they're facing,
and then also a vector that defines which way is up.

00:33:59.960 --> 00:34:08.340
So that way if you change the up direction to something
else then the ears, the stereo playback might change.

00:34:08.340 --> 00:34:13.200
So here you see a little bit of code that is
how you might set the listener orientation.

00:34:13.199 --> 00:34:23.319
We are defining an up vector here with a positive Z going
up, and, excuse me, -- the looking-at vector is positive Z.

00:34:23.320 --> 00:34:25.780
So this is like we're looking straight out.

00:34:25.780 --> 00:34:30.000
And the up vector is up, with Y up.

00:34:31.099 --> 00:34:35.049
Okay, the next fundamental objects are sources

00:34:35.050 --> 00:34:39.269
And each source is a sound in the game
-- or is a sound source in the game.

00:34:39.269 --> 00:34:45.079
And as I say, maybe these are opponents, or maybe
these are different game elements that are flying

00:34:45.079 --> 00:34:49.389
around in the 3D space, and em=itting some audio.

00:34:49.389 --> 00:34:57.400
And so there's a number of different attributes that you
can actually set to control how sources are rendered.

00:34:57.400 --> 00:35:05.240
You can change their attenuation, to have control
of the fall-off as the source moves away from you.

00:35:05.239 --> 00:35:11.439
You can adjust the pitch of the
playback of that audio from that source.

00:35:11.440 --> 00:35:18.030
You can set up the source to loop the contents of its
buffers, so it just keeps on playing it over and over again.

00:35:18.030 --> 00:35:18.890
All right, and so on.

00:35:18.889 --> 00:35:21.809
So here's again just some sample code.

00:35:21.809 --> 00:35:26.119
The first line we're attaching up
to a buffer; I'll cover that next.

00:35:26.119 --> 00:35:29.460
We're configuring some attributes on the source.

00:35:29.460 --> 00:35:32.340
So here's we're setting is, yes, we want looping.

00:35:32.340 --> 00:35:36.670
We're setting the position of the
source in space to some XYZ value,

00:35:36.670 --> 00:35:41.440
and we're defining a reference distance
for that attenuation fall-off curve.

00:35:41.440 --> 00:35:48.650
And then when we call AL source play, that source is going
to start to be rendered within our 3D sound environ=ment.

00:35:48.650 --> 00:35:53.250
And the final fundamental topic here are buffers.

00:35:53.250 --> 00:35:57.630
And these are containers of the audio data itself.

00:35:57.630 --> 00:36:05.809
And there are two different ways of defining buffers that --
and the two methods of doing this have differences in terms

00:36:05.809 --> 00:36:09.840
of how they implement their memory behavior.

00:36:09.840 --> 00:36:18.210
So AL buffer data and AL buffer data static are the two
different ways that you might load data into a buffer.

00:36:18.210 --> 00:36:24.630
With AL buffer data, OpenAL is going to copy the
data you provide into its own internal buffers.

00:36:24.630 --> 00:36:30.220
And so it will have its own handle
to, you know, that audio data,

00:36:30.219 --> 00:36:35.429
and you're able to release your own copy to conserve memory.

00:36:35.429 --> 00:36:44.629
But better may be to use the OpenAL buffer data static
extension that we've added into our implementation.

00:36:44.630 --> 00:36:53.890
So with this one, OpenAL simply references the
data, the memory that you have provided to it.

00:36:53.889 --> 00:36:59.739
And it's very important, then, for you to realize
that you can't go messing around with that data

00:36:59.739 --> 00:37:03.569
if it's potentially going to still be used by OpenAL.

00:37:03.570 --> 00:37:11.780
So you must not dispose of the data memory while it's in
use, and in particular there are some intricate behaviors

00:37:11.780 --> 00:37:17.140
that you need to be aware of if you are, if
your application is using multiple threads.

00:37:17.139 --> 00:37:24.289
And so we're going be covering some of the best practices
for using the AL buffer data static method routine

00:37:24.289 --> 00:37:34.980
in the audio playback or the audio
playback session at 5 o'clock, later today.

00:37:34.980 --> 00:37:40.349
Let's now move on to just walking
through some code for setting up OpenAL.

00:37:40.349 --> 00:37:46.860
And before I start this I want to make a mention, both this
session and the one after lunch I'm going to be having a lot

00:37:46.860 --> 00:37:49.510
of sequences like this where I walk through code.

00:37:49.510 --> 00:37:51.360
You guys don't have to write all of this down.

00:37:51.360 --> 00:37:57.559
All of the code I'm taking is either from published
examples that are already up on the Dev Center or examples

00:37:57.559 --> 00:38:04.170
that have been published for WWDC, or that we'll have
published within another day or two, there's one of those.

00:38:04.170 --> 00:38:05.789
And so don't worry about it.

00:38:05.789 --> 00:38:11.090
Just watch the sequence here and learn the flow,
and you can go back and read the details of the code

00:38:11.090 --> 00:38:13.559
so you don't have to be typing furiously.

00:38:13.559 --> 00:38:17.309
Okay, so the first step here is
creating the listener object in OpenAL.

00:38:17.309 --> 00:38:21.070
So we open the device and on the iPhone,

00:38:21.070 --> 00:38:27.550
we'll use the default system output as
the output for your OpenAL rendering.

00:38:27.550 --> 00:38:34.200
So this will depend on whether you have, for example,
the head phones plugged in or not, and so on.

00:38:34.199 --> 00:38:41.500
Now Step 2 here is to create an OpenAL context,
meaning the mixer, or implicitly what this means is

00:38:41.500 --> 00:38:45.039
to create the listener that's going
to be used for rendering.

00:38:45.039 --> 00:38:50.469
And so this will be the object that all of
the sounds are rendered sort of relative to.

00:38:50.469 --> 00:38:53.639
And so we create our context here, passing in the device.

00:38:53.639 --> 00:38:55.879
And we make that context current.

00:38:55.880 --> 00:39:01.050
And now this is going be the active object
that all of the sources that we define

00:39:01.050 --> 00:39:04.740
after this will be local to this context.

00:39:04.739 --> 00:39:11.959
I want to note, it's actually as I mentioned before,
it's interesting how this correlates over with OpenGL ES.

00:39:11.960 --> 00:39:18.519
You go through a lot of the same, you know, a lot of
the same API steps, where you're creating a context

00:39:18.519 --> 00:39:24.059
and then making it current to control a lot
of your rendering behavior there as well.

00:39:24.059 --> 00:39:26.820
All right, so going now to the next step.

00:39:26.820 --> 00:39:32.600
We have our listener, the next step is
to create sources and to create a buffer.

00:39:32.599 --> 00:39:41.110
So the first -- up here, we're going to create an OpenAL
buffer object that we're going use to store the audio data.

00:39:41.110 --> 00:39:47.550
And then we fill that buffer using the AL buffer data
static method that I'd shown you to the previous slide.

00:39:47.550 --> 00:39:51.560
Setting the format, sending in
the data, the size, and the rate.

00:39:51.559 --> 00:39:58.739
Now remember, since we're using the buffer data static
extension, OpenAL is referencing your copy of that memory,

00:39:58.739 --> 00:40:04.339
so you cannot delete it if it's
potentially still going to be used.

00:40:04.340 --> 00:40:09.829
Number 6. Okay, now we create a source object, and
we're going to hook that source up to the buffer.

00:40:09.829 --> 00:40:12.840
So now we have all of our pieces in place.

00:40:12.840 --> 00:40:17.430
And we can now go and play whatever that sound was.

00:40:17.429 --> 00:40:22.649
So we set some attributes on the source, here we're
setting the looping position and reference distance

00:40:22.650 --> 00:40:28.110
like I showed you on that previous slide,
and set the source to begin playing.

00:40:28.110 --> 00:40:31.340
And now the listener is going to start hearing that source.

00:40:31.340 --> 00:40:35.240
And then every frame or every time
through our game loop now,

00:40:35.239 --> 00:40:39.439
we can be changing different attributes
on the source and the listener.

00:40:39.440 --> 00:40:45.090
So here we're changing the position of the source, we're
changing the position of the listener in XYZ space,

00:40:45.090 --> 00:40:48.000
and changing the orientation of the listener as well.

00:40:48.000 --> 00:40:57.119
All right, so that was a quick run-through
of the sort of capability that you are --

00:40:57.119 --> 00:41:01.429
through some of the fundamental capabilities
that you'll use if you're using OpenAL.

00:41:01.429 --> 00:41:05.139
And I certainly recommend if you're
developing a game that needs any kind

00:41:05.139 --> 00:41:09.219
of spatialized audio that you'll look into this API.

00:41:09.219 --> 00:41:14.899
So moving on in our scenario now, they have
our sort of game sounds set up through OpenAL.

00:41:14.900 --> 00:41:19.880
And we want to now go and use the
iPod library access to load

00:41:19.880 --> 00:41:24.829
in background music that's going
to be the sound track to our game.

00:41:24.829 --> 00:41:29.730
So iPod library access provides you with,
well, you know, access to the iPod library.

00:41:29.730 --> 00:41:37.289
You're able to instruct the library to playback the
user's music directly from within your application.

00:41:37.289 --> 00:41:45.099
And it also provides a built-in user interface for
choosing songs, or letting them choose songs to play.

00:41:45.099 --> 00:41:53.199
Also, there are programmatic interfaces to this, so you can
programmatically search the library for content or playlist,

00:41:53.199 --> 00:41:59.949
and construct a queue of what you searched -- of those
search results -- and queue those up to be played as well.

00:41:59.949 --> 00:42:04.289
And as I said, the scenario here is that
we're mixing that playback with sort

00:42:04.289 --> 00:42:07.969
of the foreground audio sounds coming from OpenAL.

00:42:07.969 --> 00:42:13.259
So let me just kind of walk you through
the different parts of this API.

00:42:13.260 --> 00:42:17.390
The first is that you have the
iPod library on the device itself.

00:42:17.389 --> 00:42:20.909
And it contains a whole lot of media items.

00:42:20.909 --> 00:42:26.469
There's a separate media item object
for every song or every audio book,

00:42:26.469 --> 00:42:31.609
or every audio Podcast that's synced over to that device.

00:42:31.610 --> 00:42:35.329
So there could be a lot of media items.

00:42:35.329 --> 00:42:40.579
Now media items can also be organized
in the library in collections.

00:42:40.579 --> 00:42:46.480
You might think of collections being all the songs
by one artist, or all the songs on one album,

00:42:46.480 --> 00:42:52.550
or all the songs in a playlist
that have been defined by the user.

00:42:52.550 --> 00:42:56.490
You're able to get at all of that information as well.

00:42:56.489 --> 00:43:04.789
Then pull music out of the library, to select what you're
going to be playing from the library there's two objects.

00:43:04.789 --> 00:43:08.920
Either the Media Picker, which is some built
in user interface to be able to just --

00:43:08.920 --> 00:43:15.909
for the user to just select from albums and artists
and play lists and so on, or as I said, a music query,

00:43:15.909 --> 00:43:19.279
which is going to let you programmatically
decide what to playback.

00:43:19.280 --> 00:43:23.380
And then the final object here on the
right is the MusicPlayerController.

00:43:23.380 --> 00:43:30.280
And so this is the object that you'll
actually use to playback those media items.

00:43:30.280 --> 00:43:32.810
So let's look inside a media item.

00:43:32.809 --> 00:43:34.570
Here we have one song.

00:43:34.570 --> 00:43:41.460
So it's media type is "music," the song title
is "Hanging Around," by the Venetian Blinds --

00:43:41.460 --> 00:43:45.519
made it up -- and the album is "Open Up."

00:43:45.519 --> 00:43:52.159
Okay, and here also you can get at the album
art work for that particular album, all right?

00:43:52.159 --> 00:43:58.279
There's actually a whole lot of other
metadata that you can get at for each song

00:43:58.280 --> 00:44:00.870
or each media item that's in the library.

00:44:00.869 --> 00:44:04.049
This is a list of all of the things
that you can get on a media item here.

00:44:04.050 --> 00:44:05.490
It's kind of what you would expect.

00:44:05.489 --> 00:44:09.869
Over on the left-hand side, title,
artist, album, genre, and so on.

00:44:09.869 --> 00:44:14.949
Then there's some more dynamic information about
that media item over here, the items on the right.

00:44:14.949 --> 00:44:20.929
To get at the data there's a method on the
MPMediaItem object called value for property.

00:44:20.929 --> 00:44:25.500
You just pass in the appropriate
string, it's actually a longer string

00:44:25.500 --> 00:44:31.059
than what I'm showing you here,
but you can get at that data.

00:44:31.059 --> 00:44:36.590
So those are items, and now we
also have collections of items.

00:44:36.590 --> 00:44:42.970
And so here, for example, might be the collection of all
of the songs that I have loaded from the Venetian Blinds.

00:44:42.969 --> 00:44:48.039
And so we have these five songs on loaded in our library.

00:44:48.039 --> 00:44:58.210
Or a collection could be a play list that the
user has put together of a sequence of songs.

00:44:58.210 --> 00:45:05.309
All right, and to get at the data for a collection,
it's sort of a lot of the same process here.

00:45:05.309 --> 00:45:08.389
So a collection is an ordered list of items.

00:45:08.389 --> 00:45:13.469
You know, your play list, they play in a certain
order, albums play in a certain order, and so on.

00:45:13.469 --> 00:45:21.289
So you can get the array of MPMediaItems directly
out of the collection just with the items method.

00:45:21.289 --> 00:45:25.349
Every collection also has a representative item.

00:45:25.349 --> 00:45:29.860
So maybe this is the one item that you
would use to get the album art work

00:45:29.860 --> 00:45:33.320
if you were looking at a list of different albums.

00:45:33.320 --> 00:45:39.850
And in the case of the media play list
subclass, which is a subclass of the collection,

00:45:39.849 --> 00:45:45.150
there's a number of additional properties that are defined,
that you're able to get at, that are specific to play lists.

00:45:45.150 --> 00:45:52.320
Like their name, a persistent ID, attributes of
that play list, and seed items for genus play lists.

00:45:52.320 --> 00:45:59.550
It'll tell you the media item that was
used to generate that genius play list.

00:45:59.550 --> 00:46:02.700
So okay, so that's tells you what's in the library.

00:46:02.699 --> 00:46:06.299
Let's now talk to you about getting
the stuff out of the library.

00:46:06.300 --> 00:46:08.890
So first we'll show you the Media Picker.

00:46:08.889 --> 00:46:10.769
So this is it.

00:46:10.769 --> 00:46:14.539
It's a very similar user experience
to what you're probably accustomed

00:46:14.539 --> 00:46:19.130
to from using the iPod application
that's shipped with the device.

00:46:19.130 --> 00:46:23.430
You're able to choose -- it presents different
play lists, artists, songs, albums, and so on.

00:46:23.429 --> 00:46:28.139
And you can just navigate around in this in
the way that you're already familiar with.

00:46:28.139 --> 00:46:33.319
And so to present this to the user,
we're just going to call the allocate

00:46:33.320 --> 00:46:36.880
for this class, it's called the MPMediaPickerController.

00:46:36.880 --> 00:46:44.940
We'll set up a delegate, so that delegate is going to
receive the list of MPMediaItems that they selected.

00:46:44.940 --> 00:46:49.950
And then we present.- This object is a View
Controller, so we just present it modally.

00:46:49.949 --> 00:46:53.889
If we selected it to animate, then
it will animate in from the bottom,

00:46:53.889 --> 00:46:59.139
and the user is then able to then
go and pick some media items.

00:46:59.139 --> 00:47:04.099
Then after they made their choices, one of
these two delegate methods is going to fire.

00:47:04.099 --> 00:47:10.539
Either Media Picker did pick media items, and you'll
be presented with a collection of what they chose,

00:47:10.539 --> 00:47:13.509
or Media Picker did cancel, if they cancelled out.

00:47:13.510 --> 00:47:19.610
Because we've provided them with a way to get
out of this interface once they've gone in.

00:47:19.610 --> 00:47:20.640
So that's it.

00:47:20.639 --> 00:47:27.929
I mean, if you just want to present that to your user, you
just pop up that UI, they chose the music that they want

00:47:27.929 --> 00:47:32.239
to play, and boo, you're off, and you
can skip this next section and just wait

00:47:32.239 --> 00:47:35.439
for me to talk about the player controller.

00:47:35.440 --> 00:47:39.139
But let's say that you do actually
want to programmatically get

00:47:39.139 --> 00:47:43.750
at the music that's in the library to make choices yourself.

00:47:43.750 --> 00:47:46.099
So you use MediaQuery for that.

00:47:46.099 --> 00:47:52.789
And so there's sort of a sequence of events that
you go through when you're using a MediaQuery.

00:47:52.789 --> 00:48:00.380
So typically, you can just allocate one of these
and then start adding essentially predicates to it.

00:48:00.380 --> 00:48:06.110
But it's perhaps a little bit simpler if you
actually start from one of the built-in queries.

00:48:06.110 --> 00:48:10.670
And we have built-in queries for
albums, artists, songs, and so on.

00:48:10.670 --> 00:48:13.740
So down here at the bottom, two examples.

00:48:13.739 --> 00:48:19.109
So the first query is just doing a songs
query and returning the NS array of all

00:48:19.110 --> 00:48:23.579
of the media items that are loaded in the user's library.

00:48:23.579 --> 00:48:29.670
The second one is loading -- returning an
NS array of collections which represent all

00:48:29.670 --> 00:48:33.650
of the play list that they have loaded in their library.

00:48:33.650 --> 00:48:39.599
Okay, but -- so you started from one of those
built-in queries, and now you can actually go

00:48:39.599 --> 00:48:42.769
and add -- and basically filter down further.

00:48:42.769 --> 00:48:47.489
You can add media predicates to construct compound queries.

00:48:47.489 --> 00:48:52.259
So you're setting up -- excuse me
-- so your predicate will be based

00:48:52.260 --> 00:48:54.950
on a particular property that you want to search for.

00:48:54.949 --> 00:48:59.769
Maybe like album title or song title or play list name.

00:48:59.769 --> 00:49:04.519
And then you'll provide the value that you're
looking for and the search will be done.

00:49:04.519 --> 00:49:09.869
So let's look at some of those properties that
you can -- there they are, they're in the middle.

00:49:09.869 --> 00:49:20.789
So the properties you can use to search for to construct
a -- a predicate are listed here in the middle.

00:49:20.789 --> 00:49:25.050
So persistent ID, media type, artist,
and so on and so forth.

00:49:25.050 --> 00:49:31.110
And you notice that actually this is a shorter list than
all of the properties that are available for a media item.

00:49:31.110 --> 00:49:36.470
Remember, there are things like -- you know,
what track number is this on the album.

00:49:36.469 --> 00:49:41.449
You probably wouldn't want to actually do
a search for all song number 4's on albums.

00:49:41.449 --> 00:49:42.789
It just wouldn't make sense.

00:49:42.789 --> 00:49:45.840
These are the ones that make sense to be searching for.

00:49:45.840 --> 00:49:57.170
And likewise, a media play list also provides these
properties that you can construct a predicate with.

00:49:57.170 --> 00:50:04.760
And now at runtime the top line there is that you can
actually query to see which properties you're allowed

00:50:04.760 --> 00:50:10.870
to construct predicates with, and it's a
class method called "can filter by property."

00:50:10.869 --> 00:50:14.869
All right, so now let's put all of this together.

00:50:14.869 --> 00:50:21.339
So the first line here, we're going to query the
library, we're going to start with a songs query.

00:50:21.340 --> 00:50:28.620
We're going to look for all -- within that group,
we're going to construct a search predicate of all

00:50:28.619 --> 00:50:34.929
of the artists whose names match our
favorite band now, the Venetian Blinds,

00:50:34.929 --> 00:50:39.349
and we add that predicate to the
query to refine those results.

00:50:39.349 --> 00:50:47.199
So now the final NS array here is going to be an array
of MPMediaItems containing the results of that query.

00:50:47.199 --> 00:50:54.069
All right, so that is how you can construct a MediaQuery.

00:50:54.070 --> 00:50:57.660
So now let me go to the final part of this.

00:50:57.659 --> 00:51:00.460
The MusicPlayerController object.

00:51:00.460 --> 00:51:02.440
And this actually is really interesting.

00:51:02.440 --> 00:51:04.440
This comes in two flavors.

00:51:04.440 --> 00:51:10.400
You can see indicating here, the application
music player and the iPod music player.

00:51:10.400 --> 00:51:17.240
And the behaviors of these two objects are different-you
need to make a choice as to which one you're going to use.

00:51:17.239 --> 00:51:22.149
For games, I recommend that you use the
application music player, and here's why.

00:51:22.150 --> 00:51:29.440
So the application music player is going to playback
the media items sort of within your application

00:51:29.440 --> 00:51:31.679
or within the bounds of your application.

00:51:31.679 --> 00:51:36.279
And so when you quit your app the playback will stop.

00:51:36.280 --> 00:51:40.760
Okay? That's probably what you want, if you're a game.

00:51:40.760 --> 00:51:45.640
Now the other variant is to use the iPod music player.

00:51:45.639 --> 00:51:49.969
So with that one, you're essentially
handing a queue of items

00:51:49.969 --> 00:51:53.949
over to the iPod functionality and saying "Play these."

00:51:53.949 --> 00:52:00.889
And if your app quits in the meantime while they're playing,
those songs are going to actually still keep playing.

00:52:00.889 --> 00:52:06.339
Just like they do if you exit out of the
iPod app while you're using your phone today.

00:52:06.340 --> 00:52:10.260
And that behavior is really not appropriate for games.

00:52:10.260 --> 00:52:15.640
So as I say, I recommend that you
use the application music player.

00:52:15.639 --> 00:52:17.659
So now let's put it together.

00:52:17.659 --> 00:52:25.909
To construct the application music player, first you're
going to set it up with a queue of items for it to play.

00:52:25.909 --> 00:52:32.179
These items are the MPMediaItems
or a collection of MPMediaItems.

00:52:32.179 --> 00:52:33.329
Now remember, it's a queue.

00:52:33.329 --> 00:52:40.150
Because we're just going play one song after
another as we go through of the items list.

00:52:40.150 --> 00:52:44.990
We have control over the playback mode, so the
repeat mode and the shuffle mode here can be set.

00:52:44.989 --> 00:52:51.839
And there's also some subtlety here as to whether you're
using the application music player or the iPod music player.

00:52:51.840 --> 00:52:57.420
If you're using the iPod music player then
these modes you set will actually be sticky,

00:52:57.420 --> 00:53:02.639
and if you're using the application
player they'll be localized to your app.

00:53:03.690 --> 00:53:08.420
All right, now when we're using this object
controlling playback, play, pause, stop.

00:53:08.420 --> 00:53:09.550
No problem there.

00:53:09.550 --> 00:53:13.180
You also have the ability to move
through different items in the queue,

00:53:13.179 --> 00:53:15.519
so if you want in your game you can have something

00:53:15.519 --> 00:53:20.039
so the user can very easily skip to
the next item, say, in the play list.

00:53:20.039 --> 00:53:26.570
So skip to next, go back to the previous,
skip to the beginning of the list, and so on.

00:53:26.570 --> 00:53:31.150
So now, final example here of putting all of this together.

00:53:31.150 --> 00:53:36.510
The top line we construct a query using that
code that I just showed you a few minutes ago.

00:53:36.510 --> 00:53:41.720
So we have some compound query that we
have set up looking for a list of items.

00:53:41.719 --> 00:53:46.369
And it's going to return to us
the songs that we want to play.

00:53:46.369 --> 00:53:54.130
We create our MP MusicPlayerController here, we're
selecting the application music player variant of that.

00:53:54.130 --> 00:53:59.769
We're going to set our repeat mode to repeat the
whole play list, so it will just play endlessly.

00:53:59.769 --> 00:54:07.300
And finally here, we past to the music player a queue,
and we can actually just pass the query directly

00:54:07.300 --> 00:54:11.350
to the player, and it will actually perform the query.

00:54:11.349 --> 00:54:16.259
And so now the player will have a queue of items to play.

00:54:16.260 --> 00:54:18.160
Now, just pause.

00:54:18.159 --> 00:54:20.730
This is one scenario.

00:54:20.730 --> 00:54:25.889
The other scenario was that maybe you had
popped up that built-in user interface

00:54:25.889 --> 00:54:30.239
and have the user pick their music
that way, so you weren't using a query.

00:54:30.239 --> 00:54:34.709
If that was the case, then the code
would look something like this.

00:54:34.710 --> 00:54:40.210
In that delegate method of Media Picker did pick
media items, we follow the same sequence here,

00:54:40.210 --> 00:54:47.220
but the place where we pass those media items into the
queue is done by calling set queue with item collection.

00:54:47.219 --> 00:54:50.709
And that way now those results can get in as well.

00:54:50.710 --> 00:54:55.650
But whichever one of those two paths you
came from, now you can play the music.

00:54:55.650 --> 00:55:06.440
All right, so now we have our foreground
audio being played through OpenAL.

00:55:06.440 --> 00:55:10.990
We have our background music sound
track coming from the iPod Library.

00:55:10.989 --> 00:55:12.879
And we need to mix them together.

00:55:12.880 --> 00:55:16.490
And yesterday at the graphics and media
State of the Union we talked a little bit

00:55:16.489 --> 00:55:19.019
about this API called the AVAudioSession.

00:55:19.019 --> 00:55:26.699
And this is the API that really makes it very easy
for you to control how that mixing behavior happens.

00:55:26.699 --> 00:55:33.859
There's two particular categories that you can set in
the AVAudioSession that might be appropriate depending

00:55:33.860 --> 00:55:38.400
on the audio behavior you need in your game.

00:55:38.400 --> 00:55:43.730
So the first category that I want to talk
about here is called the ambient category.

00:55:43.730 --> 00:55:51.480
This is appropriate for games that have their own perhaps
foreground audio coming from OpenAL but want to mix

00:55:51.480 --> 00:55:54.880
with background music coming off of the iPod.

00:55:54.880 --> 00:55:59.829
This category is not appropriate for games
that need to play their own background music.

00:55:59.829 --> 00:56:05.079
So for example, if you have MP3s in your
game that you're going to be playing

00:56:05.079 --> 00:56:09.340
and that's your background music sound
track, you should not use this category,

00:56:09.340 --> 00:56:12.000
you should use the one I'm going to be talking about next.

00:56:12.000 --> 00:56:16.389
But back on ambient now, this also
follows some of the recommended behavior

00:56:16.389 --> 00:56:19.869
that we have for games running on the platform.

00:56:19.869 --> 00:56:25.579
Number one, their audio should silence if
the user turns the ringer switch to silent.

00:56:25.579 --> 00:56:30.389
And number two, the audio should
be silenced when the screen locks.

00:56:30.389 --> 00:56:34.389
Now that other category that I was
talking about is called ambient solo.

00:56:34.389 --> 00:56:39.379
So this is a category that's appropriate
for games that essentially want to take

00:56:39.380 --> 00:56:42.809
over and play their own background music.

00:56:42.809 --> 00:56:44.139
And play their own MP3s.

00:56:44.139 --> 00:56:51.480
Around this category will interrupt any playback
happening from the iPod, and allow your application

00:56:51.480 --> 00:56:56.909
to access the audio decoding resources
that are available on the device.

00:56:56.909 --> 00:56:59.989
So this one also follows the recommended behavior for games,

00:56:59.989 --> 00:57:04.669
obeying the ringer switch, and
silencing when the screen locks.

00:57:04.670 --> 00:57:12.809
All right, so folks, here is a quick look at
how you can set up this audio session object.

00:57:12.809 --> 00:57:20.599
So first we retrieve an instance to the audio session
singleton here, we're going to request the ambient category.

00:57:20.599 --> 00:57:26.250
So this is appropriate now, because remember our scenario:
We want to be playing the foreground music from OpenAL

00:57:26.250 --> 00:57:30.699
and have background music sound
track coming off of the iPod player.

00:57:30.699 --> 00:57:32.099
So we want to mix.

00:57:32.099 --> 00:57:36.179
And the ambient category allows us to mix those two.

00:57:36.179 --> 00:57:41.309
We set up our session as active, and
now we can go on to continue to set

00:57:41.309 --> 00:57:45.239
up OpenAL and set up access to the iPod library.

00:57:48.170 --> 00:57:55.349
So there's actually a lot more functionality
that you can get at using the AVAudioSession API.

00:57:55.349 --> 00:57:58.190
There's a number of other categories
that you might be interested in,

00:57:58.190 --> 00:58:02.809
but really probably aren't appropriate
for games in many instances.

00:58:02.809 --> 00:58:08.039
So it's only if you have a different kind of app
that you might want to go look into some of those.

00:58:08.039 --> 00:58:13.889
But it also provides functionality to handle
interruptions and notify you of state changes that happen

00:58:13.889 --> 00:58:21.819
in the audio system on the device, and enables you to set
your preferred hardware settings for different things.

00:58:21.820 --> 00:58:27.780
And also there's a lower level C based framework called
the Audio Session Services API, that really provides you

00:58:27.780 --> 00:58:31.990
with a lot of additional flexibility
and some more advanced behaviors.

00:58:31.989 --> 00:58:35.029
Just one for instance of what you'll find there.

00:58:35.030 --> 00:58:39.840
It's actually possible for you to
detect when your application starts

00:58:39.840 --> 00:58:44.460
up whether there is already audio
being played from the iPod.

00:58:44.460 --> 00:58:49.210
And you might use that knowledge
to make a decision about whether

00:58:49.210 --> 00:58:54.610
or not to play your own background sound track for music.

00:58:54.610 --> 00:59:00.190
And that decision should drive which one of those
audio session categories you choose for your game.