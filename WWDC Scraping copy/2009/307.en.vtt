WEBVTT

00:00:12.830 --> 00:00:16.559
>> Good morning, everyone, welcome to
the introduction on OpenCL session.

00:00:16.559 --> 00:00:19.769
So today's session is going to be divided in two parts.

00:00:19.769 --> 00:00:23.660
The first part is going to focus on the compute
model and the framework for using OpenCL,

00:00:23.660 --> 00:00:28.129
and then I'm going to hand it over to Mon Ping who's
going to talk about the OpenCL kernel language,

00:00:28.129 --> 00:00:30.849
which is the language used to actually
write your OpenCL kernels.

00:00:30.850 --> 00:00:33.090
So let's go ahead and get started.

00:00:33.090 --> 00:00:35.220
OpenCL compute model and framework.

00:00:35.219 --> 00:00:41.320
My name is David Black-Schaffer, I'm one of the
engineers in the OpenCL team, and so welcome to OpenCL.

00:00:41.320 --> 00:00:43.659
This is a really cool technology.

00:00:43.659 --> 00:00:48.319
We here at Apple are really excited about this, and we're
mostly excited to see what you guys out there are going

00:00:48.320 --> 00:00:51.020
to do with the level of compute power that OpenCL gives you.

00:00:51.020 --> 00:00:57.760
So to give you a feeling for this, with OpenCL you're going
to be able to leverage the CPUs and GPUs in your system

00:00:57.759 --> 00:01:04.219
in order to write parallel code which is going to give you
dramatic speedups on computationally intensive algorithms.

00:01:04.219 --> 00:01:08.640
So if your algorithm is not computationally intensive,
you may have a hard time taking advantage of OpenCL.

00:01:08.640 --> 00:01:13.569
But if you've got a lot of compute to do,
OpenCL is going to give you a huge win.

00:01:13.569 --> 00:01:18.159
OpenCL also has another major benefit, which
is it lets you write portable accelerated code.

00:01:18.159 --> 00:01:21.349
And you may not think about this, but
what it means is that you don't have

00:01:21.349 --> 00:01:25.959
to write specialized code for each
GPU and each variant of CPU.

00:01:25.959 --> 00:01:30.909
So no more SSE4, SSE3 type code,
just use OpenCL and you can run

00:01:30.909 --> 00:01:34.649
across all the devices in an optimized accelerated manner.

00:01:34.650 --> 00:01:37.560
So what are you going to learn in the presentation today?

00:01:37.560 --> 00:01:41.200
Well we're going to start out with
what is OpenCL and what is it good for?

00:01:41.200 --> 00:01:45.939
Going to take a look at an algorithm that matches well
to OpenCL and point out the parts of the algorithm

00:01:45.939 --> 00:01:48.310
that allow OpenCL to really work well with it.

00:01:48.310 --> 00:01:53.280
Now we're going to go on and talk about how OpenCL
exposes the hardware, what is the computation model

00:01:53.280 --> 00:01:55.599
in OpenCL, what is the memory model in OpenCL.

00:01:55.599 --> 00:02:02.269
And these are important to know so you can understand how to
take advantage of OpenCL in the best way for your algorithm.

00:02:02.269 --> 00:02:05.879
And then we're going to go through the nuts and
bolts of how you actually use the framework.

00:02:05.879 --> 00:02:11.109
We're going to look at sample code for setting up
OpenCL, doing resource allocation, executing your kernel,

00:02:11.110 --> 00:02:17.160
and then look at some issues with synchronization in
particular when you have multiple devices in your system.

00:02:17.159 --> 00:02:19.490
So let's get started here.

00:02:19.490 --> 00:02:20.450
What is OpenCL?

00:02:20.449 --> 00:02:25.899
Well fundamentally it's a framework that gives you
access to all the compute resources in your application.

00:02:25.900 --> 00:02:30.800
So if you've got CPUs and GPUs you
can use them all together with OpenCL.

00:02:30.800 --> 00:02:33.870
It's a low level framework for high performance.

00:02:33.870 --> 00:02:39.599
And this implies an explicit tradeoff, meaning that you may
have to do more work, but because you have to do more work,

00:02:39.599 --> 00:02:42.449
you can really get the full performance out of the system.

00:02:42.449 --> 00:02:45.689
So for example, in OpenCL you're
responsible for managing your memory.

00:02:45.689 --> 00:02:51.550
But what that means is you can do exactly the memory
movement and copying that's required for your algorithm

00:02:51.550 --> 00:02:54.450
so you get the best possible performance.

00:02:54.449 --> 00:02:59.269
OpenCL targets parallel computationally
intensive algorithms.

00:02:59.270 --> 00:03:04.350
Again, if you're not doing a lot of work,
OpenCL is not going to give you a huge benefit.

00:03:04.349 --> 00:03:07.049
Now you'll note here that I put data in parentheses here.

00:03:07.050 --> 00:03:10.280
OpenCL is not limited to data parallel computation,

00:03:10.280 --> 00:03:14.590
but today's GPUs are really optimized
for data parallel computation.

00:03:14.590 --> 00:03:19.259
So if you want to get the best bang out of your GPU,
you need to be looking at a data parallel algorithm.

00:03:19.259 --> 00:03:22.909
So with that said, there's a lot
of things you can do with OpenCL.

00:03:22.909 --> 00:03:29.569
You can accelerate everything from physics simulations
to image processing, medical imaging, financial modeling,

00:03:29.569 --> 00:03:32.759
lots and lots of stuff can get a big speed up from OpenCL.

00:03:32.759 --> 00:03:36.870
And what we're really excited about is what
you're going to bring to the table here.

00:03:36.870 --> 00:03:40.430
What algorithms are there out there that we
haven't looked at or that haven't been seen before

00:03:40.430 --> 00:03:47.170
that when we give you OpenCL you're going to say hey, I
can really accelerate this new thing and get a real boost.

00:03:47.169 --> 00:03:52.209
So I mentioned that OpenCL is particularly good
on today's hardware for data parallel computing.

00:03:52.210 --> 00:03:54.689
Let's talk about what data parallel is.

00:03:54.689 --> 00:03:58.329
So when I say data parallel computing,
I mean two primary things;

00:03:58.330 --> 00:04:02.670
you're doing similar computations on independent data.

00:04:02.669 --> 00:04:07.389
What that means is that each of the elements
you're processing is doing the same control flow.

00:04:07.389 --> 00:04:14.649
So in the same sort of computations and they're all
independent, that is the results for element 37 don't depend

00:04:14.650 --> 00:04:18.939
on the results for element 1, so you
can do them in any order in parallel.

00:04:18.939 --> 00:04:23.980
So let's take a look at an example of this, something that
will map well to OpenCL where you can get a real speed up.

00:04:23.980 --> 00:04:28.530
And the example I'm going to give you is
averaging an image, this is a box filter.

00:04:28.529 --> 00:04:31.679
We're just going to take a pixel in an image
and we're going to average the pixels around it,

00:04:31.680 --> 00:04:33.250
so we're going to do sort of a crude blur.

00:04:33.250 --> 00:04:36.839
So this is the same computation for each output.

00:04:36.839 --> 00:04:39.119
Each pixel is going to average the elements around it.

00:04:39.120 --> 00:04:41.230
And all the results are independent.

00:04:41.230 --> 00:04:45.500
What I do in the first pixel doesn't
impact what I do on the 15th pixel.

00:04:45.500 --> 00:04:50.379
So we take a look at this, we have an example image
here, and I'm going to process a region of this image.

00:04:50.379 --> 00:04:54.060
You're going to look at the pixels in this
region, can add them up and average it.

00:04:54.060 --> 00:04:57.839
And indeed I'm going to go through and do this for
all the pixels in the image, and at the end I'm going

00:04:57.839 --> 00:05:01.189
to get a result which is an average
or blurred output of this image.

00:05:01.189 --> 00:05:05.279
So this is a great example of something
that maps very well to today's GPUs.

00:05:05.279 --> 00:05:11.609
It's the same sort of computation for all the pixels
doing this averaging, and they're all independent.

00:05:11.610 --> 00:05:18.090
Indeed no matter what pixel I look at here, it doesn't
matter if I processed other pixels before it or after it.

00:05:18.089 --> 00:05:23.549
And it's this characteristic that allows today's
GPUs to really run lots of this stuff in parallel

00:05:23.550 --> 00:05:27.610
because you don't have to worry
about synchronizing between them.

00:05:27.610 --> 00:05:31.230
So if we take a look at the actual
computation kernel that's used

00:05:31.230 --> 00:05:34.069
for this example, here's the code for doing the averaging.

00:05:34.069 --> 00:05:37.089
And if you look at this, there
are three parts to the kernel.

00:05:37.089 --> 00:05:40.699
The first part is the kernel determines
what pixel am I working on.

00:05:40.699 --> 00:05:45.539
So by using this image here, I've told OpenCL I'm going
to paralyze [phonetic] this across the whole image.

00:05:45.540 --> 00:05:49.410
All of my pixels are independent,
you can process them all in parallel.

00:05:49.410 --> 00:05:54.810
So the first thing the kernel needs to do is say
okay, which one of those pixels am I processing?

00:05:54.810 --> 00:06:00.050
Then we simply go through and do a set of four loops,
this is a really simple implementation here to average

00:06:00.050 --> 00:06:03.050
out the pixels around it, and then I write out the result.

00:06:03.050 --> 00:06:06.120
So this is the kernel that OpenCL is going to execute,

00:06:06.120 --> 00:06:10.030
and it's going to be executed once
for every pixel in that image.

00:06:10.029 --> 00:06:13.829
Again, this is how I'm telling OpenCL what
sort of parallelism I have in my problem.

00:06:13.829 --> 00:06:18.689
I'm running it over the whole image, so I'm
going to get one kernel execution for each pixel.

00:06:18.689 --> 00:06:23.339
So this gives you an idea of how
you express parallelism in OpenCL.

00:06:23.339 --> 00:06:30.769
So for this example here, it's data parallel across two
dimensions, thinking about an X and Y on a pixel basis.

00:06:30.769 --> 00:06:36.599
I'm going to execute that compute kernel for every
pixel in there, there's going to be a lot of executions.

00:06:36.600 --> 00:06:44.760
So for example, if I'm processing 1,024 x 1,024
image, my global problem dimensions are 1,024 x 1,024.

00:06:44.759 --> 00:06:46.909
that's the total work in the problem.

00:06:46.910 --> 00:06:53.280
And I'm having one kernel execution per pixel, which
means I'm going to have a million total kernel executions.

00:06:53.279 --> 00:06:57.139
So I've told OpenCL here that you've got
a million things you can do in parallel.

00:06:57.139 --> 00:07:01.659
OpenCL is then going to go out on the hardware
device and try and run as many of them as it can

00:07:01.660 --> 00:07:04.670
at a time to get you really good throughput.

00:07:04.670 --> 00:07:09.610
So now that I've given you an example of the sort of
algorithm that maps well to OpenCL, and talked a little bit

00:07:09.610 --> 00:07:16.389
about how OpenCL sees the parallelism, let's talk
more specifically about the OpenCL computation model.

00:07:16.389 --> 00:07:20.810
So in OpenCL, as I've alluded to, kernels
are executed across a global domain.

00:07:20.810 --> 00:07:24.129
Each of the kernel executions is called a work item.

00:07:24.129 --> 00:07:28.730
So in the example I gave you before, the
global dimensions were 1,024 x 1,024,

00:07:28.730 --> 00:07:31.620
and that defines the range of computation.

00:07:31.620 --> 00:07:36.949
This is how you tell OpenCL here's the
degree of parallelism in my program.

00:07:36.949 --> 00:07:39.879
There's one work item for each computation.

00:07:39.879 --> 00:07:46.939
So in the example 1,024 x 1,024 there are a million
work items that OpenCL can execute in parallel.

00:07:46.939 --> 00:07:52.250
Now in addition to this global domain, you
can split up your work into local workgroups,

00:07:52.250 --> 00:07:54.720
and workgroups are defined by their local dimensions.

00:07:54.720 --> 00:08:03.140
So for example, if I had the global domain of 1,024 by 1,024
I might decide my local domain is 128 x 128, or 16 x 16.

00:08:03.139 --> 00:08:05.860
And these define the size of workgroups.

00:08:05.860 --> 00:08:11.120
Workgroups are important because these are the work items
that are executed together on one processing element.

00:08:11.120 --> 00:08:17.149
So if you have a GPU with 100 cores in it, these are
the work items that are going to run on one core.

00:08:17.149 --> 00:08:23.389
And why that's important is it allows them
to share local memory and synchronization.

00:08:23.389 --> 00:08:27.579
So within a workgroup because these
items are being executed together,

00:08:27.579 --> 00:08:30.789
you can share local memory and those items can synchronize.

00:08:30.790 --> 00:08:34.920
So there are a few caveats to this computation
model and these are important to keep in mind

00:08:34.919 --> 00:08:38.189
when you're looking at moving your algorithm to OpenCL.

00:08:38.190 --> 00:08:43.020
So the first caveat is that global work items must
be independent, there's no global synchronization.

00:08:43.019 --> 00:08:49.669
If you have an algorithm or work item of 3 million needs to
synchronize with work item 74, you're going to have to look

00:08:49.669 --> 00:08:52.839
at how to restructure your algorithm to fit into OpenCL.

00:08:52.840 --> 00:08:56.980
But what you can do is you can do
synchronization within a workgroup.

00:08:56.980 --> 00:08:59.340
So if you need synchronization,
you need to figure out how to fit

00:08:59.340 --> 00:09:03.690
that into the smaller workgroup not
across the entire global domain.

00:09:03.690 --> 00:09:06.140
So the three things you need to take away from here:

00:09:06.139 --> 00:09:11.559
for the compute model for OpenCL you have global dimensions
These define the range of your computation.

00:09:11.559 --> 00:09:15.889
This is how you tell OpenCL here are the
things that you can execute in parallel.

00:09:15.889 --> 00:09:20.569
You have local dimensions, this defines the size
of your workgroup, and a workgroup are things

00:09:20.570 --> 00:09:24.160
that get executed together, and
there's no global synchronization.

00:09:24.159 --> 00:09:29.029
If you need to do synchronization in your
algorithm, you need to do it within the workgroup.

00:09:29.029 --> 00:09:31.179
So let's take a look at an example here.

00:09:31.179 --> 00:09:34.649
Here's the same thing I had before,
my global dimensions 1k x 1k,

00:09:34.649 --> 00:09:38.529
and I'm going to run with local dimensions of 128 x 128.

00:09:38.529 --> 00:09:43.459
So what that means is that this chunk of work items
are going to be executed together on one processor

00:09:43.460 --> 00:09:47.150
on the OpenCL device, and this
chunk will be executed on another.

00:09:47.149 --> 00:09:53.179
And if you think about it, this will divide up your entire
global domain into chunks that get executed together.

00:09:53.179 --> 00:09:58.569
So what this means is if you have synchronization, say
I need to synchronization these two global work items,

00:09:58.570 --> 00:10:00.760
they're in the same workgroup so this is okay.

00:10:00.759 --> 00:10:06.409
I can go ahead and I can use barriers within my
kernel and I can use memory fences within my kernel

00:10:06.409 --> 00:10:09.990
to synchronize the execution of those two work items.

00:10:09.990 --> 00:10:14.899
However, if I have two work items that are in
different workgroups, so here are two global work items

00:10:14.899 --> 00:10:19.399
that are far apart and they're not in the
same workgroup, I cannot synchronize these.

00:10:19.399 --> 00:10:22.889
The functions for synchronization within
a kernel only work within a workgroup.

00:10:22.889 --> 00:10:27.720
So now let's talk about problem dimensions.

00:10:27.720 --> 00:10:30.550
As I gave you an example before of 1k x 1k for an image,

00:10:30.549 --> 00:10:33.059
there are a lot of other problem
dimensions you might run into.

00:10:33.059 --> 00:10:37.059
Say you have an array with a million elements
in it, maybe it's financial data over time.

00:10:37.059 --> 00:10:40.599
Maybe I wanna just use global dimensions
of a million x 1 x 1,

00:10:40.600 --> 00:10:46.210
or maybe I'm processing a high-definition
video frame, 1920 x 1200, 2.3 million pixels.

00:10:46.210 --> 00:10:54.530
Well if I want to process one pixel at a time, do per pixel
processing, if I set my global dimensions to 1920 x 1200,

00:10:54.529 --> 00:10:59.500
OpenCL will process all the pixels in
parallel with one work item per pixel.

00:10:59.500 --> 00:11:08.370
Similarly, if I have a volume, say I have 256 cube
voxels,16.7 million voxels and I want to process each voxel,

00:11:08.370 --> 00:11:16.990
I can set my global dimensions to 256 x 256 x 256, and
I'm telling OpenCL you have 16.7 million work items

00:11:16.990 --> 00:11:20.730
that you can run in parallel in
the most efficient manner you can.

00:11:20.730 --> 00:11:24.700
So this brings up the questions of how do you
choose the right dimensions for your algorithm.

00:11:24.700 --> 00:11:29.200
Well there are really two things you want to keep in
mind here, you want to pick the ones that map best

00:11:29.200 --> 00:11:33.920
to your application, so you want to pick something that
makes sense for the parallelism in your application.

00:11:33.919 --> 00:11:39.669
If you're processing images, it's very likely you'll
want a two-dimensional set that's the size of the image.

00:11:39.669 --> 00:11:44.399
But maybe you have an algorithm that processes
pixels in 8 x 8 chunks, so maybe you don't want

00:11:44.399 --> 00:11:47.720
to have something which is mapping 1 to 1 with pixels.

00:11:47.720 --> 00:11:50.519
You also want to choose something that performs well.

00:11:50.519 --> 00:11:55.689
The way GPUs today get high throughput is they
have lots of work items which allow some work items

00:11:55.690 --> 00:11:58.140
to process while others are waiting for memory.

00:11:58.139 --> 00:12:02.480
So you want to make sure you have thousands and
thousands of work items running at any given time,

00:12:02.480 --> 00:12:04.830
so you want to make sure you have enough work items.

00:12:04.830 --> 00:12:08.910
But by the same token, you want to make
sure you do enough work in each work item

00:12:08.909 --> 00:12:12.259
that you amortize the cost of that work item.

00:12:12.259 --> 00:12:17.269
So you don't want to do a single add in a work item, but at
the same time you want to make sure you have lots of them.

00:12:17.269 --> 00:12:20.509
So this is a balancing act for performance.

00:12:20.509 --> 00:12:25.939
So now we talked a bit about the OpenCL compute model,
we've talked about how OpenCL expresses parallelism

00:12:25.940 --> 00:12:29.290
and the limitations of synchronization only in workgroups.

00:12:29.289 --> 00:12:31.939
Let's take a look at the OpenCL memory model.

00:12:31.940 --> 00:12:37.220
So in OpenCL you have the host device which is the
machine you're running on, and you have the compute device

00:12:37.220 --> 00:12:41.160
which is often a GPU or it may
actually be the CPUs in that machine.

00:12:41.159 --> 00:12:45.889
The compute devices executing your work
items and they're executed in workgroups.

00:12:45.889 --> 00:12:51.539
So in terms of memory, what you have here is you
have host memory, this is the memory on the CPU,

00:12:51.539 --> 00:12:55.469
this is the 2 gigabytes or 8 gigabytes or
whatever your machine actually has in it.

00:12:55.470 --> 00:13:01.120
And you can get this memory and move it
over to the global memory on the device.

00:13:01.120 --> 00:13:05.029
So if you have a GPU with 512 megabytes
of memory, that's your global memory.

00:13:05.029 --> 00:13:07.699
You've got 512 megabytes on the GPU.

00:13:07.700 --> 00:13:11.320
Now I want to really emphasize
here, this is not synchronized.

00:13:11.320 --> 00:13:14.660
This is not a cache, data does not move automatically.

00:13:14.659 --> 00:13:21.429
You're responsible for taking your memory from the host
and moving it over to the device when you want to use it.

00:13:21.429 --> 00:13:27.239
So from this global memory on the device, you can access
it from the work items that you're actually executing.

00:13:27.240 --> 00:13:33.649
Now in addition, workgroups on the device will have local
memory, and local memory is shared within a workgroup.

00:13:33.649 --> 00:13:36.220
Now this memory is much, much smaller.

00:13:36.220 --> 00:13:40.370
On today's GPUs, we're talking in
the order of 16 kilobytes of memory.

00:13:40.370 --> 00:13:44.840
However, this memory is phenomenally
faster on devices that support it.

00:13:44.840 --> 00:13:48.210
So if your GPU has local memory available, you can get 10

00:13:48.210 --> 00:13:52.759
or 100 times the bandwidth using local
memory that you can over global memory.

00:13:52.759 --> 00:13:57.879
So one of the biggest optimizations you can make
on those devices is to really use local memory.

00:13:57.879 --> 00:14:03.269
In addition to work items, we have private
memory, and private memory is per work item.

00:14:03.269 --> 00:14:08.100
So if a particular work item needs to store some data that
it's working on, it can use private memory to do that,

00:14:08.100 --> 00:14:11.450
but it's per work item, this isn't
shared for the work group.

00:14:11.450 --> 00:14:15.910
So the important takeaway message here
is that memory management is explicit.

00:14:15.909 --> 00:14:19.230
It's up to you to move your data
through this memory hierarchy.

00:14:19.230 --> 00:14:24.190
You're going to have to take your data from the host,
move it into the global memory, move it into local memory,

00:14:24.190 --> 00:14:26.160
do your work on it, and then move it back.

00:14:26.159 --> 00:14:31.360
And the tradeoff for this is you can
control exactly how data is moved,

00:14:31.360 --> 00:14:34.169
this allows you to get the best possible performance.

00:14:34.169 --> 00:14:38.110
You're not going to have the OS or some
memory manager guessing what you want to do,

00:14:38.110 --> 00:14:41.610
you do exactly what you need and just what you need.

00:14:41.610 --> 00:14:46.490
So now that we've talked a little bit about how
OpenCL works, how it thinks about the world in terms

00:14:46.490 --> 00:14:50.560
of paralyzing computation and what the
memory model that it's exposed to is,

00:14:50.559 --> 00:14:53.279
let's take a look at actually using OpenCL.

00:14:53.279 --> 00:14:58.860
So the first thing you're going to want to do when you use
OpenCL is set up OpenCL and start allocating resources.

00:14:58.860 --> 00:15:00.470
So let's take a look at this.

00:15:00.470 --> 00:15:03.590
So in OpenCL there are a bunch of
objects you need to be aware of.

00:15:03.590 --> 00:15:12.149
The basic ones for doing setup are devices, these are GPUs
or CPUs; context, and a context is a collection of devices

00:15:12.149 --> 00:15:16.250
which tells OpenCL that they can share memory and queues.

00:15:16.250 --> 00:15:18.970
Queues are used to submit work to a device.

00:15:18.970 --> 00:15:22.290
So in the example I have here, we
have a GPU and a CPU in a context,

00:15:22.289 --> 00:15:25.779
and we have a queue for submitting
work to each of those devices.

00:15:25.779 --> 00:15:30.079
In addition for memory, we have buffers and images.

00:15:30.080 --> 00:15:35.460
Buffers are simply blocks of memory, you can use them
however you want, you can access them however you want.

00:15:35.460 --> 00:15:39.870
Images are two-dimensional or three-dimensional
formatted structures.

00:15:39.870 --> 00:15:44.190
For executing things we have programs,
and a program is a collection of kernels.

00:15:44.190 --> 00:15:45.600
And we have kernels.

00:15:45.600 --> 00:15:50.310
Kernels are basically containers that keep
track of arguments for executing a kernel.

00:15:50.309 --> 00:15:54.489
In addition, if you need to do synchronization
or profiling, you're going to use events.

00:15:54.490 --> 00:15:57.590
So here are the basic objects that
you're going to need to use in OpenCL,

00:15:57.590 --> 00:16:02.330
and let's take a look in the actual details
of you get them and manipulate them.

00:16:02.330 --> 00:16:05.330
So for setting up OpenCL there are basically three steps.

00:16:05.330 --> 00:16:08.420
The first step is you need to get
the devices you want to work on.

00:16:08.419 --> 00:16:12.990
The second step is you create a context which
allows those devices to share memory objects.

00:16:12.990 --> 00:16:18.889
And then the third one is simply create command
queues so you can submit work to all those devices.

00:16:18.889 --> 00:16:21.460
So the code for doing this is fairly straightforward.

00:16:21.460 --> 00:16:26.840
When you want to get your devices, you call
CLGetDeviceIDs and you specify the device type.

00:16:26.840 --> 00:16:33.420
So here I'm saying give me a GPU, and OpenCL will
return a GPU, if there's a GPU in your system that is.

00:16:33.419 --> 00:16:38.759
So you call CLGetDeviceIDs again, you
want a CPU, and you'll get a CPU device.

00:16:38.759 --> 00:16:44.080
Now once I've got my devices, if I want to use them
together I need to create a context that tells OpenCL

00:16:44.080 --> 00:16:46.500
that I want to share memory between these devices.

00:16:46.500 --> 00:16:53.159
So to do that I call CLCreateContext and I specify the
number of devices, and I specify which devices I want to be

00:16:53.159 --> 00:17:00.360
in that context, and OpenCL will go ahead and create
a context which tells OpenCL when I use this context,

00:17:00.360 --> 00:17:04.829
I want you to make sure that memory objects in
the context can be shared between the devices.

00:17:04.829 --> 00:17:07.659
So if you have a CPU and a GPU,
you want to make sure they're

00:17:07.660 --> 00:17:11.000
in the same context so you can share data between them.

00:17:11.000 --> 00:17:16.400
And finally, I want to submit work, so I need to
create command queues to submit work to the devices.

00:17:16.400 --> 00:17:21.000
So I call CLCreateCommandQueue, I specify the
context and the device I want to submit to,

00:17:21.000 --> 00:17:24.420
and I get a queue which allows
me to submit work to that device.

00:17:24.420 --> 00:17:26.700
Similarly I do the same thing for the CPU.

00:17:26.700 --> 00:17:32.970
So after calling the setup code, I've got two command
queues that I can use to submit work to the CPU and the GPU,

00:17:32.970 --> 00:17:36.180
and I've got a context which allows
me to create memory objects

00:17:36.180 --> 00:17:39.590
which are going to be shared between those two devices.

00:17:39.589 --> 00:17:42.139
So a few notes about setup.

00:17:42.140 --> 00:17:49.990
Devices. All of the CPU cores in your system show up as one
device, so in most machines you're going to fire up OpenCL

00:17:49.990 --> 00:17:53.390
and you're going to have a GPU and a CPU device.

00:17:53.390 --> 00:17:55.000
Now this makes a lot of sense.

00:17:55.000 --> 00:18:00.039
Remember OpenCL is going to execute your kernel in a data
parallel manner, that is it's going to run your kernel

00:18:00.039 --> 00:18:02.000
across all the processors that are available.

00:18:02.000 --> 00:18:08.309
So you if you are on an 8 core CPU, it's going to try
to run your kernel across all 8 cores at the same time.

00:18:08.309 --> 00:18:13.869
And indeed this is just what a GPU does, your GPU may
have 16 or 100 cores and whatever kernel you run is going

00:18:13.869 --> 00:18:17.539
to run across all of those cores at once.

00:18:17.539 --> 00:18:21.289
So as I've mentioned, contexts are for sharing memory.

00:18:21.289 --> 00:18:24.029
You need to make sure if you want
to share memory between two devices

00:18:24.029 --> 00:18:27.420
that you have both of those devices in your context.

00:18:27.420 --> 00:18:33.350
And queues are for submitting work, and you need a
queue for each device to which you want to submit work.

00:18:33.349 --> 00:18:38.059
So if you have a CPU and a GPU, you're going to have two
queues, and it's up to your application to figure out how

00:18:38.059 --> 00:18:42.369
to submit work to one or the other and break up your work.

00:18:42.369 --> 00:18:44.639
So how do you choose a device?

00:18:44.640 --> 00:18:46.420
I mean, a system may have several devices.

00:18:46.420 --> 00:18:52.320
If you start up a MacBook Pro today, you're going to
have the discrete GPU, the integrated GPU, and the CPU.

00:18:52.319 --> 00:18:56.049
And your application needs to choose
the best device for running on.

00:18:56.049 --> 00:18:58.750
So the way you do this is you're going
to have to look through the devices

00:18:58.750 --> 00:19:00.950
and choose the one that is best for your algorithm.

00:19:00.950 --> 00:19:04.500
So the device that's best for you is
not going to be the same for everyone.

00:19:04.500 --> 00:19:09.920
And you need to know your algorithm and you need to
know the devices and figure out the best one to run on.

00:19:09.920 --> 00:19:12.690
But OpenCL provides a rich way of querying devices,

00:19:12.690 --> 00:19:16.830
determine what capabilities they have,
and what raw performance they may have.

00:19:16.829 --> 00:19:19.289
So you can ask for the number of compute units.

00:19:19.289 --> 00:19:23.230
On a MacBook Pro the CPU might have
two compute units, it's got two cores.

00:19:23.230 --> 00:19:25.500
The GPU may have multiple cores.

00:19:25.500 --> 00:19:30.440
You can look at the maximum clock frequency for the
device, or you can look at how much memory they have.

00:19:30.440 --> 00:19:33.549
In addition, if you have an algorithm
which requires special features,

00:19:33.549 --> 00:19:36.329
say your algorithm requires double precision arithmetic.

00:19:36.329 --> 00:19:41.519
You probably want to check and make sure the device
you're using supports double precision arithmetic.

00:19:41.519 --> 00:19:44.450
So you need to pick the best device for your algorithm.

00:19:44.450 --> 00:19:47.620
And this is going to involve enumerating
the devices that are available,

00:19:47.619 --> 00:19:51.279
looking through them for the characteristics you care
about, and choosing the device you want to run on.

00:19:51.279 --> 00:19:56.809
So now that we've talked about setting
up OpenCL, how do you chose devices,

00:19:56.809 --> 00:19:59.940
how do you set up a context, and
how do you create command queues.

00:19:59.940 --> 00:20:02.620
Let's talk about allocating memory resources.

00:20:02.619 --> 00:20:07.299
As I mentioned before, there are two types of
two memory resources, they're buffers and images.

00:20:07.299 --> 00:20:11.319
So buffers are simply chunks of memory,
you're telling OpenCL I want a chunk

00:20:11.319 --> 00:20:13.619
of memory that I can do whatever I want with.

00:20:13.619 --> 00:20:17.789
Within the kernel you can access it
through pointers, as structs, as arrays,

00:20:17.789 --> 00:20:20.769
basically however you want, it's just a chunk of memory.

00:20:20.769 --> 00:20:25.049
And within a kernel you can read and write to
the buffer, so you can read data from a buffer

00:20:25.049 --> 00:20:27.960
and write data back within the same kernel.

00:20:27.960 --> 00:20:32.670
Images are different, they're opaque
2D or 3D formatted data structures.

00:20:32.670 --> 00:20:37.529
You can't access them as pointers, the only way you
can access them is through read image and write image.

00:20:37.529 --> 00:20:40.980
And they have one particular limitation
that you need to be aware of.

00:20:40.980 --> 00:20:44.839
With any given kernel, you can read
an image or write it, but not both.

00:20:44.839 --> 00:20:50.519
So if you have an image you can't use it as
both the source and destination for a kernel.

00:20:50.519 --> 00:20:55.069
So let's talk a little bit what I mean by
formatted opaque data structure for images.

00:20:55.069 --> 00:20:57.919
So images have formats and samplers.

00:20:57.920 --> 00:21:02.960
When you go to create an image, you're going to specify
a format, and the format consists of the channel order,

00:21:02.960 --> 00:21:06.160
do you want to have an RGBA image, or
maybe you just want an alpha channel,

00:21:06.160 --> 00:21:08.940
or maybe you just care about RGB but not the alpha channel.

00:21:08.940 --> 00:21:13.289
And the type of data, do you want an
8 bit image or a floating point image?

00:21:13.289 --> 00:21:19.950
And you can find out which image formats are supported
by your device by calling CLGetSupportedImageFormats.

00:21:19.950 --> 00:21:25.840
In addition, when you go to read an image, you specify a
sampler for reading the image, and the sampler allows you

00:21:25.839 --> 00:21:28.869
to control how the image is accessed when you read it.

00:21:28.869 --> 00:21:32.919
So for example, you can specify you want
linear or nearest neighbor filtering.

00:21:32.920 --> 00:21:38.470
What this means is if you read between a pixel with linear
filtering, OpenCL is going to average the pixels for you.

00:21:38.470 --> 00:21:41.610
And you can specify what happens when
you read off the side of an image.

00:21:41.609 --> 00:21:46.359
So if you're addressing you can clamp at zero,
so if you read a negative value of get zero,

00:21:46.359 --> 00:21:50.000
you can clamp at the edge value,
you can tell it to repeat the image.

00:21:50.000 --> 00:21:54.690
And you can specify whether images are accessed using
normalized coordinates or not, and that's where the X

00:21:54.690 --> 00:21:57.279
and Y ranges are scaled from zero to one.

00:21:57.279 --> 00:22:02.099
So the reason for providing this is that
on GPUs, you get hardware to do this.

00:22:02.099 --> 00:22:05.789
So if your algorithm can benefit
from using linear sampling,

00:22:05.789 --> 00:22:09.549
on the GPU you can use linear sampling
and get hardware to do that for you.

00:22:09.549 --> 00:22:13.909
In addition, OpenCL is going to store your
images in a format that's efficient both

00:22:13.910 --> 00:22:16.600
in terms of size and speed for that device.

00:22:16.599 --> 00:22:21.490
And that's why the formats are opaque, so
that we can optimize it for the device.

00:22:21.490 --> 00:22:25.660
So let's take a look at an example of how you
actually go about allocating memory objects.

00:22:25.660 --> 00:22:30.670
So first we're going to allocate an image, and to allocate
an image we obviously need to specify the format first.

00:22:30.670 --> 00:22:36.410
So here I'm going to specify the format, I want a floating
point pixel type, and I want four channels per pixel.

00:22:36.410 --> 00:22:39.509
So CLRGBA and CL_FLOAT.

00:22:39.509 --> 00:22:41.490
I then call CLCreateImage.

00:22:41.490 --> 00:22:47.059
And when I call CreateImage, note that I'm passing in
the context, I'm telling OpenCL I'm creating this image

00:22:47.059 --> 00:22:53.819
within a context which lets OpenCL know that it can
share that image across all the devices in the context.

00:22:53.819 --> 00:23:00.490
I then specify the format and the width and height of
the image, and OpenCL will create that image for me.

00:23:00.490 --> 00:23:05.849
Similarly, if I want to create a buffer, I call
CLCreateBuffer, I specify the context for the buffer,

00:23:05.849 --> 00:23:10.009
but instead of specifying a format
since the buffer is just a chunk of me,

00:23:10.009 --> 00:23:12.690
I'm just going to specify the size of the data I want.

00:23:12.690 --> 00:23:21.360
So I here I calculated the number of bytes that are in my
image and I'm telling OpenCL create a buffer of that size.

00:23:21.359 --> 00:23:25.719
So once you've created these memory objects, you want
to put data in them, or maybe you want to get data back

00:23:25.720 --> 00:23:28.970
when you're done with them, and you
do that through explicit commands.

00:23:28.970 --> 00:23:33.220
So you need to call CLEnqueueReadImage
or CLEnqueueReadBuffer.

00:23:33.220 --> 00:23:39.410
And these commands are going to specify which memory object
you want to access, where you want to access in them,

00:23:39.410 --> 00:23:43.610
and provide a pointer which either
supplies the data you're going to write in,

00:23:43.609 --> 00:23:45.699
or gives you where the data is going to come back.

00:23:45.700 --> 00:23:51.210
So it's very important to mention here that these
commands can operate synchronously or asynchronously.

00:23:51.210 --> 00:23:53.920
So you'll notice there's this blocking option in it.

00:23:53.920 --> 00:23:57.910
When you call CLEnqueueRead, you are enqueuing a command.

00:23:57.910 --> 00:24:03.300
That command is going into the queue for the GPU
and at some point later it's going to finish.

00:24:03.299 --> 00:24:07.669
If you call EnqueueRead and your program returns
immediately and you're trying to access that data,

00:24:07.670 --> 00:24:13.710
there's no guarantee the read has finished, it may still be
sitting in the queue and you're going to have garbage data.

00:24:13.710 --> 00:24:18.509
So if you run these commands in a blocking
manner, your application will block that call

00:24:18.509 --> 00:24:22.589
until the read is completed at which
point your data is safe to use.

00:24:22.589 --> 00:24:25.119
So if we take look at how we read back data,

00:24:25.119 --> 00:24:29.269
we obviously need to allocate some space
on the host for storing the results.

00:24:29.269 --> 00:24:33.210
If we're reading from an image, we need to
specify where in the image do we want to read.

00:24:33.210 --> 00:24:38.730
So in this case we're setting the origin to 000, that is I
want to start in the upper left-hand corner of the image.

00:24:38.730 --> 00:24:42.980
And we're setting this area that we want
to read, the region, to the full image.

00:24:42.980 --> 00:24:48.410
We then call CLEnqueueReadImage, we specify the
origin and the region, and we get our data back.

00:24:48.410 --> 00:24:54.250
Similarly, if I want to read from a buffer, I'm going to
call CLEnqueueReadBuffer, and here since there's no idea

00:24:54.250 --> 00:25:01.190
of X and Y of sizes, I just specify the number of bytes
I want to read back, and OpenCL will read back the data.

00:25:01.190 --> 00:25:04.600
So again, it's important to note
that you're enqueuing a command here.

00:25:04.599 --> 00:25:11.259
If you want your application to block until that
command is done, you need to set blocking to be true.

00:25:11.259 --> 00:25:15.470
So now we've talked about how do you set up
OpenCL, how do you allocate your memory resources,

00:25:15.470 --> 00:25:19.220
let's talk about actually compiling and executing kernels.

00:25:19.220 --> 00:25:22.360
So for compiling and executing
kernels there are three steps.

00:25:22.359 --> 00:25:27.279
The first one is you need to create a program, and
a program is just taking an input of source code.

00:25:27.279 --> 00:25:30.470
So you're going to provide it with
some string of here's the actual code

00:25:30.470 --> 00:25:33.779
for my kernel, or you can provide a precompiled binary.

00:25:33.779 --> 00:25:35.910
This is analagous to a dynamic library.

00:25:35.910 --> 00:25:38.200
If you think about a program, it's
going to have a bunch of kernels in it,

00:25:38.200 --> 00:25:41.680
and it may have support functions for those kernels.

00:25:41.680 --> 00:25:44.279
Once you've created a program, you're going to compile it.

00:25:44.279 --> 00:25:49.410
You specify which devices do you want to compile on, do I
want to compile on the CPU and GPU or maybe I'm just going

00:25:49.410 --> 00:25:52.259
to run this kernel on the GPU, so I
should just compile it for the GPU.

00:25:52.259 --> 00:25:55.089
You're going to do the standard
things you do when you compile.

00:25:55.089 --> 00:25:58.279
You're going to pass in compiler flags
and you're going to check for problems.

00:25:58.279 --> 00:26:00.990
So if you're going to have a syntax error in your kernel,

00:26:00.990 --> 00:26:05.569
you're going to get an error back
when you go to compile the program.

00:26:05.569 --> 00:26:07.539
And finally you create a kernel.

00:26:07.539 --> 00:26:12.149
And as I said before, a kernel is an object
which stores arguments for execution.

00:26:12.150 --> 00:26:15.500
So this is how you're going to set the
argument so that you can go execute it.

00:26:15.500 --> 00:26:16.769
So let's take a look at this.

00:26:16.769 --> 00:26:19.579
Say the file here called kernels.cl.

00:26:19.579 --> 00:26:24.059
This file has one kernel in it called
average_images at the moment, and I want to load this.

00:26:24.059 --> 00:26:30.619
So I go ahead and load it into my C program and then I call
CLCreateProgramResource, just passing a pointer to that data

00:26:30.619 --> 00:26:36.000
and it'll create a program from that
string that I loaded from a file.

00:26:36.000 --> 00:26:37.930
Now how do I compile and create the kernel?

00:26:37.930 --> 00:26:40.289
Well CLBuildProgram.

00:26:40.289 --> 00:26:44.730
Take that program called build, and then
to create the kernel, CLCreateKernel,

00:26:44.730 --> 00:26:48.250
and I specify the name of the kernel
within the program I want.

00:26:48.250 --> 00:26:51.099
Now most people are going to discover
the first time they compile their kernel

00:26:51.099 --> 00:26:53.209
that there's a syntax error in the kernel.

00:26:53.210 --> 00:26:58.660
So what you want to do is you want to check the error
output from CLBuildProgram, and if you've got an error,

00:26:58.660 --> 00:27:03.090
you can call CLGetProgramBuildInfo to return the build log.

00:27:03.089 --> 00:27:06.220
And this will give you a detailed
log which will show you exactly

00:27:06.220 --> 00:27:11.370
where the compile encountered a syntax
error or an error in the kernel code.

00:27:11.369 --> 00:27:13.219
So how do you execute kernels?

00:27:13.220 --> 00:27:17.410
You need to set the kernel arguments and
then enqueue the kernel to be executed.

00:27:17.410 --> 00:27:20.620
So setting the kernel arguments, call CLSetKernelArg.

00:27:20.619 --> 00:27:24.729
Specify the kernel you want to
set and do this for each argument.

00:27:24.730 --> 00:27:28.660
To execute the kernel you're going
to call CLEnqueueNDRangeKernel.

00:27:28.660 --> 00:27:32.990
And it's here that you specify
the global and local dimensions.

00:27:32.990 --> 00:27:38.089
So remember I mentioned that the global and local
dimensions are how you tell OpenCL the parallelism

00:27:38.089 --> 00:27:40.980
in your kernel, and here's where you actually set that.

00:27:40.980 --> 00:27:46.309
So in this case, I'm telling you to execute this kernel,
with the global dimensions it's image width by image height,

00:27:46.309 --> 00:27:48.950
and I'm passing in null for the local dimensions.

00:27:48.950 --> 00:27:53.710
I'm telling OpenCL I don't care about the local
dimensions, pick something reasonable for me.

00:27:53.710 --> 00:27:58.750
And OpenCL will pick a local dimension
and run your kernel for you.

00:27:58.750 --> 00:28:03.599
So it's very important to emphasize here
your kernel is executed asynchronously.

00:28:03.599 --> 00:28:07.419
When you call CLEnqueueNDRangeKernel, nothing may happen.

00:28:07.420 --> 00:28:08.890
You've just enqueued your kernel.

00:28:08.890 --> 00:28:13.970
If someone else is using the GPU or if the GPU
is busy executing another one of your kernels.

00:28:13.970 --> 00:28:18.930
It's just going to sit there in the queue until the GPU
is ready, so you need to be very much aware of this.

00:28:18.930 --> 00:28:24.029
If you want to get data back from your kernel after
you've enqueued it, you can use a blocking read.

00:28:24.029 --> 00:28:29.809
This means you'll enqueue the kernel, you enqueue the
read, and your program will block until the read is done.

00:28:29.809 --> 00:28:33.639
Alternatively you can use events to
track the execution status of kernels

00:28:33.640 --> 00:28:35.560
if you need to do this at a more fine grained level.

00:28:35.559 --> 00:28:40.440
So let's talk a little about synchronizing between commands.

00:28:40.440 --> 00:28:45.330
So in OpenCL each command queue is in
order, that means that everything is going

00:28:45.329 --> 00:28:47.299
to execute in the order you stuck it in there.

00:28:47.299 --> 00:28:50.480
So if I enqueue a kernel and then I enqueue a read,

00:28:50.480 --> 00:28:54.620
I'm guaranteed that the kernel is going
to finish before the read executes.

00:28:54.619 --> 00:29:01.069
And this will behave just the way you expect as long as you
don't submit to the same command queue for multiple threads.

00:29:01.069 --> 00:29:07.099
However, and this is really important, you
must explicitly synchronize between queues.

00:29:07.099 --> 00:29:11.169
So if you have a CPU and a GPU, you've got
two command queues for submitting there

00:29:11.170 --> 00:29:15.250
and there are no implicit guarantees
about the ordering between them.

00:29:15.250 --> 00:29:20.250
You are responsible for using events to make
sure that things run in the order you expect.

00:29:20.250 --> 00:29:23.180
So if you've noticed in the commands
that I've shown you before,

00:29:23.180 --> 00:29:26.940
all of the CLEnqueue commands have
three parameters at the end.

00:29:26.940 --> 00:29:31.320
And the first two parameters are
specifying an event waitlist.

00:29:31.319 --> 00:29:37.279
So here you can say there are three events in this waitlist
passing a list of those events and OpenCL will make sure

00:29:37.279 --> 00:29:43.009
that that command waits until those three
commands are done before it executes.

00:29:43.009 --> 00:29:50.400
Similarly, you can get an event back from the command, and
this is what you use to put in waitlists for other commands.

00:29:50.400 --> 00:29:53.470
So let's take a look at an example of
where you need to pay attention to this.

00:29:53.470 --> 00:29:59.470
Say I have two kernels, kernel one and kernel two, I want
to run kernel one on the GPU and kernel two on the CPU.

00:29:59.470 --> 00:30:01.589
So I'm going to have two command queues here,

00:30:01.589 --> 00:30:05.899
so I don't have any implicit ordering,
I need to take care of ordering myself.

00:30:05.900 --> 00:30:10.180
Now in this case, the output from kernel
one is used as the input from kernel two.

00:30:10.180 --> 00:30:12.039
So this implies a dependency.

00:30:12.039 --> 00:30:18.299
I need to make sure that kernel one finishes before
kernel two starts or I'm going to get the wrong results.

00:30:18.299 --> 00:30:23.819
The output from kernel one won't be done and so
I'll have incomplete data before I run kernel two.

00:30:23.819 --> 00:30:28.250
So if we take a look at what's going on here, here
I've got a CPU and a GPU and I'm going to run one

00:30:28.250 --> 00:30:30.589
of the kernels in the CPU and the other on the GPU.

00:30:30.589 --> 00:30:34.119
So I can queue kernel one on the GPU,
it's sitting there in the command queue.

00:30:34.119 --> 00:30:37.979
I can queue kernel two on the CPU, it's
sitting there in the command queue.

00:30:37.980 --> 00:30:41.700
Now some time later the GPU is
available, you don't know when that is,

00:30:41.700 --> 00:30:45.380
other applications may be using
the GPU, you may using the GPU.

00:30:45.380 --> 00:30:47.360
But your kernel one is going to start running.

00:30:47.359 --> 00:30:50.429
And some time later, kernel two is going to start running.

00:30:50.430 --> 00:30:54.080
But again, you don't know when this is,
it depends on how busy these devices are.

00:30:54.079 --> 00:30:55.639
So we have a problem here.

00:30:55.640 --> 00:30:59.990
Kernel two started running before kernel
one was done, and this means that the data

00:30:59.990 --> 00:31:04.140
that we need from kernel one wasn't done in time.

00:31:04.140 --> 00:31:06.220
So if we take a look at what we do with events,

00:31:06.220 --> 00:31:09.990
here we're going to tell kernel two
to wait on an event from kernel one.

00:31:09.990 --> 00:31:14.700
So kernel one executes, and then kernel two is
not going to execute until we get that event back.

00:31:14.700 --> 00:31:20.880
And when that event comes back, then kernel two can go
ahead and execute and we get the correct behavior here.

00:31:20.880 --> 00:31:26.300
So if you have this scenario where you're submitting
things with data dependencies to multiple command queues,

00:31:26.299 --> 00:31:30.940
you're responsible for taking care
of those dependencies using events.

00:31:30.940 --> 00:31:33.259
So you can also use these events on the host.

00:31:33.259 --> 00:31:39.009
So if you want to block your program for a bunch of events,
you can call CLWaitForEvents and pass in an event waitlist.

00:31:39.009 --> 00:31:42.129
Your program will block until those events are done.

00:31:42.130 --> 00:31:46.940
You can enqueue a marker which is basically a way of
monitoring a particular position in a command queue

00:31:46.940 --> 00:31:49.490
and you can get an event back to use for other things.

00:31:49.490 --> 00:31:54.069
And you can enqueue a wait for events
which basically tells a command queue wait

00:31:54.069 --> 00:31:57.569
on everything after this for these particular events.

00:31:57.569 --> 00:31:59.639
You can also get information about events.

00:31:59.640 --> 00:32:04.070
You can query an event to find out what type
of command it was and what its status is.

00:32:04.069 --> 00:32:07.809
Has this command been queued, submitted,
is it running, or is it finished?

00:32:07.809 --> 00:32:14.690
And if you submit a command to OpenCL that has an error,
you can get that error code back through the event.

00:32:14.690 --> 00:32:16.860
Events are also used for profiling.

00:32:16.859 --> 00:32:21.250
You can get information on profiling from events which
allows you to time how long your kernels are taking

00:32:21.250 --> 00:32:24.710
and how long other operations are taking.

00:32:24.710 --> 00:32:29.019
All right, so now I've talked about setting up OpenCL,
let's look at an actual program that uses OpenCL.

00:32:29.019 --> 00:32:30.440
So here we go.

00:32:30.440 --> 00:32:36.480
Here's a simple program and this is basically going to
run through the code that I was showing you earlier.

00:32:36.480 --> 00:32:39.450
So the first thing we're going to do is
we're going to set up our CPU and GPU.

00:32:39.450 --> 00:32:46.380
And as you can see from here, we're going to call
CLGetDeviceIDs for the GPU and CLGetDeviceIDs for the CPU.

00:32:46.380 --> 00:32:50.140
We're then going to create a command
context which is going to use these devices,

00:32:50.140 --> 00:32:56.759
and then we're going to create a
command queue for each of the devices.

00:32:56.759 --> 00:33:02.029
And I apologize that some of this
appears to be commented out here.

00:33:02.029 --> 00:33:05.420
Always dangerous to change things
as you're actually doing the demo.

00:33:05.420 --> 00:33:10.840
So the next thing I'm going to do
after I've created my command queues,

00:33:10.839 --> 00:33:13.369
is I'm going to read in the kernels from a file.

00:33:13.369 --> 00:33:16.689
So I've got a file here which has
simply got my kernels in it.

00:33:16.690 --> 00:33:18.940
And you'll notice this file has two kernels in it.

00:33:18.940 --> 00:33:23.200
The first kernel is average buffers and
the second kernel is average images.

00:33:23.200 --> 00:33:27.630
So the average images kernel is the kernel I showed
you earlier in the demo, it simply gets the X

00:33:27.630 --> 00:33:32.540
and Y ID for that particular kernel instance,
and then does a four [phonetic] loop reading

00:33:32.539 --> 00:33:34.710
in the image data and writing the output.

00:33:34.710 --> 00:33:38.210
The buffer kernel version does
the same thing but uses buffers.

00:33:38.210 --> 00:33:43.529
But because buffers don't have any idea of X and
Y coordinates, it's going to have to calculate

00:33:43.529 --> 00:33:47.529
where in the buffer that pixel should be,
and it simply treats this buffer as an array,

00:33:47.529 --> 00:33:53.539
and then it writes out the output, treating
the output buffer as an array as well.

00:33:53.539 --> 00:34:00.740
So once we've done that we go on and we create our buffers
- excuse me, after we've loaded our kernel file here,

00:34:00.740 --> 00:34:04.640
we call CreateProgramResource, we
specify the source we want to load,

00:34:04.640 --> 00:34:07.310
and then we build the program for all of those devices.

00:34:07.309 --> 00:34:11.730
So we're going to call CLBuildProgram and
we specify the program we want to build.

00:34:11.730 --> 00:34:13.990
We're then going to set up for running the buffer kernel.

00:34:13.989 --> 00:34:17.439
So you've got two kernels you want to run here,
images and buffers, so the first thing we're going

00:34:17.440 --> 00:34:20.110
to do is allocate the memory objects for the buffers.

00:34:20.110 --> 00:34:22.760
And again this is just what I was talking about from before.

00:34:22.760 --> 00:34:26.480
We all CLCreateBuffer, specify the context we're using,

00:34:26.480 --> 00:34:30.400
specify the size of the buffer we want
to create, and we'll create the buffer.

00:34:30.400 --> 00:34:35.010
Then we want to write our initial data into the
buffer, so we're going to call CLEnqueueRightBuffer,

00:34:35.010 --> 00:34:39.280
specify the buffer we're writing into, specify
how much data we want to write into it,

00:34:39.280 --> 00:34:42.530
and then give it a pointer to the
data we want to write into it.

00:34:42.530 --> 00:34:47.580
Now because we're calling EnqueueBuffer with CL true
here, this command is going to block the execution

00:34:47.579 --> 00:34:50.849
of our program until we've got the data written.

00:34:50.849 --> 00:34:57.159
Create the output buffer, and we call CLCreateKernel, create
the kernel, and then we set the arguments for the kernel.

00:34:57.159 --> 00:35:00.099
So here, the first one, I'm going to set
the input buffer and the output buffer,

00:35:00.099 --> 00:35:02.980
and I'm going to specify the image
width and height for the kernel.

00:35:02.980 --> 00:35:05.840
And if you look at the kernel code here, that makes sense.

00:35:05.840 --> 00:35:12.150
The first argument to the kernel is the intput, the second
one is the output, and then we have the width and height.

00:35:12.150 --> 00:35:16.360
So once we've done that, we're going to set up
for using images, and for images we first need

00:35:16.360 --> 00:35:19.800
to specify the format, then call CreateImage2D.

00:35:19.800 --> 00:35:22.630
When we CreateImage2D we're going to specify that format.

00:35:22.630 --> 00:35:24.670
Then we're going to write data into the image.

00:35:24.670 --> 00:35:28.619
Here we specify the region and the
origin, and we call EnqueueRightImage,

00:35:28.619 --> 00:35:32.980
and we're specifying a pointer
to the data we want to write in.

00:35:32.980 --> 00:35:37.289
Create the output image, create the kernel,
and then we set the kernel arguments.

00:35:37.289 --> 00:35:39.460
Now you'll note there are only two kernel arguments here.

00:35:39.460 --> 00:35:45.650
Well if we look at our actual kernel for doing
images, it has only two inputs, input and output.

00:35:45.650 --> 00:35:46.680
But they're images.

00:35:46.679 --> 00:35:50.839
Now images are opaque data types, so we
don't need to specify the width and height,

00:35:50.840 --> 00:35:54.170
that's implicit when I call the ReadImageF.

00:35:55.369 --> 00:35:57.920
So once I've set up, I'm done with that.

00:35:57.920 --> 00:36:01.380
Now you'll note here's a simple
straight C implementation of this filter.

00:36:01.380 --> 00:36:05.090
The C implementation is basically the same
thing, it iterates over the whole image

00:36:05.090 --> 00:36:08.160
and then it iterates over a region around each pixel.

00:36:08.159 --> 00:36:13.649
And for each pixel it has to iterate over
all four of the components in my RGBA image.

00:36:13.650 --> 00:36:18.300
So what OpenCL is giving you here is it's
automatically paralyzing these outer two loops.

00:36:18.300 --> 00:36:21.650
So I'm specifying these as the global
domain, the image width and the image height,

00:36:21.650 --> 00:36:25.099
and so OpenCL is going to paralyze across those.

00:36:25.099 --> 00:36:28.480
And it's also giving us for free this inner loop.

00:36:28.480 --> 00:36:33.519
By using the vector type, our image is a float
four, I get this vector operation for free.

00:36:33.519 --> 00:36:39.230
Now when it comes time to execute the kernel,
I'm going to specify which queue do I want

00:36:39.230 --> 00:36:42.199
to run on, this demo can run on a CPU or GPU.

00:36:42.199 --> 00:36:47.629
If I'm running the image kernel I'm going to
EnqueueNDRangeKernel with my global dimensions of width

00:36:47.630 --> 00:36:52.230
and height, and then I'm going to read
back the image with a blocking read.

00:36:52.230 --> 00:36:57.329
If I'm executing the buffer kernel, I'm going to have
the same global dimensions, image width and image height,

00:36:57.329 --> 00:37:01.590
I'm going to enqueue my buffer kernel, and then
I'm going to read the results back from a buffer.

00:37:01.590 --> 00:37:07.039
So let's go ahead and see if this will
run with those slight modifications.

00:37:07.039 --> 00:37:12.070
So here we are running, this is the kernel you would expect,

00:37:12.070 --> 00:37:17.420
it's running OpenCL on the CPU using the
buffers and it's just average the image.

00:37:17.420 --> 00:37:22.019
We can run on the GPU and we can run
it with images, run it with buffers.

00:37:22.019 --> 00:37:26.659
And the point of this is just to show that here's
a simple algorithm that I trivially run in OpenCL.

00:37:26.659 --> 00:37:31.219
Now there's one more thing that I want to show
you here which is regards to errors in OpenCL.

00:37:31.219 --> 00:37:37.039
So if we go to a particular point in OpenCL,
here's our first time we're writing to an image.

00:37:37.039 --> 00:37:40.730
We're specifying the region as width x height x 1.

00:37:40.730 --> 00:37:44.909
Now it's a 2D image so a common programming
mistake is to specify zero there.

00:37:44.909 --> 00:37:48.920
So if I specify zero, I should expect to get an error back.

00:37:48.920 --> 00:37:57.809
So if I go ahead and run this, not surprisingly, I
get an error back, CLEnqueueRightImageFailed minus 30.

00:37:57.809 --> 00:38:06.059
So minus 30 is not the most useful thing to know, and if you
go and look at CLH, the error, minus 30 is invalid value.

00:38:06.059 --> 00:38:06.960
Well great.

00:38:06.960 --> 00:38:09.000
So which invalid value?

00:38:09.000 --> 00:38:14.369
If you look at EnqueueRightImage, I'm specifying
a queue, an input image true, an origin, a region,

00:38:14.369 --> 00:38:17.250
00 input data, and then some event stuff.

00:38:17.250 --> 00:38:20.590
So it's not entirely obvious to
me which thing I did wrong here.

00:38:20.590 --> 00:38:27.260
We've provided a very useful feature to you, and that is
this environment variable that you can set, CLLogErrors.

00:38:27.260 --> 00:38:30.160
You can specify it goes to standard out, standard error.

00:38:30.159 --> 00:38:39.789
If I enable this and I rerun my program,
the error I get now is much more useful,

00:38:39.789 --> 00:38:42.619
it provides specific information on what happened there.

00:38:42.619 --> 00:38:48.789
CLInvalidErrorValueEnqueRightImageFailedRegion2
must be 1 for 2D images.st-ylH

00:38:48.789 --> 00:38:49.759
Easy to find.

00:38:49.760 --> 00:38:51.790
So we really recommend that when you're debugging,

00:38:51.789 --> 00:38:56.360
that you enable this to get much more
detailed information about your API errors.

00:38:56.360 --> 00:38:59.700
So now a few hints on performance and debugging.

00:38:59.699 --> 00:39:02.969
And so this is sort of the high value part
here, this is where you're going to benefit

00:39:02.969 --> 00:39:05.179
from our experience and what's going on here.

00:39:05.179 --> 00:39:06.029
So performance.

00:39:06.030 --> 00:39:09.510
The big killer for you in performance is
overhead, and there are a bunch of places

00:39:09.510 --> 00:39:12.770
that overhead can come from, the
big one is compiling programs.

00:39:12.769 --> 00:39:17.809
Remember, when you go out there to build your program,
you're executing a full compiler to do that compilation.

00:39:17.809 --> 00:39:23.670
So you want to compile your program once early
on in your program and use it lots of times.

00:39:23.670 --> 00:39:26.210
Moving data to and from the GPU is very expensive.

00:39:26.210 --> 00:39:30.329
If you're not doing a lot of computation you're
going to spend most of your time moving data.

00:39:30.329 --> 00:39:33.889
So you want to go in there and you want to
make sure you move your data to the GPU,

00:39:33.889 --> 00:39:37.379
do a lot of work on it, and then move it off.

00:39:37.380 --> 00:39:40.590
Starting kernels is also expensive,
it takes a fair amount of time

00:39:40.590 --> 00:39:43.680
to enqueue a kernel, get it to the GPU, get the GPU going.

00:39:43.679 --> 00:39:46.730
So you want to make sure you're
doing a lot of work for each kernel.

00:39:46.730 --> 00:39:51.500
In particular what this means is that instead of having
three small kernels, you're going to get a big benefit

00:39:51.500 --> 00:39:54.050
from merging those into one larger kernel.

00:39:54.050 --> 00:39:58.789
And events on the GPU are expensive, try
to only use events when you need them.

00:39:58.789 --> 00:40:03.179
So when you've got these dependencies that you need
to enforce, use events, otherwise take advantage

00:40:03.179 --> 00:40:06.109
of the in order properties of the command kits.

00:40:06.110 --> 00:40:12.180
So for performance on kernels with regards to memory,
you want to have a large global work size you want more

00:40:12.179 --> 00:40:17.359
than a thousand work items, tens of thousands, hundreds
of thousands are fine, but you want lots of them.

00:40:17.360 --> 00:40:19.780
This is how you keep the GPU busy.

00:40:19.780 --> 00:40:25.190
If you're doing a lot of math, you can explicitly
tradeoff performance and precision using the half

00:40:25.190 --> 00:40:28.019
and native variants of various math functions.

00:40:28.019 --> 00:40:31.820
If you're running on a GPU today,
divergent code can be a real problem.

00:40:31.820 --> 00:40:35.620
And what this means is that if you have
a bunch of work items in a workgroup

00:40:35.619 --> 00:40:40.150
and they take different control flow paths,
the GPU performance is going to suffer.

00:40:40.150 --> 00:40:44.250
So you really want to try and make sure that they're
all executing the same control flow path together

00:40:44.250 --> 00:40:46.590
to get best performance.

00:40:46.590 --> 00:40:49.320
You also want to handle data reused through local memory.

00:40:49.320 --> 00:40:52.080
Caching on the GPUs today is not very advanced.

00:40:52.079 --> 00:40:55.400
You can get a big benefit from using
local memory if the GPU is supported.

00:40:55.400 --> 00:41:00.289
And when you're accessing memory from
your work items, you really want to try

00:41:00.289 --> 00:41:03.559
and access it sequentially across all of your work items.

00:41:03.559 --> 00:41:08.849
So what that means is if work item 1 is accessing
memory location 37, it would be really nice

00:41:08.849 --> 00:41:12.049
if work item 2 was accessing memory location 38.

00:41:12.050 --> 00:41:16.920
What this allows the GPU hardware to do is coalesce
these memory reads to get much higher bandwith.

00:41:16.920 --> 00:41:22.440
So for debugging, the biggest hint
I can give you is start on the CPU.

00:41:22.440 --> 00:41:26.220
You can change to run on the CPU just
by specifying you want a CPU device.

00:41:26.219 --> 00:41:28.959
You can use printf within our
kernels, you can use Shark,

00:41:28.960 --> 00:41:31.980
and you can look at the code that
you're running assembling GDB.

00:41:31.980 --> 00:41:35.110
These are big wins that you don't have on the GPU today.

00:41:35.110 --> 00:41:39.680
Also, be very careful about reading and writing
out of bounds from your memory objects on the GPUs.

00:41:39.679 --> 00:41:45.289
We don't have the level of memory protection that
you're used to, so this can cause bad results.

00:41:45.289 --> 00:41:49.789
In fact, if you have a kernel that's crashing, I'd
really recommend that you put in explicit address checks

00:41:49.789 --> 00:41:52.949
to make sure you're really reading
and writing where you think you are.

00:41:52.949 --> 00:41:55.139
And play nicely with other apps.

00:41:55.139 --> 00:41:56.879
GPUs today are not preemptive.

00:41:56.880 --> 00:42:00.809
If you run a kernel that's going to take several
seconds, that means some other application that wants

00:42:00.809 --> 00:42:03.699
to use the GPU is going to have to wait for you.

00:42:03.699 --> 00:42:08.019
When it comes time to debug, since you can't stop
a kernel in the middle and look at what's going on,

00:42:08.019 --> 00:42:11.559
use extra output buffers to track
your state as you go through.

00:42:11.559 --> 00:42:17.139
So if you have a five-step algorithm in your kernel, write
out the results after the first step so you can verify this.

00:42:17.139 --> 00:42:21.900
And as I showed in the demo, you can set this
context callback function which will provide easily

00:42:21.900 --> 00:42:27.030
through CLLogErrors, to get much more
detailed information about API error messages.

00:42:27.030 --> 00:42:31.640
So with that, I'm going to hand it over to Mon Ping
who's going to talk about the OpenCL kernel language.

00:42:31.639 --> 00:42:33.170
>> Thank you, David.

00:42:33.170 --> 00:42:37.840
Now that you are familiar with the OpenCL program model
let's quickly go over the language used to write kernels.

00:42:37.840 --> 00:42:43.559
As you see in the examples before, it's derived from C99 so
you can really just jump in and start writing your kernels.

00:42:43.559 --> 00:42:45.469
However, there are a few restrictions. There's no C99 standard header files, so you

00:42:45.469 --> 00:42:51.969
can't use C99 standard IO standard lib for example, there's
no function pointers, you can't do recursion,

00:42:51.969 --> 00:42:55.079
there's no variable length arrays or bit fields.

00:42:55.079 --> 00:43:00.960
We provide you with a whole set of parallel extensions, you can
work-items and workgroups, we provide you with vector type,

00:43:00.960 --> 00:43:04.840
and we can give you various synchronization primitives.

00:43:04.840 --> 00:43:09.760
We give you address space so you can efficiently
use your memory hierarchy on your machine.

00:43:09.760 --> 00:43:15.290
And we give you function to optimize image access
as well a wealth of other built-in functions.

00:43:16.340 --> 00:43:20.050
Now a kernel is a data parallel
function executed for each work item.

00:43:20.050 --> 00:43:24.880
To identify that function in the kernel, you just have
to put the kernel keyword in front of the function.

00:43:24.880 --> 00:43:27.650
Kernels can call other functions or other kernels.

00:43:27.650 --> 00:43:35.260
In this example, we describe the global ID, find the
corresponding space in the input array, square the value,

00:43:35.260 --> 00:43:38.150
and put a new corresponding place in the output array.

00:43:38.150 --> 00:43:41.869
And these can be executed in parallel.

00:43:41.869 --> 00:43:44.699
Now in your kernel you may want to know your current environment is,

00:43:44.699 --> 00:43:47.779
so we give you a variety of functions
to get this information.

00:43:47.780 --> 00:43:51.190
You may want to know for example what
work dimension you have, so you use get work dimension.

00:43:51.190 --> 00:43:55.389
You may want to know how many work items
are used for your global file space.

00:43:55.389 --> 00:43:57.949
You can use GetGlobalSize for that information.

00:43:57.949 --> 00:44:04.099
You may want to know how many workgroups
are used, so you can use GetNumberOfGroups,

00:44:04.099 --> 00:44:10.619
and you also may want to know what's my current
workgroup ID for this so you use this function.

00:44:11.699 --> 00:44:15.759
You may want to know for example how many work items
are in my workgroup, so then you use GetLocalSize

00:44:15.760 --> 00:44:22.480
for that information, and you can get
your local IDs and get local as well

00:44:22.480 --> 00:44:24.320
as the global ID which you've seen before.

00:44:24.320 --> 00:44:27.220
Now, the data types that are C99 standard data types,
you get char, uchar, bool, ulong for example.

00:44:27.219 --> 00:44:30.419
We also provide you a half type which
is used for 16-bit flows for storage.

00:44:30.420 --> 00:44:37.039
We give you image types for 2D and 3D
dimensions as well as a sampler type,

00:44:37.039 --> 00:44:41.679
and we give you the vector data types
for you altivec and [inaudible] programmers.

00:44:41.679 --> 00:44:44.710
Now the great thing about vector
types here are they're portable,

00:44:44.710 --> 00:44:52.699
so you don't have to write any more instrincis for altivec or fsc, you can just wait in
a portable language which you use in your different devices.

00:44:52.699 --> 00:44:56.460
We support vector links of 2, 4, 8, and 16.

00:44:56.460 --> 00:45:04.420
And for example, if you want an 8 element vector
of integers using an int type, they're endian safe, they're aligned at

00:45:04.420 --> 00:45:13.180
the vector length, and we give you a whole
set of vector operations and built-in functions.

00:45:13.179 --> 00:45:16.809
Plus, investigate some syntax vector operations.

00:45:16.809 --> 00:45:21.860
As I spread a value across all vector elements,
I can use this cast syntax.

00:45:21.860 --> 00:45:27.269
I can set each component of a vector using this syntax.

00:45:27.269 --> 00:45:32.389
You can easily manipulate vectors, so you can take
for example a high part of one vector and store it

00:45:32.389 --> 00:45:35.730
into a low part of a vector using this high/low syntax.

00:45:35.730 --> 00:45:40.019
And you can compose a vector out of other vectors
so assume I want to create an 8-element vector,

00:45:40.019 --> 00:45:46.349
I can close out of the 4-element vector, grab the first and second
element of the vector using this  01 syntax,

00:45:46.349 --> 00:45:52.079
and grab the art elements of
a 4-element vector to complete it.

00:45:52.079 --> 00:45:58.429
Vector operations work as you expect, so we do have
vector addition, you have all the vector components.

00:45:58.429 --> 00:46:05.819
Similarly, they work for built-in functions, so you can
call built-in functions on vector, using the component get set.

00:46:05.820 --> 00:46:10.860
Now, OpenCL provides a constant flow of workgroup
functions and they're special.

00:46:10.860 --> 00:46:14.220
All work items in a workgroup must
encounter a workgroup function

00:46:14.219 --> 00:46:17.029
and they must encounter the function
with the same arguments.

00:46:17.030 --> 00:46:22.190
Some examples of these functions are
things like barrier, barrier memory fences,

00:46:22.190 --> 00:46:27.820
async workgroup copy which is just a copy from
global to local memory and vice versa.

00:46:27.820 --> 00:46:29.809
Here's an example of an illegal use.

00:46:29.809 --> 00:46:35.070
In this program we're grabbing global ID, it's less
than value, we encounter the memory fence,

00:46:35.070 --> 00:46:37.390
this is illegal because not all work-items encounter the
memory fence.

00:46:37.389 --> 00:46:42.889
Let's take another example where we're doing a barrier.

00:46:42.889 --> 00:46:46.309
In this program we're just moving
a curve averaging three values.

00:46:46.309 --> 00:46:55.250
Now when it does the average it stores into
a private variable called temp and then it has

00:46:55.250 --> 00:46:58.269
to hit the barrier, and then it outputs the result.

00:46:58.269 --> 00:47:04.789
And the reason why we need the barrier in this case
is we're using the same input and output array.

00:47:04.789 --> 00:47:12.289
So let's assume that this program is broken in two workgroups
of three items each and the current state is as so.

00:47:12.289 --> 00:47:17.150
So Item 2 will do the addition while the rest of
the items are waiting for GetGlobalID.

00:47:17.150 --> 00:47:22.700
Notice that work items are not
required to be working in lock step.

00:47:22.699 --> 00:47:28.019
So we do the addition, we store into the private temporary
variable then we move down to hit the barrier.

00:47:28.019 --> 00:47:32.610
At this point the other workgroup will hit the barrier,
we continue, it has to stop.

00:47:32.610 --> 00:47:36.170
And then items 0 and 1 can start.

00:47:36.170 --> 00:47:41.680
Item 1 starts to do the addition, it does the same thing,
sorts the value, then moves down.

00:47:41.679 --> 00:47:45.839
Now remember that barriers only synchronize
within a workgroup, not across workgroups.

00:47:45.840 --> 00:47:51.170
So at this point item 0 to 2 can
continue, hit the store, and store the value.

00:47:51.170 --> 00:47:56.230
Now this is going to be problem when item 3 starts to
execute because it's getting a value from the old array

00:47:56.230 --> 00:47:59.150
and a newly produced value, giving you an incorrect result.

00:47:59.150 --> 00:48:03.840
So this is just to emphasize the fact that barriers
work within a workgroup and they work fine,

00:48:03.840 --> 00:48:07.230
but if you try to synchronize
across a workgroup, that's not possible.

00:48:07.230 --> 00:48:13.179
I don't want to give you the impression that all
synchronization primitives are workgroup functions.

00:48:13.179 --> 00:48:16.529
You have atomic functions to
access global and local memory,

00:48:16.530 --> 00:48:19.950
for the general set of add, sub, exchange, et cetera.

00:48:19.949 --> 00:48:23.419
They work on 32-bit and 64-bit integers and
you can write to global or local memory.

00:48:23.420 --> 00:48:29.389
Remember these are extensions, you have to check your device
to see if it supports the use of CLGetDeviceInfo.

00:48:29.389 --> 00:48:34.659
Also note that can't you use these to do a
spin lock because a workgroup needs

00:48:34.659 --> 00:48:41.369
to execute completely before anything
else can be swapped in.

00:48:43.130 --> 00:48:47.130
You've seen this picture before,
this shows the memory hierarchy.

00:48:47.130 --> 00:48:51.309
And we map this memory hierarchy
to using address spaces in language.

00:48:51.309 --> 00:48:56.920
So for private memory you use the
private attribute and find out the type.

00:48:56.920 --> 00:49:02.440
Similarly for local memory you use local, for
constant and global you use these attributes.

00:49:02.440 --> 00:49:09.869
Now Address Spaces can be a little bit tricky
so let's go over quickly some rules for them.

00:49:09.869 --> 00:49:13.029
Kernel pointer arguments must
use global, local, or constant.

00:49:13.030 --> 00:49:15.560
So let's take for example distance.

00:49:15.559 --> 00:49:20.750
Here you were passing two global buffers,
stars is pointing to a local buffer,

00:49:20.750 --> 00:49:26.269
and local search point into a local buffer
which will be sure within the workgroup.

00:49:26.269 --> 00:49:29.389
Illegal use is to pass the private memory to a kernel.

00:49:29.389 --> 00:49:36.559
This makes sense because how can you pass something
that's owned by the work item from outside of it?

00:49:36.559 --> 00:49:40.340
The default address space for
arguments of a local variable is private.

00:49:40.340 --> 00:49:48.570
So you see in our prefix element when we don't put a qualifier on it
like temp here, this is a private work item variable.

00:49:48.570 --> 00:49:55.140
Image 2D and image 3D are always
in the global address space.

00:49:55.139 --> 00:50:02.259
Now program global variables, those are variables outside of a
kernel or function, must be in the constant address space.

00:50:02.260 --> 00:50:06.650
So you [inaudible], make it a constant, and
give it to all your work items.

00:50:06.650 --> 00:50:13.180
But you cannot create something like a global
variable called time to give access to all your items.

00:50:13.179 --> 00:50:16.609
Casting between different address spaces is undefined.

00:50:16.610 --> 00:50:21.710
So remember when youcasting between adresss spaces make
sure that you keep the same address space.

00:50:21.710 --> 00:50:28.840
So for example, I'm casting a float4 to global
memory and casting to a float* point into global memory.

00:50:28.840 --> 00:50:33.280
However, if I forget to put the attributes,
remember that the default address space is private,

00:50:33.280 --> 00:50:36.640
so this is doing a cast from global memory to private memory

00:50:36.639 --> 00:50:40.500
since address spaces are disjoined
this will lead to undefined behavior.

00:50:40.500 --> 00:50:51.110
Now you've seen this kernel before in David's demo,
we're just doing the average using global buffers.

00:50:51.110 --> 00:50:53.800
David also mentioned local memory may be
much faster on certain devices.

00:50:53.800 --> 00:51:00.880
For this program it might be much more efficient to
grab a tile, copy it to local memory, manipulate it,

00:51:00.880 --> 00:51:04.390
and grab another tile and do the same thing.

00:51:04.389 --> 00:51:13.480
To do this copy, you should use async workgroup copy, which
can copy from global to local as well local back to global.

00:51:13.480 --> 00:51:16.829
Let's see how our example changes when we do this.

00:51:16.829 --> 00:51:22.559
So we specify the async workgroup copy where we specify the
destination address and the local memory from the source,

00:51:22.559 --> 00:51:25.610
from global memory, and then what size you want to copy.

00:51:25.610 --> 00:51:30.250
It returns back an event because
this function is asynchronous.

00:51:30.250 --> 00:51:38.559
So you have to wait for the event to finish using
the workgroup event in the kernel then continue.

00:51:38.559 --> 00:51:42.889
Now images allow you to set image functions which
allows you to read and write the image.

00:51:42.889 --> 00:51:47.469
Note that we support getting the value back
in the format that's most useful for you.

00:51:47.469 --> 00:51:52.659
So for example, if your data is -- you want it back
in float format, you can use RightImageF.

00:51:52.659 --> 00:51:54.909
Similarly you can do integer or unsigned integer.

00:51:54.909 --> 00:51:58.399
You get information about your image
such as height, width, and depth.

00:51:58.400 --> 00:52:04.260
You can also get things like channel data
type as well as the channel order.

00:52:04.260 --> 00:52:06.580
Now conversions.

00:52:06.579 --> 00:52:13.449
Scalar and pointer versions work just like in C99,
however, there is no implicit conversions for vectors.

00:52:13.449 --> 00:52:18.079
Similarly you cannot cast for vector
types primarily for the reason this fact

00:52:18.079 --> 00:52:22.090
that there's different semantics for vectors traditionally.

00:52:22.090 --> 00:52:23.870
Casts other potential problems.

00:52:23.869 --> 00:52:27.150
Let's say I want to do a round float to the nearest integer.

00:52:27.150 --> 00:52:29.590
I might write something similar like this.

00:52:29.590 --> 00:52:37.320
Unfortunately this is wrong for values very close
to zero, and it's wrong for negative values.

00:52:37.320 --> 00:52:41.680
And you also want to make sure that
you get this converted into code

00:52:41.679 --> 00:52:44.769
which to instruction which is available on the hardware.

00:52:44.769 --> 00:52:50.980
To fix these problems, OpenCL provides you an explicit
conversion operator that lets you convert to a destination type

00:52:50.980 --> 00:52:53.800
and specify the saturation and the rounding mode.

00:52:53.800 --> 00:52:58.750
These work for both the scalar and vector
types, and there's no ambiguity with them.

00:52:58.750 --> 00:53:04.440
Let's say for example I'm converting
a vector four float values to a vector

00:53:04.440 --> 00:53:09.280
of four unsigned integers, or characters in this case.

00:53:09.280 --> 00:53:15.440
So I take the first value, -5, it's outside
the range it gets clamped down to zero.

00:53:15.440 --> 00:53:22.200
Similarly, I have 254.5, since I asked for it to round
to the nearest even, it would be rounded down to 254.

00:53:22.199 --> 00:53:29.219
254.6 would round to 255, and values
are arranged to be clamped down to 255.

00:53:29.219 --> 00:53:34.099
Notice that the left one is C99, this
would be undefined because of the range.

00:53:34.099 --> 00:53:39.960
We also provide you reinterpret data function called
as_typen this allows you to interpret the bits

00:53:39.960 --> 00:53:43.820
of one type to another, the types must be the same size.

00:53:43.820 --> 00:53:50.640
Plus we can do a select with this, but note that you should probably
use the OpenCL select built-in function for this.

00:53:50.639 --> 00:53:57.879
So we're getting two values, we can do a vector
comparison, but what happens is if the component is true,

00:53:57.880 --> 00:54:01.269
it'll set all to all ones, otherwise it will set it to all to zeros.

00:54:01.269 --> 00:54:11.280
We can then move down to taking the floating point
value we're interested in, converting to its bits,

00:54:11.280 --> 00:54:14.780
do a bitwise and, and then convert it back into float
using the f float.

00:54:19.000 --> 00:54:23.369
So OpenCL provides you a wealth of
built-in functions, and these are just a small set

00:54:23.369 --> 00:54:26.829
of what's provided, it's a very rich set.

00:54:26.829 --> 00:54:31.440
You have access to math function, geometry
function, integer function, et cetera.

00:54:31.440 --> 00:54:37.400
And they're well defined, for example, for the
floating point stuff, we support IEEE 754 compatible rounding modes.

00:54:37.400 --> 00:54:45.139
We define maximum errors of these math functions of these
math functions, and support both scalar and vector types.

00:54:45.139 --> 00:54:49.460
And they're efficient, they're tuned for the hardware
they're executed on so that it's good to use them.

00:54:49.460 --> 00:54:53.940
Now let's take a closer look at math functions.

00:54:53.940 --> 00:54:57.139
They're well defined, we handle the ambiguous C99 library cases.

00:54:57.139 --> 00:55:05.929
As David mentioned before, they come in three flavors
for example, full precision, let's take log gives you 3ulp, half precision gives you 11 bits

00:55:05.929 --> 00:55:11.419
of accuracy, but they're faster, and native precision
which may map to instruction on the hardware,

00:55:11.420 --> 00:55:16.400
so they're fastest but since it is on the hardware,
the actual precision is implementation defined,

00:55:16.400 --> 00:55:20.320
and some other features that are supported.

00:55:20.320 --> 00:55:25.019
We provide preprocessing directives as defined
by C99, and optional extensions

00:55:25.019 --> 00:55:29.259
such as double, rounding modes, atomic functions,
writing 3D image objects, et cetera.

00:55:29.260 --> 00:55:36.160
Now just to summarize, OpenCL provides
you a high performance framework.

00:55:36.159 --> 00:55:40.779
This idea for your data parallel
computationally intensive algorithms,

00:55:40.780 --> 00:55:47.470
and we give you access to all the compute resources on
the machine so you can make your algorithm run fast.

00:55:47.469 --> 00:55:53.039
It's portable across all mobile devices, and we provide
you a well defined computational in memory model,

00:55:53.039 --> 00:55:57.849
this way you can easily break down your problems into
global local dimensions, and we give you address spaces

00:55:57.849 --> 00:56:01.920
so you can use your memory hierarchy
as efficiently as possible.

00:56:01.920 --> 00:56:05.380
And we give you a good efficient parallel program language.

00:56:05.380 --> 00:56:11.070
It's basic C99 that you're all familiar
with, and we give you task and data parallelism

00:56:11.070 --> 00:56:15.400
as well a very rich set of built-in functions.

00:56:15.400 --> 00:56:18.670
So for more information please
contact our evangelist Allan Schaffer.