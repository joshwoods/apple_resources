WEBVTT

00:00:12.990 --> 00:00:14.410
>> Good morning.

00:00:14.410 --> 00:00:17.010
I'm Phil Kerly and I work for Intel.

00:00:17.010 --> 00:00:23.960
I am a Principal Engineer, and I work in
the Software and Services Group at Intel,

00:00:23.960 --> 00:00:28.060
and I've been doing performance analysis
work for probably about a decade now.

00:00:28.059 --> 00:00:36.640
And when we had the opportunity to present a session
here at WWDC, I thought about what would be something

00:00:36.640 --> 00:00:43.759
that we could add to your experience here that
Apple doesn't already have all the expertise for,

00:00:43.759 --> 00:00:46.299
and one of them happens to be related to the processor.

00:00:46.299 --> 00:00:48.939
And so I thought, well that's something we should stick to.

00:00:48.939 --> 00:00:51.299
We should stick to what we know best.

00:00:51.299 --> 00:00:53.659
We'll talk about the architecture a little bit.

00:00:53.659 --> 00:00:58.149
And then I thought about some of the experiences
that I've had working with various ISVs

00:00:58.149 --> 00:01:05.349
across the different third party ISVs
both on Mac as well as on Windows.

00:01:05.349 --> 00:01:11.799
And one of the common threads that I saw was
that a lot of people do performance analysis,

00:01:11.799 --> 00:01:18.750
but they tend to focus very much on kind of a time-based
sampling hotspot analysis, and they never really quite get

00:01:18.750 --> 00:01:24.840
to using the processor events to help them
analyze their application and their performance.

00:01:24.840 --> 00:01:29.689
And so I thought, well that might be something I can
talk about and maybe impart a little bit of knowledge

00:01:29.689 --> 00:01:32.569
about how to use those, that combined with Shark.

00:01:32.569 --> 00:01:36.369
So today, I'm going to talk about
the processor performance events.

00:01:36.370 --> 00:01:40.730
How you can use them to investigate the
architecture because a lot of times,

00:01:40.730 --> 00:01:44.930
it's just interesting to write small little
kernels to go fiddle with the hardware

00:01:44.930 --> 00:01:47.290
and see exactly what's happening under the covers.

00:01:47.290 --> 00:01:50.530
And the more that you know about the architecture,

00:01:50.530 --> 00:01:56.540
the better position you'll be enabled
in tuning your particular applications.

00:01:56.540 --> 00:02:05.960
Now, using the performance events is
a huge, probably multi-day discussion.

00:02:05.959 --> 00:02:11.409
And so, what I really want to focus on, since I think
that most people understand time-based sampling,

00:02:11.409 --> 00:02:17.729
is really just kind of a basic understanding
of how the performance of that counters work.

00:02:17.729 --> 00:02:24.269
And then just maybe leave you with a little bit of
strategy and some techniques to help improve your code

00:02:24.270 --> 00:02:26.320
and get started using the performance events.

00:02:26.319 --> 00:02:32.840
And as you get better accustomed to what they can do for
you, then you can dig a little bit deeper on your own.

00:02:32.840 --> 00:02:33.400
What it's not.

00:02:33.400 --> 00:02:36.099
It's not a tutorial on programming the performance events.

00:02:36.099 --> 00:02:42.639
I mean that's really a tools, application type
domain and for most of us we're not interested

00:02:42.639 --> 00:02:47.439
in actually writing the tools, we just want to use
the tools to their advantage and leave the programming

00:02:47.439 --> 00:02:52.560
of the events to people like the Shark engineers.

00:02:55.439 --> 00:03:00.659
Now, I'm going to talk about performance
events on the hardware,

00:03:00.659 --> 00:03:04.139
but I don't want to leave the impression
that that's where you should start.

00:03:04.139 --> 00:03:08.669
Clearly, performance analysis has to be a top-down approach.

00:03:08.669 --> 00:03:10.849
You have to start with the system level.

00:03:10.849 --> 00:03:18.310
You know, you want to focus on your disk
IO, your memory, processor utilization.

00:03:18.310 --> 00:03:22.599
If you're spending most of your time waiting on
network IO, there's no sense to even bother worrying

00:03:22.599 --> 00:03:26.060
about the processor events if the processor is mostly idle.

00:03:26.060 --> 00:03:33.110
And then of course we have multi-core
with the new Mac Pro 16-way virtual cores.

00:03:33.110 --> 00:03:38.690
Want to make sure that you're taking full advantage of
those cores, and that you've implemented you're parallelism.

00:03:38.689 --> 00:03:43.900
That you're getting good utilization across
all of those cores which means resolving any

00:03:43.900 --> 00:03:48.900
of your lock-in [Phonetic] tension that you might be having
and making sure that you're using the appropriate APIs

00:03:48.900 --> 00:03:51.310
that you need to use to get the
best performance for threading

00:03:51.310 --> 00:03:57.270
whether that be POSIX threads or Grand Central Dispatch.

00:03:57.270 --> 00:04:03.810
And then, once you've got that level of tuning down, then
you can really talk about the microarchitecture level,

00:04:03.810 --> 00:04:12.430
and we're really focusing on where are the processor stalls,
what is the branch prediction happening, what does the data

00:04:12.430 --> 00:04:16.350
and code alignment look like and focus on the architecture.

00:04:16.350 --> 00:04:18.700
So, that's what I'm going to talk about.

00:04:18.699 --> 00:04:21.000
Not that I'm ignoring the upper part because I'm assuming

00:04:21.000 --> 00:04:23.939
that that's already been done, but
focus on the architecture itself.

00:04:23.939 --> 00:04:30.269
So, what is a processor event?

00:04:30.269 --> 00:04:39.009
It's, there are counters in the processor that have been
included so that they detect microarchitectural conditions.

00:04:39.009 --> 00:04:41.920
There's two types of conditions
that you're really looking for.

00:04:41.920 --> 00:04:46.930
There is occurrences which is the
number of times the event has occurred.

00:04:48.029 --> 00:04:51.559
The number of times can happen more than once per cycle.

00:04:51.560 --> 00:04:59.120
For example, the number of instructions retired, we can
retire up to 4 micro OPS or 4 instructions per cycle.

00:04:59.120 --> 00:05:03.569
So there is the case where the number of
occurrences can actually happen more than once

00:05:03.569 --> 00:05:06.339
in a cycle, and then we also focus on duration.

00:05:06.339 --> 00:05:12.310
So some things like, you know, when is
the integer divide execution unit busy

00:05:12.310 --> 00:05:16.060
or just things like unhalted clocktick cycles.

00:05:16.060 --> 00:05:21.720
Both of these can be constrained on conditions

00:05:21.720 --> 00:05:28.900
So for example you can get the number of instructions
retired in your application but you can also,

00:05:28.899 --> 00:05:34.129
using the right tab conditions, find out
when you're not retiring any instructions.

00:05:34.129 --> 00:05:39.730
And why that might be useful is because, if you're retiring
a lot of instructions in a particular piece of code

00:05:39.730 --> 00:05:45.090
that you've got and it's doing well, you know,
sometimes you want to focus on what's not doing so well

00:05:45.089 --> 00:05:49.359
and where you're not retiring instructions
in that, in those cycles.

00:05:49.360 --> 00:05:54.840
So, conditions help constrain the events
and expand the number of possible events

00:05:54.839 --> 00:06:02.839
that you can actually collect beyond the couple of
hundred events that are already provided quite a bit.

00:06:02.839 --> 00:06:06.859
So it's a very powerful tool to
peer into the microarchitecture.

00:06:06.860 --> 00:06:14.040
So here's an example of what are some of those
event conditions we talked a little bit about,

00:06:14.040 --> 00:06:21.170
but we can constrain the number events
to only be in ring 3 which is user mode.

00:06:21.170 --> 00:06:27.290
You could constrain it to only be in ring 0, the OS
mode, or you can constrain them so that you get both,

00:06:27.290 --> 00:06:30.260
so that you get events in both ring 0 and ring 3.

00:06:30.259 --> 00:06:39.199
You could also trigger on these events, so you can cause
PMI or performance monitoring event interrupts to occur

00:06:39.199 --> 00:06:42.259
after so many number of instructions that have been retired

00:06:42.259 --> 00:06:46.430
or so many unhalted clockticks
that have retired and occurred.

00:06:46.430 --> 00:06:52.920
And then you can, as I gave the example before, you
can trigger when there is no instructions retired

00:06:52.920 --> 00:06:55.500
or you can trigger when the execution units are idle.

00:06:55.500 --> 00:07:01.740
So if the execution units are idle, there either is no
work for them to be done or they are actually be installed.

00:07:01.740 --> 00:07:07.319
Now the last two actually are ones that,
well, you can actually configure them.

00:07:07.319 --> 00:07:11.069
Currently today in Shark, the right bits to be able

00:07:11.069 --> 00:07:16.290
to actually collect those events
aren't quite exposed to the end user.

00:07:16.290 --> 00:07:22.350
So there is some limitation on Shark but certainly the full
performance event counting capability that Intel provides

00:07:22.350 --> 00:07:27.670
in the hardware would support those 2 events.

00:07:27.670 --> 00:07:33.300
So we have 2 types of events, just to give some basics.

00:07:33.300 --> 00:07:39.650
There are the model-specific events, and these are the vast
majority of the performance event counters that we have.

00:07:39.649 --> 00:07:44.969
They're very much tied to the hardware
and the architecture of the processor.

00:07:44.970 --> 00:07:51.690
The reason is because the underlying architecture changes,
and we have the tick-tock approach to processor development.

00:07:51.689 --> 00:07:58.019
And so every other year, we're actually modifying the
architecture and so those type of performance events

00:07:58.019 --> 00:08:01.889
and things that we think are of interest
to general programmers are the ones

00:08:01.889 --> 00:08:06.509
that we think they should be aware
of, we add those to the processor.

00:08:06.509 --> 00:08:13.120
But we've also been aware that there are a number of events
that are just common across all of our architectures.

00:08:13.120 --> 00:08:19.970
So we've started implementing what we call architectural
events, which are events that we basically say are going

00:08:19.970 --> 00:08:26.150
to be in every processor moving forward and that
they will be consistent with that architecture.

00:08:26.149 --> 00:08:29.629
And the number of events is evolving,
and today we're up to 3.

00:08:29.629 --> 00:08:32.769
It's not a whole lot but they're
probably the most important 3.

00:08:32.769 --> 00:08:34.059
We use them a lot.

00:08:34.059 --> 00:08:41.209
It happens to be unhalted clocktick cycles, reference
cycles, unhalted reference cycles, and instructions retired.

00:08:41.210 --> 00:08:47.240
And then we break down the counters into 2 types.

00:08:47.240 --> 00:08:53.750
There are the fixed counters which are the
architectural events, and the programmable counters.

00:08:53.750 --> 00:08:56.480
So the fixed counters are ones
that you have no control over.

00:08:56.480 --> 00:08:59.909
They are fixed count, fixed function.

00:08:59.909 --> 00:09:02.500
They're providing the architectural events.

00:09:02.500 --> 00:09:06.480
The programmable counters, again, are the vast majority

00:09:06.480 --> 00:09:10.340
of what you can actually count in
and they're tied to the architecture.

00:09:10.340 --> 00:09:17.930
So, one thing about the fixed counters is that they're not
supported in all of our platforms as you look backwards.

00:09:17.929 --> 00:09:21.939
It's only a forward looking architectural
improvement going forward.

00:09:21.940 --> 00:09:26.640
And with the programmable counters, not all
events can be counted on all of the counters.

00:09:26.639 --> 00:09:31.319
So if you go into Shark and you actually scroll
through the list for a particular counter,

00:09:31.320 --> 00:09:36.030
be aware that if you're looking for a particular
event that you know the processor should support.

00:09:36.029 --> 00:09:41.309
If it's not in that particular counter list, you should
go check the other ones because they're not all supported

00:09:41.309 --> 00:09:45.059
in both sets of the programmable counters.

00:09:45.059 --> 00:09:56.369
And because internally we have been using this
performance events to help improve a lot of software

00:09:56.370 --> 00:10:00.429
that we engage with, a lot of ISVs, we
started increasing the number of counter.

00:10:00.429 --> 00:10:06.879
So today on Nehalem, you get 4 of these programmable
counters versus 2 on previous generations.

00:10:06.879 --> 00:10:13.259
And then there are basically 2 basic modes of operation.

00:10:13.259 --> 00:10:16.960
There is the counting mode and the triggering mode.

00:10:16.960 --> 00:10:23.700
>> The counting mode is basically a query where
you decide based on however you want to count it,

00:10:23.700 --> 00:10:26.640
you just go and query the processor for the event counts.

00:10:26.639 --> 00:10:33.689
So you clear it out, you go and you just read the
event every so often or at the end of your application

00:10:33.690 --> 00:10:37.180
when it's run-- after it's finished
running, and then that's what you get.

00:10:37.179 --> 00:10:42.399
Now, you can't correlate that to an instruction
pointer because, you know, it's arbitrary in terms

00:10:42.399 --> 00:10:44.720
of when you're actually querying or pulling it.

00:10:44.720 --> 00:10:48.200
There's nothing related to the actual instruction pointer.

00:10:48.200 --> 00:10:56.220
The triggering mode is actually an event, the PMI,
a performance monitoring event that actually occurs,

00:10:56.220 --> 00:11:03.960
and this actually signals the processor that says, hey,
based on your conditions I've reached that number of events

00:11:03.960 --> 00:11:10.170
and therefore I'm telling you to-- that this has occurred
and now you have the opportunity to actually go--

00:11:10.169 --> 00:11:14.610
or the tool has an opportunity to
go and actually query the event.

00:11:14.610 --> 00:11:18.690
And this, you can actually correlate to the
instruction pointer because when the event occurs,

00:11:18.690 --> 00:11:22.020
it's usually somewhere near where that
event actually happened in your code.

00:11:22.019 --> 00:11:25.509
In the interrupt, you can go look
to see what process happened

00:11:25.509 --> 00:11:27.559
to be running and go look up the instruction pointer.

00:11:27.559 --> 00:11:34.439
So it's-- there's definitely a high correlation between
the event occurring and the location in your code.

00:11:34.440 --> 00:11:40.910
So this really enables 2 different sampling methodologies.

00:11:40.909 --> 00:11:41.929
We use both.

00:11:41.929 --> 00:11:46.120
The primary one that I use for client type applications

00:11:46.120 --> 00:11:50.049
which have different phases of
execution tend to be triggering mode.

00:11:50.049 --> 00:11:56.029
But if you have an extremely steady state type application
where you're doing basically the same processing over time

00:11:56.029 --> 00:12:01.559
and you can just query it on a periodic
level, we've also used that one as well.

00:12:01.559 --> 00:12:07.509
But the triggering mode is-- tends to be
the large majority of what I personally use.

00:12:11.409 --> 00:12:18.829
So let's talk a little bit about statistical sampling,
first, correlating the triggered samples to code.

00:12:18.830 --> 00:12:25.070
Because you're actually getting the interrupt, you have
to remember that if you're collecting and interrupting

00:12:25.070 --> 00:12:28.690
after so many instructions, it is a statistical sampling.

00:12:28.690 --> 00:12:36.910
So if you're interrupting every 300 million events,
instructions retired, or whatever it might be,

00:12:36.909 --> 00:12:48.339
then it depends on your application if you have some
kind of harmonic in the application such that, you know,

00:12:48.340 --> 00:12:58.210
you run you know 2,999,000 events in one area code and
the one extra one that causes the event happens to occur

00:12:58.210 --> 00:13:04.800
in some other location, you know, you could reach a point
where you're actually pointing at the wrong point in code.

00:13:04.799 --> 00:13:09.519
So you have to be very careful about how you
read the statistical sampling and make sure

00:13:09.519 --> 00:13:13.319
that you're getting the right picture and
we'll talk a little bit more about that.

00:13:13.320 --> 00:13:22.550
And that really leads into the calibration which
is you want to interrupt just the right amount.

00:13:22.549 --> 00:13:24.500
It's kind of the Goldilocks.

00:13:24.500 --> 00:13:29.350
You don't want too many, too much because if
you're interrupting, that's causing your program

00:13:29.350 --> 00:13:33.850
to behave differently than the
way that you originally intended.

00:13:33.850 --> 00:13:38.759
If you don't do it too-- if you do it too little, then
you don't get quite the resolution into your application

00:13:38.759 --> 00:13:43.679
that you need to know exactly where that
particular instruction pointer is that's correlating

00:13:43.679 --> 00:13:44.989
with that particular event.

00:13:44.990 --> 00:13:49.029
I kind of liken this to a camera zoom.

00:13:49.029 --> 00:13:55.839
If you take a full big picture, the resolution
and the detail that you see is not as good.

00:13:55.840 --> 00:14:03.450
But if you drill down too closely, things become fuzzy
and out of focus and you start focusing in on one location

00:14:03.450 --> 00:14:06.170
but it's not necessarily the right location.

00:14:06.169 --> 00:14:09.839
So it's a matter of getting the right focus for calibration.

00:14:09.840 --> 00:14:12.730
I'll talk a little bit more about
that when I bring up chart.

00:14:12.730 --> 00:14:20.740
And then there is the potential for a little bit of skidding
your applications when you're doing performance events.

00:14:20.740 --> 00:14:24.399
There is a delay between the actual interrupt.

00:14:24.399 --> 00:14:29.500
By the time you actually get processed in, the event
gets signaled over to the performance counters,

00:14:29.500 --> 00:14:36.230
the interrupt happens, you go query the process and find
out what the instruction pointer was for that interrupt.

00:14:36.230 --> 00:14:37.500
There's a little bit of delay.

00:14:37.500 --> 00:14:42.889
Now fortunately, this is mostly hidden by
the reorder buffer that's in the processor.

00:14:42.889 --> 00:14:47.779
But there are some events that it's particularly
more hazardous than it is for others.

00:14:47.779 --> 00:14:50.759
For example, branch instructions.

00:14:50.759 --> 00:14:58.700
If you actually have a branch instruction for which you're
collecting the branch miss event for and the event happens

00:14:58.700 --> 00:15:04.650
and then it gets signaled, and then you go to
read the IP, you're already into your target

00:15:04.649 --> 00:15:08.419
so you don't always know what the
exact branch was correlated back

00:15:08.419 --> 00:15:12.709
to the instruction pointer, and that's what skid does.

00:15:12.710 --> 00:15:17.460
The Intel processor does support what's
called precise event-based sampling

00:15:17.460 --> 00:15:20.120
which has tried to improve on this process.

00:15:20.120 --> 00:15:26.200
And in fact, if you have precise event-based
sampling capability, it will actually guarantee

00:15:26.200 --> 00:15:31.370
that your skid is no more than one instruction
away from where the event actually occurs.

00:15:31.370 --> 00:15:34.330
The hardware actually captures the IP.

00:15:34.330 --> 00:15:41.070
Now, for branch prediction and targets, it's still not
quite accurate because it's still one instruction later.

00:15:41.070 --> 00:15:41.950
But we're working on it.

00:15:41.950 --> 00:15:49.650
We'll get to the point at some point where we'll
actually capture the right IP address for the event.

00:15:49.649 --> 00:15:55.990
But anyway, the whole point of this is really that
your result-- your results are driving your analysis.

00:15:55.990 --> 00:16:02.919
And statistical sampling and how you collect the
data and how you interpret the data is very important

00:16:02.919 --> 00:16:06.750
to getting the right answer and bad
results can drive wrong conclusions.

00:16:06.750 --> 00:16:12.330
So I'm going to talk a little bit about some of the
pitfalls that you might run into to help steer you

00:16:12.330 --> 00:16:19.360
in the right direction when you start
doing performance event-based sampling.

00:16:19.360 --> 00:16:22.669
So let's just bring up Shark for a moment.

00:16:23.820 --> 00:16:29.830
Okay, so I actually have Shark here running.

00:16:29.830 --> 00:16:31.290
You've probably all seen this.

00:16:31.289 --> 00:16:36.199
So one of the first things that I did, this is
actually under the config so I'm bringing up edit,

00:16:36.200 --> 00:16:41.509
and I'm actually going to go to the view for advanced.

00:16:41.509 --> 00:16:46.620
OK, and so here you can see very clearly
that there are a number of options.

00:16:46.620 --> 00:16:49.830
One of them, the default, is the time-based sampling.

00:16:49.830 --> 00:16:54.190
So this is where you set how often you want it to sample.

00:16:54.190 --> 00:17:01.200
Whether that just be a time-based sampling and
Shark is actually getting the IP or if you were

00:17:01.200 --> 00:17:07.690
to collect some other event, how often you're
actually going to go query that counter for that data.

00:17:07.690 --> 00:17:15.970
There's also here, which is the CPU P-- PMC
program or the performance monitoring counters,

00:17:15.970 --> 00:17:19.589
and this is what I tend to use so I switch over to that.

00:17:19.589 --> 00:17:24.399
And then, this gives you, and it actually
picks the right set of events that are based

00:17:24.400 --> 00:17:27.850
on your particular processor that
you happened to be running on.

00:17:27.849 --> 00:17:30.949
So I talked about model specific events.

00:17:30.950 --> 00:17:32.789
This is where it comes into play.

00:17:32.789 --> 00:17:37.899
So Shark is smart enough to actually detect what
processor is actually running, that you're running on,

00:17:37.900 --> 00:17:44.210
and actually gives you the right
set of events to go utilize.

00:17:44.210 --> 00:17:47.860
So in looking at this, I talked a
little bit about the conditions,

00:17:47.859 --> 00:17:53.079
and one of those conditions was the privilege level
and so here we're talking about user versus supervisor.

00:17:53.079 --> 00:17:59.449
This is essentially ring 3 versus ring 0 or both.

00:17:59.450 --> 00:18:07.180
These are the actual events themselves,
and I'll bring up one here.

00:18:07.180 --> 00:18:10.720
Let me just find it.

00:18:10.720 --> 00:18:18.150
So a typical one is the CPU clock unhalted,
and it gives you a set of qualifier events.

00:18:18.150 --> 00:18:24.490
Again, this is part of the conditions on
which you can qualify the particular event.

00:18:24.490 --> 00:18:29.960
And this is the button that actually tells you
whether you're going to do the querying approach

00:18:29.960 --> 00:18:34.259
or whether you're actually going to do the trigger approach.

00:18:34.259 --> 00:18:39.069
In this event right here, or this sampling
interval, is actually the number of events you want

00:18:39.069 --> 00:18:44.329
to occur before you actually trigger
to get the location in the IP.

00:18:44.329 --> 00:18:50.740
The default in this case is 10,000 and I can
tell you that if I was to do unhalted clockticks,

00:18:50.740 --> 00:18:54.920
I would be getting interrupts way
too quickly on a 3 gigahertz machine.

00:18:54.920 --> 00:19:00.509
So, typically we would set this at about 1
millisecond in general, so we would take--

00:19:00.509 --> 00:19:05.589
we would just take the frequency of the processor
and basically put that number in there divided

00:19:05.589 --> 00:19:11.559
by 10, just to get us in the right ballpark.

00:19:11.559 --> 00:19:13.740
So be careful when you take the defaults.

00:19:13.740 --> 00:19:14.740
Make sure that you understand.

00:19:14.740 --> 00:19:20.650
A lot of times I'll actually run this, figure out how
many events actually occurred, how long it actually ran,

00:19:20.650 --> 00:19:25.170
and figure out what the right interval or
what the right sampling interval should be

00:19:25.170 --> 00:19:27.470
for the particular event that I'm looking at.

00:19:27.470 --> 00:19:31.269
Something like unhalted clocktick cycles
is going to be a very large number.

00:19:31.269 --> 00:19:39.529
Something like loop string detecting
might be a much smaller type of event.

00:19:39.529 --> 00:19:45.529
Now, I will-- now this is not a, you
know, an official release of Snow Leopard,

00:19:45.529 --> 00:19:50.460
and so Shark is still not quite correct
when you look at Nehalem, so be careful.

00:19:50.460 --> 00:19:53.049
Some of the events aren't quite configured correctly.

00:19:53.049 --> 00:19:55.250
Just looking at unhalted clockticks, I can tell you

00:19:55.250 --> 00:20:02.059
that there really is no event called
total cycles that has a bitmask of 2.

00:20:02.059 --> 00:20:07.169
So that one I happened to know is not actually configured
correctly, and hopefully we'll get that resolved

00:20:07.170 --> 00:20:12.860
with the Shark team before Snow Leopard is released.

00:20:12.859 --> 00:20:19.929
>> And so here we have the-- in Nehalem, I told you
there were 4 programmable events, so here is up to 4.

00:20:19.930 --> 00:20:24.120
And then we have the 3 fixed counter events.

00:20:24.119 --> 00:20:28.739
So you see, you can't actually change
those values or set any of the reserve,

00:20:28.740 --> 00:20:34.049
but you can still determine whether
you wanted to be user or supervisor.

00:20:34.049 --> 00:20:43.509
The other thing to be careful of is that when you
actually start looking at some of these events, OK,

00:20:43.509 --> 00:20:50.460
it might be very tempting to actually start clicking
on every one of these and saying, OK, you know what,

00:20:50.460 --> 00:20:59.360
I really want to know when the L1 data cache
store has an invalidate state, a shared exclusive

00:20:59.359 --> 00:21:01.659
or modified state and start clicking on this.

00:21:01.660 --> 00:21:10.259
Not all of the events are supported where you can actually
collect-- click on every single subevent in the event list.

00:21:10.259 --> 00:21:16.059
Some of them, it's not clear whether
it's going to be an add, so--

00:21:16.059 --> 00:21:23.490
or an and of the event, or whether it's going to be an
or of the event or whether it's just going to be garbage.

00:21:23.490 --> 00:21:27.420
So, one of the things is when you start
looking at some of these subevents,

00:21:27.420 --> 00:21:30.620
is to be very clear about what they collect.

00:21:30.619 --> 00:21:36.969
And if you're unsure, write a little routine,
in this case, that does and L1D's cache store

00:21:36.970 --> 00:21:40.720
and actually measure what these events are giving you.

00:21:40.720 --> 00:21:47.230
And then if you actually click on multiples of them,
determine whether it's actually giving the total

00:21:47.230 --> 00:21:51.680
for all of the events or some variation thereof.

00:21:51.680 --> 00:21:59.480
So anyway, that's how you configure
these performance events in Shark.

00:21:59.480 --> 00:22:04.009
So, here is an example of time-based sampling.

00:22:04.009 --> 00:22:08.059
I showed you where that actually
was in the Shark configuration.

00:22:08.059 --> 00:22:11.980
And so here is where you actually can figure it for time.

00:22:11.980 --> 00:22:21.299
And I told you that it's not necessarily a good idea to
actually take a time-based sampling with a counter event.

00:22:21.299 --> 00:22:26.389
So, for example here I actually clicked
it, I said I don't want triggering mode.

00:22:26.390 --> 00:22:33.710
You can't do triggering and time-based sampling at the
same time, so you shouldn't be able to select trigger

00:22:33.710 --> 00:22:39.400
with the timer set that way, Shark actually give
you an error and tell that you need to fix that.

00:22:39.400 --> 00:22:47.269
But you can actually set up this particular
configuration, but you really don't want to do that.

00:22:47.269 --> 00:22:52.789
And if you do, you have to be very careful
about how you're interpreting that data.

00:22:52.789 --> 00:22:57.159
Now you can actually set up time-based
sampling using the performance events as well,

00:22:57.160 --> 00:22:59.740
using the unhalted clocktick cycles event.

00:22:59.740 --> 00:23:01.589
It's essentially doing the same thing.

00:23:01.589 --> 00:23:06.689
Instead of using the OS time-- counter to
interrupt and figure out where you are,

00:23:06.690 --> 00:23:09.210
you're actually using the processor event.

00:23:09.210 --> 00:23:11.950
And again, you can pick triggering in counter.

00:23:11.950 --> 00:23:15.400
There's nothing that actually stops you from doing that.

00:23:15.400 --> 00:23:23.090
And again, you know, be careful
because-- Let me get back here.

00:23:23.089 --> 00:23:25.609
We're going too fast.

00:23:25.609 --> 00:23:30.219
The mouse is a little bit-- OK.

00:23:30.220 --> 00:23:35.370
So what I did is I actually wrote an example
showing statistical sampling with 2 functions,

00:23:35.369 --> 00:23:42.409
one that is completely compute bound, there are no load
instructions whatsoever, and one that is mostly load bound.

00:23:42.410 --> 00:23:46.230
So it's got L1 data cache misses.

00:23:46.230 --> 00:23:52.150
And if I were actually figuring out-- and I wrote
this such that I expect 75 percent of my time to be

00:23:52.150 --> 00:24:00.620
in the compute bound function, and 25
percent of my time in my load bound function.

00:24:00.619 --> 00:24:04.219
And the reason why it's 75 and 25 is
because t%hat's the way I wrote it.

00:24:04.220 --> 00:24:07.269
I wrote it and then measured it and
said, "Yeah, it's going to be 75 and 25."u

00:24:07.269 --> 00:24:13.170
But what I wanted to demonstrate is that I expect
100 percent of my load misses to actually be

00:24:13.170 --> 00:24:19.740
in the load bound function and 0 misses to be in the
compute function, because I don't have any instructions.

00:24:19.740 --> 00:24:23.640
So, if we look at the instruction, and
this is a little bit of assembly code.

00:24:23.640 --> 00:24:29.250
I tend to work in assembly because I know
exactly what I'm issuing to the machine.

00:24:29.250 --> 00:24:32.339
And in this case, I just have a
loop with a bunch of increments.

00:24:32.339 --> 00:24:35.500
There are no load instructions
whatsoever, it's very trivial.

00:24:35.500 --> 00:24:40.369
I just-- the reason why I have a bunch of increments
is because I wanted to get to that 75 percent mark,

00:24:40.369 --> 00:24:44.369
and so I just kept adding a few
increments until I got to that point.

00:24:44.369 --> 00:24:48.969
And then here is my load bound function.

00:24:48.970 --> 00:24:53.900
It's a little bit more complex, but essentially it's
just a bunch of loops that actually are doing the load,

00:24:53.900 --> 00:24:56.910
and there's only one load in the function.

00:24:56.910 --> 00:25:00.810
But I wrote it such that I'm actually
striding through the cache so that I know

00:25:00.809 --> 00:25:06.819
that I'm missing the L1 data cache every single time.

00:25:06.819 --> 00:25:15.599
And if I run that with Shark, and I have a triggering,
either time-based sample or triggered on unhalted clockticks

00:25:15.599 --> 00:25:24.889
in counter mode, what I find is is that in this particular
case, the time really was 75 percent and 25 percent.

00:25:24.890 --> 00:25:33.259
But if I looked at the L1 data cache misses, if you actually
looked at the percentage of the ratios of those 2 values,

00:25:33.259 --> 00:25:42.299
those 2 big values, you'd find that 75 percent of the
load data cache misses were in the compute function,

00:25:42.299 --> 00:25:45.019
which is not the case, and then 25 percent.

00:25:45.019 --> 00:25:50.440
And that's strictly because there is no correlation
between when you're going to have an L1D cache miss

00:25:50.440 --> 00:25:56.519
versus when you're going to have a time interrupt
or where you're spending most of your time.

00:25:56.519 --> 00:25:58.529
And so you end up just matching that.

00:25:58.529 --> 00:26:03.190
Now, you could still use some of
this data if you're careful.

00:26:03.190 --> 00:26:06.539
What you can't do is associate it
with these particular functions.

00:26:06.539 --> 00:26:10.539
If you wanted to know, kind of get
a time-based sampling of your--

00:26:10.539 --> 00:26:16.240
where you're spending your time but you just want
to know what the total count for the event was,

00:26:16.240 --> 00:26:20.130
you could add these 2 values together
and that would give you kind

00:26:20.130 --> 00:26:23.880
of what your full workload ran and what the count was.

00:26:23.880 --> 00:26:27.800
But that's about the extent to which
you can use those particular values.

00:26:27.799 --> 00:26:29.940
You can't associate them with the function.

00:26:29.940 --> 00:26:37.799
And in fact, here we just see that the-- we're
really just matching the same ratio that we measured

00:26:37.799 --> 00:26:45.879
in our time-based sampling, which is not correct.

00:26:45.880 --> 00:26:54.510
So, this is an example of actually running with the
triggering where I actually collected on L1D replacements.

00:26:54.509 --> 00:26:58.789
L1-- that every time I have a miss, right, I'm
going to have to replace something in the cache,

00:26:58.789 --> 00:27:00.970
so I happen to use that particular event.

00:27:00.970 --> 00:27:08.360
And it turns out that doing that I find that
I'm getting 99.3 percent of my L1D caches,

00:27:08.359 --> 00:27:15.359
replacements in the 25 percent or the
load bound function, which is correct.

00:27:15.359 --> 00:27:22.009
So here is a clear case of using statistical sampling
to correctly get the right location in the code.

00:27:22.009 --> 00:27:29.559
[ Pause ]

00:27:29.559 --> 00:27:35.980
>> And essentially, that takes us-- So if you actually
drill down using the trigger sampling and you actually drill

00:27:35.980 --> 00:27:42.420
down into the disassembly for this particular routine, it
actually takes you right to where that load instruction is,

00:27:42.420 --> 00:27:47.880
and you'll notice that it shows like
about 38 percent on the load instruction,

00:27:47.880 --> 00:27:52.530
and then 54 percent on the next instruction after it.

00:27:52.529 --> 00:27:54.720
That's where the skid comes in, right?

00:27:54.720 --> 00:28:00.890
So, the fact that the event occurred on the
load, by the time we actually detect and figure

00:28:00.890 --> 00:28:07.420
out what IP address we're on, sometimes we're on
the load and sometimes we're an instruction behind.

00:28:07.420 --> 00:28:09.800
But it takes you pretty much right where you want to be.

00:28:09.799 --> 00:28:11.490
The skid isn't that bad.

00:28:11.490 --> 00:28:15.859
You will find that skid with branches tend
to be a little bit harder to trace back

00:28:15.859 --> 00:28:23.579
to if you're not using the precise event-based sampling.

00:28:23.579 --> 00:28:25.750
So, some skid is not unusual.

00:28:25.750 --> 00:28:34.259
In fact, if you have extremely compute-intensive code in
which you're retireing a lot of instructions per cycle,

00:28:34.259 --> 00:28:40.579
you'll actually see banding where you'll see like 3
or 4 instructions and then you'll have your percentage

00:28:40.579 --> 00:28:45.529
on like the third or fourth instruction, and then
another set of 3 or 4 instructions with another one.

00:28:45.529 --> 00:28:51.250
And that's because the reality is that the processor is
retiring, you know, 3 or 4 instructions in one cycle,

00:28:51.250 --> 00:28:54.109
so what IP are you actually picking
when that event occurred.

00:28:54.109 --> 00:28:56.029
You're really just picking the last one.

00:28:56.029 --> 00:29:01.899
So sometimes if you see that banding, that event can
really be associated with maybe 2 or 3 instructions ahead

00:29:01.900 --> 00:29:05.970
of it just because of the way the interrupts are happening

00:29:05.970 --> 00:29:09.009
and the way the instructions are
retiring through the machine.

00:29:09.009 --> 00:29:13.710
[ Pause ]

00:29:13.710 --> 00:29:18.980
>> So that's kind of the basics on performance event
counters, and now I'm going to talk a little bit

00:29:18.980 --> 00:29:23.099
about what information you can actually
get from the processor and the events.

00:29:23.099 --> 00:29:27.019
Now, in Nehalem, there are hundreds of events.

00:29:27.019 --> 00:29:30.599
There is no way that I could talk
about every single one of them.

00:29:30.599 --> 00:29:38.879
So what I'm going to trying to do is actually walk through
the architecture and look at some of the major components

00:29:38.880 --> 00:29:44.910
of that architecture, and highlight some of the maybe
more interesting events that can be associated with them.

00:29:44.910 --> 00:29:49.340
But really to show you that just about every
point in the architecture, in the pipeline,

00:29:49.339 --> 00:29:54.389
you're able to get some performance information from it.

00:29:54.390 --> 00:29:58.400
Now, interpreting this is a little bit
trickier and we'll talk more about that.

00:29:58.400 --> 00:30:02.290
Now I'm focusing on Nehalem because
that's our latest processor.

00:30:02.289 --> 00:30:08.430
It has more capability in terms of the number
of performance events that are supported.

00:30:08.430 --> 00:30:14.920
But this is actually true for any other previous Intel
processors that are supported on the Mac platform.

00:30:14.920 --> 00:30:19.180
>> So all the way back to [inaudible], there are some
level of events that are supported but I'm not going

00:30:19.180 --> 00:30:22.180
to talk about those particular architectures.

00:30:22.180 --> 00:30:28.000
And so right here, we've actually divided
up the architecture into 3 domains.

00:30:28.000 --> 00:30:35.930
There is the front-end domain which is actually
fetching the instructions and doing the decode,

00:30:35.930 --> 00:30:39.400
and then going to be providing
that back to the execution unit.

00:30:39.400 --> 00:30:42.269
So the execution unit is the other domain.

00:30:42.269 --> 00:30:46.900
This is where all of the register
file renaming happens in allocation.

00:30:46.900 --> 00:30:49.019
This is where the retirement buffer lives.

00:30:49.019 --> 00:30:58.139
So as instructions or micro OPS are being retired, if
they're out of order because they are out of order.

00:30:58.140 --> 00:31:04.210
This is where they're maintained so they can actually
retire in order and then the actual execution units

00:31:04.210 --> 00:31:13.110
and then finally we have the kind of the memory hierarchy
which has basically the cache, and out to memory itself.

00:31:13.109 --> 00:31:20.809
And I just highlighted a couple of items in here so that
as you see from the decoder, you're actually getting

00:31:20.809 --> 00:31:27.079
up to 4 instructions decoded into this
back-end or into the execution unit.

00:31:27.079 --> 00:31:35.039
And then from the rename allocate station,
you're actually-- and can again issue 4.

00:31:35.039 --> 00:31:40.250
And once you get into the reservation station, which is
the one that actually reserves and makes sure that all

00:31:40.250 --> 00:31:46.059
of your resources are available to actually go
execute that instruction, there are actually 6 ports.

00:31:46.059 --> 00:31:51.299
So you can actually issue up to 6
micro OPS into the execution unit.

00:31:51.299 --> 00:31:58.909
There are these kinds of constraints throughout the
architecture and so this is one of the things that we look

00:31:58.910 --> 00:32:06.290
at when we start using the performance events is to actually
look at, you know, based on what is my application doing,

00:32:06.289 --> 00:32:09.460
what is my bandwidth support within the processor,

00:32:09.460 --> 00:32:15.940
and am I getting close to that theoretical
maximum throughput at each of those stages.

00:32:15.940 --> 00:32:20.650
So, let's look at the front-end first.

00:32:20.650 --> 00:32:27.160
The front-end is what actually feeds the
execution unit or the execution engine.

00:32:27.160 --> 00:32:32.880
If you're not feeding enough instructions, and it is
possible to not feed enough instructions to the back-end,

00:32:32.880 --> 00:32:40.240
you essentially starve the rest of the machine,
but it's very nice because we have lots of--

00:32:40.240 --> 00:32:46.339
Just about every point within this front-end,
we have events to be able to show you

00:32:46.339 --> 00:32:50.339
where you might be actually stalling
or running into resource issues.

00:32:50.339 --> 00:32:59.740
For example, the L1 instruction cache, you can actually see
how often you're stalling trying to fetch from the L1 cache.

00:32:59.740 --> 00:33:06.779
So if you wrote your code very branchy and you're trying
to fetch and you're only getting maybe one instruction

00:33:06.779 --> 00:33:12.000
on a fetch line, so fetch line is up to 16-bytes wide.

00:33:12.000 --> 00:33:17.130
If you're only getting one instruction because
you're branching to another fetch line and those,

00:33:17.130 --> 00:33:23.340
and you have enough of those that you're exceeding your
L1 cache, you're going to be stalling for L1 cell cycles.

00:33:23.339 --> 00:33:31.089
Now, it's also possible that you're not able to, you're
fetching enough instructions and you're actually filling

00:33:31.089 --> 00:33:37.480
up the instruction queue which is the queue right before
the decoder, because you're not decoding fast enough.

00:33:37.480 --> 00:33:43.829
And so you can actually determine, you know, am I fetching
OK and in fact I'm fetching enough but I'm filling

00:33:43.829 --> 00:33:49.949
up the queue that I couldn't really do it any faster
even if I wanted to, at least from a fetch standpoint,

00:33:49.950 --> 00:33:53.299
that you might start actually looking at the decoder.

00:33:53.299 --> 00:33:56.859
And so you can actually-- there are
events associated with the decoders.

00:33:56.859 --> 00:34:00.459
You can tell how many instructions have been decoded.

00:34:00.460 --> 00:34:06.490
We have a 411 model so we have
complex instructions in decoder 0,

00:34:06.490 --> 00:34:11.389
and then up to single micro OP decoders in the other three.

00:34:11.389 --> 00:34:18.650
There are complex instructions which are decoded
out of the ROM which bypass the decoders.

00:34:18.650 --> 00:34:24.119
So if they don't fit in the 411, so if
your instruction is larger than 4 micro OPS

00:34:24.119 --> 00:34:32.369
and the complex decoder can't resolve it, it has to go
to the micro ROM which can have a string of micro OPS.

00:34:32.369 --> 00:34:37.539
And then you could also tell when you're actually
delivering your micro OPS out of the loop stream detector.

00:34:37.539 --> 00:34:42.699
So the loop stream detector on Nehalem
supports up to 20 rate, 28 micro OPS.

00:34:42.699 --> 00:34:47.569
So after instructions are decoded, OK, if
the loop stream detector detects that, hey,

00:34:47.570 --> 00:34:52.000
I've seen this pattern consistently
after a certain amount of iterations.

00:34:52.000 --> 00:34:56.739
It will actually start issuing out of the loop stream
detector and you can actually shut down the rest

00:34:56.739 --> 00:34:59.099
of the front-end to save a little bit on power.

00:34:59.099 --> 00:35:06.489
It's not necessarily a performance saving except
if you have some decoder bandwidth issues.

00:35:06.489 --> 00:35:11.449
If you have some decoder bandwidth issues, the loop stream
detector could give you a little bit of performance boost

00:35:11.449 --> 00:35:14.199
by avoiding having to go through the decoders.

00:35:14.199 --> 00:35:17.349
But there are lots of events associated with this front-end.

00:35:17.349 --> 00:35:27.389
And in fact we have a tremendous
number of branch-related events.

00:35:27.389 --> 00:35:30.759
You can find out when you have branch address clear events

00:35:30.760 --> 00:35:36.840
for which the branch predictor doesn't know what the address
actually is going to be and so it has to clear that address.

00:35:36.840 --> 00:35:42.780
There are instructions retired associated
with, you know, when you've mispredicted,

00:35:42.780 --> 00:35:48.970
you can tell when you've mispredicted, you can tell
when you predicted and it can be taken or not taken.

00:35:48.969 --> 00:35:54.449
So you can qualify each of those events
down to exactly what you're interested in.

00:35:54.449 --> 00:35:58.059
You can find out when a branch
prediction unit is actually false,

00:35:58.059 --> 00:36:04.699
so it's possible that the branch predictor
looks at 32 bytes ahead of the units.

00:36:04.699 --> 00:36:06.399
So the fetch line is 16 bytes.

00:36:06.400 --> 00:36:08.340
The branch predictor is 32 bytes.

00:36:08.340 --> 00:36:14.610
It's actually possible to fill up that branch
predictor unit such that the queue is full

00:36:14.610 --> 00:36:17.170
and you can't actually put anymore
branch prediction in there.

00:36:17.170 --> 00:36:21.730
It's also stalling because you don't
get the correct branch predictions.

00:36:21.730 --> 00:36:28.750
So the front-end is very well heavily
supported with processor events.

00:36:28.750 --> 00:36:33.909
If you look at the execution unit or the execution engine,

00:36:33.909 --> 00:36:37.750
again there are tremendous numbers of
events here to support the analysis.

00:36:37.750 --> 00:36:45.269
So assuming that you're getting enough micro OPS out of the
front-end, OK, enough instructions into the execution units,

00:36:45.269 --> 00:36:49.610
now you can start looking at, you know, what
is going on from an execution standpoint.

00:36:49.610 --> 00:36:51.710
Am I stalling in the execution units?

00:36:51.710 --> 00:36:57.610
So, some of the resource stalls that are
supported are things like the ROB being full.

00:36:57.610 --> 00:37:04.530
So, if I'm not retiring instructions but I've gotten
enough instructions started and starting to execute

00:37:04.530 --> 00:37:08.980
but I'm not completing them fast
enough, my ROB can actually fill up.

00:37:08.980 --> 00:37:16.269
The reservation station which has 36 entries on
Nehalem is also another resource that you can fill up.

00:37:16.269 --> 00:37:23.949
So if you are actually getting enough micro OPS in but
you're not getting them out of the execution units,

00:37:23.949 --> 00:37:29.549
because the results are stored in the ROB once they finish
the execution unit waiting to retire, if that's backing up,

00:37:29.550 --> 00:37:35.720
so if you're execution units are stalling then you will
start stalling and filling up your reservation station.

00:37:35.719 --> 00:37:39.869
And then of course if you're doing load and stores,
your load and store buffers can start filling up,

00:37:39.869 --> 00:37:45.159
that will also stall waiting on
either cache or memory loads to occur.

00:37:45.159 --> 00:37:49.500
And then there are some issues with the domain.

00:37:49.500 --> 00:38:00.690
So if you were doing floating-point in SIMD and integer,
some instructions support multiple data types like that

00:38:00.690 --> 00:38:06.150
so you can actually do a load using a
floating-point load instruction and then go use it

00:38:06.150 --> 00:38:12.970
in an integer SIMD instruction, and that's actually
what's called a bypass or domain bypass stall

00:38:12.969 --> 00:38:17.659
because you're switching from one domain to
another which will also slow down your execution.

00:38:17.659 --> 00:38:21.179
And then you can actually find out
exactly what port you're dispatching.

00:38:21.179 --> 00:38:29.529
So if you have a routine that you're actually looking at
the execution for and you partition that out into the ports

00:38:29.530 --> 00:38:34.870
that are being used, it's possible to actually look at
that and find out whether you're actually being limited

00:38:34.869 --> 00:38:37.829
by the port that you're actually executing on.

00:38:37.829 --> 00:38:46.429
And it's also possible to actually decide that, hey,
I'm port 0 bound limited or port 0 and 5 limited,

00:38:46.429 --> 00:38:50.829
let me look for other instructions that
I can start to spread some of this out.

00:38:50.829 --> 00:38:57.610
Maybe instead of doing moves using registers, maybe
I'll actually load it from cache and use port 4

00:38:57.610 --> 00:39:04.650
to actually do the load and it will hide the latency,
because you're executing a lot of instructions

00:39:04.650 --> 00:39:08.650
and you're spreading this out over the number
of ports that you actually have going on.

00:39:08.650 --> 00:39:14.389
So again, there's lots of events that are supported
in the execution unit to figure out where it is.

00:39:14.389 --> 00:39:19.469
And what you tend to try to do is start
looking at the back of the pipeline first.

00:39:19.469 --> 00:39:23.250
So you're looking at where is the resources being consumed.

00:39:23.250 --> 00:39:27.880
You know, the first thing I wouldn't look
at would be at the front-end decoder.

00:39:27.880 --> 00:39:29.110
I would start looking at the end.

00:39:29.110 --> 00:39:30.510
How am I retiring micro OPS?

00:39:30.510 --> 00:39:32.420
What's my micro OP retirement rate?

00:39:32.420 --> 00:39:35.059
If I'm not retiring micro OPS, what's the bottleneck?

00:39:35.059 --> 00:39:37.429
Is it in the execution stalls?

00:39:37.429 --> 00:39:39.589
OK, are we having problems with the execution units?

00:39:39.590 --> 00:39:40.930
How to get in the micro OPS through?

00:39:40.929 --> 00:39:45.419
And if that seems to be clear, I'm never filling
up the reservation queue and everything's--

00:39:45.420 --> 00:39:51.099
then they seem to be idle, then I start
backing up and looking up further into the,

00:39:51.099 --> 00:39:54.099
to the processor, into the front-end in the decoder side.

00:39:54.099 --> 00:40:03.289
And then the cache and memory subsystem has, again,
a number of events associated with cache behavior.

00:40:03.289 --> 00:40:07.079
So, you know, you're looking at
the number of L2 cache lines in.

00:40:07.079 --> 00:40:12.920
You can do-- you can actually partition them
into the ones that you've actually demanded

00:40:12.920 --> 00:40:18.180
because you've actually written a load instruction for it
and the ones that are actually being prefetched for you.

00:40:18.179 --> 00:40:21.329
>> So you can partition that.

00:40:21.329 --> 00:40:26.569
You can see the L2 lines out, same thing, was
the L2 line out because it was a demand store

00:40:26.570 --> 00:40:31.710
or was it because I did a prefetch in
it, evicted some other instruction.

00:40:31.710 --> 00:40:35.679
And then you can just look at the number of
memory instructions retired, load, stores.

00:40:35.679 --> 00:40:43.940
And the interesting thing here is that we've
actually tied some of these events back to the DRAM.

00:40:43.940 --> 00:40:49.539
So on Nehalem, because the architecture of the
platform has actually changed from the front-side bus

00:40:49.539 --> 00:40:53.159
that we originally have to a NUMA-related architecture.

00:40:53.159 --> 00:41:01.170
If you actually have NUMA enabled, you can actually
see where the memory is actually coming from and figure

00:41:01.170 --> 00:41:05.490
out how often it's coming from
you local DRAM for your socket

00:41:05.489 --> 00:41:10.109
or whether you're actually loading
that memory from a remote socket.

00:41:10.110 --> 00:41:11.980
And the performance delta is pretty astounding.

00:41:11.980 --> 00:41:16.679
It's 30, 40 percent difference depending on
where your application happens to be running

00:41:16.679 --> 00:41:20.829
and where it's actually loading that data.

00:41:20.829 --> 00:41:26.110
So, there's lots of events to investigate the architecture.

00:41:26.110 --> 00:41:33.500
And for a lot of these events, it's really good
to just write kernels, you know, use the events,

00:41:33.500 --> 00:41:35.769
figure out how the processor is actually working.

00:41:35.769 --> 00:41:39.989
Make sure you understand what the event is
actually doing and what it's actually counted

00:41:39.989 --> 00:41:43.179
because some of them can be fairly complex.

00:41:43.179 --> 00:41:48.089
Now there's a little bit of a wrinkle
when you start looking at this.

00:41:48.090 --> 00:41:55.640
I told you at the very beginning that there is essentially
performance events can count occurrences and number

00:41:55.639 --> 00:42:01.069
of events in cycle or duration, so occurrences and duration.

00:42:01.070 --> 00:42:10.280
But, as the processors have gotten more complex a
clocktick is not always a clocktick is always a clocktick.

00:42:10.280 --> 00:42:15.660
There's actually various clock cycle domains,
and this can actually impact your results.

00:42:15.659 --> 00:42:22.029
So when you start looking at some of these cycle events,
you have to be aware what cycle you're talking about so

00:42:22.030 --> 00:42:28.360
that when you compare them to your expectations, that
you understand and can interpret the results correctly.

00:42:28.360 --> 00:42:32.789
So one is, you know, make sure that
you understand what that domain is.

00:42:32.789 --> 00:42:39.070
I'm going to talk about what the various cycle
domains are and how you compare them to each other.

00:42:39.070 --> 00:42:46.610
So one of the reasons why we have differences in clockticks
is because we support a concept called turbo mode.

00:42:46.610 --> 00:42:52.599
And so if we have, in the old days or
prior to Nehalem with turbo, you know,

00:42:52.599 --> 00:42:56.329
when you ran cores, you ran them at a certain frequency.

00:42:56.329 --> 00:43:00.059
If you didn't use them, you could halt
them and actually put them to sleep

00:43:00.059 --> 00:43:03.279
but the other cores just stayed,
you know, at the same frequency.

00:43:03.280 --> 00:43:05.030
There was no additional boost.

00:43:05.030 --> 00:43:11.680
What we found is that, if a couple of cores
actually go to idle, OK, they actually get halted,

00:43:11.679 --> 00:43:20.460
we actually have a larger thermal design point which we
can actually increase the frequency of the other cores.

00:43:20.460 --> 00:43:25.240
And so turbo mode will kick in, and
your clock domain actually changes.

00:43:25.239 --> 00:43:30.839
So while you still, a clocktick is
still a clocktick in an unhalted cycle,

00:43:30.840 --> 00:43:36.360
it actually is incrementing faster
because the frequency is different.

00:43:36.360 --> 00:43:43.289
And so I actually collected, and this is a little bit
on my chart, essentially unhalted reference cycles.

00:43:43.289 --> 00:43:47.789
Unhalted reference cycles are ones
that are cycles for which--

00:43:47.789 --> 00:43:56.000
well, it used to be tied to the
front-side bus clock, OK, but--

00:43:56.000 --> 00:44:04.630
and it still is in some sense that we say that the unhalted
reference cycle, the increments at 133 megahertz increments.

00:44:04.630 --> 00:44:08.030
So it's actually closer to wall clock time.

00:44:08.030 --> 00:44:13.840
And then we have what's called a thread cycle
which is the actual cycle for every increment

00:44:13.840 --> 00:44:19.170
for which no matter what frequency it happens to be
at, it's just incrementing at that particular rate.

00:44:19.170 --> 00:44:23.440
And then we have what's called wall
clock time or read time stamp counter.

00:44:23.440 --> 00:44:31.230
And in this case I've collected both of these
events and it turns out that the total clock cycles

00:44:31.230 --> 00:44:35.469
or the wall clock time is both the
unhalted and halted clock cycles.

00:44:35.469 --> 00:44:41.059
It's the real time stamp counter, and in this
case both numbers turned out to be the same.

00:44:41.059 --> 00:44:52.380
But if we look at the reference cycles here, and that's
the 43 million, OK, if we take that and multiply it

00:44:52.380 --> 00:44:59.599
by our frequency reference or multiplier, then we can
actually come up pretty close to the wall clock time.

00:44:59.599 --> 00:45:06.449
So essentially what this tells us is that if I take
that 43, and in my case it was a 2.66 megahertz machine,

00:45:06.449 --> 00:45:15.919
multiplied it by 20, I pretty much get
the same 874 or 875 million clockticks.

00:45:15.920 --> 00:45:24.329
So what that tells me is that, because the total cycle
is both unhalted and halted, that that particular CPU,

00:45:24.329 --> 00:45:32.219
CPU-8 was almost exclusively unhalted,
it was running consistently.

00:45:32.219 --> 00:45:37.279
And so there's really almost no
unhalted cycles associated with that CPU.

00:45:37.280 --> 00:45:41.269
So that's the unhalted reference cycles.

00:45:41.269 --> 00:45:47.739
Now if we look at all the unhalted cycles,
it's really independent of the frequency.

00:45:47.739 --> 00:45:55.500
So here's the thread or the cycles for the thread
of the core cycles, and here we're over a billion.

00:45:55.500 --> 00:45:59.219
So if you look at those 2 numbers, how
does that actually, you know, translate?

00:45:59.219 --> 00:46:02.189
I've got more clockticks than I have wall clock time?

00:46:02.190 --> 00:46:07.200
So in this case it actually turns out that
we actually got an upside from turbo mode

00:46:07.199 --> 00:46:12.519
and in this case it was really close to 15
percent improvement on this particular CPU.

00:46:12.519 --> 00:46:15.880
It turns out to be 3 bins up on the frequency.

00:46:15.880 --> 00:46:25.410
So instead of being a 2.66 part, it actually was running
for this particular core at about 3.06 gigahertz.

00:46:26.869 --> 00:46:30.369
So that's the unhalted thread clock cycles.

00:46:30.369 --> 00:46:33.859
Now if you look at some of the other CPUs
that happened to be running these are--

00:46:33.860 --> 00:46:39.670
I wrote this as a single-threaded application, you can
actually see that the other cycles are actually halted

00:46:39.670 --> 00:46:47.769
and so their numbers are extremely low, and those are the
unhalted thread clock cycles, but we're actually seeing

00:46:47.769 --> 00:46:53.650
that when it was actually running
CPU-3 was less than 0.1 percent active

00:46:53.650 --> 00:46:56.980
and the other ones are pretty much in that same ballpark.

00:46:56.980 --> 00:47:01.880
So you can see that there are a number
of clock domains to be aware of.

00:47:01.880 --> 00:47:09.900
There's the wall clock time, there's the reference clock,
and then there's actual the execution cycles associated.

00:47:09.900 --> 00:47:14.210
So be aware and know what the events are actually counting.

00:47:14.210 --> 00:47:20.470
And one of the reasons for that is because one of the things
that I talked about on the back end is that we can retire

00:47:20.469 --> 00:47:26.869
up to 4 instructions per cycle if you actually
used-- if you've collected in this particular case,

00:47:26.869 --> 00:47:34.559
if you've collected the instructions retired and the
unhalted cycles, you would see that it would appear at least

00:47:34.559 --> 00:47:36.429
if you just based it on the wall clock time

00:47:36.429 --> 00:47:42.500
that you're actually retiring more
micro OPS or more instructions than 4.

00:47:42.500 --> 00:47:47.840
And so I certainly don't want that to kind of throw you up.

00:47:47.840 --> 00:47:51.690
So, how to interpret the results.

00:47:51.690 --> 00:47:57.190
You know I get the question a lot about,
you know, here's my data, what does it mean.

00:47:57.190 --> 00:48:00.460
Well, it depends-- it depends on your application.

00:48:00.460 --> 00:48:03.679
It really depends on what is your application doing.

00:48:03.679 --> 00:48:12.299
You can't look at the performance event counters independent
of your application or independent of the architecture.

00:48:12.300 --> 00:48:17.080
Now you can use some of your ratio events
to give you a little bit better idea.

00:48:17.079 --> 00:48:24.619
Certainly if you came to me and said,
"Hey, I've got my CPI and I'm getting--

00:48:24.619 --> 00:48:33.719
and I'm getting, you know, my instructions retired per
clock is 4" And I'll tell you that there's really nothing,

00:48:33.719 --> 00:48:37.369
other than eliminating instructions,
there's really nothing you can do

00:48:37.369 --> 00:48:40.989
to improve your performance with that kind of IPC.

00:48:40.989 --> 00:48:46.409
But if you came to me and said, "Hey, that's
0.25", I would say, you have a lot of head room.

00:48:46.409 --> 00:48:53.639
You need to be up in the 2 range if
you're really more in the compute arena.

00:48:53.639 --> 00:48:57.019
So use the ratio subevents to help guide some of that.

00:48:57.019 --> 00:49:02.519
And then really understand where your performance
bottlenecks are by using the performance event counter.

00:49:02.519 --> 00:49:08.340
So if you can base them on the architecture,
start working back in the pipeline, you know,

00:49:08.340 --> 00:49:13.630
start with your instructions retired,
working back up through the pipeline,

00:49:13.630 --> 00:49:19.440
then you can start to better understand whether your data
or your results makes sense based on your application.

00:49:19.440 --> 00:49:24.240
And once you understand where the bottleneck is,
then you can start to think about what you can do

00:49:24.239 --> 00:49:27.279
to actually resolve that particular bottleneck.

00:49:27.280 --> 00:49:31.269
So you do get very big numbers, very big counts, no doubt.

00:49:31.269 --> 00:49:35.730
And-- but you really need to correlate them
to your expectations and that has to be based

00:49:35.730 --> 00:49:38.360
on the application as well as the hardware.

00:49:38.360 --> 00:49:46.550
So to kind of try to give you an idea of how to get started.

00:49:46.550 --> 00:49:53.789
Basic characterization, so a number of these events
are actually fairly obvious I think to most of us

00:49:53.789 --> 00:49:58.909
that are doing performance analysis, simple
things like in cycles per instruction,

00:49:58.909 --> 00:50:02.399
your branch prediction per micro OPS retired.

00:50:02.400 --> 00:50:10.059
So, for example, what happens with a branch retired
if you miss a lot, if you missed that prediction.

00:50:10.059 --> 00:50:14.549
The processor is still fetching all of those
instructions, all of those micro OPS and executing them.

00:50:14.550 --> 00:50:20.170
>> And no one figures out, hey, I mispredicted,
it has to throw all those micro OPS out.

00:50:20.170 --> 00:50:21.789
They never get retired.

00:50:21.789 --> 00:50:28.849
So if you find that you know the number of branches
per micro OPS retired is getting, you know,

00:50:28.849 --> 00:50:36.369
out of whack then you know that you're probably missing a
lot, OK, and you're throwing a lot of stuff away so really,

00:50:36.369 --> 00:50:40.779
the thing is to try to figure out how to eliminate
those branches, how to improve the branch prediction

00:50:40.780 --> 00:50:47.960
for those branches, maybe using SIMD type instructions
and actually operating on maybe both paths of execution

00:50:47.960 --> 00:50:52.820
and making a decision at the end
about which computation to use.

00:50:52.820 --> 00:50:55.830
But there's a number of things you can do
to try to improve your branch prediction.

00:50:55.829 --> 00:51:00.590
And then certainly L1 data cache miss
rate and L2 data cache miss rate.

00:51:00.590 --> 00:51:08.010
The reality is that you know the biggest bottlenecks are
going to be your memory and your cache latencies, right.

00:51:08.010 --> 00:51:16.600
So memory, you're in hundreds of cycles, you
know, the L3 cache, the last level cache,

00:51:16.599 --> 00:51:21.110
you're in the tens of cycles of latency for those loads.

00:51:21.110 --> 00:51:27.050
You know you get into the L2, you're
in you know 10, 12, 14 type cycles.

00:51:27.050 --> 00:51:30.600
You get into the L1 and now you're in the 3 and 4 cycles.

00:51:30.599 --> 00:51:38.980
So, based on that you can start to figure out where are
you actually being bound in terms of your memory accesses

00:51:38.980 --> 00:51:43.829
and then you start to figure out how that correlates to
your application, how much data are you actually loading?

00:51:43.829 --> 00:51:49.980
Are you loading more data repeatedly than what you
really need to even load from a processing standpoint?

00:51:49.980 --> 00:51:54.510
So you're really correlating that to kind
of the known constraints based on the L2,

00:51:54.510 --> 00:51:59.780
L1 last level of cache latency in
understanding your branch prediction.

00:51:59.780 --> 00:52:04.400
And then you have to kind of compensate
for this turbo and thermal impact.

00:52:04.400 --> 00:52:06.769
One of the things I talked about was turbo mode.

00:52:06.769 --> 00:52:08.909
The converse is true as well.

00:52:08.909 --> 00:52:15.559
If you actually have a fully loaded processor across
all cores and you're really doing extreme amount

00:52:15.559 --> 00:52:19.380
of computation, get that processor really, really hot.

00:52:19.380 --> 00:52:25.019
It will start the thermal throttle, so instead of getting 15
percent performance boost, you can actually see a reduction

00:52:25.019 --> 00:52:30.670
in frequency as well to compensate and try to keep
that processor within its thermal design limits.

00:52:30.670 --> 00:52:37.760
So, the other thing is to really start
out with the bigger bottlenecks first.

00:52:37.760 --> 00:52:44.060
It's really focused on the memory and cache bandwidth
and then starting from the execution units because that's

00:52:44.059 --> 00:52:48.230
where you're going to be loading that memory
into, you're going to be executing on that data

00:52:48.230 --> 00:52:50.740
and then start working back to the front end decoder.

00:52:50.739 --> 00:52:56.609
Rarely is the front end decoder your problem, but I
have seen applications that have had that as the issue

00:52:56.610 --> 00:52:59.660
where the execution units are mostly idle.

00:52:59.659 --> 00:53:03.519
And then beware of the cause and effect.

00:53:03.519 --> 00:53:08.289
So, you know this is really focusing
on the clog and not the symptom.

00:53:08.289 --> 00:53:14.380
So if your bathtub is overflowing, OK, the symptom
is that the water is spilling out over the tub

00:53:14.380 --> 00:53:17.059
and fixing that isn't going to help anything.

00:53:17.059 --> 00:53:20.440
It's really the clog that's down on the
floor in the drain some place that you have

00:53:20.440 --> 00:53:23.309
to unclog and that's really the same effect.

00:53:23.309 --> 00:53:29.869
If you start clogging at the execution units or the
retirement location, then you start filling up those,

00:53:29.869 --> 00:53:34.420
all those other queues all the way back into the decoder
and then you, you know, the decoder stops decoding

00:53:34.420 --> 00:53:38.650
because there are no more places
to put the decoded instructions.

00:53:38.650 --> 00:53:43.769
And really start with simple events, so
know what you're measuring simple things,

00:53:43.769 --> 00:53:51.800
you know instructions retired,
L1 cache misses, L2 cache misses.

00:53:51.800 --> 00:53:59.310
You can get into very esoteric events but start with the
simple ones first and then when you start to hone in on

00:53:59.309 --> 00:54:05.019
where you think the bottleneck is, you can start
actually looking at some of the more, you know,

00:54:05.019 --> 00:54:14.750
sub event and more complex events to
try to focus in a little bit more.

00:54:14.750 --> 00:54:21.690
So, you know, really drill down using the performance
events in really understanding your workload's behavior.

00:54:21.690 --> 00:54:25.650
And in summary, the event monitoring
is a very powerful tool.

00:54:25.650 --> 00:54:34.099
Intel has invested a lot of time and a lot of effort and a
lot of silicon to put these performance counters in there.

00:54:34.099 --> 00:54:37.360
They're certainly not there because there's,
because we don't think they have value

00:54:37.360 --> 00:54:39.490
because they have a tremendous amount of value.

00:54:39.489 --> 00:54:43.339
We use it all the time internally to
investigate and understand the architecture

00:54:43.340 --> 00:54:46.370
and understand how applications are working.

00:54:46.369 --> 00:54:52.299
For me, I come from a position where a lot of times
I have no idea what an application is really doing.

00:54:52.300 --> 00:54:58.810
You know, it's some big monstrous thing that has, you know,
10, 20, 100 engineers that have been working on this thing

00:54:58.809 --> 00:55:03.400
for the last decade and so it's very hard to
understand exactly what the application is doing.

00:55:03.400 --> 00:55:08.389
But by using the performance event counters I
can actually do a fairly good characterization

00:55:08.389 --> 00:55:11.500
to get a better idea of how that application is working.

00:55:11.500 --> 00:55:15.679
And then once you understand where the
bottlenecks are, what your constraints are,

00:55:15.679 --> 00:55:18.989
then you can start using that to
help optimize your applications.

00:55:18.989 --> 00:55:26.339
It certainly supports very simple analysis
but also supports extremely complex analysis.

00:55:26.340 --> 00:55:33.470
And then the more skillful you are as a user
with the tool, with Shark, with the event,

00:55:33.469 --> 00:55:37.439
the more you know about your application,
the more you know about the tools,

00:55:37.440 --> 00:55:40.610
the better you are at actually becoming the expert.

00:55:40.610 --> 00:55:45.250
So, you know, use the tools, use what's available

00:55:45.250 --> 00:55:50.730
and certainly performance event
counters are a contributor to that.

00:55:50.730 --> 00:55:56.179
So become your organization's expert
using processor performance events.

00:55:56.179 --> 00:56:00.339
There are a number of references that you can actually use.

00:56:00.340 --> 00:56:06.700
The Intel's Architecture Software Developer's
Manual has all of the events and what they do.

00:56:06.699 --> 00:56:14.819
Shark User Guide has a list of the events as well and their
descriptions and then if you happen to be cross-platform,

00:56:14.820 --> 00:56:23.630
Intel VTune Performance Analyzer both on Linux
and Windows, have a lot of good material to go to.

00:56:23.630 --> 00:56:29.180
In fact the Intel Performance Analyzer actually
has ratios of events that have been documented

00:56:29.179 --> 00:56:34.519
so even though Shark doesn't necessarily give
you ratios explicitly, you can actually go look

00:56:34.519 --> 00:56:37.719
at the VTune event, see what ratios they're collecting.

00:56:37.719 --> 00:56:42.480
They give you a lot of hints about what those
ratios should look like in terms of what those--

00:56:42.480 --> 00:56:45.969
what the counts should be and whether
you have a problem or not,

00:56:45.969 --> 00:56:51.129
so definitely another good reference
to get started on performance events.