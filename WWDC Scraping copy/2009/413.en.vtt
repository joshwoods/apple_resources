WEBVTT

00:00:12.810 --> 00:00:14.179
>> My name is Wade Tregaskis.

00:00:14.179 --> 00:00:17.859
I work in the Developer Tools team, working
on performance tools and primarily Shark,

00:00:17.859 --> 00:00:20.339
which is of course the subject of today's talk.

00:00:20.339 --> 00:00:23.129
And how you can use that on Mac and iPhone applications.

00:00:23.129 --> 00:00:28.269
So this is the bouquet of things, hopefully
you're all here for at least one of these things.

00:00:28.269 --> 00:00:34.179
If you're not, you should be, but last
chance to run off to a different session.

00:00:34.179 --> 00:00:36.820
And I'm not going to go through
them, it's pretty self-explanatory.

00:00:36.820 --> 00:00:40.859
Let's just straight to what Shark can
do, how Shark can address these needs.

00:00:40.859 --> 00:00:42.299
Shark has a bunch of different tools.

00:00:42.299 --> 00:00:44.799
We're going to talk about only a few today,
but we're going to cover the main ones.

00:00:44.799 --> 00:00:47.029
The first is of course time profiling.

00:00:47.030 --> 00:00:51.710
This simply shows you where your CPU time
is going when your programs are executing.

00:00:51.710 --> 00:00:52.770
It's that simple.

00:00:52.770 --> 00:00:54.230
It's the most general tool.

00:00:54.229 --> 00:00:56.659
It applies in pretty much all situations.

00:00:56.659 --> 00:00:58.209
It's usually where you start profiling.

00:00:58.210 --> 00:01:02.359
And it's going to be the focus of the
first part of our presentation today.

00:01:02.359 --> 00:01:07.400
In the second part of our presentation, we'll look at
system trace, which is also a very useful tool generally,

00:01:07.400 --> 00:01:14.670
but it particularly excels at multithreading problems,
locking, anything you can kind of come across dealing

00:01:14.670 --> 00:01:17.210
with lots of threads and system interactions.

00:01:17.209 --> 00:01:21.649
System calls, VM faults, all those sort of things.

00:01:21.650 --> 00:01:26.950
Towards the end of our talk, we'll just briefly touch
on another two tools which together are basically

00:01:26.950 --> 00:01:31.299
about showing how you're using the memory in the
machine, how you're using the processor cache,

00:01:31.299 --> 00:01:33.530
whether you're fitting inside it effectively.

00:01:33.530 --> 00:01:38.420
And how you're using the processor buff, buss,
sorry, whether you're bandwidth limited for example.

00:01:38.420 --> 00:01:43.680
Now the last tool that I want to mention
is a tool by itself that you can run

00:01:43.680 --> 00:01:46.200
on your binary, but it's actually more than that.

00:01:46.200 --> 00:01:49.170
It's throughout Shark, it's a static analysis engine.

00:01:49.170 --> 00:01:55.849
This looks at the machine code of your program and does some
calculations and looks at it, looks for common problems,

00:01:55.849 --> 00:01:58.509
patents that we recognize, things we know about.

00:01:58.510 --> 00:02:02.270
And it'll provide you things like these tips,
which will tell you hey, there's something here,

00:02:02.269 --> 00:02:04.369
maybe you want to look at it, think about it.

00:02:04.370 --> 00:02:09.490
And often it'll even tell you what you can do to work
around that or try something else that might be faster.

00:02:09.490 --> 00:02:14.920
So that all said, let's look at just a few
things we should do before we actually go

00:02:14.919 --> 00:02:17.000
to the demo and start firing up Shark.

00:02:17.000 --> 00:02:19.210
First up, debugger applications.

00:02:19.210 --> 00:02:23.800
It's of course a pain to be trying to profile something
that's crashing or just generally misbehaving.

00:02:23.800 --> 00:02:28.469
But the more important thing is that if your program
is not actually doing what it's supposed to be doing,

00:02:28.469 --> 00:02:32.849
if it's not finished, it's not quite, you know
doing exactly what it'll do when you ship,

00:02:32.849 --> 00:02:36.750
then profiling it could be pointless or worse
it could be detrimental because what you see

00:02:36.750 --> 00:02:40.319
and what you optimize may not actually be meaningful,

00:02:40.319 --> 00:02:43.650
it may not be something that's a
problem when you finish the program.

00:02:43.650 --> 00:02:46.439
So just debug it first, get it
doing what it's supposed to be doing

00:02:46.439 --> 00:02:50.669
and then start worrying about performance and optimization.

00:02:50.669 --> 00:02:56.609
That all said and done, when you do go to profile it, you
need to ensure that you're profiling the release build.

00:02:56.610 --> 00:03:00.130
You want to have all your optimizations
on, any additional compile settings, flags,

00:03:00.129 --> 00:03:03.680
whatever it is that you do before you
ship your application to the end user.

00:03:03.680 --> 00:03:07.250
Again, you want to be profiling what the end user is seeing.

00:03:07.250 --> 00:03:08.509
With two exceptions.

00:03:08.509 --> 00:03:13.009
Note here, if you've turned on F limit frame
pointers, and if you don't know what that is,

00:03:13.009 --> 00:03:14.469
don't worry about it, it's off by default.

00:03:14.469 --> 00:03:18.919
But if you have turned it on, you want to
create a separate configuration for Shark

00:03:18.919 --> 00:03:21.839
that simply turns that one setting alone off.

00:03:21.840 --> 00:03:27.599
Simply because although it's a valid optimization, the
tradeoff is that, it prevents collection of call stacks,

00:03:27.599 --> 00:03:31.579
which is a real problem when you're
trying to analyze the performance.

00:03:31.580 --> 00:03:35.760
Secondly, you need to ensure that Shark can
see your source files, your symbol information,

00:03:35.759 --> 00:03:38.549
so you'll be able to see your function names and whatnot.

00:03:38.550 --> 00:03:45.230
Now again, if you're using Xcode, you've created a project
file with Xcode 3 or later, then this is automatic.

00:03:45.229 --> 00:03:46.619
You don't have to worry about this.

00:03:46.620 --> 00:03:50.730
However, if you're using our Xcode
file from a previous version of Xcode,

00:03:50.729 --> 00:03:56.379
or just for whatever reason you're not using Dwarf
with DSIMM as your debug format, you'll need to go in

00:03:56.379 --> 00:04:00.079
and tweak all the settings appropriately to make
sure you're not stripping your symbol information,

00:04:00.080 --> 00:04:03.350
you're actually generating debug info, that sort of thing.

00:04:03.349 --> 00:04:08.639
Now that all done, you can then
fire up Shark and start profiling.

00:04:08.639 --> 00:04:12.719
But although you can just profile sort of
speculatively and just start looking around,

00:04:12.719 --> 00:04:15.000
the best way to do it is to have in mind what you want.

00:04:15.000 --> 00:04:18.389
And that really boils down to some kind of
metric, something that you want to improve.

00:04:18.389 --> 00:04:23.189
This could be frames per second, time it
takes to complete some task, throughput.

00:04:23.189 --> 00:04:27.969
Whatever is appropriate for your program and
whatever seems to be the performance issue of course.

00:04:27.970 --> 00:04:32.840
Now having done that, you then of course want to
take a measurement, a baseline that you can compare

00:04:32.839 --> 00:04:37.389
against after you make changes to ensure not
just that you did actually make an improvement,

00:04:37.389 --> 00:04:40.939
but sort of quantify that and get an idea of your progress.

00:04:40.939 --> 00:04:45.209
And this is really to put it simply,
this is about coming up with a benchmark.

00:04:45.209 --> 00:04:51.799
Something that you can run reliably and reproducibly
on your application to get numbers after each change.

00:04:51.800 --> 00:04:56.579
And I just want to point out here, although we're
not going to go into the how to do it in the session

00:04:56.579 --> 00:05:00.329
because we don't have time, you can
automate Shark in a wide variety of ways.

00:05:00.329 --> 00:05:03.649
So if you come up with a benchmark that's a
Shell script or an Apple script or some kind

00:05:03.649 --> 00:05:10.419
of you know automated benchmark, you can actually have
Shark collect profiles automatically as part of that script.

00:05:10.420 --> 00:05:13.280
If you wanted, if you're interested
in that, check out the Shark manual.

00:05:13.279 --> 00:05:14.379
There's a whole section on it.

00:05:14.379 --> 00:05:16.769
But I'm afraid we don't have time for it today.

00:05:16.769 --> 00:05:18.740
But let's go straight to the demo machine.

00:05:18.740 --> 00:05:21.579
And fire up Shark and start taking some profiles.

00:05:21.579 --> 00:05:22.109
[ Background noise ]

00:05:22.110 --> 00:05:29.949
So I'm going to launch Shark.

00:05:29.949 --> 00:05:32.279
Now when you first open Shark, this is what you get.

00:05:32.279 --> 00:05:33.289
This is the main window.

00:05:33.290 --> 00:05:37.500
This is where you can start and stop
Shark control profiling using this button.

00:05:37.500 --> 00:05:40.600
Choose which tool you want to use, which configuration.

00:05:40.600 --> 00:05:46.580
And this is where you choose the target of your
profiling, either the whole system or a particular process

00:05:46.579 --> 00:05:51.359
or you know you can launch your process, specify
command line arguments, all that sort of thing.

00:05:51.360 --> 00:05:55.319
Now, if I wanted to for example
profile MDWorkA, I'd be set to go now.

00:05:55.319 --> 00:05:57.839
I just click start and off I go.

00:05:57.839 --> 00:06:02.229
In this case though for the first demo, what I'm
going to do is profile a little iPhone application.

00:06:02.230 --> 00:06:05.129
Now I apologize that I don't have a projector here.

00:06:05.129 --> 00:06:07.659
This is what it looks like, if you're off the back.

00:06:07.660 --> 00:06:10.630
It looks more interesting on the phone,
which has a microphone of course.

00:06:10.629 --> 00:06:11.899
It's just displaying the WAV form.

00:06:11.899 --> 00:06:14.060
So that's going to be my demo app.

00:06:14.060 --> 00:06:20.100
Now to profile that, I go to the sampling
menu and choose networks/iPhoneprofiling.

00:06:21.259 --> 00:06:24.819
Now it disables the local settings because
these apply only to the current machine.

00:06:24.819 --> 00:06:30.129
Instead I have a list of all the connected iPhones and any
Macs that are on the network, running Shark in remote mode.

00:06:30.129 --> 00:06:35.399
And I can just click use on whichever
one or more of those I want to control.

00:06:35.399 --> 00:06:42.399
It launches Shark on the device, gets it ready
for profiling and when it does that, in a second,

00:06:42.399 --> 00:06:44.509
I didn't appease the demo gods apparently.

00:06:44.509 --> 00:06:47.159
[ Period of silence ]

00:06:47.160 --> 00:06:49.740
Would you believe that this worked
perfectly every other time?

00:06:49.740 --> 00:06:51.240
[ Laughter ]

00:06:51.240 --> 00:06:57.290
All right, well anyway, well let me try a few things.

00:06:57.290 --> 00:06:57.790
[ Period of silence ]

00:06:57.790 --> 00:07:01.010
First off, go away Shark.

00:07:01.009 --> 00:07:09.990
[ Period of silence ]

00:07:09.990 --> 00:07:10.650
All right.

00:07:10.649 --> 00:07:14.909
So in theory, you click this and
a few seconds later, it'll work.

00:07:14.910 --> 00:07:17.439
[ Period of silence ]

00:07:17.439 --> 00:07:18.439
Oh, it figures.

00:07:18.439 --> 00:07:23.459
All right, this is why we take sessions ahead
of time, so we can just skip that whole step.

00:07:23.459 --> 00:07:29.129
Anyway, imagine that this was working, you'd
see the same thing here that you see up here.

00:07:29.129 --> 00:07:32.319
Under the configuration you can
choose which tool you want to use.

00:07:32.319 --> 00:07:36.379
Under the target you could choose
which process or the whole system.

00:07:36.379 --> 00:07:40.139
And then you could still use the start
and stop button to then control profiling.

00:07:40.139 --> 00:07:45.029
So let me go grab a session that I took earlier.

00:07:45.029 --> 00:07:50.029
Now I'm going to take, this is a
session I took on your iPhone 3GS.

00:07:50.029 --> 00:07:57.869
And we probably won't have time, but I want to point out
just while this is loading, oops, when I relaunch Shark.

00:07:57.870 --> 00:08:00.560
That the 3GS, it's a different processor.

00:08:00.560 --> 00:08:02.149
It has more memory.

00:08:02.149 --> 00:08:04.389
It's a very different phone.

00:08:04.389 --> 00:08:11.719
So if you are doing phone stuff and you're worried
about performance, you really want to grab a 3GS phone.

00:08:11.720 --> 00:08:12.520
Oh, come on.

00:08:12.519 --> 00:08:12.870
[ Period of silence ]

00:08:12.870 --> 00:08:12.930
Ah.

00:08:12.930 --> 00:08:13.030
[ Period of silence ]

00:08:13.029 --> 00:08:21.139
It's just one of those days.

00:08:21.139 --> 00:08:24.009
What's that?

00:08:24.009 --> 00:08:26.289
Reinstall Windows?

00:08:26.290 --> 00:08:26.790
Yeah.

00:08:26.790 --> 00:08:27.560
[ Laughter ]

00:08:27.560 --> 00:08:35.100
Ok, so this is a session that I took earlier on the
one in a million chance that this would go wrong.

00:08:35.100 --> 00:08:39.750
And this is something like what you're going
to see when you first profile your application.

00:08:39.750 --> 00:08:43.220
So this is, this is ready.

00:08:43.220 --> 00:08:47.290
We can now go through and we can start looking at
this, seeing what it's telling us, start optimizing.

00:08:47.289 --> 00:08:50.289
But before we do, I want to jump back
to the slides because I want to go

00:08:50.289 --> 00:08:52.689
through what time profiling is, just really quickly.

00:08:52.690 --> 00:08:55.540
Just so you understand what it is that you're
seeing and where this data is coming from.

00:08:55.539 --> 00:08:58.339
So we jump back to the slides.

00:08:58.340 --> 00:09:02.860
So the way time profiling works
is Shark comes along periodically,

00:09:02.860 --> 00:09:05.379
every millisecond, ten milliseconds, something like that.

00:09:05.379 --> 00:09:06.639
And it takes a sample.

00:09:06.639 --> 00:09:12.289
And in a nutshell a sample is just the call stacks
of the target threads at that moment in time.

00:09:12.289 --> 00:09:17.149
Now this is quite similar to GDB for example,
if you stop your program periodically

00:09:17.149 --> 00:09:19.340
to see what it's doing and look at the back trace.

00:09:19.340 --> 00:09:21.360
However, a little different.

00:09:21.360 --> 00:09:25.039
Shark will also record any kernel stacks or driver stacks.

00:09:25.039 --> 00:09:28.659
This is time spent in the kernel or the
drivers directly on behalf of your program.

00:09:28.659 --> 00:09:32.120
So that's something you'll see
that you might not be used to.

00:09:32.120 --> 00:09:36.659
But more than that, what makes this a profiling
tool is that we take all these call stacks

00:09:36.659 --> 00:09:39.459
and we combine them into what we call a call tree.

00:09:39.460 --> 00:09:42.670
And this is basically a union merge of all the call stacks.

00:09:42.669 --> 00:09:46.449
And this lets you see at a higher
level where you're spending your time,

00:09:46.450 --> 00:09:52.110
what functions are executing while you're profiling and
of course how you got to them through the call tree.

00:09:52.110 --> 00:09:55.050
Now along with that of course,
we record some statistics based

00:09:55.049 --> 00:09:59.539
on how many times we see that function
while we're profiling.

00:09:59.539 --> 00:10:03.469
Now this is probably pretty similar or pretty
familiar I should say to a lot of people

00:10:03.470 --> 00:10:08.019
if you've used other profiling tools,
CPU sampler, tools in other platforms.

00:10:08.019 --> 00:10:12.159
But one thing you may not be familiar
with is these two different numbers,

00:10:12.159 --> 00:10:14.610
these two different columns we have here, self and total.

00:10:14.610 --> 00:10:18.830
Now this is showing slightly different
things that are useful in different context.

00:10:18.830 --> 00:10:21.810
So let me just go over what they
are, just to make it just clear.

00:10:21.809 --> 00:10:23.719
Because it's not necessarily intuitive.

00:10:23.720 --> 00:10:28.180
Self includes only the time in the
function itself, not including any functions

00:10:28.179 --> 00:10:31.479
that it calls, just in that function itself.

00:10:31.480 --> 00:10:36.970
Total on the other hand includes not just the time
self, but also all that time in the sub functions.

00:10:36.970 --> 00:10:40.889
So let me just go through just an example,
just to make this a bit more concrete.

00:10:40.889 --> 00:10:46.620
If this is our sample code that we took this profile
from, then if you're looking at the prepare function here,

00:10:46.620 --> 00:10:51.259
the 20% of self time corresponds roughly to this loop here.

00:10:51.259 --> 00:10:55.620
This is inside prepare itself doing
math or something, whatever it's doing.

00:10:55.620 --> 00:11:00.240
The other 20% of prepares total time is
actually in the MALIK function that it calls.

00:11:00.240 --> 00:11:03.680
So that corresponds to this bit of code just here.

00:11:03.679 --> 00:11:06.709
Now I wanted to point this out really
clearly because it's, as I said,

00:11:06.710 --> 00:11:09.540
it's really important to understand what the difference is

00:11:09.539 --> 00:11:12.309
because you use these different
columns in different contexts.

00:11:12.309 --> 00:11:16.899
And sometimes you use them together to
nut out some really tricky problems.

00:11:16.899 --> 00:11:20.909
Now that all said, that's pretty
much how time profiling works.

00:11:20.909 --> 00:11:25.139
But there are some caveats, now the
most important thing to understand

00:11:25.139 --> 00:11:27.699
about time profiling in Shark is that it's not a trace.

00:11:27.700 --> 00:11:29.160
It is not a function trace.

00:11:29.159 --> 00:11:34.009
If you used other tools, they might present data that looks
very similar, but they could be using function tracing.

00:11:34.009 --> 00:11:35.669
It's a different method.

00:11:35.669 --> 00:11:38.149
And there's tradeoffs, there's pros and cons to each one.

00:11:38.149 --> 00:11:46.139
But what this means really in Shark is that for example if
you have this execution here on the top, the top one here,

00:11:46.139 --> 00:11:47.929
that's what your program is really doing over time.

00:11:47.929 --> 00:11:52.279
Now the white lines we're using to denote
where Shark comes in and takes a sample.

00:11:52.279 --> 00:11:55.709
What Shark is going to see as a result
is actually this timeline on the bottom.

00:11:55.710 --> 00:11:57.210
Now that looks different obviously.

00:11:57.210 --> 00:11:58.550
So why is that?

00:11:58.549 --> 00:11:59.849
What is happening here?

00:11:59.850 --> 00:12:05.070
In the first case, you've got these functions
that are running for a relatively brief period.

00:12:05.070 --> 00:12:07.420
So they actually fall between two sample points.

00:12:07.419 --> 00:12:08.639
So Shark just doesn't see them.

00:12:08.639 --> 00:12:12.059
It doesn't know they existed, it
doesn't account for them at all.

00:12:12.059 --> 00:12:17.349
On the other hand, you have these functions here on
the other side, which maybe be quite short functions,

00:12:17.350 --> 00:12:22.379
may have only taken microseconds, yet they just
happen to fall coincidently on a sample point.

00:12:22.379 --> 00:12:25.689
Now Shark can only assume when it sees them
that they were running for the entire interval

00:12:25.690 --> 00:12:27.460
and that's why it spreads them out like that.

00:12:27.460 --> 00:12:30.480
So in this case, it's over accounting for them.

00:12:30.480 --> 00:12:33.090
Now the good thing is of course that
these things are complete opposites

00:12:33.090 --> 00:12:36.019
and when you take a number of samples,
they actually average out.

00:12:36.019 --> 00:12:38.569
And this is the basis of you know statistical sampling.

00:12:38.570 --> 00:12:42.390
If you get enough samples, a statistically
significant number of samples,

00:12:42.389 --> 00:12:46.929
then it's still showing you what's actually
a very accurate picture of what's going on.

00:12:46.929 --> 00:12:51.079
Nonetheless, there is you know one
down side that we just can't avoid

00:12:51.080 --> 00:12:54.940
with doing this statistically as
opposed to actual function tracing.

00:12:54.940 --> 00:13:00.280
And that is that we don't know in Shark how
often you call a function, how many times.

00:13:00.279 --> 00:13:04.069
We can tell you how long you spend in
it in total but not whether that was

00:13:04.070 --> 00:13:07.379
in all one call or spread out over a million calls.

00:13:07.379 --> 00:13:11.529
And that sort of thing can be very useful, but it's not
something that Shark's time profiling will tell you.

00:13:11.529 --> 00:13:14.769
So, just something to be aware of.

00:13:14.769 --> 00:13:20.939
The second thing, the second caveat is that time
profiling, it's nice and simple, it's very general,

00:13:20.940 --> 00:13:23.430
but it's really just showing you
where you're spending your time.

00:13:23.429 --> 00:13:29.269
It's not going to necessarily tell you straight out why
you're spending your time there, why your code is slow,

00:13:29.269 --> 00:13:32.799
why your algorithm, you know is doing what it's doing.

00:13:32.799 --> 00:13:35.979
So of course knowing where you're
spending your time, that's half the battle.

00:13:35.980 --> 00:13:39.860
But the other half is interpreting those
results and knowing how to fix them.

00:13:39.860 --> 00:13:45.500
And that just comes down to experience and a bit of
wisdom and having dealt with similar problems before.

00:13:45.500 --> 00:13:50.460
So we can't instill that in you in just one short
session, but in the demos we'll run through a bunch

00:13:50.460 --> 00:13:54.300
of common problems that you might see
and also hopefully give you a kick start,

00:13:54.299 --> 00:13:57.169
let you know what you're seeing in
a general typically time profile.

00:13:57.169 --> 00:14:02.019
And along those lines, I just want to
lay down some sort of rules of thumb.

00:14:02.019 --> 00:14:05.909
These aren't without exception, but
they're general things to keep in mind,

00:14:05.909 --> 00:14:08.870
sort of goals to have when you're time profiling.

00:14:08.870 --> 00:14:10.789
The first one is just intuitive.

00:14:10.789 --> 00:14:15.399
You want to make any given process,
any given task take less time.

00:14:15.399 --> 00:14:17.340
That's you know obvious.

00:14:17.340 --> 00:14:22.530
An extension of that, if you're going for throughput
in particular, you want to be using the CPUs as much

00:14:22.529 --> 00:14:25.459
as possible to get as much work as physically possible done.

00:14:25.460 --> 00:14:26.800
Then you want to avoid blocking.

00:14:26.799 --> 00:14:31.379
Blocking is where you are waiting on something,
so you're sitting there, you're not using the CPU.

00:14:31.379 --> 00:14:32.860
It's time wasted.

00:14:32.860 --> 00:14:42.419
Now a third rule and this is a little more nuanced, it's
really important to prioritize the changes you make,

00:14:42.419 --> 00:14:48.519
the optimizations you attempt based not just on how
much time is going into a particular part of the code,

00:14:48.519 --> 00:14:51.389
but how much effort it's going to take to optimize that.

00:14:51.389 --> 00:14:54.689
Meaning if you get, if it's going to take
you a month to optimize one bit of code,

00:14:54.690 --> 00:15:00.250
but in that same month you could optimize ten other
bits of code that all told will have a greater benefit,

00:15:00.250 --> 00:15:02.899
then obviously you do the ten little bits of code.

00:15:02.899 --> 00:15:08.230
So you know fairly intuitive, but it's something that takes
a bit of practice to recognize different types of problems,

00:15:08.230 --> 00:15:13.960
know which ones are going to take the most amount of time
and which ones are likely to give you the most benefit.

00:15:13.960 --> 00:15:16.540
But with these rules in mind, now
let's go back to the demo machine.

00:15:16.539 --> 00:15:21.009
And let's start looking at the results
that we talked just a moment ago.

00:15:21.009 --> 00:15:27.429
Or at least that I talked a while ago, but
let's pretend that they came from a moment ago.

00:15:27.429 --> 00:15:32.439
So this is our call tree, this is actually
looking at the call tree from the bottom up.

00:15:32.440 --> 00:15:35.630
So we're seeing all of the leaf
functions in the tree which is

00:15:35.629 --> 00:15:39.129
where we're actually sitting there
spinning and doing our time.

00:15:39.129 --> 00:15:42.220
And if we expand these, we can see how we got there.

00:15:42.220 --> 00:15:46.060
So drawer view, calls drawOscilloscope
and we can go right back up the tree,

00:15:46.059 --> 00:15:51.429
all the way right back to our main
function where everything originates.

00:15:51.429 --> 00:15:53.829
Now, let's look at this particular example.

00:15:53.830 --> 00:15:57.440
So we've got drawOscilloscope, that's
taking up nearly 17% of our time.

00:15:57.440 --> 00:16:01.880
So that's obviously something that we want
to look at, that's right up here at the top.

00:16:01.879 --> 00:16:05.250
But above that we have this GLI destroy context.

00:16:05.250 --> 00:16:07.019
Now this is part of the GL engine.

00:16:07.019 --> 00:16:10.699
I don't particularly know what
it is, I'm not an OpenGL expert.

00:16:10.700 --> 00:16:15.490
But it's taking up 20% of my time, so
I really want to know what's going on.

00:16:15.490 --> 00:16:19.940
Now if I look at what called it,
supposedly it called itself.

00:16:19.940 --> 00:16:22.950
Seems strange, and it called itself again and again.

00:16:22.950 --> 00:16:25.879
I'm pretty sure this function's not recursive.

00:16:25.879 --> 00:16:30.409
And obviously this is a bit of a canned example
because I know that this function is not recursive.

00:16:30.409 --> 00:16:33.039
So this is clearly not accurate.

00:16:33.039 --> 00:16:36.889
Now, in situations like this where I'm
a bit suspect about what I'm seeing,

00:16:36.889 --> 00:16:41.449
a really good way to actually verify this
is to go over to the advance drawer here.

00:16:41.450 --> 00:16:46.350
Now if this isn't visible, you can bring it up by
going to the window menu and show advanced settings.

00:16:46.350 --> 00:16:48.279
And I'm going to turn on show symbol length.

00:16:48.279 --> 00:16:54.009
Now this adds a length column and
this is showing me the length in bytes

00:16:54.009 --> 00:16:56.049
of the actual machine code for this function.

00:16:56.049 --> 00:16:58.549
Not the source code, but the machine code.

00:16:58.549 --> 00:17:05.029
So this is telling me that GLI destroy
context is 1.1 megabytes for one function.

00:17:05.029 --> 00:17:07.029
Yeah. I'm going to call shenanigans on that.

00:17:07.029 --> 00:17:09.269
I don't believe it for one second.

00:17:09.269 --> 00:17:16.859
I happen to know from digging into this deeper,
so this is just a, just a tip that this is bogus,

00:17:16.859 --> 00:17:21.789
this is completely wrong because this library
doesn't export all of its symbol information.

00:17:21.789 --> 00:17:26.849
So Shark looks at it, it sees a sample in it and
it just picks the nearest function that it can see.

00:17:26.849 --> 00:17:31.679
Now in this case, because there's so little symbol
info, it's picked one that probably completely off.

00:17:31.680 --> 00:17:33.820
It's probably not calling this function at all.

00:17:33.819 --> 00:17:35.210
So this is, this is pretty useless.

00:17:35.210 --> 00:17:37.120
We can't see what's going on inside this library.

00:17:37.119 --> 00:17:38.729
The time spent is accurate.

00:17:38.730 --> 00:17:41.880
But this call stack is, is meaningless.

00:17:41.880 --> 00:17:43.020
Now that's a bummer.

00:17:43.019 --> 00:17:45.379
That's you know not great.

00:17:45.380 --> 00:17:48.540
But in any case, this is a system library.

00:17:48.539 --> 00:17:51.889
This is something that you can't even look at
the source code, let alone make any changes to.

00:17:51.890 --> 00:17:56.630
So there's not necessarily a lot of merit in being
able to see what's going on inside it anyway.

00:17:56.630 --> 00:17:58.890
What we really care about is how we're calling it.

00:17:58.890 --> 00:18:04.250
What API we're using, because that's something
that we can change in our application to optimize.

00:18:04.250 --> 00:18:07.269
So what I'm going to do is make use of Shark's data mining.

00:18:07.269 --> 00:18:11.039
Now there's a lot of different things
you can do, little data mining things.

00:18:11.039 --> 00:18:15.659
Now in this case, what I want to do is charge, sorry
I should say this is a contextual menu obviously

00:18:15.660 --> 00:18:19.730
so I just right click on this, on
the library or the function name.

00:18:19.730 --> 00:18:21.710
In this case, I want to charge this library to its cause.

00:18:21.710 --> 00:18:27.299
Now what that means is it's going to take all the samples
that we're seeing in this library and instead account

00:18:27.299 --> 00:18:31.940
for them in whatever other library or whatever other
function in our code actually called into there.

00:18:31.940 --> 00:18:37.759
So when I do that, you can see that there's no more
GL engine entries here and that function is gone.

00:18:37.759 --> 00:18:42.750
Instead, we have this new GL drawer
arrays which wasn't even visible before

00:18:42.750 --> 00:18:49.130
and our drawOscilloscope method is now showing that
it's taking 17% when we account for that GL engine time.

00:18:49.130 --> 00:18:52.900
Now in this case I can see that GL Drawer
Arrays, see it reckons its 36 bytes long,

00:18:52.900 --> 00:18:55.610
that's much more plausible, so I'm going to believe this.

00:18:55.609 --> 00:18:59.779
But again, in a similar sort of
thing, I can't do much about this.

00:18:59.779 --> 00:19:03.509
I don't really care that GL Drawer
Arrays is taking 19% itself.

00:19:03.509 --> 00:19:06.029
I care more about the fact that I'm calling it.

00:19:06.029 --> 00:19:09.720
So what I'm going to do is charge
this again and now you can see

00:19:09.720 --> 00:19:13.319
that this drawOscilloscope function,
that is taking up 40% of my time.

00:19:13.319 --> 00:19:16.720
So clearly that's where I was calling GL Drawer Arrays.

00:19:16.720 --> 00:19:22.289
And now I've sort of narrowed it down to of the code
that I can change, 40% of my time is spent here.

00:19:22.289 --> 00:19:26.149
This is obviously the place I'm going to
look first to really try and optimize.

00:19:27.259 --> 00:19:29.059
So let's optimize it.

00:19:29.059 --> 00:19:30.490
How do we know what's going on?

00:19:30.490 --> 00:19:31.400
Why is this slower?

00:19:31.400 --> 00:19:34.080
So I'm going to double click on it.

00:19:34.079 --> 00:19:36.539
And this takes me to the source of the function.

00:19:36.539 --> 00:19:41.670
And you can see that the source is annotated, it's
highlighted, showing me the breakdown of which lines

00:19:41.670 --> 00:19:43.580
within that function are taking the most time.

00:19:43.579 --> 00:19:47.509
Now these percentages here are of
the total for just this one function.

00:19:47.509 --> 00:19:52.879
So if 37% of this function's time is going into
GL Drawer Arrays, we kind of already figured that.

00:19:52.880 --> 00:19:59.500
But here we have 38% of our time on
the conditional part of a fault loop.

00:19:59.500 --> 00:20:02.670
Now that, that should strike most people
here as pretty weird I would imagine

00:20:02.670 --> 00:20:05.769
because you don't normally expect your
fault loops conditions to take time.

00:20:05.769 --> 00:20:07.319
You'd expect this math to take time.

00:20:07.319 --> 00:20:09.970
This is doing some floating-point calculations.

00:20:09.970 --> 00:20:13.170
And we see that it's taking some
time but not nearly as much as this.

00:20:13.170 --> 00:20:15.560
So what's going on here?

00:20:15.559 --> 00:20:18.700
In this case, just looking at the source isn't enough.

00:20:18.700 --> 00:20:21.830
I don't, I can't tell just from
looking at this what is going on.

00:20:21.829 --> 00:20:26.849
So what I'm going to do is dig down another, another
level and actually look at the assembly as well.

00:20:26.849 --> 00:20:31.000
You can see that the highlighted line here
corresponds to these highlighted symbols here.

00:20:31.000 --> 00:20:34.299
If I choose a different line, it highlights different ones.

00:20:34.299 --> 00:20:36.899
So this is our loop here, this indented block just here.

00:20:36.900 --> 00:20:38.580
And this is the condition part.

00:20:38.579 --> 00:20:44.769
And you can see now that there's one instruction
here is taking up nearly 36% of this function's time.

00:20:44.769 --> 00:20:49.750
Now, although this may be a bit scary because
this is all strange pneumonics and lots of these,

00:20:49.750 --> 00:20:52.430
I don't even know ARM assembly myself to be honest.

00:20:52.430 --> 00:20:56.299
So I don't expect you to and the good
thing is you don't necessarily need to.

00:20:56.299 --> 00:21:01.480
In this case, I was able to figure out what was going
on without ever looking up an ARM reference manual.

00:21:01.480 --> 00:21:05.049
I looked at this instruction which I
guessed was some kind of branch instruction,

00:21:05.049 --> 00:21:08.909
presumably this is what loops back
to the start of our loop here.

00:21:08.910 --> 00:21:14.160
And I saw over here that Shark had quite a few
things to say about this, this little loop.

00:21:14.160 --> 00:21:17.700
These exclamation marks are Shark's
static analysis engine that we talked

00:21:17.700 --> 00:21:20.340
about before, telling me that it's noticed something.

00:21:20.339 --> 00:21:22.379
Something that might be interesting.

00:21:22.380 --> 00:21:27.040
So for this particular problem, I noticed those run
just here, it's not quite on this line, but it's nearby.

00:21:27.039 --> 00:21:28.809
So I click on that.

00:21:28.809 --> 00:21:32.869
And I, don't worry about reading through the details
of this, this is just specific to this exact problem.

00:21:32.869 --> 00:21:39.279
But the important thing is that this is telling me
that this takes a minimum of 20 cycles to c omplete.

00:21:39.279 --> 00:21:41.789
So this is probably accounting for some of that time.

00:21:41.789 --> 00:21:46.109
There's like a delay here after we
execute this before we can execute this.

00:21:46.109 --> 00:21:53.149
Even more useful, even more telling was when I
expand this so you can see, this stall thing,

00:21:53.150 --> 00:21:54.630
you'll see a lot of these throughout your code.

00:21:54.630 --> 00:21:59.330
They're not necessarily bad but they are when you get
to big numbers in the, you know double digits here.

00:21:59.329 --> 00:22:02.839
So this is telling me that before
this instruction could execute,

00:22:02.839 --> 00:22:07.119
every single time through the loop,
it had to wait 51 clock cycles.

00:22:07.119 --> 00:22:09.789
Now that's 51 clock cycles where nothing else was happening.

00:22:09.789 --> 00:22:11.849
It's just wasted time.

00:22:11.849 --> 00:22:17.309
The reason for this is very esoteric and it's related
through the particular chip that's in the 3GS.

00:22:17.309 --> 00:22:19.379
And I'm not going to go through the details here.

00:22:19.380 --> 00:22:25.670
But suffice to say this indicates to me why it's happening,
what's going on at least in a broad sense and if I go

00:22:25.670 --> 00:22:29.670
through and read this in detail, I can actually
dig down and I figured out what's going on.

00:22:29.670 --> 00:22:34.200
Again it's specific to this architecture
and I was able to figure out how to fix it.

00:22:34.200 --> 00:22:37.860
Now I don't want to go through the
details of how to fix this one problem.

00:22:37.859 --> 00:22:39.969
But that's the general idea.

00:22:39.970 --> 00:22:43.680
You find out what functions you're spending most of
your time in and then you figure out why by looking

00:22:43.680 --> 00:22:47.640
at the source shoe and if necessary
going right down to the assembly.

00:22:47.640 --> 00:22:53.160
I'll point out though, just before I leave this,
if you do need to know what the assembly means,

00:22:53.160 --> 00:22:56.310
what these pneumonics are, you can
highlight the instruction you're interested

00:22:56.309 --> 00:22:58.579
in and click the ASN help button down here.

00:22:58.579 --> 00:23:02.949
And this pops up the instructions that
reference that manual for that instruction.

00:23:02.950 --> 00:23:07.470
[ Applause ]

00:23:07.470 --> 00:23:13.029
So, in fact, perhaps you've realized, this is actually
new in Shark the ARM instruction set reference.

00:23:13.029 --> 00:23:15.720
We've had the Intel in PowerPC ones for a while.

00:23:15.720 --> 00:23:20.630
So that's a good way to just sort of
ease yourself into it, if you have to.

00:23:20.630 --> 00:23:25.350
Alright, so let's assume that we are either going
to optimize that or we did optimize it or whatever.

00:23:25.349 --> 00:23:27.859
But we no longer want to look at drawOscilloscope.

00:23:27.859 --> 00:23:31.169
We want to look at the next biggest
thing, the next hot spot.

00:23:31.170 --> 00:23:36.759
What I'm going to do is use data mining again, this time
I'm going to remove call stacks with that function in it.

00:23:36.759 --> 00:23:41.099
This is going to remove those entirely, just like
charging did except it doesn't bother to account

00:23:41.099 --> 00:23:43.209
for them anywhere, they just filtered out completely.

00:23:43.210 --> 00:23:49.519
So when I do that, that function disappears and all our
percentages sort of just scale up to fill in that space.

00:23:49.519 --> 00:23:52.139
And now I can move on to the next thing.

00:23:52.140 --> 00:23:56.420
Now, having said that, the next
thing is this unknown library.

00:23:56.420 --> 00:24:01.620
Now you may see this because Shark doesn't have symbol
information, well that's what it means and that may be

00:24:01.619 --> 00:24:05.029
that Shark couldn't find the symbols for
your application or something like that.

00:24:05.029 --> 00:24:13.500
However, on the phone, there's one particular library as
such that you don't have symbol information for at all

00:24:13.500 --> 00:24:16.079
and that's the kernel and all the drivers.

00:24:16.079 --> 00:24:21.259
So I can see here I have symbols for my application,
I have symbols for all these other system libraries.

00:24:21.259 --> 00:24:26.369
It's a pretty good guess, a pretty good bet that
this is the kernel time that I talked about.

00:24:26.369 --> 00:24:30.289
And if you expand this call tree,
it's just unknowns all the way down.

00:24:30.289 --> 00:24:31.700
And then it's totals.

00:24:31.700 --> 00:24:33.250
So that's not useful to us.

00:24:33.250 --> 00:24:36.440
It would be nice to be able to see that, but we can't.

00:24:36.440 --> 00:24:41.480
So I'm pointing it out simply so when you see this
in your own profiles, you know kind of what it is

00:24:41.480 --> 00:24:43.400
and that there's not a lot you can do about it

00:24:43.400 --> 00:24:48.110
Just ignore it and it's not necessarily
code you can change directly anyway.

00:24:48.109 --> 00:24:51.389
But you can influence it by looking at other things.

00:24:51.390 --> 00:24:57.980
And there's some other tools actually that
we'll use later than can help you out with this.

00:24:57.980 --> 00:25:00.240
Now, so ignoring that one.

00:25:00.240 --> 00:25:02.640
You see that there's a whole bunch of
other things here that we're doing.

00:25:02.640 --> 00:25:06.220
There's a lot of audio sort of stuff happening here.

00:25:06.220 --> 00:25:10.620
And going through every single one of these, one by
one, looking at the source, it's one way to do it.

00:25:10.619 --> 00:25:11.799
It's a little tedious though.

00:25:11.799 --> 00:25:14.730
I don't want to have to analyze every single function.

00:25:14.730 --> 00:25:17.539
Instead I could step back up through
and see how I got there,

00:25:17.539 --> 00:25:20.539
where I'm calling this from, try
and get kind of a bigger picture.

00:25:20.539 --> 00:25:23.569
But a much simpler way to do that
is to just switch over to tree view.

00:25:23.569 --> 00:25:27.319
So I go down to the popup menu here and choose tree view.

00:25:27.319 --> 00:25:32.079
This is viewing the exact same call tree
except now I'm looking at it from the top down.

00:25:32.079 --> 00:25:35.949
So I'm starting at the, the main
function or your pthread body and drilling

00:25:35.950 --> 00:25:38.190
down through all the functions progressively.

00:25:38.190 --> 00:25:42.500
And if I fold some of these up, this gives me a
much better high-level idea of what's going on.

00:25:42.500 --> 00:25:45.900
So for algorithms that encompass
more than just one function,

00:25:45.900 --> 00:25:49.019
this is a great way to see how
much the total time is all told.

00:25:49.019 --> 00:25:49.539
[ Period of silence ]

00:25:49.539 --> 00:25:53.619
And I mean that's pretty much it.

00:25:53.619 --> 00:25:58.559
That's, that's how you use heavy view, how you use
tree view, you've seen how to drill down into things.

00:25:58.559 --> 00:26:02.649
I want to move on to a second demo application we have.

00:26:02.650 --> 00:26:05.620
Now this is called Image Aware.

00:26:05.619 --> 00:26:08.839
It's a demo app you may have seen in some other sessions.

00:26:08.839 --> 00:26:11.089
We've been banding it around a bit this year.

00:26:11.089 --> 00:26:13.839
If you haven't seen it, let me just run it once.

00:26:13.839 --> 00:26:20.049
All it does is it has a bunch of PDF files on disk, which
it reads in, it goes through every single page of them,

00:26:20.049 --> 00:26:22.450
generates a little thumbnail for them
and then sticks them into this view.

00:26:22.450 --> 00:26:23.019
That's all it does.

00:26:23.019 --> 00:26:26.759
So this is something that you would
think would be very parallelizable.

00:26:26.759 --> 00:26:30.269
You could make it concurrent and
we tried a few different methods.

00:26:30.269 --> 00:26:34.019
Ultimately settling on GCD, of course, the new and shiny.

00:26:34.019 --> 00:26:37.000
Now, notice that this is taking
about 6 seconds with just one thread.

00:26:37.000 --> 00:26:39.549
That's the original single threaded implementation.

00:26:39.549 --> 00:26:42.589
So GCD, this is a Mac Pro by the
way, top of the line Mac Pro.

00:26:42.589 --> 00:26:44.889
So 16 virtual cores.

00:26:44.890 --> 00:26:46.940
So this should be 16 times faster, right?

00:26:46.940 --> 00:26:50.830
I wouldn't have asked the question
if it was going to be, would I?

00:26:50.829 --> 00:26:53.429
2.2 seconds, so it's over twice as fast.

00:26:53.430 --> 00:26:54.259
It's an improvement.

00:26:54.259 --> 00:26:58.950
It was definitely a big improvement,
definitely worth doing, but it's not 16 times.

00:26:58.950 --> 00:27:01.340
And this is our motivation for profiling it now.

00:27:01.339 --> 00:27:05.109
We want to know why we didn't get
nearly as much speed up as we expected.

00:27:05.109 --> 00:27:11.619
So what I'm going to do again is I'm going to take a time
profile of this, just so I can see where our time is going.

00:27:11.619 --> 00:27:16.569
And I'm going to use the keyboard shortcut for
this, the option escape to start and stop profiling,

00:27:16.569 --> 00:27:20.799
just so that I can use the mouse instead to
control this application at the same time.

00:27:20.799 --> 00:27:25.759
So I'll start profiling, start it
running, let it run to completion.

00:27:28.109 --> 00:27:29.289
My bad, I missed a step.

00:27:29.289 --> 00:27:34.779
I would like to profile my application
not MDWorker in this case.

00:27:34.779 --> 00:27:37.369
Try that again.

00:27:37.369 --> 00:27:37.559
[ Period of silence ]

00:27:37.559 --> 00:27:44.710
Alright. So now Shark will do its analysis, pull
in all the symbol info, all that kind of stuff.

00:27:44.710 --> 00:27:45.620
And here's our session.

00:27:45.619 --> 00:27:49.369
So let me just switch back to heavy view initially.

00:27:49.369 --> 00:27:51.159
Now the process is the same as before.

00:27:51.160 --> 00:27:52.960
You can go through this and look at different things.

00:27:52.960 --> 00:27:57.740
But I, I'm picking this particular example because it's
a very interesting one and this is a very common thing

00:27:57.740 --> 00:28:01.539
that you'll see, especially as you
go into multithreaded applications.

00:28:01.539 --> 00:28:05.789
Now we've got some time here spent in
inflate fast and this Lexus scan thing.

00:28:05.789 --> 00:28:07.680
This is actually reading the PDF, decompressing it.

00:28:07.680 --> 00:28:09.340
This is good in a sense.

00:28:09.339 --> 00:28:12.339
This is actually doing the work we intend to do.

00:28:12.339 --> 00:28:14.059
And we've got some more stuff down here and whatever.

00:28:14.059 --> 00:28:17.460
But of course the biggest two things are these two here.

00:28:17.460 --> 00:28:22.519
Now these aren't, these are kernel functions, they're
not anything that we're calling intentionally.

00:28:22.519 --> 00:28:25.509
And I want to explain what they are
because you'll see them sometimes.

00:28:25.509 --> 00:28:32.779
This set interrupts enabled function typically
indicates that you're context switching a lot.

00:28:32.779 --> 00:28:36.019
And our context switching isn't
necessarily bad, but it has overhead.

00:28:36.019 --> 00:28:39.539
So if you do it a lot, that overhead
adds up and it slows you down.

00:28:39.539 --> 00:28:44.129
So that's, that's the first hint that something
might be going on that we could optimize.

00:28:44.130 --> 00:28:46.090
The second this is this VM Map stuff.

00:28:46.089 --> 00:28:48.599
Now this is virtual memory, VM.

00:28:48.599 --> 00:28:50.579
It's to do with VM faults.

00:28:50.579 --> 00:28:56.859
But this is not particularly helpful because this is
the end of the call stack, this is all kernel stuff.

00:28:56.859 --> 00:29:03.089
So alright, we've got 10% of our time spent here, but
time profiles aren't really telling us a whole lot.

00:29:03.089 --> 00:29:05.299
Now this is actually leading on to system trace.

00:29:05.299 --> 00:29:12.500
But before we go to that, there is something else we can
do just with time profiling and I want to take a step back

00:29:12.500 --> 00:29:15.440
to explain why I decided to do it before I do.

00:29:15.440 --> 00:29:20.360
I look down here at this number here, this is how
many samples Shark took while it was profiling.

00:29:20.359 --> 00:29:26.490
So this is a 16 core machine, by default Shark samples
1000 times a second, so if we're running some Macs,

00:29:26.490 --> 00:29:31.190
we're using CPUs to the limit, we
should get 16000 samples a s`econd.

00:29:31.190 --> 00:29:35.940
This profile was for about two or three
seconds, so I'd expect at least 50000 samples.

00:29:35.940 --> 00:29:38.090
Instead we're getting 14000.

00:29:38.089 --> 00:29:41.179
This simply means that we're not
actually running for most of the time.

00:29:41.180 --> 00:29:46.009
A normal time profile only records a call
stack, a sample if the thread is running.

00:29:46.009 --> 00:29:50.500
If it's idle, if it's blocked, if it's
doing nothing, then it doesn't record it.

00:29:50.500 --> 00:29:55.190
This is telling me that while this is valid,
this is what we're doing when we are running,

00:29:55.190 --> 00:29:58.690
our biggest problem here is that
we're not running most of the time.

00:29:58.690 --> 00:30:05.340
So for that, I instead want to do a variation of the
basic time profile called Time Profile or Thread States.

00:30:05.339 --> 00:30:07.049
Now this produces data that looks much the same.

00:30:07.049 --> 00:30:10.549
I'm just going to record it while I talk.

00:30:10.549 --> 00:30:14.750
Except that it records whether we're running or not.

00:30:14.750 --> 00:30:17.319
Oops. Ignore that.

00:30:17.319 --> 00:30:18.829
And this shows us something very different.

00:30:18.829 --> 00:30:22.639
And this is actually much more useful because
now we're seeing over all our total time,

00:30:22.640 --> 00:30:27.290
not just the time we are running, but when
we were blocked, and if I can find it here,

00:30:27.289 --> 00:30:29.589
here's our inflate methods that we saw before.

00:30:29.589 --> 00:30:35.250
They were 6% of the time profile, but in the grand
scheme of things, in wall clock time if you like,

00:30:35.250 --> 00:30:37.549
they're a percent, they're insignificant.

00:30:37.549 --> 00:30:41.490
They're completely dwarfed by these
first five functions here.

00:30:41.490 --> 00:30:43.370
Now these are all system things.

00:30:43.369 --> 00:30:45.939
They're not things you generally call directly.

00:30:45.940 --> 00:30:51.420
But I want to explain all five of them because again
these are things that you're going to see in your code

00:30:51.420 --> 00:30:56.300
and it's really helpful to know what they mean because
they each indicate particular types of problems.

00:30:56.299 --> 00:30:58.220
The first one is workqueuekernreturn.

00:30:58.220 --> 00:31:00.950
If I pull this back, there's not a lot to it.

00:31:00.950 --> 00:31:06.870
This is a workqueue thread as you might find
underlying things like GCD and certain other things.

00:31:06.869 --> 00:31:08.149
It's used a lot throughout the system.

00:31:08.150 --> 00:31:12.640
They tend to pop up automatically,
system libraries use them a lot.

00:31:12.640 --> 00:31:18.290
Workqueuekernreturn is a system call that these
kind of sit in when they're waiting for more work.

00:31:18.289 --> 00:31:25.779
So this is telling me that 40% or 35% of my
time is spent waiting for something to do.

00:31:25.779 --> 00:31:29.809
So that's, that's the first indication of a problem,
that we're not perhaps enqueueing enough work

00:31:29.809 --> 00:31:32.889
or we have some kind of bottleneck
that's actually preventing us

00:31:32.890 --> 00:31:37.620
from getting a full 16 cores' worth of things to do.

00:31:37.619 --> 00:31:43.149
This next thing, assumingforwaitsignaltrap, if
I dig down it's perhaps more self-explanatory.

00:31:43.150 --> 00:31:47.670
This is called from within the locking
functions in the pthreads library.

00:31:47.670 --> 00:31:50.320
And it basically says that there's lock contention.

00:31:50.319 --> 00:31:53.779
Multiple threads are trying to take
the same lock at the same time.

00:31:53.779 --> 00:31:59.420
And all the time spent in here is waiting for that lock
to be released by another thread so we can move forward.

00:31:59.420 --> 00:32:00.759
Again this is dead time.

00:32:00.759 --> 00:32:02.829
This means that we're waiting, we're doing nothing useful.

00:32:02.829 --> 00:32:04.019
And this is bad.

00:32:04.019 --> 00:32:06.039
You pretty much never want to see this in your programs.

00:32:06.039 --> 00:32:08.609
This indicates lock contention, which we don't want.

00:32:08.609 --> 00:32:10.419
And it's just wasted time.

00:32:10.420 --> 00:32:16.840
Similarly this one is a slightly different pthread, kind
of mutual exclusion thing, pthread condition variables.

00:32:16.839 --> 00:32:17.419
Same deal.

00:32:17.420 --> 00:32:21.690
It represents time that we're waiting
for that condition variable to signal us.

00:32:21.690 --> 00:32:24.539
We don't really want to be spending any time in that.

00:32:24.539 --> 00:32:28.149
Kevent, much the same kind of thing.

00:32:28.150 --> 00:32:31.660
Again you can probably guess that
this underlies GCD as well.

00:32:31.660 --> 00:32:35.490
This is again, it's in the, it's sitting in the kernel
when it's doing this, waiting for an event to come in.

00:32:35.490 --> 00:32:37.880
So this is idle time.

00:32:37.880 --> 00:32:39.590
And there's another assumingforwardfunction.

00:32:39.589 --> 00:32:41.419
So I think you get the idea at this point.

00:32:41.420 --> 00:32:46.220
All of these things are telling me that most
of our time we're waiting and more importantly,

00:32:46.220 --> 00:32:49.730
most of our time we're waiting on a
lock or a pthread condition variable.

00:32:49.730 --> 00:32:55.990
So I would say at a guess, that the big
problem with this program is lock contention.

00:32:55.990 --> 00:33:01.940
Now in this time profile, I can drill back through these
functions to see what's using that lock and in this case,

00:33:01.940 --> 00:33:04.710
it's this rendering library is where most of the time is.

00:33:04.710 --> 00:33:06.730
Now the time profile here is useful.

00:33:06.730 --> 00:33:10.819
I can drill all the way back to
my own code, right down here.

00:33:10.819 --> 00:33:18.419
So you can see this is you know, somewhere underneath
the code that actually creates the pages in memory.

00:33:18.420 --> 00:33:20.190
And I can perhaps optimize this.

00:33:20.190 --> 00:33:26.400
I can look at what I'm doing here, try and find a different
API, maybe there's a more efficient way to do this.

00:33:26.400 --> 00:33:33.940
However, this is kind of a limitation much like it
doesn't tell me how many times I call a function.

00:33:33.940 --> 00:33:36.640
Time profiler doesn't tell me how
many times I try and take a lock.

00:33:36.640 --> 00:33:42.850
It's not telling me if I try to take this lock
once and it's simply blocked for seconds at a time.

00:33:42.849 --> 00:33:46.269
Or if I tried to take it a million
times and it was actually only blocking

00:33:46.269 --> 00:33:49.849
for a microsecond, but all those microseconds added up.

00:33:49.849 --> 00:33:55.909
So while I could go ahead right now and start optimizing
this or trying to optimize it, what I would instead do is go

00:33:55.910 --> 00:34:02.610
over to system trace because it might show
me something more indicative, more useful,

00:34:02.609 --> 00:34:05.639
giving a better idea of what I need to optimize.

00:34:05.640 --> 00:34:12.059
Now to do that, to talk to you about system trace, I want to
introduce Eric Miller, one of my coworkers, worked on Shark.

00:34:12.059 --> 00:34:18.639
And we'll just jump back to the slides, so he can take you
through what System Trace is and what the results mean.

00:34:18.639 --> 00:34:19.449
Eric.

00:34:19.449 --> 00:34:24.539
[ Applause ]

00:34:24.539 --> 00:34:26.699
>> Thanks Wade, appreciate it.

00:34:26.699 --> 00:34:28.859
Is this on?

00:34:28.860 --> 00:34:32.950
Yeah, I'm just going to dive right into system trace.

00:34:32.949 --> 00:34:37.149
I've got plenty of time.

00:34:37.150 --> 00:34:41.590
So, System Trace is all about what's
going on in the system with your threads.

00:34:41.590 --> 00:34:46.110
Which treads are running, where they're running,
when they're, on what core they're running on.

00:34:46.110 --> 00:34:55.420
And in particular, what priority they have over time and
why they change course or why they don't change course.

00:34:55.420 --> 00:34:57.099
Now there's several ways to review that data.

00:34:57.099 --> 00:35:01.199
And some of them are tables much like
system, much like the time profile.

00:35:01.199 --> 00:35:06.659
However the easiest way to look at
this is the System Trace timeline.

00:35:06.659 --> 00:35:12.649
In the System Trace timeline, each row in the
table represents one of the threads in the system.

00:35:12.650 --> 00:35:19.490
Each of the colored blocks represents some amount
of time a thread was running on a particular core.

00:35:19.489 --> 00:35:23.809
In this particular view, the colored
blocks represent the different processors

00:35:23.809 --> 00:35:27.420
that a given thread was running on over time.

00:35:27.420 --> 00:35:33.460
There are four kinds of events, major kinds
of events that System Trace can show you.

00:35:33.460 --> 00:35:36.619
The first of which, oh I skipped ahead of myself.

00:35:36.619 --> 00:35:44.400
With regard to this stuff, try to keep your
threads running on their cores as long as possible.

00:35:44.400 --> 00:35:50.240
The nice long blue line would be an excellent
example of a long tenure for a thread.

00:35:51.489 --> 00:35:53.729
And et cetera.

00:35:53.730 --> 00:35:56.829
So when we, when we're working with an
application we want a lot of throughput,

00:35:56.829 --> 00:36:00.809
we want to have the long tenure on our core for our thread.

00:36:00.809 --> 00:36:05.759
If you have an application on the other hand
that is meant to be responsive to external input,

00:36:05.760 --> 00:36:11.160
you would want your threads to block and have short
tenures when they're waiting for input and then have a,

00:36:11.159 --> 00:36:16.859
a tenure that's just long enough to get
the work done before the next input comes.

00:36:16.860 --> 00:36:22.130
So there's like I mentioned, there's these four
kinds of events, the first of which is system calls.

00:36:22.130 --> 00:36:27.619
System calls are basically requests from your,
your source code to the kernel to do work

00:36:27.619 --> 00:36:31.559
on your behalf or to make resources available.

00:36:31.559 --> 00:36:37.360
Some of the things that you might do quite regularly
that are system calls are read calls and select calls

00:36:37.360 --> 00:36:43.900
and IPC programs and the mock messages
which are pervasive throughout Mac OS X.

00:36:43.900 --> 00:36:51.170
System calls underlie locks so that pthreadmutext lock
you saw in Wade's trace is a form of a system call.

00:36:51.170 --> 00:36:58.409
And because these system calls underlie a number of
resources and you have to go and use protected things

00:36:58.409 --> 00:37:01.670
in the kernel, you can definitely
incur what we call a context switch,

00:37:01.670 --> 00:37:06.960
which is to say your thread can leave the core
it was running on, be blocked or set aside

00:37:06.960 --> 00:37:10.449
by the scheduler while your system call is serviced.

00:37:12.119 --> 00:37:19.460
So when things block, they can serialize
your otherwise parallelized thread program.

00:37:19.460 --> 00:37:25.320
When we look at system calls, there are a number
of little icons that'll show up in the timeline.

00:37:25.320 --> 00:37:29.420
The most common are the BFD system
calls which are the red telephones.

00:37:29.420 --> 00:37:36.619
The blue telephones are mock messages and the little
happy Macintosh lock icons will show you lock contention.

00:37:36.619 --> 00:37:39.929
When you make one of those pthreadmutetext
lock calls or unlock calls.

00:37:39.929 --> 00:37:45.319
And the thing that's interesting about system calls,
as we said they can block, so when that blocking shows

00:37:45.320 --> 00:37:50.269
up in the trace timeline, you'll see the
little black underbar leaving the icon

00:37:50.269 --> 00:37:52.030
on the left and proceeding to the right.

00:37:52.030 --> 00:37:58.040
What that means is that is time that that system
call is blocking the progress of the thread.

00:37:58.039 --> 00:38:05.000
And you're not, and some other thread is executing
most likely in the kernel on behalf of your program.

00:38:05.000 --> 00:38:12.809
So with regard to these system calls, what we want
to do is try not to use them in time critical areas

00:38:12.809 --> 00:38:15.880
and there are a number of slow and blocking system calls.

00:38:15.880 --> 00:38:19.539
So we want to know what these are,
learn them through experience.

00:38:19.539 --> 00:38:22.059
They change with the platform you're on.

00:38:22.059 --> 00:38:28.519
Some platforms are better at servicing
some system calls than others.

00:38:28.519 --> 00:38:35.320
So it may vary from say a Mac Book Pro
or an iPhone type device or a Mac Pro.

00:38:35.320 --> 00:38:43.840
Now the other type of System Trace event is a
virtual memory fault, is as Wade alluded to.

00:38:43.840 --> 00:38:49.450
These occur when you have new pages of
memory being accessed by your program.

00:38:49.449 --> 00:38:51.819
There's a couple ways that these comes into play.

00:38:51.820 --> 00:38:58.059
When you allocate a block of memory, the first use of
a new allocation will cause that, pages of that data,

00:38:58.059 --> 00:39:01.610
4 kilobytes at a time, to be brought
into your application space.

00:39:01.610 --> 00:39:08.210
If you share memory with another thread or with another
process, the Virtual Memory Manager will make a copy

00:39:08.210 --> 00:39:11.940
of a page for you and bring it into
your space so that you can use that one

00:39:11.940 --> 00:39:15.559
and not overwrite the other thread's
access to the same memory space.

00:39:15.559 --> 00:39:18.199
And again, one page at a time.

00:39:18.199 --> 00:39:21.049
So there's, there's one called a pagein and a pageout.

00:39:21.050 --> 00:39:28.510
And these are the most painful virtual memory faults because
what this means is physical memory is over committed.

00:39:28.510 --> 00:39:34.920
And the Virtual Memory Manager has to take some pages
from an application, write them out to the hard disk

00:39:34.920 --> 00:39:39.530
and bring in pages from another
application that comes to the foreground.

00:39:39.530 --> 00:39:42.450
Virtual memory faults look like this.

00:39:42.449 --> 00:39:44.109
They look like little pages.

00:39:44.110 --> 00:39:46.590
And they have little different icons.

00:39:46.590 --> 00:39:49.570
The most obvious one here is the little pink zeroes.

00:39:49.570 --> 00:39:53.390
The zeroes are what's called a zero page fill operation.

00:39:53.389 --> 00:39:57.019
Most of the time when you allocate a
new piece of memory and begin to use it,

00:39:57.019 --> 00:40:00.579
the Virtual Memory Manager will zero out that page.

00:40:00.579 --> 00:40:05.150
That is, takes a little bit of time, but you
certainly want a clean slate to work with.

00:40:05.150 --> 00:40:07.180
You see little check boxes.

00:40:07.179 --> 00:40:14.129
The little check page is a page that was in the Virtual
Memory Manager's data cache but not in your process yet.

00:40:14.130 --> 00:40:17.809
So bringing that page back to your
process is page cache hit.

00:40:17.809 --> 00:40:20.809
And of course there's a page cache miss as well.

00:40:20.809 --> 00:40:26.409
The third type of virtual memory fault we see
here is a little black and white spotted page.

00:40:26.409 --> 00:40:27.799
That's a copy on write.

00:40:27.800 --> 00:40:32.060
When you share pages, we call that a cow fault.

00:40:32.059 --> 00:40:32.119
[ Background noise ]

00:40:32.119 --> 00:40:32.819
Thank you.

00:40:32.820 --> 00:40:35.180
[ Laughter ]

00:40:35.179 --> 00:40:38.000
So Virtual Memory rules of thumb.

00:40:38.000 --> 00:40:41.079
Try to minimize the amount of memory in your application

00:40:41.079 --> 00:40:44.500
to achieve a minimization of the
number of page faults you take.

00:40:44.500 --> 00:40:46.960
I suppose that's fairly obvious.

00:40:46.960 --> 00:40:54.699
A good way to avoid this is to reuse memory allocations
if you can, allocate a page of memory, a block of memory

00:40:54.699 --> 00:41:00.619
and use it for work and then recycle that same block
to use again for more work as opposed to releasing it

00:41:00.619 --> 00:41:03.789
and allocating yet another new block of memory.

00:41:05.070 --> 00:41:11.400
If, and also, for audio and video type
applications, anything that's very sensitive to time,

00:41:11.400 --> 00:41:16.280
preallocate memory or even wire pages down.

00:41:16.280 --> 00:41:21.070
The fourth type of event that I'm
going to talk about is the interrupt.

00:41:21.070 --> 00:41:29.430
Interrupts are signals to the processor cores from other
devices in the system including other processor cores.

00:41:29.429 --> 00:41:31.329
They're very fast.

00:41:31.329 --> 00:41:39.779
But they can take some time if you have a heavy access to
a device, like your network interface or your disk access.

00:41:39.780 --> 00:41:42.800
PCI devices also generate lots of interrupts.

00:41:42.800 --> 00:41:49.960
The most popular of which is the graphics processor which
sends lots of interrupts to their host core for service.

00:41:49.960 --> 00:41:57.170
The cores themselves actually use interrupts to help the
scheduling software manage thread tenures on each core,

00:41:57.170 --> 00:42:01.639
every 10 milliseconds a timer is
fired on every core in the system.

00:42:01.639 --> 00:42:05.199
The interrupts look like little alarm bells.

00:42:05.199 --> 00:42:08.919
And there's not a lot of data about
them other than they occurred.

00:42:08.920 --> 00:42:12.630
You typically find them at the
beginning and end of tenures of threads,

00:42:12.630 --> 00:42:16.160
as the system has to change what
it's doing when interrupts happen.

00:42:16.159 --> 00:42:23.009
So with regard to interrupts, if you've never heard of
interrupts, they probably won't impact your day much.

00:42:23.010 --> 00:42:30.520
But if you're a device driver writer or other sort
of developer of a KEXT for software purposes,

00:42:30.519 --> 00:42:34.960
your device driver KEXT is going to generate
interrupts to get service from the core.

00:42:34.960 --> 00:42:40.920
So you want to try to minimize those and kind of plan them
strategically around what you're trying to get accomplished.

00:42:40.920 --> 00:42:44.680
So what I'd like to do now is show a demo.

00:42:44.679 --> 00:42:48.179
And hopefully Wade used up all the demo demons.

00:42:49.650 --> 00:42:52.610
Let's see.

00:42:52.610 --> 00:42:53.340
Letter C.

00:42:53.340 --> 00:42:57.570
See, get the right mouse here.

00:42:59.179 --> 00:43:00.949
Alright, let's put that away.

00:43:00.949 --> 00:43:05.469
I'll going to restart Shark, just for safety purposes.

00:43:05.469 --> 00:43:07.559
[ Background noise ]

00:43:07.559 --> 00:43:11.250
Although it may help, may or may not help.

00:43:11.250 --> 00:43:15.159
So I'm going to switch over to System Trace.

00:43:15.159 --> 00:43:21.409
What I want to show you with System
Trace is the mini configuration editor.

00:43:21.409 --> 00:43:27.889
The mini configuration editor for any of the configurations
will show you the popular controls that you might want

00:43:27.889 --> 00:43:31.869
to modify in order to collect the
information you're interested in.

00:43:31.869 --> 00:43:35.150
You can set a start delay to prevent
Shark from starting before you're ready.

00:43:35.150 --> 00:43:38.590
You can actually start Shark having a delay of some seconds.

00:43:38.590 --> 00:43:42.430
Set a time limit here, it says 10 seconds.

00:43:42.429 --> 00:43:46.369
I'm going to change that to 2.

00:43:46.369 --> 00:43:48.019
And the sample limit.

00:43:48.019 --> 00:43:53.309
That is a per core limit, System Trace
information is collected in every core.

00:43:53.309 --> 00:44:00.420
And with 16 cores, you're looking at 16 million
events collected, which could take a while to process.

00:44:00.420 --> 00:44:04.860
And while I like a coffee break as
much as the next guy, not all the time.

00:44:04.860 --> 00:44:08.890
One thing else I'm going to do is I'm going
to activate signpost which is a fifth type

00:44:08.889 --> 00:44:12.589
of System Trace kernel event that you can record.

00:44:12.590 --> 00:44:19.700
But signposts are different in that I inject them into the
program on purpose so that I can see them in the time line.

00:44:19.699 --> 00:44:21.529
And I'll describe those later.

00:44:21.530 --> 00:44:29.980
So let me start the System Trace and start the program in
GCD mode and let it run and grab, I grabbed some samples.

00:44:29.980 --> 00:44:32.909
And I only grabbed a few, but that should be sufficient.

00:44:32.909 --> 00:44:34.949
[ Period of silence ]

00:44:34.949 --> 00:44:40.449
So we see here, this is the summary view in system trace.

00:44:40.449 --> 00:44:43.969
And the first thing to look at is
the little pie chart in the corner.

00:44:43.969 --> 00:44:46.209
And the little table that goes with it.

00:44:46.210 --> 00:44:51.740
That gray area in the pie chart is the
entire system as the name suggests.

00:44:51.739 --> 00:44:57.439
93% idle with 16 cores.

00:44:57.440 --> 00:45:02.039
And we look, there's a few things going,
even Shark took a little bit of time.

00:45:02.039 --> 00:45:06.489
We see this table here has an average,
a minimum and maximum.

00:45:06.489 --> 00:45:10.569
This is the length of time each
thread in the process spent on a core.

00:45:10.570 --> 00:45:16.710
Averaging 45.7 microseconds out
of a possible 10 milliseconds.

00:45:16.710 --> 00:45:25.550
A minimum of 883 nanoseconds with a maximum, we
got a couple of good ones, 13.8 milliseconds.

00:45:25.550 --> 00:45:29.880
So that's interesting, but it's
not particularly illustrative.

00:45:29.880 --> 00:45:32.900
So I'm going to switch over to the timeline
as I said which is the most popular way

00:45:32.900 --> 00:45:37.590
to view the application's results in system trace.

00:45:37.590 --> 00:45:42.940
So it's on all processes, so I'm just going to switch
to image aware to kind of clean things up a bit.

00:45:42.940 --> 00:45:50.690
Now having done that, let me just grow this
and Wade mentioned the start WQ thread.

00:45:50.690 --> 00:45:55.920
And as we said, we use Grand Central Dispatch
in order to achieve, 1, 2, 3, 4, 5, 6, 7, 8, 9,

00:45:55.920 --> 00:46:00.119
10, 11, 12, 13, 14, 15, 16 working threads.

00:46:00.119 --> 00:46:02.099
These, there's this one up here.

00:46:02.099 --> 00:46:06.079
This was the master thread that spawned the GCD queues.

00:46:06.079 --> 00:46:07.869
This is the graphics thread.

00:46:07.869 --> 00:46:14.569
This one is the refreshing drawing thread of our images
that are in the view that you show, those pages popping up.

00:46:14.570 --> 00:46:22.610
And lastly, the extra thread that is spawned
to monitor some of the work we're doing.

00:46:22.610 --> 00:46:27.140
So and of course if you wrote this application,
you would know which threads were which.

00:46:27.139 --> 00:46:31.339
Interestingly, there wasn't a lot happening because
I kind of started Shark and then I pushed the button.

00:46:31.340 --> 00:46:38.440
But when you saw in those sample images from the
time line view, they were very thickly colored bars.

00:46:38.440 --> 00:46:48.190
And here they're not particular, there's a few
thick bits, but for the most part it's very sparse.

00:46:48.190 --> 00:46:49.500
Very sparse indeed.

00:46:49.500 --> 00:46:55.389
Now by default, System Trace comes up and doesn't,
it just shows you all the tenures are all one color.

00:46:55.389 --> 00:47:00.969
So we can enable thread coloring as you saw in
those samples and now we can taste the rainbow.

00:47:00.969 --> 00:47:03.989
We do have 16 cores, all pretty.

00:47:03.989 --> 00:47:09.399
So that's why threading is, is, can be relaxing.

00:47:09.400 --> 00:47:09.599
[ Laughter ]

00:47:09.599 --> 00:47:10.789
Maybe the only aspect.

00:47:10.789 --> 00:47:14.769
You can see that the top, the top
bar was, had some thick bits,

00:47:14.769 --> 00:47:19.179
but for the most part we have this
very faint pastel looking aspect here.

00:47:19.179 --> 00:47:24.109
So I'm going to zoom in on this by using the
mouse and just dragging a thin little sliver.

00:47:24.110 --> 00:47:29.750
Then this little sliver will now fill this, the timeline.

00:47:29.750 --> 00:47:32.119
See the little gray lines going in between everything?

00:47:32.119 --> 00:47:40.759
The little gray lines are context switch marks which means
that this little bit of, of execution was time switched out

00:47:40.760 --> 00:47:44.010
and we can trace it down and it
picked up here at this little bit.

00:47:44.010 --> 00:47:47.330
You can see that's, it is quite
confusing because it's very busy.

00:47:47.329 --> 00:47:50.619
And as Wade alluded to, there's something going on here.

00:47:50.619 --> 00:47:54.960
I'm going to zoom in a little more,
let me grab another sliver.

00:47:54.960 --> 00:47:56.440
Now I'm grabbing a very small sliver.

00:47:56.440 --> 00:47:57.300
Let me point this out.

00:47:57.300 --> 00:48:02.660
This fat section here actually has
a tenure of only 2.7 milliseconds.

00:48:02.659 --> 00:48:06.799
So that gives you an idea of the scale
of these little slivers of execution.

00:48:06.800 --> 00:48:12.820
This sliver here is 7.6, or 35.3 microseconds.

00:48:12.820 --> 00:48:19.660
And what I'm doing is I'm reading the results here in
this little info tip, total time, 35.3 microseconds.

00:48:19.659 --> 00:48:22.559
Priority, idle, reason blocked.

00:48:22.559 --> 00:48:24.199
There's another way to visualize that.

00:48:24.199 --> 00:48:25.569
We were coloring by CPU.

00:48:25.570 --> 00:48:31.720
If we color by reason why each of these threads is
being context switched away, everything turns hot pink.

00:48:31.719 --> 00:48:37.889
Everything's blocked with a few blue exceptions which is an
explicit yield, which generally means that that thread was

00:48:37.889 --> 00:48:42.909
at the end of a quantum and had to
be switched off the processor anyway.

00:48:42.909 --> 00:48:50.389
So let me just go back to the CPU and
again I'm going to zoom in a bit more.

00:48:50.389 --> 00:48:54.449
We'll just use this section here because
we can get the control thread as well.

00:48:54.449 --> 00:48:56.339
OK, now we're getting somewhere.

00:48:56.340 --> 00:48:59.840
We have a lot more white space than we have colored space.

00:48:59.840 --> 00:49:03.590
So at this point, I'm going to turn on the
system call icons we were talking about.

00:49:03.590 --> 00:49:07.300
Remember we talked about, we showed these earlier.

00:49:08.449 --> 00:49:11.129
You can see here in the top thread, lots of system calls.

00:49:11.130 --> 00:49:16.660
Now when you click on a system call, you're
going to get the call stack for that system call.

00:49:16.659 --> 00:49:20.329
Each of these events is generated by the kernel in order.

00:49:20.329 --> 00:49:21.480
So this is a trace.

00:49:21.480 --> 00:49:23.110
It's very precise.

00:49:23.110 --> 00:49:30.750
So we can see, we started down here in our application
layer, imageawarecontrollercreateimagefromPDF

00:49:30.750 --> 00:49:35.000
and we work all our way back up
to the same thing Wade showed,

00:49:35.000 --> 00:49:38.769
this font operations while we're trying to create a PDF.

00:49:38.769 --> 00:49:45.480
And down at the bottom of this, you can see the
total time for the system call, 5.9 microseconds.

00:49:45.480 --> 00:49:52.130
And then there's arguments to the system calls that
are meaningful in some cases, although not this case.

00:49:52.130 --> 00:49:55.059
Locks, here's the lock contention.

00:49:55.059 --> 00:50:01.690
For the same reason, we get to a certain point
trying to work with the fonts to create a PDF

00:50:01.690 --> 00:50:04.320
and to create glyphs actually, letters in the PDF.

00:50:04.320 --> 00:50:09.380
It appears that each letter has to take a lock.

00:50:09.380 --> 00:50:10.980
So something's not quite right with that.

00:50:10.980 --> 00:50:12.820
But it's what we have to work with.

00:50:12.820 --> 00:50:20.860
So now we're pretty sure why the reason is that we have
to waste so much time even though we've got 16 cores.

00:50:20.860 --> 00:50:23.320
Now I just turned on the signposts.

00:50:23.320 --> 00:50:27.210
I mentioned that there was a fifth type
of event that I can inject and I did.

00:50:27.210 --> 00:50:29.639
So we can read these sign posts.

00:50:29.639 --> 00:50:32.969
And before, I'm going to click this
just to show you, it has information.

00:50:32.969 --> 00:50:37.669
These, this information says arg1, 2, 3, 4, 5.

00:50:37.670 --> 00:50:41.630
These are arguments are numbers that
I injected into the system call.

00:50:41.630 --> 00:50:52.150
This is the, the arg2 in particular is the particular
instance of a call to, oh I missed it, get over one more.

00:50:52.150 --> 00:50:55.039
There's so many things here, I
really need to zoom in some more.

00:50:55.039 --> 00:50:58.690
OK, there.

00:50:58.690 --> 00:51:01.400
So these are arguments.

00:51:01.400 --> 00:51:05.369
Here is my system call which is called additem.

00:51:05.369 --> 00:51:13.769
Every time one of the documents showed up in the UI,
I have a sign post that tells me when that happened.

00:51:13.769 --> 00:51:19.460
So this is 22, 23, and so on.

00:51:19.460 --> 00:51:26.369
Now, another way to, I kind of knew where to drill down
because I put the signposts in, and the way to find them

00:51:26.369 --> 00:51:32.079
without just guessing is to use the trace tab which
shows you all the events in order as they were collected.

00:51:32.079 --> 00:51:36.539
And then when you inject signposts,
there'll be a signpost tab.

00:51:36.539 --> 00:51:38.009
Don't worry about these gray lines.

00:51:38.010 --> 00:51:42.450
That's because I only took a partial
trace and so some of them are incomplete.

00:51:42.449 --> 00:51:43.929
It's just telling me that.

00:51:43.929 --> 00:51:47.099
So here's a dispatch block and additem.

00:51:47.099 --> 00:51:49.219
Those are the two signposts I put in.

00:51:49.219 --> 00:51:57.379
So I'm going to double click on dispatchblock and somewhere
in this page, Shark has marked that system call for me.

00:51:57.380 --> 00:52:00.160
I think. Let me go back one more.

00:52:00.159 --> 00:52:02.199
We'll do it again.

00:52:02.199 --> 00:52:04.149
This dispatchblock right here.

00:52:04.150 --> 00:52:06.349
[ Period of silence ]

00:52:06.349 --> 00:52:09.259
There he is.

00:52:09.260 --> 00:52:12.290
Dispatchblock, sign post number 39.

00:52:12.289 --> 00:52:19.989
This tells me where it came from which is just at the very
beginning of a GCD dispatch queue and I put its tenure ends,

00:52:19.989 --> 00:52:22.500
remember I mentioned the little black lines that go under?

00:52:22.500 --> 00:52:25.610
Let me turn off system calls for a moment.

00:52:25.610 --> 00:52:31.690
So this guy has a tenure of 10.6 milliseconds.

00:52:31.690 --> 00:52:36.590
So the dispatch queue began and
10.6 milliseconds and ran all along.

00:52:36.590 --> 00:52:41.590
And in that time you can see all
the little bits that occurred.

00:52:41.590 --> 00:52:47.380
So what we know now is our problem with our
Image Aware application is the fact that in order

00:52:47.380 --> 00:52:55.950
to draw all those little glyphs in our PDFs
that we were processing is quite expensive.

00:52:55.949 --> 00:52:59.539
So we aren't getting our full time
share for each of these threads.

00:52:59.539 --> 00:53:00.489
We're just getting bits.

00:53:00.489 --> 00:53:02.639
We're getting microseconds.

00:53:02.639 --> 00:53:07.839
And that is the reason why we have the problem
we do with, with our Image Aware program.

00:53:07.840 --> 00:53:09.700
And so it's, but it's not something we can fix.

00:53:09.699 --> 00:53:10.899
This is a system library.

00:53:10.900 --> 00:53:16.990
So we had to contact the guys that developed
that library and maintain it and we'll be working

00:53:16.989 --> 00:53:21.039
with them to speed this up in the future.

00:53:21.039 --> 00:53:28.360
Alright, so what I would like to do is show another
application running and grab a System Trace from one

00:53:28.360 --> 00:53:31.809
of our tried and true demos from the past.

00:53:31.809 --> 00:53:36.900
This here is Noble Ape and these are apes running
around on an island, simulating ape thoughts.

00:53:36.900 --> 00:53:41.460
And by default you can see that we
get, let me set this benchmark up,

00:53:41.460 --> 00:53:45.889
it's the single threaded scalar
version oh 20008 thoughts per second.

00:53:45.889 --> 00:53:53.299
But if I go ahead and thread this and optimize it, if I
optimize it, now it goes way up to a million and change

00:53:53.300 --> 00:53:59.320
and then if I thread that on top
of that, it's vastly improved.

00:53:59.320 --> 00:54:04.970
So if I System Trace this guy, I'm
going to make this a little bit longer.

00:54:04.969 --> 00:54:17.609
[ Period of silence ]

00:54:17.610 --> 00:54:23.470
Alright. When we look at the system when it's running,

00:54:23.469 --> 00:54:32.480
we see that in this case we have 69% usage
and Noble Ape itself is that 69% usage.

00:54:32.480 --> 00:54:34.219
Noble Ape has many threads.

00:54:34.219 --> 00:54:37.769
And these are done by hand with pthreads.

00:54:37.769 --> 00:54:39.889
So there are 16 of those and then the main thread.

00:54:39.889 --> 00:54:42.199
Let's just take a quick look at that timeline.

00:54:42.199 --> 00:54:48.500
And you can see there's a rainbow here, but
the tenures of each thread are much thicker.

00:54:48.500 --> 00:54:57.949
And we go in and we see a very nice regimented flow where
each of the slave threads works for a time and then stops

00:54:57.949 --> 00:55:01.629
and the master control thread down here does its work.

00:55:01.630 --> 00:55:08.950
So once again we turn on the system calls and we see that
at the very, at the end of each of these is going to be

00:55:08.949 --> 00:55:15.029
that you know a lock taken and that lock
tenure runs over here until it starts again

00:55:15.030 --> 00:55:20.920
because the command thread told it
to from these system calls here.

00:55:20.920 --> 00:55:27.769
So this again is a very regular, very regimented
worker, slave or master slave relationship.

00:55:27.769 --> 00:55:30.449
Which is what we want to try to achieve.

00:55:30.449 --> 00:55:36.149
Since we had total control over all this code, we
actually didn't have to spend much time with worrying

00:55:36.150 --> 00:55:44.090
about the various effects of our various system
calls because we implemented them all our self.

00:55:44.090 --> 00:55:47.170
So with that I want to go back to the slides.

00:55:47.170 --> 00:55:50.090
I think that's right.

00:55:50.090 --> 00:55:52.320
Did I get it?

00:55:52.320 --> 00:55:53.700
Ah, sweet.

00:55:53.699 --> 00:55:59.289
And talk about those signposts.

00:55:59.289 --> 00:56:06.309
So here's Wade's demonstration software
and we want to add signposts to it.

00:56:06.309 --> 00:56:12.289
And we add a couple of #include files to
get the macros and constants that we need.

00:56:12.289 --> 00:56:17.779
And then we can make a system call with
number sys/kdebug trace as the first argument.

00:56:17.780 --> 00:56:20.650
Second argument is apps/debugcode macro.

00:56:20.650 --> 00:56:24.940
And what's important to notice here
is the little k prepare constant.

00:56:24.940 --> 00:56:27.980
That's just a number like 42.

00:56:27.980 --> 00:56:37.920
That number is used by the apps/debugcode macro if it has
a debugfunc/startmodifier and a debugfunc/endmodifier used

00:56:37.920 --> 00:56:43.809
by Shark to place a signpost and then
draw the tenure between start and end.

00:56:43.809 --> 00:56:49.420
The other kind of system call you can make is a
point system call which you saw those in my add item.

00:56:49.420 --> 00:56:56.809
It just simply has a different
number than the tenured signposts.

00:56:56.809 --> 00:57:02.150
Then you can see again I mentioned
debug func start and func end.

00:57:02.150 --> 00:57:05.070
There are those arguments at the bottom of the system calls.

00:57:05.070 --> 00:57:14.260
In the case of signposts, you as the developer can
insert values, 4 32-bit values into the signpost.

00:57:14.260 --> 00:57:21.670
In this case, we for example in the middle we put units
and we put the increment, we put the average of the results

00:57:21.670 --> 00:57:28.059
and we also put the contents of the particular
result we were working on in that loop entry.

00:57:28.059 --> 00:57:32.699
So that's in user space, like I
did in the Image Aware application.

00:57:32.699 --> 00:57:38.129
If you're a kernel developer, you can just
invoke the kernel debug constant macro directly.

00:57:38.130 --> 00:57:47.000
And then you see the appsdebugcode macro and its
Kprepare and debug func start, debug func stop.

00:57:47.000 --> 00:57:54.250
But I really want to point out is that the arguments where
you had four in user space there are five in the kernel.

00:57:54.250 --> 00:57:57.610
And the fifth one is always a zero.

00:57:57.610 --> 00:58:07.769
But new in Snow Leopard, you can actually add signposts to
your detrace scripts by invoking the core profile function.

00:58:07.769 --> 00:58:08.500
This will give you.

00:58:08.500 --> 00:58:08.559
[ Cheering ]

00:58:08.559 --> 00:58:08.619
[ Laughter ]

00:58:08.619 --> 00:58:11.139
You're welcome.

00:58:11.139 --> 00:58:16.549
This will give you the same, the same results in the system
trace as you would from user space or a kernel extension,

00:58:16.550 --> 00:58:21.980
so that you can, you can relate what detrace
is collecting with what Shark collects.

00:58:21.980 --> 00:58:28.320
So instead of the start, end, func
end and func start constants,

00:58:28.320 --> 00:58:32.750
you use Core Profile Signpost Start,
Core Profile Signpost End.

00:58:32.750 --> 00:58:37.420
Now the third type that's not here
is Core Profile Signpost Point.

00:58:37.420 --> 00:58:39.860
Other than that, the arguments are the same.

00:58:39.860 --> 00:58:44.980
And the four user arguments are
optional in the detrace scripts.

00:58:44.980 --> 00:58:48.039
They'll be assigned to zeroes if you don't put them in.

00:58:48.039 --> 00:58:54.460
So that's how signposts are implemented in
your code but Shark can't quite see them.

00:58:54.460 --> 00:58:58.099
What Shark needs to be able to do is know
what those codes mean when they come in.

00:58:58.099 --> 00:59:05.219
So we have to create a signpost name file
at this location in our user home directory.

00:59:05.219 --> 00:59:07.500
So you can name that file anything you want.

00:59:07.500 --> 00:59:09.590
In fact, you can have multiple files.

00:59:09.590 --> 00:59:15.650
But each file has a very simple syntax, a hexadecimal number
which you used in your code and the name that you want

00:59:15.650 --> 00:59:19.039
that signpost to have in the display in Shark.

00:59:19.039 --> 00:59:24.529
That's about all I had to say about System
Tracing, so we'll go over the other two types

00:59:24.530 --> 00:59:29.269
that Wade mentioned briefly, bandwidth
and L2 Cache Miss Profiling.

00:59:29.269 --> 00:59:39.699
Processor bandwidth profiling is basically how much
you are utilizing the processor front side buss.

00:59:41.010 --> 00:59:48.020
Memory bandwidth is not usually as high as the front side
buss band but then it's primarily due to the frequency

00:59:48.019 --> 00:59:52.219
of memory versus the frequency of front side buss.

00:59:52.219 --> 00:59:57.739
What you also will find is that the maximum
bandwidth you can achieve varies by platform.

00:59:57.739 --> 01:00:02.879
Obviously a 16 core Mac Pro has much
more bandwidth than a 2-core laptop.

01:00:02.880 --> 01:00:11.869
And you can't really just take the, you know bytes per
cache line and multiply by the frequency of the buss.

01:00:11.869 --> 01:00:16.859
That would be the electrical maximum, but
you won't be able to reach that in practice.

01:00:16.860 --> 01:00:21.730
All those interrupts and the timer pops
and that sort of thing will limit that.

01:00:21.730 --> 01:00:29.019
So we have a couple of things to tell you about processor
bandwidth and that it's kind of a good thing to remember

01:00:29.019 --> 01:00:35.949
that the Intel machines are typically capable of about
90% of that theoretical limit that you can calculate.

01:00:35.949 --> 01:00:42.909
And often times, your application will maybe
only use a few hundred megabytes of bandwidth.

01:00:42.909 --> 01:00:44.730
That could indicate 2 things.

01:00:44.730 --> 01:00:49.619
The first thing being your application is not
bandwidth limited or your application it is limited

01:00:49.619 --> 01:00:52.900
in some other way that's preventing
you from achieving bandwidth.

01:00:52.900 --> 01:00:58.550
So I just have a couple of pictures
to look at from a bandwidth profile.

01:00:58.550 --> 01:01:00.660
This is Image Aware running on a single thread.

01:01:00.659 --> 01:01:07.559
And if you look close you can see that the first
number, the first hard line is 500 megabytes per second.

01:01:07.559 --> 01:01:11.699
And most of the samples were below that.

01:01:11.699 --> 01:01:18.319
Whereas after we used GCD on it and threaded it, we
were able to increase the bandwidth on average at least

01:01:18.320 --> 01:01:24.289
to over a gigabyte per second and we got some good spikes
up into the 2500 megabytes per second range and a couple

01:01:24.289 --> 01:01:28.099
of spikes way up at 4 gigabytes a second.

01:01:28.099 --> 01:01:32.900
But interestingly there are some gaps in the
bandwidth but you see they kind of line up.

01:01:32.900 --> 01:01:35.599
So that's kind of the nature of that application.

01:01:35.599 --> 01:01:39.349
But bigger is better in terms of
bandwidth in an application like this.

01:01:39.349 --> 01:01:45.179
Now I showed you a Noble Apes system trace really for the
reason of showing you this slide which is the bandwidth

01:01:45.179 --> 01:01:50.019
from a single thread Noble Ape which is
running around 250 megabytes a second.

01:01:50.019 --> 01:01:56.030
Whereas if you, when we optimized and threaded Noble Ape,
you can see we've got some nice spikes that are using most

01:01:56.030 --> 01:02:01.010
of the bandwidth up around 5 gigabytes a second
and the average is up around 2 gigabytes a second.

01:02:01.010 --> 01:02:06.230
So you can see, that's one of the reasons why Noble Ape
goes so much faster, like 40 times faster because it has

01:02:06.230 --> 01:02:12.730
about 12 times as much bandwidth on average
than it does when it's just a single thread.

01:02:12.730 --> 01:02:17.240
So L2 Cache Miss Profiling is the last
thing we'd like to talk about briefly.

01:02:17.239 --> 01:02:22.729
And L2 Cache Miss Profiling is very
similar in appearance to time profiling.

01:02:22.730 --> 01:02:30.260
But instead of showing you how you spend time in a
processor, it shows you when you miss processor cache.

01:02:30.260 --> 01:02:36.470
And of course missing the cache means you have to go out
to memory and fetch that line of cache and bring it back.

01:02:36.469 --> 01:02:38.689
So there's a couple reasons for that.

01:02:38.690 --> 01:02:46.139
One of the common ones is that your data layout in your
application is inefficient or incompatible with the amount

01:02:46.139 --> 01:02:53.460
of cache on a processor core has access to or perhaps
the pattern in which you access it is inefficient.

01:02:53.460 --> 01:02:55.170
Time profiles won't really show this.

01:02:55.170 --> 01:03:02.190
What they tend to show is time spent in
your code that's a little bit mysterious.

01:03:02.190 --> 01:03:04.730
It looks like your code is executing albeit slowly.

01:03:04.730 --> 01:03:12.380
However, the tool tips in Shark can
definitely help to rule that out or point you

01:03:12.380 --> 01:03:15.800
in the direction of taking an L2 Cache Miss Profile.

01:03:15.800 --> 01:03:18.910
Most important thing is to keep
your working set within the cache.

01:03:18.909 --> 01:03:25.699
I can't stress this enough and I will put my own personal
spin on it and say that when I try to limit working sets

01:03:25.699 --> 01:03:34.719
in cache, I consider using half of the reported cache size
because most modern systems have 2 cores sharing L2 Cache.

01:03:34.719 --> 01:03:39.379
So if you have the threaded application, your
own threads will be contending for cache lines.

01:03:39.380 --> 01:03:42.289
So you don't want your working set of
both threads to be the whole cache,

01:03:42.289 --> 01:03:47.809
otherwise they'll simply eject each other
constantly and battle for use of the cache.

01:03:47.809 --> 01:03:52.449
And the point of that is to keep things in cache.

01:03:52.449 --> 01:03:54.289
And lastly just a brief note.

01:03:54.289 --> 01:04:01.139
Garbage collection is an automated process
that comes in and cleans up unused data pools.

01:04:01.139 --> 01:04:07.809
And garbage collection will come in sort of unannounced and
can eject data from the cache and cause your application

01:04:07.809 --> 01:04:10.239
to miss when garbage collection is complete.

01:04:10.239 --> 01:04:13.179
A couple of quick images.

01:04:13.179 --> 01:04:17.779
Here is the Image Aware program in GCD mode.

01:04:17.780 --> 01:04:22.050
And what we see at the top of the
screen is a chunk of percentage,

01:04:22.050 --> 01:04:27.370
almost 40% of the time in our code
from the L2 Cache Miss Profile.

01:04:27.369 --> 01:04:31.309
And we look down below, we see
that darn MLSetInterruptsEnabled.

01:04:31.309 --> 01:04:33.789
And Wade said that that's an indication
you're using the kernel.

01:04:33.789 --> 01:04:37.400
And he knows that because Shark
already told him ahead of time.

01:04:37.400 --> 01:04:40.220
This method disables or enables
interrupts, et cetera, et cetera.

01:04:40.219 --> 01:04:41.289
And you look at the last system.

01:04:41.289 --> 01:04:45.340
It says to get a better idea what's
happening, try using System Trace.

01:04:45.340 --> 01:04:47.240
So Shark will point you in the right direction.

01:04:47.239 --> 01:04:52.329
On the other hand, if you look at a time profile of the
same application, you see slightly different numbers.

01:04:52.329 --> 01:04:56.730
51% in our app and only 9% in the interrupts.

01:04:56.730 --> 01:05:00.320
But that's where time was spent as
opposed to where cache was missed.

01:05:00.320 --> 01:05:07.300
But you can see that cache misses make
up the bulk of the problem in our method.

01:05:07.300 --> 01:05:10.390
So with that I'm just going to wrap things up.

01:05:10.389 --> 01:05:17.069
And sort of reflect on Time Profiling as being
very useful and System Trace is really good

01:05:17.070 --> 01:05:21.519
for these application types, multithreading,
OI heavy and real time.

01:05:21.519 --> 01:05:26.659
I was going to make a joke about hyphens, but they
left some out on the slides, multithreaded, IO heavy.

01:05:26.659 --> 01:05:29.269
If it has a hyphen in it, use System Trace.

01:05:29.269 --> 01:05:32.030
Bandwidth and Cache Miss Profiles, we saw those.

01:05:32.030 --> 01:05:36.510
Those will help you categorize your data sets
and see if you're on the right track in terms

01:05:36.510 --> 01:05:40.500
of utilizing the availability in the system.

01:05:40.500 --> 01:05:43.639
And Shark does a lot more than we've talked about today.

01:05:43.639 --> 01:05:49.239
I can't stress enough that you can take
a look through our wonderful user guide.

01:05:49.239 --> 01:05:55.199
Lastly, just want to point out that we're all
here for your help, edification, to help you.

01:05:55.199 --> 01:05:58.609
Mike Jurewitz is our evangelist and he's always online.

01:05:58.610 --> 01:06:05.050
You can reach Wade and I and the other
developers at perftools-feedback@group.apple.com.

01:06:05.050 --> 01:06:10.360
And the documentation is available in the help
menu in Shark and online in developer.apple.com.