WEBVTT

00:00:12.869 --> 00:00:16.730
>> My name is James McCartney and
I'm going to talk about AVFoundation.

00:00:16.730 --> 00:00:21.740
So AVFoundation is a set of Objective-C classes.

00:00:21.739 --> 00:00:26.349
I'm going to talk about AVAudioPlayer and AVAudioRecorder.

00:00:26.350 --> 00:00:30.179
Bill covered AVAudioSession in a previous talk.

00:00:30.179 --> 00:00:40.700
So AVAudioPlayer is pretty much the simplest way to play
a sound, it's layered on top of AudioQueue and AudioFile

00:00:40.700 --> 00:00:46.570
and the AudioToolbox, and it plays
file types that are supported

00:00:46.570 --> 00:00:55.710
by AudioFile including CAF files,
M4A, and MPG layer three, et cetera.

00:00:55.710 --> 00:01:06.730
And it provides basic playback operations like creating a
player, preparing it, playing, pausing, skipping, stopping.

00:01:06.730 --> 00:01:16.439
It also supports volume, metering, and looping, and it
uses a delegate for notifications for interesting events

00:01:16.439 --> 00:01:25.329
like your sound finished playing or there was a decode
error, or there was an interruption that's begun or ended.

00:01:25.329 --> 00:01:35.179
So there's two ways to create an AVAudioPlayer, you can
create one using a URL for a file in the file system,

00:01:35.180 --> 00:01:44.410
or you can create one from NSData
that contains AudioFile format data.

00:01:44.409 --> 00:01:48.280
So the first method that you'd call would be prepareToPlay,

00:01:48.280 --> 00:01:53.090
and that allocates all the resources
that are necessary to play a sound.

00:01:53.090 --> 00:01:57.189
It creates the AudioQueue, it allocates
the buffers, and it performs priming,

00:01:57.189 --> 00:02:03.109
so it's decoding the beginning of
the sound before playback begins.

00:02:03.109 --> 00:02:09.969
And then it returns Yes if there
was no problem, and No if it failed.

00:02:09.969 --> 00:02:17.050
And if you don't call prepareToPlay and you just call
play, prepareToPlay will happen automatically anyway.

00:02:17.050 --> 00:02:24.510
But the reason you would want to call prepare
is to get that first bit of audio decoded,

00:02:24.509 --> 00:02:29.759
and so when you call play it's just
ready to go and there's going to be less

00:02:29.759 --> 00:02:36.019
of a bump in the CPU use right at that moment.

00:02:36.020 --> 00:02:42.409
So in order to play you call play
and it starts playing the sound.

00:02:42.409 --> 00:02:49.280
And if you were stopped or you were paused,
then play is the way to resume playback.

00:02:49.280 --> 00:02:59.960
So pause stops playback, but the player remains prepared,
all the resources are allocated, the AudioQueue and buffers.

00:02:59.960 --> 00:03:06.420
And so when you call play it will just
resume playing from where you left off.

00:03:06.419 --> 00:03:14.349
Now stop also stops playback, but in this
case the player is no longer prepared to play.

00:03:14.349 --> 00:03:17.560
The AudioQueue and the buffers have all been disposed.

00:03:17.560 --> 00:03:24.819
And so when you call play again it will
need to do a prepare in order to start.

00:03:24.819 --> 00:03:35.810
Now stop does not reset the player back to the beginning
of the sound, it works just like on a cassette deck

00:03:35.810 --> 00:03:39.800
if you hit stop and then you hit play, you're
going to continue from where you left off.

00:03:39.800 --> 00:03:48.060
If you want to go back to the beginning of the
sound, you need to set currentTime back to the start.

00:03:48.060 --> 00:03:54.150
So currentTime, that's the current
position in the AudioFile in seconds.

00:03:54.150 --> 00:03:58.719
And you can get the currentTime
or you can set it at any time.

00:03:58.719 --> 00:04:02.819
If you set it while you're stopped or paused,

00:04:02.819 --> 00:04:07.840
that will control where you begin
playing back when play is called again.

00:04:07.840 --> 00:04:13.870
And if you set it while you're playing,
you're going to jump to that time.

00:04:13.870 --> 00:04:24.149
And I'm going to demonstrate setting currentTime
to implementing a fast forward feature in the demo.

00:04:24.149 --> 00:04:28.829
So AVAudioPlayer also supports metering.

00:04:28.829 --> 00:04:36.329
You need to explicitly enable metering
because it's somewhat expensive to run.

00:04:36.329 --> 00:04:44.079
So once you've enabled metering, then periodically you'll
call updateMeters to have the meters update themselves,

00:04:44.079 --> 00:04:48.079
and then you can get the values for each channel.

00:04:48.079 --> 00:04:56.259
Other properties for AVAudioPlayer are the
volume, you can get and set the volume.

00:04:56.259 --> 00:05:00.949
Number of loops, you can turn looping
on or off or set the number of loops,

00:05:00.949 --> 00:05:05.899
the number of channels and duration of the file.

00:05:05.899 --> 00:05:10.509
And also there's a delegate.

00:05:10.509 --> 00:05:14.740
You can create a delegate object for your AVAudioPlayer,

00:05:14.740 --> 00:05:18.519
and then when certain interesting
events happen you'll be notified.

00:05:18.519 --> 00:05:25.120
One is that your player finished
playing, or there was a decode error

00:05:25.120 --> 00:05:27.259
because maybe your file had corrupt packets in it.

00:05:27.259 --> 00:05:38.009
And then also as was covered before, there might
have been an interruption from a phone call,

00:05:38.009 --> 00:05:44.610
and then you'll get a begin and end interruption
notification, you'll only get the end

00:05:44.610 --> 00:05:53.879
if the phone call was declined,
or if you weren't just terminated.

00:05:53.879 --> 00:06:01.319
So if you do get an end interruption, you might still
want to play, so in your delegate callback it would look

00:06:01.319 --> 00:06:07.870
like something like this, you might check a condition
to see if you still were interested in playing.

00:06:07.870 --> 00:06:14.420
And then if you were, you could just call
player play and your playback will continue.

00:06:14.420 --> 00:06:23.980
So in the demo that I'm about to do which is implementing
fast forward, what's going to happen is we're going

00:06:23.980 --> 00:06:30.800
to play .1 seconds of sound at a
time and then skip .3 seconds ahead.

00:06:30.800 --> 00:06:42.050
So that will give us a three times, sort of
CD style skipping playback ahead in your file.

00:06:42.050 --> 00:06:56.150
So when the UI fast forward button is pressed we're going to
use an NSTimer that is an instance variable in this class.

00:06:56.149 --> 00:07:05.079
If the timer exists, we're going to invalidate
it, otherwise we're going to schedule it

00:07:05.079 --> 00:07:11.359
to every .1 seconds send the fast forward
selector to this controller object,

00:07:11.360 --> 00:07:16.569
and then that's going to repeat periodically.

00:07:16.569 --> 00:07:22.370
And then when the button is released
we're going to invalidate the timer.

00:07:22.370 --> 00:07:29.519
So every time the fast forward selector is called,
which is every .1 seconds while the button is down,

00:07:29.519 --> 00:07:37.859
we're going to advance the currentTime by .3 seconds, so
that's going to give us a way of skipping through the file.

00:07:37.860 --> 00:07:43.590
And so I'll just go to the demo unit here.

00:07:46.889 --> 00:07:54.539
Okay, so I'm going to use this AVTouch
application, and so here we are playing.

00:07:54.540 --> 00:07:56.569
[ Video game sound ]

00:07:56.569 --> 00:08:03.310
>> When I hold down fast forward and we're skipping
around, and reverse is implemented the same way,

00:08:03.310 --> 00:08:07.970
just skipping around in the file by setting currentTime.

00:08:07.970 --> 00:08:11.190
So it's that simple really.

00:08:11.189 --> 00:08:16.600
Back to slides please.

00:08:16.600 --> 00:08:26.590
So new in 3.0 is AVAudioRecorder, it's very
similar to AVAudioPlayer, it records sound.

00:08:26.589 --> 00:08:35.819
You create in AVAudioRecorder, you give it
a URL of a file that you want to record to,

00:08:35.820 --> 00:08:43.920
and you give it a settings dictionary which is
an NS Dictionary that contains key-value pairs

00:08:43.919 --> 00:08:48.469
that specify what format you want to record
and the number of channels sample rate.

00:08:48.470 --> 00:08:54.620
And then for linear PCM you would
also specify bit depth and endian-ness

00:08:54.620 --> 00:08:59.360
or for an encoded format you can specify
quality, bit rate, and bit depth.

00:08:59.360 --> 00:09:14.190
So these are the keys that are supported for the settings
dictionary, and this is how you would set one up.

00:09:14.190 --> 00:09:19.670
So there's a key for the format ID.

00:09:19.669 --> 00:09:29.559
This is MPEG4AAC, the sample rate is 44-1, the number
of channels is two, encoder bit rate 128 kilobits,

00:09:29.559 --> 00:09:35.369
and then the encoder quality is set to max.

00:09:35.370 --> 00:09:39.149
So then we're just creating an NS Dictionary that way.

00:09:39.149 --> 00:09:51.230
So recording has the same corresponding methods as
AVAudioPlayer, that's prepare, record, pause, and stop.

00:09:51.230 --> 00:09:58.050
There's also metering which works exactly the same as
AVAudioPlayer, and there's delegate methods which respond

00:09:58.049 --> 00:10:05.919
to the same kinds of events that AVAudioPlayer responds to.

00:10:05.919 --> 00:10:09.319
So that's AVFoundation, it's simple.

00:10:09.320 --> 00:10:16.020
Cocoa classes to playback files
and record files on the iPhone.

00:10:16.019 --> 00:10:20.329
And so now I'm going to bring Bob
Aron, he's going to talk about OpenAL.

00:10:20.330 --> 00:10:23.800
>> Good afternoon, my name is Bob Aron,
I'm an engineer on the Core Audio team.

00:10:23.799 --> 00:10:30.169
And I'm going to talk a little bit about some best practices
for you when you're using OpenAL in your application.

00:10:30.169 --> 00:10:36.779
So first, just for a quick review OpenAL, if
you're not familiar, it's an Open Source API,

00:10:36.779 --> 00:10:40.649
cross-platform API for doing spatialized audio.

00:10:40.649 --> 00:10:47.470
There's implementations on several platforms,
Mac OS X, Windows, Linux, of course the iPhone.

00:10:47.470 --> 00:10:52.509
It's primarily used for game development
but it can be used for other audio purposes.

00:10:52.509 --> 00:10:58.230
And if you're not already familiar, I'm not really going to
go into any of the APIs directly, so if you're not familiar

00:10:58.230 --> 00:11:03.159
with OpenAL, I'd recommend that you go
to the openal.org site for more info.

00:11:03.159 --> 00:11:10.149
As far as Apple is concerned, we've been delivering an
implementation of OpenAL since we shipped Tiger 10.4.

00:11:10.149 --> 00:11:17.449
And then when the 1.1 specification was completed,
we did implementations for a Tiger update on 10.4.7,

00:11:17.450 --> 00:11:24.060
and that's also what's on the 2.0
iPhone release, and of course 3.0.

00:11:24.059 --> 00:11:28.989
So OpenAL is basically organized into four primary objects.

00:11:28.990 --> 00:11:35.750
There's an OpenAL device, an OpenAL
context, OpenAL source, and an OpenAL buffer.

00:11:35.750 --> 00:11:42.899
And the OpenAL device is basically the object
that is responsible for delivering audio

00:11:42.899 --> 00:11:45.429
to the hardware on the system that you're running.

00:11:45.429 --> 00:11:49.879
On the device this is implemented
on top of the remote I/O audio unit,

00:11:49.879 --> 00:11:53.299
which we'll go into more depth in later in the session.

00:11:53.299 --> 00:12:01.839
The OpenAL context is basically your spatial environment,
it sits on top in the implementation on our 3D mixer

00:12:01.840 --> 00:12:09.340
where all the heavy mixing and the spatialization gets
done, it's responsible, it has an implicit listener,

00:12:09.340 --> 00:12:14.820
it's responsible for rendering all of the objects
that are moving around in your virtual space.

00:12:14.820 --> 00:12:18.210
And those objects that are moving
around are the OpenAL sources.

00:12:18.210 --> 00:12:25.570
So the OpenAL source's job basically is to queue up
audio data and provide it to the mixer or the context,

00:12:25.570 --> 00:12:33.500
and then also to move around using the spatial coordinates,
X, Y, Z coordinates that you might also use in OpenGL.

00:12:33.500 --> 00:12:38.659
The last object is the OpenALBuffer, and the
reason I have that separated on the diagram here is

00:12:38.659 --> 00:12:42.980
that it doesn't really have any
connection to the sources or the context

00:12:42.980 --> 00:12:47.639
which all have some connection
in terms of how they're set up.

00:12:47.639 --> 00:12:53.289
The OpenAL buffers you would just generate a set
of these buffers, populate them with audio data,

00:12:53.289 --> 00:12:56.610
and they're available for use by any
of the sources that are being rendered

00:12:56.610 --> 00:12:59.450
by the context that you have in your application.

00:12:59.450 --> 00:13:02.550
So let's just jump into some best practices.

00:13:02.549 --> 00:13:09.509
One of the things that you have to do on the device with
the existing 2.0 iPhone OS when you get an interruption

00:13:09.509 --> 00:13:17.970
such as phone call, is to save off the state of
your contexts and all the associated sources.

00:13:17.970 --> 00:13:20.720
So you have to save that off and the
you have to destroy your contexts.

00:13:20.720 --> 00:13:25.810
And then when your interruption ends, if you're
still running, you can then create a new context

00:13:25.809 --> 00:13:28.379
and then restore all the state for those objects.

00:13:28.379 --> 00:13:34.340
Well with 3.0, you don't have to do that anymore, all
you really have to do when you get your interruption

00:13:34.340 --> 00:13:37.490
in your listener is to set the current context to NULL.

00:13:37.490 --> 00:13:42.519
And then when your interruption ends, you
can just restore your current context back

00:13:42.519 --> 00:13:45.079
to the context that you had been using.

00:13:45.080 --> 00:13:49.120
So if your application, if you're already doing
this on 2.0 where you're saving off your state,

00:13:49.120 --> 00:13:52.810
and you want to stay compatible with the 2.0 release.

00:13:52.809 --> 00:13:56.869
With a 3.0 app, you can make the decision at run time.

00:13:56.870 --> 00:14:01.980
So you can see there's a little code here
where you can go get the major OS version,

00:14:01.980 --> 00:14:07.220
and then when you get your interruption you can make
the decision on which mechanism to use at run time,

00:14:07.220 --> 00:14:10.540
so that way you can - we would
recommend that if you're running

00:14:10.539 --> 00:14:15.610
on 3.0 you really do want to use
the alcMakeContextCurrent method.

00:14:15.610 --> 00:14:20.950
The next best practice has to do with OpenAL sources.

00:14:20.950 --> 00:14:25.650
Now with OpenAL, there's no callback
mechanism, so for you to find out state

00:14:25.649 --> 00:14:28.379
about your OpenAL source, you have to pull.

00:14:28.379 --> 00:14:32.379
So if you want to find out if your
source is playing or paused or stopped,

00:14:32.379 --> 00:14:35.740
or you want to know how many buffers
have been processed, you have to pull.

00:14:35.740 --> 00:14:39.519
And so if you're using OpenAL already, you
may recognize this little code snippet,

00:14:39.519 --> 00:14:43.500
you have to kind of pull and ask the source for some state.

00:14:43.500 --> 00:14:49.440
And something we've seen in a lot of applications is a tight
loop like this where you actually can hurt your performance

00:14:49.440 --> 00:14:53.440
because you're calling the source just too often.

00:14:53.440 --> 00:14:57.050
And so what we recommend is just that you put
a short sleep while you're doing your polling

00:14:57.049 --> 00:15:01.199
and that actually can give you
a benefit in your performance.

00:15:01.200 --> 00:15:09.290
Another thing we've seen in regards to OpenAL sources in a
lot of the applications that we've had to do investigations

00:15:09.289 --> 00:15:15.569
on since we shipped the 2.0 release, is a calling
pattern that looks a little bit like this.

00:15:15.570 --> 00:15:21.790
Your application wants to play a sound so you
create a source, you attach a buffer for playback,

00:15:21.789 --> 00:15:28.029
you start playback, you pull until your sound
finishes playing, and then delete the source.

00:15:28.029 --> 00:15:33.379
And we've seen this even in circumstances like a game
that's playing a machine gun or something where each one

00:15:33.379 --> 00:15:37.480
of those rat-a-tat-tats is doing
this particular calling pattern.

00:15:37.480 --> 00:15:42.590
Now this really kind of inefficient for that
kind of scenario, so what you really need

00:15:42.590 --> 00:15:46.290
to do is be aware of what your source is doing.

00:15:46.289 --> 00:15:50.149
If your source is moving around in your space and
you know it's going to be playing some more sounds,

00:15:50.149 --> 00:15:54.360
or maybe you're going to play the same sound again,
you really just need to attach that source, play it,

00:15:54.360 --> 00:15:59.340
and then detach it, and then you'll have
the source around, and not call the generate

00:15:59.340 --> 00:16:04.930
and delete sources so often if it's not necessary.

00:16:04.929 --> 00:16:05.750
OpenAL buffers.

00:16:05.750 --> 00:16:11.159
So the OpenAL buffer object as I mentioned earlier
is the way that you encapsulate some audio data

00:16:11.159 --> 00:16:15.799
that you want to playback via an OpenAL source.

00:16:15.799 --> 00:16:22.199
And there's two ways that you can provide
that audio data to the buffer object.

00:16:22.200 --> 00:16:30.800
The first is the alBufferData call, that's the standard
OpenAL API for providing audio data to your object.

00:16:30.799 --> 00:16:36.139
And you'll notice that the parameter to
pass some information about the format

00:16:36.139 --> 00:16:39.490
how big it is, and a pointer to your data.

00:16:39.490 --> 00:16:45.409
The other way that you can provide data is using
an extended version called alBufferDataStatic.

00:16:45.409 --> 00:16:50.459
And you'll notice that the functions look
quite similar, they take the same parameters,

00:16:50.460 --> 00:16:58.290
and the main difference here is that unlike OpenAL
buffer data, which makes a copy of the data,

00:16:58.289 --> 00:17:04.710
the implementation makes a copy into its own internal
buffers so that your application can release the memory.

00:17:04.710 --> 00:17:10.480
alBufferDataStatic, the implementation uses the data
that you provided directly, so it's really important

00:17:10.480 --> 00:17:16.730
that you don't dispose that audio data
before the library is finished using it.

00:17:16.730 --> 00:17:23.289
So we'd recommend that you actually use the BufferDataStatic
method primarily because you avoid a mem copy

00:17:23.289 --> 00:17:26.019
which is kind of an expensive operation on the device.

00:17:26.019 --> 00:17:33.180
And if you're constantly changing the data that's being
played by these buffers, those mem copies can add up.

00:17:33.180 --> 00:17:39.410
And calling the BufferDataStatic method really
is a bonus for your in terms of performance.

00:17:39.410 --> 00:17:44.980
So as I mentioned, you really don't want to release this
data before we're done because we may be rendering that data

00:17:44.980 --> 00:17:46.460
at the time that you're trying to release it.

00:17:46.460 --> 00:17:53.769
So when you call alDeleteBuffers or the
BufferDataStatic in order to replace the data

00:17:53.769 --> 00:17:59.039
that that buffer object is representing, you have to check
your error, it's really important that you check that error,

00:17:59.039 --> 00:18:03.200
and if calling alGetError returns no
error, that's a great time for you

00:18:03.200 --> 00:18:07.460
to release that memory that the library is using.

00:18:07.460 --> 00:18:11.690
So the last thing I really want to
talk about is tuning your assets.

00:18:11.690 --> 00:18:18.460
And what I mean by that is being aware of sample rates
both in terms of the sample rates of the audio data

00:18:18.460 --> 00:18:25.039
in your assets, and the sample rate of the rendering chain
as it's rendering your data through the OpenAL pipeline.

00:18:25.039 --> 00:18:31.970
So what you'll see on the diagram here, on the far
right-hand side, you see 44k, that's the default rate

00:18:31.970 --> 00:18:40.000
that the device is running at, and so by default if you
don't make any other changes, will be running at 44k.

00:18:40.000 --> 00:18:46.349
And then you'll see on the left-hand side, there's a
stack of sources here that represent the audio data

00:18:46.349 --> 00:18:51.759
that each source is playing, and there's a
bunch of various sample rates, 8k, 11k, 22k.

00:18:51.759 --> 00:18:55.930
This is a really typical scenario in the
applications that we see using OpenAL.

00:18:55.930 --> 00:19:03.769
So to have an understanding of how you can
tune this, let's kind of go down the list here.

00:19:03.769 --> 00:19:13.059
By default, the context or the mixer will render its audio
at the same sample rate that the device is currently set at.

00:19:13.059 --> 00:19:17.230
So in our case here, the context is
going to do all its mixing at 44k.

00:19:17.230 --> 00:19:21.870
And this has a couple of implications
for the performance of your app.

00:19:21.869 --> 00:19:28.349
You'll see in the big red circle what's going on here is
that every one of those sources that's not at 44k has to get

00:19:28.349 --> 00:19:32.019
up sample, so that's a processing
hit, doing the sample rate conversion.

00:19:32.019 --> 00:19:37.769
The next thing that you get hit with is since
it's a 44k, we're now pushing a lot more data

00:19:37.769 --> 00:19:42.460
through the mixing process, so that's
another performance hit your going to take

00:19:42.460 --> 00:19:46.410
when you're rending this particular scenario.

00:19:46.410 --> 00:19:51.310
So one of the things that you can
do is to set the context mixer rate

00:19:51.309 --> 00:19:54.879
to whatever the highest sample rate
of your assets are for instance.

00:19:54.880 --> 00:19:58.610
So in our case here we've got - the
highest sample rate that we have

00:19:58.609 --> 00:20:03.329
in our assets is 22k, so we can set our mixer to 22k.

00:20:03.329 --> 00:20:07.480
And there's really no benefit at running at
44k, you don't get any extra quality if none

00:20:07.480 --> 00:20:10.279
of your assets are actually sampled at 44k.

00:20:10.279 --> 00:20:12.920
So we get a couple of wins in this scenario.

00:20:12.920 --> 00:20:19.519
First of all, we've cut the amount of data that the
mixer is using in half, so instead of 44k data running

00:20:19.519 --> 00:20:21.579
through the mixing process, we're at 22k.

00:20:21.579 --> 00:20:24.119
And then you'll also notice that
the samples that were already

00:20:24.119 --> 00:20:29.529
at the mixer sample rate no longer need a
sample rate conversion, so you get two wins.

00:20:29.529 --> 00:20:32.990
And this is probably one of the
best things that you can do in terms

00:20:32.990 --> 00:20:36.910
of getting some extra performance
out of your OpenAL pipeline.

00:20:36.910 --> 00:20:42.220
Now another thing that you can do is actually
resample your assets at the same rate

00:20:42.220 --> 00:20:46.410
that you know you'll be setting your
mixer to when you're at runtime.

00:20:46.410 --> 00:20:50.740
So that eliminates all the sample
rate conversion in the mixing process.

00:20:50.740 --> 00:20:55.069
And to make this decision you'll have to make
the trade-off because of course now your assets

00:20:55.069 --> 00:21:00.759
in your application package are going to be larger, so this
may or may not make sense for you, but it is another thing

00:21:00.759 --> 00:21:05.039
that you can do to maybe tweak your
performance just a little bit more.

00:21:05.039 --> 00:21:12.819
The last thing that you can do, if your application
isn't playing its own 44k stereo background track

00:21:12.819 --> 00:21:18.769
and you're just playing the sounds that are moving, you may
want to consider setting the hardware actually to 22k too.styl"v

00:21:18.769 --> 00:21:24.420
And you can do that using the AudioSession
property with the preferred hardware sample rate.

00:21:24.420 --> 00:21:29.920
So in this case we also eliminate a sample rate
conversion at the device level where we'd have

00:21:29.920 --> 00:21:32.910
to upsample our mix up to whatever the hardware rate is.

00:21:32.910 --> 00:21:38.070
As Bill mentioned earlier when he talked about
AudioSession, you may not actually succeed in setting it,

00:21:38.069 --> 00:21:43.029
the iPod may be playing or something else
may be already using the hardware 44k,

00:21:43.029 --> 00:21:45.940
so you have to be prepared to not actually get that rate.

00:21:45.940 --> 00:21:48.580
But that's yet one more thing that you can do.

00:21:48.579 --> 00:21:55.250
So those three things in tuning your assets really can
help you get some extra performance out of using OpenAL.

00:21:55.250 --> 00:22:01.730
So with that, I'd like to bring up Doug Wyatt, he's going
to talk about input and output units, and here he is.

00:22:01.730 --> 00:22:02.630
Thanks.

00:22:02.630 --> 00:22:08.130
>> I'm Doug Wyatt, I work in the Core Audio
group also, and I'd like to talk a bit

00:22:08.130 --> 00:22:13.440
about audio units used for input and output.

00:22:13.440 --> 00:22:18.400
So the names of these two audio units,
we have one on the desktop called AUHAL

00:22:18.400 --> 00:22:22.210
and on the phone we've got one called AURemoteIO.

00:22:22.210 --> 00:22:29.380
They're similar enough that I'll talk about them
collectively and describe some differences here.

00:22:29.380 --> 00:22:34.470
We'll talk about when to use one, how these
units work, and I'll dive into the details

00:22:34.470 --> 00:22:38.160
of actually setting one up and using one.

00:22:40.369 --> 00:22:47.599
So when would you want to
actually use an input/output audio unit

00:22:47.599 --> 00:22:50.659
as opposed to some of these higher level APIs?

00:22:50.660 --> 00:22:58.460
The main consideration is that your application needs
a high degree of responsiveness and low latency,

00:22:58.460 --> 00:23:03.490
and you're ready to operate in a
constrained environment in order to do that.

00:23:03.490 --> 00:23:09.549
So for example, you might have a custom audio engine
of some sort, you might be doing internet telephony,

00:23:09.549 --> 00:23:13.169
you might have a software musical instrument.

00:23:13.170 --> 00:23:21.710
On the desktop you might consider using the Core
Audio HAL directly for these kinds of applications.

00:23:21.710 --> 00:23:29.019
Although the advantage of AUHAL on the desktop is that
it takes care of a lot of the busy work of being a client

00:23:29.019 --> 00:23:34.839
of the HAL, namely responding to
certain notifications and dealing

00:23:34.839 --> 00:23:39.000
with some complexities of multi-channel audio devices.

00:23:39.000 --> 00:23:41.980
On the iPhone, the Core Audio HAL simply isn't available,

00:23:41.980 --> 00:23:52.410
so AURemoteIO is the lowest level way
for your app to get audio in and out.

00:23:52.410 --> 00:23:58.180
So just as a big picture of what you have to
do as the client of one of these I/O units,

00:23:58.180 --> 00:24:02.430
you have to say whether you want
to do input or output or both,

00:24:02.430 --> 00:24:10.430
you need to tell it what PCM formats
you want to use as your client formats.

00:24:10.430 --> 00:24:19.900
Typically you'll care about things like 16 versus 8.24
fixed bit, integer formats or floating point.

00:24:19.900 --> 00:24:25.550
But you may want to copy the sample rate from the hardware.

00:24:25.549 --> 00:24:31.269
In terms of connecting your audio unit up and
providing output to it, you can get your output

00:24:31.269 --> 00:24:35.639
from another Audio Unit like a mixer or an effect.

00:24:35.640 --> 00:24:43.390
Or you can supply a callback function that actually
synthesizes or is the last chain in a big synthesis engine.

00:24:43.390 --> 00:24:51.330
If you're doing audio input, you specify a callback
function that gets notified when input becomes available.

00:24:51.329 --> 00:24:56.329
And when your callback is called for reasons
of symmetry in the audio unit spec.

00:24:56.329 --> 00:25:02.869
At that time when you get that notification
you'll turn around and call AudioUnitRender.

00:25:02.869 --> 00:25:11.069
Now in both cases, the audio unit will convert between
some virtual format, that might be the hardware format.

00:25:11.069 --> 00:25:16.869
It will convert between that virtual or hardware
format and your client format that you set up.

00:25:16.869 --> 00:25:20.389
So we'll go into the details of that in a minute.

00:25:20.390 --> 00:25:29.720
So here looking at some code is the mechanics of
actually locating and opening one of these I/O units.

00:25:29.720 --> 00:25:37.880
This is for the iPhone where we're going to
locate the remote I/O audio unit, it's an I/O unit

00:25:37.880 --> 00:25:42.250
but for historical reasons we call
them output units in the API.

00:25:42.250 --> 00:25:47.769
So we set up this audio component description and we pass

00:25:47.769 --> 00:25:52.269
that to AudioComponentFindNext
which gives us back the component.

00:25:52.269 --> 00:25:59.009
And given the component, then we can turn around and
create an instance of it, and that's our I/O unit.

00:25:59.009 --> 00:26:04.559
Now this audio component API is also available
on the desktop starting with Snow Leopard.

00:26:04.559 --> 00:26:09.369
On earlier versions of Mac OS X you have to use the classic,

00:26:09.369 --> 00:26:12.979
well I shouldn't say classic, but
the carbon component manager.

00:26:12.980 --> 00:26:18.420
And we have a lot of examples of how to do
that, it's very parallel to audio components.

00:26:18.420 --> 00:26:25.289
But on the phone and Snow Leopard going forward,
we want you to be using the audio component APIs.

00:26:25.289 --> 00:26:36.059
Okay, so this little diagram here turns out to
be really useful when you start using this unit

00:26:36.059 --> 00:26:42.710
to just make a little drawing of this and set it next to
you while you program, because you're going to be referring

00:26:42.710 --> 00:26:48.829
to these four quadrants of the unit all the time.

00:26:48.829 --> 00:26:54.220
As you may know from working with other audio units,
they have this concept of elements, for example,

00:26:54.220 --> 00:26:59.990
a mixer has input elements for
each source that you're giving it.

00:26:59.990 --> 00:27:05.720
In the case of the I/O units, there are these two
elements, one is for input and one is for output.

00:27:05.720 --> 00:27:11.690
Element zero is for output, and I remember that
because output starts with O which looks like a 0.

00:27:11.690 --> 00:27:17.720
And similarly input starts with I which looks like 1.

00:27:17.720 --> 00:27:26.289
Now we see that two of the four corners here are
the virtual output and input formats that come

00:27:26.289 --> 00:27:32.440
from the HAL, well I'll leave that detail out.

00:27:32.440 --> 00:27:34.850
Those are your virtual input formats.

00:27:34.849 --> 00:27:43.169
And you've got your client formats, and in both
cases those orange/yellow arrows signify conversions

00:27:43.170 --> 00:27:48.019
that the audio unit is performing
for you between those two formats.

00:27:51.269 --> 00:27:56.369
And then before we get into the actual
process of specifying these formats,

00:27:56.369 --> 00:28:00.149
there's just a few things to think about, some wrinkles.

00:28:00.150 --> 00:28:03.759
These apply mostly to the iPhone.

00:28:03.759 --> 00:28:09.269
One is if you're going to be doing audio
input, you should be asking AudioSession first

00:28:09.269 --> 00:28:13.369
of all whether there's actually
even any input hardware present,

00:28:13.369 --> 00:28:22.539
because on the iPod Touch the answer will be no
unless there's a headset or other accessory attached.

00:28:22.539 --> 00:28:30.960
You will probably care a lot about the hardware sample rate,
so you can use AudioSession with a property listener to ask

00:28:30.960 --> 00:28:34.740
for notifications when sample rate changes.

00:28:34.740 --> 00:28:41.519
You will want to probably tell AudioSession
what sample rate you would like to operate at.

00:28:41.519 --> 00:28:44.849
And be aware that you won't always get that sample rate.

00:28:44.849 --> 00:28:50.759
If you're mixing with others, as Bill described in the
previous session, the hardware sample rate could be set

00:28:50.759 --> 00:28:54.559
to something else and it won't change for you.

00:28:54.559 --> 00:29:03.230
And if you're dealing with an original iPhone, when it goes
into a mode with input active, then the hardware sample rate

00:29:03.230 --> 00:29:09.460
with the built-in receiver and
microphone can only be 8 kilohertz.

00:29:09.460 --> 00:29:16.650
Unlike the newer models which support a wide
variety of sample rates for both input and output.

00:29:17.789 --> 00:29:23.079
The other thing is especially if you're familiar
with using audio units in other contexts,

00:29:23.079 --> 00:29:29.269
you want to call AudioUnitInitialize relatively
early because it's only at this time on the phone

00:29:29.269 --> 00:29:37.819
that your AudioSession parameters, most notably the sample
rate and the enabling of input, actually take effect.

00:29:37.819 --> 00:29:42.129
So you want to call that to establish your sessions,

00:29:42.130 --> 00:29:46.200
or to find out what sample rates you
actually get from your AudioSession.

00:29:48.970 --> 00:30:00.860
So moving on to choosing your client formats for AURemoteIO,
typically you will look at the virtual I/O formats.

00:30:00.859 --> 00:30:09.209
And if necessary, the audio unit is capable of doing
a sample rate conversion between a rate that you like,

00:30:09.210 --> 00:30:15.650
but ideally in a lot of apps it's a good idea to
just play along with the hardware sample rate.

00:30:15.650 --> 00:30:18.140
If you specified it, then you got the one you want.

00:30:18.140 --> 00:30:20.740
If you didn't, well then you do have to play along.

00:30:20.740 --> 00:30:24.259
The difficulty is the rate conversion
is a bit computationally expensive

00:30:24.259 --> 00:30:27.369
and it's not of the highest quality.

00:30:27.369 --> 00:30:32.750
One mistake that I've seen some people making, and it's
partially our fault because we shipped a sample like this,

00:30:32.750 --> 00:30:39.670
and have since corrected it, but don't just simply
blindly copy that virtual I/O format to your client format

00:30:39.670 --> 00:30:44.810
because you don't know what that virtual
format is, it can and will change.

00:30:44.809 --> 00:30:53.769
But on the other side of the coin, you are in control
of the code that is used in that client format of yours.

00:30:53.769 --> 00:31:01.809
So be specific about what your code is expecting and just
copy aspects of the virtual format that you want to use

00:31:01.809 --> 00:31:07.379
as variable, typically the sample
rate and the channel count.

00:31:08.930 --> 00:31:15.720
So going back to our diagram here, so this just
shows you that you use the AudioUnitSubProperty call

00:31:15.720 --> 00:31:19.809
with the property stream format to set your client formats.

00:31:19.809 --> 00:31:26.859
And a few more small considerations
for AUHAL, these are differences.

00:31:26.859 --> 00:31:33.889
Since there's no AudioSession here you get the
hardware sample right from the virtual I/O format.

00:31:33.890 --> 00:31:42.280
You use AudioUnitAddPropertyListener to
watch for changes to those sample rates.

00:31:42.279 --> 00:31:49.139
And on the desktop with AUHAL, you want to avoid
changing the hardware sample rate because the user tends

00:31:49.140 --> 00:31:58.610
to be a bit more attached to it especially if he's
an audiophile with 96 kilohertz output system.

00:31:58.609 --> 00:32:06.849
The consolation is we have very good sample rate quality,
sample rate converters in terms of quality on the desktop.

00:32:06.849 --> 00:32:09.009
Input is a little strange on the desktop.

00:32:09.009 --> 00:32:17.069
Here you may want to give your user control over the
hardware sample rate to avoid rate conversions on input.

00:32:17.069 --> 00:32:23.029
If he's making a recording for instance
in an application like ProTools or Logic,

00:32:23.029 --> 00:32:29.519
he's going to care about how much disk
space he's chewing up as he's recording.

00:32:29.519 --> 00:32:31.529
[ Laughter ]

00:32:31.529 --> 00:32:37.029
>> And part of the picture here is that AUHAL
will not do sample rate conversion on the desktop,

00:32:37.029 --> 00:32:43.559
you have to record at the hardware rate, which means
that if you want your audio at some other sample rate,

00:32:43.559 --> 00:32:47.919
you'll have to store it off in a ring
buffer and do other rate conversions.

00:32:47.920 --> 00:32:59.410
Okay, going back to some code, here's what it actually looks
like to call AudioUnitSetProperty for the stream format.

00:32:59.410 --> 00:33:06.100
One kind of pervasive data structure in the Core
Audio APIs is the audio stream basic description.

00:33:06.099 --> 00:33:08.799
And this is how you specify a PCM format.

00:33:08.799 --> 00:33:14.889
And we'll see a detailed example of that in a moment.

00:33:14.890 --> 00:33:20.030
So now we've set our client stream formats
and we're ready to connect up our code

00:33:20.029 --> 00:33:25.119
to send audio out and receive the incoming audio.

00:33:25.119 --> 00:33:28.309
On the output side, we've got two properties.

00:33:28.309 --> 00:33:36.599
The first SetRenderCallback says here's my function
pointer that I want you to callback for the audio

00:33:36.599 --> 00:33:39.309
that I as an application want to send out.

00:33:39.309 --> 00:33:46.629
You can also call or set the property MakeConnection,
and what that says is here's another audio unit

00:33:46.630 --> 00:33:50.860
from which the outgoing audio is to be obtained.

00:33:50.859 --> 00:33:59.079
On the input side, the property is called SetInputCallback,
and that just simply sets a function and user data,

00:33:59.079 --> 00:34:03.579
and that function of yours will get
called back when input becomes available.

00:34:04.920 --> 00:34:09.240
These calls are brain dead simple,
they just have one argument.

00:34:09.239 --> 00:34:17.409
The first one I mentioned earlier, AudioUnitInitialize,
that gets your session set up and established.

00:34:17.409 --> 00:34:21.769
The second one, AudioOutputUnitStart,
even if you're doing input.

00:34:21.769 --> 00:34:26.730
That actually starts the hardware running
and starts your callbacks getting called.

00:34:26.730 --> 00:34:35.130
So here is a diagram of what happens
once the hardware has been started.

00:34:35.130 --> 00:34:38.880
The first thing that happens, assuming
you're doing both input and output,

00:34:38.880 --> 00:34:44.880
the first thing that happens is your input callback
gets called and told that input is available.

00:34:44.880 --> 00:34:52.019
And in response to that you'll turn around and call
AudioUnitRender, and the AU will convert the incoming audio

00:34:52.019 --> 00:34:58.460
in the virtual input format, to your client format,
and that comes back to you from AudioUnitRender.

00:34:58.460 --> 00:35:09.659
On the output side, element zero will end up pulling
upstream from its source whether it's your callback function

00:35:09.659 --> 00:35:13.730
or that other audio unit that you connected to it.

00:35:14.940 --> 00:35:21.240
So let's talk about the context in
which your callback functions execute.

00:35:21.239 --> 00:35:28.709
This is a bit tricky and daunting to
people who haven't done it before.

00:35:28.710 --> 00:35:35.079
Your callback functions run on a real-time priority thread,
and this is the highest priority thread in the system,

00:35:35.079 --> 00:35:37.049
so you have to be pretty careful about what you do here.

00:35:37.050 --> 00:35:41.760
You can actually make the system unresponsive
for long periods of time if you get

00:35:41.760 --> 00:35:46.040
into an endless loop or try to
do inordinate amounts of work.

00:35:46.039 --> 00:35:49.420
So again, it's important to be efficient.

00:35:49.420 --> 00:35:56.610
You really can't allocate memory, touch the file
system or the network or any other API like that.

00:35:56.610 --> 00:36:01.940
And the reason is because all of these things
will take mutexes and block your thread.

00:36:01.940 --> 00:36:05.420
And you really don't want to block the audio I/O thread

00:36:05.420 --> 00:36:11.599
because the result is it will miss
its deadlines and we'll hear a glitch.

00:36:11.599 --> 00:36:17.559
So if you really have to lock, maybe you're changing
the entire configuration of the engine, okay,

00:36:17.559 --> 00:36:21.809
so you're doing that from say your U/I thread.

00:36:21.809 --> 00:36:28.840
And then your render thread at that point should be trying
to take the lock and then failing and producing silence.

00:36:28.840 --> 00:36:34.680
It's a much better and more efficient pattern
than actually blocking the render thread.

00:36:34.679 --> 00:36:39.919
One other thing to mention here, a lot of
people learn Objective-C and think it's

00:36:39.920 --> 00:36:42.579
so great they want to use it absolutely everywhere.

00:36:42.579 --> 00:36:47.119
And in the U/I application I completely understand
that until you get down into the level of working

00:36:47.119 --> 00:36:51.389
on real-time threads because there is some overhead here.

00:36:51.389 --> 00:36:56.839
In particular on the desktop, you don't want
to mix Objective-C and real-time threads

00:36:56.840 --> 00:37:03.100
in a garbage collection enabled app because that
makes your real-time thread start participating

00:37:03.099 --> 00:37:09.400
in the garbage collection scheme and
potentially causing major glitching.

00:37:09.400 --> 00:37:18.360
On the iPhone we've seen some new converts to Objective-C
invoking methods to compute every sample that they output,

00:37:18.360 --> 00:37:22.300
and that's about maybe 20 times
less efficient than it could be,

00:37:22.300 --> 00:37:31.960
so it's good enough to just use C arrays
and pointers in your real-time threads.

00:37:31.960 --> 00:37:34.409
Be careful about what you do on every sample.

00:37:34.409 --> 00:37:44.759
So that caveat aside, let's walk through a bit of an example
of just to look at all the code that's actually involved

00:37:44.760 --> 00:37:49.560
in getting some input from AURemoteIO on the phone.

00:37:49.559 --> 00:37:55.099
This is pretty much what we looked at before using the
audio component calls to find and open the I/O unit.

00:37:55.099 --> 00:38:02.949
Here is a little detail that I mentioned
earlier but we didn't look at in code.

00:38:02.949 --> 00:38:08.799
We have to explicitly enable audio input,
and while we're at it since we're not going

00:38:08.800 --> 00:38:12.690
to do output we'll save some efficiency and turn that off.

00:38:12.690 --> 00:38:20.740
So the property is called EnableIO, the value is 1 for
enabling input and we're going to do that on the input scope

00:38:20.739 --> 00:38:28.829
of element 1, which as we remember,
element 1 is like I for input.

00:38:28.829 --> 00:38:37.549
And completely parallel to that, we disable audio output,
and then we're ready to initialize the audio unit.

00:38:37.550 --> 00:38:41.920
And at that point we have a valid virtual I/O format.

00:38:44.760 --> 00:38:48.300
The type gets a little smaller here.

00:38:48.300 --> 00:38:55.740
So we can look at this virtual input format that we got
back by calling AudioUnitGetProperty for the stream format.

00:38:55.739 --> 00:39:01.069
And we're going to peek at that, that's
now in our I/O format local variable.

00:39:01.070 --> 00:39:08.180
And as you see, as you scan through the code there, the
only thing we care about there is I/O format.msamplerate.

00:39:08.179 --> 00:39:17.049
We're going to go and set up client format to otherwise
be entirely programmatically generated on the fly here

00:39:17.050 --> 00:39:21.440
to be 16 bit native endian stereo interleaved integers.

00:39:21.440 --> 00:39:25.210
But we are going to copy that hardware sample rate.

00:39:25.210 --> 00:39:32.690
So having set up that client format, then we can make that
call to AudioUnitSetProperty to set that client format

00:39:32.690 --> 00:39:39.769
as the stream format for the output
scope of the input element plus 1.

00:39:39.769 --> 00:39:45.469
So we've set our formats and we're almost ready to connect.

00:39:45.469 --> 00:39:50.379
First we need to set up a little bit of context
that we want to have available when it's time

00:39:50.380 --> 00:39:54.000
for us to render or process our incoming audio.

00:39:54.000 --> 00:40:01.610
So I've got a little structure called myContext
where I'm going to hold onto a copy of my I/O unit.

00:40:01.610 --> 00:40:06.700
I'm going to allocate my input buffer
list to be an audio buffer list structure,

00:40:06.699 --> 00:40:11.500
I'll fill that in on the fly later in the render proc.

00:40:11.500 --> 00:40:17.690
And I'm going to hold onto a copy of the client format
that I set up and we'll see why when we get there.

00:40:17.690 --> 00:40:25.590
So having set up that context, I create an
AURenderCallbackStruct, and it has two members,

00:40:25.590 --> 00:40:29.610
one is the function pointer, to
MyRecordInput which we'll see in a moment.

00:40:29.610 --> 00:40:33.490
And the other is a pointer to myContext.

00:40:33.489 --> 00:40:39.669
Just a note here about this example code MyContext
looks like a local variable here but it shouldn't be,

00:40:39.670 --> 00:40:47.230
it should be a member of something or a global because
it's going to be accessible when we get called back.

00:40:47.230 --> 00:40:54.059
Okay, so we've set the input callback and
we're ready to actually start performing I/O,

00:40:54.059 --> 00:40:57.230
and to do that we call AudioOutputUnitStart.

00:40:57.230 --> 00:41:06.289
And here in a nutshell is MyRecordInput, the
function that we call in the render proc,

00:41:06.289 --> 00:41:10.230
this is what gets called when input becomes available.

00:41:10.230 --> 00:41:18.880
So you can see the first thing we do is retrieve our context
which came in as the first function argument in RefCon.

00:41:18.880 --> 00:41:24.210
Given the context, then we can get to the
input buffer list that we allocated earlier,

00:41:24.210 --> 00:41:27.190
and now we can fill in that audio buffer list.

00:41:27.190 --> 00:41:32.519
And what we're doing here is we're preparing
it to receive the incoming audio data.

00:41:32.519 --> 00:41:38.889
And we really ought to be doing this every
time through the loop, every time we get called

00:41:38.889 --> 00:41:42.420
to be notified that input audio is available.

00:41:42.420 --> 00:41:48.309
This code is hard coded to know
that we specified stereo interleaved

00:41:48.309 --> 00:41:52.469
so I just got one buffer with m number of channels of two.

00:41:52.469 --> 00:42:00.159
mdata can be null because there's a buffer inside the audio
unit that it's already allocated for me for this purpose.

00:42:00.159 --> 00:42:04.549
When I come back from AudioUnitRender
that will be filled in.

00:42:04.550 --> 00:42:09.539
A really crucial thing here that even I
screw up from time to time when writing code,

00:42:09.539 --> 00:42:15.610
is mDataByteSize needs to be refreshed
every time we call AudioUnitRender.

00:42:15.610 --> 00:42:26.150
And the value of that should reflect exactly how many
bytes of audio I expect to get back from AudioUnitRender.

00:42:26.150 --> 00:42:32.840
And that computation is the number of frames that
is available, this render cycle, inNumberFrames.

00:42:32.840 --> 00:42:37.410
And here we see why I saved off my client input format.

00:42:37.409 --> 00:42:47.899
So instead of hard coding this, I just thought I'd show you
as a good abstract example, we know the number of frames,

00:42:47.900 --> 00:42:51.369
we know the number of bytes per
frame, and that's the computation

00:42:51.369 --> 00:42:55.469
to obtain the data byte size, multiplying those two.

00:42:55.469 --> 00:43:03.239
So now we have a ABL, which is our AudioBufferList,
just a short name for the context's InputBufferList.

00:43:03.239 --> 00:43:11.289
And I can call AudioUnitRender, and if I don't get an
error back from there, that means that ABL or context

00:43:11.289 --> 00:43:19.059
to myInputBufferList contains the samples
from the hardware in my client data format.

00:43:19.059 --> 00:43:21.150
So the unit performed the conversion.

00:43:21.150 --> 00:43:26.590
And whatever I want to do with that, whether
it's store it in a ring buffer, process it.

00:43:26.590 --> 00:43:33.960
If I had output enabled I could process it and
play it back backwards or whatever I liked.

00:43:33.960 --> 00:43:37.650
So there's my input function.

00:43:37.650 --> 00:43:44.139
So that's a pretty complete example
of using AURemoteIO to obtain input.

00:43:44.139 --> 00:43:54.750
So in summary, you would use AURemoteIO and AUHAL, use
I/O audio units to do low latency responsive audio I/O.

00:43:54.750 --> 00:44:00.059
Be careful when you're specifying your client data
formats, don't assume anything about the virtual ones.

00:44:00.059 --> 00:44:05.690
The unit knows what they are and will convert from
the virtual ones to the client ones you specify.

00:44:05.690 --> 00:44:11.889
And just be really careful about what you do in your
I/O callback functions, please don't allocate memory,

00:44:11.889 --> 00:44:16.440
take locks or do too much with Objective-C.

00:44:16.440 --> 00:44:18.000
Thanks for listening.

00:44:18.000 --> 00:44:22.900
[ Applause ]

00:44:22.900 --> 00:44:26.170
>> Here's Eric to talk about voice processing.

00:44:26.170 --> 00:44:27.500
>> Thank you, Doug.

00:44:27.500 --> 00:44:33.590
So my name is Eric Allamanche, I'm also
on the Core Audio engineering team.

00:44:33.590 --> 00:44:39.019
And so this is the last talk in this session, and I'm
going to talk of a new audio unit that we've added

00:44:39.019 --> 00:44:44.110
in iPhone OS 3.0 which we call
the Voice Processing Audio Unit.

00:44:44.110 --> 00:44:52.829
So this is kind of a remote audio
unit that Doug just presented.

00:44:52.829 --> 00:44:59.389
So first I'm going to explain why do
we need a voice processing audio unit,

00:44:59.389 --> 00:45:08.089
and it turns out to be an Acoustic Echo Canceler or part
of the processing audio unit as an Acoustic Echo Canceler.

00:45:08.090 --> 00:45:13.970
Then I will present a few features that we exposed
through the public APIs which are available

00:45:13.969 --> 00:45:17.539
to you if you want to use this in your code.

00:45:17.539 --> 00:45:26.070
And then I'm going to show a very small example how you
interface your code with the voice processing audio unit.

00:45:26.070 --> 00:45:31.460
And finally I will wrap up this talk with a small demo.

00:45:31.460 --> 00:45:35.470
So why a voice processing unit?

00:45:35.469 --> 00:45:41.579
In full-duplex audio systems like
telephony systems, the loudspeaker

00:45:41.579 --> 00:45:46.650
and the microphone are typically
in use simultaneously first.

00:45:46.650 --> 00:45:52.809
And also are typically spaced only
a few inches away from each other.

00:45:52.809 --> 00:46:00.849
This means that the microphone captures parts of
the sound which are emitted by the loudspeaker.

00:46:00.849 --> 00:46:11.319
And in a telephony like application this means that
this sound is looped back to the sender which just sent

00:46:11.320 --> 00:46:16.010
out the signal which went out to the loudspeaker.

00:46:16.010 --> 00:46:20.720
And due to various delays in the processing and
transmission chains we have nowadays which is

00:46:20.719 --> 00:46:26.109
about a ballpark figure of 100 milliseconds for roundtrips.

00:46:26.110 --> 00:46:34.240
This signal is perceived as an Echo to the other party,
and one day or another we've all experienced this kind

00:46:34.239 --> 00:46:40.769
of situation and it becomes extremely annoying,
and especially if the delay stretches over time.

00:46:40.769 --> 00:46:46.119
So you can't have conversations in such conditions.

00:46:46.119 --> 00:46:52.690
So this is a general problem in telephony
and it also applies to analog telephony.

00:46:52.690 --> 00:47:01.920
And this has been an extensive topic which has been
studied over last decades, so the solution is simply

00:47:01.920 --> 00:47:11.159
to apply what we call an Acoustic Echo
Canceler or AEC to the captured signal.

00:47:11.159 --> 00:47:19.449
So now in the context of the iPhone
for example, how would this apply?

00:47:19.449 --> 00:47:25.219
You see that on the bottom right you have the
microphone, on the bottom left you have the speaker,

00:47:25.219 --> 00:47:29.399
so these are separated by one and half inches apart.

00:47:29.400 --> 00:47:37.930
So there is a large amount of acoustic energy which is
sent to the speaker which goes back to the microphone.

00:47:37.929 --> 00:47:47.149
But even if you are using the receiver instead of the
speaker, you still have some echo which is noticeable.

00:47:47.150 --> 00:47:53.230
And this is simply due to the fact that the receiver and the
microphone are in the same enclosure, and as you can see,

00:47:53.230 --> 00:47:57.460
the distance is not that big, three inches or so.

00:47:57.460 --> 00:48:02.920
So in both cases we need to use
some echo cancellation techniques

00:48:02.920 --> 00:48:09.920
to reduce the amount of sound sent
back over the transducers.

00:48:09.920 --> 00:48:15.150
So one thing I wanted to point out is
that the iPhone is primarily a phone,

00:48:15.150 --> 00:48:27.500
so the base band unit is also already doing this
when we get a phone call, when we hit the speaker button,

00:48:27.500 --> 00:48:34.199
it goes into hands-free mode, and it
performs an echo cancellation already.

00:48:34.199 --> 00:48:41.509
But the thing is that this echo canceler is built-in
to the telephony part which isn't accessible to us,

00:48:41.510 --> 00:48:47.260
and we can't expose this to the [inaudible] for some APIs.

00:48:47.260 --> 00:48:58.370
So this is why we decided to add a new audio unit
in iPhone OS 3.0 which performs exactly this task,

00:48:58.369 --> 00:49:01.420
and of course that you can use in your applications.

00:49:01.420 --> 00:49:08.750
So now I just want to go through the
functionality of an Acoustic Echo Canceler.

00:49:08.750 --> 00:49:14.579
Now imagine the following situation on the left-hand side
there is the person who is called the near end speaker,

00:49:14.579 --> 00:49:18.360
for the sake of clarity it's going to be me, with my iPhone.

00:49:18.360 --> 00:49:28.840
And on the left-hand side, the blue
person is called the far end speaker.

00:49:28.840 --> 00:49:37.840
And so we have a peer-to-peer connection
or whatever kind of connection is set up.

00:49:37.840 --> 00:49:43.600
And so the far end speaker, which has the blue arrows,

00:49:43.599 --> 00:49:48.279
starts to speak in his own microphone
on the right-hand side.

00:49:48.280 --> 00:49:55.800
And so the signal propagates over the
transmission chain and comes into my device

00:49:55.800 --> 00:49:58.980
which is played back over the loudspeaker.

00:49:58.980 --> 00:50:07.179
So this acoustic wave gets reflected, in
this figure it's reflected in my forehead.

00:50:07.179 --> 00:50:16.419
And so the wave is bounced back to the microphone, and
this signal is then sent back to the far end speaker.

00:50:16.420 --> 00:50:23.980
So because of the delays I've mentioned before,
the far end speaker will hear himself with a delay

00:50:23.980 --> 00:50:27.369
of 100 or depending on the transmission chains.

00:50:27.369 --> 00:50:32.449
But it's going to be a noticeable
delay, it's going to be really annoying.

00:50:32.449 --> 00:50:44.589
So now if I start speaking on top of this, you see
that the blue and the red signals are mixed up together

00:50:44.590 --> 00:50:51.289
at the microphone level, and so this mix
is going back to the far end speaker.

00:50:51.289 --> 00:51:00.920
And so the far end speaker will hear my
voice but also will hear his or her voice.

00:51:00.920 --> 00:51:11.500
So to compensate this, we use what we call an Acoustic
Echo Canceler which is mainly built of two blocks;

00:51:11.500 --> 00:51:16.940
one is an adaptive filter, and
the second block is an estimator.

00:51:16.940 --> 00:51:23.440
So the estimator is hooked up between the signal
which is about to be sent to the loud speaker,

00:51:23.440 --> 00:51:27.880
and the signal which is captured by the microphone.

00:51:27.880 --> 00:51:37.099
And from these two signals, the estimator tries to
calculate a set of parameters by estimating the amount

00:51:37.099 --> 00:51:42.349
of blue signal which is captured by the microphone.

00:51:42.349 --> 00:51:48.259
And this set of parameters are then passed
over to the adaptive filter which tries

00:51:48.260 --> 00:51:56.040
to eliminate the blue component of this signal mixture.

00:51:56.039 --> 00:52:03.460
And at the end when we have the output, it's actually a red
signal which should be the near end speaker signal only.

00:52:03.460 --> 00:52:08.880
But this is of course under ideal conditions.

00:52:08.880 --> 00:52:17.710
So this is what happens basically when we have
a simple setup with two different persons.

00:52:17.710 --> 00:52:26.360
Now what happens if on my side I playback some
UI sounds or let's say a text message comes in

00:52:26.360 --> 00:52:38.160
and we have this tritone playing back or other kind
of sound effects if we are in a game scenario for example?

00:52:39.400 --> 00:52:48.840
If the signals are all the same, what would happen
depends where we're going to mix in this UI signal.

00:52:48.840 --> 00:52:58.070
So if we mix this signal in before in the chain,
then we will be able to [inaudible] this otherwise,

00:52:58.070 --> 00:53:03.830
it will go directly to the speaker and the other person will
hear exactly the sounds which I'm playing back on my side.

00:53:03.829 --> 00:53:12.259
And this is maybe an undesired effect because if we
have a multi-player game with voice chat capabilities,

00:53:12.260 --> 00:53:17.880
we will like to have the ability to play back
sounds on a local device but get rid of these sounds

00:53:17.880 --> 00:53:27.170
after some processing so that the other players
don't hear the sounds playing which are played back.

00:53:27.170 --> 00:53:37.820
So the voice signals are typically at 8 kilohertz
for using telephony sampling rates, but the UI sounds

00:53:37.820 --> 00:53:41.850
or the other sounds may have higher sampling rates.

00:53:41.849 --> 00:53:45.279
So in order to cope with the differences
of the sampling rates,

00:53:45.280 --> 00:53:50.630
we need to have some additional sample
rates, converters in the chains.

00:53:50.630 --> 00:53:53.880
So just to illustrate this again, so we go back,

00:53:53.880 --> 00:53:59.619
this was the scenario where we had only the
two speakers, and now I'm adding an app sound.

00:53:59.619 --> 00:54:07.750
And I'm adding this at this level here because I want
this sound to be estimated by the system and also removed

00:54:07.750 --> 00:54:11.889
from the signal which is going to
be sent back to the far end speaker.

00:54:11.889 --> 00:54:21.039
And so this is what's happening, so my app sound which
is characterized by a green arrow, goes into this path,

00:54:21.039 --> 00:54:29.150
and so the color changes of course of the
signal which is sent out over the loudspeaker.

00:54:29.150 --> 00:54:41.760
But since this app sound is mixed before it is rendered
through the loudspeaker, we're able to treat this.

00:54:41.760 --> 00:54:52.820
So we've seen that there are two
operation modes in this audio unit.

00:54:52.820 --> 00:55:02.650
And these operation modes relate to the
fact how do we set up the sampling rates?

00:55:02.650 --> 00:55:10.720
So if we do a voice-only application, by means of setting
up the AudioSession parameters as you have learned

00:55:10.719 --> 00:55:18.169
in the prior session, by setting the AudioSession
parameters, setting the device sampling rate

00:55:18.170 --> 00:55:21.760
to be the same sampling rate as the voice client wants,

00:55:21.760 --> 00:55:28.650
then in this case we only have only single
sampling rate in the whole system from end to end.

00:55:28.650 --> 00:55:37.389
So the far end speakers at 8 kilohertz, but also my
loudspeaker and my microphone are operated at 8 kilohertz.

00:55:37.389 --> 00:55:42.469
But this means if I play back a sound, then the
sounds will have to be down sampled to 8 kilohertz

00:55:42.469 --> 00:55:48.049
to the target sampling rate of the
device, and we lose quality at this level.

00:55:48.050 --> 00:55:55.010
On the other hand, if we use the dual sample
rates mode, as shown in the previous slides,

00:55:55.010 --> 00:56:00.360
we would set up the audio device at the highest
sampling rate, let's say 44 kilohertz for example,

00:56:00.360 --> 00:56:06.329
and then the system will internally
do the down and up sampling

00:56:06.329 --> 00:56:11.119
to the voice signals from 8k to 44.1k and back and forth.

00:56:11.119 --> 00:56:19.179
And this allows to play back UI sounds and
other sounds at the highest possible qualities.

00:56:19.179 --> 00:56:28.099
And so this an ideal mode for multi-player
games which have voice chat capabilities.

00:56:28.099 --> 00:56:36.829
Also notice that you control this indirectly by
setting the preferred hardware sampling rates,

00:56:36.829 --> 00:56:47.519
so this is done automatically behind the scenes, there
is no switch where you would change this mode directly.

00:56:47.519 --> 00:56:59.800
So UI and other sounds are canceled because they
are mixed at some point in the rendering chain.

00:56:59.800 --> 00:57:06.260
And we added also an option, an automatic game
control unit after the processing in order

00:57:06.260 --> 00:57:13.070
to boost actually the voice which is played locally.

00:57:13.070 --> 00:57:22.490
The supported voice client sampling rates are 8, 11, 12,
and 16 kilohertz, the usual suspects for voice applications.

00:57:22.489 --> 00:57:29.079
And one important note also is that the secondary
audio signal is ducked to maintain intelligibility.

00:57:29.079 --> 00:57:36.590
So the problem is that if you play back a sound over
the local speaker and the sound level is extremely high

00:57:36.590 --> 00:57:40.910
and you begin to speak on top of
this, the system may not even notice

00:57:40.909 --> 00:57:44.469
that you are talking because the sound level is too high.

00:57:44.469 --> 00:57:47.659
And if the system is not able to detect your voice,

00:57:47.659 --> 00:57:51.869
then it won't be able to remove
anything and it will remove everything.

00:57:51.869 --> 00:57:55.849
So the other person, the far end
speaker, won't hear you at all.

00:57:55.849 --> 00:58:01.809
And in order to compensate this,
we duck the secondary audio signal.

00:58:01.809 --> 00:58:08.090
But one thing to keep in mind is that audio
unit won't handle any encoding or decoding,

00:58:08.090 --> 00:58:15.340
so if you want to encode this in [inaudible]
or whatever format you wish to encode it,

00:58:15.340 --> 00:58:18.220
you have to take care of this explicitly.

00:58:20.440 --> 00:58:27.300
Currently we define three properties which can
be found in the audio unit property set up file.

00:58:27.300 --> 00:58:33.480
So the first one is BypassVoiceProcessing, as
the name implies, it's just a Boolean switch

00:58:33.480 --> 00:58:36.869
which turns the voice processing on and off.

00:58:36.869 --> 00:58:46.309
You are also in control of enabling or disabling the
automatic game control, but this is on by default.

00:58:46.309 --> 00:58:52.150
And we also have to control the
ducking of the non-voice audio.

00:58:52.150 --> 00:58:56.280
And this is also on by default.

00:58:56.280 --> 00:59:02.730
So setting up the voice processing unit is
very similar to setting up a remote I/O unit,

00:59:02.730 --> 00:59:09.440
as Doug explained in the previous talk,
so I won't go into too much details here.

00:59:09.440 --> 00:59:17.099
But basically what you do is you set up an AudioSession,
and the important thing is that you have to set up a play

00:59:17.099 --> 00:59:23.480
and record category because this is exactly what you
want to do, you want to send sound over the speaker

00:59:23.480 --> 00:59:26.590
and capture directly over the microphone also.

00:59:26.590 --> 00:59:30.460
Set the preferred sampling rate,
and this will tell the system

00:59:30.460 --> 00:59:34.300
if the user is the single rated or the dual rated mode.

00:59:34.300 --> 00:59:42.539
And then you create a remote audio unit, but
instead of using the remote I/O subtypes,

00:59:42.539 --> 00:59:47.150
you use the newly introduced voice processing I/O subtype.

00:59:47.150 --> 00:59:52.550
And of course you are in charge
of implementing both the Input

00:59:52.550 --> 00:59:56.750
and the RenderProc callbacks to handle all the signal I/O.

00:59:56.750 --> 01:00:05.260
So that being said, I will like to go over to the
overhead projector and show you a little demo.

01:00:05.260 --> 01:00:14.400
So what we have in this UI here, you
see that the lower part, the red voice,

01:00:14.400 --> 01:00:23.139
the level meter which is labeled voice unit output, is
actually what's cutting out of the voice processing unit.

01:00:23.139 --> 01:00:30.789
But at this time I'm talking and there is no far end
speaker and there is no app sound playing at the same time,

01:00:30.789 --> 01:00:41.929
so you see that the red level meter is
moving along with the loudness of my voice.

01:00:41.929 --> 01:00:49.859
So now I'm going to play back, it's just
simulation, I'm going to play back a voice file

01:00:49.860 --> 01:00:52.880
which is a recording which I made with one of my co-workers.

01:00:52.880 --> 01:00:55.820
So I'm going to play back this file.

01:00:55.820 --> 01:00:56.600
>> Hello.

01:00:56.599 --> 01:00:57.619
>> So this is -

01:00:57.619 --> 01:00:58.829
>> How's it going?

01:00:58.829 --> 01:01:00.840
>> So this simulates the far end speaking.

01:01:00.840 --> 01:01:06.750
>> I think they're doing really well,
I'm just here trying to prepare for WWDC.

01:01:06.750 --> 01:01:17.059
>> So as you see, the blue level meter is moving along
his voice, but the red one is, if I stop talking -

01:01:17.059 --> 01:01:21.079
>> Hello I'm just here trying to prepare for WWDC.

01:01:21.079 --> 01:01:24.039
>> The red one is not moving.

01:01:24.039 --> 01:01:25.739
It moves again when I talk again.

01:01:25.739 --> 01:01:27.879
So if I set the bypass on here -

01:01:27.880 --> 01:01:34.059
>> I think they're going really well, I'm just here -

01:01:34.059 --> 01:01:40.619
>> So you see that the red level meter is moving
along his voice because this is what we are capturing

01:01:40.619 --> 01:01:43.289
through the microphone and we are processing.

01:01:43.289 --> 01:01:44.000
>> How's it going?

01:01:44.000 --> 01:01:48.619
>> And now what I'm going to do, I'm going
to add some audio effects on top of this.

01:01:48.619 --> 01:01:52.859
>> I'm just trying to prepare for WWDC.

01:01:52.860 --> 01:01:57.300
>> So I hope you hear the sounds coming out.

01:01:57.300 --> 01:02:07.039
So now I'm going to play both signals so
the sound effects output, the voice output.

01:02:07.039 --> 01:02:13.199
And now I'm going record actually what is
coming out of the voice processing unit.

01:02:13.199 --> 01:02:20.909
And so I'm talking, and so we have a mixture of
three different signals, one far end is our talker

01:02:20.909 --> 01:02:32.269
and one local app sound, and now I'm going to bypass the
voice unit and so we may be able to hear the difference.

01:02:32.269 --> 01:02:40.269
So now I'm going to stop the recording and
I'm going to play back the recorded file.

01:02:40.269 --> 01:02:49.170
>> And so I'm talking, and so we have a mixture of
three different signals, one far end is our talker

01:02:49.170 --> 01:02:53.180
and one local app sound, and now I'm
going to bypass the voice unit and -

01:02:53.179 --> 01:02:59.779
>> So you hear at this point I turned
the bypass on and we hear everything.

01:02:59.780 --> 01:03:00.450
>> -- to hear the difference.

01:03:00.449 --> 01:03:05.039
So now I'm going to stop the recording.

01:03:05.039 --> 01:03:09.000
>> So this is actually what I wanted to show you.

01:03:09.000 --> 01:03:11.530
[ Applause ]

01:03:11.530 --> 01:03:14.230
>> So I just want to summarize what
we talked about in the session.

01:03:14.230 --> 01:03:22.349
So first the AVAudioPlayer recorder, the simplest
ways of playing back or recording audio files

01:03:22.349 --> 01:03:26.789
in all those formats we are supporting on the device.

01:03:26.789 --> 01:03:37.279
We presented some best practices regarding the
usage of OpenAL, and in-depth review of AURemoteIOs

01:03:37.280 --> 01:03:43.990
for very low latency, for applications
requiring low latencies.

01:03:43.989 --> 01:03:52.429
And finally some voice processing audio unit
which gives you developers the ability to use

01:03:52.429 --> 01:03:56.789
out built-in echo canceler for your applications.

01:03:56.789 --> 01:04:01.869
If you need more information please contact
Allan Shaffer, our Graphics Technology Evangelist

01:04:01.869 --> 01:04:05.150
at the email address which is shown right now.