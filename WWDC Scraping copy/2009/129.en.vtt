WEBVTT

00:00:13.480 --> 00:00:15.000
>> Good morning.

00:00:15.000 --> 00:00:16.890
>> Good morning.

00:00:16.890 --> 00:00:21.380
>> So it's 9 AM on Friday morning, last night was the party.

00:00:21.379 --> 00:00:23.929
Who stayed up too late?

00:00:23.929 --> 00:00:27.000
Yeah, thank you so much for coming.

00:00:27.000 --> 00:00:29.089
My name is Kim Silverman.

00:00:29.089 --> 00:00:37.480
I'm manager of the Spoken Language Technologies
and principle research scientist here at Apple.

00:00:37.479 --> 00:00:40.009
So who are we?

00:00:40.009 --> 00:00:48.769
We are a team of research scientists and software engineers,
organizationally we are part of the Mac OS X division,

00:00:48.770 --> 00:00:54.410
but we work on technologies that are used
across a wide range of Apple products.

00:00:54.409 --> 00:01:02.179
Our technologies include Text-to-Speech synthesis,
Speech Recognition, Latent Semantic Analysis,

00:01:02.179 --> 00:01:06.060
which is used among other things for the junk mail filter.

00:01:06.060 --> 00:01:11.780
And integration of these technologies
into quite a number of Apple products.

00:01:11.780 --> 00:01:15.799
Today, we're going to focus on Text-to-Speech synthesis.

00:01:15.799 --> 00:01:22.140
This technology has been around for quite some time,
but it's become mainstream in the last year or so.

00:01:22.140 --> 00:01:31.189
The New York Times published for example that Text-to-Speech
is the new black, which was, we thought was kind of cool.

00:01:31.189 --> 00:01:33.879
So what Text-to-Speech is there in OS X?

00:01:33.879 --> 00:01:43.060
At this stage, I'd like to point out 55% of the people
attending this conference are at WWDC for the first time.

00:01:43.060 --> 00:01:46.390
A good number of the people who are here are new to the Mac.

00:01:46.390 --> 00:01:53.170
So for that subset of you who've been programming
with our APIs for years, some of the things I'm going

00:01:53.170 --> 00:01:55.900
to say might already be familiar with, to you.

00:01:55.900 --> 00:01:58.910
If so, please bear with us.

00:01:58.909 --> 00:02:06.119
We've been shipping OS X since the Mac first came out, sorry
shipping speech synthesis since the Mac first came out.

00:02:06.120 --> 00:02:11.280
There are a range of different voices available in OS X

00:02:11.280 --> 00:02:17.129
which contain different technologies representing
different tradeoffs of quality and footprint.

00:02:17.129 --> 00:02:22.969
Although people are not aware of this,
the quality of all of the voices improves

00:02:22.969 --> 00:02:25.219
with every release of the operating system.

00:02:25.219 --> 00:02:26.990
Rather, oh thank you.

00:02:26.990 --> 00:02:32.730
Rather than talk you through it, let me just
let some of the voices introduce themselves.

00:02:32.729 --> 00:02:35.929
We'll just do a quick sampling of some of them.

00:02:35.930 --> 00:02:38.340
>> I can read books out loud.

00:02:38.340 --> 00:02:42.349
>> We do not communicate with carbon-based life forms.

00:02:42.349 --> 00:02:45.739
>> Speech synthesis music.

00:02:45.740 --> 00:02:49.870
>> Give me the plan captain.

00:02:49.870 --> 00:02:51.640
>> You have mail from your boss.

00:02:51.639 --> 00:02:54.299
>> Relax, don't do it.

00:02:54.300 --> 00:02:56.420
>> Hi, I'm Alex.

00:02:56.419 --> 00:03:01.229
Some people recognize me by my voice.

00:03:01.229 --> 00:03:02.629
>> Thank you.

00:03:02.629 --> 00:03:05.219
[ Applause ]

00:03:05.219 --> 00:03:07.629
So Alex is our highest quality voice.

00:03:07.629 --> 00:03:10.139
He's got very natural articulation.

00:03:10.139 --> 00:03:16.479
The intonation, the way he says what he says conveys
the topic structure of the text that he's speaking.

00:03:16.479 --> 00:03:19.729
And as you may have noticed, Alex breathes.

00:03:19.729 --> 00:03:23.269
Those breath sounds are not just audio candy.

00:03:23.270 --> 00:03:28.300
They help you to understand the structure of
the text, they make the speech more intelligible

00:03:28.300 --> 00:03:31.160
and they make listening, the task
of listening more pleasant.

00:03:31.159 --> 00:03:35.609
So what can be done with our speech synthesis?

00:03:35.610 --> 00:03:39.390
Well, as you probably know, it's
used a lot in accessibility.

00:03:39.389 --> 00:03:44.169
For example, VoiceOver that we ship is a
screen reader that uses our speech synthesis

00:03:44.169 --> 00:03:47.569
to enable people who are blind to use the Mac.

00:03:47.569 --> 00:03:52.359
A lot of other applications that you folk are
writing also help out folk with disabilities.

00:03:52.360 --> 00:03:56.700
For example, people with dyslexia,
people who have trouble communicating.

00:03:56.699 --> 00:04:08.319
The point is that technologies that are developed for people
with disabilities often become mainstream because people

00:04:08.319 --> 00:04:12.250
without those disabilities also say
that's cool, I want some of that.

00:04:12.250 --> 00:04:16.990
For example, the smooth ramps that go down from
footpaths onto the road were designed originally

00:04:16.990 --> 00:04:19.590
for people with wheelchairs, but we all like them.

00:04:19.589 --> 00:04:25.060
So the things that you see being done for people with
disabilities using speech, you might want to think

00:04:25.060 --> 00:04:28.250
about can I do some of that in my application?

00:04:28.250 --> 00:04:35.509
From accessibility, we then generalized to using
speech in general to enhance the user interface.

00:04:35.509 --> 00:04:44.480
For example last year we shipped the iPod nano, which added
speech so that you could control it without looking at it.

00:04:44.480 --> 00:04:50.160
If you're in a situation where it's not convenient to see
the screen, like driving your car or you're out jogging,

00:04:50.160 --> 00:04:52.270
you can still know where you're navigating.

00:04:52.269 --> 00:04:57.609
You hear the song titles, you hear all the
menu items spoken by our speech synthesis.

00:04:57.610 --> 00:04:59.960
We won awards for that product.

00:04:59.959 --> 00:05:02.599
And so we then took that to the next level and rather

00:05:02.600 --> 00:05:07.860
than just enhancing the user interface,
speech became the sole user interface.

00:05:07.860 --> 00:05:12.259
In the iPod Shuffle, which we shipped
earlier this year, there is no screen.

00:05:12.259 --> 00:05:19.599
Speech synthesis is the only way the device can tell you
what status, tell you what you're navigating to and so on.

00:05:19.600 --> 00:05:22.020
Speech is used in other areas as well.

00:05:22.019 --> 00:05:28.949
It's used, for example, in the creative arts, in
the broadcast media there are lots of radio ads,

00:05:28.949 --> 00:05:32.779
TV ads, movies that use our speech synthesis.

00:05:32.779 --> 00:05:37.019
One case that we're particularly
proud of is the award-winning feature,

00:05:37.019 --> 00:05:45.529
animated feature Wall-E used our speech synthesis for
one of the major characters, Auto, the ship's computer.

00:05:45.529 --> 00:05:55.979
The tool that they used to add emotion to that voice is
the very same tool that you all have on your developer CDs.

00:05:55.980 --> 00:05:59.410
So you can, on your developer tools.

00:05:59.410 --> 00:06:02.939
So you can play with exactly the same
thing that was used and that you'll see

00:06:02.939 --> 00:06:06.370
in the making-of videos that accompany the Wall-E DVD.

00:06:06.370 --> 00:06:08.709
It's called Repeat After Me.

00:06:08.709 --> 00:06:14.739
And it lets you say something the way you want
it spoken, with all of the emotion and interest

00:06:14.740 --> 00:06:21.879
that you're being concerned with and then it
will copy the pitch, analyze and find the pitch,

00:06:21.879 --> 00:06:26.189
the phonemes and the durations in your
recording and impose them onto the synthesizer,

00:06:26.189 --> 00:06:33.469
so the synthesizer will say the phrase,
the sentence the way you said it.

00:06:33.470 --> 00:06:38.360
We've used speech in a number of
ways in the Mac OS X user interface.

00:06:38.360 --> 00:06:42.889
For example, you go to the system
preferences, find the speech preferences,

00:06:42.889 --> 00:06:47.050
from there you can go to the text-to-speech
panel where there are some check boxes

00:06:47.050 --> 00:06:50.530
that allow you to turn on a couple of simple features.

00:06:50.529 --> 00:06:58.229
For example, every time your application puts up an alert,
if the user doesn't respond to it after a certain amount

00:06:58.230 --> 00:07:03.170
of time, say, 30 seconds, then0 we will read it out.

00:07:03.170 --> 00:07:07.560
This is useful when a person is not attending to the screen.

00:07:07.560 --> 00:07:14.420
For example, I have received email from a user who said
I was crawling around on my hands and knees under my desk

00:07:14.420 --> 00:07:17.720
and rearranging my cables and suddenly this voice came

00:07:17.720 --> 00:07:21.710
out from the computer behind me saying,
the network has been disconnected.

00:07:21.709 --> 00:07:28.159
I turned around and sure enough, I had unwittingly
kicked the Ethernet cable out of my computer.

00:07:28.160 --> 00:07:33.730
It's useful if your application is in the background
and the user has something else on the screen.

00:07:33.730 --> 00:07:37.550
Safari or Xcode, you can still attract their attention.

00:07:37.550 --> 00:07:41.270
You put up a dialogue and even though
they don't see it, they'll still hear it

00:07:41.269 --> 00:07:44.909
and know whether they want to respond to it or not.

00:07:44.910 --> 00:07:48.250
And it's useful in K12 education.

00:07:48.250 --> 00:07:49.110
This is funny.

00:07:49.110 --> 00:07:51.430
Educators tell me this.

00:07:51.430 --> 00:07:56.230
When we read out the contents of your
dialogues, we precede them by a short phrase.

00:07:56.230 --> 00:07:57.930
You can edit that phrase.

00:07:57.930 --> 00:08:04.030
Kids in schools find out that you can do that
and they replace those phrases that we ship

00:08:04.029 --> 00:08:09.569
with some rather creative alternatives as you can imagine.

00:08:09.569 --> 00:08:13.560
The point is now they care about the spelling.

00:08:13.560 --> 00:08:16.550
Because if they don't spell the words
correctly, they aren't spoken correctly.

00:08:16.550 --> 00:08:19.660
Now they care about the punctuation.

00:08:19.660 --> 00:08:24.300
The teachers love this because it's
what we call guerilla literacy.

00:08:24.300 --> 00:08:24.810
[ Laughter ]

00:08:24.810 --> 00:08:32.679
And teachers tell me if you have an application
for education and they sit a kid down in a class

00:08:32.679 --> 00:08:38.069
with that application, the kid will
stay engaged for about 5 minutes.

00:08:38.070 --> 00:08:41.050
But if they turn on these interactive speech features,

00:08:41.049 --> 00:08:45.149
the kids stay engaged with your
application for 20 to 30 minutes.

00:08:45.149 --> 00:08:49.769
That's 20 minutes of extra time that the teacher
has to attend to other kids in the class.

00:08:49.769 --> 00:08:57.250
It's possible to select any text
and have it spoken with a hotkey.

00:08:57.250 --> 00:09:02.379
This is useful for example with your
viewing your news article on the web.

00:09:02.379 --> 00:09:07.279
You can select it and have the computer read
it out to you while you do some other task.

00:09:07.279 --> 00:09:13.899
That also saves eyestrain, so you don't have to squint
at the screen to try to read some of those tiny fonts.

00:09:13.899 --> 00:09:17.259
It's useful for proofing messages that you type.

00:09:17.259 --> 00:09:25.419
A guy stuck his head into my office soon after we shipped
that and said, Kim, your speech synthesis saved my butt.

00:09:25.419 --> 00:09:30.990
You see, when we type email messages, we can
see the typing errors that create non-words.

00:09:30.990 --> 00:09:35.789
But it's much harder to become
aware of our incorrect grammar,

00:09:35.789 --> 00:09:41.019
our poor word choice, our incorrectly assembled sentences.

00:09:41.019 --> 00:09:45.710
But when you hear it spoken, it's glaringly obvious.

00:09:45.710 --> 00:09:51.769
Some parents tell their kids to have the computer read
out their homework before they submit it because otherwise

00:09:51.769 --> 00:09:55.259
as you know, kids just don't care about
spelling and sentence length and so on.

00:09:55.259 --> 00:09:58.750
But when they hear it, they care.

00:09:58.750 --> 00:10:01.879
There are a lot of people out there in the world who want

00:10:01.879 --> 00:10:07.080
to improve their pronunciation of
English and of other languages.

00:10:07.080 --> 00:10:10.720
And so with this feature, if they see a
word and want to know how's that pronounced,

00:10:10.720 --> 00:10:13.860
they can just hit the hotkey and listen to it.

00:10:13.860 --> 00:10:18.940
Language education is most effective when
the students can bring their own material.

00:10:18.940 --> 00:10:22.610
[ Period of silence ]

00:10:22.610 --> 00:10:30.600
In iChat, it's now possible for you to configure it so
that when a person sends you a text message, you are,

00:10:30.600 --> 00:10:33.580
iChat will tell you with our speech synthesis.

00:10:33.580 --> 00:10:36.990
In fact it can be configured even to read out the message.

00:10:36.990 --> 00:10:44.610
So imagine the scenario is, you send a text message
to somebody, they don't respond, so you bring up Xcode

00:10:44.610 --> 00:10:49.019
and you start typing away, five or
ten minutes later, they respond.

00:10:49.019 --> 00:10:54.360
iChat's in the background, but you hear the content
of what they've typed and you know what they said

00:10:54.360 --> 00:10:56.509
and you don't have to do a context switch.

00:10:56.509 --> 00:11:04.559
Some people each morning turn their email into podcasts,
download them into iTunes using our speech synthesis,

00:11:04.559 --> 00:11:07.889
and have their email read out on
their iPod on the way to work so that

00:11:07.889 --> 00:11:12.129
by the time they get to work, they're already ahead.

00:11:12.129 --> 00:11:15.179
So what do we do for Snow Leopard?

00:11:15.179 --> 00:11:18.979
Well, one thing we did, worked on was the size of Alex.

00:11:18.980 --> 00:11:30.720
If you look at all the files in Leopard, the biggest
single file is the Alex voice file, 670 megabytes.

00:11:30.720 --> 00:11:39.840
So we asked ourselves, can we try to get the size down while
at the same time minimizing any negative impact on quality?

00:11:39.840 --> 00:11:47.670
Well, we got the size down to 382 megabytes, a little
over half, and not only did we maintain the quality,

00:11:47.669 --> 00:11:54.370
but the actually on average, the quality of the Alex
has increased in Snow Leopard relative to Leopard.

00:11:54.370 --> 00:11:56.139
How did we do that?

00:11:56.139 --> 00:11:57.000
Well, a number of things.

00:11:57.000 --> 00:11:59.820
We made the intonation much more natural.

00:11:59.820 --> 00:12:04.150
So the way Alex speaks more directly
conveys the meaning of the texts.

00:12:04.149 --> 00:12:07.090
His rhythm is more even as well.

00:12:07.090 --> 00:12:13.710
We significantly improved the pronunciations of
things like names, technical terms, ambiguous words.

00:12:13.710 --> 00:12:20.560
And we significantly reduced the dropouts, the
discontinuities, the missed articulations, the disfluencies,

00:12:20.559 --> 00:12:24.119
the hesitations, the glugs, the
pops, the clicks, the gurgles.

00:12:24.120 --> 00:12:25.049
This was a lot of work.

00:12:25.049 --> 00:12:26.509
It was not trivial.

00:12:26.509 --> 00:12:31.870
And we added a few good ideas for Snow
Leopard that we're talking about today.

00:12:31.870 --> 00:12:39.810
We've added more to the API so that you
can do more with your code with less code.

00:12:39.809 --> 00:12:47.159
OS X has been shipping for a couple of years,
several years now with rich audio processing.

00:12:47.159 --> 00:12:52.469
It's now possible to redirect the Apple to speech
synthesis straight into that audio processing.

00:12:52.470 --> 00:12:55.220
We'll demonstrate that for you.

00:12:55.220 --> 00:13:01.910
And because we have a plug-in architecture, other companies
who have speech synthesizers have ported their synthesis

00:13:01.909 --> 00:13:09.219
to the speech manager, so you call our API and lots
of extra voices in other languages become available.

00:13:09.220 --> 00:13:11.389
Let me introduce some of them to you.

00:13:11.389 --> 00:13:14.009
From Japan, we have Create System.

00:13:14.009 --> 00:13:19.750
[ Foreign language ]

00:13:19.750 --> 00:13:21.009
Thank you.

00:13:21.009 --> 00:13:27.350
[ Foreign language ]

00:13:27.350 --> 00:13:32.389
Some of you know of Cepstral who have some
mainstream voices and some novelty voices.

00:13:32.389 --> 00:13:34.460
[ Period of silence ]

00:13:34.460 --> 00:13:36.120
>> I'm Lawrence.

00:13:36.120 --> 00:13:37.500
>> My name is Millie.

00:13:37.500 --> 00:13:38.600
>> I'm David.

00:13:38.600 --> 00:13:40.720
>> I'm Kelly, the steady choice.

00:13:40.720 --> 00:13:45.009
>> And last, I'm Alison, welcome Mac Users.

00:13:45.009 --> 00:13:55.990
[ Foreign language ]

00:13:55.990 --> 00:13:58.889
>> I'm the Kevin voice.

00:13:58.889 --> 00:14:01.000
Where's the party headed later?

00:14:01.000 --> 00:14:02.110
>> I am Conrad.

00:14:02.110 --> 00:14:04.820
Unfortunately, there is no party later.

00:14:04.820 --> 00:14:05.950
Just work.

00:14:05.950 --> 00:14:08.860
>> I'm Tamika, an urban voice, word.

00:14:08.860 --> 00:14:10.440
>> And I'm Princess.

00:14:10.440 --> 00:14:13.470
OK, like what's urban actually mean?

00:14:13.470 --> 00:14:16.800
>> I'm a wise guy, I'll be seeing you in Jersey.

00:14:16.799 --> 00:14:20.029
>> And I'm Top Hat, an old school gangster type.

00:14:20.029 --> 00:14:24.879
>> If you live on the wild side, I'm Evil Genius.

00:14:24.879 --> 00:14:28.289
>> I'm French Fry, a voice with an amusing French accent.

00:14:28.289 --> 00:14:32.449
>> I'm Vlad, the voice that
talks with a Siberian accent.

00:14:32.450 --> 00:14:34.070
[ Applause ]

00:14:34.070 --> 00:14:43.450
>> From Europe, our friends in the Acapela
Group have brought over their voices.

00:14:43.450 --> 00:14:47.600
Here's a subset of them.

00:14:47.600 --> 00:14:51.149
>> I'm Peter and this is an advertisement for my voice.

00:14:51.149 --> 00:14:55.009
>> I'm Heather and this is an advertisement for my voice.

00:14:55.009 --> 00:15:39.809
[ Foreign language ]

00:15:39.809 --> 00:15:45.589
>> From Edinburg, CereProc brought
over a few other varieties of English.

00:15:45.590 --> 00:15:47.920
>> Hello, I'm Heather.

00:15:47.919 --> 00:15:49.709
I'm a Scottish voice.

00:15:49.710 --> 00:15:53.960
My favorite foods are tatties, mince, haggis and neeps.

00:15:53.960 --> 00:15:55.690
Now to introduce Sarah.

00:15:55.690 --> 00:15:57.520
She's a bonnie wee lassie.

00:15:57.519 --> 00:15:58.939
>> Thanks Heather.

00:15:58.940 --> 00:16:00.630
My name's Sarah.

00:16:00.629 --> 00:16:03.309
I'm a voice from London, England.

00:16:03.309 --> 00:16:05.459
Sometimes I have tea with the Queen.

00:16:05.460 --> 00:16:07.259
>> My name's Sue.

00:16:07.259 --> 00:16:11.409
I am a Black Country accent.

00:16:11.409 --> 00:16:14.379
[Foreign language] stop your [inaudible]
now and get on with it.

00:16:14.379 --> 00:16:18.049
>> Hi there, I'm sorry, I didn't mean to stare.

00:16:18.049 --> 00:16:19.709
My name's Katherine.

00:16:19.710 --> 00:16:21.019
I'm an American voice.

00:16:21.019 --> 00:16:22.199
Have a nice day.

00:16:22.200 --> 00:16:24.170
>> Hi there, Apple guys.

00:16:24.169 --> 00:16:28.360
It's a bit lonely being the ex-president
of the United States of America.

00:16:28.360 --> 00:16:32.350
However, at least I have more time to listen to my iPod.

00:16:32.350 --> 00:16:35.259
[ Laughter ]

00:16:35.259 --> 00:16:36.240
>> Thanks.

00:16:36.240 --> 00:16:40.500
[ Applause ]

00:16:40.500 --> 00:16:46.110
So you can call the speech synthesis APIs confident
that your app can ship in lots of countries

00:16:46.110 --> 00:16:48.610
and that your users can have a rich experience.

00:16:48.610 --> 00:16:51.680
So how do you get started?

00:16:51.679 --> 00:16:58.299
To show you more about the new API, let me ask our senior
software engineer, Kevin Aitkin to come up on the stage.

00:16:58.299 --> 00:16:58.599
Kevin.

00:16:58.600 --> 00:16:58.659
[ Applause ]

00:16:58.659 --> 00:16:58.779
>> Thanks.

00:16:58.779 --> 00:16:59.899
OK. Good morning.

00:16:59.899 --> 00:17:10.799
So hopefully we're going to get into some code here.

00:17:10.799 --> 00:17:12.829
So hopefully I'll be exciting.

00:17:12.829 --> 00:17:18.710
So as Kim said, I'm Kevin Aitkin, one of the
engineers in the Spoken Language Technology Group.

00:17:18.710 --> 00:17:22.440
And what I'm going to do is give you a
number of examples of the various ways

00:17:22.440 --> 00:17:24.600
that you can access the text-to-speech system

00:17:24.599 --> 00:17:31.289
in Mac OS X whether you're writing a simple
script or a full blown Cocoa application.

00:17:31.289 --> 00:17:38.409
So before we get into the code, let's, let me give
you a quick summary of our text-to-speech system.

00:17:38.410 --> 00:17:40.740
So as Kim mentioned and played for you, there are a number

00:17:40.740 --> 00:17:44.609
of third party synthesizers out
there that plug into our system.

00:17:44.609 --> 00:17:48.549
The great part of it is you really
only need to worry about a single API,

00:17:48.549 --> 00:17:51.549
regardless of which synthesizer you're talking to.

00:17:51.549 --> 00:17:53.690
So that greatly simplifies that.

00:17:53.690 --> 00:17:59.289
And the synthesizer in the language
is actually determined by the voice.

00:17:59.289 --> 00:18:07.119
Whether you set the voice specifically using the API or you
just use a default system voice that the user has selected.

00:18:07.119 --> 00:18:11.069
Now our API accepts just a simple string.

00:18:11.069 --> 00:18:16.049
But you can add a number of embedded commands
in there to control exactly how it's spoken.

00:18:16.049 --> 00:18:22.690
And we have a plethora of options for setting the
speaking rate, the volume, the pitch, creating audio files

00:18:22.690 --> 00:18:26.640
and sending it to other places then just out the speakers.

00:18:26.640 --> 00:18:29.740
And our system is naturally asynchronous.

00:18:29.740 --> 00:18:33.839
That means that your application can be
speaking while the user continues to use it

00:18:33.839 --> 00:18:40.019
and you could even have several voices
speaking at the same time if you choose.

00:18:40.019 --> 00:18:48.119
So what I'm going to do is I'm going to take you through the
example in five areas that are mainly different languages

00:18:48.119 --> 00:18:51.389
or environments that you can access text-to-speech system.

00:18:51.390 --> 00:18:53.770
So let's start with AppleScript.

00:18:53.769 --> 00:18:58.809
So as you might know, AppleScript is a great
language for combining the functionality of a number

00:18:58.809 --> 00:19:02.339
of applications together in a single workflow solution.

00:19:02.339 --> 00:19:05.990
It's been used for a number of years by the
publishing industry and others that work

00:19:05.990 --> 00:19:10.609
with a plethora of graphics files and others.

00:19:10.609 --> 00:19:17.309
The great thing is if an application can run an
AppleScript then it can speak using the say command.

00:19:17.309 --> 00:19:24.329
So for example in Mail you can have an AppleScript
run when you receive mail from a particular person.

00:19:24.329 --> 00:19:31.089
AppleScript say command also integrates well with another
command that we provide for listening to spoken commands.

00:19:31.089 --> 00:19:38.169
So you can create a simple question and answer scenario
that the user can use to communicate with the computer.

00:19:38.170 --> 00:19:41.730
As I said, there's a number of options that you can specify.

00:19:41.730 --> 00:19:46.210
You can specify a voice or you can say the audio to a file.

00:19:46.210 --> 00:19:52.000
And since the AppleScript say command is part of the
standard edition's OSAX, you can just go into script editor,

00:19:52.000 --> 00:19:54.559
look at its dictionary to see all of the options.

00:19:54.559 --> 00:19:58.839
So let's see a couple of examples
of the ApScript Say command.

00:19:58.839 --> 00:20:03.769
In the simplest case, you can just give a single string.

00:20:03.769 --> 00:20:09.150
Here, we might want to provide some
progress during a lengthy AppleScript.

00:20:09.150 --> 00:20:11.350
>> Now compressing image files.

00:20:11.349 --> 00:20:15.359
>> And it can also specify a specific voice to use.

00:20:15.359 --> 00:20:18.899
So maybe you want to be warned
when you get email from your boss.

00:20:18.900 --> 00:20:21.950
>> You have mail from your boss.

00:20:21.950 --> 00:20:27.220
>> And you can also specify a file path,
so that you can send the audio to a file

00:20:27.220 --> 00:20:29.509
that you can listen to later on your iPod.

00:20:29.509 --> 00:20:32.269
>> More drivel from yours truly.

00:20:32.269 --> 00:20:36.289
>> So that's a quick sample of the AppleScript say command.

00:20:36.289 --> 00:20:42.579
We also provide a say command line tool that
you can use in terminal and simple scripts.

00:20:42.579 --> 00:20:48.019
It's just a simple application to start speaking
and it waits until it's finished and then quits.

00:20:48.019 --> 00:20:51.089
Allows you on the command line to specify the voice.

00:20:51.089 --> 00:20:52.629
You can have it read from a file.

00:20:52.630 --> 00:21:01.030
It also has a number of flexible arguments for sending
the audio to something other than just the speakers.

00:21:01.029 --> 00:21:05.619
And in terminal for the documentation,
you can just easily type man say.

00:21:05.619 --> 00:21:07.500
So let's look at some examples of this.

00:21:07.500 --> 00:21:10.769
In a simple case, we'll just give it a string.

00:21:10.769 --> 00:21:14.190
>> It's 12:30, time for lunch.

00:21:14.190 --> 00:21:18.380
>> Or it can use the -v option
to specify a particular voice.

00:21:18.380 --> 00:21:22.710
>> Your status report is due.

00:21:22.710 --> 00:21:27.130
>> Or we can read the text from a file using the -f option

00:21:27.130 --> 00:21:31.820
and then send the audio to an audio
file using the -o option.

00:21:31.819 --> 00:21:36.500
>> It was the best of times, it was the worst of times.

00:21:36.500 --> 00:21:41.089
>> Now something we do in the group every once in a
while when we start off a lengthy command such as a GREP

00:21:41.089 --> 00:21:45.679
or a BUILD or whatever, we'll just add the say
command after it so that if we're off working

00:21:45.680 --> 00:21:47.880
in another application, we'll know when it's finished.

00:21:47.880 --> 00:21:49.470
And we hear something like this.

00:21:49.470 --> 00:21:51.339
>> BUILD complete.

00:21:52.529 --> 00:21:58.109
>> So, I've given you an example of the
AppleScript say command and say command line tool.

00:21:58.109 --> 00:22:02.339
These are great, they're flexible tools
that gets you speaking really fast.

00:22:02.339 --> 00:22:03.889
But they have their limitations.

00:22:03.890 --> 00:22:11.050
So if you want the full access to the synthesizers,
then we encourage you to go use the Cocoa

00:22:11.049 --> 00:22:14.769
or the Core Foundation APIs that we provide.

00:22:14.769 --> 00:22:20.119
They allow you to directly set and get all
the synthesizer properties that are provided.

00:22:20.119 --> 00:22:27.089
You can receive a number of notifications during
speaking, such as when speaking is finished

00:22:27.089 --> 00:22:31.199
or when a word or a phoneme is about to be spoken.

00:22:31.200 --> 00:22:36.200
You also have access to a list of voices that are
installed in your system along with their properties,

00:22:36.200 --> 00:22:39.950
such as the name, age, gender of that voice.

00:22:39.950 --> 00:22:47.700
And we also have some specialty routines for converting
text into the phonemes or for adding a custom dictionary.

00:22:47.700 --> 00:22:52.259
So you can fine-tune how a particular
word is spoken, like your company name.

00:22:52.259 --> 00:22:57.779
So let me talk about the Cocoa class NSSpeechSynthesizer.

00:22:58.980 --> 00:23:05.140
So all you need to do is link with the
AppKit.framework, or the Cocoa.framework.

00:23:05.140 --> 00:23:09.960
You should use a RunLoop so you'll
get any kind of messages back.

00:23:09.960 --> 00:23:13.120
So let's see a quick example of this.

00:23:13.119 --> 00:23:18.689
So we'll start off by creating an
instance of our NSSpeechSynthesizer class.

00:23:18.690 --> 00:23:22.000
We just use a default initializer
here so that means we're going

00:23:22.000 --> 00:23:25.980
to use a voice the user has selected
in his system preferences.

00:23:25.980 --> 00:23:28.700
We're going to set the delegate object
and that's the object that's going

00:23:28.700 --> 00:23:31.880
to receive a notification when speaking is finished.

00:23:31.880 --> 00:23:36.370
And then we're going to begin speaking
by calling startSpeakingString.

00:23:36.369 --> 00:23:38.049
So it sounds a little bit like this.

00:23:38.049 --> 00:23:40.569
>> Don't object to my objects.

00:23:40.569 --> 00:23:45.679
>> So, as I said, we want to be
notified when the speaking has finished.

00:23:45.680 --> 00:23:53.400
And so how we do that is we implement the
didFinishSpeaking method in our delegate object.

00:23:53.400 --> 00:23:58.790
So this is handy for cases where
you want to update your interface.

00:23:58.789 --> 00:24:04.269
So for example in system preferences, when
you click the play button it turns to stop

00:24:04.269 --> 00:24:07.910
and then when speaking is finished,
I turn that back to play.

00:24:07.910 --> 00:24:12.680
And so that's where I do that in a routine like this.

00:24:12.680 --> 00:24:18.720
Now as I mentioned you'll have access to all available
voices in the system in the NSSpeechSynthesizer class.

00:24:18.720 --> 00:24:24.430
And so what's really nice, what you may want to
do in your application is provide a popup menu.

00:24:24.430 --> 00:24:32.450
In the case of Apple's chess program, it enables
you to speak the plays as they're performed.

00:24:32.450 --> 00:24:38.610
And so what it does in this preferences dialogue is
it allows you to select a voice for both players.

00:24:38.609 --> 00:24:40.599
So let me show you how you populate this.

00:24:40.599 --> 00:24:42.939
It's really easy.

00:24:42.940 --> 00:24:49.190
You'll call availableVoices, which will give you
a list of the voices installed in the system.

00:24:49.190 --> 00:24:54.750
And then using fast numeration, we're just going to
ask to for the attributes of each one of those voices.

00:24:54.750 --> 00:24:58.890
We'll get a simple NS Dictionary back.

00:24:58.890 --> 00:25:04.250
And then from that dictionary, we're going to
ask for the name value for each one of those.

00:25:04.250 --> 00:25:07.630
And for that name string, we're just
going to add it to the popup menu.

00:25:07.630 --> 00:25:12.400
So it's really quite simple.

00:25:12.400 --> 00:25:20.310
So if you're writing a C or a C++ application, then
you'll probably want to use our Core Foundation-based API.

00:25:20.309 --> 00:25:22.889
You'll need a link with ApplicationServices.framework

00:25:22.890 --> 00:25:27.360
and although a running loop is a good
idea, it's not required in every situation.

00:25:27.359 --> 00:25:31.099
So let's see a simple example of this.

00:25:31.099 --> 00:25:34.509
We'll start by calling NewSpeechChannel.

00:25:34.509 --> 00:25:37.950
That'll give us an instance of the speech channel.

00:25:37.950 --> 00:25:41.480
That's just kind of like an open
connection with the synthesizer

00:25:41.480 --> 00:25:45.029
that you'll normally dispose of
later when you're finished of it.

00:25:45.029 --> 00:25:52.139
We're passing in NULL as a voice so that we can use the
voice the user has chosen in the system preferences.

00:25:52.140 --> 00:25:59.590
And imagine we're going to write an application for kids
in the classroom to learn speech, learn language better.

00:25:59.589 --> 00:26:03.000
So we're going to highlight the words
as they're spoken on the screen.

00:26:03.000 --> 00:26:08.650
So to do that, we're going to define a word
callback routine called HighlightSpokenWord.

00:26:08.650 --> 00:26:15.690
And before we pass that in, we're just going to take
that reference and convert it into a CFNumber object.

00:26:15.690 --> 00:26:20.830
And then we're going to set our word
callback by calling SetSpeechProperty.

00:26:20.829 --> 00:26:24.750
And then finally we're going to begin
speaking by calling SpeakCFString.

00:26:24.750 --> 00:26:27.359
So it sounds a little bit like this.

00:26:27.359 --> 00:26:30.289
>> See Spot code.

00:26:30.289 --> 00:26:37.500
>> So let's look at the implementation for the word
callback when just before each word is about to be spoken.

00:26:37.500 --> 00:26:39.769
It passes us four parameters.

00:26:39.769 --> 00:26:43.119
The first is the speech channel that we set up earlier.

00:26:43.119 --> 00:26:45.509
The next is an optional refcon.

00:26:45.509 --> 00:26:47.589
That's kind of like a context or user data.

00:26:47.589 --> 00:26:49.299
You can use it for whatever you want.

00:26:49.299 --> 00:26:50.859
But it's optional.

00:26:50.859 --> 00:26:57.419
Then the string that we originally started speaking
and then finally a CFRange which describes the location

00:26:57.420 --> 00:27:01.720
and the length of the word within
that string that we're about to speak.

00:27:01.720 --> 00:27:06.000
So that's a glimpse of using the word callback.

00:27:06.000 --> 00:27:12.950
So let's talk about one other example
of accessing the text-to-speech system.

00:27:12.950 --> 00:27:21.130
And that's from languages that use Mac OS X's
bridge support, such as Ruby, Python and Lua.

00:27:21.130 --> 00:27:26.490
And so what they allow you to do is actually
access Cocoa's NSSpeechSynthesizer class

00:27:26.490 --> 00:27:28.549
from a language other than Objective-C.

00:27:28.549 --> 00:27:31.730
So let's look at a quick Ruby example.

00:27:31.730 --> 00:27:39.460
So we initialize or instantiate our
NSSpeechSynthesizer object for the default voice.

00:27:39.460 --> 00:27:41.470
Then we start speaking.

00:27:41.470 --> 00:27:44.259
>> Calling Cocoa from Ruby is easy.

00:27:44.259 --> 00:27:49.079
>> Now instead of using a notification to tell us when
speaking is finished, what we're going to do is we're going

00:27:49.079 --> 00:27:52.500
to call the synthesizer's isSpeaking method in a loop.

00:27:52.500 --> 00:27:58.559
And then we're going to call NSRunLoop to
wait for a second each time we call that.

00:27:58.559 --> 00:28:03.349
And so that's one way of determining
when speaking is finished.

00:28:03.349 --> 00:28:07.609
So as Kim mentioned, we made a number
of refinements in Snow Leopard.

00:28:07.609 --> 00:28:09.809
And we did add a few new APIs.

00:28:09.809 --> 00:28:13.069
So let me talk about one specifically.

00:28:13.069 --> 00:28:17.579
We're calling it On-the-Fly Voice/Synthesizer Loading.

00:28:17.579 --> 00:28:25.549
And so it's a set of APIs that allows you to access voices
and synthesizers outside of the standard directories.

00:28:25.549 --> 00:28:32.159
So what this allows you to do is bundle a
third party synthesizer inside your application

00:28:32.160 --> 00:28:34.310
and then ship that separately.

00:28:34.309 --> 00:28:39.329
So this greatly simplifies installation
so that your user doesn't have to install

00:28:39.329 --> 00:28:43.859
that third party synthesizer before
they can begin using your application.

00:28:43.859 --> 00:28:47.789
It also really simplifies licensing issues
because oftentimes you're going to license

00:28:47.789 --> 00:28:52.349
that third party synthesizer just for your application.

00:28:52.349 --> 00:28:58.579
And before this, it was kind of confusing because the
user would see those voices listed in system preferences

00:28:58.579 --> 00:29:02.029
and be confused why they couldn't
use them in other applications.

00:29:02.029 --> 00:29:03.430
So how do you do that?

00:29:03.430 --> 00:29:04.350
Well it's quite simple.

00:29:04.349 --> 00:29:11.509
You'll just start with your application, then you'll go
license one of those third party synthesizers and a voice.

00:29:11.509 --> 00:29:17.170
And then you're going to embed that synthesizer
and voice inside your application bundle.

00:29:17.170 --> 00:29:21.320
It's easy to do this with Xcode and set it up.

00:29:21.319 --> 00:29:29.689
And then you'll just need to add a couple of lines
of code to register the location of that synthesizer

00:29:29.690 --> 00:29:34.830
and voice so that they'll show up in
the listed voices for your application.

00:29:34.829 --> 00:29:41.759
So the third party synthesizer developers will
have to make some minor changes to support this,

00:29:41.759 --> 00:29:44.940
so they should be rolling this out in the next year.

00:29:44.940 --> 00:29:49.070
We've been working with the Acapela Group,
which you heard some of those examples.

00:29:49.069 --> 00:29:55.389
And they've given us a test French
synthesizer that we would like to demo today.

00:29:55.390 --> 00:29:57.750
So let me just give you a quick demo of this.

00:29:57.750 --> 00:30:00.480
I'm going to come here to demo 2.

00:30:00.480 --> 00:30:02.309
OK, great.

00:30:03.880 --> 00:30:14.460
So before I begin, let me just show that on
this system, we have just a stock Apple voices.

00:30:14.460 --> 00:30:17.410
So there they are, nothing else.

00:30:17.410 --> 00:30:20.640
So what I've done is I've created an application.

00:30:20.640 --> 00:30:24.840
Actually I used, you'll see in a
minute, one of our example applications.

00:30:24.839 --> 00:30:30.629
I've gone through the steps that I
described and embedded their synthesizer.

00:30:30.630 --> 00:30:40.230
And so I stuck it on the flash drive
here, so let me stick it in the machine.

00:30:40.230 --> 00:30:40.289
[ Period of silence ]

00:30:40.289 --> 00:30:47.029
It should show up here any moment, there it is.

00:30:47.029 --> 00:30:47.259
[ Period of silence ]

00:30:47.259 --> 00:30:48.629
And so here, it is.

00:30:48.630 --> 00:30:53.130
So let's go choose the voice and
so we should have the Bruno voice.

00:30:53.130 --> 00:30:53.740
[ Period of silence ]

00:30:53.740 --> 00:31:00.380
And we'll play some French text here that we have.

00:31:00.380 --> 00:31:00.440
[ Foreign language ]

00:31:00.440 --> 00:31:07.670
So that's an example of how we took one of our examples.

00:31:07.670 --> 00:31:13.300
We just embedded their synthesizer, added
those two lines of code that you showed

00:31:13.299 --> 00:31:18.549
and created something we can easily distribute on to users.

00:31:18.549 --> 00:31:23.589
You also saw an example here where we were
highlighting the words using the word callback.

00:31:23.589 --> 00:31:32.099
And also, the little guy here animates
this mouth using the phoneme callback.

00:31:32.099 --> 00:31:41.529
So I'll go back to the slides here and so I'll just
finish up by saying that we've showed you several examples

00:31:41.529 --> 00:31:46.470
of numerous ways you can actually access
the text-to-speech system in Mac OS X.

00:31:46.470 --> 00:31:51.470
We have a number of examples that we ship with Snow Leopard.

00:31:51.470 --> 00:31:57.170
You can access those at /developers/examples/speech.

00:31:57.170 --> 00:32:02.720
And of course, all of our documentation
is online at developer.apple.com.

00:32:02.720 --> 00:32:10.430
Now I showed you a couple of examples of where I was sending
speech out to an audio file, but there's a number of ways

00:32:10.430 --> 00:32:15.910
that you can take the audio from a synthesizer
and massage it and do fun things with it.

00:32:15.910 --> 00:32:21.700
So with that, I'd like to bring up Matthias
Neerarcher, senior software engineer in our group.

00:32:21.700 --> 00:32:24.950
And he's going to show you some really
cool things to do with the audio output.

00:32:24.950 --> 00:32:25.750
[ Applause ]

00:32:25.750 --> 00:32:29.569
>> Thank you Kevin.

00:32:29.569 --> 00:32:30.609
Good morning everybody.

00:32:30.609 --> 00:32:34.169
So far, we've mostly seen speech
that goes through different,

00:32:34.170 --> 00:32:37.180
through the default speakers attached to the system.

00:32:37.180 --> 00:32:43.360
And that's of course what in many
situations is what you want to do.

00:32:43.359 --> 00:32:46.479
But there are also a number of other output methods.

00:32:46.480 --> 00:32:48.779
And I'm going to go through these now.

00:32:48.779 --> 00:32:54.230
So as we said, by default you just go
through the default sound output device.

00:32:54.230 --> 00:33:00.380
And in the Core Foundation based API, that's
just once you have established your channel,

00:33:00.380 --> 00:33:04.610
you just call SpeakCFString for instance.

00:33:04.609 --> 00:33:09.829
On the command line you would just use say and
then give it the text you want to be spoken.

00:33:09.829 --> 00:33:13.250
It's going to be sent to your default sound output device.

00:33:13.250 --> 00:33:21.609
But we've also seen is sending the speech to
another AIFF file instead of speaking it directly.

00:33:21.609 --> 00:33:28.750
And you will actually find that this is a lot
quicker than, it doesn't take us a minute to,

00:33:28.750 --> 00:33:32.940
to send a minute of text, spoken text to an AIFF file.

00:33:32.940 --> 00:33:43.650
To do that, all you need to do is add one extra line
before to the CF API, before speaking on the channel,

00:33:43.650 --> 00:33:50.900
you set the properties speech output to file
URL property on a channel to a URL pointing

00:33:50.900 --> 00:33:54.700
at the file you want the output to be sent to.

00:33:54.700 --> 00:33:58.980
And then you can speak on the channel
just like you did before.

00:33:58.980 --> 00:34:09.619
On the command line, you would give the -o for output
option and then a file name that you would want to speak to.

00:34:09.619 --> 00:34:13.269
So those were other two traditional
methods for speech output.

00:34:13.269 --> 00:34:16.269
And over the years we've added some more.

00:34:16.269 --> 00:34:21.650
One of the most sophisticated one is
post processing of the speech output.

00:34:21.650 --> 00:34:26.900
Let's say you want to write the
next great railroad simulator.

00:34:26.900 --> 00:34:33.490
So you want to have a station announcement that sounds
really, like it's bouncing off the walls everywhere.

00:34:33.489 --> 00:34:35.500
You want it to sound something like this.

00:34:35.500 --> 00:34:35.559
[ Inaudible ]

00:34:35.559 --> 00:34:48.480
You do this by sending your speech output to,
here we've used a matrix reverb audio unit

00:34:48.480 --> 00:34:52.420
and then going to the default output unit.

00:34:52.420 --> 00:35:00.750
Now, to set up an audio unit, you generally
want more than just a speech output.

00:35:00.750 --> 00:35:03.409
Or more than just a speech audio unit.

00:35:03.409 --> 00:35:07.980
You also want to have an audio unit
that processes the speech and generally,

00:35:07.980 --> 00:35:13.289
you want some output audio unit or
unit to pick up the sound yourself.

00:35:13.289 --> 00:35:17.800
Here I'm only going to show you how to
set up the speech audio unit itself.

00:35:17.800 --> 00:35:22.460
You start by defining an audio component description,

00:35:22.460 --> 00:35:29.869
stating that you want the speech synthesis
generator audio unit made by Apple.

00:35:29.869 --> 00:35:34.299
You then instantiate the component
you find with that description,

00:35:34.300 --> 00:35:38.180
which gives you the speech audio unit and speech unit.

00:35:38.179 --> 00:35:44.219
And now the big difference between this output method
and any other method that I'm going to show you is

00:35:44.219 --> 00:35:47.480
that here you don't create the speech channel yourself.

00:35:47.480 --> 00:35:53.449
The audio unit has created that speech channel and
it's what you do instead of calling new speech channel,

00:35:53.449 --> 00:35:59.849
is you retrieve the speech channel from the
audio unit with the AudioUnitGetProperty call.

00:35:59.849 --> 00:36:08.480
And the property AudioUnitProperty_SpeechChannel, you will
get that speech channel that has already been created.

00:36:08.480 --> 00:36:15.740
And once you have that channel and once you have
hooked up your speech audio unit to other audio units,

00:36:15.739 --> 00:36:20.969
you use it just like you would use any other speech channel.

00:36:20.969 --> 00:36:26.649
Now obviously for prototyping, we also would like
to make this available from the command line.

00:36:26.650 --> 00:36:33.639
But how do you specify a graph of audio, arbitrary
complex graphs of audio units on the command line?

00:36:33.639 --> 00:36:39.869
What we do is something simple yet suitable for prototyping.

00:36:39.869 --> 00:36:47.429
We allow you to set the speech output to an AUNetSend
audio unit, which is an audio unit that feeds

00:36:47.429 --> 00:36:51.349
into a pipeline, feeds your audio into a pipeline.

00:36:51.349 --> 00:36:56.960
And then you can hook up another application
on the receiving end of that pipeline

00:36:56.960 --> 00:37:00.349
and define a more complex audio graph then.

00:37:00.349 --> 00:37:07.920
Now, this doesn't have a way of synchronizing the
audio so you want to test this with a lengthy file.

00:37:07.920 --> 00:37:14.970
And I'm going to demo a little
bit what you can do with these.

00:37:14.969 --> 00:37:22.189
[ Period of silence ]

00:37:22.190 --> 00:37:35.289
So here we have already a command line available for
use say -n for send to network, we can give any name,

00:37:35.289 --> 00:37:40.050
a colon is kind of the simplest name
for a, for a locally named pipeline.

00:37:40.050 --> 00:37:48.570
And then you say we want an input file and I have
something lengthy enough available right now.

00:37:48.570 --> 00:37:50.380
So we've hooked up the sending end.

00:37:50.380 --> 00:37:56.970
We don't hear anything because right now that
pipeline is just going into the big void.

00:37:56.969 --> 00:37:58.769
We want to hook up the receiving end.

00:37:58.769 --> 00:38:03.929
And Apple has a convenient application
available for this, AU Lab which you will find

00:38:03.929 --> 00:38:07.769
in the developer/applications/audio/AULab.

00:38:07.769 --> 00:38:14.099
And it's an incredibly versatile
application for testing audio units.

00:38:14.099 --> 00:38:16.900
Let's first create a document by hand.

00:38:16.900 --> 00:38:20.400
The output set up like this is good.

00:38:20.400 --> 00:38:24.710
We don't really need an input set up.

00:38:24.710 --> 00:38:28.970
Audio device, you generally want
built in output in this room here.

00:38:28.969 --> 00:38:31.179
We need line out.

00:38:31.179 --> 00:38:42.029
And you have your output set up and now you want to hook
up the speech that's coming in by adding a generator

00:38:42.030 --> 00:38:49.400
which in this case is AUNetReceive which is the receiving
end of the pipeline where the audio has been going in

00:38:49.400 --> 00:38:52.920
and luckily not really piling up over time.

00:38:52.920 --> 00:38:58.389
And now guess what happens if I hit connect.

00:38:58.389 --> 00:38:59.799
>> Man in their degree.

00:38:59.800 --> 00:39:04.800
Sometime or other, Jerry very nearly had the
same feelings towards the ocean with Nate.

00:39:04.800 --> 00:39:07.100
>> Now, you're hearing unadulterated audio.

00:39:07.099 --> 00:39:09.969
>> Of the man handles fell to ground by war of.

00:39:09.969 --> 00:39:15.899
[ Inaudible ]

00:39:15.900 --> 00:39:20.920
>> Now if we pause this process by adding an audio affect.

00:39:20.920 --> 00:39:24.280
Let's start with a matrix reverb.

00:39:24.280 --> 00:39:29.960
You can try all these sliders yourself
or you can try some of the presets.

00:39:29.960 --> 00:39:38.300
Let's say you wanted, here we are in a
large hall, so let's add some extra reverb.

00:39:38.300 --> 00:39:40.430
[ Background noise ]

00:39:40.429 --> 00:39:43.899
>> Then northward.

00:39:43.900 --> 00:39:45.960
What do you see?

00:39:45.960 --> 00:39:59.490
[ Inaudible ]

00:39:59.489 --> 00:40:07.829
>> So you can see that with a reverb, you already have
quite a few options and if you go for the big reverb,

00:40:07.829 --> 00:40:13.779
obviously you're going to get rather bombastic effects.

00:40:13.780 --> 00:40:21.210
But sometimes it's sufficient to add just a tiny bit
of reverb to achieve a somewhat different effect.

00:40:21.210 --> 00:40:31.449
Oh, let's say you want to, to alter
your voice, make it somewhat deeper.

00:40:31.449 --> 00:40:39.119
[ Period of silence ]

00:40:39.119 --> 00:40:53.559
Let's first compare again how Alex sounds by default.

00:40:53.559 --> 00:40:53.929
[ Period of silence ]

00:40:53.929 --> 00:40:56.710
>> East, south and west.

00:40:56.710 --> 00:40:58.429
Yet here they all.

00:40:58.429 --> 00:41:01.960
>> So let's make Alex a little bit deeper.

00:41:01.960 --> 00:41:06.000
>> The compasses of all those ships attract them thither?

00:41:06.000 --> 00:41:11.610
Once more say you are in the country
in some high land of lakes.

00:41:11.610 --> 00:41:15.070
Take almost any path you please and.

00:41:15.070 --> 00:41:19.660
>> So here, we added a pitch change
audio unit in the post processing.

00:41:19.659 --> 00:41:27.629
And finally if you have let's say you want to write the next
big violent computer game and you want to have some kind

00:41:27.630 --> 00:41:34.390
of a combat oriented radio transmissions
with all sorts of distortion going in.

00:41:34.389 --> 00:41:36.679
You might want to go for something like this.

00:41:36.679 --> 00:41:55.259
>> American desert, try this experiment if your caravan
happens to be supplied with a metaphysical professor.

00:41:55.260 --> 00:42:00.670
[beep] Yes, as everyone knows, [beep]
meditation and water are wedded forever.

00:42:00.670 --> 00:42:01.409
[beep] But--

00:42:01.409 --> 00:42:06.859
>> So here we are, [beep] so here we
add a beep in the pauses of speech.

00:42:06.860 --> 00:42:10.940
We add a band pass filter to, to limit
the range of audio that's transmitted

00:42:10.940 --> 00:42:14.019
and we throw in some extra distortion as a bonus.

00:42:14.019 --> 00:42:26.989
So you can see you can play with AU Lab and
get a rather wide range of audio effects

00:42:26.989 --> 00:42:29.469
that you can all play in real time with your speech output.

00:42:29.469 --> 00:42:29.529
[ Applause ]

00:42:29.530 --> 00:42:29.690
Thank you.

00:42:29.690 --> 00:42:31.920
Now in the Snow Leopard we have,
we've added two more output methods.

00:42:31.920 --> 00:42:39.610
One of them is we noticed that of those people
who use the speak to AIFF file output path,

00:42:39.610 --> 00:42:45.059
many of them then immediately turned around
and put that AIFF file into another encoder

00:42:45.059 --> 00:42:49.699
because if you download to your
iPod you don't really want AIFF.

00:42:49.699 --> 00:42:55.739
You would have your audio books taking up more
space than, than the rest of your music library.

00:42:55.739 --> 00:42:58.069
You want to encode it somehow.

00:42:58.070 --> 00:43:03.150
So in Snow Leopard we are offering you a
way of directly encoding your speech output

00:43:03.150 --> 00:43:05.730
without going through an intermediate step.

00:43:05.730 --> 00:43:13.050
And we do that by allowing you to pass in
a reference to an open Extended Audio File.

00:43:13.050 --> 00:43:18.310
Extended Audio File is an API in
the audio toolbox framework.

00:43:18.309 --> 00:43:23.949
So what you do here is you use the Audio
Toolbox framework to open an extended audio file

00:43:23.949 --> 00:43:28.199
with a URL you're passing, a file
format which is container format.

00:43:28.199 --> 00:43:30.429
Let's say you want a Core Audio file.

00:43:30.429 --> 00:43:33.710
You want an MP4 container.

00:43:33.710 --> 00:43:38.289
You're passing a data format, let's say
you want Apple Lossless, so you want AAC.

00:43:38.289 --> 00:43:42.659
You can specify any codec that is installed in the system.

00:43:42.659 --> 00:43:45.469
And that gives you an audio file.

00:43:45.469 --> 00:43:50.750
You pass that audio file to the speech
property, speech output to Extended Audio file,

00:43:50.750 --> 00:43:56.730
and then when you use the channel, the
output gets encoded to that audio file.

00:43:56.730 --> 00:43:58.500
We also allow this on the command line.

00:43:58.500 --> 00:44:04.079
We allow you to access pretty much
any codec installed on the system.

00:44:04.079 --> 00:44:09.980
Again, with a number of options if you want
to get really into very specific PCM encoding

00:44:09.980 --> 00:44:16.730
like 16 bit little engine floating point
or whatever then you might have to play

00:44:16.730 --> 00:44:19.579
with the options a bit and look at the man page.

00:44:19.579 --> 00:44:27.049
Here we want to store Apple Lossless into a core audio
file which we achieve by giving just the right extension

00:44:27.050 --> 00:44:31.680
to the output option and then specifying
the data format stored in it

00:44:31.679 --> 00:44:36.699
because there are many encodings
you can store in a Core Audio file.

00:44:36.699 --> 00:44:45.669
And one final point here is it doesn't need to be a file.

00:44:45.670 --> 00:44:54.150
If you look at the audio file API in Audio Toolbox, you can
see that you can also open an audio file simply by passing

00:44:54.150 --> 00:45:01.090
in a whole bunch of callbacks for what is supposed to
be happening, happening if you write your file and such.

00:45:01.090 --> 00:45:06.980
You can then wrap that audio file in an extended audio
file and pass that to us which would allow you for instance

00:45:06.980 --> 00:45:14.829
to render speech to a memory buffer and then
process it in memory if that's what you desire.

00:45:14.829 --> 00:45:23.079
And the final output method is we've generalized the default
output method a little bit that you can now also speak

00:45:23.079 --> 00:45:26.029
to the audio devices other than the default.

00:45:26.030 --> 00:45:31.970
If you for instance have a mixing desk clocked into
your computer where you would like to send your audio

00:45:31.969 --> 00:45:37.250
for post processing or if you want
to go some speech to headphones or--

00:45:37.250 --> 00:45:47.260
and some speech to a speaker, you simply find out the audio
device ID of the speakers you want to go to and then pass

00:45:47.260 --> 00:45:51.880
that audio device ID to the OutputToAudioDeviceProperty.

00:45:51.880 --> 00:45:56.840
And again, you then speak.

00:45:56.840 --> 00:46:03.900
So now I've shown you all these methods, you might want
to ask yourself, we here at Apple usually have the luxury

00:46:03.900 --> 00:46:07.340
of just writing our codes to run on Snow Leopard and later.

00:46:07.340 --> 00:46:09.950
You generally want to be backward compatible somewhat.

00:46:09.949 --> 00:46:14.460
So it's important for you to know
when these methods are available.

00:46:14.460 --> 00:46:19.710
Obviously speaking to the default audio device
has been available all the way back to 10.0.

00:46:19.710 --> 00:46:23.460
Does anybody here still supporting 10.0 in their apps?

00:46:23.460 --> 00:46:24.679
Raise your hands.

00:46:24.679 --> 00:46:29.869
Speaking to an AIFF file has been
available all the way back to 10.2.

00:46:29.869 --> 00:46:34.679
Now speaking to audio units is
available in Tiger or later, 10.4.

00:46:34.679 --> 00:46:41.789
Speaking to an extended audio file and speaking to
an audio device, those two are available in 10.6.

00:46:41.789 --> 00:46:45.509
When it comes to third party synthesizers,
the picture is a little bit more complex.

00:46:45.510 --> 00:46:49.420
Obviously they all support speaking
to the default audio device.

00:46:49.420 --> 00:46:54.200
And I'm pretty sure they all support
speaking to an AIFF file.

00:46:54.199 --> 00:47:00.509
However, the other methods shown here might not
be available yet with third party synthesizers.

00:47:00.510 --> 00:47:05.820
And I hope they will be more widely adopted in the future.

00:47:05.820 --> 00:47:11.630
With that, I would like to bring back
Kim to wrap up this presentation.

00:47:11.630 --> 00:47:17.559
[ Applause ]

00:47:17.559 --> 00:47:20.059
>> So, I don't know about you, but
I think that stuff is pretty cool.

00:47:20.059 --> 00:47:21.739
What do you think?

00:47:21.739 --> 00:47:21.909
[ Applause ]

00:47:21.909 --> 00:47:22.739
Yeah.

00:47:22.739 --> 00:47:25.909
[ Applause ]

00:47:25.909 --> 00:47:28.399
So you've got three steps.

00:47:28.400 --> 00:47:31.610
Step one, there is no step one.

00:47:31.610 --> 00:47:34.570
Your application already speaks.

00:47:34.570 --> 00:47:37.580
I showed you all those items that are in the user interface.

00:47:37.579 --> 00:47:45.679
That means all of your user interface controls, all of
your alerts, all of your dialogues will already be spoken.

00:47:45.679 --> 00:47:50.059
It's up to you to make sure that
your app doesn't sound stupid.

00:47:50.059 --> 00:47:51.400
Make sure they're spoken well.

00:47:51.400 --> 00:47:57.650
We haven't talked about speech recognition
today, but speakable items allows users

00:47:57.650 --> 00:48:01.289
to control your application by
speaking the names of your controls.

00:48:01.289 --> 00:48:04.509
Again, make sure that that works well.

00:48:04.510 --> 00:48:08.230
So since your application already
speaks, make sure it sounds good.

00:48:08.230 --> 00:48:12.090
Step two, differentiate your application.

00:48:12.090 --> 00:48:16.120
By that, I mean use speech in ways
that are specifically appropriate

00:48:16.119 --> 00:48:20.549
to how you know your users interact with your application.

00:48:20.550 --> 00:48:23.650
The things that we do in the user
interface are very general.

00:48:23.650 --> 00:48:24.980
You can do better.

00:48:24.980 --> 00:48:28.969
Let me give you a few guidelines
about where you might want to do that.

00:48:28.969 --> 00:48:31.509
One is asynchronous notifications.

00:48:31.510 --> 00:48:36.310
When something happens outside of the
user's immediate control, tell them.

00:48:36.309 --> 00:48:42.250
For example if they're doing some action that's
timed and they run out of time, let them know.

00:48:42.250 --> 00:48:43.889
>> Time's up.

00:48:43.889 --> 00:48:50.000
>> If you've got some task that's been happening that
will take a while, when it completes, let them know.

00:48:50.000 --> 00:48:52.170
>> Data backup is complete.

00:48:52.170 --> 00:49:02.070
>> The point is, up until now, we have tended to encode
messages like this to our user by playing arbitrary sounds.

00:49:02.070 --> 00:49:07.370
The burden is on the user to learn what
message is associated with what sound.

00:49:07.369 --> 00:49:13.029
If your application knows why it wants
to get a person's attention, tell them.

00:49:13.030 --> 00:49:17.130
If something happens that you need their
attention really quickly, let them know.

00:49:17.130 --> 00:49:19.130
>> Urgent email.

00:49:19.130 --> 00:49:21.740
>> You can even do this in games.

00:49:21.739 --> 00:49:25.219
>> Enemy planes at 12 o'clock.

00:49:25.219 --> 00:49:31.199
>> So, after asynchronous notifications, think
about other ways of giving feedback to the user

00:49:31.199 --> 00:49:34.059
when the user does something with your application.

00:49:34.059 --> 00:49:38.190
For example, when they're playing a
game and it finishes, let them know.

00:49:38.190 --> 00:49:40.400
>> Game over, game over, game over.

00:49:40.400 --> 00:49:44.470
>> That by the way was one of Matthias's audio effects.

00:49:44.469 --> 00:49:51.309
If you have a user who's doing some
interaction with an application at the point

00:49:51.309 --> 00:49:53.469
that they make some choice, you
need to feed that back to them.

00:49:53.469 --> 00:49:59.959
For example, I've noticed in first person shooting games,
often you'll need to change weapons as you enter a new phase

00:49:59.960 --> 00:50:02.460
in the game where a different weapon would be appropriate.

00:50:02.460 --> 00:50:04.490
So you can have the application speak them.

00:50:04.489 --> 00:50:05.139
Here's an example.

00:50:05.139 --> 00:50:05.199
[ Background noise ]

00:50:05.199 --> 00:50:10.149
>> M16, machine gun.

00:50:10.150 --> 00:50:18.860
Magnum, semi automatic, plasma
rifle, boomerang, water pistol.

00:50:18.860 --> 00:50:20.420
[ Laughter ]

00:50:20.420 --> 00:50:21.079
>> The water.

00:50:21.079 --> 00:50:23.019
Thank you.

00:50:23.019 --> 00:50:24.449
[ Applause ]

00:50:24.449 --> 00:50:27.899
The water pistol was for those who
didn't get the joke about the boomerang.

00:50:27.900 --> 00:50:30.369
[ Laughter ]

00:50:30.369 --> 00:50:32.109
Yeah, I'm an Aussie.

00:50:32.110 --> 00:50:36.349
That also was using Matthias's audio effects.

00:50:36.349 --> 00:50:40.929
If a user interacts with your application
in a way that takes a number of steps,

00:50:40.929 --> 00:50:44.539
you might want to summarize all
of those into a single sentence.

00:50:44.539 --> 00:50:49.739
For-- suppose for example I'm working in a
finance application and I'm organizing a payment.

00:50:49.739 --> 00:50:53.189
So I have to specify the from account
and the to account and who's the,

00:50:53.190 --> 00:50:56.220
who the payee is and the amount of money to transfer.

00:50:56.219 --> 00:51:03.559
So just before it finishes, you can actually say
back to the user, "Sending a check to AT&T for $60."

00:51:03.559 --> 00:51:05.719
Just summarize it.

00:51:05.719 --> 00:51:09.099
If, say that again?

00:51:09.099 --> 00:51:10.969
Oh all right.

00:51:10.969 --> 00:51:14.819
If a user's entering application-- entering
information you can use this to help them.

00:51:14.820 --> 00:51:17.490
For example, it's tax time.

00:51:17.489 --> 00:51:20.689
I have a pile of receipts from charitable donations.

00:51:20.690 --> 00:51:24.880
I laboriously read each receipt
and type it into my application.

00:51:24.880 --> 00:51:31.750
It would be great if I could just press a button and have
it read back the column of the information that I've typed

00:51:31.750 --> 00:51:35.869
in without me having to look at the screen, so I
could just look at my receipts as it reads it back

00:51:35.869 --> 00:51:37.900
and make sure I've got the amounts right.

00:51:37.900 --> 00:51:42.440
[ Period of silence ]

00:51:42.440 --> 00:51:49.490
And there are other ways that you can add speech
that's specific to how you know your users interact.

00:51:49.489 --> 00:51:57.239
For example, suppose you have an application
that allows users to write a script for a play.

00:51:57.239 --> 00:51:59.919
At some stage, they might want to hear it read back.

00:51:59.920 --> 00:52:03.789
Well using our hotkey, they could select
all the text and have it all read back.

00:52:03.789 --> 00:52:05.400
But you can do better.

00:52:05.400 --> 00:52:09.430
For example, you could read each
character with a different voice.

00:52:09.429 --> 00:52:16.509
You could know that you don't want to read back the acting
directions, the blocking queues, the lighting instructions.

00:52:16.510 --> 00:52:21.350
And here's an idea for those of you who have
such an application that you might want to go

00:52:21.349 --> 00:52:24.440
and implement and this idea came from Kevin Aitkin.

00:52:24.440 --> 00:52:30.190
And that is allow users to select
just one character to not be read out.

00:52:30.190 --> 00:52:32.950
So then, you could use that application for rehearsal.

00:52:32.949 --> 00:52:39.879
So the third step is customize.

00:52:39.880 --> 00:52:44.630
Sometimes a speech could say things
better than the way it does by default.

00:52:44.630 --> 00:52:47.019
So what kinds of things would you want to customize?

00:52:47.019 --> 00:52:48.949
Well, what words are spoken?

00:52:48.949 --> 00:52:54.839
If you just take the strings in your application and
pass them to our speech synthesizer, it will speak them.

00:52:54.840 --> 00:52:56.980
That may not be what you really want.

00:52:56.980 --> 00:53:02.530
Here for example is the name of a weapon
in a common first person shooter game.

00:53:02.530 --> 00:53:06.900
You pass that to the speech synthesizer,
it will dutifully read it out.

00:53:06.900 --> 00:53:09.260
>> USC.44, Magnum mega class A1.

00:53:09.260 --> 00:53:13.180
>> But wouldn't it be better if you could just say.

00:53:13.179 --> 00:53:14.469
>> Magnum.

00:53:14.469 --> 00:53:20.849
>> After you've figured out what words you really want
to speak, you might want to check how they're pronounced.

00:53:20.849 --> 00:53:25.779
Sometimes there might be words that are
specific to your application or company names

00:53:25.780 --> 00:53:28.910
that are not pronounced the way
you want them to be pronounced.

00:53:28.909 --> 00:53:35.809
For example, Raiden in, which is
actually I believe a character in a game,

00:53:35.809 --> 00:53:40.320
is not pronounced the best way
that we'd like by our synthesizer.

00:53:40.320 --> 00:53:45.870
So you can specify the phonemes to indicate
precisely how you would like it pronounced.

00:53:45.869 --> 00:53:50.509
>> Don't say Raydon, instead pronounce it Raiden.

00:53:50.510 --> 00:53:58.590
>> If the synthesizer does not say things the way you
want them to, you'll be tempted to use funny spelling.

00:53:58.590 --> 00:54:00.070
Spell things differently.

00:54:00.070 --> 00:54:05.019
Well, we advise against that because we
cannot guarantee that something that's spelt

00:54:05.019 --> 00:54:10.440
with nonstatic spelling will be pronounced the
same way with the next release of our synthesizer.

00:54:10.440 --> 00:54:17.300
However, if you use phonemes like this, then you
can be absolutely sure that all future versions

00:54:17.300 --> 00:54:21.539
of our synthesizer will maintain the correct pronunciation.

00:54:21.539 --> 00:54:27.529
After you've specified how the words are pronounced,
then you want to customize the speaking rate.

00:54:27.530 --> 00:54:35.220
>> Sometimes, the default speaking rate might make
the speech sound somewhat tedious and pedantic.

00:54:35.219 --> 00:54:41.209
>> But you can control the rate, as I have there,
with a rate command embedded into the text-to-speech.

00:54:41.210 --> 00:54:48.090
The general principle is, the more predictable
something is, the faster you want to speak it.

00:54:48.090 --> 00:54:50.480
And then customize the intonation.

00:54:50.480 --> 00:54:55.000
Now the intonation is the way we say what we say.

00:54:55.000 --> 00:54:59.440
You know the expression "It's not what
you said, it's the way that she said it."

00:54:59.440 --> 00:55:06.119
By intonation we mean the pauses, the, the pitch,
where the pitch goes up, where the pitch goes down,

00:55:06.119 --> 00:55:10.130
which words are accented, which words are not accented.

00:55:10.130 --> 00:55:19.539
One piece of low hanging fruit that is very effective and is
easy is liberally sprinkle punctuation through your strings.

00:55:19.539 --> 00:55:25.329
Things like commas, periods, question marks, double
quotes, are all paid attention to by our synthesizer

00:55:25.329 --> 00:55:28.469
and will change the way it reads things out.

00:55:28.469 --> 00:55:32.179
There are other commands that you can embed
into the text that you send to the speaker,

00:55:32.179 --> 00:55:35.809
to the synthesizer to further control the intonation.

00:55:35.809 --> 00:55:42.829
One of those is the emph - command,
which takes the stress off the next word.

00:55:42.829 --> 00:55:46.699
This is particularly useful for noun compounds.

00:55:46.699 --> 00:55:50.529
Rather than me giving you a linguistics
lesson, let me just illustrate it.

00:55:50.530 --> 00:55:52.260
[ Period of silence ]

00:55:52.260 --> 00:56:00.400
>> Flame thrower, flamethrower,
plasma grenade, plasma grenade,

00:56:00.400 --> 00:56:07.490
project list, project list, street view, street view.

00:56:07.489 --> 00:56:10.479
>> Do you hear a difference?

00:56:10.480 --> 00:56:13.990
Yeah, emph - is your friend.

00:56:15.579 --> 00:56:19.409
So let's do a Venn diagram.

00:56:19.409 --> 00:56:24.309
This circle represents the set of all
the things that could be done with speech

00:56:24.309 --> 00:56:27.519
in the user interface of your application.

00:56:27.519 --> 00:56:33.079
This smaller circle represents the tiny
subset of those things that we have done

00:56:33.079 --> 00:56:38.289
by the general speech user interface features in OS X.

00:56:38.289 --> 00:56:41.750
The rest is what you can do.

00:56:41.750 --> 00:56:47.469
At least test your application with speech
turned on so that you don't embarrass yourself.

00:56:47.469 --> 00:56:51.589
Then add extra speech in ways that are
specific to how you creatively think

00:56:51.590 --> 00:56:55.340
about the way your users interact with your application.

00:56:55.340 --> 00:56:58.440
And finally customize.

00:56:58.440 --> 00:57:03.070
Do all of those things and you'll be building
speech into your application in a way

00:57:03.070 --> 00:57:08.500
that gives your users a more engaging,
engulfing experience, multimedia,

00:57:08.500 --> 00:57:11.869
and differentiates your application
from some of your competitors.

00:57:11.869 --> 00:57:18.400
So that's what we have to say today
about text-to-speech synthesis.

00:57:18.400 --> 00:57:23.639
At this stage, I'm going to invite the rest of the team
back onto the stage and we'll take any questions you have.

00:57:23.639 --> 00:57:24.839
Thanks for your time.