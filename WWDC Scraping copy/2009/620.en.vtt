WEBVTT

00:00:12.560 --> 00:00:14.589
>> My name is Steve Parker.

00:00:14.589 --> 00:00:20.559
I know that I caught you right after lunch
and just a little while before the beer bash,

00:00:20.559 --> 00:00:28.859
but I think what I have is pretty exciting so I hope
it keeps your attention and keeps you interested.

00:00:28.859 --> 00:00:35.659
In particular, I'm going to talk about
the general server performance results

00:00:35.659 --> 00:00:39.049
in Snow Leopard which I think are outstanding.

00:00:39.049 --> 00:00:50.039
I want to suggest some things that you can do to help plan
for and create a system that performs just as well as some

00:00:50.039 --> 00:00:54.039
of the performance information
that I want to share with you.

00:00:54.039 --> 00:01:00.439
And I am going to mention a couple of things that
are very important if you happen to be a developer,

00:01:00.439 --> 00:01:05.879
and you're building a service that is
either large in scale or high performing.

00:01:05.879 --> 00:01:15.060
And then finally, I'm going to spend a bunch of time talking
about how you evaluate the performance of your system,

00:01:15.060 --> 00:01:23.780
how you identify bottlenecks and in general
how to make sure that it's performing its best.

00:01:23.780 --> 00:01:35.090
So, let's begin with the server performance data, but I
want to start by talking a little bit about benchmarks

00:01:35.090 --> 00:01:42.810
I'm about to present a bunch of benchmark data, how fast
does the system go, and in particular this is important

00:01:42.810 --> 00:01:46.540
because we'd like our performance not to be a surprise.

00:01:46.540 --> 00:01:52.910
We'd like to be able to know if I get this kind of a
system I can get this kind of performance out of it;

00:01:52.909 --> 00:01:59.840
we'd like to be able to use that in planning as we
purchase our systems; and we also need to be able

00:01:59.840 --> 00:02:03.280
to know when the performance is actually wrong.

00:02:03.280 --> 00:02:07.730
When is it not performing as well as it should be.

00:02:07.730 --> 00:02:14.379
So I'm going to start by talking
about a couple of SPEC results.

00:02:14.379 --> 00:02:19.049
SPEC.org is an industry consortium
that does standardized benchmarking.

00:02:19.050 --> 00:02:22.240
They write some really excellent benchmarks.

00:02:22.240 --> 00:02:32.439
One of the ones that I'm going to talk about today is
SPECjbb2005; that's a general Java performance benchmark.

00:02:32.439 --> 00:02:42.300
This is something that is based on an order
entry inventory system written entirely in Java.

00:02:42.300 --> 00:02:51.770
They have chosen that the benchmark not actually utilize
any file systems so it runs entirely from in-memory tables;

00:02:51.770 --> 00:03:02.670
and it basically scales up this order entry and inventory
system until at the end it finds the very peak level

00:03:02.669 --> 00:03:06.599
of performance as it scales up threads in Java and so on--

00:03:06.599 --> 00:03:11.379
the very peak performance that
you can get out of that engine.

00:03:11.379 --> 00:03:18.329
So Snow Leopard with a combination of improvements to the OS

00:03:18.330 --> 00:03:24.219
and the Java Virtual machine is
30% faster than it was in Leopard.

00:03:24.219 --> 00:03:29.659
I think this is a really excellent result.

00:03:29.659 --> 00:03:38.819
You might want to know how fast is your file system
so SPECsfs is actually a very long-standing benchmark;

00:03:38.819 --> 00:03:50.060
it was originally only an NFS benchmark, and in particular
what it does is it simulates both NFS and Samba clients.

00:03:50.060 --> 00:03:58.150
And the SPEC style is very carefully written
to simulate the real activity of users.

00:03:58.150 --> 00:04:04.689
Sometimes you can build an artificial benchmark
that's like if I copy files how long does it take.

00:04:04.689 --> 00:04:09.300
Well those benchmarks are usually
scripted and very sequential.

00:04:09.300 --> 00:04:16.170
And in fact when you have a group of users using
a file server, their requests actually arrive

00:04:16.170 --> 00:04:22.060
at a somewhat randomized interval to the server,
and that is a little more challenging load

00:04:22.060 --> 00:04:26.560
than you sometimes see with a simple scripted test.

00:04:26.560 --> 00:04:35.550
So SPECsfs is careful to simulate that behavior, it's a
metadata heavy benchmark, so it realizes that in fact a lot

00:04:35.550 --> 00:04:44.949
of what you're doing is searching file trees, looking
things up; it does a combination of read and write.

00:04:44.949 --> 00:04:59.990
It also makes sure that there is 120MB worth of
data behind every op/sec, so this is to make sure

00:04:59.990 --> 00:05:03.699
that you've actually got a full size file tree;

00:05:03.699 --> 00:05:08.259
because in fact if you're measuring a
large busy server it's not just going

00:05:08.259 --> 00:05:12.139
to have a tiny number of files that might all get cached.

00:05:12.139 --> 00:05:24.189
So this insures that there is a file tree that grows
in size, it reflects the real life behavior on disk.

00:05:24.189 --> 00:05:29.810
If I look at my disk at the beginning of the day and I
look at it at the end there's always more data on it.

00:05:29.810 --> 00:05:33.649
So the course of the run actually grows the data set.

00:05:33.649 --> 00:05:40.500
And the other thing is because the goal of this
benchmark is to measure the server's performance,

00:05:40.500 --> 00:05:51.000
it's careful to provide its own NFS and CIFS client code
so that it can make sure to do exactly the same thing.

00:05:51.000 --> 00:06:00.269
Sometimes a client implementation can interact badly with
the server and can make it either look better or look worse.

00:06:00.269 --> 00:06:13.259
So a combination of the K64 kernel, VM scalability, IP route
locking, work in Snow Leopard has increased this by well

00:06:13.259 --> 00:06:19.539
over 2X for the Samba performance
which I think is outstanding.

00:06:19.540 --> 00:06:30.379
NFS performance likewise is benefiting
from all those same things.

00:06:30.379 --> 00:06:34.300
That one is a full 2X.

00:06:34.300 --> 00:06:43.800
So, those are slightly simpler workloads.

00:06:43.800 --> 00:06:51.449
Another key workload, some of you have
e-mail, yes, would be your mail server.

00:06:51.449 --> 00:06:58.629
So the mail server, there is also
a SPEC benchmark, SPECmail 2009,

00:06:58.629 --> 00:07:02.850
we've been very active in helping
SPEC develop this benchmark.

00:07:02.850 --> 00:07:07.110
It does a sophisticated simulation of IMAP.

00:07:07.110 --> 00:07:11.350
IMAP is a very complex protocol as some of you may know.

00:07:11.350 --> 00:07:16.439
There are a lot of different ways
to accomplish the mail operations.

00:07:16.439 --> 00:07:26.480
SPECmail has a huge array of these different ways of
doing things, and it balances a randomized distribution

00:07:26.480 --> 00:07:36.129
across them all so that it simulates an average
amalgam of a wide variety of IMAP clients.

00:07:36.129 --> 00:07:45.600
It tracks the message contents and makes sure to
keep in mind that some of the operations ought to be

00:07:45.600 --> 00:07:52.150
against messages that have big attachments;
many of them are actually much smaller,

00:07:52.149 --> 00:07:55.859
and those will have different performance properties.

00:07:55.860 --> 00:08:07.730
So in Snow Leopard I'm pleased to announce that we
have posted a new world record in SPECmail 2009 wellsty lg

00:08:07.730 --> 00:08:20.580
above the Sun Java messaging server so you can
treat us as a contender for mail performance.

00:08:20.579 --> 00:08:23.379
I'm incredibly excited about that.

00:08:23.379 --> 00:08:27.029
So, those are great results.

00:08:27.029 --> 00:08:36.620
Let's talk however about how you actually get that
performance on your system; and first of all I do want

00:08:36.620 --> 00:08:45.259
to start by acknowledging and talking a
little bit about benchmarking versus reality.

00:08:45.259 --> 00:08:52.879
So a show of hands-- every one of you who has
ever once found that a benchmark suggested

00:08:52.879 --> 00:08:58.080
that you could make a change and get a certain
performance gain and you got something different.

00:08:58.080 --> 00:09:00.490
How many of you had that experience?

00:09:00.490 --> 00:09:14.000
Right. So, one of the things that I am going to focus on in
this talk is how you do an evaluation that can be specific

00:09:14.000 --> 00:09:21.220
to your system and your workload, and try and answer
questions about how much faster will my system be.

00:09:21.220 --> 00:09:30.399
And if there's one thing that you take away from this talk,
it should be that this is an end to end system problem,

00:09:30.399 --> 00:09:34.649
and it is necessary to make sure that all of the plumbing

00:09:34.649 --> 00:09:39.549
for whatever services you are providing
are really up to snuff.

00:09:39.549 --> 00:09:47.589
And the way I like to think about it is we are
in this world that is filled with incredibly fast

00:09:47.590 --> 00:09:57.690
and powerful multicore CPUs; and they are connected by what
are by and large small pipes to the network and to the disks

00:09:57.690 --> 00:10:04.470
of system; and it is getting this balance
of how your workload needs to flow

00:10:04.470 --> 00:10:07.899
across all of those things that's necessary.

00:10:07.899 --> 00:10:18.209
If you set it up so that you have too small a pipe, which
you end up having is that the flow of your requests backs up

00:10:18.210 --> 00:10:24.200
and that is what you have to try and work to avoid.

00:10:24.200 --> 00:10:32.030
What you really need to do is make sure that there are
parallel paths adequate for the network and the disk paired

00:10:32.029 --> 00:10:38.389
to the system that you're using
and the workload that you have

00:10:38.389 --> 00:10:42.600
So let's talk a little bit about this NFS score.

00:10:42.600 --> 00:10:51.450
So it is absolutely a disk bottleneck benchmark,
and in fact there's a whole lot of extra CPU power.

00:10:51.450 --> 00:10:55.410
If I add more disks, I can probably get a higher score.

00:10:55.409 --> 00:10:57.480
That's what we did.

00:10:57.480 --> 00:11:05.889
That is still a really great example of
the incredible speed that's available,

00:11:05.889 --> 00:11:14.720
but one of the things that you should definitely
take away if you go and look for example on SPEC.org

00:11:14.720 --> 00:11:21.970
at that publication and it lists this
big pile of Fibre Channel storage.

00:11:21.970 --> 00:11:27.820
That's a key part of achieving that benchmark score.

00:11:27.820 --> 00:11:35.870
So keep in mind that these SPEC Postings are
what I like to call the "Speed of Light".

00:11:35.870 --> 00:11:43.259
If I tune this all the way up, if I get the
biggest and best, what can I do with it?

00:11:43.259 --> 00:11:52.669
So in this particular case, it really
is that array of 64 15K SAS drives,

00:11:52.669 --> 00:11:57.349
there is 8 megabytes' worth of
battery-backed RAM caching it.

00:11:57.350 --> 00:12:05.470
In fact a lot of disk I/OS are actually only taking
100 microseconds from the point of view of the server

00:12:05.470 --> 00:12:09.769
because it's delivered to the RAM in the Promise unit.

00:12:09.769 --> 00:12:15.689
The Promise unit makes sure that there are redundant
copies and two sets of RAM and then it writes back

00:12:15.690 --> 00:12:19.060
across the Fibre Channels that it's good to go.

00:12:19.059 --> 00:12:27.409
So that's really important to providing
that level of service.

00:12:27.409 --> 00:12:29.919
However, that's not for free.

00:12:29.919 --> 00:12:34.079
That's $80K worth of disk drives.

00:12:34.080 --> 00:12:40.379
So how many people actually have a
system with that many drives connected?

00:12:40.379 --> 00:12:42.029
Wow, all right.

00:12:42.029 --> 00:12:46.230
Wow. OK. Excellent.

00:12:46.230 --> 00:12:47.620
So I'm kind of surprised.

00:12:47.620 --> 00:12:57.330
I think the majority of people though-- how many of you
have nothing but a SATA drive internal in your equipment?

00:12:57.330 --> 00:13:00.230
Internal Ray, OK.

00:13:00.230 --> 00:13:09.920
So, the other thing that you'll notice in that posting,
I use 32 separate HFS+2 file systems across that array.

00:13:09.919 --> 00:13:14.250
It turns out that's also important and
I'm going to talk about that later on.

00:13:14.250 --> 00:13:21.649
If you are actually going to get that performance level
though, you are going to need to do all of those things

00:13:21.649 --> 00:13:27.100
to plan for getting the storage adequate behind that.

00:13:27.100 --> 00:13:36.710
Well, so, if you haven't got the $80K worth of disks behind
it, you might be asking why is this actually good news?

00:13:36.710 --> 00:13:47.379
Well, the really good news about it is that this is an
indication of the ability for the server to scale up.

00:13:47.379 --> 00:13:57.700
So it's telling you that the plumbing that the OS
underneath it, that the systems services are able to have--

00:13:57.700 --> 00:14:04.080
it turns out during the peak of that run there are
10's of thousands of I/Os in parallel for example.

00:14:04.080 --> 00:14:12.400
There are paths capable of handling
really large volumes of data.

00:14:12.399 --> 00:14:20.970
That particular set of postings is using 8 gigabit NICs.

00:14:20.970 --> 00:14:29.290
And is driving at peak 840 megabytes
of Random I/O to that array.

00:14:29.289 --> 00:14:37.019
So the good news about that is that you do
have this big multicore chip that does a lot.

00:14:37.019 --> 00:14:47.429
Unfortunately back in Leopard, you were only able to
put some of the cores to work in some important areas.

00:14:47.429 --> 00:14:55.189
What's different is that all those cores are now available
and if you are not in a position to be driving this

00:14:55.190 --> 00:15:03.100
to that kind of level, then you should be considering
putting a lot more than just NFS on this box.

00:15:03.100 --> 00:15:09.509
You should be running some web services; you
should be running maybe Wikis, Podcast Producer,

00:15:09.509 --> 00:15:17.519
but in general all of those workloads will
operate in parallel because this is just available

00:15:17.519 --> 00:15:23.730
and spare machine capacity; and there were a number
of limitations in Leopard that did not allow you

00:15:23.730 --> 00:15:27.450
to get at that especially for server workloads.

00:15:27.450 --> 00:15:33.879
So the good news is that where you
might have in the past used a server

00:15:33.879 --> 00:15:39.509
for a single purpose, that's no longer necessary.

00:15:41.769 --> 00:15:56.230
So, definitely understand that all of the
focus that I've given on disk performance is

00:15:56.230 --> 00:16:04.879
because the CPUs have gotten faster and faster, sort of
Moore's law kind of every three years or so twice as fast,

00:16:04.879 --> 00:16:10.600
the clock speed 's not faster now, now we're
adding more cores but it's still going faster.

00:16:10.600 --> 00:16:21.490
However, you definitely do need to pay attention to
disk storage which has not gotten that much faster.

00:16:21.490 --> 00:16:25.850
And keep in mind that all of those factors

00:16:25.850 --> 00:16:32.860
that I've mentioned are absolutely
necessary for that level of performance.

00:16:32.860 --> 00:16:41.200
So now I want to drop it back, however, and I want to talk
about what I think is actually a little more common scenario

00:16:41.200 --> 00:16:46.020
which is I've just got a SATA drive,
maybe some hardware RAID.

00:16:46.019 --> 00:16:54.449
So to put it in perspective, you should be
expecting to be like 1/40th of that score.

00:16:54.450 --> 00:17:02.720
It is definitely going to go a lot slower and that
will be because the disks of system can't carry it.

00:17:02.720 --> 00:17:08.509
Now, a lot of us do have it because
we care about having a lot of bytes.

00:17:08.509 --> 00:17:17.170
My MP3 collection doesn't get any smaller; I'll be yours
doesn't, and neither does the data in your Data Center,

00:17:17.170 --> 00:17:23.160
and that is a lot less than 1/40th of the
cost@-- it's about 1/400th of the cost.

00:17:23.160 --> 00:17:32.350
So but keep in mind those differences and design for it.

00:17:32.349 --> 00:17:36.439
How many of you use RAID5 underneath your storage?

00:17:36.440 --> 00:17:37.779
Lots of people.

00:17:37.779 --> 00:17:44.160
So the slowest of those options that I listed is RAID5.

00:17:44.160 --> 00:17:52.540
RAID5 is requiring that I actually lay down three
different stripes of the data so I get the reliability.

00:17:52.539 --> 00:18:04.460
That means that it's not actually delivering additional
speed; so this chart is intended to give you a sort

00:18:04.460 --> 00:18:16.069
of a reference for that general difference between
what you can achieve like in a single Xserve

00:18:16.069 --> 00:18:25.559
with traditional drive technology and all
of that is still definitely at at least 10%

00:18:25.559 --> 00:18:33.179
of that score against the Promise Fibre Channel array.

00:18:33.180 --> 00:18:42.900
So, I have several slides here that I definitely
recommend that after the conference you pick up my slides.

00:18:42.900 --> 00:18:51.040
These are reference information that I think you'll find
very handy because it breaks down what these choices are,

00:18:51.039 --> 00:18:57.180
and I'm not going to go through them in
great detail, but it's pretty simple.

00:18:57.180 --> 00:19:07.289
At the 1 to 3 terabyte range you do still have
the option of simply SATA and SAS drive modules.

00:19:07.289 --> 00:19:18.129
If you need very large data sets you're going to be on Fibre
Channel anyway, and keep in mind that those are in order

00:19:18.130 --> 00:19:28.160
of speed so each one of the steps you take down that list,
that's providing you a significant jump up in performance.

00:19:28.160 --> 00:19:38.990
Now before I go through two other sets of important
tables about disk drive choices, I want to take a second

00:19:38.990 --> 00:19:45.829
and talk about what the really tough
challenge is around files systems and storage;

00:19:45.829 --> 00:19:52.569
and it's a big mouthful of geek
speak-- Metadata Rate of Change.

00:19:52.569 --> 00:19:57.490
So, what do I mean by Metadata Rate of Change?

00:19:57.490 --> 00:20:08.390
Well, anything that creates, removes, renames
a file, ownership, permissions or attributes,

00:20:08.390 --> 00:20:20.850
all of those things require synchronous round trips to the
disk, and that volume has to basically stop other people

00:20:20.849 --> 00:20:28.419
who have updates to that file system, until
that change is committed, reliably to the disk.

00:20:28.420 --> 00:20:39.390
So, for example on HFS+, it needs to rebalance the
B-tree if you've done a rename, create, delete,

00:20:39.390 --> 00:20:46.040
changes need to be committed to its journal, that's a
synchronous operation, and it has a single catalog file

00:20:46.039 --> 00:20:51.359
that has to be updated when the volume
is changed in these significant ways.

00:20:51.359 --> 00:20:56.319
Now, this is the opposite of the data inside the user file.

00:20:56.319 --> 00:21:04.149
So none of those things are actually
requiring me to stop other threads of work.

00:21:04.150 --> 00:21:11.680
So this is one of the reasons why I
mentioned that I use 32 separate HFS+ volumes.

00:21:11.680 --> 00:21:24.330
So the reason was I got the Metadata Rate of Change of the
system higher; and you should definitely keep that in mind

00:21:24.329 --> 00:21:29.259
as you lay out volumes and choose your technology.

00:21:29.259 --> 00:21:35.190
So in this particular case I wanted to
highlight that between HFS+ and Xsan,

00:21:35.190 --> 00:21:38.509
you should keep in mind some of the difference choices.

00:21:38.509 --> 00:21:48.190
One of the things the Xsan is fabulous at, if you have
files that tend to be only accessed by a particular client,

00:21:48.190 --> 00:21:54.370
it can deliver incredible bandwidth,
incredible read and write speed.

00:21:54.369 --> 00:22:04.739
On the other hand, HFS+ will work particularly well if
many different processes on a single system are sharing it,

00:22:04.740 --> 00:22:13.269
and it will use RAM as a cache to help
prevent your needing to go to disk,

00:22:13.269 --> 00:22:22.440
and definitely both of them do have
reasonably good data write and data read rates.

00:22:22.440 --> 00:22:31.549
But now what I want to do is I want to pull it all back
in to what you actually do and what you actually set up.

00:22:31.549 --> 00:22:35.329
That was all abstract about disks
and technologies, yes whatever.

00:22:35.329 --> 00:22:42.299
So, what I have done here is sort of connect
those different things and brought back

00:22:42.299 --> 00:22:47.009
down to particular services a set
of specific recommendations.

00:22:47.009 --> 00:22:52.910
So for example, Web service is
actually a very low disk load service;

00:22:52.910 --> 00:23:00.790
that's something where if all you have is
a built-in SATA drive that's not a problem;

00:23:00.789 --> 00:23:05.289
mail like file services are at the other end.

00:23:05.289 --> 00:23:09.789
Those are areas where if you're
doing a high performance system,

00:23:09.789 --> 00:23:13.970
you definitely need to consider
something like Fibre Channel solutions.

00:23:13.970 --> 00:23:22.470
So, the details again will be available
in slides after the conference.

00:23:22.470 --> 00:23:26.000
Definitely pick that stuff up and keep it handy.

00:23:26.000 --> 00:23:31.990
It will help you to plan for how to get
a system that scales and performs well.

00:23:31.990 --> 00:23:40.490
All right so I've talked about how fast it
goes; I've talked about the things that you need

00:23:40.490 --> 00:23:43.920
to be thinking about in planning for performance.

00:23:43.920 --> 00:23:52.090
Now I want to talk about how you create
services that are going to go fast.

00:23:52.089 --> 00:23:57.059
So this is really absolutely targeted
for the developers in the audience.

00:23:57.059 --> 00:24:06.200
So, in particular the three areas that I'm going to talk
about-- the first is if you are using select and poll,

00:24:06.200 --> 00:24:12.000
and if you are using more than 100 or so files-- don't.

00:24:12.000 --> 00:24:14.589
Give it up, switch to kqueue.

00:24:14.589 --> 00:24:21.829
Select and poll are at best O of N algorithms.

00:24:21.829 --> 00:24:33.379
They will never scale; kqueue is an O of 1 interface,
it's an interface which is also incredibly convenient

00:24:33.380 --> 00:24:38.340
because you simply create one descriptor
for all the activities of the service,

00:24:38.339 --> 00:24:43.259
and you just collect events back
with kevent off that service.

00:24:43.259 --> 00:24:51.289
It can give you a handle that points in to a record
of your choosing that helps drive the service;

00:24:51.289 --> 00:24:59.119
so for example if I'm writing an IMAP server,
in fact Dovecot uses this, it calls kevent,

00:24:59.119 --> 00:25:05.659
kevent returns a file descriptor and a
pointer to information about the IMAP session.

00:25:05.660 --> 00:25:14.930
And in fact that mail score would probably be 1/3 of
what it is if all I did was to switch back to select.

00:25:14.930 --> 00:25:17.660
It is a significant factor-- don't do it.

00:25:17.660 --> 00:25:21.910
You've got a better interface-- use it.

00:25:21.910 --> 00:25:28.800
Resource Limits-- services launched by
launchd and I do encourage you to look

00:25:28.799 --> 00:25:33.490
into launchd, it's very easy to use, very powerful.

00:25:33.490 --> 00:25:42.029
By setting up the plist for your service you can
help launchd do its job of playing traffic cop.

00:25:42.029 --> 00:25:49.349
So one of the aspects of traffic cop is to
protect the system from a runaway process,

00:25:49.349 --> 00:25:58.809
from a process that has actually failed in some way and is
effectively its own denial of service attack on your system.

00:25:58.809 --> 00:26:08.129
And in particular, it will allow you to set this
variety of resources; but in particular pay attention

00:26:08.130 --> 00:26:16.380
to the largest file size and open files at peak those
are the two that I see services most often get wrong.

00:26:16.380 --> 00:26:20.460
If you're going to have a large number of open files,

00:26:20.460 --> 00:26:26.009
nominate how many you want, get
yourself launched from launchd.

00:26:26.009 --> 00:26:30.950
I definitely encourage you to look into that and use that.

00:26:30.950 --> 00:26:35.660
One of the things which is improved in Snow Leopard,

00:26:35.660 --> 00:26:44.160
if you are launched from launchd there is a new
higher listen queue depth; listen queue is important

00:26:44.160 --> 00:26:50.730
for that Thursday after lunch when
everyone hits refresh on their browser

00:26:50.730 --> 00:26:58.180
and in particular what it does is it makes sure
that spikes in traffic are dealt with smoothly.

00:26:58.180 --> 00:27:04.000
And it does that by making sure that the
kernel doesn't simply drop a connection

00:27:04.000 --> 00:27:10.150
because the service is ramping itself back
up and doesn't respond quickly enough.

00:27:10.150 --> 00:27:16.280
And in particular that is very common
because Apache for example is going

00:27:16.279 --> 00:27:20.089
to scale itself way down if there's no activity.

00:27:20.089 --> 00:27:26.220
It takes a little while for it to ramp up and have
enough daemons available to answer connections;

00:27:26.220 --> 00:27:32.600
and it may be spawning that process but you
still want a response back to the client

00:27:32.599 --> 00:27:37.589
that lets the client know the server
is coming with a response.

00:27:37.589 --> 00:27:46.559
So, those were the three specific recommendations I really
wanted to make for our developers so now I want to turn

00:27:46.559 --> 00:27:54.799
to the final point which was the section
on Diagnosing Performance Problems.

00:27:54.799 --> 00:28:00.009
If you haven't hit the speed of light, why not?

00:28:00.009 --> 00:28:11.250
So I wanted to begin by showing you these two graphs and
these two graphs are server performance in a nutshell

00:28:11.250 --> 00:28:20.240
Basically, pretty much every server performance
problem has these two graphs that are key

00:28:20.240 --> 00:28:26.069
to the understanding how the workload
behaves and how you study it.

00:28:26.069 --> 00:28:35.500
And in particular, the thing to notice here is
that most server workloads which are composed

00:28:35.500 --> 00:28:44.720
of relatively small amounts of work but lots of clients,
lots of separate requests is that it steps up in a more

00:28:44.720 --> 00:28:52.210
or less linear fashion until it hits the
system peak; and it should do so linearly

00:28:52.210 --> 00:28:59.759
so if you have this much additional client
load, you should see this much additional CPU,

00:28:59.759 --> 00:29:04.619
this much additional network traffic,
this much additional disk I/O.

00:29:04.619 --> 00:29:13.269
That general guideline is that first level of
triage to understanding how the system is behaving;

00:29:13.269 --> 00:29:19.410
and finally the other thing to be aware of is that
once you've reached the point where you're presenting

00:29:19.410 --> 00:29:27.640
as much load as the system has capacity for, you want
to pay attention to how it falls off in overload.

00:29:27.640 --> 00:29:33.009
You hope it falls off in the manner that I've graphed here.

00:29:33.009 --> 00:29:40.750
If the system is badly behaved, if it has scalability
difficulties, that curve will drop back down.

00:29:40.750 --> 00:29:47.650
So you want to keep in mind that as
increments of client load are added

00:29:47.650 --> 00:29:54.769
that you should see equivalent increments,
proportional increments in throughput.

00:29:54.769 --> 00:30:02.440
So a similar evaluation, it's basically
like two sides of the same coin really,

00:30:02.440 --> 00:30:05.289
is the latency of these different operations.

00:30:05.289 --> 00:30:10.849
And the general curve for most all
server benchmarks looks like this.

00:30:10.849 --> 00:30:20.119
So the latency is constant under low
load so what might be a good example?

00:30:20.119 --> 00:30:30.339
I'm perhaps logging into a secure web page so I connect
to the web server, I provide it some credentials,

00:30:30.339 --> 00:30:33.559
it does some processing, it sends it back.

00:30:33.559 --> 00:30:42.039
So that takes a certain length of time and no matter
whether I've got 16 cores or a little Mac mini,

00:30:42.039 --> 00:30:45.690
there's a certain amount of time that's going to take.

00:30:45.690 --> 00:30:52.559
Now on my server machine as I add more of those,
they probably are going to get separate cores,

00:30:52.559 --> 00:30:59.750
they're going to run in parallel and I'm not actually
going to see the latency increase until I hit the peak.

00:30:59.750 --> 00:31:07.549
So when I hit the peak what's going to happen
is that there now in the example I mentioned,

00:31:07.549 --> 00:31:14.930
there now isn't going to be any available CPU to immediately
run this request; so the latency is going to be the latency

00:31:14.930 --> 00:31:19.420
of queuing for whatever resource is being contended here.

00:31:19.420 --> 00:31:28.720
And that latency is basically-- you're just going
to continue in overload also in a linear fashion.

00:31:28.720 --> 00:31:32.230
So why are these graphs important?

00:31:32.230 --> 00:31:40.640
These graphs are important to understand because if you've
got say a lab or you're providing AFP to your customers,

00:31:40.640 --> 00:31:45.180
what you're looking for is where that peak is.

00:31:45.180 --> 00:31:51.060
That is the point at which the system is
going to go non-linear and some resource

00:31:51.059 --> 00:31:56.549
or other is no longer available adequately.

00:31:56.549 --> 00:32:00.899
How many of you came to my talk last year?

00:32:00.900 --> 00:32:02.350
A few of you?

00:32:02.349 --> 00:32:09.579
So in that talk last year I introduced a tool that
will be available as part of the session materials,

00:32:09.579 --> 00:32:14.159
probably later this afternoon but no later than tomorrow.

00:32:14.160 --> 00:32:26.050
And it's something that we use in-house all the time because
it gives us the network, the CPU and the disk utilization

00:32:26.049 --> 00:32:37.789
at a glance; it allows me to break down whether my CPU has
headroom, whether or not my network traffic is peaked out,

00:32:37.789 --> 00:32:45.200
have I reached 940 megabits a second out that 1
gigabit NIC, because if I have that's a problem

00:32:45.200 --> 00:32:54.970
and I've hit the peak point; and I would be identifying
network reconfiguration to improve that performance.

00:32:54.970 --> 00:33:04.700
So I will be demonstrating this tool in our lab afterwards
as well as a bunch of the rest of these techniques,

00:33:04.700 --> 00:33:14.830
and will be happy to go into very specific examples
about how you use this, exactly what you're looking for.

00:33:14.829 --> 00:33:28.029
But basically what you want to do is you want to be looking
at that information for your server, and in particular,

00:33:28.029 --> 00:33:35.490
what you want to do is look for those opportunities
when you can measure the system at different loads

00:33:35.490 --> 00:33:40.859
and do that same attempt-- see where it fits on that graph.

00:33:40.859 --> 00:33:49.539
And in particular, the example that I've got
here would be if you find this opportunity

00:33:49.539 --> 00:34:00.450
where you can identify I've got 200 AFP clients and I've hit
60% CPU, and I'm wondering wow you know how is that doing?

00:34:00.450 --> 00:34:06.660
The best thing that you can do is to save
that data off, look for an opportunity

00:34:06.660 --> 00:34:12.970
when perhaps there are 50 more clients connected
and doing work, and then repeat that measurement.

00:34:12.969 --> 00:34:19.669
And what I've done with the math in there
is to look for whether or not it was linear.

00:34:19.670 --> 00:34:28.010
And in fact what I see in that example is it's gone
non-linear, and that would tell me that I should be looking

00:34:28.010 --> 00:34:35.960
for lock-thrashing; that something in the
system is preventing using the CPU effectively,

00:34:35.960 --> 00:34:38.710
and that is my bottleneck.

00:34:40.710 --> 00:34:50.690
So also from last year's talk there is a KBase
article HT1992, which is a great place to start.

00:34:50.690 --> 00:35:00.119
Not only does it have a version of the top-guide script
there for Leopard as well, it has a breakdown of some

00:35:00.119 --> 00:35:08.489
of these different areas; in particular in the case of
CPU bottlenecks, there's a list of three techniques.

00:35:08.489 --> 00:35:13.379
One is using DTrace plockstat providers.

00:35:13.380 --> 00:35:21.240
Did any of you go to the first talk on advanced
debugging and performance this morning?

00:35:21.239 --> 00:35:23.239
OK somebody.

00:35:23.239 --> 00:35:29.259
So those DTrace providers are excellent
ways to get an insight

00:35:29.260 --> 00:35:35.620
to whether there are kernel resources being
contended that are preventing CPU scalability.

00:35:35.619 --> 00:35:38.659
And there are a couple of scripts
attached to that KBase article.

00:35:38.659 --> 00:35:46.379
In addition the plockstat -C, that is a
DTrace based command that will tell you

00:35:46.380 --> 00:35:55.750
if a pthread synchronization thrashing is happening
in a process; it also talks in a little more detail

00:35:55.750 --> 00:36:04.320
than I can cover today about evaluating network
traffic making sure that you've actually stayed

00:36:04.320 --> 00:36:13.000
within both packet count and data rate limits,
as well as talking a little bit about disk I/O.

00:36:13.000 --> 00:36:20.920
Disk I/O is a little bit harder to evaluate directly, but
I am going to talk about some very specific techniques

00:36:20.920 --> 00:36:28.710
that you can use to try and identify whether or
not a disk I/O bottleneck is actually present.

00:36:28.710 --> 00:36:43.210
So, just briefly plockstat -C output gives you information
on pthread mutex blocks, reader or writer blocks for read

00:36:43.210 --> 00:36:46.099
as well as reader writer blocks for write.

00:36:46.099 --> 00:36:50.549
If you're a developer this means that
it can actually give you an insight

00:36:50.550 --> 00:37:01.130
into whether your application is multithreaded well
and using the pthread synchronization primitives well.

00:37:01.130 --> 00:37:18.640
And the other thing is that count field-- what that
is giving you is the number of times when you went

00:37:18.639 --> 00:37:29.769
after a lock, you were blocked from getting the lock
because some other thread owned it, and one of the beauties

00:37:29.769 --> 00:37:34.369
of running plockstat in this fashion is zero overhead.

00:37:34.369 --> 00:37:42.400
Many times an instrumentation methodology
will have overhead that affects the behavior.

00:37:42.400 --> 00:37:46.980
This has zero overhead unless you
were actually blocking on a mutex.

00:37:46.980 --> 00:37:51.829
That is the only overhead for this DTrace-based primitive.

00:37:51.829 --> 00:38:01.480
So those are the three methods for going
after when the CPU might be a bottleneck.

00:38:01.480 --> 00:38:09.880
One of the other cute tricks that I wanted to show
you so I talked about HFS+ and metadata updates,

00:38:09.880 --> 00:38:22.690
so in particular you can also use DTrace in a clever
way to find out if the journal on HFS+ is overloaded.

00:38:22.690 --> 00:38:31.690
So the mtxheat.dscript that is part of that KBase
article, it turns out that there's one particular path--

00:38:31.690 --> 00:38:39.030
this is showing a stack backtrace inside the kernel,
and when you see that journal start transaction,

00:38:39.030 --> 00:38:50.710
that specifically means that you are seeing another process
being blocked because someone is doing a journal update.

00:38:50.710 --> 00:39:01.990
So that only becomes a relatively common
occurrence when you have overloaded a volume.

00:39:01.989 --> 00:39:09.689
The address of the lock which is printed by the
script there actually correlates to a specific volume,

00:39:09.690 --> 00:39:17.349
and in the lab I'll show you how you can identify
the specific volume that is associated with that.

00:39:17.349 --> 00:39:23.799
And that number that's being recorded at the
bottom that is the number of times per second;

00:39:23.800 --> 00:39:31.810
so that particular sample 179 times
someone couldn't request a journal I/O

00:39:31.809 --> 00:39:35.799
because the journal was locked for a current write.

00:39:35.800 --> 00:39:49.400
That is a file system constrained
performance problem right there-- that's bad.

00:39:49.400 --> 00:39:50.710
Too big a number.

00:39:50.710 --> 00:39:58.179
All right so per volume disk usage is also available.

00:39:58.179 --> 00:40:05.389
This is also a DTrace-based tool,
iopending and it's in all 10 server systems,

00:40:05.389 --> 00:40:10.369
and it's telling you how deep is the disk queue?

00:40:10.369 --> 00:40:13.880
How many of you are familiar with
the concept of load average?

00:40:13.880 --> 00:40:22.500
Yes. So that's essentially how many processes
are waiting to be scheduled on the CPU.

00:40:22.500 --> 00:40:26.010
This is telling you the exact same thing.

00:40:26.010 --> 00:40:34.000
This is telling you how deep is the disk queue here,
and with the minus -m option you're able to focus it

00:40:34.000 --> 00:40:41.039
on a specific volume which is also very handy; because
it allows you to look at the configuration of your system

00:40:41.039 --> 00:40:47.279
and perhaps you can identify a way to partition
your workload, partition your data sets

00:40:47.280 --> 00:40:52.860
so that you get a more effective
use of your disk subsystems.

00:40:52.860 --> 00:41:00.110
So in this particular case this is actually a pretty good
curve because we see that the distribution is normally

00:41:00.110 --> 00:41:05.000
about zero so that means that most of
the time the disk controller is idle

00:41:05.000 --> 00:41:07.280
and ready to accept a command for that disk.

00:41:07.280 --> 00:41:15.390
If it were up around sort of 3 that would be a
little bit warm, and if it were say, up around 5,

00:41:15.389 --> 00:41:21.889
6 or it can get much higher that would
definitely mean you are disk constrained.

00:41:21.889 --> 00:41:30.349
So that is a way to look per volume at whether
or not your disk I/O is performing well.

00:41:30.349 --> 00:41:40.589
In addition that disk_r and disk_w are an indication of
the amount of the activity that is read or write and notice

00:41:40.590 --> 00:41:43.870
that in this particular case it was entirely write activity.

00:41:43.869 --> 00:41:48.719
It was with single 4K read so.

00:41:48.719 --> 00:41:50.489
All right.

00:41:50.489 --> 00:41:58.239
So another thing that you can do that's
actually really easy is check whether

00:41:58.239 --> 00:42:05.389
or not Spotlight is an impact on your system performance.

00:42:05.389 --> 00:42:15.179
So it does keep an index, it's maintained on the fly,
and in particular if you do have a very write-heavy load,

00:42:15.179 --> 00:42:20.719
that may end up being a performance difficulty.

00:42:20.719 --> 00:42:32.509
In particular you can watch for mds or md importer
processes using ps or top; if they have a significant amount

00:42:32.510 --> 00:42:41.780
of CPU usage, then it would probably
be a good idea to try this technique.

00:42:41.780 --> 00:42:46.870
And I find the easiest one is to just throw the big switch.

00:42:46.869 --> 00:42:57.159
So that command just stops it, and so if you're running
top-guide in one window and you throw the switch,

00:42:57.159 --> 00:43:07.429
if you don't see a reduction in CPU and perhaps
disk I/O then Spotlight is not impacting you.

00:43:07.429 --> 00:43:10.329
In that case, turn it back on.

00:43:10.329 --> 00:43:14.190
That's not a performance problem in that case.

00:43:14.190 --> 00:43:18.300
In addition, if it turns out that
that was a performance problem;

00:43:18.300 --> 00:43:24.240
one thing that you can do is to
perhaps by volume disable Spotlight.

00:43:24.239 --> 00:43:30.489
Perhaps you can identify that certain volumes
it's less critical, they have high write activity,

00:43:30.489 --> 00:43:34.099
and you can simply disable it on that volume.

00:43:34.099 --> 00:43:45.369
In addition, Spotlight is able to be turned on and off
per file name or per-node as I phrase it in the slide.

00:43:45.369 --> 00:43:55.679
So basically if you have a directory fu and you create a
fu.noindex right beside it, Spotlight will leave it alone.

00:43:55.679 --> 00:44:03.299
So if you have specific directories that don't
need to be indexed that would not be value add,

00:44:03.300 --> 00:44:07.690
that would be the way to let Spotlight know.

00:44:07.690 --> 00:44:16.420
OK so for more information Mark Malone is our Evangelist.

00:44:16.420 --> 00:44:23.590
Again the KBase article is excellent as well as DTrace.