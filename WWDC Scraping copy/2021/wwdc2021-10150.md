# Wwdc2021 10150

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Explore hybrid rendering with Metal ray tracingDiscover how you can combine ray tracing with your rasterization engine to implement simplified graphics techniques and elevate visuals in your app or game. We'll explore how you can use natural algorithms to accurately simulate the interplays of light, and learn how to take advantage of the latest tools in Xcode to capture, inspect, and debug your ray-traced scenes.ResourcesAccelerating ray tracing using MetalApplying realistic material and lighting effects to entitiesManaging groups of resources with argument buffersMetalMetal Shading Language SpecificationRendering reflections in real time using ray tracingHD VideoSD VideoRelated VideosWWDC23Your guide to Metal ray tracingWWDC22Go bindless with Metal 3WWDC21Discover Metal debugging, profiling, and asset creation toolsEnhance your app with Metal ray tracingExplore bindless rendering in MetalOptimize high-end games for Apple GPUs

Discover how you can combine ray tracing with your rasterization engine to implement simplified graphics techniques and elevate visuals in your app or game. We'll explore how you can use natural algorithms to accurately simulate the interplays of light, and learn how to take advantage of the latest tools in Xcode to capture, inspect, and debug your ray-traced scenes.

Accelerating ray tracing using Metal

Applying realistic material and lighting effects to entities

Managing groups of resources with argument buffers

Metal

Metal Shading Language Specification

Rendering reflections in real time using ray tracing

HD VideoSD Video

HD Video

SD Video

Your guide to Metal ray tracing

Go bindless with Metal 3

Discover Metal debugging, profiling, and asset creation tools

Enhance your app with Metal ray tracing

Explore bindless rendering in Metal

Optimize high-end games for Apple GPUs

Search this video…♪ Bass music playing ♪♪Ali de Jong: Welcome to WWDC 2021.My name is Ali de Jong,and I'm a GPU software engineer at Apple.And today, along with my colleague David Núñez Rubio,we'll explore hybrid renderingwith Metal ray tracing.We'll start by showing you some improvementsthat ray tracing can bring to your visuals;then discuss how to incorporate a ray tracing passinto rasterization,using a technique called "hybrid rendering;"then David will walk us through the new toolsto help you implement ray tracing.Let's start by taking a lookat some great use cases for ray tracing.Games and movies are in the constant pursuitof ever-increasing realism,and for many years,the approach to graphics has been by means of rasterization.Rasterization is great at producing beautiful imagesat real-time rates.However, there are limitations to what we can achieve.Ray tracing is a mechanismthat allows us to query the world from shaders,opening the door to new, exciting techniques.And by combining it with rasterization,we can greatly improve our visuals.Let's take a look at a few examples.One area that's always been problematic for rasterizationis reflections.This is because when we're shading a rasterized pixel,we have no context of the rest of the scenefor accurate reflectionsand we have to do extra work to generate that information.Ray tracing allows us to trace arrayfrom the pixel being shadedand discover what's out there in the world.Even better, we can apply this process recursivelyto apply correct shadowsand even reflections in reflections.Another area where ray tracing excels is shadows.Notice with rasterization, the shadows' general blurrinessand the aliasing caused by a shadow map resolutionon the curved surfaces of the moped.Ray-traced shadows are sharper and address aliasing issueswithout the need of artificial parameterssuch as a shadow bias.Soft shadows can also be approximated more accurately.We can naturally produce shadows that are harder or softer,depending on the proximity of the occluding objectto the shaded point.With rasterization,we would need to rely on filtering the shadow mapat sampling time.But with ray tracing,we can simply trace rays in a cone to get this result.Lastly, another areawhere ray tracing can elevate our visuals is transparency.This is traditionally very hard to handle accuratelyfor rasterization techniques.In this image,note how the sunlight is coming through the window,yet the opaque letters on the glass produce no shadow.Traditional shadow-mapping techniquesoften have problems with transparent objects.With ray tracing,we can create a custom intersection functionfor transparent materials.With this, we can definewhich rays are able to pass through the materialand which ones don't,naturally producing projected shadowslike the letters on the head of the bust.And of course, all the shadows look sharper overall.So why is it that ray tracing is able to improve our visualsso dramatically?To understand this, let's take a lookat how the traditional rasterization process works.In the rasterization process,meshes are sent down to Metal to be rendered.They are placed in the world and in front of the cameraby a vertex shader,and those primitives are placed onto pixels --or fragments -- by the rasterizer.These pixels are then shaded by a fragment shader,and the result is blended onto the output image.As you know, each pixel can be shaded independently,operating in parallel,which is what makes GPUs so good at the rasterization process.The tradeoff, however,is at the time we're applying our shading,we've completely lost the contextof the rest of our sceneand we don't know what objects might be surroundingthe point associated with this pixel.Advanced game engines make up for this situationby adding extra render passesthat generate intermediate information.The fragment shader can then leverage that datato approximate the details of the geometric contextthe point is in.Let's take a look at how this works in a bit more detail.For this technique,you rasterize geometric information about the sceneto intermediate textures instead of directly to the screen.This can be things like albedo, depth, or normals.This is commonly referred to as a Geometry Buffer passor G-Buffer pass for short.The intermediate textures are used as inputfor light approximation passes that use smart tricksto approximate how the light would interact with the objectsin the scene.Some examples are screen-space ambient occlusionand screen-space reflection.In the last step, our intermediate attachmentsare often denoised or blurred slightlyto make for a smoother image,and everything is combined togetherto produce the final image.While these sometimes elaborate techniquescan help improve the image,they're still just approximations.On the other hand,ray tracing takes a completely different approachthat enables more accurate visualsand simplified visual techniques.Instead of processing meshes one at a time,in ray tracing, we build an acceleration structurethat encompasses the whole scene.Once we have that,we can have the GPU trace rays from a pointtoward a given direction and find intersections.This gives us accessto all the contextual scene information.Since ray tracing models ray interactions,it has applications beyond rendering, too.It can be used for audio and physics simulation,collision detection, or AI and pathfinding.Since ray tracing is such a powerful technique,we'd like to bring together ray tracing and rasterizationto get the unique benefits of each,and we can do so through a techniquecalled "hybrid rendering."Now let's look at how to create a hybrid rendered frameand some use cases for this technique.If we start from our rasterized frame diagram,we can use ray tracing to replace some or allof our light approximation passes.We still rasterize our G-Buffer --it plays the role of our primary rays --and then we use ray tracingto more realistically simulate the properties of lightby querying into the rest of the scene.We still denoise and do a light composition pass,but our results are much more accurate to the scene data.This frame architecture provides a good foundationto explore a number of hybrid rendering techniques.Let's take a look at how we can encode a frame like thisusing Metal.We start with filling out our G-Buffer.To do so, we create a render passand fill out a G-Bufferand set its textures as the attachments for our pass.We make sure our images are stored to memoryso the rendered contents are available to subsequent passes.We start our pass, encode our rendering,and end the render pass.Next, we'll add a ray-tracing compute dispatch to this.So after we create the intermediate textures,let's encode our ray-tracing pass.We create the compute pass from the same command bufferand make sure to set the G-Buffer textures as inputs.By default, Metal will track write-read dependencies for you,so you're free to focus on your algorithmwithout being too concerned by synchronization.Since this is compute, we set our output texturesto write the results of our ray-tracing work.We set the PipelineState object for our ray-tracing technique.Each thread in the compute shaderwill calculate the ray-tracing result for a pixel or region.Finally, we dispatch our 2D grid and end this pass.After this pass is encoded,we can now continue to encode more worksuch as the light accumulation pass,or we can submit the command buffer nowso the GPU starts working on itwhile we encode the rest of the frame.Since we encoded our work in two passes,this requires saving our intermediate render attachmentsto system memory for the passes to communicate with each other.This works, but on Apple Silicon and iOS devicesthere's an opportunity to make this even better.On Apple GPUs, the hardware utilizes tile memoryto hold our pixel data as we work on it.At the end of the pass,this tile memory is fleshed out to system memoryand must be reloaded at the beginning of the next pass.Ideally, though, we would have the compute passeswork directly on tile memory,avoiding the round trips to system memory.I'm excited to sharethis year we've added the ability to do thatby dispatching ray-tracing work from render pipelines.This allows mixing render and compute via tile shadersin a single pass to leverage on-tile memory for ray tracing.This will reduce bandwidth use, memory consumption,and help your users' devices run cooler.Please make sure to reviewour 2019 "Modern rendering with Metal" sessionto learn how to efficiently mix render and compute,so you can apply that for ray tracing from render;as well as this year's"Enhance your app with Metal ray-tracing" sessionto learn about other improvementscoming to Metal ray tracing this year.Now that we know how to encode a hybrid rendering workload,let's review some techniquesthat can be improved with ray tracing.We'll focus on shadows, ambient occlusion,and reflections.Let's start with shadows.Shadows help convey the proximity of objectsto each other within the scene.This is a challenge for rasterization, though,because we lose the context of the sceneat the time of shading.Shadow mapping can help supplementthis lack of informationbut requires extra rendering from each light's point of view.This rasterization technique starts by rendering the scenefrom every light's perspective.This produces a series of depth mapsthat need to be storedalongside each light's transformation matrix.Then we render from the main camera's perspective.To shade each pixel, we need to convert the pointto the light's coordinates.We sample the depth coming from the depth mapand ultimately compare these depth valuesto determine if we're in light or shadowfor each light source.There's a couple of drawbacks with this technique.First, we'll have to render the scenefrom the light's perspective for each light.This means processing the scene multiple times.Second, the shadow maps have a predetermined resolution,which means our shadows will be subject to aliasing;and worse, we won't have informationfor pixels that didn't fit in the image.Let's compare this to ray-traced shadows.To compute shadows with ray tracing,we can simply trace a ray from a pointtoward the direction of the light sourceand determine if any object is blocking its path.If we don't find anything,that means the point should take this light sourceinto consideration for shading.In the case an object is blocking the path,we just exclude that light source's contributionin the lighting equation.Notice how this produces a natural shadowcorresponding to the silhouette of the occluding object.Even better, we're no longer limited to the informationstored in a depth map.We can determine shadowsfor points outside the light's frustum or camera's view.Let's see how our shadow technique is simplifiedwith ray tracing.We start by rendering from the main camera.Next, we take the acceleration structureand the depth map rendered from the camera's positionand feed it into our ray-tracing kernel.Calculate the pixel position,and then we simply trace a ray in the light's directionFrom this, we determine if the point is lit or in shadow,depending on whether an intersection was foundwith an occluding object.This process produces a shadow texturethat we can then combine with our render pass resultsto get the final image.Let's look at how to code a Metal shader to do this.In our shader code,we start by calculating the positioneach thread will process from the depth and the thread_id.We create our shadow ray from the calculated positionand set it up to trace in the light's direction.For most light types, like point lights,spotlights, and area lights,we set the min and max to trace all the way from the pointto the light source.For directional shadows,we may want to set the max to infinity.Additionally, if we decide to implement cone ray tracingfor softer shadows,this is a great place to add jitter to our shadowRay.We then create an intersector object.If we find any one intersection, that means we're in shadow,so we configure the intersector to accept any intersection.Finally, we intersect against the acceleration structure.Based on that intersection result,we write whether the point is lit or not,which creates a shadow texturethat's more accurate to the scene.When that shadow texture is applied,you can see we get much more realistic shadowsand get rid of the aliasing.With ray tracing,determining shadows becomes a very natural technique.We just trace a ray to find if somethingoccludes the light source for that point or not.There is no longer a need to have intermediate depth maps,and we can avoid having multiple extra render passesfor each light.This technique is easy to implement into a deferredor forward renderer, as it only depends on depth.And finally, it allows for custom intersection functionsfor translucent materials.Next, let's take a look at ambient occlusion.Conceptually, a point surrounded by geometry is less likelyto receive a large amount of ambient light.Ambient occlusion consists in muting the ambient lightreceived at a point based on how busy its neighborhood is,which naturally darkens crevices,giving the final image more depth.Rasterization techniques to achieve thisdepend on sampling the depth and normalsin the neighborhood of the point,to determine if there are objects surroundingand potentially occluding it.Based on how many nearby objects are found,we calculate an attenuation factorto mute the ambient lightand create a texture to apply to our image.Relying on screen-space information like depth bufferand surface normals, however, is missing informationfor nonvisible occludersand objects outside the border of the image.With ray tracing,instead of relying on screen-space information,we can rely on actual geometric data of our scene.The idea is for every pixel to shade,we generate random rays in a hemisphereand search for intersections against objects.If any intersections are found, we take it into considerationfor our ambient occlusion factor.We start again with the acceleration structure.For this technique, we require normal data as well as depth,so we collect those in our G-Buffer pass.The depth and normals are usedto generate the random rays in the hemisphere.Next, we trace raysand calculate the attenuation factor.This produces an image where crevices are naturally darkened,creating the effect.Let's take a look at a Metal shaderfor ambient occlusion.First, we generate the random rays.In this case, we take a cosineWeightedRayalong the normal in each thread.We set the max_distance to a small number,as we're only interested in a small neighborhood.Next, we create our intersectorand intersect the acceleration structure.Depending on the result,we accumulate into our attenuation factor.Here's a side-by-side comparison.And we can immediately seehow much better the ray-traced approach looks.I want to highlight a few places that really show the limitationsof a screen-space effect.Here is an example where the neighborhoodis misrepresented due to limited screen-space information.This is because the actual geometryis almost perpendicular to the camera and, therefore,not in the depth buffer at this angle.The same problem occurs across the image,in particular, under the moped.From this angle, the bottom of the mopedis missing from the depth buffer.So the screen-space techniquecompletely misses the attenuation.The ray-traced version, on the other hand,correctly discovers the intersectionsagainst the bottom of the moped for the floor pixels.And here's a great exampleof the limitations around the screen border.The occluding geometry is offscreen,so its contribution is lost in the screen-space technique,but accounted for in ray tracing.As we can see, hybrid rendering providesa significant quality improvementby using the actual geometry of the scene,freeing the technique from limitationsin screen-space information.And finally, let's take a look at reflections.Reflections have traditionally beenvery difficult for rasterization.Reflection probes is a technique that works wellbut is limited in resolution, requires filtering,and struggles with dynamic geometry.Screen-space reflection techniquesare limited by screen-space information.Reflection probes are a solution that requiresstrategically placing cameras along the entire sceneto capture surrounding color information.To use reflection probes, cube maps are capturedfrom different locations in the scene.This is essentially a rendering of the scene in six directionsfrom the same point.When a pixel is shaded, you calculate the relationto the probes and sample the cube mapsto produce the reflected shading.For realistic results,usually many probes need to be scattered across the scene.And as dynamic objects move across the scene,shaders need to sample from more than one cube mapand manually interpolate reflected colors.The cube maps also need to be prefilteredto accurately represent irradianceand are limited in resolution.Another rasterization technique, screen-space reflection,avoids some of these problems by basing its reflectionson pixels already on the framebuffer.The fragment shader uses the normalsto incrementally march outwardsand check the depth map for potential nearby objects.If we find something,we sample the color directly from the frame bufferand shade it onto the output image.It does suffer, however, from the screen-space limitationswe discussed earlier.Notice in this moped examplehow only part of the surface can get an accurate reflection,corresponding to the floor tiles present in the framebuffer.The rest of the scene is missing.Worse, the lower portion behind the fender, marked in yellow,is missing information; we have no way of knowingwhat the surface facing away from the camera would look like.The ray marching can also get computationally expensive.Ray-traced reflections, however,helps us overcome both sets of problems,as we can rely on the true scene informationin the acceleration structure.Let's take a look at how a perfect mirror would work.First, we take the incident ray from the camera's positionto the point.Then, we reflect this pointon the normal associated with the point.This provides us with a directionwe can trace a ray towards,and find any reflected objects.For this, we provide our reflection ray-tracing kernelwith the normals and depth of the G-Buffer.This ray-tracing kernel calculates the view vector,from the camera to each point,reflects this vector,and traces a ray in that direction from the point.Finally, for accurate reflections,we can shade the intersectionfound directly in the ray-tracing kernel.Let's take a look at coding this shader.Once again, we start from the point's depth,and reconstruct its position.This time, we want the position to be in world space.So in our calculatePosition function,we'll need to multiply the inverse of the view matrix.Then, we calculate our reflected incident vector over the normaland create a ray in that direction.Next, we create our intersector and trace our reflectedRay.If we hit an object,we now shade that point to produce the reflection.If the intersection missed all objects,we can just sample a skybox and return its colorto simulate a reflection that's showing the sky.Note that the shading is performeddirectly in the compute kernel for this technique.Let's compare reflection probes to ray-traced reflections.The image on the right used hybrid rendering,and we can see the details of the floor tilesmuch more clearly.The buildings are present, and we can even see shadowsreflected on the front panel of the moped.Reflections are a natural fit for ray tracing.It nicely handles mirror-like reflectionsand rough reflections.Those can be achieved by tracing multiple raysalong a cone and filtering the results.Because they rely on perfect informationcoming from the acceleration structure,ray-traced reflections are free from screen-space artifactsand can handle both static and dynamic geometryin the scene.Now, one important detail:we mentioned that for reflections,we need to shade the point directly in the compute kernel.Some techniques like this one or global illuminationrequire accessing vertex data and Metal resourcesfrom the compute kernel directly.For these cases we need to make surethe GPU has access to the data that it needsto apply our shading equations.This is achieved with a bindless binding modelwhich in Metal is represented as argument buffers.Please make sure to check out this year's"Bindless rendering in Metal" talk for more detail.We just saw how hybrid rendering can be put into practicewith several different techniques.This leads to more natural algorithmsthat also have the advantage of producing more accurate results.In some cases, when we compareto the traditional rasterization techniques,we see that we can remove render passesand save memory and bandwidth in some cases.With the addition of ray tracing from render,we can even keep our entire work on chip.Ray-tracing adoption is a big task,and we have excellent new toolingto assist you in the processof bringing these techniques to your engine.This year, we're introducing toolsthat enable you to capture ray-tracing work,inspect acceleration structures,and inspect visible and intersection functions.Now David will give us a tour of these new tools.David Núñez Rubio: Thanks, Ali. My name is David Núñez Rubio,and I am a GPU software engineer.Last year, we introduced ray-tracing support in Metal.However, developing complex application can be challenging.Fortunately, Metal Debugger is here to help you.This year, we introduced ray-tracing supportin Metal Debugger.Thanks to the adoption of hybrid rendering,our demo is looking better than ever.Ray-traced soft shadows, reflections, ambient occlusion;the results are amazing.During development of the demo, we hit some issues.This is how tools can help you resolving these problems.In this early version of the demo,ray-traced shadows have already been implemented.But if you look carefully, you'll notice missing shadowsfrom the tree leaves on the ground.It is more obvious if we compare with the reference version.See reference versus ray traced.Let's jump into Xcode and take a captureto see how the tools can help us debugging this issue.We need to press the Metal buttonand click on Capture.Since this is a static problem, we just need a single frame.In the debugger, API calls are organized in the left sideon the debug navigator.Let's unfold the offscreen command bufferto look for our shadow encoding.I have labeled my compute command encoderas "Raytrace Shadows."It is a good practice to label your Metal objectsso you can easily find them in the Metal Debugger.The thumbnail also gives us a hint that,indeed, this is the encoder we are looking for.We can now click on the dispatch Threadgroups API callto show band resources.This is a list of all the objectsassociated with our current kernel dispatch.And here, we can see an acceleration of structure,which we have conveniently labeled as well.Our kernel uses an acceleration structureto cast rays.This is commonly implemented as a bounding volume hierarchyor BVH, which is a tree-like data structurerepresenting the 3D world that rays will intersect.Now, double-click to open the acceleration structure viewer.This is a great new tool built into the Metal Debugger.Let me give you an overview of how it is organized.On the right side, we have the 3D viewwhere we have a ray traced visualization of our 3D scene,including any custom geometry or intersection functions.This works great with custom geometry such as hairor when using alpha testing.You can use familiar controlsto move the camera and look around.And here's a tip:press Option key while scrolling to zoom in and out.We have built some great visualization toolsto better understand our scene.Let's click on the highlighted menuto see the different modes available.For instance, we can visualize bounding volume traversals.This is a heat map showinghow many nodes a single ray will need to traversebefore hitting a surface.Darker colors mean more nodes need to be traversedand a slower intersection test.We can also color-code our scenebased on acceleration structures...geometries...instances...or intersection functions.Now that we are a bit more familiar with the tool,we can go back to our original problem.Thanks to the 3D view,we have confirmed that our geometry is there.So there must be something else.On the left side, there is the navigator area.Here we can seeour top- and bottom-level acceleration structures.We can unfold any acceleration structuresto see the list of geometries it is built from.We can unfold again to see their propertiessuch as opacity or primitive count.We can also see the list of instancesof this acceleration structure.Let's click on the tree leaves to reveal their instanceon the navigator and inspect its properties.The matrix looks correct, and there are no flags set,but it seems that the mask is missing something.In this demo, we are using intersection masks.We use the lowest bit of the maskto flag objects casting shadows.Our intersector then will test this maskusing a bitwise and operationand reject the intersection if it fails.We can visualize this behavior directly in the 3D view.We need to open the intersector hints menu.Here we can configure ray traversal optionsfor visualization.We can change culling operations,disable custom intersections,or change the intersector’s mask.By default, it will intersect everything.Let's change it to the value that we are using for shadows.This will show us an exact visualization of our scenewhen using our shadow mask.And indeed, we have confirmedthat our tree leaves are now missing.Once we have identified the problem,we need to go back to our sourceand make sure we are setting the right mask value.This is how shadows looked before.And this is how they look after fixing the mask value.This is an example of the workflowsthat can help you debug your ray-tracing applications.If you want to learn more about tools,make sure you check out this year's"Discover Metal debugging, profiling,and asset creation tools" WWDC session.In this session, we have reviewedhow ray tracing can elevate your visuals.Hybrid rendering is the combinationof rasterization and ray tracing.This allows replacing light approximation techniqueswith more accurate ones that also happen to be simpler.We also saw the new tools to aid you in the processof adopting ray tracing in your engine.We have only scratched the surfaceon what new possibilities are available to youby combining rasterization and ray tracing.We can't wait to seehow you put these technologies in practiceto develop the new innovative graphics techniquesof the future.Thank you and enjoy the rest of WWDC.♪

♪ Bass music playing ♪♪Ali de Jong: Welcome to WWDC 2021.My name is Ali de Jong,and I'm a GPU software engineer at Apple.And today, along with my colleague David Núñez Rubio,we'll explore hybrid renderingwith Metal ray tracing.We'll start by showing you some improvementsthat ray tracing can bring to your visuals;then discuss how to incorporate a ray tracing passinto rasterization,using a technique called "hybrid rendering;"then David will walk us through the new toolsto help you implement ray tracing.Let's start by taking a lookat some great use cases for ray tracing.Games and movies are in the constant pursuitof ever-increasing realism,and for many years,the approach to graphics has been by means of rasterization.Rasterization is great at producing beautiful imagesat real-time rates.However, there are limitations to what we can achieve.Ray tracing is a mechanismthat allows us to query the world from shaders,opening the door to new, exciting techniques.And by combining it with rasterization,we can greatly improve our visuals.Let's take a look at a few examples.One area that's always been problematic for rasterizationis reflections.This is because when we're shading a rasterized pixel,we have no context of the rest of the scenefor accurate reflectionsand we have to do extra work to generate that information.Ray tracing allows us to trace arrayfrom the pixel being shadedand discover what's out there in the world.Even better, we can apply this process recursivelyto apply correct shadowsand even reflections in reflections.Another area where ray tracing excels is shadows.Notice with rasterization, the shadows' general blurrinessand the aliasing caused by a shadow map resolutionon the curved surfaces of the moped.Ray-traced shadows are sharper and address aliasing issueswithout the need of artificial parameterssuch as a shadow bias.Soft shadows can also be approximated more accurately.We can naturally produce shadows that are harder or softer,depending on the proximity of the occluding objectto the shaded point.With rasterization,we would need to rely on filtering the shadow mapat sampling time.But with ray tracing,we can simply trace rays in a cone to get this result.Lastly, another areawhere ray tracing can elevate our visuals is transparency.This is traditionally very hard to handle accuratelyfor rasterization techniques.In this image,note how the sunlight is coming through the window,yet the opaque letters on the glass produce no shadow.Traditional shadow-mapping techniquesoften have problems with transparent objects.With ray tracing,we can create a custom intersection functionfor transparent materials.With this, we can definewhich rays are able to pass through the materialand which ones don't,naturally producing projected shadowslike the letters on the head of the bust.And of course, all the shadows look sharper overall.So why is it that ray tracing is able to improve our visualsso dramatically?To understand this, let's take a lookat how the traditional rasterization process works.In the rasterization process,meshes are sent down to Metal to be rendered.They are placed in the world and in front of the cameraby a vertex shader,and those primitives are placed onto pixels --or fragments -- by the rasterizer.These pixels are then shaded by a fragment shader,and the result is blended onto the output image.As you know, each pixel can be shaded independently,operating in parallel,which is what makes GPUs so good at the rasterization process.The tradeoff, however,is at the time we're applying our shading,we've completely lost the contextof the rest of our sceneand we don't know what objects might be surroundingthe point associated with this pixel.Advanced game engines make up for this situationby adding extra render passesthat generate intermediate information.The fragment shader can then leverage that datato approximate the details of the geometric contextthe point is in.Let's take a look at how this works in a bit more detail.For this technique,you rasterize geometric information about the sceneto intermediate textures instead of directly to the screen.This can be things like albedo, depth, or normals.This is commonly referred to as a Geometry Buffer passor G-Buffer pass for short.The intermediate textures are used as inputfor light approximation passes that use smart tricksto approximate how the light would interact with the objectsin the scene.Some examples are screen-space ambient occlusionand screen-space reflection.In the last step, our intermediate attachmentsare often denoised or blurred slightlyto make for a smoother image,and everything is combined togetherto produce the final image.While these sometimes elaborate techniquescan help improve the image,they're still just approximations.On the other hand,ray tracing takes a completely different approachthat enables more accurate visualsand simplified visual techniques.Instead of processing meshes one at a time,in ray tracing, we build an acceleration structurethat encompasses the whole scene.Once we have that,we can have the GPU trace rays from a pointtoward a given direction and find intersections.This gives us accessto all the contextual scene information.Since ray tracing models ray interactions,it has applications beyond rendering, too.It can be used for audio and physics simulation,collision detection, or AI and pathfinding.Since ray tracing is such a powerful technique,we'd like to bring together ray tracing and rasterizationto get the unique benefits of each,and we can do so through a techniquecalled "hybrid rendering."Now let's look at how to create a hybrid rendered frameand some use cases for this technique.If we start from our rasterized frame diagram,we can use ray tracing to replace some or allof our light approximation passes.We still rasterize our G-Buffer --it plays the role of our primary rays --and then we use ray tracingto more realistically simulate the properties of lightby querying into the rest of the scene.We still denoise and do a light composition pass,but our results are much more accurate to the scene data.This frame architecture provides a good foundationto explore a number of hybrid rendering techniques.Let's take a look at how we can encode a frame like thisusing Metal.We start with filling out our G-Buffer.To do so, we create a render passand fill out a G-Bufferand set its textures as the attachments for our pass.We make sure our images are stored to memoryso the rendered contents are available to subsequent passes.We start our pass, encode our rendering,and end the render pass.Next, we'll add a ray-tracing compute dispatch to this.So after we create the intermediate textures,let's encode our ray-tracing pass.We create the compute pass from the same command bufferand make sure to set the G-Buffer textures as inputs.By default, Metal will track write-read dependencies for you,so you're free to focus on your algorithmwithout being too concerned by synchronization.Since this is compute, we set our output texturesto write the results of our ray-tracing work.We set the PipelineState object for our ray-tracing technique.Each thread in the compute shaderwill calculate the ray-tracing result for a pixel or region.Finally, we dispatch our 2D grid and end this pass.After this pass is encoded,we can now continue to encode more worksuch as the light accumulation pass,or we can submit the command buffer nowso the GPU starts working on itwhile we encode the rest of the frame.Since we encoded our work in two passes,this requires saving our intermediate render attachmentsto system memory for the passes to communicate with each other.This works, but on Apple Silicon and iOS devicesthere's an opportunity to make this even better.On Apple GPUs, the hardware utilizes tile memoryto hold our pixel data as we work on it.At the end of the pass,this tile memory is fleshed out to system memoryand must be reloaded at the beginning of the next pass.Ideally, though, we would have the compute passeswork directly on tile memory,avoiding the round trips to system memory.I'm excited to sharethis year we've added the ability to do thatby dispatching ray-tracing work from render pipelines.This allows mixing render and compute via tile shadersin a single pass to leverage on-tile memory for ray tracing.This will reduce bandwidth use, memory consumption,and help your users' devices run cooler.Please make sure to reviewour 2019 "Modern rendering with Metal" sessionto learn how to efficiently mix render and compute,so you can apply that for ray tracing from render;as well as this year's"Enhance your app with Metal ray-tracing" sessionto learn about other improvementscoming to Metal ray tracing this year.Now that we know how to encode a hybrid rendering workload,let's review some techniquesthat can be improved with ray tracing.We'll focus on shadows, ambient occlusion,and reflections.Let's start with shadows.Shadows help convey the proximity of objectsto each other within the scene.This is a challenge for rasterization, though,because we lose the context of the sceneat the time of shading.Shadow mapping can help supplementthis lack of informationbut requires extra rendering from each light's point of view.This rasterization technique starts by rendering the scenefrom every light's perspective.This produces a series of depth mapsthat need to be storedalongside each light's transformation matrix.Then we render from the main camera's perspective.To shade each pixel, we need to convert the pointto the light's coordinates.We sample the depth coming from the depth mapand ultimately compare these depth valuesto determine if we're in light or shadowfor each light source.There's a couple of drawbacks with this technique.First, we'll have to render the scenefrom the light's perspective for each light.This means processing the scene multiple times.Second, the shadow maps have a predetermined resolution,which means our shadows will be subject to aliasing;and worse, we won't have informationfor pixels that didn't fit in the image.Let's compare this to ray-traced shadows.To compute shadows with ray tracing,we can simply trace a ray from a pointtoward the direction of the light sourceand determine if any object is blocking its path.If we don't find anything,that means the point should take this light sourceinto consideration for shading.In the case an object is blocking the path,we just exclude that light source's contributionin the lighting equation.Notice how this produces a natural shadowcorresponding to the silhouette of the occluding object.Even better, we're no longer limited to the informationstored in a depth map.We can determine shadowsfor points outside the light's frustum or camera's view.Let's see how our shadow technique is simplifiedwith ray tracing.We start by rendering from the main camera.Next, we take the acceleration structureand the depth map rendered from the camera's positionand feed it into our ray-tracing kernel.Calculate the pixel position,and then we simply trace a ray in the light's directionFrom this, we determine if the point is lit or in shadow,depending on whether an intersection was foundwith an occluding object.This process produces a shadow texturethat we can then combine with our render pass resultsto get the final image.Let's look at how to code a Metal shader to do this.In our shader code,we start by calculating the positioneach thread will process from the depth and the thread_id.We create our shadow ray from the calculated positionand set it up to trace in the light's direction.For most light types, like point lights,spotlights, and area lights,we set the min and max to trace all the way from the pointto the light source.For directional shadows,we may want to set the max to infinity.Additionally, if we decide to implement cone ray tracingfor softer shadows,this is a great place to add jitter to our shadowRay.We then create an intersector object.If we find any one intersection, that means we're in shadow,so we configure the intersector to accept any intersection.Finally, we intersect against the acceleration structure.Based on that intersection result,we write whether the point is lit or not,which creates a shadow texturethat's more accurate to the scene.When that shadow texture is applied,you can see we get much more realistic shadowsand get rid of the aliasing.With ray tracing,determining shadows becomes a very natural technique.We just trace a ray to find if somethingoccludes the light source for that point or not.There is no longer a need to have intermediate depth maps,and we can avoid having multiple extra render passesfor each light.This technique is easy to implement into a deferredor forward renderer, as it only depends on depth.And finally, it allows for custom intersection functionsfor translucent materials.Next, let's take a look at ambient occlusion.Conceptually, a point surrounded by geometry is less likelyto receive a large amount of ambient light.Ambient occlusion consists in muting the ambient lightreceived at a point based on how busy its neighborhood is,which naturally darkens crevices,giving the final image more depth.Rasterization techniques to achieve thisdepend on sampling the depth and normalsin the neighborhood of the point,to determine if there are objects surroundingand potentially occluding it.Based on how many nearby objects are found,we calculate an attenuation factorto mute the ambient lightand create a texture to apply to our image.Relying on screen-space information like depth bufferand surface normals, however, is missing informationfor nonvisible occludersand objects outside the border of the image.With ray tracing,instead of relying on screen-space information,we can rely on actual geometric data of our scene.The idea is for every pixel to shade,we generate random rays in a hemisphereand search for intersections against objects.If any intersections are found, we take it into considerationfor our ambient occlusion factor.We start again with the acceleration structure.For this technique, we require normal data as well as depth,so we collect those in our G-Buffer pass.The depth and normals are usedto generate the random rays in the hemisphere.Next, we trace raysand calculate the attenuation factor.This produces an image where crevices are naturally darkened,creating the effect.Let's take a look at a Metal shaderfor ambient occlusion.First, we generate the random rays.In this case, we take a cosineWeightedRayalong the normal in each thread.We set the max_distance to a small number,as we're only interested in a small neighborhood.Next, we create our intersectorand intersect the acceleration structure.Depending on the result,we accumulate into our attenuation factor.Here's a side-by-side comparison.And we can immediately seehow much better the ray-traced approach looks.I want to highlight a few places that really show the limitationsof a screen-space effect.Here is an example where the neighborhoodis misrepresented due to limited screen-space information.This is because the actual geometryis almost perpendicular to the camera and, therefore,not in the depth buffer at this angle.The same problem occurs across the image,in particular, under the moped.From this angle, the bottom of the mopedis missing from the depth buffer.So the screen-space techniquecompletely misses the attenuation.The ray-traced version, on the other hand,correctly discovers the intersectionsagainst the bottom of the moped for the floor pixels.And here's a great exampleof the limitations around the screen border.The occluding geometry is offscreen,so its contribution is lost in the screen-space technique,but accounted for in ray tracing.As we can see, hybrid rendering providesa significant quality improvementby using the actual geometry of the scene,freeing the technique from limitationsin screen-space information.And finally, let's take a look at reflections.Reflections have traditionally beenvery difficult for rasterization.Reflection probes is a technique that works wellbut is limited in resolution, requires filtering,and struggles with dynamic geometry.Screen-space reflection techniquesare limited by screen-space information.Reflection probes are a solution that requiresstrategically placing cameras along the entire sceneto capture surrounding color information.To use reflection probes, cube maps are capturedfrom different locations in the scene.This is essentially a rendering of the scene in six directionsfrom the same point.When a pixel is shaded, you calculate the relationto the probes and sample the cube mapsto produce the reflected shading.For realistic results,usually many probes need to be scattered across the scene.And as dynamic objects move across the scene,shaders need to sample from more than one cube mapand manually interpolate reflected colors.The cube maps also need to be prefilteredto accurately represent irradianceand are limited in resolution.Another rasterization technique, screen-space reflection,avoids some of these problems by basing its reflectionson pixels already on the framebuffer.The fragment shader uses the normalsto incrementally march outwardsand check the depth map for potential nearby objects.If we find something,we sample the color directly from the frame bufferand shade it onto the output image.It does suffer, however, from the screen-space limitationswe discussed earlier.Notice in this moped examplehow only part of the surface can get an accurate reflection,corresponding to the floor tiles present in the framebuffer.The rest of the scene is missing.Worse, the lower portion behind the fender, marked in yellow,is missing information; we have no way of knowingwhat the surface facing away from the camera would look like.The ray marching can also get computationally expensive.Ray-traced reflections, however,helps us overcome both sets of problems,as we can rely on the true scene informationin the acceleration structure.Let's take a look at how a perfect mirror would work.First, we take the incident ray from the camera's positionto the point.Then, we reflect this pointon the normal associated with the point.This provides us with a directionwe can trace a ray towards,and find any reflected objects.For this, we provide our reflection ray-tracing kernelwith the normals and depth of the G-Buffer.This ray-tracing kernel calculates the view vector,from the camera to each point,reflects this vector,and traces a ray in that direction from the point.Finally, for accurate reflections,we can shade the intersectionfound directly in the ray-tracing kernel.Let's take a look at coding this shader.Once again, we start from the point's depth,and reconstruct its position.This time, we want the position to be in world space.So in our calculatePosition function,we'll need to multiply the inverse of the view matrix.Then, we calculate our reflected incident vector over the normaland create a ray in that direction.Next, we create our intersector and trace our reflectedRay.If we hit an object,we now shade that point to produce the reflection.If the intersection missed all objects,we can just sample a skybox and return its colorto simulate a reflection that's showing the sky.Note that the shading is performeddirectly in the compute kernel for this technique.Let's compare reflection probes to ray-traced reflections.The image on the right used hybrid rendering,and we can see the details of the floor tilesmuch more clearly.The buildings are present, and we can even see shadowsreflected on the front panel of the moped.Reflections are a natural fit for ray tracing.It nicely handles mirror-like reflectionsand rough reflections.Those can be achieved by tracing multiple raysalong a cone and filtering the results.Because they rely on perfect informationcoming from the acceleration structure,ray-traced reflections are free from screen-space artifactsand can handle both static and dynamic geometryin the scene.Now, one important detail:we mentioned that for reflections,we need to shade the point directly in the compute kernel.Some techniques like this one or global illuminationrequire accessing vertex data and Metal resourcesfrom the compute kernel directly.For these cases we need to make surethe GPU has access to the data that it needsto apply our shading equations.This is achieved with a bindless binding modelwhich in Metal is represented as argument buffers.Please make sure to check out this year's"Bindless rendering in Metal" talk for more detail.We just saw how hybrid rendering can be put into practicewith several different techniques.This leads to more natural algorithmsthat also have the advantage of producing more accurate results.In some cases, when we compareto the traditional rasterization techniques,we see that we can remove render passesand save memory and bandwidth in some cases.With the addition of ray tracing from render,we can even keep our entire work on chip.Ray-tracing adoption is a big task,and we have excellent new toolingto assist you in the processof bringing these techniques to your engine.This year, we're introducing toolsthat enable you to capture ray-tracing work,inspect acceleration structures,and inspect visible and intersection functions.Now David will give us a tour of these new tools.David Núñez Rubio: Thanks, Ali. My name is David Núñez Rubio,and I am a GPU software engineer.Last year, we introduced ray-tracing support in Metal.However, developing complex application can be challenging.Fortunately, Metal Debugger is here to help you.This year, we introduced ray-tracing supportin Metal Debugger.Thanks to the adoption of hybrid rendering,our demo is looking better than ever.Ray-traced soft shadows, reflections, ambient occlusion;the results are amazing.During development of the demo, we hit some issues.This is how tools can help you resolving these problems.In this early version of the demo,ray-traced shadows have already been implemented.But if you look carefully, you'll notice missing shadowsfrom the tree leaves on the ground.It is more obvious if we compare with the reference version.See reference versus ray traced.Let's jump into Xcode and take a captureto see how the tools can help us debugging this issue.We need to press the Metal buttonand click on Capture.Since this is a static problem, we just need a single frame.In the debugger, API calls are organized in the left sideon the debug navigator.Let's unfold the offscreen command bufferto look for our shadow encoding.I have labeled my compute command encoderas "Raytrace Shadows."It is a good practice to label your Metal objectsso you can easily find them in the Metal Debugger.The thumbnail also gives us a hint that,indeed, this is the encoder we are looking for.We can now click on the dispatch Threadgroups API callto show band resources.This is a list of all the objectsassociated with our current kernel dispatch.And here, we can see an acceleration of structure,which we have conveniently labeled as well.Our kernel uses an acceleration structureto cast rays.This is commonly implemented as a bounding volume hierarchyor BVH, which is a tree-like data structurerepresenting the 3D world that rays will intersect.Now, double-click to open the acceleration structure viewer.

This is a great new tool built into the Metal Debugger.Let me give you an overview of how it is organized.On the right side, we have the 3D viewwhere we have a ray traced visualization of our 3D scene,including any custom geometry or intersection functions.This works great with custom geometry such as hairor when using alpha testing.You can use familiar controlsto move the camera and look around.And here's a tip:press Option key while scrolling to zoom in and out.We have built some great visualization toolsto better understand our scene.Let's click on the highlighted menuto see the different modes available.For instance, we can visualize bounding volume traversals.This is a heat map showinghow many nodes a single ray will need to traversebefore hitting a surface.Darker colors mean more nodes need to be traversedand a slower intersection test.We can also color-code our scenebased on acceleration structures...geometries...instances...or intersection functions.

Now that we are a bit more familiar with the tool,we can go back to our original problem.Thanks to the 3D view,we have confirmed that our geometry is there.So there must be something else.On the left side, there is the navigator area.Here we can seeour top- and bottom-level acceleration structures.We can unfold any acceleration structuresto see the list of geometries it is built from.We can unfold again to see their propertiessuch as opacity or primitive count.We can also see the list of instancesof this acceleration structure.Let's click on the tree leaves to reveal their instanceon the navigator and inspect its properties.The matrix looks correct, and there are no flags set,but it seems that the mask is missing something.In this demo, we are using intersection masks.We use the lowest bit of the maskto flag objects casting shadows.Our intersector then will test this maskusing a bitwise and operationand reject the intersection if it fails.We can visualize this behavior directly in the 3D view.We need to open the intersector hints menu.Here we can configure ray traversal optionsfor visualization.We can change culling operations,disable custom intersections,or change the intersector’s mask.By default, it will intersect everything.Let's change it to the value that we are using for shadows.This will show us an exact visualization of our scenewhen using our shadow mask.And indeed, we have confirmedthat our tree leaves are now missing.Once we have identified the problem,we need to go back to our sourceand make sure we are setting the right mask value.This is how shadows looked before.And this is how they look after fixing the mask value.This is an example of the workflowsthat can help you debug your ray-tracing applications.If you want to learn more about tools,make sure you check out this year's"Discover Metal debugging, profiling,and asset creation tools" WWDC session.In this session, we have reviewedhow ray tracing can elevate your visuals.Hybrid rendering is the combinationof rasterization and ray tracing.This allows replacing light approximation techniqueswith more accurate ones that also happen to be simpler.We also saw the new tools to aid you in the processof adopting ray tracing in your engine.We have only scratched the surfaceon what new possibilities are available to youby combining rasterization and ray tracing.We can't wait to seehow you put these technologies in practiceto develop the new innovative graphics techniquesof the future.Thank you and enjoy the rest of WWDC.♪

6:32 -Hybrid rendering in Metal 1

6:50 -Hybrid rendering in Metal 2

7:06 -Hybrid rendering in Metal 3

11:54 -Ray-traced shadow kernel

15:07 -Ray-traced ambient occlusion kernel

19:34 -Ray-traced reflection kernel

## Code Samples

```swift
// Create render pass



MTLRenderPassDescriptor
* gbufferPass = [
MTLRenderPassDescriptor
 new];
gbufferPass.depthAttachment.texture = gbuffer.depthTexture;
gbufferPass.depthAttachment.storeAction = 
MTLStoreActionStore
;

gbufferPass.colorAttachments[
0
].texture = gbuffer.normalTexture;
gbufferPass.colorAttachments[
0
].storeAction = 
MTLStoreActionStore
;
```

```swift
// Create render pass



id
< 
MTLRenderCommandEncoder
 > renderEncoder =
             [commandBuffer renderCommandEncoderWithDescriptor:gbufferPass];

encodeRenderScene( scene, renderEncoder );

[renderEncoder endEncoding];
```

```swift
// Dispatch ray tracing via compute



id
< 
MTLComputeCommandEncoder
 > compEncoder = [commandBuffer computeCommandEncoder];

[compEncoder setTexture:gbuffer.depthTexture atIndex:
0
];
[compEncoder setTexture:gbuffer.normalTexture atIndex:
1
];

[compEncoder setTexture:outReflectionMap atIndex:
2
];

[compEncoder setComputePipelineState:raytraceReflectionKernel];

encode2dDispatch( width, height, compEncoder );

[compEncoder endEncoding];
```

```swift
// Calculate shadow ray from G-Buffer


float3 p = calculatePosition( depth_texture, thread_id );
ray shadowRay( p, lightDirection, 
0.01
f, 
1.0
f );


// Trace for any intersections


intersector< triangle_data, instancing > shadowIntersector;
shadowIntersector.accept_any_intersection( 
true
 );

auto shadowIntersection = shadowIntersector.intersect( shadowRay, accel_structure );


// Point is in light if no intersections are found



if
 ( intersection.type == intersection_type::none ) {
   
// Point is illuminated by this light

}
```

```swift
// Generate ray in hemisphere


ray ray = cosineWeightedRay( thread_id );

ray.max_distance = 
0.5
f;


// Trace nearby intersections


intersector< triangle_data, instancing > i;

auto intersection = i.intersect( ray, accel_structure );


if
 ( intersection.type != intersection_type::none ) {
   
// Point is obscured by nearby geometry

}
```

```swift
// Calculate shadow ray from G-Buffer


float3 p 
=
 calculatePosition( depth_texture, thread_id );
float3 reflectedDir 
=
 reflect( p 
-
 cameraPosition, normal );

ray reflectedRay( p, reflectedDir, 
0
.01f, 
FLT_MAX
 );


// Trace for any intersections


intersector
<
 triangle_data, instancing 
>
 refIntersector;
auto intersection 
=
 refIntersector.intersect( reflectedRay, accel_structure );


// Shade depending on intersection



if
 ( intersection.type 
!=
 intersection_type::none ) {
   
// Reflected ray hit an object: perform shading

}

else
 {
   
// No intersection: draw skybox

}
```

