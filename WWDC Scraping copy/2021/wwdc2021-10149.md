# Wwdc2021 10149

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Enhance your app with Metal ray tracingAchieve photorealistic 3D scenes in your apps and games through ray tracing, a core part of the Metal graphics framework and Shading Language. We'll explore the latest improvements in implementing ray tracing and take you through upgrades to the production rendering process. Discover Metal APIs to help you create more detailed scenes, integrate natively-supported content with motion, and more.ResourcesAccelerating ray tracing using MetalApplying realistic material and lighting effects to entitiesManaging groups of resources with argument buffersMetalMetal Shading Language SpecificationRendering reflections in real time using ray tracingHD VideoSD VideoRelated VideosTech TalksExplore GPU advancements in M3 and A17 ProWWDC23Your guide to Metal ray tracingWWDC22Go bindless with Metal 3WWDC21Discover Metal debugging, profiling, and asset creation toolsExplore bindless rendering in MetalExplore hybrid rendering with Metal ray tracingOptimize high-end games for Apple GPUs

Achieve photorealistic 3D scenes in your apps and games through ray tracing, a core part of the Metal graphics framework and Shading Language. We'll explore the latest improvements in implementing ray tracing and take you through upgrades to the production rendering process. Discover Metal APIs to help you create more detailed scenes, integrate natively-supported content with motion, and more.

Accelerating ray tracing using Metal

Applying realistic material and lighting effects to entities

Managing groups of resources with argument buffers

Metal

Metal Shading Language Specification

Rendering reflections in real time using ray tracing

HD VideoSD Video

HD Video

SD Video

Explore GPU advancements in M3 and A17 Pro

Your guide to Metal ray tracing

Go bindless with Metal 3

Discover Metal debugging, profiling, and asset creation tools

Explore bindless rendering in Metal

Explore hybrid rendering with Metal ray tracing

Optimize high-end games for Apple GPUs

Search this video…♪ Bass music playing ♪♪Juan Rodriguez Cuellar: Hello, welcome to WWDC.My name is Juan Rodriguez Cuellar,and I'm a GPU compiler engineer at Apple.In this session, we’re going to talk aboutthe brand-new features we’ve added this yearto enhance our Metal ray tracing API.But first, let’s do a quick recap about ray tracing.Ray-tracing applications are based on tracing the pathsthat rays take as they interact with a scene.Ray tracing is applied in a lot of domains such as audio,physics simulation, and AI;but one of the main applications is photorealistic rendering.In rendering applications,ray tracing is used to model individual rays of light,which allow us to simulate effects such as reflections,soft shadows, and indirect lighting.That is just a general definition of ray tracing.Let’s talk about Metal’s approach to it.We start with a compute kernel.In our kernel, we generate rays,which are emitted into the scene.We then test those rays for intersectionsagainst the geometry in the scenewith an intersector and an acceleration structure.Each intersection pointrepresents light bouncing off a surface;how much light bounces and in what directiondetermines what the object looks like.We then compute a color for each intersectionand update the image.This process is called shadingand it can also generate additional rays,and those rays are also tested for intersection.We repeat the process as many times as we’d liketo simulate light bouncing around the scene.This year, we focused our new featuresaround three major areas.First, I will talk about how we’ve added ray-tracing supportto our render pipelines,which allows us to mix ray tracing with our rendering.Then I will introduce you to the new featuresthat focus on the usability and portability.These features will ease the use of Metal ray tracing API.Finally, I will cover the production rendering featureswe’ve added this yearthat will help you create more realistic content.Let's start with ray tracing from render pipelines.Let’s consider the basic caseof a render that has a single render pass.With our new support for ray tracing from render pipelines,this makes it super easy to add ray tracing into the render.However, without this support,to add ray tracing to this renderwith last year’s Metal ray tracing API,we need to add a compute pass.Let’s just start by adding it after renderingto augment the rendered image.Adding this extra compute passmeans writing more output to memoryfor the compute pass to use for ray tracing.Now, what if we wanted to use ray tracingin the middle of our render pass to calculate a valuesuch as shadowing per pixel?This means that we need to split our renderingand introduce a compute pass.Thinking more about what this means,we need to write out pixel positions and normalsto memory as inputs to the ray tracing,and then read the intersection results back --possibly several times.But with the new support for ray tracing from render stages,we never need to leave our render passand we just write our outputs to memory.Let's see how we use our new API.Preparing our render pipeline for ray tracingis similar to a compute pipeline.You start by building your acceleration structureand defining custom intersection functions.To support custom intersection,we need an intersection function tableand we need to fill it with intersection functions.This part has some differences compared to last year’s API.Let’s walk through how we can do that.Let’s consider some simple intersection functions.I have some functions here that will allow usto analytically intersect objects such as a sphere,a cone, or torus.When we create our pipeline, we add these functionsas linked functions that we may call.In this case, we are adding themto the fragment stage of the pipeline.To use the functions,we need to create an intersection function tablefrom the pipeline state and stage.Once we have the table, we can create function handlesfrom the pipeline state and stageand populate the table.Specifying the functions for the fragment stagereuses the linkedFunctions objectthat we introduced last year.Each stage has its own set of linkedFunctionson the render pipeline descriptor.Creating an intersection function tableis much the same as when done for the compute pipeline.The only change is the addition of the stage argument.To populate the table, we create the function handle.Again, the handle is specific to the stage,so we need to specify the stage when requesting the handle.Once we have the function handle,we just insert it in the function table.And that’s all you need to do to prepare your function tablesin render pipeline.Now we just have to use everything we have built so farto intersect.Actual use is straightforward.The accelerationStructure and the intersection function tableare both bound to buffer indices on the render encoder.The shaders can then use these resourcesto intersect rays with an intersectorthe same way you would in a compute kernel.More details about how to prepare your pipelinefor ray tracing can be found in last year’s presentation.In that talk, you will learn aboutbuilding acceleration structures,creating function tables,and using the intersector in the shading language.With ray-tracing support from render pipelines,we are opening the door for even more opportunitieslike adding ray tracing within a single render pass,mixing ray tracing with rasterizationin hybrid rendering,and taking advantage of optimizationssuch as tile functions on Apple Silicon.In fact, we will be soon adding ray tracing to our sample appthat we demoed during the "Modern Rendering in Metal"session at WWDC 2019.With ray tracing from render pipelines,we can update the code to use tile functionsto keep everything in tile memory.For more details about this, see this year’s"Explore hybrid rendering with Metal ray tracing" presentation.Next up, I want to introduce the new features we've addedthis year to improve the usability and portabilityof Metal ray tracing API.These features not only providesimpler use of Metal ray tracing,but they also provide portabilityfrom other ray tracing APIs.One of these new features is intersection query.With intersection query,we’re allowing you to have more controlover the intersection process.Intersection query is aimed toward simple use caseswhere intersector can create overhead.It is a new way of traversing the acceleration structurethat gives you the option of performingin-line custom intersection testing.Let's take a look at how we currently handlecustom intersection using last year’s intersector.Going back to the alpha test example from last year’sray tracing with Metal presentation,we demonstrated how alpha testing is usedto add a lot of geometric detail to the scene,as you see here in the chains and leaves.We also learned how easy it is to implement alpha testingby customizing an intersectorusing the triangle intersection function.The logic inside this triangle intersection functionis responsible for accepting or rejecting intersectionsas the ray traverses the acceleration structure.In this case, test logic will reject the first intersection,but it will accept the second intersectionsince an opaque surface has been intersected.Let’s see how intersection functions are used.When using Intersector, when you call intersect(),we start traversing the acceleration structureto find an intersectionand fill our intersection_result.Within the intersector,intersection function gets calledeach time a potential intersection is found.And intersections are then accepted or rejectedbased on intersection function logic.This is a great programming model using intersectorsince it is both performant and convenient,but it does require creating a new intersection functionand linking it to the pipeline.There might be caseswhere the logic inside the intersection functionis only a few lines of code,as it is the case for alpha test logic.This is the intersection functionthat contains the logic to do alpha testing.With intersection query, we can place this logic in-linewithout the need of this intersection function.Here is how.With intersection query,when you start the intersection process,your ray traverses the acceleration structure,and the query object contains the state of the traversaland the result.Each time the ray intersects a custom primitiveor a nonopaque triangle,control is returned to the shaderfor you to evaluate the intersection candidates.If the current candidate passes your custom intersection logic,you commit it to updatethe current committed intersectionand then continue the intersection process.On the other hand, if candidate fails custom intersection logic,you can just ignore it and continue.Let me show you the code to do alpha testingusing intersection query.First, you start the traversal.Note that we loop on nextto evaluate all candidate intersections.Second, you evaluate each candidatestarting by checking the candidate type.For alpha test example, you're interestedon intersections of triangle type.After checking the type, you'll want to querysome intersection information about the candidate.We perform three queries for information neededin the alpha test logic which is now in-lined.Finally, if the candidate intersection passes alpha test,you will commit it so that it becomesthe current committed intersection.Up to now, you have traversed the whole acceleration structureevaluating candidate intersectionsand commit the intersectionsthat passed the alpha test logic.Now, you need to query committed intersection informationto do the shading.First, you will query the committed type.If none of the candidate intersectionsmet your conditions to become a committed intersection,committed type will be none,which means the current ray missed.On the other hand,if there is a committed intersection,you would want to query informationabout the intersectionapplicable to the intersection typeand then use it for shading.That’s all the code you need to perform alpha testingusing intersection query.With the introduction of intersection queryand the introduction of intersector to render pipeline,we are giving you more opportunitiesto start bringing Metal ray tracing to your apps.Here are some things to consider when choosing betweenintersector objects and intersection queries.Start by considering if you have existing code,such as using the intersector in computeand your plans for porting that code.If you have existing query code from other APIs,intersection query can help to port that code.Next, you have the complexity of handling custom intersection.Intersector requires intersection functionsand tables, and it might be easier to use intersection queryto handle custom intersection yourself.The last question is performance.In simpler cases,intersection query can avoid overheadwhen building your pipeline for ray tracing,but custom intersection handlingrequires returning to your code during traversal,which could have a performance impact,depending on the use case.Also, use of multiple query objectswill require more memory.On the other hand,intersector can support those more complex casesby encapsulating all of the intersection work.If you have the opportunity, we would recommendcomparing the performance of both solutions.That’s all about intersection query.Now let’s move on to some other new features.The next two features we’re going to talk aboutare user instance IDs and instance transforms.These features will help you add more informationto your acceleration structureand access more of the data that is already there.Here is why we think these are really useful features.If we look back to the sample codefrom last year’s presentation,we have multiple instances of the kernel box.Underneath this, we have an instance acceleration structurewith a set of nodes that branch until you reach the instances.Looking at two of these instances,they are at the lowest levelof the instance acceleration structure.Currently, when you intersect one of these instances,you only get the system’s instance IDfrom the intersection results.With this, you could maintain your own table of data,but there’s data that we can exposein the acceleration structure to help you.Let’s talk about user-defined instance IDs first.With this feature, you can specify a custom 32-bit valuefor each instance and then you get this valueas part of the intersection results.This is really useful for youto index into your own data structures,but it can also be used to encode custom data.For example, here we’re using the user IDto encode a custom color for each instance.You could use this for a simpler reflectionwith no need to look up any additional material information.This is just one example,but the opportunities are endless.I can see how you would want to encode thingslike per-instance material ID or per-instance flags.We have created an extended versionof instance descriptor type that is used to specify these IDs.Make sure you specify which type of descriptor you’re usingon the instance acceleration structure descriptor.In the shading language,the value of the current user instance IDis available as an inputto intersection functions with the instancing tag.To obtain the values after intersection,the user-defined instance IDis available from the intersection resultwhen using an intersector object.And when using intersection query object,there is a corresponding queryto access the user-defined instance IDfor both candidate and committed intersections.Just like user instance ID, we have added supportfor accessing your instance transformation matrices.This data is already specified in the instance descriptor,and it is stored in the acceleration structure.This year, we’ve exposed these matricesfrom the shading language.You can access the instance transformsin the intersection functions when you applythe instancing and world_space_data tags.Similarly, the instance transforms are providedin the intersection results when using an intersectorwith instancing and world_space_data tags.When using intersection query with instancing tag,there are corresponding queriesfor accessing the instance transformsfor both candidate and committed intersections.To summarize, this year we are improvingthe usability and portability of Metal ray tracing APIby introducing three new features.Intersection query comes as an alternative to intersectorthat provides more control over the intersection process.And with the introduction of user instance IDand instance transforms features,we are providing you the ability to access datafrom the acceleration structure instead of having to handlesome external mapping in your code.In addition, these three featuresoffer portability from other ray tracing APIs,making cross-platform development easier.So far in the session, we have talked aboutour new support for ray tracing in render pipelineand the different usability and portability featureswe have added this year.Now, let me show you what features we are introducingto enhance production rendering.Since Metal ray tracing API was introduced last year,people have been using it to rendersome amazing high-quality content.This year, we’ve added two new featuresto make it possible to render even better content.Let's start with extended limits.Since we released the Metal ray tracing API,some users have started hitting the internal limitsof our acceleration structures,especially in production-scale use cases.So we are adding support for an extended-limits modeto support even larger scenes.Last year, we chose these limitsto balance acceleration structure sizein order to favor performance with typical scene sizes.There is a potential performance trade-offto turning on this feature, so you’ll need to determinewhich mode is best for your application.Extended-limits mode increases the limiton the number of primitives, geometries, instances,as well as the size of the maskused for filtering out instances.To turn it on, you first specify extended limits modewhen building your acceleration structures.Then specify the extended_limits tagon the intersector object in the shading language.That’s all you need to do to turn on extended limits!Next, let’s talk about motion.In computer graphics,we often assume that the camera exposure is instantaneous.However, in real life,the camera exposure lasts for a nonzero period of time.If an object moves relative to the camera during that time,it will appear blurred in the image.In this extreme example,the person in the center has been standing stillduring the whole exposurewhile everyone else has been moving,causing them to be blurred.This effect can go a long way towards makingcomputer-generated images look more realistic.In this example,the sphere is animated across several frames,but each frame is still an instantaneous exposure,resulting in a choppy animation.Using the motion API,we can simulate camera exposurelasting for a nonzero amount of time.This results in a smootherand more realistic-looking animation.If we freeze the video,you can see that the boundaries of the sphere are blurredin the direction of motion just like a real camera.Real-time applications like gamesoften approximate this effect in screen space.But ray tracing allows us to simulatephysically accurate motion blur,which even extends to indirect effectslike shadows and reflections.Let’s take a lookat how the motion-blurred version was rendered.Motion blur is a straightforward extension to ray tracing.Most ray-tracing applications already randomly samplethe physical dimensions such as incoming light directionsfor indirect lighting.To add motion blur,we can just choose a random time for each ray as well.Metal will intersect the scene to match the point in timeassociated with each ray.For instance, this ray will see the scene like this.Another ray will see the scene like this.As we accumulate more and more samples,we’ll start to converge on a motion-blurred image.You could actually already implement this todayusing custom intersection functions.You could compute the bounding boxesof each primitive throughout the entire exposureand then use these bounding boxesto build an acceleration structure.However, this would be inefficient;the bounding boxes could be so largethat some rays would need to check for intersectionwith primitives that they will never actually intersect.Instead, we can use Metal’s built-in support for motion blurwhich is designed to efficiently handle cases like this.The first thing we need to do is associate a random timewith each ray in our Metal shading language code.We start by generating a random timewithin the exposure interval,then we just pass it to the intersector.The next thing we need to dois to provide our animated geometry to Metal.We do this using a common method of animationcalled keyframe animation.The animation is created by modeling the ballat key points in time called keyframes.These keyframes are uniformly distributedbetween the start and end of our animation.As rays traverse the acceleration structure,they can fetch data from any keyframebased on their time value.For instance Ray A would see the sceneas it was modeled in Keyframe 11because its time happens to match Keyframe 11.In contrast, Ray B’s time is in between Keyframes 3 and 4.Therefore, the geometry of the two keyframesis interpolated for Ray B.Motion is supported at both the instanceand primitive level.Instance animation can be usedto rigidly transform entire objects.This is cheaper than primitive animationbut doesn’t allow objects to deform.On the other hand,primitive animation is more expensive,but can be used for things like skinned-character animation.Note that both instance and primitive animationare based on keyframe animation.Let’s first talk about instance motion.In an instance acceleration structure,each instance is associated with a transformation matrix.This matrix describeswhere to place the geometry in the scene.In this example,we have two primitive acceleration structures:one for the sphere and another for the static geometry.Each primitive acceleration structure has a single instance.To animate the sphere,we will provide two transformation matrices,representing the start and end point of the animation.Metal will then interpolate these two matricesbased on the time parameter for each ray.Keep in mind that this is a specific exampleusing two keyframes,but Metal supports an arbitrary number of key frames.We provide these matricesusing acceleration structure descriptors.The standard Metal instance descriptoronly has room for a single transformation matrix.So instead, we’ll use the new motion instance descriptor.With this descriptor,the transformation matrices are stored in a separate buffer.The instance descriptor then contains a start indexand count representing a range of transformation matricesin the transform buffer.Each matrix represents a single keyframe.Let's see how to set up an instance descriptorwith the new motion instance descriptor type.We start by creating the usualinstance acceleration structure descriptor.We then specify we’re usingthe new motion instance descriptor type.Then, we specify the instanceDescriptorBufferthat contains the motion instance descriptors.Finally, we’ll need to bind the transformsBuffercontaining the vertex buffer for each keyframe.The remaining properties are the sameas any other instance acceleration structure,and we can build it like any otheracceleration structure as well.We only need to make one change in the shading language,which is to specify the instance_motion tag.This tells the intersector to expectan acceleration structure with instance motion.And that’s all we need to do to set up instance motion.Next, let’s talk about primitive motion.With primitive motion,each primitive can move separately,meaning it can be used for thingslike skinned-character animation.Remember that we need to provide a separate 3D modelfor each keyframe,and Metal will then interpolate between them.We’ll need to provide vertex data for each keyframe.Let’s see how to set this up.We’ll start by collecting each keyframe’s vertex bufferinto an array.The MTLMotionKeyframeData objectallows you to specify a buffer and offset.We’ll use it to specify the vertex bufferfor each keyframe.Next, we’ll create a motion triangle geometry descriptor.This is just like creating any other geometry descriptor,except we use a slightly different type.And instead of providing a single vertex buffer,we’ll provide our array of vertexBuffers.Finally, we’ll create the usualprimitive acceleration structure descriptor.Next, we provide our geometryDescriptor.Then we’ll specify the number of keyframes.Similar to instance motion,we’ll need to make a small changein the shading languageto specify the primitive_motion tag.And that’s all we need to do to set up primitive motion!Keep in mind that you can actually useboth types of animation at the same timefor even more dynamic scenes.Next, let’s take a look at this all in action!This is a path-traced renderingcreated by our Advanced Content team.The video was rendered on a Mac Prowith an AMD Radeon Pro Vega II GPU.The ninja character was animatedusing a skinned skeletal animation techniquewhich allows each primitive to move separately.Each frame was rendered by combining256 randomly timed samples taken using the primitive motion API.We can slow it down to see the difference more clearly.The version on the left doesn’t have motion blur,while the version on the right does.And we can increase the exposure time even furtherto simulate a long exposure.Motion blur can make a big difference in realismand it’s now easy to add with the new motion API.So that’s it for motion.Thanks for watching this talk.We have been putting a lot of workinto our Metal ray tracing APIto provide the tools you need to enhance your app.We can’t wait to see the amazing contentthat you will create with it.Thank you, and have a great WWDC!♪

♪ Bass music playing ♪♪Juan Rodriguez Cuellar: Hello, welcome to WWDC.

My name is Juan Rodriguez Cuellar,and I'm a GPU compiler engineer at Apple.

In this session, we’re going to talk aboutthe brand-new features we’ve added this yearto enhance our Metal ray tracing API.

But first, let’s do a quick recap about ray tracing.

Ray-tracing applications are based on tracing the pathsthat rays take as they interact with a scene.

Ray tracing is applied in a lot of domains such as audio,physics simulation, and AI;but one of the main applications is photorealistic rendering.

In rendering applications,ray tracing is used to model individual rays of light,which allow us to simulate effects such as reflections,soft shadows, and indirect lighting.

That is just a general definition of ray tracing.

Let’s talk about Metal’s approach to it.

We start with a compute kernel.

In our kernel, we generate rays,which are emitted into the scene.

We then test those rays for intersectionsagainst the geometry in the scenewith an intersector and an acceleration structure.

Each intersection pointrepresents light bouncing off a surface;how much light bounces and in what directiondetermines what the object looks like.

We then compute a color for each intersectionand update the image.

This process is called shadingand it can also generate additional rays,and those rays are also tested for intersection.

We repeat the process as many times as we’d liketo simulate light bouncing around the scene.

This year, we focused our new featuresaround three major areas.

First, I will talk about how we’ve added ray-tracing supportto our render pipelines,which allows us to mix ray tracing with our rendering.

Then I will introduce you to the new featuresthat focus on the usability and portability.

These features will ease the use of Metal ray tracing API.

Finally, I will cover the production rendering featureswe’ve added this yearthat will help you create more realistic content.

Let's start with ray tracing from render pipelines.

Let’s consider the basic caseof a render that has a single render pass.

With our new support for ray tracing from render pipelines,this makes it super easy to add ray tracing into the render.

However, without this support,to add ray tracing to this renderwith last year’s Metal ray tracing API,we need to add a compute pass.

Let’s just start by adding it after renderingto augment the rendered image.

Adding this extra compute passmeans writing more output to memoryfor the compute pass to use for ray tracing.

Now, what if we wanted to use ray tracingin the middle of our render pass to calculate a valuesuch as shadowing per pixel?This means that we need to split our renderingand introduce a compute pass.

Thinking more about what this means,we need to write out pixel positions and normalsto memory as inputs to the ray tracing,and then read the intersection results back --possibly several times.

But with the new support for ray tracing from render stages,we never need to leave our render passand we just write our outputs to memory.

Let's see how we use our new API.

Preparing our render pipeline for ray tracingis similar to a compute pipeline.

You start by building your acceleration structureand defining custom intersection functions.

To support custom intersection,we need an intersection function tableand we need to fill it with intersection functions.

This part has some differences compared to last year’s API.

Let’s walk through how we can do that.

Let’s consider some simple intersection functions.

I have some functions here that will allow usto analytically intersect objects such as a sphere,a cone, or torus.

When we create our pipeline, we add these functionsas linked functions that we may call.

In this case, we are adding themto the fragment stage of the pipeline.

To use the functions,we need to create an intersection function tablefrom the pipeline state and stage.

Once we have the table, we can create function handlesfrom the pipeline state and stageand populate the table.

Specifying the functions for the fragment stagereuses the linkedFunctions objectthat we introduced last year.

Each stage has its own set of linkedFunctionson the render pipeline descriptor.

Creating an intersection function tableis much the same as when done for the compute pipeline.

The only change is the addition of the stage argument.

To populate the table, we create the function handle.

Again, the handle is specific to the stage,so we need to specify the stage when requesting the handle.

Once we have the function handle,we just insert it in the function table.

And that’s all you need to do to prepare your function tablesin render pipeline.

Now we just have to use everything we have built so farto intersect.

Actual use is straightforward.

The accelerationStructure and the intersection function tableare both bound to buffer indices on the render encoder.

The shaders can then use these resourcesto intersect rays with an intersectorthe same way you would in a compute kernel.

More details about how to prepare your pipelinefor ray tracing can be found in last year’s presentation.

In that talk, you will learn aboutbuilding acceleration structures,creating function tables,and using the intersector in the shading language.

With ray-tracing support from render pipelines,we are opening the door for even more opportunitieslike adding ray tracing within a single render pass,mixing ray tracing with rasterizationin hybrid rendering,and taking advantage of optimizationssuch as tile functions on Apple Silicon.

In fact, we will be soon adding ray tracing to our sample appthat we demoed during the "Modern Rendering in Metal"session at WWDC 2019.

With ray tracing from render pipelines,we can update the code to use tile functionsto keep everything in tile memory.

For more details about this, see this year’s"Explore hybrid rendering with Metal ray tracing" presentation.

Next up, I want to introduce the new features we've addedthis year to improve the usability and portabilityof Metal ray tracing API.

These features not only providesimpler use of Metal ray tracing,but they also provide portabilityfrom other ray tracing APIs.

One of these new features is intersection query.

With intersection query,we’re allowing you to have more controlover the intersection process.

Intersection query is aimed toward simple use caseswhere intersector can create overhead.

It is a new way of traversing the acceleration structurethat gives you the option of performingin-line custom intersection testing.

Let's take a look at how we currently handlecustom intersection using last year’s intersector.

Going back to the alpha test example from last year’sray tracing with Metal presentation,we demonstrated how alpha testing is usedto add a lot of geometric detail to the scene,as you see here in the chains and leaves.

We also learned how easy it is to implement alpha testingby customizing an intersectorusing the triangle intersection function.

The logic inside this triangle intersection functionis responsible for accepting or rejecting intersectionsas the ray traverses the acceleration structure.

In this case, test logic will reject the first intersection,but it will accept the second intersectionsince an opaque surface has been intersected.

Let’s see how intersection functions are used.

When using Intersector, when you call intersect(),we start traversing the acceleration structureto find an intersectionand fill our intersection_result.

Within the intersector,intersection function gets calledeach time a potential intersection is found.

And intersections are then accepted or rejectedbased on intersection function logic.

This is a great programming model using intersectorsince it is both performant and convenient,but it does require creating a new intersection functionand linking it to the pipeline.

There might be caseswhere the logic inside the intersection functionis only a few lines of code,as it is the case for alpha test logic.

This is the intersection functionthat contains the logic to do alpha testing.

With intersection query, we can place this logic in-linewithout the need of this intersection function.

Here is how.

With intersection query,when you start the intersection process,your ray traverses the acceleration structure,and the query object contains the state of the traversaland the result.

Each time the ray intersects a custom primitiveor a nonopaque triangle,control is returned to the shaderfor you to evaluate the intersection candidates.

If the current candidate passes your custom intersection logic,you commit it to updatethe current committed intersectionand then continue the intersection process.

On the other hand, if candidate fails custom intersection logic,you can just ignore it and continue.

Let me show you the code to do alpha testingusing intersection query.

First, you start the traversal.

Note that we loop on nextto evaluate all candidate intersections.

Second, you evaluate each candidatestarting by checking the candidate type.

For alpha test example, you're interestedon intersections of triangle type.

After checking the type, you'll want to querysome intersection information about the candidate.

We perform three queries for information neededin the alpha test logic which is now in-lined.

Finally, if the candidate intersection passes alpha test,you will commit it so that it becomesthe current committed intersection.

Up to now, you have traversed the whole acceleration structureevaluating candidate intersectionsand commit the intersectionsthat passed the alpha test logic.

Now, you need to query committed intersection informationto do the shading.

First, you will query the committed type.

If none of the candidate intersectionsmet your conditions to become a committed intersection,committed type will be none,which means the current ray missed.

On the other hand,if there is a committed intersection,you would want to query informationabout the intersectionapplicable to the intersection typeand then use it for shading.

That’s all the code you need to perform alpha testingusing intersection query.

With the introduction of intersection queryand the introduction of intersector to render pipeline,we are giving you more opportunitiesto start bringing Metal ray tracing to your apps.

Here are some things to consider when choosing betweenintersector objects and intersection queries.

Start by considering if you have existing code,such as using the intersector in computeand your plans for porting that code.

If you have existing query code from other APIs,intersection query can help to port that code.

Next, you have the complexity of handling custom intersection.

Intersector requires intersection functionsand tables, and it might be easier to use intersection queryto handle custom intersection yourself.

The last question is performance.

In simpler cases,intersection query can avoid overheadwhen building your pipeline for ray tracing,but custom intersection handlingrequires returning to your code during traversal,which could have a performance impact,depending on the use case.

Also, use of multiple query objectswill require more memory.

On the other hand,intersector can support those more complex casesby encapsulating all of the intersection work.

If you have the opportunity, we would recommendcomparing the performance of both solutions.

That’s all about intersection query.

Now let’s move on to some other new features.

The next two features we’re going to talk aboutare user instance IDs and instance transforms.

These features will help you add more informationto your acceleration structureand access more of the data that is already there.

Here is why we think these are really useful features.

If we look back to the sample codefrom last year’s presentation,we have multiple instances of the kernel box.

Underneath this, we have an instance acceleration structurewith a set of nodes that branch until you reach the instances.

Looking at two of these instances,they are at the lowest levelof the instance acceleration structure.

Currently, when you intersect one of these instances,you only get the system’s instance IDfrom the intersection results.

With this, you could maintain your own table of data,but there’s data that we can exposein the acceleration structure to help you.

Let’s talk about user-defined instance IDs first.

With this feature, you can specify a custom 32-bit valuefor each instance and then you get this valueas part of the intersection results.

This is really useful for youto index into your own data structures,but it can also be used to encode custom data.

For example, here we’re using the user IDto encode a custom color for each instance.

You could use this for a simpler reflectionwith no need to look up any additional material information.

This is just one example,but the opportunities are endless.

I can see how you would want to encode thingslike per-instance material ID or per-instance flags.

We have created an extended versionof instance descriptor type that is used to specify these IDs.

Make sure you specify which type of descriptor you’re usingon the instance acceleration structure descriptor.

In the shading language,the value of the current user instance IDis available as an inputto intersection functions with the instancing tag.

To obtain the values after intersection,the user-defined instance IDis available from the intersection resultwhen using an intersector object.

And when using intersection query object,there is a corresponding queryto access the user-defined instance IDfor both candidate and committed intersections.

Just like user instance ID, we have added supportfor accessing your instance transformation matrices.

This data is already specified in the instance descriptor,and it is stored in the acceleration structure.

This year, we’ve exposed these matricesfrom the shading language.

You can access the instance transformsin the intersection functions when you applythe instancing and world_space_data tags.

Similarly, the instance transforms are providedin the intersection results when using an intersectorwith instancing and world_space_data tags.

When using intersection query with instancing tag,there are corresponding queriesfor accessing the instance transformsfor both candidate and committed intersections.

To summarize, this year we are improvingthe usability and portability of Metal ray tracing APIby introducing three new features.

Intersection query comes as an alternative to intersectorthat provides more control over the intersection process.

And with the introduction of user instance IDand instance transforms features,we are providing you the ability to access datafrom the acceleration structure instead of having to handlesome external mapping in your code.

In addition, these three featuresoffer portability from other ray tracing APIs,making cross-platform development easier.

So far in the session, we have talked aboutour new support for ray tracing in render pipelineand the different usability and portability featureswe have added this year.

Now, let me show you what features we are introducingto enhance production rendering.

Since Metal ray tracing API was introduced last year,people have been using it to rendersome amazing high-quality content.

This year, we’ve added two new featuresto make it possible to render even better content.

Let's start with extended limits.

Since we released the Metal ray tracing API,some users have started hitting the internal limitsof our acceleration structures,especially in production-scale use cases.

So we are adding support for an extended-limits modeto support even larger scenes.

Last year, we chose these limitsto balance acceleration structure sizein order to favor performance with typical scene sizes.

There is a potential performance trade-offto turning on this feature, so you’ll need to determinewhich mode is best for your application.

Extended-limits mode increases the limiton the number of primitives, geometries, instances,as well as the size of the maskused for filtering out instances.

To turn it on, you first specify extended limits modewhen building your acceleration structures.

Then specify the extended_limits tagon the intersector object in the shading language.

That’s all you need to do to turn on extended limits!Next, let’s talk about motion.

In computer graphics,we often assume that the camera exposure is instantaneous.

However, in real life,the camera exposure lasts for a nonzero period of time.

If an object moves relative to the camera during that time,it will appear blurred in the image.

In this extreme example,the person in the center has been standing stillduring the whole exposurewhile everyone else has been moving,causing them to be blurred.

This effect can go a long way towards makingcomputer-generated images look more realistic.

In this example,the sphere is animated across several frames,but each frame is still an instantaneous exposure,resulting in a choppy animation.

Using the motion API,we can simulate camera exposurelasting for a nonzero amount of time.

This results in a smootherand more realistic-looking animation.

If we freeze the video,you can see that the boundaries of the sphere are blurredin the direction of motion just like a real camera.

Real-time applications like gamesoften approximate this effect in screen space.

But ray tracing allows us to simulatephysically accurate motion blur,which even extends to indirect effectslike shadows and reflections.

Let’s take a lookat how the motion-blurred version was rendered.

Motion blur is a straightforward extension to ray tracing.

Most ray-tracing applications already randomly samplethe physical dimensions such as incoming light directionsfor indirect lighting.

To add motion blur,we can just choose a random time for each ray as well.

Metal will intersect the scene to match the point in timeassociated with each ray.

For instance, this ray will see the scene like this.

Another ray will see the scene like this.

As we accumulate more and more samples,we’ll start to converge on a motion-blurred image.

You could actually already implement this todayusing custom intersection functions.

You could compute the bounding boxesof each primitive throughout the entire exposureand then use these bounding boxesto build an acceleration structure.

However, this would be inefficient;the bounding boxes could be so largethat some rays would need to check for intersectionwith primitives that they will never actually intersect.

Instead, we can use Metal’s built-in support for motion blurwhich is designed to efficiently handle cases like this.

The first thing we need to do is associate a random timewith each ray in our Metal shading language code.

We start by generating a random timewithin the exposure interval,then we just pass it to the intersector.

The next thing we need to dois to provide our animated geometry to Metal.

We do this using a common method of animationcalled keyframe animation.

The animation is created by modeling the ballat key points in time called keyframes.

These keyframes are uniformly distributedbetween the start and end of our animation.

As rays traverse the acceleration structure,they can fetch data from any keyframebased on their time value.

For instance Ray A would see the sceneas it was modeled in Keyframe 11because its time happens to match Keyframe 11.

In contrast, Ray B’s time is in between Keyframes 3 and 4.

Therefore, the geometry of the two keyframesis interpolated for Ray B.

Motion is supported at both the instanceand primitive level.

Instance animation can be usedto rigidly transform entire objects.

This is cheaper than primitive animationbut doesn’t allow objects to deform.

On the other hand,primitive animation is more expensive,but can be used for things like skinned-character animation.

Note that both instance and primitive animationare based on keyframe animation.

Let’s first talk about instance motion.

In an instance acceleration structure,each instance is associated with a transformation matrix.

This matrix describeswhere to place the geometry in the scene.

In this example,we have two primitive acceleration structures:one for the sphere and another for the static geometry.

Each primitive acceleration structure has a single instance.

To animate the sphere,we will provide two transformation matrices,representing the start and end point of the animation.

Metal will then interpolate these two matricesbased on the time parameter for each ray.

Keep in mind that this is a specific exampleusing two keyframes,but Metal supports an arbitrary number of key frames.

We provide these matricesusing acceleration structure descriptors.

The standard Metal instance descriptoronly has room for a single transformation matrix.

So instead, we’ll use the new motion instance descriptor.

With this descriptor,the transformation matrices are stored in a separate buffer.

The instance descriptor then contains a start indexand count representing a range of transformation matricesin the transform buffer.

Each matrix represents a single keyframe.

Let's see how to set up an instance descriptorwith the new motion instance descriptor type.

We start by creating the usualinstance acceleration structure descriptor.

We then specify we’re usingthe new motion instance descriptor type.

Then, we specify the instanceDescriptorBufferthat contains the motion instance descriptors.

Finally, we’ll need to bind the transformsBuffercontaining the vertex buffer for each keyframe.

The remaining properties are the sameas any other instance acceleration structure,and we can build it like any otheracceleration structure as well.

We only need to make one change in the shading language,which is to specify the instance_motion tag.

This tells the intersector to expectan acceleration structure with instance motion.

And that’s all we need to do to set up instance motion.

Next, let’s talk about primitive motion.

With primitive motion,each primitive can move separately,meaning it can be used for thingslike skinned-character animation.

Remember that we need to provide a separate 3D modelfor each keyframe,and Metal will then interpolate between them.

We’ll need to provide vertex data for each keyframe.

Let’s see how to set this up.

We’ll start by collecting each keyframe’s vertex bufferinto an array.

The MTLMotionKeyframeData objectallows you to specify a buffer and offset.

We’ll use it to specify the vertex bufferfor each keyframe.

Next, we’ll create a motion triangle geometry descriptor.

This is just like creating any other geometry descriptor,except we use a slightly different type.

And instead of providing a single vertex buffer,we’ll provide our array of vertexBuffers.

Finally, we’ll create the usualprimitive acceleration structure descriptor.

Next, we provide our geometryDescriptor.

Then we’ll specify the number of keyframes.

Similar to instance motion,we’ll need to make a small changein the shading languageto specify the primitive_motion tag.

And that’s all we need to do to set up primitive motion!Keep in mind that you can actually useboth types of animation at the same timefor even more dynamic scenes.

Next, let’s take a look at this all in action!This is a path-traced renderingcreated by our Advanced Content team.

The video was rendered on a Mac Prowith an AMD Radeon Pro Vega II GPU.

The ninja character was animatedusing a skinned skeletal animation techniquewhich allows each primitive to move separately.

Each frame was rendered by combining256 randomly timed samples taken using the primitive motion API.

We can slow it down to see the difference more clearly.

The version on the left doesn’t have motion blur,while the version on the right does.

And we can increase the exposure time even furtherto simulate a long exposure.

Motion blur can make a big difference in realismand it’s now easy to add with the new motion API.

So that’s it for motion.

Thanks for watching this talk.

We have been putting a lot of workinto our Metal ray tracing APIto provide the tools you need to enhance your app.

We can’t wait to see the amazing contentthat you will create with it.

Thank you, and have a great WWDC!♪

4:48 -Specify intersection functions on render pipeline state

5:02 -Create intersection function table

5:14 -Populate intersection function table

5:48 -Bind resources

5:57 -Intersect from fragment shader

9:32 -Triangle intersection function

10:36 -Custom intersection with intersection query

10:39 -Custom intersection with intersection query 2

15:30 -Specifying user instance IDs

15:47 -Retrieving user instance IDs 1

15:58 -Retrieving user instance IDs 2

16:36 -Instance transforms

16:51 -Instance transforms 2

17:03 -Instance transforms 3

19:17 -Extended limits

22:30 -Sampling time

25:54 -Motion instance descriptor

26:33 -Instance motion

27:24 -Primitive motion 1

27:39 -Primitive motion 2

27:57 -Primitive motion 3

28:10 -Primitive motion 4

## Code Samples

```swift
// Create and attach MTLLinkedFunctions object


NSArray
 <
id
 <
MTLFunction
>> *functions = @[ sphere, cone, torus ];


MTLLinkedFunctions
 *linkedFunctions = [
MTLLinkedFunctions
 linkedFunctions];
linkedFunctions.functions = functions;

pipelineDescriptor.fragmentLinkedFunctions = linkedFunctions;


// Create pipeline


id
<
MTLRenderPipelineState
> rayPipeline;
rayPipeline = [device newRenderPipelineStateWithDescriptor:pipelineDescriptor
                                                     error:&error];
```

```swift
// Fill out intersection function table descriptor


MTLIntersectionFunctionTableDescriptor
 *tableDescriptor =
    [
MTLIntersectionFunctionTableDescriptor
 intersectionFunctionTableDescriptor];

tableDescriptor.functionCount = functions.count;


// Create intersection function table


id
<
MTLIntersectionFunctionTable
> table;
table = [rayPipeline newIntersectionFunctionTableWithDescriptor:tableDescriptor
                                                          stage:
MTLRenderStageFragment
];
```

```swift
id
<
MTLFunctionHandle
> handle;


for
 (
NSUInteger
 i = 
0
 ; i < functions.count ; i++) {
    
// Get a handle to the linked intersection function in the pipeline state

    handle = [rayPipeline functionHandleWithFunction:functions[i]
                                               stage:
MTLRenderStageFragment
];

    
// Insert the function handle into the table

    [table setFunction:handle atIndex:i];
}
```

```swift
[renderEncoder setFragmentAccelerationStructure:accelerationStructure atBufferIndex:
0
];
[renderEncoder setFragmentIntersectionFunctionTable:table atBufferIndex:
1
];
```

```swift
[[fragment]]
float4 rayFragmentShader(vertex_output vo [[stage_in]],
                         primitive_acceleration_structure accelerationStructure,
                         intersection_function_table<triangle_data> functionTable,
                         
/* ... */
)
{
    
// generate ray, create intersector...


    intersection = intersector.intersect(ray, accelerationStructure, functionTable);

    
// shading...

}
```

```swift
[[intersection(triangle, triangle_data)]]

bool
 alphaTestIntersectionFunction(uint primitiveIndex        [[primitive_id]],
                                   uint geometryIndex         [[geometry_id]],
                                   float2 barycentricCoords   [[barycentric_coord]],
                                   device Material *materials [[buffer(
0
)]])

{
    texture2d<
float
> alphaTexture = materials[geometryIndex].alphaTexture;

    float2 UV = interpolateUVs(materials[geometryIndex].UVs,
        primitiveIndex, barycentricCoords);

    
float
 alpha = alphaTexture.sample(sampler, UV).x;

    
return
 alpha > 
0.5
f;
}
```

```swift
intersection_query<instancing, triangle_data> iq(ray, as, params);


// Step 1: start traversing acceleration structure


while
 (iq.next())
{
    
// Step 2: candidate was found. Check type and run custom intersection.

    
switch
 (iq.get_candidate_intersection_type())
    {
	    
case
 intersection_type::triangle:
	    { 
	       
bool
 alphaTestResult = alphaTest(iq.get_candidate_geometry_id(),
	                                iq.get_candidate_primitive_id(),
	                                iq.get_candidate_triangle_barycentric_coord());
    	   
// Step 3: commit candidate or ignore

           
if
 (alphaTestResult) 
               iq.commit_triangle_intersection()
  	  }
    }
}
```

```swift
switch
 (iq.get_committed_intersection_type())
{
  
// Miss case  

  
case
 intersection_type::none:
  {
      missShading();
      
break
;
  } 
  
  
// Triangle intersection was committed. Query some info and do shading.

  
case
 intersection_type::triangle:
  {
      shadeHitTriangle(iq.get_committed_instance_id(),
                       iq.get_committed_distance(),
                       iq.get_committed_triangle_barycentric_coord());
      
break
;
  }
}
```

```swift
// New instance descriptor type



typedef
 
struct
 {
    uint32_t userID;
    
// Members from MTLAccelerationStructureInstanceDescriptor...

} 
MTLAccelerationStructureUserIDInstanceDescriptor
;


// Specify instance descriptor type through acceleration structure descriptor


accelDesc.instanceDescriptorType = 
MTLAccelerationStructureInstanceDescriptorTypeUserID
;
```

```swift
// Available in intersection functions


[[intersection(bounding_box, instancing)]]
IntersectionResult sphereInstanceIntersectionFunction(
unsigned
 
int
 userID[[user_instance_id]],
                                                      
/** other args **/
)
{
    
// ...

}
```

```swift
// Available from intersection result


intersection_result<instancing> intersection = instanceIntersector.intersect(
/* args */
);


if
 (intersection.type != intersection_type::none)
    instanceIndex = intersection.user_instance_id;


// Available from intersection query


intersection_query<instancing> iq(
/* args */
);

iq.next()


if
 (iq.get_committed_intersection_type() != intersection_type::none)
    instanceIndex = iq.get_committed_user_instance_id();
```

```swift
// Available in intersection functions


[[intersection(bounding_box, instancing, world_space_data)]]
IntersectionResult intersectionFunction(float4x3 objToWorld [[object_to_world_transform]],
                                        float4x3 worldToObj [[world_to_object_transform]],
                                        
/** other args **/
)
{
    
// ...

}
```

```swift
// Available from intersection result


intersection_result<instancing, world_space_data> result = 
    intersector.intersect(
/* args */
);


if
 (result.type != intersection_type::none) {
    output.myObjectToWorldTransform = result.object_to_world_transform;
    output.myWorldToObjectTransform = result.world_to_object_transform;
}
```

```swift
// Available from intersection query


intersection_query<instancing> iq(
/* args */
);

iq.next()


if
(iq.get_committed_intersection_type() != intersection_type::none){
    output.myObjectToWorldTransform = iq.get_committed_object_to_world_transform();
    output.myWorldToObjectTransform = iq.get_committed_world_to_object_transform();
}
```

```swift
// Specify through acceleration structure descriptor


accelDesc.usage = 
MTLAccelerationStructureUsageExtendedLimits
;


// Specify intersector tag


intersector<extended_limits> extendedIntersector;
```

```swift
// Randomly sample time



float
 time = random(exposure_start, exposure_end);

result = intersector.intersect(ray, acceleration_structure, time);
```

```swift
descriptor = [
MTLInstanceAccelerationStructureDescriptor
 new];

descriptor.instanceDescriptorType = 
MTLAccelerationStructureInstanceDescriptorTypeMotion
;


// Buffer containing motion instance descriptors

descriptor.instanceDescriptorBuffer = instanceBuffer;
descriptor.instanceCount = instanceCount;


// Buffer containing MTLPackedFloat4x3 transformation matrices

descriptor.motionTransformBuffer = transformsBuffer;
descriptor.motionTransformCount = transformCount;

descriptor.instancedAccelerationStructures = primitiveAccelerationStructures;
```

```swift
// Specify intersector tag


kernel 
void
 raytracingKernel(acceleration_structure<instancing, instance_motion> as,
                             
/* other args */
)
{
    intersector<instancing, instance_motion> intersector;

    
// ...

}
```

```swift
// Collect keyframe vertex buffers



NSMutableArray
<
MTLMotionKeyframeData
*> *vertexBuffers = [
NSMutableArray
 new];


for
 (
NSUInteger
 i = 
0
 ; i < keyframeBuffers.count ; i++) {
    
MTLMotionKeyframeData
 *keyframeData = [
MTLMotionKeyframeData
 data];

    keyframeData.buffer = keyframeBuffers[i];

    [vertexBuffers addObject:keyframeData];
}
```

```swift
// Create motion geometry descriptor



MTLAccelerationStructureMotionTriangleGeometryDescriptor
 *geometryDescriptor =
    [
MTLAccelerationStructureMotionTriangleGeometryDescriptor
 descriptor];

geometryDescriptor.vertexBuffers = vertexBuffers;
geometryDescriptor.triangleCount = triangleCount;
```

```swift
// Create acceleration structure descriptor



MTLPrimitiveAccelerationStructureDescriptor
 *primitiveDescriptor =
    [
MTLPrimitiveAccelerationStructureDescriptor
 descriptor];

primitiveDescriptor.geometryDescriptors = @[ geometryDescriptor ];

primitiveDescriptor.motionKeyframeCount = keyframeCount;
```

```swift
// Specify intersector tag


kernel 
void
 raytracingKernel(acceleration_structure<primitive_motion> as,
                             
/* other args */
)
{
    intersector<primitive_motion> intersector;

    
// ...

}
```

