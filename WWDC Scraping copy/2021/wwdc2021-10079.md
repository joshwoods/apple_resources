# Wwdc2021 10079

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Discover geometry-aware audio with the Physical Audio Spatialization Engine (PHASE)Explore how geometry-aware audio can help you build complex, interactive, and immersive audio scenes for your apps and games. Meet PHASE, Apple's spatial audio API, and learn how the Physical Audio Spatialization Engine (PHASE) keeps the sound aligned with your experience at all times — helping you create spatial soundscapes and scenes during the development process, rather than waiting until post production. We'll take you through an overview of the API and its classes, including Sources, Listeners, Acoustic Geometry, and Materials, and introduce the concept of Spatial Modeling. We'll also show you how to quickly combine PHASE's basic building blocks to start building an integrated audio experience for your app or game.ResourcesPHASEHD VideoSD VideoRelated VideosWWDC23Bring your game to Mac, Part 1: Make a game planWWDC22Plug-in and play: Add Apple frameworks to your Unity game projects

Explore how geometry-aware audio can help you build complex, interactive, and immersive audio scenes for your apps and games. Meet PHASE, Apple's spatial audio API, and learn how the Physical Audio Spatialization Engine (PHASE) keeps the sound aligned with your experience at all times — helping you create spatial soundscapes and scenes during the development process, rather than waiting until post production. We'll take you through an overview of the API and its classes, including Sources, Listeners, Acoustic Geometry, and Materials, and introduce the concept of Spatial Modeling. We'll also show you how to quickly combine PHASE's basic building blocks to start building an integrated audio experience for your app or game.

PHASE

HD VideoSD Video

HD Video

SD Video

Bring your game to Mac, Part 1: Make a game plan

Plug-in and play: Add Apple frameworks to your Unity game projects

Search this video…Hello, my name is Bharath,and I am from Apple's Core Audio team.And today, and I would like to talk to youabout geometry-aware audio with PHASE.We will talk about why you would want to usethe new PHASE framework.We will introduce you to some of the featuresthe framework offers.Then, I will hand it over to my colleague, David Thall,to take you through concepts in some depthand sample use cases with this API.Let's begin.Audio is a critical aspect of any gaming experience.Spatial audio on headphonestakes the overall gameplay to the next leveland makes you feel more involved.In games today,various subsystems of the engine like physics,animation, visual effects, et cetera,all communicate with each other and movethe game flow or storyline forwardbased on the player's actions.However, the audio subsystem is generally managedand driven separately from the rest.They sometimes are also authored through middlewarethat isn't always aware of the simulation.Audio assets are post-produced, pre-baked,hand-tuned to tell an audio storythat matches the game visuals.As visuals evolve, audio system, sound design,and associated assets needs to be regeneratedto keep audio experience cohesiveon a wide range of platforms.This iterative development process needs to beaccounted for during the game development.This typically leads to audio experienceslagging the visual side of the gameplay.To provide a better game audio experience,we want to bring the audio systemcloser to other subsystems.We also want to make it easier for youto write applications that can providea consistent spatial audio experienceon all these supported devices.Let us now take you throughthe new audio framework PHASE and its features.PHASE is a new framework that will enable you to providegeometry information to the audio engine,help you build a sound-design friendly,event-driven audio playback system,enable you to write applications that can automatically providea consistent spatial audio experienceacross all supported devices and can be integratedwith existing authoring solutions and pipelines.Before we jump into learning more about PHASE,let us review a commonly used game audio workflow.Here's an example outdoor scenewith a listener, a sound source,a creek flowing and an occluder, a barn.Occluders are objects in the scenethat could dampen the sound between a sourceand the listener.Typically, you would placemultiple point sources along an area.As the listener moves,you have to use various techniquessuch as ray tracingto determine the proper filtering and mix ratiobetween the point sourcesand manually blend between themto provide a good audio experience.During the natural course of the game development,if visual scenes change, for example,the barn in the example scene,you have to update and hand-tune the audio experienceto match the visual scene change.Imagine you can build applicationswhere audio sources are not pointsyou need to manage and mix based on the scenebut rather as sound emanating over an areaor volume that the audio systemcan automatically manage for you.PHASE does exactly that.Introducing volumetric sources.The new framework provides API that allows you to pass soundsources as geometric shapes to the underlying audio engine.In addition to volumetric sound sources,you can also pass occludersin the scene as geometric shapes.You can also choose acoustic material propertiesfrom a set of presets and attach them to occluders.PHASE framework also allows you to set medium propagationand source directivity for point sourcesas required by your application.We are looking at an outdoor sceneas an example here.However, if your application has indoor scenes,you can choose early reflectionsand late reverberation properties froma library of presets.Once you tell the framework where various sound sources,occluders, and the listener are,PHASE will help with the heavy lifting and modelthe occlusion and spreading effects of various sound sourcesin the scene for you.Now that your application's audio system is geometry aware,it can adapt to visual scene changesmuch quicker as the game development evolves.In addition to geometry awareness,PHASE also provides an event-basedinteractive playback system.Sound events are basic units for describing audio playback eventsin PHASE.They encapsulate the selection, blending,and playback of audio assets.Sound events can range from simple eventslike one-shot playback, looping,to complex sequences organized as a treethat can blend or switch between subtreesof playback events.Let us take a simple example of playing footsteps.Here we have a random nodethat will automatically choose between three differentsounds of a footstep on gravel.We can have another sound event for cloth rustle.Both event trees can be grafted onto another tree,"near" in this example,to play back a mix of cloth rustle and footsteps sounds.We can have another tree, "far" in this example,to play a different set of soundswhen the character is at a distance.The mix of near and far treescan be now controlled by distance based on the gameplay.You can add more event treeslike footstep on snow or grass, for example,and build a complex sequence of playback eventsthat can be triggered by user interaction or by subsystemslike physics and animation.With PHASE, sounds can be played either to a simplechannel configuration or in a 3D spacewith orientation and position, or as ambient bedswhere the sounds have orientation but no position.The underlying engine is built uponour spatial audio rendering capabilitiesalready available on supported iOS, macOS devices,and also Air Pods family of headphones.This enables you to build applicationsthat provide a consistent spatial audio experienceon all the supported devices automatically.Next, I would like to invite David Thallto dive deeper into PHASE and talk more aboutthe concepts and the API for some example use cases.Hi, everyone, my name is David Thall,and I'm System Architect and Development Lead on PHASE.Today I'm going to walk you through the PHASE API.In this section, I'll introduce you to the general concepts.Following this, I'll run through some sample use casesto get you started.The PHASE API can be separated into three main concepts.The engine manages assets.Nodes control playback.And mixers control spatialization.The PHASE engine can be broken down intothree main sections.The asset registry, scene graph, and rendering state.Throughout the engine life cycle you'll register and unregisterassets with the engine.Today, PHASE supports registering sound assetsand sound event assets.Sound assets can be loaded directly from an audio fileor packed and embedded as raw audio datain your own assets and loaded directly into the engine.Sound event assets are a collection of one or morehierarchical nodes that control sound playbackand downstream mixers that control spatialization.The scene graph is a hierarchy of objects that take partin the simulation.This includes listeners, sources, and occluders.A listener is an object that represents the location in spacefrom which you hear the simulation.A source is an objectthat represents where sound originates.As Bharath mentioned earlier,PHASE supports both point sourcesand volumetric sources.An occluder is an object that represents the geometryin the simulation that affects sound transmissionas it moves through the environment.Occluders are also assigned materials that affecthow they absorb and transmit sound.PHASE comes with a library of material presets thatcan be assigned to occludersto simulate everything from cardboard boxesto glass windows to brick walls.As you add objects to your scene, you'll organize them intoa hierarchy and attach them to the engine's root object,either directly or indirectly.This will ensure they take part in the simulationfrom frame to frame.The rendering state manages playing sound eventsand audio IO.When you first create the engine, audio IO is disabled.This allows you to register your assets,build your scene graph, construct sound events,and perform other engine operations,all without having to run audio IO.Once you're ready to play back sound events,you can start the engine,which will internally start the audio IO.Likewise, when you're finished playing back sound events,you can stop the engine.This will stop the audio IOand stop any playing sound events.Nodes in PHASE control playback of audio content.Nodes are a hierarchical collection of objectsthat either generate or control audio playback.Generator nodes produce audio.They are always leaf nodes in a node hierarchy.Control nodes set the logic for how generators are selected,mixed and parameterized before spatialization.Control nodes are always parent nodesand can be organized into hierarchiesfor complex sound design scenarios.A sampler node is a type of generator node.Samplers play back registered sound assets.Once constructed, you can set some basic propertieson the sampler node to get it to play back correctly.The playback mode determines how the audio file will be played.If you set the playback mode to OneShot,the audio file will play once and automatically stop itself.This can be used in "fire-and-forget" scenarios,such as triggering sound effects.If you set the playback mode to looping,the audio file will play indefinitely,until you explicitly stop the sampler.The cull option tells PHASE what to dowhen the sound becomes inaudible.If you set the cull option to terminate,the sound will automatically stop itselfwhen it becomes inaudible.If you set the cull option to sleep,the sound will stop rendering when it becomes inaudibleand start rendering again when it becomes audible.This makes it so you don't have tomanually start and stop soundswhen they are culled by the engine.The calibration level sets the real-world level of the soundin decibels SPL.PHASE also supports four types of control nodes.These include random, switch, blend and container nodes.The random node selects one of its childrenaccording to a weighted random choice.For example, in this case,the left sampler has 4:1 odds over the right sampler of beingselected the next time the sound event is triggered.The switch node switches between its childrenbased on a parameter name.For example, you could change the terrain switchfrom "creaky wood" to "soft gravel."The next time the sound event is triggered,it'll select the sampler that matches the parameter name.The blend node blends between its childrenbased on a parameter value.For example, you could assign a wetness parameterto a blend node,which could blend between a loud footstepand quiet splash on the dry endand a quiet footstep and loud splash on the wet end.The container node plays all its children at once.For example, you could have one samplerthat plays back a footstepand another sampler that plays back the sound of clothing,like the sound of a ruffling Gor-Tex jacket.Every time the container is triggered,both samplers will play back at the same time.Mixers in PHASE control spatialization of audio content.PHASE currently supports channel,ambient, and spatial mixers.Channel mixers render audio without spatializationand environmental effects.Use channel mixers for regular stem-based content thatneeds to be rendered directly to the output device, such asstereo music or center channel narrative dialogue.Ambient mixers render audio with externalizationbut without distance modeling or environmental effects.As the listener rotates their head,the sound will continue to come fromthe same relative location in space.Use ambient mixers for multichannel contentthat isn't being simulated in the environment but shouldstill sound like it's coming from somewhere out in space,for example, a background of crickets chirpingin a large forest.Spatial mixers perform full spatialization.As the sound source moves relative to the listener,you'll hear changes in perceived location,level, and frequency response based on panning,distance modeling, and directivity modeling algorithms.In addition to this,geometry-aware environmental effectsare applied to the path between the source and listener.If you're wearing headphones, you'll also get externalizationthrough the application of binaural filters.Use spatial mixers for soundsthat should take part in the full environmental simulation.Spatial mixers supporttwo unique distance modeling algorithms.You can set up standard geometric spreading lossfor natural attenuation over distance.You can also increase or decrease the effectto your liking.For example, lowering the value could be usefulif you wanted to boom mic a conversation at a distance.On the other end of the spectrum,you can add full piecewise curvedsegments of attenuation over distance.For example, you could construct a set of segmentswith natural distance attenuationat the start and end of the range,but decrease the attenuation in the middleto keep important dialogue audible at increased distances.For point sources, spatial mixers supporttwo different directivity modeling algorithms.You can add cardioid directivity modelingto your spatial mix.Using some simple modifications,you could model a human speakerwith a cardioid directivity patternor the sound of an acoustic stringed instrumentwith a hyper-cardioid pattern.You can also add cone directivity modeling.This classic mode allows you to limit directivity filteringto within a specific range of rotation.Spatial mixers also supportgeometry-aware environmental effectsbased on a spatial pipeline.The spatial pipeline selects environmental effectsto enable or disable,as well as the send levels to each.PHASE currently supports direct path transmission,early reflections, and late reverb.Direct path transmission renders the direct and occludedpaths between the source and listener.Note that with occluded sound,some energy is absorbed by materials,while other energy is transmittedto the other side of the object.Early reflections provide both intensitymodification and coloration to the direct path.These are usually built from the specular reflectionsoff the walls and floor.In larger spaces, they also add noticeable echoesto the experience.Late reverb provides the sound of the environment.It's a dense build-up of the diffuse scattered energythat coalesces into the final audible representationof the space.In addition to providing cues for room size and shape,it's also responsible for giving you a sense of envelopment.Now that I've reviewed the conceptsbehind the PHASE engine,nodes and mixers, it's time to bring these concepts togetherwith some sample use cases.In this section, I'll walk you through playing an audio file,building a spatial audio experience,and building a behavioral sound event.These three key areaswill give you a wide overview of the capabilities of PHASE,with a gentle introduction at the beginning and a deep diveinto the more interesting capabilitiestoward the middle and end.To kick things off,I'll show you how to play an audio file.First, let's create a PHASE engine instance.Next, I'll retrieve the URL to an audio fileand register the sound asset with PHASE.I'll give it the name "drums" so I can refer to it later.Here I'll create an engineand register a sound asset in code.First, I'll create a PHASE engine instancein automatic update mode.This is the preferred mode to get things up and running,so I'm using it here for demoing simple playback.Note that when a game requires more precise synchronizationwith the frame update, manual mode is the way to go.Check out the documentation for more details.Next, I'll retrieve the URL to an audio filestored in the application bundle.This is a mono 24bit, 48kHz WAV filewith a prepared drum loop sample.When registering the sound asset with the engine, I'll providesome additional arguments.I'll give the sound asset a unique nameso I can refer to it later.I'll specify that the audio data within the sound assetshould be pre-loaded into resident memory,as opposed to streaming it into memory in real time.This should be fine since the drum loop is fairly short,and I might want to play it back several timesin short succession.I'm also choosing to normalize the sound assetfor calibrated loudness on the output device.In general, it's advised to normalize the input.This'll make it easier to mix the contentonce we assign it to a samplerand set its target output level.Now that I've registered a sound asset with the engine,I'll construct a sound event asset.First I'll create a channel mixer from a channel layout.Then I'll create a sampler node.The sampler node takes the nameof the registered sound assetand a reference to the downstream channel mixer.Next I'll set some basic properties on the sampler nodeto get it to play back correctly.The playback mode will set whether or notthe sampler will loop the audio file,and the calibration level will setthe perceived loudness of the sampler within the mix.Now that I've hooked the output of the sampler nodeto the input of the channel mixerand set some basic parameters,it's time to register the sound event asset with the engine.In this case, I'll register the sound event assetwith the name drumEvent,which I'll use to refer to it later.Here I'll register a sound event asset in code.I'll create a channelLayout from a mono ChannelLayoutTag.Then I'll initialize the channel mixer with the monochannelLayout.Next I'll create the sampler node and pass in the name drums,which refers to the mono drum assetI previously registered with the engine.The sampler node will be routed to the downstream channel mixer.I'll set the playbackMode to looping.This will ensure the sound continues to play backuntil I explicitly stop it from code.I'll set the CalibrationMode to relativeSpl,and the level to 0 decibels.This will ensure a comfortable listening levelfor the experience. Finally I'll registerthe soundEventAsset with the engine,passing in the name drumEvent so I can refer to it laterwhen I start creating sound events for playback.Once a sound event asset is registered,I can create an instance and start playback.The first thing I'll do is create a sound eventfrom the registered sound event asset named drumEvent.Now that I have a sound event, I'll goahead and start the engine.This will start the audio IO so I can listen to the audiofeeding the output device.Finally I'll start the sound event.At this point, the loaded sound assetwill be played through the sampler,routed to the channel mixer,remapped to the current output format,and played back through the output device.Here I'll start a soundEvent in code.The sound event asset is constructed from the nameof the registered sound asset.Here, I'm passing in drumEvent.I'll go ahead and start the engine,which will start the audio IO and start the sound event.Once I'm finished playing back the sound event,I can clean up the engine.First I'll stop the sound event.Then I'll stop the engine.This will stop the audio IO and stop any playing sound events.Next I'll unregister the sound event asset named drumEventand unregister the sound asset named drums.Finally I'll destroy the engine.Here I'll do the cleanup in code.First I'll stop the sound eventonce I'm finished listening to the drum loop.then I'll stop the engine, which will internally stopthe audio IO.Next I'll unregister the sound event asset named drumEventand unregister the sound asset named drums.Finally I'll destroy the engine.Now that I've covered the basics,I'll show you how to build a simplespatial audio experience in PHASE.We'll cover topics including spatial mixers,volumetric sound sources, and occluders.This first thing I'll do is register a sound event assetwith the engine.For this example,I'll start with an engine that already has the drums soundevent registered.From here, I'll upgrade the mix to gofrom simple channel-based playback to full spatialization.The first thing I'll do is construct a spatial pipelineto selectively apply different environmental effectsto my sound source.Then I'll create a spatial mixer from the spatial pipeline.Once constructed, I'll set some basic propertieson the spatial mixer to get it to play back correctly.In this example, I'll set the distance modelto control level attenuation over distance.I'll also set the directivity modelto control level attenuation based on the angle betweenthe source relative to the listener.Then I'll create a sampler node.The sampler node takes the name of the registered sound assetand a reference to the downstream spatial mixer.Next I'll set some basic properties on the sampler nodeto get it to play back correctly.In addition to the playback modeand calibration level, I'll also set the cull option here.This will tell PHASE what to dowhen the sampler becomes inaudible.Now that I've hooked the output of our sampler nodeto the input of the spatial mixerand set some basic parameters,it's time to register the sound event asset with the engine.I'll use the same name as before.Here I'll create a sound event asset in code.First I'll create a spatialPipelineto render .directPathTransmissionand .lateReverb. I'll also go ahead and setthe .lateReverb .sendLevelto control the direct-to-reverberant ratioand choose a .mediumRoom presetfor the late reverb simulation.Then I'll create a spatial mixer with the spatialPipeline.Next I'll create a natural-soundingGeometricSpreadingDistanceModeland assign it to the spatial mixer.I'll set the cullDistance to 10 meters.If the source were to go beyond this distance,I'd want to automatically cull it from the mix.And I'll adjust the rolloffFactor a littleto de-emphasize the distance attenuation effect.Then I'll create a sampler nodeand pass it the name "drums,"which refers to the mono drum sound asset I previouslyregistered with the engine.I'll set the playbackMode to .looping.I'll set the calibration mode to .relativeSpl,and the level to +12 decibels to increase the output levelof the sampler.And I'll set the cullOption to sleep.Finally, I'll register the soundEventAssetwith the engine, passing in the name drumEvent so I canrefer to it later when I start creating sound events.Now that I have a sound event asset registered with the engineI need to create a scene for the simulation.This involves creating a listener, source,and occluder.In this example, I'll place the occluderbetween the source and listener.First I'll create a listener.Then I'll set its transform.Once I'm ready to make the listener activewithin the scene graph, I'll attach it to the engine'sroot object or one of its children.Here I'll set up the listener in code.First I'll create a listener. Then I'll set its transform.In this example,I'll set the listener to the origin with no rotation.Finally I'll attach the listener to the engine's root object.Now let's set up a volumetric source.First I'll create a source shape from a mesh.Then I'll create a source from the shape.This inherently constructs a volumetric source.Next I'll set its transform.And once I'm ready to make the source activewithin the scene graph,I'll attach it to the engine's root objector one of its children.Here I'll set up a volumetric source in code.First I'll create an Icosahedron mesh and scale itto about the size of a HomePod Mini.Then I'll create a shape from the mesh.This shape can be reused to construct multiple instancesof a volumetric source.For example, I could place multiple HomePod Minisin the simulation that share the same mesh.Next I'll create a volumetric source from the shape.Note that I could also create a simple point source by usinga version of the initializerthat doesn't take a shape as input.Then I'll set its transform.I'll translate the source 2 meters in frontof the listener and rotate it back toward the listenerso they are facing each other.Finally I'll attach the source to the engine's root object.Now let's set up an occluder.First I'll create a shape from a mesh.Then I'll create a cardboard materialand assign it to the shape.Now the shape has a geometry and an associated material.Next I'll create an occluder from the shape.Then I'll set its transform.And once I'm ready to make the occluder activewithin the scene graph,I'll attach it to the engine's root objector one of its children.Here I'll set up the occluder in code.First I'll create a boxMeshand scale its dimensions accordingly.Then I'll create a shape from the mesh.This shape can be reused to constructmultiple instances of an occluder.For example, I can place multiple boxes in the simulationthat share the same mesh.Next, I'll create a material from a cardboard box presetand assign the material to the shape.Next I'll create an occluder from the shape.Then I'll set its transform.I'll translate the occluder 1 meter in front of the listenerand rotate it back toward the listenerso they are facing each other.This puts the occluder halfway between the sourceand listener.Finally I'll attach the occluder to the engine's root object.At this point, I have a scene with an occluder halfwaybetween a source and listener.Next I'll create a sound event from our registeredsound event asset and associate itwith the source and listener in the scene graph.When I start the sound event, I'll hear an occluded drum loopplaying from a small volumetric sourceon the opposite side of a cardboard box.Here I'll start the sound event in code.First I'll associate the source and listenerwith the spatial mixer in the sound event.Then I'll create a soundEvent from the registered soundEvent assetnamed drumEvent.The rest is the same as before.Make sure the engine is running, then start the sound event.Now that I've covered spatial audio,I'll show you how to build a complex sound event.Sound events can be organized intobehavioral hierarchies for interactive sound design.In this section, I'll walk you through consecutive examplesthat build upon each type of sound event nodeto create the final sound event.Here we'll model an actor wearing a noisy Gore-Tex jacketwalking on different types of terrainwith variable surface wetness.First I'll create a sampler node that plays back a footstepon creaky wood.In code, I'll create a sampler nodewith a registered sound assetnamed "footstep_wood_clip_1." For this example,this node and all others will play backon a single pre-constructed channel mixer.Now I'll add some randomness.I'll create a random node with two child sampler nodesthat play slightly different footstep on creaky wood samples.In code, I'll create two sampler nodes.The first uses a registered sound assetnamed "footstep_wood_clip_1and the second uses a registered sound assetnamed "footstep_wood_clip_2."Then , I'll create a random nodeand add the sampler nodes as children.Note that a weighting factor is applied to each child nodeto control the likelihood of that childbeing chosen over successive iterations.In this case, the first childhas twice the chance of being selected over the second child.Next I'll add a terrain switch.I'll create a switch node and two random nodes as children.In this case, the second random node plays back randomfootsteps on gravel as opposed to random footsteps on wood.I'll use a terrain parameter to control the switch.In code, I'll create two sampler nodes.The first uses a registered sound assetnamed "footstep_gravel_clip_1" and the seconduses a registered sound assetnamed "footstep_gravel_clip_2."Then I'll create a random nodeand add the sampler nodes as children.Next I'll create a terrain parameter.The default value will be "creaky_wood."Then I'll create a switch nodethat's controlled by the terrain parameter.I'll add two children, the wood random nodeand gravel random node.If I set the parameter to "creaky_wood,"it'll select the wood random node.Likewise, if I set the parameter to "soft_gravel,"it'll select the gravel random node.Next I'll add a wetness blend.I'll create a blend node with a terrain switch nodeand random splash node as children.The new random splash nodeplays random splash noises as the actor takes steps,while the terrain switch determinesif the actor's feet are walking on creaky woodor soft gravel.The blend between the dry footstep soundsand the splashes is dependent on the wetness parameter,from completely dry--loud footsteps and no splashes--to completely wet--quiet footstepsand loud splashes.In code, I'll create two sampler nodes.The first uses a registered sound asset named"splash_clip_1" and the second uses a registered sound assetnamed "splash_clip_2."Then I'll create a random node and add the sampler nodesas children.Next I'll create a wetness parameter.The range will be 0 to 1,and the default value will be 0.5.Note that I can set the parameterto any value and range supported by my game.Then I'll create a blend nodethat's controlled by the wetness parameter.I'll add two children, the terrain switch nodeand random splash node.If I set the parameter to 0,I'll only hear dry footsteps on creaky wood or gravel,depending on the terrain.As I increase the wetness from 0 to 1,I'll increase the loudness of splash noisesthat accompany each footstep, simulating wet terrain.Finally I'll create a container node with the wetnessblend node and a random noisy clothing node as children.The new noisy clothing node plays backthe ruffling noises of a Gore-Tex jacketwhenever the actor takes a stepon changing terrain with variable wetness.With this final node hierarchy in place,I have a complete representationof the actor walking in a scene.Every time the actor takes a step,I'll hear the ruffling of the jacket,plus footsteps on creaky wood or soft gravel,depending on the terrain parameter.In addition to this, I'll hear more or lessof the splashes with each footstep,depending on the wetness parameter.In code, I'll create two sampler nodes.The first uses a registered sound asset named"gortex_clip_1" and the second uses a registered sound assetnamed "gortex_clip_2."Then I'll create a random node and add the sampler nodesas children.Finally I'll create an actor_container node.I'll add two children, the wetness_blend nodeand noisy_clothing_random node.Together, they represent the complete soundof the actor.In review, we learned how to play an audio file.Following this, we expanded our knowledge by divingheadfirst into constructinga simple yet effective spatial audio experience.Here we learned about listeners,volumetric sources, and occluders.Finally, we learned about building behavioral sound eventsfor interactive sound design.Here we learned about grafting random,switch, blend, and container nodes togetherto form hierarchical, interactive sound events.Taken together,you should now have a broad understanding of the innerworkings of PHASEand be able to take a deeper dive into the underlying systemcomponents when you're ready to settle in and build yournext geometry-aware game audio experience.Thank you. Have a great WWDC21.[upbeat music]

Hello, my name is Bharath,and I am from Apple's Core Audio team.And today, and I would like to talk to youabout geometry-aware audio with PHASE.We will talk about why you would want to usethe new PHASE framework.We will introduce you to some of the featuresthe framework offers.Then, I will hand it over to my colleague, David Thall,to take you through concepts in some depthand sample use cases with this API.Let's begin.

Audio is a critical aspect of any gaming experience.Spatial audio on headphonestakes the overall gameplay to the next leveland makes you feel more involved.In games today,various subsystems of the engine like physics,animation, visual effects, et cetera,all communicate with each other and movethe game flow or storyline forwardbased on the player's actions.However, the audio subsystem is generally managedand driven separately from the rest.They sometimes are also authored through middlewarethat isn't always aware of the simulation.

Audio assets are post-produced, pre-baked,hand-tuned to tell an audio storythat matches the game visuals.As visuals evolve, audio system, sound design,and associated assets needs to be regeneratedto keep audio experience cohesiveon a wide range of platforms.This iterative development process needs to beaccounted for during the game development.This typically leads to audio experienceslagging the visual side of the gameplay.

To provide a better game audio experience,we want to bring the audio systemcloser to other subsystems.We also want to make it easier for youto write applications that can providea consistent spatial audio experienceon all these supported devices.

Let us now take you throughthe new audio framework PHASE and its features.PHASE is a new framework that will enable you to providegeometry information to the audio engine,help you build a sound-design friendly,event-driven audio playback system,enable you to write applications that can automatically providea consistent spatial audio experienceacross all supported devices and can be integratedwith existing authoring solutions and pipelines.Before we jump into learning more about PHASE,let us review a commonly used game audio workflow.Here's an example outdoor scenewith a listener, a sound source,a creek flowing and an occluder, a barn.

Occluders are objects in the scenethat could dampen the sound between a sourceand the listener.Typically, you would placemultiple point sources along an area.As the listener moves,you have to use various techniquessuch as ray tracingto determine the proper filtering and mix ratiobetween the point sourcesand manually blend between themto provide a good audio experience.During the natural course of the game development,if visual scenes change, for example,the barn in the example scene,you have to update and hand-tune the audio experienceto match the visual scene change.Imagine you can build applicationswhere audio sources are not pointsyou need to manage and mix based on the scenebut rather as sound emanating over an areaor volume that the audio systemcan automatically manage for you.PHASE does exactly that.Introducing volumetric sources.The new framework provides API that allows you to pass soundsources as geometric shapes to the underlying audio engine.In addition to volumetric sound sources,you can also pass occludersin the scene as geometric shapes.You can also choose acoustic material propertiesfrom a set of presets and attach them to occluders.PHASE framework also allows you to set medium propagationand source directivity for point sourcesas required by your application.We are looking at an outdoor sceneas an example here.However, if your application has indoor scenes,you can choose early reflectionsand late reverberation properties froma library of presets.Once you tell the framework where various sound sources,occluders, and the listener are,PHASE will help with the heavy lifting and modelthe occlusion and spreading effects of various sound sourcesin the scene for you.Now that your application's audio system is geometry aware,it can adapt to visual scene changesmuch quicker as the game development evolves.

In addition to geometry awareness,PHASE also provides an event-basedinteractive playback system.Sound events are basic units for describing audio playback eventsin PHASE.They encapsulate the selection, blending,and playback of audio assets.Sound events can range from simple eventslike one-shot playback, looping,to complex sequences organized as a treethat can blend or switch between subtreesof playback events.Let us take a simple example of playing footsteps.Here we have a random nodethat will automatically choose between three differentsounds of a footstep on gravel.We can have another sound event for cloth rustle.Both event trees can be grafted onto another tree,"near" in this example,to play back a mix of cloth rustle and footsteps sounds.We can have another tree, "far" in this example,to play a different set of soundswhen the character is at a distance.The mix of near and far treescan be now controlled by distance based on the gameplay.You can add more event treeslike footstep on snow or grass, for example,and build a complex sequence of playback eventsthat can be triggered by user interaction or by subsystemslike physics and animation.

With PHASE, sounds can be played either to a simplechannel configuration or in a 3D spacewith orientation and position, or as ambient bedswhere the sounds have orientation but no position.The underlying engine is built uponour spatial audio rendering capabilitiesalready available on supported iOS, macOS devices,and also Air Pods family of headphones.This enables you to build applicationsthat provide a consistent spatial audio experienceon all the supported devices automatically.Next, I would like to invite David Thallto dive deeper into PHASE and talk more aboutthe concepts and the API for some example use cases.Hi, everyone, my name is David Thall,and I'm System Architect and Development Lead on PHASE.Today I'm going to walk you through the PHASE API.In this section, I'll introduce you to the general concepts.Following this, I'll run through some sample use casesto get you started.The PHASE API can be separated into three main concepts.The engine manages assets.Nodes control playback.And mixers control spatialization.The PHASE engine can be broken down intothree main sections.The asset registry, scene graph, and rendering state.Throughout the engine life cycle you'll register and unregisterassets with the engine.Today, PHASE supports registering sound assetsand sound event assets.Sound assets can be loaded directly from an audio fileor packed and embedded as raw audio datain your own assets and loaded directly into the engine.Sound event assets are a collection of one or morehierarchical nodes that control sound playbackand downstream mixers that control spatialization.The scene graph is a hierarchy of objects that take partin the simulation.This includes listeners, sources, and occluders.A listener is an object that represents the location in spacefrom which you hear the simulation.A source is an objectthat represents where sound originates.As Bharath mentioned earlier,PHASE supports both point sourcesand volumetric sources.An occluder is an object that represents the geometryin the simulation that affects sound transmissionas it moves through the environment.Occluders are also assigned materials that affecthow they absorb and transmit sound.PHASE comes with a library of material presets thatcan be assigned to occludersto simulate everything from cardboard boxesto glass windows to brick walls.As you add objects to your scene, you'll organize them intoa hierarchy and attach them to the engine's root object,either directly or indirectly.This will ensure they take part in the simulationfrom frame to frame.The rendering state manages playing sound eventsand audio IO.When you first create the engine, audio IO is disabled.This allows you to register your assets,build your scene graph, construct sound events,and perform other engine operations,all without having to run audio IO.Once you're ready to play back sound events,you can start the engine,which will internally start the audio IO.Likewise, when you're finished playing back sound events,you can stop the engine.This will stop the audio IOand stop any playing sound events.Nodes in PHASE control playback of audio content.Nodes are a hierarchical collection of objectsthat either generate or control audio playback.Generator nodes produce audio.They are always leaf nodes in a node hierarchy.Control nodes set the logic for how generators are selected,mixed and parameterized before spatialization.Control nodes are always parent nodesand can be organized into hierarchiesfor complex sound design scenarios.A sampler node is a type of generator node.Samplers play back registered sound assets.Once constructed, you can set some basic propertieson the sampler node to get it to play back correctly.The playback mode determines how the audio file will be played.If you set the playback mode to OneShot,the audio file will play once and automatically stop itself.This can be used in "fire-and-forget" scenarios,such as triggering sound effects.If you set the playback mode to looping,the audio file will play indefinitely,until you explicitly stop the sampler.The cull option tells PHASE what to dowhen the sound becomes inaudible.If you set the cull option to terminate,the sound will automatically stop itselfwhen it becomes inaudible.If you set the cull option to sleep,the sound will stop rendering when it becomes inaudibleand start rendering again when it becomes audible.This makes it so you don't have tomanually start and stop soundswhen they are culled by the engine.The calibration level sets the real-world level of the soundin decibels SPL.PHASE also supports four types of control nodes.These include random, switch, blend and container nodes.The random node selects one of its childrenaccording to a weighted random choice.For example, in this case,the left sampler has 4:1 odds over the right sampler of beingselected the next time the sound event is triggered.The switch node switches between its childrenbased on a parameter name.For example, you could change the terrain switchfrom "creaky wood" to "soft gravel."The next time the sound event is triggered,it'll select the sampler that matches the parameter name.The blend node blends between its childrenbased on a parameter value.For example, you could assign a wetness parameterto a blend node,which could blend between a loud footstepand quiet splash on the dry endand a quiet footstep and loud splash on the wet end.The container node plays all its children at once.For example, you could have one samplerthat plays back a footstepand another sampler that plays back the sound of clothing,like the sound of a ruffling Gor-Tex jacket.Every time the container is triggered,both samplers will play back at the same time.

Mixers in PHASE control spatialization of audio content.PHASE currently supports channel,ambient, and spatial mixers.Channel mixers render audio without spatializationand environmental effects.Use channel mixers for regular stem-based content thatneeds to be rendered directly to the output device, such asstereo music or center channel narrative dialogue.Ambient mixers render audio with externalizationbut without distance modeling or environmental effects.As the listener rotates their head,the sound will continue to come fromthe same relative location in space.Use ambient mixers for multichannel contentthat isn't being simulated in the environment but shouldstill sound like it's coming from somewhere out in space,for example, a background of crickets chirpingin a large forest.Spatial mixers perform full spatialization.As the sound source moves relative to the listener,you'll hear changes in perceived location,level, and frequency response based on panning,distance modeling, and directivity modeling algorithms.In addition to this,geometry-aware environmental effectsare applied to the path between the source and listener.If you're wearing headphones, you'll also get externalizationthrough the application of binaural filters.Use spatial mixers for soundsthat should take part in the full environmental simulation.Spatial mixers supporttwo unique distance modeling algorithms.You can set up standard geometric spreading lossfor natural attenuation over distance.You can also increase or decrease the effectto your liking.For example, lowering the value could be usefulif you wanted to boom mic a conversation at a distance.On the other end of the spectrum,you can add full piecewise curvedsegments of attenuation over distance.For example, you could construct a set of segmentswith natural distance attenuationat the start and end of the range,but decrease the attenuation in the middleto keep important dialogue audible at increased distances.For point sources, spatial mixers supporttwo different directivity modeling algorithms.You can add cardioid directivity modelingto your spatial mix.Using some simple modifications,you could model a human speakerwith a cardioid directivity patternor the sound of an acoustic stringed instrumentwith a hyper-cardioid pattern.You can also add cone directivity modeling.This classic mode allows you to limit directivity filteringto within a specific range of rotation.Spatial mixers also supportgeometry-aware environmental effectsbased on a spatial pipeline.The spatial pipeline selects environmental effectsto enable or disable,as well as the send levels to each.PHASE currently supports direct path transmission,early reflections, and late reverb.Direct path transmission renders the direct and occludedpaths between the source and listener.Note that with occluded sound,some energy is absorbed by materials,while other energy is transmittedto the other side of the object.Early reflections provide both intensitymodification and coloration to the direct path.These are usually built from the specular reflectionsoff the walls and floor.In larger spaces, they also add noticeable echoesto the experience.Late reverb provides the sound of the environment.It's a dense build-up of the diffuse scattered energythat coalesces into the final audible representationof the space.In addition to providing cues for room size and shape,it's also responsible for giving you a sense of envelopment.Now that I've reviewed the conceptsbehind the PHASE engine,nodes and mixers, it's time to bring these concepts togetherwith some sample use cases.In this section, I'll walk you through playing an audio file,building a spatial audio experience,and building a behavioral sound event.These three key areaswill give you a wide overview of the capabilities of PHASE,with a gentle introduction at the beginning and a deep diveinto the more interesting capabilitiestoward the middle and end.To kick things off,I'll show you how to play an audio file.First, let's create a PHASE engine instance.Next, I'll retrieve the URL to an audio fileand register the sound asset with PHASE.I'll give it the name "drums" so I can refer to it later.Here I'll create an engineand register a sound asset in code.First, I'll create a PHASE engine instancein automatic update mode.This is the preferred mode to get things up and running,so I'm using it here for demoing simple playback.Note that when a game requires more precise synchronizationwith the frame update, manual mode is the way to go.Check out the documentation for more details.Next, I'll retrieve the URL to an audio filestored in the application bundle.This is a mono 24bit, 48kHz WAV filewith a prepared drum loop sample.When registering the sound asset with the engine, I'll providesome additional arguments.I'll give the sound asset a unique nameso I can refer to it later.I'll specify that the audio data within the sound assetshould be pre-loaded into resident memory,as opposed to streaming it into memory in real time.This should be fine since the drum loop is fairly short,and I might want to play it back several timesin short succession.I'm also choosing to normalize the sound assetfor calibrated loudness on the output device.In general, it's advised to normalize the input.This'll make it easier to mix the contentonce we assign it to a samplerand set its target output level.Now that I've registered a sound asset with the engine,I'll construct a sound event asset.First I'll create a channel mixer from a channel layout.Then I'll create a sampler node.The sampler node takes the nameof the registered sound assetand a reference to the downstream channel mixer.Next I'll set some basic properties on the sampler nodeto get it to play back correctly.The playback mode will set whether or notthe sampler will loop the audio file,and the calibration level will setthe perceived loudness of the sampler within the mix.Now that I've hooked the output of the sampler nodeto the input of the channel mixerand set some basic parameters,it's time to register the sound event asset with the engine.In this case, I'll register the sound event assetwith the name drumEvent,which I'll use to refer to it later.Here I'll register a sound event asset in code.I'll create a channelLayout from a mono ChannelLayoutTag.Then I'll initialize the channel mixer with the monochannelLayout.Next I'll create the sampler node and pass in the name drums,which refers to the mono drum assetI previously registered with the engine.The sampler node will be routed to the downstream channel mixer.I'll set the playbackMode to looping.This will ensure the sound continues to play backuntil I explicitly stop it from code.I'll set the CalibrationMode to relativeSpl,and the level to 0 decibels.This will ensure a comfortable listening levelfor the experience. Finally I'll registerthe soundEventAsset with the engine,passing in the name drumEvent so I can refer to it laterwhen I start creating sound events for playback.Once a sound event asset is registered,I can create an instance and start playback.The first thing I'll do is create a sound eventfrom the registered sound event asset named drumEvent.Now that I have a sound event, I'll goahead and start the engine.This will start the audio IO so I can listen to the audiofeeding the output device.Finally I'll start the sound event.At this point, the loaded sound assetwill be played through the sampler,routed to the channel mixer,remapped to the current output format,and played back through the output device.Here I'll start a soundEvent in code.The sound event asset is constructed from the nameof the registered sound asset.Here, I'm passing in drumEvent.I'll go ahead and start the engine,which will start the audio IO and start the sound event.

Once I'm finished playing back the sound event,I can clean up the engine.First I'll stop the sound event.Then I'll stop the engine.This will stop the audio IO and stop any playing sound events.Next I'll unregister the sound event asset named drumEventand unregister the sound asset named drums.Finally I'll destroy the engine.Here I'll do the cleanup in code.First I'll stop the sound eventonce I'm finished listening to the drum loop.then I'll stop the engine, which will internally stopthe audio IO.Next I'll unregister the sound event asset named drumEventand unregister the sound asset named drums.Finally I'll destroy the engine.

Now that I've covered the basics,I'll show you how to build a simplespatial audio experience in PHASE.We'll cover topics including spatial mixers,volumetric sound sources, and occluders.This first thing I'll do is register a sound event assetwith the engine.For this example,I'll start with an engine that already has the drums soundevent registered.From here, I'll upgrade the mix to gofrom simple channel-based playback to full spatialization.The first thing I'll do is construct a spatial pipelineto selectively apply different environmental effectsto my sound source.Then I'll create a spatial mixer from the spatial pipeline.Once constructed, I'll set some basic propertieson the spatial mixer to get it to play back correctly.In this example, I'll set the distance modelto control level attenuation over distance.I'll also set the directivity modelto control level attenuation based on the angle betweenthe source relative to the listener.Then I'll create a sampler node.The sampler node takes the name of the registered sound assetand a reference to the downstream spatial mixer.Next I'll set some basic properties on the sampler nodeto get it to play back correctly.In addition to the playback modeand calibration level, I'll also set the cull option here.This will tell PHASE what to dowhen the sampler becomes inaudible.Now that I've hooked the output of our sampler nodeto the input of the spatial mixerand set some basic parameters,it's time to register the sound event asset with the engine.I'll use the same name as before.Here I'll create a sound event asset in code.First I'll create a spatialPipelineto render .directPathTransmissionand .lateReverb. I'll also go ahead and setthe .lateReverb .sendLevelto control the direct-to-reverberant ratioand choose a .mediumRoom presetfor the late reverb simulation.Then I'll create a spatial mixer with the spatialPipeline.Next I'll create a natural-soundingGeometricSpreadingDistanceModeland assign it to the spatial mixer.I'll set the cullDistance to 10 meters.If the source were to go beyond this distance,I'd want to automatically cull it from the mix.And I'll adjust the rolloffFactor a littleto de-emphasize the distance attenuation effect.Then I'll create a sampler nodeand pass it the name "drums,"which refers to the mono drum sound asset I previouslyregistered with the engine.I'll set the playbackMode to .looping.I'll set the calibration mode to .relativeSpl,and the level to +12 decibels to increase the output levelof the sampler.And I'll set the cullOption to sleep.Finally, I'll register the soundEventAssetwith the engine, passing in the name drumEvent so I canrefer to it later when I start creating sound events.Now that I have a sound event asset registered with the engineI need to create a scene for the simulation.This involves creating a listener, source,and occluder.In this example, I'll place the occluderbetween the source and listener.First I'll create a listener.Then I'll set its transform.Once I'm ready to make the listener activewithin the scene graph, I'll attach it to the engine'sroot object or one of its children.Here I'll set up the listener in code.First I'll create a listener. Then I'll set its transform.In this example,I'll set the listener to the origin with no rotation.Finally I'll attach the listener to the engine's root object.

Now let's set up a volumetric source.First I'll create a source shape from a mesh.Then I'll create a source from the shape.This inherently constructs a volumetric source.Next I'll set its transform.And once I'm ready to make the source activewithin the scene graph,I'll attach it to the engine's root objector one of its children.Here I'll set up a volumetric source in code.First I'll create an Icosahedron mesh and scale itto about the size of a HomePod Mini.Then I'll create a shape from the mesh.This shape can be reused to construct multiple instancesof a volumetric source.For example, I could place multiple HomePod Minisin the simulation that share the same mesh.

Next I'll create a volumetric source from the shape.Note that I could also create a simple point source by usinga version of the initializerthat doesn't take a shape as input.Then I'll set its transform.I'll translate the source 2 meters in frontof the listener and rotate it back toward the listenerso they are facing each other.Finally I'll attach the source to the engine's root object.

Now let's set up an occluder.First I'll create a shape from a mesh.

Then I'll create a cardboard materialand assign it to the shape.Now the shape has a geometry and an associated material.Next I'll create an occluder from the shape.Then I'll set its transform.And once I'm ready to make the occluder activewithin the scene graph,I'll attach it to the engine's root objector one of its children.

Here I'll set up the occluder in code.First I'll create a boxMeshand scale its dimensions accordingly.Then I'll create a shape from the mesh.This shape can be reused to constructmultiple instances of an occluder.For example, I can place multiple boxes in the simulationthat share the same mesh.Next, I'll create a material from a cardboard box presetand assign the material to the shape.

Next I'll create an occluder from the shape.Then I'll set its transform.I'll translate the occluder 1 meter in front of the listenerand rotate it back toward the listenerso they are facing each other.This puts the occluder halfway between the sourceand listener.Finally I'll attach the occluder to the engine's root object.

At this point, I have a scene with an occluder halfwaybetween a source and listener.

Next I'll create a sound event from our registeredsound event asset and associate itwith the source and listener in the scene graph.When I start the sound event, I'll hear an occluded drum loopplaying from a small volumetric sourceon the opposite side of a cardboard box.Here I'll start the sound event in code.First I'll associate the source and listenerwith the spatial mixer in the sound event.Then I'll create a soundEvent from the registered soundEvent assetnamed drumEvent.The rest is the same as before.Make sure the engine is running, then start the sound event.Now that I've covered spatial audio,I'll show you how to build a complex sound event.Sound events can be organized intobehavioral hierarchies for interactive sound design.In this section, I'll walk you through consecutive examplesthat build upon each type of sound event nodeto create the final sound event.Here we'll model an actor wearing a noisy Gore-Tex jacketwalking on different types of terrainwith variable surface wetness.First I'll create a sampler node that plays back a footstepon creaky wood.In code, I'll create a sampler nodewith a registered sound assetnamed "footstep_wood_clip_1." For this example,this node and all others will play backon a single pre-constructed channel mixer.Now I'll add some randomness.I'll create a random node with two child sampler nodesthat play slightly different footstep on creaky wood samples.In code, I'll create two sampler nodes.The first uses a registered sound assetnamed "footstep_wood_clip_1and the second uses a registered sound assetnamed "footstep_wood_clip_2."Then , I'll create a random nodeand add the sampler nodes as children.Note that a weighting factor is applied to each child nodeto control the likelihood of that childbeing chosen over successive iterations.In this case, the first childhas twice the chance of being selected over the second child.

Next I'll add a terrain switch.I'll create a switch node and two random nodes as children.In this case, the second random node plays back randomfootsteps on gravel as opposed to random footsteps on wood.I'll use a terrain parameter to control the switch.In code, I'll create two sampler nodes.The first uses a registered sound assetnamed "footstep_gravel_clip_1" and the seconduses a registered sound assetnamed "footstep_gravel_clip_2."Then I'll create a random nodeand add the sampler nodes as children.Next I'll create a terrain parameter.The default value will be "creaky_wood."Then I'll create a switch nodethat's controlled by the terrain parameter.I'll add two children, the wood random nodeand gravel random node.If I set the parameter to "creaky_wood,"it'll select the wood random node.Likewise, if I set the parameter to "soft_gravel,"it'll select the gravel random node.

Next I'll add a wetness blend.I'll create a blend node with a terrain switch nodeand random splash node as children.The new random splash nodeplays random splash noises as the actor takes steps,while the terrain switch determinesif the actor's feet are walking on creaky woodor soft gravel.The blend between the dry footstep soundsand the splashes is dependent on the wetness parameter,from completely dry--loud footsteps and no splashes--to completely wet--quiet footstepsand loud splashes.In code, I'll create two sampler nodes.The first uses a registered sound asset named"splash_clip_1" and the second uses a registered sound assetnamed "splash_clip_2."Then I'll create a random node and add the sampler nodesas children.Next I'll create a wetness parameter.The range will be 0 to 1,and the default value will be 0.5.Note that I can set the parameterto any value and range supported by my game.Then I'll create a blend nodethat's controlled by the wetness parameter.I'll add two children, the terrain switch nodeand random splash node.If I set the parameter to 0,I'll only hear dry footsteps on creaky wood or gravel,depending on the terrain.As I increase the wetness from 0 to 1,I'll increase the loudness of splash noisesthat accompany each footstep, simulating wet terrain.

Finally I'll create a container node with the wetnessblend node and a random noisy clothing node as children.The new noisy clothing node plays backthe ruffling noises of a Gore-Tex jacketwhenever the actor takes a stepon changing terrain with variable wetness.With this final node hierarchy in place,I have a complete representationof the actor walking in a scene.Every time the actor takes a step,I'll hear the ruffling of the jacket,plus footsteps on creaky wood or soft gravel,depending on the terrain parameter.In addition to this, I'll hear more or lessof the splashes with each footstep,depending on the wetness parameter.In code, I'll create two sampler nodes.The first uses a registered sound asset named"gortex_clip_1" and the second uses a registered sound assetnamed "gortex_clip_2."Then I'll create a random node and add the sampler nodesas children.Finally I'll create an actor_container node.I'll add two children, the wetness_blend nodeand noisy_clothing_random node.Together, they represent the complete soundof the actor.In review, we learned how to play an audio file.Following this, we expanded our knowledge by divingheadfirst into constructinga simple yet effective spatial audio experience.Here we learned about listeners,volumetric sources, and occluders.Finally, we learned about building behavioral sound eventsfor interactive sound design.Here we learned about grafting random,switch, blend, and container nodes togetherto form hierarchical, interactive sound events.Taken together,you should now have a broad understanding of the innerworkings of PHASEand be able to take a deeper dive into the underlying systemcomponents when you're ready to settle in and build yournext geometry-aware game audio experience.Thank you. Have a great WWDC21.[upbeat music]

18:31 -Create an Engine and Register a Sound Asset

20:47 -Register a Sound Event Asset

22:21 -Start a Sound Event

23:05 -Cleanup

25:14 -Create a Sound Event Asset

27:05 -Set Up a Listener

27:46 -Set Up a Volumetric Source

29:15 -Set Up an Occluder

30:33 -Start a Spatial Sound Event

31:28 -Example 1: Footstep on creaky wood

31:54 -Example 2: Random footsteps on creaky wood

32:47 -Example 3: Random footsteps on creaky wood or soft gravel

34:08 -Example 4: Random footsteps on changing terrain with a variably wet surface

35:53 -Example 5: Random footsteps on changing terrain with a variably wet surface and noisy Gore-Tex Jacket

## Code Samples

```swift
// Create an Engine in Automatic Update Mode.


let
 engine 
=
 
PHASEEngine
(updateMode: .automatic)


// Retrieve the URL to an Audio File stored in our Application Bundle.


let
 audioFileUrl 
=
 
Bundle
.main.url(forResource: 
"DrumLoop_24_48_Mono"
, withExtension: 
"wav"
)
!



// Register the Audio File at the URL.


// Name it "drums", load it into resident memory and apply dynamic normalization to prepare it for playback.


let
 soundAsset 
=
 
try
 engine.assetRegistry.registerSoundAsset(url: audioFileUrl,
                                                             identifier: 
"drums"
,
                                                             assetType: .resident,
                                                             channelLayout: 
nil
,
                                                             normalizationMode: .dynamic)
```

```swift
// Create a Channel Layout from a Mono Layout Tag.


let
 channelLayout 
=
 
AVAudioChannelLayout
(layoutTag: kAudioChannelLayoutTag_Mono)
!

     

// Create a Channel Mixer from the Channel Layout.


let
 channelMixerDefinition 
=
 
PHASEChannelMixerDefinition
(channelLayout: channelLayout)


// Create a Sampler Node from "drums" and hook it into the downstream Channel Mixer.


let
 samplerNodeDefinition 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"drums"
,
                                                       mixerDefinition: channelMixerDefinition)


// Set the Sampler Node's Playback Mode to Looping.

samplerNodeDefinition.playbackMode 
=
 .looping;


// Set the Sampler Node's Calibration Mode to Relative SPL and Level to 0 dB.

samplerNodeDefinition.setCalibrationMode(.relativeSpl, level: 
0
)


// Register a Sound Event Asset with the Engine named "drumEvent".


let
 soundEventAsset 
=
 
try
 engine.assetRegistry.registerSoundEventAsset(rootNode:samplerNodeDefinition,
                                             identifier: 
"drumEvent"
)
```

```swift
// Create a Sound Event from the Sound Event Asset "drumEvent".


let
 soundEvent 
=
 
try
 
PHASESoundEvent
(engine: engine, assetIdentifier: 
"drumEvent"
)


// Start the Engine.


// This will internally start the Audio IO Thread.


try
 engine.start()


// Start the Sound Event.


try
 soundEvent.start()
```

```swift
// Stop and invalidate the Sound Event.

soundEvent.stopAndInvalidate()


// Stop the Engine.


// This will internally stop the Audio IO Thread.

engine.stop()


// Unregister the Sound Event Asset.

engine.assetRegistry.unregisterAsset(identifier: 
"drumEvent"
, completionHandler:
nil
)


// Unregister the Audio File.

engine.assetRegistry.unregisterAsset(identifier: 
"drums"
, completionHandler:
nil
)


// Destroy the Engine.

engine 
=
 
nil
```

```swift
// Create a Spatial Pipeline.


let
 spatialPipelineOptions: 
PHASESpatialPipeline
.
Options
 
=
 [.directPathTransmission, .lateReverb]

let
 spatialPipeline 
=
 
PHASESpatialPipeline
(options: spatialPipelineOptions)
!

spatialPipeline.entries[
PHASESpatialCategory
.lateReverb]
!
.sendLevel 
=
 
0.1
;
engine.defaultReverbPreset 
=
 .mediumRoom


// Create a Spatial Mixer with the Spatial Pipeline.


let
 spatialMixerDefinition 
=
 
PHASESpatialMixerDefinition
(spatialPipeline: spatialPipeline)


// Set the Spatial Mixer's Distance Model.


let
 distanceModelParameters 
=
 
PHASEGeometricSpreadingDistanceModelParameters
()
distanceModelParameters.fadeOutParameters 
=
 
PHASEDistanceModelFadeOutParameters
(cullDistance: 
10.0
)
distanceModelParameters.rolloffFactor 
=
 
0.25

spatialMixerDefinition.distanceModelParameters 
=
 distanceModelParameters


// Create a Sampler Node from "drums" and hook it into the downstream Spatial Mixer.


let
 samplerNodeDefinition 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"drums"
, mixerDefinition:spatialMixerDefinition)


// Set the Sampler Node's Playback Mode to Looping.

samplerNodeDefinition.playbackMode 
=
 .looping


// Set the Sampler Node's Calibration Mode to Relative SPL and Level to 12 dB.

samplerNodeDefinition.setCalibrationMode(.relativeSpl, level: 
12
)


// Set the Sampler Node's Cull Option to Sleep.

samplerNodeDefinition.cullOption 
=
 .sleepWakeAtRealtimeOffset;


// Register a Sound Event Asset with the Engine named "drumEvent".


let
 soundEventAsset 
=
 
try
 engine.assetRegistry.registerSoundEventAsset(rootNode: samplerNodeDefinition, identifier: 
"drumEvent"
)
```

```swift
// Create a Listener.


let
 listener 
=
 
PHASEListener
(engine: engine)


// Set the Listener's transform to the origin with no rotation.

listener.transform 
=
 matrix_identity_float4x4;


// Attach the Listener to the Engine's Scene Graph via its Root Object.


// This actives the Listener within the simulation.


try
 engine.rootObject.addChild(listener)
```

```swift
// Create an Icosahedron Mesh.


let
 mesh 
=
 
MDLMesh
.newIcosahedron(withRadius: 
0.0142
, inwardNormals: 
false
, allocator:
nil
)


// Create a Shape from the Icosahedron Mesh.


let
 shape 
=
 
PHASEShape
(engine: engine, mesh: mesh)


// Create a Volumetric Source from the Shape.


let
 source 
=
 
PHASESource
(engine: engine, shapes: [shape])


// Translate the Source 2 meters in front of the Listener and rotated back toward the Listener.


var
 sourceTransform: simd_float4x4
sourceTransform.columns.
0
 
=
 simd_make_float4(
-
1.0
, 
0.0
, 
0.0
, 
0.0
)
sourceTransform.columns.
1
 
=
 simd_make_float4(
0.0
, 
1.0
, 
0.0
, 
0.0
)
sourceTransform.columns.
2
 
=
 simd_make_float4(
0.0
, 
0.0
, 
-
1.0
, 
0.0
)
sourceTransform.columns.
3
 
=
 simd_make_float4(
0.0
, 
0.0
, 
2.0
, 
1.0
)
source.transform 
=
 sourceTransform;


// Attach the Source to the Engine's Scene Graph.


// This actives the Listener within the simulation.


try
 engine.rootObject.addChild(source)
```

```swift
// Create a Box Mesh.


let
 boxMesh 
=
 
MDLMesh
.newBox(withDimensions: simd_make_float3(
0.6096
, 
0.3048
, 
0.1016
),
                             segments: simd_uint3(repeating: 
6
),
                             geometryType: .triangles,
                             inwardNormals: 
false
,
                             allocator: 
nil
)


// Create a Shape from the Box Mesh.


let
 boxShape 
=
 
PHASEShape
(engine: engine, mesh:boxMesh)


// Create a Material.


// In this case, we'll make it 'Cardboard'.


let
 material 
=
 
PHASEMaterial
(engine: engine, preset: .cardboard)


// Set the Material on the Shape.

boxShape.elements[
0
].material 
=
 material


// Create an Occluder from the Shape.


let
 occluder 
=
 
PHASEOccluder
(engine: engine, shapes: [boxShape])
    

// Translate the Occluder 1 meter in front of the Listener and rotated back toward the Listener.


// This puts the Occluder half way between the Source and Listener.


var
 occluderTransform: simd_float4x4
occluderTransform.columns.
0
 
=
 simd_make_float4(
-
1.0
, 
0.0
, 
0.0
, 
0.0
)
occluderTransform.columns.
1
 
=
 simd_make_float4(
0.0
, 
1.0
, 
0.0
, 
0.0
)
occluderTransform.columns.
2
 
=
 simd_make_float4(
0.0
, 
0.0
, 
-
1.0
, 
0.0
)
occluderTransform.columns.
3
 
=
 simd_make_float4(
0.0
, 
0.0
, 
1.0
, 
1.0
)
occluder.transform 
=
 occluderTransform


// Attach the Occluder to the Engine's Scene Graph.


// This actives the Occluder within the simulation.


try
 engine.rootObject.addChild(occluder)
```

```swift
// Associate the Source and Listener with the Spatial Mixer in the Sound Event.


let
 mixerParameters 
=
 
PHASEMixerParameters
()
mixerParameters.addSpatialMixerParameters(identifier: spatialMixerDefinition.identifier, source: source, listener: listener)


// Create a Sound Event from the built Sound Event Asset "drumEvent".


let
 soundEvent 
=
 
try
 
PHASESoundEvent
(engine: engine, assetIdentifier: 
"drumEvent"
, mixerParameters: mixerParameters)
```

```swift
// Create a Sampler Node from "footstep_wood_clip_1" and hook it into a Channel Mixer.


let
 footstep_wood_sampler_1 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"footstep_wood_clip_1"
, mixerDefinition: channelMixerDefinition)
```

```swift
// Create a Sampler Node from "footstep_wood_clip_1" and hook it into a Channel Mixer.


let
 footstep_wood_sampler_1 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"footstep_wood_clip_1"
, mixerDefinition: channelMixerDefinition)


// Create a Sampler Node from "footstep_wood_clip_2" and hook it into a Channel Mixer.


let
 footstep_wood_sampler_2 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"footstep_wood_clip_2"
, mixerDefinition: channelMixerDefinition)


// Create a Random Node.


// Add 'Footstep on Creaky Wood' Sampler Nodes as children of the Random Node.


// Note that higher weights increase the likelihood of that child being chosen.


let
 footstep_wood_random 
=
 
PHASERandomNodeDefinition
()
footstep_wood_random.addSubtree(footstep_wood_sampler_1, weight: 
2
)
footstep_wood_random.addSubtree(footstep_wood_sampler_2, weight: 
1
)
```

```swift
// Create a Sampler Node from "footstep_gravel_clip_1" and hook it into a Channel Mixer.


let
 footstep_gravel_sampler_1 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"footstep_gravel_clip_1"
, mixerDefinition: channelMixerDefinition)


// Create a Sampler Node from "footstep_gravel_clip_2" and hook it into a Channel Mixer.


let
 footstep_gravel_sampler_2 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"footstep_gravel_clip_2"
, mixerDefinition: channelMixerDefinition)


// Create a Random Node.


// Add 'Footstep on Soft Gravel' Sampler Nodes as children of the Random Node.


// Note that higher weights increase the likelihood of that child being chosen.


let
 footstep_gravel_random 
=
 
PHASERandomNodeDefinition
()
footstep_gravel_random.addSubtree(footstep_gravel_sampler_1, weight: 
2
)
footstep_gravel_random.addSubtree(footstep_gravel_sampler_2, weight: 
1
)


// Create a Terrain String MetaParameter.


// Set the default value to "creaky_wood".


let
 terrain 
=
 
PHASEStringMetaParameterDefinition
(value: 
"creaky_wood"
)


// Create a Terrain Switch Node.


// Add 'Random Footstep on Creaky Wood' and 'Random Footstep on Soft Gravel' as Children.


let
 terrain_switch 
=
 
PHASESwitchNodeDefinition
(switchMetaParameterDefinition: terrain)
terrain_switch.addSubtree(footstep_wood_random, switchValue: 
"creaky_wood"
)
terrain_switch.addSubtree(footstep_gravel_random, switchValue: 
"soft_gravel"
)
```

```swift
// Create a Sampler Node from "splash_clip_1" and hook it into a Channel Mixer.


let
 splash_sampler_1 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"splash_clip_1"
, mixerDefinition: channelMixerDefinition)
    

// Create a Sampler Node from "splash_clip_2" and hook it into a Channel Mixer.


let
 splash_sampler_2 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"splash_clip_2"
, mixerDefinition: channelMixerDefinition)


// Create a Random Node.


// Add 'Splash' Sampler Nodes as children of the Random Node.


// Note that higher weights increase the likelihood of that child being chosen.


let
 splash_random 
=
 
PHASERandomNodeDefinition
()
splash_random.addSubtree(splash_sampler_1, weight: 
9
)
splash_random.addSubtree(splash_sampler_2, weight: 
7
)


// Create a Wetness Number MetaParameter.


// The range is [0, 1], from dry to wet. The default value is 0.5.


let
 wetness 
=
 
PHASENumberMetaParameterDefinition
(value: 
0.5
, minimum: 
0
, maximum: 
1
)
    

// Create a 'Wetness' Blend Node that blends between dry and wet terrain.


// Add 'Terrain' Switch Node and 'Splash' Random Node as children.


// As you increase the wetness, the mix between the dry footsteps and splashes will change.


let
 wetness_blend 
=
 
PHASEBlendNodeDefinition
(blendMetaParameterDefinition: wetness)
wetness_blend.addRangeForInputValues(belowValue: 
1
, fullGainAtValue: 
0
, fadeCurveType: .linear, subtree: terrain_switch)
wetness_blend.addRangeForInputValues(aboveValue: 
0
, fullGainAtValue: 
1
, fadeCurveType: .linear, subTree: splash_random)
```

```swift
// Create a Sampler Node from "gortex_clip_1" and hook it into a Channel Mixer.


let
 noisy_clothing_sampler_1 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"gortex_clip_1"
, mixerDefinition: channelMixerDefinition)


// Create a Sampler Node from "gortex_clip_2" and hook it into a Channel Mixer.


let
 noisy_clothing_sampler_2 
=
 
PHASESamplerNodeDefinition
(soundAssetIdentifier: 
"gortex_clip_2"
, mixerDefinition: channelMixerDefinition)


// Create a Random Node.


// Add 'Noisy Clothing' Sampler Nodes as children of the Random Node.


// Note that higher weights increase the likelihood of that child being chosen.


let
 noisy_clothing_random 
=
 
PHASERandomNodeDefinition
()
noisy_clothing_random.addSubtree(noisy_clothing_sampler_1, weight: 
3
)
noisy_clothing_random.addSubtree(noisy_clothing_sampler_2, weight: 
5
)


// Create a Container Node.


// Add 'Wetness' Blend Node and 'Noisy Clothing' Random Node as children.


let
 actor_container 
=
 
PHASEContainerNodeDefinition
()
actor_container.addSubtree(wetness_blend)
actor_container.addSubtree(noisy_clothing_random)
```

