# Wwdc2021 10076

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Create 3D models with Object CaptureObject Capture provides a quick and easy way to create lifelike 3D models of real-world objects using just a few images. Learn how you can get started and bring your assets to life with Photogrammetry for macOS. And discover best practices with object selection and image capture to help you achieve the highest-quality results.ResourcesCapturing photographs for RealityKit Object CaptureCreating 3D objects from photographsCreating a Photogrammetry Command-Line AppExplore the RealityKit Developer ForumsPhotogrammetrySamplePhotogrammetrySessionTaking Pictures for 3D Object CaptureHD VideoSD VideoRelated VideosWWDC22Bring your world into augmented realityExplore USD tools and renderingWWDC21AR Quick Look, meet Object CaptureCreate 3D workflows with USDMonday@WWDC21

Object Capture provides a quick and easy way to create lifelike 3D models of real-world objects using just a few images. Learn how you can get started and bring your assets to life with Photogrammetry for macOS. And discover best practices with object selection and image capture to help you achieve the highest-quality results.

Capturing photographs for RealityKit Object Capture

Creating 3D objects from photographs

Creating a Photogrammetry Command-Line App

Explore the RealityKit Developer Forums

PhotogrammetrySample

PhotogrammetrySession

Taking Pictures for 3D Object Capture

HD VideoSD Video

HD Video

SD Video

Bring your world into augmented reality

Explore USD tools and rendering

AR Quick Look, meet Object Capture

Create 3D workflows with USD

Monday@WWDC21

Search this video…♪ Bass music playing ♪♪Michael Patrick Johnson: Hi!My name is Michael Patrick Johnson,and I am an engineer on the Object Capture team.Today, my colleague Dave McKinnon and Iwill be showing you how to turn real-world objectsinto 3D models using our new photogrammetry API on macOS.You may already be familiar with creating augmented reality appsusing our ARKit and RealityKit frameworks.You may have also used Reality Composerand Reality Converter to produce 3D models for AR.And now, with the Object Capture API,you can easily turn images of real-world objectsinto detailed 3D models.Let's say you have some freshly baked pizza in front of youon the kitchen table.Looks delicious, right?Suppose we want to capture the pizza in the foregroundas a 3D model.Normally, you'd need to hire a professional artistfor many hours to model the shape and texture.But, wait, it took you only minutes to bakein your own oven!With Object Capture, you start by taking photos of your objectfrom every angle.Next, you copy the images to a Macwhich supports the new Object Capture API.Using a computer vision techniquecalled "photogrammetry",the stack of 2D images is turned into a 3D modelin just minutes.The output model includes both a geometric meshas well as various material maps,and is ready to be dropped right into your appor viewed in AR Quick Look.Now let's look at each of these steps in slightly more detail.First, you capture photos of your object from all sides.Images can be taken on your iPhone or iPad,DSLR, or even a drone.You just need to make sure you get clear photosfrom all angles around the object.We will provide best practices for capturelater in the session.If you capture on iPhone or iPad,we can use stereo depth data from supported devicesto allow the recovery of the actual object size,as well as the gravity vectorso your model is automatically created right-side up.Once you have captured a folder of images,you need to copy them to your Macwhere you can use the Object Capture APIto turn them into a 3D model in just minutes.The API is supported on recent Intel-based Macs,but will run fastest on all the newest Apple silicon Macssince we can utilize the Apple Neural Engineto speed up our computer vision algorithms.We also provide HelloPhotogrammetry,a sample command-line app to help you get started.You can also use it directly on your folder of imagesto try building a model for yourselfbefore writing any code.Finally, you can preview the USDZ output modelsright on your Mac.We can provide models at four detail levelsoptimized for your different use cases,which we discuss in more detail later.Reduced, Medium, and Full details are ready to useright out of the box, like the pizza shown here.Raw is intended for custom workflows.By selecting USDZ output at the Medium detail level,you can view the new model in AR Quick Lookright on your iPhone or iPad.And that's all there is to getting lifelike objectsthat are optimized for AR!Oh wait, remember the pizzas from before?We have to come clean.This image wasn't really a photo,but was actually created using Object Captureon several pizzas.These models were then combined into this scenein a post-production tool and rendered using a ray tracerwith the advanced material maps.So you see, Object Capture can supporta variety of target use cases,from AR apps on an iPhone or iPadto film-ready production assets.In the remainder of this session,we'll show you how to get startedusing the Object Capture APIand then offer our best practicesto achieve the highest-quality results.In the getting started section,we'll go into more details about the Object Capture APIand introduce the essential code conceptsfor creating an app.Next we will discuss best practices for image capture,object selection, and detail-level selection.Let's begin by working through the essential stepsin using the API on macOS.In this section, you will learn the basic componentsof the Object Capture API and how to put them together.Let's say we have this cool new sneakerwe want to turn into a 3D model to view in AR.Here we see a graphical diagramof the basic workflow we will explore in this section.There are two main steps in the process:Setup, where we point to our set of images of an object;and then Process, where we request generationof the models we want to be constructed.First, we will focus on the Setup block,which consists of two substeps: creating a sessionand then connecting up its associated output stream.Once we have a valid session,we can use it to generate our models.The first thing we need to dois to create a PhotogrammetrySession.To create a session,we will assume you already have a folder of images of an object.We have provided some sample image capture foldersin the API documentation for you to get started quickly.A PhotogrammetrySession is the primary top-level classin the API and is the main point of control.A session can be thought of as a containerfor a fixed set of images to which photogrammetry algorithmswill be applied to produce the resulting 3D model.Here we have 123 HEIC images of the sneakertaken using an iPhone 12 Pro Max.Currently there are several waysto specify the set of images to use.The simplest is just a file URL to a directory of images.The session will ingest these one by oneand report on any problems encountered.If there is embedded depth data in HEIC images,it will automatically be usedto recover the actual scale of the object.Although we expect most people will prefer folder inputs,we also offer an interface for advanced workflowsto provide a sequence of custom samples.A PhotogrammetrySample includes the imageplus other optional data such as a depth map,gravity vector, or custom segmentation mask.Once you have created a session from an input source,you will make requests on it for model reconstruction.The session will output the resulting modelsas well as status messages on its output message stream.Now that we've seen what a session is,let's see how to create one using the API.Here we see the code to perform the initial setup of a sessionfrom a folder of images.The PhotogrammetrySessionlives within the RealityKit framework.First, we specify the input folder as a file URL.Here, we assume that we already have a folder on the local diskcontaining the images of our sneaker.Finally, we create the session by passing the URLas our input source.The initializer will throw an errorif the path doesn't exist or can't be read.You can optionally provideadvanced configuration parameters,but here we'll just use the defaults.That's all it takes to create a session!Now that we've successfully created a session object,we need to connect the session's output streamso that we can handle messages as they arrive.After the message stream is connected,we will see how to request modelsthat will then arrive on that stream.We use an AsyncSequence -- a new Swift feature this year --to provide the stream of outputs.Output messages include the results of requests,as well as status messages such as progress updates.Once we make the first process call,messages will begin to flow on the output message stream.The output message sequence will not endwhile the session is alive.It will keep producing messagesuntil either the session is deinitializedor in the case of a fatal error.Now, let's take a closer lookat the types of messages we will receive.After a request is made,we expect to receive periodic requestProgress messageswith the fraction completed estimate for each request.If you're building an app that calls the Object Capture API,you can use these to drive a progress bar for each requestto indicate status.Once the request is done processing,we receive a requestComplete messagecontaining the resulting payload,such as a model or a bounding box.If something went wrong during processing,a requestError will be output for that request instead.For convenience, a processingComplete messageis output when all queued requestshave finished processing.Now that we've been introducedto the concept of the session output streamand seen the primary output messages,let's take a look at some example codethat processes the message stream.Once we have this, we'll see how to request a model.Here is some code that creates an async taskthat handles messages as they arrive.It may seem like a lot of code,but most of it is simply message dispatching as we will see.We use a "for try await" loop to asynchronously iterateover the messages in session.outputs as they arrive.The bulk of the code is a message dispatcherwhich switches on the output message.Output is an enum with different message types and payloads.Each case statement will handle a different message.Let's walk through them.First, if we get a progress message,we'll just print out the value.Notice that we get progress messages for each request.For our example, when the request is complete,we expect the result payload to be a modelFilewith the URL to where the model was saved.We will see how to make such a request momentarily.If the request failed due to a photogrammetry error,we will instead get an error message for it.After the entire set of requestsfrom a process call has finished,a processingComplete message is generated.For a command-line app, you might exit the app here.Finally there are other status messages that you can read aboutin the documentation,such as warnings about images in a folderthat couldn't be loaded.And that's it for the message handling!This message-handling task will keep iteratingand handling messages asynchronouslyfor as long as the session is alive.OK, let's see where we are in our workflow.We've fully completed the Setup phaseand have a session ready to go.We're now ready to make requests to process the models.Before we jump into the code, let's take a closer lookat the various types of requests we can make.There are three different data types you can receivefrom a session: a ModelFile, a ModelEntity,and a BoundingBox.These types have an associated case in the Request enum:modelFile, modelEntity, and bounds;each with different parameters.The modelFile request is the most commonand the one we will use in our basic workflow.You simply create a modelFile requestspecifying a file URL with a USDZ extension,as well as a detail level.There is an optional geometry parameter for usein the interactive workflow, but we won't use that here.For more involved postprocessing pipelineswhere you may need USDA or OBJ output formats,you can provide an output directory URL instead,along with a detail level.The session will then write USDA and OBJ files into that folder,along with all the referenced assetssuch as textures and materials.A GUI app is also able to request a RealityKitModelEntity and BoundingBoxfor interactive preview and refinement.A modelEntity request also takes a detail leveland optional geometry.A bounds request will returnan estimated capture volume BoundingBox for the object.This box can be adjusted in a UIand then passed in the geometry argument of a subsequent requestto adjust the reconstruction volume.We'll see how this works a bit later in the session.Most requests also take a detail level.The preview level is intended only for interactive workflows.It is very low visual quality but is created the fastest.The primary detail levelsin order of increasing quality and sizeare Reduced, Medium, and Full.These levels are all ready to use out of the box.Additionally, the Raw level is provided for professional useand will need a post-production workflow to be used properly.We will discuss these in more detailin the best practices section.OK, now that we've seen what kinds of requests we can make,let's see how to do this in code.We will now see how to generate two models simultaneouslyin one call,each with a different output filename and detail level.Here we see the first call to process on the session.Notice that it takes an array of requests.This is how we can request two models at once.We will request one model at Reduced detail leveland one at Medium, each saving to a different USDZ file.Requesting all desired detail levelsfor an object capture simultaneously in one callallows the engine to share computationand will produce all the models fasterthan requesting them sequentially.You can even ask for all details levels at once.Process may immediately throw an error if a request is invalid,such as if the output location can't be written.This call returns immediatelyand soon messages will begin to appear on the output stream.And that's the end of the basic workflow!You create the session with your images,connect the output stream, and then request models.The processing time for each of your modelswill depend on the number of images and quality level.Once the processing is complete,you will receive the output messagethat the model is available.You can open the resulting USDZ file of the sneaker you createdright on your Mac and inspect the results in 3Dfrom any angle, including the bottom.Later in this session,we'll show you how to achieve coveragefor all sides of your object in one capture session,avoiding the need to combine multiple captures together.It's looking great!Now that you've seen the basic workflow,we will give a high-level overviewof a more advanced interactive workflowthat the Object Capture API also supports.The interactive workflow is designed to allowseveral adjustments to be made on a preview modelbefore the final reconstruction,which can eliminate the need for post-production model editsand optimize the use of memory.First, note that the Setup step and the Process stepon both ends of this workflow are the same as before.You will still create a session and connect the output stream.You will also request final models as before.However, notice that we've added a block in the middlewhere a 3D UI is presentedfor interactive editing of a preview model.This process is iterated until you are happy with the preview.You can then continue to make the final model requestsas before.You first request a preview modelby specifying a model request with detail level of preview.A preview model is of low visual qualityand is generated as quickly as possible.You can ask for a model file and load it yourselfor directly request a RealityKit ModelEntity to display.Typically, a bounds request is also made at the same timeto preview and edit the capture volume as well.You can adjust the capture volumeto remove any unwanted geometry in the capture,such as a pedestal needed to hold the object uprightduring capture.You can also adjust the root transform to scale,translate, and rotate the model.The geometry property of the request we saw earlierallows a capture volume and relative root transformto be provided before the model is generated.This outputs a 3D model that's ready to use.Let's look at this process in action.Here we see an example interactive Object Capture appwe created using the APIto demonstrate this interactive workflow.First, we select the Images foldercontaining images of a decorative rock,as well as an output folderwhere the final USDZ will be written.Then we hit Preview to request the preview modeland estimated capture volume.After some time has passed,the preview model of our rock and its capture volume appear.But let's say that we only wantthe top part of the rock in the outputas if the bottom were underground.We can adjust the bounding boxto avoid reconstructing the bottom of the model.Once we are happy,we hit Refine Model to produce a new previewrestricted to this modified capture volume.This also optimizes the output model for just this portion.Once the refined model is ready, the new preview appears.You can see the new model's geometry has been clippedto stay inside the box.This is useful for removing unwanted items in a capturesuch as a pedestal holding up an object.Once we are happy with the cropped preview,we can select a Full detail final renderwhich starts the creation process.After some time, the Full detail model is completeand replaces the preview model.Now we can see the Full detail of the actual model,which looks great.The model is saved in the output directoryand ready to use without the needfor any additional post-processing.And that's all there is to getting startedwith the new Object Capture API.We saw how to create a session from an input sourcesuch as a folder of images.We saw how to connect the async output streamto dispatch messages.We then saw how to requesttwo different level of detail models simultaneously.Finally, we described the interactive workflowwith an example RealityKit GUI app for ObjectCapture.Now I will hand it off to my colleague Dave McKinnon,who will discuss best practices with Object Capture.Dave McKinnon: Thanks, Michael.Hi, I'm Dave McKinnon, and I am an engineerworking on the Object Capture team.In the next section we’ll be covering best practicesto help you achieve the highest-quality results.First, we'll look into tips and tricksfor selecting an object that has the right characteristics.Followed by a discussion of how to controlthe environmental conditions and camerato get the best results.Next, we'll walk through how to use the CaptureSample App.This app allows you to capture imagesin addition to depth data and gravity informationto recover true scale and orientation of your object.We illustrate the use of this app for both in-handas well as turntable capture.Finally, we will discuss how to selectthe right output detail level for your use caseas well as providing some links for further reading.The first thing to consider when doing a scanis picking an object that has the right characteristics.For the best results,pick an object that has adequate texture detail.If the object contains texturelessor transparent regions,the resulting scan may lack detail.Additionally, try to avoid objects that containhighly reflective regions.If the object is reflective, you will get the best resultsby diffusing the lighting when you scan.If you plan to flip the object throughout the capture,make sure it is rigid so that it doesn't change shape.Lastly, if you want to scan an objectthat contains fine surface detail,you'll need to use a high-resolution camerain addition to having many close-up photosof the surface to recover the detail.We will now demonstrate the typical scanning process.Firstly, for best results,place your object on an uncluttered backgroundso the object clearly stands out.The basic process involves moving slowly around the objectbeing sure to capture it uniformly from all sides.If you would like to reconstruct the bottom of the object,flip it and continue to capture images.When taking the images,try to maximize the portion of the field of viewcapturing the object.This helps the API to recover as much detail as possible.One way to do this is to use portrait or landscape modedepending on the object's dimensions and orientation.Also, try to maintain a high degree of overlapbetween the images.Depending on the object, 20 to 200 close-up imagesshould be enough to get good results.To help you get started capturing high-quality photoswith depth and gravity on iOS,we provide the CaptureSample App.This can be used as a starting point for your own apps.It is written in SwiftUIand is part of the developer documentation.This app demonstrates how to take high-quality photosfor Object Capture.It has a manual and timed shutter mode.You could also modify the app to sync with your turntable.It demonstrates how to use the iPhone and iPadswith dual camera to capture depth data and embed itright into the output HEIC files.The app also shows you how to save gravity data.You can view your gallery to quickly verifythat you have all good-quality photos with depth and gravityand delete bad shots.Capture folders are saved in the app's Documents folderwhere it is easy to copy to your Mac using iCloud or AirDrop.There are also help screensthat summarize some of the best practice guidelinesto get a good capture that we discuss in this section.You can also find this informationin developer documentation.We recommend turntable captureto get the best results possible.In order to get started,you'll need a setup like we have here.This contains an iOS device for capture,but you can also use a digital SLR;mechanical turntable to rotate the object;some lighting panels in addition to a light tent.The goal is to have uniform lightingand avoid any hard shadows.A light tent is a good way to achieve this.In this case, the CaptureSample Appcaptures images using the timed shutter modesynced with the motion of the turntable.We can also flip the object and do multiple turntable passesto capture the object from all sides.Here is the resulting USDZ file from the turntable captureshown in Preview on macOS.Now that we've covered tips and tricks for capturing images,let's move to our last sectionon how to select the right output.There's a variety of different output detail settingsavailable for a scan.Let's take a look.Here is the table showing the detail levels.The supported levels are shown along the leftd side.Reduced and Medium are optimized for usein web-based and mobile experiences,such as viewing 3D content in AR Quick Look.They have fewer triangles and material channelsand consequently consume less memory.The Full and Raw are intended for high-end interactive usesuch as computer games or post-production workflows.They contain the highest geometric detailand give you the flexibilityto choose between baked and unbaked materials.Reduced and Medium detail levels are best for contentthat you wish to display on the internet or mobile device.In this instance, Object Capture will compressthe geometric and material information from the Raw resultdown to a level that will be appropriate for displayin AR apps or through AR Quick Look.Both detail levels, Reduced and Medium,contain the diffuse, normal,and ambient occlusion PBR material channels.If you would like to display a single scan in high detail,Medium will maximize the quality against the file sizeto give you both more geometric and material detail.However, if you would like to display multiple scansin the same scene,you should use the Reduced detail setting.If you want to learn more about how to use Object Captureto create mobile or web AR experiences,please see the "AR Quick Look, meet Object Capture" session.Exporting with Full output levelis a great choice for pro workflows.In this instance, you are getting the maximum detailavailable for your scan.Full will optimize the geometry of the scanand bake the detail into a PBR materialcontaining Diffuse, Normal, Ambient Occlusion, Roughness,and Displacement information.We think that this output level will give you everything you needfor the most challenging renders.Lastly, if you don't need material bakingor you have your own pipeline for this,the Raw level will return the maximum poly countalong with the maximum diffuse texture detailfor further processing.If you want to learn more about how to use Object Capturefor pro workflows on macOS,please see the "Create 3D Workflows with USD" session.Finally, and most importantly,if you plan to use your scan on both iOS, as well as macOS,you can select multiple detail levels to make sureyou have all the right outputsfor current and future use cases.And that's a wrap.Let's recap what have we have learned.First, we covered, through example, the main conceptsbehind the Object Capture API.We showed you how to create an Object Capture sessionand to use this sessionto process your collection of images to produce a 3D model.We showed you an example of how the API can supportan interactive preview applicationto let you adjust the capture volume and model transform.Next, we covered best practices for scanning.We discussed what type of objects to useas well the environment, lighting, and camera settingsthat give best results.Lastly, we discussed how to choosethe right output detail settings for your application.If you want to learn how to bring Object Captureto your own app,check out both the iOS capture and macOS CLI processing appsto get started.Along with these apps comes a variety of sample datathat embodies best practiceand can help when planning on how to capture your own scans.Additionally, please check out the detailed documentationon best practice online at developer.apple.com,as well these related WWDC sessions.The only thing that remains is for you to go outand use Object Capture for your own scans.We are excited to see what objects you will scan and share.♪

♪ Bass music playing ♪♪Michael Patrick Johnson: Hi!My name is Michael Patrick Johnson,and I am an engineer on the Object Capture team.

Today, my colleague Dave McKinnon and Iwill be showing you how to turn real-world objectsinto 3D models using our new photogrammetry API on macOS.

You may already be familiar with creating augmented reality appsusing our ARKit and RealityKit frameworks.

You may have also used Reality Composerand Reality Converter to produce 3D models for AR.

And now, with the Object Capture API,you can easily turn images of real-world objectsinto detailed 3D models.

Let's say you have some freshly baked pizza in front of youon the kitchen table.

Looks delicious, right?Suppose we want to capture the pizza in the foregroundas a 3D model.

Normally, you'd need to hire a professional artistfor many hours to model the shape and texture.

But, wait, it took you only minutes to bakein your own oven!With Object Capture, you start by taking photos of your objectfrom every angle.

Next, you copy the images to a Macwhich supports the new Object Capture API.

Using a computer vision techniquecalled "photogrammetry",the stack of 2D images is turned into a 3D modelin just minutes.

The output model includes both a geometric meshas well as various material maps,and is ready to be dropped right into your appor viewed in AR Quick Look.

Now let's look at each of these steps in slightly more detail.

First, you capture photos of your object from all sides.

Images can be taken on your iPhone or iPad,DSLR, or even a drone.

You just need to make sure you get clear photosfrom all angles around the object.

We will provide best practices for capturelater in the session.

If you capture on iPhone or iPad,we can use stereo depth data from supported devicesto allow the recovery of the actual object size,as well as the gravity vectorso your model is automatically created right-side up.

Once you have captured a folder of images,you need to copy them to your Macwhere you can use the Object Capture APIto turn them into a 3D model in just minutes.

The API is supported on recent Intel-based Macs,but will run fastest on all the newest Apple silicon Macssince we can utilize the Apple Neural Engineto speed up our computer vision algorithms.

We also provide HelloPhotogrammetry,a sample command-line app to help you get started.

You can also use it directly on your folder of imagesto try building a model for yourselfbefore writing any code.

Finally, you can preview the USDZ output modelsright on your Mac.

We can provide models at four detail levelsoptimized for your different use cases,which we discuss in more detail later.

Reduced, Medium, and Full details are ready to useright out of the box, like the pizza shown here.

Raw is intended for custom workflows.

By selecting USDZ output at the Medium detail level,you can view the new model in AR Quick Lookright on your iPhone or iPad.

And that's all there is to getting lifelike objectsthat are optimized for AR!Oh wait, remember the pizzas from before?We have to come clean.

This image wasn't really a photo,but was actually created using Object Captureon several pizzas.

These models were then combined into this scenein a post-production tool and rendered using a ray tracerwith the advanced material maps.

So you see, Object Capture can supporta variety of target use cases,from AR apps on an iPhone or iPadto film-ready production assets.

In the remainder of this session,we'll show you how to get startedusing the Object Capture APIand then offer our best practicesto achieve the highest-quality results.

In the getting started section,we'll go into more details about the Object Capture APIand introduce the essential code conceptsfor creating an app.

Next we will discuss best practices for image capture,object selection, and detail-level selection.

Let's begin by working through the essential stepsin using the API on macOS.

In this section, you will learn the basic componentsof the Object Capture API and how to put them together.

Let's say we have this cool new sneakerwe want to turn into a 3D model to view in AR.

Here we see a graphical diagramof the basic workflow we will explore in this section.

There are two main steps in the process:Setup, where we point to our set of images of an object;and then Process, where we request generationof the models we want to be constructed.

First, we will focus on the Setup block,which consists of two substeps: creating a sessionand then connecting up its associated output stream.

Once we have a valid session,we can use it to generate our models.

The first thing we need to dois to create a PhotogrammetrySession.

To create a session,we will assume you already have a folder of images of an object.

We have provided some sample image capture foldersin the API documentation for you to get started quickly.

A PhotogrammetrySession is the primary top-level classin the API and is the main point of control.

A session can be thought of as a containerfor a fixed set of images to which photogrammetry algorithmswill be applied to produce the resulting 3D model.

Here we have 123 HEIC images of the sneakertaken using an iPhone 12 Pro Max.

Currently there are several waysto specify the set of images to use.

The simplest is just a file URL to a directory of images.

The session will ingest these one by oneand report on any problems encountered.

If there is embedded depth data in HEIC images,it will automatically be usedto recover the actual scale of the object.

Although we expect most people will prefer folder inputs,we also offer an interface for advanced workflowsto provide a sequence of custom samples.

A PhotogrammetrySample includes the imageplus other optional data such as a depth map,gravity vector, or custom segmentation mask.

Once you have created a session from an input source,you will make requests on it for model reconstruction.

The session will output the resulting modelsas well as status messages on its output message stream.

Now that we've seen what a session is,let's see how to create one using the API.

Here we see the code to perform the initial setup of a sessionfrom a folder of images.

The PhotogrammetrySessionlives within the RealityKit framework.

First, we specify the input folder as a file URL.

Here, we assume that we already have a folder on the local diskcontaining the images of our sneaker.

Finally, we create the session by passing the URLas our input source.

The initializer will throw an errorif the path doesn't exist or can't be read.

You can optionally provideadvanced configuration parameters,but here we'll just use the defaults.

That's all it takes to create a session!Now that we've successfully created a session object,we need to connect the session's output streamso that we can handle messages as they arrive.

After the message stream is connected,we will see how to request modelsthat will then arrive on that stream.

We use an AsyncSequence -- a new Swift feature this year --to provide the stream of outputs.

Output messages include the results of requests,as well as status messages such as progress updates.

Once we make the first process call,messages will begin to flow on the output message stream.

The output message sequence will not endwhile the session is alive.

It will keep producing messagesuntil either the session is deinitializedor in the case of a fatal error.

Now, let's take a closer lookat the types of messages we will receive.

After a request is made,we expect to receive periodic requestProgress messageswith the fraction completed estimate for each request.

If you're building an app that calls the Object Capture API,you can use these to drive a progress bar for each requestto indicate status.

Once the request is done processing,we receive a requestComplete messagecontaining the resulting payload,such as a model or a bounding box.

If something went wrong during processing,a requestError will be output for that request instead.

For convenience, a processingComplete messageis output when all queued requestshave finished processing.

Now that we've been introducedto the concept of the session output streamand seen the primary output messages,let's take a look at some example codethat processes the message stream.

Once we have this, we'll see how to request a model.

Here is some code that creates an async taskthat handles messages as they arrive.

It may seem like a lot of code,but most of it is simply message dispatching as we will see.

We use a "for try await" loop to asynchronously iterateover the messages in session.outputs as they arrive.

The bulk of the code is a message dispatcherwhich switches on the output message.

Output is an enum with different message types and payloads.

Each case statement will handle a different message.

Let's walk through them.

First, if we get a progress message,we'll just print out the value.

Notice that we get progress messages for each request.

For our example, when the request is complete,we expect the result payload to be a modelFilewith the URL to where the model was saved.

We will see how to make such a request momentarily.

If the request failed due to a photogrammetry error,we will instead get an error message for it.

After the entire set of requestsfrom a process call has finished,a processingComplete message is generated.

For a command-line app, you might exit the app here.

Finally there are other status messages that you can read aboutin the documentation,such as warnings about images in a folderthat couldn't be loaded.

And that's it for the message handling!This message-handling task will keep iteratingand handling messages asynchronouslyfor as long as the session is alive.

OK, let's see where we are in our workflow.

We've fully completed the Setup phaseand have a session ready to go.

We're now ready to make requests to process the models.

Before we jump into the code, let's take a closer lookat the various types of requests we can make.

There are three different data types you can receivefrom a session: a ModelFile, a ModelEntity,and a BoundingBox.

These types have an associated case in the Request enum:modelFile, modelEntity, and bounds;each with different parameters.

The modelFile request is the most commonand the one we will use in our basic workflow.

You simply create a modelFile requestspecifying a file URL with a USDZ extension,as well as a detail level.

There is an optional geometry parameter for usein the interactive workflow, but we won't use that here.

For more involved postprocessing pipelineswhere you may need USDA or OBJ output formats,you can provide an output directory URL instead,along with a detail level.

The session will then write USDA and OBJ files into that folder,along with all the referenced assetssuch as textures and materials.

A GUI app is also able to request a RealityKitModelEntity and BoundingBoxfor interactive preview and refinement.

A modelEntity request also takes a detail leveland optional geometry.

A bounds request will returnan estimated capture volume BoundingBox for the object.

This box can be adjusted in a UIand then passed in the geometry argument of a subsequent requestto adjust the reconstruction volume.

We'll see how this works a bit later in the session.

Most requests also take a detail level.

The preview level is intended only for interactive workflows.

It is very low visual quality but is created the fastest.

The primary detail levelsin order of increasing quality and sizeare Reduced, Medium, and Full.

These levels are all ready to use out of the box.

Additionally, the Raw level is provided for professional useand will need a post-production workflow to be used properly.

We will discuss these in more detailin the best practices section.

OK, now that we've seen what kinds of requests we can make,let's see how to do this in code.

We will now see how to generate two models simultaneouslyin one call,each with a different output filename and detail level.

Here we see the first call to process on the session.

Notice that it takes an array of requests.

This is how we can request two models at once.

We will request one model at Reduced detail leveland one at Medium, each saving to a different USDZ file.

Requesting all desired detail levelsfor an object capture simultaneously in one callallows the engine to share computationand will produce all the models fasterthan requesting them sequentially.

You can even ask for all details levels at once.

Process may immediately throw an error if a request is invalid,such as if the output location can't be written.

This call returns immediatelyand soon messages will begin to appear on the output stream.

And that's the end of the basic workflow!You create the session with your images,connect the output stream, and then request models.

The processing time for each of your modelswill depend on the number of images and quality level.

Once the processing is complete,you will receive the output messagethat the model is available.

You can open the resulting USDZ file of the sneaker you createdright on your Mac and inspect the results in 3Dfrom any angle, including the bottom.

Later in this session,we'll show you how to achieve coveragefor all sides of your object in one capture session,avoiding the need to combine multiple captures together.

It's looking great!Now that you've seen the basic workflow,we will give a high-level overviewof a more advanced interactive workflowthat the Object Capture API also supports.

The interactive workflow is designed to allowseveral adjustments to be made on a preview modelbefore the final reconstruction,which can eliminate the need for post-production model editsand optimize the use of memory.

First, note that the Setup step and the Process stepon both ends of this workflow are the same as before.

You will still create a session and connect the output stream.

You will also request final models as before.

However, notice that we've added a block in the middlewhere a 3D UI is presentedfor interactive editing of a preview model.

This process is iterated until you are happy with the preview.

You can then continue to make the final model requestsas before.

You first request a preview modelby specifying a model request with detail level of preview.

A preview model is of low visual qualityand is generated as quickly as possible.

You can ask for a model file and load it yourselfor directly request a RealityKit ModelEntity to display.

Typically, a bounds request is also made at the same timeto preview and edit the capture volume as well.

You can adjust the capture volumeto remove any unwanted geometry in the capture,such as a pedestal needed to hold the object uprightduring capture.

You can also adjust the root transform to scale,translate, and rotate the model.

The geometry property of the request we saw earlierallows a capture volume and relative root transformto be provided before the model is generated.

This outputs a 3D model that's ready to use.

Let's look at this process in action.

Here we see an example interactive Object Capture appwe created using the APIto demonstrate this interactive workflow.

First, we select the Images foldercontaining images of a decorative rock,as well as an output folderwhere the final USDZ will be written.

Then we hit Preview to request the preview modeland estimated capture volume.

After some time has passed,the preview model of our rock and its capture volume appear.

But let's say that we only wantthe top part of the rock in the outputas if the bottom were underground.

We can adjust the bounding boxto avoid reconstructing the bottom of the model.

Once we are happy,we hit Refine Model to produce a new previewrestricted to this modified capture volume.

This also optimizes the output model for just this portion.

Once the refined model is ready, the new preview appears.

You can see the new model's geometry has been clippedto stay inside the box.

This is useful for removing unwanted items in a capturesuch as a pedestal holding up an object.

Once we are happy with the cropped preview,we can select a Full detail final renderwhich starts the creation process.

After some time, the Full detail model is completeand replaces the preview model.

Now we can see the Full detail of the actual model,which looks great.

The model is saved in the output directoryand ready to use without the needfor any additional post-processing.

And that's all there is to getting startedwith the new Object Capture API.

We saw how to create a session from an input sourcesuch as a folder of images.

We saw how to connect the async output streamto dispatch messages.

We then saw how to requesttwo different level of detail models simultaneously.

Finally, we described the interactive workflowwith an example RealityKit GUI app for ObjectCapture.

Now I will hand it off to my colleague Dave McKinnon,who will discuss best practices with Object Capture.

Dave McKinnon: Thanks, Michael.

Hi, I'm Dave McKinnon, and I am an engineerworking on the Object Capture team.

In the next section we’ll be covering best practicesto help you achieve the highest-quality results.

First, we'll look into tips and tricksfor selecting an object that has the right characteristics.

Followed by a discussion of how to controlthe environmental conditions and camerato get the best results.

Next, we'll walk through how to use the CaptureSample App.

This app allows you to capture imagesin addition to depth data and gravity informationto recover true scale and orientation of your object.

We illustrate the use of this app for both in-handas well as turntable capture.

Finally, we will discuss how to selectthe right output detail level for your use caseas well as providing some links for further reading.

The first thing to consider when doing a scanis picking an object that has the right characteristics.

For the best results,pick an object that has adequate texture detail.

If the object contains texturelessor transparent regions,the resulting scan may lack detail.

Additionally, try to avoid objects that containhighly reflective regions.

If the object is reflective, you will get the best resultsby diffusing the lighting when you scan.

If you plan to flip the object throughout the capture,make sure it is rigid so that it doesn't change shape.

Lastly, if you want to scan an objectthat contains fine surface detail,you'll need to use a high-resolution camerain addition to having many close-up photosof the surface to recover the detail.

We will now demonstrate the typical scanning process.

Firstly, for best results,place your object on an uncluttered backgroundso the object clearly stands out.

The basic process involves moving slowly around the objectbeing sure to capture it uniformly from all sides.

If you would like to reconstruct the bottom of the object,flip it and continue to capture images.

When taking the images,try to maximize the portion of the field of viewcapturing the object.

This helps the API to recover as much detail as possible.

One way to do this is to use portrait or landscape modedepending on the object's dimensions and orientation.

Also, try to maintain a high degree of overlapbetween the images.

Depending on the object, 20 to 200 close-up imagesshould be enough to get good results.

To help you get started capturing high-quality photoswith depth and gravity on iOS,we provide the CaptureSample App.

This can be used as a starting point for your own apps.

It is written in SwiftUIand is part of the developer documentation.

This app demonstrates how to take high-quality photosfor Object Capture.

It has a manual and timed shutter mode.

You could also modify the app to sync with your turntable.

It demonstrates how to use the iPhone and iPadswith dual camera to capture depth data and embed itright into the output HEIC files.

The app also shows you how to save gravity data.

You can view your gallery to quickly verifythat you have all good-quality photos with depth and gravityand delete bad shots.

Capture folders are saved in the app's Documents folderwhere it is easy to copy to your Mac using iCloud or AirDrop.

There are also help screensthat summarize some of the best practice guidelinesto get a good capture that we discuss in this section.

You can also find this informationin developer documentation.

We recommend turntable captureto get the best results possible.

In order to get started,you'll need a setup like we have here.

This contains an iOS device for capture,but you can also use a digital SLR;mechanical turntable to rotate the object;some lighting panels in addition to a light tent.

The goal is to have uniform lightingand avoid any hard shadows.

A light tent is a good way to achieve this.

In this case, the CaptureSample Appcaptures images using the timed shutter modesynced with the motion of the turntable.

We can also flip the object and do multiple turntable passesto capture the object from all sides.

Here is the resulting USDZ file from the turntable captureshown in Preview on macOS.

Now that we've covered tips and tricks for capturing images,let's move to our last sectionon how to select the right output.

There's a variety of different output detail settingsavailable for a scan.

Let's take a look.

Here is the table showing the detail levels.

The supported levels are shown along the leftd side.

Reduced and Medium are optimized for usein web-based and mobile experiences,such as viewing 3D content in AR Quick Look.

They have fewer triangles and material channelsand consequently consume less memory.

The Full and Raw are intended for high-end interactive usesuch as computer games or post-production workflows.

They contain the highest geometric detailand give you the flexibilityto choose between baked and unbaked materials.

Reduced and Medium detail levels are best for contentthat you wish to display on the internet or mobile device.

In this instance, Object Capture will compressthe geometric and material information from the Raw resultdown to a level that will be appropriate for displayin AR apps or through AR Quick Look.

Both detail levels, Reduced and Medium,contain the diffuse, normal,and ambient occlusion PBR material channels.

If you would like to display a single scan in high detail,Medium will maximize the quality against the file sizeto give you both more geometric and material detail.

However, if you would like to display multiple scansin the same scene,you should use the Reduced detail setting.

If you want to learn more about how to use Object Captureto create mobile or web AR experiences,please see the "AR Quick Look, meet Object Capture" session.

Exporting with Full output levelis a great choice for pro workflows.

In this instance, you are getting the maximum detailavailable for your scan.

Full will optimize the geometry of the scanand bake the detail into a PBR materialcontaining Diffuse, Normal, Ambient Occlusion, Roughness,and Displacement information.

We think that this output level will give you everything you needfor the most challenging renders.

Lastly, if you don't need material bakingor you have your own pipeline for this,the Raw level will return the maximum poly countalong with the maximum diffuse texture detailfor further processing.

If you want to learn more about how to use Object Capturefor pro workflows on macOS,please see the "Create 3D Workflows with USD" session.

Finally, and most importantly,if you plan to use your scan on both iOS, as well as macOS,you can select multiple detail levels to make sureyou have all the right outputsfor current and future use cases.

And that's a wrap.

Let's recap what have we have learned.

First, we covered, through example, the main conceptsbehind the Object Capture API.

We showed you how to create an Object Capture sessionand to use this sessionto process your collection of images to produce a 3D model.

We showed you an example of how the API can supportan interactive preview applicationto let you adjust the capture volume and model transform.

Next, we covered best practices for scanning.

We discussed what type of objects to useas well the environment, lighting, and camera settingsthat give best results.

Lastly, we discussed how to choosethe right output detail settings for your application.

If you want to learn how to bring Object Captureto your own app,check out both the iOS capture and macOS CLI processing appsto get started.

Along with these apps comes a variety of sample datathat embodies best practiceand can help when planning on how to capture your own scans.

Additionally, please check out the detailed documentationon best practice online at developer.apple.com,as well these related WWDC sessions.

The only thing that remains is for you to go outand use Object Capture for your own scans.

We are excited to see what objects you will scan and share.

♪

6:56 -Creating a PhotogrammetrySession with a folder of images

9:26 -Creating the async message stream dispatcher

13:44 -Calling process on two models simultaneously

## Code Samples

```swift
import
 RealityKit


let
 inputFolderUrl 
=
 
URL
(fileURLWithPath: 
"/tmp/Sneakers/"
, isDirectory: 
true
)

let
 session 
=
 
try!
 
PhotogrammetrySession
(input: inputFolderUrl,
                                         configuration: 
PhotogrammetrySession
.
Configuration
())
```

```swift
// Create an async message stream dispatcher task



Task
 {
    
do
 {
        
for
 
try
 
await
 output 
in
 session.outputs {
            
switch
 output {
            
case
 .requestProgress(
let
 request, 
let
 fraction):
                
print
(
"Request progress: 
\(fraction)
"
)
            
case
 .requestComplete(
let
 request, 
let
 result):
                
if
 
case
 .modelFile(
let
 url) 
=
 result {
                    
print
(
"Request result output at 
\(url)
."
)
                }
            
case
 .requestError(
let
 request, 
let
 error):
                
print
(
"Error: 
\(request)
 error=
\(error)
"
)
            
case
 .processingComplete:
                
print
(
"Completed!"
)
                handleComplete()
            
default
:  
// Or handle other messages...

                
break

            }
        }
    } 
catch
 {
       
print
(
"Fatal session error! 
\(error)
"
)
    }
}
```

```swift
try!
 session.process(requests: [
    .modelFile(
"/tmp/Outputs/model-reduced.usdz"
, detail: .reduced),
    .modelFile(
"/tmp/Outputs/model-medium.usdz"
, detail: .medium)
])
```

