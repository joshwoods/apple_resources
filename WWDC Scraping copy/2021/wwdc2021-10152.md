# Wwdc2021 10152

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Accelerate machine learning with Metal Performance Shaders GraphMetal Performance Shaders Graph is a compute engine that helps you build, compile, and execute customized multidimensional graphs for linear algebra, machine learning, computer vision, and image processing. Discover how MPSGraph can accelerate the popular TensorFlow platform through a Metal backend for Apple products. Learn how to add control flow to your graphs, manage the graph compilation for optimal performance, and use the MPSGraph operations to accelerate the hardest compute applications with only a few lines of code.ResourcesMetalMetal Performance ShadersMetal Shading Language SpecificationTraining a Neural Network with Metal Performance ShadersHD VideoSD VideoRelated VideosWWDC22Accelerate machine learning with MetalWWDC20Build customized ML models with the Metal Performance Shaders Graph

Metal Performance Shaders Graph is a compute engine that helps you build, compile, and execute customized multidimensional graphs for linear algebra, machine learning, computer vision, and image processing. Discover how MPSGraph can accelerate the popular TensorFlow platform through a Metal backend for Apple products. Learn how to add control flow to your graphs, manage the graph compilation for optimal performance, and use the MPSGraph operations to accelerate the hardest compute applications with only a few lines of code.

Metal

Metal Performance Shaders

Metal Shading Language Specification

Training a Neural Network with Metal Performance Shaders

HD VideoSD Video

HD Video

SD Video

Accelerate machine learning with Metal

Build customized ML models with the Metal Performance Shaders Graph

Search this video…Hi. I'm Saharsh Oza.I'm with the GPU Software Engineering team at Apple.Today my colleague, Yuliya Pylypiv, and Iwill talk about what's new in Metal Performance Shaders Graph.Let us begin.MPS is a library of metal-based, high-performance,GPU-accelerated primitives for varied fieldslike image processing, linear algebra,ray tracing, and machine learning.MPS team optimizes Metal kernelsto give the best performance on each hardwareacross Apple's various platforms.Last year, we introduced the MPSGraph framework,a general purpose compute graph for the GPU.It is supported on macOS, iOS,iPadOS and tvOS,same as the MPS framework.Please watch our last year's sessionto get more introductory details on the MPSGraph.Let's take a look at the agenda.We have a lot to cover.We will discuss ML inference and training accelerationthrough MPSGraph.We will introduce some exciting new MPSGraph operations.We will introduce new waysfor you to control compilation in MPSGraph.And finally, we will lookat the all new control-flow capabilities of MPSGraph.I'd like to introduce my colleague, Yuliya,who will share some exciting updatesfor inference and training acceleration.Thanks, Saharsh.Hi. I'm Yuliya Pylypiv.I am a part of GPU Software team at Apple.Today, I want to share the improvements we've madeto boost training and inference performance on GPU.Let's get right into it.The MPSGraph framework has been adoptedby higher level machine learning frameworks like Core MLand TensorFlow for GPU acceleration.This year, we have optimized MPSGraph even furtherwith a combination of kernel improvementsand stitching adoption.This has translated to large performance gainsto the machine learning frameworks that use MPS.Let's take a closer lookat the new Metal Plugin for TensorFlow.TensorFlow is a popular machine learning training platform,and GPUs are the predominant accelerator device.This year, we have developed a new Metal Pluginusing TensorFlow PluggableDevice Interfacereleased in TensorFlow 2.5.This brings the power of Metal to TensorFlowusing MPS and MPSGraph.This allows us to train any machine learning modelon Mac platform GPUs without modifications.Now, let's see one of these in action.For this demo, I am going to use a Jupyter environment.On my M1 system,I have the latest available TensorFlow installed.When we list physical devices,you can see there is only a CPU device registered.Here I am defining a popular machine learning model,ResNet50, which is widely used for image classification,transfer learning, and more.Current model uses a standard ImageNet datasetwith 224 by 224 image sizes.As you can see, the current ETA for the first epochrunning on CPU is around 20 minutes.Let me install the TensorFlow Metal Pluginwhich we introduced earlierand see if we can add some speedup to the current network.To do so,I'm gonna use pip install tensorflow-metal...going back to the same ResNet50 model we used before.Only this time, you can seethere is a new GPU device registered.This is the GPU device we introducedas a part of the TensorFlow platform using Metal Plugin.All callbacks and network definitionremain unchanged.Kicking off the network again so we can compare ETAs.You can see that the GPU version of the same networkis training around four times fasterusing TensorFlow Metal Plugin.Now let's take a closer look at the other networks.Here we show the performanceon key machine learning training benchmarks relative to the CPU.As you can see, we have a good speedup across all benchmarks,going up to eight times faster on the M1 MacBook Pro.Installing the new Metal Plugin for TensorFlow is easy.After installing the base TensorFlowusing pip install tensorflow-macos,you can install Metal Pluginusing pip install tensorflow-metal.The Metal Plugin will be availableon the official Python package repo, pypi.org.For details on environment setup and installation,please refer to Metal Developer Resource.That's it for TensorFlow.Next, let's talk about Inference acceleration in Core ML.Core ML is Apple's machine learning inference framework.We also saw significant performance improvementson Core ML with MPSGraph.We show here inference speedup of key classesof machine learning networks on M1.We get a 2x speedup on BERT,which is a canonical transformer networkused for NLP applications.ResNet50, which is central to computer vision applications,has been tuned for texture paths in previous releases.This is an additional performance improvement of 16%with our new buffer backend through MPSGraph.These performance improvements in Core ML and TensorFloware due to performance improvementsin MPS primitives like Convolution2D.Here, we show the speedup of Convolution2Don NHWC and NCHW data layoutswhich are used for training and inference respectively.That's it for the improvements in inference and training.Next, let's go back to Saharsh to learn moreabout the new operations in MPSGraph.Thanks, Yuliya.Now we will take a look at the new setof operations supported by MPSGraph.We support a plethora of operations on the MPSGraph,from multiple variants of convolutions and reductionsto all the basic math ops you may need in your compute graphs.This year, we added special operationsto enable you to do even more with MPSGraph.We will introduce three new primitives:control dependency, stencil operator,and gather operator.First, we'll look at control dependency.Control dependency is neededto explicitly order operations in the graph.To understand this,let's formally define a graph operation.Operations in the graph connect with each othervia three kinds of edges:input tensors, which represent which tensorsact as data inputs to the op,output tensors, which are created by the op itself,and finally, a special kind of edgecalled control dependency.They must execute before the current operation,even if the current operation itself does not depend on it.This API also offersa convenient way to prevent operationsfrom being optimized away by MPSGraph.This is needed to implementmachine learning layers like batch normalization.Let's see this in practice.Batch normalization is a standard layerused in ML trainingto make the network more stable and converge faster.Here we see the computational graph for batch normalizationthat is used for training.The first step is to compute the mean and variance.These are, in turn, used to update the running meanand running variance which are needed for inference.However, the training graph resultdoes not require these variables,so the MPSGraph might optimize them away.We can solve this by explicitly ordering thembefore the final normalization operatorusing control dependencies.Let's look at a simple example with some codethat shows how you can use this API.This graph shows an exponent and assign operator.The assign operator is not used by anything else in the graph.So it may be optimized away.One way to solve this is to explicitly set the assignas a targetOperation.However, this requires the developerto track dependencies globally across the graph.Instead, with the new control dependency API,you can make the exponent operationdepend on the assignment.This removes the need to have a targetOperationand also ensures that the graph does not optimize it away.Next, we will see this in code.We first define the operatorthat the exponent is dependent on.Next, we create a dependentBlockwhich defines the exponent operator.Finally, we call the run API on this graph.Note that no targetOperations need to be tracked globally.That's it for control dependency.Now let's talk about stencil operators.A stencil operation is a generalizationof sliding window operators like image convolution.These operators are essential in finite element methods,machine learning,and image processing applications.Here, we see a five-point 2D stencilcommonly used to implement Laplacian operations.The stencil operator shown herecan be applied to higher dimensions too,as shown with this seven-point 3D stencil diagram.Let's take a closer look at the operator.For each output value, it computes a weighted reductionover the stencil window on the input tensor, as shown.The operator supports various reduction modesincluding argmin/argmax, and various padding modes,including reflection and clampToZero.MPSGraph enables stitching across MPS kernelsfor optimal performance.With stitching support, the stencil operator allows youto express complex mathematical operationsin a single kernel launch.Let us see one such example in action.Local response normalization is a pytorch opused for normalizing in the channel dimension.It's very straightforward to implement thiswith the new stencil operation.Here, we see the graph for this normalization technique.We see that it's just element wise opsaround the stencil operation.Without the new operation,multiple dispatches will be needed.Now, since the stencil op supports stitching,this entire graph can be launched in a single dispatch.So that's it for the stencil operator.Next, let's take a look at improvementsin gather operations.This year, new gather operationshave been added to MPSGraph.These allow for efficient copyingof arbitrary sized slicesin noncontiguous memory locations.Conceptually, we are gathering the valuesfrom locations marked in blue from a chunk of memory.These gather layers allow for efficient implementationof embedding lookup and dynamic matrix copy.GatherND is a powerful extension of the gather operation.While the normal gather supports linear indexing,the gatherND operation enables N-dimensional indexing.This allows for seamless copying of datafrom anywhere in an N-dimensional input.The input to this operation is a vector of coordinates,and each coordinate can be upto the rank of the input tensor.Any dimensions not specified in the coordinatesresult in slice copies.We can step through an exampleof a gather of row slices from a 3D tensor.In this example, the indices specify two coordinatescorresponding to the matrixand row coordinates.With no third coordinate to column index,this gatherND will copy entire rows.The result tensor is a 2D matrix of the rows gatheredfrom the input matrix.GatherND can represent nearly any form of gather operationand give great performance.For example, let's see how we can implementembedding lookup using gather operations.Embedding lookup is a common operationused to find embedding vectorsfor a provided set of input objects.Commonly, this layer is used in language processing networks,where an embedding matrix is generatedassociating each word in the vocabularyto an embedding vector.The ID of the words in the vocabularycan be used as the indices to a gather operation,and the embedding matrix is our input tensor.We would like to get the corresponding rowsfor each word ID,which we can do easily using a gather layer.We only specify one coordinate,so the entire row will be copied for each input word.The resulting tensor is a 2D matrixof each input word's embedding vector along the rows.That's it for the new MPSGraph operationswe have introduced this year.Now let's talk about the compilation APIs.This year, we are introducingthe new MPSGraphExecutable API.This compilation APIimproves performance in two ways.First, it gives the developer controlon when to compile the graph.Second, it allows you to reduce the number of compilation callsthrough deferred type inference.Now let's take a closer look at each.Last year, we provided a really convenient APIto define and execute an MPSGraph.Under the hood, the first time an evaluation was requested,MPSGraph invoked compilation for the input typesand internally created an executable.For any subsequent executions,MPSGraph seamlessly cached this executableto ensure compilation cost is not paid again.Users now have the abilityto invoke compilation ahead of timeso you can choose the timeline for compilation.With the compiled executable, you can call run directlyon the MPSGraphExecutable.This gives the user control on when the graph is compiledand also the ability to cache the compiled executableso you can gain even more performance.Let's see this in code.Here, we have a simple graph to add two tensors.Now to compile, we provide the types for the feedsand target tensors along with the operations.What we get is a compiled graph and an executable.And the evaluation method is just as simple.We provide a Metal command queueand our input tensor data.So those are the basics of compiling an MPS graph.Next, let's talk about how we reducethe number of compilation calls through deferred type inference.Type inference is a compilation passwhere MPSGraph must determine tensor shapeswhere they are not specified by the user.In this graph, we are performinga matrix multiplication of two 2D tensors.The shapes of the input tensors are shown.However, the output tensor is of an unknown shape.Once the type inference pass is complete,the output tensor shape is determinedbased on the inputs and operation type.In standard neural networks, the inputs to the networkare not always the same size.For natural language processing, the sentences or sequencescan be of different lengths.For CNNs, we see different-sized imagescoming in to be evaluated.Before the compilation upgrades of this year,for every new sized image,a compilation would be invoked to do type inferencefor the whole graph.Now with control over compilation, you, the developer,can invoke compilationwith type inference pass turned off.This can save tens or hundreds of many secondsof compilation timeon each iteration and get the best performance.MPSGraph runtime will infer typesjust in time during encoding and seamlessly make things work.It is a tradeoff between saving compilation timeversus getting the most optimal graph.Let's see how this can be used in the code exampleshared before.Disabling the type inference pass can be achievedby setting the compilation descriptor as shown.That's it for compilation APIs.Finally, let's talk about the newcontrol flow APIs of MPSGraph.These APIs let you dynamically dispatch operationsbased on tensors previously evaluated by the graph.This is common in applications like batch normalizationand recurrent neural networks.Let's take a look at how a “while loop” can be implementedwith MPSGraph today without the new API.First, we create a graphthat computes the predicate.Next, the predicate is evaluated on the CPUthrough an explicit memory synchronization.If the predicate is true, the previously created graphis re-executed with the new inputs.Otherwise, if the predicate is false,the loop ends and a second MPSGraph is createdand executed to consume the result.With the new control flow API, all these steps can be launchedas part of a single MPSGraph execution.This is more convenient to implementbecause you don't have to introduceexplicit memory synchronization primitives.Now let's take a lookat how this can be potentially more efficient.Here we see the control flow timelinewithout the new API.We encode the first kernel on the CPU.Once the kernel is complete,we have to synchronize memory to read the result.This is potentially inefficient,as the CPU has to wait for the GPU to finish executing.Similarly, the GPU also has to waitfor the CPU synchronizationand subsequent encoding to complete.This happens in each iteration.Now let's see the benefitsof using the new MPSGraph API.We have to perform only one CPU encode call.Since the predicate is evaluated on the GPU timeline,no synchronization overhead is incurred,and the kernels can be launched without any bubbles.Now let's see what the new APIs are.We added three new control flow APIs:if/else, for loops,and while loops.Let's start with the if/else primitive.We are all familiar with this.Based on a predicate,different code paths are executed.We are provided a Boolean predicatealong with a code block for the “if” and “else” conditions.If this predicate is true,we execute the then block of code.Otherwise, if it's false,the else branch is executed.Having the if/else operationis very useful in neural networks.One canonical usage is in batchNormalization operation,which has different behavior in training and inference.With the isTraining Boolean, we can have a single graphto represent both variants of the normalizer.Let's look at how to set up an if/else branch in code.Let's take a very simple exampleof two input scalar tensors.If the first tensor is smaller than the second,we return the sum of the operations.Else, we return the difference.First, we compute the predicateand pass that to the API.Next, when the predicate is true,we compute the then block and add the tensors.Finally, when the predicate is false,we compute the else block and subtract the tensors.Next, let's see how to implement a for loop.The for loop primitive loops over a set of operationsa fixed number of times.This is common in recurrent neural networkswhere we have to loop over sequencesof different lengths during training.We need to provide the numberOfIterationsof the for loop.The index is initialized to 0and compared against the numberOfIterationseach loop iteration.If it's less than the numberOfIterations,we execute the body of the for loopand increment the index by 1.When the index is equal toor greater than the numberOfIterations,we end the loop.Let's see how to implement this in code.Let's say we wanted to implement a really simple example.We'll initialize the result variable to some input value.Then we loop four times, multiplying the resultby another input value each time.First, we create two graph tensors.The output tensor will be initialized to input0.In each iteration,this tensor will be multiplied by input1.Next, we set the numberOfIterations to 4so that we can execute the loop four times,from index 0 to index 3.Next, we create the body of the for loop.This is done by creating a closurewhich represents a single iteration.Each iteration is passed the index of the current iteration,as well as the output of the previous iteration.Then, we'll update the result and return it,to be passed to the next iteration.Finally, we pass all these argumentsto the for loop API in the graph.Note that the iterationArguments of the bodyare initialized to input0 tensor.That's it for the for loop.Now let's look at the while loop API.This primitive executes a set of operationswhile a condition is met.We need to provide two blocks of codeto use this API.In the first block,the condition is checked with a predicate.When the predicate is true, the body of the while loopin the after block is executed.This recomputes the predicate.MPSGraph then uses this predicatein the next iteration of the before block.If the condition evaluated is false,it exits the loop.The API also allows for implementing the do-while loopby swapping the bodyand condition evaluation code blocks.Let's say we wanted to implement a really simple example.We'll initialize the result variable to some input value.Then we'll multiply the result by a multiplier each timein a loop till we exceed a threshold.First, we define a block of code that will evaluate the predicateusing the result of the previous iteration.It also stores the results of the previous iterationin a returnTensors NSArray.This array will be used as the inputto the next iteration when the predicate is trueand used as the final resultif the predicate is false.Next, we define the body of the while loopwhere the tensors are multiplied.The product is returned for the condition block to read.Finally, we'll pass all these argumentsto the while loop API as shown.Note the initialInputs argumentis used in the first iteration of the before block.That's it for while loops.Next, we'll see how this can be used in a real application.Image composition is a common image editing utility.Here, an object is implanted into a target image.We start with a source imageand a background image, as shown.Next, we create a mask on the source image.Let's place this mask of the source imagedirectly against the background.That does not look great,as we can clearly see the edges of the source image.Through image composition,we want to smoothen these edges.Pairing a Laplacian edge filter with an iterative linear solveris a common way to achieve this.Now let's look at the details.Here, we see the pipeline neededto perform image composition with MPSGraph.We start with our input tensors, the background image,source image, and a mask of the object.Next we use an iterative linear solvercoupled with a Laplacian edge detector.The output of this set of operations isa composite image with smooth edges.Let's take a look at the Laplacian edge filter.Implementing the Laplacian edge filterinvolves a windowed reductionover the source image with a set of weights.The stencil operator is usedto implement this as shown.Using this operator, we are ableto see the edges of the source object.The edges computed here will be usedas the input to the linear solver.Next, let's take a look at the linear solver.We start with the background imageand feed it into the linear solver.The solver updates this image,and the result is subsequently read back in.As we can see, this is an iterative process.As the iterations progress,the solution image improves till we arriveat the perfect blend at the edges.The loop terminates when the erroris below a user defined tolerance.This requires a while loop.You can now use the MPSGraph Control Flow APIto implement this.Now, let's look at the demo.We have implemented an image composition utilityusing the MPSGraph as an iPad Pro application.We start with a source image on the topand a target image below.We will be cloning objectsfrom the source to the target.The first thing we need to do is to draw a maskaround the cow that we want to move.Let's see how this looks with a naive clone.That does not look very good, as we can see the rough edges.Now let's try the image composition techniquewe just described.We will start by settingthe initial solution to the background image.Let's run this for about 50 iterations.Clearly, the solution image has not yet converged.Let's run it for about 50 more iterations.This looks way more natural as the edges smoothen out.The ease of programming with MPSGraphmakes experimenting with different techniquesstraightforward.Initializing the solver with the cloned imageinstead of the background imagecan result in faster convergence.We can enable this initialization modeby toggling this switch.Let's see this in action by setting the iteration countto 50 again and resetting to the naive clone.Now let's rerun the solver.We can see the solution imageafter 50 iterations looks pretty good.Since we already start with the source object,we also observe less bleeding at the edges.This is great.But what we really want is to automate convergencebased on an error tolerance.This will require a while loopwhich we will enable by using this switch.We have implemented this with the new MPSGraph API.The error tolerance can be controlled with this slider.We have set it to 0.1, as shown.Let's reset this back to the naive clone.Now we start the solver.With this while loop, we converge to the solution imagein about 80 iterationswithout me having to specify any iteration count.Now let's have some funby cloning other animals onto this background.Let's try this cute puppy.All right, done tracing.I think it would look great at the bottom rightof this image.Maybe we can try a bird next.This would look good on the top right of the background.The new background with all these images looks pretty neat.That's it for the demo.In summary, we showed how adopting MPSGraphled to amazing performance improvementsfor CoreML and TensorFlow.Inference is now up to twice as fast.We introduced useful new compute primitives,including the stencil operator that is going to enablea wide range of applications.We showed new compilation flexibilitythat MPSGraph offers.This is going to shave off latency from inference networks.And finally, we showed all newcontrol flow capabilities of MPSGraph.This API is key to expressingseveral linear algebra applicationsin addition to machine learning networks.We are excited to see how youwill take advantage of these features.Thank you, and have a great WWDC 2021.[upbeat music]

Hi. I'm Saharsh Oza.I'm with the GPU Software Engineering team at Apple.Today my colleague, Yuliya Pylypiv, and Iwill talk about what's new in Metal Performance Shaders Graph.Let us begin.MPS is a library of metal-based, high-performance,GPU-accelerated primitives for varied fieldslike image processing, linear algebra,ray tracing, and machine learning.MPS team optimizes Metal kernelsto give the best performance on each hardwareacross Apple's various platforms.Last year, we introduced the MPSGraph framework,a general purpose compute graph for the GPU.It is supported on macOS, iOS,iPadOS and tvOS,same as the MPS framework.Please watch our last year's sessionto get more introductory details on the MPSGraph.Let's take a look at the agenda.We have a lot to cover.We will discuss ML inference and training accelerationthrough MPSGraph.We will introduce some exciting new MPSGraph operations.We will introduce new waysfor you to control compilation in MPSGraph.And finally, we will lookat the all new control-flow capabilities of MPSGraph.I'd like to introduce my colleague, Yuliya,who will share some exciting updatesfor inference and training acceleration.Thanks, Saharsh.Hi. I'm Yuliya Pylypiv.I am a part of GPU Software team at Apple.Today, I want to share the improvements we've madeto boost training and inference performance on GPU.Let's get right into it.The MPSGraph framework has been adoptedby higher level machine learning frameworks like Core MLand TensorFlow for GPU acceleration.This year, we have optimized MPSGraph even furtherwith a combination of kernel improvementsand stitching adoption.This has translated to large performance gainsto the machine learning frameworks that use MPS.Let's take a closer lookat the new Metal Plugin for TensorFlow.TensorFlow is a popular machine learning training platform,and GPUs are the predominant accelerator device.This year, we have developed a new Metal Pluginusing TensorFlow PluggableDevice Interfacereleased in TensorFlow 2.5.This brings the power of Metal to TensorFlowusing MPS and MPSGraph.This allows us to train any machine learning modelon Mac platform GPUs without modifications.Now, let's see one of these in action.For this demo, I am going to use a Jupyter environment.On my M1 system,I have the latest available TensorFlow installed.When we list physical devices,you can see there is only a CPU device registered.

Here I am defining a popular machine learning model,ResNet50, which is widely used for image classification,transfer learning, and more.

Current model uses a standard ImageNet datasetwith 224 by 224 image sizes.

As you can see, the current ETA for the first epochrunning on CPU is around 20 minutes.Let me install the TensorFlow Metal Pluginwhich we introduced earlierand see if we can add some speedup to the current network.To do so,I'm gonna use pip install tensorflow-metal...

going back to the same ResNet50 model we used before.

Only this time, you can seethere is a new GPU device registered.This is the GPU device we introducedas a part of the TensorFlow platform using Metal Plugin.

All callbacks and network definitionremain unchanged.Kicking off the network again so we can compare ETAs.You can see that the GPU version of the same networkis training around four times fasterusing TensorFlow Metal Plugin.

Now let's take a closer look at the other networks.Here we show the performanceon key machine learning training benchmarks relative to the CPU.As you can see, we have a good speedup across all benchmarks,going up to eight times faster on the M1 MacBook Pro.

Installing the new Metal Plugin for TensorFlow is easy.After installing the base TensorFlowusing pip install tensorflow-macos,you can install Metal Pluginusing pip install tensorflow-metal.The Metal Plugin will be availableon the official Python package repo, pypi.org.For details on environment setup and installation,please refer to Metal Developer Resource.That's it for TensorFlow.Next, let's talk about Inference acceleration in Core ML.

Core ML is Apple's machine learning inference framework.We also saw significant performance improvementson Core ML with MPSGraph.We show here inference speedup of key classesof machine learning networks on M1.We get a 2x speedup on BERT,which is a canonical transformer networkused for NLP applications.ResNet50, which is central to computer vision applications,has been tuned for texture paths in previous releases.This is an additional performance improvement of 16%with our new buffer backend through MPSGraph.These performance improvements in Core ML and TensorFloware due to performance improvementsin MPS primitives like Convolution2D.Here, we show the speedup of Convolution2Don NHWC and NCHW data layoutswhich are used for training and inference respectively.That's it for the improvements in inference and training.Next, let's go back to Saharsh to learn moreabout the new operations in MPSGraph.Thanks, Yuliya.Now we will take a look at the new setof operations supported by MPSGraph.

We support a plethora of operations on the MPSGraph,from multiple variants of convolutions and reductionsto all the basic math ops you may need in your compute graphs.This year, we added special operationsto enable you to do even more with MPSGraph.We will introduce three new primitives:control dependency, stencil operator,and gather operator.First, we'll look at control dependency.Control dependency is neededto explicitly order operations in the graph.To understand this,let's formally define a graph operation.Operations in the graph connect with each othervia three kinds of edges:input tensors, which represent which tensorsact as data inputs to the op,output tensors, which are created by the op itself,and finally, a special kind of edgecalled control dependency.They must execute before the current operation,even if the current operation itself does not depend on it.This API also offersa convenient way to prevent operationsfrom being optimized away by MPSGraph.This is needed to implementmachine learning layers like batch normalization.Let's see this in practice.Batch normalization is a standard layerused in ML trainingto make the network more stable and converge faster.Here we see the computational graph for batch normalizationthat is used for training.The first step is to compute the mean and variance.These are, in turn, used to update the running meanand running variance which are needed for inference.However, the training graph resultdoes not require these variables,so the MPSGraph might optimize them away.We can solve this by explicitly ordering thembefore the final normalization operatorusing control dependencies.Let's look at a simple example with some codethat shows how you can use this API.

This graph shows an exponent and assign operator.The assign operator is not used by anything else in the graph.So it may be optimized away.One way to solve this is to explicitly set the assignas a targetOperation.However, this requires the developerto track dependencies globally across the graph.Instead, with the new control dependency API,you can make the exponent operationdepend on the assignment.This removes the need to have a targetOperationand also ensures that the graph does not optimize it away.Next, we will see this in code.

We first define the operatorthat the exponent is dependent on.Next, we create a dependentBlockwhich defines the exponent operator.Finally, we call the run API on this graph.Note that no targetOperations need to be tracked globally.That's it for control dependency.Now let's talk about stencil operators.

A stencil operation is a generalizationof sliding window operators like image convolution.These operators are essential in finite element methods,machine learning,and image processing applications.Here, we see a five-point 2D stencilcommonly used to implement Laplacian operations.The stencil operator shown herecan be applied to higher dimensions too,as shown with this seven-point 3D stencil diagram.Let's take a closer look at the operator.For each output value, it computes a weighted reductionover the stencil window on the input tensor, as shown.The operator supports various reduction modesincluding argmin/argmax, and various padding modes,including reflection and clampToZero.MPSGraph enables stitching across MPS kernelsfor optimal performance.With stitching support, the stencil operator allows youto express complex mathematical operationsin a single kernel launch.Let us see one such example in action.Local response normalization is a pytorch opused for normalizing in the channel dimension.It's very straightforward to implement thiswith the new stencil operation.Here, we see the graph for this normalization technique.We see that it's just element wise opsaround the stencil operation.Without the new operation,multiple dispatches will be needed.Now, since the stencil op supports stitching,this entire graph can be launched in a single dispatch.So that's it for the stencil operator.Next, let's take a look at improvementsin gather operations.

This year, new gather operationshave been added to MPSGraph.These allow for efficient copyingof arbitrary sized slicesin noncontiguous memory locations.Conceptually, we are gathering the valuesfrom locations marked in blue from a chunk of memory.These gather layers allow for efficient implementationof embedding lookup and dynamic matrix copy.GatherND is a powerful extension of the gather operation.While the normal gather supports linear indexing,the gatherND operation enables N-dimensional indexing.This allows for seamless copying of datafrom anywhere in an N-dimensional input.The input to this operation is a vector of coordinates,and each coordinate can be upto the rank of the input tensor.Any dimensions not specified in the coordinatesresult in slice copies.We can step through an exampleof a gather of row slices from a 3D tensor.In this example, the indices specify two coordinatescorresponding to the matrixand row coordinates.With no third coordinate to column index,this gatherND will copy entire rows.The result tensor is a 2D matrix of the rows gatheredfrom the input matrix.GatherND can represent nearly any form of gather operationand give great performance.For example, let's see how we can implementembedding lookup using gather operations.

Embedding lookup is a common operationused to find embedding vectorsfor a provided set of input objects.Commonly, this layer is used in language processing networks,where an embedding matrix is generatedassociating each word in the vocabularyto an embedding vector.The ID of the words in the vocabularycan be used as the indices to a gather operation,and the embedding matrix is our input tensor.We would like to get the corresponding rowsfor each word ID,which we can do easily using a gather layer.We only specify one coordinate,so the entire row will be copied for each input word.The resulting tensor is a 2D matrixof each input word's embedding vector along the rows.That's it for the new MPSGraph operationswe have introduced this year.Now let's talk about the compilation APIs.This year, we are introducingthe new MPSGraphExecutable API.This compilation APIimproves performance in two ways.First, it gives the developer controlon when to compile the graph.Second, it allows you to reduce the number of compilation callsthrough deferred type inference.Now let's take a closer look at each.Last year, we provided a really convenient APIto define and execute an MPSGraph.Under the hood, the first time an evaluation was requested,MPSGraph invoked compilation for the input typesand internally created an executable.For any subsequent executions,MPSGraph seamlessly cached this executableto ensure compilation cost is not paid again.Users now have the abilityto invoke compilation ahead of timeso you can choose the timeline for compilation.With the compiled executable, you can call run directlyon the MPSGraphExecutable.This gives the user control on when the graph is compiledand also the ability to cache the compiled executableso you can gain even more performance.Let's see this in code.Here, we have a simple graph to add two tensors.Now to compile, we provide the types for the feedsand target tensors along with the operations.What we get is a compiled graph and an executable.And the evaluation method is just as simple.We provide a Metal command queueand our input tensor data.So those are the basics of compiling an MPS graph.Next, let's talk about how we reducethe number of compilation calls through deferred type inference.Type inference is a compilation passwhere MPSGraph must determine tensor shapeswhere they are not specified by the user.In this graph, we are performinga matrix multiplication of two 2D tensors.The shapes of the input tensors are shown.However, the output tensor is of an unknown shape.

Once the type inference pass is complete,the output tensor shape is determinedbased on the inputs and operation type.In standard neural networks, the inputs to the networkare not always the same size.For natural language processing, the sentences or sequencescan be of different lengths.For CNNs, we see different-sized imagescoming in to be evaluated.Before the compilation upgrades of this year,for every new sized image,a compilation would be invoked to do type inferencefor the whole graph.Now with control over compilation, you, the developer,can invoke compilationwith type inference pass turned off.This can save tens or hundreds of many secondsof compilation timeon each iteration and get the best performance.

MPSGraph runtime will infer typesjust in time during encoding and seamlessly make things work.It is a tradeoff between saving compilation timeversus getting the most optimal graph.Let's see how this can be used in the code exampleshared before.

Disabling the type inference pass can be achievedby setting the compilation descriptor as shown.That's it for compilation APIs.Finally, let's talk about the newcontrol flow APIs of MPSGraph.These APIs let you dynamically dispatch operationsbased on tensors previously evaluated by the graph.This is common in applications like batch normalizationand recurrent neural networks.Let's take a look at how a “while loop” can be implementedwith MPSGraph today without the new API.

First, we create a graphthat computes the predicate.Next, the predicate is evaluated on the CPUthrough an explicit memory synchronization.If the predicate is true, the previously created graphis re-executed with the new inputs.Otherwise, if the predicate is false,the loop ends and a second MPSGraph is createdand executed to consume the result.With the new control flow API, all these steps can be launchedas part of a single MPSGraph execution.

This is more convenient to implementbecause you don't have to introduceexplicit memory synchronization primitives.Now let's take a lookat how this can be potentially more efficient.Here we see the control flow timelinewithout the new API.We encode the first kernel on the CPU.Once the kernel is complete,we have to synchronize memory to read the result.This is potentially inefficient,as the CPU has to wait for the GPU to finish executing.Similarly, the GPU also has to waitfor the CPU synchronizationand subsequent encoding to complete.This happens in each iteration.Now let's see the benefitsof using the new MPSGraph API.We have to perform only one CPU encode call.Since the predicate is evaluated on the GPU timeline,no synchronization overhead is incurred,and the kernels can be launched without any bubbles.

Now let's see what the new APIs are.

We added three new control flow APIs:if/else, for loops,and while loops.Let's start with the if/else primitive.We are all familiar with this.Based on a predicate,different code paths are executed.We are provided a Boolean predicatealong with a code block for the “if” and “else” conditions.If this predicate is true,we execute the then block of code.Otherwise, if it's false,the else branch is executed.Having the if/else operationis very useful in neural networks.One canonical usage is in batchNormalization operation,which has different behavior in training and inference.With the isTraining Boolean, we can have a single graphto represent both variants of the normalizer.Let's look at how to set up an if/else branch in code.

Let's take a very simple exampleof two input scalar tensors.If the first tensor is smaller than the second,we return the sum of the operations.Else, we return the difference.First, we compute the predicateand pass that to the API.Next, when the predicate is true,we compute the then block and add the tensors.Finally, when the predicate is false,we compute the else block and subtract the tensors.Next, let's see how to implement a for loop.

The for loop primitive loops over a set of operationsa fixed number of times.This is common in recurrent neural networkswhere we have to loop over sequencesof different lengths during training.We need to provide the numberOfIterationsof the for loop.The index is initialized to 0and compared against the numberOfIterationseach loop iteration.If it's less than the numberOfIterations,we execute the body of the for loopand increment the index by 1.

When the index is equal toor greater than the numberOfIterations,we end the loop.Let's see how to implement this in code.

Let's say we wanted to implement a really simple example.We'll initialize the result variable to some input value.Then we loop four times, multiplying the resultby another input value each time.First, we create two graph tensors.The output tensor will be initialized to input0.In each iteration,this tensor will be multiplied by input1.Next, we set the numberOfIterations to 4so that we can execute the loop four times,from index 0 to index 3.Next, we create the body of the for loop.This is done by creating a closurewhich represents a single iteration.Each iteration is passed the index of the current iteration,as well as the output of the previous iteration.Then, we'll update the result and return it,to be passed to the next iteration.Finally, we pass all these argumentsto the for loop API in the graph.Note that the iterationArguments of the bodyare initialized to input0 tensor.That's it for the for loop.Now let's look at the while loop API.

This primitive executes a set of operationswhile a condition is met.We need to provide two blocks of codeto use this API.In the first block,the condition is checked with a predicate.When the predicate is true, the body of the while loopin the after block is executed.This recomputes the predicate.MPSGraph then uses this predicatein the next iteration of the before block.If the condition evaluated is false,it exits the loop.The API also allows for implementing the do-while loopby swapping the bodyand condition evaluation code blocks.Let's say we wanted to implement a really simple example.We'll initialize the result variable to some input value.Then we'll multiply the result by a multiplier each timein a loop till we exceed a threshold.First, we define a block of code that will evaluate the predicateusing the result of the previous iteration.It also stores the results of the previous iterationin a returnTensors NSArray.This array will be used as the inputto the next iteration when the predicate is trueand used as the final resultif the predicate is false.Next, we define the body of the while loopwhere the tensors are multiplied.The product is returned for the condition block to read.

Finally, we'll pass all these argumentsto the while loop API as shown.Note the initialInputs argumentis used in the first iteration of the before block.

That's it for while loops.Next, we'll see how this can be used in a real application.Image composition is a common image editing utility.Here, an object is implanted into a target image.We start with a source imageand a background image, as shown.Next, we create a mask on the source image.Let's place this mask of the source imagedirectly against the background.That does not look great,as we can clearly see the edges of the source image.Through image composition,we want to smoothen these edges.Pairing a Laplacian edge filter with an iterative linear solveris a common way to achieve this.Now let's look at the details.Here, we see the pipeline neededto perform image composition with MPSGraph.We start with our input tensors, the background image,source image, and a mask of the object.Next we use an iterative linear solvercoupled with a Laplacian edge detector.The output of this set of operations isa composite image with smooth edges.Let's take a look at the Laplacian edge filter.Implementing the Laplacian edge filterinvolves a windowed reductionover the source image with a set of weights.The stencil operator is usedto implement this as shown.Using this operator, we are ableto see the edges of the source object.The edges computed here will be usedas the input to the linear solver.Next, let's take a look at the linear solver.

We start with the background imageand feed it into the linear solver.The solver updates this image,and the result is subsequently read back in.As we can see, this is an iterative process.As the iterations progress,the solution image improves till we arriveat the perfect blend at the edges.The loop terminates when the erroris below a user defined tolerance.This requires a while loop.You can now use the MPSGraph Control Flow APIto implement this.Now, let's look at the demo.We have implemented an image composition utilityusing the MPSGraph as an iPad Pro application.

We start with a source image on the topand a target image below.We will be cloning objectsfrom the source to the target.The first thing we need to do is to draw a maskaround the cow that we want to move.

Let's see how this looks with a naive clone.

That does not look very good, as we can see the rough edges.Now let's try the image composition techniquewe just described.We will start by settingthe initial solution to the background image.Let's run this for about 50 iterations.

Clearly, the solution image has not yet converged.Let's run it for about 50 more iterations.

This looks way more natural as the edges smoothen out.The ease of programming with MPSGraphmakes experimenting with different techniquesstraightforward.Initializing the solver with the cloned imageinstead of the background imagecan result in faster convergence.We can enable this initialization modeby toggling this switch.Let's see this in action by setting the iteration countto 50 again and resetting to the naive clone.

Now let's rerun the solver.We can see the solution imageafter 50 iterations looks pretty good.Since we already start with the source object,we also observe less bleeding at the edges.This is great.But what we really want is to automate convergencebased on an error tolerance.This will require a while loopwhich we will enable by using this switch.We have implemented this with the new MPSGraph API.The error tolerance can be controlled with this slider.We have set it to 0.1, as shown.Let's reset this back to the naive clone.

Now we start the solver.With this while loop, we converge to the solution imagein about 80 iterationswithout me having to specify any iteration count.Now let's have some funby cloning other animals onto this background.Let's try this cute puppy.

All right, done tracing.I think it would look great at the bottom rightof this image.

Maybe we can try a bird next.

This would look good on the top right of the background.The new background with all these images looks pretty neat.That's it for the demo.

In summary, we showed how adopting MPSGraphled to amazing performance improvementsfor CoreML and TensorFlow.Inference is now up to twice as fast.We introduced useful new compute primitives,including the stencil operator that is going to enablea wide range of applications.

We showed new compilation flexibilitythat MPSGraph offers.This is going to shave off latency from inference networks.

And finally, we showed all newcontrol flow capabilities of MPSGraph.This API is key to expressingseveral linear algebra applicationsin addition to machine learning networks.

We are excited to see how youwill take advantage of these features.Thank you, and have a great WWDC 2021.[upbeat music]

8:35 -Control dependencies 1

9:01 -Control dependencies 2

14:42 -Evaluation method

16:38 -Disabling the type inference pass

19:22 -If/else in batch normalization

19:46 -If/else

20:58 -For loop 1

21:12 -For Loop 2

21:33 -For Loop 3

21:52 -For Loop 4

22:51 -While loop 1

23:01 -While loop 2

23:22 -While loop 3

23:33 -While loop 4

25:00 -Edge filter

## Code Samples

```swift
// Execute the graph


let
 results 
=
 graph.run(feeds: [inputTensor: inputs],
                        targetTensors: [exp],
                        targetOperations: [assign])
```

```swift
// Create control dependency



let
 exp 
=
 graph.controlDependency(with: [assign],
                                  dependentBlock: { 
                                      
return
 [graph.exponent(with: input, 
                                                             name: 
nil
)]
                                  },
                                  name: 
nil
)


// Execute the graph



let
 results 
=
 graph.run(feeds: [inputTensor: inputs],
                        targetTensors: [exp],
                        targetOperations: 
nil
)
```

```swift
// Create the graph



let
 placeholder0 
=
 graph.placeholder(shape: [
1
, 
3
], 
                                     dataType: .float32, 
                                     name: 
nil
)


let
 placeholder1 
=
 graph.placeholder(shape: [
2
, 
1
], 
                                     dataType: .float32, 
                                     name: 
nil
)


let
 addTensor 
=
 graph.addition(placeholder0, 
                               placeholder1, 
                               name: 
nil
)


// Compile the graph into an executable



let
 executable 
=
 graph.compile(with: 
nil
,
                               feeds: [placeholder0: 
MPSGraphShapedType
(shape: [
1
, 
3
], 
                                                                        dataType: .float32),
                                       placeholder1: 
MPSGraphShapedType
(shape: [
2
, 
1
], 
                                                                        dataType: .float32)],
                               targetTensors: [addTensor],
                               targetOperations: 
nil
,
                               compilationDescriptor: 
nil
)


// Execute the graph into an executable



let
 fetch 
=
 executable.run(with: commandQueue,
                           inputs: [
MPSGraphTensorData
(input0),        
                                    
MPSGraphTensorData
(input1)],
                           results: 
nil
,
                           executionDescriptor: 
nil
)
```

```swift
// Create the graph compilation descriptor



let
 descriptor 
=
 
MPSGraphCompilationDescriptor
()


// Disable type inference


descriptor.disableTypeInference()


// Compile the graph into an executable



let
 executable 
=
 graph.compile(with: 
nil
,
                               feeds: 
/* feeds */
,
                               targetTensors: 
/* target tensors */
,
                               targetOperations: 
nil
,
                               compilationDescriptor: descriptor)


// execute the graph
```

```swift
// Different behavior during inference and training



let
 results 
=
 graph.if(isTraining,
                       then: { 
...
 },    
// compute mean and variance

                       else: { 
...
 },    
// use running_mean and running_variance

                       name: 
nil
)
```

```swift
let
 predicate 
=
 graph.lessThan(a, 
                               b, 
                               name: 
nil
)


let
 results 
=
 graph.if(predicate,
    then: {[
        graph.addition(a, 
                       b, 
                       name: 
nil
)
    ]},
    else: {[
        graph.subtraction(a, 
                          b, 
                          name: 
nil
)
    ]},
    name: 
nil
)
```

```swift
var
 result 
=
 input0


for
 i 
in
 
0
..<
4
 {
    result 
*=
 input1
}
```

```swift
// Initialize inputs



let
 input0 
=
 graph.placeholder(shape: [], 
                               dataType: .int32, 
                               name: 
nil
)


let
 input1 
=
 graph.placeholder(shape: [], 
                               dataType: .int32, 
                               name: 
nil
)
        

let
 numberOfIterations 
=
 graph.constant(
4
, 
                                        shape: [], 
                                        dataType: .int32)
```

```swift
// Define Body



let
 body 
=
 {
    (index: 
MPSGraphTensor
, iterationArguments: [
MPSGraphTensor
]) -> [
MPSGraphTensor
] 
in

        
let
 iterationResult 
=
 graph.multiplication(iterationArguments[
0
], input1, name: 
nil
)
        
return
 [iterationResult]
}
```

```swift
// Create for loop operation



let
 result 
=
 graph.for(numberOfIterations: numberOfIterations,
                       initialIterationArguments: [input0],
                       body: body)
```

```swift
var
 result 
=
 initialValue


while
 result 
<
 threshold {
    result 
*=
 multiplier
}
```

```swift
// Evaluate condition



let
 condition 
=
 {
    (inputs: [
MPSGraphTensor
], returnTensors: 
NSMutableArray
) -> 
MPSGraphTensor
 
in

        
let
 predicate 
=
 graph.lessThan(inputs[
0
], threshold, name: 
nil
)
        returnTensors.add(inputs[
0
])
        
return
 predicate
}
```

```swift
// Define body



let
 body 
=
 {
    (inputs: [
MPSGraphTensor
]) -> [
MPSGraphTensor
] 
in

        
let
 iterationResult 
=
 graph.multiplication(inputs[
0
], multiplier, name: 
nil
)
        
return
 [iterationResult]
}
```

```swift
// Create while loop operation



let
 results 
=
 graph.while(initialInputs: [initialValue],
                          before: condition,
                          after: body,
                          name: 
nil
)
```

```swift
// Apply the laplacian edge filter on the source image



let
 edges 
=
 graph.stencil(with: source, 
                          weights: laplacianWeights, 
                          descriptor: desc, 
                          name: 
nil
)
```

