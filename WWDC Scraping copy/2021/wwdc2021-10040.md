# Wwdc2021 10040

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Detect people, faces, and poses using VisionDiscover the latest updates to the Vision framework to help your apps detect people, faces, and poses. Meet the Person Segmentation API, which helps your app separate people in images from their surroundings, and explore the latest contiguous metrics for tracking pitch, yaw, and the roll of the human head. And learn how these capabilities can be combined with other APIs like Core Image to deliver anything from simple virtual backgrounds to rich offline compositing in an image-editing app.

To get the most out of this session, we recommend watching “Detect Body and Hand Pose with Vision” from WWDC20 and “Understanding Images in Vision Framework” from WWDC19.
To learn even more about people analysis, see “Detect Body and Hand Pose with Vision” from WWDC20 and “Understanding Images in Vision Framework” from WWDC19.ResourcesApplying Matte Effects to People in Images and VideoVisionHD VideoSD VideoRelated VideosWWDC22What's new in VisionWWDC21Classify hand poses and actions with Create MLWWDC20Detect Body and Hand Pose with VisionWWDC19Understanding Images in Vision Framework

Discover the latest updates to the Vision framework to help your apps detect people, faces, and poses. Meet the Person Segmentation API, which helps your app separate people in images from their surroundings, and explore the latest contiguous metrics for tracking pitch, yaw, and the roll of the human head. And learn how these capabilities can be combined with other APIs like Core Image to deliver anything from simple virtual backgrounds to rich offline compositing in an image-editing app.

To get the most out of this session, we recommend watching “Detect Body and Hand Pose with Vision” from WWDC20 and “Understanding Images in Vision Framework” from WWDC19.
To learn even more about people analysis, see “Detect Body and Hand Pose with Vision” from WWDC20 and “Understanding Images in Vision Framework” from WWDC19.

Applying Matte Effects to People in Images and Video

Vision

HD VideoSD Video

HD Video

SD Video

What's new in Vision

Classify hand poses and actions with Create ML

Detect Body and Hand Pose with Vision

Understanding Images in Vision Framework

Search this video…♪ Bass music playing ♪♪Sergey Kamensky: Hello, everybody and welcome to WWDC.My name is Sergey Kamensky and I'm a software engineerin Vision framework team.The topic of today's session is to show how Vision frameworkcan help with people analysis.Our agenda today consists of two main items.First we're going to have an overviewof people analysis technology in Vision framework.While doing that,we will be specifically focusing on the new additions.And second, we're going to have the in-depth reviewof the new person segmentation feature.Let's start with people analysis technology first.The cornerstone of people analysis in Visionis person face analysis.Since the inception of Vision framework,we've been adding and enhancinghuman face analysis capabilities.We currently offer face detection,face landmarks detection, and face capture quality detection.Face detection functionality in Vision frameworkis exposed by the means of DetectFaceRectanglesRequest.Our face detector offers high precision and recall metrics,it can find faces with arbitrary orientation,different sizes, and also partially occluded.So far we have supported occlusionslike glasses and hats.Now we are upgrading our face detectorto revision number three which,in addition to improving all existing great qualities,can now also detect faces covered by masks.The main function of our face detectoris of course to find face bounding box,but it can also detect face pose metrics.Previously, we have provided roll and yaw metrics only.Most metrics are reported in radiansand their values are returned in discrete bins.With the new revision introduction,we're also adding a pitch metricand thus completing the full picture.But we didn't stop there.We're also making all three metricsreported in continuous space.All face pose metrics are returned as propertyof our FaceObservation object,which is the result of executing the DetectFaceRectanglesRequest.Let's look at the demo app that is designed to showface pose detection functionality.The app processes camera feedby running Vision face detectorand presents face pose metrics to the userafter converting the results from radians to degrees.For better tracking of the metric changes,the app uses red color gradient to show when face pose metricsincrease in positive directionand blue color gradient to show when the metricsincrease in negative direction.In both cases, the lighter the color is,the closer the metric is to the zero position.The zero position for each metric is what you would calla neutral position of a human head,when the person is looking straight -- just like this.As we already discussed, we have three face pose metrics:roll, yaw, and pitch.These terms come from flight dynamicsand they describe aircraft principle axeswith respect to the aircraft's center of gravity.The same terms have been adopted to describehuman head pose as well.When applied to head pose --or as we also call it, face pose --they track human head movement as following.Roll is tracking head movement in this direction.When I'm going from the most negativeto the most positive values of roll,you can see that the background color changesfrom dark blue to light blue, neutral, then light red,and finally dark red.Similar color changes are happening with the yaw metric,which is tracking the anglewhen the head is turning right or left.And finally, the pitch metric is tracking my head movementwhen my head is nodding up or down.Here you can see again similar color changeswhen I'm going from the most negativeto the most positive end of the spectrum.Face landmarks detection is another important functionof our face analysis suite.Face landmarks detection is offered by means ofDetectFaceLandmarksRequest,and the latest revision is revision number three.This revision offers 76-point constellationto better represent major face regionsand also provide accurate pupil detection.Face analysis suitealso includes face capture quality detection.This holistic measure takes into accountattributes like human face expressions, lighting,occlusions, blur, focusing, et cetera.It is exposed via DetectFaceCaptureQualityRequest APIand the latest revision of this requestis revision number two.It is important to remember that face capture qualityis a comparative measure of the same subject.This feature works great, for example,to pick the best photo out of the photo burst seriesor to pick the best photo to represent a personin the photo library.This feature is not designed to compare facesof different people.Human body analysis is another big section ofthe people analysis technology offered by Vision framework.Vision provides several functions in this area,which include human body detection,human pose detection, and last but not least,human hand pose detection.First let's take a look at the human body detection.This function is offered via DetectHumanRectanglesRequest,and it currently detects human upper body only.We are adding new functionality to this request,and therefore upgrading this revision to revision number two.With the new revision,in addition to the upper-body detection,we also offer a full-body detection.The choice between the upper-bodyand the full-body detection is controlled viathe new upperBodyOnly propertyof the DetectHumanRectanglesRequest.The default value for this property is set to trueto maintain backwards compatibility.Human body pose detection is offered in Vision frameworkvia DetectHumanBodyPoseRequest.Processing this request providesa collection of human body joint locations.Revision number one is the latestand the only available revision of this request.Vision framework also offers human hand pose detectionas DetectHumanHandPoseRequest.Similar to human body pose detection,processing of the hand pose requestreturns a collection of human hand joint locations.We're upgrading functionality of this requestby adding an important property to the resulting observation,hand chirality.The new chirality property of the HumanHandPoseObservationwill contain informationwhether the detected hand was left or right.If you want to learn more detailsabout hand pose detection, I recommend watchingthe "Classify hand poses and actions with Create ML" session.This concludes our overview of the new upgradesto the people analysis technology suite.It is time now to move to the second topic of our session,which is person segmentation.What is person segmentation?In very simple terms,it's the ability to separate people from the scene.There are numerous applicationsof the person segmentation technology nowadays.You are all familiar, for example,with virtual background feature on video conferencing apps.It's also used in live sport analysis,autonomous driving, and many more places.Person segmentation also powers our famous Portrait mode.Person segmentation in Vision frameworkis a feature designed to work with a single frame.You can use it in streaming pipeline,and it's also suitable for the offline processing.This feature is supported on multiple platformslike macOS, iOS, iPadOS, and tvOS.Vision framework implements semantic person segmentation,which means that it will return a single maskfor all people in the frame.Vision API for person segmentationis implemented by means ofGeneratePersonSegmentationRequest,This is a stateful request.As opposed to traditional requests in Vision framework,the stateful request objects are reused throughoutthe entire sequence of frames.In our particular case, using request objecthelps with smoothing temporal changes between the framesin fast-quality level model.Let's take a look at the Person Segmentation APIoffered by Vision framework.This API follows an already familiarand established pattern.Create a request,create a request handler,process your request with the request handler,and finally, review the results.Default initialization ofGeneratePersonSegmentationRequest objectis equivalent to setting revision, qualityLevel,and outputPixelFormat properties to their default values.Let's review all properties one by one.First is the revision property.Here we set the revision to revision number one.This is the default and the only available revision,since we're dealing with the new request type.Even though there is technically no choice here today,we always recommend to set explicitlyto guarantee deterministic behavior in the future.This is because if new revisions are introduced,the default will also change to representthe latest available revision.Second is the qualityLevel property.Vision API offers three different levels:accurate, which is also the default one;balanced; and fast.As far as the use cases go, we recommend usingaccurate level for computational photography.This is the use case where you would like to achievethe highest possible qualityand are typically not limited in time.Using similar logic, balanced level is recommendedfor video frame-by-frame segmentationand fast for streaming processing.The third property is the output mask format.We are going to review the resulting mask in details,but here I would like to mention that, as a client,you can specify which formatthe resulting mask will be returned in.There are three choices here: unsigned 8-bit integer maskwith a typical 0 to 255 quantization range,and two floating point mask formats --one with 32-bit full precisionand another with 16-bit half precision.The 16-bit half precision is intended to provide you witha reduced memory floating point formatthat can be inserted directlyinto further GPU-based processing with Metal.So far we have learned how to create, configure,and execute our person segmentation request.It is time now to look at the results.The result of processing person segmentation requestcome in form of PixelBufferObservation object.PixelBufferObservation derives from observationand it adds an important pixelBuffer property.The actual CVPixelBuffer object stored in this propertyhas the same pixel format as our person segmentation requestwas configured with.Processing of person segmentation requestwill produce a segmentation mask.Let's look at the original imageand three different quality level masksgenerated by executing person segmentation request.Fast, balanced, and accurate.Let's zoom in to look at the details of each mask.As expected, when we go from fast to balancedand eventually to accurate,the quality of the mask increasesand we'll start seeing more and more details.Now let's examine the different mask levelsas a function of quality versus performance.When we move from fast to balanced,and eventually to accurate, the quality of the mask goes upbut so does the resource usage.The dynamic range, mask resolution,memory consumption, processing time all go upwhen the mask quality increases.This represents a trade-offbetween the quality of the segmentation maskand the resource consumption needed to compute the mask.So you already know everything about mask generationand their properties.What can you actually do with the masks?Let's start with three images.The input image, the segmentation maskthat was obtained by processing the input image,and the background image.What we would like to do is to replace the backgroundin the original image in the area outside of the mask regionwith the background from a different image.When you execute such blending operation,we end up with the young man in the original imagebeing transported from the beach promenade to the forest.How does this blending sequence look like in the code?First let's assume we have done all relevant processingand already have our three images:the input image, the mask, and the background.Now we need to scale both the mask and the backgroundto the size of the original image.Then we will create and initializethe Core Image blending filter.You probably noticed that I created my blending filterwith the red mask.This is because when CIImage is initializedwith one component PixelBuffer --as all our masks are --it creates an object with the red channel by default.Finally, we perform the blending operation to get our results.Let's take a look how we can use person segmentation featurein Vision framework.Our second demo app,which is available for downloading,combines face pose metric detectionwith the new person segmentation capability.The app processes camera feed by running face detectionand person segmentation.Then that takes up the end segmentation maskand uses it to replace the background in the areaoutside of the mask pixels with a different color.The decision of what background color to usecomes from the combination of valuesfor roll, yaw, and pitch at any given point in time.I'm currently located in a room with a table and chairs,and the demo app shows my segmented silhouetteover the new background, which is the color mixcorresponding to my head position.Let's see if it tracks roll, yaw, and pitch changes.When I turn my head like this, roll is a major contributorto the background color mix decision.When I turn my head left and right,yaw becomes the major contributor.And finally, nodding my head up and downmakes pitch the major contributor.Vision framework is not the only placethat offers person segmentation API.There are several other frameworks that offersimilar functionality powered by the same technology.Let's take a brief look at each one of them.First is the AVFoundation.AVFoundation can return a person segmentation maskin some of the newer-generation devicesduring photo capture session.The segmentation mask is returnedvia PortraitEffectsMatte property of AVCapturePhoto.In order to get it, you will first need to checkif it's supported;and if it is, enable the delivery of it.The second framework that offers person segmentation APIis ARKit.This functionality is supported on A12 Bionic and later devices,and is generated when processing camera feed.The segmentation mask is returnedvia segmentationBuffer property of ARFrame.Before attempting to retrieve it,you need to check if it's supported by examiningsupportsFrameSemantics propertyof ARWorldTrackingConfiguration class.The third framework is Core Image.Core Image offers a thin wrapperon top of Vision person segmentation API,so you can perform the entire use casewithin the Core Image domain.Let's take a look now at how person segmentationcan be implemented using Core Image API.We will start with logging an imageto perform segmentation on.Then we will create a person segmentation CIFilter,assign an inputImage to it,and execute the filter to get our segmentation mask.We have just reviewed multiple versionsof person segmentation APIs and Apple SDKs.Let's summarize to see where each one could be used.AVFoundation is available on some of iOS deviceswith AVCaptureSession.If you have a capture session running,this will be your choice.If you're developing an ARKit app,you should already have an AR sessionwhere you can get your segmentation mask from.In this case, ARKit API is the recommended one to use.Vision API is available on multiple platformsfor online and offline single-frame processing.And finally, Core Image offers a thin wrapperaround Vision API, which is a convenient optionif you want to stay within the Core Image domain.As any algorithm, person segmentationhas its best practices -- or in other words,the set of conditions where it works best.If you're planning to use person segmentation feature,your app will perform betterif you try to follow these rules.First, you should try to segment up to four people in the scenewhere all people are mostly visible,while maybe with natural occlusions.Second, the height of each personshould be at least half of the image height,ideally with good contrast compared to the background.And third, we also recommend you to avoid ambiguitieslike statues, pictures of people,people at far distance.This concludes our session.Let's take a brief look at what we have learned today.First, we had an overviewof the person analysis technology in Vision frameworkwhile focusing on the upgrades like masked face detection,adding face pitch metric,and making all face pose metrics reported in continuous space.We also introduced new hand chirality metricto the human hand pose detection.In the second part, we took a deep dive intothe new person segmentation API added to Vision framework.We also looked into other APIs offering similar functionalityand provided the guidance where each one could be used.I really hope that by watching this session,you have learned new tools for developing your appsand are really eager to try them right away.Before we finish today,I would like to thank you for watching,wish you good luck,and have a great rest of the WWDC.♪

♪ Bass music playing ♪♪Sergey Kamensky: Hello, everybody and welcome to WWDC.My name is Sergey Kamensky and I'm a software engineerin Vision framework team.The topic of today's session is to show how Vision frameworkcan help with people analysis.Our agenda today consists of two main items.First we're going to have an overviewof people analysis technology in Vision framework.While doing that,we will be specifically focusing on the new additions.And second, we're going to have the in-depth reviewof the new person segmentation feature.Let's start with people analysis technology first.The cornerstone of people analysis in Visionis person face analysis.Since the inception of Vision framework,we've been adding and enhancinghuman face analysis capabilities.We currently offer face detection,face landmarks detection, and face capture quality detection.Face detection functionality in Vision frameworkis exposed by the means of DetectFaceRectanglesRequest.Our face detector offers high precision and recall metrics,it can find faces with arbitrary orientation,different sizes, and also partially occluded.So far we have supported occlusionslike glasses and hats.Now we are upgrading our face detectorto revision number three which,in addition to improving all existing great qualities,can now also detect faces covered by masks.The main function of our face detectoris of course to find face bounding box,but it can also detect face pose metrics.Previously, we have provided roll and yaw metrics only.Most metrics are reported in radiansand their values are returned in discrete bins.With the new revision introduction,we're also adding a pitch metricand thus completing the full picture.But we didn't stop there.We're also making all three metricsreported in continuous space.All face pose metrics are returned as propertyof our FaceObservation object,which is the result of executing the DetectFaceRectanglesRequest.Let's look at the demo app that is designed to showface pose detection functionality.The app processes camera feedby running Vision face detectorand presents face pose metrics to the userafter converting the results from radians to degrees.For better tracking of the metric changes,the app uses red color gradient to show when face pose metricsincrease in positive directionand blue color gradient to show when the metricsincrease in negative direction.In both cases, the lighter the color is,the closer the metric is to the zero position.The zero position for each metric is what you would calla neutral position of a human head,when the person is looking straight -- just like this.As we already discussed, we have three face pose metrics:roll, yaw, and pitch.These terms come from flight dynamicsand they describe aircraft principle axeswith respect to the aircraft's center of gravity.The same terms have been adopted to describehuman head pose as well.When applied to head pose --or as we also call it, face pose --they track human head movement as following.Roll is tracking head movement in this direction.When I'm going from the most negativeto the most positive values of roll,you can see that the background color changesfrom dark blue to light blue, neutral, then light red,and finally dark red.Similar color changes are happening with the yaw metric,which is tracking the anglewhen the head is turning right or left.And finally, the pitch metric is tracking my head movementwhen my head is nodding up or down.Here you can see again similar color changeswhen I'm going from the most negativeto the most positive end of the spectrum.Face landmarks detection is another important functionof our face analysis suite.Face landmarks detection is offered by means ofDetectFaceLandmarksRequest,and the latest revision is revision number three.This revision offers 76-point constellationto better represent major face regionsand also provide accurate pupil detection.Face analysis suitealso includes face capture quality detection.This holistic measure takes into accountattributes like human face expressions, lighting,occlusions, blur, focusing, et cetera.It is exposed via DetectFaceCaptureQualityRequest APIand the latest revision of this requestis revision number two.It is important to remember that face capture qualityis a comparative measure of the same subject.This feature works great, for example,to pick the best photo out of the photo burst seriesor to pick the best photo to represent a personin the photo library.This feature is not designed to compare facesof different people.Human body analysis is another big section ofthe people analysis technology offered by Vision framework.Vision provides several functions in this area,which include human body detection,human pose detection, and last but not least,human hand pose detection.First let's take a look at the human body detection.This function is offered via DetectHumanRectanglesRequest,and it currently detects human upper body only.We are adding new functionality to this request,and therefore upgrading this revision to revision number two.With the new revision,in addition to the upper-body detection,we also offer a full-body detection.The choice between the upper-bodyand the full-body detection is controlled viathe new upperBodyOnly propertyof the DetectHumanRectanglesRequest.The default value for this property is set to trueto maintain backwards compatibility.Human body pose detection is offered in Vision frameworkvia DetectHumanBodyPoseRequest.Processing this request providesa collection of human body joint locations.Revision number one is the latestand the only available revision of this request.Vision framework also offers human hand pose detectionas DetectHumanHandPoseRequest.Similar to human body pose detection,processing of the hand pose requestreturns a collection of human hand joint locations.We're upgrading functionality of this requestby adding an important property to the resulting observation,hand chirality.The new chirality property of the HumanHandPoseObservationwill contain informationwhether the detected hand was left or right.If you want to learn more detailsabout hand pose detection, I recommend watchingthe "Classify hand poses and actions with Create ML" session.This concludes our overview of the new upgradesto the people analysis technology suite.It is time now to move to the second topic of our session,which is person segmentation.What is person segmentation?In very simple terms,it's the ability to separate people from the scene.There are numerous applicationsof the person segmentation technology nowadays.You are all familiar, for example,with virtual background feature on video conferencing apps.It's also used in live sport analysis,autonomous driving, and many more places.Person segmentation also powers our famous Portrait mode.Person segmentation in Vision frameworkis a feature designed to work with a single frame.You can use it in streaming pipeline,and it's also suitable for the offline processing.This feature is supported on multiple platformslike macOS, iOS, iPadOS, and tvOS.Vision framework implements semantic person segmentation,which means that it will return a single maskfor all people in the frame.

Vision API for person segmentationis implemented by means ofGeneratePersonSegmentationRequest,This is a stateful request.As opposed to traditional requests in Vision framework,the stateful request objects are reused throughoutthe entire sequence of frames.In our particular case, using request objecthelps with smoothing temporal changes between the framesin fast-quality level model.Let's take a look at the Person Segmentation APIoffered by Vision framework.This API follows an already familiarand established pattern.Create a request,create a request handler,process your request with the request handler,and finally, review the results.Default initialization ofGeneratePersonSegmentationRequest objectis equivalent to setting revision, qualityLevel,and outputPixelFormat properties to their default values.Let's review all properties one by one.First is the revision property.Here we set the revision to revision number one.This is the default and the only available revision,since we're dealing with the new request type.Even though there is technically no choice here today,we always recommend to set explicitlyto guarantee deterministic behavior in the future.This is because if new revisions are introduced,the default will also change to representthe latest available revision.Second is the qualityLevel property.Vision API offers three different levels:accurate, which is also the default one;balanced; and fast.As far as the use cases go, we recommend usingaccurate level for computational photography.This is the use case where you would like to achievethe highest possible qualityand are typically not limited in time.Using similar logic, balanced level is recommendedfor video frame-by-frame segmentationand fast for streaming processing.The third property is the output mask format.We are going to review the resulting mask in details,but here I would like to mention that, as a client,you can specify which formatthe resulting mask will be returned in.There are three choices here: unsigned 8-bit integer maskwith a typical 0 to 255 quantization range,and two floating point mask formats --one with 32-bit full precisionand another with 16-bit half precision.The 16-bit half precision is intended to provide you witha reduced memory floating point formatthat can be inserted directlyinto further GPU-based processing with Metal.So far we have learned how to create, configure,and execute our person segmentation request.It is time now to look at the results.The result of processing person segmentation requestcome in form of PixelBufferObservation object.PixelBufferObservation derives from observationand it adds an important pixelBuffer property.The actual CVPixelBuffer object stored in this propertyhas the same pixel format as our person segmentation requestwas configured with.Processing of person segmentation requestwill produce a segmentation mask.Let's look at the original imageand three different quality level masksgenerated by executing person segmentation request.Fast, balanced, and accurate.Let's zoom in to look at the details of each mask.As expected, when we go from fast to balancedand eventually to accurate,the quality of the mask increasesand we'll start seeing more and more details.Now let's examine the different mask levelsas a function of quality versus performance.When we move from fast to balanced,and eventually to accurate, the quality of the mask goes upbut so does the resource usage.The dynamic range, mask resolution,memory consumption, processing time all go upwhen the mask quality increases.This represents a trade-offbetween the quality of the segmentation maskand the resource consumption needed to compute the mask.So you already know everything about mask generationand their properties.What can you actually do with the masks?Let's start with three images.The input image, the segmentation maskthat was obtained by processing the input image,and the background image.What we would like to do is to replace the backgroundin the original image in the area outside of the mask regionwith the background from a different image.When you execute such blending operation,we end up with the young man in the original imagebeing transported from the beach promenade to the forest.How does this blending sequence look like in the code?First let's assume we have done all relevant processingand already have our three images:the input image, the mask, and the background.Now we need to scale both the mask and the backgroundto the size of the original image.Then we will create and initializethe Core Image blending filter.You probably noticed that I created my blending filterwith the red mask.This is because when CIImage is initializedwith one component PixelBuffer --as all our masks are --it creates an object with the red channel by default.Finally, we perform the blending operation to get our results.Let's take a look how we can use person segmentation featurein Vision framework.Our second demo app,which is available for downloading,combines face pose metric detectionwith the new person segmentation capability.The app processes camera feed by running face detectionand person segmentation.Then that takes up the end segmentation maskand uses it to replace the background in the areaoutside of the mask pixels with a different color.The decision of what background color to usecomes from the combination of valuesfor roll, yaw, and pitch at any given point in time.I'm currently located in a room with a table and chairs,and the demo app shows my segmented silhouetteover the new background, which is the color mixcorresponding to my head position.Let's see if it tracks roll, yaw, and pitch changes.When I turn my head like this, roll is a major contributorto the background color mix decision.When I turn my head left and right,yaw becomes the major contributor.And finally, nodding my head up and downmakes pitch the major contributor.Vision framework is not the only placethat offers person segmentation API.There are several other frameworks that offersimilar functionality powered by the same technology.Let's take a brief look at each one of them.First is the AVFoundation.AVFoundation can return a person segmentation maskin some of the newer-generation devicesduring photo capture session.The segmentation mask is returnedvia PortraitEffectsMatte property of AVCapturePhoto.In order to get it, you will first need to checkif it's supported;and if it is, enable the delivery of it.The second framework that offers person segmentation APIis ARKit.This functionality is supported on A12 Bionic and later devices,and is generated when processing camera feed.The segmentation mask is returnedvia segmentationBuffer property of ARFrame.Before attempting to retrieve it,you need to check if it's supported by examiningsupportsFrameSemantics propertyof ARWorldTrackingConfiguration class.The third framework is Core Image.Core Image offers a thin wrapperon top of Vision person segmentation API,so you can perform the entire use casewithin the Core Image domain.Let's take a look now at how person segmentationcan be implemented using Core Image API.We will start with logging an imageto perform segmentation on.Then we will create a person segmentation CIFilter,assign an inputImage to it,and execute the filter to get our segmentation mask.We have just reviewed multiple versionsof person segmentation APIs and Apple SDKs.Let's summarize to see where each one could be used.AVFoundation is available on some of iOS deviceswith AVCaptureSession.If you have a capture session running,this will be your choice.If you're developing an ARKit app,you should already have an AR sessionwhere you can get your segmentation mask from.In this case, ARKit API is the recommended one to use.Vision API is available on multiple platformsfor online and offline single-frame processing.And finally, Core Image offers a thin wrapperaround Vision API, which is a convenient optionif you want to stay within the Core Image domain.As any algorithm, person segmentationhas its best practices -- or in other words,the set of conditions where it works best.If you're planning to use person segmentation feature,your app will perform betterif you try to follow these rules.First, you should try to segment up to four people in the scenewhere all people are mostly visible,while maybe with natural occlusions.Second, the height of each personshould be at least half of the image height,ideally with good contrast compared to the background.And third, we also recommend you to avoid ambiguitieslike statues, pictures of people,people at far distance.This concludes our session.Let's take a brief look at what we have learned today.First, we had an overviewof the person analysis technology in Vision frameworkwhile focusing on the upgrades like masked face detection,adding face pitch metric,and making all face pose metrics reported in continuous space.We also introduced new hand chirality metricto the human hand pose detection.In the second part, we took a deep dive intothe new person segmentation API added to Vision framework.We also looked into other APIs offering similar functionalityand provided the guidance where each one could be used.I really hope that by watching this session,you have learned new tools for developing your appsand are really eager to try them right away.Before we finish today,I would like to thank you for watching,wish you good luck,and have a great rest of the WWDC.♪

8:13 -Get segmentation mask from an image

8:33 -Configuring the segmentation request

12:24 -Applying a segmentation mask

14:37 -Segmentation from AVCapture

14:58 -Segmentation in ARKit

15:31 -Segmentation in CoreImage

## Code Samples

```swift
// Create request 


let
 request 
=
 
VNGeneratePersonSegmentationRequest
()


// Create request handler


let
 requestHandler 
=
 
VNImageRequestHandler
(url: imageURL, options: options)


// Process request


try
 requestHandler.perform([request])


// Review results


let
 mask 
=
 request.results
!
.first
!


let
 maskBuffer 
=
 mask.pixelBuffer
```

```swift
let
 request 
=
 
VNGeneratePersonSegmentationRequest
()

request.revision 
=
 

VNGeneratePersonSegmentationRequestRevision1


request.qualityLevel 
=
 

VNGeneratePersonSegmentationRequest
.
QualityLevel
.accurate

request.outputPixelFormat 
=
 
kCVPixelFormatType_OneComponent8
```

```swift
let
 input 
=
 
CIImage
?(contentsOf: imageUrl)
!


let
 mask 
=
 
CIImage
(cvPixelBuffer: maskBuffer)

let
 background 
=
 
CIImage
?(contentsOf: backgroundImageUrl)
!



let
 maskScaleX 
=
 input.extent.width 
/
 mask.extent.width

let
 maskScaleY 
=
 input.extent.height 
/
 mask.extent.height

let
 maskScaled 
=
 mask.transformed(by: __CGAffineTransformMake(
                                  maskScaleX, 
0
, 
0
, maskScaleY, 
0
, 
0
))


let
 backgroundScaleX 
=
 input.extent.width 
/
 background.extent.width

let
 backgroundScaleY 
=
 input.extent.height 
/
 background.extent.height

let
 backgroundScaled 
=
 background.transformed(by: __CGAffineTransformMake(
                          backgroundScaleX, 
0
, 
0
, backgroundScaleY, 
0
, 
0
))


let
 blendFilter 
=
 
CIFilter
.blendWithRedMask()
blendFilter.inputImage 
=
 input
blendFilter.backgroundImage 
=
 backgroundScaled 
blendFilter.maskImage 
=
 maskScaled


let
 blendedImage 
=
 blendFilter.outputImage
```

```swift
private
 
let
 photoOutput 
=
 
AVCapturePhotoOutput
()

…


if
 
self
.photoOutput.isPortraitEffectsMatteDeliverySupported {
   
self
.photoOutput.isPortraitEffectsMatteDeliveryEnabled 
=
 
true

}


open
 
class
 
AVCapturePhoto
 
{

…


var
 portraitEffectsMatte: 
AVPortraitEffectsMatte
? { 
get
 } 
// nil if no people in the scene


…

}
```

```swift
if
 
ARWorldTrackingConfiguration
.supportsFrameSemantics(.personSegmentationWithDepth) {

// Proceed with getting Person Segmentation Mask


…

}


open
 
class
 
ARFrame
 
{

…


var
 segmentationBuffer: 
CVPixelBuffer
? { 
get
 }

…

}
```

```swift
let
 input 
=
 
CIImage
?(contentsOf: imageUrl)
!



let
 segmentationFilter 
=
 
CIFilter
.personSegmentation()
segmentationFilter.inputImage 
=
 input


let
 mask 
=
 segmentationFilter.outputImage
```

