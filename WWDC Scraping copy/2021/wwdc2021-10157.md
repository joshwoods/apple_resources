# Wwdc2021 10157

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Discover Metal debugging, profiling, and asset creation toolsExplore how Xcode can help you take your Metal debugging, profiling and asset creation workflows to the next level. Discover the latest tools for ray tracing and GPU profiling, and learn about Metal Debugger workflows. We'll also show you how to use the Texture Converter tool, which supports all modern GPU texture formats and can easily integrate into your multi-platform asset creation pipelines.ResourcesDebugging the shaders within a draw command or compute dispatchMetalMetal Developer Tools on WindowsHD VideoSD VideoRelated VideosWWDC23Bring your game to Mac, Part 3: Render with MetalMeet RealityKit TraceWWDC22Maximize your Metal ray tracing performanceProfile and optimize your game's memoryWWDC21Discover compilation workflows in MetalEnhance your app with Metal ray tracingExplore hybrid rendering with Metal ray tracingOptimize high-end games for Apple GPUsWWDC20Debug GPU-side errors in MetalDiscover ray tracing with MetalGet to know Metal function pointersOptimize Metal apps and games with GPU counters

Explore how Xcode can help you take your Metal debugging, profiling and asset creation workflows to the next level. Discover the latest tools for ray tracing and GPU profiling, and learn about Metal Debugger workflows. We'll also show you how to use the Texture Converter tool, which supports all modern GPU texture formats and can easily integrate into your multi-platform asset creation pipelines.

Debugging the shaders within a draw command or compute dispatch

Metal

Metal Developer Tools on Windows

HD VideoSD Video

HD Video

SD Video

Bring your game to Mac, Part 3: Render with Metal

Meet RealityKit Trace

Maximize your Metal ray tracing performance

Profile and optimize your game's memory

Discover compilation workflows in Metal

Enhance your app with Metal ray tracing

Explore hybrid rendering with Metal ray tracing

Optimize high-end games for Apple GPUs

Debug GPU-side errors in Metal

Discover ray tracing with Metal

Get to know Metal function pointers

Optimize Metal apps and games with GPU counters

Search this video…Hi! My name is Egor,and today, I’d like to tell you about all the improvementsand new features in Metal Debugger.This year, we are bringing supportfor more Metal features,such as ray tracing and function pointers.We have added brand-new profiling workflows,like the GPU timelineand consistent GPU performance state,to help you get the most out of GPUs across Apple platforms.We’ve made improvements to other debugging workflows you know and love,including broader support for shader validation,and precise capture controls.We are also introducing advances in texture compression,which my colleague Amanda will talk about later.First, let’s talk about ray tracing.Last year, we introduced a new Metal ray tracing API,and now, in Xcode 13,we support it in Metal Debugger,along with function pointers and function tables,which bring flexibility to your shaders.And dynamic libraries,which give you a way to build well-abstracted and reusableshader library code.Also, for ray tracing, we are introducinga brand-new tool,the Acceleration Structure Viewer.Take a look at ray tracing in the Metal Debugger.I’ve opened a GPU traceof the ModernRenderer sample app.It was modified to use the Metal ray tracingto achieve effects such as shadows and ambient occlusion.This encoder creates a beautiful ray traced shadow map.I’ve selected a dispatch callso that you can see the acceleration structurein bound resources.From here,I will open the acceleration structureto go to our new acceleration structure viewer.Here, you can see the geometry of the familiar bistro sceneon the right, and its outline on the left.Clicking on an instance in the scenewill select it in the viewer and also in the scene outline.You can see the transformation matrixand other instance properties by expanding it.You can also select an individual geometryby holding the Option keywhile clicking in the scene viewer.This will also select it in the scene outline,and vice versa.You can also see the relevant intersection functionsused with the acceleration structureright here in the viewer.But the Acceleration Structure Viewercan do much more than simply display geometry.Here on the bottom-right,you will also find a number of highlighting modesthat will help you visualize some of the propertiesof your scene.For example,the bounding volume traversals modecan help you visualize the complexity of your geometry.A deeper blue color shows areaswhere the bounding volume hierarchyis more computationally expensive to traverserelative to other parts of it.For all of the modes, we have this small viewthat shows the relevant informationwhen you hover over different parts of your scene.Here, it displays the number of bounding box traversalsand primitive intersections.To give you more flexibility,we also included traversal settings.With them, you can configurethe acceleration structure viewer,using same properties you can findon an intersector object inside your shaders.There’s so much more to talk aboutwhen it comes to ray tracing.If you’d like to learn more, check this year’s session“Explore hybrid rendering with Metal ray tracing.”And if you want to know more about the API in general,check out last year’s talk,“Discover ray tracing with Metal.”Next, let’s talk about profiling.Profiling your app is an important step,and we already have a lot of great tools at your disposal.For example, using the Metal systemtrace in instruments,you can explore a timeline viewthat shows CPU and GPU durationsfor different rendering stages,GPU counters, and shader timelines.And in the Metal Debugger,GPU counters show a rich set of measurementsdirectly from the GPU,either per encoder or per draw.Both are excellent tools,which provide complementary views of your app’s performance.But aligning those views may take additional effort.So, that’s why I’m excited to show youa new GPU profiling toolthat combines Metal system traceand GPU counters in a unified experience.Introducing GPU Timeline in Metal Debugger,a new tool designed specifically for Apple GPUs.It gives you a different perspectiveon the performance data,and it can help you find potential points of optimizationin your app.Let’s walk through this latest additionto our suite of profiling tools.The GPU Timeline is availableunder the Performance panel.You can find it in the debug navigatorafter you’ve captured a frame from your app.When you open the Performance panel,you’ll be greeted by a set of different trackslaid out in parallel.Before we continue, I want to explainwhy encoder tracks are in parallel.On Apple GPUs,vertex and fragment stages of different render passes,and also compute dispatches, can run simultaneously.This is enabled by Apple GPU architectureand use of rendering techniquecalled “tile-based deferred rendering.”We thought it is important for you to be able to see thisparallel nature of Apple GPUs in the context of your app.And that’s where GPU Timeline comes in handy.At the top, you can seethe Vertex, Fragment, and Compute encoder timelines,with each encoder showing the resources it usesat a quick glance.Below the encoders,you will find Occupancy, Bandwidth, and limiter counters.Let’s take a closer look at the encoder timeline.You can expand each encoder trackto see an aggregated shader timeline.Expanding the timeline even furtherwill show you each individual shaderin a waterfall-like fashion.It’s easy to navigate the encoders.Select an encoder trackto see a list of all the encoders on the right.There, you can sort them by their average duration.Clicking on an individual encoder in the timelinewill show you more information about it in the sidebar.For example, here you can seethe attachments for this render command encoder.You may have noticed that when you select an encoder,time ranges where it’s activebecome highlighted across all the tracks.With this, you can easily examinehow different stages overlap,and also correlate counter values for the encoder.Moving away from the timeline view,you can access the GPU countersby switching to the Counters tab,or you can just open encoder’s context menuand reveal it in Counters from there.And this is just a sneak peek at the GPU Timeline.To learn more about using Metal Debuggerto understand your app’s performance,check out this year’s session“Optimize high-end games for Apple GPUs.”Now that I’ve showed you a new way to profile your app,it’s important to understandthat its performance depends on several factors.When we are talking about Metal,the GPU performance state is a very important factor.It’s managed by the operating system,which will lower or raise the statedepending on device thermals,system settings, GPU utilization,and other parameters.These state changes can affectprofiling results you are seeing.This year, we are introducing new waysfor you to profile your app with more consistent results.We have added ways for you to see and changethe GPU performance stateacross our whole suite of Metal tools,starting with Instruments and Metal system tracefor live performance recordings,continuing with Metal Debugger,for profiling GPU traces,and finally, Device conditions in Xcode,for general use cases.First, let’s talk about Instruments.This year, we’ve added a track for GPU performance stateto the Metal system trace.Use it in conjunction with the other tracksto correlate your app’s performancewith the device’s performance state.Keep in mind, though,that being able to see the performance stateis just part of the equation.In order to get profiling resultsthat are consistent and reproducible,you also need a way to seta GPU performance state on a device.New this year is the abilityto induce a specific GPU performance statewhen you are recording a trace in Instruments.Simply go to Recording Options,and choose a performance state before the recording starts.After that, you can record the performance trace as usual.Instruments will induce the state you chosefor the duration of the trace, if the device can sustain it.Sometimes, you might need to checkif an existing Instruments trace had a GPU performance stateinduced during recording.You can find this informationin the “Recording Settings” sectionin the information popover.And now, you know how to viewand induce the GPU performance state from Instruments.The second way to leveragea consistent GPU performance stateis by using Metal Debugger.By default, when you capture a GPU trace of your app,Xcode will profile the trace for you.And it will do so using the same performance statethe device was in at the time of the capture.That state may have fluctuated,depending on the factors we mentioned previously.If, instead, you would like to selecta certain performance state yourself,use the Stopwatch button in the debug bar.After you make a selection,Metal Debugger will profile your GPU trace again.After it’s done, the button is highlightedto reflect that the consistent performance state was achieved.Also, “Performance” section on the summary pagenow shows new performance data at a glance,as well as the selected performance state.These two approaches are tied to the suite of Metal tools.But sometimes, you might want to inducea consistent performance stateoutside of profiling workflows.The third way to set a GPU performance stateis through Device conditions.If you want to test how your app performsunder different GPU performance states,this is the option for you.In Xcode 13, we have addedthe GPU performance state device condition.It forces the operating systemto use the specified state on a device,as long as it can sustain itand it stays connected to the Xcode.You can add this condition from Xcode,if you go to Window, Devices and Simulators,choose your device there,then scroll to the “Device Conditions” section,and add a “GPU Performance State”condition with the desired level.Press Start when you want to applythe GPU performance state change on the device.Then, when you are done, press Stop.These new ways to see and change GPU performance stateright from our toolsshould help you with profiling and testing your apps.And I think you are going to loveour latest additions and improvementsto profiling workflows,and I hope they will help you make your apps even better.Now, let’s talk about some other improvementswe are bringing to Metal Debugger this year.First, I’ll tell you aboutimprovements to shader validation.Then, I’ll show you precise capture controls.And after that, I’ll give you a lookat the new pipeline state workflows.Finally, I want to introduce two new featuresrelated to shader debugging and profiling,separate debug information and selective shader debugging.Last year, in Xcode 12,we introduced shader validation,which helps you diagnose runtime errors on the GPU,like out of bounds access, and others.Remember that if shader validation is enabled,and an encoder raises a validation error,you will get a runtime issue in the issue navigator,showing both the CPU and GPU backtracesfor the call that faulted.We already have a session that covers this in a greater detail,so to learn more about using shader validation,check out last year’s talk, called“Debugging GPU-side errors in Metal.”This year, we are extending shader validationto support more use cases,making it availablewhen you are using indirect command buffers,dynamic libraries,and function pointers and tables.This should allow you to use shader validationmore extensively throughout your appduring its development.Next, I wanna show youour new precise capture controls.But first, take a look at the Capture button,which now looks like the Metal logo.It’s located on the debug bar,at the bottom of the Xcode window.When you click it, a new menu appears.This menu lets you choose a scope for your capture.The default is to capture one frame,but you can specify how many you want to capture,up to five.You can also choose to capture a number of command buffersthat have the same parent device,or a command queue,as well as those that present a certain Metal layer,and even custom scopes that you can definein your app’s code using MTLCaptureScope API.These new controls give you incredible powerout-of-the-box in deciding how and whenyour Metal calls are captured.Next, let’s talk about Metal libraries and pipeline states.These are the essential building blocks of your Metal app.And in Xcode 13,we’ve made it easier than everto examine all the pipeline states and librariesyour app is using.Now, let’s see how it looks in practice.Here, I’ve captured a GPU tracefrom a ModernRenderer sample app.I wanted to see how the GBuffer pipeline state works,so I selected this draw call.If I look in bound resources,I can now see the pipeline state which was used.Opening it takes me to the Pipeline State Viewer.From here, I can examine the functionsand see other properties the pipeline state was created with.Further, from the viewer,I can either check out the performance dataassociated with the state,or I can go to Memory Viewerand reveal the state there.In Xcode 13,Memory Viewer now showshow much memory the pipeline statesare taking up in your app.These are just some of the additionsthat make it easier to inspect pipeline statesacross the Metal Debuggerwhen you’re looking at GPU traces of your app.Next, let’s talk about shader debugging and profilingin Metal Debugger.Right now, if you wanna use these features,you have two choices.First option is compiling your librariesfrom source code when the app is running.A second, better optionis building Metallib files with sources embedded offline,and then loading those at runtime.But then, App Store rulesdon’t allow you to publish your appswith these debug Metallibs.All of that meansthat if you compile your libraries offlineand you want to be able to debug your shaders,you have to compile them twice:once with sources embedded, for use during development,and once without sources, for distribution.This year, we are changing that.You can now generate a separate file with sourcesand other debugging information while compiling a Metallib.These files have a Metallibsym extension,and they allow you to debug and profile shaderswithout embedding additional informationin the libraries themselves.The most important benefit of having them separately,is that now you don’t need to have two versionsof the same Metallib.Another benefit is that with these Metallibsym files,you will now be able to debug shaderseven in release versions of your app,without having to compromise your shader sources.I’ll show you an exampleof how to compile a shader source fileinto a Metallib with Metallibsym file alongside it.I’ll start with xcrun terminal commandthat compiles a Metallib as normal.To generate a Metallibsym file,I simply need to add the flag“record-sources” with the “flat” option,and then run the compiler.Now, when I try to debug a shaderthat was compiledwith a separate debug information file,I’ll be prompted to import it.Clicking on Import Sources opens up a dialogthat lists all the librariesand whether they have their source files imported.From here, I can import any Metallibsym files,and once imported, the libraries and their sourceswill be matched automatically.When I’m done importing,I can close the dialog,and now I can see the sources for the shader and debug it.There is one last debugging improvement I wanna show you.It’s called “selective shader debugging.”If your app uses large shaders,you might have noticedthat the shader debugging may take a while to start.To help in such cases,this year, we are bringing selective shader debugging.It helps you narrow down the debugging scopes,so you can debug your shaders quicker.Let’s see it in action with one of such large shaders.I would like to debug this GPU ASTCDecoder.I know that if I tried to debug this whole kernel,Shader Debugger would take a long time to start.I wouldn’t want to wait that long,so instead,I can narrow down the debugging scopeto just this function, decodeIntegerSequence.To do so, I can right-click itand select Debug Functions.This opens “functions to debug” menu,with the function scope already selected.Now, the debugger will start almost instantly.Selective shader debuggingis a great way to pinpoint bugsin huge shaders quickly.These are all the Metal tools improvementsI wanted to show you today.And now, Amanda will tell you about advances we’ve madein texture compression.Amanda?Thanks, Egor.I’m going to walk you through the updates we’ve made this yearto texture compression tools.Before I dive into the tools,I’m going to briefly discussthe basics of texture compression on Apple platforms.Texture compression, in this case,is fixed-rate, lossy compression of texture data.This is primarily intended for offline compressionof static texture data,such as decals or normal maps.While you can compress dynamic texture data at runtime,that’s not something I’m covering today.Most texture compressionworks by splitting a texture into blocksand compressing each block as a pair of colors.This pair defines a localized palette,including other colorsinterpolated from these endpoints,and a per-pixel index that selects from this palette.Each format has different strengthsthat suit different kinds of texture data.Apple GPUs also supportlossless frame-buffer compressionstarting in our A12 devices,and is great for optimizing bandwidth.Check out last year’s session“Optimize Metal apps and games with GPU counters”to learn more about measuring the memory bandwidththe GPU is using for your app.Another option is to perform lossless compressionof texture fileson top of the GPU texture compressionI’m covering in this presentation.This can give you additional reductionsin the size of your app download.Now that I’ve defined texture compressionfor this talk,I’ll talk about the benefitstexture compression can bring to your app.Texture compression is an important stepin the development of your apps.In general,most of the memory footprint of gamesconsists of textures.Using texture compression allows you to load more texturesinto memory,and use more detailed texturesto create more visually compelling games.Compression may also allow youto reduce the size and memory footprint of your app.Now that I’ve covered the basics,I’ll discuss the current state of texture compression toolson Apple platforms.The existing TextureTool in the iOS SDKhas a relatively simple pipeline.TextureTool reads the input image,generates mipmaps if desired,compresses the texture, block by block,then writes the results to a new output file.But as graphics algorithms increase in complexity,textures need more advanced processing.The core of these processes is performing operationsin the correct color space,while minimizing rounding from transformationsbetween numeric precisions.Understanding this,we’ve designed a new compression toolcalled TextureConverterto handle the necessary increasein texture processing sophistication,and give you access to a host of new options.Let’s take a closer lookat how we’ve revampedthe texture processing pipelineon Apple platforms.The texture processing pipeline has been rebuiltfrom the ground up to give you accessto a fully-featured texture processing pipelinewith TextureConverter.TextureConverter leveragesa set of industry-recognized compressorsto support a wide range of compression formats,and give you the option to tradeoffbetween compression speed and image quality.You can specify which compressor to use,or allow TextureConverter to selectbased on the compression format, quality level,and other options.Each stage is now fully configurable by you,and texture processing is gamma-aware.To support integration into all your content pipelines,TextureConverter is available for both macOS and Windows,and is optimized for use with Apple Silicon.Let’s step through each stage of the expanded pipeline,starting with gamma.Gamma correction is a nonlinear operationto encode and decode luminance in images.Textures can be encoded in many gamma spaces.The best choice is dependent on the type of datathat the texture represents.Most visual data, such as decals or light maps,do best when encoded in a non-linear space,like sRGB.Non-visual data, like normal maps,should be encoded in linear space.This choice gives you more accuracyin the dark areas where it’s needed.Non-visual data, like normal maps,should be encoded in linear space.Compression should be performed in your target color space,specified with the “gamma_in” and “gamma_out” options.You can either input a float valuefor linear gamma space,or use the string “sRGB” to specify that color space.You also have the flexibility to use these optionsto convert to a different target space.Other operations, such as mipmap generation,should be performed in linear space.I’ll walk through the linear space processing stages now.Now that the input has been converted to linear gamma space,the linear space operations are performedbefore the input texture is convertedto the specified target gamma space.The three stages are physical transforms,mipmap generation,and alpha handling,and some of these have substages.I’ll start with physical transforms.By defining the maximum size in any axis,you can downscale your image as you needfor your top-level mipmap.In this stage, you also have controlover the resize filter and resize rounding mode.The resize filter options use different algorithmsto help you reduce blurriness of your mipmapsas they go down in dimension size.Resize round modeis used in conjunction with max_extentwhen resizing your image.If max_extent is exceeded,the source image is resizedby maintaining the original image’s aspect.The specified round modewill be used when finding the target dimensions.If you’re unsure which resize filter or rounding mode to use,we’ve picked defaults that work well in most cases.And the flip options in this stagegive you control over linear transformationson the X, Y, and Z axes.After transforms is mipmap generation,used in the majority of common texture processing situations.Mipmaps are a precalculated sequences of imagesthat reduce in resolution over the sequence,used to increase rendering speed and reduce aliasing.The height and width of each levelis a power of two smaller than the previous level.When customizing mipmap generation,specify the maximum number you want,and which mip filter to use.TextureConverter defaults to Kaiser filtering,with options for “box” and “triangle” filtering.The last stage in linear space processing is alpha handling.If alpha to coverage is enabled,this is applied first,using the specified alpha reference value.Alpha to coveragereplaces alpha blending with a coverage mask.When antialiasingor semitransparent textures are usedthis gives you order-independent transparency,and is a particularly useful toolfor rendering dense greenery in your game.Afterwards, the option to discard,preserve, or premultiply the alpha channel is presented.In premultiplied alpha,partly transparent pixels of your imagewill be premultiplied with a matting color.At the end of the linear space processing stages,we’re ready to move back to the target gamma spaceand compress the processed mip levels.The last step in texture processing is the compression.The compression stage can be dividedinto two substages, channel mapping and encoding.Channel mapping is a techniqueto optimize general purpose texture compression algorithmsfor particular data types.Specifying a channel mapping in TextureConverter is optional.If you do want to use it,TextureConverter currently supportstwo modes of channel mapping,RGBM encoding and normal map encoding.I’m going to cover both of these formats in more depth,starting with RGBM encoding.RGBM encoding is a technique to compress HDR datain LDR formatsby storing a multiplier in the alpha channeland scaling the RGB channels by this multiplier.Here’s an example HDR image of a classroom.And here’s the same classroom image againwith the multiplier stored in the alpha channelvisible in grayscale.I’ll show you how to calculate the multiplierto encode to RGBM with a code example.EncodeRGBM is a simplified pseudocode functionthat I’ll walk you through to help you understandthe mechanics of encoding to RGBM.This snippet includes use of RGBM_Range,the brand-new parameter for setting the rangeof RGBM and defaults to 6.0.In order to calculate the RGBM alpha value,the multiplier,first, I’ll determine the maximumof the input texture’s red, green, and blue channels.This is done with Metal’s max3 function.Then this maximum is divided by RGBM_Range.In order to calculate the encoded RGBM’sred, green, and blue channel values,first, the previously calculated multiplieris multiplied back by RGBM_Range,which was used to scale the valuefor storage in the alpha channel.Then, the input texture is dividedby the final multiplier value.To decode RGBM in your shader,you multiply the sample’s RGBby alpha and the fixed factor,as I showed you in the encoding function.I’ll walk through the DecodeRGBM code snippetto show you how to do this.The scaling factor is recalculatedby multiplying the RGBM alpha channel,where the multiplier is stored, by RGBM_Range.The original texture’s RGBis calculated by multiplying the RGBM sampleby the calculated multiplier.Now that I’ve introduced you to RGBM encoding,I’ll move on to normal map encoding.In most cases, when referring to normal maps,we’re specifically referring to object-space normal maps.When encoding our normals in object-space,we know that each normal is a unit vector,which has the benefit that it can be representedin two axes with the third axis trivially derivable at runtime.This allows us to remap these two channelsto best take advantageof texture compression algorithms,and achieve superior compression qualitycompared to compressing XYZ as RGB.How you remap channels varies dependingon the compression format.I’ll walk through an example of encoding a normal with ASTC,using this chart as a guide.When encoding with ASTC,the red, green, and blue channelsare set to the X component,and the alpha channel is set to the Y component.The colors correspond to which channelthe X and Y components will be reassigned back towhen sampling the encoded normal.TextureConverter takes care of encoding remapping for youby automatically remapping to your chosen formatif you pass the normal map parameter.When sampling normal maps in your shader,it’s important to know the channel mapping.While the X component is read from the red or alpha channel,the Y component comes from the alpha or green channeldepending on the compression format.Coming back to the ASTC example,to sample a texture, the X componentis sampled from the red channel,and the Y component is sampled from the alpha channel,the reverse of how the normal was encoded.If you’re encoding to multiple formatsto achieve the best possible quality on any device,then this mapping is somethingthat you’ll need to handle at runtime.I’ll walk through an example of runtime normal samplingusing Metal texture swizzles.Encoding to multiple formatscould lead to needing multiple shader variantsif the different formats used different channel mappings.To avoid this,Metal allows you to apply custom swizzles to your texture.Swizzles allow you to remap X and Y componentsto red and green channelsso your shaders can be compression format neutral.Here’s an example of remapping channelsto red and greenfor a normal map compressed with ASTC,as we saw in the diagram previously.After the texture descriptor is initialized,the red channel is set to MTLTextureSwizzleRed,and the green channel is set to MTLTextureSwizzleAlpha.Since this is a normal map,only two channels are needed for sampling.Since the red and green channels are now assignedto the X and Y componentsoriginally encoded to the red and alpha channels,the blue and alpha channels are set to zero.Once that’s done,the last line is to assemble the final swizzlewith the remapped channelsusing MTLTextureSwizzleChannelsMake.Once the X and Y channelsare sampled in your shader,you can reconstruct the Z component.I’ll walk you through the ReconstructNormal functionto show you how.First, the code rebiases the X and Y componentsinto the correct range,which is negative one to one for a normal.The next step is to subtract the dot productof the X and Y components from one,to ensure the result of the dot product has the right sign.The saturate functionis then used to clamp this resultwithin the range of zero to one.The last step to calculate the Z componentis to take the square root of the outputof the saturate function.Now that I’ve explainedRGBM and normal map encoding optionsavailable for channel mapping,I’ll finish the discussionof the texture compression pipelinewith the final compression substate,encoding.All TextureConverter command linesrequire specification of the target compression formatwith the compression_format argument.You can also specify which compressor to useor let TextureConverter make the selectionbased on the compression formatand other options you’ve selected.You may also select the compression qualityfrom these four options.Note that there’s a tradeoff between compression speedand image quality,and you may wish to select a lower compression qualitywhile iterating on your game,but use the highest quality for released builds.Now, I’ll cover the texture compression formatsavailable for you to select.Here’s an overviewof the texture compression format familiessupported on Apple platforms.iOS and Apple Silicon platformssupport ASTC and PVRTC families,and all macOS platforms support BCn families.I’ll go over each of these format families in more detail,and give you some guidelines to help you choosethe best ones for your needs.I’ll start with BCn formats.BCn is a set of seven formatsthat all operate using four-by-four blocks of pixels,and either use four or eight bits per pixel.Each compression format is ideal for a different data format.BC1 and BC3 are commonly used for RGB and RGBA compression,BC6 is ideal for HDR images,and BC5, with its dual independent channels,is ideal for normal map encoding.Next is ASTC,a family of RGBA formats in LDR,sRGB, and HDR color spaces.The ASTC family of formats allows for the highest qualityat all sizes,and is therefore more generally recommended over PVRTC.There’s a range of bits per pixelversus quality for each format.With ASTC, the byte size of each blockis the same regardless of the format,while the number of texels it represents varies.This gives you a continuumbetween the highest quality compressionbut lowest compression rateat the four-by-four block sizeversus the lowest compression qualitybut highest compression rate at the 12-by-12 block size.The LDR, sRGB, and HDR variantsdescribe the color range for compressed ASTC textures.LDR and sRGB are both in the zero-to-one range,in either linear or sRGB space,while the HDR variantis for data outside of the zero-to-one range.Lastly, PVRTC formats are availablein RGB and RGBA in 2-bit or 4-bit mode.A data block in this format always occupies eight bytes,so in 2-bit mode there’ll be one blockfor every eight-by-four pixels,and in 4-bit mode,there’s one block for each four-by-four pixel.Now that I’ve introduced the supported format families,I’m going to give some recommendationsfor choosing formats for your app.On iOS devices, you should always be usingASTC compression by default,with the addition of PVRTC compressionand per-device thinningonly if you’re supporting A7 GPUs and earlier.If you have any HDR textures,you can take advantage of ASTC HDR on A13 and later GPUs.For macOS, BCn is available across the board.On Apple Silicon Macs,you also have the option of using ASTC,and you should consider this optionif you’re also targeting iOS devices.While PVRTC is available on Apple Silicon,we don’t recommend this option,and it’s intended only for iOS legacy support.Since there are a lot of different formatswithin each compression format family to choose between,the guideline for selecting the most effectivetexture compression formats for your appis to select per-texture and per-target when possible.Unless all of your textures are RGB or RGBA data,you should select the compression formatbased on the type of data you’re compressing,like choosing a format that allows compressionas two independent channels for normal data.When compressing to an ASTC format,you may want to select a subset of the formats.Consider bucketing texturesby those that require the highest qualityversus those that are acceptable at higher compression rates.Now, let’s review what we’ve covered.We’ve completely remadethe texture processing pipeline from TextureToolto give you complete control over every stage of the pipelinewith our new TextureConverter tool.I’ve walked through each stage of this new pipelineand explored all of the options available for you to useat each stage,and introduced you to the channel mappingand texture compression format familiessupported on Apple platforms.We want to make it as easy as possibleto update your workflows from using TextureToolto TextureConverter,so we’ve added a compatibility modeto help you switch over your command lines.Whether using TextureTool compatibility modeor calling TextureConverter with native options,invoke with xcrun TextureConverter.Here’s an example command line of TextureConverterbeing called with TextureTool options.TextureConverter will translate the optionsto native TextureConverter options,do the compression,and then tell you what the new native options are,so that you can update your build scripts easily.That was an introduction to TextureConverter.Here’s how to get it.TextureConverter ships as a part of Xcode 13and is available to use in seed 1.On Windows, TextureConverter shipsas a part of the Metal Developer Tools for Windows 2.0 package,available from developer.apple.com.Seed 1 is available now.Be aware that in Windows,there’s no support for compressing to PVRTC formats,as PVRTC is available in macOSfor supporting legacy iOS platforms.Another important partof the Metal Developer Tools for Windowsis the Metal Compiler for Windows.The Metal Compiler for Windows was introduced last year,with support for Metal Shading Language version 2.3.Updates throughout the yearmirrored the updates to the Metal compilershipped in Xcode.The latest release version is 1.2,which includes support for Metal Shading Languageon Apple Silicon Macs.Seed 1 of version 2.0 is now availablewith support for all of the great new featuresin Metal Shading Language 2.4.Here’s a summary of everything we’ve covered today:Egor discussed support for more Metal features,like ray tracing and function pointers.He introduced brand-new profiling workflows,like GPU Timeline and consistent GPU performance state,to help you get the most out of the GPUsacross all Apple platforms.And he demonstrated improvements to debugging workflowsyou’re already familiar with to give you more supportfor shader validation and precise capture controls.And I introduced you to TextureConverter,a new tool to help you take full advantageof the texture processing pipelineand all of the supported texture compression formatsavailable on Apple platforms.Thanks, and have a great rest of WWDC 2021.[music]

Hi! My name is Egor,and today, I’d like to tell you about all the improvementsand new features in Metal Debugger.This year, we are bringing supportfor more Metal features,such as ray tracing and function pointers.

We have added brand-new profiling workflows,like the GPU timelineand consistent GPU performance state,to help you get the most out of GPUs across Apple platforms.

We’ve made improvements to other debugging workflows you know and love,including broader support for shader validation,and precise capture controls.

We are also introducing advances in texture compression,which my colleague Amanda will talk about later.First, let’s talk about ray tracing.

Last year, we introduced a new Metal ray tracing API,and now, in Xcode 13,we support it in Metal Debugger,along with function pointers and function tables,which bring flexibility to your shaders.And dynamic libraries,which give you a way to build well-abstracted and reusableshader library code.Also, for ray tracing, we are introducinga brand-new tool,the Acceleration Structure Viewer.Take a look at ray tracing in the Metal Debugger.

I’ve opened a GPU traceof the ModernRenderer sample app.It was modified to use the Metal ray tracingto achieve effects such as shadows and ambient occlusion.This encoder creates a beautiful ray traced shadow map.I’ve selected a dispatch callso that you can see the acceleration structurein bound resources.From here,I will open the acceleration structureto go to our new acceleration structure viewer.Here, you can see the geometry of the familiar bistro sceneon the right, and its outline on the left.Clicking on an instance in the scenewill select it in the viewer and also in the scene outline.You can see the transformation matrixand other instance properties by expanding it.You can also select an individual geometryby holding the Option keywhile clicking in the scene viewer.This will also select it in the scene outline,and vice versa.

You can also see the relevant intersection functionsused with the acceleration structureright here in the viewer.But the Acceleration Structure Viewercan do much more than simply display geometry.Here on the bottom-right,you will also find a number of highlighting modesthat will help you visualize some of the propertiesof your scene.For example,the bounding volume traversals modecan help you visualize the complexity of your geometry.A deeper blue color shows areaswhere the bounding volume hierarchyis more computationally expensive to traverserelative to other parts of it.For all of the modes, we have this small viewthat shows the relevant informationwhen you hover over different parts of your scene.Here, it displays the number of bounding box traversalsand primitive intersections.

To give you more flexibility,we also included traversal settings.With them, you can configurethe acceleration structure viewer,using same properties you can findon an intersector object inside your shaders.

There’s so much more to talk aboutwhen it comes to ray tracing.

If you’d like to learn more, check this year’s session“Explore hybrid rendering with Metal ray tracing.”And if you want to know more about the API in general,check out last year’s talk,“Discover ray tracing with Metal.”Next, let’s talk about profiling.

Profiling your app is an important step,and we already have a lot of great tools at your disposal.For example, using the Metal systemtrace in instruments,you can explore a timeline viewthat shows CPU and GPU durationsfor different rendering stages,GPU counters, and shader timelines.And in the Metal Debugger,GPU counters show a rich set of measurementsdirectly from the GPU,either per encoder or per draw.Both are excellent tools,which provide complementary views of your app’s performance.But aligning those views may take additional effort.So, that’s why I’m excited to show youa new GPU profiling toolthat combines Metal system traceand GPU counters in a unified experience.

Introducing GPU Timeline in Metal Debugger,a new tool designed specifically for Apple GPUs.It gives you a different perspectiveon the performance data,and it can help you find potential points of optimizationin your app.Let’s walk through this latest additionto our suite of profiling tools.

The GPU Timeline is availableunder the Performance panel.You can find it in the debug navigatorafter you’ve captured a frame from your app.When you open the Performance panel,you’ll be greeted by a set of different trackslaid out in parallel.Before we continue, I want to explainwhy encoder tracks are in parallel.On Apple GPUs,vertex and fragment stages of different render passes,and also compute dispatches, can run simultaneously.This is enabled by Apple GPU architectureand use of rendering techniquecalled “tile-based deferred rendering.”We thought it is important for you to be able to see thisparallel nature of Apple GPUs in the context of your app.And that’s where GPU Timeline comes in handy.

At the top, you can seethe Vertex, Fragment, and Compute encoder timelines,with each encoder showing the resources it usesat a quick glance.Below the encoders,you will find Occupancy, Bandwidth, and limiter counters.Let’s take a closer look at the encoder timeline.

You can expand each encoder trackto see an aggregated shader timeline.

Expanding the timeline even furtherwill show you each individual shaderin a waterfall-like fashion.

It’s easy to navigate the encoders.Select an encoder trackto see a list of all the encoders on the right.There, you can sort them by their average duration.Clicking on an individual encoder in the timelinewill show you more information about it in the sidebar.For example, here you can seethe attachments for this render command encoder.

You may have noticed that when you select an encoder,time ranges where it’s activebecome highlighted across all the tracks.With this, you can easily examinehow different stages overlap,and also correlate counter values for the encoder.

Moving away from the timeline view,you can access the GPU countersby switching to the Counters tab,or you can just open encoder’s context menuand reveal it in Counters from there.

And this is just a sneak peek at the GPU Timeline.To learn more about using Metal Debuggerto understand your app’s performance,check out this year’s session“Optimize high-end games for Apple GPUs.”Now that I’ve showed you a new way to profile your app,it’s important to understandthat its performance depends on several factors.

When we are talking about Metal,the GPU performance state is a very important factor.It’s managed by the operating system,which will lower or raise the statedepending on device thermals,system settings, GPU utilization,and other parameters.

These state changes can affectprofiling results you are seeing.

This year, we are introducing new waysfor you to profile your app with more consistent results.We have added ways for you to see and changethe GPU performance stateacross our whole suite of Metal tools,starting with Instruments and Metal system tracefor live performance recordings,continuing with Metal Debugger,for profiling GPU traces,and finally, Device conditions in Xcode,for general use cases.First, let’s talk about Instruments.This year, we’ve added a track for GPU performance stateto the Metal system trace.Use it in conjunction with the other tracksto correlate your app’s performancewith the device’s performance state.Keep in mind, though,that being able to see the performance stateis just part of the equation.In order to get profiling resultsthat are consistent and reproducible,you also need a way to seta GPU performance state on a device.

New this year is the abilityto induce a specific GPU performance statewhen you are recording a trace in Instruments.Simply go to Recording Options,and choose a performance state before the recording starts.After that, you can record the performance trace as usual.Instruments will induce the state you chosefor the duration of the trace, if the device can sustain it.Sometimes, you might need to checkif an existing Instruments trace had a GPU performance stateinduced during recording.You can find this informationin the “Recording Settings” sectionin the information popover.

And now, you know how to viewand induce the GPU performance state from Instruments.The second way to leveragea consistent GPU performance stateis by using Metal Debugger.By default, when you capture a GPU trace of your app,Xcode will profile the trace for you.And it will do so using the same performance statethe device was in at the time of the capture.That state may have fluctuated,depending on the factors we mentioned previously.If, instead, you would like to selecta certain performance state yourself,use the Stopwatch button in the debug bar.After you make a selection,Metal Debugger will profile your GPU trace again.After it’s done, the button is highlightedto reflect that the consistent performance state was achieved.Also, “Performance” section on the summary pagenow shows new performance data at a glance,as well as the selected performance state.These two approaches are tied to the suite of Metal tools.But sometimes, you might want to inducea consistent performance stateoutside of profiling workflows.The third way to set a GPU performance stateis through Device conditions.

If you want to test how your app performsunder different GPU performance states,this is the option for you.In Xcode 13, we have addedthe GPU performance state device condition.It forces the operating systemto use the specified state on a device,as long as it can sustain itand it stays connected to the Xcode.

You can add this condition from Xcode,if you go to Window, Devices and Simulators,choose your device there,then scroll to the “Device Conditions” section,and add a “GPU Performance State”condition with the desired level.Press Start when you want to applythe GPU performance state change on the device.Then, when you are done, press Stop.

These new ways to see and change GPU performance stateright from our toolsshould help you with profiling and testing your apps.And I think you are going to loveour latest additions and improvementsto profiling workflows,and I hope they will help you make your apps even better.Now, let’s talk about some other improvementswe are bringing to Metal Debugger this year.First, I’ll tell you aboutimprovements to shader validation.Then, I’ll show you precise capture controls.And after that, I’ll give you a lookat the new pipeline state workflows.Finally, I want to introduce two new featuresrelated to shader debugging and profiling,separate debug information and selective shader debugging.

Last year, in Xcode 12,we introduced shader validation,which helps you diagnose runtime errors on the GPU,like out of bounds access, and others.

Remember that if shader validation is enabled,and an encoder raises a validation error,you will get a runtime issue in the issue navigator,showing both the CPU and GPU backtracesfor the call that faulted.

We already have a session that covers this in a greater detail,so to learn more about using shader validation,check out last year’s talk, called“Debugging GPU-side errors in Metal.”This year, we are extending shader validationto support more use cases,making it availablewhen you are using indirect command buffers,dynamic libraries,and function pointers and tables.This should allow you to use shader validationmore extensively throughout your appduring its development.Next, I wanna show youour new precise capture controls.But first, take a look at the Capture button,which now looks like the Metal logo.It’s located on the debug bar,at the bottom of the Xcode window.When you click it, a new menu appears.This menu lets you choose a scope for your capture.The default is to capture one frame,but you can specify how many you want to capture,up to five.You can also choose to capture a number of command buffersthat have the same parent device,or a command queue,as well as those that present a certain Metal layer,and even custom scopes that you can definein your app’s code using MTLCaptureScope API.These new controls give you incredible powerout-of-the-box in deciding how and whenyour Metal calls are captured.

Next, let’s talk about Metal libraries and pipeline states.These are the essential building blocks of your Metal app.And in Xcode 13,we’ve made it easier than everto examine all the pipeline states and librariesyour app is using.Now, let’s see how it looks in practice.

Here, I’ve captured a GPU tracefrom a ModernRenderer sample app.I wanted to see how the GBuffer pipeline state works,so I selected this draw call.If I look in bound resources,I can now see the pipeline state which was used.Opening it takes me to the Pipeline State Viewer.From here, I can examine the functionsand see other properties the pipeline state was created with.Further, from the viewer,I can either check out the performance dataassociated with the state,or I can go to Memory Viewerand reveal the state there.In Xcode 13,Memory Viewer now showshow much memory the pipeline statesare taking up in your app.These are just some of the additionsthat make it easier to inspect pipeline statesacross the Metal Debuggerwhen you’re looking at GPU traces of your app.Next, let’s talk about shader debugging and profilingin Metal Debugger.Right now, if you wanna use these features,you have two choices.First option is compiling your librariesfrom source code when the app is running.

A second, better optionis building Metallib files with sources embedded offline,and then loading those at runtime.But then, App Store rulesdon’t allow you to publish your appswith these debug Metallibs.All of that meansthat if you compile your libraries offlineand you want to be able to debug your shaders,you have to compile them twice:once with sources embedded, for use during development,and once without sources, for distribution.This year, we are changing that.You can now generate a separate file with sourcesand other debugging information while compiling a Metallib.These files have a Metallibsym extension,and they allow you to debug and profile shaderswithout embedding additional informationin the libraries themselves.The most important benefit of having them separately,is that now you don’t need to have two versionsof the same Metallib.Another benefit is that with these Metallibsym files,you will now be able to debug shaderseven in release versions of your app,without having to compromise your shader sources.

I’ll show you an exampleof how to compile a shader source fileinto a Metallib with Metallibsym file alongside it.

I’ll start with xcrun terminal commandthat compiles a Metallib as normal.To generate a Metallibsym file,I simply need to add the flag“record-sources” with the “flat” option,and then run the compiler.Now, when I try to debug a shaderthat was compiledwith a separate debug information file,I’ll be prompted to import it.Clicking on Import Sources opens up a dialogthat lists all the librariesand whether they have their source files imported.

From here, I can import any Metallibsym files,and once imported, the libraries and their sourceswill be matched automatically.

When I’m done importing,I can close the dialog,and now I can see the sources for the shader and debug it.

There is one last debugging improvement I wanna show you.It’s called “selective shader debugging.”If your app uses large shaders,you might have noticedthat the shader debugging may take a while to start.To help in such cases,this year, we are bringing selective shader debugging.It helps you narrow down the debugging scopes,so you can debug your shaders quicker.Let’s see it in action with one of such large shaders.

I would like to debug this GPU ASTCDecoder.I know that if I tried to debug this whole kernel,Shader Debugger would take a long time to start.I wouldn’t want to wait that long,so instead,I can narrow down the debugging scopeto just this function, decodeIntegerSequence.To do so, I can right-click itand select Debug Functions.This opens “functions to debug” menu,with the function scope already selected.Now, the debugger will start almost instantly.

Selective shader debuggingis a great way to pinpoint bugsin huge shaders quickly.These are all the Metal tools improvementsI wanted to show you today.And now, Amanda will tell you about advances we’ve madein texture compression.Amanda?Thanks, Egor.I’m going to walk you through the updates we’ve made this yearto texture compression tools.Before I dive into the tools,I’m going to briefly discussthe basics of texture compression on Apple platforms.Texture compression, in this case,is fixed-rate, lossy compression of texture data.This is primarily intended for offline compressionof static texture data,such as decals or normal maps.While you can compress dynamic texture data at runtime,that’s not something I’m covering today.Most texture compressionworks by splitting a texture into blocksand compressing each block as a pair of colors.This pair defines a localized palette,including other colorsinterpolated from these endpoints,and a per-pixel index that selects from this palette.Each format has different strengthsthat suit different kinds of texture data.Apple GPUs also supportlossless frame-buffer compressionstarting in our A12 devices,and is great for optimizing bandwidth.Check out last year’s session“Optimize Metal apps and games with GPU counters”to learn more about measuring the memory bandwidththe GPU is using for your app.Another option is to perform lossless compressionof texture fileson top of the GPU texture compressionI’m covering in this presentation.This can give you additional reductionsin the size of your app download.Now that I’ve defined texture compressionfor this talk,I’ll talk about the benefitstexture compression can bring to your app.Texture compression is an important stepin the development of your apps.In general,most of the memory footprint of gamesconsists of textures.Using texture compression allows you to load more texturesinto memory,and use more detailed texturesto create more visually compelling games.Compression may also allow youto reduce the size and memory footprint of your app.Now that I’ve covered the basics,I’ll discuss the current state of texture compression toolson Apple platforms.The existing TextureTool in the iOS SDKhas a relatively simple pipeline.TextureTool reads the input image,generates mipmaps if desired,compresses the texture, block by block,then writes the results to a new output file.But as graphics algorithms increase in complexity,textures need more advanced processing.The core of these processes is performing operationsin the correct color space,while minimizing rounding from transformationsbetween numeric precisions.Understanding this,we’ve designed a new compression toolcalled TextureConverterto handle the necessary increasein texture processing sophistication,and give you access to a host of new options.Let’s take a closer lookat how we’ve revampedthe texture processing pipelineon Apple platforms.

The texture processing pipeline has been rebuiltfrom the ground up to give you accessto a fully-featured texture processing pipelinewith TextureConverter.TextureConverter leveragesa set of industry-recognized compressorsto support a wide range of compression formats,and give you the option to tradeoffbetween compression speed and image quality.You can specify which compressor to use,or allow TextureConverter to selectbased on the compression format, quality level,and other options.Each stage is now fully configurable by you,and texture processing is gamma-aware.To support integration into all your content pipelines,TextureConverter is available for both macOS and Windows,and is optimized for use with Apple Silicon.

Let’s step through each stage of the expanded pipeline,starting with gamma.Gamma correction is a nonlinear operationto encode and decode luminance in images.Textures can be encoded in many gamma spaces.The best choice is dependent on the type of datathat the texture represents.Most visual data, such as decals or light maps,do best when encoded in a non-linear space,like sRGB.Non-visual data, like normal maps,should be encoded in linear space.This choice gives you more accuracyin the dark areas where it’s needed.Non-visual data, like normal maps,should be encoded in linear space.Compression should be performed in your target color space,specified with the “gamma_in” and “gamma_out” options.You can either input a float valuefor linear gamma space,or use the string “sRGB” to specify that color space.You also have the flexibility to use these optionsto convert to a different target space.Other operations, such as mipmap generation,should be performed in linear space.I’ll walk through the linear space processing stages now.

Now that the input has been converted to linear gamma space,the linear space operations are performedbefore the input texture is convertedto the specified target gamma space.The three stages are physical transforms,mipmap generation,and alpha handling,and some of these have substages.I’ll start with physical transforms.By defining the maximum size in any axis,you can downscale your image as you needfor your top-level mipmap.In this stage, you also have controlover the resize filter and resize rounding mode.The resize filter options use different algorithmsto help you reduce blurriness of your mipmapsas they go down in dimension size.Resize round modeis used in conjunction with max_extentwhen resizing your image.If max_extent is exceeded,the source image is resizedby maintaining the original image’s aspect.The specified round modewill be used when finding the target dimensions.If you’re unsure which resize filter or rounding mode to use,we’ve picked defaults that work well in most cases.And the flip options in this stagegive you control over linear transformationson the X, Y, and Z axes.After transforms is mipmap generation,used in the majority of common texture processing situations.Mipmaps are a precalculated sequences of imagesthat reduce in resolution over the sequence,used to increase rendering speed and reduce aliasing.The height and width of each levelis a power of two smaller than the previous level.

When customizing mipmap generation,specify the maximum number you want,and which mip filter to use.TextureConverter defaults to Kaiser filtering,with options for “box” and “triangle” filtering.

The last stage in linear space processing is alpha handling.

If alpha to coverage is enabled,this is applied first,using the specified alpha reference value.Alpha to coveragereplaces alpha blending with a coverage mask.When antialiasingor semitransparent textures are usedthis gives you order-independent transparency,and is a particularly useful toolfor rendering dense greenery in your game.Afterwards, the option to discard,preserve, or premultiply the alpha channel is presented.In premultiplied alpha,partly transparent pixels of your imagewill be premultiplied with a matting color.

At the end of the linear space processing stages,we’re ready to move back to the target gamma spaceand compress the processed mip levels.

The last step in texture processing is the compression.The compression stage can be dividedinto two substages, channel mapping and encoding.Channel mapping is a techniqueto optimize general purpose texture compression algorithmsfor particular data types.

Specifying a channel mapping in TextureConverter is optional.If you do want to use it,TextureConverter currently supportstwo modes of channel mapping,RGBM encoding and normal map encoding.I’m going to cover both of these formats in more depth,starting with RGBM encoding.RGBM encoding is a technique to compress HDR datain LDR formatsby storing a multiplier in the alpha channeland scaling the RGB channels by this multiplier.Here’s an example HDR image of a classroom.And here’s the same classroom image againwith the multiplier stored in the alpha channelvisible in grayscale.I’ll show you how to calculate the multiplierto encode to RGBM with a code example.EncodeRGBM is a simplified pseudocode functionthat I’ll walk you through to help you understandthe mechanics of encoding to RGBM.This snippet includes use of RGBM_Range,the brand-new parameter for setting the rangeof RGBM and defaults to 6.0.In order to calculate the RGBM alpha value,the multiplier,first, I’ll determine the maximumof the input texture’s red, green, and blue channels.This is done with Metal’s max3 function.Then this maximum is divided by RGBM_Range.In order to calculate the encoded RGBM’sred, green, and blue channel values,first, the previously calculated multiplieris multiplied back by RGBM_Range,which was used to scale the valuefor storage in the alpha channel.Then, the input texture is dividedby the final multiplier value.To decode RGBM in your shader,you multiply the sample’s RGBby alpha and the fixed factor,as I showed you in the encoding function.I’ll walk through the DecodeRGBM code snippetto show you how to do this.The scaling factor is recalculatedby multiplying the RGBM alpha channel,where the multiplier is stored, by RGBM_Range.The original texture’s RGBis calculated by multiplying the RGBM sampleby the calculated multiplier.Now that I’ve introduced you to RGBM encoding,I’ll move on to normal map encoding.In most cases, when referring to normal maps,we’re specifically referring to object-space normal maps.When encoding our normals in object-space,we know that each normal is a unit vector,which has the benefit that it can be representedin two axes with the third axis trivially derivable at runtime.This allows us to remap these two channelsto best take advantageof texture compression algorithms,and achieve superior compression qualitycompared to compressing XYZ as RGB.How you remap channels varies dependingon the compression format.I’ll walk through an example of encoding a normal with ASTC,using this chart as a guide.When encoding with ASTC,the red, green, and blue channelsare set to the X component,and the alpha channel is set to the Y component.The colors correspond to which channelthe X and Y components will be reassigned back towhen sampling the encoded normal.TextureConverter takes care of encoding remapping for youby automatically remapping to your chosen formatif you pass the normal map parameter.When sampling normal maps in your shader,it’s important to know the channel mapping.While the X component is read from the red or alpha channel,the Y component comes from the alpha or green channeldepending on the compression format.Coming back to the ASTC example,to sample a texture, the X componentis sampled from the red channel,and the Y component is sampled from the alpha channel,the reverse of how the normal was encoded.If you’re encoding to multiple formatsto achieve the best possible quality on any device,then this mapping is somethingthat you’ll need to handle at runtime.I’ll walk through an example of runtime normal samplingusing Metal texture swizzles.

Encoding to multiple formatscould lead to needing multiple shader variantsif the different formats used different channel mappings.To avoid this,Metal allows you to apply custom swizzles to your texture.Swizzles allow you to remap X and Y componentsto red and green channelsso your shaders can be compression format neutral.Here’s an example of remapping channelsto red and greenfor a normal map compressed with ASTC,as we saw in the diagram previously.After the texture descriptor is initialized,the red channel is set to MTLTextureSwizzleRed,and the green channel is set to MTLTextureSwizzleAlpha.

Since this is a normal map,only two channels are needed for sampling.Since the red and green channels are now assignedto the X and Y componentsoriginally encoded to the red and alpha channels,the blue and alpha channels are set to zero.Once that’s done,the last line is to assemble the final swizzlewith the remapped channelsusing MTLTextureSwizzleChannelsMake.Once the X and Y channelsare sampled in your shader,you can reconstruct the Z component.I’ll walk you through the ReconstructNormal functionto show you how.

First, the code rebiases the X and Y componentsinto the correct range,which is negative one to one for a normal.The next step is to subtract the dot productof the X and Y components from one,to ensure the result of the dot product has the right sign.The saturate functionis then used to clamp this resultwithin the range of zero to one.The last step to calculate the Z componentis to take the square root of the outputof the saturate function.

Now that I’ve explainedRGBM and normal map encoding optionsavailable for channel mapping,I’ll finish the discussionof the texture compression pipelinewith the final compression substate,encoding.All TextureConverter command linesrequire specification of the target compression formatwith the compression_format argument.You can also specify which compressor to useor let TextureConverter make the selectionbased on the compression formatand other options you’ve selected.You may also select the compression qualityfrom these four options.Note that there’s a tradeoff between compression speedand image quality,and you may wish to select a lower compression qualitywhile iterating on your game,but use the highest quality for released builds.Now, I’ll cover the texture compression formatsavailable for you to select.

Here’s an overviewof the texture compression format familiessupported on Apple platforms.iOS and Apple Silicon platformssupport ASTC and PVRTC families,and all macOS platforms support BCn families.I’ll go over each of these format families in more detail,and give you some guidelines to help you choosethe best ones for your needs.I’ll start with BCn formats.BCn is a set of seven formatsthat all operate using four-by-four blocks of pixels,and either use four or eight bits per pixel.Each compression format is ideal for a different data format.BC1 and BC3 are commonly used for RGB and RGBA compression,BC6 is ideal for HDR images,and BC5, with its dual independent channels,is ideal for normal map encoding.Next is ASTC,a family of RGBA formats in LDR,sRGB, and HDR color spaces.The ASTC family of formats allows for the highest qualityat all sizes,and is therefore more generally recommended over PVRTC.There’s a range of bits per pixelversus quality for each format.

With ASTC, the byte size of each blockis the same regardless of the format,while the number of texels it represents varies.This gives you a continuumbetween the highest quality compressionbut lowest compression rateat the four-by-four block sizeversus the lowest compression qualitybut highest compression rate at the 12-by-12 block size.The LDR, sRGB, and HDR variantsdescribe the color range for compressed ASTC textures.LDR and sRGB are both in the zero-to-one range,in either linear or sRGB space,while the HDR variantis for data outside of the zero-to-one range.

Lastly, PVRTC formats are availablein RGB and RGBA in 2-bit or 4-bit mode.A data block in this format always occupies eight bytes,so in 2-bit mode there’ll be one blockfor every eight-by-four pixels,and in 4-bit mode,there’s one block for each four-by-four pixel.

Now that I’ve introduced the supported format families,I’m going to give some recommendationsfor choosing formats for your app.

On iOS devices, you should always be usingASTC compression by default,with the addition of PVRTC compressionand per-device thinningonly if you’re supporting A7 GPUs and earlier.If you have any HDR textures,you can take advantage of ASTC HDR on A13 and later GPUs.For macOS, BCn is available across the board.On Apple Silicon Macs,you also have the option of using ASTC,and you should consider this optionif you’re also targeting iOS devices.While PVRTC is available on Apple Silicon,we don’t recommend this option,and it’s intended only for iOS legacy support.Since there are a lot of different formatswithin each compression format family to choose between,the guideline for selecting the most effectivetexture compression formats for your appis to select per-texture and per-target when possible.Unless all of your textures are RGB or RGBA data,you should select the compression formatbased on the type of data you’re compressing,like choosing a format that allows compressionas two independent channels for normal data.When compressing to an ASTC format,you may want to select a subset of the formats.Consider bucketing texturesby those that require the highest qualityversus those that are acceptable at higher compression rates.

Now, let’s review what we’ve covered.

We’ve completely remadethe texture processing pipeline from TextureToolto give you complete control over every stage of the pipelinewith our new TextureConverter tool.I’ve walked through each stage of this new pipelineand explored all of the options available for you to useat each stage,and introduced you to the channel mappingand texture compression format familiessupported on Apple platforms.We want to make it as easy as possibleto update your workflows from using TextureToolto TextureConverter,so we’ve added a compatibility modeto help you switch over your command lines.Whether using TextureTool compatibility modeor calling TextureConverter with native options,invoke with xcrun TextureConverter.Here’s an example command line of TextureConverterbeing called with TextureTool options.TextureConverter will translate the optionsto native TextureConverter options,do the compression,and then tell you what the new native options are,so that you can update your build scripts easily.

That was an introduction to TextureConverter.Here’s how to get it.TextureConverter ships as a part of Xcode 13and is available to use in seed 1.On Windows, TextureConverter shipsas a part of the Metal Developer Tools for Windows 2.0 package,available from developer.apple.com.Seed 1 is available now.Be aware that in Windows,there’s no support for compressing to PVRTC formats,as PVRTC is available in macOSfor supporting legacy iOS platforms.Another important partof the Metal Developer Tools for Windowsis the Metal Compiler for Windows.The Metal Compiler for Windows was introduced last year,with support for Metal Shading Language version 2.3.Updates throughout the yearmirrored the updates to the Metal compilershipped in Xcode.The latest release version is 1.2,which includes support for Metal Shading Languageon Apple Silicon Macs.Seed 1 of version 2.0 is now availablewith support for all of the great new featuresin Metal Shading Language 2.4.

Here’s a summary of everything we’ve covered today:Egor discussed support for more Metal features,like ray tracing and function pointers.He introduced brand-new profiling workflows,like GPU Timeline and consistent GPU performance state,to help you get the most out of the GPUsacross all Apple platforms.And he demonstrated improvements to debugging workflowsyou’re already familiar with to give you more supportfor shader validation and precise capture controls.And I introduced you to TextureConverter,a new tool to help you take full advantageof the texture processing pipelineand all of the supported texture compression formatsavailable on Apple platforms.Thanks, and have a great rest of WWDC 2021.[music]

27:51 -RGBM Encoding Pseudocode

28:41 -RGBM Decoding Pseudocode

30:55 -MetalTextureSwizzles

31:55 -ReconstructNormal

## Code Samples

```swift
float4 
EncodeRGBM
(float3 
in
)
{ 
    float4 rgbm; 
    rgbm.a 
=
 max3(
in
.r, 
in
.g, 
in
.b) 
/
 
RGBM_Range
;
    rgbm.rgb 
=
 
in
 
/
 (rgbm.a 
*
 
RGBM_Range
);
    
return
 rgbm;
}
```

```swift
float3 
DecodeRGBM
(float4 sample)
{ 
    const float 
RGBM_Range
 
=
 
6
.0f;
    float scale 
=
 sample.a 
*
 
RGBM_Range
;
    
return
 sample.rgb 
*
 scale;
}
```

```swift
// Remap the X and Y channels to red and green channels for normal maps 

compressed with ASTC.
 

MTLTextureDescriptor
* descriptor = [[
MTLTextureDescriptor
 alloc] init];


MTLTextureSwizzle
 r = 
MTLTextureSwizzleRed
;

MTLTextureSwizzle
 g = 
MTLTextureSwizzleAlpha
;

MTLTextureSwizzle
 b = 
MTLTextureSwizzleZero
;

MTLTextureSwizzle
 a = 
MTLTextureSwizzleZero
;

descriptor.swizzle = 
MTLTextureSwizzleChannelsMake
( r, g, b, a );
```

```swift
// Reconstruct z-axis from normal sample in shader code.


float3 ReconstructNormal(float2 sample)
{
    float3 normal;

    normal.xy = sample.xy * 
2.0
f - 
1.0
f;
    normal.z  = sqrt( saturate( 
1.0
f - dot( normal.xy, normal.xy ) ) );

    
return
 normal;
}
```

