# Wwdc2021 10254

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Swift concurrency: Behind the scenesDive into the details of Swift concurrency and discover how Swift provides greater safety from data races and thread explosion while simultaneously improving performance. We'll explore how Swift tasks differ from Grand Central Dispatch, how the new cooperative threading model works, and how to ensure the best performance for your apps.

To get the most out of this session, we recommend first watching “Meet async/await in Swift,” “Explore structured concurrency in Swift,” and “Protect mutable state with Swift actors.”ResourcesThe Swift Programming Language: ConcurrencyHD VideoSD VideoRelated VideosWWDC23Analyze hangs with InstrumentsBeyond the basics of structured concurrencyWhat’s new in SwiftWWDC22Eliminate data races using Swift ConcurrencyVisualize and optimize Swift concurrencyWWDC21Explore structured concurrency in SwiftMeet async/await in SwiftProtect mutable state with Swift actorsSwift concurrency: Update a sample appWhat‘s new in SwiftWWDC17Modernizing Grand Central Dispatch UsageWWDC16Concurrent Programming With GCD in Swift 3

Dive into the details of Swift concurrency and discover how Swift provides greater safety from data races and thread explosion while simultaneously improving performance. We'll explore how Swift tasks differ from Grand Central Dispatch, how the new cooperative threading model works, and how to ensure the best performance for your apps.

To get the most out of this session, we recommend first watching “Meet async/await in Swift,” “Explore structured concurrency in Swift,” and “Protect mutable state with Swift actors.”

The Swift Programming Language: Concurrency

HD VideoSD Video

HD Video

SD Video

Analyze hangs with Instruments

Beyond the basics of structured concurrency

What’s new in Swift

Eliminate data races using Swift Concurrency

Visualize and optimize Swift concurrency

Explore structured concurrency in Swift

Meet async/await in Swift

Protect mutable state with Swift actors

Swift concurrency: Update a sample app

What‘s new in Swift

Modernizing Grand Central Dispatch Usage

Concurrent Programming With GCD in Swift 3

Search this video…♪ Bass music playing ♪♪Rokhini Prabhu: Hello, and welcometo "Swift Concurrency Behind the Scenes."My name is Rokhini, and I work on the Darwin Runtime team.Today, my colleague Varun and I are very excited to talk to youabout some of the underlying nuancesaround Swift concurrency.This is an advanced talk which builds uponsome of the earlier talks on Swift concurrency.If you are unfamiliar with the concepts of async/await,structured concurrency, and actorsI encourage you to watch these others talks first.In the previous talks on Swift concurrency you've learned aboutthe various language features available this yearnative to Swift and about how to use them.In this talk, we will be diving deeper to understandwhy these primitives are designed the way they are,not only for language safetybut also for performance and efficiency.As you experiment and adopt Swift concurrencyin your own apps, we hope this talk will give youa better mental model of how to reason about Swift concurrencyas well how it interfaces with existing threading librarieslike Grand Central Dispatch.We're going to discuss a few things today.First, we'll talk about the threading modelbehind Swift concurrencyand contrast it with Grand Central Dispatch.We'll talk about how we've taken advantageof concurrency language featuresto build a new thread pool for Swift,thereby achieving better performance and efficiency.Lastly, in this section,we'll touch on considerations you need to keep in mindwhen porting your code to using Swift concurrency.Varun will then talk about synchronizationin Swift concurrency via actors.We'll talk how actors work under the hood,how they compare to existing synchronization primitivesyou already may be familiar with --like serial dispatch queues --and finally, some things to keep in mindwhen writing code with actors.We have a lot of ground to cover today so let's dive right in.In our discussion today about threading models,we'll start by taking a look at an example appwritten with the technologies available todaylike Grand Central Dispatch.We will then see how the same application fareswhen rewritten with Swift concurrency.Suppose I wanted to write my own news feed reader app.Let's talk about what the high-level componentsof my application might be.My app will have a main threadthat will be driving the user interface.I will have a database keeping track of the news feedsthat the user is subscribed to,and finally, a subsystem to handle the networking logicto fetch the latest contents from the feeds.Let's consider how one might structure this appwith Grand Central Dispatch queues.Let's suppose that the user has asked to see the latest news.On the main thread, we will handle the user event gesture.From here, we will dispatch async the requestonto a serial queue that handles the database operations.The reason for this is twofold.Firstly, by dispatching the work onto a different queue,we ensure that the main threadcan remain responsive to user input even while waitingfor a potentially large amount of work to happen.Secondly, access to the database is protected,since a serial queue guarantees mutual exclusion.While on the database queue,we'll iterate through the news feedsthe user has subscribed to and for each of them,schedule a networking request to our URLSessionto download the contents of that feed.As the results of the networking requests come in,the URLSession callback will be called on our delegate queuewhich is a concurrent queue.In the completion handler for each of the results,we will synchronously update the databasewith the latest requests from each of these feeds,so as to cache it for future use.And finally, we'll wake up the main threadto refresh the UI.This seems like a perfectly reasonable wayto structure such an application.We've made sure not to block the main threadwhile working on requests.And by handling the network requests concurrently,we've taken advantage of the inherent parallelismin our program.Let's take a closer look at a code snippetthat shows how we process the resultsof our networking requests.First, we have created a URLSessionfor performing downloads from our news feeds.As you can see here, we've set the delegate queueof this URLSession to be a concurrent queue.We then iterate over all the news feedsthat need to be updated and for each of them,schedule a data task in the URLSession.In the completion handler of the data task --which will be invoked on the delegate queue --we deserialize the results of our downloadand format them into articles.We then dispatch sync against our database queuebefore updating the results for the feed.So here you can see that we've writtensome straight-line code to do something fairly straightforwardbut this code has some hidden performance pitfalls.To understand more about these performance problems,we need to first dig into how threads are brought upto handle work on GCD queues.In Grand Central Dispatch,when work is enqueued onto a queue,the system will bring up a threadto service that work item.Since a concurrent queuecan handle multiple work items at once,the system will bring up several threadsuntil we have saturated all the CPU cores.However, if a thread blocks --as seen on the first CPU core here --and there is more work to be done on the concurrent queue,GCD will bring up more threadsto drain the remaining work items.The reason for this is twofold.Firstly, by giving your process another thread,we are able to ensure that each corecontinues to have a thread that executes workat any given time.This gives your applicationsa good, continuing level of concurrency.Secondly the blocked thread may be waiting on a resourcelike a semaphore, before it can make further progress.The new thread that is brought upto continue working on the queuemay be able to help unblock the resourcethat is being waited on by the first thread.So now that we understand a bit moreabout thread bringups in GCD, let's go backand look at the CPU execution of the codefrom our news app.On a two-core device like the Apple Watch,GCD will first bring up two threadsto process the feed update results.As the threads block on accessing the database queue,more threads are created to continue workingon the networking queue.The CPU then has to context switch between different threadsprocessing the networking resultsas indicated by the white vertical linesbetween the various threads.This means that in our news application,we could easily end up with a very large number of threads.If the user has a hundred feeds that need to be updated,then each of those URL data taskswill have a completion block on the concurrent queuewhen the network requests complete.GCD will bring up more threads when each of the callbacksblock on the database queue,resulting in the application having lots of threads.Now you might ask,what's so bad about having a lot of threads in our application?Having a lot of threads in our applicationsmeans that the system is overcommittedwith more threads than we have CPU cores.Consider an iPhone with six CPU cores.If our news application has a hundred feed updatesthat need to be processed,this means that we have overcommitted the iPhonewith 16 times more threads than cores.This is the phenomenon we call thread explosion.Some of our previous WWDC talks have gone into further detailon the risks associated with this,including the possibility of deadlocks in your application.Thread explosion also comes with memoryand scheduling overhead that may not be immediately obvious,so let's look into this further.Looking back at our news application,each of the blocked threads is holding onto valuable memoryand resources while waiting to run again.Each blocked thread has a stackand associated kernel data structures to track the thread.Some of these threads may be holding onto lockswhich other threads that are running might need.This is a large number of resources and memoryto be holding onto for threads which are not making progress.There is also greater scheduling overheadas a result of thread explosion.As new threads are brought up,the CPU need to perform a full thread context switchin order to switch away from the old threadto start executing the new thread.As the blocked threads become runnable again,the scheduler will have to timeshare the threads on the CPUso that they are all able to make forward progress.Now, timesharing of threadsis fine if that happens a few times --that is the power of concurrency.But when there is thread explosion,having to timeshare hundreds of threads on a devicewith limited cores can lead to excessive context switching.The scheduling latencies of these threadsoutweigh the amount of useful work they would do,therefore, resulting in the CPUrunning less efficiently as well.As we've seen so far, it is easy to misssome of these nuances about the threading hygienewhen writing applications with GCD queuesthereby resulting in poor performanceand greater overhead.Building on this experience,Swift has taken a different approachwhen designing concurrency into the language.We've built Swift concurrency with performanceand efficiency in mind as well so that your appscan enjoy controlled, structured,and safe concurrency.With Swift, we want to change the execution model of appsfrom the following model,which has lots of threads and context switches, to this.Here you see that we have just two threadsexecuting on our two-core systemand there are no thread context switches.All of our blocked threads go away and insteadwe have a lightweight object known as a continuationto track resumption of work.When threads execute work under Swift concurrencythey switch between continuationsinstead of performing a full thread context switch.This means that we now only paythe cost of a function call instead.So the runtime behavior that we want for Swift concurrencyis to create only as many threads as there are CPU cores,and for threads to be able to cheaply and efficientlyswitch between work items when they are blocked.We want you to be able to write straight-line codethat is easy to reason aboutand also gives you safe, controlled concurrency.In order to achieve this behavior that we are after,the operating system needs a runtime contractthat threads will not block, and that is only possibleif the language is able to provide us with that.Swift's concurrency model and the semantics around ithave therefore been designed with this goal in mind.To that end, I'd like to dive into twoof Swift's language-level features that enable usto maintain a contract with the runtime.The first comes from the semantics of awaitand the second, from the tracking of task dependenciesin the Swift runtime.Let's consider these language featuresin the context of our example news application.This is the code snippet we walked through earlierthat handles the results of our news feed updates.Let's see what this logic looks likewhen written with Swift concurrency primitives instead.We'd first start with creating an async implementationof our helper function.Then, instead of handling the resultsof our networking requests on a concurrent dispatch queue,here we are using a task group insteadto manage our concurrency.In the task group, we will create child tasksfor each feed that needs to be updated.Each child task will perform a download from the feed's URLusing the shared URLSession.It will then deserialize the results of the download,format them into articles and finally,we will call an async function to update our database.Here, when calling any async functions,we annotate it with an await keyword.From the "Meet async/await in Swift" talk,we learned that an await is an asynchronous wait.That is, it does not block the current threadwhile waiting for results from the async function.Instead, the function may be suspendedand the thread will be freed up to execute other tasks.How does this happen?How does one give up a thread?My colleague Varun will now shed lighton what is done under the hood in the Swift runtimeto make this possible.Varun Gandhi: Thanks, Rokhini.Before jumping into how async functions are implemented,let's do a quick refresher on how nonasync functions work.Every thread in a running program has one stack,which it uses to store state for function calls.Let's focus on one thread for now.When the thread executes a function call,a new frame is pushed onto its stack.This newly created stack frame can be used by the functionto store local variables, the return address,and any other information that is needed.Once the function finishes executing and returns,its stack frame is popped.Now let's consider async functions.Suppose that a thread called an add(_:) method on the Feed typefrom the updateDatabase function.At this stage, the most recent stack frame will be for add(_:).The stack frame stores local variablesthat do not need to be available across a suspension point.The body of add(_:) has one suspension point,marked by await.The local variables, id and article, are immediatelyused in the body of the for loop after being defined,without any suspension points in-between.So they will be stored in the stack frame.Additionally, there will be two async frames on the heap,one for updateDatabase and one for add.Async frames store information that does need to be availableacross suspension points.Notice that the newArticles argument is definedbefore await but needs to be available after the await.This means that the async frame for addwill keep track of newArticles.Suppose the thread continues executing.When the save function starts executing,the stack frame for add is replacedby a stack frame for save.Instead of adding new stack frames,the top most stack frame is replacedsince any variables that will be needed in the futurewill already have been stored in the list of async frames.The save function also gains an async frame for its use.While the articles are being saved to the database,it would be better if the thread could do some useful workinstead of being blocked.Suppose the execution of the save function is suspended.And the thread is reused to do some other useful workinstead of being blocked.Since all information that is maintainedacross a suspension point is stored on the heap,it can be used to continue execution at a later stage.This list of async frames is the runtime representationof a continuation.Say after a little while,the database request is complete,and suppose some thread is freed up.This could be the same thread as before,or it could be a different thread.Suppose the save function resumes executingon this thread.Once it finishes executingand returns some IDs, then the stack frame for savewill again be replaced by a stack frame for add.After that, the thread can start executing zip.Zipping two arrays is a nonasync operation,so it will create a new stack frame.Since Swift continues to use the operating system stack,both async and nonasync Swift codecan efficiently call into C and Objective-C.Moreover, C and Objective-C code can continueefficiently calling nonasync Swift code.Once the zip function finishes, its stack frame will be poppedand execution will continue.So far, I've described how await is designed to ensureefficient suspension and resumption,while freeing up a thread's resources to do other work.Next, Rokhini will describe the second language feature,which is the runtime's tracking of dependencies between tasks.Rokhini: Thanks, Varun.As described earlier, a function can be broken upinto continuations at an await,also known as a potential suspension point.In this case,the URLSession data task is the async functionand the remaining work after it is the continuation.The continuation can only be executedafter the async function is completed.This is a dependency tracked by the Swift concurrency runtime.Similarly, within the task group,a parent task may create several child tasksand each of those child tasks needs to completebefore a parent task can proceed.This is a dependency that is expressed in your codeby the scope of the task group and therefore explicitly knownto the Swift compiler and runtime.In Swift, tasks can only await other tasks that are knownto the Swift runtime --be it continuations or child tasks.Therefore, code when structuredwith Swift's concurrency primitivesprovide the runtime with a clear understandingof the dependency chain between the tasks.So far, you've learned how Swift's language featuresallow a task to be suspended during an await.Instead, the executing thread is able to reasonabout task dependenciesand pick up a different task instead.This means that code written with Swift concurrencycan maintain a runtime contractthat threads are always able to make forward progress.We have taken advantage of this runtime contractto build integrated OS support for Swift concurrency.This is in the form of a new cooperative thread poolto back Swift concurrency as the default executor.The new thread pool will only spawn as many threadsas there are CPU cores,thereby making sure not to overcommit the system.Unlike GCD's concurrent queues, which will spawn more threadswhen work items block,with Swift threads can always make forward progress.Therefore, the default runtime can be judiciousabout controlling how many threads are spawned.This lets us give your applicationsthe concurrency you need while making sure to avoidthe known pitfalls of excessive concurrency.In previous WWDC talks about concurrencywith Grand Central Dispatch, we've recommended thatyou structure your applications into distinct subsystemsand maintain one serial dispatch queue per subsystemto control the concurrency of your application.This meant that it was difficult for you to get concurrencygreater than one within a subsystemwithout running the risk of thread explosion.With Swift, the language gives us strong invariantswhich the runtime has leveraged,thereby being able to transparently provide youwith better-controlled concurrencyin the default runtime.Now that you understand a bit more about the threading modelfor Swift concurrency,let's go through some considerations to keep in mindwhile adopting these exciting new features in your code.The first consideration that you need to keep in mindhas to do with performance when converting synchronous codeinto asynchronous code.Earlier, we talked through some of the costsassociated with concurrency such as additional memory allocationsand logic in the Swift runtime.As such, you need to be careful to only write new codewith Swift concurrency when the costof introducing concurrency into your codeoutweighs the cost of managing it.The code snippet here might not actually benefitfrom the additional concurrency of spawning a child tasksimply to read a value from the user's defaults.This is because the useful work done by the child taskis diminished by the cost of creating and managing the task.We therefore recommend profiling your codewith Instruments system traceto understand it's performance characteristicsas you adopt Swift concurrency.The second thing to be careful aboutis the notion of atomicity around an await.Swift makes no guarantee that the threadwhich executed the code before the await is the same threadwhich will pick up the continuation as well.In fact, await is an explicit point in your codewhich indicates that atomicity is brokensince the task may be voluntarily descheduled.As such, you should be careful not to hold locksacross an await.Similarly, thread-specific data will not be preservedacross an await either.Any assumptions in your code which expect thread localityshould be revisited to account for the suspending behaviorof await.And lastly, the final considerationhas to do with the runtime contract that is foundationalto the efficient threading model in Swift.Recall that with Swift, the language allows usto uphold a runtime contractthat threads will always be able to make forward progress.It is based on this contractthat we have built a cooperative thread poolto be the default executor for Swift.As you adopt Swift concurrency,it is important to ensure that you continue to maintainthis contract in your code as wellso that the cooperative thread pool can function optimally.It is possible to maintain this contractwithin the cooperative thread pool by using safe primitivesthat make the dependencies in your code explicit and known.With Swift concurrency primitiveslike await, actors, and task groups,these dependencies are made known at compile time.Therefore, the Swift compiler enforces thisand helps you preserve the runtime contract.Primitives like os_unfair_locks and NSLocks are also safebut caution is required when using them.Using a lock in synchronous code is safewhen used for data synchronizationaround a tight, well-known critical section.This is because the thread holding the lockis always able to make forward progresstowards releasing the lock.As such, while the primitive may block a threadfor a short period of time under contention,it does not violate the runtime contractof forward progress.It is worth noting that unlike Swift concurrency primitives,there is no compiler support to aid in correct usage of locks,so it is your responsibility to use this primitive correctly.On the other hand, primitives like semaphoresand condition variables are unsafe to usewith Swift concurrency.This is because they hide dependency informationfrom the Swift runtime,but introduce a dependency in execution in your code.Since the runtime is unaware of this dependency,it cannot make the right scheduling decisionsand resolve them.In particular, do not use primitives that createunstructured tasks and then retroactivelyintroduce a dependency across task boundariesby using a semaphore or an unsafe primitive.Such a code pattern means that a thread can block indefinitelyagainst the semaphore until another threadis able to unblock it.This violates the runtime contractof forward progress for threads.To help you identify uses of such unsafe primitivesin your codebase, we recommend testing your appswith the following environment variable.This runs your app under a modified debug runtime,which enforces the invariant of forward progress.This environment variable can be set in Xcodeon the Run Arguments pane of your project schemeas shown here.When running your apps with this environment variable,if you see a thread from the cooperative thread poolthat appears to be hungit indicates the use of an unsafe blocking primitive.Now, having learned about how the threading modelhas been designed for Swift concurrency,let's discover a little bit more about the primitivesthat are available to us to synchronize statein this new world.Varun: In the Swift concurrency talk on actorsyou've seen how actors can be usedto protect mutable state from concurrent access.Put differently, actors provide a powerful new synchronizationprimitive that you can use.Recall that actors guarantee mutual exclusion:an actor may be executing at most one method call at a time.Mutual exclusion means that the actor's stateis not accessed concurrently, preventing data races.Let's see how actors compareto other forms of mutual exclusion.Consider the earlier exampleof updating the database with some articlesby syncing to a serial queue.If the queue is not already runningwe say that there is no contention.In this case, the calling thread is reused to executethe new work item on the queue without any context switch.Instead, if the serial queue is already runningthe queue is said to be under contention.In this situation, the calling thread is blocked.This blocking behavior is what triggered thread explosionas Rokhini described earlier in the talk.Locks have this same behavior.Because of the problems associated with blocking,we have generally advisedthat you prefer using dispatch async.The primary benefit of dispatch async is that it is nonblocking.So even under contention,it will not lead to thread explosion.The downside of using dispatch async with a serial queueis that when there is no contentionDispatch needs to request a new threadto do the async workwhile the calling thread continues to do something else.Hence, frequent use of dispatch async can leadto excess thread wakeups and context switches.This brings us to actors.Swift's actors combine the best of both worldsby taking advantage of the cooperative thread poolfor efficient scheduling.When you call a method on an actor that is not running,the calling thread can be reused to execute the method call.In the case where the called actor is already running,the calling thread can suspend the function it is executingand pick up other work.Let's look at how these two propertieswork in the context of the example news app.Let's focus on the database and networking subsystems.When updating the application to use Swift concurrency,the serial queue for the databasewould be replaced by a database actor.The concurrent queue for networkingcould be replaced by one actor for each news feed.For simplicity, I've only shown three feed actors here --for the sports feed, the weather feedand the health feed --but in practice, there will be many more.These actors would run on the cooperative thread pool.The feed actors interact with the databaseto save articles and perform other actions.This interaction involves execution switchingfrom one actor to another.We call this process actor hopping.Let's discuss how actor hopping works.Suppose that the actor for the sports feedis running on a thread from the cooperative pool,and it decides to save some articles into the database.For now, let's consider that the database is not being used.This is the uncontended case.The thread can directly hop from the sports feed actorto the database actor.There are two things to notice here.First, the thread did not block while hopping actors.Second, hopping did not require a different thread;the runtime can directly suspendthe work item for the sports feed actorand create a new work item for the database actor.Say the database actor runs for a whilebut it has not completed the first work item.At this moment, suppose that the weather feed actortries to save some articles in the database.This creates a new work item for the database actor.An actor ensures safety by guaranteeing mutual exclusion;at most, one work item may be active at a given time.Since there is already one active work item, D1,the new work item, D2, will be kept pending.Actors are also nonblocking.In this situation, the weather feed actor will be suspendedand the thread it was executing on is now freed upto do other work.After a little while,the initial database request is completed,so the active work item for the database actor is removed.At this point, the runtime may choose to start executingthe pending work item for the database actor.Or it may choose to resume one of the feed actors.Or it could do some other work on the freed-up thread.When there is a lot of asynchronous work,and especially a lot of contention,the system needs to make trade-offsbased on what work is more important.Ideally, high-priority work such as thatinvolving user interaction, would take precedenceover background work, such as saving backups.Actors are designed to allow the system to prioritize work welldue to the notion of reentrancy.But to understand why reentrancy is important here,let's first take a look at how GCD handles priorities.Consider the original news applicationwith the serial database queue.Suppose the database receives some high-priority work,such as for fetching the latest datato update the UI.It also receives low-priority work,such as for backing up the database to iCloud.This needs to be done at some point,but not necessarily immediately.As the code runs, new work items are createdand added to the database queue in some interleaved order.Dispatch queues execute the items receivedin a strict first-in, first-out order.Unfortunately, this means that after item A has executedfive low-priority items need to executebefore getting to the next high-priority item.This is called a priority inversion.Serial queues work around priority inversionby boosting the priority of all of the work in the queuethat's ahead of the high-priority work.In practice, this means that the work in the queuewill be done sooner.However, it does not resolve the main issue,which is that items 1 through 5 still needed to completebefore item B could start executing.Solving this issue requires changing the semantic modelaway from strict first-in, first-out.This brings us to actor reentrancy.Let's explore how reentrancy is connected to orderingwith an example.Consider the database actor executing on a thread.Suppose that it is suspended, awaiting some work,and the sports feed actor starts executing on that thread.Suppose after a while,the sports feed actor calls the database actorto save some articles.Since the database actor is uncontended,the thread can hop to the database actoreven though it has one pending work item.To perform the save operation,a new work item will be created for the database actor.This is what actor reentrancy means;new work items on an actor can make progresswhile one or more older work items on it are suspended.The actor still maintains mutual exclusion:at most one work item can be executing at a given time.After some time, item D2 will finish executing.Notice that D2 finished executing before D1,even though it was created after D1.Hence, support for actor reentrancy meansthat actors can execute items in an orderthat is not strictly first-in, first-out.Let's revisit the example from beforebut with a database actor instead of a serial queue.First, work item A will execute, as it has high priority.Once that's done,there is the same priority inversion as before.Since actors are designed for reentrancy,the runtime may choose to move the higher-priority itemto the front of the queue,ahead of the lower-priority items.This way, higher-priority work could be executed first,with lower-priority work following later.This directly addresses the problem of priority inversion,allowing for more effective schedulingand resource utilization.I've talked a bitabout how actors using the cooperative poolare designed to maintain mutual exclusionand support effective prioritization of work.There is another kind of actor, the main actor,and its characteristics are somewhat differentsince it abstracts over an existing notion in the system:the main thread.Consider the example news app using actors.When updating the user interface,you will need to make calls to and from MainActor.Since the main thread is disjoint from the threadsin the cooperative pool, this requires a context switch.Let's look at the performance implicationsof this with a code example.Consider the following codewhere we have a function updateArticles on MainActor,which loads articles out of the databaseand updates the UI for each article.Each iteration of the looprequires at least two context switches:one to hop from the main actor to the database actorand one to hop back.Let's see how the CPU usage for such a loop would look like.Since each loop iteration requires two context switches,there is a repeating pattern where two threadsrun one after another for a short span of time.If the number of loop iterations is low,and substantial work is being done in each iteration,that is probably all right.However, if execution hops on and offthe main actor frequently,the overhead of switching threads can start to add up.If your application spends a large fraction of timein context switching, you should restructure your codeso that work for the main actor is batched up.You can batch work by pushing the loop into the loadArticlesand updateUI method calls,making sure they process arrays instead of one value at a time.Batching up work reduces the number of context switches.While hopping between actors on the cooperative pool is fast,you still need to be mindful of hops to and from the main actorwhen writing your app.Looking back, in this talkyou've learned how we've worked on making the systemthe most efficient it can be,from the design of the cooperative thread pool --the mechanism for nonblocking suspension --to how actors are implemented.At each step, we're using some aspect of the runtime contractto improve the performance of your applications.We are excited to see how you usethese incredible new language features to write clear,efficient, and delightful Swift code.Thank you for watching and have a great WWDC.♪

♪ Bass music playing ♪♪Rokhini Prabhu: Hello, and welcometo "Swift Concurrency Behind the Scenes."My name is Rokhini, and I work on the Darwin Runtime team.Today, my colleague Varun and I are very excited to talk to youabout some of the underlying nuancesaround Swift concurrency.

This is an advanced talk which builds uponsome of the earlier talks on Swift concurrency.If you are unfamiliar with the concepts of async/await,structured concurrency, and actorsI encourage you to watch these others talks first.In the previous talks on Swift concurrency you've learned aboutthe various language features available this yearnative to Swift and about how to use them.In this talk, we will be diving deeper to understandwhy these primitives are designed the way they are,not only for language safetybut also for performance and efficiency.As you experiment and adopt Swift concurrencyin your own apps, we hope this talk will give youa better mental model of how to reason about Swift concurrencyas well how it interfaces with existing threading librarieslike Grand Central Dispatch.We're going to discuss a few things today.First, we'll talk about the threading modelbehind Swift concurrencyand contrast it with Grand Central Dispatch.We'll talk about how we've taken advantageof concurrency language featuresto build a new thread pool for Swift,thereby achieving better performance and efficiency.Lastly, in this section,we'll touch on considerations you need to keep in mindwhen porting your code to using Swift concurrency.Varun will then talk about synchronizationin Swift concurrency via actors.We'll talk how actors work under the hood,how they compare to existing synchronization primitivesyou already may be familiar with --like serial dispatch queues --and finally, some things to keep in mindwhen writing code with actors.We have a lot of ground to cover today so let's dive right in.In our discussion today about threading models,we'll start by taking a look at an example appwritten with the technologies available todaylike Grand Central Dispatch.We will then see how the same application fareswhen rewritten with Swift concurrency.Suppose I wanted to write my own news feed reader app.Let's talk about what the high-level componentsof my application might be.My app will have a main threadthat will be driving the user interface.I will have a database keeping track of the news feedsthat the user is subscribed to,and finally, a subsystem to handle the networking logicto fetch the latest contents from the feeds.Let's consider how one might structure this appwith Grand Central Dispatch queues.Let's suppose that the user has asked to see the latest news.On the main thread, we will handle the user event gesture.From here, we will dispatch async the requestonto a serial queue that handles the database operations.The reason for this is twofold.Firstly, by dispatching the work onto a different queue,we ensure that the main threadcan remain responsive to user input even while waitingfor a potentially large amount of work to happen.Secondly, access to the database is protected,since a serial queue guarantees mutual exclusion.While on the database queue,we'll iterate through the news feedsthe user has subscribed to and for each of them,schedule a networking request to our URLSessionto download the contents of that feed.As the results of the networking requests come in,the URLSession callback will be called on our delegate queuewhich is a concurrent queue.In the completion handler for each of the results,we will synchronously update the databasewith the latest requests from each of these feeds,so as to cache it for future use.And finally, we'll wake up the main threadto refresh the UI.This seems like a perfectly reasonable wayto structure such an application.We've made sure not to block the main threadwhile working on requests.And by handling the network requests concurrently,we've taken advantage of the inherent parallelismin our program.Let's take a closer look at a code snippetthat shows how we process the resultsof our networking requests.First, we have created a URLSessionfor performing downloads from our news feeds.As you can see here, we've set the delegate queueof this URLSession to be a concurrent queue.We then iterate over all the news feedsthat need to be updated and for each of them,schedule a data task in the URLSession.In the completion handler of the data task --which will be invoked on the delegate queue --we deserialize the results of our downloadand format them into articles.We then dispatch sync against our database queuebefore updating the results for the feed.So here you can see that we've writtensome straight-line code to do something fairly straightforwardbut this code has some hidden performance pitfalls.To understand more about these performance problems,we need to first dig into how threads are brought upto handle work on GCD queues.In Grand Central Dispatch,when work is enqueued onto a queue,the system will bring up a threadto service that work item.Since a concurrent queuecan handle multiple work items at once,the system will bring up several threadsuntil we have saturated all the CPU cores.However, if a thread blocks --as seen on the first CPU core here --and there is more work to be done on the concurrent queue,GCD will bring up more threadsto drain the remaining work items.The reason for this is twofold.Firstly, by giving your process another thread,we are able to ensure that each corecontinues to have a thread that executes workat any given time.This gives your applicationsa good, continuing level of concurrency.Secondly the blocked thread may be waiting on a resourcelike a semaphore, before it can make further progress.The new thread that is brought upto continue working on the queuemay be able to help unblock the resourcethat is being waited on by the first thread.So now that we understand a bit moreabout thread bringups in GCD, let's go backand look at the CPU execution of the codefrom our news app.On a two-core device like the Apple Watch,GCD will first bring up two threadsto process the feed update results.As the threads block on accessing the database queue,more threads are created to continue workingon the networking queue.The CPU then has to context switch between different threadsprocessing the networking resultsas indicated by the white vertical linesbetween the various threads.This means that in our news application,we could easily end up with a very large number of threads.If the user has a hundred feeds that need to be updated,then each of those URL data taskswill have a completion block on the concurrent queuewhen the network requests complete.GCD will bring up more threads when each of the callbacksblock on the database queue,resulting in the application having lots of threads.Now you might ask,what's so bad about having a lot of threads in our application?Having a lot of threads in our applicationsmeans that the system is overcommittedwith more threads than we have CPU cores.Consider an iPhone with six CPU cores.If our news application has a hundred feed updatesthat need to be processed,this means that we have overcommitted the iPhonewith 16 times more threads than cores.This is the phenomenon we call thread explosion.Some of our previous WWDC talks have gone into further detailon the risks associated with this,including the possibility of deadlocks in your application.Thread explosion also comes with memoryand scheduling overhead that may not be immediately obvious,so let's look into this further.Looking back at our news application,each of the blocked threads is holding onto valuable memoryand resources while waiting to run again.Each blocked thread has a stackand associated kernel data structures to track the thread.Some of these threads may be holding onto lockswhich other threads that are running might need.This is a large number of resources and memoryto be holding onto for threads which are not making progress.There is also greater scheduling overheadas a result of thread explosion.As new threads are brought up,the CPU need to perform a full thread context switchin order to switch away from the old threadto start executing the new thread.As the blocked threads become runnable again,the scheduler will have to timeshare the threads on the CPUso that they are all able to make forward progress.Now, timesharing of threadsis fine if that happens a few times --that is the power of concurrency.But when there is thread explosion,having to timeshare hundreds of threads on a devicewith limited cores can lead to excessive context switching.The scheduling latencies of these threadsoutweigh the amount of useful work they would do,therefore, resulting in the CPUrunning less efficiently as well.As we've seen so far, it is easy to misssome of these nuances about the threading hygienewhen writing applications with GCD queuesthereby resulting in poor performanceand greater overhead.Building on this experience,Swift has taken a different approachwhen designing concurrency into the language.We've built Swift concurrency with performanceand efficiency in mind as well so that your appscan enjoy controlled, structured,and safe concurrency.With Swift, we want to change the execution model of appsfrom the following model,which has lots of threads and context switches, to this.Here you see that we have just two threadsexecuting on our two-core systemand there are no thread context switches.All of our blocked threads go away and insteadwe have a lightweight object known as a continuationto track resumption of work.When threads execute work under Swift concurrencythey switch between continuationsinstead of performing a full thread context switch.This means that we now only paythe cost of a function call instead.So the runtime behavior that we want for Swift concurrencyis to create only as many threads as there are CPU cores,and for threads to be able to cheaply and efficientlyswitch between work items when they are blocked.We want you to be able to write straight-line codethat is easy to reason aboutand also gives you safe, controlled concurrency.In order to achieve this behavior that we are after,the operating system needs a runtime contractthat threads will not block, and that is only possibleif the language is able to provide us with that.Swift's concurrency model and the semantics around ithave therefore been designed with this goal in mind.To that end, I'd like to dive into twoof Swift's language-level features that enable usto maintain a contract with the runtime.The first comes from the semantics of awaitand the second, from the tracking of task dependenciesin the Swift runtime.Let's consider these language featuresin the context of our example news application.This is the code snippet we walked through earlierthat handles the results of our news feed updates.Let's see what this logic looks likewhen written with Swift concurrency primitives instead.We'd first start with creating an async implementationof our helper function.Then, instead of handling the resultsof our networking requests on a concurrent dispatch queue,here we are using a task group insteadto manage our concurrency.In the task group, we will create child tasksfor each feed that needs to be updated.Each child task will perform a download from the feed's URLusing the shared URLSession.It will then deserialize the results of the download,format them into articles and finally,we will call an async function to update our database.Here, when calling any async functions,we annotate it with an await keyword.From the "Meet async/await in Swift" talk,we learned that an await is an asynchronous wait.That is, it does not block the current threadwhile waiting for results from the async function.Instead, the function may be suspendedand the thread will be freed up to execute other tasks.How does this happen?How does one give up a thread?My colleague Varun will now shed lighton what is done under the hood in the Swift runtimeto make this possible.Varun Gandhi: Thanks, Rokhini.Before jumping into how async functions are implemented,let's do a quick refresher on how nonasync functions work.Every thread in a running program has one stack,which it uses to store state for function calls.Let's focus on one thread for now.When the thread executes a function call,a new frame is pushed onto its stack.This newly created stack frame can be used by the functionto store local variables, the return address,and any other information that is needed.Once the function finishes executing and returns,its stack frame is popped.Now let's consider async functions.Suppose that a thread called an add(_:) method on the Feed typefrom the updateDatabase function.At this stage, the most recent stack frame will be for add(_:).The stack frame stores local variablesthat do not need to be available across a suspension point.The body of add(_:) has one suspension point,marked by await.The local variables, id and article, are immediatelyused in the body of the for loop after being defined,without any suspension points in-between.So they will be stored in the stack frame.Additionally, there will be two async frames on the heap,one for updateDatabase and one for add.Async frames store information that does need to be availableacross suspension points.Notice that the newArticles argument is definedbefore await but needs to be available after the await.This means that the async frame for addwill keep track of newArticles.Suppose the thread continues executing.When the save function starts executing,the stack frame for add is replacedby a stack frame for save.Instead of adding new stack frames,the top most stack frame is replacedsince any variables that will be needed in the futurewill already have been stored in the list of async frames.The save function also gains an async frame for its use.While the articles are being saved to the database,it would be better if the thread could do some useful workinstead of being blocked.Suppose the execution of the save function is suspended.And the thread is reused to do some other useful workinstead of being blocked.Since all information that is maintainedacross a suspension point is stored on the heap,it can be used to continue execution at a later stage.This list of async frames is the runtime representationof a continuation.Say after a little while,the database request is complete,and suppose some thread is freed up.This could be the same thread as before,or it could be a different thread.Suppose the save function resumes executingon this thread.Once it finishes executingand returns some IDs, then the stack frame for savewill again be replaced by a stack frame for add.After that, the thread can start executing zip.Zipping two arrays is a nonasync operation,so it will create a new stack frame.Since Swift continues to use the operating system stack,both async and nonasync Swift codecan efficiently call into C and Objective-C.Moreover, C and Objective-C code can continueefficiently calling nonasync Swift code.Once the zip function finishes, its stack frame will be poppedand execution will continue.So far, I've described how await is designed to ensureefficient suspension and resumption,while freeing up a thread's resources to do other work.Next, Rokhini will describe the second language feature,which is the runtime's tracking of dependencies between tasks.Rokhini: Thanks, Varun.As described earlier, a function can be broken upinto continuations at an await,also known as a potential suspension point.In this case,the URLSession data task is the async functionand the remaining work after it is the continuation.The continuation can only be executedafter the async function is completed.This is a dependency tracked by the Swift concurrency runtime.Similarly, within the task group,a parent task may create several child tasksand each of those child tasks needs to completebefore a parent task can proceed.This is a dependency that is expressed in your codeby the scope of the task group and therefore explicitly knownto the Swift compiler and runtime.In Swift, tasks can only await other tasks that are knownto the Swift runtime --be it continuations or child tasks.Therefore, code when structuredwith Swift's concurrency primitivesprovide the runtime with a clear understandingof the dependency chain between the tasks.So far, you've learned how Swift's language featuresallow a task to be suspended during an await.Instead, the executing thread is able to reasonabout task dependenciesand pick up a different task instead.This means that code written with Swift concurrencycan maintain a runtime contractthat threads are always able to make forward progress.We have taken advantage of this runtime contractto build integrated OS support for Swift concurrency.This is in the form of a new cooperative thread poolto back Swift concurrency as the default executor.The new thread pool will only spawn as many threadsas there are CPU cores,thereby making sure not to overcommit the system.Unlike GCD's concurrent queues, which will spawn more threadswhen work items block,with Swift threads can always make forward progress.Therefore, the default runtime can be judiciousabout controlling how many threads are spawned.This lets us give your applicationsthe concurrency you need while making sure to avoidthe known pitfalls of excessive concurrency.In previous WWDC talks about concurrencywith Grand Central Dispatch, we've recommended thatyou structure your applications into distinct subsystemsand maintain one serial dispatch queue per subsystemto control the concurrency of your application.This meant that it was difficult for you to get concurrencygreater than one within a subsystemwithout running the risk of thread explosion.With Swift, the language gives us strong invariantswhich the runtime has leveraged,thereby being able to transparently provide youwith better-controlled concurrencyin the default runtime.Now that you understand a bit more about the threading modelfor Swift concurrency,let's go through some considerations to keep in mindwhile adopting these exciting new features in your code.The first consideration that you need to keep in mindhas to do with performance when converting synchronous codeinto asynchronous code.Earlier, we talked through some of the costsassociated with concurrency such as additional memory allocationsand logic in the Swift runtime.As such, you need to be careful to only write new codewith Swift concurrency when the costof introducing concurrency into your codeoutweighs the cost of managing it.The code snippet here might not actually benefitfrom the additional concurrency of spawning a child tasksimply to read a value from the user's defaults.This is because the useful work done by the child taskis diminished by the cost of creating and managing the task.We therefore recommend profiling your codewith Instruments system traceto understand it's performance characteristicsas you adopt Swift concurrency.The second thing to be careful aboutis the notion of atomicity around an await.Swift makes no guarantee that the threadwhich executed the code before the await is the same threadwhich will pick up the continuation as well.In fact, await is an explicit point in your codewhich indicates that atomicity is brokensince the task may be voluntarily descheduled.As such, you should be careful not to hold locksacross an await.Similarly, thread-specific data will not be preservedacross an await either.Any assumptions in your code which expect thread localityshould be revisited to account for the suspending behaviorof await.And lastly, the final considerationhas to do with the runtime contract that is foundationalto the efficient threading model in Swift.Recall that with Swift, the language allows usto uphold a runtime contractthat threads will always be able to make forward progress.It is based on this contractthat we have built a cooperative thread poolto be the default executor for Swift.As you adopt Swift concurrency,it is important to ensure that you continue to maintainthis contract in your code as wellso that the cooperative thread pool can function optimally.It is possible to maintain this contractwithin the cooperative thread pool by using safe primitivesthat make the dependencies in your code explicit and known.With Swift concurrency primitiveslike await, actors, and task groups,these dependencies are made known at compile time.Therefore, the Swift compiler enforces thisand helps you preserve the runtime contract.Primitives like os_unfair_locks and NSLocks are also safebut caution is required when using them.Using a lock in synchronous code is safewhen used for data synchronizationaround a tight, well-known critical section.This is because the thread holding the lockis always able to make forward progresstowards releasing the lock.As such, while the primitive may block a threadfor a short period of time under contention,it does not violate the runtime contractof forward progress.It is worth noting that unlike Swift concurrency primitives,there is no compiler support to aid in correct usage of locks,so it is your responsibility to use this primitive correctly.On the other hand, primitives like semaphoresand condition variables are unsafe to usewith Swift concurrency.This is because they hide dependency informationfrom the Swift runtime,but introduce a dependency in execution in your code.Since the runtime is unaware of this dependency,it cannot make the right scheduling decisionsand resolve them.In particular, do not use primitives that createunstructured tasks and then retroactivelyintroduce a dependency across task boundariesby using a semaphore or an unsafe primitive.Such a code pattern means that a thread can block indefinitelyagainst the semaphore until another threadis able to unblock it.This violates the runtime contractof forward progress for threads.To help you identify uses of such unsafe primitivesin your codebase, we recommend testing your appswith the following environment variable.This runs your app under a modified debug runtime,which enforces the invariant of forward progress.This environment variable can be set in Xcodeon the Run Arguments pane of your project schemeas shown here.When running your apps with this environment variable,if you see a thread from the cooperative thread poolthat appears to be hungit indicates the use of an unsafe blocking primitive.Now, having learned about how the threading modelhas been designed for Swift concurrency,let's discover a little bit more about the primitivesthat are available to us to synchronize statein this new world.Varun: In the Swift concurrency talk on actorsyou've seen how actors can be usedto protect mutable state from concurrent access.Put differently, actors provide a powerful new synchronizationprimitive that you can use.Recall that actors guarantee mutual exclusion:an actor may be executing at most one method call at a time.Mutual exclusion means that the actor's stateis not accessed concurrently, preventing data races.Let's see how actors compareto other forms of mutual exclusion.Consider the earlier exampleof updating the database with some articlesby syncing to a serial queue.If the queue is not already runningwe say that there is no contention.In this case, the calling thread is reused to executethe new work item on the queue without any context switch.Instead, if the serial queue is already runningthe queue is said to be under contention.In this situation, the calling thread is blocked.This blocking behavior is what triggered thread explosionas Rokhini described earlier in the talk.Locks have this same behavior.Because of the problems associated with blocking,we have generally advisedthat you prefer using dispatch async.The primary benefit of dispatch async is that it is nonblocking.So even under contention,it will not lead to thread explosion.The downside of using dispatch async with a serial queueis that when there is no contentionDispatch needs to request a new threadto do the async workwhile the calling thread continues to do something else.Hence, frequent use of dispatch async can leadto excess thread wakeups and context switches.This brings us to actors.Swift's actors combine the best of both worldsby taking advantage of the cooperative thread poolfor efficient scheduling.When you call a method on an actor that is not running,the calling thread can be reused to execute the method call.In the case where the called actor is already running,the calling thread can suspend the function it is executingand pick up other work.Let's look at how these two propertieswork in the context of the example news app.Let's focus on the database and networking subsystems.When updating the application to use Swift concurrency,the serial queue for the databasewould be replaced by a database actor.The concurrent queue for networkingcould be replaced by one actor for each news feed.For simplicity, I've only shown three feed actors here --for the sports feed, the weather feedand the health feed --but in practice, there will be many more.These actors would run on the cooperative thread pool.The feed actors interact with the databaseto save articles and perform other actions.This interaction involves execution switchingfrom one actor to another.We call this process actor hopping.Let's discuss how actor hopping works.Suppose that the actor for the sports feedis running on a thread from the cooperative pool,and it decides to save some articles into the database.For now, let's consider that the database is not being used.This is the uncontended case.The thread can directly hop from the sports feed actorto the database actor.There are two things to notice here.First, the thread did not block while hopping actors.Second, hopping did not require a different thread;the runtime can directly suspendthe work item for the sports feed actorand create a new work item for the database actor.Say the database actor runs for a whilebut it has not completed the first work item.At this moment, suppose that the weather feed actortries to save some articles in the database.This creates a new work item for the database actor.An actor ensures safety by guaranteeing mutual exclusion;at most, one work item may be active at a given time.Since there is already one active work item, D1,the new work item, D2, will be kept pending.Actors are also nonblocking.In this situation, the weather feed actor will be suspendedand the thread it was executing on is now freed upto do other work.After a little while,the initial database request is completed,so the active work item for the database actor is removed.At this point, the runtime may choose to start executingthe pending work item for the database actor.Or it may choose to resume one of the feed actors.Or it could do some other work on the freed-up thread.When there is a lot of asynchronous work,and especially a lot of contention,the system needs to make trade-offsbased on what work is more important.Ideally, high-priority work such as thatinvolving user interaction, would take precedenceover background work, such as saving backups.Actors are designed to allow the system to prioritize work welldue to the notion of reentrancy.But to understand why reentrancy is important here,let's first take a look at how GCD handles priorities.Consider the original news applicationwith the serial database queue.Suppose the database receives some high-priority work,such as for fetching the latest datato update the UI.It also receives low-priority work,such as for backing up the database to iCloud.This needs to be done at some point,but not necessarily immediately.As the code runs, new work items are createdand added to the database queue in some interleaved order.Dispatch queues execute the items receivedin a strict first-in, first-out order.Unfortunately, this means that after item A has executedfive low-priority items need to executebefore getting to the next high-priority item.This is called a priority inversion.Serial queues work around priority inversionby boosting the priority of all of the work in the queuethat's ahead of the high-priority work.In practice, this means that the work in the queuewill be done sooner.However, it does not resolve the main issue,which is that items 1 through 5 still needed to completebefore item B could start executing.Solving this issue requires changing the semantic modelaway from strict first-in, first-out.This brings us to actor reentrancy.Let's explore how reentrancy is connected to orderingwith an example.

Consider the database actor executing on a thread.Suppose that it is suspended, awaiting some work,and the sports feed actor starts executing on that thread.Suppose after a while,the sports feed actor calls the database actorto save some articles.Since the database actor is uncontended,the thread can hop to the database actoreven though it has one pending work item.To perform the save operation,a new work item will be created for the database actor.This is what actor reentrancy means;new work items on an actor can make progresswhile one or more older work items on it are suspended.The actor still maintains mutual exclusion:at most one work item can be executing at a given time.After some time, item D2 will finish executing.Notice that D2 finished executing before D1,even though it was created after D1.Hence, support for actor reentrancy meansthat actors can execute items in an orderthat is not strictly first-in, first-out.Let's revisit the example from beforebut with a database actor instead of a serial queue.First, work item A will execute, as it has high priority.Once that's done,there is the same priority inversion as before.Since actors are designed for reentrancy,the runtime may choose to move the higher-priority itemto the front of the queue,ahead of the lower-priority items.This way, higher-priority work could be executed first,with lower-priority work following later.This directly addresses the problem of priority inversion,allowing for more effective schedulingand resource utilization.I've talked a bitabout how actors using the cooperative poolare designed to maintain mutual exclusionand support effective prioritization of work.There is another kind of actor, the main actor,and its characteristics are somewhat differentsince it abstracts over an existing notion in the system:the main thread.Consider the example news app using actors.When updating the user interface,you will need to make calls to and from MainActor.Since the main thread is disjoint from the threadsin the cooperative pool, this requires a context switch.Let's look at the performance implicationsof this with a code example.Consider the following codewhere we have a function updateArticles on MainActor,which loads articles out of the databaseand updates the UI for each article.Each iteration of the looprequires at least two context switches:one to hop from the main actor to the database actorand one to hop back.Let's see how the CPU usage for such a loop would look like.Since each loop iteration requires two context switches,there is a repeating pattern where two threadsrun one after another for a short span of time.If the number of loop iterations is low,and substantial work is being done in each iteration,that is probably all right.However, if execution hops on and offthe main actor frequently,the overhead of switching threads can start to add up.If your application spends a large fraction of timein context switching, you should restructure your codeso that work for the main actor is batched up.You can batch work by pushing the loop into the loadArticlesand updateUI method calls,making sure they process arrays instead of one value at a time.Batching up work reduces the number of context switches.While hopping between actors on the cooperative pool is fast,you still need to be mindful of hops to and from the main actorwhen writing your app.Looking back, in this talkyou've learned how we've worked on making the systemthe most efficient it can be,from the design of the cooperative thread pool --the mechanism for nonblocking suspension --to how actors are implemented.At each step, we're using some aspect of the runtime contractto improve the performance of your applications.We are excited to see how you usethese incredible new language features to write clear,efficient, and delightful Swift code.Thank you for watching and have a great WWDC.♪

4:57 -GCD code with hidden performance pitfalls

13:18 -Swift concurrency equivalent using a task group

15:16 -Async functions: stack frames and async frames

37:13 -Excessive context switching due to Main actor hoppping

38:18 -Batch UI work to reduce the number of context switches

## Code Samples

```swift
func
 
deserializeArticles
(
from
 
data
: 
Data
)
 
throws
 -> [
Article
] { 
/* ... */
 }

func
 
updateDatabase
(
with
 
articles
: [
Article
], 
for
 
feed
: 
Feed
)
 { 
/* ... */
 }


let
 urlSession 
=
 
URLSession
(configuration: .default, delegate: 
self
, delegateQueue: concurrentQueue)


for
 feed 
in
 feedsToUpdate {
    
let
 dataTask 
=
 urlSession.dataTask(with: feed.url) { data, response, error 
in

        
// ...

        
guard
 
let
 data 
=
 data 
else
 { 
return
 }
        
do
 {
            
let
 articles 
=
 
try
 deserializeArticles(from: data)
            databaseQueue.sync {
                updateDatabase(with: articles, for: feed)
            }
        } 
catch
 { 
/* ... */
 }
    }
    dataTask.resume()
}
```

```swift
func
 
deserializeArticles
(
from
 
data
: 
Data
)
 
throws
 -> [
Article
] { 
/* ... */
 }

func
 
updateDatabase
(
with
 
articles
: [
Article
], 
for
 
feed
: 
Feed
)
 
async
 { 
/* ... */
 }


await
 withThrowingTaskGroup(of: [
Article
].
self
) { group 
in

    
for
 feed 
in
 feedsToUpdate {
        group.async {
            
let
 (data, response) 
=
 
try
 
await
 
URLSession
.shared.data(from: feed.url)
            
// ...

            
let
 articles 
=
 
try
 deserializeArticles(from: data)
            
await
 updateDatabase(with: articles, for: feed)
            
return
 articles
        }
    }
}
```

```swift
// on Database


func
 
save
(
_
 
newArticles
: [
Article
], 
for
 
feed
: 
Feed
)
 
async
 
throws
 -> [
ID
] { 
/* ... */
 }


// on Feed


func
 
add
(
_
 
newArticles
: [
Article
])
 
async
 
throws
 {
    
let
 ids 
=
 
try
 
await
 database.save(newArticles, for: 
self
)
    
for
 (id, article) 
in
 
zip
(ids, newArticles) {
        articles[id] 
=
 article
    }
}


func
 
updateDatabase
(
with
 
articles
: [
Article
], 
for
 
feed
: 
Feed
)
 
async
 
throws
 {
    
// skip old articles ...

    
try
 
await
 feed.add(articles)
}
```

```swift
// on database actor


func
 
loadArticle
(
with
 
id
: 
ID
)
 
async
 
throws
 -> 
Article
 { 
/* ... */
 }


@MainActor
 
func
 
updateUI
(
for
 
article
: 
Article
)
 
async
 { 
/* ... */
 }


@MainActor
 
func
 
updateArticles
(
for
 
ids
: [
ID
])
 
async
 
throws
 {
    
for
 id 
in
 ids {
        
let
 article 
=
 
try
 
await
 database.loadArticle(with: id)
        
await
 updateUI(for: article)
    }
}
```

```swift
// on database actor


func
 
loadArticles
(
with
 
ids
: [
ID
])
 
async
 
throws
 -> [
Article
]


@MainActor
 
func
 
updateUI
(
for
 
articles
: [
Article
])
 
async



@MainActor
 
func
 
updateArticles
(
for
 
ids
: [
ID
])
 
async
 
throws
 {
    
let
 articles 
=
 
try
 
await
 database.loadArticles(with: ids)
    
await
 updateUI(for: articles)
}
```

