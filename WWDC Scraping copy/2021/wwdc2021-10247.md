# Wwdc2021 10247

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Capture high-quality photos using video formatsYour app can take full advantage of the powerful camera systems on iPhone by using the AVCapture APIs. Learn how to choose the most appropriate photo or video formats for your use cases while balancing the trade-offs between photo quality and delivery speed. Discover some powerful new algorithms which can help you deliver greatly improved photo quality when you use video formats in your app.

To learn more about improvements in AVCapture, be sure to also see the "What's new in camera capture" video.ResourcesCapture setupHD VideoSD VideoRelated VideosWWDC22Bring Continuity Camera to your macOS appWWDC21What’s new in camera captureWWDC16Advances in iOS Photography

Your app can take full advantage of the powerful camera systems on iPhone by using the AVCapture APIs. Learn how to choose the most appropriate photo or video formats for your use cases while balancing the trade-offs between photo quality and delivery speed. Discover some powerful new algorithms which can help you deliver greatly improved photo quality when you use video formats in your app.

To learn more about improvements in AVCapture, be sure to also see the "What's new in camera capture" video.

Capture setup

HD VideoSD Video

HD Video

SD Video

Bring Continuity Camera to your macOS app

What’s new in camera capture

Advances in iOS Photography

Search this video…♪ ♪Hi, my name is Roy.I'm an engineer on the Camera Software team.Today I will be walking you through some excitingphoto quality improvementswe made with our most popular video formats,and how your applications can make use of themto deliver an even better experience.iPhone is the most popular camera in the world,and for many years, developers have beentaking advantage of its powerful camera systemsto provide a diverse set of world-class experiences,from professional photography apps to video streaming tools.Different scenarios call for different levelsof photo quality.For example, apps dedicated to taking still photoswill demand the absolute best qualitythat the cameras can provide.A social app, on the other hand,might need to apply face effect overlayson top of the video frames being streamed.And this custom rendering might be computationally expensive.In order to avoid frame drops,the developer might prefer lower resolution framesso there are fewer pixels to process per frame.This diversity in use cases calls for an easy wayto specify where you want to landon the scale of quality versus performance.Before we dive into photo quality, however,let's have a brief refresheron how photos are taken on iOS in general.We will start with an AVCaptureSession object,around which we can build our object graph.Since we are taking photos, we will use a cameraas our AVCaptureDevice.Then, an AVCaptureDeviceInput is instantiatedbased on that device,and it will provide input data to the session.An AVCapturePhotoOutput will then be added to the graphas the recipients of the photos.And all these elements are then connected togetherusing an AVCaptureConnection.After the session started running,we can capture photos by calling the capturePhoto methodon the AVCapturePhotoOutput instance.Further customization can be doneusing the AVCapturePhotoSettings objectpassed to the capturePhoto method.The captured photo will be representedas an AVCapturePhoto objectthat you will receive in your delegate method.We had a very detailed discussion on these APIsin the 2016 session, Advances in iOS Photography.Please check it out if you haven't already.Now that we know how to take photos on iOS in general,let's see how high quality photos can be taken.Historically, if you wanted to capture photosof the best possible quality, you would set theisAutoStillImageStabilization Enabled propertyon your AVCapturePhotoSettings to true,and that's because still image stabilizationwas the main method for getting higher quality photos.But over the years, we have been continuouslyevolving our photo quality-enhancing algorithms.In addition to still image stabilization,we now have a far richer set of techniques to draw from,such as a variety of multi-image fusion technologies,including Smart HDR and Deep Fusion.Consequently, the nameisAutoStillImageStabilization Enabledhas become quite obsolete as a proxy for high photo quality.To solve this problem, in iOS 13, we introducedAVCapturePhotoOutput.QualityPrio ritization.It's a very easy way to tell AVCapturePhotoOutputhow to prioritize quality in your photo captures.We haven't had a chance to talk about this important APIat previous WWDCs,so let's now take a moment to see how it works.There are three quality prioritization levelsto choose from:speed, balanced, and quality.With speed, you are telling the frameworkthat the speed of the capture is what you care about the most,even at the expense of photo quality.If a balance needs to be struck betweenphoto quality and delivery speed,balanced should be used.Quality does the opposite of speed.It says image quality should be prioritizedfirst and foremost,while the potential slowness of the capture processcan be tolerated.Please note that the quality prioritization specifiedonly serves as a hint to the AVCapturePhotoOutput,and it doesn't dictate what algorithms to use.Ultimately, the AVCapturePhotoOutputwill consider a variety of constraintsand choose the most appropriate algorithm for the current scene.For instance, it might choose a different methodfor a low-light situation than in a well-lit space.That being said, we understand thatbased on different capture durations,you might want to plan your user experience differently.So on the AVCaptureResolvedPhotoSettingsobject passed to some of the AVCapturePhotoOutputDelegatemethods, we give you a property calledphotoProcessingTimeRange that indicates how longit will take to deliver the photo to your delegate.This, for instance, can help you decide whetheryou want to put out a spinner if the capture will take a while.Let's see how it works in code.When you are setting up your AVCapturePhotoOutput,you can specify a max quality prioritizationthat the particular capture session will require.If you choose not to do so, the default value is balanced.This only has to be set once.The importance of doing so is that depending ondifferent settings, we will configureour capture pipelines differently.For instance, if we know you will not gobeyond speed prioritization,we can construct a capture pipelinethat consumes much less memory and powerthan the one for, say, balanced prioritization.So we encourage you to choose responsiblyand only take what you need.Before you call the capturePhoto method,you can customize the quality prioritizationfor this particular captureby setting the photoQualityPrioritizationproperty on your AVCapturePhotoSettings object.The default value is balanced.As demonstrated here, we are usingtwo different levels in two different situations.Please note that the per-capture quality prioritizationcannot go beyond your AVCapturePhotoOutput'smax quality prioritization,otherwise you will get an exception.The performance characteristics of the three levelsare determined by the underlying algorithms we use.The mappings differ based on the kinds of format you use,and we will talk some more on the differencebetween photo and video formats momentarily.With photo formats, Speed will get you WYSIWYG photos--that is What You See is What You Get photos--which are lightly processedonly with some noise reduction applied.If Balanced is specified, we will choose from a collectionof fast fusion algorithms that produce much betterphoto quality than WYSIWYG photosat a somewhat slower capture rate.For Quality, depending on the current deviceand lux level, the framework will usesome heavy machineries such as Deep Fusionin order to provide the best possible photo quality.The photos will look great,but there is no free lunch.You pay for it by using more time.For video formats, on the other hand,all levels will use the lightest processingto deliver photos as fast as possible.We have been talking about photo and video formatsfor a while now.Let's take a closer lookat the differences between them.By using photo formats, you are signaling to the frameworkthat taking still photos is what you care about the most.For example, if you are using an AVCaptureVideoDataOutputwith a photo format,the sample buffers you get by defaultwill only be of preview resolutions,and that's because knowing that taking photosis your top priority, we can assume that these frameswill be used for preview rather than video recordings.A good reason to choose photo formats is thatsome photo-centric features are exclusive for photo formats,such as Live Photo and ProRAW, et cetera.If that's something you want to do,then photo format is the way to go.Photo formats come with the highest resolutions available,but the frame rates are limited to 30 frames per second.To choose photo formats, you can setyour session preset to photo.Or you can pick a format whereisHighestPhotoQualitySupported is true.The usage of video formats, on the other hand,indicates that the experiences will now center around videos.You will get resolutions more suitablefor recording and streaming, and you will be ableto use high frame rates such as 60 fps.If a format is not a photo format,then it's considered a video format.So you can select one by using a non-photo session presetor choosing a format whereisHighestPhotoQualitySupported is false.You might be wondering why we are not applyingsome of the powerful algorithms to video formats.It's not because we're lazy.We have good reasons for that.Many apps choose to use video formats because they need to doheavy custom processing,and video formats are well-suited for this purposebecause of their low overhead.If we leverage some of the aforementionedphoto enhancing techniques,we might introduce degradations in these apps' experiences.For instance, an AR app might allow usersto snap a photo of the 3D scene that they are interacting with.Running the existing fusion algorithms at this pointis likely to introduce frame dropsin the app's camera feed,interrupting its core feature.So we have been very conscious of this delicate balance betweenquality and speed, and we designed our video formatsto work responsively even under the most demanding conditions.But those compromises stop today with iOS 15.We are taking a major leap in photo qualitywith our most popular video formats.With some improved algorithms, we are now able to radicallyimprove photo quality without impacting other aspectsof your apps' experiences.With this new feature, your apps can now take amazing photos,while retaining the same flexibilityto perform sophisticated custom computation.So how big of a quality leap are we talking about here?Let's take a look at some before-and-after comparisons.The improvement is quite substantial.The little boy's face on the righthas much less noise,and thus looks much more natural.And we can better perceive the light coming off his hair.The catch lights in your subjects' eyesare simply more vibrant and lively.In this outdoor low-light situation,there is superior de-noising on her face and clothes.Lastly, the environment also looks better.The leather texture on the chair is much better preserved.Now that you're enticed, let's take a lookat the algorithm mappings for the supported video formats.Speed will still get you the lightly processedWYSIWYG photos.They are still the fastest way of getting a photo,and since speed is now your top priority,this fits the bill perfectly, so we didn't change it.You will not be getting any frame dropsin your video recordingsor any disruptions in your preview feed.With Balanced, however, you are now gettinga significant bump in quality, while only gettinga very slight increase in the photo's processing time.And just like in Speed, your video recordingwill not have any frame drops.Your preview feed will not get interrupted,even when those great-looking photosare taken and processed.Finally, for Quality,we are running more expensive algorithmsto get even better qualities.This might drop frames or cause preview feed interruptions,depending on how recent your devices are.This feature will be available on all iPhoneswith support all the way back to iPhone XS.The video formats that are getting this upgradeare the most popular ones:1280x720with support for both 30 and 60 frames per second.1920x1080, also for both 30 and 60 fps.1920x1440, for 30 fps.And we even added support for 4K, with 30 fps.So how can you make sure you are usingthe right formats in your code?It's very simple.In iOS 15, we are introducing a new property calledisHighPhotoQualitySupportedon the AVCaptureDevice.Format type.For formats that support this feature,this property will be true.Any format with this property being trueis guaranteed to be a video format,so you don't have to worryabout accidentally picking a photo format.Let's say you want to get any such format.You just need to get the formats availableon your AVCaptureDevice instance.Then just select the onewith isHighPhotoQualitySupported being true.We updated our sample code AVCam to use this feature.Please check it out if you want to see a working example.It's possible to confuse the new propertyisHighPhotoQualitySupportedwith the existing isHighestPhotoQualitySupported.Like we mentioned earlier, the latter tells youwhether a format is a photo format,and it doesn't tell you whether a video formatsupports high photo quality.Now, do you need to put in any work at allto get this new feature?The answer is maybe.If you are already using AVCapturePhotoOutput,and the .balanced prioritization,then congratulations, you will automaticallyget better-looking photos on iOS 15.If your app is using speed prioritization,by simply updating it to balanced,you will receive better photoswithout having to worry about any frame drops.If you're still using the deprecatedAVCaptureStillImageOutput,then hopefully this will give youa big incentive to switch over.Since now using the quality prioritizationmight introduce frame drops in your videos,we don't want to impose that new behavior on your appswithout you opting in.So we put in a link time check to make sure thatif your app is using quality prioritizationwith a video format and was compiled prior to iOS 15,then we will automatically change it to balanced.If you indeed would like to get the best quality,all you need to do is re-compile your app with the iOS 15 SDK.There are a few caveats to be aware of.This feature currently only workswith AVCaptureSession,and not with AVCaptureMultiCamSession.The deprecated AVCaptureStillImageOutputwill not support this feature.If you are using .balanced or .quality prioritization,some of the algorithms we usemight fuse several differently exposed imagesto improve dynamic range.Photos will have great quality,but they might look different from videobeing recorded at the same time.If you need your video and photo to look exactly the same,please use .speed instead.Lastly, let's summarize what we just covered.When designing your app's experience,be aware of the decision to choosebetween quality and speed.Once you figured out the role photo quality will playin your use cases, use the appropriateprioritization levels to accomplish that.And with a minimal amount of workand sometimes no work at all,you will now be getting amazing photos with video formats.Thank you very much.[percussive music]

♪ ♪Hi, my name is Roy.I'm an engineer on the Camera Software team.Today I will be walking you through some excitingphoto quality improvementswe made with our most popular video formats,and how your applications can make use of themto deliver an even better experience.iPhone is the most popular camera in the world,and for many years, developers have beentaking advantage of its powerful camera systemsto provide a diverse set of world-class experiences,from professional photography apps to video streaming tools.Different scenarios call for different levelsof photo quality.For example, apps dedicated to taking still photoswill demand the absolute best qualitythat the cameras can provide.A social app, on the other hand,might need to apply face effect overlayson top of the video frames being streamed.And this custom rendering might be computationally expensive.In order to avoid frame drops,the developer might prefer lower resolution framesso there are fewer pixels to process per frame.This diversity in use cases calls for an easy wayto specify where you want to landon the scale of quality versus performance.Before we dive into photo quality, however,let's have a brief refresheron how photos are taken on iOS in general.We will start with an AVCaptureSession object,around which we can build our object graph.Since we are taking photos, we will use a cameraas our AVCaptureDevice.Then, an AVCaptureDeviceInput is instantiatedbased on that device,and it will provide input data to the session.

An AVCapturePhotoOutput will then be added to the graphas the recipients of the photos.

And all these elements are then connected togetherusing an AVCaptureConnection.

After the session started running,we can capture photos by calling the capturePhoto methodon the AVCapturePhotoOutput instance.Further customization can be doneusing the AVCapturePhotoSettings objectpassed to the capturePhoto method.The captured photo will be representedas an AVCapturePhoto objectthat you will receive in your delegate method.We had a very detailed discussion on these APIsin the 2016 session, Advances in iOS Photography.Please check it out if you haven't already.Now that we know how to take photos on iOS in general,let's see how high quality photos can be taken.Historically, if you wanted to capture photosof the best possible quality, you would set theisAutoStillImageStabilization Enabled propertyon your AVCapturePhotoSettings to true,and that's because still image stabilizationwas the main method for getting higher quality photos.But over the years, we have been continuouslyevolving our photo quality-enhancing algorithms.In addition to still image stabilization,we now have a far richer set of techniques to draw from,such as a variety of multi-image fusion technologies,including Smart HDR and Deep Fusion.Consequently, the nameisAutoStillImageStabilization Enabledhas become quite obsolete as a proxy for high photo quality.

To solve this problem, in iOS 13, we introducedAVCapturePhotoOutput.QualityPrio ritization.It's a very easy way to tell AVCapturePhotoOutputhow to prioritize quality in your photo captures.We haven't had a chance to talk about this important APIat previous WWDCs,so let's now take a moment to see how it works.

There are three quality prioritization levelsto choose from:speed, balanced, and quality.With speed, you are telling the frameworkthat the speed of the capture is what you care about the most,even at the expense of photo quality.If a balance needs to be struck betweenphoto quality and delivery speed,balanced should be used.Quality does the opposite of speed.It says image quality should be prioritizedfirst and foremost,while the potential slowness of the capture processcan be tolerated.Please note that the quality prioritization specifiedonly serves as a hint to the AVCapturePhotoOutput,and it doesn't dictate what algorithms to use.

Ultimately, the AVCapturePhotoOutputwill consider a variety of constraintsand choose the most appropriate algorithm for the current scene.For instance, it might choose a different methodfor a low-light situation than in a well-lit space.

That being said, we understand thatbased on different capture durations,you might want to plan your user experience differently.

So on the AVCaptureResolvedPhotoSettingsobject passed to some of the AVCapturePhotoOutputDelegatemethods, we give you a property calledphotoProcessingTimeRange that indicates how longit will take to deliver the photo to your delegate.

This, for instance, can help you decide whetheryou want to put out a spinner if the capture will take a while.Let's see how it works in code.When you are setting up your AVCapturePhotoOutput,you can specify a max quality prioritizationthat the particular capture session will require.If you choose not to do so, the default value is balanced.This only has to be set once.The importance of doing so is that depending ondifferent settings, we will configureour capture pipelines differently.For instance, if we know you will not gobeyond speed prioritization,we can construct a capture pipelinethat consumes much less memory and powerthan the one for, say, balanced prioritization.So we encourage you to choose responsiblyand only take what you need.

Before you call the capturePhoto method,you can customize the quality prioritizationfor this particular captureby setting the photoQualityPrioritizationproperty on your AVCapturePhotoSettings object.The default value is balanced.As demonstrated here, we are usingtwo different levels in two different situations.

Please note that the per-capture quality prioritizationcannot go beyond your AVCapturePhotoOutput'smax quality prioritization,otherwise you will get an exception.The performance characteristics of the three levelsare determined by the underlying algorithms we use.The mappings differ based on the kinds of format you use,and we will talk some more on the differencebetween photo and video formats momentarily.With photo formats, Speed will get you WYSIWYG photos--that is What You See is What You Get photos--which are lightly processedonly with some noise reduction applied.If Balanced is specified, we will choose from a collectionof fast fusion algorithms that produce much betterphoto quality than WYSIWYG photosat a somewhat slower capture rate.For Quality, depending on the current deviceand lux level, the framework will usesome heavy machineries such as Deep Fusionin order to provide the best possible photo quality.The photos will look great,but there is no free lunch.You pay for it by using more time.

For video formats, on the other hand,all levels will use the lightest processingto deliver photos as fast as possible.We have been talking about photo and video formatsfor a while now.Let's take a closer lookat the differences between them.By using photo formats, you are signaling to the frameworkthat taking still photos is what you care about the most.For example, if you are using an AVCaptureVideoDataOutputwith a photo format,the sample buffers you get by defaultwill only be of preview resolutions,and that's because knowing that taking photosis your top priority, we can assume that these frameswill be used for preview rather than video recordings.A good reason to choose photo formats is thatsome photo-centric features are exclusive for photo formats,such as Live Photo and ProRAW, et cetera.If that's something you want to do,then photo format is the way to go.Photo formats come with the highest resolutions available,but the frame rates are limited to 30 frames per second.

To choose photo formats, you can setyour session preset to photo.Or you can pick a format whereisHighestPhotoQualitySupported is true.The usage of video formats, on the other hand,indicates that the experiences will now center around videos.You will get resolutions more suitablefor recording and streaming, and you will be ableto use high frame rates such as 60 fps.If a format is not a photo format,then it's considered a video format.So you can select one by using a non-photo session presetor choosing a format whereisHighestPhotoQualitySupported is false.You might be wondering why we are not applyingsome of the powerful algorithms to video formats.It's not because we're lazy.We have good reasons for that.Many apps choose to use video formats because they need to doheavy custom processing,and video formats are well-suited for this purposebecause of their low overhead.If we leverage some of the aforementionedphoto enhancing techniques,we might introduce degradations in these apps' experiences.For instance, an AR app might allow usersto snap a photo of the 3D scene that they are interacting with.Running the existing fusion algorithms at this pointis likely to introduce frame dropsin the app's camera feed,interrupting its core feature.So we have been very conscious of this delicate balance betweenquality and speed, and we designed our video formatsto work responsively even under the most demanding conditions.But those compromises stop today with iOS 15.We are taking a major leap in photo qualitywith our most popular video formats.With some improved algorithms, we are now able to radicallyimprove photo quality without impacting other aspectsof your apps' experiences.With this new feature, your apps can now take amazing photos,while retaining the same flexibilityto perform sophisticated custom computation.

So how big of a quality leap are we talking about here?Let's take a look at some before-and-after comparisons.

The improvement is quite substantial.The little boy's face on the righthas much less noise,and thus looks much more natural.And we can better perceive the light coming off his hair.

The catch lights in your subjects' eyesare simply more vibrant and lively.

In this outdoor low-light situation,there is superior de-noising on her face and clothes.Lastly, the environment also looks better.The leather texture on the chair is much better preserved.

Now that you're enticed, let's take a lookat the algorithm mappings for the supported video formats.

Speed will still get you the lightly processedWYSIWYG photos.They are still the fastest way of getting a photo,and since speed is now your top priority,this fits the bill perfectly, so we didn't change it.You will not be getting any frame dropsin your video recordingsor any disruptions in your preview feed.With Balanced, however, you are now gettinga significant bump in quality, while only gettinga very slight increase in the photo's processing time.

And just like in Speed, your video recordingwill not have any frame drops.Your preview feed will not get interrupted,even when those great-looking photosare taken and processed.Finally, for Quality,we are running more expensive algorithmsto get even better qualities.This might drop frames or cause preview feed interruptions,depending on how recent your devices are.This feature will be available on all iPhoneswith support all the way back to iPhone XS.The video formats that are getting this upgradeare the most popular ones:1280x720with support for both 30 and 60 frames per second.

1920x1080, also for both 30 and 60 fps.

1920x1440, for 30 fps.And we even added support for 4K, with 30 fps.

So how can you make sure you are usingthe right formats in your code?It's very simple.In iOS 15, we are introducing a new property calledisHighPhotoQualitySupportedon the AVCaptureDevice.Format type.For formats that support this feature,this property will be true.Any format with this property being trueis guaranteed to be a video format,so you don't have to worryabout accidentally picking a photo format.

Let's say you want to get any such format.You just need to get the formats availableon your AVCaptureDevice instance.Then just select the onewith isHighPhotoQualitySupported being true.We updated our sample code AVCam to use this feature.Please check it out if you want to see a working example.It's possible to confuse the new propertyisHighPhotoQualitySupportedwith the existing isHighestPhotoQualitySupported.Like we mentioned earlier, the latter tells youwhether a format is a photo format,and it doesn't tell you whether a video formatsupports high photo quality.Now, do you need to put in any work at allto get this new feature?The answer is maybe.If you are already using AVCapturePhotoOutput,and the .balanced prioritization,then congratulations, you will automaticallyget better-looking photos on iOS 15.

If your app is using speed prioritization,by simply updating it to balanced,you will receive better photoswithout having to worry about any frame drops.

If you're still using the deprecatedAVCaptureStillImageOutput,then hopefully this will give youa big incentive to switch over.

Since now using the quality prioritizationmight introduce frame drops in your videos,we don't want to impose that new behavior on your appswithout you opting in.So we put in a link time check to make sure thatif your app is using quality prioritizationwith a video format and was compiled prior to iOS 15,then we will automatically change it to balanced.If you indeed would like to get the best quality,all you need to do is re-compile your app with the iOS 15 SDK.There are a few caveats to be aware of.

This feature currently only workswith AVCaptureSession,and not with AVCaptureMultiCamSession.

The deprecated AVCaptureStillImageOutputwill not support this feature.If you are using .balanced or .quality prioritization,some of the algorithms we usemight fuse several differently exposed imagesto improve dynamic range.Photos will have great quality,but they might look different from videobeing recorded at the same time.If you need your video and photo to look exactly the same,please use .speed instead.Lastly, let's summarize what we just covered.

When designing your app's experience,be aware of the decision to choosebetween quality and speed.Once you figured out the role photo quality will playin your use cases, use the appropriateprioritization levels to accomplish that.And with a minimal amount of workand sometimes no work at all,you will now be getting amazing photos with video formats.Thank you very much.[percussive music]

## Code Samples

