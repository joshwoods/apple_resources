# Wwdc2021 10078

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

AR Quick Look, meet Object CaptureDiscover simple ways to bring your Object Capture assets to AR Quick Look while optimizing for visual quality and file size. Explore ways you can integrate AR Quick Look and Object Capture to help create entirely new experiences.

To get the most out of this session, we recommend first watching “Advances in AR Quick Look” from WWDC19. You can also learn how to integrate Apple Pay and custom actions with AR on the web through “Shop online with AR Quick Look” from WWDC20.ResourcesAdding an Apple Pay Button or a Custom Action in AR Quick LookAR Quick Look GalleryARKitSearch developer forums for AR Quick LookHD VideoSD VideoRelated VideosWWDC21Create 3D models with Object CaptureCreate 3D workflows with USDWednesday@WWDC21WWDC20Shop online with AR Quick LookWWDC19Advances in AR Quick LookBuilding AR Experiences with Reality Composer

Discover simple ways to bring your Object Capture assets to AR Quick Look while optimizing for visual quality and file size. Explore ways you can integrate AR Quick Look and Object Capture to help create entirely new experiences.

To get the most out of this session, we recommend first watching “Advances in AR Quick Look” from WWDC19. You can also learn how to integrate Apple Pay and custom actions with AR on the web through “Shop online with AR Quick Look” from WWDC20.

Adding an Apple Pay Button or a Custom Action in AR Quick Look

AR Quick Look Gallery

ARKit

Search developer forums for AR Quick Look

HD VideoSD Video

HD Video

SD Video

Create 3D models with Object Capture

Create 3D workflows with USD

Wednesday@WWDC21

Shop online with AR Quick Look

Advances in AR Quick Look

Building AR Experiences with Reality Composer

Search this video…Hi, everyone. Thank you for tuning in.My name is Jerry,and I'm excited to present to youhow to bring Object Capture assets into AR Quick Look.As a reminder,AR Quick Look is the built-in, system-wide AR viewerfor 3D models across iOS,available in Safari, Messages, Files, and more.Apps and websites can embed 3D models,allowing people to view and interact with themin their own environment.This can be a great way to showcase a product,promote an event,or provide additional content in an immersive experience,such as this interactive demofor the Apple TV+ show "For All Mankind"where you can learn more about a cosmonaut space suit.In this presentation,I'll be talking about how to use the Object Capture APIto create 3D content for AR Quick Look,as well as some of the best practicesto keep in mind, depending on your use case.Then, I'll give a quick recapon how to integrate AR Quick Lookin your app or website.And finally,I'll show some exciting applicationsthat AR Quick Lookand Object Capture can help create.Let's get started with the 3D content creation processfor your AR Quick Look experience.Previously,if you wanted to create 3D content for AR Quick Look,it required the use of a 3D-modeling software.However, they are usually expensiveand can be difficult to approach for everyone.This year,we've introduced our new Object Capture API,built into RealityKit,as an alternative way to generate a USDZ file.It allows you to create a high-quality 3D modelby using a collection of still imagesof the real-world object.Object Capture does the heavy liftingto create a USDZ filethat can be viewed directly in AR Quick Look.You can also use Reality Composerif you want to add interactive custom behaviors to your model.For example,you can add tap triggersand camera actions to your digital scene.Together, these technologies now make it easier than everfor anyone to create an immersive AR experience.Let's see a demo of this workflow in action.I've just started an online shop selling handmade pottery,and here's one of them.I'll show you how to create a 3D model of this potand add behaviorsso that customers can preview it on their own deskwith a variety of succulentsbefore buying.But first, let's see it in action.Here is the virtual succulent placed on my desk.Let me go in for a closer look.You can see the geometric spiralsin great detail.OK, now let's preview a few other succulents.I'll tap on the green one...and it switches to the red one.Let's check out a few more.Now I'll tap again.Oh, I really like this gray one!It really matches with the pot.I'm curious how it looks compared to the real one.Wow, that looks very convincing,except the leaves have grown a lot biggersince I scanned this a few weeks ago.Now, as a friendly reminder,Object Capture does not make your virtual plants growautomatically,but if you want, you can simulate thatby changing the scale using Reality Composer.I think this would make a wonderful assetfor my online store.Now let me walk you through how I created this from scratch.The first step for mewas to take photos of the succulent pot by itself,from all angles, on a neutral background.I then used Object Capture to generate a USDZ model.Similarly, I repeated these stepsfor the each of the succulent plantsto generate three separate USDZ models.Now let me show youhow to swap different succulent plantsin the same potby adding behaviors in Reality Composer.I've already started a projectwhere I've added the pot baseand arranged the three succulentsso it properly rests inside the pot.Now let's add the behaviors.First, I added a "hide on start" behaviorso initially only the green succulent is shown.When someone taps on the green succulent,we want it to swap and show the red one.For this, I've added a tap trigger,along with "hide" and "show" actions.I then repeated these stepsso we can swap from the red to grayand gray to green.And that's it!Now we just need to export the asset,and we can view it in AR Quick Looklike we just saw earlier.Now that we've just seen how easy it is to create contentusing Object Capture,let's talk about the best practiceswhen exporting models for viewing in AR Quick Look.When exporting a USDZ file using Object Capture,it is important to determine which detail setting to use.Here's a table to illustratethe different characteristics of each setting.There's always a tradeoffbetween visual fidelity and file size.Exporting a USDZ with a small file sizeis an important consideration for AR Quick Lookbecause it results in a shorter download timefor users.No matter which detail setting you choose,Object Capture will always use the same underlying algorithmsto generate a high-quality reconstruction,but applies varying levels of mesh decimationand texture down-sampling,depending on the specified detail setting.The Reduced and Medium detail settingsoffer a great balancebetween visual fidelity and file size.Therefore, when creating content for AR Quick Look,we recommend that you first exportin both Reduced and Medium detail settings.Then you can choose which to keepafter evaluating both the visual differencesand other important considerations,which I will talk about next.As we've just seen,the Reduced detail setting will generate modelswith the smallest file size.This makes it ideal for web distribution,as it reduces the download time before your model can be viewed.This is also the recommended settingif you plan on combining multiple assetsin a single scene,perhaps to showcase a collection of succulents.These attributes make the Reduced detail settinga great choice for a majority of use cases.In certain situationswhere you notice a significant differencein visual qualitybetween the Reduced and Medium settings,it may be more suitable to use the Medium detail.This is usually the case when you have a very complex object,and would require you to take hundreds of imagesfor reconstructing it using Object Capture.Keep in mind, however,that this will generate a USDZ with a larger file size,so it's best not to combine multiple models together.For example,here I've taken photosof the succulent and pot base together.Similarly,the Medium detail setting is more suitable for appswhere there is already a local copy of the asset on the device,which eliminates download times.In summary,we recommend you evaluate both detail settingsand base your decision depending on your object and use case.Be sure to test your asset on a variety of iOS hardwareto ensure device compatibility and performance.With either detail setting,it's best to ensure quality at the source.So always take sharp imagesand avoid motion blurto generate a good, quality reconstruction.Also, make sure to provide sufficient overlapof at least 70% between adjacent photosand fill the frame with the object.For more details,I encourage you to readthe "Capturing Photographs" articleand check out the "Object Capture" session.Now that you know the best practices,let's recap how to integrate AR Quick Look.As you know, it's possible to embed AR Quick Lookin your app or website with just a few lines of code.Let's go over a quick recap of the integration.If you are embedding AR Quick Look in an app,you can use the Quick Look framework.Here we create a new QLPreviewControllerand assign "self" as its dataSource.Then we present it like a regular view controller.Of course, we also have to implementthe QLPreviewController dataSource protocol,which tells Quick Look how many previews to showand what each is going to be.Here we create an ARQL previewItemwith the URL to a local file on disk.Then we return itso the system knows to present AR Quick Look.We also provide ways to customize the experience,such as disabling content scaling in AR modeso that people can always see the object at its true scalewhen placed into the world.To disable scaling,set the allowsContentScaling property to be "false."If you are integrating the AR Quick Look experienceon your website,you can add this a-tag snippet to your HTML,replacing the URL to your own modeland image thumbnail.And be sure to include the rel="ar" attribute.This will add the AR badge icon to the thumbnail.Similarly,it's possible to disable content scaling in ARwhen embedded on web pages too.To disable content scaling,set allowsContentScaling to be 0.When embedded on websites,you can also surface Apple Payand custom actions, like preorder,directly in AR Quick Lookto allow customers to take the next stepin your order flow.So that's a quick summaryof just some of the things you can do with AR Quick Look.For more details,I encourage you to check out our previous sessions.Now let's talk about some real-world applicationswith AR Quick Look.With the announcement of Object Capture,we've now made the entire processof creating and distributing 3D contentmuch more accessible for everyone.Object Capture does the heavy liftingof generating a USDZ file.Reality Composer makes it easy to create multi-asset scenesand add interactivity to models.AR Quick Look provides a great viewing experiencefor apps and websites on iOS and is also available on macOS.Together,these technologies allow for the creationof both improved and new experiencesin a variety of fields.Let's take a look at some examples for inspiration.E-commerce is one of the most popular use casesfor AR Quick Look, and for good reason.Viewing 3D models with AR Quick Lookhelps customers visualize products in their own space,whether it's shoes, photo frames, or furniture.Now, with Object Capture,you, as a retailer, can easily create 3D modelsof your productwithout needing to have prior 3D experience,like this Fragment Design x Air Jordan 3 sneakerthat the team behind GOAT app has createdusing Object Capture.You can also take advantage of Reality Composerto add additional behaviors to enhance the viewing experience.For example, similar to the succulent demo,where I can swap between different succulent plants,you can make an app that allows customersto preview different outfits or purses.Another use case is for museums.Historical artifacts are often stored in protective cases,making it difficult to view from all angles.However, by capturing the object in 3D,viewers can see and interactwith a detailed virtual representation of the artifacton their own device.Not only does AR Quick Lookallow you to tour museums remotely,but it can also enable new forms of in-person exhibitsthat can only be experienced in a virtual world.You can also add voiceover audio to explain the 3D contentor help people who are hard of seeing,like this Zemi figure from the Metropolitan Museum of Art.Let's have a listen.It was made around 1000 ADby a sculptor of the Taino civilizationsof the Antilles archipelago in the Caribbean Sea,probably in what is now the Dominican Republic.Another great use caseof Object Capture and AR Quick Lookis in the field of education.Diagrams and videos have historically been 2D,which can sometimes make it challengingto convey 3D concepts.Now, with augmented reality, we can teach in, well, 3D.Teachers can now create engaging lessonsby leveraging 3D models generated with Object Capture.And technology can provide immediate feedbackand interactivityso students can learn and discover at their own pace.This doesn't just help teachers with lesson planning.Kids can also express their creativity in AR.For example,the Qlone app allows kids to create 3D modelsof their favorite toy or art project.It provides a convenient visual guideand automatically takes pictures as appropriateso kids can scan objects by themselves.Qlone is working on incorporatingthe Object Capture technology with seamless integrationbetween their iOS and Mac apps,making it easier for kids to share their creationswith friends and family.I've invited the kids of our Apple colleaguesto share some of the work that they've done.After capturing photos and generating 3D modelswith Object Capture,the kids then added digital interactionsusing Reality Composer.Let's check it out.The first is a dinosaur costume that comes to life with audio.We also have a nice set of plates and cups displayed.And here's a cool interactionwhere the speech bubblesbetween the puffer fish and octopiare always facing you.Those are some really cool, creative art projects.As you've just seen,anyone can now develop immersive AR experiences from scratchusing a combination of Object Capture,Reality Composer, and AR Quick Look.Object Capture creates high-quality assetsthat are ready to be viewed in AR Quick Look.Reality Composer makes it easyto combine multiple assets into a single sceneand add interactivity to models.For more information,I encourage you to visit the AR Quick Look Gallery pagefor examplesand check out the "Object Capture" session.And that's it.Thank you for watching, and enjoy the rest of WWDC.[music]

Hi, everyone. Thank you for tuning in.My name is Jerry,and I'm excited to present to youhow to bring Object Capture assets into AR Quick Look.As a reminder,AR Quick Look is the built-in, system-wide AR viewerfor 3D models across iOS,available in Safari, Messages, Files, and more.Apps and websites can embed 3D models,allowing people to view and interact with themin their own environment.

This can be a great way to showcase a product,promote an event,or provide additional content in an immersive experience,such as this interactive demofor the Apple TV+ show "For All Mankind"where you can learn more about a cosmonaut space suit.

In this presentation,I'll be talking about how to use the Object Capture APIto create 3D content for AR Quick Look,as well as some of the best practicesto keep in mind, depending on your use case.

Then, I'll give a quick recapon how to integrate AR Quick Lookin your app or website.

And finally,I'll show some exciting applicationsthat AR Quick Lookand Object Capture can help create.

Let's get started with the 3D content creation processfor your AR Quick Look experience.

Previously,if you wanted to create 3D content for AR Quick Look,it required the use of a 3D-modeling software.However, they are usually expensiveand can be difficult to approach for everyone.This year,we've introduced our new Object Capture API,built into RealityKit,as an alternative way to generate a USDZ file.It allows you to create a high-quality 3D modelby using a collection of still imagesof the real-world object.

Object Capture does the heavy liftingto create a USDZ filethat can be viewed directly in AR Quick Look.You can also use Reality Composerif you want to add interactive custom behaviors to your model.For example,you can add tap triggersand camera actions to your digital scene.

Together, these technologies now make it easier than everfor anyone to create an immersive AR experience.Let's see a demo of this workflow in action.I've just started an online shop selling handmade pottery,and here's one of them.

I'll show you how to create a 3D model of this potand add behaviorsso that customers can preview it on their own deskwith a variety of succulentsbefore buying.But first, let's see it in action.

Here is the virtual succulent placed on my desk.Let me go in for a closer look.You can see the geometric spiralsin great detail.

OK, now let's preview a few other succulents.I'll tap on the green one...and it switches to the red one.Let's check out a few more.Now I'll tap again.

Oh, I really like this gray one!It really matches with the pot.I'm curious how it looks compared to the real one.

Wow, that looks very convincing,except the leaves have grown a lot biggersince I scanned this a few weeks ago.

Now, as a friendly reminder,Object Capture does not make your virtual plants growautomatically,but if you want, you can simulate thatby changing the scale using Reality Composer.I think this would make a wonderful assetfor my online store.Now let me walk you through how I created this from scratch.

The first step for mewas to take photos of the succulent pot by itself,from all angles, on a neutral background.

I then used Object Capture to generate a USDZ model.Similarly, I repeated these stepsfor the each of the succulent plantsto generate three separate USDZ models.Now let me show youhow to swap different succulent plantsin the same potby adding behaviors in Reality Composer.

I've already started a projectwhere I've added the pot baseand arranged the three succulentsso it properly rests inside the pot.Now let's add the behaviors.First, I added a "hide on start" behaviorso initially only the green succulent is shown.When someone taps on the green succulent,we want it to swap and show the red one.For this, I've added a tap trigger,along with "hide" and "show" actions.I then repeated these stepsso we can swap from the red to grayand gray to green.And that's it!Now we just need to export the asset,and we can view it in AR Quick Looklike we just saw earlier.

Now that we've just seen how easy it is to create contentusing Object Capture,let's talk about the best practiceswhen exporting models for viewing in AR Quick Look.

When exporting a USDZ file using Object Capture,it is important to determine which detail setting to use.

Here's a table to illustratethe different characteristics of each setting.There's always a tradeoffbetween visual fidelity and file size.Exporting a USDZ with a small file sizeis an important consideration for AR Quick Lookbecause it results in a shorter download timefor users.No matter which detail setting you choose,Object Capture will always use the same underlying algorithmsto generate a high-quality reconstruction,but applies varying levels of mesh decimationand texture down-sampling,depending on the specified detail setting.The Reduced and Medium detail settingsoffer a great balancebetween visual fidelity and file size.Therefore, when creating content for AR Quick Look,we recommend that you first exportin both Reduced and Medium detail settings.Then you can choose which to keepafter evaluating both the visual differencesand other important considerations,which I will talk about next.As we've just seen,the Reduced detail setting will generate modelswith the smallest file size.This makes it ideal for web distribution,as it reduces the download time before your model can be viewed.This is also the recommended settingif you plan on combining multiple assetsin a single scene,perhaps to showcase a collection of succulents.These attributes make the Reduced detail settinga great choice for a majority of use cases.

In certain situationswhere you notice a significant differencein visual qualitybetween the Reduced and Medium settings,it may be more suitable to use the Medium detail.This is usually the case when you have a very complex object,and would require you to take hundreds of imagesfor reconstructing it using Object Capture.Keep in mind, however,that this will generate a USDZ with a larger file size,so it's best not to combine multiple models together.For example,here I've taken photosof the succulent and pot base together.Similarly,the Medium detail setting is more suitable for appswhere there is already a local copy of the asset on the device,which eliminates download times.In summary,we recommend you evaluate both detail settingsand base your decision depending on your object and use case.Be sure to test your asset on a variety of iOS hardwareto ensure device compatibility and performance.With either detail setting,it's best to ensure quality at the source.So always take sharp imagesand avoid motion blurto generate a good, quality reconstruction.Also, make sure to provide sufficient overlapof at least 70% between adjacent photosand fill the frame with the object.For more details,I encourage you to readthe "Capturing Photographs" articleand check out the "Object Capture" session.

Now that you know the best practices,let's recap how to integrate AR Quick Look.As you know, it's possible to embed AR Quick Lookin your app or website with just a few lines of code.Let's go over a quick recap of the integration.

If you are embedding AR Quick Look in an app,you can use the Quick Look framework.Here we create a new QLPreviewControllerand assign "self" as its dataSource.Then we present it like a regular view controller.

Of course, we also have to implementthe QLPreviewController dataSource protocol,which tells Quick Look how many previews to showand what each is going to be.

Here we create an ARQL previewItemwith the URL to a local file on disk.Then we return itso the system knows to present AR Quick Look.

We also provide ways to customize the experience,such as disabling content scaling in AR modeso that people can always see the object at its true scalewhen placed into the world.To disable scaling,set the allowsContentScaling property to be "false."If you are integrating the AR Quick Look experienceon your website,you can add this a-tag snippet to your HTML,replacing the URL to your own modeland image thumbnail.

And be sure to include the rel="ar" attribute.This will add the AR badge icon to the thumbnail.Similarly,it's possible to disable content scaling in ARwhen embedded on web pages too.To disable content scaling,set allowsContentScaling to be 0.

When embedded on websites,you can also surface Apple Payand custom actions, like preorder,directly in AR Quick Lookto allow customers to take the next stepin your order flow.

So that's a quick summaryof just some of the things you can do with AR Quick Look.For more details,I encourage you to check out our previous sessions.

Now let's talk about some real-world applicationswith AR Quick Look.With the announcement of Object Capture,we've now made the entire processof creating and distributing 3D contentmuch more accessible for everyone.Object Capture does the heavy liftingof generating a USDZ file.Reality Composer makes it easy to create multi-asset scenesand add interactivity to models.AR Quick Look provides a great viewing experiencefor apps and websites on iOS and is also available on macOS.

Together,these technologies allow for the creationof both improved and new experiencesin a variety of fields.Let's take a look at some examples for inspiration.E-commerce is one of the most popular use casesfor AR Quick Look, and for good reason.Viewing 3D models with AR Quick Lookhelps customers visualize products in their own space,whether it's shoes, photo frames, or furniture.Now, with Object Capture,you, as a retailer, can easily create 3D modelsof your productwithout needing to have prior 3D experience,like this Fragment Design x Air Jordan 3 sneakerthat the team behind GOAT app has createdusing Object Capture.You can also take advantage of Reality Composerto add additional behaviors to enhance the viewing experience.For example, similar to the succulent demo,where I can swap between different succulent plants,you can make an app that allows customersto preview different outfits or purses.

Another use case is for museums.Historical artifacts are often stored in protective cases,making it difficult to view from all angles.However, by capturing the object in 3D,viewers can see and interactwith a detailed virtual representation of the artifacton their own device.Not only does AR Quick Lookallow you to tour museums remotely,but it can also enable new forms of in-person exhibitsthat can only be experienced in a virtual world.

You can also add voiceover audio to explain the 3D contentor help people who are hard of seeing,like this Zemi figure from the Metropolitan Museum of Art.Let's have a listen.It was made around 1000 ADby a sculptor of the Taino civilizationsof the Antilles archipelago in the Caribbean Sea,probably in what is now the Dominican Republic.

Another great use caseof Object Capture and AR Quick Lookis in the field of education.Diagrams and videos have historically been 2D,which can sometimes make it challengingto convey 3D concepts.Now, with augmented reality, we can teach in, well, 3D.Teachers can now create engaging lessonsby leveraging 3D models generated with Object Capture.And technology can provide immediate feedbackand interactivityso students can learn and discover at their own pace.

This doesn't just help teachers with lesson planning.Kids can also express their creativity in AR.For example,the Qlone app allows kids to create 3D modelsof their favorite toy or art project.It provides a convenient visual guideand automatically takes pictures as appropriateso kids can scan objects by themselves.Qlone is working on incorporatingthe Object Capture technology with seamless integrationbetween their iOS and Mac apps,making it easier for kids to share their creationswith friends and family.I've invited the kids of our Apple colleaguesto share some of the work that they've done.After capturing photos and generating 3D modelswith Object Capture,the kids then added digital interactionsusing Reality Composer.Let's check it out.

The first is a dinosaur costume that comes to life with audio.We also have a nice set of plates and cups displayed.

And here's a cool interactionwhere the speech bubblesbetween the puffer fish and octopiare always facing you.

Those are some really cool, creative art projects.

As you've just seen,anyone can now develop immersive AR experiences from scratchusing a combination of Object Capture,Reality Composer, and AR Quick Look.Object Capture creates high-quality assetsthat are ready to be viewed in AR Quick Look.Reality Composer makes it easyto combine multiple assets into a single sceneand add interactivity to models.

For more information,I encourage you to visit the AR Quick Look Gallery pagefor examplesand check out the "Object Capture" session.And that's it.Thank you for watching, and enjoy the rest of WWDC.[music]

8:02 -Integrating AR Quick Look in your app

## Code Samples

```swift
// File: MyPreviewController.swift


func
 
presentARQuickLook
()
 {
	
let
 previewController 
=
 
QLPreviewController
()
	previewController.dataSource 
=
 
self

	present(previewController, animated: 
true
)
}


// MARK: QLPreviewControllerDataSource


func
 
previewController
(
  
_
 
controller
: 
QLPreviewController
, 
previewItemAt
 
index
: 
Int
)
 -> 
QLPreviewItem
 {
	  
let
 previewItem 
=
 
ARQuickLookPreviewItem
(fileAt: fileURL) 
// Local file URL


	  
return
 previewItem
}
```

