# Wwdc2021 10158

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Explore low-latency video encoding with VideoToolboxSupporting low latency encoders has become an important aspect of video application development process. Discover how VideoToolbox supports low-delay H.264 hardware encoding to minimize end-to-end latency and achieve new levels of performance for optimal real-time communication and high-quality video playback.ResourcesVideo ToolboxHD VideoSD VideoRelated VideosWWDC21What’s new in AVFoundationWWDC20Edit and play back HDR video with AVFoundation

Supporting low latency encoders has become an important aspect of video application development process. Discover how VideoToolbox supports low-delay H.264 hardware encoding to minimize end-to-end latency and achieve new levels of performance for optimal real-time communication and high-quality video playback.

Video Toolbox

HD VideoSD Video

HD Video

SD Video

What’s new in AVFoundation

Edit and play back HDR video with AVFoundation

Search this video…♪ ♪Hi.My name is Peikang, and I’m from Video Codingand the Processing team.Welcome to “Exploring low-latency video encodingwith Video Toolbox.”The low-latency encoding is very importantfor many video applications,especially real-time video communication apps.In this talk, I’m going to introducea new encoding mode in Video Toolboxto achieve low-latency encoding.The goal of this new mode is to optimizethe existing encoder pipeline for real-time applications.So what does a real-time video application require?We need to minimize the end-to-end latencyin the communication so that peoplewon’t be talking over each other.We need to enhance the interoperabilityby letting the video apps capableof communicating with more devices.The encoder pipeline should be efficientwhen there are more than one recipients in the call.The app needs to present the videoin its best visual quality.We need a reliable mechanism to recoverthe communication from errors introduced by network loss.The low-latency video encodingthat I’m going to talk about todaywill optimize in all these aspects.With this mode, your real-time applicationcan achieve new levels of performance.In this talk, first I’m going to givean overview of low-latency video encoding.We can have the basic idea about how we achievelow latency in the pipeline.Then I’m going to show how to use VTCompressionSession APIsto build the pipeline and encode with low-latency mode.Finally, I will talk about multiple featureswe are introducing in low-latency mode.Let me first give an overview on low-latency video encoding.Here is a brief diagram of a video encoder pipelineon Apple’s platform.The Video Toolbox takes the CVImagebufferas the input image.It asks the video encoderto perform compression algorithmssuch as H.264 to reduce the size of raw data.The output compressed data is wrappedin CMSampleBuffer,and it can be transmitted through networkfor video communication.As we may notice from the previous diagram,the end-to-end latency can be affectedby two factors: the processing timeand the network transmission time.To minimize the processing time,the low-latency mode eliminates frame reordering.A one in, one out encoding pattern is followed.Also, the rate controller in this modehas a faster adaptation in responseto the network change,so the delay caused by network congestionis minimized as well.With these two optimizations, we can already seeobvious performance improvements compared with the default mode.The low-latency encoding can reduce the delayof up to 100 milliseconds for a 720p 30fps video.Such saving can be critical for video conferencing.As we reduce the latency, we can achievea more efficient encoding pipelinefor real-time communicationslike video conferencing and live broadcasting.Also, the low-latency mode always usesa hardware-accelerated video encoderin order to save powers.Note, the supported video codec typein this mode is H.264,and we’re bringing this feature on both iOS and macOS.Next, I want to talk about how to use low-latency modewith Video Toolbox.I’m going to first recap the use of VTCompressionSessionand then show you the stepwe need to enable low-latency encoding.When we use VTCompressionSession,the first thing is to create the sessionwith VTCompressionSessionCreate API.We can optionally config the session,such as target bit rate,through VTSessionSetProperty API.If the configuration is not provided,the encoder will operate with the default behavior.After the session is created and properly configured,we can pass the CVImageBuffer to the session withVTCompressionSessionEncodeFrame call.The encoded result can be retrievedfrom the output handlerprovided during the session creation.Enabling low-latency encodingin the compression session is easy.The only change we need is in the session creation.Here is a code snippet showing how to do that.First we need a CFMutableDictionaryfor the encoderSpecification.The encoderSpecification is used to specifya particular video encoder that the session must use.And then we need to set EnableLowLatencyRateControl flagin the encoderSpecification.Finally, we need to give this encoderSpecificationto VTCompressionSessionCreate, and the compression sessionwill be operating in low-latency mode.The configuration step is the same as usual.For example, we can set the target bit ratewith AverageBitRate property.OK, we’ve covered the basics of the low-latency modewith Video Toolbox.I’d like to move on to the new featuresin this mode that can further help youdevelop a real-time video application.So far, we’ve talked about the latency benefitby using the low-latency mode.The rest of the benefits can be achievedby the features I’m going introduce.The first feature is the new profiles.We enhanced the interoperabilityby adding two new profiles to the pipeline.And we are also excited to talk about temporal scalability.This feature can be very helpful in video conferencing.You can now have a fine-grained controlover the image qualitywith max frame quantization parameter.Last, we want to improve the error resilienceby adding the support of long-term reference.Let’s talk about the new profile support.Profile defines a group of coding algorithmsthat the decoder is capable to support.In order to communicate with the receiver side,the encoded bitstream should complywith the specific profile that the decoder supports.Here in Video Toolbox, we support a bunch of profiles,such as baseline profile, main profile, and high profile.Today we added two new profiles to the family:constrained baseline profile, CBP,and constrained high profile, CHP.CBP is primarily used for low-cost applicationsand CHP, on the other hand,has more advanced algorithms for better compression ratio.You should check the decoder capabilitiesin order to know which profile should be used.To request CBP, simply set ProfileLevelsession property to ContrainedBaseLine_AutoLevel.Similarly, we can set the profile levelto ContrainedHigh_AutoLevel to use CHP.Now let’s talk about temporal scalability.We can use temporal scalability to enhance the efficiencyfor multi-party video calls.Let us consider a simple, three-partyvideo conferencing scenario.In this model, receiver "A" hasa lower bandwidth of 600kbps,and receiver B has a higher bandwidthof 1,000kbps.Normally, the sender needs to encode two sets of bitstreamsin order to meet the downlink bandwidthof each receiver side. This may not be optimal.The model can be more efficient with temporal scalability,where the sender only needs to encodeone single bitstream but can be later dividedinto two layers.Let me show you how this process works.Here is a sequence of encoded video frameswhere each of the frames uses the previous frameas predictive reference.We can pull half of the frames into another layer,and we can change the reference so that only the framesin the original layer are used for prediction.The original layer is called base layer,and the new constructed layer is called enhancement layer.The enhancement layer can be used as a supplementof the base layer in order to improve the frame rate.For receiver "A," we can send base layer framesbecause the base layer itself is decodable already.And more importantly, since the base layercontains only half of the frames,the transmitted data rate will be low.On the other hand, receiver B can enjoya smoother video since it has a sufficient bandwidthto receive base layer frames and enhancement layer frames.Let me show you the videos encodedusing temporal scalability.I’m going to play two videos, one from the base layer,and the other from the base layer togetherwith the enhancement layer.The base layer itself can be played normally,but at the same time, we may notice the videois not quite smooth.We can immediately see the differenceif we play the second video.The right video has a higher frame ratecompared with the left onebecause it contains both base layerand enhancement layer.The left video has 50% of the input frame rate,and it uses 60% of the target bit rate.These two videos only require the encoderto encode one single bitstream at one time.This will be much more power efficientwhen we are doing multi-party video conferencing.Another benefit of temporal scalabilityis error resilience.As we can see,the frames in the enhancement layerare not used for prediction,so there is no dependency on these frames.This would mean if one or more enhancement layer framesare dropped during network transmission,other frames won’t be affected.This makes the whole session more robust.The way to enable temporal scalabilityis pretty straightforward.We created a new session propertyin low-latency mode called BaseLayerFrameRateFraction.Simply set this property to 0.5,meaning half of the input frames are assignedto base layer and the rest are assignedto enhancement layer.You can check the layer informationfrom the sample buffer attachment.For base layer frames, the CMSampleAttachmentKey_IsDependedOnByOthers will be true,and otherwise it will be false.We also have the option to set the target bit ratefor each layer.Remember that we use the session propertyAverageBitRate to config the target bit rate.After the target bit rate is configured, we can setthe new BaseLayerBitRateFraction propertyto control the percentage of the target bit rateneeded for the base layer.If this property is not set,a default value of 0.6 will be used.And we recommend the base layer bit rate fractionshould range from 0.6 to 0.8.Now, let’s move to max frame quantization parameter,or max frame QP.Frame QP is used to regulate image quality and data rate.We can use low-frame QPto generate a high-quality image.The image size will be large in this case.On the other hand, we can use a high-frame QPto generate an image in low qualitybut with smaller size.In low-latency mode, the encoder adjusts frame QPusing factors such as image complexity,input frame rate, video motionin order to produce the best visual qualityunder current target bit rate constraint.So we encourage to relyon the encoder’s default behaviorfor adjusting frame QP.But in some cases where the clienthas a specific requirement for the video quality,we now let you control the max frame QPthat the encoder is allowed to use.With the max frame QP, the encoder will always choosethe frame QP that is smaller than this limit,so the client can have a fine-grained controlover the image quality.It’s worth mentioning that the regular rate controlstill works even with the max frame QP specified.If the encoder hits the max frame QP capbut is running out of bit rate budget,it will start dropping framesin order to maintain the target bit rate.One example of using this feature is to transmitscreen content video over a poor network.You can make a trade-off by sacrificing the frame ratein order to send sharp screen content images.Setting max frame QP can meet this requirement.Let’s look at the interface.You can pass the max frame QPwith the new session property MaxAllowedFrameQP.Keep in mind that the value of max frame QPmust range from 1 to 51 according to the standard.Let’s talk about the last featurewe’ve developed in low-latency mode,long-term reference.Long-term reference or LTR can be usedfor error resilience.Let’s look at this diagram showing the encoder,sender client, and the receiver clientin the pipeline.Suppose the video communicationgoes through a network with poor connection.Frame loss can happenbecause of the transmission error.When the receiver client detects a frame loss,it can request a refresh frame in order to reset the session.If the encoder gets the request, normally it will encodea key frame for the refresh purpose.But the key frame is usually quite large.A large key frame takes a longer timeto get to the receiver.Since the network condition is already poor,a large frame could compound the network congestion issue.So, can we use a predictive frameinstead of a key frame for refresh?The answer is yes,if we have frame acknowledgement.Let me show you how it works.First, we need to decide frames that require acknowledgement.We call these frames long-term reference, or LTR.This is the decision from the encoder.When the sender client transmits an LTR frame,it also needs to request acknowledgementfrom the receiver client.If the LTR frame is successfully received,an acknowledgement needs to be sent back.Once the sender client gets the acknowledgementand passes that information to the encoder,the encoder knows which LTR frameshave been received by the other side.Let’s look at the bad network situation again.When the encoder gets the refresh request,since this time, the encoder hasa bunch of acknowledged LTRs, it is ableto encode a frame that is predictedfrom one of these acknowledged LTRs.A frame that is encoded in this way is called LTR-P.Usually an LTR-P is much smaller in terms of encoded frame sizecompared to a key frame,so it is easier to transmit.Now, let’s talk about the APIs for LTR.Note that the frame acknowledgementneeds to be handled by application layer.It can be done with mechanisms such as RPSI messagein RTP Control Protocol.Here we’re only going to focus on how the encoderand the sender client communicate in this process.Once you have enabled low-latency encoding,you can enable this featureby setting EnableLTR session property.When an LTR frame is encoded,the encoder will signal a unique frame tokenin the sample attachment RequireLTRAcknowledgementToken.The sender client is responsible for reportingthe acknowledged LTR frames to the encoderthrough AcknowledgedLTRTokens frame property.Since more than one acknowledgement can comeat a time, we need to use an arrayto store these frame tokens.You can request a refresh frame at any timethrough ForceLTRRefresh frame property.Once the encoder receives this request,an LTR-P will be encoded.If there is no acknowledged LTR available,the encoder will generate a key frame in this case.All right.Now we’ve covered the new featuresin low-latency mode.We can talk about using these features together.For example, we can use temporal scalabilityand max frame quantization parameterfor a group screen sharing application.The temporal scalabilitycan efficiently generate output videosfor each recipient,and we can lower the max frame QPfor a sharper UI and text in the screen content.If the communication goes through a poor networkand a refresh frame is needed to recoverfrom the error, long-term reference can be used.And if the receiver can only decode constrained profiles,we can encode with constrained baseline profileor constrained high profile.OK.We’ve covered a few topics here.We’ve introduced a low-latency encoding mode in Video Toolbox.We’ve talked about how to use VTCompressionSession APIsto encode videos in low-latency mode.Besides the latency benefit, we also developeda bunch of new features to address the requirementsfor real-time video application.With all these improvements, I hope the low-latency modecan make your video app more amazing.Thanks for watching and have a great WWDC 2021.[upbeat music]

♪ ♪Hi.My name is Peikang, and I’m from Video Codingand the Processing team.Welcome to “Exploring low-latency video encodingwith Video Toolbox.”The low-latency encoding is very importantfor many video applications,especially real-time video communication apps.In this talk, I’m going to introducea new encoding mode in Video Toolboxto achieve low-latency encoding.The goal of this new mode is to optimizethe existing encoder pipeline for real-time applications.So what does a real-time video application require?We need to minimize the end-to-end latencyin the communication so that peoplewon’t be talking over each other.

We need to enhance the interoperabilityby letting the video apps capableof communicating with more devices.The encoder pipeline should be efficientwhen there are more than one recipients in the call.

The app needs to present the videoin its best visual quality.

We need a reliable mechanism to recoverthe communication from errors introduced by network loss.

The low-latency video encodingthat I’m going to talk about todaywill optimize in all these aspects.With this mode, your real-time applicationcan achieve new levels of performance.

In this talk, first I’m going to givean overview of low-latency video encoding.We can have the basic idea about how we achievelow latency in the pipeline.Then I’m going to show how to use VTCompressionSession APIsto build the pipeline and encode with low-latency mode.Finally, I will talk about multiple featureswe are introducing in low-latency mode.Let me first give an overview on low-latency video encoding.Here is a brief diagram of a video encoder pipelineon Apple’s platform.The Video Toolbox takes the CVImagebufferas the input image.It asks the video encoderto perform compression algorithmssuch as H.264 to reduce the size of raw data.

The output compressed data is wrappedin CMSampleBuffer,and it can be transmitted through networkfor video communication.As we may notice from the previous diagram,the end-to-end latency can be affectedby two factors: the processing timeand the network transmission time.

To minimize the processing time,the low-latency mode eliminates frame reordering.A one in, one out encoding pattern is followed.Also, the rate controller in this modehas a faster adaptation in responseto the network change,so the delay caused by network congestionis minimized as well.With these two optimizations, we can already seeobvious performance improvements compared with the default mode.The low-latency encoding can reduce the delayof up to 100 milliseconds for a 720p 30fps video.Such saving can be critical for video conferencing.

As we reduce the latency, we can achievea more efficient encoding pipelinefor real-time communicationslike video conferencing and live broadcasting.

Also, the low-latency mode always usesa hardware-accelerated video encoderin order to save powers.Note, the supported video codec typein this mode is H.264,and we’re bringing this feature on both iOS and macOS.

Next, I want to talk about how to use low-latency modewith Video Toolbox.I’m going to first recap the use of VTCompressionSessionand then show you the stepwe need to enable low-latency encoding.When we use VTCompressionSession,the first thing is to create the sessionwith VTCompressionSessionCreate API.

We can optionally config the session,such as target bit rate,through VTSessionSetProperty API.If the configuration is not provided,the encoder will operate with the default behavior.

After the session is created and properly configured,we can pass the CVImageBuffer to the session withVTCompressionSessionEncodeFrame call.The encoded result can be retrievedfrom the output handlerprovided during the session creation.

Enabling low-latency encodingin the compression session is easy.The only change we need is in the session creation.

Here is a code snippet showing how to do that.First we need a CFMutableDictionaryfor the encoderSpecification.The encoderSpecification is used to specifya particular video encoder that the session must use.And then we need to set EnableLowLatencyRateControl flagin the encoderSpecification.

Finally, we need to give this encoderSpecificationto VTCompressionSessionCreate, and the compression sessionwill be operating in low-latency mode.

The configuration step is the same as usual.For example, we can set the target bit ratewith AverageBitRate property.

OK, we’ve covered the basics of the low-latency modewith Video Toolbox.I’d like to move on to the new featuresin this mode that can further help youdevelop a real-time video application.So far, we’ve talked about the latency benefitby using the low-latency mode.The rest of the benefits can be achievedby the features I’m going introduce.

The first feature is the new profiles.We enhanced the interoperabilityby adding two new profiles to the pipeline.

And we are also excited to talk about temporal scalability.This feature can be very helpful in video conferencing.

You can now have a fine-grained controlover the image qualitywith max frame quantization parameter.Last, we want to improve the error resilienceby adding the support of long-term reference.

Let’s talk about the new profile support.Profile defines a group of coding algorithmsthat the decoder is capable to support.In order to communicate with the receiver side,the encoded bitstream should complywith the specific profile that the decoder supports.

Here in Video Toolbox, we support a bunch of profiles,such as baseline profile, main profile, and high profile.

Today we added two new profiles to the family:constrained baseline profile, CBP,and constrained high profile, CHP.

CBP is primarily used for low-cost applicationsand CHP, on the other hand,has more advanced algorithms for better compression ratio.You should check the decoder capabilitiesin order to know which profile should be used.

To request CBP, simply set ProfileLevelsession property to ContrainedBaseLine_AutoLevel.

Similarly, we can set the profile levelto ContrainedHigh_AutoLevel to use CHP.

Now let’s talk about temporal scalability.We can use temporal scalability to enhance the efficiencyfor multi-party video calls.

Let us consider a simple, three-partyvideo conferencing scenario.In this model, receiver "A" hasa lower bandwidth of 600kbps,and receiver B has a higher bandwidthof 1,000kbps.

Normally, the sender needs to encode two sets of bitstreamsin order to meet the downlink bandwidthof each receiver side. This may not be optimal.

The model can be more efficient with temporal scalability,where the sender only needs to encodeone single bitstream but can be later dividedinto two layers.

Let me show you how this process works.

Here is a sequence of encoded video frameswhere each of the frames uses the previous frameas predictive reference.

We can pull half of the frames into another layer,and we can change the reference so that only the framesin the original layer are used for prediction.

The original layer is called base layer,and the new constructed layer is called enhancement layer.

The enhancement layer can be used as a supplementof the base layer in order to improve the frame rate.

For receiver "A," we can send base layer framesbecause the base layer itself is decodable already.And more importantly, since the base layercontains only half of the frames,the transmitted data rate will be low.

On the other hand, receiver B can enjoya smoother video since it has a sufficient bandwidthto receive base layer frames and enhancement layer frames.

Let me show you the videos encodedusing temporal scalability.I’m going to play two videos, one from the base layer,and the other from the base layer togetherwith the enhancement layer.

The base layer itself can be played normally,but at the same time, we may notice the videois not quite smooth.

We can immediately see the differenceif we play the second video.The right video has a higher frame ratecompared with the left onebecause it contains both base layerand enhancement layer.

The left video has 50% of the input frame rate,and it uses 60% of the target bit rate.These two videos only require the encoderto encode one single bitstream at one time.This will be much more power efficientwhen we are doing multi-party video conferencing.

Another benefit of temporal scalabilityis error resilience.As we can see,the frames in the enhancement layerare not used for prediction,so there is no dependency on these frames.

This would mean if one or more enhancement layer framesare dropped during network transmission,other frames won’t be affected.This makes the whole session more robust.

The way to enable temporal scalabilityis pretty straightforward.We created a new session propertyin low-latency mode called BaseLayerFrameRateFraction.Simply set this property to 0.5,meaning half of the input frames are assignedto base layer and the rest are assignedto enhancement layer.

You can check the layer informationfrom the sample buffer attachment.For base layer frames, the CMSampleAttachmentKey_IsDependedOnByOthers will be true,and otherwise it will be false.

We also have the option to set the target bit ratefor each layer.Remember that we use the session propertyAverageBitRate to config the target bit rate.

After the target bit rate is configured, we can setthe new BaseLayerBitRateFraction propertyto control the percentage of the target bit rateneeded for the base layer.

If this property is not set,a default value of 0.6 will be used.And we recommend the base layer bit rate fractionshould range from 0.6 to 0.8.

Now, let’s move to max frame quantization parameter,or max frame QP.

Frame QP is used to regulate image quality and data rate.

We can use low-frame QPto generate a high-quality image.The image size will be large in this case.

On the other hand, we can use a high-frame QPto generate an image in low qualitybut with smaller size.

In low-latency mode, the encoder adjusts frame QPusing factors such as image complexity,input frame rate, video motionin order to produce the best visual qualityunder current target bit rate constraint.So we encourage to relyon the encoder’s default behaviorfor adjusting frame QP.

But in some cases where the clienthas a specific requirement for the video quality,we now let you control the max frame QPthat the encoder is allowed to use.

With the max frame QP, the encoder will always choosethe frame QP that is smaller than this limit,so the client can have a fine-grained controlover the image quality.

It’s worth mentioning that the regular rate controlstill works even with the max frame QP specified.If the encoder hits the max frame QP capbut is running out of bit rate budget,it will start dropping framesin order to maintain the target bit rate.

One example of using this feature is to transmitscreen content video over a poor network.

You can make a trade-off by sacrificing the frame ratein order to send sharp screen content images.Setting max frame QP can meet this requirement.

Let’s look at the interface.You can pass the max frame QPwith the new session property MaxAllowedFrameQP.

Keep in mind that the value of max frame QPmust range from 1 to 51 according to the standard.

Let’s talk about the last featurewe’ve developed in low-latency mode,long-term reference.

Long-term reference or LTR can be usedfor error resilience.Let’s look at this diagram showing the encoder,sender client, and the receiver clientin the pipeline.

Suppose the video communicationgoes through a network with poor connection.Frame loss can happenbecause of the transmission error.

When the receiver client detects a frame loss,it can request a refresh frame in order to reset the session.

If the encoder gets the request, normally it will encodea key frame for the refresh purpose.But the key frame is usually quite large.

A large key frame takes a longer timeto get to the receiver.Since the network condition is already poor,a large frame could compound the network congestion issue.

So, can we use a predictive frameinstead of a key frame for refresh?The answer is yes,if we have frame acknowledgement.Let me show you how it works.

First, we need to decide frames that require acknowledgement.We call these frames long-term reference, or LTR.This is the decision from the encoder.When the sender client transmits an LTR frame,it also needs to request acknowledgementfrom the receiver client.

If the LTR frame is successfully received,an acknowledgement needs to be sent back.

Once the sender client gets the acknowledgementand passes that information to the encoder,the encoder knows which LTR frameshave been received by the other side.

Let’s look at the bad network situation again.

When the encoder gets the refresh request,since this time, the encoder hasa bunch of acknowledged LTRs, it is ableto encode a frame that is predictedfrom one of these acknowledged LTRs.

A frame that is encoded in this way is called LTR-P.

Usually an LTR-P is much smaller in terms of encoded frame sizecompared to a key frame,so it is easier to transmit.Now, let’s talk about the APIs for LTR.Note that the frame acknowledgementneeds to be handled by application layer.It can be done with mechanisms such as RPSI messagein RTP Control Protocol.

Here we’re only going to focus on how the encoderand the sender client communicate in this process.

Once you have enabled low-latency encoding,you can enable this featureby setting EnableLTR session property.

When an LTR frame is encoded,the encoder will signal a unique frame tokenin the sample attachment RequireLTRAcknowledgementToken.

The sender client is responsible for reportingthe acknowledged LTR frames to the encoderthrough AcknowledgedLTRTokens frame property.Since more than one acknowledgement can comeat a time, we need to use an arrayto store these frame tokens.

You can request a refresh frame at any timethrough ForceLTRRefresh frame property.Once the encoder receives this request,an LTR-P will be encoded.If there is no acknowledged LTR available,the encoder will generate a key frame in this case.

All right.Now we’ve covered the new featuresin low-latency mode.We can talk about using these features together.

For example, we can use temporal scalabilityand max frame quantization parameterfor a group screen sharing application.The temporal scalabilitycan efficiently generate output videosfor each recipient,and we can lower the max frame QPfor a sharper UI and text in the screen content.

If the communication goes through a poor networkand a refresh frame is needed to recoverfrom the error, long-term reference can be used.And if the receiver can only decode constrained profiles,we can encode with constrained baseline profileor constrained high profile.

OK.We’ve covered a few topics here.We’ve introduced a low-latency encoding mode in Video Toolbox.

We’ve talked about how to use VTCompressionSession APIsto encode videos in low-latency mode.

Besides the latency benefit, we also developeda bunch of new features to address the requirementsfor real-time video application.With all these improvements, I hope the low-latency modecan make your video app more amazing.Thanks for watching and have a great WWDC 2021.[upbeat music]

5:03 -VTCompressionSession creation

7:35 -New profiles

## Code Samples

```swift
CFMutableDictionaryRef
 encoderSpecification =
            
CFDictionaryCreateMutable
(kCFAllocatorDefault, 
0
, 
NULL
, 
NULL
);


CFDictionarySetValue
(encoderSpecification,
                     kVTVideoEncoderSpecification_EnableLowLatencyRateControl,
                     kCFBooleanTrue)

VTCompressionSessionRef compressionSession;

OSStatus err = VTCompressionSessionCreate(kCFAllocatorDefault, 
                                          width, 
                                          height,
                                          kCMVideoCodecType_H264, 
                                          encoderSpecification,
                                          
NULL
, 
                                          
NULL
, 
                                          outputHandler, 
                                          
NULL
,
                                          &compressionSession);
```

```swift
// Request CBP


VTSessionSetProperty(compressionSession, 
                     kVTCompressionPropertyKey_ProfileLevel, 
                     kVTProfileLevel_H264_ConstrainedBaseline_AutoLevel);


// Request CHP


VTSessionSetProperty(compressionSession, 
                     kVTCompressionPropertyKey_ProfileLevel, 
                     kVTProfileLevel_H264_ConstrainedHigh_AutoLevel);
```

