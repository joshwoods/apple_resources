# Wwdc2021 10073

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Explore ARKit 5Build the next generation of augmented reality apps with ARKit 5. Explore how you can use Location Anchors in additional regions and more easily onboard people into your location-based AR experience. Learn more about Face Tracking and Motion Capture. And discover best practices for placing your AR content in the real world. We'll also show you how you can integrate App Clip Codes into your AR app for easy discovery and precise positioning of your virtual content.ResourcesARKitExplore the ARKit Developer ForumsHuman Interface Guidelines: App Clip CodesInteracting with App Clip Codes in ARTracking geographic locations in ARHD VideoSD VideoRelated VideosWWDC22Discover ARKit 6WWDC21Build light and fast App ClipsDive into RealityKit 2What's new in App ClipsWWDC20Explore ARKit 4WWDC19Bringing People into AR

Build the next generation of augmented reality apps with ARKit 5. Explore how you can use Location Anchors in additional regions and more easily onboard people into your location-based AR experience. Learn more about Face Tracking and Motion Capture. And discover best practices for placing your AR content in the real world. We'll also show you how you can integrate App Clip Codes into your AR app for easy discovery and precise positioning of your virtual content.

ARKit

Explore the ARKit Developer Forums

Human Interface Guidelines: App Clip Codes

Interacting with App Clip Codes in AR

Tracking geographic locations in AR

HD VideoSD Video

HD Video

SD Video

Discover ARKit 6

Build light and fast App Clips

Dive into RealityKit 2

What's new in App Clips

Explore ARKit 4

Bringing People into AR

Search this video…Hello.I’m David, an engineer from the ARKit team.Today Christopher and I will be sharing a broad rangeof improvements to ARKit 5.We’re excited to discuss the changes coming to iOS 15.This year, we’ve made many upgrades across the board,and we’ll be discussing multiple features.Before we do that, we want to showcasethe experiences you all have been building with LiDAR.We’ve seen a variety of LiDAR-enabled apps usingthe scene reconstruction and depth APIs: productivity,photo filter effects, entertainment,and even games that you can play in your living room.We’re really happy to see the creativityand resourcefulness shown by the ARKit community.While you’re creating these apps, we’re working hardon bringing you the world’s best AR frameworkand pushing the boundaries of what’s possible.Let’s go over the changes coming in ARKit 5.First, we’ll share some updates and best practicesfor location anchors, which enable AR experiencesin real-world outdoor locations.Next we’ll cover App Clip Codes,which are a great way to discover app clipsand also allow you to position your content in AR.We’ll highlight some improvements to face trackingusing the ultra-wide front-facing cameraon the new iPad Pro.And we’ll finish with some enhancementsto ARKit motion capture.We’ll begin with location anchors,where we’ve worked to expand region supportand provide some quality of life improvements.We’ll also recommend some best practicesfor creating applications.Location anchors were introduced last year to allow placementof AR content at a specific latitude and longitude.Their purpose is to allow creationof AR experiences tied to geographic locations.Let’s take a look at an example.This is the New Nature experiencefrom the ScavengAR application,built using the location anchors API.ScavengAR hosts AR content at real-world locationsand enables the creationof virtual public art installations and activities.It’s a good example of how location anchors canpower outdoor experiences as the world reopens.The Maps app is also introducing a new AR featurethat uses the API in iOS 15.Let’s take a look.This year Maps is adding turn-by-turn walking directionsshown in AR, using the location anchors API.They incorporate several practices we recommend.We’ll cover these later onto show how you can build great applications.Now that we’ve seen a few samples, let’s recap howlocation anchors can be used to create them, startingwith the steps required to set up a GeoTrackingConfiguration.First, verify that the feature is supported on the device.Location anchors require an A12 chip or newerand cellular and GPS support.Next, check that the feature is availableat the location before launching.Camera and location permissions must be approvedby the device owner.ARKit will prompt for permissions if needed.Last year’s presentation introducing ARKit 4and the sample project, “Tracking Geographic Locationsin AR,” cover all these topics and API usage in greater depth.We highly recommend familiarizing yourselfwith both of these sources.This code sample shows how to perform the checksfrom the previous slide.It queries for device support and then verifiesif the feature is available at the current locationbefore attempting to run a GeoTrackingConfiguration.GeoAnchors can then be added to the ARSessionlike other types of anchors.They’re specified with latitude-longitude coordinatesand, optionally, altitude.It’s important to monitor the GeoTrackingConfiguration’sstatus to see if the feature has localizedand what issues may remain to be resolved.The developer sample contains an exampleof how to implement a method to receive status updates.Checking availability near the device location is importantfor starting an application with geo tracking.We’re constantly working to support more regions.Location anchors were limited to five metro areasfor their initial release and, since then,support has expanded to more than 25 cities across the U.S.We’re also working hard to bring location anchorsto cities around the globe.For the first time, we’re excited to announcea market outside the United States.Location anchors are coming to London.We’ll continue working to add new regions over time.If you don’t live in a supported metro area,you can also begin to experiment with location anchorsthrough the use of recording and replay,which we’ll cover later on in this session.For the list of supported regions,refer to the online documentationfor ARGeoTrackingConfiguration at any time.As location anchors become available in more regions,we recognize the need to have a common visual languageto guide people.To assist with a consistent onboarding process, we’re addinga new .geoTracking goal to use with the ARCoachingOverlayView.Similar to the existing overlay for world tracking, it displaysan animation to help people achieve a good experience.Since coaching overlays are used across many different AR apps,including Maps, people will already havesome familiarity with them and know how to respond.We encourage you to include the coaching overlayto ease the learning curve for this feature.Even while using the coaching overlays, it’s still recommendedto monitor the .geoTracking status updates,which contain more detailed information on tracking state.Here’s what the .geoTracking coaching overlay looks like.The UI shows an instruction to point the device awayfrom the ground and then towards building facades.After a few seconds, tracking succeeds,and your app can place geo-tracked content.The code for displaying this animation is very similarto that used for other coaching overlays.What’s unique is the introductionof the .geoTracking goal for the overlay.Make sure to set this goal to display the correct guide.We’ve seen how the coaching overlay cancreate a uniform onboarding process.Now we’ll go over some other best practicesthat will help you create geo-tracked AR experiences.Our first recommendation is to use recordingand replay for faster development.ARKit sessions can be recorded on devices usingReality Composer, which is available on the App Store.This is especially useful for location anchorsso you don’t have to go outside as often to test.It also allows collaboration with remotely located creators.The recordings can be replayed on a device using Xcode.To avoid incompatibility issues, it’s recommendedto use the same device and iOS version.This also works for other types of ARKit applications.Replay is not specific to location anchors.Let’s walk through the process for capturing a recording.To record, open Reality Composerand tap for more options in the upper right.Then open the Developer pane and select Record AR Session.Make sure location services are enabled.Tap the red button to start and stop the recording.To replay the recording, connect the deviceto a computer running Xcode.Click Edit Scheme and set the ARKit Replay data optionfor the run configuration.Then run the application.While recording and replay can help speed up development,there are other practices we recommend for content placement.Here’s a video demonstrating these.Notice how the AR content is large and clearly visible,and information is conveyed without needing to be overlaidwith a structure in the environment.As a trade-off between development timeand placement precision,consider creating content that floats in the airrather than trying to closely overlap real-world objects.We have a few other recommendationsfor placing content.To obtain latitude and longitude coordinates to place objects,use the Apple Maps app and copy coordinateswith at least six digits of precision.The steps for this were shown in the video introducing ARKit 4,so please refer there for more details.When creating an application, it’s also importantto adjust the altitude of the content relativeto the location anchor as needed to produce a good experience.If the app requires more precise content placement,add the geo anchor when the device iswithin 50 meters of its location.If ARKit places the anchor with precise altitude,it will update the anchor’s altitude source fieldto indicate this.The CLLocation class has a method that can be usedto compute the distances in meters between two points.This can be used to verify that someoneis close to a location before adding an anchor.This concludes our session on location anchors.There are more ways to place AR contentin your apps using ARKit 5.So let me hand it off to Christopher,who will tell you more.Thank you, David.Hi, my name is Christopher,and I’m an engineer on the ARKit team.I'm excited to tell you moreabout the other great new features in ARKit 5.Let me start with App Clip Codes in ARKit.You probably remember that we introduced App Clipsat WWDC last year.An app clip is a small slice of an app which takes peoplethrough one contextual workflow of your appwithout having to install the whole app.Owing to its small file size, an app clip saves download timeand instantly takes people directlyto a specific part of the app that’s highly relevantto their context at the moment.We also introduced App Clip Codes,which are a great way for people to visually discoverand launch your app clips.No trips to the App Store necessary.This is what App Clip Codes look like.They can come in a variety of shapes and colors.As the developer, you can create a lookwhich works best for your scenario.You also decide what data to encode in the App Clip Codeand which app clip is associated with which code.All App Clip Codes contain a visual scannable patternand some, like the red, blue and orange codes shown here,also contain an NFC tag for the user’s convenience.People can scan the code with their cameraor hold the phone to the embedded NFC tagto launch your associated app clip.And now, you can also recognizeand track App Clip Codes in your AR experiences.We’ll take a look at how that’s done later in this session.But first, let’s take a look at this app clipdeveloped by Primer, where they use an App Clip Codeto launch an AR experience.Primer partnered with Cle Tileto show people what their sampleswill look like in AR with the help of App Clip Codes.Simply place your iPhone and iPadover the App Clip Code to invoke an AR experience.Now people can preview the tile swatch on their wall,all without downloading an app.That’s pretty cool, right?So, starting with iOS and iPad 14.3, you can detectand track App Clip Codes in AR experiences.Note that App Clip Code tracking requires deviceswith an A12 Bionic processor or later, like the iPhone XS.Let’s take a closer look at how to use App Clip Codes in ARKit.In iOS 14.3, we introduced a new typeof ARAnchor, an ARAppClipCodeAnchor.This anchor has three new properties: the URL embeddedin the App Clip Code, a URL decoding state,and the radius of the App Clip Code in meters.Let me explain.Each App Clip Code contains a URLthat is decoded to display the correct content.Decoding the URL is not instant.ARKit can detect the presence of an App Clip Code quickly.But it can take a little bit longer for ARKitto decode the URL, depending on the user’s distanceto the code and other factors like lighting.This is why the App Clip Code anchorcontains a .decoding state property,and it can be in one of three states.The initial state .decoding indicatesthat ARKit is still decoding the URL.As soon as ARKit has successfully decoded the URL,the state will then switch to .decoded.When decoding the URL is not possible,the state will switch to .failed instead.This can, for example, occur when someone scansan App Clip Code which is not associated with the app clip.To use App Clip Code tracking, you should first checkif it is supported on the device.Remember the App Clip Code tracking is only supportedon devices with an A12 Bionic processor or later.Then set the appClipCodeTrackingEnabled propertyon your configuration to true and run the session.To read the URL of an App Clip Code,monitor the AR sessions did update Anchors callbackand check the decoding stateof any detected App Clip Code anchors.While ARKit is decoding the App Clip Code,you might want to display a placeholder visualizationon top of the App Clip Code to givethe user instant feedback that the App Clip Codewas detected but still needs to be decoded.As mentioned before, decoding App Clip Codes canalso fail. For example, when someone points the phoneat the App Clip Code which does not belong to your app clip.We recommend that you also give feedback in that case.Once the App Clip Code has been decoded,you can finally access its URL and start displayingthe right content for this App Clip Code.For example, in case of the Primer app clipwhich you saw earlier, the URL contains informationabout which tile swatch to display.Once an App Clip Code has been decoded, the question is, whereshould you display the content associated with this code?One option is to display it directly on topof the App Clip Code anchor.However, depending on your use case, the App Clip Code itselfmight not be the best place to display the content.So, for example, you could position the contentnearby the App Clip Code with a fixed relative position.This works well when the App Clip Codeis printed on an object, say, a coffeemaker,and you want to display the virtual instructionson how to operate it on top of the machine’s buttons.Or you could combine the App Clip Code trackingwith other tracking technologies supported by ARKit.For example, image tracking.Let’s take a look at an implementation of that.The videos and code which you see next are basedon the “Interacting with App Clip Codes in AR”sample code which you can download on developer.apple.com.What you see now is a recording of the sample’s AR experience.First, I’m starting in the Camera app,scanning a sunflower seed package.Maybe I’m shopping in the gardening store,trying to decide what plant seeds to buy.iOS recognizes the App Clip Code on the packageand launches the associated Seed Shop app clip.Here, I’m scanning the App Clip Code a second time,and then the grown sunflower appears on the seed package.Note that the app clip uses image trackingof the entire seed package and places the sunflower on it.This approach makes sense in this use case,as the person’s attention is most likelyon the entire seed package and noton the smaller App Clip Code in the top right.But what if someone wanted to see the plant growin their garden?Here is what that could look like.Here we see that when the code is scannedfor the first time, it invokes an app clip download.Then when the same code is scanned againfrom within the app clip, it associates the codewith a sunflower seed box and then tapping on the lawnmakes a sunflower appear there.If instead, the app clip saw the code on the rose seed box,it would have spawned a rose plant on the lawn.Note that app clips are supposed to contain only one workflow.But the app clip can offer a button to downloadthe full Seed Shop app to experience other plantsthey could preview in their space.Remember, App Clip Code tracking alsoworks in App Clip’s parent app.Let’s take a look at the code which we needto place sunflowers on the lawn.First, you add a tapGestureRecognizer tothe view to detect taps on the screen.When the person taps on the screenyou can cast a ray into the worldand get back a resulting locationon the horizontal plane in front of their device.In our scenario, this would be the person’s lawn.You then grab the last App Clip Code URL that was decodedand add a new ARAnchor on the lawn.Lastly, you download the sunflower 3D modeland display it on the lawn.Now, let’s talk about some best practicesfor App Clip Codes in ARKit.App clips can be used in different environmentsand for different use cases.Consider whether it’s an option for youto create NFC App Clip Codes.We recommend NFC App Clip Codes for environmentswhere people can physically access the code.When using an NFC App Clip Code,use appropriate call to action text that guides peopleto tap onto the tag or, alternatively,offers an explicit affordance to scan the code.Last but not least, you need to make surethat your App Clip Codes are printedon the appropriate size for the user’s environment.For example, a restaurant menu might be printed on A4 paper,and people will be comfortablescanning a 2.5-centimeter App Clip Code on that menufrom a distance of up to 50 centimeters.A movie poster, however, is usually much largerand might have enough space fora 12-centimeter App Clip Code which people would be ableto scan with their phone from up to 2.5 meters away.Please check out our Human Interface Guidelineson App Clip Codes for more informationon recommended code sizes.So that’s how you use App Clip Codes in ARKit.If you want to dive deeper into app clips and App Clip Codes,be sure to check out “What’s new in App Clips”and “Build light and fast App Clips” sessions.Now let’s jump over to face tracking.Face tracking allows you to detect facesin the front-facing camera, overlay virtual content,and animate facial expressions in real time.Since the launch of iPhone X, ARKit has seen a tonof great apps that take advantage of face tracking.From tracking multiple faces to running face trackingin simultaneous front and back camera use case,this API has received a number of advancements over the years.Last year, we introduced face tracking on deviceswithout a TrueDepth sensor, as long as they havean A12 Bionic processor or later.And earlier this year,we launched the new iPad Pro that provides youwith an ultra wide field of view front-facing camerafor your AR face tracking experiences.Let’s take a look.Here you see the regularfront-facing camera’s field of view.And this is the new ultra-wide field of viewon the new iPad Pro.It really makes a difference, doesn’t it?Be aware that your existing apps will keep usingthe normal camera for face tracking.If you want to upgrade your user’s experienceto the ultra-wide field of view on the new iPad Pro,you have to check which video formatsare available and opt-in for the new ultra-wide format.You can do this by iterating over all supported video formatsand checking for the builtInUltraWideCamera option.You then set this format on your AR configurationand run the session.One thing to note is that the new iPad Pro’sultra-wide camera has a much larger field of viewthan the TrueDepth sensor.Therefore you will not get a capturedDepthData bufferon the ARFrame when using the ultra-wide video format.Last but not least, let’s talk about motion capture.Since its launch in 2019, motion capture has enabledrobust integration of real people in AR scenes,such as animating virtual charactersalong with being used in 2D and 3D simulation.In iOS 15, motion capture is getting even better.On devices with an Apple A14 Bionic processorlike the iPhone 12,motion capture now supports a wider range of body poses.And this requires no code changes at all.All motion capture apps on iOS 15 will benefit from this.Most notably, rotations are more accurate than ever,helping you track sports actions with much more precision.Another big improvement is that your device camera cannow track body joints from a much further distance.Also there has been a significant increasein tracking the range of limb movement.Let’s take a look at an example.Here is one of my coworkers, Ejler,tracking his workouts with the app Driven2win.The results on iOS 15 are more precise than ever.To recap, ARKit 5 brings lotsof new features and improvements.Location anchors are available in new citiesand feature a new coaching overlay.App Clip Code tracking assists in the easy discoveryand use of AR in your app clip,as well as precise positioning of your virtual content.Face tracking works with the new ultra-wide field of viewon the new iPad Pro, and motion captureadds better accuracy and larger range of motion.I’m so excited to see all the amazing experiencesyou will create with ARKit 5.[music]

Hello.I’m David, an engineer from the ARKit team.

Today Christopher and I will be sharing a broad rangeof improvements to ARKit 5.We’re excited to discuss the changes coming to iOS 15.

This year, we’ve made many upgrades across the board,and we’ll be discussing multiple features.Before we do that, we want to showcasethe experiences you all have been building with LiDAR.

We’ve seen a variety of LiDAR-enabled apps usingthe scene reconstruction and depth APIs: productivity,photo filter effects, entertainment,and even games that you can play in your living room.We’re really happy to see the creativityand resourcefulness shown by the ARKit community.While you’re creating these apps, we’re working hardon bringing you the world’s best AR frameworkand pushing the boundaries of what’s possible.Let’s go over the changes coming in ARKit 5.First, we’ll share some updates and best practicesfor location anchors, which enable AR experiencesin real-world outdoor locations.Next we’ll cover App Clip Codes,which are a great way to discover app clipsand also allow you to position your content in AR.We’ll highlight some improvements to face trackingusing the ultra-wide front-facing cameraon the new iPad Pro.And we’ll finish with some enhancementsto ARKit motion capture.We’ll begin with location anchors,where we’ve worked to expand region supportand provide some quality of life improvements.

We’ll also recommend some best practicesfor creating applications.

Location anchors were introduced last year to allow placementof AR content at a specific latitude and longitude.Their purpose is to allow creationof AR experiences tied to geographic locations.

Let’s take a look at an example.This is the New Nature experiencefrom the ScavengAR application,built using the location anchors API.ScavengAR hosts AR content at real-world locationsand enables the creationof virtual public art installations and activities.It’s a good example of how location anchors canpower outdoor experiences as the world reopens.The Maps app is also introducing a new AR featurethat uses the API in iOS 15.Let’s take a look.This year Maps is adding turn-by-turn walking directionsshown in AR, using the location anchors API.

They incorporate several practices we recommend.We’ll cover these later onto show how you can build great applications.

Now that we’ve seen a few samples, let’s recap howlocation anchors can be used to create them, startingwith the steps required to set up a GeoTrackingConfiguration.First, verify that the feature is supported on the device.Location anchors require an A12 chip or newerand cellular and GPS support.Next, check that the feature is availableat the location before launching.Camera and location permissions must be approvedby the device owner.

ARKit will prompt for permissions if needed.

Last year’s presentation introducing ARKit 4and the sample project, “Tracking Geographic Locationsin AR,” cover all these topics and API usage in greater depth.

We highly recommend familiarizing yourselfwith both of these sources.

This code sample shows how to perform the checksfrom the previous slide.It queries for device support and then verifiesif the feature is available at the current locationbefore attempting to run a GeoTrackingConfiguration.GeoAnchors can then be added to the ARSessionlike other types of anchors.They’re specified with latitude-longitude coordinatesand, optionally, altitude.It’s important to monitor the GeoTrackingConfiguration’sstatus to see if the feature has localizedand what issues may remain to be resolved.

The developer sample contains an exampleof how to implement a method to receive status updates.

Checking availability near the device location is importantfor starting an application with geo tracking.We’re constantly working to support more regions.Location anchors were limited to five metro areasfor their initial release and, since then,support has expanded to more than 25 cities across the U.S.We’re also working hard to bring location anchorsto cities around the globe.For the first time, we’re excited to announcea market outside the United States.

Location anchors are coming to London.

We’ll continue working to add new regions over time.

If you don’t live in a supported metro area,you can also begin to experiment with location anchorsthrough the use of recording and replay,which we’ll cover later on in this session.

For the list of supported regions,refer to the online documentationfor ARGeoTrackingConfiguration at any time.

As location anchors become available in more regions,we recognize the need to have a common visual languageto guide people.To assist with a consistent onboarding process, we’re addinga new .geoTracking goal to use with the ARCoachingOverlayView.

Similar to the existing overlay for world tracking, it displaysan animation to help people achieve a good experience.

Since coaching overlays are used across many different AR apps,including Maps, people will already havesome familiarity with them and know how to respond.We encourage you to include the coaching overlayto ease the learning curve for this feature.

Even while using the coaching overlays, it’s still recommendedto monitor the .geoTracking status updates,which contain more detailed information on tracking state.

Here’s what the .geoTracking coaching overlay looks like.The UI shows an instruction to point the device awayfrom the ground and then towards building facades.

After a few seconds, tracking succeeds,and your app can place geo-tracked content.The code for displaying this animation is very similarto that used for other coaching overlays.What’s unique is the introductionof the .geoTracking goal for the overlay.

Make sure to set this goal to display the correct guide.We’ve seen how the coaching overlay cancreate a uniform onboarding process.Now we’ll go over some other best practicesthat will help you create geo-tracked AR experiences.Our first recommendation is to use recordingand replay for faster development.

ARKit sessions can be recorded on devices usingReality Composer, which is available on the App Store.This is especially useful for location anchorsso you don’t have to go outside as often to test.It also allows collaboration with remotely located creators.The recordings can be replayed on a device using Xcode.

To avoid incompatibility issues, it’s recommendedto use the same device and iOS version.

This also works for other types of ARKit applications.Replay is not specific to location anchors.

Let’s walk through the process for capturing a recording.

To record, open Reality Composerand tap for more options in the upper right.Then open the Developer pane and select Record AR Session.

Make sure location services are enabled.Tap the red button to start and stop the recording.To replay the recording, connect the deviceto a computer running Xcode.Click Edit Scheme and set the ARKit Replay data optionfor the run configuration.

Then run the application.While recording and replay can help speed up development,there are other practices we recommend for content placement.Here’s a video demonstrating these.Notice how the AR content is large and clearly visible,and information is conveyed without needing to be overlaidwith a structure in the environment.As a trade-off between development timeand placement precision,consider creating content that floats in the airrather than trying to closely overlap real-world objects.We have a few other recommendationsfor placing content.

To obtain latitude and longitude coordinates to place objects,use the Apple Maps app and copy coordinateswith at least six digits of precision.The steps for this were shown in the video introducing ARKit 4,so please refer there for more details.When creating an application, it’s also importantto adjust the altitude of the content relativeto the location anchor as needed to produce a good experience.If the app requires more precise content placement,add the geo anchor when the device iswithin 50 meters of its location.

If ARKit places the anchor with precise altitude,it will update the anchor’s altitude source fieldto indicate this.The CLLocation class has a method that can be usedto compute the distances in meters between two points.This can be used to verify that someoneis close to a location before adding an anchor.This concludes our session on location anchors.There are more ways to place AR contentin your apps using ARKit 5.So let me hand it off to Christopher,who will tell you more.Thank you, David.Hi, my name is Christopher,and I’m an engineer on the ARKit team.

I'm excited to tell you moreabout the other great new features in ARKit 5.Let me start with App Clip Codes in ARKit.You probably remember that we introduced App Clipsat WWDC last year.An app clip is a small slice of an app which takes peoplethrough one contextual workflow of your appwithout having to install the whole app.Owing to its small file size, an app clip saves download timeand instantly takes people directlyto a specific part of the app that’s highly relevantto their context at the moment.We also introduced App Clip Codes,which are a great way for people to visually discoverand launch your app clips.No trips to the App Store necessary.This is what App Clip Codes look like.

They can come in a variety of shapes and colors.As the developer, you can create a lookwhich works best for your scenario.You also decide what data to encode in the App Clip Codeand which app clip is associated with which code.All App Clip Codes contain a visual scannable patternand some, like the red, blue and orange codes shown here,also contain an NFC tag for the user’s convenience.People can scan the code with their cameraor hold the phone to the embedded NFC tagto launch your associated app clip.And now, you can also recognizeand track App Clip Codes in your AR experiences.

We’ll take a look at how that’s done later in this session.But first, let’s take a look at this app clipdeveloped by Primer, where they use an App Clip Codeto launch an AR experience.Primer partnered with Cle Tileto show people what their sampleswill look like in AR with the help of App Clip Codes.Simply place your iPhone and iPadover the App Clip Code to invoke an AR experience.Now people can preview the tile swatch on their wall,all without downloading an app.

That’s pretty cool, right?So, starting with iOS and iPad 14.3, you can detectand track App Clip Codes in AR experiences.

Note that App Clip Code tracking requires deviceswith an A12 Bionic processor or later, like the iPhone XS.Let’s take a closer look at how to use App Clip Codes in ARKit.In iOS 14.3, we introduced a new typeof ARAnchor, an ARAppClipCodeAnchor.This anchor has three new properties: the URL embeddedin the App Clip Code, a URL decoding state,and the radius of the App Clip Code in meters.Let me explain.Each App Clip Code contains a URLthat is decoded to display the correct content.Decoding the URL is not instant.ARKit can detect the presence of an App Clip Code quickly.

But it can take a little bit longer for ARKitto decode the URL, depending on the user’s distanceto the code and other factors like lighting.

This is why the App Clip Code anchorcontains a .decoding state property,and it can be in one of three states.

The initial state .decoding indicatesthat ARKit is still decoding the URL.As soon as ARKit has successfully decoded the URL,the state will then switch to .decoded.When decoding the URL is not possible,the state will switch to .failed instead.

This can, for example, occur when someone scansan App Clip Code which is not associated with the app clip.

To use App Clip Code tracking, you should first checkif it is supported on the device.Remember the App Clip Code tracking is only supportedon devices with an A12 Bionic processor or later.

Then set the appClipCodeTrackingEnabled propertyon your configuration to true and run the session.

To read the URL of an App Clip Code,monitor the AR sessions did update Anchors callbackand check the decoding stateof any detected App Clip Code anchors.

While ARKit is decoding the App Clip Code,you might want to display a placeholder visualizationon top of the App Clip Code to givethe user instant feedback that the App Clip Codewas detected but still needs to be decoded.As mentioned before, decoding App Clip Codes canalso fail. For example, when someone points the phoneat the App Clip Code which does not belong to your app clip.We recommend that you also give feedback in that case.Once the App Clip Code has been decoded,you can finally access its URL and start displayingthe right content for this App Clip Code.

For example, in case of the Primer app clipwhich you saw earlier, the URL contains informationabout which tile swatch to display.Once an App Clip Code has been decoded, the question is, whereshould you display the content associated with this code?One option is to display it directly on topof the App Clip Code anchor.

However, depending on your use case, the App Clip Code itselfmight not be the best place to display the content.

So, for example, you could position the contentnearby the App Clip Code with a fixed relative position.

This works well when the App Clip Codeis printed on an object, say, a coffeemaker,and you want to display the virtual instructionson how to operate it on top of the machine’s buttons.

Or you could combine the App Clip Code trackingwith other tracking technologies supported by ARKit.For example, image tracking.Let’s take a look at an implementation of that.The videos and code which you see next are basedon the “Interacting with App Clip Codes in AR”sample code which you can download on developer.apple.com.What you see now is a recording of the sample’s AR experience.First, I’m starting in the Camera app,scanning a sunflower seed package.Maybe I’m shopping in the gardening store,trying to decide what plant seeds to buy.iOS recognizes the App Clip Code on the packageand launches the associated Seed Shop app clip.Here, I’m scanning the App Clip Code a second time,and then the grown sunflower appears on the seed package.Note that the app clip uses image trackingof the entire seed package and places the sunflower on it.This approach makes sense in this use case,as the person’s attention is most likelyon the entire seed package and noton the smaller App Clip Code in the top right.

But what if someone wanted to see the plant growin their garden?Here is what that could look like.Here we see that when the code is scannedfor the first time, it invokes an app clip download.

Then when the same code is scanned againfrom within the app clip, it associates the codewith a sunflower seed box and then tapping on the lawnmakes a sunflower appear there.

If instead, the app clip saw the code on the rose seed box,it would have spawned a rose plant on the lawn.

Note that app clips are supposed to contain only one workflow.

But the app clip can offer a button to downloadthe full Seed Shop app to experience other plantsthey could preview in their space.Remember, App Clip Code tracking alsoworks in App Clip’s parent app.Let’s take a look at the code which we needto place sunflowers on the lawn.First, you add a tapGestureRecognizer tothe view to detect taps on the screen.When the person taps on the screenyou can cast a ray into the worldand get back a resulting locationon the horizontal plane in front of their device.In our scenario, this would be the person’s lawn.

You then grab the last App Clip Code URL that was decodedand add a new ARAnchor on the lawn.

Lastly, you download the sunflower 3D modeland display it on the lawn.

Now, let’s talk about some best practicesfor App Clip Codes in ARKit.App clips can be used in different environmentsand for different use cases.Consider whether it’s an option for youto create NFC App Clip Codes.

We recommend NFC App Clip Codes for environmentswhere people can physically access the code.When using an NFC App Clip Code,use appropriate call to action text that guides peopleto tap onto the tag or, alternatively,offers an explicit affordance to scan the code.

Last but not least, you need to make surethat your App Clip Codes are printedon the appropriate size for the user’s environment.For example, a restaurant menu might be printed on A4 paper,and people will be comfortablescanning a 2.5-centimeter App Clip Code on that menufrom a distance of up to 50 centimeters.A movie poster, however, is usually much largerand might have enough space fora 12-centimeter App Clip Code which people would be ableto scan with their phone from up to 2.5 meters away.

Please check out our Human Interface Guidelineson App Clip Codes for more informationon recommended code sizes.

So that’s how you use App Clip Codes in ARKit.If you want to dive deeper into app clips and App Clip Codes,be sure to check out “What’s new in App Clips”and “Build light and fast App Clips” sessions.Now let’s jump over to face tracking.

Face tracking allows you to detect facesin the front-facing camera, overlay virtual content,and animate facial expressions in real time.

Since the launch of iPhone X, ARKit has seen a tonof great apps that take advantage of face tracking.From tracking multiple faces to running face trackingin simultaneous front and back camera use case,this API has received a number of advancements over the years.

Last year, we introduced face tracking on deviceswithout a TrueDepth sensor, as long as they havean A12 Bionic processor or later.

And earlier this year,we launched the new iPad Pro that provides youwith an ultra wide field of view front-facing camerafor your AR face tracking experiences.Let’s take a look.Here you see the regularfront-facing camera’s field of view.And this is the new ultra-wide field of viewon the new iPad Pro.It really makes a difference, doesn’t it?Be aware that your existing apps will keep usingthe normal camera for face tracking.If you want to upgrade your user’s experienceto the ultra-wide field of view on the new iPad Pro,you have to check which video formatsare available and opt-in for the new ultra-wide format.You can do this by iterating over all supported video formatsand checking for the builtInUltraWideCamera option.You then set this format on your AR configurationand run the session.One thing to note is that the new iPad Pro’sultra-wide camera has a much larger field of viewthan the TrueDepth sensor.

Therefore you will not get a capturedDepthData bufferon the ARFrame when using the ultra-wide video format.

Last but not least, let’s talk about motion capture.Since its launch in 2019, motion capture has enabledrobust integration of real people in AR scenes,such as animating virtual charactersalong with being used in 2D and 3D simulation.

In iOS 15, motion capture is getting even better.On devices with an Apple A14 Bionic processorlike the iPhone 12,motion capture now supports a wider range of body poses.And this requires no code changes at all.All motion capture apps on iOS 15 will benefit from this.

Most notably, rotations are more accurate than ever,helping you track sports actions with much more precision.Another big improvement is that your device camera cannow track body joints from a much further distance.

Also there has been a significant increasein tracking the range of limb movement.Let’s take a look at an example.Here is one of my coworkers, Ejler,tracking his workouts with the app Driven2win.The results on iOS 15 are more precise than ever.To recap, ARKit 5 brings lotsof new features and improvements.

Location anchors are available in new citiesand feature a new coaching overlay.

App Clip Code tracking assists in the easy discoveryand use of AR in your app clip,as well as precise positioning of your virtual content.Face tracking works with the new ultra-wide field of viewon the new iPad Pro, and motion captureadds better accuracy and larger range of motion.I’m so excited to see all the amazing experiencesyou will create with ARKit 5.

[music]

3:29 -Geo Tracking Recap I

3:42 -Geo Tracking Recap II

6:02 -Geo Tracking Coaching Overlay

8:53 -GeoTracking Distance Method

12:16 -App Clip Code: check device support

12:34 -Accessing the URL of an App Clip Code

15:34 -Adding a gesture recognizer

15:45 -Tap to place the sunflower

18:33 -Checking for supported video formats for face tracking

## Code Samples

```swift
// Check device support for geo-tracking


guard
 
ARGeoTrackingConfiguration
.isSupported 
else
 {
    
// Geo-tracking not supported on this device

    
return

}


// Check current location is supported for geo-tracking


ARGeoTrackingConfiguration
.checkAvailability { (available, error) 
in

    
guard
 available 
else
 {
        
// Geo-tracking is not available at this location

        
return

    }

    
// Run ARSession

    
let
 arView 
=
 
ARView
()
    arView.session.run(
ARGeoTrackingConfiguration
())
}
```

```swift
// Create Location Anchor and add to session


let
 coordinate 
=
 
CLLocationCoordinate2D
(latitude: 
37.795313
, longitude: 
-
122.393792
)

let
 geoAnchor 
=
 
ARGeoAnchor
(name: “
Ferry
 
Building
”, coordinate: coordinate)
arView.session.add(anchor: geoAnchor)


// Monitor geo-tracking status updates


func
 
session
(
_
 
session
: 
ARSession
, 
didChange
 
geoTrackingStatus
: 
ARGeoTrackingStatus
)
 {
    
…

}
```

```swift
// Declare coaching view


let
 coachingOverlay 
=
 
ARCoachingOverlayView
()


// Set up coaching view (assuming ARView already exists)

coachingOverlay.session 
=
 
self
.arView.session
coachingOverlay.delegate 
=
 
self

coachingOverlay.goal 
=
 .geoTracking
     
coachingOverlay.translatesAutoresizingMaskIntoConstraints 
=
 
false


self
.arView.addSubview(coachingOverlay)


NSLayoutConstraint
.activate([
    coachingOverlay.centerXAnchor.constraint(equalTo: view.centerXAnchor),
    coachingOverlay.centerYAnchor.constraint(equalTo: view.centerYAnchor),
    coachingOverlay.widthAnchor.constraint(equalTo: view.widthAnchor),
    coachingOverlay.heightAnchor.constraint(equalTo: view.heightAnchor),
])
```

```swift
// Method to compute distance (in meters) between points


func
 
distance
(
from
 
location
: 
CLLocation
)
 -> 
CLLocationDistance
```

```swift
func
 
viewDidLoad
()
 {
    
// Check device support for app clip code tracking

    
guard
 
ARWorldTrackingConfiguration
.supportsAppClipCodeTracking 
else
 { 
return
 }
    
    
let
 worldConfig 
=
 
ARWorldTrackingConfiguration
()
    worldConfig.appClipCodeTrackingEnabled 
=
 
true

    arSession.run(worldConfig)
}
```

```swift
/// Accessing the URL of an App Clip Code 


override
 
func
 
session
(
_
 
session
: 
ARSession
, 
didUpdate
 
anchors
: [
ARAnchor
])
 {
    
for
 anchor 
in
 anchors {
        
guard
 
let
 appClipCodeAnchor 
=
 anchor 
as?
 
ARAppClipCodeAnchor
, appClipCodeAnchor.isTracked 
else
 { 
return
 }
        
        
switch
 appClipCodeAnchor.urlDecodingState {
        
case
 .decoding:
            displayPlaceholderVisualizationOnTopOf(anchor: appClipCodeAnchor)
        
case
 .failed:
            displayNoURLErrorMessageOnTopOf(anchor: appClipCodeAnchor)
        
case
 .decoded:
            
let
 url 
=
 appClipCodeAnchor.url
!

            
let
 anchorEntity 
=
 
AnchorEntity
(anchor: appClipCodeAnchor)
            arView.scene.addAnchor(anchorEntity)
            
let
 visualization 
=
 
AppClipCodeVisualization
(url: url, radius: appClipCodeAnchor.radius)
            anchorEntity.addChild(visualization)
          }
    }
}
```

```swift
/// Adding a gesture recognizer for user interaction


func
 
viewDidLoad
()
 {
    initializeARView()
    initializeCoachingOverlays()
        
    
// Place sunflower on the ground when the user taps the screen

    
let
 tapGestureRecognizer 
=
 
UITapGestureRecognizer
(
     target: 
self
,
     action: #selector(handleTap(recognizer:)))
    arView.addGestureRecognizer(tapGestureRecognizer)
}
```

```swift
func
 
handleTap
(
recognizer
: 
UITapGestureRecognizer
)
 {
    
let
 location 
=
 recognizer.location(in: arView)
    
// Attempt to find a 3D location on a horizontal

    
// surface underneath the user's touch location.

    
let
 results 
=
 arView.raycast(
      from: location, 
      allowing: .estimatedPlane,
      alignment: .horizontal)
    
guard
 
let
 firstResult 
=
 results.first 
else
 { 
return
 }
    
// Fetch the last decoded app clip code URL

    
guard
 
let
 appClipCodeURL 
=
 decodedURLs.last 
else
 { 
return
 }
    
// Add an ARAnchor & AnchorEntity at the touch location

    
let
 anchor 
=
 
ARAnchor
(transform: firstResult.worldTransform)
    arView.session.add(anchor: anchor)
    
let
 anchorEntity 
=
 
AnchorEntity
(anchor: anchor)
    arView.scene.addAnchor(anchorEntity)    
    
// Download the 3D model associated with this app clip code.

    downloadAndDisplay(appClipCodeURL, on: anchorEntity)
}
```

```swift
// Check if the ultra wide video format is available.


// If so, set it on a face tracking configuration & run the session with that.



let
 config 
=
 
ARFaceTrackingConfiguration
()

for
 videoFormat 
in
 
ARFaceTrackingConfiguration
.supportedVideoFormats {
    
if
 videoFormat.captureDeviceType 
==
 .builtInUltraWideCamera {
        config.videoFormat 
=
 videoFormat
        
break

    }
}
session.run(config)
```

