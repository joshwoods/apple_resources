# Wwdc2021 10153

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Create image processing apps powered by Apple siliconDiscover how to optimize your image processing app for Apple silicon. Explore how to take advantage of Metal render command encoders, tile shading, unified memory architecture, and memoryless attachments. We'll show you how to use Apple's unique tile based deferred renderer architecture to create power efficient apps with low memory footprint, and take you through best practices when migrating your compute-based apps from discrete GPUs to Apple silicon.ResourcesDebugging the shaders within a draw command or compute dispatchMetalMetal Feature Set TablesMetal Shading Language SpecificationHD VideoSD VideoRelated VideosWWDC20Harness Apple GPUs with MetalOptimize Metal Performance for Apple silicon Macs

Discover how to optimize your image processing app for Apple silicon. Explore how to take advantage of Metal render command encoders, tile shading, unified memory architecture, and memoryless attachments. We'll show you how to use Apple's unique tile based deferred renderer architecture to create power efficient apps with low memory footprint, and take you through best practices when migrating your compute-based apps from discrete GPUs to Apple silicon.

Debugging the shaders within a draw command or compute dispatch

Metal

Metal Feature Set Tables

Metal Shading Language Specification

HD VideoSD Video

HD Video

SD Video

Harness Apple GPUs with Metal

Optimize Metal Performance for Apple silicon Macs

Search this video…♪ Bass music playing ♪♪Eugene Zhidkov: Hi and welcome to WWDC.My name is Eugene Zhidkov. I am from GPU software.And together, with Harsh Patil from Mac system architecture,we'll show you how to create image-processing applicationspowered by Metal on Apple silicon.First, I will focuson the best practices and lessons learned,optimizing image-processing applications for M1based on developer engagements we have had over the last year.And then Harsh will give you a step-by-step guideto how you can redesign your image-processing pipelinefor optimal performance on Apple silicon.So let’s jump right in!To start, let’s briefly revisit Apple systemon-the-chip architecture and its benefits.Many image-processing and video-editing appsare designed with discrete GPUs in mind.So it’s important to highlight what’s so differentabout Apple GPUs.First, all Apple chips use Unified Memory Architecture.All blocks --such as CPU, GPU, Neural and Media engines --have access to the same system memoryusing unified memory interface.And second, our GPUs are Tile Based Deferred Renderers,or TBDRs.TBDRs have two main phases: tiling,where whole render surfaces split into tilesand processed geometry is then handled independently;and rendering,where all of the pixels will be processed for each tile.So in order to be most efficient on Apple silicon,your image-processing appshould start leveraging unified memory --to avoid any copies your pipeline used to have --and TBDR architecture by exploiting tile memoryand local image block.To learn more about how Apple TBDR works at low leveland how to target our shader core,please watch these sessions from the last year.And now, let’s talk aboutthe exact things we are going to do to optimizeimage-processing compute workloadsfor Apple silicon.Last year,we’ve been working closely with many great developerson their image pipeline transitions.We picked the six most rewarding tips to share.First, we’ll discuss how to avoidunnecessary memory copies or blits.This is really importantgiven we are now working with images up to 8K.Then, we wanted to highlight the benefits of usingrender pipeline and texturesinstead of using compute on buffersand how you could do thatin your own image-processing pipeline.Once we have the render and textures pathsup and running,we wanted to show you the importance of properload/store actions and memoryless attachments.This will help you getting the most out of the tile memory.Then, we’ll talk how to best approach Uber-shaderswith its dynamic control flowand also how to leverage smaller data types --such as short and half --to improve performance and efficiency.And we’ll finish with important adviceabout texture formats to get the best throughput.All right.So let’s get started with one of the most rewarding tips:avoiding unneeded blits on Apple silicon.Most image-processing apps are designed around discrete GPUs.With discrete GPUs,you have separate system memory and video memory.To make the frame image visible or resident to the GPU,explicit copy is required.Moreover, it is usually required twice;to upload the data for the GPU to process it,and to pull it back.Let’s consider we are decoding an 8K video, processing it,and saving it to disk.So this is a CPU thread, decoding, in this case.That’s where we need to copy the decoded frame to GPU VRAM.And here is GPU timeline,where all the effects and filters are applied.Let’s take it one step further and let’s recallwe need to save the results to disk, right?So we must also consider bringing the processed frameback to system memoryand the actual encoding of the frame.So, these are known as "copy" or "blit gaps",and advanced image-processing applicationshad to do deep pipelining and other smart thingsto fill them in.Well, the good news is that on Apple GPUs,blitting for the sake of residenceis no longer needed.Since memory is shared,both the CPU and GPU can access it directly.So please add a simple check to detect if you are runningon unified memory system and avoid unnecessary copies.It will save you memory, time,and is an absolute first step to do.So this is where we land on Unified Memory Architecturewith the blits removed.By removing the blits, we completely avoid copy gapsand can start processing immediately.This also gives better CPU and GPU pipeliningwith less hassle.Let’s make sure you implement unified memory pathwith no copies involved.If you just leave blit copies exactly as it wason the discrete GPU,you’ll pay with system memory bandwidth,less GPU time for actual processing,and potential scheduling overhead.Not to mention we no longer need separate VRAM image allocated.GPU frame capture can help you with spotting large blits.Please inspect your application blitsand make sure you only do the copies required.Now, let’s talk about how exactly we should startleveraging Apple GPU TBDR architecturefor image processing.Most image-processing applications operateon image buffers by dispatching series of compute kernels.When you dispatch a compute kernelin default serial mode,Metal guarantees that all subsequent dispatchessee all the memory writes.This guarantee implies memory coherency for all shader cores,so every memory write is made visible to all other coresby the time the next dispatch starts.This also means memory traffic could be really high;the whole image has to be read and written to.With M1, Apple GPUs enable tile dispatches on MacOS.In contrast to regular compute, they operate in tile memorywith tile-only sync points.Some filters -- like convolutions --cannot be mapped to the tile paradigm,but many other filters can!Deferring system memory flush until the encoder end pointprovides solid efficiency gains.You can execute more useful GPU workwhen not limited by system memory bandwidth.To take it even further,let’s notice that many per-pixel operationsdon’t require access to neighboring pixels,so tile sync point is not necessary.This maps really well to fragment functions.Fragment functions can be executedwithout implicit tile sync,requiring sync only at the encoder boundaryor when tile kernels are dispatched seriallyafter the fragment kernels.We now learned that Apple GPUs enable fragment functionsand tile kernels for more efficient image processing.Let’s see how we could use that.We do that by converting regular compute dispatcheson buffers to render command encoder on textures.As we just discussed, rule of thumb is the following.Per-pixel operations with no interpixel dependencyshould be implemented using fragment functions.Any filter with threadgroup scoped operationsshould be implemented with tile shading,since neighbor pixels access within a tile is required.Scatter-gather and convolution filterscannot be mapped to tile paradigmsince they require random access,so these should still remain compute dispatches.Render command encoder also enablesa unique Apple GPU feature:lossless bandwidth compression for textures and render targets.This is a really great bandwidth saver,especially for an image-processing pipeline,so let’s see how we should use it.Well, speaking of enabling lossless compression,it’s actually easier to say what you should not do.First, already-compressed texture formatscannot benefit from lossless.Second, there are three particular texture flagswhich cannot work with this compression,so make sure you don’t set them just by an accident.And third, linear textures -- or backed by an MTLBuffer --are not allowed as well.Some special treatment is also requiredfor nonprivate textures;make sure to call optimizeContentsForGPUAccessto stay on the fastest path.GPU frame capture Summary pane now shows youlossless compression warningsand highlights the reasons why the texture has opted out.In this example, PixelFormatView flag was set.In many cases,developers are setting these flags unintentionally.Don’t set PixelFormatViewif all you need is components swizzle or sRGB conversion.All right, we have the render and textures pathup and running.Now, let’s make sure we properly use tile memory.Tile memory TBDR concepts --such as load/store actions and memoryless attachments --are totally new to the desktop world.So let’s make sure we use them properly.Let’s start with load/store actions!As we already know,the whole render target is split into the tiles.Load/store are per-tile bulk actionsguaranteed to take the most optimal paththrough memory hierarchy.They are executed at the beginning of the render pass --where we tell the GPU how to initialize the tile memory --and at the end of the pass to inform the GPUwhat attachments need to be written back.The key thing here is to avoid loading what we don’t need.If we are overwriting the whole image,or the resource is temporary,set load action to LoadActionDontCare.With render encoder,you no longer need to clear your output or temporary data,as you probably did before with dedicated compute passor fillBuffer call.By setting LoadActionClear,you can efficiently specify the clear value.And the same goes for the store action.Make sure to only store the data you will later need --like the main attachment --and don’t store anything temporary.Besides explicit load and store actions,Apple GPUs saves your memory footprintwith memoryless attachments.We can explicitly define an attachmentas having memoryless storage mode.This enables tile-only memory allocation,meaning that your resource will persistfor each and every tile only within encoder lifetime.This can greatly reduce your memory footprint,especially for 6K/8K images,where every frame takes hundreds of megabytes.Let’s see how this all can be done in code.We start by creating the textureDescriptorand then create the outputTexture.We then create a temporary texture.Notice that I’ve marked it memoryless,as we don’t want any storage here.Then we create the render passby first describing what the attachments areand then what are the load/store actions.We don’t care about loading the outputsince it is fully overwritten, but we need to store it.As for the temporary texture, we don’t load but clear it,and we don’t need to store it either.Finally, we create our renderPass from the descriptor.That’s it.So we are using unified memory,moved our image-processing pipelineto render command encoder,and are properly leveraging tile memory.Now, let’s talk about uber-shaders.Uber-shaders, or uber-kernels,is a pretty popular way to make developers' life easier.Host code sets up the control structure,and shader just loops through a series of if/else statements,for example, if tone mapping is enabledor if the input is in HDR or SDR formats.This approach is also known as "ubers-shader"and is really good at bringingtotal number of pipeline state objects down.However, it has drawbacks.The main one is increased register pressureto keep up with more complex control flow.Using more registers can easily limit maximum occupancyyour shader is running at.Consider a simple kernel where we pass in the control struct.We use flags inside the struct to control what we do.We have two features here:if the input is in HDR and if tonemapping is enabled.All look good, right?Well, here is what happens on the GPU.Since we cannot deduce anything at compile time,we have to assume we could take both paths --HDR and non-HDR --and then combine based on the flag.Same goes for tone mapping.We evaluate it and then mask it in or out,based on the input flag.The problem here is registers.Every control flow path needs live registers.This is where uber-shaders are not so good.As you recall, registers used by the kerneldefine maximum occupancy the shader could run.That happens because registers file is shared by all simdlaneson the shader core.If we could only run what’s only needed,that would enable higher simdgroup concurrencyand GPU utilization.Let’s talk how to fix this.Metal API has the right tool for the job,and it's called "function_constants".We define both control parametersas function_constants,and we modify the code accordingly.Here, we are showing the modified kernel code.Host side must be also updated to providefunction_constant value at pipeline creation time.Another great way to reduce register pressureis using 16-bit types in your shaders.Apple GPUs have native 16-bit type support.So, when using smaller data types,your shaders will require less registers,increasing occupancy.Half and short types also require less energyand might achieve higher peak rates.So, please use half and short typesinstead of float and int when possible,since type conversions are usually free.In this example,consider a kernel using the thread_positionin threadgroup for some computations.We are using unsigned int,but the maximum threadgroup size supported by Metalcan easily fit in unsigned short.threadgroup_position_in_grid, however,could potentially require a larger data type.But for the grid sizes we’re using in image processing --up to 8K or 16K -- unsigned short is also enough.If we use 16-bit types instead, the resulting codewill use a smaller number of registers,potentially increasing the occupancy.Now, let me show you where you can haveall the details on registers.GPU frame debugger in Xcode13now has advanced pipeline state object view for render,tile, and compute PSOs.You can inspect detailed pipeline statistics --now with registers usage -- and fine-tune all your shaders.With register concerns covered,let’s talk about texture formats.First, we want to note that different pixel formatsmight have different sampling rates.Depending on hardware generation and number of channels,wider floating-point types might havereduced point sampling rate.Especially floating-point formats such as RGBA32Fwill be slower than FP16 variantswhen sampling filtered values.Smaller types reduce memory storage,bandwidth, and cache footprint as well.So we encourage, again,to use the smallest type possible,but in this case, for the textures storage.This was actually a common case for 3D LUTs in image processing;most applications we worked with were using float RGBAfor a 3D LUT application phase with bilinear filtering enabled.Please consider if your app can instead use halfsand the precision will be enough.If that’s the case,switch to FP16 right away to get peak sampling rates.If half precision is not enough,we found out that fixed-point unsigned shortprovides great uniform range of values,so encoding your LUTs in unit scaleand providing LUT range to the shaderwas a great way to get both peak sampling rateand sufficient numerical accuracy.All right, so we just went overhow we should leverage Apple GPU architecture to makeyour image-processing pipeline run as efficient as possible.To apply it all right away, please meet Harsh!Harsh Patil: Thanks, Eugene.Now let’s walk through redesigningan image-processing pipeline for Apple siliconbased on all the best practices we have learned so far.To be specific,we are going to tailor the image-processing phaseof the video-processing pipeline for Apple GPUs.Real-time image processing is very GPU computeand memory bandwidth intensive.We will first understand how it is usually designedand then how we can optimize it for Apple silicon.We are not going to go into the detailsof video-editing workflow in this section,so please refer to our talk from two years ago.We will solely focus on transitioningthe compute part of image processing to render path.Before we start,let's begin quickly take a look at where image-processing phasestands in a typical video-processing pipeline.We'll take ProRes-encoded input file as an example.We first read the ProRes-encoded frame from the diskor external storage.We then decode the frame on CPU,and now the image-processing phaseexecutes on this decoded frame on the GPUand renders the final output frame.Finally, we display this output frame.We could additionally also encode the final rendered framefor delivery.Next, let’s take a look at what comprisesan image-processing pipeline.Image processing starts with unpacking different channelsof the source image RGB in alphainto separate buffers in the beginning.We will process each of these channelsin our image-processing pipeline,either together or separately.Next, there might be color space conversionsto operate in the desired color-managed environment.We then apply a 3D LUT; perform color corrections;and then apply spatial-temporal noise reduction, convolutions,blurs, and other effects.And finally,we pack the individually processed channels togetherfor final output.What do these selected steps have in common?They are all point filters,operating only on a single pixel with no interpixel dependency.These map well to fragment shader implementation.Spatial and convolution-style operationsrequire access to large radius of pixels,and we have scatteredread-write access patterns as well.These are well-suited for compute kernels.We’ll use this knowledge later.For now, let’s see how these operations are executed.Applications represent chain of effectsapplied to an image as a filter graph.Every filter is its own kernel,processing the inputs from the previous stageand producing outputs for the next stage.Every arrow here means a buffer being writtento/from output of one stageand read as the input in the next stage.Since memory is limited,applications usually linearize the graphby doing a topological sort.This is done to keep the total number of intermediate resourcesas low as possible while also avoiding race conditions.This simple filter graph in that examplewould need two intermediate buffers to be able to operatewithout race conditions and produce the final output.The linearized graph here roughly representsthe GPU command buffer encoding as well.Let’s look deeper on why this filter graphis very device memory bandwidth intensive.Every filter operation has to load whole imagefrom device memory into the registersand write the result back to the device memory.And that’s quite a bit of memory traffic.Let’s estimate the memory footprintfor a 4K-frame image processingbased on our example image-processing graph.A 4K decoded frame itself takes 67 megabytes of memoryfor floating-point 16 precisionor 135 megabytes of memory for floating-point 32 precision,and professional workflowsabsolutely need floating-point 32 precision.For processing one 4K frame in floating-point 32 precisionthrough this image-processing graph, we are talking more thantwo gigabytes of read-write traffic to device memory.Also, writes to buffers holding the intermediate outputthrashes the cache hierarchyand impacts other blocks on the chip as well.Regular compute kernelsdon’t benefit from the on-chip tile memory implicitly.Kernels can explicitly allocate threadgroup-scoped memory,which will be backed by the on-chip tile memory.However, that tile memory is not persistentacross dispatches within a compute encoder.In contrast, the tile memory is actually persistentacross draw passes within one render command encoder.Let’s see how we can redesign this representativeimage-processing pipeline to leverage the tile memory.We are going to address this by following three steps.We first change the compute pass to render passand all the intermediate output buffers to textures.We then encode per-pixel operationswith no interpixel dependency as fragment shader invocationswithin one render command encoder,making sure to account for all the intermediate resultsand setting appropriate load/store actions.And finally, we discuss what do we do in more complex situationthan just point filters.Our first step is to use separateMTLRenderCommandEncoder to encode eligible shaders.In this filter graph, unpack, color space conversion, LUT,and color-correction filters are all point per-pixel filtersthat we can convert to fragment shaderand encode them using one render command encoder.Similarly, mixer and pack shaders --which are towards the endof this image-processing pipeline --can also be converted to fragment shadersand encoded using another MTLRenderCommandEncoder.Then we can invoke these shaderswithin their respective render passes.When you create the render pass,all the resources attached to the color attachmentsin that render pass are implicitly tiled for you.A fragment shader can only update the image block dataassociated with fragment’s position in the tile.Next shader in the same render passcan pick up the output of the previous shaderdirectly from the tile memory.In the next section, we will take a lookat how we can structure the fragment shaderswhich map to these filters.We will also take a look at what constructswe need to define and use to enable accessto the underlying tile memoryfrom within these fragment shaders.And finally, we will take a lookat how the output generated in the tile memoryby one fragment shader can be consumeddirectly from the tile memory by the next fragment shaderwithin the same render command encoder.This is what you have to do in your code.Here I have attached output image as a textureattached to color attachment 0 of the render pass descriptor.I have attached texture holding intermediate resultto color attachment 1 of the render pass descriptor.Both of these will be implicitly tiled for you.Please set the appropriate load/store propertiesas discussed earlier in the talk.Now, set up a structure to access these texturesin your fragment shader.In the coming examples,we will show how to use this structurewithin your fragment shaders.You simply access the output and the intermediate textureswithin your fragment shader as highlightedby using the structure we defined earlier.Writes to these textures are doneto appropriate tile memory locationcorresponding to the fragment.Output produced by the unpack shader is consumed as inputby color space conversion shaderusing the same structure that we defined earlier.This fragment shader can do its own processingand update the output and intermediate textureswhich will, once again,update the corresponding tile memory location.You are to continue the same stepsfor all the other fragment shaderswithin the same render encoder pass.Next, let's visualizehow this sequence of operations looks now with these changes.As you can see, now you have unpack,color space conversion, application of 3D LUT,and color-correction steps, all executed on the tile memoryusing one render passwith no device memory passes in between.At the end of the render pass,render targets that are not memorylessare flushed to the device memory.You can then execute the next class of filters.Let’s talk a bit about filtersthat have scatter-gather access patterns.Kernels representing such filterscan directly operate on the data in the device memory.Convolution filters are very well-suitedfor tile-based operations in compute kernels.Here, you can express intent to use tile memoryby declaring a threadgroup-scoped memory.Now, you bring in the block of pixels into the tile memoryalong with all the necessary halo pixels,depending upon the filter radius,and perform the convolution operationdirectly on the tile memory.Remember, tile memory is not persistentacross compute dispatches within a compute encoder.So after executing Filter1,you have to explicitly flush the tile memory contentsto device memory.That way, Filter2 can consume the output of Filter1.So where do we land once we make all of these changes?For processing one 4K frame in floating-point 32 precisionthrough our example restructured image-processing graph,here’s what we have now.Bandwidth goes down from 2.16 gigabytesto just load and store worth 810 megabytes,and that’s 62 percent reduction in memory trafficto the device memory.We don't need two intermediate device bufferssaving 270 megabytes of memory per frame.And finally, we have reduced cache thrashing,and that's because all the fragment shaderswithin that render pass are operatingdirectly on the tile memory.One of the key features of Apple siliconis its Unified Memory Architecture.Let’s see an example of how to leveragethis Unified Memory Architecture for interactionbetween different blocks on the Apple silicon.We will take HEVC encoding of the final video framerendered by GPU as a case study.This encoding is done using dedicated hardware media engineson Apple silicon.The final output frame rendered by the GPUcan be consumed directly by our media engineswith no extra memory copies.In the coming section,we will walk through an example on how to set up a pipelinefor HEVC encoding of the final output frameproduced by the GPU in the most efficient way.For that, first we will leverage CoreVideo APIto create a pool of pixel buffersbacked by IOSurfaces.Then, using the Metal API,we render the final frames into Metal texturesbacked by IOSurfaces from the pool we just created.And finally, we dispatch these pixel buffersdirectly to the media engine for encodewithout any additional copies of the output framesproduced by the GPU,thus leveraging the Unified Memory Architecture.Let’s walk through on how to do this step by stepand covering all the constructs we need to enable this flow.First, we create a CVPixelBufferPoolbacked by IOSurface in the desired pixel format.Here, we will use the biplanarchroma-subsampled pixel format for HEVC encode.Now, you get a CVPixelBuffer from this CVPixelBufferPool.Pass this CVPixelBuffer to the MetalTextureCachewith the right plane indexto get the CVMetalTextureReference.Since we are using biplanar pixel format,you need to perform this stepfor both planes of the biplanar pixel buffer.Next, get the underlying Metal texture from theCVMetalTextureReference object.Perform this step for both luma and chroma planes.Remember that these Metal texturesare backed by the same IOSurfaceswhich are also backing the CVPixelBuffer planes.Using Metal API, render into the texturescorresponding to luma and chroma planes.This will update the IOSurfacewhich backs these Metal textures as well.We highly recommend doing the chroma subsampling stepon the chroma planes on the GPU itselfas a shader pass within your image-processing pipeline.An important thing to noteis that both CVPixelBuffer and the Metal textures --which we just rendered into --are backed by the same underlying IOSurface copyin the system memory.You can now send this CVPixelBufferdirectly to the media engine for encode.As you can see,due to the Unified Memory Architecture,we can seamlessly move data between GPUand media engine block with no memory copies.And finally, remember to release the CVPixelBufferand CVMetalTexture reference after every frame.Releasing the CVPixelBufferenables recycling of this buffer for future frames.To wrap up, we encourage you again to do the following:leverage Unified Memory Architecture,use MTLRenderCommandEncoder instead of computewhen applicable,merge all your eligible render passeswithin single render command encoder,set appropriate load/store actions,use memoryless for transient resources,leverage tile shading when applicable,and use buffer pools with other APIs for zero-copy.We want to thank you for joining this session today.Enjoy the rest of WWDC 2021!♪

♪ Bass music playing ♪♪Eugene Zhidkov: Hi and welcome to WWDC.

My name is Eugene Zhidkov. I am from GPU software.

And together, with Harsh Patil from Mac system architecture,we'll show you how to create image-processing applicationspowered by Metal on Apple silicon.

First, I will focuson the best practices and lessons learned,optimizing image-processing applications for M1based on developer engagements we have had over the last year.

And then Harsh will give you a step-by-step guideto how you can redesign your image-processing pipelinefor optimal performance on Apple silicon.

So let’s jump right in!To start, let’s briefly revisit Apple systemon-the-chip architecture and its benefits.

Many image-processing and video-editing appsare designed with discrete GPUs in mind.

So it’s important to highlight what’s so differentabout Apple GPUs.

First, all Apple chips use Unified Memory Architecture.

All blocks --such as CPU, GPU, Neural and Media engines --have access to the same system memoryusing unified memory interface.

And second, our GPUs are Tile Based Deferred Renderers,or TBDRs.

TBDRs have two main phases: tiling,where whole render surfaces split into tilesand processed geometry is then handled independently;and rendering,where all of the pixels will be processed for each tile.

So in order to be most efficient on Apple silicon,your image-processing appshould start leveraging unified memory --to avoid any copies your pipeline used to have --and TBDR architecture by exploiting tile memoryand local image block.

To learn more about how Apple TBDR works at low leveland how to target our shader core,please watch these sessions from the last year.

And now, let’s talk aboutthe exact things we are going to do to optimizeimage-processing compute workloadsfor Apple silicon.

Last year,we’ve been working closely with many great developerson their image pipeline transitions.

We picked the six most rewarding tips to share.

First, we’ll discuss how to avoidunnecessary memory copies or blits.

This is really importantgiven we are now working with images up to 8K.

Then, we wanted to highlight the benefits of usingrender pipeline and texturesinstead of using compute on buffersand how you could do thatin your own image-processing pipeline.

Once we have the render and textures pathsup and running,we wanted to show you the importance of properload/store actions and memoryless attachments.

This will help you getting the most out of the tile memory.

Then, we’ll talk how to best approach Uber-shaderswith its dynamic control flowand also how to leverage smaller data types --such as short and half --to improve performance and efficiency.

And we’ll finish with important adviceabout texture formats to get the best throughput.

All right.

So let’s get started with one of the most rewarding tips:avoiding unneeded blits on Apple silicon.

Most image-processing apps are designed around discrete GPUs.

With discrete GPUs,you have separate system memory and video memory.

To make the frame image visible or resident to the GPU,explicit copy is required.

Moreover, it is usually required twice;to upload the data for the GPU to process it,and to pull it back.

Let’s consider we are decoding an 8K video, processing it,and saving it to disk.

So this is a CPU thread, decoding, in this case.

That’s where we need to copy the decoded frame to GPU VRAM.

And here is GPU timeline,where all the effects and filters are applied.

Let’s take it one step further and let’s recallwe need to save the results to disk, right?So we must also consider bringing the processed frameback to system memoryand the actual encoding of the frame.

So, these are known as "copy" or "blit gaps",and advanced image-processing applicationshad to do deep pipelining and other smart thingsto fill them in.

Well, the good news is that on Apple GPUs,blitting for the sake of residenceis no longer needed.

Since memory is shared,both the CPU and GPU can access it directly.

So please add a simple check to detect if you are runningon unified memory system and avoid unnecessary copies.

It will save you memory, time,and is an absolute first step to do.

So this is where we land on Unified Memory Architecturewith the blits removed.

By removing the blits, we completely avoid copy gapsand can start processing immediately.

This also gives better CPU and GPU pipeliningwith less hassle.

Let’s make sure you implement unified memory pathwith no copies involved.

If you just leave blit copies exactly as it wason the discrete GPU,you’ll pay with system memory bandwidth,less GPU time for actual processing,and potential scheduling overhead.

Not to mention we no longer need separate VRAM image allocated.

GPU frame capture can help you with spotting large blits.

Please inspect your application blitsand make sure you only do the copies required.

Now, let’s talk about how exactly we should startleveraging Apple GPU TBDR architecturefor image processing.

Most image-processing applications operateon image buffers by dispatching series of compute kernels.

When you dispatch a compute kernelin default serial mode,Metal guarantees that all subsequent dispatchessee all the memory writes.

This guarantee implies memory coherency for all shader cores,so every memory write is made visible to all other coresby the time the next dispatch starts.

This also means memory traffic could be really high;the whole image has to be read and written to.

With M1, Apple GPUs enable tile dispatches on MacOS.

In contrast to regular compute, they operate in tile memorywith tile-only sync points.

Some filters -- like convolutions --cannot be mapped to the tile paradigm,but many other filters can!Deferring system memory flush until the encoder end pointprovides solid efficiency gains.

You can execute more useful GPU workwhen not limited by system memory bandwidth.

To take it even further,let’s notice that many per-pixel operationsdon’t require access to neighboring pixels,so tile sync point is not necessary.

This maps really well to fragment functions.

Fragment functions can be executedwithout implicit tile sync,requiring sync only at the encoder boundaryor when tile kernels are dispatched seriallyafter the fragment kernels.

We now learned that Apple GPUs enable fragment functionsand tile kernels for more efficient image processing.

Let’s see how we could use that.

We do that by converting regular compute dispatcheson buffers to render command encoder on textures.

As we just discussed, rule of thumb is the following.

Per-pixel operations with no interpixel dependencyshould be implemented using fragment functions.

Any filter with threadgroup scoped operationsshould be implemented with tile shading,since neighbor pixels access within a tile is required.

Scatter-gather and convolution filterscannot be mapped to tile paradigmsince they require random access,so these should still remain compute dispatches.

Render command encoder also enablesa unique Apple GPU feature:lossless bandwidth compression for textures and render targets.

This is a really great bandwidth saver,especially for an image-processing pipeline,so let’s see how we should use it.

Well, speaking of enabling lossless compression,it’s actually easier to say what you should not do.

First, already-compressed texture formatscannot benefit from lossless.

Second, there are three particular texture flagswhich cannot work with this compression,so make sure you don’t set them just by an accident.

And third, linear textures -- or backed by an MTLBuffer --are not allowed as well.

Some special treatment is also requiredfor nonprivate textures;make sure to call optimizeContentsForGPUAccessto stay on the fastest path.

GPU frame capture Summary pane now shows youlossless compression warningsand highlights the reasons why the texture has opted out.

In this example, PixelFormatView flag was set.

In many cases,developers are setting these flags unintentionally.

Don’t set PixelFormatViewif all you need is components swizzle or sRGB conversion.

All right, we have the render and textures pathup and running.

Now, let’s make sure we properly use tile memory.

Tile memory TBDR concepts --such as load/store actions and memoryless attachments --are totally new to the desktop world.

So let’s make sure we use them properly.

Let’s start with load/store actions!As we already know,the whole render target is split into the tiles.

Load/store are per-tile bulk actionsguaranteed to take the most optimal paththrough memory hierarchy.

They are executed at the beginning of the render pass --where we tell the GPU how to initialize the tile memory --and at the end of the pass to inform the GPUwhat attachments need to be written back.

The key thing here is to avoid loading what we don’t need.

If we are overwriting the whole image,or the resource is temporary,set load action to LoadActionDontCare.

With render encoder,you no longer need to clear your output or temporary data,as you probably did before with dedicated compute passor fillBuffer call.

By setting LoadActionClear,you can efficiently specify the clear value.

And the same goes for the store action.

Make sure to only store the data you will later need --like the main attachment --and don’t store anything temporary.

Besides explicit load and store actions,Apple GPUs saves your memory footprintwith memoryless attachments.

We can explicitly define an attachmentas having memoryless storage mode.

This enables tile-only memory allocation,meaning that your resource will persistfor each and every tile only within encoder lifetime.

This can greatly reduce your memory footprint,especially for 6K/8K images,where every frame takes hundreds of megabytes.

Let’s see how this all can be done in code.

We start by creating the textureDescriptorand then create the outputTexture.

We then create a temporary texture.

Notice that I’ve marked it memoryless,as we don’t want any storage here.

Then we create the render passby first describing what the attachments areand then what are the load/store actions.

We don’t care about loading the outputsince it is fully overwritten, but we need to store it.

As for the temporary texture, we don’t load but clear it,and we don’t need to store it either.

Finally, we create our renderPass from the descriptor.

That’s it.

So we are using unified memory,moved our image-processing pipelineto render command encoder,and are properly leveraging tile memory.

Now, let’s talk about uber-shaders.

Uber-shaders, or uber-kernels,is a pretty popular way to make developers' life easier.

Host code sets up the control structure,and shader just loops through a series of if/else statements,for example, if tone mapping is enabledor if the input is in HDR or SDR formats.

This approach is also known as "ubers-shader"and is really good at bringingtotal number of pipeline state objects down.

However, it has drawbacks.

The main one is increased register pressureto keep up with more complex control flow.

Using more registers can easily limit maximum occupancyyour shader is running at.

Consider a simple kernel where we pass in the control struct.

We use flags inside the struct to control what we do.

We have two features here:if the input is in HDR and if tonemapping is enabled.

All look good, right?Well, here is what happens on the GPU.

Since we cannot deduce anything at compile time,we have to assume we could take both paths --HDR and non-HDR --and then combine based on the flag.

Same goes for tone mapping.

We evaluate it and then mask it in or out,based on the input flag.

The problem here is registers.

Every control flow path needs live registers.

This is where uber-shaders are not so good.

As you recall, registers used by the kerneldefine maximum occupancy the shader could run.

That happens because registers file is shared by all simdlaneson the shader core.

If we could only run what’s only needed,that would enable higher simdgroup concurrencyand GPU utilization.

Let’s talk how to fix this.

Metal API has the right tool for the job,and it's called "function_constants".

We define both control parametersas function_constants,and we modify the code accordingly.

Here, we are showing the modified kernel code.

Host side must be also updated to providefunction_constant value at pipeline creation time.

Another great way to reduce register pressureis using 16-bit types in your shaders.

Apple GPUs have native 16-bit type support.

So, when using smaller data types,your shaders will require less registers,increasing occupancy.

Half and short types also require less energyand might achieve higher peak rates.

So, please use half and short typesinstead of float and int when possible,since type conversions are usually free.

In this example,consider a kernel using the thread_positionin threadgroup for some computations.

We are using unsigned int,but the maximum threadgroup size supported by Metalcan easily fit in unsigned short.

threadgroup_position_in_grid, however,could potentially require a larger data type.

But for the grid sizes we’re using in image processing --up to 8K or 16K -- unsigned short is also enough.

If we use 16-bit types instead, the resulting codewill use a smaller number of registers,potentially increasing the occupancy.

Now, let me show you where you can haveall the details on registers.

GPU frame debugger in Xcode13now has advanced pipeline state object view for render,tile, and compute PSOs.

You can inspect detailed pipeline statistics --now with registers usage -- and fine-tune all your shaders.

With register concerns covered,let’s talk about texture formats.

First, we want to note that different pixel formatsmight have different sampling rates.

Depending on hardware generation and number of channels,wider floating-point types might havereduced point sampling rate.

Especially floating-point formats such as RGBA32Fwill be slower than FP16 variantswhen sampling filtered values.

Smaller types reduce memory storage,bandwidth, and cache footprint as well.

So we encourage, again,to use the smallest type possible,but in this case, for the textures storage.

This was actually a common case for 3D LUTs in image processing;most applications we worked with were using float RGBAfor a 3D LUT application phase with bilinear filtering enabled.

Please consider if your app can instead use halfsand the precision will be enough.

If that’s the case,switch to FP16 right away to get peak sampling rates.

If half precision is not enough,we found out that fixed-point unsigned shortprovides great uniform range of values,so encoding your LUTs in unit scaleand providing LUT range to the shaderwas a great way to get both peak sampling rateand sufficient numerical accuracy.

All right, so we just went overhow we should leverage Apple GPU architecture to makeyour image-processing pipeline run as efficient as possible.

To apply it all right away, please meet Harsh!Harsh Patil: Thanks, Eugene.

Now let’s walk through redesigningan image-processing pipeline for Apple siliconbased on all the best practices we have learned so far.

To be specific,we are going to tailor the image-processing phaseof the video-processing pipeline for Apple GPUs.

Real-time image processing is very GPU computeand memory bandwidth intensive.

We will first understand how it is usually designedand then how we can optimize it for Apple silicon.

We are not going to go into the detailsof video-editing workflow in this section,so please refer to our talk from two years ago.

We will solely focus on transitioningthe compute part of image processing to render path.

Before we start,let's begin quickly take a look at where image-processing phasestands in a typical video-processing pipeline.

We'll take ProRes-encoded input file as an example.

We first read the ProRes-encoded frame from the diskor external storage.

We then decode the frame on CPU,and now the image-processing phaseexecutes on this decoded frame on the GPUand renders the final output frame.

Finally, we display this output frame.

We could additionally also encode the final rendered framefor delivery.

Next, let’s take a look at what comprisesan image-processing pipeline.

Image processing starts with unpacking different channelsof the source image RGB in alphainto separate buffers in the beginning.

We will process each of these channelsin our image-processing pipeline,either together or separately.

Next, there might be color space conversionsto operate in the desired color-managed environment.

We then apply a 3D LUT; perform color corrections;and then apply spatial-temporal noise reduction, convolutions,blurs, and other effects.

And finally,we pack the individually processed channels togetherfor final output.

What do these selected steps have in common?They are all point filters,operating only on a single pixel with no interpixel dependency.

These map well to fragment shader implementation.

Spatial and convolution-style operationsrequire access to large radius of pixels,and we have scatteredread-write access patterns as well.

These are well-suited for compute kernels.

We’ll use this knowledge later.

For now, let’s see how these operations are executed.

Applications represent chain of effectsapplied to an image as a filter graph.

Every filter is its own kernel,processing the inputs from the previous stageand producing outputs for the next stage.

Every arrow here means a buffer being writtento/from output of one stageand read as the input in the next stage.

Since memory is limited,applications usually linearize the graphby doing a topological sort.

This is done to keep the total number of intermediate resourcesas low as possible while also avoiding race conditions.

This simple filter graph in that examplewould need two intermediate buffers to be able to operatewithout race conditions and produce the final output.

The linearized graph here roughly representsthe GPU command buffer encoding as well.

Let’s look deeper on why this filter graphis very device memory bandwidth intensive.

Every filter operation has to load whole imagefrom device memory into the registersand write the result back to the device memory.

And that’s quite a bit of memory traffic.

Let’s estimate the memory footprintfor a 4K-frame image processingbased on our example image-processing graph.

A 4K decoded frame itself takes 67 megabytes of memoryfor floating-point 16 precisionor 135 megabytes of memory for floating-point 32 precision,and professional workflowsabsolutely need floating-point 32 precision.

For processing one 4K frame in floating-point 32 precisionthrough this image-processing graph, we are talking more thantwo gigabytes of read-write traffic to device memory.

Also, writes to buffers holding the intermediate outputthrashes the cache hierarchyand impacts other blocks on the chip as well.

Regular compute kernelsdon’t benefit from the on-chip tile memory implicitly.

Kernels can explicitly allocate threadgroup-scoped memory,which will be backed by the on-chip tile memory.

However, that tile memory is not persistentacross dispatches within a compute encoder.

In contrast, the tile memory is actually persistentacross draw passes within one render command encoder.

Let’s see how we can redesign this representativeimage-processing pipeline to leverage the tile memory.

We are going to address this by following three steps.

We first change the compute pass to render passand all the intermediate output buffers to textures.

We then encode per-pixel operationswith no interpixel dependency as fragment shader invocationswithin one render command encoder,making sure to account for all the intermediate resultsand setting appropriate load/store actions.

And finally, we discuss what do we do in more complex situationthan just point filters.

Our first step is to use separateMTLRenderCommandEncoder to encode eligible shaders.

In this filter graph, unpack, color space conversion, LUT,and color-correction filters are all point per-pixel filtersthat we can convert to fragment shaderand encode them using one render command encoder.

Similarly, mixer and pack shaders --which are towards the endof this image-processing pipeline --can also be converted to fragment shadersand encoded using another MTLRenderCommandEncoder.

Then we can invoke these shaderswithin their respective render passes.

When you create the render pass,all the resources attached to the color attachmentsin that render pass are implicitly tiled for you.

A fragment shader can only update the image block dataassociated with fragment’s position in the tile.

Next shader in the same render passcan pick up the output of the previous shaderdirectly from the tile memory.

In the next section, we will take a lookat how we can structure the fragment shaderswhich map to these filters.

We will also take a look at what constructswe need to define and use to enable accessto the underlying tile memoryfrom within these fragment shaders.

And finally, we will take a lookat how the output generated in the tile memoryby one fragment shader can be consumeddirectly from the tile memory by the next fragment shaderwithin the same render command encoder.

This is what you have to do in your code.

Here I have attached output image as a textureattached to color attachment 0 of the render pass descriptor.

I have attached texture holding intermediate resultto color attachment 1 of the render pass descriptor.

Both of these will be implicitly tiled for you.

Please set the appropriate load/store propertiesas discussed earlier in the talk.

Now, set up a structure to access these texturesin your fragment shader.

In the coming examples,we will show how to use this structurewithin your fragment shaders.

You simply access the output and the intermediate textureswithin your fragment shader as highlightedby using the structure we defined earlier.

Writes to these textures are doneto appropriate tile memory locationcorresponding to the fragment.

Output produced by the unpack shader is consumed as inputby color space conversion shaderusing the same structure that we defined earlier.

This fragment shader can do its own processingand update the output and intermediate textureswhich will, once again,update the corresponding tile memory location.

You are to continue the same stepsfor all the other fragment shaderswithin the same render encoder pass.

Next, let's visualizehow this sequence of operations looks now with these changes.

As you can see, now you have unpack,color space conversion, application of 3D LUT,and color-correction steps, all executed on the tile memoryusing one render passwith no device memory passes in between.

At the end of the render pass,render targets that are not memorylessare flushed to the device memory.

You can then execute the next class of filters.

Let’s talk a bit about filtersthat have scatter-gather access patterns.

Kernels representing such filterscan directly operate on the data in the device memory.

Convolution filters are very well-suitedfor tile-based operations in compute kernels.

Here, you can express intent to use tile memoryby declaring a threadgroup-scoped memory.

Now, you bring in the block of pixels into the tile memoryalong with all the necessary halo pixels,depending upon the filter radius,and perform the convolution operationdirectly on the tile memory.

Remember, tile memory is not persistentacross compute dispatches within a compute encoder.

So after executing Filter1,you have to explicitly flush the tile memory contentsto device memory.

That way, Filter2 can consume the output of Filter1.

So where do we land once we make all of these changes?For processing one 4K frame in floating-point 32 precisionthrough our example restructured image-processing graph,here’s what we have now.

Bandwidth goes down from 2.16 gigabytesto just load and store worth 810 megabytes,and that’s 62 percent reduction in memory trafficto the device memory.

We don't need two intermediate device bufferssaving 270 megabytes of memory per frame.

And finally, we have reduced cache thrashing,and that's because all the fragment shaderswithin that render pass are operatingdirectly on the tile memory.

One of the key features of Apple siliconis its Unified Memory Architecture.

Let’s see an example of how to leveragethis Unified Memory Architecture for interactionbetween different blocks on the Apple silicon.

We will take HEVC encoding of the final video framerendered by GPU as a case study.

This encoding is done using dedicated hardware media engineson Apple silicon.

The final output frame rendered by the GPUcan be consumed directly by our media engineswith no extra memory copies.

In the coming section,we will walk through an example on how to set up a pipelinefor HEVC encoding of the final output frameproduced by the GPU in the most efficient way.

For that, first we will leverage CoreVideo APIto create a pool of pixel buffersbacked by IOSurfaces.

Then, using the Metal API,we render the final frames into Metal texturesbacked by IOSurfaces from the pool we just created.

And finally, we dispatch these pixel buffersdirectly to the media engine for encodewithout any additional copies of the output framesproduced by the GPU,thus leveraging the Unified Memory Architecture.

Let’s walk through on how to do this step by stepand covering all the constructs we need to enable this flow.

First, we create a CVPixelBufferPoolbacked by IOSurface in the desired pixel format.

Here, we will use the biplanarchroma-subsampled pixel format for HEVC encode.

Now, you get a CVPixelBuffer from this CVPixelBufferPool.

Pass this CVPixelBuffer to the MetalTextureCachewith the right plane indexto get the CVMetalTextureReference.

Since we are using biplanar pixel format,you need to perform this stepfor both planes of the biplanar pixel buffer.

Next, get the underlying Metal texture from theCVMetalTextureReference object.

Perform this step for both luma and chroma planes.

Remember that these Metal texturesare backed by the same IOSurfaceswhich are also backing the CVPixelBuffer planes.

Using Metal API, render into the texturescorresponding to luma and chroma planes.

This will update the IOSurfacewhich backs these Metal textures as well.

We highly recommend doing the chroma subsampling stepon the chroma planes on the GPU itselfas a shader pass within your image-processing pipeline.

An important thing to noteis that both CVPixelBuffer and the Metal textures --which we just rendered into --are backed by the same underlying IOSurface copyin the system memory.

You can now send this CVPixelBufferdirectly to the media engine for encode.

As you can see,due to the Unified Memory Architecture,we can seamlessly move data between GPUand media engine block with no memory copies.

And finally, remember to release the CVPixelBufferand CVMetalTexture reference after every frame.

Releasing the CVPixelBufferenables recycling of this buffer for future frames.

To wrap up, we encourage you again to do the following:leverage Unified Memory Architecture,use MTLRenderCommandEncoder instead of computewhen applicable,merge all your eligible render passeswithin single render command encoder,set appropriate load/store actions,use memoryless for transient resources,leverage tile shading when applicable,and use buffer pools with other APIs for zero-copy.

We want to thank you for joining this session today.

Enjoy the rest of WWDC 2021!♪

10:53 -Memoryless attachments

12:25 -Uber-shaders impact on registers

13:32 -Function constants for Uber-shaders

23:02 -Image processing filter graph

## Code Samples

```swift
let
 textureDescriptor 
=
 
MTLTextureDescriptor
.texture2DDescriptor(
…
)

let
 outputTexture 
=
 device.makeTexture(descriptor: textureDescriptor)

textureDescriptor.storageMode 
=
 .memoryless

let
 tempTexture 
=
 device.makeTexture(descriptor: textureDescriptor) 


let
 renderPassDesc 
=
 
MTLRenderPassDescriptor
()
renderPassDesc.colorAttachments[
0
].texture      
=
 outputTexture
renderPassDesc.colorAttachments[
0
].loadAction   
=
 .dontCare
renderPassDesc.colorAttachments[
0
].storeAction  
=
 .store
renderPassDesc.colorAttachments[
1
].texture      
=
 tempTexture
renderPassDesc.colorAttachments[
1
].loadAction   
=
 .clear
renderPassDesc.colorAttachments[
1
].storeAction  
=
 .dontCare


let
 renderPass 
=
 commandBuffer.makeRenderCommandEncoder(descriptor: renderPassDesc)
```

```swift
fragment float4 processPixel(
const
 constant ParamsStr* cs [[ buffer(
0
) ]])
{
  
if
 (cs->inputIsHDR) {
    
// do HDR stuff

  } 
else
 {
    
// do non-HDR stuff

  }
  
if
 (cs->tonemapEnabled) {
    
// tone map

  }
}
```

```swift
constant 
bool
 featureAEnabled[[function_constant(
0
)]];
constant 
bool
 featureBEnabled[[function_constant(
1
)]];

fragment float4 processPixel(...)
{
  
if
 (featureAEnabled) {
    
// do A stuff

  } 
else
 {
    
// do not-A stuff

  }
  
if
 (featureBEnabled) {
    
// do B stuff

  }
}
```

```swift
typedef
 
struct

{
    float4 OPTexture        [[ color(
0
) ]];
    float4 IntermediateTex  [[ color(
1
) ]];
} FragmentIO;

fragment FragmentIO Unpack(RasterizerData 
in
 [[ stage_in ]],
                           texture2d<
float
, access::sample> srcImageTexture [[texture(
0
)]])
{
    FragmentIO 
out
;
    
    
//...

                         
    
// Run necessary per-pixel operations

    
out
.OPTexture       = 
// assign computed value;

    
out
.IntermediateTex = 
// assign computed value;

    
return
 
out
;
}

fragment FragmentIO CSC(RasterizerData 
in
 [[ stage_in ]], FragmentIO Input)
{
    FragmentIO 
out
;

    
//...    

    
    
out
.IntermediateTex = 
// assign computed value;

    
return
 
out
;
}
```

