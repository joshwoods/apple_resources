# Wwdc2021 10148

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Optimize high-end games for Apple GPUsOptimize your high-end games for Apple GPUs: We'll show you how you can use our rendering and debugging tools to eliminate performance issues and make your games great on Apple platforms. Learn from our experiences working with developers at Larian Studios and 4A Games as we help them optimize their games for Apple GPUs.

We'll explore various techniques for improving your game's performance, including optimizing shaders, reducing memory bandwidth utilization, and increasing the overlap of your GPU workloads. We'll also dive into the new GPU Timeline profiling tool in Xcode 13 to identify possible performance bottlenecks in “Divinity: Original Sin 2” when running on iPad.

For this session, you should be familiar with the tile-based deferred rendering architecture in Apple GPUs, and have a working knowledge of Xcode and the Metal API.

Check out “Discover Metal debugging, profiling, and asset creation tools” or the WWDC20 session “Optimize Metal apps and games with GPU counters” to learn more about using our tools to profile graphics workloads.ResourcesDebugging the shaders within a draw command or compute dispatchMetalMetal Performance ShadersMetal Shading Language SpecificationHD VideoSD VideoRelated VideosWWDC21Discover Metal debugging, profiling, and asset creation toolsEnhance your app with Metal ray tracingExplore bindless rendering in MetalExplore hybrid rendering with Metal ray tracingWWDC20Harness Apple GPUs with MetalOptimize Metal apps and games with GPU counters

Optimize your high-end games for Apple GPUs: We'll show you how you can use our rendering and debugging tools to eliminate performance issues and make your games great on Apple platforms. Learn from our experiences working with developers at Larian Studios and 4A Games as we help them optimize their games for Apple GPUs.

We'll explore various techniques for improving your game's performance, including optimizing shaders, reducing memory bandwidth utilization, and increasing the overlap of your GPU workloads. We'll also dive into the new GPU Timeline profiling tool in Xcode 13 to identify possible performance bottlenecks in “Divinity: Original Sin 2” when running on iPad.

For this session, you should be familiar with the tile-based deferred rendering architecture in Apple GPUs, and have a working knowledge of Xcode and the Metal API.

Check out “Discover Metal debugging, profiling, and asset creation tools” or the WWDC20 session “Optimize Metal apps and games with GPU counters” to learn more about using our tools to profile graphics workloads.

Debugging the shaders within a draw command or compute dispatch

Metal

Metal Performance Shaders

Metal Shading Language Specification

HD VideoSD Video

HD Video

SD Video

Discover Metal debugging, profiling, and asset creation tools

Enhance your app with Metal ray tracing

Explore bindless rendering in Metal

Explore hybrid rendering with Metal ray tracing

Harness Apple GPUs with Metal

Optimize Metal apps and games with GPU counters

Search this video…Welcome to WWDC.Hi, I'm Jonathan Metzgar.I'm a member of the Metal Ecosystem teamat Apple.We get to work with game developers to help themget the best graphics performance on our Apple GPUs.Dustin and I are going to show you howwe optimize high-end games for Apple GPUs.In this video, I'm going to coverthe process that we use to optimize games.Then, I'm going to show you the kindsof optimizations that are usedin the games Baldur's Gate 3 and Metro Exodus.And lastly, Dustin is going to do a toolsdemonstration featuring the game Divinity: Original Sin 2,while he introduces the new GPU Timeline in Xcode 13.Let's dive in and talk about optimization.So, over the past year, we collaboratedwith Larian Studios and 4A Games to find ways to tunethe graphics performance in their games for Apple GPUs.I am sure you'll be excited to see the details,and I want to take a moment and thank both Larian Studiosand 4A Games for giving us permission to showdevelopment materials in this presentation.Looking back over the course of the year,we have analyzed many games and identified some common scenariosthat affect graphics performance.You're probably interested in finding opportunitiesto optimize your own game, so we have geared this sessionto emphasize how our GPU tools are especially helpfulin pinpointing these problem areasand to suggest ways to solve them.And, in particular, I'd like to share some of the principlesour team uses to help developers optimize their games.When we optimize a graphics application, it's importantto have a methodology, a set of principlesthat define how we solve a particular problem.So, let me show you a four-step process.First, you need to choose what data to collect,or measure, so it will help you understandwhat's happening with your game.Soon after you begin measuring data,you will want to choose some performance targets,or where you want to be when you finish.You may decide the in-game locationto take your GPU frame captures and Metal system traces,the scene complexity, graphics settings,and other metrics important to you, like frame time.Then, you analyze the data to learn aboutthe behavior of your engine.In-depth analysis helps you find whereand why the bottlenecks are occurring.Once you know what is causing a bottleneck,then you can make improvements to the game,but normally you pick one or two at a time,so you can understand the impact of each change.Lastly, you verify your improvements by comparingsome new measurements with your original ones.Since optimization is a process, you will go back and repeatuntil your performance targets have been met.For these games, we use Xcode's Metal Debuggerto give us insights about their performanceand how their frame graphs are structured,and we use Metal System Trace in Instruments to learnabout a game's performance over time.It's a great idea to save a GPU trace fileand an Instruments trace file so you can haveyour before and after data, both before and after optimization.So, I have a little list of things you could consider,or look for, in your game.As I mentioned, Xcode and Instruments are great toolsto help you understand your Metal application.Optimization is about getting the best out of several areas,ranging from shader performance to memory bandwidth.Another area is getting good overlap across your vertex,fragment, and compute workloads.And while rendering several frames in flight,some Apple GPUs can overlap workloads between them.I'll show you some pointers to help you with resourcedependencies, which might prevent that overlap.And since some developers use a custom workflowfor their shaders, I'll show you howcompiler settings can affect performance.Lastly, I'll talk about how to reduce the impactof redundant bindings.Let's start with Baldur's Gate 3 from Larian Studios.Baldur's Gate 3 is an RPG buildingon a 20-year gaming legacy and stands outwith its cinematic visual effects.Our engagement with Larian Studioshelped us identify how they could optimizetheir amazing rendering engine for Apple GPUs.First, we started with a GPU frame capture,like the Ravaged Beach scene we see here.Then, we break down the scene into a frame graph.The frame graph is a breakdownof the order and purpose of each rendering pass.High-end games have many render passes specializing in achievinga certain visual effect, such as ambient occlusion,shadow mapping, post processing, and so on.Baldur's Gate 3 has a complex frame graph,so this is a simplified version.By using Xcode's Metal Debugger, we capture a GPU traceand use it to see all the render passes in the game.Clicking on Show Dependencies bringsup a visualization that you can pan and zoom.It shows how your render passes depend on the resultsof previous ones to help you understand what's going on.For example, I am zooming into this deferreddecal render stage to get more details.Next, I will show you the Instruments tools.We spend time analyzing games using the Instruments trace,using the Metal System Trace, or Game performance templates.Metal System Trace is ideal if you wanna focus on GPU executionand scheduling analysis, and Game Performanceexpands on that to help you with other issues,like thread stalls or thermal notifications.Let's choose Metal System Trace to seethe behavior of our engine from frame to frame.Instruments allows you to view several channelsof data along a timeline.Here, we find our first problem:An expensive workload in our render passes.An expensive workload might mean that we needto optimize a shader.For instance, we see a long compute shaderholding up the rest of our frame.We call these gaps "bubbles."Let's switch back over to the GPU traceand investigate this further.This is the "before" GPU trace.Let's change the grouping from API CALL to PIPELINE STATE.You may notice the pipeline statesare sorted by execution time.Let's check the first compute pipeline.We can expand the compute function detailsto take a closer look at its statistics.Notice here that there areover four-and-a-half-thousand instructions.That's quite a lot. So, what else?Let's see what resources are being usedby this compute function.Depending on the input data, this function uses upto 120 textures to produce the output.However, we discovered that only sixto 12 are actually used 90% of the time.So, let's talk about how this shader could be improved.Shaders that need to handle many different conditions can reservemore registers than necessary, and this can reducethe number of threads that run in parallel.Splitting your workload into smaller,more focused shaders, which need fewer registers,can improve the utilization of the shader cores.So, instead of selecting the appropriate algorithmin the shader, you would choose the appropriate shaderpermutation when you issue your GPU workload.Additionally, a shader function which uses too many registerscan result in register pressure, when an execution unit runs outof fast register memory and has to use device memory instead.That's one reason to use 16-bit types, like half,when appropriate, since they use half the registerspace than 32-bit types, like floats.In this case, Larian Studios already optimized their shaderto use half-precision floating point and decided to creatededicated shader variants, instead.So, let's see what happened.When comparing the numbers before, in the box on the left,with the numbers in the box on the right, the numberof instructions reduced by 84%, branches reduced 90%,registers reduced 25%, and texture reads reduced 92%.This shader variant is used 90% of the time.We can also see this in the Metal System Trace.Notice here, in the before trace,the bubbles we saw earlier.And here, in the after trace, they have been minimized.Larian Studios was able to reduce this shaderby eight milliseconds, on average.That is a huge win!If you look at your most expensive pipeline state objectsand shaders, you may find a complicated shaderthat could be simplified.This is especially true if the resultsof that shader are used by a later pass.This was a huge improvement for the game,but short of the developer's performance target.We just mentioned memory as an issue, and oneof the features of our GPUs is lossless compression,which is enabled in certain conditions.So, maybe there was a flag we either accidentally setor forgot to set.Lossless compression helps reduce bandwidth by compressingtextures when they are stored from tile to device memory.If you look at the Bandwidth Insightson the Summary page, you may noticeLossless Compression warnings for some textures.They tell you that these texturescan't be lossless compressed,and you may pay a bandwidth penalty.Metal Debugger will also let you know why these texturescan't be lossless compressed.Here we see it's because of the ShaderWrite usage flag.We can see all the usage flags by going to the memory section.Once in the memory section, we can filter by render targets.Then, right click on the table header,choose texture, and then usage.Now, we can sort by usageand find the textures using ShaderWrite.If you set the ShaderWrite or PixelFormatView flagwhen you create your textures,you will disable lossless compression.Let's take a look at these flags in more detail.The Unknown, ShaderWrite, and PixelFormatView flagsprevent your textures from being lossless compressed.The general rule of thumb is to use these flagsonly when required.For example, you would use the ShaderWrite flagif you use the write() method to store valuesin a texture from a fragment or compute function.Rendering to a texture bound as a color attachmentdoesn't require the ShaderWrite flag.And don't set the PixelFormatView optionif you only need to read the component valuesin a different order.Instead, create a texture view usinga swizzle pattern to specify the new order.Similarly, don't set the PixelFormatView option ifyour texture view only converts between linear space and sRGB.Check the documentation for more information.Shader optimization and lossless compressionare two techniques that have helped us out,but another problem area is getting good overlapacross the vertex, fragment, and compute channels.Let's take a look at two ways to optimize workloadsacross channels.First, we'll start by looking at our Metal System Trace again.Here, we can see that we have low overlapon our vertex, fragment, and compute channels.It would be nice to improve this to keep the GPU busy.One way to solve this problem is to see if we canrestructure the encoding order in our frame graph.In other words, we want to move this workover to where the vertex stage has very low occupancy.We would like to process those vertices earlier,along with the fragment stage of an earlier render pass.We can think of our frame graph as a listof rendering tasks, like this pseudocode example.Getting good overlap can be as simple as changingthe order of your render tasks in your frame graph.Some tasks may rely on results from earlier ones,but not always.It turns out that the CascadedShadowBuffer stage,which is vertex-shader heavy, could be moveda few tasks earlier, since it has few dependencies.And now, we see that our regionwith low overlap has better utilization of the vertexand fragment channels, giving us another 1 ms win.But there is another optimizationthat we can try out.Games often have two to three frames in flight.So, a cool feature in our tile-baseddeferred rendering, or TBDR architecture GPUs,is to overlap workloads from two frameswhen there are no resource dependencies between them.So, I'm going to show you how to optimizefor this possibility.Let's have a look at the GPU track in Instruments once again.Here, you can see that these framesare processed, almost serially.This is caused by using a blit encoder to updateconstant buffers, like per-frame animation data, and so on.To efficiently update constant buffer data with a discrete GPU,we blit from shared buffers on the CPU to a private bufferon the GPU, which will be used for rendering the frame.This strategy is efficient for GPUs with discrete memory,so you want to keep this behavior for that purpose.If your device has a unified memory architecture,then there is no need to use a blit encoderto copy your data to a private buffer.However, when you use a shared buffer in a ring-buffer pattern,you need to watch out for synchronization issuesbecause visual corruption can happen if your CPUwrites to data currently being read by the GPU.Let's see this in action.Here, you can see in this diagramthe encoding and rendering of our frames.We are using colors to representthe shared buffers, which are updatedat the beginning of the frame: blue for buffer one,green for buffer two, and yellow for buffer three.Ring buffers are typically used to implement queues,which need to use a compact amount of memory.Here, there is no concern of a data race conditionwith this arrangement, as our writing and readingof our shared buffers is mutually exclusive.It's very common to have latency between encodingthe frame and the rendering of a frame.This causes a shiftof when the rendering actually begins.As long as the latency isn't too long,you will not have a data race condition.However, what happens if latency continues to increase?Well, this introduces a data race condition,where the main thread is updating its shared buffersduring the time the GPU is rendering the frame.And if that happens, you could get visual corruptionif elements of your frame are dependent on this data.In the case of Baldur's Gate 3, removing the private bufferand blit encoder eliminatedthe synchronization point, but introduced a race condition,which affected their temporal anti-aliasing render pass.So, let's see how to avoid this situation.To avoid this race condition,you need to make sure you are not writinginto the same resource the GPU is reading from.For example, you could utilizea completion handler, and then wait until it is safeto update the shared buffer in your encoding thread.But let me show you how we avoided a wait time.We maintained our completion handler, but added an extrabuffer to our ring buffer to avoid the wait.The extra buffer is colored purple on the bottom diagram.The memory consumption remains the same as with a discrete GPU.But if you need to save on memory,and the CPU wait time doesn't affect frame rate of your game,then you can just use three buffers.So, let's look at an easy way to decide how many sharedand private buffers to create with a pseudocode example.In this code snippet, you can see how to choosethe number of shared and private buffers at initialization time.Once we have created our device,we can check to see if the device hasunified memory or not, and then ensure that we createan extra shared buffer, or to use a private buffer.This extra buffer will help reducethe impact of waiting for a completion handler,which we are using to avoid a data race condition.And now, we can see how Fragment workloads from the previousframe overlaps with Vertex workloads from the next frame.Overall, this can give us oneto two milliseconds, depending on the scene.And, of course, this approach can be applied notonly for the constant buffer data we'veshown in this example, but for all of the buffer datayou transfer from the CPU to the GPU.So, let's review.Larian Studios was able to achieve their performancetargets by applying the following optimizations:Optimizing their most expensive shaders to reduce bubbles,opting in to lossless compression to improvebandwidth, overlapping vertex and fragment workloadsto get better GPU utilization, and checkingfor resource dependencies that prevent frame overlap.When they were finished, Larian Studios not only mettheir performance targets, but got a 33% improvementin frame time for their game.And now, we will look at a different setof optimizations with the game Metro Exodus.Metro Exodus is known for its epic storylineand demanding visual effects, as you can seein this series of game-play clips.After the integration of our suggested optimizations,4A Games was able to meet their performance targets.So now, let's have a look at an in-game scenefrom Metro Exodus.Metro Exodus uses a custom workflow to translaterender commands into Metal API commands,which is quite common for cross-platform games.The translation layer they are using is optimizedfor Metal, but some issues can arisewhen two complex systems come together in practice.So, additional performance tuningwas required to meet their project goals.As in the previous game, we startby investigating how a frame is being rendered.Modern renderers have a lot of different techniques involvedso first we try to understand the high-level frame graph.Again, we start analysis by looking at the GPU trace.It always gives us useful insights about game performance.So first, let's start with the GPU time,which doesn't meet the developer performance targets.So, let's find the shader or pipelinewhich is the most time-consuming.To do this, we are going to group by pipeline stateonce again and look at the most expensive one.Let's quickly look at its statistics.You can see that there is a high number of ALU instructionscompared to the total, meaning this is a math-heavy shader.We also see that the number of registers being usedby the shader is quite high.The number of registers used by a particular shader directlyaffects how its workload will scale during execution.The higher this number is, the less work can be donein parallel by the GPU.Sometimes it's just a complex shader, such as SSAOin this example, that requires lotsof computations and registers, but sometimesthe compiler settings can affect the generated instructionsand register allocation, as well.Let's also take a look at the shader compiler options.And it turns out, this shader was compiledwith the fast math flag disabled.Fast math allows the shader compiler to optimizevarious instructions, and it is enabledfor the Metal shader compiler, by default.However, there might be some cases,for example, using custom shader workflows,that can disable this compilation flag.In this case, we discovered that the translation layer,which 4A Games was using to invoke the compiler,had its default behavior set to not use fast math.So, what is fast math?Fast math is a set of optimizationsfor floating-point arithmetic,that trades between speed and correctness.For example, assumptions can be made that there will be no NANs,infinity, or signed zeros as either a result or argument.Fast math optimizations can also applyalgebraically-equivalent transformations,which may affect the precision in floating-point results.However in most scenarios,fast math is a great choice for games.This can significantly improve performance,especially in ALU-bound cases.Our recommendation to you is to check your compiler optionsto verify that you have enabled fast math, if your shaders donot depend on the things that we just mentioned.The fast math flag works at the front-and back-end compiler levels.When you are building your shader source,the front-end shader compiler will select fast math functions,which will be used in intermediate code.This will hint to the back-end shader compiler that it cangenerate more optimal GPU machine code.Here, you can see how the Instructionsand Register counters on the left have been improvedin the box on the right after we recompiled this shader.So, after changing the behavior of the translation layerto enable fast math for all the shaders,we got a 21% frame time decrease in our test workloadusing the built-in game benchmark.So, the next area I wanna talk about is redundant bindings.If we go back to the summary page,and look at the API insights, we can see there aremany redundant bindings when rendering the frame.Redundant bindings can be either resources like textures,buffers, and samplers; or render stateslike depth stencil state, viewport configuration, etc.Repeatedly binding resources might negatively affectyour encoding time, but redundant renderstate changes may also affect the GPU time.Let's have a look at the encodingand GPU times in the Metal System Trace.For a given frame, it takes eight-and-a-half millisecondsfor all the commands to be encodedand around 22 millisecondsfor the GPU to render this frame.When we investigated the cause of the redundant bindings,we found that the translation layercould be modified to reduce them.So, let me show you a pseudocode example which shows howto check for and reduce redundant bindings.Instead of binding textures directly to the encoder,you can pre-cache them and only bind them if they change.And to minimize interactions with the API, you can set allthe textures with one call to the setFragmentTextures methodinstead of setting them in a loop, one by one.Additionally, you can apply a similar approachto other shader stages and other binding types,like buffers and samplers, as well as render states.So, let's see what happened in the Metal System Trace.4A Games was able to reduce encoding timebetween 30% and 50%, depending on the scene,because the translation layer wasn't repeatedlybinding the same resources and render states.However, GPU time also decreased by upto three milliseconds and, overall,resulted in a 15% speedup in their in-game benchmark.If you have a few redundant binding warnings,it's not an issue, but we definitely see an impactwith hundreds or thousands of redundant bindings.So, avoiding redundant bindings gave us a further 15% reductionin average frame time.After these two improvements, 4A Games was able to meettheir performance targets.So now, let's summarize what we learnedfrom optimizing Metro Exodus for Apple GPUs.First, if you're using a custom workflow for shaders,you should check your compiler settings to ensureyou are using the best options for your Metal applications.And if you see a lot of redundant binding warningsin the Metal Debugger, I showed you a techniqueto reduce encoding and GPU time overhead,which you can apply either to your engineor the translation layer that you are using.And now, I'd like to hand it over to Dustin,who's going to talk to you about Divinity: Original Sin 2and demo the new Xcode GPU timeline features.Thanks, Jonathan.Hi, my name is Dustin, and I workon the GPU Software team here at Apple.And today, I'm excited to show you a hands-on demooptimizing an early build of Larian Studios hit title,Divinity: Original Sin 2.Last year, Larian announced they werebringing their critically-acclaimedrole-playing game Divinity: Original Sin 2 to the iPad.And over the last year, Larian has worked hardoptimizing their game to run great on Apple GPU's,and the game is a lot of fun to play.Larian was able to achieve these results with the help of a greatset of tools in Metal Debugger and Metal System Tracethat are getting even better this year in Xcode 13with the addition of the new GPU Timeline.Let's get started by taking a look at a frameof Divinity: Original Sin 2 I captured earlier.We are here on the Summary Page, which contains an overviewof your frame that helps to guide you along the wayas you debug and optimize your game.From the Summary Page, we can quickly navigateto all the great tools offered by the Metal Debugger,including the new GPU Timeline.And accessing it is as easy as clickingon the new Performance page here.So, let me go ahead and do that.Introducing the new GPU Timeline.The Timeline has been designedaround Apple GPU's unique architecturethat allows each GPU pipeline stage to run in parallel.In order to maximize performance, we need to keepall pipeline stages as busy as possible by maximizing overlap,which the Timeline allows you to easily see.The Timeline is composed of two sections.On the top, we have the GPU section,which is composed of separate tracksfor each pipeline stage, making it really easy to seewhich stages are active and running in parallel.Underneath, we have the Counters section,which contains a curated set of important counters,such as shader occupancy, bandwidth,and performance limiters that provides uswith deeper insight into how the GPU's system performancechanges over the course of your workload.The encoders in the GPU tracks provide us with a lotof useful information, with even more just a click away.Selecting a Render Encoder brings upthe Timeline's sidebar, which containsadditional information for the currently-selected item.In this case, the sidebar containsrender pass information, such as texture details,load/store actions, and the number of draw calls.Notice that since Render Encoders are composedof two shader stages, both the vertexand fragment stages are highlighted, as well.If we select the Fragment track instead,the sidebar contains all of the encoders in the Timeline,which can then be sorted based on time.But that's not all because we can expand the Fragment trackto reveal the Shader Timeline, which shows all of the shadersused by the encoders during their execution.We can easily identify long-running shaders, as wellas which shaders are running in parallel with others.For the Fragment track, we also havetwo additional tracks for load/store actions.This is useful to be able to see when the GPUis loading and storing attachment textures betweenlocal and main memory, and is an importantconsideration in order to reduce bandwidth usage.Selecting a shader will highlight all the regionson the timeline where it is active,and we can learn more about it from its compiler statisticsand runtime performance metrics presented in the sidebar.Expanding the shader timeline shows each shaderin its own track, which is usefulfor understanding the flow of your GPU workloadand the order of shader execution.Now that you're a bit more familiarwith the new GPU Timeline and thinkingof all the ways that you will be able to use it yourself,let me show you just how how easy it isto find performance bottlenecks using the GPU Timeline.Shader performance can suffer as a result of many factors,one of which is register pressure,and when this happens, the GPU runs outof fast register memory and has to use main memory instead.A high ALU limiter alone does not indicatea performance bottleneck.It may just be that your shader is math heavy.However, when combined with low shader occupancy, this may bean indicator of a shader experiencing register pressure,which will cause your shader to run slower.In order to highlight this better for today's demo,let me pin both the ALU track and the shader occupancy trackto the top of the Timelineby clicking the "Plus" button here on the left.As I scan over these two tracks, the first thing I noticeis this region here, where the ALU spikes and,at the same time, shader occupancy drops.I can highlight a region on the Timeline to seehow long it takes to execute.Notice as I do this, the counters in the sidebarupdate dynamically based on the selected region.This region here is taking about 3.7 milliseconds to execute.Let's zoom in and take a closer look.It looks like our issue is relatedto these first four encoders of the Ambient Occlusion pass.Let's see what shaders are being usedby taking a look at the shader timeline.Looks like our issue is related to this shader here,as it's the only one being used.From the sidebar's runtime performance metrics, not only isthis shader ALU intensive, it is float heavy, as well,so let's take a lookat the Floating Point Utilization track.Notice as I hover over this track,this shader is only using F32.F16 is at 0%.From the Timeline, we can navigate directlyto shader source by right clicking and opening the shader.Here in the source editor, we can see a simplified versionof the shader source for demo purposes.Along with source, we can also see per-line cost informationwith the help of the shader profiler.Hovering over the shader profiler pie chart provides uswith confirmation that this function islikely causing register pressure,as it is both ALU and float heavy.Situations like this are candidates for using F16,which gives us double the amount of registers in placeswhere the full precision of F32 is not required,which will help to reduce register pressure.Metal Debugger makes it really convenientto update source code directly inside the source editor.Let me make this change here that uses an updated versionof the shader that uses a mixture of F32 and F16.After making this change, I can click the "Reload Shaders"button down here at the bottom, which will triggera shader update that both recompiles and reprofilesour shader, as well as updating the per-line shader costs.Let's see what effect this change has madeby going back to the Timeline.The first thing I'd like to do is seehow long those first four encodersof the Ambient Occlusion pass are taking.Looks like this region here is taking about 2.6 millisecondsto execute.The change we just made has improvedour shader execution timeby over one millisecond, or 30%, which is a huge improvement.Taking a look at some of the counters from earlier,while ALU is still high,that is to be expected for a math-heavy shader.But notice now, our shader is experiencingless register pressure, as our shader occupancyhas improved by almost double.This was accomplished by using a mixture of F32 and F16,which we can see usingthe Floating Point Utilization track.The GPU Timeline made it really easy for meto identify the issue, navigate to wherethe problem existed, and get it fixed.The GPU Timeline is a great toolfor identifying not only shader performance issues,but also memory bandwidth and many other kinds of issues.I hope you enjoyed this demo of the brand-new GPU Timelineand are already thinking of all the waysthat you're going to use it to optimize your gamesto run even better on Apple GPUs.Thank you, and enjoy the rest of WWDC.Back to Jonathan.Thank you, Dustin, for that amazing demo.And thank you for watching.It was great to share with you how we workedwith Larian Studios and 4A Games to take advantageof the features on our Apple GPUs.They provide many ways to improve performance, rangingfrom lossless compression to overlapping shader workloads.And our tools, like Metal System Traceand the new GPU timeline in Xcode,will really be helpful to you as you improve your games.If there's one thing I can leave you with, a thorough examinationof your rendering is essential to delivering a highly-optimizedgame, and our tools are there to help you with this.If you'd like to learn more,please refer to the related sessions,"Discover Metal debugging, profiling,and asset creation tools" in this year's WWDC,or "Optimize Metal apps and games with GPU counters"from WWDC20.Thank you, and farewell![music]

Welcome to WWDC.Hi, I'm Jonathan Metzgar.I'm a member of the Metal Ecosystem teamat Apple.We get to work with game developers to help themget the best graphics performance on our Apple GPUs.Dustin and I are going to show you howwe optimize high-end games for Apple GPUs.In this video, I'm going to coverthe process that we use to optimize games.Then, I'm going to show you the kindsof optimizations that are usedin the games Baldur's Gate 3 and Metro Exodus.And lastly, Dustin is going to do a toolsdemonstration featuring the game Divinity: Original Sin 2,while he introduces the new GPU Timeline in Xcode 13.Let's dive in and talk about optimization.So, over the past year, we collaboratedwith Larian Studios and 4A Games to find ways to tunethe graphics performance in their games for Apple GPUs.I am sure you'll be excited to see the details,and I want to take a moment and thank both Larian Studiosand 4A Games for giving us permission to showdevelopment materials in this presentation.Looking back over the course of the year,we have analyzed many games and identified some common scenariosthat affect graphics performance.You're probably interested in finding opportunitiesto optimize your own game, so we have geared this sessionto emphasize how our GPU tools are especially helpfulin pinpointing these problem areasand to suggest ways to solve them.And, in particular, I'd like to share some of the principlesour team uses to help developers optimize their games.When we optimize a graphics application, it's importantto have a methodology, a set of principlesthat define how we solve a particular problem.So, let me show you a four-step process.First, you need to choose what data to collect,or measure, so it will help you understandwhat's happening with your game.Soon after you begin measuring data,you will want to choose some performance targets,or where you want to be when you finish.You may decide the in-game locationto take your GPU frame captures and Metal system traces,the scene complexity, graphics settings,and other metrics important to you, like frame time.Then, you analyze the data to learn aboutthe behavior of your engine.In-depth analysis helps you find whereand why the bottlenecks are occurring.Once you know what is causing a bottleneck,then you can make improvements to the game,but normally you pick one or two at a time,so you can understand the impact of each change.Lastly, you verify your improvements by comparingsome new measurements with your original ones.Since optimization is a process, you will go back and repeatuntil your performance targets have been met.For these games, we use Xcode's Metal Debuggerto give us insights about their performanceand how their frame graphs are structured,and we use Metal System Trace in Instruments to learnabout a game's performance over time.It's a great idea to save a GPU trace fileand an Instruments trace file so you can haveyour before and after data, both before and after optimization.So, I have a little list of things you could consider,or look for, in your game.As I mentioned, Xcode and Instruments are great toolsto help you understand your Metal application.Optimization is about getting the best out of several areas,ranging from shader performance to memory bandwidth.Another area is getting good overlap across your vertex,fragment, and compute workloads.And while rendering several frames in flight,some Apple GPUs can overlap workloads between them.I'll show you some pointers to help you with resourcedependencies, which might prevent that overlap.And since some developers use a custom workflowfor their shaders, I'll show you howcompiler settings can affect performance.Lastly, I'll talk about how to reduce the impactof redundant bindings.Let's start with Baldur's Gate 3 from Larian Studios.Baldur's Gate 3 is an RPG buildingon a 20-year gaming legacy and stands outwith its cinematic visual effects.Our engagement with Larian Studioshelped us identify how they could optimizetheir amazing rendering engine for Apple GPUs.First, we started with a GPU frame capture,like the Ravaged Beach scene we see here.Then, we break down the scene into a frame graph.The frame graph is a breakdownof the order and purpose of each rendering pass.High-end games have many render passes specializing in achievinga certain visual effect, such as ambient occlusion,shadow mapping, post processing, and so on.Baldur's Gate 3 has a complex frame graph,so this is a simplified version.By using Xcode's Metal Debugger, we capture a GPU traceand use it to see all the render passes in the game.Clicking on Show Dependencies bringsup a visualization that you can pan and zoom.It shows how your render passes depend on the resultsof previous ones to help you understand what's going on.For example, I am zooming into this deferreddecal render stage to get more details.Next, I will show you the Instruments tools.We spend time analyzing games using the Instruments trace,using the Metal System Trace, or Game performance templates.Metal System Trace is ideal if you wanna focus on GPU executionand scheduling analysis, and Game Performanceexpands on that to help you with other issues,like thread stalls or thermal notifications.Let's choose Metal System Trace to seethe behavior of our engine from frame to frame.Instruments allows you to view several channelsof data along a timeline.Here, we find our first problem:An expensive workload in our render passes.An expensive workload might mean that we needto optimize a shader.For instance, we see a long compute shaderholding up the rest of our frame.We call these gaps "bubbles."Let's switch back over to the GPU traceand investigate this further.This is the "before" GPU trace.Let's change the grouping from API CALL to PIPELINE STATE.You may notice the pipeline statesare sorted by execution time.Let's check the first compute pipeline.We can expand the compute function detailsto take a closer look at its statistics.Notice here that there areover four-and-a-half-thousand instructions.That's quite a lot. So, what else?Let's see what resources are being usedby this compute function.Depending on the input data, this function uses upto 120 textures to produce the output.However, we discovered that only sixto 12 are actually used 90% of the time.So, let's talk about how this shader could be improved.Shaders that need to handle many different conditions can reservemore registers than necessary, and this can reducethe number of threads that run in parallel.Splitting your workload into smaller,more focused shaders, which need fewer registers,can improve the utilization of the shader cores.So, instead of selecting the appropriate algorithmin the shader, you would choose the appropriate shaderpermutation when you issue your GPU workload.Additionally, a shader function which uses too many registerscan result in register pressure, when an execution unit runs outof fast register memory and has to use device memory instead.That's one reason to use 16-bit types, like half,when appropriate, since they use half the registerspace than 32-bit types, like floats.In this case, Larian Studios already optimized their shaderto use half-precision floating point and decided to creatededicated shader variants, instead.So, let's see what happened.When comparing the numbers before, in the box on the left,with the numbers in the box on the right, the numberof instructions reduced by 84%, branches reduced 90%,registers reduced 25%, and texture reads reduced 92%.This shader variant is used 90% of the time.We can also see this in the Metal System Trace.Notice here, in the before trace,the bubbles we saw earlier.And here, in the after trace, they have been minimized.Larian Studios was able to reduce this shaderby eight milliseconds, on average.That is a huge win!If you look at your most expensive pipeline state objectsand shaders, you may find a complicated shaderthat could be simplified.This is especially true if the resultsof that shader are used by a later pass.This was a huge improvement for the game,but short of the developer's performance target.We just mentioned memory as an issue, and oneof the features of our GPUs is lossless compression,which is enabled in certain conditions.So, maybe there was a flag we either accidentally setor forgot to set.Lossless compression helps reduce bandwidth by compressingtextures when they are stored from tile to device memory.If you look at the Bandwidth Insightson the Summary page, you may noticeLossless Compression warnings for some textures.They tell you that these texturescan't be lossless compressed,and you may pay a bandwidth penalty.Metal Debugger will also let you know why these texturescan't be lossless compressed.Here we see it's because of the ShaderWrite usage flag.We can see all the usage flags by going to the memory section.Once in the memory section, we can filter by render targets.Then, right click on the table header,choose texture, and then usage.Now, we can sort by usageand find the textures using ShaderWrite.If you set the ShaderWrite or PixelFormatView flagwhen you create your textures,you will disable lossless compression.Let's take a look at these flags in more detail.The Unknown, ShaderWrite, and PixelFormatView flagsprevent your textures from being lossless compressed.The general rule of thumb is to use these flagsonly when required.For example, you would use the ShaderWrite flagif you use the write() method to store valuesin a texture from a fragment or compute function.Rendering to a texture bound as a color attachmentdoesn't require the ShaderWrite flag.And don't set the PixelFormatView optionif you only need to read the component valuesin a different order.Instead, create a texture view usinga swizzle pattern to specify the new order.Similarly, don't set the PixelFormatView option ifyour texture view only converts between linear space and sRGB.Check the documentation for more information.Shader optimization and lossless compressionare two techniques that have helped us out,but another problem area is getting good overlapacross the vertex, fragment, and compute channels.Let's take a look at two ways to optimize workloadsacross channels.First, we'll start by looking at our Metal System Trace again.Here, we can see that we have low overlapon our vertex, fragment, and compute channels.It would be nice to improve this to keep the GPU busy.One way to solve this problem is to see if we canrestructure the encoding order in our frame graph.In other words, we want to move this workover to where the vertex stage has very low occupancy.We would like to process those vertices earlier,along with the fragment stage of an earlier render pass.We can think of our frame graph as a listof rendering tasks, like this pseudocode example.Getting good overlap can be as simple as changingthe order of your render tasks in your frame graph.Some tasks may rely on results from earlier ones,but not always.It turns out that the CascadedShadowBuffer stage,which is vertex-shader heavy, could be moveda few tasks earlier, since it has few dependencies.And now, we see that our regionwith low overlap has better utilization of the vertexand fragment channels, giving us another 1 ms win.But there is another optimizationthat we can try out.Games often have two to three frames in flight.So, a cool feature in our tile-baseddeferred rendering, or TBDR architecture GPUs,is to overlap workloads from two frameswhen there are no resource dependencies between them.So, I'm going to show you how to optimizefor this possibility.Let's have a look at the GPU track in Instruments once again.Here, you can see that these framesare processed, almost serially.This is caused by using a blit encoder to updateconstant buffers, like per-frame animation data, and so on.To efficiently update constant buffer data with a discrete GPU,we blit from shared buffers on the CPU to a private bufferon the GPU, which will be used for rendering the frame.This strategy is efficient for GPUs with discrete memory,so you want to keep this behavior for that purpose.If your device has a unified memory architecture,then there is no need to use a blit encoderto copy your data to a private buffer.However, when you use a shared buffer in a ring-buffer pattern,you need to watch out for synchronization issuesbecause visual corruption can happen if your CPUwrites to data currently being read by the GPU.Let's see this in action.Here, you can see in this diagramthe encoding and rendering of our frames.We are using colors to representthe shared buffers, which are updatedat the beginning of the frame: blue for buffer one,green for buffer two, and yellow for buffer three.Ring buffers are typically used to implement queues,which need to use a compact amount of memory.Here, there is no concern of a data race conditionwith this arrangement, as our writing and readingof our shared buffers is mutually exclusive.It's very common to have latency between encodingthe frame and the rendering of a frame.This causes a shiftof when the rendering actually begins.As long as the latency isn't too long,you will not have a data race condition.However, what happens if latency continues to increase?Well, this introduces a data race condition,where the main thread is updating its shared buffersduring the time the GPU is rendering the frame.And if that happens, you could get visual corruptionif elements of your frame are dependent on this data.In the case of Baldur's Gate 3, removing the private bufferand blit encoder eliminatedthe synchronization point, but introduced a race condition,which affected their temporal anti-aliasing render pass.So, let's see how to avoid this situation.To avoid this race condition,you need to make sure you are not writinginto the same resource the GPU is reading from.For example, you could utilizea completion handler, and then wait until it is safeto update the shared buffer in your encoding thread.But let me show you how we avoided a wait time.We maintained our completion handler, but added an extrabuffer to our ring buffer to avoid the wait.The extra buffer is colored purple on the bottom diagram.The memory consumption remains the same as with a discrete GPU.But if you need to save on memory,and the CPU wait time doesn't affect frame rate of your game,then you can just use three buffers.So, let's look at an easy way to decide how many sharedand private buffers to create with a pseudocode example.In this code snippet, you can see how to choosethe number of shared and private buffers at initialization time.Once we have created our device,we can check to see if the device hasunified memory or not, and then ensure that we createan extra shared buffer, or to use a private buffer.This extra buffer will help reducethe impact of waiting for a completion handler,which we are using to avoid a data race condition.And now, we can see how Fragment workloads from the previousframe overlaps with Vertex workloads from the next frame.Overall, this can give us oneto two milliseconds, depending on the scene.And, of course, this approach can be applied notonly for the constant buffer data we'veshown in this example, but for all of the buffer datayou transfer from the CPU to the GPU.So, let's review.Larian Studios was able to achieve their performancetargets by applying the following optimizations:Optimizing their most expensive shaders to reduce bubbles,opting in to lossless compression to improvebandwidth, overlapping vertex and fragment workloadsto get better GPU utilization, and checkingfor resource dependencies that prevent frame overlap.When they were finished, Larian Studios not only mettheir performance targets, but got a 33% improvementin frame time for their game.And now, we will look at a different setof optimizations with the game Metro Exodus.Metro Exodus is known for its epic storylineand demanding visual effects, as you can seein this series of game-play clips.After the integration of our suggested optimizations,4A Games was able to meet their performance targets.So now, let's have a look at an in-game scenefrom Metro Exodus.Metro Exodus uses a custom workflow to translaterender commands into Metal API commands,which is quite common for cross-platform games.The translation layer they are using is optimizedfor Metal, but some issues can arisewhen two complex systems come together in practice.So, additional performance tuningwas required to meet their project goals.As in the previous game, we startby investigating how a frame is being rendered.Modern renderers have a lot of different techniques involvedso first we try to understand the high-level frame graph.Again, we start analysis by looking at the GPU trace.It always gives us useful insights about game performance.So first, let's start with the GPU time,which doesn't meet the developer performance targets.So, let's find the shader or pipelinewhich is the most time-consuming.To do this, we are going to group by pipeline stateonce again and look at the most expensive one.Let's quickly look at its statistics.You can see that there is a high number of ALU instructionscompared to the total, meaning this is a math-heavy shader.We also see that the number of registers being usedby the shader is quite high.The number of registers used by a particular shader directlyaffects how its workload will scale during execution.The higher this number is, the less work can be donein parallel by the GPU.Sometimes it's just a complex shader, such as SSAOin this example, that requires lotsof computations and registers, but sometimesthe compiler settings can affect the generated instructionsand register allocation, as well.Let's also take a look at the shader compiler options.And it turns out, this shader was compiledwith the fast math flag disabled.Fast math allows the shader compiler to optimizevarious instructions, and it is enabledfor the Metal shader compiler, by default.However, there might be some cases,for example, using custom shader workflows,that can disable this compilation flag.In this case, we discovered that the translation layer,which 4A Games was using to invoke the compiler,had its default behavior set to not use fast math.So, what is fast math?Fast math is a set of optimizationsfor floating-point arithmetic,that trades between speed and correctness.For example, assumptions can be made that there will be no NANs,infinity, or signed zeros as either a result or argument.Fast math optimizations can also applyalgebraically-equivalent transformations,which may affect the precision in floating-point results.However in most scenarios,fast math is a great choice for games.This can significantly improve performance,especially in ALU-bound cases.Our recommendation to you is to check your compiler optionsto verify that you have enabled fast math, if your shaders donot depend on the things that we just mentioned.

The fast math flag works at the front-and back-end compiler levels.When you are building your shader source,the front-end shader compiler will select fast math functions,which will be used in intermediate code.This will hint to the back-end shader compiler that it cangenerate more optimal GPU machine code.Here, you can see how the Instructionsand Register counters on the left have been improvedin the box on the right after we recompiled this shader.So, after changing the behavior of the translation layerto enable fast math for all the shaders,we got a 21% frame time decrease in our test workloadusing the built-in game benchmark.So, the next area I wanna talk about is redundant bindings.If we go back to the summary page,and look at the API insights, we can see there aremany redundant bindings when rendering the frame.Redundant bindings can be either resources like textures,buffers, and samplers; or render stateslike depth stencil state, viewport configuration, etc.Repeatedly binding resources might negatively affectyour encoding time, but redundant renderstate changes may also affect the GPU time.Let's have a look at the encodingand GPU times in the Metal System Trace.For a given frame, it takes eight-and-a-half millisecondsfor all the commands to be encodedand around 22 millisecondsfor the GPU to render this frame.When we investigated the cause of the redundant bindings,we found that the translation layercould be modified to reduce them.So, let me show you a pseudocode example which shows howto check for and reduce redundant bindings.Instead of binding textures directly to the encoder,you can pre-cache them and only bind them if they change.And to minimize interactions with the API, you can set allthe textures with one call to the setFragmentTextures methodinstead of setting them in a loop, one by one.Additionally, you can apply a similar approachto other shader stages and other binding types,like buffers and samplers, as well as render states.So, let's see what happened in the Metal System Trace.4A Games was able to reduce encoding timebetween 30% and 50%, depending on the scene,because the translation layer wasn't repeatedlybinding the same resources and render states.However, GPU time also decreased by upto three milliseconds and, overall,resulted in a 15% speedup in their in-game benchmark.If you have a few redundant binding warnings,it's not an issue, but we definitely see an impactwith hundreds or thousands of redundant bindings.So, avoiding redundant bindings gave us a further 15% reductionin average frame time.After these two improvements, 4A Games was able to meettheir performance targets.So now, let's summarize what we learnedfrom optimizing Metro Exodus for Apple GPUs.First, if you're using a custom workflow for shaders,you should check your compiler settings to ensureyou are using the best options for your Metal applications.And if you see a lot of redundant binding warningsin the Metal Debugger, I showed you a techniqueto reduce encoding and GPU time overhead,which you can apply either to your engineor the translation layer that you are using.And now, I'd like to hand it over to Dustin,who's going to talk to you about Divinity: Original Sin 2and demo the new Xcode GPU timeline features.Thanks, Jonathan.Hi, my name is Dustin, and I workon the GPU Software team here at Apple.And today, I'm excited to show you a hands-on demooptimizing an early build of Larian Studios hit title,Divinity: Original Sin 2.Last year, Larian announced they werebringing their critically-acclaimedrole-playing game Divinity: Original Sin 2 to the iPad.And over the last year, Larian has worked hardoptimizing their game to run great on Apple GPU's,and the game is a lot of fun to play.Larian was able to achieve these results with the help of a greatset of tools in Metal Debugger and Metal System Tracethat are getting even better this year in Xcode 13with the addition of the new GPU Timeline.Let's get started by taking a look at a frameof Divinity: Original Sin 2 I captured earlier.We are here on the Summary Page, which contains an overviewof your frame that helps to guide you along the wayas you debug and optimize your game.From the Summary Page, we can quickly navigateto all the great tools offered by the Metal Debugger,including the new GPU Timeline.And accessing it is as easy as clickingon the new Performance page here.So, let me go ahead and do that.Introducing the new GPU Timeline.The Timeline has been designedaround Apple GPU's unique architecturethat allows each GPU pipeline stage to run in parallel.In order to maximize performance, we need to keepall pipeline stages as busy as possible by maximizing overlap,which the Timeline allows you to easily see.The Timeline is composed of two sections.On the top, we have the GPU section,which is composed of separate tracksfor each pipeline stage, making it really easy to seewhich stages are active and running in parallel.Underneath, we have the Counters section,which contains a curated set of important counters,such as shader occupancy, bandwidth,and performance limiters that provides uswith deeper insight into how the GPU's system performancechanges over the course of your workload.The encoders in the GPU tracks provide us with a lotof useful information, with even more just a click away.Selecting a Render Encoder brings upthe Timeline's sidebar, which containsadditional information for the currently-selected item.In this case, the sidebar containsrender pass information, such as texture details,load/store actions, and the number of draw calls.Notice that since Render Encoders are composedof two shader stages, both the vertexand fragment stages are highlighted, as well.If we select the Fragment track instead,the sidebar contains all of the encoders in the Timeline,which can then be sorted based on time.But that's not all because we can expand the Fragment trackto reveal the Shader Timeline, which shows all of the shadersused by the encoders during their execution.We can easily identify long-running shaders, as wellas which shaders are running in parallel with others.For the Fragment track, we also havetwo additional tracks for load/store actions.This is useful to be able to see when the GPUis loading and storing attachment textures betweenlocal and main memory, and is an importantconsideration in order to reduce bandwidth usage.Selecting a shader will highlight all the regionson the timeline where it is active,and we can learn more about it from its compiler statisticsand runtime performance metrics presented in the sidebar.Expanding the shader timeline shows each shaderin its own track, which is usefulfor understanding the flow of your GPU workloadand the order of shader execution.Now that you're a bit more familiarwith the new GPU Timeline and thinkingof all the ways that you will be able to use it yourself,let me show you just how how easy it isto find performance bottlenecks using the GPU Timeline.Shader performance can suffer as a result of many factors,one of which is register pressure,and when this happens, the GPU runs outof fast register memory and has to use main memory instead.A high ALU limiter alone does not indicatea performance bottleneck.It may just be that your shader is math heavy.However, when combined with low shader occupancy, this may bean indicator of a shader experiencing register pressure,which will cause your shader to run slower.In order to highlight this better for today's demo,let me pin both the ALU track and the shader occupancy trackto the top of the Timelineby clicking the "Plus" button here on the left.

As I scan over these two tracks, the first thing I noticeis this region here, where the ALU spikes and,at the same time, shader occupancy drops.I can highlight a region on the Timeline to seehow long it takes to execute.Notice as I do this, the counters in the sidebarupdate dynamically based on the selected region.This region here is taking about 3.7 milliseconds to execute.Let's zoom in and take a closer look.It looks like our issue is relatedto these first four encoders of the Ambient Occlusion pass.Let's see what shaders are being usedby taking a look at the shader timeline.Looks like our issue is related to this shader here,as it's the only one being used.From the sidebar's runtime performance metrics, not only isthis shader ALU intensive, it is float heavy, as well,so let's take a lookat the Floating Point Utilization track.Notice as I hover over this track,this shader is only using F32.F16 is at 0%.From the Timeline, we can navigate directlyto shader source by right clicking and opening the shader.Here in the source editor, we can see a simplified versionof the shader source for demo purposes.Along with source, we can also see per-line cost informationwith the help of the shader profiler.Hovering over the shader profiler pie chart provides uswith confirmation that this function islikely causing register pressure,as it is both ALU and float heavy.Situations like this are candidates for using F16,which gives us double the amount of registers in placeswhere the full precision of F32 is not required,which will help to reduce register pressure.Metal Debugger makes it really convenientto update source code directly inside the source editor.Let me make this change here that uses an updated versionof the shader that uses a mixture of F32 and F16.After making this change, I can click the "Reload Shaders"button down here at the bottom, which will triggera shader update that both recompiles and reprofilesour shader, as well as updating the per-line shader costs.Let's see what effect this change has madeby going back to the Timeline.The first thing I'd like to do is seehow long those first four encodersof the Ambient Occlusion pass are taking.

Looks like this region here is taking about 2.6 millisecondsto execute.The change we just made has improvedour shader execution timeby over one millisecond, or 30%, which is a huge improvement.Taking a look at some of the counters from earlier,while ALU is still high,that is to be expected for a math-heavy shader.But notice now, our shader is experiencingless register pressure, as our shader occupancyhas improved by almost double.This was accomplished by using a mixture of F32 and F16,which we can see usingthe Floating Point Utilization track.The GPU Timeline made it really easy for meto identify the issue, navigate to wherethe problem existed, and get it fixed.The GPU Timeline is a great toolfor identifying not only shader performance issues,but also memory bandwidth and many other kinds of issues.I hope you enjoyed this demo of the brand-new GPU Timelineand are already thinking of all the waysthat you're going to use it to optimize your gamesto run even better on Apple GPUs.Thank you, and enjoy the rest of WWDC.Back to Jonathan.Thank you, Dustin, for that amazing demo.And thank you for watching.It was great to share with you how we workedwith Larian Studios and 4A Games to take advantageof the features on our Apple GPUs.They provide many ways to improve performance, rangingfrom lossless compression to overlapping shader workloads.And our tools, like Metal System Traceand the new GPU timeline in Xcode,will really be helpful to you as you improve your games.If there's one thing I can leave you with, a thorough examinationof your rendering is essential to delivering a highly-optimizedgame, and our tools are there to help you with this.If you'd like to learn more,please refer to the related sessions,"Discover Metal debugging, profiling,and asset creation tools" in this year's WWDC,or "Optimize Metal apps and games with GPU counters"from WWDC20.Thank you, and farewell![music]

15:24 -Pseudocode to choose shared and private buffer count

21:40 -Pseudocode to avoid redundant bindings

## Code Samples

```swift
// Number of frames application is processing


static
 
const
 uint32_t MAX_FRAMES_IN_FLIGHT = 
3
;

uint32_t sharedBuffersCount  = 
0
; 
// Number of buffers with MTLStorageModeShared to create

uint32_t privateBuffersCount = 
0
; 
// Number of buffers with MTLStorageModePrivate to create


if
 (device.hasUnifiedMemory)
{     
// Use extra buffer to reduce impact of completion handler

    sharedBuffersCount = MAX_FRAMES_IN_FLIGHT + 
1
;
    privateBuffersCount = 
0
;
}

else
 
// GPUs with dedicated memory

{
    sharedBuffersCount = MAX_FRAMES_IN_FLIGHT;
    privateBuffersCount = 
1
;
}


// Create shared buffers MTLStorageModeShared


// If applicable, create private buffer
```

```swift
void
 Renderer::SetFragmentTexture(uint32_t index, 
id
<
MTLTexture
> texture)
{
    
if
 (m_FragmentTextures[index] != texture)
    {
        m_FragmentTextures[index] = texture;
        m_FragmentTexturesChanged = 
true
;
    }
}


void
 Renderer::BindFragmentTextures()
{
    
if
 (m_FragmentTexturesChanged)
    {
        [m_RenderCommandEncoder setFragmentTextures:m_FragmentTextures 
                          withRange:
NSMakeRange
(
0
, m_LastFragmentTexture + 
1
)];

        m_FragmentTexturesChanged = 
false
;
    }
}
```

