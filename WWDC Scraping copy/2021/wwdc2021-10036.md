# Wwdc2021 10036

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Discover built-in sound classification in SoundAnalysisExplore how you can use the Sound Analysis framework in your app to detect and classify discrete sounds from any audio source — including live sounds from a microphone or from a video or audio file — and identify precisely in a moment where that sound occurs. Learn how the built-in sound classifier makes it easy for you to identify over 300 different types of sounds without the need for a custom trained model. This includes a variety of noises, ranging from human sounds, musical instruments, animals, and various items.

For custom models, see how you can leverage the Audio Feature Print feature extractor to create smaller models with variable sound window control to better serve your app's purposes.

For more about Sound Classification and the Sound Analysis framework, watch “Training Sound Classification Models in Create ML” from WWDC19.ResourcesClassifying Live Audio Input with a Built-in Sound ClassifierSound AnalysisHD VideoSD VideoRelated VideosWWDC22Compose advanced models with Create ML ComponentsWWDC21Explore ShazamKitTuesday@WWDC21WWDC19Training Sound Classification Models in Create ML

Explore how you can use the Sound Analysis framework in your app to detect and classify discrete sounds from any audio source — including live sounds from a microphone or from a video or audio file — and identify precisely in a moment where that sound occurs. Learn how the built-in sound classifier makes it easy for you to identify over 300 different types of sounds without the need for a custom trained model. This includes a variety of noises, ranging from human sounds, musical instruments, animals, and various items.

For custom models, see how you can leverage the Audio Feature Print feature extractor to create smaller models with variable sound window control to better serve your app's purposes.

For more about Sound Classification and the Sound Analysis framework, watch “Training Sound Classification Models in Create ML” from WWDC19.

Classifying Live Audio Input with a Built-in Sound Classifier

Sound Analysis

HD VideoSD Video

HD Video

SD Video

Compose advanced models with Create ML Components

Explore ShazamKit

Tuesday@WWDC21

Training Sound Classification Models in Create ML

Search this video…♪ ♪Hi.Welcome to the Sound Analysis session at WWDC.My name is Jon Huang.I'm a researcher on the audio team.Today, my colleague, Kevin, and Iwill introduce enhancements to sound classificationmade available through the SoundAnalysis frameworkand CreateML.In 2019, we made it possibleto train sound classification models using CreateML.We showed it was easy to create sound classification modelsand to deploy them in Apple devices.When you use this framework,all of the computationsare optimized for hardware accelerationand are done locally on-device.This helps preserve your user's privacybecause audio is never sent to the Cloud.We leveraged the Sound Analysis framework to introducethe Accessibility Feature called Sound Recognition.This feature can provide notifications to the userwhen certain sounds are heard in the environmentlike alarms, pets,and other household sounds.This is just one application of sound classification.Let's see what else we can do with it.The demo app is using my Mac's built-in microphoneto listen to sounds in the environment.It's passing audio through a sound classifierand displaying the classification resultsin the UI.So as I'm chatting to you, speech is detected.Please sit back with me for a bit and see what happens.Make yourself comfortable.Let's start with some music.Hey Siri, play "Catch a Vibe" by Karun and Mombru.Now playing "Catch a Vibe" by Karun and Mombru.♪ ♪♪ Oh, oh, oh ♪Notice the classifier is picking upboth music and singing sounds.♪ I don't know anyone ♪♪ Who feels so right ♪♪ Don't know anyone who makes me ♪♪ Catch a vibe, feel the frequency...♪Now please join me for some tea.♪ Feel all right, baby ♪♪ We could see ♪♪ If we could be right ♪♪ See if it's something about the way you smile ♪♪ Hold your breath and, baby, we could ♪♪ Catch a vibe, feel the frequency ♪♪ Catch a vibe, only you and me ♪♪ Feel all right, baby, we could see ♪♪ If we could be ♪♪ I've said, I've said ♪♪ It's part of me ♪♪ All the part you see ♪♪ You should be proud of...♪This is good tea.♪ You're my baby, gee ♪♪ You a cart, I see... ♪♪ How could this be? ♪♪ How could this be?... ♪Hey, Siri. Stop.Now it's reasonable to assumethat I collected some data for each of these sound categoriesand trained a custom model using CreateML.Yes, I could have done that,but actually, the classifier used is built-in.New to this year, we have a sound classifierbuilt right into the Sound Analysis framework.It's never been easier to enablesound classification in your app.It's supported on all platforms.The built-in classifier is all about simplifyingthe developer experience.It removes the need for collecting and labelingmassive amount of data,specialized machine learning and audio expertise,and lots of compute powerin order to develop a high-accuracy model.The less time you have to worry about these details,the more time you can spendenriching the user experience in your app.It only takes a few lines of codeto enable sound classification.I'll show you what this classifier can do.There's over 300 categories for you to use.Let's take a closer look.We can classify sounds of domestic animals,livestock animals, and wild animals.For music, many instruments can be recognized:keyboard instruments, percussion,string instruments, wind instruments.We can detect various human sounds:group activities,respiratory sounds, vocalizations.Then there's sounds of things like vehicles,alarms, tools, liquids,and so much more.These ready-to-use categories are available for you to try.I'm going to turn it over to Kevinto walk you through how to use this sound classifier.Thanks, Jon.Hi, I'm Kevin,a software engineer on the Audio team.I'd like to show you how to usethe new built-in sound classifierby taking a look at a small application I built.I was hoping Jon would showhow well the classifier works with cowbell,but since it didn't fit in his demo,I came up with another idea.I have some media on my Mac that I collectedwhile preparing for this session,and I'm pretty sure that I havesome old footage containing cowbell.I'd love to show it to you,but first I have to find it.This means I'll have to find the right file,and I'll have to look insideto find just the right part.So how will I do this?I'll write a simple programthat uses the built-in sound classifierto read a file and tell mewhether a certain sound is inside.If the sound is found,the program can use the classifierto tell me the time at which it occurred.I can then use Shortcuts,now available on macOS,to create a workflowthat runs my program over lots of files.When my program finds a sound,the workflow can automaticallyuse the reported detection timeto extract a video clip containing the sound.This will work great for finding a cowbell clip.So let's see it in action.Here, I have a folder full of videos.I'll select all the filesand kick off my shortcut using the Quick Actions menu.I'm asked to select the sound that I'd like to find,so from the list of options, I'll choose Cowbell.Now, my shortcut is running,and after just a few moments,it finds the sound I'm looking forand shows it to me in a Finder window.Let's take a look.Looking good, Jon.Let's take a closer look at the shortcut that I used.When my shortcut starts,it collects a list of all the soundsthat it can recognize,and it asks me to pick one.Using my choice, it visits each of the filesthat I selected in Finderand looks for the sound inside.When my sound is detected,it extracts a few seconds from the videoaround the time that the sound occurredand shows me the resulting clip.Of these steps, two of them make useof the built-in classifier.These are the steps that I implementedin my own custom applicationfor Shortcuts to use on demand.Though I won't talk about Shortcuts in more detail,if you're curious to learn more,please refer to the WWDC sessionentitled "Meet Shortcuts for macOS."Now let's look at the implementationfor my app's two custom actions.The first action reports all of the soundsthat the app can recognize.Since the app is using the built-in sound classifier,it can recognize a few hundred sounds.Here's a function I wrote to get these sounds.I create an SNClassifySoundRequestusing a new initializer that allows me to selectthe built-in classifier.Once I have this request,I can use it to query the list of soundsthat the classifier supports.The app's second action tells Shortcutswhen a sound is heard within a file.To implement this, I'll perform sound classificationand report back the first timethat the sound is detected, if it's detected at all.To perform sound classification,I'll need to prepare three objects.First, I'll need an SNClassifySoundRequest,which I can use to configure sound classification.Second, I'll need an SNAudioFileAnalyzer,which will let me target classificationtoward a particular file.The third object will requirea little extra attention.I'll need to define my own Observer type,which will handle the results of classification.Skipping the Observer for now,here's some code for preparing the first two of these objects.I can create the SNClassifySoundRequestusing the built-in classifier,and I can create an SNAudioFileAnalyzerusing a URL to the file I want to classify.If, at this point, I had an Observer ready to go,it would be easy to start sound classification,but defining that Observer is the missing piece.So let's do that.I'm starting here with a bare Observerthat inherits from NSObjectand conforms to the SNResultsObserving protocol.I'll initialize instanceswith the sound label I want to search for,and I'll add a CMTime member variableto store the time at which I detect the sound.I just need to implement the request:didProduce result method.This method will be called when results are producedby sound classification,so I expect to receive instancesof SNClassificationResult.I can use the classificationForIdentifiermethod of the result to extract informationabout the label that I'm searching for.I'll query the confidence score associated with the label,and if that score exceeds a certain threshold,I'll consider the sound to be detected.When I notice the detection for the first time,I'll save the time at which the sound occurredso that I can provide it to Shortcuts later.With that, my Observer is complete,and I have all the pieces neededto determine when a sound occurs within a file.This example touches on two important topicsthat I'd like to discuss further.First, I'll talk about time of detection,and then I'll talk about detection thresholds.Let's start with time of detection.When you classify audio,the signal gets broken up into overlapping windows.For each of these windows,you'll get a resultthat tells you what sounds were detectedand how confidently.You'll also get a time range,which tells you what partof the audio was classified.In my app, when I detect a sound,I use the result's time rangeto determine when the sound occurred.But the time range can be impactedwhen the duration of a window changes.You can customize the window durationto make it big or smallbased on your use case.Short windows work well when working with short sounds,like a drum tap.This is because you can captureall the important features of that soundwithin a small window of time.The small window doesn't cut outany important information.The advantage of using a small window durationis that it allows you to more closely pinpointthe moment at which a sound occurred.But small window durations may not be appropriatewhen working with longer sounds.A siren, for example,may contain both rising and falling pitchesover a longer period of time.Capturing all of these pitches together in a single windowcan help sound classificationcorrectly detect the sound.In general, it's good to use a window durationlong enough to capture all the important partsof the sounds that you're interested in.If you'd like to edit the window duration,you can set the windowDuration propertyof SNClassifySoundRequest.Note, though,that not all window durations are supported.Different classifiers might supportdifferent window durations.You can check what window durations are supportedby reading the windowDurationConstraintproperty of SNClassifySoundRequest.The built-in classifier supports window durationsbetween 1/2 second long and 15 seconds long.A duration of one second or longeris a great starting pointwhen adopting the classifier in your app.Let's talk next about confidence thresholds.In my app, I considered a sound to be detectedany time the confidence for that sound roseabove a fixed threshold.I chose a value of 0.5 for my threshold,but there are some things to considerwhen choosing a threshold for your own app.The classifier can detectmultiple sounds at the same time.When this happens, you may noticethat several labels score with high confidence.Unlike when using a custom model trained using CreateML,label scores do not add upto a value of one.The confidences are independentand shouldn't be compared against one another.Because confidence scores are independent,you may find it useful to choosedifferent confidence thresholds for different sounds.Choosing a threshold involves a tradeoff.A higher confidence threshold reduces the probabilitythat a sound will be falsely detected,but it also increases the probabilitythat a true detection will be missedbecause it wasn't strong enough.When you select a threshold for your application,you'll need to find a value that achievesthe right balance of these factors for your use case.Note that confidence scores may changewhen you set a custom window duration,so this can impact your thresholds as well.One final thing to keep in mindwhen using the built-in classifieris that some sounds are similar.Among the large number of sounds that the classifier can identifyare several groups of sounds that can be difficultto differentiate using audio alone,even for a human.Where possible, it's best to be selectiveabout the sounds that you pay attention to.You should try to watch only for soundsthat are likely to occur in the contextswhere your app will be used.With that, let's turn back to Jonto learn about what's new in CreateMLregarding sound classification.Thanks, Kevin, for that great example.I'm glad you had fun with that cowbell video.Now let me show you what's new in CreateMLor, specifically,how you can improve your custom modelsby leveraging the power of the built-in classifier.The built-in classifier was trained with a ton of dataacross a vast number of categories.So the model actually contains a lot of knowledgeabout sound classification.All this knowledge can be utilizedfor the training of your custom model using CreateML.I'll show you how this works.The sound classifier can be separatedinto two different networks.The first part is the feature extractor,and the second part is the classifier model.The feature extractor,sometimes known as the embedding model,is the backbone of the network.It takes an audio waveform and transforms itinto a low-dimensional space.A well-trained feature extractororganizes acoustically similar soundsinto nearby locations in the space.For example,sounds of guitar would cluster togetherbut are placed away from drum and car sounds.Now, the second part of this pipelineis the classifier model.It takes the output of the feature extractorand computes the class probabilities.The classifier benefits from being pairedwith a good feature extractor,like the one we have embedded into the built-in classifier.We're making the built-in classifier'sfeature extractor available to you.It is called Audio Feature Print.When you train your own custom model in CreateML,your model will be paired with Audio Feature Print.With this, your model benefitsfrom all the knowledge contained in the built-in classifier.Compared to the previous generation feature extractor,Audio Feature Print has improvements across the board.Even though this network is smaller and faster,it achieves higher accuracyon all benchmark data sets we compared against.And like the built-in classifier,models using Audio Feature Print supporta flexible window duration.You can select a long window duration to optimizefor sounds like sirensor a short window duration for sounds like drum taps.Audio Feature Print is the new default feature extractorwhen you train a custom model using CreateML.The Window duration is the length of audio usedto generate a single feature while training.It defaults to three seconds,but you can adjust it to suit your needs.CreateML gives you the option to selecta window duration between 1/2 second to 15 seconds.For a more detailed example of training a custom model,you can check out the 2019 sessionon "Training Sound Classification Modelsin CreateML."It also shows you how to use the Sound Analysis Frameworkto run the custom model.Thanks for joining usin the session on Sound Analysis.Today, we introduced a powerful new sound classifierbuilt into the OS.Along with it,we've upgraded the feature extractor in CreateML.These will unlock new possibilities,and we can't wait to see what you'll do with them in your app.Enjoy the rest of WWDC.[upbeat music]

♪ ♪Hi.Welcome to the Sound Analysis session at WWDC.My name is Jon Huang.I'm a researcher on the audio team.Today, my colleague, Kevin, and Iwill introduce enhancements to sound classificationmade available through the SoundAnalysis frameworkand CreateML.In 2019, we made it possibleto train sound classification models using CreateML.We showed it was easy to create sound classification modelsand to deploy them in Apple devices.When you use this framework,all of the computationsare optimized for hardware accelerationand are done locally on-device.This helps preserve your user's privacybecause audio is never sent to the Cloud.We leveraged the Sound Analysis framework to introducethe Accessibility Feature called Sound Recognition.This feature can provide notifications to the userwhen certain sounds are heard in the environmentlike alarms, pets,and other household sounds.This is just one application of sound classification.Let's see what else we can do with it.The demo app is using my Mac's built-in microphoneto listen to sounds in the environment.It's passing audio through a sound classifierand displaying the classification resultsin the UI.So as I'm chatting to you, speech is detected.Please sit back with me for a bit and see what happens.Make yourself comfortable.Let's start with some music.Hey Siri, play "Catch a Vibe" by Karun and Mombru.

Now playing "Catch a Vibe" by Karun and Mombru.♪ ♪♪ Oh, oh, oh ♪Notice the classifier is picking upboth music and singing sounds.♪ I don't know anyone ♪♪ Who feels so right ♪♪ Don't know anyone who makes me ♪♪ Catch a vibe, feel the frequency...♪Now please join me for some tea.♪ Feel all right, baby ♪♪ We could see ♪♪ If we could be right ♪♪ See if it's something about the way you smile ♪♪ Hold your breath and, baby, we could ♪♪ Catch a vibe, feel the frequency ♪♪ Catch a vibe, only you and me ♪♪ Feel all right, baby, we could see ♪♪ If we could be ♪♪ I've said, I've said ♪♪ It's part of me ♪♪ All the part you see ♪♪ You should be proud of...♪This is good tea.♪ You're my baby, gee ♪♪ You a cart, I see... ♪♪ How could this be? ♪♪ How could this be?... ♪Hey, Siri. Stop.Now it's reasonable to assumethat I collected some data for each of these sound categoriesand trained a custom model using CreateML.Yes, I could have done that,but actually, the classifier used is built-in.New to this year, we have a sound classifierbuilt right into the Sound Analysis framework.It's never been easier to enablesound classification in your app.It's supported on all platforms.

The built-in classifier is all about simplifyingthe developer experience.It removes the need for collecting and labelingmassive amount of data,specialized machine learning and audio expertise,and lots of compute powerin order to develop a high-accuracy model.The less time you have to worry about these details,the more time you can spendenriching the user experience in your app.It only takes a few lines of codeto enable sound classification.I'll show you what this classifier can do.There's over 300 categories for you to use.Let's take a closer look.

We can classify sounds of domestic animals,livestock animals, and wild animals.

For music, many instruments can be recognized:keyboard instruments, percussion,string instruments, wind instruments.We can detect various human sounds:group activities,respiratory sounds, vocalizations.

Then there's sounds of things like vehicles,alarms, tools, liquids,and so much more.These ready-to-use categories are available for you to try.I'm going to turn it over to Kevinto walk you through how to use this sound classifier.Thanks, Jon.Hi, I'm Kevin,a software engineer on the Audio team.I'd like to show you how to usethe new built-in sound classifierby taking a look at a small application I built.I was hoping Jon would showhow well the classifier works with cowbell,but since it didn't fit in his demo,I came up with another idea.I have some media on my Mac that I collectedwhile preparing for this session,and I'm pretty sure that I havesome old footage containing cowbell.I'd love to show it to you,but first I have to find it.This means I'll have to find the right file,and I'll have to look insideto find just the right part.So how will I do this?I'll write a simple programthat uses the built-in sound classifierto read a file and tell mewhether a certain sound is inside.If the sound is found,the program can use the classifierto tell me the time at which it occurred.

I can then use Shortcuts,now available on macOS,to create a workflowthat runs my program over lots of files.When my program finds a sound,the workflow can automaticallyuse the reported detection timeto extract a video clip containing the sound.This will work great for finding a cowbell clip.

So let's see it in action.

Here, I have a folder full of videos.

I'll select all the filesand kick off my shortcut using the Quick Actions menu.

I'm asked to select the sound that I'd like to find,so from the list of options, I'll choose Cowbell.

Now, my shortcut is running,and after just a few moments,it finds the sound I'm looking forand shows it to me in a Finder window.Let's take a look.Looking good, Jon.Let's take a closer look at the shortcut that I used.

When my shortcut starts,it collects a list of all the soundsthat it can recognize,and it asks me to pick one.Using my choice, it visits each of the filesthat I selected in Finderand looks for the sound inside.When my sound is detected,it extracts a few seconds from the videoaround the time that the sound occurredand shows me the resulting clip.Of these steps, two of them make useof the built-in classifier.These are the steps that I implementedin my own custom applicationfor Shortcuts to use on demand.

Though I won't talk about Shortcuts in more detail,if you're curious to learn more,please refer to the WWDC sessionentitled "Meet Shortcuts for macOS."Now let's look at the implementationfor my app's two custom actions.The first action reports all of the soundsthat the app can recognize.Since the app is using the built-in sound classifier,it can recognize a few hundred sounds.Here's a function I wrote to get these sounds.I create an SNClassifySoundRequestusing a new initializer that allows me to selectthe built-in classifier.Once I have this request,I can use it to query the list of soundsthat the classifier supports.The app's second action tells Shortcutswhen a sound is heard within a file.To implement this, I'll perform sound classificationand report back the first timethat the sound is detected, if it's detected at all.To perform sound classification,I'll need to prepare three objects.First, I'll need an SNClassifySoundRequest,which I can use to configure sound classification.Second, I'll need an SNAudioFileAnalyzer,which will let me target classificationtoward a particular file.The third object will requirea little extra attention.I'll need to define my own Observer type,which will handle the results of classification.Skipping the Observer for now,here's some code for preparing the first two of these objects.

I can create the SNClassifySoundRequestusing the built-in classifier,and I can create an SNAudioFileAnalyzerusing a URL to the file I want to classify.

If, at this point, I had an Observer ready to go,it would be easy to start sound classification,but defining that Observer is the missing piece.So let's do that.I'm starting here with a bare Observerthat inherits from NSObjectand conforms to the SNResultsObserving protocol.

I'll initialize instanceswith the sound label I want to search for,and I'll add a CMTime member variableto store the time at which I detect the sound.I just need to implement the request:didProduce result method.This method will be called when results are producedby sound classification,so I expect to receive instancesof SNClassificationResult.I can use the classificationForIdentifiermethod of the result to extract informationabout the label that I'm searching for.I'll query the confidence score associated with the label,and if that score exceeds a certain threshold,I'll consider the sound to be detected.When I notice the detection for the first time,I'll save the time at which the sound occurredso that I can provide it to Shortcuts later.

With that, my Observer is complete,and I have all the pieces neededto determine when a sound occurs within a file.This example touches on two important topicsthat I'd like to discuss further.First, I'll talk about time of detection,and then I'll talk about detection thresholds.

Let's start with time of detection.When you classify audio,the signal gets broken up into overlapping windows.

For each of these windows,you'll get a resultthat tells you what sounds were detectedand how confidently.You'll also get a time range,which tells you what partof the audio was classified.In my app, when I detect a sound,I use the result's time rangeto determine when the sound occurred.But the time range can be impactedwhen the duration of a window changes.You can customize the window durationto make it big or smallbased on your use case.

Short windows work well when working with short sounds,like a drum tap.This is because you can captureall the important features of that soundwithin a small window of time.The small window doesn't cut outany important information.The advantage of using a small window durationis that it allows you to more closely pinpointthe moment at which a sound occurred.

But small window durations may not be appropriatewhen working with longer sounds.A siren, for example,may contain both rising and falling pitchesover a longer period of time.Capturing all of these pitches together in a single windowcan help sound classificationcorrectly detect the sound.In general, it's good to use a window durationlong enough to capture all the important partsof the sounds that you're interested in.If you'd like to edit the window duration,you can set the windowDuration propertyof SNClassifySoundRequest.Note, though,that not all window durations are supported.

Different classifiers might supportdifferent window durations.You can check what window durations are supportedby reading the windowDurationConstraintproperty of SNClassifySoundRequest.The built-in classifier supports window durationsbetween 1/2 second long and 15 seconds long.A duration of one second or longeris a great starting pointwhen adopting the classifier in your app.

Let's talk next about confidence thresholds.In my app, I considered a sound to be detectedany time the confidence for that sound roseabove a fixed threshold.I chose a value of 0.5 for my threshold,but there are some things to considerwhen choosing a threshold for your own app.The classifier can detectmultiple sounds at the same time.When this happens, you may noticethat several labels score with high confidence.

Unlike when using a custom model trained using CreateML,label scores do not add upto a value of one.The confidences are independentand shouldn't be compared against one another.

Because confidence scores are independent,you may find it useful to choosedifferent confidence thresholds for different sounds.

Choosing a threshold involves a tradeoff.A higher confidence threshold reduces the probabilitythat a sound will be falsely detected,but it also increases the probabilitythat a true detection will be missedbecause it wasn't strong enough.When you select a threshold for your application,you'll need to find a value that achievesthe right balance of these factors for your use case.Note that confidence scores may changewhen you set a custom window duration,so this can impact your thresholds as well.One final thing to keep in mindwhen using the built-in classifieris that some sounds are similar.Among the large number of sounds that the classifier can identifyare several groups of sounds that can be difficultto differentiate using audio alone,even for a human.Where possible, it's best to be selectiveabout the sounds that you pay attention to.You should try to watch only for soundsthat are likely to occur in the contextswhere your app will be used.With that, let's turn back to Jonto learn about what's new in CreateMLregarding sound classification.Thanks, Kevin, for that great example.I'm glad you had fun with that cowbell video.Now let me show you what's new in CreateMLor, specifically,how you can improve your custom modelsby leveraging the power of the built-in classifier.The built-in classifier was trained with a ton of dataacross a vast number of categories.So the model actually contains a lot of knowledgeabout sound classification.All this knowledge can be utilizedfor the training of your custom model using CreateML.I'll show you how this works.The sound classifier can be separatedinto two different networks.The first part is the feature extractor,and the second part is the classifier model.The feature extractor,sometimes known as the embedding model,is the backbone of the network.It takes an audio waveform and transforms itinto a low-dimensional space.A well-trained feature extractororganizes acoustically similar soundsinto nearby locations in the space.

For example,sounds of guitar would cluster togetherbut are placed away from drum and car sounds.

Now, the second part of this pipelineis the classifier model.It takes the output of the feature extractorand computes the class probabilities.The classifier benefits from being pairedwith a good feature extractor,like the one we have embedded into the built-in classifier.

We're making the built-in classifier'sfeature extractor available to you.It is called Audio Feature Print.When you train your own custom model in CreateML,your model will be paired with Audio Feature Print.With this, your model benefitsfrom all the knowledge contained in the built-in classifier.Compared to the previous generation feature extractor,Audio Feature Print has improvements across the board.Even though this network is smaller and faster,it achieves higher accuracyon all benchmark data sets we compared against.And like the built-in classifier,models using Audio Feature Print supporta flexible window duration.You can select a long window duration to optimizefor sounds like sirensor a short window duration for sounds like drum taps.Audio Feature Print is the new default feature extractorwhen you train a custom model using CreateML.

The Window duration is the length of audio usedto generate a single feature while training.It defaults to three seconds,but you can adjust it to suit your needs.CreateML gives you the option to selecta window duration between 1/2 second to 15 seconds.For a more detailed example of training a custom model,you can check out the 2019 sessionon "Training Sound Classification Modelsin CreateML."It also shows you how to use the Sound Analysis Frameworkto run the custom model.Thanks for joining usin the session on Sound Analysis.Today, we introduced a powerful new sound classifierbuilt into the OS.Along with it,we've upgraded the feature extractor in CreateML.These will unlock new possibilities,and we can't wait to see what you'll do with them in your app.Enjoy the rest of WWDC.[upbeat music]

8:12 -Get list of recognized sounds

9:19 -Create sound classification request

9:52 -Implement sound classification observer

## Code Samples

```swift
func
 
getListOfRecognizedSounds
()
 
throws
 -> [
String
] {
    
let
 request 
=
 
try
 
SNClassifySoundRequest
(classifierIdentifier: .version1)
    
return
 request.knownClassifications
}
```

```swift
let
 request 
=
 
try
 
SNClassifySoundRequest
(classifierIdentifier: .version1)


let
 analyzer 
=
 
try
 
SNAudioFileAnalyzer
(url: url)


var
 observer: 
SNResultsObserving
 
// TODO



try
 analyzer.add(request, withObserver: observer)
analyzer.analyze()
```

```swift
class
 
FirstDetectionObserver
: 
NSObject
, 
SNResultsObserving
 
{
    
var
 firstDetectionTime 
=
 
CMTime
.invalid
    
var
 label: 
String

    
    
init
(
label
: 
String
)
 {
        
self
.label 
=
 label
    }
    
    
func
 
request
(
_
 
request
: 
SNRequest
, 
didProduce
 
result
: 
SNResult
)
 {
        
if
 
let
 result 
=
 result 
as?
 
SNClassificationResult
,
           
let
 classification 
=
 result.classification(forIdentifier: label),
           classification.confidence 
>
 
0.5
,
           firstDetectionTime 
==
 
CMTime
.invalid {
                firstDetectionTime 
=
 result.timeRange.start
        }
    }
}
```

