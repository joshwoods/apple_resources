# Wwdc2021 10146

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

What’s new in AVFoundationDiscover the latest updates to AVFoundation, Apple's framework for inspecting, playing, and authoring audiovisual presentations. We'll explore how you can use AVFoundation to query attributes of audiovisual assets, further customize your custom video compositions with timed metadata, and author caption files.ResourcesAVFoundationLoading media data asynchronouslyHD VideoSD VideoRelated VideosWWDC22Create a more responsive media appWWDC21Explore dynamic pre-rolls and mid-rolls in HLSExplore HLS variants in AVFoundationExplore low-latency video encoding with VideoToolboxImmerse your app in Spatial AudioMeet async/await in SwiftWhat's new in AVKit

Discover the latest updates to AVFoundation, Apple's framework for inspecting, playing, and authoring audiovisual presentations. We'll explore how you can use AVFoundation to query attributes of audiovisual assets, further customize your custom video compositions with timed metadata, and author caption files.

AVFoundation

Loading media data asynchronously

HD VideoSD Video

HD Video

SD Video

Create a more responsive media app

Explore dynamic pre-rolls and mid-rolls in HLS

Explore HLS variants in AVFoundation

Explore low-latency video encoding with VideoToolbox

Immerse your app in Spatial Audio

Meet async/await in Swift

What's new in AVKit

Search this video…♪ Bass music playing ♪♪Adam Sonnanstine: Hello! my name is Adam,and I'm here today to show you what's new in AVFoundation.We have three new features to discuss today.We're going to spend most of our time talking aboutwhat's new in the world of AVAsset inspection,and then we'll give a quick introductionto two other features:video compositing with metadata and caption file authoring.So without further ado, let's jump into our first topic,which is AVAsset async Inspection.But first, a little bit of background,starting with a refresher on AVAsset.AVAsset is AVFoundation's core model objectfor representing things like movie filesstored on the user's device;movie files stored elsewhere, such as a remote server;and other forms of audiovisual content,such as HTTP Live Streams and compositions.And when you have an asset, you most often want to play it,but just as often, you're going to want to inspect it.You want to ask it questions like, what's its durationor what are the formats of audio and video it contains?And that's what we're really going to be talking aboutin this topic: asset inspection.And whenever you're inspecting an asset,there are two important things to keep in mind.The first is that asset inspection happens on demand.This is mainly because movie files can be quite large.A feature-length film could be several gigabytes in size.You wouldn't want the assetto eagerly download the entire file,just in case you ask for its duration later.Instead, the asset waitsuntil you ask it to load a property value,then it downloads just the information it needsto give you that value.The second thing to keep in mind is that asset inspectionis an asynchronous process.This is really important because network I/O can take some time.If the asset is stored across the network,you wouldn't want your app's main thread to be blockedwhile AVAsset issues a synchronous network request.Instead, AVAsset will asynchronouslydeliver the result when it's ready.With these two things in mind, we have a new APIfor inspecting asset properties,and it looks a little bit like this.The main thing to notice is this new load method,which takes in a property identifier --in this case .duration --in order for you to tell it which property value to load.Each property identifier is associated with a result typeat compile time,which determines the return type of the load method.In this case, the duration is a CMTime,so the result is a CMTime.One thing you may not have seen before is this await keyword.This is a new feature in Swift, and it's used to mark,at the call site, that the load method is asynchronous.For all the details on async/awaitand the broader concurrency effort in Swift,I encourage you to check out the session called"Meet async/await in Swift."For now, as a quick way of understandinghow to use our new property loading method,I like to think of the await keywordas dividing the calling function into two parts.First, there's the part that happensbefore the asynchronous operation begins.In this case, we create an asset and ask it to load its duration.At this point, the asset goes off and does the I/Oand parsing necessary to determine its durationand we await its result.While we're waiting, the calling function is suspended,which means the code written after the awaitdoesn't execute right away.However, the thread we were running on isn't blocked.Instead, it's free to do more work while we're waiting.Once the asynchronous duration loading has finished,then the second half of the function is scheduled to be run.In this case, if the duration loading was successful,we store the duration into a local constantand send it off to another function.Or, if the operation failed, an error will be thrownonce the calling function resumes.So that's the basics of loading a property value asynchronously.You can also load the values of multiple properties at once,and you do this simplyby passing in more than one property identifierto the load method.In this case, we're loading both the duration and the tracksat the same time.This is not only convenient, but it can also be more efficient.If the asset knows all the properties you're interested in,it can batch up the work required to load their values.The result of loading multiple property values is a tuple,with the loaded values in the same order you usedfor the property identifiers.Just like loading a single property value,this is type safe.In this case, the first element of the result tuple is a CMTimeand the second element is an array of AVAssetTracks.And of course, just like with loading a single value,this is an asynchronous operation.In addition to asynchronously loading property values,you can also check the status of a propertywithout waiting for the value to loadat any time using the new status(of: ) method.You pass in the same property identifierthat you use for the load method,and this'll return an enum with four possible cases.Each property starts off as .notYetLoaded.Remember that asset inspection happens on demand,so until you ask to load a property value,the asset won't have done any work to load it.If you happen to check the statuswhile the loading is in progress,you'll get the .loading case.Or, if the property is already loaded,you'll get the .loaded case, which comes bundledwith the value that was loaded as an associated value.Finally, if a failure occurred --perhaps because the network went down --you'll get the .failed case, which comes bundledwith an error to describe what went wrong.Note that this'll be the same error that was thrownby the invocation of the load methodthat initiated the failed loading request.So that's the new API for loading asynchronous propertiesand checking their status.AVAsset has quite a few propertieswhose values can be loaded asynchronously.Most of these vend a self-contained value,but the .tracks and .metadata propertiesvend more complex objects you can useto descend into the hierarchical structure of the asset.In the case of the .tracks property,you'll get an array of AVAssetTracks.An AVAssetTrack has its own collection of propertieswhose values can be loaded asynchronouslyusing that same load method.Similarly, the .metadata propertygives you an array of AVMetadataItems,and several AVMetadataItem propertiescan also be loaded asynchronouslyusing the load method.The last bit of new API in this areais a collection of asynchronous methods that you can useto get at specific subsets of certain property values.So instead of loading all the tracks, for example,you can use one of these first three methodsto load just some of the tracks --for example, just the audio tracks.There are several new methods like thison both AVAsset and AVAssetTrack.So that's all the new API we havefor inspecting assets asynchronously.But at this point, I have a small confession to make.None of this functionality is actually new.The APIs are new,but these classes have always had the abilityto load their property values asynchronously.It's just that, with the old APIs,you would've had to write code more like this.It was a three-step process.You first have to callthe loadValuesAsynchronously method,giving it strings to tell it which properties to load.Then you need to make sure that each of the propertiesactually did successfully load and didn't fail.Then, once you've gotten that far,you can fetch the loaded value either by queryingthe corresponding synchronous propertyor by calling one of the synchronous filtering methods.This is not only verbose and repetitive,it's also easy to misuse.For example, it's very easy to forgetto do these essential loading and status-checking steps.What you're left with are these synchronous propertiesand methods that can be called at any time,but if you call them without first loadingthe property values, you'll end up doing blocking I/O.If you do this on your main thread,this means that your app can end up hangingat unpredictable times.So in addition to the fact that the new APIsare simply easier to use,the fact that they also eliminate these common misusesmeans that we plan to deprecate the old synchronous APIsfor Swift clients in a future release.This is an excellent time to moveto the new asynchronous versions of these interfaces,and to help you do thatwe've prepared a short migration guide.So, if you're doing that trifecta of loading the value,checking its status, and then grabbing a synchronous property,now you can simply call the load methodand do that all in one asynchronous step.Similarly, if you're doing that three-step processbut using a synchronous filtering methodinstead of a property,you can now call the asynchronous equivalentof that filtering method and do that in one step.If you're switching over the status of a propertyusing the old statusOfValue(forKey: ) methodand then grabbing the synchronous property valuewhen you see that you're in the .loaded case,now you can take advantage of the fact that the .loaded caseof the new status enum comes bundledwith that .loaded value.If your app is doing something a little bit more interesting,like loading the value of a property in one part of the codeand then fetching the loaded valuein a different part of the code, there are a couple waysyou could do this with the new interface.I recommend just calling the load method again.This is the easiest and safest way to do it,and if the property has already been loaded,this won't duplicate the work that's already been done.Instead, it'll just return a cached value.However, there's one caveat to this and that is that,because the load method is an async method,it can only be called from an async context.So if you really need to get the value of the propertyfrom a pure synchronous context,you can do something like get the status of the propertyand assert that it's loadedin order to grab the value of the property synchronously.Still, you have to be careful doing this,because it's possible for a property to become failedeven after it's already been loaded.Finally, if you're skipping the loadingand status-checking stepsand just relying on the current behavior of the propertiesand methods in that they block until the result is available,well, we're actually not providing a replacementfor this.This has never been the recommended way to use the API,and so we've always discouraged it.We designed the new property-loading APIsto be just about as easy to use as fetching a simple property,so migrating to the new APIs should be straightforward.And with that, that's all for our first topic.I'm really excited about our new way to inspect assets,using Swift's new async features,and I hope you'll enjoy using them as much as I have.Now let's move on to the first of our two shorter topics:video compositing with metadata.Here we're talking about video compositing,which is the process of taking multiple video tracksand composing them into a single stream of video frames.And in particular,we have an enhancement for custom video compositors,which is where you provide the codethat does the compositing.New this year, you can get per-frame metadatadelivered to youin your custom compositor's frame composition callback.As an example, let's say you have a sequence of GPS data,and that data is time-stamped and synchronizedwith your video, and you want to use that GPS datain order to influence how your frames are composed together.You can do that now, and the first step is to writethe GPS data to a timed metadata track in your source movie.In order do to this with AVAssetWriter,check out the existing class,AVAssetWriter InputMetadataAdaptor.Now let's take a look at the new API.Let's say you're starting with a source moviethat has a certain collection of tracks.Perhaps it has an audio track, two video tracks,and three timed metadata tracks.But let's say that tracks four and five contain metadatathat's useful for your video compositing,but track six is unrelated.You have two setup steps to perform, and the firstis to use the new source SampleDataTrackIDs propertyto tell your video composition objectthe IDs of all the timed metadata tracksthat are relevant for the entire video composition.Once you've done that, the second stepis to take each of your video composition instructionsand do something similar, but this time you setthe requiredSourceSampleData TrackIDs propertyto tell it the track ID -- or IDs --that are relevant for that particular instruction.It's important that you do both of these setup stepsor you simply won't get any metadatain your composition callback.Now let's move over to the callback itself.When you get your asynchronous video composition request objectin your callback, there are two new APIs that you usein order to get the metadata for your video composition.The first is the source SampleDataTrackIDs property,which replays the track IDs for the metadata tracksthat are relevant to that request.Then for each of the track IDs, you can use thesourceTimedMetadata(byTrackID :) methodin order to get the current timed metadata groupfor that track.Now, AVTimedMetadataGroup is a high-level representationof the metadata, with the value parsedinto a string, date, or other high-level object.If you'd rather work with the raw bytes of the metadata,you can use the sourceSampleBuffer(byTrackID: )method to get a CMSampleBufferinstead of an AVTimedMetadataGroup.Once you have the metadata in hand,you can use the metadata along with your source video framesto generate your output video frameand finish off the request.So that's all it takes to get metadatainto your custom video compositor callbackso you can do more interesting thingswith your video compositions.Now onto our final topic, which is caption file authoring.New this year for macOS,AVFoundation is adding support for two file formats.First, we have iTunes Timed Text, or .itt files,which contain subtitles.The other file format is Scenarist Closed Captions --or .scc files -- which contain closed captions.AVFoundation is adding supportfor authoring these two file formats,ingesting captions from these types of files,and also for previewing captions at runtimeto see what they'll look like during playback.On the authoring side, we have some new APIs,starting with AVCaption,which is the model object that represents a single caption.It has properties for things like the text, position,styling, and other attributes of a single caption.You can create AVCaptions yourselfand use an AVAssetWriterInputCaptionAdaptorin order to write them to one of these two file formats.In addition, we have a new validation servicein the AVCaptionConversion Validator class,which helps you make sure the captions you're writingare actually compatible with the file format you've chosen.As an example of why this is important, consider .scc files.They contain CEA-608 captions,which is a format that has very specific limitationsabout how many captions you can have in a given amount of time,all the way down to having a fixed bit budgetfor the data representing the individual charactersand their styling.So the validator will help you not only ensurethat your stream of captions is compatible with the file format,it'll also suggest tweaks you can make to your captions,such as adjusting their time stamps,in order to make them compatible.The new API for ingesting captionsis AVAssetReader OutputCaptionAdaptorwhich allows you to take one of these filesand read in AVCaption objects from it.Finally, we have an AVCaptionRenderer class,which allows you to take a single captionor a group of captions and render them to a CGContextin order to get a preview of what they'll look likeduring playback.So that's just the tip of the icebergfor our new caption file authoring APIs.If you're interested in adopting them,we encourage you to get in touch with us --either in the forums or in the conference labs --and we can help answer any questions that you have.And that was our final topic, so let's wrap up.Our big topic for the day was inspecting AVAsset properties,the importance of doing so on demand and asynchronously,the new APIs in this area,and some tips for migrating from the old APIs.We then talked about using timed metadatato further customize your custom video compositions.Finally, I gave a brief introductionto caption file authoring and the new APIs in that area.That's all for today.Thank you very much for watching and enjoy WWDC21.♪

♪ Bass music playing ♪♪Adam Sonnanstine: Hello! my name is Adam,and I'm here today to show you what's new in AVFoundation.

We have three new features to discuss today.

We're going to spend most of our time talking aboutwhat's new in the world of AVAsset inspection,and then we'll give a quick introductionto two other features:video compositing with metadata and caption file authoring.

So without further ado, let's jump into our first topic,which is AVAsset async Inspection.

But first, a little bit of background,starting with a refresher on AVAsset.

AVAsset is AVFoundation's core model objectfor representing things like movie filesstored on the user's device;movie files stored elsewhere, such as a remote server;and other forms of audiovisual content,such as HTTP Live Streams and compositions.

And when you have an asset, you most often want to play it,but just as often, you're going to want to inspect it.

You want to ask it questions like, what's its durationor what are the formats of audio and video it contains?And that's what we're really going to be talking aboutin this topic: asset inspection.

And whenever you're inspecting an asset,there are two important things to keep in mind.

The first is that asset inspection happens on demand.

This is mainly because movie files can be quite large.

A feature-length film could be several gigabytes in size.

You wouldn't want the assetto eagerly download the entire file,just in case you ask for its duration later.

Instead, the asset waitsuntil you ask it to load a property value,then it downloads just the information it needsto give you that value.

The second thing to keep in mind is that asset inspectionis an asynchronous process.

This is really important because network I/O can take some time.

If the asset is stored across the network,you wouldn't want your app's main thread to be blockedwhile AVAsset issues a synchronous network request.

Instead, AVAsset will asynchronouslydeliver the result when it's ready.

With these two things in mind, we have a new APIfor inspecting asset properties,and it looks a little bit like this.

The main thing to notice is this new load method,which takes in a property identifier --in this case .duration --in order for you to tell it which property value to load.

Each property identifier is associated with a result typeat compile time,which determines the return type of the load method.

In this case, the duration is a CMTime,so the result is a CMTime.

One thing you may not have seen before is this await keyword.

This is a new feature in Swift, and it's used to mark,at the call site, that the load method is asynchronous.

For all the details on async/awaitand the broader concurrency effort in Swift,I encourage you to check out the session called"Meet async/await in Swift."For now, as a quick way of understandinghow to use our new property loading method,I like to think of the await keywordas dividing the calling function into two parts.

First, there's the part that happensbefore the asynchronous operation begins.

In this case, we create an asset and ask it to load its duration.

At this point, the asset goes off and does the I/Oand parsing necessary to determine its durationand we await its result.

While we're waiting, the calling function is suspended,which means the code written after the awaitdoesn't execute right away.

However, the thread we were running on isn't blocked.

Instead, it's free to do more work while we're waiting.

Once the asynchronous duration loading has finished,then the second half of the function is scheduled to be run.

In this case, if the duration loading was successful,we store the duration into a local constantand send it off to another function.

Or, if the operation failed, an error will be thrownonce the calling function resumes.

So that's the basics of loading a property value asynchronously.

You can also load the values of multiple properties at once,and you do this simplyby passing in more than one property identifierto the load method.

In this case, we're loading both the duration and the tracksat the same time.

This is not only convenient, but it can also be more efficient.

If the asset knows all the properties you're interested in,it can batch up the work required to load their values.

The result of loading multiple property values is a tuple,with the loaded values in the same order you usedfor the property identifiers.

Just like loading a single property value,this is type safe.

In this case, the first element of the result tuple is a CMTimeand the second element is an array of AVAssetTracks.

And of course, just like with loading a single value,this is an asynchronous operation.

In addition to asynchronously loading property values,you can also check the status of a propertywithout waiting for the value to loadat any time using the new status(of: ) method.

You pass in the same property identifierthat you use for the load method,and this'll return an enum with four possible cases.

Each property starts off as .notYetLoaded.

Remember that asset inspection happens on demand,so until you ask to load a property value,the asset won't have done any work to load it.

If you happen to check the statuswhile the loading is in progress,you'll get the .loading case.

Or, if the property is already loaded,you'll get the .loaded case, which comes bundledwith the value that was loaded as an associated value.

Finally, if a failure occurred --perhaps because the network went down --you'll get the .failed case, which comes bundledwith an error to describe what went wrong.

Note that this'll be the same error that was thrownby the invocation of the load methodthat initiated the failed loading request.

So that's the new API for loading asynchronous propertiesand checking their status.

AVAsset has quite a few propertieswhose values can be loaded asynchronously.

Most of these vend a self-contained value,but the .tracks and .metadata propertiesvend more complex objects you can useto descend into the hierarchical structure of the asset.

In the case of the .tracks property,you'll get an array of AVAssetTracks.

An AVAssetTrack has its own collection of propertieswhose values can be loaded asynchronouslyusing that same load method.

Similarly, the .metadata propertygives you an array of AVMetadataItems,and several AVMetadataItem propertiescan also be loaded asynchronouslyusing the load method.

The last bit of new API in this areais a collection of asynchronous methods that you can useto get at specific subsets of certain property values.

So instead of loading all the tracks, for example,you can use one of these first three methodsto load just some of the tracks --for example, just the audio tracks.

There are several new methods like thison both AVAsset and AVAssetTrack.

So that's all the new API we havefor inspecting assets asynchronously.

But at this point, I have a small confession to make.

None of this functionality is actually new.

The APIs are new,but these classes have always had the abilityto load their property values asynchronously.

It's just that, with the old APIs,you would've had to write code more like this.

It was a three-step process.

You first have to callthe loadValuesAsynchronously method,giving it strings to tell it which properties to load.

Then you need to make sure that each of the propertiesactually did successfully load and didn't fail.

Then, once you've gotten that far,you can fetch the loaded value either by queryingthe corresponding synchronous propertyor by calling one of the synchronous filtering methods.

This is not only verbose and repetitive,it's also easy to misuse.

For example, it's very easy to forgetto do these essential loading and status-checking steps.

What you're left with are these synchronous propertiesand methods that can be called at any time,but if you call them without first loadingthe property values, you'll end up doing blocking I/O.

If you do this on your main thread,this means that your app can end up hangingat unpredictable times.

So in addition to the fact that the new APIsare simply easier to use,the fact that they also eliminate these common misusesmeans that we plan to deprecate the old synchronous APIsfor Swift clients in a future release.

This is an excellent time to moveto the new asynchronous versions of these interfaces,and to help you do thatwe've prepared a short migration guide.

So, if you're doing that trifecta of loading the value,checking its status, and then grabbing a synchronous property,now you can simply call the load methodand do that all in one asynchronous step.

Similarly, if you're doing that three-step processbut using a synchronous filtering methodinstead of a property,you can now call the asynchronous equivalentof that filtering method and do that in one step.

If you're switching over the status of a propertyusing the old statusOfValue(forKey: ) methodand then grabbing the synchronous property valuewhen you see that you're in the .loaded case,now you can take advantage of the fact that the .loaded caseof the new status enum comes bundledwith that .loaded value.

If your app is doing something a little bit more interesting,like loading the value of a property in one part of the codeand then fetching the loaded valuein a different part of the code, there are a couple waysyou could do this with the new interface.

I recommend just calling the load method again.

This is the easiest and safest way to do it,and if the property has already been loaded,this won't duplicate the work that's already been done.

Instead, it'll just return a cached value.

However, there's one caveat to this and that is that,because the load method is an async method,it can only be called from an async context.

So if you really need to get the value of the propertyfrom a pure synchronous context,you can do something like get the status of the propertyand assert that it's loadedin order to grab the value of the property synchronously.

Still, you have to be careful doing this,because it's possible for a property to become failedeven after it's already been loaded.

Finally, if you're skipping the loadingand status-checking stepsand just relying on the current behavior of the propertiesand methods in that they block until the result is available,well, we're actually not providing a replacementfor this.

This has never been the recommended way to use the API,and so we've always discouraged it.

We designed the new property-loading APIsto be just about as easy to use as fetching a simple property,so migrating to the new APIs should be straightforward.

And with that, that's all for our first topic.

I'm really excited about our new way to inspect assets,using Swift's new async features,and I hope you'll enjoy using them as much as I have.

Now let's move on to the first of our two shorter topics:video compositing with metadata.

Here we're talking about video compositing,which is the process of taking multiple video tracksand composing them into a single stream of video frames.

And in particular,we have an enhancement for custom video compositors,which is where you provide the codethat does the compositing.

New this year, you can get per-frame metadatadelivered to youin your custom compositor's frame composition callback.

As an example, let's say you have a sequence of GPS data,and that data is time-stamped and synchronizedwith your video, and you want to use that GPS datain order to influence how your frames are composed together.

You can do that now, and the first step is to writethe GPS data to a timed metadata track in your source movie.

In order do to this with AVAssetWriter,check out the existing class,AVAssetWriter InputMetadataAdaptor.

Now let's take a look at the new API.

Let's say you're starting with a source moviethat has a certain collection of tracks.

Perhaps it has an audio track, two video tracks,and three timed metadata tracks.

But let's say that tracks four and five contain metadatathat's useful for your video compositing,but track six is unrelated.

You have two setup steps to perform, and the firstis to use the new source SampleDataTrackIDs propertyto tell your video composition objectthe IDs of all the timed metadata tracksthat are relevant for the entire video composition.

Once you've done that, the second stepis to take each of your video composition instructionsand do something similar, but this time you setthe requiredSourceSampleData TrackIDs propertyto tell it the track ID -- or IDs --that are relevant for that particular instruction.

It's important that you do both of these setup stepsor you simply won't get any metadatain your composition callback.

Now let's move over to the callback itself.

When you get your asynchronous video composition request objectin your callback, there are two new APIs that you usein order to get the metadata for your video composition.

The first is the source SampleDataTrackIDs property,which replays the track IDs for the metadata tracksthat are relevant to that request.

Then for each of the track IDs, you can use thesourceTimedMetadata(byTrackID :) methodin order to get the current timed metadata groupfor that track.

Now, AVTimedMetadataGroup is a high-level representationof the metadata, with the value parsedinto a string, date, or other high-level object.

If you'd rather work with the raw bytes of the metadata,you can use the sourceSampleBuffer(byTrackID: )method to get a CMSampleBufferinstead of an AVTimedMetadataGroup.

Once you have the metadata in hand,you can use the metadata along with your source video framesto generate your output video frameand finish off the request.

So that's all it takes to get metadatainto your custom video compositor callbackso you can do more interesting thingswith your video compositions.

Now onto our final topic, which is caption file authoring.

New this year for macOS,AVFoundation is adding support for two file formats.

First, we have iTunes Timed Text, or .itt files,which contain subtitles.

The other file format is Scenarist Closed Captions --or .scc files -- which contain closed captions.

AVFoundation is adding supportfor authoring these two file formats,ingesting captions from these types of files,and also for previewing captions at runtimeto see what they'll look like during playback.

On the authoring side, we have some new APIs,starting with AVCaption,which is the model object that represents a single caption.

It has properties for things like the text, position,styling, and other attributes of a single caption.

You can create AVCaptions yourselfand use an AVAssetWriterInputCaptionAdaptorin order to write them to one of these two file formats.

In addition, we have a new validation servicein the AVCaptionConversion Validator class,which helps you make sure the captions you're writingare actually compatible with the file format you've chosen.

As an example of why this is important, consider .scc files.

They contain CEA-608 captions,which is a format that has very specific limitationsabout how many captions you can have in a given amount of time,all the way down to having a fixed bit budgetfor the data representing the individual charactersand their styling.

So the validator will help you not only ensurethat your stream of captions is compatible with the file format,it'll also suggest tweaks you can make to your captions,such as adjusting their time stamps,in order to make them compatible.

The new API for ingesting captionsis AVAssetReader OutputCaptionAdaptorwhich allows you to take one of these filesand read in AVCaption objects from it.

Finally, we have an AVCaptionRenderer class,which allows you to take a single captionor a group of captions and render them to a CGContextin order to get a preview of what they'll look likeduring playback.

So that's just the tip of the icebergfor our new caption file authoring APIs.

If you're interested in adopting them,we encourage you to get in touch with us --either in the forums or in the conference labs --and we can help answer any questions that you have.

And that was our final topic, so let's wrap up.

Our big topic for the day was inspecting AVAsset properties,the importance of doing so on demand and asynchronously,the new APIs in this area,and some tips for migrating from the old APIs.

We then talked about using timed metadatato further customize your custom video compositions.

Finally, I gave a brief introductionto caption file authoring and the new APIs in that area.

That's all for today.

Thank you very much for watching and enjoy WWDC21.

♪

2:16 -AVAsset property loading

4:02 -Load multiple properties

4:52 -Check status

6:32 -Async filtering methods

7:16 -Async loading: Old API

8:09 -This is the equivalent using the new API:

8:36 -load(_:)

8:51 -Async filtering method

8:58 -status(of:)

9:18 -load(_:) again (returns cached value)

9:49 -Assert status is .loaded()

11:49 -Custom video composition with metadata: Setup

12:44 -Custom video composition with metadata: Compositing

## Code Samples

```swift
func
 
inspectAsset
()
 
async
 
throws
 {
	
let
 asset 
=
 
AVAsset
(url: movieURL)
	
let
 duration 
=
 
try
 
await
 asset.load(.duration)
	myFunction(thatUses: duration)
}
```

```swift
func
 
inspectAsset
()
 
async
 
throws
 {
	
let
 asset 
=
 
AVAsset
(url: movieURL)
	
let
 (duration, tracks) 
=
 
try
 
await
 asset.load(.duration, .tracks)
	myFunction(thatUses: duration, and: tracks)
}
```

```swift
switch
 asset.status(of: .duration) {

case
 .notYetLoaded:
	
// This is the initial state after creating an asset. 


case
 .loading:
	
// This means the asset is actively doing work.


case
 .loaded(
let
 duration):
	
// Use the asset's property value.


case
 .failed(
let
 error):
	
// Handle the error.

}
```

```swift
let
 asset: 
AVAsset


let
 trk1 
=
 
try
 
await
 asset.loadTrack(withTrackID: 
1
)

let
 atrs 
=
 
try
 
await
 asset.loadTracks(withMediaType: .audio)

let
 ltrs 
=
 
try
 
await
 asset.loadTracks(withMediaCharacteristic: .legible)

let
 qtmd 
=
 
try
 
await
 asset.loadMetadata(for: .quickTimeMetadata)

let
 chcl 
=
 
try
 
await
 asset.loadChapterMetadataGroups(withTitleLocale: .current)

let
 chpl 
=
 
try
 
await
 asset.loadChapterMetadataGroups(bestMatchingPreferredLanguages: [
"en-US"
])

let
 amsg 
=
 
try
 
await
 asset.loadMediaSelectionGroup(for: .audible)


let
 track: 
AVAssetTrack


let
 seg0 
=
 
try
 
await
 track.loadSegment(forTrackTime: .zero)

let
 spts 
=
 
try
 
await
 track.loadSamplePresentationTime(forTrackTime: .zero)

let
 ismd 
=
 
try
 
await
 track.loadMetadata(for: .isoUserData)

let
 fbtr 
=
 
try
 
await
 track.loadAssociatedTracks(ofType: .audioFallback)
```

```swift
asset.loadValuesAsynchronously(forKeys: [
"duration"
, 
"tracks"
]) {
	
var
 error: 
NSError
?
	
guard
 asset.statusOfValue(forKey: 
"duration"
, error: 
&
error) 
==
 .loaded 
else
 { 
...
 }
	
guard
 asset.statusOfValue(forKey: 
"tracks"
, error: 
&
error) 
==
 .loaded 
else
 { 
...
 }
	
let
 duration 
=
 asset.duration
	
let
 audioTracks 
=
 asset.tracks(withMediaType: .audio)
	
// Use duration and audioTracks.

}
```

```swift
let
 duration 
=
 
try
 
await
 asset.load(.duration)

let
 audioTracks 
=
 
try
 
await
 asset.loadTracks(withMediaType: .audio)

// Use duration and audioTracks.
```

```swift
let
 tracks 
=
 
try
 
await
 asset.load(.tracks)
```

```swift
let
 audioTracks 
=
 
try
 
await

    asset.loadTracks(withMediaType: .audio)
```

```swift
switch
 status(of: .tracks) {
    
case
 .loaded(
let
 tracks):
    
// Use tracks.
```

```swift
guard
 
case
 .loaded (
let
 tracks)
    
=
 asset.status(of: .tracks) 
else
 { 
...
 }
```

```swift
/*
 Source movie:
 - Track 1: Audio
 - Track 2: Video
 - Track 3: Video
 - Track 4: Metadata
 - Track 5: Metadata
 - Track 6: Metadata
 */



// Tell AVMutableVideoComposition about all the metadata tracks.

videoComposition.sourceSampleDataTrackIDs 
=
 [
4
, 
5
]


// For each AVMutableVideoCompositionInstruction, specify the metadata track ID(s) to include.

instruction1.requiredSourceSampleDataTrackIDs 
=
 [
4
]
instruction2.requiredSourceSampleDataTrackIDs 
=
 [
4
, 
5
]
```

```swift
// This is an implementation of a AVVideoCompositing method:


func
 
startRequest
(
_
 
request
: 
AVAsynchronousVideoCompositionRequest
)
 {
	
for
 trackID 
in
 request.sourceSampleDataTrackIDs {
		
let
 metadata: 
AVTimedMetadataGroup
? 
=
 request.sourceTimedMetadata(byTrackID: trackID)
		
// To get CMSampleBuffers instead, use sourceSampleBuffer(byTrackID:).


	}

	
// Compose input video frames, using metadata, here.


	request.finish(withComposedVideoFrame: composedFrame)
}
```

