# Wwdc2021 10207

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Embrace Expected Failures in XCTestTesting is a crucial part of building a great app: Great tests can help you track down important issues before release, improve your workflow, and provide a quality experience upon release. For issues that can't be immediately resolved, however, XCTest can help provide better context around those problems with XCTExpectFailure. Learn how this API works, its strict behavior, and how to improve the signal-to-noise ratio in your tests to identify new issues more efficiently.ResourcesExpected FailuresHD VideoSD VideoRelated VideosWWDC22Author fast and reliable tests for Xcode CloudWWDC21Diagnose unreliable code with test repetitionsExplore Digital Crown, Trackpad, and iPad pointer automation

Testing is a crucial part of building a great app: Great tests can help you track down important issues before release, improve your workflow, and provide a quality experience upon release. For issues that can't be immediately resolved, however, XCTest can help provide better context around those problems with XCTExpectFailure. Learn how this API works, its strict behavior, and how to improve the signal-to-noise ratio in your tests to identify new issues more efficiently.

Expected Failures

HD VideoSD Video

HD Video

SD Video

Author fast and reliable tests for Xcode Cloud

Diagnose unreliable code with test repetitions

Explore Digital Crown, Trackpad, and iPad pointer automation

Search this video…♪ Bass music playing ♪♪Wil Addario-Turner: Hi, welcome to"Embrace Expected Failures in XCTest."My name is Wil, and in this session,I'm going to discuss ways of improving the data you getwhen you run your project's tests.To begin with,let's consider why we test our code in the first place.Of course, at a high level,it's how we ensure the quality of the product.But in more concrete terms,I would say it's to discover bugsbefore we ship and not afterwards.Now, testing is an investment.It takes resources to create, run, and maintain tests.As with any investment,we want to maximize our returns while minimizing our cost.This session focuses on toolsfor reducing the maintenance cost.By maintenance,I'm primarily referring to how you handle failureswhen they occur in your test suites.When a test that's been passing begins to fail,that's a valuable piece of new information.This indicates either a flaw in the product,a problem in the test itself,or some issue in one of the dependencies --that is, all of the frameworks and subsystemson top of which the product sits.Regardless of the type of problem,once that failure has been registered,subsequent reports of the same failureare significantly less valuable,because they represent informationthat you already have.Ideally, any new failure is triaged and fixed quickly.However, your team may not be able to resolve a problemright away, which means that the failure quickly goesfrom being a valuable piece of new informationto a noisy distraction.Given a known failure in your teststhat cannot be immediately resolved,what tools are available for managing the noise?Two approaches that might come to mindare disabling and skipping.Let's consider the tradeoffs for theseand then we'll talk about the best tool --and the topic of this session -- XCTest's ExpectedFailures API.Xcode lets you disable tests in the test plan or scheme.You can use this for known test failures,and one advantage is that your test codewill continue to be compiled.However, since the code won't execute,you won't see it in the test report.This reduced visibility makes it harder to trackas an issue that needs to be resolved.Where this feature --the ability to choose which tests are enabled or disabled --really shines is for curating collections of testsfor specific purposes.But it's rarely the best way to handle a known failure.XCTSkip is another way you might manage a failing test.With this approach, not only does the codecontinue to get built with your tests,it also executes up until the point where XCTSkip is called.This means that it's included in the test report,giving you much better visibility of the issue.However, it doesn't execute all of your test,which means you lose out on potentially useful informationin the form of new issues and changes to the existing issue.XCTSkip is a great tool for managingconfiguration-based limitations on your test,such as requiring a specific OS version or device type.In the example here,the test will be skipped if it's not running on an iPad.This brings us to XCTExpectFailure,a set of functions in XCTestspecifically designed for managing known failures.In Swift, it has a number of overloadsfor different use cases,and Objective-C provides the same capabilitieswith several distinct functions.With this API, your test executes normally,but the results are changed as follows:Failure in the test will be reported as an expected failure.Failure in the test suite containing that testwill be reported as a pass,unless of course some other test in it fails.This eliminates the noise generated by the failure,making it easier to see whether there are any other issuesin your tests.Of course, suppressing the noisedoesn't solve the underlying issue.So to help you keep track of it, the API takes a failure reason.This string documents the problem in your codeand you can even embed a URL for your issue-tracking system.Xcode's test report UI shows expected failuresjust as it does normal failures or skipped tests.When you hover, if the failure reason contains a URL,an issue-tracking button appearsthat lets you jump out to the link.So let's see how this works!I have here a simple project with some unit testsfor my VendorAccount class.I'm going to run the tests, and when they finish,we'll see that one is failing while the other is passing.You can see three test result icons,one for each test.A red X for the failing testand a green check for the passing test,and one for the test suite;a red X because one of the tests in the suite has failed,so we consider the suite itself to have failed.Now I'm going to add a call to XCTExpectFailureat the beginning of the failing test.You can see the failure reason begins with a URLthat references the bug I've filedto keep track of this failure.Now I'll rerun the testsand we'll see how this affects the outcome.OK, so the red X icon for the failing testhas changed to a gray X,which is the indicator for an expected failure.What's even more interesting is that the test suite iconhas changed from a red X to a green dash.This icon indicates that the test suite has passedwith a mixed state,meaning that one or more of its tests did not pass,but was either a skip or an expected failure.So that's how easy it is to use XCTExpectFailureto handle a failing test.Now let's take a closer look at the API.The first consideration when using XCTExpectFailureis which API variant to call.There are two approaches:a stateful approach where you call XCTExpectFailureand any subsequent failure in the test is treated as expected;alternatively, you can use the scoped approach,in which you wrap the failing code in a closurepassed to XCTExpectFailure.Let's look at some examples.Here's a very simple test that calls some functionin my project.The test begins to failbecause the function is no longer returning true.Here's what it looks liketo use the stateful expected failure approach,just as we did in the demo.Alternatively, we could use the scoped approachby wrapping the failing code in a closuretrailing the call to XCTExpectFailure.This means any failure in the code outside the closurewill be reported normally.The API also supports nesting.In other words, you can call the API more than once in a test,including inside the closure from another call.This is an important considerationwhen using the API in test library code.For example, if a common utility function begins to fail,many tests could be impacted,some of which might already be using XCTExpectFailurefor different issues.When a failure occurs in the context of nested callsto XCTExpectFailure,the issue is matched against the nearest call site first,and if rejected by the matcher,will be passed on to the next call and so onwith stack semantics for the calls to XCTExpectFailure.For this reason, with shared code,it's best to use the closure-based APIto limit the effects on test state.The next thing to consideris how precisely to match the issue.By default, any failure in the affected scope is caught,but you can be more selectiveby specifying an issue-matching filter.In this example, we construct an objectof type XCTExpectedFailure.Optionsand define its issueMatcher.The matcher is passed the XCTIssue objectwith the failure details,so you have full access to that informationin determining whether or not to match.If the matcher rejects the failure,then it won't be handled as an expected failure.This can be useful in detecting when new problems show upin the code being tested.The options object also has a propertythat can be used to disable the expected failurein certain configurations.For example, my test may be passing on macOSbut failing on iOS, so I only want to expect failures on iOS.To achieve that,I disable the expected failure via the options,but only for platforms where I don't need it.So what happens when your expected failures stop failing?Usually this means the underlying issuehas been resolved, which is great.But how does XCTExpectFailure behave?If you're still calling the API and no failure is occurring,it will generate a new and distinct failure.We call this an "unmatched expected failure"and it's part of the strict behavior that is the defaultfor XCTExpectFailure.This behavior helps you maintain your codeby prompting you to remove unnecessary calls to the API.But what about tests that only fail some of the time?There are cases in which a test might fail sometimesbut not other times.These fall into two categories,the first of which is deterministicand includes environmental or other knowable conditionssuch as the earlier exampleof a test that only fails on certain platforms.On the other hand, some failures are inherently nondeterministic.These might be caused by timing issues,unreliable ordering dependencies,or concurrency bugs.For nondeterministic failures,the strict behavior isn't helpful;it just generates noise.Once again, the options object provides a way to control this.The isStrict flag, which defaults to true,can be turned off.Then, if XCTExpectFailure does not catch a failure,it will still allow the test to pass.In Swift, you can also specify the strict behavioras a direct parameter to XCTExpectFailure.Disabling strict behavior is great way to handle flakyor nondeterministic tests in your project.As an aside,when you need to investigate a nondeterministic failure,Xcode makes it easy to run a test multiple times,stopping when it fails or some other condition is met.This can be really helpful in tracking down failuresin flaky tests.For more about this, watch the session"Diagnose Unreliable Code with Test Repetitions."So that's XCTExpectFailure --APIs in XCTest for improving the signal-to-noisein your test suite results.This helps you identify new issues more efficiently,leading to higher-quality code.Thanks for watching!♪

♪ Bass music playing ♪♪Wil Addario-Turner: Hi, welcome to"Embrace Expected Failures in XCTest."My name is Wil, and in this session,I'm going to discuss ways of improving the data you getwhen you run your project's tests.To begin with,let's consider why we test our code in the first place.Of course, at a high level,it's how we ensure the quality of the product.But in more concrete terms,I would say it's to discover bugsbefore we ship and not afterwards.Now, testing is an investment.It takes resources to create, run, and maintain tests.As with any investment,we want to maximize our returns while minimizing our cost.This session focuses on toolsfor reducing the maintenance cost.By maintenance,I'm primarily referring to how you handle failureswhen they occur in your test suites.When a test that's been passing begins to fail,that's a valuable piece of new information.This indicates either a flaw in the product,a problem in the test itself,or some issue in one of the dependencies --that is, all of the frameworks and subsystemson top of which the product sits.Regardless of the type of problem,once that failure has been registered,subsequent reports of the same failureare significantly less valuable,because they represent informationthat you already have.Ideally, any new failure is triaged and fixed quickly.However, your team may not be able to resolve a problemright away, which means that the failure quickly goesfrom being a valuable piece of new informationto a noisy distraction.Given a known failure in your teststhat cannot be immediately resolved,what tools are available for managing the noise?Two approaches that might come to mindare disabling and skipping.Let's consider the tradeoffs for theseand then we'll talk about the best tool --and the topic of this session -- XCTest's ExpectedFailures API.Xcode lets you disable tests in the test plan or scheme.You can use this for known test failures,and one advantage is that your test codewill continue to be compiled.However, since the code won't execute,you won't see it in the test report.This reduced visibility makes it harder to trackas an issue that needs to be resolved.Where this feature --the ability to choose which tests are enabled or disabled --really shines is for curating collections of testsfor specific purposes.But it's rarely the best way to handle a known failure.XCTSkip is another way you might manage a failing test.With this approach, not only does the codecontinue to get built with your tests,it also executes up until the point where XCTSkip is called.This means that it's included in the test report,giving you much better visibility of the issue.However, it doesn't execute all of your test,which means you lose out on potentially useful informationin the form of new issues and changes to the existing issue.XCTSkip is a great tool for managingconfiguration-based limitations on your test,such as requiring a specific OS version or device type.In the example here,the test will be skipped if it's not running on an iPad.This brings us to XCTExpectFailure,a set of functions in XCTestspecifically designed for managing known failures.In Swift, it has a number of overloadsfor different use cases,and Objective-C provides the same capabilitieswith several distinct functions.With this API, your test executes normally,but the results are changed as follows:Failure in the test will be reported as an expected failure.Failure in the test suite containing that testwill be reported as a pass,unless of course some other test in it fails.This eliminates the noise generated by the failure,making it easier to see whether there are any other issuesin your tests.Of course, suppressing the noisedoesn't solve the underlying issue.So to help you keep track of it, the API takes a failure reason.This string documents the problem in your codeand you can even embed a URL for your issue-tracking system.Xcode's test report UI shows expected failuresjust as it does normal failures or skipped tests.When you hover, if the failure reason contains a URL,an issue-tracking button appearsthat lets you jump out to the link.So let's see how this works!I have here a simple project with some unit testsfor my VendorAccount class.I'm going to run the tests, and when they finish,we'll see that one is failing while the other is passing.You can see three test result icons,one for each test.A red X for the failing testand a green check for the passing test,and one for the test suite;a red X because one of the tests in the suite has failed,so we consider the suite itself to have failed.Now I'm going to add a call to XCTExpectFailureat the beginning of the failing test.You can see the failure reason begins with a URLthat references the bug I've filedto keep track of this failure.Now I'll rerun the testsand we'll see how this affects the outcome.OK, so the red X icon for the failing testhas changed to a gray X,which is the indicator for an expected failure.What's even more interesting is that the test suite iconhas changed from a red X to a green dash.This icon indicates that the test suite has passedwith a mixed state,meaning that one or more of its tests did not pass,but was either a skip or an expected failure.So that's how easy it is to use XCTExpectFailureto handle a failing test.Now let's take a closer look at the API.

The first consideration when using XCTExpectFailureis which API variant to call.There are two approaches:a stateful approach where you call XCTExpectFailureand any subsequent failure in the test is treated as expected;alternatively, you can use the scoped approach,in which you wrap the failing code in a closurepassed to XCTExpectFailure.Let's look at some examples.Here's a very simple test that calls some functionin my project.The test begins to failbecause the function is no longer returning true.Here's what it looks liketo use the stateful expected failure approach,just as we did in the demo.Alternatively, we could use the scoped approachby wrapping the failing code in a closuretrailing the call to XCTExpectFailure.This means any failure in the code outside the closurewill be reported normally.The API also supports nesting.In other words, you can call the API more than once in a test,including inside the closure from another call.This is an important considerationwhen using the API in test library code.For example, if a common utility function begins to fail,many tests could be impacted,some of which might already be using XCTExpectFailurefor different issues.When a failure occurs in the context of nested callsto XCTExpectFailure,the issue is matched against the nearest call site first,and if rejected by the matcher,will be passed on to the next call and so onwith stack semantics for the calls to XCTExpectFailure.For this reason, with shared code,it's best to use the closure-based APIto limit the effects on test state.The next thing to consideris how precisely to match the issue.By default, any failure in the affected scope is caught,but you can be more selectiveby specifying an issue-matching filter.In this example, we construct an objectof type XCTExpectedFailure.Optionsand define its issueMatcher.The matcher is passed the XCTIssue objectwith the failure details,so you have full access to that informationin determining whether or not to match.If the matcher rejects the failure,then it won't be handled as an expected failure.This can be useful in detecting when new problems show upin the code being tested.The options object also has a propertythat can be used to disable the expected failurein certain configurations.For example, my test may be passing on macOSbut failing on iOS, so I only want to expect failures on iOS.To achieve that,I disable the expected failure via the options,but only for platforms where I don't need it.So what happens when your expected failures stop failing?Usually this means the underlying issuehas been resolved, which is great.But how does XCTExpectFailure behave?If you're still calling the API and no failure is occurring,it will generate a new and distinct failure.We call this an "unmatched expected failure"and it's part of the strict behavior that is the defaultfor XCTExpectFailure.This behavior helps you maintain your codeby prompting you to remove unnecessary calls to the API.But what about tests that only fail some of the time?There are cases in which a test might fail sometimesbut not other times.These fall into two categories,the first of which is deterministicand includes environmental or other knowable conditionssuch as the earlier exampleof a test that only fails on certain platforms.On the other hand, some failures are inherently nondeterministic.These might be caused by timing issues,unreliable ordering dependencies,or concurrency bugs.For nondeterministic failures,the strict behavior isn't helpful;it just generates noise.Once again, the options object provides a way to control this.The isStrict flag, which defaults to true,can be turned off.Then, if XCTExpectFailure does not catch a failure,it will still allow the test to pass.In Swift, you can also specify the strict behavioras a direct parameter to XCTExpectFailure.Disabling strict behavior is great way to handle flakyor nondeterministic tests in your project.As an aside,when you need to investigate a nondeterministic failure,Xcode makes it easy to run a test multiple times,stopping when it fails or some other condition is met.This can be really helpful in tracking down failuresin flaky tests.For more about this, watch the session"Diagnose Unreliable Code with Test Repetitions."So that's XCTExpectFailure --APIs in XCTest for improving the signal-to-noisein your test suite results.This helps you identify new issues more efficiently,leading to higher-quality code.Thanks for watching!♪

3:31 -XCTSkip unless device is iPad

4:31 -XCTExpectFailure

7:14 -Scoped XCTExpectFailure

8:34 -XCTExpectFailure with issue matcher

9:03 -Disable XCTExpectFailure for some platforms

10:39 -Disable strict XCTExpectFailure behavior via options

10:53 -Disable strict XCTExpectFailure behavior via parameter

## Code Samples

```swift
try
 
XCTSkipUnless
(
UIDevice
.current.userInterfaceIdiom 
==
 .pad, 
"Only supported on iPad"
)
```

```swift
XCTExpectFailure
(
"<https://dev.myco.com/bugs/4923> myValidationFunction is returning false"
)
```

```swift
XCTExpectFailure
(
"<https://dev.myco.com/bugs/4923> fix myValidationFunction"
) {
    
XCTAssert
(myValidationFunction())
}
```

```swift
let
 options 
=
 
XCTExpectedFailure
.
Options
()
options.issueMatcher 
=
 { issue 
in

    
return
 issue.type 
==
 .assertionFailure
}


XCTExpectFailure
(
"<https://dev.myco.com/bugs/4923> fix myValidationFunction"
, options: options)
```

```swift
let
 options 
=
 
XCTExpectedFailure
.
Options
()

#if
 os(macOS)
options.isEnabled 
=
 
false


#endif



XCTExpectFailure
(
"<https://dev.myco.com/bugs/4923> fix myValidationFunction"
, options: options) {
    
XCTAssert
(myValidationFunction())
}
```

```swift
let
 options 
=
 
XCTExpectedFailure
.
Options
()
options.isStrict 
=
 
false



XCTExpectFailure
(
"<https://dev.myco.com/bugs/4923> fix myValidationFunction"
, options: options) {
    
XCTAssert
(myValidationFunction())
}
```

```swift
XCTExpectFailure
(
"<https://dev.myco.com/bugs/4923> fix myValidationFunction"
, strict: 
false
) {
    
XCTAssert
(myValidationFunction())
}
```

