# Wwdc2021 10229

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Discover compilation workflows in MetalThe Metal shading language is a powerful C++ based language that allows apps to render stunning effects while maintaining a flexible shader development pipeline. Discover how to more easily build and extend your render pipelines using Dynamic Libraries and Function Pointers. We'll also show you how to accelerate your shader compilation at runtime with Binary Function Archives, Function Linking, and Function Stitching.ResourcesCreating a Metal Dynamic LibraryMetalMetal Feature Set TablesMetal Shading Language SpecificationShader LibrariesHD VideoSD VideoRelated VideosWWDC23Bring your game to Mac, Part 2: Compile your shadersOptimize GPU renderers with MetalWWDC22Target and optimize GPU binaries with Metal 3WWDC21Discover Metal debugging, profiling, and asset creation toolsWWDC20Build GPU binaries with MetalGet to know Metal function pointers

The Metal shading language is a powerful C++ based language that allows apps to render stunning effects while maintaining a flexible shader development pipeline. Discover how to more easily build and extend your render pipelines using Dynamic Libraries and Function Pointers. We'll also show you how to accelerate your shader compilation at runtime with Binary Function Archives, Function Linking, and Function Stitching.

Creating a Metal Dynamic Library

Metal

Metal Feature Set Tables

Metal Shading Language Specification

Shader Libraries

HD VideoSD Video

HD Video

SD Video

Bring your game to Mac, Part 2: Compile your shaders

Optimize GPU renderers with Metal

Target and optimize GPU binaries with Metal 3

Discover Metal debugging, profiling, and asset creation tools

Build GPU binaries with Metal

Get to know Metal function pointers

Search this video…Hello. My name is Rini Patel,and I'm from the GPU software engineering team.In this session, I'll be introducing the newshader compilation workflows in Metal.The Metal shading language is a C++-based language,and its compilation model closely resembles the CPU compilation model.As GPU workloads are increasing in complexity, Metal has similarly evolvedto address the flexibility and performance needed to support the modern use cases.Some common challenges that you may have already facedwhile authoring your shaders can besharing utility code across pipelines,modifying shader behavior at runtime without recompilation,or an ability to reuse compiled GPU binaries between application launches.So, let's walk through a simple shader code and discuss the scenarios.Here, we have a simple fragment shader that returns the resultof foo() or bar(), depending on the result of condition.Now, if those functions are called by multiple pipelines,we might want to compile them just once and link them to each pipeline instead.We might need to link a different implementationof these functions at the runtime,or we might need a fragment shader to be extensiblein order to handle a new case statement for baz().We may also want to be able to call a user-provided function bat()instead of baz() from our fragment function.As you can see, there are many different requirementsthat a shader authoring pipeline may have, and Metal offersvarious APIs to support different implementations.Each approach will have a different trade-off between compilation timeand shader performance.And today, I'd like to talk about the new compilation workflows that willhelp you find the perfect balance between performance and flexibility.We are going to start with the new support for dynamic librariesfor render pipelines,as well as function pointers for render pipelines.We'll talk a bit about the additions to binaryArchive API.Then, we will see private linked functions.And finally, we'll introduce a brand-new featurefor stitching visible functions in Metal.So, let's start with the dynamic library support in Metal.Dynamic libraries are a common tool in software engineering.They are shared object files that allow you to compartmentalizeutility code into a stand-alone compilation unit.They help reduce the amount of compiled shader codeand reuse it between multiple pipelines.Additionally, they allow you to dynamically link, load,and share the GPU binary code.Last year, we introduced dynamic languages for compute pipelines,and to get more of an introduction to dynamic libraries in Metal,I encourage you to check out our previous year's presentation,"Build GPU Binaries with Metal."This year, we are bringing dynamic libraries to render and tile pipelines.With this added support, you can now share utility librariesacross all your compute and render workloads.And now that we know what dynamic libraries are,it's time to talk about some of the use cases.Helper functions are often used across general compute,vertex, fragment, and tile shaders.With the addition of dynamic libraries to render pipelines, you can nowmanage large amount of utility code and share it across all of your workloads.You can precompile the libraries that may be used at runtimewithout experiencing the compilation slow down,and even swap out functions of the runtime,simply by changing the libraries that are loaded while creating the pipeline.They also help your users to author shader code that you can loadas part of your pipeline without needing to provide the source.So, we have talked about when to use dynamic libraries.Let's now look at how to build and work with them.In our example fragment shader, we call functions foo() and bar(),but we do not provide the implementation for either at compile time.Instead, the implementation for these functions exist in a Metal librarythat we later link when creating our render pipeline.And it's also possible to provide separate librariesfor each of the functions that you may use.Let's now discuss the tools and flexibilities you haveby building dynamic libraries in Metal.You first need to compile your Metal shader source to AIR, and you can do thateither by using Xcode's Metal toolchain as part of your build process,or by compiling from source by using newLibraryWithSource API at runtime.With your compiled Metal shader in AIR, you can now create a dynamic librarysimply by using the newDynamicLibrary API.This library is ready to be used in its GPU binary format.But what if you want to reuse it in later runs?To do that, you can serialize your dynamic library to disk,and this can be done using serializeToURL API.And later, you can reuse it by calling the newDynamicLibraryWithURL API.Let's now go through an example which calls functions fromdynamic library and fragment shader.In this example, we declare functions foo() and bar() using the extern keyword,but we do not provide definitions for them.To use them, we simply call these functions from our fragment shader.And you can provide an implementation for the extern functionswhen building your Metal library.Now, keep in mind that it's also possible to replace thoseimplementations with something different at the runtime.And to do this, you just need to add your dynamic librariesto the appropriate preloaded libraries array.In our case, it's fragment, but a similar property is availablefor each stage and pipeline.And the symbols will be resolved in the same orderin which the libraries are added to this array.This workflow is well suited for experimentingwith new implementations.And that's it for the dynamic libraries.If you are looking to use Metal dynamic libraries, the support is availablefor compute pipelines on Apple GPU family 7 and above in macOS Monterey.It's also available for other GPUs, as well as most Mac family 2 devices,although you’ll need to call Metal device supports dynamic libraries.On iOS 15, this feature is available on all Apple 6 devices and above.Render and tile pipeline support is available on all devicesthat support Apple 6 feature set.Next, let's talk about the improvements to function pointers coming this year.Function pointer is a simple construct for referring to a code that we can callto make your code extensible by allowing you to call functionsthat we have not seen before.Last year, we introduced compute pipeline function pointers, and for that,I recommend you to check out our previous year's presentation,"Get to know Metal function pointers."This year, we are extending function pointer supportto the render and tile pipelines on Apple Silicon.Similar to dynamic libraries, function pointers allowcreating customizable pipelines.With function pointers, a GPU pipeline can call codethat has not been seen during the pipeline compilation.With function pointer tables, code execution behaviorcan change dynamically, either when you bind the differentfunction table or when the GPU pipeline indexes into the function pointer table.You can also decide how to balance compilation performanceagainst the runtime performance using function pointers.For fastest compilation, for example, you can precompilefunction pointers to a GPU binary and quickly deal-in the pipeline.For the best runtime performance, on the other hand,you can have your pipeline reference AIR representations of functions,allowing the compiler to do maximum optimizations.Let's now see how you can set the function pointers in your code.There are three basic building blocks.We start by instantiating the function,then configuring a pipeline with these functions,and finally, creating function tables.Once this is done, using a new render loop doesn't involve much code.So let's dive in to the details of each of the steps.To start using function pointers, we first declare the function descriptorand instantiate to compile a GPU binary version of the function.This will accelerate pipeline creation time, and it's as simple asdeclaring a descriptor and setting the option to compile to binary.When the Metal function foo is created from library using this descriptor,the function will be compiled by the GPU backend compiler.Next, we need to configure the render pipeline descriptor.First, we add the functions via the pipeline descriptorto the stage where they'll be used,which can be vertex, fragment, or tile stage.We can choose to add functions in AIR or binary form.When adding AIR functions,the compiler will statically link the visible functions,allowing for the backend compilers to optimize the code.Adding binary functions, on the other hand, will inform the driverwhich externally-compiled functions are callable from a given pipeline.One additional thing to note here is when you create a pipelinethat uses binary functionsand if the code that you are calling has a complex call chain,as illustrated here, it's important to specify the maximum call stack depthnecessary, as the compiler cannot do static analysis to determine the depth.The compiler will default to maximum depth to be run,and you may get a stack overflow if the depth isn't specified correctly.In contrast, specifying the depth correctlyleads to a better resource conception and optimal performance.So once the descriptor is fully set up, you can create your pipelinethat's ready to use function pointer.After creating your pipeline, the next steps are to createvisible function tables and populate them with function handles in the API.First, we create a visible function table using a descriptorand specify a render stage.Then, we create function handles to reference those functions.Both the function handles and table are specific to a given pipelineand selected stage.You can then insert the handles into the function table using setFunction API.Let's now see how do you use this function tables after we have done all the set up.First, as a part of the command and coding,we bind the visible function table to a buffer index.In the shader itself, the visible_function_table is passedas a buffer binding, and we can then call our functions through this table.And that was a simple case of using function pointers.When using function pointers, it's not uncommon to create a pipelinejust to find out later that you need to access one or more additional functions.Now, if you could achieve that by creating the second pipelinefrom an identical descriptor, which adds the additional functions,but that would trigger a pipeline compilation.To accelerate this process, Metal lets you specify ifyou plan on extending the original pipeline.This way, a new pipeline can be created faster from an existing pipeline,and it can use all the function pointer tables that wereinitially created for the original pipeline.To do that from code, when creating the original pipeline,set supportAddingBinaryFunctions option to YES,for all the stages that you wish to extend.Then when you need to create an extended pipeline,create RenderPipelineFunctionDescriptorand include new binary function bat() on the fragment length function list.Finally, call new RenderPipelineState with additional binary functionson renderPipeline1 to create renderPipeline2,which will be identical, but includes the additional function pointer bat.And that's pretty much it.Now that we have seen how to use function pointers, here is where you can use them.Function pointers in compute pipelines are supported on Apple GPU family 6 and above,in macOS Big Sur, and iOS 14.They are also supported on Mac family 2 devices.And this year, we are extending function pointer support for renderand tile pipelines on Apple GPU family 6 and above,in Mac OS Monterey, and iOS 15.The next topic I'd like to talk about is managing binary functioncompilation overhead.Compiling shaders can be extremely time intensive,and you may want to control the overhead it brings to an application.To help with that, we added binaryArchives to Metal last year.BinaryArchives can collect and store the compiled binary versionsof the pipeline to disk, saving compilation time and subsequent runsand reducing the memory cost associated with the compilation.This year, we are adding the ability to store visible and intersection functionto binaryArchives, allowing you to significantly reduce the overhead.So, let's take a look at how you store into and load from binaryArchives.To add a function to BinaryArchive, simply call addFunctionWithDescriptorand pass the function descriptor and source library as arguments.To load the binary function pointer from BinaryArchive,place the BinaryArchive on the function descriptor's binaryArchives array,and then call Metal library method newFunctionWithDescriptor.If any of the archives in the array has the compiled function pointer,it will be returned immediately, without having to recompile.Here are some of the rules that illustrate hownewFunctionWithDescriptor will behave with the binaryArchives.We first search for the binary version of the function in the BinaryArchive list.If the function is found, it will be returned,and if it's not found, we'll check the CompileToBinary optionand return AIR version of the function, if binary compilation wasn't requested.On the other hand, if the binary compilation was requested,then depending on the pipeline option, FailOnBinaryArchiveMisswill either compile the function binary at runtime or return nil.When you integrate MTLBinaryArchive into your application,you can use the same archive to store all your GPU-compiled code.Your render, tile, and compute pipelines, as well as your binary function pointers.And after your archive has been prepopulated with the pipeline stateobjects and binary functions, you can serialize it to a disk.Collecting and storing your GPU binaries this way will helpaccelerate shader compilation on to subsequent application runs.When using a pipeline with function pointers,you might want to cache the pipeline state object itself.But why should you cache when you have a pipeline that comeswith different function pointer combinations?For example, here, there are three pipeline descriptorswhich are identical, except for their user function pointers.So, if you are using AIR function pointers,you need to cache all permutations of the pipeline.However, when using the binary function pointers,it's enough to cache a single variant because the pipeline binary codedoesn't change when a new function pointer is added to it.And you can use that archive to find all other variantsof the pipelines, independent of which binaryfunction pointers are used in the pipeline descriptor.To wrap it up, you always want to use binaryArchives in Metal,as it's a great tool to control your pipeline compilation cost.The support for binaryArchives is available on all devices,but adding function pointers to a BinaryArchivedepends on the function pointer ability.Now, I'd like to briefly talk about our next addition this year,which is private linked functions.So far, we have discussed how dynamic libraries and function pointersprovide a lot of flexibility to your shader development pipeline,but sometimes, for performance reasons,you may want to statically link an external function into your pipeline.Last year, we added linkedFunctions API with a supportfor statically linking AIR functions.However, this requires function pointer support,as these are usable in a function table.This year, we are introducing privateFunctions.Both functions and privateFunctions are linked statically at the AIR level,but since those are private,no function handling can be made for a function pointer,and this allows compiler to fully optimize your shader code.So, where are they available?Because this feature works with code at the AIR level,it is available on all devices in macOS Monterey and iOS 15.Moving on to the last addition I'd like to discuss today,function stitching.Some applications need to generate dynamic content at the runtime.For example, to implement customizations for the graphic effectsbased on user input.Or, say, complex compute kernel based on incoming data.Function stitching is a great tool to solve this.Prior to function stitching, the only way to do thiswas by generating Metal source strings.The string-manipulation technique can be somewhat inefficient,and it also implies that the translation from Metal to AIRwill happen at runtime, which can be an expensive opration.So let's take a look at how function stitching works.Function stitching provides a mechanism to generate functionsfrom computation graphs and precompiled functions at runtime.The computation graph is a Directed Acyclic Graph.And in a graph, there are two kind of nodes:input nodes that represent the arguments of the generated function,and function nodes that represent function calls.There are also two kind of edges: data edges that represent how data flowsfrom one node to another, and control edges that representthe order in which function calls should be executed.We'll see how function stitching uses a computation graphto generate a function.We'll start with the notion of stitchable functions.The function in a graph must have the stitchable attribute.Such function is a visible function,which can be used with the functionStitching API.The stitchable functions can be part of the Metal libraryshipped with your application bundle to avoid Metal to AIR translation cost.The stitching process generates the functionsdirectly in AIR and completely skipping the Metal frontend.The generated function is a regular stitchable function,so it can be linked into a pipeline, or used directly as function pointer,or it can be used for generating other functions.So, consider the previous graph, and let's assume that we havedual functions A and C from library as described previously.Let's now see what happens when we bind these functions to the graph.The stitcher here associates a corresponding function typeto each function node.N0 and N1 get the type from FunctionA,and N2 gets the type from FunctionC.After that, the stitcher infers the types for the input nodesby looking at the parameter types of the functions using them.For example, Input0 is inferred to be the type devisedin pointer because it's the first argument for N0 and N1.The stitcher then generates a function equivalentto the following one described in Metal.With the functionStitching API, we can generate a librarycontaining such functions directly from AIR.And now that we have fair idea on how stitching works,here is how you can use it in the API.First, we need to define the stitched function inputs.In this case, we simply make enough input nodes to account for all the arguments.Next, we create function nodes for each function we wish to call in our graph.For each function call, we define the name, arguments,and control dependencies if we have any explicit ordering requirements.And finally, we create the graph with the function name,the function nodes used in a graph,and any function attribute we want to apply.You also assign an outputNode, which will return the output valueof the resulting stitched function.So, we have our graph.Now, we can create a function using it.The first step is to create a StitchedLibraryDescriptor.We add our stitchableFunctions and functionGraph to this descriptor.We then create a library using the descriptor, and now,we can create our stitched function out of this library.This stitched function is now ready to be used anywhere a stitchable functionis expected, including as a function in another stitching graph.And that's it for the function stitching.This API is also available across all devices in macOS Monterey and iOS 15.So, as a quick recap, today, we looked at dynamic librariesand function pointers for render pipelines.Private linked functions, which can be used to statically link visible functions.And how function stitching can save compilation timewhen dynamically creating shaders.So, when would you choose one over the other?Dynamic libraries are an excellent choice to link helper and utility functions.And they are best used when you have a fixed set of utility functions,and those functions do not change very frequently.Function pointers add the ability for a shader to invoke functionsthat it knows nothing about, other than their signature.It doesn't need to know how many functions exist, the names,or even what speed-flexibility trade-offthe developer has made using AIR or binaries.And this year, you can also cache function pointers.Private functions offer you a way to statically link functionsto a pipeline state object by name.They are internal to the pipeline, so they cannot be encodedin a visible function table, but they allow the compilerto do maximum optimization,and they are supported across all GPU families.And finally, function stitching gives you a way to precompile snippetsof your code directly to AIR and perform function compilation at runtime.If you are compositing Metal shader strings todayand have to pay the cost of compiling from source at runtime,then function stitching would greatly accelerate this workflow.I hope that you can leverage these compiler featuresto develop new experiences using Metal.Thanks for watching, and enjoy the rest of WWDC 2021.[music]

Hello. My name is Rini Patel,and I'm from the GPU software engineering team.In this session, I'll be introducing the newshader compilation workflows in Metal.The Metal shading language is a C++-based language,and its compilation model closely resembles the CPU compilation model.As GPU workloads are increasing in complexity, Metal has similarly evolvedto address the flexibility and performance needed to support the modern use cases.Some common challenges that you may have already facedwhile authoring your shaders can besharing utility code across pipelines,modifying shader behavior at runtime without recompilation,or an ability to reuse compiled GPU binaries between application launches.So, let's walk through a simple shader code and discuss the scenarios.

Here, we have a simple fragment shader that returns the resultof foo() or bar(), depending on the result of condition.Now, if those functions are called by multiple pipelines,we might want to compile them just once and link them to each pipeline instead.

We might need to link a different implementationof these functions at the runtime,or we might need a fragment shader to be extensiblein order to handle a new case statement for baz().We may also want to be able to call a user-provided function bat()instead of baz() from our fragment function.As you can see, there are many different requirementsthat a shader authoring pipeline may have, and Metal offersvarious APIs to support different implementations.Each approach will have a different trade-off between compilation timeand shader performance.And today, I'd like to talk about the new compilation workflows that willhelp you find the perfect balance between performance and flexibility.We are going to start with the new support for dynamic librariesfor render pipelines,as well as function pointers for render pipelines.We'll talk a bit about the additions to binaryArchive API.Then, we will see private linked functions.And finally, we'll introduce a brand-new featurefor stitching visible functions in Metal.So, let's start with the dynamic library support in Metal.Dynamic libraries are a common tool in software engineering.They are shared object files that allow you to compartmentalizeutility code into a stand-alone compilation unit.They help reduce the amount of compiled shader codeand reuse it between multiple pipelines.Additionally, they allow you to dynamically link, load,and share the GPU binary code.

Last year, we introduced dynamic languages for compute pipelines,and to get more of an introduction to dynamic libraries in Metal,I encourage you to check out our previous year's presentation,"Build GPU Binaries with Metal."This year, we are bringing dynamic libraries to render and tile pipelines.With this added support, you can now share utility librariesacross all your compute and render workloads.And now that we know what dynamic libraries are,it's time to talk about some of the use cases.Helper functions are often used across general compute,vertex, fragment, and tile shaders.With the addition of dynamic libraries to render pipelines, you can nowmanage large amount of utility code and share it across all of your workloads.You can precompile the libraries that may be used at runtimewithout experiencing the compilation slow down,and even swap out functions of the runtime,simply by changing the libraries that are loaded while creating the pipeline.They also help your users to author shader code that you can loadas part of your pipeline without needing to provide the source.So, we have talked about when to use dynamic libraries.Let's now look at how to build and work with them.In our example fragment shader, we call functions foo() and bar(),but we do not provide the implementation for either at compile time.Instead, the implementation for these functions exist in a Metal librarythat we later link when creating our render pipeline.And it's also possible to provide separate librariesfor each of the functions that you may use.Let's now discuss the tools and flexibilities you haveby building dynamic libraries in Metal.You first need to compile your Metal shader source to AIR, and you can do thateither by using Xcode's Metal toolchain as part of your build process,or by compiling from source by using newLibraryWithSource API at runtime.With your compiled Metal shader in AIR, you can now create a dynamic librarysimply by using the newDynamicLibrary API.This library is ready to be used in its GPU binary format.But what if you want to reuse it in later runs?To do that, you can serialize your dynamic library to disk,and this can be done using serializeToURL API.And later, you can reuse it by calling the newDynamicLibraryWithURL API.Let's now go through an example which calls functions fromdynamic library and fragment shader.In this example, we declare functions foo() and bar() using the extern keyword,but we do not provide definitions for them.To use them, we simply call these functions from our fragment shader.And you can provide an implementation for the extern functionswhen building your Metal library.Now, keep in mind that it's also possible to replace thoseimplementations with something different at the runtime.And to do this, you just need to add your dynamic librariesto the appropriate preloaded libraries array.In our case, it's fragment, but a similar property is availablefor each stage and pipeline.And the symbols will be resolved in the same orderin which the libraries are added to this array.This workflow is well suited for experimentingwith new implementations.And that's it for the dynamic libraries.

If you are looking to use Metal dynamic libraries, the support is availablefor compute pipelines on Apple GPU family 7 and above in macOS Monterey.It's also available for other GPUs, as well as most Mac family 2 devices,although you’ll need to call Metal device supports dynamic libraries.On iOS 15, this feature is available on all Apple 6 devices and above.Render and tile pipeline support is available on all devicesthat support Apple 6 feature set.

Next, let's talk about the improvements to function pointers coming this year.

Function pointer is a simple construct for referring to a code that we can callto make your code extensible by allowing you to call functionsthat we have not seen before.Last year, we introduced compute pipeline function pointers, and for that,I recommend you to check out our previous year's presentation,"Get to know Metal function pointers."This year, we are extending function pointer supportto the render and tile pipelines on Apple Silicon.Similar to dynamic libraries, function pointers allowcreating customizable pipelines.With function pointers, a GPU pipeline can call codethat has not been seen during the pipeline compilation.With function pointer tables, code execution behaviorcan change dynamically, either when you bind the differentfunction table or when the GPU pipeline indexes into the function pointer table.You can also decide how to balance compilation performanceagainst the runtime performance using function pointers.For fastest compilation, for example, you can precompilefunction pointers to a GPU binary and quickly deal-in the pipeline.For the best runtime performance, on the other hand,you can have your pipeline reference AIR representations of functions,allowing the compiler to do maximum optimizations.Let's now see how you can set the function pointers in your code.There are three basic building blocks.We start by instantiating the function,then configuring a pipeline with these functions,and finally, creating function tables.Once this is done, using a new render loop doesn't involve much code.So let's dive in to the details of each of the steps.To start using function pointers, we first declare the function descriptorand instantiate to compile a GPU binary version of the function.This will accelerate pipeline creation time, and it's as simple asdeclaring a descriptor and setting the option to compile to binary.When the Metal function foo is created from library using this descriptor,the function will be compiled by the GPU backend compiler.Next, we need to configure the render pipeline descriptor.First, we add the functions via the pipeline descriptorto the stage where they'll be used,which can be vertex, fragment, or tile stage.We can choose to add functions in AIR or binary form.When adding AIR functions,the compiler will statically link the visible functions,allowing for the backend compilers to optimize the code.Adding binary functions, on the other hand, will inform the driverwhich externally-compiled functions are callable from a given pipeline.One additional thing to note here is when you create a pipelinethat uses binary functionsand if the code that you are calling has a complex call chain,as illustrated here, it's important to specify the maximum call stack depthnecessary, as the compiler cannot do static analysis to determine the depth.The compiler will default to maximum depth to be run,and you may get a stack overflow if the depth isn't specified correctly.In contrast, specifying the depth correctlyleads to a better resource conception and optimal performance.So once the descriptor is fully set up, you can create your pipelinethat's ready to use function pointer.After creating your pipeline, the next steps are to createvisible function tables and populate them with function handles in the API.First, we create a visible function table using a descriptorand specify a render stage.Then, we create function handles to reference those functions.Both the function handles and table are specific to a given pipelineand selected stage.You can then insert the handles into the function table using setFunction API.Let's now see how do you use this function tables after we have done all the set up.First, as a part of the command and coding,we bind the visible function table to a buffer index.In the shader itself, the visible_function_table is passedas a buffer binding, and we can then call our functions through this table.And that was a simple case of using function pointers.When using function pointers, it's not uncommon to create a pipelinejust to find out later that you need to access one or more additional functions.Now, if you could achieve that by creating the second pipelinefrom an identical descriptor, which adds the additional functions,but that would trigger a pipeline compilation.To accelerate this process, Metal lets you specify ifyou plan on extending the original pipeline.This way, a new pipeline can be created faster from an existing pipeline,and it can use all the function pointer tables that wereinitially created for the original pipeline.To do that from code, when creating the original pipeline,set supportAddingBinaryFunctions option to YES,for all the stages that you wish to extend.Then when you need to create an extended pipeline,create RenderPipelineFunctionDescriptorand include new binary function bat() on the fragment length function list.Finally, call new RenderPipelineState with additional binary functionson renderPipeline1 to create renderPipeline2,which will be identical, but includes the additional function pointer bat.And that's pretty much it.Now that we have seen how to use function pointers, here is where you can use them.Function pointers in compute pipelines are supported on Apple GPU family 6 and above,in macOS Big Sur, and iOS 14.They are also supported on Mac family 2 devices.And this year, we are extending function pointer support for renderand tile pipelines on Apple GPU family 6 and above,in Mac OS Monterey, and iOS 15.

The next topic I'd like to talk about is managing binary functioncompilation overhead.Compiling shaders can be extremely time intensive,and you may want to control the overhead it brings to an application.To help with that, we added binaryArchives to Metal last year.BinaryArchives can collect and store the compiled binary versionsof the pipeline to disk, saving compilation time and subsequent runsand reducing the memory cost associated with the compilation.This year, we are adding the ability to store visible and intersection functionto binaryArchives, allowing you to significantly reduce the overhead.So, let's take a look at how you store into and load from binaryArchives.To add a function to BinaryArchive, simply call addFunctionWithDescriptorand pass the function descriptor and source library as arguments.To load the binary function pointer from BinaryArchive,place the BinaryArchive on the function descriptor's binaryArchives array,and then call Metal library method newFunctionWithDescriptor.If any of the archives in the array has the compiled function pointer,it will be returned immediately, without having to recompile.Here are some of the rules that illustrate hownewFunctionWithDescriptor will behave with the binaryArchives.We first search for the binary version of the function in the BinaryArchive list.If the function is found, it will be returned,and if it's not found, we'll check the CompileToBinary optionand return AIR version of the function, if binary compilation wasn't requested.On the other hand, if the binary compilation was requested,then depending on the pipeline option, FailOnBinaryArchiveMisswill either compile the function binary at runtime or return nil.When you integrate MTLBinaryArchive into your application,you can use the same archive to store all your GPU-compiled code.Your render, tile, and compute pipelines, as well as your binary function pointers.

And after your archive has been prepopulated with the pipeline stateobjects and binary functions, you can serialize it to a disk.Collecting and storing your GPU binaries this way will helpaccelerate shader compilation on to subsequent application runs.When using a pipeline with function pointers,you might want to cache the pipeline state object itself.But why should you cache when you have a pipeline that comeswith different function pointer combinations?For example, here, there are three pipeline descriptorswhich are identical, except for their user function pointers.So, if you are using AIR function pointers,you need to cache all permutations of the pipeline.However, when using the binary function pointers,it's enough to cache a single variant because the pipeline binary codedoesn't change when a new function pointer is added to it.And you can use that archive to find all other variantsof the pipelines, independent of which binaryfunction pointers are used in the pipeline descriptor.To wrap it up, you always want to use binaryArchives in Metal,as it's a great tool to control your pipeline compilation cost.The support for binaryArchives is available on all devices,but adding function pointers to a BinaryArchivedepends on the function pointer ability.Now, I'd like to briefly talk about our next addition this year,which is private linked functions.So far, we have discussed how dynamic libraries and function pointersprovide a lot of flexibility to your shader development pipeline,but sometimes, for performance reasons,you may want to statically link an external function into your pipeline.Last year, we added linkedFunctions API with a supportfor statically linking AIR functions.However, this requires function pointer support,as these are usable in a function table.This year, we are introducing privateFunctions.Both functions and privateFunctions are linked statically at the AIR level,but since those are private,no function handling can be made for a function pointer,and this allows compiler to fully optimize your shader code.So, where are they available?Because this feature works with code at the AIR level,it is available on all devices in macOS Monterey and iOS 15.Moving on to the last addition I'd like to discuss today,function stitching.Some applications need to generate dynamic content at the runtime.For example, to implement customizations for the graphic effectsbased on user input.Or, say, complex compute kernel based on incoming data.

Function stitching is a great tool to solve this.

Prior to function stitching, the only way to do thiswas by generating Metal source strings.The string-manipulation technique can be somewhat inefficient,and it also implies that the translation from Metal to AIRwill happen at runtime, which can be an expensive opration.So let's take a look at how function stitching works.Function stitching provides a mechanism to generate functionsfrom computation graphs and precompiled functions at runtime.The computation graph is a Directed Acyclic Graph.And in a graph, there are two kind of nodes:input nodes that represent the arguments of the generated function,and function nodes that represent function calls.There are also two kind of edges: data edges that represent how data flowsfrom one node to another, and control edges that representthe order in which function calls should be executed.We'll see how function stitching uses a computation graphto generate a function.We'll start with the notion of stitchable functions.The function in a graph must have the stitchable attribute.Such function is a visible function,which can be used with the functionStitching API.The stitchable functions can be part of the Metal libraryshipped with your application bundle to avoid Metal to AIR translation cost.

The stitching process generates the functionsdirectly in AIR and completely skipping the Metal frontend.The generated function is a regular stitchable function,so it can be linked into a pipeline, or used directly as function pointer,or it can be used for generating other functions.So, consider the previous graph, and let's assume that we havedual functions A and C from library as described previously.Let's now see what happens when we bind these functions to the graph.The stitcher here associates a corresponding function typeto each function node.N0 and N1 get the type from FunctionA,and N2 gets the type from FunctionC.After that, the stitcher infers the types for the input nodesby looking at the parameter types of the functions using them.For example, Input0 is inferred to be the type devisedin pointer because it's the first argument for N0 and N1.

The stitcher then generates a function equivalentto the following one described in Metal.With the functionStitching API, we can generate a librarycontaining such functions directly from AIR.And now that we have fair idea on how stitching works,here is how you can use it in the API.First, we need to define the stitched function inputs.In this case, we simply make enough input nodes to account for all the arguments.Next, we create function nodes for each function we wish to call in our graph.For each function call, we define the name, arguments,and control dependencies if we have any explicit ordering requirements.And finally, we create the graph with the function name,the function nodes used in a graph,and any function attribute we want to apply.You also assign an outputNode, which will return the output valueof the resulting stitched function.So, we have our graph.Now, we can create a function using it.The first step is to create a StitchedLibraryDescriptor.We add our stitchableFunctions and functionGraph to this descriptor.We then create a library using the descriptor, and now,we can create our stitched function out of this library.This stitched function is now ready to be used anywhere a stitchable functionis expected, including as a function in another stitching graph.And that's it for the function stitching.This API is also available across all devices in macOS Monterey and iOS 15.So, as a quick recap, today, we looked at dynamic librariesand function pointers for render pipelines.Private linked functions, which can be used to statically link visible functions.And how function stitching can save compilation timewhen dynamically creating shaders.

So, when would you choose one over the other?Dynamic libraries are an excellent choice to link helper and utility functions.And they are best used when you have a fixed set of utility functions,and those functions do not change very frequently.Function pointers add the ability for a shader to invoke functionsthat it knows nothing about, other than their signature.It doesn't need to know how many functions exist, the names,or even what speed-flexibility trade-offthe developer has made using AIR or binaries.And this year, you can also cache function pointers.Private functions offer you a way to statically link functionsto a pipeline state object by name.They are internal to the pipeline, so they cannot be encodedin a visible function table, but they allow the compilerto do maximum optimization,and they are supported across all GPU families.And finally, function stitching gives you a way to precompile snippetsof your code directly to AIR and perform function compilation at runtime.If you are compositing Metal shader strings todayand have to pay the cost of compiling from source at runtime,then function stitching would greatly accelerate this workflow.I hope that you can leverage these compiler featuresto develop new experiences using Metal.Thanks for watching, and enjoy the rest of WWDC 2021.[music]

5:38 -Shading language

9:01 -Declare and instantiate visible functions

9:30 -Configure pipeline descriptor

10:47 -Create and populate visible function table

11:21 -Encoding and calling function pointers

12:20 -Incremental pipeline creation

20:30 -Stitching process

21:32 -Creating the graph

22:18 -Configure stitched library descriptor

## Code Samples

```swift
// Declare external functions



extern
 float4 foo(FragmentInput input);

extern
 float4 bar(FragmentInput input);


// Use functions in shader


fragment float4 main(FragmentInput input [[stage_in]])
{
    
switch
(condition(input)) 
    {
        
case
 
0
: 
            
return
 foo(input);
        
case
 
1
:
            
return
 bar(input);
    }
}
```

```swift
// Declare a descriptor and set CompileToBinary options



MTLFunctionDescriptor
* functionDescriptor = [
MTLFunctionDescriptor
 new];
functionDescriptor.options = 
MTLFunctionOptionCompileToBinary
;


// Backend compile the function


functionDescriptor.name = 
@"foo"
;

id
<
MTLFunction
> foo = [library newFunctionWithDescriptor:functionDescriptor
```

```swift
// Provide a list of functions that the pipeline stage may call



// AIR functions


renderPipeDesc.fragmentLinkedFunctions.functions = @[foo, bar, baz];


// Binary functions


renderPipeDesc.fragmentLinkedFunctions.binaryFunctions = @[foo, bar, baz];
```

```swift
// Create visible function table


    [renderPipeline newVisibleFunctionTableWithDescriptor:stage:];


// Create function handles


    [renderPipeline functionHandleWithFunction:stage:];


// Insert handles into table


    [visibleFunctionTable setFunction:atIndex:];
```

```swift
// Bind visible function table objects to each stage


    [renderCommandEncoder setFragmentVisibleFunctionTable:atBufferIndex:];


// Usage in shader


   fragment float4 shaderFunc(FragmentData vo[[stage_in]],
                              visible_function_table<float4(float3)>materials[[buffer(
0
)]])
   {
   		 
//...

     
       
return
 materials[materialSelector](coord);
   }
```

```swift
// Enable incrementally adding binary functions per stage


renderPipeDesc.supportAddingFragmentBinaryFunctions = 
YES
;


// Create render pipeline functions descriptor



MTLRenderPipelineFunctionsDescriptor
 extraDesc;
extraDesc.fragmentAdditionalBinaryFunctions = @[bat];


// Instantiate render pipeline state



id
<
MTLRenderPipelineState
> renderPipeline2 =
  [renderPipeline1 newRenderPipelineStateWithAdditionalBinaryFunctions:extraDesc
```

```swift
[[stitchable]] 
int
 FunctionA(device 
int
*, 
int
) {…}
[[stitchable]] 
int
 FunctionC(
int
, 
int
) {…}

[[stitchable]]

int
 ResultFunction(device 
int
* Input0,
                   
int
 Input1, 
                   
int
 Input2)
{
  
int
 N0 = FunctionA(Input0, Input1);
  
int
 N1 = FunctionA(Input0, Input2);
  
int
 N2 = FunctionC(N0, N1);    
  
return
 N2;
}
```

```swift
// Create input nodes


  inputs[
0
] = [[
MTLFunctionStitchingInputNode
 alloc] initWithArgumentIndex:
0
];


// Create function nodes


  n0 = [[
MTLFunctionStitchingFunctionNode
 alloc] initWithName:
@"FunctionA"

                                                    arguments:@[inputs[
0
], inputs[
1
]]
                                          controlDependencies:@[]];
  n1 = [[
MTLFunctionStitchingFunctionNode
 alloc] initWithName:
@"FunctionA"

                                                    arguments:@[inputs[
0
], inputs[
2
]]
                                          controlDependencies:@[]];
  n2 = [[
MTLFunctionStitchingFunctionNode
 alloc] initWithName:
@"FunctionC"

                                                    arguments:@[n0, n1]
                                          controlDependencies:@[]];


// Create graph


  graph = [[
MTLFunctionStitchingGraph
 alloc] initWithFunctionName:
@"ResultFunction"

                                                            nodes:@[n0, n1]
                                                       outputNode:n2
                                                       attributes:@[]];
```

```swift
// Configure stitched library descriptor

  
  
MTLStitchedLibraryDescriptor
* descriptor = [
MTLStitchedLibraryDescriptor
 new];

  descriptor.functions      = @[stitchableFunctions];
  descriptor.functionGraphs = @[graph];


// Create stitched function


  
id
<
MTLLibrary
> lib = [device newLibraryWithDescriptor:descriptor 
                                                  error:&error];

  
id
<
MTLFunction
> stitchedFunction = [lib newFunctionWithName:
@"ResultFunction"
];
```

