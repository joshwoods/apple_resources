WEBVTT

00:00:00.506 --> 00:00:06.456
[ Music ]

00:00:06.955 --> 00:00:08.486
>> Hello. My name is Simon

00:00:08.486 --> 00:00:09.886
Gladman and I'm with the Vector

00:00:09.886 --> 00:00:10.696
and Numerics group.

00:00:11.316 --> 00:00:12.396
In this presentation, I'll be

00:00:12.396 --> 00:00:14.276
talking about two topics.

00:00:14.546 --> 00:00:16.736
First, our new Swift Overlay for

00:00:16.736 --> 00:00:17.426
Accelerate.

00:00:17.466 --> 00:00:19.086
And second, measuring

00:00:19.086 --> 00:00:20.636
Accelerate's performance using

00:00:20.636 --> 00:00:21.816
the Linpack benchmark.

00:00:22.166 --> 00:00:23.396
Before we dive into the Swift

00:00:23.396 --> 00:00:25.806
Overlay, let's recap exactly

00:00:25.806 --> 00:00:27.106
what the Accelerate framework

00:00:27.106 --> 00:00:27.506
is.

00:00:27.826 --> 00:00:30.476
The primary purpose of

00:00:30.476 --> 00:00:31.796
Accelerate is to provide

00:00:31.796 --> 00:00:33.486
thousands of low-level math

00:00:33.486 --> 00:00:35.676
primitives that run on a CPU and

00:00:35.676 --> 00:00:36.776
support image and signal

00:00:36.776 --> 00:00:38.676
processing, vector arithmetic,

00:00:38.866 --> 00:00:40.466
linear algebra, and machine

00:00:40.466 --> 00:00:41.006
learning.

00:00:41.716 --> 00:00:42.816
Most of these primitives are

00:00:42.816 --> 00:00:43.956
hand tuned to the

00:00:43.956 --> 00:00:45.076
microarchitecture of the

00:00:45.076 --> 00:00:45.646
processor.

00:00:46.206 --> 00:00:47.416
This means we get excellent

00:00:47.416 --> 00:00:49.636
performance and this performance

00:00:49.636 --> 00:00:51.356
translates directly into energy

00:00:51.356 --> 00:00:52.036
savings.

00:00:52.276 --> 00:00:54.776
So, if you're an app developer

00:00:54.886 --> 00:00:56.096
and you use the Accelerate

00:00:56.096 --> 00:00:57.666
framework, not only will your

00:00:57.666 --> 00:00:59.456
application run faster, but

00:00:59.456 --> 00:01:00.836
you'll also use less battery

00:01:00.836 --> 00:01:01.196
life.

00:01:03.366 --> 00:01:05.215
We provide the primitives across

00:01:05.215 --> 00:01:06.696
all of Apple's platforms.

00:01:06.986 --> 00:01:09.616
This includes not only macOS and

00:01:09.616 --> 00:01:13.966
iOS but watchOS and tVOS as

00:01:13.966 --> 00:01:14.316
well.

00:01:15.766 --> 00:01:17.236
This means your users are going

00:01:17.236 --> 00:01:18.526
to have an overall better

00:01:18.526 --> 00:01:19.296
experience.

00:01:20.746 --> 00:01:22.856
Accelerate's libraries are

00:01:22.856 --> 00:01:24.556
immensely powerful but up until

00:01:24.556 --> 00:01:26.226
now, their interfaces weren't

00:01:26.226 --> 00:01:27.266
that friendly to Swift

00:01:27.266 --> 00:01:27.996
developers.

00:01:28.516 --> 00:01:30.196
We've looked at four libraries

00:01:30.196 --> 00:01:32.506
and created new Swift-friendly

00:01:32.506 --> 00:01:33.946
APIs to make using Acclerate in

00:01:33.946 --> 00:01:35.646
Swift projects really easy.

00:01:36.446 --> 00:01:37.906
The four libraries we focused on

00:01:37.906 --> 00:01:41.386
are vDSP that provides digital

00:01:41.386 --> 00:01:42.806
signal processing routines

00:01:42.966 --> 00:01:44.256
including arithmetic on large

00:01:44.256 --> 00:01:46.506
vectors, Fourier transforms,

00:01:46.696 --> 00:01:48.216
biquadratic filtering, and

00:01:48.216 --> 00:01:49.696
powerful type conversion.

00:01:49.876 --> 00:01:53.536
vForce that provides arithmetic

00:01:53.536 --> 00:01:54.956
and transcendental functions

00:01:54.956 --> 00:01:56.606
including trig and logarithmic

00:01:56.606 --> 00:01:57.306
routines.

00:01:57.686 --> 00:02:00.206
Quadrature, that's dedicated to

00:02:00.206 --> 00:02:01.706
the numerical integration of

00:02:01.706 --> 00:02:02.456
functions.

00:02:02.836 --> 00:02:05.726
And vImage, that provides a huge

00:02:05.726 --> 00:02:07.126
selection of image processing

00:02:07.126 --> 00:02:09.096
functions and integrates easily

00:02:09.096 --> 00:02:10.336
with core graphics and core

00:02:10.336 --> 00:02:10.686
video.

00:02:11.476 --> 00:02:13.716
Accelerate gets its performance

00:02:13.716 --> 00:02:15.636
benefits by using vectorization.

00:02:15.946 --> 00:02:18.806
To understand vectorization,

00:02:18.876 --> 00:02:20.306
let's first look at a simple

00:02:20.306 --> 00:02:21.956
calculation over the elements of

00:02:21.956 --> 00:02:23.606
an array using scalar code.

00:02:24.666 --> 00:02:26.576
If, for example, you're writing

00:02:26.576 --> 00:02:28.006
code that multiplies each

00:02:28.006 --> 00:02:29.916
element of one array with the

00:02:29.916 --> 00:02:31.176
corresponding element in

00:02:31.176 --> 00:02:32.936
another, and you're using a four

00:02:32.936 --> 00:02:34.886
loop, each pair of elements are

00:02:34.886 --> 00:02:36.676
separately loaded, multiplied

00:02:36.676 --> 00:02:37.906
together, and the results

00:02:37.906 --> 00:02:38.306
stored.

00:02:39.496 --> 00:02:41.376
So, after the first elements in

00:02:41.376 --> 00:02:42.866
A and B are multiplied together

00:02:42.866 --> 00:02:44.166
to calculate the first element

00:02:44.166 --> 00:02:46.026
in C, the second pair are

00:02:46.026 --> 00:02:46.686
processed.

00:02:47.256 --> 00:02:49.196
Then, the third.

00:02:49.906 --> 00:02:54.686
And, finally, the fourth.

00:02:55.006 --> 00:02:56.466
However, if you're processing

00:02:56.466 --> 00:02:57.736
the elements of an array using

00:02:57.736 --> 00:02:59.826
Accelerate, your calculation is

00:02:59.826 --> 00:03:01.396
performed on single instruction

00:03:01.396 --> 00:03:03.156
multiple data, or simD

00:03:03.156 --> 00:03:03.816
registers.

00:03:04.616 --> 00:03:05.886
These registers can perform the

00:03:05.886 --> 00:03:07.506
same instruction on multiple

00:03:07.506 --> 00:03:09.606
items of data by packing those

00:03:09.606 --> 00:03:10.896
multiple items into a single

00:03:10.896 --> 00:03:11.416
register.

00:03:12.096 --> 00:03:14.676
For example, a single 128-bit

00:03:14.676 --> 00:03:17.116
register can actually store four

00:03:17.116 --> 00:03:19.486
32-bit floating point values.

00:03:19.876 --> 00:03:21.146
So, a vectorized multiply

00:03:21.146 --> 00:03:22.916
operation can simultaneously

00:03:22.916 --> 00:03:24.666
multiply four pairs of elements

00:03:24.666 --> 00:03:25.226
at a time.

00:03:26.256 --> 00:03:27.636
This means that not only will

00:03:27.636 --> 00:03:29.236
the task be quicker, it will

00:03:29.236 --> 00:03:30.936
also be significantly more

00:03:30.936 --> 00:03:31.756
energy efficient.

00:03:34.276 --> 00:03:35.926
The multiply function we just

00:03:35.926 --> 00:03:37.456
looked at part of Accelerate's

00:03:37.456 --> 00:03:38.686
digital signal processing

00:03:38.686 --> 00:03:40.146
library, vDSP.

00:03:40.766 --> 00:03:42.236
So, let's begin by looking at

00:03:42.236 --> 00:03:44.486
how the new Swift API simplifies

00:03:44.486 --> 00:03:46.146
using vDSP.

00:03:47.496 --> 00:03:51.176
vDSP provides vectorized digital

00:03:51.176 --> 00:03:52.516
signal processing functions

00:03:52.516 --> 00:03:53.956
including Fourier transforms,

00:03:53.956 --> 00:03:56.116
biquadratic filtering,

00:03:56.256 --> 00:03:58.496
convolution, and correlation.

00:03:59.676 --> 00:04:02.226
Furthermore, vDSP also provides

00:04:02.226 --> 00:04:03.506
some powerful, more general

00:04:03.506 --> 00:04:05.166
functions including element-wise

00:04:05.166 --> 00:04:07.326
arithmetic and type conversion.

00:04:09.056 --> 00:04:10.656
So, even if you don't have an

00:04:10.656 --> 00:04:12.396
immediate need to, for example,

00:04:12.396 --> 00:04:13.686
compute the coherence of two

00:04:13.686 --> 00:04:16.305
signals, you may find that

00:04:16.305 --> 00:04:17.676
vDSP's general computation

00:04:17.676 --> 00:04:19.586
routines offer a solution to

00:04:19.586 --> 00:04:21.315
improve your app's performance.

00:04:24.616 --> 00:04:26.096
Let's take a look at some basic

00:04:26.096 --> 00:04:26.816
arithmetic.

00:04:27.206 --> 00:04:29.776
An example could be given four

00:04:29.776 --> 00:04:31.056
arrays of single-precision

00:04:31.056 --> 00:04:33.066
values, you need to calculate

00:04:33.066 --> 00:04:34.456
the element-wise sum of two of

00:04:34.456 --> 00:04:36.266
the array's the element-wise

00:04:36.266 --> 00:04:37.806
difference in the other two, and

00:04:37.806 --> 00:04:40.306
multiply those results with each

00:04:40.846 --> 00:04:40.966
other.

00:04:41.666 --> 00:04:43.446
Using a four loop is a perfectly

00:04:43.446 --> 00:04:44.496
reasonable solution to this

00:04:44.496 --> 00:04:46.126
problem and calculates the

00:04:46.126 --> 00:04:47.106
expected results.

00:04:47.906 --> 00:04:49.706
Here's how you perform that

00:04:49.706 --> 00:04:52.096
calculation using vDSP's classic

00:04:52.096 --> 00:04:52.416
API.

00:04:53.526 --> 00:04:55.276
Using vDSP is approximately

00:04:55.276 --> 00:04:57.166
three times faster than the four

00:04:57.166 --> 00:04:57.536
loop.

00:04:59.536 --> 00:05:00.926
Here's the same computation

00:05:00.926 --> 00:05:02.566
using our new Swift API for

00:05:02.566 --> 00:05:03.206
vDSP.

00:05:03.206 --> 00:05:05.426
We're exposing the new

00:05:05.426 --> 00:05:06.906
Swift-friendly functions through

00:05:06.906 --> 00:05:09.076
our vDSP namespace and you can

00:05:09.076 --> 00:05:10.366
see the function and parameter

00:05:10.366 --> 00:05:11.896
names explain the operation.

00:05:12.806 --> 00:05:14.176
Because the new functions work

00:05:14.176 --> 00:05:15.626
with familiar types including

00:05:15.626 --> 00:05:17.146
arrays and array slices rather

00:05:17.146 --> 00:05:18.866
than pointers, you no longer

00:05:18.866 --> 00:05:20.296
need to explicitly pass the

00:05:20.296 --> 00:05:20.736
count.

00:05:21.246 --> 00:05:22.536
So, the entire function call is

00:05:22.536 --> 00:05:24.616
clearer and more concise.

00:05:25.776 --> 00:05:28.676
Passing an initialized result

00:05:28.676 --> 00:05:29.686
array offers the best

00:05:29.686 --> 00:05:31.236
performance and you can

00:05:31.236 --> 00:05:32.936
obviously reuse that array in

00:05:32.936 --> 00:05:34.476
other operations for further

00:05:34.476 --> 00:05:35.646
performance benefits.

00:05:36.446 --> 00:05:38.946
However, we're also providing

00:05:38.946 --> 00:05:40.726
self-allocating functions.

00:05:41.096 --> 00:05:42.846
These make use of Swift's new

00:05:42.846 --> 00:05:44.596
ability to access an array's

00:05:44.596 --> 00:05:46.786
uninitialized buffer to return

00:05:46.786 --> 00:05:48.156
the result of a computation.

00:05:48.936 --> 00:05:50.496
Although not quite as fast as

00:05:50.496 --> 00:05:52.386
passing existing storage, it's

00:05:52.386 --> 00:05:53.936
still faster than the scalar

00:05:53.936 --> 00:05:55.936
approach and, in some cases,

00:05:55.936 --> 00:05:58.846
will simplify your code.

00:06:00.076 --> 00:06:02.006
Another common task that vDSP

00:06:02.006 --> 00:06:03.286
can vectorize is type

00:06:03.286 --> 00:06:03.976
conversion.

00:06:04.536 --> 00:06:06.416
This example converts an array

00:06:06.416 --> 00:06:07.566
containing double precision

00:06:07.566 --> 00:06:10.256
values to 16-bit unsigned

00:06:10.256 --> 00:06:11.846
integer values rounding toward

00:06:11.846 --> 00:06:12.326
zero.

00:06:13.676 --> 00:06:16.796
The scalar version uses map with

00:06:16.796 --> 00:06:17.776
explicit rounding.

00:06:18.136 --> 00:06:20.026
Again, this is a perfectly

00:06:20.026 --> 00:06:21.776
reasonable technique to use, but

00:06:21.776 --> 00:06:24.156
vDSP can vectorize this task to

00:06:24.156 --> 00:06:25.266
improve performance.

00:06:25.806 --> 00:06:29.196
In this example, vDSP is

00:06:29.196 --> 00:06:31.036
approximately four times faster

00:06:31.036 --> 00:06:32.616
than the previous scalar

00:06:32.616 --> 00:06:33.446
implementation.

00:06:34.876 --> 00:06:37.386
The new Swift version of the

00:06:37.386 --> 00:06:38.996
vDSP function offers a clear

00:06:38.996 --> 00:06:39.746
interface.

00:06:40.596 --> 00:06:42.166
The function accepts a source

00:06:42.166 --> 00:06:42.486
array.

00:06:42.916 --> 00:06:44.116
The integer type you ought to

00:06:44.116 --> 00:06:46.036
convert each element to, and an

00:06:46.036 --> 00:06:47.636
enumeration to specify the

00:06:47.636 --> 00:06:48.196
rounding.

00:06:51.936 --> 00:06:54.416
vDSP provides Fourier transforms

00:06:54.416 --> 00:06:55.946
for transforming one-dimensional

00:06:56.056 --> 00:06:57.966
and two-dimensional data between

00:06:57.966 --> 00:06:59.176
the time domain and the

00:06:59.176 --> 00:07:00.176
frequency domain.

00:07:00.456 --> 00:07:03.276
A forward Fourier transform of a

00:07:03.276 --> 00:07:05.286
signal decomposes it into its

00:07:05.286 --> 00:07:06.486
component sign waves.

00:07:06.896 --> 00:07:08.106
That's the frequency domain

00:07:08.106 --> 00:07:08.926
representation.

00:07:10.006 --> 00:07:12.056
Conversely, an inverse transform

00:07:12.056 --> 00:07:13.076
of that frequency domain

00:07:13.076 --> 00:07:14.886
representation recreates the

00:07:14.886 --> 00:07:16.506
original signal and that's the

00:07:16.506 --> 00:07:17.986
time domain representation.

00:07:18.446 --> 00:07:20.576
Fourier transforms have many

00:07:20.576 --> 00:07:22.296
uses in both signal and image

00:07:22.296 --> 00:07:22.936
processing.

00:07:23.436 --> 00:07:24.936
For example, once an audio

00:07:24.936 --> 00:07:26.146
signal has been forward

00:07:26.146 --> 00:07:27.816
transformed, you can easily

00:07:27.816 --> 00:07:29.726
reduce or increase certain

00:07:29.726 --> 00:07:31.026
frequencies to equalize the

00:07:31.026 --> 00:07:31.406
audio.

00:07:33.136 --> 00:07:35.056
The classic API is reasonably

00:07:35.056 --> 00:07:36.086
easy to follow if you're

00:07:36.086 --> 00:07:36.666
familiar with it.

00:07:37.116 --> 00:07:38.686
You begin by creating a setup

00:07:38.686 --> 00:07:40.346
object specifying the number of

00:07:40.346 --> 00:07:41.666
elements you want to transform

00:07:41.746 --> 00:07:42.766
and the direction.

00:07:43.386 --> 00:07:45.166
Then, after creating two arrays

00:07:45.166 --> 00:07:46.886
to receive results, you call the

00:07:46.886 --> 00:07:47.776
execute function.

00:07:47.776 --> 00:07:49.496
Once you're done, you need to

00:07:49.496 --> 00:07:51.036
remember to destroy the setup to

00:07:51.036 --> 00:07:52.796
free the resources allocated to

00:07:53.256 --> 00:07:53.326
it.

00:07:53.956 --> 00:07:56.476
The new API simplifies the

00:07:56.476 --> 00:07:57.946
instantiation of the setup

00:07:57.946 --> 00:08:00.596
object and the transform itself

00:08:00.596 --> 00:08:02.536
is a method with parameter names

00:08:02.606 --> 00:08:04.016
on the DFT instance.

00:08:04.016 --> 00:08:06.336
And now you don't need to worry

00:08:06.336 --> 00:08:07.576
about freeing the resources.

00:08:07.576 --> 00:08:08.546
We do that for you.

00:08:10.376 --> 00:08:12.686
And much like the vDSP functions

00:08:12.686 --> 00:08:14.046
we've looked at, there's a

00:08:14.046 --> 00:08:15.626
self-allocating version of the

00:08:15.626 --> 00:08:17.796
transform function that creates

00:08:17.796 --> 00:08:19.516
and returns the result's arrays

00:08:19.516 --> 00:08:20.046
for you.

00:08:23.996 --> 00:08:25.796
If you work with audio data, you

00:08:25.796 --> 00:08:27.596
may be familiar with biquadratic

00:08:27.596 --> 00:08:28.776
or biquad filtering.

00:08:29.606 --> 00:08:31.036
Biquad filters can be used to

00:08:31.036 --> 00:08:32.655
equalize audio to shape the

00:08:32.655 --> 00:08:34.216
frequency response, allowing you

00:08:34.216 --> 00:08:36.506
to, for example, remove either

00:08:36.506 --> 00:08:37.966
low or high frequencies.

00:08:39.186 --> 00:08:41.966
vDSP's biquad feature operates

00:08:41.966 --> 00:08:43.395
on single and multichannel

00:08:43.395 --> 00:08:45.356
signals, and uses a set of

00:08:45.386 --> 00:08:46.996
individual filter objects called

00:08:46.996 --> 00:08:47.716
sections.

00:08:48.286 --> 00:08:49.986
The filters are cascaded; that

00:08:49.986 --> 00:08:51.156
is, they are set up in a

00:08:51.156 --> 00:08:53.006
sequence and the entire signal

00:08:53.006 --> 00:08:54.476
passes through each filter in

00:08:54.476 --> 00:08:54.956
turn.

00:08:55.956 --> 00:08:57.166
The filters are defined by a

00:08:57.166 --> 00:08:59.086
series of coefficients that plug

00:08:59.086 --> 00:09:00.286
into the equation shown here.

00:09:00.896 --> 00:09:05.336
In this example, these values

00:09:05.336 --> 00:09:07.286
form a low pass filter; that is,

00:09:07.286 --> 00:09:08.756
a filter that reduces high

00:09:08.756 --> 00:09:09.456
frequencies.

00:09:09.876 --> 00:09:11.906
Here's the code using vDSP's

00:09:11.906 --> 00:09:13.816
classic API to create the biquad

00:09:13.816 --> 00:09:15.516
setup using the coefficients in

00:09:15.516 --> 00:09:16.446
the previous slide.

00:09:17.196 --> 00:09:19.506
And here's the code to apply

00:09:19.506 --> 00:09:21.386
that biquad filter to an array

00:09:21.386 --> 00:09:23.056
named signal, returning the

00:09:23.056 --> 00:09:25.086
result to an array named output.

00:09:25.086 --> 00:09:27.006
Let's look at the same

00:09:27.006 --> 00:09:28.696
functionality implemented with a

00:09:28.696 --> 00:09:29.436
new API.

00:09:31.716 --> 00:09:33.906
As you can see, the new API

00:09:34.036 --> 00:09:35.866
vastly simplifies the creation

00:09:35.866 --> 00:09:36.846
of the biquad structure.

00:09:37.596 --> 00:09:39.196
You simply pass the coefficients

00:09:39.196 --> 00:09:40.896
to the biquad initializer and

00:09:40.896 --> 00:09:42.466
specify the number of channels

00:09:42.466 --> 00:09:43.326
and sections.

00:09:44.286 --> 00:09:46.346
Applying the biquad filter to a

00:09:46.346 --> 00:09:47.926
signal is a single function

00:09:47.926 --> 00:09:48.226
call.

00:09:51.496 --> 00:09:54.456
Now, let's look at the new API

00:09:54.456 --> 00:09:55.686
we've created for Accelerate's

00:09:55.686 --> 00:09:57.356
library for fast mathematical

00:09:57.356 --> 00:09:58.856
operations on large arrays,

00:09:59.146 --> 00:09:59.956
vForce.

00:10:01.896 --> 00:10:03.856
vForce provides transcendental

00:10:03.856 --> 00:10:05.976
functions not included in vDSP.

00:10:06.526 --> 00:10:08.196
These include exponential,

00:10:08.346 --> 00:10:09.576
logarithmic, and trig

00:10:09.576 --> 00:10:10.376
operations.

00:10:12.256 --> 00:10:13.996
A typical example of vForce

00:10:13.996 --> 00:10:15.496
would be to calculate the square

00:10:15.496 --> 00:10:16.956
root of each element in a large

00:10:16.956 --> 00:10:17.356
array.

00:10:18.006 --> 00:10:19.556
The scalar version of this code

00:10:19.556 --> 00:10:20.466
could use map.

00:10:22.196 --> 00:10:24.116
vForce provides a vectorized

00:10:24.116 --> 00:10:25.806
function to calculate the square

00:10:25.806 --> 00:10:27.666
roots that in some situations

00:10:27.666 --> 00:10:29.436
can be up to 10 times faster

00:10:29.586 --> 00:10:31.096
than the scalar implementation.

00:10:32.856 --> 00:10:34.996
The new Swift overlay offers an

00:10:34.996 --> 00:10:36.756
API that's consistent with the

00:10:36.756 --> 00:10:39.536
new vDSP functions and provides

00:10:39.536 --> 00:10:40.806
the performance and energy

00:10:40.806 --> 00:10:41.866
efficiency benefits of

00:10:41.866 --> 00:10:42.756
vectorization.

00:10:43.296 --> 00:10:46.186
And much like we've seen

00:10:46.186 --> 00:10:46.846
earlier, there's a

00:10:46.846 --> 00:10:48.746
self-allocating version that

00:10:48.746 --> 00:10:50.486
returns an array containing the

00:10:50.486 --> 00:10:51.806
square roots of each element in

00:10:51.806 --> 00:10:53.000
the supplied array.

00:10:57.056 --> 00:10:58.256
Next, we'll take a look at the

00:10:58.256 --> 00:11:01.036
new API we've created for

00:11:01.736 --> 00:11:02.456
Quadrature.

00:11:02.566 --> 00:11:04.516
Quadrature is a historic term

00:11:04.516 --> 00:11:06.006
for determining the area under a

00:11:06.006 --> 00:11:06.416
curve.

00:11:07.246 --> 00:11:09.126
It provides an approximation of

00:11:09.126 --> 00:11:10.266
the definite integrative

00:11:10.266 --> 00:11:11.946
function over a finite or

00:11:11.946 --> 00:11:12.956
infinite interval.

00:11:13.556 --> 00:11:15.736
In this example, we'll use

00:11:15.736 --> 00:11:17.186
Quadrature to approximate the

00:11:17.186 --> 00:11:19.046
area of a semicircle, shown here

00:11:19.046 --> 00:11:21.026
in green, by integrating the

00:11:21.026 --> 00:11:22.536
functions shown.

00:11:24.396 --> 00:11:26.836
Much like the Biquad code for

00:11:26.836 --> 00:11:28.896
vDSP, there's a fair amount of

00:11:28.896 --> 00:11:30.376
code required to use the

00:11:30.376 --> 00:11:31.926
existing Quadrature API.

00:11:32.616 --> 00:11:35.006
The first step is to define a

00:11:35.006 --> 00:11:36.346
structure that describes a

00:11:36.346 --> 00:11:37.616
function to integrate.

00:11:39.076 --> 00:11:41.716
The second step is to define the

00:11:41.716 --> 00:11:43.456
integration options including

00:11:43.456 --> 00:11:44.726
the integration algorithm.

00:11:45.496 --> 00:11:48.296
Finally, with the function on

00:11:48.296 --> 00:11:50.416
options defined, you can perform

00:11:50.416 --> 00:11:51.566
the integration using the

00:11:51.566 --> 00:11:53.016
Quadrature integrate function.

00:11:55.946 --> 00:11:58.386
The new API simplifies the code.

00:11:58.886 --> 00:12:00.476
One great advantage is that you

00:12:00.476 --> 00:12:02.256
can specify the integrand, that

00:12:02.256 --> 00:12:03.196
is, the function to be

00:12:03.196 --> 00:12:05.246
integrated, as a trading closure

00:12:05.426 --> 00:12:06.626
rather than as a C function

00:12:06.626 --> 00:12:07.066
pointer.

00:12:07.636 --> 00:12:09.186
This means you can easily pass

00:12:09.186 --> 00:12:10.646
values into the integrand.

00:12:11.606 --> 00:12:13.716
Also note that integrators are

00:12:13.716 --> 00:12:15.616
now enumerations with associated

00:12:15.616 --> 00:12:16.286
values.

00:12:16.736 --> 00:12:18.146
So, there's no need to supply

00:12:18.146 --> 00:12:19.986
unnecessary points for interval

00:12:20.016 --> 00:12:21.446
or maximum intervals here.

00:12:21.806 --> 00:12:24.896
For example, you can pass the

00:12:24.896 --> 00:12:26.236
enumeration for the globally

00:12:26.236 --> 00:12:28.376
adaptive integrator specifying

00:12:28.456 --> 00:12:29.596
the points for interval and

00:12:29.596 --> 00:12:30.686
maximum intervals.

00:12:33.456 --> 00:12:36.676
Now, let's look at the new API

00:12:36.806 --> 00:12:38.316
we've created for Accelerate's

00:12:38.316 --> 00:12:39.536
image processing library,

00:12:39.536 --> 00:12:40.216
vImage.

00:12:41.876 --> 00:12:43.606
vImage is a library containing a

00:12:43.606 --> 00:12:44.936
rich collection of image

00:12:44.936 --> 00:12:45.906
processing tools.

00:12:46.536 --> 00:12:48.456
It's designed to work seamlessly

00:12:48.456 --> 00:12:51.186
with both core graphics and core

00:12:51.186 --> 00:12:51.736
video.

00:12:51.786 --> 00:12:54.506
It includes operations such as

00:12:54.506 --> 00:12:56.966
alpha blending, format

00:12:56.966 --> 00:12:59.216
conversions, histogram

00:12:59.216 --> 00:13:01.696
operations, convolution,

00:13:02.596 --> 00:13:05.216
geometry, and morphology.

00:13:07.516 --> 00:13:10.336
Our new Swift API introduces

00:13:10.336 --> 00:13:11.806
lots of new features that makes

00:13:11.806 --> 00:13:14.116
using vImage in Swift easier and

00:13:14.116 --> 00:13:15.006
more concise.

00:13:15.556 --> 00:13:16.906
We've implemented flags as an

00:13:16.906 --> 00:13:17.576
option set.

00:13:18.236 --> 00:13:20.436
vImages throw Swift errors.

00:13:20.436 --> 00:13:21.886
And we've hidden some of the

00:13:21.886 --> 00:13:23.466
requirements for mutability and

00:13:23.466 --> 00:13:25.236
working with unmanaged types.

00:13:27.646 --> 00:13:28.696
If you're working with core

00:13:28.696 --> 00:13:30.076
graphics images, there's a

00:13:30.076 --> 00:13:31.606
common workflow to get that

00:13:31.606 --> 00:13:33.356
image data into a vImage buffer.

00:13:35.076 --> 00:13:36.706
First, you need to create a

00:13:36.706 --> 00:13:38.386
description of the CG images

00:13:38.386 --> 00:13:38.916
format.

00:13:40.286 --> 00:13:42.166
Then, instantiate a vImage

00:13:42.166 --> 00:13:42.596
buffer.

00:13:43.366 --> 00:13:44.766
Initialize that buffer from the

00:13:44.766 --> 00:13:45.246
image.

00:13:45.246 --> 00:13:46.986
And finally, check for errors in

00:13:46.986 --> 00:13:48.426
a non-Swift way.

00:13:48.696 --> 00:13:50.656
And that's a lot of boilerplate

00:13:50.656 --> 00:13:52.236
code for a common operation.

00:13:53.226 --> 00:13:55.936
The new API wraps up all of that

00:13:55.936 --> 00:13:58.056
code into a single throwable

00:13:58.056 --> 00:13:58.696
initializer.

00:13:59.906 --> 00:14:02.396
However, since we're going to

00:14:02.396 --> 00:14:04.196
use a CG images format later,

00:14:04.356 --> 00:14:05.896
here's similar functionality

00:14:05.966 --> 00:14:07.516
implemented in two steps with a

00:14:07.516 --> 00:14:07.976
new API.

00:14:08.726 --> 00:14:10.876
We've added a new initializer to

00:14:10.876 --> 00:14:12.686
CG image format using a CG

00:14:12.686 --> 00:14:15.086
image, and an alternative buffer

00:14:15.086 --> 00:14:16.976
initializer that accepts a CG

00:14:16.976 --> 00:14:18.716
image and an explicit format

00:14:18.716 --> 00:14:19.266
description.

00:14:19.666 --> 00:14:22.546
Once you're finished working

00:14:22.546 --> 00:14:23.806
with a buffer, here's the

00:14:23.806 --> 00:14:25.236
classic vImage function to

00:14:25.236 --> 00:14:26.716
create a CG image from the

00:14:26.716 --> 00:14:27.736
buffer's contents.

00:14:28.696 --> 00:14:30.856
And our new API simplifies that

00:14:30.856 --> 00:14:33.336
operation too with a new create

00:14:33.336 --> 00:14:34.796
CG image method that uses the

00:14:34.796 --> 00:14:36.296
format we've just generated from

00:14:36.296 --> 00:14:36.776
the image.

00:14:37.876 --> 00:14:40.156
One important use case for

00:14:40.156 --> 00:14:41.916
vImage is converting between

00:14:41.916 --> 00:14:43.286
different domains and different

00:14:43.286 --> 00:14:44.026
formats.

00:14:44.576 --> 00:14:46.636
vImage's any-to-any convertors

00:14:46.636 --> 00:14:48.246
can convert between core video

00:14:48.246 --> 00:14:50.206
and core graphics, and convert

00:14:50.206 --> 00:14:51.966
between different core graphics

00:14:51.966 --> 00:14:52.586
formats.

00:14:53.926 --> 00:14:55.926
For example, you might want to

00:14:55.926 --> 00:14:57.956
convert a CMYK core graphics

00:14:57.956 --> 00:14:59.876
image to RGB.

00:15:01.056 --> 00:15:03.706
The existing API to create a

00:15:03.706 --> 00:15:05.516
convertor accepts the source and

00:15:05.516 --> 00:15:06.756
destination formats for the

00:15:06.756 --> 00:15:08.446
conversion and returns an

00:15:08.446 --> 00:15:09.646
unmanaged convertor.

00:15:11.036 --> 00:15:12.486
You take the managed reference

00:15:12.486 --> 00:15:14.126
of the convertor and pass that

00:15:14.126 --> 00:15:15.116
to the function that does the

00:15:15.116 --> 00:15:15.756
conversion.

00:15:17.216 --> 00:15:19.906
Our new API adds a new static

00:15:19.906 --> 00:15:21.276
make function to the existing

00:15:21.276 --> 00:15:22.996
convertor type that returns a

00:15:22.996 --> 00:15:24.336
convertor instance.

00:15:25.006 --> 00:15:27.566
The conversion is done with the

00:15:27.566 --> 00:15:29.156
convert method on the convertor

00:15:29.156 --> 00:15:29.856
instance.

00:15:31.076 --> 00:15:33.176
Finally, let's look at working

00:15:33.176 --> 00:15:34.896
with core video image formats.

00:15:35.456 --> 00:15:37.396
In a typical example, you may

00:15:37.396 --> 00:15:39.176
want to create an image format

00:15:39.176 --> 00:15:40.926
description from a core video

00:15:40.926 --> 00:15:43.276
pixel buffer and calculate its

00:15:43.276 --> 00:15:43.996
channel count.

00:15:45.406 --> 00:15:46.786
Here's the code required by the

00:15:46.786 --> 00:15:49.366
classic vImage API to create an

00:15:49.366 --> 00:15:50.756
image format description from a

00:15:50.756 --> 00:15:52.406
pixel buffer and get its channel

00:15:52.406 --> 00:15:52.766
count.

00:15:53.216 --> 00:15:55.976
The new API provides the same

00:15:55.976 --> 00:15:57.516
functionality in two lines of

00:15:57.516 --> 00:15:57.946
code.

00:15:58.756 --> 00:16:00.446
You create an instance of a core

00:16:00.446 --> 00:16:01.946
video image format from a pixel

00:16:01.946 --> 00:16:03.826
buffer using a new static make

00:16:03.826 --> 00:16:04.316
function.

00:16:04.956 --> 00:16:07.136
And simply access its channel

00:16:07.136 --> 00:16:11.866
count as a property.

00:16:13.266 --> 00:16:14.806
That was a quick tour of a

00:16:14.806 --> 00:16:15.936
fraction of the new API.

00:16:16.426 --> 00:16:18.506
Let's now take a look at Linpack

00:16:18.506 --> 00:16:20.126
Benchmark and see just how much

00:16:20.126 --> 00:16:22.036
faster and more energy efficient

00:16:22.036 --> 00:16:23.016
Accelerate can be.

00:16:24.076 --> 00:16:25.696
The Linpack Benchmark came out

00:16:25.696 --> 00:16:27.146
of the Linpack library which

00:16:27.146 --> 00:16:28.776
started as a set of routines for

00:16:28.776 --> 00:16:31.026
providing fast computational

00:16:31.026 --> 00:16:31.856
linear algebra.

00:16:32.206 --> 00:16:34.576
This was later subsumed by a

00:16:34.576 --> 00:16:36.006
library called LApack, which

00:16:36.006 --> 00:16:37.216
stands for Linear algebra

00:16:37.216 --> 00:16:37.796
package.

00:16:38.396 --> 00:16:40.536
LApack was developed to take

00:16:40.536 --> 00:16:42.006
advantage of these new things at

00:16:42.006 --> 00:16:43.066
the time called caches.

00:16:43.066 --> 00:16:45.506
LApack is comprised of many

00:16:45.616 --> 00:16:46.436
blocked algorithms.

00:16:46.436 --> 00:16:48.426
These algorit6thms are built on

00:16:48.426 --> 00:16:49.496
top of another library called

00:16:49.496 --> 00:16:51.036
BLAS, which stands for basic

00:16:51.036 --> 00:16:52.786
linear algebra subroutines.

00:16:52.956 --> 00:16:55.416
We'll talk more about BLAS later

00:16:55.416 --> 00:16:56.366
in this presentation.

00:16:56.786 --> 00:16:58.806
For now, keep in mind that the

00:16:58.806 --> 00:17:00.536
Linpack Benchmark runs on top of

00:17:00.536 --> 00:17:02.246
LApack, which runs on top of

00:17:02.246 --> 00:17:03.000
BLAS.

00:17:05.955 --> 00:17:07.435
The Linpack Benchmark measures

00:17:07.435 --> 00:17:09.396
how quickly a platform can solve

00:17:09.396 --> 00:17:10.675
a general system of linear

00:17:10.675 --> 00:17:11.376
equations.

00:17:12.156 --> 00:17:13.806
It is comprised of two steps.

00:17:14.116 --> 00:17:15.896
The matrix factorization step,

00:17:16.036 --> 00:17:17.665
followed by the backsole step.

00:17:18.425 --> 00:17:19.935
By fixing the algorithm, we're

00:17:19.935 --> 00:17:21.336
able to see how well different

00:17:21.336 --> 00:17:22.596
platforms are at running the

00:17:22.596 --> 00:17:23.146
algorithm.

00:17:24.036 --> 00:17:25.415
This provides us with a method

00:17:25.415 --> 00:17:26.425
of comparing different

00:17:26.425 --> 00:17:27.096
platforms.

00:17:27.776 --> 00:17:29.036
The Linpack Benchmark has

00:17:29.036 --> 00:17:30.086
evolved over time.

00:17:30.666 --> 00:17:32.626
Originally, it solved a 100 by

00:17:32.626 --> 00:17:35.746
100 system, and later a 1000 by

00:17:35.746 --> 00:17:36.826
1000 system.

00:17:37.426 --> 00:17:39.556
The variant most often used

00:17:39.556 --> 00:17:41.326
today is the no holds barred

00:17:41.326 --> 00:17:43.476
variant, where the problem size

00:17:43.536 --> 00:17:44.696
can be as large as you want.

00:17:45.366 --> 00:17:47.206
This is the variant we will be

00:17:47.206 --> 00:17:47.876
running today.

00:17:48.476 --> 00:17:51.026
We are now going to compare

00:17:51.026 --> 00:17:52.726
Linpack performance on an iPhone

00:17:52.726 --> 00:17:53.316
10S.

00:17:53.806 --> 00:17:55.876
At the top in orange, we're

00:17:55.876 --> 00:17:57.676
going to run an unoptimized

00:17:57.676 --> 00:17:58.226
Linpack.

00:17:58.996 --> 00:18:00.656
This Linpack Benchmark does not

00:18:00.656 --> 00:18:01.766
make use of the accelerate

00:18:01.766 --> 00:18:02.306
framework.

00:18:02.966 --> 00:18:04.366
It relies on software that is

00:18:04.366 --> 00:18:05.846
not tuned to the process that it

00:18:05.846 --> 00:18:06.796
is running on.

00:18:07.106 --> 00:18:10.976
Let's see what that looks like.

00:18:11.046 --> 00:18:12.446
We are now going to compare that

00:18:12.446 --> 00:18:13.436
with using the Accelerate

00:18:13.436 --> 00:18:15.636
framework; that is, we're going

00:18:15.636 --> 00:18:17.486
to run the same benchmark on the

00:18:17.486 --> 00:18:19.346
same platform, but using the

00:18:19.346 --> 00:18:21.086
Accelerate framework which is

00:18:21.086 --> 00:18:22.286
tuned to the platform.

00:18:25.856 --> 00:18:27.456
We can see that by using the

00:18:27.456 --> 00:18:29.096
Accelerate framework, we are

00:18:29.096 --> 00:18:30.996
over 24 times faster.

00:18:31.836 --> 00:18:33.376
This will not only save time,

00:18:33.376 --> 00:18:35.416
but also energy, which improves

00:18:35.416 --> 00:18:36.166
battery life.

00:18:36.936 --> 00:18:38.436
We're now going to shift gears

00:18:38.436 --> 00:18:39.606
and take a look at the primary

00:18:39.606 --> 00:18:40.876
workhorse routine for the

00:18:40.876 --> 00:18:42.716
Linpack Benchmark called GEMM.

00:18:44.396 --> 00:18:46.916
As I mentioned earlier, Linpack,

00:18:47.166 --> 00:18:49.316
which runs on LApack, is built

00:18:49.316 --> 00:18:50.276
on top of BLAS.

00:18:51.036 --> 00:18:52.746
Within BLAS is a routine called

00:18:52.746 --> 00:18:54.346
GEMM, which stands for general

00:18:54.346 --> 00:18:55.446
matrix multiplier.

00:18:56.086 --> 00:18:58.156
This routine is used to

00:18:58.156 --> 00:18:59.746
implement several other blocked

00:18:59.746 --> 00:19:01.866
routines in BLAS, which are used

00:19:01.866 --> 00:19:03.536
inside the blocked algorithms at

00:19:03.596 --> 00:19:06.106
LApack, most notably the matrix

00:19:06.106 --> 00:19:07.556
factorization and solver

00:19:07.556 --> 00:19:08.256
routines.

00:19:09.516 --> 00:19:11.006
Because of this, GEMM is

00:19:11.006 --> 00:19:12.746
sometimes used as a proxy for

00:19:12.746 --> 00:19:13.456
performance.

00:19:13.456 --> 00:19:15.246
For this presentation, we are

00:19:15.246 --> 00:19:16.436
specifically going to look at

00:19:16.436 --> 00:19:18.146
the single-precision variant of

00:19:18.146 --> 00:19:18.526
GEMM.

00:19:20.466 --> 00:19:21.846
Here, we're going to compare the

00:19:21.846 --> 00:19:23.516
performance of the Eigen library

00:19:23.516 --> 00:19:24.606
with that of Accelerate.

00:19:25.436 --> 00:19:26.916
Both the Eigen library and the

00:19:26.916 --> 00:19:28.856
Accelerate framework will run on

00:19:28.856 --> 00:19:30.456
top of an iPhone 10S.

00:19:30.916 --> 00:19:32.006
Both will be performing a

00:19:32.006 --> 00:19:33.286
single-precision matrix

00:19:33.286 --> 00:19:33.836
multiplier.

00:19:34.236 --> 00:19:38.536
Let's see how well Eigen does.

00:19:38.536 --> 00:19:40.346
Eigen tops out at about 51

00:19:40.346 --> 00:19:41.096
gigaflops.

00:19:41.296 --> 00:19:43.086
Now, let's see how well

00:19:43.086 --> 00:19:43.916
Accelerate does.

00:19:44.476 --> 00:19:47.616
We can see that the Accelerate

00:19:47.616 --> 00:19:48.916
framework is almost two and a

00:19:48.916 --> 00:19:50.716
half times faster than Eigen on

00:19:50.716 --> 00:19:51.606
the same platform.

00:19:52.636 --> 00:19:53.996
This is because the Accelerate

00:19:53.996 --> 00:19:55.696
framework is hand-tuned to the

00:19:55.696 --> 00:19:57.706
platform, allowing us to fully

00:19:57.706 --> 00:19:59.146
take advantage of what the

00:19:59.146 --> 00:20:01.046
platform can offer.

00:20:01.046 --> 00:20:03.706
So, if you're a developer, using

00:20:03.706 --> 00:20:04.876
Accelerate in your app will

00:20:04.876 --> 00:20:05.996
offer better performance.

00:20:06.486 --> 00:20:08.106
This performance translates into

00:20:08.106 --> 00:20:09.676
less energy, which means better

00:20:09.676 --> 00:20:11.696
battery life and an overall

00:20:11.886 --> 00:20:13.056
better experience for your

00:20:13.056 --> 00:20:13.566
users.

00:20:15.586 --> 00:20:17.896
In summary, Accelerate provides

00:20:17.896 --> 00:20:19.036
functions for performing

00:20:19.036 --> 00:20:20.756
large-scale mathematical

00:20:20.756 --> 00:20:22.166
computations and image

00:20:22.166 --> 00:20:24.136
calculations that are fast and

00:20:24.136 --> 00:20:24.966
energy efficient.

00:20:25.296 --> 00:20:27.006
And now we've added a

00:20:27.006 --> 00:20:29.136
Swift-friendly API that makes

00:20:29.136 --> 00:20:30.616
Accelerate's libraries super

00:20:30.616 --> 00:20:32.396
easy to work with so your users

00:20:32.396 --> 00:20:33.256
will benefit from that

00:20:33.256 --> 00:20:34.536
performance and energy

00:20:34.536 --> 00:20:35.126
efficiency.

00:20:36.516 --> 00:20:37.866
Please visit our site where we

00:20:37.866 --> 00:20:39.536
have samples, articles, and

00:20:39.536 --> 00:20:41.036
extensive reference material

00:20:41.036 --> 00:20:42.496
that covers the entire

00:20:42.536 --> 00:20:43.906
Accelerate framework.

00:20:44.266 --> 00:20:45.176
Thank you very much.