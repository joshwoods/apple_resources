WEBVTT

00:00:00.506 --> 00:00:05.516
[ Music ]

00:00:06.516 --> 00:00:13.566
[ Applause ]

00:00:14.066 --> 00:00:15.986
>> Hey, everyone.

00:00:16.436 --> 00:00:17.476
My name is Michael Brennan.

00:00:17.576 --> 00:00:19.116
I'm a software engineer here at

00:00:19.116 --> 00:00:20.386
Apple working on Core ML.

00:00:21.336 --> 00:00:23.026
And I'm thrilled to be able to

00:00:23.026 --> 00:00:23.996
share with you some of the

00:00:23.996 --> 00:00:25.636
amazing new features we've

00:00:25.636 --> 00:00:27.726
introduced this year for Core ML

00:00:27.776 --> 00:00:27.976
3.

00:00:30.336 --> 00:00:31.856
Core ML makes it as easy as

00:00:31.856 --> 00:00:33.946
possible to seamlessly integrate

00:00:33.946 --> 00:00:35.076
machine learning into your

00:00:35.076 --> 00:00:37.156
application allowing you to

00:00:37.156 --> 00:00:38.686
create a wide variety of novel

00:00:39.136 --> 00:00:40.336
and compelling experiences.

00:00:41.376 --> 00:00:43.156
At the center of all of this is

00:00:43.246 --> 00:00:46.856
the model itself.

00:00:47.046 --> 00:00:48.716
You can get a model from a

00:00:48.716 --> 00:00:49.776
number of supported training

00:00:49.776 --> 00:00:51.736
libraries by converting them

00:00:51.736 --> 00:00:53.026
using the converters we have as

00:00:53.546 --> 00:00:55.066
a part of our Core ML tools

00:00:55.226 --> 00:00:57.146
which we provide.

00:00:57.276 --> 00:00:59.966
And with the addition of the new

00:00:59.966 --> 00:01:01.716
Create ML app, we've made it

00:01:01.716 --> 00:01:03.906
even easier than before to get a

00:01:03.906 --> 00:01:04.256
model.

00:01:06.596 --> 00:01:07.816
So in this session we're going

00:01:07.816 --> 00:01:09.926
to cover a number of topics from

00:01:10.006 --> 00:01:11.276
personalizing those models on

00:01:11.276 --> 00:01:13.106
device to additional support

00:01:13.106 --> 00:01:14.296
we've added for neural networks

00:01:14.936 --> 00:01:16.196
and even more.

00:01:17.476 --> 00:01:18.436
So let's get started by talking

00:01:18.436 --> 00:01:20.116
a little bit about personalizing

00:01:20.116 --> 00:01:21.496
your models on-device.

00:01:24.396 --> 00:01:27.046
Traditionally, these models are

00:01:27.046 --> 00:01:28.196
either bundled with or

00:01:28.196 --> 00:01:29.506
downloaded to your application.

00:01:30.066 --> 00:01:31.206
And once on your device are

00:01:31.206 --> 00:01:33.136
completely mutable being

00:01:33.136 --> 00:01:34.426
optimized heavily for

00:01:34.426 --> 00:01:36.306
performance within your app.

00:01:38.596 --> 00:01:39.506
The great thing about

00:01:39.506 --> 00:01:40.596
associating a model with your

00:01:40.596 --> 00:01:42.036
app is it means you can provide

00:01:42.036 --> 00:01:43.756
the same great experience across

00:01:43.756 --> 00:01:44.776
all of your users.

00:01:45.326 --> 00:01:47.626
But each user is unique.

00:01:48.206 --> 00:01:49.856
They each have their own

00:01:49.856 --> 00:01:51.696
specialized needs, their own

00:01:51.736 --> 00:01:53.526
individual ways of interacting

00:01:53.526 --> 00:01:54.256
with your application.

00:01:55.416 --> 00:01:56.366
I think this creates an

00:01:56.366 --> 00:01:56.946
opportunity.

00:01:57.506 --> 00:02:00.626
What if each user could get a

00:02:00.626 --> 00:02:02.746
model personalized specifically

00:02:02.836 --> 00:02:03.356
for them?

00:02:03.906 --> 00:02:06.746
Well, let's look at the case of

00:02:06.746 --> 00:02:07.966
one user on how to do this.

00:02:08.955 --> 00:02:10.306
We can take data from that user,

00:02:11.156 --> 00:02:12.226
set up some sort of cloud

00:02:12.226 --> 00:02:14.636
service, send that data to that

00:02:14.636 --> 00:02:17.026
cloud service, and in the cloud

00:02:17.126 --> 00:02:18.216
create that new model

00:02:18.446 --> 00:02:20.056
personalized on that data which

00:02:20.056 --> 00:02:21.096
will then deploy back to that

00:02:21.096 --> 00:02:21.386
user.

00:02:23.276 --> 00:02:23.956
This creates a number of

00:02:23.956 --> 00:02:25.076
challenges, both to you, the

00:02:25.076 --> 00:02:26.956
developers, and to our users.

00:02:28.486 --> 00:02:30.086
For you, it creates the

00:02:30.086 --> 00:02:32.756
challenge of added expenses with

00:02:32.756 --> 00:02:33.976
making these cloud services

00:02:34.026 --> 00:02:35.886
managing that service and

00:02:35.886 --> 00:02:36.836
creating an infrastructure that

00:02:36.836 --> 00:02:38.316
could scale to million of users.

00:02:38.976 --> 00:02:41.036
And for the user, obviously they

00:02:41.036 --> 00:02:43.266
have to expose all their data.

00:02:43.266 --> 00:02:43.976
It's unwanted.

00:02:44.146 --> 00:02:44.856
It's invasive.

00:02:44.856 --> 00:02:46.346
They have to expose this and

00:02:46.346 --> 00:02:47.116
send it up to the cloud where

00:02:47.116 --> 00:02:48.766
it'll be managed by some other

00:02:48.766 --> 00:02:49.116
party.

00:02:49.716 --> 00:02:53.786
On Core ML 3, you can do this

00:02:53.786 --> 00:02:55.946
kind of personalization all on

00:02:55.946 --> 00:02:56.376
device.

00:02:57.516 --> 00:03:03.176
[ Applause ]

00:03:03.676 --> 00:03:05.006
Simply take that general model

00:03:05.566 --> 00:03:06.806
and with the data the users

00:03:06.806 --> 00:03:09.186
created or regenerated, you can

00:03:09.186 --> 00:03:10.676
personalize that model with that

00:03:11.276 --> 00:03:14.756
data for that user.

00:03:14.906 --> 00:03:16.376
This means that the data stays

00:03:16.476 --> 00:03:16.816
private.

00:03:17.426 --> 00:03:18.686
It never leaves the device.

00:03:19.676 --> 00:03:20.856
This means there's no need for a

00:03:20.856 --> 00:03:21.946
server to do this kind of

00:03:21.946 --> 00:03:22.616
interactivity.

00:03:22.616 --> 00:03:24.566
And this means you can do it

00:03:24.566 --> 00:03:24.896
anywhere.

00:03:25.356 --> 00:03:26.956
You're not tying your users down

00:03:26.956 --> 00:03:29.426
to Wi-Fi or some data plan just

00:03:29.426 --> 00:03:30.106
to update their model.

00:03:33.306 --> 00:03:34.376
Before we continue, let's look

00:03:34.376 --> 00:03:36.056
inside one of these models and

00:03:36.496 --> 00:03:38.706
see what's there today.

00:03:38.876 --> 00:03:40.176
Currently, your model consists

00:03:40.176 --> 00:03:42.046
of mostly parameters, things

00:03:42.046 --> 00:03:43.506
like the weights of the layers

00:03:43.506 --> 00:03:44.346
if it's a neural network for

00:03:44.346 --> 00:03:44.776
example.

00:03:45.396 --> 00:03:46.556
And some metadata describing

00:03:46.556 --> 00:03:48.196
things like licensing and

00:03:48.196 --> 00:03:50.056
authors, as well as an

00:03:50.056 --> 00:03:50.716
interface.

00:03:50.716 --> 00:03:51.736
And this where your app concerns

00:03:51.736 --> 00:03:52.226
itself with.

00:03:52.776 --> 00:03:53.756
It describes how you can

00:03:53.756 --> 00:03:55.736
interact with this model.

00:03:56.516 --> 00:03:58.396
On Core ML 3, we've added a new

00:03:58.436 --> 00:04:00.026
set of parameters to help

00:04:00.026 --> 00:04:00.496
updating.

00:04:00.496 --> 00:04:01.496
It describes what parts of the

00:04:01.496 --> 00:04:02.986
model are updatable and how it's

00:04:02.986 --> 00:04:03.446
updatable.

00:04:03.976 --> 00:04:04.966
We've added a new update

00:04:04.966 --> 00:04:06.146
interface your app can leverage

00:04:06.146 --> 00:04:07.096
to make these updates happen.

00:04:10.216 --> 00:04:11.396
This means that all the

00:04:11.396 --> 00:04:13.206
functionality is accessed

00:04:13.206 --> 00:04:14.396
directly through that interface.

00:04:14.686 --> 00:04:16.136
We encapsulate away a lot of

00:04:16.136 --> 00:04:18.366
those heavy details and it's all

00:04:18.366 --> 00:04:19.755
contained within that one model

00:04:19.755 --> 00:04:20.096
file.

00:04:20.726 --> 00:04:23.166
So just like in making a

00:04:23.166 --> 00:04:24.616
prediction, we supply a set of

00:04:24.616 --> 00:04:25.446
inputs and you get a

00:04:25.446 --> 00:04:26.756
corresponding set of outputs.

00:04:27.756 --> 00:04:29.146
Making an update is easy, you

00:04:29.146 --> 00:04:30.216
just supply a set of training

00:04:30.216 --> 00:04:31.796
examples and you'll get out a

00:04:31.796 --> 00:04:33.006
new variant of that model that

00:04:33.106 --> 00:04:34.636
was fit or fine-tuned to that

00:04:34.636 --> 00:04:34.856
data.

00:04:38.096 --> 00:04:40.356
For Core ML 3, we support making

00:04:40.356 --> 00:04:41.736
updatable models of nearest

00:04:41.736 --> 00:04:43.586
neighbor classifiers as well as

00:04:43.586 --> 00:04:44.416
neural networks.

00:04:44.646 --> 00:04:45.496
And we're going to support

00:04:45.496 --> 00:04:46.706
embedding an updatable model

00:04:46.706 --> 00:04:48.296
within your pipeline, meaning

00:04:48.296 --> 00:04:49.626
your pipelines are updatable

00:04:49.626 --> 00:04:49.896
too.

00:04:50.486 --> 00:04:54.576
And with that, I think we should

00:04:54.576 --> 00:04:55.386
see this in action.

00:04:55.746 --> 00:04:56.976
So I'd like to hand it over to

00:04:56.976 --> 00:04:58.286
my colleague Anil Katti.

00:04:59.016 --> 00:04:59.336
Anil.

00:05:00.516 --> 00:05:06.176
[ Applause ]

00:05:06.676 --> 00:05:07.516
>> Thanks, Michael.

00:05:07.896 --> 00:05:11.756
Hello. Today I show how I build

00:05:11.756 --> 00:05:12.846
an experience that was

00:05:12.916 --> 00:05:15.536
customized for my users using

00:05:15.906 --> 00:05:17.126
model personalization.

00:05:17.716 --> 00:05:19.486
For this I have an app that

00:05:19.486 --> 00:05:21.216
helps teachers grade students'

00:05:21.216 --> 00:05:21.616
homework.

00:05:22.536 --> 00:05:23.846
I added a cool new feature to

00:05:23.846 --> 00:05:25.176
this app using model

00:05:25.176 --> 00:05:25.826
personalization.

00:05:26.046 --> 00:05:27.186
But before we talk about the

00:05:27.186 --> 00:05:28.426
feature, let's see what the app

00:05:28.426 --> 00:05:28.946
does first.

00:05:28.946 --> 00:05:30.456
Let me switch over to the demo

00:05:30.456 --> 00:05:30.976
screen.

00:05:37.376 --> 00:05:40.136
Great. So here's the app.

00:05:40.136 --> 00:05:41.306
It's called a Personalized

00:05:41.306 --> 00:05:41.886
Grading App.

00:05:42.396 --> 00:05:44.356
What it allows you to do is take

00:05:44.356 --> 00:05:47.626
a picture of the homework and

00:05:47.626 --> 00:05:49.896
start grading the way our

00:05:49.896 --> 00:05:51.006
teacher would on a sheet of

00:05:51.006 --> 00:05:51.396
paper.

00:05:52.266 --> 00:05:53.846
You'd mark something as correct

00:05:53.846 --> 00:05:55.556
like that and something as wrong

00:05:55.856 --> 00:05:56.436
like that.

00:05:56.736 --> 00:05:58.166
It's pretty straightforward.

00:05:58.356 --> 00:05:59.626
Pretty simple.

00:06:00.186 --> 00:06:02.416
The app uses PencilKit to

00:06:02.416 --> 00:06:04.126
capture the input and the

00:06:04.126 --> 00:06:05.736
pretrained Core ML model to

00:06:05.736 --> 00:06:07.746
recognize the grading samples.

00:06:08.856 --> 00:06:10.726
Now recently, I added a new

00:06:10.726 --> 00:06:12.686
feature that allows teachers

00:06:13.086 --> 00:06:14.556
give something special for the

00:06:14.556 --> 00:06:16.026
kids for encouragement.

00:06:17.216 --> 00:06:18.066
And that is stickers.

00:06:19.136 --> 00:06:21.026
Kids absolutely love collecting

00:06:21.026 --> 00:06:22.026
stickers on their homework.

00:06:22.436 --> 00:06:23.306
So we thought it might be a

00:06:23.306 --> 00:06:24.816
really nice enhancement to the

00:06:25.006 --> 00:06:26.736
app if they could allow-- if we

00:06:26.736 --> 00:06:27.566
could allow them to give

00:06:27.566 --> 00:06:27.936
stickers.

00:06:29.356 --> 00:06:31.056
So to give a sticker, I could

00:06:31.056 --> 00:06:32.596
tap on this plus button up here.

00:06:32.596 --> 00:06:35.396
Scroll through the list of

00:06:35.396 --> 00:06:38.236
stickers, maybe pick one, and

00:06:38.236 --> 00:06:40.466
drag it to the right location

00:06:40.466 --> 00:06:40.726
[inaudible].

00:06:40.726 --> 00:06:41.146
This works.

00:06:41.146 --> 00:06:46.396
But I think we can make the flow

00:06:46.396 --> 00:06:48.356
even better and absolutely

00:06:48.356 --> 00:06:49.576
magical for our users.

00:06:49.846 --> 00:06:52.266
Let's think about it a little

00:06:52.766 --> 00:06:52.866
bit.

00:06:53.776 --> 00:06:55.786
The user is only using an Apple

00:06:55.786 --> 00:06:57.156
Pencil for grading.

00:06:57.156 --> 00:06:59.296
How cool would it be if they

00:06:59.296 --> 00:07:00.646
could quickly sketch something

00:07:01.046 --> 00:07:03.246
anywhere on the entire screen

00:07:03.476 --> 00:07:04.936
and the app automatically pick

00:07:04.936 --> 00:07:06.316
the right sticker of the correct

00:07:06.316 --> 00:07:07.766
size and place it at the right

00:07:07.766 --> 00:07:09.876
location for them.

00:07:10.096 --> 00:07:11.656
Well, we can definitely do

00:07:11.656 --> 00:07:12.806
something like that with machine

00:07:12.806 --> 00:07:13.306
learning, right?

00:07:13.306 --> 00:07:15.036
So let's explore some options.

00:07:15.296 --> 00:07:16.686
Let me switch over to the

00:07:16.686 --> 00:07:16.956
slides.

00:07:17.666 --> 00:07:18.496
Well, some of you who have

00:07:18.496 --> 00:07:20.646
already worked on Core ML might

00:07:20.646 --> 00:07:21.826
be thinking, well, we could

00:07:21.826 --> 00:07:25.426
actually pre-train a model that

00:07:25.426 --> 00:07:27.286
can recognize stickers and ship

00:07:27.286 --> 00:07:29.316
it as part of the app.

00:07:29.406 --> 00:07:30.496
Well, we could do that but there

00:07:30.496 --> 00:07:31.476
are a few issues with this

00:07:31.476 --> 00:07:31.966
approach.

00:07:32.586 --> 00:07:34.836
For one, there are a lot of

00:07:34.836 --> 00:07:36.556
stickers out there and we might

00:07:36.556 --> 00:07:37.746
have to collect a lot of

00:07:37.746 --> 00:07:39.176
training data in order to

00:07:39.176 --> 00:07:41.966
pre-train these models, although

00:07:41.966 --> 00:07:43.326
a particular user might only be

00:07:43.326 --> 00:07:46.026
interested in a small subset of

00:07:46.756 --> 00:07:47.336
these stickers.

00:07:47.336 --> 00:07:50.436
Second, how does this pretrained

00:07:50.436 --> 00:07:52.196
model work if I plan to

00:07:52.196 --> 00:07:53.426
introduce a bunch of new

00:07:53.426 --> 00:07:55.826
stickers in the future?

00:07:56.006 --> 00:07:57.386
Well, I might have to re-train

00:07:57.386 --> 00:07:59.666
the model and ship it as an app

00:07:59.666 --> 00:08:01.716
update or maybe allow the app to

00:08:01.716 --> 00:08:02.986
download it.

00:08:04.136 --> 00:08:05.766
Lastly, and I think this is the

00:08:05.766 --> 00:08:07.706
most crucial point, how does

00:08:07.706 --> 00:08:08.956
this pretrained model work for

00:08:08.956 --> 00:08:09.786
different users?

00:08:12.316 --> 00:08:14.496
Different users have different

00:08:14.496 --> 00:08:15.326
ways of sketching.

00:08:16.376 --> 00:08:18.076
If I went out and I asked a

00:08:18.076 --> 00:08:19.516
hundred different people to

00:08:19.516 --> 00:08:20.896
quickly sketch this mind-blowing

00:08:20.896 --> 00:08:22.606
sticker, I'll get at least 50

00:08:22.606 --> 00:08:23.276
different answers.

00:08:24.006 --> 00:08:25.476
Now, it's truly mind-blowing how

00:08:25.476 --> 00:08:26.376
different people are.

00:08:26.906 --> 00:08:28.546
So what should the app do in

00:08:28.546 --> 00:08:29.626
this case?

00:08:30.616 --> 00:08:32.856
Should we-- Should the app train

00:08:32.856 --> 00:08:35.655
the users on how to sketch

00:08:35.655 --> 00:08:39.626
something, or should the users

00:08:39.626 --> 00:08:41.666
train the app, enhance the model

00:08:41.876 --> 00:08:43.816
how to recognize your sketches?

00:08:44.826 --> 00:08:46.516
Well, that's exactly what model

00:08:46.516 --> 00:08:49.556
personalization is all about.

00:08:49.756 --> 00:08:51.626
What I did was use model

00:08:51.626 --> 00:08:53.926
personalization to train-- to

00:08:53.926 --> 00:08:55.856
define a model using the data

00:08:55.856 --> 00:08:56.866
that I collected about the

00:08:56.866 --> 00:08:59.206
user-- from the user to suit

00:08:59.206 --> 00:08:59.816
their needs.

00:08:59.966 --> 00:09:01.246
Let me show how that translates

00:09:01.246 --> 00:09:02.536
to user experience.

00:09:08.516 --> 00:09:09.636
I'll switch back to the demo

00:09:09.636 --> 00:09:11.286
screen to continue my grading.

00:09:12.046 --> 00:09:13.336
So the third answer-- third

00:09:13.336 --> 00:09:14.666
question there is pretty tricky

00:09:14.666 --> 00:09:15.846
for kindergarten care, but I

00:09:15.846 --> 00:09:17.246
think Jane got it spot on.

00:09:17.396 --> 00:09:18.796
There are four triangles in that

00:09:18.796 --> 00:09:19.646
diagram.

00:09:20.016 --> 00:09:20.996
So, good job, Jane.

00:09:21.896 --> 00:09:23.856
Well, now I want to give her a

00:09:23.856 --> 00:09:25.476
sticker to recognize the

00:09:25.476 --> 00:09:26.716
attention to detail, but I want

00:09:26.716 --> 00:09:27.946
to do it by quickly sketching

00:09:27.946 --> 00:09:28.286
something.

00:09:28.886 --> 00:09:29.956
Let's try doing that.

00:09:30.466 --> 00:09:35.526
So the app seems to tell me that

00:09:35.526 --> 00:09:36.826
it could not recognize what I

00:09:36.826 --> 00:09:38.406
sketch, which is pretty fair

00:09:38.406 --> 00:09:39.396
because this is the first time

00:09:39.396 --> 00:09:40.176
I'm using this app.

00:09:40.616 --> 00:09:42.406
So let me try to add a sticker,

00:09:42.666 --> 00:09:43.946
but this time I'll say I want to

00:09:43.946 --> 00:09:45.966
create a shortcut, right.

00:09:45.966 --> 00:09:47.146
And I'll pick the same sticker

00:09:47.146 --> 00:09:48.836
that I used last time.

00:09:49.876 --> 00:09:52.446
So now the app is asking me to

00:09:52.446 --> 00:09:54.646
give an example of how I would

00:09:54.646 --> 00:09:55.896
sketch this particular sticker.

00:09:56.616 --> 00:09:57.876
It doesn't have to be perfect.

00:09:57.876 --> 00:09:59.466
It doesn't even have to resemble

00:09:59.466 --> 00:09:59.926
the sticker.

00:10:00.296 --> 00:10:02.566
It's just my own representation

00:10:02.916 --> 00:10:05.336
of how I would want to sketch

00:10:05.336 --> 00:10:05.976
that sticker, right?

00:10:06.286 --> 00:10:09.516
So I'll just use a single star

00:10:10.136 --> 00:10:13.576
to represent that particular

00:10:13.576 --> 00:10:13.996
sticker.

00:10:13.996 --> 00:10:16.926
The app asked me for a couple of

00:10:16.926 --> 00:10:18.226
more examples, which is pretty

00:10:18.226 --> 00:10:19.726
fair, and it just took a few

00:10:19.726 --> 00:10:21.726
seconds but now it looks like

00:10:21.726 --> 00:10:23.126
the app-- the shortcut has been

00:10:23.126 --> 00:10:23.916
registered.

00:10:24.586 --> 00:10:26.516
Now I see the sticker as well as

00:10:26.516 --> 00:10:28.186
examples I provided in the

00:10:28.186 --> 00:10:28.736
screen.

00:10:29.866 --> 00:10:31.356
Before I go back, let me add one

00:10:31.356 --> 00:10:33.416
more sticker to my library.

00:10:34.536 --> 00:10:35.806
So, there was this really nice

00:10:35.966 --> 00:10:37.616
high five sticker that I wanted

00:10:37.616 --> 00:10:39.026
to use, which is right here.

00:10:39.676 --> 00:10:42.236
I wonder what I should use as

00:10:42.236 --> 00:10:42.976
shortcut for this.

00:10:43.706 --> 00:10:45.186
I wanted-- I want something that

00:10:45.186 --> 00:10:46.596
is easy to remember, easy to

00:10:46.596 --> 00:10:47.076
sketch.

00:10:47.426 --> 00:10:49.236
How about the number 5, which

00:10:49.236 --> 00:10:52.406
kind of says high five?

00:10:53.276 --> 00:10:55.926
Great. So just two more examples

00:10:55.926 --> 00:10:56.576
and I'm done.

00:10:58.006 --> 00:11:00.006
Let's go back to the screen and

00:11:00.006 --> 00:11:01.166
try giving that sticker that

00:11:01.166 --> 00:11:02.456
Jane has been desperately

00:11:02.456 --> 00:11:03.156
waiting for.

00:11:04.626 --> 00:11:05.826
Isn't that cool?

00:11:06.516 --> 00:11:10.966
[ Applause ]

00:11:11.466 --> 00:11:12.986
I'm so glad I didn't have the go

00:11:12.986 --> 00:11:14.656
through the sticker picker and

00:11:14.656 --> 00:11:15.756
move the sticker around and all

00:11:15.756 --> 00:11:16.746
those mess.

00:11:17.736 --> 00:11:19.256
Great. For the next one-- yeah,

00:11:19.256 --> 00:11:20.166
that's right as well.

00:11:20.166 --> 00:11:21.526
So let me give another small

00:11:21.526 --> 00:11:22.416
star there.

00:11:23.176 --> 00:11:24.456
I'm super happy with the way the

00:11:24.456 --> 00:11:26.256
app behaves, so a big high five

00:11:26.256 --> 00:11:27.026
at the top.

00:11:27.446 --> 00:11:28.976
How about that?

00:11:29.516 --> 00:11:35.786
[ Applause ]

00:11:36.286 --> 00:11:38.166
So this was just an example.

00:11:38.166 --> 00:11:39.316
You guys can think about really

00:11:39.316 --> 00:11:41.566
amazing experiences that you can

00:11:41.566 --> 00:11:42.856
kind of solve using this new

00:11:42.856 --> 00:11:43.226
feature.

00:11:43.876 --> 00:11:45.516
So before I wrap up, let me

00:11:45.516 --> 00:11:46.696
quickly go with the things that

00:11:46.696 --> 00:11:47.856
I had to do in order to

00:11:47.856 --> 00:11:48.736
implement this feature.

00:11:49.856 --> 00:11:51.376
So the first thing I had to do

00:11:51.866 --> 00:11:54.416
was import an updatable Core ML

00:11:54.416 --> 00:11:56.366
model into my project.

00:11:57.096 --> 00:11:58.186
And this is how it looks in

00:11:58.186 --> 00:11:58.856
Xcode.

00:12:00.656 --> 00:12:01.626
Since I was trying to

00:12:01.626 --> 00:12:03.396
personalize the model by adding

00:12:03.396 --> 00:12:05.776
new stickers or classes, using

00:12:05.776 --> 00:12:07.046
the nearest neighbor classifier

00:12:07.046 --> 00:12:10.356
made a lot of sense, although

00:12:10.356 --> 00:12:12.176
that is a pretrained neural

00:12:12.176 --> 00:12:14.046
network feature extractor, that

00:12:14.046 --> 00:12:15.216
is filling in into the nearest

00:12:15.216 --> 00:12:16.676
neighbor classifier to help

00:12:16.676 --> 00:12:18.096
improve the prediction

00:12:18.096 --> 00:12:19.116
efficiency and robustness.

00:12:20.016 --> 00:12:21.696
But that's it, you don't have to

00:12:21.696 --> 00:12:23.306
worry about any of model details

00:12:23.666 --> 00:12:25.466
since Core ML abstracts the way

00:12:25.866 --> 00:12:27.026
all the model details from the

00:12:27.026 --> 00:12:27.946
API surface.

00:12:31.576 --> 00:12:32.906
You can use this model for

00:12:32.906 --> 00:12:33.276
prediction.

00:12:33.276 --> 00:12:34.586
There is no change there.

00:12:35.486 --> 00:12:36.606
In Xcode you will see the

00:12:36.606 --> 00:12:37.976
prediction inputs and outputs

00:12:37.976 --> 00:12:38.636
like before.

00:12:40.096 --> 00:12:42.276
But what's new this year is an

00:12:42.276 --> 00:12:43.066
update section.

00:12:44.366 --> 00:12:45.666
This describes the set of

00:12:45.666 --> 00:12:47.446
inputs, yeah, this model

00:12:47.446 --> 00:12:48.656
requires for personalization.

00:12:49.226 --> 00:12:50.646
And as expected, it requires a

00:12:50.646 --> 00:12:51.896
sticker which is a grayscale

00:12:51.896 --> 00:12:54.066
image, and a corresponding

00:12:54.066 --> 00:12:55.086
sketch which is a grayscale

00:12:55.086 --> 00:12:56.496
image, and a corresponding

00:12:56.496 --> 00:12:57.856
sticker which serves as a true

00:12:57.856 --> 00:12:59.346
label in order to personalize.

00:13:00.326 --> 00:13:02.206
In terms of code, you may recall

00:13:02.206 --> 00:13:04.326
that Core ML auto-generates a

00:13:04.326 --> 00:13:06.196
set of classes to help you with

00:13:06.196 --> 00:13:06.786
the prediction.

00:13:07.296 --> 00:13:09.086
And now it generates new class

00:13:09.596 --> 00:13:10.716
that helps you provide the

00:13:10.716 --> 00:13:11.896
training data in a [inaudible]

00:13:11.896 --> 00:13:12.246
manner.

00:13:13.336 --> 00:13:14.606
If you peek into this class,

00:13:14.986 --> 00:13:16.466
you'll see a set of properties

00:13:16.676 --> 00:13:17.906
which are in line with what you

00:13:17.906 --> 00:13:19.156
saw in Xcode, right?

00:13:19.156 --> 00:13:21.646
So we see the sketch and the

00:13:21.646 --> 00:13:22.596
sticker here.

00:13:24.556 --> 00:13:26.986
So once you have collected all

00:13:26.986 --> 00:13:29.056
the training data from the user

00:13:29.556 --> 00:13:31.596
that is required to personalize

00:13:31.596 --> 00:13:34.016
a model for-- or for the user,

00:13:34.746 --> 00:13:36.076
the actual personalization

00:13:36.076 --> 00:13:37.746
itself can happen in three easy

00:13:37.746 --> 00:13:38.106
steps.

00:13:38.106 --> 00:13:40.206
First, you need to get the

00:13:40.206 --> 00:13:43.356
updatable model URL that you

00:13:43.356 --> 00:13:44.886
could get either from the bundle

00:13:45.136 --> 00:13:46.526
or from a previous snapshot of

00:13:46.526 --> 00:13:47.576
the updated model.

00:13:47.876 --> 00:13:50.616
Next, you would prepare the

00:13:50.616 --> 00:13:52.056
training data in the expected

00:13:52.056 --> 00:13:52.556
format.

00:13:53.176 --> 00:13:55.236
For this, you could either use

00:13:55.666 --> 00:13:57.786
the origin rated class that I

00:13:57.786 --> 00:13:59.336
showed or build a feature

00:13:59.336 --> 00:14:00.416
provider of your own.

00:14:01.616 --> 00:14:03.766
And lastly, you would kick off

00:14:03.766 --> 00:14:04.596
an update task.

00:14:05.176 --> 00:14:07.736
Once the update is complete,

00:14:08.146 --> 00:14:09.686
completion handler gets invoked

00:14:10.176 --> 00:14:11.636
which provides you the updated

00:14:11.636 --> 00:14:13.666
model that you can use for

00:14:13.666 --> 00:14:14.766
predictions immediately.

00:14:15.236 --> 00:14:16.026
It's that simple.

00:14:16.026 --> 00:14:18.246
I can't wait to see all the cool

00:14:18.246 --> 00:14:19.446
things that you will build on

00:14:19.446 --> 00:14:21.006
top of this in your apps and

00:14:21.006 --> 00:14:21.576
frameworks.

00:14:21.916 --> 00:14:23.016
With that, let me hand it back

00:14:23.046 --> 00:14:24.756
to Michael to recap and cover

00:14:24.756 --> 00:14:26.036
some more complex scenarios and

00:14:26.036 --> 00:14:26.976
model personalization.

00:14:27.236 --> 00:14:27.586
Thank you.

00:14:28.516 --> 00:14:32.500
[ Applause ]

00:14:36.526 --> 00:14:37.506
>> All right.

00:14:37.586 --> 00:14:38.156
Thanks, Anil.

00:14:41.436 --> 00:14:43.476
So just a recap, Anil just

00:14:43.476 --> 00:14:44.396
showed us, we took our base

00:14:44.396 --> 00:14:46.886
model and we tried to make a

00:14:46.886 --> 00:14:48.066
personal experience out of that

00:14:48.676 --> 00:14:50.306
by supplying some drawings which

00:14:50.396 --> 00:14:52.706
were just beautiful to our model

00:14:52.706 --> 00:14:53.566
and seeing what we got as an

00:14:53.566 --> 00:14:53.936
output.

00:14:54.946 --> 00:14:56.936
Initially, we didn't get much.

00:14:58.016 --> 00:14:59.546
That's because internally, even

00:14:59.546 --> 00:15:00.446
though we have that pretrained

00:15:00.446 --> 00:15:02.216
neural network, it feeds into a

00:15:02.216 --> 00:15:02.996
K-nearest neighbor base

00:15:02.996 --> 00:15:04.816
classifier which is actually

00:15:04.816 --> 00:15:05.146
empty.

00:15:05.146 --> 00:15:06.476
It has no neighbors to classify

00:15:06.476 --> 00:15:06.596
on.

00:15:10.046 --> 00:15:11.156
So we actually update this and

00:15:11.156 --> 00:15:11.936
give us some neighbors.

00:15:11.936 --> 00:15:13.896
Well, as Anil showed us, we took

00:15:13.896 --> 00:15:15.516
our training data, the stickers

00:15:15.516 --> 00:15:16.656
we chose and the drawings we

00:15:16.656 --> 00:15:17.916
drew to correspond with that,

00:15:17.916 --> 00:15:19.896
and we feed those along with our

00:15:19.896 --> 00:15:22.186
base model, with that updatable

00:15:22.186 --> 00:15:23.256
K-nearest neighbor base model in

00:15:23.256 --> 00:15:24.696
it to our update task.

00:15:25.966 --> 00:15:27.056
And this gives us that new

00:15:27.056 --> 00:15:27.916
variant of our model.

00:15:31.046 --> 00:15:32.216
And this new variant can more

00:15:32.216 --> 00:15:34.136
reliably recognize what it was

00:15:34.136 --> 00:15:36.866
trained on while internally the

00:15:36.866 --> 00:15:38.196
only thing that's changed was

00:15:38.196 --> 00:15:39.426
that updatable K-nearest

00:15:39.426 --> 00:15:43.896
neighbor base model.

00:15:44.056 --> 00:15:45.106
So let's look at the ML update

00:15:45.106 --> 00:15:45.976
task class, which we've

00:15:45.976 --> 00:15:47.576
introduced this year to help

00:15:47.576 --> 00:15:48.536
with managing these update

00:15:48.566 --> 00:15:49.146
processes.

00:15:50.606 --> 00:15:52.066
So it has a state associated

00:15:52.066 --> 00:15:53.006
with it where it's at in the

00:15:53.006 --> 00:15:54.326
process as well as the ability

00:15:54.326 --> 00:15:56.066
to resume and cancel that task.

00:15:56.596 --> 00:15:58.566
And to construct one you just

00:15:58.566 --> 00:16:00.756
pass in that base URL for your

00:16:00.756 --> 00:16:03.246
model along with configuration

00:16:03.416 --> 00:16:05.076
and the training data.

00:16:05.656 --> 00:16:07.086
And as Anil showed, you need to

00:16:07.086 --> 00:16:08.336
pass in a completion handler as

00:16:08.336 --> 00:16:08.556
well.

00:16:09.526 --> 00:16:11.096
As completion handler, we'll

00:16:11.096 --> 00:16:11.926
call when you're done with the

00:16:11.926 --> 00:16:13.106
update task and it gives you an

00:16:13.106 --> 00:16:14.046
update context.

00:16:14.736 --> 00:16:16.376
And you can use this to write

00:16:16.376 --> 00:16:17.426
out that model when you're done

00:16:17.426 --> 00:16:19.076
with your update, to make

00:16:19.076 --> 00:16:20.746
predictions on it, and query

00:16:20.746 --> 00:16:22.006
other parts of that task which

00:16:22.006 --> 00:16:23.206
allows us to get part of this

00:16:24.126 --> 00:16:26.436
update context.

00:16:26.436 --> 00:16:27.616
In code, as Anil showed us,

00:16:27.886 --> 00:16:28.696
really straightforward to

00:16:28.696 --> 00:16:29.046
implement.

00:16:29.746 --> 00:16:31.036
You just construct an

00:16:31.036 --> 00:16:33.586
MLupdateTask provided that URL,

00:16:33.636 --> 00:16:35.106
your configuration, and your

00:16:35.106 --> 00:16:36.636
completion handler, which here

00:16:36.636 --> 00:16:38.166
we use to check for accuracy,

00:16:38.766 --> 00:16:40.056
retain the model outside the

00:16:40.056 --> 00:16:42.266
scope of this block and save out

00:16:42.266 --> 00:16:42.606
that model.

00:16:43.286 --> 00:16:43.776
This is great.

00:16:44.056 --> 00:16:45.556
It works really well and it got

00:16:45.556 --> 00:16:46.216
us through this demo.

00:16:46.766 --> 00:16:49.096
But what about the more complex

00:16:49.096 --> 00:16:49.916
use cases, right?

00:16:49.916 --> 00:16:51.146
What about neural networks?

00:16:51.686 --> 00:16:53.756
Well, a neural network in its

00:16:53.756 --> 00:16:55.066
simplest case is just a

00:16:55.066 --> 00:16:57.046
collection of layers, each with

00:16:57.096 --> 00:16:58.426
some inputs and some weights

00:16:58.776 --> 00:17:00.146
that it uses in combination to

00:17:00.146 --> 00:17:02.376
create an output.

00:17:02.556 --> 00:17:03.766
To adjust that output, we'll

00:17:03.766 --> 00:17:04.836
need to adjust the weights in

00:17:04.836 --> 00:17:05.536
those layers.

00:17:05.786 --> 00:17:07.036
And to do that, we need to mark

00:17:07.036 --> 00:17:08.215
these layers as updatable.

00:17:08.726 --> 00:17:11.556
And we need some way of telling

00:17:11.556 --> 00:17:12.886
it by how much our output was

00:17:12.886 --> 00:17:13.215
off.

00:17:13.715 --> 00:17:15.415
If we expected a star smiley

00:17:15.415 --> 00:17:17.146
face and we got a high five, we

00:17:17.146 --> 00:17:17.915
need to correct for that.

00:17:17.915 --> 00:17:18.945
And that's going to be done to

00:17:18.945 --> 00:17:20.366
our loss functions, which

00:17:20.366 --> 00:17:21.586
describes that delta there.

00:17:22.796 --> 00:17:23.816
And finally we need to provide

00:17:23.816 --> 00:17:25.026
this to our optimizer.

00:17:25.026 --> 00:17:26.536
And this takes that loss, our

00:17:26.536 --> 00:17:27.876
output, and it figures out by

00:17:27.876 --> 00:17:29.146
how much to actually adjust the

00:17:29.146 --> 00:17:30.086
weights in these layers.

00:17:30.616 --> 00:17:35.316
On Core ML 3, we support

00:17:35.316 --> 00:17:37.406
updating of convolution and

00:17:37.406 --> 00:17:38.956
fully collect-- fully connected

00:17:38.956 --> 00:17:40.906
layers as well as having the

00:17:40.906 --> 00:17:42.016
ability to back-propagate

00:17:42.016 --> 00:17:42.606
through many more.

00:17:43.166 --> 00:17:44.426
And we support categorical

00:17:44.426 --> 00:17:46.256
cross-entropy and mean squared

00:17:46.256 --> 00:17:46.986
error loss types.

00:17:48.036 --> 00:17:49.436
In addition, we support

00:17:49.436 --> 00:17:51.166
stochastic gradient descent and

00:17:51.166 --> 00:17:52.646
Adam optimization strategies.

00:17:53.166 --> 00:17:56.076
This is awesome, right?

00:17:56.516 --> 00:17:58.316
But there's other parameters

00:17:58.316 --> 00:17:59.536
associated with neural networks,

00:17:59.946 --> 00:18:00.766
like learning rate and the

00:18:00.766 --> 00:18:01.936
number of epoch to run for.

00:18:02.816 --> 00:18:03.556
This is all are going to be

00:18:03.556 --> 00:18:05.036
encapsulated within that model.

00:18:06.436 --> 00:18:07.796
We understand that some of you

00:18:07.796 --> 00:18:08.676
may want to change that at

00:18:08.676 --> 00:18:09.746
runtime though.

00:18:10.536 --> 00:18:12.126
And you can do that.

00:18:12.206 --> 00:18:13.286
By overriding the update

00:18:13.316 --> 00:18:14.866
parameters dictionary and your

00:18:14.866 --> 00:18:16.476
model configuration, you can

00:18:16.476 --> 00:18:17.736
override those values within,

00:18:18.606 --> 00:18:20.096
using MLParameterKey and just

00:18:20.096 --> 00:18:21.216
specifying values for them,

00:18:21.746 --> 00:18:23.406
giving you a lot of flexibility

00:18:24.006 --> 00:18:24.096
here.

00:18:26.216 --> 00:18:27.936
In addition, if you're wondering

00:18:27.936 --> 00:18:28.956
what parameters are actually

00:18:28.956 --> 00:18:30.436
embedded within my model anyway,

00:18:31.026 --> 00:18:32.136
you can check out the parameter

00:18:32.136 --> 00:18:33.716
section of that model view in

00:18:33.766 --> 00:18:35.106
Xcode and it will give you

00:18:35.106 --> 00:18:36.606
details for the values of those

00:18:36.636 --> 00:18:37.826
parameters as well as what

00:18:37.826 --> 00:18:39.056
parameters are actually defined

00:18:39.056 --> 00:18:39.646
within that model.

00:18:43.676 --> 00:18:45.766
You may want more flexibility

00:18:45.766 --> 00:18:47.136
than the API we showed earlier

00:18:47.776 --> 00:18:48.576
and we provide that.

00:18:49.666 --> 00:18:50.986
In your MLUpdateTask, you can

00:18:50.986 --> 00:18:51.486
provide an

00:18:51.486 --> 00:18:53.896
MLupdateProgressHandler instead

00:18:53.896 --> 00:18:55.566
of the completion handler and

00:18:55.566 --> 00:18:56.756
supply progress handlers and

00:18:56.756 --> 00:18:58.356
that completion handler that

00:18:58.356 --> 00:18:59.246
will allow you to key in

00:18:59.246 --> 00:19:00.096
specific events.

00:19:00.176 --> 00:19:01.626
So when your training began or

00:19:01.626 --> 00:19:03.896
when an epoch ended, we'll call

00:19:03.896 --> 00:19:06.066
your progress handler and we'll

00:19:06.066 --> 00:19:07.316
provide you that update context

00:19:07.316 --> 00:19:08.536
just as we did in the completion

00:19:09.006 --> 00:19:09.236
handler.

00:19:10.696 --> 00:19:12.526
In addition, we'll tell you what

00:19:12.526 --> 00:19:13.976
event caused us to call that

00:19:13.976 --> 00:19:16.176
callback as well as giving you

00:19:16.176 --> 00:19:17.416
key metrics, so you can query

00:19:17.416 --> 00:19:19.126
for your training loss as each

00:19:19.126 --> 00:19:20.446
mini batch or each epoch gets

00:19:20.476 --> 00:19:20.936
processed.

00:19:21.416 --> 00:19:24.546
And in code, still really

00:19:24.546 --> 00:19:25.186
straightforward.

00:19:25.536 --> 00:19:26.186
You construct this

00:19:26.246 --> 00:19:28.126
MLupdateProgressHandler, tell it

00:19:28.126 --> 00:19:29.046
what events you're interested

00:19:29.046 --> 00:19:30.596
in, and supply a block to be

00:19:30.596 --> 00:19:30.926
called.

00:19:31.476 --> 00:19:33.666
In addition, you'll just provide

00:19:33.666 --> 00:19:34.656
the completion handler there as

00:19:34.656 --> 00:19:37.246
well and the MLUpdateTask you

00:19:37.246 --> 00:19:38.696
just provide the set of progress

00:19:38.696 --> 00:19:39.086
handlers.

00:19:43.276 --> 00:19:44.456
So we've talked about how to

00:19:44.456 --> 00:19:45.696
update your models and what it

00:19:45.696 --> 00:19:46.556
means to update them.

00:19:46.926 --> 00:19:47.746
Well, we haven't really

00:19:47.746 --> 00:19:49.446
discussed yet so far is when

00:19:49.446 --> 00:19:50.356
it's appropriate to do that.

00:19:51.506 --> 00:19:52.446
Now, on the grading app we

00:19:52.446 --> 00:19:53.666
showed earlier, we're doing it

00:19:54.156 --> 00:19:55.246
the second he interacted with

00:19:55.286 --> 00:19:56.756
the app and drew a drawing.

00:19:57.126 --> 00:19:58.686
And we took that data and sent

00:19:58.686 --> 00:19:59.266
it off to the model.

00:19:59.266 --> 00:20:00.976
But that may not be appropriate.

00:20:01.116 --> 00:20:02.886
What if you have thousands or

00:20:02.886 --> 00:20:03.936
millions of data points?

00:20:04.946 --> 00:20:05.826
And what if your model's

00:20:05.826 --> 00:20:06.896
incredibly complicated?

00:20:07.966 --> 00:20:09.406
Well, I'm thrilled to say that

00:20:09.406 --> 00:20:10.596
using the BackgroundTask

00:20:10.596 --> 00:20:12.876
framework will allot several

00:20:12.876 --> 00:20:13.956
minutes of runtime to your

00:20:13.956 --> 00:20:14.566
application.

00:20:15.076 --> 00:20:16.336
Even if the user isn't using

00:20:16.336 --> 00:20:17.566
your app or even if they're not

00:20:17.596 --> 00:20:19.636
interacting with the device, you

00:20:19.636 --> 00:20:21.666
just use the BGTaskScheduler and

00:20:21.666 --> 00:20:23.566
make a BGProcessingTaskRequest

00:20:24.046 --> 00:20:24.726
and we'll give you several

00:20:24.726 --> 00:20:26.216
minutes to do further updates

00:20:26.586 --> 00:20:27.696
and further computations.

00:20:29.016 --> 00:20:29.916
And to see more about this--

00:20:29.916 --> 00:20:29.983
Yeah

00:20:30.516 --> 00:20:33.506
[ Applause ]

00:20:34.006 --> 00:20:34.726
It's great.

00:20:34.796 --> 00:20:35.846
And to see more about this, I

00:20:35.846 --> 00:20:37.746
highly recommend you check out

00:20:37.746 --> 00:20:39.046
Advances in App Background

00:20:39.046 --> 00:20:39.636
Execution.

00:20:42.976 --> 00:20:44.276
In addition, how do you get an

00:20:44.276 --> 00:20:44.956
updatable model?

00:20:44.956 --> 00:20:46.246
Well, as we talked about earlier

00:20:46.246 --> 00:20:47.766
in the session, you can get a

00:20:47.766 --> 00:20:49.156
model by converting them from a

00:20:49.156 --> 00:20:50.296
number of supported training

00:20:50.296 --> 00:20:52.216
libraries, that story has not

00:20:52.216 --> 00:20:52.586
changed.

00:20:53.986 --> 00:20:55.186
You simply pass the respect

00:20:55.186 --> 00:20:56.886
trainable flag in when you're

00:20:56.886 --> 00:20:58.616
converting your model and it'll

00:20:58.616 --> 00:20:59.986
take those trainable parameters

00:20:59.986 --> 00:21:00.636
- the things like what

00:21:00.636 --> 00:21:01.936
optimization strategy you want,

00:21:02.376 --> 00:21:04.136
what layers are updatable -- and

00:21:04.136 --> 00:21:05.366
we'll take that and embed that

00:21:05.366 --> 00:21:06.096
within your model.

00:21:06.516 --> 00:21:07.336
And then the rest of it, it's

00:21:07.336 --> 00:21:08.256
just as what it was before.

00:21:08.786 --> 00:21:11.036
And for those of you who want to

00:21:11.036 --> 00:21:12.226
modify the model itself

00:21:12.226 --> 00:21:13.886
directly, that's still fully

00:21:13.886 --> 00:21:17.646
supported as well.

00:21:18.056 --> 00:21:19.056
So we've talked about how to

00:21:19.056 --> 00:21:20.496
make a more personal experience

00:21:20.496 --> 00:21:22.386
for a user in your application,

00:21:22.766 --> 00:21:24.066
using on-device model

00:21:24.066 --> 00:21:26.376
personalization, and you can do

00:21:26.376 --> 00:21:27.366
that through our new flexible

00:21:27.366 --> 00:21:28.026
yet simple API.

00:21:29.276 --> 00:21:29.716
It's great.

00:21:30.046 --> 00:21:30.966
You can do this completely

00:21:30.966 --> 00:21:31.546
on-device.

00:21:32.206 --> 00:21:34.346
And with that, I'd like to hand

00:21:34.346 --> 00:21:35.796
it over to my colleague Aseem

00:21:35.796 --> 00:21:36.906
Wadhwa to tell you more about

00:21:36.906 --> 00:21:37.906
some of the amazing new features

00:21:37.906 --> 00:21:39.226
we've added this year for neural

00:21:39.516 --> 00:21:39.976
networks.

00:21:40.516 --> 00:21:47.376
[ Applause ]

00:21:47.876 --> 00:21:49.566
>> OK. So I'm very excited to

00:21:49.566 --> 00:21:52.256
talk about what's new in Core ML

00:21:52.256 --> 00:21:53.906
this year for neural networks.

00:21:54.676 --> 00:21:57.126
But before we jump into that, I

00:21:57.126 --> 00:21:58.946
do want to spend a few minutes

00:21:58.976 --> 00:22:01.016
talking about neural networks in

00:22:02.096 --> 00:22:02.346
general.

00:22:02.866 --> 00:22:04.686
Well, we all know that neural

00:22:04.686 --> 00:22:06.816
networks are great at solving

00:22:06.816 --> 00:22:08.906
challenging task, such as

00:22:08.906 --> 00:22:10.176
understanding the content of an

00:22:10.176 --> 00:22:12.786
image or a document or an audio

00:22:12.786 --> 00:22:13.096
clip.

00:22:14.516 --> 00:22:17.006
And we can use this capability

00:22:17.006 --> 00:22:18.436
to build some amazing apps

00:22:18.436 --> 00:22:20.806
around them.

00:22:21.836 --> 00:22:23.726
Now, if you look inside a neural

00:22:23.726 --> 00:22:25.416
network and try to see its

00:22:25.416 --> 00:22:27.646
structure, we can maybe get a

00:22:27.646 --> 00:22:28.926
better understanding about it.

00:22:29.746 --> 00:22:31.666
So let's try to do that.

00:22:31.826 --> 00:22:33.176
But instead of looking from a

00:22:33.176 --> 00:22:35.066
point of view of a researcher,

00:22:35.416 --> 00:22:36.396
let's try a different

00:22:36.396 --> 00:22:36.866
perspective.

00:22:37.286 --> 00:22:39.276
Let's try to look at it from a

00:22:39.636 --> 00:22:40.716
programmer's perspective.

00:22:41.316 --> 00:22:43.576
So if you visualize inside a

00:22:43.576 --> 00:22:45.966
Core ML model, what we see is

00:22:45.966 --> 00:22:47.466
something like a graph.

00:22:47.856 --> 00:22:50.056
And if you look closely, we

00:22:50.056 --> 00:22:52.286
realize that graph is just

00:22:52.286 --> 00:22:54.116
another representation of a

00:22:54.116 --> 00:22:54.526
program.

00:22:55.386 --> 00:22:57.766
Let's look at it again.

00:22:58.446 --> 00:23:00.406
So here is a very simple code

00:23:00.406 --> 00:23:02.056
snippet that we all understand.

00:23:02.766 --> 00:23:04.456
And here is its corresponding

00:23:04.596 --> 00:23:05.536
graph representation.

00:23:06.016 --> 00:23:07.806
So as you might have noticed

00:23:07.806 --> 00:23:09.356
that the operations in the code

00:23:09.736 --> 00:23:11.386
have become these notes in the

00:23:11.386 --> 00:23:13.846
graph and the readable such as

00:23:13.846 --> 00:23:16.786
X, Y, Z, are these edges in the

00:23:16.786 --> 00:23:17.196
graph.

00:23:18.026 --> 00:23:19.416
Now, if you go back to a neural

00:23:19.416 --> 00:23:22.236
network graph and now show a

00:23:22.236 --> 00:23:24.216
corresponding code snippet, it

00:23:24.216 --> 00:23:25.616
looks pretty similar to what we

00:23:25.616 --> 00:23:26.166
had before.

00:23:26.586 --> 00:23:27.856
But there are a couple of

00:23:27.886 --> 00:23:32.286
differences I want to point out.

00:23:32.326 --> 00:23:35.206
One, instead of those simple

00:23:35.396 --> 00:23:37.156
numeric variables, we now have

00:23:37.186 --> 00:23:39.236
these multidimensional variables

00:23:39.646 --> 00:23:41.096
that have couple of attributes.

00:23:41.166 --> 00:23:43.226
So it has an attribute called

00:23:43.226 --> 00:23:46.276
shape and something called rank,

00:23:46.436 --> 00:23:47.596
which is a number of dimensions

00:23:47.596 --> 00:23:48.056
that it has.

00:23:48.476 --> 00:23:50.526
And the second thing is the

00:23:50.526 --> 00:23:52.246
functions operating on these

00:23:52.246 --> 00:23:53.866
multidimensional variables are

00:23:53.866 --> 00:23:55.626
now these specialized math

00:23:55.626 --> 00:23:57.276
functions, which is sometimes

00:23:57.336 --> 00:23:59.236
called as layers or operations.

00:24:00.546 --> 00:24:06.646
So if you really look at the

00:24:07.406 --> 00:24:09.056
neural network, we see that it's

00:24:09.056 --> 00:24:12.206
essentially either a graph or a

00:24:12.206 --> 00:24:15.506
program that involves these very

00:24:15.506 --> 00:24:17.426
large multidimensional variables

00:24:17.746 --> 00:24:19.226
and these very heavy

00:24:19.366 --> 00:24:20.336
mathematical functions.

00:24:20.656 --> 00:24:21.806
So essentially it's a very

00:24:21.806 --> 00:24:23.646
compute and memory intensive

00:24:24.006 --> 00:24:24.986
program.

00:24:24.986 --> 00:24:27.276
And the nice thing about Core ML

00:24:27.276 --> 00:24:30.306
is that it encapsulates all

00:24:30.306 --> 00:24:32.236
these complexity into a single

00:24:32.236 --> 00:24:34.526
file format, which the Core ML

00:24:34.526 --> 00:24:35.796
framework and then sort of

00:24:35.796 --> 00:24:37.626
optimize and execute very

00:24:37.626 --> 00:24:38.246
efficiently.

00:24:39.516 --> 00:24:41.906
So if we go back last year and

00:24:41.906 --> 00:24:43.286
see what we had in Core ML 2,

00:24:43.286 --> 00:24:45.766
well, we could easily represent

00:24:45.956 --> 00:24:47.856
straight-line code using acyclic

00:24:47.856 --> 00:24:48.246
graphs.

00:24:48.846 --> 00:24:50.976
And we had about 40 different

00:24:50.976 --> 00:24:52.526
layer types using which we could

00:24:52.526 --> 00:24:54.246
represent most of the common

00:24:54.396 --> 00:24:56.436
convolutional and recurrent

00:24:56.816 --> 00:24:57.726
architectures.

00:24:58.496 --> 00:24:59.666
So what's new this year?

00:25:00.356 --> 00:25:02.086
Well, as you might have noticed

00:25:02.506 --> 00:25:05.806
with my analogy to program, we

00:25:05.806 --> 00:25:07.376
all know that code can be much

00:25:07.376 --> 00:25:08.516
more complex than simple

00:25:08.516 --> 00:25:09.546
straight-line code, right?

00:25:09.886 --> 00:25:12.556
For instance, it's quite common

00:25:12.556 --> 00:25:14.996
to use a control flow like a

00:25:14.996 --> 00:25:16.946
branch as shown in the code

00:25:16.976 --> 00:25:17.636
snippet here.

00:25:18.106 --> 00:25:19.966
And now, in Core ML 3, what's

00:25:19.966 --> 00:25:21.356
new is that we can represent the

00:25:21.446 --> 00:25:23.466
same concept of a branch within

00:25:23.466 --> 00:25:24.006
the neural network

00:25:24.006 --> 00:25:24.696
specification.

00:25:25.516 --> 00:25:30.346
[ Applause ]

00:25:30.846 --> 00:25:32.546
So another common form of

00:25:32.576 --> 00:25:34.566
control flow is obviously loops,

00:25:34.856 --> 00:25:36.356
and that too can be really

00:25:36.356 --> 00:25:38.496
easily expressed within a Core

00:25:38.496 --> 00:25:39.366
ML network.

00:25:40.116 --> 00:25:42.046
Moving on to another complex

00:25:42.046 --> 00:25:44.586
feature of code, something we do

00:25:44.586 --> 00:25:45.806
often when we're writing a

00:25:45.806 --> 00:25:49.026
dynamic code is allocate memory

00:25:49.026 --> 00:25:49.646
at runtime.

00:25:50.056 --> 00:25:51.356
As shown in this code snippet,

00:25:51.356 --> 00:25:52.836
we are allocating a memory that

00:25:52.836 --> 00:25:54.466
depends on the input of the

00:25:54.466 --> 00:25:56.026
program so it can change at

00:25:56.026 --> 00:25:56.406
runtime.

00:25:56.856 --> 00:25:58.406
Now we can do exactly the same

00:25:58.406 --> 00:26:00.696
in Core ML this year using what

00:26:00.696 --> 00:26:03.796
we call dynamic layers, which

00:26:03.796 --> 00:26:05.686
allows us to change the shape of

00:26:05.686 --> 00:26:07.116
the multi array based on the

00:26:07.116 --> 00:26:08.296
input of the graph.

00:26:08.296 --> 00:26:09.486
So you might be thinking that

00:26:09.486 --> 00:26:11.476
why are we adding sort of these

00:26:11.476 --> 00:26:14.446
new complex code constructs

00:26:14.446 --> 00:26:15.896
within the Core ML graph, and

00:26:15.896 --> 00:26:17.336
the answer is simple, because

00:26:17.476 --> 00:26:18.976
neural networks research is

00:26:18.976 --> 00:26:20.986
actively exploring these ideas

00:26:21.266 --> 00:26:22.916
to make even more powerful

00:26:22.916 --> 00:26:24.006
neural networks.

00:26:24.006 --> 00:26:25.546
In fact, many state of the art

00:26:25.546 --> 00:26:28.286
neural networks sort of-- have

00:26:28.286 --> 00:26:30.036
some sort of control flow built

00:26:30.036 --> 00:26:31.346
into them.

00:26:31.656 --> 00:26:33.136
Another aspect that researchers

00:26:33.486 --> 00:26:36.376
are constantly exploring is sort

00:26:36.376 --> 00:26:38.076
of new operations.

00:26:38.436 --> 00:26:41.186
And for that, we have added lots

00:26:41.186 --> 00:26:43.076
of new layers to Core ML this

00:26:43.136 --> 00:26:43.356
year.

00:26:43.986 --> 00:26:45.736
Not only have we made the

00:26:45.736 --> 00:26:47.606
existing layers more generic but

00:26:47.606 --> 00:26:49.756
we have added lots of new basic

00:26:49.756 --> 00:26:50.876
mathematical operations.

00:26:51.336 --> 00:26:53.186
So, if you come across a new

00:26:53.186 --> 00:26:55.046
layer, it's highly likely that

00:26:55.046 --> 00:26:56.556
it can be expressed in terms of

00:26:56.556 --> 00:26:58.416
the layers that are there in

00:26:58.416 --> 00:26:58.976
Core ML 3.

00:26:59.516 --> 00:27:06.186
[ Applause ]

00:27:06.686 --> 00:27:08.646
So with all these features of

00:27:08.646 --> 00:27:11.076
control flow, dynamic behavior,

00:27:11.076 --> 00:27:13.306
and new layers, the Core ML

00:27:13.306 --> 00:27:14.826
model has become much more

00:27:14.826 --> 00:27:16.006
expressive than before.

00:27:16.586 --> 00:27:18.406
And the great part of it is that

00:27:18.776 --> 00:27:20.466
most of the popular

00:27:20.466 --> 00:27:21.816
architectures out there can now

00:27:21.816 --> 00:27:23.686
be easily expressed in the Core

00:27:23.686 --> 00:27:24.176
ML format.

00:27:24.766 --> 00:27:26.006
So as you can see on this slide,

00:27:26.006 --> 00:27:27.736
we have listed a few of those.

00:27:28.066 --> 00:27:29.956
And the ones highlighted have

00:27:29.956 --> 00:27:31.996
really come in the last few

00:27:31.996 --> 00:27:33.016
months and they're really

00:27:33.016 --> 00:27:33.956
pushing the boundaries of

00:27:33.956 --> 00:27:34.996
machine learning research.

00:27:35.296 --> 00:27:36.976
And now you can easily express

00:27:37.026 --> 00:27:39.176
them in Core ML and integrate

00:27:39.176 --> 00:27:40.636
them in your apps.

00:27:40.756 --> 00:27:41.356
So that's great.

00:27:42.516 --> 00:27:47.476
[ Applause ]

00:27:47.976 --> 00:27:49.156
So now you might be wondering

00:27:49.156 --> 00:27:50.836
that how do I make use of these

00:27:51.066 --> 00:27:52.976
new features within a Core ML

00:27:53.016 --> 00:27:53.416
model.

00:27:53.656 --> 00:27:55.146
Well, the answer is same as it

00:27:55.146 --> 00:27:55.876
was last year.

00:27:56.326 --> 00:27:58.076
There are two options to build a

00:27:58.076 --> 00:27:59.876
Core ML model.

00:28:00.326 --> 00:28:02.626
One, since Core ML is an open

00:28:02.626 --> 00:28:04.366
source Protobuf specification,

00:28:04.746 --> 00:28:06.836
we can always specify a model

00:28:06.976 --> 00:28:08.556
programmatically using any

00:28:08.556 --> 00:28:09.536
programming language of your

00:28:09.536 --> 00:28:09.946
choice.

00:28:10.126 --> 00:28:12.026
So that option is always

00:28:12.026 --> 00:28:13.066
available to us.

00:28:13.506 --> 00:28:14.786
But in the majority of the

00:28:14.786 --> 00:28:16.596
cases, we like to use converters

00:28:16.666 --> 00:28:18.086
that can automatically do that

00:28:18.086 --> 00:28:20.486
for us by translating a graph

00:28:20.586 --> 00:28:22.186
from a different representation

00:28:22.716 --> 00:28:24.276
into the Core ML representation.

00:28:24.656 --> 00:28:25.996
So let's look at both of these

00:28:25.996 --> 00:28:27.296
approaches a little bit in

00:28:27.296 --> 00:28:27.626
detail.

00:28:28.916 --> 00:28:30.876
So here I'm showing a simple

00:28:30.976 --> 00:28:33.326
neural network and how it can be

00:28:33.326 --> 00:28:35.996
easily expressed using Core ML

00:28:35.996 --> 00:28:38.336
tools, which is a simple Python

00:28:38.336 --> 00:28:40.096
wrapper around the Protobuf

00:28:40.096 --> 00:28:40.776
specification.

00:28:41.716 --> 00:28:43.366
I personally like this approach,

00:28:43.536 --> 00:28:44.816
especially when I'm converting a

00:28:44.816 --> 00:28:46.726
model whose architecture I

00:28:46.726 --> 00:28:47.496
understand well.

00:28:47.896 --> 00:28:49.616
And if I have the pretrained

00:28:49.616 --> 00:28:51.206
weights available to me in a

00:28:51.206 --> 00:28:54.166
nice data such as numbered

00:28:54.166 --> 00:28:54.656
arrays.

00:28:55.316 --> 00:28:57.316
Having said that, neural

00:28:57.316 --> 00:28:58.906
networks are often much more

00:28:58.906 --> 00:29:01.156
complex, and we are better off

00:29:01.296 --> 00:29:03.346
using converters and we have a

00:29:03.346 --> 00:29:03.926
few of them.

00:29:04.166 --> 00:29:06.256
So we have a few converters

00:29:06.256 --> 00:29:08.406
available on GitHub using which

00:29:08.406 --> 00:29:10.746
you can target most of the

00:29:10.746 --> 00:29:11.906
machine learning frameworks out

00:29:11.956 --> 00:29:12.136
there.

00:29:12.666 --> 00:29:14.396
And the great news is that with

00:29:14.396 --> 00:29:15.736
the backing of Core ML 3

00:29:15.736 --> 00:29:17.416
specification, all these

00:29:17.416 --> 00:29:19.146
converters are getting updated

00:29:19.386 --> 00:29:20.556
and much more robust.

00:29:21.316 --> 00:29:23.276
So, for some of you who have

00:29:23.276 --> 00:29:24.686
used our converters in the past,

00:29:25.186 --> 00:29:26.856
you might have come across some

00:29:26.916 --> 00:29:28.666
error messages like this, like

00:29:28.846 --> 00:29:29.956
maybe it's complaining about a

00:29:29.956 --> 00:29:31.266
missing layer or a missing

00:29:31.266 --> 00:29:32.256
attribute in the layer.

00:29:33.166 --> 00:29:35.476
Now, all of this will go away as

00:29:35.476 --> 00:29:36.986
the converters are updated and

00:29:36.986 --> 00:29:39.016
they make full use of the Core

00:29:39.016 --> 00:29:39.976
ML 3 specification.

00:29:40.516 --> 00:29:46.156
[ Applause ]

00:29:46.656 --> 00:29:50.086
So, we went through a lot of

00:29:50.086 --> 00:29:50.616
slides.

00:29:50.616 --> 00:29:51.756
Now it's time to look at the

00:29:51.756 --> 00:29:52.706
model in action.

00:29:52.906 --> 00:29:54.026
And for that, I'll invite my

00:29:54.026 --> 00:29:54.676
friend Allen.

00:29:56.056 --> 00:29:58.056
[ Applause ]

00:29:58.136 --> 00:29:58.466
>> Thank you.

00:29:59.096 --> 00:30:01.076
Thank you, Aseem.

00:30:01.836 --> 00:30:03.416
Hi. I'm Allen.

00:30:03.786 --> 00:30:05.526
Today, I'm going to show you how

00:30:05.526 --> 00:30:07.166
to use the new features in Core

00:30:07.166 --> 00:30:09.436
ML 3 to bring state of the art

00:30:09.436 --> 00:30:10.606
machine learning models for

00:30:10.606 --> 00:30:12.196
natural language processing into

00:30:12.196 --> 00:30:12.576
our apps.

00:30:15.006 --> 00:30:16.736
So, I love reading about

00:30:16.736 --> 00:30:17.126
history.

00:30:17.836 --> 00:30:19.176
Whenever I come across some

00:30:19.176 --> 00:30:20.276
interesting articles about

00:30:20.276 --> 00:30:22.326
history, I often have some

00:30:22.326 --> 00:30:23.786
questions on top of my head and

00:30:23.786 --> 00:30:25.416
I really like to get answers for

00:30:25.416 --> 00:30:25.546
that.

00:30:26.316 --> 00:30:27.546
But sometimes I'm just

00:30:27.546 --> 00:30:28.026
impatient.

00:30:28.416 --> 00:30:29.486
I don't want to read through the

00:30:29.486 --> 00:30:30.156
entire article.

00:30:30.866 --> 00:30:33.256
So, wouldn't it be nice if I can

00:30:33.256 --> 00:30:35.086
build an app that scan through

00:30:35.086 --> 00:30:36.826
the document and then give the

00:30:36.826 --> 00:30:37.496
answers for me?

00:30:38.186 --> 00:30:40.186
So then I started out build this

00:30:40.186 --> 00:30:41.746
app with the new features in

00:30:41.746 --> 00:30:42.266
Core ML 3.

00:30:42.656 --> 00:30:43.286
And let me show you.

00:30:51.416 --> 00:30:52.806
OK. So, here's my app.

00:30:54.056 --> 00:30:56.266
As you can see, it shows article

00:30:56.266 --> 00:30:57.866
about history of a company

00:30:57.866 --> 00:30:58.306
called NeXT.

00:30:59.226 --> 00:31:01.456
So, as you can see, this is a

00:31:01.456 --> 00:31:02.976
long article, and I certainly

00:31:02.976 --> 00:31:03.946
don't have time to go through

00:31:03.946 --> 00:31:04.096
it.

00:31:04.096 --> 00:31:05.706
And I have some questions about

00:31:05.706 --> 00:31:05.773
it.

00:31:06.276 --> 00:31:10.326
So let me just ask this app.

00:31:10.586 --> 00:31:12.306
Let's try my first question.

00:31:13.416 --> 00:31:16.046
Who started NeXT?

00:31:19.076 --> 00:31:20.806
>> Steve Jobs.

00:31:21.516 --> 00:31:26.186
[ Applause ]

00:31:26.686 --> 00:31:28.666
>> OK. I think that's right.

00:31:29.276 --> 00:31:33.966
Let me try another one.

00:31:34.176 --> 00:31:35.756
Where was the main office

00:31:35.756 --> 00:31:36.406
located?

00:31:39.546 --> 00:31:41.536
>> Redwood City, California

00:31:42.516 --> 00:31:45.716
[ Applause ]

00:31:46.216 --> 00:31:48.036
>> Now I also have a question

00:31:48.036 --> 00:31:49.136
that I'm very interested in.

00:31:49.556 --> 00:31:50.906
Let me try that.

00:31:52.876 --> 00:31:55.086
How much were engineers paid?

00:31:58.286 --> 00:31:59.826
>> Seventy-five thousand or

00:31:59.826 --> 00:32:02.826
$50,000 [applause].

00:32:03.136 --> 00:32:03.466
>> Interesting.

00:32:04.016 --> 00:32:06.000
[ Applause ]

00:32:09.436 --> 00:32:11.126
So, isn't this cool?

00:32:11.846 --> 00:32:16.886
So now, let's get deeper into it

00:32:17.076 --> 00:32:18.176
and see what the app really

00:32:18.176 --> 00:32:18.386
does.

00:32:20.486 --> 00:32:22.146
So, the central piece of this

00:32:22.146 --> 00:32:24.026
app is a state of the art

00:32:24.106 --> 00:32:25.596
machine learning model called

00:32:25.986 --> 00:32:27.406
Bidirectional Encoder

00:32:27.406 --> 00:32:28.566
Representation from

00:32:28.566 --> 00:32:29.156
Transformers.

00:32:29.666 --> 00:32:30.876
And this is a very long name.

00:32:31.236 --> 00:32:32.786
So, let's just call it the BERT

00:32:32.786 --> 00:32:34.426
model as other researchers do.

00:32:35.216 --> 00:32:37.656
So, what does BERT model do?

00:32:38.186 --> 00:32:40.636
Well, it is actually a-- it's

00:32:40.776 --> 00:32:42.796
actually a neural network that

00:32:42.796 --> 00:32:47.436
can perform multiple tasks for

00:32:47.436 --> 00:32:48.846
natural language understanding.

00:32:49.466 --> 00:32:52.986
But what's inside the BERT

00:32:52.986 --> 00:32:53.326
model?

00:32:54.766 --> 00:32:55.536
A bunch of modules.

00:32:56.096 --> 00:32:57.466
And what's inside these modules?

00:32:58.586 --> 00:33:00.296
Layers, many, many layers.

00:33:01.316 --> 00:33:02.326
So you can see, it's rather

00:33:02.326 --> 00:33:04.476
complicated, but with the new

00:33:04.476 --> 00:33:06.106
features and tools in Core ML 3,

00:33:06.476 --> 00:33:08.256
I can easily bring this model

00:33:08.256 --> 00:33:09.506
into my app.

00:33:10.896 --> 00:33:13.996
But first, the question is, how

00:33:13.996 --> 00:33:14.676
do I get the model?

00:33:15.406 --> 00:33:19.466
Well, you can get a model on our

00:33:19.626 --> 00:33:22.976
model gallery website, or you

00:33:22.976 --> 00:33:25.296
can-- if you like, you can train

00:33:25.296 --> 00:33:26.806
a model and then convert it.

00:33:28.016 --> 00:33:30.166
For example, the other night I

00:33:30.166 --> 00:33:32.046
trained my BERT model with

00:33:32.046 --> 00:33:32.566
TensorFlow.

00:33:33.186 --> 00:33:35.336
And here is a screenshot of my

00:33:35.486 --> 00:33:35.946
workspace.

00:33:36.476 --> 00:33:38.286
Sorry for me being very messy

00:33:38.286 --> 00:33:38.986
with my workspace.

00:33:39.686 --> 00:33:41.916
But to use new Core ML

00:33:42.426 --> 00:33:44.906
converter, I just need to export

00:33:44.906 --> 00:33:47.606
a model into the Protobuf format

00:33:48.146 --> 00:33:49.696
right here.

00:33:50.936 --> 00:33:52.826
And after that, all I need to do

00:33:53.086 --> 00:33:54.586
is to type three lines of Python

00:33:54.586 --> 00:33:54.826
code.

00:33:55.746 --> 00:33:57.436
Import TF Core ML converter,

00:33:58.046 --> 00:33:59.086
call the convert function, and

00:33:59.736 --> 00:34:02.966
then save out as an ML model.

00:34:03.076 --> 00:34:04.476
So as you can see, it's rather

00:34:04.476 --> 00:34:06.336
easy to bring a model into the

00:34:06.336 --> 00:34:06.776
app.

00:34:07.066 --> 00:34:08.116
But to use the app for a

00:34:08.146 --> 00:34:09.735
question and answering, there

00:34:09.735 --> 00:34:11.606
are few more steps and I like to

00:34:11.606 --> 00:34:12.916
explain a little further.

00:34:13.315 --> 00:34:15.795
So to use the Q and A model, I

00:34:15.795 --> 00:34:17.916
need to prepare a question and a

00:34:17.916 --> 00:34:19.956
paragraph and then separate them

00:34:19.956 --> 00:34:20.906
as work tokens.

00:34:21.366 --> 00:34:23.485
What the model will predict is

00:34:23.596 --> 00:34:26.556
the location of the answer

00:34:27.045 --> 00:34:28.146
that's in the paragraph.

00:34:28.795 --> 00:34:30.186
Think of it as a highlighter as

00:34:30.186 --> 00:34:33.275
you just saw in the demo.

00:34:33.436 --> 00:34:34.775
So from there, I can start

00:34:34.775 --> 00:34:35.735
building up my app.

00:34:36.396 --> 00:34:37.626
The model itself does not make

00:34:38.036 --> 00:34:38.706
the app.

00:34:39.076 --> 00:34:40.856
And in addition to that, I also

00:34:40.856 --> 00:34:43.676
utilize many features from other

00:34:43.676 --> 00:34:45.335
frameworks that makes this app.

00:34:45.775 --> 00:34:47.516
For example, I use the

00:34:47.516 --> 00:34:49.525
speech-to-text API from speech

00:34:49.525 --> 00:34:51.966
framework to translate my voice

00:34:51.966 --> 00:34:52.516
into text.

00:34:53.505 --> 00:34:56.536
I use natural language API to

00:34:56.536 --> 00:34:58.066
help me build the tokenizer.

00:34:58.756 --> 00:35:01.546
And finally, I use the

00:35:01.936 --> 00:35:03.186
text-to-speech API from the

00:35:03.186 --> 00:35:05.356
AVFoundation to play out the

00:35:05.356 --> 00:35:06.266
audio of an answer.

00:35:06.966 --> 00:35:08.316
So, all these components

00:35:08.576 --> 00:35:09.986
utilizes machine learning

00:35:10.046 --> 00:35:12.066
on-device, so that to use this

00:35:12.066 --> 00:35:13.946
app, no internet access is

00:35:13.946 --> 00:35:14.396
required.

00:35:15.516 --> 00:35:17.776
[ Applause ]

00:35:18.276 --> 00:35:19.456
Thank you.

00:35:19.456 --> 00:35:21.636
Just think about how much more

00:35:21.636 --> 00:35:24.126
possibility of new ideas and new

00:35:24.246 --> 00:35:25.606
user experience you can bring

00:35:25.606 --> 00:35:26.436
into our app.

00:35:27.056 --> 00:35:27.646
That's all.

00:35:27.646 --> 00:35:27.976
Thank you.

00:35:28.516 --> 00:35:35.516
[ Applause ]

00:35:36.016 --> 00:35:38.766
>> OK. Thank you, Allen.

00:35:40.706 --> 00:35:42.876
OK. So before we conclude the

00:35:42.876 --> 00:35:44.266
session, I want to highlight

00:35:44.266 --> 00:35:45.416
three more features that we

00:35:45.416 --> 00:35:48.096
added this year in Core ML that

00:35:48.096 --> 00:35:50.086
I'm sure a lot of Core ML users

00:35:50.086 --> 00:35:51.026
will find really useful.

00:35:51.766 --> 00:35:56.926
So let's look at the first one.

00:35:56.926 --> 00:35:58.876
So consider a scenario shown in

00:35:58.876 --> 00:36:00.606
the slide, let's say we have two

00:36:00.606 --> 00:36:02.886
models that classify different

00:36:02.886 --> 00:36:03.796
breeds of animal.

00:36:04.516 --> 00:36:06.556
And if you look inside, both

00:36:06.556 --> 00:36:09.086
these models are pipeline models

00:36:09.586 --> 00:36:10.976
that share a common feature

00:36:10.976 --> 00:36:11.456
extractor.

00:36:12.136 --> 00:36:13.406
Now this happens quite often.

00:36:13.576 --> 00:36:15.976
It's quite common to share-- to

00:36:15.976 --> 00:36:18.256
train deep neural network to get

00:36:18.256 --> 00:36:20.056
features and those features can

00:36:20.056 --> 00:36:21.436
be fed to different neural

00:36:21.436 --> 00:36:21.956
networks.

00:36:22.816 --> 00:36:25.906
So actually in the slide, we

00:36:25.906 --> 00:36:27.876
note that we are using multiple

00:36:27.876 --> 00:36:29.736
copies of the same model within

00:36:29.736 --> 00:36:30.716
these two pipelines.

00:36:31.146 --> 00:36:32.296
Now this is clearly not

00:36:32.296 --> 00:36:32.756
efficient.

00:36:33.446 --> 00:36:35.036
To get rid of this inefficiency,

00:36:35.296 --> 00:36:36.426
we are launching a new model

00:36:36.466 --> 00:36:38.596
type called linked model, as

00:36:38.596 --> 00:36:38.996
shown here.

00:36:38.996 --> 00:36:42.266
So just see, the idea is quite

00:36:42.316 --> 00:36:42.696
simple.

00:36:43.016 --> 00:36:44.936
Linked model is simply a

00:36:44.936 --> 00:36:47.236
reference to a model sitting at

00:36:47.236 --> 00:36:47.606
desk.

00:36:48.266 --> 00:36:50.376
And this really makes it easy to

00:36:50.376 --> 00:36:51.876
share a model across different

00:36:52.096 --> 00:36:52.896
models.

00:36:54.876 --> 00:36:56.006
Another way I like to think

00:36:56.006 --> 00:36:58.146
about linked model is it behaves

00:36:58.146 --> 00:36:59.806
like linking to a dynamic

00:37:00.126 --> 00:37:01.296
library.

00:37:01.686 --> 00:37:03.436
And it has only a couple of

00:37:03.436 --> 00:37:04.166
parameters.

00:37:04.166 --> 00:37:05.636
One, the name of the model that

00:37:05.636 --> 00:37:07.466
it's linking to and a search

00:37:07.466 --> 00:37:07.816
path.

00:37:08.826 --> 00:37:10.396
So this would be very useful

00:37:10.396 --> 00:37:12.126
for-- when we are using

00:37:12.126 --> 00:37:13.856
updatable models or pipelines.

00:37:14.676 --> 00:37:17.446
Let's look at the next feature.

00:37:17.726 --> 00:37:19.296
Let's say you have a Core ML

00:37:19.296 --> 00:37:21.206
model that takes an image as an

00:37:21.206 --> 00:37:21.586
input.

00:37:22.136 --> 00:37:23.776
Now as of now, Core ML expects

00:37:23.816 --> 00:37:26.066
the image to be in the form of a

00:37:26.066 --> 00:37:27.716
CVPixelBuffer.

00:37:27.716 --> 00:37:29.806
But what happens if your image

00:37:29.806 --> 00:37:30.786
is coming from a different

00:37:30.786 --> 00:37:32.076
source and it's in a different

00:37:32.076 --> 00:37:32.516
format?

00:37:33.466 --> 00:37:34.706
Now, in most of the cases, you

00:37:35.276 --> 00:37:36.996
could use the vision framework

00:37:37.276 --> 00:37:39.746
so you can invoke Core ML via

00:37:39.746 --> 00:37:41.566
the VNCoreMLRequest class.

00:37:42.156 --> 00:37:43.736
And this has the advantage that

00:37:43.736 --> 00:37:45.646
vision can handle many different

00:37:45.646 --> 00:37:47.686
formats of images for us, and

00:37:47.686 --> 00:37:49.526
they can also do preprocessing

00:37:49.526 --> 00:37:51.486
like image scaling and cropping,

00:37:51.486 --> 00:37:53.016
et cetera.

00:37:53.216 --> 00:37:54.796
In some cases, though, we might

00:37:54.796 --> 00:37:56.756
have to call the Core ML API

00:37:56.756 --> 00:37:58.926
directly, for instance when we

00:37:58.926 --> 00:38:01.046
are trying to invoke the update

00:38:01.046 --> 00:38:01.366
APIs.

00:38:01.766 --> 00:38:04.146
For such cases, we have launched

00:38:04.146 --> 00:38:05.576
a couple of new initializer

00:38:05.576 --> 00:38:07.186
methods, I showed on the slide.

00:38:07.546 --> 00:38:09.206
So now we can directly get an

00:38:09.206 --> 00:38:11.906
image from a URL or a CGimage.

00:38:12.516 --> 00:38:17.776
[ Applause ]

00:38:18.276 --> 00:38:19.976
So this should make it really

00:38:19.976 --> 00:38:21.886
convenient to use images with

00:38:21.886 --> 00:38:23.886
Core ML.

00:38:24.366 --> 00:38:25.506
Moving on to the last feature I

00:38:25.506 --> 00:38:26.896
want to highlight.

00:38:26.896 --> 00:38:28.256
So there's a class in Core ML

00:38:28.256 --> 00:38:30.486
API called MLModelConfiguration.

00:38:30.486 --> 00:38:33.536
And it can be used to constrain

00:38:33.536 --> 00:38:35.566
the set of devices on which a

00:38:35.566 --> 00:38:36.966
Core ML model can execute.

00:38:37.246 --> 00:38:39.476
For example, the default value

00:38:39.476 --> 00:38:41.056
that the [inaudible] takes is

00:38:41.166 --> 00:38:44.616
called all which gives all the

00:38:44.616 --> 00:38:45.736
computer devices available

00:38:45.876 --> 00:38:47.326
including the neural engine.

00:38:48.136 --> 00:38:49.756
Now we have added a couple of

00:38:49.796 --> 00:38:52.026
more options to this class.

00:38:52.756 --> 00:38:55.546
The first one is the ability to

00:38:55.546 --> 00:38:58.336
specify preferred metal device

00:38:59.166 --> 00:39:00.696
on which the model can execute.

00:39:01.236 --> 00:39:02.426
So as you can imagine, this

00:39:02.426 --> 00:39:03.716
would be really useful if you

00:39:03.716 --> 00:39:05.436
are running a Core ML model on a

00:39:05.436 --> 00:39:06.846
Mac which can have many

00:39:06.846 --> 00:39:09.706
different GPUs attached to it.

00:39:10.106 --> 00:39:11.036
The other option that we've

00:39:11.036 --> 00:39:13.066
added this called low precision

00:39:13.066 --> 00:39:13.776
accumulation.

00:39:13.776 --> 00:39:15.996
And the idea here is that if

00:39:15.996 --> 00:39:17.576
your model is learning on the

00:39:17.576 --> 00:39:19.906
GPU, instead of doing

00:39:19.906 --> 00:39:21.766
accumulation in float32, that

00:39:21.766 --> 00:39:22.946
happens in float60.

00:39:23.296 --> 00:39:25.026
Now this can offer some really

00:39:25.026 --> 00:39:27.566
nice speed enhancement for your

00:39:27.566 --> 00:39:27.906
model.

00:39:28.636 --> 00:39:29.896
But whenever we reduce the

00:39:29.896 --> 00:39:31.616
precision, always remember to

00:39:31.656 --> 00:39:33.416
check the accuracy of the model,

00:39:33.826 --> 00:39:36.056
which might degrade or not

00:39:36.056 --> 00:39:37.776
degrade, depending on the model

00:39:37.776 --> 00:39:37.936
type.

00:39:38.246 --> 00:39:41.436
So always try to experiment

00:39:41.436 --> 00:39:43.266
whenever you vary the precision

00:39:43.266 --> 00:39:43.726
of the model.

00:39:43.866 --> 00:39:45.136
So I would highly encourage you

00:39:45.136 --> 00:39:46.676
to go and try out this option to

00:39:46.676 --> 00:39:49.706
see if it helps with your model.

00:39:50.346 --> 00:39:52.306
OK. So, we talked about a lot of

00:39:52.306 --> 00:39:53.276
stuff in this session.

00:39:53.276 --> 00:39:55.206
Let briefly summarize it for

00:39:55.206 --> 00:39:55.476
you.

00:39:55.996 --> 00:39:58.976
We discussed how it's really

00:39:58.976 --> 00:40:00.656
easy to make a personalized

00:40:00.656 --> 00:40:02.426
experience for our users by

00:40:02.426 --> 00:40:03.646
updating the Core ML model

00:40:03.736 --> 00:40:04.346
on-device.

00:40:05.126 --> 00:40:07.006
We discussed how we have added

00:40:07.066 --> 00:40:08.066
many more features to our

00:40:08.066 --> 00:40:09.386
specification and now we can

00:40:09.386 --> 00:40:11.156
bring the state of the art

00:40:11.156 --> 00:40:12.296
neural network architectures

00:40:12.646 --> 00:40:14.266
into our apps.

00:40:14.266 --> 00:40:15.746
And we talk about a few

00:40:16.026 --> 00:40:17.466
convenience APIs and options

00:40:17.466 --> 00:40:18.036
around GPU.

00:40:18.536 --> 00:40:21.366
Here are a few sessions that you

00:40:21.366 --> 00:40:22.796
might find interesting and

00:40:22.796 --> 00:40:23.786
related to the session.

00:40:24.346 --> 00:40:25.656
And thank you.

00:40:26.516 --> 00:40:31.500
[ Applause ]