WEBVTT

00:00:00.506 --> 00:00:04.500
[ Music ]

00:00:07.516 --> 00:00:14.196
[ Applause ]

00:00:14.696 --> 00:00:15.336
>> Good morning.

00:00:15.676 --> 00:00:16.976
My name is Brittany Weinert, and

00:00:16.976 --> 00:00:17.946
I'm a software engineer on the

00:00:17.946 --> 00:00:18.866
Vision Framework Team.

00:00:19.566 --> 00:00:20.596
This year the Vision Team has a

00:00:20.596 --> 00:00:22.086
lot of exciting new updates that

00:00:22.086 --> 00:00:23.096
we think you're all going to

00:00:23.096 --> 00:00:23.316
love.

00:00:23.916 --> 00:00:24.846
Because we have so much new

00:00:24.846 --> 00:00:26.076
stuff to cover, we're going to

00:00:26.076 --> 00:00:27.106
dive right into the new

00:00:27.106 --> 00:00:27.516
features.

00:00:27.876 --> 00:00:29.256
If you're completely new to

00:00:29.256 --> 00:00:30.706
Vision, don't worry.

00:00:30.706 --> 00:00:31.466
You should still be able to

00:00:31.466 --> 00:00:33.096
follow along, and our hope is

00:00:33.096 --> 00:00:34.216
that the new capabilities that

00:00:34.216 --> 00:00:35.956
we introduce today will motivate

00:00:35.956 --> 00:00:37.366
you to learn about Vision and to

00:00:37.366 --> 00:00:39.606
use it in your apps.

00:00:39.836 --> 00:00:40.906
Today well be covering four

00:00:40.906 --> 00:00:44.206
completely new topics, saliency,

00:00:44.416 --> 00:00:46.066
image classification, Image

00:00:46.066 --> 00:00:47.556
Similarity, and face quality.

00:00:47.556 --> 00:00:49.456
We also have some technology

00:00:49.456 --> 00:00:50.796
upgrades for the Object Tracker

00:00:50.796 --> 00:00:52.826
and Face Landmarks as well as

00:00:52.826 --> 00:00:54.666
new detectors and improved Core

00:00:54.666 --> 00:00:55.196
ML support.

00:00:55.196 --> 00:00:58.486
Today, I'm going to be talking

00:00:58.486 --> 00:00:59.276
about saliency.

00:00:59.736 --> 00:01:00.836
Let's start with a definition.

00:01:01.386 --> 00:01:02.806
I'm about to show you a photo,

00:01:02.806 --> 00:01:06.426
and I want you to pay attention

00:01:06.426 --> 00:01:07.586
to where your eyes are first

00:01:07.586 --> 00:01:08.036
drawn.

00:01:08.526 --> 00:01:13.046
When you first saw this photo of

00:01:13.046 --> 00:01:14.126
the three puffins sitting on a

00:01:14.126 --> 00:01:15.696
cliff, did you notice what stood

00:01:15.696 --> 00:01:16.366
out to you first?

00:01:17.326 --> 00:01:19.956
According to our models, most of

00:01:19.956 --> 00:01:21.226
you looked at the puffins faces

00:01:21.226 --> 00:01:21.476
first.

00:01:22.436 --> 00:01:23.556
This is saliency.

00:01:24.006 --> 00:01:26.366
There are two types of saliency,

00:01:26.766 --> 00:01:28.156
attention based and objectness

00:01:28.216 --> 00:01:28.566
based.

00:01:29.086 --> 00:01:30.976
The overlay that you saw on the

00:01:30.976 --> 00:01:32.566
puffin image just now called the

00:01:32.566 --> 00:01:34.226
heatmap was generated by

00:01:34.226 --> 00:01:35.686
attention based saliency.

00:01:36.146 --> 00:01:37.216
But before we get into more

00:01:37.216 --> 00:01:38.556
visual examples, I want to go

00:01:38.556 --> 00:01:39.696
over the basics of each

00:01:39.696 --> 00:01:40.246
algorithm.

00:01:41.776 --> 00:01:45.186
Attention based saliency is a

00:01:45.186 --> 00:01:47.786
human aspected saliency, and by

00:01:47.786 --> 00:01:49.436
this, I mean that the attention

00:01:49.436 --> 00:01:51.006
based saliency models were

00:01:51.006 --> 00:01:53.566
generated by where people looked

00:01:53.906 --> 00:01:55.006
when they were shown a series of

00:01:55.006 --> 00:01:55.446
images.

00:01:56.326 --> 00:01:57.646
This means that the heatmap

00:01:57.646 --> 00:02:00.026
reflects and highlights where

00:02:00.026 --> 00:02:00.966
people first look when they're

00:02:00.966 --> 00:02:01.646
shown an image.

00:02:02.676 --> 00:02:04.556
Objectness based saliency on the

00:02:04.556 --> 00:02:06.006
other hand was trained on

00:02:06.056 --> 00:02:07.976
subject segmentation in an image

00:02:08.515 --> 00:02:10.286
with the goal to highlight the

00:02:10.286 --> 00:02:12.086
foreground objects or the

00:02:12.086 --> 00:02:13.176
subjects of an image.

00:02:13.766 --> 00:02:15.996
So, in the heatmap, the subjects

00:02:15.996 --> 00:02:17.196
or foreground objects should be

00:02:17.196 --> 00:02:17.656
highlighted.

00:02:18.226 --> 00:02:19.916
Let's look at some examples now.

00:02:20.496 --> 00:02:23.566
So, here are the puffins from

00:02:23.566 --> 00:02:23.946
earlier.

00:02:24.636 --> 00:02:26.956
Here's the attention based

00:02:26.956 --> 00:02:28.436
heatmap overlaid on the image,

00:02:29.446 --> 00:02:30.916
and here's the objectness based

00:02:32.316 --> 00:02:33.046
heatmap.

00:02:33.046 --> 00:02:34.806
As I said, people tend to look

00:02:34.806 --> 00:02:36.056
at the puffins' faces first, so

00:02:36.346 --> 00:02:37.686
the area around the puffins'

00:02:37.686 --> 00:02:38.996
heads is very salient for the

00:02:38.996 --> 00:02:39.956
attention based heatmap.

00:02:40.896 --> 00:02:42.286
For objectness, we're just

00:02:42.286 --> 00:02:43.676
trying to pick up the subjects,

00:02:43.676 --> 00:02:45.246
and in this case, it's the three

00:02:45.246 --> 00:02:45.636
puffins.

00:02:45.786 --> 00:02:46.846
So, all the puffins are

00:02:46.846 --> 00:02:47.266
highlighted.

00:02:48.486 --> 00:02:50.206
Let's look at how saliency works

00:02:50.206 --> 00:02:51.206
with images of people.

00:02:51.746 --> 00:02:56.556
For attention based saliency,

00:02:57.046 --> 00:02:58.656
the areas around peoples' faces

00:02:58.656 --> 00:02:59.926
tend to be the most salient,

00:03:00.536 --> 00:03:02.146
unsurprisingly because we tend

00:03:02.146 --> 00:03:04.146
to look at people's faces first.

00:03:04.846 --> 00:03:06.616
For objectness based saliency,

00:03:06.616 --> 00:03:08.196
if the person is the subject of

00:03:08.196 --> 00:03:09.646
the image, the entire person

00:03:09.646 --> 00:03:10.386
should be highlighted.

00:03:12.596 --> 00:03:14.896
So, attention based saliency

00:03:14.896 --> 00:03:16.266
though is the more complicated

00:03:16.266 --> 00:03:17.846
of the two saliencies, I'd say,

00:03:18.436 --> 00:03:19.926
because it is determined by a

00:03:19.926 --> 00:03:21.386
number of very human factors.

00:03:22.226 --> 00:03:23.846
And the number, the main factors

00:03:23.846 --> 00:03:25.126
that determine attention based

00:03:25.126 --> 00:03:26.236
saliency and what's salient or

00:03:26.236 --> 00:03:28.426
not, is contrast, faces,

00:03:28.916 --> 00:03:31.046
subjects, horizons, and light.

00:03:32.416 --> 00:03:33.666
But interestingly enough, it can

00:03:33.666 --> 00:03:35.516
also be affected by perceived

00:03:35.516 --> 00:03:36.026
motion.

00:03:36.556 --> 00:03:39.816
In this example, the umbrella

00:03:39.816 --> 00:03:42.436
colors really pop, so the area

00:03:42.436 --> 00:03:43.716
around the umbrella is salient,

00:03:44.026 --> 00:03:45.636
but the road is also salient

00:03:46.006 --> 00:03:47.666
because our eyes try to track

00:03:47.666 --> 00:03:48.796
where the umbrella is headed.

00:03:50.706 --> 00:03:52.216
For objectness based saliency,

00:03:52.216 --> 00:03:53.406
we just pick up on the umbrella

00:03:55.016 --> 00:03:55.096
guy.

00:03:55.316 --> 00:03:56.876
So, I could do this all day and

00:03:56.876 --> 00:03:58.456
show you more examples, but

00:03:58.456 --> 00:03:59.456
honestly, the best way to

00:03:59.456 --> 00:04:00.966
understand saliency is to try it

00:04:00.966 --> 00:04:01.766
out for yourself.

00:04:02.476 --> 00:04:03.776
I encourage everybody to

00:04:03.776 --> 00:04:05.726
download the Saliency app and

00:04:05.726 --> 00:04:06.656
try it on their own photo

00:04:06.656 --> 00:04:07.136
libraries.

00:04:07.796 --> 00:04:10.536
So, let's get into what's

00:04:10.536 --> 00:04:12.236
returned from the saliency

00:04:12.236 --> 00:04:15.726
request, mainly the heatmap.

00:04:16.055 --> 00:04:17.375
So, the images that I've been

00:04:17.375 --> 00:04:19.596
showing to you up until now, the

00:04:19.596 --> 00:04:22.356
heatmap has been scaled,

00:04:22.356 --> 00:04:25.996
overlaid, and colorized and put

00:04:25.996 --> 00:04:27.536
onto the image, but in

00:04:27.536 --> 00:04:29.656
actuality, the heatmap is a very

00:04:29.656 --> 00:04:32.206
small CV pixel buffer that's

00:04:32.206 --> 00:04:34.186
made up of Floats in the range

00:04:34.186 --> 00:04:37.246
of 0 to 1, 0 designating

00:04:37.526 --> 00:04:39.356
nonsalient and 1 being most

00:04:39.356 --> 00:04:39.746
salient.

00:04:40.256 --> 00:04:44.066
And there's extra code that

00:04:44.066 --> 00:04:45.206
you'd have to do to get the

00:04:45.206 --> 00:04:46.526
exact same effect like you see

00:04:46.526 --> 00:04:46.746
here.

00:04:47.526 --> 00:04:48.896
But let's go into how to

00:04:48.946 --> 00:04:50.926
formulate a request at the very

00:04:50.926 --> 00:04:51.716
basic level.

00:04:53.256 --> 00:04:56.686
Okay. So, first we start out

00:04:56.686 --> 00:04:58.776
with a VNImageRequestHandler to

00:04:58.776 --> 00:05:00.206
handle a single image.

00:05:01.266 --> 00:05:03.126
Next, you choose the algorithm

00:05:03.126 --> 00:05:04.346
that you want to run, in this

00:05:04.346 --> 00:05:06.516
case, AttentionBasedSaliency,

00:05:06.566 --> 00:05:09.256
and set the revision if you

00:05:09.256 --> 00:05:10.586
always want to be using the same

00:05:10.586 --> 00:05:10.976
algorithm.

00:05:12.976 --> 00:05:14.896
Next, you call perform request,

00:05:15.196 --> 00:05:17.466
like you usually would, and if

00:05:17.466 --> 00:05:19.476
it's successful, the results

00:05:19.476 --> 00:05:20.996
property on the request should

00:05:20.996 --> 00:05:22.456
be populated with a

00:05:22.456 --> 00:05:24.266
VNSaliencyImageObservation.

00:05:25.326 --> 00:05:28.396
To access the heatmap, you call

00:05:28.396 --> 00:05:30.086
the pixelBuffer property on the

00:05:30.086 --> 00:05:32.906
VNSaliencyImageObservation like

00:05:35.256 --> 00:05:35.356
so.

00:05:35.596 --> 00:05:37.516
If you wanted to do objectness

00:05:37.516 --> 00:05:39.316
based saliency, all you would

00:05:39.316 --> 00:05:41.096
have to do is change the request

00:05:41.246 --> 00:05:43.786
name and the revision to be

00:05:43.786 --> 00:05:44.246
objectness.

00:05:45.426 --> 00:05:46.676
So, for attention, it's

00:05:46.736 --> 00:05:48.206
VNGenerateAttentionBased

00:05:48.206 --> 00:05:50.106
SaliencyImageRequest and for

00:05:50.106 --> 00:05:50.836
objectness, it's

00:05:50.836 --> 00:05:52.386
VNGenerateObjectnessBased

00:05:52.386 --> 00:05:53.076
SaliencyRequest.

00:05:53.796 --> 00:05:56.856
So, let's get into another tool

00:05:56.856 --> 00:05:58.226
other than at heatmap, the

00:05:58.226 --> 00:05:58.856
bounding box.

00:05:59.476 --> 00:06:03.196
The bounding boxes encapsulate

00:06:03.196 --> 00:06:04.536
all the salient regions in an

00:06:04.536 --> 00:06:04.796
image.

00:06:05.156 --> 00:06:06.726
For attention based saliency,

00:06:06.726 --> 00:06:08.006
you should always have one

00:06:08.006 --> 00:06:09.866
bounding box, and for objectness

00:06:09.866 --> 00:06:11.116
based saliency, you can have up

00:06:11.116 --> 00:06:12.616
to three bounding boxes.

00:06:13.726 --> 00:06:15.826
The bounding boxes are in

00:06:15.826 --> 00:06:17.336
normalized coordinate space with

00:06:17.336 --> 00:06:18.546
respect to the image, the

00:06:18.546 --> 00:06:21.726
original image, and the lower

00:06:21.726 --> 00:06:23.316
left-hand corner is the origin

00:06:23.316 --> 00:06:25.066
point, much like bounding boxes

00:06:25.066 --> 00:06:26.826
returned by other algorithms in

00:06:26.826 --> 00:06:27.186
Vision.

00:06:27.956 --> 00:06:30.716
So, I wrote up a small method to

00:06:30.716 --> 00:06:32.266
show how to access the bounding

00:06:32.266 --> 00:06:33.146
boxes and use them.

00:06:33.916 --> 00:06:35.346
Here we have a

00:06:35.346 --> 00:06:37.386
VNSaliencyImageObservation, and

00:06:37.826 --> 00:06:40.156
all you have to do is access the

00:06:40.156 --> 00:06:41.706
salientObjects property on that

00:06:41.706 --> 00:06:43.876
observation, and you should get

00:06:44.036 --> 00:06:46.456
a list of bounding boxes, and

00:06:46.456 --> 00:06:48.296
you can access them like so.

00:06:48.296 --> 00:06:50.566
Okay. So, now that you know how

00:06:50.566 --> 00:06:53.136
to formulate a request and now

00:06:53.136 --> 00:06:54.906
that you know what saliency is,

00:06:55.686 --> 00:06:57.306
let's get into some of the use

00:06:57.306 --> 00:06:57.736
cases.

00:06:58.346 --> 00:07:02.606
First, for a bit of fun, you can

00:07:02.816 --> 00:07:05.816
use saliency as a graphical mask

00:07:06.076 --> 00:07:07.056
to edit your photos with.

00:07:07.056 --> 00:07:09.496
So, here you have the heatmaps.

00:07:10.216 --> 00:07:14.306
On the left-hand side, I've

00:07:14.306 --> 00:07:15.886
desaturated all the nonsalient

00:07:15.886 --> 00:07:17.716
regions, and on the right-hand

00:07:17.716 --> 00:07:19.416
side, I've added a Gaussian blur

00:07:19.416 --> 00:07:20.636
to all the nonsalient regions.

00:07:21.006 --> 00:07:22.336
It really makes the subjects

00:07:22.336 --> 00:07:22.576
pop.

00:07:25.536 --> 00:07:27.246
Another use case of saliency is

00:07:27.246 --> 00:07:28.396
you can enhance your photo

00:07:28.396 --> 00:07:29.156
viewing experience.

00:07:30.106 --> 00:07:31.666
So, let's say that you're at

00:07:31.666 --> 00:07:32.036
home.

00:07:32.036 --> 00:07:33.846
You're sitting on the couch, and

00:07:33.946 --> 00:07:35.236
either your TV or your computer

00:07:35.236 --> 00:07:38.216
has gone into standby mode, and

00:07:38.216 --> 00:07:39.066
it's going through your photo

00:07:39.066 --> 00:07:39.906
library.

00:07:40.336 --> 00:07:41.706
A lot of times, these

00:07:41.706 --> 00:07:44.446
photo-showing algorithms can be

00:07:44.446 --> 00:07:45.306
a little bit awkward.

00:07:45.306 --> 00:07:47.006
They zoom into seemingly random

00:07:47.006 --> 00:07:50.506
parts of the image, and it's not

00:07:50.506 --> 00:07:51.346
always what you expect.

00:07:52.086 --> 00:07:53.546
But with saliency, you always

00:07:53.546 --> 00:07:56.026
know where the subjects are, so

00:07:56.026 --> 00:07:57.036
you can get a more

00:07:57.136 --> 00:07:59.366
documentary-like effect like

00:08:02.476 --> 00:08:02.576
this.

00:08:02.766 --> 00:08:04.616
Finally, saliency works really

00:08:04.616 --> 00:08:05.656
great with other vision

00:08:05.656 --> 00:08:06.206
algorithms.

00:08:07.486 --> 00:08:09.246
Let's say we have an image, and

00:08:09.246 --> 00:08:11.236
we want to classify the objects

00:08:11.236 --> 00:08:11.806
in the image.

00:08:12.966 --> 00:08:14.426
We can run objectness based

00:08:14.426 --> 00:08:16.226
saliency to pick up on the

00:08:16.226 --> 00:08:19.196
objects in the image, crop the

00:08:19.196 --> 00:08:21.266
image to the bounding boxes

00:08:21.266 --> 00:08:22.456
returned by objectness based

00:08:22.456 --> 00:08:25.006
saliency, and run these crops

00:08:25.366 --> 00:08:26.816
through the algorithm through a

00:08:26.886 --> 00:08:28.736
image classification algorithm

00:08:29.106 --> 00:08:31.286
to find out what the objects

00:08:31.286 --> 00:08:31.536
are.

00:08:32.316 --> 00:08:33.996
So, not only do you know where

00:08:34.285 --> 00:08:35.576
they are in the image because of

00:08:35.576 --> 00:08:36.885
the bounding boxes, but it

00:08:36.885 --> 00:08:39.905
allows you the hone in on what

00:08:39.905 --> 00:08:42.035
the objects are by just picking

00:08:42.035 --> 00:08:43.366
out the crops that have those

00:08:43.366 --> 00:08:44.976
objects in it.

00:08:45.286 --> 00:08:46.756
Now, you can already classify

00:08:46.756 --> 00:08:48.996
things with Core ML, but this

00:08:48.996 --> 00:08:50.606
year, Vision has new image

00:08:50.606 --> 00:08:52.296
classification technique that

00:08:52.296 --> 00:08:54.126
Rohan will now present to you.

00:08:55.516 --> 00:09:02.736
[ Applause ]

00:09:03.236 --> 00:09:04.696
>> Good morning.

00:09:05.136 --> 00:09:06.806
My name is Rohan Chandra, and

00:09:06.806 --> 00:09:08.176
I'm a researcher on the Vision

00:09:08.176 --> 00:09:08.476
Team.

00:09:09.266 --> 00:09:10.846
Today, I'm going to be talking

00:09:10.846 --> 00:09:12.106
about some of the new image

00:09:12.106 --> 00:09:13.476
classification requests we're

00:09:13.476 --> 00:09:14.926
introducing to the Vision API

00:09:15.126 --> 00:09:15.606
this year.

00:09:16.806 --> 00:09:18.856
Now, image classification as a

00:09:18.856 --> 00:09:20.626
task is fundamentally meant to

00:09:20.626 --> 00:09:22.096
answer the question, what are

00:09:22.096 --> 00:09:23.406
the objects that appear in my

00:09:23.406 --> 00:09:23.916
image.

00:09:25.066 --> 00:09:26.546
Many of you will already be

00:09:26.546 --> 00:09:27.386
familiar with image

00:09:27.386 --> 00:09:28.206
classification.

00:09:28.626 --> 00:09:30.146
You may have used Create ML or

00:09:30.146 --> 00:09:31.776
Core ML to train your own

00:09:31.776 --> 00:09:33.306
classification networks on your

00:09:33.356 --> 00:09:35.366
own data as we showed in the

00:09:35.366 --> 00:09:36.846
Vision with Core ML talk last

00:09:36.846 --> 00:09:37.046
year.

00:09:38.156 --> 00:09:39.306
Others of you may have been

00:09:39.356 --> 00:09:40.456
interested in image

00:09:40.456 --> 00:09:41.636
classification but felt you

00:09:41.636 --> 00:09:43.246
lacked the resources or the

00:09:43.246 --> 00:09:44.696
expertise to develop your own

00:09:44.696 --> 00:09:45.116
networks.

00:09:45.896 --> 00:09:48.716
In practice, developing a

00:09:48.716 --> 00:09:50.026
large-scale classification

00:09:50.026 --> 00:09:51.576
network from scratch can take

00:09:51.676 --> 00:09:53.076
millions of images to annotate,

00:09:53.596 --> 00:09:55.246
thousands of hours to train, and

00:09:55.246 --> 00:09:56.406
very specialized domain

00:09:56.406 --> 00:09:57.566
expertise to develop.

00:09:58.736 --> 00:10:00.606
We here at Apple have already

00:10:00.606 --> 00:10:02.296
gone through this process, and

00:10:02.336 --> 00:10:03.646
so we wanted to share our

00:10:03.646 --> 00:10:05.206
large-scale, on-device

00:10:05.256 --> 00:10:06.706
classification network with you

00:10:07.176 --> 00:10:08.206
so that you can leverage this

00:10:08.206 --> 00:10:09.936
technology without needing to

00:10:09.936 --> 00:10:11.396
invest a huge amount of time or

00:10:11.396 --> 00:10:12.816
resources into developing it

00:10:12.816 --> 00:10:13.306
yourself.

00:10:14.296 --> 00:10:16.116
We've also strived to put tools

00:10:16.116 --> 00:10:17.346
in the API to help you

00:10:17.346 --> 00:10:19.216
contextualize and understand the

00:10:19.216 --> 00:10:20.646
results in a way that makes

00:10:20.646 --> 00:10:21.806
sense for your application.

00:10:22.986 --> 00:10:24.116
Now, the network we're talking

00:10:24.116 --> 00:10:25.796
about exposing here is in fact

00:10:25.846 --> 00:10:27.326
the same network we ourselves

00:10:27.326 --> 00:10:28.686
use to power the photo search

00:10:28.686 --> 00:10:29.306
experience.

00:10:30.096 --> 00:10:31.136
This is a network we've

00:10:31.136 --> 00:10:32.646
developed specifically to run

00:10:32.646 --> 00:10:34.256
efficiently on device without

00:10:34.256 --> 00:10:35.516
requiring any service side

00:10:35.576 --> 00:10:36.236
processing.

00:10:36.956 --> 00:10:38.446
We've also developed it to

00:10:38.446 --> 00:10:39.746
identify over a thousand

00:10:39.746 --> 00:10:41.136
different categories of objects.

00:10:42.516 --> 00:10:44.396
Now, it's also important to note

00:10:44.516 --> 00:10:45.776
that this is a multi-label

00:10:45.776 --> 00:10:47.876
network capable of identifying

00:10:48.236 --> 00:10:49.936
multiple objects in a single

00:10:49.936 --> 00:10:51.986
image, in contrast to more

00:10:51.986 --> 00:10:53.976
typical mono-label networks that

00:10:53.976 --> 00:10:55.496
try to focus on identifying a

00:10:55.496 --> 00:10:57.666
single large central object in

00:10:57.666 --> 00:10:58.226
an image.

00:10:59.586 --> 00:11:01.076
Now, as I talk about this new

00:11:01.076 --> 00:11:03.226
classification API, I think one

00:11:03.226 --> 00:11:04.186
of the first questions that

00:11:04.186 --> 00:11:05.786
comes to mind is what are the

00:11:05.786 --> 00:11:06.856
objects it can actually

00:11:06.856 --> 00:11:07.426
identify?

00:11:08.336 --> 00:11:09.916
Well, the set of objects that a

00:11:09.916 --> 00:11:11.536
classifier can predict is known

00:11:11.536 --> 00:11:12.426
as the taxonomy.

00:11:13.436 --> 00:11:15.106
The taxonomy has a hierarchical

00:11:15.106 --> 00:11:16.456
structure with directional

00:11:16.456 --> 00:11:17.936
relationships between classes.

00:11:19.006 --> 00:11:20.606
These relationships are based

00:11:20.606 --> 00:11:22.226
upon shared semantic meaning.

00:11:22.916 --> 00:11:24.866
For instance, a class like dog

00:11:24.966 --> 00:11:26.436
might have children like Beagle,

00:11:26.616 --> 00:11:28.176
Poodle, Husky, and other

00:11:28.176 --> 00:11:29.156
sub-breeds of dogs.

00:11:30.256 --> 00:11:31.866
In this sense, a parent class

00:11:31.896 --> 00:11:33.226
tends to be more general while

00:11:33.226 --> 00:11:34.866
child classes are more specific

00:11:34.866 --> 00:11:36.106
instances of their parent.

00:11:37.046 --> 00:11:38.466
You can of course see the entire

00:11:38.466 --> 00:11:39.816
taxonomy using

00:11:40.006 --> 00:11:40.926
ImageRequest.known

00:11:40.926 --> 00:11:41.756
Classifications.

00:11:43.136 --> 00:11:44.416
Now, when we constructed the

00:11:44.416 --> 00:11:46.276
taxonomy, we had a few specific

00:11:46.276 --> 00:11:47.226
rules that we applied.

00:11:48.656 --> 00:11:49.816
The first is that the classes

00:11:49.856 --> 00:11:51.516
must be visually identifiable.

00:11:52.836 --> 00:11:54.456
That is, we avoid more abstract

00:11:54.456 --> 00:11:55.756
concepts like holiday or

00:11:55.756 --> 00:11:56.306
festival.

00:11:57.426 --> 00:11:59.196
We also avoid any classes that

00:11:59.196 --> 00:11:59.876
might be considered

00:11:59.876 --> 00:12:01.746
controversial or offensive as

00:12:01.746 --> 00:12:03.066
well as those to do with proper

00:12:03.066 --> 00:12:04.866
names, nouns, excuse me,

00:12:04.866 --> 00:12:06.256
adjectives, or basic shapes.

00:12:07.346 --> 00:12:09.266
Finally, we omit occupations,

00:12:09.526 --> 00:12:10.686
and this might seem odd at

00:12:10.686 --> 00:12:11.076
first.

00:12:11.716 --> 00:12:12.856
But consider the range of

00:12:12.856 --> 00:12:14.266
answers you'd get if we asked

00:12:14.266 --> 00:12:15.226
something like what does an

00:12:15.226 --> 00:12:16.216
engineer look like.

00:12:16.556 --> 00:12:18.496
There probably isn't a single

00:12:18.496 --> 00:12:19.546
concise description you could

00:12:19.546 --> 00:12:20.856
give that would apply to every

00:12:20.856 --> 00:12:22.416
engineer aside from sleep

00:12:22.416 --> 00:12:23.716
deprived and usually glued to a

00:12:23.716 --> 00:12:24.476
computer screen.

00:12:25.556 --> 00:12:26.646
Let's take a look at the code

00:12:26.646 --> 00:12:27.816
you need to use in order to

00:12:27.816 --> 00:12:28.706
classify an image.

00:12:28.706 --> 00:12:31.636
So, as usual, you form an

00:12:31.636 --> 00:12:32.896
ImageRequestHandler to your

00:12:32.896 --> 00:12:33.526
source image.

00:12:34.196 --> 00:12:35.026
You then perform the

00:12:35.026 --> 00:12:36.836
VNClassifyImageRequest and

00:12:36.836 --> 00:12:38.066
retrieve your observations.

00:12:38.746 --> 00:12:40.036
Now, in this case, you actually

00:12:40.036 --> 00:12:41.406
get an array of observations,

00:12:41.696 --> 00:12:42.976
one for every class in the

00:12:42.976 --> 00:12:44.976
taxonomy and its associated

00:12:44.976 --> 00:12:45.456
confidence.

00:12:46.366 --> 00:12:47.876
In a mono-label problem, you'd

00:12:47.876 --> 00:12:48.946
probably expect that these

00:12:48.946 --> 00:12:50.906
probabilities sum up to 1, but

00:12:50.906 --> 00:12:51.896
this is a multi-label

00:12:51.936 --> 00:12:53.876
classification network, and each

00:12:53.876 --> 00:12:55.376
prediction is an independent

00:12:55.376 --> 00:12:56.756
confidence associated with a

00:12:56.756 --> 00:12:57.416
particular class.

00:12:58.416 --> 00:13:00.096
As such, they won't sum to 1,

00:13:00.406 --> 00:13:01.466
and they're meant to be compared

00:13:01.466 --> 00:13:02.956
within the same class, not

00:13:02.956 --> 00:13:04.256
across different classes.

00:13:04.496 --> 00:13:06.196
So we can't simply take the max

00:13:06.196 --> 00:13:07.486
amongst them in order to

00:13:07.486 --> 00:13:08.816
determine our final prediction.

00:13:09.696 --> 00:13:11.166
You might be wondering then, how

00:13:11.166 --> 00:13:12.806
do I deal with so many classes

00:13:12.856 --> 00:13:13.756
and so many numbers.

00:13:14.516 --> 00:13:15.836
Well, there are a few key tools

00:13:15.836 --> 00:13:17.066
in the API that we've

00:13:17.066 --> 00:13:18.186
implemented to help you make

00:13:18.256 --> 00:13:19.086
sense of the result.

00:13:20.376 --> 00:13:22.026
Now, in order to talk about

00:13:22.086 --> 00:13:23.916
these tools in the API, we first

00:13:23.916 --> 00:13:25.586
need to define some basic terms.

00:13:26.266 --> 00:13:28.716
The first is when you get a

00:13:28.716 --> 00:13:30.576
confidence for a class, we

00:13:30.576 --> 00:13:31.936
typically compare that to a

00:13:31.936 --> 00:13:33.606
class-specific threshold, which

00:13:33.606 --> 00:13:34.866
we refer to as an operating

00:13:34.866 --> 00:13:35.186
point.

00:13:35.976 --> 00:13:37.976
If the class confidence is above

00:13:37.976 --> 00:13:39.466
the threshold, then we say that

00:13:39.466 --> 00:13:40.796
class is present in the image.

00:13:41.266 --> 00:13:42.996
If the class confidence is below

00:13:42.996 --> 00:13:44.516
the class threshold, then we say

00:13:44.516 --> 00:13:46.316
that object is not present in

00:13:46.316 --> 00:13:46.756
the image.

00:13:47.656 --> 00:13:49.316
In this sense, we want to pick

00:13:49.316 --> 00:13:50.866
thresholds such that objects

00:13:50.866 --> 00:13:52.346
with the target class typically

00:13:52.346 --> 00:13:53.456
have a confidence higher than

00:13:53.456 --> 00:13:55.646
the threshold, and images

00:13:55.736 --> 00:13:56.886
without the target class

00:13:57.026 --> 00:13:58.556
typically have a score lower

00:13:58.556 --> 00:13:59.406
than the threshold.

00:14:00.376 --> 00:14:02.216
However, machine learning is not

00:14:02.216 --> 00:14:03.866
infallible, and there will be

00:14:03.866 --> 00:14:05.636
instances where the network is

00:14:05.636 --> 00:14:07.136
unsure and the confidence is

00:14:07.136 --> 00:14:08.276
proportionally lower.

00:14:09.056 --> 00:14:10.496
This can happen when objects are

00:14:10.536 --> 00:14:12.156
office gated, appear in odd

00:14:12.156 --> 00:14:14.006
lighting or at odd angles, for

00:14:14.006 --> 00:14:14.506
instance.

00:14:15.176 --> 00:14:15.866
So how do we pick our

00:14:15.866 --> 00:14:16.426
thresholds?

00:14:17.596 --> 00:14:18.306
Well, there are essentially

00:14:18.436 --> 00:14:19.756
three different regimes we can

00:14:19.756 --> 00:14:20.926
be in depending on our choice of

00:14:20.926 --> 00:14:22.266
threshold that yield three

00:14:22.266 --> 00:14:23.326
different kinds of searches.

00:14:24.416 --> 00:14:25.426
To make this a little more

00:14:25.426 --> 00:14:26.816
concrete, let's say I have a

00:14:26.876 --> 00:14:28.696
library of images for which I've

00:14:28.806 --> 00:14:30.346
already performed classification

00:14:30.386 --> 00:14:31.246
and stored the results.

00:14:32.026 --> 00:14:33.296
Let's say in this particular

00:14:33.296 --> 00:14:35.016
case I'm looking for images of

00:14:35.016 --> 00:14:35.866
motorcycles.

00:14:36.726 --> 00:14:37.626
Now, I want to pick my

00:14:37.626 --> 00:14:39.466
thresholds such that images with

00:14:39.466 --> 00:14:40.776
motorcycles typically have a

00:14:40.776 --> 00:14:41.826
confidence higher than this

00:14:41.876 --> 00:14:43.906
threshold and images without

00:14:43.906 --> 00:14:45.306
motorcycles typically have a

00:14:45.306 --> 00:14:46.946
score lower than this threshold.

00:14:47.596 --> 00:14:49.186
So, what happens if I just pick

00:14:49.186 --> 00:14:50.006
a low threshold.

00:14:50.826 --> 00:14:52.286
As you can see behind me, when I

00:14:52.286 --> 00:14:54.246
apply this low threshold, I do

00:14:54.246 --> 00:14:55.356
in fact get my motorcycle

00:14:55.356 --> 00:14:56.926
images, but I'm also getting

00:14:56.926 --> 00:14:58.696
these images of mopeds in the

00:14:58.696 --> 00:14:59.246
bottom right.

00:14:59.246 --> 00:15:01.106
And if my users are motorcycle

00:15:01.106 --> 00:15:02.106
enthusiasts, they might be a

00:15:02.106 --> 00:15:03.386
little annoyed with that result.

00:15:04.516 --> 00:15:06.186
When we talk about a search that

00:15:06.186 --> 00:15:07.806
tries to maximize the percentage

00:15:07.806 --> 00:15:09.346
of the target class retrieved

00:15:09.346 --> 00:15:11.156
amongst the entire library, and

00:15:11.246 --> 00:15:12.656
isn't as concerned with these

00:15:12.656 --> 00:15:14.196
missed predictions where we say

00:15:14.196 --> 00:15:15.596
the motorcycle is present when

00:15:15.596 --> 00:15:16.266
it actually isn't.

00:15:16.656 --> 00:15:18.036
We are typically talking about a

00:15:18.036 --> 00:15:19.076
high recall search.

00:15:20.156 --> 00:15:22.236
Now, I could maximize recall by

00:15:22.236 --> 00:15:23.806
simply returning as many images

00:15:23.806 --> 00:15:25.086
as possible, but I would get a

00:15:25.176 --> 00:15:26.526
huge number of these false

00:15:26.526 --> 00:15:27.516
predictions where I say my

00:15:27.516 --> 00:15:28.806
target class is present when it

00:15:28.806 --> 00:15:30.436
actually isn't, and so we need

00:15:30.436 --> 00:15:31.666
to find a more balanced point of

00:15:31.666 --> 00:15:32.676
recall to operate at.

00:15:33.586 --> 00:15:34.956
Let's take a look at how I need

00:15:34.956 --> 00:15:36.596
to change my code in order to

00:15:36.596 --> 00:15:38.256
perform this high recall search.

00:15:39.716 --> 00:15:41.566
So, here I have the same code

00:15:41.566 --> 00:15:43.466
snippet as before, but this time

00:15:43.606 --> 00:15:45.516
I'm performing a filtering with

00:15:45.516 --> 00:15:47.206
hasMinimumPrecision and a

00:15:47.206 --> 00:15:48.666
specific recall value.

00:15:49.496 --> 00:15:51.526
For each observation in my array

00:15:51.526 --> 00:15:53.646
of observations, the filter only

00:15:53.646 --> 00:15:55.166
retains it if the confidence

00:15:55.166 --> 00:15:56.336
associated with the class

00:15:56.656 --> 00:15:57.946
achieves the level of recall

00:15:57.946 --> 00:15:58.896
that I specified.

00:15:59.646 --> 00:16:01.526
Now, the actual operating point

00:16:01.526 --> 00:16:02.856
needed to determine this is

00:16:02.856 --> 00:16:04.196
going to be different for every

00:16:04.196 --> 00:16:06.106
class, and it's something we've

00:16:06.106 --> 00:16:07.756
determined based on our internal

00:16:07.756 --> 00:16:08.986
tests of how the network

00:16:08.986 --> 00:16:10.476
performs on every class in the

00:16:10.476 --> 00:16:11.116
taxonomy.

00:16:12.086 --> 00:16:13.736
However, the filter handles this

00:16:13.736 --> 00:16:14.696
for you automatically.

00:16:15.026 --> 00:16:16.756
All you need to do is specify

00:16:16.756 --> 00:16:18.016
the level of recall you want to

00:16:18.016 --> 00:16:18.696
operate at.

00:16:19.736 --> 00:16:20.686
So, we talked about a high

00:16:20.686 --> 00:16:22.426
recall search here, but what if

00:16:22.426 --> 00:16:24.196
I have an application that can't

00:16:24.196 --> 00:16:25.646
tolerate these false predictions

00:16:25.646 --> 00:16:26.786
where I'm saying motorcycles are

00:16:26.786 --> 00:16:27.696
present when they're not.

00:16:28.106 --> 00:16:29.746
That is, I want to be absolutely

00:16:29.746 --> 00:16:31.456
sure that the images I retrieve

00:16:31.626 --> 00:16:32.566
actually do contain a

00:16:32.566 --> 00:16:33.246
motorcycle.

00:16:34.026 --> 00:16:35.326
Well, let's come back to our

00:16:35.326 --> 00:16:36.956
library of images then and see

00:16:36.956 --> 00:16:38.236
what would happen if we applied

00:16:38.266 --> 00:16:39.186
the higher threshold.

00:16:39.956 --> 00:16:41.406
As you can see behind me, when I

00:16:41.406 --> 00:16:43.186
apply my high threshold, I do in

00:16:43.186 --> 00:16:44.776
fact only get motorcycle images,

00:16:45.106 --> 00:16:46.506
but I get far fewer images

00:16:46.506 --> 00:16:47.166
overall.

00:16:48.536 --> 00:16:50.176
When we talk about a search that

00:16:50.176 --> 00:16:51.816
tries to maximize the percentage

00:16:51.876 --> 00:16:53.506
of the target class amongst the

00:16:53.506 --> 00:16:55.586
retrieved images and isn't as

00:16:55.586 --> 00:16:57.146
concerned with overlooking some

00:16:57.146 --> 00:16:58.486
of the more ambiguous images

00:16:58.536 --> 00:16:59.696
that actually do contain the

00:16:59.696 --> 00:17:01.216
target class, we are typically

00:17:01.216 --> 00:17:03.296
talking about a high precision

00:17:03.296 --> 00:17:03.736
search.

00:17:04.445 --> 00:17:06.296
Again, like with high recall, we

00:17:06.296 --> 00:17:07.316
need to find a more balanced

00:17:07.316 --> 00:17:08.656
operating point where I have an

00:17:08.656 --> 00:17:10.435
acceptable likelihood about my

00:17:10.435 --> 00:17:11.486
target class appearing in my

00:17:11.486 --> 00:17:12.886
results, but I'm not getting too

00:17:12.986 --> 00:17:13.756
few images.

00:17:14.816 --> 00:17:16.116
So, let's take a look at how I

00:17:16.116 --> 00:17:17.816
need to modify my code in order

00:17:17.816 --> 00:17:19.175
to perform this high precision

00:17:19.175 --> 00:17:19.586
search.

00:17:20.175 --> 00:17:22.546
So here's the same code snippet,

00:17:22.715 --> 00:17:24.215
but this time my filtering is

00:17:24.215 --> 00:17:26.366
done with hasMinimumRecall and a

00:17:26.366 --> 00:17:27.935
precision value I've specified.

00:17:28.806 --> 00:17:30.636
Again, I only retain the

00:17:30.636 --> 00:17:32.186
observation if the confidence

00:17:32.186 --> 00:17:33.786
associated with it achieves the

00:17:33.786 --> 00:17:34.736
level of precision that I

00:17:34.736 --> 00:17:35.406
specified.

00:17:36.166 --> 00:17:37.546
The actual threshold needed for

00:17:37.546 --> 00:17:38.806
this is going to be different

00:17:38.806 --> 00:17:40.286
for every class, but the filter

00:17:40.286 --> 00:17:41.646
handles that for me

00:17:41.646 --> 00:17:42.256
automatically.

00:17:42.626 --> 00:17:44.056
All I need to do is tell it the

00:17:44.056 --> 00:17:45.026
level of precision I want to

00:17:45.026 --> 00:17:45.616
operate at.

00:17:46.936 --> 00:17:48.576
So we've talked about two

00:17:48.576 --> 00:17:49.846
different extremes here, one of

00:17:49.906 --> 00:17:51.236
high recall and one of high

00:17:51.236 --> 00:17:53.516
precision, but in practice, it

00:17:53.516 --> 00:17:55.216
can be better to find a balanced

00:17:55.416 --> 00:17:57.076
tradeoff between the two.

00:17:58.096 --> 00:17:59.496
So, let's see how we can go

00:17:59.496 --> 00:18:00.896
about doing that, and in order

00:18:00.896 --> 00:18:01.966
to understand what's happening,

00:18:02.246 --> 00:18:03.266
I first need to introduce

00:18:03.396 --> 00:18:04.836
something known as the precision

00:18:04.836 --> 00:18:05.646
and recall curve.

00:18:06.346 --> 00:18:08.976
So, in practice, there is a

00:18:08.976 --> 00:18:10.406
tradeoff to be made where

00:18:10.406 --> 00:18:11.986
increasing one of precision and

00:18:11.986 --> 00:18:13.846
recall can lead to a decrease in

00:18:13.846 --> 00:18:14.196
the other.

00:18:14.646 --> 00:18:16.376
I can represent this tradeoff as

00:18:16.376 --> 00:18:17.956
a graph, where for each

00:18:18.026 --> 00:18:19.586
operating point I can compute

00:18:19.586 --> 00:18:20.936
the corresponding precision and

00:18:20.936 --> 00:18:21.336
recall.

00:18:22.016 --> 00:18:23.476
For instance, at the operating

00:18:23.506 --> 00:18:25.156
point at where I achieve a

00:18:25.156 --> 00:18:27.116
recall of 0.7, I find that I get

00:18:27.116 --> 00:18:28.456
a corresponding precision of

00:18:28.456 --> 00:18:29.436
0.74.

00:18:29.436 --> 00:18:31.886
I can compute this for a

00:18:31.886 --> 00:18:33.486
multitude of operating points in

00:18:33.486 --> 00:18:35.016
order to form my full curve.

00:18:36.246 --> 00:18:37.956
As I said before, I want to find

00:18:37.956 --> 00:18:39.846
a balance point along this curve

00:18:40.056 --> 00:18:41.056
that achieves the level of

00:18:41.056 --> 00:18:42.286
recall and precision that makes

00:18:42.286 --> 00:18:43.536
sense for my application.

00:18:44.486 --> 00:18:45.726
So let's see how I need to

00:18:45.756 --> 00:18:47.366
change my code in order to

00:18:47.366 --> 00:18:48.636
accomplish it and how the

00:18:48.636 --> 00:18:50.376
precision and recall curve plays

00:18:51.106 --> 00:18:52.146
into that.

00:18:52.286 --> 00:18:54.276
So here I have a filtering with

00:18:54.276 --> 00:18:55.896
hasMinimumPrecision where I'm

00:18:55.896 --> 00:18:57.546
specifying the minimum precision

00:18:57.546 --> 00:18:57.976
and a recall value.

00:18:58.416 --> 00:19:00.746
When I specify a

00:19:00.746 --> 00:19:02.486
MinimumPrecision, I'm actually

00:19:02.486 --> 00:19:04.286
selecting an area along the

00:19:04.286 --> 00:19:05.566
graph that I want to operate

00:19:05.566 --> 00:19:05.976
within.

00:19:06.866 --> 00:19:08.236
When I select a recall point

00:19:08.286 --> 00:19:10.226
with forRecall, I'm choosing a

00:19:10.296 --> 00:19:11.896
point along the curve that will

00:19:11.896 --> 00:19:13.006
be my operating point.

00:19:13.876 --> 00:19:15.516
Now, if the operating point is

00:19:15.576 --> 00:19:16.666
in the valid region that I

00:19:16.666 --> 00:19:18.166
selected, then that is the

00:19:18.166 --> 00:19:19.616
threshold that the filter will

00:19:19.616 --> 00:19:20.916
apply when looking at that

00:19:20.916 --> 00:19:21.626
particular class.

00:19:22.586 --> 00:19:24.076
If the operating point is not in

00:19:24.076 --> 00:19:25.636
the valid region, then there is

00:19:25.636 --> 00:19:27.046
no operating point that meets

00:19:27.046 --> 00:19:28.536
the constraints I stated, and

00:19:28.536 --> 00:19:29.606
the class will always be

00:19:29.606 --> 00:19:30.816
filtered out of my results.

00:19:31.856 --> 00:19:33.556
In this sense, all you need to

00:19:33.556 --> 00:19:35.166
do is provide the level of

00:19:35.166 --> 00:19:36.476
precision and recall that you

00:19:36.476 --> 00:19:37.686
want to operate at, and the

00:19:37.686 --> 00:19:38.626
filter will determine the

00:19:38.626 --> 00:19:39.916
necessary thresholds for you

00:19:40.076 --> 00:19:40.796
automatically.

00:19:41.386 --> 00:19:44.906
So, to summarize, the

00:19:44.906 --> 00:19:46.226
observation I get back when

00:19:46.226 --> 00:19:47.656
performing image classification

00:19:47.986 --> 00:19:49.106
is actually an array of

00:19:49.106 --> 00:19:50.656
observations, one for every

00:19:50.656 --> 00:19:51.736
class in the taxonomy.

00:19:52.876 --> 00:19:54.106
Because this is a multi-label

00:19:54.206 --> 00:19:55.566
problem, the confidences will

00:19:55.566 --> 00:19:56.456
not sum to 1.

00:19:57.166 --> 00:19:58.716
Instead, we have independent

00:19:58.716 --> 00:20:00.376
confidence values, one for every

00:20:00.376 --> 00:20:02.616
class between 0 to 1, and we

00:20:02.616 --> 00:20:04.096
need to understand precision and

00:20:04.096 --> 00:20:05.656
recall and how they apply to our

00:20:05.656 --> 00:20:07.676
specific use case in order to

00:20:07.676 --> 00:20:08.606
apply a filtering with

00:20:08.606 --> 00:20:10.226
hasMinimumPrecision or

00:20:10.226 --> 00:20:11.856
hasMinimumRecall that makes

00:20:11.856 --> 00:20:13.166
sense for our application.

00:20:13.606 --> 00:20:15.846
So, that concludes the portion

00:20:16.146 --> 00:20:17.496
on image classification.

00:20:17.986 --> 00:20:19.416
I'd like to switch gears and

00:20:19.416 --> 00:20:20.676
talk about a related topic,

00:20:21.226 --> 00:20:22.866
Image -- excuse me.

00:20:23.176 --> 00:20:23.976
Image Similarity.

00:20:26.216 --> 00:20:27.216
When we talk about Image

00:20:27.216 --> 00:20:29.076
Similarity, what we really mean

00:20:29.076 --> 00:20:30.406
is a method to describe the

00:20:30.406 --> 00:20:32.476
content of an image and another

00:20:32.476 --> 00:20:34.286
method to compare those

00:20:34.316 --> 00:20:34.956
descriptions.

00:20:36.116 --> 00:20:38.106
The most basic way in which I

00:20:38.106 --> 00:20:39.676
can describe the contents of an

00:20:39.676 --> 00:20:41.436
image is using the source pixels

00:20:41.436 --> 00:20:41.986
themselves.

00:20:43.536 --> 00:20:45.286
That is, I can search for other

00:20:45.286 --> 00:20:47.076
images that have close to or

00:20:47.076 --> 00:20:49.196
exactly the same pixel values

00:20:49.256 --> 00:20:50.076
and retrieve them.

00:20:51.146 --> 00:20:52.136
If I did a search in this

00:20:52.136 --> 00:20:54.046
fashion, however, it's extremely

00:20:54.046 --> 00:20:55.886
fragile, and it's easily fooled

00:20:55.986 --> 00:20:57.646
by small changes like rotations

00:20:57.916 --> 00:20:59.466
or lighting augmentations that

00:20:59.466 --> 00:21:00.656
drastically change the pixel

00:21:00.656 --> 00:21:02.256
values but not the semantic

00:21:02.256 --> 00:21:03.376
content in the image.

00:21:04.266 --> 00:21:05.536
What I really want is a more

00:21:05.536 --> 00:21:07.246
high-level description of what

00:21:07.246 --> 00:21:08.836
the content of the image is,

00:21:08.836 --> 00:21:09.926
perhaps something like natural

00:21:09.926 --> 00:21:10.426
language.

00:21:10.936 --> 00:21:13.126
I could make use of the image

00:21:13.126 --> 00:21:14.696
classification API I was

00:21:14.696 --> 00:21:16.426
describing previously in order

00:21:16.426 --> 00:21:17.636
to extract a set of words that

00:21:17.636 --> 00:21:18.626
describe my image.

00:21:19.446 --> 00:21:20.936
I could then retrieve other

00:21:20.936 --> 00:21:22.426
images with a similar set of

00:21:22.426 --> 00:21:23.316
classifications.

00:21:23.756 --> 00:21:25.106
I might even combine this with

00:21:25.106 --> 00:21:26.256
something like word vectors to

00:21:26.326 --> 00:21:27.716
account for similar but not

00:21:27.716 --> 00:21:29.386
exactly matching words like cat

00:21:29.386 --> 00:21:30.356
and kitten.

00:21:30.696 --> 00:21:31.896
Well, if I performed a search

00:21:31.896 --> 00:21:33.556
like this, I might get similar

00:21:33.556 --> 00:21:35.046
objects in a very general sense,

00:21:35.446 --> 00:21:36.406
but the way in which those

00:21:36.406 --> 00:21:37.556
objects appear and the

00:21:37.556 --> 00:21:39.066
relationships between them could

00:21:39.066 --> 00:21:40.026
be very different.

00:21:40.966 --> 00:21:43.036
As well, I would be limited by

00:21:43.036 --> 00:21:44.586
the taxonomy of my classifier.

00:21:45.386 --> 00:21:46.736
That is, any object that

00:21:46.736 --> 00:21:48.426
appeared in my image that wasn't

00:21:48.426 --> 00:21:49.756
in my classification networks

00:21:49.756 --> 00:21:51.686
taxonomy couldn't be expressed

00:21:51.786 --> 00:21:52.876
in a search like this.

00:21:54.216 --> 00:21:55.486
What I really want is a

00:21:55.486 --> 00:21:56.966
high-level description of the

00:21:56.996 --> 00:21:58.176
objects that appear in the image

00:21:58.386 --> 00:21:59.856
that isn't fixated on the exact

00:21:59.856 --> 00:22:01.096
pixel values but still cares

00:22:01.096 --> 00:22:01.556
about them.

00:22:02.256 --> 00:22:04.006
I also want this to apply to any

00:22:04.006 --> 00:22:05.896
natural image and not just those

00:22:05.896 --> 00:22:07.266
within a specific taxonomy.

00:22:07.746 --> 00:22:09.946
As it turns out, this kind of

00:22:09.946 --> 00:22:11.756
representation learning is

00:22:11.756 --> 00:22:12.676
something that's naturally

00:22:12.676 --> 00:22:14.016
engendered in our classification

00:22:14.016 --> 00:22:15.296
network as part of its training

00:22:15.296 --> 00:22:15.786
process.

00:22:16.786 --> 00:22:18.736
The upper layers of the network

00:22:18.976 --> 00:22:20.356
contain all of the salient

00:22:20.356 --> 00:22:22.166
information necessary to perform

00:22:22.166 --> 00:22:23.686
classification while discarding

00:22:23.756 --> 00:22:25.466
any redundant or unnecessary

00:22:25.466 --> 00:22:26.726
information that doesn't aid it

00:22:26.726 --> 00:22:28.336
in that task.

00:22:28.386 --> 00:22:29.596
We can make use of these upper

00:22:29.596 --> 00:22:31.136
layers then to act as our

00:22:31.136 --> 00:22:32.456
feature descriptor, and it's

00:22:32.456 --> 00:22:33.516
something we refer to as the

00:22:33.516 --> 00:22:34.156
feature print.

00:22:35.196 --> 00:22:36.706
Now, the feature print is a

00:22:36.706 --> 00:22:37.946
vector that describes the

00:22:37.946 --> 00:22:39.666
content of the image that isn't

00:22:39.666 --> 00:22:40.696
constrained to a particular

00:22:40.696 --> 00:22:42.086
taxonomy, even the one that the

00:22:42.086 --> 00:22:43.146
classification network was

00:22:43.146 --> 00:22:43.636
trained on.

00:22:43.896 --> 00:22:45.106
It simply leverages what the

00:22:45.106 --> 00:22:46.486
network has learned about images

00:22:46.486 --> 00:22:47.736
during its training process.

00:22:48.816 --> 00:22:50.296
If we look at these pairs of

00:22:50.296 --> 00:22:51.696
images, we can compare how

00:22:51.696 --> 00:22:52.786
similar their feature prints

00:22:52.786 --> 00:22:54.236
are, and the smaller the value

00:22:54.236 --> 00:22:55.486
is, the more similar the two

00:22:55.486 --> 00:22:57.296
images are in a semantic sense.

00:22:58.046 --> 00:22:59.316
We can see that even though the

00:22:59.316 --> 00:23:00.556
two images of the cats are

00:23:00.556 --> 00:23:02.026
visually dissimilar, they have a

00:23:02.026 --> 00:23:03.506
much more similar feature print

00:23:03.746 --> 00:23:05.526
than the visually similar pairs

00:23:05.596 --> 00:23:06.636
of different animals.

00:23:07.136 --> 00:23:09.416
To make this a little more

00:23:09.416 --> 00:23:10.646
concrete, let's go through a

00:23:10.646 --> 00:23:11.686
specific example.

00:23:12.396 --> 00:23:13.476
Let's say I have the source

00:23:13.476 --> 00:23:15.186
image on screen, and I want to

00:23:15.186 --> 00:23:16.706
find other semantically similar

00:23:16.706 --> 00:23:17.476
images to it.

00:23:18.336 --> 00:23:19.656
I'm going to take a library of

00:23:19.656 --> 00:23:21.196
images and compute the feature

00:23:21.196 --> 00:23:22.896
print for each image and then

00:23:22.896 --> 00:23:24.476
retrieve those images with the

00:23:24.476 --> 00:23:26.296
most similar feature print to my

00:23:26.296 --> 00:23:26.986
source image.

00:23:27.746 --> 00:23:29.136
When I do it for this image of

00:23:29.136 --> 00:23:29.926
the gentleman in the coffee

00:23:29.926 --> 00:23:31.876
shop, I find I get other images

00:23:31.926 --> 00:23:33.176
of people in coffee shop and

00:23:33.176 --> 00:23:33.946
restaurant settings.

00:23:34.886 --> 00:23:36.266
If I focus on a crop of the

00:23:36.266 --> 00:23:38.246
newspaper, however, I get other

00:23:38.246 --> 00:23:39.586
images of newspapers.

00:23:40.246 --> 00:23:42.156
And if I focus on the teapot, I

00:23:42.236 --> 00:23:43.906
get other images of teapots.

00:23:45.326 --> 00:23:46.616
I'd like to now invite the

00:23:46.616 --> 00:23:48.616
Vision Team onstage to help me

00:23:48.616 --> 00:23:50.116
with a quick demonstration to

00:23:50.116 --> 00:23:51.546
expand a little more on how

00:23:51.546 --> 00:23:52.646
Image Similarity works.

00:23:54.516 --> 00:23:58.166
[ Applause ]

00:23:58.666 --> 00:23:59.466
>> Hello everyone.

00:23:59.886 --> 00:24:00.846
My name is Brett, and we have a

00:24:00.846 --> 00:24:02.046
really fun way to demonstrate

00:24:02.046 --> 00:24:03.546
Image Similarity for you today.

00:24:03.546 --> 00:24:05.286
We have very creatively called

00:24:05.286 --> 00:24:06.846
it the Image Similarity game.

00:24:07.876 --> 00:24:09.086
And here is how you play.

00:24:09.676 --> 00:24:11.066
You draw something on a piece of

00:24:11.066 --> 00:24:14.026
paper, then ask a few friends to

00:24:14.026 --> 00:24:15.166
re-create your original as close

00:24:15.166 --> 00:24:15.726
as possible.

00:24:16.296 --> 00:24:17.596
So I will start by drawing the

00:24:17.596 --> 00:24:17.976
original.

00:24:30.096 --> 00:24:33.496
Okay. Tap continue to scan it in

00:24:33.496 --> 00:24:33.976
as my original.

00:24:42.046 --> 00:24:42.976
And then save.

00:24:44.526 --> 00:24:46.456
Now, my team will act as

00:24:46.456 --> 00:24:47.886
contestants, and they will draw

00:24:48.156 --> 00:24:48.976
this as best as they can.

00:24:54.266 --> 00:24:55.206
Now while they're drawing, I

00:24:55.206 --> 00:24:57.316
should tell you that this sample

00:24:57.316 --> 00:24:58.476
app is currently available to

00:24:58.536 --> 00:25:00.576
you now on the developer

00:25:00.576 --> 00:25:04.356
documentation website as sample

00:25:04.356 --> 00:25:05.986
code, and also, we are using the

00:25:05.986 --> 00:25:07.586
Vision kit document scanner to

00:25:07.586 --> 00:25:08.886
scan in our drawings, and you

00:25:08.886 --> 00:25:09.966
can learn more about that at our

00:25:09.966 --> 00:25:11.226
text recognition session.

00:25:12.706 --> 00:25:15.076
Let's them give a few more

00:25:16.636 --> 00:25:16.886
seconds.

00:25:16.966 --> 00:25:19.896
Five, four, three, okay, I guess

00:25:19.926 --> 00:25:20.186
they're done.

00:25:20.266 --> 00:25:22.186
Okay. Let's bring them up and

00:25:22.186 --> 00:25:24.266
start scanning them in.

00:25:24.996 --> 00:25:26.256
Contestant number one.

00:25:31.046 --> 00:25:31.576
Pretty good [applause].

00:25:32.756 --> 00:25:33.406
That might be a winner.

00:25:33.406 --> 00:25:35.256
Let's see contestant number two.

00:25:38.296 --> 00:25:39.786
Still pretty good.

00:25:40.096 --> 00:25:40.506
Nicely done.

00:25:41.256 --> 00:25:43.256
[ Applause ]

00:25:43.496 --> 00:25:44.706
Contestant number three please.

00:25:48.056 --> 00:25:50.056
[ Laughter and Applause ]

00:25:50.096 --> 00:25:50.976
I think that's pretty good.

00:25:51.016 --> 00:25:52.586
[ Applause ]

00:25:52.586 --> 00:25:53.696
And contestant number four.

00:25:57.046 --> 00:25:58.136
Well, I don't know about that,

00:25:58.196 --> 00:26:00.836
but we'll see how it goes.

00:26:01.151 --> 00:26:03.151
[ Applause ]

00:26:03.286 --> 00:26:03.836
All right.

00:26:03.836 --> 00:26:07.006
So let's save those, and we find

00:26:07.006 --> 00:26:08.286
out that the winner is

00:26:08.286 --> 00:26:09.636
contestant number one.

00:26:09.636 --> 00:26:10.446
Congratulations.

00:26:11.016 --> 00:26:12.736
[ Applause ]

00:26:12.736 --> 00:26:14.716
Now I can swipe over, and we can

00:26:14.796 --> 00:26:16.076
see that the faces are more

00:26:16.076 --> 00:26:17.656
semantically similar that way.

00:26:18.336 --> 00:26:19.996
They are closer to the original

00:26:20.096 --> 00:26:21.376
while the tree is semantically

00:26:21.376 --> 00:26:22.416
different, it was much further

00:26:22.416 --> 00:26:22.686
away.

00:26:23.336 --> 00:26:24.426
And that is the Image Similarity

00:26:24.426 --> 00:26:25.376
game, and background check to

00:26:25.376 --> 00:26:25.726
Rohan.

00:26:26.516 --> 00:26:31.776
[ Applause ]

00:26:32.276 --> 00:26:32.876
>> Thanks everyone.

00:26:33.476 --> 00:26:34.736
I want to take a quick look at a

00:26:34.736 --> 00:26:35.846
snippet from that demo

00:26:35.846 --> 00:26:37.196
application to show how we

00:26:37.196 --> 00:26:37.946
determined the winning

00:26:37.946 --> 00:26:38.546
contestant.

00:26:39.686 --> 00:26:41.906
So here I have the portion of

00:26:41.906 --> 00:26:43.506
the code that compares each of

00:26:43.506 --> 00:26:45.286
the contestant's drawings

00:26:45.396 --> 00:26:46.796
feature print to Brett's

00:26:46.796 --> 00:26:47.666
drawing's feature print.

00:26:48.326 --> 00:26:49.006
Now, I extracted the

00:26:49.006 --> 00:26:50.276
contestant's feature print with

00:26:50.276 --> 00:26:51.396
a function we have defined in

00:26:51.396 --> 00:26:52.566
the application called

00:26:52.566 --> 00:26:54.466
featureprintObservationForImage.

00:26:55.186 --> 00:26:56.956
Once I have each feature print,

00:26:57.226 --> 00:26:58.666
I then need to determine how

00:26:58.666 --> 00:27:00.056
similar it was to the original

00:27:00.056 --> 00:27:01.846
drawing, and I can do that using

00:27:01.846 --> 00:27:03.246
computeDistance, which returns

00:27:03.246 --> 00:27:04.576
me a floating-point value.

00:27:05.166 --> 00:27:05.826
Now, the smaller the

00:27:05.826 --> 00:27:07.206
floating-point value, the more

00:27:07.206 --> 00:27:08.646
similar the two images are.

00:27:09.236 --> 00:27:10.456
And so, once I've determined

00:27:10.456 --> 00:27:11.636
this for every contestant, I

00:27:11.636 --> 00:27:13.116
simply need to sort them in

00:27:13.116 --> 00:27:14.276
order to determine the winner.

00:27:15.336 --> 00:27:17.006
Well, this concludes the portion

00:27:17.156 --> 00:27:18.156
on Image Similarity.

00:27:18.486 --> 00:27:19.536
I'd now like to hand the mic

00:27:19.536 --> 00:27:20.816
over to Sergey to talk about

00:27:20.816 --> 00:27:21.866
some of the changes coming to

00:27:21.866 --> 00:27:22.836
Face Technologies.

00:27:23.516 --> 00:27:28.500
[ Applause ]

00:27:33.056 --> 00:27:33.876
>> Good morning everybody.

00:27:34.196 --> 00:27:35.226
My name is Sergey Kamensky.

00:27:35.226 --> 00:27:36.196
I'm a software engineer on the

00:27:36.196 --> 00:27:36.926
Vision Framework Team.

00:27:37.356 --> 00:27:38.456
I'm excited to share with you

00:27:38.456 --> 00:27:39.786
today even more new features

00:27:39.876 --> 00:27:40.926
coming to the Framework this

00:27:40.926 --> 00:27:41.096
year.

00:27:41.096 --> 00:27:43.286
Let's talk about Face Technology

00:27:43.286 --> 00:27:43.606
first.

00:27:44.206 --> 00:27:45.956
Remember, two years ago when we

00:27:45.956 --> 00:27:47.836
introduced Vision Framework, we

00:27:47.836 --> 00:27:49.366
also talked about Face Landmark

00:27:49.416 --> 00:27:49.806
detector.

00:27:50.266 --> 00:27:51.596
This year, we're coming with a

00:27:51.596 --> 00:27:52.796
new revision for this algorithm.

00:27:53.176 --> 00:27:54.806
So, what are the changes?

00:27:55.916 --> 00:27:57.786
Well, first, we now have

00:27:57.926 --> 00:27:59.616
76-point cancellation, and this

00:27:59.616 --> 00:28:01.426
is versus 65-point cancellation

00:28:01.426 --> 00:28:02.166
as we had before.

00:28:02.166 --> 00:28:04.116
The 76-point cancellation gives

00:28:04.116 --> 00:28:05.146
us a greater density to

00:28:05.146 --> 00:28:06.276
represent different face

00:28:06.276 --> 00:28:06.686
regions.

00:28:07.556 --> 00:28:09.556
Second, we now report confidence

00:28:09.556 --> 00:28:11.306
score per landmark point, and

00:28:11.306 --> 00:28:12.886
this is versus a single average

00:28:12.886 --> 00:28:14.446
confidence score, as we reported

00:28:14.446 --> 00:28:14.836
before.

00:28:14.836 --> 00:28:16.646
But the biggest improvement

00:28:16.646 --> 00:28:18.126
comes in the pupil detection.

00:28:18.706 --> 00:28:19.996
As you can see, the image on the

00:28:19.996 --> 00:28:21.296
right-hand side has pupils

00:28:21.296 --> 00:28:22.406
detected with much better

00:28:22.566 --> 00:28:23.066
accuracy.

00:28:23.516 --> 00:28:25.886
Let's take a look at the client

00:28:25.886 --> 00:28:26.416
code sample.

00:28:27.986 --> 00:28:29.676
This code snippet will repeat

00:28:29.676 --> 00:28:31.056
throughout the presentation so

00:28:31.056 --> 00:28:32.386
the first time we're going to go

00:28:32.386 --> 00:28:33.206
line by line.

00:28:33.496 --> 00:28:35.516
Also, I use for [inaudible] in

00:28:35.516 --> 00:28:36.246
my samples.

00:28:36.246 --> 00:28:37.946
If this is just to simplify the

00:28:37.946 --> 00:28:39.376
slides, when you develop your

00:28:39.376 --> 00:28:40.736
apps, you probably should use

00:28:40.786 --> 00:28:42.276
proper error handling to avoid

00:28:42.276 --> 00:28:43.516
unwanted boundary conditions.

00:28:44.256 --> 00:28:45.156
Let's get back to the sample.

00:28:46.276 --> 00:28:47.276
In order to get your facial

00:28:47.276 --> 00:28:48.866
landmarks, first you need to

00:28:48.866 --> 00:28:49.416
create a

00:28:49.416 --> 00:28:50.716
DetectFaceLandmarksRequest.

00:28:51.296 --> 00:28:52.536
Then, you need to create

00:28:52.536 --> 00:28:54.036
ImageRequestHandler, passing the

00:28:54.036 --> 00:28:56.096
image into it the image that

00:28:56.096 --> 00:28:58.346
needs to be processed, and then

00:28:58.626 --> 00:28:59.626
you need to use that request

00:28:59.626 --> 00:29:00.776
handler to process your request.

00:29:01.426 --> 00:29:03.136
Finally, you need to look at the

00:29:03.136 --> 00:29:03.606
results.

00:29:04.386 --> 00:29:05.446
The results for everything that

00:29:05.446 --> 00:29:06.736
this human face related in

00:29:06.736 --> 00:29:08.156
Vision Framework will come in

00:29:08.196 --> 00:29:09.946
forms of face observations.

00:29:10.506 --> 00:29:12.006
Face observation derives some

00:29:12.006 --> 00:29:13.486
detected object observation.

00:29:13.856 --> 00:29:15.006
It inherits bounding box

00:29:15.046 --> 00:29:16.396
property, and it also adds

00:29:16.396 --> 00:29:17.766
several other properties on its

00:29:17.766 --> 00:29:19.886
level to describe human face.

00:29:20.766 --> 00:29:21.806
This time we'll be interested in

00:29:21.806 --> 00:29:22.636
the landmarks property.

00:29:23.136 --> 00:29:24.366
The landmarks property is of

00:29:24.426 --> 00:29:25.976
FaceLandmarks2D class.

00:29:26.176 --> 00:29:28.166
FaceLandmarks2D class consists

00:29:28.416 --> 00:29:29.896
of the confidence score.

00:29:30.086 --> 00:29:31.356
This is the average single

00:29:31.356 --> 00:29:32.476
average confidence score for the

00:29:32.476 --> 00:29:34.716
entire set and multiple face

00:29:34.716 --> 00:29:37.296
regions where each face region

00:29:37.296 --> 00:29:38.196
is represented by

00:29:38.196 --> 00:29:40.176
FaceLandmarksRegion2D class.

00:29:40.636 --> 00:29:41.856
Let's take a closer look at the

00:29:41.856 --> 00:29:44.536
properties of this class.

00:29:44.716 --> 00:29:46.166
First is pointCount.

00:29:46.666 --> 00:29:48.156
PointCount will tell you how

00:29:48.156 --> 00:29:49.546
many points represent a

00:29:49.546 --> 00:29:50.756
particular face region.

00:29:50.996 --> 00:29:52.256
This property will [inaudible] a

00:29:52.256 --> 00:29:53.616
different value depending how

00:29:53.616 --> 00:29:55.366
you configure your request, with

00:29:55.366 --> 00:29:56.936
65-point cancellation or

00:29:56.936 --> 00:29:58.376
76-point cancellation.

00:29:59.646 --> 00:30:01.146
The normalizedPoints property

00:30:01.916 --> 00:30:03.876
will represent the actual

00:30:03.876 --> 00:30:05.596
landmarks point, and the

00:30:05.596 --> 00:30:07.326
precisionEstimatesPerPoint will

00:30:07.326 --> 00:30:08.916
represent the actual confidence

00:30:08.916 --> 00:30:10.416
score for teach landmark point.

00:30:11.456 --> 00:30:12.386
Let's take a look at the codes

00:30:12.386 --> 00:30:12.666
needed.

00:30:12.666 --> 00:30:14.496
This is the same code snippet as

00:30:14.496 --> 00:30:16.106
in the previous slide, but now

00:30:16.106 --> 00:30:17.056
we're going to look at it from a

00:30:17.056 --> 00:30:18.196
slightly different perspective.

00:30:18.576 --> 00:30:20.266
We want to see how revisioning

00:30:20.266 --> 00:30:22.296
of the algorithm works in Vision

00:30:22.296 --> 00:30:22.696
Framework.

00:30:23.446 --> 00:30:25.056
If you take this code snippet

00:30:25.056 --> 00:30:26.276
and recompile it with the last

00:30:26.276 --> 00:30:28.076
[inaudible], what you will get

00:30:28.076 --> 00:30:30.086
is that the request object will

00:30:30.086 --> 00:30:31.806
be configured as follows: the

00:30:31.806 --> 00:30:33.196
revision property will be set to

00:30:33.196 --> 00:30:34.786
revision number 2, and the

00:30:34.786 --> 00:30:36.136
cancellation property will be

00:30:36.136 --> 00:30:37.426
set to cancellation of 65

00:30:37.426 --> 00:30:37.816
points.

00:30:38.396 --> 00:30:39.056
Technically, we didn't have

00:30:39.056 --> 00:30:40.286
cancellation property last year,

00:30:40.286 --> 00:30:41.456
but if we did, we could have set

00:30:41.456 --> 00:30:42.616
it to a single value only.

00:30:43.426 --> 00:30:45.726
Now, if on the other hand you

00:30:45.726 --> 00:30:47.246
take the same code snippet and

00:30:47.246 --> 00:30:49.136
recompile it with the current

00:30:49.686 --> 00:30:50.336
[inaudible], what you will get

00:30:50.336 --> 00:30:51.856
is that the revision property

00:30:51.856 --> 00:30:52.846
will be set to revision number

00:30:52.846 --> 00:30:55.056
3, and the cancellation property

00:30:55.056 --> 00:30:56.726
will be set to cancellation 76

00:30:56.726 --> 00:30:57.076
points.

00:30:58.626 --> 00:30:59.706
This actually represents the

00:30:59.706 --> 00:31:00.846
philosophy of how Vision

00:31:00.846 --> 00:31:02.606
Framework handles revisions of

00:31:02.606 --> 00:31:04.006
algorithms by default.

00:31:04.096 --> 00:31:05.766
If you don't specify a revision,

00:31:05.766 --> 00:31:07.716
what we will do is, we will give

00:31:07.716 --> 00:31:09.366
the latest supported by the SDK

00:31:09.366 --> 00:31:11.436
your code is compiled and linked

00:31:11.436 --> 00:31:11.916
against.

00:31:12.116 --> 00:31:14.066
Of course, we'll always

00:31:14.066 --> 00:31:14.856
recommend to set those

00:31:14.856 --> 00:31:15.836
properties explicitly.

00:31:16.116 --> 00:31:17.106
This is just to guarantee

00:31:17.106 --> 00:31:18.296
deterministic behavior in the

00:31:18.296 --> 00:31:18.656
future.

00:31:19.386 --> 00:31:22.266
Let's take a new metric that we

00:31:22.266 --> 00:31:23.556
developed this year, Face

00:31:23.556 --> 00:31:24.286
Capture Quality.

00:31:24.786 --> 00:31:25.686
There are two images on the

00:31:25.686 --> 00:31:25.946
screen.

00:31:26.296 --> 00:31:27.416
You can clearly see that one

00:31:27.416 --> 00:31:28.536
image was captured with better

00:31:28.536 --> 00:31:29.436
lighting and focusing

00:31:29.436 --> 00:31:29.996
conditions.

00:31:30.846 --> 00:31:32.016
We wanted to develop the metric

00:31:32.016 --> 00:31:33.376
that looks at the image as a

00:31:33.376 --> 00:31:35.016
whole and gives you one score

00:31:35.016 --> 00:31:36.796
back saying how bad or good the

00:31:36.796 --> 00:31:37.966
capture quality was.

00:31:37.966 --> 00:31:40.366
As a result, we came up with a

00:31:40.366 --> 00:31:41.596
Face Capture Quality metric.

00:31:42.466 --> 00:31:43.656
We trained our models for this

00:31:43.656 --> 00:31:45.026
metric in such a way so they

00:31:45.026 --> 00:31:46.876
tend to score lower if the image

00:31:46.876 --> 00:31:48.456
was captured with low light or

00:31:48.456 --> 00:31:50.206
bad focus, or for example, if a

00:31:50.236 --> 00:31:51.936
person had negative expressions.

00:31:52.896 --> 00:31:54.316
If we run this metric on these

00:31:54.346 --> 00:31:55.626
two images, we will get our

00:31:55.626 --> 00:31:56.206
scores back.

00:31:57.036 --> 00:31:57.876
These are floating-point

00:31:57.876 --> 00:31:58.316
numbers.

00:31:58.636 --> 00:31:59.636
You can compare them against

00:31:59.636 --> 00:32:00.776
each other, and you can say that

00:32:00.776 --> 00:32:02.856
the image that scored higher is

00:32:02.856 --> 00:32:04.056
the image that was captured with

00:32:04.056 --> 00:32:04.716
better quality.

00:32:05.346 --> 00:32:07.466
Let's take a look at the code

00:32:07.466 --> 00:32:08.136
sample.

00:32:09.606 --> 00:32:10.756
This is very similar to what we

00:32:10.756 --> 00:32:11.986
saw just a couple of slides ago,

00:32:12.196 --> 00:32:13.416
with the differences being in

00:32:13.416 --> 00:32:15.596
the request type and the

00:32:15.596 --> 00:32:16.136
results.

00:32:16.846 --> 00:32:18.526
Since we still with C1 faces,

00:32:18.736 --> 00:32:19.556
we're going to get our face

00:32:19.556 --> 00:32:20.826
observation back, but now we're

00:32:20.826 --> 00:32:21.686
going to look at a different

00:32:21.686 --> 00:32:22.426
property of the face

00:32:22.426 --> 00:32:23.926
observation, Face Capture

00:32:23.926 --> 00:32:24.626
Quality property.

00:32:24.876 --> 00:32:27.546
Let's take a look at the broader

00:32:27.546 --> 00:32:27.946
example.

00:32:28.616 --> 00:32:29.646
Let's say I have a sequence of

00:32:29.646 --> 00:32:30.586
images that could have been

00:32:30.586 --> 00:32:31.976
obtained by using the burst mode

00:32:31.976 --> 00:32:33.346
on the selfie camera or in the

00:32:33.346 --> 00:32:34.316
photo burst, for example.

00:32:34.626 --> 00:32:35.776
And you will ask yourself a

00:32:35.776 --> 00:32:36.166
question.

00:32:36.426 --> 00:32:37.816
Which image was captured with

00:32:37.816 --> 00:32:38.566
the best quality?

00:32:39.686 --> 00:32:40.966
What you can do now, you can run

00:32:40.966 --> 00:32:42.166
our algorithm on each image,

00:32:42.506 --> 00:32:45.366
assign scores, rank them, and

00:32:45.366 --> 00:32:46.466
the image that apps on the most

00:32:46.466 --> 00:32:47.876
light is the image that was

00:32:47.876 --> 00:32:50.896
captured with the best quality.

00:32:50.966 --> 00:32:52.436
Let's try to understand how we

00:32:52.436 --> 00:32:54.026
can interpret the results that

00:32:54.026 --> 00:32:55.166
are coming from the Face Capture

00:32:55.196 --> 00:32:55.856
Quality metric.

00:32:56.586 --> 00:32:58.206
I have two sequences of images

00:32:58.356 --> 00:32:58.886
on the slide.

00:32:59.426 --> 00:33:00.726
Each sequence is of the same

00:33:00.726 --> 00:33:02.146
person, and each sequence is

00:33:02.146 --> 00:33:03.456
represented by the images that

00:33:03.456 --> 00:33:05.286
scores lowest and the highest in

00:33:05.286 --> 00:33:06.346
the sequence with respect to

00:33:06.346 --> 00:33:07.216
Face Capture Quality.

00:33:08.116 --> 00:33:09.076
What can we say about these

00:33:09.076 --> 00:33:09.496
ranges?

00:33:10.816 --> 00:33:12.356
Well, there is some overlapping

00:33:12.356 --> 00:33:13.696
region, but there are some also

00:33:13.696 --> 00:33:15.006
regions that belong to one and

00:33:15.006 --> 00:33:16.336
don't belong to the other.

00:33:16.516 --> 00:33:18.216
If you had yet another sequence,

00:33:18.536 --> 00:33:19.306
it could have happened that

00:33:19.306 --> 00:33:20.536
there was no overlapping region

00:33:20.536 --> 00:33:20.926
at all.

00:33:21.856 --> 00:33:22.976
The point I'm trying to make

00:33:22.976 --> 00:33:24.686
here is that the Face Capture

00:33:24.686 --> 00:33:26.146
Quality should not be compared

00:33:26.146 --> 00:33:26.906
against a threshold.

00:33:28.126 --> 00:33:29.466
In this particular example, if I

00:33:29.466 --> 00:33:32.096
picked 0.52, I would have missed

00:33:32.326 --> 00:33:33.876
all the images on the left, and

00:33:33.876 --> 00:33:36.556
I would pretty much can get any

00:33:36.556 --> 00:33:37.406
image that's just past the

00:33:37.406 --> 00:33:38.626
midpoint on the right.

00:33:39.996 --> 00:33:41.286
But then what is Face Capture

00:33:41.346 --> 00:33:41.676
Quality?

00:33:42.706 --> 00:33:44.246
We define Face Capture Quality

00:33:44.246 --> 00:33:46.286
is a comparative or ranking

00:33:46.326 --> 00:33:47.766
measure of the same subject.

00:33:48.186 --> 00:33:49.756
Now, comparative and same are

00:33:49.756 --> 00:33:51.086
the key words in this sentence.

00:33:51.526 --> 00:33:53.216
If you're thinking, cool, I have

00:33:53.216 --> 00:33:54.476
this great new metric, I'm going

00:33:54.476 --> 00:33:55.606
to develop my beauty contest

00:33:55.606 --> 00:33:55.726
app.

00:33:56.946 --> 00:33:58.126
Probably not a good idea.

00:33:58.606 --> 00:33:59.636
In a beauty contest app, you

00:33:59.636 --> 00:34:01.296
would have to compare faces of

00:34:01.296 --> 00:34:02.806
different people, and that's not

00:34:02.806 --> 00:34:03.996
what this metric was developed

00:34:03.996 --> 00:34:04.626
and designed for.

00:34:06.266 --> 00:34:07.696
And that's Face Technology.

00:34:09.295 --> 00:34:10.076
Let's take a look at the new

00:34:10.076 --> 00:34:11.146
detectors we're adding this

00:34:11.146 --> 00:34:11.335
year.

00:34:12.896 --> 00:34:15.466
We're introducing Human Detector

00:34:15.466 --> 00:34:16.726
that detects human upper body

00:34:16.726 --> 00:34:18.076
that consists of human head and

00:34:18.076 --> 00:34:20.386
torso and also a pet detector,

00:34:20.795 --> 00:34:22.326
an Animal Detector that detects

00:34:22.696 --> 00:34:23.406
cats and dogs.

00:34:23.735 --> 00:34:24.755
The Animal Detector gives you

00:34:24.755 --> 00:34:26.166
bounding box back, and in

00:34:26.166 --> 00:34:27.286
addition to bounding boxes it

00:34:27.286 --> 00:34:28.636
gives you also a label saying

00:34:28.866 --> 00:34:30.235
which animal was detected.

00:34:31.795 --> 00:34:32.746
Let's take a look at the client

00:34:32.746 --> 00:34:33.226
code sample.

00:34:35.956 --> 00:34:37.916
Two snippets, one for Human

00:34:37.916 --> 00:34:39.366
Detector, one for Animal

00:34:39.366 --> 00:34:39.815
Detector.

00:34:40.326 --> 00:34:41.386
Very similar to what we had

00:34:41.386 --> 00:34:41.835
before.

00:34:42.016 --> 00:34:43.646
Again, the differences are in

00:34:43.646 --> 00:34:44.596
the request types that you

00:34:44.596 --> 00:34:46.386
create and in the results.

00:34:47.186 --> 00:34:49.525
Now, for Human Detector, all we

00:34:49.826 --> 00:34:51.275
care about is the bounding box.

00:34:51.886 --> 00:34:53.166
So, we use for that

00:34:53.306 --> 00:34:54.606
DetectedObjectObservation.

00:34:55.726 --> 00:34:56.706
For the Animal Detector on the

00:34:56.706 --> 00:34:57.706
other hand, we also need the

00:34:57.706 --> 00:34:58.986
label, so we use

00:34:59.676 --> 00:35:01.206
RecognizedObjectObservation that

00:35:01.206 --> 00:35:02.496
derives from detected object

00:35:02.496 --> 00:35:03.076
observation.

00:35:03.186 --> 00:35:04.536
It inherits bounding box, but it

00:35:04.536 --> 00:35:06.736
also adds a label property on

00:35:06.806 --> 00:35:07.296
the [inaudible].

00:35:07.806 --> 00:35:10.486
And that's new detectors.

00:35:11.246 --> 00:35:12.456
Let's take a look at what's new

00:35:12.456 --> 00:35:13.336
in tracking this year.

00:35:14.216 --> 00:35:15.126
We're coming up with a new

00:35:15.126 --> 00:35:16.556
revision for the Tracker.

00:35:16.556 --> 00:35:17.996
The changes are, we have

00:35:17.996 --> 00:35:19.516
improvements in the bounding

00:35:19.516 --> 00:35:20.396
boxes expansion area.

00:35:21.266 --> 00:35:22.386
We can now handle better

00:35:22.386 --> 00:35:22.976
occlusions.

00:35:23.536 --> 00:35:25.716
We are machine learning based

00:35:25.716 --> 00:35:26.176
this time.

00:35:26.886 --> 00:35:28.086
And we can run with low power

00:35:28.086 --> 00:35:29.046
consumption on multiple

00:35:29.046 --> 00:35:29.476
[inaudible] devices.

00:35:29.476 --> 00:35:32.516
Let's take a look at a sample.

00:35:32.956 --> 00:35:35.056
I have a mini video clip where a

00:35:35.056 --> 00:35:36.446
man is running in the forest,

00:35:36.606 --> 00:35:38.126
and he appears sometimes behind

00:35:38.126 --> 00:35:38.646
the trees.

00:35:38.976 --> 00:35:40.126
As you can see, the tracker is

00:35:40.126 --> 00:35:41.606
able to successfully recapture

00:35:41.606 --> 00:35:42.926
the tracked object and keep

00:35:42.926 --> 00:35:43.726
going with the tracking

00:35:43.726 --> 00:35:44.186
sequence.

00:35:46.016 --> 00:35:47.046
[ Applause ]

00:35:47.046 --> 00:35:47.486
Thank you.

00:35:48.516 --> 00:35:51.976
[ Applause ]

00:35:52.476 --> 00:35:53.756
Let's take a look at the client

00:35:53.756 --> 00:35:54.256
code sample.

00:35:54.676 --> 00:35:56.046
This is exactly the same snippet

00:35:56.046 --> 00:35:56.996
that we showed last year.

00:35:57.256 --> 00:35:58.306
It represents probably the

00:35:58.306 --> 00:35:59.416
simplest tracking sequence you

00:35:59.416 --> 00:35:59.946
can imagine.

00:36:00.166 --> 00:36:01.106
It tracks your object of

00:36:01.106 --> 00:36:02.336
interest for five consecutive

00:36:02.336 --> 00:36:02.786
frames.

00:36:03.986 --> 00:36:05.356
I want to go line-by-line, but I

00:36:05.356 --> 00:36:06.916
want to emphasize two points

00:36:06.916 --> 00:36:07.116
here.

00:36:07.486 --> 00:36:09.116
First is we use our

00:36:09.116 --> 00:36:10.036
SequenceRequestHandler.

00:36:11.066 --> 00:36:12.046
That is as opposite to

00:36:12.046 --> 00:36:13.226
ImageRequestHandler as we have

00:36:13.226 --> 00:36:14.226
used so far throughout the

00:36:14.226 --> 00:36:14.816
presentation.

00:36:15.336 --> 00:36:16.616
SequenceRequestHandler is used

00:36:16.616 --> 00:36:17.986
in Vision when you work with a

00:36:17.986 --> 00:36:19.216
sequence of frames and you need

00:36:19.216 --> 00:36:20.426
to cache some information from

00:36:20.426 --> 00:36:21.496
frame to frame to frame.

00:36:22.826 --> 00:36:24.356
Second point is when you

00:36:24.356 --> 00:36:25.106
implement your tracking

00:36:25.106 --> 00:36:26.836
sequence, you need to get your

00:36:26.836 --> 00:36:28.356
results from iteration number n

00:36:28.356 --> 00:36:30.026
and feed it as an input to a

00:36:30.026 --> 00:36:31.276
duration number n plus 1.

00:36:31.976 --> 00:36:35.216
Of course, if you recompiled

00:36:35.216 --> 00:36:36.106
this quote with the current

00:36:36.106 --> 00:36:37.646
[inaudible] SDK, the revision of

00:36:37.646 --> 00:36:38.846
the request will be set to

00:36:38.846 --> 00:36:40.316
revision number 2 by default.

00:36:40.496 --> 00:36:41.836
But we also recommend to set it

00:36:41.836 --> 00:36:42.426
explicitly.

00:36:42.966 --> 00:36:45.106
And that's the tracking.

00:36:46.136 --> 00:36:47.376
Let's take a look at the news

00:36:47.376 --> 00:36:49.696
with respect to Vision and Core

00:36:49.696 --> 00:36:50.396
ML integration.

00:36:51.146 --> 00:36:52.766
Last year, we presented

00:36:52.766 --> 00:36:53.836
integration with Vision and Core

00:36:53.836 --> 00:36:55.496
ML, and we showed how you can

00:36:55.496 --> 00:36:57.066
run Core ML models through

00:36:57.066 --> 00:36:57.596
Vision API.

00:36:57.596 --> 00:36:59.776
The advantage of doing that was

00:36:59.776 --> 00:37:01.706
that you can use 1 over 5

00:37:01.816 --> 00:37:03.236
different overloads of the image

00:37:03.236 --> 00:37:04.916
request handler to translate the

00:37:04.916 --> 00:37:06.266
image that you have in your hand

00:37:06.586 --> 00:37:08.976
to the image type, size, and

00:37:08.976 --> 00:37:10.416
color scheme that the Core ML

00:37:10.416 --> 00:37:11.266
model requires.

00:37:12.246 --> 00:37:13.256
We will run the inference for

00:37:13.256 --> 00:37:15.016
you, and we'll pack the outputs

00:37:15.016 --> 00:37:16.436
or results coming from Core ML

00:37:16.436 --> 00:37:17.886
model into Vision observations.

00:37:20.716 --> 00:37:22.426
Now, if you have a different

00:37:22.426 --> 00:37:23.696
task in mind, for example, if

00:37:23.696 --> 00:37:24.646
you want to do image style

00:37:24.646 --> 00:37:26.066
transfer, you need to have at

00:37:26.066 --> 00:37:27.416
least two images, the image

00:37:27.416 --> 00:37:29.056
content and the image style.

00:37:29.256 --> 00:37:30.356
You may also need to have some

00:37:30.356 --> 00:37:31.816
mixed ratio saying how much of a

00:37:31.926 --> 00:37:33.386
style needs to be applied on the

00:37:33.386 --> 00:37:33.826
content.

00:37:34.306 --> 00:37:35.506
So, I have three parameters now.

00:37:36.826 --> 00:37:37.886
Well, this year we're going to

00:37:37.886 --> 00:37:39.666
introduce API where we can use

00:37:39.666 --> 00:37:41.716
multiple inputs through Vision

00:37:41.896 --> 00:37:43.476
to Core ML, and that's including

00:37:43.476 --> 00:37:44.426
multi-image inputs.

00:37:44.426 --> 00:37:47.696
Also, on the output section,

00:37:48.186 --> 00:37:49.346
this sample shows only one

00:37:49.346 --> 00:37:49.686
output.

00:37:49.686 --> 00:37:50.616
But, for example, if you had

00:37:50.616 --> 00:37:52.116
more than one, especially if you

00:37:52.116 --> 00:37:53.376
have more than one of the same

00:37:53.376 --> 00:37:55.256
type, it's hard to distinguish

00:37:55.256 --> 00:37:56.356
them when they come in forms of

00:37:56.356 --> 00:37:57.946
observation later on.

00:37:58.416 --> 00:37:59.616
So, what we do this year, we

00:37:59.616 --> 00:38:00.646
introduce a new field in the

00:38:00.646 --> 00:38:02.616
observation that maps exactly to

00:38:02.616 --> 00:38:04.376
the name that shows up here in

00:38:04.376 --> 00:38:05.056
the output section.

00:38:06.216 --> 00:38:07.696
Let's take a look at the inputs

00:38:07.696 --> 00:38:08.316
and outputs.

00:38:08.316 --> 00:38:09.316
We will use them in the next

00:38:09.316 --> 00:38:09.576
slide.

00:38:12.936 --> 00:38:14.136
This is the code snippet that

00:38:14.176 --> 00:38:16.816
represents how to use Core ML

00:38:16.956 --> 00:38:17.396
through Vision.

00:38:18.676 --> 00:38:20.236
The highlighted sections show

00:38:20.446 --> 00:38:21.256
what's new this year.

00:38:21.716 --> 00:38:22.796
Let's keep them for now, and

00:38:22.796 --> 00:38:23.896
we'll go over the code, and

00:38:23.896 --> 00:38:25.316
we'll return to them later.

00:38:26.156 --> 00:38:27.466
In order to run Core ML through

00:38:27.466 --> 00:38:29.446
Vision, first you need to log

00:38:29.446 --> 00:38:30.236
your Core ML model.

00:38:31.266 --> 00:38:32.926
Then, you need to create Vision

00:38:32.926 --> 00:38:34.956
CoreMLmodel wrapper around it.

00:38:35.726 --> 00:38:37.476
Then, you need to create Vision

00:38:37.476 --> 00:38:39.346
CoreMLRequest and pass in that

00:38:39.346 --> 00:38:39.676
wrapper.

00:38:41.266 --> 00:38:42.326
Then you create

00:38:42.556 --> 00:38:44.136
ImageRequestHandler, you process

00:38:44.136 --> 00:38:45.446
your request, and you look at

00:38:45.446 --> 00:38:46.086
the results.

00:38:47.386 --> 00:38:49.596
Now, with the new API that we

00:38:49.596 --> 00:38:51.596
added this year, that only image

00:38:51.596 --> 00:38:53.116
that you could use last year is

00:38:53.116 --> 00:38:54.986
the default or the main image is

00:38:54.986 --> 00:38:56.276
the image that is passing to

00:38:56.476 --> 00:38:58.046
ImageRequestHandler, but that's

00:38:58.046 --> 00:38:59.526
also the image whose name needs

00:38:59.526 --> 00:39:01.126
to be assigned to input feature

00:39:01.506 --> 00:39:03.456
name field of the CoreMLModel

00:39:03.456 --> 00:39:03.796
wrapper.

00:39:05.026 --> 00:39:06.886
All other parameters whether

00:39:06.886 --> 00:39:08.646
images or not will have to be

00:39:08.716 --> 00:39:10.406
passed through feature provider

00:39:10.446 --> 00:39:11.686
property of the CoreMLModel

00:39:11.686 --> 00:39:12.016
wrapper.

00:39:12.376 --> 00:39:14.036
As you can see, image style and

00:39:14.036 --> 00:39:15.436
mixed ratio are passed in that

00:39:15.436 --> 00:39:15.666
way.

00:39:16.956 --> 00:39:18.336
Finally, when you look at the

00:39:18.336 --> 00:39:19.506
results, you can look at the

00:39:19.786 --> 00:39:21.386
feature name property of the

00:39:21.386 --> 00:39:22.646
observation that comes out, and

00:39:22.646 --> 00:39:24.186
you can compare it in this case

00:39:24.186 --> 00:39:25.246
against image result.

00:39:25.506 --> 00:39:26.656
That's exactly the name that

00:39:26.656 --> 00:39:27.706
appears in the output section of

00:39:27.756 --> 00:39:29.066
Core ML, and that way you can

00:39:29.096 --> 00:39:30.006
process your results

00:39:30.146 --> 00:39:30.666
accordingly.

00:39:32.636 --> 00:39:33.626
This slide actually concludes

00:39:33.626 --> 00:39:34.666
our presentation for today.

00:39:34.966 --> 00:39:36.076
For more information you can

00:39:36.076 --> 00:39:37.136
refer to the links on the slide.

00:39:37.466 --> 00:39:39.276
Thank you, and have a great rest

00:39:39.276 --> 00:39:39.976
of your WWDC.

00:39:40.016 --> 00:39:42.000
[ Applause ]