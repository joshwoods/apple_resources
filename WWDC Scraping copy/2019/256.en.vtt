WEBVTT

00:00:00.506 --> 00:00:05.916
[ Music ]

00:00:06.416 --> 00:00:08.846
>> Hi. I'm Neha Agrawal, and I'm

00:00:08.846 --> 00:00:10.516
a software engineer working on

00:00:10.516 --> 00:00:11.426
speech recognition.

00:00:12.426 --> 00:00:14.776
In 2016, we introduced the

00:00:14.776 --> 00:00:16.436
Speech Recognition framework for

00:00:16.686 --> 00:00:18.756
developers to solve their speech

00:00:18.756 --> 00:00:19.686
recognition needs.

00:00:20.326 --> 00:00:22.096
For anyone who is new to this

00:00:22.096 --> 00:00:24.096
framework, I highly recommend

00:00:24.316 --> 00:00:25.876
watching this Speech Recognition

00:00:25.876 --> 00:00:28.166
API session by my colleague

00:00:28.316 --> 00:00:29.246
Henry Mason.

00:00:31.066 --> 00:00:32.695
In this video, we're going to

00:00:32.695 --> 00:00:35.106
discuss exciting new advances in

00:00:35.106 --> 00:00:35.426
the APIs.

00:00:35.426 --> 00:00:36.906
Let's get started.

00:00:39.216 --> 00:00:40.596
Speech recognition is now

00:00:40.596 --> 00:00:42.206
supported for macOS.

00:00:43.036 --> 00:00:44.706
The support is available for

00:00:44.706 --> 00:00:47.376
both AppKit and iPad apps on

00:00:47.376 --> 00:00:47.716
Mac.

00:00:48.886 --> 00:00:51.536
Just like iOS, over 50 languages

00:00:51.536 --> 00:00:52.266
are supported.

00:00:53.816 --> 00:00:55.756
You need approval from your

00:00:55.756 --> 00:00:57.856
users to access the microphone

00:00:57.976 --> 00:00:59.616
and record their speech, and

00:01:00.406 --> 00:01:01.736
they also need to have Siri

00:01:01.736 --> 00:01:02.246
enabled.

00:01:02.876 --> 00:01:05.596
In addition to supporting speech

00:01:05.596 --> 00:01:08.366
recognition on macOS, we are now

00:01:08.366 --> 00:01:10.056
allowing developers to run

00:01:10.056 --> 00:01:12.026
recognition on-device for

00:01:12.026 --> 00:01:13.936
privacy sensitive applications.

00:01:14.646 --> 00:01:17.876
With on-device support, your

00:01:17.876 --> 00:01:19.746
user's data will not be sent to

00:01:19.746 --> 00:01:20.586
Apple servers.

00:01:22.096 --> 00:01:24.156
Your app no longer needs to rely

00:01:24.446 --> 00:01:27.136
on a network connection, and

00:01:27.136 --> 00:01:28.626
cellular data will not be

00:01:28.626 --> 00:01:29.196
consumed.

00:01:31.216 --> 00:01:33.556
However, there are tradeoffs to

00:01:33.556 --> 00:01:34.026
consider.

00:01:34.736 --> 00:01:37.426
Accuracy is good on-device, but

00:01:37.426 --> 00:01:39.536
you may find it is better on

00:01:39.536 --> 00:01:40.996
server due to a continuous

00:01:40.996 --> 00:01:41.416
learning.

00:01:41.666 --> 00:01:43.896
A server-based recognition

00:01:43.896 --> 00:01:46.246
support has limits on number of

00:01:46.246 --> 00:01:48.196
requests and audio duration.

00:01:48.976 --> 00:01:50.496
With on-device recognition,

00:01:50.876 --> 00:01:52.436
these limits do not apply.

00:01:53.726 --> 00:01:54.776
The number of languages

00:01:54.776 --> 00:01:57.256
supported on server are more

00:01:57.256 --> 00:01:58.536
than on-device.

00:01:59.846 --> 00:02:02.126
Also, if server isn't available,

00:02:02.126 --> 00:02:04.426
our server mode automatically

00:02:04.426 --> 00:02:06.376
falls back on on-device

00:02:06.376 --> 00:02:08.276
recognition if it is supported.

00:02:09.156 --> 00:02:11.736
All iPhones and iPads with Apple

00:02:12.156 --> 00:02:13.196
A9 or later processors are

00:02:13.196 --> 00:02:15.156
supported, and all Mac devices

00:02:15.156 --> 00:02:15.746
are supported.

00:02:16.776 --> 00:02:18.086
There are over 10 languages

00:02:18.086 --> 00:02:19.246
supported for on-device

00:02:19.246 --> 00:02:19.856
recognition.

00:02:20.426 --> 00:02:23.746
Now, let's look at how to enable

00:02:23.976 --> 00:02:25.876
on-device recognition in code.

00:02:26.536 --> 00:02:29.016
To recognize pre-recorded audio,

00:02:29.386 --> 00:02:30.986
we first create an

00:02:30.986 --> 00:02:33.066
SFSpeechRecognizer object and

00:02:33.066 --> 00:02:35.096
check for availability of speech

00:02:35.096 --> 00:02:36.856
recognition on that object.

00:02:39.206 --> 00:02:40.346
If speech recognition is

00:02:40.346 --> 00:02:42.516
available, we can create a

00:02:42.516 --> 00:02:44.376
recognition request with the

00:02:44.376 --> 00:02:46.366
audio file URL and start

00:02:46.366 --> 00:02:47.026
recognition.

00:02:49.676 --> 00:02:51.896
Now, in order to use on-device

00:02:51.896 --> 00:02:53.616
recognition, you need to first

00:02:53.616 --> 00:02:55.416
check if on-device recognition

00:02:55.416 --> 00:02:57.396
is supported and then set

00:02:57.606 --> 00:02:59.446
requiresOnDeviceRecognition

00:02:59.446 --> 00:03:01.396
property on the request object.

00:03:03.136 --> 00:03:04.586
Now that we have looked at this

00:03:04.586 --> 00:03:06.296
in code, let's talk about the

00:03:06.296 --> 00:03:07.006
results you get.

00:03:07.466 --> 00:03:11.466
Since iOS 10 in speech

00:03:11.466 --> 00:03:13.016
recognition results, we have

00:03:13.016 --> 00:03:14.616
provided transcriptions,

00:03:14.966 --> 00:03:16.486
alternative interpretations,

00:03:16.786 --> 00:03:18.456
confidence levels and timing

00:03:18.456 --> 00:03:19.096
information.

00:03:20.636 --> 00:03:22.146
We're making a few more

00:03:22.146 --> 00:03:23.616
additions to the speech

00:03:23.616 --> 00:03:24.556
recognition results.

00:03:26.776 --> 00:03:29.186
Speaking rate measures how fast

00:03:29.186 --> 00:03:31.016
a person speaks in words per

00:03:31.016 --> 00:03:31.396
minute.

00:03:33.366 --> 00:03:35.006
Average pause duration measures

00:03:35.276 --> 00:03:36.556
the average length of pause

00:03:36.556 --> 00:03:37.336
between words.

00:03:37.906 --> 00:03:41.086
And voice analytics features

00:03:41.306 --> 00:03:43.196
include various measures of

00:03:43.196 --> 00:03:44.566
vocal characteristics.

00:03:46.116 --> 00:03:47.906
Now, voice analytics gives

00:03:47.906 --> 00:03:49.416
insight into four features.

00:03:50.186 --> 00:03:52.216
Jitter measures how pitch varies

00:03:52.216 --> 00:03:52.726
in audio.

00:03:53.446 --> 00:03:55.296
With voice analytics, you can

00:03:55.296 --> 00:03:57.066
now understand the amount of

00:03:57.066 --> 00:03:59.066
jitter in speech expressed as a

00:03:59.066 --> 00:03:59.816
percentage.

00:04:01.236 --> 00:04:02.926
Shimmer measures how amplitude

00:04:02.926 --> 00:04:04.886
varies in audio, and with voice

00:04:04.886 --> 00:04:06.706
analytics, you can understand

00:04:06.706 --> 00:04:10.336
shimmer in speech expressed in

00:04:11.536 --> 00:04:11.946
decibels.

00:04:12.096 --> 00:04:13.376
Let's listen to some audio

00:04:13.376 --> 00:04:14.466
samples to understand what

00:04:14.466 --> 00:04:16.166
speech with high jitter and

00:04:16.166 --> 00:04:17.125
shimmer sounds like.

00:04:17.606 --> 00:04:19.526
First, let's hear audio with

00:04:19.526 --> 00:04:20.276
normal speech.

00:04:20.866 --> 00:04:21.546
>> Apple.

00:04:23.276 --> 00:04:24.756
>> Now, audio with perturbed

00:04:24.756 --> 00:04:25.236
speech.

00:04:25.956 --> 00:04:26.846
>> Apple.

00:04:27.926 --> 00:04:29.176
>> Next feature is pitch.

00:04:29.966 --> 00:04:31.976
Pitch measures the highness and

00:04:31.976 --> 00:04:33.286
lowness of the tone.

00:04:33.656 --> 00:04:35.476
Often, women and children have

00:04:35.476 --> 00:04:36.266
higher pitch.

00:04:37.496 --> 00:04:39.426
And voicing is used to identify

00:04:39.776 --> 00:04:41.766
voiced regions in speech.

00:04:42.316 --> 00:04:44.956
The voice analytics features are

00:04:44.956 --> 00:04:46.516
specific to an individual, and

00:04:46.516 --> 00:04:49.036
they can vary with time and

00:04:49.036 --> 00:04:49.926
circumstances.

00:04:50.636 --> 00:04:52.506
For example, if the person is

00:04:52.506 --> 00:04:54.876
tired, these features will be

00:04:54.876 --> 00:04:56.616
different than when they're not.

00:04:57.466 --> 00:04:59.346
Also, depending on who the

00:04:59.346 --> 00:05:00.996
person is talking to, these

00:05:00.996 --> 00:05:01.976
features may vary.

00:05:04.416 --> 00:05:06.556
These new results are part of

00:05:06.556 --> 00:05:08.436
the SF transcription object and

00:05:08.436 --> 00:05:10.016
will be available periodically.

00:05:10.556 --> 00:05:12.776
We will have them at the end

00:05:12.986 --> 00:05:14.586
when the isFinal flag is sent,

00:05:14.996 --> 00:05:16.246
but we could also see them

00:05:16.246 --> 00:05:16.586
before.

00:05:17.306 --> 00:05:19.806
You can access speakingRate and

00:05:19.806 --> 00:05:21.796
averagePauseDuration as shown.

00:05:23.736 --> 00:05:26.566
To access voice analytics, you

00:05:26.566 --> 00:05:28.446
would have to access the SF

00:05:28.446 --> 00:05:30.346
transcription segment object,

00:05:30.586 --> 00:05:32.256
and then you can access it as

00:05:32.256 --> 00:05:32.846
shown here.

00:05:34.636 --> 00:05:37.006
To summarize, we have made three

00:05:37.006 --> 00:05:37.846
key advances.

00:05:38.506 --> 00:05:40.326
You can now build apps on macOS

00:05:40.656 --> 00:05:42.366
using speech recognition APIs.

00:05:43.546 --> 00:05:45.066
Speech recognition can be run

00:05:45.066 --> 00:05:47.196
on-device in a privacy-friendly

00:05:47.196 --> 00:05:47.606
manner.

00:05:48.386 --> 00:05:50.696
And you now have access to voice

00:05:50.696 --> 00:05:52.686
analytics features for getting

00:05:52.686 --> 00:05:54.076
insight into vocal

00:05:54.076 --> 00:05:55.136
characteristics.

00:05:57.176 --> 00:05:59.486
For more information, check out

00:05:59.486 --> 00:06:01.006
the session's web page and

00:06:01.006 --> 00:06:01.976
thanks for watching.