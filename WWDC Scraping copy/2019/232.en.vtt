WEBVTT

00:00:00.506 --> 00:00:04.500
[ Music ]

00:00:07.516 --> 00:00:11.286
[ Applause ]

00:00:11.786 --> 00:00:12.636
>> Hello and good afternoon

00:00:12.636 --> 00:00:13.076
everyone.

00:00:13.546 --> 00:00:14.736
Welcome to our session on

00:00:14.736 --> 00:00:15.976
Natural Language Processing.

00:00:16.566 --> 00:00:18.266
I'm Vivek, and I'll be jointly

00:00:18.266 --> 00:00:19.656
presenting this session with my

00:00:19.656 --> 00:00:20.806
colleague, Doug Davidson.

00:00:21.646 --> 00:00:23.086
Let's get started.

00:00:23.716 --> 00:00:26.076
As you know, text is ubiquitous.

00:00:26.836 --> 00:00:27.796
You see it everywhere.

00:00:28.436 --> 00:00:29.826
If you think of how users

00:00:29.826 --> 00:00:31.256
interact with text in apps,

00:00:31.906 --> 00:00:33.286
there are two primary modes in

00:00:33.286 --> 00:00:34.796
which they interact.

00:00:34.796 --> 00:00:36.376
One, is through Natural Language

00:00:36.456 --> 00:00:38.996
input, wherein the user is

00:00:39.036 --> 00:00:41.036
writing text or generating text

00:00:41.266 --> 00:00:42.296
within the application.

00:00:43.176 --> 00:00:44.326
For instance, the user may be

00:00:44.326 --> 00:00:46.136
typing text on the keyboard

00:00:46.136 --> 00:00:48.266
inside the app.

00:00:48.526 --> 00:00:49.996
Examples of apps are, for

00:00:49.996 --> 00:00:51.506
example, messages where you're

00:00:51.506 --> 00:00:53.566
writing text and sharing it with

00:00:53.566 --> 00:00:55.456
other people, notes, or any

00:00:55.456 --> 00:00:56.956
productivity app where you're

00:00:56.956 --> 00:00:58.156
writing text as part of the

00:00:58.156 --> 00:00:58.776
application.

00:01:00.246 --> 00:01:01.136
The other sort of user

00:01:01.136 --> 00:01:03.126
interaction with text in apps is

00:01:03.126 --> 00:01:05.626
through Natural Language output

00:01:06.046 --> 00:01:07.696
wherein the app presents the

00:01:07.696 --> 00:01:09.126
text content to the user, and

00:01:09.286 --> 00:01:11.176
the user is consuming or reading

00:01:11.176 --> 00:01:11.706
this text.

00:01:12.836 --> 00:01:14.256
If you think of an application,

00:01:14.436 --> 00:01:17.346
like Apple News, the information

00:01:17.346 --> 00:01:18.466
or text is presented to the

00:01:18.466 --> 00:01:20.106
user, and the user is reading

00:01:20.106 --> 00:01:20.796
this information.

00:01:21.666 --> 00:01:24.116
So, both in cases of text input

00:01:24.226 --> 00:01:26.686
and text output, in order to

00:01:26.686 --> 00:01:28.366
extract actionable intelligence

00:01:28.546 --> 00:01:30.346
out of raw text, Natural

00:01:30.346 --> 00:01:31.686
Language processing is really

00:01:31.686 --> 00:01:32.146
important.

00:01:32.146 --> 00:01:34.816
And last year, we introduced the

00:01:34.816 --> 00:01:36.086
Natural Language Framework.

00:01:36.796 --> 00:01:38.236
The Natural Language Framework

00:01:38.406 --> 00:01:41.016
is the NLP workhorse for all

00:01:41.016 --> 00:01:42.346
things across all Apple

00:01:42.346 --> 00:01:42.796
platforms.

00:01:43.406 --> 00:01:44.476
So, we provide several

00:01:44.546 --> 00:01:46.256
fundamental NLP building blocks

00:01:46.696 --> 00:01:48.276
such as language identification,

00:01:48.276 --> 00:01:49.796
tokenization, part of speech

00:01:49.796 --> 00:01:51.666
tagging, and so on, and we

00:01:51.666 --> 00:01:53.036
present these functionalities

00:01:53.036 --> 00:01:54.846
and provide this across several

00:01:54.846 --> 00:01:55.796
different languages.

00:01:55.796 --> 00:01:58.456
And we do this by seamlessly

00:01:58.456 --> 00:02:00.156
blending linguistics and machine

00:02:00.156 --> 00:02:01.516
learning so that you can just

00:02:01.516 --> 00:02:03.356
focus on building your apps by

00:02:03.356 --> 00:02:05.526
using these APIs, and we do all

00:02:05.526 --> 00:02:07.066
of the heavy lifting under the

00:02:07.616 --> 00:02:07.706
hood.

00:02:08.336 --> 00:02:09.955
Now, if you step back and look

00:02:09.955 --> 00:02:11.306
at all of these functionalities,

00:02:11.536 --> 00:02:13.466
actually if you look at most NLP

00:02:13.466 --> 00:02:14.936
functionalities, they can be

00:02:14.936 --> 00:02:16.676
broken down into two broad

00:02:17.006 --> 00:02:19.176
categories of tasks.

00:02:19.176 --> 00:02:20.936
One is text classification.

00:02:21.066 --> 00:02:22.796
And the objective in text

00:02:22.796 --> 00:02:24.866
classification is given a piece

00:02:24.866 --> 00:02:26.286
of text, and this text can

00:02:26.286 --> 00:02:28.266
either be a sentence, can be a

00:02:28.266 --> 00:02:30.256
paragraph, or a document, you

00:02:30.256 --> 00:02:31.566
would like to assign labels to

00:02:31.566 --> 00:02:33.336
this piece of text, and these

00:02:33.336 --> 00:02:35.096
labels can be sentiment labels,

00:02:35.096 --> 00:02:36.486
topic labels, or any labels that

00:02:36.486 --> 00:02:37.096
you want to assign.

00:02:38.426 --> 00:02:40.036
The other category of tasks by

00:02:40.036 --> 00:02:41.696
NLP is called word tagging.

00:02:41.696 --> 00:02:43.576
And the task here or the

00:02:43.576 --> 00:02:45.176
objective here is given a

00:02:45.176 --> 00:02:46.756
sequence of words or what we

00:02:46.756 --> 00:02:48.606
call is tokens, we would like to

00:02:48.606 --> 00:02:50.796
assign a label to every token in

00:02:50.796 --> 00:02:51.456
this sequence.

00:02:51.656 --> 00:02:54.366
And this year, we have exciting

00:02:54.366 --> 00:02:55.986
new APIs in both text

00:02:55.986 --> 00:02:57.476
classification as well as word

00:02:57.476 --> 00:02:57.776
tagging.

00:02:57.776 --> 00:02:59.646
Let's start off with the first

00:02:59.646 --> 00:03:00.976
one, which is sentiment

00:03:00.976 --> 00:03:01.616
analysis.

00:03:02.356 --> 00:03:04.016
Sentiment analysis is a text

00:03:04.016 --> 00:03:05.666
classification API, this is a

00:03:05.666 --> 00:03:07.946
new API, and it works this way.

00:03:08.536 --> 00:03:09.766
What you do is you bring your

00:03:09.766 --> 00:03:11.496
text, and you pass your text to

00:03:11.496 --> 00:03:14.016
this API, and the API analyzes

00:03:14.046 --> 00:03:15.816
the text and gives you out a

00:03:15.816 --> 00:03:16.656
sentiment score.

00:03:17.466 --> 00:03:18.666
Now this sentiment score

00:03:18.786 --> 00:03:20.536
captures the degree of sentiment

00:03:20.536 --> 00:03:21.686
that is contained in your text.

00:03:22.786 --> 00:03:24.106
You provide a sentiment score

00:03:24.106 --> 00:03:26.626
from -1 to +1, which indicates

00:03:26.626 --> 00:03:27.516
the degree of sentiment.

00:03:28.216 --> 00:03:29.516
For instance, if you have -1, it

00:03:29.516 --> 00:03:31.396
indicates a very strong negative

00:03:31.396 --> 00:03:33.256
sentiment, and +1 indicates a

00:03:33.256 --> 00:03:34.336
very positive sentiment.

00:03:34.886 --> 00:03:36.056
So, essentially we provide the

00:03:36.056 --> 00:03:37.506
score and let you calibrate the

00:03:37.506 --> 00:03:38.846
score for your application.

00:03:39.036 --> 00:03:41.356
As an example, if you had a

00:03:41.356 --> 00:03:42.836
sentence such as we had a fun

00:03:42.836 --> 00:03:44.386
time in Hawaii with the family,

00:03:44.586 --> 00:03:45.946
the API might give you a score

00:03:45.946 --> 00:03:48.166
of 0.8, which shows that this

00:03:48.166 --> 00:03:50.476
sentence is a positive sentence.

00:03:51.616 --> 00:03:52.756
In contrast, if you had a

00:03:52.796 --> 00:03:54.266
sentence such as we had a not so

00:03:54.266 --> 00:03:55.656
fun time in Hawaii because mom

00:03:55.656 --> 00:03:57.166
twisted her ankle, well that's

00:03:57.166 --> 00:03:59.156
not positive, so you get a score

00:03:59.156 --> 00:04:00.956
of minus 0.8, and you can

00:04:00.956 --> 00:04:02.446
calibrate that to be a negative

00:04:02.446 --> 00:04:02.836
sentiment.

00:04:04.126 --> 00:04:05.466
This is great; how do you use

00:04:05.466 --> 00:04:05.576
it?

00:04:06.376 --> 00:04:07.676
It's really simple to use.

00:04:08.006 --> 00:04:09.646
So those of you who are used to

00:04:09.646 --> 00:04:11.036
using NaturalLanguage, this is

00:04:11.036 --> 00:04:11.886
extremely easy.

00:04:12.416 --> 00:04:13.736
You import NaturalLanguage.

00:04:14.236 --> 00:04:15.496
You create an instance of

00:04:15.536 --> 00:04:17.766
NLTagger and all that you do now

00:04:17.836 --> 00:04:19.646
is specify a new tag scheme.

00:04:19.786 --> 00:04:21.676
And the new tag scheme is called

00:04:21.676 --> 00:04:22.456
sentiment score.

00:04:23.266 --> 00:04:25.116
Once you do this, you attach the

00:04:25.116 --> 00:04:26.466
string that you want to analyze

00:04:26.466 --> 00:04:28.256
to the tagger, and you simply

00:04:28.256 --> 00:04:29.576
ask for the sentiment score

00:04:29.906 --> 00:04:31.666
either at the sentence level or

00:04:31.666 --> 00:04:32.446
the paragraph level.

00:04:33.476 --> 00:04:34.996
Now let's see this in action.

00:04:36.316 --> 00:04:37.576
So what we have here is a

00:04:37.576 --> 00:04:38.516
hypothetical app.

00:04:38.876 --> 00:04:40.106
It's a cheese application.

00:04:40.596 --> 00:04:42.766
And as part of this application,

00:04:42.766 --> 00:04:44.246
users can do a bunch of things.

00:04:44.536 --> 00:04:45.726
They can write notes about

00:04:45.726 --> 00:04:46.136
cheese.

00:04:46.416 --> 00:04:47.946
They can write reviews, express

00:04:47.946 --> 00:04:49.016
their opinions about different

00:04:49.016 --> 00:04:49.756
kinds of cheese.

00:04:50.176 --> 00:04:51.496
So, even though this application

00:04:51.496 --> 00:04:53.156
is about cheese, there's nothing

00:04:53.156 --> 00:04:53.886
cheesy about it.

00:04:53.886 --> 00:04:55.436
It really deals with the finer

00:04:55.436 --> 00:04:56.276
points of cheese.

00:04:56.656 --> 00:04:57.846
And what I'm going to show you

00:04:57.846 --> 00:05:00.646
here is a user writing a review,

00:05:01.066 --> 00:05:03.006
and as the user is writing the

00:05:03.006 --> 00:05:05.036
review, the text gets passed to

00:05:05.036 --> 00:05:06.286
the sentiment classification

00:05:06.286 --> 00:05:09.246
API, we get a score, and we

00:05:09.316 --> 00:05:10.746
color the text based on the

00:05:10.746 --> 00:05:11.416
sentiment score.

00:05:11.726 --> 00:05:13.646
So, let's see, if you were to

00:05:13.646 --> 00:05:15.496
type something like fantastic

00:05:16.336 --> 00:05:19.016
taste, really delicious, you can

00:05:19.016 --> 00:05:19.926
see this is a positive

00:05:19.926 --> 00:05:20.386
sentiment.

00:05:21.796 --> 00:05:24.886
In contrast, if you said great

00:05:24.886 --> 00:05:28.306
at first but a horrible

00:05:28.306 --> 00:05:30.296
aftertaste, you see that this is

00:05:30.296 --> 00:05:32.006
a negative sentiment, right.

00:05:32.276 --> 00:05:33.936
And what you also realize here

00:05:33.936 --> 00:05:35.726
is all of this can be happening

00:05:35.726 --> 00:05:36.386
in real time.

00:05:36.746 --> 00:05:38.046
That's because the API is

00:05:38.046 --> 00:05:39.156
extremely performing.

00:05:39.556 --> 00:05:41.126
It uses a Neural Network model

00:05:41.126 --> 00:05:43.206
underneath, and it's hardware

00:05:43.206 --> 00:05:44.516
activated across all Apple

00:05:44.516 --> 00:05:46.336
platforms, so essentially, you

00:05:46.336 --> 00:05:47.616
can do this in real time.

00:05:48.756 --> 00:05:49.846
And we support the sentiment

00:05:49.846 --> 00:05:51.936
analysis API in seven different

00:05:51.936 --> 00:05:53.966
languages, English, French,

00:05:54.016 --> 00:05:55.596
Italian, German, Spanish,

00:05:55.636 --> 00:05:57.176
Portuguese, and simplified

00:05:57.176 --> 00:05:57.646
Chinese.

00:05:58.176 --> 00:05:58.916
I think you're really going to

00:05:58.916 --> 00:05:59.436
like this.

00:06:00.516 --> 00:06:06.546
[ Applause ]

00:06:07.046 --> 00:06:08.126
And, of course, all of this is

00:06:08.126 --> 00:06:09.656
happening completely on device,

00:06:09.656 --> 00:06:11.026
and the user data never has to

00:06:11.026 --> 00:06:11.536
leave the device.

00:06:11.896 --> 00:06:13.216
We bring all that power on

00:06:13.216 --> 00:06:13.786
device to you.

00:06:14.756 --> 00:06:16.306
I like to just spend a brief

00:06:16.306 --> 00:06:17.926
moment on language assets.

00:06:18.296 --> 00:06:19.746
As I mentioned, the NLP

00:06:19.746 --> 00:06:21.246
functionalities are quite

00:06:21.896 --> 00:06:23.476
diverse, and we provide this in

00:06:23.476 --> 00:06:24.646
several different languages.

00:06:25.176 --> 00:06:26.186
Now, for users of our

00:06:26.186 --> 00:06:28.006
applications, we make sure that

00:06:28.006 --> 00:06:29.986
they always have the assets in

00:06:29.986 --> 00:06:31.136
the language they're interested

00:06:31.136 --> 00:06:31.286
in.

00:06:31.856 --> 00:06:33.316
But for you, for development

00:06:33.316 --> 00:06:34.686
purposes, you might be

00:06:34.686 --> 00:06:36.546
interested in getting assets on

00:06:36.546 --> 00:06:36.846
demand.

00:06:36.846 --> 00:06:38.266
This is in fact a very common

00:06:38.266 --> 00:06:39.286
request that we've heard from

00:06:39.286 --> 00:06:40.716
several of you, and we're

00:06:40.716 --> 00:06:42.746
introducing a new convenience

00:06:42.746 --> 00:06:44.236
API called Request Assets.

00:06:44.686 --> 00:06:46.076
So, you can trigger off a

00:06:46.076 --> 00:06:48.336
download for a particular asset

00:06:48.576 --> 00:06:49.216
on demand.

00:06:49.436 --> 00:06:50.896
You just specify the language

00:06:50.896 --> 00:06:51.836
and the tag scheme that you're

00:06:51.836 --> 00:06:53.306
interested in, and we'll fork

00:06:53.306 --> 00:06:54.076
off a download in the

00:06:54.076 --> 00:06:55.186
background, and the next

00:06:55.276 --> 00:06:56.476
opportune moment you're going to

00:06:56.476 --> 00:06:57.736
get the assets on your device.

00:06:57.736 --> 00:06:59.376
So, this is really going to help

00:06:59.376 --> 00:07:00.226
you with development and

00:07:00.226 --> 00:07:01.866
increase your productivity as

00:07:01.866 --> 00:07:03.476
you're building your apps.

00:07:05.156 --> 00:07:06.366
So, we talked about text

00:07:06.366 --> 00:07:07.226
classification.

00:07:07.226 --> 00:07:08.616
Now let's shift our attention to

00:07:08.616 --> 00:07:09.856
Word Tagging.

00:07:10.956 --> 00:07:12.486
To just refresh your memory,

00:07:12.486 --> 00:07:14.616
Word Tagging is a task where

00:07:14.616 --> 00:07:16.116
given a sequence of tokens we'd

00:07:16.116 --> 00:07:17.576
like to assign a label to every

00:07:17.576 --> 00:07:18.896
single token in the sequence.

00:07:19.086 --> 00:07:21.776
As an example here, we could

00:07:21.856 --> 00:07:23.056
assign a bunch of tokens

00:07:23.056 --> 00:07:24.246
different labels.

00:07:24.376 --> 00:07:25.546
Timothy is a person name.

00:07:25.546 --> 00:07:27.326
Switzerland is a location, and

00:07:27.326 --> 00:07:28.736
we have a bunch of nouns here in

00:07:28.736 --> 00:07:29.366
this sentence.

00:07:30.466 --> 00:07:31.226
Now, this is great.

00:07:31.226 --> 00:07:32.736
If you're just looking to do

00:07:32.736 --> 00:07:34.556
named entity recognition using

00:07:34.556 --> 00:07:36.346
our APIs or part of speech

00:07:36.346 --> 00:07:37.976
tagging using our APIs, this is

00:07:37.976 --> 00:07:39.616
fine, but there are several

00:07:39.616 --> 00:07:41.006
instances where you want to do

00:07:41.006 --> 00:07:42.386
something that's more customized

00:07:42.386 --> 00:07:43.096
to your task.

00:07:43.826 --> 00:07:44.736
You don't want to know just

00:07:44.736 --> 00:07:46.456
Gruyere cheese are two nouns,

00:07:46.816 --> 00:07:48.296
but what you want to do is you

00:07:48.296 --> 00:07:49.256
want to know that it's Swiss

00:07:49.256 --> 00:07:49.626
cheese.

00:07:49.736 --> 00:07:50.426
Of course, we are building a

00:07:50.426 --> 00:07:52.106
cheese application, you want to

00:07:52.106 --> 00:07:53.136
get this information out.

00:07:53.856 --> 00:07:55.626
But, the default tagger doesn't

00:07:55.626 --> 00:07:56.696
have any information about

00:07:56.756 --> 00:07:57.966
cheese, so how are we going to

00:07:57.966 --> 00:07:58.846
provide this information?

00:07:59.446 --> 00:08:00.926
So, we have a new functionality

00:08:01.336 --> 00:08:02.526
in Natural Language Framework

00:08:02.526 --> 00:08:04.366
that we call a Text Catalog.

00:08:05.646 --> 00:08:07.326
A Text Catalog is very simple.

00:08:07.866 --> 00:08:09.446
What you do is you provide your

00:08:09.446 --> 00:08:11.446
custom list, and this can be a

00:08:11.446 --> 00:08:13.036
very large list of entities.

00:08:13.276 --> 00:08:14.546
For each of the entities in your

00:08:14.546 --> 00:08:15.666
list, you have a label.

00:08:16.556 --> 00:08:18.476
In practice, these lists can be

00:08:18.476 --> 00:08:20.026
millions or even like, you know,

00:08:20.026 --> 00:08:21.046
a couple of hundred millions.

00:08:21.856 --> 00:08:23.416
What you do is you pass this

00:08:23.676 --> 00:08:25.646
sort of a dictionary into Create

00:08:25.646 --> 00:08:27.596
ML, create an instance of

00:08:27.596 --> 00:08:29.316
MLGazetteer, and Gazetteer is

00:08:29.316 --> 00:08:30.746
just a terminology that we use

00:08:31.026 --> 00:08:32.196
interchangeably with Text

00:08:32.196 --> 00:08:34.015
Catalog, and what you get as an

00:08:34.015 --> 00:08:35.436
output is a Text Catalog.

00:08:35.785 --> 00:08:37.145
Now, this is an extremely

00:08:37.216 --> 00:08:39.006
compressed and efficient form of

00:08:39.006 --> 00:08:40.066
the input dictionary that you

00:08:40.066 --> 00:08:40.456
provided.

00:08:41.015 --> 00:08:43.736
It's very simple to use.

00:08:43.856 --> 00:08:45.056
What you do is you provide this

00:08:45.056 --> 00:08:45.486
dictionary.

00:08:45.486 --> 00:08:46.956
As I mentioned, we can't show

00:08:46.956 --> 00:08:48.066
your million entries here.

00:08:48.066 --> 00:08:49.166
We're just showing as an example

00:08:49.166 --> 00:08:50.546
a few entries, but this can be

00:08:50.546 --> 00:08:52.086
an extremely large dictionary.

00:08:53.406 --> 00:08:54.916
Once you do that, you can create

00:08:54.916 --> 00:08:56.946
an instance of MLGazetteer, pass

00:08:56.946 --> 00:08:58.846
the dictionary, and you write it

00:08:58.846 --> 00:08:59.326
out to disc.

00:08:59.776 --> 00:09:01.026
It looks really innocuous.

00:09:01.026 --> 00:09:02.386
You might be thinking, I'm just

00:09:02.386 --> 00:09:03.386
writing a dictionary to disc.

00:09:03.386 --> 00:09:04.236
What are you doing here?

00:09:05.176 --> 00:09:06.866
Something magical happens when

00:09:06.866 --> 00:09:07.896
you make the right call.

00:09:08.516 --> 00:09:10.536
Create ML calls Natural Language

00:09:10.596 --> 00:09:12.246
under the hood, and Natural

00:09:12.246 --> 00:09:13.916
Language takes this very large

00:09:13.956 --> 00:09:15.566
dictionary and compresses it

00:09:15.706 --> 00:09:17.306
into a bloom filter, which is an

00:09:17.306 --> 00:09:18.286
extremely compact

00:09:18.286 --> 00:09:19.946
representation, and what you get

00:09:19.946 --> 00:09:21.586
as an output is a Text Catalog.

00:09:22.426 --> 00:09:24.046
In fact, we've used this to

00:09:24.046 --> 00:09:25.146
create effect internally.

00:09:25.146 --> 00:09:27.466
We've been able to compress

00:09:27.876 --> 00:09:29.166
almost all the person,

00:09:29.166 --> 00:09:30.986
organization, and location names

00:09:30.986 --> 00:09:32.536
in Wikipedia, which is almost

00:09:32.536 --> 00:09:33.916
two and a half million names,

00:09:34.306 --> 00:09:35.726
into two megabytes on disc.

00:09:36.136 --> 00:09:39.206
Sort of implicitly you've been

00:09:39.206 --> 00:09:40.136
using this model.

00:09:40.356 --> 00:09:41.466
When you call the named entity

00:09:41.466 --> 00:09:42.476
recognition API in

00:09:42.476 --> 00:09:44.306
NaturalLanguage, in conjunction

00:09:44.306 --> 00:09:45.386
with the statistical model,

00:09:45.386 --> 00:09:47.146
you're using this bloom filter

00:09:47.146 --> 00:09:48.636
and Gazetteer and now we are

00:09:48.636 --> 00:09:51.816
bringing that power to you.

00:09:51.896 --> 00:09:53.906
Once you create a Gazetteer or a

00:09:53.906 --> 00:09:55.726
Text Catalog, using it is

00:09:55.726 --> 00:09:57.276
extremely easy.

00:09:58.116 --> 00:09:59.666
You create an instance of

00:09:59.696 --> 00:10:01.376
MLGazetteer by specifying the

00:10:01.376 --> 00:10:03.576
path to the Text Catalog that

00:10:03.576 --> 00:10:06.616
you just wrote out to disc.

00:10:06.616 --> 00:10:08.036
You can work with your favorite

00:10:08.036 --> 00:10:08.706
tag scheme here.

00:10:08.706 --> 00:10:09.826
It can be lexical class.

00:10:09.826 --> 00:10:11.246
It can be name type, any tag

00:10:11.246 --> 00:10:12.836
scheme, and you can simply

00:10:12.836 --> 00:10:14.286
attach your Gazetteer to this

00:10:14.286 --> 00:10:14.756
tag scheme.

00:10:15.636 --> 00:10:17.696
Once you do this, every time you

00:10:17.696 --> 00:10:19.456
have a piece of text, this

00:10:19.456 --> 00:10:20.976
customized Gazetteer is going to

00:10:20.976 --> 00:10:22.736
override the default tags that

00:10:22.736 --> 00:10:23.916
NaturalLanguage provides.

00:10:25.246 --> 00:10:26.886
Consequently, you can customize

00:10:26.886 --> 00:10:27.526
your application.

00:10:28.646 --> 00:10:29.456
Now, if you go back to the

00:10:29.486 --> 00:10:30.876
cheese application, and if you

00:10:30.876 --> 00:10:32.626
have a sentence such as lighter

00:10:32.626 --> 00:10:34.196
than Camembert or Vacherin, you

00:10:34.196 --> 00:10:36.796
can use your Text Catalog for

00:10:36.846 --> 00:10:39.956
cheese and identify that one is

00:10:39.956 --> 00:10:41.116
a French cheese, and the other

00:10:41.116 --> 00:10:42.596
is a Swiss cheese, and you can

00:10:42.596 --> 00:10:44.076
perhaps hyperlink it and create

00:10:44.076 --> 00:10:45.586
a much more cooler application

00:10:45.586 --> 00:10:46.116
out of this.

00:10:46.586 --> 00:10:49.306
So that's one way to use Text

00:10:49.306 --> 00:10:51.686
Catalog in a word tagger in

00:10:51.686 --> 00:10:52.726
NaturalLanguage for this

00:10:52.726 --> 00:10:53.066
release.

00:10:53.626 --> 00:10:56.386
So we've talked about text

00:10:56.386 --> 00:10:57.246
classification.

00:10:57.246 --> 00:10:58.676
We've talked about word tagging.

00:10:59.356 --> 00:11:01.086
But the field of NLP has moved

00:11:01.086 --> 00:11:02.846
significantly in the past few

00:11:02.846 --> 00:11:04.676
years, and there have been the

00:11:04.676 --> 00:11:06.036
without catalysts for this

00:11:06.126 --> 00:11:06.586
change.

00:11:07.436 --> 00:11:08.916
One is the notion of Word

00:11:08.916 --> 00:11:10.876
Embeddings, and Word Embeddings

00:11:10.946 --> 00:11:12.006
are nothing but vector

00:11:12.006 --> 00:11:13.206
representation of words.

00:11:13.206 --> 00:11:15.876
And the other one is the use of

00:11:15.876 --> 00:11:17.986
Neural Networks in NLP.

00:11:19.056 --> 00:11:21.056
We are delighted to tell you

00:11:21.386 --> 00:11:22.576
that we are bringing both these

00:11:22.576 --> 00:11:24.056
things to your apps in

00:11:24.056 --> 00:11:25.366
NaturalLanguage this year.

00:11:25.946 --> 00:11:28.446
So, let's start off with Word

00:11:28.446 --> 00:11:28.956
Embeddings.

00:11:29.436 --> 00:11:30.166
Thank you.

00:11:31.596 --> 00:11:33.606
Before we jump or dive into Word

00:11:33.606 --> 00:11:34.996
Embeddings, I'd like to spend a

00:11:34.996 --> 00:11:36.426
couple of slides talking about

00:11:36.426 --> 00:11:37.726
what an embedding is.

00:11:37.726 --> 00:11:40.336
At a conceptual level, embedding

00:11:40.336 --> 00:11:41.896
is nothing but a mapping from a

00:11:41.896 --> 00:11:43.946
discrete set of objects into a

00:11:43.946 --> 00:11:44.726
continuous vector

00:11:44.726 --> 00:11:45.376
representation.

00:11:46.046 --> 00:11:47.186
So, you have these bunch of

00:11:47.186 --> 00:11:48.026
discrete objects.

00:11:48.466 --> 00:11:50.356
Each object in this set can be

00:11:50.356 --> 00:11:51.816
represented by some sort of a

00:11:51.816 --> 00:11:52.536
finite vector.

00:11:52.536 --> 00:11:54.176
In this example, we're showing

00:11:54.176 --> 00:11:55.466
it with a 3-dimensional vector.

00:11:56.146 --> 00:11:57.096
Three dimensions because it's

00:11:57.146 --> 00:11:59.236
easy to plot and visualize, but

00:11:59.236 --> 00:12:01.176
in reality, these vectors can be

00:12:01.176 --> 00:12:02.476
of arbitrary dimensions.

00:12:02.786 --> 00:12:04.556
You can have 100 dimensions, 300

00:12:04.556 --> 00:12:05.966
dimensions, or in some cases

00:12:06.266 --> 00:12:07.656
even 1000 dimensional vector.

00:12:08.756 --> 00:12:10.146
Now, the neat property about

00:12:10.146 --> 00:12:11.926
these embeddings is that when

00:12:11.926 --> 00:12:13.326
you plot these embeddings,

00:12:14.176 --> 00:12:15.456
objects that are similar

00:12:15.556 --> 00:12:16.896
semantically are clustered

00:12:16.896 --> 00:12:17.316
together.

00:12:18.446 --> 00:12:19.976
So, in this example, if you had

00:12:19.976 --> 00:12:21.306
to look at the paint can and the

00:12:21.306 --> 00:12:22.826
paint roller, they are clustered

00:12:22.826 --> 00:12:23.246
together.

00:12:24.376 --> 00:12:25.716
Or, if you had to look at the

00:12:25.716 --> 00:12:27.166
sneakers and the high heels,

00:12:27.376 --> 00:12:28.276
they are clustered together.

00:12:28.716 --> 00:12:30.206
So, this is a very neat property

00:12:30.206 --> 00:12:30.896
of embeddings.

00:12:31.096 --> 00:12:33.086
And this property of embeddings

00:12:33.086 --> 00:12:34.806
is not only proof of words but

00:12:34.806 --> 00:12:36.046
actually across several

00:12:36.046 --> 00:12:37.126
different modalities.

00:12:37.646 --> 00:12:38.576
You can think of image

00:12:38.576 --> 00:12:39.066
embeddings.

00:12:39.166 --> 00:12:40.826
When you take an image and pass

00:12:40.826 --> 00:12:42.876
it through a VGG network or any

00:12:42.876 --> 00:12:43.946
sort of a convolution Neural

00:12:43.946 --> 00:12:45.456
Network, the feature that you

00:12:45.456 --> 00:12:47.266
get as an output is nothing but

00:12:47.266 --> 00:12:48.046
an image embedding.

00:12:48.986 --> 00:12:49.836
Similarly, you can have

00:12:49.836 --> 00:12:51.696
embeddings for words, for

00:12:51.696 --> 00:12:54.016
phrases, and when you work with

00:12:54.016 --> 00:12:55.676
recommendation systems where

00:12:55.676 --> 00:12:57.136
you're working with song titles

00:12:57.136 --> 00:12:58.656
or product names, they are

00:12:58.656 --> 00:13:00.076
represented by using a vector.

00:13:00.526 --> 00:13:01.456
So, they are nothing but just

00:13:01.506 --> 00:13:02.066
embeddings.

00:13:02.926 --> 00:13:04.326
So, in summary, embedding is

00:13:04.326 --> 00:13:06.506
nothing but a mapping from a

00:13:06.506 --> 00:13:08.376
string into a continuous

00:13:08.376 --> 00:13:09.996
sequence of numbers or a vector

00:13:09.996 --> 00:13:10.556
of numbers.

00:13:11.076 --> 00:13:15.026
We've actually used these

00:13:15.066 --> 00:13:16.836
embeddings very successfully in

00:13:16.836 --> 00:13:18.656
iOS 12, and let me tell you how

00:13:18.656 --> 00:13:19.836
we use this in Photos.

00:13:21.226 --> 00:13:23.236
In Photos search, when you type

00:13:23.316 --> 00:13:24.426
a particular term that you're

00:13:24.426 --> 00:13:26.366
looking for, maybe pictures of a

00:13:26.416 --> 00:13:28.506
thunderstorm, what we do

00:13:28.506 --> 00:13:29.936
underneath the hood is all the

00:13:29.936 --> 00:13:31.756
images in your photo library are

00:13:31.756 --> 00:13:33.086
indexed by using a convolution

00:13:33.086 --> 00:13:33.626
Neural Network.

00:13:33.626 --> 00:13:35.106
And the output of the

00:13:35.106 --> 00:13:36.466
convolution Neural Network is

00:13:36.466 --> 00:13:37.696
fixed to some certain number of

00:13:37.696 --> 00:13:39.566
classes, perhaps to 1000 classes

00:13:39.566 --> 00:13:40.556
or 2000 classes.

00:13:41.426 --> 00:13:42.406
Now, if your convolution Neural

00:13:42.406 --> 00:13:43.666
Network doesn't know what

00:13:43.666 --> 00:13:45.506
thunderstorm is, you will never

00:13:45.506 --> 00:13:46.716
find the pictures that were

00:13:46.716 --> 00:13:48.476
indexed because they didn't have

00:13:48.476 --> 00:13:50.396
the term thunderstorm, but with

00:13:50.396 --> 00:13:52.466
the power of Word Embeddings, we

00:13:52.466 --> 00:13:53.646
know that thunderstorm is

00:13:53.646 --> 00:13:55.076
actually related to sky and

00:13:55.076 --> 00:13:56.986
cloudy, and those are labels

00:13:57.876 --> 00:13:58.936
that your convolution Neural

00:13:58.936 --> 00:13:59.996
Network understands.

00:14:00.386 --> 00:14:02.266
So, by doing this, in iOS 12,

00:14:02.266 --> 00:14:04.366
you're able to enable fuzzy

00:14:04.366 --> 00:14:06.416
search in Photos search by using

00:14:06.416 --> 00:14:07.076
Word Embeddings.

00:14:07.706 --> 00:14:09.396
So, as a consequence of this,

00:14:09.666 --> 00:14:11.676
you can find the images through

00:14:11.676 --> 00:14:12.876
the power of Word Embeddings.

00:14:12.876 --> 00:14:14.006
In fact, it can be applied to

00:14:14.006 --> 00:14:15.216
any search application.

00:14:15.466 --> 00:14:16.716
If you want to do fuzzy search

00:14:16.716 --> 00:14:18.226
and you have a string, you can

00:14:18.226 --> 00:14:19.426
use the Word Embedding to get

00:14:19.426 --> 00:14:20.756
neighbors related to that

00:14:20.756 --> 00:14:21.946
original word.

00:14:22.656 --> 00:14:24.826
Having said that, what can you

00:14:24.826 --> 00:14:25.846
do with an embedding?

00:14:26.676 --> 00:14:27.696
There are four primary

00:14:27.696 --> 00:14:29.066
operations that you can do with

00:14:29.066 --> 00:14:29.716
Word Embeddings.

00:14:30.556 --> 00:14:32.486
One is, given a word, you can

00:14:32.716 --> 00:14:33.926
obviously get the vector for

00:14:33.926 --> 00:14:34.466
that word.

00:14:35.576 --> 00:14:37.486
Given two words, you can find

00:14:37.486 --> 00:14:38.716
the distance between two words

00:14:39.116 --> 00:14:40.356
because for each of those words,

00:14:40.356 --> 00:14:41.156
you can look at the

00:14:41.156 --> 00:14:42.356
corresponding vectors.

00:14:42.616 --> 00:14:44.446
So, if I say dog and cat and ask

00:14:44.446 --> 00:14:45.576
for the distance, I'm going to

00:14:45.576 --> 00:14:46.816
get a distance, and that

00:14:46.816 --> 00:14:47.866
distance is going to be pretty

00:14:47.866 --> 00:14:48.996
close to each other.

00:14:49.886 --> 00:14:51.976
If I say dog and a boot, those

00:14:51.976 --> 00:14:53.706
are fairly distant in the

00:14:53.706 --> 00:14:55.246
semantic space, and you're going

00:14:55.246 --> 00:14:56.426
to get something that's much

00:14:56.606 --> 00:14:57.366
higher distance.

00:14:58.546 --> 00:14:59.696
The third thing that you can do

00:14:59.696 --> 00:15:01.086
is get the nearest neighbors for

00:15:01.086 --> 00:15:02.916
a word, and this probably is by

00:15:02.916 --> 00:15:04.676
far the most popular way of

00:15:04.676 --> 00:15:06.106
using word embeddings, and the

00:15:06.106 --> 00:15:07.516
Photos search application that I

00:15:07.516 --> 00:15:08.886
just showed you was doing

00:15:08.886 --> 00:15:10.236
exactly that.

00:15:10.236 --> 00:15:11.736
Given a word, you're looking for

00:15:11.736 --> 00:15:12.936
nearest neighbors of a word.

00:15:12.936 --> 00:15:15.836
Last but not the least is you

00:15:15.836 --> 00:15:17.056
can also get the nearest

00:15:17.246 --> 00:15:19.376
neighbors for a vector, so let's

00:15:19.376 --> 00:15:20.776
assume that you have a sentence,

00:15:20.836 --> 00:15:22.116
and you have multiple words in

00:15:22.116 --> 00:15:23.726
the sentence, and for each of

00:15:23.726 --> 00:15:25.176
the words in the sentence, you

00:15:25.176 --> 00:15:26.276
can get the word embedding, you

00:15:26.326 --> 00:15:27.516
can sum it up.

00:15:27.516 --> 00:15:28.886
So what you get is a new vector,

00:15:28.886 --> 00:15:31.036
and given that vector, you can

00:15:31.036 --> 00:15:32.466
ask for all the words that are

00:15:32.466 --> 00:15:33.266
close to that vector.

00:15:33.266 --> 00:15:35.046
That's a neat way of using Word

00:15:35.046 --> 00:15:35.576
Embeddings too.

00:15:35.576 --> 00:15:38.186
So a lot of stuff about Word

00:15:38.186 --> 00:15:39.396
Embeddings, but the most

00:15:39.396 --> 00:15:40.476
important thing is we are

00:15:40.476 --> 00:15:42.436
providing this easy for you to

00:15:42.436 --> 00:15:43.786
use on the OS.

00:15:44.076 --> 00:15:45.326
So, we're delighted to tell you

00:15:45.456 --> 00:15:46.906
that these Word Embeddings come

00:15:46.906 --> 00:15:48.396
on the OS in seven different

00:15:48.396 --> 00:15:50.206
languages for you to use, and

00:15:50.206 --> 00:15:51.426
all of the functionalities I

00:15:51.456 --> 00:15:53.366
just mentioned, you can use it

00:15:53.496 --> 00:15:54.946
with one or two lines of code.

00:15:55.386 --> 00:15:56.686
So, we're supporting it in seven

00:15:56.686 --> 00:15:58.406
languages from English, Spanish,

00:15:58.406 --> 00:15:59.686
French, Italian, German,

00:15:59.766 --> 00:16:01.226
Portuguese, and simplified

00:16:01.226 --> 00:16:01.706
Chinese.

00:16:02.986 --> 00:16:04.146
Now, this is great.

00:16:04.146 --> 00:16:05.546
The OS embeddings are generally

00:16:05.626 --> 00:16:07.536
framed on general corpora, large

00:16:07.536 --> 00:16:09.016
amounts of text, billions and

00:16:09.016 --> 00:16:09.866
billions of words.

00:16:10.406 --> 00:16:11.936
So, they have a general notion

00:16:12.056 --> 00:16:14.126
of what a relationship with a

00:16:14.126 --> 00:16:15.156
particular word is.

00:16:15.876 --> 00:16:17.226
But many a time, you want to do

00:16:17.226 --> 00:16:18.216
something that's even more

00:16:18.296 --> 00:16:18.716
custom.

00:16:19.846 --> 00:16:21.016
So, perhaps you're working with

00:16:21.216 --> 00:16:22.366
different sort of domains,

00:16:22.926 --> 00:16:24.206
something in the medical domain

00:16:24.206 --> 00:16:25.566
or the legal domain or the

00:16:25.566 --> 00:16:26.316
financial domain.

00:16:27.046 --> 00:16:28.286
So, if your domain is very

00:16:28.286 --> 00:16:30.316
different, and the vocabulary of

00:16:30.316 --> 00:16:31.916
words that you want to use in

00:16:31.916 --> 00:16:33.076
your application is extremely

00:16:33.076 --> 00:16:34.806
different, or maybe you just

00:16:34.806 --> 00:16:36.076
want to train a word embedding

00:16:36.076 --> 00:16:37.426
for a language that's not

00:16:37.426 --> 00:16:39.746
supported on the OS, how do you

00:16:39.746 --> 00:16:40.886
do that?

00:16:40.886 --> 00:16:41.966
We have a provision for that

00:16:42.076 --> 00:16:42.466
too.

00:16:43.366 --> 00:16:45.096
You can use and bring custom

00:16:45.096 --> 00:16:45.786
word embeddings.

00:16:46.506 --> 00:16:47.246
So, those of you who are

00:16:47.246 --> 00:16:48.466
familiar with word embeddings

00:16:48.466 --> 00:16:50.166
and have seen this field evolve,

00:16:50.166 --> 00:16:51.856
there are many third-party tools

00:16:51.856 --> 00:16:53.576
to train your own embedding such

00:16:53.576 --> 00:16:55.866
as word2vec, GloVe, fasttext.

00:16:56.366 --> 00:16:57.906
So, you can bring your own text

00:16:58.306 --> 00:16:59.666
or you can even use a Custom

00:16:59.666 --> 00:17:01.726
Neural Network that you train

00:17:01.726 --> 00:17:01.793
in Keras TensorFlow or PyTorch.

00:17:01.793 --> 00:17:04.756
So, go from raw data, you can

00:17:04.756 --> 00:17:06.756
build your own embeddings, or

00:17:06.756 --> 00:17:07.935
you can go to any one of these

00:17:07.935 --> 00:17:09.256
websites and download their

00:17:09.256 --> 00:17:10.526
pretrained word embeddings.

00:17:11.205 --> 00:17:13.106
Now, the challenge there is when

00:17:13.106 --> 00:17:14.185
you download any of these

00:17:14.185 --> 00:17:15.425
embeddings, they are very, very

00:17:15.425 --> 00:17:15.866
large.

00:17:16.256 --> 00:17:17.336
They're 1 gigabyte or 2

00:17:17.336 --> 00:17:18.306
gigabytes in size.

00:17:18.695 --> 00:17:19.826
But you want to use it in your

00:17:19.826 --> 00:17:20.896
app in a very compact and

00:17:20.896 --> 00:17:22.945
efficient way, and we do just

00:17:23.006 --> 00:17:23.266
that.

00:17:23.266 --> 00:17:25.336
When you bring these embeddings

00:17:25.606 --> 00:17:26.915
from third-party applications

00:17:26.915 --> 00:17:28.406
and they're really large, we

00:17:28.406 --> 00:17:30.076
automatically compress them into

00:17:30.076 --> 00:17:32.296
a very compact format, and once

00:17:32.296 --> 00:17:33.576
you have this compact format,

00:17:33.746 --> 00:17:35.446
you can use it just like how you

00:17:35.446 --> 00:17:36.716
use the OS embeddings.

00:17:37.436 --> 00:17:38.666
But to tell you how you use

00:17:38.666 --> 00:17:40.206
these embeddings, both the OS as

00:17:40.206 --> 00:17:41.746
well as the custom, I'm going to

00:17:41.746 --> 00:17:42.836
turn it over to Doug, who is

00:17:42.936 --> 00:17:45.666
going to do a demo and then walk

00:17:45.666 --> 00:17:46.426
you through the rest of the

00:17:46.426 --> 00:17:46.786
session.

00:17:47.426 --> 00:17:48.066
Over to you, Doug.

00:17:49.516 --> 00:17:55.836
[ Applause ]

00:17:56.336 --> 00:17:57.036
>> All right.

00:17:57.036 --> 00:17:58.196
So, let's go over to the demo

00:17:58.196 --> 00:18:00.066
machine here, and let's see some

00:18:00.066 --> 00:18:00.976
of this in action.

00:18:01.656 --> 00:18:02.986
So, the first thing I've done

00:18:02.986 --> 00:18:04.796
here is to write a very tiny

00:18:04.796 --> 00:18:06.496
demo application that helps us

00:18:06.496 --> 00:18:08.566
explore Word Embeddings.

00:18:08.606 --> 00:18:10.456
So, I type a word in here, and

00:18:10.456 --> 00:18:11.636
it shows us the nearest

00:18:11.636 --> 00:18:13.676
neighbor, nearest neighbors of

00:18:13.676 --> 00:18:15.296
that word in embedding space.

00:18:15.636 --> 00:18:16.826
Let's start by using the

00:18:16.826 --> 00:18:18.976
built-in OS Word Embeddings for

00:18:18.976 --> 00:18:19.456
English.

00:18:19.806 --> 00:18:21.466
So, I type a word like chair,

00:18:21.666 --> 00:18:23.076
and we see the nearest neighbors

00:18:23.076 --> 00:18:24.476
of chair are words that are

00:18:24.476 --> 00:18:25.886
similar in meaning to chair,

00:18:25.886 --> 00:18:28.246
sofa, couch, and so forth, or I

00:18:28.246 --> 00:18:29.636
could type in something like

00:18:29.716 --> 00:18:31.116
bicycle.

00:18:31.796 --> 00:18:33.466
And the nearest neighbors, bike

00:18:33.466 --> 00:18:34.666
and motorcycle and so forth,

00:18:34.666 --> 00:18:36.166
these are words that are close

00:18:36.166 --> 00:18:38.596
in meaning to bicycle or maybe

00:18:39.216 --> 00:18:39.596
book.

00:18:40.406 --> 00:18:41.256
And we get words that are

00:18:41.256 --> 00:18:42.576
similar in meaning to book.

00:18:43.026 --> 00:18:44.616
So, what we can see from this,

00:18:44.936 --> 00:18:46.746
we can understand that the

00:18:46.746 --> 00:18:48.316
built-in OS Word Embeddings

00:18:48.716 --> 00:18:50.826
represent the ordinary meanings

00:18:50.826 --> 00:18:52.936
of words and the language and

00:18:53.006 --> 00:18:54.736
recognize the similarity of

00:18:54.776 --> 00:18:57.876
meanings as expressed in general

00:18:57.876 --> 00:18:59.206
text in that language.

00:19:00.206 --> 00:19:02.596
But, of course, what I'm really

00:19:02.596 --> 00:19:04.686
interested in here is knowing

00:19:05.066 --> 00:19:06.216
what do these embeddings

00:19:06.216 --> 00:19:08.916
understand about cheese, because

00:19:08.916 --> 00:19:09.976
we're dealing with a cheese

00:19:09.976 --> 00:19:10.726
application here.

00:19:11.426 --> 00:19:12.936
So, let me type in a cheese

00:19:12.936 --> 00:19:13.246
word.

00:19:14.556 --> 00:19:16.156
And take a look here, and what I

00:19:16.156 --> 00:19:18.996
can see right away is that these

00:19:18.996 --> 00:19:21.446
built-in embeddings do know what

00:19:21.446 --> 00:19:23.846
cheese is, but I'm very

00:19:23.846 --> 00:19:24.506
disappointed.

00:19:24.966 --> 00:19:26.196
I can see these embeddings know

00:19:26.196 --> 00:19:27.566
nothing about the finer points

00:19:27.566 --> 00:19:28.206
of cheese.

00:19:29.406 --> 00:19:31.086
Otherwise, they would never have

00:19:31.086 --> 00:19:32.796
put these particular cheeses and

00:19:32.796 --> 00:19:34.086
cheese-related things together.

00:19:34.086 --> 00:19:35.436
They don't go together at all.

00:19:36.236 --> 00:19:37.706
What I really want here is

00:19:37.706 --> 00:19:39.136
something that understands the

00:19:39.136 --> 00:19:40.486
relationships of cheeses.

00:19:40.776 --> 00:19:42.046
So, I've taken the opportunity

00:19:42.046 --> 00:19:44.316
to train my own custom cheese

00:19:44.316 --> 00:19:46.446
embedding that puts cheeses

00:19:46.446 --> 00:19:47.316
together based on their

00:19:47.316 --> 00:19:48.016
similarity.

00:19:48.936 --> 00:19:50.116
And let's switch over to it.

00:19:51.436 --> 00:19:52.616
So, here are the neighbors of

00:19:52.616 --> 00:19:54.276
cheddar in my own custom cheese

00:19:54.276 --> 00:19:54.716
embedding.

00:19:54.966 --> 00:19:55.806
This is much better.

00:19:56.586 --> 00:19:57.786
We can see that it puts near

00:19:57.786 --> 00:19:59.646
cheddar, it puts some fine

00:20:00.216 --> 00:20:01.596
cheeses that are similar to

00:20:01.596 --> 00:20:03.446
cheddar in texture like our

00:20:03.446 --> 00:20:05.096
Lancashires, Double Gloucester,

00:20:05.096 --> 00:20:05.676
and Cheshire.

00:20:06.666 --> 00:20:07.826
So this is something that we can

00:20:07.826 --> 00:20:09.356
use in our cheese application.

00:20:09.946 --> 00:20:10.896
So, let's take a look at the

00:20:10.896 --> 00:20:12.886
cheese application now.

00:20:15.426 --> 00:20:16.476
So, I've been trying out some

00:20:16.476 --> 00:20:17.646
ideas for our cheese

00:20:17.646 --> 00:20:18.266
application.

00:20:18.266 --> 00:20:20.676
Let's see how this looks.

00:20:21.226 --> 00:20:22.276
So, when the user types

00:20:22.276 --> 00:20:23.406
something in, the first thing

00:20:23.406 --> 00:20:25.486
I'm going to do is get a

00:20:25.486 --> 00:20:27.826
sentiment score on it to see and

00:20:27.826 --> 00:20:30.066
check does this represent a

00:20:30.066 --> 00:20:31.446
sentence with a positive

00:20:31.446 --> 00:20:34.146
sentiment, and if it does, then

00:20:34.146 --> 00:20:35.436
I'm going to go through it using

00:20:35.436 --> 00:20:39.156
my tagger using, of course, our

00:20:39.156 --> 00:20:41.776
custom cheese Gazetteer to see

00:20:41.776 --> 00:20:43.116
whether the user mentioned any

00:20:43.146 --> 00:20:44.656
cheeses in it.

00:20:45.356 --> 00:20:47.076
And so I'll look for a cheese,

00:20:47.126 --> 00:20:48.826
and if the user did mention a

00:20:48.826 --> 00:20:50.736
cheese, then I'm going to pass

00:20:50.736 --> 00:20:52.296
it through my custom cheese

00:20:52.296 --> 00:20:55.146
embedding to find similar

00:20:55.146 --> 00:20:56.366
related cheeses.

00:20:57.426 --> 00:20:58.336
Sounds plausible?

00:20:58.336 --> 00:21:01.386
Let's try it out.

00:21:01.656 --> 00:21:02.726
Let's bring up our cheese

00:21:02.726 --> 00:21:08.816
application, and so I visited

00:21:08.816 --> 00:21:10.136
the Netherlands last year, and I

00:21:10.136 --> 00:21:11.796
fell in love with Dutch cheeses,

00:21:11.796 --> 00:21:15.826
so I'm going to tell my app

00:21:16.636 --> 00:21:16.986
that.

00:21:16.986 --> 00:21:19.616
So, this is a certainly a

00:21:19.616 --> 00:21:20.686
sentence with a positive

00:21:20.686 --> 00:21:22.716
sentiment, and it does reference

00:21:22.716 --> 00:21:24.006
a particular cheese.

00:21:24.416 --> 00:21:27.586
So, I go through, and now my app

00:21:27.876 --> 00:21:29.336
can make recommendations of

00:21:29.336 --> 00:21:31.216
cheeses that are similar to the

00:21:31.216 --> 00:21:32.296
one that I mentioned here.

00:21:33.376 --> 00:21:35.186
So, this shows of the power of

00:21:35.226 --> 00:21:36.946
Word Embeddings, but even more

00:21:36.946 --> 00:21:39.166
than that, it shows how the

00:21:39.166 --> 00:21:40.626
natural, various NaturalLanguage

00:21:40.626 --> 00:21:43.566
APIs can come together for

00:21:43.566 --> 00:21:44.906
application functionality.

00:21:46.516 --> 00:21:50.736
[ Applause ]

00:21:51.236 --> 00:21:52.656
So, now, let's go back to the

00:21:52.656 --> 00:21:54.976
slides, and I want to review

00:21:54.976 --> 00:21:57.496
briefly how this looks in API.

00:21:58.466 --> 00:22:00.546
So, if you want to use a

00:22:00.666 --> 00:22:02.926
built-in OS Word Embedding, it's

00:22:02.926 --> 00:22:03.546
very simple.

00:22:03.806 --> 00:22:04.996
All you do is ask for it.

00:22:05.206 --> 00:22:06.696
Ask for the word embedding for a

00:22:06.696 --> 00:22:07.876
particular language, and we'll

00:22:07.876 --> 00:22:08.346
give it to you.

00:22:08.346 --> 00:22:10.976
Once you have one of these NL

00:22:10.976 --> 00:22:12.766
embedding objects, there are

00:22:12.766 --> 00:22:13.936
various things you can do with

00:22:13.936 --> 00:22:14.106
it.

00:22:14.536 --> 00:22:15.736
You can, of course, get the

00:22:15.736 --> 00:22:16.886
components, the vector

00:22:16.886 --> 00:22:18.866
components, corresponding to any

00:22:18.866 --> 00:22:19.856
particular entry.

00:22:21.136 --> 00:22:22.476
You can find the distance

00:22:22.476 --> 00:22:24.326
between two words, be it short

00:22:24.326 --> 00:22:26.436
or far, in embedding space.

00:22:27.346 --> 00:22:28.936
And as we saw in our cheese

00:22:28.936 --> 00:22:30.676
application, you can go through

00:22:30.676 --> 00:22:32.166
and find the nearest neighbors

00:22:32.596 --> 00:22:35.296
of any particular item in this

00:22:35.426 --> 00:22:37.186
embedding space.

00:22:37.796 --> 00:22:40.436
If you want to use a custom word

00:22:40.436 --> 00:22:43.346
embedding, then to create it,

00:22:43.346 --> 00:22:46.496
you go over to Create ML, and

00:22:46.546 --> 00:22:48.486
you need, of course, all of the

00:22:48.486 --> 00:22:50.316
vectors that represent your

00:22:50.316 --> 00:22:50.846
embedding.

00:22:51.186 --> 00:22:52.596
I can't really show them all to

00:22:52.596 --> 00:22:53.986
you right here in the slide

00:22:53.986 --> 00:22:55.536
because there are 50 or 100

00:22:55.856 --> 00:22:57.736
components long, but here's an

00:22:57.736 --> 00:22:59.906
example of what they look like.

00:22:59.906 --> 00:23:01.146
In practice, you're probably

00:23:01.176 --> 00:23:02.396
going to be bringing them in

00:23:02.396 --> 00:23:04.486
from a file using the various

00:23:04.486 --> 00:23:06.656
Create ML facilities for loading

00:23:06.656 --> 00:23:07.746
data from files.

00:23:08.076 --> 00:23:12.476
And then you just create a word

00:23:12.476 --> 00:23:15.336
embedding object from it and

00:23:15.336 --> 00:23:16.376
write it out to disc.

00:23:16.376 --> 00:23:18.056
Now, what's going on when you do

00:23:18.056 --> 00:23:18.376
this?

00:23:19.036 --> 00:23:22.256
Well, in practice, these

00:23:22.256 --> 00:23:23.796
embeddings tend to be quite

00:23:23.796 --> 00:23:25.936
large, hundreds of dimensions

00:23:25.936 --> 00:23:27.206
times thousands of entries.

00:23:27.206 --> 00:23:30.066
It could be huge, and they could

00:23:30.066 --> 00:23:32.206
take a lot of space on disc,

00:23:32.776 --> 00:23:34.846
naively and be expensive to

00:23:34.846 --> 00:23:35.286
search.

00:23:35.856 --> 00:23:38.336
But when you compile them into

00:23:38.336 --> 00:23:41.416
our word embedding object, then

00:23:41.416 --> 00:23:43.836
under the hood what we do is we

00:23:43.836 --> 00:23:46.346
use a product quantization

00:23:46.346 --> 00:23:47.906
technique to achieve a high

00:23:47.906 --> 00:23:50.856
degree of compression, and we

00:23:50.856 --> 00:23:53.336
add indexes so you can do fast

00:23:53.506 --> 00:23:55.456
searching for nearest neighbors,

00:23:55.456 --> 00:23:56.926
as you saw in our examples.

00:23:57.606 --> 00:23:59.196
Just try this out.

00:23:59.626 --> 00:24:00.926
We took some very large

00:24:00.926 --> 00:24:02.766
embeddings that are readily

00:24:02.766 --> 00:24:04.256
available as open source.

00:24:04.446 --> 00:24:06.456
This is some GloVe and fasttext

00:24:06.456 --> 00:24:06.916
embeddings.

00:24:06.916 --> 00:24:08.876
These are a gigabyte or 2

00:24:08.876 --> 00:24:11.046
gigabytes in uncompressed form.

00:24:11.686 --> 00:24:13.956
When we put them into our NL

00:24:13.956 --> 00:24:15.606
embedding compressed format,

00:24:16.206 --> 00:24:17.736
they're only tens of megabytes,

00:24:18.136 --> 00:24:19.086
and you can search through them

00:24:19.086 --> 00:24:20.146
for nearest neighbors in just a

00:24:20.146 --> 00:24:21.176
couple of milliseconds.

00:24:23.056 --> 00:24:23.976
Closer to home--

00:24:24.516 --> 00:24:27.716
[ Applause ]

00:24:28.216 --> 00:24:30.146
For an example, closer to home,

00:24:30.266 --> 00:24:32.406
Apple does a lot with podcasts,

00:24:32.536 --> 00:24:34.516
so our podcast group, we talked

00:24:34.516 --> 00:24:36.616
to them, and they, as it

00:24:36.616 --> 00:24:38.226
happens, have an embedding for

00:24:38.226 --> 00:24:40.426
podcasts that represents the

00:24:40.426 --> 00:24:42.786
similarity of various podcasts,

00:24:42.786 --> 00:24:43.686
one to another.

00:24:44.346 --> 00:24:46.196
So, we thought we'd try it out

00:24:46.226 --> 00:24:48.186
and see what would happen if we

00:24:48.186 --> 00:24:49.886
took this embedding and put it

00:24:49.886 --> 00:24:51.826
into our NL embedding format.

00:24:52.456 --> 00:24:54.156
So this embedding represents

00:24:54.156 --> 00:24:57.466
66,000 different podcasts, and

00:24:57.466 --> 00:24:59.486
source form is 167 megabytes,

00:24:59.526 --> 00:25:00.906
but we compress it down to just

00:25:00.906 --> 00:25:02.976
3 megabytes on disc.

00:25:03.326 --> 00:25:05.716
So what NL embedding does is it

00:25:05.716 --> 00:25:08.246
makes it practical to include

00:25:08.246 --> 00:25:10.376
these embeddings and use them on

00:25:11.036 --> 00:25:12.616
, on the device, in your

00:25:12.616 --> 00:25:13.236
application.

00:25:16.716 --> 00:25:17.276
All right.

00:25:17.996 --> 00:25:19.716
So, next I want to switch and

00:25:19.716 --> 00:25:22.646
talk about another thing that's

00:25:22.646 --> 00:25:24.976
related to Word Embeddings, and

00:25:24.976 --> 00:25:26.886
that is Transfer Learning for

00:25:26.956 --> 00:25:28.036
Text Classification.

00:25:29.356 --> 00:25:31.886
So, I'd like to start by talking

00:25:31.886 --> 00:25:33.516
a little bit about what we do,

00:25:33.516 --> 00:25:36.806
what it is we do when we train a

00:25:36.806 --> 00:25:37.936
text classifier.

00:25:39.176 --> 00:25:40.886
So, when we're training a text

00:25:40.886 --> 00:25:44.076
classifier, we give it a set of

00:25:44.076 --> 00:25:47.096
examples for various classes.

00:25:47.636 --> 00:25:50.476
You can pass that in to Create

00:25:50.476 --> 00:25:50.826
ML.

00:25:50.826 --> 00:25:52.346
Create ML will call on Natural

00:25:52.346 --> 00:25:52.946
Language.

00:25:53.506 --> 00:25:55.766
We will trained a classifier and

00:25:55.876 --> 00:25:58.136
out will come a Core ML model.

00:25:58.506 --> 00:26:00.686
And what we hope is that these

00:26:00.686 --> 00:26:04.256
examples will give sufficient

00:26:04.576 --> 00:26:06.626
information about the various

00:26:06.686 --> 00:26:08.546
classes that the model can

00:26:08.546 --> 00:26:11.916
generalize to classify examples

00:26:11.916 --> 00:26:12.926
that it hasn't seen.

00:26:13.406 --> 00:26:15.126
And, of course, we have already

00:26:15.366 --> 00:26:17.786
shipped this last year, and we

00:26:17.786 --> 00:26:20.566
have algorithms for training

00:26:20.566 --> 00:26:23.136
these models, most notably our

00:26:23.136 --> 00:26:24.846
standard algorithm is what we

00:26:24.846 --> 00:26:26.926
call the maxEnt algorithm, based

00:26:26.926 --> 00:26:27.966
on logistic compression.

00:26:28.336 --> 00:26:30.266
It's very fast, robust, and

00:26:30.266 --> 00:26:30.816
effective.

00:26:31.666 --> 00:26:35.126
But one thing about it is it

00:26:35.126 --> 00:26:36.876
doesn't know anything except

00:26:36.876 --> 00:26:39.076
what it learns from the training

00:26:39.076 --> 00:26:41.846
data that you give it.

00:26:42.116 --> 00:26:45.566
So, you have to make sure that

00:26:45.566 --> 00:26:46.876
the training material you give

00:26:46.876 --> 00:26:50.736
it covers essentially all of the

00:26:50.736 --> 00:26:52.296
sort of things that you expect

00:26:52.296 --> 00:26:55.156
to see in examples in practice.

00:26:55.386 --> 00:26:57.336
So, in some sense, we've done

00:26:57.336 --> 00:26:58.736
the easy part of creating the

00:26:58.736 --> 00:27:00.136
algorithm and left you the hard

00:27:00.136 --> 00:27:02.276
part of producing the training

00:27:02.966 --> 00:27:03.096
data.

00:27:03.716 --> 00:27:06.916
But, wouldn't it be nice if we

00:27:06.916 --> 00:27:09.216
could take advantage of prior

00:27:09.216 --> 00:27:11.386
knowledge of the language and

00:27:12.316 --> 00:27:14.736
then maybe use that in

00:27:14.766 --> 00:27:16.966
conjunction with some smaller

00:27:16.966 --> 00:27:18.526
amount of training material that

00:27:18.526 --> 00:27:21.366
you have to provide in order to

00:27:21.366 --> 00:27:24.616
train a model that would combine

00:27:24.696 --> 00:27:26.886
these two and so hopefully

00:27:26.886 --> 00:27:28.736
understand more about the

00:27:28.736 --> 00:27:31.176
examples it's going to see with

00:27:31.176 --> 00:27:32.496
less in the way of training

00:27:32.496 --> 00:27:33.046
material.

00:27:34.426 --> 00:27:36.686
And so this is the promise of

00:27:36.856 --> 00:27:37.866
Transfer Learning.

00:27:38.576 --> 00:27:41.036
This is a highly active research

00:27:41.036 --> 00:27:43.876
area in NLP, and I'm happy to

00:27:43.876 --> 00:27:45.786
say we have a solution for this

00:27:46.136 --> 00:27:47.326
that we're delivering now.

00:27:48.286 --> 00:27:50.426
Again, NaturalLanguage trains a

00:27:50.426 --> 00:27:52.396
model, and the outcome is a Core

00:27:52.396 --> 00:27:52.946
ML model.

00:27:53.326 --> 00:27:54.916
But how are we going to

00:27:54.916 --> 00:27:56.966
incorporate previous knowledge

00:27:56.966 --> 00:27:57.696
of the language?

00:27:58.026 --> 00:27:58.886
Where are we going to get it?

00:27:59.946 --> 00:28:03.966
Well, Word Embeddings provide a

00:28:04.006 --> 00:28:05.806
great deal of knowledge of the

00:28:05.806 --> 00:28:06.376
language.

00:28:06.376 --> 00:28:07.656
In particular, they know quite a

00:28:07.656 --> 00:28:10.626
bit about the meaning of words.

00:28:11.296 --> 00:28:13.996
So, our solution uses Word

00:28:13.996 --> 00:28:16.026
Embeddings, takes the training

00:28:16.026 --> 00:28:18.056
material you provide, puts them

00:28:18.056 --> 00:28:19.466
through the Word Embeddings, and

00:28:19.466 --> 00:28:21.686
then on top of that we train a

00:28:21.686 --> 00:28:25.596
Neural Network model, and that

00:28:26.096 --> 00:28:27.986
is what we provide as a Transfer

00:28:27.986 --> 00:28:30.026
Learning Text Classification

00:28:30.026 --> 00:28:30.456
model.

00:28:31.556 --> 00:28:32.686
Now, there's a lot of work going

00:28:32.686 --> 00:28:34.716
on here, but if you want to use

00:28:34.716 --> 00:28:36.786
it, all you have to do is ask

00:28:36.786 --> 00:28:37.156
for it.

00:28:38.496 --> 00:28:42.416
You just change one parameter in

00:28:42.416 --> 00:28:43.566
the specification of the

00:28:43.566 --> 00:28:45.436
algorithm that you want when

00:28:45.436 --> 00:28:46.516
training a Transfer Learning

00:28:46.516 --> 00:28:46.876
model.

00:28:47.216 --> 00:28:48.566
Now, there are a few different

00:28:48.566 --> 00:28:49.366
options here.

00:28:50.056 --> 00:28:52.776
So, the first one, most obvious

00:28:52.776 --> 00:28:54.696
is, you can use the built-in OS

00:28:54.696 --> 00:28:56.806
Word Embeddings that represent

00:28:57.296 --> 00:29:00.486
the ordinary meaning of words,

00:29:00.656 --> 00:29:02.696
and if you have a custom Word

00:29:02.696 --> 00:29:03.936
Embeddings, you could also use

00:29:03.936 --> 00:29:06.106
that as well.

00:29:06.306 --> 00:29:10.066
We know that a given word can

00:29:10.066 --> 00:29:11.236
have very different meanings,

00:29:11.236 --> 00:29:12.456
depending on how it appears in

00:29:12.456 --> 00:29:13.586
the context of a sentence.

00:29:14.016 --> 00:29:15.316
So, for example, Apple in these

00:29:15.316 --> 00:29:16.526
two sentences has very different

00:29:16.526 --> 00:29:16.896
meanings.

00:29:18.206 --> 00:29:20.586
So, what we'd like is to have an

00:29:20.586 --> 00:29:22.356
embedding for Transfer Learning

00:29:22.356 --> 00:29:24.766
purposes that gives different

00:29:25.386 --> 00:29:28.816
values for these words depending

00:29:28.816 --> 00:29:31.226
on their meaning and context.

00:29:31.226 --> 00:29:32.366
And, of course, an ordinary word

00:29:32.366 --> 00:29:34.966
embedding, it just maps words to

00:29:34.966 --> 00:29:37.306
vectors, and it will give the

00:29:37.306 --> 00:29:39.036
same value for the word no

00:29:39.036 --> 00:29:40.076
matter how it appears.

00:29:41.466 --> 00:29:44.876
But, what we have done is

00:29:44.876 --> 00:29:46.866
trained a specialized embedding

00:29:46.976 --> 00:29:50.356
that gives different values for

00:29:50.356 --> 00:29:51.656
the words depending on their

00:29:51.656 --> 00:29:52.866
meaning and context.

00:29:53.446 --> 00:29:54.886
Now, to give you some idea of

00:29:54.886 --> 00:29:56.086
how fast the field is moving,

00:29:56.086 --> 00:29:57.456
this is something that was just

00:29:57.456 --> 00:30:00.166
researched a year ago, and we're

00:30:00.166 --> 00:30:03.156
delivering it now.

00:30:03.376 --> 00:30:04.796
And again, if you want to use

00:30:04.796 --> 00:30:06.546
it, you just ask for it.

00:30:07.076 --> 00:30:08.486
You specify the dynamic

00:30:08.486 --> 00:30:08.956
embedding.

00:30:09.186 --> 00:30:10.426
So, the dynamic embedding

00:30:10.956 --> 00:30:13.366
changes the value of the

00:30:13.366 --> 00:30:15.106
embedding for words depending on

00:30:15.106 --> 00:30:17.766
their sentence context, and this

00:30:17.766 --> 00:30:20.016
is a very powerful technique for

00:30:20.016 --> 00:30:21.566
doing Transfer Learning for Test

00:30:21.566 --> 00:30:22.296
Classification.

00:30:23.546 --> 00:30:25.206
Well, let's see it.

00:30:27.056 --> 00:30:27.676
All right.

00:30:27.906 --> 00:30:30.716
So, here what I have is some

00:30:30.716 --> 00:30:32.636
fairly standard code for

00:30:32.676 --> 00:30:34.566
training a Text Classifier using

00:30:34.566 --> 00:30:37.236
Create ML, and what I'm going to

00:30:37.236 --> 00:30:39.056
be training, this is based on a

00:30:39.056 --> 00:30:41.326
dataset using an Open Source

00:30:41.766 --> 00:30:43.836
encyclopedia called DBpedia.

00:30:44.676 --> 00:30:46.486
It has short entries about many

00:30:46.486 --> 00:30:47.426
different topics.

00:30:47.496 --> 00:30:49.456
Some of them are people,

00:30:49.456 --> 00:30:51.096
artists, writers, plants,

00:30:51.096 --> 00:30:52.616
animals, and so forth, and the

00:30:52.616 --> 00:30:55.146
task here is to determine from

00:30:55.146 --> 00:30:56.746
the entry what the

00:30:56.876 --> 00:30:59.186
classification is, whether it's

00:30:59.186 --> 00:31:01.396
a person or writer or artist,

00:31:01.396 --> 00:31:01.886
etc.

00:31:01.956 --> 00:31:04.186
And there are 14 different

00:31:04.186 --> 00:31:05.546
classes here, and I'm going to

00:31:05.546 --> 00:31:07.036
try and train a classifier using

00:31:07.036 --> 00:31:08.406
only 200 examples.

00:31:08.906 --> 00:31:11.016
So, it's a fairly difficult task

00:31:11.016 --> 00:31:13.766
here, and so let's just-- let's

00:31:13.766 --> 00:31:15.656
try it with our existing maxEnt

00:31:15.776 --> 00:31:16.236
model.

00:31:17.356 --> 00:31:19.006
So, I'll fire it off.

00:31:19.436 --> 00:31:21.316
It starts, and it's done.

00:31:22.776 --> 00:31:25.296
Very fast and easy, and on our

00:31:25.296 --> 00:31:29.026
training-- if we take a look at

00:31:29.026 --> 00:31:30.966
our performance on our test set,

00:31:31.286 --> 00:31:34.116
we see it got 77% accuracy.

00:31:35.116 --> 00:31:39.566
That's okay, but can we do

00:31:39.676 --> 00:31:39.796
better?

00:31:39.796 --> 00:31:41.756
Well, let's make one little

00:31:41.756 --> 00:31:43.926
change to our code here.

00:31:44.266 --> 00:31:46.856
Instead of using the maxEnt

00:31:46.856 --> 00:31:47.986
model, we're going to use the

00:31:47.986 --> 00:31:49.526
Transfer Learning with Dynamic

00:31:49.526 --> 00:31:53.266
Embeddings, and let's start it.

00:31:54.736 --> 00:31:56.186
Now, as I mentioned, this is

00:31:56.186 --> 00:31:57.956
training a Neural Network model,

00:31:58.136 --> 00:31:59.526
so it takes a little bit longer,

00:32:00.256 --> 00:32:02.606
so while it's training, let's

00:32:02.606 --> 00:32:04.286
just take a little closer look

00:32:04.326 --> 00:32:05.596
at the data that we're training

00:32:05.596 --> 00:32:05.916
on.

00:32:06.326 --> 00:32:08.196
As it turns out, when you're

00:32:08.196 --> 00:32:09.646
training Neural Network models,

00:32:09.916 --> 00:32:11.556
it's important to pay close

00:32:11.556 --> 00:32:13.616
attention to the data that you

00:32:13.616 --> 00:32:14.216
train it on.

00:32:14.596 --> 00:32:17.166
So, notice that this data is a

00:32:17.276 --> 00:32:19.386
random sample across the various

00:32:19.386 --> 00:32:22.646
classes, and I've arranged it so

00:32:22.646 --> 00:32:23.846
that they're roughly the same

00:32:23.846 --> 00:32:25.996
number of instances for each

00:32:26.046 --> 00:32:26.676
class.

00:32:27.186 --> 00:32:28.346
It's a balanced set.

00:32:29.366 --> 00:32:30.626
This is our training set.

00:32:30.626 --> 00:32:31.926
In addition, we also have a

00:32:31.926 --> 00:32:34.146
separate validation set that is

00:32:34.146 --> 00:32:36.226
similarly a random sampling of

00:32:36.226 --> 00:32:38.546
examples across classes, maybe

00:32:38.546 --> 00:32:39.756
not quite as large as the

00:32:39.756 --> 00:32:41.296
training set, but balanced.

00:32:41.986 --> 00:32:43.246
And the validation set is

00:32:43.306 --> 00:32:44.886
particularly important in this

00:32:44.886 --> 00:32:45.536
kind of training.

00:32:45.996 --> 00:32:48.466
Neural Network training has a

00:32:48.516 --> 00:32:50.846
tendency to over fitting, where

00:32:50.846 --> 00:32:52.666
it more or less memorizes the

00:32:52.666 --> 00:32:53.916
training material and doesn't

00:32:54.056 --> 00:32:54.816
generalize.

00:32:55.096 --> 00:32:56.616
The validation set helps keep it

00:32:56.616 --> 00:32:57.976
honest and make sure it

00:32:57.976 --> 00:32:59.316
continues to generalize.

00:33:00.146 --> 00:33:01.376
And then, of course, we also

00:33:01.376 --> 00:33:03.946
have a separate test set that's

00:33:03.946 --> 00:33:05.846
similarly randomly sampled and

00:33:05.846 --> 00:33:08.056
balanced, and of course there

00:33:08.056 --> 00:33:10.196
can't be any overlap between the

00:33:10.196 --> 00:33:12.006
training validation test sets.

00:33:12.006 --> 00:33:12.796
That would be cheating.

00:33:13.966 --> 00:33:16.606
And the test set we need to see

00:33:16.606 --> 00:33:17.576
how well we're doing, in

00:33:17.576 --> 00:33:19.586
particular in this case, we need

00:33:19.586 --> 00:33:22.296
it so that we can see whether

00:33:22.296 --> 00:33:24.646
the Transfer Learning model is

00:33:24.646 --> 00:33:27.766
doing better than our maxEnt

00:33:27.766 --> 00:33:28.126
model.

00:33:28.596 --> 00:33:29.866
And it looks like it's finished

00:33:29.866 --> 00:33:31.046
now, so let's take a look and

00:33:31.046 --> 00:33:31.326
see.

00:33:31.606 --> 00:33:36.486
And we can see here that the

00:33:36.486 --> 00:33:38.576
Transfer Learning has achieved

00:33:38.576 --> 00:33:41.106
an accuracy of 86.5%, much

00:33:41.186 --> 00:33:45.976
better than our maxEnt model.

00:33:46.516 --> 00:33:51.796
[ Applause ]

00:33:52.296 --> 00:33:55.996
So, how does this apply to our

00:33:55.996 --> 00:33:56.976
cheese application?

00:33:57.056 --> 00:33:59.536
Well, what I've done is I've

00:33:59.576 --> 00:34:01.746
taken my cheese tasting notes,

00:34:03.146 --> 00:34:05.726
and I labeled them each by the

00:34:05.756 --> 00:34:07.116
cheese that they referred to,

00:34:07.396 --> 00:34:09.626
and I use that to train a cheese

00:34:09.626 --> 00:34:10.676
classifier model.

00:34:10.676 --> 00:34:13.196
And so my cheese classifier

00:34:13.196 --> 00:34:16.146
model can take a sentence and

00:34:16.246 --> 00:34:18.216
try to classify it and determine

00:34:18.216 --> 00:34:20.156
which cheese it most closely

00:34:20.156 --> 00:34:20.866
refers to.

00:34:21.016 --> 00:34:24.036
And I put that into my cheese

00:34:24.036 --> 00:34:27.936
application, and so, what I'm

00:34:27.936 --> 00:34:28.826
going to do in the cheese

00:34:28.826 --> 00:34:32.366
application is if the user

00:34:32.366 --> 00:34:33.545
didn't refer to a specific

00:34:33.585 --> 00:34:34.936
cheese, then I'm going to try to

00:34:34.936 --> 00:34:36.136
figure out which cheese they

00:34:36.136 --> 00:34:36.746
might want.

00:34:37.255 --> 00:34:39.456
And all I do is ask the model

00:34:39.456 --> 00:34:41.306
for the label for the text.

00:34:42.076 --> 00:34:43.226
And very simple.

00:34:43.226 --> 00:34:45.000
Let's try it out.

00:35:01.046 --> 00:35:02.336
So, I'm going to type in

00:35:02.336 --> 00:35:03.000
something here.

00:35:15.486 --> 00:35:18.936
And we'll let the cheese

00:35:18.936 --> 00:35:20.966
classifier work on it.

00:35:21.516 --> 00:35:23.536
And so the cheese classifier

00:35:23.696 --> 00:35:24.996
determined that this is most

00:35:25.046 --> 00:35:26.716
closely resembling Camembert,

00:35:27.066 --> 00:35:29.066
and then my cheese embedding has

00:35:29.066 --> 00:35:30.516
recommended some other cheeses

00:35:30.516 --> 00:35:31.936
that are very similar, Brie and

00:35:31.936 --> 00:35:32.756
so on and so forth.

00:35:34.156 --> 00:35:36.000
Or, maybe I say--

00:35:45.386 --> 00:35:46.826
Something firm and sharp and it

00:35:46.826 --> 00:35:47.946
recognizes that as resembling

00:35:48.286 --> 00:35:49.826
cheddar, and it recommends some

00:35:50.036 --> 00:35:50.966
similar cheeses to us.

00:35:51.001 --> 00:35:53.001
[applause]

00:35:53.036 --> 00:35:54.386
So, this again shows us the

00:35:54.426 --> 00:35:57.596
power of Text Classification in

00:35:57.596 --> 00:35:59.746
combination with the other

00:35:59.956 --> 00:36:01.486
NaturalLanguage APIs.

00:36:01.736 --> 00:36:03.276
So, I'd like to finish off with

00:36:03.276 --> 00:36:05.366
some considerations for the use

00:36:05.456 --> 00:36:07.656
of the Text Classification.

00:36:07.656 --> 00:36:09.266
First of all, we need to note

00:36:09.266 --> 00:36:10.756
the languages that are supported

00:36:11.166 --> 00:36:12.846
for transfer learning, either

00:36:12.846 --> 00:36:14.956
via Static Embeddings or the

00:36:14.956 --> 00:36:17.746
special Dynamic Embeddings that

00:36:17.886 --> 00:36:19.576
take context into account.

00:36:20.026 --> 00:36:23.196
And then I want to talk a bit

00:36:23.196 --> 00:36:25.056
about, more about data.

00:36:26.326 --> 00:36:28.446
So, the first commandment when

00:36:28.446 --> 00:36:29.626
dealing with data is that you

00:36:29.626 --> 00:36:31.676
need to understand the domain

00:36:31.676 --> 00:36:32.296
you're working with.

00:36:33.506 --> 00:36:35.036
What kind of text do you expect

00:36:35.036 --> 00:36:36.176
to see in practice?

00:36:36.396 --> 00:36:37.626
Is it going to be sentence

00:36:37.626 --> 00:36:39.086
fragments, full sentences,

00:36:39.086 --> 00:36:41.556
multiple sentences, and make

00:36:41.556 --> 00:36:43.576
sure that your training data is

00:36:43.576 --> 00:36:45.526
as similar as possible to the

00:36:45.526 --> 00:36:47.106
text that you expect to see and

00:36:47.106 --> 00:36:48.576
try to classify in practice.

00:36:49.646 --> 00:36:52.026
And covers as much as possible

00:36:52.286 --> 00:36:53.646
of the variations that you're

00:36:53.646 --> 00:36:55.866
likely to see when you encounter

00:36:55.866 --> 00:36:57.776
this text in your application.

00:36:58.906 --> 00:37:02.746
And, as we saw in our DBpedia

00:37:02.746 --> 00:37:04.806
example, you want to make sure

00:37:05.266 --> 00:37:08.806
that you have randomly sampled

00:37:08.806 --> 00:37:11.316
as much as possible distinct

00:37:11.316 --> 00:37:13.606
sets for training, for

00:37:13.606 --> 00:37:15.266
validation, and tests.

00:37:15.766 --> 00:37:17.696
This is basic data hygiene.

00:37:18.096 --> 00:37:21.706
And how do you know which

00:37:21.706 --> 00:37:23.306
algorithm will be best for your

00:37:23.306 --> 00:37:23.916
case?

00:37:24.336 --> 00:37:25.616
Well, in general, you'll have to

00:37:25.616 --> 00:37:28.426
try it, but some guidelines.

00:37:28.626 --> 00:37:29.886
You can start with the maxEnt

00:37:29.886 --> 00:37:30.466
classifier.

00:37:30.466 --> 00:37:31.456
It's very fast.

00:37:31.456 --> 00:37:32.606
It will give you an answer.

00:37:33.766 --> 00:37:35.206
But what does a maxEnt

00:37:35.206 --> 00:37:36.356
classifier do?

00:37:36.896 --> 00:37:38.926
A maxEnt classifier works by

00:37:38.926 --> 00:37:41.556
noticing the words that are used

00:37:42.146 --> 00:37:44.056
most often in the training

00:37:44.056 --> 00:37:44.556
material.

00:37:45.096 --> 00:37:46.806
So, for example, if you're

00:37:46.806 --> 00:37:49.756
trying to train positive and

00:37:49.756 --> 00:37:51.486
negative, it might notice words

00:37:51.486 --> 00:37:52.676
like love and happy are

00:37:52.736 --> 00:37:54.166
positive, hate and unhappy

00:37:54.166 --> 00:37:54.666
negative.

00:37:55.326 --> 00:37:57.406
And if the examples you

00:37:57.406 --> 00:37:59.106
encounter in practice use these

00:37:59.106 --> 00:38:00.426
same words, then the maxEnt

00:38:00.426 --> 00:38:01.876
classifier is going to work very

00:38:01.876 --> 00:38:02.146
well.

00:38:02.676 --> 00:38:08.146
What the Transfer Learning does

00:38:08.456 --> 00:38:10.076
is it notices the meaning of

00:38:10.076 --> 00:38:10.546
words.

00:38:11.056 --> 00:38:12.906
So if the examples you encounter

00:38:12.906 --> 00:38:14.476
in practice are likely to

00:38:14.476 --> 00:38:16.926
express a similar meaning with

00:38:17.016 --> 00:38:20.426
different words, then that is

00:38:20.426 --> 00:38:21.786
the case where Transfer Learning

00:38:21.786 --> 00:38:24.476
shines and it's likely to do

00:38:24.476 --> 00:38:27.156
better than the simple maxEnt

00:38:27.156 --> 00:38:27.546
model.

00:38:28.046 --> 00:38:33.086
So, to summarize, we have new

00:38:33.086 --> 00:38:35.676
APIs available for Sentiment

00:38:35.676 --> 00:38:38.706
Analysis, for Text Catalogs with

00:38:38.706 --> 00:38:41.906
MLGazetteer, for Word Embeddings

00:38:41.996 --> 00:38:45.376
with NL embedding, and we have a

00:38:45.376 --> 00:38:47.306
new type of text classification,

00:38:47.466 --> 00:38:49.046
a particularly powerful new type

00:38:49.716 --> 00:38:51.736
that takes advantage of Transfer

00:38:51.736 --> 00:38:52.056
Learning.

00:38:53.096 --> 00:38:55.226
I hope you'll take advantage of

00:38:55.226 --> 00:38:58.086
these in your applications, and

00:38:58.426 --> 00:38:59.566
there's much more information

00:38:59.566 --> 00:39:01.496
available online, and there are

00:39:01.496 --> 00:39:04.256
other related sessions that you

00:39:04.256 --> 00:39:05.256
can take a look at.

00:39:06.356 --> 00:39:06.796
Thank you.

00:39:07.516 --> 00:39:12.500
[ Applause ]